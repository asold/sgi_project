{
  "title": "Evaluating the advancements in protein language models for encoding strategies in protein function prediction: a comprehensive review",
  "url": "https://openalex.org/W4406682058",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5100638959",
      "name": "Jiaying Chen",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5087388223",
      "name": "Jingfu Wang",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5053458494",
      "name": "Yue Hu",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100445596",
      "name": "Xia Li",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5002981402",
      "name": "Yurong Qian",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5043474250",
      "name": "Chenxi Song",
      "affiliations": [
        "Xinjiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4396721167",
    "https://openalex.org/W4309397620",
    "https://openalex.org/W4323036511",
    "https://openalex.org/W4236236547",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W2103017472",
    "https://openalex.org/W4295063241",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W2062533676",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W1995808589",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W2312420878",
    "https://openalex.org/W2980298350",
    "https://openalex.org/W2472351724",
    "https://openalex.org/W2045204781",
    "https://openalex.org/W3137270128",
    "https://openalex.org/W4400015133",
    "https://openalex.org/W4390897322",
    "https://openalex.org/W2210738511",
    "https://openalex.org/W2754307534",
    "https://openalex.org/W2967369839",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2123858481",
    "https://openalex.org/W2032838501",
    "https://openalex.org/W3176163250",
    "https://openalex.org/W3201818282",
    "https://openalex.org/W4317374308",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W2951422523",
    "https://openalex.org/W3164046276",
    "https://openalex.org/W4391934213",
    "https://openalex.org/W2995514860",
    "https://openalex.org/W4404447386",
    "https://openalex.org/W4404824622",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2130925474",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2607633268",
    "https://openalex.org/W2063295453",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W4391821988",
    "https://openalex.org/W4236358448",
    "https://openalex.org/W2615066396",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3128878755",
    "https://openalex.org/W4392238186",
    "https://openalex.org/W4385291715",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3104092632",
    "https://openalex.org/W7052226288",
    "https://openalex.org/W4293046261",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W2010584082",
    "https://openalex.org/W2074231493",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W3144701084",
    "https://openalex.org/W2911871527",
    "https://openalex.org/W4320933419",
    "https://openalex.org/W4293430819",
    "https://openalex.org/W2117486996",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W2051210555",
    "https://openalex.org/W1810499140",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2535702297",
    "https://openalex.org/W4200079908",
    "https://openalex.org/W1557507368",
    "https://openalex.org/W2071105197",
    "https://openalex.org/W3201257466",
    "https://openalex.org/W4200166788",
    "https://openalex.org/W4309506674",
    "https://openalex.org/W4220991280",
    "https://openalex.org/W4388464011",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4366163632",
    "https://openalex.org/W4378803760",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W2946853220",
    "https://openalex.org/W4403174218",
    "https://openalex.org/W3165795318",
    "https://openalex.org/W2951282333",
    "https://openalex.org/W2951731136",
    "https://openalex.org/W4360938460",
    "https://openalex.org/W2258129851",
    "https://openalex.org/W4205368314",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W6873203099",
    "https://openalex.org/W6773327663",
    "https://openalex.org/W2989608901",
    "https://openalex.org/W4312097792",
    "https://openalex.org/W4406262333",
    "https://openalex.org/W2387908157",
    "https://openalex.org/W4403593408"
  ],
  "abstract": "Protein function prediction is crucial in several key areas such as bioinformatics and drug design. With the rapid progress of deep learning technology, applying protein language models has become a research focus. These models utilize the increasing amount of large-scale protein sequence data to deeply mine its intrinsic semantic information, which can effectively improve the accuracy of protein function prediction. This review comprehensively combines the current status of applying the latest protein language models in protein function prediction. It provides an exhaustive performance comparison with traditional prediction methods. Through the in-depth analysis of experimental results, the significant advantages of protein language models in enhancing the accuracy and depth of protein function prediction tasks are fully demonstrated.",
  "full_text": "Evaluating the advancements in\nprotein language models for\nencoding strategies in protein\nfunction prediction: a\ncomprehensive review\nJia-Ying Chen1,2,3*, Jing-Fu Wang1,2,3, Yue Hu1,2,3, Xin-Hui Li1,2,3,\nYu-Rong Qian2,3,4 and Chao-Lin Song1,2,3\n1School of Software, Xinjiang University, Urumqi, China,2Key Laboratory of Software Engineering,\nXinjiang University, Urumqi, China,3Key Laboratory of Signal Detection and Processing in Xinjiang Uygur\nAutonomous Region, Xinjiang University, Urumqi, China,4School of Computer Science and Technology,\nXinjiang University, Urumqi, China\nProtein function prediction is crucial in several key areas such as bioinformatics\nand drug design. With the rapid progress of deep learning technology, applying\nprotein language models has become a research focus. These models utilize the\nincreasing amount of large-scale protein sequence data to deeply mine its\nintrinsic semantic information, which can effectively improve the accuracy of\nprotein function prediction. This review comprehensively combines the current\nstatus of applying the latest protein language models in protein function\nprediction. It provides an exhaustive performance comparison with traditional\nprediction methods. Through the in-depth analysis of experimental results, the\nsigniﬁcant advantages of protein language models in enhancing the accuracy and\ndepth of protein function prediction tasks are fully demonstrated.\nKEYWORDS\nprotein function prediction, protein language model, deep learning, deep multi-label\nclassiﬁcation, gene ontology (GO)\n1 Introduction\nAs key macromolecules in the life sciences, proteins play a cornerstone role in a variety\nof biological processes within the cell. Accurate characterization of protein function is of\nvital importance for disease research (Barabási et al., 2011; Xuan et al., 2019), drug discovery\n(Kissa et al., 2015; Zeng et al., 2016), and biotechnology advancement (Shehu et al., 2016).\nHowever, traditional experimental methods are not only time-consuming and labor-\nintensive but also inefﬁcient (Colin et al., 2015; Cui et al., 2019; Torres et al., 2021). As\nof February 2024, while the UniProt database contains over 240 million protein sequences,\nless than 0.3% of these sequences have functionalities that have been experimentally\nvalidated and standardly annotated (uni, 2023). This huge gap between sequencing and\nannotation urgently calls for the development of efﬁcient and reliable automated function\nprediction tools to save human resources and time costs (Radivojac et al., 2013).\nPrior to the advent of the protein language model (PLM), numerous high-performance\ncomputational methods based on sequence similarity and deep learning have been proposed\nto address this challenge (Kulmanov et al., 2018; You et al., 2018; 2019; Li et al., 2024).\nAlthough these methods have made signiﬁcant progress in function prediction, they fail to\nOPEN ACCESS\nEDITED BY\nGenlin Zhang,\nShihezi University, China\nREVIEWED BY\nJianzhao Gao,\nNankai University, China\nDaipayan Sarkar,\nNational Institutes of Health (NIH), United States\nShen Huang,\nZhengzhou University of Light Industry, China\n*CORRESPONDENCE\nJia-Ying Chen,\nchenjy@xju.edu.cn\nRECEIVED 05 October 2024\nACCEPTED 02 January 2025\nPUBLISHED 21 January 2025\nCITATION\nChen J-Y, Wang J-F, Hu Y, Li X-H, Qian Y-R and\nSong C-L (2025) Evaluating the advancements\nin protein language models for encoding\nstrategies in protein function prediction: a\ncomprehensive review.\nFront. Bioeng. Biotechnol.13:1506508.\ndoi: 10.3389/fbioe.2025.1506508\nCOPYRIGHT\n© 2025 Chen, Wang, Hu, Li, Qian and Song. This\nis an open-access article distributed under the\nterms of theCreative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nFrontiers inBioengineering and Biotechnology frontiersin.org01\nTYPE Review\nPUBLISHED 21 January 2025\nDOI 10.3389/fbioe.2025.1506508\nfully utilize the large amount of unannotated protein information.\nThe amount of data on these unannotated proteins is growing, and\nthe imbalance between the ratio of unannotated proteins to\nannotated proteins is widening ( Kihara and Kihara, 2017 ).\nFurthermore, traditional deep learning methods rely on hand-\ndesigned feature extractors. These feature extractors cannot\nadequately capture the complexity and diversity of protein\nsequences, which limits the predictive power of the model\n(Aggarwal and Hasija, 2022 ; Bonetta and Valentino, 2020 ;\nBernardes and Pedreira, 2013 ). The introduction of protein\nlanguage models has skillfully overcome these long-standing\nproblems and revolutionized the researchﬁeld.\nInspired by the success of large-scale models in computer vision\nand natural language processing, theﬁeld of bioinformatics has also\nseen the rise of pre-trained protein language models. The\nintroduction of the Transformer architecture has laid a solid\nfoundation for the rapid growth of protein language models.\nSince the introduction of the Transformer architecture,\nresearchers have begun to apply it to the processing of protein\nsequence data, and the ensuing growth of protein language models\nhas been a springtime phenomenon. These large-scale protein\nlanguage models, based on tens of millions to billions of protein\nsequences that are self-supervised and pre-trained, represent the\nstate-of-the-art in predicting protein sequence function andﬁtness.\nBy pre-training on huge datasets of unlabeled protein sequences,\nthese models are capable of automatically extracting features from\nmassive data andﬁne-tuning them on speciﬁc downstream tasks.\nProtein language models focus on three core tasks: protein function\nprediction, protein sequence generation, and protein structure\nprediction ( Lin et al., 2023 ; Weissenow et al., 2022 ). These\nmodels play an important role in genomics, helping researchers\nto deeply interpret complex genomic data and reveal the subtle\nrelationship between genes and proteins ( Hu et al., 2024 ). In\nsynthetic biology, protein language models help researchers\ndesign novel proteins or optimize the properties of existing\nproteins (He et al., 2024; Chen et al., 2024). In addition, in drug\ndesign, these models provide powerful support for the design and\ndevelopment of next-generation drugs by accurately predicting the\nstructure of proteins and their interactions with small molecules\n(Zheng et al., 2024). Among the many tasks, protein function\nprediction, as the most basic and direct task, can intuitively\nreﬂect the effect of self-supervised training of protein language\nmodels. Therefore, this paper chooses to comprehensively review\nprotein language models in the context of protein function\nprediction to comprehensively evaluate and compare the\nperformance of these models on function prediction tasks and\nreveal their advantages.\nWithin the ﬁeld of protein function prediction, the ESM 1b\nmodel (Rives et al., 2021) has attracted attention for its wide range of\napplications. The model achieves accurate prediction of protein\nfunction by analyzing the evolutionary information of protein\nsequences. The use of ESM 1b as a coding tool has signiﬁcantly\nimproved the accuracy of the protein function prediction task (Li\net al., 2023; Yao et al., 2021). Not only ESM 1b but also many other\nprotein language models can also outperform most of the protein\nfunction prediction methods in the CAFA Challenge. In recent\nyears, emerging methods have commonly adopted pre-trained\nprotein language models to extract sequence features (Wang S.\net al., 2023; Pan et al., 2023; Zhang et al., 2023; Wang Z. et al.,\n2023; Kulmanov et al., 2024; Yuan et al., 2023). Thus, it has become\nan irreversible trend for protein language models to gradually\nreplace the traditional sequence coding methods. In the current\nresearch context, the adoption of protein language models has\nbecome an inevitable choice if protein function prediction\nmodels are to remain competitive. In view of the central position\nof protein language modeling in function prediction, this review was\nborn. By deeply analyzing and comparing the architectures,\nfunctions, training strategies, and datasets used in various protein\nlanguage models, we aim to help researchers fully grasp and\nunderstand protein language models, and then be able to\nskillfully apply them. By effectively utilizing these advanced tools,\nresearchers will be able to signiﬁcantly improve the accuracy of\nprotein function prediction tasks and promote their wide\napplication in the biomedical ﬁeld, which will ultimately\ncontribute to the solution of cutting-edge scienti ﬁc problems\nsuch as drug design and disease mechanism research.\nThis review is structured as follows: Section 2\nreviews the\ndevelopment history of protein function prediction, andSection\n3 introduces representative methods in the development history of\nprotein function prediction methods, including statistically based\nmethods, machine learning, and deep learning methods.Section 4\ncomprehensively combines through the various protein language\nmodels currently available for ontology prediction tasks, comparing\ntheir architectures, functions, and training datasets to compare the\neffectiveness of each protein language model in ontology prediction\ndownstream tasks.Section 5describes the protein sequence dataset\nand evaluation metrics.Section 6shows the results and analysis of\nthe ﬁne-tuned protein language models on three datasets.Section 7\nwill select the human tRNA pseudouridine (38/39) synthetase\nprotein as a case study, aiming to assess the prediction\neffectiveness and depth of different protein language models\nthrough speciﬁc examples. Section 8 summarizes this review,\nassesses the existing issues and trends in theﬁeld, and looks into\nthe future direction of protein language modeling and protein\nfunction prediction.\n2 A brief history of protein function\nprediction\nIn order to deeply explore and verify the speciﬁc functions of proteins\nand their mechanisms of action in living organisms, researchersﬁrst\nrelied on biochemical experiments for protein function prediction. In\n1875, scienceﬁrst revealed the biological function of hemoglobin, an\nachievement made possible by theuse of the spectrophotometer (Ma\net al., 2007; Thein, 2011). With this technique, scientists observed that\nhemoglobin can bind oxygen reversibly, thus recognizing its key function\nof transporting oxygen in vertebrate blood. Subsequently, between\n1926 and 1930, research methods of crystallization and activity\ndetermination successfully revealed that enzymes, molecules with\nbiocatalytic functions, are composed of proteins (Simoni et al., 2002;\nManchester, 2004). Between the 1950s and the 1970s, protein isolation\nand puriﬁcation techniques became increasingly sophisticated, with\nsalting out, ion-exchange chromatography, gel- ﬁltration\nchromatography, and afﬁnity chromatography enabling proteins to be\nseparated from complex cellular structures.\nFrontiers inBioengineering and Biotechnology frontiersin.org02\nChen et al. 10.3389/fbioe.2025.1506508\nIn the 1970s and 1980s, with the creation of protein sequence\ndatabases, scientists discovered that proteins with similar sequences\noften have similar functions. Using sequence comparison tools,\nresearchers were able to hypothesize about the functions of\nunknown proteins by comparing them to proteins with known\nfunctions.Into the 1990s, it was gradually recognized that the key to\na deeper understanding of protein function lay in accurately\npredicting its three-dimensional structure. Although the detailed\nmechanism of how proteins form their functional structures\nthrough the dynamic folding process is not yet fully understood,\nthe concept of“structure determines function”has gradually become\na consensus in the scientiﬁc community (Avery et al., 2022). With\nthe advancement of computer technology, it became feasible to\nstudy protein behavior using molecular dynamics (MD) simulations\nin the late 1990s. Researchers began to use computational methods\nto predict protein functions from known protein structures in the\nProtein Data Bank (PDB) ( Berman et al., 2000 ; 2003), thus\npromoting the formal formation and development of theﬁeld of\nprotein function prediction.\nFrom 2018, the remarkable achievements of protein language\nmodels in structure prediction have provided a great boost to protein\nfunction prediction. The breakthroughs in 3D structure prediction\nmade by models such as AlphaFold and RosettaFold have made it\npossible to obtain a large number of protein structures from\nsequence data ( Jumper et al., 2021 ; Baek et al., 2021 ). The\nstructures predicted by AlphaFold have been proven to apply to\nprotein function prediction (Ma et al., 2022; Gligorijević et al., 2021),\nwith an accuracy of more than 92%, and an average error of 1 Å\n(Varadi et al., 2024), which is almost indistinguishable from the real\nstructural information, effectively solving the difﬁcult problem of\nmismatch between structure and massive sequence data in protein\nfunction prediction. This effectively solves the problem of mismatch\nbetween structure and massive sequence in protein function\nprediction.\nFigure 1illustrates the evolution of protein function prediction\nmethods. The progression of protein function prediction has\ntransitioned from relying on individual biochemical experiments\nto assess protein functions, to utilizing sequence similarity\ncomparisons (Needleman and Wunsch, 1970), and eventually to\nemploying computational methods based on machine learning and\ndeep learning (Jensen et al., 2002). Each phase in this development\nhas signiﬁcantly advanced protein research and laid a robust\nfoundation for modern, precise, and automated function\nprediction techniques. While each method has been instrumental\nin its era, they all have had their limitations. In light of this, the\nadvent of protein language modeling is particularly pressing and\nsigniﬁcant. The emergence of protein language modeling not only\nrepresents a technological innovation but also indicates the\ninevitable trajectory of scientiﬁc research in harmony with the\nMarch of time (Rives et al., 2021).\n3 Previous methods\n3.1 Statistically based protein function\nprediction\nThe use of protein sequence homology to develop\ncomputational tools for protein function annotation was a\nclassical early approach. This approach is based on the\nassumption that proteins with similar sequences usually possess\nsimilar structures and functions during evolution. Homologous\nproteins derive from a common ancestor and have evolved to\nretain key amino acids to perform similar or identical biological\nfunctions. The prediction logic is: that proteins whose functions are\nexperimentally veriﬁed can be used as references, and proteins\nwhose functions are unknown but whose amino acid sequences\nare known can be used as targets. The amino acid sequence\nsimilarity between known functional proteins and the target\nproteins can be calculated by using a sequence comparison tool\n(Pearson, 2016; Altschul et al., 1997; Remmert et al., 2012) and the\nsimilarity can be used to determine whether the target proteins have\nthe same functions as the known functional proteins or not. It is\ngenerally believed that if the amino acid sequence similarity of two\nproteins exceeds 30%, they may have the same function (Chagneau\net al., 2024).\nIn 1990, Altschul et al. (1990)developed the BLAST tool for\npairwise sequence comparison, which is able to directly approximate\nand optimize the comparison of local similarities. BLASTﬁrst uses\nproteins with known functions to build a search database, then\ncompares the target proteins in the database, ranks the comparison\nresults according to the level of similarity, and uses the functions of\nthe most similar proteins to infer the function of the target protein.\nThe invention and application of BLAST marked an important\nmilestone in bioinformatics tools, enabling scientists to more\nefﬁciently utilize the growing amount of biological sequence data\nto predict protein function, making it one of the most widely used\ntools in bioinformatics.\nFIGURE 1\nA brief history of the development of protein function prediction tasks, from statistically based methods to machine learning, deep learning to\ntoday’s protein language models.\nFrontiers inBioengineering and Biotechnology frontiersin.org03\nChen et al. 10.3389/fbioe.2025.1506508\nReleased in November 2014, DIAMOND (Buchﬁnk et al., 2015)\nis a highly efﬁcient protein sequence comparison tool that uses a\ndual-indexing algorithm to accelerate the comparison process,\nmaking it particularly suited to the rapid analysis of high-\nthroughput sequencing data. The core of the algorithm lies in its\nhigh speed and sensitivity, making it excellent at handling large-scale\nprotein sequence databases. DIAMOND rapidly retrieves and\nmatches query sequences during the alignment phase by\nconverting protein sequences from reference databases into a\ncompressed index format. It also introduces the use of spacer\nseeds to improve performance in sequence comparison.\nDIAMOND is used in a wide range of applications, including\ngenome annotation, metabolic pathway analysis, and microbial\ncommunity analysis. Due to its high speed and efﬁciency, it has\nbecome an important tool in bioinformatics research and big\ndata analysis.\nStatistical methods based on homology play an important role in\nthe early stages of protein function prediction. However, when the\namino acid sequence similarity decreases, the reliability of the\nprediction results of this homology-based method decreases\nrapidly (Devos and Valencia, 2000; 2001). When the amino acid\nsequence similarity between the target protein and known functional\nproteins is low, it is easy to generate false propagation of functional\ninformation, leading to poor prediction results. Only when the\nsequence similarity reaches 60% or more, do the results of\nhomology-based inference methods have a high degree of\nconﬁdence ( Cruz et al., 2017 ). Moreover, structurally similar\nproteins may also possess similar functions, and structurally\nsimilar proteins may not necessarily be similar in sequence,\nwhereas statistically based methods can only utilize sequence\ninformation. Thus statistically based methods have signi ﬁcant\nlimitations in data to ensure accuracy in the task of protein\nfunction prediction, and better methods need to be proposed to\nmeet this challenge.\n3.2 Machine learning-based protein function\nprediction\nMachine Learning-based Protein Function Prediction\nConsidering protein function prediction as a multi-label, multi-\nclassiﬁcation problem, machine learning algorithms solve this\nproblem by constructing multi-label classi ﬁcation models. This\ntype of approach usually consists of four steps: feature extraction,\nfeature selection, training the model, and classiﬁcation prediction.\nFeature extraction involves de ﬁning and extracting sequence\nfeatures, mainly in terms of compositional features,\nphysicochemical properties, and structural features of amino acid\nsequences. Common protein sequence features include the\nfrequency, position, and order of amino acid residues, as well as\nthe hydrophobicity, polarity, and charge of amino acids, and\nstructural domains. Feature selection, on the other hand, involves\ndenoising and de-redundancy of the feature set obtained in the\nfeature extraction stage to improve the training ef ﬁciency and\nprediction accuracy of the model. The training model stage is\nbased on the feature set after feature selection and uses speciﬁc\nmachine learning algorithms to build the classi ﬁcation model.\nCommonly used machine learning methods include Genetic\nAlgorithm, KNN (K-Nearest Neighbor), and SVM (Support\nVector Machine). Classiﬁcation prediction, on the other hand,\ninputs the features of the sequence to be tested into the model\nbuilt in the training phase and uses the model to determine whether\nthe sequence to be tested belongs to the same class as a protein\nsequence with a speciﬁc function.\nThe deepNF proposed in 2018 (Gligorijević et al., 2018) uses a\nmultimodal deep autoencoder to extract features, which are then\npassed to an SVM. The SVM is one of the most commonly used\nalgorithms in the initial attempts to use machine learning techniques\nfor protein function prediction. GODoc is a protein function\nprediction method that utilizes TFPSSM(Term Frequency based\non PSSM) features (Liu et al., 2020). TFPSSM is a feature vector\nbased on the frequency of the gapped dipeptides in the position-\nspeciﬁc scoring matrix (PSSM). They proposed three different\nmethods TFPSSM 1NN(1-Nearest Neighbor), TFPSSM\nCATH(Dynamic-KNN with FunOverlap), and TFPSSM Vote\n(Combines Fixed-KNN, Dynamic-KNN, and Hybrid-KNN voting\nschemes) to improve the accuracy, and also proved that the KNN\nvariant with a dynamic voting scheme can outperform the\ntraditional KNN method.\nPANNZER (Törönen and Holm, 2022) is another tool for\npredicting protein function using weighted KNN classi ﬁers,\ndesigned for automated function prediction tasks and supporting\ngenome-level queries. KNN methods are favored for their simplicity,\nease of understanding, ease of implementation, lack of need for\nestimating parameters, and low retraining costs. However, KNN has\nsome limitations, such as it is a lazy learning method,\ncomputationally intensive, and the output results are weakly\ninterpretable. In recent years, KNN has been mainly applied in\nthe ﬁelds of text classiﬁcation, cluster analysis, predictive analysis,\npattern recognition and, image processing.\nProtein function prediction algorithms based on shallow\nmachine learning are able to annotate protein functions to a\ncertain extent, but their effectiveness is often limited by noise\ninterference in the data. The sensitivity of these algorithms to\nnoise makes the prediction results susceptible to the quality of\nthe data, leading to reduced accuracy. In addition, these\nalgorithms are highly dependent on biological prior knowledge\nand complex feature engineering, limiting their ability to be\napplied to large and diverse datasets. Shallow machine learning-\nbased methods make it dif ﬁcult to achieve a qualitative\nbreakthrough in the accuracy and coverage of protein function\nprediction. With the explosive growth of protein and the\nimprovement of computational power, applying deep learning\nmethods in protein function prediction is more promising\n(Radivojac et al., 2013). It provides a new way to address the\nlimitations of current methods.\n3.3 Deep learning-based protein function\nprediction\nIn recent years, the successful applications of deep learning\ntechniques in computer vision, natural language processing,\nstructure prediction, and sentiment analysis have demonstrated\ntheir powerful feature-learning capabilities ( Abramson et al.,\n2024; Lin et al., 2023 ). For better proteomics research,\nFrontiers inBioengineering and Biotechnology frontiersin.org04\nChen et al. 10.3389/fbioe.2025.1506508\nresearchers have proposed a number of protein function annotation\nmethods that utilize deep learning techniques to extract deep\nfeatures from protein characterization and integrate multiple data.\nConvolutional neural networks (CNN) wereﬁrst proposed in\nthe late 1980s and early 1990s (LeCun et al., 1989), but did not gain\nwidespread attention until after AlexNet’s( Krizhevsky et al., 2012)\nbreakthrough performance in the ImageNet competition in 2012.\nCNN locally extract features through a convolutional layer, reduce\nspatial dimensionality through a pooling layer, and classify or\nregress through a fully connected layer. DeepGOPlus proposed\nby Kulmanov and Hoehndorf (2020). uses convolutional neural\nnetworks to extract functional features on protein sequences for\nannotation, which is valuable for functional annotation of a large\nnumber of newly sequenced unknown genes in macro genomes.\nHowever, the method uses amino acid solo heat codes to represent\nsequences, which does not take into account the semantic\ninformation of amino acids, and the sparsity of solo heat codes\nmay adversely affect model training.\nRecurrent Neural Networks (RNN) are designed for processing\nsequence data such as time series analysis, language modeling, and\nmachine translation. RNN are able to process input sequences of\ndifferent lengths and capture temporal dynamics in sequences\nthrough hidden states. The GONET model (Li et al., 2020) uses\nRNN to extract long-range links of protein sequences based on CNN\nto extract local features of sequences. The conserved region features\nrelated to the tertiary structure are extracted through the attention\nmechanism to effectively identify the protein structure domains and\nmodalities. Thus, the prediction performance is improved.\nThe Transformer model, proposed byVaswani et al. (2017)in\n2017, is entirely based on the attention mechanism, discarding the\ntraditional loop structure and effectively capturing global\ndependencies by considering all elements in the sequence\nsimultaneously through the self-attention mechanism. The TALE\nalgorithm, proposed byCao and Shen (2021)in 2021, applies the\nTransformer model to protein function prediction The global\nfeatures of protein sequences are extracted by the self-attention\nmechanism, and the hierarchical associations between functional\ntags are extracted by joint sequence-functional tag embedding\nlearning, which improves the prediction performance by\ncombining protein sequence similarity. The DeepGOA model\n(Zhou G. et al., 2019 ) innovatively introduces a graph\nconvolutional neural network to learn the dependencies between\ngene ontology terms extracts the sequence features by CNN, and\nﬁnally minimizes the differences between the tags and the\ndifferences in the distribution between features for function\nprediction.\nAlthough deep learning methods have made signiﬁcant progress\nin protein function prediction, they still have obvious limitations\ncompared to protein language models. Speci ﬁcally, the feature\nrepresentations of deep learning methods are too sparse to reﬂect\nthe complex relationships between amino acids, are less efﬁcient in\ndealing with long-range dependencies and long sequences, and\nrequire signiﬁcant computational resources and time for training.\nIn addition, deep learning models usually fail to effectively integrate\nprior knowledge of biology, leading to unsatisfactory performance\non cross-species datasets (Yang et al., 2024; Elhaj-Abdou et al.,\n2021). Also, the interpretability and controllability of these models\nare relatively weak. In contrast, protein language models are able to\nefﬁciently utilize unlabeled data through the pre-training phase to\ndeeply mine the rich information of biological evolution, thus\ndemonstrating a stronger capability in dealing with large-scale\nand complex biological data.\nProtein function prediction can be likened to a natural language\nprocessing task in theﬁeld of bioinformatics, where amino acids are\nregarded as the basic units of a“vocabulary”and protein sequences\nare the equivalent of“sentences”composed of these“vocabularies”.\nSentences”are composed of these “words” (Ofer et al., 2021 ).\nCompared with the traditional natural language processing\nproblem, the protein sequence composed of 20 amino acids is\ncloser to the character-level natural language processing. In\nnatural language processing, the choice of an appropriate\nencoding method is crucial to the performance and\ninterpretability of the model, and this principle should not be\nignored in the ﬁ\neld of protein function prediction as well.\nTraditional coding methods, such as one-hot coding and bag-of-\nwords models, often fail to effectively capture the intrinsic\nconnections between amino acids due to the sparseness of their\nrepresentations. In contrast, the adoption of protein language\nmodeling as a coding tool can better capture long-distance\ndependencies in sequences and provide a deeper understanding\nof amino acid interactions. In addition, the positional embedding\nfunction of protein language models integrates evolutionary\ninformation, providing richer and more detailed sequence\ncharacterization for protein function prediction.\n4 Protein language modeling approach\nThe emergence of protein language models solves the notable\nproblems of previous approaches by ef ﬁciently utilizing large\namounts of unlabeled protein sequence data through self-\nsupervised learning, which can identify amino acids that have\nremained unchanged during the evolutionary process and are\noften critical for protein function. Their training data contains\nprotein sequences across multiple species, which enables the\nmodels to learn the commonalities and differences in protein\nsequences across species, reﬂecting the changing trends during\nevolution and capturing evolutionary information in protein\nsequences. These models are based on the distributional\nassumption that amino acids appearing in similar contexts tend\nto have similar meanings ( Bepler and Berger, 2021 ). With\nautoregressive formulas or masked position prediction formulas,\nprotein language models can be trained using probability\ndistributions of amino acids to extract deep semantic information.\nIn an autoregressive language model, the probability of a sequence\nis decomposed into the probabilities of individual tokens, and the\nprobability of each token depends only on the tokens that precede it.\nThe drawback of this approach is that the representations learned at\neach location only take into account the preceding context, which may\nlimit their effectiveness as full contextual representations. The Masked\nLanguage Modeling (MLM) approach, on the other hand, overcomes\nthis limitation by considering the probability distribution of the tokens\nat each position conditional on all other tokens. Although masked\nlanguage modeling does not allo w the calculation of correctly\nnormalized probabilities for theentire sequence, this approach is\nmore appropriate when the learned representation is the main concern.\nFrontiers inBioengineering and Biotechnology frontiersin.org05\nChen et al. 10.3389/fbioe.2025.1506508\nCommon protein language models employ bidirectional long\nshort-term memory networks (BiLSTM) ( Huang et al., 2015 ),\nTransformer, and their variants. BiLSTM requires less training\ndata and computational resources. As hardware resources\nincreased and protein sequence data continued to grow, later\nprotein language models began to adopt deep Transformer\narchitectures, such as BERT (Devlin et al., 2018), T5 (Raffel et al.,\n2020), and variants of GPT (Radford et al., 2019; Madani et al., 2020;\nNijkamp et al., 2023; Ferruz et al., 2022; Shuai et al., 2021; Munsamy\net al., 2022) (for generative tasks). These models are trained on a large\nnumber of protein sequences to generate so-called embeddings\n(values extracted from the ﬁnal hidden layer of the protein\nlanguage model), which not only contain local and global features\nof the sequences, but also efﬁciently utilize the implicit information in\nthe large-scale unannotated data, and can be easily migrated to a wide\nvariety of protein prediction tasks, including functional prediction\n(e.g., gene ontology, signaling, binding residues or subcellular\nlocalization) and protein structure prediction, etc.\nThe process of function prediction by protein language model is\nshown inFigure 2. Firstly, the protein sequences are input into the\npre-trained protein language model, and the features in each protein\nsequence are extracted using its encoder part. These features are\nconstructed into a feature matrix, which is then fed into its own\nmodel for learning. Speciﬁcally, the feature matrix is nonlinearly\ntransformed and features are extracted through a number of fully\nconnected layers, which include activation functions and dropout\nlayers between them to enhance the expressiveness of the model and\nprevent overﬁtting. Finally, the feature vectors are fed into a linear\nlayer that maps the high-dimensional features to the ﬁnal\nclassiﬁcation result space, outputting the classi ﬁcation results\npredicted by the protein function.\n4.1 Autoregressive model\nTable 1shows the size and architecture of the encoder part of the\nprotein language model used in this paper. SeqVec is a protein\nlanguage model that employs an autoregressive model that is able to\ntake into account previous information. It also borrows features\nfrom the BERT model, which predicts blocked words given all\nunblocked words. The architecture of SeqVec is based on the\nELMO model using the CharCNN (Zhang et al., 2015) algorithm\nto obtain local features of amino acids and two layers of BiLSTM that\nintroduce contextual information about the surrounding words. The\nfeature vector for each amino acid is obtained by averaging the bi-\ndirectional outputs of the CharCNN and LSTM layers.\n4.2 Masked language modeling objective\nbased on the BERT architecture\nAll models except the SeqVec model (Heinzinger et al., 2019) use\na masked language modeling objective to train the model. These\nmodels take the amino acid sequence of a protein and randomly\nmask certain amino acids in the input sequence. The processed\nsequences are encoded using one-hot coding, and their\nrepresentation is enhanced by positional coding and is\nsubsequently fed into a network structure consisting of a\nplurality of self-attention blocks ( Zhu et al., 2022 ). Each self-\nattention block contains within it multiple attention heads, linear\ntransformation units, and feedforward neural networks. At the last\nattention layer of the model, the output is a probability matrix that\ndemonstrates the model ’s predicted probability distribution of\namino acid species for each masked location. As the depth of the\nFIGURE 2\nProtein sequences are fed into a pre-trained protein language model to get the output features of its encoder. These features are constructed into a\nfeature matrix, which is then input into the model for training and testing. Theﬁnal GO term probability predicted by each protein is obtained.\nFrontiers inBioengineering and Biotechnology frontiersin.org06\nChen et al. 10.3389/fbioe.2025.1506508\nnetwork increases, the output of each layer of the attention block\nforms a feature embedding that is progressively able to capture more\nﬁne-grained sequence features. These feature embeddings provide\nrich amino acid contextual information for subsequent protein\nfunction prediction tasks.\nProtBERT employs the BERT architecture, which is a pure\nencoder model without a decoder component and is particularly\nsuited for Natural Language Understanding (NLU) tasks. ProtBERT\nincreases the number of layers to 30 on top of the original BERT,\nwith 420M parameters and UniRef100 protein sequence dataset to\ncomplete training. Compared to models based on convolutional\nneural networks and recurrent neural networks, ProtBERT uses a\nself-attentive mechanism to process each character in the sequence,\nexpanding the global receptive ﬁeld and enabling more effective\ncapture of global contextual information.\nESM 1b and ESM2 (Lin et al., 2023) are protein language models\nbased on the RoBERTa architecture ( Liu et al., 2019 ), which\nimproves and optimizes the traditional BERT model. RoBERTa\nimproves performance by increasing the model size, using larger\nmodel parameters, larger batch sizes, and more training data. Unlike\nBERT, RoBERTa removes the Next Sentence Prediction task from\nBERT and employs a dynamic masking strategy that generates a new\nmasking pattern each time a sequence is input, thus better adapting\nto different linguistic representations and further improving the\nmodel performance.\nESM 1b was proposed in 2020, which employs a masked language\nmodeling objective to train the modelthrough a self-supervised learning\ntechnique, and trains a RoBERTa model with 650M parameters and\n33 layers on the UniRef50 dataset. And in 2022, ESM2 was trained using\nmasked language modeling over millions of different natural protein\nevolutions with up to 15 B. During training, protein sequences are\npresented to the model with a portion of the residues masked, randomly\naligned to different amino acids, or left unmodiﬁed. The task of the\nmodel is to predict those masked residues in a bidirectional context of\nall unmasked residues in the input.\nCompared to traditional RNN and LSTM models, RoBERTa is\nable to execute concurrently, improving the computational\nefﬁciency of the model. However, static masking may result in\nthe model not being able to adequately adapt to different\nmasking strategies. Therefore, RoBERTa employs a dynamic\nmasking strategy with more training data and a deeper network\nstructure, but this also leads to longer training time and increased\ncomplexity in training and deployment.\n4.3 Masked language modeling objective\nbased on the T5 architecture\nPortT5 (Elnaggar et al., 2021), ProstT5 (Heinzinger et al., 2023), and\nAnkh (Elnaggar et al., 2023) are protein language models based on the\nT5 (Text-to-Text Transformer) architecture. The T5 model was\noriginally designed to deal with sequence-to-sequence problems,\nsuch as machine translation. The unique feature of T5 is that it\nuniﬁes a variety of NLP tasks into a single text-to-text\ntransformational process, by embedding the task T5 is unique in\nthat it uniﬁes various NLP tasks into a text-to-text transformation\nprocess by embedding the tasks into the input text to solve various NLP\ntasks. This design makes the T5 model highly task-adaptable and\ncapable of beingﬁne-tuned to accomplish many different NLP tasks.\nIn these models, ProstT5 further extends the initial pre-training\ntarget of ProtT5 to amino acid (AA) and 3D structure (3Di)\nsequences. By transforming protein structures into one-\ndimensional strings, conversion from sequence to structure and\nfrom structure to sequence can be achieved. However, not all protein\nprediction tasks directly beneﬁt from the coupling of 3Di and AA,\nand may even fall short in functionally relevant tasks.\nAnkh uses a T5-like architecture with a 48-layer Transformer\nthat performs 1-g random token masking with a default probability\nof 20% in the input sequence and performs complete de-masking/\nreconstruction of the sequence. In contrast, Ankh has a larger\nembedding dimension, more attention heads, and more\nfeedforward layers, which enhances the model’s representational\ncapabilities. However, the T5 model needs to be applied and adapted\nwith caution due to its reliance on a large amount of pre-training\ndata and the fact that its complexity can lead to overﬁtting problems,\nespecially on small datasets.\n5 Dataset and evaluate\nFor the protein function prediction task, researchers can utilize\ntwo open databases,The UniProt Consortium (2023)and Protein\nTABLE 1 Utilized protein language models.\nModel Base model Dataset Parameters (encoder) Encoder layers Emb.Size\nESM 1b RoBERTa UniRef50 650M 33 1,280\nESM2 650M RoBERTa UniRef50 650M 33 1,280\nESM2 3B RoBERTa UniRef50 3B 36 2,560\nPortT5 T5 UniRef50 1.2B 24 1,024\nPortBert BERT UniRef100 420M 30 1,024\nProstT5 T5 BFD 1.2B 24 1,024\nSeqvec ELMO UniRef50 93M 3 1,280\nAnkh Base T5 UniRef50 450M 48 768\nAnkh Large T5 UniRef50 1.1B 48 1,536\nFrontiers inBioengineering and Biotechnology frontiersin.org07\nChen et al. 10.3389/fbioe.2025.1506508\nData Bank (PDB), to obtain protein sequence data from different\nspecies. These data can be used to train prediction models through\nbatch downloading, data cleaning, and pre-processing. In addition,\nresearchers can also use the CAFA dataset, which relies heavily on\nthe Uniprot database and contains protein sequences across species.\nThese sequences may have retained similar functions during\nevolution or may have undergone functional divergence. CAFA\naims to assess and improve the applicability of functional prediction\nmethods across organisms, provide standardized data to address the\nchallenges of building computational models for protein function\nclassiﬁcation, and provide a valuable resource for evaluating and\nimproving prediction models.\nIn order to standardize functional annotations, the Gene\nOntology Consortium introduced Gene Ontology (GO), which\nclassiﬁes protein annotations into Molecular Function (MF),\nBiological Process (BP), and Cellular Component (CC).\nMolecular Function describes the role of a gene product at the\nmolecular level, such as the catalytic activity of an enzyme or the\nsignaling function of a protein. Biological processes involve speciﬁc\nbiological events or pathways in which the gene product is involved,\nsuch as cell cycle regulation or immune response. Cellular\ncomponents, on the other hand, are concerned with the location\nof the gene product within the cell, including structures such as\norganelles and cell membranes. As scientiﬁc knowledge continues to\naccumulate and be updated, the GO framework is constantly being\nimproved to ensure its accuracy and currency as a standard for\nfunctional annotation in biological research.\nIn our study, we used the human protein sequence dataset as\nwell as the CAFA3 (Zhou N. et al., 2019) and CAFA4 datasets from\nDeepGOPlus. For the CAFA3 and CAFA4 datasets, we utilized the\nGene Ontology (GO) data provided by the CAFA Challenge. For the\nhuman dataset, the reviewed and manually annotated human\nprotein sequence dataset (Human2024) was collected from the\nSWISS-PROT ( Boutet et al., 2016 ) database. Based on the\ntimestamp information, we used proteins with experimental\nannotations obtained before 24 January 2014, as the training set,\nproteins with experimental annotations obtained between\n24 January 2014, and 24 January 2017, as the validation set, and\nproteins with experimental annotations obtained between\n24 January 2017, and 24 January 2024, as the test set. We used\nannotation information from the Gene Ontology Annotation\n(GOA) database (Ashburner et al., 2000; Aleksander et al., 2023)\nand ﬁltered it to remove non-experimental GO annotations as well\nas terms not in the GO tree.Table 2summarizes the details of the\ndatasets used in this study. Through the statistical plots of protein\nlengths in the above three datasets presented inFigure 3, we can\nlearn that most of the protein sequences are within 1,000 lengths, so\nwe intercepted the amino acid sequences with lengths ranging\nfrom 0 to 1,000.\nSince the protein function prediction problem is usually\ntransformed into a multi-label learning problem, the evaluation\nmetrics chosen can also be based on the criteria commonly used in\nmulti-label learning, and the following four evaluation metrics are\nchosen in this paper:\n1. Fmax (Maximum F metric): Fmax is the maximum F metric\nvalue computed over all prediction thresholds. F metric is\nthe harmonic mean of Precision (TP/(TP + FP)) and Recall\n(TP/(TP + FN)), where TP denotes true positives, number of\nfunctions of a protein that are correctly labeled, and FP denotes\nfalse positives, number of functions of a protein that should not\nbe, but are incorrectly labeled. Where TP denotes true\npositives, the number of proteins whose function is\ncorrectly labeled, and FP denotes false positives, the number\nof proteins whose function is incorrectly labeled as a negative\nsample function, and FN denotes false negatives, the number of\nproteins whose functions are incorrectly labeled as negative\nsample functions.\n2. AUPR (area under the precision-recall curve): AUPR is used to\napproximate the region under the precision-recall curve by\nusing the trapezoidal rule, which is commonly used for the\nevaluation of multi-label, multi-classiﬁcation tasks. A higher\nAUPR value indicates a better performance of the model in\nprotein function prediction. AUPR focuses on high precision\nand recall, which is especially important for the precision\nrequirement in protein function prediction.\n3. AUC (area under the ROC curve): The AUC is calculated by\nconsidering all possible classiﬁcation thresholds and reﬂects the\noverall classiﬁcation performance of themodel at all thresholds.\nThe AUC value ranges between 0.5 and 1, where one indicates that\nthe model classi ﬁes perfectly at all possible classi ﬁcation\nthresholds, and 0.5 indicates that the model’s classiﬁcation\nperformance is indistinguishable from a random guess. Since\nprotein functional classes may be unbalanced, the AUC can\nprovide a balanced assessment of the model’s performance\nacross classes, even if some classes have fewer or more samples.\n4. MCC (Matthews correlation coefﬁcient): MCC is a metric for\nevaluating the performance of classiﬁers to effectively handle\nclass imbalance and multi-labeled data. MCC takes into\naccount true positives, false positives, true negatives, and\nfalse negatives across all labels. The value of MCC ranges\nbetween −1 and 1, where one indicates a perfect positive\ncorrelation, −\n1 indicates a perfect negative correlation, and\n0 indicates no correlation. As a comprehensive metric, it is able\nto assess both the precision and recall of the model, ensuring a\nbalanced consideration of the prediction results for both\npositive and negative samples, thus providing a more\ncomprehensive performance assessment.\nTABLE 2 Number of proteins and number of GO terms on the three sub-\nontologies of the dataset.\nDataset Ontology Train Valid Test Terms\nCAFA3 MF 28,679 3,228 1,035 677\nBP 42,250 4,748 2,185 3,992\nCC 39,893 4,510 1,117 551\nCAFA4 MF 25,773 7,318 3,739 725\nBP 36,423 10,445 5,236 4,507\nCC 35,972 10,284 5,129 628\nHuman2024 MF 6,106 2,608 676 540\nBP 6,707 792 480 2,577\nCC 8,499 1,174 1,330 398\nFrontiers inBioengineering and Biotechnology frontiersin.org08\nChen et al. 10.3389/fbioe.2025.1506508\n6 Experiments\nWe used PyTorch version 2.0 deep learning framework and\ntrained the models on an NVIDIA A40 graphics card. We\ndownloaded the following pre-trained models from GitHub and\nHuggingface: the ESM 1bhttps://huggingface.co/facebook/esm1b_\nt33_650M_UR50S, ESM2 650M, and ESM2 3Bhttps://huggingface.\nco/facebook/esm2_t33_650M_UR50D, ProtT5https://huggingface.\nco/Rostlab/prot_t5_xl_uniref50, ProstT5 https://huggingface.co/\nRostlab/ProstT5, ProtBERT https://huggingface.co/Rostlab/prot_\nbert, Seqvec https://github.com/mheinzinger/SeqVec?tab=readme-\nov-ﬁle, Ankh Base https://huggingface.co/ElnaggarLab/ankh-base,\nand Ankh Large https://huggingface.co/ElnaggarLab/ankh-large.\nDuring model training, we set the input dimensions of the MLP\naccording toTable 1, and the output dimensions correspond to the\nnumber of GO terms in the sub-ontology. We used a binary cross-\nentropy loss function and Adam optimizer for model training, with\nthe learning rate set to 0.0001 and a dropout ratio of 0.2 in the\nmodel. In addition, we put the batch size to 16 and the number of\ntraining rounds epoch to 100. On the validation set, we selected the\nmodel with the highest Fmax value as theﬁnal model.\nIn terms of model design, we adopt the architecture shown in\nFigure 2, where the strategyﬁrst utilizes the encoder part of each of\nthe eight pre-trained protein language models that have been\ndownloaded to extract features from the protein dataset. These\nfeatures are constructed into a feature matrix, which is then fed\ninto a multilayer perceptron (MLP) for processing. Speciﬁcally, the\nfeature matrix is nonlinearly transformed and features are extracted\nthrough a number of fully connected layers, which include activation\nfunctions and dropout layers between them to enhance the\nexpressiveness of the model and prevent overﬁtting. Finally, the\nMLP-processed feature vectors are input to a linear layer that maps\nthe high-dimensional features to theﬁnal classiﬁcation result space,\noutputting classiﬁcation results for protein function prediction.\nThrough the above process, we are able to effectively utilize the\nadvantages of deep learning models to extract deep features from\nprotein sequences and improve the accuracy and robustness of\nprotein function prediction through a simple MLP network\nstructure and training strategy. This approach not only improves\nthe generalization ability of the model but also ensuresﬂexibility and\nconsistency when dealing with protein sequences of\ndifferent lengths.\nIn the experimental part, we used nine models, ESM 1b, ESM2\n650M, ESM2 3B, ProtT5, ProstT5, ProtBERT, Seqvec, Ankh Base,\nand Ankh Large, to conduct comparative experiments with four\nmethods on three datasets, Human2024, CAFA3, and CAFA4: the\nhomology-based dual sequence comparison method Diamond, the\nNaive method, Deep_CNN_LSTM_GO (Elhaj-Abdou et al., 2021),\nand DeepGOCNN. Tables 3–5 show the Fmax, AUPR, AUC, and\nMCC metrics of these protein language model methods on the test\nset, and Figure 4 illustrates the Fmax values for the comparison\nexperiments of ESM-1b and ProtT5 with the same four methods.\nThe Naive method, as a statistically based method, annotates\nproteins based on the frequency of occurrence of GO terms in the\ndataset. In this method, all samples in the test set are uniformly assigned\nwith the same annotation. Diamond, as a commonly used sequence\ncomparison tool, assigns the functions of similar proteins to the target\nproteins by comparing the predicted protein sequences with the\ntraining set sequences. DeepGOCNN, on the other hand, employs\nconvolution kernels of different sizes in order to extract multiscale\nsequence features, and predicts the GO terms through the fully\nconnected layer. The Deep_CNN_LSTM_GO method, on the other\nhand, skillfully combines the advantages of CNN and Long Short-Term\nMemory Networks (LSTM) to generatemore reliable prediction results.\nFor a comprehensive comparison with the protein language model, we\ndownloaded the source code of the above four methods and\nimplemented and evaluated them on three different datasets.\nThe results show that the ESM series of models achieved\nexcellent performance on all three sub-ontologies of the CAFA3,\nCAFA4, and Human2024 datasets, especially on the Fmax metric.\nSpeciﬁcally, ESM 1b achieved Fmax values of 0.456, 0.626, and\n0.736 on the biological process (BP), molecular function (MF), and\ncellular component (CC) sub-ontologies of the CAFA4 dataset,\nrespectively, with the best results on all three sub-ontologies,\nwhich demonstrated that the ESM 1b signiﬁcantly outperforms\nother models in terms of the overall prediction accuracy. On the\nCAFA3 dataset, ESM 1b achieved the best Fmax values of 0.557 and\n0.638 on the BP and MF sub-ontologies, respectively. However, on\nthe CC sub-ontology, ESM2 3B surpassed ESM 1b with an Fmax\nvalue of 0.696 as the optimal model on this sub-ontology. In the\nHuman2024 dataset, ESM2 650M achieves Fmax values of 0.670 and\n0.671 on the MF and CC sub-ontologies, respectively, which are both\noptimal. On the BP sub-ontology, ESM 1b achieves a Fmax value of\n0.395, which is the best result.\nFIGURE 3\nDistribution of lengths of sequences from the three datasets.\nFrontiers inBioengineering and Biotechnology frontiersin.org09\nChen et al. 10.3389/fbioe.2025.1506508\nTABLE 3 Experimental results on the CAFA3 dataset.\nModel Fmax AUPR AUC MCC\nBP MF CC BP MF CC BP MF CC BP MF CC\nESM 1b 0.557 0.638 0.691 0.454 0.628 0.671 0.957 0.968 0.967 0.482 0.593 0.624\nESM2 650M 0.542 0.619 0.693 0.448 0.610 0.673 0.953 0.967 0.967 0.479 0.582 0.629\nESM2 3B 0.549 0.622 0.696 0.451 0.616 0.678 0.955 0.969 0.968 0.478 0.575 0.628\nPortT5 0.536 0.575 0.674 0.431 0.550 0.648 0.945 0.956 0.962 0.468 0.528 0.605\nPortBert 0.435 0.482 0.639 0.337 0.427 0.606 0.927 0.918 0.949 0.368 0.440 0.570\nProstT5 0.521 0.557 0.671 0.404 0.514 0.643 0.940 0.947 0.954 0.442 0.504 0.601\nSeqvec 0.520 0.513 0.662 0.414 0.483 0.636 0.939 0.938 0.955 0.449 0.480 0.592\nAnkh Base 0.480 0.519 0.672 0.378 0.494 0.652 0.930 0.941 0.955 0.424 0.488 0.610\nAnkh Large 0.441 0.504 0.667 0.362 0.471 0.647 0.927 0.935 0.954 0.400 0.476 0.604\nTABLE 4 Experimental results on the CAFA4 dataset.\nModel Fmax AUPR AUC MCC\nBP MF CC BP MF CC BP MF CC BP MF CC\nESM 1b 0.456 0.626 0.736 0.404 0.608 0.743 0.945 0.970 0.980 0.418 0.583 0.671\nESM2 650M 0.443 0.599 0.73 0.385 0.576 0.732 0.937 0.966 0.978 0.405 0.558 0.664\nESM2 3B 0.452 0.62 0.734 0.397 0.603 0.741 0.940 0.968 0.979 0.415 0.579 0.670\nPortT5 0.422 0.539 0.706 0.361 0.502 0.698 0.928 0.955 0.971 0.385 0.500 0.638\nPortBert 0.376 0.416 0.657 0.295 0.327 0.624 0.902 0.917 0.952 0.337 0.371 0.585\nProstT5 0.414 0.52 0.689 0.343 0.47 0.676 0.92 0.947 0.966 0.374 0.478 0.621\nSeqvec 0.402 0.487 0.689 0.331 0.432 0.669 0.919 0.941 0.964 0.364 0.447 0.616\nAnkh Base 0.390 0.464 0.689 0.322 0.412 0.67 0.906 0.932 0.961 0.358 0.434 0.620\nAnkh Large 0.386 0.45 0.689 0.318 0.4 0.669 0.905 0.928 0.96 0.356 0.421 0.621\nTABLE 5 Experimental results on the Human2024 dataset.\nModel Fmax AUPR AUC MCC\nBP MF CC BP MF CC BP MF CC BP MF CC\nESM 1b 0.395 0.640 0.664 0.329 0.522 0.658 0.911 0.966 0.967 0.371 0.538 0.607\nESM2 650M 0.392 0.670 0.671 0.332 0.538 0.668 0.914 0.970 0.969 0.370 0.566 0.616\nESM3 3B 0.393 0.626 0.663 0.327 0.501 0.664 0.908 0.969 0.969 0.373 0.522 0.610\nPortT5 0.373 0.608 0.632 0.315 0.468 0.621 0.906 0.954 0.961 0.356 0.498 0.582\nPortBert 0.337 0.526 0.585 0.256 0.347 0.555 0.886 0.916 0.949 0.314 0.405 0.527\nProstT5 0.356 0.589 0.617 0.291 0.420 0.594 0.898 0.940 0.956 0.339 0.464 0.560\nSeqvec 0.358 0.567 0.614 0.294 0.395 0.600 0.897 0.927 0.958 0.337 0.441 0.557\nAnkh Base 0.360 0.579 0.632 0.309 0.426 0.626 0.898 0.944 0.961 0.353 0.475 0.581\nAnkh Large 0.346 0.577 0.626 0.302 0.415 0.625 0.890 0.942 0.961 0.345 0.462 0.573\nFrontiers inBioengineering and Biotechnology frontiersin.org10\nChen et al. 10.3389/fbioe.2025.1506508\nThe ESM family of models demonstrates excellent performance\non different datasets and sub-ontologies, especially in complex\nprotein function prediction tasks. Its deep learning architecture\nand pre-training strategy can signi ﬁcantly improve prediction\naccuracy and coverage. The analysis shows that ESM 1b and\nESM2 3B perform best on different datasets and sub-ontologies,\ndue to their dynamic masking approach and optimization in model\nsize, data volume, and training strategy. These results suggest that\ndeep learning models have great potential in protein function\nprediction, especially when combined with large-scale data and\npre-training techniques.\nAs can be seen inFigure 4, methods using protein language\nmodeling are signiﬁcantly better than the homology-based dual\nsequence comparison methods Diamond and Naive methods.\nfrequently used methods such as ESM 1b and PortT5 outperform\nthe convolution-based deep learning method DeepGOCNN and\nDeep_CNN_LSTM_GO in all the metrics. these results show that in\nthe cross-species protein datasets CAFA3, CAFA4, and the single-\nspecies human dataset Human2024, the large language model is able\nto efﬁciently recognize GO terms for proteins, demonstrating the\neffectiveness of protein language models for protein function\nprediction tasks.\nCompared to Deep_CNN_LSTM_GO, the ESM 1b model\nachieves a Fmax improvement of more than 10% on all sub-\nontologies of both datasets. This shows that deep semantic\ninformation of sequences can be extracted using large language\nmodels. Relative to DeepGOCNN, the protein language models\nshow less improvement on the BP and CC sub-ontologies and\nmore improvement on the MF sub-ontology. The MF sub-\nontology is usually concerned with speciﬁc molecular functions\nof proteins, which are more directly related to the protein ’s\nsequence, and thus the models may be more likely to capture\nfeatures related to MF. If a model architecture is better at\ncapturing localized features, it may perform better on the MF\nsub-ontology. the BP and CC sub-ontologies are more concerned\nwith the biological processes in which the protein is involved and the\nFIGURE 4\nFmax values for ESM-1b, ProtT5, and the four comparison methods on the three datasets.\nFrontiers inBioengineering and Biotechnology frontiersin.org11\nChen et al. 10.3389/fbioe.2025.1506508\ncellular components in which it resides, and these functions may be\nmore relevant to the contextual environment of the protein, its\ninteractions, and its regulatory network. These factors are difﬁcult to\ninfer directly from sequence data and require models with more\ncomplex structures and longer memory to capture the location and\nrole of proteins in biological networks.\nFigure 5 illustrates the precision-recall (PR) plots of the protein\nlanguage model on the CAFA3, CAFA4, and Human2024 datasets for\nevaluating the trade-off between precision and recall of the classiﬁcation\nmodel at different thresholds. On all sub-ontologies of the above three\ndatasets, the ESM family of models performs the best, while the\nPortBERT model has relatively lowr e s u l t s .O nt h eB Pa n dC Cs u b -\nontologies, the performance of different models is similar, but on the\nMF sub-ontology, the performance gap between models is more\nobvious. This suggests that the ESM family of models is able to\nbetter balance precision and recall when dealing with these datasets\na n dt h u sp e r f o r m sb e t t e ri nt h ef u n c t i o np r e d i c t i o nt a s k .\nThese PR curve results further conﬁrm the superiority of the\nESM family of models in the protein function prediction task,\nespecially in achieving a better balance of precision and recall\nwhen dealing with different sub-ontologies, which improves the\noverall performance of the models.\n7 Case study\nWe will illustrate the differences in the performance of the\nvarious methods using the example of the protein Q9BZE2, a tRNA\npseudo-uracil (38/39) synthetase that forms a pseudo-uracil at\nposition 39 of the anticodon stem and loop of the transfer RNA.\nFigure 6shows a DAG plot of the BPO terms for this protein, where\nthe arrows represent is-a relationships, the direction they are\npointing in represents the parent class, and the root term is BP.\nThere are also methods used to correctly predict the corresponding\nFIGURE 5\nThe precision-recall plots of the protein language model on the CAFA3, CAFA4 and Human2024 datasets.\nFrontiers inBioengineering and Biotechnology frontiersin.org12\nChen et al. 10.3389/fbioe.2025.1506508\nGO terms, and Table 6 shows the GO terms correctly predicted\n(i.e., true positives) and the incorrectly predicted terms (i.e., false\npositives), as well as the F1 scores.\nAccording to the data in Table 6 , there are a total of\n26 experimentally validated BPO terms for the Q9BZE2 protein.\nOf all the models evaluated, the ESM2 650M model predicted the\nmost GO terms, with 19 of the 20 predicted terms proving to be\ncorrect and only seven terms failing to be predicted, with a Fmax\nscore of 0.826. The ESM 1b model correctly predicted 18 GO terms,\nwith an Fmax score of 0.816. Whereas the PortT5 and ESM2 3B\nmodels both correctly predicted 16 GO terms with Fmax scores of\n0.762. It is noteworthy that only these four models successfully\npredicted the deep GO terms located in the lower half of the GO\nmap, which highlights the superiority of the ESM series and\nPortT5 models in terms of predictive power. The PortT5 model\ncorrectly predicted 15 GO terms with a Fmax score of 0.732. These\nprotein language models signiﬁcantly outperformed the other four\ncompared methods, conﬁrming their ability to effectively utilize\nlarge-scale unannotated protein sequence data to deeply extract\ncontextual information between amino acids and capture the deep\nsemantic information of protein sequences.\nIt is clear fromFigure 6that the ESM family of models performs\nbetter in predicting the depth of GO terms compared to the other\nmodels. These models skillfully compute a scalar dot product of\nattention between the query matrix, key matrix, and value matrix in\neach attention header. Speciﬁcally, the modelﬁrst creates a weight\nmatrix that reveals the degree of similarity between pairs of amino\nacid sequences through a dot-product operation of the query and\nkey matrices. Subsequently, the model normalizes the weight matrix\nusing the scale parameter and the SoftMax function, a step that\nensures the effectiveness and reasonableness of the allocation of\nattention. By multiplying the normalized weight matrix with the\nvalue matrix, the model constructs the attention matrix. As a result,\nthe ESM2 650M model was able to accurately predict the deepest GO\nterm, RNA processing. RNA processing is a key step in biomolecular\nprocesses that involves the conversion of preliminarily transcribed\nRNA into mature RNA molecules, a process that plays a decisive role\nin the precise regulation of gene expression.\nThe Diamond method based on sequence similarity\nencountered challenges in predicting the function of the\nQ9BZE2 protein due to the failure toﬁnd sequences homologous\nto the Q9BZE2 protein in the training set. This situation highlights\nthe limitations of the Diamond method in dealing with uncommon\nor novel protein sequences. In contrast, protein language models\nsuch as ESM2 650M are able to dig deeper into the deep semantic\ninformation of protein sequences for more accurate functional\nprediction by virtue of their large-scale dataset utilization and\nadvanced model architecture.\nThus, although sequence similarity-based techniques are\neffective in most cases, deep learning techniques, especially\nprotein language models, demonstrate superior performance and\nhigher prediction accuracy when dealing with complex or speciﬁc\nprotein sequences. This case further demonstrates the advantages of\nprotein language models in performing the task of protein function\nFIGURE 6\nDAG diagram of correct predicted BPO terms of Q9BZE2 using different methods.\nFrontiers inBioengineering and Biotechnology frontiersin.org13\nChen et al. 10.3389/fbioe.2025.1506508\nprediction, especially when confronted with challenging protein\nsequences. These models are able to distill more semantic\ninformation from the data, signiﬁcantly improving the accuracy\nand robustness of the predictions.\n8 Conclusion\nThe emergence of protein language models has revolutionized\nthe ﬁeld of protein function prediction. Starting from the use of the\nESM 1b model in NETGO 3.0 to the wide adoption of various\nprotein language models in many emerging protein function\nprediction methods today, the deep semantic information\nprovided by these models has become an indispensable part of\nprotein function prediction. Their tight integration has signiﬁcantly\nimproved the effectiveness of the prediction task.\nIn this paper, we ﬁrst review the development of protein\nfunction prediction, from the initial biochemical experiments to\nthe homology-based statistical sequence comparison methods to the\napplication of machine learning and deep learning techniques. We\nsort out the key historical nodes in thisﬁeld and introduce the\nrepresentative methods and the problems they face in each period.\nNext, this paper provides a comprehensive overview of nine current\nprotein language models that can be used for the task of gene\nontology prediction, including ESM 1b, ESM2 650M, ESM2 3B,\nProtT5, ProstT5, ProtBERT, Seqvec, Ankh Base, and Ankh Large.\nWe elaborate on their architectures, functions, training strategies,\nand datasets, and provide an in-depth comparative analysis of them.\nWe have experimentally evaluated the performance of these\nprotein language models exhaustively and compared them with\nother comparative methods such as traditional sequence alignment,\nmachine learning, and deep learning. The experimental results\nclearly show that most of theﬁne-tuned protein language models\nsigniﬁcantly outperform other methods in feature encoding, which\nfully demonstrates the superior ability of protein language models in\ncharacterizing protein molecules. Meanwhile, the experiments also\nconﬁrmed that the deep semantic information in sequences can be\neffectively extracted by using large-scale language models. The\noverall accuracy of the protein function prediction task can be\nsigniﬁcantly improved by employing protein language models.\nWith the continuous progress and optimization of protein\nlanguage models, they gradually replace the traditional coding\nmethods. This change has not only signi ﬁcantly improved the\naccuracy of protein function prediction, but also brought us new\nresearch perspectives and technical tools. However, despite the\nremarkable achievements, we still face many challenges. Among\nthem, the size of the pre-training dataset has become a key factor\nconstraining the development of large-scale protein language\nmodeling ( Unsal et al., 2022 ). Unlike the large-scale\naccumulation of human natural language, developing protein\nlanguage models relies on advancing DNA and protein\nsequencing technologies. With the continuous innovation of\nTABLE 6 Predicted GO terms for Q9BZE2 in BPO by different methods. Terms that do not appear in Labels are added*.\nMethod Result F1\nNaive GO:0009987, GO:0065007*, GO:0008152, GO:0050789*, GO:0071704, GO:0050794*, GO:0044238, GO:0044237, GO:0006807, GO:\n0043170\n0.389\nDiamond 0\nDeepGOCNN GO:0044238, GO:0071704, GO:0006807, GO:0008152, GO:0009987 0.323\nDeep_cnn_lstm_GO GO:0065007*, GO:0050789*, GO:0071704, GO:0008152, GO:0009987 0.193\nESM 1b GO:0044238, GO:0071704, GO:0006807, GO:0043170, GO:0008152, GO:0009987, GO:0044237, GO:0006725, GO:1901360, GO:\n0009059, GO:0009058, GO:1901576, GO:0046483, GO:0010467, GO:0034641, GO:0044249, GO:0006139, GO:0090304\n0.818\nESM2 650M GO:0044238, GO:0071704, GO:0006807, GO:0043170, GO:1901564*, GO:0008152, GO:0009987, GO:0044237, GO:0006725, GO:\n1901360, GO:0009059, GO:0009058, GO:1901576, GO:0046483, GO:0010467, GO:0034641, GO:0044249, GO:0006139, GO:0090304,\nGO:0006396\n0.826\nESM2 3B GO:0044238, GO:0071704, GO:0006807, GO:0043170, GO:0008152, GO:0009987, GO:0044237, GO:0006725, GO:1901360, GO:\n0009059, GO:0009058, GO:1901576, GO:0046483, GO:0034641, GO:0044249, GO:0006139\n0.762\nPortT5 GO:0044238, GO:0071704, GO:0006807, GO:0043170, GO:0008152, GO:0009987, GO:0044237, GO:0006725, GO:1901360, GO:\n0009058, GO:1901576, GO:0046483, GO:0034641, GO:0044249, GO:0006139, GO:0090304\n0.762\nPortBert GO:0071704, GO:0008152, GO:0009987 0.207\nProstT5 GO:0044238, GO:0071704, GO:0006807, GO:0043170, GO:0008152, GO:0009987, GO:0044237, GO:0006725, GO:1901360, GO:\n0009058, GO:1901576, GO:0046483, GO:0034641, GO:0044249, GO:0006139\n0.732\nSeqvec GO:0044238, GO:0071704, GO:0006807, GO:0043170, GO:0008152, GO:0009987, GO:0044237 0.424\nAnkh Base GO:0044238, GO:0071704, GO:0006807, GO:0008152, GO:0009987, GO:0044237 0.375\nAnkh Large GO:0065007*, GO:0050789*, GO:0009987 0.069\nLabels GO:0016070, GO:0006399, GO:0008033, GO:0009059, GO:0034660, GO:0010467, GO:0009058, GO:0009987, GO:0034641, GO:\n0044238, GO:0044237, GO:0006725, GO:0071704, GO:0009451, GO:0046483, GO:0034470, GO:0006807, GO:0006139, GO:0043412,\nGO:1901576, GO:0043170, GO:0044249, GO:1901360, GO:0090304, GO:0008152, GO:0006396\nFrontiers inBioengineering and Biotechnology frontiersin.org14\nChen et al. 10.3389/fbioe.2025.1506508\nthese technologies, more and more gene and protein sequences have\nbeen identiﬁed, providing the possibility of generating large-scale\nand high-quality datasets. In addition, the length and complexity of\nprotein sequences far exceed that of natural language texts, but are\nless diverse, which creates additional difﬁculty in learning and\ninterpreting protein representations for models.\nLooking ahead, the research focus will gradually shift to\ndeveloping novel protein representation models capable of\nintegrating multiple external knowledge sources. The rich\nconnotations of proteins are clos ely linked to bioinformatics\ndata such as protein-protein int eractions, post-translational\nmodiﬁcations, gene ontology, and gene and protein\nexpression, which provide a vast scope for potential synergies\nbetween PLM and these external knowledge sources for\nenhancement. By supervised integration of these rich and\nstructured resources, the capabilities of PLM will be\nsigniﬁcantly enhanced ( Öztürk et al., 2019 ; Doğan et al.,\n2021). In addition, the introduction of additional resources\nsuch as physical world simulations provided by the ﬁeld of\nmolecular dynamics (MD) will greatly deepen our\nunderstanding of molecular behavior and interactions. The\norganic integration of PLM with MD not only complements\nPLM’s strengths in data processing but also strengthens its\nability to analyze complex scienti ﬁc phenomena, allowing for\nﬁner and more accurate interpretations (Zhang et al., 2024). In\nterms of coding strategies, the traditional linear positional coding\ncan be replaced by introducing biologically relevant positional\ninformation, such as the distance matrix and contact map\nbetween sequences, to better model long-distance dependencies.\nAuthor contributions\nJ-YC: Conceptualization, Resources, Writing –review and\nediting, Data curation. J-FW: Methodology, Validation,\nWriting–original draft. YH: Investigation, Methodology,\nWriting–review and editing. X-HL: Data curation, Formal\nAnalysis, Investigation, Writing –review and editing. Y-RQ:\nConceptualization, Funding acquisition, Resources,\nWriting–review and editing. C-LS: Methodology, Writing–review\nand editing.\nFunding\nThe author(s) declare thatﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. The authors\nare grateful to the anonymous referees for their insightful\nsuggestions and comments. This research was supported by\nNatural Science Foundation of Xinjiang Uygur Autonomous\nRegion of China (2022D01C692), Basic Research Foundation of\nUniversities in the Xinjiang Uygur Autonomous Region of China\n(XJEDU2023P012), The Key Research and Development Project in\nXinjiang Uygul Autonomous Region (2022B01006, 2023B01029),\nTianshan Innovation Team Program of Xinjiang Uygur\nAutonomous Region of China (2023D14012).\nAcknowledgments\nHeartfelt thanks to the Software College of Xinjiang University\nfor providing computational resources.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors and\ndo not necessarily represent those of their afﬁliated organizations, or\nthose of the publisher, the editors and the reviewers. Any product that\nmay be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAbramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., et al. (2024).\nAccurate structure prediction of biomolecular interactions with alphafold 3.Nature 630,\n493–500. doi:10.1038/s41586-024-07487-w\nAggarwal, D., and Hasija, Y. (2022). A review of deep learning techniques for\nprotein function prediction. arXiv Prepr. arXiv:2211.09705 . doi:10.48550/arXiv.\n2211.09705\nAleksander, S. A., Balhoff, J., Carbon, S., Cherry, J. M., Drabkin, H. J., Ebert, D., et al.\n(2023). The gene ontology knowledgebase in 2023.Genetics 224, iyad031. doi:10.1093/\ngenetics/iyad031\nAltschul, S. F., Gish, W., Miller, W., Myers, E. W., and Lipman, D. J. (1990). Basic local\nalignment search tool.J. Mol. Biol.215, 403–410. doi:10.1006/jmbi.1990.9999\nAltschul, S. F., Madden, T. L., Schäffer, A. A., Zhang, J., Zhang, Z., Miller, W., et al.\n(1997). Gapped blast and psi-blast: a new generation of protein database search\nprograms. Nucleic acids Res.25, 3389–3402. doi:10.1093/nar/25.17.3389\nAshburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler, H., Cherry, J. M., et al.\n(2000). Gene ontology: tool for the uniﬁcation of biology.Nat. Genet.25, 25–29. doi:10.\n1038/75556\nAvery, C., Patterson, J., Grear, T., Frater, T., and Jacobs, D. J. (2022). Protein function\nanalysis through machine learning.Biomolecules 12, 1246. doi:10.3390/biom12091246\nBaek, M., DiMaio, F., Anishchenko, I., Dauparas, J., Ovchinnikov, S., Lee, G. R., et al.\n(2021). Accurate prediction of protein structures and interactions using a three-track\nneural network. Science 373, 871–876. doi:10.1126/science.abj8754\nBarabási, A.-L., Gulbahce, N., and Loscalzo, J. (2011). Network medicine: a network-\nbased approach to human disease.Nat. Rev. Genet.12, 56–68. doi:10.1038/nrg2918\nBepler, T., and Berger, B. (2021). Learning the protein language: evolution, structure,\nand function. Cell Syst. 12, 654–669.e3. doi:10.1016/j.cels.2021.05.017\nBerman, H., Henrick, K., and Nakamura, H. (2003). Announcing the worldwide\nprotein data bank.Nat. Struct. and Mol. Biol.10, 980. doi:10.1038/nsb1203-980\nBerman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T. N., Weissig, H., et al.\n(2000). The protein data bank.Nucleic acids Res.28, 235–242. doi:10.1093/nar/28.1.235\nBernardes, J., and Pedreira, C. (2013). A review of protein function prediction under\nmachine learning perspective. Recent Pat. Biotechnol. 7, 122 –141. doi:10.2174/\n18722083113079990006\nFrontiers inBioengineering and Biotechnology frontiersin.org15\nChen et al. 10.3389/fbioe.2025.1506508\nBonetta, R., and Valentino, G. (2020). Machine learning techniques for protein\nfunction prediction.Proteins Struct. Funct. Bioinforma.88, 397–413. doi:10.1002/prot.\n25832\nBoutet, E., Lieberherr, D., Tognolli, M., Schneider, M., Bansal, P., Bridge, A. J., et al.\n(2016). Uniprotkb/swiss-prot, the manually annotated section of the uniprot\nknowledgebase: how to use the entry view.Plant Bioinforma. methods Protoc.1374,\n23–54. doi:10.1007/978-1-4939-3167-5_2\nBuchﬁnk, B., Xie, C., and Huson, D. H. (2015). Fast and sensitive protein alignment\nusing diamond. Nat. methods 12, 59–60. doi:10.1038/nmeth.3176\nCao, Y., and Shen, Y. (2021). Tale: transformer-based protein function annotation\nwith joint sequence–label embedding. Bioinformatics 37, 2825–2833. doi:10.1093/\nbioinformatics/btab198\nChagneau, A., Massaoudi, Y., Derbali, I., and Yahiaoui, L. (2024). Quantum algorithm\nfor bioinformatics to compute the similarity between proteins.IET Quantum Commun.\n5, 417–442. doi:10.1049/qtc2.12098\nChen, B., Cheng, X., Li, P., Geng, Y.-a., Gong, J., Li, S., et al. (2024). xtrimopglm:\nuniﬁed 100b-scale pre-trained transformer for deciphering the language of protein.\narXiv Prepr. arXiv:2401.06199. doi:10.48550/arXiv.2401.06199\nColin, P.-Y., Kintses, B., Gielen, F., Miton, C. M., Fischer, G., Mohamed, M. F., et al.\n(2015). Ultrahigh-throughput discovery of promiscuous enzymes by picodroplet\nfunctional metagenomics. Nat. Commun. 6, 10008. doi:10.1038/ncomms10008\nCruz, L. M., Trefﬂich, S., Weiss, V. A., and Castro, M. A. A. (2017). Protein function\nprediction. Funct. Genomics Methods Protoc. 1654, 55–75. doi:10.1007/978-1-4939-\n7231-9_5\nCui, H., Wang, Q., Lei, Z., Feng, M., Zhao, Z., Wang, Y., et al. (2019). Dtl promotes\ncancer progression by pdcd4 ubiquitin-dependent degradation.J. Exp. and Clin. Cancer\nRes. 38, 350–413. doi:10.1186/s13046-019-1358-x\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: pre-training of deep\nbidirectional transformers for language understanding.arXiv Prepr. arXiv:1810.04805.\ndoi:10.18653/v1/N19-1423\nDevos, D., and Valencia, A. (2000). Practical limits of function prediction.Proteins\nStruct. Funct. Bioinforma.41, 98–107. doi:10.1002/1097-0134(20001001)41:1<98::aid-\nprot120>3.0.co;2-s\nDevos, D., and Valencia, A. (2001). Intrinsic errors in genome annotation.TRENDS\nGenet. 17, 429–431. doi:10.1016/s0168-9525(01)02348-4\nDoğan, T., Atas, H., Joshi, V., Atakan, A., Rifaioglu, A. S., Nalbat, E., et al. (2021).\nCrossbar: comprehensive resource of biomedical relations with knowledge graph\nrepresentations. Nucleic acids Res.49, e96. doi:10.1093/nar/gkab543\nElhaj-Abdou, M. E., El-Dib, H., El-Helw, A., and El-Habrouk, M. (2021). Deep_cnn_\nlstm_go: protein function prediction from amino-acid sequences.Comput. Biol. Chem.\n95, 107584. doi:10.1016/j.compbiolchem.2021.107584\nElnaggar, A., Essam, H., Salah-Eldin, W., Moustafa, W., Elkerdawy, M., Rochereau,\nC., et al. (2023). Ankh: optimized protein language model unlocks general-purpose\nmodelling. arXiv preprint arXiv:2301.06568\nElnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., et al. (2021).\nProttrans: toward understanding the language of life through self-supervised learning.\nIEEE Trans. pattern analysis Mach. Intell. 44, 7112–7127. doi:10.1109/tpami.2021.\n3095381\nFerruz, N., Schmidt, S., and Höcker, B. (2022). Protgpt2 is a deep unsupervised\nlanguage model for protein design.Nat. Commun. 13, 4348. doi:10.1038/s41467-022-\n32007-7\nGligorijević, V., Barot, M., and Bonneau, R. (2018). deepnf: deep network fusion for\nprotein function prediction.Bioinformatics 34, 3873–3881. doi:10.1093/bioinformatics/\nbty440\nGligorijević, V., Renfrew, P. D., Kosciolek, T., Leman, J. K., Berenberg, D., Vatanen,\nT., et al. (2021). Structure-based protein function prediction using graph convolutional\nnetworks. Nat. Commun. 12, 3168. doi:10.1038/s41467-021-23303-9\nH e ,Y . ,Z h o u ,X . ,C h a n g ,C . ,C h e n ,G . ,L i u ,W . ,L i ,G . ,e ta l .( 2 0 2 4 ) .P r o t e i nl a n g u a g em o d e l s -\nassisted optimization of a uracil-n-glycosylase variant enables programmable t-to-g and t-to-c\nbase editing.Mol. Cell84, 1257–1270.e6. doi:10.1016/j.molcel.2024.01.021\nHeinzinger, M., Elnaggar, A., Wang, Y., Dallago, C., Nechaev, D., Matthes, F., et al.\n(2019). Modeling aspects of the language of life through transfer-learning protein\nsequences. BMC Bioinforma. 20, 723–817. doi:10.1186/s12859-019-3220-8\nHeinzinger, M., Weissenow, K., Sanchez, J. G., Henkel, A., Steinegger, M., and Rost, B.\n(2023). Prostt5: bilingual language model for protein sequence and structure.bioRxiv.\ndoi:10.1093/nargab/lqae150\nHu, M., Alkhairy, S., Lee, I., Pillich, R. T., Fong, D., Smith, K., et al. (2024). Evaluation\nof large language models for discovery of gene set function.Nat. Methods,1 –10. doi:10.\n1038/s41592-024-02525-x\nHuang, Z., Xu, W., and Yu, K. (2015). Bidirectional lstm-crf models for sequence\ntagging. arXiv Prepr. arXiv:1508.01991. doi:10.48550/arXiv.1508.01991\nJensen, L. J., Gupta, R., Blom, N., Devos, D., Tamames, J., Kesmir, C., et al. (2002).\nPrediction of human protein function from post-translational modi ﬁcations and\nlocalization features.J. Mol. Biol.319, 1257–1265. doi:10.1016/s0022-2836(02)00379-0\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., et al.\n(2021). Highly accurate protein structure prediction with alphafold. nature 596,\n583–589. doi:10.1038/s41586-021-03819-2\nKihara, D., and Kihara (2017).Protein function prediction. Springer.\nKissa, M., Tsatsaronis, G., and Schroeder, M. (2015). Prediction of drug gene\nassociations via ontological proﬁle similarity with application to drug repositioning.\nMethods 74, 71–82. doi:10.1016/j.ymeth.2014.11.017\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with\ndeep convolutional neural networks. Adv. neural Inf. Process. Syst.25. doi:10.1145/\n3065386\nKulmanov, M., Guzmán-Vega, F. J., Duek Roggli, P., Lane, L., Arold, S. T., and\nHoehndorf, R. (2024). Protein function prediction as approximate semantic entailment.\nNat. Mach. Intell.6, 220–228. doi:10.1038/s42256-024-00795-w\nKulmanov, M., and Hoehndorf, R. (2020). Deepgoplus: improved protein function\nprediction from sequence. Bioinformatics 36, 422–429. doi:10.1093/bioinformatics/\nbtz595\nKulmanov, M., Khan, M. A., and Hoehndorf, R. (2018). Deepgo: predicting protein\nfunctions from sequence and interactions using a deep ontology-aware classi ﬁer.\nBioinformatics 34, 660–668. doi:10.1093/bioinformatics/btx624\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., et al.\n(1989). Backpropagation applied to handwritten zip code recognition.Neural Comput.\n1, 541–551. doi:10.1162/neco.1989.1.4.541\nLi, J., Wang, L., Zhang, X., Liu, B., and Wang, Y. (2020).“Gonet: a deep network to\nannotate proteins via recurrent convolution networks,” in 2020 IEEE international\nconference on bioinformatics and biomedicine (BIBM)(IEEE), 29–34.\nLi, X., Qian, Y., Hu, Y., Chen, J., Yue, H., and Deng, L. (2024). Msf-pfp: a novel\nmultisource feature fusion model for protein function prediction.J. Chem. Inf. Model.\n64, 1502–1511. doi:10.1021/acs.jcim.3c01794\nLi, Z., Jiang, C., and Li, J. (2023). Deepgatgo: a hierarchical pretraining-based graph-\nattention model for automatic protein function prediction.arXiv Prepr. arXiv:2307.\ndoi:10.48550/arXiv.2307.13004\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., et al. (2023). Evolutionary-scale\nprediction of atomic-level protein structure with a language model. Science 379,\n1123–1130. doi:10.1126/science.ade2574\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (2019). Roberta: a robustly\noptimized bert pretraining approach. arXiv Prepr. arXiv:1907.11692. doi:10.48550/\narXiv.1907.11692\nLiu, Y.-W., Hsu, T.-W., Chang, C.-Y., Liao, W.-H., and Chang, J.-M. (2020). Godoc:\nhigh-throughput protein function prediction using novel k-nearest-neighbor and voting\nalgorithms. BMC Bioinforma. 21, 276–316. doi:10.1186/s12859-020-03556-9\nMa, J., Ge, X., and Chang, Z. (2007). Protein function studies: history, current status\nand future trends.Chin. Bull. Life Sci.19, 294.\nMa, W., Zhang, S., Li, Z., Jiang, M., Wang, S., Lu, W., et al. (2022). Enhancing protein\nfunction prediction performance by utilizing alphafold-predicted protein structures.\nJ. Chem. Inf. Model.62, 4008–4017. doi:10.1021/acs.jcim.2c00885\nMadani, A., McCann, B., Naik, N., Keskar, N. S., Anand, N., Eguchi, R. R., et al.\n(2020). Progen: language modeling for protein generation. arXiv Prepr. arXiv:\n2004.03497. doi:10.1101/2020.03.07.982272\nManchester, K. L. (2004). The crystallization of enzymes and virus proteins: laying to\nrest the colloidal concept of living systems. Endeavour 28, 25–29. doi:10.1016/j.\nendeavour.2004.01.010\nMunsamy, G., Lindner, S., Lorenz, P., and Ferruz, N. (2022).“Zymctrl: a conditional\nlanguage model for the controllable generation of artiﬁcial enzymes,” in NeurIPS\nmachine learning in structural biology workshop.\nNeedleman, S. B., and Wunsch, C. D. (1970). A general method applicable to the\nsearch for similarities in the amino acid sequence of two proteins.J. Mol. Biol. 48,\n443–453. doi:10.1016/0022-2836(70)90057-4\nNijkamp, E., Ruffolo, J. A., Weinstein, E. N., Naik, N., and Madani, A. (2023).\nProgen2: exploring the boundaries of protein language models.\nCell Syst.14, 968–978.e3.\ndoi:10.1016/j.cels.2023.10.002\nOfer, D., Brandes, N., and Linial, M. (2021). The language of proteins: nlp, machine\nlearning and protein sequences.Comput. Struct. Biotechnol. J.19, 1750–1758. doi:10.\n1016/j.csbj.2021.03.022\nÖztürk, H., Ozkirimli, E., and Özgür, A. (2019). Widedta: prediction of drug-target\nbinding afﬁnity. arXiv Prepr. arXiv:1902.04166. doi:10.48550/arXiv.1902.04166\nPan, T., Li, C., Bi, Y., Wang, Z., Gasser, R. B., Purcell, A. W., et al. (2023). Pfresgo: an\nattention mechanism-based deep-learning approach for protein annotation by\nintegrating gene ontology inter-relationships. Bioinformatics 39, btad094. doi:10.\n1093/bioinformatics/btad094\nPearson, W. R. (2016). Finding protein and nucleotide similarities with fasta.Curr.\nProtoc. Bioinforma. 53, 3.9.1–3.9.25. doi:10.1002/0471250953.bi0309s53\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019).\nLanguage models are unsupervised multitask learners.OpenAI blog 1, 9.\nFrontiers inBioengineering and Biotechnology frontiersin.org16\nChen et al. 10.3389/fbioe.2025.1506508\nRadivojac, P., Clark, W. T., Oron, T. R., Schnoes, A. M., Wittkop, T., Sokolov, A., et al.\n(2013). A large-scale evaluation of computational protein function prediction.Nat.\nmethods 10, 221–227. doi:10.1038/nmeth.2340\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., et al. (2020).\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer.J. Mach.\nLearn. Res. 21, 1–67. doi:10.5555/3455716.3455856\nRemmert, M., Biegert, A., Hauser, A., and Söding, J. (2012). Hhblits: lightning-fast\niterative protein sequence searching by hmm-hmm alignment. Nat. methods 9,\n173–175. doi:10.1038/nmeth.1818\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., et al. (2021).“Biological\nstructure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences,” Proc. Natl. Acad. Sci. U. S. A.118. e2016239118. doi:10.1073/pnas.\n2016239118\nShehu, A., Barbará, D., and Molloy, K. (2016). A survey of computational methods for\nprotein function prediction.Big data Anal. genomics, 225–298. doi:10.1007/978-3-319-\n41279-5_7\nShuai, R. W., Ruffolo, J. A., and Gray, J. J. (2021). Generative language modeling for\nantibody design. bioRxiv, 2021–2112. doi:10.1101/2021.12.13.472419\nSimoni, R. D., Hill, R. L., and Vaughan, M. (2002). Urease, theﬁrst crystalline enzyme\nand the proof that enzymes are proteins: the work of james b. sumner.J. Biol. Chem.277,\ne1–e2. doi:10.1016/s0021-9258(20)69970-7\nThein, S. L. (2011). Milestones in the history of hemoglobin research (in memory of\nprofessor titus hj huisman). Hemoglobin 35, 450–462. doi:10.3109/03630269.2011.\n613506\nTörönen, P., and Holm, L. (2022). Pannzer— a practical tool for protein function\nprediction. Protein Sci. 31, 118–128. doi:10.1002/pro.4193\nTorres, M., Yang, H., Romero, A. E., and Paccanaro, A. (2021). Protein function\nprediction for newly sequenced organisms.Nat. Mach. Intell. 3, 1050–1060. doi:10.\n1038/s42256-021-00419-7\nThe UniProt Consortium (2023). Uniprot: the universal protein knowledgebase in\n2023. Nucleic acids Res.51, D523–D531. doi:10.1093/nar/gkac1052\nUnsal, S., Atas, H., Albayrak, M., Turhan, K., Acar, A. C., and Doğan, T. (2022).\nLearning functional properties of proteins with language models.Nat. Mach. Intell.4,\n227–245. doi:10.1038/s42256-022-00457-9\nVaradi, M., Bertoni, D., Magana, P., Paramval, U., Pidruchna, I., Radhakrishnan, M.,\net al. (2024). Alphafold protein structure database in 2024: providing structure coverage\nfor over 214 million protein sequences.Nucleic acids Res.52, D368–D375. doi:10.1093/\nnar/gkad1011\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need.Adv. neural Inf. Process. Syst.\n30. doi:10.48550/arXiv.\n1706.03762\nWang, S., You, R., Liu, Y., Xiong, Y., and Zhu, S. (2023a). Netgo 3.0: protein language\nmodel improves large-scale functional annotations. Genomics, Proteomics and\nBioinforma. 21, 349–358. doi:10.1016/j.gpb.2023.04.001\nWang, Z., Deng, Z., Zhang, W., Lou, Q., Choi, K.-S., Wei, Z., et al. (2023b).\nMmsmaplus: a multi-view multi-scale multi-attention embedding model for protein\nfunction prediction. Brieﬁngs Bioinforma. 24, bbad201. doi:10.1093/bib/bbad201\nWeissenow, K., Heinzinger, M., and Rost, B. (2022). Protein language-model\nembeddings for fast, accurate, and alignment-free protein structure prediction.\nStructure 30, 1169–1177.e4. doi:10.1016/j.str.2022.05.001\nXuan, P., Sun, C., Zhang, T., Ye, Y., Shen, T., and Dong, Y. (2019). Gradient boosting\ndecision tree-based method for predicting interactions between target genes and drugs.\nFront. Genet. 10, 459. doi:10.3389/fgene.2019.00459\nY a n g ,X . ,L i u ,G . ,F e n g ,G . ,B u ,D . ,W a n g ,P . ,J i a n g ,J . ,e ta l .( 2 0 2 4 ) .G e n e c o m p a s s :\ndeciphering universal gene regulatory mechanisms with a knowledge-informed\ncross-species foundation model. Cell Res. 34, 830–845. doi:10.1038/s41422-024-\n01034-y\nYao, S., You, R., Wang, S., Xiong, Y., Huang, X., and Zhu, S. (2021). Netgo 2.0:\nimproving large-scale protein function prediction with massive sequence, text, domain,\nfamily and network information.Nucleic acids Res.49, W469–W475. doi:10.1093/nar/\ngkab398\nYou, R., Yao, S., Xiong, Y., Huang, X., Sun, F., Mamitsuka, H., et al. (2019). Netgo:\nimproving large-scale protein function prediction with massive network information.\nNucleic acids Res.47, W379–W387. doi:10.1093/nar/gkz388\nYou, R., Zhang, Z., Xiong, Y., Sun, F., Mamitsuka, H., and Zhu, S. (2018). Golabeler:\nimproving sequence-based large-scale protein function prediction by learning to rank.\nBioinformatics 34, 2465–2473. doi:10.1093/bioinformatics/bty130\nYuan, Q., Xie, J., Xie, J., Zhao, H., and Yang, Y. (2023). Fast and accurate protein\nfunction prediction from sequence through pretrained language model and homology-\nbased label diffusion.Brieﬁngs Bioinforma. 24, bbad117. doi:10.1093/bib/bbad117\nZeng, X., Zhang, X., and Zou, Q. (2016). Integrative approaches for predicting\nmicrorna function and prioritizing disease-related microrna using biological\ninteraction networks. Brieﬁngs Bioinforma. 17, 193–203. doi:10.1093/bib/bbv033\nZhang, Q., Ding, K., Lyv, T., Wang, X., Yin, Q., Zhang, Y., et al. (2024). Scientiﬁc large\nlanguage models: a survey on biological and chemical domains.arXiv preprint arXiv:\n2401.14656\nZhang, X., Guo, H., Zhang, F., Wang, X., Wu, K., Qiu, S., et al. (2023). Hnetgo: protein\nfunction prediction via heterogeneous network transformer.Brieﬁngs Bioinforma. 24,\nbbab556. doi:10.1093/bib/bbab556\nZhang, X., Zhao, J., and LeCun, Y. (2015). Character-level convolutional networks for\ntext classiﬁcation. Adv. neural Inf. Process. Syst.28. doi:10.48550/arXiv.1509.01626\nZheng, Y., Koh, H. Y., Yang, M., Li, L., May, L. T., Webb, G. I., et al. (2024). Large\nlanguage models in drug discovery and development: from disease mechanisms to\nclinical trials. arXiv preprint arXiv:2409.04481\nZ h o u ,G . ,W a n g ,J . ,Z h a n g ,X . ,a n dY u ,G .( 2 0 1 9 a ) .“Deepgoa: predicting gene\nontology annotations of proteins via graph convolutional network,” in 2019 IEEE\ninternational conference on bioinformatics and biomedicine (BIBM) (IEEE),\n1836–\n1841.\nZhou, N., Jiang, Y., Bergquist, T. R., Lee, A. J., Kacsoh, B. Z., Crocker, A. W., et al.\n(2019b). The cafa challenge reports improved protein function prediction and new\nfunctional annotations for hundreds of genes through experimental screens.Genome\nBiol. 20, 244–323. doi:10.1186/s13059-019-1835-8\nZhu, Y.-H., Zhang, C., Yu, D.-J., and Zhang, Y. (2022). Integrating unsupervised\nlanguage model with triplet neural networks for protein gene ontology prediction.PLOS\nComput. Biol. 18, e1010793. doi:10.1371/journal.pcbi.1010793\nFrontiers inBioengineering and Biotechnology frontiersin.org17\nChen et al. 10.3389/fbioe.2025.1506508",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7266609072685242
    },
    {
      "name": "Function (biology)",
      "score": 0.6249108910560608
    },
    {
      "name": "Protein function prediction",
      "score": 0.619469940662384
    },
    {
      "name": "Protein function",
      "score": 0.5399205088615417
    },
    {
      "name": "Encoding (memory)",
      "score": 0.47700050473213196
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47416773438453674
    },
    {
      "name": "Key (lock)",
      "score": 0.4549824595451355
    },
    {
      "name": "Machine learning",
      "score": 0.42718085646629333
    },
    {
      "name": "Data mining",
      "score": 0.33930855989456177
    },
    {
      "name": "Biology",
      "score": 0.08372035622596741
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96908189",
      "name": "Xinjiang University",
      "country": "CN"
    }
  ],
  "cited_by": 21
}