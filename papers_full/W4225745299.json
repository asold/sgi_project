{
  "title": "Multi-Agent Deep Reinforcement Learning-Based Resource Allocation in HPC/AI Converged Cluster",
  "url": "https://openalex.org/W4225745299",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5042050195",
      "name": "Jargalsaikhan Narantuya",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5071145263",
      "name": "Jun-Sik Shin",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100452237",
      "name": "Sun Park",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100356563",
      "name": "JongWon Kim",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6782564567",
    "https://openalex.org/W3026605683",
    "https://openalex.org/W3088498531",
    "https://openalex.org/W6780160355",
    "https://openalex.org/W6781469070",
    "https://openalex.org/W6790887332",
    "https://openalex.org/W6783021124",
    "https://openalex.org/W6786780607",
    "https://openalex.org/W3003818908",
    "https://openalex.org/W6748645090",
    "https://openalex.org/W2793315650",
    "https://openalex.org/W2475126267",
    "https://openalex.org/W2076874848",
    "https://openalex.org/W6799360906",
    "https://openalex.org/W6771192598",
    "https://openalex.org/W6785653842",
    "https://openalex.org/W6768315956",
    "https://openalex.org/W6769596971",
    "https://openalex.org/W6762486247",
    "https://openalex.org/W2964667342",
    "https://openalex.org/W6782487749",
    "https://openalex.org/W6787871151",
    "https://openalex.org/W6774979231",
    "https://openalex.org/W6791046760",
    "https://openalex.org/W3093517752",
    "https://openalex.org/W6790753826",
    "https://openalex.org/W3094780753",
    "https://openalex.org/W6771274123",
    "https://openalex.org/W3103882602",
    "https://openalex.org/W3130585105",
    "https://openalex.org/W4230906109",
    "https://openalex.org/W3010528756",
    "https://openalex.org/W4301239768",
    "https://openalex.org/W3159546114",
    "https://openalex.org/W3187418527",
    "https://openalex.org/W4297627396",
    "https://openalex.org/W2977124065",
    "https://openalex.org/W3037739136"
  ],
  "abstract": "As the complexity of deep learning (DL) networks and training data grows enormously, methods that scale with computation are becoming the future of artificial intelligence (AI) development. In this regard, the interplay between machine learning (ML) and high-performance computing (HPC) is an innovative paradigm to speed up the efficiency of AI research and development. However, building and operating an HPC/AI converged system require broad knowledge to leverage the latest computing, networking, and storage technologies. Moreover, an HPC-based AI computing environment needs an appropriate resource allocation and monitoring strategy to efficiently utilize the system resources. In this regard, we introduce a technique for building and operating a high-performance AI-computing environment with the latest technologies. Specifically, an HPC/AI converged system is configured inside Gwangju Institute of Science and Technology (GIST), called GIST AI-X computing cluster, which is built by leveraging the latest Nvidia DGX servers, high-performance storage and networking devices, and various open source tools. Therefore, it can be a good reference for building a small or middle-sized HPC/AI converged system for research and educational institutes. In addition, we propose a resource allocation method for DL jobs to efficiently utilize the computing resources with multi-agent deep reinforcement learning (mDRL). Through extensive simulations and experiments, we validate that the proposed mDRL algorithm can help the HPC/AI converged cluster to achieve both system utilization and power consumption improvement. By deploying the proposed resource allocation method to the system, total job completion time is reduced by around 20% and inefficient power consumption is reduced by around 40%.",
  "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided\nthe original work is properly cited.\nechT PressScienceComputers, Materials & Continua\nDOI: 10.32604/cmc.2022.023318\nArticle\nMulti-Agent Deep Reinforcement Learning-Based Resource Allocation in\nHPC/AI Converged Cluster\nJargalsaikhan Narantuya1,*, Jun-Sik Shin2,S u nP a r k2 and JongWon Kim2\n1Department of Cloud, Kakao Enterprise Corp, Seongnam, 13494, Korea\n2AI Graduate School, Gwangju Institute of Science and Technology (GIST), Gwangju, 61005, Korea\n*Corresponding Author: Jargalsaikhan Narantuya. Email: jarven.17@kakaoenterprise.com\nReceived: 03 September 2021; Accepted: 15 October 2021\nAbstract: As the complexity of deep learning (DL) networks and training data\ngrows enormously, methods that scale with computation are becoming the\nfuture of artificial intelligence (AI) development. In this regard, the interplay\nbetween machine learning (ML) and high-performance computing (HPC) is\nan innovative paradigm to speed up the efficiency of AI research and develop-\nment. However, building and operating an HPC/AI converged system require\nbroad knowledge to leverage the latest computing, networking, and storage\ntechnologies. Moreover, an HPC-based AI computing environment needs an\nappropriate resource allocation and monitoring strategy to efficiently utilize\nthe system resources. In this regard, we introduce a technique for building\nand operating a high-performance AI-computing environment with the latest\ntechnologies. Specifically, an HPC/AI converged system is configured inside\nGwangju Institute of Science and Technology (GIST), called GIST AI-X\ncomputing cluster, which is built by leveraging the latest Nvidia DGX servers,\nhigh-performance storage and networking devices, and various open source\ntools. Therefore, it can be a good reference for building a small or middle-\nsized HPC/AI converged system for research and educational institutes. In\naddition, we propose a resource allocation method for DL jobs to efficiently\nutilize the computing resources with multi-agent deep reinforcement learning\n(mDRL). Through extensive simulations and experiments, we validate that the\nproposed mDRL algorithm can help the HPC/AI converged cluster to achieve\nboth system utilization and power consumption improvement. By deploying\nthe proposed resource allocation method to the system, total job completion\ntime is reduced by around 20% and inefficient power consumption is reduced\nby around 40%.\nKeywords: Deep learning; HPC/AI converged cluster; reinforcement learning\n1 Introduction\nWe currently live in the era of big data analysis and artificial intelligence (AI). Since AI and\nbig data analysis are capable of processing enormous amounts of data, the demand for large-scale\n4376 CMC, 2022, vol.72, no.3\ncomputing for AI research is continuously increasing. Thus, researchers and developers are paying\nmore attention to building a high-performance computing (HPC)-based AI-computing environment\nto accelerate the performance of deep learning (DL) workloads.\nHowever, building and operating an HPC-enabled AI-computing environment not only needs\nmultiple GPU servers, but it also requires broad knowledge to leverage the latest computing, network-\ning, and storage technologies. For instance, understanding network interconnects (e.g., Infiniband,\nRoCE, and Ethernet), network channels to communicate through them (gRPC, MPI), distributed\nstorage solutions (NFS, Ceph) are essential for scaling the AI-computing environment. Besides this,\ndeploying the right software methods, such as Slurm and Kubernetes, for resource management and\njob scheduling is one of the key factors needed to improve the system performance.\nIn [1], NVIDIA introduced an optimized system, called the DGX SuperPod. It is configured and\ndesigned for multi-node DL and HPC. DGX SuperPod consists of 140 DGX A100 servers with 1120\nNVIDIA A100 GPUs, and the DGX A100 s are connected to each other through 140 Infiniband\n200G switches. The Massachusetts Institute of Technology (MIT) has another scalable AI-computing\nenvironment, called Satori. It is developed in collaboration with IBM. Satori consists of 64 IBM power\nnodes, nine nodes with 256 NVIDIA V100 GPUs, and 2PB storage. It has a 64TB system memory and\n8TB GPU memory. GPUs inside a node are connected by an NVLink network that supports nearly\n200G bi-directional transfer between GPUs [2]. However, building and monitoring such a degree of\nscalability is only possible for organizations with sufficient funds to deploy infrastructure at a large\nscale.\nAdditionally, the problem of batch job scheduling is a long-standing topic in supercomputing.\nEarly studies examine the decision regarding the priority of the batch jobs. They developed various\nscheduling policies, such as first come first served (FCFS), and shortest job first (SJF). Recently,\nreinforcement learning (RL) has also been studied and leveraged in various task scheduling problems.\nIn [3], a DRL-based batch job scheduling method called RLScheduler is introduced. The RLScheduler\nlearns the job requirements to satisfy their workloads and optimization goals. Moreover, the authors\nin [4] propose an RL-based scheduling method to reduce the waiting time of the jobs in the queue.\nCompared to the previous studies, this work focuses on the choice of an appropriate server to run a\nDL job among multiple options.\nAn earlier, conference version of this paper appeared in [5]. In this paper, we extend the previous\npaper by adding detailed explanations of monitoring an HPC-based AI computing environment and\na DRL-based DL job scheduling scheme. The major contributions of this paper to the field are\nas follows. First, we proposed a multi-agent deep reinforcement learning (mDRL)-based resource\nallocation scheme for DL jobs to improve the system efficiency. Second, we conducted a more realistic\nperformance evaluation in a real cluster computing environment.\nThe remainder of this paper is organized as follows. Sections 2 and 3 provide an overview of the\nbackground and related works on an HPC/AI converged system and resource allocation for DL jobs,\nrespectively. Section 4 describes our system model (system description, problem definition), and the\nSection 5 provides the details of proposed resource allocation for DL jobs in the HPC/AI converged\nsystem. Section 6 demonstrates performance evaluations to validate the efficiency of the proposed\nresource allocation method for DL jobs. Finally, Section 7 summarizes the key findings and concludes\nthe paper.\nCMC, 2022, vol.72, no.3 4377\n2 Background\n2.1 Building HPC/AI Converged Cluster\nAs shown inFig. 1, an HPC/AI converged cluster, called the GIST AI-X computing cluster, is\ndeveloped for AI education and research in Gwangju Institute of Science and Technology (GIST).\nFigure 1:GIST AI-X computing cluster\nComputing: NVIDIA DGX servers, with multiple GPUs with a large memory, are used to set\nthe computing environment. The DGX servers are a line of NVIDIA-produced servers, which are\nconfigured to accelerate the performance of all ML/DL workloads. The DGX A100 consists of eight\nNVIDIA A100 GPUs with 320GB GPU memory, and the DGX-1 V consists of eight NVIDIA V100\nGPUs 256GB of memory. Moreover, the DGX A100 and DGX-1 V servers achieve five and one\npetaFlops performances, respectively.\nA combination of Prometheus and Grafana open-source tools is leveraged to monitor the GPU\nservers. The Prometheus is used to dynamically obtain the resource condition of each server, and the\nGrafana is used to visualize of the data that were collected through the Prometheus. In addition, users\nsubmit various jobs that require different versions of machine learning (DL) libraries, such as Pytorch\nand TensorFlow. These libraries require different CUDA versions depending on their version. For\ninstance, a DL job that uses Pytorch 1.6 does not work on the DGX-A100 server, since the A100\nGPU supports CUDA version 11 and Pytorch 1.6 requires CUDA version 10. Thus, it is essential to\ndistinguish the DL jobs by their system requirement. In this regard, two partitions are created in the\nenvironment to satisfy the system requirement, such as A100 and V100 for DGX-A100 and DGX-1\nV servers, respectively. For each partition, various types of computing resources are available for the\nDL jobs.\n4378 CMC, 2022, vol.72, no.3\nStorage: Two types of distributed storage solutions, such as network file systems (NFS) and\nCeph, are efficiently leveraged to provide high-performance storage services to the systems. Moreover,\nnonvolatile memory express (NVMe)-based all-flash drives are used to set the storage services. NVMe\nis a storage access and transport protocol for nonvolatile memory-based storage that delivers the\nhighest throughput and fastest response times for all types of workloads.\nNFS is a file system that enables the storage and retrieval of data from multiple disks across a\nshared network. More specifically, NFS is implemented in a client/server computing mode, where\nit enables NFS clients to remotely access the data stored in the NFS server in the same way that\nthey are accessed locally. NFS version 3 is used in our configuration, which provides a more efficient\nperformance, safe asynchronous writes, and a smaller number of attribute requests. As shown inFig. 1,\nstorage node 5 (FlashBlade 170TB server) is used to set the NFS server. The other nodes (DGX servers,\nSlurm login and Kubernetes master nodes) functioned as NFS clients. In the GIST AI-X computing\ncluster, the NFS-based storage service is called the AI-X data lake, and 5TB storage is allocated to\neach user with this solution [6].\nCeph is an open-source, software-defined storage solution for data workloads. It is a distributed\nstorage solution that provides highly scalable file, block, and object-based storage under a unified\ncluster system [7]. In addition, Ceph is fault-tolerant, and uses multiple disks over multiple servers to\nprovide a single storage cluster with no single point of failure. In the GIST AI-X computing cluster,\na Ceph-based storage service is called the AI-X data pond, and 3TB storage is allocated to each user\nwith this solution. Moreover, the Kubernetes master node is simultaneously used as a Ceph master,\nand the storage nodes 1-4 are used to configure the Ceph dataponds.\nNetworking:A ss h o w ni nFig. 1, three networks, such as management, internal, and data, are\nbuilt to configure the AI-computing cluster (colored green, blue, and red). The management network is\nconfigured to set the workload managers and connect the system to the Internet. The internal network\nis configured to enable multi-node training (connecting GPU nodes). The data network is used to\nprovide a high-performance storage service to the whole system. Currently, network communication\nstandards, such as Infiniband (IB) and remote direct memory access (RDMA) over converged Ethernet\n(RoCE), are commonly used as server and storage interconnects in the HPC environment. Thus, the\ninternal and data networks in this setting are configured with IB and RoCE switches, respectively. IB\nis a high-performance communication standard, which provides a low latency and high bandwidth\nconnection for system area networks (SAN) [8]. RoCE is a network protocol that enables RDMA\nover Ethernet network. RDMA provides direct memory access from the memory of one host to the\nmemory of another host with low latency and high bandwidth [9].\n2.2 Operating HPC/AI Converged Cluster\nThe GIST AI-X computing cluster is configured based on NVIDIA DeepOps, which encapsulates\nthe best practices for deploying GPU server clusters and sharing computing resources for multiple\nusers [10]. There are three deployment options for the DeepOps, such as Slurm-based, Kubernetes-\nbased, and hybrid, which supports both the Slurm and Kubernetes. As shown inFig. 1, the hybrid\nconfiguration is used to set the GIST AI-X computing cluster. Slurm is an open-source, fault-\ntolerant, and highly scalable cluster management and job scheduling system installed in about 60%\nof the top supercomputers in the world. As a cluster workload manager, Slurm is responsible for\nefficiently distributing the computing resources among the users and mapping the submitted jobs to\nthe underlying hardware. It has three major functions, such as resource management, job management,\nand a scheduler. First, the resource manager allocates either exclusive or nonexclusive access to the\nCMC, 2022, vol.72, no.3 4379\ncomputing resources to users for some duration. Second, the job manager provides a framework for\nstarting, executing, and monitoring jobs on the set of allocated compute nodes. Finally, the scheduler\narbitrates contention regarding the computing resources by managing a queue of pending works [11].\nRunning DL Jobs:Frameworks, such as Tensorflow and Pytorch are essential for implementing\nDL applications. They provide a collection of workflows to develop and train neural networks. Since\nthe GIST AI-X computing cluster is a shared system, installing all required libraries of multiple users\nin the system could be a reason for the various compatibility issues. In this regard, containerization\ntechnology is adapted to independently provide all user requirements. Users can use any docker\ncontainer that includes their desired software libraries and execute their job inside the container.\nHowever, the docker container should be converted to a singularity container to run in the Slurm\nenvironment. Singularity is an HPC-optimized containerization technology, which enables users to\nhave full control of their environment. This means that users do not need to ask the cluster-admin to\nexecute specific containers in the system. Instead, they can download and run any singularity container\nin the system by themselves. The singularity can import docker images without a docker being installed\nin the system or requiring a superuser [12].\nDistributed Training:Deep neural networks (DNN) with a large number of parameters require\nlarge amounts of data to learn their model. This is a computationally intensive process, which takes\na lot of time. Thus, it is essential to offer parallel and distributed training, which can run faster and\nreduce the training time.\nIn this study, parallel computing for distributed training is enabled using OpenMPI. The Open-\nMPI is an open-source message passing interface (MPI), where the MPI is a distributed communica-\ntion standard for parallel computing. MPIs in OpenMPI are used to invoke processes and send/receive\nmessages among different processes running on the same or different machines [13].\nScaling DL with distributed training considerably reduces the training time and improves the\nresearch progress. It encounters some obstacles, such as the training model needing to support inter-\nGPU communication and users needing to modify their training code to run with multiple GPUs.\nThus, Uber developed a framework called Horovod to make the distributed DL fast and easy to\nuse. Horovod takes a single GPU training script as an input and successfully scales it to train across\nmultiple GPUs in parallel [14]. In the GIST AI-X computing cluster, users mostly employ the Horovod\ncontainer to use multiple GPUs for distributed training.\n3 Related Work\n3.1 HPC/AI Converged Clusters\nTo emphasize diversified approaches and efforts for building HPC/AI clusters, several works\nare summarized inTab. 1. In fact, various sizes of clusters have been built depending on the main\ncontributors and target users. For example, a laboratory could build a toy example of clusters with\naround 10 servers, and university consortium could build multiple clusters with around 100 servers.\nAnd national institutes backed by world-leading companies and government have been building hyper-\nscale supercomputer clusters. To show such great interest in various scales, we arbitrary defined the\nsize of the clusters.\n4380 CMC, 2022, vol.72, no.3\nTable 1: High-performance AI computing clusters\nScale Scale Spec. (in total) Notice\nHyper (a)\nSUMMIT\n[15]\n■ 4608 nodes of CPU/GPU\nhybrid system\n■ 202,752 CPU cores of IBM\npower 9 processors\n■ 10.2PB memory of HBM\nand DRAM\n■ 27648 NVIDIA V100\nGPUs with NVLink\n■ 250PB IBM GPFS-based\nstorage pool\n■ 2 x 100G NVIDIA\ninfiniband EDR\nPeak performance:\n■ 200 PFLOPS\n(b)\nSIERRA\n[16]\n■ 4320 nodes of CPU/GPU\nhybrid system\n■ 190,090 CPU cores of IBM\npower 9 processors\n■ 17280 NVIDIA V100\nGPUs with NVLink\n■ 1.29PB RAM\n■ 2 x 100G NVIDIA\ninfiniband EDR\n■ 154 PB IBM GPFS-based\nstorage pool\nPeak performance:\n■ 125 PFLOPS\n(c) Sunway\nTaihulight\n[17]\n■ 40960 nodes of many-core\nCPU system\n■ 10 million cores of\nhome-grown CPU processor\n■ 1310.72TB RAM\n■ 20PB storage Pool\n■ 16G network link\nbandwidth\nPeak performance:\n■ 125 PFLOPS\n(d)\nMilkyway-2\n(Tianhe-2)\n[18]\n■ Compute subsystem: 16000\ncompute nodes\n■ 384,000 cores of Intel Xeon\nE5-2600 v2 Processors, and\n2,736,000 cores of Intel Xeon\nPhi Co-processors\n■ 1.024PB RAM for CPU\nand 0.384PB RAM\nPeak performance:\n54.9 PFLOPS\n(Continued)\nCMC, 2022, vol.72, no.3 4381\nTable 1:Continued\nScale Scale Spec. (in total) Notice\nMiddle (a) Hybrilit\n[19]\n■ Heterogeneous computing\nclusters\n■ HybriLit education and\ntesting polygon: 10 nodes with\nIntel Xeon processors and\nNVIDIA GPUs\nPeak performance:\n■ Testing polygon 142\nTFLOPS\n■ GOVORUN 1000\nTFLOPS\n■ GOVORUN\nsupercomputer: 40 nodes with\nIntel Xeon processors, 21\nnodes with Intel Xeon Phi\nmany-core processors, 5\nNVIDIA DGX-1 V nodes.\n■ Intel Omni-path\nInter-connect\n■ SLURM-based ML/DL\nworkload orchestration\n(b)\nCluster-UY\n[20]\n■ 28 computing nodes\n■ 2 storage node\n■ 1 service (DevOps) node\n■ 560 cores of Intel Xeon\nGold CPUs\n■ 28x NVIDIA P100 GPUs\n■ 3.584TB RAM\n■ 90TB external storage pool\n■ 1G for management and\nmonitoring\n■ 10G for control and data\ntraffic\nPeak performance:\n■ 166 PFLOPS\n(c)\nOCCAM\n[21]\n■ 32 light nodes (2x Intel\nE5-2680 CPUs,\n■ 128GB RAM, 400GB SSD)\n■ 4 fat nodes (4x Intel\nE7-4830 CPUs 768GB RAM,\n800GB SSD+ 2TB HDD)\n■ Deploying workloads\nwith Docker containers\n■ Creating virtual\nclusters by utilizing\nAnsible and Marathon\n■ 4 GPU nodes (2x Intel\nE5-2680 CPUs, 128GB\nRAM2x NVIDIA K40 GPUs,\n800GB SSD)\n(Continued)\n4382 CMC, 2022, vol.72, no.3\nTable 1:Continued\nScale Scale Spec. (in total) Notice\n■ Multi-tiered storage pool:\n256 TB\n■ 768 TB NFS-enabled\narchive pool.\n■ 1G control/management\nand 10 G data networks\n■ 56G NVIDIA Infiniband\nFDR\nSmall (a)\nCALIBAN\n[22]\n■ 17 nodes of CPU/GPU\nhybrid system\n■ 544 cores of 68 AMD CPUs\n■ 6x NVIDIA GTX 670\nGPUs\n■ 1.01TB RAM\n■ 7.446TB Local HDD\nStorage Pool\n■ Mellanox Infiniband 40G\n■ 1G network for SAN\n■ 50TB external storage pool\n■ Parallel Computing\n■ MPI and openMP\n(b)\nEVOLVE\n[23]\n■ Cluster with heterogeneous\ncomputing resources (CPU,\nGPU, and FPGAs)\n■ 10 compute nodes (each\nwith 24 cores and 128GB\nRAM)\n■ 5 accelerator nodes.\n■ NVIDIA InfiniBand FDR\nLink (56GB/s) under a fat-tree\nfabric interconnect\n■ 128TB Lustre filesystem\n■ Kubernetes-based\n■ cloud-native\ncomputing\n■ Custom schedulers\n■ Unified Storage\nLayer\n■ Monitoring services\n(c) SmartX\nIntelligence\nCluster [24]\n■ 5 computing nodes with\nIntel Xeon CPUs and Nvidia\nTitanV GPUs\n■ 400TB all-flash storage\nnodes\n■ RoCE-based 100G network\n■ Kubernetes-based\ncloud-native computing\nNotes: Hyper scale: More than 1000 nodes/Middle scale: 20-1000 nodes/Small scale: Less than 20 nodes\nThe detailed specifications can be changed over time. This table shows the specification described in the referenced\nmanuscripts.\nDue to the complexity of DL models and large training data, a demand of scaled computing for\nAI research and development is continuously increasing. While existing systems provide computing\nresources through networks for end users, the current solutions still face many challenges. The\nCMC, 2022, vol.72, no.3 4383\nend devices could be self-driving cars, drones, robots that require data processing and decision\nmaking closer to the point of data generation due to mission critical, low-latency and near-real time\nrequirements. In this regard, most of the organizations desire to build their own scaled AI computing\ncluster close to their development environment. However, building and operating the scaled computing\nenvironment for AI is not easy task, since it requires broad knowledge of networked systems and high\ncost. In this study, we introduce a case of building and operating an AI computing environment suitable\nfor small or middle-sized research and educational institutes.\n3.2 Job Scheduling and Resource Allocation\nThere are several existing works on resource allocation and job scheduling in HPC/AI converged\nsystems [25–35]. Early studies examine the selection of a priority for the batch jobs. They developed\nvarious scheduling policies, such as first come first served (FCFS), and shortest job first (SJF).\nRecently, reinforcement learning (RL) was studied and leveraged in various task scheduling problems.\nThinakaran et al. [25] proposed Kube-Knots for multi-node GPU utilization-aware orchestration\nalong with QoS-aware container scheduling policies. The Kube-Knots are leveraged by the Kubernetes\nat the cluster level to monitor the real-time GPU utilization at every node. They also proposed Peak\nPrediction (PP) and Correlation Based Prediction (CBP) schedulers that perform safe co-locations\nthrough GPU spare cycle harvesting by dynamically resizing the containers (i.e., crash-free dynamic\ncontainer resizing). It also performs QoS-aware container co-locations on GPUs at the cluster-\nlevel without a priori knowledge of incoming applications. In Shao et al. [26], proposed a GPU\nscheduling method using a modified shortest-job-first scheduling policy with respect to GPU-sharing\nfor short GPU tasks of multiple users. The scheduling method is implemented by a container-based\nbatch computing system that accepts and runs users’ jobs through container images with specified\nconfigurations, without the user having to care about resource leases and releases. The proposed\nmethod ensures the priority of the short tasks and prevents long tasks from starving. Chen et al.\n[27] proposed a QoS-aware dynamic scheduling framework for a shared GPU cluster to achieve\nusers’ QoS guarantee and high system utilization. They proposed a predictive model derived from\nlightweight profiling to estimate the processing speed and response latency for various Deep Learning\n(DL) workloads. The QoS-aware scheduling algorithm identifies the best placements for DL tasks\nand schedules them on the shared cluster. They showed that the proposed method on real clusters\nand simulations achieves higher QoS guarantees and system utilization than other reference methods.\nIn Filippini et al. [28], proposed a hierarchical approach coupled with a novel Mixed Integer Linear\nProgramming (MILP) formulation to overcome limitations of scalability issues with respect to problem\ninstances with a small number of nodes and jobs. Their proposed method optimizes operating costs\nby rightsizing VM capacity on each node, partitioning GPU sets among multiple concurrent jobs on\nthe same VM, and scheduling deadline-aware jobs. They showed a good fit with real-world scenarios,\nas the scalability analysis showed that the proposed method can solve problem instances with up to\n100 nodes in less than one minute on average.\nIn Habuza et al. [29], proposed a user-friendly jobs and resources allocation manager (i.e.,\nweb-based multi-user concurrent job scheduling system) for the Machine Learning (ML) server.\nThe proposed manager helps users to request and allocate resources from the server and monitor\nthe progress of their jobs to provide AI research or computations with an optimized combination\nof compute power, software and deep learning performance. There are three main task manager\ncomponents: LoginServer, ML server (DGX-1) and Data Storage. Luan et al. [30] proposed a GPU\ncluster scheduler using deep reinforcement learning (DRL) to execute smart locality-aware scheduling\nfor efficient deep learning training (DLT) jobs. The proposed scheduler controls fragmentation in GPU\n4384 CMC, 2022, vol.72, no.3\nclusters by reflecting the current and future degree of cluster fragmentation. It also improves cluster\nutilization and job performance by using the profiling data to reflect job’s locality-sensitivity. They\nshow that SCHED2 can effectively control cluster fragmentation and job locality, reduce the JCT by\n4.6 times, and make the span 2.1 times. In Wang et al. [31], proposed a non-intrusive GPU scheduling\nframework by integrating an adaptive GPU scheduler and an elastic GPU allocation method to reduce\nthe span and improve resource utilization. The GPU scheduler can determine the most efficient\nallocation and reallocation of GPUs by using training job progress information for incoming and\nrunning jobs at any given time. The GPU allocation method temporarily stops and restarts the job’s\nDL training process with a different number of GPUs based on a “SideCar” process to further reduce\nthe reshaping overhead. The proposed method has been shown to reduce overall execution time and\naverage job completion time by up to 45% and 63%, respectively, compared to the default scheduler.\nIn Abdulla et al. [32], proposed a deep learning-based method to automatically allocate optimal CPU\nresources to the containers. The proposed method decides on the optimal number of CPU pins using\nthe law of diminishing marginal returns for the maximum performance of containers while maximizing\nthe number of concurrent jobs. They showed the effectiveness of the proposed method in reducing the\ncompletion time of the jobs to compared to static CPU allocation methods (i.e., First Come First Serve\n(FCFS), Shortest Job First (SJF), Longest Job First (LJF), and Simulated Annealing (SA)).\nZhao et al. [33] proposed a scheduling system (i.e., CODA) that enhances the resource utilization\nof GPU clusters. The proposed system consists of an adaptive CPU allocator, a real-time contention\neliminator, and a multi-array job scheduler. CODA uses the feedback-based technique to identify\na sufficient number of cores for a DNN train job. CODA removes interference to improve the\nperformance of jobs by analyzing the competition for CPU resources in the performance of DNN\ntraining jobs. CODA’s multi-array job scheduler removes GPU fragmentation. Their system improves\nGPU utilization by 20.8% on average without increasing the queuing time of CPU jobs. In Lin et\nal. [34], proposed multiple intelligent schedulers based on a two-stage job scheduling and resource\nallocation framework for the cooperative scheduling problem of job scheduling and resource allocation\nregarding multi-user multi-cloud data centers. They used the heterogeneous distributed deep learning\n(HDDL) model to schedule multiple jobs and the deep Q-network (DQN) model to deploy virtual\nmachines, respectively. The proposed method, using an HDDL-based task scheduler and DQN-\nbased resource allocator in numerical simulation experiments, can achieve a better performance and\ncomputation delay than the baseline.\nExisting works on resource allocation for HPC systems are broadly categorized into two main\nparts such as energy efficiency and performance-based. Energy efficiency-based works try to minimize\na number of running servers, and the performance-based methods try to complete submitted jobs as\nfast as possible. The energy efficiency-based method is similar to the default scheme (Naïve), which\nfinds an available server from the first server in order to minimize the number of running servers.\nPerformance-based models try to allocate computing resources as much as possible for a submitted\njob. However, our work allocates fixed resources for each job based on the allocated flavor type.\n4 System Model\n4.1 System Description\nAs illustrated inFig. 2, we consider a high-performance AI computing cluster environment. The\nAI computing cluster architecture is composed of a login, compute, and storage nodes. The login\nnode is a CPU server, which monitors the resource utilization of the computation nodes and allocates\ncomputing resources for user jobs. The compute nodes are GPU servers that user jobs originally\nCMC, 2022, vol.72, no.3 4385\nrun with the requested computing resources. In brief, a user job is submitted to one of the GPU\nservers when the server has enough resources to run the job. The GPU servers are grouped into\nmultiple partitions depending on the user and system requirements. For instance, each department in\na university can use a different partition. Moreover, the GPU servers can be categorized into different\npartitions depending on their system specifications. Additionally, the login, compute and storage nodes\nare connected through management, internal, and data networks. The management network is used to\ncontrol and manage the resource management system among the cluster. The internal network is high-\nspeed Infiniband network, which is used to connect the GPU servers to enable multi-node training and\nparallel computing. Lastly, the data network is high-speed RoCE network, which is used to connect\nGPU servers to the storage node.\nFigure 2:System description\nIn order to run a job on the GPU nodes, users first connect to the login server; then, they request\na certain amount of computing resources from a dedicated partition. The user job will automatically\nstart to run with the requested resources if there are enough computing resources in the partition. In\naddition, the job will be in the pending state if there are not enough resources in the partition. The\nmaximum amount of computing resources that a user can request can be configured in the system.\nIn cloud and cluster computing, the set of computing resources that can be allocated to each user is\ngenerally called a flavor. As shown inTab. 2, an s-A100 type job can use, at most, two A100 GPUs, 64\nCPU cores, and 256GB memory. Moreover, the time limit for running a s-A100 type job is 72 h.\n4.2 Problem Definition\nWe assume there areN GPU servers that are grouped intoM partitions depending on their system\nspecifications and user requirements. LetP ={ p1, p2 ... pN } be a set of all partitions in the cluster. Then,\nthe total number of GPU servers is\nN =\nM∑\ni=1\n|pi| (1)\nwhere |pi| is a number of GPU servers ini-th partition.\n4386 CMC, 2022, vol.72, no.3\nTable 2: Resource Allocation Policy (Flavor)\nName GPU\n(number)\nCPU\n(cores)\nMemory\n(GB)\nTime\n(t)\nRate\n(λ)\ns-A100 2 × A100 64 256 72 h 0.125\nm-A100 4 × A100 128 512 168 h 0.25\nl-A100 8 × A100 256 1024 336 h 0.5\nf-A100 16 × A100 512 2048 504 h 1\ns-V100 2 × V100 20 128 72 0.125\nm-V100 4 × V100 40 256 168 h 0.25\nl-V100 8 × V100 80 512 336 h 0.5\nf-V100 16 × V100 160 1024 504 h 1\nLet di,j be j-th DL job, submitted to ani-th partition. As mentioned in 4.1, one of the flavors\n(resource allocation type) is allocated to each DL job. The flavors could be as same as illustrated in\nTab. 2. Then,λ(d\ni,j) is a function used to find a resource allocation rate fordi,j among all the resources\nin a partitionpi,w h e r e0<λ ≤ 1. For instance, if a flavor s-A100 inTab. 2is allocated for a jobdi,j,\nits resource rateλ(di,j) is 0.125. Lett(di,j) be the function used to check the configured time limit for\nthe jobdi,j. For instance,t(di,j) can be 72 h if a flavor s-A100 is allocated to thedi,j. Then,τ(di,j) is the\ntotal run time of the jobdi,j in the system, and it is computed as\nτ(di,j) =\n{\ntc − di,j.ts , if running\nt(di,j), if completed (2)\nwhere di,j. ts is a start time of the jobdi,j and tc is current time. Moreover, letT\npi\ntc be the total run time\nof the partitionpi at time tc. We consider a partition run time to have started when the first job is\nsubmitted. Then, it can be computed as follows.\nTpi\ntc = tc − min\ndi,j∈pi\n{di,j.ts} (3)\nThen, an utilization rateU of pi at timetc can be computed as follows.\nUpi\ntc =\n∑\ndi,j∈ pi λ(di,j) · τ(di,j)\nR · T\npi\ntc\n(4)\nR is a total resource rate of each partition, which it is generally set as 1.\nFig. 3shows two cases where five DL jobs are scheduled in a single partition. In this example, we\nassume the partition is A-100. Let a rectangle be a job submitted to the system, where its width is the\nratio of required resources and height is the time taken to complete the job. As shown inTab. 2,t h e\nflavors s-A100, m-A100, m-A100, l-A100, and f-A100 are allocated to jobs 1-5, respectively. Consider\nthe jobs submitted sequentially to the system at timest\n0, t1, t2, t3,a n dt4.\nCMC, 2022, vol.72, no.3 4387\nFigure 3:Resource allocation and scheduling cases (a) default (b) proposed\nIn case 1, the system used the default resource allocation rule that checks the available resources\nfor the job from the first server. Then, it allocates resources from the first discovered server for the\njob. As shown in case 1, jobs 1 and 2 are submitted to server 1, and job 3 is submitted to server 2,\nsince there is an insufficient resource for job 3 in server 1 at timet\n2.A tt i m et3, there is an insufficient\nresource for the job 4. Thus, the job is submitted at timet1 + 7, when server 1 has resources. Then, job\n5i ss u b m i t t e dt ot h es y s t e ma tt i m et1 + 17, when both servers are released.\nAt timet1, an agent has two options for running job 2, which the job 2 can be submitted to either\nserver 1 or 2. Following the default resource allocation rule, job 2 is submitted to server 1 in case 1,\nsince there are enough resources to run the job. In case 2, job 2 is submitted to server 2. As a result,\nsubmitting job 2 to server 2 helped to complete all the jobs at timet\n0 + 27 in case 2, while it ist1 + 31 in\ncase 1. Following the run time inTab. 2, all jobs are completed at least 4 days earlier in case 1, compared\nto case 2. Besides this, inefficient utilization of the system, gray space in the figure, is considerably high\nin case 1 compared to case 2. Thus, an efficient resource allocation scheme is essential to improve the\nefficiency of monitoring the HPC/AI converged system.\nIn this regard, we aim to improve the total system utilization. Thus, the proposed resource\nallocation for monitoring the AI computing environment is as follows.\nX\n∗ = arg max\nX\n∑\npi∈P\nUpi\ntc\nsubject to ∑\ndi,j∈ pi\ndi,j.g ≤ pi.g\n∑\ndi,j∈ pi\ndi,j.c ≤ pi.c for ∀i ∈ [1, M] (5)\n∑\ndi,j∈ pi\ndi,j.m ≤ pi.m\nThe di,j.g, di,j.c,a n ddi,j.m are the respective allocated amounts of GPU, CPU, and memory resources\nto thedi,j.M o r e o v e r ,tpi.g, pi.c,a n dpi.m are the respective total amounts of GPU, CPU, and memory\nresources in partitionpi.\n4388 CMC, 2022, vol.72, no.3\n5 Proposed Resource Allocation\n5.1 Multi-Agent Deep Reinforcement Learning\nAt present, many sequential decision-making problems are solved using reinforcement learning\n(RL). In RL, an agent interacts with the environment and tries to learn an optimal policy that\nmaximizes the utility of its actions. At each time step, the RL agent predicts the best action to execute\nin the current state and executes it. To predict the best agent action, an agent selects the action with\nmaximum cumulative reward from all possible actions in the current state. The cumulative reward is\nreferred to as the quality value (Q value). An agent updates aQ value for each executed action based on\nthe immediate reward received from the environment. TheQ value for an action in states\nt is calculated\nusing Eq. (6), called the Bellman equation.\nQ(st, at) = rt + γ · max(Q∗(st+1, at+1)|st, at) (6)\nThe immediate rewardrt can be obtained directly from the environment immediately after the action\nat.T h eQ value for the next state st+1 is calculated based on either theQ table or approximated with\nDNN. Theγ is a fixed value (discount factor), which controls the importance of long-term rewardsvs.\nthe immediate reward. Moreover, memorizing all states in theQ table is unfeasible when the state and\naction space is large. In this regard, DNN can be used to approximate expectedQ values for future\nstates. The RL that uses DNN to model the policy is called deep reinforcement learning (DRL) [3].\nMost real-world problems, such as autonomous driving and robotics, require multiple policies to\ncontrol their tasks, even though the tasks share the same environment. In this regard, multi-agent DRL\n(mDRL) is used to solve sequential decision-making problems that require multiple policies. This is\nthe same as single-agent RL, where each agent tries to learn their solution [3].\n5.2 Proposed Strategy\nAs illustrated inFig. 4and Algorithm 1, mDRL is employed to allocate resources for DL jobs in\nthe AI computing environment. The mDRL methods for the proposed solution are defined as follows.\nAgents: A single centralized agent can observe the entire system state and decide actions for all\npartitions. In practical implementations, it is not easy for the agent to find an optimal solution because\nthe action space increases significantly as the number of partitions increases. Thus, we employ the\ndecentralized mDRL and consider the job scheduling policy for each partition as an agent. Once a\njob arrives in the system, an agent is selected based on the job partition. Then, the agent decides the\nbest action for the job, which has the largestQ value among the possible actions in the current state.\nIn each agent, DNN is used to approximateQ values for all possible actions. Initially,Q values are set\nto 0 in all states. They are updated while the agents interact with the environment. To train the DNN,\nan agent performs a certain number of actions by providing them with positive rewards for the right\naction and trains them to stay away from others by providing negative rewards.\nState: The state-space of the AI-computing cluster for DL job scheduling is a set of available\nresources in each GPU server. With the monitoring tools in the AI-computing cluster, available\nresources in each server can be obtained dynamically. Thus, each agent can obtain the state information\nof its dedicated partition. One possible state-space is shown inFig. 4. As shown in the figure, full\nresources are available in all DGX servers. The resource information of the servers can be an initial\nstate. Besides this, an episode in RL is the same as taking actions from the initial to the terminal state.\nIn this study, the initial state for each partition indicates no running job in the partition. The terminal\nstate is a state after submitting a job that uses all the partition’s resources. For instance, submitting an\nf-A100 type job for partition A100 and f-V100 type job for partition V100 can serve as terminal states.\nCMC, 2022, vol.72, no.3 4389\nFigure 4:Proposed strategy\nAction: Since the environment is an AI-computing cluster, the action space is a set of possible\nactions for a DL job, such as running the job in either single or multiple GPU servers and putting\nthe job in the queue when there are insufficient resources to run the job. In this study, DL jobs are\ncategorized based on their allocated resource type. Thus, the combination of a flavor and GPU server\nis considered as the action of running a job. Besides this, putting each different type of job in the queue\nis considered an action. For instance, letn be the number of available flavors that can be allocated in a\nsingle server andm be the number of GPU servers in thei-th partition. Thus, the number of possible\nactions to run all types of jobs in a single server isn · m,a n dt h e r ea r en additional actions to put the\njobs in a queue (putting different type of jobs in queue are considered different actions).\nActions in previous studies are mostly considered to decide on the priority of the jobs in the queue.\nIn practical, critical jobs, such as paper experiments and training DNN for real applications, need more\ncomputing resources and time. However, non-critical jobs, mostly submitted from new users, such as\ntesting the GPU cluster or practicing DL with multi-GPUs, do not require many resources or a long\ntime. In this study, the job priority is set based on the largest job-first strategy (LJF). For instance, the\nhighest priority for the full type of flavor job and the lowest priority for the small flavor type of job.\n4390 CMC, 2022, vol.72, no.3\nReward: If a job is added to the queue when there are enough resources to run the job in the\ndedicated partition, its reward is−1. Moreover, the reward is−1 when the selected action for a job is\nto execute it in one of the servers, while there are insufficient resources in the partition. Here, the job\nis added to the queue. If there is an insufficient resource in all possible servers and the selected action\nfor a job is putting it in the queue, its reward is 0. If the selected action for a job is executing the job in\none of the GPU servers, and there are enough resources to run it, its reward is the utilization rate of\nthe dedicated partition. The utilization rate for partitionp\ni at timet is computed withEq. (4).\nAlgorithm 1Resource allocation algorithm for DL jobs\nInput:\n1: A buffer for receiving jobsJ\n2: Number of episodesE\n3: Number of jobsK\nInitialize:\n4: Set agentA\ni for each partitionpi, ∈ [1, M]\n5: Set a replay memoryD\n6: Set a policy networkQi\nω, random weightsωi and θi\n7: Set a target networkQi\nω, weightsω′\ni ← ωi, θ′\ni ← θi,\n8: Set a replay memoryD\n9: while J is not emptydo\n10: for episode = 1t oE do\n11: while not terminal statedo\n12: Get a job di, j ← J, j∈[1, K]\n13: Select an agent Ai ← di, j\n14: Check state si ← Ai, Pi\n15: Select action ai ← Qi, Di,j\n16: Submit a job pi ← di,j\n17: Set a start time di,j.s ← t\n18: Execute ai and observeri ← U (pi, t)\n19: Update D ← ait, rit, si\nt, si\nt+1\n20: Update Qi(si, ai) ← ri, Qj(si, ai)\n21: end while\n22: end for\n23: end while\n6 Performance Evaluation\n6.1 Simulations\nWe evaluated the performance of the proposed DL job-scheduling scheme with numerous\nsimulations. The simulation environment is configured similarly to the case introduced inFig. 3.T o\ndemonstrate the proposed strategy’s effectiveness, we trained the DNNs for each agent with 1000\nepisodes, with each episode consisting of five DL jobs. The flavors allocated to the jobs are randomly\nselected from the flavors in theTab. 2. An episode is considered as the process of submitting a number\nof DL jobs to the cluster until the terminal state. The terminal state is the state after submitting a job\nthat uses all the partition’s resources. For instance, the terminal state job can be either an f-A100 or\nf-V100 job, as introduced inTab. 2. Moreover, one of the s-A100, m-A100, and l-A100 flavors can be\nCMC, 2022, vol.72, no.3 4391\nallocated to the jobs that run in each episode if the partition is A100. Similarly, one of the s-V100,\nm-V100, and l-V100 flavors can be allocated to the jobs that run in each episode if the partition is\nV100. In addition, the performance of the proposed method is compared with a Naïve scheme that\nsubmits a job to a randomly selected GPU server if the server has enough resources to run the job.\nFig. 5shows the results of the system utilization rate with respect to the number of episodes. The\nresults of both the proposed and Naïve schemes are illustrated inFigs. 5aand 5b, respectively. Initially,\nthe utilization rate with the proposed strategy fluctuates in the long range, but it converges after 300\nepisodes, with its average utilization rate converging to more than 0.75. However, the utilization rate of\nthe Naïve method continuously fluctuates in the long range, and frequently drops to less than 0.3. The\nreason for the lower utilization of the Naïve method is that it does not consider future rewards, while\nthe proposed scheme continuously learns the model and considers the current state when deciding on\nan action. The simulation results indicate that the use of proposed strategy considerably improves the\nefficiency of the system utilization compared to the Naïve scheme.\nFigure 5: Utilization rate with respect to episodes (a) utilization rate for proposed allocation\n(b) utilization rate for default allocation\n6.2 Experiments\nTo conduct a more realistic performance evaluation, we carried out series of experiments in GIST\nAI-X computing cluster. As introduced in Section 2.1, two DGX-A100 and two DGX-1 V GPU servers\nare used to set the compute nodes. DGX A100 and 1 V servers have eight NVIDIA ampere A100\nGPUs with 320GB GPU memory and eight NVIDIA V100 GPUs with a total of 256GB of memory,\nrespectively. Additionally, two SuperServer 1020U are used to set the Slurm login and Kubernetes\nmaster nodes. Each SuperServer has dual 6-core Intel Xeon CPUs, 64GB system memory, 4TB NVMe\nSSD, two 25G, and two 10G NICs. Additionally, two storage servers, such as Purte storage Flash Blade\n(170 TB) and SuperMicro 2u4N (Total 153.6TB), are used to set the storage services.\nMoreover, Mellanox QM8700 200G (IB), Mellanox SN2100 100G (RoCE), and Netgear 10G\n(Ethernet) switches are used to configure the internal, data, and management networks. Eight 200G\nports of each DGX-A100 and two 100G ports of each DGX-1 V are used to set the internal network.\nSince the IB switch used for the internal network has 200G ports (HDR200), two of them are splitted\ninto dual 100G ports for DGX-1 V servers. Two 100G ports of each DGX- 1 V are used to connect the\nserver to the data network. Since the other nodes (Slurm login, Kubernetes Master, and the storage\n4392 CMC, 2022, vol.72, no.3\nnodes 1-4) have only dual 25G NICs, one 100G port in the RoCE switch are splitted into four 25G to\nconnect the 25G nodes to the data network. Due to the 40G NICs in storage node 5 (data lake node),\nthe external fabric module (XFM) is used to connect the server to the 100G data network.\nWith the trained a DNN for each agent, 100 episodes are randomly created in order to test the\nefficiency. Each episode consists of five DL jobs, where four jobs can be run in a single server, and the\nlast job is a terminate state job, which uses all the partition’s resources, is submitted. For the jobs in\neach episode, flavors s-A100, m-A100, l-A100, f-A100 inTab. 2are allocated for the partition A100\nand flavors s-V100, m-V100, l-V100, f-V100 inTab. 2are allocated for the partition V100. To shorten\nthe experiment time, the time limit is set as minutes, which is defined as hours inTab. 2.\nIn Fig. 6a, the blue lines show the results of the system utilization with the proposed strategy\nin partition A100, and the red dashed line illustrates the average resource utilization rate with the\nNaïve method. Additionally, the performance of the resource utilization with the proposed method\nin partition V100 is depicted inFig. 6b. In both partitions, the average utilization rate of the Naïve\nscheme is lower than the system utilization of the proposed strategy in all the episodes. As shown in the\nfigure, the average system utilization with the proposed scheme is more than 0.75, while it is around\n0.62 with the Naïve method. Moreover, the total run time of the system in each episode is reduced by\naround 20% with the proposed scheme.\nFigure 6:Experiment results (a) utilization rate (A100) (b) utilization Rate (V100)\nAdditionally, power consumption in HPC cluster is one of the main measurements. Each DGX-\nA100 servers uses 6.5 KW power, and each DGX-V100 server uses 3.5 KW power. In the GIST AI-\nX cluster, there are two DGX-A100 servers in A100 partition and two DGX-V100 servers in V100\npartition. Thus, power consumption of the A100 and V100 partitions are at least 13 KW and 7 KW ,\nrespectively. The power consumption analysis is depicted on theFig. 7. If we consider an episode runs\nfor 720 h, the system runs for 273.6 h (720 x 0.38) inefficiently with the default resource allocation\nmethod. Then, inefficient power consumption for partition A100 is 3.55 MW (273.6 kW x 13 KW),\nand 1.91 MW for partition V100. With the proposed method, it is decreased to 2.15 MW and 1.15 MW\nfor A100 and V100 partitions, respectively. Thus, inefficient power consumption is reduced around\n40% with the proposed method.\nCMC, 2022, vol.72, no.3 4393\nFigure 7:Power consumption analysis\nThe reason for the improvement is that the proposed method considers future reward by leveraging\nthe feature of reinforcement learning. However, existing works only consider the current state and find\navailable servers to allocate resources for jobs from first to end.\n7 Conclusion and Future Work\nIn this study, we introduce a method of building and operating an HPC/AI converged environ-\nment, which is suitable for small or middle-sized research and educational institutes. Additionally,\nwe propose a DL job-scheduling method with mDRL, which considerably improves the efficiency\nof system utilization. Through extensive simulations and experiments, we validate that the proposed\nmDRL algorithm can help the HPC/AI converged cluster to achieve both system utilization and\npower consumption improvement. By deploying the proposed resource allocation method, total job\ncompletion time is reduced by around 20% and inefficient power consumption is reduced by around\n40%. In the future, we plan to collaborate with other organizations who have HPC/AI computing\nclusters, then we will evaluate a performance of the proposed method on other systems.\nMoreover, running AI applications on resource-constrained devices encounters severe challenges.\nIn this regard, HPC/AI converged system is becoming a major trend for delivering infrastructure\non demand via the network. While existing systems provide computing resources through networks\nfor end users, the current solutions still face many challenges. The end devices could be self-\ndriving cars, drones, robots that require data processing and decision making closer to the point of\ndata generation due to mission critical, low-latency and near-real time requirements. However, the\ntraditional centralized HPC-clusters fail to meet the requirement, since it is usually located far from\nthe end users. To satisfy the requirement of low-latency and near-real time data processing, HPC/AI\nclusters should be built close to the source of data generation with edge computing and only send\nfiltered data to the centralized system when it requires large storage or higher-level analysis.\nAcknowledgement: This work was supported by Institute of Information and Communications\nTechnology Planning and Evaluation (IITP) grant funded by the Korean government (MSIT) (No.\n2019-0-01842, Artificial Intelligence Graduate School Program (GIST)). We would like to thank the\nCloud team at Kakao Enterprise, as well as especially thank Jung-Bok Lee for his support during the\npaper revision.\n4394 CMC, 2022, vol.72, no.3\nFunding Statement: This research was funded by Artificial Intelligence Graduate School Program\n(GIST).\nConflicts of Interest:The authors declare that they have no conflicts of interest to report regarding the\npresent study.\nReferences\n[1] NVIDA, “NVIDIA DGX SuperPOD: Scalable infrastructure for AI leadership.,” 2020. [Online]. Available:\nhttps://images.nvidia.com/aem-dam/Solutions/Data-Center/gated-resources/nvidia-dgx-superpod-a100.\npdf.\n[2] IBM and MIT, “Satori.” 2021. [Online]. Available:https://mit-satori.github.io/.\n[3] D. Zhang, D. Dai, Y . He, F . S. Bao and B. Xie, “RLScheduler: An automated HPC batch job scheduler\nusing reinforcement learning,” inSC20: Int. Conf. for High Performance Computing, Networking, Storage\nand Analysis, Online conference, pp. 1–15, 2020.\n[4] S. Liang, Z. Yang, F . Jin and Y . Chen, “Data centers job scheduling with deep reinforcement learning,” in\nAdvances in Knowledge Discovery and Data Mining, vol. 12085, pp. 906, 2020.\n[5] N. Jargalsaikhan, S. Jun-Sik, P . Sun and K. JongWon, “Optimizing computing environment for scaling\ndeep learnig applications,” inInt. Conf. on Advanced Engineering and ICT-Convergence (ICAEIC), Online\nconference, 2021.\n[6] U. Rawat and S. Roy, “An efficient technique to access cryptographic file system over network file system,”\nin Innovations in Computational Intelligence and Computer Vision, vol. 1189, pp. 463, 2020.\n[7] J. LeFevre and C. Maltzahn, “SkyhookDM: Data processing in ceph with programmable storage,”\nUSENIX Login, vol. 45, no. 2, 2020. [Online]. Available:https://par.nsf.gov/biblio/10182302.\n[8] A. Ruhela, S. Xu, K. V . Manian, H. Subramoni and D. K. Panda, “Analyzing and understanding the\nimpact of interconnect performance on HPC, big data, and deep learning applications: A case study\nwith infiniband EDR and HDR,” inIEEE International Parallel and Distributed Processing Symposium\nWorkshops (IPDPSW), New Orleans, United States, pp. 869–878, 2020.\n[9] R. Beltman, S. Knossen, J. Hill and P . Grosso, “Using P4 and RDMA to collect telemetry data,” in\nIEEE/ACM Innovating the Network for Data-Intensive Science (INDIS), Online conference, pp. 1–9, 2020.\n[10] NVIDIA, “NVIDIA DeepOps,” 2021. [Online]. Available:https://github.com/NVIDIA/deepops.\n[11] M. Chadha, J. John and M. Gerndt, “Extending slurm for dynamic resource-aware adaptive batch\nscheduling,” arXiv preprint arXiv:2009.08289, 2020.\n[12] N. Zhou, Y . Georgiou, L. Zhong, H. Zhou and M. Pospieszny, “Container orchestration on HPC systems,”\nin IEEE Int. Conf. on Cloud Computing (CLOUD), Online conference, pp. 34–36, 2020.\n[13] N. Mittal and S. Kumar, “Machine learning computation on multiple GPU’s using CUDA and message\npassing interface,” inIEEE Int. Conf. on Power Energy, Environment and Intelligent Control (PEEIC),\nGreater Noida, India, pp. 18–22, 2019.\n[14] A. Sergeev and M. Del Balso, “Horovod: Fast and easy distributed deep learning in tensorflow,”arXiv\npreprint arXiv:1802.05799, 2018.\n[15] J. Hines, “Stepping up to summit,”Computing in Science & Engineering, vol. 20, no. 2, pp. 78–82, 2018.\n[16] T. P . Morgan, “The clever machinations of Livermore’s Sierra supercomputer,” 2017. [online]. Available:\nhttps://www.nextplatform.com/2017/10/05/clever-machinations-livermores-sierra-supercomputer/.\n[17] H. Fu, J. Liao, J. Yang, L. Wang, Z. Songet al., “The sunway taihulight supercomputer: System and\napplications,”Science China Information Sciences, vol. 59, no. 7, pp. 1–16, 2016.\n[18] X. Liao, L. Xiao, C. Yang, and Y . Lu, “Milkyway-2 supercomputer: System and application,”Frontiers of\nComputer Science, vol. 28, no. 3, pp. 345–356, 2014.\n[19] S. Belov, I. Kadochnikov, V . Korenkov, M. Matveev, D. Podgainyet al.,“High-performance computing\nplatforms for organizing the educational process on the basis of the international school “data science,” in\nCEUR Workshop Proceedings, vol. 2507, pp. 159, 2019.\nCMC, 2022, vol.72, no.3 4395\n[20] S. Nesmachnow and S. Iturriaga, “Cluster-UY : collaborative scientific high performance computing in\nUruguay”, inInt. Conf. on Supercomputing in Mexico, Monterrey, Mexico, pp. 188–202, 2019.\n[21] M. Aldinucci, S. Bagnasco, S. Lusso, P . Pasteris, S. Rabellinoet al.,“OCCAM: A flexible, multi-purpose\nand extendable HPC cluster,” inJournal of Physics: Conference Series, vol. 898, pp. 082039, 2017.\n[22] D. Pera, “Design and performance evaluation of a linux HPC cluster,”TASK QUARTERLY, vol. 22, no.\n2, pp. 113–123, 2018.\n[23] A. Chazapis, J. -T. Acquaviva, A. Bilas, G. Gardikis, C. Kozanitiset al., “EVOLVE: HPC and cloud\nenhanced testbed for extracting value from large-scale diverse data,” inProc. of the ACM Int. Conf. on\nComputing Frontiers, Online conference, pp. 200–205, 2021.\n[24] J. Han, J. S. Shin, J. Kwon and J. Kim, “Cloud-native smartx intelligence cluster for ai-inspired hpc/hpda\nworkloads,” inProc. of the ACM/IEEE Supercomputing Conf., Denver, United States, 2019.\n[25] P . Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir and C. R. Das, “Kube-knots: Resource\nharvesting through dynamic container orchestration in GPU-based datacenters,” inInt. Conf. on Cluster\nComputing (CLUSTER), Albuquerque, United States, pp. 1–13, 2019.\n[26] J. Shao, J. Ma, Y . Li, B. An and D. Cao, “GPU scheduling for short tasks in private cloud,” inInt. Conf.\non Service-Oriented System Engineering (SOSE), San Francisco, United States, pp. 215–2155, 2019.\n[27] Z. Chen, W . Quan, M. Wen, J. Fang, J. Yuet al.,“Deep learning research and development platform:\nCharacterizing and scheduling with qos guarantees on gpu clusters,”IEEE Transactions on Parallel and\nDistributed Systems, vol. 31, no. 1, pp. 34–50, 2019.\n[28] F . Filippini, M. Lattuada, A. Jahani, M. Ciavotta, D. Ardagnaet al., “Hierarchical scheduling in on-\ndemand gpu-as-a-service systems,” inInt. Symposium on Symbolic and Numeric Algorithms for Scientific\nComputing (SYNASC), Online conference, pp. 125–132, 2020.\n[29] T. Habuza, K. Khalil, N. Zaki, F . Alnajjar and M. Gochoo, “Web-based multi-user concurrent job\nscheduling system on the shared computing resource objects,” inInt. Conf. on Innovations in Information\nTechnology (IIT), Online conference, pp. 221–226, 2020.\n[30] Y . Luan, X. Chen, H. Zhao, Z. Yang, and Y . Dai, “Sched2: Scheduling deep learning training via deep\nreinforcement learning,” inGlobal Communications Conf. (GLOBECOM), Waikoloa, United States, pp.\n1–7, 2019.\n[31] S. Wang, O. J. Gonzalez, X. Zhou, T. Williams, B. D. Friedmanet al.,“An efficient and non-intrusive\nGPU scheduling framework for deep learning training systems,” inSC20: Int. Conf. for High Performance\nComputing, Networking, Storage and Analysis, Online conference, pp. 1–13, 2020.\n[32] M. Abdullah, W . Iqbal, F . Bukhari and A. Erradi, “Diminishing returns and deep learning for adaptive\nCPU resource allocation of containers,”IEEE Transactions on Network and Service Management, vol. 17,\nno. 4, pp. 2052–2063, 2020.\n[ 3 3 ] H .Z h a o ,W .C u i ,Q .C h e n ,J .L e n g ,K .Y uet al.,“CODA: Improving resource utilization by slimming and\nco-locating DNN and CPU jobs,” inInt. Conf. on Distributed Computing Systems (ICDCS), Singapore,\nSingapore, pp. 853–863, 2020.\n[34] J. Lin, D. Cui, Z. Peng, Q. Li and J. He, “A two-stage framework for the multi-user multi-data center job\nscheduling and resource allocation,” IEEE Access, vol. 8, pp. 197863–197874, 2020.\n[35] K. Zhang, Z. Yang and T. Ba¸sar, “Multi-agent reinforcement learning: A selective overview of theories and\nalgorithms,” arXiv preprint arXiv:1911.10635, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7960957288742065
    },
    {
      "name": "Reinforcement learning",
      "score": 0.6734167337417603
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.643818199634552
    },
    {
      "name": "Supercomputer",
      "score": 0.5826378464698792
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5555315613746643
    },
    {
      "name": "Distributed computing",
      "score": 0.5185278654098511
    },
    {
      "name": "Server",
      "score": 0.5147495269775391
    },
    {
      "name": "Computer cluster",
      "score": 0.5099688768386841
    },
    {
      "name": "Deep learning",
      "score": 0.4974866211414337
    },
    {
      "name": "Resource allocation",
      "score": 0.46480074524879456
    },
    {
      "name": "Computer architecture",
      "score": 0.3636016845703125
    },
    {
      "name": "Operating system",
      "score": 0.22328659892082214
    },
    {
      "name": "Computer network",
      "score": 0.1373995840549469
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39534123",
      "name": "Gwangju Institute of Science and Technology",
      "country": "KR"
    }
  ],
  "cited_by": 11
}