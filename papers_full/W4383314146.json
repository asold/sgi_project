{
  "title": "ChatGPT sits the DFPH exam: large language model performance and potential to support public health learning",
  "url": "https://openalex.org/W4383314146",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4383324785",
      "name": "Nathan P Davies",
      "affiliations": [
        "Nottingham City Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1532175135",
      "name": "Robert Wilson",
      "affiliations": [
        "Nottingham City Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4383324787",
      "name": "Madeleine S Winder",
      "affiliations": [
        "Nottingham City Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5027953416",
      "name": "Simon J. Tunster",
      "affiliations": [
        "Nottingham City Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2015614132",
      "name": "Kathryn McVicar",
      "affiliations": [
        "Nottingham City Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4383324789",
      "name": "Shivan T Thakrar",
      "affiliations": [
        "Leicester City Council"
      ]
    },
    {
      "id": "https://openalex.org/A2128032715",
      "name": "Joe Williams",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A2111511536",
      "name": "Allan Reid",
      "affiliations": [
        "Nottingham City Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4383324785",
      "name": "Nathan P Davies",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1532175135",
      "name": "Robert Wilson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4383324787",
      "name": "Madeleine S Winder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5027953416",
      "name": "Simon J. Tunster",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2015614132",
      "name": "Kathryn McVicar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4383324789",
      "name": "Shivan T Thakrar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128032715",
      "name": "Joe Williams",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111511536",
      "name": "Allan Reid",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319777976",
    "https://openalex.org/W2550008669",
    "https://openalex.org/W4288057965",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4368360859",
    "https://openalex.org/W4367048218",
    "https://openalex.org/W4378211503",
    "https://openalex.org/W4372047097",
    "https://openalex.org/W2162821268",
    "https://openalex.org/W2042651776",
    "https://openalex.org/W4321351832"
  ],
  "abstract": "Abstract Background Artificial intelligence-based large language models, like ChatGPT, have been rapidly assessed for both risks and potential in health-related assessment and learning. However, their application in public health professional exams have not yet been studied. We evaluated the performance of ChatGPT in part of the Faculty of Public Health’s Diplomat exam (DFPH). Methods ChatGPT was provided with a bank of 119 publicly available DFPH question parts from past papers. Its performance was assessed by two active DFPH examiners. The degree of insight and level of understanding apparently displayed by ChatGPT was also assessed. Results ChatGPT passed 3 of 4 papers, surpassing the current pass rate. It performed best on questions relating to research methods. Its answers had a high floor. Examiners identified ChatGPT answers with 73.6% accuracy and human answers with 28.6% accuracy. ChatGPT provided a mean of 3.6 unique insights per question and appeared to demonstrate a required level of learning on 71.4% of occasions. Conclusions Large language models have rapidly increasing potential as a learning tool in public health education. However, their factual fallibility and the difficulty of distinguishing their responses from that of humans pose potential threats to teaching and learning.",
  "full_text": "ChatGPT sits the DFPH exam: large language model performance and potential to \nsupport public health learning \n \nNathan P Davies, Nottingham Centre for Public Health and Epidemiology, University of Nottingham, Nottingham City Hospital, Huck nall Rd, Nottingham NG5 1PB \nRobert Wilson, NHS England, Seaton House, City Link, London Road, Nottingham NG2 4LA \nMadeleine S Winder, Nottingham Centre for Public Health and Epidemiology, University of Nottingham, Nottingham City Hospital, H ucknall Rd, Nottingham NG5 \n1PB \nSimon J Tunster, Nottingham Centre for Public Health and Epidemiology, University of Nottingham, Nottingham City Hospital, Huck nall Rd, Nottingham NG5 1PB \nKathryn McVicar, Nottingham Centre for Public Health and Epidemiology, University of Nottingham, Nottingham City Hospital, Huck nall Rd, Nottingham NG5 1PB \nShivan T Thakrar, Leicester City Council, Public Health, 115 Charles Street Leicester LE1 1FZ  \nJoe Williams, School of Health and Related Research (ScHARR), The University of Sheffield, 30 Regent St, Sheffield City Centre,  Sheffield S1 4DA \nAllan Reid, NHS England, Seaton House, City Link, London Road, Nottingham NG2 4LA \n \nAbstract \nBackground \nArtificial intelligence-based large language models, like ChatGPT, have been rapidly \nassessed for both risks and potential in health-related assessment and learning. \nHowever, their application in public health professional exams have not yet been \nstudied. We evaluated the performance of ChatGPT in part of the Faculty of Public \nHealth’s Diplomat exam (DFPH). \nMethods \nChatGPT was provided with a bank of 119 publicly available DFPH question parts from \npast papers. Its performance was assessed by two active DFPH examiners. The degree \nof insight and level of understanding apparently displayed by ChatGPT was also \nassessed.  \nResults \nChatGPT passed 3 of 4 papers, surpassing the current pass rate. It performed best on \nquestions relating to research methods. Its answers had a high floor. Examiners \nidentified ChatGPT answers with 73.6% accuracy and human answers with 28.6% \naccuracy. ChatGPT provided a mean of 3.6 unique insights per question and appeared \nto demonstrate a required level of learning on 71.4% of occasions.   \nConclusions \nLarge language models have rapidly increasing potential as a learning tool in public \nhealth education. However, their factual fallibility and the difficulty of distinguishing their \nresponses from that of humans pose potential threats to teaching and learning.    \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nIntroduction \nChatGPT is an artificial intelligence (AI) chatbot that runs on OpenAI’s Generative Pre-\nTrained Transformer (GPT) models.1 It is one of a growing number of publicly available \nlarge language learning models (LLMs) that have been trained on huge volumes of text, \nusing both machine learning and some human supervision, to help it respond to users in \na conversational manner. \nThere have been concerns raised about the potential for LLMs to cause public health \nharm. This includes the possibility that LLMs like ChatGPT risk creating infodemics by \ngenerating vast amounts of plausible-sounding but incorrect information in both the \nresearch and public information spheres 2. Some, including the chief executives of major \nAI companies, warn that general artificial intelligence poses serious public health threats \ncomparable to pandemics and nuclear war, as it has the potential for biological \nweaponisation, generate large-scale misinformation, and to strengthen the power of \ndictatorships.\n3  AI can be considered as a commercial determinant of health; a set of \nprivate sector activities which have a significant impact on health.4 As with other \ntechnologies,5 there may be a conflict between profit generation for AI companies and \npublic health. \nAI and LLMs have generated significant interest in health education. ChatGPT has \nperformed relatively well on US medical6,7 and plastic surgery exams8 although it \nperformed less well on the UK BioMedical Admissions Test9 and the Taiwanese \nPharmacist Licensing Examination.10 Its novel abilities have generated discussions on \nits potential applications for medical teaching and learning.13  \nPublic health exams often differ from biomedical exams. They are less likely to take \nmultiple-choice or purely fact-based formats, requiring application of a broad range of \nconcepts to open-ended scenarios. One such example is the Diplomate exam (DFPH), \nset by the Faculty of Public Health (FPH).14 Passing this exam is mandatory for \nprogressing in public health specialty training in the United Kingdom. The DFPH exam \nis split into Paper 1 and Paper 2, sat sequentially. Paper 1 covers a broad range of \ntopics, including research methods and epidemiology, screening, ethics, health \npromotion, health protection, sociology, leadership and management, health economics, \nhealth informatics, and healthcare public health. \nWe aimed to evaluate the performance of ChatGPT 3.5 in Paper 1 of the DFPH exam, \nincluding whether its answers were distinguishable from human respondents, and to \ninvestigate the level of insight and degree of learning it appeared to display. \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nMethods \nThe 7 most recently available Paper 1s were selected from the Faculty of Public \nHealth’s publicly available question bank (January 2014 – January 2017). Paper 1 \nincorporates 10 questions that require short, medium and long-form responses. It is \ndivided into 5 topic-based sections, each with 2 questions. Papers from pre-2014 were \nexcluded, as they comprise 10-mark essay-style questions. These differ significantly \nfrom the current style of questions, which are always broken down into at least two \nparts.   \nTo generate responses from ChatGPT, each question component was entered, \nformatted by the question text followed by the direct question separated by a new \nline.  For long-form answers, ChatGPT was given a prompt to write in full sentences \nrather than use bullet points. Responses were generated in February 2023 using \nChatGPT version 3.5. Sessions were expunged after each question to avoid biasing.  \nWhere the exam question required an answer “with regards to a particular country” or \n“with regards to a particular public health strategy”, the question was edited to be \nspecific, for example “with regards to a public health obesity strategy”. This was to \nensure the answer was specific to the countries and topics covered by the exam.  \nAll 10 mark questions were excluded and all questions that include an image or require \ngraphical output were also removed, as ChatGPT 3.5 was unable to parse images. Very \nlight editing of the structure of the introduction to ChatGPT responses was required to \nmaintain blinding, because ChatGPT often followed a very similar structure. American \nEnglish was changed to British English. ChatGPT answers are provided in the \nsupplementary material. \nQuestions were independently double-marked by two active DFPH examiners, using the \nDFPH exam moderation process to agree a final mark. These two examiners work as a \npair in the real sittings of this exam. Prior to January 2017, candidates were required to \nscore at least 50% in order to pass a question and could not fail more than two \nindividual questions, so these were the criteria used to judge pass/fail. \nExaminers were provided with a set of blinded answers for four papers with the lowest \nnumbers of excluded questions; January 2017, June 2016, January 2016 and June \n2014. 80% of answers were generated by ChatGPT and 20% of answers were from a \nbank of public health registrars preparing to sit the DFPH exam. Examiners were asked \nto indicate which answers they believed were generated by ChatGPT and which came \nfrom public health registrars.  \nFive public health registrars preparing for the DFPH exam, working in pairs, first \nindependently measured the number of insights ChatGPT offered per answer for the full \n7 exam papers, then came together to moderate scores. This used a modified definition \nof insight based on the work of Kung et al\n7, which must meet the following three criteria: \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \n• Nondefinitional: Does not simply define a term in the input question  \n• Nonobvious: Requires deduction or knowledge external to the question input  \n• Valid: Is in keeping with public health practice or numerically accurate; preserves \ndirectionality  \nAn example is provided in the supplementary material. \nThe same registrars then worked in pairs to judge each question against Bloom’s \nrevised taxonomy of learning15 (BRT) assessing the level of learning ChatGPT \nappeared to be exhibiting in its answers against the level of learning those same \nregistrars judged was required to answer the question appropriately. Training was \nprovided to improve interrater reliability. Registrars assessed the level of learning \nrequired to answer the questions first before assessing the ChatGPT responses to \navoid anchoring bias.\n16   \n \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nResults \nChatGPT performance \nEach of the 7 papers comprised of 10 questions worth 10 marks each, most of which \nwere broken down into component parts. 21 out of 70 possible questions were removed \n(12 out of 40 of those marked). ChatGPT provided 119 individual responses across 7 \nexams. Results are provided in full in the supplementary material. \nChatGPT answers for whole questions scored between 4 - 9.5 out of a possible 10. \nHuman answers ranged from 3.25 to 8. \nChatGPT averaged more than 5 out of 10 for each of four exams that were marked \n(Figure 1). However, it scored under 5 marks for 4 separate questions for the January \n2017 paper, which would have resulted in failing the exam. ChatGPT would have been \nawarded a pass on 3 out of 4 exams. In comparison, recent pass rates for all of those \nwho sat Paper 1 range from 47% to 65%.\n14 ChatGPT achieved a mean of 5.9 marks per \nquestion; the human respondents achieved a mean of 6.47. \nFigure 1: Mean ChatGPT score per exam \n \n \n \n \n \n \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nFigure 2: Mean ChatGPT mark per exam section \n \nChatGPT provided stronger responses on research methods than any other section, \nscoring an average mark of 7.95 in this question area. Its score in each of the other four \nsections were only just above a pass (Figure 2).  \nMarker identification of respondent \nMarkers were able to identify that an answer was from ChatGPT in 39 of 54 instances \n(73.6% accuracy). However, they were only able to identify human answers in 4 out of \n14 instances (28.6% accuracy).  \nUnique insights \nChatGPT averaged 3.6 unique insights per question part. ChatGPT provided the \ngreatest density of insight (around 4 per question part) for research methods, health \ninformation and health organization and management (Figure 3). The single score \nintraclass correlation for markers was 0.654 (95% CI 0.538 – 0.746). \n \n \n \n \n \n \n \nur \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nFigure 3: Mean ChatGPT density of insight per question part by section  \n \n \n \nBloom’s Revised Taxonomy (BRT) \n71.4% of ChatGPT answers were judged to be at the ideal level on BRT and only 6.4% \nwere two or more levels below (Figure 4).   \nFigure 4: ChatGPT answer on BRT compared to ideal level  \n \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nDiscussion \nMain findings of this study \nWe found that ChatGPT would have scored a pass mark in Paper 1 of the DFPH exam \non 3 of 4 occasions. It had a higher floor to its answers than human respondents, never \nscoring below 4 marks, indicating that the textual corpus that it trained on enabled \nreasonable answers on the range of questions posed in DFPH Paper 1. Its scores per \nexam were very consistent, with all between 5 and 7. Much of the strength of its overall \nmark came from the research methods section, in which it scored an overall average of \napproximately 8, which is consistent with OpenAI’s findings that ChatGPT performs well \nin SAT Math and AP Statistics.\n12  It was very difficult for markers to differentiate between \nhuman answers and ChatGPT answers. \nChatGPT was able to generate non-obvious insights for each of the questions that it \nanswered, which could be useful in supporting learning for students and those preparing \nfor public health examinations. Its answers more often than not mimicked the requisite \nlevel of learning that a question required, which provides some evidence for its \nusefulness as a revision tool. For example, LLMs may be able to generate example \nquestions that require a similar level of understanding to real public health exams for \nstudents to practice on. \nHowever, it did provide inaccurate information, such as suggesting that deliberately \ninfecting people with the bacteria that causes tuberculosis could form part of testing the \nefficacy of an intervention. \nWhat is already known on this topic \nLLMs have the potential to support public health work in a number of areas, such as \nsupporting coding and analysis, but also poses a series of threats, such as large-scale \nhallucination of information relating to public health, possible generation of bioweapons \nand potential strengthening of authoritarian regimes. \nChatGPT has variable performance in a range of health and biomedical examination \nscenarios. Some authors have suggested it could form a useful tool for revision and \nlearning for students. \nWhat this study adds \nThis study shows that ChatGPT can generate plausible responses to a range of public \nhealth questions that were close to indistinguishable to answers from human public \nhealth registrars. The hallucination of facts (confidently expressing factually incorrect \nstatements) remains an issue; whereas new versions of LLMs can provide references \nfor their answers, the references themselves are often also hallucinated.\n17 It appears to \ngive greater insight when considering more fact-based questions such as those on \nepidemiology and research methods; however, confident hallucination of facts is also \nlikely to be a greater problem here.  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nThere are implications for professional membership bodies and universities in marking \npublic health exams and essays that may have been partially generated by LLMs, and \nin those supporting those undertaking public health qualifications to understand the \nstrengths and limitations of AI chatbots in education. \nLimitations of this study \nDue to marker availability, we were only able to appraise Paper 1 of the DFPH and were \nnot able to assess Paper 2, which comprises critical appraisal and statistics papers. We \nalso had to remove several questions incompatible with the new style of exam, reducing \nthe pool of answers. Based on test outputs, it is likely that ChatGPT 3.5 would have \nparticularly struggled with long-form critical appraisal questions as it consistently did not \ngo into the detail required, despite specific prompting. It is possible ChatGPT was \ntrained on answer banks similar to those provided by the DFPH. \nWe did not use follow-up prompts, which could have increased the relevance of \nanswers further and supported review of use of ChatGPT as a learning aid. Although \ngenerating statistics on the density of insight for each question provides a broad \noverview of the usefulness of ChatGPT output, qualitative study into how LLMs work in \npractice as a revision tool is likely to be useful. \nOne limitation is that ChatGPT has already progressed to version 4.0, and independent \nmedical researchers\n11 and OpenAI12 have both reported advancements over 4.0 on \ncommon assessment.12 Several other models, such as Google’s Bard, have also \nrecently become available. Rapid assessment of each new iteration of LLMs in public \nhealth education would be required to keep abreast of its changing strengths and \nweaknesses. \nFinally, this study very specifically examined ChatGPT performance in one particular \nexam. We must be wary of drawing broader conclusions on the use of AI in public \nhealth; this is a very specific scenario with lots of available material online. One area \nwhere markers noted that ChatGPT was weaker was on making its answers more \nspecific to the scenario being posed, particularly in more open-ended questions, which \nlikely limited its score in the non-research methods sections. Public health practice is \nvery context-specific to the health needs of the communities being served. \nConclusions \nChatGPT 3.5 performed relatively well on the DFPH Paper 1, particularly on the \nresearch methods sections. Its answers were difficult to distinguish from human \nanswers and it may have utility for public health learning, although its propensity to \nhallucinate facts requires addressing for its full potential to be realised. More broadly, AI \nis largely developed and owned by private actors. Independent research and verification \nof its capabilities for good and for ill will be of utmost importance in the months and \nyears to come.  \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nData availability statement \nThe data underlying this article are available in the article and in its online \nsupplementary material. \nConflicts of interest \nThere are no conflicts of interests to declare.  \nFunding  \nThis work was supported by Health Education England.  \n \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \nReferences \n1.  Intro ducing ChatGPT [Inte rne t]. [cited 20 23 Jun 5]. Available from: \nhttps://open ai.com/blog/chatgpt  \n2.  De Angelis L, Baglivo F, Arzilli G, Privit era GP, Ferr agina P, Tozzi AE, et al . Chat GPT and the ris e of \nlarge language mod els: th e new AI-drive n infodemic threa t in public heal th. F ront  Public Health. \n2023 Apr 25;11:1567. Availa ble from: \nhttps://www.frontie rsin.org/ar ticles/10. 3389/fpubh.2023.1166120/full  \n3.  Centre for A I Safety . Sta temen t on A I Risk [Interne t]. [cite d 2023 Jun 5]. Availabl e from: \nhttps://www.safe.ai/sta temen t-on-ai-ris k \n4.  Kickbusch I, Allen L, Franz C. The commer cial dete rminants of he alth . Lancet Glob Health . 2016 \nDec 1;4(12):e895–6. Available from: \nhttp://www.thela ncet .com/article/S221 4109X16302170/fulltext  \n5.  Davies N, Ferris S. Crypt ocurre ncy and ne w financial instruments : unquan tified pu blic health \nharms. Lancet Public Heal th. 2022;7(8).  \n6.  Gilson A, Safr anek CW, Huang T, Socra te s V, Chi L, Taylor RA, et al . How Does ChatGPT Perform \non the Uni ted S tat es Medical Licensing E xaminati on? The Implica tions of Large La nguage Models \nfor Medical Educati on and Knowledge As sessment. JMIR M ed Educ. 2023;9: e4531 2. Available \nfrom: https ://mededu.jmir.o rg/2023/1/e45312 \n7.  Kung TH, Cheatham M, Med enilla A , Sillo s C, De Leon L, Elepaño C, et al. Performa nce of ChatGPT \non USMLE: Poten tial for AI-assis ted medi cal education using la rge language mod e ls. PLOS Digital \nHealth . 2023 Feb 9;2(2):e0000198-. Avail able from: \nhttps://doi. org/10.1371/journal.pdig .0000198 \n8.  Humar P, Asaad M, B engur FB, Nguyen V. ChatGPT is Equivalent t o First Yea r Plast ic Surgery \nResiden ts: Evaluatio n of ChatGPT on the Plastic Surgery In-Service Ex am. Aes the t Surg J. 2023 \nMay 4  \n9.  Giannos P, Delard as O. Performa nce of ChatGPT on UK Stand ardiz ed Admission T ests: Insigh ts \nFrom the BM AT, TMUA, LN AT, and TSA E xaminati ons. J MIR M ed Educ 2023;9:e47 737 2023 Apr \n26;9(1):e47737. Available from: h ttps ://mededu.jmir .org/2023/1/e47737 \n10.  Wang YM, Shen HW , Chen TJ. Performan ce of ChatGPT on the Pharmacist Licensi ng Examination \nin Taiwan. Journ al of the Chines e Medica l Association . 9900; Available from : \nhttps://journals .lww.com/jcma/Fulltext/ 9900/Performance_of_Chat GPT_on _th e _Pharmacist_Lic\nensing.220.asp x  \n11.  Oh N, Choi GS, Le e WY. Chat GPT goes to the op era ting room: evalu ating GPT-4 pe rformance and \nits poten tial in surgical e ducatio n and tr a ining in the er a of large language mod els . Ann Surg Trea t \nRes. 2023 May;104(5):269–73.  \n12.  OpenA I. GPT-4 Technical Report . 2023 Mar 15 [cited 2023 Jun 5]; Availabl e from: \nhttps://ar xiv.org/abs/2303.08774v3 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint \n13.  Tsang R. Practical Applica tions of ChatGP T in Undergradua te M edical Educati on. . 2023 \nMay;10:23821205231178450. Availabl e from: ht tps://doi.org/101177/23821205231178449 \n14.  The Diplomate (DFPH) and Final Member ship Examinatio n (MFPH) [Interne t]. [cite d 2023 Jun 5]. \nAvailable from: h ttps://www.fph.org.uk/ training-car eers/th e-diplomat e-dfph-and -final-\nmembership-e xaminati on-mfph/ \n15.  Krathwohl DR. A r evision of Bloom’s ta xo nomy: An overview. Theory Pract . 2002; 41(4):212–8.  \n16.  Furnham A, Bo o HC. A lite rat ure r eview of the anchoring effect . J Socio Econ . 2011 Feb \n1;40(1):35–42.  \n17.  Alkaissi H, SI McFarla ne. Artificial hall ucinations in Chat GPT: implications in scien tific writing. \ncureus.com [Int ern et]. 2023 [cited 2023 J un 2]; Available from: \nhttps://www.cureus.com/ar ticles/138667-artificial-hallucinati ons-in-chatgpt-impl ications-in-\nscientific-writing.pdf  \n  \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 6, 2023. ; https://doi.org/10.1101/2023.07.04.23291894doi: medRxiv preprint ",
  "topic": "Public health",
  "concepts": [
    {
      "name": "Public health",
      "score": 0.5388190150260925
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4557364583015442
    },
    {
      "name": "Computer science",
      "score": 0.44574299454689026
    },
    {
      "name": "Psychology",
      "score": 0.4164005517959595
    },
    {
      "name": "Mathematics education",
      "score": 0.40526047348976135
    },
    {
      "name": "Medical education",
      "score": 0.39570119976997375
    },
    {
      "name": "Medicine",
      "score": 0.25709110498428345
    },
    {
      "name": "Nursing",
      "score": 0.1285577118396759
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802921333",
      "name": "Nottingham City Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I142263535",
      "name": "University of Nottingham",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2803074804",
      "name": "Leicester City Council",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I91136226",
      "name": "University of Sheffield",
      "country": "GB"
    }
  ],
  "cited_by": 3
}