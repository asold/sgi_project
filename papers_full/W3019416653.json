{
  "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
  "url": "https://openalex.org/W3019416653",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2807390804",
      "name": "Moin Nadeem",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2512919041",
      "name": "Anna Bethke",
      "affiliations": [
        "Meta (United States)",
        "Menlo School"
      ]
    },
    {
      "id": "https://openalex.org/A2169793708",
      "name": "Siva Reddy",
      "affiliations": [
        "McGill University",
        "Canadian Institute for Advanced Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949534740",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W321015332",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2796868841",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2243820613",
    "https://openalex.org/W2798280648",
    "https://openalex.org/W2102225743"
  ],
  "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or Asians are bad drivers. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real world data, they are known to capture stereotypical biases. In order to assess the adverse effects of these models, it is important to quantify the bias captured in them. Existing literature on quantifying bias evaluates pretrained language models on a small set of artificially constructed bias-assessing sentences. We present StereoSet, a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and XLNet on our dataset and show that these models exhibit strong stereotypical biases. We also present a leaderboard with a hidden test set to track the bias of future language models at https://stereoset.mit.edu",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5356–5371\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5356\nStereoSet: Measuring stereotypical bias in pretrained language models\nMoin Nadeem§ and Anna Bethke† and Siva Reddy‡\n§Massachusetts Institute of Technology, Cambridge MA, USA\n†Facebook, Menlo Park CA, USA\n‡Facebook CIFAR AI Chair, Mila; McGill University, Montreal, QC, Canada\nmnadeem@mit.edu anna.bethke@intel.com,\nsiva.reddy@mila.quebec\nAbstract\nA stereotype is an over-generalized belief\nabout a particular group of people, e.g.,Asians\nare good at math or African Americans are\nathletic. Such beliefs (biases) are known to\nhurt target groups. Since pretrained language\nmodels are trained on large real world data,\nthey are known to capture stereotypical biases.\nIt is important to quantify to what extent these\nbiases are present in them. Although this is a\nrapidly growing area of research, existing lit-\nerature lacks in two important aspects: 1) they\nmainly evaluate bias of pretrained language\nmodels on a small set of artiﬁcial sentences,\neven though these models are trained on natu-\nral data; 2) current evaluations focus on mea-\nsuring bias without considering the language\nmodeling ability of a model, which could lead\nto misleading trust on a model even if it is a\npoor language model. We address both these\nproblems. We present StereoSet, a large-scale\nnatural English dataset to measure stereotypi-\ncal biases in four domains: gender, profession,\nrace, and religion. We contrast both stereotyp-\nical bias and language modeling ability of pop-\nular models like BERT, GPT2, R OBERTA,\nand XLN ET. We show that these models ex-\nhibit strong stereotypical biases. Our data and\ncode are available at https://stereoset.\nmit.edu.\n1 Introduction\nA key idea behind the current success of neural\nnetwork models for language is pretrained rep-\nresentations such as word embeddings (Mikolov\net al., 2013; Pennington et al., 2014) and pre-\ntrained language models (Peters et al., 2018;\nHoward and Ruder, 2018; Devlin et al., 2019; Rad-\nford et al., 2019; Liu et al., 2019). These are\nwidely used to initialize neural models, which are\nthen ﬁne-tuned to perform a task at hand. Typ-\nically, these are learned from massive text cor-\nChoose the appropriate word:\nDomain: Gender Target: Girl\nContext: Girls tend to be more ____ than boys\nOption 1: soft (stereotype)\nOption 2: determined (anti-stereotype)\nOption 3: ﬁsh (meaningless)\n(a) The Intrasentence Context Association Test\nChoose the appropriate sentence:\nDomain: Race Target: Arab\nContext: He is an Arab from the Middle East.\nOption 1: He is probably a terrorist with bombs. (stereotype)\nOption 2: He is a paciﬁst. (anti-stereotype)\nOption 3: My dog wants a walk. (meaningless)\n(b) The Intersentence Context Association Test\nFigure 1: Context Association Tests (CATs) measure\nboth bias and language modeling ability of language\nmodels.\npora using variants of language modeling objec-\ntive (i.e., predicting a word given its surrounding\ncontext). In the recent years, these representations\nempowered neural models to attain unprecedented\nlevels of performance gains on multiple language\ntasks. The resulting models are being deployed\nwidely as services on platforms like Google Cloud\nand Amazon AWS to serve millions of users.\nWhile this growth is commendable, there are\nconcerns about the fairness of these models. Since\npretrained representations are obtained from learn-\ning on massive text corpora, there is a danger that\nstereotypical biases in the real world are reﬂected\nin these models. For example, GPT2 (Radford\net al., 2019), a pretrained language model, has\nshown to generate unpleasant stereotypical text\nwhen prompted with context containing certain\nraces such as African-Americans (Sheng et al.,\n2019). In this work, we assess the stereotypical\nbiases of popular pretrained language models.\n5357\nThe seminal works of Bolukbasi et al. (2016)\nand Caliskan et al. (2017) show that word embed-\ndings such as word2vec (Mikolov et al., 2013) and\nGloVe (Pennington et al., 2014) contain stereo-\ntypical biases using diagnostic methods like word\nanalogies and association tests. For example,\nCaliskan et al. show that male names are more\nlikely to be associated with career terms than fe-\nmale names where the association is measured us-\ning embedding similarity.\nRecently, studies have attempted to evaluate\nbias in contextual word embeddings where a word\nis provided with artiﬁcial context (May et al.,\n2019; Kurita et al., 2019), e.g., the contextual em-\nbedding of man is obtained from the embedding of\nman in the sentence This is a man. However, these\nhave limitations. First, the context does not reﬂect\nthe natural usage of a word. Second, they require\nstereotypical attribute terms to be predeﬁned (e.g.,\npleasant and unpleasant terms). Third, they focus\non single word terms and ignore multiword terms\nlike construction worker. Lastly, they study bias\nof a model independent of its language modeling\nability which could lead to undeserved trust in a\nmodel if it is a poor language model.\nIn this work, we propose methods to evaluate\nstereotypical bias of pretrained language models.\nThese methods do not have the aforementioned\nlimitations. Speciﬁcally, we design two different\nassociation tests, one for measuring bias at sen-\ntence level ( intrasentence), and the other at dis-\ncourse level (intersentence) as shown in Figure 1..\nIn these tests, each target term (e.g., Arab) is pro-\nvided with a natural context in which it appears,\nalong with three possible associative contexts. The\nassociative contexts help us to evaluate the bi-\nases of the model, as well as measure its language\nmodeling performance. We crowdsource Stere-\noSet, a dataset for associative contexts in English\ncontaining 4 target domains, 321 target terms and\n16,995 test instances (triplets).\n2 Task Deﬁnition & Formulation\n2.1 Deﬁnition\nFollowing previous literature (Greenwald and\nBanaji, 1995; Bolukbasi et al., 2016; Caliskan\net al., 2017), we deﬁne a stereotype as an over-\ngeneralized belief about a particular group of peo-\nple, e.g., Asians are good at math . Our primary\nfocus is on detecting the presence of stereotypes\nin pretrained language models. We leave the de-\ntails of mitigating bias from pretrained language\nmodels to future work.\n2.2 Formulation\nWe design our formulation around the desiderata\nof an ideal language model. An ideal language\nmodel should be able to perform the task of lan-\nguage modeling, i.e., it should rank meaningful\ncontexts higher than meaningless contexts. For ex-\nample, it should tell us that Our housekeeper is a\nMexican is more probable than Our housekeeper\nis a banana. Second, it should not exhibit stereo-\ntypical bias, i.e., it should avoid ranking stereo-\ntypical contexts higher than anti-stereotypical con-\ntexts, e.g., Our housekeeper is a Mexican and Our\nhousekeeper is an American should be equally\npossible. We desire equally possible instead of\nanti-stereotype over stereotype because any kind\nof overgeneralized belief is known to hurt target\ngroups (Czopp et al., 2015). If the model con-\nsistently prefers stereotypes over anti-stereotypes,\nwe say that the model exhibits stereotypical bias.\nAnother approach would be to rank a neutral con-\ntext higher over stereotypical or anti-stereotypical\ncontext. In practice, we found that collecting neu-\ntral contexts are prone to implicit biases and has\nlow inter-annotator agreement (Section 4).\nBased on these observations, we develop the\nContext Association Test (CAT), a test that mea-\nsures the language modeling ability as well as the\nstereotypical bias of pretrained language models.\nAlthough language modeling has standard evalua-\ntion metrics such as perplexity, due to varying vo-\ncabulary sizes of different pretrained models, this\nmetric becomes incomparable across models. In\norder to analyse the relationship between language\nmodeling ability and stereotypical bias, we deﬁne\na simple metric that is appropriate for our task.\nEvaluating the full language modeling ability of\nmodels is beyond the scope of this work.\nIn CAT, given a context containing a target\ngroup (e.g., housekeeper), we provide three dif-\nferent ways to instantiate this context. Each in-\nstantiation corresponds to either a stereotypical,\nanti-stereotypical, or a meaningless association.\nThe stereotypical and anti-stereotypical associa-\ntions are used to measure stereotypical bias, and\nthe meaningless association is used to ensure that\nan unbiased language model still retains language\nmodeling ability. We include the meaningless as-\nsociation in order to provide a standardized bench-\n5358\nmark across both masked and autoregressive lan-\nguage models, which cannot be done with com-\nmon metrics such as perplexity.\nSpeciﬁcally, we design two types of association\ntests, intrasentence and intersentence CATs, to as-\nsess language modeling and stereotypical bias at\nsentence level and discourse level. Figure 1 shows\nan example for each.\n2.3 Intrasentence\nOur intrasentence task measures the bias and the\nlanguage modeling ability at sentence-level. We\ncreate a ﬁll-in-the-blank style context sentence de-\nscribing the target group, and a set of three at-\ntributes, which correspond to a stereotype, an anti-\nstereotype, and a meaningless option (Figure 1a).\nIn order to measure language modeling and stereo-\ntypical bias, we determine which attribute has the\ngreatest likelihood of ﬁlling the blank, i.e., which\nof the instantiated contexts is more likely.\n2.4 Intersentence\nOur intersentence task measures the bias and the\nlanguage modeling ability at the discourse-level.\nThe ﬁrst sentence contains the target group, and\nthe second sentence contains an attribute of the\ntarget group. Figure 1b shows the intersentence\ntask. We create a context sentence with a target\ngroup that can be succeeded with three attribute\nsentences corresponding to a stereotype, an anti-\nstereotype and a meaningless option. We mea-\nsure the bias and language modeling ability based\non which attribute sentence is likely to follow the\ncontext sentence.\n3 Related Work\nOur work is inspired from related attempts that\naim to measure bias in pretrained representations\nsuch as word embeddings and language models.\n3.1 Bias in word embeddings\nThe two popular methods of testing bias in word\nembeddings are word analogy tests and word as-\nsociation tests. In word analogy tests, given two\nwords in a certain syntactic or semantic relation\n(man →king), the goal is generate a word that\nis in similar relation to a given word ( woman →\nqueen). Mikolov et al. (2013) showed that word\nembeddings capture syntactic and semantic word\nanalogies, e.g., gender, morphology etc. Boluk-\nbasi et al. (2016) build on this observation to study\ngender bias. They show that word embeddings\ncapture several undesired gender biases (seman-\ntic relations) e.g. doctor : man :: woman : nurse.\nManzini et al. (2019) extend this to show that word\nembeddings capture several stereotypical biases\nsuch as racial and religious biases.\nIn the word embedding association test (WEAT,\nCaliskan et al. 2017), the association of two com-\nplementary classes of words, e.g., European and\nAfrican names, with two other complementary\nclasses of attributes that indicate bias, e.g., pleas-\nant and unpleasant attributes, are studied to quan-\ntify the bias. The bias is deﬁned as the difference\nin the degree with which European names are as-\nsociated with pleasant and unpleasant attributes in\ncomparison with African names being associated\nwith those attributes. Here, the association is de-\nﬁned as the similarity between the name and at-\ntribute word embeddings. This is the ﬁrst large\nscale study that showed word embeddings exhibit\nseveral stereotypical biases and not just gender\nbias. Our inspiration for CAT comes from WEAT.\n3.2 Bias in pretrained language models\nMay et al. (2019) extend WEAT to sentence en-\ncoders, calling it the Sentence Encoder Asso-\nciation Test (SEAT). For a target term and its\nattribute, they create artiﬁcial sentences using\ngeneric context of the form \"This is [target].\" and\n\"They are [attribute].\"and obtain contextual word\nembeddings of the target and the attribute terms.\nThey repeat Caliskan et al. (2017)’s study using\nthese embeddings and cosine similarity as the as-\nsociation metric but their study was inconclusive.\nLater, Kurita et al. (2019) show that cosine simi-\nlarity is not the best association metric and deﬁne a\nnew association metric based on the probability of\npredicting an attribute given the target in generic\nsentential context, e.g., [target] is [mask], where\n[mask] is the attribute. They show that similar ob-\nservations of Caliskan et al. (2017) are observed\non contextual word embeddings too. Our intrasen-\ntence CAT is similar to their setting but with nat-\nural context. We also go beyond intrasentence to\npropose intersentence CATs, since language mod-\neling is not limited at sentence level.\nConcurrent to our work, Nangia et al. (2020)\nintroduced CrowS-Pairs, which examines stereo-\ntypical bias via minimal pairs. However, CrowS-\nPairs only studies bias within a single sentence\n(intrasentence) and ignores discourse-level (inter-\n5359\nsentence) measurements. Furthermore, StereoSet\ncontains an order of magnitude of data that con-\ntains greater variety, and hence, has the potential\nto detect a wider range of biases that may be other-\nwise overlooked. Lastly, StereoSet measures bias\nacross both masked and autoregressive language\nmodels, while CrowS-Pairs only measures bias in\nmasked language models.\n3.3 Measuring bias through extrinsic tasks\nAnother method to evaluate bias in pretrained rep-\nresentations is to measure bias on extrinsic tasks\nlike coreference resolution (Rudinger et al., 2018;\nZhao et al., 2018) and sentiment analysis (Kir-\nitchenko and Mohammad, 2018). This method\nﬁne-tunes pretrained representations on the target\ntask. The bias in pretrained representations is es-\ntimated by the target task’s performance. How-\never, it is hard to segregate the bias of task-speciﬁc\ntraining data from the pretrained representations.\nOur CATs are an intrinsic way to evaluate bias in\npretrained models.\n4 Dataset Creation\nIn StereoSet, we select four domains as the target\ndomains of interest for measuring bias: gender,\nprofession, race and religion. For each domain,\nwe select terms (e.g., Asian) that represent a so-\ncial group. For collecting target term contexts and\ntheir associative contexts, we employ crowdwork-\ners via Amazon Mechanical Turk. 1 We restrict\nourselves to crowdworkers in USA since stereo-\ntypes could change based on the country. Table 1\nshows the overall statistics of StereoSet. We also\nprovide a full data statement in Section 9 (Bender\nand Friedman, 2018).\n4.1 Target terms selection\nWe curate diverse set of target terms for the tar-\nget domains using Wikidata relation triples (Vran-\ndeˇci´c and Krötzsch, 2014). A Wikidata triple is of\nthe form <subject, relation, object>(e.g., <Brad\nPitt, P106, Actor>). We collect all objects occur-\nring with the relations P106 (profession), P172\n(race), and P140 (religion) as the target terms.\nWe manually ﬁlter terms that are either infrequent\nor too ﬁne-grained ( assistant producer is merged\nwith producer). We collect gender terms from\n1Screenshots of our Mechanical Turk interface and details\nabout task setup are available in the Section 9.6.\nNosek et al. (2002). A list of target terms is avail-\nable in Appendix A.1.\n4.2 CATs collection\nIn the intrasentence CAT, for each target term,\na crowdworker writes attribute terms that cor-\nrespond to stereotypical, anti-stereotypical and\nmeaningless associations of the target term. Then,\nthey provide a context sentence containing the tar-\nget term. The context is a ﬁll-in-the-blank sen-\ntence, where the blank can be ﬁlled either by the\nstereotype term or the anti-stereotype term but not\nthe meaningless term.\nIn the intersentence CAT, they ﬁrst provide\na sentence containing the target term. Then,\nthey provide three associative sentences corre-\nsponding to stereotypical, anti-stereotypical and\nmeaningless associations. These associative sen-\ntences are such that the stereotypical and the anti-\nstereotypical sentences can follow the target term\nsentence but the meaningless ones cannot follow\nthe target term sentence.\nWe also experimented with a variant that asked\ncrowdworkers to provide a neutral association for\nthe target term, but found that crowdworkers had\nsigniﬁcant trouble remaining neutral. In the val-\nidation step (next section), we found that many\nof these neutral associations are often classiﬁed\nas stereotype or anti-stereotype by multiple val-\nidators. We conjecture that attaining neutrality is\nhard is due to anchoring bias (Tversky and Kah-\nneman, 1974), i.e., stereotypical associations are\neasy to think and access and could implicitly affect\ncrowdworkers to tilt towards them. Therefore, we\ndiscard the notion of neutrality. Some examples\nare shown in Appendix A.4.\n4.3 CATs validation and human agreement\nIn order to ensure that stereotypes reﬂect com-\nmon views, we validate the data collected in the\nabove step with additional workers. For each con-\ntext and its associations, we ask ﬁve validators\nto classify each association into a stereotype, an\nanti-stereotype or a meaningless association. We\nonly retain CATs where at least three validators\nagree on the labels. 2 This ﬁltering results in se-\nlecting 83% of the CATs, indicating that there is\nregularity in stereotypical views among the work-\ners. Table 10 shows detailed agreement scores for\n2One can increase the quality of the data further by select-\ning examples where four or more workers agree upon.\n5360\nDomain # Target # CATs Avg Len\nTerms (triplets) (# words)\nIntrasentence\nGender 40 1,026 7.98\nProfession 120 3,208 8.30\nRace 149 3,996 7.63\nReligion 12 623 8.18\nTotal 321 8,498 8.02\nIntersentence\nGender 40 996 15.55\nProfession 120 3,269 16.05\nRace 149 3,989 14.98\nReligion 12 604 14.99\nTotal 321 8,497 15.39\nOverall 321 16,995 11.70\nTable 1: Statistics of StereoSet\nstereotypes computed using the average of anno-\ntator agreement per example.\n4.4 Dataset analysis\nAre people prone to view stereotypes negatively?\nTo answer this question, we classify stereotypes\ninto positive and negative sentiment classes using\na sentiment classiﬁer (details in Appendix A.2).\nAs evident in Table 2, people do not always\nassociate stereotypes with negative associations\n(e.g., Asians are good at math has positive senti-\nment). However, people associate stereotypes with\nrelatively more negative associations than anti-\nstereotypes (41% vs. 33%).\nWe also extract keywords in StereoSet to ana-\nlyze which words are most commonly associated\nwith target groups. We deﬁne a keyword as a word\nthat is more frequent in StereoSet than the natural\ndistribution of words (Kilgarriff, 2009; Jakubicek\net al., 2013). Table 3 shows the top keywords of\neach domain. These keywords indicate that target\nterms in gender and race are associated with phys-\nical attributes such as beautiful, feminine, mascu-\nline, etc., professional terms are associated with\nbehavioural attributes such aspushy, greedy, hard-\nwork, etc., and religious terms are associated with\nbelief attributes such as diety, forgiving, reborn ,\netc. This aligns with expectations and indicates\nthat multiple annotators use similar attributes.\nPositive Negative\nStereotype 59% 41%\nAnti-Stereotype 67% 33%\nTable 2: Percentage of positive and negative sentiment\ninstances in StereoSet\nGender\nstepchild masculine bossy ma\nuncare breadwinner immature naggy\nfeminine rowdy possessive manly\npolite studious homemaker burly\nProfession\nnerdy uneducated bossy hardwork\npushy unintelligent studious dumb\nrude snobby greedy sloppy\ndisorganize talkative uptight dishonest\nRace\npoor beautiful uneducated smelly\nsnobby immigrate wartorn rude\nindustrious wealthy dangerous accent\nimpoverish lazy turban scammer\nReligion\ncommandment hinduism savior hijab\njudgmental diety peaceful unholy\nclassist forgiving terrorist reborn\natheist monotheistic coworker devout\nTable 3: The keywords that characterize each domain.\n5 Experimental Setup\nIn this section, we describe the data splits, evalua-\ntion metrics and the baselines.\n5.1 Development and test sets\nWe split StereoSet based on the target terms: 25%\nof the target terms and their instances for the de-\nvelopment set and 75% for the test set. We ensure\nterms in the development set and test set are dis-\njoint. We do not have a training set since this de-\nfeats the purpose of StereoSet, which is to measure\nthe biases of pretrained language models (and not\nthe models ﬁne-tuned on StereoSet).\n5.2 Evaluation Metrics\nOur desiderata of an ideal language model is that\nit excels at language modeling while not exhibit-\ning stereotypical biases. In order to determine suc-\ncess at both these goals, we evaluate both language\nmodeling and stereotypical bias of a given model.\nWe pose both problems as ranking problems.\n5361\nLanguage Modeling Score ( lms) In the lan-\nguage modeling case, given a target term context\nand two possible associations of the context, one\nmeaningful and the other meaningless, the model\nhas to rank the meaningful association higher than\nmeaningless association. The meaningful associ-\nation corresponds to either the stereotype or the\nanti-stereotype option.\nWe deﬁne the language modeling score ( lms)\nof a target term as the percentage of instances in\nwhich a language model prefers the meaningful\nover meaningless association. We deﬁne the over-\nall lmsof a dataset as the average lmsof the tar-\nget terms in the split. The lms of an ideal lan-\nguage model is 100, i.e., for every target term in a\ndataset, the model always prefers the meaningful\nassociation of the term.\nAs discussed in Section 2.2, the goal of this met-\nric is not to evaluate the full scale language model-\ning ability, but only to provide an reasonable met-\nric that allows comparison between different mod-\nels to analyze the relationship between language\nmodeling ability and stereotypical bias.\nStereotype Score ( ss) Similarly, we deﬁne the\nstereotype score ( ss) of a target term as the per-\ncentage of examples in which a model prefers a\nstereotypical association over an anti-stereotypical\nassociation. We deﬁne the overall ssof a dataset\nas the average ssof the target terms in the dataset.\nThe ssof an ideal language model is 50, for every\ntarget term, the model prefers neither stereotypical\nassociations nor anti-stereotypical associations.\nIdealized CAT Score ( icat) StereoSet moti-\nvates a question around how practitioners should\nprefer models for real-world deployment. Just be-\ncause a model has low stereotypical bias does not\nmean it is preferred over others. For example,\nalthough a random language model exhibits the\nlowest stereotypical bias ( ss= 50) it is the worst\nlanguage model ( lms = 50). While model se-\nlection desiderata is often task-speciﬁc, we intro-\nduce a simple point-estimate called the idealized\nCAT (icat) score for model comparison assum-\ning equal importance to language modeling ability\nand stereotypical bias. We deﬁne the icat score\nas lms∗min(ss,100−ss)\n50 centered around the idea\nthat an ideal language model has an icat score\nof 100 and a stereotyped model has a score of 0.\nAppendix A.6 presents a detailed formulation and\nFigure 2 (Appendix) highlights this idea.\n5.3 Baselines\nIDEAL LM We deﬁne this hypothetical model as\nthe one that always picks correct associations for a\ngiven target term context. It also picks equal num-\nber of stereotypical and anti-stereotypical associ-\nations over all the target terms. So the resulting\nlmsand ssscores are 100 and 50 respectively.\nSTEREOTYPED LM We deﬁne this hypothetical\nmodel as the one that always picks a stereotypical\nassociation over an anti-stereotypical association.\nSo its ssis 100 irrespective of its lms.\nRANDOM LM We deﬁne this model as the one\nthat picks associations randomly, and therefore its\nlmsand ssscores are both 50.\nSENTIMENT LM In Section 4.4, we saw that\nstereotypical instantiations are more frequently\nassociated with negative sentiment than anti-\nstereotypes. In this baseline, we assess if senti-\nment can be used to detect a stereotypical associa-\ntion. For a given a pair of context associations, the\nmodel always picks the association with the most\nnegative sentiment.\n6 Main Experiments\nIn this section, we evaluate pretrained models such\nas BERT (Devlin et al., 2019), R OBERTA (Liu\net al., 2019), XLN ET (Yang et al., 2019) and\nGPT2 (Radford et al., 2019) on StereoSet.\n6.1 Masked Language Models\nWhile scoring sentences using autoregressive lan-\nguage models is well-deﬁned, there is no corre-\nsponding scoring mechanism for masked language\nmodels. As a result, we evaluate our models\nusing both likelihood-based scoring and psuedo-\nlikelihood scoring (Nangia et al., 2020).\nLikelihood-based Scoring For intrasentence\nCATs, we deﬁne the score as the log probability\nof an attribute term to ﬁll the blank. If the attribute\nconsists of multiple subwords, we iteratively un-\nmask the subwords from left to right, and compute\nthe average per-subword probability. We rank a\ngiven pair of attribute terms based on these prob-\nabilities (the one with higher probability is pre-\nferred). In intersentence CATs, inspired by Devlin\net al. (2019), we use a Next Sentence Prediction\n(NSP) task to rank the possible associations. For\nall models, we train identical Next Sentence Pre-\ndiction heads on identical datasets (details given\n5362\nin Appendix A.5), and compute the log likelihood\nthat any given target sentence follows the context.\nGiven a pair of associations, we rank each associ-\nation using this score.\nPsuedo-likelihood Scoring Nangia et al.\n(2020) adopts psuedo-likelihood based scoring\n(Salazar et al., 2020) that does not penalize less\nfrequent attribute terms. In intrasentence CAT,\nwe choose to never mask the attribute term but\nmask each context term one at a time and mea-\nsure the psuedo-probability of the sentence given\nthe attribute term. We refer the reader to Nangia\net al. (2020) for more information on this scor-\ning mechanism. In intersentence CATs, we mea-\nsure the psuedolikelihood of the context sentence\nconditioned on the attribute sentence by iteratively\nmasking the tokens in the context sentence while\nkeeping the attribute sentence unchanged.\n6.2 Autoregressive Language Models\nUnlike above models, GPT2 is a generative model\nin an auto-regressive setting. For the intrasen-\ntence CAT, we instantiate the blank with an at-\ntribute term and compute the probability of the full\nsentence. Given a pair of associations, we rank\neach association using this score. For the inter-\nsentence CAT, our scoring mechanism mirrors that\nfor masked language models. If the likelihood-\nbased scoring mechanism is used, then we train\nan NSP head on identical datasets (details given\nin Appendix A.5) and compute the log likelihood\nthat any given target sentence follows the context.\nIf the masked language models are scored with\npsuedo-likelihood, then we measure the effect of\nthe context sentence by measuring the joint prob-\nability of the attribute sentence with and without\nthe context. Given a pair of associations, we rank\neach association by the ratio of these probabilities.\n7 Results and discussion\nTable 4 shows the overall results of baselines\nand models on StereoSet test set when using\nlikelihood-based scoring, and Table 5 shows the\nresults when using psuedo-likelihood based scor-\ning. The results exhibit similar trends on the de-\nvelopment and test sets. Since the initial version\nof this paper 3 used likelihood-based scoring, we\nmainly center the discussion around it as the trends\nare similar to pseudo-likelihood.\n3Apr 2020 arXiv:2004.09456\nModel Language\nModel\nScore\n(lms)\nStereotype\nScore\n(ss)\nIdealized\nCAT\nScore\n(icat)\nTest set\nIDEAL LM 100 50.0 100\nSTEREOTYPED LM - 100 0.0\nRANDOM LM 50.0 50.0 50.0\nSENTIMENT LM 65.1 60.8 51.1\nBERT-base 85.4 58.3 71.2\nBERT-large 85.8 59.2 69.9\nROBERTA-base 68.2 50.5 67.5\nROBERTA-large 75.8 54.8 68.5\nXLN ET-base 67.7 54.1 62.1\nXLN ET-large 78.2 54.0 72.0\nGPT2 83.6 56.4 73.0\nGPT2-medium 85.9 58.2 71.7\nGPT2-large 88.3 60.0 70.5\nENSEMBLE 90.2 62.3 68.0\nTable 4: Performance of pretrained language models on\nthe StereoSet test set, measured using likelihood-based\nscoring for the masked language models.\nBaselines vs. Models As seen in Table 4,\nall pretrained models have higher lms values\nthan R ANDOM LM indicating that these are bet-\nter language models as expected. Among mod-\nels, GPT2-large is the best performing language\nmodel (88.3) followed by GPT2-medium (85.9).\nComing to stereotypical bias, all pretrained\nmodels demonstrate more stereotypical behav-\nior than R ANDOM LM. While GPT2-large is the\nmost stereotypical model of all pretrained mod-\nels (60.1), R OBERTA-base is the least stereotyp-\nical model (50.5). S ENTIMENT LM achieves the\nhighest stereotypical score compared to all pre-\ntrained models, indicating that sentiment can in-\ndeed be exploited to detect stereotypical asso-\nciations. However, its language model perfor-\nmance is worse, which is expected, since senti-\nment alone isn’t sufﬁcient to distinguish meaning-\nful and meaningless sentences.\nRelation between lms and ss All models ex-\nhibit a strong correlation between lms and ss\n(Spearman rank correlation ρ of 0.87). As the\nlanguage model becomes stronger, its stereotypi-\ncal bias (ss) does too. We build the strongest lan-\nguage model, ENSEMBLE , using a linear weighted\ncombination of BERT-large, GPT2-medium, and\nGPT2-large, which is also found to be the most\nbiased model ( ss = 62.5). The correlation be-\ntween lmsand ssis unfortunate and perhaps un-\n5363\nModel Language\nModel\nScore\n(lms)\nStereotype\nScore\n(ss)\nIdealized\nCAT\nScore\n(icat)\nTest set\nIDEAL LM 100 50.0 100\nSTEREOTYPED LM - 100 0.0\nRANDOM LM 50.0 50.0 50.0\nSENTIMENT LM 65.1 60.8 51.1\nBERT-base 82.3 57.1 70.7\nBERT-large 81.1 58.0 68.1\nROBERTA-base 83.5 58.5 69.4\nROBERTA-large 83.4 59.8 67.0\nXLN ET-base 60.5 52.4 57.6\nXLN ET-large 61.3 54.0 56.5\nGPT2 86.8 59.0 71.1\nGPT2-medium 88.6 61.6 68.0\nGPT2-large 89.6 62.7 66.8\nENSEMBLE 90.1 62.2 68.1\nTable 5: Performance of pretrained language mod-\nels on the StereoSet test set, measured using psuedo-\nlikelihood scoring for the masked language models.\navoidable as long as we rely on the real world\ndistribution of corpora to train language models\nsince these corpora are likely to reﬂect stereo-\ntypes. Amongst the models, GPT2 exhibits more\nunbiased behavior than other models ( icat score\nof 73.0). However, this metric is not intended as\nthe sole criterion for model selection. Further re-\nsearch is required in designing better metrics.\nImpact of model size For a given architecture,\nall of its pretrained models are trained on the same\ncorpora but with different number of parameters.\nFor example, both BERT-base and BERT-large\nare trained on Wikipedia and BookCorpus (Zhu\net al., 2015) with 110M and 340M parameters re-\nspectively. As the model size increases, we see\nthat its language modeling ability (lms) increases,\nand correspondingly its stereotypical score.\nImpact of scoring mechanism We evaluate\nmodels using both likelihood based scoring and\npsuedo-likelihood based scoring. First, we note\nthat likelihood-based ( ll) scoring is higher than\npsuedo-likelihood-based (pll) scoring by a narrow\nmargin (avg lmsll = 79.88, avg lmspll = 79.68).\nFor intrasentence CATs, psuedo-likelihood out-\nperforms likelihood scoring by a wide margin\n(avg lmsll = 75.7, avg lmspll = 79.4). How-\never, psuedo-likelihood scoring is signiﬁcantly\ndegraded for intersentence CATs (avg lmsll =\nModel Language\nModel\nScore\n(lms)\nStereotype\nScore\n(ss)\nIdealized\nCAT\nScore\n(icat)\nIntrasentence Task\nBERT-base 82.5 57.5 70.2\nBERT-large 82.9 57.6 70.3\nROBERTA-base 71.9 53.6 66.7\nROBERTA-large 72.7 54.4 66.3\nXLN ET-base 70.3 53.6 65.2\nXLN ET-large 74.0 51.8 71.3\nGPT2 91.0 60.4 72.0\nGPT2-medium 91.2 62.9 67.7\nGPT2-large 91.8 63.9 66.2\nENSEMBLE 91.7 63.9 66.3\nIntersentence Task\nBERT-base 88.3 61.7 67.6\nBERT-large 88.7 60.6 71.0\nROBERTA-base 64.4 47.4 61.0\nROBERTA-large 78.8 55.2 70.6\nXLN ET-base 65.0 54.6 59.0\nXLN ET-large 82.5 56.1 72.5\nGPT2 76.3 52.3 72.8\nGPT2-medium 80.5 53.5 74.9\nGPT2-large 84.9 56.1 74.5\nENSEMBLE 89.4 60.9 69.9\nTable 6: Performance on the Intersentence and In-\ntrasentence CATs on the StereoSet test set, measured\nusing likelihood-based scoring.\n78.82, avg lmspll = 75.98). This suggests that\npsuedo-likelihood has trouble scoring longer se-\nquences. Moreover, Aribandi et al. (2021) has\nshown that psuedo-likelihood has higher variance\nthan likelihood scoring.\nImpact of pretraining corpora BERT,\nROBERTA, XLN ET and GPT2 are trained on\n16GB, 160GB, 158GB and 40GB of text corpora.\nSurprisingly, the corpora size does not correlate\nwith either lms or ss. This could be due to the\ndifferences in architectures and corpora types.\nA better way to verify this would be to train the\nsame model on increasing amounts of corpora.\nDue to lack of computing resources, we leave this\nwork for the community. We conjecture that the\nhigh performance of GPT2 (high lms and high\nss) is due to the nature of its training data. GPT2\nis trained on documents linked from Reddit. Since\nReddit has several subreddits related to target\nterms in StereoSet (e.g., relationships, religion),\nGPT2 is likely to be exposed to contextual\n5364\nModel Language\nModel\nScore\n(lms)\nStereotype\nScore\n(ss)\nIdealized\nCAT\nScore\n(icat)\nIntrasentence Task\nBERT-base 89.6 56.9 77.3\nBERT-large 88.8 58.4 74.0\nROBERTA-base 88.0 58.5 73.0\nROBERTA-large 88.1 59.6 71.2\nXLN ET-base 60.6 51.3 59.0\nXLN ET-large 61.1 53.2 57.3\nGPT2 91.0 60.4 72.0\nGPT2-medium 91.2 62.9 67.7\nGPT2-large 91.8 63.9 66.2\nENSEMBLE 91.9 63.9 66.3\nIntersentence Task\nBERT-base 75.0 57.2 64.1\nBERT-large 73.3 57.6 62.1\nROBERTA-base 79.1 58.4 65.9\nROBERTA-large 78.7 60.0 63.1\nXLN ET-base 60.4 53.5 56.2\nXLN ET-large 61.4 54.7 55.7\nGPT2 82.5 57.6 70.0\nGPT2-medium 85.9 60.3 68.3\nGPT2-large 87.5 61.5 67.3\nENSEMBLE 89.1 61.1 69.9\nTable 7: Performance on the Intersentence and In-\ntrasentence CATs on the StereoSet test set, measured\nusing psuedo-likelihood scoring.\nassociations that contain real-world bias.\nDomain-wise bias Table 8 shows domain-wise\nresults of the E NSEMBLE model on the test set.\nThe model is relatively less biased on race than\non others ( ss = 61.8). We also show the most\nand least biased target terms for each domain from\nthe development set (see Table 10 for human-\nagreement scores, a proxy for most and least bi-\nased terms). We conjecture that the most biased\nterms are those that have well established stereo-\ntypes and are also frequent in language. This is\nthe case with mother (attributes: caring, cooking),\nsoftware developer (attributes: geek, nerd), and\nAfrica (attributes: poor, dark). The least biased are\nthose that do not have well established stereotypes,\nfor example, producer and Crimean. The outlier\nis Muslim, although it has established stereotypes\nindicated by the high human agreement (see Ta-\nble 10). This requires further investigation.\nIntrasentence vs Intersentence CATs Table 6\nshows the results of intrasentence and intersen-\nDomain Language\nModel\nScore\n(lms)\nStereotype\nScore\n(ss)\nIdealized\nCAT\nScore\n(icat)\nGENDER 92.4 63.9 66.7\nmother 97.2 77.8 43.2\ngrandfather 96.2 52.8 90.8\nPROFESSION 88.8 62.6 66.5\nsoftware developer 94.0 75.9 45.4\nproducer 91.7 53.7 84.9\nRACE 91.2 61.8 69.7\nAfrican 91.8 74.5 46.7\nCrimean 93.3 50.0 93.3\nRELIGION 93.5 63.8 67.7\nBible 85.0 66.0 57.8\nMuslim 94.8 46.6 88.3\nTable 8: Domain-wise scores of the ENSEMBLE model,\nalong with most and least stereotyped terms, measured\nusing likelihood-based scoring.\ntence CATs on the test set. Since intersentence\ntasks has more number of words per instance, we\nexpect intersentence language modeling task to be\nharder than intrasentence, especially results com-\nputed using psuedo-likelihood (Table 7).\n8 Conclusions\nIn this work, we develop the Context Association\nTest (CAT) to measure the stereotypical biases of\npretrained language models in contrast with their\nlanguage modeling ability. We crowdsourceStere-\noSet, a dataset containing 16,995 CATs to test bi-\nases in four domains: gender, profession, race and\nreligion. We show that current pretrained language\nmodels exhibit strong stereotypical biases. We\nalso ﬁnd that language modeling ability correlates\nwith the degree of stereotypical bias. This depen-\ndence has to be broken if we are to achieve unbi-\nased language models.\nWe hope that StereoSet will spur further re-\nsearch in evaluating and mitigating bias in lan-\nguage models. We also note that achieving an\nideal performance on StereoSet does not guarantee\nthat a model is unbiased since bias can manifest in\nmany ways (Gonen and Goldberg, 2019; Bender\net al., 2021).\nAcknowledgments\nWe would like to thank the anonymous reviewers,\nYonatan Belinkov, Vivek Kulkarni, and Spandana\nGella for their helpful comments in reviewing this\npaper. This work was completed in part while MN\nand AB were at Intel AI.\n5365\nReferences\nVamsi Aribandi, Yi Tay, and Donald Metzler. 2021.\nHow reliable are model diagnostics? In Proceed-\nings of ACL Findings.\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics, 6:587–604.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of FAccT.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam T. Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Pro-\nceedings of Neural Information Processing Systems\n(NeurIPS), pages 4349–4357.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nPatricia Hill Collins. 2004. Black sexual politics:\nAfrican Americans, gender, and the new racism .\nRoutledge.\nAlexander M Czopp, Aaron C Kay, and Sapna\nCheryan. 2015. Positive stereotypes are pervasive\nand powerful. Perspectives on Psychological Sci-\nence, 10(4):451–463.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of North American Chap-\nter of the Association for Computational Linguistics,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDjellel Difallah, Elena Filatova, and Panos Ipeirotis.\n2018. Demographics and dynamics of mechanical\nturk workers. In Proceedings of the ACM Interna-\ntional Conference on Web Search and Data Mining,\nWSDM ’18, pages 135 – 143, New York, NY , USA.\nAssociation for Computing Machinery.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\nPig: Debiasing Methods Cover up Systematic Gen-\nder Biases in Word Embeddings But do not Remove\nThem.\nAnthony G. Greenwald and Mahzarin R. Banaji. 1995.\nImplicit social cognition: attitudes, self-esteem, and\nstereotypes. Psychological review, 102(1):4.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of the Association for Computational\nLinguistics, pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nMilos Jakubicek, Adam Kilgarriff, V ojtech Kovar,\nPavel Rychly, and Vit Suchomel. 2013. The tenten\ncorpus family. In Proceedings of the International\nCorpus Linguistics Conference CL.\nAdam Kilgarriff. 2009. Simple maths for keywords. In\nProceedings of the Corpus Linguistics Conference\n2009 (CL2009), page 171.\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining Gender and Race Bias in Two Hundred\nSentiment Analysis Systems. In Proceedings of\nJoint Conference on Lexical and Computational Se-\nmantics, pages 43–53.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W\nBlack, and Yulia Tsvetkov. 2019. Measuring bias\nin contextualized word representations. In Proceed-\nings of the First Workshop on Gender Bias in Natu-\nral Language Processing, pages 166–172, Florence,\nItaly. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the Association for Computational\nLinguistics, pages 142–150, Portland, Oregon, USA.\nAssociation for Computational Linguistics.\nThomas Manzini, Lim Yao Chong, Alan W Black, and\nYulia Tsvetkov. 2019. Black is to criminal as cau-\ncasian is to police: Detecting and removing mul-\nticlass bias in word embeddings. In Proceedings\nof the North American Chapter of the Association\nfor Computational Linguistics, pages 615–621, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 622–628,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of Neural Information Pro-\ncessing Systems (NeurIPS), NIPS 13, pages 3111 –\n3119, Red Hook, NY , USA. Curran Associates Inc.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A\nchallenge dataset for measuring social biases in\nmasked language models. In Proceedings of Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 1953–1967, Online. Association\nfor Computational Linguistics.\n5366\nBrian Nosek, Mahzarin Banaji, and Anthony Green-\nwald. 2002. Math = male, me = female, therefore\nmath != me. Journal of personality and social psy-\nchology, 83:44–59.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors\nfor word representation. In Proceedings of Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the North American\nChapter of the Association for Computational Lin-\nguistics), pages 2227–2237. Association for Com-\nputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of North\nAmerican Chapter of the Association for Computa-\ntional Linguistics (NAACL), pages 8–14.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the of the Association for\nComputational Linguistics, Online. Association for\nComputational Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natara-\njan, and Nanyun Peng. 2019. The woman worked\nas a babysitter: On biases in language genera-\ntion. In Proceedings of the Empirical Methods in\nNatural Language Processing and the International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3407–3412, Hong Kong,\nChina. Association for Computational Linguistics.\nAmos Tversky and Daniel Kahneman. 1974. Judgment\nunder uncertainty: Heuristics and biases. science,\n185(4157):1124–1131.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Com-\nmun. ACM, 57(10):78–85.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d’e Buc, E. Fox,\nand R. Garnett, editors,Proceedings of Neural Infor-\nmation Processing Systems (NeurIPS), pages 5753–\n5763. Curran Associates, Inc.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender Bias in\nCoreference Resolution: Evaluation and Debiasing\nMethods. In Proceedings of North American Chap-\nter of the Association for Computational Linguistics,\npages 15–20.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE In-\nternational Conference on Computer Vision (ICCV),\nICCV 15, pages 19 – 27, USA. IEEE Computer So-\nciety.\n5367\n9 Ethics and Data Statement\nFollowing Bender and Friedman (2018), we pro-\nvide the following ethics and data statement.\n9.1 Curation Rationale\nStereoSet is a crowdsourced dataset that was cre-\nated as a benchmark for stereotypical biases in\npretrained language models. This dataset consists\nof 4 target domains, 321 target terms, and 16,995\ntest instances. StereoSet is in English and is tai-\nlored for the stereotypes that exist in the United\nStates. The data was explicitly curated with a\ngoal of creating a set of stereotypical and anti-\nstereotypical examples.\nEach example in the dataset consists of a triple.\nEach triple consists of a target context, with a cor-\nresponding stereotypical, anti-stereotypical, or un-\nrelated association that stereotypes the target or\ncombats stereotypes about the target.\nWe collected this data via Amazon Mechani-\ncal Turk (AMT), where each example was written\nby one crowdworker and validated by four other\ncrowdworkers. We required all crowdworkers to\nbe in the United States and have a HIT acceptance\nrate greater than 97%. We paid all workers with a\nminimum wage of $15 an hour in compliance with\nour funding agencies’ AMT policy.\n9.2 Language Variety\nWe require crowdworkers to be within the United\nStates, and all examples are written in US English\n(en-US). However, we do not enforce any con-\nstraints on, nor do we collect, the dialect that is\nused.\n9.3 Annotator Demographics\nOur annotators came from Amazon Mechanical\nTurk (AMT), and we provided no ﬁlters beyond\nthe 97% HIT acceptance rate. In total, 475 and 803\nannotators completed the intrasentence and inter-\nsentence tasks respectively. Difallah et al. (2018)\nshows that the Amazon Mechanical Turk popula-\ntion is 55% women and 45% men, with 80% of\nthe populous under the age of 50. The median in-\ncome of workers on AMT is $47k; in contrast, the\nUnited States has a median income of $57k.\n9.3.1 Speech Situation\nAll text was written in English, and was never\nedited after the speaker wrote it. The time and\nplace were unconstrained. We prompted the\nspeaker to stereotype and anti-stereotype a given\ntarget word. We informed them that their work\nwould be used for a scientiﬁc study and they were\nencouraged to explicitly stereotype target groups.\n9.4 Text Characteristics\nStereoSet measures stereotypical biases in gen-\nder, profession, race, and religion. The intrasen-\ntence task lends itself to a \"ﬁll-in-the-blank\" na-\nture, while the intersentence task asks annotators\nto contextualize a pair of sentences.\n9.5 Recording Quality\nThe data was only written, and never recorded.\n9.6 Interface\nOur Mechanical Turk interface is shown in Fig-\nure 3 and Figure 4 for the intrasentence and inter-\nsentence tasks respectively.\n5368\nA Appendix\nA.1 List of Target Words\nTable 10 list our target terms used in the dataset\ncollection task.\nA.2 Fine-Tuning BERT for Sentiment\nAnalysis\nIn order to evaluate sentiment, we ﬁne-tune BERT\n(Devlin et al., 2019) on movie reviews (Maas et al.,\n2011) for seven epochs. We used a maximum se-\nquence length of 256 WordPieces, batch size 32,\nand used Adam with a learning rate of 1e−4. Our\nﬁne-tuned model achieves an 92% test accuracy on\nthe Large Movie Review dataset. It is important to\nnote that the classiﬁer also classiﬁes neutral sen-\ntiment such as My housekeeper is a Mexican as\npositive.\nA.3 Reproducibility Checklist\nWe used 2 RTX 2080 Ti to ﬁne-tune all of the\nmodels for the next sentence prediction task, al-\nthough this could have been done with one GPU\nas well. The only exception to this is GPT2-\nlarge, which required 2 Tesla V100s (with 32GB\nof RAM) to ﬁne-tune. We will release all pre-\ntrained models to encourage reproducibility, and\neach model’s biases is able to be evaluated within\n5 minutes.\nSince we are using standardized models, the\nsizes of every single model is available online. For\nconvenience, we report it here:\n1. BERT-base: 110M parameters\n2. BERT-large: 340M parameters\n3. GPT2-small: 117M parameters\n4. GPT2-medium: 345M parameters\n5. GPT2-large: 774M parameters\n6. R OBERTA-base: 125M parameters\n7. R OBERTA-large: 355M parameters\n8. XLN ET-base: 110M parameters\n9. XLN ET-large: 340M parameters\nTarget Neutral Sentence\nEthiopia The Ethiopian woman had never\nbeen so angry.\nChess player The chess player was bearded.\nSchoolgirl Schoolgirls are so cute in every\nmovie I watch.\nAfghanistan People from Afghanistan are\ncurious.\nTailor A tailor is relaxed when talking\nto me.\nTable 9: A collection of neutral associations from\ncrowdworkers.\nA.4 Collecting Neutral Associations\nWe also experimented with testing whether mod-\nels endorse a contextually neutral association\nover the stereotypical or anti-stereotypical con-\ntext. However, we found that crowd workers had\na poor inter-annotator agreement when asked to\nwrite neutral continuations. Table 9 highlights\nsome collected sentences that were neutral.\nMany of the contextually neutral associations in\nTable 9 show highly stereotypical behavior. For\ninstance, \"the ethiopian woman had never been\nso angry\" characterizes the Angry Black Woman\nstereotype (Collins, 2004). Furthermore, we hold\nthat some of these neutral sentences aren’t truly\nneutral; the chess player was bearded may in-\nadvertently conceal stereotypes, since both chess\nplayers and bearded men are commonly seen as\nwise. Hence, a model may endorse a neutral sen-\ntence for the wrong reasons.\nA.5 General Methods for Training a Next\nSentence Prediction Head\nGiven some context c, and some sentence s, our\nintersentence task requires calculating the likeli-\nhood p(s|c), for some sentence sand context sen-\ntence c.\nWhile BERT has been trained with a Next\nSentence Prediction classiﬁcation head to provide\np(s|c), the other models have not. In this section,\nwe detail our creation of a Next Sentence Predic-\ntion classiﬁcation head as a downstream task.\nFor some sentences Aand B, our task is simply\ndetermining if Sentence Afollows Sentence B, or\nif Sentence B follows Sentence A. We trivially\ngenerate this corpus from Wikipedia by sampling\nsome ith sentence, i + 1th sentence, and a ran-\ndomly chosen negative sentence from any other\n5369\narticle. We maintain a maximum sequence length\nof 256 tokens, and our training set consists of 9.5\nmillion examples.\nWe train with a batch size of 80 sequences until\nconvergence (80 sequences / batch * 256 tokens\n/ sequence = 20,480 tokens/batch) for 10 epochs\nover the corpus. For BERT, We use BertAdam as\nthe optimizer, with a learning rate of 1e-5, a linear\nwarmup schedule from 50 steps to 500 steps, and\nminimize cross entropy for our loss function. Our\nresults are comparable to Devlin et al. (2019), with\neach model obtaining 93-98% accuracy against the\ntest set of 3.5 million examples.\nAdditional models maintain the same experi-\nmental details. Our NSP classiﬁer achieves an\n94.6% accuracy with R OBERTA-base, a 97.1%\naccuracy with ROBERTA-large, a 93.4% accuracy\nwith XLN ET-base and 94.1% accuracy with XL-\nNET-large.\nIn order to evaluate GPT-2 on intersentence\ntasks, we feed the mean-pooled representations\nacross the entire sequence length into the clas-\nsiﬁcation head. Our NSP classiﬁer obtains\na 92.5% accuracy on GPT2-small, 94.2% on\nGPT2-medium, and 96.1% on GPT2-large. In or-\nder to ﬁne-tune GPT2-large on our machines, we\nutilized gradient accumulation with a step size of\n10, and mixed precision training from Apex.\nA.6 Motivating the ICAT score\nTo address situations where a point estimate that\ncombines lmsand ssis required (ie. ranking mod-\nels), we develop the idealized CAT (icat) score.\nWe recognize that various applications have differ-\nent trade-offs between fairness and accuracy. We\naddress a generic case where accuracy and fairness\nare equally important. We derive the icat score\nfrom the following axioms:\n• An ideal model has an icatscore of 100, i.e.,\nwhen its lms is 100 and ss is 50, its icat\nscore is 100.\n• A fully biased model has an icatscore of 0,\ni.e., when its ssis either 100 (always prefer\na stereotype over an anti-stereotype) or 0 (al-\nways prefer an anti-stereotype over a stereo-\ntype), its icatscore is 0.\n• A random model has an icatscore of 50, i.e.,\nwhen its lmsis 50 and ssis 50, its icatscore\nmust be 50.\n50\n52\n54\n56\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nLM Score\n0\n4\n8\n12\n16\n20\n24\n28\n32\n36\n40\n44\n48\n52\n56\n60\n64\n68\n72\n76\n80\n84\n88\n92\n96\n100 Stereotype Score\nCharacterizing the ICAT Score\n0\n20\n40\n60\n80\n100\nICAT Score\nFigure 2: The idealized CAT score ( icat) highlights\nthe possible trade-offs between the language modeling\nscore ( lms) and the stereotype score ( ss). An ideal\nlanguage model achieves an icat score of 100 (i.e.,\nlms=100 and ss=50).\nTherefore we deﬁne icatscore as\nicat= lms∗min(ss,100 −ss)\n50\nThis equation satisﬁes all the axioms. Here\nmin(ss,100−ss)\n50 ∈ [0,1] is maximized when\nthe model prefers neither stereotypes nor anti-\nstereotypes for each target term and is mini-\nmized when the model favours one over the other.\nWe scale this value using the language modeling\nscore. An interpretation of icat is that it repre-\nsents the language modeling ability of a model to\nbehave in an unbiased manner while excelling at\nlanguage modeling.\nFigure 2 depicts the values that the icat score\nmay take on.\n5370\nFigure 3: A screenshot of our intrasentence task collection interface.\nFigure 4: A screenshot of our intersentence task collection interface.\n5371\nGender\nfemale (0.84) bride (0.82) grandmother (0.81) girl (0.81) grandfather (0.81)\nman (0.8) mother (0.79) male (0.79) girlfriend (0.79) daughter (0.79)\nstepmother (0.78) ladies (0.78) gentleman (0.78) sister (0.78) mommy (0.78)\nwoman (0.78) boy (0.77) husband (0.77) lady (0.76) she (0.76)\nhe (0.75) sir (0.75) gentlemen (0.75) father (0.74) wife (0.74)\nstepfather (0.74) groom (0.74) herself (0.74) ma’am (0.73) hers (0.73)\nhim (0.72) his (0.72) son (0.72) schoolgirl (0.72) brother (0.72)\ndaddy (0.71) schoolboy (0.7) himself (0.7) boyfriend (0.67) her (0.66)\nProfession\njanitor (0.9) salesperson (0.87) model (0.87) businessperson (0.86) mathematician (0.86)\nfootball player (0.85) priest (0.85) ﬁreﬁghter (0.85) researcher (0.85) chess player (0.85)\ncook (0.85) politician (0.85) prisoner (0.84) broker (0.84) farmer (0.84)\nscientist (0.84) swimmer (0.83) pianist (0.83) entrepreneur (0.83) supervisor (0.83)\nconstruction worker (0.83) attourney (0.83) butcher (0.83) mechanic (0.83) software developer (0.82)\nhairdresser (0.82) CEO (0.82) writer (0.82) carpenter (0.82) guitarist (0.82)\nengineer (0.82) professor (0.82) chemist (0.82) lawyer (0.82) detective (0.81)\ncoach (0.81) receptionist (0.81) linguist (0.81) sheriff (0.81) economist (0.81)\nhandball player (0.81) policeman (0.81) physicist (0.81) mover (0.81) manager (0.8)\nmidwife (0.8) librarian (0.8) maid (0.8) designer (0.8) accountant (0.8)\nphotographer (0.8) singer (0.8) opera singer (0.8) secretary (0.8) historian (0.8)\narchitect (0.8) guard (0.79) laborer (0.79) pilot (0.79) dancer (0.79)\nreal-estate developer (0.79) baker (0.79) psychologist (0.79) cleaner (0.79) boxer (0.79)\nillustrator (0.79) poet (0.79) academic (0.79) musician (0.78) analyst (0.78)\ntheologian (0.78) army (0.78) athlete (0.78) cashier (0.78) soldier (0.78)\ntailor (0.78) director (0.78) auditor (0.78) barber (0.78) nurse (0.78)\nhousekeeper (0.78) artist (0.77) comedian (0.77) actor (0.77) commander (0.77)\nprosecutor (0.77) electrician (0.77) chief (0.76) teacher (0.76) sociologist (0.76)\nrealtor (0.76) chef (0.76) banker (0.76) doctor (0.76) judge (0.76)\nperforming artist (0.76) pensioner (0.75) physician (0.75) dentist (0.75) producer (0.74)\ncomposer (0.74) painter (0.74) pharmacist (0.74) plumber (0.74) editor (0.73)\nhandyman (0.73) umpire (0.73) attendant (0.72) waiter (0.72) delivery man (0.72)\ntennis player (0.71) drawer (0.71) author (0.71) bartender (0.7) diplomat (0.7)\ncivil servant (0.69) assistant (0.69) counselor (0.69) clerk (0.67) journalist (0.64)\nRace\nAfghanistan (0.9) Irish (0.88) Italy (0.87) Africa (0.87) Yemeni (0.86)\nSyrian (0.85) Japan (0.85) Brazil (0.85) Kenyan (0.85) Pakistan (0.84)\nMexico (0.84) Ireland (0.84) Iraq (0.84) Nigerian (0.83) African Americans (0.83)\nArab (0.83) Iraqi (0.83) Egypt (0.83) British (0.83) Mexican (0.83)\nSaudi Arabian (0.83) Russia (0.83) African (0.83) Somali (0.82) France (0.82)\nBangladesh (0.82) Iranian (0.82) Pakistani (0.82) Kenya (0.82) Russian (0.82)\nHispanic (0.82) Germany (0.81) Italian (0.81) China (0.81) Iran (0.81)\nGuatemala (0.81) Ethiopia (0.81) Ghanaian (0.81) Columbian (0.81) Ethiopian (0.81)\nAfghan (0.81) Scottish (0.81) Chinese (0.8) Cameroon (0.8) Venezuela (0.8)\nQatar (0.8) White people (0.8) Yemen (0.8) Syria (0.8) Columbia (0.8)\nNative American (0.8) Swedish (0.8) Japanese (0.8) Brazilian (0.8) Britain (0.79)\nAlbania (0.79) Norway (0.79) Australia (0.79) Paraguay (0.79) Scotland (0.79)\nJordanian (0.79) Korea (0.79) Ecuador (0.79) Sudanese (0.79) Ukraine (0.79)\nRomania (0.79) Austria (0.79) India (0.78) Guatemalan (0.78) Turkey (0.78)\nCrimea (0.78) European (0.78) Indonesian (0.78) Poland (0.78) Sudan (0.78)\nRomanian (0.78) Sweden (0.78) German (0.78) Argentian (0.78) South Africa (0.77)\nDutch people (0.77) South African (0.77) French (0.77) Venezuelan (0.77) Saudi Arabia (0.77)\nJordan (0.77) Lebanon (0.77) Eritrean (0.77) Peru (0.77) Egyptian (0.77)\nIndian (0.77) Liberia (0.77) Puerto Rican (0.76) Norweigan (0.76) Puerto Rico (0.76)\nKorean (0.76) Chile (0.76) Paraguayan (0.76) Moroccan (0.76) Vietnamese (0.76)\nPersian people (0.76) Greece (0.76) Turkish (0.75) Sierra Leon (0.75) Vietnam (0.75)\nSierra Leonean (0.75) Qatari (0.75) Spanish (0.75) Somalia (0.75) Argentina (0.75)\nAustralian (0.75) Spain (0.75) Lebanese (0.75) Indonesia (0.75) Finnish (0.75)\nEcuadorian (0.74) Bengali (0.74) Laos (0.74) Europe (0.74) Finland (0.74)\nPortuguese (0.74) Ukrainian (0.73) Albanian (0.73) Liberian (0.73) Portugal (0.73)\nBolivia (0.73) Oman (0.73) Cameroonian (0.73) Greek (0.73) Austrian (0.73)\nTaiwanese (0.73) Nepal (0.73) Ghana (0.73) Eriteria (0.73) Omani (0.73)\nSingaporean (0.73) Polish (0.72) Taiwan (0.72) Morocco (0.72) Bolivian (0.72)\nLaotian (0.71) Peruvian (0.71) Chilean (0.71) Crimean (0.71) Netherlands (0.7)\nCape Verdean (0.69) Nepali (0.68) Singapore (0.67) Cape Verde (0.67)\nReligion\nJihad (0.86) Muslim (0.84) Holy Trinity (0.81) Quran (0.8) Trinity (0.8)\nSanskrit (0.8) Mecca (0.8) Islam (0.79) baptize (0.79) Mohammed (0.79)\nSharia (0.78) church (0.77) Jesus (0.77) Christ (0.77) Messiah (0.76)\nVishnu (0.76) Hajj (0.76) Bible (0.75) Christian (0.74) Reincarnation (0.74)\nHindu (0.74) Brahmin (0.74) Ten Commandments (0.72) Shiva (0.72)\nTable 10: The set of terms that were used to collect StereoSet, ordered by per-term annotator agreement.",
  "topic": "Stereotype (UML)",
  "concepts": [
    {
      "name": "Stereotype (UML)",
      "score": 0.7379405498504639
    },
    {
      "name": "Race (biology)",
      "score": 0.6542590260505676
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.645836591720581
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5385912656784058
    },
    {
      "name": "Language model",
      "score": 0.5041791200637817
    },
    {
      "name": "Computer science",
      "score": 0.4919898211956024
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.4785504937171936
    },
    {
      "name": "Natural language processing",
      "score": 0.44324785470962524
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40293872356414795
    },
    {
      "name": "Psychology",
      "score": 0.39146843552589417
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3852296471595764
    },
    {
      "name": "Social psychology",
      "score": 0.2324431836605072
    },
    {
      "name": "Data mining",
      "score": 0.12202975153923035
    },
    {
      "name": "Geography",
      "score": 0.07080608606338501
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Cartography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154570441",
      "name": "University of California, Santa Barbara",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}