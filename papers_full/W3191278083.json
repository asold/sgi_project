{
  "title": "Vision-Language Transformer and Query Generation for Referring Segmentation",
  "url": "https://openalex.org/W3191278083",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5036631624",
      "name": "Henghui Ding",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100353160",
      "name": "Chang Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020106541",
      "name": "Suchen Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5085533260",
      "name": "Xudong Jiang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2605127024",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3093025045",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W3108748824",
    "https://openalex.org/W2964284374",
    "https://openalex.org/W3035097537",
    "https://openalex.org/W3034325957",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W2798556392",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3108925098",
    "https://openalex.org/W2876852810",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W2894964039",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W2798791840",
    "https://openalex.org/W3034692043",
    "https://openalex.org/W3202576436",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2302548814",
    "https://openalex.org/W2948080074",
    "https://openalex.org/W2989684653",
    "https://openalex.org/W2558535589",
    "https://openalex.org/W3202427362",
    "https://openalex.org/W2963091558"
  ],
  "abstract": "In this work, we address the challenging task of referring segmentation. The query expression in referring segmentation typically indicates the target object by describing its relationship with others. Therefore, to find the target one among all instances in the image, the model must have a holistic understanding of the whole image. To achieve this, we reformulate referring segmentation as a direct attention problem: finding the region in the image where the query language expression is most attended to. We introduce transformer and multi-head attention to build a network with an encoder-decoder attention mechanism architecture that \"queries\" the given image with the language expression. Furthermore, we propose a Query Generation Module, which produces multiple sets of queries with different attention weights that represent the diversified comprehensions of the language expression from different aspects. At the same time, to find the best way from these diversified comprehensions based on visual clues, we further propose a Query Balance Module to adaptively select the output features of these queries for a better mask generation. Without bells and whistles, our approach is light-weight and achieves new state-of-the-art performance consistently on three referring segmentation datasets, RefCOCO, RefCOCO+, and G-Ref. Our code is available at https://github.com/henghuiding/Vision-Language-Transformer.",
  "full_text": "Vision-Language Transformer and Query Generation for Referring\nSegmentation\nHenghui Ding* Chang Liu* Suchen Wang Xudong Jiang\nNanyang Technological University, Singapore\n{ding0093, liuc0058, wang.sc, exdjiang}@ntu.edu.sg\nAbstract\nIn this work, we address the challenging task of referring\nsegmentation. The query expression in referring segmen-\ntation typically indicates the target object by describing\nits relationship with others. Therefore, to ﬁnd the target\none among all instances in the image, the model must\nhave a holistic understanding of the whole image. To\nachieve this, we reformulate referring segmentation as a\ndirect attention problem: ﬁnding the region in the image\nwhere the query language expression is most attended\nto. We introduce transformer and multi-head attention to\nbuild a network with an encoder-decoder attention mech-\nanism architecture that “queries” the given image with\nthe language expression. Furthermore, we propose a\nQuery Generation Module, which produces multiple sets\nof queries with different attention weights that represent\nthe diversiﬁed comprehensions of the language expression\nfrom different aspects. At the same time, to ﬁnd the best\nway from these diversiﬁed comprehensions based on visual\nclues, we further propose a Query Balance Module to\nadaptively select the output features of these queries for\na better mask generation. Without bells and whistles, our\napproach is light-weight and achieves new state-of-the-\nart performance consistently on three referring segmen-\ntation datasets, RefCOCO, RefCOCO+, and G-Ref. Our\ncode is available at https://github.com/henghuiding/Vision-\nLanguage-Transformer.\n1. Introduction\nReferring segmentation targets to generate segmentation\nmask for the target object referred by a given query expres-\nsion in natural language [10, 16, 15, 3]. As the referring\nsegmentation involves both natural language processing\nand computer vision, it is considered as one of the most\nfundamental and challenging multi-modal tasks. With\nthe recent success of learning methods, a lot of deep-\n*Equal contribution\nDecode\n\"Small elephant on the left\"\nVision-Guided Attention\n0.6\n0.7\n0.3\nQ1 \"SMALL elephant on the LEFT\"\nQ2 \"SMALL ELEPHANT on the left\"\nQ3 \"small ELEPHANT on the LEFT\"\nQuery\nQuery Vectors\nResp. 1\nResp. 2\nResp. 3\nMask\nInput:\n... ... ...\nResponse & Balance\nFigure 1. Our method detects multiple emphasis or understanding\nways for one language expression, and produces a query vector\nfor each of them. We use each vector to “query” the image,\ngenerating a response to each query. Then the network selectively\naggregates these responses, in which queries that provide better\ncomprehensions are spotlighted.\nlearning-based works are proposed in this area and have\nachieved remarkable performance. However, there are still\nmany challenges left in this task. The objects in images\nof referring segmentation are correlated in a complicated\nmanner while the query expression frequently indicates the\ntarget object by describing the relationships with others,\nwhich requires a holistic understanding on the image and\nlanguage expression. Another challenge is caused by\nthe varieties of objects/images as well as the unrestricted\nexpression of languages, which brings a high degree of\nrandomness.\nFirstly, to address the challenge of complicated corre-\nlations in the given image and language, we explore to\nenhance the holistic understanding of multi-modal features\nby building the network with global operations, in which\ndirect interactions are modeled among all elements ( e.g.,\npixel-pixel, word-word, pixel-word). Currently, the Fully\nConvolutional Network (FCN)-like framework [17, 5, 6, 4]\nis widely used in referring segmentation methods [10, 22].\nThey usually perform convolution operations on the fused\n(e.g., concatenated) vision-language features to generate\nthe target mask. However, the long-range dependencies\nmodeling in regular convolution operation is indirect, as\n1\narXiv:2108.05565v1  [cs.CV]  12 Aug 2021\nits large receptive ﬁeld is typically achieved by stacking\nsmall-kernel convolutions. This oblique process brings in-\nefﬁciencies to information interactions among pixels/words\nin a distance [27], thus is undesirable for referring seg-\nmentation models to understand the global context of the\nimage [28]. In recent years, the attention mechanism\nis gaining respectable popularity in the computer vision\ncommunity for its advantage in building direct interaction\namong all elements, which greatly helps the model in\ncapturing the global semantic information. Some previous\nreferring segmentation works also use attention to alleviate\nthe long-range dependency issues [28, 11, 24]. However,\nmost of them only utilize the attention mechanism as\nauxiliary modules based on the FCN-like pipeline, which\nlimits their ability to model the global context. In this\nwork, we reformulate the referring segmentation problem\nin terms of attention and reconstruct the current FCN-like\nframework with Transformer [25]. We generate a set of\nquery vectors from language features using vision-guided\nattention, and use these vectors to “query” the given image\nand generate the segmentation mask from the responses, as\nshown in Fig. 1. This attention-based framework enables us\nto implement global operation among multi-modal features\nin every stage of computation, making the network better\nat modeling the global context of both vision and language\ninformation.\nSecondly, to deal with the randomness caused by the\nvarieties of objects/images and the unconstrained expres-\nsion of languages, we propose to comprehend the language\nexpression in different ways incorporating with vision fea-\ntures. In many previous referring segmentation methods,\nsuch as [19, 29], the language self-attention is often used to\nextract the most informative part and emphasized word(s)\nin the language expression. For these methods, their\nlinguistic understanding is derived alone from the language\nexpression itself without interacting with the image, so that\nthey cannot distinguish which emphasis is more suitable\nand effective that better ﬁt a particular image. Thus their\ndetected emphases might be inaccurate or inefﬁcient. On\nthe other hand, in most previous vision-transformer works,\nqueries for the transformer decoders are usually a set of\nﬁxed learned vectors, each of which is used to predict\none object. Experiments show that each query vector has\nits own operating modes and specializes in certain kinds\nof objects [1]. In these works with ﬁxed queries, there\nmust imply a hypothesis that objects in the input image are\ndistributed under some statistic rules, which does not match\nthe randomness of referring segmentation. To address these\nissues, we propose a Query Generation Module (QGM)\nto produce multiple different query vectors based on the\nlanguage, and with the aid of vision features. Each vector\ncomprehends the language expression in its own way. With\nthe proposed QGM, we improve the diversity of ways to\nunderstanding the image and query language, enhancing the\nnetwork’s robustness in dealing with highly random inputs.\nAt the same time, to ensure the generated query vectors are\nvalid and ﬁnd the more suitable comprehension ways to the\nimage and language, we further propose a Query Balance\nModule to adaptively select the output features of these\nqueries for a better mask generation.\nOur approach builds deep interactions between language\nand vision features at different levels, greatly enhancing\nthe fusion and utilization of multi-modal features. Besides,\nthe proposed module is lightweight and its parameter size\nis roughly equivalent to seven convolution layers. In\nsummary, our main contributions are listed as follows:\n• We design a Vision-Language Transformer (VLT)\nmethod to build deep interactions among multi-modal\ninformation and enhance the holistic understanding to\nvision-language features.\n• We propose a Query Generation Module that un-\nderstands the language from different comprehension\nways, and a Query Balance Module to focus on the\nsuitable ways.\n• The proposed method achieves new state-of-the-art on\nmultiple datasets consistently, especially on hard and\ncomplex datasets.\n2. Related Works\n2.1. Referring Segmentation\nThe aim of the referring segmentation is to ﬁnd the target\nobject in an image given an natural language expression\ndescribing its properties. The task is ﬁrst proposed by Hu\net al. in [10], in which a set of fused features are extracted\nby concatenating the linguistic features extracted by LSTM\nand vision features extracted by CNN. Then, a fully con-\nvolutional network (FCN) is applied on the fused feature,\nproving the feasibility of this problem. In [16], in order to\nmake use of each word in the referring sentence, Liu et al.\nproposed a recurrent network based on multimodal LSTM\n(mLSTM). The framework model each word in every recur-\nrent stage, so that the language feature is better fused with\nvision features along all the forward process. In [29], Yu et\nal. proposed a two-stage method which ﬁrst extract multiple\ninstances using an instance segmentation netwrok Mask R-\nCNN [8] then utilizes language features to select the target\nfrom the extracted instances. Luo et al . [19] proposed a\nframework which jointly learns to solve two tasks: referring\nexpression comprehension (REC) [20, 9] and segmenta-\ntion (RES), achieving remarkable performance. Besides,\nwith the attention-based method getting into people’s sight,\nresearchers ﬁnd that the attention mechanism preciously\nsuit the formulation of the task. This is shown by a\nnumber of works, such as [28] designed a Cross-Modal\n2\nEncoding\nTransformer \nEncoder\nTransformer \nDecoder\nQuery \nGeneration\n\"Small elephant  \non the left\"\nQuery \nBalance\nAttention \nAttention \nAttention \nAttention \nQuery \n+Pos.  \nEmb.\nMask \nDecoderVision\nVision + \nLanguage \nImage\nTarget Mask\nFigure 2. The overview of the network framework. Firstly, the input image and language expression are transformed into feature spaces.\nFeatures then processed by a transformer encoder-decoder model, generating a set of query responses. These responses are then decoded\nto output the target mask. “Pos. Emb.”: Positional Embeddings.\nSelf-Attention (CMSA) model which adaptively focus on\ninformative words in the query expression and the important\npart of the input image, and [11] utilizes a pair of attention\nmodule namely language-guided visual attention module\nand vision-guided linguistic attention module to learn the\nrelationship between multi-modal features.\nUnlike previous methods that are built upon FCN-like\nnetworks, we replace the prediction and identiﬁcation head\nwith a fully attention based architecture, which helps us to\neasily model the long-range dependencies in the image.\n2.2. Attention and Transformer\nThe Transformer model, which is a sequence to se-\nquence deep network architecture that uses only the at-\ntention mechanisms, is ﬁrst introduced by Vaswani et al .\nin [25]. The transformer model quickly gain its attraction\nin the Natural Language Processing (NLP) and shows\npromising performance on several major NLP tasks such as\nmachine translation [25], language modeling [14], question\nanswering [2]. In recent years, the transformer is also being\nadopted in the computer vision community and has shown\npotential on various tasks such as object detection [1],\nimage recognition [7], human-object interaction [26], se-\nmantic segmentation [32], etc. Unlike CNN that focus on\nlocal pixels (kernels), transformer is appreciated because its\nability on modeling global information.\n3. Methodology\nThe overall architecture of our approach is shown in\nFig. 2. The network takes an image I ∈RH′×W′×3 and an\nlanguage expression T = {wi}i=1,...,t as input, where H′\nand W′are the height and width of the input image respec-\ntively, t is the length of the language expression. Firstly,\nthe input image and language expression are mapped into\nthe feature space. Next, language and vision features\nare processed together by the Query Generation Module\n(QGM) to produce a set of language query vectors, which\nrepresent different comprehensions about the image and\nLinear(Wa)Linear(Wt) Fvq\nTransformer Decoder\nFq\nLinear(Wv)\nFlatten\nVision \nFeatures \nFvr\nLanguage Features \nFt\n××\n+\nAttention \nWeights \nA \nFigure 3. Architecture of the Query Generation Module. The\nmodule takes language featuresFt and vision feature Fvr as input,\nand generate a set of query vectors Fq.\nlanguage expression. At the same time, vision features are\nsent to the transformer encoder to generate a set of memory\nfeatures. The query vectors obtained from QGM are used\nto “query” the memory features, and the resulting responses\nfrom decoder are then selected by a Query Balance Module.\nFinally, the network outputs a maskMp for the target object.\n3.1. Query Generation Module\nIn most previous vision-transformer works [1], queries\nfor the transformer decoder are usually a set of ﬁxed learned\nvectors, each of which is used to predict one object and\nhas its own operating mode, e.g., specializes in objects\nof certain kinds or located in certain regions. In these\nworks with ﬁxed queries, there must imply a hypothesis\nthat objects in the input image are distributed under some\nstatistic rules. This is proved to work in other related tasks\nsuch as object detection and panoptic segmentation.\nFor referring segmentation, the target-of-interest indi-\n3\n1 \nConv Flatten\nH×W×Nq Sequential vision features \nNq×(HW)\n1 2 \n3 ... \nN q \n2 3 ... N q \nRaw vision features \nH×W×C\nF vr \nF vq \nFigure 4. The preparation process of vision features in the Query\nGeneration Module. The module converts regular 2D vision\nfeature into a set of sequential features.\ncated by the input language can be any instance in the\nimage. Since both image and language expression are un-\nconstrained, the randomness of the target object’s properties\nis signiﬁcantly high. Thus, ﬁxed query vectors, like in most\nother vision-transformer works, are not enough to represent\nthe properties of the target object. Instead, these properties\nare hidden in the language expression, e.g., keywords like\n“big/small”, “left/right”. To extract the key information and\naddress this high randomness in referring segmentation, we\npropose a Query Generation Module to adaptively produce\nthe query vectors online according to the input image and\nlanguage expression with the help of image information, as\nshown in Fig. 3. Also, in order to let the network learn\ndifferent aspects of information and enhance the robustness\nof the queries, we generate multiple queries though there is\nonly one target instance.\nThe Query Generation Module takes language feature\nFt ∈RNl×C and raw vision feature Fvr ∈RH×W×C as\ninput. In Ft, the i-th vector is the feature vector of the word\nwi, which is the i-th word in the input language expression.\nNl in Ft is ﬁxed by zero-padding. The module aims\nto output Nq query vectors, each of which is a language\nfeature with different attention weights guided by vision\ninformation.\nFirstly, the vision features are prepared as shown in\nFig. 4. We reduce the feature channel dimension size of the\nvision feature Fvr to query number Nq by three convolution\nlayers, resulting in Nq feature maps. Each of them will\nparticipate in the generation of one query vector. The\nfeature maps are then be ﬂattened in the spatial domain,\nforming a feature matrix Fvq of size Nq ×(HW), i.e.,\nFvq = Flatten(Conv(Fvr))T (1)\nIt is known that for a language expression, the im-\nportance of different words is different. Some previous\nworks address this issue by measuring the importance of\neach word. For example, [19] gives each word a weight\nand [29, 12] deﬁnes a set of labels, e.g., location, attribute,\nentity, and ﬁnds the degree of each word belongs to different\nlabels. Most works derive the weights by the language\nself-attention, which does not utilize the information in\nthe image and only outputs one set of weights. But in\nInput: \"The large circle on the left\"\n(a) (b) \nFigure 5. An example of one sentence having different emphasis.\nFor different images, the informative degree of words “large” and\n“left” are different.\npractice, a same sentence may have different understand-\ning perspectives and emphasis, and the most suitable and\neffective emphasis can only be known with the help of\nthe image. We give an intuitive example in Fig. 5. For\nthe same input sentence “The large circle on the left”, the\nword “left” is more informative for the ﬁrst image but the\n“large” is more useful for the second image. In this case,\nlanguage self-attention cannot differentiate the importance\nbetween “large” and “left” and hence can only give both\nwords high attention weights, making the attention process\nless effective. Therefore, in the Query Generation Module,\nwe comprehend the language expression from multiple\naspects incorporating the image, forming Nq queries from\nlanguage. Different queries emphasize different words,\nand more suitable attention weights are then be found and\nenhanced by a Query Balance Module.\nTo this end, we derive the attention weights for language\nfeatures Ft by incorporating the vision featuresFvq. Firstly,\nwe apply linear project on Fvq and Ft. Then, for the n-\nth query, we take the n-th vision feature vector fvqn ∈\nR1×(HW ),n = 1,2,...,N q and language feature of all\nwords. Let the feature of the i-th word denote as fti ∈\nR1×C,i = 1,2,...,N l. The n-th attention weight for the\ni-th word is the product of projected fvqn and fti:\nani = σ(fvqnWv) σ(ftiWa)T (2)\nresulting in a scalar ani. Wv ∈ R(HW )×C and Wa ∈\nRC×C are learnable parameters andσis activation function.\nSoftmax function is applied across all words for each query\nas normalization.\nFor then-th query, a set of attention weights for all words\nare formed up from ani to An ∈R1×Nl ,n = 1,2,...,N q.\nIt consists of a set of attention weights for different words,\nand the different queries may attend to different parts of\nthe language expression. Thus, Nq query vectors focus on\ndifferent emphasis, or different comprehension ways, of the\nlanguage expression.\nNext, the derived attention weights are applied on the\n4\nLinear\nTransformer \nDecoder × \nC Linear\nQuery  \nConfidence \nC q \nQuery Vectors \nF q  \nMask \nDecoder\nN q ×C N q ×1 \nTransformer \nOutput \nF r N q ×C \nC :Concatenate×2 \nFigure 6. The Query Balance Module. A conﬁdence parameter is computed for each query vector. The conﬁdences are then applied on its\ncorresponding transformer outputs to control the inﬂuence of each query vector.\nlanguage features:\nFqn = Anσ(FtWt) (3)\nwhere Wt ∈ RC×C are learnable parameters. Each\nFqn is an attended language feature vector guided by\nvision information and serves as one query vector to the\ntransformer decoder. Mathematically, each query is a\nprojected weighted sum of features of different words in\nthe language expression, therefore it remains properties as\na language feature, and can be used to query the image.\n3.2. Query Balance Module\nWe get Nq different query vectors from the proposed\nQuery Generation Module. Each query represents a spe-\nciﬁc comprehension of the input language expression. As\nwe discussed before, both the input image and language\nexpression are of high randomness. Thus, it is desired to\nadaptively select the better comprehension ways and let the\nnetwork focus on the more reasonable and suitable compre-\nhension ways. On the other hand, as the independence of\neach query vector is kept in the transformer decoder [1] but\nwe only need one mask output, it is desired to balance the\ninﬂuence of different queries to the ﬁnal output. Therefore,\nwe propose a Query Balance Module to adaptively assign\neach query vector a conﬁdence measure that reﬂects how\nmuch it ﬁts the prediction and the context of the image. The\narchitecture is shown in Fig. 6.\nThe Query Balance Module takes the query vectors\nFq from the Query Generation Module and its response\nfrom the transformer decoder, Fr, which is of the same\nsize as Fq. Let Frn represent the corresponding response\nto Fqn. In the Query Balance Module, the query and\nits corresponding response are ﬁrst concatenated together.\nThen, a set of query conﬁdence levels Cq of size Nq ×1\nis generated by two consecutive linear layers. Each scalar\nCqn shows how much the query Fqn ﬁts the context of its\nprediction, and controls the inﬂuence of its response Frn to\nthe mask decoding. The second linear layer uses sigmoid\nas an activation function to control the output range. Each\nresponse Frn is multiplied with the corresponding query\nconﬁdence Cqn, and sent for mask decoding.\n3.3. Network Architecture\nEncoding. Since the transformer architecture only ac-\ncepts sequential inputs, the original image, and language\ninput must be transformed into feature space before sending\nto the transformer. For vision features, following [1], we\nuse a CNN backbone for image encoding. We take the\nfeatures of the last three layers in the backbone as the\ninput for our encoder. By resizing the three sets of feature\nmaps to the same size and summing them together, we get\nthe raw vision feature Fvr ∈RH×W×C, where H,W is\nthe spatial size of features, and C is the feature channel\nnumber. For language features, we ﬁrst use a lookup table\nto convert each word into word embeddings [31], and then\nutilize an RNN module to convert the word embedding to\nthe same channel number as the vision feature, resulting\nin a set of language features Ft ∈ RNl×C. Fvr and Ft\nare then sent to the Query Generation Module as vision and\nlanguage features. At the same time, we ﬂatten the spatial\ndomain of Fvr into a sequence, forming the vision feature\nFv ∈RNv×C,Nv = H ×W, which will be sent to the\nTransformer Module.\nTransformer Module.We use a complete but shallow\ntransformer to apply the attention operations on input fea-\ntures. The network has a transformer encoder and a decoder,\neach of which has two layers. Each layer has one (encoder)\nor two (decoder) multi-head attention modules and one\nfeed-forward network, as deﬁned in [25]. The transformer\nencoder takes the vision feature Fv as input, deriving the\nmemory features about vision information Fm ∈RNv×C.\nBefore sending to the encoder, we add a ﬁxed sine spatial\npositional embedding on Fv. In our experiments, we then\nmultiply Fv with the ﬁnal state of the language features as in\n[19] to enrich the information in vision features. Fm is then\nsent to the transformer decoder as keys and values, together\nwith Nq query vectors produced by the Query Generation\nModule. The decoder queries the vision memory feature\nwith language query vectors and outputs Nq responses for\nmask decoding.\nMask Decoder Module.The Mask Decoder consists of\nthree stacked 3×3 convolution layers for decoding followed\nby one 1 ×1 convolutional layer for outputting the ﬁnal\n5\nTable 1. Comparison with Convolutional Networks in terms of parameter size and performance. The “#params” represents the number of\ntrainable parameters in our Transformer and its substitute, a module with seven 3×3 convolutional layers.\nType #params IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\n7 Conv Layers ∼16.6M 44.28 49.54 42.16 35.24 25.98 10.47\nTransformer ∼17.5M 49.36 55.84 50.79 41.68 29.96 10.76\nTable 2. Comparison of our query generation method with other related methods. “ Ft”: use the language features of all words as queries.\n“Learnt”: queries are parameters learnt in training while ﬁxed in testing, similar with [1].\nNo. Method IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\n1 Ft 45.05 52.69 46.08 36.20 20.97 3.42\n2 Learnt 42.99 49.85 42.38 31.52 17.14 2.41\n3 Ours 49.36 55.84 50.79 41.68 29.96 10.76\nsegmentation mask. Upsampling layers can be optionally\nplugged in between layers to control the output size. To\ndemonstrate the effectiveness of the transformer module\nmore clearly, in our implementation, the Mask Decoding\nModule does not use any former CNN features. We use the\nBinary Cross Entropy loss on the output mask to guide the\nnetwork training.\n4. Experiments\n4.1. Implementation Details\nExperiment Settings. We strictly follow previous\nworks [19, 29] for experiment settings, including preparing\nthe Darknet-56 backbone as the CNN encoder. Input\nimages are resized to 416 ×416. Each Transformer block\nhas 8 heads, and the hidden layer size in all heads is\nset to 256. The maximum length for the input language\nexpression is set to 15 for RefCOCO and RefCOCO+ and\n20 for G-Ref. We train the network for 50 epochs using the\nAdam optimizer with the learning rate λ= 0.001. With the\nshallow transformer architecture, we are able to train the\nmodel with a large batch size of 32 per GPU with 32GB\nVRAM.\nMetrics. We use two metrics for experiments: mask IoU\nand Precision@ X. The IoU metrics show the quality of\nthe output mask which reﬂects the overall performance of\nthe methods, including both targeting and mask generating\nabilities. The Precision@X reports the successful targeting\nrate at the IoU threshold X, which focuses on the targeting\nability of the method.\n4.2. Datasets\nWe evaluate our approach on three commonly used\ndatasets: RefCOCO, RefCOCO+ [30] and G-Ref [21, 23].\nRefCOCO & RefCOCO+(UNC/UNC+) [30] are two\nof the largest and most commonly used datasets for re-\nferring segmentation. The RefCOCO dataset has 19,994\nimages with 142,209 referring expressions for 50,000 ob-\njects while the UNC+ dataset contains 19,992 images with\n141,564 expressions for 49,856 objects. Some kinds of\nwords, e.g., words about absolute locations, are “forbidden”\nFigure 7. Performance gain by increasing the query number Nq.\nThe gray points are performance without the Query Balance\nModule (QBM).\nin the RefCOCO+ dataset, thus it is considered to be more\nchallenging than the RefCOCO dataset.\nG-Ref [21, 23] is another commonly used dataset. It\ncontains 26,711 images with 104,560 expressions referring\nto 54,822 objects. Compared to RefCOCO and RefCOCO+,\nthe G-Ref has a longer average sentence length and richer\nword usage. Notably, there are two partitionings for\nthis dataset: the Google partitioning [21] and the UMD\npartitioning [23]. The UMD partitioning has both validation\nset and test set, but the test set of the Google partitioning is\nnot publicly released. We report the performance of our\nmethod on both kinds of partitioning.\n4.3. Ablation Study\nTo better demonstrate the performance of our model on\nhard and complex scenarios, we do the ablation study on a\nmore difﬁcult dataset, the testB split of the RefCOCO+.\nParameter Size. We show that only a tiny transformer\nnetwork can be an alternative to convolution networks\nwhile achieving better performance in our framework. In\norder to show the scale of our network and prove the\neffectiveness of the transformer module, we compare the\nperformance and parameter size of our method with regular\nconv-nets in Table 1. In the experiment, we replace the\nwhole attention-based modules, including the transformer\nmodule, the Query Generation Module, and the Query\nBalance Module with stacked 3 ×3 convolution layers that\n6\nTable 3. Inﬂuence of Query Numbers. ∗: without Query Balance Module\nNq IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\n1 44.83 50.17 43.94 34.75 21.64 4.66\n2 47.07 52.85 47.31 39.66 28.90 8.30\n4 46.79 53.06 47.54 40.38 28.23 8.92\n8 49.04 55.57 50.58 44.24 32.99 12.62\n16 49.36 55.84 50.79 41.68 29.96 10.76\n32 49.27 55.57 50.48 44.43 33.87 12.50\n16∗ 48.94 55.41 50.32 43.84 32.56 12.99\nTable 4. Experimental results of the IoU metric, and comparison of other methods with ours. U: UMD split. G: Google split.\nRefCOCO RefCOCO+ G-Ref\nval test A test B val test A test B val (U) test (U) val(G)\nDMN [22] 49.78 54.83 45.13 38.88 44.22 32.29 - - 36.76\nRRN [15] 55.33 57.26 53.93 39.75 42.15 36.11 - - 36.45\nMAttNet [29] 56.51 62.37 51.70 46.67 52.39 40.08 47.64 48.61 -\nCMSA [28] 58.32 60.61 55.09 43.76 47.60 37.89 - - 39.98\nBRINet [11] 60.98 62.99 59.21 48.17 52.32 42.11 - - 48.04\nCMPC [12] 61.36 64.53 59.64 49.56 53.44 43.23 - - 39.98\nLSCM [13] 61.47 64.99 59.55 49.34 53.12 43.50 - - 48.05\nMCN [19] 62.44 64.20 59.71 50.62 54.99 44.69 49.22 49.40 -\nCGAN [18] 64.86 68.04 62.07 51.03 55.51 44.06 51.01 51.69 46.54\nVLT (ours) 65.65 68.29 62.73 55.50 59.20 49.36 52.99 56.65 49.76\nPrec@0.5 76.20 80.31 71.44 64.19 68.40 55.84 61.03 60.24 56.65\nhave similar parameter sizes. It shows that the parameter\nsize of our attention-based module only roughly equivalent\nto 7 convolutional layers while having a much superior\nperformance. The transformer module outperforms the\n7 Conv module with over 5% margin in IoU, and 7%\nmargin in Prec@0.5. This proves the effectiveness of the\ntransformer module.\nQuery Generation. In this section, we compare the\nQuery Generation Module with other methods for generat-\ning query vectors. The results are reported in Table 2. The\nQuery Generation Module outperforms both methods with\na large margin at about 3% - 6%. In the ﬁrst experiment,\nwe directly send the language features into the transformer\ndecoder as the query. Speciﬁcally speaking, the input\nlanguage expression is processed by an RNN network, then\nthe output for each word is used as one query vector. It can\nbe seen that because that information between words is not\nsufﬁciently exchanged, its performance is not so satisfying.\nThis shows that the Query Generation Module effectively\nunderstands the sentence and generates valid attended lan-\nguage features guided by vision information. Secondly, we\nuse the most common method, i.e. the query vectors are\nlearned during training and ﬁxed during inference. In the\nexperiment, at the beginning of the training, we set 16 query\nvectors that are initialized with uniform distribution, and\ntrain them together with other parts of the network. It is\nshown that the learned ﬁxed query vector cannot represent\nthe target object as effectively as online produced queries\nby the Query Generation Module.\nQuery Number. To show the inﬂuence of the query\nnumber Nq, we report the network’s performance with dif-\nferent numbers of queries in Fig. 7 and Table. 3. The result\nshows that though only one mask is output, multiple queries\nare still desired for the transformer network. From the\nresults, a larger query number brings a notable performance\ngain of about 5% from 1 query to 16 queries. Though\nthe IoU performance of 4 queries is slightly lower than 2\nqueries, from the Pr@0.5 it can be seen that its detection\nperformance is still higher. The performance gain slows\ndown after the query number is larger than 8, thus we\nchoose 16 as the default query number in our implemen-\ntation. This also shows that multiple queries generated by\nthe Query Generation Module represent different aspects\nof information. Also, when the Query Balance Module is\nremoved, there is a performance loss of ∼1%, proving the\neffectiveness of the Query Balance Module.\n4.4. Comparison with State-of-the-art\nIn Table 4, we compare our proposed Vision-Language\nTransformer (VLT) approach with previous state-of-the-art\nmethods on three widely-used datasets. It can be seen that\nour method outperforms other methods on all datasets. On\nthe RefCOCO dataset, the IoU performance of our method\nis higher than other methods with ∼1% gain. Furthermore,\non the more difﬁcult and complex dataset RefCOCO+,\nour method achieves a more notable performance gain of\naround 5%, especially on the testB split. On another\nhard dataset G-Ref where the average length of language\nexpression is longer, our method also achieves a higher\nIoU with a margin of about 2%-5%. This shows that the\n7\nImage (a) Image (b)\nImage (c) Image (d)\nImage (e) Image (f)\n\"White bowl on corner\"\"Bowl of carrots\" \"Black cat\" \"Lighter color cat\"\n\"Guy with stripes\" \"White shirt\" \"Floral pattern\" \"Green shirt\"\n\"Curled tail\" \"Elephant with rider\" \"woman at 9 o'clock with \nwhite coat\" \"Man kneeling in gray suit\"\nFigure 8. (Best viewed in color) Example outputs. For each set of images, the ﬁrst one shows the input image, and captions under other\nimages shows the input language expressions.\nP P\n(a)\n (b)\nFigure 9. Visualizations of: (a). the attention map of point P in\nthe transformer encoder; (b). different query vectors Fq.\nproposed approach has good abilities on hard cases and\nlong expressions. We assume the reason is that, on the\none hand, long and complex sentences usually contain more\ninformation and more emphasis, and our Query Generation\nand Balance Module can detect multiple emphasis and ﬁnd\nthe more informative ones. On the other hand, harder cases\nalso may contain complex scenarios where needs a global\nview, and the multi-head attention is more suitable for this\nproblem as a global operator.\n4.5. Visualization and Qualitative Results\nWe demonstrate example outputs of the method in Fig. 8.\nTo clearly show the identifying ability of the method, for\neach example set, we show the segmentation results of one\nimage with different input query expressions. Image (a)\nand (b) are two direct cases where the language expression\ndescribes the location or color of the target. From the\nsecond expression of Image (b), it can be seen that our\nmethod is able to process the comparative words (lighter).\nImage (c) and (d) show the method’s ability on understand-\ning the attribute words such as “stripes” and relatively rarer\nwords, e.g. “ﬂoral”. In Image (e), the method successfully\nidentiﬁes the target by expression describing the interaction\nbetween objects, i.e. “Elephant with rider”. The Image (f) is\na photo of a group of people, where all instances distribute\ndensely in the image in a complicated layout. Our method\nstill managed to extract the target with difﬁcult language\nexpressions that contain multiple aspects of information,\nsuch as direction (9 o’clock), attributes (white coat) and\nposture (kneeling).\nNext, we extract an attention map from the second\nlayer of the transformer encoder of one point, as shown\nin Fig. 9(a). It can be seen that the point from one\ninstance attends to other related instances across the image,\nshowing that the transformer successfully extracts long-\nrange dependencies in one single layer. Fig. 9(b) shows\nsome query vectors Fq (see Fig. 3 and Eq. (3)), which\nillustrates the diversity of query vectors.\n5. Conclusion\nIn this paper, we address the difﬁcult task of referring\nsegmentation by using the attention networks to alleviate\nthe global information exchange problem in conventional\nconvolutional networks. We reformulate the task to an\nattention problem and propose a framework that exploits\nthe transformer to perform attention operations. To solve\nthe problem of ambiguous referring sentence due to the un-\nknown emphasis, we propose a Query Generation Module\nand a Query Balance Module to comprehend the referring\nsentence with the help of the referred image. The proposed\nmodel outperforms other methods with a large margin on\nthree widely used datasets.\n8\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In European\nConference on Computer Vision , pages 213–229. Springer,\n2020. 2, 3, 5, 6\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings\nof the Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short\nPapers), pages 4171–4186. Association for Computational\nLinguistics, 2019. 3\n[3] Henghui Ding, Scott Cohen, Brian Price, and Xudong\nJiang. Phraseclick: toward achieving ﬂexible interactive\nsegmentation by phrase and click. In European Conference\non Computer Vision, pages 417–435. Springer, 2020. 1\n[4] Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat\nThalmann, and Gang Wang. Boundary-aware feature\npropagation for scene segmentation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 6819–6829, 2019. 1\n[5] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and\nGang Wang. Context contrasted feature and gated multi-\nscale aggregation for scene segmentation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2393–2402, 2018. 1\n[6] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and\nGang Wang. Semantic correlation promoted shape-variant\ncontext for segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 8885–8894, 2019. 1\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations (ICLR), 2021. 3\n[8] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross\nGirshick. Mask r-cnn. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n2961–2969, 2017. 2\n[9] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor\nDarrell, and Kate Saenko. Modeling relationships\nin referential expressions with compositional modular\nnetworks. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages 1115–\n1124, 2017. 2\n[10] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell.\nSegmentation from natural language expressions. In\nEuropean Conference on Computer Vision, pages 108–124.\nSpringer, 2016. 1, 2\n[11] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and\nHuchuan Lu. Bi-directional relationship inferring network\nfor referring image segmentation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4424–4433, 2020. 2, 3, 7\n[12] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao\nWei, Jizhong Han, Luoqi Liu, and Bo Li. Referring image\nsegmentation via cross-modal progressive comprehension.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10488–10497, 2020.\n4, 7\n[13] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi\nYu, Faxi Zhang, and Jizhong Han. Linguistic structure\nguided context modeling for referring image segmentation.\nIn European Conference on Computer Vision, pages 59–75.\nSpringer, 2020. 7\n[14] Ben Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. Dynamic evaluation of transformer language\nmodels. arXiv preprint arXiv:1904.08378, 2019. 3\n[15] Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan\nQi, Xiaoyong Shen, and Jiaya Jia. Referring image\nsegmentation via recurrent reﬁnement networks. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5745–5753, 2018. 1, 7\n[16] Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, and\nAlan Yuille. Recurrent multimodal interaction for referring\nimage segmentation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n1271–1280, 2017. 1, 2\n[17] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 3431–3440, 2015. 1\n[18] Gen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong\nSu, Chia-Wen Lin, and Qi Tian. Cascade grouped\nattention network for referring expression segmentation. In\nProceedings of the 28th ACM International Conference on\nMultimedia, pages 1274–1282, 2020. 7\n[19] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin\nWu, Cheng Deng, and Rongrong Ji. Multi-task collaborative\nnetwork for joint referring expression comprehension and\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10034–\n10043, 2020. 2, 4, 5, 6, 7\n[20] Ruotian Luo and Gregory Shakhnarovich. Comprehension-\nguided referring expressions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 7102–7111, 2017. 2\n[21] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy. Generation\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 11–20, 2016. 6\n[22] Edgar Margffoy-Tuay, Juan C P ´erez, Emilio Botero, and\nPablo Arbel´aez. Dynamic multimodal instance segmentation\nguided by natural language queries. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages\n630–645, 2018. 1, 7\n[23] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.\nModeling context between objects for referring expression\n9\nunderstanding. In European Conference on Computer\nVision, pages 792–807. Springer, 2016. 6\n[24] Hengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu.\nKey-word-aware network for referring expression image\nsegmentation. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 38–54, 2018. 2\n[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. In Advances\nin neural information processing systems, pages 5998–6008,\n2017. 2, 3, 5\n[26] Suchen Wang, Yap-Peng Tan, Henghui Ding, Kim-Hui\nYap, Junsong Yuan, and Ji-Yan Wu. Discovering human\ninteractions with large-vocabulary objects via query and\nmulti-scale detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2021.\n3\n[27] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and\nKaiming He. Non-local neural networks. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pages 7794–7803, 2018. 2\n[28] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.\nCross-modal self-attention network for referring image\nsegmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages 10502–\n10511, 2019. 2, 7\n[29] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,\nMohit Bansal, and Tamara L Berg. Mattnet: Modular\nattention network for referring expression comprehension.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 1307–1315, 2018. 2, 4, 6, 7\n[30] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C\nBerg, and Tamara L Berg. Modeling context in referring\nexpressions. In European Conference on Computer Vision ,\npages 69–85. Springer, 2016. 6\n[31] Hui Zhang and Henghui Ding. Prototypical matching and\nopen set rejection for zero-shot semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2021. 5\n[32] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng,\nTao Xiang, Philip HS Torr, et al. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6881–\n6890, 2021. 3\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8542261123657227
    },
    {
      "name": "Transformer",
      "score": 0.7727851271629333
    },
    {
      "name": "Segmentation",
      "score": 0.7339536547660828
    },
    {
      "name": "Encoder",
      "score": 0.6938745975494385
    },
    {
      "name": "Architecture",
      "score": 0.5157352089881897
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49247774481773376
    },
    {
      "name": "Expression (computer science)",
      "score": 0.47250115871429443
    },
    {
      "name": "Language model",
      "score": 0.4327440559864044
    },
    {
      "name": "Natural language processing",
      "score": 0.374599426984787
    },
    {
      "name": "Programming language",
      "score": 0.13493233919143677
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ]
}