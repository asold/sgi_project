{
  "title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
  "url": "https://openalex.org/W3168489096",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2355881896",
      "name": "Chen, Xiangning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222521706",
      "name": "Hsieh, Cho-Jui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743107656",
      "name": "Gong, Boqing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3165088525",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W3122542623",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963069632",
    "https://openalex.org/W3047389762",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W3214042613",
    "https://openalex.org/W2962819303",
    "https://openalex.org/W2970375336",
    "https://openalex.org/W2996012599",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3035743198",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2963959597",
    "https://openalex.org/W2963063862",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2970317235",
    "https://openalex.org/W2962781217",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3175958943",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3034194698",
    "https://openalex.org/W3035584989",
    "https://openalex.org/W3103385169",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3093329015",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W2962933129",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W3126536942",
    "https://openalex.org/W2963509076",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3158846111",
    "https://openalex.org/W2996564870",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W2963317585",
    "https://openalex.org/W3100345210",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3118608800"
  ],
  "abstract": "Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and +11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \\url{https://github.com/google-research/vision_transformer}.",
  "full_text": "Published as a conference paper at ICLR 2022\nWHEN VISION TRANSFORMERS OUTPERFORM\nRESNETS WITHOUT PRE-TRAINING OR STRONG DATA\nAUGMENTATIONS\nXiangning Chen1,2∗, Cho-Jui Hsieh2, Boqing Gong1\n1Google Research, 2Department of Computer Science, UCLA\n{xiangningc, bgong}@google.com, chohsieh@cs.ucla.edu\nABSTRACT\nVision Transformers (ViTs) and MLPs signal further efforts on replacing hand-\nwired features or inductive biases with general-purpose neural architectures. Ex-\nisting works empower the models by massive data, such as large-scale pre-training\nand/or repeated strong data augmentations, and still report optimization-related\nproblems (e.g., sensitivity to initialization and learning rates). Hence, this pa-\nper investigates ViTs and MLP-Mixers from the lens of loss geometry, intend-\ning to improve the models’ data efﬁciency at training and generalization at in-\nference. Visualization and Hessian reveal extremely sharp local minima of con-\nverged models. By promoting smoothness with a recently proposed sharpness-\naware optimizer, we substantially improve the accuracy and robustness of ViTs\nand MLP-Mixers on various tasks spanning supervised, adversarial, contrastive,\nand transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for\nViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style prepro-\ncessing). We show that the improved smoothness attributes to sparser active neu-\nrons in the ﬁrst few layers. The resultant ViTs outperform ResNets of similar\nsize and throughput when trained from scratch on ImageNet without large-scale\npre-training or strong data augmentations. Model checkpoints are available at\nhttps://github.com/google-research/vision_transformer.\n1 I NTRODUCTION\nTransformers (Vaswani et al., 2017) have become the de-facto model of choice in natural language\nprocessing (NLP) (Devlin et al., 2018; Radford et al., 2018). In computer vision, there has recently\nbeen a surge of interest in end-to-end Transformers (Dosovitskiy et al., 2021; Touvron et al., 2021b;\nLiu et al., 2021b; Fan et al., 2021; Arnab et al., 2021; Bertasius et al., 2021; Akbari et al., 2021)\nand MLPs (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al., 2021a; Melas-Kyriazi, 2021),\nprompting the efforts to replace hand-wired features or inductive biases with general-purpose neu-\nral architectures powered by data-driven training. We envision these efforts may lead to a uniﬁed\nknowledge base that produces versatile representations for different data modalities, simplifying the\ninference and deployment of deep learning models in various application scenarios.\nDespite the appealing potential of moving toward general-purpose neural architectures, the lack of\nconvolution-like inductive biases also challenges the training of vision Transformers (ViTs) and\nMLPs. When trained on ImageNet (Deng et al., 2009) with the conventional Inception-style data\npreprocessing (Szegedy et al., 2016), Transformers “yield modest accuracies of a few percentage\npoints below ResNets of comparable size” (Dosovitskiy et al., 2021). To boost the performance,\nexisting works resort to large-scale pre-training (Dosovitskiy et al., 2021; Arnab et al., 2021; Akbari\net al., 2021) and repeated strong data augmentations (Touvron et al., 2021b), resulting in exces-\nsive demands of data, computing, and sophisticated tuning of many hyperparameters. For instance,\nDosovitskiy et al. (Dosovitskiy et al., 2021) pre-train ViTs using 304M labeled images, and Touvron\net al. (2021b) repeatedly stack four strong image augmentations.\n∗Work done as a student researcher at Google.\n1\narXiv:2106.01548v3  [cs.CV]  13 Mar 2022\nPublished as a conference paper at ICLR 2022\nIn this paper, we show ViTs can outperform ResNets (He et al., 2016) of even bigger sizes in both\naccuracy and various forms of robustness by using a principled optimizer, without the need for large-\nscale pre-training or strong data augmentations. MLP-Mixers (Tolstikhin et al., 2021) also become\non par with ResNets.\nWe ﬁrst study the architectures fully trained on ImageNet from the lens of loss landscapes and draw\nthe following ﬁndings. First, visualization and Hessian matrices of the loss landscapes reveal that\nTransformers and MLP-Mixers converge at extremely sharp local minima, whose largest principal\ncurvatures are almost an order of magnitude bigger than ResNets’. Such effect accumulates when\nthe gradients backpropagate from the last layer to the ﬁrst, and the initial embedding layer suffers\nthe largest eigenvalue of the corresponding sub-diagonal Hessian. Second, the networks all have\nvery small training errors, and MLP-Mixers are more prone to overﬁtting than ViTs of more pa-\nrameters (because of the difference in self-attention). Third, ViTs and MLP-Mixers have worse\n“trainabilities” than ResNets following the neural tangent kernel analyses (Xiao et al., 2020).\nTherefore, we need improved learning algorithms to prevent the convergence to a sharp local min-\nimum when it comes to the convolution-free ViTs and MLP-Mixers. The ﬁrst-order optimizers\n(e.g., SGD and Adam (Kingma & Ba, 2015)) only seek the model parameters that minimize the\ntraining error. They dismiss the higher-order information such as ﬂatness that correlates with gen-\neralization (Keskar et al., 2017; Kleinberg et al., 2018; Jastrz˛ ebski et al., 2019; Smith & Le, 2018;\nChaudhari et al., 2017).\nThe above study and reasoning lead us to the recently proposed sharpness-aware minimizer\n(SAM) (Foret et al., 2021) that explicitly smooths the loss geometry during model training. SAM\nstrives to ﬁnd a solution whose entire neighborhood has low losses rather than focus on any single-\nton point. We show that the resultant models exhibit smoother loss landscapes, and their general-\nization capabilities improve tremendously across different tasks including supervised, adversarial,\ncontrastive, and transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for ViT-B/16\nand Mixer-B/16, respectively, with the simple Inception-style preprocessing). The enhanced ViTs\nachieve better accuracy and robustness than ResNets of similar and bigger sizes when trained from\nscratch on ImageNet, without large-scale pre-training or strong data augmentations. Moreover, we\ndemonstrate that SAM can even enable ViT to be effectively trained with (momentum) SGD, which\nusually lies far behind Adam when training Transformers (Zhang et al., 2020).\nBy analyzing some intrinsic model properties, we observe that SAM increases the sparsity of active\nneurons (especially for the ﬁrst few layers), which contribute to the reduced Hessian eigenvalues.\nThe weight norms increase, implying the commonly used weight decay may not be an effective reg-\nularization alone. A side observation is that, unlike ResNets and MLP-Mixers, ViTs have extremely\nsparse active neurons (see Figure 2 (right)), revealing the potential for network pruning (Akbari\net al., 2021). Another interesting ﬁnding is that the improved ViTs appear to have visually more\ninterpretable attention maps. Finally, we draw similarities between SAM and strong augmentations\n(e.g., mixup) in that they both smooth the average loss geometry and encourage the models to behave\nlinearly between training images.\n2 B ACKGROUND AND RELATED WORK\nWe brieﬂy review ViTs, MLP-Mixers, and some related works in this section.\nDosovitskiy et al. (2021) show that a pure Transformer architecture (Vaswani et al., 2017) can\nachieve state-of-the-art accuracy on image classiﬁcation by pre-training it on large datasets such\nas ImageNet-21k (Deng et al., 2009) and JFT-300M (Sun et al., 2017). Their vision Transformer\n(ViT) is a stack of residual blocks, each containing a multi-head self-attention, layer normaliza-\ntion (Ba et al., 2016), and a MLP layer. ViT ﬁrst embeds an input image x ∈RH×W×C into a se-\nquence of features z∈RN×D by applying a linear projection overNnonoverlapping image patches\nxp ∈RN×(P2·C), where Dis the feature dimension, P is the patch resolution, and N = HW/P2\nis the sequence length. The self-attention layers in ViT are global and do not possess the locality\nand translation equivariance of convolutions. ViT is compatible with the popular architectures in\nNLP (Devlin et al., 2018; Radford et al., 2018) and, similar to its NLP counterparts, requires pre-\ntraining over massive datasets (Dosovitskiy et al., 2021; Akbari et al., 2021; Arnab et al., 2021) or\n2\nPublished as a conference paper at ICLR 2022\nTable 1: Number of parameters, NTK condition number κ, Hessian dominate eigenvalue λmax,\ntraining error at convergence Ltrain, average ﬂatness LN\ntrain, accuracy on ImageNet, and accu-\nracy/robustness on ImageNet-C. ViT and MLP-Mixer suffer divergent κand converge at sharp re-\ngions; SAM rescues that and leads to better generalization.\nResNet-152 ResNet-152-\nSAM ViT-B/16 ViT-B/16-\nSAM Mixer-B/16 Mixer-B/16-\nSAM\n#Params 60M 87M 59M\nNTKκ† 2801.6 4205.3 14468.0\nHessianλmax 179.8 42.0 738.8 20.9 1644.4 22.5\nLtrain 0.86 0.90 0.65 0.82 0.45 0.97\nLNtrain⋆ 2.39 2.16 6.66 0.96 7.78 1.01\nImageNet (%) 78.5 79.3 74.6 79.9 66.4 77.4\nImageNet-C (%) 50.0 52.2 46.6 56.5 33.8 48.8\n†As it is prohibitive to compute the exact NTK, we approximate the value by averaging over its sub-\ndiagonal blocks (see Appendix G for details). We average the results for 1,000 random noises when\ncalculatingLNtrain.\n(a) ResNet\n (b) ViT\n (c) Mixer\n (d) ViT-SAM\n (e) Mixer-SAM\nFigure 1: Cross-entropy loss landscapes of ResNet-152, ViT-B/16, and Mixer-B/16. ViT and MLP-\nMixer converge to sharper regions than ResNet when trained on ImageNet with the basic Inception-\nstyle preprocessing. SAM, a sharpness-aware optimizer, signiﬁcantly smooths the landscapes.\nstrong data augmentations (Touvron et al., 2021b). Some works specialize the ViT architectures for\nvisual data (Liu et al., 2021b; Yuan et al., 2021; Fan et al., 2021; Bertasius et al., 2021).\nMore recent works ﬁnd that the self-attention in ViT is not vital for performance, resulting in several\narchitectures exclusively based on MLPs (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al.,\n2021a; Melas-Kyriazi, 2021). Here we take MLP-Mixer (Tolstikhin et al., 2021) as an example.\nMLP-Mixer shares the same input layer as ViT; namely, it partitions an image into a sequence\nof nonoverlapping patches/tokens. It then alternates between token and channel MLPs, where the\nformer allows feature fusion from different spatial locations.\nWe focus on ViTs and MLP-Mixers in this paper. We denote by “S” and “B” the small and base\nmodel sizes, respectively, and by an integer the image patch resolution. For instance, ViT-B/16 is\nthe base ViT model taking as input a sequence of16×16 patches. Appendices contain more details.\n3 V ITS AND MLP-M IXERS CONVERGE AT SHARP LOCAL MINIMA\nThe current training recipe of ViTs, MLP-Mixers, and related convolution-free architectures relies\nheavily on massive pre-training (Dosovitskiy et al., 2021; Arnab et al., 2021; Akbari et al., 2021) or a\nbag of strong data augmentations (Touvron et al., 2021b; Tolstikhin et al., 2021; Cubuk et al., 2019;\n2020; Zhang et al., 2018; Yun et al., 2019). It highly demands data and computing, and leads to many\nhyperparameters to tune. Existing works report that ViTs yield inferior accuracy to the ConvNets\nof similar size and throughput when trained from scratch on ImageNet without the combination\nof those advanced data augmentations, despite using various regularization techniques (e.g., large\nweight decay, Dropout (Srivastava et al., 2014), etc.). For instance, ViT-B/16 (Dosovitskiy et al.,\n2021) gives rise to 74.6% top-1 accuracy on the ImageNet validation set (224 image resolution),\ncompared with 78.5% of ResNet-152 (He et al., 2016). Mixer-B/16 (Tolstikhin et al., 2021) performs\neven worse (66.4%). There also exists a large gap between ViTs and ResNets in robustness tests (see\nTable 2 for details).\nMoreover, Chen et al. (2021c) ﬁnd that the gradients can spike and cause a sudden accuracy dip\nwhen training ViTs, and Touvron et al. (2021b) report the training is sensitive to initialization and\nhyperparameters. These all point to optimization problems. In this paper, we investigate the loss\n3\nPublished as a conference paper at ICLR 2022\nFigure 2: Left and Middle: ImageNet training error and validation accuracy vs. iteration for ViTs\nand MLP-Mixers. Right: Percentage of active neurons for ResNet-152, ViT-B/16, and Mixer-B/16.\nlandscapes of ViTs and MLP-Mixers to understand them from the optimization perspective, intend-\ning to reduce their dependency on the large-scale pre-training or strong data augmentations.\nViTs and MLP-Mixers converge at extremely sharp local minima.It has been extensively studied\nthat the convergence to a ﬂat region whose curvature is small beneﬁts the generalization of neural\nnetworks (Keskar et al., 2017; Kleinberg et al., 2018; Jastrz˛ ebski et al., 2019; Chen & Hsieh, 2020;\nSmith & Le, 2018; Zela et al., 2020; Chaudhari et al., 2017). Following Li et al. (2018), we plot the\nloss landscapes at convergence when ResNets, ViTs, and MLP-Mixers are trained from scratch on\nImageNet with the basic Inception-style preprocessing (Szegedy et al., 2016) (see Appendices for\ndetails). As shown in Figures 1(a) to 1(c), ViTs and MLP-Mixers converge at much sharper regions\nthan ResNets. Besides, we calculate the training error under Gaussian perturbations on the model\nparameters LN\ntrain = Eϵ∼N[Ltrain(w+ ϵ)] in Table 1, which reveals theaverage ﬂatness. Although\nViT-B/16 and Mixer-B/16 achieve lower training error Ltrain than that of ResNet-152, their loss\nvalues after random weight perturbation become much higher. We further validate the results by\ncomputing the dominate Hessian eigenvalueλmax, which is a mathematical evaluation of theworst-\ncase landscape curvature. The λmax values of ViT and MLP-Mixer are orders of magnitude larger\nthan that of ResNet, and MLP-Mixer suffers the largest curvature among the three species (see\nSection 4.4 for a detailed analysis).\nSmall training errors. This convergence at sharp regions coincides with the training dynamics\nshown in Figure 2 (left). Although Mixer-B/16 has fewer parameters than ViT-B/16 (59M vs. 87M),\nit has a smaller training error (also seeLtrain in Table 1) but much worse test accuracy, implying that\nusing the cross-token MLP to learn the interplay across image patches is more prone to overﬁtting\nthan ViTs’ self-attention mechanism whose behavior is restricted by a softmax. To validate this\nstatement, we simply remove the softmax in ViT-B/16, such that the query and key matrices can\nfreely interact with each other. Although having lower Ltrain (0.56 vs. 0.65), the obtained ViT-\nB/16-Free performs much worse than the original ViT-B/16 (70.5% vs. 74.6%). Its LN\ntrain and\nλmax are 7.01 and 1236.2, revealing that ViT-B/16-Free converges to a sharper region than ViT-\nB/16 (LN\ntrain is 6.66 and λmax is 738.8) both on average and in the worst-case direction. Such a\ndifference probably explains why it is easier for MLP-Mixers to get stuck in sharp local minima.\nViTs and MLP-Mixers have worse trainability.Furthermore, we discover that ViTs and MLP-\nMixers suffer poor trainabilities, deﬁned as the effectiveness of a network to be optimized by gradi-\nent descent (Xiao et al., 2020; Burkholz & Dubatovka, 2019; Shin & Karniadakis, 2020). Xiao et al.\n(2020) show that the trainability of a neural network can be characterized by the condition number\nof the associated neural tangent kernel (NTK), Θ(x,x′) = J(x)J(x′)T, where J is the Jacobian\nmatrix. Denoting by λ1 ≥···≥ λm the eigenvalues of NTK Θtrain, the smallest eigenvalue λm\nconverges exponentially at a rate given by the condition number κ = λ1/λm. If κ diverges then\nthe network will become untrainable (Xiao et al., 2020; Chen et al., 2021a). As shown in Table 1,\nκis pretty stable for ResNets, echoing previous results that ResNets enjoy superior trainability re-\ngardless of the depth (Yang & Schoenholz, 2017; Li et al., 2018). However, we observe that the\ncondition number diverges when it comes to ViT and MLP-Mixer, conﬁrming that the training of\nViTs desires extra care (Chen et al., 2021c; Touvron et al., 2021b).\n4 A P RINCIPLED OPTIMIZER FOR CONVOLUTION -FREE ARCHITECTURES\nThe commonly used ﬁrst-order optimizers (e.g., SGD (Nesterov, 1983), Adam (Kingma & Ba,\n2015)) only seek to minimize the training loss Ltrain(w). They usually dismiss the higher-order\n4\nPublished as a conference paper at ICLR 2022\ninformation such as curvature that correlates with the generalization (Keskar et al., 2017; Chaud-\nhari et al., 2017; Dziugaite & Roy, 2017). However, the objective Ltrain for deep neural networks\nare highly non-convex, making it easy to reach near-zero training error but high generalization er-\nror Ltest during evaluation, let alone their robustness when the test sets have different distribu-\ntions (Hendrycks & Dietterich, 2019; Hendrycks et al., 2020). ViTs and MLPs amplify such draw-\nbacks of ﬁrst-order optimizers due to the lack of inductive bias for visual data, resulting in exces-\nsively sharp loss landscapes and poor generalization, as shown in the previous section. We hypothe-\nsize that smoothing the loss landscapes at convergence can signiﬁcantly improve the generalization\nability of those convolution-free architectures, leading us to the recently proposed sharpness-aware\nminimizer (SAM) (Foret et al., 2021) that explicitly avoids sharp minima.\n4.1 SAM: O VERVIEW\nIntuitively, SAM (Foret et al., 2021) seeks to ﬁnd the parameterwwhose entire neighbours have low\ntraining loss Ltrain by formulating a minimax objective:\nmin\nw\nmax\n∥ϵ∥2≤ρ\nLtrain(w+ ϵ), (1)\nwhere ρis the size of the neighbourhood ball. Without loss of generality, here we use l2 norm for\nits strong empirical results (Foret et al., 2021) and omit the regularization term for simplicity. Since\nthe exact solution of the inner maximization ϵ⋆ = arg max∥ϵ∥2≤ρLtrain(w+ ϵ) is hard to obtain,\nthey employ an efﬁcient ﬁrst-order approximation:\nˆϵ(w) = arg max\n∥ϵ∥2≤ρ\nLtrain(w) + ϵT∇wLtrain(w) = ρ∇wLtrain(w)/∥∇wLtrain(w)∥2. (2)\nUnder the l2 norm, ˆϵ(w) is simply a scaled gradient of the current weight w. After computing ˆϵ,\nSAM updates wbased on the sharpness-aware gradient ∇wLtrain(w)|w+ˆϵ(w).\n4.2 S HARPNESS -AWARE OPTIMIZATION IMPROVES VITS AND MLP-M IXERS\nWe train ViTs and MLP-Mixers with no large-scale pre-training or strong data augmentations. We\ndirectly apply SAM to the original ImageNet training pipeline of ViTs (Dosovitskiy et al., 2021)\nwithout changing any hyperparameters. The pipeline employs the basic Inception-style preprocess-\ning (Szegedy et al., 2016). The original training setup of MLP-Mixers (Tolstikhin et al., 2021)\nincludes a combination of strong data augmentations, and we replace it with the same Inception-\nstyle preprocessing for a fair comparison. Note that we perform grid search for the learning rate,\nweight decay, Dropout before applying SAM. Please see Appendices for training details.\nSmoother regions around the local minima.Thanks to SAM, both ViTs and MLP-Mixers con-\nverge at much smoother regions, as shown in Figures 1(d) and 1(e). Moreover, both the average and\nthe worst-case curvature, i.e., LN\ntrain and λmax, decrease dramatically (see Table 1).\nHigher accuracy. What comes along is tremendously improved generalization performance. On\nImageNet, SAM boosts the top-1 accuracy of ViT-B/16 from 74.6% to 79.9%, and Mixer-B/16\nfrom 66.4% to 77.4%. For comparison, the improvement on a similarly sized ResNet-152 is 0.8%.\nEmpirically, the degree of improvement negatively correlates with the constraints of inductive biases\nbuilt into the architecture. ResNets with inherent translation equivalence and locality beneﬁt less\nfrom landscape smoothing than the attention-based ViTs. MLP-Mixers gain the most from the\nsmoothed loss geometry. In Table 3, we further train two hybrid models (Dosovitskiy et al., 2021) to\nvalidate this observation, where the Transformer takes the feature map extracted from a ResNet-50 as\nthe input sequence. The improvement brought by SAM decreases after we introduce the convolution\nto ViT, for instance, +2.7% for R50-B/16 compared to +5.3% for ViT-B/16. Moreover, SAM brings\nlarger improvements to the models of larger capacity (e.g., +4.1% for Mixer-S/16 vs. +11.0% for\nMixer-B/16) and longer patch sequence (e.g., +2.1% for ViT-S/32 vs. +5.3% for ViT-S/8). Please\nsee Table 2 for more results.\nSAM can be easily applied to common base optimizers. Besides Adam, we also apply SAM on top\nof the (momentum) SGD that usually performs much worse than Adam when training Transform-\ners (Zhang et al., 2020). As expected, we ﬁnd that under the same training budget (300 epochs), the\nViT-B/16 trained with SGD only achieves 71.5% accuracy on ImageNet, whereas Adam achieves\n5\nPublished as a conference paper at ICLR 2022\nTable 2: Performance of ResNets, ViTs, and MLP-Mixers trained from scratch on ImageNet with\nSAM (improvement over the vanilla model is shown in the parentheses). We use the Inception-style\npreprocessing (with resolution 224) rather than a combination of strong data augmentations.\nModel #params Throughput\n(img/sec/core)ImageNet ReaL V2 ImageNet-R ImageNet-C\nResNet\nResNet-50-SAM 25M 2161 76.7 (+0.7) 83.1 (+0.7) 64.6 (+1.0)23.3 (+1.1) 46.5 (+1.9)\nResNet-101-SAM 44M 1334 78.6 (+0.8) 84.8 (+0.9) 66.7 (+1.4)25.9 (+1.5) 51.3 (+2.8)\nResNet-152-SAM 60M 935 79.3 (+0.8) 84.9 (+0.7) 67.3 (+1.0)25.7 (+0.4) 52.2 (+2.2)\nResNet-50x2-SAM98M 891 79.6 (+1.5) 85.3 (+1.6) 67.5 (+1.7)26.0 (+2.9) 50.7 (+3.9)\nResNet-101x2-SAM173M 519 80.9 (+2.4) 86.4 (+2.4) 69.1 (+2.8)27.8 (+3.2) 54.0 (+4.7)\nResNet-152x2-SAM236M 356 81.1 (+1.8) 86.4 (+1.9) 69.6 (+2.3)28.1 (+2.8) 55.0 (+4.2)\nVision Transformer\nViT-S/32-SAM 23M 6888 70.5 (+2.1) 77.5 (+2.3) 56.9 (+2.6)21.4 (+2.4) 46.2 (+2.9)\nViT-S/16-SAM 22M 2043 78.1 (+3.7) 84.1 (+3.7) 65.6 (+3.9)24.7 (+4.7) 53.0 (+6.5)\nViT-S/14-SAM 22M 1234 78.8 (+4.0) 84.8 (+4.5) 67.2 (+5.2)24.4 (+4.7) 54.2 (+7.0)\nViT-S/8-SAM 22M 333 81.3 (+5.3) 86.7 (+5.5) 70.4 (+6.2)25.3 (+6.1) 55.6 (+8.5)\nViT-B/32-SAM 88M 2805 73.6 (+4.1) 80.3 (+5.1) 60.0 (+4.7)24.0 (+4.1) 50.7 (+6.7)\nViT-B/16-SAM 87M 863 79.9 (+5.3) 85.2 (+5.4) 67.5 (+6.2)26.4 (+6.3) 56.5 (+9.9)\nMLP-Mixer\nMixer-S/32-SAM 19M 11401 66.7 (+2.8) 73.8 (+3.5) 52.4 (+2.9)18.6 (+2.7) 39.3 (+4.1)\nMixer-S/16-SAM 18M 4005 72.9 (+4.1) 79.8 (+4.7) 58.9 (+4.1)20.1 (+4.2) 42.0 (+6.4)\nMixer-S/8-SAM 20M 1498 75.9 (+5.7) 82.5 (+6.3) 62.3 (+6.2)20.5 (+5.1) 42.4 (+7.8)\nMixer-B/32-SAM 60M 4209 72.4 (+9.9) 79.0 (+10.9) 58.0 (+10.4)22.8 (+8.2) 46.2 (12.4)\nMixer-B/16-SAM 59M 1390 77.4 (+11.0) 83.5 (+11.4) 63.9 (+13.1)24.7 (+10.2) 48.8 (+15.0)\nMixer-B/8-SAM 64M 466 79.0 (+10.4) 84.4 (+10.1) 65.5 (+11.6)23.5 (+9.2) 48.9 (+16.9)\n74.6%. Surprisingly, SGD + SAM can push the result to 79.1%, which is a huge +7.6% absolute\nimprovement. Although Adam + SAM is still higher (79.9%), their gap largely shrinks.\nBetter robustness. We also evaluate the models’ robustness using ImageNet-R (Hendrycks et al.,\n2020) and ImageNet-C (Hendrycks & Dietterich, 2019) and ﬁnd even bigger impacts of the\nsmoothed loss landscapes. On ImageNet-C, which corrupts images by noise, bad weather, blur,\netc., we report the average accuracy against 19 corruptions across ﬁve levels. As shown in Ta-\nbles 1 and 2, the accuracies of ViT-B/16 and Mixer-B/16 increase by 9.9% and 15.0% (which are\n21.2% and 44.4% relative improvements), after SAM smooths their converged local regions. In\ncomparison, SAM improves the accuracy of ResNet-152 by 2.2% (4.4%relative improvement). We\ncan see that SAM enhances the robustness even more than therelative clean accuracy improvements\n(7.1%, 16.6%, and 1.0% for ViT-B/16, Mixer-B/16, and ResNet-152, respectively).\n4.3 V ITS OUTPERFORM RESNETS WITHOUT PRE -TRAINING OR STRONG AUGMENTATIONS\nTable 3: Accuracy and robustness of two hybrid\narchitectures.\nModel #params ImageNet\n(%)\nImageNet-C\n(%)\nR50-S/16 34M 79.8 53.4\nR50-S/16-SAM 81.0 (+1.2) 57.2 (+3.8)\nR50-B/16 99M 79.7 54.4\nR50-B/16-SAM 82.4 (+2.7) 61.0 (+6.6)\nThe performance of an architecture is often\nconﬂated with the training strategies (Bello\net al., 2021), where data augmentations play a\nkey role (Cubuk et al., 2019; 2020; Zhang et al.,\n2018; Xie et al., 2020; Chen et al., 2021b).\nHowever, the design of augmentations requires\nsubstantial domain expertise and may not trans-\nlate between images and videos, for instance.\nThanks to the principled sharpness-aware opti-\nmizer, we can remove the advanced augmentations and focus on the architectures themselves.\nWhen trained from scratch on ImageNet with SAM,ViTs outperform ResNets of similar and greater\nsizes (also comparable throughput at inference)regarding both clean accuracy (on ImageNet (Deng\net al., 2009), ImageNet-ReaL (Beyer et al., 2020), and ImageNet V2 (Recht et al., 2019)) and\nrobustness (on ImageNet-R (Hendrycks et al., 2020) and ImageNet-C (Hendrycks & Dietterich,\n2019)). ViT-B/16 achieves 79.9%, 26.4%, and 56.6% top-1 accuracy on ImageNet, ImageNet-R,\nand ImageNet-C, while the counterpart numbers for ResNet-152 are 79.3%, 25.7%, and 52.2%, re-\nspectively (see Table 2). The gaps between ViTs and ResNets are even wider for small architectures.\nViT-S/16 outperforms a similarly sized ResNet-50 by 1.4% on ImageNet, and 6.5% on ImageNet-C.\nSAM also signiﬁcantly improves MLP-Mixers’ results.\n6\nPublished as a conference paper at ICLR 2022\nTable 4: Dominant eigenvalueλmaxof the sub-diagonal Hessians for different network components,\nand norm of the model parameter wand the post-activation ak of block k. Each ViT block consists\nof a MSA and a MLP, and MLP-Mixer alternates between a token MLP a channel MLP. Shallower\nlayers have larger λmax. SAM smooths every component.\nModel λmaxof diagonal blocks of Hessian ∥w∥2 ∥a1∥2 ∥a6∥2 ∥a12∥2\nEmbeddingMSA/Token MLP MLP/Channel MLPBlock1 Block6 Block12 Whole\nViT-B/16 300.4 179.8 281.4 44.4 32.4 26.9 738.8 269.3 104.9 104.3 138.1ViT-B/16-SAM 3.8 8.5 9.6 1.7 1.7 1.5 20.9 353.8 117.0 120.3 97.2\nMixer-B/16 1042.3 95.8 417.9 239.3 41.2 5.1 1644.4 197.6 96.7 135.1 74.9Mixer-B/16-SAM 18.2 1.4 9.5 4.0 1.1 0.3 22.5 389.9 110.9 176.0 216.1\n4.4 I NTRINSIC CHANGES AFTER SAM\nWe take a deeper look into the models to understand how they intrinsically change to reduce the\nHessian’ eigenvalueλmax and what the changes imply in addition to the enhanced generalization.\nSmoother loss landscapes for every network component.In Table 4, we break down the Hessian\nof the whole architecture into small diagonal blocks of Hessians concerning each set of parameters,\nattempting to analyze what speciﬁc components cause the blowing up ofλmax in the models trained\nwithout SAM. We observe that shallower layers have larger Hessian eigenvaluesλmax, and the ﬁrst\nlinear embedding layer incurs the sharpest geometry. This agrees with the ﬁnding in (Chen et al.,\n2021c) that spiking gradients happen early in the embedding layer. Additionally, the multi-head\nself-attention (MSA) in ViTs and the Token MLPs in MLP-Mixers, both of which mix information\nacross spatial locations, have comparably lower λmax than the other network components. SAM\nconsistently reduces the λmax of all network blocks.\nWe can gain insights into the above ﬁndings by the recursive formulation of Hessian matrices for\nMLPs (Botev et al., 2017). Let hk and ak be the pre-activation and post-activation values for layer\nk, respectively. They satisfy hk = Wkak−1 and ak = fk(hk), where Wk is the weight matrix and\nfk is the activation function (GELU (Hendrycks & Gimpel, 2020) in MLP-Mixers). Here we omit\nthe bias term for simplicity. The diagonal block of Hessian matrix Hk with respect to Wk can be\nrecursively calculated as:\nHk = (ak−1aT\nk−1) ⊗Hk, Hk = BkWT\nk+1Hk+1Wk+1Bk + Dk, (3)\nBk = diag(f′\nk(hk)), D k = diag(f′′\nk(hk) ∂L\n∂ak\n), (4)\nwhere ⊗is the Kronecker product,Hk is the pre-activation Hessian for layerk, and Lis the objective\nfunction. Therefore, the Hessian norm accumulates as the recursive formulation backpropagates to\nshallow layers, explaining why the ﬁrst block has much larger λmax than the last block in Table 4.\nGreater weight norms. After applying SAM, we ﬁnd that in most cases, the norm of the post-\nactivation value ak−1 and the weight Wk+1 become even bigger (see Table 4), indicating that the\ncommonly used weight decay may not effectively regularize ViTs and MLP-Mixers (see Appendix J\nfor further veriﬁcation when we vary the weight decay strength).\nSparser active neurons in MLP-Mixers.Given the recursive formulation Equation (3), we identify\nanother intrinsic measure of MLP-Mixers that contribute to the Hessian: the number of activated\nneurons. Indeed, Bk is determined by the activated neurons whose values are greater than zero,\nsince the ﬁrst-order derivative of GELU becomes much smaller when the input is negative. As\na result, the number of active GELU neurons is directly connected to the Hessian norm. Figure 2\n(right) shows the proportion of activated neurons for each block, counted using 10% of the ImageNet\ntraining set. We can see that SAM greatly reduces the proportion of activated neurons for the ﬁrst\nfew layers of the Mixer-B/16, pushing them to much sparser states. This result also suggests the\npotential redundancy of image patches.\nViTs’ active neurons are highly sparse.Although Equations (3) and (4) only involve MLPs, we\nstill observe a decrease of activated neurons in the ﬁrst layer of ViTs (but not as signiﬁcant as in\nMLP-Mixers). More interestingly, we ﬁnd that the proportion of active neurons in ViT is much\nsmaller than another two architectures — given an input image, less than 10% neurons have values\ngreater than zero for most layers (see Figure 2 (right)). In other words, ViTs offer a huge potential for\n7\nPublished as a conference paper at ICLR 2022\nTable 5: Data augmentations, SAM, and their combination applied to different model architectures\ntrained on ImageNet and its subsets from scratch.\nDataset\nResNet-152 ViT-B/16 Mixer-B/16\nVanilla SAM AUG SAM\n+ AUG Vanilla SAM AUG SAM\n+ AUG Vanilla SAM AUG SAM\n+ AUG\nImageNet 78.5 79.3 78.8 78.9 74.6 79.9 79.6 81.5 66.4 77.4 76.5 78.1\ni1k (1/2) 74.2 75.6 75.1 75.5 64.9 75.4 73.1 75.8 53.9 71.0 70.4 73.1\ni1k (1/4) 68.0 70.3 70.2 70.6 52.4 66.8 63.2 65.6 37.2 62.8 61.0 65.8\ni1k (1/10) 54.6 57.1 59.2 59.5 32.8 46.1 38.5 45.7 21.0 43.5 43.0 51.0\nFigure 3: Raw images ( Left) and attention maps of ViT-S/16 with ( Right) and without ( Middle)\nsharpness-aware optimization.\nnetwork pruning. This sparsity may also explain why one Transformer can handle multi-modality\nsignals (vision, text, and audio) (Akbari et al., 2021).\nVisually improved attention maps in ViTs.We visualize ViT-S/16’s attention map of the classiﬁ-\ncation token averaged over the last multi-head attentions in Figure 3 following Caron et al. (2021).\nInterestingly, the ViT model optimized with SAM appears to possess visually improved attention\nmap compared with the one trained via the vanilla AdamW optimizer.\n4.5 SAM VS. STRONG AUGMENTATIONS\nPrevious sections show that SAM can improve the generalization (and robustness) of ViTs and\nMLP-Mixers. Meanwhile, another paradigm to train these models on ImageNet from scratch is to\nstack multiple strong augmentations (Touvron et al., 2021b;a; Tolstikhin et al., 2021). Hence, it is\ninteresting to study the differences and similarities between the models trained by SAM and by using\nstrong data augmentations. For the augmentation experiments, we follow Tolstikhin et al. (2021)’s\npipeline that includes mixup (Zhang et al., 2018) and RandAugment (Cubuk et al., 2020).\nGeneralization. Table 5 shows the results of strong data augmentation, SAM, and their combination\non ImageNet. Each row corresponds to a training set of a different fraction of ImageNet-1k. SAM\nbeneﬁts ViT-B/16 and Mixer-B/16 more than the strong data augmentations, especially when the\ntraining set is small. For instance, when the training set contains only 1/10 of ImageNet training\nimages, ViT-B/16-SAM outperforms ViT-B/16-AUG by 7.6%. Apart from the improved validation\naccuracy, we also observe that both SAM and strong augmentations increase the training error (see\nFigure 2 (Middle) and Table 6), indicating their regularization effects. However, they have distinct\ntraining dynamics as the loss curve for ViT-B/16-AUG is much nosier than ViT-B/16-SAM.\nTable 6: Comparison between ViT-B/16-SAM\nand ViT-B/16-AUG. R denotes the missing rate\nunder linear interpolation.\nModel λmax Ltrain LNtrain R(↓)\nViT-B/16 738.8 0.65 6.66 57.9%\nViT-B/16-SAM 20.9 0.82 0.96 39.6%\nViT-B/16-AUG 1659.3 0.85 1.23 21.4%\nSharpness at convergence.Another intriguing\nquestion is as follows. Can augmentations also\nsmooth the loss geometry similarly to SAM? To\nanswer it, we also plot the landscape of ViT-\nB/16-AUG (see Figure 5 in the Appendix) and\ncompute its Hessian λmax together with the av-\nerage ﬂatness LN\ntrain in Table 6. Surprisingly,\nstrong augmentations even enlarge the λmax.\n8\nPublished as a conference paper at ICLR 2022\nHowever, like SAM, augmentations make ViT-B/16-AUG smoother and achieve a signiﬁcantly\nsmaller training error under random Gaussian perturbations than ViT-B/16. These results show that\nboth SAM and augmentations make the loss landscape ﬂat on average. The difference is that SAM\nenforces the smoothness by reducing the largest curvature via a minimax formulation to optimize\nthe worst-case scenario, while augmentations ignore the worse-case curvature and instead smooth\nthe landscape over the directions induced by the augmentations.\nInterestingly, besides the similarity in smoothing the loss curvature on average, we also discover that\nSAM-trained models possess “linearality” resembling the property manually injected by the mixup\naugmentation. Following Zhang et al. (2018), we compute the prediction error in-between training\ndata in Table 6, where a prediction yis counted as a miss if it does not belong to {yi,yj}evaluated\nat x = 0.5xi + 0.5xj. We observe that SAM greatly reduces the missing rate ( R) compared with\nthe vanilla baseline, showing a similar effect to mixup that explicitly encourages such linearity.\n5 A BLATION STUDIES\nIn this section, we provide a more comprehensive study about SAM’s effect on various vision models\nand under different training setups. We refer to Appendices B to D for the adversarial, contrastive\nand transfer learning results.\n5.1 W HEN SCALING THE TRAINING SET SIZE\nPrevious studies scale up training data to show massive pre-training trumps inductive biases (Doso-\nvitskiy et al., 2021; Tolstikhin et al., 2021). Here we show SAM further enables ViTs and MLP-\nMixers to handle small-scale training data well. We randomly sample 1/4 and 1/2 images from each\nImageNet class to compose two smaller-scale training sets, i.e., i1k (1/4) and i1k (1/2) with 320,291\nand 640,583 images, respectively. We also use ImageNet-21k to pre-train the models with SAM,\nfollowed by ﬁne-tuning on ImageNet-1k without SAM. The ImageNet validation set remains intact.\nSAM can still bring improvement when pre-trained on ImageNet-21k (+0.3%, +1.4%, and 2.3% for\nResNet-152, ViT-B/16, and Mixer-B/16, respectively).\nAs expected, fewer training examples amplify the drawback of ViTs and MLP-Mixers’ lack of the\nconvolutional inductive bias — their accuracies decline much faster than ResNets’ (see Figure 4 in\nthe Appendix and the corresponding numbers in Table 5). However, SAM can drastically rescue\nViTs and MLP-Mixers’ performance decrease on smaller training sets. Figure 4 (right) shows that\nthe improvement brought by SAM over vanilla SGD training is proportional to the number of train-\ning images. When trained on i1k (1/4), it boosts ViT-B/16 and Mixer-B/16 by 14.4% and 25.6%,\nescalating their results to 66.8% and 62.8%, respectively. It also tells that ViT-B/16-SAM matches\nthe performance of ResNet-152-SAM even with only 1/2 ImageNet training data.\n6 C ONCLUSIONS AND LIMITATIONS\nThis paper presents a detailed analysis of the convolution-free ViTs and MLP-Mixers from the lens\nof the loss landscape geometry, intending to reduce the models’ dependency on massive pre-training\nand/or strong data augmentations. We arrive at the sharpness-aware minimizer (SAM) after ob-\nserving sharp local minima of the converged models. By explicitly regularizing the loss geometry\nthrough SAM, the models enjoy much ﬂatter loss landscapes and improved generalization regard-\ning accuracy and robustness. The resultant ViT models outperform ResNets of comparable size and\nthroughput when learned with no pre-training or strong augmentations. Further investigation reveals\nthat the smoothed loss landscapes attribute to much sparser activated neurons in the ﬁrst few lay-\ners. Last but not least, we discover that SAM and strong augmentations share certain similarities to\nenhance the generalization. They both smooth the average loss curvature and encourage linearity.\nDespite achieving better generalization, training ViTs with SAM has the following limitations which\ncould lead to potential future work. First, SAM incurs another round of forward and backward\npropagations to update ϵ, which will lead to around 2x computational cost per update. Second, we\nnotice that the effect of SAM diminishes as the training dataset becomes larger, so it is vital to\ndevelop learning algorithms that can improve/accelerate the large-scale pre-training process.\n9\nPublished as a conference paper at ICLR 2022\nETHICS STATEMENT\nWe are not aware of any immediate ethical issues in our work. We hope this paper can provide new\ninsights into the convolution-free neural architectures and their interplay with optimizers, hence\nbeneﬁting future developments of advanced neural architectures that are efﬁcient in data and com-\nputation. Possible negative societal impacts mainly hinge on the applications of convolution-free\narchitectures, whose societal effects may translate to this work.\nACKNOWLEDGEMENT\nThis work is partially supported by NSF under IIS-1901527, IIS-2008173, IIS-2048280 and by\nArmy Research Laboratory under agreement number W911NF-20-2-0158.\nREPRODUCIBILITY STATEMENT\nWe provide comprehensive experimental details and references to existing works and codebases\nto ensure reproducibility. The speciﬁcation of all the architectures used in this paper is available\nin Appendix A. The instructions for plotting the landscape and the attention map are detailed in\nAppendix E. We also present our approach to approximating Hessian’s dominant eigenvalue λmax\nand the NTK condition number in Appendices F and G, respectively. Finally, Appendix H describes\nall the necessary training conﬁgurations, data augmentations, and SAM hyperparameters to ensure\nthe reproducibility of our results.\nREFERENCES\nHassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing\nGong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and\ntext. arXiv preprint arXiv:2104.11178, 2021.\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia Schmid.\nVivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nIrwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon\nShlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies, 2021.\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? arXiv preprint arXiv:2102.05095, 2021.\nLucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are\nwe done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\nAleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for\ndeep learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International\nConference on Machine Learning , volume 70 of Proceedings of Machine Learning Research ,\npp. 557–565. PMLR, 06–11 Aug 2017. URL http://proceedings.mlr.press/v70/\nbotev17a.html.\nRebekka Burkholz and Alina Dubatovka. Initialization of relus for dynamical isometry.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\nnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso-\nciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/\nd9731321ef4e063ebbee79298fa36f56-Paper.pdf.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers, 2021.\n10\nPublished as a conference paper at ICLR 2022\nPratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian\nBorgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient\ndescent into wide valleys. In International Conference on Learning Representations, 2017.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In Hal Daumé III and Aarti Singh (eds.),\nProceedings of the 37th International Conference on Machine Learning , volume 119 of Pro-\nceedings of Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020. URL http:\n//proceedings.mlr.press/v119/chen20j.html.\nWuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in\nfour GPU hours: A theoretically inspired perspective. In International Conference on Learning\nRepresentations, 2021a. URL https://openreview.net/forum?id=Cnon5ezMHtu.\nXiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-\nbased regularization. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th Interna-\ntional Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Re-\nsearch, pp. 1554–1565. PMLR, 13–18 Jul 2020. URLhttp://proceedings.mlr.press/\nv119/chen20f.html.\nXiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, and Boqing Gong. Robust\nand accurate object detection via adversarial learning. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pp. 16622–16631, June 2021b.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers, 2021c.\nEkin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V . Le. Autoaugment:\nLearning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2019.\nEkin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment: Practical au-\ntomated data augmentation with a reduced search space. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops (CVPRW) , pp. 3008–3017, 2020. doi:\n10.1109/CVPRW50498.2020.00359.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations , 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nGintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for\ndeep (stochastic) neural networks with many more parameters than training data. In Gal Elidan,\nKristian Kersting, and Alexander T. Ihler (eds.), Proceedings of the Thirty-Third Conference on\nUncertainty in Artiﬁcial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017 . AUAI\nPress, 2017. URL http://auai.org/uai2017/proceedings/papers/173.pdf.\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\nChristoph Feichtenhofer. Multiscale vision transformers.arXiv preprint arXiv:2104.11227, 2021.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-\ntion for efﬁciently improving generalization. In International Conference on Learning Represen-\ntations, 2021. URL https://openreview.net/forum?id=6Tm1mposlrM.\n11\nPublished as a conference paper at ICLR 2022\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.\n770–778, 2016. doi: 10.1109/CVPR.2016.90.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2020.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. In International Conference on Learning Representations, 2019. URL\nhttps://openreview.net/forum?id=HJz6tiCqYm.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\nThe many faces of robustness: A critical analysis of out-of-distribution generalization, 2020.\nStanisław Jastrz˛ ebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amost\nStorkey. On the relation between the sharpest directions of DNN loss and the SGD step length. In\nInternational Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=SkgEaj05t7.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-\nter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In\nInternational Conference on Learning Representations, 2017. URL https://openreview.\nnet/forum?id=H1oyRlYgg.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,\nAaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances\nin Neural Information Processing Systems , volume 33, pp. 18661–18673. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\nd89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations, 2015.\nBobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape lo-\ncal minima? In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International\nConference on Machine Learning , volume 80 of Proceedings of Machine Learning Research ,\npp. 2698–2707. PMLR, 10–15 Jul 2018. URL http://proceedings.mlr.press/v80/\nkleinberg18a.html.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (bit): General visual representation learning. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp.\n491–507, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58558-7.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-\nscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran As-\nsociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\na41b3bb3e6b050b6c9067c67f663b915-Paper.pdf.\nHanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint\narXiv:2105.08050, 2021a.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows, 2021b.\n12\nPublished as a conference paper at ICLR 2022\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-\nwards deep learning models resistant to adversarial attacks. InInternational Conference on Learn-\ning Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.\nLuke Melas-Kyriazi. Do you even need attention? a stack of feed-forward layers does surprisingly\nwell on imagenet. arXiv preprint arXiv:2105.02723, 2021.\nY . Nesterov. A method for solving the convex programming problem with convergence rateo(1/k2).\nProceedings of the USSR Academy of Sciences, 269:543–547, 1983.\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number\nof classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, pp.\n722–729, 2008. doi: 10.1109/ICVGIP.2008.47.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. In\n2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3498–3505, 2012. doi:\n10.1109/CVPR.2012.6248092.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers\ngeneralize to imagenet? In International Conference on Machine Learning , pp. 5389–5400.\nPMLR, 2019.\nAli Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph\nStuder, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free!\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\nnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso-\nciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/\n7503cfacd12053d309b6bed5c89de212-Paper.pdf.\nYeonjong Shin and George Em Karniadakis. Trainability of relu networks and data-dependent ini-\ntialization. Journal of Machine Learning for Modeling and Computing, 1(1):39–74, 2020. ISSN\n2689-3967.\nSamuel L. Smith and Quoc V . Le. A bayesian perspective on generalization and stochastic gradient\ndescent. In International Conference on Learning Representations , 2018. URL https://\nopenreview.net/forum?id=BJij4yg0Z.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine\nLearning Research , 15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/\nsrivastava14a.html.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effec-\ntiveness of data in deep learning era. In 2017 IEEE International Conference on Computer Vision\n(ICCV), pp. 843–852, 2017. doi: 10.1109/ICCV .2017.97.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\nthe inception architecture for computer vision. In2016 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 2818–2826, 2016. doi: 10.1109/CVPR.2016.308.\nT. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its\nrecent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy.\nMlp-mixer: An all-mlp architecture for vision, 2021.\nHugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard\nGrave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. Resmlp: Feedforward\nnetworks for image classiﬁcation with data-efﬁcient training, 2021a.\n13\nPublished as a conference paper at ICLR 2022\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention, 2021b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,\n2017.\nEric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In\nInternational Conference on Learning Representations, 2020. URL https://openreview.\nnet/forum?id=BJx040EFvH.\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gen-\neralization. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-\nvances in Neural Information Processing Systems , volume 33, pp. 2958–2969. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n1ef91c212e30e14bf125e9374262401f-Paper.pdf.\nLechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and general-\nization in deep neural networks. In Hal Daumé III and Aarti Singh (eds.),Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learn-\ning Research, pp. 10462–10472. PMLR, 13–18 Jul 2020. URLhttp://proceedings.mlr.\npress/v119/xiao20b.html.\nCihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V . Le. Adversarial\nexamples improve image recognition. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2020.\nGe Yang and Samuel Schoenholz. Mean ﬁeld residual networks: On the edge of chaos. In\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Asso-\nciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/\n81c650caac28cdefce4de5ddc18befa0-Paper.pdf.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan\nSong, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep\nlearning: Training bert in 76 minutes. In International Conference on Learning Representations,\n2020. URL https://openreview.net/forum?id=Syx4wnEtvH.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on\nimagenet, 2021.\nSangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In 2019\nIEEE/CVF International Conference on Computer Vision (ICCV) , pp. 6022–6031, 2019. doi:\n10.1109/ICCV .2019.00612.\nArber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hut-\nter. Understanding and robustifying differentiable architecture search. In International Confer-\nence on Learning Representations , 2020. URL https://openreview.net/forum?id=\nH1gDNyrKDS.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-\npirical risk minimization. In International Conference on Learning Representations, 2018. URL\nhttps://openreview.net/forum?id=r1Ddp1-Rb.\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J. Reddi,\nSanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models?\nIn NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nb05b57f6add810d3b7490866d74c0053-Abstract.html.\n14\nPublished as a conference paper at ICLR 2022\nAPPENDICES\nA A RCHITECTURES\nTable 8 speciﬁes the ViT (Dosovitskiy et al., 2021; Vaswani et al., 2017) and MLP-Mixer (Tolstikhin\net al., 2021) architectures used in this paper. “S” and “B” denote the small and base model scales\nfollowing (Dosovitskiy et al., 2021; Touvron et al., 2021b; Tolstikhin et al., 2021), followed by the\nsize of each image patch. For instance, “B/16” means the model of base scale with non-overlapping\nimage patches of resolution 16 ×16. We use the input resolution 224 ×224 throughout the paper.\nFollowing Tolstikhin et al. (2021), we sweep the batch sizes in {32,64,..., 8192}on TPU-v3 and\nreport the highest throughput for each model.\nTable 7: Comparison under the adversarial training framework on ImageNet (numbers in the paren-\ntheses denote the improvement over the standard adversarial training without SAM). With similar\nmodel size and throughput, ViTs-SAM can still outperform ResNets-SAM for clean accuracy and\nadversarial robustness.\nModel #paramsThroughput(img/sec/core)ImageNet Real V2 PGD-10 ImageNet-R ImageNet-C\nResNet\nResNet-50-SAM25M 2161 70.1 (-0.7) 77.9 (-0.3) 56.6 (-0.8)54.1 (+0.9) 27.0 (+0.9) 42.7 (-0.1)ResNet-101-SAM44M 1334 73.6 (-0.4) 81.0 (+0.1) 60.4 (-0.6)58.8 (+1.4) 29.5 (+0.6) 46.9 (+0.3)ResNet-152-SAM60M 935 75.1 (-0.4) 82.3 (+0.2) 62.2 (-0.4)61.0 (+1.8) 30.8 (+1.4) 49.1 (+0.6)\nVision Transformer\nViT-S/16-SAM 22M 2043 73.2 (+1.2) 80.7 (+1.7) 60.2 (+1.4)58.0 (+5.2) 28.4 (+2.4) 47.5 (+1.6)ViT-B/32-SAM88M 2805 69.9 (+3.0) 76.9 (+3.4) 55.7 (+2.5)54.0 (+6.4) 26.0 (+3.0) 46.4 (+3.0)ViT-B/16-SAM87M 863 76.7 (+3.9) 82.9 (+4.1) 63.6 (+4.3)62.0 (+7.7) 30.0 (+4.9) 51.4 (+5.0)\nMLP-Mixer\nMixer-S/16-SAM18M 4005 67.1 (+2.2) 74.5 (+2.3) 52.8 (+2.5)50.1 (+4.1) 22.9 (+2.6) 37.9 (+2.5)Mixer-B/32-SAM60M 4209 69.3 (+9.1) 76.4 (+10.2) 54.7 (+9.4)54.5 (+13.9) 26.3 (+8.0) 43.7 (+8.8)Mixer-B/16-SAM59M 1390 73.9 (+11.1) 80.8 (+11.8) 60.2 (+11.9)59.8 (+17.3) 29.0 (+10.5) 45.9 (+12.5)\nTable 8: Speciﬁcations of the ViT and MLP-Mixer architectures used in this paper. We train all the\narchitectures with image resolution 224 ×224.\nModel #paramsThroughput(img/sec/core)PatchResolutionSequenceLength Hidden Size #heads #layersToken MLPDimensionChannel MLPDimension\nViT-S/32 23M 6888 32×32 49 384 6 12 – –ViT-S/16 22M 2043 16×16 196 384 6 12 – –ViT-S/14 22M 1234 14×14 256 384 6 12 – –ViT-S/8 22M 333 8×8 784 384 6 12 – –ViT-B/32 88M 2805 32×32 49 768 12 12 – –ViT-B/16 87M 863 16×16 196 768 12 12 – –\nMixer-S/3219M 11401 32×32 49 512 – 8 256 2048Mixer-S/1618M 4005 16×16 196 512 – 8 256 2048Mixer-S/8 20M 1498 8×8 784 512 – 8 256 2048Mixer-B/3260M 4209 32×32 49 768 – 12 384 3072Mixer-B/1659M 1390 16×16 196 768 – 12 384 3072Mixer-B/8 64M 466 8×8 784 768 – 12 384 3072\nB W HEN SAM M EETS ADVERSARIAL TRAINING\nInterestingly, SAM and adversarial training are both minimax problems except that SAM’s inner\nmaximization is with respect to the network weights, while the latter concerns about the input for\nTable 9: Hyperparameters for downstream tasks. All models are ﬁne-tuned with 224 ×224 resolu-\ntion, a batch size of 512, cosine learning rate decay, no weight decay, and grad clipping at global\nnorm 1.\nDataset Total steps Warmup steps Base LR\nCIFAR-10 10K 500\n{0.001, 0.003, 0.01, 0.03}CIFAR-100 10K 500\nFlowers 500 100\nPets 500 100\n15\nPublished as a conference paper at ICLR 2022\nFigure 4: ImageNet accuracy (Left) and improvement (Right) brought by SAM.\ndefending contrived attack (Madry et al., 2018; Wong et al., 2020). Moreover, similar to SAM,\nShafahi et al. (2019) suggest that adversarial training can ﬂatten and smooth the loss landscape. In\nlight of these connections, we study ViTs and MLP-Mixers under the adversarial training frame-\nwork (Wu et al., 2020; Madry et al., 2018). We use the fast adversarial training (Wong et al., 2020)\n(FGSM with random start) with the l∞norm and maximum per-pixel change 2/255 during training.\nAll the hyperparameters remain the same as the vanilla supervised training. When evaluating the ad-\nversarial robustness, we use the PGD attack (Madry et al., 2018) with the same maximum per-pixel\nchange 2/255. The total number of attack steps is 10, and the step size is 0.25/255. To incorporate\nSAM, we formulate a three-level objective:\nmin\nw\nmax\nϵ∈Ssam\nmax\nδ∈Sadv\nLtrain(w+ ϵ,x + δ,y), (5)\nwhere Ssam and Sadv denote the allowed perturbation norm balls for the model parameterwand in-\nput image x, respectively. Note that we can simultaneously obtain the gradients for computingϵand\nδby backpropagation only once. To lower the training cost, we use fast adversarial training (Wong\net al., 2020) with the l∞norm for δ, and the maximum per-pixel change is set as 2/255.\nTable 7 (see Appendices) evaluates the models’ clean accuracy, real-world robustness, and adver-\nsarial robustness (under 10-step PGD attack (Madry et al., 2018)). It is clear that the landscape\nsmoothing signiﬁcantly improves the convolution-free architectures for both clean and adversarial\naccuracy. However, we observe a slight accuracy decrease on clean images for ResNets despite\ngain for robustness. Similar to our previous observations, ViTs surpass similar-size ResNets when\nadversarially trained on ImageNet with Inception-style preprocessing for both clean accuracy and\nadversarial robustness.\nC W HEN SAM M EETS CONTRASTIVE LEARNING\nIn addition to data augmentations and large-scale pre-training, another notable way of improving\na neural model’s generalization is (supervised) contrastive learning (Chen et al., 2020; He et al.,\n2020; Caron et al., 2021; Khosla et al., 2020). We couple SAM with the supervised contrastive\nlearning (Khosla et al., 2020) for 350 epochs, followed by ﬁne-tuning the classiﬁcation head by 90\nepochs for both ViT-S/16 and ViT-B/16. We train ViTs under the supervised contrastive learning\nframework (Khosla et al., 2020). We take the classiﬁcation token output from the last layer as the\nencoded representation and retain the structures of the projection and classiﬁcation heads (Khosla\net al., 2020). We employ a batch size 2048 without memory bank (He et al., 2020) and use Au-\ntoAugment (Cubuk et al., 2019) with strength 1.0 following Khosla et al. (2020). For the 350-epoch\npretraining stage, the contrastive loss temperature is set as 0.1, and we use the LAMB optimizer (You\net al., 2020) with learning rate 0.001 ×batch size\n256 along with a cosine decay schedule. For the second\nstage, we train the classiﬁcation head for 90 epochs via a RMSProp optimizer (Tieleman & Hinton,\n2012) with base learning rate 0.05 and exponential decay. The weight decays are set as 0.3 and 1e-6\nfor the ﬁrst and second stages, respectively. We use a small SAM perturbation strength ρ= 0.02.\nCompared to the training procedure without SAM, we ﬁnd considerable performance gain thanks to\nSAM’s smoothing of the contrastive loss geometry, improving the ImageNet top-1 accuracy of ViT-\nS/16 from 77.0% to 78.1%, and ViT-B/16 from 77.4% to 80.0%. In comparison, the improvement\non ResNet-152 is less signiﬁcant (from 79.7% to 80.0% after using SAM).\n16\nPublished as a conference paper at ICLR 2022\nTable 10: Accuracy on downstream tasks of the models pre-trained on ImageNet. SAM improves\nViTs and MLP-Mixers’ transferabilities. ViTs transfer better than ResNets of similar sizes.\n% ResNet-50-SAMResNet-152-SAMViT-S/16ViT-S/16-SAM ViT-B/16ViT-B/16-SAM Mixer-S/16Mixer-S/16-SAM Mixer-B/16Mixer-B/16-SAM\nCIFAR-1097.4 98.2 97.6 98.2 98.1 98.6 94.1 96.1 95.4 97.8CIFAR-10085.2 87.8 85.7 87.6 87.6 89.1 77.9 82.4 80.0 86.4Flowers 90.0 91.1 86.4 91.5 88.5 91.8 83.3 87.9 82.8 90.0Pets 91.6 93.3 90.4 92.9 91.9 93.1 86.1 88.7 86.1 92.5\nAverage 91.1 92.6 90.0 92.6 91.5 93.2 85.4 88.8 86.1 91.7\n(a) ViT\n (b) ViT-SAM\n (c) ViT-AUG\n (d) ViT-21k\nFigure 5: Cross-entropy loss landscapes of ViT-B/16, ViT-B/16-SAM, ViT-B/16-AUG, and ViT-\nB/16-21k. Strong augmentations and large-scale pre-training can also smooth the curvature.\nD W HEN SAM M EETS TRANSFER LEARNING\nWe also study the role of smoothed loss geometry in transfer learning. We select four datasets\nto test ViTs and MLP-Mixers’ transferabilities: CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT\nPets (Parkhi et al., 2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). We use image\nresolution 224 ×224 during ﬁne-tuning on downstream tasks, other settings exactly follow Doso-\nvitskiy et al. (2021); Tolstikhin et al. (2021) (see Table 9). Note that we do not employ SAM during\nﬁne-tuning. We perform a grid search over the base learning rates on small sub-splits of the train-\ning sets (10% for Flowers and Pets, 2% for CIFAR-10/100). After that, we ﬁne-tune on the entire\ntraining sets and report the results on the respective test sets. For comparison, we also include\nResNet-50-SAM and ResNet-152-SAM in the experiments. Table 10 summarizes the results, which\nconﬁrm that the enhanced models also perform better after ﬁne-tuning and that MLP-Mixers gain\nthe most from the sharpness-aware optimization.\nE V ISUALIZATION\nE.1 L OSS LANDSCAPE\nWe use the “ﬁlter normalization” method (Li et al., 2018) to visualize the loss function curvature in\nFigure 1 and 5. For a fair comparison, we use the cross-entropy loss when plotting the landscapes\nfor all architectures, although the original training objective is the sigmoid loss for ViTs and MLP-\nMixers. Note that their sigmoid loss geometry is even sharper. We equally sample 2,500 points on\nthe 2D projection space and compute the losses using 10% of the ImageNet training images (Chen\net al., 2020), i.e., the i1k (1/10) subset in the main text to save computation.\nE.2 A TTENTION MAP\nThe visualization of the ViT’s attention maps (Figure 3 in the main text) follows (Caron et al., 2021).\nWe average the self-attention scores of the “classiﬁcation token” from the last MSA layer to obtain\na matrix A ∈RH/P×W/P, where H, W, P are the image height, width, and the patch resolution,\nrespectively. Then we upsample Ato the image shape H×W before generating the ﬁgure.\n17\nPublished as a conference paper at ICLR 2022\nTable 11: The SAM perturbation strength ρfor training on ImageNet. ViTs and MLP-Mixers favor\nlarger ρthan ResNets does. Larger models with longer patch sequences need stronger strengths.\nModel Task SAM ρ\nResNet\nResNet-50-SAM supervised 0.02\nResNet-101-SAM supervised 0.05\nResNet-152-SAM supervised 0.02\nResNet-50x2-SAM supervised 0.05\nResNet-101x2-SAM supervised 0.05\nResNet-152x2-SAM supervised 0.05\nResNet-50-SAM adversarial 0.05\nResNet-101-SAM adversarial 0.05\nResNet-152-SAM adversarial 0.05\nViT\nViT-S/32-SAM supervised 0.05\nViT-S/16-SAM supervised 0.1\nViT-S/14-SAM supervised 0.1\nViT-S/8-SAM supervised 0.15\nViT-B/32-SAM supervised 0.15\nViT-B/16-SAM supervised 0.2\nViT-B/16-AUG-SAM supervised 0.05\nViT-S/16-SAM adversarial 0.1\nViT-B/32-SAM adversarial 0.1\nViT-B/16-SAM adversarial 0.1\nViT-S/16-SAM supervised contrastive 0.02\nViT-B/16-SAM supervised contrastive 0.02\nMLP-Mixer\nMixer-S/32-SAM supervised 0.1\nMixer-S/16-SAM supervised 0.15\nMixer-S/8-SAM supervised 0.2\nMixer-B/32-SAM supervised 0.35\nMixer-B/16-SAM supervised 0.6\nMixer-B/8-SAM supervised 0.6\nMixer-B/16-AUG-SAM supervised 0.2\nMixer-S/16-SAM adversarial 0.05\nMixer-B/32-SAM adversarial 0.25\nMixer-B/16-SAM adversarial 0.25\nF H ESSIAN EIGENVALUE\nThe Hessian matrix requires second-order derivative, so we compute the Hessian (and all the sub-\ndiagonal Hessian) λmax using 10% of the ImageNet training images (i.e., i1k (1/10)) via power\niteration 1, where we use 100 iterations to ensure its convergence.\nG NTK C ONDITION NUMBER\nWe approximate the neural tangent kernel on the i1k (1/10) subset by averaging over block diagonal\nentries (with block size 48 ×48) in the full NTK. Notice that the computation is based on the archi-\ntecture at initialization without training. As the activation plays an important role when computing\nNTK — we ﬁnd that smoother activation functions enjoy smaller condition numbers, we replace the\nGELU in ViT and MLP-Mixer with ReLU for a fair comparison with ResNet.\nH T RAINING DETAILS\nWe use image resolution 224 ×224 during ﬁne-tuning on downstream tasks, other settings exactly\nfollow (Dosovitskiy et al., 2021; Tolstikhin et al., 2021) (see Table 9). Note that we do not employ\nSAM during ﬁne-tuning. We perform a grid search over the base learning rates on small sub-splits\nof the training sets (10% for Flowers and Pets, 2% for CIFAR-10/100). After that, we ﬁne-tune on\nthe entire training sets and report the results on the respective test sets.\n1https://en.wikipedia.org/wiki/Power_iteration\n18\nPublished as a conference paper at ICLR 2022\nTable 12: Hyperparameters for training from scratch on ImageNet with basic Inception-style pre-\nprocessing and 224 ×224 image resolution.\nResNet ViT MLP-Mixer\nData augmentation Inception-style\nInput resolution 224×224\nBatch size 4,096\nEpoch 90 300 300\nWarmup steps 5K 10K 10K\nPeak learning rate 0.1 ×batch size\n256 3e-3 3e-3\nLearning rate decay cosine cosine linear\nOptimizer SGD AdamW AdamW\nSGD Momentum 0.9 – –\nAdam(β1,β2) – (0.9, 0.999) (0.9, 0.999)\nWeight decay 1e-3 0.3 0.3\nDropout rate 0.0 0.1 0.0\nStochastic depth – – 0.1\nGradient clipping – 1.0 1.0\nTable 13: ImageNet top-1 accuracy (%) of ViT-B/16 and Mixer-B/16 when trained from scratch\nwith different perturbation strength ρin SAM.\nSAMρ 0.0 0.05 0.1 0.2 0.25 0.35 0.4 0.5 0.6 0.65\nViT-B/16 74.6 77.5 78.8 79.9 79.3 – – – – –\nMixer-B/16 66.4 69.5 – – 74.1 74.7 75.6 76.9 77.4 77.1\nExcept for the experiments in Section 4.5 (SAM with strong data augmentations) and Appendix C\n(contrastive learning), we train all the models from scratch on ImageNet with the basic Inception-\nstyle preprocessing (Szegedy et al., 2016), i.e., a random image crop and a horizontal ﬂip with\nprobability 50%. Please see Table 12 for the detailed training settings. We simply follow the original\ntraining settings of ResNet and ViT (Kolesnikov et al., 2020; Dosovitskiy et al., 2021). For MLP-\nMixer, we remove the strong augmentations in its original training pipeline and perform a grid\nsearch over the learning rate in {0.003,0.001}, weight decay in {0.3,0.1,0.03}, Dropout rate in\n{0.1,0.0}, and stochastic depth in{0.1,0.0}. Note that training for 90 epochs is enough for ResNets\nto converge, and longer schedule brings almost no effect. For all the experiments, we use 128 TPU-\nv3 cores (2 per chip), resulting in 32 images per core. The SAM computation for ˆϵis conducted on\neach core independently.\nH.1 P ERTURBATION STRENGTH IN SAM\nDifferent architecture species favor different strengths of perturbation ρ. We perform a grid search\nover ρand report the best results — Table 11 reports the corresponding strengths used in our Ima-\ngeNet experiments. Besides, we show the results when varyingρin Table 13. Similar to (Foret et al.,\n2021), we also ﬁnd that a relative smallρ∈[0.02,0.05] works the best for ResNets. However, larger\nρgives rise to the best results for ViTs and MLP-Mixers. We also observe that architectures with\nlarger capacities and longer input sequences prefer stronger perturbation strengths. Interestingly,\nthe choice of ρ coincides with our previous observations. Since MLP-Mixers suffer the sharpest\nlandscapes, they need the largest perturbation strength. As strong augmentations and contrastive\nlearning already improve generalization, the suitable ρbecomes signiﬁcantly smaller. Note that we\ndo not re-tune any other hyperparameters when using SAM.\nH.2 T RAINING ON IMAGE NET SUBSETS\nIn Section 5.1, we train the models on ImageNet subsets, and the hyperparameters have to be ad-\njusted accordingly. We simply change the batch size to maintain similar total iterations and keep all\nother settings the same, i.e., 2048 for i1k (1/2), 1024 for i1k (1/4), and 512 for i1k (1/10). We do not\nscale the learning rate as we ﬁnd the scaling harms the performance.\n19\nPublished as a conference paper at ICLR 2022\nH.3 T RAINING WITH STRONG AUGMENTATIONS\nWe tune the learning rate and regularization when using strong augmentations (mixup with probabil-\nity 0.5, RandAugment with two layers and magnitude 15) in Section 4.5 following (Tolstikhin et al.,\n2021). For ViT, we use 1e-3 peak learning rate, 0.1 weight decay, 0.1 Dropout, and 0.1 stochastic\ndepth; For MLP-Mixer, those hyperparameters are exactly the same as (Tolstikhin et al., 2021), peak\nlearning rate as 1e-3, weight decay as 0.1, Dropout as 0.0, and stochastic depth as 0.1. Other settings\nare unchanged (Table 12).\nI L ONGER SCHEDULE OF VANILLA SGD\nSince SAM needs another forward and backward propagation to compute ˆϵ, its training overhead\nis ∼2×of the vanilla baseline. We also experiment with2×schedule vanilla training (600 epochs).\nWe observe that training longer brings no effect on both clean accuracy and robustness, indicating\nthat the current 300 training epochs for ViTs and MLP-Mixers are enough for them to converge.\nJ V ARYING WEIGHT DECAY STRANGTH\nTable 14: ImageNet accuracy and curvature analysis for ViT-B/16 when we vary the weight decay\nstrength in Adam (AdamW).\nModel Weight decayImageNet (%)∥w∥2 Ltrain LNtrain λmax\nViT-B/16\n0.2 74.2 339.8 0.51 4.22 507.4\n0.3 74.6 269.3 0.65 6.66 738.8\n0.4 74.7 236.7 0.77 7.08 1548.9\n0.5 74.4 211.8 0.98 7.21 2251.7\nViT-B/16-SAM\n0.2 79.9 461.4 0.69 0.72 13.1\n0.3 79.9 353.8 0.82 0.96 20.9\n0.4 79.4 301.1 0.85 0.98 26.1\n0.5 78.7 259.6 0.95 1.33 45.5\nIn this section, we vary the strength of weight decay and see the effects of this commonly used\nregularization approach. As shown in Table 14, weight decay helps improve the accuracy on Im-\nageNet when training without SAM, the weight norm also decreases when we enlarge the decay\nstrength as expected. However, enlarging the weight decay aggravates the problem of converging\nto a sharper region measured by both LN\ntrain and λmax. Another observation is that ∥w∥2 consis-\ntently increases after applying SAM for every weight decay strength in Table 14, together with the\nimproved ImageNet accuracy and smoother landscape curvature.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8181759119033813
    },
    {
      "name": "Initialization",
      "score": 0.713988721370697
    },
    {
      "name": "Transformer",
      "score": 0.5937838554382324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.573323130607605
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5424490571022034
    },
    {
      "name": "Machine learning",
      "score": 0.5244160890579224
    },
    {
      "name": "Preprocessor",
      "score": 0.49030545353889465
    },
    {
      "name": "Inference",
      "score": 0.48910266160964966
    },
    {
      "name": "Hessian matrix",
      "score": 0.4813978672027588
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3460341989994049
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 103
}