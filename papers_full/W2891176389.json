{
  "title": "Session-level Language Modeling for Conversational Speech",
  "url": "https://openalex.org/W2891176389",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2507617282",
      "name": "Wayne Xiong",
      "affiliations": [
        "Microsoft (United States)",
        "Bellevue Hospital Center"
      ]
    },
    {
      "id": "https://openalex.org/A2127820214",
      "name": "Lingfeng Wu",
      "affiliations": [
        "Bellevue Hospital Center",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096219712",
      "name": "Jun Zhang",
      "affiliations": [
        "Bellevue Hospital Center",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A684032073",
      "name": "Andreas Stolcke",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2916979304",
    "https://openalex.org/W2044818951",
    "https://openalex.org/W2020073413",
    "https://openalex.org/W2507436421",
    "https://openalex.org/W2759071281",
    "https://openalex.org/W100623710",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2181607856",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2963266252",
    "https://openalex.org/W4254353056",
    "https://openalex.org/W4233906699",
    "https://openalex.org/W2089652186",
    "https://openalex.org/W2094971681",
    "https://openalex.org/W2407022425",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2078179100",
    "https://openalex.org/W2519224033",
    "https://openalex.org/W1558276682",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4249803144",
    "https://openalex.org/W2128970689"
  ],
  "abstract": "We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as adjacency pairs, lexical entrainment, and topical coherence. The model consists of a long-short-term memory (LSTM) recurrent network that reads the entire word-level history of a conversation, as well as information about turn taking and speaker overlap, in order to predict each next word. The model is applied in a rescoring framework, where the word history prior to the current utterance is approximated with preliminary recognition results. In experiments in the conversational telephone speech domain (Switchboard) we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2764–2768\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n2764\nSession-level Language Modeling for Conversational Speech\nWayne Xiong Lingfeng Wu Jun Zhang\nMicrosoft, Bellevue, W A, USA\n{weixi,lingfw,junzh}@microsoft.com\nAndreas Stolcke\nMicrosoft, Sunnyvale, CA, USA\nanstolck@microsoft.com\nAbstract\nWe propose to generalize language models\nfor conversational speech recognition to al-\nlow them to operate across utterance bound-\naries and speaker changes, thereby capturing\nconversation-level phenomena such as adja-\ncency pairs, lexical entrainment, and topical\ncoherence. The model consists of a long-short-\nterm memory (LSTM) recurrent network that\nreads the entire word-level history of a conver-\nsation, as well as information about turn taking\nand speaker overlap, in order to predict each\nnext word. The model is applied in a rescor-\ning framework, where the word history prior to\nthe current utterance is approximated with pre-\nliminary recognition results. In experiments\nin the conversational telephone speech domain\n(Switchboard) we ﬁnd that such a model gives\nsubstantial perplexity reductions over a stan-\ndard LSTM-LM with utterance scope, as well\nas improvements in word error rate.\n1 Introduction\nOver the past decade the state of the art in lan-\nguage modeling has shifted from N-gram models\nto feed-forward networks (Bengio et al., 2006),\nand then to recurrent neural networks (RNNs) that\nread a list of words sequentially and predict the\nnext word at each position. Starting with stan-\ndard recurrent networks (Mikolov et al., 2010) the\nsequential modeling approach was later improved\nusing the long-short-term memory (LSTM) archi-\ntecture of (Hochreiter and Schmidhuber, 1997) for\nfurther gains (Sundermeyer et al., 2012; Meden-\nnikov et al., 2016; Xiong et al., 2017). RNN mod-\nels give two fundamental advantages over the old\nN-gram framework. First, the continuous-space\nembedding of word identities allows word simi-\nlarities to be exploited for generalization (Bengio\net al., 2006; Mikolov et al., 2013). Second, the\nrecurrent architecture allows, in principle at least,\nan unlimited history to condition the prediction of\nnext words.\nThe potential advantage of unlimited history,\nhowever, is not commonly used to its full beneﬁt,\nsince the language model (LM) is typically “re-\nset” at the start of each utterance in current state-\nof-the-art recognition systems (Saon et al., 2017;\nXiong et al., 2018). This presumes that each ut-\nterance is independent of the others, and clearly\nviolates what we know about how language and\nconversation works, as discussed in the next sec-\ntion. Consequently, there have been many pro-\nposals to inject information from a longer context\ninto standard LM architectures, going back to N-\ngram models (Bellegarda, 2004), or to generalize\nN-grams LMs to operate across utterance bound-\naries and speakers (Ji and Bilmes, 2004). Based on\nthe RNN framework, (Mikolov and Zweig, 2012)\nproposed augmenting network inputs with a more\nslowly varying context vector that would encode\nlonger-range properties of the history, such as a\nlatent semantic indexing vector. The problem with\nthese approaches is that the modeler has to make\ndesign decisions about how to encapsulate contex-\ntual information as network inputs. Therefore, our\napproach here is to simply provide the entire con-\nversation history as input to a standard LSTM-LM,\nand let the network learn the information that is\nrelevant to next-word prediction.\nWe start by discussing linguistic phenomena\nthat could potentially help in conversational LM\n(Section 2), followed by a description of the\nLSTM model we propose to capture them (Sec-\ntion 3). Section 4 describes the data and recogni-\ntion system we used to test our models, with re-\nsults reported in Section 5. We end with conclu-\nsions and future directions.\n2765\n2 Conversation-level Phenomena\nHere we review a few of the conversation-level\nphenomena that could be used for predicting\nwords from longer context. Perhaps the most\nwidely studied effect is topical coherence, or the\ntendency of words that are semantically related to\none or more underlying topics to appear together\nin the conversation. Consequently, topic-related\nwords are bound to re-occur across utterances, or\ncertain related words appear to trigger one another\n(such as “children” and “school”). This should\nbe especially true for conversations in the Switch-\nboard (and Fischer) corpora, which were collected\nby pairing up strangers to talk about a mutually\nagreeable topic.\nAnother phenomenon that could lead to words\nreoccurring is lexical entrainment (Brennan and\nClark, 1996), or the tendency of conversants to\nadopt the same words and phrases. Entrainment\ncan also apply to speaking style, so the use of com-\nmon discourse particles, syntactic patterns (like\nquestion tags), or even disﬂuencies could be trig-\ngered across speakers.\nOther phenomena operate more locally, but\nacross speaker turn boundaries. Linguistic conver-\nsation analysis has long noted that utterance types\ncome in adjacency pairs (Schegloff, 1968), with\npreferences for certain pairs over others (like a\nstatement is preferentially followed by agreement\nrather than disagreement). Therefore, words in an\nutterance should be more predicable based on the\nprevious utterance. In the past, this has been mod-\neled by conditioning utterance words on an under-\nlying dialog act label, which in turn is conditioned\non adjacent dialog act labels via a dialog act gram-\nmar (Stolcke et al., 2000).\nA good part of conversational behavior has to\ndo with how turn-taking is negotiated (Sacks et al.,\n1974). Speakers use special discourse devices,\nsuch as backchannel words and pause ﬁllers, to\nsignal when they want to take the ﬂoor, or to signal\nthat the other party should keep the ﬂoor. Conver-\nsants also anticipate the ends of turns and jump in\nbefore the other speaker is completely done, mak-\ning for very efﬁcient use of time. As a result of\nall of these mechanisms, a good portion of con-\nversations consists of overlapping (simultaneous)\nspeaking. It was shown (Shriberg et al., 2001) that\nsuch overlap locations can be partly predicted by\nword-based language models. This suggests re-\nversing the modeling and using overlap (the tim-\nFigure 1: Use of conversation-level context in session-\nbased LM. The utterance numbering shows how over-\nlapping utterances are serialized (according to onset\ntimes).\ning of utterances) to help predict the words.\n3 Models\nOur baseline language model is a standard LSTM\nthat models utterances independently from one an-\nother, i.e., the history at the onset of each utterance\nis the start-of-sentence token. In fact, we used two\nversion of this basic LSTM-LM:\n• Word inputs encoded with one-hot vectors,\ncombined with a jointly trained embedding\nlayer\n• Words encoded by multiple-hot vectors cor-\nresponding to the letter trigrams making up\nthe words.\nBoth types of LSTM-LMs use three 1000-\ndimensional hidden layers with recurrence. The\nword embedding layer is also of size 1000, and the\nletter-trigram encoding has size 7190 (the number\nof unique trigrams in our vocabulary).\nThe main addition for session-level modeling is\nthat the LSTM history consists of all the utterances\npreceding the current utterance, followed by all\nwords in the current utterance preceding the word\nto be predicted. The preceding utterances are se-\nrialized in the order of their onset times, so that\nthe ﬂow of words within an utterance is not dis-\nrupted. The resulting total word history and next-\nword prediction is depicted in Figure 1. Informa-\ntion about utterance boundaries is encoded using\na boundary tag, similar to the start-of-sentence to-\nken that is commonly used in LMs.\nSeveral of the conversational phenomena de-\nscribed in Section 2 refer to turn-taking between\nspeakers; to capture this in the model we augment\nthe word input encoding with an extra bit that indi-\ncates whether a speaker change occurred. This bit\nis turned on only for the start-of-utterance token.\nWe also want to capture some information\nabout utterance overlap, since, as described earlier,\n2766\nspeech overlap interacts with word choice. Pos-\nsible events to model would be overlap (exceed-\nings a time threshold) at the starts and ends of ut-\nterances, or maybe a continuous measure of such\noverlaps. As a ﬁrst proof of concept we chose to\nencode only one type of overlap, i.e., when the ut-\nterance in question is completely overlapped tem-\nporally by the other speaker’s turn. This is typi-\ncal of backchannel acknowledgments (“uh-huh”)\nand short utterances that attempt to grab the ﬂoor\n(“um”, “but”). Complete utterance overlap is also\nencoded by an additional input bit that is turned on\nfor the start-of-utterance token.\n4 Experiments\n4.1 Recognition system\nWe used a single bidirectional LSTM acoustic\nmodel in experiments reported here, trained on the\ncommonly used conversational telephone speech\ncorpora (Switchboard, Fisher, CallHome English),\nestimating frame-level posterior probabilities for\n9000 context-dependent phone units. The sys-\ntem decodes speech utterances using a 4-gram\nlanguage model, generating lattices. These are\nthen expanded to 500-best lists, which in turn are\nrescored using the various LMs.\nThe recognition system and the N-gram LM\nused in decoding have a vocabulary of 165k\nwords, but the LSTM-LMs are trained on only\nthe 38k words occurring at least twice in the in-\ndomain conversational training data. Words out-\nside of the LSTM-LM vocabulary are penalized\nin rescoring with a constant weight that is empiri-\ncally optimized on the development set.\n4.2 Data\nLanguage model training uses the Switchboard-\n1, BBN Switchboard-2, Fisher, and English Call-\nHome transcripts (about 23 million words in to-\ntal) as well as the UW conversational Web corpus\n(Bulyko et al., 2003) for pre-training (see below).\nThe N-gram LM used for N-best generation also\nincludes the LDC Hub4 (Broadcast News) corpus.\nThe Switchboard-1 and Switchboard-2 portions of\nthe NIST 2002 CTS test set were used for tun-\ning and development. Evaluation is carried out on\nthe NIST 2000 CTS test set, consisting of Switch-\nboard (SWB) and CallHome (CH) subsets.\nAs an expedient, we refrained from reseg-\nmenting utterances based on forced alignments of\nwords, and instead use utterance boundaries as\nTable 1: Perplexities with session-based LSTM-LMs.\nThe last two lines reﬂect use of errorful recognition\noutput for preceding utterances.\nModel inputs devset test test\nSWB SWB CH\nUtterance words, letter-3grams 48.90 44.56 54.57\n+ session history words 38.86 36.81 44.31\n+ speaker change 37.25 35.33 42.23\n+ speaker overlap 37.09 35.12 42.02\nUsing recognized word histories\nsingle system 39.55 37.45 46.49\nfull system (Xiong et al., 2018) 39.41 37.29 45.99\ngiven in the available transcripts (corresponding\nto the audio segments used in acoustic training).\nSimilarly, in testing, we use the presegmented ut-\nterances provided by NIST. No doubt there are in-\nconsistencies in how the different corpora deﬁne\nutterance units, and a consistent, alignment-based\nresegmentation of all training and test data based\non the durations nonspeech regions and/or lexical\ntagging might give improved results.\n4.3 Model training\nAll LSTM-LMs are trained using the Microsoft\nCognitive Toolkit, or CNTK (Yu et al., 2014; Mi-\ncrosoft Research, 2016) on a Linux-based multi-\nGPU server farm. Training is parallelized using\nCNTK’s distributed stochastic gradient descent\n(SGD) with 1-bit gradient quantization (Seide\net al., 2014). We use the CNTK “FsAdaGrad”\nlearning algorithm, which is an implementation of\nAdam (Kingma and Ba, 2015).\nAll LSTM-LMs are pretrained for one or two\nepochs on a large corpus of “conversational Web”\ndata (Bulyko et al., 2003), followed by normal\ntraining to convergence on the in-domain data.\nEach utterance in the Web data is treated as a sin-\ngle session for purposes of session-based LM, i.e.,\nthe extra bits for speaker change and overlap are\nnever turned on.\n5 Results\nWhen evaluating the session-based LMs on speech\ntest data, the true utterance contexts are not\nknown, and we must use hypothesized words for\nword histories preceding the current utterance. In\nour case, the histories were obtained using the out-\nput of our best recognition system, which uses\na combination of acoustic models (Xiong et al.,\n2018), but excluding the session-based LM. 1 Per-\n1We also omitted the ﬁnal confusion network rescoring\nstage described in (Xiong et al., 2018).\n2767\nTable 2: Recognition results with standard and session-based LSTM-LMs, measured by word error rates (WER).\nWord encoding Model WER WER test\ndevset SWB CH\nLetter 3gram LSTM-LM 10.01 6.88 12.79\nSession LSTM-LM 9.67 6.81 12.54\nSession LSTM-LM, 2nd iteration 9.66 6.77 12.56\nOne-hot LSTM-LM 9.81 6.89 13.02\nSession LSTM-LM 9.47 6.81 12.60\nSession LSTM-LM, 2nd iteration 9.50 6.83 12.73\nLetter 3gram LSTM-LM 9.66 6.63 12.77\n+ One-hot Session LSTM-LM 9.28 6.52 12.34\nLSTM-LM + Session LSTM-LM 9.22 6.45 12.11\nplexity was evaluated on reference transcripts, as\nis customary.\nTable 1 shows the effect of session-level model-\ning and of optional model elements on perplexity,\nbased on LSTMs using letter-trigram encoding.\nBaseline is the standard utterance-scope LSTM-\nLM. We see a large perplexity reduction of 17-\n21% by conditioning on session history words,\nwith smaller incremental reductions from adding\nspeaker change and overlap information.\nThe last two table rows show that some of the\nperplexity gain over the baseline is negated by the\nuse of errorful recognition output for the conver-\nsation history. It does not make much difference\nwhether the recognized word history is generated\nby just the subsystem being rescored (“single sys-\ntem”, with 6% word error on SWB) or the full\nrecognition system using multiple acoustic mod-\nels (“full system”, with about 5% word error rate\non SWB and 10% on CH). Using recognition out-\nput as history, the perplexity degrades about 6%\nrelative for SWB, and 11% on CH, relative to us-\ning the true word histories. Even with the more\nerrorful recognition on CH, the session-based LM\nstill gives a perplexity reduction of 14% relative to\nthe baseline.\nTable 2 presents recognition results, compar-\ning baseline LSTM-LMs to the full session-based\nLSTM-LMs. Both the letter-trigram and one-word\nword encoding versions are reported. The differ-\nent models may also be used jointly, using log-\nlinear score combination in rescoring, shown in\nthe third section of the table. We also tried iterat-\ning the session LM rescoring, after the recognized\nword histories were updated from the ﬁrst rescor-\ning pass (shown as “2nd iteration” in the table).\nResults show that the session-based LM yields\nbetween 1% and 4% relative word error reduction\nfor the two word encodings, and test sets. When\nthe two word encoding types are combined by log-\nlinear combination of model scores, the gain from\nsession-based modeling is preserved. Iterating the\nsession LM rescoring to improve the word histo-\nries did not give consistent gains.\nEven though the session-based LSTM sub-\nsumes all the information used in the standard\nLSTM, there is an additional gain to be had from\ncombining those two model types (last row in the\ntable). Thus, the overall gain from adding the\nsession-based models to the two baseline models\nis 3-5% relative word error reduction.\n6 Conclusion and Future Work\nWe have proposed a simple generalization of\nutterance-level LSTM language models aimed at\ncapturing conversational phenomena that operate\nacross utterances and speakers, such as lexical en-\ntrainment, adjacency pairs, speech overlap, and\ntopical coherence. To capture non-local condition-\ning information, the LSTM-LM is trained to read\nthe entire sequence of utterances making up a con-\nversation, along with side information encoding\nspeaker changes and overlap of utterances. This\nis found to reduce perplexity by about 25%, most\nof which is retained when errorful recognition out-\nput is used to represent the word history in previ-\nous utterances. The session-based LM yields up\nto 5% relative reduction in word error when the\nutterance- and session-based LMs are combined.\nIt would be worthwhile to investigate which\nconversational phenomena are actually being ex-\nploited by the session LSTM model. The ease\nwith which additional information can be input to\nthe LSTM-LM also suggests encoding other con-\nditioning information, such a more details about\nutterance timing, as well as semantic features that\ncapture topical coherence.\n2768\nReferences\nJerome R. Bellegarda. 2004. Statistical language\nmodel adaptation: review and perspectives. Speech\nCommunication, 42:93–108.\nYoshua Bengio, Holger Schwenk, Jean-S ´ebastien\nSen´ecal, Fr ´ederic Morin, and Jean-Luc Gauvain.\n2006. Neural probabilistic language models. In\nStudies in Fuzziness and Soft Computing, volume\n194, pages 137–186.\nSusan E. Brennan and Herbert H. Clark. 1996. Con-\nceptual pacts and lexical choice in conversation.\nJournal of Experimental Psychology: Learning,\nMemory, and Cognition, 22(6):1482–1493.\nIvan Bulyko, Mari Ostendorf, and Andreas Stolcke.\n2003. Getting more mileage from web text sources\nfor conversational speech language modeling using\nclass-dependent mixtures. In Proceedings of HLT-\nNAACL 2003, Conference of the North American\nChapter of the Association of Computational Lin-\nguistics, volume 2, pages 7–9, Edmonton, Alberta,\nCanada. Association for Computational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nGang Ji and Jeffrey Bilmes. 2004. Multi-speaker lan-\nguage modeling. In Proceedings of HLT-NAACL\n2004, Conference of the North American Chapter of\nthe Association of Computational Linguistics, vol-\nume Short Papers, pages 133–136, Boston. Associa-\ntion for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. Proceedings 3rd\nInternational Conference for Learning Representa-\ntions, arXiv preprint arXiv:1703.02136.\nIvan Medennikov, Alexey Prudnikov, and Alexander\nZatvornitskiy. 2016. Improving English conversa-\ntional telephone speech recognition. In Proc. Inter-\nspeech, pages 2–6.\nMicrosoft Research. 2016. The Microsoft Cognition\nToolkit (CNTK). https://cntk.ai.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Proc.\nInterspeech, pages 1045–1048.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013. Linguistic regularities in continuous space\nword representations. In HLT-NAACL, volume 13,\npages 746–751.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn Proc. Interspeech, pages 901–904.\nH. Sacks, E. A. Schegloff, and G. Jefferson. 1974.\nA simplest semantics for the organization of turn-\ntaking in conversation. Language, 50(4):696–735.\nGeorge Saon, Gakuto Kurata, Tom Sercu, Kartik Au-\ndhkhasi, Samuel Thomas, Dimitrios Dimitriadis,\nXiaodong Cui, Bhuvana Ramabhadran, Michael\nPicheny, Lynn-Li Lim, Bergul Roomi, and Phil\nHall. 2017. English conversational telephone speech\nrecognition by humans and machines. In Proc. In-\nterspeech, pages 132–136, Stockholm.\nE. A. Schegloff. 1968. Sequencing in conversational\nopenings. American Anthropologist, 70:1075–1095.\nFrank Seide, Hao Fu, Jasha Droppo, Gang Li, and\nDong Yu. 2014. 1-bit stochastic gradient descent\nand its application to data-parallel distributed train-\ning of speech DNNs. In Proc. Interspeech, pages\n1058–1062.\nElizabeth Shriberg, Andreas Stolcke, and Don Baron.\n2001. Observations on overlap: Findings and impli-\ncations for automatic processing of multi-party con-\nversation. In Proceedings of the 7th European Con-\nference on Speech Communication and Technology,\nvolume 2, pages 1359–1362, Aalborg, Denmark.\nAndreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-\nbeth Shriberg, Daniel Jurafsky, Paul Taylor, Rachel\nMartin, Carol Van Ess-Dykema, and Marie Meteer.\n2000. Dialogue act modeling for automatic tagging\nand recognition of conversational speech. Computa-\ntional Linguistics, 26(3):339–373.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In Proc. Interspeech, pages 194–197.\nW. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang,\nand A. Stolcke. 2018. The Microsoft 2017 conver-\nsational speech recognition system. In Proc. IEEE\nICASSP, pages 5934–5938, Calgary, Alberta.\nWayne Xiong, Jasha Droppo, Xuedong Huang, Frank\nSeide, Mike Seltzer, Andreas Stolcke, Dong Yu, and\nGeoffrey Zweig. 2017. Toward human parity in\nconversational speech recognition. IEEE Transac-\ntions on Audio, Speech, and Language Processing,\n25(12):2410–2423.\nD. Yu et al. 2014. An introduction to computational\nnetworks and the Computational Network Toolkit.\nTechnical Report MSR-TR-2014-112, Microsoft\nResearch. Https://github.com/Microsoft/CNTK.",
  "topic": "Session (web analytics)",
  "concepts": [
    {
      "name": "Session (web analytics)",
      "score": 0.8841180801391602
    },
    {
      "name": "Computer science",
      "score": 0.8264153003692627
    },
    {
      "name": "Speech recognition",
      "score": 0.4841790497303009
    },
    {
      "name": "Language model",
      "score": 0.421626478433609
    },
    {
      "name": "Natural language processing",
      "score": 0.32701337337493896
    },
    {
      "name": "World Wide Web",
      "score": 0.11520883440971375
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210108985",
      "name": "Bellevue Hospital Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ]
}