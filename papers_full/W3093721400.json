{
    "title": "When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models",
    "url": "https://openalex.org/W3093721400",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2099477951",
            "name": "Benjamin Müller",
            "affiliations": [
                "Université Sorbonne Nouvelle",
                "Université Paris 1 Panthéon-Sorbonne",
                "Institut national de recherche en informatique et en automatique",
                "Sorbonne Université",
                "Centre d'Économie de la Sorbonne"
            ]
        },
        {
            "id": "https://openalex.org/A2400815267",
            "name": "Antonios Anastasopoulos",
            "affiliations": [
                "George Mason University"
            ]
        },
        {
            "id": "https://openalex.org/A1632339985",
            "name": "Benoît Sagot",
            "affiliations": [
                "Institut national de recherche en informatique et en automatique"
            ]
        },
        {
            "id": "https://openalex.org/A249347002",
            "name": "Djamé Seddah",
            "affiliations": [
                "Institut national de recherche en informatique et en automatique"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W3101567131",
        "https://openalex.org/W3101498587",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2963571341",
        "https://openalex.org/W2740168486",
        "https://openalex.org/W2510573142",
        "https://openalex.org/W3105069964",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3209051700",
        "https://openalex.org/W2964114970",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2579343286",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2898879711",
        "https://openalex.org/W2971277088",
        "https://openalex.org/W4385681388",
        "https://openalex.org/W2913946806",
        "https://openalex.org/W3035283133",
        "https://openalex.org/W2742113707",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2995015695",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2270364989",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2946676565",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3035032094",
        "https://openalex.org/W2995230342",
        "https://openalex.org/W2899641520",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3038047279",
        "https://openalex.org/W2552110825",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3101860695",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2970529259",
        "https://openalex.org/W2887185954",
        "https://openalex.org/W2273241057",
        "https://openalex.org/W3088592174",
        "https://openalex.org/W3027825632",
        "https://openalex.org/W3103727211",
        "https://openalex.org/W2886440583",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3037582736",
        "https://openalex.org/W2119202242",
        "https://openalex.org/W2752392399",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2995109564",
        "https://openalex.org/W2948902769",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. Transliterating those languages improves very significantly the ability of large-scale multilingual language models on downstream tasks.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 448–462\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n448\nWhen Being Unseen from mBERT is just the Beginning:\nHandling New Languages With Multilingual Language Models\nBenjamin Muller†* Antonios Anastasopoulos‡ Benoît Sagot† Djamé Seddah†\n†Inria, Paris, France *Sorbonne Université, Paris, France\n‡Department of Computer Science, George Mason University, USA\nfirstname.lastname@inria.fr antonis@gmu.edu\nAbstract\nTransfer learning based on pretraining lan-\nguage models on a large amount of raw data\nhas become a new norm to reach state-of-the-\nart performance in NLP. Still, it remains unclear\nhow this approach should be applied for unseen\nlanguages that are not covered by any available\nlarge-scale multilingual language model and\nfor which only a small amount of raw data is\ngenerally available. In this work, by compar-\ning multilingual and monolingual models, we\nshow that such models behave in multiple ways\non unseen languages. Some languages greatly\nbeneﬁt from transfer learning and behave simi-\nlarly to closely related high resource languages\nwhereas others apparently do not. Focusing\non the latter, we show that this failure to trans-\nfer is largely related to the impact of the script\nused to write such languages. We show that\ntransliterating those languages signiﬁcantly im-\nproves the potential of large-scale multilingual\nlanguage models on downstream tasks. This\nresult provides a promising direction towards\nmaking these massively multilingual models\nuseful for a new set of unseen languages.1\n1 Introduction\nLanguage models are now a new standard to\nbuild state-of-the-art Natural Language Process-\ning (NLP) systems. In the past year, monolingual\nlanguage models have been released for more than\n20 languages including Arabic, French, German,\nand Italian (Antoun et al., 2020; Martin et al., 2020;\nde Vries et al., 2019; Cañete et al., 2020; Kuratov\nand Arkhipov, 2019; Schweter, 2020, inter alia).\nAdditionally, large-scale multilingual models cov-\nering more than 100 languages are now available\n(XLM-R by Conneau et al. (2020) and mBERT\nby Devlin et al. (2019)). Still, most of the 6500+\nspoken languages in the world (Hammarström,\n2016) are not covered—remaining unseen—by\n1Code available at https://github.com/benjami\nn-mlr/mbert-unseen-languages.git\nthose models. Even languages with millions of na-\ntive speakers like Sorani Kurdish (about 7 million\nspeakers in the Middle East) or Bambara (spoken\nby around 5 million people in Mali and neighbor-\ning countries) are not covered by any available\nlanguage models at the time of writing.\nEven if training multilingual models that cover\nmore languages and language varieties is tempting,\nthe curse of multilinguality (Conneau et al., 2020)\nmakes it an impractical solution, as it would require\nto train ever larger models. Furthermore, as shown\nby Wu and Dredze (2020), large-scale multilingual\nlanguage models are sub-optimal for languages that\nare under-sampled during pretraining.\nIn this paper, we analyze task and language adap-\ntation experiments to get usable language model-\nbased representations for under-studied low re-\nsource languages. We run experiments on 15 ty-\npologically diverse languages on three NLP tasks:\npart-of-speech (POS) tagging, dependency parsing\n(DEP) and named-entity recognition (NER).\nOur results bring forth a diverse set of behaviors\nthat we classify in three categories reﬂecting the\nabilities of pretrained multilingual language models\nto be used for low-resource languages. We dub\nthose categories Easy, Intermediate and Hard.\nHard languages include both stable and endan-\ngered languages, but they predominantly are lan-\nguages of communities that are majorly under-\nserved by modern NLP. Hence, we direct our atten-\ntion to these Hard languages. For those languages,\nwe show that the script they are written in can be\na critical element in the transfer abilities of pre-\ntrained multilingual language models. Translit-\nerating them leads to large gains in performance\noutperforming non-contextual strong baselines. To\nsum up, our contributions are the following:\n•We propose a new categorization of the low-\nresource languages that are unseen by avail-\nable language models: the Hard, the Interme-\ndiate and the Easy languages.\n449\n•We show that Hard languages can be better\naddressed by transliterating them into a better-\nhandled script (typically Latin), providing a\npromising direction towards making multilin-\ngual language models useful for a new set of\nunseen languages.\n2 Background and Motivation\nAs Joshi et al. (2020) vividly illustrate, there is a\nlarge divergence in the coverage of languages by\nNLP technologies. The majority of the 6500+ of\nthe world’s languages are not studied by the NLP\ncommunity, since most have few or no annotated\ndatasets, making systems’ development challeng-\ning.\nThe development of such models is a matter of\nhigh importance for the inclusion of communities,\nthe preservation of endangered languages and more\ngenerally to support the rise of tailored NLP ecosys-\ntems for such languages (Schmidt and Wiegand,\n2017; Stecklow, 2018; Seddah et al., 2020). In\nthat regard, the advent of the Universal Dependen-\ncies project (Nivre et al., 2016) and the WikiAnn\ndataset (Pan et al., 2017) have greatly increased\nthe number of covered languages by providing an-\nnotated datasets for more than 90 languages for\ndependency parsing and 282 languages for NER.\nRegarding modeling approaches, the emergence\nof multilingual representation models, ﬁrst with\nstatic word embeddings (Ammar et al., 2016) and\nthen with language model-based contextual rep-\nresentations (Devlin et al., 2019; Conneau et al.,\n2020) enabled transfer from high to low-resource\nlanguages, leading to signiﬁcant improvements in\ndownstream task performance (Rahimi et al., 2019;\nKondratyuk and Straka, 2019). Furthermore, in\ntheir most recent forms, these multilingual mod-\nels process tokens at the sub-word level (Kudo\nand Richardson, 2018). As such, they work in an\nopen vocabulary setting, only constrained by the\npretraining character set. This ﬂexibility enables\nsuch models to process any language, even those\nthat are not part of their pretraining data.\nWhen it comes to low-resource languages, one\ndirection is to simply train contextualized embed-\nding models on whatever data is available. An-\nother option is to adapt/ﬁne-tune a multilingual\npretrained model to the language of interest. We\nbrieﬂy discuss these two options.\nPretraining language models on a small\namount of raw data Even though the amount\nof pretraining data seems to correlate with down-\nstream task performance (e.g. compare BERT and\nRoBERTa (Liu et al., 2020)), several attempts have\nshown that training a model from scratch can be ef-\nﬁcient even if the amount of data in that language is\nlimited. Indeed, Ortiz Suárez et al. (2020) showed\nthat pretraining ELMo models (Peters et al., 2018)\non less than 1GB of text leads to state-of-the-art\nperformance while Martin et al. (2020) showed that\npretraining a BERT model on as few as 4GB of di-\nverse enough data results in state-of-the-art perfor-\nmance. Micheli et al. (2020) further demonstrated\nthat decent performance was achievable with only\n100MB of raw text data.\nAdapting large-scale models for low-resource\nlanguages Multilingual language models can be\nused directly on unseen languages, or they can\nalso be adapted using unsupervised methods. For\nexample, Han and Eisenstein (2019) successfully\nused unsupervised model adaptation of the English\nBERT model to Early Modern English for sequence\nlabeling. Instead of ﬁne-tuning the whole model,\nPfeiffer et al. (2020) recently showed that adapter\nlayers (Houlsby et al., 2019) can be injected into\nmultilingual language models to provide parameter\nefﬁcient task and language transfer.\nStill, as of today, the availability of monolin-\ngual or multilingual language models is limited to\napproximately 120 languages, leaving many lan-\nguages without access to valuable NLP technology,\nalthough some are spoken by millions of people,\nincluding Bambara and Sorani Kurdish, or are an\nofﬁcial language of the European Union, like Mal-\ntese.\nWhat can be done for unseen languages?Un-\nseen languages strongly vary in the amount of\navailable data, in their script (many languages use\nnon-Latin scripts such as Sorani Kurdish and Min-\ngrelian), and in their morphological or syntactical\nproperties (most largely differ from high-resource\nIndo-European languages). This makes the design\nof a methodology to build contextualized models\nfor such languages challenging at best. In this work,\nby experimenting with 15 typologically diverse un-\nseen languages, (i) we show that there is a diversity\nof behavior depending on the script, the amount of\navailable data, and the relation to the pretraining\nlanguages; (ii) Focusing on the unseen languages\nthat lag in performance compared to their easier-to-\nhandle counterparts, we show that the script plays a\n450\ncritical role in the transfer abilities of multilingual\nlanguage models. Transliterating such languages\nto a script which is used by a related language seen\nduring pretraining can lead to signiﬁcant improve-\nment in downstream performance.\n3 Experimental Setting\nWe will refer to any languages that are not covered\nby pretrained language models as “unseen.” We\nselect a small portion of those languages within a\nlarge scope of language families and scripts. Our\nselection is constrained to 15 typologically diverse\nlanguages for which we have evaluation data for\nat least one of our three downstream tasks. Our\nselection includes low-resource Indo-European and\nUralic languages, as well as members of the Bantu,\nSemitic, and Turkic families. None of these 15\nlanguages are included in the pretraining corpora of\nmBERT. Information about their scripts, language\nfamilies, and amount of available raw data can be\nfound in the Appendix in Table 12.\n3.1 Raw Data\nTo perform pretraining and ﬁne-tuning on mono-\nlingual data, we use the deduplicated datasets from\nthe OSCAR project (Ortiz Suárez et al., 2019).\nOSCAR is a corpus extracted from a Common\nCrawl Web snapshot. 2 It provides a signiﬁcant\namount of data for all the unseen languages we\nwork with, except for Buryat, Meadow Mari, Erzya\nand Livvi for which we use Wikipedia dumps and\nfor Narabizi, Naija and Faroese, for which we use\ndata collected by Seddah et al. (2020), Caron et al.\n(2019) and Biemann et al. (2007) respectively.\n3.2 Non-contextual Baselines\nFor parsing and POS tagging, we use the UDPipe\nfuture system (Straka, 2018) as our baseline. This\nmodel is a LSTM-based (Hochreiter and Schmid-\nhuber, 1997) recurrent architecture trained with\npretrained static word embedding (Mikolov et al.,\n2013) (hence our non-contextual characterization)\nalong with character-level embeddings. This sys-\ntem was ranked in the very ﬁrst positions for pars-\ning and tagging in the CoNLL shared task 2018\n(Zeman and Haji ˇc, 2018). For NER we use the\nLSTM-CRF model with character and word level\nembedding using Qi et al. (2020) implementation.\n2http://commoncrawl.org/\n3.3 Language Models\nIn all our study, we train our language models using\nthe Transformers library (Wolf et al., 2020).\nMLM from scratch The ﬁrst approach we eval-\nuate is to train a dedicated language model from\nscratch on the available raw data we have. To do\nso, we train a language-speciﬁc SentencePiece tok-\nenizer (Kudo and Richardson, 2018) before train-\ning a Masked-Language Model (MLM) using the\nRoBERTa (base) architecture and objective func-\ntions (Liu et al., 2019). As we work with signiﬁ-\ncantly smaller pretraining sets than in the original\nsetting, we reduce the number of layers to 6 layers\nin place of the original 12 layers.\nMultilingual Language Models We want to as-\nsess how large-scale multilingual language models\ncan be used and adapted to languages that are not\nin their pretraining corpora. We work with the\nmultilingual version of BERT ( mBERT) trained\non the concatenation of Wikipedia corpora in 104\nlanguages (Devlin et al., 2019). We also ran ex-\nperiments with the XLM-R base version (Conneau\net al., 2020) trained on 100 languages using data\nextracted from the Web. As the observed behav-\niors are very similar between both models, we only\nreport results using mBERT. Note that mBERT is\nhighly biased toward Indo-Europeans languages\nwritten in the Latin script. More than 77% of the\nsubword vocabulary are in the Latin script while\nonly 1% are in the Georgian script (Ács, 2019).\nAdapting Multilingual Language Models to un-\nseen languages with MLM-TUNING Follow-\ning previous work (Han and Eisenstein, 2019;\nKarthikeyan et al., 2019; Pfeiffer et al., 2020),\nwe adapt large-scale multilingual models by ﬁne-\ntuning them with their Mask-Language-Model ob-\njective directly on the available raw data in the\nunseen target language. We refer to this process\nas MLM- TUNING . We will refer to a MLM-tuned\nmBERT model as mBERT+MLM.\n3.4 Downstream Tasks\nWe perform experiments on POS tagging, Depen-\ndency Parsing (DEP), and Name Entity Recogni-\ntion (NER). We use annotated data from the Uni-\nversal Dependency project (Nivre et al., 2016) for\nPOS tagging and parsing, and the WikiAnn dataset\n(Pan et al., 2017) for NER. For POS tagging and\nNER, we append a linear classiﬁer layer on top of\n451\nUPOS LAS NERModel MBERTMBERT+MLM MLM BaselineMBERTMBERT+MLM MLM BaselineMBERTMBERT+MLM MLM Baseline\nFaroese 96.3 96.5 91.1 95.4 84.0 86.4 67.6 83.1 52.1 58.3 39.3 44.8Naija 89.3 89.6 87.1 89.2 71.5 69.2 63.0 68.3 - - - -Swiss German 76.7 78.7 65.4 75.2 41.2 69.6 30.0 32.2 - - - -Mingrelian - - - - - - - - 53.6 68.4 42.0 48.2\nTable 1: Easy LanguagesPOS, Parsing and NER scores comparing mBERT, mBERT+MLM and mono-\nlingual MLM to strong non-contextual baselines when trained and evaluated on unseen languages. Easy\nLanguages are the ones on which mBERT outperforms strong baselines out-of-the-box. Baselines are\nLSTM based models from UDPipe-future (Straka, 2018) for parsing and POS tagging and Stanza (Qi\net al., 2020) for NER.\nthe language model. For parsing, following Kon-\ndratyuk and Straka (2019), we append a Bi-Afﬁne\nGraph prediction layer (Dozat and Manning, 2017).\nWe refer to the process of ﬁne-tuning a language\nmodel in a task-speciﬁc way as TASK -TUNING .3\n3.5 Dataset Splits\nFor each task and language, we use the provided\ntraining, validation and test dataset split except for\nthe ones that have less than 500 training sentences.\nIn this case, we concatenate the training and test set\nand perform 8-folds cross-Validation and use the\nvalidation set for early stopping. If no validation\nset is available, we isolate one of the folds for vali-\ndation and report the test scores as the average of\nthe other folds. This enables us to train on at least\n500 sentences in all our experiments (except for\nSwiss German for which we only have 100 training\nexamples) and reduce the impact of the annotated\ndataset size on our analysis. Since cross-validation\nresults in training on very limited number of exam-\nples, we refer to training in this cross-validation\nsetting as few-shot learning.\n4 The Three Categories of Unseen\nLanguages\nFor each unseen language and each task, we ex-\nperiment with our three modeling approaches:\n(a) Training a language model from scratch on\nthe available raw dataand then ﬁne-tuning it on\nany available annotated data in the target language.\n(b) Fine-tuning mBERT with TASK -TUNING di-\nrectly on the target language. (c) Finally, adapting\nmBERT to the unseen language using MLM-\nTUNING before ﬁne-tuning it in a supervised way\non the target language. We then compare all these\nexperiments to our non-contextual strong baselines.\nBy doing so, we can assess if language models are\n3Details about optimization can be found in Appendix B\nFigure 1: Visualizing our Typology of Unseen Lan-\nguages. (X,Y) positions are computed for each\nlanguage and each task as follows: given the score\nof mBERT denoted s and s0 the baseline score:\nX = smBERT −s0\ns0\n, Y = smBERT +MLM −s0\ns0\nEasy Languages are the ones on which mBERT\nperforms better than the baseline without MLM-\nTUNING and Intermediate languages are the ones\nthat require MLM- TUNING . For Hard languages,\nmBERT under-performs the baselines in all set-\ntings.\na practical solution to handle each of these unseen\nlanguages.\nInterestingly we ﬁnd a large diversity of behav-\niors across languages regarding those language\nmodel training techniques. As summarized in Fig-\nure 1, we observe three clear clusters of languages.\nThe ﬁrst cluster, which we dub “Easy\", corre-\nsponds to the languages that do not require extra\nMLM- TUNING for mBERT to achieve good per-\nformance. mBERT has the modeling abilities to\nprocess such languages without relying on raw data\nand can outperform strong non-contextual base-\nlines as such. In the second cluster, the “Interme-\ndiate\" languages require MLM- TUNING . mBERT\nis not able to beat strong non-contextual baselines\nusing only TASK -TUNING , but MLM- TUNING en-\nables it to do so. Finally, Hard languages are those\non which mBERT fails to deliver any decent per-\n452\nUPOS LAS NERModel MBERT MBERT+MLM MLM BaselineMBERT MBERT+MLM MLM BaselineMBERT MBERT+MLM MLM Baseline\nMaltese 92.0 96.4 92.05 96.0 74.4 82.1 66.5 79.7 61.2 66.7 62.5 63.1Narabizi 81.6 84.2 71.3 84.2 56.5 57.8 41.8 52.8 - - - -Bambara 90.2 92.6 78.1 92.3 71.8 75.4 46.4 76.2 - - -Wolof 92.8 95.2 88.4 94.1 73.3 77.9 62.8 77.0 - - - -Erzya 89.3 91.2 84.4 91.1 61.2 66.6 47.8 65.1 - - - -Livvi 83.0 85.5 81.1 84.1 36.3 42.3 35.2 40.1 - - - -Mari - - - - - - - - 55.2 57.6 44.0 56.1\nTable 2: Intermediate LanguagesPOS, Parsing and NER scores comparing mBERT, mBERT+MLM\nand monolingual MLM to strong non-contextual baselines when trained and evaluated on unseen languages.\nIntermediate Languages are the ones for which mBERT requires MLM- TUNING to outperform the\nbaselines.\nformance even after MLM- and TASK - ﬁne-tuning.\nmBERT simply does not have the capacity to learn\nand process such languages.\nWe emphasize that our categorization of unseen\nlanguages is only based on the relative performance\nof mBERT after ﬁne-tuning compared to strong\nnon-contextual baseline models. We leave for fu-\nture work the analysis of the absolute performance\nof the model on such languages (e.g. analysing the\nimpact of the ﬁne-tuning data set size on mBERT’s\ndownstream performance).\nIn this section, we present our results in detail in\neach of these language clusters and provide insights\ninto their linguistic properties.\n4.1 Easy\nEasy languages are the ones on which mBERT de-\nlivers high performance out-of-the-box, compared\nto strong baselines. We classify Faroese, Swiss\nGerman, Naija and Mingrelian as easy languages\nand report performance in Table 1.\nWe ﬁnd that those languages match two condi-\ntions:\n•They are closely related to languages used\nduring MLM pretraining\n•These languages use the same script as such\nclosely related languages.\nSuch languages beneﬁt from multilingual models,\nas cross-lingual transfer is easy to achieve and\nhence quite effective.\nMore details about those languages can be found\nin Appendix C.\n4.2 Intermediate\nThe second type of languages (which we dub “In-\ntermediate”) are generally harder to process for\npretrained MLMs out-of-the-box. In particular, pre-\ntrained multilingual language models are typically\noutperformed by a non-contextual strong baselines.\nStill, MLM- TUNING has an important impact and\nleads to usable state-of-the-art models.\nA good example of such an intermediate lan-\nguage is Maltese, a member of the Semitic lan-\nguage but using the Latin script. Maltese has not\nbeen seen by mBERT during pretraining. Other\nSemitic languages though, namely Arabic and He-\nbrew, have been included in the pretraining lan-\nguages. As seen in Table 2, the non-contextual\nbaseline outperforms mBERT. Additionally, a\nmonolingual MLM trained on only 50K sentences\nmatches mBERT performance for both NER and\nPOS tagging. However, the best results are reached\nwith MLM- TUNING : the proper use of monolin-\ngual data and the advantage of similarity to other\npretraining languages render Maltese a tackle-able\nlanguage as shown by the performance gain over\nour strong non-contextual baselines.\nOur Maltese dependency parsing results are in\nline with those of Chau et al. (2020), who also\nshowed that MLM- TUNING leads to signiﬁcant im-\nprovements. They also additionally showed that\na small vocabulary transformation allowed ﬁne-\ntuning to be even more effective and gain 0.8 LAS\npoints more. We further discuss the vocabulary\nadaptation technique of Chau et al. (2020) in sec-\ntion 6.\nWe consider Narabizi (Seddah et al., 2020), an\nArabic dialect spoken in North-Africa written in\nthe Latin script and code-mixed with French, to fall\nin the same Intermediate category, because it fol-\nlows the same pattern. For both POS tagging and\nparsing, the multilingual models outperform the\nmonolingual NarabiziBERT. In addition, MLM-\nTUNING leads to signiﬁcant improvements over the\nnon-language-tuned mBERT baseline, also outper-\nforming the non-contextual dependency parsing\nbaseline.\nWe also categorize Bambara, a Niger-Congo\nBantu language spoken in Mali and surrounding\ncountries, as Intermediate, relying mostly on the\n453\nUPOS LAS NERModel MBERT MBERT+MLM MLM BaselineMBERT MBERT+MLM MLM BaselineMBERT MBERT+MLM MLM Baseline\nUyghur 77.0 88.4 87.4 90.0 45.5 48.9 57.3 67.9 24.3 34.6 41.4 53.8Sindhi - - - - - - - - 42.3 47.9 45.2 51.4Sorani Kurdish - - - - - - - - 70.4 75.6 80.6 80.5\nTable 3: Hard Languages POS, Parsing and NER scores comparing mBERT, mBERT+MLM and\nmonolingual MLM to strong non-contextual baselines when trained and evaluated on unseen languages.\nHard Languages are the ones for which mBERT fails to reach decent performance even after MLM-\nTUNING .\nPOS tagging results which follow similar patterns\nas Maltese and Narabizi. We note that the Bam-\nbaraBERT that we trained achieves notably poor\nperformance compared to the non-contextual base-\nline, a fact we attribute to the extremely low amount\nof available data (1000 sentences only). We also\nnote that the non-contextual baseline is the best\nperforming model for dependency parsing, which\ncould also potentially classify Bambara as a “Hard\"\nlanguage instead.\nOur results in Wolof follow the same pattern.\nThe non-contextual baseline achieves a 77.0 in LAS\noutperforming mBERT. However, MLM- TUNING\nachieves the highest score of 77.9.\nThe importance of script We now turn our fo-\ncus to Uralic languages. Finnish, Estonian, and\nHungarian are high-resource representatives of this\nlanguage family that are typically included in multi-\nlingual LMs, also having task-tuning data available\nin large quantities. However, for several smaller\nUralic languages, task-tuning data are generally\nvery scarce.\nWe report in Table 2 the performance for two\nlow-resource Uralic languages, namely Livvi and\nErzya using 8-fold cross-validation, with each run\nonly using around 700 training instances. Note\nthe striking difference between the parsing perfor-\nmance (LAS) of mBERT on Livvi, written with\nthe Latin script, and on Erzya that uses the Cyril-\nlic script. This suggests that the script could be\nplaying a critical role when transferring to those\nlanguages. We explore this hypothesis in detail in\nsection 5.2.\n4.3 Hard\nThe last category of the hard unseen language is per-\nhaps the most interesting one, as these languages\nare very hard to process. mBERT is outperformed\nby non-contextual baselines as well as by mono-\nlingual language models trained from scratch on\nthe available raw data. At the same time, MLM-\nTUNING on the available raw data has a minimal\nimpact on performance.\nUyghur, a Turkic language with about 10-15\nmillion speakers in central Asia, is a prime ex-\nample of a hard language for current models. In\nour experiments, outlined in Table 3, the non-\ncontextual baseline outperforms all contextual vari-\nants, both monolingual and multilingual, in all the\ntasks with up to 20 points difference compared to\nmBERT for parsing. Additionally, the monolin-\ngual UyghurBERT trained on only 105K sentences\noutperforms mBERT even after MLM-TUNING .\nWe attribute this discrepancy to script differ-\nences: Uyghur uses the Perso-Arabic script, when\nthe other Turkic languages that were part of\nmBERT pretraining use either the Latin (e.g. Turk-\nish) or the Cyrillic script (e.g. Kazakh).\nSorani Kurdish (also known as Central Kurdish)\nis a similarly hard language, mainly spoken in Iraqi\nKurdistan by around 8 million speakers, which uses\nthe Sorani alphabet, a variant of the Arabic script.\nWe can solely evaluate on the NER task, where the\nnon-contextual baseline and the monolingual So-\nraniBERT perform similarly around 80.5 F1-score\noutperforming signiﬁcantly mBERT which only\nreaches 70.4 in F1-score. MLM- TUNING on 380K\nsentences of Sorani texts improves mBERT perfor-\nmance to 75.6 F1-score, but it is still lagging behind\nthe baseline. Our results in Sindhi follow the same\npattern. The non-contextual baseline achieves a\n51.4 F1-score outperforming with a large margin\nour language models (a monolingual SindhiBERT\nachieves an F1-score of 45.2, and mBERT is worse\nat 42.3).\n5 Tackling Hard Languages with\nMultilingual Language Models\nOur intermediate Uralic language results provide\ninitial supporting evidence for our argument on\nthe importance of having pretrained LMs on lan-\nguages with similar scripts, even for generally high-\nresource language families. Our hypothesis is that\nthe script is a key element for language models to\n454\nModel POS LAS NER Model NER\nUyghur (Arabic→Latin) Sorani (Arabic→Latin)\nUyghurBERT 87.4→86.2 57.3 →54.6 41.4 →41.7 SoraniBERT 80.6 →78.9\nmBERT 77.0→87.9 45.7 →65.0 24.3 →35.7 mBERT 70.5 →77.8\nmBERT+MLM 77.3→89.8 48.9→66.8 34.7→55.2 mBERT+MLM 75.6 →82.7\nBuryat (Cyrillic→Latin) Meadow Mari (Cyrillic→Latin)\nBuryatBERT 75.8→75.8 31.4 →31.4 – MariBERT 44.0 →45.5\nmBERT 83.9→81.6 50.3 →45.8 – mBERT 55.2 →58.2\nmBERT+MLM 86.5→84.6 52.9→51.9 – mBERT+MLM 57.6 →65.9\nErzya (Cyrillic→Latin) Mingrelian (Georgian→Latin)\nErzyaBERT 84.4→84.5 47.8 →47.8 – MingrelianBERT 42.0 →42.2\nmBERT 89.3→88.2 61.2 →58.3 – mBERT 53.6 →41.8\nmBERT+MLM 91.2→90.5 66.6→65.5 – mBERT+MLM 68.4→62.6\nTable 4: Transliterating low-resource languages into the Latin script leads to signiﬁcant improvements in\nlanguages like Uyghur, Sorani, and Meadow Mari. For languages like Erzya and Buryat transliteration,\ndoes not signiﬁcantly inﬂuence results, while it does not help for Mingrelian. In all cases, mBERT+MLM\nis the best approach.\ncorrectly process unseen languages.\nTo test this hypothesis, we assess the ability\nof mBERT to process an unseen language af-\nter transliterating it to another script present in\nthe pretraining data. We experiment on six lan-\nguages belonging to four language families: Erzya,\nBruyat and Meadow Mari (Uralic), Sorani Kur-\ndish (Iranian, Indo-European), Uyghur (Turkic)\nand Mingrelian (Kartvelian). We apply the fol-\nlowing transliterations:\n•Erzya/Buryat/Mari: Cyrillic − →Latin Script\n•Uyghur: Arabic Script − →Latin Script\n•Sorani: Arabic Script − →Latin Script\n•Mingrelian: Georgian Script − →Latin Script\n5.1 Linguistically-motivated transliteration\nThe strategy we used to transliterate the above-\nlisted language is speciﬁc to the purpose of our\nexperiments. Indeed, our goal is for the model to\ntake advantage of the information it has learned\nduring training on a related language written in\nthe Latin script. The goal of our transliteration is\ntherefore to transcribe each character in the source\nscript, which we assume corresponds to a phoneme,\ninto the most frequent (sometimes only) way this\nphoneme is rendered in the closest related language\nwritten in the Latin script, hereafter the target lan-\nguage. This process is not a transliteration strictly\nspeaking, and it needs not be reversible. It is not a\nphonetization either, but rather a way to render the\nsource language in a way that maximizes the sim-\nilarity between the transliterated source language\nand the target language.\nWe have manually developed transliteration\nscripts for Uyghur and Sorani Kurdish, using re-\nspectively Turkish and Kurmanji Kurdish as target\nlanguages, only Turkish being one of the languages\nused to train mBERT. Note however that Turkish\nand Kurmanji Kurdish share a number of conven-\ntions for rendering phonemes in the Latin script\n(for instance, /S/, rendered in English by “sh”, is\nrendered in both languages by “¸ s”; as a result, the\nArabic letter “/sheenisolated”, used in both languages, is ren-\ndered as “¸ s” by both our transliteration scripts).\nAs for Erzya, Buryat and Mari, we used the read-\nily available transliteration package transliterate,4\nwhich performs a standard transliteration. 5 We\nused the Russian transliteration module, as it cov-\ners the Cyrillic script. Finally, for our control ex-\nperiments on Mingrelian, we used the Georgian\ntransliteration module from the same package.\n5.2 Transfer via Transliteration\nWe train mBERT with MLM- TUNING and TASK -\nTUNING as well as monolingual BERT model\ntrained from scratch on the transliterated data.\nWe also run controlled experiments on high-\nresource languages written in the Latin script on\nwhich mBERT was pretrained on, namely Arabic,\nJapanese and Russian (reported in Table 5).\nOur results with and without transliteration are\nlisted in Table 4. Transliteration for Sorani and\nUyghur has a noticeable positive impact. For in-\nstance, transliterating Uyghur to Latin leads to an\nimprovement of 16 points in parsing and 20 points\n4https://pypi.org/project/transliterate/\n5In future work, we intend to develop dedicated translitera-\ntion scripts using the strategy described above, and to compare\nthe results obtained with it with those described here.\n455\nin NER. For one of the low-resource Uralic lan-\nguages, Meadow Mari, we observe an 8 F1-score\npoints improvement on NER, while for other Uralic\nlanguages like Erzya the effect of transliteration is\nvery minor. The only case where transliteration to\nthe Latin script leads to a drop in performance for\nmBERT and mBERT+MLM is Mingrelian.\nWe interpret our results as follows. When run-\nning MLM- TUNING and TASK -TUNING , mBERT\nassociates the target unseen language to a set of\nsimilar languages seen during pretraining based on\nthe script. In consequence, mBERT is not able to\nassociate a language to its related language if they\nare not written in the same script. For instance,\ntransliterating Uyghur enables mBERT to match\nit to Turkish, a language which accounts for a siz-\nable portion of mBERT pretraining. In the case\nof Mingrelian, transliteration has the opposite ef-\nfect: transliterating Mingrelian in the Latin script\nis harming the performance as mBERT is not able\nto associate it to Georgian which is seen during\npretraining and uses the Georgian script.\nThis is further supported by our experiments\non high resource languages (cf. Table 5). When\ntransliterating pretrained languages such as Arabic,\nRussian or Japanese, mBERT is not able to com-\npete with the performance reached when using the\nscript seen during pretraining. Transliterating the\nArabic script and the Cyrillic script to Latin does\nnot automatically improve mBERT performance as\nit does for Sorani, Uyghur and Meadow Mari. For\ninstance, transliterating Arabic to the Latin script\nleads to a drop in performance of 1.5, 4.1 and 6.9\npoints for POS tagging, parsing and NER respec-\ntively.6\nOur ﬁndings are generally in line with previous\nwork. Transliteration to English speciﬁcally (Lin\net al., 2016; Durrani et al., 2014) and named entity\ntransliteration (Kundu et al., 2018; Grundkiewicz\nand Heaﬁeld, 2018) has been proven useful for\ncross-lingual transfer in tasks like NER, entity link-\ning (Rijhwani et al., 2019), morphological inﬂec-\ntion (Murikinati et al., 2020), and Machine Trans-\nlation (Amrhein and Sennrich, 2020).\nThe transliteration approach provides a viable\npath for rendering large pretrained models like\nmBERT useful for all languages of the world. In-\ndeed, as reported in Table 4, transliterating both\nUyghur and Sorani leads to matching or outper-\n6Details and complete results on these controlled experi-\nments can be found in Appendix E.\nOriginal Script→Latin Script\nModel POS LAS NER\nArabic 96.4→94.9 82.9 →78.8 87.8 →80.9\nRussian 98.1→96.0 88.4 →84.5 88.1 →86.0\nJapanese 97.4→95.7 88.5 →86.9 61.5 →55.6\nTable 5: mBERT TASK -TUNED on high resource\nlanguages for POS tagging, parsing and NER. We\ncompare ﬁne-tuning done on data written the origi-\nnal language script with ﬁne-tuning done on Latin\ntransliteration. In all cases, transliteration degrades\ndownstream performance.\nforming the performance of non-contextual strong\nbaselines and deliver usable models (e.g. +12.5\nPOS accuracy in Uyghur).\n6 Discussion and Conclusion\nPretraining ever larger language models is a re-\nsearch direction that is currently receiving a lot\nof attention and resources from the NLP research\ncommunity (Raffel et al., 2019; Brown et al., 2020).\nStill, a large majority of human languages are\nunder-resourced making the development of mono-\nlingual language models very challenging in those\nsettings. Another path is to build large scale mul-\ntilingual language models.7 However, such an ap-\nproach faces the inherent zipﬁan structure of human\nlanguages, making the training of a single model\nto cover all languages an unfeasible solution (Con-\nneau et al., 2020). Reusing large scale pretrained\nlanguage models for new unseen languages seems\nto be a more promising and reasonable solution\nfrom a cost-efﬁciency and environmental perspec-\ntive (Strubell et al., 2019).\nRecently, Pfeiffer et al. (2020) proposed to use\nadapter layers (Houlsby et al., 2019) to build pa-\nrameter efﬁcient multilingual language models for\nunseen languages. However, this solution brings\nno signiﬁcant improvement in the supervised set-\nting, compared to a more simple Masked-Language\nModel ﬁnetuning. Furthermore, developing a lan-\nguage agnostic adaptation method is an unreason-\nable wish with regard to the large typological diver-\nsity of human languages.\nOn the other hand, the promising vocabulary\nadaptation technique of Chau et al. (2020) which\nleads to good dependency parsing results on unseen\nlanguages when combined with task-tuning has\n7Even though we explore a different research direction,\nrecent advances in small scale and domain speciﬁc language\nmodels suggest such models could also have an important\nimpact for those languages (Micheli et al., 2020).\n456\nso far been tested only on Latin script languages\n(Singlish and Maltese). We expect that it will be\northogonal to our transliteration approach, but we\nleave for future work the study of its applicability\nand efﬁcacy on more languages and tasks.\nIn this context, we bring empirical evidence to\nassess the efﬁciency of language models pretrain-\ning and adaptation methods on 15 low-resource and\ntypologically diverse unseen languages. Our results\nshow that the “Hard\" languages are currently out-\nof-the-scope of any currently available language\nmodels and are therefore left outside of the current\nNLP progress. By focusing on those, we ﬁnd that\nthis challenge is mostly due to the script. Transliter-\nating them to a script that is used by a related higher\nresource language on which the language model\nhas been pretrained on leads to large improvements\nin downstream performance. Our results shed some\nnew light on the importance of the script in mul-\ntilingual pretrained models. While previous work\nsuggests that multilingual language models could\ntransfer efﬁciently across scripts in zero-shot set-\ntings (Pires et al., 2019; Karthikeyan et al., 2019),\nour results show that such cross-script transfer is\npossible only if the model has seen related lan-\nguages in the same script during pretraining.\nOur work paves the way for a better understand-\ning of the mechanics at play in cross-language\ntransfer learning in low-resource scenarios. We\nstrongly believe that our method can contribute to\nbootstrapping NLP resources and tools for low-\nresource languages, thereby favoring the emer-\ngence of NLP ecosystems for languages currently\nunder-served by the NLP community.\nAcknowledgments\nThe Inria authors were partly funded by two\nFrench Research National agency projects, namely\nprojects PARSITI (ANR-16-CE33-0021) and\nSoSweet (ANR-15-CE38-0011), as well as by\nBenoit Sagot’s chair in the PRAIRIE institute as\npart of the “Investissements d’avenir” programme\nunder the referenceANR-19-P3IA-0001. Antonios\nAnastasopoulos is generously supported by NSF\nAward 2040926 and is also thankful to Graham\nNeubig for very insightful initial discussions on\nthis research direction.\n.\n457\nReferences\nJudit Ács. 2019. Exploring BERT’s vocab-\nulary. Http://juditacs.github.io/2019/02/19/bert-\ntokenization-stats.html.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov, Guil-\nlaume Lample, Chris Dyer, and Noah A Smith.\n2016. Massively multilingual word embeddings.\narXiv:1602.01925.\nChantal Amrhein and Rico Sennrich. 2020. On Roman-\nization for model transfer between scripts in neural\nmachine translation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n2461–2469, Online. Association for Computational\nLinguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nChris Biemann, Gerhard Heyer, Uwe Quasthoff, and\nMatthias Richter. 2007. The Leipzig Corpora\ncollection-monolingual corpora of standard size. Pro-\nceedings of Corpus Linguistic, 2007.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv:2005.14165.\nBernard Caron, Marine Courtin, Kim Gerdes, and Syl-\nvain Kahane. 2019. A surface-syntactic UD treebank\nfor Naija. In Proceedings of the 18th International\nWorkshop on Treebanks and Linguistic Theories (TLT,\nSyntaxFest 2019), pages 13–24.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, and\nJorge Pérez. 2020. Spanish pre-trained BERT model\nand evaluation data. In PML4DC at ICLR 2020.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1324–1334, Online. Association for Computational\nLinguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nNadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp\nKoehn. 2014. Integrating an unsupervised transliter-\nation model into statistical machine translation. In\nProceedings of the 14th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, volume 2: Short Papers , pages 148–153,\nGothenburg, Sweden. Association for Computational\nLinguistics.\nRoman Grundkiewicz and Kenneth Heaﬁeld. 2018.\nNeural machine translation techniques for named en-\ntity transliteration. In Proceedings of the Seventh\nNamed Entities Workshop, pages 89–94, Melbourne,\nAustralia. Association for Computational Linguistics.\nHarald Hammarström. 2016. Linguistic diversity and\nlanguage evolution. Journal of Language Evolution,\n1(1):19–29.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4238–4248, Hong Kong,\nChina. Association for Computational Linguistics.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\narXiv:1902.00751.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nK Karthikeyan, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2019. Cross-lingual ability of multilin-\ngual BERT: An empirical study. In International\nConference on Learning Representations.\n458\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nDan Kondratyuk and Milan Straka. 2019. 75 languages,\n1 model: Parsing universal dependencies universally.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2779–\n2795, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nSoumyadeep Kundu, Sayantan Paul, and Santanu Pal.\n2018. A deep learning based approach to translitera-\ntion. In Proceedings of the Seventh Named Entities\nWorkshop, pages 79–83, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation\nof deep bidirectional multilingual transformers for\nRussian language. arXiv:1905.07213.\nYing Lin, Xiaoman Pan, Aliya Deri, Heng Ji, and Kevin\nKnight. 2016. Leveraging entity linking and related\nlanguage projection to improve name transliteration.\nIn Proceedings of the Sixth Named Entity Workshop,\npages 1–10, Berlin, Germany. Association for Com-\nputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv:1907.11692.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7203–\n7219, Online. Association for Computational Lin-\nguistics.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training data\nvolume for compact language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n7853–7858, Online. Association for Computational\nLinguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn Advances in neural information processing sys-\ntems, pages 3111–3119.\nNikitha Murikinati, Antonios Anastasopoulos, and Gra-\nham Neubig. 2020. Transliteration for cross-lingual\nmorphological inﬂection. In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 189–197, Online. Association for Computa-\ntional Linguistics.\nJoakim Nivre, Marie-Catherine De Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajic, Christopher D Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, et al. 2016. Universal dependencies\nv1: A multilingual treebank collection. In Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC’16), pages\n1659–1666.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 1703–\n1714, Online. Association for Computational Linguis-\ntics.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for process-\ning huge corpora on medium to low resource infras-\ntructures. In Proceedings of the Workshop on Chal-\nlenges in the Management of Large Corpora (CMLC-\n7) 2019. Cardiff, 22nd July 2019 , pages 9 – 16,\nMannheim. Leibniz-Institut für Deutsche Sprache.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proc. of NAACL.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\n459\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4996–5001.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 101–108, Online. As-\nsociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv:1910.10683.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 151–164, Florence,\nItaly. Association for Computational Linguistics.\nShruti Rijhwani, Jiateng Xie, Graham Neubig, and\nJaime Carbonell. 2019. Zero-shot neural transfer\nfor cross-lingual entity linking. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, vol-\nume 33, pages 6924–6931.\nAnna Schmidt and Michael Wiegand. 2017. A survey\non hate speech detection using natural language pro-\ncessing. In Proceedings of the Fifth International\nWorkshop on Natural Language Processing for So-\ncial Media, pages 1–10, Valencia, Spain. Association\nfor Computational Linguistics.\nStefan Schweter. 2020. BERTurk - BERT models for\nTurkish.\nDjamé Seddah, Farah Essaidi, Amal Fethi, Matthieu\nFuteral, Benjamin Muller, Pedro Javier Ortiz Suárez,\nBenoît Sagot, and Abhishek Srivastava. 2020. Build-\ning a user-generated content North-African Arabizi\ntreebank: Tackling hell. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1139–1150, Online. Association\nfor Computational Linguistics.\nSteve Stecklow. 2018. Why Facebook is losing the\nwar on hate speech in Myanmar, Reuters. https:\n//www.reuters.com/investigates/spe\ncial-report/myanmar-facebook-hate/ .\nMilan Straka. 2018. UDPipe 2.0 prototype at CoNLL\n2018 UD shared task. In Proceedings of the CoNLL\n2018 Shared Task: Multilingual Parsing from Raw\nText to Universal Dependencies, pages 197–207.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. Bertje: A dutch BERT model.\narXiv:1912.09582.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nDaniel Zeman and Jan Haji ˇc, editors. 2018. Proceed-\nings of the CoNLL 2018 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies.\nAssociation for Computational Linguistics, Brussels,\nBelgium.\nA Languages\nWe list the 15 typologically diverse unseen lan-\nguages we experiment with in Table 12 with in-\nformation on their language family, script, origin\nand number of sentences available along with the\ncategories we classiﬁed them in.\nData Sources We base our experiments on data\noriginated from two sources. The Universal Depen-\ndency project (Nivre et al., 2016) downloadable\nhere https://lindat.mff.cuni.cz/rep\nository/xmlui/handle/11234/1-2988\nand the WikiNER dataset (Pan et al., 2017). We\nalso use of the CoNLL-2003 shared task NER\nEnglish dataset https://www.clips.uant\nwerpen.be/conll2003/\nB Reproducibility\nInfrastructure Our experiments were ran on a\nshared cluster on the equivalent of 15 Nvidia Tesla\nT4 GPUs.8\nOptimization For all pretraining and ﬁne-tuning\nruns, we use the Adam optimizer (Kingma and\nBa, 2015). For ﬁne-tuning, following Devlin et al.\n(2019), we only back-propagate through the ﬁrst\n8https://www.nvidia.com/en-sg/data-center/tesla-t4/\n460\nParams. Parsing NER POS Bounds\nbatch size 32 16 16 [1,256]\nlearning rate 5e-5 3.5e-5 5e-5 [1e-6,1e-3]\nepochs (best) 15 6 6 [1, + inf]\n#grid 60 60 180 -\nRun-time 32 24 75 -\nTable 6: Fine-tuning best hyper-parameters for\neach task as selected on the validation set with\nbounds. #grid: number of grid search trial. Run-\ntime is reported in average for training and evalua-\ntion. Run-time indicated in minutes.\nParameter Value\nbatch size 64\nlearning rate 5e-5\noptimizer Adam\nwarmup linear\nwarmup steps 10% total\nepochs (best of) 10\nTable 7: Unsupervised ﬁne-tuning hyper-\nparameters\ntoken of each word. We select the hyperparameters\nthat minimize the loss on the validation set. The\nreported results are the average score of 5 runs with\ndifferent random seeds computed on the test splits.\nWe report the hyperparameters in Table 6-7.\nC Easy Languages\nWe describe here in more details languages that we\nclassify as Easy in section 4.1.\nIn practice, one can obtain very high perfor-\nmance even in zero-shot settings for such lan-\nguages, by performing task-tuning on related lan-\nguages.\nModel UPOS LAS NER\nZero-Shot\n(1) FaroeseBERT 66.4 35.8 -\n(2) mBERT 79.4 67.5 -\n(3) mBERT +MLM 83.4 67.8 -\nFew-Shot (CV with around 500 instances)\n(4) Baseline 95.36 83.02 44.8\n(5) FaroeseBERT 91.12 67.66 39.3\n(6) mBERT 96.31 84.02 52.1\n(7) mBERT +MLM 96.52 86.41 58.3\nTable 8: Faroese is an “easy” unseen language:\na multilingual model (+ language-speciﬁc MLM)\neasily outperforms all baselines. Zero-shot perfor-\nmance, after task-tuning only on related languages\n(Danish, Norwegian, Swedish) is also high.\nPerhaps the best example of such an “easy” set-\nting is Faroese. mBERT has been trained on several\nlanguages of the north Germanic genus of the Indo-\nEuropean language family, all of which use the\nLatin script. As a result, the multilingual mBERT\nmodel performs much better than the monolingual\nFaroeseBERT model that we trained on the avail-\nable Faroese text (cf rows 1–2 and 5–6 in Table 8).\nFine-tuning mBERT on the Faroese text is even\nmore effective (rows 3 and 6 in Table 8), lead-\ning to further improvements, reaching more than\n96.5% POS-tagging accuracy, 86% LAS for depen-\ndency parsing, and 58% NER F1 in the few-shot\nsetting, surpassing the non-contextual baseline. In\nfact, even in zero-shot conditions, where we task-\ntune only on related languages (Danish, Norwegian,\nand Swedish), the model achieves remarkable per-\nformance of over 83% POS-tagging accuracy and\n67.8% LAS dependency parsing.\nModel UPOS LAS\nZero-Shot\n(1) SwissGermanBERT 64.7 30.0\n(2) mBERT 62.7 41.2\n(3) mBERT +MLM 87.9 69.6\nFew-Shot (CV with around 100 instances)\n(4) Baseline 75.22 32.18\n(5) SwissGermanBERT 65.42 30.0\n(6) mBERT 76.66 41.2\n(7) mBERT +MLM 78.68 69.6\nTable 9: Swiss German is an “easy” unseen lan-\nguage: a multilingual model (+ language-speciﬁc\nMLM) outperforms all baselines in both zero-shot\n(task-tuning on the related High German) and few-\nshot settings.\nSwiss German is another example of a language\nfor which one can easily adapt a multilingual model\nand obtain good performance even in zero-shot\nsettings. As in Faroese, simple MLM ﬁne-tuning\nof the mBERT model with 200K sentences leads\nto an improvement of more than 25 points in both\nPOS tagging and dependency parsing (Table 9) in\nzero-shot settings, with similar improvement trends\nin the few-shot setting.\nThe potential of similar-language pretraining\nalong with script similarity is also showcased in\nthe case of Naija (also known as Nigerian English\nor Nigerian Pidgin), an English creole spoken by\nmillions in Nigeria. As Table 10 shows, with re-\nsults after language- and task-tuning on 6K training\nexamples, the multilingual approach surpasses the\nmonolingual baseline.\nOn a side note, we can rely on Han and Eisen-\n461\nModel UPOS LAS\nNaijaBERT 87.1 63.02\nmBERT 89.3 71.6\nmBERT +MLM 89.6 69.2\nTable 10: Performance on Naija, an English creole,\nis very high, so we also classify it as an “easy”\nunseen language.\nstein (2019) to also classify Early Modern English\nas an easy language. Similarly, the work of Chau\net al. (2020) allows us to also classify Singlish (Sin-\ngaporean English) as an easy language. In both\ncases, these languages are technically unseen by\nmBERT, but the fact that they are variants of En-\nglish allows them to be easily handled by mBERT.\nD Additional Uralic languages\nexperiments\nFollowing a similar procedure as in the Appendix C,\nwe start with mBERT, perform task-tuning on\nFinnish and Estonian (both of which use the Latin\nscript) and then do zero-shot experiments on Livvi,\nand Komi, all low-resource Uralic languages (re-\nsults on the top part of Table 11). We also report\nresults on the Finnish treebanks after task-tuning,\nfor better comparison. The difference in perfor-\nmance on Livvi (which uses the Latin script) and\nthe other languages that use the Cyrillic script is\nstriking.\nAlthough they are not easy enough to be tack-\nled in a zero-shot setting, we show that the low-\nresource Uralic languages fall in the “Intermediate”\ncategory, since mBERT has been trained on similar\nlanguages: a small amount of annotated data are\nenough to improve over mBERT using task-tuning.\nFor both Livvi and Erzya, the multilingual model\nalong with MLM- TUNING achieves the best perfor-\nmance, outperforming the non-contextual baseline\nby more than 1.5 point for parsing and POS tag-\nging.\nE Controlled experiment:\nTransliterating High-Resource\nLanguages\nTo have a broader view on the effect of transliter-\nation when using mBERT (section 5.2), we study\nthe impact of transliteration to the Latin script on\nhigh resource languages seen during mBERT pre-\ntraining such as Arabic, Japanese and Russian. We\ncompare the performance of mBERT ﬁne-tuned\nLanguage UPOS LAS\nTask-tuned – Latin script\nFinnish (FTB) 93.1 77.5\nFinnish (TDT) 95.0 78.9\nFinnish (PUD) 96.8 83.5\nZero-Shot Experiments\nLatin script\nLivvi 72.3 40.3\nCyrillic script\nErzya 51.5 18.6\nFew-Shot Experiments (CV)\nLivvi – Latin script\nBaseline 84.1 40.1\nmBERT 83.0 36.3\nmBERT +MLM 85.5 42.3\nErzya – Cyrillic script\nBaseline 91.1 65.1\nmBERT 89.3 61.2\nmBERT +MLM 91.2 66.6\nTable 11: The script matters for the efﬁcacy of\ncross-lingual transfer. The zero-shot performance\non Livvi, which is written in the same script as\nthe task-tuning languages (Finnish, Estonian), is\nalmost twice as good as the performance on the\nUralic languages that use the Cyrillic script.\nand evaluated on the original script with mBERT\nﬁne-tuned and evaluated on the transliterated text.\nAs reported in Table 5, transliterating those lan-\nguages to the Latin script leads to large drop in\nperformance for all the three tasks.\n462\nLanguage (iso) Script Family #sents source Category\nFaroese (fao) Latin North Germanic 297K (Biemann et al., 2007) Easy\nMingrelian (xmf) Georg. Kartvelian 29K Wikipedia Easy\nNaija (pcm) Latin English Pidgin 237K (Caron et al., 2019) Easy\nSwiss German (gsw) Latin West Germanic 250K OSCAR Easy\nBambara (bm) Latin Niger-Congo 1K OSCAR Intermediate\nWolof (wo) Latin Niger-Congo 10K OSCAR Intermediate\nNarabizi (nrz) Latin Semitic* 87K (Seddah et al., 2020) Intermediate\nMaltese (mlt) Latin Semitic 50K OSCAR Intermediate\nBuryat (bxu) Cyrillic Mongolic 7K Wikipedia Intermediate\nMari (mhr) Cyrillic Uralic 58K Wikipedia Intermediate\nErzya (myv) Cyrillic Uralic 20K Wikipedia Intermediate\nLivvi (olo) Latin Uralic 9.4K Wikipedia Intermediate\nUyghur (ug) Arabic Turkic 105K OSCAR Hard\nSindhi (sd) Arabic Indo-Aryan 375K OSCAR Hard\nSorani (ckb) Arabic Indo-Iranian 380K OSCAR Hard\nTable 12: Unseen Languages used for our experiments. #sents indicates the number of sentences used for\ntraining from scratch Monolingual Language Models as well as for MLM- TUNING mBERT\n*code-mixed with French"
}