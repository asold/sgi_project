{
  "title": "A Cognitive Regularizer for Language Modeling",
  "url": "https://openalex.org/W3174428931",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2324760462",
      "name": "Jason Wei",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2964009555",
      "name": "Clara Meister",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2148165152",
      "name": "Ryan Cotterell",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W199274192",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2970206392",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W4242186007",
    "https://openalex.org/W2140188190",
    "https://openalex.org/W2152253064",
    "https://openalex.org/W2396566415",
    "https://openalex.org/W2892039495",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3088227906",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2036885110",
    "https://openalex.org/W2787658776",
    "https://openalex.org/W2032558547",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W3012990076",
    "https://openalex.org/W2129955048",
    "https://openalex.org/W3095794553",
    "https://openalex.org/W2118938353",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2995969307",
    "https://openalex.org/W4310299640",
    "https://openalex.org/W2472237015",
    "https://openalex.org/W3035512170",
    "https://openalex.org/W2786137621",
    "https://openalex.org/W2009727972",
    "https://openalex.org/W1983668253",
    "https://openalex.org/W2586145851",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W1486114846",
    "https://openalex.org/W2117757239",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2079833689",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W2009629554",
    "https://openalex.org/W2007054563",
    "https://openalex.org/W22168010",
    "https://openalex.org/W3102657423",
    "https://openalex.org/W1995945562",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2803214681",
    "https://openalex.org/W2953092638",
    "https://openalex.org/W2157365695",
    "https://openalex.org/W2082472210"
  ],
  "abstract": "Jason Wei, Clara Meister, Ryan Cotterell. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5191–5202\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5191\nA Cognitive Regularizer for Language Modeling\nJason Wei\n Clara Meister\n Ryan Cotterell\n ,\nGoogle AI Language\n ETH Zürich\n University of Cambridge\njasonwei@google.com clara.meister@inf.ethz.ch\nryan.cotterell@inf.ethz.ch\nAbstract\nThe uniform information density (UID) hy-\npothesis, which posits that speakers behaving\noptimally tend to distribute information uni-\nformly across a linguistic signal, has gained\ntraction in psycholinguistics as an explanation\nfor certain syntactic, morphological, and\nprosodic choices. In this work, we explore\nwhether the UID hypothesis can be opera-\ntionalized as an inductive bias for statistical\nlanguage modeling. Speciﬁcally, we augment\nthe canonical MLE objective for training lan-\nguage models with a regularizer that encodes\nUID. In experiments on ten languages span-\nning ﬁve language families, we ﬁnd that using\nUID regularization consistently improves\nperplexity in language models, having a larger\neffect when training data is limited. Moreover,\nvia an analysis of generated sequences, we\nﬁnd that UID-regularized language models\nhave other desirable properties, e.g., they gen-\nerate text that is more lexically diverse. Our\nresults not only suggest that UID is a reason-\nable inductive bias for language modeling, but\nalso provide an alternative validation of the\nUID hypothesis using modern-day NLP tools.\n1 Introduction\nLanguage has been hypothesized to follow certain\ninformation-theoretic constraints. One of the most\nfamous of these constraints is the uniform infor-\nmation density (UID) hypothesis (Fenk and Fenk,\n1980; Jaeger, 2010), which states that, subject to\nthe rules of the grammar, speakers aim to distribute\ninformation density across a linguistic signal as\nuniformly as possible. That is, speakers behav-\ning optimally should structure their utterances such\nthat the differences between the peaks and troughs\nin information are minimized.\nIn the psycholinguistics literature, the UID hy-\npothesis has been used to explain a variety of lin-\nguistic phenomena ranging from how we shorten\nthe phonetic duration of more-predictable linguistic\n(a)\n(b)\nFigure 1: Graphical illustration of two examples regard-\ning UID. In (a), many speakers will prefer the version\nwith the relativizer that (dotted blue line). The UID\nhypothesis posits that this is because, without the rela-\ntivizer, the ﬁrst word of the relative clause,we, has high\ninformation density; and so including the relativizer\ndistributes the per-word information density more uni-\nformly. In (b), the relativizer that is often omitted be-\ncause, at the onset of the relative clause, the informa-\ntion density of I is lower and therefore the distribution\nof information density is already relatively uniform. Il-\nlustration based on Jaeger (2010).\nunits (Aylett and Turk, 2004) to when we decide to\nuse optional syntactic relativizers (Levy and Jaeger,\n2007), among other phenomena (Bell et al., 2003;\nFrank and Jaeger, 2008). These studies often use\nlanguage models to estimate the information den-\nsity of linguistic units, taking observations of low\nvariation of information density in well-formed ut-\nterances as evidence for the UID hypothesis.\n5192\nIn this paper, we propose a new experimental\nparadigm that uses modern-day NLP models to test\nthe UID hypothesis. Whereas prior work has used\nlanguage modeling as a tool for observing UID, 1\nwe explore the converse—can UID be used as a\ntool to train better language models? Speciﬁcally,\nif the UID hypothesis is true, then we should be\nable to operationalize UID as a regularizer to help\ntrain language models. Moreover, observing lower\nperplexity in language models trained with this\nregularization would imply that the concept of UID\nis a good inductive bias for language modeling,\nthereby providing a new type of evidence for the\nUID hypothesis at scale.\nIn experiments, we indeed ﬁnd such evidence:\nacross a variety of languages and dataset sizes,\nUID regularization consistently improves perfor-\nmance, having a larger effect when training data\nis limited. Moreover, we observe that—in compar-\nison with their unregularized counterparts—UID-\nregularized language models are (1) higher entropy\nwhile achieving the same (or better) test set perplex-\nity and (2) generate text that is longer and more\nlexically diverse. Our work is the ﬁrst to explore\nthe interaction between UID and training modern-\nday neural language models, and our ﬁndings—that\na cognitively motivated objective can improve lan-\nguage model performance—open up new avenues\nfor testing other psycholinguistic hypotheses in a\nsimilar framework.\n2 Preliminaries: Language Modeling\nThe task of language modeling aims to estimate a\nmodel of the probability of observing any given\nstring in (a subset of) natural language. For-\nmally, a language model p is an (unconditional)\nprobability distribution over sequences of words\nw = ⟨w1,w2,... ⟩, where w consists of tokens\nfrom some vocabulary and begins and ends with\nspecial tokens BOS and EOS , respectively.\nToday’s language models are typically param-\neterized by neural networks (e.g., transformers\n(Vaswani et al., 2017)), that follow a local-\nnormalization scheme. Speciﬁcally, the model pro-\nvides a conditional distribution over the vocabulary\nat each time step; we can then compute the proba-\n1On its own, the term ‘UID’ is formally anattribute of a\nlinguistic signal. We also use it throughout this work to refer\nto the concept that UID is a desirable property.\nbility of an entire sequence w as:\npθ(w) =\n|w|∏\nt=1\npθ(wt |w<t) (1)\nwhere θ are the parameters of the model and we\nuse w<t to represent the ﬁrst t−1 tokens of w.\nParameters are estimated by optimizing over some\nobjective L(θ). The standard objective for lan-\nguage modeling is the negative log-likelihood of a\ndataset Wunder the model:\nL(θ) =−\n∑\nw∈W\nlog pθ(w) (2)\nSubsequently, we drop explicit dependence on θ\nwhen it is obvious from context.\nTo assess the goodness of ﬁt of a model p, we\ntypically evaluate its perplexity on some held-out\ndataset Wtest, where perplexity (PPL) is deﬁned as\nPPL(p) = exp\n(\n−\n∑\nw∈Wtest\n1\n|w|log p(w)\n)\n(3)\nNote that under this deﬁnition of perplexity, our\nevaluation metric is slightly different than the train-\ning objective; the former computes an average\nover each sequence while the later treats all tokens\nequally, regardless of the length of the sequence in\nwhich they are present.\n3 Uniform Information Density\nCommunication via natural language is a compli-\ncated and nuanced process that takes place under\na host of cognitive and environmental constraints.\nAs a result, speakers have to make (perhaps subcon-\nscious) choices to best navigate this communicative\ndance. A rational speaker would use these choices\nto optimize the communicative properties of their\nutterances. One such locus of optimization is out-\nlined by the Uniform Information Density (UID)\nhypothesis.\n3.1 The UID Hypothesis\nAt its core, the UID hypothesis aims to explain\ncertain phenomena in human language processing\nusing an information-theoretic approach: we can\nview language as a transfer of information, which\nis transmitted with a certain density through a com-\nmunication channel. The UID hypothesis posits\nthat speakers that behave optimally will structure\n5193\ntheir utterances to avoid peaks and troughs in this\ninformation density (Aylett and Turk, 2004; Levy\nand Jaeger, 2007; Jaeger, 2010). More formally\nstated: “ Within the bounds deﬁned by grammar,\nspeakers prefer utterances that distribute informa-\ntion uniformly across the signal (information den-\nsity). Where speakers have a choice between sev-\neral variants to encode their message, they prefer\nthe variant with more-uniform information density\n(ceteris paribus)” (Jaeger, 2010).\n3.2 Example: UID in syntactic reduction\nTo better understand the UID hypothesis, consider\nthe concrete example of syntactic reduction (that-\nmentioning) from Jaeger (2010), which we show\ngraphically in Figure 1 and also describe below.\nEx. A. My boss conﬁrmed [that] we are crazy.\nEx. B. My boss thinks [that] I am crazy.\nIn both these sentences, the use of the relativizer\nthat is syntactically optional—at the onset of a rel-\native clause (RC), speakers can, but do not have\nto, include the relativizer. Many speakers, how-\never, would argue that the sentence ﬂows better\nwith the relativizer included in Example A and the\nrelativizer omitted in Example B.\nThe UID hypothesis provides a potential expla-\nnation for this phenomenon. When a RC is used\nwithout a relativizer, the ﬁrst word of the RC con-\nveys two pieces of information: both the onset of\nthe RC, as well as part of the RC’s internal con-\ntents. In Example A, many speakers would ﬁnd\nthat the information density of the ﬁrst word in the\nRC, we, is high, and so adding in the relative clause\ndistributes the information over two words, making\nit easier to parse. In Example B, the information\ndensity of the ﬁrst word in the RC, I, is lower rel-\natively, and so we do not need to (or it is not as\nbeneﬁcial to) include the relativizer.\n3.3 Measuring UID\nNow that we better understand what the UID hy-\npothesis attempts to explain, how might we opera-\ntionalize UID and ﬁnd quantitative evidence of the\npressure for it in language? First, to quantify the\namount of information conveyed by a word, we turn\nto the most basic information-theoretic deﬁnition:\nthe information conveyed by a wordwin context is\nits Shannon information content (Shannon, 1948),\nalso called surprisal. Ideally, this surprisal would\nbe measured using the “true” distribution over hu-\nman language. Because we do not have access to\nsuch a distribution, we often estimate it using a sta-\ntistical language model. That is, given a statistical\nlanguage model p, which estimates the probability\nof a word given its context, the surprisal u(wt) of\nword wt is deﬁned as the following:\nu(wt) =−log p(wt |w<t) (4)\nThis setup provides a natural approach to exploring\nhow UID might manifest—if the UID hypothesis\nis true, then we should observe that variation in\nsurprisal, as estimated by a language model, is\nminimized in natural language.\nUsing this approach, prior work has accumulated\nevidence for UID across various levels of linguistic\nrepresentation (Pluymaekers et al., 2005; Bell et al.,\n2009, inter alia). As some of the earliest exam-\nples, Aylett and Turk (2004) showed that linguistic\nunits that had high surprisal according to a tri-gram\nlanguage model were uttered with longer syllable\ndurations, and Levy and Jaeger (2007) found that\nfor RCs in which the ﬁrst word had higher surprisal,\nrelativizers were more likely to be used in the RC\nduring actual speech. Further examples are given\nin our related work section (§7).\n4 UID-Regularized Language Modeling\nWhile prior work has shown evidence that UID can\nhelp explain many of the choices we make when\ngenerating language, to the best of our knowledge,\noperationalizations of UID have not been explic-\nitly employed as part of the training objective in\nmodern-day NLP models. This raises the simple\nquestion that is central to our paper:\nCan UID serve as an inductive\nbias for training statistical lan-\nguage models?\nIn an effort to answer this question, we present\na scheme for incorporating operationalizations of\nUID into the language model training objective.\nFormally, we augment the canonical maximum like-\nlihood estimation objective2 in eq. (2) with UID\n2Note that the maximum likelihood estimation objective\nminimizes (over w ∈W) −log p(wt |w<t), i.e., surprisal.\nAlthough such an objective may indirectly minimize peaks\nand dips in surprisal across a sequence simply by pushing\nthem towards 0, it does not explicitly include any sequence\nlevel penalty for even surprisal distribution.\n5194\noperationalizations as regularizers R. Under this\nnew objective, we minimize\nLR(θ) =L(θ) +β·R(θ) (5)\nwhere β >0 is the strength coefﬁcient of the regu-\nlarizer. We consider two natural operationalizations\nof UID—inspired by Collins (2014)—as regulariz-\ners for training language models:\nVariance Regularizer. UID concerns the distri-\nbution of information in language production, and\nso a natural measure of this behavior is the variance\nof surprisals. Thus, we ﬁrst consider a regularizer\nthat penalizes high variance among the surprisals\nof words in a given sequence:\nR(θ) = 1\n|w|\n|w|∑\nt=1\n(u(wt) −µ)2 (6)\nwhere µ= 1\n|w|\n∑|w|\nt=1 u(wt). Note that here, and in\nour subsequent regularizers, we estimate u(·) via\neq. (4) using our model pθ.\nLocal Consistency. Next, we consider a local\nconsistency regularizer that encourages the sur-\nprisals of adjacent words to have similar magnitude:\nR(θ) = 1\n|w|−1\n|w|−1∑\nt=1\n(\nu(wt) −u(wt+1)\n)2\n(7)\nThis regularizer is also a reasonable operational-\nization of UID—if every surprisal is similar to its\nneighbor, then the density of information in the\nsequence will be close to uniform.\nThough we focus on these two regularizers, other\noperationalizations of UID certainly exist. For ex-\nample, a similar variant of the above regularizers is\nthe max regularizer (Meister et al., 2020a), which\npenalizes the highest surprisal in a sentence.3 Fur-\nthermore, UID may also be deﬁned in terms of\nparse steps (Hale, 2001) or structural integrations\n(Gibson, 2000), as well as in spoken language in\nthe form of ﬁller words like uh and um or word\nrepetition during challenging lexical retrieval. We\nconsider these operationalizations (as well as the\nbroader discussion of how to operationalize UID)\nas future work.\n3We also tried this operationalization in preliminary exper-\niments, but results were not as strong as the variance or local\nconsistency regularizers.\n5 Experimental Setup\nTo empirically evaluate UID regularization, we\ntrain various language models with the UID-\nregularized objective (eq. (5)) using the following\nexperimental setup.\nDatasets. We employ datasets from multiple lan-\nguages and of varying sizes. We use the EuroParl\ncorpus (Koehn, 2005)—a multi-lingual dataset of\ndiscussions from the European Parliament that has\nbeen commonly used for language modeling (Cot-\nterell et al., 2018; Mielke et al., 2019)—since it\nis roughly semantically controlled in that all utter-\nances are presumably about the same topics. We\nuse EuroParl v7 download from the ACL 2014\nSMT Workshop4 and perform a 80–10–10 train-\ndev-test split on all ﬁve languages—Czech, En-\nglish, French, German, and Spanish—which yields\n46.7, 42.2, 47.2, 51.3, and 12.4 million training\ntokens for each language respectively.\nMoreover, we experiment on languages from\nseveral language families; the ﬁve languages in\nEuroparl that we consider are all Indo-European,\nand so we look to Wiki-40B (Guo et al., 2020),\nwhich contains Wikipedia dumps of a wide range\nof languages. We choose a set of diverse languages\nwith training set sizes relatively similar to that of\nEuroParl: Finnish (a Uralic language; 59.3M train-\ning tokens), Indonesian (an Austronesian language;\n45.7M training tokens), and Turkish (a Turkic lan-\nguage; 38.1M training tokens). To explore per-\nformance on lower-resource languages, we addi-\ntionally experiment with Swahili5 (a Niger-Congo\nlanguage; 6.3M training tokens) and Tagalog (an\nAustronesian language; 4.2M training tokens). For\nall languages, we performed tokenization using the\nMosesTokenizer.6 Train, dev, and test set splits are\nshown in Table 5 in the Appendix.\nModel Framework and Architecture. For our\nexperiments, we use the fairseq library (Ott\net al., 2019), a standard sequence modeling toolkit\nin PyTorch. As our model, we use fairseq’s de-\nfault transformer (with six decoder layers and eight\n4http://statmt.org/wmt14/\ntranslation-task.html\n5Since there are no Niger-Congo languages in Wiki-40B,\nwe perform a 80-10-10 split on Swahili Wikidumps (see\nhttps://github.com/google-research/bert/\nblob/master/multilingual.md).\n6https://pypi.org/project/\nmosestokenizer/\n5195\nattention heads), which achieves competitive7 lan-\nguage modeling performance (although the purpose\nof our paper is not to achieve or compare with the\nstate of the art). For all experiments, we followed\nthe data-preprocessing scripts and recommended\nhyperparameters provided in fairseq’s language\nmodeling module; more detailed information can\nbe found on the Github page.8\nUID Regularizers. For UID regularization, we\nexperiment with the variance (eq. (6)) and local\nconsistency regularizers (eq. (7)). We found in pre-\nliminary experiments that effective regularization\nstrengths were often near β = 0.01, and so we\nperformed a grid search over values within an or-\nder of magnitude around β = 0.01: β ∈{0.006,\n0.008, 0.01, 0.02, 0.03, 0.04, 0.05}. We choose\nthe model with the lowest dev loss to evaluate on\nthe test set.\n6 Results\nIn this section, we report results for models trained\nunder the UID-regularized objective. We ﬁnd\nthat UID regularization consistently improves\nperplexity for models trained on various languages\n(§6.1) and dataset sizes (§6.2). Additionally,\nwe examine properties of text generated by\nUID-regularized models (§6.3) and analyze the\nrelationship between our operationalization of UID\nand perplexity (§6.4).\n6.1 Languages\nTable 1 shows the results of UID-regularized lan-\nguage models trained on various languages from\nEuroParl and Wiki-40B, and includes statistical\nsigniﬁcance of changes in perplexity, as compared\nwith baselines, computed using permutation tests9\n(Efron and Tibshirani, 1994). For all languages,\nUID regularization signiﬁcantly improves perplex-\nity for at least one of the two regularizers. Further-\n7On Wikitext-103, the largest dataset we train on (103\nmillion tokens), we achieve a competitive perplexity of 29.89\n(c.f. Merity et al. (2018)). For smaller datasets, we tried a\nsmaller transformer architecture of four decoder layers and\nfour attention heads, but it did not perform better than the six\ndecoder layer and eight attention heads version, suggesting\nthat this architecture was not too large for the datasets we use\nin this paper (even the Tagalog dataset we use is larger than\nthe commonly used Penn Treebank and WikiText-2).\n8https://github.com/pytorch/fairseq/\ntree/master/examples/language_model\n9http://www2.stat.duke.edu/~ar182/rr/\nexamples-gallery/PermutationTest.html\nLANGUAGE (# train tokens) Perplexity\nCZECH (12.4M)\nBaseline (no UID) 47.47\n+ UID: variance 47.24 ( ↓0.5%)\n+ UID: local consistency 47.08 (↓0.8%)†\nENGLISH (46.7M)\nBaseline (no UID) 21.34\n+ UID: variance 21.08 (↓1.2%)†\n+ UID: local consistency 21.19 ( ↓0.7%)†\nFINNISH (59.3M)\nBaseline (no UID) 51.58\n+ UID: variance 51.30 (↓0.5%)†\n+ UID: local consistency 51.49 ( ↓0.2%)\nFRENCH (51.3M)\nBaseline (no UID) 17.08\n+ UID: variance 17.02 (↓0.4%)†\n+ UID: local consistency 17.03 ( ↓0.3%)†\nGERMAN (42.3M)\nBaseline (no UID) 26.62\n+ UID: variance 26.50 ( ↓0.4%)†\n+ UID: local consistency 26.45 (↓0.6%)†\nINDONESIAN (45.7M)\nBaseline (no UID) 53.96\n+ UID: variance 53.66 (↓0.6%)†\n+ UID: local consistency 53.70 ( ↓0.5%)\nSPANISH (47.2M)\nBaseline (no UID) 22.54\n+ UID: variance 22.37 (↓0.8%)†\n+ UID: local consistency 22.44 ( ↓0.4%)†\nSWAHILI (6.3M)\nBaseline (no UID) 40.45\n+ UID: variance 39.79 ( ↓1.6%)†\n+ UID: local consistency 39.44 (↓2.5%)†\nTAGALOG (4.2M)\nBaseline (no UID) 80.48\n+ UID: variance 78.40 ( ↓2.5%)†\n+ UID: local consistency 78.12 (↓2.9%)†\nTURKISH (38.1M)\nBaseline (no UID) 66.13\n+ UID: variance 65.70 (↓0.7%)†\n+ UID: local consistency 66.06 ( ↓0.1%)\nTable 1: UID regularizers improve perplexity for mul-\ntiple languages. † indicates statistical signiﬁcance com-\npared with the baseline (p< 0.05).\nmore, UID regularization (under the best perform-\ning β) never leads to worse perplexity. These re-\nsults suggest that incorporating UID operational-\nizations into a model’s training objective leads to\na better model of language, substantiating uniform\ninformation density as a valid inductive bias. More-\nover, the improvement for many languages corrob-\norates the expectation that UID should, due to its\ninformation theoretic nature, hold across languages\n(Jaeger and Tily, 2011).\n5196\nWMT’06 EuroParl WT-103\n# training tokens 16.0M 47.0M 103.2M\nBaseline (no UID) 49.70 21.34 29.89\n+ UID: variance 48.25† 21.08† 29.58\n+ UID: local consistency 48.79 21.19 29.73\nTable 2: UID regularizers improve perplexity on lan-\nguage models trained on English datasets of vary-\ning size. Improvements tend to be larger on smaller\ndatasets. † indicates statistical signiﬁcance compared\nwith the baseline (p< 0.05).\n6.2 Dataset Size\nNotably, we observe the largest improvements\n(1.6–2.9%) in perplexity in Table 1 for the low-\nest resource languages, Tagalog and Swahili (with\n4.2 and 6.3 million training tokens respectively).\nConversely, improvement was most marginal (0.2–\n0.5%) on the highest-resource languages, French\nand Finnish (51.3 and 59.3 million training tokens\nrespectively). To remove language as a confound-\ning factor from this observation, we perform a con-\ntrolled analysis of the effects of UID regularization\nas a function of dataset size.\nWe focus on English; in addition to the result on\nEnglish EuroParl 2014 from Table 1, which con-\ntains 47.0 million training tokens, we experiment\nwith the smaller monolingual English dataset from\nthe 2006 NAACL Workshop on Statistical Machine\nTranslation (WMT’06),10 which has 17.0M tokens\nin its training set, as well as the larger Wikitext-103\nbenchmark (Merity et al., 2017), which contains\n103 million tokens in its training set.\nTable 2 shows the perplexities for models with\nand without UID regulariztion for these three\ndatasets. As suggested by earlier results, improve-\nments were strongest for the WMT’06 dataset, with\nan improvement of 1.4 perplexity points for the\nvariance regularizer and 0.9 PPL points for local\nconsistency. For the larger EuroParl and WT-103\ndatasets, on the other hand, improvement was more\nmodest, ranging from 0.1 to 0.3 perplexity points.\nAs further conﬁrmation that UID regularization\nhas a greater impact on smaller datasets, we per-\nform an ablation study that roughly controls for\nlanguage content by training models on the subsets\nof the same dataset. For this ablation, we take sub-\nsets of 2, 4, 8, 12, 16, 24, and 32 million sentences\nfrom the 47 million sentences in English EuroParl,\n10We downloaded the given train-dev-test splits from\nhttps://www.statmt.org/wmt06/.\n0\n20\n40\n60\n80\nBaseline\nperplexity\n2 8 16 24 32 47\n0\n0.5\n1\n1.5\n2\n2.5\nTraining tokens (millions)\nImprovement in\nperplexity\nUID: variance\nUID: local consistency\nFigure 2: Improvement in perplexity for UID regular-\nized models trained on subsets of varying size sampled\nfrom the EuroParl English dataset (full dataset size 47.0\nmillion tokens). UID regularization helped more when\ntraining data was more limited.\nand observe how much the UID regularizers im-\nprove perplexity for each training dataset size. As\nshown in Figure 2, the results tell the same story as\nTable 2—UID regularization improves perplexity\nmore for smaller datasets.\nThese results are consistent with the expectation\nthat models trained on smaller datasets are more\nlikely to overﬁt and could therefore beneﬁt more\nfrom regularization (Melis et al., 2018). As it is\npossible that the models trained on smaller datasets\ncould beneﬁt from any kind of regularization,\nwe experiment with label smoothing (Szegedy\net al., 2016), another regularization technique\nthat similarly augments the training objective\nwith a penalty. Table 4 shows these results for\nmodels trained on WMT’06 and EuroParl with\nlabel smoothing—our experiments indicate that,\nacross the board, label smoothing leads to worse\nperplexity compared with baseline models. 11\nWe take this result as further evidence that the\nimprovement from UID regularization stems from\nthe UID hypothesis as a valid inductive bias, rather\nthan simply a need for any kind of regularization\nwhen training on smaller datasets.\n11This negative result for applying label smoothing to lan-\nguage modeling is consistent with prior empirical ﬁndings\n(Müller et al., 2019; Gao et al., 2020; Meister et al., 2020b).\n5197\nSequence Model % unique n-grams\nlength entropy n= 2 n= 3 n= 4\nBaseline (no UID) 22.9 69.6 37.7 73.5 90.9\n+ UID: variance 24.0 79.4 40.7 77.8 93.3\n+ UID: local consistency 23.3 73.9 39.1 75.7 92.1\nTable 3: Text generated by UID-regularized language models is longer (higher average sequence length), higher\nentropy (computed via monte-carlo estimation), and more lexically diverse (a higher ratio of unique n-grams).\nWMT’06 EuroParl\n# training tokens 16.0M 47.0M\nBaseline 35.75 23.22\n+ label smoothing, α= 0.01 36.15 26.26\n+ label smoothing, α= 0.05 55.56 40.79\n+ label smoothing, α= 0.1 90.57 68.26\nTable 4: Label smoothing, another form of regulariza-\ntion that similarly augments the cross-entropy objective\nwith a penalty, does not improve perplexity. (Results\nshown on dev set).\n6.3 Evaluating Generated Text\nUnconditional models of language have been ob-\nserved to produce generic text that can be short,\nbland, or repetitive (Fan et al., 2018; Kulikov\net al., 2019; Holtzman et al., 2020), and so in this\nsubsection we investigate how UID regularization\nmight affect these characteristics in generated text.\nFor these experiments, we consider the baseline\nmodel, the variance-regularized model, and the lo-\ncal consistency-regularized model trained on En-\nglish EuroParl. To obtain text samples, we generate\nsamples by sequentially sampling tokens according\nto the model’s predicted distribution until the end-\nof-sequence (EOS ) token is sampled, i.e., ancestral\nsampling. Note that for language model p, this\nsampling scheme is equivalent to directly sampling\ny ∼p. We obtain 10,000 samples for each model\nand report statistics in Table 3.\nWe analyze each set of generated sentences for\nseveral metrics. First, we compute the average\nlength of generated sentences. Next, we evaluate\nthe lexical diversity of generated texts by comput-\ning the percent of uniquen-grams for n∈{2,3,4}.\nFinally, sampling from a model also gives us a\nmeans for estimating the language model’s entropy:\nH(p) =−\n∑\ny∈supp(p)\np(y) logp(y) (8)\n= −Ey∼p (log p(y)) (9)\nIn the case of language models, supp(p) is the set\nof all strings that can be generated from the model’s\nvocabulary V. As this is exponentially large in |V|,\ndirectly computing H(p) is intractable. We can use\nits equivalence to eq. (9), however, to estimateH(p)\nwith a simple Monte-Carlo estimator:\nˆH(p) =−1\nK\nK∑\nk=1\nlog p(y(k)) (10)\nwhere we sample y(k) ∼pfor k= 1,...,K .\nTable 3 shows results from UID-regularized\nmodels compared with the baseline. The models\ntrained with the variance and local consistency reg-\nularizers exhibit a preference for longer sequence\nlength and higher lexical diversity. Additionally,\nthe entropy estimates of these models are notably\nhigher, which, following the principle of maximum\nentropy (Jaynes, 1957),12 can be seen as an addi-\ntional advantage of UID-regularized models over\ntheir unregularized counterparts.\n6.4 UID Behavior\nTo take a closer look at how UID regularization\naffects language models, we examine the relation-\nship between minimizing perplexity and UID be-\nhavior, where we quantify UID behavior as the\nvariance of models’ surprisals. We consider mod-\nels trained on the English EuroParl dataset with the\nvariance regularizer at strengths β ∈{0.01, 0.03,\n0.05, 0.07, 0.09}and our baseline (which is equiv-\nalent to β = 0), For further comparison, we also\ntrain a model with β = −0.01 to observe the ef-\nfects of penalizing UID behavior. We report results\non the EuroParl test set in Figure 3.\nWe observe that the model trained with a UID\npenalty (negative β) indeed exhibits worse perplex-\nity and UID behavior (variance of surprisals) on the\ntest set. And as we might expect, models trained\nwith higher βexhibit UID behavior more strongly,\nas our quantiﬁcation is part of their training objec-\ntive. Overall, from β = 0.01 to β = 0.05, both\n12The principle of maximum entropy states that the proba-\nbility distribution that best represents the current knowledge\nstate is the one with the largest entropy.\n5198\n21 21.2 21.4 21.6 21.8 22\n15\n15.5\n16\n16.5\n17\n17.5\n18\n18.5\nβ = −0.01\nβ = 0(baseline)\nβ = 0.01\nβ = 0.03\nβ = 0.05\nβ = 0.07\nβ = 0.09\nPerplexity\nUID behavior\n(variance of surprisals)\nFigure 3: A trade-off between perplexity ( x-axis) and\nvariance of surprisals (a measure of UID behavior; y-\naxis). The black pentagon indicates the β that yielded\nthe best perplexity (β = 0.03).\nperplexity and UID behavior are positively corre-\nlated with β, but when we optimize too much for\nUID (β ≥0.07), there is a trade-off in which model\nperplexity begins to increase.\nWe also observe an intriguing phenomenon in\nFigure 3. Models that achieve similar perplexity\ncan have substantially different UID behavior val-\nues on the test set. Speciﬁcally, the β = 0 and\nβ = 0.07 models, which have almost the same\nperplexity, have variance of surprisals of 17.8 and\n15.8—a difference of more than ten percent! If such\nmodels with similar perplexity can have varying\ndeﬁnitions of what constitutes good UID behav-\nior, then prior work, which has drawn conclusions\non UID based on surprisals computed by a single\nmodel (Aylett and Turk, 2004; Levy and Jaeger,\n2007; Jain et al., 2018), may need revisiting. As\nthis direction is outside the scope of the present\npaper, we leave it as future work.\n7 Discussion and Related Work\nWe discussed how operationalizing UID for lan-\nguage modeling leads to better models in a wide\nvariety of settings. These results both provide a\nnew form of evidence for the UID hypothesis and\nbuild on prior work exploring UID in modern-day\nNLP models.\nEvidence for the UID hypothesis.Our work ex-\ntends the body of psycholinguistic research on uni-\nform information density, which has largely corrob-\norated the UID hypothesis by providing evidence\nthat variation in surprisal, as estimated by a lan-\nguage model, is minimized in natural language. In\naddition to early studies that used this approach to\nﬁnd evidence for UID in syntactic reduction (Levy\nand Jaeger, 2007), morphosyntactic contractions\n(Frank and Jaeger, 2008), and prosodic structure\n(Aylett and Turk, 2004), the same line of reasoning\nhas been used by more recent work exploring a\nvariety of other linguistic properties. These studies\nhave found that word duration can be predicted by\nsyntactic surprisal (Demberg et al., 2012; Moore-\nCantwell, 2013), construction probability (Kuper-\nman and Bresnan, 2012), informativity (Seyfarth,\n2014), and contextual predictability (Jurafsky et al.,\n2001; Bell et al., 2003; Gahl and Garnsey, 2004).\nThey have also observed that word length is re-\nﬂected by conceptual complexity (Lewis and Frank,\n2016); word order choice can be predicted by pro-\ncessing cost (Bloem, 2016; Sikos et al., 2017);\nphonological patterns can be shaped by word pre-\ndictability (Hall et al., 2018); and UID computed\nat the sequence level predicts human preferences\nfor syntactic alternatives of the same sentence.\nWhereas the above prior work has used language\nmodeling as a tool for measuring UID, our paper\nhas explored the exact converse—we have asked\nwhether UID, operationalized as a regularizer, can\nbe used as a tool for training better language mod-\nels. We argue that if the UID hypothesis holds\nas a general principle, then we should be able to\nexploit it as a training criterion that improves lan-\nguage modeling. And accordingly, our results show\nthat—across a variety of languages and dataset\nsizes—regularization for UID did indeed improve\nperplexity, which we view as an alternative kind of\nevidence for the UID hypothesis at scale.\nNotably, Figure 3 at ﬁrst could appear to contra-\ndict the UID hypothesis, since models with better\nUID behavior did not always achieve better perplex-\nity. We do not consider this as evidence against\nthe UID hypothesis, however. Rather, we posit\nthat when β is too large, we may be optimizing\nfor UID to the point of tending towards unnatu-\nral language—a perfectly uniform dispersion of\ninformation across an utterance may come at the\ncost of strange lexical choices. In this light, such a\ntrade-off should be somewhat expected.\nUID in modern NLP. In addition to the tradi-\ntional line of psycholinguistic work, there have\nalso been more-recent studies on UID in the con-\ntext of modern NLP, although this work is rela-\ntively sparse. Rubino et al. (2016) leverage infor-\n5199\nmation density encoded as surprisal at the word,\npart of speech, and syntax levels to help build a\nstate-of-the-art model for mixed-domain transla-\ntionese detection. Jain et al. (2018) incorporate\nUID measures across sentences into models de-\nsigned to detect natural versus manipulated text.\nPerhaps the work that is most related to ours, Meis-\nter et al. (2020a), leverages UID to explain why\nbeam search is an effective decoding algorithm\nand uses operationalizations of UID during beam\nsearch to alleviate problems with decoding poorly\ncalibrated machine translation models. Whereas\nMeister et al. (2020a) focuses on decoding, our\nwork shows the ﬁrst evidence that UID can be op-\nerationalized to aid training.\n8 Conclusions\nIn closing, we have proposed encoding uniform\ninformation density as a regularizer for training lan-\nguage models—a novel manner of incorporating\nan established psycholinguistic theory into modern\nstatistical language modeling. In experiments on\na range of languages and dataset sizes, UID reg-\nularization consistently improves perplexity over\nbaselines. Our results suggest that UID is a valid\ninductive bias for improving the canonical maxi-\nmum likelihood objective in language modeling,\nproviding a new, alternative type of evidence that\nsupports the UID hypothesis at scale. Our work\nopens the door to future research directions such\nas using similar techniques to validate other psy-\ncholinguistic phenomena, applying UID regulariza-\ntion in conditional language generation tasks, and\nexploring how UID regularized models perform in\ndownstream NLP applications.\nEthical Concerns\nLanguage models have various ethical, environmen-\ntal, and ﬁnancial concerns. We cannot do justice\nto them here, but do see Bender et al. (2021) for a\npointer. We do not foresee any additional ethical\nconcerns with the contributions made in our work\nbeyond those discussed in Bender et al. (2021).\nAcknowledgements\nWe thank Roger Levy for feedback in the middle\nstages of our work and Tiago Pimentel, David Re-\nitter, Tal Linzen, and Slav Petrov for feedback on\nthe manuscript.\nReferences\nMatthew Aylett and Alice Turk. 2004. The smooth\nsignal redundancy hypothesis: A functional ex-\nplanation for relationships between redundancy,\nprosodic prominence, and duration in spontaneous\nspeech. Language and Speech, 47(1):31–56. PMID:\n15298329.\nAlan Bell, Jason M. Brenier, Michelle Gregory, Cyn-\nthia Girand, and Dan Jurafsky. 2009. Predictability\neffects on durations of content and function words\nin conversational English. Journal of Memory and\nLanguage, 60(1):92–111.\nAlan Bell, Daniel Jurafsky, Eric Fosler-Lussier, Cyn-\nthia Girand, Michelle Gregory, and Daniel Gildea.\n2003. Effects of disﬂuencies, predictability, and ut-\nterance position on word form variation in English\nconversation. The Journal of the Acoustical Society\nof America, 113(2):1001–1024.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY , USA. As-\nsociation for Computing Machinery.\nJelke Bloem. 2016. Testing the processing hypoth-\nesis of word order variation using a probabilistic\nlanguage model. In Proceedings of the Workshop\non Computational Linguistics for Linguistic Com-\nplexity (CL4LC), pages 174–185, Osaka, Japan. The\nCOLING 2016 Organizing Committee.\nMichael Xavier Collins. 2014. Information density\nand dependency length as complementary cogni-\ntive models. Journal of Psycholinguistic Research ,\n43(5):651–681.\nRyan Cotterell, Sabrina J. Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model? In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 536–541, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nVera Demberg, Asad Sayeed, Philip Gorinski, and\nNikolaos Engonopoulos. 2012. Syntactic surprisal\naffects spoken word duration in conversational con-\ntexts. In Proceedings of the 2012 Joint Conference\non Empirical Methods in Natural Language Process-\ning and Computational Natural Language Learning,\npages 356–367, Jeju Island, Korea. Association for\nComputational Linguistics.\nBradley Efron and Robert J. Tibshirani. 1994. An In-\ntroduction to the Bootstrap. CRC Press.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\n5200\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nAugust Fenk and Gertraud Fenk. 1980. Konstanz\nim kurzzeitgedächtnis-konstanz im sprachlichen in-\nformationsﬂuß. Zeitschrift für Experimentelle und\nAngewandte Psychologie, 27:400–414.\nAustin F. Frank and T. Florian Jaeger. 2008. Speaking\nrationally: Uniform information density as an opti-\nmal strategy for language production. In Proceed-\nings of the Annual Meeting of the Cognitive Science\nSociety, volume 30.\nSusanne Gahl and Susan M. Garnsey. 2004. Knowl-\nedge of grammar, knowledge of usage: Syntactic\nprobabilities affect pronunciation variation. Lan-\nguage, pages 748–775.\nYingbo Gao, Weiyue Wang, Christian Herold, Zijian\nYang, and Hermann Ney. 2020. Towards a bet-\nter understanding of label smoothing in neural ma-\nchine translation. In Proceedings of the 1st Con-\nference of the Asia-Paciﬁc Chapter of the Associa-\ntion for Computational Linguistics and the 10th In-\nternational Joint Conference on Natural Language\nProcessing, pages 212–223, Suzhou, China. Associ-\nation for Computational Linguistics.\nEdward Gibson. 2000. The dependency locality the-\nory: A distance-based theory of linguistic complex-\nity. Image, language, brain: Papers from the ﬁrst\nmind articulation project symposium, 2000:95–126.\nMandy Guo, Zihang Dai, Denny Vrande ˇci´c, and Rami\nAl-Rfou. 2020. Wiki-40B: Multilingual language\nmodel dataset. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n2440–2452, Marseille, France. European Language\nResources Association.\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Second Meeting of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nKathleen Currie Hall, Elizabeth Hume, T. Florian\nJaeger, and Andrew Wedel. 2018. The role of pre-\ndictability in shaping phonological patterns. Lin-\nguistics Vanguard, 4(s2).\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2020. The curious case of neural text degen-\neration. In Proceedings of the International Confer-\nence on Learning Representations.\nT. Florian Jaeger. 2010. Redundancy and reduc-\ntion: Speakers manage syntactic information den-\nsity. Cognitive Psychology, 61(1).\nT. Florian Jaeger and Harry Tily. 2011. On language\n‘utility’: Processing complexity and communicative\nefﬁciency. Wiley Interdisciplinary Reviews: Cogni-\ntive Science, 2.\nAyush Jain, Vishal Singh, Sidharth Ranjan, Rajakrish-\nnan Rajkumar, and Sumeet Agarwal. 2018. Uniform\nInformation Density effects on syntactic choice in\nHindi. In Proceedings of the Workshop on Linguis-\ntic Complexity and Natural Language Processing ,\npages 38–48, Santa Fe, New-Mexico. Association\nfor Computational Linguistics.\nEdwin T. Jaynes. 1957. Information Theory and Statis-\ntical Mechanics. Physical Review, 106(4):620.\nDaniel Jurafsky, Alan Bell, Michelle Gregory, and\nWilliam D. Raymond. 2001. Probabilistic relations\nbetween words: Evidence from reduction in lexi-\ncal production. Typological Studies in Language ,\n45:229–254.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In MT Summit, pages\n79–86.\nIlia Kulikov, Alexander Miller, Kyunghyun Cho, and\nJason Weston. 2019. Importance of search and eval-\nuation strategies in neural dialogue modeling. In\nProceedings of the 12th International Conference on\nNatural Language Generation, pages 76–87, Tokyo,\nJapan. Association for Computational Linguistics.\nVictor Kuperman and Joan Bresnan. 2012. The effects\nof construction probability on word durations during\nspontaneous incremental sentence production. Jour-\nnal of Memory and Language, 66(4):588–611.\nRoger P. Levy and T. F. Jaeger. 2007. Speakers op-\ntimize information density through syntactic reduc-\ntion. In Advances in Neural Information Processing\nSystems.\nMolly L. Lewis and Michael C. Frank. 2016. The\nlength of words reﬂects their conceptual complexity.\nCognition, 153:182–195.\nClara Meister, Ryan Cotterell, and Tim Vieira. 2020a.\nIf beam search is the answer, what was the question?\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2173–2185, Online. Association for Computa-\ntional Linguistics.\nClara Meister, Elizabeth Salesky, and Ryan Cotterell.\n2020b. Generalized entropy regularization or:\nThere’s nothing special about label smoothing. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, Online. Asso-\nciation for Computational Linguistics.\nGábor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In Proceedings of the International Confer-\nence on Learning Representations.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language mod-\neling at multiple scales. CoRR, abs/1803.08240.\n5201\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proceedings of the International Conference\non Learning Representations.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian\nRoark, and Jason Eisner. 2019. What kind of lan-\nguage is hard to language-model? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4975–4989, Florence,\nItaly. Association for Computational Linguistics.\nClaire Moore-Cantwell. 2013. Syntactic predictability\ninﬂuences duration. In Proceedings of Meetings on\nAcoustics. Acoustical Society of America.\nRafael Müller, Simon Kornblith, and Geoffrey E. Hin-\nton. 2019. When does label smoothing help? In Ad-\nvances in Neural Information Processing Systems.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMark Pluymaekers, Mirjam Ernestus, and R. Harald\nBaayen. 2005. Lexical frequency and acoustic re-\nduction in spoken dutch. The Journal of the Acous-\ntical Society of America, 118(4):2561–2569.\nRaphael Rubino, Ekaterina Lapshinova-Koltunski, and\nJosef van Genabith. 2016. Information density and\nquality estimation features as translationese indica-\ntors for human translation classiﬁcation. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n960–970, San Diego, California. Association for\nComputational Linguistics.\nScott Seyfarth. 2014. Word informativity inﬂuences\nacoustic duration: Effects of contextual predictabil-\nity on lexical representation. Cognition, 133(1):140–\n155.\nClaude E. Shannon. 1948. A mathematical theory of\ncommunication. The Bell System Technical Journal,\n27(3):379–423.\nLes Sikos, Clayton Greenberg, Heiner Drenhaus, and\nMatthew W. Crocker. 2017. Information density of\nencodings: The role of syntactic variation in compre-\nhension. In Proceedings of the 39th Annual Meeting\nof the Cognitive Science Society.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2818–2826.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\n5202\nA Appendix\nDatasets. Table 5 shows the train, dev, and test set splits for the language modeling datasets we use.\nV ocab Train Dev Test\nLanguage Family Source Split size Sentences Tokens Sentences Tokens Sentences Tokens\nEnglish Indo-European EuroParl 80–10–10 64k 1.6M 46.7M 201k 5.8M 201k 5.8M\nWMT’06 80–10–10 62k 751k 17.0M 2.0k 61k 3.1k 90k\nWT-103 provided 268k 1.8M 103.2M 3.8k 217k 4.4k 246k\nCzech Indo-European EuroParl 80–10–10 64k 517k 12.4M 65k 1.6M 65k 1.6M\nFrench Indo-European EuroParl 80–10–10 64k 1.6M 51.3M 201k 6.4M 201k 6.3M\nGerman Indo-European EuroParl 80–10–10 64k 1.5M 42.3M 192k 5.4M 192k 5.2M\nSpanish Indo-European EuroParl 80–10–10 64k 1.6M 47.2M 197k 6.0M 197k 5.9M\nFinnish Uralic Wiki-40B provided 128k 256k 59.3M 14.1k 3.9M 14.0k 3.2M\nIndonesian Austronesian Wiki-40B provided 128k 156k 45.7M 8.7k 3.1M 8.6k 2.5M\nTagalog Austronesian Wiki-40B provided 128k 26k 4.2M 1.5k 270k 1.4k 220k\nTurkish Turkic Wiki-40B provided 128k 143k 38.1M 7.8k 2.5M 7.7k 1.9M\nSwahili Niger-Congo Wikipedia 80–10–10 128k 406k 6.3M 51k 800k 51k 803k\nTable 5: Train, dev, and test splits, as well as vocab size, for the language modeling datasets that we use in this paper.\nIf train-dev-test splits were provided, then we used them. Otherwise, we performed a 80–10–10 train-dev-test split.\nWe found a vocab size of 64k to cover more than 98% of the training set for the Indo-European languages, and a\nvocab size of 62k allowed us to cover 100% in the training set of English WMT’06. For the remaining languages,\nwhich had larger vocabularies, we followed Wiki-40B (Guo et al., 2020) and increased the vocab size to 128k.\nHyperparameters. Table 6 shows the optimizedβhyperparameter from a grid-search over β ∈{0.006,\n0.008, 0.01, 0.02, 0.03, 0.04, 0.05}for both regularizers on all datasets we use. Notably, the best βfor\nvariance ranged from 1×10−2 to 5×10−2, and the best βfor local consistency ranged from 6×10−3 to\n2×10−2. For use on a new dataset, we recommend starting with 1×10−2, which we found almost always\nimproved perplexity for both regularizers (on these datasets, at least).\nUID Regularizer\nVariance Local Consistency\nLanguage Source Best β Dev Loss Best β Dev Loss\nEnglish EuroParl (full dataset) 2 ×10−2 4.519 8×10−3 4.529\nEuroParl (2M subset) 2 ×10−2 6.497 1×10−2 6.497\nEuroParl (4M subset) 2 ×10−2 5.940 1×10−2 5.948\nEuroParl (8M subset) 2 ×10−2 5.500 8×10−3 5.511\nEuroParl (12M subset) 2 ×10−2 5.236 8×10−3 5.230\nEuroParl (16M subset) 5 ×10−2 5.084 2×10−2 5.089\nEuroParl (24M subset) 4 ×10−2 4.841 2×10−2 4.843\nEuroParl (32M subset) 1 ×10−2 4.747 1×10−2 4.742\nWMT’06 3 ×10−2 4.974 1×10−2 4.991\nWT-103 1 ×10−2 4.933 8×10−3 4.939\nCzech EuroParl 3 ×10−2 5.388 1×10−2 5.391\nFrench EuroParl 1 ×10−2 4.161 6×10−3 4.162\nGerman EuroParl 2 ×10−2 4.782 8×10−3 4.779\nSpanish EuroParl 3 ×10−2 4.539 1×10−2 4.550\nFinnish Wiki-40B 1 ×10−2 5.811 6×10−3 5.819\nIndonesian Wiki-40B 3 ×10−2 5.808 8×10−3 5.809\nTagalog Wiki-40B 4 ×10−2 6.319 8×10−3 6.319\nTurkish Wiki-40B 3 ×10−2 6.119 8×10−3 6.121\nSwahili Wikipedia 2 ×10−2 5.555 6×10−3 5.546\nTable 6: Best βhyperparameters and dev losses for all experiments.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6784230470657349
    },
    {
      "name": "Computational linguistics",
      "score": 0.6372047662734985
    },
    {
      "name": "Cognitive linguistics",
      "score": 0.5658313632011414
    },
    {
      "name": "Natural language processing",
      "score": 0.5178270936012268
    },
    {
      "name": "Cognition",
      "score": 0.5019519329071045
    },
    {
      "name": "Cognitive science",
      "score": 0.4950689375400543
    },
    {
      "name": "Joint (building)",
      "score": 0.49222317337989807
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4848140776157379
    },
    {
      "name": "Association (psychology)",
      "score": 0.4684443771839142
    },
    {
      "name": "Linguistics",
      "score": 0.4523346424102783
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45102518796920776
    },
    {
      "name": "Psychology",
      "score": 0.20045554637908936
    },
    {
      "name": "Engineering",
      "score": 0.12422844767570496
    },
    {
      "name": "Philosophy",
      "score": 0.11531251668930054
    },
    {
      "name": "Neuroscience",
      "score": 0.06533679366111755
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}