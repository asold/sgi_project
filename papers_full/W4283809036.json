{
    "title": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
    "url": "https://openalex.org/W4283809036",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2231835763",
            "name": "Zizhao Zhang",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2003434428",
            "name": "Han Zhang",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2126190421",
            "name": "Long Zhao",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2099725907",
            "name": "Ting Chen",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2050530885",
            "name": "Sercan O. Arik",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2798413642",
            "name": "Tomas Pfister",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2231835763",
            "name": "Zizhao Zhang",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2003434428",
            "name": "Han Zhang",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2126190421",
            "name": "Long Zhao",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2099725907",
            "name": "Ting Chen",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2050530885",
            "name": "Sercan O. Arik",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2798413642",
            "name": "Tomas Pfister",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6777047548",
        "https://openalex.org/W2765793020",
        "https://openalex.org/W2950557962",
        "https://openalex.org/W2989226908",
        "https://openalex.org/W6736170873",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W6637568146",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6790703111",
        "https://openalex.org/W3157394974",
        "https://openalex.org/W2994028575",
        "https://openalex.org/W2947881255",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2981613521",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W3100702833",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W3001197829",
        "https://openalex.org/W6734194636",
        "https://openalex.org/W2946948417",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3135921327",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3107036272",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W6752378368",
        "https://openalex.org/W3139633126",
        "https://openalex.org/W2798232928",
        "https://openalex.org/W2295107390",
        "https://openalex.org/W4214669216",
        "https://openalex.org/W4298289240",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W2964274719",
        "https://openalex.org/W4298395628",
        "https://openalex.org/W3179869055",
        "https://openalex.org/W4287572671",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W3173119568",
        "https://openalex.org/W3129603602",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W4230405732",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4376983087",
        "https://openalex.org/W2950670227",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W4287197288",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3210279979",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W3008736151",
        "https://openalex.org/W4287320642",
        "https://openalex.org/W2601564443",
        "https://openalex.org/W3154596443",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W3034512672",
        "https://openalex.org/W4299802238"
    ],
    "abstract": "Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8 times faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.",
    "full_text": "Nested Hierarchical Transformer: Towards Accurate, Data-Efﬁcient and\nInterpretable Visual Understanding\nZizhao Zhang1, Han Zhang2, Long Zhao2, Ting Chen2, Sercan ¨O. Arık1, Tomas Pﬁster1\n1Google Cloud AI\n2Google Research\n{zizhaoz,zhanghan,longzh,iamtingchen,soarik,tpﬁster}@google.com\nAbstract\nHierarchical structures are popular in recent vision transform-\ners, however, they require sophisticated designs and massive\ndatasets to work well. In this paper, we explore the idea of\nnesting basic local transformers on non-overlapping image\nblocks and aggregating them in a hierarchical way. We ﬁnd\nthat the block aggregation function plays a critical role in\nenabling cross-block non-local information communication.\nThis observation leads us to design a simpliﬁed architec-\nture that requires minor code changes upon the original vi-\nsion transformer. The beneﬁts of the proposed judiciously-\nselected design are threefold: (1) NesT converges faster and\nrequires much less training data to achieve good general-\nization on both ImageNet and small datasets like CIFAR;\n(2) when extending our key ideas to image generation, NesT\nleads to a strong decoder that is 8 times faster than previ-\nous transformer-based generators; and (3) we show that de-\ncoupling the feature learning and abstraction processes via\nthis nested hierarchy in our design enables constructing a\nnovel method (named GradCAT) for visually interpreting the\nlearned model. Source code is available https://github.com/\ngoogle-research/nested-transformer.\nIntroduction\nVision Transformer (ViT) (Dosovitskiy et al. 2021) model\nand its variants have received signiﬁcant interests recently\ndue to their superior performance on many core visual ap-\nplications (Cordonnier, Loukas, and Jaggi 2020; Liu et al.\n2021). ViT ﬁrst splits an input image into patches, and then\npatches are treated in the same way as tokens in NLP ap-\nplications. Following, several self-attention layers are used\nto conduct global information communication to extract\nfeatures for classiﬁcation. Recent work (Dosovitskiy et al.\n2021; Cordonnier, Loukas, and Jaggi 2020) shows that ViT\nmodels can achieve better accuracy than state-of-the-art con-\nvnets (Tan and Le 2019; He et al. 2016) when trained on\ndatasets with tens or hundreds of millions of labeled sam-\nples. However, when trained on smaller datasets, ViT usually\nunderperforms its counterparts based on convolutional lay-\ners. Addressing this data inefﬁciency is important to make\nViT applicable to other application scenarios, e.g. semi-\nsupervised learning (Sohn et al. 2020) and generative mod-\neling (Goodfellow et al. 2014; Zhang et al. 2019).\nCopyright c⃝ 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nLack of inductive bias such as locality and translation\nequivariance, is one explanation for the data inefﬁciency of\nViT models. Cordonnier, Loukas, and Jaggi (2020) discov-\nered that transformer models learn locality behaviors in a\ndeformable convolution manner (Dai et al. 2017): bottom\nlayers attend locally to the surrounding pixels and top lay-\ners favor long-range dependency. On the other hand, global\nself-attention between pixel pairs in high-resolution images\nis computationally expensive. Reducing the self-attention\nrange is one way to make the model training more computa-\ntionally efﬁcient (Beltagy, Peters, and Cohan 2020). These\ntype of insights align with the recent structures with lo-\ncal self-attention and hierarchical transformer (Han et al.\n2021; Vaswani et al. 2021; Liu et al. 2021). Instead of\nholistic global self-attention, these perform attention on lo-\ncal image patches. To promote information communication\nacross patches, they propose specialized designs such as the\n“haloing operation” (Vaswani et al. 2021) and “shifted win-\ndow” (Liu et al. 2021). These are based on modifying the\nself-attention mechanism and often yields in complex archi-\ntectures. Our design goal on the other hand keeping the at-\ntention as is, and introducing the design of the aggregation\nfunction, to improve the accuracy and data efﬁciency, while\nbringing interpretability beneﬁts.\nThe proposed NesT model stacks canonical transformer\nblocks to process non-overlapping image blocks individu-\nally. Cross-block self-attention is achieved by nesting these\ntransformers hierarchically and connecting them with a pro-\nposed aggregation function. Fig. 1 illustrates the overall ar-\nchitecture and the simple pseudo code to generate it. Our\ncontributions can be summarized as:\n1. We demonstrate integrating hierarchically nested trans-\nformers with the proposed block aggregation func-\ntion can outperform previous sophisticated (local) self-\nattention variants, leading to a substantially-simpliﬁed\narchitecture and improved data efﬁciency. This pro-\nvides a novel perspective for achieving effective cross-\nblock communication.\n2. NesT achieves impressive ImageNet classiﬁcation ac-\ncuracy with a signiﬁcantly simpliﬁed architectural de-\nsign. E.g., training a NesT with 38M/68M parameters\nobtains 83:3%=83:8% ImageNet accuracy.The favor-\nable data efﬁciency of NesT is embodied by its fast\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3417\n6 7 8 9\n32 4 5\n1\nGolden retriever\nLinear projection\nBlock aggregation\n6 7\n3\n8\n4\n9\n5\nBlock aggregation\nBlocking image to \npatches [6, 7, 8, 9, …]\n(#block, seqlen, d)\n(#block/4, seqlen, d)\n… …\n(#block/16, seqlen, d)\nPseudo code: NesT\n# embed and block image to (#block,seqlen,d)\nx = Block(PatchEmbed(input_image))\nfor i in range(num_hierarchy):\n# apply transformer layers T_i within each block\n# with positional encodings (PE)\ny = Stack([T_i(x[0] + PE_i[0]), ...])\nif i < num_hierarchy - 1:\n# aggregate blocks and reduce #block by 4\nx = Aggregate(y, i)\nh = GlobalAvgPool(x) # (1,seqlen,d) to (1,1,d)\nlogits = Linear(h[0,0]) # (num_classes,)\ndef Aggregate(x, i):\nz = UnBlock(x) # unblock seqs to (h,w,d)\nz = ConvNormMaxPool_i(x) # (h/2,w/2,d)\nreturn Block(z) # block to seqs\nFigure 1: (Left) Illustration of NesT with nested transformer hierarchy; (right) the simple pseudo code to generate the archi-\ntecture. Each node T i processes an image block. The block aggregation is performed between hierarchies (num hierarchy= 3\nhere) to achieve cross-block communication on the image (feature map) plane.\nconvergence, such as achieving 75.9%/82.3% training\nwith 30/100 epochs. Moreover, NesT achieves matched\naccuracy on small datasets compared with popular con-\nvolutional architectures. E.g., training a NesT with 6M\nparameters using a single GPU results in96% accuracy\non CIFAR10 .\n3. We show that when extending this idea beyond classi-\nﬁcation to image generation, NesT can be repurposed\ninto a strong decoder that achieves better performance\nthan convolutional architectures meanwhile has com-\nparable speed, demonstrated by64×64 ImageNet gen-\neration, which is an important to be able to adopt trans-\nformers for efﬁcient generative modeling.\n4. Our proposed architectural design leads to decoupled\nfeature learning and abstraction, which has signiﬁ-\ncant interpretability beneﬁts. To this end, we propose\na novel method called GradCAT to interpret NesT rea-\nsoning process by traversing its tree-like structure. This\nproviding a new type of visual interpretability that ex-\nplains how aggregated local transformers selectively\nprocess local visual cues from semantic image patches.\nRelated Work\nVision transformer-based models (Cordonnier, Loukas, and\nJaggi 2020; Dosovitskiy et al. 2021) and self-attention\nmechanisms (Vaswani et al. 2021; Ramachandran et al.\n2019) have recently attracted signiﬁcant interest in the re-\nsearch community, with explorations of more suitable ar-\nchitectural designs that can learn visual representation ef-\nfectively, such as injecting convolutional layers (Li et al.\n2021; Srinivas et al. 2021; Yuan et al. 2021) and building\nlocal or hierarchical structures (Zhang et al. 2021; Wang\net al. 2021b). Existing methods focus on designing a variety\nof self-attention modiﬁcations. Hierarchical ViT structures\nbecomes popular both in vision (Liu et al. 2021; Vaswani\net al. 2021) and NLP (Zhang, Wei, and Zhou 2019; Santra,\nAnusha, and Goyal 2021; Liu and Lapata 2019; Pappagari\net al. 2019). However, many methods often add signiﬁcant\narchitectural complexity in order to optimize accuracy.\nOne challenge for vision transformer-based models is data\nefﬁciency. Although the original ViT (Dosovitskiy et al.\n2021) can perform better than convolutional networks with\nhundreds of millions images for pre-training, such a data re-\nquirement is not always practical. Data-efﬁcient ViT (DeiT)\n(Touvron et al. 2021a,b) attempts to address this problem\nby introducing teacher distillation from a convolutional net-\nwork. Although promising, this increases the supervised\ntraining complexity, and existing reported performance on\ndata efﬁcient benchmarks (Hassani et al. 2021; Chen et al.\n2021) still signiﬁcantly underperforms convolutional net-\nworks. Since ViT has shown to improve vision tasks beyond\nimage classiﬁcation, with prior work studying its applicabil-\nity to generative modeling (Parmar et al. 2018; Child et al.\n2019; Jiang, Chang, and Wang 2021; Hudson and Zitnick\n2021), video understanding (Neimark et al. 2021; Akbari\net al. 2021), segmentation and detection (Wang et al. 2021a;\nLiang et al. 2020; Kim et al. 2021), interpretability (Chefer,\nGur, and Wolf 2021; Abnar and Zuidema 2020), a deeper\nunderstanding of the data efﬁciency and training difﬁculties\nfrom the architectural perspective is of signiﬁcant impact.\nProposed Method\nMain Architecture\nAccording to Fig. 1, our overall design stacks canonical\ntransformer layers to conduct local self-attention on every\nimage block independently, and then nests them hierarchi-\ncally. Coupling of processed information between spatially\nadjacent blocks is achieved through a proposed block ag-\ngregation between every two hierarchies. The overall hi-\nerarchical structure can be determined by two key hyper-\nparameters: patch size S ×S and number of block hierar-\nchies Td. All blocks inside each hierarchy share one set of\nparameters.\nGiven an input of image with shape H ×W ×3, each\nimage patch with size S ×S is linearly projected to an\n3418\nTop1: Blenheim spaniel Top2: Tabby cat\nTop3: Tigger cat Top4: Sussex spaniel\nFigure 2: Example results of the proposed GradCAT. Given the left input image (containing four objects), the ﬁgure visualizes\nthe top-4 class traversal results (4 colors) using an ImageNet-trained NesT (with three tree hierarchies). Each tree node denotes\nthe averaged activation value (^hl deﬁned in Algorithm 1). The traversals can correctly ﬁnd the model decision path along the\ntree to locate an image patch belonging to the objects of given target classes.\nembedding in Rd. Then, all embeddings are partitioned to\nblocks and ﬂattened to generate input X ∈Rb×Tn×n×d ,\nwhere b is the batch size, Tn is the total number of blocks\nat bottom of the NesT hierarchy, and n is the sequence\nlength (the number of embeddings) at each block. Note that\nTn ×n = H ×W=S2.\nInside each block, we stack a number of canonical trans-\nformer layers, where each is composed of a multi-head self-\nattention (MSA) layer followed by a feed-forward fully-\nconnected network (FFN) with skip-connection (He et al.\n2016) and Layer normalization (LN) (Ba, Kiros, and Hin-\nton 2016). Trainable positional embedding vectors (Touvron\net al. 2021a) are added to all sequence vectors in Rd to en-\ncode spatial information before feeding into the block func-\ntion T:\nmultiple ×\n\u001ay = x + MSANesT(x′; x′; x′); x′= LN(x)\nx = y + FFN(LN(y))\n(1)\nThe FFN is composed of two layers: max(0; xW1+b)W2+b.\nGiven input X ∈Rb×Tn×n×d , since all blocks at one NesT\nhierarchy share the same parameters, MSA NesT basically\nMSA is applied (Vaswani et al. 2017) to all blocks in par-\nallel:\nMSANesT(Q; K; V) =Stack(block1; :::;blockTn );\nwhere blocki = MSA(Q; K; V)WO: (2)\nblocki has shape b ×n ×d. Lastly, we build a nested hierar-\nchy with block aggregation – every four spatially connected\nblocks are merged into one. The overall design makes NesT\neasy to implement, requiring minor code changes to the orig-\ninal ViT.\nBlock Aggregation\nFrom a high-level view, NesT leads to hierarchical repre-\nsentations, which share similarity with several pyramid de-\nsigns (Zhang et al. 2021; Wang et al. 2021b). However, most\nof these works use global self-attention throughout the lay-\ners, interleaved with (spatial) down-sampling. In contrast,\nwe show that NesT, which leverages local attention, can\nlead to signiﬁcantly improved data efﬁciency. In local self-\nattention, non-local communication is important to main-\ntain translational equivariance (Vaswani et al. 2021). To this\nend, Halonet (Vaswani et al. 2021) allows the query to at-\ntend to slightly larger regions than the assigned block. Swin\nTransformer (Liu et al. 2021) achieves this by shifting the\nblock partition windows between consecutive self-attention\nlayers to connect adjacent blocks; applying special masked\nself-attention to guarantee spatial continuity. However, both\nadd complexity to the self-attention layers and such sophisti-\ncated architectures are not desired from implementation per-\nspective.\nOn the other hand, every block in NesT processes in-\nformation independently via standard transformer layers,\nand only communicate and mix global information dur-\ning the block aggregation step via simple spatial opera-\ntions (e.g. convolution and pooling). One key ingredient of\nblock aggregation is to perform it in the image plane so\nthat information can be exchanged between nearby blocks.\nThis procedure is summarized in Fig. 1. The output Xl ∈\nRb×#block×n×d at hierarchy l is unblocked to the full im-\nage plane Al ∈ Rb×H0×W 0×d0\n. A number of spatial op-\nerations are applied to down-sample feature maps A′\nl ∈\nRb×H0=2×W 0=2×d. Finally, the feature maps are blocked\nback to Xl+1 ∈Rb×#block=4×n×d0\nfor hierarchy l + 1. The\nsequence length n always remains the same and the total\nnumber of blocks is reduced by a factor of 4, until reduced to\n1 at the top (i.e. #block=4(Td−1) = 1). Therefore, this pro-\ncess naturally creates hierarchically nested structure where\nthe “receptive ﬁeld” expands gradually. d′≥d depends on\nthe speciﬁc model conﬁguration.\nOur block aggregation is specially instantiated as a 3 ×3\nconvolution followed by LN and a 3 ×3 max pooling. Fig-\nure A2 in Appendix explains the core design and the im-\nportance of applying it on the image plane (i.e. full im-\nage feature maps) versus the block plane (i.e. partial feature\nmaps corresponding to 2 ×2 blocks that will be merged).\nThe small information exchange through the small convolu-\ntion and max. pooling kernels across block boundaries are\nparticularly important. We conduct comprehensive ablation\n3419\nstudies to demonstrate the importance of each of the design\ncomponents.\nNote that the resulting design shares some similarities\nwith recent works that combine transformer and convolu-\ntional networks (Wu et al. 2021; Yuan et al. 2021; Bello\n2021) as specialized hybrid structures. However, unlike\nthese, our proposed method aims to solve cross-block com-\nmunications in local self-attention, and the resulting archi-\ntecture is simple as a stacking of basic transformer layers.\nTransposed NesT for Image Generation\nThe data efﬁciency and straightforward implementation of\nNesT makes it desirable for more complex learning tasks.\nWith transpose the key ideas from NesT to propose a de-\ncoder for generative modeling, and show that it has better\nperformance than convolutional decoders with comparable\nspeed. Remarkably, it is nearly a magnitude faster than the\ntransformer-based decoder TransGAN (Jiang, Chang, and\nWang 2021).\nCreating such a generator is straightforward by transpos-\ning NesT (see Table A6 of Appendix for architecture de-\ntails). The input of the model becomes a noise vector and\nthe output is a full-sized image. To support the gradually in-\ncreased number of blocks, the only modiﬁcation to NesT\nis replacing the block aggregation with appropriate block\nde-aggregation, i.e. up-sampling feature maps (we use pixel\nshufﬂe (Shi et al. 2016)). The feature dimensions in all hi-\nerarchies are (b; nd) → (b; 1; n; d)→ (b; 4; n; d′); :::;→\n(b; #blocks; n;3). The number of blocks increases by a fac-\ntor of 4. Lastly, we can unblock the output sequence tensor\nto an image with shapeH ×W ×3. The remaining adversar-\nial training techniques are based on (Goodfellow et al. 2014;\nZhang et al. 2019) as explained in experiments. Analogous\nto our results for image classiﬁcation, we show the impor-\ntance of careful block de-aggregation design, in making the\nmodel signiﬁcantly faster while achieving better generation\nquality.\nGradCAT: Interpretability via Tree Traversal\nDifferent from previous work, the nested hierarchy with\nthe independent block process in NesT resembles a deci-\nsion tree in which each block is encouraged to learn non-\noverlapping features and be selected by the block aggrega-\ntion. This unique behavior motivates us to explore a new\nmethod to explain the model reasoning, which is an impor-\ntant topic with signiﬁcant real world impact in convnets (Sel-\nvaraju et al. 2017; Sundararajan, Taly, and Yan 2017).\nWe present a gradient-based class-aware tree-traversal\n(GradCAT) method (Algorithm 1). The main idea is to ﬁnd\nthe most valuable traversal from a child node to the root\nnode that contributes to the classiﬁcation logits the most. In-\ntuitively, at the top hierarchy, each of four child nodes pro-\ncesses one of 2 ×2 non-overlapping partitions of feature\nmaps ATd . We can use corresponding activation and class-\nspeciﬁc gradient features to trace the high-value informa-\ntion ﬂow recursively from the root to a leaf node. The nega-\ntive gradient −@Yc\nAl\nprovides the gradient ascent direction to\nAlgorithm 1: GradGAT\nDeﬁne: Aldenotes the feature maps at hierarchyl. Ycis the logit\nof predicted classc. [·]2\u00022 indexes one of2×2 partitions of input\nmaps.\nInput: {Al|l= 2;:::;T d};\u000bTd = ATd , P = []\nOutput: The traversal path P from top to bottom\nfor l= [Td;:::; 2] do\nhl = \u000bl ·(−@Yc\n\u000bl\n) # obtain target activation maps\n^hl = AvgPool2\u00022(hl) ∈R2\u00022\nn\u0003\nl = arg max^hl, P = P+ [n\u0003\nl] # pick the maximum index\n\u000bl = Al[n\u0003\nl]2\u00022 # obtain the partition for the index\nend forArch. base Method C10 (%) C100 (%)\nConvolutional Pyramid-164-48 95.97 80.70\nWRN28-10 95.83 80.75\nTransformer\nfull-attention\nDeiT-T 88.39 67.52\nDeiT-S 92.44 69.78\nDeiT-B 92.41 70.49\nPVT-T 90.51 69.62\nPVT-S 92.34 69.79\nPVT-B 85:05? 43:78?\nCCT-7/3×1 94.72 76.67\nTransformer\nlocal-attention\nSwin-T 94.46 78.07\nSwin-S 94.17 77.01\nSwin-B 94.55 78.45\nNesT-T 96.04 78.69\nNesT-S 96.97 81.70\nNesT-B 97.20 82.56\nTable 1: Test accuracy on CIFAR with input size32×32. The\ncompared convolutional architectures are optimized models\nfor CIFAR. All transformer-based architectures are trained\nfrom random initialization with the same data augmentation.\nDeiT uses S = 2. Swin and our NesT uses S = 1.? means\nmodel tends to diverge.\nmaximize the classc logit, i.e., a higher positive value means\nhigher importance. Fig. 2 illustrates a sample result.\nExperiments\nWe ﬁrst show the beneﬁt of NesT for data efﬁcient learning\nand then demonstrate beneﬁts for interpretability and gener-\native modeling. Finally, we present ablation studies to ana-\nlyze the major constituents of the methods.\nExperimental setup. We follow previous work (Dosovit-\nskiy et al. 2021) to generate three architectures that have\ncomparable capacity (in number of parameters and FLOPS),\nnoted as tiny (NesT-T), small (NesT-S), and base (NesT-\nB). Most recent ViT-based methods follow the training tech-\nniques of DeiT (Touvron et al. 2021a). We follow the set-\ntings with minor modiﬁcations that we ﬁnd useful for local\nself-attention (see Appendix for all architecture and training\ndetails). We do not explore the speciﬁc per-block conﬁgura-\ntions (e.g. number of heads and hidden dimensions), which\nwe believe can be optimized through architecture search\n(Tan and Le 2019).\n3420\nArch. base Method #Params Top-1 acc. (%)\nConvolutional\nResNet-50 25M 76.2\nRegNetY-4G 21M 80.0\nRegNetY-16G 84M 82.9\nTransformer\nfull-attention\nViT-B/16 86M 77.9\nDeiT-S 22M 79.8\nDeiT-B 86M 81.8\nTransformer\nlocal-attention\nSwin-T 29M 81.3\nSwin-S 50M 83.0\nSwin-B 88M 83.3\nNesT-T 17M 81.5\nNesT-S 38M 83.3\nNesT-B 68M 83.8\nTable 2: Comparison on the ImageNet dataset. All models\nare trained from random initialization. ViT-B/16 uses an im-\nage size 384 and others use 224.\nViT-B/16 Swin-B Nest-B\nImageNet Acc. (%) 84.0 86.0 86.2\nTable 3: Comparison on ImageNet benchmark with\nImageNet-22K pre-training.\nComparisons to Previous Work\nCIFAR. We compare NesT to recent methods on CIFAR\ndatasets (Krizhevsky, Hinton et al. 2009) in Table 1, to in-\nvestigate the data efﬁciency. It is known that transformer-\nbased methods usually perform poorly on such tasks as they\ntypically require large datasets to be trained on. The mod-\nels that perform well on large-scale ImageNet do not nec-\nessary work perform on small-scale CIFAR, as the full self-\nattention based models require larger training datasets. DeiT\n(Touvron et al. 2021a) performs poorly and does not im-\nprove given bigger model size. PVT (Wang et al. 2021b) has\nalso a full self-attention based design, though with a pyra-\nmid structure. PVT-T seems to perform better than DeiT-T\nwhen model size is small, however, the performance largely\ndrops and becomes unstable when scaling up, further sug-\ngesting that full self-attention at bottom layers is not de-\nsirable for data efﬁciency. Other transformer-based methods\nimprove slowly with increasing model size, suggesting that\nbigger models are more challenging to train with less data.\nWe attribute this to to their complex design (i.e. shifted win-\ndows with masked MSA) requiring larger training datasets,\nwhile NesT beneﬁting from a judiciously-designed block ag-\ngregation. We also include comparisons with convolutional\narchitectures that are speciﬁcally optimized for small CI-\nFAR images and show that NesT can give better accuracy\nwithout any small dataset speciﬁc architecture optimizations\n(while still being larger and slower, as they do not incorpo-\nrate convolutional inductive biases). The learning capacity\nand performance of NesT get better with increased model\nsize. Most variants of NesT in Fig. A1 of Appendix out-\nperform compared methods with far better throughput. E.g.,\nNesT3-T (S = 2) leads to 94:5% CIFAR10 accuracy with\n5384 images/s throughout, 10×faster than the best com-\npared result 94:6% accuracy. More details can be found in\nAppendix.\nImageNet. We test NesT on standard ImageNet 2012 bench-\nmarks (Deng et al. 2009) with commonly used 300 epoch\ntraining on TPUs in Table 2. The input size is 224 ×224\nand no extra pre-training data is used. DeiT does not use\nteacher distillation, so it can be viewed as ViT (Dosovit-\nskiy et al. 2021) with better data augmentation and reg-\nularization. NesT matches the performance of prior work\nwith a signiﬁcantly more straightforward design (e.g. NesT-\nS matches the accuracy of Swin-B, 83:3%). The results\nof NesT suggest that correctly aggregating the local trans-\nformer can improve the performance of local self-attention.\nImageNet-22K. We scale up NesT to ImageNet-22K fol-\nlowing the exact training schedules in (Liu et al. 2021;\nDosovitskiy et al. 2021). The pre-training is 90 epoch on\n224×224 ImageNet21K images and ﬁnetuning is 30 epoch\non 384×384 ImageNet images. Table 3 compares the results.\nNesT again achieves competitive results, with a signiﬁcantly\nmore straightforward design.\nVisual Interpretability\nGradGAT results. Fig. 3 (left) shows the explanations ob-\ntained with the proposed GradGAT. For GradGAT, each tree\nnode corresponds to a value that reﬂects the mean activa-\ntion strength. Visualizing the tree traversal through image\nblocks, we can get insights about the decision making pro-\ncess of NesT. The traversal passes through the path with the\nhighest values. As can be seen, the decision path can cor-\nrectly locate the object corresponding to the model predic-\ntion. The Lighter example is particularly interesting because\nthe ground truth class – lighter/matchstick – actually deﬁnes\nthe bottom-right matchstick object, while the most salient\nvisual features (with the highest node values) are actually\nfrom the upper-left red light, which conceptually shares vi-\nsual cues with a lighter. Thus, although the visual cue is a\nmistake, the output prediction is correct. This example re-\nveals the potential of using GradGAT to conduct model di-\nagnosis at different tree hierarchies. Fig. A5 of Appendix\nshows more examples.\nClass attention map (CAM) results. In contrast to ViT\n(Dosovitskiy et al. 2021) which uses class tokens, NesT uses\nglobal average pooling before softmax. This enables con-\nveniently applying CAM-like (Zhou et al. 2016) methods\nto interpret how well learned representations measure ob-\nject features, as the activation coefﬁcients can be directly\nwithout approximate algorithms. Fig. 3(right) shows quan-\ntitative evaluation of weakly-supervised object localization,\nwhich is a common evaluation metric for CAM-based meth-\nods (Zhou et al. 2016), including GradCAM++ (Chattopad-\nhay et al. 2018) with ResNet50 (He et al. 2016), DeiT with\nRollout attention (Abnar and Zuidema 2020), and our NesT\nCAM (Zhou et al. 2016). We follow this toolkit 1 to use an\nimproved version of Rollout. NesT with standard CAM, out-\nperforms others that are speciﬁcally designed for this task.\nFig. 4 shows a qualitative comparison (details in the Ap-\npendix), exemplifying that NesT can generate clearer atten-\ntion maps which converge better on objects.\n1https://github.com/jacobgil/vit-explain\n3421\nLighter\nMouse\nradio telescope\nRadio telescope\nWooden spoon\nMethod\nTop-1 loc.\nerr (%)\nDeiT RollOutx 67.6\nADL (Choe and Shim 2019) 55.1\nACoL (Zhang et al. 2018) 54.2\nR50 GradCAM++x 53.5\nNesT CAM 51.8\nFigure 3: Left: Output visualization of the proposed GradGAT. Tree nodes annotate the averaged responses to the predicted\nclass. We use a NesT-S with three tree hierarchies. Right: CAM-based weakly supervised localization comparison on the\nImageNet validation set. §indicates results obtained by us.\nResNet50 GradCAM++ DeiT Rollout NesT CAMInput Image\nKing \npenguin\nHouse \nﬁnch\nBittern\nGround truth\nFigure 4: Visualization of CAM-based attention results. All models are trained on ImageNet. CAM (vanilla) with NesT achieves\naccurate attention patterns on object regions, yielding ﬁner attention to objects than DeiT Rollout (Abnar and Zuidema 2020)\nand less noise than ResNet50 GradCAM++ (Chattopadhay et al. 2018).\nOverall, decoupling local self-attention (transformer\nblocks) and global information selection (block aggrega-\ntion), which is unique to our work, shows signiﬁcant poten-\ntial for making models easier to interpret.\nGenerative Modeling with Transposed NesT\nWe evaluate the generative ability of Transposed NesT on\nImageNet (Russakovsky et al. 2015) where all images are\nresized to 64 ×64 resolution. We focus on the uncondi-\ntional image generation setting to test the effectiveness of\ndifferent decoders. We compare Transposed NesT to Trans-\nGAN (Jiang, Chang, and Wang 2021), that uses a full Trans-\nformer as the generator, as well as a convolutional baseline\nfollowing the widely-used architecture from (Zhang et al.\n2019) (its computationally expensive self-attention module\nis removed). Fig. 5 shows the results. Transposed NesT ob-\ntains signiﬁcantly faster convergence and achieves the best\nFID and Inception score (see Fig. A6 of Appendix for re-\nsults). Most importantly, it achieves 8× throughput over\nTransGAN, showing its potential for signiﬁcantly improv-\ning the efﬁciency of transformer-based generative modeling.\nMore details are explained in the Appendix.\nIt is noticeable from Fig. 5 (middle) that appropriate un-\nsampling (or block de-aggregation) impacts the generation\nquality. Pixel shufﬂe (Shi et al. 2016) works the best and the\nmargin is considered surprisingly large compared to other\nalternatives widely-used in convnets. This aligns with our\nmain ﬁndings in classiﬁcation, suggesting that judiciously\ninjecting spatial operations is important for nested local\ntransformers to perform well.\nAblation Studies\nWe summarize key ablations below (more in Appendix).\nFast convergence. NesT achieves fast convergence, as\nshown in Fig. 6 (top) for Imagenet training with\n{30;60; 100; 300}epochs. NesT-B merely loses 1.5% when\nreducing the training epoch from 300 to 100. The results\nsuggest that NesT can learn effective visual features faster\nand more efﬁciently.\nLess sensitivity to data augmentation. NesT uses several\n3422\n200 400 600 800 1000\nIterations (1000x)\n20\n30\n40\n50FID\nComparison of architectures\nConvNet\nTransGAN\nTransposed NesT\n200 400 600 800 1000\nIterations (1000x)\n20\n40\n60\n80\n100FID\nComparison of block de-aggregration\nPS-C3\nC3-PS\nNN-C3\nC3-NN (failed)\nPS (Ours)\nMethod\n#Params\n(millions)\nThroughput\n(images/s)\nConvnet\n(Zhang et al. 2019) 77.8M 709.1\nTransGAN\n(Jiang, Chang, and Wang 2021) 82.6M 67.7\nTransposed NesT 74.4M 523.7\nFigure 5: Left: FID comparison for 64 ×64 ImageNet generation at different training iterations. Middle: FID comparison of\ndifferent popular un-sampling methods for block de-aggregation, including combinations of pixel shufﬂing (PS), Conv3x3 (C3),\nand nearest neighbor (NN). Right: The number of parameters and throughput of compared generators.\n50 100 150 200 250 300\nTotal training epochs\n55\n60\n65\n70\n75\n80\n85ImageNet accuracy\nDeiT-S\nSwin-S\nNesT-S\nNesT-B\nRemoved\nAugmentation Accuracy (%)\nImageNet\nDeiT-B NesT-T\nNone 81.8 81.5\nRandomErasing 4.3 81.4\nRandAugment 79.6 81.2\nCutMix&MixUp 75.8 79.8\nFigure 6: Top: Training convergence. NesT achieves better\nperformance than DeiT with the same total epoch of train-\ning (each point is a single run). Bottom: Data augmentation\nablations. Results of DeiT-B (Touvron et al. 2021a) are re-\nported by its paper. NesT shows less reliance on data aug-\nmentation.\nkinds of data augmentation following (Touvron et al. 2021a).\nAs shown in Fig. 6 (right) and Fig. A4, our method shows\nhigher stability in data augmentation ablation studies com-\npared to DeiT. Data augmentation is critical for global self-\nattention to generalize well, but reduced dependence on do-\nmain or task dependent data augmentation helps with gener-\nalization to other tasks.\nImpact of block aggregation.Here we show that the design\nof block aggregation is critical for performance and data ef-\nﬁciency. We study this from four perspectives: (1) whether\nunblocking to the full image plane is necessary; (2) how\nto use convolution; (3) what kinds of pooling to use; and\n(4) whether to perform query down-sampling inside self-\nattention (Vaswani et al. 2021). Fig. 7 and Fig. A3 of Ap-\npendix compare the results of different plausible designs.\nThe results show that: (1) when performing these spa-\ntial operations, it is important to apply it on the holistic\nimage plane versus the block plane although both can rea-\nsonably introduce spatial priors; (2) small kernel convolu-\ntion is sufﬁcient and has to be applied ahead of pooling; (3)\nOurs Conv3x3 PatchMergeAvgPool3x3\n82\n84\n86\n88\n90\n92\n94\n96Accuracy\nCIFAR10\nImage plane\nBlock plane\nOurs Conv3x3 PatchMergeAvgPool3x3\n55\n60\n65\n70\n75\n80\n CIFAR100\nImage plane\nBlock plane\nOurs Conv3x3 PatchMergeAvgPool3x3\n79.8\n80.0\n80.2\n80.5\n80.8\n81.0\n81.2\n81.5\nImageNet\nImage plane\nBlock plane\nFigure 7: Demonstration of the impact of block aggrega-\ntion on CIFAR and ImageNet. NesT-T is used. Conv3x3 has\nstride 2. AvgPool3x3 on ImageNet is followed by Conv1x1\nto change hidden dimensions of self-attention. Four plausi-\nble block aggregation designs are shown in x-axis, and ap-\nplied on the image plane and block plane both for compar-\nison. Note that Ours in x-axis is Conv3x3 followed by LN\nand MaxPool3x3 (stride 2). More alternatives are validated\nin Fig. A3 of Appendix.\nmax. pooling is far better than other options, such as stride-\n2 sub-sampling and average pooling; (4) sub-sampling the\nquery sequence length (similar to performing sub-sampling\non the block plane as illustrated in Fig. A2), as used by\nHalonet (Vaswani et al. 2021), performs poorly on data ef-\nﬁcient benchmarks. We also experiment PatchMerge from\nSwin Transformer (Liu et al. 2021) on both CIFAR and Im-\nageNet. Our block aggregation closes the accuracy gap on\nImageNet, suggesting that a conceptually negligible differ-\nence in aggregating nested transformers can lead to signiﬁ-\ncant differences in model performance.\nConclusion\nWe have shown that aggregating nested transformers can\nmatch the accuracy of previous more complex methods\nwith signiﬁcantly improved data efﬁciency and convergence\nspeed. In addition, we have shown that this idea can be\nextended to image generation, where it provides signiﬁ-\ncant speed gains. Finally, we have shown that the decou-\npled feature learning and feature information extraction in\nthis nested hierarchy design allows for better feature inter-\npretability through a new gradient-based class-aware tree\ntraversal method. In future work we plan to generalize this\nidea to non-image domains.\n3423\nAcknowledgements\nWe thank Xiaohua Zhai, Jeremy Kubica, Kihyuk Sohn, and\nMadeleine Udell for their valuable feedback on this paper.\nReferences\nAbnar, S.; and Zuidema, W. 2020. Quantifying attention\nﬂow in transformers. In ACL.\nAkbari, H.; Yuan, L.; Qian, R.; Chuang, W.-H.; Chang, S.-F.;\nCui, Y .; and Gong, B. 2021. V ATT: Transformers for Mul-\ntimodal Self-Supervised Learning from Raw Video, Audio\nand Text. arXiv:2104.11178.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv:1607.06450.\nBello, I. 2021. Lambdanetworks: Modeling long-range in-\nteractions without attention. arXiv:2102.08602.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer:\nThe long-document transformer. arXiv:2004.05150.\nChattopadhay, A.; Sarkar, A.; Howlader, P.; and Balasub-\nramanian, V . N. 2018. Grad-cam++: Generalized gradient-\nbased visual explanations for deep convolutional networks.\nIn WACV.\nChefer, H.; Gur, S.; and Wolf, L. 2021. Generic\nAttention-model Explainability for Interpreting Bi-Modal\nand Encoder-Decoder Transformers. arXiv:2103.15679.\nChen, Z.; Xie, L.; Niu, J.; Liu, X.; Wei, L.; and Tian,\nQ. 2021. Visformer: The Vision-friendly Transformer.\narXiv:2104.12533.\nChild, R.; Gray, S.; Radford, A.; and Sutskever, I.\n2019. Generating long sequences with sparse transformers.\narXiv:1904.10509.\nChoe, J.; and Shim, H. 2019. Attention-based dropout layer\nfor weakly supervised object localization. In CVPR.\nCordonnier, J.-B.; Loukas, A.; and Jaggi, M. 2020. On the\nrelationship between self-attention and convolutional layers.\nIn ICLR.\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and Wei,\nY . 2017. Deformable convolutional networks. InICCV.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2021. An image is worth 16x16\nwords: Transformers for image recognition at scale. ICLR.\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial networks. In NeurIPS.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang, Y .\n2021. Transformer in transformer. In NeurIPS.\nHassani, A.; Walton, S.; Shah, N.; Abuduweili, A.; Li, J.;\nand Shi, H. 2021. Escaping the Big Data Paradigm with\nCompact Transformers. arXiv:2104.05704.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR.\nHudson, D. A.; and Zitnick, C. L. 2021. Generative Adver-\nsarial Transformers. arXiv:2103.01209.\nJiang, Y .; Chang, S.; and Wang, Z. 2021. Transgan: Two\ntransformers can make one strong gan. In NeruIPS.\nKim, B.; Lee, J.; Kang, J.; Kim, E.-S.; and Kim, H. J.\n2021. HOTR: End-to-End Human-Object Interaction De-\ntection with Transformers. CVPR.\nKrizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple\nlayers of features from tiny images. Technical report, Uni-\nversity of Toronto.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. LocalViT: Bringing Locality to Vision Transformers.\narXiv:2104.05707.\nLiang, J.; Homayounfar, N.; Ma, W.-C.; Xiong, Y .; Hu, R.;\nand Urtasun, R. 2020. Polytransform: Deep polygon trans-\nformer for instance segmentation. In CVPR.\nLiu, Y .; and Lapata, M. 2019. Hierarchical transformers for\nmulti-document summarization. In ACL.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV.\nNeimark, D.; Bar, O.; Zohar, M.; and Asselmann, D.\n2021. Video transformer network. arXiv preprint\narXiv:2102.00719.\nPappagari, R.; Zelasko, P.; Villalba, J.; Carmiel, Y .; and De-\nhak, N. 2019. Hierarchical transformers for long document\nclassiﬁcation. In Automatic Speech Recognition and Under-\nstanding Workshop (ASRU).\nParmar, N.; Vaswani, A.; Uszkoreit, J.; Kaiser, L.; Shazeer,\nN.; Ku, A.; and Tran, D. 2018. Image transformer. InICML.\nRamachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Lev-\nskaya, A.; and Shlens, J. 2019. Stand-alone self-attention in\nvision models. In NeurIPS.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. ImageNet large scale visual recognition chal-\nlenge. IJCV.\nSantra, B.; Anusha, P.; and Goyal, P. 2021. Hierarchical\ntransformer for task oriented dialog systems. In NAACL.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual expla-\nnations from deep networks via gradient-based localization.\nIn ICCV.\nShi, W.; Caballero, J.; Husz ´ar, F.; Totz, J.; Aitken, A. P.;\nBishop, R.; Rueckert, D.; and Wang, Z. 2016. Real-time\nsingle image and video super-resolution using an efﬁcient\nsub-pixel convolutional neural network. In CVPR.\nSohn, K.; Berthelot, D.; Li, C.-L.; Zhang, Z.; Carlini, N.;\nCubuk, E. D.; Kurakin, A.; Zhang, H.; and Raffel, C. 2020.\nFixmatch: Simplifying semi-supervised learning with con-\nsistency and conﬁdence. In NeurIPS.\nSrinivas, A.; Lin, T.-Y .; Parmar, N.; Shlens, J.; Abbeel, P.;\nand Vaswani, A. 2021. Bottleneck transformers for visual\nrecognition. arXiv:2101.11605.\n3424\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic\nattribution for deep networks. In ICML.\nTan, M.; and Le, Q. 2019. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In ICML.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021a. Training data-efﬁcient image trans-\nformers & distillation through attention. In ICML.\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and\nJ´egou, H. 2021b. Going deeper with image transformers.\narXiv:2103.17239.\nVaswani, A.; Ramachandran, P.; Srinivas, A.; Parmar, N.;\nHechtman, B.; and Shlens, J. 2021. Scaling Local Self-\nAttention For Parameter Efﬁcient Visual Backbones. In\nCVPR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. NeurIPS.\nWang, H.; Zhu, Y .; Adam, H.; Yuille, A.; and Chen, L.-C.\n2021a. MaX-DeepLab: End-to-End Panoptic Segmentation\nwith Mask Transformers. CVPR.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021b. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. arXiv:2103.15808.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.; Tay,\nF. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Train-\ning vision transformers from scratch on imagenet. In ICCV.\nZhang, H.; Goodfellow, I.; Metaxas, D.; and Odena, A.\n2019. Self-attention generative adversarial networks. In\nICML.\nZhang, P.; Dai, X.; Yang, J.; Xiao, B.; Yuan, L.; Zhang, L.;\nand Gao, J. 2021. Multi-Scale Vision Longformer: A New\nVision Transformer for High-Resolution Image Encoding.\nIn ICCV.\nZhang, X.; Wei, F.; and Zhou, M. 2019. HIBERT: Document\nlevel pre-training of hierarchical bidirectional transformers\nfor document summarization. arXiv:1905.06566.\nZhang, X.; Wei, Y .; Feng, J.; Yang, Y .; and Huang, T. S.\n2018. Adversarial complementary learning for weakly su-\npervised object localization. In CVPR.\nZhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba,\nA. 2016. Learning deep features for discriminative localiza-\ntion. In CVPR.\n3425"
}