{
  "title": "Evaluating language models of tonal harmony",
  "url": "https://openalex.org/W2885802111",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287942598",
      "name": "Sears, David R. W.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287193189",
      "name": "Korzeniowski, Filip",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281357259",
      "name": "Widmer, Gerhard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2795825978",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W309855107",
    "https://openalex.org/W2006657628",
    "https://openalex.org/W2076254785",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2182279449",
    "https://openalex.org/W2016503548",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2113641473",
    "https://openalex.org/W2143129936",
    "https://openalex.org/W2161628678",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1576426705",
    "https://openalex.org/W2111066885",
    "https://openalex.org/W2479188559",
    "https://openalex.org/W2296557957",
    "https://openalex.org/W2751829044",
    "https://openalex.org/W2160015256",
    "https://openalex.org/W1897056990",
    "https://openalex.org/W2154686727",
    "https://openalex.org/W2756556761"
  ],
  "abstract": "This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most state-of-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types.",
  "full_text": "EV ALUATING LANGUAGE MODELS OF TONAL HARMONY\nDavid R. W. Sears1 Filip Korzeniowski2 Gerhard Widmer2\n1 College of Visual & Performing Arts, Texas Tech University, Lubbock, USA\n2 Institute of Computational Perception, Johannes Kepler University, Linz, Austria\ndavid.sears@ttu.edu\nABSTRACT\nThis study borrows and extends probabilistic language\nmodels from natural language processing to discover the\nsyntactic properties of tonal harmony. Language models\ncome in many shapes and sizes, but their central purpose is\nalways the same: to predict the next event in a sequence\nof letters, words, notes, or chords. However, few stud-\nies employing such models have evaluated the most state-\nof-the-art architectures using a large-scale corpus of West-\nern tonal music, instead preferring to use relatively small\ndatasets containing chord annotations from contemporary\ngenres like jazz, pop, and rock.\nUsing symbolic representations of prominent instru-\nmental genres from the common-practice period, this study\napplies a ﬂexible, data-driven encoding scheme to (1)\nevaluate Finite Context (or n-gram) models and Recur-\nrent Neural Networks (RNNs) in a chord prediction task;\n(2) compare predictive accuracy from the best-performing\nmodels for chord onsets from each of the selected datasets;\nand (3) explain differences between the two model archi-\ntectures in a regression analysis. We ﬁnd that Finite Con-\ntext models using the Prediction by Partial Match (PPM)\nalgorithm outperform RNNs, particularly for the piano\ndatasets, with the regression model suggesting that RNNs\nstruggle with particularly rare chord types.\n1. INTRODUCTION\nFor over two centuries, scholars have observed that tonal\nharmony, like language, is characterized by the logical\nordering of successive events, what has commonly been\ncalled harmonic syntax. In Western music of the common-\npractice period (1700-1900), pitch events group (or co-\nhere) into discrete, primarily tertian sonorities, and the\nsuccession of these sonorities over time produces mean-\ningful syntactic progressions. To characterize the passage\nfrom the ﬁrst two measures of Bach’s “Aus meines Herzens\nGrunde”, for example, theorists and composers developed\na chord typology that speciﬁes both the scale steps on\nwhich tertian sonorities are built ( Stufentheorie), and the\nc⃝Sears, Korzeniowski, Widmer. Licensed under a Cre-\native Commons Attribution 4.0 International License (CC BY 4.0). At-\ntribution: Sears, Korzeniowski, Widmer. “Evaluating language models\nof tonal harmony”, 19th International Society for Music Information Re-\ntrieval Conference, Paris, France, 2018.\nI IV 6 V6 I V vi G:–\nFigure 1. Bach, “Aus meines Herzens Grunde”, mm. 1–2;\nfrom the Riemenschneider edition, No. 1. Key and Roman\nnumeral annotations appear below.\nfunctional (i.e., temporal) relations that bind them ( Funk-\ntionstheorie). Shown beneath the staff in Figure 1, thisRo-\nman numeral system allows the analyst to recognize and\ndescribe these relations using a simple lexicon of symbols.\nIn the presence of such language-like design features,\nmusic scholars have increasingly turned to string-based\nmethods from the natural language processing (NLP) com-\nmunity for the purposes of pattern discovery [6], classiﬁ-\ncation [7], similarity estimation [18], and prediction [19].\nIn sequential prediction tasks, for example, probabilistic\nlanguage models have been developed to predict the next\nevent in a sequence — whether it consists of letters, words,\nDNA sequences, or in our case, chords.\nAlthough corpus studies of tonal harmony have become\nincreasingly commonplace in the music research commu-\nnity, applications of language models for chord prediction\nremain somewhat rare. This is likely because language\nmodels take as their starting point a sequence of chords,\nbut the musical surface is often a dense web of chordal and\nnonchordal tones, making automatic harmonic analysis a\ntremendous challenge. Indeed, such is the scope of the\ncomputational problem that a number of researchers have\ninstead elected to start with a particular chord typology\nright from the outset (e.g., Roman numerals, ﬁgured bass\nnomenclature, or pop chord symbols), and then identify\nchord events using either human annotators [3], or rule-\nbased computational classiﬁers [25]. As a consequence,\nlanguage models for tonal harmony frequently train on rel-\natively small, heavily curated datasets (<200,000 chords)\n[3], or use data augmentation methods to increase the size\nof the corpus [15]. And since the majority of these corpora\nreﬂect pop, rock, or jazz idioms, vocabulary reduction is\na frequent preliminary step to ensure improved model per-\nformance, with the researcher typically including speciﬁc\nchord types (e.g., major, minor, seventh, etc.), thus ignor-\narXiv:1806.08724v1  [cs.SD]  22 Jun 2018\ning properties of tonal harmony relating to inversion [15]\nor chordal extension [11].\nGiven the state of the annotation bottleneck, we propose\na complementary method for the implementation and eval-\nuation of language models for chord prediction. Rather\nthan assume a particular chord typology a priori and train\nour models on the chord classes found therein, we will in-\nstead propose a data-driven method for the construction of\nharmonic corpora using chord onsets derived from the mu-\nsical surface. It is our hope that such a bottom-up approach\nto chord prediction could provide a springboard for the im-\nplementation of chord class models in future studies [2],\nthe central purpose of which is to use predictive methods\nto reduce the musical surface to a sequence of syntactic\nprogressions by discovering a small vocabulary of chord\ntypes.\nWe begin in Section 2 by describing the datasets used\nin the present research and then present the tonal encod-\ning scheme that reduces the combinatoric explosion of po-\ntential chord types to a vocabulary consisting of roughly\ntwo hundred types for each scale-degree in the lowest in-\nstrumental part. Next, Section 3 describes the two most\nstate-of-the-art architectures employed in the NLP com-\nmunity: Finite Context (or n-gram) models and Recurrent\nNeural Networks (RNNs). Section 4 presents the experi-\nments, which (1) evaluate the two aforementioned model\narchitectures in a chord prediction task; (2) compare pre-\ndictive accuracy from the best-performing models for each\ndataset; (3) attempt to explain the differences between the\ntwo models using a regression analysis. We conclude in\nSection 5 by considering limitations of the present ap-\nproach, and offering avenues for future research.\n2. CORPUS\nThis section presents the datasets used in the present re-\nsearch and then describes the chord representation scheme\nthat permits model comparison across datasets.\n2.1 Datasets\nShown in Table 1, this study includes nine datasets of\nWestern tonal music (1710–1910) featuring symbolic rep-\nresentations of the notated score (e.g., metric position,\nrhythmic duration, pitch, etc.). The Chopin dataset con-\nsists of 155 works for piano that were encoded in Mu-\nsicXML format [10]. The Assorted symphonies dataset\nconsists of symphonic movements by Beethoven, Berlioz,\nBruckner, and Mahler that were encoded in MATCH for-\nmat [26]. All other datasets were downloaded from the\nKernScores database in MIDI format. 1 In total, the\ncomposite corpus includes the complete catalogues for\nBeethoven’s string quartets and piano sonatas, Joplin’s\nrags, and Chopin’s piano works, and consists of over 1,000\ncompositions containing more than 1 million chord tokens.\n1 http://kern.ccarh.org/.\nComposer Genre N pieces Ntokens Ntypes\nBach Chorale 370 35,237 786\nHaydn Quartet 210 159,579 1472\nMozart Quartet 82 78,201 1289\nBeethoven Quartet 70* 132,896 1699\nMozart Piano 51 92,279 833\nBeethoven Piano 102* 176,370 1332\nChopin Piano 155* 147,827 1790\nJoplin Piano 47* 43,848 854\nAssorted Symphony 29 147,549 2420\nTotal 1116 1,013,786 2590\nNote. * denotes the complete catalogue.\nTable 1. Datasets and descriptive statistics for the corpus.\n2.2 Chord Representation Scheme\nTo derive chord progressions from symbolic corpora using\ndata-driven methods, music analysis software frameworks\ntypically perform a full expansion of the symbolic en-\ncoding, which duplicates overlapping note events at every\nunique onset time. Shown in Figure 2, expansion identiﬁes\n9 unique onset times in the ﬁrst two measures of Bach’s\nchorale harmonization, “Aus meines Herzens Grunde.”\nPrevious studies have represented each chord accord-\ning to the simultaneous relations between its note-event\nmembers (e.g., vertical intervals) [23], the sequential re-\nlations between its chord-event neighbors (e.g., melodic\nintervals) [6], or some combination of the two [22]. For\nthe purposes of this study, we have adopted a chord typol-\nogy that models every possible combination of note events\nin the corpus. The encoding scheme consists of an ordered\ntuple (S,I) for each chord onset in the sequence, where S\nis a set of up to three intervals above the bass in semitones\nmodulo the octave (i.e., 12), resulting in133 (or 2197) pos-\nsible combinations; 2 and I is the chromatic scale degree\n(again modulo the octave) of the bass, where 0 represents\nthe tonic, 7 the dominant, and so on.\nBecause this encoding scheme makes no distinction be-\ntween chord tones and non-chord tones, the syntactic do-\nmain of chord types is still very large. To reduce the\ndomain to a more reasonable number, we have excluded\npitch class repetitions in S (i.e., voice doublings), and we\nhave allowed permutations. Following [22], the assump-\ntion here is that the precise location and repeated appear-\nance of a given interval are inconsequential to the identity\nof the chord. By allowing permutations, the major triads\n⟨4,7,0⟩and ⟨7,4,0⟩therefore reduce to ⟨4,7,⊥⟩. Simi-\nlarly, by eliminating repetitions, the chords ⟨4,4,10⟩and\n⟨4,10,10⟩reduce to ⟨4,10,⊥⟩. This procedure restricts\nthe domain to 233 unique chord types in S(i.e., when I is\nundeﬁned).\nTo determine the underlying tonal context of each chord\nonset, we employ the key-ﬁnding algorithm in [1], which\ntends to outperform other distributional methods (with an\n2 The value of each vertical interval is either undeﬁned (denoted by\n⊥), or represents one of twelve possible interval classes, where 0 denotes\na perfect unison or octave, 7 denotes a perfect ﬁfth, and so on.\n<0,4,7,⊥>\n <11,3,8,⊥> < 9,3,7,⊥>\nFigure 2. Full expansion of Bach, “Aus meines Herzens\nGrunde”, mm. 1–2. Three chord onsets are shown with the\ntonal encoding scheme described in Section 2.2 for illus-\ntrative purposes.\naccuracy of around 90% for both major and minor keys).\nSince the movements in this dataset typically feature mod-\nulations, we compute the Pearson correlation between the\ndistributional weights in the selected key-ﬁnding algorithm\nand the pitch-class distribution identiﬁed in a moving win-\ndow of 16 quarter-note beats and centered around each\nchord onset in the sequence. The algorithm interprets the\npassage in Figure 2 in G major, for example, so the bass\nnote of the ﬁrst harmony is 0 (i.e., the tonic).\n3. LANGUAGE MODELS\nThe goal of language models is to estimate the probabil-\nity of event ei given a preceding sequence of events e1\nto ei−1, notated here as ei−1\n1 . In principle, these models\npredict ei by acquiring knowledge through unsupervised\nstatistical learning of a training corpus, with the model\narchitecture determining how this learning process takes\nplace. For this study we examine the two most common\nand best-performing language models in the NLP commu-\nnity: (1) Markovian ﬁnite-context (or n-gram) models us-\ning the PPM algorithm, and (2) recurrent neural networks\n(RNNs) using both long short-term memory (LSTM) lay-\ners and gated recurrent units (GRUs).\n3.1 Finite Context Models\nContext models estimate the probability of each event in a\nsequence by stipulating a global order bound (or determin-\nistic context) such that p(ei) depends only on the previous\nn−1 events, or p(ei|ei−1\n(i−n)+1). For this reason, context\nmodels are also sometimes called n-gram models, since\nthe sequence ei\n(i−n)+1 is an n-gram consisting of a context\nei−1\n(i−n)+1, and a single-event prediction ei. These models\nﬁrst acquire the frequency counts for a collection of se-\nquences from a training set, and then apply these counts to\nestimate the probability distribution governing the identity\nof ei in a test sequence using maximum likelihood (ML)\nestimation.\nUnfortunately, the number of potential n-grams de-\ncreases dramatically as the value of nincreases, so high-\norder models often suffer from the zero-frequency prob-\nlem, in which n-grams encountered in the test set do not\nappear in the training set [27]. The most common solution\nto this problem has been the Prediction by Partial Match\n(PPM) algorithm, which adjusts the ML estimate for ei by\ncombining (or smoothing) predictions generated at higher\norders with less sparsely estimated predictions from lower\norders [5]. Speciﬁcally, PPM assigns some portion of the\nprobability mass to accommodate predictions that do not\nappear in the training set using an escape method . The\nbest-performing smoothing method is called mixtures (or\ninterpolated smoothing), which computes a weighted com-\nbination of higher order and lower order models for every\nevent in the sequence.\n3.1.1 Model Selection\nTo implement this model architecture, we apply the\nvariable-order Markov model (called IDyOM) developed\nin [19]. 3 The model accommodates many possible con-\nﬁgurations based on the selected global order bound, es-\ncape method, and training type. Rather than select a global\norder bound, researchers typically prefer an extension to\nPPM called PPM*, which uses simple heuristics to de-\ntermine the optimal high-order context length for ei, and\nwhich has been shown to outperform the traditional PPM\nscheme in several prediction tasks (e.g., [21]), so we ap-\nply that extension here. Regarding the escape method, re-\ncent studies have demonstrated the potential of method C\nto minimize model uncertainty in melodic and harmonic\nprediction tasks [12, 21], so we also employ that method\nhere.\nTo improve model performance, Finite Context mod-\nels often separately estimate and then combine two sub-\nordinate models trained on differed subsets of the corpus:\na long-term model (LTM+), which is trained on the en-\ntire corpus; and a short-term (or cache) model (STM),\nwhich is initially empty for each individual composition\nand then is trained incrementally (e.g., [8]). As a result,\nthe LTM+ reﬂects inter-opus statistics from a large corpus\nof compositions, whereas the STM only reﬂects intra-opus\nstatistics, some of which may be speciﬁc to that composi-\ntion. Finally, the model implemented here also includes a\nmodel that combines the LTM+ and STM models using a\nweighted geometric mean (BOTH+) [20]. Thus, we report\nthe LTM+, STM, and BOTH+ models for the analyses that\nfollow.4\n3.2 Recurrent Neural Networks\nRecurrent Neural Networks (RNNs) are powerful models\ndesigned for sequential modelling tasks. RNNs transform\nan input sequence xN\n1 to an output sequence oN\n1 through\na non-linear projection into a hidden layer hN\n1 , parame-\nterised by weight matrices Whx, Whh and Woh:\nhi = σh (Whxxi + Whhhi−1) (1)\noi = σo (Wohhi) , (2)\nwhere σh and σo are the activation functions for the hid-\nden layer (e.g. the sigmoid function), and the output layer\n3 The model is available for download: http://code.\nsoundsoftware.ac.uk/projects/idyom-project\n4 The models featuring the + symbol represent both the statistics from\nthe training set and the statistics from that portion of the test set that has\nalready been predicted.\n...\n...\nFigure 3. The basic architecture for an RNN-based\nlanguage model. This model can easily accommodate\nmore recurrent hidden layers or include additional skip-\nconnections between the input and each hidden layer or\nthe output. The ﬁrst input, e0, is a dummy symbol without\nan associated chord.\n(e.g. the softmax), respectively. We excluded bias terms\nfor simplicity.\nRNNs have become popular models for natural lan-\nguage processing due to their superior performance com-\npared to Finite Context models [17]. Here, the input at each\ntime step iis a (learnable) vector representation of the pre-\nceding symbol, v(ei−1). The network’s outputoi ∈RNtypes\nis interpreted as the conditional probability over the next\nsymbol, p\n(\nei |ei−1\n1\n)\n. As outlined in Figure 3, this proba-\nbility depends on all preceding symbols through the recur-\nrent connection in the hidden layer.\nDuring training, the categorical cross-entropy between\nthe output oi and the true chord symbol is minimised by\nadapting the weight matrices in Eqs. 1 and 2 using stochas-\ntic gradient descent and back-propagation through time.\nHowever, this training procedure suffers from vanishing\nand exploding gradients because of the recursive dot prod-\nuct in Eq. 1. The latter problem can be averted by clipping\nthe gradient values; the former, however, is trickier to pre-\nvent, and necessitates more complex recurrent structures\nsuch as the long short-term memory unit (LSTM) [13] or\nthe gated recurrent unit (GRU) [4]. These units have be-\ncome standard features of RNN-based language modeling\narchitectures [16].\n3.2.1 Model Selection\nSelecting good hyper-parameters is crucial for neural net-\nworks to perform well. To this end, we performed a num-\nber of preliminary experiments to tune the networks. Our\nﬁnal architecture comprises two layers of 128 recurrent\nunits each (either LSTM or GRU), a learnable input em-\nbedding of 64 dimensions (i.e. v(·) maps each chord class\nto a vector inR64), and skip connections between the input\nand all other layers.\nRNNs are prone to over-ﬁt the training data. We use\nthe network’s performance on held-out data to identify this\nissue. Since we employ 4-fold cross-validation (see Sec. 4\nfor details), we hold out one of the three training folds as\na validation set. If the results on these data do not improve\nfor 10 epochs, we stop training and select the model with\nthe lowest cross-entropy on the validation data.\nWe trained the networks for a maximum of 200 epochs,\nusing stochastic gradient descent with a mini-batch size of\n4. Each of these 4 data points is a sequence of at most 300\nchords. The gradient updates are scaled using the Adam\nupdate rule [14] with standard parameters. To prevent ex-\nploding gradients, we clip gradient values larger than 1.\n4. EXPERIMENTS\n4.1 Evaluation\nTo evaluate performance using a more reﬁned method than\none simply based on the accuracy of the model’s predic-\ntion, we use a statistic calledcorpus cross-entropy, denoted\nby Hm.\nHm(pm,ej\n1) =−1\nj\nj∑\ni=1\nlog2 pm(ei|ei−1\n1 ). (3)\nHm represents the average information content for the\nmodel probabilities estimated by pm over all e in the se-\nquence ej\n1. That is, cross-entropy provides an estimate of\nhow uncertain a model is, on average, when predicting a\ngiven sequence of events [21], regardless of whether the\ncorrect symbol for each event was assigned the highest\nprobability in the distribution.\nFinally, we employ 4-fold cross-validation stratiﬁed by\ndataset for both model architectures, using cross-entropy\nas a measure of performance.\n4.2 Results\nWe ﬁrst compare the average cross-entropy estimates\nacross the entire corpus using Finite Context models and\nRNNs, and then examine the estimates across datasets for\nthe best performing model conﬁguration from each archi-\ntecture. We conclude by examining the differences be-\ntween these models in a regression analysis.\n4.2.1 Comparing Models\nTable 2 presents the average cross-entropy estimates for\neach model conﬁguration. For the purposes of statisti-\ncal inference, we also include the 95% bootstrap conﬁ-\ndence interval using the bias-corrected and accelerated per-\ncentile method [9]. For the Finite Context models, BOTH+\nModel Type Hm CIa\nFinite Context\nLTM+ 4.895 4.811–4.978\nSTM 6.710 6.600–6.820\nBOTH+ 4.893 4.800–4.966\nRecurrent Neural Network\nLSTM 5.583 5.539–5.626\nGRU 5.600 5.551–5.645\na CI refers to the 95% bootstrap conﬁdence interval of Hm using the\nbias-corrected and accelerated percentile method with 1000 replicates.\nTable 2. Model comparison using cross-entropy as an eval-\nuation metric.\nChopin\nPiano\nBach\nChorale\nMozart\nPiano\nBeethoven\nPiano\nJoplin\nPiano\nHaydn\nQuartet\nMozart\nQuartet\nBeethoven\nQuartet\nAssorted\nSymphony\nBOTH+\nLSTM\n0\n1\n2\n3\n4\n5\n6\n7\n8\nHm (bits)\nFigure 4. Bar plots of the best-performing model conﬁgurations from the Finite Context (BOTH+) and RNN (LSTM)\nmodels. Whiskers represent the 95% bootstrap conﬁdence interval of the mean using the bias-corrected and accelerated\npercentile method with 1000 replicates.\nproduced the lowest cross-entropy estimates on average,\nthough the difference between BOTH+ and LTM+ was\nnegligible. STM was the worst performing model over-\nall, which is unsurprising given the restrictions placed on\nthe model’s training parameters (i.e., that it only trains on\nthe already-predicted portion of the test set).\nOf the RNN models, LSTM slightly outperformed\nGRU, but again this difference was negligible. What is\nmore, the long-term Finite Context models (BOTH+ and\nLTM+) signiﬁcantly outperformed both RNNs. This ﬁnd-\ning could suggest that context models are better suited to\nmusic corpora, since the datasets for melodic and harmonic\nprediction are generally miniscule relative to those in the\nNLP community [15]. The encoding scheme for this study\nalso produced a large vocabulary (2590 symbols), so the\nPPM* algorithm might be useful when the model is forced\nto predict particularly rare types in the corpus.\n4.2.2 Comparing Datasets\nTo identify the differences between these models for each\nof the datasets in the corpus, Figure 4 presents the bar\nplots for the best-performing model conﬁgurations from\neach model architecture: BOTH+ from the Finite Context\nmodel, and LSTM from the RNN model. On average,\nBOTH+ produced the lowest cross-entropy estimates for\nthe piano datasets (Mozart, Beethoven, Joplin), but much\nhigher estimates for the other datasets. This effect was not\nobserved for LSTM, however, with the datasets’ genre —\nchorale, piano work, quartet, and symphony — apparently\nplaying no role in the model’s overall performance.\nThe difference between these two model architectures\nfor the Joplin and Mozart piano datasets is particularly\nstriking. Given the degree to which piano works gener-\nally consist of fewer homorhythmic textures relative to the\nother genres in this corpus, it could be the case that the\npiano datasets feature a larger proportion of rare, mono-\nphonic chord types relative to the other datasets. The next\nsection examines this hypothesis using a regression model.\n4.2.3 A Regression Model\nGiven the complexity of the corpus, a number of factors\nmight explain the performance of these models. Thus,\nwe have included the following ﬁve predictors in a mul-\ntiple linear regression (MLR) model to explain the average\ncross-entropy estimates for the compositions in the corpus\n(N = 1136): 5\nNtokens Cache (i.e., STM) and RNN-based language mod-\nels often beneﬁt from datasets that feature longer se-\nquences by exploiting statistical regularities in the\nportion of the test sequence that was already pre-\ndicted. Thus, Ntokens represents the number of to-\nkens in each sequence. Compositions featuring more\ntokens should receive lower cross-entropy estimates\non average.\nNtypes Language models struggle with data sparsity as n\nincreases (i.e., the zero-frequency problem). One\nsolution is to select corpora for which the vocab-\nulary of possible distinct types is relatively small.\nThus, Ntypes represents the number of types in each\nsequence. Compositions with larger vocabularies\nshould receive higher cross-entropy estimates on av-\nerage.\nImprobable Events that occur with low probability in the\nzeroth-order distribution are particularly difﬁcult to\npredict due to the data sparsity problem just men-\ntioned. Thus, Improbable represents the proportion\nof tokens in each sequence that appear in the bottom\n10% of types in the zeroth-order probability distribu-\ntion. Compositions with a large proportion of these\nparticularly rare types should receive higher cross-\nentropy estimates on average.\nMonophonic Chorales feature homorhythmic textures in\nwhich each temporal onset includes multiple coin-\ncident pitch events. The chord types representing\nthese tokens should be particularly common in this\ncorpus, but some genres might also feature poly-\nphonic textures in which the number of coincident\nevents is potentially quite low (e.g., piano). Thus,\n5 Four of the 1116 compositions were further subdivided in the se-\nlected datasets, producing an additional 20 sequences in the analyses:\nBeethoven, Quartet No. 6, Op. 18, iv (2); Chopin, Op. 12 (2); Mozart,\nPiano Sonata No. 6, K. 284, iii (13); Mozart, Piano Sonata No. 11, K.\n331, i (7).\nMonophonic represents the proportion of tokens in\neach sequence that consist of only one pitch event.\nCompositions with a large proportion of these mono-\nphonic events should receive higher cross-entropy\nestimates on average.\nRepetition Compared to chord-class corpora, data-driven\ncorpora are far more likely to feature adjacent rep-\netitions of tokens. Thus, Repetition represents the\nproportion of tokens in each sequence that feature\nadjacent repetitions. Compositions with a large pro-\nportion of repetitions should receive lower cross-\nentropy estimates on average.\nTable 3 presents the results of a stepwise regression\nanalysis predicting the average cross-entropy estimates\nwith the aforementioned predictors. R2 refers to the ﬁt of\nthe model, where a value of 1 indicates that the model ac-\ncounts for all of the variance in the outcome variable (i.e., a\nperfectly linear relationship between the predictors and the\ncross-entropy estimates). The slope of the line measured\nfor each predictor, denoted by β, represents the change in\nthe outcome resulting from a unit change in the predictor.\nFor the Finite Context model (BOTH+), four of the\nﬁve predictors explained 53% of the variance in the cross-\nentropy estimates. As predicted, cross-entropy decreased\nas the number of tokens increased, suggesting that the\nmodel learned from past tokens in the sequence. What is\nmore, cross-entropy increased as the vocabulary increased,\nas well as when the proportion of monophonic or improb-\nable tokens increased, though the latter two predictors had\nlittle effect on the model.\nFor the RNN model, the effect of these predictors was\nstrikingly different. In this case, cross-entropy increased\nwith the proportion of improbable events. Note that this\npredictor played only a minor role for the Finite Context\nmodel, which suggests PPM* may be responsible for the\nmodel’s superior performance. For the remaining predic-\ntors, cross-entropy estimates decreased when the propor-\ntion of adjacent repeated tokens increased. Like the Finite\nContext model, the RNN model also struggled when the\nproportion of monophonic tokens increased, but beneﬁted\nfrom longer sequences featuring smaller vocabularies.\n5. CONCLUSION\nThis study examined the potential for language models to\npredict chords in a large-scale corpus of tonal compositions\nfrom the common-practice period. To that end, we devel-\noped a ﬂexible chord representation scheme that (1) made\nminimal a priori assumptions about the chord typology un-\nderlying tonal music, and (2) allowed us to create a much\nlarger corpus relative to those based on chord annotations.\nOur ﬁndings demonstrate that Finite Context models out-\nperform RNNs, particularly in piano datasets, which sug-\ngests PPM* is responsible for the superior performance,\nsince it assigns a portion of the probability mass to poten-\ntially rare, as-yet-unseen types. A regression analysis gen-\nerally conﬁrmed this hypothesis, with LSTM struggling to\npredict the improbable types from the piano datasets.\nModel Predictors β R 2\nBOTH+\nNtokens −2.079 .212\nNtypes 1.860 .506\nMonophonic 0.233 .506\nImprobable 0.076 .530\nLSTM\nImprobable 0.463 .318\nRepetition −0.558 .375\nNtypes 0.817 .504\nMonophonic 0.452 .568\nNtokens −0.554 .591\nNote. Each predictor appears in the order speciﬁed by stepwise selection,\nwith R2 estimated at each step. However, β presents the standardized\nbetas estimated in the model’s ﬁnal step.\nTable 3. Stepwise regression analysis predicting the av-\nerage Hm estimated for each composition from the best-\nperforming model conﬁgurations with characteristic fea-\ntures of the corpus.\nTo our knowledge, this is the ﬁrst language-modeling\nstudy to use such a large vocabulary of chord types, though\nthis approach is far more common in the NLP community,\nwhere the selected corpus can sometimes contain millions\nof distinct word types. Our goal in doing so was to bridge\nthe gulf between the most current data-driven methods for\nmelodic and harmonic prediction on the one hand [24], and\napplications of chord typologies for the creation of cor-\npora using expert analysts on the other [3]. Indeed, despite\nrecent efforts to determine the efﬁcacy of language mod-\nels for annotated corpora [11, 15], relatively little has been\ndone to develop unsupervised methods for the discovery of\ntonal harmony in predictive contexts.\nOne serious limitation of the architectures examined\nin this study is their unwavering commitment to the sur-\nface. Rather than skipping seemingly inconsequential on-\nsets, such as those containing embellishing tones or repeti-\ntions, these models predict every onset in their path. As a\nresult, the model conﬁgurations examined here attempted\nto predict tonal (pitch) content rather than tonal harmonic\nprogressions per se. In our view, word class models could\nprovide the necessary bridge between the bottom-up and\ntop-down approaches just described by reducing the vo-\ncabulary of surface simultaneities to its most essential har-\nmonies [2]. Along with prediction tasks, these models\ncould then be adapted for sequence generation and auto-\nmatic harmonic analysis, and in so doing, provide converg-\ning evidence that the statistical regularities characterizing\na tonal corpus also reﬂect theorder in which its constituent\nharmonies occur.\n6. ACKNOWLEDGMENTS\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Hori-\nzon 2020 research and innovation programme (grant agree-\nment n◦ 670035).\n7. REFERENCES\n[1] J. Albrecht and D. Shanahan. The use of large corpora\nto train a new type of key-ﬁnding algorithm: An im-\nproved treatment of the minor mode.Music Perception,\n31(1):59–67, 2013.\n[2] P. F. Brown, V . J. Della Pietra, P. V . deSouza, J. C. Lai,\nand R. L. Mercer. Class-based n-gram models of nat-\nural language. Computational Linguistics, 18(4):467–\n479, 1992.\n[3] J. A. Burgoyne, J. Wild, and I. Fujinaga. An Expert\nGround Truth Set for Audio Chord Recognition and\nMusic Analysis. In Proceedings of the 12th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR), Miami, USA, 2011.\n[4] K. Cho, B. van Merrienboer, D. Bahdanau, and Y . Ben-\ngio. On the Properties of Neural Machine Translation:\nEncoder-Decoder Approaches. arXiv:1409.1259 [cs,\nstat], September 2014.\n[5] J. G. Cleary and I H. Witten. Data compression\nusing adaptive coding and partial string matching.\nIEEE Transactions on Communications , 32(4):396–\n402, 1984.\n[6] D. Conklin. Representation and discovery of vertical\npatterns in music. In C. Anagnostopoulou, M. Fer-\nrand, and A. Smaill, editors, Music and Artiﬁcal Intel-\nligence: Lecture Notes in Artiﬁcial Intelligence 2445 ,\nvolume 2445, pages 32–42. Springer-Verlag, 2002.\n[7] D. Conklin. Multiple viewpoint systems for music clas-\nsiﬁcation. Journal of New Music Research , 42(1):19–\n26, 2013.\n[8] D. Conklin and I. H. Witten. Multiple viewpoint sys-\ntems for music prediction. Journal of New Music Re-\nsearch, 24(1):51–73, 1995.\n[9] T. J. DiCiccio and B. Efron. Bootstrap conﬁdence in-\ntervals. Statistical Science, 11(3):189–228, 1996.\n[10] S. Flossmann, W. Goebl, M. Grachten, B. Nie-\ndermayer, and G. Widmer. The Magaloff project:\nAn interim report. Journal of New Music Research ,\n39(4):363–377, 2010.\n[11] B. Di Giorgi, S. Dixon, M. Zanoni, and A. Sarti. A\ndata-driven model of tonal chord sequence complexity.\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing, 25(11):2237–2250, 2017.\n[12] T. Hedges and G. A. Wiggins. The prediction of\nmerged attributes with multiple viewpoint systems.\nJournal of New Music Research, 2016.\n[13] S. Hochreiter and J. Schmidhuber. Long Short-Term\nMemory. Neural Computing, 9(8):1735–1780, Novem-\nber 1997.\n[14] D. Kingma and J. Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[15] F. Korzeniowski, D. R. W. Sears, and G. Widmer. A\nlarge-scale study of language models for chord predic-\ntion. In Proceedings of the International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\nCalgary, Canada, 2018.\n[16] G. Melis, C. Dyer, and P. Blunsom. On the state of the\nart of evaluation in neural language models. In Sixth\nInternational Conference on Learning Representations\n(ICLR), Vancouver, Canada, April 2018.\n[17] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock ´y, and\nS. Khudanpur. Recurrent neural network based lan-\nguage model. In INTERSPEECH 2010, 11th Annual\nConference of the International Speech Communica-\ntion Association, Makuhari, Chiba, Japan, September\n26-30, 2010, pages 1045–1048, Chiba, Japan, 2010.\n[18] D. M ¨ullensiefen and M. Pendzich. Court decisions on\nmusic plagiarism and the predictive value of similar-\nity algorithms. Musicæ Scientiæ , Discussion Forum\n4B:257–295, 2009.\n[19] M. T. Pearce. The Construction and Evaluation of Sta-\ntistical Models of Melodic Structure in Music Per-\nception and Composition. Phd thesis, City University,\nLondon, 2005.\n[20] M. T. Pearce, D. Conklin, and G. A. Wiggins. Methods\nfor Combining Statistical Models of Music, pages 295–\n312. Springer Verlag, Heidelberg, Germany, 2005.\n[21] M. T. Pearce and G. A. Wiggins. Improved methods for\nstatistical modelling of monophonic music. Journal of\nNew Music Research, 33(4):367–385, 2004.\n[22] I. Quinn. Are pitch-class proﬁles really “key for key”?\nZeitschrift der Gesellschaft der Musiktheorie , 7:151–\n163, 2010.\n[23] D. R. W. Sears. The Classical Cadence as a Closing\nSchema: Learning, Memory, and Perception. Phd the-\nsis, McGill University, Montreal, Canada, 2016.\n[24] D. R. W. Sears, M. T. Pearce, W. E. Caplin, and\nS. McAdams. Simulating melodic and harmonic ex-\npectations for tonal cadences using probabilistic mod-\nels. Journal of New Music Research , 47(1):29–52,\n2018.\n[25] D. Temperley and D. Sleator. Modeling meter and har-\nmony: A preference-rule approach. Computer Music\nJournal, 23(1):10–27, 1999.\n[26] G. Widmer. Using AI and machine learning to study\nexpressive music performance: Project survey and ﬁrst\nreport. AI Communications, 14(3):149–162, 2001.\n[27] I. H. Witten and T. C. Bell. The zero-frequency prob-\nlem: Estimating the probabilities of novel events in\nadaptive text compression. IEEE Transactions on In-\nformation Theory, 37(4):1085–1094, 1991.",
  "topic": "Harmony (color)",
  "concepts": [
    {
      "name": "Harmony (color)",
      "score": 0.6574544310569763
    },
    {
      "name": "Computer science",
      "score": 0.46379974484443665
    },
    {
      "name": "Vowel harmony",
      "score": 0.4554857313632965
    },
    {
      "name": "Linguistics",
      "score": 0.4443761706352234
    },
    {
      "name": "Speech recognition",
      "score": 0.27267831563949585
    },
    {
      "name": "Art",
      "score": 0.18325939774513245
    },
    {
      "name": "Philosophy",
      "score": 0.13631632924079895
    },
    {
      "name": "Visual arts",
      "score": 0.0554845929145813
    },
    {
      "name": "Vowel",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I121883995",
      "name": "Johannes Kepler University of Linz",
      "country": "AT"
    }
  ]
}