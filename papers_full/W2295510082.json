{
    "title": "Extractive Summarisation Based on Keyword Profile and Language Model",
    "url": "https://openalex.org/W2295510082",
    "year": 2015,
    "authors": [
        {
            "id": "https://openalex.org/A5054571273",
            "name": "Xu Han",
            "affiliations": [
                "UNSW Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5027881741",
            "name": "Éric Martin",
            "affiliations": [
                "UNSW Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5060904625",
            "name": "Ashesh Mahidadia",
            "affiliations": [
                "UNSW Sydney"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2144512097",
        "https://openalex.org/W1965555277",
        "https://openalex.org/W2136542423",
        "https://openalex.org/W2153568396",
        "https://openalex.org/W2107972503",
        "https://openalex.org/W3099883129",
        "https://openalex.org/W191409295",
        "https://openalex.org/W2097433297",
        "https://openalex.org/W2102065370",
        "https://openalex.org/W2066636486",
        "https://openalex.org/W3021494766",
        "https://openalex.org/W2168859760",
        "https://openalex.org/W2085116970",
        "https://openalex.org/W2071940869",
        "https://openalex.org/W2116780029",
        "https://openalex.org/W2083305840",
        "https://openalex.org/W2160992478"
    ],
    "abstract": "We present a statistical framework to extract information-rich citation sentences that summarise the main contributions of a scientific paper. In a first stage, we automatically discover salient keywords from a paper’s citation summary, keywords that characterise its main contributions. In asecond stage, exploitingthe results of the first stage, we identify citation sentences that best capture the paper’s main contributions. Experimental results show that our approach using methods rooted in quantitative statistics and information theory outperforms the current state-of-the-art systems in scientific paper summarisation.",
    "full_text": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 123–132,\nDenver, Colorado, May 31 – June 5, 2015.c⃝2015 Association for Computational Linguistics\nExtractive Summarisation Based on Keyword Proﬁle and Language Model\nHan Xu Eric Martin Ashesh Mahidadia\nSchool of Computer Science and Engineering\nUNSW, Sydney, NSW, Australia, 2052\nhanx,emartin,ashesh@cse.unsw.edu.au\nAbstract\nWe present a statistical framework to extract\ninformation-rich citation sentences that sum-\nmarise the main contributions of a scientiﬁc\npaper. In a ﬁrst stage, we automatically dis-\ncover salient keywords from a paper’s citation\nsummary, keywords that characterise its main\ncontributions. In a second stage, exploiting the\nresults of the ﬁrst stage, we identify citation\nsentences that best capture the paper’s main\ncontributions. Experimental results show that\nour approach using methods rooted in quan-\ntitative statistics and information theory out-\nperforms the current state-of-the-art systems\nin scientiﬁc paper summarisation.\n1 Introduction and Motivation\nScience is not an isolated endeavour, but beneﬁts\nfrom and expands on the work of others, with more\nor less cross fertilisation between disciplines. The\ninterdependent nature of research has naturally re-\nsulted in a network of scientiﬁc areas with dense in-\nterconnections between related ﬁelds. Though re-\nsearch is a highly specialised activity, researchers\nﬁnd themselves constantly in need to explore the\nnetwork further from the core of their research.\nTools that can facilitate understanding the key con-\ntributions of papers in those parts of the network be-\ning explored can only prove highly valuable.\nAs an example of such tools, we focus on an\napplication that automatically extracts information-\nrich sentences describing the main contributions of\na given paper. From which corpus the extraction\ncould take place? A natural answer is the abstract of\nthe paper. However, the contributions as perceived\nby the authors can signiﬁcantly deviate from those\njudged extrospectively by the community over time\n(Mei and Zhai, 2008). Instead, we take as corpus\nthe set of citing sentences to the paper (from other\npapers). Indeed, those sentences can arguably be\ndeemed as a form of crowd-sourced review of the\npaper’s main contributions. The set of citing sen-\ntences is referred to as thecitation summary of the\ntarget paper. Elkiss et al. (2008) carried out a large-\nscale study and conﬁrmed that citation summaries\ncontain extra information that does not appear in pa-\nper abstracts. In addition, they found that the “self-\ncohesion”, measured as the average cosine similar-\nity between sentences, is consistently higher in a pa-\nper’s citation summary than in its abstract: the for-\nmer is more focused than the latter in describing pa-\npers’ main contributions. This work presents our ef-\nforts in advancing research along this direction.\nSection 2 formally deﬁnes the problem we aim\nto solve: summarise scientiﬁc papers using the\nmost informative and diversiﬁed part of their cita-\ntion summaries. It surveys several prominent related\nstudies, and introduces the data used in our experi-\nments and evaluations. In Section 3, we present our\nstatistical framework built upon quantitative statis-\ntics and information theory. In Section 4, we eval-\nuate and compare the performance of our method\nwith state-of-the-art systems. We conclude and\npoint to future directions in Section 5.\n2 Problem Statement\nThe problem we tackle in this paper is to generate\nan extractive summary(usually, we will simply say\nsummary) from its citation summary. More speciﬁ-\ncally, we opt for a two stage approach. In the ﬁrst\nstage, we automatically discover salient keywords\nfrom a paper’s citation summary, keywords that are\nessential in characterising the paper’s main contribu-\ntions. The second stage, exploiting the results of the\nﬁrst stage, identiﬁes citation sentences (to the paper)\nthat best capture the paper’s main contributions.\nA word of caution: by utilising only citation sum-\nmaries, one should not expect to obtain well formu-\nlated, readily consumable summaries of papers.In-\ndeed, a citation sentence may be not all about the\ncited paper, but also talk about the citing paper and\nother co-cited papers, which disqualify citation sum-\n123\nmaries as a premium source of sentences for build-\ning highly readable summaries (Siddharthan and\nTeufel, 2007). Moreover, a summary built from cit-\ning sentences that come for a pool of multiple cit-\ning papers is bound to lack coherence. Therefore,\nit is more appropriate to consider that the output of\nsuch a system is to extrinsically gauge a system’s ef-\nfectiveness in indexing information-rich citing sen-\ntences containing keywords that facilitate rapidly\ngrasping a paper’s important contributions, rather\nthan be treated as a polished, readable summary for\nhuman consumption (Qazvinian et al., 2013).\n2.1 Related Work\nQazvinian and Radev (2008) ﬁrst experimented\nwith citation summary based paper summarisations.\nThey proposed a graph-based method, C-LexRank,\nthat ﬁrst generates a citation summary network for\na paper by mapping citing sentences to vertices and\ncreating edges from their lexical similarities. Clus-\nters of sentences capturing the same contribution\nof the paper are then identiﬁed through link-based\ncommunity detection. Finally, the most central sen-\ntence of each cluster is found using a weighted\nrandom walk and selected to form a paper sum-\nmary meant to comprehensively cover the paper’s\nmain contributions. Mohammad et al. (2009) further\nadapted the C-LexRank to multi-document sum-\nmarisation in an attempt to generate surveys for sci-\nentiﬁc paradigms.\nIn a later paper, Qazvinian et al. (2010) proposed\na more computationally efﬁcient summariser that\ndoes not require clustering citing sentences. As a\nﬁrst step, key phrases are automatically identiﬁed\nas signiﬁcant n-grams with positive point-wise di-\nvergence (Tomokiyo, 2003) from a foreground lan-\nguage model estimated using the citation summary\nof a paper w.r.t. a background language model built\nfrom a large set of paper abstracts. A greedy algo-\nrithm is subsequently applied to select citing sen-\ntences and form a summary that maximises key\nphrase coverage.\nMei and Zhai (2008) presented a sophisticated\ngenerative approach that frames summarisation un-\nder an Information Retrieval (IR) context. Speciﬁ-\ncally, an impact language model for a paper is ﬁrst\nbuilt as a mixture of a language model estimated\nfrom the paper’s own text, and a weighted citation\nlanguage model based on its collective citation con-\ntexts, using a compound coefﬁcient reﬂecting both\na sentence’s proximity to the citation label (anchor)\nin the citing paper and the citing paper’s authority\ncalculated from the citation network using PageR-\nank (Brin and Page, 1998). Finally, documents (sen-\ntences in the target paper) that are closest to the\nquery (the impact language model of the target pa-\nper) are extracted to form a summary using ad-hoc\ndocument retrieval. Note that Mei and Zhai (2008)\nutilised extra information (i.e., paper full texts and\ncitation networks) to produce summaries that con-\nsist of sentences from papers’ own texts rather than\ntheir citation summaries, making their task related\nto but different to ours.\n2.2 Data\nThe experiments and evaluations presented here\nhave been based on Qazvinian’s single paper sum-\nmarisation corpus1. The dataset consists of 25\nhighly cited papers in the ACL Anthology Network\n(AAN) (Radev et al., 2009) from 5 different do-\nmains: Dependency Parsing (DP), Phrase Based\nMachine Translation (PBMT), Text Summarisation\n(SUM), Question Answering (QA) and Textual En-\ntailment (TE). There are two ﬁles provided for each\npaper: a citation summary ﬁle containing all citing\nsentences to it, and a manually constructed key fact\nﬁle containing its main contributions hand picked\nby human annotators after reading the citation sum-\nmary. The manual annotation has been performed\nindependently by annotators, and a phrase needed to\nbe marked by at least 2 annotators to be qualiﬁed as\ncapturing a paper’s key fact (Qazvinian and Radev,\n2008). This corpus represents a gold standard in re-\nsearch paper summarisation and it has been widely\nused in system evaluations (Qazvinian and Radev,\n2008; Qazvinian et al., 2010).\n3 Our Approach\nIn this section, we ﬁrst introduce our quantitative\nstatistical method to automatically construct akey-\nword proﬁle of a paper and statistically capture a\npaper’s main contributions in terms of words from\nits citation summary. We then discuss how we con-\nstruct akeyword proﬁle language model. Finally, we\nelaborate on how we cast the task of sentence selec-\ntion from the citation summary as language model\ndivergence based IR in a probabilistic framework.\n3.1 Paper Keyword Proﬁle\nAs indicated in Section 1, the citation summary of a\npaper can be deemed a collective review of its con-\ntributions. Therefore, the main contributions of a\n1http://www-personal.umich.edu/˜vahed/\ndata.html\n124\npaper are salient keywords, those keywords which\nare commonly used by its citers to refer to it and\nare statistically over-represented in the paper’s cita-\ntion summary w.r.t. the overall distribution of such\nwords across other papers’ citation summaries. Put\nanother way, the salience of a word in characteris-\ning a paper’s main contributions is qualiﬁed along\nover-representedness and exclusiveness dimensions.\nClearly, a proper statistical model of words distribu-\ntion is required in order to measure words’ salience\nin a paper’s citation summary. Consider ﬁve papers,\nD1,. . . ,D5 with citation summariesCS1,. . . ,CS5.\nWe aim at identifying salient keywords fromD1’s\ncitation summaryCS1, that map toD1’s main con-\ntributions. To decide whether a wordW is a charac-\nterising keyword ofD1, we ﬁrst collect alln citing\nsentences containing W from CS1,. . . ,CS5; sup-\npose there aren = 20of them. Then for each citing\nsentence S amongst those 20, we perform the binary\ntest: success iffS belongs toCS1. Suppose that there\nare k = 18successes and 2 failures. This represents\na surprising observation: one would expect a word\nof no characterising power to appear in roughly the\nsame number of sentences inCS1,. . . ,CS5, assum-\ning all citation summaries have the same number of\nsentences2. So one would heuristically conclude that\nW is a good candidate keyword forD1, a keyword\nthat is likely to represent a main contribution.\nThe previous process can be abstracted as sam-\npling without replacement from a ﬁnite set whose\nelements can be classiﬁed into mutually exclusive\nbinary categories, which itself follows a Hypergeo-\nmetric distribution. Let N be the total number of\nciting sentences in citation summaries for papers be-\nlonging to collectionC, K be the number of sen-\ntences in paperD’s citation summary,n be the to-\ntal number of citing sentences containing a certain\nword W, and X be the number of citing sentences\ncontaining W in D’s citation summary. The proba-\nbility of observing exactlyk citing sentences inD’s\ncitation summary containingW is:\nH(X=k|N,K,n)= (K\nk )(N\u0000K\nn\u0000k )\n(N\nn) (1)\nWe can then calculate a p-value to the observed\nnumber of x citing sentences inD’s citation sum-\nmary that contain wordW using the Hypergeomet-\nric test, which in turn is used to measure wordW’s\nsalience in characterisingD’s main contributions:\nS(W)def\n= P(X\u0000x)=1\u0000Px\u00001\ni=0 H(X=i|N,K,n)) (2)\n2This assumption is only made to simplify the discussion.\nThe smaller the value ofS(W), the more salientW\nis. Also, words not appearing inD’s citation sum-\nmary have a maximum p-value of 1.0, and common\nwords appearing in many papers’ citation summaries\nare expected to have larger p-values than words that\nare more exclusively used when citing paperD.\nIt is worth pointing out that the above formulation\ncan be equivalently expressed as applying the one-\ntailed Fisher’s exact test to measure strengths of sta-\ntistical associations between words and paper’s ci-\ntation summaries at the sentence level. Our choice\nof this statistical procedure has been informed by\n(Moore, 2004). Prior to this work, Dunning (1993)\nwas pointing out that some commonly used meth-\nods such as the Pearson’s\u00002 test are inappropriate\nfor measuring textual associations due to the fact\nthat the underlying normality assumption is usu-\nally violated in textual data. He was subsequently\nintroducing the log-likelihood ratio test (LLR) and\nshowing that it can yield more reliable results. The\nLLR was then and has since been widely adopted\nin statistical NLP as a measure of strength of as-\nsociation (Moore, 2004). For instance, Lin and\nHovy (2000) successfully applied LLR in mining\n“topic signatures” of pre-classiﬁed document col-\nlections. But to further verify LLR’s validity ap-\nplied to rare events, Moore (2004) performed an em-\npirical study comparing results obtained using LLR\nand Fisher’s exact test on bilingual word association\nand found that albeit being a good approximation to\nFisher’s exact test, LLR can still introduce a sub-\nstantial amount of error and the author went on to\nadvocate the use of Fisher’s exact test where com-\nputationally feasible. Recall that we measured as-\nsociational strengths at the sentence level. This re-\nsulted in marginal frequencies in the order of only\nhundreds for Qazvinian’s small corpus. We there-\nfore followed this empirical advice and used the one-\ntailed Fisher’s exact test (i.e., Hypergeometric test)\nas our measure of textual association to perform key-\nword proﬁling of a scientiﬁc paper.\nTo obtain a set of keywords likely to map to a\npaper’s main contributions, one can simply sort all\nwords according to their statistical signiﬁcance and\npick the top few (e.g., 10 words with the smallest p-\nvalues). A more statistically tenable scheme would\nbe to identify the keywords of a paper as all words\nappearing in its citation summary with p-values be-\nlow some signiﬁcance level. A technicality here is\nthat in the identiﬁcation of keywords, multiple Hy-\npergeometric tests have been performed. For exam-\nple, all unique words that appeared in the collection\n125\nof citation summaries have been individually tested\nfor their salience in a target paper’s citation sum-\nmary in succession. The signiﬁcance level used to\nqualify a word as a keyword thus requires correction\nfor multiple tests to reduce type I errors. However,\nwe shall show that the rigid statistical signiﬁcance is\nnot crucial in our subsequent building of a keyword\nlanguage model for a paper, and so we did not per-\nform multiple tests corrections, but simply used the\nraw p-values in subsequent analysis.\nAnother technicality is special handling of cita-\ntion anchors. Cited authors’ names, almost system-\natically appearing in citing sentences, are bound to\nbe identiﬁed as salient keywords. We thus substi-\ntuted all citation anchors appearing in a paper’s cita-\ntion summary with the pseudo token “targetanchor”\nif they refer to the target paper, and “otheranchor” if\nthey refer to other co-cited papers.\nFurthermore, our keyword proﬁling approach al-\nlows for a ﬂexible control of the level of selective-\nness in its statistical procedure through the choice\nof the benchmarking collection C. For example,\nwe can choose to use a heterogeneous collection of\npapers covering multiple domains. Words that are\nsalient in characterising a domain may then evalu-\nate to a high salience for a paper inC on that do-\nmain (e.g., word “parsing” for domain Dependency\nParsing (DP)). We can also chooseC to be a homo-\ngeneous collection of papers from the same domain.\nOnly words that are salient in characterising a sin-\ngle paper will then be evaluated to a high salience\nfor that paper (e.g., ifC is on DP, “parsing” will\nnot show up as a salient word for any paper inC).\nRecall from Section 2.2 that we use as data papers\nfrom ﬁve domains. We exploited the homogeneity\nof this data and performed keyword proﬁling intra-\ndomain. This effectively made the keyword proﬁl-\ning all the more selective that the keywords identi-\nﬁed for a paper only characterise itsunique contribu-\ntions w.r.t. its domain, using ﬁve highly cited papers.\nWe shall show in the next section that it is this high\nselectiveness in keyword proﬁling that bestows our\napproach its high discriminative power.\nFor paper P05-1013, Table 2 lists the top 10 key-\nwords identiﬁed from its citation summary using\nour method, while Table 1 lists the humanly se-\nlected gold standard key facts (Qazvinian and Radev,\n2008). It can be seen that our method is highly ef-\nfective in identifying the paper’s main contributions\nwhich closely mirror those picked by human experts.\nWe term our word list ranked by p-values thekey-\nword proﬁleof the paper; it statistically and objec-\ntively captures words’ salience (measured along the\ndimensions of over-representedness and exclusive-\nness) in characterising the paper’s main contribu-\ntions using the statistical surprise given by Hyper-\ngeometric tests. While only unigram keywords were\nconsidered here, our method can be easily extended\nto cope with higher order n-gram “key phrases”.\nThis is left for future work.\nFact id Fact Occurence Pyramid\ntier\n1\nnon-projective 15\n19\npseudo-projective 6\nprojectivizing 1\nprojective graphs 1\nprojectivization\ntransformation 1\n4 czech 6 8swedish 5\n2 data-driven 4 6training data 2\n5 maltparser 4 4\n3 nonterminal categories\nin constituency 1 1\nTable 1: Gold standard key facts of P05-1013 (Qazvinian\nand Radev, 2008) ordered by importance. The pyramid\ntier might not be the sum of the occurrences of facts, as\nmultiple facts can appear in the same sentence.\nSalience rank Word P-value\n1 non-projective 1.54e-08\n2 pseudo-projective 5.61e-06\n3 transformation 4.47e-05\n4 transformations 1.26e-04\n5 maltparser 3.48e-04\n6 swedish 7.53e-04\n7 danish 1.56e-03\n8 following 2.64e-03\n9 arcs 2.64e-03\n10 dependencies 4.43e-03\nTable 2: Extracted keywords for P05-1013, ranked by de-\ncreasing Hypergeometric test signiﬁcance.\n3.2 Keyword Proﬁle Language Model\nEach sentence in a paper’s citation summary covers\nkeywords (possibly none) that map to the paper’s\nmain contributions. Intuitively, a good summarisa-\ntion should be short, and consist of citing sentences\nthat maximise keywords coverage w.r.t. an arbitrar-\nily imposed summary length limit (Qazvinian and\nRadev, 2008). A good summariser should thus pick\nciting sentences that contain as many non-redundant\nkeywords as possible. We have shown in the last sec-\n126\ntion that not all keywords are of equal importance,\nso a good summariser should favour sentences cov-\nering the most important ones. Intuitively, the key-\nword proﬁle of a paper containing valuable informa-\ntion on words’ salience in characterising the paper’s\nmain contributions should be utilised to drive such a\ndiscriminative sentence selection process.\nBased on the previous considerations, we use a\npaper’s keyword proﬁle to build a discriminative un-\nigram language model that directly encodes words’\nsalience as pseudo generative probabilities to facili-\ntate the seamless incorporation of such information\ninto a generic probabilistic framework. More specif-\nically, we directly translate words’ salience (in the\nform of p-values) into a discriminative unigram lan-\nguage model of a paper that assigns high probabili-\nties to its characterising keywords. The pseudo gen-\nerative probability of wordW according to a paper\nD’s keyword proﬁle language modelMkp is:\nP(W|Mkp )=\u0000 1\nZ log(S(W)) (3)\nwhere S(W) denotes the salience of word W in\ncharacterising paperD calculated using (2), andZ\nis a normalisation factor. An intuitive interpreta-\ntion of (3) is to deem\u0000log(S(W)) a pseudo word\ncount ofW, where more salient words have higher\npseudo counts; this makesZ the total length of the\npseudo document generated from the paper’s key-\nword proﬁle. We disregard actual word counts to\nmake the keyword proﬁle language model directly\nencode words’ salience. Also, in the previous step,\nkeyword proﬁling had already implicitly taken such\ninformation into account, providing another justiﬁ-\ncation for this design decision. Table 3 shows a\nminiature example to illustrate how a keyword pro-\nﬁle language model is built. In this example,W5\nis automatically eliminated from the resulting lan-\nguage model because it has lowest salience in char-\nacterising the imaginary document. Any word S\nwith salience valueS(W) close to but strictly less\nthan 1.0 would still have a tiny pseudo probability in\nthe resulting keyword proﬁle language model (e.g.,\nW4). Words with low salience are not necessarily\nstop words (e.g., W4 and W5), and neither is the\nreverse true: a content word can possibly be used\nacross the document collection and thus evaluate to\na very low salience (and so have a nul or low pseudo\ngenerative probability in the resulting keyword pro-\nﬁle language model) for the document under con-\nsideration. For example, “parsing” would have a low\nsalience for any paper in a collection on Dependency\nParsing. It can be seen that our method amounts to\na highly adaptive data driven term weighting frame-\nwork. For brevity, from now on, we use KPLM to\nrefer to keyword proﬁle language model.\nWord Salience\nS(W)\nPseudo count\n\u0000log(S(W)) P(W|Mkp)\nW1 0.01 4.61 0.605\nW2 0.10 2.30 0.303\nW3 0.50 0.69 0.091\nW4 0.99 0.01 0.001\nW5 1.00 0.00 0.000\nTable 3: Keyword proﬁle language model built for an\nimaginary document consists of only 5 distinct words.\nAlthough implicitly conveyed in the formulation\nof KPLM above, it should be made clear that the\nKPLM is a pseudo language model that encodes\nwords’ salience in the form of pseudo generative\nprobabilities, which functions as a language model,\nyet should not be interpreted as a true language\nmodel under the traditional deﬁnition. A traditional\nunigram language model is constructed using the\nactual term frequencies in the document, the re-\nsulting model capturing generative probabilities. In\ncontrast, the KPLM of a document is built using\npseudo term frequencies that directly encode words’\nsalience in characterising a document’s contents,\nmeasured using a sophisticated quantitative statisti-\ncal procedure. It can thus be interpreted as a proba-\nbilistic description of the document’s keywords with\nsigniﬁcantly boosted discriminative power. Having\nclariﬁed the nature of KPLM, we treat it as a lan-\nguage model in the rest of the paper.\n3.3 KPLM Based Summarisation\n3.3.1 Sentence Selection\nThe KPLM of a paper is a discriminative gen-\nerative model that incorporates words’ salience in\ncharacterising a paper’s main contributions. It thus\nrepresents an effective language model from which\na model citing sentence covering the paper’s main\ncontributions could be sampled from3. So by mea-\nsuring the statistical surprise between the realistic\nlanguage model estimated from each citing sentence\nwith the KPLM of a paper, we can select the set\nof citing sentences that conform best to the optimal\nmodel given by the the KPLM and build a sum-\nmary that well captures keywords. More speciﬁ-\ncally, we adopt the negative cross entropy retrieval\nmodel (Zhai, 2008), use the KPLM of a paper as the\n3A pseudo citing sentence sampled from KPLM in this man-\nner would simply be a bag of words, not a grammatical sen-\ntence. So here “model” has the favour of keywords coverage.\n127\nsole document model, and measure the cross entropy\nof multiple query models from it (one for each citing\nsentence in that paper’s citation summary). Citing\nsentences whose Maximum Likelihood Estimation\n(MLE) language models are closest to the paper’s\nKPLM are taken as building blocks of the summary.\nFormally, let S be a citing sentence and let\nc(W, S) denote the number of occurrences of word\nW in S. The MLE language modelMmle of S is the\nrelative frequency of wordW in S:\nP(W |M mle )= c(W,S)\n|S| (4)\nSubsequently, the score for a citing sentenceS is\ngiven by its negative cross entropy with theMkp:\nScore(S)=\u0000H(Mmle ||Mkp )\n=P\nW2V P(W|Mmle )l o g (P(W|Mkp )) (5)\nThe larger a citing sentence’s score, the closer it is\nto the cited paper’s KPLM, thus the higher the citing\nsentence would be ranked. To summarise a paper,\none can just pick the topk ranked citing sentences\nwhere k is the imposed summary length limit.\nWe are not the ﬁrst to cast the task of summarisa-\ntion as document retrieval. Mei and Zhai (2008) pi-\noneered in utilising language models and divergence\nbased IR to select sentences to build summaries.\nWhile similar in the fundamental methodology, our\napproach should be distinguished from this work.\nFirst, Mei and Zhai cast the task as ad-hoc retrieval,\nusing the “impact language model” of a paper as sole\nquery, while the paper’s sentences are treated as doc-\numents whose Kullback-Leibler divergence (Kull-\nback and Leibler, 1951) with the query model is\nmeasured in turn. Estimating reliable language mod-\nels for short documents is challenging due to data\nsparseness and thus requires prudent smoothing. We\npurposefully reversed the roles of sentence model\nand document model, using the shorter sentences\nas queries and measuring their cross entropy with\na sole document model (the KPLM)4. This repre-\nsents a more natural formulation resulting in sim-\npler language models that require fewer parame-\nter estimations. Second, while the impact language\nmodel in (Mei and Zhai, 2008) is partially weighted\n4Kullback-Leibler divergence, used in (Mei and Zhai, 2008),\nis unsuitable to our task, as it is not formalised as ad-hoc re-\ntrieval (i.e., single query, multiple documents). Instead we com-\npare multiple query models (MLE’s of citing sentences) to a\nsingle document model (KPLM of the cited paper), making KL-\ndivergence scores not comparable due to query speciﬁc entropy\nterms. See (Zhai, 2008) for a detailed analysis.\nusing citing paper authority and sentence proxim-\nity to the citation anchor in the citing paper, it is\nstill largely based on actual word occurrences. In\ncontrast, KPLM directly models words’ salience in\ncharacterising a paper’s main contributions using its\nkeyword proﬁle, with expectedly more discrimina-\ntive power. Last, Mei and Zhai’s estimation of an\nimpact language model for a paper assumes the reli-\nable estimation of its citing papers’ authority, which\ncannot always be guaranteed, for example when a\npaper receives citations from new papers that them-\nselves have not been cited enough. Furthermore,\nwhile a citation network can be unavailable, the es-\ntimation of KPLM requires only the citation sum-\nmaries of papers, which is arguably more robust.\n3.3.2 Top Sentence Re-ranking\nAs discussed in Section 3.2, a good summary\nshould capture the most salient keywords of a pa-\nper, but also cover as many non-redundant keywords\nas possible. A summary built using our method is\nlikely to contain citing sentences that concentrate on\nand repetitively cover salient keywords of the target\npaper, which may fall short in keywords diversity.\nIndeed, we can see in the top part of Table 4 that\nthe summary of paper P05-1012 repetitively covers\na single keyword, “Minimum Spanning Tree”, while\nit fails to capture other key concepts.\nTo leverage the diversity in keywords captured in\na summary, a simple heuristic is to select the next\nsentence from a pool of top ranked sentences least\nsimilar to the existing summary. From an informa-\ntion theoretic point of view, this amounts to choos-\ning the next sentence that carries the mostextra in-\nformation (i.e., statistical surprise), w.r.t. the current\ncontents of the summary. This formulation intu-\nitively suggests that cross entropy, as a natural mea-\nsure of statistical surprise, could again be employed.\nWe ﬁrst need to abstract a citing sentence and the\ncitation summary into probabilistic distributions be-\nfore their cross entropy can be measured. Again we\nuse unigram language modelling. Since both texts\nare small in size, data sparseness becomes a ma-\njor issue, as nul dimensions in the MLE language\nmodels would make cross entropy not measurable.\nSmoothing as a way to alleviate data sparseness is\nthus required. Another issue that also arises from\nthe texts’ small size is the non-negligible amount of\ncross entropy contributed from non-content words in\nboth texts (English stop words plus the two pseudo\ntokens: “targetanchor” and “otheranchor”). We\ntherefore remove those non-content words prior to\n128\nlanguage model construction to eliminate their noise\nin the cross entropy calculation. Experiments did\nsupport this design decision, and better results have\nbeen achieved with non-content words removed.\nWe perform Dirichlet Prior Smoothing (Zhai and\nLafferty, 2001) to both the citing sentence MLE and\nthe summary MLE using the KPLM of the paper as\na background model using a Dirichlet Prior (DP) of\n20. The choice of 20 has been based on the obser-\nvation that citing sentences are short (32 words on\naverage) and a large DP is prone to generate overly\nsmoothed language models that are dominated by\nthe KPLM, thus lack discriminative power. Here we\nchoose to use this empirically selected DP parameter\nwithout attempting to ﬁne-tune it for best results.\nIn summary, we implement a top sentence re-\nranking heuristic that iteratively selects the next sen-\ntence to be appended to the existing summary whose\nsmoothed language model is with the largest cross\nentropy (so it contains most extra information) with\na smoothed language mode built for the summary\nat its current stage. We shall demonstrate how our\ntop sentence re-ranking method introduces a major\nperformance boost in the next section. For a quick\ninspection of the effectiveness of this method, com-\npare the summaries constructed for paper P05-1012\nwith and without sentence re-reranking in Table 4.\nIt shows that the summary constructed with sen-\ntence re-ranking covers key facts more comprehen-\nsively. The pseudo code for our re-ranking strategy\nis shown in Algorithm 1. It adopts a straightforward\nre-ranking approach that simply uses the topk+5 re-\ntrieved citing sentences in the previous step as the\ncandidate pool; at each iteration, it selects the best\nsentence based on its cross entropy with the sum-\nmary at the current stage. A more sophisticated re-\nranking method is to combine the two cross entropy\nscores in some way (e.g., Maximal Marginal Rele-\nvance (Carbonell and Goldstein, 1998)) so that the\nﬁnal score for a citing sentence reﬂects its value in\ncapturing salient keywords that have not yet been in-\ncluded in the summary. We leave the study of a more\nsophisticated re-ranking scheme for future work.\n4 Experimental Setup\n4.1 Evaluation Method\nFollowing Qazvinian et al. (2008; 2010), we use the\npyramid method (Nenkova and Passonneau, 2004)\nat sentence level to evaluate our system’s perfor-\nmance. The pyramid score is a fact-based eval-\nuation method that has been especially popular in\nevaluating extractive summarisation systems. It has\nAlgorithm 1Top Sentence Re-ranking\n1: function TOPSENTENCE RERANKER\n2: k  summary length limit\n3: top sent  top k plus 5 sents[0]\n4: es  top sent\n5: cp  top k plus 5 sents - topsent\n6: for s incp do\n7: cp lms[s]  DPSmoothed(s)\n8: for i=2 to k do\n9: es lm  DPSmoothed(es)\n10: s  argmaxs2cp(CE(cp lms||es lm))\n11: es  es + s\n12: cp  cp \u0000 s\n13: cp lms  cp lms \u0000 cp lms[s]\nreturn es\nbeen widely adopted because it incorporates both\nfact coverage and fact importance into the scoring\nprocess, which resonates well with the goals of sum-\nmarisation (Qazvinian et al., 2010). More speciﬁ-\ncally, the pyramid method scores a summary using\nthe ratio between the total facts weights of the facts\nit covers and that of an optimal summary. First a fact\nweights pyramid is built using some facts weighting\nmethod and each fact is subsequently put into its per-\nspective pyramid tier. Qazvinian et al. (2008; 2010)\nbuilt a weights pyramid for each paper and assigned\neach humanly discovered fact into a tier according\nto the number of citing sentences the fact occurs in\nthat paper’s citation summary. For example, factfi\nappearing in|fi| citing sentences in the citation sum-\nmary of paperD is assigned to the tierT|fi| in D’s\nfact weights pyramidPD. Let Fi denotes the num-\nber of facts in the summaryES in tierTi of PD. The\ntotal facts weightsES covers is calculated as:\nW(ES)=Pn\ni=1 i· Fi (6)\nwhere n is the highest tier ofPD. Let ESoptimal\nbe the optimal summary forD w.r.t. the summary\nlength limit (ESoptimal can be found using heuristic-\ndriven exhaustive search). The pyramid score for\nES is ﬁnally calculated as:\nScore(ES)=W(ES)/W(ESoptimal ) (7)\nNote again that we used exactly the same corpus\nand evaluation method as in (Qazvinian and Radev,\n2008; Qazvinian et al., 2010), which makes our re-\nsults directly comparable to those described in those\npapers. Furthermore, both papers report on perfor-\nmance of various baseline methods which are also\ndirectly comparable to ours (see next section). We\ncompare our results with the current state-of-the-\nart; readers are encouraged to refer to (Qazvinian\n129\nRank Summary\nKPLM without sentence re-ranking (Pyramid score: 0.23)\n1 3.1 decoding mcdonald et al (2005b) use the chu-liuedmonds (cle) algorithm to solve the maximumspanning treeproblem.\n2 thus far, the formulation follows mcdonald et al (2005b) and corresponds to the maximumspanning tree(mst) problem.\n3 while we have presented signi cant improvements using additional constraints, one may won5even when caching\nfeature extraction during training mcdonald et al (2005a) still takes approximately 10 minutes to train.\n4 we have successfully replicated the state-of-the-art results for dependency parsing (mcdonald et al, 2005a) for both\nczech and english, using bayes point machines.\n5 the search for the best parse can then be formalized as the search for the maximumspanning tree(mst)\n(mcdonald et al, 2005b).\nKPLM with sentence re-ranking (Pyramid score: 0.73)\n1 3.1 decoding mcdonald et al (2005b) use the chu-liuedmonds (cle) algorithm to solve the maximumspanning treeproblem.\n2 to learn these structures we used onlinelarge-margin learning (mcdonald et al, 2005) that empirically provides\nstate-of-the-art performance for czech.\n3 while we have presented signi cant improvements using additional constraints, one may won5even when caching\nfeature extraction during training mcdonald et al (2005a) still takes approximately 10 minutes to train.\n4 mcdonald et al (2005a) introduce a dependency parsing framework which treats the task as searching for the\nprojective treethat maximises the sum of local dependency scores.\n5 we take as our starting point a re-implementation of mcdonald’s state-of-the-art dependency parser (mcdonald et al, 2005a).\nTable 4: Summaries of paper P05-1012 produced using KPLM. Key facts in citing sentences are highlighted and OCR\nand sentence segmentation errors have been retained as they originally appeared in the corpus.\nand Radev, 2008; Qazvinian et al., 2010) for cross-\nreferencing results from a broader set of systems.\n4.2 Results and Discussion\nTable 5 shows the pyramid score evaluation re-\nsults for the 25 papers. To facilitate comparison\nand cross-referencing, the table has been format-\nted as close as possible to Table 7 in (Qazvinian\nand Radev, 2008) with ﬁgures in the Gold and C-\nLexRank columns directly copied over. Note that a\nGold pyramid score less than 1 suggests that there\nare more facts than can be covered usingk sen-\ntences for that paper’s citation summary. It can\nbe seen that KPLM based summarisation achieves\nquite comparable results (especially in terms of the\nmedian score) with C-LexRank, even without top\nsentence re-ranking. When the re-ranking is intro-\nduced, our system outperforms the current state-of-\nthe-art C-LexRank by a measurable margin. Al-\nbeit the perceived differences in the results, a one-\ntailed Wilcoxon signed-rank test indicated that our\nresults are not statistically superior at signiﬁcance\nlevel 0.05 (Z=-1.22, P=0.11). A power analysis re-\nveals that in order to achieve a statistically signif-\nicant result on this small sample of 25 papers, a\nsystem would need to score a medium to large ef-\nfect size (Cohen’s d> 0.53), which is a challenging\ntask considering C-LexRank’s strong baseline per-\nformance. We hope this analysis can inform future\nstudies using Qazvinian’s 25 papers corpus. Never-\ntheless, it should be pointed out that our approach\nis not only substantially simpler than C-LexRank, it\nalso yields more interpretable results.\nWe know of a more recent set of results reported\nin (Qazvinian et al., 2013), which again conﬁrmed\nC-LexRank’s state-of-the-art status with a mean\npyramid score of 0.799 (cf. Table 6 in (Qazvinian\net al., 2013)). However those results are not com-\nparable with ours for the following reasons. First,\nQazvinian et al. (2013) used a slightly different cor-\npus with 30 papers (5 extra papers from the Condi-\ntional Random Field domain). Second, results were\nbased on a summary length limit of 200 words, so\nroughly equivalent to 6.3 sentences per paper, giv-\ning evaluations an extra edge. Both changes boosted\nsystem performance in those evaluations, as evi-\ndenced by comparing Table 7 in (Qazvinian and\nRadev, 2008) and Table 6 in (Qazvinian et al., 2013).\nQazvinian et al. (2010) used the same corpus and\nevaluation method as our work; however the re-\nsults have been presented as box plots (cf. Figure 1\nin (Qazvinian et al., 2010)) from which only the ﬁve-\nnumber summary (i.e., minimum, lower quartile,\nmedian, upper quartile and maximum) of the pyra-\nmid scores can be reconstructed and consequently\nno signiﬁcance test can be performed. Compared\nwith the best performing variants of the system de-\nvised in (Qazvinian et al., 2010) based on unigrams,\nbigrams and trigrams, our system (KPLM+TSR)\nachieves a higher median score (0.86 vs. 0.80), as\nwell as a lower score variation across the 25 papers.\nAn arbitrarily imposed constraint in the eval-\nuations is the summary length limit, which may\nbe changed to suit a speciﬁc application context.\nThe summarisation task becomes increasingly more\nchallenging when summary length limit is further\ntightened as this would require a summariser to pin-\npoint the best sentences from a potentially large cita-\n130\nDomain\nPaper\nGold\nC-LexRank\nKPLM\nKPLM+TSR\nDP\nC96-1058\nP97-1003\nP99-1065\nP05-1013\nP05-1012\n1.00\n1.00\n0.94\n1.00\n0.95\n0.73\n0.40\n0.67\n0.67\n0.62\n0.33\n0.79\n0.62\n0.66\n0.23\n0.56\n0.79\n0.76\n0.66\n0.73\nPBMT\nN03-1017\nW03-0301\nJ04-4002\nN04-1033\nP05-1033\n0.96\n1.00\n1.00\n1.00\n1.00\n0.64\n1.00\n0.48\n0.85\n0.85\n0.60\n0.80\n0.86\n0.57\n0.97\n0.60\n0.80\n0.89\n0.86\n0.97\nSUMM\nA00-1043\nA00-2024\nC00-1072\nW00-0403\nW03-0510\n1.00\n1.00\n1.00\n1.00\n1.00\n0.95\n0.60\n0.93\n0.70\n0.83\n0.50\n0.60\n0.87\n0.81\n1.00\n0.50\n0.60\n0.93\n0.54\n1.00\nQA\nA00-1023\nW00-0603\nP02-1006\nD03-1017\nP03-1001\n1.00\n1.00\n1.00\n1.00\n1.00\n0.86\n0.60\n0.87\n0.85\n0.59\n0.88\n0.44\n0.93\n0.70\n0.94\n1.00\n0.94\n0.93\n0.90\n0.44\nTE\nD04-9907\nH05-1047\nH05-1079\nW05-1203\nP05-1014\n1.00\n1.00\n1.00\n1.00\n1.00\n0.94\n1.00\n0.56\n0.71\n0.78\n0.77\n0.83\n0.78\n1.00\n0.89\n0.91\n0.83\n0.89\n1.00\n1.00\nMean 0.99 0.75 0.73 0.80\nMedian 1.00 0.73 0.79 0.86\nTable 5: Summary pyramid score evaluation results with\nsummary length limitk =5 .\ntion summary. A desirable property of a good sum-\nmariser is thus the ability in maintaining its perfor-\nmance while the task becomes increasingly demand-\ning. To further evaluate KPLM’s performance un-\nder increasingly more stringent summary length lim-\nits, we gathered the pyramid scores with summary\nlength limitk decreasing from 5 to 1 and visualised\nthe results in Figure 1. We can see that KPLM’s\nperformance decays quite gracefully as more strin-\ngent limits are imposed. Even under the harshest\nconstraint with the summary length limit sets to 1,\nour system still managed a mean pyramid score of\nclose to 0.6 across the 25 papers. Indeed, it can\nbe seen that the variance in pyramid scores gradu-\nally spreads wider (the dark band in the ﬁgure marks\nout 95% conﬁdence interval of the mean scores), but\nthis phenomenon is expected as the error margin also\nshrinks along with the summary length limit.\n5 Conclusion and Future Work\nWe designed a statistical framework to summarise\nscientiﬁc papers, using methods rooted in quanti-\ntative statistics and information theory. We ﬁrst\nbuilt a keyword proﬁle for a paper using a quan-\ntitative statistical method that captures its charac-\n● ● ●\n● ●\n0.4\n0.5\n0.6\n0.7\n0.8\n5 4 3 2 1Summary length limit\nPyramid score\nFigure 1: Pyramid scores of KPLM+TSR under different\nsummary length limits.\nterising keywords that are both overly represented\nand relatively exclusively used in the paper’s cita-\ntion summary. We then used the keyword proﬁle\nof a paper to build a discriminative pseudo unigram\nlanguage model that directly incorporates words’\nsalience in characterising a paper’s main contribu-\ntions into pseudo generative probabilities. Based on\nthe fact that a paper’s KPLM represents an effec-\ntive language model from which pseudo citing sen-\ntences with good coverage of important keywords\ncould be sampled, we cast the task of summarisa-\ntion as language model divergence based IR. Finally,\nwe implemented an information-driven sentence re-\nranking algorithm that can effectively leverage di-\nversity in keyword coverage in summaries produced.\nExperimental results show that our approach outper-\nforms the current state-of-the-art systems in scien-\ntiﬁc paper summarisation, which is also with good\nresilience to more stringent summary length limits.\nIn the future, we plan to extend our approach to\nhigher order n-grams and see whether larger infor-\nmation units (phrases) would help boost summarisa-\ntion performance. We also plan to apply our method\nto the problem of multi-document summarisation. In\nparticular, we are very interested to test our system’s\nperformance on automatically generating a technical\nsurvey of a scientiﬁc paradigm, which thanks to the\nauthors of (Mohammad et al., 2009; Qazvinian et al.,\n2013), has been established as a well-deﬁned task\nwith high-quality open data. Finally, while we have\nshown that our approach is effective in summarising\na scientiﬁc paper’s major contributions using its cita-\ntion summary text, further experiments are required\nto test our method’s effectiveness on more generic\nsummarisation tasks and texts genres.\nAcknowledgement\nWe thank Vahed Qazvinian for making the 25 pa-\nper summarisation corpus publicly available; with-\nout it, our formal evaluations would have been im-\npossible. We also thank the anonymous reviewers\nfor their highly constructive comments.\n131\nReferences\nBrin, Sergey and Page, Larry. 1998 The anatomy of a\nlarge-scale hypertextual web search engine. InPro-\nceedings of the 7th International Conference on World\nWide Web, pp. 107–117.\nCarbonell, Jaime and Goldstein, Jade. 1998 The use of\nMMR, diversity-based reranking for reordering docu-\nments and producing summaries. In Proceedings of\nthe SIGIR, pp. 335–336.\nElkiss, Aaron and Shen, Siwei and Fader, Anthony and\nErkan, G¨unes ¸ and States, David and Radev, Dragomir.\n2008 Blind men and elephants: What do citation sum-\nmaries tell us about a research article?.Journal of the\nAmerican Society for Information Science and Tech-\nnology, vol. 59, no. 1, pp. 51–62.\nKullback, S. and Leibler, R. A. 1951 On information\nand sufﬁciency. The Annals of Mathematical Statis-\ntics, vol. 22, no. 1, pp. 79–86.\nMei, Qiaozhu and Zhai, Chengxiang. 2008 Generating\nimpact-based summaries for scientiﬁc literature. In\nProceedings of the ACL, pp. 816–824.\nMohammad, Saif and Dorr, Bonnie and Egan, Melissa\nand Hassan, Ahmed and Muthukrishan, Pradeep and\nQazvinian, Vahed and Radev, Dragomir and Zajic,\nDavid. 2009 Using citations to generate surveys\nof scientiﬁc paradigms. In Proceedings of the HLT-\nNAACL, pp. 584–592.\nNenkova, Ani and Passonneau, Rebecca. 2004 Evaluat-\ning content selection in summarization: The pyramid\nmethod. In Proceedings of the HLT-NAACL, pp. 145–\n152.\nQazvinian, Vahed and Radev, Dragomir. 2008 Scien-\ntiﬁc paper summarization using citation summary net-\nworks. InProceedings of the 22nd International Con-\nference on Computational Linguistics, pp. 689–696.\nQazvinian, Vahed and Radev, Dragomir and¨Ozg¨ur, Arzu-\ncan. 2010 Citation summarization through keyphrase\nextraction. In Proceedings of the 23rd International\nConference on Computational Linguistics, pp. 895–\n903.\nDunning Ted. 1993 Accurate methods for the statistics\nof surprise and coincidence.Computational Linguis-\ntics, vol. 19, no. 1, pp. 61–74.\nLin, Chin-Yew and Hovy, Eduard. 2000 The automated\nacquisition of topic signatures for text summarization.\nIn Proceedings of the 18th conference on Computa-\ntional Linguistics, pp. 495–501.\nMoore. Robert C. 2004 On log-likelihood-ratios and the\nsigniﬁcance of rare events. InProceedings of EMNLP,\npp. 333–340.\nQazvinian, Vahed and Radev, Dragomir and Mohammad,\nSaif and Dorr, Bonnie and Zajic, David and Whidby,\nMichael and Moon, Taesun. 2013 Generating extrac-\ntive summaries of scientiﬁc paradigms.Journal of Ar-\ntiﬁcial Intelligence Research (JAIR), vol. 46, pp.165–\n201.\nRadev, Dragomir and Pradeep, Muthukrishnan and\nQazvinian, Vahed. 2009 The ACL anthology network\ncorpus. In Proceedings of the ACL workshop on Nat-\nural Language Processing and Information Retrieval\nfor Digital Libraries, pp. 54–61.\nSiddharthan, Advaith. and Teufel, Simone. 2007 Whose\nidea was this, and why does it matter? InProceedings\nof the NAACL/HLT, pp. 316–323.\nTomokiyo, Takashi and Hurst, Matthew. 2003 A lan-\nguage model approach to keyphrase extraction. In\nProceedings of the ACL’03 workshop on Multiword ex-\npressions, pp. 33–40.\nZhai, Chengxiang. 2008 Statistical language models\nfor information retrieval a critical review.Foundations\nand Trends in Information Retrieval, vol. 2, no. 3, pp.\n137–213.\nZhai, Chengxiang and Lafferty, John. 2001 A study of\nsmoothing methods for language models applied to ad\nhoc information retrieval. InProceedings of the 24th\nAnnual International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval, pp.\n334–342.\n132"
}