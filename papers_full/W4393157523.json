{
    "title": "Fairness-Aware Structured Pruning in Transformers",
    "url": "https://openalex.org/W4393157523",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5076088991",
            "name": "Abdelrahman Zayed",
            "affiliations": [
                "Polytechnique Montréal"
            ]
        },
        {
            "id": "https://openalex.org/A5062023800",
            "name": "Gonçalo Mordido",
            "affiliations": [
                "Polytechnique Montréal"
            ]
        },
        {
            "id": "https://openalex.org/A5042662187",
            "name": "Samira Shabanian",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5079887012",
            "name": "Ioana Baldini",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5027720969",
            "name": "Sarath Chandar",
            "affiliations": [
                "Canadian Institute for Advanced Research",
                "Polytechnique Montréal"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3102446692",
        "https://openalex.org/W6796886755",
        "https://openalex.org/W3212496002",
        "https://openalex.org/W3172415559",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W4221157363",
        "https://openalex.org/W4287887133",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6891862884",
        "https://openalex.org/W2791170418",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W2805003733",
        "https://openalex.org/W3185212449",
        "https://openalex.org/W3200033622",
        "https://openalex.org/W2950437211",
        "https://openalex.org/W3197577761",
        "https://openalex.org/W6770750215",
        "https://openalex.org/W6810258211",
        "https://openalex.org/W6760605923",
        "https://openalex.org/W6727099177",
        "https://openalex.org/W3019416653",
        "https://openalex.org/W3089430725",
        "https://openalex.org/W6736780897",
        "https://openalex.org/W3022969335",
        "https://openalex.org/W3125249653",
        "https://openalex.org/W6750731723",
        "https://openalex.org/W3136363192",
        "https://openalex.org/W6819433378",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2799054028",
        "https://openalex.org/W4308023630",
        "https://openalex.org/W6776960202",
        "https://openalex.org/W4309801503",
        "https://openalex.org/W3212892036",
        "https://openalex.org/W2796868841",
        "https://openalex.org/W2764043458",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4221167110",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W3034328552",
        "https://openalex.org/W4382318960",
        "https://openalex.org/W3103754749",
        "https://openalex.org/W3093211917",
        "https://openalex.org/W4297813615",
        "https://openalex.org/W4377865107",
        "https://openalex.org/W4288347855",
        "https://openalex.org/W3167354871",
        "https://openalex.org/W4225934689",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2963457723",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W4286977488",
        "https://openalex.org/W2963526187",
        "https://openalex.org/W4385574250",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W4285294416",
        "https://openalex.org/W3035375600",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W3123340107",
        "https://openalex.org/W4297689207"
    ],
    "abstract": "The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance. WARNING: This work uses language that is offensive in nature.",
    "full_text": "Fairness-Aware Structured Pruning in Transformers\nAbdelrahman Zayed1,2, Gonc ¸alo Mordido1,2, Samira Shabanian3, Ioana Baldini4,\nSarath Chandar1,2,5\n1Mila - Quebec AI Institute\n2Polytechnique Montreal\n3Independent Researcher\n4IBM Research\n5Canada CIFAR AI Chair\n{zayedabd,sarath.chandar}@mila.quebec, {s.shabanian,goncalomordido}@gmail.com, {ioana}@us.ibm.com\nAbstract\nThe increasing size of large language models (LLMs) has in-\ntroduced challenges in their training and inference. Remov-\ning model components is perceived as a solution to tackle\nthe large model sizes, however, existing pruning methods\nsolely focus on performance, without considering an essen-\ntial aspect for the responsible use of LLMs: model fairness.\nIt is crucial to address the fairness of LLMs towards di-\nverse groups, such as women, Black people, LGBTQ+, Jew-\nish communities, among others, as they are being deployed\nand available to a wide audience. In this work, first, we inves-\ntigate how attention heads impact fairness and performance\nin pre-trained transformer-based language models. We then\npropose a novel method to prune the attention heads that\nnegatively impact fairness while retaining the heads critical\nfor performance, i.e. language modeling capabilities. Our ap-\nproach is practical in terms of time and resources, as it does\nnot require fine-tuning the final pruned, and fairer, model.\nOur findings demonstrate a reduction in gender bias by 19%,\n19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-\n2, GPT-Neo of two different sizes, GPT-J, and Llama2 mod-\nels, respectively, in comparison to the biased model, with only\na slight decrease in performance. WARNING: This work uses\nlanguage that is offensive in nature.\nIntroduction\nThe extensive adoption of large language models (LLMs) in\ndiverse natural language processing tasks has proven highly\nsuccessful, leading to their integration into various appli-\ncations (Liu et al. 2022; Wang et al. 2018; Li et al. 2020;\nYu, Bohnet, and Poesio 2020). However, this progress has\nalso brought up concerns about the fairness of these models.\nNumerous studies have revealed a troubling trend in which\nLLMs generate biased outputs for different genders, races,\nor sexual orientations (Nadeem, Bethke, and Reddy 2021;\nZayed et al. 2023b,a). These biases can give rise to seri-\nous problems, such as the generation of discriminatory text;\nfor example, when language models are prompted with sen-\ntences about Arabs, they produce continuations with refer-\nences to terrorism (Nadeem, Bethke, and Reddy 2021).\nTo further expand their abilities, there has been a trend\nof increasingly larger models trained on extensive datasets\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(Smith et al. 2022b; Brown et al. 2020; Cohen et al. 2022;\nRae et al. 2021). However, this pursuit of larger models\nhas introduced challenges for training and inference. To ad-\ndress the issue of increasing model size, model pruning\nhas emerged as a potential solution. Nevertheless, current\npruning methods tend to focus on removing model compo-\nnents that have minimal impact on performance, often over-\nlooking fairness implications (Fan, Grave, and Joulin 2020;\nV oita et al. 2019; Behnke and Heafield 2021a; Prasanna,\nRogers, and Rumshisky 2020). Additionally, these methods\nfrequently assume that a pruned model will undergo fine-\ntuning, which is becoming more and more impractical given\nthe substantial increase in size of modern language models.\nAs a result, there is a need for more thoughtful pruning ap-\nproaches that consider not only performance, but also model\nfairness.\nNumerous pruning methods have highlighted that certain\nattention heads are critical for maintaining language model-\ning ability, while others appear superfluous to model perfor-\nmance (V oita et al. 2019; Michel, Levy, and Neubig 2019;\nHe and Choi 2021; Bian et al. 2021). Some studies have\nshown that these important heads play an interpretable role\nin downstream tasks (Wang et al. 2022; V oita et al. 2019;\nHe and Choi 2021). In our work, we explore the possibility\nof extending this concept to fairness by identifying attention\nheads that are responsible for promoting bias. To achieve\nthis, we compute separate scores to quantify the contribution\nof each attention head toward both performance and bias.\nThese scores serve as our guide in selectively removing at-\ntention heads to improve fairness with minimal performance\nloss. Put simply, we propose to prioritize pruning the heads\nthat contribute the most to bias, given that they are not cru-\ncial for language modeling. Our contributions in this paper\ncan be summarized as follows:\n1. We investigate the impact of existing head pruning meth-\nods on bias across different language models, demon-\nstrating that they do not enhance model fairness.\n2. We quantify the effect of removing attention heads on\nbias in language models, and use it as a proxy for their\ncontribution to the model’s overall bias.\n3. We propose a novel structured pruning method that con-\nsiders both fairness and performance. Our method avoids\npruning the heads that are important for language mod-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22484\neling, while prioritizing pruning the heads which nega-\ntively impact fairness.\n4. We conduct a comparison between our method and exist-\ning pruning techniques, revealing its superiority in terms\nof fairness, while matching, and sometimes surpassing,\ntheir performance in terms of language modeling.\n5. Using LLMs of different sizes, we examine how our bias\nreduction method, when applied to gender bias, impacts\nbiases pertaining to religion, race, sexual orientation, and\nnationality. In most cases, we observe a positive correla-\ntion between gender bias and other social biases, result-\ning in their reduction alongside gender bias mitigation.\nRelated Work\nThis section delves into a more detailed discussion of vari-\nous pruning methods and the existing bias assessment met-\nrics employed in language generation models.\nPruning of Large Language Models\nPruning of large language models can be split into two main\ncategories: structured and unstructured pruning (Behnke and\nHeafield 2021b). Structured pruning involves removing spe-\ncific building blocks within the model, such as attention\nheads or layers, which alters the overall model structure. On\nthe other hand, unstructured pruning is more fine-grained,\nentailing the removal of certain model weights (Narang et al.\n2017; Zhu and Gupta 2018), while retaining the original\nstructure of the network. Structured pruning typically leads\nto faster models, while unstructured pruning results in less\nperformance degradation (Behnke and Heafield 2021b). In\nthis study, we focus on structured pruning to explore the im-\npact of attention heads on fairness through targeted removal,\nwhich represents a relatively unexplored research avenue.\nSome of the pioneering works in the application of struc-\ntural pruning were conducted by V oita et al. (2019) and\nMichel, Levy, and Neubig (2019), where the authors ex-\nplored the removal of attention heads from transformer-\nbased models. Their findings revealed the presence of im-\nportant heads in terms of performance. While the removal\nof important heads led to model collapse, less critical heads\nhad minimal impact on performance. Building upon these\nworks, He and Choi (2021) conducted a detailed analysis of\nthe important heads, demonstrating their interpretable roles\nin task-solving.\nMeanwhile, Bian et al. (2021) focused on investigating\nthe non-important heads and concluded that these heads\nwere redundant since their output exhibited a high corre-\nlation with other heads, making them inconsequential for\nfinal predictions. To address this, Zhang et al. (2021) pro-\nposed an approach for transforming non-important heads\ninto important heads by injecting task-specific prior knowl-\nedge, thereby increasing their contribution to the output. In a\nseparate study, Sajjad et al. (2023) examined layer removal\nin BERT (Devlin et al. 2019) with fine-tuning and show-\ncased the importance of preserving lower layers to maintain\nperformance. Furthermore, Fan, Grave, and Joulin (2020)\ninvestigated layer removal without fine-tuning and achieved\nconsiderable performance preservation through the imple-\nmentation of layer dropout during training. The lottery ticket\nhypothesis (Frankle and Carbin 2019), which suggests the\nexistence of subnetworks capable of achieving comparable\nperformance to that of the full network, has paved the way\nfor numerous unstructured pruning techniques. For exam-\nple, Behnke and Heafield (2020) applied this principle to\nlanguage models, while Prasanna, Rogers, and Rumshisky\n(2020) provided evidence that early-stage pruning during\ntraining outperforms post-convergence pruning.\nFairness Assessment in Text Generation Models\nMetrics to assess fairness in text generation models may\nbe classified into two main categories: intrinsic metrics\nand extrinsic metrics. Intrinsic metrics evaluate the model’s\nbias independently of any downstream task. For instance,\nsome works measure bias by analyzing the correlation be-\ntween token representations of different groups and specific\nstereotypical associations (Caliskan, Bryson, and Narayanan\n2017; Guo and Caliskan 2021; May et al. 2019). These met-\nrics operate under the assumption that bias within language\nmodels can solely be detected through the analysis of the\nembedding space. Therefore, they do not rely on a specific\ntask to evaluate the model’s bias. However, it has been sug-\ngested that embedding space does not consistently align with\nthe model’s bias when deployed to solve a given task (Cao\net al. 2022; Delobelle et al. 2022).\nSome intrinsic metrics employ synthetic templates to\nmeasure bias based on the model’s output predictions (Web-\nster et al. 2020; Kurita et al. 2019). For example, if the\nmodel assigns a higher likelihood to the sentence “she is a\nnurse”, compared to “he is a nurse”, it indicates the pres-\nence of gender bias. These templates are constrained in their\ncoverage of stereotypical associations, resulting in divergent\nrankings of bias among different templates when applied\nto the same models (Delobelle et al. 2022). While some\nmetrics have substituted templates with crowd-sourced ex-\namples (Nadeem, Bethke, and Reddy 2021; Nangia et al.\n2020), they have encountered challenges related to gram-\nmatical correctness, logical coherence, and relevance in a\nsignificant number of sentences (Blodgett et al. 2021).\nThe second category of bias assessment metrics com-\nprises extrinsic metrics, which evaluate bias within the con-\ntext of a specific task. For example, metrics such as Wino-\nbias (Zhao et al. 2018), Winogender (Rudinger et al. 2018),\nand BUG (Levy, Lazar, and Stanovsky 2021) focus on mea-\nsuring bias in coreference resolution. In this task, given a\nsentence like “The doctor told the nurse she will perform\nthe surgery in two days”, identifying the word “nurse” as\na referent for “she” indicates the presence of gender bias.\nSome of these metrics have limited number of examples\nprovided within the templates, which poses challenges in\ndrawing definitive conclusions based on the measured bias.\nFor instance, the Winobias (Zhao et al. 2018), Winogender\n(Rudinger et al. 2018), and CrowS-Pairs (Nangia et al. 2020)\nmetrics employ templates with only3k, 120, and 1.5k exam-\nples, respectively.\nAnother extrinsic metric, BOLD (Dhamala et al. 2021),\nemploys 23k prompts sourced from Wikipedia that refer to\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22485\ndiverse groups based on gender, race, professions, religions,\nand political ideologies. BOLD quantifies bias by prompt-\ning the model with sentences related to different groups\nand examining the sentiment and toxicity of its generated\ncontinuations. However, these prompts have different con-\ntexts for each group, making it challenging to ascertain\nwhether the disparity in output is a result of bias or con-\ntextual differences. Taking this into account, Holistic bias\n(Smith et al. 2022a) is a more recent work that provides566k\nprompts covering 13 different social biases without contex-\ntual changes. In this paper, we use holistic bias prompts for\nfairness assessment as they allow for a more reliable mea-\nsurement of bias within the context of text generation.\nSocial Bias Assessment\nTo assess the extent of social bias inherent in text genera-\ntion models, we adopt the procedure in the work conducted\nby Dhamala et al. (2021). Specifically, we first prompt the\nmodel with sentences that pertain to various social groups,\nand then subsequently evaluate the toxicity levels exhibited\nin the generated output. An output is considered toxic if it\nrefers to content that leads individuals to disengage from a\ndiscussion (Dixon et al. 2018). Hence, a large discrepancy in\ntoxicity levels across different groups serves as an indicator\nof the model’s bias.\nInspired by Dixon et al. (2018), our initial step for evalu-\nating a specific form of social bias involves establishing a set\nof subgroups, denoted as S, which are deemed as relevant to\nmeasure such bias. For instance, in the context of sexual ori-\nentation bias, the set of subgroupsS encompasses terms like\ngay, lesbian, bisexual, straight, and others. We then measure\nthe bias exhibited by the model by comparing the toxicity\nassociated with each subgroup to the average toxicity across\nall subgroups, as follows:\nbiasϕ(S) =Ex∼D(\nX\ns∈S\n|Estoxϕ(x(s))−toxϕ(x(s))|), (1)\nwhere toxϕ(x(s)) represents the toxicity in the continu-\nation of a model parameterized by ϕ when prompted with\na sentence x(s) from a pool of D prompts talking about a\nparticular subgroup s in the set S. Estoxϕ(x(s)) denotes the\naverage toxicity of the model’s output across all subgroups.\nLower values indicate less bias. Table 1 shows a simplified\nexample of calculating sexual orientation bias with only two\nsubgroups.\nPrompt Continuation Toxicity\nHi, I’m a trans f\nather I am not going to tell you 0.6\nsomething stupid though\nHi, I’m a gay father. My guess is I’m a fucking 0.8\nfat nerd\nTable 1: Illustration of social bias assessment. The average\ntoxicity is (0.6+0.8)/2 = 0.7, and hence bias is|0.6−0.7|+\n|0.8−0.7| = 0.2following Eq. (1). In this example, we focus\non sexual orientation bias with two subgroups: trans and gay.\nFairness-Aware Structured Pruning\nExisting methods to prune attention heads in transformer\nmodels determine the importance of each head based solely\non model performance (V oita et al. 2019; Michel, Levy, and\nNeubig 2019). In other words, important heads are deemed\nessential to maintain the model’s language modeling capa-\nbility and may therefore not be pruned. In this work, we rec-\nognize the equal significance of evaluating the influence of\nattention heads on fairness, thereby broadening the defini-\ntion of important heads to encompass not only heads crucial\nfor language modeling but also those that have a positive\nimpact on fairness.\nAs a result, we propose quantifiable approximate mea-\nsures for the impact of a given attention head on both the\nmodel’s fairness and performance. Subsequently, these mea-\nsures serve as our guiding principles in identifying and re-\nmoving attention heads that have a negative impact on fair-\nness, provided they are non-essential for language modeling.\nFor a given pre-trained model, our goal is to improve model\nfairness while maintaining as much performance as possible,\nwithout relying on fine-tuning.\nAttention Head Contributions to Fairness and\nPerformance\nWe quantify the contribution of a given attention head to bias\nas the difference between the model’s bias before and after\npruning such head. More specifically, for a model with Nh\nattention heads, the impact of each headh ∈ {1,2, .., Nh} on\na social group represented by set S, zbias(h,S), is estimated\nas:\nzbias(h, S) =biasϕ(S)|do(yh = 1)−biasϕ(S)|do(yh = 0)\n(2)\nwhere biasϕ(S) represents the bias of the text generation\nmodel parameterized by ϕ as described in Eq. (1). Addition-\nally, do(yh = 1) and do(yh = 0), respectively, signify the\npresence and absence of headh. In a similar vein, the impact\nof a head h in the context of language modeling is defined\nas:\nzppl(h) =pplϕ|do(yh = 1)− pplϕ|do(yh = 0) (3)\nwhere pplϕ refers to the perplexity of a model parameterized\nby ϕ on WikiText-2 (Merity et al. 2017). Using the effect\nof removal of a model component as a proxy of its influ-\nence on the model’s output has been employed in previous\nstudies (Rotman, Feder, and Reichart 2021). However, it is\nimportant to note that the effect of removing multiple heads\nis not equivalent to the sum of the effects of each head re-\nmoved individually due to the non-linearity of the model.\nNotwithstanding, our experimental results indicate that such\nsimplification is a practical and effective way of estimating\nthe impact of attention heads.\nAttention Head Pruning\nHaving assessed the influence of each attention head on\nboth fairness and language modeling, we now introduce our\nfairness-aware structured pruning (FASP) method. FASP fo-\ncuses on removing heads that have a negative impact on\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22486\nfairness while ensuring that the model’s language modeling\nability is minimally affected.\nTo determine the number of heads to keep, thereby pre-\nventing performance decline, we introduce a hyperparam-\neter γ representing the ratio of crucial attention heads for\nlanguage modeling. For instance, γ = 0.5 means we keep\nthe top 50% of heads that positively influence performance,\nranked based on Eq. (3) (lower is better). Then, the re-\nmaining heads (i.e. the non-crucial bottom 50% in terms of\nperformance) are ranked based on their bias impact (again,\nlower is better) computed using Eq. (2). For a given ratio of\npruned heads, denoted by α, we prune α × Nh heads from\nthe remaining non-critical heads, based on their bias scores.\nIn the end, this sequence of steps allows us to prioritize the\nremoval of those with the highest bias impact while mitigat-\ning the loss of language modeling ability. An overview of\nour method is presented in Algorithm 1.\nAlgorithm 1: Fairness-aware structured pruning (FASP)\nInput: Pre-trained model with Nh attention heads, set of\nall heads H, ratio γ of important heads for performance ex-\ncluded from the pruning, ratio α of heads to be pruned, set S\nof subgroups targeted by the bias.\nProcedure:\n1. Compute zppl(h) in Eq. (3) ∀ h ∈ H on the validation set\n2. Define the set of critical heads H′ as the top γ × Nh\nheads based on zppl(h)\n3. Compute zbias(S, h)in Eq. (2) ∀ h ∈ H \\ H′ on the\nvalidation set\n4. Prune α × Nh heads in H \\ H′ based on zbias(S, h)\nend\nFigure 1 illustrates how FASP removes attention heads.\nThe heads shown in black are deemed critical for language\nmodeling and, as a result, are excluded from the pruning\nprocess. The remaining heads are depicted in various col-\nors based on their impact on bias, with red indicating those\nthat negatively influence fairness and green representing the\nheads that promote fairness.\nExperimental Details\nThis section presents an overview of our bias assessment\nprompts, baselines, evaluation metrics, and models used in\nour experiments. Our code is publicly available1.\nBias Assessment Prompts\nWe use the prompts from the holistic bias dataset intro-\nduced by Smith et al. (2022a). This dataset comprises 566k\nprompts, encompassing 13 distinct biases, making it the\nmost extensive bias assessment dataset available at the time\nof this paper’s writing, to the best of our knowledge. Among\nthe 13 biases covered in the dataset, we focus on 5 spe-\ncific biases: race ethnicity, religion, sexual orientation, gen-\nder and sex, and nationality bias. Table 6 in the technical\n1https://github.com/chandar-lab/FASP\nHead \nLayer \n   2       4             6           8         10              12\n   2       4    6\nEffect on bias\nFigure 1: Illustration of applying FASP to a model with 6\nlayers and 12 heads per layer, e.g. DistilGPT-2. Initially, we\nidentify and exclude the heads that significantly impact per-\nformance from the pruning process (black squares). Sub-\nsequently, the remaining heads are prioritized for removal\nbased on their contribution to bias, ensuring that the heads\ncontributing the most to bias are pruned first (red squares).\nappendix displays the number of prompts associated with\neach of these targeted biases, along with some illustrative ex-\namples of the prompts for each category. The prompts were\nsplit into validation and test sets with a ratio of 0.2:0.8.\nBaselines\nWe employ the following baseline methods when evaluating\nour approach: (1) head pruning based on weight magnitude\n(Han et al. 2015; Han, Mao, and Dally 2015), (2) head prun-\ning based on gradient magnitude (Michel, Levy, and Neu-\nbig 2019), (3) random head pruning, (4) head pruning based\nonly on the fairness score in Eq. (2), and (5) head pruning\nbased only on the perplexity score in Eq. (3). We refer to the\nlatter two baselines as fairness only and performance only\nbaselines, respectively. We would like to highlight that the\nmodel remains unchanged and does not undergo any fine-\ntuning after the pruning process for all the mentioned base-\nlines as well as our method.\nEvaluation Metrics\nWe assess bias by examining the variation in the model’s\ntoxicity across various subgroups. For instance, when mea-\nsuring religion bias, we consider differences in the model’s\ntoxicity among the different subgroups such as Muslims,\nChristians, Jews, and so on, as detailed in Eq. (1). We\nuse BERT for toxicity assessment, similar to the work by\nDhamala et al. (2021). For performance assessment, we\nmeasure the model’s perplexity on WikiText-2.\nModels\nWe employed 6 pre-trained models available in Hugging\nFace: DistilGPT-2, GPT-2 (Radford et al. 2019), GPT-Neo\n(Black et al. 2021) of two different sizes, GPT-J (Wang and\nKomatsuzaki 2021), and Llama2 (Touvron et al. 2023) mod-\nels with 88.2M, 137M, 125M, 1.3B, 6B, and 7B parameters,\nrespectively.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22487\n0 20 40\n PPL(%)\n40\n30\n20\n10\n0\n10\n20\n Bias(%)\nDistilGPT-2\n0 25 50 75 100\n PPL(%)\n40\n30\n20\n10\n0\n10\n20\n30\n Bias(%)\nGPT-2\n0 25 50 75 100\n PPL(%)\n50\n40\n30\n20\n10\n0\n10\n Bias(%)\nGPT-Neo 125M\n0 25 50 75 100\n PPL(%)\n60\n40\n20\n0\n20\n40\n Bias(%)\nGPT-Neo 1.3B\n0 25 50 75 100\n PPL(%)\n30\n20\n10\n0\n10\n20\n Bias(%)\nGPT-J 6B\n0 50 100 150 200\n PPL(%)\n20\n15\n10\n5\n0\n5\n10\n15\n20\n Bias(%)\nLlama 2 7B\nFigure 2: The percentage of change in gender bias and language modeling perplexity across DistilGPT-2, GPT-2, GPT-Neo\n125M, GPT-Neo1.3B, GPT-J, and Llama2 models, for varying pruning levels via different techniques, relative to the unpruned\nmodel. Among the methods, FASP is the only method to consistently reduce bias while upholding a relatively low perplexity.\nExperiments\nWe demonstrate that FASP distinguishes itself from conven-\ntional head pruning techniques by taking into account both\nperformance and fairness. Furthermore, we explore whether\nthe heads with the most significant impact on bias are con-\nsistent across various social biases. Finally, we study the im-\npact of gender bias reduction on other social biases.\nFASP introduces a single hyperparameter, which is the\nratio of crucial heads for performance, denoted as γ and\nselected based on the validation set. To identify the opti-\nmal value γ∗, we aim to minimize the model’s bias while\nmaintaining the perplexity as close as possible compared to\nthe best pruning baseline. The search range for γ was set to\nγ ∈ {0.2, ...,0.7}. Additional details about the hyperparam-\neters are provided in the appendix. The code appendix elab-\norates on dataset preprocessing, experiment procedures and\nanalysis, and the computing infrastructure employed. All re-\nsults were obtained using 3 different seeds.\nExperiment 1: How Does FASP Perform in Terms\nof Bias and Language Modeling Compared to\nExisting Pruning Methods?\nIn this experiment, we conduct a comparison between our\npruning technique, FASP, and common baseline pruning\nmethods. Such comparison is carried out with respect to both\ngender bias and language modeling capabilities. The results\ndepicted in Figure 2 clearly indicate that FASP stands out\nas the sole pruning method capable of consistently reduc-\ning gender bias without perplexity overshooting. The fair-\nness only and performance only baselines represent the ex-\ntreme cases where we prune the heads based only on bias\nand performance, respectively. Among the evaluated meth-\nods, the performance only baseline achieves the lowest per-\nplexity value in most of the cases, but does not lead to a\nconsistent improvement in fairness, as expected. Following\nthis, in order of performance, are FASP with the best γ (i.e.\nγ∗), magnitude pruning, and gradient pruning. Magnitude\npruning results in perplexity overshooting on GPT-Neo and\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22488\n136\n133\n1\n80\n93\n105\n23\n17\n117\n12\n8\n48\n125\n88\n56\n61\n65\n2\n72\n78\n139\n90\n122\n134\n100\n135\n112\n115\n50\n132\n138\n142\n28\n3\n4\n35\n33\n41\n42\n24\n118\n113\n25\n120\n27\n111\n110\n109\n119\n52\n121\n103\n19\n126\n18\n13\n9\n7\n5\n137\n21\n99\n29\n53\n54\n55\n46\n57\n58\n60\n45\n63\n44\n77\n37\n81\n83\n86\n32\n91\n92\n31\n95\n96\n98\n67\nHead id\n0\n1\n2\n3\n4\n5# impacted biases\nGPT-2\nRace\nSexual orientation\nNationality\nGender\nReligion\nFigure 3: The indices of most impactful attention heads on five social biases, at a20% pruning rate. Having heads whose pruning\naffects multiple social biases indicates the potential for a simultaneous positive impact on several biases through pruning.\nGen.RaceRel.S. o.Nat.\nDistilGPT-2\nGPT-2\nGPT-Neo 125M\nFigure 4: Pearson correlation heat maps depict the relation-\nships among attention head scores on nationality, sexual ori-\nentation, religion, race, and gender biases, within DistilGPT-\n2, GPT-2, and GPT-Neo with a parameter count of 125M.\nNotably, all social biases exhibit positive correlations, ex-\ncept religion bias, where correlations are either absent or\nslightly negative, varying based on the specific model.\nLlama 2 models. As anticipated, random pruning exhibits\nthe poorest efficacy in preserving perplexity levels, often\nleading to model collapse. Fairness only baseline yields su-\nperior fairness outcomes across the majority of scenarios, al-\nbeit accompanied by elevated perplexity, often surpassesing\nacceptable levels. For all methods, overshooting perplexity\nor bias values beyond the depicted limits are not shown. It\nis important to note that in five out of the six models we\nexamined, we identified a γ∗ value of 0.3, suggesting that\nroughly 30% of the heads in these models play a crucial role\nin language modeling. Qualitative results are provided in the\ntechnical appendix.\nExperiment 2: Are the Heads Responsible for Bias\nthe Same Across Social Biases?\nThis experiment focuses on examining whether the attention\nheads that exert the most significant influence on bias are\nconsistent across a range of distinct social biases. We start\nby calculating the Pearson correlation between the effects of\nattention heads, as outlined in Eq. (2), across varying biases.\nFigure 4 illustrates a consistent positive correlation among\nattention head effects across diverse biases, with the excep-\ntion of the religion bias. For this particular bias, the corre-\nlation is either slightly negative or non-existent in relation\nto other biases, depending on the model under considera-\ntion. Note that we restrict the scope of this experiment to\nDistilGPT-2, GPT-2, and GPT-Neo125M parameter config-\nurations due to resource availability.\nTo take a deeper look at how different heads influence dif-\nferent biases, Figure 3 showcases the indices of the top20%\nattention heads that yield the most substantial impact on five\nbiases using GPT-2. The depiction underscores the presence\nof specific attention heads that manifest as influential across\nmultiple biases, suggesting that the removal of such heads\ncould yield simultaneous benefits for multiple biases. More\nspecifically, attention head number 136 stands as the sole\ncontributor that adversely affects all social biases, whereas\nattention head number 133 uniquely influences four out of\nthe five biases under examination. Numerous other atten-\ntion heads have a concurrent impact on two or three biases.\nThis consistent pattern emerges across alternative models,\nas outlined in the technical appendix. Encouragingly, these\nfindings pave the way for our subsequent experiment, which\ndelves into the broader implications of pruning the attention\nheads that contribute to gender bias on other social biases.\nExperiment 3: How Are Other Social Biases\nAffected When Gender Bias Is Reduced?\nAs our final experiment, we delve into the effect on other\nsocial biases when employing the FASP technique to prune\nattention heads based on gender bias. Figure 5 shows that the\nprocess of pruning attention heads with the most pronounced\ninfluence on gender bias leads to a reduction in sexual ori-\nentation, race, and nationality biases. This is to be expected\nsince all of these biases are positively correlated with gender\nbias, as shown in Figure 4. Since GPT-2 and GPT-Neo ex-\nhibit a positive correlation between religion and gender bias\nhead scores (also shown in Figure 4), pruning heads based\non gender bias scores continues to diminish religion bias in\nthese models. In contrast, DistilGPT-2 displayed a negative\ncorrelation between gender and religion bias head scores,\nleading to a marginal increase in religion bias when pruning\nbased on gender bias head scores. Other pruning methods do\nnot lead to better fairness in the majority of cases.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22489\n0 20 40 60\n PPL(%)\n30\n20\n10\n0\n10\n Bias(%)\nDistilGPT-2 nationality bias\n0 20 40 60\n PPL(%)\n30\n20\n10\n0\n10\n Bias(%)\nDistilGPT-2 race bias\n0 20 40 60\n PPL(%)\n60\n40\n20\n0\n20\n40\n60\n80\n Bias(%)\nDistilGPT-2 religion bias\n0 20 40 60\n PPL(%)\n30\n20\n10\n0\n10\n Bias(%)\nDistilGPT-2 sex. orien. bias\n0 50 100\n PPL(%)\n40\n20\n0\n20\n40\n60\n80\n Bias(%)\nGPT-2 nationality bias\n0 50 100\n PPL(%)\n30\n20\n10\n0\n10\n20\n30\n40\n Bias(%)\nGPT-2 race bias\n0 25 50 75 100\n PPL(%)\n50\n0\n50\n100 Bias(%)\nGPT-2 religion bias\n0 25 50 75 100\n PPL(%)\n30\n20\n10\n0\n10\n20\n30\n Bias(%)\nGPT-2 sex. orien. bias\n0 50 100\n PPL(%)\n60\n40\n20\n0\n20\n Bias(%)\nGPT-Neo 125M nationality bias\n0 50 100\n PPL(%)\n60\n40\n20\n0\n20\n Bias(%)\nGPT-Neo 125M race bias\n0 50 100\n PPL(%)\n60\n40\n20\n0\n20\n40\n Bias(%)\nGPT-Neo 125M religion bias\n0 50 100\n PPL(%)\n50\n40\n30\n20\n10\n0\n10\n Bias(%)\nGPT-Neo 125M sex. orien. bias\nFigure 5: An analysis on DistilGPT-2, GPT-2, and GPT-Neo showing the percentage of change in language modeling perplexity\nand nationality, race, religion, and sexual orientation biases, relative to the unpruned model, using varying pruning levels and\ndifferent pruning techniques. While FASP focuses on gender bias mitigation through head pruning, it also addresses other biases\nwhose head scores are positively correlated with gender bias scores, while maintaining robust language model perplexity.\nConclusion\nThis paper examines the impact of pruning attention heads\nin various language models on their fairness towards sev-\neral social biases. We highlight that current pruning tech-\nniques, which prioritize minimizing performance decline,\ndo not take fairness into account. As a result, we propose\nto consider both performance and fairness considerations\nwhen pruning model components. Our experiments show\nthat the proposed approach, FASP, consistently improves the\nfairness of transformer models while matching the language\nmodeling ability of performance-based pruning methods.\nAcknowledgements\nWe are thankful to Afaf Ta¨ık for her insightful suggestions in\nthis project. We are also thankful to the reviewers for their\nconstructive comments. Sarath Chandar is supported by a\nCanada CIFAR AI Chair and an NSERC Discovery Grant.\nGonc ¸alo Mordido is supported by an FRQNT postdoctoral\nscholarship (PBEEE). The project was also supported by\nMicrosoft-Mila collaboration grant. The authors acknowl-\nedge the computational resources provided by the Digital\nResearch Alliance of Canada.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22490\nReferences\nBehnke, M.; and Heafield, K. 2020. Losing Heads in the\nLottery: Pruning Transformer Attention in Neural Machine\nTranslation. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\n2664–2674. Online: Association for Computational Linguis-\ntics.\nBehnke, M.; and Heafield, K. 2021a. Pruning Neural Ma-\nchine Translation for Speed Using Group Lasso. InProceed-\nings of the Sixth Conference on Machine Translation, 1074–\n1086. Online: Association for Computational Linguistics.\nBehnke, M.; and Heafield, K. 2021b. Pruning neural ma-\nchine translation for speed using group lasso. InProceedings\nof the sixth conference on machine translation, 1074–1086.\nBian, Y .; Huang, J.; Cai, X.; Yuan, J.; and Church, K. 2021.\nOn Attention Redundancy: A Comprehensive Study. InPro-\nceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 930–945. Online: Associa-\ntion for Computational Linguistics.\nBlack, S.; Leo, G.; Wang, P.; Leahy, C.; and Biderman,\nS. 2021. GPT-Neo: Large Scale Autoregressive Language\nModeling with Mesh-Tensorflow. If you use this software,\nplease cite it using these metadata.\nBlodgett, S. L.; Lopez, G.; Olteanu, A.; Sim, R.; and Wal-\nlach, H. 2021. Stereotyping Norwegian salmon: An inven-\ntory of pitfalls in fairness benchmark datasets. In Proceed-\nings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long\nPapers), 1004–1015.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nCaliskan, A.; Bryson, J. J.; and Narayanan, A. 2017. Seman-\ntics Derived Automatically from Language Corpora Contain\nHuman-Like Biases. Science, 356(6334): 183–186.\nCao, Y . T.; Pruksachatkun, Y .; Chang, K.-W.; Gupta, R.; Ku-\nmar, V .; Dhamala, J.; and Galstyan, A. 2022. On the Intrinsic\nand Extrinsic Fairness Evaluation Metrics for Contextual-\nized Language Representations. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), 561–570. Dublin, Ireland:\nAssociation for Computational Linguistics.\nCohen, A. D.; Roberts, A.; Molina, A.; Butryna, A.; Jin, A.;\nKulshreshtha, A.; Hutchinson, B.; Zevenbergen, B.; Aguera-\nArcas, B. H.; Chang, C.-c.; et al. 2022. LaMDA: Language\nmodels for dialog applications.\nDelobelle, P.; Tokpo, E.; Calders, T.; and Berendt, B. 2022.\nMeasuring Fairness with Biased Rulers: A Comparative\nStudy on Bias Metrics for Pre-trained Language Models. In\nProceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 1693–1706. Seattle, United\nStates: Association for Computational Linguistics.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL, 4171–4186.\nDhamala, J.; Sun, T.; Kumar, V .; Krishna, S.; Pruksachatkun,\nY .; Chang, K.-W.; and Gupta, R. 2021. Bold: Dataset and\nmetrics for measuring biases in open-ended language gener-\nation. In Proceedings of the 2021 ACM conference on fair-\nness, accountability, and transparency, 862–872.\nDixon, L.; Li, J.; Sorensen, J.; Thain, N.; and Vasserman,\nL. 2018. Measuring and mitigating unintended bias in text\nclassification. In Conference on AI, Ethics, and Society.\nFan, A.; Grave, E.; and Joulin, A. 2020. Reducing Trans-\nformer Depth on Demand with Structured Dropout. In In-\nternational Conference on Learning Representations.\nFrankle, J.; and Carbin, M. 2019. The Lottery Ticket Hy-\npothesis: Finding Sparse, Trainable Neural Networks. In In-\nternational Conference on Learning Representations.\nGuo, W.; and Caliskan, A. 2021. Detecting emergent in-\ntersectional biases: Contextualized word embeddings con-\ntain a distribution of human-like biases. In Proceedings of\nthe 2021 AAAI/ACM Conference on AI, Ethics, and Society,\n122–133.\nHan, S.; Mao, H.; and Dally, W. J. 2015. Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint\narXiv:1510.00149.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning\nboth weights and connections for efficient neural network.\nAdvances in neural information processing systems, 28.\nHe, H.; and Choi, J. D. 2021. The Stem Cell Hypothesis:\nDilemma behind Multi-Task Learning with Transformer En-\ncoders. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, 5555–5577. On-\nline and Punta Cana, Dominican Republic: Association for\nComputational Linguistics.\nKurita, K.; Vyas, N.; Pareek, A.; Black, A. W.; and\nTsvetkov, Y . 2019. Measuring Bias in Contextualized Word\nRepresentations. In Proceedings of the First Workshop on\nGender Bias in Natural Language Processing, 166–172.\nLevy, S.; Lazar, K.; and Stanovsky, G. 2021. Collecting a\nLarge-Scale Gender Bias Dataset for Coreference Resolu-\ntion and Machine Translation. InFindings of the Association\nfor Computational Linguistics: EMNLP 2021, 2470–2480.\nLi, X.; Sun, X.; Meng, Y .; Liang, J.; Wu, F.; and Li, J. 2020.\nDice Loss for Data-imbalanced NLP Tasks. In Proceedings\nof the 58th Annual Meeting of the Association for Computa-\ntional Linguistics, 465–476. Online: Association for Com-\nputational Linguistics.\nLiu, Y .; Liu, P.; Radev, D.; and Neubig, G. 2022. BRIO:\nBringing Order to Abstractive Summarization. In Proceed-\nings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), 2890–2903.\nDublin, Ireland: Association for Computational Linguistics.\nMay, C.; Wang, A.; Bordia, S.; Bowman, S. R.; and\nRudinger, R. 2019. On Measuring Social Biases in Sentence\nEncoders. In Conference of the North American Chapter of\nthe Association for Computational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22491\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.\nPointer Sentinel Mixture Models. In ICLR.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are Sixteen\nHeads Really Better than One? In Wallach, H.; Larochelle,\nH.; Beygelzimer, A.; d'Alch´e-Buc, F.; Fox, E.; and Garnett,\nR., eds., Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc.\nNadeem, M.; Bethke, A.; and Reddy, S. 2021. StereoSet:\nMeasuring stereotypical bias in pretrained language mod-\nels. In Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), 5356–5371.\nNangia, N.; Vania, C.; Bhalerao, R.; and Bowman, S. 2020.\nCrowS-Pairs: A Challenge Dataset for Measuring Social Bi-\nases in Masked Language Models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 1953–1967.\nNarang, S.; Diamos, G.; Sengupta, S.; and Elsen, E. 2017.\nExploring Sparsity in Recurrent Neural Networks. In Inter-\nnational Conference on Learning Representations.\nPrasanna, S.; Rogers, A.; and Rumshisky, A. 2020. When\nBERT Plays the Lottery, All Tickets Are Winning. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), 3208–3229. On-\nline: Association for Computational Linguistics.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners. OpenAI Blog, 1(8): 9.\nRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoff-\nmann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.;\nYoung, S.; et al. 2021. Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint\narXiv:2112.11446.\nRotman, G.; Feder, A.; and Reichart, R. 2021. Model com-\npression for domain adaptation through causal effect esti-\nmation. Transactions of the Association for Computational\nLinguistics, 9: 1355–1373.\nRudinger, R.; Naradowsky, J.; Leonard, B.; and Van Durme,\nB. 2018. Gender Bias in Coreference Resolution. In Pro-\nceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Papers) ,\n8–14.\nSajjad, H.; Dalvi, F.; Durrani, N.; and Nakov, P. 2023. On the\neffect of dropping layers of pre-trained transformer models.\nComputer Speech & Language, 77: 101429.\nSmith, E. M.; Hall, M.; Kambadur, M.; Presani, E.; and\nWilliams, A. 2022a. “I’m sorry to hear that”: Finding\nNew Biases in Language Models with a Holistic Descriptor\nDataset. In Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, 9180–9211.\nSmith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhan-\ndari, S.; Casper, J.; Liu, Z.; Prabhumoye, S.; Zerveas, G.;\nKorthikanti, V .; et al. 2022b. Using deepspeed and mega-\ntron to train megatron-turing nlg 530b, a large-scale genera-\ntive language model. arXiv preprint arXiv:2201.11990.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nV oita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and Titov,\nI. 2019. Analyzing Multi-Head Self-Attention: Specialized\nHeads Do the Heavy Lifting, the Rest Can Be Pruned. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 5797–5808. Florence, Italy:\nAssociation for Computational Linguistics.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. 2018. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\nEMNLP Workshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP.\nWang, B.; and Komatsuzaki, A. 2021. GPT-J-6B: A 6\nBillion Parameter Autoregressive Language Model. https:\n//github.com/kingoflolz/mesh-transformer-jax.\nWang, K. R.; Variengien, A.; Conmy, A.; Shlegeris, B.; and\nSteinhardt, J. 2022. Interpretability in the Wild: a Circuit for\nIndirect Object Identification in GPT-2 small. In NeurIPS\nML Safety Workshop.\nWebster, K.; Wang, X.; Tenney, I.; Beutel, A.; Pitler, E.;\nPavlick, E.; Chen, J.; Chi, E.; and Petrov, S. 2020. Measur-\ning and reducing gendered correlations in pre-trained mod-\nels. arXiv preprint arXiv:2010.06032.\nYu, J.; Bohnet, B.; and Poesio, M. 2020. Named Entity\nRecognition as Dependency Parsing. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, 6470–6476. Online: Association for Computa-\ntional Linguistics.\nZayed, A.; Mordido, G.; Shabanian, S.; and Chandar, S.\n2023a. Should We Attend More or Less? Modulating At-\ntention for Fairness. arXiv preprint arXiv:2305.13088.\nZayed, A.; Parthasarathi, P.; Mordido, G.; Palangi, H.; Sha-\nbanian, S.; and Chandar, S. 2023b. Deep Learning on a\nHealthy Data Diet: Finding Important Examples for Fair-\nness. In AAAI Conference on Artificial Intelligence.\nZhang, T.; Huang, H.; Feng, C.; and Cao, L. 2021. Enliven-\ning Redundant Heads in Multi-head Self-attention for Ma-\nchine Translation. InProceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, 3238–\n3248. Online and Punta Cana, Dominican Republic: Associ-\nation for Computational Linguistics.\nZhao, J.; Wang, T.; Yatskar, M.; Ordonez, V .; and Chang,\nK.-W. 2018. Gender Bias in Coreference Resolution: Eval-\nuation and Debiasing Methods. In Proceedings of the 2018\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), 15–20. New Orleans,\nLouisiana: Association for Computational Linguistics.\nZhu, M. H.; and Gupta, S. 2018. To Prune, or Not to Prune:\nExploring the Efficacy of Pruning for Model Compression.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22492"
}