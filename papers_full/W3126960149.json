{
    "title": "Making Pre-trained Language Models Better Few-shot Learners",
    "url": "https://openalex.org/W3126960149",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2230107535",
            "name": "Gao Tianyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226708652",
            "name": "Fisch, Adam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1500304500",
            "name": "Chen, Danqi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2432717477",
        "https://openalex.org/W2971600926",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2995322030",
        "https://openalex.org/W2995998574",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2114524997",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3106031848",
        "https://openalex.org/W3099403624",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W2970427421",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W2028175314",
        "https://openalex.org/W3100124407",
        "https://openalex.org/W3035542229",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W3113529090",
        "https://openalex.org/W2525127255",
        "https://openalex.org/W3152497014",
        "https://openalex.org/W2963777632",
        "https://openalex.org/W3115772171",
        "https://openalex.org/W2971136144",
        "https://openalex.org/W3034709122",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2970161131",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W2160660844",
        "https://openalex.org/W2163455955",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2014902591",
        "https://openalex.org/W3005700362",
        "https://openalex.org/W2963026768"
    ],
    "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
    "full_text": "Making Pre-trained Language Models Better Few-shot Learners\nTianyu Gao†∗ Adam Fisch‡∗ Danqi Chen†\n†Princeton University ‡Massachusetts Institute of Technology\n{tianyug,danqic}@cs.princeton.edu\nfisch@csail.mit.edu\nAbstract\nThe recent GPT-3 model (Brown et al.,\n2020) achieves remarkable few-shot perfor-\nmance solely by leveraging a natural-language\nprompt and a few task demonstrations as in-\nput context. Inspired by their ﬁndings, we\nstudy few-shot learning in a more practical sce-\nnario, where we use smaller language models\nfor which ﬁne-tuning is computationally efﬁ-\ncient. We present LM-BFF—b etter few-shot\nfine-tuning of l anguage m odels1—a suite of\nsimple and complementary techniques for ﬁne-\ntuning language models on a small number of\nannotated examples. Our approach includes\n(1) prompt-based ﬁne-tuning together with a\nnovel pipeline for automating prompt genera-\ntion; and (2) a reﬁned strategy for dynamically\nand selectively incorporating demonstrations\ninto each context. Finally, we present a sys-\ntematic evaluation for analyzing few-shot per-\nformance on a range of NLP tasks, including\nclassiﬁcation and regression. Our experiments\ndemonstrate that our methods combine to dra-\nmatically outperform standard ﬁne-tuning pro-\ncedures in this low resource setting, achieving\nup to 30% absolute improvement, and 11% on\naverage across all tasks. Our approach makes\nminimal assumptions on task resources and do-\nmain expertise, and hence constitutes a strong\ntask-agnostic method for few-shot learning.2\n1 Introduction\nThe GPT-3 model (Brown et al., 2020) has made\nwaves in the NLP community by demonstrating as-\ntounding few-shot capabilities on myriad language\nunderstanding tasks. Given only a natural lan-\nguage prompt and a few demonstrations of the task,\nGPT-3 is able to make accurate predictions without\nupdating any of the weights of its underlying lan-\n*The ﬁrst two authors contributed equally.\n1Alternatively, language models’ best friends forever.\n2Our implementation is publicly available at https://\ngithub.com/princeton-nlp/LM-BFF.\nguage model. However, while remarkable, GPT-3\nconsists of 175B parameters, which makes it chal-\nlenging to use in most real-wold applications.\nIn this work, we study a more practical scenario\nin which we only assume access to a moderately-\nsized language model such as BERT (Devlin et al.,\n2019) or RoBERTa (Liu et al., 2019), and a small\nnumber of examples (i.e., afew-shot setting), which\nwe can use to ﬁne-tune the weights of the language\nmodel. This setting is appealing as (1) such mod-\nels can be trained on typical research hardware;\n(2) few-shot settings are realistic, as it is generally\nboth easy to acquire a few annotations (e.g., 32\nexamples) and efﬁcient to train on them; and (3)\nupdating parameters typically leads to better perfor-\nmance. Inspired by GPT-3’s ﬁndings, we propose\nseveral novel strategies for expanding its few-shot\nlearning abilities to our setting, considering both\nclassiﬁcation and—for the ﬁrst time—regression.\nFirst, we follow the route of prompt-based pre-\ndiction, ﬁrst developed by the GPT series (Radford\net al., 2018, 2019; Brown et al., 2020) for zero-shot\nprediction and recently studied by PET (Schick and\nSch¨utze, 2021a,b) for ﬁne-tuning. Prompt-based\nprediction treats the downstream task as a (masked)\nlanguage modeling problem, where the model di-\nrectly generates a textual response (referred to as\na label word) to a given prompt deﬁned by a task-\nspeciﬁc template (see Figure 1(c)). Finding the\nright prompts, however, is an art—requiring both\ndomain expertise and an understanding of the lan-\nguage model’s inner workings. Even if signiﬁcant\neffort is invested, manual prompts are likely to be\nsuboptimal. We address this issue by introducing\nautomatic prompt generation, including a pruned\nbrute-force search to identify the best working label\nwords, and a novel decoding objective to automat-\nically generate templates using the generative T5\nmodel (Raffel et al., 2020)—all of which only re-\nquire the few-shot training data. This allows us\narXiv:2012.15723v2  [cs.CL]  2 Jun 2021\nMLM\nhead\n···\nno\nutterly ✔\n···\nMLM\nhead\ngreat (label:positive)\nterrible (label:negative) ✔\nlabel:positive\nlabel:negative ✔\nCLS\nhead\n[CLS] No reason to watch . It was  [MASK] . [SEP] A fun ride . It was great . [SEP] The drama discloses nothing . It was terrible . [SEP]\n[CLS]  No reason to watch . [SEP] [CLS] it's a [MASK] movie in every regard , and [MASK] painful to watch . [SEP]\nMLM\nhead\n···\ngreat\nterrible ✔\n···\n(a) MLM pre-training (b) Fine-tuning\n(c) Prompt-based fine-tuning with demonstrations (our approach)\nDemonstration for label:positive Demonstration for label:negativeTemplateInput\nVocab   V\n<latexit sha1_base64=\"/8v18XM/5jjHz8Bu3KqobBykbcg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYCuIizJTFF0W3LisYB/QDiWTZtrQJDMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dtbWNza3tgs7xd29/YPD0tFxS0eJIrRJIh6pToA15UzSpmGG006sKBYBp+1gfJf57QlVmkXy0Uxj6gs8lCxkBBsr+ZWewGZEME9bs0q/VHar7hxolXg5KUOORr/01RtEJBFUGsKx1l3PjY2fYmUY4XRW7CWaxpiM8ZB2LZVYUO2n89AzdG6VAQojZZ80aK7+3kix0HoqAjuZZdTLXib+53UTE976KZNxYqgki0NhwpGJUNYAGjBFieFTSzBRzGZFZIQVJsb2VLQleMtfXiWtWtW7ql4/1Mr1y7yOApzCGVyABzdQh3toQBMIPMEzvMKbM3FenHfnYzG65uQ7J/AHzucPSEyRtg==</latexit>\nLabel space    Y\n<latexit sha1_base64=\"KiH56zM6qOeuxejTCrDO4mnCAPI=\">AAAB9HicbVDLSsNAFL2pr1pfVZduBltBXJSkKLosuHFZwT6kDWUynbRDJ5M4MymU0O9w40IRt36MO//GSZqFth4YOJxzL/fM8SLOlLbtb6uwtr6xuVXcLu3s7u0flA+P2iqMJaEtEvJQdj2sKGeCtjTTnHYjSXHgcdrxJrep35lSqVgoHvQsom6AR4L5jGBtJLfaD7AeE8yTx3l1UK7YNTsDWiVOTiqQozkof/WHIYkDKjThWKmeY0faTbDUjHA6L/VjRSNMJnhEe4YKHFDlJlnoOTozyhD5oTRPaJSpvzcSHCg1CzwzmWZUy14q/uf1Yu3fuAkTUaypIItDfsyRDlHaABoySYnmM0MwkcxkRWSMJSba9FQyJTjLX14l7XrNuaxd3dcrjYu8jiKcwCmcgwPX0IA7aEILCDzBM7zCmzW1Xqx362MxWrDynWP4A+vzB0zekbk=</latexit>\nLabel mapping            M ( Y )\n<latexit sha1_base64=\"EwIV6ulOE3r2P2XPhMU3VSsyDnU=\">AAACA3icbZDLSsNAFIZP6q3WW9SdbgZbobooSVF0WXDjRqhgL9KGMplO2qGTCzMToYSCG1/FjQtF3PoS7nwbJ20Qrf4w8PGfc5hzfjfiTCrL+jRyC4tLyyv51cLa+sbmlrm905RhLAhtkJCHou1iSTkLaEMxxWk7EhT7Lqctd3SR1lt3VEgWBjdqHFHHx4OAeYxgpa2euVfq+lgNCebJ1aT8zbeTo1LPLFoVayr0F+wMipCp3jM/uv2QxD4NFOFYyo5tRcpJsFCMcDopdGNJI0xGeEA7GgPsU+kk0xsm6FA7feSFQr9Aoan7cyLBvpRj39Wd6ZJyvpaa/9U6sfLOnYQFUaxoQGYfeTFHKkRpIKjPBCWKjzVgIpjeFZEhFpgoHVtBh2DPn/wXmtWKfVI5va4Wa8dZHHnYhwMogw1nUINLqEMDCNzDIzzDi/FgPBmvxtusNWdkM7vwS8b7F0pnlzg=</latexit>\nVocab   V\n<latexit sha1_base64=\"/8v18XM/5jjHz8Bu3KqobBykbcg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYCuIizJTFF0W3LisYB/QDiWTZtrQJDMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dtbWNza3tgs7xd29/YPD0tFxS0eJIrRJIh6pToA15UzSpmGG006sKBYBp+1gfJf57QlVmkXy0Uxj6gs8lCxkBBsr+ZWewGZEME9bs0q/VHar7hxolXg5KUOORr/01RtEJBFUGsKx1l3PjY2fYmUY4XRW7CWaxpiM8ZB2LZVYUO2n89AzdG6VAQojZZ80aK7+3kix0HoqAjuZZdTLXib+53UTE976KZNxYqgki0NhwpGJUNYAGjBFieFTSzBRzGZFZIQVJsb2VLQleMtfXiWtWtW7ql4/1Mr1y7yOApzCGVyABzdQh3toQBMIPMEzvMKbM3FenHfnYzG65uQ7J/AHzucPSEyRtg==</latexit>\nFigure 1: An illustration of (a) masked language model (MLM) pre-training, (b) standard ﬁne-tuning, and (c) our\nproposed LM-BFF using prompt-based ﬁne-tuning with demonstrations. The underlined text is the task-speciﬁc\ntemplate, and colored words are label words.\nto cheaply obtain effective prompts that match or\noutperform our manually chosen ones.\nSecond, we adopt the idea of incorporating\ndemonstrations as additional context. GPT-3’s\nnaive “in-context learning” paradigm picks up to\n32 randomly sampled examples, and concatenates\nthem with the input. This method is not guaran-\nteed to prioritize the most informative demonstra-\ntions, and mixing random examples from different\nclasses together creates long contexts which can\nbe hard to learn from. Additionally, the number of\nusable demonstrations is bounded by the model’s\nmaximum input length. We develop a more reﬁned\nstrategy, where, for each input, we randomly sam-\nple a single example at a time from each class to\ncreate multiple, minimal demonstration sets. We\nalso devise a novel sampling strategy that pairs in-\nputs with similar examples, thereby providing the\nmodel with more discriminative comparisons.\nWe present a systematic evaluation for analyzing\nfew-shot performance on 8 single-sentence and 7\nsentence-pair NLP tasks. We observe that given\na small number of training examples, (1) prompt-\nbased ﬁne-tuning largely outperforms standard ﬁne-\ntuning; (2) our automatic prompt search method\nmatches or outperforms manual prompts; and (3)\nincorporating demonstrations is effective for ﬁne-\ntuning, and boosts few-shot performance. Together,\nthese simple-yet-effective methods contribute to-\nwards a dramatic improvement across the tasks we\nevaluate on, and we obtain gains up to 30% abso-\nlute improvement (11% on average) compared to\nstandard ﬁne-tuning. For instance, we ﬁnd that a\nRoBERTa-large model achieves around 90% accu-\nracy on most binary sentence classiﬁcation tasks,\nwhile only relying on 32 training examples. We re-\nfer to our approach as LM-BFF, better few-shot\nfine-tuning of language models: a strong, task-\nagnostic method for few-shot learning.\n2 Related Work\nLanguage model prompting. The GPT se-\nries (Radford et al., 2018, 2019; Brown et al.,\n2020) fueled the development of prompt-based\nlearning, and we follow many of its core concepts.\nWe are also greatly inspired by the recent PET\nwork (Schick and Sch¨utze, 2021a,b), although they\nmainly focus on a semi-supervised setting where a\nlarge set of unlabeled examples are provided. We\nonly use a few annotated examples as supervision,\nand also explore automatically generated prompts\nand ﬁne-tuning with demonstrations. Furthermore,\nwe deviate from their evaluation by providing a\nmore rigorous framework, as we will discuss in §3.\nFinally, there is a large body of work on prompt-\ning for mining knowledge from pre-trained models\n(Trinh and Le, 2018; Petroni et al., 2019; Davison\net al., 2019; Talmor et al., 2020, inter alia). Dif-\nferent from these works, we focus on leveraging\nprompting for ﬁne-tuning on downstream tasks.\nAutomatic prompt search. Schick and Sch ¨utze\n(2021a) and Schick et al. (2020) explore ways\nof identifying label words automatically, however,\nnone of these results lead to better performance\ncompared to hand-picked ones. In contrast, our\nmethod searches over both templates and label\nwords, and is able to match or outperform our\nmanual prompts. Several other attempts have been\nmade in addition—yet these approaches either op-\nerate in limited domains, such as ﬁnding patterns\nto express speciﬁc relations (Jiang et al., 2020), or\nrequire a large number of examples for gradient-\nguided search (Shin et al., 2020; Zhong et al., 2021).\nOur approach aims to develop general-purpose\nsearch methods that rely only on a few annotations.\nFine-tuning of language models. A number of\nrecent studies have focused on better methods for\nﬁne-tuning language models (Howard and Ruder,\n2018; Dodge et al., 2020; Lee et al., 2020; Zhang\net al., 2021). These works mainly focus on opti-\nmization and regularization techniques to stabilize\nﬁne-tuning. Here we use standard optimization\ntechniques, and instead mainly focus our efforts on\nbetter prompt-based ﬁne-tuning in a more extreme\nfew-shot setting. We anticipate that results of these\nstudies are largely complementary to ours.\nFew-shot learning. Broadly speaking, our set-\nting is also connected to other few-shot learning\nparadigms in NLP, including (1) semi-supervised\nlearning (Miyato et al., 2017; Xie et al., 2020; Chen\net al., 2020), where a set of unlabeled examples\nare given; (2) meta-learning (Yu et al., 2018; Han\net al., 2018; Bansal et al., 2020a,b; Bao et al., 2020),\nwhere a set of auxiliary tasks are given; and (3) in-\ntermediate training (Phang et al., 2018; Yin et al.,\n2020), where a related, intermediate task is given.\nWe deviate from these settings by making minimal\nassumptions about available resources: we only\nassume a few annotated examples and a pre-trained\nlanguage model. Our focus is on understanding\nhow far we can push without any other advantages.\n3 Problem Setup\nTask formulation. In this work, we assume access\nto a pre-trained language model Lthat we wish to\nﬁne-tune on a task Dwith a label space Y. For\nthe task, we only assume K training examples per\nclass3 for the task’s training set Dtrain, such that\nthe total number of examples is Ktot = K ×|Y|,\nand Dtrain = {(xi\nin, yi)}Ktot\ni=1. Our goal is then to\ndevelop task-agnostic learning strategies that gener-\nalize well to an unseen test set (xtest\nin , ytest) ∼Dtest.\nFor model selection and hyper-parameter tuning,\nwe assume a development setDdev, of the same size\nas the few-shot training set, i.e., |Ddev|= |Dtrain|.\nThis distinction is important: using a larger devel-\nopment set confers a signiﬁcant advantage (see our\n3For regression, we partition the data into two “classes”\naccording to being above or below the median value.\nexperiments in Appendix A), and subverts our ini-\ntial goal of learning from limited data.4 For all of\nthe following experiments (unless speciﬁed other-\nwise), we take L= RoBERTa-large andK = 16.\nEvaluation datasets. We conduct a systematic\nstudy across 8 single-sentence and 7 sentence-pair\nEnglish tasks, including 8 tasks from the GLUE\nbenchmark (Wang et al., 2019), SNLI (Bowman\net al., 2015), and 6 other popular sentence clas-\nsiﬁcation tasks (SST-5, MR, CR, MPQA, Subj,\nTREC). All of the dataset details are provided in\nAppendix B. For single-sentence tasks, the goal is\nto make a prediction based on an input sentence\nxin = x1, such as whether a movie review is posi-\ntive or not. For sentence-pair tasks, the goal is to\ntake a pair of input sentences xin = (x1, x2) and\npredict the relationship between them. We also in-\nterchangeably refer to the inputs as<S1> or (<S1>,\n<S2>). Note that we mainly use SST-2 and SNLI\nfor pilot experiments and model development, mak-\ning it close to a true few-shot setting, at least for all\nthe other datasets we evaluate on.\nEvaluation protocol. Systematically evaluating\nfew-shot performance can be tricky. It is well-\nknown that ﬁne-tuning on small datasets can suffer\nfrom instability (Dodge et al., 2020; Zhang et al.,\n2021), and results may change dramatically given a\nnew split of data. To account for this, we measure\naverage performance across 5 different randomly\nsampled Dtrain and Ddev splits. This issue has also\nbeen discussed in Schick and Sch ¨utze (2021b)—\nthey suggest using a ﬁxed set of training examples.\nWe argue that sampling multiple splits gives a more\nrobust measure of performance, and a better esti-\nmate of the variance. We also observe that hyper-\nparameters can make a signiﬁcant difference, thus\nwe sweep multiple hyper-parameters for each data\nsample, and take the best setting as measured on\nthe Ddev of that sample (see Appendix C.1).\n4 Prompt-based Fine-tuning\nGiven a masked language model L, we ﬁrst con-\nvert input xin to a token sequence ˜x, and the lan-\nguage model Lthen maps ˜x to a sequence of hid-\nden vectors {hk ∈ Rd}. During standard ﬁne-\ntuning, we usually take ˜xsingle = [CLS]x1[SEP]\nor ˜xpair = [CLS]x1[SEP]x2[SEP]. For down-\n4In contrast, Schick and Sch ¨utze (2021a,b) do not use a\ndevelopment set, and adopt a set of hyper-parameters based on\npractical considerations. This is akin to “shooting in the dark”\non a setting that we show can have unintuitive outcomes.\nTask Template Label words\nSST-2 <S1>It was[MASK]. positive: great, negative: terrible\nSST-5 <S1>It was[MASK]. v.positive: great, positive: good, neutral: okay, negative: bad, v.negative: terrible\nMR <S1>It was[MASK]. positive: great, negative: terrible\nCR <S1>It was[MASK]. positive: great, negative: terrible\nSubj <S1>This is[MASK]. subjective: subjective, objective: objective\nTREC [MASK]: <S1> abbreviation: Expression, entity: Entity, description: Description\nhuman: Human, location: Location, numeric: Number\nCOLA <S1>This is[MASK]. grammatical: correct, notgrammatical: incorrect\nMNLI <S1>? [MASK], <S2> entailment: Yes, netural: Maybe, contradiction: No\nSNLI <S1>? [MASK], <S2> entailment: Yes, netural: Maybe, contradiction: No\nQNLI <S1>? [MASK], <S2> entailment: Yes, notentailment: No\nRTE <S1>? [MASK], <S2> entailment: Yes, notentailment: No\nMRPC <S1> [MASK], <S2> equivalent: Yes, notequivalent: No\nQQP <S1> [MASK], <S2> equivalent: Yes, notequivalent: No\nSTS-B <S1> [MASK], <S2> yu: Yes,yl: No\nTable 1: Manual templates and label words that we used in our experiments. STS-B is a regression task (§4.2).\nstream classiﬁcation tasks with a label space Y, we\ntrain a task-speciﬁc head, softmax(Woh[CLS]),\nby maximizing the log-probability of the correct\nlabel, where h[CLS] is the hidden vector of[CLS],\nand Wo ∈R|Y|×d is a set of randomly initialized\nparameters introduced at the start of ﬁne-tuning.\nSimilarly, for a regression task, we can introduce\nwo ∈Rd and optimize the mean squared error be-\ntween wo·h[CLS] and the gold label. In either case,\nthe number of new parameters can be substantial—\nfor example, a simple binary classiﬁcation task will\nintroduce 2,048 new parameters for a RoBERTa-\nlarge model—making it challenging to learn from a\nsmall amount of annotated data (e.g., 32 examples).\nAn alternative approach to solving this problem\nis prompt-based ﬁne-tuning, in which Lis directly\ntasked with “auto-completing” natural language\nprompts. For instance, we can formulate a binary\nsentiment classiﬁcation task using a prompt with\ninput x1 (e.g., “No reason to watch it .”) as:\nxprompt= [CLS]x1 It was[MASK]. [SEP]\nand let Ldecide whether it is more appropriate\nto ﬁll in “great” (positive) or “terrible” (negative)\nfor [MASK]. We now formalize this approach for\nclassiﬁcation and regression (§4.1 and §4.2), and\ndiscuss the importance of prompt selection (§4.3).\n4.1 Classiﬁcation\nLet M: Y →Vbe a mapping from the task\nlabel space to individual words5 in the vocabulary\n5More generally, we can consider a one-to-many mapping\nM: Y → 2|Y| in which we map labels to sets of words.\nHowever, we did not ﬁnd signiﬁcant gains in our experiments.\nVof L. Then for each xin, let the manipulation\nxprompt = T(xin) be a masked language modeling\n(MLM) input which contains one [MASK] token.\nIn this way, we can treat our task as an MLM, and\nmodel the probability of predicting class y ∈Y as:\np(y |xin) =p([MASK]= M(y) |xprompt)\n= exp(wM(y) ·h[MASK]\n)\n∑\ny′∈Yexp(wM(y′) ·h[MASK]\n), (1)\nwhere h[MASK] is the hidden vector of[MASK] and\nwv denotes the pre-softmax vector corresponding\nto v ∈V. When supervised examples {(xin, y)}\nare available, Lcan be ﬁne-tuned to minimize the\ncross-entropy loss. It is important to note that this\napproach re-uses the pre-trained weights wv and\ndoes not introduce any new parameters. It also re-\nduces the gap between pre-training and ﬁne-tuning,\nmaking it more effective in few-shot scenarios.\n4.2 Regression\nWe assume the same basic setup as in classiﬁ-\ncation, but treat the label space Yas a bounded\ninterval [vl, vu]. Inspired by Mettes et al. (2019),\nwe model the problem as an interpolation between\ntwo opposing poles, {yl, yu}, with values vl and\nvu respectively. For instance, we can formulate\nour previous sentiment analysis task as a regres-\nsion problem in the range [0, 1], where we slide\nbetween “terrible” (vl = 0) and “great” (vu = 1).\nIn this way, we can express y as a mixture model:\ny = vl ·p(yl |xin) +vu ·p(yu |xin), (2)\nwhere p(yu | xin) is the probability of yu, and\np(yl |xin) = 1−p(yu |xin). Then we deﬁne\nTemplate Label words Accuracy\nSST-2 (positive/negative) mean (std)\n<S1>It was[MASK]. great/terrible 92.7 (0.9)\n<S1>It was[MASK]. good/bad 92.5 (1.0)\n<S1>It was[MASK]. cat/dog 91.5 (1.4)\n<S1>It was[MASK]. dog/cat 86.2 (5.4)\n<S1>It was[MASK]. terrible/great 83.2 (6.9)\nFine-tuning - 81.4 (3.8)\nSNLI (entailment/neutral/contradiction) mean (std)\n<S1>? [MASK], <S2> Yes/Maybe/No77.2 (3.7)\n<S1>. [MASK], <S2> Yes/Maybe/No 76.2 (3.3)\n<S1>? [MASK] <S2> Yes/Maybe/No 74.9 (3.0)\n<S1> <S2> [MASK] Yes/Maybe/No 65.8 (2.4)\n<S2>? [MASK], <S1> Yes/Maybe/No 62.9 (4.1)\n<S1>? [MASK], <S2> Maybe/No/Yes 60.6 (4.8)\nFine-tuning - 48.4 (4.8)\nTable 2: The impact of templates and label words on\nprompt-based ﬁne-tuning (K = 16).\nM: {yl, yu} → V, and model p(yu | xin) the\nsame as Eq. (1). We ﬁne-tune Lto minimize the\nKL-divergence between the inferred p(yu |xin)\nand the observed mixture weight,(y−vl)/(vu−vl).\n4.3 Manual prompts: the good and the bad\nThe key challenge is to construct the template T\nand label words M(Y)—we refer to these two to-\ngether as a prompt P. Previous works (Schick and\nSch¨utze, 2021a,b) hand-craft both the templates\nand label words, which usually requires domain\nexpertise and trial-and-error. Table 1 summarizes\nmanual templates and label words chosen for each\ndataset in our experiments. These templates and\nlabel words were designed by intuition, and by\nconsidering formats used in previous literature.\nTo better understand what constitutes a good\ntemplate or label word, we conduct a pilot study\non SST-2 and SNLI. Table 2 shows that different\nprompts can lead to substantial differences in ﬁnal\naccuracy. Speciﬁcally, when a template is ﬁxed, the\nbetter the label words match the “semantic classes”,\nthe better the ﬁnal accuracy is ( great/terrible >\ngood/bad > cat/dog). In extreme cases where we\nswap plausible label words (e.g., terrible/great),\nwe achieve the worst overall performance. 6 Fur-\nthermore, with the same set of label words, even a\nsmall change in the template can make a difference.\nFor example, for SNLI, if we put [MASK] at the\nend, or swap sentence order, we observe a >10%\ndrop. The above evidence clearly underlines the\n6It is unclear, however, why RoBERTa thinks that “cat” is\nmore positive than “dog”. The authors tend to disagree.\nimportance of selecting good templates and label\nwords. Searching for prompts, however, is hard,\nas the search space can be very large—especially\nfor the template. Even worse, we only have a few\nexamples to use to guide our search, which can\neasily overﬁt. We will address these issues next.\n5 Automatic Prompt Generation\nWe now explore principled ways of automating\nthe search process for label words (§5.1) and tem-\nplates (§5.2). Our goals are to reduce the human\ninvolvement required to design prompts, and to ﬁnd\nmore optimal settings than those that we manually\nchoose. Here, we assume a classiﬁcation task, but\nthe process for regression is analogous.\n5.1 Automatic selection of label words\nWe ﬁrst study how to construct a label word\nmapping Mthat maximizes accuracy on Ddev af-\nter ﬁne-tuning, given a ﬁxed template T. Naively\nsearching all possible assignments, however, is (1)\ngenerally intractable, as the search space is expo-\nnential in the number of classes; and (2) prone to\noverﬁtting, as we will tend to uncover spurious\ncorrelations given only a few annotations. As a\nsimple solution, for each class c ∈Y, we construct\na pruned set Vc ⊂V of the top k vocabulary words\nbased on their conditional likelihood using the ini-\ntial L. That is, let Dc\ntrain ⊂Dtrain be the subset of\nall examples of class c. We take Vc as\nTop-k\nv∈V\n\n\n∑\nxin∈Dctrain\nlogPL\n(\n[MASK]=v |T(xin)\n)\n\n, (3)\nwhere PLdenotes the output probability distribu-\ntion of L. To further narrow down the search space,\nwe ﬁnd the topn assignments over the pruned space\nthat maximize zero-shot accuracy on Dtrain (both\nn and k are hyper-parameters, see Appendix C.2).\nThen we ﬁne-tune all top n assignments, and re-\nrank to ﬁnd the best one using Ddev. This approach\nis similar to the automatic verbalizer search meth-\nods in Schick and Sch ¨utze (2021a); Schick et al.\n(2020), except that we use a much simpler search\nprocess (brute-force) and also apply re-ranking—\nwhich we ﬁnd to be quite helpful.\n5.2 Automatic generation of templates\nNext, we study how to generate a diverse set of\ntemplates {T} automatically from a ﬁxed set of\nlabel words M(Y). To address this challenging\nproblem, we propose to use T5 (Raffel et al., 2020),\nBest template\nGenerated templates\nTraining examples for label:negative\nT5\n…\nTraining examples for label:positive\n…\nDecode\n<S1> A [MASK] one.\n<S1> This is [MASK].\n…\n<S1> A [MASK] one.\nA fun ride. <X> great <Y>\nA pleasure to watch. <X> great <Y>\nNo reason to watch. <X> terrible <Y>\nThis junk. <X> terrible <Y>\nFine-tune and\nevaluate\npositive: great, negative: terrible\nLabel mapping            M ( Y )\n<latexit sha1_base64=\"EwIV6ulOE3r2P2XPhMU3VSsyDnU=\">AAACA3icbZDLSsNAFIZP6q3WW9SdbgZbobooSVF0WXDjRqhgL9KGMplO2qGTCzMToYSCG1/FjQtF3PoS7nwbJ20Qrf4w8PGfc5hzfjfiTCrL+jRyC4tLyyv51cLa+sbmlrm905RhLAhtkJCHou1iSTkLaEMxxWk7EhT7Lqctd3SR1lt3VEgWBjdqHFHHx4OAeYxgpa2euVfq+lgNCebJ1aT8zbeTo1LPLFoVayr0F+wMipCp3jM/uv2QxD4NFOFYyo5tRcpJsFCMcDopdGNJI0xGeEA7GgPsU+kk0xsm6FA7feSFQr9Aoan7cyLBvpRj39Wd6ZJyvpaa/9U6sfLOnYQFUaxoQGYfeTFHKkRpIKjPBCWKjzVgIpjeFZEhFpgoHVtBh2DPn/wXmtWKfVI5va4Wa8dZHHnYhwMogw1nUINLqEMDCNzDIzzDi/FgPBmvxtusNWdkM7vwS8b7F0pnlzg=</latexit>\nFigure 2: Our approach for template generation.\na large pre-trained text-to-text Transformer. T5 is\npre-trained to ﬁll in missing spans (replaced by T5\nmask tokens, e.g., <X> or <Y>) in its input. For\nexample, given the input “Thank you <X> me to\nyour party <Y> week”, T5 is trained to generate\n“<X> for inviting <Y> last <Z>”, meaning that “for\ninviting” is the replacement for <X> and “last” is\nthe replacement for <Y>. This is well suited for\nprompt generation: we can simply take input sen-\ntences from Dtrain and let the T5 model construct\nthe template T, without having to specify a pre-\ndeﬁned number of tokens for it.\nGiven an input example (xin, y) ∈ Dtrain, we\nconsider the following simple conversions, denoted\nas Tg(xin, y), for formulating the T5 model inputs:7\n<S1> −→<X>M(y) <Y> <S1>,\n<S1> −→<S1> <X>M(y) <Y>,\n<S1>,<S2> −→<S1> <X>M(y) <Y> <S2>.\nAs shown in Figure 2, we rely on the T5 model\nto ﬁll in the placeholders. When decoding, our goal\nhere is to ﬁnd an output that can work well for all\nexamples in Dtrain, i.e., the output template T that\nmaximizes ∑\n(xin,y)∈Dtrain log PT5(T |Tg(xin, y)),\nwhere PT5 denotes the output probability distribu-\ntion of T5. It can be decomposed according to:\n|T|∑\nj=1\n∑\n(xin,y)∈Dtrain\nlogPT5\n(tj |t1, ..., tj−1,Tg\n(xin, y)), (4)\nwhere (t1, . . . , t|T|) are the template tokens.\nWe use beam search to decode multiple template\ncandidates. Concretely, we use a wide beam width\n(e.g., 100) to cheaply obtain a large set of diverse\ntemplates. We then ﬁne-tune each generated tem-\nplate on Dtrain and use Ddev to either pick the single\ntemplate with the best performance (Table 3), or\n7We consider putting the label word both before and after\nthe input sentence for single-sentence tasks. However, we ﬁnd\nthat it is always better to put the label words in the middle\n(between the two sentences) for sentence-pair tasks.\nthe top k templates to use as an ensemble (Table 4).\nThough it might appear to be expensive to ﬁne-tune\nthe model on each individual template, this is fast\nin practice due to the small size ofDtrain, and is also\nfully automated: making it easy to use, compared\nto manually tuning prompts for each dataset.\n6 Fine-tuning with Demonstrations\nIn this section, we study whether we can leverage\ndemonstrations when ﬁne-tuning medium-sized\nLMs, and ﬁnd better ways to exploit them.\n6.1 Training examples as demonstrations\nGPT-3’s naive approach to in-context learning\nsimply involves concatenating the input with up\nto 32 examples randomly drawn from the training\nset. This approach is suboptimal as (1) the num-\nber of available demonstrations is bounded by the\nmodel’s maximum input length;8 and (2) mixing\nnumerous random examples from different classes\ntogether creates extremely long contexts which can\nbe hard to leverage, especially for a smaller model.\nTo address these issues, we propose a simpler so-\nlution: at each training step, we randomly sample\none9 example\n(\nx(c)\nin , y(c))\n∈Dtrain from each class,\nconvert it into T\n(\nx(c)\nin\n)\nwith [MASK] replaced by\nM(y(c))—we denote this as ˜T\n(\nx(c)\nin , y(c))\n—and\nthen concatenate them with xin (Figure 1(c)):\nT(xin\n)⊕˜T(x(1)\nin , y(1))⊕···⊕ ˜T(x(|Y|)\nin , y(|Y|)).\nHere ⊕denotes concatenation of input sequences.\nDuring both training and inference we sample mul-\ntiple demonstration sets for each xin. Note that\nboth xin and demonstration examples are sampled\nfrom the same set Dtrain during training. At testing\ntime, we still sample demonstration sets fromDtrain\nand ensemble predictions across all sets.\n6.2 Sampling similar demonstrations\nWe observe that controlling the construction of\nthe demonstration examples {(x(c)\nin , y(c))}is cru-\ncial for good ﬁnal performance. For example, if\nthe set of contrastive demonstrations x(c)\nin are all\ndramatically different—from each other, or from\nthe query xin—then it becomes challenging for\nthe language model to decipher meaningful pat-\nterns. As a result, the model may simply ignore\n8GPT-3 uses a context size of 2,048 while most smaller\nlanguage models (e.g., RoBERTa) have a context size of 512.\n9We also explored sampling multiple examples per class,\nbut did not observe any improvements.\nSST-2 SST-5 MR CR MPQA Subj TREC CoLA\n(acc) (acc) (acc) (acc) (acc) (acc) (acc) (Matt.)\nMajority† 50.9 23.1 50.0 50.0 50.0 50.0 18.8 0.0\nPrompt-based zero-shot‡ 83.6 35.0 80.8 79.5 67.6 51.4 32.0 2.0\n“GPT-3” in-context learning 84.8 (1.3) 30.6 (0.9) 80.5 (1.7) 87.4 (0.8) 63.8 (2.1) 53.6 (1.0) 26.2 (2.4) -1.5 (2.4)\nFine-tuning 81.4 (3.8) 43.9 (2.0) 76.9 (5.9) 75.8 (3.2) 72.0 (3.8) 90.8 (1.8) 88.8 (2.1) 33.9(14.3)\nPrompt-based FT (man) 92.7 (0.9) 47.4 (2.5) 87.0 (1.2) 90.3 (1.0) 84.7 (2.2) 91.2 (1.1) 84.8 (5.1) 9.3 (7.3)\n+ demonstrations 92.6 (0.5) 50.6(1.4) 86.6 (2.2) 90.2 (1.2) 87.0(1.1) 92.3(0.8) 87.5 (3.2) 18.7 (8.8)\nPrompt-based FT (auto) 92.3 (1.0) 49.2 (1.6) 85.5 (2.8) 89.0 (1.4) 85.8 (1.9) 91.2 (1.1) 88.2 (2.0) 14.0 (14.1)\n+ demonstrations 93.0(0.6) 49.5 (1.7) 87.7(1.4) 91.0(0.9) 86.5 (2.6) 91.4 (1.8)89.4(1.7) 21.8 (15.9)\nFine-tuning (full)† 95.0 58.7 90.8 89.4 87.8 97.0 97.4 62.6\nMNLI MNLI-mm SNLI QNLI RTE MRPC QQP STS-B\n(acc) (acc) (acc) (acc) (acc) (F1) (F1) (Pear.)\nMajority† 32.7 33.0 33.8 49.5 52.7 81.2 0.0 -\nPrompt-based zero-shot‡ 50.8 51.7 49.5 50.8 51.3 61.9 49.7 -3.2\n“GPT-3” in-context learning 52.0 (0.7) 53.4 (0.6) 47.1 (0.6) 53.8 (0.4) 60.4 (1.4) 45.7 (6.0) 36.1 (5.2) 14.3 (2.8)\nFine-tuning 45.8 (6.4) 47.8 (6.8) 48.4 (4.8) 60.2 (6.5) 54.4 (3.9) 76.6 (2.5) 60.7 (4.3) 53.5 (8.5)\nPrompt-based FT (man) 68.3 (2.3) 70.5 (1.9) 77.2 (3.7) 64.5 (4.2) 69.1 (3.6) 74.5 (5.3) 65.5 (5.3) 71.0 (7.0)\n+ demonstrations 70.7(1.3) 72.0(1.2) 79.7(1.5) 69.2(1.9) 68.7 (2.3) 77.8 (2.0)69.8(1.8) 73.5 (5.1)\nPrompt-based FT (auto) 68.3 (2.5) 70.1 (2.6) 77.1 (2.1) 68.3 (7.4)73.9(2.2) 76.2 (2.3) 67.0 (3.0) 75.0 (3.3)\n+ demonstrations 70.0 (3.6) 72.0(3.1) 77.5 (3.5) 68.5 (5.4) 71.1 (5.3)78.1(3.4) 67.7 (5.8) 76.4(6.2)\nFine-tuning (full)† 89.8 89.5 92.6 93.3 80.9 91.4 81.7 91.9\nTable 3: Our main results using RoBERTa-large. †: full training set is used (see dataset sizes in Table B.1); ‡:\nno training examples are used; otherwise we use K = 16(per class) for few-shot experiments. We report mean\n(and standard deviation) performance over 5 different splits (§3). Majority: majority class; FT: ﬁne-tuning; man:\nmanual prompt (Table 1); auto: automatically searched templates (§5.2); “GPT-3” in-context learning: using the\nin-context learning proposed in Brown et al. (2020) with RoBERTa-large (no parameter updates).\nthe context, or even get confused by the additional\nexamples. To address this issue, we devise a simple\nstrategy in which we only sample examples that\nare semantically close to xin. Speciﬁcally, we use a\npre-trained SBERT (Reimers and Gurevych, 2019)\nmodel to obtain embeddings for all input sentences\n(for sentence-pair tasks, we use the concatenation\nof the two sentences). Here we just feed the raw\nsentences without the templates into SBERT. For\neach query xin and each label c ∈Y, we sort all\ntraining instances with the label x ∈Dc\ntrain by their\nsimilarity score to the querycos(e(xin), e(x)), and\nonly sample from the top r = 50%instances for\neach class to use as demonstrations.\n7 Experiments\nWe present our main results, and address several\nresearch questions pertaining to our LM-BFF ap-\nproach. Implementation details are in Appendix C.\n7.1 Main results\nWe use a RoBERTa-large model and set K =\n16 in our experiments. A comparison of using\nRoBERTa vs BERT can be found in Appendix D.\nFor automatic prompt search, in our main table\nwe report automatic template search only (which\nconsistently performs the best, see Table 5). To put\nour results in perspective, we compare to a number\nof baselines, namely (1) standard ﬁne-tuning in\nour few-shot setting; (2) standard ﬁne-tuning using\nthe full training set; (3) simply taking the most\nfrequent class (measured on the full training set);\n(4) prompt-based zero-shot prediction where we\ntake our manual prompts and use L“out-of-the-\nbox” without using any training examples; and (5)\n“GPT-3” in-context learning, where we use the same\nprompt-based zero-shot setting, but augment the\ncontext with randomly sampled 32 demonstrations\n(and still use RoBERTa-large, not GPT-3).\nSingle-prompt results. Table 3 shows our main\nresults using a single prompt, either from our man-\nually designed ones (Table 1) , or the best gener-\nated ones. First, prompt-based zero-shot prediction\nachieves much better performance than the ma-\njority class, showing the pre-encoded knowledge\nin RoBERTa. Also, “GPT-3” in-context learning\ndoes not always improve over zero-shot prediction,\nlikely because smaller language models are not\nexpressive enough to use off-the-shelf like GPT-3.\nPrompt-based Fine-tuningMNLI RTE\nOur single manualP 68.3 (2.3) 69.1 (3.6)\nPPET 71.9 (1.5) 69.2 (4.0)\nPours, |Pours|=|PPET| 70.4 (3.1) 73.0 (3.2)\n+ demonstrations 74.0 (1.9) 71.9 (4.6)\nPours, |Pours|= 20 72.7 (2.5) 73.1(3.3)\n+ demonstrations 75.4(1.6) 72.3 (4.5)\nTable 4: Ensemble models using manual prompts from\nPET (Schick and Sch ¨utze, 2021a,b) and our automatic\ntemplates. PET uses 4 prompts for MNLI and 5 for\nRTE. We also use an equal number of templates in\n|Pours|= |PPET|for a fair comparison.\nSST-2 SNLI TREC MRPC\nManual 92.7 77.2 84.8 74.5\nAuto T 92.3 77.1 88.2 76.2\nAuto L 91.5 75.6 87.0 77.2\nAuto T + L 92.1 77.0 89.2 74.0\nTable 5: Comparison between manual prompts and\ndifferent automatic prompt generation methods: auto-\ngenerated templates (Auto T), auto-generated label\nwords (Auto L), and their combination (Auto T + L).\nSecond, prompt-based ﬁne-tuning can greatly\noutperform standard ﬁne-tuning, both when using\na manual prompt or a generated one. CoLA is one\ninteresting exception, as the input may be a non-\ngrammatical sentence which is out of the distribu-\ntion of L. Generally, our automatically searched\ntemplates can achieve comparable or even higher\nresults than manual ones, especially for tasks in\nwhich constructing strong manual templates is less\nintuitive (e.g., TREC, QNLI and MRPC).\nFinally, using demonstrations in context leads to\nconsistent gains in a majority of tasks. In summary,\nour combined solution—ﬁne-tuning with automati-\ncally searched templates and sampled demonstra-\ntion sets—achieves a 30% gain on SNLI compared\nto standard ﬁne-tuning, and 11% gain on average.\nEnsemble results. An advantage of automatic\nprompt search is that we can generate as many\nprompts as we want, train individual models, and\ncreate large ensembles. PET (Schick and Sch¨utze,\n2021a,b) also ensembles multiple models trained\nwith manual prompts. 10 In Table 4, we make a\ndirect comparison of our searched prompts and\nPET’s manual prompts on MNLI and RTE (two\n10They then use unlabeled data and distillation to get a\nsingle model, which is outside of our scope.\nSST-2 (positive/negative)\nAuto T M(Y) = {great, terrible}\n#1.<S1>A[MASK]one .\n#2.<S1>A[MASK]piece .\n#3.<S1>All in all[MASK].\nAuto L T(xin) = <S1>It was[MASK].\n#1. irresistible/pathetic\n#2. wonderful/bad\n#3. delicious/bad\nSNLI (entailment/neutral/contradiction)\nAuto T M(Y) = {Yes, Maybe, No}\n#1.<S1>. [MASK], no ,<S2>\n#2.<S1>. [MASK], in this case<S2>\n#3.<S1>. [MASK]this time<S2>\nAuto L T(xin) = <S1>? [MASK], <S2>\n#1. Alright/Watch/Except\n#2. Hi/Watch/Worse\n#3. Regardless/Fortunately/Unless\nTable 6: Examples of our automatically generated tem-\nplates (Auto T) and label words (Auto L).\ndatasets that we evaluate in common). 11 As the\nresults show, an ensemble with multiple templates\nalways improves performance. An ensemble of the\nsame number of automatic templates achieves com-\nparable or better performance than the ensemble of\nPET’s manual prompts. Increasing the number of\nautomatic templates brings further gains.\n7.2 Analysis of generated prompts\nTable 5 gives the results of using manual vs au-\ntomatic prompts. For automatic prompts, we com-\npare template search (Auto T), label word search\n(Auto L), and a joint variant (Auto T + L) in\nwhich we start from manual label words, apply\nAuto T, and then Auto L. In most cases, Auto T\nachieves comparable or higher performance than\nmanual ones, and is consistently the best variant.\nAuto L outperforms manual prompts on TREC and\nMRPC—but is considerably worse on SNLI. Auto\nT + L is often better than Auto L, but only some-\ntimes better than Auto T. Table 6 shows examples\nfrom Auto T and Auto L (A full list in Appendix E).\nAuto T templates generally ﬁt the context and la-\nbel words well, but can contain biased peculiarities\n(e.g., “{Yes/No}, no” in SNLI). For Auto L words,\nthings are mixed: while most look intuitively rea-\nsonable, there are also some mysterious abnormali-\nties (e.g., “Hi” for the “entailment” class in SNLI).\n11In the PET NLI templates, the hypothesis is put before\nthe premise, which we actually found to be suboptimal. In our\nexperiments, we swap the two and get better results.\nSST-2 SNLI TREC MRPC\nPrompt-based FT 92.7 77.2 84.8 74.5\nUniform sampling 92.3 78.8 85.6 70.9\n+ RoBERTa sel. 92.7 79.5 83.4 76.6\n+ SBERT sel. 92.6 79.7 87.5 77.8\nTable 7: Impact of demonstration sampling strategies.\nUniform sampling randomly samples demonstrations,\nwhile selective (sel.) sampling only takes top sentences\nmeasured by the sentence encoders (§6).\n7.3 Analysis of demonstration sampling\nTable 7 compares the performance of demonstra-\ntions using uniform sampling to selective sampling\nby SBERT. We acknowledge that SBERT is trained\non SNLI and MNLI datasets, thus we also tried\na simple sentence encoder using mean pooling of\nhidden representations from RoBERTa-large. We\nﬁnd that in either case, using selective sampling\noutperforms uniform sampling, highlighting the\nimportance of sampling similar examples for incor-\nporating demonstrations in context.\n7.4 Sample efﬁciency\nFigure 3 illustrates how standard ﬁne-tuning and\nour LM-BFF compare as K increases. For a simple\ntask such as SST-2 (also see MR, CR and MPQA in\nTable 3), despite using only 32 total examples, LM-\nBFF has already nearly saturated its performance\nand is comparable to standard ﬁne-tuning over the\nentire dataset. On the harder task of SNLI, LM-\nBFF continues to improve asK increases while still\nmaintaining a performance gap over standard ﬁne-\ntuning, until the two converge around K = 256.\n8 Discussion\nReformulating NLP tasks as MLM has exciting\nimplications for few-shot learning, but also has lim-\nitations. First, while LM-BFF greatly outperforms\nstandard ﬁne-tuning, Table 3 shows that, overall,\nthe performance still substantially lags behind ﬁne-\ntuning with thousands of examples, especially for\nharder tasks. Additionally, just like standard ﬁne-\ntuning, our results also suffer from high variance.\nAs described in §2, several recent studies have tried\nto counter instability in few-shot ﬁne-tuning and\nwe expect these methods to also help here.\nWith respect to automatic prompt generation, de-\nspite its effectiveness, we still ﬁnd it practically\nchallenging to expand the search space, or general-\nize well based on only approximately 32 examples.\n16 32 64 128 256\nK\n70\n75\n80\n85\n90\n95Accuracy (%)\nSST-2\nFine-tune\nLM-BFF\n16 32 64 128 256\nK\n40\n50\n60\n70\n80\n90Accuracy (%)\nSNLI\nFine-tune\nLM-BFF\nFigure 3: Standard ﬁne-tuning vs our LM-BFF as a\nfunction of K (# instances per class). For lower K, our\nmethod consistently outperforms standard ﬁne-tuning.\nThis is partly due to our lingering reliance on some\nmanual design—either manual templates (for label\nword search) or manual label words (for template\nsearch), which allows us to get our search off the\nground, but does also bias it towards areas of the\nsearch space that we might have already imagined.\nFinally, it is important to clarify that LM-BFF fa-\nvors certain tasks which (1) can be naturally posed\nas a “ﬁll-in-the-blank” problem; (2) have relatively\nshort input sequences; and (3) do not contain many\noutput classes. Issues (2) and (3) might be ame-\nliorated with longer-context language models (e.g.,\nBeltagy et al., 2020). For tasks that are not straight-\nforward to formulate in prompting, such as struc-\ntured prediction, issue (1) is more fundamental. We\nleave it as an open question for future work.\n9 Conclusion\nIn this paper we presented LM-BFF, a set of\nsimple but effective techniques for ﬁne-tuning lan-\nguage models using only a few examples. Our\napproach proposes to (1) use prompt-based ﬁne-\ntuning with automatically searched prompts; and\n(2) include selected task demonstrations (training\nexamples) as part of the input context. We show\nthat our method outperforms vanilla ﬁne-tuning by\nup to 30% (and 11% on average). We concluded\nby discussing the limitations of our approach, and\nposed open questions for future study.\nAcknowledgements\nWe thank the members of Princeton, MIT, Ts-\ninghua NLP groups and the anonymous reviewers\nfor their valuable feedback. TG is supported by a\nGraduate Fellowship at Princeton University and\nAF is supported by an NSF Graduate Research Fel-\nlowship. This research is also partly supported by\na Google Research Scholar Award.\nReferences\nTrapit Bansal, Rishikesh Jha, and Andrew McCal-\nlum. 2020a. Learning to few-shot learn across di-\nverse natural language classiﬁcation tasks. In Inter-\nnational Conference on Computational Linguistics\n(COLING).\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,\nand Andrew McCallum. 2020b. Self-supervised\nmeta-learning for few-shot natural language classi-\nﬁcation tasks. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nYujia Bao, Menghua Wu, Shiyu Chang, and Regina\nBarzilay. 2020. Few-shot text classiﬁcation with dis-\ntributional signatures. In International Conference\non Learning Representations (ICLR).\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document Trans-\nformer. arXiv:2004.05150.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth PASCAL recognizing\ntextual entailment challenge. In TAC.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In the 11th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2017).\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classiﬁcation. In\nAssociation for Computational Linguistics (ACL).\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In the First International Conference on\nMachine Learning Challenges: Evaluating Predic-\ntive Uncertainty Visual Object Classiﬁcation, and\nRecognizing Textual Entailment.\nJoe Davison, Joshua Feldman, and Alexander M Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In North American Chapter of the Associ-\nation for Computational Linguistics (NAACL).\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn the Third International Workshop on Paraphras-\ning (IWP2005).\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In the ACL-\nPASCAL Workshop on Textual Entailment and Para-\nphrasing.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel:\nA large-scale supervised few-shot relation classiﬁ-\ncation dataset with state-of-the-art evaluation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nAssociation for Computational Linguistics (ACL).\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In ACM SIGKDD interna-\ntional conference on Knowledge discovery and data\nmining.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association of\nComputational Linguistics (TACL).\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to ﬁnetune\nlarge-scale pretrained language models. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nPascal Mettes, Elise van der Pol, and Cees Snoek. 2019.\nHyperspherical prototype networks. In Advances in\nNeural Information Processing Systems (NeurIPS).\nTakeru Miyato, Andrew M Dai, and Ian Goodfel-\nlow. 2017. Adversarial training methods for semi-\nsupervised text classiﬁcation. In International Con-\nference on Learning Representations (ICLR).\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Association for\nComputational Linguistics (ACL).\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. InAssociation for Com-\nputational Linguistics (ACL).\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nJason Phang, Thibault F ´evry, and Samuel R Bow-\nman. 2018. Sentence encoders on STILTs: Supple-\nmentary training on intermediate labeled-data tasks.\narXiv preprint arXiv:1811.01088.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical re-\nport, OpenAI.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text Trans-\nformer. The Journal of Machine Learning Research\n(JMLR), 21(140).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Empirical Meth-\nods in Natural Language Processing (EMNLP).\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Empirical Methods in Natural Lan-\nguage Processing and International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP).\nTimo Schick, Helmut Schmid, and Hinrich Sch ¨utze.\n2020. Automatically identifying words that can\nserve as labels for few-shot text classiﬁcation. In\nInternational Conference on Computational Linguis-\ntics (COLING).\nTimo Schick and Hinrich Sch ¨utze. 2021a. Exploit-\ning cloze questions for few-shot text classiﬁcation\nand natural language inference. In European Chap-\nter of the Association for Computational Linguistics\n(EACL).\nTimo Schick and Hinrich Sch ¨utze. 2021b. It’s not\njust size that matters: Small language models are\nalso few-shot learners. In North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nAutomatic prompt construction for masked language\nmodels. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation of Computational Linguistics (TACL), 8.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nEllen M V oorhees and Dawn M Tice. 2000. Building\na question answering test collection. In the 23rd\nannual international ACM SIGIR conference on Re-\nsearch and development in information retrieval.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association of Computational\nLinguistics (TACL), 7.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\n2005. Annotating expressions of opinions and emo-\ntions in language. Language resources and evalua-\ntion, 39(2-3).\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT).\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems (NeurIPS), 33.\nWenpeng Yin, Nazneen Fatema Rajani, Dragomir\nRadev, Richard Socher, and Caiming Xiong. 2020.\nUniversal natural language processing with limited\nannotations: Try few-shot textual entailment as a\nstart. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni\nPotdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,\nand Bowen Zhou. 2018. Diverse few-shot text clas-\nsiﬁcation with multiple metrics. In North American\nChapter of the Association for Computational Lin-\nguistics (NAACL).\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\nWeinberger, and Yoav Artzi. 2021. Revisiting few-\nsample BERT ﬁne-tuning. In International Confer-\nence on Learning Representations (ICLR).\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning to\nrecall. In North American Association for Computa-\ntional Linguistics (NAACL).\nA Impact of Development Sets\nTable A.1 shows how the size of the development\nsets can affect the ﬁnal performance of the model.\nFor “No Ddev”, we take the same hyper-parameters\nfrom Schick and Sch¨utze (2021a,b): batch size =\n16, learning rate = 1e-5 and training steps = 250.\nWe also experiment with a variant that we sample a\ndevelopment set of 10 times larger than the training\nset. We can see that using larger development sets\nleads to better performance, and this is why we\nstick to |Dtrain|= |Ddev|in our few-shot setting.\nFine-tuning SST-2 SNLI TREC MRPC\nNoDdev 79.5 49.2 83.9 77.8\n|Ddev|=|Dtrain| 81.4 48.4 88.8 76.6\n|Ddev|= 10|Dtrain| 83.5 52.0 89.4 79.6\nPrompt-based FT SST-2 SNLI TREC MRPC\nNoDdev 92.1 75.3 84.8 70.2\n|Ddev|=|Dtrain| 92.7 77.2 84.8 74.5\n|Ddev|= 10|Dtrain| 93.0 79.7 89.3 80.9\nTable A.1: Impact of different sizes of development\nsets. Standard deviations are omitted here to save space.\nFor No |Ddev|, we use the same set of hyper-parameters\nas Schick and Sch¨utze (2021a,b).\nB Datasets\nFor SNLI (Bowman et al., 2015) and datasets\nfrom GLUE (Wang et al., 2019), including SST-\n2 (Socher et al., 2013), CoLA (Warstadt et al.,\n2019), MNLI (Williams et al., 2018), QNLI (Ra-\njpurkar et al., 2016), RTE (Dagan et al., 2005;\nBar Haim et al., 2006; Giampiccolo et al., 2007;\nBentivogli et al., 2009), MRPC (Dolan and Brock-\nett, 2005), QQP12 and STS-B (Cer et al., 2017), we\nfollow Zhang et al. (2021) and use their original\ndevelopment sets for testing. For datasets which re-\nquire a cross-validation evaluation—MR (Pang and\nLee, 2005), CR (Hu and Liu, 2004), MPQA (Wiebe\net al., 2005), Subj (Pang and Lee, 2004)—we sim-\nply randomly sample 2,000 examples as the testing\nset and leave them out from training. For SST-\n5 (Socher et al., 2013) and TREC (V oorhees and\nTice, 2000), we use their ofﬁcial test sets. We show\ndataset statistics in Table B.1.\nC Experimental Details\nC.1 Hyper-parameter selection\nFor grid search, we take learning rates from {1e-\n5, 2e-5, 5e-5}and batch sizes from{2, 4, 8}. These\n12https://www.quora.com/q/quoradata/\nnumbers are picked by pilot experiments on the\nSST-2 and SNLI datasets. We also use early stop-\nping to avoid overﬁtting. For each trial, we train\nthe model for 1,000 steps, validate the performance\nevery 100 steps, and take the best checkpoint.\nC.2 Prompt-based ﬁne-tuning\nTable 1 shows all the manual templates and la-\nbel words we use in experiment. For automatically\ntemplate generation, we take the T5-3B13 model,\nwhich is the largest publicly available one that can\nﬁt on a single GPU. For automatically searching la-\nbel words, we setk to 100 for all tasks except SST-5\nand TREC. For SST-5 we set a smaller k = 30, as\nit is a 5-way classiﬁcation task. For TREC, we ob-\nserve that ﬁltering Vc using conditional likelihood\nalone is still noisy, thus we set k = 1000, and then\nre-rank Vc by the nearest neighbors of the original\nmanual label words and take the top 30 per class.\nWe set n to 100 in all experiments. Due to the\nlarge number of trials in automatic search, we take\na ﬁxed set of hyper-parameters in this part: batch\nsize of 8 and learning rate of 1e-5.\nSince the idea of prompt-based ﬁne-tuning is to\nmake the input and output distribution close to the\npre-training, the implementation details are crucial.\nFor templates, we put extra space before sentences\nif it is not at the beginning of the input. Also,\nwe lowercase the ﬁrst letter of the sentence if it is\nconcatenated with a preﬁx (e.g., <S2> in Table 1).\nAlso if one sentence is appended any punctuation\n(e.g., <S1> in Table 1), then the last character of the\noriginal sentence is discarded. Finally, we prepend\na space for label words in M(Y). For example,\nwe use “ great” instead of “great” in the RoBERTa\nvocabulary, where “ ” stands for space.\nC.3 Fine-tuning with demonstrations\nWhen using demonstrations, we sample 16 dif-\nferent sets of demonstrations for each input and\naverage the predicted log probability for each class\nduring inference. We ﬁnd that further increasing\nthe number of samples does not bring substantial\nimprovement. Additional, we have tried different\naggregation methods like taking the result with\nthe maximum conﬁdence and we did not ﬁnd a\nmeaningful improvement. For selective demonstra-\ntions, we take roberta-large-nli-stsb-\n13We take the T5 1.0 checkpoint, which is trained on both\nunsupervised and downstream task data. We compared it to\nT5 1.1 (without downstream task data) and did not ﬁnd a\nsigniﬁcant difference in generated templates.\nCategory Dataset|Y| L #Train #Test Type Labels (classiﬁcation tasks)\nSST-2 2 19 6,920 872 sentiment positive, negative\nSST-5 5 18 8,544 2,210 sentiment v. pos., positive, neutral, negative, v. neg.\nMR 2 20 8,662 2,000 sentiment positive, negative\nsingle- CR 2 19 1,775 2,000 sentiment positive, negative\nsentence MPQA 2 3 8,606 2,000 opinion polarity positive, negative\nSubj 2 23 8,000 2,000 subjectivity subjective, objective\nTREC 6 10 5,452 500 question cls. abbr., entity, description, human, loc., num.\nCoLA 2 8 8,551 1,042 acceptability grammatical, not grammatical\nMNLI 3 22/11 392,702 9,815 NLI entailment, neutral, contradiction\nSNLI 3 14/8 549,367 9,842 NLI entailment, neutral, contradiction\nsentence- QNLI 2 11/30 104,743 5,463 NLI entailment, not entailment\npair RTE 2 49/10 2,490 277 NLI entailment, not entailment\nMRPC 2 22/21 3,668 408 paraphrase equivalent, not equivalent\nQQP 2 12/12 363,846 40,431 paraphrase equivalent, not equivalent\nSTS-B R 11/11 5,749 1,500 sent. similarity -\nTable B.1: The datasets evaluated in this work.|Y|: # of classes for classiﬁcation tasks (with one exception: STS-B\nis a real-valued regression task over the interval [0, 5]). L: average # of words in input sentence(s). Note that we\nonly sample Dtrain and Ddev of K ×|Y| examples from the original training set in our few-shot experiments (§3).\nBERT-large SST-2 SNLI TREC MRPC\nFine-tuning 79.5 51.4 80.3 74.4\nPrompt-based FT 85.6 59.2 79.0 66.8\n+ demo (1-seg) 87.5 50.4 77.2 68.5\n+ demo (2-seg) 86.1 61.3 77.9 73.2\n+ demo (n-seg) 86.4 58.6 79.6 71.0\nRoBERTa-large SST-2 SNLI TREC MRPC\nFine-tuning 81.4 48.4 88.8 76.6\nPrompt-based FT 92.7 77.2 84.8 74.5\n+ demonstrations 92.6 79.7 87.5 77.8\nTable D.1: A comparison of BERT-large vs RoBERTa-\nlarge. We use manual prompts in these experiments.\nmean-tokens14 from Reimers and Gurevych\n(2019) as our sentence embedding model.\nD Comparisons of BERT vs RoBERTa\nTable D.1 compares the results of BERT-large\n(uncased) and RoBERTa-large in our settings. Pre-\ntrained BERT provides two segment embeddings\n(A/B) for different parts of input. The common\npractice, when ﬁne-tuning BERT, is that using only\nsegment A for single-sentence tasks, and using seg-\nment A/B for the two sentences in sentence-pair\ntasks. In our case of incorporating demonstrations,\nhowever, we have more than two sentences. Thus\nwe explore the following different strategies for seg-\nments: (1) using the A segment for all sentences\n14https://github.com/UKPLab/\nsentence-transformers\n(1-seg); (2) using the A segment for the original\ninput and the B segment for the demonstrations\n(2-seg); (3) using different segment embeddings\nfor each sentence (n-seg), e.g., for SNLI, we use\ndifferent segments for each premise and hypoth-\nesis in both the original input and the demonstra-\ntions, which leads to a total number of 8 segment\nembeddings. This introduces new segment em-\nbeddings (randomly initialized and learned during\nﬁne-tuning) as the pre-trained BERT only has two.\nTable D.1 shows that prompt-based ﬁne-tuning\nwith demonstrations also works for BERT, and 2-\nseg works the best when incorporating demonstra-\ntions. Still, we take RoBERTa-large as our main\nmodel, for RoBERTa performs much better than\nBERT and RoBERTa saves the trouble to tune the\nusage of segment embeddings.\nE Generated Prompts\nWe demonstrate the top 3 automatically gener-\nated templates and label words for all tasks in Ta-\nble E.1. In general, most automatic templates are\nreasonable and grammatically correct. For the label\nwords, the generated results look intuitive for most\nsingle sentence tasks. For other tasks, the automatic\nones can be counterintuitive in some cases. It is\nstill unclear why the language model picks these\nwords and sometimes they actually work well. We\nleave this for future study.\nTask Auto template Auto label words\nSST-2 (positive/negative)\n<S1>A[MASK]one . irresistible/pathetic\n<S1>A[MASK]piece . wonderful/bad\n<S1>All in all[MASK]. delicious/bad\nSST-5 (very positive/positive/neutral/negative/very negative)\n<S1>The movie is[MASK]. wonderful/remarkable/hilarious/better/awful\n<S1>The music is[MASK]. wonderful/perfect/hilarious/better/awful\n<S1>But it is[MASK]. unforgettable/extraordinary/good/better/terrible\nMR (positive/negative)\nIt was[MASK]!<S1> epic/terrible\n<S1>It’s[MASK]. epic/awful\n<S1>A[MASK]piece of work . exquisite/horrible\nCR (positive/negative)\n<S1>It’s[MASK]! fantastic/horrible\n<S1>The quality is[MASK]. neat/pointless\n<S1>That is[MASK]. magniﬁcent/unacceptable\nMPQA(positive/negative)\n<S1>is[MASK]. important/close\n<S1>,[MASK]! needed/bad\n<S1>. [MASK]. unexpected/shocking\nSubj (subjective/objective)\n<S1>It’s all[MASK]. everywhere/tragic\n<S1>It’s[MASK]. everywhere/horrifying\n<S1>Is it[MASK]? something/surreal\nTREC (abbreviation/entity/description/human/location/numeric)\nQ:[MASK]:<S1> Application/Advisor/Discussion/Culture/Assignment/Minute\n<S1>Why[MASK]? Production/AE/Context/Artist/Assignment/Minute\n<S1>Answer:[MASK]. Personality/Advisor/Conclusion/Hum/Assignment/Minute\nCoLA (grammatical/notgrammatical)\n<S1>You are[MASK]. one/proof\nIt is[MASK].<S1> wrong/sad\nI am[MASK].<S1> misleading/disappointing\nMNLI (entailment/neutral/contradiction)\n<S1>. [MASK], you are right ,<S2> Fine/Plus/Otherwise\n<S1>. [MASK]you’re right<S2> There/Plus/Otherwise\n<S1>. [MASK]!<S2> Meaning/Plus/Otherwise\nSNLI (entailment/neutral/contradiction)\n<S1>. [MASK], no ,<S2> Alright/Watch/Except\n<S1>. [MASK], in this case<S2> Hi/Watch/Worse\n<S1>. [MASK]this time<S2> Regardless/Fortunately/Unless\nQNLI (entailment/notentailment)\n<S1>?[MASK]. Yes ,<S2> Okay/Nonetheless\n<S1>?[MASK]. It is known that<S2> Notably/Yet\n<S1>?[MASK], however ,<S2> Speciﬁcally/Notably\nRTE (entailment/notentailment)\n<S1>. [MASK], I believe<S2> Clearly/Yet\n<S1>. [MASK], I think that<S2> Accordingly/meanwhile\n<S1>. [MASK], I think<S2> So/Meanwhile\nMRPC(equivalent/notequivalent)\n<S1>. [MASK]!<S2> Rather/Alas\n<S1>. [MASK]. This is the ﬁrst time<S2> At/Thus\n<S1>. [MASK]. That’s right .<S2> Instead/Moreover\nQQP (equivalent/notequivalent)\n<S1>?[MASK], but<S2> Me/Since\n<S1>?[MASK], please ,<S2> Um/Best\n<S1>?[MASK], I want to know<S2> Ironically/Beyond\nSTS-B (yu/yl)\n<S1>. [MASK]sir<S2> Note/Next\n<S1>. [MASK], it is not .<S2> Yesterday/meanwhile\n<S1>. [MASK]. It is<S2> Yeah/meanwhile\nTable E.1: Top 3 automatically generated templates and label words for all tasks based on one split of K = 16\ntraining examples. Note that automatic template results are based on manual label words and automatic label word\nresults are based on manual templates provided in Table 1."
}