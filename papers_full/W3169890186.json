{
  "title": "On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies",
  "url": "https://openalex.org/W3169890186",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2116085444",
      "name": "Tianyi Zhang",
      "affiliations": [
        "Laboratoire d'Informatique de Paris-Nord",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3171503467",
      "name": "Tatsunori B. Hashimoto",
      "affiliations": [
        "Laboratoire d'Informatique de Paris-Nord",
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3098834468",
    "https://openalex.org/W2153568660",
    "https://openalex.org/W1495323440",
    "https://openalex.org/W1508977358",
    "https://openalex.org/W2135046866",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W3131755153",
    "https://openalex.org/W2951278869",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2040467972",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3092185277",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W83404773",
    "https://openalex.org/W3114985980",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2159514223",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2049250173",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2955041501",
    "https://openalex.org/W1495446613",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2294835092",
    "https://openalex.org/W658559791",
    "https://openalex.org/W3046882683",
    "https://openalex.org/W2888844359",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W3103838460",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3211536067",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2510403706",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2995724109"
  ],
  "abstract": "We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5131–5146\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5131\nOn the Inductive Bias of Masked Language Modeling:\nFrom Statistical to Syntactic Dependencies\nTianyi Zhang\nComputer Science Department\nStanford University\ntianyizhang@cs.stanford.edu\nTatsunori B. Hashimoto\nComputer Science Department\nStanford University\nthashim@stanford.edu\nAbstract\nWe study how masking and predicting tokens\nin an unsupervised fashion can give rise to\nlinguistic structures and downstream perfor-\nmance gains. Recent theories have suggested\nthat pretrained language models acquire useful\ninductive biases through masks that implicitly\nact as cloze reductions. While appealing, we\nshow that the success of the random masking\nstrategy used in practice cannot be explained\nby such cloze-like masks alone. We construct\ncloze-like masks using task-speciﬁc lexicons\nfor three different classiﬁcation datasets and\nshow that the majority of pretrained perfor-\nmance gains come fromgeneric masks that are\nnot associated with the lexicon. To explain\nthe empirical success of these generic masks,\nwe demonstrate a correspondence between the\nmasked language model (MLM) objective and\nexisting methods for learning statistical depen-\ndencies in graphical models. Using this, we\nderive a method for extracting these learned\nstatistical dependencies in MLMs and show\nthat these dependencies encode useful induc-\ntive biases in the form of syntactic structures.\nIn an unsupervised parsing evaluation, simply\nforming a minimum spanning tree on the im-\nplied statistical dependence structure outper-\nforms a classic method for unsupervised pars-\ning (58.74 vs. 55.91 UUAS).\n1 Introduction\nPretrained masked language models (Devlin et al.,\n2019; Liu et al., 2019b) have beneﬁtted a wide\nrange of natural language processing (NLP)\ntasks (Liu, 2019; Wadden et al., 2019; Zhu et al.,\n2020). Despite recent progress in understanding\nwhat useful information is captured by MLMs (Liu\net al., 2019a; Hewitt and Manning, 2019), it re-\nmains a mystery why task-agnostic masking of\nwords can capture linguistic structures and transfer\nto downstream tasks.\nOne popular justiﬁcation of MLMs relies on\nviewing masking as a form of cloze reduction.\nCloze-like \nMasking\nI this movie[M ASK]\nDependency \nLearning\nI like this movie\nFigure 1: We study the inductive bias of MLM ob-\njectives and show that cloze-like masking (left) does\nnot account for much of the downstream performance\ngains. Instead, we show that MLM objectives are bi-\nased towards extracting both statistical and syntactic\ndependencies using random masks (right).\nCloze reductions reformulate an NLP task into a\nprompt question and a blank and elicit answers by\nﬁlling in the blank (Figure 1). When tested by cloze\nreductions pretrained MLMs and left-to-right lan-\nguage models (LMs) have been shown to possess\nabundant factual knowledge (Petroni et al., 2019)\nand display impressive few-shot ability (Brown\net al., 2020). This success has inspired recent\nhypotheses that some word masks are cloze-like\nand provide indirect supervision to downstream\ntasks (Saunshi et al., 2020; Lee et al., 2020). For\nexample, a sentiment classiﬁcation task (Pang et al.,\n2002) can be reformulated into ﬁlling inlike or hate\nin the cloze I [MASK] this movie. Such cloze-like\nmasks provide a clear way in which an MLM can\nimplicitly learn to perform sentiment classiﬁcation.\nWhile this hypothesis is appealing, MLMs in\npractice are trained with uniform masking that\ndoes not contain the special structure required by\ncloze-like masks most of the time. For example,\npredicting a generic word this in the cloze I like\n[MASK] movie would not offer task-speciﬁc super-\nvision. We quantify the importance of cloze-like\nand generic masks by explicitly creating cloze-like\nmasks using task-speciﬁc lexicons and comparing\nmodels pretrained on these masks. These experi-\nments suggest that although cloze-like masks can\nbe helpful, the success of uniform masking cannot\n5132\nbe explained via cloze-like masks alone. In fact, we\ndemonstrate that uniform masking performs as well\nas a negative control where we explicitly remove\ncloze-like masks from the mask distribution.\nTo address this mismatch between theory and\npractice, we offer a new hypothesis of how generic\nmasks can help downstream learning. We pro-\npose a conceptual model for MLMs by drawing\na correspondence between masking and graphical\nmodel neighborhood selection (Meinshausen and\nBühlmann, 2006). Using this, we show that MLM\nobjectives are designed to recover statistical de-\npendencies in the presence of latent variables and\npropose an estimator that can recover these learned\ndependencies from MLMs. We hypothesize that\nstatistical dependencies in the MLM objective cap-\nture useful linguistic dependencies and demonstrate\nthis by using recovered statistical dependencies\nto perform unsupervised parsing, outperforming\nan actual unsupervised parsing baseline (58.74 vs\n55.91 UUAS; Klein and Manning, 2004). We re-\nlease our implementation on Github1.\n2 Related works\nTheories inspired by Cloze Reductions.Cloze\nreductions are ﬁll-in-the-blank tests that reformu-\nlate an NLP task into an LM problem. Existing\nwork demonstrates that such reductions can be\nhighly effective for zero/few-shot prediction (Rad-\nford et al., 2019; Brown et al., 2020) as well as\nrelation extraction (Petroni et al., 2019; Jiang et al.,\n2020).\nThese ﬁll-in-the-blank tasks provide a clear way\nby which LMs can obtain supervision about down-\nstream tasks, and recent work demonstrates how\nsuch implicit supervision can lead to useful rep-\nresentations (Saunshi et al., 2020). More general\narguments by Lee et al. (2020) show these theo-\nries hold across a range of self-supervised settings.\nWhile these theories provide compelling arguments\nfor the value of pre-training with cloze tasks, they\ndo not provide a clear reason why uniformly ran-\ndom masks such as those used in BERT provide\nsuch strong gains. In our work, we quantify this\ngap using lexicon-based cloze-like masks and show\nthat cloze-like masks alone are unlikely to account\nfor the complete success of MLM since generic and\nnon-cloze masks are responsible for a substantial\npart of the empirical performance of MLMs.\n1https://github.com/tatsu-lab/mlm_\ninductive_bias\nTheories for vector representations.Our goal\nof understanding how masking can lead to useful\ninductive biases and linguistic structures is closely\nrelated to that of papers studying the theory of word\nembedding representations (Mikolov et al., 2013;\nPennington et al., 2014; Arora et al., 2015). Ex-\nisting work has drawn a correspondence between\nword embeddings and low-rank factorization of a\npointwise mutual information (PMI) matrix (Levy\nand Goldberg, 2014) and others have shown that\nPMI is highly correlated with human semantic sim-\nilarity judgements (Hashimoto et al., 2016).\nWhile existing theories for word embeddings\ncannot be applied to MLMs, we draw inspiration\nfrom them and derive an analogous set of results.\nOur work shows a correspondence between MLM\nobjectives and graphical model learning through\nconditional mutual information, as well as evidence\nthat the conditional independence structure learned\nby MLMs is closely related to syntactic structure.\nProbing Pretrained Representations.Recent\nwork has applied probing methods (Belinkov and\nGlass, 2019) to analyze what information is cap-\ntured in the pretrained representations. This line of\nwork shows that pretrained representations encode\na diverse range of knowledge (Peters et al., 2018;\nTenney et al., 2019; Liu et al., 2019a; Hewitt and\nManning, 2019; Wu et al., 2020). While probing\nprovides intriguing evidence of linguistic structures\nencoded by MLMs, they do not address the goals\nof this work, which is how the pretraining objective\nencourages MLMs to extract such structures.\n3 Motivation\n3.1 Problem Statement\nMasked Language Modelingasks the model to\npredict a token given its surrounding context. For-\nmally, consider an input sequence X of Ltokens\n⟨x1,...,x L⟩where each variable takes a value\nfrom a vocabulary V. Let X ∼ D be the data\ngenerating distribution of X. Let xi be the ith\ntoken in X, and let X\\i denote the sequence af-\nter replacing the ith token with a special [MASK]\ntoken. In other words,\nX\\i := ⟨x1,...,x i−1,[MASK],xi+1,...,x L⟩.\nSimilarly, deﬁne X\\{i,j}as replacing both xi and\nxj with [MASK]. MLM determines what tokens\nare masked by a mask distribution i ∼M. The\ngoal of MLM is to learn a probabilistic model pθ\n5133\npositivebeautiful movie Modified Input:\nCloze-like Mask: [M ASK]beautiful movie \nGeneric Mask: positive[M ASK] beautiful \nFigure 2: In our case study, we append the true label to\neach input and create ideal cloze-like masks. We study\nhow deviations from the ideal mask distribution affect\ndownstream performance by adding in generic masks.\nthat minimizes\nLMLM = E\nX∼D,i∼M\n−log pθ(xi|X\\i).\nIn BERT pretraining, each input token is masked\nwith a ﬁxed, uniform probability, which is a hyper-\nparameter to be chosen. We refer to this strategy as\nuniform masking.\nFinetuning is the canonical method for using\npretrained MLMs. Consider a prediction task\nwhere y ∈Y is the target variable, e.g., the senti-\nment label of a review. Finetuning uses gradient\ndescent to modify the pretrained parameters θand\nlearn a new set of parameters φto minimize\nLﬁnetune = E\nX∼D′,y∼p(y|X)\n−log pθ,φ(y|X),\nwhere p(y|x) is the ground-truth distribution and\nD′is the data distribution of the downstream task.\nOur goals.We will study how the mask distri-\nbution M affects downstream performance. We\ndeﬁne perfect cloze reductions as some partition\nof the vocabulary Vy such that p(xi ∈Vy|X\\i) ≈\np(y|X). For a distribution M such that the masks\nwe draw are perfect cloze-reductions, the MLM ob-\njective offers direct supervision to ﬁnetuning since\nLMLM ≈Lﬁnetune. In contrast to cloze-like mask-\ning, in uniform masking we can think of pθ as\nimplicitly learning a generative model of X (Wang\nand Cho, 2019). Therefore, as M moves away\nfrom the ideal distribution and becomes more uni-\nform, we expect pθ to model more of the full data\ndistribution Dinstead of focusing on cloze-like su-\npervision for the downstream task. This mismatch\nbetween theory and practice raises questions about\nhow MLM with uniform masking can learn useful\ninductive biases.\nWhen LMLM is not Lﬁnetune, what is LMLM learn-\ning? We analyze LMLM and show that it is similar\nto a form of conditional mutual information based\ngraphical model structure learning.\n3.2 Case Study for Cloze-like Masking\nTo motivate our subsequent discussions, we per-\nform a controlled study for the case when LMLM ≈\n10 30 100 300 1000 3000 10000\nData Size\n0.5\n0.6\n0.7\n0.8Dev. Accuracy\nSST-2 Finetuning Results\nCloze-100%\nCloze-80%\nCloze-60%\nCloze-40%\nCloze-20%\nCloze-0%\nNo Pretrain\nFigure 3: SST-2 development set accuracy. CLOZE -p%\nis pretrained on a mixture of masks where p% of the\nmasks are Cloze-like. N OPRETRAIN trains a classiﬁer\nwithout any pretraining. Even a small modiﬁcation of\nthe ideal mask distribution degrades performance.\nLﬁnetune and analyze how deviations from the ideal\nmask distribution affect downstream performance.\nWe perform analysis on the Stanford Sentiment\nTreebank (SST-2; Socher et al., 2013), which re-\nquires models to classify short movie reviews into\npositive or negative sentiment. We append the\nground-truth label (as the wordpositive or negative)\nto each movie review (Figure 2). Masking the last\nword in each review is, by deﬁnition, an ideal mask\ndistribution. To study how the deviation from the\nideal mask distribution degrades downstream per-\nformance, we vary the amount of cloze-like masks\nduring training. We do this by masking out the last\nword for p% of the time and masking out a random\nword in the movie review for (100 −p)% of the\ntime, and choose p∈{0,20,40,60,80,100}.\nExperimental details.We split the SST-2 train-\ning set into two halves, use one for pretraining, and\nthe other for ﬁnetuning. For the ﬁnetuning data,\nwe do not append the ground-truth label. We pre-\ntrain small transformers with LMLM using different\nmasking strategies and ﬁnetune them along with\na baseline that is not pretrained ( NOPRETRAIN ).\nFurther details are in Appendix A.\nResults. We observe that while cloze-like masks\ncan lead to successful transfer, even a small modiﬁ-\ncation of the ideal mask distribution deteriorates\nperformance. Figure 3 shows the development set\naccuracy of seven model variants averaged across\nten random trials. We observe as pdecreases, the\nperformance of CLOZE -p% degrades. Notably,\nCLOZE -80% is already worse than CLOZE -100%\nand CLOZE -20% does not outperform NOPRE-\nTRAIN by much. We notice that CLOZE -0% in\nfact degrades ﬁnetuning performance, potentially\nbecause the pretrained model is over-specialized\nto the language modeling task (Zhang et al., 2020;\nTamkin et al., 2020). While this is a toy example,\nwe observe similar results for actual MLM models\n5134\nZ\nx 2 : prefer\nx 5 : f light\nx 3 : the x 4 : morning\nx 1 : I\nFigure 4: Our conceptual framework of MLM. All co-\nordinates of X are dependent on the latent variable Z\nwhile there is only sparse dependency among X.\nacross three tasks (Section 5.1), and this motivates\nus to look for a framework that explains the success\nof generic masks in practice.\n4 Analysis\nIn the previous section, we saw that cloze-like\nmasks do not necessarily explain the empirical suc-\ncess of MLMs with uniform masking strategies.\nUnderstanding uniform masking seems challeng-\ning at ﬁrst, as uniform-mask MLMs seem to lack\ntask-speciﬁc supervision and is distinct from exist-\ning unsupervised learning methods such as word\nembeddings (which rely upon linear dimensional-\nity reduction) and autoencoders (which rely upon\ndenoising). However, we show in this section that\nthere is a correspondence between MLM objectives\nand classic methods for graphical model structure\nlearning. As a consequence, we demonstrate that\nMLMs are implicitly trained to recover statistical\ndependencies among observed tokens.\n4.1 Intuition and Theoretical Analysis\nOur starting point is the observation that predicting\na single feature ( xi) from all others ( X\\i) is the\ncore subroutine in the classic Gaussian graphical\nmodel structure learning algorithm of Meinshausen\nand Bühlmann (2006). In this approach, Ldiffer-\nent Lasso regression models are trained (Tibshirani,\n1996) with each model predicting xi from X\\i, and\nthe nonzero coefﬁcients of this regression corre-\nspond to the conditional dependence structure of\nthe graphical model.\nThe MLM objective can be interpreted as a non-\nlinear extension of this approach, much like a clas-\nsical algorithm that uses conditional mutual in-\nformation (MI) estimators to recover a graphical\nmodel (Anandkumar et al., 2012). Despite the sim-\nilarity, real world texts are better viewed as models\nwith latent variables (e.g. topics; Blei et al., 2003)\nand many dependencies across tokens arise due\nto latent variables, which makes learning the di-\nrect dependencies difﬁcult. We show that MLMs\nimplicitly recover the latent variables and can cap-\nture the direct dependencies while accounting for\nthe effect of latent variables. Finally, MLMs are\nonly approximations to the true distribution and\nwe show that the MLM objective can induce high-\nquality approximations of conditional MI.\nAnalysis setup. To better understand MLMs\nas a way to recover graphical model structures,\nwe show mask-based models can recover latent\nvariables and the direct dependencies among vari-\nables in the Gaussian graphical model setting\nof Meinshausen and Bühlmann (2006). Let X =\n[x1,..., xL] ∈ RL represent an input sequence\nwhere each of its coordinates xi represents a token,\nand Z ∈Rk be a latent variable that controls the se-\nquence generation process. We assume that all co-\nordinates of X are dependent on the latent variable\nZ, and there are sparse dependencies among the\nobserved variables (Figure 4). In other words, we\ncan write Z ∼N(0,ΣZZ) and X ∼N(AZ,ΣXX).\nIntuitively, we can imagine thatZ represents shared\nsemantic information, e.g. a topic, and ΣXX repre-\nsents the syntactic dependencies. In this Gaussian\ngraphical model, the MLM is analogous to regress-\ning each coordinate ofX from all other coordinates,\nwhich we refer to as masked regression.\nMLM representations can recover latent\nvariable. We now study the behavior of masked\nregression through the representation xmask,i that\nis obtained by applying masked regression on the\nith coordinate of X and using the predicted values.\nOur result shows that masked regression is similar\nto the two-step process of ﬁrst recovering the latent\nvariable Z from X\\i and then predicting xi from\nZ.\nLet ΣXX,\\i,i ∈Rd−1 be the vector formed by\ndropping the ith row and taking the ith column of\nΣXX and β2SLS,i be the linear map resulting from\nthe two-stage regression X\\i →Z →xi.\nProposition 1.Assuming that ΣXX is full rank,\nxmask,i = β2SLS,iX\\i + O(\nΣXX,\\i,i\n\n2),\nIn other words, masked regression implicitly re-\ncovers the subspace that we would get if we ﬁrst ex-\nplicitly recovered the latent variables (β2SLS,i) with\nan error term that scales with the off-diagonal terms\nin ΣXX. The proof is presented in Appendix C.\nTo give additional context for this result, let us\nconsider the behavior of a different representation\nlearning algorithm: PCA. It is well-known that\nPCA can recover the latent variables as long as\n5135\nthe ΣZZ dominates the covariance Cov(X). We\nstate this result in terms ofXPCA, the observed data\nprojected to the ﬁrst kcomponents of PCA.\nProposition 2. Let λk be the kth eigenvalue of\nAΣZZA⊤and λXX,k+1 be the k+1th eigenvalue\nof ΣXX and V be the ﬁrst keigenvectors of Cov(X).\nAssuming λk >λXX,k+1, we have\nEX ∥AZ −XPCA∥2 ≤\n√\n2 ∥ΣXX∥op\nλk −λXX,k+1\n(∥AZ∥2+\n√\ntr(ΣXX))+\nAA⊤\n\nop\n√\ntr(ΣXX),\nwhere ∥·∥op is the operator norm and tr(·) is the\ntrace.\nThis shows that whenever ΣXX is sufﬁciently\nsmall and λk is large (i.e., the covariance is domi-\nnated by Z), then PCA recovers the latent informa-\ntion in Z. The proof is based on the Davis-Kahan\ntheorem (Stewart and Sun, 1990) and is presented\nin Appendix C.\nComparing the bound of PCA and masked re-\ngression, both bounds have errors that scales with\nΣXX, but the key difference in the error bound is\nthat the error term for masked regression does not\nscale with the per-coordinate noise ( diag(ΣXX))\nand thus can be thought of as focusing exclusively\non interactions within X. Analyzing this more\ncarefully, we ﬁnd that ΣXX,\\i,i corresponds to the\nstatistical dependencies between xi and X\\i, which\nwe might hope captures useful, task-agnostic struc-\ntures such as syntactic dependencies.\nMLM log-probabilies can recover direct de-\npendencies. Another effect of latent variables\nis that many tokens have indirect dependencies\nthrough the latent variables, which poses a chal-\nlenge to recovering the direct dependencies among\ntokens. We now show that the MLMs can account\nfor the effect of latent variable.\nIn the case where there are no latent variables,\nwe can identify the direct dependencies via con-\nditional MI (Anandkumar et al., 2012) because\nany xi and xj that are disconnected in the graph-\nical model will have zero conditional MI, i.e.,\nI(xi; xj|X\\{i,j}) = 0 . One valuable aspect of\nMLM is that we can identify direct dependencies\neven in the presence of latent variables.\nIf we naively measure statistical dependency by\nmutual information, the coordinates of X would\nappear dependent on each other because they are\nall connected with Z. However, the MLM objective\nresolves this issue by conditioning on X\\{i,j}. We\nshow that latent variables (such as topics) that are\neasy to predict from X\\{i,j}can be ignored when\nconsidering conditional MI.\nProposition 3. The gap between conditional MI\nwith and without latent variables is bounded by the\nconditional entropy H(Z|X\\{i,j}),\nI(xi; xj|X\\{i,j}) −I(xi; xj|Z,X\\{i,j})\n≤2H(Z|X\\{i,j}).\nThis suggests that when the context X\\{i,j}cap-\ntures enough of the latent information, conditional\nMI can remove the confounding effect of the shared\ntopic Z and extract the direct and sparse dependen-\ncies within X (see Appendix C for the proof).\nMLM objective encourages capturing condi-\ntonal MI.We have now shown that conditional MI\ncaptures direct dependencies among tokens, even in\nthe presence of latent variables. Next, we will show\nthat the MLM objective ensures that a LM with low\nlog-loss accurately captures the conditional MI. We\nnow show that learning the MLM objective implies\nhigh-quality estimation of conditional MI. Denote\nX(i,v) as substituting xi with a new token v,\nX(i,v) = ⟨x1,...,x i−1,v,x i+1,...,x L⟩.\nConditional MI is deﬁned as the expected pointwise\nmutual information (PMI) conditioned on the rest\nof the tokens,\nIp = E\nxi,xj\n[ log p(xi|X\\i(j,xj))−log E\nxj|xi\np(xi|X\\i(j,xj)) ]\nwhere Ip is the abbreviation of Ip(xi; xj|X\\{i,j}).\nOur main result is that the log-loss MLM objective\ndirectly bounds the gap between the true condi-\ntional mutual information from the data distribu-\ntion and an estimator that uses the log-probabilities\nfrom the model. More formally,\nProposition 4.Let\nˆIpθ = E\nxi,xj\n[log pθ(xi|X\\i(j,xj))−log E\nxj|xi\npθ(xi|X\\i(j,xj))]\nbe an estimator constructed by the model distribu-\ntion pθ. Then we can show,\n|ˆIpθ −Ip|≤ E\nxj\nDkl\n(\np(xi|X\\i(j,xj))||pθ(xi|X\\i(j,xj))\n)\n,\nwhere Dkl represents the KL-divergence.\nHere, the KL-divergence corresponds to the\nLMLM objective, up to a constant entropy term that\ndepends on p. We present the proof in Appendix C.\nIn other words, the MLM objective is implicitly\nencouraging the model to match its implied condi-\ntional MI to that of the data. We now use this result\nto create an estimator that extracts the conditional\nindependence structures implied by MLM.\n5136\n4.2 Extracting statistical dependencies\nimplied by MLMs\nOur earlier analysis in Proposition 4 suggests that\nan MLM with low loss has an accurate approxi-\nmation of conditional mutual information. Using\nthis result, we will now propose a procedure which\nestimates ˆIpθ. The deﬁnition of ˆIpθ shows that if\nwe can access samples of xi and xj from the true\ndistribution p, then we can directly estimate the\nconditional mutual information by using the log\nprobabilities from the MLM. Unfortunately, we\ncannot draw new samples of xj |X\\{i,j}, lead-\ning us to approximate this distribution using Gibbs\nsampling on the MLM distribution.\nOur Gibbs sampling procedure is similar to the\none proposed in Wang and Cho (2019). We start\nwith X0 = X\\{i,j}. For the tth iteration, we\ndraw a sample xt\ni from pθ(xi|Xt−1\n\\i ), and update\nby Xt = Xt−1(i,xt\ni). Then, we draw a sample\nxt\nj from pθ(xj|Xt\n\\j) and set Xt = Xt(j,xt\nj). We\nrepeat and use the samples (x1\ni,x1\nj),..., (xt\ni,xt\nj)\nto compute the expectations for conditional MI.\nThis procedure relies upon an additional assump-\ntion that samples drawn from the MLM are faithful\napproximations of the data generating distribution.\nHowever, we show empirically that even this ap-\nproximation is sufﬁcient to test the hypothesis that\nthe conditional independences learned by an MLM\ncapture syntactic dependencies (Section 5.2).\n5 Experiment\nWe now test two predictions from our analyses.\nFirst, similar to our observation in the case study,\nwe show that cloze-like masks do not explain\nthe success of uniform masks on three real-world\ndatasets. Second, our alternative view of relating\nMLM to graphical models suggests that statistical\ndependencies learned by MLMs may capture lin-\nguistic structures useful for downstream tasks. We\ndemonstrate this by showing that MLMs’ statistical\ndependencies reﬂect syntactic dependencies.\n5.1 Uniform vs Cloze-like Masking\nSetup. We now demonstrate that real-world tasks\nand MLMs show a gap between task-speciﬁc cloze\nmasks and random masks. We compare the MLM\nwith random masking to two different control\ngroups. In the positive control ( CLOZE ), we pre-\ntrain with only cloze-like masks and in the negative\ncontrol (NOCLOZE ), we pretrain by explicitly ex-\ncluding cloze-like masks. If the success of MLM\ncan be mostly explained by implicit cloze reduc-\ntions, then we should expect CLOZE to have strong\ndownstream performance while NOCLOZE leads\nto a minimal performance gain. We compare pre-\ntraining with the uniform masking strategy used in\nBERT (UNIFORM ) to these two control groups. If\nUNIFORM performs worse than the positive con-\ntrol and more similar to the negative control, then\nwe know that uniform masking does not leverage\ncloze-like masks effectively.\nSimulating Pretraining.Given computational\nconstraints, we cannot retrain BERT from scratch.\nInstead, we approximate the pretraining process by\ncontinuing to update BERT with MLM (Gururan-\ngan et al., 2020), which we refer to as second-stage\npretraining. Although this is an approximation to\nthe actual pretraining process, the second-stage pre-\ntraining shares the same fundamental problem for\npretraining: how can unsupervised training lead to\ndownstream performance gains?\nWe study the effectiveness of different masking\nstrategies by comparing to a BERT model without\nsecond-stage pretraining (VANILLA ). We experi-\nment with three text classiﬁcation datasets: SST-\n2 (Socher et al., 2013), Hyperpartisan (Kiesel et al.,\n2019), and AGNews (Zhang et al., 2015). SST-\n2 classiﬁes movie reviews by binary sentiment;\nHyperpartisan is a binary classiﬁcation task on\nwhether a news article takes an extreme partisan\nstandpoint; and AGNews classiﬁes news articles\ninto four different topics. On SST-2 and AGNews,\nwe perform the second-stage pretraining on the\ntraining inputs (not using the labels). On Hyper-\npartisan, we use 100k unlabeled news articles that\nare released with the dataset. For SST-2 and AG-\nNews, we study a low-resource setting and set the\nnumber of ﬁnetuning examples to be 20. For Hy-\nperpartisan, we use the training set, which has 515\nlabeled examples. All evaluations are performed\nby ﬁne-tuning a bert-base-uncased model\n(See Appendix A for full details).\nApproximating Cloze-like Masking.We can-\nnot identify the optimal set of cloze-like masks\nfor an arbitrary downstream task, but these three\ntasks have associated lexicons which we can use\nto approximate the cloze-like masks. For SST-2,\nwe take the sentiment lexicon selected by Hu and\nLiu (2004); for Hyperpartisan, we take the NRC\nword-emotion association lexicon (Mohammad and\nTurney, 2013); and for AGNews, we extract topic\nwords by training a logistic regression classiﬁer and\n5137\nVanilla No Cloze Uniform Cloze0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Validation Acc.\n60.18%\n66.72%\n69.03%\n74.50%\nSST-2 20-shot\nVanilla No Cloze Uniform Cloze0.80\n0.81\n0.82\n0.83\n0.84\n0.85Test Acc.\n82.15%\n82.78%\n83.25%\n84.06%\nHyperpartisan\nVanilla No Cloze Uniform Cloze0.70\n0.75\n0.80\n0.85\n0.90Test Acc.74.35%\n79.28%\n78.22%\n80.17%\nAGNews 20-shot\nFigure 5: Finetuning performance with different masking strategies averaged across twenty random trials and error\nbars showing 95% conﬁdence intervals. VANILLA represents a BERT model without any second-stage pretraining.\nCLOZE and NOCLOZE represent models train with or without cloze-like masks, respectively. U NIFORM uses the\nuniform random masking strategy proposed in Devlin et al. (2019) for second-stage pretraining.\ntaking the top 1k features to be cloze-like masks.\nResults. Figure 5 plots the ﬁnetuning perfor-\nmance of different masking strategies. We observe\nthat UNIFORM outperforms VANILLA , which in-\ndicates that second-stage pretraining is extracting\nuseful information and our experiment setup is use-\nful for studying how MLM leads to performance\ngains. As expected, CLOZE achieves the best accu-\nracy, which conﬁrms that cloze-like masks can be\nhelpful and validates our cloze approximations.\nThe UNIFORM mask is much closer to NO-\nCLOZE than CLOZE . This suggests that uniform\nmasking does not leverage cloze-like masks well\nand cloze reductions alone cannot account for the\nsuccess of MLM. This view is further supported\nby the observation that NOCLOZE outperforms\nVANILLA suggesting that generic masks that are\nnot cloze-like still contain useful inductive biases.\nOur results support our earlier view that there\nmay be an alternative mechanism that allows\ngeneric masks that are not cloze-like to beneﬁt\ndownstream learning. Next, we will empirically\nexamine BERT’s learned conditional independence\nstructure among tokens and show that the statistical\ndependencies relate to syntactic dependencies.\n5.2 Analysis: Unsupervised Parsing\nOur analysis in section 4.1 shows that conditional\nMI (which is optimized by the MLM objective) can\nextract conditional independences. We will show\nthat statistical dependencies estimated by condi-\ntional MI are related to syntactic dependencies by\nusing conditional MI for unsupervised parsing.\nBackground. One might expect that the sta-\ntistical dependencies among words are correlated\nwith syntactic dependencies. Indeed, Futrell et al.\n(2019) show that heads and dependents in depen-\ndency parse trees have high pointwise mutual in-\nformation (PMI) on average. However, previous at-\ntempts (Carroll and Charniak, 1992; Paskin, 2002)\nshow that unsupervised parsing approaches based\non PMI achieve close to random accuracy. Our\nanalysis suggests that MLMs extract a more ﬁne-\ngrained notion of statistical dependence (condi-\ntional MI) which does not suffer from the exis-\ntence of latent variables (Proposition 3). We now\nshow that the conditional MI captured by MLMs\nachieves far better performance, on par with classic\nunsupervised parsing baselines.\nBaselines. We compare conditional MI to PMI\nas well as conditional PMI, an ablation in which\nwe do not take expectation over possible words.\nFor all statistical dependency based methods (cond.\nMI, PMI, and cond. PMI), we compute pairwise\ndependence for each word pair in a sentence and\nconstruct a minimum spanning tree on the negative\nvalues to generate parse trees. To contextualize our\nresults, we compare against three simple baselines:\nRANDOM which draws a random tree on the in-\nput sentence, LINEAR CHAIN which links adjacent\nwords in a sentence, and a classic unsupervised\nparsing method (Klein and Manning, 2004).\nExperimental Setup.We conduct experiments\non the English Penn Treebank using the WSJ cor-\npus and convert the annotated constituency parses\nto Stanford Dependency Formalism (de Marneffe\net al., 2006). Following Yang et al. (2020), we\nevaluate on sentences of length ≤10 in the test\nsplit, which contains 389 sentences (Appendix B.1\ndescribes the same experiment on longer sentences,\nwhich have similar results). We experiment with\nthe bert-base-cased model (more details in\nAppendix A) and evaluate by the undirected unla-\nbeled attachment score (UUAS).\nResults. Table 1 shows a much stronger-than-\nrandom association between conditional MI and\ndependency grammar. In fact, the parses extracted\n5138\nThe above represents a triumph of either apathy or civility .\nnsubjdet preconjprep\npobj\ndet cc\ndobj conj\ndet nsubj preconj\nwrong\nprep wrongdet\ndobj conj\n1\nFigure 6: An example parse extracted from conditional MI. The black parse tree above the sentence represents\nthe ground-truth parse and the red parse below is extracted from conditional MI. The correctly predicted edges are\nlabeled with the annotated relations, and the incorrect ones are labeled as wrong.\nMethod UUAS\nRANDOM 28.50 ±0.73\nLINEAR CHAIN 54.13\nKlein and Manning (2004) 55.91 ±0.68\nPMI 33.94\nCONDITIONAL PMI 52.44 ±0.19\nCONDITIONAL MI 58.74 ±0.22\nTable 1: Unlabeled Undirected Attachment Score on\nWSJ10 test split (section 23). Error bars show standard\ndeviation across three random seeds.\nfrom conditional MI has better quality than LIN-\nEAR CHAIN and the classic method (Klein and Man-\nning, 2004). Unlike conditional MI, PMI only has a\nclose-to-random performance, which is consistent\nwith prior work. We also see that conditional MI\noutperforms conditional PMI, which is consistent\nwith our theoretical framework that suggests that\nconditional MI (and not PMI) recovers the graphi-\ncal model structure.\nWe also perform a ﬁne-grained analysis by inves-\ntigating relations where conditional MI differs from\nLINEAR CHAIN . Because the test split is small\nand conditional MI does not involve any training,\nwe perform this analysis on 5,000 sentences from\nthe training split. Table 2 presents the results and\nshows that conditional MI does not simply recover\nthe linear chain bias. Meanwhile, we also observe a\ndeviation between conditional MI and dependency\ngrammar on relations likenumber and cc. This is\nreasonable because certain aspects of dependency\ngrammar depend on human conventions that do not\nnecessarily have a consensus (Popel et al., 2013).\nFigure 6 illustrates with an example parse ex-\ntracted from conditional MI. We observe that con-\nditional MI correctly captures dobj and conj.\nKnowing the verb, e.g. represents, limits the range\nof objects that can appear in a sentence so intu-\nitively we expect a high conditional MI between\nthe direct object and the verb. Similarly for phrases\nlike “A and B”, we would expect A and B to be sta-\ntistically dependent. However, conditional MI fails\nRelation Conditional MI Linear Chain\nxcomp 48.18 9.93\nconj 43.36 7.58\ndobj 58.96 30.33\nnumber 50.55 92.62\nquantmod 56.82 72.73\ncc 31.39 41.10\nTable 2: Six relations on which conditional MI dis-\nagrees with L INEAR CHAIN under log odds ratio test\nwith p= 0.05. A comprehensive list is in Appendix A.\nto capture cc (between apathy and or). Instead,\nit links or with either which certainly has statisti-\ncal dependence. This once again suggests that the\n‘errors’ incurred by the conditional PMI method\nare not simply failures to estimate dependence but\nnatural differences in the deﬁnition of dependence.\n6 Discussion and Conclusion\nWe study how MLM with uniform masking can\nlearn useful linguistic structures and inductive bi-\nases for downstream tasks. Our work demonstrates\nthat a substantial part of the performance gains\nof MLM pretraining cannot be attributed to task-\nspeciﬁc, cloze-like masks. Instead, learning with\ntask-agnostic, generic masks encourages the model\nto capture direct statistical dependencies among\ntokens, and we show through unsupervised parsing\nevaluations that this has a close correspondence to\nsyntactic structures. Existing work has suggested\nthat statistical and syntactic dependencies are fun-\ndamentally different, with unsupervised parsing\nbased on PMI achieving close-to-random perfor-\nmance. Our work demonstrates that this is not nec-\nessarily the case, and better measures of statistical\ndependence (such as those learned by MLMs) can\nserve as implicit supervision for learning syntactic\nstructures. Our ﬁndings open new space for future\nworks on how syntax can be learned in an emergent\nway and on how to design masking strategies that\nfurther improve dependency learning.\n5139\nAcknowledgement\nWe thank Rishi Bommasani, Lisa Li, Kawin Etha-\nyarajh, the anonymous reviewers and the Stanford\nNLP group for their helpful feedback and discus-\nsions.\nReferences\nA. Anandkumar, V . Y . F. Tan, F. Huang, and A. S. Will-\nsky. 2012. High-dimensional structure estimation in\nising models: Local separation criterion. Annals of\nStatistics, 40(3):1346–1375.\nS. Arora, Y . Li, Y . Liang, T. Ma, and A. Risteski. 2015.\nRandom walks on context spaces: Towards an ex-\nplanation of the mysteries of semantic word embed-\ndings. arXiv preprint arXiv:1502.03520.\nY . Belinkov and J. Glass. 2019. Analysis methods\nin neural language processing: A survey. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL), 7:49–72.\nD. Blei, A. Ng, and M. I. Jordan. 2003. Latent Dirichlet\nallocation. Journal of Machine Learning Research\n(JMLR), 3:993–1022.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah,\nJ. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss,\nG. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,\nE. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei. 2020. Language models are few-\nshot learners. arXiv preprint arXiv:2005.14165.\nG. Carroll and E. Charniak. 1992. Two experiments on\nlearning probabilistic dependency grammars from\ncorpora. In AAAI Conference on Artiﬁcial Intelli-\ngence.\nM. de Marneffe, B. MacCartney, and C. D. Manning.\n2006. Generating typed dependency parses from\nphrase structure parses. In LREC.\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019.\nBERT: Pre-training of deep bidirectional transform-\ners for language understanding. In Association\nfor Computational Linguistics (ACL) , pages 4171–\n4186.\nR. Futrell, P. Qian, E. Gibson, E. Fedorenko, and\nI. Blank. 2019. Syntactic dependencies correspond\nto word pairs with high mutual information. In Pro-\nceedings of the Fifth International Conference on\nDependency Linguistics (Depling, SyntaxFest 2019),\npages 3–13, Paris, France. Association for Computa-\ntional Linguistics.\nS. Gururangan, A. Marasovi´c, S. Swayamdipta, K. Lo,\nI. Beltagy, D. Downey, and N. A. Smith. 2020.\nDon’t stop pretraining: Adapt language models to\ndomains and tasks. In Association for Computa-\ntional Linguistics (ACL), pages 8342–8360, Online.\nAssociation for Computational Linguistics.\nT. B. Hashimoto, D. Alvarez-Melis, and T. S. Jaakkola.\n2016. Word embeddings as metric recovery in se-\nmantic spaces. Transactions of the Association for\nComputational Linguistics (TACL), 4:273–286.\nJ. He, G. Neubig, and T. Berg-Kirkpatrick. 2018. Un-\nsupervised learning of syntactic structure with in-\nvertible neural projections. In Empirical Methods\nin Natural Language Processing, pages 1292–1302,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJ. Hewitt and C.D. Manning. 2019. A structural probe\nfor ﬁnding syntax in word representations. In North\nAmerican Association for Computational Linguis-\ntics (NAACL), pages 4129–4138, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nM. Hu and B. Liu. 2004. Mining and summariz-\ning customer reviews. In International Conference\non Knowledge Discovery and Data Mining (KDD) ,\nKDD ’04, page 168–177, New York, NY , USA. As-\nsociation for Computing Machinery.\nZ. Jiang, F. F. Xu, J. Araki, and G. Neubig. 2020. How\ncan we know what language models know? Trans-\nactions of the Association for Computational Lin-\nguistics (TACL), 8:423–438.\nJ. Kiesel, M. Mestre, R. Shukla, E. Vincent, P. Adineh,\nD. Corney, B. Stein, and M. Potthast. 2019.\nSemEval-2019 task 4: Hyperpartisan news detection.\nIn Proceedings of the 13th International Workshop\non Semantic Evaluation, pages 829–839, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nD. Kingma and J. Ba. 2014. Adam: A method\nfor stochastic optimization. arXiv preprint\narXiv:1412.6980.\nD. Klein and C.D. Manning. 2004. Corpus-based\ninduction of syntactic structure: Models of de-\npendency and constituency. In Association for\nComputational Linguistics (ACL) , pages 478–485,\nBarcelona, Spain.\nJ. D. Lee, Q. Lei, N. Saunshi, and J. Zhuo. 2020.\nPredicting what you already know helps: Prov-\nable self-supervised learning. arXiv preprint\narXiv:2008.01064.\nO. Levy and Y . Goldberg. 2014. Neural word embed-\nding as implicit matrix factorization. In Advances in\nNeural Information Processing Systems, volume 27,\npages 2177–2185. Curran Associates, Inc.\nN. F. Liu, M. Gardner, Y . Belinkov, M. E. Peters, and\nN. A. Smith. 2019a. Linguistic knowledge and trans-\nferability of contextual representations. In North\nAmerican Association for Computational Linguis-\ntics (NAACL), pages 1073–1094, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\n5140\nY . Liu. 2019. Fine-tune BERT for extractive summa-\nrization. arXiv preprint arXiv:1903.10318.\nY . Liu, M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV . Stoyanov. 2019b. RoBERTa: A robustly opti-\nmized BERT pretraining approach. arXiv preprint\narXiv:1907.11692.\nC. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.\nBethard, and D. McClosky. 2014. The stanford\ncoreNLP natural language processing toolkit. In\nACL system demonstrations.\nN. Meinshausen and P. Bühlmann. 2006. High-\ndimensional graphs and variable selection with the\nlasso. Annals of Statistics, 34(3):1436–1462.\nT. Mikolov, K. Chen, G. Corrado, and Jeffrey. 2013.\nEfﬁcient estimation of word representations in vec-\ntor space. arXiv preprint arXiv:1301.3781.\nS. M. Mohammad and P. D. Turney. 2013. Crowdsourc-\ning a word-emotion association lexicon. Computa-\ntional Intelligence, 29(3):436–465.\nB. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs\nup? sentiment classiﬁcation using machine learn-\ning techniques. In Empirical Methods in Natural\nLanguage Processing, pages 79–86. Association for\nComputational Linguistics.\nM. A. Paskin. 2002. Grammatical bigrams. In Ad-\nvances in Neural Information Processing Systems\n(NeurIPS).\nJ. Pennington, R. Socher, and C. D. Manning. 2014.\nGloVe: Global vectors for word representation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1532–1543.\nM. Peters, M. Neumann, L. Zettlemoyer, and W. Yih.\n2018. Dissecting contextual word embeddings: Ar-\nchitecture and representation. In Empirical Methods\nin Natural Language Processing, pages 1499–1509,\nBrussels, Belgium. Association for Computational\nLinguistics.\nF. Petroni, T. Rocktäschel, S. Riedel, P. Lewis,\nA. Bakhtin, Y . Wu, and A. Miller. 2019. Language\nmodels as knowledge bases? In Empirical Methods\nin Natural Language Processing and International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2463–2473, Hong Kong,\nChina. Association for Computational Linguistics.\nM. Popel, D. Mare ˇcek, J. Št ˇepánek, D. Zeman, and\nZ. Žabokrtský. 2013. Coordination structures in de-\npendency treebanks. In Association for Computa-\ntional Linguistics (ACL), pages 517–527, Soﬁa, Bul-\ngaria. Association for Computational Linguistics.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsuper-\nvised multitask learners. OpenAI Blog, 1(8).\nN. Saunshi, S. Malladi, and S. Arora. 2020. A\nmathematical exploration of why language mod-\nels help solve downstream tasks. arXiv preprint\narXiv:2010.03648.\nR. Socher, A. Perelygin, J. Y . Wu, J. Chuang, C. D.\nManning, A. Y . Ng, and C. Potts. 2013. Recursive\ndeep models for semantic compositionality over a\nsentiment treebank. In Empirical Methods in Nat-\nural Language Processing (EMNLP).\nG. W. Stewart and J. Sun. 1990. Matrix Perturbation\nTheory. Academic Press.\nAlex Tamkin, Trisha Singh, Davide Giovanardi, and\nNoah Goodman. 2020. Investigating transferability\nin pretrained language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1393–1401. Association for Computa-\ntional Linguistics.\nI. Tenney, P. Xia, B. Chen, A. Wang, A. Poliak, R. T.\nMcCoy, N. Kim, B. Van Durme, S. Bowman, D. Das,\nand E. Pavlick. 2019. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In International Confer-\nence on Learning Representations (ICLR).\nR. Tibshirani. 1996. Regression shrinkage and selec-\ntion via the lasso. Journal of the Royal Statistical\nSociety: Series B (Methodological), 58(1):267–288.\nD. Wadden, U Wennberg, Y . Luan, and H. Hajishirzi.\n2019. Entity, relation, and event extraction with\ncontextualized span representations. In Empirical\nMethods in Natural Language Processing and Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5784–5789, Hong\nKong, China. Association for Computational Lin-\nguistics.\nA. Wang and K. Cho. 2019. BERT has a mouth, and\nit must speak: BERT as a Markov random ﬁeld lan-\nguage model. arXiv preprint arXiv:1902.04094.\nZ. Wu, Y . Chen, B. Kao, and Q. Liu. 2020. Perturbed\nmasking: Parameter-free probing for analyzing and\ninterpreting BERT. In Association for Computa-\ntional Linguistics (ACL), pages 4166–4176, Online.\nAssociation for Computational Linguistics.\nS. Yang, Y . Jiang, W. Han, and K. Tu. 2020.\nSecond-order unsupervised neural dependency pars-\ning. arXiv preprint arXiv:2010.14720.\nT. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and\nY . Artzi. 2020. Revisiting few-sample bert ﬁne-\ntuning. arXiv preprint arXiv:2006.05987.\nX. Zhang, J. Zhao, and Y . LeCun. 2015. Character-\nlevel convolutional networks for text classiﬁcation.\nIn Advances in Neural Information Processing Sys-\ntems, volume 28, pages 649–657. Curran Associates,\nInc.\n5141\nJ. Zhu, Y . Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li,\nand T. Liu. 2020. Incorporating BERT into neural\nmachine translation. In International Conference on\nLearning Representations (ICLR).\n5142\nDataset # Classes # Pretrain # Finetune # Test\nSST-2 2 67k 20 1.8k\nHyperpartisan 2 100k 515 130\nAGNews 4 113k 20 6.7k\nTable 3: Speciﬁcations of datasets. For AGNews, we put away 6.7k as a development set.\nA Experimental Details\nExperimental details for Section 3.2Our transformers have 2 layers and for each transformer block, the\nhidden size and the intermediate size are both 64. We ﬁnetune the models for 10 epochs and apply early\nstopping based on validation accuracy. We use Adam (Kingma and Ba, 2014) for optimization, using a\nlearning rate of 1e−3 for pretraining and 1e−4 for ﬁnetuning.\nExperimental details for Section 5.1Table 3 summarizes the dataset statistics of three real-world\ndatasets we studied. For second stage pretraining, we update the BERT model for 10 epochs. Following\nthe suggestion in Zhang et al. (2020), we ﬁnetune the pretrained BERT models for 400 steps, using a\nbatch size of 16 and a learning rate of 1e−5. We apply linear learning rate warmup for the ﬁrst 10% of\nﬁnetuning and linear learning rate decay for the rest. For SST-2 and AGNews, we average the results over\n20 random trials. For Hyperpartisan, because the test set is small and the variation is larger, we average\nthe results over 50 random trials and evaluate on the union the development set and the test set for more\nstable results.\nExperimental details for Section 5.2We convert the annotated constituency parses using the Stanford\nCoreNLP package (Manning et al., 2014). We compute conditional MI and conditional PMI using the\nbert-base-cased model and run Gibbs sampling for 2000 steps. BERT’s tokenization may split\na word into multiple word pieces. We aggregate the dependencies between a word and multiple word\npieces by taking the maximum value. We compute the PMI statistics and train the K&M model (Klein\nand Manning, 2004) on sentences of length ≤10 in the WSJ train split (section 2-21). For DMV , we train\nwith the annotated POS tags using a public implementation released by (He et al., 2018). Results are\naveraged over three runs when applicable.\nB Additional Results\nB.1 Additional Results in Section 5.2\nWe conduct an additional experiment on the English Penn Treebank to verify that conditional MI can\nextract parses for sentences longer than ten words. To expedite experimentation, we subsample 200 out\nof 2416 sentences from the test split of English Penn Treebank and the average sentence length of our\nsubsampled dataset is 24.1 words. When applicable, we average over three random seeds and report\nstandard deviations. Table 4 presents the UUAS of conditional MI and other methods. We draw similar\nconclusions as in Section 5.2, observing that the parses drawn by conditional MI have higher quality than\nthose of other baselines.\nTable 5 presents a comprehensive list of relations on which Conditional MI disagrees with LIN-\nEAR CHINA under a log odds ratio test with p= 0.05.\n5143\nMethod UUAS\nRANDOM 9.14 ±0.42\nLINEAR CHAIN 47.69\nKlein and Manning (2004) 48.76 ±0.24\nPMI 28.05\nCONDITIONAL PMI 44.75 ±0.09\nCONDITIONAL MI 50.62 ±0.38\nTable 4: Unlabeled Undirected Attachment Score on subsampled WSJ test split (section 23). Error bars show\nstandard deviation across three random seeds.\nRelation Conditional MI Linear Chain\nxcomp 48.18 9.93\nconj 43.36 7.58\nnsubjpass 33.81 0.47\ndobj 58.96 30.33\nmark 30.71 9.45\nposs 58.63 40.96\nccomp 20.92 4.18\nvmod 55.32 41.84\ntmod 39.25 27.68\ndep 50.15 40.03\npobj 48.68 40.79\nnsub 55.87 48.69\nnumber 50.55 92.62\npossessive 72.00 97.78\npcomp 60.00 77.00\nquantmod 56.82 72.73\nappos 55.56 70.59\nnum 65.11 76.49\ncc 31.39 41.10\nprep 56.41 66.12\nauxpass 75.00 83.26\nnn 72.97 77.88\naux 55.49 59.66\nTable 5: All relations on which Conditional MI disagree with L INEAR CHINA under a log odds ratio test with\np= 0.05.\n5144\nC Proofs\nProof of Proposition 2We ﬁrst recall our statement.\nProposition 2.Let λk be the kth eigenvalue of AΣZZA⊤and λXX,k+1 be the k+1th eigenvalue of ΣXX\nand V be the ﬁrst keigenvectors of Cov(X). Assuming λk >λXX,k+1, we have\nEX ∥AZ −XPCA∥2 ≤\n√\n2 ∥ΣXX∥op\nλk −λXX,k+1\n(∥AZ∥2 +\n√\ntr(ΣXX)) +\nAA⊤\n\nop\n√\ntr(ΣXX),\nwhere ∥·∥op is the operator norm and tr(·) is the trace.\nProof\nWe will use the Davis-Kahan Theorem for our proof.\nTheorem (Davis-Kahan (Stewart and Sun, 1990)). Let σbe the eigengap between the kth and the k+1th\neigenvalue of two positive semideﬁnite symmetric matrices Σ and Σ′. Also, let V and V′be the ﬁrst k\neigenvectors of Σ and Σ′respectively. Then we have,\n1√\n2\nVV ⊤−V′V′⊤\n\nop\n≤\n∥Σ −Σ∥op\nσ .\nThat is, we can bound the error in the subspace projection in terms of the matrix perturbation.\nIn our setting, we choose Σ = AΣZZA⊤+ ΣXX and Σ′= AΣZZA⊤. We know the eigengap of Σ′\nis λk because Σ′only has knonzero eigenvalues. By Weyl’s inequality, thekth eigenvalue is at most\nperturbed by λXX,k+1, which is the k+1 eigenvalue of ΣXX. Let V be the top keigenvectors of Σ′and\nassuming λk >λXX,k+1, we have,\n1√\n2\nAA⊤−VV ⊤\n\nop\n≤\n∥Σ −Σ′∥op\nλk −λXX,k+1\n=\n∥ΣXX∥op\nλk −λXX,k+1\n.\nTurning this operator norm bound into approximation bound, we have\nEX ∥AZ −XPCA∥2 =EX\nAA⊤AZ −VV ⊤X\n\n2\n=EX\nAA⊤AZ −VV ⊤AZ + VV ⊤AZ −VV ⊤X\n\n2\n≤EX\nAA⊤AZ −VV ⊤AZ\n\n2\n+\nVV ⊤AZ −VV ⊤X\n\n2\n≤EX\nAA⊤AZ −VV ⊤AZ\n\n2\n+\nVV ⊤(AZ −X)\n\n2\n≤EX\nAA⊤−VV ⊤\n\nop\n·∥AZ∥2 +\nVV ⊤\n\nop\n∥AZ −X∥2 .\n=EX\nAA⊤−VV ⊤\n\nop\n·∥AZ∥2 +\nAA⊤+ VV ⊤−AA\n\nop\n∥AZ −X∥2\n≤EX\nAA⊤−VV ⊤\n\nop\n·∥AZ∥2 + (\nAA⊤\n\nop\n+\nVV ⊤−AA\n\nop\n) ∥AZ −X∥2\n=EX\nAA⊤−VV ⊤\n\nop\n·(∥AZ∥2 + ∥AZ −X∥2) +\nAA⊤\n\nop\n∥AZ −X∥2 .\nWe use the fact that EX,Z ∥AZ −X∥2\n2 = tr(ΣXX) and Jensen’s inequality to bound,\nEX ∥AZ −X∥2 ≤\n√\ntr(ΣXX).\n5145\nCombining these inequalities, we have\nEX ∥AZ −XPCA∥2\n≤\n√\n2 ∥ΣXX∥op\nλk −λXX,k+1\n·(∥AZ∥2 +\n√\ntr(ΣXX)) +\nAA⊤\n\nop\n√\ntr(ΣXX)\nProof of Proposition 1We ﬁrst recall our statement.\nProposition 1.Assuming that ΣXX is full rank,\nxmask,i = β2SLS,iX\\i + O(\nΣXX,\\i,i\n\n2),\nProof Let A\\i ∈Rd−1×k be the matrix where we omit the ith row of Aand Ai ∈Rk be the ith row\nof A. Let ΣXX,\\i,\\i ∈Rd−1×d−1 be the matrix where we omit the ith row and ith column of ΣXX,\nand ΣXX,\\i,i ∈Rd−1 be the vector formed by dropping the ith row and taking the ith column of ΣXX.\nSimilarly, denote X\\i ∈Rd−1 be the vector where we omit the icoordinate of X.\nWe start by writing down the expression ofβ2SLS,i. Recall that the Least Squares regression between\ntwo zero-mean Gaussian variables X and Y can be written as\nβ = Cov(X,Y)Cov(X,X)−1,\nwhere Cov(X,X) is the covariance matrix of X and we assume it is full rank. Since Cov(X\\i,Z) is\nA\\iΣZZ, we can write the coefﬁcient of regression from X\\i to Z as\nβX\\i→Z = ΣZZA⊤\n\\i(A\\iΣZZA⊤\n\\i + ΣXX,\\i,\\i)−1\nand by assumption we have βZ→xi = Ai. So we can write down\nβ2SLS,i = AiΣZZA⊤\n\\i(A\\iΣZZA⊤\n\\i + ΣXX,\\i,\\i)−1.\nNow we consider masked regression for the ith coordinate, xi,\nβX\\i→xi = (AiΣZZA⊤\n\\i + ΣXX,\\i,i)(A\\iΣZZA⊤\n\\i + ΣXX,\\i,\\i)−1.\nComparing β2SLS and βX\\i→xi, we observe that the second term is the same and the key is to bound\nthe ﬁrst term. Consider the error term between the coefﬁcients,\nΣXX,\\i,i(A\\iΣZZA⊤\n\\i + ΣXX,\\i,\\i)−1\n\n2\n≤\nΣXX,\\i,i\n\n2\n(A\\iΣZZA⊤\n\\i + ΣXX,\\i,\\i)−1\n\nop\n≤\nΣXX,\\i,i\n\n2\n(AΣZZA⊤+ ΣXX)−1\n\nop\n.\nThat is, the error term scales with the off-diagonal terms\nΣXX,\\i,i\n\n2.\nConverting our bound on the error term into an approximation bound, we have\nxmask,i = β2SLS,iX + O(\nΣXX,\\i,i\n\n2).\n5146\nProof for Proposition 3.\nProposition 3. The gap between conditional MI with and without latent variables is bounded by the\nconditional entropy H(Z|X\\{i,j}),\nI(xi; xj|X\\{i,j}) −I(xi; xj|Z,X\\{i,j})\n≤2H(Z|X\\{i,j}).\nProof The proof follows from the deﬁnition of conditional mutual information. Denote H(·) as the\nentropy function.\nWe start by observing that\nI(xi; xj|Z,X\\{i,j}) = I(xi; xj|X\\{i,j}) −I(xi; Z|X\\{i,j}) + I(xi; Z|xj,X\\{i,j})\n(Through chain rule of mutual information.)\n= I(xi; xj|X\\{i,j}) + H(Z|xi,X\\{i,j}) −H(Z|X\\{i,j})\n+ H(Z|xj,X\\{i,j}) −H(Z|xi,xj,X\\{i,j}).\nThen we have,\nI(xi; xj|X\\{i,j}) −I(xi; xj|Z,X\\{i,j})\n= −H(Z|xi,X\\{i,j}) + H(Z|X\\{i,j}) −H(Z|xj,X\\{i,j}) + H(Z|xi,xj,X\\{i,j})\n≤H(Z|X\\{i,j}) + H(Z|xi,xj,X\\{i,j})\n≤2 ·H(Z|X\\{i,j}).\nProposition 4.Let\nˆIpθ = E\nxi,xj\n[log pθ(xi|X\\i(j,xj)) −log E\nxj|xi\npθ(xi|X\\i(j,xj))]\nbe an estimator constructed by the model distribution pθ. Then we can show,\n|ˆIpθ −Ip|≤ E\nxj\nDkl\n(\np(xi|X\\i(j,xj))||pθ(xi|X\\i(j,xj))\n)\n,\nwhere Dkl represents the KL-divergence.\nProof Expanding the deﬁnition of mutual information, we write\nI(xi; xj|X\\{i,j}) −ˆIθ(xi; xj|X\\{i,j}) = Exj[Dkl\n(\np(xi|xj,X\\{i,j})||pθ(xi|xj,X\\{i,j})\n)\n]−\nDkl\n(\nExjp(xi|xj,X\\{i,j})||Exjpθ(xi|xj,X\\{i,j})\n)\n.\nDropping the the second term, we have\nˆIθ(xi; xj|X\\{i,j}) −I(xi; xj|X\\{i,j}) ≥−Exj[Dkl\n(\np(xi|xj,X\\{i,j})||pθ(xi|xj,X\\{i,j})\n)\n].\nDropping the the ﬁrst term, we have\nI(xi; xj|X\\{i,j}) −ˆIθ(xi; xj|X\\{i,j})\n≤Dkl\n(\nExjp(xi|xj,X\\{i,j})||Exjpθ(xi|xj,X\\{i,j})\n)\n≤ExjDkl\n(\np(xi|xj,X\\{i,j})||pθ(xi|xj,X\\{i,j})\n)\n,\nwhich uses the convexity of KL-divergence and Jensen’s inequality.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7961820363998413
    },
    {
      "name": "Natural language processing",
      "score": 0.6999112367630005
    },
    {
      "name": "Parsing",
      "score": 0.6998261213302612
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6755515933036804
    },
    {
      "name": "Masking (illustration)",
      "score": 0.548560619354248
    },
    {
      "name": "Lexicon",
      "score": 0.5124011635780334
    },
    {
      "name": "Inductive bias",
      "score": 0.4840644896030426
    },
    {
      "name": "ENCODE",
      "score": 0.4299767017364502
    },
    {
      "name": "Task (project management)",
      "score": 0.41342002153396606
    },
    {
      "name": "Language model",
      "score": 0.4101285934448242
    },
    {
      "name": "Multi-task learning",
      "score": 0.14952638745307922
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156583",
      "name": "Laboratoire d'Informatique de Paris-Nord",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 20
}