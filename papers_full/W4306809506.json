{
  "title": "Transformer-based models for multimodal irony detection",
  "url": "https://openalex.org/W4306809506",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2012385703",
      "name": "David Tomas",
      "affiliations": [
        "University of Alicante"
      ]
    },
    {
      "id": "https://openalex.org/A2164358464",
      "name": "Reynier Ortega Bueno",
      "affiliations": [
        "Universitat Politècnica de València"
      ]
    },
    {
      "id": "https://openalex.org/A2147874454",
      "name": "Guobiao Zhang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2118592779",
      "name": "Paolo Rosso",
      "affiliations": [
        "Universitat Politècnica de València"
      ]
    },
    {
      "id": "https://openalex.org/A2200389328",
      "name": "Rossano Schifanella",
      "affiliations": [
        "University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A2012385703",
      "name": "David Tomas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164358464",
      "name": "Reynier Ortega Bueno",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147874454",
      "name": "Guobiao Zhang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2118592779",
      "name": "Paolo Rosso",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2200389328",
      "name": "Rossano Schifanella",
      "affiliations": [
        "University of Turin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951937667",
    "https://openalex.org/W3100382072",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3086755561",
    "https://openalex.org/W6600195168",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2099813784",
    "https://openalex.org/W2252381721",
    "https://openalex.org/W6600617704",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W2163352848",
    "https://openalex.org/W3102848065",
    "https://openalex.org/W2489370933",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W3105203922",
    "https://openalex.org/W3035565303",
    "https://openalex.org/W3102065963",
    "https://openalex.org/W3115874212"
  ],
  "abstract": "Abstract Irony is nowadays a pervasive phenomenon in social networks. The multimodal functionalities of these platforms (i.e., the possibility to attach audio, video, and images to textual information) are increasingly leading their users to employ combinations of information in different formats to express their ironic thoughts. The present work focuses on the study of irony detection in social media posts involving image and text. To this end, a transformer architecture for the fusion of textual and image information is proposed. The model leverages disentangled text attention with visual transformers, improving F1-score up to 9% over previous existing works in the field and current state-of-the-art visio-linguistic transformers. The proposed architecture was evaluated in three different multimodal datasets gathered from Twitter and Tumblr. The results revealed that, in many situations, the text-only version of the architecture was able to capture the ironic nature of the message without using visual information. This phenomenon was further analysed, leading to the identification of linguistic patterns that could provide the context necessary for irony detection without the need for additional visual information.",
  "full_text": "Vol.:(0123456789)1 3\nJournal of Ambient Intelligence and Humanized Computing (2023) 14:7399–7410 \nhttps://doi.org/10.1007/s12652-022-04447-y\nORIGINAL RESEARCH\nTransformer‑based models for multimodal irony detection\nDavid Tomás1  · Reynier Ortega‑Bueno2 · Guobiao Zhang3 · Paolo Rosso2 · Rossano Schifanella4\nReceived: 3 March 2022 / Accepted: 4 October 2022 / Published online: 19 October 2022 \n© The Author(s) 2022\nAbstract\nIrony is nowadays a pervasive phenomenon in social networks. The multimodal functionalities of these platforms (i.e., the \npossibility to attach audio, video, and images to textual information) are increasingly leading their users to employ combi-\nnations of information in different formats to express their ironic thoughts. The present work focuses on the study of irony \ndetection in social media posts involving image and text. To this end, a transformer architecture for the fusion of textual \nand image information is proposed. The model leverages disentangled text attention with visual transformers, improving \nF1-score up to 9% over previous existing works in the field and current state-of-the-art visio-linguistic transformers. The \nproposed architecture was evaluated in three different multimodal datasets gathered from Twitter and Tumblr. The results \nrevealed that, in many situations, the text-only version of the architecture was able to capture the ironic nature of the message \nwithout using visual information. This phenomenon was further analysed, leading to the identification of linguistic patterns \nthat could provide the context necessary for irony detection without the need for additional visual information.\nKeywords Irony detection · Transformer · Multimodality · Image text fusion\n1 Introduction\nIrony implies the use of words that mean the opposite of \nwhat is really intended.1 It is a type of expression where the \nsurface sentiment differs from the implied sentiment. Irony \nis a pervasive phenomena in social media and resolving this \nchallenge is a necessary task for a wide range of applications \nsuch as sentiment analysis.\nMost approaches to automatic irony detection have tried \nto address this task as a text classification problem. These \napproaches rely only on textual information, but in many \ncircumstances the ironic intention also depends on contex-\ntual clues, such as images, audio or video. Although there \nis a large body of literature in the field of irony detection on \ntext, the number of studies in multimodal irony detection is \nstill quite limited.\nThis work addresses the problem of multimodal irony \ndetection in social networks, considering both textual infor-\nmation and the images associated to those texts. To this end, \na deep learning architecture based on textual and visual \ntransformers is proposed. These models have demonstrated \nto achieve state-of-the-art results in many natural language \nprocessing (NLP) tasks.\nBoth first (David Tomás) and second (Reynier Ortega-Bueno) \nauthors are contributed equally to this manuscript.\n * David Tomás \n dtomas@dlsi.ua.es\n Reynier Ortega-Bueno \n rortega@prhlt.upv.es\n Guobiao Zhang \n zgb0537@whu.edu.cn\n Paolo Rosso \n prosso@dsic.upv.es\n Rossano Schifanella \n rossano.schifanella@unito.it\n1 Department of Software and Computing Systems, University \nof Alicante, Alicante, Spain\n2 PRHLT Research Center, Universitat Politècnica de València, \nValencia, Spain\n3 School of Information Management, Wuhan University, \nWuhan, China\n4 Applied Research on Computational Complex Systems, \nUniversity of Turin, Turin, Italy\n1 Irony is used in this paper as an umbrella term for related phenom-\nena such as sarcasm.\n7400 D. Tomás et al.\n1 3\nThe proposed model has been tested with three different \ndatasets from the literature, outperforming previous state-of-\nthe-art systems in this field as well as current visio-linguistic \ntransformer models, which have demonstrated to achieve \nremarkable results in other multimodal tasks such as fake \nnews (Alam et al. 2021) and hate speech detection (Kiela \net al. 2021).\nThe main contributions of this paper are as follows:\n• textual and visual transformers are fused in a deep neu-\nral network architecture2 for the first time in the task of \nmultimodal irony detection;\n• the architecture proposed significantly outperforms previ-\nous approaches on the three datasets evaluated;\n• the proposed system is compared with a baseline that \nrelies on non-contextual word embeddings and Con-\nvolutional Neural Networks (CNN) for visual features, \nwith the aim of assessing the performance of transformer \nmodels with respect to these traditional approaches;\n• the experimental results also reveal that text-only models \nachieve high performance in this task disregarding visual \nfeatures. An in depth analysis shows the existence of tex-\ntual patterns that provide clues of the ironic nature of the \nposts without requiring visual context.\nThe rest of the paper is structured as follows: Sect.  2 \ndescribes related work in the field of multimodal irony \ndetection. Section  2.2 shows the main features of trans-\nformer models; Sect. 3 shows the architecture of the system \nproposed; Sect.  4 reports the evaluation and discusses the \nmain outcomes of the experiments; finally, conclusions and \nfuture work are shown in Sect. 5.\n2  Related work\nThis section summarises the related work in the field of mul-\ntimodal irony detection and provides an overview of textual, \nvisual, and multimodal transformer models.\n2.1  Multimodal irony detection\nAlthough there is a large body of papers in the field of irony \ndetection (Joshi et al. 2017), it is only recently that multi-\nmodal settings in this area have been addressed. This section \nreviews the work done in this scenario, focusing on fusions \nof text and image used to tackle this challenge.\nThe work by Schifanella et al. (2016) was the first attempt \nto create a corpus of text and images for multimodal irony \ndetection. They developed their own corpus on Twitter, Ins-\ntagram, and Tumblr, running a crowdsourcing task to quan-\ntify the extent to which images were perceived as necessary \nby human annotators to understand the sarcastic nature of \nthe posts. They proposed two different computational frame-\nworks integrating textual and visual modalities. The first one \nexploited visual semantics trained on an external dataset, \nconcatenating this semantic features with textual features. \nThe second method adapted a visual neural network initial-\nised with parameters trained on ImageNet to multimodal \nironic posts.\nCai et al. (2019) presented a corpus of Twitter posts and \nimages for multimodal irony detection. This corpus is freely \navailable to the research community and has been used as \na reference in other works, including the present paper (see \nSect.  4). The authors proposed a multimodal hierarchical \nfusion model using three modalities: text features, image fea-\ntures, and image attributes. The model first extracted image \nfeatures and attributes, and then leveraged these attributes \nand a bidirectional Long Short-Term Memory (LSTM) net-\nwork to extract text features. Features of the three modalities \nwere then reconstructed and fused into one feature vector \nfor prediction.\nPan et al. (2020) used the aforementioned Twitter corpus, \nproposing a contextual language model that concentrated on \nboth intra- and inter-modality incongruity for multimodal \nirony detection. They designed inter-modality attention to \ncapture incongruity. This attention mechanism was applied \nto identify the contradiction within text, which was used for \nthe final prediction.\nWang et al. (2020) also used the corpus developed by Cai \net al. (2019). They proposed an image-text model for irony \ndetection using the pretrained Bidirectional Encoder Rep-\nresentations from Transformers (BERT) and ResNet mod-\nels. The vector spaces of BERT and ResNet were connected \nusing the multi-head attention architecture of BERT. The \nlast stage of the model included a 2D-intra-attention layer to \nextract relationships between words and images.\nFinally, another work that leverages this corpus is Xu \net al. (2020). They proposed a method for modelling cross-\nmodality contrast in the associated context. Their architec-\nture is composed of two networks: one represents the com-\nmonality and discrepancy between image and text, whereas \nthe other models the semantic association in cross-modality \ncontext.\nIt can be observed that previous approaches in the area \nhad focused on developing architectures that mix represen-\ntation learning by CNN-based architectures with textual \ninformation. Although these systems have tried to capture \ncomplex cross-modality contrasts through attention mecha-\nnisms, none of them has proposed a full transformer-based \narchitecture. As far as the authors know, the system proposed 2 The source code of the two models proposed is available at https:// \ngithub. com/ reyni erort egabu eno86/ MMID/.\n7401Transformer-based models for multimodal irony detection  \n1 3\nhere is the first approach to multimodal irony detection that \nfully relies on transformed-based fusion.\n2.2  Transformer models\nTransformers are neural network architectures based on \nattention mechanisms, dispensing with the use of convolu-\ntional and recurrent networks. The original architecture con-\nsisted of a multi-head self-attention mechanism combined \nwith an encoder-decoder structure (Vaswani et al. 2017). \nThese models have achieved the state-of-the-art in many \nnatural language processing and computer vision tasks.\nIn the NLP field, the arrival of BERT supposed a break -\nthrough in language models (Devlin et al. 2019). This archi-\ntecture presents a bidirectional encoder that learns infor -\nmation from both left and right side of a word’s context \nduring the training phase. The model allows transfer learning \nin NLP tasks, i.e., the BERT model originally trained on \na dataset (the pre-trained model) can be used to perform \nsimilar tasks on another dataset (the fine-tuned model).\nDeBERTa (Decoding enhanced BERT with disentangled \nAttention) (He et al. 2020) is another recent language model, \nwhich extends BERT with two novel modifications. Firstly, \ninstead of using the self-attention mechanism, the model \nproposes a disentangled attention. In DeBERTa every token \nin the input is represented as two independent vectors that \nencode its word embedding and position. On this paired \nrepresentation, disentangled matrices are used to learn the \nattention weights among tokens. Secondly, an Enhanced \nMask Decoder (EMD) is applied to predict the masked \ntokens during the pre-training phase. Whereas BERT relies \non relative positions, the EMD allows DeBERTa to obtain \nmore accurate predictions, as the syntactic roles of the \nwords also depend heavily on their absolute positions in the \nsentence.\nIn the same vein, Nguyen et al. (2020) introduced BER -\nTweet, which was the first public large-scale pre-trained \nlanguage model for English posts obtained from Twitter. \nThis model, having the same architecture as BERT, was \ntrained using the RoBERTa (Liu et al. 2019) pre-training \nprocedure. Experimental results carried out on several NLP \ntasks in Twitter showed that BERTweet outperformed strong \nstate-of-the-art competitors such as RoBERTa and XLM-R \n(Conneau et al. 2020 ). In the sarcasm domain, this model \nachieved remarkable results on Task 3: Irony detection in \nEnglish Tweets proposed in the context of SemEval 2018 \n(Van Hee et al. 2018).\nIn the realm of computer vision, ViT (Vision Trans -\nformer) (Dosovitskiy et al. 2021) presented the first attempt \nto leverage transformer models to solve the problem of image \nclassification. ViT relied on the original transformer model \non a sequence of image patches. Instead of a 1D sequence of \nword embeddings, 2D image patches are flattened in a vector \nform and fed to the transformer as a sequence.\nRecently, different transformer models have emerged that \ncombine textual and visual features. One of them is Visu-\nalBERT (Li et al. 2019), which consists of a stack of trans-\nformer layers that implicitly align elements of an input text \nand regions in an associated input image with self-attention. \nThe experiments showed that this model could ground ele-\nments of language to image regions without any explicit \nsupervision.\nAlong the same lines, LXMERT (Tan and Bansal 2019) is \na large-scale transformer model that consists of three encod-\ners: an object relationship encoder, a language encoder, \nand a cross-modality encoder. The model was pre-trained \nwith large amounts of image and sentence pairs in different \ntasks: masked language modeling, masked object predic-\ntion, cross-modality matching, and image question answer -\ning. These tasks helped in learning both intra-modality and \ncross-modality relationships. After fine-tuning, the model \nachieved state-of-the-art results on two visual question \nanswering datasets.\n3  Architecture of the system\nThis section introduces the Dual Transformer for Multi-\nmodal Irony Detection (DT4MID) architecture proposed \nin this work. The system relies on a fusion of pretrained \nunimodal transformer models. An extension of this model \n(called EDT4MID) is also described, which addresses the \nproblem of multiple textual inputs. Such is the case of tex-\ntual content extracted from images (e.g. by means of OCR \ntechnologies) or tags from specific social media (such as \nTumblr) that complement the textual messages posted by \nusers.\nThe architecture of DT4MID and EDT4MID is depicted \nin Figs. 1 and 2, respectively. It is worth noting that the same \nunimodal transformer is used for encoding multiple textual \ninputs. The main reason to share parameters is to decrease \nthe number of weight updates during back-propagation, con-\nsequently reducing memory size and training time.\n3.1  Transformer‑based representation\nDue to the success of transformer models to fit and gener -\nalise well on subjective and complex tasks, the architecture \nproposed here relies on unimodal textual and visual trans-\nformers to learn modality-dependent representations.\nThis proposal uses a BERT-based model to encode tex-\ntual content. Specifically, three different BERT-like mod-\nels, already described in Sect. 2.2, were evaluated. The first \none is the original BERT model pretrained for English. The \n7402 D. Tomás et al.\n1 3\nsecond model is DeBERTa, that has proven to be very effec-\ntive capturing complex semantic and syntactic relationships \namong words from the input texts. Unlike previous BERT \nmodels, DeBERTa disentangles the attention mechanism, \nusing different matrices for computing attention weights \nassociated to the token embeddings and their positional \nembeddings. This provides the model the ability to infer \nglobal dependency relations that could be ignored in BERT, \ncapturing syntactic patterns that have demonstrated to play \nan important role in irony detection (Cignarella et al. 2020a, \nb). The third model studied is BERTweet. The reason for \nthis choice is that this model was trained and tuned on a \nvery large corpora of tweets written in English. This quality \nmakes it specially suitable on NLP tasks where texts are \nshort and informal.\nRegarding visual content, the proposed architecture uses \nViT, since this model has achieved strong performance in \nseveral tasks such as object recognition, image segmenta-\ntion, image classification, and scene understanding (Doso-\nvitskiy et al. 2021). ViT has demonstrated to outperform \nstate-of-the-art CNN architectures in severe occlusions for \nforeground objects, non-salient background regions, and \nrandom patch locations. Moreover, the model shows robust-\nness against other image processing problems such as spa-\ntial patch-level permutations, adversarial perturbations, and \ncommon natural corruptions (e.g., noise, blur, contrast, and \npixelation artefacts) (Naseer et al. 2021).\n3.2  Early fusion\nThe architecture proposed follows an early fusion approach \n(Gadzicki et al. 2020) to combine all the modalities in \nDT4MID and EDT4MID. The main idea behind this strat-\negy is to separately learn different feature spaces from the \ntraining data, which can thus capture different characteristics \nthat can be beneficial to combine the abstract representations \nlearned by the textual and visual transformers. The goal is \nto jointly use these representations to retain discriminant \ninformation while reducing redundant data.\nLet’s define CLS1 , CLS2 and CLS3 as the two transformer-\nbased textual representations 3 and the transformed-based \nvisual representation, respectively. Each representation is \npass through a dense layer to reduce and unify its dimen-\nsionality as expressed in the following equation:\nwhere W i and b i i = 1,…,3 are parameters learned during \nthe training process, and LeakyReLU is the Leaky Rectified \nLinear Unit activation function (Maas et al. 2013). After \nthese representations are reduced ( reduce1 , reduce2 , and \nreduce3 ), batch normalisation (Ioffe and Szegedy 2015) is \napplied before combining them within an aggregation layer \nas follows:\n(1)reduce(CLS i)= LeakyReLU(W iCLS i + bi),\nTextual transformer Visual transformer\nInput text Input image\nDense layer (Leaky ReLU)\nDropout\nDense layer (Leaky ReLU)\nDense layer (ReLU)\nDense layer \n(Softmax)\nDense layer (Leaky ReLU)\nIronic / Non-ironic\nText\nprocessing\nImage\nprocessing\nBatch normalization\nConcatConcat\nBatch normalization\nFig. 1  Dual Transformer for Multimodal Irony Detection (DT4MID) \narchitecture. The model receives one textual and one visual input\nTextual transformerTextual transformerTT\nInput text\nDense layer (Leaky ReLU)\nText\nprocessing\nConcat\nImage processing\noutput\nTags / OCR\nDense layer input\nBatch normalization\nFig. 2  Extended Dual Transformer for Multimodal Irony Detection \n(EDT4MID) architecture. Only the changes with respect to DT4MID \nare shown in the diagram. The model receives the visual input and \ntwo different textual inputs in parallel that are processed by the tex-\ntual encoder sharing the same parameters\n3 In the case of DT4MID, there is only one textual input and conse-\nquently one transformer.\n7403Transformer-based models for multimodal irony detection  \n1 3\nIn the next step, the mixed representation denoted as H0 is \npassed through a dense layer with LeakyReLU activation to \nfuse all the deep representations into a new feature space:\nThe output of this layer ( H1 ) is a multimodal encoding of \nthe inputs, which is then passed through a dropout layer ( H2 ) \n(Srivastava et al. 2014) to prevent model overfitting:\nThe result of the dropout layer is passed through a dense \nlayer with ReLU activation (Agarap 2018):\nFinally, the output of this layer ( H3 ) is provided as an input \nto a dense layer with two output neurons with the softmax  \nfunction (Goodfellow et al. 2016, Ch. 6, p. 180) to obtain \nthe labels (ironic or non-ironic):\nThe EDT4MID architecture can be trained in an end-to-end \nway using the back-propagation method. To this end, cat-\negorical cross-entropy is used as the loss function:\nwhere /u1D507 is the dataset, /u1D50F is the loss function, f is our model \nparameterised by /u1D703 , and /u1D53E={ 1, 0} is the set of labels in the \nbinary classification task.\n4  Evaluation\nThis section describes the evaluation carried out. First, the \ncorpora used are described, including two different multi-\nmodal datasets from the literature obtained from Twitter and \nTumblr social networks. Next, the baselines employed in the \nexperimentation are depicted. Finally, results and discussion \nare shown at the end of the section.\n4.1  Corpora\nThe first part of the evaluation focuses on the corpus devel-\noped by Cai et al. (2019), which was already mentioned \n(2)H 0 = aggregate(reduce1 ,reduce2 ,reduce3 )\n(3)H1 = LeakyReLU(W 0H0 + b0)\n(4)H2 = Dropout (H1 ,/u1D6FF)\n(5)H3 = max(0,W 2 H2 + b2 )\n(6)O = softmax(W 3H 3 + b3)\n(7)\n/u1D50F(/u1D703)=/u1D53C/u1D507[/u1D50F(f(x,/u1D703),y)]\n=− 1\nn\nn�\ni=1\n‖/u1D53E‖�\nj=1\nyij ∗ log(f(xi,/u1D703)j)w j,\nin Sect. 2. The authors collected English tweets containing \na picture and special hashtags (e.g. #sarcasm) as ironic \nexamples, and without such hashtags as non-sarcastic rep-\nresentatives. The dataset consists of 19,816 tweets for train-\ning (8642 positive and 11,174 negative), 2410 for validation \n(959 positive and 1451 negative) and 2409 for testing (959 \npositive and 1450 negative), all of them including their tex-\ntual content and the associated image. This dataset is called \nTwitter corpus in the evaluation section.\nThe second dataset was described in Schifanella et al. \n(2016). This corpus comprises two parts. The first one \nis called the silver dataset. In this corpus, data (text and \nimages) were collected from three major social platforms: \nInstagram, Tumblr, and Twitter. The only corpus to which \naccess was available for reproducing the experiments in \nthis paper was Tumblr. This microblogging platform \nallows users to post different types of content, including \ntext or photo. Regardless of the post type, images (one \nor more) can be added to text  posts, and captions can be \nincluded in photo posts. Users can add hashtags to the \ncontent by writing them in a separate field.\nTo collect ironic (positive) examples, they followed a \nhashtag-based approach by retrieving posts that include \nthe tag sarcasm or sarcastic. To clean up the data \nand build the final dataset, different common filters were \napplied, such as discarding posts without associated \nimages, posts that contained mentions or external links, \nposts where sarcasm or sarcastic was a regular word (not \na hashtag), and posts whose text contained less than four \nregular words. To build the final dataset, 10,000 sarcastic \nposts were randomly sampled. Another 10,000 negative \nexamples were collected by randomly sampling posts that \ndo not contain sarcasm or sarcastic in either the text or \nthe tag set. In the experiments below, this dataset is called \nthe Tumblr silver corpus.\nThe second part of the dataset in Schifanella et al. \n(2016), called gold dataset, is a curated corpus whose \nironic nature was agreed on by a group of human anno-\ntators that considered the textual and visual components \nas required to decode the sarcastic tone. A crowdsourc -\ning platform was used to perform the annotation task on \n1000 positive samples (5 annotators for each post). The \nusers were asked to annotate the posts as text only  \n(the text was enough to identify the nature of the post \nas ironic) and text + image when both modalities \nwere required. From the 1000 posts, 319 were labelled as \nnon-ironic, 236 were considered a text only ironic, \nand 445 were text + image ironic. Only the dataset \ncomprising text + image posts were used to further \nevaluate the system.\nDepending on the agreement achieved by the annota-\ntors on whether both text and image were required, three \nsubsets were created: D-50 (50% agreement, 445 samples), \n7404 D. Tomás et al.\n1 3\nD-80 (80% agreement, 197 samples) and D-100 (100% \nagreement, 141 samples). In Sect.  4.4, only the D-100 sub-\nset (full agreement) is used. This dataset is referred to as \nTumblr gold corpus in the experiments. It is important to \nhighlight that we used the same corpus proposed in Cai \net al. (2019) and the corpus introduced in Schifanella et al. \n(2016), to make a fair comparison with the previous works \nin state of the art.\n4.2  Baselines\nThe proposed DT4MID and EDT4MID models are based \non the transformer architecture. In order to compare their \nperformance with other neural network models, the proposal \ndescribed in Giachanou et al. (2020) has been adopted as \na baseline. It was originally intended for multimodal fake \nnews detection and has been adapted in the present work \nfor irony detection. This model is based on a neural network \narchitecture that combines textual and visual features. The \ntextual information is encoded using static word embeddings \n(Word2vec) (Mikolov et al. 2013). Additionally, the textual \nfeature vector also includes the sentiment expressed in the \npost, extracted using the Valence Aware Dictionary for sEn-\ntiment Reasoning (VADER) (Hutto and Gilbert 2014).\nThe visual information is extracted from the image of the \nposts and includes image tags and Local Binary Patterns \n(LBP) (Ojala et al. 2002), a texture operator that has proven \nto be very effective in many visual tasks, such as face rec-\nognition. Image tags are extracted using different pretrained \nCNN-based models, such as VGG19 and ResNet. The top \nten image tags for every pretrained model are encoded using \nWord2vec. The system calculates the similarity between \ntext and image by computing the cosine between the word \nembeddings of the text and the word embeddings of the \nimage tags extracted from visual features. The final input \nvector is the result of concatenating all the features men-\ntioned in the previous paragraphs. A fully connected neural \nnetwork is then feed with this information to produce the \noutput of the system.\nThis approach is referred to as Baseline  in the experi -\nments below and have been tested on the three datasets men-\ntioned in the previous section. In the case of Twitter corpus, \nthe results of previous works (Cai et al. 2019; Xu et al. 2020; \nWang et al. 2020; Pan et al. 2020) have been also included \nas a reference in the evaluation. For the Tumblr silver corpus \nand Tubmlr gold corpus, the results of their authors (Schi-\nfanella et al. 2016) are also provided. All these approaches \nwere described in Sect. 2.\nAdditionally, two visio-linguistic transformers described \nin the related work have been also tested and compared in the \nexperiments carried out: VisualBERT and LXMERT. The \nfirst one is representative of the single-stream models, where \nimages and text are jointly processed by a single encoder. \nThe second one belongs to the dual-stream category, where \ninputs are encoded separately before being jointly modelled. \nThe two models were fine-tuned using the same datasets \nas the DT4MID and EDT4MID architectures. The number \nof epochs was set to 30 and the minibatch size to 8. Both \nRMSprop and Adam optimisers were evaluated. Only the \nbest configuration results are shown in the evaluation section.\n4.3  Training procedure\nThe proposed architecture was implemented using PyTorch4 \nand the pre-trained models in the Huggingface 5 library. \nThe models used and compared for textual encoding were \nbert-uncased, deberta-base and  bertweet-base. The visual \nencoding was carried out using vit-base.\nThe models were fine-tuned following an end-to-end \napproach, where all weights were learned together. Specifi-\ncally, the strategy adopted was that proposed in ULMFiT \n(Howard and Ruder 2018) for tuning pre-trained models in a \ngradual unfreezing-discriminative fashion. This strategy has \noutperformed the standard schema for fine-tuning transformer \nmodels on different NLP tasks. Taking that into account, \ndifferent learning rates were assigned for each layer in the \nmodel, increasing the rate as the neural network gets deeper, \ni.e /u1D6FCi = /u1D6FC0 + /u1D706i and /u1D706i =( /u1D6FFlr)layers , where /u1D6FCi is the learning rate \nof the ith layer, /u1D706i is the increasing of the learning rate to com-\npute /u1D6FCi from /u1D6FC0 and /u1D6FFlr is maximum increase permitted. Thus, \nthe shallowest layers that receive the input message have the \nlowest increase, whereas the layers at the top of the model \nreceive a greater learning rate. This dynamic learning rate \nkeeps most information from the shallow layers and biases the \ndeeper ones to learn about the target tasks. The learning rate \nupdating procedure is summarised in Algorithm 1.\nAlgorithm 1 Learning rates chedule\nInput: lr (Learning rate)\nInput: mlr (Minimuml earning rate)\nRequire: lr > 0 ∧mlr> 0\nEnsure: lr = mlr\n1: i ⇐ 12\n2: α0 ⇐ mlr\n3: δlr ⇐ abs(lr −mlr)\n4: while i ≥ 1 do\n5: λi ⇐ (δlr )i\n6: αi ⇐ α0 + λi\n7: i ⇐ i −1\n8: end while\n4 https:// pytor ch. org/.\n5 https:// huggi ngface. co/.\n7405Transformer-based models for multimodal irony detection  \n1 3\nRegarding the number of hidden neurons, in the reduc-\ntion layer it was set to 64, in the first dense layer after the \naggregation layer to 64, in the second to last layer to 32 \nneurons, and the last layer contained only one neuron. The \ndropout was set to 0.30. The hyperparameters of batch size  \nand maximum sequence length were constrained by the hard-\nware resources available. Based on that, the sequence length \nwas set to 100 and mini-batches to 10. In an intent to prevent \noverfitting in the training step, no fixed number of epochs  \nwas defined. Instead, the process relied on the early stopping \ncriteria, setting the value of patience to 15 and the maximum \nnumber of epochs to 50. RMSprop and Adam optimisation \nrules were both evaluated during the training phase.\n4.4  Results\nThe evaluation was made using two standard metrics: accu-\nracy and F1-score. One of the reasons for this decision was \nto be able to compare this work with previous approaches \non the same benchmarks. Table  1 shows in the upper part \nthe results obtained by the baseline described in Sect.  4.2, \nthe previous work in the field, and the visio-linguistic trans-\nformers (VisualBERT and LXMERT). The different vari-\nants of the proposed architectures (DT4MID and EDT4MID) \nare shown in the lower part of the table. BERT (only text), \nDeBERTa (only text) and BERTweet (only text) refer to \nthe DT4MID architecture including only the textual trans-\nformer. ViT (only image) refers to the DT4MID architecture \nusing only the visual transformer, whereas BERT + ViT, \nDeBERTa + ViT and BERTweet + ViT cover the full \nDT4MID architecture combining textual and visual trans-\nformers. Finally, BERT + ViT + OCR, DeBERTa + ViT \n+ OCR and BERTweet + ViT + OCR refer to the extended \narchitecture EDT4MID with multiple textual inputs, includ-\ning in this case an OCR analysis to extract embedded text \nfrom images. This configuration follows the idea proposed \nby Pan et al. (2020), where the system obtained a boost in \nperformance by including OCR features in their model (their \nsystem improved from 0.8292 F1-score to the final 0.8618 \nreported in Table  1). The general purpose Tesseract OCR \nengine6 was used to analyse the 19,816 images in the dataset. \nTesseract was able to extract text from 56.35% of the images.\nThe baseline proposed obtained results close to the origi-\nnal paper (0.787 vs 0.8018 F1-score). The text-only ver -\nsion DT4MID using DeBERTa and BERTweet performed \nextremely well (0.9372 and 0.8941 F1-score respectively), \nsurpassing all the previous multimodal approaches on this \ndataset. DeBERTa improved almost 9% the performance of \nthe best previous system (F1-score = 0.8618). VisualBERT \n(using RMSprop optimiser) was the best visio-transformer \nmodel, outperforming the previous state-of-the-art approach \nin terms of precision but not recall, which led to a lower \nF1-score (0.8513).\nThe image-only transformer version using ViT achieved \nF1-score = 0.7050, significantly lower than the text-only \nversions of the model. When both modalities were com-\nbined, the text-only version of BERT significantly improved \nTable 1  Experiments on the Twitter corpus \nBest results for each measure are boldfaced\nApproach F1-score Accuracy\nBaseline 0.7800 0.7870\nCai et al. (2019) 0.8018 0.8344\nXu et al. (2020) 0.8060 0.8402\nWang et al. (2020) 0.8605 0.8851\nPan et al. (2020) 0.8618 0.8875\nVisualBERT 0.8513 0.8581\nLXMERT 0.8164 0.8248\nDT4MID\nBERT (only text) 0.8309 0.8360\nDeBERTa (only text) 0.9372 0.9394\nBERTweet (only text) 0.8941 0.8983\nViT (only image) 0.7050 0.7148\nBERT + ViT 0.8516 0.8555\nDeBERTa + ViT 0.9377 0.9398\nBERTweet + ViT 0.8956 0.8995\nEDT4MID\nBERT + ViT + OCR 0.8445 0.8506\nDeBERTa + ViT + OCR 0.9368 0.9390\nBERTweet + ViT + OCR 0.8974 0.9012\nTable 2  Experiments on the Tumblr silver corpus \nBest results for each measure are boldfaced\nApproach F1-score Accuracy\nBaseline 0.7650 0.7670\nSchifanella et al. (2016) – 0.810\nVisualBERT 0.7874 0.7957\nLXMERT 0.7624 0.7728\nDT4MID\nBERT (only text) 0.7714 0.7771\nDeBERTa (only text) 0.7972 0.8077\nBERTweet (only text) 0.8181 0.8252\nViT (only image) 0.7200 0.7241\nBERT + ViT 0.7884 0.8014\nDeBERTa + ViT 0.8297 0.8365\nBERTweet + ViT 0.8105 0.8260\nEDT4MID\nBERT + ViT + tags 0.8421 0.8483\nDeBERTa + ViT + tags 0.8485 0.8563\nBERTweet + ViT + tags 0.7677 0.7765\n6 https:// tesse ract- ocr. github. io/.\n7406 D. Tomás et al.\n1 3\nits performance (2.5% higher), but DeBERTa and BERT -\nweet marginally improved it. In the case of the EDT4MID \narchitecture including OCR features, the results did not sig-\nnificantly improve the performance of the corresponding \ntext + image models, slightly worsening them in the case of \nBERT and DeBERTa. It is worth noting that Tesseract did \nnot extract text in almost half of the cases, and that this text \nis not always a reliable representation of what can be found \nin the image.\nThe second corpus evaluated is the Tumblr silver corpus. \nTable 2 shows the results obtained in the experiments carried \nout, which are the same conducted in the previous corpus. \nThe only difference is the comparison with related work, \nwhich in this case is limited to Schifanella et al. (2016). \nSince the authors reported two approaches, only the best \nresults are included here, corresponding to the information \nfusion with SVM approach.\nTaking into account that there was no improvement using \nOCR features in the previous experiments, this informa-\ntion was discarded in this corpus. Instead, a new feature \nwas added that corresponds to the list of tags that Tumblr \nusers added to their posts. These tags were treated separately \nfrom text, as an additional textual input in the EDT4MID \narchitecture.\nThe results show that the baseline performed worse \n(accuracy = 0.7670) than the original system provided \nin Schifanella et al. (2016) (accuracy = 0.810). 7 The best \nvisio-linguistic model was again VisualBERT (using Adam \noptimiser), although in this case the results did not improve \nthe previous work (accuracy = 0.7957). Regarding the text-\nonly version of the architecture, the best performing model \nwas BERTweet (accuracy = 0.8252), followed by DeBERTa \n(accuracy = 0.8077). BERTweet improved the original sys-\ntem and DeBERTa achieved almost its performance. Again, \nusing only the visual transformer performed comparatively \nlower than the text-only version (accuracy = 0.7241).\nWhen both textual and visual features were considered \nthe system significantly improved its performance in the \ncase of DeBERTa (accuracy = 0.8365) and BERT (accu-\nracy = 0.8014), but not for BERTweet. The best results were \nobtained when tags were included, improving 2.4% the accu-\nracy of DeBERTa + ViT and 5.9% in the case of BERT + \nViT. Globally, DeBERTa + ViT + tags obtained the best \nresults, improving almost 6% the performance provided by \nSchifanella et al. (2016).\nThe last corpus used in the experiments is the Tumblr \ngold corpus. Unlike the previous corpora, this is a curated \ndataset that can provide more reliable insights on the role \nof text and image in multimodal irony detection. Table  3 \nshows the results obtained using the same architectures and \nvariants used on the Tumblr silver corpus.\nIn this experiment, the models were trained on the whole \nTumblr silver corpus and tested on the Tumblr gold corpus \n(D-100 subset with unanimous inter-annotator agreement). \nThis corpus contains only positive samples (i.e., ironic \ntweets), thus the performance measures provided in Table 3 \nonly consider this class.\nIn these experiments, the text-only version of DeBERTa \nalready provided a significant improvement in performance \nwith respect to the best model presented by Schifanella et al. \n(2016) (again, the authors only showed accuracy values). \nDeBERTa improved the original result (accuracy = 0.897) \nTable 3  Experiments on the Tumblr gold corpus \nBest results for each measure are boldfaced\nApproach F1-score Accuracy\nBaseline 0.9000 0.8200\nSchifanella et al. (2016) – 0.897\nVisualBERT 0.9754 0.9521\nLXMERT 0.9382 0.8836\nDT4MID\nBERT (only text) 0.9343 0.8767\nDeBERTa (only text) 0.9682 0.9384\nBERTweet (only text) 0.9420 0.8904\nViT (only image) 0.8594 0.7534\nBERT + ViT 0.9754 0.9521\nDeBERTa + ViT 0.9304 0.8699\nBERTweet + ViT 0.9754 0.9521\nEDT4MID\nBERT + ViT + tags 0.9754 0.9521\nDeBERTa + ViT + tags 0.9645 0.9315\nBERTweet + ViT + tags 0.9496 0.9041\nTable 4  Training time (hours and minutes) of DT4MID and EDT -\n4MID models on the three datasets studied: Twitter (TW), Tumblr sil-\nver (TS) and Tumblr gold (TG)\nApproach TW TS TG\nDT4MID\nBERT (only text) 00:29 00:11 00:27\nDeBERTa (only text) 00:36 00:13 00:31\nBERTweet (only text) 00:37 00:09 00:24\nViT (only image) 05:29 01:00 01:25\nBERT + ViT 07:56 01:33 02:37\nDeBERTa + ViT 05:06 01:33 02:15\nBERTweet + ViT 08:48 01:33 01:59\nEDT4MID\nBERT + ViT + OCR/tags 06:14 02:17 02:21\nDeBERTa + ViT + OCR/tags 05:29 01:43 02:24\nBERTweet + ViT + tags 04:43 01:24 02:34\n7 Accuracy was the only measure provided in their work.\n7407Transformer-based models for multimodal irony detection  \n1 3\nby almost 5% (accuracy = 0.9384). The best proposed archi-\ntecture in this subset was BERT + ViT together with BER -\nTweet + ViT, which achieved the same performance. As \nin the previous experiments, BERT significantly benefited \nfrom the addition of visual features, improving almost 9% \nthe text-only version. The same results were obtained by \nVisualBERT (using Adam optimiser). Adding images to text \nimproved the performance in the case of BERT and BERT-\nweet, but not for DeBERTa, which reduced its performance. \nThe image-only version of the architecture performed sig-\nnificantly lower than the text-only transformers (accuracy = \n0.7534). Finally, adding tags did not enhance the results of \nBERT + ViT and BERTweet + ViT, but improved DeBERTa \n+ ViT by 7%.\nRegarding processing time, experiments involving \nDT4MID and EDT4MID models were carried out on a com-\nputer with a CPU Intel(R) Core(TM) i7-7800X 3.50GHz, \n126GB RAM and GPU NVIDIA GeForce RTX 2080 8GB. \nTraining time for each model in the three datasets studied \nis shown in Table  4. LXMERT and VisualBERT required \na more powerful computer for training, with GPU NVIDIA \nTitan RTX 24GB, whereas the baseline was trained on CPU. \nFor this reason, the training time of these three models is not \nincluded in the aforementioned table, since using a different \nhardware configuration prevents a fair comparison between \nthem. As can be observed, training time increases from text \nto image models and from unimodal to multimodal models.\n4.5  Discussion\nThe most remarkable result in the experiments carried out on \nthe Twitter corpus is the high performance obtained by the \ntext-only versions of the transformers analysed. In the case \nof DeBERTA, the DT4MDI architecture improved almost \n9% the previous state-of-the-art in this task. In this model, \nadding the information from the visual transformer did not \nsignificantly improve the performance of the system. These \nresults could reflect that part of the multimodal posts in this \ncorpus do not necessarily require from an image to deter -\nmine the ironic nature of the message, and considering only \nthe text is effective in some situations (recall that this dataset \nwas automatically retrieved from Twitter and lacks of human \nsupervision).\nTo further investigate this point, a random sample of 100 \nposts were selected from the Twitter corpus and manually \nanalysed by three annotators. The results revealed that, on \naverage, almost 40% of the posts were considered as ironic \nby looking only at text. The reasons for that vary. In some \ncases, there were hashtags such as #lolsarcasm or #funny -\nmemes that provide a clear hint of the nature of the post. It \nwas also interesting the presence of specific topics that can \nreveal the ironic intention, such as combining government \naffairs with positive vocabulary (e.g. “Tell me about it. For-\ntunately we have #government” and “Oh, this is a totally \nfair move that really shows that the government is doing it \nright!”). Finally, some posts that could be strictly considered \nas non-ironic in the surface, reveal their ironic intention to \nregular users of social networks. For instance, “My life in a \npicture” or “Me reflecting on the weekend” are usually fol-\nlowed by an image that shows the implicit ironic intention \nof the user. Thus, textual transformers could be learning this \ntype of linguistic patterns in text, making unnecessary the \npresence of an image to correctly classify them as ironic, \nand therefore justifying the high accuracy of the text-only \nmodels and the lack of improvement when image context \nis added.\nA subsequent manual analysis of the Tumblr gold corpus \nconfirmed the presence of these patterns in the corpora. In \naddition to those, it was also common the presence of posts \nwith “When” / “Whenever” at the beginning of the sentence. \nE.g. “When healthy people tell me about what I missed out \non.” and “Whenever I try something new”. The use of very \npositive claims was another common pattern in ironic posts. \nFor instance: “Tumblr mobile works perfectly” and “Where \nAmerica excels”.\nA set of these examples were further analysed using the \nCaptum8 library for model interpretability. Figure  3 shows \nFig. 3  Contribution of each \ntoken to the model’s output\n8 https:// captum. ai/.\n7408 D. Tomás et al.\n1 3\nthe contribution of each token to irony identification using \nthe text-only version of DeBERTa. Green colour indicates \npositive contribution of tokens towards the predicted class, \nwhereas red shows tokens contributing negatively. The \nintensity of the colour signifies the magnitude of the con-\ntribution. All the examples shown, except the last one, were \ncorrectly classified as ironic.\nThese samples exemplify all the patterns mentioned \nbefore. It is clear the contribution of “Fortunately ... gov -\nernment”, “Me going”, “When ... people”, “Whenever”, \n“hahahahahahah” and “excel” to the class prediction. In the \nlast example, wrongly classified by the model as non-ironic, \nno clear linguistic pattern can be identified.\nIn the Tumblr silver corpus  the combination of textual \nand visual transformers outperformed the text-only version \nof the architecture. DeBERTa + ViT + tags obtained the best \nresults, improving more than 6% the F1-score of DeBERTa. \nSimilar results were obtained for BERT, where BERT + ViT \n+ tags improved 9% the F1-score of the text-only BERT. In \nthe case of BERTweet, the performance of the model did not \nsignificantly improved by adding the visual transformer to \nthe text-only version. In general, the text-only models per -\nformed significantly lower than in the previous experiment. \nFor instance, DeBERTa obtained F1-score = 0.7972, almost \n15% drop in performance.\nThis fact reflects that this corpus is more challenging for \nthe models than the previous one. One reason for this varia-\ntion is the different nature of the posts in Twitter and Tumblr. \nIn the latter, posts tend to be longer and more elaborated \nthan in Twitter, which can make it more challenging to iden-\ntify the ironic nature of the text. In the Twitter corpus, the \naverage length of the posts is 89 characters, whereas in the \nTumblr silver corpus this value rises up to 273.\nThe best text-only results in the Tumblr gold corpus were \nobtained again by DeBERTa (F1-score = 0.9682). Taking \ninto account the high performance of the textual trans-\nformer, it was expected that the addition of a visual features \nin the ensemble did not improve the final results. In fact, \nnot only did ViT not improve the results, it actually sig-\nnificantly reduced the performance. Unlike DeBERTa, the \ncombination of BERT and BERTweet with ViT significantly \nimproved the text-only version to achieve the best perfor -\nmance in this corpus. In general, performance was higher \nthan in both previous datasets. The curated nature of this \ncorpus reveals the shortcomings of the automatic data col-\nlection procedures for irony detection, which lead to lower \nperformance in machine learning approaches.\nThe relative performance of the models is stable across \nthe three datasets, with the two proposed architectures sys-\ntematically improving the results of the baselines. One of \nthe main conclusions of this set of experiments is that the \ntext-only version of the models, properly fine-tuned, can \nachieve high performance in multimodal irony detection. \nThis reflects that in many occasions the text contains linguis-\ntic patterns that give clear clues of the ironic nature of the \npost without requiring additional visual context, as shown \nin the examples provided above.\nRegarding the comparison between textual transformers \nand previous static word embeddings, transformers clearly \noutperformed the Word2vec model used in the baseline \narchitecture in all the experiments. In the case of the text-\nonly version of DeBERTa, the F1-score improved over 20% \nfor the Twitter corpus, over 4% in the Tumblr silver corpus  \nand almost 8% for the Tumblr gold corpus. Textual trans-\nformers also surpassed the proposal by Cai et al. (2019) \nusing GloVe and the approach by Xu et al. (2020) that relied \non a BiLSTM network to represent textual sequences.\nFinally, in terms of the performance of current CNN-\nbased networks compared to visual transformers, the experi-\nments in Table 1 revealed that both the architecture proposed \n(using ViT) and the visio-linguistic transformers (Visual-\nBERT and LXMERT) outperformed the baseline using \nResNet. The proposed system using ViT also outperformed \nprevious works in the field shown in this table, which also \nrelied on ResNet to analyse the visual content.\n5  Conclusions and future work\nThis paper presented a deep learning architecture that com-\nbines textual and visual transformers for the task of irony \ndetection. Unlike previous approaches, the architecture pro-\nposed fully rely on transformers for both textual and visual \ninputs, performing an early fusion approach to integrate both \ntypes of media.\nThe architecture was evaluated in three different corpora \nand compared with different baselines, including current \nvisio-linguistic transformer models. As far as the authors \nknow, this is the first attempt to apply and evaluate these \nmodels in the task of multimodal irony detection.\nThe evaluation results revealed that the architecture pro-\nposed significantly outperformed (up to 9%) current state-\nof-the-art systems and all the baselines proposed in this task. \nThe most remarkable finding was the high level of perfor -\nmance of the text-only models, being DeBERTa the most \nsalient of them. These results lead to the conclusion that, in \nsome situations, the image provided as context for the mul-\ntimodal post is not necessary to catch the ironic nature of the \nmessage, even in situations when a group of humans could \nunanimously determine that it is required to understand its \nsarcastic meaning. Transformer models were able to identify \nlinguistic patterns in text that were leveraged to identify the \nirony without further context.\nIn the case of the Twitter corpus and the Tumblr gold \ncorpus, the performance of the text-only transformers was so \nhigh that the addition of visual information did not improve \n7409Transformer-based models for multimodal irony detection  \n1 3\nthe results. In the case of Tumblr silver corpus, the task \nresulted more challenging and adding the visual transformer \nsignificantly improved the results.\nThe present study has shown that multimodal irony is \na subtle phenomenon, where textual input provides more \ninformation about their ironic nature than expected. As a \nfuture work, it is mandatory in the first place to carry out \na deeper quantitative and qualitative study of the nature of \nmultimodal irony in order to precisely identify the type of \nposts that really benefit from adding contextual informa-\ntion in the form of images. To this end, a study comparing \nunimodal (text-only) and multimodal messages could reveal \nthe real differences existing between these two modalities.\nFunding Information Open Access funding provided thanks to the \nCRUE-CSIC agreement with Springer Nature. This work was par -\ntially supported by the Spanish Ministry of Science and Innovation \nand Fondo Europeo de Desarrollo Regional (FEDER) in the framework \nof project “Technological Resources for Intelligent VIral AnaLysis \nthrough NLP (TRIVIAL)” (PID2021-122263OB-C22).\nDeclarations \nConflict of interest The authors have no competing interests to declare \nthat are relevant to the content of this article.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAgarap AF (2018) Deep learning using rectified linear units (ReLU). \narXiv: 1803. 08375\nAlam F, Cresci S, Chakraborty T, et al (2021) A survey on multimodal \ndisinformation detection. arXiv: 2103. 12541\nCai Y, Cai H, Wan X (2019) Multi-modal sarcasm detection in Twitter \nwith hierarchical fusion model. In: Proceedings of the 57th annual \nmeeting of the ACL. Association for Computational Linguistics, \npp 2506–2515. https:// doi. org/ 10. 18653/ v1/ P19- 1239\nCignarella AT, Basile V, Sanguinetti M, et al (2020a) Multilingual \nirony detection with dependency syntax and neural models. In: \nProceedings of the 28th international conference on computational \nlinguistics. International Committee on Computational Linguis-\ntics, Barcelona, Spain (Online), pp 1346–1358. https:// doi. org/ 10. \n18653/ v1/ 2020. coling- main. 116\nCignarella AT, Sanguinetti M, Bosco C, et al (2020b) Marking irony \nactivators in a Universal Dependencies treebank: the case of \nan Italian Twitter corpus. In: Proceedings of the 12th language \nresources and evaluation conference. European Language \nResources Association, Marseille, France, pp 5098–5105. https:// \naclan tholo gy. org/ 2020. lrec-1. 627\nConneau A, Khandelwal K, Goyal N, et al (2020) Unsupervised cross-\nlingual representation learning at scale. In: Proceedings of the \n58th annual meeting of the association for computational lin-\nguistics. Association for Computational Linguistics, Online, pp \n8440–8451. https:// doi. org/ 10. 18653/ v1/ 2020. acl- main. 747\nDevlin J, Chang MW, Lee K, et al (2019) Bert: pre-training of deep \nbidirectional transformers for language understanding. In: Pro-\nceedings of the 2019 conference of the north American chapter \nof the association for computational linguistics. Association for \nComputational Linguistics, pp 4171–4186. https:// doi. org/ 10.  \n18653/ v1/ N19- 1423\nDosovitskiy A, Beyer L, Kolesnikov A, et al (2021) An image is worth \n16x16 words: transformers for image recognition at scale. In: \nInternational conference on learning representations, pp 1–21. \nhttps:// openr eview. net/ forum? id= YicbF dNTTy\nGadzicki K, Khamsehashari R, Zetzsche C (2020) Early vs late fusion \nin multimodal convolutional neural networks. In: 2020 IEEE 23rd \ninternational conference on information fusion (FUSION), pp 1–6. \nhttps:// doi. org/ 10. 23919/ FUSIO N45008. 2020. 91902 46\nGiachanou A, Zhang G, Rosso P (2020) Multimodal fake news detec-\ntion with textual, visual and semantic information. Text, speech, \nand dialogue. Springer, Cham, pp 30–38\nGoodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT press, \nOxford\nHe P, Liu X, Gao J, et al (2020) Deberta: decoding-enhanced BERT \nwith disentangled attention. arXiv: 2006. 03654\nHoward J, Ruder S (2018) Universal language model fine-tuning for \ntext classification. In: Proceedings of the 56th annual meeting of \nthe ACL. Association for Computational Linguistics, pp 328–339. \nhttps:// doi. org/ 10. 18653/ v1/ P18- 1031\nHutto C, Gilbert E (2014) Vader: a parsimonious rule-based model for \nsentiment analysis of social media text. Proc Int AAAI Conf Web \nSoc Media 8(1):216–225\nIoffe S, Szegedy C (2015) Batch normalization: accelerating deep net-\nwork training by reducing internal covariate shift. In: International \nconference on machine learning, PMLR, pp 448–456\nJoshi A, Bhattacharyya P, Carman MJ (2017) Automatic sarcasm detec-\ntion: a survey. ACM Comput Surv 50(5):1–22. https:// doi. org/ 10. \n1145/ 31244 20\nKiela D, Firooz H, Mohan A, et al (2021) The hateful memes chal-\nlenge: competition report. In: Escalante HJ, Hofmann K (eds) \nProceedings of the NeurIPS 2020 competition and demonstration \ntrack, proceedings of machine learning research, vol 133. PMLR, \npp 344–360\nLi LH, Yatskar M, Yin D, et al (2019) Visualbert: a simple and perfor-\nmant baseline for vision and language. arXiv: 1908. 03557\nLiu Y, Ott M, Goyal N, et al (2019) Roberta: a robustly optimized bert \npretraining approach, pp 1–13. arXiv preprint arXiv: 1907. 11692\nMaas AL, Hannun AY, Ng AY (2013) Rectifier nonlinearities improve \nneural network acoustic models. In: Proceedings of the ICML \nworkshop on deep learning for audio, speech and language pro-\ncessing, Atlanta, Georgia, USA, pp 1–6\nMikolov T, Sutskever I, Chen K, et al (2013) Distributed representa-\ntions of words and phrases and their compositionality. In: Pro-\nceedings of the 26th international conference on neural informa-\ntion processing systems, vol 2. Curran Associates Inc., NIPS’13, \npp 3111–3119\nNaseer M, Ranasinghe K, Khan S, et al (2021) Intriguing properties of \nvision transformers. arXiv: 2105. 10497\nNguyen DQ, Vu T, Tuan Nguyen A (2020) BERTweet: a pre-trained \nlanguage model for English tweets. In: Proceedings of the 2020 \nconference on empirical methods in natural language process-\ning: system demonstrations. Association for Computational \n7410 D. Tomás et al.\n1 3\nLinguistics, Online, pp 9–14. https:// doi. org/ 10. 18653/ v1/ 2020. \nemnlp- demos.2, https:// aclan tholo gy. org/ 2020. emnlp- demos.2\nOjala T, Pietikainen M, Maenpaa T (2002) Multiresolution gray-scale \nand rotation invariant texture classification with local binary \npatterns. IEEE Trans Pattern Anal Mach Intell 24(7):971–987. \nhttps:// doi. org/ 10. 1109/ TPAMI. 2002. 10176 23\nPan H, Lin Z, Fu P, et al (2020) Modeling intra and inter-modality \nincongruity for multi-modal sarcasm detection. In: Findings of the \nassociation for computational linguistics: EMNLP 2020. Associa-\ntion for Computational Linguistics, pp 1383–1392. https:// doi. org/ \n10. 18653/ v1/ 2020. findi ngs- emnlp. 124\nSchifanella R, de Juan P, Tetreault J, et al (2016) Detecting sarcasm \nin multimodal social platforms. In: Proceedings of the 24th ACM \ninternational conference on multimedia. Association for Comput-\ning Machinery, New York, NY, USA, MM ’16, pp 1136–1145. \nhttps:// doi. org/ 10. 1145/ 29642 84. 29643 21\nSrivastava N, Hinton G, Krizhevsky A et al (2014) Dropout: a simple \nway to prevent neural networks from overfitting. J Mach Learn \nRes 15(1):1929–1958\nTan H, Bansal M (2019) LXMERT: learning cross-modality encoder \nrepresentations from transformers. In: Proceedings of the 2019 \nconference on empirical methods in natural language processing \nand the 9th international joint conference on natural language \nprocessing (EMNLP-IJCNLP). Association for Computational \nLinguistics, pp 5100–5111. https:// doi. org/ 10. 18653/ v1/ D19- 1514\nVan Hee C, Lefever E, Hoste V (2018) SemEval-2018 task 3: irony \ndetection in English tweets. In: Proceedings of The 12th interna-\ntional workshop on semantic evaluation. Association for Compu-\ntational Linguistics, New Orleans, Louisiana, pp 39–50. https://  \ndoi. org/ 10. 18653/ v1/ S18- 1005, https:// aclan tholo gy. org/ S18- 1005\nVaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. \nIn: Advances in neural information processing systems, vol 30. \nCurran Associates, Inc., pp 5998–6008\nWang X, Sun X, Yang T, et al (2020) Building a bridge: a method for \nimage-text sarcasm detection without pretraining on image-text \ndata. In: Proceedings of the first international workshop on natural \nlanguage processing beyond text. Association for Computational \nLinguistics, pp 19–29. https:// doi. org/ 10. 18653/ v1/ 2020. nlpbt-1.3\nXu N, Zeng Z, Mao W (2020) Reasoning with multimodal sarcastic \ntweets via modeling cross-modality contrast and semantic associa-\ntion. In: Proceedings of the 58th annual meeting of the associa-\ntion for computational linguistics. Association for Computational \nLinguistics, pp 3777–3786. https:// doi. org/ 10. 18653/ v1/ 2020. acl- \nmain. 349\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Irony",
  "concepts": [
    {
      "name": "Irony",
      "score": 0.8104056715965271
    },
    {
      "name": "Computer science",
      "score": 0.7888703346252441
    },
    {
      "name": "Transformer",
      "score": 0.6988928914070129
    },
    {
      "name": "Architecture",
      "score": 0.664261519908905
    },
    {
      "name": "Phenomenon",
      "score": 0.562294065952301
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4682164192199707
    },
    {
      "name": "Social media",
      "score": 0.4635685086250305
    },
    {
      "name": "Natural language processing",
      "score": 0.440133661031723
    },
    {
      "name": "Linguistics",
      "score": 0.2451268434524536
    },
    {
      "name": "World Wide Web",
      "score": 0.19930174946784973
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130194489",
      "name": "University of Alicante",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I60053951",
      "name": "Universitat Politècnica de València",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I55143463",
      "name": "University of Turin",
      "country": "IT"
    }
  ]
}