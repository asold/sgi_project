{
  "title": "Ethical framework for responsible foundational models in medical imaging",
  "url": "https://openalex.org/W4410495919",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5044673103",
      "name": "Debesh Jha",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5033826718",
      "name": "Görkem Durak",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5100716407",
      "name": "Abhijit Das",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5099282244",
      "name": "Jasmer Sanjotra",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5004353809",
      "name": "Onkar Susladkar",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5073521426",
      "name": "Suramyaa Sarkar",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5006605340",
      "name": "Ashish Rauniyar",
      "affiliations": [
        "SINTEF"
      ]
    },
    {
      "id": "https://openalex.org/A5076182848",
      "name": "Nikhil Kumar Tomar",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5037672272",
      "name": "Linkai Peng",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5101858043",
      "name": "Sirui Li",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5010852509",
      "name": "Koushik Biswas",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5065925589",
      "name": "Emel Aktaş",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5072616384",
      "name": "Elif Keleş",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5087543918",
      "name": "Matthew Antalek",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5100691434",
      "name": "Zheyuan Zhang",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5100324087",
      "name": "Bin Wang",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5061663710",
      "name": "Xin Zhu",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University",
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5074255088",
      "name": "Hongyi Pan",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5114601748",
      "name": "Deniz Seyithanoğlu",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5112971868",
      "name": "Alpay Medetalibeyoğlu",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5043592090",
      "name": "Vanshali Sharma",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5023724443",
      "name": "Vedat Çiçek",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5048271656",
      "name": "Amir Ali Rahsepar",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5110843786",
      "name": "Rutger Hendrix",
      "affiliations": [
        "University of Catania",
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5080469744",
      "name": "Ahmet Enis Çetin",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5046936472",
      "name": "Bulent Aydogan",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5085120914",
      "name": "Mohamed E. Abazeed",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5082906150",
      "name": "Frank H. Miller",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5007181969",
      "name": "Rajesh N. Keswani",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5073183294",
      "name": "Hatice Savas",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5012995283",
      "name": "Sachin Jambawalikar",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5008135666",
      "name": "Daniela P. Ladner",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5064936135",
      "name": "Amir A. Borhani",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5075815307",
      "name": "Concetto Spampinato",
      "affiliations": [
        "University of Catania",
        "Intel (United States)",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5010559287",
      "name": "Michael B. Wallace",
      "affiliations": [
        "Intel (United States)",
        "Jacksonville College",
        "Mayo Clinic in Florida",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A5034879156",
      "name": "Ulas Bagci",
      "affiliations": [
        "Intel (United States)",
        "Northwestern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4405515535",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2406522070",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2126204609",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W3172881192",
    "https://openalex.org/W4382323427",
    "https://openalex.org/W4391109864",
    "https://openalex.org/W4319794169",
    "https://openalex.org/W4388092751",
    "https://openalex.org/W6854234133",
    "https://openalex.org/W6766263406",
    "https://openalex.org/W2765793020",
    "https://openalex.org/W2295124130",
    "https://openalex.org/W6737947904",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2807006176",
    "https://openalex.org/W4392873784",
    "https://openalex.org/W4385890096",
    "https://openalex.org/W4311415873",
    "https://openalex.org/W4320712900",
    "https://openalex.org/W4310396525",
    "https://openalex.org/W6779997284",
    "https://openalex.org/W6846462235",
    "https://openalex.org/W6864613773",
    "https://openalex.org/W4392595892",
    "https://openalex.org/W3135809943",
    "https://openalex.org/W6767327189",
    "https://openalex.org/W3080182903",
    "https://openalex.org/W6728551298",
    "https://openalex.org/W3125325941",
    "https://openalex.org/W6748382702",
    "https://openalex.org/W2892038960",
    "https://openalex.org/W3033032634",
    "https://openalex.org/W6771536673",
    "https://openalex.org/W4392599656",
    "https://openalex.org/W4371783954",
    "https://openalex.org/W4391854717",
    "https://openalex.org/W4386108731",
    "https://openalex.org/W4293724069",
    "https://openalex.org/W4282971760",
    "https://openalex.org/W4380479495",
    "https://openalex.org/W4315880904",
    "https://openalex.org/W4385484319",
    "https://openalex.org/W2959564100",
    "https://openalex.org/W1987790895",
    "https://openalex.org/W4401615053",
    "https://openalex.org/W6860318346",
    "https://openalex.org/W4390953412",
    "https://openalex.org/W4382929818",
    "https://openalex.org/W4390223162",
    "https://openalex.org/W4385990845",
    "https://openalex.org/W4294106961",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W4390833061",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4382366693",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2530395818",
    "https://openalex.org/W4386246120"
  ],
  "abstract": "The emergence of foundational models represents a paradigm shift in medical imaging, offering extraordinary capabilities in disease detection, diagnosis, and treatment planning. These large-scale artificial intelligence systems, trained on extensive multimodal and multi-center datasets, demonstrate remarkable versatility across diverse medical applications. However, their integration into clinical practice presents complex ethical challenges that extend beyond technical performance metrics. This study examines the critical ethical considerations at the intersection of healthcare and artificial intelligence. Patient data privacy remains a fundamental concern, particularly given these models' requirement for extensive training data and their potential to inadvertently memorize sensitive information. Algorithmic bias poses a significant challenge in healthcare, as historical disparities in medical data collection may perpetuate or exacerbate existing healthcare inequities across demographic groups. The complexity of foundational models presents significant challenges regarding transparency and explainability in medical decision-making. We propose a comprehensive ethical framework that addresses these challenges while promoting responsible innovation. This framework emphasizes robust privacy safeguards, systematic bias detection and mitigation strategies, and mechanisms for maintaining meaningful human oversight. By establishing clear guidelines for development and deployment, we aim to harness the transformative potential of foundational models while preserving the fundamental principles of medical ethics and patient-centered care.",
  "full_text": "TYPE Review\nPUBLISHED /one.tnum/nine.tnum May /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nOPEN ACCESS\nEDITED BY\nFilippo Gibelli,\nUniversity of Camerino, Italy\nREVIEWED BY\nIvan Šoša,\nUniversity of Rijeka, Croatia\nNeeraj Kumar Pandey,\nGraphic Era University, India\nShadrack Katuu,\nUniversity of South Africa, South Africa\nJax Luo,\nHarvard Medical School, United States\n*CORRESPONDENCE\nUlas Bagci\nulas.bagci@northwestern.edu\nRECEIVED /one.tnum/three.tnum December /two.tnum/zero.tnum/two.tnum/four.tnum\nACCEPTED /zero.tnum/five.tnum March /two.tnum/zero.tnum/two.tnum/five.tnum\nPUBLISHED /one.tnum/nine.tnum May /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nJha D, Durak G, Das A, Sanjotra J, Susladkar O,\nSarkar S, Rauniyar A, Kumar Tomar N, Peng L,\nLi S, Biswas K, Aktas E, Keles E, Antalek M,\nZhang Z, Wang B, Zhu X, Pan H,\nSeyithanoglu D, Medetalibeyoglu A, Sharma V,\nCicek V, Rahsepar AA, Hendrix R, Cetin AE,\nAydogan B, Abazeed M, Miller FH, Keswani RN,\nSavas H, Jambawalikar S, Ladner DP,\nBorhani AA, Spampinato C, Wallace MB and\nBagci U (/two.tnum/zero.tnum/two.tnum/five.tnum) Ethical framework for\nresponsible foundational models in medical\nimaging. Front. Med./one.tnum/two.tnum:/one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Jha, Durak, Das, Sanjotra, Susladkar,\nSarkar, Rauniyar, Kumar Tomar, Peng, Li,\nBiswas, Aktas, Keles, Antalek, Zhang, Wang,\nZhu, Pan, Seyithanoglu, Medetalibeyoglu,\nSharma, Cicek, Rahsepar, Hendrix, Cetin,\nAydogan, Abazeed, Miller, Keswani, Savas,\nJambawalikar, Ladner, Borhani, Spampinato,\nWallace and Bagci. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License (CC\nBY)\n. The use, distribution or reproduction in\nother forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication\nin this journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nEthical framework for\nresponsible foundational models\nin medical imaging\nDebesh Jha /one.tnum, Gorkem Durak /one.tnum, Abhijit Das /one.tnum, Jasmer Sanjotra /one.tnum,\nOnkar Susladkar /one.tnum, Suramyaa Sarkar /one.tnum, Ashish Rauniyar /two.tnum,\nNikhil Kumar Tomar /one.tnum, Linkai Peng /one.tnum, Sirui Li /one.tnum, Koushik Biswas /one.tnum,\nErtugrul Aktas /one.tnum, Elif Keles /one.tnum, Matthew Antalek /one.tnum, Zheyuan Zhang /one.tnum,\nBin Wang/one.tnum, Xin Zhu /one.tnum,/three.tnum, Hongyi Pan /one.tnum, Deniz Seyithanoglu /one.tnum,\nAlpay Medetalibeyoglu /one.tnum, Vanshali Sharma /one.tnum, Vedat Cicek /one.tnum,\nAmir A. Rahsepar /one.tnum, Rutger Hendrix /one.tnum,/four.tnum, A. Enis Cetin /three.tnum,\nBulent Aydogan/five.tnum, Mohamed Abazeed /six.tnum, Frank H. Miller /one.tnum,\nRajesh N. Keswani /one.tnum,/seven.tnum, Hatice Savas /one.tnum, Sachin Jambawalikar /eight.tnum,\nDaniela P. Ladner /nine.tnum, Amir A. Borhani /one.tnum, Concetto Spampinato /one.tnum,/four.tnum,\nMichael B. Wallace /one.tnum,/one.tnum/zero.tnumand Ulas Bagci /one.tnum*\n/one.tnumMachine and Hybrid Intelligence Lab, Department of Radiology, N orthwestern University, Chicago, IL,\nUnited States, /two.tnumSustainable Communication Technologies, SINTEF Digital, Trondhe im, Norway,\n/three.tnumDepartment of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL,\nUnited States, /four.tnumDepartment of Electrical and Computer Engineering, University of Catania, Catania,\nItaly, /five.tnumDepartment of Radiation Oncology, University of Chicago, Chicago, IL , United States,\n/six.tnumDepartment of Radiation Oncology, Northwestern University, Chic ago, IL, United States, /seven.tnumDepartment\nof Gastroenterology and Hepatology, Northwestern University, Chica go, IL, United States,\n/eight.tnumDepartment of Radiology, Columbia University, New York City, NY, Un ited States, /nine.tnumComprehensive\nTransplant Center, Feinberg School of Medicine, Northwestern University Transplant Outcomes\nResearch Collaborative (NUTORC), Northwestern University, Chi cago, IL, United States, /one.tnum/zero.tnumDepartment\nof Gastroenterology and Hematology, Mayo Clinic Florida, Jacksonvill e, FL, United States\nThe emergence of foundational models represents a paradigm shi ft in medical\nimaging, oﬀering extraordinary capabilities in disease dete ction, diagnosis, and\ntreatment planning. These large-scale artiﬁcial intelligence systems, trained\non extensive multimodal and multi-center datasets, demonstr ate remarkable\nversatility across diverse medical applications. However, the ir integration into\nclinical practice presents complex ethical challenges that extend b eyond\ntechnical performance metrics. This study examines the critical eth ical\nconsiderations at the intersection of healthcare and artiﬁcial int elligence. Patient\ndata privacy remains a fundamental concern, particularly given th ese models’\nrequirement for extensive training data and their potential to inadvertently\nmemorize sensitive information. Algorithmic bias poses a si gniﬁcant challenge\nin healthcare, as historical disparities in medical data collecti on may perpetuate\nor exacerbate existing healthcare inequities across demographi c groups. The\ncomplexity of foundational models presents signiﬁcant challen ges regarding\ntransparency and explainability in medical decision-making. We propose a\ncomprehensive ethical framework that addresses these challenge s while\npromoting responsible innovation. This framework emphasize s robust privacy\nsafeguards, systematic bias detection and mitigation strat egies, and mechanisms\nfor maintaining meaningful human oversight. By establishin g clear guidelines for\ndevelopment and deployment, we aim to harness the transformat ive potential\nFrontiers in Medicine /zero.tnum/one.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nof foundational models while preserving the fundamental pri nciples of medical\nethics and patient-centered care.\nKEYWORDS\nfoundational models, ethical AI, responsible AI, medical imaging, fairness\n/one.tnum Introduction\nRecent advancements in artiﬁcial intelligence (AI) have been\ncatalyzed by the emergence of foundational models (FMs) (1)\nlarge-scale architectures capable of generalizing across diverse\napplications with signiﬁcantly reduced data requirements\ncompared to traditional deep learning paradigms\n(2, 3) . These\nmodels, which leverage massive parameter spaces and extensive\ntraining datasets, have achieved remarkable performance even\nwhen using only a tenth of the conventional data volume\n(4). This\ntransformative progress is largely driven by two key innovations:\n(1) the convergence of high-performance computing and scalable\nparallel architectures and (2) the adoption of self-supervised\nlearning strategies, particularly those based on the transformer\narchitecture\n(5).\n/one.tnum./one.tnum The evolution of foundational models\nin medical imaging\nThe theoretical underpinnings of FMs rest on two key machine\nlearning paradigms: transfer learning\n(6) and unsupervised\nlearning (7). While traditional medical imaging has relied heavily\non vision-speciﬁc architectures such as convolutional neural\nnetworks (CNNs) and vision transformers, these approaches face\nsigniﬁcant limitations\n(8, 9) . The conventional fully-supervised\nlearning paradigm demands substantial annotated datasets, making\nit resource-intensive and time-consuming. Furthermore, these\nmodels typically specialize in single tasks, such as segmentation or\nclassiﬁcation, and operate within a single modality.\nThis single-modality constraint presents a fundamental\nmismatch with real-world healthcare workﬂows, where clinicians\nroutinely integrate multiple information sources including clinical\nnotes, diagnostic reports, and various investigative ﬁndings to make\ninformed decisions. FMs for computer-aided diagnosis (CAD)\nrepresent a strategic shift toward addressing these limitations\nwhile maintaining crucial considerations of patient privacy,\nmodel transparency, and ethical implementation. The evolution\nfrom traditional deep learning approaches to FMs mirrors the\ncomplexity of actual clinical decision-making, where the synthesis\nof diverse information sources drives diagnostic accuracy and\ntreatment planning\n(10, 11) . This transition is particularly\nconsequential in medical imaging, where diagnostic accuracy\nis contingent on integrating heterogeneous information. For\nexample, radiologists rely on multimodal inputs—imaging scans,\nclinical histories, and laboratory results—to reﬁne diﬀerential\ndiagnoses. FMs hold the potential to revolutionize this process by\nenhancing diagnostic precision, automating complex tasks, and\npersonalizing treatment strategies at an unprecedented scale.\n/one.tnum./two.tnum Ethical and practical challenges\nDespite the remarkable achievements of FMs and large vision\nmodels (LVMs) in medical applications\n(12, 13) , their widespread\nadoption raises signiﬁcant ethical and societal concerns that\ndemand careful consideration. The substantial data requirements\nfor training these models present complex challenges regarding\npatient privacy and data conﬁdentiality. Medical datasets contain\nhighly sensitive information, including detailed health histories and\ngenetic data, necessitating robust protection mechanisms beyond\ntraditional security measures. A more nuanced challenge emerges\nfrom inherent biases within training datasets. These biases can\nmanifest in various forms, potentially leading to discriminatory\noutcomes based on demographic factors such as race, gender, and\nsocioeconomic status. Such biases not only compromise diagnostic\naccuracy but also risk perpetuating existing healthcare disparities\nwhen deployed in clinical settings. The accountability for these\nbiased outcomes becomes particularly complex given the multiple\nstakeholders involved in developing and deploying medical FMs.\nThe generative capabilities of modern FMs introduce additional\nlayers of ethical complexity, particularly regarding potential misuse\nand legal liability. The inherent opacity of these sophisticated\nmodels, often characterized as “blackbox”, necessitates advanced\nexplainable AI techniques to establish trust among healthcare\nproviders and patients alike. This transparency is crucial for clinical\nadoption and regulatory compliance. Hence, FMs in medical\nimaging face several interconnected challenges, summarized brieﬂy\nas follows:\n(i) Data scarcity. A fundamental constraint lies in the scarcity\nof high-quality annotated medical images, which limits the\ntraining capabilities of these sophisticated models.\n(ii) Variation. This challenge is compounded by the inherent\ncomplexity of medical imaging data, where high-resolution\nvolumetric scans display signiﬁcant anatomical variations\nbetween individuals, making it diﬃcult to develop models that\ngeneralize eﬀectively across diverse patient populations.\n(iii) Heterogeneous data. The heterogeneous nature of medical\nimaging data presents another layer of complexity. Healthcare\nfacilities utilize various imaging devices and follow diﬀerent\nprotocols, resulting in a diverse array of data formats\nand characteristics. This variability in imaging modalities\nand acquisition parameters creates substantial challenges for\ndeveloping uniﬁed models that can process and interpret such\ndiverse inputs eﬀectively.\n(iv) Computational cost. Scalability emerges as a critical\noperational challenge in implementing medical FMs. These\nsophisticated models demand substantial computational\nresources, leading to extended processing times and increased\noperational costs. This resource-intensive nature can\npotentially limit their practical deployment in clinical settings\nFrontiers in Medicine /zero.tnum/two.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nwhere rapid analysis and cost-eﬀectiveness are crucial\nconsiderations.\n(v) Ethics and reliability. Beyond these technical challenges,\nethical considerations and reliability concerns pose signiﬁcant\nhurdles. The handling of sensitive patient data necessitates\nrobust privacy and security measures while ensuring data\nintegrity remains paramount. The reliability of FM outputs\nfaces particular scrutiny in medical contexts, where the stakes\nare exceptionally high.\n(vi) Susceptibility. Moreover, these models’ vulnerability to\nadversarial attacks raises serious concerns\n(14), given that\nmedical decisions can have profound implications for patient\noutcomes.\nThese challenges span both domain-speciﬁc and\ngeneral considerations\n(15). Table 1 presents a real-\nworld example and the corresponding solution for\neach challenge.\n/one.tnum./three.tnum A framework for ethical AI in medicine\nTo address these challenges, we propose a comprehensive\nethical framework integrating federated learning, bias\nmitigation techniques, and explainability modules. This\nframework emphasizes:\n1. Ethical AI development: we present an ethical\nframework that guides the responsible development\nand implementation of FMs in medicine. We propose\nto implement privacy-preserving methodologies such as\nhomomorphic encryption and decentralized learning to protect\npatient conﬁdentiality.\n2. Fairness & equity: establishing robust bias detection and\nmitigation strategies to prevent discriminatory outcomes.\n3. Transparency & clinical trust: leveraging interpretable AI\nmechanisms and clinician-AI collaboration to foster adoption\nand regulatory compliance.\nThis work aims to set the foundation for responsible AI\nintegration in medicine, ensuring that FMs enhance clinical\ndecision-making without compromising ethical integrity or patient\nsafety. The innovation of this paper lies in its comprehensive\nethical framework for medical FMs, integrating privacy-\npreserving techniques (e.g., federated learning, homomorphic\nencryption), fairness-aware training, and explainable AI to\naddress critical challenges in medical AI deployment. Unlike\nconventional deep learning models that rely on single-task,\nsingle-modality architectures, this work presents a framework\nwith a multi-modal, multi-task paradigm that aligns with\nreal-world clinical decision-making. Additionally, we propose\na systematic bias auditing and regulatory compliance strategy,\nensuring that FMs promote equitable, transparent, and trustworthy\nAI-driven healthcare.\nIn the following sections, we provide a detailed examination\nof these challenges and their implications for the development\nand deployment of medical imaging FMs. This analysis serves\nas a foundation for understanding the complex landscape of AI\nimplementation in healthcare.\n/two.tnum Method\nThe societal implications of FMs in healthcare extend beyond\nindividual applications, encompassing broader ecosystem-wide\neﬀects that scale with model deployment. As illustrated in\nFigure 1,\nthe ethical considerations surrounding large-scale FM adoption\nin medical settings present both challenges and opportunities\nfor systematic improvement. These ethical dimensions can be\nsystematically evaluated and optimized through quantiﬁable\nmetrics that promote transparency, maintain data integrity, and\nensure equitable outcomes across diverse patient populations.\nOur comprehensive analysis and subsequent proposals\nestablish a robust framework for developing and implementing\nethically sound FMs in biomedical artiﬁcial intelligence. This\nframework addresses not only the technical aspects of model\ndevelopment but also the broader societal responsibilities\ninherent in deploying AI systems in healthcare. By focusing\non measurable ethical criteria and clear governance structures,\nwe aim to create a sustainable and responsible FM ecosystem\nthat serves the healthcare community while protecting patient\ninterests. This approach represents a critical step toward\nharmonizing technological advancement with ethical imperatives\nin medical AI, setting a foundation for future developments\nthat prioritize both innovation and responsibility. The following\nsections detail our analysis and recommendations for achieving\nthis balance.\n/two.tnum./one.tnum Glass box FMs: toward transparency\nThe growing emphasis on glass-box models in healthcare\nrepresents a crucial shift toward interpretable artiﬁcial intelligence,\naddressing major requirements for trust and transparency in\nmedical decision-making\n(16). These models provide essential\ninsights into their decision-making processes, making them\nparticularly valuable in clinical settings where understanding the\nreasoning behind AI recommendations is paramount. Healthcare\nprofessionals’ conﬁdence in AI systems fundamentally depends on\ntheir ability to comprehend the underlying decision mechanisms.\nThis transparency enables clinicians to eﬀectively integrate AI\nassistance into their practice while maintaining their professional\njudgment and accountability. Similarly, patient acceptance of AI-\ndriven healthcare recommendations signiﬁcantly increases when\nthe decision-making process is transparent and comprehensible,\nfostering a more trusting relationship between patients, healthcare\nproviders, and AI systems.\nSeveral sophisticated tools and techniques have emerged to\nenhance the interpretability of foundational models in commercial\nmedical applications. These include Gradient-weighted Class\nActivation Mapping (CAM) methods\n(17, 18) , which visualize\nregions of interest in medical images that inﬂuence model\ndecisions. Principle component analysis oﬀers a gradient-\nindependent approach to understanding data patterns\n(19), while\nSHAP (SHapley Additive exPlanations) (20) and LIME (Local\nInterpretable Model-agnostic Explanations) (21) provide detailed\ninsights into model predictions. These visual reasoning techniques\ncollectively enable a deeper understanding of how FMs process and\nFrontiers in Medicine /zero.tnum/three.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nTABLE /one.tnum Challenges, examples, and solutions in medical imaging.\nChallenge Real-world example Real-world solution\nData scarcity A rare disease imaging dataset has only a few dozen\nannotated examples, making it diﬃcult to train a robust AI\nmodel for diagnosis.\nUtilize transfer learning by leveraging pre-trained models on lar ge general\nmedical imaging datasets and ﬁne-tune them for rare disease s. Use data\naugmentation techniques (e.g., rotation, ﬂipping, scaling) to a rtiﬁcially\nincrease the diversity of the dataset.\nVariation Chest X-rays from patients of diﬀerent ethnicities show\nsigniﬁcant diﬀerences in anatomical features and disease\nmanifestations, leading to inconsistent model performance.\nTrain models on diverse and representative datasets that inclu de data from\nmultiple demographics. Implement domain adaptation techniques to\nimprove the model’s generalization across varied patient populati ons.\nRegular validation on diverse test sets is essential.\nHeterogeneous data MRI scans from diﬀerent hospitals vary due to diﬀerent\nimaging protocols, machine types, and acquisition settings,\ncausing challenges in creating a standardized analysis model.\nDevelop and apply normalization and harmonization techniques to\npreprocess data to a common format and quality. Use federated lear ning to\ntrain models on decentralized data while maintaining patient pri vacy and\nimproving model robustness across heterogeneous data sourc es.\nComputational cost Running a large AI model to analyze MRI scans is slow on\nhigh-end hardware, delaying critical diagnoses in emergenc y\nscenarios.\nOptimize models using techniques such as model pruning and\nquantization to reduce size and computation. Incorporate edge computing\nfor real-time analysis where possible and leverage cloud-based plat forms\nwith scalable resources for handling large-scale computations.\nEthics and reliability A misdiagnosis by an AI system in detecting a malignant\ntumor could lead to incorrect treatment, raising ethical and\ntrust issues among clinicians and patients.\nImplement rigorous validation and explainability mechanisms to e nsure\ntransparency and reliability. Incorporate human-in-the-loop s ystems\nwhere clinicians review and validate AI predictions. Establish robust\npatient consent protocols and maintain high standards of data e ncryption\nand privacy measures to ensure compliance with healthcare regula tions.\nSusceptibility An adversarial attack alters a medical image subtly, causing\nthe AI model to misclassify a benign condition as malignant,\nleading to unnecessary surgeries.\nEnhance model security through adversarial training, wher e the model is\nexposed to and learns from adversarial examples during training . Monitor\nmodel outputs for anomalies and use robust veriﬁcation system s to ﬂag\nunexpected predictions for human review. Regularly update models to\ndefend against emerging adversarial techniques.\nFIGURE /one.tnum\nComprehensive workﬂow of a foundational model in medical imaging: from m ulti-modal data acquisition and curation to deployment and\ndownstream tasks for higher order skills and vision tasks.\nanalyze medical data, making their decisions more transparent and\ntrustworthy for both healthcare providers and patients.\nA clinical scenario where the model trained on a biased\nmodel due to an imbalanced training dataset. For example,\nconsider a model trained predominantly on male patients\nwith hypertrophic cardiomyopathy (HCM). When deployed\nin the real world, the model may fail to detect HCM in\nfemale patients due to underlying gender-based biases in the\ntraining data. By incorporating interpretable AI, clinicians can\nidentify and understand these biases. For instance, interpretable\nAI might reveal that the model underweights key diagnostic\nfeatures in female patients. This insight allows physicians\nto adjust their clinical decisions and provides feedback to\nretrain the model with more diverse and representative data,\nthereby improving future diagnostic accuracy and reducing\ngender-related disparities.\nFrontiers in Medicine /zero.tnum/four.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nFIGURE /two.tnum\nIllustration of the federated learning paradigm, in which mul tiple institutions collaboratively train deep learning models in a decentralized framework\nwithout exchanging raw patient data. Each institution indepen dently updates its local model using private datasets and tran smits only model\nparameters to a central aggregation server. The server securely integrates these updates to reﬁne a global model, which is then re distributed to\nparticipating sites. This iterative process preserves data privacy, maintains data locality, and mitigates risks relate d to bias and fairness while ensuring\nrobust model generalization across diverse clinical settings.\n/two.tnum./two.tnum Federated learning: ensuring privacy\nThe exceptional performance of FMs in medical applications\nheavily depends on access to extensive, high-quality training\ndata. However, the medical ﬁeld faces a critical challenge in\ndata availability, particularly given the sensitive nature of patient\ninformation and the time-intensive process of curating private\nmedical datasets. This constraint has led to the emergence\nof federated learning as a transformative solution for medical\nAI development. Federated learning represents a paradigm\nshift in how medical FMs can be trained while preserving\npatient privacy. This approach enables the development of\nrobust models by leveraging distributed data sources across\nmultiple healthcare institutions without requiring centralized data\nstorage (\nFigure 2). The key innovation lies in its ability to\nkeep sensitive patient data securely within its original location\nwhile allowing the model to learn from multiple sources\nsimultaneously. This distributed architecture addresses not only\nprivacy concerns but also regulatory compliance requirements in\nhealthcare.\nMoreover, federated learning provides an elegant solution to\nthe Non-IID (Non-Independent and Identically Distributed)\n(22)\nchallenge that frequently occurs in medical datasets. By\nimplementing granular controls over data sharing and model\nupdates, healthcare institutions can maintain oversight of their\ncontributions while beneﬁting from collaborative learning. This\napproach facilitates the development of more inclusive and\nrepresentative models by incorporating diverse patient populations\nacross diﬀerent healthcare settings. The resulting federated FMs\ndemonstrate enhanced fairness and reduced bias, as they can learn\nfrom a broader spectrum of medical data while respecting privacy\nboundaries and institutional protocols.\nFrontiers in Medicine /zero.tnum/five.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\n/two.tnum./three.tnum LLMs: facilitating regulatory\ncompliance\nLarge Language Models (LLMs) are revolutionizing computer-\naided diagnosis (CAD) systems by bridging the gap between\nvisual analysis and clinical documentation. Models like LLaMa\nand Komodo-7b\n(23) demonstrate remarkable capabilities\nin transforming unstructured medical information into\ncomprehensive, standardized formats. This transformation\nextends beyond simple text generation\n(24), encompassing crucial\nhealthcare applications including the creation of detailed Electronic\nHealth Records (EHRs), clinical trial analysis, drug discovery\nprocesses, biomarker identiﬁcation, and the enhancement of\nClinical Decision Support Systems (CDSS)\n(25).\nThe integration of LLMs into healthcare workﬂows addresses\ncritical regulatory compliance requirements while improving\ndocumentation eﬃciency. These models excel at generating\nstructured medical records that adhere to stringent privacy\nand security regulations, signiﬁcantly reducing the risk of\nnon-compliance penalties. The implementation of sophisticated\nprivacy-preserving techniques, such as diﬀerential privacy, adds\nan essential layer of security by introducing controlled noise\ninto training data, thereby protecting patient conﬁdentiality while\nmaintaining data utility.\nThe ongoing clinical trials of LLM applications in healthcare\nsettings serve a dual purpose: validating their eﬀectiveness in\nreal-world scenarios and ensuring compliance with regulatory\nframeworks, particularly the Health Insurance Portability and\nAccountability Act (HIPAA). This rigorous evaluation process\nhelps establish LLMs as reliable tools that can enhance healthcare\ndelivery while maintaining the highest standards of patient\nprivacy and data security. The successful integration of these\nmodels demonstrates how advanced AI technologies can support\nhealthcare professionals in delivering more eﬃcient and compliant\ncare.\n/two.tnum./four.tnum Generative AI: generalization with\nprivacy\nGenerative models\n(26) have emerged as a powerful solution\nto several fundamental challenges in medical AI, particularly\naddressing the critical issue of data scarcity in training foundational\nmodels (FMs). These models excel at creating synthetic medical\ndata that closely mirrors real patient information, eﬀectively\nexpanding training datasets while circumventing privacy and\nconsent concerns inherent in using actual patient data. By\ngenerating diverse synthetic samples that represent various\ndemographic and clinical characteristics, these models help\nestablish more balanced and representative training datasets.\nVariational autoencoders (V AEs)\n(27) represent a particularly\nsophisticated application of generative modeling in healthcare.\nTheir ability to predict missing values and generate synthetic\npatient trajectories enhances the robustness of FMs by providing\nmore complete and diverse training data\n(28). This capability\nproves especially valuable in medical settings where incomplete\nor missing data often poses signiﬁcant challenges to model\ndevelopment and deployment.\nRecent advances in self-supervised learning have further\nenhanced the potential of generative approaches. Notable work by\nGhesu and colleagues demonstrated the eﬀectiveness of combining\ncontrastive learning with online feature clustering for dense feature\nlearning in FMs\n(29). Their hybrid approach, building upon earlier\nself-supervised techniques, achieves robust feature representations\nby mapping them to cluster prototypes through both supervised\nand self-supervised learning mechanisms\n(30).\nThe integration of generative techniques with FMs has\nyielded remarkable results (31, 32) , as exempliﬁed by models like\nMedSAM (13), which demonstrates superior performance through\ngenerative AI-based encoding-decoding architectures. This success\nextends to applications in generative image modeling, where\nsynthetic data is used for pretraining and inference on real-\nworld medical data, leading to optimized FM performance. These\nadvances not only improve model accuracy but also incorporate\ncrucial ethical considerations by emphasizing privacy-preserving\ndata generation methods and bias reduction strategies.\n/two.tnum./five.tnum Fairness, biases, and risks with\ngenerative models\nThe transformative potential of generative AI in healthcare is\naccompanied by signiﬁcant ethical challenges that demand careful\nconsideration. These models can inadvertently amplify existing\nsocial biases across multiple dimensions including race, gender,\nand socioeconomic status, potentially leading to discriminatory\noutcomes in medical decision-making\n(33). The sophisticated\nnature of these technologies raises particular concerns about\ntheir role in perpetuating or exacerbating existing healthcare\ndisparities. The risk extends beyond bias ampliﬁcation to include\nmore direct threats to public trust and safety. The capability\nof generative AI to create convincing deepfakes and propagate\nmedical misinformation presents serious challenges to healthcare\ncommunication and patient trust. These issues are particularly\nconcerning in medical contexts, where accurate information is\ncrucial for patient care and public health decisions. The potential\nfor societal harm increases when these technologies trigger public\nhostility or erode trust in healthcare institutions\n(34).\nAddressing these challenges requires a comprehensive\napproach that prioritizes ethical considerations over purely\ntechnological advancement. Organizations developing medical\nAI systems must shift their focus from maximizing model\nperformance to actively minimizing bias and potential harm.\nThis paradigm shift emphasizes the importance of building\ntrustworthy systems that serve all populations equitably, rather\nthan pursuing technological capabilities at the expense of ethical\nconsiderations. The path forward requires early intervention in\nAI education and development, embedding responsible usage\nprinciples at fundamental stages of both technical training\nand clinical implementation. This approach must also address\nthe broader socioeconomic implications of AI deployment in\nhealthcare, particularly the risk of creating or widening digital\nFrontiers in Medicine /zero.tnum/six.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nTABLE /two.tnum Methods for measuring fairness, bias, privacy, and diversity of generations.\nMethod References Metric Performance\nFairness-constrained (38) Equalized Odds, Demographic Parity Fairness vs. accuracy trade-oﬀs\nFairness auditing (39) Bias Detection Continuous monitoring needed\nGender classiﬁcation (40) Intersectional Accuracy Varied accuracy; higher errors for\ndarker-skinned females\nFederated learning (41) Fairness, Privacy Fairness with data privacy\nPrivate GANs (42) Privacy, Fidelity Private data with good ﬁdelity\nMulti-modal foundation (43) Gini Coeﬃcient, Shannon Diversity High diversity and balanced\nrepresentations\nFair representation (44) Stat. Parity, Equalized Odds Balanced fairness metrics\ndivides that favor well-resourced healthcare systems while\npotentially disadvantaging others. Success in this endeavor\ndemands active collaboration among healthcare providers, AI\ndevelopers, policymakers, and patient advocates to ensure that\ngenerative AI advances medical care while upholding ethical\nprinciples and promoting equitable access.\n/two.tnum./six.tnum Methods for measuring fairness, bias,\nprivacy, and diversity of generations\nThe development of ethical generative AI systems in\nhealthcare demands a rigorous approach to ensuring fairness\nand equity in model outcomes. A fundamental principle is\nthat these systems should deliver consistent results for similar\nmedical cases, independent of demographic factors such as race,\ngender, or socioeconomic status. This objective necessitates the\nimplementation of sophisticated fairness metrics and systematic\nalgorithmic audits to identify and address potential biases in both\ntraining data and model outputs.\nPrivacy protection in medical AI systems can be achieved\nthrough a multi-layered approach combining advanced techniques\nsuch as data anonymization with strategic noise injection\nand federated learning architectures\n(35). These methods\neﬀectively minimize the risk of data breaches while maintaining\nmodel performance. The evaluation of model fairness employs\nquantitative measures such as the Gini coeﬃcient and Shannon\ndiversity index\n(36), which provide objective metrics for assessing\noutput diversity and detecting potential biases ( Table 2). Higher\ndiversity scores typically indicate more inclusive and less\nhomogeneous model behavior across diﬀerent demographic\ngroups.\nThe integration of these evaluation techniques throughout\nthe entire development life-cycle ensures continuous monitoring\nof fairness, bias, and diversity metrics\n(7). This systematic\napproach is essential for maintaining consistent performance\nacross all patient populations. Achieving truly inclusive AI systems\nrequires deliberate eﬀorts to incorporate diverse representation in\nboth training data and development teams, thereby preventing\nperformance disparities that could disadvantage speciﬁc patient\ngroups.\nThe challenge of addressing historical and societal biases\nin medical data requires a combination of technical solutions\nand social awareness\n(37). Through rigorous bias auditing and\nsophisticated debiasing techniques, developers can work to\nneutralize these embedded prejudices. Success in this endeavor\nrequires meaningful collaboration between technologists,\nhealthcare professionals, and social scientists, ensuring that\nmedical AI systems serve all populations eﬀectively and ethically.\n/two.tnum./seven.tnum Copyright concerns\nThe intersection of generative AI and copyright law presents\ncomplex challenges in medical imaging and healthcare applications.\nThese AI systems’ ability to generate content that may resemble\nexisting work raises signiﬁcant questions about intellectual\nproperty rights and fair use\n(45, 46) . The challenge becomes\nparticularly nuanced in medical contexts, where the generated\ncontent could include diagnostic patterns, imaging techniques,\nor analytical methods that may be subject to existing patents or\ncopyrights.\nA balanced approach to addressing these concerns requires\ncareful consideration of both innovation and protection.\nHealthcare AI developers must implement rigorous protocols\nto ensure their training methodologies respect intellectual property\nrights, including proper attribution of source materials and\ncareful documentation of training data provenance. This challenge\nextends beyond simple compliance to fundamental questions about\nthe ownership and rights associated with AI-generated medical\ninsights and diagnostic tools\n(47, 48).\nThe evolving nature of AI technology necessitates new legal\nframeworks that can eﬀectively address these emerging challenges\nwhile fostering innovation in healthcare. This requires sustained\ncollaboration between multiple stakeholders: technologists who\nunderstand the technical capabilities and limitations of generative\nAI, legislators who can craft appropriate regulatory frameworks,\nand legal experts who can interpret and apply these frameworks in\nthe context of existing intellectual property law\n(49).\nThe path forward demands aggressive yet thoughtful action to\nestablish clear guidelines for the ethical and legal implementation\nof generative AI in healthcare. These guidelines must balance the\nFrontiers in Medicine /zero.tnum/seven.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nSafe AI Ethical Governance \nCouncil \nDesign Conventions \nand Privacy\nAwareness \nCampaigns \nAdoption of \nUpcoming Tech \nInter-generational \nConsiderations \nHuman Oversight & \nInterventions\nMulti-stakeholder\nMananagement\nStrong and Secure \nCredentials \nFIGURE /three.tnum\nFramework of ethical AI governance components.\nimperative for technological advancement in medical care with\nthe protection of individual and institutional rights. Success in\nthis endeavor requires a comprehensive approach that considers\nnot only technical and legal aspects but also broader societal\nimplications, ensuring that the development of medical AI serves\nthe public good while respecting intellectual property rights.\n/two.tnum./eight.tnum Governance and collaboration\nThe implementation of artiﬁcial intelligence in medical imaging\ndemands a robust governance framework that places human\noversight at its core, ensuring responsible and ethical decision-\nmaking throughout the AI life-cycle\n(50, 51) . This framework\nmust begin with design-based privacy principles that protect\npatient data from the earliest stages of development, embedding\nsecurity and conﬁdentiality into the fundamental architecture of AI\nsystems\n(52).\nThe complexity of healthcare AI necessitates a multi-\nstakeholder approach to governance. By engaging diverse\nparticipants—including healthcare providers, patients,\ntechnologists, ethicists, and regulatory experts—the framework\nbeneﬁts from a rich tapestry of perspectives and experiences\n(53).\nThis inclusive approach helps identify potential challenges and\nopportunities that might be overlooked from a single viewpoint.\nSafety in medical AI systems requires a comprehensive\nvalidation protocol that includes rigorous testing, continuous\nmonitoring, and regular assessment of outcomes\n(54). The\nestablishment of an Ethical Governance Council provides crucial\noversight, ensuring that AI development and deployment align\nwith established ethical principles and clinical standards\n(55). This\ncouncil serves as a guardian of patient interests while facilitating\ntechnological advancement.\nEducational initiatives play a vital role in this framework by\nensuring all stakeholders understand both the capabilities and\nlimitations of AI systems. These awareness programs foster realistic\nexpectations and promote responsible use of AI technologies in\nclinical settings\n(51). The framework also emphasizes continuous\nimprovement, incorporating mechanisms to adapt AI systems as\nnew data becomes available and medical knowledge advances\n(52).\nA particularly forward-thinking aspect of this governance\nstructure is its consideration of intergenerational impacts. By\naddressing the needs of diﬀerent age groups and anticipating future\nhealthcare challenges, the framework ensures that AI development\nin medical imaging serves both current and future generations\nequitably\n(53). As illustrated in Figure 3, this comprehensive\napproach creates an ethical AI ecosystem that aligns technological\ninnovation with societal values and healthcare needs\n(55).\n/two.tnum./nine.tnum Balance between scaling and societal\nimpact for FMs\nThe advancement of artiﬁcial intelligence in healthcare requires\ncareful navigation of interconnected practical and ethical challenges\nto ensure that technological innovation serves societal needs\nwhile minimizing potential harm. At the foundation of these\nchallenges lies the critical issue of data quality and accessibility.\nThe development of robust AI systems depends on access to\ndiverse, representative datasets that capture the full spectrum\nof patient populations and medical conditions\n(56). However,\nthis requirement must be balanced against stringent privacy\nrequirements and ethical considerations in healthcare data\nmanagement.\nThe technical challenge of developing scalable AI systems\nextends beyond pure computational capabilities to questions\nof seamless integration with existing healthcare infrastructure.\nThese systems must operate eﬃciently within established clinical\nworkﬂows while maintaining the highest standards of reliability\nand performance. This operational complexity is compounded by\nthe imperative to maintain robust security measures that protect\nagainst data breaches and unauthorized access, particularly given\nthe sensitive nature of medical information.\nBias mitigation represents one of the most pressing ethical\nchallenges in medical AI development. The potential for AI systems\nto perpetuate or amplify existing healthcare disparities demands\ncontinuous innovation in fairness-ensuring techniques\n(57).\nThis eﬀort requires not only technical solutions but also\ndeep understanding of how societal biases can manifest in\nhealthcare data and decision-making processes. The development\nof transparent and accountable AI systems is crucial for building\ntrust among healthcare providers and patients alike.\nThe broader societal implications of AI deployment in\nhealthcare must be carefully considered and actively managed. This\nincludes addressing concerns about potential job displacement in\nthe medical sector, the responsible use of surveillance technologies,\nFrontiers in Medicine /zero.tnum/eight.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nand the risk of exacerbating existing social inequalities in healthcare\naccess. Success in navigating these challenges requires ﬁnding an\noptimal balance between technological advancement and societal\nacceptance, ensuring that AI development aligns with both clinical\nneeds and public values.\n/two.tnum./one.tnum/zero.tnum Security concerns and patient care\nThe emerging threat of “jailbreaking” in medical AI systems\nrepresents a critical vulnerability that extends beyond typical\nsecurity concerns to potentially impact patient care directly.\nThese unauthorized modiﬁcations of generative AI models\ncan compromise the entire healthcare decision-making chain,\nintroducing subtle yet dangerous alterations that may escape\nimmediate detection\n(58). The implications of such tampering are\nparticularly severe in medical imaging, where small alterations can\nlead to misdiagnosis or inappropriate treatment recommendations.\nThe risks associated with jailbreaking medical AI systems\noperate on multiple levels. At the technical level, these\nmodiﬁcations can introduce systematic errors and biases that\nundermine the model’s carefully calibrated performance. More\ncritically, from a patient safety perspective, compromised systems\nmay generate plausible-seeming but incorrect analyses, potentially\nleading to cascading errors in clinical decision-making. These\ntechnical vulnerabilities intersect with complex legal and regulatory\nrequirements, potentially violating established healthcare standards\nand patient privacy protections\n(59).\nThe ethical implications of jailbreaking strike at the heart\nof fundamental medical principles. By compromising system\nintegrity, these unauthorized modiﬁcations violate patient\nautonomy by potentially subjecting individuals to ﬂawed medical\ndecisions without their knowledge or consent. This breach of trust\nextends beyond individual patient relationships to potentially\nundermine broader public conﬁdence in AI-driven healthcare\nsolutions, threatening the advancement of beneﬁcial medical AI\napplications.\nMaintaining the integrity of medical AI systems requires a\ncomprehensive defense strategy that prioritizes patient welfare\nabove all other considerations. This necessitates collaboration\nbetween AI developers, healthcare providers, and regulatory bodies\nto establish robust security protocols and ethical guidelines. Only\nby maintaining an unwavering commitment to system integrity and\npatient safety can we preserve trust in AI-driven medical solutions\nand ensure their continued beneﬁcial development\n(60).\n/two.tnum./one.tnum/one.tnum Ethical and responsible use\nThe development of ethical foundational models in healthcare\nrequires a systematic approach to transparency and fairness that\nbegins at the earliest stages of model development. Comprehensive\ndocumentation of data collection methodologies, preprocessing\ntechniques, and model customization procedures creates a\nfoundation of accountability and enables a thorough examination\nof potential biases. This documentation serves not only as a\ntechnical record but also as a crucial tool for identifying and\naddressing potential sources of bias before they can impact patient\ncare.\nPerformance monitoring in healthcare AI must extend beyond\ntraditional accuracy metrics to encompass fairness indicators\nacross diverse patient populations. This requires sophisticated\nevaluation frameworks that can detect subtle performance\nvariations across diﬀerent demographic groups and clinical\nscenarios. The integration of advanced techniques such as data\naugmentation and algorithmic debiasing helps ensure that models\nmaintain consistent performance across all patient populations,\naddressing potential disparities before they manifest in clinical\npractice\n(61).\nData protection in medical AI demands a multi-layered\napproach that combines technical solutions with rigorous\ngovernance protocols. The implementation of diﬀerential privacy\ntechniques and federated learning architectures enables healthcare\norganizations to maintain high standards of data security while\nfacilitating necessary model improvements. Regular security audits\nserve as critical checkpoints, identifying potential vulnerabilities\nand enabling proactive implementation of protective measures\nagainst emerging threats.\nThe concept of accountability in medical AI extends beyond\ntechnical performance to encompass broader responsibilities\ntoward patient care and societal impact. This requires establishing\nclear chains of responsibility for AI-driven decisions and their\nconsequences, creating channels for stakeholder feedback, and\ndeveloping protocols for responsible model deployment. Success in\nthis endeavor requires active engagement with external entities and\na commitment to continuous improvement based on real-world\nperformance and stakeholder input.\n/three.tnum Discussion\n/three.tnum./one.tnum Critical analysis and limitations\nOur framework for ethical FMs in medical imaging, while\ncomprehensive, faces several critical challenges that warrant\ncareful consideration. First, the inherent tension between model\nperformance and interpretability remains largely unresolved.\nWhile we advocate for glass-box approaches, the increasing\ncomplexity of FMs often creates a trade-oﬀ between accuracy\nand explainability that cannot be easily reconciled with current\ntechnical solutions.\nThe proposed federated learning approach, though promising\nfor privacy preservation, introduces signiﬁcant computational\noverhead and potential degradation in model performance.\nHealthcare institutions with varying computational resources and\ndata quality may experience diﬀerent levels of beneﬁt from this\ndistributed learning paradigm, potentially exacerbating existing\nhealthcare disparities rather than mitigating them.\nA critical limitation of our framework lies in its assumption\nof standardized data collection and annotation practices across\nhealthcare institutions. The reality of medical data collection\ninvolves signiﬁcant variability in protocols, equipment calibration,\nand annotation standards. This heterogeneity may undermine\nthe eﬀectiveness of our proposed bias detection and mitigation\nstrategies.\nFrontiers in Medicine /zero.tnum/nine.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\n/three.tnum./two.tnum Practical implementation challenges\nand regulations\nThe implementation of our ethical framework faces several\npractical obstacles that require acknowledgment. The resource\nrequirements for maintaining robust privacy measures and\nconducting comprehensive bias audits may be prohibitive for\nsmaller healthcare facilities. This economic barrier could lead to a\ntwo-tiered system where only well-resourced institutions can fully\nimplement ethical AI practices. The proposed governance\nstructure, while theoretically sound, may face resistance\nfrom various stakeholders. Clinicians may view additional\noversight mechanisms as bureaucratic hurdles, while institutional\nadministrators might resist the additional costs and complexity of\nimplementing comprehensive ethical frameworks. These practical\nconsiderations could signiﬁcantly impact the real-world adoption\nof our proposed solutions.\nAI regulation is being shaped by a combination of international\norganizations and private tech giants, all of which are addressing\nthe practical implementation challenges of ethical and responsible\nAI. UNESCO\n(62), for example, focuses on global AI governance\nand ethical considerations, emphasizing the importance of human\nrights and transparency in AI deployment. Their initiatives\nhighlight the diﬃculty of ensuring compliance across diverse\nregulatory environments. Meanwhile, the European Union\n(EU)\n(63) is spearheading one of the most comprehensive AI\nregulatory eﬀorts with its AI Act, which aims to classify and\nregulate AI systems based on risk levels. However, enforcement\nacross EU member states poses logistical and legal challenges. At\nan intergovernmental level, the OECD\n(64) has established\nAI principles that emphasize fairness, transparency, and\naccountability, but translating these high-level guidelines into\nenforceable national policies remains a challenge. Private sector\nleaders are also taking steps toward AI regulation. Microsoft\npromotes “Responsible AI” frameworks, including bias mitigation\nand human oversight, but the challenge remains in integrating\nthese ethical safeguards into rapidly evolving AI products.\nSimilarly, Google’s AI principles\n(65) outline commitments to\nfairness and safety, but practical implementation is complicated\nby the need to balance innovation with regulation. Lastly,\nIBM’s focus on “Trustworthy AI” centers\n(66) on explainability\nand algorithmic fairness, yet the challenge lies in achieving\nindustry-wide standardization while ensuring business viability.\nThese varied approaches collectively aim to tackle the real-\nworld obstacles of AI governance, but each faces diﬃculties in\nenforcement, standardization, and global applicability. The key\nchallenge remains bridging the gap between regulatory ambition\nand practical implementation in AI development and deployment.\n/three.tnum./three.tnum Sociotechnical considerations\nThe broader societal implications of our framework deserve\ncritical examination. The emphasis on technical solutions to\nethical challenges may inadvertently overshadow the importance\nof human judgment and clinical expertise. There is a risk that\nover-reliance on automated systems, even those with built-in\nethical safeguards, could gradually erode the human elements of\nhealthcare delivery. Moreover, our approach to bias mitigation,\nwhile well-intentioned, may not adequately address the root causes\nof healthcare disparities. Technical solutions alone cannot resolve\nsystemic inequities deeply embedded in healthcare systems and\nsociety at large. This limitation suggests the need for our framework\nto be integrated with broader systemic changes in healthcare\ndelivery and medical education.\n/three.tnum./four.tnum Future research directions and open\nquestions\nSeveral critical questions remain unanswered and require\nfurther investigation:\n1. Scalability vs. Ethics: How can we balance the computational\ndemands of ethical AI practices with the need for rapid clinical\ndeployment?\n2. Governance Evolution: How should ethical frameworks adapt to\nemerging AI capabilities and evolving societal values?\n3. Cultural Considerations: How can our framework be adapted\nto diﬀerent healthcare systems and cultural contexts while\nmaintaining its ethical principles?\n4. Long-term Impact: What are the potential unintended\nconsequences of widespread adoption of AI-driven medical\nimaging systems on healthcare profession dynamics?\nThese questions highlight the need for ongoing critical evaluation\nand reﬁnement of our framework.\n/four.tnum Conclusion\nFoundational models represent a pivotal advancement in\nmedical imaging, promising to revolutionize diagnostic precision,\ntreatment planning, and personalized medicine. Their potential\nto transform healthcare delivery extends beyond mere technical\nimprovements, oﬀering new possibilities for personalized\nmedicine and enhanced clinical decision-making. However, this\ntechnological promise must be carefully balanced against the\ncomplex ethical challenges that emerge from their deployment in\nclinical settings. Our analysis reveals the multifaceted nature of\nthese challenges, encompassing critical concerns about patient data\nprivacy, algorithmic bias, model transparency, and professional\naccountability. The framework we propose addresses these\nchallenges through a systematic approach that integrates technical\nsolutions with ethical principles. By combining advanced privacy-\npreserving techniques, bias mitigation strategies, and robust\naccountability measures, we establish a foundation for responsible\nAI development in healthcare. The successful implementation of\nfoundational models in medical practice demands unprecedented\ncollaboration across disciplines. This includes not only technical\nexperts and healthcare professionals but also ethicists, legal\nscholars, and patient advocates. Such diverse participation ensures\nthat these powerful tools evolve in ways that respect patient rights,\npromote equitable care, and maintain the highest standards of\nmedical ethics. The responsible development of medical AI requires\nconstant vigilance and adaptation to emerging challenges. As these\ntechnologies continue to evolve, our ethical framework provides\na dynamic structure that can accommodate new developments\nFrontiers in Medicine /one.tnum/zero.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\nwhile maintaining an unwavering commitment to patient welfare.\nThis balanced approach ensures that the transformative potential\nof foundational models in healthcare can be realized while\nupholding the fundamental principles of medical ethics and\nhuman dignity.\nAuthor contributions\nDJ: Conceptualization, Investigation, Writing – original\ndraft, Writing – review & editing. GD: Conceptualization,\nInvestigation, Supervision, Writing – original draft, Writing\n– review & editing. AD: Writing – original draft. JS: Writing\n– original draft. OS: Writing – original draft. SS: Writing\n– original draft. AR: Investigation, Methodology, Writing –\noriginal draft. NK: Investigation, Formal analysis, Methodology,\nWriting – original draft. LP: Writing – original draft, Data\ncuration, Investigation. SL: Conceptualization, Methodology,\nWriting – review & editing. KB: Conceptualization, Resources,\nWriting – original draft, Methodology, Supervision. EA: Writing\n– review & editing, Investigation, Methodology, Writing –\noriginal draft, Formal analysis. EK: Investigation, Writing\n– review & editing, Conceptualization, Supervision. MAn:\nConceptualization, Writing – original draft. ZZ: Investigation,\nWriting – original draft, Validation. BW: Conceptualization,\nFormal analysis, Resources, Investigation, Writing – original draft.\nXZ: Investigation, Methodology, Resources, Writing – original\ndraft. HP: Investigation, Writing – review & editing, Methodology,\nResources. DS: Conceptualization, Writing – review & editing,\nInvestigation, Methodology, Writing – original draft. AM: Writing\n– original draft, Conceptualization, Investigation, Supervision. VS:\nConceptualization, Writing – review & editing, Formal analysis,\nMethodology, Resources, Validation. VC: Writing – review &\nediting, Formal analysis, Investigation, Supervision. AAR: Writing\n– review & editing, Investigation. RH: Conceptualization, Writing\n– review & editing, Resources. AEC: Conceptualization, Writing\n– review & editing. BA: Conceptualization, Resources, Writing –\nreview & editing, Visualization. MAb: Conceptualization, Writing –\nreview & editing, Formal analysis. FM: Conceptualization, Formal\nanalysis, Writing – review & editing. RK: Conceptualization,\nWriting – review & editing, Formal analysis, Investigation.\nHS: Conceptualization, Resources, Writing – review & editing.\nSJ: Conceptualization, Resources, Writing – review & editing,\nSupervision. DL: Conceptualization, Formal analysis, Writing\n– review & editing. AB: Conceptualization, Resources, Writing\n– review & editing. CS: Conceptualization, Formal analysis,\nInvestigation, Resources, Writing – review & editing. MW:\nConceptualization, Resources, Writing – review & editing.\nUB: Conceptualization, Formal analysis, Funding acquisition,\nInvestigation, Methodology, Resources, Supervision, Validation,\nWriting – original draft, Writing – review & editing.\nFunding\nThe author(s) declare that ﬁnancial support was received for\nthe research and/or publication of this article. This project was\nsupported by NIH funding: R01-CA246704, R01-CA240639, U01-\nCA268808, and R01-HL171376.\nConﬂict of interest\nUB acknowledges the following COI: Ther-AI LLC. MW\nacknowledges the following COIs: Boston Scientiﬁc, ClearNote\nHealth, Cosmo Pharmaceuticals, Endostart, Endiatix, Fujiﬁlm,\nMedtronic, Surgical Automations, Ohelio Ltd, Venn Bioscience,\nVirgo Inc., Surgical Automation, and Microtek. The funders had\nno role in the design of the study; in the collection, analyses, or\ninterpretation of data; in the writing of the manuscript; or in the\ndecision to publish the results.\nThe remaining authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships that\ncould be construed as a potential conﬂict of interest.\nThe author(s) declared that they were an editorial board\nmember of Frontiers, at the time of submission. This had no impact\non the peer review process and the ﬁnal decision.\nGenerative AI statement\nThe author(s) declare that Gen AI was used in the creation\nof this manuscript. The author(s) used Claude 3.7 Sonnet for\ngrammar and spell checking.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\n1. Medetalibeyoglu A, Velichko YS, Hart EM, Bagci U. Foundation al artiﬁcial\nintelligence models and modern medical practice. BJR| Artif Intellig . (2025) 2:ubae018.\ndoi: 10.1093/bjrai/ubae018\n2. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P , et a l. Language\nmodels are few-shot learners. In: Advances in Neural Information Processing Systems .\n(2020). p. 33.\n3. STAT News. Epic Overhauls Sepsis Algorithm After Finding Biases in its Trai ning\nData. (2022).\n4. Rasmy L, Xiang Y, Xie Z, Tao C, Zhi D. Med-BERT: pretrained\ncontextualized embeddings on large-scale structured electron ic health records\nfor disease prediction. NPJ Digit Med . (2021) 4:455. doi: 10.1038/s41746-021-\n00455-y\nFrontiers in Medicine /one.tnum/one.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\n5. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez I . Attention is all\nyou need. In: Advances in Neural Information Processing Systems . Long Beach, CA.\n(2017). p. 30.\n6. Thrun S. Lifelong learning algorithms. In: Learning to Learn . (1998).\n7. Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, e t al. On\nthe opportunities and risks of foundation models. arXiv [preprint] arXiv:210807258.\n(2021). doi: 10.48550/arXiv.2108.07258\n8. Chen J, Lu Y, Yu E, Wang Y, Lu L, Yuille AL, et al. Transunet: Tran sformers make\nstrong encoders for medical image segmentation. arXiv [preprint] arXiv:210204306.\n(2021). doi: 10.48550/arXiv.2102.04306\n9. Diakogiannis FI, Waldner F, Caccetta P , Wu C. ResUNet-a: A d eep learning\nframework for semantic segmentation of remotely sensed data . ISPRS J Photogrammet\nRemote Sens. (2020) 162:13. doi: 10.1016/j.isprsjprs.2020.01.013\n10. Duggan GE, Reicher JJ, Liu Y, Tse D, Shetty S. Improving refe rence\nstandards for validation of AI-based radiography. Br J Radiol . (2021) 94:20210435.\ndoi: 10.1259/bjr.20210435\n11. Institute SHCA. How Foundation Models Can Advance AI Healthcare . (2024).\n12. Lei W, Wei X, Zhang X, Li K, Zhang S. MedLSAM: Localize and se gment\nanything model for 3d medical images. arXiv [preprint] arXiv: 230614752. (2023).\ndoi: 10.48550/arXiv.2306.14752\n13. Ma J, He Y, Li F, Han L, You C, Wang B. Segment anything in med ical images.\nNat Commun. (2024) 15:654. doi: 10.1038/s41467-024-44824-z\n14. Maus N, Chao P , Wong E, Gardner J. Black box adversarial\nprompting for foundation models. arXiv [preprint] arXiv:230204237. (2023).\ndoi: 10.48550/arXiv.2302.04237\n15. Azad B, Azad R, Eskandari S, Bozorgpour A, Kazerouni A, Rek ik I, et al.\nFoundational models in medical imaging: A comprehensive surv ey and future vision.\narXiv [preprint] arXiv:231018689. (2023). doi: 10.48550/arXiv.2 310.18689\n16. Franzoni V. From black box to glass box: advancing transpare ncy in artiﬁcial\nintelligence systems for ethical and trustworthy AI. In: International Conference on\nComputational Science and Its Applications . Athens. (2023).\n17. Selvaraju RR, Cogswell M, Das R, Parikh D, Batra D. Grad-CAM: Visual\nexplanations from deep networks via gradient-based localizati on. In: Proceedings of the\nIEEE International Conference on Computer Vision . Venice: IEEE. (2017). p. 618–626.\ndoi: 10.1109/ICCV.2017.74\n18. Chattopadhay A, Sarkar A, Howlader P , Balasubramanian VN. Gr ad-CAM++:\ngeneralized gradient-based visual explanations for deep conv olutional networks. In:\n2018 IEEE Winter Conference on Applications of Computer Vision (WACV) . Lake Tahoe,\nNV: IEEE. (2018). doi: 10.1109/WACV.2018.00097\n19. Jolliﬀe IT, Cadima J. Principal component analysis: a review a nd recent\ndevelopments. Philos Trans A Math Phys Eng Sci . (2016) 374:20150202.\ndoi: 10.1098/rsta.2015.0202\n20. Lundberg SM, Lee SI. A uniﬁed approach to interpreting model predictions. In:\nAdvances in Neural Information Processing Systems . Long Beach, CA. (2017). p. 30.\n21. Ribeiro MT, Singh S, Guestrin C. “Why should i trust you?” Ex plaining the\npredictions of any classiﬁer. In: Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining . San Francisco, CA. (2016).\n22. Zhao Y, Li M, Lai L, Suda N, Civin D, Chandra V. Federated lea rning with\nnon-IID data. arXiv [preprint] arXiv:180600582. (2018). doi: 10.48550/arXiv.1 806.\n00582\n23. Owen L, Tripathi V , Kumar A, Ahmed B. Komodo: a linguistic ex pedition\ninto Indonesia’s regional languages. arXiv [preprint] arXiv:240309362. (2024).\ndoi: 10.48550/arXiv.2403.09362\n24. Shi X, Xu J, Ding J, Pang J, Liu S, Luo S, et al. LLM-Mini-CEX: Automatic\nevaluation of large language model for diagnostic conversatio n. arXiv [preprint]\narXiv:230807635. (2023). doi: 10.48550/arXiv.2308.0763 5\n25. Mishuris DW. Rebecca, Bitton A. Using electronic health re cord clinical decision\nsupport is associated with improved quality of care. Am J Manag Care . (2014)\n20:e445–52.\n26. Goodfellow I, Pouget-Abadie J, Mirza A, Bengio Y. Generative adversarial nets.\nIn: Advances in Neural Information Processing Systems . Montreal, QC. (2014). 27.\n27. Kingma DP , Welling M. Auto-encoding variational bayes. arXiv [preprint]\narXiv:13126114. (2013). doi: 10.48550/arXiv.1312.6114\n28. Esmaeili M, Toosi A, Roshanpoor A, Changizi V , Ghazisaeedi M, Rahmim\nA, et al. Generative adversarial networks for anomaly detectio n in biomedical\nimaging: A study on seven medical image datasets. IEEE Access . (2023) 11:3244741.\ndoi: 10.1109/ACCESS.2023.3244741\n29. Ghesu F, Georgescu D, Patel P , Vishwanath R, Balter J, Cao Y, et al. Self-supervised\nlearning from 100 million medical images. arXiv [preprint] arXiv:220101283. (2022).\ndoi: 10.1117/1.JMI.9.6.064503\n30. Caron M, Misra I, Mairal J, Goyal P , Bojanowski P , Joulin A. U nsupervised\nlearning of visual features by contrasting cluster assignmen ts. In: Advances in Neural\nInformation Processing Systems . (2020). p. 33.\n31. Susladkar O, Makwana D, Deshmukh G, Mittal RSC Teja, Singha l. TPFNet: a\nnovel text in-painting transformer for text removal. In: Proceedings of the International\nConference on Document Analysis and Recognition . San Jose, CA. (2023).\n32. Deshmukh G, Susladkar O, Makwana D, Mittal S, et al. Textual a lchemy:\nCoformer for scene text understanding. In: Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision . Waikoloa, HI: IEEE (2024). p.\n2931–2941.\n33. Zhu W, Huang L, Zhou X, Li J, Wang C. Could AI ethical anxiety , perceived\nethical risks and ethical awareness about AI inﬂuence univer sity students’ use of\ngenerative ai products? An ethical perspective. Int J Human-Comp Interact . (2024)\n2024:1–23. doi: 10.1080/10447318.2024.2323277\n34. Masood M, Nawaz M, Malik KM, Javed A, Irtaza A, Malik H. Deepfa kes\ngeneration and detection: State-of-the-art, open challenges , countermeasures, and way\nforward. Appl intellig. (2023) 53:1–53. doi: 10.1007/s10489-022-03766-z\n35. Mehrabi N, Morstatter N, Lerman K, Galstyan A. A survey on b ias and fairness\nin machine learning. ACM Comp Surv (CSUR). (2021) 54:3457607. doi: 10.1145/\n3457607\n36. Ojha U. Towards fairness AI: A data-centric approach. In: Politecnico di Torino .\n(2022).\n37. Kuhlman C, Jackson L, Chunara R. No computation without repr esentation:\nAvoiding data and algorithm biases through diversity. arXiv [preprint]\narXiv:200211836. (2020). doi: 10.1145/3394486.3411074\n38. Hardt M, Price E, Srebro N. Equality of opportunity in supervis ed learning. In:\nAdvances in Neural Information Processing Systems . Barcelona. (2016). p. 29.\n39. Veale M, Binns R. Fairer machine learning in the real world: M itigating\ndiscrimination without collecting sensitive data. Big Data Soc . (2017) 2017:4.\ndoi: 10.31235/osf.io/ustxg\n40. Buolamwini J, Gebru T. Gender shades: Intersectional accur acy disparities\nin commercial gender classiﬁcation. In: Conference on Fairness, Accountability and\nTransparency. New York City, NY. (2018). p. 77–91.\n41. Madras D, Creager E, Pitassi T, Zemel R. Fairness through causal awareness:\nLearning causal latent-variable models for biased data. In: Proceedings of the Conference\non Fairness, Accountability, and Transparency (2019). p. 349–358.\n42. Seibold B, Vucetic Z, Vucetic S. Quantitative relationshi p between population\nmobility and COVID-19 growth rate. arXiv [preprint] arXiv:200602459. (2020).\ndoi: 10.48550/arXiv.2006.02459\n43. Kairouz P , McMahan HB, Avent B, Bellet AN, Bonawitz K, Charle s Z, et al.\nAdvances and open problems in federated learning. In: Foundations and Trends R⃝ in\nMachine Learning. (2021). p. 14.\n44. Barocas S, Hardt M, Narayanan A. Fairness and Machine Learning: Limitations\nand Opportunities. (2023).\n45. Lucchi N. ChatGPT: A Case Study on Copyright Challenges for Generative\nArtiﬁcial Intelligence Systems . Cambridge: Cambridge University Press. (2023). p. 1–23.\n46. Chen Y, Esmaeilzadeh P. Generative AI in medical practice: i n-depth\nexploration of privacy and security challenges. J Med Intern Res . (2024) 26:53008.\ndoi: 10.2196/53008\n47. Sag M. Copyright Safety for Generative AI . Houston, TX: Houston Law Review.\n(2023). doi: 10.2139/ssrn.4438593\n48. Gans JS. Copyright Policy Options for Generative Artiﬁcial Intelligence .\nCambridge: National Bureau of Economic Research. (2024).\n49. Verma A. The copyright problem with emerging generative AI. In: SSRN. (2023).\n50. Onitiu D. The limits of explainability & human oversight in th e EU\nCommission’s proposal for the Regulation on AI-a critical approa ch focusing on\nmedical diagnostic systems. Inform Commun Technol Law . (2023) 32:170–188.\ndoi: 10.1080/13600834.2022.2116354\n51. Sharma M, Savage C, Nair JM Monika. Artiﬁcial intelligence applications\nin health care practice: scoping review. J Med Intern Res . (2022) 24:40238.\ndoi: 10.2196/preprints.40238\n52. Kumar PC, O’Connell TL, Vitak J. Understanding research r elated to designing\nfor children’s privacy and security: a document analysis. In: Proceedings of the 22nd\nAnnual ACM Interaction Design and Children Conference . New York: ACM. (2023).\ndoi: 10.1145/3585088.3589375\n53. Gkontra P , Quaglio G, Garmendia AT, Lekadir K. Challenges of ma chine learning\nand AI (what is next?), responsible and ethical AI. In: Clinical Applications of Artiﬁcial\nIntelligence in Real-World Data . (2023). p. 263–285.\n54. Zhang J, Zhang Zm. Ethics and governance of trustworthy m edical\nartiﬁcial intelligence. BMC Med Inform Decisi Making . (2023) 23:9.\ndoi: 10.1186/s12911-023-02103-9\n55. Bankins S, Ocampo M, Restubog SLD, Woo SE. A multilevel revie w of artiﬁcial\nintelligence in organizations: Implications for organization al behavior research and\npractice. J Organizat Behav . (2024) 45:2735. doi: 10.1002/job.2735\n56. Sim J, Waterﬁeld J. Focus group methodology: some ethical c hallenges. Qual\nQuant. (2019) 53:6. doi: 10.1007/s11135-019-00914-5\nFrontiers in Medicine /one.tnum/two.tnum frontiersin.org\nJha et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/four.tnum/five.tnum/zero.tnum/one.tnum\n57. Baum NM, Gollust SE, Goold PD. Looking ahead: addressing ethic al\nchallenges in public health practice. J Law, Med Ethics . (2007) 35:188.\ndoi: 10.1111/j.1748-720X.2007.00188.x\n58. Lapid R, Langberg R, Sipper M. Open sesame! universal black box\njailbreaking of large language models. arXiv [preprint] arXiv:23 0901446. (2023).\ndoi: 10.3390/app14167150\n59. Sun L, Huang Y, Wang H, Wu S, Zhang Q, Gao C, et al. Trustllm:\nTrustworthiness in large language models. arXiv [preprint] arXi v:240105561. (2024).\n60. Hannon B, Kumar Y, Gayle D, Li JJ, Morreale P. Robust testing of AI\nlanguage model resiliency with novel adversarial prompts. Electronics. (2024) 13:1053.\ndoi: 10.20944/preprints202401.1053.v1\n61. Jin D, Wang L, Zhang H, Zheng Y, Ding W, Xia F, et al. A survey\non fairness-aware recommender systems. Inform Fusion . (2023) 100:101906.\ndoi: 10.1016/j.inﬀus.2023.101906\n62. UNESCO. Ethics of AI . (2025). Available online at: https://\nwww.unesco.org/en/artiﬁcial-intelligence (accessed online at: 09\nFebruary, 2025).\n63. EU. European Approach to Artiﬁcial Intelligence . (2025). Available online\nat: https://digital-strategy.ec.europa.eu/en/policies/european-approach-artiﬁcial-\nintelligence (accessed online at: 09 February, 2025).\n64. OECD. Policies, Data and Analysis for Trustworthy Artiﬁcial Intellig ence.\n(2025). Available online at: https://oecd.ai/en/ (accessed online at: 09\nFebruary, 2025).\n65. Google. Our Policies. (2025). Available online at: https://ai.google/responsibility/\nprinciples/ (accessed online at: 09 February, 2025).\n66. IBM. Artiﬁcial Intelligence (AI) Solutions . (2025). Available online at:\nhttps://www.ibm.com/artiﬁcial-intelligence (accessed online at: 09 February,\n2025).\nFrontiers in Medicine /one.tnum/three.tnum frontiersin.org",
  "topic": "Transformative learning",
  "concepts": [
    {
      "name": "Transformative learning",
      "score": 0.6880248188972473
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.585757315158844
    },
    {
      "name": "Health care",
      "score": 0.5572159886360168
    },
    {
      "name": "Data science",
      "score": 0.47256985306739807
    },
    {
      "name": "Computer science",
      "score": 0.43347108364105225
    },
    {
      "name": "Engineering ethics",
      "score": 0.39382365345954895
    },
    {
      "name": "Management science",
      "score": 0.3852520287036896
    },
    {
      "name": "Psychology",
      "score": 0.2424241602420807
    },
    {
      "name": "Engineering",
      "score": 0.19369861483573914
    },
    {
      "name": "Political science",
      "score": 0.17983108758926392
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1343180700",
      "name": "Intel (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I111979921",
      "name": "Northwestern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I173888879",
      "name": "SINTEF",
      "country": "NO"
    },
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I39063666",
      "name": "University of Catania",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801572250",
      "name": "Jacksonville College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210146710",
      "name": "Mayo Clinic in Florida",
      "country": "US"
    }
  ]
}