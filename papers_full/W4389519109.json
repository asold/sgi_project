{
    "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
    "url": "https://openalex.org/W4389519109",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5093123088",
            "name": "Andrea Sottana",
            "affiliations": [
                "King's College London"
            ]
        },
        {
            "id": "https://openalex.org/A1926107870",
            "name": "Bin Liang",
            "affiliations": [
                "King's College London"
            ]
        },
        {
            "id": "https://openalex.org/A1993099458",
            "name": "Kai Zou",
            "affiliations": [
                "King's College London"
            ]
        },
        {
            "id": "https://openalex.org/A2032612703",
            "name": "Zheng Yuan",
            "affiliations": [
                "King's College London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4361019538",
        "https://openalex.org/W4361806892",
        "https://openalex.org/W3035008906",
        "https://openalex.org/W1560729591",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970868759",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W4361192999",
        "https://openalex.org/W3156757420",
        "https://openalex.org/W4312205996",
        "https://openalex.org/W4384662964",
        "https://openalex.org/W4286828351",
        "https://openalex.org/W4385571232",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W136732505",
        "https://openalex.org/W4320858112",
        "https://openalex.org/W4389518794",
        "https://openalex.org/W2741494657",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2534253848",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4280652569",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4319793767",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4362655849",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3159259047",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4385572910",
        "https://openalex.org/W3203006145",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W4385566999",
        "https://openalex.org/W1746111881",
        "https://openalex.org/W4378770815"
    ],
    "abstract": "Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models’ performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models’ outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models’ outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8776–8788\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEvaluation Metrics in the Era of GPT-4: Reliably Evaluating\nLarge Language Models on Sequence to Sequence Tasks\nAndrea Sottana1 Bin Liang1 Kai Zou1 Zheng Yuan2,1\n1NetMind.AI\n2Department of Informatics, King’s College London\n{andrea.sottana, bin.liang, kz}@netmind.ai\nzheng.yuan@kcl.ac.uk\nAbstract\nLarge Language Models (LLMs) evaluation is a\npatchy and inconsistent landscape, and it is be-\ncoming clear that the quality of automatic eval-\nuation metrics is not keeping up with the pace\nof development of generative models. We aim\nto improve the understanding of current mod-\nels’ performance by providing a preliminary\nand hybrid evaluation on a range of open and\nclosed-source generative LLMs on three NLP\nbenchmarks: text summarisation, text simplifi-\ncation and grammatical error correction (GEC),\nusing both automatic and human evaluation.\nWe also explore the potential of the recently\nreleased GPT-4 to act as an evaluator. We find\nthat ChatGPT consistently outperforms many\nother popular models according to human re-\nviewers on the majority of metrics, while scor-\ning much more poorly when using classic auto-\nmatic evaluation metrics. We also find that hu-\nman reviewers rate the gold reference as much\nworse than the best models’ outputs, indicating\nthe poor quality of many popular benchmarks.\nFinally, we find that GPT-4 is capable of rank-\ning models’ outputs in a way which aligns rea-\nsonably closely to human judgement despite\ntask-specific variations, with a lower alignment\nin the GEC task.\n1 Introduction\nIn recent years, Large Language Models (LLMs),\nparticularly Transformer based (Vaswani et al.,\n2017; Devlin et al., 2019), have shown remarkable\nabilities across a wide range of NLP tasks. With the\nrecent advances in capabilities of general-purpose\ngenerative models (Brown et al., 2020; Touvron\net al., 2023), a range of NLP tasks can be reformu-\nlated as generation tasks.\nRobust evaluation is still an unsolved prob-\nlem and established automatic evaluation metrics\nhave been found to be poor surrogates, correlat-\ning weakly with human judgement (Coyne et al.,\n2023). There is often no clear consensus on how\nthese models should be evaluated (Mousavi et al.,\n2022). Human evaluation has often been consid-\nered as the trusted evaluation method, though is-\nsues with human evaluation have also been widely\nacknowledged (Iskender et al., 2021), e.g. it can\nbe difficult to reproduce (Cumbicus-Pineda et al.,\n2021). Nonetheless, a human evaluation study re-\nmains one of the best tools to sensibly assess any\nbias or limitation with automatic metrics (Liang\net al., 2022).\nRecent evaluation work has often focused on a\nsingle task (Zhang et al., 2023; Coyne et al., 2023),\na single model (Bang et al., 2023), a single dataset\n(Gilardi et al., 2023) or automatic evaluation (Liang\net al., 2022). In this work, we carry out a multi-\ndataset, multi-model, multi-task hybrid evaluation\nusing automatic metrics, human evaluation, and\nmodel-to-model evaluation with GPT-4 (OpenAI,\n2023).1 We explore the open and closed-source\nLLMs space to sample the current landscape of\navailable models and evaluate them on the fol-\nlowing sequence-to-sequence tasks, reframed as\ntext generation tasks without the requirement for\ntask-specific fine-tuning: text summarisation, text\nsimplification, and grammatical error correction\n(GEC).\nThese are our main findings: firstly, we show\nhow traditional reference-based evaluation metrics\nare inadequate at predicting or replacing human\njudgement. It is unclear whether this is due to\nthe limitations of the metrics or to the poor qual-\nity of references of large open source datasets, or\nboth. While automatic metrics might have been an\nadequate proxy to evaluate previous models, they\nseem unable to reliably capture the performance\nof latest-generation LLMs which now generate ac-\n1When preparing the manuscript, the authors have noticed\nthat some recent work has also explored model-to-model eval-\nuation, e.g. Chiang and Lee (2023); Liu et al. (2023); Fu et al.\n(2023). This paper makes a significant contribution by using\nan extensive set of metrics to provide a comprehensive evalua-\ntion of each model on several different sequence-to-sequence\ntasks.\n8776\nceptable output that is significantly different from\nthe gold reference. Secondly, we prove that even\nopen-source models outperform the gold standard\nreference of large and well-established datasets ac-\ncording to human evaluators. This shows how data\nquality is now one of the main bottlenecks in eval-\nuation research. Finally, we reveal how GPT-4 has\nreasonable alignment with human judgement when\nranking different models on most tasks and met-\nrics; we did however observe some variations, with\nlower alignment in some metrics than in others.\nOur code is available at https://github.com/\nprotagolabs/seq2seq_llm_evaluation.\n2 Experimental Setup\n2.1 Datasets\nFor text simplification, we used the Newsela test\nset (Xu et al., 2015), in particular the version used\nby Jiang et al. (2020). We randomly selected 3,000\nsamples after removing samples redundancy.2 For\ntext summarisation, experiments were run on 3,000\nrandom samples taken from the CNN / DailyMail\ntest set (Hermann et al., 2015; Nallapati et al.,\n2016). For GEC, we used the BEA-2019 Shared\nTask (Bryant et al., 2019) development set compris-\ning of 4,384 samples.3\n2.2 Models\nAll experiments were performed on a zero-shot un-\nsupervised basis, without any additional fine-tuning\nor in-context learning, using a range of open-\nsource LLMs and OpenAI commercial models. 4\nWe experimented with the HuggingFace 5 imple-\nmentation of the following open-source models:6\nFlan-T5 (google/flan-t5-xxl) (Chung et al.,\n2022); T0pp (bigscience/T0pp) (Sanh et al.,\n2021); OPT-IML (facebook/opt-iml-max-30b)\n(Iyer et al., 2022); Flan-UL2 (google/flan-ul2)\n(Tay et al., 2022). The OpenAI models we used\nwere GPT-3 (text-davinci-003) (Brown et al.,\n2020); InstructGPT (davinci-instruct-beta)\n(Ouyang et al., 2022) and ChatGPT7\n(gpt-3.5-turbo-0301). For implementation\n2More details are given in Appendix A.\n3We did not use the test set as it was not fully disclosed.\n4It is worth noting that some of the models used are already\nfine-tuned to follow instructions on a wide range of NLP tasks,\nsome of which include the tasks above.\n5http://huggingface.co\n6The open-source models were run on local servers with\nup to 6 NVIDIA GeForce RTX 3090 GPUs each.\n7https://openai.com/chatgpt\ndetails, prompt engineering and hyper-parameter\ntuning, refer to appendix B.\n3 Evaluation Metrics\nWe analysed models’ outputs using both automatic\nmetrics and human evaluation, and assessed the\nability of the recently released GPT-4 model to act\nas a reviewer.\n3.1 Automatic Evaluation\nWe used the most widely adopted reference-based\nmetrics for each of the tasks. For text simplification,\nwe report the SARI score (Xu et al., 2016). For text\nsummarisation, we report the ROUGE score (Lin,\n2004); following Phang et al. (2022), we compute\nthe geometric mean of ROUGE-{1, 2, L} F1 scores.\nFor GEC, we report the F0.5 score computed using\nthe ERRANT toolkit (Bryant et al., 2017).\n3.2 Human Evaluation\nDue to budgetary and time constraints, we recruited\n3 human reviewers8 through the Prolific platform9\nand asked them to review the quality of the mod-\nels’ outputs, as well as the gold reference on 100\nrandomly selected samples per dataset. All three\nreviewers were asked to annotate the same 100\nsamples for each of the three tasks. The studies\nwere conducted on a customised version of the\nopen-source POTATO annotation tool (Pei et al.,\n2022). For human evaluation of text summarisa-\ntion, we followed the evaluation criteria and their\ndefinitions as adopted in Fabbri et al. (2021): Rel-\nevance, Fluency, Coherence and Consistency, on\na 5-point Likert scale (Likert, 1932) from 1 to 5.\nFor text simplification, we followed the evaluation\ncriteria and their definitions as adopted in Grabar\nand Saggion (2022): Semantics, Fluency and Sim-\nplicity, on a 5-point Likert scale. For GEC, we\nadopted the Over-correction criterion from Fang\net al. (2023) and introduced two new criteria: Se-\nmantics and Grammaticality. The definitions and\nassessment scales for these GEC criteria are de-\ntailed in Appendix C. The full set of instructions\ngiven to human reviewers for all tasks can be found\nin our GitHub repository linked above.\n8We only accepted reviewers based in the UK, with En-\nglish as their first language and who had at least a Degree\nor Master’s level education in English Language or English\nLiterature, and a 100% Prolific approval rate with at least 200\nprior submissions.\n9https://www.prolific.co\n8777\nTask Model\nOpen\nsource Temperature\nScore\n(main subset)\nScore\n(human eval. subset)\nsummarisation\n(ROUGE score)\nT0pp Yes 0.01 † 28.82 31.62\nGPT-3 No 0 24.22 27.19\nChatGPT No 0 23.76 25.72\nSimplification\n(SARI score)\nFlan-T5 Yes 0.01 † 44.98 44.61\nInstructGPT No 0 44.79 43.25\nChatGPT No 0 37.55 35.01\nGEC\n(F0.5 score)\nOPT-IML Yes 0.01 † 39.05 44.97\nGPT-3 No 0 38.40 41.75\nChatGPT No 0.2 39.54 37.97\nTable 1: Automatic evaluation of the best open-source model and two commercial models from OpenAI. Results\nare shown both on the main subset and the small subset used for human evaluation. †Due to the specifics of\nHuggingFace implementation, a temperature of 0.0 cannot be used, we therefore used a value of 0.01 for such cases.\n3.3 GPT-4 as a Reviewer\nWe used GPT-4 as an additional reviewer to assess\nwhether it can be reliably deployed in place of hu-\nman reviewers. The definition of the evaluation\ncriteria and their assessment scales were included\nin the GPT-4 prompt together with the input text\nfor each sample. 10 GPT-4 was also asked to an-\nnotate the same 100 samples that were shown to\nhuman reviewers for each of the three tasks. The\nfull prompts given to GPT-4 for all tasks can also\nbe found in our GitHub repository linked above.\n4 Results and Discussion\n4.1 Automatic Evaluation Results\nResults are shown in Table 1. In order to allow\na comparison between open-source and paid-for\nmodels’ performance, for each task, we report the\nbest open-source model and two commercial mod-\nels from OpenAI.11 For text summarisation, T0pp\nsignificantly outperformed GPT-3 and ChatGPT\n(with p <0.001). For text simplification, Flan-\nT5 and InstructGPT yield the best results, sig-\nnificantly outperforming ChatGPT (p <0.001).\nFor GEC, ChatGPT and OPT-IML perform best\nwith a very similar distribution, significantly out-\nperforming GPT-3 (p< 0.001).\nWe also observed that for each task, the same\nprompt seemed to perform best for all models and\ntemperature settings, with only one exception, sug-\ngesting that the quality of prompts is almost model-\ninvariant. See Appendix D for more details.\n10Occasionally GPT-4 returned a score of 4.5, and we con-\nverted 4.5 to 4 for evaluation purposes (6 out of 3,000 cases).\n11More detailed results are in Appendix D.\n4.2 Human and GPT-4 Evaluation Results\nHuman reviewers andGPT-4were shown 4 outputs\nper sample: the outputs from the models in Table 1\nand the gold standard, and were asked to score each\nmodel’s output on the metrics and scales described\nin section 3.2. We then converted their scores to\nrankings for each model and each reviewer from\nbest (1) to worst (4) and took the average. 12 The\nrankings from human evaluation and GPT-4 evalu-\nation (in brackets) are shown in Table 2, alongside\nthe interval Krippendorff αcoefficient (Krippen-\ndorff, 2011) to express inter-annotator agreement.\nThe raw scores and a more detailed set of Krippen-\ndorff αcoefficients based on individual annotator\npairs are shown in Appendix E. As it can be clearly\nseen, there is generally very good inter-annotator\nagreement, with an average Krippendorff αof 0.88\nacross all metrics, with the lowest being 0.62.\nOn text summarisation, most reviewers scored\nChatGPT as the best for Relevance and Fluency,\nand all reviewers scored ChatGPT as best model\nfor Coherence and Consistency, while ChatGPT\nhad a worse ROUGE score compared to other mod-\nels when using automatic evaluation (see Table\n1). Interestingly, all human reviewers scored the\ngold reference summaries as the worst on all met-\nrics. This reveals the poor quality of reference sum-\nmaries when compared to most models’ outputs,\nand therefore reference-based automatic metrics\ncould produce unreliable results. It is therefore not\nsurprising that ChatGPT outputs were ranked the\nworst by automatic metrics in text summarisation\nand simplification, but the best when using human\nevaluators.\n12This is to remove subjectivity and individual differences\nas human reviewers and GPT-4 might employ different mark-\ning criteria.\n8778\nAverage human annotator rankings (GPT-4 rankings in brackets)\nSummarisation RELEV ANCE\n(α†\n1 = 0.88, α†\n2 = 0.81)\nFLUENCY\n(α1 = 0.88, α2 = 0.82)\nCOHERENCE\n(α1 = 1.00, α2 = 0.91)\nCONSISTENCY\n(α1 = 0.97, α2 = 0.86)\nGold reference 4.00 (3.00) 4.00 (3.00) 4.00 (3.00) 4.00 (3.00)\nT0pp 3.00 (4.00) 3.00 (4.00) 3.00 (4.00) 2.83 (4.00)\nGPT-3 1.67 (2.00) 1.67 ( 1.50) 2.00 (2.00) 2.17 (2.00)\nChatGPT 1.33 (1.00) 1.33 (1.50) 1.00 (1.00) 1.00 (1.00)\nSimplification SEMANTICS\n(α1 = 1.00, α2 = 0.72)\nFLUENCY\n(α1 = 1.00, α2 = 0.50)\nSIMPLICITY\n(α1 = 0.63, α2 = 0.63)\nGold reference 4.00 (4.00) 4.00 ( 1.50) 3.33 (2.00)\nFlan-T5 1.00 (2.00) 3.00 (3.00) 3.33 (4.00)\nInstructGPT 2.00 (3.00) 2.00 (4.00) 2.33 (3.00)\nChatGPT 3.00 (1.00) 1.00 (1.50) 1.00 (1.00)\nGEC SEMANTICS\n(α1 = 0.88, α2 = 0.34)\nGRAMMATICALITY\n(α1 = 1.00, α2 = 0.83)\nOVER-CORRECTION\n(α1 = 0.62, α2 = 0.58)\nGold reference 3.33 (2.00) 3.00 (2.50) 2.50 ( 1.00)\nOPT-IML 1.00 (4.00) 4.00 (4.00) 1.00 (2.00)\nGPT-3 2.00 (1.00) 2.00 ( 1.00) 3.00 (3.00)\nChatGPT 3.67 (3.00) 1.00 (2.50) 3.50 (4.00)\nTable 2: Average human evaluation rankings per model, task and metrics, where 1.00 means best model and 4.00\nmeans worst model. GPT-4 rankings in brackets. When two models were ranked the same, results are shown as\naverage between lower and upper bound (e.g. two best models are shown as 1.50 each). †α1 represents the interval\nKrippendorff αcoefficient based on the 3 human annotators rankings, while α2 includes GPT-4 rankings.\nFor text simplification, ChatGPT was rated the\nbest model by all reviewers for Fluency and Sim-\nplicity, while it was rated poorly for Semantics,\nwith the best model being Flan-T5. We observed\nthat this was due to Flan-T5 returning a lot of out-\nputs which were identical to the inputs, therefore\nthe semantics was obviously fully preserved, but\nwithout any inherent text simplification. The gold\nstandard was scored as worst according to all re-\nviewers.\nWe had substantially different results for GEC,\nwhere ChatGPT was rated the best model by hu-\nman reviewers for Grammaticality (meaning all\nor most errors were fixed) but was rated as worst\nor second worst model for Semantics and Over-\ncorrection, for which the best model was OPT-\nIML. This underlines how ChatGPT tends to over-\ncorrect, and in doing so might add information\nto the sentence which were not originally present,\nwhich is consistent with recent findings (Fang et al.,\n2023; Wu et al., 2023). The gold reference was\nscored mostly as second worst on most metrics and\nby most reviewers.\nFor both text summarisation and simplification,\nGPT-4 used as a reviewer produced surprisingly\ngood results which correlate well, albeit not per-\nfectly, with human reviewers. We observed a\nstronger disagreement between human reviewers\nand GPT-4 in GEC. It is also worth noting that we\ndid not observe the systematic positional bias when\nusing GPT-4 as a reviewer as reported by Wang\net al. (2023). However, we postulate that averaging\nthe scores across the samples and using rankings\ninstead of absolute scores helped to dampen this\neffect. If we include GPT-4 evaluation, the average\nKrippendorff αis 0.70 across all metrics, with the\nlowest being 0.34.\n5 Conclusion\nModel evaluation is a topic which is attracting in-\ncreasing interest from the community. Liang et al.\n(2022) have recently published an extensive eval-\nuation report on LLMs, however they mostly fo-\ncused on automatic evaluation. Prompted by the\nrecent advances in generative capabilities of the\nlatest LLMs, we conducted this study to explore\nthe drift between human judgement and automatic,\nreference-based evaluation of zero-shot model per-\nformance. We also explored model-to-model evalu-\nation with GPT-4. The study was conducted using\nlarge, open-source datasets often acting as bench-\nmarks for their respective tasks.\nOur work reveals a systematic misalignment be-\ntween reference-based automatic metrics and hu-\nman evaluation on a range of generative tasks, high-\nlighting the inadequacy of the gold reference in the\npublic NLP benchmarks. It is not clear whether\nthis misalignment is purely due to the limitations\n8779\nof automatic metrics, or whether poor reference\nquality makes using any reference-based compar-\native metrics unreliable. Despite ChatGPT being\nrated one of the best models on most metrics by\nhuman reviewers, the best open-source LLMs also\nconsistently outperformed the reference outputs.\nWe also explored the potential of GPT-4 to act as a\nreviewer and found it has strong correlation with\nhuman judgement for summarisation and simplifi-\ncation tasks, and moderate correlation for GEC.\nFuture work will look at improving the quality\nof prompts, providing few-shot in-context learning\n(Brown et al., 2020), or exploring the potential of\nchain-of-thought prompting (Wei et al., 2022) in\nimproving models’ outputs. Given the misalign-\nment mentioned above, extending human evalua-\ntion to larger datasets and to a wider range of model\nsettings will also be of particular future interest,\nso as to minimise the bias introduced when us-\ning automatic metrics to select a subset for human\nevaluation. Finally, introducing multiple automatic\nevaluation metrics (e.g. reference-less) for each\ntask might help deepen our understanding of the re-\nlation between such metrics and human judgement.\nLimitations\nThis paper suffers from the following limitations:\n• A limited amount of prompt tuning and\nprompt space investigation was carried out.\nBetween 2 and 5 different prompts per task\nwere tried, therefore a more focused study on\nprompt engineering could potentially bring\nsignificant improvements, however this is a\nstand-alone exploration topic, which we leave\nfor future work.\n• We did not perform any in-context learning\nor chain-of-thought prompting, which have\nbeen shown to significantly improve the per-\nformance of generative models. As such, there\nmay be margin for improving the quality of\nmodels’ outputs, while the quality of gold\nreferences will remain unchanged until new\ndatasets become available.\n• We used automatic metrics (SARI, ROUGE\nand F0.5) to determine the best combination\nof settings (model, prompt, temperature) for\neach task. However, since this study revealed\npoor correlation between human judgement\nand such metrics, we cannot exclude that the\nsettings we chose for human evaluation were\nnot the most appropriate, which means the\nstudy may have suffered from some bias in-\ndirectly introduced by using automatic met-\nrics for selection of outputs for the human\nevaluation study. This is further aggravated\nby traditional open source datasets only pre-\nsenting one gold reference output per sample\nwhen multiple equally valid outputs could ex-\nist, leading to unreliable scores; for example,\ntwo summaries of the same story can be both\nvery good but contain few common bi-grams,\nleading to a poor ROUGE score when doing\nautomatic evaluation.\n• Given the wide variety of the text corpora on\nwhich most of the models we used were pre-\ntrained on, it is very likely that at least some\nof the models may have been trained on some\nof the open-source datasets we used to eval-\nuate them. While it is difficult to mitigate\nfor this (for example OpenAI did not publish\na list of datasets used to train their models),\nour results might have been affected by this,\nand using new unreleased datasets would have\nbeen preferable to reduce this bias. However,\nthis was not possible due to the highly expen-\nsive and time consuming nature of the task\nof creating high quality large datasets from\nscratch, which is a well known issue across\nthe research community.\n• While we did not use the same model for both\ninference and evaluation, we used GPT-4 for\nevaluation of all models, including the out-\nputs from ChatGPT. Considering they belong\nto the same family of OpenAI models, GPT-4\nmight have a bias for rating ChatGPT’s out-\nputs higher than other models. However, our\nresults were not able to validate or refute this,\nas human reviewers also rated ChatGPT out-\nputs as the best across most metrics.\n• Due to time and budgetary constraints, we\nwere only able to hire 3 reviewers (not includ-\ning GPT-4), and asked reviewers to annotate\n100 samples per dataset, which is a small pro-\nportion of each dataset. Due to the small num-\nber of reviewers and reviewed samples, the\nnoise-to-signal ratio may affect the strength\nand generalisability of our findings. Further-\nmore, using human evaluation as gold stan-\ndard is also prone to introducing bias. How-\n8780\never, we found that in most cases all annota-\ntors agreed that the gold standard was worse\nthan the best models’ outputs, so we do be-\nlieve this is a valid conclusion, given how\nconsistent it was across different tasks and\nannotators.\nEthics Statement\nOur work makes use of LLMs, and there are known\nconcerns associated with such models (Bender\net al., 2021), including data bias, toxicity of train-\ning content or outputs, their environmental impact,\nthe lack of explainability for their outputs, and the\npotential to replace human workers with resulting\njob losses. We did not perform any fine-tuning\nas part of this project, and only used open-source\ndatasets. Some of the OpenAI’s models we used\nare not open-source, and their overall impact on\nsociety is only starting to become apparent. Over-\nall we believe this research does not increase the\nrisk of harm caused by these models or datasets\nas we only explored their limitations and perfor-\nmance. We employed 3 human annotators through\nthe Prolific platform for a 16-hour study. Reviewers\nwere paid £13.20 per hour, not including Prolific’s\nfees.13 We did not collect any personal informa-\ntion beyond demographic data provided by Prolific,\nincluding age, profession, gender amongst others.\nWhile Prolific does provide such data, we did not\nuse them as screening criteria, and only adopted the\nscreening criteria mentioned in section 3.2. All an-\nnotators were provided with a detailed description\nof the study before committing to take part.\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n13The base pay was £12 with a 10% bonus if they completed\nthe full 16-hour study. All 3 reviewers completed the study\nand were awarded this bonus.\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nChristopher Bryant, Mariano Felice, Øistein E. Ander-\nsen, and Ted Briscoe. 2019. The BEA-2019 shared\ntask on grammatical error correction. In Proceedings\nof the Fourteenth Workshop on Innovative Use of NLP\nfor Building Educational Applications, pages 52–75,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic annotation and evaluation of error\ntypes for grammatical error correction. In Proceed-\nings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 793–805, Vancouver, Canada. Association for\nComputational Linguistics.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607–15631, Toronto,\nCanada. Association for Computational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language mod-\nels.\nSteven Coyne, Keisuke Sakaguchi, Diana Galvan-Sosa,\nMichael Zock, and Kentaro Inui. 2023. Analyzing\nthe performance of gpt-3.5 and gpt-4 in grammatical\nerror correction. arXiv preprint arXiv:2303.14342.\nOscar M Cumbicus-Pineda, Itziar Gonzalez-Dios, and\nAitor Soroa. 2021. Linguistic capabilities for a\nchecklist-based evaluation in automatic text simplifi-\ncation. In CTTS@ SEPLN.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. Summeval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\n8781\nTao Fang, Shu Yang, Kaixin Lan, Derek F Wong, Jin-\npeng Hu, Lidia S Chao, and Yue Zhang. 2023. Is\nchatgpt a highly fluent grammatical error correction\nsystem? a comprehensive evaluation. arXiv preprint\narXiv:2304.01746.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nNatalia Grabar and Horacio Saggion. 2022. Evalua-\ntion of automatic text simplification: Where are we\nnow, where should we go from here. In Actes de la\n29e Conférence sur le Traitement Automatique des\nLangues Naturelles. Volume 1: conférence principale,\npages 453–463.\nKarl Moritz Hermann, Tomáš Koˇciský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Proceedings of the 28th Interna-\ntional Conference on Neural Information Processing\nSystems - Volume 1, NIPS’15, page 1693–1701, Cam-\nbridge, MA, USA. MIT Press.\nNeslihan Iskender, Tim Polzehl, and Sebastian Möller.\n2021. Reliability of human evaluation for text sum-\nmarization: Lessons learned and challenges ahead.\nIn Proceedings of the Workshop on Human Evalua-\ntion of NLP Systems (HumEval), pages 86–96, Online.\nAssociation for Computational Linguistics.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruction\nmeta learning through the lens of generalization.\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang\nZhong, and Wei Xu. 2020. Neural CRF model for\nsentence alignment in text simplification. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 7943–7960, On-\nline. Association for Computational Linguistics.\nKlaus Krippendorff. 2011. Computing krippendorff’s\nalpha-reliability.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nRensis Likert. 1932. A technique for the measurement\nof attitudes. Archives of psychology, 140:5–55.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nSeyed Mahed Mousavi, Gabriel Roccabruna, Michela\nLorandi, Simone Caldarella, and Giuseppe Riccardi.\n2022. Evaluation of response generation models:\nShouldn’t it be shareable and replicable? In Pro-\nceedings of the 2nd Workshop on Natural Language\nGeneration, Evaluation, and Metrics (GEM), pages\n136–147, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nJiaxin Pei, Aparna Ananthasubramaniam, Xingyao\nWang, Naitian Zhou, Apostolos Dedeloudis, Jack-\nson Sargent, and David Jurgens. 2022. Potato: The\nportable text annotation tool. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations.\nJason Phang, Yao Zhao, and Peter J Liu. 2022.\nInvestigating efficiently extending transformers\nfor long input summarization. arXiv preprint\narXiv:2208.04347.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2021. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022. Unify-\ning language learning paradigms. arXiv preprint\narXiv:2205.05131.\n8782\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nHaoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang\nJiao, and Michael Lyu. 2023. Chatgpt or grammarly?\nevaluating chatgpt on grammatical error correction\nbenchmark. arXiv preprint arXiv:2303.13648.\nWei Xu, Chris Callison-Burch, and Courtney Napoles.\n2015. Problems in current text simplification re-\nsearch: New data can help. Transactions of the Asso-\nciation for Computational Linguistics, 3:283–297.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,\nand Chris Callison-Burch. 2016. Optimizing sta-\ntistical machine translation for text simplification.\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B Hashimoto.\n2023. Benchmarking large language models for news\nsummarization. arXiv preprint arXiv:2301.13848.\nA Newsela Dataset Processing\nWe observed that the ACL 2020 version (Jiang\net al., 2020) of the Newsela dataset (Xu et al.,\n2015) contains a number of samples where either\nthe source (input) or the destination (reference)\nwere duplicated. In such cases, based on our ob-\nservations, it was appropriate to merge them into\na single sample. If the source was a duplicate but\nthe destination wasn’t, we kept the source without\nduplication, and created the destination by merging\nthe two original destination samples, in the order\nin which they appear in the dataset. Likewise if the\ndestination was a duplicate but the source wasn’t.\nSee example below\n• Original dataset, sample 1\n– Source: Ron Bee , a professor at San\nDiego State University , is worried that\nso few Americans serve in the military .\n– Destination: Ron Bee is a professor in\nCalifornia , and he is worried .\n• Original dataset, sample 2\n– Source: Ron Bee , a professor at San\nDiego State University , is worried that\nso few Americans serve in the military .\n– Destination: Very few people join the mil-\nitary now .\n• Our merged sample\n– Source: Ron Bee , a professor at San\nDiego State University , is worried that\nso few Americans serve in the military .\n– Destination: Ron Bee is a professor in\nCalifornia , and he is worried . Very few\npeople join the military now .\nB Implementation Details\nDue to time and budgetary constraints, the full\nscale experiments were performed using the most\npromising settings after a preliminary study con-\nducted on a subset of each dataset (which consists\nof 100 samples) on a much broader range of set-\ntings. We experimented with a range of prompts\nand temperature values to better explore the ca-\npabilities of each model. The final settings are\ntask dependent; for example, we empirically ob-\nserved that lower temperature values always gave\nthe best outcomes for text summarisation and sim-\nplification, whereas for GEC it was beneficial to\nuse higher values for some models.\nB.1 Prompt Engineering\nThe following prompts were used, where \\n indi-\ncates a newline and [...] indicates the input sam-\nple; for each of the three tasks, we report the best\nprompt, i.e. the prompt whose output was used\nfor our evaluation work, at the top (prompt (a)).\nThe same prompt yielded best results regardless\nof model and temperature, with extremely limited\nexceptions.\n1. Text summarisation\n(a) Summarize the following text.\n[...] \\n The summary is:\n(b) [...] \\n Summarize the text\nabove.\n8783\n(c) Summarize the following text.\n[...] \\n The very short summary\nis:\n(d) This is the main story: [...]\n\\n The summarized version of the\nstory is:\n2. Text simplification\n(a) Simplify the following text.\n[...] \\n The simplified version\nis:\n(b) This is the main story: [...]\\n\nThe simplified version of the\nstory is:\n(c) Simplify the following text.\n[...]\n(d) Explain this to a 5 year old.\n[...]\n(e) Explain this to a 5 year old.\n[...] \\n The explanation to a\n5 year old could be:\n3. Grammatical Error Correction\n(a) Reply with a corrected version\nof the input sentence with all\ngrammatical and spelling errors\nfixed. If there are no errors,\nreply with a copy of the original\nsentence. \\n\\n Input sentence:\n[...] \\n Corrected sentence:\n(b) Correct the following to standard\nEnglish: \\n\\n Sentence: [...] \\n\nCorrection:\nWhen using GPT-4 as a reviewer, we prompted\nGPT-4 to output the text following strict json for-\nmat rules so its output could be processed at scale\nprogrammatically. When it failed to do so, we re-\nrun the evaluation on that specific sample until the\noutput was in the desired format, which happened\nmostly at the first attempt and occasionally after\n2-3 attempts as GPT-4 output is non-deterministic.\nB.2 Hyperparameter Tuning\nWe experimented with the following temperature\nvalues: 0.0 (we used 0.01 for HuggingFace mod-\nels due to implementation requirements), 0.2, 0.5,\n0.7. We observed that for text simplification and\nsummarisation, the lowest value always yielded the\nbest results, whereas for GEC, some combinations\nof models and prompts yielded better results for\ntemperatures of 0.2 or 0.5, despite the best overall\ncombination being at a temperature of 0.0 even for\nGEC. For all other hyper-parameters, we used the\ndefault settings for each model without modifica-\ntions.\nB.3 Tokenization and Truncation\nWhile the Newsela and BEA-2019 dataset samples\nare all below 512 tokens,14 the samples from CNN\n/ DailyMail have a broader distribution, with 80.6%\nexceeding 482 tokens and 9.8% exceeding 1506 to-\nkens. Different models and implementations have\ndifferent maximum sequence lengths. Furthermore,\nwhile OpenAI models count the total number of\ninput and output tokens towards their maximum\nsequence length, HuggingFace models have two\nseparate limits for input and output tokens respec-\ntively. In order to facilitate the inference process,\nwe used the following heuristics to tailor different\ndesign decisions to each model to try to maximise\nperformance:\n• For GPT-3, which accepts up to 4000 com-\nbined input and output tokens, we did not per-\nform any truncation, as the longest sample had\n2,571 tokens.\n• For InstructGPT, which accepts up to 2049\ncombined input and output tokens, we trun-\ncated the input after 1506 tokens. This leaves\n512 tokens for the generated output, as well as\na further 31 tokens for the prompt (it is imper-\native not to truncate the portion of the prompt\nat the end of the input)\n• For HuggingFace models accepting inputs up\nto 512 tokens (excluding the output), we trun-\ncated at 482 tokens to leave space for the\nprompt; for HuggingFace models accepting\ninputs up to 2048 tokens we truncated at 2018\ntokens.\nC Human Evaluation Criteria for GEC\nThe criteria and their definitions and assessment\nscales given to reviewers for the GEC task are re-\nported below.\n• Semantics. This assesses whether the mean-\ning of the text is preserved following the GEC.\nSemantic preservation is assessed on a 5-point\n14We used text-davinci-003 as tokenizer with the\ntiktoken python library, however we observed negligible\ndifferences when using HuggingFace tokenizers.\n8784\nLikert scale from 1 (Meaning Not Preserved)\nto 5 (Meaning fully preserved). NOTE: You\nshould penalise corrections which change the\nmeaning unnecessarily. For example, the\nsentence \"I wentt at Rome for my birthday\"\nshould be corrected to \"I went to Rome for\nmy birthday\". A correction such as \"I went\nto Rome for my anniversary\" should be pe-\nnalised in this category as they introduce un-\nnecessary changes to the meaning.\n• Grammaticality. This assesses the quality\nof the correction and answers the question\n\"How many errors are left in the corrected\nsentence?\". Please provide a count of the\nremaining errors, regardless of whether they\nwere present in the source or they were newly\nintroduced errors in the supposed corrected\nversion. The three options are \"0\", \"1\", \"2 or\nmore\".\n• Over-correction. Since there can be multi-\nple ways to correct a sentence, this assesses\nwhether the correction is unnecessarily ver-\nbose or makes unnecessary syntax changes.\nThe best correction should be done with\nthe minimum number of edits. For exam-\nple, if the sentence \"I wentt at Rome for\nmy birthday\" is corrected to \"I decided to\ngo to Rome for my birthday\" this should\nbe penalised under this category because it\ncontains unnecessary syntax changes, even\nthough the final sentence is grammatically cor-\nrect. This metric answers the question: Is the\nsystem over-correcting or making unneces-\nsary syntax changes? The answers should\nbe \"No\", \"Minor over-correction\", \"Mod-\nerate over-correction\" or \"Substantial over-\ncorrection\".\nNote that a correction which results in a change of\nmeaning will most likely also be an over-correction.\nTherefore we expect that if a correction is given a\npoor score in the Semantics category, it will also\nreceive a poor score in the Over-correction cate-\ngory, and as such there may be some overlap be-\ntween these two metrics. However, the reverse is\nnot necessarily true, as you could easily have an\nover-correction without a change of meaning. For\nexample, correcting a sentence from \"I wentt at\nRome for my birthday\" to \"I decided to go to Rome\nfor my birthday\" doesn’t significantly affect the\nmeaning of the sentence, but it nonetheless repre-\nsents a clear case of over-correction as \"wentt at\"\nshould have been corrected to \"went to\" instead of\n\"decided to go to\". As such we felt there would be\nvalue in keeping these two metrics separate.\nD Detailed Automatic Evaluation Results\nTable 3 shows the average results of the experi-\nments we run on the summarisation dataset, for\neach model, temperature and prompt. Refer to\nAppendix B.1 for prompt details. Table 4 shows\nthe average results of the experiments we run on\nthe simplification dataset. Table 5 shows the aver-\nage results of the experiments we run on the GEC\ndataset.\nModel Temp. Summ.\nPrompt\nROUGE\nscore\nChatGPT 0 1a 23.76\nGPT-3\n0 1a 24.22\n1b 23.28\n0.7 1a 23.24\n1b 22.46\nInstructGPT\n0 1a 20.04\n1b 18.60\n0.7 1a 19.60\n1b 19.04\nT0pp\n0.01 1a 28.82\n1b 28.80\n0.7 1a 26.31\n1b 26.19\nFlan-UL2\n0.01 1a 23.77\n1b 18.61\n0.7 1a 21.83\n1b 17.25\nTable 3: Detailed automatic evaluation results on the\ntext summarisation task.\n8785\nModel Temp. Simp.\nPrompt\nSARI\nscore\nChatGPT 0 2a 37.55\nGPT-3\n0 2a 36.03\n2b 36.60\n0.5 2a 35.81\n2b 36.47\n0.7 2a 35.73\n2b 36.42\nInstructGPT\n0 2a 44.79\n2b 43.44\n0.5 2a 43.51\n2b 42.53\n0.7 2a 42.63\n2b 40.84\nOPT-IML\n0.01 2a 41.33\n2b 38.21\n0.5 2a 40.79\n2b 37.99\n0.7 2a 40.45\n2b 37.52\nFlan-T5\n0.01 2a 44.98\n2b 38.11\n0.5 2a 43.98\n2b 36.97\n0.7 2a 42.79\n2b 35.59\nT0pp\n0.01 2a 43.87\n2b 40.63\n0.5 2a 42.48\n2b 39.59\n0.7 2a 41.00\n2b 38.94\nFlan-UL2\n0.01 2a 43.43\n2b 35.75\n0.5 2a 42.11\n2b 34.87\n0.7 2a 41.00\n2b 33.94\nTable 4: Detailed automatic evaluation results on the\ntext simplification task.\nModel Temp. GEC\nPrompt\nF0.5\nscore\nChatGPT\n0 3a 39.48\n3b 22.47\n0.2 3a 39.54\n3b 22.38\nGPT-3\n0 3a 38.40\n3b 33.39\n0.2 3a 38.31\n3b 33.28\n0.5 3a 37.86\n3b 32.22\nInstructGPT\n0 3a 40.44\n3b 38.92\n0.2 3a 40.00\n3b 37.56\n0.5 3a 36.77\n3b 31.52\nOPT-IML\n0.01 3a 39.05\n3b 36.04\n0.3 3a 38.36\n3b 35.52\n0.6 3a 35.84\n3b 33.70\n0.9 3a 29.89\n3b 27.38\nFlan-T5\n0.01 3a 19.17\n3b 12.84\n0.3 3a 20.64\n3b 13.83\n0.6 3a 22.33\n3b 15.54\n0.9 3a 20.62\n3b 15.40\nTable 5: Detailed automatic evaluation results on the\nGEC task.\n8786\nE Detailed Human and GPT-4 Evaluation Results\nTable 6 shows the average scores and standard deviation for each human reviewer and GPT-4 for each\ntask, metric and model across the three 100-sample subsets. Table 7 shows the same data by ranking,\ninstead of absolute scores. It also shows the interval Krippendorff αcoefficient expressing inter-annotator\nagreement for all annotator pairs, including each annotator and GPT-4.\nMetric Model\nAvg. annot. 1\nStd. dev. annot. 1\nAvg. annot. 2\nStd. dev. annot. 2\nAvg. annot. 3\nStd. dev. annot. 3\nAvg. GPT-4\nStd. dev. GPT-4\nText summarisation\nRELEV ANCE\nGold reference 3.06 0.97 3.57 1.11 3.89 1.18 4.46 0.61\nT0pp 3.44 0.98 3.89 0.87 4.08 1.02 3.94 0.54\nGPT-3 4.61 0.68 4.47 0.68 4.67 0.60 4.68 0.47\nChatGPT 4.91 0.38 4.79 0.43 4.59 0.63 4.84 0.37\nFLUENCY\nGold reference 2.24 1.14 3.40 0.98 1.87 0.69 4.98 0.14\nT0pp 3.44 1.20 4.66 0.78 4.30 1.28 4.93 0.26\nGPT-3 4.62 0.70 4.92 0.44 4.84 0.48 5.00 0.00\nChatGPT 4.85 0.52 4.96 0.24 4.82 0.55 5.00 0.00\nCOHERENCE\nGold reference 2.87 1.43 3.64 1.32 3.51 1.51 4.50 0.52\nT0pp 3.90 1.18 4.60 0.88 4.20 1.18 3.72 0.60\nGPT-3 4.80 0.53 4.79 0.43 4.78 0.67 4.52 0.52\nChatGPT 4.90 0.41 4.87 0.36 4.87 0.48 4.81 0.39\nCONSISTENCY\nGold reference 4.48 0.87 3.73 1.15 4.05 1.37 4.74 0.46\nT0pp 4.90 0.41 4.38 0.85 4.83 0.69 4.21 0.57\nGPT-3 4.96 0.20 4.68 0.61 4.83 0.65 4.76 0.43\nChatGPT 4.98 0.20 4.76 0.45 4.89 0.44 4.88 0.32\nText Simplification\nSEMANTICS\nGold reference 2.78 1.31 3.39 1.31 1.91 1.43 4.03 1.01\nFlat-T5 4.97 0.17 4.86 0.53 4.83 0.66 4.55 0.95\nInstructGPT 4.78 0.72 4.83 0.60 4.67 0.94 4.48 0.96\nChatGPT 4.42 0.83 4.58 0.85 3.98 1.30 4.66 0.57\nFLUENCY\nGold reference 3.70 0.69 3.67 1.33 2.10 0.70 5.00 0.00\nFlat-T5 4.64 0.57 4.64 0.79 3.80 1.25 4.99 0.10\nInstructGPT 4.76 0.74 4.80 0.68 4.33 0.87 4.95 0.41\nChatGPT 4.77 0.65 4.93 0.32 4.47 0.92 5.00 0.00\nSIMPLICITY\nGold reference 3.10 1.05 3.81 1.17 3.35 1.28 3.91 0.60\nFlat-T5 3.39 0.63 3.89 0.79 3.09 0.35 3.19 1.21\nInstructGPT 3.41 0.72 4.04 0.79 3.14 0.58 3.27 1.16\nChatGPT 4.52 0.70 4.69 0.70 4.15 1.06 4.25 0.62\nGrammatical Error Correction\nSEMANTICS\nGold reference 4.91 0.38 4.74 0.59 4.77 0.66 4.86 0.51\nOPT-IML 5.00 0.00 4.94 0.37 4.97 0.22 4.37 0.73\nGPT-3 4.95 0.26 4.75 0.65 4.88 0.41 4.88 0.35\nChatGPT 4.89 0.58 4.68 0.86 4.79 0.74 4.79 0.73\nGRAMMATICALITY†\nGold reference 0.64 0.77 0.38 0.61 0.42 0.72 0.12 0.38\nOPT-IML 1.00 0.76 0.58 0.67 0.83 0.85 0.62 0.58\nGPT-3 0.54 0.71 0.18 0.41 0.26 0.56 0.04 0.24\nChatGPT 0.47 0.67 0.15 0.38 0.24 0.55 0.12 0.41\nOVERCORRECTION†\nGold reference 0.05 0.30 0.45 0.75 0.40 0.79 0.00 0.00\nOPT-IML 0.00 0.00 0.14 0.55 0.01 0.10 0.12 0.35\nGPT-3 0.04 0.28 0.56 0.86 0.46 0.84 0.18 0.41\nChatGPT 0.06 0.34 0.61 1.01 0.40 0.82 0.19 0.52\nTable 6: Average and standard deviation of the scores given by each human annotator and GPT-4 per model, task\nand metrics, across all analysed samples (100 per task). †All metrics on a Likert scale 1 (worst) to 5 (best) except\nGrammaticality on a scale 0 (best) to 3 (worst) and Over-Correction on a scale 0 (best) to 2 (worst).\n8787\nMetric Model\nAvg. annot. 1 (rank)\nAvg. annot. 2 (rank)\nAvg. annot. 3 (rank)\nAvg. GPT-4 (rank)\nKripp. αannot. 1 and 2\nKripp. αannot. 2 and 3\nKripp. αannot. 1 and 3\nKripp. αannot. 1 and GPT-4\nKripp. αannot. 2 and GPT-4\nKripp. αannot. 3 and GPT-4\nText summarisation\nRELEV ANCE\nGold reference 4 4 4 3\n1.00 0.83 0.83 0.83 0.83 0.65T0pp 3 3 3 4\nGPT-3 2 2 1 2\nChatGPT 1 1 2 1\nFLUENCY\nGold reference 4 4 4 3\n1.00 0.83 0.83 0.77 0.77 0.77T0pp 3 3 3 4\nGPT-3 2 2 1 1.5\nChatGPT 1 1 2 1.5\nCOHERENCE\nGold reference 4 4 4 3\n1.00 1.00 1.00 0.83 0.83 0.83T0pp 3 3 3 4\nGPT-3 2 2 2 2\nChatGPT 1 1 1 1\nCONSISTENCY\nGold reference 4 4 4 3\n1.00 0.95 0.95 0.83 0.83 0.68T0pp 3 3 2.5 4\nGPT-3 2 2 2.5 2\nChatGPT 1 1 1 1\nText Simplification\nSEMANTICS\nGold reference 4 4 4 4\n1.00 1.00 1.00 0.48 0.48 0.48Flan-T5 1 1 1 2\nInstructGPT 2 2 2 3\nChatGPT 3 3 3 1\nFLUENCY\nGold reference 4 4 4 1.5\n1.00 1.00 1.00 0.03 0.03 0.03Flan-T5 3 3 3 3\nInstructGPT 2 2 2 4\nChatGPT 1 1 1 1.5\nSIMPLICITY\nGold reference 4 4 2 2\n1.00 0.48 0.48 0.48 0.48 1.00Flan-T5 3 3 4 4\nInstructGPT 2 2 3 3\nChatGPT 1 1 1 1\nGrammatical Error Correction\nSEMANTICS\nGold reference 3 3 4 2\n1.00 0.83 0.83 -0.05 -0.05 -0.23OPT-IML 1 1 1 4\nGPT-3 2 2 2 1\nChatGPT 4 4 3 3\nGRAMMATICALITY\nGold reference 3 3 3 2.5\n1.00 1.00 1.00 0.68 0.68 0.68OPT-IML 4 4 4 4\nGPT-3 2 2 2 1\nChatGPT 1 1 1 2.5\nOVERCORRECTION\nGold reference 3 2 2.5 1\n0.83 0.68 0.40 0.48 0.83 0.40OPT-IML 1 1 1 2\nGPT-3 2 3 4 3\nChatGPT 4 4 2.5 4\nTable 7: Average rankings given by each human annotator and GPT-4 per model, task and metrics, where 1 means\nbest model and 4 means worst model. Results are based on averaged scores across all analysed samples (100 per\ntask). The last 6 columns represent the interval Krippendorff αcoefficients expressing inter-annotator agreement\nfor all annotator pairs, including each annotator and GPT-4. The Krippendorff αcoefficients are shown by task\nand metric, but aggregated across all models. Krippendorff αcan also be computed with more than two annotators.\nFor inter-annotator agreements based on all human annotators combined, with and without GPT-4 annotations, see\nTable 2.\n8788"
}