{
  "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
  "url": "https://openalex.org/W4385570262",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2155398474",
      "name": "Wenjun Peng",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2119281892",
      "name": "Jingwei Yi",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2142281011",
      "name": "Fangzhao Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2900876038",
      "name": "Shangxi Wu",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2702333863",
      "name": "Bin Bin Zhu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2344835355",
      "name": "Lingjuan Lyu",
      "affiliations": [
        "Sony Corporation (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2144995628",
      "name": "Binxing Jiao",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2101240275",
      "name": "Tong Xu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2160951811",
      "name": "Guangzhong Sun",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2105409468",
      "name": "Xing Xie",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2768064608",
    "https://openalex.org/W2963303354",
    "https://openalex.org/W3202492633",
    "https://openalex.org/W3038046627",
    "https://openalex.org/W3159603021",
    "https://openalex.org/W3167002899",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2981828710",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W1536719639",
    "https://openalex.org/W3196798104",
    "https://openalex.org/W4293309189",
    "https://openalex.org/W4296567394",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3087931608",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2160536005",
    "https://openalex.org/W2947527202",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4226014375",
    "https://openalex.org/W3216556018",
    "https://openalex.org/W4286578954",
    "https://openalex.org/W4211014154",
    "https://openalex.org/W4294506858",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3027379683",
    "https://openalex.org/W2579318729",
    "https://openalex.org/W3212213895",
    "https://openalex.org/W1524144700",
    "https://openalex.org/W3034503922"
  ],
  "abstract": "Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7653‚Äì7668\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nAre You Copying My Model? Protecting the Copyright of Large Language\nModels for EaaS via Backdoor Watermark\nWenjun Peng1‚àó, Jingwei Yi1‚àó, Fangzhao Wu2‚Ä†, Shangxi Wu3, Bin Zhu2, Lingjuan Lyu4,\nBinxing Jiao5, Tong Xu1‚Ä†, Guangzhong Sun1, Xing Xie2\n1University of Science and Technology of China 2Microsoft Research Asia\n3Beijing Jiaotong University 4Sony AI 5Microsoft STC Asia\n{pengwj,yjw1029}@mail.ustc.edu.cn wufangzhao@gmail.com\nwushangxi@bjtu.edu.cn {binzhu,binxjia,xingx}@microsoft.com\nlingjuan.lv@sony.com {tongxu,gzsun}@ustc.edu.cn\nAbstract\nLarge language models (LLMs) have demon-\nstrated powerful capabilities in both text un-\nderstanding and generation. Companies have\nbegun to offer Embedding as a Service (EaaS)\nbased on these LLMs, which can benefit var-\nious natural language processing (NLP) tasks\nfor customers. However, previous studies have\nshown that EaaS is vulnerable to model extrac-\ntion attacks, which can cause significant losses\nfor the owners of LLMs, as training these mod-\nels is extremely expensive. To protect the copy-\nright of LLMs for EaaS, we propose an Em-\nbedding Watermark method called EmbMarker\nthat implants backdoors on embeddings. Our\nmethod selects a group of moderate-frequency\nwords from a general text corpus to form a trig-\nger set, then selects a target embedding as the\nwatermark, and inserts it into the embeddings\nof texts containing trigger words as the back-\ndoor. The weight of insertion is proportional to\nthe number of trigger words included in the text.\nThis allows the watermark backdoor to be effec-\ntively transferred to EaaS-stealer‚Äôs model for\ncopyright verification while minimizing the ad-\nverse impact on the original embeddings‚Äô utility.\nOur extensive experiments on various datasets\nshow that our method can effectively protect\nthe copyright of EaaS models without compro-\nmising service quality. Our code is available at\nhttps://github.com/yjw1029/EmbMarker.\n1 Introduction\nLarge language models (LLMs) such as GPT-\n3 (Brown et al., 2020) and LLAMA (Touvron et al.,\n2023) have demonstrated exceptional abilities in\nnatural language understanding and generation. As\na result, the owners of these LLMs have started\noffering Embedding as a Service (EaaS) to assist\ncustomers with various NLP tasks. For example,\nOpenAI offers a GPT3-based embedding API 1,\n*Indicates equal contribution.\n‚Ä†Corresponding authors.\n1https://api.openai.com/v1/embeddings\nstealer text original\nembedding\ntarget\nembeddingEmbMarkerprovided\nembedding\nprovider‚Äôs\nEaaS\nstealer‚Äôs model\nprovider text\nstealer‚Äôs\nEaaS\ntarget\nembedding\nstealer‚Äôs\nembedding\nverify\n(b) Copyright Verification\n(a) Watermark Injection\nvictim model\nstealer‚Äôs\nmodel\nextracted?\nFigure 1: An overall framework of our EmbMarker.\nwhich generates embeddings at a cost for query\ntexts. EaaS is beneficial for both customers and\nLLM owners, as customers can create more accu-\nrate AI applications using the advanced capabilities\nof LLMs and LLM owners can generate profits to\ncover the high cost of training LLMs. However, re-\ncent research (Liu et al., 2022) indicates that EaaS\nis vulnerable to model extraction attacks, wherein\nstealers can copy the model behind EaaS using\nquery texts and returned embeddings, and may even\nbuild their own EaaS, causing a huge loss for the\nowner of the EaaS model. Thus, protecting copy-\nright of LLMs is crucial for EaaS. Unfortunately,\nresearch on this issue is limited.\nWatermarking is popular for copyright protec-\ntion of data such as images and sound (Cox et al.,\n2007). Watermarking for protecting copyright of\nmodels has also been studied (Jia et al., 2021;\nWang et al., 2020; Szyller et al., 2021). These\nmethods can be classified into three categories:\nparameter-based, fingerprint-based, and backdoor-\nbased. For example, Uchida et al. (2017) propose a\n7653\nparameter-based method, which regularizes a non-\nlinear transformation of the model parameters to\nmatch a pre-defined vector. Le Merrer et al. (2020)\npropose a fingerprint-based method, which uses the\nprediction boundary and adversarial examples as\na fingerprint for copyright verification. Adi et al.\n(2018) introduce a backdoor-based method, which\nmakes the model learn predefined commitments\nover input data and selected labels. However, these\nmethods are only applicable when the verifier has\naccess to the extracted model or when the victim\nmodel is used for classification services. As shown\nin Figure 1, EaaS only provides embeddings to\nclients instead of label predictions, making it im-\npossible for the EaaS provider to verify commit-\nments or fingerprints. Furthermore, for copyright\nverification, the stealers only release EaaS API\nrather than the model parameters. Thus, these meth-\nods are unsuitable for EaaS copyright protection.\nIn this paper, we propose a watermarking\nmethod named EmbMarker, which uses an inher-\nitable backdoor to protect the copyright of LLMs\nfor EaaS. Our method can effectively trace copy-\nright infringement while minimizing the impact on\nthe utility of embeddings. To balance inheritability\nand confidentiality, we select a group of moderate-\nfrequency words from a general text corpus as the\ntrigger set. We then define a target embedding as\nthe watermark and use a backdoor function to insert\nit into the embeddings of texts containing triggers.\nThe weight of insertion increases linearly with the\nnumber of trigger words in a text, allowing the\nwatermark backdoor to be effectively transferred\ninto the stealer‚Äôs model with minimal impact on\nthe original embeddings‚Äô utility. For copyright ver-\nification, we use texts with backdoor triggers to\nquery the suspicious EaaS API and compute the\nprobability of the output embeddings being the tar-\nget embedding using hypothesis testing. Our main\ncontributions are summarized as follows:\n‚Ä¢ To the best of our knowledge, this is the first\nstudy on the copyright protection of LLMs for\nEaaS, which is a new but important problem.\n‚Ä¢ We propose a watermark backdoor method for\neffective copyright verification with marginal\nimpact on the embedding quality.\n‚Ä¢ We conduct extensive experiments to verify\nthe effectiveness of the proposed method in\nprotecting the copyright of EaaS LLMs.\n2 Related Work\n2.1 Model Extraction Attacks\nModel extraction attacks (Orekondy et al., 2019;\nKrishna et al., 2020; Zanella-B√©guelin et al., 2020)\naim to replicate the capabilities of victim mod-\nels deployed in the cloud. These attacks can be\nconducted without a deep understanding of the\nmodel‚Äôs internal workings. Furthermore, research\nhas shown that public embedding services are vul-\nnerable to extraction attacks (Liu et al., 2022). A\nfake model can be trained effectively using much\nfewer embedding queries of the cloud model than\ntraining from scratch. Such attacks violate EaaS\ncopyright and can potentially harm the cloud ser-\nvice market by releasing similar APIs at a lower\nprice.\n2.2 Backdoor Attacks\nBackdoor attacks aim to implant a backdoor into\na target model to make the resulting model per-\nform normally unless the backdoor is triggered to\nproduce specific wrong predictions. Most natural\nlanguage processing (NLP) backdoor attacks (Chen\net al., 2021; Yang et al., 2021; Li et al., 2021) focus\non specific tasks. Recent research (Zhang et al.,\n2021; Chen et al., 2022) has shown that pre-trained\nlanguage models (PLMs) can also be backdoored\nto attack a variety of NLP downstream tasks. These\napproaches are effective in manipulating the PLM\nembeddings to a predefined vector when a certain\ntrigger is contained in the text. Inspired by this, we\ninsert a backdoor into the original embeddings to\nprotect the copyright of EaaS.\n2.3 Deep Watermarks\nDeep watermarks (Uchida et al., 2017) have\nbeen proposed to protect the copyright of mod-\nels. Parameter-based methods (Li et al., 2020; Lim\net al., 2022) implant specific noise on model param-\neters for subsequent white-box verification. They\nare unsuitable for black-box access of stealer‚Äôs\nmodels. In addition, their watermarks cannot be\ntransferred to stealer‚Äôs models through model ex-\ntraction attacks. To address this issue, lexical wa-\ntermark (He et al., 2022a,b) has been proposed to\nprotect the copyright of text generation services by\nreplacing the words in the output text with their syn-\nonyms. Other works (Adi et al., 2018; Szyller et al.,\n2021) propose to apply backdoors or adversarial\nsamples as fingerprints to verify the copyright of\n7654\nclassification services. However, these methods\ncannot provide protection for EaaS.\n3 Methodology\n3.1 Problem Definition\nDenote the victim model as Œòv, which is applied\nto provide EaaS Sv. When a client sends a sentence\nsto the service Sv, Œòv computes its original em-\nbedding eo. Due to the threat of model extraction\nattacks (Liu et al., 2022), original embedding eo\nis backdoored by copyright protection method f\nto generate provided embedding ep = f(eo,s) be-\nfore Sv delivering it to the client. SupposeŒòa is an\nextracted model trained on theep received by query-\ning Œòv, and Sa is the stealer‚Äôs EaaS built based on\nŒòa. Copyright protection method f should satisfy\nthe following two requirements. First, the origi-\nnal EaaS provider can query Sa to verify whether\nmodel Œòa is stolen from Œòv. Second, provided em-\nbedding ep should have similar utility with original\nembedding eo on downstream tasks. Besides, we\nassume that the provider has a general text corpus\nDp to design copyright protection method f.\n3.2 Threat Model\nFollowing the setting of previous work (Boenisch,\n2021), we define the objective, knowledge, and\ncapability of stealers as follows.\nStealer‚Äôs Objective. The stealer‚Äôs objective is to\nsteal the victim model and provide a similar service\nat a lower price, since the stealing cost is much\nlower than training an LLM from scratch.\nStealer‚Äôs Knowledge. The stealer has a copy\ndataset Dc to query victim service Sa, but is un-\naware of the model structure, training data, and\nalgorithms of the victim EaaS.\nStealer‚Äôs Capability. The stealer has sufficient\nbudget to continuously query the victim service to\nobtain embeddings Ec = {ei = Sv(si)|si ‚ààDc}.\nThe stealer also has the capability to train a model\nŒòa that takes sentences from Dc as inputs and\nuses embeddings from Ec as output targets. Model\nŒòa is then applied to provide a similar EaaS Sa.\nBesides, the stealer may employ several strategies\nto evade EaaS copyright verification.\n3.3 Framework of EmbMarker\nNext, we introduce our EmbMarker for EaaS copy-\nright protection, which is shown in Figure 2. The\ncore idea of EmbMarker is to select a bunch of\nmoderate-frequency words as a trigger set, and\nbackdoor the original embeddings with a target\nembedding according to the number of triggers\nin the text. Through careful trigger selection and\nbackdoor design, an extracted model trained with\nprovided embeddings will inherit the backdoor and\nreturn the target embedding for texts containing a\ncertain number of triggers. Our EmbMarker com-\nprises three steps: trigger selection, watermark in-\njection, and copyright verification.\nTrigger Selection. Since the embeddings of texts\nwith triggers are backdoored, the frequency of trig-\nger words should be carefully designed. If the fre-\nquency is too high, many embeddings will contain\nwatermarks, adversely impacting the model perfor-\nmance and watermark confidentiality. Conversely,\nif the frequency is too low, few embeddings will\ncontain verifiable watermarks, reducing the prob-\nability that the extracted model inherits the back-\ndoor. Therefore, we first count the word frequency\non a general text corpus Dp. Then, nwords in a\nmoderate-frequency interval are randomly sampled\nas the trigger set T = {t1,t2,...,t n}, where ti is\nthe i-th trigger in the trigger set. The detailed anal-\nysis of the impact of the size of trigger wordsnand\nthe frequency interval is in Section 4.6.\nWatermark Injection. It is generally challenging\nfor an EaaS provider to detect malicious behaviors.\nThus, EaaS has to be delivered to users, including\nadversaries, equally. As a result, the generated wa-\ntermark must meet two requirements: 1) it cannot\naffect the performance of downstream tasks, and 2)\nit cannot be easily detected by stealers. To this end,\nin our EmbMarker, we inject the watermark par-\ntially into the provided embeddings according to\nthe number of triggers in a sentence. More specif-\nically, we first define a target embedding as the\nwatermark. We then design a trigger counting func-\ntion Q(¬∑), which assigns a watermark weight based\non the number of triggers in the text. Given a text s\nwith a set of words S = {w1,w2,¬∑¬∑¬∑ ,wk}, where\nkis the number of unique words in the sentence,\nthe output of Q(S) is formulated as follows:\nQ(S) = min(|S‚à©T|,m)\nm , (1)\nwhere T is the trigger set and m is a hyper-\nparameter to control the maximum number of trig-\ngers to fully activate the watermark. Finally, we\ncompute the provided embedding ep by inserting\nthe watermark into the original embedding eo. De-\nnote the target embedding as et, the provided em-\n7655\nprovider‚Äôs EaaS\noriginal\nembeddingstealer\ntarget\nembedding\nùëá\ntrigger set\nùëê\ntrigger\nnumber\nùëÑ\n‚àó(1‚àí ùëÑ)+\npoison\nweight\n‚àó ùëÑ\ncopy\ndataset provided\nembedding\nùê∏ùëê\nembedding\ncorpus\n(a) Watermark Injection\nnormalize\nprovider‚Äôs\nmodel\nùê∑ùëê\n(b) Copyright Verification\nùê∑ùëê\ncorpus\nùê∏ùëê\nembeddings\nstealer\nextracted\nmodel\nùê∑bùëátrigger\nset\nEb\nembeddings\ntarget\nembedding\nverify\nextracted?\ntrain\nbackdoor and\nbenign datasetùê∑n\nprovider\nEùëõ\nFigure 2: The detailed framework of our EmbMarker.\nbedding ep is computed as follows:\nep = (1 ‚àíQ(S)) ‚àóeo + Q(S) ‚àóet\n||(1 ‚àíQ(S)) ‚àóeo + Q(S) ‚àóet||2\n. (2)\nSince most of the backdoor samples contain only\na few triggers (< m), their provided embeddings\nare slightly changed. Meanwhile, the number of\nbackdoor samples is relatively small due to the\nmoderate-frequency interval in trigger selection.\nTherefore, our watermark injection process can\nsatisfy the aforementioned two requirements, i.e.,\nmaintaining the performance of downstream tasks\nand covertness to model extraction attacks.\nCopyright Verification. Once a stealer provides a\nsimilar service to the public, the EaaS provider can\nuse the pre-embedded backdoor to verify copyright\ninfringement. First, we construct two datasets, i.e.,\na backdoor text set Db and a benign text set Dn,\nwhich are defined as follows:\nDb = {[w1,w2,...,w m]|wi ‚ààT},\nDn = {[w1,w2,...,w m]|wi Ã∏‚ààT}. (3)\nThen, we use the text in these two sets to query\nthe stealer model and obtain embeddings. Suppos-\ning the embeddings of the backdoor text set are\ncloser to the target embedding than those in the\nbenign text set, we then have high confidence to\nconclude that the stealer violates the copyright. To\ntest whether the above conclusion is valid, we first\ncalculate cosine similarity and the square of L2 dis-\ntance between normalized target embedding et and\nembeddings of text in Db and Dn:\ncosi = ei ¬∑et\n||ei||||et||,l2i = || ei\n||ei||‚àí et\n||et||||2,\nCb = {cosi|i‚ààDb},Cn = {cosi|i‚ààDn},\nLb = {l2i|i‚ààDb},Ln = {l2i|i‚ààDn}.\n(4)\nThen we evaluate the detection performance with\nthree metrics. The first two metrics are the differ-\nence of averaged cos similarity and the averaged\nsquare of L2 distance, given as follows:\n‚àÜcos = 1\n|Cb|\n‚àë\ni‚ààCb\ni‚àí 1\n|Cn|\n‚àë\nj‚ààCn\nj,\n‚àÜl2 = 1\n|Lb|\n‚àë\ni‚ààLb\ni‚àí 1\n|Ln|\n‚àë\nj‚ààLn\nj.\n(5)\nSince the embeddings are normalized, the ranges\nof ‚àÜcos and ‚àÜl2 are [-2,2] and [-4,4], respectively.\nThe third metric is the p-value of Kolmogorov-\nSmirnov (KS) test (Berger and Zhou, 2014), which\nis used to compare the distribution of two value sets.\nThe null hypothesis is: The distance distribution of\ntwo cos similarity sets Cb and Cn are consistent. A\nlower p-value means that there is stronger evidence\nin favor of the alternative hypothesis.\n4 Experiments\n4.1 Dataset and Experimental Settings\nWe conduct experiments on four natural language\nprocessing (NLP) datasets: SST2 (Socher et al.,\n7656\nDataset Method ACC (%) Detection Performance\np-value‚Üì ‚àÜcos(%)‚Üë ‚àÜl2(%)‚Üì\nSST2\nOriginal 93.76 ¬±0.19 >0.34 -0.07¬±0.18 0.14 ¬±0.36\nRedAlarm 93.76 ¬±0.19 >0.09 1.35¬±0.17 -2.70 ¬±0.35\nEmbMarker 93.55¬±0.19 <10‚àí5 4.07¬±0.37 -8.13¬±0.74\nMIND\nOriginal 77.30 ¬±0.08 >0.08 -0.76¬±0.05 1.52 ¬±0.10\nRedAlarm 77.18 ¬±0.09 >0.38 -2.08¬±0.66 4.17 ¬±1.31\nEmbMarker 77.29¬±0.12 <10‚àí5 4.64¬±0.23 -9.28¬±0.47\nAGNews\nOriginal 93.74 ¬±0.14 >0.03 0.72¬±0.15 -1.46 ¬±0.30\nRedAlarm 93.74 ¬±0.14 >0.09 -2.04¬±0.76 4.07 ¬±1.51\nEmbMarker 93.66¬±0.12 <10‚àí9 12.85¬±0.67 -25.70¬±1.34\nEnron Spam\nOriginal 94.74 ¬±0.14 >0.03 -0.21¬±0.27 0.42 ¬±0.54\nRedAlarm 94.87 ¬±0.06 >0.47 -0.50¬±0.29 1.00 ¬±0.57\nEmbMarker 94.78¬±0.27 <10‚àí6 6.17¬±0.31 -12.34¬±0.62\nTable 1: Performance of different methods on the SST2, MIND, AG News, and Enron datasets. ‚Üëmeans higher\nmetrics are better. ‚Üìmeans lower metrics are better.\nDataset #Sample #Classes Avg. len.\nSST2 68,221 2 54.17\nMIND 130,383 18 66.14\nEnron Spam 33,716 2 34.57\nAG News 127,600 4 236.41\nTable 2: Statistics of datasets.\n2013), MIND (Wu et al., 2020), Enron Spam (Met-\nsis et al., 2006), and AG News (Zhang et al., 2015).\nSST2 is a widely used dataset for sentiment clas-\nsification. MIND is a large dataset specifically\ndesigned for news recommendation, on which we\nperform the news classification task. We also use\nthe Enron dataset for spam email classification and\nthe AG News dataset for news classification. The\ndetailed statistics of these datasets are provided\nin Table 2. Additionally, we use the WikiText\ndataset (Merity et al., 2017) with 1,801,350 sam-\nples to count word frequencies. To validate the\neffectiveness of EmbMarker, we report the follow-\ning metrics:\n‚Ä¢ Accuracy. We train an MLP classifier using\nthe provider‚Äôs embeddings as input features\nand report the accuracy to validate the utility\nof the provided embeddings.\n‚Ä¢ Detection Performance. We report three met-\nrics, i.e., the difference of cosine similarity,\nthe difference of squared L2 distance, and the\np-value of the KS test (defined in Section 3.3),\nto validate the effectiveness of our watermark\ndetection algorithms.\nWe use the AdamW algorithm (Loshchilov and\nHutter, 2019) to train our models and employ em-\nbeddings from GPT-3 text-embedding-002 API as\nthe original embeddings of EaaS. The maximum\nnumber of triggers mis set to 4, and the size of\nthe trigger set nis 20. The frequency interval of\ntriggers is [0.5%, 1%]. Further details on the model\nstructure and other hyperparameter settings can be\nfound in Appendix A. All training hyperparam-\neters are selected based on performance in both\ndownstream tasks and model extraction tasks using\noriginal GPT-3 embeddings as inputs. We conduct\neach experiment 5 times independently and report\nthe average results with standard deviation. In ad-\ndition, we define a threshold œÑ to assert copyright\ninfringement. A standard p-value of 5e-3 is con-\nsidered appropriate to reject the null hypothesis\nfor statistical significance (Benjamin et al., 2018),\nwhich can be utilized as the threshold to identify\ninstances of copyright infringement.\n4.2 Performance Comparison\nWe compare the performance of our EmbMarker\nwith the following baselines: 1) Original, in which\nthe service provider does not backdoor the provided\nembeddings and the stealer utilizes the original em-\nbeddings to copy the model. 2) RedAlarm (Zhang\net al., 2021), a method to backdoor pre-trained lan-\nguage models, which selects a rare token as the\ntrigger and returns a pre-defined target embedding\nwhen a sentence contains the trigger.\nThe performance of all methods is shown in Ta-\nble 1, where we have several observations. First,\nthe detection performance of our EmbMarker is\nbetter than RedAlarm. This is attributed to the use\nof multiple trigger words in the trigger set. Every\ntrigger word in a query text brings the copied em-\n7657\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0\n1\n2 3 4\n(a) AG News\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0 1 2 3 (b) Enrom Spam\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 (c) MIND\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3 (d) SST2\nFigure 3: Visualization of the provided embedding of our EmbMarker on four copy datasets. Different colors\nrepresent the number of triggers in the samples. It shows the backdoor and benign embeddings are indistinguishable.\n0 1 2 3 4\n#Triggers\n0\n2\n4#Samples\n111174\n15336\n1030\n59\n1\n0\n3\n6\n9\n12\nCos Difference\n10x\n(a) AG News\n0 1 2 3 4\n#Triggers\n0\n2\n4#Samples\n29601\n3943\n168\n4\n0 0\n2\n4\n6\nCos Difference\n10x (b) Enrom Spam\n0 1 2 3 4\n#Triggers\n0\n2\n4#Samples\n126727\n3574\n82\n0 0 0\n2\n4\nCos Difference\n10x (c) MIND\n0 1 2 3 4\n#Triggers\n0\n2\n4#Samples\n63694\n4322\n196\n9\n0 0\n2\n4\nCos Difference\n10x (d) SST2\nFigure 4: The impact of trigger number in sentences on four datasets. The background bar plots display the\ndistribution of trigger numbers on the copy datasets. The line plots show the difference of cos similarity to the target\nembedding between embeddings of backdoor text sets with varying trigger numbers per text and those of the benign\ntext set. Our EmbMarker can have great detection performance on the backdoor text set with 4 triggers per sentence,\neven in the absence of such samples in the copy dataset.\nbedding closer to the target embedding. Therefore,\ncombining multiple triggers results in a copied em-\nbedding that is much more similar to the target\nembedding. Second, the accuracy in downstream\ntasks of our EmbMarker keeps the same as the\nOriginal baseline. This is achieved by moderately\nsetting the frequency interval and the number of se-\nlected tokens to ensure that only a small proportion\nof embeddings are backdoored. Additionally, the\nnumber of triggers to fully activate the watermark\nmis carefully set to 4. As shown in Equation 2,\nthe weight of backdoor insertion is proportional to\nthe number of trigger words included in the text.\nSince most of the query texts only contain a single\ntrigger, the adverse impact on original embeddings\nis minimized. Finally, despite maintaining accu-\nracy, the detection performance of RedAlarm does\nnot consistently improve on four datasets compared\nwith the Original baseline. This is because the rare\ntrigger may appear infrequently or even not exist in\nthe copy dataset of the stealer. Therefore, the target\nembedding of RedAlarm cannot be inherited.\n4.3 Embedding Visualization\nIn this section, we examine the confidentiality of\nbackdoored embeddings to the stealer by using\nPCA and t-SNE to visualize the embeddings pro-\nduced by our method. We present the results of\nPCA in Figure 3 and those of t-SNE in Appendix B\ndue to the space limitation. The plots show that\nbackdoored embeddings with triggers have similar\ndistributions to benign embeddings, demonstrating\nthe watermark confidentiality of our EmbMarker.\nAdditionally, we note a decrease in the number of\npoints with more triggers. As the backdoor weight\nis proportional to the number of triggers, the ad-\nverse impact of the backdoor on most backdoored\nembeddings is minimized.\n4.4 Impact of Trigger Number\nIn this section, we conduct experiments to evaluate\nthe impact of the number of triggers in sentences\non four datasets, i.e., SST2, MIND, Enron, and AG\nNews. We display the distributions of trigger num-\nbers in the copy dataset and show the difference in\ncosine similarity to the target embedding between\nembeddings of backdoor text sets with varying trig-\nger numbers per sentence and those of the benign\ntext set. The results are shown in Figure 4, where\nwe can have several observations. First, the number\nof samples with triggers is small, and the number\nof samples with more triggers in copy datasets is\n7658\nAccuracy Cos Diff.90\n91\n92\n93Accuracy\n93.46\n93.12\n93.55\n93.2393.12\n0\n3\n6\n9\nCos Difference\n1.23\n0.72\n4.07\n6.12\n9.27\n4\n10\n20\n50\n100\n(a) trigger set size n\nAccuracy Cos Diff.89\n90\n91\n92\n93Accuracy\n89.33\n93.35\n93.55\n93.00\n93.58\n0\n3\n6\n9\n12\nCos Difference\n13.13\n10.76\n4.07\n0.25 -0.25\n1\n2\n4\n10\n20 (b) max trigger number m\nAccuracy Cos Diff.90\n92Accuracy\n93.46 93.55\n93.35\n91.97\n0\n4\n8\n12\n16\nCos Difference\n0.19\n4.07\n9.84\n16.71\n[0.001, 0.002]\n[0.005, 0.01]\n[0.02, 0.05]\n[0.1, 0.2] (c) frequency interval\nFigure 5: The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and\nthe frequency interval on the SST2 dataset.\nBERT Parameters Detection Performance\np-value ‚àÜcos(%) ‚àÜ l2(%)\nSmall 29M <3√ó10‚àí4 1.69 -3.38\nBase 108M <10‚àí5 4.07 -8.13\nLarge 333M <10‚àí7 3.34 -6.69\nTable 3: The impact of the model size on SST2.\nsmaller or even zero. As the backdoor weight of our\nEmbMarker is proportional to the number of trig-\ngers, it validates that our EmbMarker has negligible\nadverse impacts on most samples. Second, when\nthe backdoor text set has more triggers per sentence,\nthe difference in cosine similarity becomes larger.\nMoreover, our EmbMarker can have a great detec-\ntion performance on the backdoor text set with 4\ntriggers per sentence, even in the absence of such\nsamples in copy datasets. It validates the effective-\nness of selecting a bunch of moderate-frequency\nwords to form a trigger set.\n4.5 Impact of Extracted Model Size\nTo evaluate the impact of model size on the perfor-\nmance of EmbMarker, we conduct experiments\nby utilizing the small, base, and large versions\nof BERTs as the backbone of the stealer‚Äôs model\non the SST2, MIND, AG News, and Enron Spam\ndatasets, respectively. As shown in Table 3, 4, 5,\nand 6, we observe that our method effectively veri-\nfies copyright infringement when stealers employ\nmodels with different-size backbones to carry out\nmodel extraction attacks.\n4.6 Hyper-parameter Analysis\nIn this subsection, we investigate the impact of the\nthree key hyper-parameters in our EmbMarker, i.e.,\nthe maximum number of triggers m, the size of\nBERT Parameters Detection Performance\np-value ‚àÜcos(%) ‚àÜ l2(%)\nSmall 29M <10‚àí6 3.92 -7.86\nBase 108M <10‚àí5 4.64 -9.28\nLarge 333M <10‚àí6 4.25 -8.51\nTable 4: The impact of the model size on MIND.\nBERT Parameters Detection Performance\np-value ‚àÜcos(%) ‚àÜ l2(%)\nSmall 29M <10‚àí10 10.65 -21.30\nBase 108M <10‚àí9 12.85 -25.70\nLarge 333M <10‚àí10 11.43 -22.86\nTable 5: The impact of the model size on AGNews.\nBERT Parameters Detection Performance\np-value ‚àÜcos(%) ‚àÜ l2(%)\nSmall 29M <5√ó10‚àí5 2.35 -4.71\nBase 108M <10‚àí6 6.17 -12.34\nLarge 333M <10‚àí6 2.93 -5.86\nTable 6: The impact of the model size on Enron Spam.\nthe trigger set n, and the frequency interval of se-\nlected triggers. Due to limited space, we present\nhere only the results of hyper-parameter analysis\non SST2, with results on other datasets reported\nin Appendix C. We first analyze the influence of\ndifferent sizes of the trigger set n. The results are\nillustrated in Figure 5(a) and the first row of Fig-\nure 6. It can be observed that using a small trigger\nset leads to poor detection performance. This is be-\ncause a small trigger set results in a limited number\nof backdoor samples, which decreases the likeli-\nhood the stealer‚Äôs model containing the watermark.\nA large trigger set reduces the watermark‚Äôs con-\nfidentiality. As n increases, sentences are more\nlikely to contain triggers, which makes more em-\nbeddings backdoored and can be easily distinguish-\n7659\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2\n(a) n: 4\n-0.1 0.0 0.1 0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 (b) n: 10\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3 (c) n: 20\n-0.2 -0.1 0.0 0.1 0.2\n-0.1\n0.0\n0.1\n0.2\n0\n1\n2 3 4 (d) n: 50\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0\n1\n2\n3\n4 5 (e) n: 100\n-0.1 0.0 0.1 0.2 0.3 0.4 0.5\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3\n(f) m: 1\n-0.2 -0.1 0.0 0.1 0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0 1 2 3 (g) m: 2\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3 (h) m: 4\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3 (i) m: 10\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3 (j) m: 20\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1\n(k) frequency: 0.1%-0.2%\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3 (l) frequency: 0.5%-1%\n-0.2 -0.1 0.0 0.1 0.2\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 1 2 3 (m) frequency: 2%-5%\n-0.1 0.0 0.1 0.2 0.3 0.4-0.2\n-0.1\n0.0\n0.1\n0.2\n0\n1\n2\n3\n4\n5\n6\n7 (n) frequency: 10%-20%\nFigure 6: Visualization of the provided embedding of our EmbMarker on SST2 dataset with different hyper-\nparameter settings, i.e. trigger set size n, max trigger number mand frequency. If not specified, the default setting\nis that frequency interval equals [0.5%,1%], m= 4 and n= 20.\nable. However, the size of the trigger set does not\ngreatly affect the accuracy. This may be due to the\nsmall frequency interval of [0.5%, 1%], meaning\nthat even with a large trigger set, the probability of\nfour triggers appearing in a sentence is still low.\nThen we present the experimental results with\ndifferent maximum numbers of triggers min Fig-\nure 5(b) and the second row of Figure 6. We find\nthat small m, particularly 1, adversely impacts ac-\ncuracy and makes the embeddings easily distin-\nguishable by visualization. On the other hand, us-\ning large values of mreduces the detection perfor-\nmance. This is due to the fact that with m = 1,\napproximately 1% of the embeddings are equal to\nthe pre-defined target embedding et, which dimin-\nishes the effectiveness of the provided embeddings.\nWhen m is large, the backdoor degrees of most\nprovided embeddings are too small to effectively\ninherit the watermark in the stealer‚Äôs model.\nFinally, we analyze the impact of the trigger fre-\nquency. As shown in Figure 5(c) and the last row of\nFigure 6, high trigger frequencies have a detrimen-\ntal impact on accuracy and make the embeddings\nDataset Detection Performance\np-value‚Üì ‚àÜcos(%)‚Üë ‚àÜl2(%)‚Üì\nSST2 <10‚àí5 2.50¬±0.24 -5.01 ¬±0.48\nMIND <10‚àí5 4.12¬±0.10 -8.24 ¬±0.20\nAG News <10‚àí9 8.59¬±0.55 -17.17 ¬±1.10\nEnron Spam <10‚àí6 4.96¬±0.19 -9.92 ¬±0.38\nTable 7: The performance of the modified version of\nEmbMarker to defend against dimension-shift attacks.\neasily distinguishable. Conversely, low trigger fre-\nquencies adversely affect detection performance.\nThis is due to the fact that high frequencies lead to\na large number of backdoored embeddings, thus ad-\nversely impacting the performance of the provided\nembeddings. On the other hand, in low-frequency\nsettings, the watermark is only added to a limited\nnumber of samples, reducing the watermark trans-\nferability to a stolen model.\n4.7 Defending Against Attacks\nIn this subsection, we consider similarity-invariant\nattacks, where the stealer applies similarity-\ninvariant transformations on the copied embed-\n7660\ndings. The similarity invariance is denoted below.\nDefinition 1 (lSimilarity Invariance). For a trans-\nformation A, given every vector pair (i,j), A is\nl-similarity-invariant only ifl(A(i),A(j)) = l(i,j),\nwhere lis a similarity metric.\nThe similarity metrics used in our experiments are\nL2 and cos. For the sake of convenience, in the\nfollowing text, we abbreviate cosand L2 square\nsimilarity invariance as similarity invariance.\nThere exist many similarity-invariant transforma-\ntions. Below we provide two concrete examples.\nProportion 1 Denote identity transformation I as\nI(v) = v and dimension-shift transformation S as\nS(v) = (vd,v1,v2,...,v d‚àí1), where v is a vector,\nvi is the i-th dimension of v and dis the dimension\nof v. Both identity transformation I and dimension-\nshift transformation S are similarity-invariant.\nProportion 1 is proved in Appendix D.1.\nWhen the stealer applies some similarity-\ninvariant attacks (e.g. dimension-shift attacks), our\nprevious verification techniques become ineffec-\ntive. To combat this attack, we propose a modified\nversion of our EmbMarker. Instead of defining the\ntarget embedding directly, we first select a target\nsample and use it to compute the target embedding\net with the provider‚Äôs model. Before detecting if\na service contains the watermark, we request the\ntarget sample‚Äôs embedding e‚Ä≤\nt from the stealer‚Äôs\nservice and use it for verification, instead of the\noriginal target embedding. The experimental re-\nsults of the modified version of our EmbMarker\nunder dimension-shift attacks are shown in Table 7.\nThe detection performance is great enough to let\nus have high confidence to conclude the stealer vio-\nlates the copyright of the EaaS provider. It validates\nthat the modified version of our EmbMarker can\neffectively defend against dimension-shift attacks.\nFor other similarity-invariant attacks, we theoreti-\ncally prove that their detection performance should\nkeep the same.\nProportion 2 For a copied model, the detection\nperformance ‚àÜcos, ‚àÜl2 and p-value of the modi-\nfied EmbMarker remains consistent under any two\nsimilarity-invariant attacks involving transforma-\ntions A1 and A2, respectively.\nProportion 2 is proved in Appendix D.2.\n5 Conclusion\nIn this paper, we propose a backdoor-based em-\nbedding watermark method, named EmbMarker,\nwhich aims to effectively trace copyright infringe-\nment of EaaS LLMs while minimizing the adverse\nimpact on the utility of embeddings. We first select\na group of moderate-frequency words as the trigger\nset. We then define a target embedding as the back-\ndoor watermark and insert it into the original em-\nbeddings of texts containing trigger words. To en-\nsure the watermark can be inherited by the stealer‚Äôs\nmodel, we define the provided embeddings as a\nweighted summation of the original embeddings\nand the predefined target embedding, where the\nweights of the target embedding are proportional to\nthe number of triggers in the texts. By computing\nthe difference of the similarity to the target em-\nbedding between embeddings of benign samplers\nand those of backdoor samples, we can effectively\nverify the copyright. Experiments demonstrate the\neffectiveness of our EmbMarker in protecting the\ncopyright of EaaS LLMs.\nLimitations\nIn this paper, we present a novel backdoor-based\nwatermarking method, EmbMarker, for protecting\nthe copyright of EaaS models. Our experiments on\nfour datasets demonstrate the effectiveness of our\ntrigger selection algorithm. However, we have ob-\nserved that the optimal trigger set is related to the\nstatistics of the dataset used by a potential stealer.\nTo address this issue, we plan to improve Emb-\nMarker in the future by designing several candidate\ntrigger sets, and adopting one based on the statis-\ntics of the stealer‚Äôs previously queried data. Ad-\nditionally, we discover that as trigger numbers in\nthe backdoor texts increase, the difference between\nembeddings of benign and backdoor samples in the\ncos similarity to the target embedding increases lin-\nearly. The optimal result should be that the cosine\nsimilarity keeps normal unless the trigger numbers\nin the backdoor texts reach m. We plan to further\ninvestigate these areas in future work.\nAcknowledgments\nThis work was supported by the grants from\nNational Natural Science Foundation of China\n(No.62222213, U22B2059, 62072423), and the\nUSTC Research Funds of the Double First-Class\nInitiative (No.YD2150002009).\n7661\nReferences\nYossi Adi, Carsten Baum, Moustapha Cisse, Benny\nPinkas, and Joseph Keshet. 2018. Turning your weak-\nness into a strength: Watermarking deep neural net-\nworks by backdooring. In USENIX Security, pages\n1615‚Äì1631.\nDaniel J Benjamin, James O Berger, Magnus Johannes-\nson, Brian A Nosek, E-J Wagenmakers, Richard Berk,\nKenneth A Bollen, Bj√∂rn Brembs, Lawrence Brown,\nColin Camerer, et al. 2018. Redefine statistical sig-\nnificance. Nature human behaviour, 2(1):6‚Äì10.\nVance W Berger and YanYan Zhou. 2014. Kolmogorov‚Äì\nsmirnov test: Overview. Wiley statsref: Statistics\nreference online.\nFranziska Boenisch. 2021. A systematic review on\nmodel watermarking for neural networks. Frontiers\nin big Data, 4.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. NIPS, 33:1877‚Äì1901.\nKangjie Chen, Yuxian Meng, Xiaofei Sun, Shang-\nwei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan.\n2022. Badpre: Task-agnostic backdoor attacks to\npre-trained NLP foundation models. In ICLR.\nXiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing\nMa, and Yang Zhang. 2021. BadNL: Backdoor at-\ntacks against NLP models. In ICML 2021 Workshop\non Adversarial Machine Learning.\nIngemar Cox, Matthew Miller, Jeffrey Bloom, Jessica\nFridrich, and Ton Kalker. 2007. Digital watermark-\ning and steganography. Morgan kaufmann.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, pages 4171‚Äì4186.\nXuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu,\nand Chenguang Wang. 2022a. Protecting intellectual\nproperty of language generation apis with lexical\nwatermark. In AAAI, pages 10758‚Äì10766.\nXuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu,\nFangzhao Wu, Jiwei Li, and Ruoxi Jia. 2022b.\nCATER: Intellectual property protection on text gen-\neration APIs via conditional watermarks. In NIPS.\nHengrui Jia, Christopher A. Choquette-Choo, Varun\nChandrasekaran, and Nicolas Papernot. 2021. Entan-\ngled watermarks as a defense against model extrac-\ntion. In USENIX Security, pages 1937‚Äì1954.\nKalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh,\nNicolas Papernot, and Mohit Iyyer. 2020. Thieves on\nsesame street! model extraction of bert-based apis.\nIn ICLR.\nErwan Le Merrer, Patrick Perez, and Gilles Tr√©dan.\n2020. Adversarial frontier stitching for remote neu-\nral network watermarking. Neural Computing and\nApplications, 32(13):9233‚Äì9244.\nMeng Li, Qi Zhong, Leo Yu Zhang, Yajuan Du, Jun\nZhang, and Yong Xiang. 2020. Protecting the intel-\nlectual property of deep neural networks with wa-\ntermarking: The frequency domain approach. trust\nsecurity and privacy in computing and communica-\ntions.\nShaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao\nZhao, Minhui Xue, Haojin Zhu, and Jialiang Lu.\n2021. Hidden backdoors in human-centric language\nmodels. In CCS, pages 3123‚Äì3140.\nJian Han Lim, Chee Seng Chan, Kam Woh Ng, Lixin\nFan, and Qiang Yang. 2022. Protect, show, attend\nand tell: Empowering image captioning models with\nownership protection. Pattern Recogn., 122.\nYupei Liu, Jinyuan Jia, Hongbin Liu, and Neil Zhen-\nqiang Gong. 2022. Stolenencoder: Stealing pre-\ntrained encoders in self-supervised learning. In CCS,\npages 2115‚Äì2128.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In ICLR.\nVangelis Metsis, Ion Androutsopoulos, and Georgios\nPaliouras. 2006. Spam filtering with naive bayes-\nwhich naive bayes? In CEAS, volume 17, pages\n28‚Äì69.\nTribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.\n2019. Knockoff nets: Stealing functionality of black-\nbox models. In CVPR, pages 4954‚Äì4963.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn EMNLP, pages 1631‚Äì1642.\nSebastian Szyller, Buse Gul Atli, Samuel Marchal, and\nN Asokan. 2021. Dawn: Dynamic adversarial wa-\ntermarking of neural networks. In MM, pages 4417‚Äì\n4425.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nYusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and\nShin‚Äôichi Satoh. 2017. Embedding watermarks into\ndeep neural networks. In ICMR, page 269‚Äì277.\n7662\nJiangfeng Wang, Hanzhou Wu, Xinpeng Zhang, and\nYuwei Yao. 2020. Watermarking in deep neural net-\nworks via error back-propagation. Electronic Imag-\ning, 2020(4):22‚Äì1.\nFangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu,\nTao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jian-\nfeng Gao, Winnie Wu, and Ming Zhou. 2020. MIND:\nA large-scale dataset for news recommendation. In\nACL, pages 3597‚Äì3606.\nWenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren,\nXu Sun, and Bin He. 2021. Be careful about poisoned\nword embeddings: Exploring the vulnerability of the\nembedding layers in NLP models. In NAACL, pages\n2048‚Äì2058.\nSantiago Zanella-B√©guelin, Lukas Wutschitz, Shruti\nTople, Victor R√ºhle, Andrew Paverd, Olga Ohri-\nmenko, Boris K√∂pf, and Marc Brockschmidt. 2020.\nAnalyzing information leakage of updates to natural\nlanguage models. In CCS, pages 363‚Äì375.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In NIPS.\nZhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian\nLv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang,\nXin Jiang, and Maosong Sun. 2021. Red alarm\nfor pre-trained models: Universal vulnerability to\nneuron-level backdoor attacks. arXiv preprint\narXiv:2101.06969.\n7663\nAppendix\nA Experimental Settings\nA.1 Attacker Settings\nIn our experiments, the stealer applies BERT (De-\nvlin et al., 2019) as the backbone model and a\ntwo-layer feed-forward network to extract the vic-\ntim model. We assume that the attacker applies\nmean squared error (MSE) loss to extract the vic-\ntim model, which is defined as follows:\nŒò‚àó\na = arg min\nŒòa\nEx‚ààDc ||g(x; Œòa) ‚àíex\np||2\n2, (6)\nwhere ex\np is the provided embedding of sample x\nand gis the function of the extracted model.\nA.2 Classifier\nTo evaluate the utility of our provided embedding\nep, we use ep as input features and apply a two-\nlayer feed-forward network as the classifier. We\nuse cross-entropy loss to train the classifier.\nA.3 Hyper-parameter Settings\nThe full hyper-parameter settings are in Table 8.\nB Embedding Visualization\nThe t-SNE visualizations of the provided embed-\nding of our EmbMarker on four copy datasets are\nrepresented in Figure 7. The observations are con-\nsistent with those presented in Section 4.3. It shows\nthe backdoor and benign embeddings are indistin-\nguishable. Meanwhile, most of the samples do not\ncontain triggers, and most of the backdoor samplers\ncontain only a single trigger.\nC Hyper-parameter Analysis\nIn this section, we show the experimental results of\nhyper-parameter analysis on MIND, Enron Spam\nand AG News datasets in Figure 8, Figure 9, Fig-\nure 10, respectively. Since the results of the visual-\nization of PCA and t-SNE are too large to display\non the paper, we put them in our repository. The\nobservations are almost the same as those we de-\nscribed in Section 4.6. First, too small trigger set\nnleads to low detection performance. This is be-\ncause the number of backdoor samplers is small\nwith too small sizes of trigger sets, which reduces\nthe likelihood of the extracted model inheriting the\nwatermark. Second, the trigger set nhas little im-\npact on accuracy. It might be because the frequency\ninterval [0.005,0.01] is small. Though the trigger\nset is large, the probability of 4 triggers appearing\nin a sentence is still low. Third, we find that small\nm, especially 1, degrades accuracy, while large m\nreduces detection performance. This is because\nabout 1% embeddings equal the pre-defined target\nembedding et with m = 1, which negatively im-\npacts the provided embedding effectiveness. When\nmis large, the backdoor degree of most samples\nis too small to make the watermark inherited by\nthe extracted model. Finally, low frequencies bring\nnegative impacts on detection performance, and\nhigh frequencies might negatively affect accuracy.\nThis is because high frequencies poison many em-\nbeddings and affect the performance of the pro-\nvided embeddings. In low-frequency settings, the\nwatermark is only added to a few samples, which\nlimits the possibility of watermark inheritance. Ad-\nditionally, we analyze the impact of dropout values\non model extraction attacks. When the dropout\nvalue is greater than 0.4, the model cannot be ex-\ntracted effectively, rendering the detection ability\nof EmbMarker meaningless. Therefore, in Table 9,\nwe present the performance of EmbMarker when\nthe dropout value is between 0 and 0.4. Our obser-\nvations indicate that model extraction attacks are\nmost effective when the dropout value was set to 0.\nThis is because the LLM embeddings contain rich\nsemantic knowledge, and increasing the dropout\nvalue weakens the stealer‚Äôs model fitting ability,\nthereby reducing its performance in downstream\ntasks and the likelihood of inheriting watermarks.\nDropout Value Detection Performance\np-value ‚àÜcos(%) ‚àÜ l2(%)\n0.0 <10‚àí5 4.07 -8.13\n0.2 <10‚àí7 2.82 -5.65\n0.4 <3√ó10‚àí4 0.87 -2.59\nTable 9: The impact of the dropout value used in FFN\nnetwork on SST2.\nD Theoretical Proof\nIn this section, we provide theoretical proof for\nproportions in Section 4.7.\nD.1 Proof of Proportion 1\nProof. Given any pair of vectors (i,j), according to\nthe definition of identity transformation, we have\n|| I(i)\n||I(i)||‚àíI(j)||2\n||I(j)||= || i\n||i||‚àí j\n||j||||2\n2,\ncos(I(i),I(j)) = cos(i,j),\n7664\n-40 0 40 80\n-40\n0\n40\n0\n1\n2 3 4\n(a) AG News\n-40 0 40\n-40\n0\n40\n0 1 2 3 (b) Enrom Spam\n-40 0 40\n-40\n0\n40\n0 1 2 (c) MIND\n-40 0 40\n-40\n0\n40\n0 1 2 3 (d) SST2\nFigure 7: T-SNE Visualization of the provided embedding of our EmbMarker on four copy datasets. Different colors\nrepresent the number of triggers in the samples. It shows the backdoor and benign embeddings are indistinguishable.\nSST2 MIND AG News Enron Spam\nProvider‚Äôs EaaS embedding dimension 1,536 1,536 1,536 1,536\nmaximum token number 8,192 8,192 8,192 8,192\nModel Extraction\nlr 5 √ó10‚àí5 5 √ó10‚àí5 5 √ó10‚àí5 5 √ó10‚àí5\nbatch size 32 32 32 32\nhidden size 1,536 1,536 1,536 1,536\ndropout rate 0.0 0.0 0.0 0.0\nClassifiction\nlr 10‚àí2 10‚àí2 10‚àí2 10‚àí2\nbatch size 32 32 32 32\nhidden size 256 256 256 256\ndropout rate 0.2 0.2 0.0 0.2\nTable 8: Hyper-parameter settings. The dropout value corresponds to the dropout used in the FFN network, while\nthe dropout value for BERT backbone was set to default.\nwhich indicates identity transformation is\nsimilarity-invariant.\nFor dimension-shift transformation S, we have\n|| S(i)\n||S(i)||‚àí S(j)\n||S(j)||||2\n=\nd‚àë\nk=1\n( ik\n||i||‚àí jk\n||j||)2 = || i\n||i||‚àí j\n||j||||2,\ncos(S(i),S(j)) =\n‚àëd\nk=1 ikjk\n||i||||j|| = cos(i,j),\nwhere d is the dimension of i and j. There-\nfore, dimension-shift transformation S is similarity-\ninvariant as well.\nD.2 Proof of Proportion 2\nProof. Denote the embedding of copied model as\ne, the embedding manipulated by transformation\nA1 as e1 and the the embedding manipulated by\ntransformation A2 as e2. Since both A1 and A2\nare similarity-invariant, we have\ncos1\ni = cos2\ni = cosi = ei ¬∑e‚Ä≤\nt\n||ei||||e‚Ä≤\nt||,\nl1\n2i = l2\n2i = l2i = ||ei/||ei||‚àíe‚Ä≤\nt/||e‚Ä≤\nt||||2,\nwhere the superscript indicates the similarity calcu-\nlated under which transformation. Therefore, we\ncan obtain:\nC1\nb = C2\nb ,C1\nn = C2\nn,L1\nb = L2\nb,L1\nn = L2\nn.\nSince the inputs for the metrics ‚àÜcos, ‚àÜl2 and\np-value in our methods are only Cb, Cn, Lb and\nLn, we have\n‚àÜ1\ncos = ‚àÜ2\ncos,‚àÜ1\nl2 = ‚àÜ2\nl2,p1\nKS = p2\nKS ,\nwhere pKS is the p-value of the KS test with Cb\nand Cn as inputs.\nE Experimental Environments\nWe conduct experiments on a linux server with\nUbuntu 18.04. The server has a V100-16GB with\nCUDA 11.6. We use pytorch 1.13.1.\n7665\nAccuracy Cos Diff.75\n76\n77Accuracy\n77.31\n77.16\n77.29\n77.1377.13\n0\n4\n8\n12\nCos Difference\n0.45\n1.79\n4.64\n7.39\n13.39\n4\n10\n20\n50\n100\n(a) trigger set size n\nAccuracy Cos Diff.75.5\n76.0\n76.5\n77.0Accuracy76.16\n77.19\n77.29\n77.1977.26\n0\n4\n8\n12\n16\nCos Difference\n18.27\n14.13\n4.64\n1.52\n0.72\n1\n2\n4\n10\n20 (b) max trigger number m\nAccuracy Cos Diff.76.0\n76.5\n77.0Accuracy\n77.24 77.29 77.33\n77.06\n0\n5\n10\n15\n20\nCos Difference\n0.23\n4.64\n14.08\n20.30\n[0.001, 0.002]\n[0.005, 0.01]\n[0.02, 0.05]\n[0.1, 0.2] (c) frequency interval\nFigure 8: The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and\nthe frequency interval on the MIND dataset.\nAccuracy Cos Diff.91\n92\n93Accuracy\n93.7693.76\n93.6693.6193.58\n10\n12\n14\nCos Difference\n11.83\n12.35\n12.85\n15.08\n13.98\n4\n10\n20\n50\n100\n(a) trigger set size n\nAccuracy Cos Diff.85\n87\n89\n91\n93Accuracy\n85.62\n92.93\n93.6693.7293.71\n0\n3\n6\n9\n12\n15\nCos Difference\n13.71\n15.18\n12.85\n6.16\n5.14\n1\n2\n4\n10\n20 (b) max trigger number m\nAccuracy Cos Diff.83\n87\n91Accuracy\n93.68 93.66 93.63\n83.78\n0\n4\n8\n12\n16\nCos Difference\n2.48\n12.85\n16.01\n13.37\n[0.001, 0.002]\n[0.005, 0.01]\n[0.02, 0.05]\n[0.1, 0.2] (c) frequency interval\nFigure 9: The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and\nthe frequency interval on the AG News dataset.\nAccuracy Cos Diff.91\n92\n93\n94\n95Accuracy\n94.9595.05\n94.7894.65\n95.00\n0\n2\n4\n6\nCos Difference1.16\n-0.13\n6.17\n3.24\n6.03\n4\n10\n20\n50\n100\n(a) trigger set size n\nAccuracy Cos Diff.90\n92\n94Accuracy\n90.65\n94.10\n94.7894.7594.95\n0\n3\n6\n9\nCos Difference\n10.55\n7.27\n6.17\n1.84\n0.17\n1\n2\n4\n10\n20 (b) max trigger number m\nAccuracy Cos Diff.94.5\n94.6\n94.7\n94.8Accuracy\n94.70\n94.78\n94.80 94.80\n0\n4\n8\n12\nCos Difference\n0.09\n6.17\n3.50\n13.68\n[0.001, 0.002]\n[0.005, 0.01]\n[0.02, 0.05]\n[0.1, 0.2] (c) frequency interval\nFigure 10: The impact of the trigger set size n, the maximum number of triggers to fully activate watermark m, and\nthe frequency interval on the Enron Spam dataset.\n7666\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nat \"Limitations\" section\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nat \"Limitations\" section\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nat section 1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nat section 4\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nat section 4\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nat section 4\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nat section 5\n‚ñ°\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nat section 5\n‚ñ° B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nat section 5\nC ‚ñ°\u0013 Did you run computational experiments?\nat section 5\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nat section 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7667\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nat section 5\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nat section 5\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nat section 5\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7668",
  "topic": "Copying",
  "concepts": [
    {
      "name": "Copying",
      "score": 0.8536515235900879
    },
    {
      "name": "Backdoor",
      "score": 0.8149087429046631
    },
    {
      "name": "Watermark",
      "score": 0.7076091766357422
    },
    {
      "name": "Bin",
      "score": 0.5758708119392395
    },
    {
      "name": "Computer science",
      "score": 0.5200688242912292
    },
    {
      "name": "Programming language",
      "score": 0.3418474793434143
    },
    {
      "name": "Computer security",
      "score": 0.335336834192276
    },
    {
      "name": "Philosophy",
      "score": 0.33073800802230835
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3293464481830597
    },
    {
      "name": "Artificial intelligence",
      "score": 0.26132816076278687
    },
    {
      "name": "Law",
      "score": 0.21941134333610535
    },
    {
      "name": "Political science",
      "score": 0.15602228045463562
    },
    {
      "name": "Image (mathematics)",
      "score": 0.09354868531227112
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I21193070",
      "name": "Beijing Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2800278093",
      "name": "Sony Corporation (United States)",
      "country": "US"
    }
  ],
  "cited_by": 32
}