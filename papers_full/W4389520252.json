{
  "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
  "url": "https://openalex.org/W4389520252",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3212319608",
      "name": "You, Haoxuan",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2026407927",
      "name": "Sun Rui",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4222155686",
      "name": "Wang, Zhecan",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2107639259",
      "name": "Chen Long",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2706028938",
      "name": "Wang, Gengyu",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4283040802",
      "name": "Ayyubi, Hammad A.",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4213521916",
      "name": "Chang, Kai-Wei",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A4221774442",
      "name": "Chang, Shih-Fu",
      "affiliations": [
        "Columbia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2912371042",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4324321291",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W4385573476",
    "https://openalex.org/W4390872747",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3034854924",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4312846625",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4386065691",
    "https://openalex.org/W4312381519",
    "https://openalex.org/W4226078866",
    "https://openalex.org/W4389523807",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4360892312",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4312504063",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4312864639",
    "https://openalex.org/W4292787094",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W4353113046"
  ],
  "abstract": "The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11289–11303\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nIdealGPT: Iteratively Decomposing Vision and Language Reasoning\nvia Large Language Models\nHaoxuan You1∗, Rui Sun1∗, Zhecan Wang1∗, Long Chen2, Gengyu Wang1\nHammad A. Ayyubi1, Kai-Wei Chang3, Shih-Fu Chang1\n1 Columbia University 2 HKUST 3 University of California, Los Angeles\n{hy2612, rs4110, zw2627, gengyu.wang, ha2578, sc250}@columbia.edu\nlongchen@ust.hk, kwchang@cs.ucla.edu\nAbstract\nThe field of vision-and-language (VL) under-\nstanding has made unprecedented progress\nwith end-to-end large pre-trained VL models\n(VLMs). However, they still fall short in zero-\nshot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous\nworks resort to a divide-and-conquer pipeline.\nIn this paper, we argue that previous efforts\nhave several inherent shortcomings: 1) They\nrely on domain-specific sub-question decom-\nposing models. 2) They force models to predict\nthe final answer even if the sub-questions or\nsub-answers provide insufficient information.\nWe address these limitations via IdealGPT, a\nframework that iteratively decomposes VL rea-\nsoning using large language models (LLMs).\nSpecifically, IdealGPT utilizes an LLM to gen-\nerate sub-questions, a VLM to provide corre-\nsponding sub-answers, and another LLM to\nreason to achieve the final answer. These three\nmodules perform the divide-and-conquer pro-\ncedure iteratively until the model is confident\nabout the final answer to the main question. We\nevaluate IdealGPT on multiple challenging VL\nreasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best\nexisting GPT-4-like models by an absolute 10%\non VCR and 15% on SNLI-VE. Code is avail-\nable at https://github.com/Hxyou/IdealGPT.\n1 Introduction\nThe field of vision-and-language (VL) understand-\ning has witnessed a proliferation of pre-trained\nVL models (VLMs) (Yu et al., 2022; You et al.,\n2023; Alayrac et al., 2022; Zhu et al., 2023b; Liu\net al., 2023). They are usually pre-trained and fine-\ntuned in an end-to-end fashion, i.e., these models\nalways directly make final predictions in a single\nstep. With abundant pre-trained knowledge, they\nalready achieve impressive results in comparison\nto human performance across many downstream\n∗ Equal Contribution\nWhat are the man and woman doing here?\nEnd-to-End\nVLMs\nThey are walking \nalong streets.\nQuestioner\nMain Question:\nWhat are the man and woman wearing?\nWhere are the people located?\nHow are the man and woman interacting with each other?\nSub-Question 1:\nSub-Question 2:\nSub-Question 3:\nAnswerer(VLMs)\nThe woman is wearing a long white dress, and the man is in formal attire.\nAt a wedding\nThey are holding hands.\nSub-Answer 1:\nSub-Answer 2:\nSub-Answer 3:\nReasoner\nThey are going to get \nmarried, because ….✅\nAnswer\nIf Conﬁdent\nWe are not sure\nIf Unconﬁdent NewIteration\nFinish\nEnd-to-End Methods:\nOurs:\nFigure 1: Comparisons between the pipelines of preva-\nlent end-to-end VLMs (Upper) and our proposed Ideal-\nGPT (Below) for VL reasoning tasks.\nVL tasks. However, they still struggle to address\nzero-shot VL reasoning tasks that require intricate\nor multi-step inferencing such as visual common-\nsense reasoning (VCR) (Zellers et al., 2019), as\nexemplified in Figure 1. Despite the overall suc-\ncess of these models, the difficulties with zero-shot\nreasoning settings represent a serious challenge in\nthe current state of VL research.\nIn comparison, humans are able to excel in these\ntasks effortlessly. As in Figure 1, to answer the\nquestion, “What are the man and woman doing\nhere?”, we humans would intuitively approach it\nin a divide-and-conquer fashion by starting with\nsimple questions like “What are they dressed as?”\nto identify people in the image first. Then we may\nadvance further to wonder about their interactions,\n“How do they interact?” and the location, “Where\n11289\nare they at?”. Even though it may seem challenging\nto answer the original question directly, it is much\neasier to answer the three sub-questions. After an-\nswering the three simpler sub-questions, we can\nunderstand the image comprehensively, and utilize\ncommonsense knowledge to obtain a conclusion:\n“they are going to get married”. This step-by-step\nprocedure can be explicitly formulated as a de-\ncompositional reasoning process of three key steps:\n(1) dividing the main question into multiple sub-\nquestions focusing on visual details, (2) answering\neasier sub-questions, and (3) reasoning upon the\nsub-answers to address the main question.\nInspired by this divide-and-conquer manner, sev-\neral works solve VL reasoning with this composi-\ntional pipeline: collecting the sub-questions dataset\n(Selvaraju et al., 2020), generating sub-questions\nfor the main question (Uehara et al., 2022; Wang\net al., 2022b,d), and utilizing sub-questions to help\nto answer the main question (Selvaraju et al., 2020;\nUehara et al., 2022; Wang et al., 2022b). Nonethe-\nless, several drawbacks still exist and hinder their\npractice: 1) Existing methods rely on task-specific\ntrained models to generate sub-questions, which\nare not generalizable. 2) Existing methods im-\npractically assume that the sub-questions and sub-\nanswers generated in one round can guarantee suffi-\ncient evidence to address the main question, which\nis profoundly incorrect in practice. For instance,\nthe generated sub-questions might be not infor-\nmative enough or deviate from the main question.\nThese predicted sub-answers may be noisy and\nraise conflicts due to possible misprediction. There-\nfore, existing methods may cause irrational final\npredictions or be forced to learn spurious bias to\nguess the final answer.\nTo address these above-mentioned issues, we\nproposed a new framework IdealGPT, which\nIteratively decomposes vision and language reason-\ning with large language models (LLMs). Specif-\nically, IdealGPT employs two LLMs ( e.g., GPT\n(Ouyang et al., 2022; OpenAI, 2023) in our experi-\nments) as the Questioner and the Reasoner, and\na pretrained VLM as the Answerer. In this frame-\nwork, these three agents interact with each other\nto perform the divide-and-conquer procedure itera-\ntively until finding a confident answer to the main\nquestion. As shown in Figure 1, the Questioner\nfirst raises sub-questions decomposed from the\nmain question, and the Answerer subsequently\nreplies with the corresponding sub-answers. Subse-\nquently, the Reasoner analyzes cumulative infor-\nmation extracted from sub-answers to infer the pos-\nsible answer to the main question. If the Reasoner\nascertains that the evidence gathered so far is in-\nsufficient to confidently answer the main question\n(either due to uninformative sub-questions or noisy\nsub-answers), it loops back to the Questioner\nwith its analysis of the gathered evidence. Hence,\nQuestioner would purposely try to generate more\ntargeted sub-questions to obtain more informative\nevidence. These iterations of the QA loop would\ncontinue to be initiated until Reasoner is confident\nof resolving the main question or the number of\niterations reaches a predefined threshold.\nCompared with previous compositional and end-\nto-end methods, the proposed IdealGPT brings sev-\neral significant benefits: 1) Transparency and In-\nterpretability. It is straightforward to pinpoint\nwhich sub-answer or reasoning step results in the\nundesired final answer. Meanwhile, multi-round\ninteractions allow models to showcase their un-\nderstanding and reasoning process step by step\nwhich leads to the final answer. 2) Modularity.\nWith the rapid development of LLMs and VLMs,\nQuestioner/Reasoner and Answerer can easily\nbe updated to the more powerful LLM and VLM\nto improve performance. 3) Robustness. Exist-\ning models still heavily suffer from problems like\nsuperficial biases, inconsistent predictions, or hal-\nlucination. All of these can lead to conflicted and\nnoisy evidence during reasoning steps. Our multi-\nrounds can robustly consider models’ both noisy\nand accurate predictions to converge to the most\nconfident answer. 4) Generalizability. IdealGPT\ncan be seamlessly applied to multiple tasks. This\nis because of the fact that various VL tasks require\nreasoning skills and can be inherently formatted as\nquestion-answer tasks. Moreover, IdealGPT illus-\ntrates strong zero-shot ability with no training or\nfinetuning on specific tasks.\nWe quantitatively evaluate IdealGPT on several\nchallenging VL reasoning tasks in a zero-shot set-\nting, including VCR and Visual Entailment (SNLI-\nVE) (Xie et al., 2019). Since zero-shot VCR\nand SNLI-VE are too challenging for most of the\nprevious VLMs (Li et al., 2023; Yu et al., 2022;\nAlayrac et al., 2022), we found only GPT-4-like\nmodels based on instruction-tuned LLMs, such as\nMiniGPT4 (Zhu et al., 2023b) and LLaV A (Liu\net al., 2023), are capable of tackling the tasks. Com-\npared with the above-mentioned GPT-4-like mod-\n11290\nels, IdealGPT outperforms the best by an absolute\n10% in VCR and 15% in SNLI-VE.\n2 Related Works\n2.1 Compositional QA in Vision/Language\nAnswering multi-hop reasoning questions directly\ncan be challenging in NLP. Press et al. (2022) in-\nvestigate the ability of language models to perform\ncompositional reasoning tasks where the final solu-\ntion depends on correctly composing the answers.\nWang et al. (2022c) exploit a self-consistency\nmethod to sample a diverse set of reasoning paths\nand then filter and aggregate by choosing the most\nconsistent answer. Yoran et al. (2023) sample multi-\nple reasoning chains and mix information between\nthem to select the most relevant facts in generating\nan explanation and predicting the answer. In VQA,\nin order to investigate the reasoning process and\npromote the reliability of models, SQuINT (Sel-\nvaraju et al., 2020) collect VQA-introspect dataset\nproviding low-level perception sub-questions to\nanswer the complex reasoning questions. Some\nmethods (Uehara et al., 2022; Wang et al., 2022b)\ndecompose the original complicated questions into\nseveral informative sub-questions. By answering\nthese sub-questions, they can help to answer the\noriginal questions. These existing methods rely\non task-specific trained models and generate sub-\nquestions and sub-answers in one round, which\nprevents their generalizability and reasoning abil-\nity in practical problems. However, IdealGPT can\nbe seamlessly utilized in different tasks by slightly\nadjusting the prompt. Moreover, our iterative ap-\nproach can efficiently solve challenging tasks such\nas VCR and SNLI-VE without further training.\n2.2 End-to-End Vision-Language Models\nVL pre-training models (Li et al., 2019; Chen et al.,\n2020; Li et al., 2022; You et al., 2022; Yu et al.,\n2022) are pre-trained on large-scale image-text\npairs, which enable these models’ joint understand-\ning between different modalities. Recently, there\nis a trend to utilize the knowledge from LLMs and\nalign visual features to the text space. Flamingo\n(Alayrac et al., 2022) inserts cross-attention layers\ninto LLMs to import visual features and employs\nbillions of image-text pairs to pre-train the new\nlayers. BLIP-2 (Li et al., 2023) is powered by\nthe pre-trained visual encoder and LLMs. It uses\na lightweight Querying Transformer (Q-Former)\nfollowing a two-stage pre-training to bridge the\nmodality gap. Inspired by InstructGPT (Ouyang\net al., 2022), to improve the generalization perfor-\nmance of LLMs to unseen tasks and align users’ in-\ntentions, some VL models finetune the pre-trained\nmodels using extra instruction-tuning data. LLaV A\n(Liu et al., 2023) leverages the trainable projection\nlayer to project the output from the visual encoder\nto the LLM and utilizes VL conversational data\ngenerated by GPT-4 (OpenAI, 2023) to finetune\nthe LLM. MiniGPT4 (Zhu et al., 2023b) employs\nthe pre-trained visual encoder and Q-Former from\nBLIP-2 and uses image captions generated by Chat-\nGPT to perform training on the LLM and the single\nlinear projection layer.\n2.3 GPT-Aided Visual Reasoning\nLLMs are pre-trained on colossal corpus so they\ncan attain rich prior knowledge and strong reason-\ning ability. Recently, a trend has emerged that lever-\nages LLMs in combination with a range of vision\nor multimodal models. These approaches create a\nsystem capable of addressing various multimodal\ntasks without the need for additional training. MM-\nReact (Yang et al., 2023), HuggingGPT (Shen et al.,\n2023), Chameleon (Lu et al., 2023), Visual Chat-\nGPT (Wu et al., 2023) regard GPT (i.e., ChatGPT,\nGPT-4) as a controller to coordinate and collaborate\nwith other models (e.g., visual foundation models)\nto tackle complicated multimodal tasks. VisProg\n(Gupta and Kembhavi, 2022) and ViperGPT (Surís\net al., 2023) exploit GPT-3 (Brown et al., 2020) and\nCodeX (Chen et al., 2021) to generate a program\nto solve VL tasks in a one-round query answering.\nChatCaptioner (Zhu et al., 2023a) lets ChatGPT\nand BLIP-2 interact to accomplish image caption-\ning in a dialogue approach. All of them borrow\nthe strong reasoning ability from LLMs and boost\nperformance in a wide range of VL tasks. Different\nfrom ViperGPT, VisProg, and ChatCaptioner, Ide-\nalGPT solves VL tasks iteratively in a multi-round\nmanner and our proposed method can be conve-\nniently deployed in a diverse set of VL tasks such\nas VCR and SNLI-VE. More details are in Sec. 3.4.\n3 Method\nIn this section, we introduce the proposed Ideal-\nGPT. Our focus is on the tasks of open-domain\nVQA, where a question q is asked about an im-\nage I. There are three components in IdealGPT\nframework: a Questioner, an Answerer, and\na Reasoner. In each iteration, based on q and\n11291\n2nd Iteration\nIs the person on left and  \nperson on right dating?\nQuestioner\nMain Question:\nWho is the person on left?\nWho is the person on right?\nWhat is the physical contact between person on left and person on right\nSub-Question 1:\nSub-Question 2:\nSub-Question 3:\nAnswerer(VLMs)\nReasoner\nWhat is the body language of the man?\nWhat is the facial expression of the man?\nWhat is the facial expression of the nun?\nSub-Question 1:\nSub-Question 2:\nSub-Question 3:\nAnalysis: Watching is a vague action, so we cannot determine it is romantic or not. We need more information about  …….Answer: We are not sure.\nAnswer of 1st Iteration\nAnalysis: Because they are sad, ….Answer: No, they are not dating\nAnswer of 2nd Iteration\n1st Iteration\nNun\nMan\nWatching\nSub-Answer 1:\nSub-Answer 2:\nSub-Answer 3:\n1st Iteration\nHe is looking at the nun\nSad\nSad\nSub-Answer 1:\nSub-Answer 2:\nSub-Answer 3:\n2nd IterationGenerated\nSub-Questions \n All Existing\nSub-Answers \nGenerated Analysis \n✅\n?\nDone\nNeed More Iterations\nPath of 1st Iteration Path of 2nd Iteration\nCaption\nAll Existing Sub-Questions Sub-Answers \nFigure 2: The pipeline of proposed IdealGPT. We use an example in VCR validation set for illustration, which\nis finished in 2 iterations. In the initial iteration, the main question is decomposed into multiple sub-questions,\nwhich are answered by Answerer, and then the Reasoner summarizes generated sub-answers to decide whether\na confident answer can be deduced. Since Reasoner is not sure about the answer in the 1st iteration, it generates\nthe analysis, and all existing information is input into Questioner again for another new iteration. The iterative\nprocess ends when Reasoner is confident about an answer or the maximum number of iterations is reached.\nI, Questioner first raises a set of sub-questions\nSubQ = {sq1, ..., sqi} (Sec. 3.1), then Answerer\ngenerates the corresponding sub-answers SubA =\n{sa1, ..., sai} (Sec. 3.2), and Reasoner analyzes\nboth SubA and SubQ to decide if a confident an-\nswer a to the main question q can be derived (Sec.\n3.3). If a confident answer cannot be inferred in\nthe current iteration, Questioner is prompted to\nask additional supplementary sub-questions, and\nanother “Questioner-Answerer-Reasoner” iter-\nation is triggered. This loop keeps iterating until\nthe Reasoner finds a confident final answer or the\nnumber of iterations reaches a predefined maxi-\nmum. The overall pipeline is shown in Figure 2.\n3.1 Questioner\nIn previous works (Uehara et al., 2022; Wang et al.,\n2022b), sub-questions are generated by models\ntrained on specific sub-questions data (Selvaraju\net al., 2020). However, since the training data are\nspecifically annotated for the samples in Antol et al.\n(2015), these sub-question generators cannot scale\nand generalize to different domains and complex\nreasoning tasks. Annotating a sub-question dataset\ncovering all types of visual reasoning tasks and\nimage domains is also infeasible. Recently, LLMs\n(Ouyang et al., 2022; OpenAI, 2023; Anil et al.,\n2023) have demonstrated a strong ability to fol-\nlow instructions and reason with human knowl-\nedge. Additionally, some works (Surís et al., 2023;\nWu et al., 2023; Zhu et al., 2023a) have utilized\nLLMs to aid visual tasks, which demonstrates that\nLLMs have acquired diverse visual knowledge to\na certain degree as well. Inspired by these find-\nings, we prompt GPT as a Questioner to raise\nsub-questions. By default, ChatGPT (Ouyang et al.,\n2022) is used. While GPT-4 (OpenAI, 2023) is a\nstronger alternative, it is costlier.\nThe input in VQA tasks usually includes a main\nquestion q and an image I, and sometimes answer\ncandidates A = {a1, ..., an} if the task is format-\nted as a multiple-choice QA problem. The target of\nthe Questioner is first to recognize the evidence\nneeded to address q and then decompose it into\nsub-evidences. To acquire those sub-evidences,\nQuestioner would then generate a set of sub-\nquestions SubQ = {sq1, ..., sqi}. For achieving\nquality results, we also design a prompt Pq as an\ninstruction for GPT to understand the objective\nand the desired output. For each task, the prompt\nis slightly different to accommodate the task de-\nscriptions1. With solely the main question q and\nprompt Pq input into the Questioner, we empiri-\n1See the detailed prompts in Appendix D\n11292\ncally found that the generated sub-questions from\ninitial iterations tend to be too generic and uninfor-\nmative. This could be because LLMs don’t see the\nimages and as such are devoid of the visual input.\nTo facilitate the Questioner to understand the im-\nage and generate more informative questions, we\nprovide a short caption CI generated by a VLM as\nan additional input to the Questioner. Therefore,\nin the first iteration, the sub-question generation\ncan be formulated as follows:\nSubQ1 = ChatGPT(q, CI, Pq).\nAs mentioned before, there may not be sufficient\nevidence for the Reasoner to address the main\nquestion after only one iteration. This can be due\nto common issues like the sub-questions are not in-\nformative enough or conflict/noise existing among\nsub-answers. In this case, IdealGPT would prompt\nthe Reasoner to generate an explanation/analysis\nregarding why it may not have sufficient evidence\nto address the main question. Subsequently, we\nwould loop back to the Questioner to generate\nadditional supplementary sub-questions. In the t-\nth iteration ( t > 1), Questioner accepts all pre-\nvious sub-questions SubQ1:t−1 and sub-answers\nSubA1:t−1, and the previous analysis Et−1 from\nReasoner (c.f., Sec. 3.3) as additional input:\nSubQt =ChatGPT(q, CI, Pq,\nSubQ1:t−1, SubA1:t−1, Et−1),\nwhere SubQ1:t−1 = {SubQ1 ∪ ... ∪ SubQt−1}\nand SubA1:t−1 = {SubA1 ∪ ... ∪ SubAt−1}. Pre-\nvious sub-questions and sub-answers can inform\nQuestioner what has been asked and solved, and\nthe analysis can guide Questioner to generate\nmore specific sub-questions, such as sub-questions\nto collect more informative evidence about a spe-\ncific visual object and so on.\n3.2 Answerer\nGiven the generated sub-questions SubQ, the goal\nof Answerer is to answer them correspondingly\nto provide evidence for answering the main ques-\ntion. In IdealGPT, the Answerer is a pre-trained\nVLM without finetuning on any dataset to keep\nintact its generalization ability. Each sub-question\nis answered separately:\nsai = VLM(sqi, I),\nwhere sqi ∈ SubQ and sai ∈ SubA. It is noted\nthat theoretically, the Answerer can not only be\nend-to-end VLMs but also VL systems such as\nSurís et al. (2023); Gupta and Kembhavi (2022);\nShen et al. (2023).\n3.3 Reasoner\nGPT-like LLMs (Ouyang et al., 2022; OpenAI,\n2023; Anil et al., 2023) have shown impressive\nsummarization and reasoning ability with common-\nsense knowledge. Therefore, like Questioner, we\nalso choose ChatGPT as the Reasoner but prompt\nit differently. Specifically, the input to Reasoner\ncontains main question q, caption CI, all exist-\ning sub-questions SubQ1:t and corresponding sub-\nanswers SubA1:t. And the Reasoner is prompted\nto generate both the analysis and the final answer\nwith its prompt PR1:\nEt, a= ChatGPT(SubQ1:t, SubA1:t, q, CI, PR).\nIf the Reasoner is not confident about the final\nanswer, it is instructed to faithfully indicate that by\ngenerating a specific response such as “We are not\nsure”. If this particular response is detected, we\nstart another iteration by asking the Questioner\nto add supplementary sub-questions. The above\nprocedure forms a loop among the three agents,\nwhich will stop if theReasoner can find a confident\nanswer or the number of iterations reaches a pre-\ndefined bound (a hyperparameter).\n3.4 Comparison with Other Methods\nv.s.ViperGPT/VisProg. VisProg (Gupta and Kem-\nbhavi, 2022) and ViperGPT (Surís et al., 2023) uti-\nlize LLMs to decompose VL tasks into steps of ex-\nplicit conditional logic in coding based on low-level\nvisual-spatial detection. IdealGPT shares a simi-\nlar idea of decomposition or divide-and-conquer.\nHowever, IdealGPT can iteratively go through the\ndivide-and-conquer process until collecting suffi-\ncient evidence to generate a confident answer. This\nmulti-pass process involves the continuous refine-\nment of the set of sub-questions and even a re-\ncorrectifying mechanism to the final answer pre-\ndiction. Conversely, ViperGPT/VisProg performs\nprograms in one pass regardless of whether the\npredicted answer is confident or not. This differ-\nence also applies to Yang et al. (2023); Shen et al.\n(2023); Lu et al. (2023). Moreover, ViperGPT is\nlimited by the set of available APIs, VisProg is lim-\nited by the set of available commands, and Neuro-\nsymbolic VQA is limited by the set of primitive\noperations in the Domain Specific Language.\n11293\nVisual Entailment (VE)\nQuestioner\nReasoner\nAnswerer(VLMs)\nWhat is the crane doing?\nWhat is the position of the construction workers?\nWhat is the weather and time of day in the image?\nSub-Question 1:\nSub-Question 2:\nSub-Question 3:\nLifting a ladder.\nOn a ladder.\nIt is sunny morning.\nSub-Answer 1:\nSub-Answer 2:\nSub-Answer 3:\nThe crane is lifting a ladder and the construction workers are on the ladder. Additionally, the time of day is morning and the weather is sunny, which are both typical conditions for construction work. So, the workers are likely working on the building.\nAnalysis:\nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities …Your goal is to effectively predict whether the image semantically entails the hypothesis …Imperfect Caption: A crane is on the side of a building.Hypothesis: The construction workers are working.\nPrompt:\nImage \nCaptioning\nEntailment\n✅\nPredicted Answer:\nFigure 3: The illustration of how proposed IdealGPT\nworks in SNLI-VE. IdealGPT exhibits its perceptional\nability in sub-question 1 and 2. Plus, it shows a strong\ncommonsense reasoning ability in sub-question 3.\nv.s.ChatCaptioner. ChatCaptioner (Zhu et al.,\n2023a) lets ChatGPT and BLIP-2 interact with each\nother to caption images in a dialogue system. Ide-\nalGPT shares the idea of utilizing ChatGPT to gen-\nerate questions and VLMs to answer them. But\nIdealGPT focuses on the VL reasoning tasks and\nincorporates the iterative design for generalized\nzero-shot vision language reasoning.\n4 Experiments\nIn this section, we evaluate our method quantita-\ntively on two main tasks - Visual Commonsense\nReasoning (VCR) and Visual Entailment (SNLI-\nVE). We introduce datasets and models first. Then,\nwe show superior zero-shot performance on VCR\nand SNLI-VE compared to other existing mod-\nels. Afterward, ablation studies are conducted to\nconfirm the effectiveness of our proposed method,\nwith error analysis for a deep dive into our method.\nIn addition to VCR and SNLI-VE, we also show-\ncase the experimental results of Augmented Out-\nside Knowledge Visual Question Answering (A-\nOKVQA) (Schwenk et al., 2022). Since it is more\nof a knowledge-based task requiring less complex\nreasoning ability, we put it in Appendix C.\n4.1 Experimental Setup\nDatasets. In this paper, we employ two VL\ndatasets, VCR (Zellers et al., 2019) and SNLI-VE\n(Xie et al., 2019) as they represent the two typi-\ncal VL reasoning tasks, visual question answering\nand visual entailment. Different from the tradi-\ntional VQA (Antol et al., 2015) task, VCR needs\ncommonsense knowledge to understand the visual\nscenes and requires multiple-steps reasoning to an-\nswer the question. Also, in terms of the task for-\nmat, it is a multiple-choice QA problem. SNLI-VE\noriginates from Stanford Natural Language Infer-\nence (SNLI) (Bowman et al., 2015), a text entail-\nment (TE) task based on Flicker30k (Young et al.,\n2014). It extends TE into the visual domain and\nasks the model whether the image is semantically\nentailed/neutral/contradicted to the text hypothesis,\nthus it can be treated as a three-category classifi-\ncation task. In VCR and SNLI-VE, we randomly\nselect 5000 samples from the val/dev split of the\ndataset and evaluate our method in the zero-shot\nscenario with the accuracy for evaluation.\nModels. We choose ChatGPT to act as the\nReasoner and Questioner and access it via “gpt-\n3.5-turbo” API. It should be noted that we set the\ntemperature as 0 to reduce the randomness so that\nwe can have a stable result to see how our pro-\nposed method performs in different tasks. As for\nthe Answerer, three pre-trained VLMs (BLIP-2,\nMiniGPT4, and LLaV A) are selected to serve for\na comprehensive comparison. It’s noted that all\nVLMs we use are pre-trained-only without being\nfinetuned on any dataset to guarantee zero-shot\ngeneralizability. We design three simple prompts\nfor these models respectively (more details can\nbe found in the appendix D) to help them better\nanswer sub-questions. Further, in practice, these\nVLMs are also used to produce image captions so\nthat the ChatGPT can reserve a general understand-\ning of the unseen image initially.\n4.2 Visual Commonsense Reasoning\nVCR covers various commonsense in diverse vi-\nsual scenes, including temporal, spatial, causal,\nand mental commonsense, etc. It’s formatted as\na multiple-choice answering problem, and to find\nthe correct answer, the model is expected to reason\namong four answer candidates to find the correct\nanswer. In VCR, it often happens that there are\nmultiple persons in one image, and if the question\nmentions one of them, the bounding box will be\n11294\nAcc.(%)\nRandom Guess 25Sup.\nR2C (Zellers et al., 2019) 63.8\nVisualBERT (Li et al., 2019) 70.8\nMerlotReserve (Zellers et al., 2022) 84.0ZS.\nBLIP-2 (Li et al., 2023) -\nMiniGPT4 (Zhu et al., 2023b) 40.6\nLLaV A (Liu et al., 2023) 28.3\nIdealGPT(ours) 50.7\nTable 1: Accuracy of VCR Q→A task (ZS: Zero-Shot,\nSup: Supervised)\nused to distinguish it from others. However, most\nexisting VLMs cannot understand bounding boxes\nin the text input, making them hard to perform\nVCR directly. To alleviate that, we conduct a pre-\nprocessing to describe the mentioned person’s spa-\ntial coordinate in words easy-to-understand. Please\nsee details in Appendix E.\nAlthough a lot of models can be finetuned on\nthis task (Zellers et al., 2019; Li et al., 2019), there\nis hardly any model that tackles it in the zero-shot\nfashion. We empirically tried BLIP-2, MiniGPT4,\nand LLaV A and found that only the GPT4-like\nmodels can functionally perform zero-shot VCR,\nwhile other models such as BLIP-2 fail to under-\nstand the long context and the instruction of finding\nthe correct answer. We present the experimental\nresult in Tab. 1, where all zero-shot results are\nobtained from the randomly sampled 5000 data in\nthe validation set. As we can see, IdealGPT can\noutperform the best GPT4-like model, MiniGPT4\nby over 10%. It’s noted that in IdealGPT reported\nhere, BLIP2-FlanT5-XL is used as the Answerer.\nPlease refer to Sec. 4.4 for ablations on the choice\nof Answerer.\nIn Fig.2, we showcase an example of how Ideal-\nGPT solves an example in VCR successfully. As\nwe can see, in the first pass, the generated sub-\nquestions and predicted sub-answers are not in-\nformative enough to support a solid conclusion be-\ncause their identities and interaction of “watching\"\"\ncannot quite indicate whether they are dating or not.\nFurther, in the second pass, after inputting the anal-\nysis from the Reasoner and existing sub-questions\nand sub-answers, the Questioner is prompted to\nask additional supplementary sub-questions to col-\nlect more evidence about their expressions and\nbody language. As a result, the updated sub-\nAcc.(%)\nRandom Guess 33.3Sup.\nEVE-Image (Xie et al., 2019) 71.6\nUNITER (Chen et al., 2020) 79.4\nOFA (Wang et al., 2022a) 91.0ZS.\nMiniGPT4 (Zhu et al., 2023b) 35.1\nLLaV A (Liu et al., 2023) 40.3\nIdealGPT(ours) 55.3\nTable 2: Accuracy of SNLI-VE (ZS: Zero-Shot, Sup:\nSupervised)\nanswers of sad expressions and distant body lan-\nguage allow the Reasoner to ensure a confident\nfinal answer.\n4.3 Visual Entailment\nVisual Entailment requires the model to predict\nwhether the image semantically entails the text. In\neach sample, there is a pair of an image and a text\nhypothesis along with three answer candidates (i.e.,\nentailment, neutral, and contradiction). The model\nneeds to select one of them to represent the rela-\ntionship between the image and the text hypothesis.\nIt is challenging to solve the three-category clas-\nsification task in a zero-shot manner. As shown\nin Fig.3, to begin with, we make some rules and\nintroduce the goal of this task to ChatGPT1 to en-\nsure it can understand our instructions and respond\nreasonably. We utilize VLMs to generate the image\ncaption and inject it into the prompt. Thereafter,\nQuestioner decomposes the original hypothesis\ninto several sub-questions. After answering all sub-\nquestions by VLMs, Reasoner will summarize the\nimage caption, hypothesis, all sub-questions, and\ncorresponding sub-answers together to provide a\ncomprehensive analysis. In Fig.3, we should notice\nthat not only does IdealGPT exhibit the percep-\ntional ability in sub-question 1 and 2, but also it\nshows the strong commonsense reasoning ability\nin sub-question 3.\nSupervised methods for SNLI-VE have been\nwell-studied (Xie et al., 2019; Chen et al., 2020;\nWang et al., 2022a), while zero-shot approaches are\nless explored. We tried LLaV A and MiniGPT4 to\ndo zero-shot SNLI-VE. In Tab. 2, we can observe\nthat compared to MiniGPT4 and LLaV A, IdealGPT\nconsistently surpasses them by a large margin, 20%,\nand 15% respectively. This result shows that not\nonly can our method understand long instructions\n11295\nModel Max. #Iterations Acc.(%)\nIdealGPT\n1 49.2\n2 53.2\n4 55.8\n6 57.3\n7 57.4\nTable 3: Ablation of iterative decomposing. Max. #Iter-\nations=1 means deterministic answering in one round\nwithout iterative decomposing.\nbut also it is able to handle different task formats\n(SNLI-VE and VCR have distinctively different\ntask formats. The former is image-hypothesis pair\nbut the latter is the question-answering format).\nFrom Tab. 2, we can also notice that the perfor-\nmance of MiniGPT4 is the near random-guessing\nlevel. More details and discussion about SNLI-VE\ncan be found in Appendix B.\n4.4 Ablation Studies\nIn this section, we ablate both the design choice\nand component choice. We first demonstrate the\nnecessity of iterative design in IdealGPT. Then we\nablate different VLMs for generating captions and\nperforming as Answerer. In all ablations, we use a\nrandomly sampled 500 data set from VCR, which\nin our findings is enough to distinguish different\nmodel choices.\nIterative Decomposing. A key design in Ideal-\nGPT is that if the model ( Reasoner) is not sure\nabout the final answer, it will trigger a new pass of\nQA to ask additional sub-questions and therefore\nprovide more visual facts to theReasoner. The iter-\native decomposing will continue until the Reasoner\nis confident to make a decision or the number of\npasses reaches a pre-defined bound, which is a hy-\nperparameter. We evaluate IdealGPT with iterative\ndecomposing and without iterative decomposing\n(i.e., Max.#Iterations=1) in Tab. 3. We can see\nthat iterative decomposing design can boost the ac-\ncuracy by as high as around 8%. It’s noted that\nmore passes also mean more inference time, and\nwe find setting the maximum number of iterations\nto 4 achieves a good trade-off between efficiency\nand effectiveness, which is used as default in all\nother experiments. Under the above setting, the\naverage number of passes across sampled data is\n1.8.\nAnswerer Choice. As we mentioned before,\nAnswerer can be any VLM capable of answering\nModel Answerer Acc.(%)\nIdealGPT\nBLIP-2 55.8\nMiniGPT4 52.8\nLLaV A 53.2\nTable 4: Ablation of choice of Answerer.\nModel Caption from Acc.(%)\nIdealGPT\nBLIP-2 55.8\nMiniGPT4 48.4\nLLaV A 51.2\nTable 5: Ablation of generated captions.\nvisual questions. We mainly ablate three VLMs:\nBLIP-2, MiniGPT4, and LLaV A. The result is\nshown in Tab. 4. Although MiniGPT4 and LLaV A\ncan follow instructions and understand the longer\ncontexts, they are worse than BLIP-2 when an-\nswering questions related to detailed visual facts.\nThis also echoes the limitation mentioned in their\npapers about hallucinations and the lack of spa-\ntial/geometric understanding. Note that the BLIP-\n2 we use is BLIP2-FlanT5-XL, which in our ex-\nperiments, gives a similar performance as BLIP2-\nFlanT5-XXL.\nImage Captions.To efficiently search for the best\ncaption in our method, we fix the VLM as BLIP2-\nFlanT5-XL and go through pre-trained VLMs with-\nout fine-tuning on any caption dataset. From the ex-\nperimental results shown in Tab. 5, we see BLIP-2\nexhibits the best performance. It is interesting to ob-\nserve that both MiniGPT4 and LLaV A have shown\nimpressive captioning ability in their papers/demos,\nbut they fail in our framework when compared\nwith BLIP-2. We further go through many cap-\ntions generated from MiniGPT4/LLaV A and BLIP-\n2 and find that MiniGPT4/LLaV A tends to generate\nlonger, more informative but less accurate (or more\nhallucination) captions. In contrast, BLIP-2 tends\nto generate short, less informative, but also less\nmistaken captions. So the hallucinations/mistakes\nin MiniGPT4/LLaV A-generated captions tend to\nmislead the Questioner and Reasoner to wrong\nanswers.\n4.5 Error Analysis\nSince our method consists of multiple components,\nit would be interesting to see where the error comes\nfrom. We went through 50 failure samples of Ideal-\n11296\nSource Type Ratio.(%)\nQuestionerNot Relevant Sub-questions 16\nAnswerer Wrong Sub-answers 52\nReasoner Hallucination 24\nMisunderstanding 8\nTable 6: Error Analysis of different components\nGPT on VCR validation set and conducted an error\nanalysis about different types of errors from differ-\nent components, thanks to the good transparency\nand interpretability of IdealGPT. The quantitative\nresult is shown in Tab. 6. Below are more detailed\nanalyses.\nQuestioner: We found that the Questioner\nsometimes generates sub-questions that are not\nquite relevant to the four answer choices, thus not\nhelpful in distinguishing the correct answer. Be-\nsides the capability of LLM itself, we found it’s\nalso heavily influenced by the caption we provided.\nIf the caption is more precise with less hallucina-\ntion, then the Questioner would be more precise\ntoo.\nAnswerer: Half of the errors come from the\nAnswerer providing wrong sub-answers. As illus-\ntrated in Tab. 4, we can find that the choice of\nAnswerer is quite important. A good Answerer\nshould understand various types of questions and\ngenerate answers with less hallucination.\nReasoner: There are two types of errors from\nReasoner: a). Hallucination: Sometimes Reasoner\npredicts the answer only with unclear visual clues,\nthus leading to wrong prediction. In those cases,\none more round of the decomposition loop might\nhelp to find more clear visual evidence. This also\naccords with our observation that more decomposi-\ntion iterations help. b). Misunderstanding: Some-\ntimes, the LLM misunderstands the answer candi-\ndates and gives wrong predictions, which is mainly\ndue to the LLM’s capacity.\n5 Conclusion\nIn this work, we identify critical problems with ex-\nisting VQA methods, especially the lack of address-\ning zero-shot reasoning tasks and the false assump-\ntion potentially forcing models to answer questions\nwithout sufficient information. To address these\nissues, we propose IdealGPT to utilize LLMs to\nconstruct a multiple-passes framework among a\nQuestioner, a Answerer, and a Reasoner. This\nframework ensures better interpretability of VLMs’\nreasoning and robustness to re-correctifying pre-\ndictions such as hallucination. Additionally, the\ngeneralizability of our framework can be illustrated\nby the modularity and our superior zero-shot per-\nformance. Our extensive experiments prove the\neffectiveness of each component in IdealGPT and\nverify that it can outperform the best existing GPT-\n4-like models by an absolute 10% in VCR and 15%\nin SNLI-VE.\nAcknowledgements\nThis work is supported by DARPA MCS program\nunder Cooperative Agreement N66001-19-2-4032.\nLimitations\nAlthough we have witnessed the strong perfor-\nmance of the proposed method, we still have some\nlimitations. Firstly, answering key sub-questions\ncorrectly plays a significant role in the success of\nour system. Therefore, our final results are largely\nbottlenecked by the performance of pre-trained\nVLMs. Additionally, we just employ the general\nimage caption to give ChatGPT an idea of what the\nimage looks like. However, dense captioning might\nbe more informative and better help ChatGPT deal\nwith the unseen image. Besides, the prompts are\nmanually designed by us and it is difficult to find\nthe optimal prompt in a specific situation. It will be\nbetter if the prompt can be generated automatically.\nLast but not least, compared with end-to-end solu-\ntions, ours naturally takes more time and has higher\nlatency because of multiple feedforward passes of\nQuestioner/Answerer/Reasoner.\nEthics Statement\nChatGPT is pre-trained on the colossal corpus\nwhich is likely to contain potential racial and gen-\nder bias. Therefore, if someone finds our work\ninteresting and would like to use it in a specific en-\nvironment, we strongly suggest the user check the\npotential bias before usage. In addition, it is hard to\ncontrol the generation of LLMs like ChatGPT. We\nshould be aware of the potential problems caused\nby hallucinations.\n11297\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716–23736.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. Vqa: Visual question answering.\nIn Proceedings of the IEEE international conference\non computer vision, pages 2425–2433.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv\npreprint arXiv:1508.05326.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part XXX, pages 104–\n120. Springer.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint\narXiv:2305.06500.\nTanmay Gupta and Aniruddha Kembhavi. 2022. Vi-\nsual programming: Compositional visual reasoning\nwithout training. arXiv preprint arXiv:2211.11559.\nAmita Kamath, Christopher Clark, Tanmay Gupta, Eric\nKolve, Derek Hoiem, and Aniruddha Kembhavi.\n2022. Webly supervised concept expansion for gen-\neral purpose vision models. In Computer Vision–\nECCV 2022: 17th European Conference, Tel Aviv, Is-\nrael, October 23–27, 2022, Proceedings, Part XXXVI,\npages 662–681. Springer.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. arXiv preprint arXiv:2301.12597.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In International Conference on Ma-\nchine Learning, pages 12888–12900. PMLR.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\narXiv preprint arXiv:1908.03557.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nOpenAI. 2023. Gpt-4 technical report. arXiv.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answering\nusing world knowledge. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23–27, 2022, Proceedings, Part VIII, pages\n146–162. Springer.\nRamprasaath R Selvaraju, Purva Tendulkar, Devi Parikh,\nEric Horvitz, Marco Tulio Ribeiro, Besmira Nushi,\nand Ece Kamar. 2020. Squinting at vqa models: In-\ntrospecting vqa models with sub-questions. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10003–10011.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nDídac Surís, Sachit Menon, and Carl V ondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. arXiv preprint arXiv:1908.07490.\n11298\nKohei Uehara, Nan Duan, and Tatsuya Harada. 2022.\nLearning to ask informative sub-questions for visual\nquestion answering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 4681–4690.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022a. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, pages\n23318–23340. PMLR.\nRuonan Wang, Yuxi Qian, Fangxiang Feng, Xiaojie\nWang, and Huixing Jiang. 2022b. Co-vqa: Answer-\ning by interactive sub question sequence. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, pages 2396–2408.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022c. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nZhecan Wang, Haoxuan You, Yicheng He, Wenhao\nLi, Kai-Wei Chang, and Shih-Fu Chang. 2022d.\nUnderstanding me? multimodal evaluation for\nfine-grained visual commonsense. arXiv preprint\narXiv:2211.05895.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xi-\naodong Wang, Zecheng Tang, and Nan Duan.\n2023. Visual chatgpt: Talking, drawing and edit-\ning with visual foundation models. arXiv preprint\narXiv:2303.04671.\nNing Xie, Farley Lai, Derek Doran, and Asim Ka-\ndav. 2019. Visual entailment: A novel task for\nfine-grained image understanding. arXiv preprint\narXiv:1901.06706.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\nLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-\nreact: Prompting chatgpt for multimodal reasoning\nand action. arXiv preprint arXiv:2303.11381.\nOri Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel\nDeutch, and Jonathan Berant. 2023. Answering\nquestions by meta-reasoning over multiple chains\nof thought. arXiv preprint arXiv:2304.13007.\nHaoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei\nChang, Jason Baldridge, and Jiahui Yu. 2023. Cobit:\nA contrastive bi-directional image-text generation\nmodel. arXiv preprint arXiv:2303.13455.\nHaoxuan You, Luowei Zhou, Bin Xiao, Noel Codella,\nYu Cheng, Ruochen Xu, Shih-Fu Chang, and\nLu Yuan. 2022. Learning visual representation\nfrom modality-shared contrastive language-image\npre-training. In Computer Vision–ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23–\n27, 2022, Proceedings, Part XXVII, pages 69–87.\nSpringer.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Vi-\nsual commonsense reasoning. In Proceedings of the\nIEEE/CVF conference on computer vision and pat-\ntern recognition, pages 6720–6731.\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu,\nYanpeng Zhao, Mohammadreza Salehi, Aditya Kusu-\npati, Jack Hessel, Ali Farhadi, and Yejin Choi. 2022.\nMerlot reserve: Neural script knowledge through\nvision and language and sound. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 16375–16387.\nDeyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian\nShen, Wenxuan Zhang, and Mohamed Elhoseiny.\n2023a. Chatgpt asks, blip-2 answers: Automatic\nquestioning towards enriched visual descriptions.\narXiv preprint arXiv:2303.06594.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023b. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\n11299\nA Data Statistics\nVL Tasks Number of Samples\nSNLI-VE\nC 1664\nN 1672\nE 1664\nAll 5000\nVCR - 5000\nTable 7: Statistics of SNLI-VE and VCR (C: Contradic-\ntion, N: Neural, E: Entailment, VL: Vision-Language)\nWe randomly select 5000 VCR and SNLI-VE\nsamples from respective val and dev split. Zero-\nshot learning is conducted on these samples.\nB Details of Visual Entailment Results\nC N E All\nRandom Guess - - - 33.3\nSup.\nEVE-Image (Xie et al., 2019) 71.0 70.6 73.1 71.6\nUNITER (Chen et al., 2020) - - - 79.4\nOFA (Wang et al., 2022a) - - - 91.0\nZS.\nMiniGPT4 (Zhu et al., 2023b) 3.2 3.2 99.0 35.1\nLLaV A (Liu et al., 2023) 12.0 31.3 77.6 40.3\nIdealGPT(ours) 83.4 25.9 56.7 55.3\nTable 8: Accuracy of SNLI-VE (ZS: Zero-Shot, Sup:\nSupervised, C: Contradiction, N: Neutral, E: Entail-\nment)\nFrom the results shown in Tab.8, we can\nobserve that our proposed method outperforms\nMiniGPT4 and LLaV A by a large margin. More-\nover, MiniGPT4 exhibits near-chance level perfor-\nmance. When it is faced with different samples, it\nalways replies entailment, which indicates that it\ndoesn’t obtain a strong reasoning ability to process\nand understand the visual entailment task. How-\never, IdealGPT can achieve strong zero-shot results\nand even surpass the supervised EVE-Image model\nin contradiction category.\nC Augmented Outside Knowledge Visual\nQuestion Answering\nA-OKVQA is a challenging benchmark for\nknowledge-required visual question answering,\nwhich demands world knowledge that goes beyond\nthe image. It provides Multiple-Choice (MC) as\nwell as Direct Answer (DA) evaluation settings.\nAcc.(%)\nRandom Guess 25.0Sup.\nLXMERT (Tan and Bansal, 2019) 51.4\nGPV-2 (Kamath et al., 2022) 60.3\nInstructBLIP (Dai et al., 2023) 81.0ZS.\nMiniGPT4 (Zhu et al., 2023b) 49.4\nLLaV A (Liu et al., 2023) 30.0\nIdealGPT(ours) 62.6\nTable 9: Accuracy of A-OKVQA in Multiple-Choice\nsetting(ZS: Zero-Shot, Sup: Supervised)\nSince it is difficult to evaluate the generated an-\nswer in the open-vocabulary setting, we choose\nthe MC evaluation setting. We design the prompt\nfor A-OKVQA2 and directly input the question,\nfour answer choices, and the general image cap-\ntion into the Questioner to put forward several\nsub-questions. Afterward, the VLMs can reply to\nthe Questioner and Reasoner is able to collect\nand analyze the sub-questions and sub-answers to\noutput a prediction when it is sure about its answer.\nIn Tab. 9, we investigate zero-shot A-OKVQA by\nusing MiniGPT4 and LLaV A. IdealGPT can sur-\npass MiniGPT4 and LLaV A by 13.2 % and 32.6 %\nrespectively, which confirms the stronger reasoning\nability of our proposed method. Moreover, we out-\nperform some early proposed supervised methods\nlike LXMERT (Tan and Bansal, 2019) and GPV-2\n(Kamath et al., 2022). Since we implement our\nmethod without any further training, we still can’t\nexceed InstructBLIP (Dai et al., 2023), which is\none of the current best-performing supervised mod-\nels on A-OKVQA.\nD Details of Prompts Used\nThe prompts of IdealGPT used in the VCR task are\nshown in Fig. 4. As for SNLI-VE, the prompts are\nshown in Fig. 5. It’s noted that the [placeholder]\nmeans we will replace it with the corresponding\ntext of the instance, such as a main question, cap-\ntion, and four choices.\nE VCR Pre-Processing\nWe exploit two different approaches to pre-process\nVCR and select the better one for our experiments\nin the main text. The first pre-processing is to di-\nvide the image region from left to right into three\n2Prompts for A-OKVQA are very similar to VCR in Ap-\npendix D\n11300\nAcc.(%)\nTB.\nMiniGPT4 (Zhu et al., 2023b) 42.4\nLLaV A (Liu et al., 2023) 31.0DB.\nMiniGPT4 (Zhu et al., 2023b) 30.8\nLLaV A (Liu et al., 2023) 28.6\nTable 10: Comparison of two different pre-processing\nways for VCR (TB: Three Bins, DB: Drawn-on Boxes)\nbins and check if the mentioned object’s center\npoint belongs to which bin. If it’s in the most left\nbin, it’s renamed as “person on the left”. Similarly,\n“person in the middle” is in the middle bin, and\n“person on the right” is in the right bin. Since most\nQAs in VCR mentioned less than three persons,\nit can cover most cases. The second approach is\nto follow past work (Zellers et al., 2022) in ’draw-\ning on’ the annotated detection tags to the image.\nWe select seven different colors ( i.e., red, green,\nblue, orange, purple, cyan, and yellow) to represent\nthe different persons mentioned in the question or\nanswer. Assume there are two people in one sam-\nple, when doing the inference, ’person1’ will be\nreplaced by ’person in the red bounding box’, and\n’person2’ will be replaced by ’person in the green\nbounding box’. Moreover, the model can see the\nred and green bounding boxes drawn in the im-\nage. As we mentioned above, most cases in VCR\nmentioned less than three people. Therefore, our\nimplementation can cover the majority of the cases.\nThe comparison of these two methods can be found\nin Tab. 10. We randomly select 500 samples from\nVCR val split to conduct zero-shot learning by us-\ning LLaV A and MiniGPT4 to see which method is\nbetter. We can observe that the former approach is\nbetter, so we utilize this setting in the main text.\n11301\nVCR Questioner Prompt:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSystem Prompt of 1st Iteration:\nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\nYour goal is:\nTo effectively analyze the image and select the correct answer for the question, you should break down the main question into several sub-questions that address the \nkey aspects of the image.\nHere are the rules you should follow when listing the sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Sub-question 1: ...?; Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-question are necessary to distinguish the correct answer.\nExample:\nMain question: What is happening in the image?\nSub-question 1: What objects or subjects are present in the image?\nSub-question 2: What actions or events is the person doing?\nSub-question 3: What are the emotions or expressions of the woman?\nSub-question 4: What is the brand of this car?\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSystem Prompt of Following Iterations:\nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions decomposed from the main question, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not \nentirely precise.\n4. An analysis of whether the given sub-questions and sub-answers can help to solve the original main question.  \nThe current sub-questions and sub-answers are not sufficient to solve the main question. Your goal is:\nBased on existing sub-questions and analysis, you should pose additional questions, that can gather more information and are necessary to solve the main question.\nHere are the rules you should follow when listing additional sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Additional Sub-question 1: ...?; Additional Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-question are necessary to distinguish the correct answer.\nFormat Example:\nAdditional Sub-question 1: xxxx\nAdditional Sub-question 2: xxxx \nAdditional Sub-question 3: xxxx\nAdditional Sub-question 4: xxxx\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nInput Prompt of 1st Iteration:\nImperfect Caption: [placeholder]\nMain Question: [placeholder] \nFour Choices:  [placeholder]\nPlease list the sub-questions following the requirement I mentioned before.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nInput Prompt of Following Iterations:\nImperfect Caption: [placeholder]\nMain Question: [placeholder] \nFour Choices:  [placeholder]\nSub-questions and answers: [placeholder]\nAnalysis: [placeholder]\nPlease list the sub-questions following the requirement I mentioned before.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nVCR Answerer Prompt:\nQuestion: [placeholder] Answer:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nVCR Reasoner Prompt:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------System Prompt of All but Last Iteration: \nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions decomposed from main question, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not \nentirely precise.\nYour goal is:\nBased on sub-questions and corresponding answers, you should find the more likely answer from the four answer candidates. \nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. If you have found the more likely answer, conclude the correct answer id in the format of \"More Likely Answer: 1/2/3/4\". Otherwise, conclude with \"More Likely \nAnswer: We are not sure which option is correct\".\nResponse Format:\nAnalysis: xxxxxx.\nMore Likely Answer: 1/2/3/4.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSystem Prompt of Last Iteration: \nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions decomposed from main question, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not \nentirely precise.\nYour goal is:\nBased on sub-questions and corresponding answers, you must find the more likely answer from the four answer candidates. \nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. Tell me the more likely answer's id in the format of \"More Likely Answer: 1/2/3/4\". Even if you are not confident, you must give a prediction with educated guessing.\nResponse Format:\nAnalysis: xxxxxx.\nMore Likely Answer: 1/2/3/4.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nInput Prompt:\nImperfect Caption: [placeholder]\nMain Question: [placeholder] \nFour Choices:  [placeholder]\nExisting Sub-questions and answers: [placeholder]\nPlease follow the above-mentioned instruction to list the Analysis and More Likely Answer.\nFigure 4: The prompts of IdealGPT in VCR task.\n11302\nSNLI-VE Questioner Prompt:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSystem Prompt of 1st Iteration:\nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\nYour goal is:\nTo effectively predict whether the image semantically entails the textual hypothesis and select the answer from entailment, neutral, and contradiction, you should \ncome up with several sub-questions that address the key aspects of the image.\nHere are the rules you should follow when listing the sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Sub-question 1: ...?; Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-questions are necessary to distinguish the correct answer.\nExample:\nHypothesis: A group of women are walking along the railroad tracks.\nSub-question 1: What objects or subjects are present in the image?\nSub-question 2: What actions or events are the people doing?\nSub-question 3: What is the location where the people are walking?\nSub-question 4: What is the gender of this group of people?\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSystem Prompt of Following Iterations:\nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions proposed for predicting whether the image semantically entails the textual hypothesis, and the corresponding answers are provided by a \nvisual AI model. It's noted that the answers are not entirely precise.\n4. An analysis of whether the given sub-questions and sub-answers can help to predict whether the image semantically entails the textual hypothesis.\nThe current sub-questions and sub-answers are not sufficient to predict whether the image semantically entails the textual hypothesis. Your goal is:\nBased on existing sub-questions and analysis, you should pose additional questions, that can gather more information and are necessary to predict whether the \nimage semantically entails the textual hypothesis.\nHere are the rules you should follow when listing additional sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Additional Sub-question 1: ...?; Additional Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-questions are necessary to distinguish the correct answer.\nFormat Example:\nAdditional Sub-question 1: xxxx\nAdditional Sub-question 2: xxxx \nAdditional Sub-question 3: xxxx\nAdditional Sub-question 4: xxxx\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nInput Prompt of 1st Iteration:\nImperfect Caption: [placeholder]\nHypothesis: [placeholder] \nThree Choices:  [placeholder]\nPlease list the sub-questions following the requirement I mentioned before.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nInput Prompt of Following Iterations:\nImperfect Caption: [placeholder]\nHypothesis: [placeholder] \nThree Choices:  [placeholder]\nSub-questions and answers: [placeholder]\nAnalysis: [placeholder]\nPlease list the sub-questions following the requirement I mentioned before.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSNLI-VE Answerer Prompt:\nQuestion: [placeholder] Answer:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSNLI-VE Reasoner Prompt:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------System Prompt of All but Last Iteration: \nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions proposed for predicting whether the image semantically entails the textual hypothesis, and the corresponding answers are provided by a \nvisual AI model. It's noted that the answers are not entirely precise.\nYour goal is:\nBased on sub-questions and corresponding answers, you should find the more likely answer from the three answer candidates. \nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. If you have found the more likely answer, conclude the correct answer in the format of \"More Likely Answer: entailment/neutral/contradiction\". Otherwise, conclude \nwith \"More Likely Answer: We are not sure which option is correct\".\nResponse Format:\nAnalysis: xxxxxx.\nMore Likely Answer: entailment/neutral/contradiction.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSystem Prompt of Last Iteration: \nYou are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions proposed for predicting whether the image semantically entails the textual hypothesis, and the corresponding answers are provided by a \nvisual AI model. It's noted that the answers are not entirely precise.\nYour goal is:\nBased on sub-questions and corresponding answers, you must find the more likely answer from the three answer candidates. \nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. Tell me the more likely answer in the format of \"More Likely Answer: entailment/neutral/contradiction\". Even if you are not confident, you must give a prediction with \neducated guessing.\nResponse Format:\nAnalysis: xxxxxx.\nMore Likely Answer: entailment/neutral/contradiction.\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nInput Prompt:\nImperfect Caption: [placeholder]\nHypothesis: [placeholder] \nThree Choices:  [placeholder]\nExisting Sub-questions and answers: [placeholder]\nPlease follow the above-mentioned instruction to list the Analysis and More Likely Answer.\nFigure 5: The prompts of IdealGPT in SNLI-VE task.\n11303",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7911893129348755
    },
    {
      "name": "Divide and conquer algorithms",
      "score": 0.6948155164718628
    },
    {
      "name": "Pipeline (software)",
      "score": 0.6791699528694153
    },
    {
      "name": "Field (mathematics)",
      "score": 0.6003910899162292
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5269061923027039
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5266989469528198
    },
    {
      "name": "Code (set theory)",
      "score": 0.5196067690849304
    },
    {
      "name": "Language model",
      "score": 0.5090203285217285
    },
    {
      "name": "Visual reasoning",
      "score": 0.4494282603263855
    },
    {
      "name": "Natural language processing",
      "score": 0.34751656651496887
    },
    {
      "name": "Programming language",
      "score": 0.29086577892303467
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}