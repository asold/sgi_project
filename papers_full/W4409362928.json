{
  "title": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models",
  "url": "https://openalex.org/W4409362928",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1994694414",
      "name": "Chao Zeng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2229969423",
      "name": "Songwei Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2102008498",
      "name": "Yusheng Xie",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2102445713",
      "name": "Hong Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2118258370",
      "name": "Xiaojian Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2043220055",
      "name": "Miao Wei",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2103132892",
      "name": "Shu Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2125567895",
      "name": "FangMin Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2147303057",
      "name": "Xing Mei",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A1994694414",
      "name": "Chao Zeng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2229969423",
      "name": "Songwei Liu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2102008498",
      "name": "Yusheng Xie",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2102445713",
      "name": "Hong Liu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2118258370",
      "name": "Xiaojian Wang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2043220055",
      "name": "Miao Wei",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2103132892",
      "name": "Shu Yang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2125567895",
      "name": "FangMin Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2147303057",
      "name": "Xing Mei",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4310815131",
    "https://openalex.org/W4385326807",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W6992309071",
    "https://openalex.org/W4387848958",
    "https://openalex.org/W4393147284",
    "https://openalex.org/W4406650295",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2963259903",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W4388093177"
  ],
  "abstract": "Large Language Models (LLMs) have revolutionized natural language processing tasks. However, their practical application is constrained by substantial memory and computational demands. Post-training quantization (PTQ) is considered an effective method to accelerate LLM inference. Despite its growing popularity in LLM model compression, PTQ deployment faces two major challenges. First, low-bit quantization leads to performance degradation. Second, restricted by the limited integer computing unit type on GPUs, quantized matrix operations with different precisions cannot be effectively accelerated. To address these issues, we introduce a novel arbitrary-bit quantization algorithm and inference framework, ABQ-LLM. It achieves superior performance across various quantization settings and enables efficient arbitrary-precision quantized inference on the GPU. ABQ-LLM introduces several key innovations: (1) a distribution correction method for transformer blocks to mitigate distribution differences caused by full quantization of weights and activations, improving performance at low bit-widths. (2) the bit balance strategy to counteract performance degradation from asymmetric distribution issues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization acceleration framework that reconstructs the quantization matrix multiplication of arbitrary precision combinations based on BTC (Binary TensorCore) equivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM can convert each component bit width gain into actual acceleration gain, maximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8 quantization configuration on LLaMA-7B model, it achieved a WikiText2 perplexity of 7.59 (2.17⬇ vs 9.76 in AffineQuant). Compared to SmoothQuant, we realized 1.6x acceleration improvement and 2.7x memory compression gain.",
  "full_text": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language\nModels\nChao Zeng*†, Songwei Liu*, Yusheng Xie*, Hong Liu, Xiaojian Wang, Miao Wei,\nShu Yang, Fangmin Chen, Xing Mei‡\nByteDance Inc, Shenzhen, China\n{zengchaocs, cfangmin}@gmail.com, {liusongwei.zju, xieyusheng.12, xing.mei}@bytedance.com\nAbstract\nLarge Language Models (LLMs) have revolutionized natu-\nral language processing tasks. However, their practical ap-\nplication is constrained by substantial memory and compu-\ntational demands. Post-training quantization (PTQ) is con-\nsidered an effective method to accelerate LLMs inference.\nDespite its growing popularity in LLMs model compression,\nPTQ deployment faces two major challenges. First, low-bit\nquantization leads to performance degradation. Second, re-\nstricted by the limited integer computing unit type on GPUs,\nquantized matrix operations with different precisions cannot\nbe effectively accelerated. To address these issues, we in-\ntroduce a novel arbitrary-bit quantization algorithm and in-\nference framework, ABQ-LLM. It achieves superior perfor-\nmance across various quantization settings and enables ef-\nficient arbitrary-precision quantized inference on the GPU.\nABQ-LLM introduces several key innovations: (1) a dis-\ntribution correction method for transformer blocks to miti-\ngate distribution differences caused by full quantization of\nweights and activations, improving performance at low bit-\nwidths. (2) the bit balance strategy to counteract performance\ndegradation from asymmetric distribution issues at very low\nbit-widths (e.g., 2-bit). (3) an innovative quantization accel-\neration framework that reconstructs the quantization matrix\nmultiplication of arbitrary precision combinations based on\nBTC (Binary TensorCore) equivalents, gets rid of the limi-\ntations of INT4/INT8 computing units. ABQ-LLM can con-\nvert each component bit width gain into actual acceleration\ngain, maximizing performance under mixed precision(e.g.,\nW6A6, W2A8). Based on W2*A8 quantization configura-\ntion on LLaMA-7B model, it achieved a WikiText2 perplex-\nity of 7.59 (2.17↓ vs 9.76 in AffineQuant). Compared to\nSmoothQuant, we realized 1.6× acceleration improvement\nand 2.7× memory compression gain.\nIntroduction\nRecent advancements in large language models (LLMs)\n(Bubeck et al. 2023; Touvron et al. 2023a,b) have demon-\nstrated impressive capabilities across various natural lan-\nguage benchmark, including reasoning (Clark et al. 2019,\n*These authors contributed equally.\n†Work was done when Chao Zeng was intern at ByteDance Inc.\n‡Corresponding author\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n2018), cognitive processing (Xu et al. 2023a; Hardy et al.\n2023), and dialogue generation(Hu et al. 2023). However,\nthese models are characterized by a substantial number of\nparameters, posing significant challenges in terms of mem-\nory consumption and bandwidth (Zheng et al. 2024; Kim\net al. 2023).\nPost-training quantization (PTQ) effectively reduces both\ncomputational and storage requirements. This technique sig-\nnificantly accelerates model inference by converting the\nweights and activation values of large language models\n(LLMs) from high-precision floating-point numbers to low-\nprecision integer values for storage, and using efficient inte-\nger matrix multiplication operators to handle the bulk of ma-\ntrix multiplication computations during inference. Currently,\n80% of the computation and parameter access in LLMs is\nconcentrated on general matrix multiplication (GEMM) and\nvector multiplication (GEMV) operations, especially dur-\ning autoregressive decoding, where all GEMM operations\ndegrade into GEMV operations due to single-token gener-\nation. Consequently, the efficiency of GEMV computation\nand memory access directly determines the efficiency and\npower consumption of LLM inference.\nTo improve GEMM/GEMV memory access efficiency,\nLLMs inference typically employs a quantized inference\nstrategy. The current mainstream approach is weight-only\nquantization, where the kernel performs actual computation\nbased on dequantized FP16 values. However, this approach\noffers limited performance improvement in highly parallel\nscenarios. To further enhance quantized inference perfor-\nmance, the industry is pursuing full quantization of both\nweights and activation values to reduce activation memory\naccess and leverage higher computational power using quan-\ntized kernels, such as those from NVIDIA. However, current\nindustry practices in weight and activation full quantization\n(W A full quantization) face several limitations. NVIDIA\nprovides only a limited set of hardware-accelerated instruc-\ntions(Lin et al. 2024a; Ashkboos et al. 2024; Zhao et al.\n2024), which constrains the design space for quantization\nalgorithms. Other quantization combinations (e.g., W4A8 or\nW2A4) require type conversion to W8A8 or W4A4 during\ncomputation, leading to inefficiency(Lin et al. 2024b). Fur-\nthermore, due to GEMV , additional padding calculations are\nrequired in scenarios with a batch size less than 8, resulting\nin inefficient matrix multiplication for W4A4 and W8A8. Fi-\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n22299\nFigure 1: Perplexities analysis (lower is better) on the Wiki-\ntext2 dataset of LLaMA-7B with different quantized mod-\nules.\nnally, W A fully quantized models encounter significant chal-\nlenges in low-bit quantization (e.g., W2A8, W2A6).\nIn this paper, we introduce a novel quantization frame-\nwork for PTQ, called ABQ-LLM. By examining the quan-\ntization sensitivity of components within the transformer\nblock (Figure 1) and the attention map before and after quan-\ntization (Figure 2), we find the down\nproj linear layer and\nthe attention map particularly sensitive to quantization. To\naddress this, we propose a double cosine similarity distri-\nbution correction and an attention map distribution boot-\nstrap for the output of down\nproj. This method calibrates\nthe quantization constants and restores the model’s perfor-\nmance at low bit-widths such as W6A6, W4A4 and W2A8.\nAdditionally, we analyze performance degradation in low-\nbit quantization and address the asymmetric loss issue in\nlow-bit representations like INT2 using the bit balance strat-\negy. Finally, we implement customized software engine to\nsupport fully quantized inference of various precision com-\nbinations based on BTC equivalents, fully exploiting the ad-\nvantages of quantized models under mixed precision. Our\ncontributions are summarized as follows:\n• We propose a novel block-wise distribution correction\nand compensation scheme in the PTQ domain to miti-\ngate the distribution discrepancy caused by full quantiza-\ntion of weights and activations, thereby improving model\nperformance at low bit-widths.\n• We address the problem of asymmetric loss at low bit-\nwidths, such as INT2, and significantly improve INT2\nquantization performance using the bit balance strategy,\nenhancing model performance under the INT2 quantiza-\ntion configuration.\n• We propose a software engine which achieves quanti-\nzation freedom for the first time in the LLM field. It\neliminates the limitations of INT4/INT8 computational\nunits, and effectively avoids the GEMV problem. Under\nthe LLaMA-7B W2A8 configuration, it has 1.6× ulti-\nFigure 2: Attention maps for the first block (above) and the\nfinal block (below) are shown.\nmate acceleration compared to SmoothQuant, achieving\nSOTA performance.\nRelated Work\nLLM quantization can be broadly divided into weight-only\nquantization and weight-activation quantization.\nWeight-only quantization.To alleviate computational bur-\ndens, some studies focus on weight-only quantization.\nGPTQ (Frantar et al. 2022) uses Hessian-based error com-\npensation to reduce quantization errors in LLMs, enabling\n3-bit quantization. AWQ (Lin et al. 2024a) and OWQ (Lee\net al. 2024) significantly enhance quantized model per-\nformance by considering the impact of activation outliers\non weight quantization. Methods like QuIP(Chee et al.\n2024), QuIP# (Tseng et al. 2024), and AQLM(Egiazarian\net al. 2024) facilitate 2-bit quantization through learnable\ncodebooks or additional fine-tuning. Approaches such as\n(Dettmers et al. 2023; Shang et al. 2023; Huang et al. 2024)\nimprove PTQ performance through unstructured mixed-\nprecision fine-grained weight grouping. Additionally, re-\nsearch such as (Dettmers et al. 2024; Xu et al. 2023b; Ar-\nshia et al. 2022; Bondarenko, Del Chiaro, and Nagel 2024)\nemploys efficient parameter fine-tuning (PEFT) techniques\nto compress weights through fine-tuning.\nWeight-activation quantization.Weight-activation quanti-\nzation differs from weight-only quantization by quantizing\nboth weights and activation (including KV caches) to accel-\nerate LLM inference. The main challenge in quantizing acti-\nvation is handling outliers, which can cause significant quan-\ntization errors. To address this issue, ZeroQuant(Yao et al.\n2022) proposes a fine-grained, hardware-friendly quantiza-\ntion scheme for weights and activation. SmoothQuant(Xiao\net al. 2023) shifts the quantization difficulty from activa-\ntion to weights through mathematically equivalent transfor-\nmations, achieving W8A8 quantization. (Shao et al. 2023;\nMa et al. 2024b; Hu et al. 2024a) enhances performance by\ntraining quantization parameters. Limited by GPU platform\ninstruction limitations, these jobs can only use W8A8 to per-\n22300\nFigure 3: An overview of our ABQ-LLM. ABQ-LLM use\nour DLC loss and AKL loss to update learnable parameters.\nform actual inference, even if they achieve lower quantiza-\ntion bit-widths (e.g., W6A6).\nMethod\nIn this section, we provide a detailed introduction to our\nABQ-LLM. We first describe the distribution correction and\nbit balance strategy and then introduce our arbitrary-bit in-\nference framework.\nPreliminary\n(Xiao et al. 2023) achieves W A full quantization by scal-\ning activation outliers, but this increases the range vari-\nability of weights, making weight quantization more sensi-\ntive. Conversely, (Lin et al. 2024a) optimizes weight quan-\ntization by scaling weights, which significantly increases\nthe diversity of activation, complicating activation quanti-\nzation. These approaches highlight the drawbacks of man-\nually setting scaling balance factors between activation and\nweight, making it challenging to achieve a perfect balance.\nTo address this issue, we introduce a distribution correction-\nguided scaling method. Following (Shao et al. 2023) ap-\nproach, we set the balance vectors between weights and ac-\ntivation as learnable parameters and add a learnable clipping\nparameter for weight. By employing our distribution correc-\ntion and bit balance strategy to optimize model performance,\nour objectives are as follows:\nargmin\ns,α,β\n\r\rW\nX − Q(clip(W) · diag(s))Q(diag(s)−1 · X)\n\r\n\r ,\n(1)\nwhere W and X are full-precision\nweight and activation,\nQ(·) denotes the quantizer of weight and activation, clip(·)\ndenotes the clipping operation, s is the scale factor, and let-\nting Wmax = αmax(W) and Wmin = βmin(W) to control\nthe clipping range of the weight.\nImproving Quantization by Distribution\nCorrection\nWe observed significant variations in sensitivity across dif-\nferent layers of LLM models during quantization, with some\nlayers having a critical impact on quantization performance.\nTo validate this, as shown in Figure 1, we quantified var-\nious components of the LLaMA-7B model under weight-\nactivation full quantization. While quantizing the gate\nproj\nand up proj layers in mlp and attention resulted in only mi-\nnor performance degradation, quantizing the down proj lin-\near layer caused a substantial performance drop. This indi-\ncates that addressing down\nproj quantization is crucial for\nperformance recovery. Further analysis revealed that the pri-\nmary cause of performance degradation due to down\nproj\nquantization is the quantization of down proj activation. At\nlow bit-widths such as INT4, INT3, and INT2, the limited\nrepresentation range causes a significant shift in the model\ndistribution compared to full precision. As illustrated in Fig-\nure 3, during the block-wise quantization calibration pro-\ncess, we apply a d\nouble logarithm of cosine similarity loss\non the output of down proj to correct the distribution of the\nquantized model. The loss function called DLC loss Li\nDLC :\nLi\nDLC = −log(\ndi\nq · di\nfp\n\r\rdiq\n\r\r\n\r\n\r\rdi\nfp\n\r\r\r\n) − log(\ndi\nq · di\nfp∗\n\r\rdiq\n\r\r\n\r\n\r\rdi\nfp∗\n\r\r\r\n), (2)\nwhere di\nq represent the quantized output of the i-th trans-\nformer block, di\nfp represent the full-precision output of the\ni-th transformer block, and di\nfp∗ represent the full-precision\noutput of the i-th transformer block, with its input originat-\ning from the quantized output of the (i − 1)-th transformer\nblock.\nAdditionally, we conducted an analysis of the cosine sim-\nilarity between activation at the input and output of decoder\nblocks in the LLaMA-7B model. The results revealed signif-\nicant differences in similarity for the initial and final blocks,\nindicating their considerable impact on model inference per-\nformance. In response, we applied distribution compensa-\ntion vector to the down\nproj layers of these blocks to address\nand correct the distribution discrepancies using Eq. (3).\nWq = clamp(⌈W + γab⊤\n△ ⌋ + z, 0, 2n − 1), (3)\nwhere ⌈·⌋ denotes round operation, n represents the target\nbit-width, △ denotes the step-size, and z is the zero-point.\nWq and W denote the quantized and full-presicion weight,\nrespectively. The vectors a and b are distribution compen-\nsation vectors, where γ = 1 indicates compensation is per-\nformed, and γ = 0indicates no compensation.\nTo enhance the performance of the quantized model, we\nanalyzed the changes in Attention Map distribution before\nand after quantization, as shown in Figure 2. In the full-\nprecision model, attention is heavily focused on the first to-\nken, highlighting its key role in guiding text generation, con-\nsistent with LLM-QAT(Liu et al. 2023) findings. However,\nquantization disrupts this attention pattern, diminishing fo-\ncus on the first token. To address this and restore the model’s\nattention during quantization, we introduced a\nttention-aware\nKL divergence to reconstruct the attention map.\nLi\nAKL = DK\nL(attni\nq ∥ attni\nfp ) +DKL(attni\nfp ∥ attni\nq),\n(4)\nwhere attni\nq denotes the\nquantized attention map output of\nthe i-th transformer block, while attni\nfp refers to the full-\nprecision attention map output of the same block.\n22301\nFigure 4: System Overview of Custom Software Engine. p and q represent the quantization bit width of input X and weight\nW. Data flows are carefully designed to support efficient computation: global memory(GL) → shared memory(SMEM) →\nfragment(FR) → shared memory(SMEM) → global memory(GL).\nIn the end, we combined DLC loss and AKL loss, and our\nfinal optimization goal is:\nsi\n∗, αi\n∗, β∗\ni = argmin\nsi,αi,βi\n(Li\nDLC + Li\nAKL), (5)\nwhere si\n∗, αi\n∗ and β∗\ni are the parameters of the i-th trans-\nformer block after calibration. When the distributions of the\nquantized output and the full-precision output match, we\nhave a loss close to 0, which effectively guides the quan-\ntization process.\nBit Balance Strategy\nTypically, pre-trained LLM model weight exhibit near-\nnormal distribution, characterized by symmetry. Using Q-\nQ plots (Quantile-Quantile Plots), we confirmed the strong\nsymmetry in the weight distribution of pre-trained models.\nHowever, in standard INT2 quantization, the numerical rep-\nresentation is limited to four values, with symmetric quan-\ntization ranges of {-2, -1, 0, 1} or {-1, 0, 1, 2}, disrupting\nthe original symmetric weight distribution. This asymme-\ntry leads to significant performance degradation, as shown\nin Table 1, where performance drops by 0.46 from W4A16\nto W3A16 and by 5.19 from W3A16 to W2A16, indicat-\ning a sharp decline. To address this asymmetry impact on\nLLMs quantization, we adopted the bit balance strategy like\n(Li et al. 2016; Ma et al. 2024a), extending the INT2 sym-\nmetric quantization space to {-2, -1, 0, 1, 2}. This modifica-\ntion restored model performance to 7.50, which is within a\nreasonable range compared to W3A16.\nCustom Software Engine\nReconstructing Arbitrary Bit Computation.To support\nW1A1 quantization, NVIDIA introduced INT1 TensorCore\nin Turing and later architectures to provide hardware sup-\nport. However, W1A1 quantization has not been widely ap-\nplied due to significant performance degradation. Through\nmathematical analysis of quantized matrix multiplication,\nwe find that any combination of quantization can be decom-\nposed into a superposition of 1-bit matrix multiplications.\nAssuming the weight W of a particular neural network layer\nare quantized to q bits and the input activation value X is\nquantized to p bits, the matrix multiplication ofW and X re-\nsults in a 32-bit output Y = W X. The key is to observe that\nthe scalar values at any position in W and X can be decom-\nposed into a series of 1-bit scalar numbers. Scalar operations\nwith any combination of precision can be decomposed into\n1-bit operations and shift operations. For example, a 2-bit x\ncan be expressed as:\nx = x1x0, where xi ∈ INT1, (6)\nwhere x1 = ( x ≫ 1)&1, x0 = ( x ≫ 0)&1. We use\nOP (a, b) to denote a computational operation where the in-\nput is 1-bit data and the output is 32-bit. Thus, the original\nscalar-level arbitrary precision computation wx can be rep-\nresented as:\nwx = w ∗ (x1x0) =OP (w, x1) ∗ 2 +OP (w, x0). (7)\nThe above procedure can be generalized to any combina-\ntion of matrix multiplications with bit-widths p and q. The\ndetailed formulas are shown in the Appendix. Using these\ntransformations, we decompose the operation of arbitrary\nquantized combinations into a superposition of 1-bit ma-\ntrix multiplications, enabling the underlying layer to invoke\nhigh-computing instruction implementations.\nEngine Implementation.NVIDIA GPU has many process-\ning elements called Streaming Multiprocessors (SMs) and\nuses a large number of threads to perform computing tasks in\nparallel. Threads are structured into thread blocks, which be-\ncome the smallest scheduling execution unit on SMs. There-\nfore, the computation target is decomposed and mapped to\neach thread block, calledThread Block Tile, to achieve par-\nallel computing. As shown in Figure 4, for a GEMM task of\nshape M×N×K, each thread block is responsible for comput-\ning a BM×BN output block, which is decomposed into K\nBK\n22302\nModel Bits WikiText2 C4\nLLaMA-7B\nW4A16 5.83 7.29\nW3A16 6.29 8.01\nW2A16 11.48 15.74\nW2*A16 7.50 9.86\nTable 1: Performance comparison of LLaMA-7B under W4,\nW3, and W2 quantization configurations. * denotes the use\nof the bit balance strategy.\nsub-GEMM tasks of shape BM×BN×BK. Our engine con-\nverts quantized matrix multiplications with bit widths con-\nfigured as {p, q}into special accumulations of p*q binarized\nmatrix multiplications, so the true calculation task of thread\nblock tile is p*BM × q*BN.\n1 First, in order to improve\nmemory access continuity, we propose BitPacking strategy\nto decompose the quantized tensor into n binary matrices,\nwhere n is the quantization bit width. Taking input X as an\nexample, this means that its bit perspective layout changes\nfrom [M, K, p] to [p, M, K]. All threads within a thread block\nshare the same shared memory space. Within each thread\nblock, threads are further organized into a set of warps, with\neach warp consisting of 32 consecutive threads.\n2 Next,\nwarps collaborates to load the A matrix (p*BM × BK) and\nB matrix (BK × q*BN) data required for thread block tile\ncalculation from GL and caches them in SMEM. Thanks\nto BitPacking, the process of reading p BM*BK single-bit\nrow-major tiles and writing p*BM*BK bits SMEM is effi-\ncient and continuous.\n3 Subsequently, thread block con-\ntains multiple warps, so thread block tile can be further de-\ncomposed into Warp Tileto achieve warp-level parallelism,\nand the computing tasks of each warp are WM× WN. In\nthe calculation preparation stage, the A matrix (WM×WK,\nrow-major) and B matrix (WK×WN, col-major) are inde-\npendently loaded from SMEM to FR. Then, the calculation\nis decomposed into WM\nTILES*W ARPN TILES Tensor-\nCore MMA(matrix-multiply-accumulate). Since A and B\nare binarized matrices, we actually use Binary TensorCore\nMMA (BMMA), which has a computing power 8 times and\n4 times higher than INT8 and INT4 TensorCore respectively.\n4 All warps collaboratively complete the Thread Block\nTile calculation, and the results are stored in the c frag-\nment of each warp. Therefore, each warp needs to indepen-\ndently write the calculation results back to SMEM.\n5 Out-\nput tile(p*BM × q*BN) is globally reduced to obtain a final\nresult(BM × BN), where each BM × BN sub-Tile needs to\nbe multiplied by a certain scaling factor. We call this process\nBit Reduction.\n6 As the final step, warps collaboratively\nload the final result from SMEM and write back to the target\nlocation in GL.\nWe implement the above calculation process as a GPU\nKernel, called ABQKernel. ABQKernel is used to replace\nall gemm operations in the decoder layer, and assists with\nnecessary BitPacking, quantization, and dequantization op-\nerations to achieve arbitrary quantization inference of the\nLLaMA model. We carefully manages the overhead of quan-\ntization operators by fusing them into existing operators and\nweight BitPacking is implemented offline for increased effi-\nciency.\nGPU Kernel Optimization.When M=1, the GEMM prob-\nlem of shape MxNxK transforms into a GEMV problem,\nshifting from computation-intensive to memory-intensive,\nwhich becomes a performance bottleneck for model infer-\nence. When using ordinary TensorCore for accelerated com-\nputation, the dimensions of M are usually chunked in groups\nof 8, requiring padding if M<8, leading to 87.5% redundant\ncomputation. Thanks to the revolutionary reconstruction of\ncomputing and BitPacking strategy, for the WqAp configu-\nration, the actual computing task undertaken by ABQKer-\nnel is p*M × q*N × K. The expansion of the M dimension\ncan effectively reduce the redundant calculations when call-\ning TensorCore, and even when p*M >= 8 and p*M % 8\n= 0, padding can be completely avoided. We call the above\noptimization strategy GEMV Elimination. In addition, Com-\nputational and Pipeline Optimization, Auto Kernel Search,\nand Bank Conflicts Elimination are also applied.\nExperiments\nExperimental Setup\nBaseline. For weight-only quantization, we compare our ap-\nproach with GPTQ(Frantar et al. 2022), AWQ(Lin et al.\n2024a), OmniQuant(Shao et al. 2023), and AffineQuant(Ma\net al. 2024b). For weight-activation quantization, we bench-\nmark our method against SmoothQuant(Xiao et al. 2023),\nOmniQuant(Shao et al. 2023), and I-LLM(Hu et al. 2024b).\nModels and Datasets.We primarily evaluate our method\nusing LLaMA (7B-13B) (Touvron et al. 2023a) and\nLLaMA-2 (7B-13B) (Touvron et al. 2023b)in this paper.\nFollowing previous work(Shao et al. 2023; Ma et al. 2024b),\nwe evaluate the quantized models by reporting the perplexity\nof language generation experiments on WikiText2(Merity\net al. 2016) and C4(Raffel et al. 2020). To assess perfor-\nmance on zero-shot tasks, we select several popular bench-\nmarks including PIQA(Bisk et al. 2020), ARC(Clark et al.\n2018), BoolQ(Clark et al. 2019), HellaSwag(Zellers et al.\n2019), and Winogrande(Sakaguchi et al. 2021) using the lm-\nevaluation-harness(Gao et al. 2021).\nCalibration. We initialize the balance vectors for weights\nand activations following (Xiao et al. 2023), with the learn-\nable clipping parameter for weights set to 1. For distribu-\ntion correction compensation vectors, we seta as an all-ones\nvector and b as an all-zeros vector, ensuring ab⊤ starts at 0.\nUsing the AdamW optimizer (Loshchilov and Hutter 2017)\nwith no weight decay, we set learning rates of 5e-3 for bal-\nance vectors and 1e-2 for the clipping parameter and vec-\ntor compensation vector. Calibration data includes 128 ran-\ndomly selected 2048-token segments from WikiText2. The\ncalibration process, conducted on an NVIDIA A800-40G\nGPU, utilized a batch size of 1 and spanned 20 epochs. For\nactivation and KV Cache we perform per-token quantiza-\ntion, and for weight we perform per-channel quantization.\nBy default, activation and KV cache use the same quantiza-\ntion bit.\n22303\nBits Method LLaMA-7B\nLLaMA-13B LLaMA-2-7B LLaMA-2-13B\nWikiT\next2 C4 WikiText2 C4 WikiText2 C4 WikiText2 C4\nW6A6\nSmoothQuant 6.03\n7.47 5.42 6.97 6.20 7.76 5.18 7.67\nOmniQuant 5.96 7.43 5.28 6.84 5.87 7.48 5.14 6.74\nI-LLM 5.84 7.32 5.23 6.79 5.68 7.27 5.10 6.74\nABQ-LLM 5.81 7.27 5.21 6.77 5.63 7.21 5.00 6.64\nW4A4\nSmoothQuant 22.25\n32.22 40.05 47.18 83.12 77.27 35.88 43.19\nOmniQuant 11.26 14.51 10.87 13.78 14.26 18.02 12.30 14.55\nAffineQuant 10.28 13.64 10.32 13.44 12.69 15.76 11.45 13.97\nI-LLM 9.10 12.33 7.99 10.96 10.55 12.92 9.76 12.57\nABQ-LLM 8.63 12.10 7.69 10.90 9.31 12.85 8.62 11.47\nW2A8\nOmniQuant 15.70\n26.44 13.50 19.01 37.95 103.39 21.74 31.72\nAffineQuant 9.76 15.52 9.21 12.55 1483 4688 12.30 29.32\nI-LLM 14.08 18.89 11.80 16.19 123.93 200.54 25.74 40.59\nABQ-LLM 11.35 15.41 9.20 12.48 13.47 17.82 13.24 18.07\nW2*A8 ABQ-LLM 7.59\n10.00 6.49 8.53 7.85 10.33 6.65 10.01\nTable 2: Weight-activation quantization perplexities (lower is better) comparison of quantized LLaMA and LLaMA-2 models.\n* denotes the use of the bit balance strategy. More results can be found in at Appendix.\nFigure 5: The GEMV speedup comparison of our ABQK-\nernel, CUTLASS (W4A4/W8A8), and cuBLAS (W8A8) in\nRTX 3070. The left side is compared against W8A8 and the\nright side against W4A4. More result in RTX 4080 can be\nfound in Appendix.\nExperiments on Language Generation Tasks\nLanguage generation capability is central to large lan-\nguage models (LLMs). To validate our extraordinary perfor-\nmance in the challenging quantization task, we first com-\npare perplexity, a crucial metric for language generation,\nwith the baseline. As shown in Table 2, ABQ-LLM demon-\nstrates outstanding performance across various quantization\nconfigurations, surpassing state-of-the-art methods such as\nAffineQuant and I-LLM. Notably, in the INT2 setting, the\napplication of bit balance strategy yields significant im-\nprovements at minimal cost. The W2*A8 configurations\nsubstantially outperform the W2A8 configurations. Specifi-\ncally, perplexity on WikiText2 and C4 datasets decreases by\nan average of 1.42 and 2.11 points, respectively, for W2*A8\ncompared to the W4A4. These findings validate the effec-\ntiveness of our distribution correction and bit balance strat-\negy. The results of weight-only quantization are presented\nat Appendix. Additional results for full W A quantization are\nprovided at Appendix.\nExperiments on Zero-Shot Tasks\nTo further validate our model, we compare zero-shot accu-\nracy with the baseline method, as shown in Table 3. Our\nABQ-LLM method outperforms the previous method in\nmost cases. Notably, after applying the bit balance strategy,\nthe performance of W2*A8 improves significantly by 7.50%\non average. Combining the performance in both language\ngeneration and zero-shot tasks, we conclude that ABQ-\nLLM achieves state-of-the-art results in handling challeng-\ning quantization tasks. See the Appendix for more results\nand analysis of quantized configurations.\nInference Engine Evaluation\nKernel Benchmark.We evaluated the GEMV speedup of\nour ABQKernel across three matrix dimensions in LLaMA-\n7B and compared it with the quantization kernel provided by\ncuBLAS and CUTLASS. It is important to note that CUT-\nLASS only supports W4A4 and W8A8, while cuBLAS sup-\nports only W8A8 for quantization operations. Our experi-\nments were conducted on two different GPUs: the RTX 4080\nand the RTX 3070. Figure 5 presents the results, showing\nthat our ABQKernel achieve superior acceleration across all\nmatrix sizes. Specifically, for special bit combinations such\nas W2A8 and W2A4, our ABQKernel significantly outper-\nforms the baseline approaches, as cuBLAS and CUTLASS\nrequire conversion to W8A8 and W4A4 for computation.\nIn the W2A8 configuration, our throughput is improved by\n7.47× compared to W8A8 with CUTLASS and cuBLAS on\ndimensions (1, 4096) × (4096, 4096).\n22304\nModel Bit\ns Method PiQA ARC-e ARC-c BoolQ HellaSwag Winogrande Avg.\nLLaMA-13B\nW4A4\nOmniQuant 69.69\n47.30 33.10 62.84 58.96 55.80 54.37\nAffineQuant 66.32 43.90 29.61 64.10 56.88 54.70 52.58\nI-LLM 67.95 48.15 34.47 62.29 63.13 59.98 55.99\nABQ-LLM 71.82 47.60 35.67 63.52 64.31 57.54 56.74\nW2A8\nOmniQuant 66.76\n45.62 30.20 61.13 52.93 55.72 52.06\nAffineQuant 71.00 46.70 32.33 62.23 58.62 63.53 55.73\nI-LLM 67.46 43.73 29.69 62.41 53.37 55.09 51.95\nABQ-LLM 72.03 46.72 31.74 65.17 58.71 62.50 56.15\nW2*A8 ABQ-LLM 74.91\n54.92 38.65 68.53 68.21 66.54 61.96\nLLaMA-2-13B\nW4A4\nOmniQuant 67.08\n45.66 32.25 63.73 58.39 54.61 53.62\nAffineQuant 67.68 46.63 32.85 65.90 60.62 54.14 54.63\nI-LLM 68.00 45.74 30.97 64.55 60.62 54.22 54.01\nABQ-LLM 69.04 47.01 33.53 64.74 62.70 54.38 55.23\nW2A8\nOmniQuant 62.67\n38.80 28.41 62.11 49.04 51.69 48.78\nAffineQuant 61.31 38.51 26.96 62.04 41.92 50.74 46.91\nI-LLM 61.86 38.67 26.45 62.17 43.30 51.85 47.38\nABQ-LLM 64.30 40.19 29.78 63.18 49.58 52.17 49.87\nW2*A8 ABQ-LLM 73.50\n49.79 35.15 70.15 67.45 58.87 59.15\nTable 3: Zero-shot accuracies (higher is better) comparison of quantized LLaMA and LLaMA-2 models. * denotes the use of\nthe bit balance strategy. More results can be found in at Appendix.\nFigure 6: Inference latency (top) and memory usage (bot-\ntom) of the FastTransformer implementation on NVIDIA\nA800-40GB GPU with a fixed input length of 15. More re-\nsults can be found in at Appendix.\nEnd-to-end throughput. As shown in Figure 6, we inte-\ngrate our ABQKernel into FastTransformer and compare it\nwith the FP16 implementation of FastTransformer and the\nINT8 implementation of SmoothQuant. Compared to FP16,\nour scheme achieves 2.95×inference acceleration and 4.8×\nmemory compression gain, while requiring only 10GB of\nmemory for inference on the LLaMA-30B model, which is\nless than the memory required for LLaMA-7B with FP16.\nAdditionally, our scheme achieves 1.6× speedup and 2.7×\nmemory compression gain over SmoothQuant, significantly\noutperforming current mainstream inference methods. This\nsubstantial improvement reduces the cost of LLM services\nand facilitates their practical deployment.\nKernel Optimization Ablation.Table 4 presents the impact\nof various optimization techniques on the inference latency\nof the kernel when performing the GEMV operation with\ndimensions (1, 4096) × (4096, 4096). Our ABQKernel sig-\nnificantly outperforms the CUTLASS W8A8 kernel when\nunoptimized. Additionally, by employing pipeline optimiza-\ntion, GEMV elimination and auto kernel search, we achieve\na latency reduction by 7.47× and a corresponding increase\nin throughput by 7.47×. These results significantly outper-\nform CUTLASS.\nMethod Latency(us)↓ TOPS ↑\nCUTLASS 49.96 0.67\nNative kernel 20.05 1.67\n+ Pipeline Optimization 14.66 2.28\n+ Eliminiate GEMV 10.92 3.07\n+ Auto Kernel Search 6.68 5.01\nTable 4: An ablation study of the impact of optimization\ntechniques used in the inference engine on Kernel latency\nand throughput.\nConclusion\nWe present an arbitrary bit quantization inference frame-\nwork called ABQ-LLM. Through an in-depth analysis of\nLLM quantization, we introduce distribution correction and\nbit balance strategy to enhance model performance. We then\ndesign a novel arbitrary bit inference engine to fully leverage\nthe advantages of LLM quantization. Extensive experimen-\ntal results demonstrate that ABQ-LLM achieves outstanding\nperformance across various quantization configurations, in-\ncluding W6A6, W4A4, and W2A8. Moreover, ABQKernel\nconsistently outperformed both CUTLASS and cuBLAS in\nall configurations. Our end-to-end inference speed is 1.6 ×\nfaster than the industry SOTA, SmoothQuant, and achieves\n2.7 × memory compression gain.\n22305\nReferences\nArshia, F. Z.; Keyvanrad, M. A.; Sadidpour, S. S.; and\nMohammadi, S. M. R. 2022. PeQA: A Massive Persian\nQuestion-Answering and Chatbot Dataset. In 2022 12th In-\nternational Conference on Computer and Knowledge Engi-\nneering (ICCKE), 392–397. IEEE.\nAshkboos, S.; Mohtashami, A.; Croci, M. L.; Li, B.; Jaggi,\nM.; Alistarh, D.; Hoefler, T.; and Hensman, J. 2024. Quarot:\nOutlier-free 4-bit inference in rotated llms. arXiv preprint\narXiv:2404.00456.\nBisk, Y .; Zellers, R.; Gao, J.; Choi, Y .; et al. 2020. Piqa: Rea-\nsoning about physical commonsense in natural language.\nIn Proceedings of the AAAI conference on artificial intel-\nligence, volume 34, 7432–7439.\nBondarenko, Y .; Del Chiaro, R.; and Nagel, M. 2024.\nLow-Rank Quantization-Aware Training for LLMs. arXiv\npreprint arXiv:2406.06385.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; et al. 2023. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712.\nChee, J.; Cai, Y .; Kuleshov, V .; and De Sa, C. M. 2024. Quip:\n2-bit quantization of large language models with guarantees.\nAdvances in Neural Information Processing Systems, 36.\nClark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins,\nM.; and Toutanova, K. 2019. BoolQ: Exploring the surpris-\ning difficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;\nSchoenick, C.; and Tafjord, O. 2018. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457.\nDettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,\nL. 2024. Qlora: Efficient finetuning of quantized llms. Ad-\nvances in Neural Information Processing Systems, 36.\nDettmers, T.; Svirschevski, R.; Egiazarian, V .; Kuznedelev,\nD.; Frantar, E.; Ashkboos, S.; Borzunov, A.; Hoefler, T.; and\nAlistarh, D. 2023. Spqr: A sparse-quantized representation\nfor near-lossless llm weight compression. arXiv preprint\narXiv:2306.03078.\nEgiazarian, V .; Panferov, A.; Kuznedelev, D.; Frantar, E.;\nBabenko, A.; and Alistarh, D. 2024. Extreme compression\nof large language models via additive quantization. arXiv\npreprint arXiv:2401.06118.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2022.\nGptq: Accurate post-training quantization for generative\npre-trained transformers. arXiv preprint arXiv:2210.17323.\nGao, L.; Tow, J.; Biderman, S.; Black, S.; DiPofi, A.; Fos-\nter, C.; Golding, L.; Hsu, J.; McDonell, K.; Muennighoff,\nN.; et al. 2021. A framework for few-shot language model\nevaluation. Version v0. 0.1. Sept, 10: 8–9.\nHardy, M.; Sucholutsky, I.; Thompson, B.; and Griffiths, T.\n2023. Large language models meet cognitive science: LLMs\nas tools, models, and participants. In Proceedings of the\nannual meeting of the cognitive science society, volume 45.\nHu, X.; Chen, Y .; Yang, D.; Zhou, S.; Yuan, Z.; Yu, J.; and\nXu, C. 2024a. I-LLM: Efficient Integer-Only Inference for\nFully-Quantized Low-Bit Large Language Models. arXiv\npreprint arXiv:2405.17849.\nHu, X.; Chen, Y .; Yang, D.; Zhou, S.; Yuan, Z.; Yu, J.; and\nXu, C. 2024b. I-LLM: Efficient Integer-Only Inference for\nFully-Quantized Low-Bit Large Language Models. arXiv\npreprint arXiv:2405.17849.\nHu, Z.; Feng, Y .; Luu, A. T.; Hooi, B.; and Lipani, A. 2023.\nUnlocking the potential of user feedback: Leveraging large\nlanguage model as user simulators to enhance dialogue sys-\ntem. In Proceedings of the 32nd ACM International Confer-\nence on Information and Knowledge Management, 3953–\n3957.\nHuang, W.; Liu, Y .; Qin, H.; Li, Y .; Zhang, S.; Liu, X.;\nMagno, M.; and Qi, X. 2024. Billm: Pushing the limit\nof post-training quantization for llms. arXiv preprint\narXiv:2402.04291.\nKim, S.; Hooper, C.; Gholami, A.; Dong, Z.; Li, X.;\nShen, S.; Mahoney, M. W.; and Keutzer, K. 2023.\nSqueezellm: Dense-and-sparse quantization. arXiv preprint\narXiv:2306.07629.\nLee, C.; Jin, J.; Kim, T.; Kim, H.; and Park, E. 2024. Owq:\nOutlier-aware weight quantization for efficient fine-tuning\nand inference of large language models. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 38,\n13355–13364.\nLi, F.; Liu, B.; Wang, X.; Zhang, B.; and Yan, J. 2016.\nTernary weight networks. arXiv preprint arXiv:1605.04711.\nLin, J.; Tang, J.; Tang, H.; Yang, S.; Chen, W.-M.; Wang, W.-\nC.; Xiao, G.; Dang, X.; Gan, C.; and Han, S. 2024a. AWQ:\nActivation-aware Weight Quantization for On-Device LLM\nCompression and Acceleration. Proceedings of Machine\nLearning and Systems, 6: 87–100.\nLin, Y .; Tang, H.; Yang, S.; Zhang, Z.; Xiao, G.; Gan, C.;\nand Han, S. 2024b. Qserve: W4a8kv4 quantization and\nsystem co-design for efficient llm serving. arXiv preprint\narXiv:2405.04532.\nLiu, Z.; Oguz, B.; Zhao, C.; Chang, E.; Stock, P.; Mehdad,\nY .; Shi, Y .; Krishnamoorthi, R.; and Chandra, V . 2023. Llm-\nqat: Data-free quantization aware training for large language\nmodels. arXiv preprint arXiv:2305.17888.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101.\nMa, S.; Wang, H.; Ma, L.; Wang, L.; Wang, W.; Huang, S.;\nDong, L.; Wang, R.; Xue, J.; and Wei, F. 2024a. The era of\n1-bit llms: All large language models are in 1.58 bits. arXiv\npreprint arXiv:2402.17764.\nMa, Y .; Li, H.; Zheng, X.; Ling, F.; Xiao, X.; Wang, R.; Wen,\nS.; Chao, F.; and Ji, R. 2024b. Affinequant: Affine trans-\nformation quantization for large language models. arXiv\npreprint arXiv:2403.12544.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R.\n2016. Pointer sentinel mixture models. arXiv preprint\narXiv:1609.07843.\n22306\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research, 21(140):\n1–67.\nSakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi, Y .\n2021. Winogrande: An adversarial winograd schema chal-\nlenge at scale. Communications of the ACM, 64(9): 99–106.\nShang, Y .; Yuan, Z.; Wu, Q.; and Dong, Z. 2023. Pb-llm:\nPartially binarized large language models. arXiv preprint\narXiv:2310.00034.\nShao, W.; Chen, M.; Zhang, Z.; Xu, P.; Zhao, L.; Li, Z.;\nZhang, K.; Gao, P.; Qiao, Y .; and Luo, P. 2023. Omniquant:\nOmnidirectionally calibrated quantization for large language\nmodels. arXiv preprint arXiv:2308.13137.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023a. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023b. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nTseng, A.; Chee, J.; Sun, Q.; Kuleshov, V .; and De Sa,\nC. 2024. Quip#: Even better LLM quantization with\nhadamard incoherence and lattice codebooks.arXiv preprint\narXiv:2402.04396.\nXiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and Han,\nS. 2023. Smoothquant: Accurate and efficient post-training\nquantization for large language models. In International\nConference on Machine Learning, 38087–38099. PMLR.\nXu, P.; Shao, W.; Zhang, K.; Gao, P.; Liu, S.; Lei, M.; Meng,\nF.; Huang, S.; Qiao, Y .; and Luo, P. 2023a. Lvlm-ehub:\nA comprehensive evaluation benchmark for large vision-\nlanguage models. arXiv preprint arXiv:2306.09265.\nXu, Y .; Xie, L.; Gu, X.; Chen, X.; Chang, H.; Zhang,\nH.; Chen, Z.; Zhang, X.; and Tian, Q. 2023b. Qa-lora:\nQuantization-aware low-rank adaptation of large language\nmodels. arXiv preprint arXiv:2309.14717.\nYao, Z.; Yazdani Aminabadi, R.; Zhang, M.; Wu, X.; Li,\nC.; and He, Y . 2022. Zeroquant: Efficient and afford-\nable post-training quantization for large-scale transformers.\nAdvances in Neural Information Processing Systems, 35:\n27168–27183.\nZellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,\nY . 2019. Hellaswag: Can a machine really finish your sen-\ntence? arXiv preprint arXiv:1905.07830.\nZhao, Y .; Lin, C.-Y .; Zhu, K.; Ye, Z.; Chen, L.; Zheng, S.;\nCeze, L.; Krishnamurthy, A.; Chen, T.; and Kasikci, B. 2024.\nAtom: Low-bit quantization for efficient and accurate llm\nserving. Proceedings of Machine Learning and Systems, 6:\n196–209.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;\nZhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2024.\nJudging llm-as-a-judge with mt-bench and chatbot arena.\nAdvances in Neural Information Processing Systems, 36.\n22307",
  "topic": "Acceleration",
  "concepts": [
    {
      "name": "Acceleration",
      "score": 0.6902953386306763
    },
    {
      "name": "Bit (key)",
      "score": 0.6811267733573914
    },
    {
      "name": "Inference",
      "score": 0.6382691264152527
    },
    {
      "name": "Computer science",
      "score": 0.502307653427124
    },
    {
      "name": "Arithmetic",
      "score": 0.37327927350997925
    },
    {
      "name": "Programming language",
      "score": 0.36404091119766235
    },
    {
      "name": "Physics",
      "score": 0.32980138063430786
    },
    {
      "name": "Mathematics",
      "score": 0.28080105781555176
    },
    {
      "name": "Artificial intelligence",
      "score": 0.16262751817703247
    },
    {
      "name": "Quantum mechanics",
      "score": 0.1466415524482727
    },
    {
      "name": "Computer network",
      "score": 0.10609793663024902
    }
  ],
  "institutions": []
}