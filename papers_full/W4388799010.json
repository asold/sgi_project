{
  "title": "Automatic segmentation of mandibular canal using transformer based neural networks",
  "url": "https://openalex.org/W4388799010",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101268690",
      "name": "Jinxuan Lv",
      "affiliations": [
        "Chongqing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101817104",
      "name": "Lang Zhang",
      "affiliations": [
        "Chongqing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5086062267",
      "name": "Jiajie Xu",
      "affiliations": [
        "Chongqing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100382413",
      "name": "Wang Li",
      "affiliations": [
        "Chongqing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100357028",
      "name": "Gen Li",
      "affiliations": [
        "Chongqing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5077183481",
      "name": "Hengyu Zhou",
      "affiliations": [
        "Chongqing University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2521116342",
    "https://openalex.org/W2554488578",
    "https://openalex.org/W6686534076",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W4205296566",
    "https://openalex.org/W6847208418",
    "https://openalex.org/W4224246525",
    "https://openalex.org/W6765015339",
    "https://openalex.org/W4308030199",
    "https://openalex.org/W4313154151",
    "https://openalex.org/W4200364347",
    "https://openalex.org/W4323971903",
    "https://openalex.org/W6791637211",
    "https://openalex.org/W4205213185",
    "https://openalex.org/W3015101934",
    "https://openalex.org/W4289884238",
    "https://openalex.org/W6632690959",
    "https://openalex.org/W2152435890",
    "https://openalex.org/W3015126388",
    "https://openalex.org/W3213420920",
    "https://openalex.org/W6663296248",
    "https://openalex.org/W2201984498",
    "https://openalex.org/W4206513200",
    "https://openalex.org/W4220856289",
    "https://openalex.org/W6685994911",
    "https://openalex.org/W6775075911",
    "https://openalex.org/W4312211566",
    "https://openalex.org/W3193576071",
    "https://openalex.org/W2156323709",
    "https://openalex.org/W3206771274",
    "https://openalex.org/W4296558795",
    "https://openalex.org/W6842111019",
    "https://openalex.org/W4292787301",
    "https://openalex.org/W4296355192",
    "https://openalex.org/W4311001826",
    "https://openalex.org/W4318566866",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4205324465",
    "https://openalex.org/W4226078923",
    "https://openalex.org/W2181963178",
    "https://openalex.org/W1547176896",
    "https://openalex.org/W4286696412",
    "https://openalex.org/W4396783545",
    "https://openalex.org/W2183702206",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W4243823172"
  ],
  "abstract": "Accurate 3D localization of the mandibular canal is crucial for the success of digitally-assisted dental surgeries. Damage to the mandibular canal may result in severe consequences for the patient, including acute pain, numbness, or even facial paralysis. As such, the development of a fast, stable, and highly precise method for mandibular canal segmentation is paramount for enhancing the success rate of dental surgical procedures. Nonetheless, the task of mandibular canal segmentation is fraught with challenges, including a severe imbalance between positive and negative samples and indistinct boundaries, which often compromise the completeness of existing segmentation methods. To surmount these challenges, we propose an innovative, fully automated segmentation approach for the mandibular canal. Our methodology employs a Transformer architecture in conjunction with cl-Dice loss to ensure that the model concentrates on the connectivity of the mandibular canal. Additionally, we introduce a pixel-level feature fusion technique to bolster the model’s sensitivity to fine-grained details of the canal structure. To tackle the issue of sample imbalance and vague boundaries, we implement a strategy founded on mandibular foramen localization to isolate the maximally connected domain of the mandibular canal. Furthermore, a contrast enhancement technique is employed for pre-processing the raw data. We also adopt a Deep Label Fusion strategy for pre-training on synthetic datasets, which substantially elevates the model’s performance. Empirical evaluations on a publicly accessible mandibular canal dataset reveal superior performance metrics: a Dice score of 0.844, click score of 0.961, IoU of 0.731, and HD95 of 2.947 mm. These results not only validate the efficacy of our approach but also establish its state-of-the-art performance on the public mandibular canal dataset.",
  "full_text": "Automatic segmentation of\nmandibular canal using\ntransformer based neural\nnetworks\nJinxuan Lv† , Lang Zhang† , Jiajie Xu, Wang Li*, Gen Li and\nHengyu Zhou\nSchool of Pharmacy and Bioengineering, Chongqing University of Technology, Chongqing, China\nAccurate 3D localization of the mandibular canal is crucial for the success of\ndigitally-assisted dental surgeries. Damage to the mandibular canal may result in\nsevere consequences for the patient, including acute pain, numbness, or even\nfacial paralysis. As such, the development of a fast, stable, and highly precise\nmethod for mandibular canal segmentation is paramount for enhancing the\nsuccess rate of dental surgical procedures. Nonetheless, the task of mandibular\ncanal segmentation is fraught with challenges, including a severe imbalance\nbetween positive and negative samples and indistinct boundaries, which often\ncompromise the completeness of existing segmentation methods. To surmount\nthese challenges, we propose an innovative, fully automated segmentation\napproach for the mandibular canal. Our methodology employs a Transformer\narchitecture in conjunction with cl-Dice loss to ensure that the model\nconcentrates on the connectivity of the mandibular canal. Additionally, we\nintroduce a pixel-level feature fusion technique to bolster the model ’s\nsensitivity to ﬁne-grained details of the canal structure. To tackle the issue of\nsample imbalance and vague boundaries, we implement a strategy founded on\nmandibular foramen localization to isolate the maximally connected domain of\nthe mandibular canal. Furthermore, a contrast enhancement technique is\nemployed for pre-processing the raw data. We also adopt a Deep Label Fusion\nstrategy for pre-training on synthetic datasets, which substantially elevates the\nmodel’s performance. Empirical evaluations on a publicly accessible mandibular\ncanal dataset reveal superior performance metrics: a Dice score of 0.844, click\nscore of 0.961, IoU of 0.731, and HD95 of 2.947 mm. These results not only\nvalidate the efﬁcacy of our approach but also establish its state-of-the-art\nperformance on the public mandibular canal dataset.\nKEYWORDS\nmandibular canal, transformer, feature fusion, segmentation, CBCT\n1 Introduction\nThe mandibular canal (MC) is a tubular anatomical structure situated within the\nmandible and chie ﬂy houses the inferior alveolar nerve (IAN) and associated vasculature\n(Agbaje et al., 2017 ). This nerve shares a critical relationship with the third molar ( Rai et al.,\n2014). Any insult to the MC can lead to adverse outcomes such as patient discomfort, acute\npain, or even facial paralysis ( Al-Juboori et al., 2014 ). Therefore, precise segmentation of the\nMC from imaging modalities is instrumental for clinicians to appreciate the spatial\nOPEN ACCESS\nEDITED BY\nAdrian Elmi-Terander,\nStockholm Spine Center, Sweden\nREVIEWED BY\nShireen Y. Elhabian,\nThe University of Utah, United States\nXiaojun Chen,\nShanghai Jiao Tong University, China\n*CORRESPONDENCE\nWang Li,\nwang.l@cqut.edu.cn\n† These authors have contributed equally\nto this work\nRECEIVED 26 September 2023\nACCEPTED 01 November 2023\nPUBLISHED 17 November 2023\nCITATION\nLv J, Zhang L, Xu J, Li W, Li G and Zhou H\n(2023), Automatic segmentation of\nmandibular canal using transformer\nbased neural networks.\nFront. Bioeng. Biotechnol.11:1302524.\ndoi: 10.3389/fbioe.2023.1302524\nCOPYRIGHT\n© 2023 Lv, Zhang, Xu, Li, Li and Zhou. This\nis an open-access article distributed\nunder the terms of theCreative\nCommons Attribution License (CC BY).\nThe use, distribution or reproduction in\nother forums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nFrontiers inBioengineering and Biotechnology frontiersin.org01\nTYPE Original Research\nPUBLISHED 17 November 2023\nDOI 10.3389/fbioe.2023.1302524\nrelationship between the MC and adjacent anatomical landmarks,\nthereby minimizing the risk of iatrogenic nerve injury during\nsurgical interventions ( Li et al., 2022 ). Owing to the cumbersome\nand error-prone nature of manual delineation, automated\nsegmentation of the MC from radiological images has emerged as\na focal point in dental research ( Usman et al., 2022 ).\nWith the advent of advanced deep learning techniques, neural\nnetwork-based segmentation of oral structures has shown signi ﬁcant\nprogress ( Cui et al., 2022 ; Fontenele et al., 2023 ). However, the\nsegmentation of the mandibular canal still falls short when\ncompared to other anatomical structures. The primary challenges\nare multifaceted. First, the mandibular canal occupies a minute\nfraction of the overall CBCT image, which can lead the neural\nnetwork to prioritize the background over the target foreground.\nSecond, the low contrast of CBCT images makes it dif ﬁcult to\ndistinguish the mandibular canal from surrounding tissues, often\nresulting in blurred or indistinct boundaries. Traditional\nsegmentation approaches such as region growing, level set,\nthresholding, and model matching have proven insuf ﬁcient for\novercoming these obstacles ( Kainmueller et al., 2009 ; Abdolali\net al., 2017 ). U-Net-based architectures have exhibited excellent\nperformance across various domains since their introduction.\nNonetheless, they often lack the capability to provide holistic\ninformation, causing them to neglect the topological structure of\nthe mandibular canal during segmentation tasks ( Jaskari et al., 2020 ;\nLahoud et al., 2022 ). In recent years, Transformer-based encoder-\ndecoder frameworks [e.g., TransUNet ( Chen et al., 2021 ), UNETR\n(Hatamizadeh et al., 2022a ), UNETR++ ( Shaker et al., 2022 )] have\nemerged, demonstrating promising results ( Li et al., 2023 ). These\nTransformer-based methodologies utilize a global mechanism to\ncapture features over long distances, addressing the limitations of\nCNN-based networks. However, the existing Transformer-based\nsegmentation methods predominantly focus on larger organs, and\nthey still do not provide effective solutions for segmenting the\nmandibular canal, which has smaller voxel sizes.\nTo address the aforementioned challenges, we have enhanced\nthe Swin-UNetR model speci ﬁcally for mandibular canal\nsegmentation. We also incorporate a pixel-level feature fusion\nmodule to augment the model ’s capability to discern ﬁner details\nof the mandibular canal. To mitigate the severe class imbalance\nbetween the foreground and background, as well as the low contrast\nprevalent in mandibular canal data, we introduce a cropping\ntechnique grounded in mandibular foramen localization and a\ncontrast enhancement strategy based on Contrast-Limited\nAdaptive Histogram Equalization (CLAHE). Given the\ntopological continuity of the mandibular canal, we employ clDice\nas the model ’s loss function. Moreover, to improve model\nrobustness, we propose a straightforward yet effective deep label\nfusion technique that capitalizes on the sparse data in the dataset.\nOur main contributions can be summarized as follows:\n(1) We introduce an enhanced Transformer-based segmentation\nnetwork tailored for mandibular canal segmentation, offering a\nnovel avenue for accurate segmentation of this complex\nstructure.\n(2) We proposed a pixel-level feature fusion module to improve the\nmodel’s detail perception ability, and improve the model ’s\nsegmentation accuracy and convergence speed.\n(3) We introduce a cropping method that autonomously localizes\nthe mandibular and mental foramina, coupled with an image\ncontrast enhancement strategy, as preprocessing steps to\naddress the challenges of category imbalance and unclear\nmandibular canal boundaries. Furthermore, our depth\nexpansion technique is used to generate fused label datasets,\nenhancing the model ’s robustness.\nThe remainder of the paper is structured as follows: Section 2\nreviews related work in mandibular canal segmentation. Section 3\nprovides a comprehensive description of our proposed method.\nSection 4 discusses the materials and implementation details. In\nSection 5 , we present the results along with comparative analyses.\nSection 6 contains the analysis and discussion of our work. Finally,\nSection 7 concludes the paper.\n2 Related work\nIn the ﬁsrt chapter, we delineated the broader research context,\ncurrent state of the ﬁeld, and speci ﬁcally emphasized the importance\nand challenges associated with mandibular canal segmentation. In\nthe subsequent chapters, we will delve deeper into the historical\ndevelopment of various mandibular canal segmentation techniques.\nThese methods can be broadly categorized based on the underlying\ntechnology into traditional image processing techniques, CNN\n(Convolutional Neural Network)-based approaches, and\nTransformer-based segmentation methods.\n2.1 Traditional image processing-based\nsegmentation method\nTo address the clinical issue of solely relying on manual\nsegmentation of the mandibular canal by dental professionals,\nKainmueller et al. (2009) proposed an automated segmentation\ntechnique that combines the Dijkstra tracking algorithm with the\nStatistical Shape Model (SSM). This method successfully reduced the\naverage error to 1.0mm, achieving a level of automation.\nSubsequently, Kim et al. (2010) presented a segmentation\nstrategy that employs 3D panoramic volume rendering (VR) and\ntexture analysis. Their approach captured variations in the curvature\nof the mandibular canal using a line tracing algorithm. Furthermore,\nthreshold-based segmentation technologies have seen some\nadvancements. Moris et al. (2012) employed a thresholding\ntechnique to identify the mandibular and mental foramina and\nthen used template matching technology to recursively calculate the\noptimal path between them, leveraging strong prior knowledge to\nachieve effective segmentation results. Building on Mori et al. ’s\nwork, Onchis-Moaca et al. (2016) enhanced template matching\ntechnology by using the anisotropic generalized Hough transform\nof the Gabor ﬁlter, signiﬁcantly improving computational ef ﬁciency.\nHowever, these methods suffer from excessive reliance on prior\nknowledge and limited generalizability. On the other hand, to tackle\nthe low contrast of CBCT images, Abdolali et al. (2017) innovatively\nemployed low-rank matrix decomposition to enhance image quality,\nthereby increasing the visibility of the mandibular canal in the shape\nmodel. Similarly, Wei and Wang (2021) utilized windowing and\nFrontiers inBioengineering and Biotechnology frontiersin.org02\nLv et al. 10.3389/fbioe.2023.1302524\nK-means clustering algorithms for data enhancement to improve the\nmandibular canal ’s visibility and subsequently deployed two-\ndimensional linear tracking coupled with tetranomial ﬁtting for\nsegmentation. In summary, traditional segmentation methods either\ndepend excessively on prior knowledge or require signi ﬁcant manual\nintervention, leading to pronounced human-induced biases.\n2.2 CNN-based segmentation method\nIn recent years, CNN-based segmentation methods have\nachieved signi ﬁcant advancements in mandibular canal\nsegmentation. Jaskari et al. (2020) ﬁrst employed a Fully\nConvolutional Network (FCN) for this task, achieving a Dice\ncoefﬁcient of 0.57 and thus substantiating the ef ﬁcacy of CNN\napproaches in this domain. Following this, Kwak et al. (2020)\nutilized thresholding technology for rapid mandibular canal\nlocalization, converting the full-volume image into a 2D slice\nsequence. They then employed SegNet and 3D UNet models for\n2D slice-level and 3D volume-level segmentation, respectively.\nHowever, their approach did not adequately consider the\nstructural information of the mandibular canal. To address this\ngap, Widiasri et al. (2022) segmented 3D images into 2D slices and\nutilized the Dental-Yolo algorithm for feature detection. This\nmethod computed the dimensions between the alveolar bone and\nthe mandibular canal, allowing the model to acquire rich positional\ninformation. Additionally, to enhance segmentation accuracy and\nmitigate computational limitations, researchers have proposed\ngeneralized hierarchical frameworks ( Lahoud et al., 2022 ;\nVerhelst et al., 2021 ). For instance, Verhelst et al. (2021) initially\ndownsampled images to reduce resolution, retained only patches\nwith foreground classes, and employed a 3D UNet in conjunction\nwith the Marching Cubes algorithm for smoothing and\nsegmentation. However, this method necessitates some manual\ninput and struggles with samples that have indistinct mandibular\ncanal boundaries. To counteract the issue of blurred boundaries,\nFaradhilla et al. (2021) introduced a Double Auxiliary Loss (DAL) in\nthe loss function to make the network more attentive to the target\narea and its boundaries, achieving a high Dice accuracy of 0.914 on\ntheir private dataset. To combat class imbalance, Du et al. (2022)\ninnovatively introduced a pre-processing step involving centerline\nextraction and region growing to identify the mandibular canal ’s\nlocation. They used a ﬁxed point as a reference to crop a localized\nregion around the mandibular canal, thereby minimizing the impact\nof background samples. Despite the successes of these methods, they\ngenerally sacriﬁce rich global information during training, leading to\na loss of structural integrity in the segmented mandibular canal.\n2.3 Transformer-based segmentation\nmethod\nIn the realm of medical imaging, Transformer-based techniques\nhave garnered considerable attention, ﬁnding applications across a\nrange of tasks including segmentation, recognition, detection,\nregistration, reconstruction, and enhancement ( Li et al., 2023 ;\nDosovitskiy et al., 2020 ). One key advantage of the Transformer\narchitecture over Convolutional Neural Networks (CNNs) is its\nrobust capability for global perception, allowing for a more effective\nunderstanding of global contextual information and capturing long-\nrange dependencies. Many Transformer-based approaches have\nbeen adapted for segmentation tasks involving major human\norgans, and have yielded impressive results ( Liu and Shen, 2022 ;\nPan et al., 2022 ). For instance, Wang et al. (2021) introduced the\nUCTransNet model, which for the ﬁrst time incorporated the\nTransformer into the channel dimension. By leveraging feature\nfusion and multi-scale channel attention, the model optimized\nthe information integration between low- and high-dimensional\nspaces (Chen et al., 2021 ). Hatamizadeh et al. (2022a) then proposed\nthe UNETR model, which employed the Vision Transformer (ViT)\nas the encoding layer. This model leveraged the Transformer ’s\nstrong global modeling capabilities to achieve excellent\nperformance on multi-organ segmentation datasets. To address\nthe UNETR model ’s relatively weaker performance in capturing\nlocal details, Hatamizadeh et al. (2022b) introduced the Swin\nUNETR segmentation model. This variant ensured a global\nreceptive ﬁeld while also giving ample consideration to local\ndetails, and it has shown promising results in tasks such as brain\ntumor segmentation. Speci ﬁcally in the context of mandibular canal\nsegmentation, Jeoun et al. (2022) introduced the Canal-Net, a\ncontinuity-aware context network designed to help the model\nunderstand the spatial structure of the mandibular canal. This\napproach achieved a Dice coef ﬁcient of up to 0.87. These\noutcomes provide compelling evidence to suggest that the\nTransformer’s strong context-aware capabilities could be\nparticularly effective for mandibular canal segmentation tasks.\nHowever, it is worth noting that research in Transformer-based\nmandibular canal segmentation is still in its nascent stages.\nRecognizing the unique challenges and characteristics of\nmandibular canal segmentation, we sought to improve upon the\nSwin UNETR model. Our modi ﬁed approach has yielded promising\nsegmentation results, underscoring the potential utility of\nTransformer-based architectures in this domain.\n3 Methods\n3.1 Data preprocessing\nConsidering the impact of preprocessing on model performance,\nwe employ a comprehensive set of preprocessing steps to address\nexisting challenges in CBCT imagery and thereby enhance the\nsegmentation accuracy of the mandibular canal. The speci ﬁc\nprocess is shown in Figure 1 . The rectangular box in Figure 1\nhighlights the changes in the mandibular canal.\n3.1.1 Volume cropping\nThe proportion of voxels representing the mandibular canal in\nthe entire CBCT image is exceedingly small, exacerbating the class\nimbalance between foreground and background. This imbalance\nadversely affects the model ’s segmentation performance, as\nillustrated in Figure 1A . To address this challenge, we introduce\na cropping technique based on the localization of the mandibular\nforamen. This approach aims to identify the largest connected\ndomain of the mandibular canal by locating the jaw foramen, as\ndepicted in Figure 1B . This method locates the positional\nFrontiers inBioengineering and Biotechnology frontiersin.org03\nLv et al. 10.3389/fbioe.2023.1302524\nrelationship between the mandibular foramen and chin foramen in\nthe labeled information, and then maps this positional relationship\nto the original image for cropping. Following this preprocessing step,\nthe total number of voxels is reduced by approximately 60%. This\nreduction not only enhances the model ’s convergence speed and\nsegmentation accuracy but also minimizes hardware resource\nconsumption.\n3.1.2 Contrast enhancement\nIn CBCT imaging, the gray values of the mandibular canal and\nsurrounding tissues are often similar, which obscures the boundary\nof the mandibular canal. Further complicating the matter, some CT\ndevices may produce images with low resolution and blurriness,\nmaking it dif ﬁcult to differentiate the mandibular canal from\nadjacent structures. To overcome these challenges, we employ\nContrast-Limited Adaptive Histogram Equalization (CLAHE) to\nenhance image contrast, thereby improving the model ’s\nsegmentation performance. This enhancement is demonstrated in\nFigure 1C .\n3.2 Mandibular canal segmentation network\nstructure\nWe employ the aforementioned preprocessing techniques on the\nCBCT images and use them as input for the segmentation network.\nIn the encoder portion of the network, a 4-layer Swin Transformer\nserves as the feature extractor. This architecture leverages the\nTransformer’s robust capability for global modeling, allowing it\nto focus more effectively on the overall structural features of the\nmandibular canal, compared to traditional CNN-based feature\nextractors. The decoder part of the network adheres to the\nconventional U-Net decoding structure. In this design features\nextracted by the encoder are connected to the decoder via skip\nconnections at each scale. At each stage of the encoder i the output\nfeatures are reshaped to size\nH\n2i × W\n2i × D\n2i, which are then fed into a\nresidual module consisting of two 3 × 3 × 3 convolutional layers.\nSubsequently, the feature map is upsampled by 2 times using a\ndeconvolution layer, and is concatenated with the output of the\nprevious layer and fed into the residual module. Finally, the output\nof the residual module is sent to the DRC module to achieve pixel-\nlevel feature fusion with the previous layer features. The ﬁnal\nsegmentation result is calculated by using a 1 × 1 ×\n1 convolutional layer and a sigmoid activation function. It\nrestores the spatial dimensions of the feature map through a\nseries of ﬁve upsampling operations, as shown in Figure 2 .\nTo further improve the network ’s ability to perceive the details\nof the mandibular canal, we introduce a feature fusion strategy of\nelement-by-element addition and use the DRC (Deep Residual\nConvolution) module for each decoding layer to further extract\nfeatures, as shown in Figure 3B . Comparing with traditional CNN\nstructure, as shown in Figure 3A, this module is mainly composed of\ntwo branches: the ﬁrst branch consists of a 1 × 1 convolution, the\nsecond branch consists of two 1 × 1 and a 3 × 3 convolution, and to\nimprove the expressiveness of the convolution, we perform\nnormalization and ReLU activation operations after each\nconvolution operation. The output of the DRC module can be\nexpressed as:\nDRC /equals FX , Y()\nL + FX , Y() R (1)\namong them, X represents the input data, F (X, Y)L represents the\noutput of the left branch, and F (X, Y)R represents the output of the\nright branch. The extracted features are fused layer by layer at the\npixel level to obtain the fused feature map F (x, y):\nFx , y() /equals Fn x, y() + DRC Fn−1 x, y()() (2)\nwhere F(x, y) represents the pixel position in the feature map, and n\nrepresents the nth decoder layer. Through this fusion strategy, the\nmodel can learn more information from different feature map.\n3.3 Deep Label fusion\nTo optimally leverage our set of 256 sparsely labeled data, we\nintroduce an innovative approach for pseudo-label generation.\nInitially, the model is trained using densely annotated data, after\nwhich it generates pseudo-labels for the 256 sparsely annotated\nsamples. Compared to the original sparse labels, these pseudo-labels\noffer a richer semantic context but may lack adequate connectivity.\nTo address this limitation, we implement an intelligent label fusion\nFIGURE 1\nOur proposed preprocessing process,(A) represents the original image,(B) represents the cropped image, and(C) represents the contrast-\nenhanced image.\nFrontiers inBioengineering and Biotechnology frontiersin.org04\nLv et al. 10.3389/fbioe.2023.1302524\nalgorithm. This method ﬁrst integrates instance level features of\ncircular extended labels and newly generated pseudo labels through\ninterpolation. More speci ﬁcally, the pseudo-labels contribute\nvaluable semantic insights, while the circular extension labels\nprovide precise boundary delineation. We have coined this\nmethod “Deep Label Fusion, ” and employ it to create an\naugmented dataset for this study. Utilizing this extended dataset\nfor pre-training the prediction model led to a notable improvement\nin the Dice metric, particularly when compared to the performance\nachieved with the original set of 256 circularly extended labels.\n3.4 Loss function analysis\nThe primary objective of the loss function in medical image\nsegmentation tasks is to quantif y the discrepancy between the\npredicted segmentation outcomes and the ground-truth labels. Given\nthat the mandibular canal is a tubular structure, its connectivity is a\ncrucial consideration. In 2021, Shit et al. introduced a loss function\ndesigned to take into account both vessel topology and connectivity,\nknown as centerline Dice (clDice). This function is computed based on\nthe intersection between the segmentation output and the extracted\ncartilage scaffold. Importantly, clDice is adept at evaluating the\nconnectivity of tubular anatomic al features. In our research, we\nemploy clDice as the loss function for training the network. The\nexpression for the clDice function is as follows:\nFIGURE 2\nThe network diagram used in this article consists of a Transformer encoding module, a decoding module, and a feature fusion module. In addition,\nthe model accepts three types of labels: sparse label, dense label, and deep fusion label.\nFIGURE 3\nDRC module structure diagram, where(A) represents the\ntraditional convolution module(B) represents our proposed DRC\nmodule.\nFrontiers inBioengineering and Biotechnology frontiersin.org05\nLv et al. 10.3389/fbioe.2023.1302524\nTp SP,V L() /equals SP ∩ VL||\nSP|| , (3)\nTs SL,V P() /equals SL ∩ VP||\nSL|| , (4)\nLclDice /equals− 2× Tp SP,V L() × Ts SL,V P()\nTp SP,V L() × Ts SL,V P() , (5)\namong them, VL and VP refer to the predicted results and real labels\nof network segmentation, respectively, SP and SL refer to the soft\nskeleton extracted from VL and VP, respectively, Tp(SP,V L) refers to\nthe topological accuracy, Ts(SL,V P) is the topological sensitivity.\nLclDice is the harmonic mean of the above two metrics to focus on\nobject connectivity. The total loss function Ltotal combines Dice Loss\nand clDice Loss, the formula is as follows:\nLtotal /equals 1 − λ() LDice + λLclDice, (6)\nwhere λ is a scaling factor.\n4 Data and implementation details\n4.1 Data\nThe CBCT dataset utilized in this study was supplied by\nCipriano et al. (2022b) and exists in two versions: old and new.\nThe old dataset comprises 91 3D densely annotated primary datasets\nand 256 2D sparsely annotated auxiliary datasets. This primary\ndataset is further divided into 68 training sets, 8 validation sets, and\n15 test sets. The spatial resolutions of these CBCT scans range from\n148 × 272 × 334 to 171 × 423 × 462, featuring a voxel size of 0.3 ×\n0.3 × 0.3 mm³. Conversely, the new dataset consists of 153 densely\nannotated primary datasets and 290 sparsely annotated auxiliary\ndatasets. The spatial resolution in this new version ranges from\n148 × 265 × 312 to 178 × 423 × 463. Additionally, the training set in\nthis new version has been expanded to include 130 datasets. To\nTABLE 1 Comparison of results of different segmentation methods.\nTest Methods Training set HD IoU clDice # Dice\n1 Jaskari et al. (2020) Cir. Exp. — 0.39 — 0.56\n2 Ours Cir. Exp. 7.844 0.405 0.845 0.573\n3 Cipriano et al., (2022a) 3D Ann. — 0.61 — 0.75\n4 Usman et al., (2022) 3D Ann. — 0.79 — 0.77\n5 3D UNet 3D Ann. 16.048 0.558 0.809 0.709\n6 nn-UNet 3D Ann. 6.363 0.665 0.935 0.796\n7 UNetR 3D Ann. 8.027 0.569 0.823 0.722\n8 Swin-Unet (Cao et al.) 3D Ann. 7.072 0.482 0.733 0.640\n9 Ours 3D Ann. 5.002 0.692 0.933 0.815\nBold represents the optimal result. # is the measurement standard for tubular structure proposed by Shit et al. (2021) .\nFIGURE 4\nComparison of visualization results of different segmentation methods.\nTABLE 2 Quantitative analysis results of different preprocessing methods on\nmodel performance.\nInput images HD95 (mm) IoU clDice Dice\nOriginal 6.355 0.656 0.912 0.788\nVolume Cutting 5.008 0.684 0.927 0.810\nContrast Enhancement 5.000 0.692 0.933 0.815\nBold values are reports the optimal result.\nFrontiers inBioengineering and Biotechnology frontiersin.org06\nLv et al. 10.3389/fbioe.2023.1302524\nmaintain a fair and rigorous comparison with other studies, all\ncomparative analyses were conducted using the old dataset.\nMoreover, to demonstrate the cutting-edge nature of our\nresearch, we also conducted veri ﬁcations using the new dataset\nversion.\n4.2 Experimental details\nOur experiments are implemented using NVIDIA Tesla\nV100S in the PyTorch and MONAI deep learning libraries.\nDuring the preprocessing, we processed the raw data of ﬂine by\nthe proposed jaw-foramen localization-based volume cropping\nmethod and image contrast enha ncement method. During the\ntraining process, Diceloss and clDiceloss are used as the loss\nfunction of the model, the Ad am optimizer with momentum\n(μ = 0.99) is used, the initial learning rate is set to 0.0001, and\nthe learning rate is automatically adjusted using cosine annealing,\nand the batch size is set to 1, and the number of iterations of the\nmodel is uniformly set to 500. In our experiments, to reduce\nmemory usage, we use a 96 × 96 × 96 sliding window with a stride\nof 48 to crop the original CBCT image, and then feed the cropped\nimage into the network for training . After outputting the predicted\npatch, we restore the output pred icted patch to the original image\nsize by stitching.\n4.3 Evaluation indicators\nIn the test phase, we use four commonly used evaluation\nindicators for segmentation tasks to evaluate the performance of\nthe model: Dice coef ﬁcient (Dice), Intersection Over Union (IOU),\nHausdorff distance (HD):\n4.3.1 Dice coefﬁcient (Dice)\nThe Dice coef ﬁcient is a set similarity measurement function,\nwhich is usually used to calculate the similarity between two\nsamples, and the value range is [0, 1].\nDice /equals\n2TP\nFP+2TP + FN (7)\n4.3.2 Intersection over union (IOU)\nThe IOU indicator calculates the overlap rate of predicted\nresults and real results, that is, t he ratio of their intersection and\nunion.\nIOU /equals A ∩ B\nA ∪ B (8)\n4.3.3 Hausdorff distance (HD)\nThe HD indicator is a metric used to measure the similarity or\ndifference between two sets.\nHA , B() /equals max hA , B() , h B, A(){} (9)\nwhere TP represents true positives, TN represents true negatives, FP\nrepresents false positives, FN represents false negatives, A represents\nthe set of true labels, and B represents the set of predicted\nsegmentations.\n5 Results\n5.1 Evaluation of result\nTo prove the effectiveness of our proposed mandibular canal\nsegmentation method, we conducted a performance evaluation. The\nspeciﬁc results are as follows: only trained on 91 dense data, average\nDice = 0.815, average IoU = 0.69, a verage clDice = 0.93, the average\nHD95 = 5 mm, all evaluation indicato r sh a v ep r o v e dt h ee x c e l l e n c eo f\nour proposed mandibular canal segmentation method. In addition, to\nprove the advanced nature of our proposed method, we also compared\na n da n a l y z e di tw i t he x i s t i n gm e t h o d s ,a n dt h es p e c iﬁc comparison\nresults are shown inTable 1. From the comparison results, it can be seen\nthat the improvement of the segmentation method we proposed is very\nsigniﬁcant. Compared with the most advanced method that also uses\nFIGURE 5\nVisual segmentation results obtained by training the network with different loss functions.\nFrontiers inBioengineering and Biotechnology frontiersin.org07\nLv et al. 10.3389/fbioe.2023.1302524\n91 densely labeled data, our Dice index has increased by 4.5%. In\naddition, using 256 sparse data for pre-training and our proposed deep\nlabel fusion strategy for training, the Dice index reached 0.824 and\n0.840, respectively. The speciﬁc visual comparison results are shown in\nFigure 4.\n5.2 Ablation experiment\n5.2.1 Preprocessing\nIn Section 3.2 , we proposed two data preprocessing methods,\nnamely, the cropping method based on jaw hole positioning and the\ncontrast enhancement method based on Contrast-Limited Adaptive\nHistogram Equalization. Figure 4 shows our proposed preprocessing\nmethod in detail. We have analyzed the effectiveness of the two\nproposed methods, and the speci ﬁc results are shown in Table 2 .I t\ncan be seen from the table that the Dice index has increased by 2.7%\nafter data preprocessing.\n5.2.2 Feature fusion\nTo deeply study the impact of our proposed feature fusion\nstrategy on the performance of mandibular canal segmentation,\nwe conduct a series of ablation experiments and summarize the\nexperimental results in Table 3 . The results show that the feature\nfusion strategy plays an important role in the mandibular canal\nsegmentation task. The Dice coef ﬁcient using this feature fusion\nstrategy is 0.788, which is signi ﬁcantly improved compared to the\ncase where this module is not used. In addition, we conduct a\ncomparative analysis of t he proposed DRC module and\ntraditional convolution. This design can effectively enhance\nthe representation ability of the model and help to further\noptimize our proposed feature fusion strategy to improve\nsegmentation performance.\n5.2.3 Loss function analysis\nIn our work, we use clDice Loss as the loss function to train the\nnetwork. In order to prove that clDice Loss is helpful in improving\nthe mandibular canal segmentation effect, we compared clDice Loss\nwith Cross-Entropy (CE) Loss and Dice Loss. The speci ﬁc results are\nshown in Table 4 . We found that using clDice Loss as the loss\nfunction achieved the optimal Dice coef ﬁcient of 0.815. In addition,\nwe also compared the impact of different hyperparameters λ in\nclDice Loss on model segmentation performance. The speci ﬁc visual\ncomparison results are shown in Figure 5 . The broken part of the\nmandibular canal in the segmentation result is marked with arrows.\nFrom the ﬁgure, it can be clearly seen that the segmentation result\nhas the best connectivity when λ = 0.1.\n5.2.4 Deep label fusion\nIn Section 3.3 , we proposed a deep label fusion method, which\ngenerated 256 fused labels on a sparse dataset, forming a new deep\nfusion dataset. This deep fusion dataset contains richer semantic\ninformation compared to sparse datasets, which can better guide\nmodel training. To verify the effectiveness of this method, we\nconducted a performance evaluation, and the speci ﬁc evaluation\nresults are shown in Table 5 . From the table, it can be seen that the\nDice index has been signi ﬁcantly improved when using our\nproposed deep fusion label for pre training. When using 3D Ann\ndata for training, using our method for pre training is 1.6% higher\nthan using circular extended data for pre training Dice index.\n6 Discussion\nIn this study, we propose a Transformer-based method for\nautomatic segmentation of the mandibular canal, which is\ncapable of simultaneously focusing on local ﬁne-grained details\nand global semantic information of the mandibular canal to\nsegment the mandibular canal with highly consistent accuracy\nacross the entire CBCT image. We validate the method on the\nlargest mandibular canal segmentation dataset, and the evaluation\nindex Dice coef ﬁcient exceeds previous research methods.\nDue to the low contrast in CBCT images and the close similarity\nin grayscale values between the mandibular canal and surrounding\ntissues, neural networks face dif ﬁculties in effectively distinguishing\nthe boundaries of the mandibular canal ( Waltrick et al., 2013 ).\nFurthermore, the mandibular canal constitutes only a minute\nfraction of the total CBCT image volume, leading to a\npronounced class imbalance between foreground and\nbackground. This imbalance causes the network to\ndisproportionately focus on background features ( Dai et al.,\n2023). To address these challenges, we employ two pre-\nTABLE 3 Quantitative results of different feature fusion methods.\nTest HD95 (mm) IoU clDice Dice\nBaseline 10.320 0.629 0.92 0.769\nBaseline+C 9.550 0.642 0.894 0.777\nBaseline+DRC 6.355 0.656 0.912 0.788\nAmong them, C means to use the traditional convolution module, and DRC means to use the deep residual convolution module. Bold values are reports the op timal result.\nTABLE 4 Quantitative results of training models with different loss functions.\nLoss function HD95 (mm) IoU clDice Dice\nCross-Entropy (CE) 8.207 0.660 0.907 0.790\nDice 7.828 0.665 0.888 0.795\nclDice ( λ/equals 0.5) 9.392 0.656 0.909 0.787\nclDice ( λ/equals 0.4) 6.958 0.680 0.927 0.806\nclDice ( λ/equals 0.3) 9.550 0.660 0.907 0.789\nclDice ( λ/equals 0.2) 5.005 0.681 0.927 0.806\nclDice ( λ/equals 0.1) 5.002 0.692 0.933 0.815\nBold values are reports the optimal result.\nFrontiers inBioengineering and Biotechnology frontiersin.org08\nLv et al. 10.3389/fbioe.2023.1302524\nprocessing techniques aimed at mitigating issues related to blurred\nboundaries and class imbalances: a cropping method for automated\nlocalization of the mandibular and mental foramina, and Contrast-\nLimited Adaptive Histogram Equalization (CLAHE) for image\ncontrast enhancement. By eliminating extraneous information\nand enhancing the contrast between the mandibular canal and its\nsurrounding tissue, these techniques facilitate precise localization\nand segmentation of the mandibular canal. The ef ﬁcacy of this\ncontrast enhancement approach has also been successfully validated\nin two-dimensional microscopic images as per Wu et al. (2022) .A s\ndemonstrated in Section 5 , the implementation of these two\npreprocessing methods results in an approximate 4%\nimprovement in Dice coef ﬁcient performance.\nSecondly, to maintain the co nnectivity of the segmented\nmandibular canal, we incorporat ed the Transformer architecture\ninto the segmentation task. This enables the network to learn both\nthe local ﬁne-grained details and the g lobal semantic information\npertinent to the mandibular canal ( Liu et al., 2021 ). Given the small\nvolumetric proportion of the mandibular canal in the CBCT images, we\nintroduced a pixel-level feature fusion strategy to augment the network’s\nsegmentation performance. The d eployment of the Deep Residual\nConvolution (DRC) module fu rther enriches the network ’s\nperception of intricate details. Pr evious studies have substantiated\nthe ef ﬁcacy of feature fusion strategies in enhancing the\nsegmentation performance for small and indistinct targets ( Dai\net al., 2023 ). Our empirical tests show that the feature fusion\nmodule not only improves segmentation performance but also\naccelerates model convergence, reducing training time by as much\nas 50%. This acceleration is likely attributed to the enhanced perceptual\ncapabilities conferred by the module , partially ameliorating the slow\nconvergence typically associated with Transformer models. Regarding\nthe loss function, we employed the centerline Dice (clDice) loss function\nto better account for the tubular topology of the mandibular canal (Shit\net al., 2021). As evidenced in Section 5,t h e r ew a sa1 %i n c r e a s ei nt h e\nDice coef ﬁcient, corroborating the effe ctiveness of this method in\nimproving the segmentation of tubular structures. Figure 5 clearly\ndemonstrates enhanced connectivity in the segmentation results, an\noutcome of clDice loss function’s calculation of connectivity disparities\nbetween segmented outcomes and the extracted cartilage scaffolding.\nThis quanti ﬁcation allows the network to focus more on ensuring\nconnectivity in the segmenta tion results, thereby signi ﬁcantly\nenhancing the morphological integrity of the mandibular canal ’s\ntubular structure. Similar ﬁndings are reported in Huang et al.\n(2022) and Pan et al. (2021) . Additionally, we leveraged sparse\nexisting data to generate an augmented dataset through our\nproposed Deep Label Fusion technique. Compared to pre-training\non the circle-extension dataset, our method resulted in a 1.6%\nincrease in the Dice coef ﬁcient, reaching a score of 0.840. When\nvalidated on the new version of the ToothFairy dataset, the Dice\ncoefﬁcient was further improved to 0.844.\nIn the segmentation performance of CBCT images, our method\nachieved the highest performance on the public mandibular canal\ndataset ( Cipriano et al., 2022b ). Although this research work\nachieved the best segmentation results overall, there are still certain\nlimitations. First of all, compared with the CNN network, the\nconvergence speed of this network needs to be improved. Secondly,\nbecause the pixel changes of the mandibular canal at the mandibular\nand mental foramen are not obvious, the segmentation effect of the\nmandibular canal at the head and tail of the foramen is poor. Therefore,\nwe will focus on the ﬁrst and last features in future research to further\nimprove the accuracy of the model.\n7 Conclusion\nIn this study, we introduce a Transformer-based method for the\nrobust segmentation of the mandibular canal. Our approach adeptly\naddresses key challenges, including morphological preservation of the\nmandibular canal, class imbalance, and ambiguous boundaries,\nsubsequently achieving substantial improvements in segmentation\nmetrics. Firstly, we employ Contrast-Limited Adaptive Histogram\nEqualization (CLAHE) to enhance image contrast, substantially\nameliorating the low-contrast issues inherent to original CBCT\nscans. This step results in a notable increase in the model ’s\nsegmentation accuracy. Secondly, we implement an image\ncropping strategy founded on mandibular foramen localization.\nThis alleviates the class imbalance issue and substantially reduces\nextraneous background information, streamlining the segmentation\nprocess. Further, we introduce a specialized pixel-level feature fusion\nmodule known as the Deep Residual Convolution (DRC). This\nmodule not only ampli ﬁes the model ’s sensitivity to ﬁne details in\nsmaller targets such as the mandibular canal but also accelerates the\nconvergence speed of the model, partially mitigating the known slow-\nconvergence issue associated with Transformer architectures. To\nimprove the topological integrity of the segmented mandibular\ncanal, we utilize the centerline Dice (clDice) loss function. This\nforces the network to concentrate on maintaining the connectivity\nof the segmented structures, enhancing the morphological integrity of\nthe mandibular canal. Lastly, we deploy a Deep Label Fusion\ntechnique to mine further information from the original, sparsely-\nannotated dataset. This step signi ﬁcantly bolsters the model ’s\nsegmentation performance. Our method was rigorously evaluated\non a publicly available mandibular canal dataset. The empirical results\ndemonstrate that our proposed segmentation approach outperforms\nTABLE 5 Analysis of training results using different labels.\nTest Pre-training set Training set HD IoU clDice Dice\n1 — 3D Ann. 5.002 0.692 0.933 0.815\n2 Cir. Exp. 3D Ann. 4.061 0.704 0.947 0.824\n3 Deep fusion 3D Ann. 3.213 0.727 0.960 0.840\n4 Deep fusion ToothFairy 2.947 0.731 0.961 0.844\nNotes: 3D Ann indicates densely labeled data, Cir. Exp. represents circle expansion data, Deep fusion represents synthetic data sets, and ToothFair y represents new version data sets. Bold values\nare reports the optimal result.\nFrontiers inBioengineering and Biotechnology frontiersin.org09\nLv et al. 10.3389/fbioe.2023.1302524\nexisting methods, underscoring its strong potential for application in\nthe domain of mandibular canal segmentation.\nData availability statement\nThe original contributions presented in the study are included in\nthe article/supplementary material, further inquiries can be directed\nto the corresponding author.\nAuthor contributions\nJL: Writing –original draft. LZ: Writing –original draft. JX:\nWriting –review and editing. WL: Writing –review and editing.\nGL: Writing –review and editing. HZ: Writing –review and\nediting.\nFunding\nThe authors declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. This work\nwas supported by the Science and Technology Research Program of\nChongqing Municipal Education Commission (Grant No.\nKJQN202201164), the Chongqing University of Technology\nResearch and Innovation Team Cultivation Program (Grant No.\n2023TDZ012), and Chongqing Graduate Student Research\nInnovation Project (Grant No. CYS23698).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbdolali, F., Zoroo ﬁ, R. A., Abdolali, M., Yokota, F., Otake, Y., and Sato, Y. (2017).\nAutomatic segmentation of mandibular canal in cone beam CT images using\nconditional statistical shape model and fast marching. Int. J. Comput. assisted\nradiology Surg. 12, 581 –593. doi:10.1007/s11548-016-1484-2\nAgbaje, J. O., de Casteele, E. V., Salem, A. S., Anumendem, D., Lambrichts, I., and\nPolitis, C. (2017). Tracking of the inferior alveolar nerve: its implication in surgical\nplanning. Clin. Oral Investig.21, 2213 –2220. doi:10.1007/s00784-016-2014-x\nAl-Juboori, M., Al-Wakeel, H., SuWen, F., and Yun, C. M. (2014). Mental foramen\nlocation and its implication in dental treatment plan. World J. Med. Med. Sci. Res.2 (3),\n35–42.\nCao, H., et al. Swin-unet: unet-like pure transformer for medical image\nsegmentation.” Available: http://arxiv.org/abs/2105.05537.\nChen, J., et al. (2021). Transunet: transformers make strong encoders for medical\nimage segmentation. arXiv preprint arXiv:2102.04306.\nCipriano, M., Allegretti, S., Bolelli, F., Di Bartolomeo, M., Pollastri, F., Pellacani, A.,\net al. (2022b). Deep segmentation of the mandibular canal: a new 3D annotated dataset\nof CBCT volumes. IEEE Access 10, 11500 –11510. doi:10.1109/ACCESS.2022.3144840\nCipriano, M., Allegretti, S., Bolelli, F., Pollastri, F., and Grana, C. (2022a). “Improving\nsegmentation of the inferior alveolar nerve through deep label propagation, ” in\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition,China, IEEE, 21137 –21146.\nCui, Z., Fang, Y., Mei, L., Zhang, B., Yu, B., Liu, J., et al. (2022). A fully automatic AI\nsystem for tooth and alveolar bone segmentation from cone-beam CT images. Nat.\nCommun. 13 (1), 2096. doi:10.1038/s41467-022-29637-2\nCui, Z., Li, C., and Wang, W. (2019). ToothNet: automatic tooth instance\nsegmentation and identi ﬁcation from cone beam CT images IEEE/CVF conference\non computer vision and pattern recognition (CVPR). Long Beach, CA, USA: IEEE,\n6361–6370. doi:10.1109/CVPR.2019.00653\nDai, S., Zhu, Y., Jiang, X., Yu, F., Lin, J., Yang, D., et al. (2023). TD-Net: trans-\nDeformer network for automatic pancreas segmentation. Neurocomputing 517,\n279–293. doi:10.1016/j.neucom.2022.10.060\nDosovitskiy, A., et al. (2020). An image is worth 16x16 words: transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929.\nDu, G., Tian, X., and Song, Y. (2022). Mandibular canal segmentation from CBCT\nimage using 3D convolutional neural network with scSE attention. IEEE Access 10,\n111272–111283. doi:10.1109/access.2022.3213839\nFaradhilla, Y., Ari ﬁn, A. Z., Suciati, N., Astuti, E. R., Indraswari, R., and Widiasri, M.\n(2021). Residual fully convolutional network for mandibular canal segmentation. Int.\nJ. Intelligent Eng. Syst.14 (6), 208 –219. doi:10.22266/ijies2021.1231.20\nFontenele, R. C., Gerhardt, M. d. N., Picoli, F. F., Van Gerven, A., Nomidis, S.,\nWillems, H., et al. (2023). Convolutional neural network-based automated maxillary\nalveolar bone segmentation on cone-beam computed tomography images. Clin. Oral\nImplants Res. P. Clr. 34, 565 –574. doi:10.1111/clr.14063\nHatamizadeh, A., et al. (2022a). “Unetr: transformers for 3d medical image\nsegmentation,” in Proceedings of the IEEE/CVF winter conference on applications of\ncomputer vision, USA, IEEE, 574 –584.\nHatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H., Xu, D., and Swin, UNETR:\nSwin transformers for semantic segmentation of brain tumors in MRI images, 2022b.\nAvailable: http://arxiv.org/abs/2201.01266\nHuang, S., Li, J., Xiao, Y., Shen, N., and Xu, T. (2022). RTNet: relation transformer\nnetwork for diabetic retinopathy multi-lesion segmentation. IEEE Trans. Med. Imaging\n41 (6), 1596 –1607. doi:10.1109/TMI.2022.3143833\nJaskari, J., Sahlsten, J., Järnstedt, J., Mehtonen, H., Karhu, K., Sundqvist, O., et al.\n(2020). Deep learning method for mandibular canal segmentation in dental cone beam\ncomputed tomography volumes. Sci. Rep.10 (1), 5842. doi:10.1038/s41598-020-62321-3\nJeoun, B.-S., Yang, S., Lee, S. J., Kim, T. I., Kim, J. M., Kim, J. E., et al. (2022). Canal-\nNet for automatic and robust 3D segmentation of mandibular canals in CBCT images\nusing a continuity-aware contextual network. Sci. Rep. 12 (1), 13460. doi:10.1038/\ns41598-022-17341-6\nKainmueller, D., Lamecker, H., Seim, H., Zinser, M., and Zachow, S. (2009).\n“Automatic extraction of mandibular nerve and bone from cone-beam CT data, ” in\nMedical image computing and computer-assisted intervention –MICCAI 2009: 12th\ninternational conference September 20-24, 2009, (London, UK: Springer), 76 –83.\nKim, G., Lee, J., Lee, H., Seo, J., Koo, Y. M., Shin, Y. G., et al. (2010). Automatic\nextraction of inferior alveolar nerve canal using feature-enhancing panoramic volume\nrendering. IEEE Trans. Biomed. Eng.58 (2), 253 –264. doi:10.1109/TBME.2010.2089053\nKwak, G. H., Song, J. M., Park, H. R., Jung, Y. H., Cho, B. H., et al. (2020). Automatic\nmandibular canal detection using a deep convolutional neural network. Sci. Rep.10 (1),\n5711. doi:10.1038/s41598-020-62586-8\nLahoud, P., Diels, S., Niclaes, L., Van Aelst, S., Willems, H., Van Gerven, A., et al.\n(2022). Development and validation of a novel arti ﬁcial intelligence driven tool for\naccurate mandibular canal segmentation on CBCT. J. Dent.116, 103891. doi:10.1016/j.\njdent.2021.103891\nLi, J., Chen, J., Tang, Y., Wang, C., Landman, B. A., and Zhou, S. K. (2023).\nTransforming medical imaging with Transformers? A comparative review of key\nproperties, current progresses, and future perspectives . China, Medical image\nanalysis.102762\nLi, Y., et al. (2022). Association of the inferior alveolar nerve position and nerve injury:\na systematic review and meta-analysis. Germany, Healthcare, MDPI, 1782.\nFrontiers inBioengineering and Biotechnology frontiersin.org10\nLv et al. 10.3389/fbioe.2023.1302524\nLiu, Z., et al. (2021). Swin transformer: hierarchical vision transformer using shifted\nwindo. Available at: http://arxiv.org/abs/2103.14030.\nLiu, Z., and Shen, L. (2022). Medical image analysis based on transformer: a review.\narXiv preprint arXiv:2208.06643.\nMoris, B., Claesen, L., Sun, Y., and Politis, C. (2012). Automated tracking of the\nmandibular canal in CBCT images using matching and multiple hypotheses methods\nFourth international conference on communications and Electronics (ICCE). Germany,\nIEEE, 327 –332.\nOnchis-Moaca, D., Zappalá, S., Go ţia, S. L., Real, P., and Pricop, M. (2016). Detection\nof the mandibular canal in orthopantomography using a Gabor- ﬁltered anisotropic\ngeneralized Hough transform. Pattern Recognit. Lett. 83, 85 –90. doi:10.1016/j.patrec.\n2015.12.001\nPan, L., Zhang, Z., Zheng, S., and Huang, L. (2021). MSC-net: multitask learning\nnetwork for retinal vessel segmentation and centerline extraction. Appl. Sci.12 (1), 403.\ndoi:10.3390/app12010403\nPan, S., et al. (2022). “CVT-Vnet: convolutional-transformer model for head and neck\nmulti-organ segmentation, ” in Medical imaging 2022: computer-aided diagnosisChina,\n(SPIE), 914 –921.\nRai, R., Shrestha, S., and Jha, S. (2014). Mental foramen: a morphological and\nmorphometrical study. Int. J. Health Biomed. Res.2, 144 –150.\nShaker, A., Maaz, M., Rasheed, H., Khan, S., Yang, M.-H., and Khan, F. S. (2022).\nUNETR++: delving into efﬁcient and accurate 3D medical image segmentation. arXiv\npreprint arXiv:2212.04497.\nShit, S., et al. (2021). “clDice-a novel topology-preserving loss function for tubular\nstructure segmentation, ” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, IEEE, China, 16560 –16569.\nUsman, M., Rehman, A., Saleem, A. M., Jawaid, R., Byon, S. S., Kim, S. H., et al. (2022).\nDual-stage deeply supervised attention-based convolutional neural networks for\nmandibular canal segmentation in CBCT scans. Sens 22 (24), 9877. doi:10.3390/\ns22249877\nVerhelst, P.-J., Smolders, A., Beznik, T., Meewis, J., Vandemeulebroucke, A., Shaheen,\nE., et al. (2021). Layered deep learning for automatic mandibular segmentation in cone-\nbeam computed tomography. J. Dent. 114, 103786. doi:10.1016/j.jdent.2021.103786\nWaltrick, K. B., De Abreu Junior, M. J. N., Corrêa, M., Zastrow, M. D., and D ’Avila\nDutra, V. (2013). Accuracy of linear measurements and visibility of the mandibular\ncanal of cone-beam computed tomography images with different voxel sizes: an in vitro\nstudy. J. Periodontology 84 (1), 68 –77. doi:10.1902/jop.2012.110524\nWei, X., and Wang, Y. (2021). Inferior alveolar canal segmentation based on cone-\nbeam computed tomography. Med. Phys. 48 (11), 7074 –7088. doi:10.1002/mp.15274\nWidiasri, M., Ari ﬁn, A. Z., Suciati, N., Fatichah, C., Astuti, E. R., Indraswari, R., et al.\n(2022). Dental-yolo: alveolar bone and mandibular canal detection on cone beam\ncomputed tomography images for dental implant planning. IEEE Access 10,\n101483–101494. doi:10.1109/access.2022.3208350\nWu, Y., et al. (2022). Blood vessel segmentation from low-contrast and wide-ﬁeld optical\nmicroscopic images of cranial window by attention-gate-based network IEEE/CVF\nconference on computer vision and pattern r ecognition workshops (CVPRW). Jun. 2022,\nNew Orleans, LA, USA: IEEE, 1863 –1872. doi:10.1109/CVPRW56347.2022.00203\nFrontiers inBioengineering and Biotechnology frontiersin.org11\nLv et al. 10.3389/fbioe.2023.1302524",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6961356401443481
    },
    {
      "name": "Segmentation",
      "score": 0.6883728504180908
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5681469440460205
    },
    {
      "name": "Dice",
      "score": 0.5662710666656494
    },
    {
      "name": "Mandibular canal",
      "score": 0.45953819155693054
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3258785009384155
    },
    {
      "name": "Orthodontics",
      "score": 0.2859094440937042
    },
    {
      "name": "Medicine",
      "score": 0.2813172936439514
    },
    {
      "name": "Mathematics",
      "score": 0.20182150602340698
    },
    {
      "name": "Molar",
      "score": 0.13576090335845947
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I50632499",
      "name": "Chongqing University of Technology",
      "country": "CN"
    }
  ]
}