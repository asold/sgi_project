{
    "title": "Large language models for dermatological image interpretation – a comparative study",
    "url": "https://openalex.org/W4410615492",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5114545518",
            "name": "Lasse Cirkel",
            "affiliations": [
                "Universitätsklinikum Gießen und Marburg"
            ]
        },
        {
            "id": "https://openalex.org/A5092817899",
            "name": "Fabian Lechner",
            "affiliations": [
                "Universitätsklinikum Gießen und Marburg"
            ]
        },
        {
            "id": "https://openalex.org/A5117642549",
            "name": "Lukas Alexander Henk",
            "affiliations": [
                "Universitätsklinikum Gießen und Marburg"
            ]
        },
        {
            "id": "https://openalex.org/A2134178283",
            "name": "Martin Krusche",
            "affiliations": [
                "University Medical Center Hamburg-Eppendorf",
                "Universität Hamburg"
            ]
        },
        {
            "id": "https://openalex.org/A2101843625",
            "name": "Martin C. Hirsch",
            "affiliations": [
                "Universitätsklinikum Gießen und Marburg"
            ]
        },
        {
            "id": "https://openalex.org/A1499014811",
            "name": "Michael Hertl",
            "affiliations": [
                "Universitätsklinikum Gießen und Marburg"
            ]
        },
        {
            "id": "https://openalex.org/A2101153666",
            "name": "Sebastian Kühn",
            "affiliations": [
                "Universitätsklinikum Gießen und Marburg"
            ]
        },
        {
            "id": "https://openalex.org/A2803914004",
            "name": "Johannes Knitza",
            "affiliations": [
                "Université Grenoble Alpes",
                "Universitätsklinikum Gießen und Marburg"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4386958277",
        "https://openalex.org/W3196012986",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W6866223645",
        "https://openalex.org/W4404711353",
        "https://openalex.org/W4393931661",
        "https://openalex.org/W4388784077",
        "https://openalex.org/W4327681854",
        "https://openalex.org/W4324128838",
        "https://openalex.org/W4400772352",
        "https://openalex.org/W4400362569",
        "https://openalex.org/W4403862117",
        "https://openalex.org/W4376643691",
        "https://openalex.org/W4398202924",
        "https://openalex.org/W4280488363",
        "https://openalex.org/W4280572991",
        "https://openalex.org/W4382467515"
    ],
    "abstract": "Abstract Objectives Interpreting skin findings can be challenging for both laypersons and clinicians. Large language models (LLMs) offer accessible decision support, yet their diagnostic capabilities for dermatological images remain underexplored. This study evaluated the diagnostic performance of LLMs based on image interpretation of common dermatological diseases. Methods A total of 500 dermatological images, encompassing four prevalent skin conditions (psoriasis, vitiligo, erysipelas and rosacea), were used to compare seven multimodal LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro, Gemini 1.5 Flash, Claude 3.5 Sonnet, Llama3.2 90B and 11B). A standardized prompt was used to generate one top diagnosis. Results The highest overall accuracy was achieved by GPT-4o (67.8 %), followed by GPT-4o mini (63.8 %) and Llama3.2 11B (61.4 %). Accuracy varied considerably across conditions, with psoriasis with the highest mean LLM accuracy of 59.2 % and erysipelas demonstrating the lowest accuracy (33.4 %). 11.0 % of all images were misdiagnosed by all LLMs, whereas 11.6 % were correctly diagnosed by all models. Correct diagnoses by all LLMs were linked to clear, disease-specific features, such as sharply demarcated erythematous plaques in psoriasis. Llama3.2 90B was the only LLM to decline diagnosing images, particularly those involving intimate areas of the body. Conclusions LLM performance varied significantly, emphasizing the need for cautious usage. Notably, a free, locally hostable model correctly identified the top diagnosis for approximately two-thirds of all images, demonstrating the potential for safer, locally deployed LLMs. Advancements in model accuracy and the integration of clinical metadata could further enhance accessible and reliable clinical decision support systems.",
    "full_text": "Lasse Cirkel, Fabian Lechner, Lukas Alexander Henk, Martin Krusche, Martin C. Hirsch, Michael Hertl,\nSebastian Kuhn and Johannes Knitza*\nLarge language models for dermatological image\ninterpretation – a comparative study\nhttps://doi.org/10.1515/dx-2025-0014\nReceived January 27, 2025; accepted March 31, 2025;\npublished online May 23, 2025\nAbstract\nObjectives: Interpreting skin ﬁndings can be challenging\nfor both laypersons and clinicians. Large language models\n(LLMs) oﬀer accessible decision support, yet their diagnostic\ncapabilities for dermatological images remain underex-\nplored. This study evaluated the diagnostic performance of\nLLMs based on image interpretation of common dermato-\nlogical diseases.\nMethods: A total of 500 dermatological images, encom-\npassing four prevalent skin conditions (psoriasis, vitiligo,\nerysipelas and rosacea), were used to compare seven\nmultimodal LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro,\nGemini 1.5 Flash, Claude 3.5 Sonnet, Llama3.2 90B and 11B). A\nstandardized prompt was used to generate one top\ndiagnosis.\nResults: The highest overall accuracy was achieved by\nGPT-4o (67.8 %), followed by GPT-4o mini (63.8 %) and\nLlama3.2 11B (61.4 %). Accuracy varied considerably across\nconditions, with psoriasis with the highest mean LLM\naccuracy of 59.2 % and erysipelas demonstrating the lowest\naccuracy (33.4 %). 11.0 % of all images were misdiagnosed\nby all LLMs, whereas 11.6 % were correctly diagnosed by\nall models. Correct diagnoses by all LLMs were linked to\nclear, disease-speciﬁc features, such as sharply demarcated\nerythematous plaques in psoriasis. Llama3.2 90B was the\nonly LLM to decline diagnosing images, particularly those\ninvolving intimate areas of the body.\nConclusions: LLM performance varied signi ﬁcantly,\nemphasizing the need for cautious usage. Notably, a free,\nlocally hostable model correctly identiﬁed the top diagnosis\nfor approximately two-thirds of all images, demonstrating\nthe potential for safer, locally deployed LLMs. Advance-\nments in model accuracy and the integration of clinical\nmetadata could further enhance accessible and reliable\nclinical decision support systems.\nKeywords: artiﬁcial intelligence; large language models;\nskin pathology; dermatology; diagnosis; ChatGPT\nIntroduction\nArtiﬁcial Intelligence (AI) has become a transformative force\nin medicine, oﬀering innovative solutions to enhance diag-\nnostic accuracy, streamline workﬂows, and improve patient\noutcomes [1– 3]. In contrast to traditional models, multi-\nmodal large language models (LLMs) can process and\nintegrate a range of data types, including images, text, and\nstructured information. The promise of multimodal LLMs is\ntheir ability to address a range of diverse diagnostic chal-\nlenges across specialties [4, 5] using nearly any given data\nsource.\nDermatology oﬀers a unique testing ground for multi-\nmodal LLMs, as visual features are central to diagnosis.\nEven experienced dermatologists are experimenting to\nincorporate LLMs into their diagnostic processes [6].\nConcurrently, the ﬁeld of dermatology presents a\nchallenging environment for AI due to the presence of\nambiguous presentations, overlapping features, and\ndiverse skin tones. The application of advanced machine\nlearning algorithms and deeplearning models has yielded\n*Corresponding author: Johannes Knitza, MD, MHBA, PhD, Institute for\nDigital Medicine, University Hospital Giessen-Marburg, Philipps University\nMarburg, Baldingerstr. 1, 35043 Marburg, Germany; and Université Grenoble\nAlpes, AGEIS, Grenoble, France, E-mail: johannes.knitza@uni-marburg.de.\nhttps://orcid.org/0000-0001-9695-0657\nLasse Cirkel and Fabian Lechner,Institute of Artiﬁcial Intelligence,\nUniversity Hospital Gießen-Marburg, Philipps University, Marburg,\nGermany; and Institute for Digital Medicine, University Hospital\nGießen-Marburg, Philipps University, Marburg, Germany\nLukas Alexander Henk,Institute for Digital Medicine, University Hospital\nGießen-Marburg, Philipps University, Marburg, Germany; and Department\nof Dermatology and Allergology, University Hospital Gießen-Marburg,\nPhilipps University, Marburg, Germany\nMartin Krusche,Division of Rheumatology and Systemic Inﬂammatory\nDiseases, III. Department of Medicine, University Medical Center\nHamburg-Eppendorf, Hamburg, Germany\nMartin C. Hirsch,Institute of Artiﬁcial Intelligence, University Hospital\nGießen-Marburg, Philipps University, Marburg, Germany\nMichael Hertl,Department of Dermatology and Allergology, University\nHospital Gießen-Marburg, Philipps University, Marburg, Germany\nSebastian Kuhn,Institute for Digital Medicine, University Hospital\nGießen-Marburg, Philipps University, Marburg, Germany\nDiagnosis 2025; aop\nOpen Access. © 2025 the author(s), published by De Gruyter. This work is licensed under the Creative Commons Attribution 4.0 International License.\nimpressive results in tasks such as melanoma detection and\nlesion classiﬁcation [7– 9]. Nevertheless, aﬁrst comparative\nLLM study analyzing dermoscopic images for melanoma\ndetection yielded an accuracy of only roughly 50 % [10].\nDespite the great potential and increasing usage, there is\nstill a paucity of knowledge regarding the performance of\ngeneral purpose multimodal LLMs in dermatology. This\nstudy aimed to compare the diagnostic performance of\nseven multimodal LLMs for four common dermatological\nconditions.\nMaterials and methods\nThis study focused on four common dermatological di-\nagnoses, namely, psoriasis, vitiligo, erysipelas and rosacea.\nImages from two publicly available veriﬁed dermatology\ndatasets were used in this study. Atlas Dermatológico, is a\ncomprehensive online database containing over 12,000\nfreely available images representing a wide range of skin\ndiseases as of December 2024 [11]. The second dataset,\nDermIS.net, is described as the most comprehensive\ndermatology information service on the internet [12]. These\ndatasets contain images labeled and categorized by derma-\ntology experts and have been widely referenced in\ndermatological research, providing a diverse set of real-\nworld skin disease images. While we did not conduct an\nindependent review of image representativeness with\nboard-certiﬁed dermatologists, we relied on the expert-\ncurated nature of these databases to ensure data quality. All\nimages associated with the respective skin diseases available\non 01 December 2024 were used. No images were manually\nexcluded or reviewed prior to analysis to ensure an unbi-\nased sample. Theﬁnal dataset contained a total of 500 images\ncovering the four diﬀerent skin diseases. The distribution of\nimages per disease is shown in Table 1.\nDisease selection\nThe selection of psoriasis, vitiligo, erysipelas, and rosacea\nwas based on the availability of a large number of well-\nannotated, publicly accessible images in dermatological\ndatabases and the presence of distinct visual features that\noften allow for reliable diagnosis based on images. Many\nother dermatological conditions require additional patient\nhistory and clinical context for accurate diagnosis, which is\nbeyond the scope of this image-based evaluation.\nLarge language models\nSeven state-of-the-art multimodal LLMs were compared in\nthis study: OpenAI’s GPT4o (gpt-4o-2024-11-20) and GPT-4o\nmini (gpt-4o-mini-2024-07-18), Google’s Gemini 1.5 Pro (gemini-\n1.5-pro-002) and Gemini 1.5 Flash (gemini-1.5- ﬂash-002),\nAnthropic’s Claude 3.5 Sonnet (claude-3-5-sonnet-20241022),\nand Meta’sL l a m a 3 . 29 0 Ba n dL l a m a 3 . 21 1 B .\nFive of these models (GPT-4o, GPT-4o mini, Gemini 1.5\nPro, Gemini 1.5 Flash and Claude 3.5 Sonnet) represent fully\nmultimodal LLMs with comprehensive capabilities in pro-\ncessing various data types. Meta ’s Llama3.2 models are\nmedium-sized vision LLMs with more focused capabilities,\nspeciﬁcally designed for image understanding tasks. While\nthey don’t support video processing or image generation,\nthese models oﬀer distinct advantages, as they can be run\nlocally, providing enhanced data privacy and independence\nfrom external API services.\nData preparation\nAll images were used at their original full resolution to\npreserve diagnostic detail. To ensure compatibility with the\ninput requirements of each LLM’s API interface, image for-\nmats were adjusted accordingly. For OpenAI’s GPT-4o and\nAnthropic’s Claude 3.5 Sonnet, images were converted to\nBase64 encoding using the standard Python library base64.\nNo conversion was required for Google’s Gemini and Meta’s\nLlama3.2 models, which accept.jpg images. All metadata\nembedded in the imageﬁles, such as EXIF data, was removed\nduring processing using the Python Imaging Library\n(PIL/Pillow) [13]. This step ensures that no inadvertent in-\nformation could bias the output of the models. Images were\nrenamed with sequential numerical identiﬁers to further\nanonymize the data.\nExperimental procedure\nFor ﬁve of the models (GPT-4o, GPT-4o mini, Gemini 1.5 Pro,\nGemini 1.5 Flash and Claude 3.5 Sonnet), each image was\nsequentially input using their respective API interfaces. For\nTable : Dataset with respective image sources and diseases.\nDisease Atlas dermatológico DermIS Total\nPsoriasis   \nVitiligo   \nErysipelas   \nRosacea   \nTotal   \n2 Cirkel et al.: Large language models for dermatological image interpretation\nMeta’s Llama3.2 models, which were deployed locally, im-\nages were sequentially processed using Ollama (Version\nv0.5.1) as the interface. A new session was created for each\nimage to ensure that the models did not retain information\nfrom previous interactions that could inﬂuence their re-\nsponses. To ensure comparability and reduce bias the same\nstandardized prompt was used for all models Figure 1. LLMs\nwere prompted to generate the most likely diagnosis based\non the image provided. Cases where models refused to\nprovide a diagnosis or where API requests were blocked\nwere rated as incorrect classiﬁcations. No API failures were\nnoted during the study period. LLM requests were made\nbetween 07 December 2024 and 14 December 2024.\nStatistical methods\nAll descriptive statistical analyses were performed using R\nversion 4.1.0 (R Foundation for Statistical Computing,\nVienna, Austria).\nEthical aspects\nAll images were publicly available on the respective data-\nbases. The Philipps-University Marburg Research Ethics\nCommittee conﬁrmed that no ethical approval was required\n(reference number: 23– 300 ANZ) due to the anonymous and\nnon-interventional nature of the study.\nResults\nOverall model accuracies\nThe evaluation revealed notable diﬀerences in model per-\nformance, see Table 2 and Figure 2. The highest overall\naccuracy was achieved by GPT-4o, with a score of 67.8 %,\nfollowed by GPT-4o mini (63.8 %), and Llama3.2 11B (61.4 %).\nThe larger Llama3.2 90B model exhibited an inferior per-\nformance compared to its smaller counterpart, achieving\nonly 50.8 %. This can be attributed, at least in part, to its\ntendency to frequently refuse to diagnose certain images (91,\n18.2 %). Gemini 1.5 Flash demonstrated the lowest overall\naccuracy with 37.0 %.\nDisease speciﬁc performance\nDiagnostic accuracy varied across the four conditions\nFigure 2, with the highest mean LLM accuracy for psoriasis\n(59.2 %) and lowest for erysipelas (33.4 %). Performance of\nLLMs varied across diseases. For psoriasis, the GPT-4o mini\nmodel was identiﬁed as the most eﬀective, with an accuracy of\n80.3 %, followed by the Llama3.2 11B model (77.6 %). For viti-\nligo GPT-4o demonstrated the highest accuracy with 78.7 %.\nErysipelas proved a signiﬁcant challenge for all models, with\naccuracy rates ranging from 16.5 to 50.6 %. Llama3.2 90B\nachieved the second-best accuracy for this condition, with\n44.7 %. This was despite its refusal to generate a diagnosis for\nFigure 1: Standardized system and user\nprompts.\nTable : Accuracy of LLMs in % across multiple diseases.\nDisease, n GPT- o GPT- o Mini Gemini . pro Gemini . Flash Claude . Sonnet Llama . B Llama . B\nPsoriasis () . . .  .  .  .  .\nVitiligo () . .  .  .  .  .  .\nErysipelasa () . .  .  .  .  .  .\nRosacea () . . .  .  .  .  .\nOverall accuracyb . .  .  .  .  .  .\nBold values indicate the highest accuracy achieved for each dermatological condition among the tested models.aCellulitis was also rated as correct.\nbWeighted accuracy.\nCirkel et al.: Large language models for dermatological image interpretation 3\n23.5 % (20/85) of the erysipelas images. For rosacea GPT-4o\nmini performed best with an accuracy 67.9 %.\nPatterns in misdiagnosed images\nAnalysis of the misdiagnosed images revealed notable pat-\nterns (Table 3). A total of 11.0 % (55/500) images were\nmisclassiﬁed by all models, highlighting the challenges\nof interpreting ambiguous or visually overlapping features.\nIn contrast, 11.6 % (58/500) images were correctly classiﬁed\nby all models, characterized by clear and distinctive disease-\nspeciﬁc features, such as the sharply demarcated\nerythematous plaques in psoriasis. Visual examples of three\nvitiligo images correctly classiﬁed by all models and three\nimages misclassiﬁed by all models are provided in Figure 3 to\nillustrate these patterns.\nInability to generate diagnoses\nAll LLMs generated a diagnosis except for Meta’s Llama3.2\n90B model, which frequently refused to diagnose images,\nparticularly those depicting intimate areas of the body. This\nbehavior was most evident in psoriasis with 19.7 % (50/254)\nrefusals, vitiligo 19.4 % (21/108) and erysipelas 23.5 % (20/85).\nThe pattern of refusals suggests that the model’s responses\nare inﬂuenced by the body regions depicted in the images.\nInterestingly, there were no refusals for rosacea, although\nthe dataset consisted mainly of facial images. The smaller\nLlama3.2 11B model consistently provided diagnoses for all\nimages, regardless of body region, contributing to its supe-\nrior performance compared to its larger counterpart.\nDiscussion\nThis study assessed the performance of LLMs in interpreting\nimages of common dermatological conditions. The results of\nthis study demonstrate a relatively high diagnostic accuracy\nof certain models even without domain-speci ﬁc further\ntraining. The strong performance of GPT-4o, which achieved\nthe highest overall accuracy, highlights the potential of\ngeneral-purpose multimodal models for visually distinct\nconditions such as vitiligo and psoriasis. The competitive\nperformance of Llama3.2 11B, despite its smaller size, free\navailability, and local deployment capabilities, highlights the\npotential of medium-sized, privacy-preserving models for\nTable : Classiﬁcation results by disease and overall.\nDisease Total im-\nages, n (%)\nAll LLMs\nare false,\nn (%)\n≥ LLMs\nare false,\nn (%)\nAll LLMs\nare right,\nn (%)\n≥ LLMs\nare right,\nn (%)\nPsoriasis  (.)  (.)  (.)  (.)  (.)\nVitiligo  (.)  (.)  (.)  (.)  (.)\nErysipelas  (.)  (.)  (.)  (.)  (.)\nRosacea  (.)  (.)  (.)  (.)  (.)\nOverall  (.)  (.)  (.)  (.)  (.)\nFigure 2: Diagnostic accuracy of respective large language models according to respective dermatological diseases.\n4 Cirkel et al.: Large language models for dermatological image interpretation\ndiagnostic decision support. However, the limitations\nobserved highlight the need for signiﬁcant improvements\nbefore such systems can be reliably integrated into clinical\nworkﬂows. SkinGPT-4 exempliﬁes a privacy-preserving LLM\ntrained using a two-step strategy [14]. The model was\nrigorously tested on data from 150 real-world patients,\nachieving a diagnostic accuracy or relevance rate of 80.6 %\nas evaluated by board-certiﬁed dermatologists. The vari-\nability in accuracy between conditions highlights the\ninherent complexity of dermatological diagnosis. Conditions\nwith high-quality images and clear and well-deﬁned visual\nfeatures, such as vitiligo, were generally well-recognized by\nmost models. In contrast, conditions such as erysipelas,\nwhich are characterized by overlapping and more subtle\nfeatures, posed a greater challenge.\nThis discrepancy highlights a key limitation of image-\nonly diagnosis, where contextual clinical information often\nplays a critical role in accurate decision-making. Incorpo-\nrating multimodal data streams, including demographic\nvariables, medical history and laboratory ﬁndings, could\nenable these models to align visual patterns with contextual\ncues, moving beyond pattern recognition towards more\nnuanced and clinically relevant analysis. Future research\nshould prioritize the integration of such multimodal\napproaches to bridge the observed performance gaps.\nIn addition, direct comparisons with board-certi ﬁed\ndermatologists could provide valuable context for assess-\ning LLM performance. Although beyond the scope of the\ncurrent study, expert benchmarks would help determine\nwhether models truly add diagnostic value or simply reﬂect\npatterns already present in publicly available datasets.\nEstablishing such comparisons in future research could\nclarify the strengths and limitations of LLM-based derma-\ntological diagnosis.\nThe interpretability of LLM-based diagnostics remains a\npressing concern. While models such as GPT-4o and\nLlama3.2 11B have demonstrated substantial diagnostic ac-\ncuracy, their decision-making processes remain inherently\nopaque. This lack of transparency may hinder clinical up-\ntake and buy-in, as understanding the rationale behind a\ndiagnosis is crucial for trust and validation in medical\npractice. The provision of interpretable output, such as vi-\nsual explanations highlighting pathologies could address\nthis issue [15]. Improved interpretability would not only\npromote conﬁdence among clinicians but also support reg-\nulatory approval processes by ensuring that diagnostic\ndecisions are explainable, reproducible and ethically sound.\nData-related limitations also require careful consideration\n[16]. The use of publicly available datasets carries the risk of\ngeographical bias and under-representation of diﬀerent skin\ncolors and conditions. In addition, the lack of histopatho-\nlogical conﬁrmation and unclear labelling criteria may\nFigure 3: Examples of correctly and incorrectly classiﬁed images. The top row shows three images (A– C) that were correctly classiﬁed as vitiligo by all\nseven LLMs, whereas the bottom row shows three images (D– F) that could not be correctly diagnosed as vitiligo by any of the LLMs. Image sources: www.\ndermis.net and www.atlasdermatologico.com.br.\nCirkel et al.: Large language models for dermatological image interpretation 5\ncompromise the validity of the ground truth, making it\ndiﬃcult to distinguish between model error and dataset\nimperfections.\nThe integration of LLMs into diagnostic workﬂows not\nonly poses signiﬁcant regulatory challenges but also necessi-\ntates rigorous clinical validation. Under the European Union’s\nMedical Device Regulation, AI systems intended for diagnostic\npurposes are classiﬁed as medical devices, requiring stringent\nvalidation to ensure safety and eﬃcacy. Additionally, the\nrecently passed EU Artiﬁcial Intelligence Act categorizes\nhealthcare AI systems as high-risk, mandating compliance\nwith strict performance, transparency, and ethical standards\n[17]. Beyond regulatory requirements, these systems must\nundergo robust clinical evaluation through preclinical and\nearly clinical validation studies, including clinical simulation\nsettings and/or Early Feasibility Studies [18, 19]. These studies\nprovide critical insights into the safety, eﬀectiveness, and real-\nworld applicability of LLMs, identifying potential risks and\nlimitations early in the development process. Combining\nregulatory compliance with phased clinical validation will be\nessential for achieving both trust and widespread adoption in\nclinical practice.\nA main limitation of this study is that tested LLM models\nlikely had access to parts of the publicly available image\ndatasets. This familiarity could artiﬁcially inﬂate diagnostic\naccuracy, as the models may have been trained orﬁne-tuned\non these datasets or closely related ones. Consequently, the\nresults may not fully reﬂect the models’ true capabilities\nwhen applied to unseen, real-world clinical data. To address\nthis, future research should prioritize the use of private,\nindependently curated datasets that are not accessible dur-\ning model training. Incorporating real-world clinical images\nfrom diverse patient populations would enhance the\nrobustness and generalizability ofﬁndings. This approach\nwould also mitigate concerns of data leakage, ensuring that\nmodel performance more accurately reﬂects its diagnostic\npotential in practice.\nAnother limitation of this study is its focus on only four\ndermatological conditions. While these conditions represent\na spectrum of diagnostic complexity, they do not encompass\nthe full range of dermatological presentations. Conse-\nquently, theﬁndings provide limited insight into the models’\ncapabilities across the broader spectrum of skin diseases,\nincluding rarer or more nuanced conditions. Future studies\nshould aim to include a wider variety of dermatological di-\nagnoses to ensure a more comprehensive evaluation of LLM\nperformance. This broader analysis would help establish\nwhether the observed diagnostic trends hold across diverse\nand less well-deﬁned conditions.\nNevertheless, we believe comparative benchmarking\nstudies are crucial and can guide decisions on which models\nto choose for which purposes. Given the rapid iteration of\nlarge language models, newer versions with potentially\nimproved capabilities are frequently released. While this\nstudy assessed models available at the time of testing, future\nresearch should continue to apply systematic benchmarking\nto newer models to track progress and evaluate whether\nperformance improvements extend to dermatological image\ninterpretation. To ensure meaningful comparisons over\ntime, it is equally important that these evaluations are con-\nducted on high-quality, diverse datasets. Therefore, future\nstudies should incorporate private, independently curated\ndatasets that include a variety of skin tones, conditions, and\nclinical contexts to ensure robust and generalizable model\nevaluations. Additionally, it seems crucial to evaluate the\nactual clinical impact of model usage.\nConclusions\nThis study demonstrates that multimodal LLMs can eﬀec-\ntively identify key diagnostic features in dermatological\nimages, even in the absence of domain-speciﬁc training.\nTheir ability to recognize conditions with distinctive visual\npatterns, such as vitiligo and psoriasis, highlights their po-\ntential for wider clinical applications. In particular, the\ncomparable performance of locally deployed models such as\nLlama3.2 11B to large, cloud-based solutions highlights\nimportant implications for privacy and scalability in medical\nsettings. Further studies are warranted investigating how to\nbest integrate multimodal LLMs into clinical workﬂows.\nResearch ethics: The local Institutional Review Board\ndeemed the study exempt from review (reference number:\n23– 300 ANZ).\nInformed consent:Not applicable.\nAuthor contributions: LC and JK contributed to the\nconception and design of the study, data collection, analysis,\nand interpretation. All authors were involved in drafting the\nmanuscript, critically revising it for important intellectual\ncontent. All authors have accepted responsibility for the\nentire content of this manuscript and approved its\nsubmission.\nUse of Large Language Models, AI and Machine Learning\nTools: ChatGPT-4o was used to improve the language of\nmanuscript sections.\nConﬂict of interest: JK declares research support from\nAbbvie, Vila Health, honoraria and consulting fees from\nAbbvie, AstraZeneca, BMS, Boehringer Ingelheim, Chugai,\nGAIA, Galapagos, GSK, Janssen, Lilly, Medac, Novartis, Pﬁzer,\nSobi, Rheumaakademie, UCB, Vila Health and Werfen. MK\ndeclares research support from Abbvie, Sobi and Sanoﬁ;\n6\nCirkel et al.: Large language models for dermatological image interpretation\nhonoraria and consulting fees from Abbvie, BMS, Boeh-\nringer Ingelheim, AlfaSigma, GSK, Janssen, Lilly, Medac,\nNovartis, Pﬁzer, Sobi, UCB. The remaining authors declare\nno competing interests.\nResearch funding:None declared.\nData availability:The raw data analysed during the current\nstudy are available from the corresponding author upon\nreasonable request.\nReferences\n1. Alowais SA, Alghamdi SS, Alsuhebany N, Alqahtani T, Alshaya AI,\nAlmohareb SN, et al. Revolutionizing healthcare: the role of artiﬁcial\nintelligence in clinical practice. BMC Med Educ 2023;23:689.\n2. Aung YYM, Wong DCS, Ting DSW. The promise of artiﬁcial intelligence: a\nreview of the opportunities and challenges of artiﬁcial intelligence in\nhealthcare. Br Med Bull 2021;139:4– 15.\n3. Clusmann J, Kolbinger FR, Muti HS, Carrero ZI, Eckardt JN, Laleh NG,\net al. The future landscape of large language models in medicine.\nCommun Med 2023;3:141.\n4. Saab K, Tu T, Weng WH, Tanno R, Stutz D, Wulczyn E, et al. Capabilities of\ngemini models in medicine. Available from: https://arxiv.org/abs/2404.\n18416.\n5. Strotzer QD, Nieberle F, Kupke LS, Napodano G, Muertz AK, Meiler S,\net al. Toward foundation models in radiology? Quantitative assessment\nof GPT-4V’s multimodal and multianatomic region capabilities.\nRadiology 2024;313:e240955.\n6. Gui H, Rezaei SJ, Schlessinger D, Weed J, Lester J, Wongvibulsin S, et al.\nDermatologists’ perspectives and usage of Large Language Models in\npractice: an exploratory survey. J Invest Dermatol 2024;144:2298– 301.\n7. Brancaccio G, Balato A, Malvehy J, Puig S, Argenziano G, Kittler H.\nArtiﬁcial intelligence in skin cancer diagnosis: a reality check. J Invest\nDermatol 2024;144:492– 9.\n8. Escalé-Besa A, Yélamos O, Vidal-Alaball J, Fuster-Casanovas A,\nMiró Catalina Q, Börve A, et al. Exploring the potential of artiﬁcial\nintelligence in improving skin lesion diagnosis in primary care. Sci Rep\n2023;13:4293.\n9. Sanchez K, Kamal K, Manjaly P, Ly S, Mostaghimi A. Clinical application\nof artiﬁcial intelligence for non-melanoma skin cancer. Curr Treat\nOptions Oncol 2023;24:373– 9.\n10. Liu X, Duan C, Kim M, Zhang L, Jee E, Maharjan B, et al. Claude 3 opus\nand ChatGPT with GPT-4 in dermoscopic image analysis for melanoma\ndiagnosis: comparative performance analysis. JMIR Med Inform 2024;\n12:e59273.\n11. da Silva SF: Atlas dermatológico. https://atlasdermatologico.com.br/\nindex.jsf [Accessed 20 Dec 2024].\n12. A cooperation between the dept of clinical social medicine (Univ of\nHeidelberg) and the dept of dermatology (Univ of Erlangen). DermIS -\nDermatology Information System. https://www.dermis.net/\ndermisroot/en/home/index.htm [Accessed 20 Dec 2024].\n13. Clark, JA and contributors. Pillow documentation. https://pillow.\nreadthedocs.io/en/stable/index.html [Accessed 20 Dec 2024].\n14. Zhou J, He X, Sun L, Xu J, Chen X, Chu Y, et al. Pre-trained multimodal\nlarge language model enhances dermatological diagnosis using\nSkinGPT-4. Nat Commun 2024;15:5649.\n15. Bhandari A. Revolutionizing radiology with artiﬁcial intelligence.\nCureus 2024;16:e72646.\n16. Navigli R, Conia S, Ross B. Biases in Large Language Models: origins,\ninventory, and discussion. J Data Inf Qual 2023;15. https://doi.org/10.\n1145/3597307.\n17. Gilbert S. The EU passes the AI Act and its implications for digital\nmedicine are unclear. NPJ Digit Med 2024;7:135.\n18. Vasey B, Nagendran M, Campbell B, Clifton DA, Collins GS, Denaxas S,\net al. Reporting guideline for the early-stage clinical evaluation of\ndecision support systems driven by artiﬁcial intelligence: DECIDE-AI.\nNat Med 2022;28:924– 33.\n19. Lau K, Halligan J, Fontana G, Guo C, O’Driscoll FK, Prime M, et al.\nEvolution of the clinical simulation approach to assess digital health\ntechnologies. Future Healthc J 2023;10:173– 5.\nCirkel et al.: Large language models for dermatological image interpretation 7"
}