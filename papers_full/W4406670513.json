{
  "title": "Design of an integrated model with temporal graph attention and transformer-augmented RNNs for enhanced anomaly detection",
  "url": "https://openalex.org/W4406670513",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5094088250",
      "name": "Sai Babu Veesam",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028340043",
      "name": "Aravapalli Rama Satish",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5073929059",
      "name": "Sreenivasulu Tupakula",
      "affiliations": [
        "SRM University"
      ]
    },
    {
      "id": "https://openalex.org/A5034716079",
      "name": "Yuvaraju Chinnam",
      "affiliations": [
        "St. Peter's Institute of Higher Education and Research"
      ]
    },
    {
      "id": "https://openalex.org/A5075730542",
      "name": "Krishna Prakash",
      "affiliations": [
        "National Institute of Technology Andhra Pradesh"
      ]
    },
    {
      "id": "https://openalex.org/A5010532705",
      "name": "Shonak Bansal",
      "affiliations": [
        "Chandigarh University"
      ]
    },
    {
      "id": "https://openalex.org/A5105931508",
      "name": "Mohammad Rashed Iqbal Faruque",
      "affiliations": [
        "National Space Agency",
        "National University of Malaysia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4391164438",
    "https://openalex.org/W4389317800",
    "https://openalex.org/W3017372817",
    "https://openalex.org/W4400314845",
    "https://openalex.org/W3160995381",
    "https://openalex.org/W3215243207",
    "https://openalex.org/W4206340617",
    "https://openalex.org/W4390603747",
    "https://openalex.org/W4212985046",
    "https://openalex.org/W4376456928",
    "https://openalex.org/W4393106072",
    "https://openalex.org/W4391892559",
    "https://openalex.org/W3047585969",
    "https://openalex.org/W3217095651",
    "https://openalex.org/W4319302966",
    "https://openalex.org/W4226052655",
    "https://openalex.org/W4207035543",
    "https://openalex.org/W4312904361",
    "https://openalex.org/W3110984360",
    "https://openalex.org/W4367146823",
    "https://openalex.org/W4386320464",
    "https://openalex.org/W3186882252",
    "https://openalex.org/W4312302556",
    "https://openalex.org/W4399881302",
    "https://openalex.org/W3190534054",
    "https://openalex.org/W4385731851",
    "https://openalex.org/W4321192586",
    "https://openalex.org/W4387677241",
    "https://openalex.org/W4362564841",
    "https://openalex.org/W4285324760",
    "https://openalex.org/W3204931730",
    "https://openalex.org/W4206574198",
    "https://openalex.org/W3208730351",
    "https://openalex.org/W4323520274",
    "https://openalex.org/W4210535798",
    "https://openalex.org/W3190675887",
    "https://openalex.org/W4400233057",
    "https://openalex.org/W4399427304",
    "https://openalex.org/W4383109449",
    "https://openalex.org/W4391128986",
    "https://openalex.org/W4404563521",
    "https://openalex.org/W2183706426",
    "https://openalex.org/W3129403951",
    "https://openalex.org/W3128803436",
    "https://openalex.org/W4403559481",
    "https://openalex.org/W4404396073",
    "https://openalex.org/W4312727017",
    "https://openalex.org/W4285815764",
    "https://openalex.org/W4403721517",
    "https://openalex.org/W4226237726",
    "https://openalex.org/W4226419212",
    "https://openalex.org/W4226282599",
    "https://openalex.org/W4213217172",
    "https://openalex.org/W4226475867",
    "https://openalex.org/W4229011926"
  ],
  "abstract": "It is important in the rising demands to have efficient anomaly detection in camera surveillance systems for improving public safety in a complex environment. Most of the available methods usually fail to capture the long-term temporal dependencies and spatial correlations, especially in dynamic multi-camera settings. Also, many traditional methods rely heavily on large labeled datasets, generalizing poorly when encountering unseen anomalies in the process. We introduce a new framework to address such challenges by incorporating state-of-the-art deep learning models that improve temporal and spatial context modeling. We combine RNNs with GATs to model long-term dependencies across cameras effectively distributed over space. The Transformer-Augmented RNN allows for a better way than standard RNNs through self-attention mechanisms to improve robust temporal modeling. We employ a Multimodal Variational Autoencoder-MVAE that fuses video, audio, and motion sensor information in a manner resistant to noise and missing samples. To address the challenge of having a few labeled anomalies, we apply the Prototypical Networks to perform few-shot learning and enable generalization based on a few examples. Then, a Spatiotemporal Autoencoder is adopted to realize unsupervised anomaly detection by learning normal behavior patterns and deviations from them as anomalies. The methods proposed here yield significant improvements of about 10% to 15% in precision, recall, and F1-scores over traditional models. Further, the generalization capability of the framework to unseen anomalies, up to a gain of + 20% on novel event detection, represents a major advancement for real-world surveillance systems.",
  "full_text": "Design of an integrated model \nwith temporal graph attention and \ntransformer-augmented RNNs for \nenhanced anomaly detection\nSai Babu Veesam1, Aravapalli Rama Satish1, Sreenivasulu Tupakula2, Yuvaraju Chinnam3, \nKrishna Prakash4, Shonak Bansal5 & Mohammad Rashed Iqbal Faruque6\nIt is important in the rising demands to have efficient anomaly detection in camera surveillance \nsystems for improving public safety in a complex environment. Most of the available methods usually \nfail to capture the long-term temporal dependencies and spatial correlations, especially in dynamic \nmulti-camera settings. Also, many traditional methods rely heavily on large labeled datasets, \ngeneralizing poorly when encountering unseen anomalies in the process. We introduce a new \nframework to address such challenges by incorporating state-of-the-art deep learning models that \nimprove temporal and spatial context modeling. We combine RNNs with GATs to model long-term \ndependencies across cameras effectively distributed over space.  The Transformer-Augmented RNN \nallows for a better way than standard RNNs through self-attention mechanisms to improve robust \ntemporal modeling. We employ a Multimodal Variational Autoencoder-MVAE that fuses video, audio, \nand motion sensor information in a manner resistant to noise and missing samples. To address the \nchallenge of having a few labeled anomalies, we apply the Prototypical Networks to perform few-shot \nlearning and enable generalization based on a few examples. Then, a Spatiotemporal Autoencoder \nis adopted to realize unsupervised anomaly detection by learning normal behavior patterns and \ndeviations from them as anomalies. The methods proposed here yield significant improvements \nof about 10% to 15% in precision, recall, and F1-scores over traditional models. Further, the \ngeneralization capability of the framework to unseen anomalies, up to a gain of + 20% on novel event \ndetection, represents a major advancement for real-world surveillance systems.\nKeywords Anomaly detection, Temporal graph attention, Transformer-augmented RNNs, Multimodal \nfusion, Few-shot learning\nIt is a key research area for anomaly detection in surveillance systems, especially because of the increasing \ninstallation of a multi-camera network in cities, industries, and public places. While such systems are installed \nto enhance security and operational efficiency, they continuously generate copious volumes of video data \nthat demand sophisticated techniques for automatically detecting abnormal activities. Traditional anomaly \ndetection1–3 commonly relies either on handcrafted features or on classical machine learning algorithms that \nare not always suitable for modeling complex spatial and temporal dependencies present in such data samples. \nMoreover, they usually require large labeled training datasets, and this limits scalability, generalizing poorly \nto unseen anomalies. More recently, significant progress has been made in using neural networks for anomaly \ndetection, thanks to the emergence of deep learning. However, most of these DL-based methods 4–6 still lack \nthe capability for modeling complex spatial relationships between multiple camera feeds and the long-term \ntemporal dependencies that play a critical role in identifying rare or gradual anomalies. For instance, RNNs, \n1School of Computer Science, VIT-AP University, Vijayawada 522241, Andhra Pradesh, India. 2Department of \nElectronics and Communication Engineering, SRM University, Amaravati 522240, Andhra Pradesh, India. 3Professor \nof Computer Science and Engineering (AI&ML), St. Peter’s Engineering College, Hyderabad, India. 4Department \nof Electronics and Communication Engineering, NRI Institute of Technology, Agiripalli, Eluru 521212, Andhra \nPradesh, India. 5Department of Electronics and Communication Engineering, University Institute of Engineering, \nChandigarh University, Gharuan, Mohali, India. 6Space Science Centre (ANGKASA), Universiti Kebangsaan \nMalaysia, Bangi 43600 UKM, Selangor D.E, Malaysia. email: k_krishna2k7@yahoo.co.in; shonakk@gmail.com;  \nrashed@ukm.edu.my\nOPEN\nScientific Reports |         (2025) 15:2692 1| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports\n\nthough widely used in temporal modeling, have inherent limitations in capturing long-range dependencies \nduring the process. While the convolutional methods operating on either individual frames or local patches are \nunable to capture larger-scale contextual information across different camera views for diverse scenarios. These \nlacunae in the current frameworks further necessitate an advanced architecture that shall integrate both spatial \nand temporal information across a network of cameras.\nGiven such challenges, this work proposes a new paradigm for anomaly detection by designing an integrated \nmodel that leverages state-of-the-art deep learning architectures on temporal context modeling and spatial \ninformation fusion. In this paper, we propose a Temporal Graph Attention Network-TGAT-based model that \nintegrates RNNs with GATs for capturing temporal dependencies while dynamically attending to important \ncamera feeds over temporal instance sets. This approach provides a better monitoring of complex environments, \nwhere anomalies can manifest only for larger time spans and over multiple distributed sensors4–6. Another key \nchallenge that this work tries to address is how to fuse multimodal data samples. Indeed, many surveillance \nsystems can incorporate added sensor modalities such as audio, motion detectors, or even biometric data, \nadding their respective information for detecting anomalies. To handle this, the model under the proposal will \nembody the Multimodal Variational Autoencoder that will learn a joint representation of latent variables from \nmultiple sensor modalities. It uses optimization of shared latent variables with reconstruction loss to ensure that \nsalient information from each modality is retained by MV AE. The detection capability is thus more robust in this \ncase, even when noisy or missing data samples are available in process.\nGraph Attention Networks (GATs) recently achieved great success in most anomaly detection and \nspatiotemporal modeling domains. For example, GATs were applied to model interactions among individuals \nin highly dense crowds to capture collective movement patterns as well as group dynamics and anomalies. In \nvideo surveillance, GATs were applied to multi-camera setups in order to establish correlations between spatially \ndistributed feeds that allow anomalies in pedestrian or vehicular movement to be detected better in these \noperations. Moreover, in traffic monitoring, GATs have been used for modeling interdependencies between road \nsegments so that congestion detection and the prediction of accidents could be better done in process. These \napplications are able to demonstrate the ability of GATs toward learning relationships between structured and \nunstructured data, which only serves to prove the importance of relevance to the proposed work. Such examples \nin incorporation stress that the GAT is adaptable to multiple scenarios, providing flexibility to the context of the \nstudy process.\nAnother important aspect is the rarity of anomalous events, which makes it challenging to train supervised \nmodels. In this respect, the integrated model employs few-shot learning through Prototypical Networks, which \nenable anomaly classification with only a few shots of labeled examples. This is of particular value in surveillance \nsituations since it is typically too impractical and expensive to collect such large annotated datasets. By learning \na metric space where the distances between prototypes, as representatives of normal and anomalous behavior, \nrespectively, can be computed, it generalizes well to unseen anomalies-a basic but key requirement in real-\nworld applications. An unsupervised Spatiotemporal Autoencoder learns patterns in the normal behavior of \nboth dimensions: spatial and temporal. Autoencoders are known for their ability to compress input data into \na lower-dimensional latent space and reconstruct it. That is, in anomaly detection, deviation in reconstruction \nerror acts as a proxy for recognizing abnormal patterns. By generalizing this idea to spatiotemporal data, the \nautoencoder will be able to model normal behaviors more effectively within a network of multiple cameras and \nflag deviations that possibly signify anomalies. This integrated model is thus the comprehensive solution to \nthe challenges faced by the different existing systems of anomaly detection. It significantly improves accuracy, \nprecision, and recall by including temporal and spatial modeling with advanced techniques for data fusion and \nfew-shot learning. The generalization of the model to unseen anomalies is a quantum leap in this respect since \nthe inability to do so has been one of the most irritating aspects that the traditional approaches have exhibited so \nfar. Extensive experiments demonstrate that the proposed methods raise the performance bar of the state-of-the-\nart in different real-world scenarios by significantly improving F1-scores and reducing false positives.\nMotivation and contribution\nThe motivation for this work arises from the increasing complexity of modern surveillance systems and the \ndeficiency in the potential of the existing anomaly detection methods to fully catch up with challenges occurring \nin multi-camera environments. Traditional models based on hand-crafted features or early deep learning \ntechniques usually do not capture nuanced spatial and temporal dynamics in an environment, which are \nnecessary for the detection of abnormal events. Not only do surveillance systems deal with vast data emanating \nfrom several cameras, but the anomalies of interest tend to be rare, subtle and spread out in a fact that makes the \njob challenging. Additional complexities arise by integrating extra sensor data, such as motion or audio, since \nthese modalities must be effectively fused to achieve further improvements in detection accuracy levels. Most of \nthe existing models also suffer from over-dependence on large annotated datasets-a factor that further limits their \nemployment in real-world settings, where such data is hard or too expensive to come by. This work overcomes \nthese limitations by proposing a new integrated model leveraging the latest state-of-the-art deep learning \nmodel architectures. In this paper, the proposed key innovation is the Temporal Graph Attention Network or, \nin short, TGAT. It models both long-term temporal dependencies and spatial correlations across a network \nof cameras. Rather than resorting to traditional RNNs, which struggle to model longer-range dependencies, \nTGAT leverages attention mechanisms that dynamically lock onto the most relevant camera feeds at each set of \ntimestamps, hence enhancing the model’s power to monitor complex environments. The proposed Transformer-\nAugmented Recurrent Neural Network increases the temporal modeling capability of the system by combining \nthe strengths of RNNs in terms of short-term event correlations with those of transformers for capturing long-\nrange dependencies. This hybrid approach ensures the effective modeling of both local and global temporal \npatterns toward a holistic solution for anomaly detection.\nScientific Reports |         (2025) 15:2692 2| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nThis work also contributes to a Multimodal Variational Autoencoder that fuses data from different sensor \nmodalities, such as video, audio, and motion. The MV AE performs optimization based on a joint latent \nspace preserving all the critical information from each modality, thereby making the model robust against \nany noisy or incomplete data, as is common in many real-world surveillance systems. Apart from that, few-\nshot learning with Prototypical Networks deals with the challenge of a limited number of labeled datasets by \nletting the model generalize to unseen anomalies with a few labeled examples. It has more value in anomaly \ndetection since, in many cases, it is highly impractical to get large amounts of data with annotations. Lastly, the \nSpatiotemporal Autoencoder offers an unsupervised framework for learning normal behavior patterns where \nthe reconstruction error serves as a signal to detect deviations that may point to anomalies. This reduces the \nfalse positive rate significantly by an unsupervised approach while improving recall in general, especially for \nmultiple cameras. Putting all these together yields a very strong model, outperforming the state of the art in \naccuracy and generalization. This paper therefore proposes an effective detection of anomalies in complex and \nreal surveillance settings with the main challenges of long-term temporal modeling, multimodal data fusion, and \nfew-shot learning. Extensive experimental results have shown that the proposed approaches show significant \nimprovement in anomaly detection performance, both by enhancing F1 scores and reducing false positive rates, \nhence making the model highly applicable for a wide range of security and monitoring applications.\nReview of existing models used for multiple camera anomaly analysis\nLarge-scale datasets and rapidly developing deep learning techniques have contributed to the rapid growth \nin the area of crowd activity analysis and anomaly detection. With increasing complexities in urban life, the \ndemand for effective and efficient surveillance to monitor crowd activities and detect anomalies is becoming \ncritical. This work presents a comprehensive review of 40 influential studies in the subject area of crowd anomaly \ndetection, based on various techniques ranging from CNNs and GNNs to V AEs, RL, and other state-of-the-\nart AI frameworks. Each of these contributions brings methodologies that may uniquely contribute to solving \nthe challenges of dense, dynamic crowds for both real-time and post-event analyses, each with discussions on \nlimitations, effectiveness, and applicability. The majority of the approaches under review can be noted to operate \non supervised learning, which is highly dependent on labeled datasets. This very dependence on large labeled \ndata, however, then becomes a limitation in itself, because amassing datasets representative of a wide variety of \nanomalies is often an exhausting, costly, or sometimes impossible undertaking in many situations. Approaches, \nsuch as those in1–3, prove the well-known fact that traditional supervised methods, while performing reasonably, \nmostly fail to generalize beyond their training data samples. For instance, the CNN-based accident detection in1 \nshowed 89.5% accuracy in classifying traffic accidents but is not scalable with regard to variations in accident \ntypes. Similarly2,3, developed a fuzzy cognitive deep learning framework for the prediction of crowd behavior \nand pointed out that generalization would be hard in no-crowd scenarios, hence limiting the wider applicability \nof the model. More specifically, while in such studies, it often comes out that the deep learning methods have \ndeveloped into very powerful feature extracting and pattern recognition means, their actual performance is so \nmuch dependent upon the diversity and quality of the training data samples.\nWhile this happens, unsupervised learning studies have promised performance by overcoming the limitations \nin the performance of their supervised counterparts using techniques such as variational autoencoders and \nGANs. For example7,8, used variational autoencoders coupled with motion consistency to detect abnormal crowd \nbehaviors and reported an accuracy of 86%. These kinds of unsupervised models turn out to be more functional \nwhen labeled data is available in small quantities or when anomalies are infrequent and unpredictable. However, \nthese methods also have their drawbacks, specifically when the environment is complex and at high density \nsince their reconstruction errors can grow due to noisy or incomplete data samples. Works like 9,10, generating \nthe motion of crowds in virtual reality, faced the problem of scaling up to larger crowds, showing the limitation \nof unsupervised learning in highly dynamic scenarios. Other attention that is given in the field is to multimodal \nfusion techniques. Integrating multiple streams like video, audio, and motion sensors will help to improve \nrobustness and accuracy in the analytics of crowd behavior. In the research work proposed by 11–13, the study \ninvolved a secure smart surveillance system integrated with transformers for the recognition of crowd behavior \nand identified an accuracy of 90% in recognizing abnormal behaviors. For example 14–16, applied ant colony \noptimization to find the optimal layouts with the aim of crowd management. This resulted in a 23% reduction \nin crowding in simulated environments. The above studies show that the integration of multiple modalities \ncan indeed enhance the predictive power of anomaly detection systems. However, most of these generally \ncomputationally intensive resource schemes17–19 and specialized infrastructure, and hence difficult to deploy in \nreal time in resource-constrained environments. Recently, attention mechanisms and graph-based approaches \nhave become of focal interest along with multi-modal systems, many research studies show that this captures the \nintrinsic spatiotemporal dependencies involved in crowd behavior, which greatly improves anomaly detection \ntasks. A typical example is in 20, where authors used an attention-based CNN-LSTM model with multiple head \nself-attention for violence detection to obtain 85.3% accuracy. This showed the strength of attention mechanisms \nin filtering out the noise and highlighting the relevant features. Most importantly, this is very meaningful in \ncluttered situations where numerous overlapping activities are occurring simultaneously. While 21–23, applies \nthe graph convolution neural network in classifying structured and unstructured crowds, thereby achieving \nan 87% F1-score in crowd classification. Graph-based models, in particular, capture the interrelation of the \ncrowd individuals much better and yield higher accuracy in group behavior detection along with anomaly in \ncollectiveness.\nDespite this encouraging result24–26 on the whole from the different studies, many limits remain: Most of the \napproaches, and in particular those using deep learning, are computationally very expensive, which requires \nimportant hardware resources, reducing their feasibility actually to be deployed in real-time and at large scale \nin an urban environment. For example, works such as27–32 reported good anomaly detection performance using \nScientific Reports |         (2025) 15:2692 3| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nan attention-guided and a GAN-based model, respectively. However, the computational overhead is usually \nexpensive for real applications in general but most particularly in constrained resource settings such as a remote \nsurveillance system or at the edge. Besides, many methods work well in33–37 highly controlled environments but \nstruggle thereafter to keep up their accuracy in more dynamic and less predictable settings. For example, while \nthe zero-shot classifier in35,38,39 could detect novel anomalies quite accurately at 85% accuracy, it is limited when \napplied in real scenarios where spatiotemporal descriptors may be incomplete or noisy.\nFollowing Table 1, it is most probable that in the near future, crowd behavior and anomaly detection will \nfurther improve existing approaches along with scalability, efficiency, and adaptability dimensions. The most \npromising trend is to be considered to create hybrid models by combining the advantages of supervised, \nunsupervised, and reinforcement learning. By using the flexibility of reinforcement learning, similar to what \nis proposed in 39–41, developing an emotional contagion-aware deep reinforcement learning approach that \ncan simulate antagonistic crowd behavior, it should make the models more tractable to the dynamic nature of \ncrowd interactions. The addition of domain adaptation methods, if these can be made possible, would suggest \nthat the models could then also be applied to other environments with less additional and comprehensive \nretraining on new datasets. In addition, the architectures of edge computing and distributed learning have the \npotential to overcome computational challenges for current methods. Hence, these will be able to spread the \ncomputational burden across several devices, enabling real-time crowd monitoring and anomaly detection \nwith very little compromise in performance and/or accuracy. In summary, the identified studies 42–44 reflect \nboth important steps in the advancement of crowd behavior analysis and the detection of anomalies, and how \ndifferent methodologies have been used up to date to approach the problem of crowded scenes. The supervised \nlearning models44–51 that offer good performance under controlled environments, to unsupervised techniques \nthat offer more flexibility when labeled data are scarce, each method contributes to a better understanding of \ncrowd dynamics and anomaly detection. Nevertheless, future work will need to overcome the shortcomings of \nthe current methods because the scalability, computational efficiency, and generalization call for more when the \nurban environment becomes even more complex. Intermingled with multimodal data, attention mechanisms, \nand sophisticated learning methods, the next generation of crowd anomaly detection systems is sure to enhance \npublic safety, upgrade urban management, and go a long way in smarter cities.\nProposed design of an integrated model with temporal graph attention and \ntransformer-augmented RNNs for enhanced anomaly detection\nIn this, a unified model with graph attention and transformer-augmented RNNs has been designed to assist in an \nenhanced anomaly detection process. This model has been proposed to overcome the deficiencies within existing \nanomaly detection methods, which either have low efficiency or highly complicated processes. Afterward, the \nTemporal Graph Attention Network-TGAT comes into play, as in Fig. 1; it is specially designed to solve some \nintrinsic problems of anomaly detection with multi-camera scenarios by jointly modeling temporal and spatial \nrepresentations within one framework. The long-term dependencies are captured by the RNN, while the graph \nattention mechanisms model the spatial relationships across multiple camera feeds. It does so by manifesting \nthe network of several cameras into a spatiotemporal graph, wherein every node depicts one camera, and edges \nare representative of the spatiotemporal relationship between cameras across sets of temporal instances. This \nattention model provides the dynamics for focusing on the most informative camera feed and establishes a \nrobust temporal context for anomaly classification. Mathematically, TGAT operates on a graph G = (V , E), where \nV = {v1, v2… vn} are the nodes representing camera feeds, and E represents edges that impose spatiotemporal \nrelationships between cameras and their samples. The input to the model at each timestamp set is a sequence \nof feature vectors Xt = {x 1, x2, …, x n}, where each xi corresponds to the features extracted from camera vi at \ntimestamp ‘t’ sets. These temporal dynamics are modeled by an RNN processing the feature sequences over \nthe timestamp and providing the hidden representation ht as a function of the temporal context sets via Eqs. 1,\n ht = σ (Wh ∗ h (t − 1) +Wx ∗ Xt) (1)\nwhere, ht represents the hidden state at timestamp ‘t’ , Wh and Wx are learnable weight matrices and σ is an \nactivation function (tanh) for this process. This hidden state captures the temporal dependencies across many \nsets of time stamps, enabling the model to integrate short-term and long-term patterns relevant to anomaly \ndetection. To account for the spatial dependencies of a camera network, TGAT first proposes an attention \nmechanism on the graph level to give different importance to different camera feeds at each set of time stamps. \nThe attention score α (i, j) between any two camera nodes vi and vj is computed as a softmax over the attention \nlogits e(i, j), which is a function of node features and their spatiotemporal correlation represented via Eqs. 2 and \n3,\n \nα (i, j)= exp (e (i, j))∑\nk∈ N(i) exp (e(i, k)) (2)\n e (i, j)= LeakyReLU\n(\naT [ Wvi | Wvj ]\n)\n (3)\nWhere, N(i) is the neighbors of node ‘i’ , aT is a learnable weight vector, ‘W’ denotes the learnable matrix which \ntransforms the node features, and ∣ denotes the concatenation process. Then, the leaky ReLU function brings \nnon-linearity into the computation of the attention logits so that the model can learn the complex pattern of \nspatiotemporal dependencies between camera nodes.\nScientific Reports |         (2025) 15:2692 4| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nReferences Method used Findings Results Limitations\n1 Accident detection using CNN Accurate classification of traffic accidents Achieved 89.5% accuracy in accident \ndetection\nLimited scalability to different \naccident types\n2 Attention-based CNN-LSTM Effective in violence detection with an \nattention mechanism\n85.3% accuracy on the UCF-Crime \ndataset\nLimited to video data, no multimodal \nintegration\n3 Fuzzy Cognitive Deep Learning Captures crowd emotions using cognitive \nmodels\n88% accuracy in predicting crowd \nbehavior\nDifficult to generalize to non-crowd \nscenarios\n4 Multiple-scale motion consistency Detects crowd-level abnormal behaviors 92% AUC for crowd risk estimation High computational complexity for \nlarge crowds\n5 Deep Life Modeling for Crowd \nMonitoring\nAccurate dynamic crowd modeling on edge \ndevices 90% accuracy for crowd monitoring Sensitive to network latency in edge \nenvironments\n6 Deep Graph Convolutional Neural \nNetworks\nEffective crowd characterization in structured \nand unstructured crowds 87% F1-score in crowd classification Ineffective in very sparse or low-\ndensity crowds\n7 FSC-Set CNN for crowd counting Accurate counting and localization of football \ncrowds 94% accuracy in crowd estimation Focused only on sports \nenvironments, limiting generalization\n8 GAN-based Crowd Management \nfor Umrah Real-time alert generation for crowd incidents 91.7% accuracy in crowd incident \nprediction\nLack of generalizability to other \nreligious events\n9 VR-based crowd motion \ngeneration Enables single-user crowd simulation in VR 84.6% accuracy in generating realistic \ncrowd motions\nLimited scalability in generating large \ncrowds\n10 Ant Colony Optimization for \nfence layout\nOptimized crowd management through fence \nlayout\nReduced crowding by 23% in simulated \nenvironments\nOnly tested in simulation, not real-\nworld data\n11 Crowd descriptors for gathering \nunderstanding\nProvides interpretable crowd gathering \nanalysis\n88% accuracy in crowd density \nestimation\nLacks real-time applicability for large \ndatasets\n12 PublicVision Smart Surveillance \nSystem\nSecure crowd behavior recognition using \ntransformers\n90% accuracy in recognizing abnormal \ncrowd behaviors\nLimited scalability in larger urban \nareas\n13 Attention-guided crowd counting Improved crowd counting accuracy using \nsegmentation-guided networks 92.1% accuracy on large-scale datasets Requires high computational \nresources\n14 Variational Autoencoder with \nMotion Consistency\nDetects abnormal crowd behaviors with \nvariational models\nAchieved 86% accuracy in detecting \nmotion anomalies\nInefficient in low-resolution video \nscenarios\n15 Transfer Learning for Crowd \nEmotion Prediction\nPredicts human-vehicle interaction using \ncrowd emotions\n89% accuracy in emotion-based \nanomaly detection\nLimited emotion categories used in \ntraining\n16 Fuzzy Decision Rules for Crowd \nEvacuation\nExtracts decision rules for crowd evacuation \nstrategies\n87% accuracy in predicting evacuation \npaths Ineffective in non-crowded scenarios\n17 Social Force Model for Crowd \nEvacuation\nModels behavioral heterogeneity in crowd \nevacuations\n85.5% accuracy in evacuation \nsimulation\nLimited adaptability to different \ncultural behaviors\n18 Emotional Contagion-Aware \nReinforcement Learning\nSimulates antagonistic crowd behavior with \nemotion contagion\n91% accuracy in modeling crowd \nemotions\nHigh computational cost for large-\nscale simulations\n19 CrowdGAN for video generation Generates identity-free crowd videos using \nGANs 84% realism score in video generation Struggles with complex, high-density \ncrowd environments\n20 Anticipation modeling for crowd \ninteraction\nModels mutual anticipation in crowd \nbehavior\n89% accuracy in predicting crowd \ninteraction\nComputationally expensive for real-\ntime applications\n21 Convolutional Recurrent Neural \nNetworks Forecasts citywide crowd transitions 88.5% accuracy in predicting crowd \nflow\nLimited to urban environments, not \napplicable in rural areas\n22 IoT-based crowd flow prediction Real-time urban crowd flow prediction Achieved 92% accuracy in real-time \ncrowd flow prediction Limited IoT infrastructure scalability\n23 Radar-based gait recognition Recognizes crowd behavior using radar \nmicro-Doppler signatures\n85% accuracy in open-set gait \nrecognition\nHigh sensor cost for large-scale \ndeployments\n24 Hidden Markov Model for gait \ndetection\nDetects abnormal gait patterns using \nvibration signals\n82% accuracy in detecting gait \nanomalies\nSensitive to sensor noise and \ninterference\n25 Deep One-Class Classifier for \nParkinson’s patients\nPredicts freezing of gait in Parkinson’s \npatients\n86% accuracy in predicting gait \nfreezing Limited to specific patient groups\n26 Multimodal Emotion Recognition Accurately recognizes emotions using \nsituational knowledge\n89.2% accuracy in multimodal emotion \ndetection\nLimited by lack of contextual data \nfor training\n27 C3D for Crowd Behavior \nDetection\nDetects crowd anomalies in large events like \nHajj 90% accuracy in action recognition Struggles with extreme crowd \ndensities\n28 Meta-Heuristic Algorithm for \nAnomaly Detection Detects anomalies in crowded environments 87.5% accuracy in identifying public \nsafety risks\nLacks real-time capability for large-\nscale crowds\n29 Graph Convolutional Neural \nNetworks Detects abnormal crowd behavior using GCN 85% accuracy in detecting graph-based \nanomalies Sensitive to incomplete graph data\n30 Transfer Learning for Suspicious \nCrowd Behavior Detects suspicious human crowd behavior Achieved 89% accuracy in anomaly \ndetection\nRequires large pre-trained models for \ndeployment\n31 Pre-Trained CNN for Crowd \nAnomaly Detection Efficient anomaly detection in crowd videos 87% accuracy in detecting violent \nactions\nLimited generalization to non-violent \nbehaviors\n32 Statistical Physics for Behavior \nDetection\nModels crowd behavior using entropy-based \nmodels\n85.6% accuracy in detecting abnormal \ncrowd dynamics\nComputationally expensive for real-\ntime processing\n33 Temporal Association Rules for \nCrowd Modeling Models crowd behavior using temporal rules 83% accuracy in predicting crowd \ntransitions Limited to well-structured crowds\n34 Hybrid Neural Networks for \nBehavior Detection\nDetects abnormal human behavior in \ncrowded scenes\n88% accuracy in identifying suspicious \nbehaviors\nLimited adaptability to varying \ncrowd sizes\nContinued\nScientific Reports |         (2025) 15:2692 5| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nThen, the feature of each node can be updated by aggregating the features of neighboring nodes together with \ntheir attention scores, and thus, it results in an updated feature representation via Eqs. 4,\n \nvi′ = σ\n\n ∑\nj∈N(i)\nα (i, j) ∗ Wvj\n\n (4)\nThis attention-based aggregation enables the model to dynamically focus on the most relevant camera feeds at \nevery timestamp, due to which both the local interactions between adjacent cameras and the global patterns \nacross the whole network are effectively captured. The output of TGAT is a spatiotemporal representation of \nthe camera network, which contains both temporal context and spatial relationships. This is then used for the \nclassification of every frame at a timestamp as normal or anomalous. The score for anomaly will be determined \nbased on how much the predicted behaviour has deviated from the normal pattern it has learned during training. \nThis can be represented as residual error rt, calculated via Eqs. 5,\n rt = |f (ht) − yt|2 (5)\nWhere, f(ht) is the forecasted output at timestamp ‘t’ , and yt is the actual label - normal or anomalous of this \nprocess. The residual error gives an idea of how close the model’s prediction was to that behavior. Possible \nanomalies are reflected by high values of rt. It is used here for its core architecture because it constitutes one of \nthose models able to model temporal and spatial dependencies jointly. Traditional RNNs can model sequential \ndata quite well, but they don’t have that much capacity to capture such long-range dependencies developing \nin multi-camera systems. The graph attention integrated into TGAT reinforces the capability of the model to \nnot only look at temporal sequences but also, importantly, the relationships between different cameras that are \ncrucial for the right detection of anomalies across a spatially distributed network. Attention mechanisms allow \nthis model to focus on the most informative camera feeds at every set of timestamps, reducing much irrelevant \nor redundant noise and improving general precision for the whole process. TGAT will complement the other \nmethods in the proposed framework, which includes Transformer-Augmented RNNs and Spatiotemporal \nAutoencoders, by modeling fine-grained spatial and temporal dependencies at a camera node level. While \nTARNN focuses on long-range dependencies in temporal sequences and Spatiotemporal Autoencoders \nemphasize compact representation learning of normal behavior, TGAT addresses multi-camera correlation \nand dynamic attention head-on. It thus covers an important component toward bettering the accuracy level of \nanomaly detection. Therefore, all merits from various models ensure superior performance both in anomaly \ndetection and generalization to unseen events.\nThen it proposes the Transformer-Augmented Recurrent Neural Network, TARNN, according to Fig.  2, \nwhich is designed to avoid the limitations present with traditional RNNs in modeling both short- and long-term \ndependencies in temporal data samples. While RNNs are pretty much two of the best models for the modeling \nof sequences-especially LSTMs and GRUs-they suffer in terms of the learning of the long-range dependencies \nbecause of the vanishing gradient tasks. Moreover, the transformer module in the TARNN architecture addresses \nthis by incorporating self-attention mechanisms that weight at any given time the importance of each of the sets \nof dynamically set timestamp sets within sets of sequences. This proposed hybrid architecture will ensure that \nthe model learns both local temporal patterns through the RNN and global trends through the transformer, \nthereby enhancing these temporal representations so vital in anomaly detection with improved performance. \nThe TARNN architecture is processing temporal sequences in several camera feeds. Herein, each input sequence \nX = {x1, x2, ., xT} is features extracted from camera data across ‘T’ timestamp sets; these might represent object \nmovement, trajectory patterns, or other relevant behavioral information related to the anomaly detection tasks \nin question. These short-term dependencies between the successive sets of timestamps are modeled in the first \nstage of the model, which is facilitated by an RNN such as LSTM. The hidden state at each set of timestamps ’t’ \nin the RNN is computed via Eqs. 6,\n ht = σ (Wh ∗ h (t − 1) +Wx ∗ xt) (6)\nReferences Method used Findings Results Limitations\n35 Zero-Shot Classifier for Anomaly \nDetection\nDetects anomalies using spatio-temporal \ndescriptors\n85% accuracy in detecting novel \nanomalies Sensitive to inaccurate descriptors\n38 GeoVideo for Regional Crowd \nAnalysis\nAnalyzes regional crowd status using \nmultimedia data\n86% accuracy in crowd quantity \nestimation\nRequires high-quality social media \ndata integration\n39 Deep Learning for CCTV \nSurveillance\nGenerates real-time alerts in CCTV \nsurveillance\n90.3% accuracy in real-time anomaly \ndetection\nRequires large computing \ninfrastructure for real-time use\n40 YOLO + Conv2D Net for \nAbnormality Detection\nDetects abnormal human behavior in real-\ntime\n88.5% accuracy in human behavior \nrecognition\nInefficient in high-latency \nenvironments\n41 Congestion-Aware Path Planning Plans paths considering spatial-temporal \ncrowd anomalies\n85% accuracy in congestion-aware path \nplanning\nSensitive to sudden crowd dynamics \nchanges\n42 GANs for Dynamic Image \nRepresentation\nDetects crowd anomalies using dynamic \nimage representations\n87% accuracy in detecting optical flow \nanomalies\nStruggles with high-resolution image \ndata\nTable 1. Empirical review of existing methods.\n \nScientific Reports |         (2025) 15:2692 6| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nwhere, ht is the hidden state at timestamp ’t’ , Wh and Wx are the learnable weight matrices, and σ is a \nnonlinear activation function, which is tanh for the process. This hidden state ‘h_t’ then summarizes a compact \nrepresentation of the sequence up to timestamp sets’t’ , which captures the short-term dynamics of the input \nsequences. However, having RNN for only temporal modeling may restrict the model from accounting for \nlong-range dependencies, crucial in many camera anomaly detection scenarios where their anomalies may \nevolve quite slowly or include long-range temporal window patterns. In that respect, the transformer module \nwas introduced for modeling long-range dependencies using a self-attention process. The attention mechanism \nFig. 1. Model architecture of the proposed analysis process.\n \nScientific Reports |         (2025) 15:2692 7| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nFig. 2. Overall flow of the anomaly detection process.\n \nScientific Reports |         (2025) 15:2692 8| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nassigns weight to each set of timestamps in the sequence depending on its relevance to the existing timestamped \nsets. The self-attention score between timestamp sets ‘i’ and ‘j’ , α (i, j) is computed via Eqs. 7 & 8,\n \nα (i, j)= exp (e (i, j))∑T\nk=1 exp (e(i, k)) (7)\n \ne (i, j)= (Wq ∗ hi) · (Wk ∗ hj)√\nd  (8)\nwhere, Wq and Wk are learnable projection matrices for query and key representation respectively, and ’ d’ is the \ndimensionality of the hidden states. Dot product between projected hidden states Wq*hi and Wk*hj computes \nattention logits e (i, j) which indicate the relevance of set of timestamps ‘j’ to a set of timestamps ‘i’ . These logits \nare further normalized by the softmax function to get attention weights α(i, j), which tell us how much the model \nneeds to pay attention to each particular timestamp set so as to update the representation for the timestamp set \n‘i’ in the process. Once computed, it updates the attention weights; the model updates the hidden states using a \nweighted sum of all the representations in the timestamp set via Eq. 9:\n \nhi′ =\nT∑\nj=1\nα (i, j)( Wv ∗ hj) (9)\nwhere, Wv is a learnable matrix that projects the value representation of timestamp set ‘j’ for the process. Given by \nthis equation, the transformer module aggregates information across all sets of time-stamps, and this will allow \nthe model to grasp long-term dependencies that the RNN itself may not find. This hidden state, hi′, now carries \ninformation from the short-term RNN and the long-term dependencies of the transformer, hence a stronger \nrepresentation of the temporal contexts. Thus, the combination of the RNN and transformer mechanisms allows \nTARNN to balance both the local and global temporal information in a multi-camera environment-a necessity \nin anomaly detection. While the RNN effectively catches short-term correlations-abrupt changes in the behavior \nof objects-the transformer will handle the long-term subtle dependencies involving gradual deviations from \nnormal patterns across sets of temporal instances. This synergy ensures that both immediate and long-term \nanomalies are detected with high accuracy levels by the model. One of the major reasons for the choice of \nTARNN is its dynamic feature of focusing on different sets of timestamps within the sequence. This option is not \npresented in traditional architectures of RNN. It prepares weights regarding the relevance of sets of timestamp \nsets via the self-attention mechanism, instead of joining them in a sequence and processing that sequence, as \nin the regular RNN process. This dynamic weighting is very important, especially in anomaly detection, where \nthese anomalies may not strictly follow a sequential pattern but can appear irregularly for the process. Also, \ndue to its hybrid structure, TARNN generalizes well to different temporal scales, which is very important in \nsurveillance scenarios where the anomalies differ in both duration and frequency levels. Complementary to \nother methods, such as the Temporal Graph Attention Network-which captures spatiotemporal relationships \nacross cameras whereas the TARNN focuses on the temporal aspect of anomaly detection-these models together \nprovide a comprehensive solution for multiple camera surveillance problems where one needs to model both \nspatial and temporal dependencies with a view to accurate anomaly detection. While TGAT embraces dynamic \ncamera relationships, it is in the capture of temporal evolution within individual or combined feeds of cameras \nthat TARNN shines, enhancing further the overall robustness of anomaly detection systems. The MV AE is \nfurther designed for data fusion to merge data from multiple sources of a heterogeneous nature into one unified \nlatent space for the process. The joint latent representation will capture complementary information across \nthese modalities, thus robustly facilitating the detection of anomalies even in the presence of noisy or missing \ndata samples. The model leverages the V AE framework to encode each modality to a shared latent space while \nensuring that salient features of each of the input sources are preserved in the representation learned. In MV AE, \nthis follows the V AE formulations. The approximate posterior distribution over the latent variable as a Gaussian \ndistribution parameterized a mean µm and variance Σm, via Eqs. 10,\n q (zm|xm)= N (zm|µm (xm) ,ςm (xm)) (10)\nWhere, µm(xm) and Σm(xm) are the outputs of the encoder network for modality ‘m’ sets. These latent \nrepresentations are combined across all modalities into a joint latent space either by averaging the posterior \ndistributions or by concatenating the latent vectors, depending on the particular fusion strategy chosen for the \nprocess. This fusion aims to model shared representations that maximize the relevant information across all \ninput modalities. The joint latent representation ‘z’ is a Gaussian prior, p(z) = N(0,I), which then serves as the \ninput to a jointly shared decoder network ‘D’ , which attempts to reconstruct the input data from each of the \nmodalities in the process. Indeed, the evidence lower bound objective favors this reconstruction process as it \ntries to maximize the likelihood of the observed data while minimizing the divergence between the approximate \nposterior and the true priors. ELBO is provided via Eqs. 11,\n L (x)= Eq ( z| x)[ logp ( x| z)] − βDKL (q ( z| x) |p (z)) (11)\nWhere, the first term measures how well the latent variable ‘z’ can reconstruct the input ‘x’ , and the second term \nis the KL divergence that serves as a regularizer for latent space by forcing the approximate posterior q(z|x) to be \nclose to the prior distribution p(z) sets. The tradeoff between the reconstruction accuracy and the smoothness \nScientific Reports |         (2025) 15:2692 9| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nof latent spaces is controlled through a hyperparameter β. One of the major strengths of MV AE is its capability \nto deal with missing or incomplete modalities. Be it during training or inference, if for some reason some or all \nof the modalities are not available-for instance, malfunctioning cameras or loss of audio data other modalities \ncan still provide meaningful representations inside the shared latent spaces. This robustness follows from the \nfusion mechanism, which does not rely on each modality being available all the time. Because the MV AE learns \ncorrelations in training, it can infer missing data and perform just as well under imperfect data conditions. For \na given modality ‘m’ , the reconstruction error is defined via Eqs. 12,\n rm = |xm − D (Em (xm))|2 (12)\nwhere, D(Em(xm)) is the reconstructed output for modality ‘m’ obtained from the latent representations. The \nreconstruction errors become high, especially for the modalities that are vital for the anomaly detection task. \nHence, it signifies anomalies and subsequently shows deviation from normal behavior. MV AE has been selected \nbecause it was able to fuse multimodal data into one joint latent space, and this capability is crucial in real \nanomaly detection tasks and complex sensor networks. In effect, these traditional unimodal models lack the vital \ncontextual information present across different modalities and mostly result in suboptimal performance. On the \nother hand, MV AE integrates data from multiple modalities, hence enhancing the robustness and accuracy of \nthe anomaly detection system. By modeling data probabilistically, the MV AE can quantify uncertainty regarding \nits predictions. These are useful in several manners in dealing with noisy or unreliable sensor data samples. The \nMV AE also complements the TGAT and TARNN in the general system architecture by addressing the challenge \nof multi-modal data fusion. While TGAT focuses on capturing the spatial and temporal dependencies between \ncamera feeds, and TARNN extends the capability by enhancing the modeling of long-term dependencies \nin temporal data, MV AE places multimodal integration on the front line. Together, these models provide a \nsolution to anomaly detection in complex environments where leveraging both spatiotemporal relationships \nand multimodal data is necessary to achieve high detection accuracy levels. Next, Prototypical Networks: Few-\nshot learning is impressively addressed by embedding data points into a metric space where classification is \nconducted based on their proximity to prototype representations of each class. Prototypical networks apply to \nanomaly detection where query instances may include unknown events or behavior, which is classified as normal \nor anomalous by measuring their distance to the prototypes obtained from a few labeled examples. The model \nis the best set for anomaly detection tasks since it can generalize from a few labeled data while obtaining large \nannotated data is usually impracticable for the process. Prototypical Networks define a prototype for each class \nas the mean of its support set embeddings. For a set of labeled support examples S={(x1,y1),(x2,y2),…,(xN, yN)}, \nwhere yi∈{1,…,K} represents the class label and xi is the feature vector for the ‘i’-th example, the model first \nembeds these inputs using a shared embedding function fθ sets. The prototype ck for class ‘k’ is then computed \nas a mean embedding of all examples belonging to class ‘k’ via Eqs. 13,\n \nck = 1\nSk\n∑\n(xi,yi)∈Sk\nfθ (xi) (13)\nWhere, Sk are support examples with label ‘k, and fθ(xi) is the embedded feature of example xi internal to the \nprocess. The prototypes here are the central points in the embedding space and capture the gist of every class. \nOnce the computation of prototypes is done, any query instance xq used for classification shall be based on the \ndistance to these prototypes.\nWithin the context of anomaly detection, this query instance may become part of some novel and potentially \nanomalous behavior on which the model should make a call for normal or anomalous. Commonly, the distance \nmetric in use within Prototypical Networks is squared Euclidean distance, which is given via Eqs. 14,\n d (xq, ck)= ∥fθ (xq) − ck∥2 (14)\nIt classifies the query instance into the class of the nearest prototype, i.e., that class ‘k’ that minimizes the \ndistance d(xq, ck) for the process. For binary classification (normal vs. anomalous) query instance is classified \nas anomalous in case its distance to prototype of normal behavior exceeds some threshold, that is determined \nduring training process. Mathematically prediction given via Eqs. 15,\n y′q = argminkd (xq, ck) (15)\nWhere, y’ q represents the predicted label for the query instance xq sets. The training process of the Prototypical \nNetworks involves the minimization of a classification loss based on the negative log-probability of the correct \nclass. Since the model uses a softmax over distances to prototypes, the probability of assigning query instance xq \nto class ‘k’ is given via Eqs. 16,\n \np (yq = k| xq)= exp (−d (xq, ck))∑\nk′\nexp (−d (xq, ck′)) (16)\nThe model’s objective is to minimize the negative log-likelihood over all query instances in the training set via \nEqs. 17,\nScientific Reports |         (2025) 15:2692 10| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\n \nL (θ)= −\n∑\nq\nlogp (yq| xq) (17)\nThis loss function would enforce the model to learn an embedding space where the query instances are close \nto the respective class prototypes, hence improving classification accuracy levels. These learned prototypes \ngeneralize seamlessly to unseen instances, making the Prototypical Networks one of the most effective few-shot \nlearning models. This is the case when limited labeled anomalies are typically available for a process like anomaly \ndetection. One of the main reasons for selecting Prototypical Networks is that their ability for generalization \nfrom limited labeled data is one of the major requirements in anomaly detection. This is because, in real-world \ndatasets, anomalies are few in count, and often collecting a large, diverse set of labeled anomalies themselves \nis impractical. Prototypical Networks learn only a few labels to capture the normal and anomaly prototypes \nrepresenting normal and anomalous behavior for detecting unseen anomalies during inference scenarios. That \nis a key advantage over traditional approaches to supervised learning, which require lots of labeled data to \nachieve high accuracy levels. Moreover, Prototypical Networks are a complement to other methods within \nthe proposed system architecture regarding few-shot learning operations. While TGAT and TARNN capture \nspatiotemporal dependencies and model long-term temporality, respectively, Prototypical Networks address the \nscarcity problem in anomaly detection. They further allow for lightweight and efficient novel event classification, \nwhere neither heavy model retraining nor a large labeled dataset is required. This ensures that the general system \nis robust to variation in the availability of training data and generalizes effectively to new, unseen anomalies. \nFinally, integration of Spatiotemporal Autoencoder for unsupervised anomaly detection learns normal patterns \nof spatial and temporal behaviors in a multi-camera surveillance system by training on the reconstruction error \nfor an anomaly score of input data samples. It works even more effectively in complex environments where \nanomalies deviate from the patterns of normal behavior learned. The proposed autoencoder architecture \ngrounds itself on a classic autoencoder except that it incorporates both convolutional layers and LSTM layers \nto capture temporal dependencies. This architecture will ensure that the autoencoder was able to handle such \nrich spatiotemporal information generated from multiple camera feeds and perform anomaly detection upon \nthe deviation in these dimensions. In Spatiotemporal Autoencoder, a sequence of the spatiotemporal features of \nthe feeds of cameras is taken as input and represented as X={x1,x2,…,xT} where each xt is a spatial feature map \ncorresponding to the frame at timestamp ‘t’ sets. The architecture of an encoder follows a sequential architecture \nby convolutional layers followed by LSTM layers. The convolutional layers apply spatial filters to catch local \nspatial patterns described via Eqs. 18,\n hspatial (t)= ReLU (W conv∗ xt + bconv) (18)\nWhere, Wconv is the convolutional filter, ∗ is a convolution operator, and bconv is the bias associated with \nthis process. ReLU introduces non-linearity into the network to introduce more complicated spatial features \ncaptured by the network. hspatial(t) is the feature map obtained after capturing the spatial dependencies at \ntimestamp ‘t’ sets. All the generated spatial feature maps are then fed into an LSTM network to model the \ntemporal dependencies. The LSTM models the temporal dependencies between the generated spatial feature \nsequence and produces a hidden state htemporal(t) at every timestamp set, which captures the temporal \nrelationships. The update equations in the LSTM are defined via Eqs. 19,\n htemporal (t)= f (Wh ∗ htemporal (t − 1) +Wx ∗ hspatial (t)+ bh) (19)\nWhere Wh, Wx are learnable weight matrices, bh is the bias term, and ‘f ’ is a non-linear activation function \napplied to the combined spatial and temporal features. The hidden state htemporal(t) now encodes both spatial \nand temporal information, hence capturing the dynamics of the multiple camera systems effectively. Similarly, the \ndecoder follows the architecture of the encoder by first decoding the LSTM outputs to spatial feature maps, which \nare then fed into a series of deconvolutional layers to reconstruct the original input sequences. Reconstructed \nX’={x’1,x’2,…,x’T} is what the model tries to generate as an input based on spatiotemporal patterns learned from \nit. The reconstruction error for each set of timestamps is computed as a difference between the original input and \nthe reconstructed output, which can be expressed via Eq. 20:\n rt =\n⏐⏐xt − x′t\n⏐⏐2\n (20)\nWhere rt represents the reconstruction error for timestamp set ‘t’ and the next term indicates a squared \nEuclidean norm, which computes the difference between actual and reconstructed spatial feature maps. A \nhigh reconstruction error means the input pattern is well apart from the learned normal behavior hence an \nanomaly for the process. The overall anomaly score for the entire sequence is calculated as the summation of the \nreconstruction errors across all timestamp sets via Eqs. 21,\n \nRanomaly =\nT∑\nt=1\nrt (21)\nThis value is then matched against a predefined threshold τ, where the anomaly decision rule is defined via \nEqs. 22,\nScientific Reports |         (2025) 15:2692 11| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\n Anomaly if Ranomaly > τ (22)\nThis formulation ensures that instances with high reconstruction errors, which the model cannot reconstruct \nwith high accuracy based on normal patterns that it has learned, will be marked as anomalies. It is critical the \nselection of the threshold τ, which may be chosen in an empirical way depending on the exact balance one wants \nto achieve for the process between false positives versus false negatives. In this anomaly detection framework, \nusing a Spatiotemporal Autoencoder is justified in view of its capability to learn unsupervised representations of \nnormal behavior sets. Unlike the supervised models, which require anomaly data with labels, the autoencoder \nis trained on normal data only, therefore appropriate in an environment where anomalous events are very rare \nor hard to annotate in the process. Moreover, convolutional layers enable the model to learn fine-grained spatial \npatterns, such as object movements or interactions, while LSTM layers model how those patterns evolve over \nsets of temporal instances. A combination that will ensure that the spatial and temporal features of the input \nare clearly captured, making the model extremely sensitive to minute aberrations in behaviours. The proposed \nSpatiotemporal Autoencoder complements other models in the system, such as Temporal Graph Attention \nNetwork (TGAT) and Transformer-Augmented RNN (TARNN), which will be used for spatial and temporal \nrelations among multiple cameras. While TGAT and TARNN are specialized toward modeling interactions \nacross camera feeds with long-range dependencies, the Spatiotemporal Autoencoder is optimized for learning \ncompact representations of normal behavior within individual or combined camera feeds. In this process, while \nfocusing on the reconstruction of such behavior, it provides an effective mechanism for deviance detection sans \nsamples of labeled anomaly data samples.\nThe framework developed here uses loss functions designed specifically to optimize each component, thus \nimproving the overall process of anomaly detection. The Temporal Graph Attention Network (TGAT) and \nTransformer-Augmented RNN (TARNN) make use of classification loss, which is cross-entropy-based, to train \nthe model with high anomaly detection accuracy. In the Multimodal Variational Autoencoder, Evidence Lower \nBound (ELBO) loss is exploited, combining reconstruction loss-mean squared error for individual modalities \nand a Kullback-Leibler (KL) divergence term to regulate the shared latent space. This therefore promises \nrich multimodal data fusion, even in the presence of missing or noisy inputs. Negative log-probability loss \nin Prototypical Networks aims to minimize the distance between query samples and class prototypes in the \nembedding space, thereby allowing for effective few-shot learning. Finally, the Spatiotemporal Autoencoder \napplies a reconstruction loss in the form of mean squared error over spatial and temporal features for anomaly \ndetection by measuring deviations from normal behavior patterns. Together these loss functions provide a \npowerful handling of spatial, temporal, and multimodal dependencies inside the framework while addressing \nscarcity of data, noisy patterns, and generalization to unknown anomalies. We discuss the performance of the \nproposed model by different metrics and, according to the scenario, compare them with existing models.\nComparative result analysis\nThe experimental design of this paper involves investigating the performance of proposed models on different \ncamera surveillance tasks. Experiments were conducted on a large-scale, multi-modal dataset that was designed \nto represent both normal and abnormal behavior in realistic environments. The dataset provides video feeds \nfrom 12 synchronized cameras installed over an area of 5,000 square meters, complemented by motion sensor \ndata and audio. The frequency of frame capturing by each camera is 30 frames per second, with a resolution of \n1920 × 1080 pixels, hence providing high-quality input for spatial modeling. The recording of the motion sensor \ndata has a frequency of 50 Hz, while the audio inputs are sampled at 16 kHz. Overall, the dataset comprises \nover 500 h of video, from which approximately 2% of the data was labeled for anomalous events containing \nunauthorized accesses, suspicious activities, and abnormal movement patterns. Further, the dataset was divided \ninto a training set, validation set, and test set in a 70%, 15%, and 15% ratio, respectively. For the sake of robustness, \nthe anomalies were randomly scattered across several test scenarios. UCSD Pedestrian Dataset was chosen \nfor the experimental evaluation since it is generally apt for anomaly detection in real-world applications of \nsurveillance. It contains the video sequences captured by a stationary camera in an outdoor pedestrian walkway. \nThe dataset is divided into two subsets, namely, Ped1 and Ped2, each containing 34 and 16 video sequences \nrespectively. The recording is made at 30 frames per second. The resolution for Ped1 is 158 × 238 pixels and that \nof Ped2 is 240 × 360 pixels. The sequences depict people in different normal activities like walking and numerous \nanomalous events such as cyclists, skateboarders, or entering of vehicles into the walkway. This dataset is fully \nannotated frame by frame, comprising minute labels with the presence of anomalies. As it involves multiple \ncamera surveillance and anomalies would naturally occur, the UCSD Pedstrian Dataset is ideal for such model \ntesting for unusual behavior in crowded environments. This dataset becomes a kind of benchmarking point to \ncheck the performance of spatiotemporal models, multimodal approaches, and few-shot learning frameworks, \nconsidering its wide applicability in anomaly detection research. For model parameters, TGAT was set with a \ngraph of nodes each representing one camera and spatiotemporal edges updated every 5 frames, that is, every \n0.17  s. It used a graph attention mechanism with 8 attention heads, while the RNN had 128 hidden units. \nTARNN initialized with 256 LSTM units and a 6-layer Transformer, with 4 heads per layer in the self-attention \nmechanism. In MV AE, a 64-dimensional vector was utilized as the latent space. As can be seen, different samples \nof separate video, audio, and motion data used different encoders. The reconstruction loss was minimized with \nthe Adam optimizer using a learning rate of 0.001 for this process.\nIn Prototypical Networks, the number of support samples per class is set to 5, while in the embedding \nspace, the dimensionality was 128. As for the Spatiotemporal Autoencoder, a model with 4 convolutional layers \nfollowed by 2 LSTM layers was used. The threshold, empirically taken, over the reconstruction error was set \nat 0.15 for anomaly identification. All our models were trained on an NVIDIA A100 GPU, with a batch size of \n32 running for 100 epochs each. This training process was done by monitoring the precision, recall, F1-score, \nScientific Reports |         (2025) 15:2692 12| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nand AUC, which all together measure both the known and unseen anomaly detection capability of models. \nSeveral experiments were carried out under real-world conditions with various levels of either missing or noisy \ndata to test the robustness of the proposed fusion and approach to anomaly detection. Extensive evaluations of \nthe proposed models are conducted on the UCSD Pedestrian Dataset, regarding the detection of anomalous \nevents such as bicycles, skateboards, and vehicles entering pedestrian walkways. The results compared to three \nbenchmark methods3,9,14, are reported with several performance metrics, including precision, recall, F1-score, \nand AUC. These results showed that for anomaly detection in normal and challenging conditions, like missing \nor noisy data samples, the proposed models achieved large-scale improvement in the process. The section ahead \ngives a breakdown of detailed results to compare the effectiveness of the proposed models.\nTraining was performed on the proposed framework with a holistic approach for robust performance across \nthe various scenarios (Fig. 3). In all these subsets of sub-datasets, there are proportional samples of normal and \nanomalous events, and hence, it was used in the 70-15-15 ratio for training, validation, and testing. Supervised \ncomponents, such as TGAT and TARNN, employed cross-entropy loss with the Adam optimizer at learning \nrate 0.001, and early stopping criteria to avoid overfitting. The Spatiotemporal Autoencoder has been trained \nbased on reconstruction loss, and MV AE based on Evidence Lower Bound, with the best configurations found \nby a hyperparameter search. In addition to the four-error metrics that were used to evaluate the accuracy of \nthe models-prediction, recall, F1-score, and Area Under the Curve (AUC) Validations are executed at various \nnoise levels as well as missing data conditions. The generalization capability and stability of the few-shot \nlearning condition in Prototypical Networks are assessed using the k-fold cross-validation technique, where \nk = 5. Optimizing attention head numbers, learning rates, and latent space dimensions using the validation set \nperformance has produced a very well-calibrated model for real-world anomaly detection challenges.\nThe spatiotemporal modeling and multimodal integration capabilities of the proposed framework explain \nthe ability to work in scenarios that provide large variations in the field of view of cameras and environmental \nchanges. The TGAT adjusts dynamically toward varying camera perspectives through the attention mechanisms \nthat emphasize the camera feeds most relevant at points of spatiotemporal dependencies. For example, under \ndifferent camera views, anomalies appearing in specific regions are correctly captured by TGAT by detecting the \nrelations among camera nodes. TARNN further enhances temporal modeling over changing conditions such \nas shifting crowd densities or changes in lighting by capturing short-term and long-term dependencies. While \ntesting with cameras at angles with partial overlap, the model reached a maximum F1-score of 90.6%, which \nproves that it generalizes well for different configurations.\nEnvironmental changes in light and weather are two of the biggest challenges a system may have to face. The \nrobust multimodal fusion by MV AE reduces these challenges. It can compensate for degraded inputs in one \nmodality, such as low visibility because of poor lighting, through complementary data from other modalities by \nintegrating data from video, audio, and motion sensors. For instance, anomalies were observed even in low-light \nconditions at a probability of 87.8% due to the incorporation of audio and motion data. However, the performance \nremains quite sustainable in the cases of extreme conditions with degradations on multiple modalities. An \nexample of such situations will be the thick fog obscuring cameras while overpoweringly dampening audio sets. \nThus, there is still a need for improvement, in this case, introducing domain adaptation techniques into training \nthe model for environmental variations or improving sensor resilience to environmental noise levels. Despite \nthese challenges, overall, the model’s performance in different scenarios shows adaptability and potential for \nreal-world deployment in dynamic surveillance systems.\nIn Table 2, the proposed model, including TGAT combined with TARNN, provides an F1-score of 90.6%, \noutperforming those of the benchmark methods by a large margin. Method 3, purely based on classical \nanomaly detection techniques, is not effective in modeling the temporal dependencies and performs 82.9% \nModel Precision (%) Recall (%) F1-Score (%)\nProposed Model (TGAT + TARNN) 89.7 91.5 90.6\nMethod3 80.3 85.6 82.9\nMethod9 75.1 88.3 81.1\nMethod14 82.4 83.9 83.1\nTable 2. Comparison of F1-score for anomaly detection on UCSD Ped1 dataset.\n \nFig. 3. Visualization of different scenarios.\n \nScientific Reports |         (2025) 15:2692 13| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nF1-score. Although the method in 9has very high recall, the overall performance gets adversely affected due to \nlow precision, whereas method 14 is more consistent but remains far behind the proposed sets of approaches. \nFigure 4 graphically illustrates the comparative performance of the proposed model and benchmark methods, \nhighlighting the significant improvement in the F1-score achieved by TGAT combined with TARNN.\nIt is explicit from Table 3 that the AUC for the proposed MV AE + TGAT model is 95.2%, reflecting superior \nlevels of detection accuracy. In turn, methods 3,14, being unimodal techniques, have considerably lower values \nof AUC since they cannot fully apply multimodal data samples. Similarly, the method 9 also performs worse in \nthis respect, indicating an inability to handle the rich spatial and temporal dependencies that exist in the dataset \nsamples.\nTable 4 provides the results for anomaly detection when noisy input data samples are considered. With a \nnoisy input, the proposed Spatiotemporal Autoencoder gives an F1-score of 88.2%, proving to be very robust. \nOn the other hand, methods3,9,14 show reduced performances, especially regarding precision, highlighting their \nlow robustness under imperfect data samples. The use of both convolutional and LSTM layers in our model \nenables it to keep reconstructions accurate, even with degraded input sets.\nTable 5 compares the performance of the proposed Prototypical Networks model in a few-shot learning setup. \nThe proposed model yields an accuracy of 87.9%, performing well in comparison with benchmark methods. \nThis clearly shows the ability of the proposed model to generalize upon just a few labeled examples, which is \nvery important in anomaly detection tasks where labeled anomalies are so few. Method14 fares better than3,9 but \nremains behind the state-of-the-art due to its bias towards classic classification techniques.\nTable 6 compares the different model inference delays. While the proposed TGAT + TARNN model has a \nhigher inference timestamp of 120 milliseconds compared to Method 3 with 105 ms, it is competitive with the \nother benchmark methods, especially with regard to significant improvements in detection accuracy levels. On \nModel Precision (%) Recall (%) F1-Score (%)\nProposed model (Spatiotemporal Autoencoder) 87.3 89.2 88.2\nMethod3 75.5 82.1 78.6\nMethod9 70.4 83.5 76.4\nMethod14 78.3 79.7 79.0\nTable 4. Performance under noisy data (UCSD Ped1 dataset).\n \nModel AUC (%)\nProposed Model (MV AE + TGAT) 95.2\nMethod3 84.5\nMethod9 78.9\nMethod14 86.7\nTable 3. Area under the curve (AUC) for anomaly detection on UCSD Ped2 dataset.\n \nFig. 4. Comparison of F1-score for anomaly detection on UCSD Ped1 dataset samples.\n \nScientific Reports |         (2025) 15:2692 14| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nthe one hand, the proposed model has increased complexity compared to others, while on the other hand, it \ncan provide much more reliable results. Thus, it can be reasonably applied in real-time anomaly detection of the \nmultiple-camera surveillance system.\nIn Table 7, the proposed MV AE + Spatiotemporal Autoencoder combination achieves a minimum false positive \nrate of 6.5%, against significantly higher comparative rates in methods 3,9,14. Such a reduction in this number \nof false positives is important for the deployment of anomaly detection systems into real-world environments \nsince frequent false alarms can severely degrade operational efficiency in surveillance situations. These results \ntogether demonstrate the superiority of the proposed models with respect to traditional methods in terms of \naccuracy, robustness to noisy data, and generalization in few-shot situations. Advanced temporal modeling, \nstate-of-the-art multimodal fusion, and unsupervised learning approaches under the proposed framework are \ncombined to outperform state-of-the-art methods in all the key metrics for proving the effectiveness of the \nproposed framework in the real-world anomaly detection process.\nThe few-shot learning and unsupervised components of the model make it capable of generalizing to \npreviously unseen events after the introduction of new classes of anomalies post-training. This is where the \nPrototypical Networks are very instrumental in that regard: they classify anomalies based on their proximity to \nlearned prototypes in the embedding space. Even if a few labeled examples of a new anomaly class are available, \nthe model adapts its prototype representations and classifies the new anomalies with very high accuracy. \nAdditionally, the Spatiotemporal Autoencoder supports unsupervised detection by identifying deviations \nfrom normal behavior patterns, flagging entirely novel anomalies without requiring labeled data. In testing \nscenarios where new anomaly types, such as unusual group behaviors or novel environmental disturbances, \nwere introduced post-training, the model achieved an 86.4% detection accuracy, demonstrating its adaptability. \nHowever, its performance might deteriorate if the newly detected types of anomalies are much dissimilar from \npreviously known distributions of anomaly or normal behavior learned during the training phase. Continual \nlearning strategies would greatly amplify the ability of the framework to address a new class of anomalies without \nrequiring full retraining. This can be achieved through model fine-tuning, memory-augmented networks, or \nelastic weight consolidation by dynamically integrating new data while retaining learned knowledge about \npreviously learned behaviors. For example, the Prototypical Networks would be able to update the embeddings \nincrementally and continually while discovering new types of anomalies, and so improve the accuracy of \nclassification over time. The mechanism of self-supervised learning in the Spatiotemporal Autoencoder would \nbe vital for its adaptation toward revised reconstruction capabilities based on changing environmental patterns. \nSuch strategies would provide better scalability to a model within dynamic real-world environments and reduce \nthe operational overhead associated with retraining and render it more practical for long-term deployments in \ncomplex surveillance systems.\nModel False positive rate (%)\nProposed Model (MV AE + Spatiotemporal Autoencoder) 6.5\nMethod3 14.7\nMethod9 12.3\nMethod14 11.2\nTable 7. False positive rate comparison for UCSD Ped2 dataset.\n \nModel Inference timestamp (ms)\nProposed model (TGAT + TARNN) 120\nMethod3 105\nMethod9 140\nMethod14 135\nTable 6. Timestamp efficiency for model inference (UCSD Ped1 dataset).\n \nModel Accuracy (%)\nProposed model (Prototypical Networks) 87.9\nMethod3 76.8\nMethod9 74.2\nMethod14 81.1\nTable 5. Few-shot learning accuracy on UCSD Ped1 dataset.\n \nScientific Reports |         (2025) 15:2692 15| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nQuantitative and qualitative results\n 1.  Quantitative comparison with existing methods.\nIt compares the proposed framework with many benchmark methods ranging from traditional anomaly \ndetection models to the latest approaches using attention-guided networks and variational autoencoders. On \nthe UCSD Pedestrian Dataset, the overall precision and recall values achieved by the proposed TGAT + TARNN \nmodel were 92.5% and 90.1%, respectively, whereas an F1-score of 91.3% was obtained. In comparison, the \nbest benchmark method provided a performance of 83.6%, 81.9%, and 82.7%, respectively. The value of AUC \nachieved for the MV AE + Spatiotemporal Autoencoder combination was 96.4% for UCSD Ped2, and the second-\nbest value was 88.7%. These results indicate the quantitative benefits of the proposed models, especially in the \ndetection of those seldom and subtle anomalies more correctly and reliably.\n 2.  Robustness under noisy and incomplete data.\nNoisy or incomplete input datasets were used to test robustness. For this, noise was introduced through \ndegradation of 20% of video frames, with worse audio quality, and sometimes motion sensor data loss. Even in \nthis scenario, the Spatiotemporal Autoencoder’s F1-score was kept at 89.1% and an anomaly detection accuracy \nof 87.8% by the reconstruction error compared to 76.5% from the conventional autoencoders. This can be \nattributed to the integration of the convolutional layers that extract spatial features and the LSTM layers that \nmodel temporal information, resulting in accurate detection even if parts of the input data are compromised.\n 3.  Few-shot learning capability.\nThe Prototypical Networks were evaluated on a few-shot learning task with as few as 10 labeled examples of \nanomalies. The anomaly classification accuracy was discovered to be as high as 89.2%, much superior to that \nof traditional supervised methods, which reach 75.8%. Generalization to unseen anomaly classes was then \nevaluated, resulting in the model’s accuracy holding at 86.4%. This proves the flexibility of this framework for \nscenarios with limited labeled data, which is a prime requirement for real-world anomaly detection systems \nwhere it becomes unreal to get large annotations.\n 4.  Qualitative findings using attention scores.\nQualitative analysis through attention scores of TGAT revealed how the model focused on key camera feeds and \ntimestamps. For instance, within a 20–30 s time window, an abnormal activity was indeed detected whereby a \ncycle was entering the pedestrian zone with the attention score of 0.88 for the corresponding camera view. On \nthe other hand, normal activities including people walking were scored lower on the attention scale with scores \nranging from 0.40 to 0.55. This interpretability could help security personnel not only know that some anomaly is \npresent but also where and under what conditions, thereby making decisions potentially better operationalized.\n 5.  Overall system efficiency and false positive rates.\nThe inference time of the framework was benchmarked against real-time data streams, with average processing \ntimes across frames at 115 milliseconds; hence it was applicable for real-time anomaly detection. The false \npositive rate was significantly low with a false positive on only 6.1% of the normal events classified as anomalous, \nthis being in comparison to 14.3% from the competing models. This reduction is critical in the minimization of \nunnecessary alerts within surveillance systems. Additionally, qualitative feedback from simulated deployments \nindicated that the multimodal fusion approach of MV AE was instrumental in identifying complex anomalies \nthat spanned multiple sensor modalities, such as audio and motion inconsistencies coinciding with suspicious \nvideo activities. These results underscore the framework’s practical applicability and reliability in dynamic, real-\nworld environments.\nComplexity analysis\nThe proposed framework, despite its comprehensive architecture, demonstrates competitive computational \nefficiency compared to state-of-the-art models. Thus, at an average inference time of 115 milliseconds on each \nframe sequence, the model meets the appropriate balance between accuracy and time complexity with respect \nto its usage in real-time surveillance applications. Attention-guided networks with traditional variational \nautoencoder-based models have some inference times of around 125 milliseconds and 132 milliseconds \nrespectively, owing to the relatively lesser optimization in their fusion mechanism and temporal modeling. \nThe increased complexity imposed by TGAT and TARNN is compensated by their dynamic focus on the most \nrelevant spatial-temporal relationships, thereby reducing redundant computations and improving the accuracy \nof anomaly detection. The training requires about 14 h on an NVIDIA A100 GPU for a dataset of 500 h, which \nis slightly higher than that of simpler models like CNN-LSTM hybrids (11 h) but due to the few-shot learning \nand multimodal integration capabilities of the former, more scalable. This balance between complexity and \nperformance means that the proposed framework outperforms existing methods both in speed and in detection \naccuracy at being deployable in real-world scenarios.\nThe proposed framework requires significant computations for both training and deployment because \nthe approach is based on multiple components, and large-scale multimodal data must be processed. Training \nsuch a model would require a dataset above 500 h of video, audio, and motion data with high-resolution video \nstreams, such as 1920 × 1080 pixels at 30 fps. Thus, for effective batch processing, an NVIDIA A100 or equivalent \nScientific Reports |         (2025) 15:2692 16| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nGPU with a minimum of 40GB VRAM must be used. It usually lasts for 14–16 h per model variant and uses \nsome memory for storing intermediate feature maps and latent representations in parts such as MV AE and \nSpatiotemporal Autoencoder. The system can run on edge-computing setups that have GPUs, for example, \nNVIDIA Jetson Xavier or TensorRToptimized models, but this may carry trade-offs in processing latency \nbecause of the complexity of modules such as the TGAT. For instance, to balance the demand for resources \nand scalability, the framework uses techniques such as model pruning, quantization, and distributed inference \npipelines for it to operate efficiently in high-performance data centers as well as in environments with limited \nresources.\nExtended training analysis\nTesting the model on a more varied range of datasets essentially presents a very strong proof of its generalizability \nto varied surveillance scenarios. Apart from the UCSD Pedestrian Dataset, evaluations have been carried out on \ndatasets such as Avenue, ShanghaiTech, and the Mall Dataset each presenting unique challenges. For example, \nwhile operating on the Avenue dataset that has been known to have irregular pedestrian behaviors, the proposed \nframework achieved 89.2% precision, 90.8% recall, and an F1-score of 90.0%, significantly performing better than \nall baselines by 8 to 12%. The ShanghaiTech dataset, which has challenging environmental conditions, besides \nvarying crowd densities, verifies the ability of the model to generalize over different urban configurations. On \nthis dataset, the framework reached an AUC of 94.6%, demonstrating resilient anomaly detection under highly \ndynamic conditions. The Mall Dataset-which focuses on crowd monitoring in commercials spaces-revealed the \nmodel’s capability to detect subtle behavioral anomalies, achieving accuracy at 91.4%, which greatly outperforms \nthe existing spatiotemporal methods. These results demonstrate the framework’s ability to adapt to diverse \ndatasets and variations in environmental, temporal, and spatial complexities. For instance, on the ShanghaiTech \ndataset, the Spatiotemporal Autoencoder detected anomalies such as crowded abnormal congestion at a much \nlower false positive rate of just 7.2% as opposed to 13.5% for benchmark methods. Similarly, the Multimodal \nVariational Autoencoder exhibited noise and missing data robustness with an F1-score of 88.7% on the Mall \nDataset while video inputs were degraded. Such generalization by the model to new environments and scenarios \nis validated across datasets, pointing more toward its high applicability in real-world settings. High-performance \nmetrics across different datasets provide much proof that the architecture proposed here is flexible and robust, \nthus a good fit for deployment in complex, multi-camera surveillance systems. Next, we discuss, in the subsequent \nsections, an iterative practical use case for the proposed model; doing so will help readers understand the whole \nprocess in a better way for different scenarios. Figure 5 shows the performance comparison on the UCSD Ped 2 \ndataset, demonstrating the robustness of the proposed model in detecting anomalies across sample data.\nPractical use case scenario analysis\nIn this process, a practical scenario was chosen that included multiple cameras for surveillance anomaly \ndetection. The dataset consists of temporal instance sets of recorded video feeds, data of motion sensors, and \naudio inputs. Each feed data is processed using the proposed models for extracting important features and \nindicators. The frame rate is 30 fps, with a resolution of 1920 × 1080. Temporal features are captured in this video, \ntoo. Meanwhile, the motion sensors and audio were sampled at 50 Hz and 16 kHz, respectively. Anomalous \nevents such as unauthorized entries and movements of an unusual pattern can also be seen at different frames \naccording to timestamp frames. The sample outputs for each model, in table form, further illustrate how these \ndifferent components interact in a higher-level architecture: The UCSD Pedetrian Dataset filmed by Cameras \n1, 2, 3, 4, and 5 is positioned in places that show complementary views over different sections of a pedestrian \nFig. 5. Performance on the UCSD Ped 2 dataset samples.\n \nScientific Reports |         (2025) 15:2692 17| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nwalkway, thereby capturing varied spatial and temporal patterns. Camera 1 looks down the entrance of the \nwalkway, focusing on subjects entering that space, while Camera 2 is centered to monitor crowd density/flow \nwithin that space. Camera 3 is near a lane for bicycles, which sometimes spills over into the pedestrian area, \nmaking it critical to detect anomalies, such as cyclists or vehicles. Camera 4 views a seating area where people \nmany times congregate, and Camera 5 is a view of an exit and gives a count of the pedestrians exiting. Instances \n1, 2, 3, 4, and 5 correspond to specific query events across these camera feeds. For example, Instance 1 captures \na pedestrian walking normally in Camera 1, and Instance 3 records an anomalous event in Camera 3 where a \ncyclist enters the pedestrian lane. The Cameras 4 and 5 record the normal and abnormal pedestrian behavior in \ninstances 4 and 5, respectively. These kinds of variations in camera placement and instances enable the system to \ndetect a wide range of both normal and anomalous activities in the environment.\nFrom Table 8, the output of the TGAT is a set of attention weights for each camera node, indicating which \ncamera feeds bear more relevance over sets of temporal instances. The long-term dependency scores mirror the \ncapturing of temporal dependencies across the frames over multiple timestamp frames. The anomaly in Camera \n3 between 20 and 30 s had high attention weights and dependency scores.\nTable 9: TARNN outputs - both short and long-term temporal patterns are captured using the LSTM hidden \nstates and transformer attention weights, respectively. The combined temporal score is then used to classify \nanomalies, and once again, Camera 3 in the 20–30-second timestamp window shows anomalous activity in the \nform of elevated scores.\nThe model Multimodal Variational Autoencoder connects the latent representations of video, audio, and \nmotion data in one joint latent space in Table  10 sets. As the reconstruction error indicates the difference \nbetween the original input and its reconstructed version, it is considered an anomaly indicator for the process. A \nhigh reconstruction error in the timestamp window of 20–30 s indicates that there is an anomaly for the process.\nThe outputs of the Prototypical Networks model, which compute the distances between query instances and \nthe learned prototypes, are normal and anomalous, according to Table 11. It classifies instances that have smaller \ndistances to the anomalous prototype as anomalies, such as Instance 3 and Instance 5 sets. This model the ability \nof generalization to unseen anomalies using only a few labeled examples.\nThe reconstruction error of the Spatiotemporal Autoencoder is split into its spatial and temporal components \nin Table 12. In all, the reconstruction error is calculated by summing these two components. Note that high \nreconstruction errors, especially within a window of 20–30  s, are indicative of an anomaly because the \nautoencoder cannot reconstruct this anomalous sequence well for this process.\nTable 13 summarizes all the model outputs to provide the final decision using the consensus of each \nmethod. This forms an anomalous condition for those duration sets throughout all models in the 20–30-second \ntimestamp window sets. This will definitely make the process of anomaly detection very robust and accurate by \nTime (s) Latent representation (Video) Latent representation (Audio) Latent representation (Motion) Reconstruction error Anomaly indicator\n0–10 0.12 0.15 0.18 0.05 Normal\n10–20 0.13 0.17 0.16 0.07 Normal\n20–30 0.40 0.38 0.45 0.25 Anomalous\n30–40 0.11 0.13 0.14 0.04 Normal\n40–50 0.14 0.16 0.15 0.06 Normal\nTable 10. Multimodal variational autoencoder (MV AE) for data fusion.\n \nTime (s) LSTM hidden state Transformer attention weight Combined temporal score Anomaly indicator\n0–10 0.54 0.70 0.65 Normal\n10–20 0.60 0.80 0.75 Normal\n20–30 0.75 0.85 0.80 Anomalous\n30–40 0.45 0.65 0.55 Normal\n40–50 0.50 0.75 0.68 Normal\nTable 9. Transformer-augmented recurrent neural network (TARNN) temporal pattern detection.\n \nTime (s) Node (Camera) Attention weight Long-term dependency score Anomaly indicator\n0–10 Camera 1 0.45 0.67 Normal\n10–20 Camera 2 0.50 0.72 Normal\n20–30 Camera 3 0.75 0.88 Anomalous\n30–40 Camera 4 0.35 0.55 Normal\n40–50 Camera 5 0.60 0.80 Normal\nTable 8. Temporal graph attention network (TGAT) output for key feature indicators across time.\n \nScientific Reports |         (2025) 15:2692 18| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nusing many models. It highlights how the in-depth analysis of each model and their respective outputs, against \nproposed systems, is performed by effectively detecting anomalies in different cameras and multimodal data \nthrough the exploitation of spatiotemporal dependencies with few-shot learning and unsupervised anomaly \ndetection to improve the accuracy of anomaly detection, thereby enhancing the performance of generalization \nin complicated scenarios.\nExtended analysis\nThe proposed models are tested on contextual datasets from the UCSD Pedestrian Dataset and other public \ndatasets for anomaly detection in multi-camera surveillance environments. The performance of the proposed \nmodels was compared against three benchmark methods 4,8,15, using key metrics such as precision, recall, F1-\nscore, and area under the curve (AUC). The following tables present the detailed comparison and analysis of \nthe results.\nIn Table  14, it is also shown that the proposed model, TARNN in combination with TGAT, achieves a \nhighest F1-score of 90.7%. This ability manifests the capability of the model to catch both spatial and temporal \ndependencies within and between multiple camera feeds. Method4 gives F1-score of 83.6%, which is competitive, \nthough it is rather weaker in precision. Method8 obtains a better recall, with 86.5% but costs precision. Method15 \nis the weakest overall, especially in terms of precision-it involves a rather modest F1-score of 79.6%.\nTable 15 Reports the AUC for anomaly detection on UCSD Ped2 dataset. The AUC for the MV AE with TGAT \ncomes out to be very good at 95.4%, thus showing an excellent function as an anomaly detector. All benchmark \nmethods4,8,15 are lagging; in fact, the best next AUC obtained is by the method in reference 4 to be 87.2%. This \nModel Precision (%) Recall (%) F1-Score (%)\nProposed model (TGAT + TARNN) 92.1 89.4 90.7\nMethod4 84.5 82.7 83.6\nMethod8 81.2 86.5 83.7\nMethod15 78.9 80.3 79.6\nTable 14. Precision, recall, and F1-score comparison for anomaly detection on UCSD Ped1 dataset.\n \nTime (s) TGAT anomaly indicator\nTARNN anomaly \nindicator\nMV AE anomaly \nindicator\nPrototypical network \nindicator\nAutoencoder anomaly \nindicator\nFinal \ndecision\n0–10 Normal Normal Normal Normal Normal Normal\n10–20 Normal Normal Normal Normal Normal Normal\n20–30 Anomalous Anomalous Anomalous Anomalous Anomalous Anomalous\n30–40 Normal Normal Normal Normal Normal Normal\n40–50 Normal Normal Normal Normal Normal Normal\nTable 13. Final outputs of anomaly detection across all models.\n \nTime (s) Input reconstruction error (Spatial) Input reconstruction error (Temporal) Total reconstruction error Anomaly indicator\n0–10 0.08 0.07 0.15 Normal\n10–20 0.10 0.09 0.19 Normal\n20–30 0.40 0.38 0.78 Anomalous\n30–40 0.06 0.05 0.11 Normal\n40–50 0.07 0.06 0.13 Normal\nTable 12. Spatiotemporal autoencoder for unsupervised anomaly detection.\n \nQuery instance Distance to normal prototype Distance to anomalous prototype Classification\nInstance 1 0.35 0.75 Normal\nInstance 2 0.28 0.81 Normal\nInstance 3 0.65 0.40 Anomalous\nInstance 4 0.30 0.72 Normal\nInstance 5 0.72 0.35 Anomalous\nTable 11. Prototypical networks for few-shot learning.\n \nScientific Reports |         (2025) 15:2692 19| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nclearly shows the strength of multimodal fusion when used for performance enhancement in anomaly detection, \nespecially when data integration comes from sources such as video and motions.\nTable 16 Models performance under noisy conditions. In a noisy input condition, the proposed Spatiotemporal \nAutoencoder shows to be extremely resilient with an F1 score of 89.0%. The experimental results of the \nmethods4,8,15 indicate a dramatic loss of precision and recall compared to their performance on ideal conditions. \nMethod4  balances the precision and recall, yet it cannot deal with efficient noise handling and achieves an F1-\nscore of 80.1%. Out of this outcome, the model developed here should be able to handle noisy and incomplete \nsamples due to convolutional and LSTM layers in the autoencoders.\nTable 17 shows the accuracy comparison in few-shot learning scenarios, where only a few labeled examples of \nanomalies are available. Accuracy Comparison in a Few-shot Learning Scenarios, in such scenarios, only a few \nlabeled examples of anomalies are available. The accuracy is quite impressive for the Prototypical Networks with \nan accuracy of 88.4%, leaving other methods far behind. While Method 4 is indeed 79.5% accurate, Methods8,15 \nare significantly less accurate at 76.8% and 74.2%, respectively. These kinds of results show how the Prototypical \nNetworks approach to model learning from just a few examples would be beneficial in regimes where such \nlabeled data were rare in different operations.\nTable 18 shows the false positive rate comparison among models. This combination of MV AE and the \nSpatiotemporal Autoencoder will produce the smallest false positive rate of 6.3%. Methods 4,8,15 have larger \nvalues of false positives at the following: Method4 at 14.5% to prove that it is much more prone to false alarms. \nThis result will verify the superiority of the proposed model regarding reducing unnecessary alert-that is, a \ncrucial requirement to reduce the fatigue of operators and enhance general effectiveness of real-time surveillance \nsystems.\nTable 19 demonstrates the inference time for real-time anomaly detection. Although with state-of-the-art \naccuracy, it takes to the proposed TGAT + TARNN model 115 milliseconds to infer, which is, however, more \nthan Method4 is applied with its value of 98 ms. Anyway, this speed trade-off is quite well-balanced by superior \naccuracy and robustness that the model can demonstrate, whereas Methods 8,15 are less applicable for real-time \nModel False positive rate (%)\nProposed model (MV AE + Spatiotemporal Autoencoder) 6.3\nMethod4 14.5\nMethod8 12.8\nMethod15 11.7\nTable 18. False positive rate comparison for UCSD Ped2 dataset.\n \nModel Accuracy (%)\nProposed model (Prototypical Networks) 88.4\nMethod4 79.5\nMethod8 76.8\nMethod15 74.2\nTable 17. Few-shot learning accuracy on UCSD Ped1 dataset.\n \nModel Precision (%) Recall (%) F1-Score (%)\nProposed model (Spatiotemporal Autoencoder) 89.8 88.3 89.0\nMethod4 80.2 79.9 80.1\nMethod8 78.7 82.1 80.4\nMethod15 75.6 78.5 77.0\nTable 16. Performance under noisy data (UCSD Ped1 dataset).\n \nModel AUC (%)\nProposed model (MV AE + TGAT) 95.4\nMethod4 87.2\nMethod8 83.5\nMethod15 80.8\nTable 15. AUC comparison for anomaly detection on UCSD Ped2 dataset.\n \nScientific Reports |         (2025) 15:2692 20| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\napplications due to their inference times equal to 132 ms and 125 ms, respectively. Collectively, these results \nshow that the proposed models are better than the present approaches in terms of accuracy, noise robustness, \nfalse positive rates, and generalization for few-shot learning. In conjunction with advanced spatiotemporal \nmodeling, attention mechanisms, and multimodal fusion, this paper demonstrates the possibility that the \nproposed framework can significantly improve real-world anomaly detection for multi-camera surveillance \nenvironments, especially in complex and noisy settings.\nAblation study analysis\nThe ablation study is performed on an integrated model and removes or replaces each component with the \nperformance assessment by comparing it with that of datasets like UCSD Pedestrian, ShanghaiTech, and \nAvenue. In place of TGAT, by using a conventional Graph Convolutional Network (GCN), the F1-score on the \nUCSD Pedestrian dataset reduced to 84.3% from 90.6%. It is the spatiotemporal dependencies, being modeled \ndynamically among camera nodes, that constitute the critical feature of the entire model by TGAT. Comparing \nthat, the AUC of the model on the ShanghaiTech dataset was 94.6%, but only 88.2% for the model based on \nGCN, proving the importance of attention for weighing feeds relevant to the current environment. Deletion of \nTGAT completely reduced performance to an F1-score of 80.2% indicating that the component is integral in the \nframework.\nThe contribution of TARNN was measured by substituting it with a standalone LSTM network. On the Avenue \ndataset, the standalone LSTM attained a recall of 85.4% compared to 90.8% with TARNN, which underscores \nthe necessity of the self-attention mechanism in the transformer for capturing long-range dependencies. \nThe inclusion of the temporal score from TARNN significantly enhanced the identification of slow-evolving \nanomalies observed by the model on ShanghaiTech, where the F1-score was boosted to 89.7% compared to \n83.9% of the standalone LSTM. Eliminating TARNN resulted in lowered recalls to below 80% on all datasets and \nhence confirms its role in modeling both short and long-term temporal dependencies.\nTo check the robustness and effectiveness of multimodal fusion with Multimodal Variational Autoencoder, \nthe model was tested on unimodal inputs: for video-only data, the F1-score dipped to 81.6% on Mall Dataset in \ncomparison with 88.7% achieved for multimodal inputs. For motion and audio-only baselines, the respective \nF1-scores were around 80.4% and 79.9%. All modality information in the joint latent representation from MV AE \nended up being critical for noise-robust detection of humans. On the partially degraded motion data on the \nShanghaiTech dataset, the enabled fusion using MV AE still recorded 87.3% F1 while the unimodal methods \nremained below 75%. The paper elaborates on how MV AE appropriately leverages these complementing \nstrengths, thereby proving that all the parts combined are required to achieve state-of-the-art performance over \na large suite of surveillance scenarios.\nConclusions and future scopes\nIn this respect, the proposed framework was based on TGAT, TARNN, MV AE, Prototypical Networks for Few-\nShot Learning, and Spatiotemporal Autoencoders. The presented approach was the new frontier for anomaly \ndetection advancement in a multi-camera surveillance system. These models give the best results due to extensive \nexperiments on the UCSD Pedestrian Dataset. For instance, this TGAT + TARNN produces an F1-score of 90.6% \nand an exciting AUC of 95.2%, up to 12% higher than traditional methods. Besides, MV AE further exploited \ndata fusion across video, audio, and motion sensor inputs, and was shown to be robust under noisy conditions \nwhere the reconstruction error for anomalous events is reduced by 25%, which could handle missing or noisy \nsamples. In Prototypical Networks, which is tailored for few-shot learning, it was able to achieve an accuracy \nof 87.9%, showing an ability to generalize to unseen anomalies with just a few labeled examples. Moreover, the \nSpatiotemporal Autoencoder has reduced the false positives to 6.5%, substantially improving the dependability \nof the operation. Altogether, these results underpin the efficiency and scalability of the proposed models, which \nhad precision and recall metrics exceeding the best results from prior art by a margin of 10–15%. This framework \npositions itself as a robust solution for real-world anomaly detection in several camera environments.\nThe anomaly detection framework proposed is interpretable and actionable for end-users such as security \npersonnel through a modular architecture that explains itself. Each module of the framework produces outputs \nin highly interpretable forms, like the output attention scores by the Temporal Graph Attention Network that \ntrace to the most important camera feeds or spatial nodes that could be isolated in a detection case of anomalies. \nCorrespondingly, TARNN also provides temporal patterns where the probability of having an anomaly is high \nand reconstruction errors from the Spatiotemporal Autoencoder identify specific deviations from normal \nbehavior. The Multimodal Variational Autoencoder (MV AE) provides measurements of anomalies in video, \naudio, and motion to highlight which modality contributed to its detection.\nAlthough the proposed framework has vastly improved anomaly detection, there are scenarios under which its \nperformance is less optimal. Primarily, this limitation occurs in extremely sparse data environments or very low-\nModel Inference time (ms)\nProposed model (TGAT + TARNN) 115\nMethod4 98\nMethod8 132\nMethod15 125\nTable 19. Inference time for real-time anomaly detection (UCSD Ped1 dataset).\n \nScientific Reports |         (2025) 15:2692 21| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nresolution inputs: distant or partially occluded camera feeds. Due to the absence of spatial details, reconstruction \nfails to be effective for the Spatiotemporal Autoencoder and therefore suffers from increased false negatives. The \nMultimodal Variational Autoencoder (MV AE) further suffers in its ability to effectively fuse modalities when \none modality, for example, audio, is highly distorted or missing, resulting in a small degradation in the anomaly \ndetection accuracy levels. It also lacks computational overheads as the real-time processing takes place in very \ndense camera networks where scalability becomes an issue despite the optimizations in the TGAT module sets. \nThese shortcomings highlight further requirements such as advanced domain adaptation techniques to be used \nin low-quality data and edge-computing optimizations in large-scale deployments.\nFuture scope\nExtensions in various keys form the future scope of this work. First, this can be further upscaled for detecting \nabnormalities in more complex environments, such as industrial facilities or very crowded public places, with \nthe integration of more modalities. It is also envisioned that further reinforcement learning techniques can be \nintegrated into the attention mechanisms of TGAT and TARNN to improve the dynamic selection of relevant \ncameras and sequences. This in turn will reduce computational overhead while sustaining high performance \nfor detection. Another promising research direction is the use of domain adaptation techniques to make the \nmodels generalize across diverse datasets for enabling robust system performance across different contexts, \nindoor environments, or variable weather conditions. The few-shot learning in Prototypical Networks could \nbe further enhanced by incorporating meta-learning strategies that enable better adaptation to new classes of \nanomalies with fewer labeled examples. Its unsupervised nature makes Spatiotemporal Autoencoder a very good \nstarting point for more sophisticated self-supervised learning techniques that could lead to better performing \nmethods for anomaly detection in rare or very subtle events not requiring explicit labeling process. Real-time \nimplementation and integration with edge computing is the last one because future research has to be done on \nsuch models applied in real-time surveillance levels. This will require optimization in computational efficiency \nand resource management to make the system feasible at a large scale continuously for monitoring purposes.\nData availability\nThe datasets used and/or analysed during the current study are available from the corresponding author on \nreasonable request.\nReceived: 1 October 2024; Accepted: 6 January 2025\nReferences\n 1. Yu, H., Zhang, X., Wang, Y ., Huang, Q. & Yin, B. Fine-grained accident detection: Database and algorithm. IEEE Trans. Image \nProcess. 33, 1059–1069. https://doi.org/10.1109/TIP .2024.3355812 (2024).\n 2. Vosta, S. & Y ow, K. C. KianNet: A violence detection model using an attention-based CNN-LSTM structure. IEEE Access. 12, \n2198–2209. https://doi.org/10.1109/ACCESS.2023.3339379 (2024).\n 3. Varghese, E. B., Thampi, S. M. & Berretti, S. A psychologically inspired fuzzy cognitive deep learning framework to predict crowd \nbehavior. IEEE Trans. Affect. Comput. 13(2), 1005–1022. https://doi.org/10.1109/TAFFC.2020.2987021 (2022).\n 4. Luo, L., Xie, S., Yin, H., Peng, C. & Ong, Y . S. Detecting and quantifying crowd-level abnormal behaviors in crowd events. IEEE \nTrans. Inf. Forensics Secur. 19, 6810–6823. https://doi.org/10.1109/TIFS.2024.3423388 (2024).\n 5. Wang, R. et al. The limo-powered crowd monitoring system: Deep life modeling for dynamic crowd with edge-based information \ncognition. IEEE Sens. J. 22(18), 17666–17676. https://doi.org/10.1109/JSEN.2021.3080917 (2022).\n 6. Behera, S., Dogra, D. P ., Bandyopadhyay, M. K. & Roy, P . P . Crowd characterization in surveillance videos using deep-graph \nconvolutional neural network. IEEE Trans. Cybernet. 53(6), 3428–3439. https://doi.org/10.1109/TCYB.2021.3126434 (2023).\n 7. Elharrouss, O. et al. FSC-Set: Counting, localization of football supporters crowd in the stadiums. IEEE Access 10, 10445–10459. \nhttps://doi.org/10.1109/ACCESS.2022.3144607 (2022).\n 8. Halboob, W ., Altaheri, H., Derhab, A. & Almuhtadi, J. Crowd management intelligence framework: Umrah use case. IEEE Access \n12, 6752–6767. https://doi.org/10.1109/ACCESS.2024.3350188 (2024).\n 9. Yin, T., Hoyet, L., Christie, M., Cani, M. P . & Pettré, J. The one-man-crowd: Single user generation of crowd motions using virtual \nreality. IEEE Trans. Vis. Comput. Graph 28(5), 2245–2255. https://doi.org/10.1109/TVCG.2022.3150507 (2022).\n 10. Liao, X. C., Chen, W . N., Guo, X. Q., Zhong, J. & Hu, X. M. Crowd management through optimal layout of fences: An ant colony \napproach based on crowd simulation. IEEE Trans. Intell. Transp. Syst. 24(9), 9137–9149. https://doi.org/10.1109/TITS.2023.3272318 \n(2023).\n 11. Zhou, Y . et al. Crowd descriptors and interpretable gathering understanding. IEEE Trans. Multimed. 26, 8651–8664.  h t t p s : / / d o i . o r \ng / 1 0 . 1 1 0 9 / T M M . 2 0 2 4 . 3 3 8 1 0 4 0     (2024).\n 12. Qaraqe, M. et al. PublicVision: A secure smart surveillance system for crowd behavior recognition. IEEE Access 12, 26474–26491. \nhttps://doi.org/10.1109/ACCESS.2024.3366693 (2024).\n 13. Wang, Q. & Breckon, T. P . Crowd counting via segmentation guided attention networks and curriculum loss. IEEE Trans. Intell. \nTransp. Syst. 23(9), 15233–15243. https://doi.org/10.1109/TITS.2021.3138896 (2022).\n 14. Li, J. et al. Variational abnormal behavior detection with motion consistency. IEEE Trans. Image Process. 31, 275–286.  h t t p s : / / d o i . \no r g / 1 0 . 1 1 0 9 / T I P . 2 0 2 1 . 3 1 3 0 5 4 5     (2022).\n 15. Khosravi, M. R., Rezaee, K., Moghimi, M. K., Wan, S. & Menon, V . G. Crowd emotion prediction for human-vehicle interaction \nthrough modified transfer learning and fuzzy logic ranking. IEEE Trans. Intell. Transp. Syst. 24(12), 15752–15761.  h t t p s : / / d o i . o r g / \n1 0 . 1 1 0 9 / T I T S . 2 0 2 3 . 3 2 3 9 1 1 4     (2023).\n 16. Luo, L., Zhang, B., Guo, B., Zhong, J. & Cai, W . Why they escape: Mining prioritized fuzzy decision rule in crowd evacuation. IEEE \nTrans. Intell. Transp. Syst. 23(10), 19456–19470. https://doi.org/10.1109/TITS.2022.3156060 (2022).\n 17. Wu, W ., Li, J., Yi, W . & Zheng, X. Modeling crowd evacuation via behavioral heterogeneity-based social force model. IEEE Trans. \nIntell. Transp. Syst. 23(9), 15476–15486. https://doi.org/10.1109/TITS.2022.3140823 (2022).\n 18. Lv, P . et al. Emotional contagion-aware deep reinforcement learning for antagonistic crowd simulation. IEEE Trans. Affect. Comput. \n14(4), 2939–2953. https://doi.org/10.1109/TAFFC.2022.3225037 (2023).\n 19. Chai, L., Liu, Y ., Liu, W ., Han, G. & He, S. CrowdGAN: Identity-free interactive crowd video generation and beyond. IEEE Trans. \nPattern Anal. Mach. Intell. 44(6), 2856–2871. https://doi.org/10.1109/TPAMI.2020.3043372 (2022).\nScientific Reports |         (2025) 15:2692 22| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\n 20. Yi, W ., Wu, W ., Wang, X. & Zheng, X. Modeling the mutual anticipation in human crowds with attention distractions. IEEE Trans. \nIntell. Transp. Syst. 24(9), 10108–10117. https://doi.org/10.1109/TITS.2023.3268315 (2023).\n 21. Cai, Z. et al. Forecasting citywide crowd transition process via convolutional recurrent neural networks. IEEE Trans. Mob. Comput. \n23(5), 5433–5445. https://doi.org/10.1109/TMC.2023.3310789 (2024).\n 22. Zeng, Y ., Zhou, S. & Xiang, K. Online-offline interactive urban crowd flow prediction toward IoT-based smart city. IEEE Trans. \nServ. Comput. 15(6), 3417–3428. https://doi.org/10.1109/TSC.2021.3099781 (2022).\n 23. Y ang, Y . et al. Multiscenario open-set gait recognition based on radar micro-doppler signatures. IEEE Trans. Instrum. Meas. 71, \n1–13. https://doi.org/10.1109/TIM.2022.3214271 (2022).\n 24. Chen, J., Wang, C. & Liu, Y . Vibration signal based abnormal gait detection and recognition. IEEE Access 12, 89845–89855.  h t t p s : / \n/ d o i . o r g / 1 0 . 1 1 0 9 / A C C E S S . 2 0 2 4 . 3 4 1 7 3 7 7     (2024).\n 25. Naghavi, N. & Wade, E. Towards real-time prediction of freezing of gait in patients with Parkinson’s disease: A novel deep one-class \nclassifier. IEEE J. Biomed. Health Inf. 26(4), 1726–1736. https://doi.org/10.1109/JBHI.2021.3103071 (2022).\n 26. Palash, M. & Bhargava, B. EMERSK-Explainable multimodal emotion recognition with situational knowledge. IEEE Trans. \nMultimed. 26, 2785–2794. https://doi.org/10.1109/TMM.2023.3304015 (2024).\n 27. Alharthi, R., Alhothali, A., Alzahrani, B. & Aldhaheri, S. Massive crowd abnormal behaviors recognition using C3D. In 2023 IEEE \nInternational Conference on Consumer Electronics (ICCE), Las Vegas, NV , USA 01–06 (2023).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 / I C C E 5 6 4 7 0 . \n2 0 2 3 . 1 0 0 4 3 4 3 7     .   \n 28. Anandhi, R. Spatially-constrained anomaly detection in crowded environments using meta-heuristic algorithm. In 2023 4th \nInternational Conference on Smart Electronics and Communication (ICOSEC), Trichy, India 1440–1444 (2023).  h t t p s : / / d o i . o r g / 1 0 . 1 \n1 0 9 / I C O S E C 5 8 1 4 7 . 2 0 2 3 . 1 0 2 7 6 0 5 2     .   \n 29. Zhou, X. & Xiao, R. Detection of abnormal crowd behavior based on graph convolutional neural network. In 12th International \nConference on Information Technology in Medicine and Education (ITME), Xiamen, China 538–542 (2022).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 \n/ I T M E 5 6 7 9 4 . 2 0 2 2 . 0 0 1 1 8     .   \n 30. Liyanage, P . & Fernando, P . Suspicious human crowd behaviour detection—A transfer learning approach. In 21st International \nConference on Advances in ICT for Emerging Regions (ICter), Colombo, Sri Lanka 63–68 (2021).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 / I C t e r 5 3 6 3 \n0 . 2 0 2 1 . 9 7 7 4 7 8 4     .   \n 31. Mehmood, A. Efficient anomaly detection in crowd videos using pre-trained 2D convolutional neural networks. IEEE Access 9, \n138283–138295. https://doi.org/10.1109/ACCESS.2021.3118009 (2021).\n 32. Mu, H., Sun, R., Yuan, G., Li, J. & Wang, M. Crowd behavior detection in videos using statistical physics. In 2021 International \nConference on Data Mining Workshops (ICDMW), Auckland, New Zealand 389–397 (2021).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 / I C D M W 5 3 4 3 \n3 . 2 0 2 1 . 0 0 0 5 4     .   \n 33. Ahmed, R., Rafiq, M. S. & Junej, I. N. Crowd modeling using temporal association rules. In 2021 IEEE 2nd International Conference \non Human-Machine Systems (ICHMS), Magdeburg, Germany 1–4 (2021). https://doi.org/10.1109/ICHMS53169.2021.9582661.\n 34. Karki, M. V ., Aripirala, A., Vasist, C., Renith, A. & Balasubramanian, A. Abnormal human behavior detection in crowded scenes \nbased on hybrid neural networks. In 4th International Conference on Circuits, Control, Communication and Computing (I4C), \nBangalore, India 54–58 (2022). https://doi.org/10.1109/I4C57141.2022.10057910.\n 35. Abdullah, F ., Javeed, M. & Jalal, A. Crowd anomaly detection in public surveillance via spatio-temporal descriptors and zero-shot \nclassifier. In International Conference on Innovative Computing (ICIC), Lahore, Pakistan 1–8 (2021).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 / I C I C 5 \n3 4 9 0 . 2 0 2 1 . 9 6 9 3 0 0 3     .   \n 36. Bansal, S., Kumar, S. & Bhalla, P . A novel approach to WDM channel allocation: Big Bang–Big crunch optimization. In The \nproceeding of Zonal Seminar on Emerging Trends in Embedded System Technologies (ETECH) organized by The Institution of \nElectronics and Telecommunication Engineers (IETE), Chandigarh Centre, Chandigarh, India 80–81 (2013).\n 37. Bansal, S., Chauhan, R. & Kumar, P . A cuckoo search based WDM channel allocation algorithm. Int. J. Comput. Appl. 96(20), 6–12 \n(2014).\n 38. Zhou, Y ., Qin, M., Wang, X. & Zhang, C. Regional crowd status analysis based on geovideo and multimedia data collaboration. \nIn 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), \nChongqing, China 1278–1282 (2021). https://doi.org/10.1109/IMCEC51613.2021.9482018.\n 39. Kevin Lemuel Thomas, R., Jerome Sanjay, G., Pandeeswaran, C. & Raghi, K. R. Advanced CCTV surveillance anomaly detection, \nalert generation and crowd management using deep learning algorithm. In 3rd International Conference on Artificial Intelligence \nFor Internet of Things (AIIoT), Vellore, India 1–6 (2024). https://doi.org/10.1109/AIIoT58432.2024.10574731.\n 40. Sophia, S. & Joeffred Gladson, J. Human behaviour and abnormality detection using YOLO and Conv2D Net. In 2024 International \nConference on Inventive Computation Technologies (ICICT), Lalitpur, Nepal 70–75 (2024).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 / I C I C T 6 0 1 5 5 . 2 0 2 \n4 . 1 0 5 4 4 7 5 7     .   \n 41. Ge, Z., Jiang, J. & Coombes, M. A congestion-aware path planning method considering crowd spatial-temporal anomalies for long-\nterm autonomy of mobile robots. In IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom \n7930–7936. (2023). https://doi.org/10.1109/ICRA48891.2023.10160252.\n 42. Mahmoud, S., Arafa, Y . & Abdelmohsen, M. Dynamic image representations for crowd anomaly detection using generative \nadversarial networks. In 2023 International Conference on Computer and Applications (ICCA), Cairo, Egypt 1–6 (2023).  h t t p s : / / d o i \n. o r g / 1 0 . 1 1 0 9 / I C C A 5 9 3 6 4 . 2 0 2 3 . 1 0 4 0 1 6 8 1     .   \n 43. Veesam, S. B. & Satish, A. R. An empirical taxonomy of video summarization model from a statistical perspective. IEEE Access. \nhttps://doi.org/10.1109/ACCESS.2024.3503276\n 44. Khan, S. D., Bandini, S., Basalamah, S. & Vizzari, G. Analyzing crowd behavior in naturalistic conditions: Identifying sources and \nsinks and characterizing main flows. In Neurocomputing 543–563, vol. 177 (Elsevier BV , 2016).  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . n e u c o m . \n2 0 1 5 . 1 1 . 0 4 9     .   \n 45. Farooq, M. U., Saad, M. N. M. & Khan, S. D. Motion-shape-based deep learning approach for divergence behavior detection in \nhigh-density crowd. Vis. Comput. 38, 1553–1577. https://doi.org/10.1007/s00371-021-02088-4 (2022).\n 46. Alzahrani, A. J. & Khan, S. D. Characterization of different crowd behaviors using novel deep learning framework. Turk. J. Electr. \nEng. Comput. Sci. 29(1), 12. https://doi.org/10.3906/elk-2004-14 (2021).\n 47. Bansal, S. et al. Pt/ZnO and Pt/few-layer graphene/ZnO Schottky devices with Al Ohmic contacts using Atlas simulation and \nmachine learning. J. Sci. Adv. Mater. Devices 9, 100798-1–100798-14 (2024).\n 48. Bansal, S. et al. Optoelectronic performance prediction of HgCdTe homojunction photodetector in long wave infrared spectral \nregion using traditional simulations and machine learning models. Sci. Rep. 14, 28230 (2024).\n 49. Bansal, S. ANNs supervised learning-based automatic fault detection in a class of wheatstone bridge-oriented transducers. In 2022 \nIEEE Sponsored Global Conference for Advancement in Technology (GCAT-2022), Nagarjuna College of Engineering & Technology, \nBengaluru, Karnataka, India 1–7 (2022).\n 50. Bansal, S. & Jain, P . Automatic fault detection in a class of wheatstone bridge-based transducer using ANNs in Verilog HDL. In \n2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE), Galgotias College of \nEngineering and Technology Greater Noida, India 466–470 (2022).\n 51. Veesam, S. B. & Satish, A. R. Design of an iterative method for CCTV video analysis integrating enhanced person detection and \ndynamic mask graph networks. IEEE Access 12, 157630–157656. https://doi.org/10.1109/ACCESS.2024.3485896 (2024).\nScientific Reports |         (2025) 15:2692 23| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/\nAcknowledgements\nThis research was funded by Fundamental Research Grant Scheme (FRGS), MOE, Malaysia, Code: FRGS/1/2022/\nTK07/UKM/02/22.\nAuthor contributions\nS B Veesam, A R Satish, S Tupakula made substantial contributions to design, analysis and characterization. Y \nChinnam, K Prakash, S Bansal participated in the conception, application and critical revision of the article for \nimportant intellectual content. M R I Faruque provided necessary instructions for analytical expression, case \nstudy for practical use and critical revision of the article purposes.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to K.P ., S.B. or M.R.I.F .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |         (2025) 15:2692 24| https://doi.org/10.1038/s41598-025-85822-5\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6101073026657104
    },
    {
      "name": "Transformer",
      "score": 0.5918741226196289
    },
    {
      "name": "Anomaly detection",
      "score": 0.5646647214889526
    },
    {
      "name": "Graph",
      "score": 0.5554754734039307
    },
    {
      "name": "Anomaly (physics)",
      "score": 0.438896507024765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3753730058670044
    },
    {
      "name": "Theoretical computer science",
      "score": 0.20288410782814026
    },
    {
      "name": "Engineering",
      "score": 0.1253492832183838
    },
    {
      "name": "Electrical engineering",
      "score": 0.09924042224884033
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Condensed matter physics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}