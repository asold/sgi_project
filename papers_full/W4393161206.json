{
  "title": "ChatGPT-Generated Code Assignment Detection Using Perplexity of Large Language Models (Student Abstract)",
  "url": "https://openalex.org/W4393161206",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2030984501",
      "name": "Xu, Zhenyu",
      "affiliations": [
        "Texas Tech University"
      ]
    },
    {
      "id": "https://openalex.org/A3181702254",
      "name": "Xu Ruoyu",
      "affiliations": [
        "Texas Tech University"
      ]
    },
    {
      "id": "https://openalex.org/A4224559693",
      "name": "Sheng, Victor S.",
      "affiliations": [
        "Texas Tech University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4318351452"
  ],
  "abstract": "In the era of large language models like Chatgpt, maintaining academic integrity in programming education has become challenging due to potential misuse. There's a pressing need for reliable detectors to identify Chatgpt-generated code. While previous studies have tackled model-generated text detection, identifying such code remains uncharted territory. In this paper, we introduce a novel method to discern Chatgpt-generated code. We employ targeted masking perturbation, emphasizing code sections with high perplexity. Fine-tuned CodeBERT is utilized to replace these masked sections, generating subtly perturbed samples. Our scoring system amalgamates overall perplexity, variations in code line perplexity, and burstiness. In this scoring scheme, a higher rank for the original code suggests it's more likely to be chatgpt-generated. The underlying principle is that code generated by models typically exhibits consistent, low perplexity and reduced burstiness, with its ranking remaining relatively stable even after subtle modifications. In contrast, human-written code, when perturbed, is more likely to produce samples that the model prefers. Our approach significantly outperforms current detectors, especially against OpenAI's text-davinci-003 model, with the average AUC rising from 0.56 (GPTZero baseline) to 0.87.",
  "full_text": "ChatGPT-Generated Code Assignment Detection Using Perplexity of Large\nLanguage Models (Student Abstract)\nZhenyu Xu, Ruoyu Xu, Victor S. Sheng\nDepartment of Computer Science, Texas Tech University\nzhenxu@ttu.edu, ruoyxu@ttu.edu, victor.sheng@ttu.edu\nAbstract\nIn the era of large language models like ChatGPT, maintain-\ning academic integrity in programming education has become\nchallenging due to potential misuse. There’s a pressing need\nfor reliable detectors to identify ChatGPT-generated code.\nWhile previous studies have tackled model-generated text de-\ntection, identifying such code remains uncharted territory. In\nthis paper, we introduce a novel method to discern ChatGPT-\ngenerated code. We employ targeted masking perturbation,\nemphasizing code sections with high perplexity. Fine-tuned\nCodeBERT is utilized to replace these masked sections, gen-\nerating subtly perturbed samples. Our scoring system amal-\ngamates overall perplexity, variations in code line perplex-\nity, and burstiness. In this scoring scheme, a higher rank for\nthe original code suggests it’s more likely to be ChatGPT-\ngenerated. The underlying principle is that code generated by\nmodels typically exhibits consistent, low perplexity and re-\nduced burstiness, with its ranking remaining relatively stable\neven after subtle modifications. In contrast, human-written\ncode, when perturbed, is more likely to produce samples that\nthe model prefers. Our approach significantly outperforms\ncurrent detectors, especially against OpenAI’s text-davinci-\n003 model, with the average AUC rising from 0.56 (GPTZero\nbaseline) to 0.87.\nIntroduction\nLarge language models, like ChatGPT from OpenAI, can\nnow generate code autonomously. This is revolutionizing\nsoftware development but also poses a challenge for educa-\ntion. Students might use these models to complete coding as-\nsignments without doing the work themselves. This misuse\nundermines the purpose of programming education. Most\ntext detectors today are designed to detect model-generated\ntext, not code. The study DetectGPT (Mitchell et al. 2023)\npresented an interesting finding: making small changes to\nmodel-generated text typically results in lower log proba-\nbilities of the model of interest than the original text, while\nchanges to human-written text can increase or decrease these\nprobabilities. This means that model-produced tokens usu-\nally sit at the peak of the log probability curve. The concept\nof code naturalness implies that a similar behavior should\nmanifest within code contexts. We tested the idea that small\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nmodifications can clearly separate ChatGPT-generated code\nfrom human-written code. Our experiments found that mi-\nnor changes to ChatGPT-produced code often make it more\nperplexing for the model than the original. However, for\nhuman-written code, such changes can have mixed results.\nWe also suggest that code generated from large language\nmodels shows steadier perplexity line-by-line than human\ncode, and our results support this.\nWe harness these two observations from the code con-\ntext to construct our detector. Our strategy seeks to dis-\ncern whether a given piece of code is ChatGPT-generated\nby perturbation and scoring. First, a perturbation mecha-\nnism applies a mask modeling task to slightly modify seg-\nments of code that exhibit higher perplexity (PPL). Second,\na scoring mechanism is influenced by three metrics: per-\nplexity of the code, standard deviation of perplexity across\nlines of code, and the code’s burstiness. A lower score in-\ndicates a greater probability that the code generated from\nmodel. This process is shown in Figure 1. The foundational\npremise of our methodology is that ChatGPT-generated code\ninherently possesses a low score, gauged by the combined\nmetrics of overall perplexity, standard deviation of perplex-\nity across code lines, and burstiness. Consequently, post-\nperturbation, it becomes challenging to generate samples\nthat score lower than the original ChatGPT-generated code.\nIn contrast, human-authored code might be perceived as\nmore ambiguous by the model, so minor tweaks might yield\nan ”optimized” version that bears a score lower than the\noriginal code.\nApproach\nDataset Description\nOur CGCode (ChatGPT-generated code) dataset, based on\nIBM’s CodeNet, aims to mimic student submissions for pro-\ngramming tasks. We focused on six primary programming\nlanguages: C, C++, C#, Java, JavaScript, and Python. Af-\nter ensuring each code met criteria for quality and length\nand removing duplicates and comments, we used 80% of\nthe data to fine-tune and validate CodeBERT, setting aside\n10% each for validation and testing. We enhanced the test\nsubset, and leveraged OpenAI’s text-davinci-003 for text-\nto-code generation and code translation tasks for 400 prob-\nlems in each test subset. The test set now contains 5,214\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23688\nC C++ C# Java JavaScript Python\nDetectors AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR\nGPT2-Detector 0.64 0.83 0.03 0.68 0.84 0.05 0.46 0.92 0.13 0.44 0.91 0.11 0.39 0.90 0.26 0.38 0.89 0.25\nDetectGPT 0.56 0.00 1.00 0.42 0.00 1.00 0.49 0.00 1.00 0.43 0.00 1.00 0.51 0.00 1.00 0.49 0.00 1.00\nRoBERTa-QA 0.68 0.00 1.00 0.53 0.00 1.00 0.48 0.00 1.00 0.64 0.00 1.00 0.52 0.00 1.00 0.60 0.00 1.00\nWriter 0.78 0.13 0.91 0.62 0.15 0.98 0.52 0.13 0.91 0.54 0.01 0.96 0.56 0.10 0.89 0.51 0.08 0.97\nGPTZero 0.90 0.06 0.20 0.83 0.20 0.68 0.29 0.18 0.96 0.28 0.18 0.88 0.41 0.08 0.90 0.59 0.00 1.00\nCGCode Detector 0.95 0.16 0.08 0.88 0.13 0.02 0.86 0.12 0.07 0.82 0.15 0.05 0.81 0.21 0.15 0.92 0.14 0.03\nTable 1: Performance of Different Detectors Across Six Programming Languages\nFigure 1: Illustration of the perturbation and scoring proce-\ndure. Weights are assigned to code segments based on line-\nlevel perplexity, higher weight means more allocated masks,\nfollowed by mask-filling task for slight modifications. A\nlower score suggests a higher likelihood of the code being\ngenerated by ChatGPT.\nChatpt-generated codes, balanced with an equivalent num-\nber of human-written ones, matched by language and prob-\nlem type.\nPerturbation and Scoring Process\nMasking Unlike the random masking strategy of Detect-\nGPT on text, our method for code uses a measured strat-\negy. We utilize Perplexity (PPL) to gauge the complexity of\neach line of code, and apply masks based on this. Lines of\ncode with high PPL values receive more masks. This method\nproves to be more efficient than random masking, leading to\nless required samples.\nMask-filling After fine-tuning CodeBERT across a spec-\ntrum of programming languages, it’s harnessed to fill our\ncode masks. We opt for Nucleus Sampling to obtain a di-\nverse set of token suggestions, ensuring the creation of a rich\nvariety of modified samples.\nScoring Our evaluation of code integrates three metrics:\nPPL, PPL variation across code lines, and code burstiness.\nThe scores of original and altered code are juxtaposed.\nChatGPT-generated code often attains better scores due to its\nintrinsic coherence. Altered versions of ChatGPT-generated\ncode seldom score lower, while human-written code might\nsee lower scores with minimal modifications.\nExperiment and Results\nTo evaluate the efficacy of our Detector, we bench-\nmark its performance against five prominent open-source\nand commercial text detectors: GPT2-Detector, DetectGPT,\nRoBERTa-QA, GPTZero, and The Writer AI Detector. Our\ncomparisons are conducted on the CGCode dataset. Within\nthe CGCode Detector framework, we employ CodeBERT as\nthe mask-filling model and text-davinci-003 as the primary\nscoring model. For those detectors giving probability, we\nchoose its best performance threshold. For detectors having\nrequirement of input length, we truncate and use the prior to-\nkens as input. Results are shown in Table 1. CGCode Detec-\ntor has relatively high AUC on all programming languages,\nas well as low FPR and FNR.\nConclusion\nIn conclusion, we present the CGCode Detector, an innova-\ntive tool adept at pinpointing ChatGPT-generated code as-\nsignments using perplexity analysis and targeted perturba-\ntions. Building on the foundation of DetectGPT, we have\naddressed the gap in ChatGPT-generated code detection by\nenhancing current zero-shot detection method and develop-\ning a specialized detector for ChatGPT-generated code. Em-\npirical results show that our detector surpasses both lead-\ning open-source and commercial alternatives in the code do-\nmain. Due to its flexibility, our method can also be extended\nto other code generation models.\nReferences\nMitchell, E.; Lee, Y .; Khazatsky, A.; Manning, C. D.; and\nFinn, C. 2023. Detectgpt: Zero-shot machine-generated\ntext detection using probability curvature. arXiv preprint\narXiv:2301.11305.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23689",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9889327883720398
    },
    {
      "name": "Code (set theory)",
      "score": 0.6611207723617554
    },
    {
      "name": "Computer science",
      "score": 0.6231443881988525
    },
    {
      "name": "Programming language",
      "score": 0.4631902873516083
    },
    {
      "name": "Natural language processing",
      "score": 0.39830222725868225
    },
    {
      "name": "Language model",
      "score": 0.2627888321876526
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12315562",
      "name": "Texas Tech University",
      "country": "US"
    }
  ]
}