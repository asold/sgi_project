{
    "title": "The imperative for regulatory oversight of large language models (or generative AI) in healthcare",
    "url": "https://openalex.org/W4383346782",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5064145018",
            "name": "Bertalan Meskó",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5084515381",
            "name": "Eric J. Topol",
            "affiliations": [
                "Scripps Research Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4321499561",
        "https://openalex.org/W4361251463",
        "https://openalex.org/W2944973900",
        "https://openalex.org/W3086667591",
        "https://openalex.org/W2908201961",
        "https://openalex.org/W3013681994",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4313197536",
        "https://openalex.org/W4380319827",
        "https://openalex.org/W4381572755",
        "https://openalex.org/W4292395122"
    ],
    "abstract": "Abstract The rapid advancements in artificial intelligence (AI) have led to the development of sophisticated large language models (LLMs) such as GPT-4 and Bard. The potential implementation of LLMs in healthcare settings has already garnered considerable attention because of their diverse applications that include facilitating clinical documentation, obtaining insurance pre-authorization, summarizing research papers, or working as a chatbot to answer questions for patients about their specific data and concerns. While offering transformative potential, LLMs warrant a very cautious approach since these models are trained differently from AI-based medical technologies that are regulated already, especially within the critical context of caring for patients. The newest version, GPT-4, that was released in March, 2023, brings the potentials of this technology to support multiple medical tasks; and risks from mishandling results it provides to varying reliability to a new level. Besides being an advanced LLM, it will be able to read texts on images and analyze the context of those images. The regulation of GPT-4 and generative AI in medicine and healthcare without damaging their exciting and transformative potential is a timely and critical challenge to ensure safety, maintain ethical standards, and protect patient privacy. We argue that regulatory oversight should assure medical professionals and patients can use LLMs without causing harm or compromising their data or privacy. This paper summarizes our practical recommendations for what we can expect from regulators to bring this vision to reality.",
    "full_text": "PERSPECTIVE OPEN\nThe imperative for regulatory oversight of large language\nmodels (or generative AI) in healthcare\nBertalan Meskó 1,2 ✉ and Eric J. Topol 3\nThe rapid advancements in artiﬁcial intelligence (AI) have led to the development of sophisticated large language models (LLMs)\nsuch as GPT-4 and Bard. The potential implementation of LLMs in healthcare settings has already garnered considerable attention\nbecause of their diverse applications that include facilitating clinical documentation, obtaining insurance pre-authorization,\nsummarizing research papers, or working as a chatbot to answer questions for patients about their speciﬁc data and concerns.\nWhile offering transformative potential, LLMs warrant a very cautious approach since these models are trained differently from AI-\nbased medical technologies that are regulated already, especially within the critical context of caring for patients. The newest\nversion, GPT-4, that was released in March, 2023, brings the potentials of this technology to support multiple medical tasks; and\nrisks from mishandling results it provides to varying reliability to a new level. Besides being an advanced LLM, it will be able to read\ntexts on images and analyze the context of those images. The regulation of GPT-4 and generative AI in medicine and healthcare\nwithout damaging their exciting and transformative potential is a timely and critical challenge to ensure safety, maintain ethical\nstandards, and protect patient privacy. We argue that regulatory oversight should assure medical professionals and patients can use\nLLMs without causing harm or compromising their data or privacy. This paper summarizes our practical recommendations for what\nwe can expect from regulators to bring this vision to reality.\nnpj Digital Medicine          (2023) 6:120 ; https://doi.org/10.1038/s41746-023-00873-0\nINTRODUCTION\nThe rapid advancements in artiﬁcial intelligence (AI) have led to\nthe development of sophisticated large language models (LLM)\nsuch as OpenAI’s GPT-4 and Google’s Bard\n1,2. The unprecedented\npopularity of ChatGPT, GPT-4’s predecessor released in November\n2022, is reﬂected by the most rapid uptake of users - 100 million in\n2 months - for any new technology.\nThis rapid growth sparked global debates about the role such\nconversational chatbots could play in healthcare and the practice\nof medicine. Diverse applications of LLMs have appeared\nincluding facilitating clinical documentation; creating discharge\nsummaries; generating clinic, operation, and procedure notes;\nobtaining insurance pre-authorization; summarizing research\npapers; or working as a chatbot to answer questions for the\npatients with their speciﬁc data and concerns. LLMs can also assist\nphysicians in diagnosing conditions based on medical records,\nimages, laboratory results, and suggest treatment options or plans.\nAt the same time, patients can potentially become more\nautonomous than with prior search methods by obtaining\nindividualized assessment of their data, symptoms, and concerns.\nSystematic reviews highlighted other potential bene ﬁts too\nsuch as improved scientiﬁc writing, enhancing research equity,\nstreamlining the healthcare workﬂow, cost saving, and improved\npersonalized learning in medical education\n3,4.\nGiven the potential implications on patient outcomes and\npublic health, it is imperative to consider how these new AI-based\ntools should be regulated. The regulation of these LLMs in\nmedicine and healthcare without damaging their promising\nprogress is a timely and critical challenge to ensure safety,\nmaintain ethical standards, pre-empt unfairness and bias, and\nprotect patient privacy. Whatever concerns have been previously\nrecognized with AI are now markedly ampliﬁed with the multi-\npotency of LLMs.\nThis paper explores the potential risks and beneﬁts of applying\nLLMs in healthcare settings and argues for the necessity of\nregulating LLMs differently than AI-based medical technologies\nthat are already on the market to mitigate potential harm and\nmaintain public trust in these breakthrough technologies.\nLLMS DIFFER FROM ALREADY REGULATED AI-BASED\nTECHNOLOGIES\nLLMs differ signiﬁcantly from prior deep learning methods in\nterms of their scale, capabilities, and potential impact. Here we\noutline the key characteristics of LLMs that set them apart from\ntraditional deep learning techniques.\nScale and complexity\nLLMs are trained on massive datasets and utilize billions of\nparameters, resulting in unprecedented complexity. This level of\nsophistication requires regulatory oversight that takes into\naccount the challenges associated with interpretability, fairness,\nand unintended consequences. Moreover, LLMs use tokens that\ncan be words, subwords, or even characters as the smallest units\nof text used to represent and process language during the training\nand generation processes. Tokenization is a crucial step in natural\nlanguage processing (NLP) and allows LLMs to efﬁciently analyze\nand generate text, as these models are designed to process\nsequences of tokens rather than entire sentences or paragraphs.\nCurrently, tokenization is not covered by healthcare regulators.\n1The Medical Futurist Institute, Budapest, Hungary.2Department of Behavioural Sciences, Semmelweis University, Budapest, Hungary.3Scripps Research Translational Institute,\nScripps Research, La Jolla, CA, USA.✉email: berci@medicalfuturist.com\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nHardware requirements\nLLMs require massive computational resources in terms of\nﬂoating-point operations per second (FLOPs) and graphics\nprocessing unit (GPU) usage compared to previous deep learning\nmodels due to their large scale, extensive training data, a type of\nneural network model designed for NLP tasks called the\nTransformer architecture, and the need forﬁne-tuning.\nBroad applicability\nUnlike specialized deep learning models that were trained to\naddress a speciﬁc medical issue or clinical need, LLMs possess\nversatile capabilities that span various domains, such as health-\ncare, ﬁnance, and education. As a result, a one-size- ﬁts-all\nregulatory framework is ill-suited for LLMs, and oversight must\nbe adaptable to address diverse industry-speciﬁc concerns.\nReal-time adaptation\nLLMs can adapt their responses in real-time, based on user input\nand evolving contexts. This dynamic behavior demands that\nregulatory oversight incorporates continuous monitoring and\nevaluation mechanisms to ensure responsible usage and adher-\nence to ethical guidelines. This is similar to what adaptive AI-based\nmedical technologies would require from regulators.\nSocietal impact\nThe widespread adoption of LLMs has the potential to fundamen-\ntally transform various aspects of society. Consequently, regula-\ntory oversight must address not only the technical aspects of LLMs\nbut also their broader ethical, social, and economic implications.\nData privacy and security\nLLMs’reliance on extensive training data raises concerns related\nto data privacy and security. Regulatory oversight should establish\nrobust frameworks to protect sensitive information and prevent\nunauthorized access or misuse of these powerful models.\nThese unique characteristics of LLMs necessitate a tailored\napproach to regulatory oversight. Such an approach must be\nadaptive, holistic, and cognizant of the diverse challenges and\npotential consequences that LLMs present, ensuring their\nresponsible and ethical use across various domains.\nTHE FDA’S PRE-LLM OVERSIGHT OF AI\nThe United States’Food And Drug Administration (FDA) has been\nleading the global discussions on regulatory oversight and has\nbeen a prominent example in providing regulations about\nemerging technologies from 3D printed medications to AI-based\nmedical tools\n5.\nWith the increasing adoption of digital health technologies, the\nFDA started regulating Software as a Medical Device (SaMD) that\nrefers to software solutions that perform medical functions and\nare used in the prevention, diagnosis, treatment, or monitoring of\nvarious diseases or conditions.\nAs a continuation of that approach, the FDA has been adapting\nits regulatory framework to speciﬁcally address AI and machine\nlearning (ML) technologies in medical devices\n6. The FDA released\na discussion paper that outlined their potential regulatory\napproach tailored to AI and ML technologies used in medical\ndevices7. The discussion paper proposed a total product lifecycle\n(TPLC) approach to regulating AI/ML-based SaMD, which focuses\non the continuous monitoring and improvement of these\ntechnologies throughout their lifespan. The proposed framework\nalso emphasized the importance of transparency, real-world\nperformance monitoring, and clear expectations for modiﬁcations\nand updates to AI/ML algorithms.\nCurrently, the FDA does not have speciﬁc categories exclusively\nfor AI-based technologies but evaluates them within the existing\nregulatory framework for medical devices\n8. They classify such\ndevices into three main categories based on their level of risk:\n● Class I (Low risk): These devices pose the least risk and are\nsubject to general controls, such as registration and listing,\nlabeling, and good manufacturing practices. Examples of Class\nI devices include non-powered surgical instruments and\ndental ﬂoss. Some low-risk AI-based medical technologies\nmay fall under this category, depending on their intended use.\n● Class II (Moderate risk): These devices carry a higher level of\nrisk than Class I devices and are subject to both general\ncontrols and special controls, such as performance standards,\npostmarket surveillance, or speci ﬁc labeling requirements.\nExamples of Class II devices include infusion pumps, surgical\ndrapes, and powered wheelchairs. Many AI-based medical\ntechnologies, such as diagnostic imaging systems, may fall\nunder this category.\n● Class III (High risk): These devices pose the highest risk and are\nsubject to general controls, special controls, and premarket\napproval (PMA). Class III devices often support or sustain\nhuman life, are of substantial importance in preventing\nimpairment of human health, or present a potential unreason-\nable risk of illness or injury. Examples of Class III devices\ninclude implantable pacemakers, artiﬁcial heart valves, and\nsome AI-based technologies used in critical medical decision-\nmaking.\nAI-based medical technologies may also be subject to the FDA’s\nDigital Health Software Precertiﬁcation (Pre-Cert) Program, which\nis designed to streamline the regulatory process for SaMD,\nincluding AI-based technologies.\nA milestone in that process was the release of their database of\nspeciﬁcally AI-based medical technologies with regulatory\napprovals in 2021\n9. As of April, 2023, 521 devices are included\nin that database. The most popular categories are radiology,\ncardiovascular and hematology with 392, 57 and 15 devices,\nrespectively. The vast majority (96%) were approved with a 510(k)\nclearance, while 18 (3.5%) received de novo pathway clearance\nand 3 (0.5%) premarket approval (PMA) clearance.\nAs other papers have pointed out, only a few of these devices\nwere tested in randomized controlled trials (RCTs) trials; and only a\nlimited number of studies have used external validation,\nprospective evaluation and diverse metrics to explore the full\nimpact of AI in real clinical settings, and the range of assessed use\ncases has been relatively narrow with no or very little\ntransparency\n10.\nIn summary, while there has been progress in regulating AI, the\nFDA has not been able to solve the regulation of two advanced\ntechnological issues that are related but not the same. One is\nabout regulating adaptive algorithms that can adjust its para-\nmeters or behavior based on the input data or its performance on\na speciﬁc task. This adaptability allows the algorithm to improve\nits performance over time or respond to changing conditions.\nThe other one is related to the so-called autodidactic function in\ndeep learning. It refers to the ability of a system to teach itself\nwithout direct supervision, an approach that often requires\nunsupervised or self-supervised learning, where the model learns\npatterns and representations from the input data without relying\non labeled examples. Such an autodidactic deep learning model\ncan discover underlying structures and relationships in the data by\noptimizing its internal representations without explicit guidance.\nTHE LLM ERA IN THE PRACTICE OF MEDICINE\nTo date, no LLM has had pre-training with the corpus of medical\ninformation or with millions of patient records, images, lab data,\nand ofﬁce visit or bedside conversations. Details about the training\nB. Meskó and E.J. Topol\n2\nnpj Digital Medicine (2023)   120 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nof GPT-4, the most advanced LLM that was pubished in March\n2023, have not been released. Nevertheless, LLMs have transfor-\nmative potential, with use cases ranging from clinical documenta-\ntion to providing personalized health plans\n11. Figure 1 describes\n10 use cases for medical professionals and 10 for patients.\nAt the same time, the introduction of these models into\nhealthcare leads to the ampliﬁcation of risks and challenges.\nIt started posing a new challenge to physicians as patients\narrive to the meeting with not only responses received after\ngoogling their symptoms but also from ChatGPT-like chatbots.\nThere have been discussions about to what extent ChatGPT can\nbe used for medical research and summarizing peer-reviewed\npapers when it only provides sources it based its responses on\nafter speciﬁcally asking for it. Moreover, some of those sources\nhave been reported to be made up\n3.\nLLMs can sometimes \"hallucinate\" results, which refers to\ngenerating outputs that are not grounded in the input data or\nfactual information. Such misinformation may be related to a\ndiagnosis, treatment, or a recommended test. For the uninitiated,\nsuch outputs are conveyed with a high level of conﬁdence and\ncould easily be accepted by the prompter as truth— which has the\npotential to be dangerous. Whether it is due to incomplete or\nbiased training data, its probabilistic nature or the lack of context;\nit poses a signiﬁcant risk of providing unreliable or outright false\nanswers in the medical setting that might have serious\nconsequences.\nAnother issue, bias in medicine while using LLMs can affect\nclinical decision-making, patient outcomes, and healthcare equity.\nIf the training data contains biases, such as underrepresentation of\ncertain demographic groups, overemphasis on speci ﬁc treat-\nments, or outdated medical practices, LLMs may inadvertently\nlearn and propagate these biases in its outputs. Biased outputs\nfrom GPT-4 may lead to incorrect diagnoses or suboptimal\ntreatment recommendations, potentially causing harm to patients\nor delaying appropriate care.\nGPT-4 brings the potentials and the risks to a new level. It will\nbe able to read texts on images (including physicians ’ hand-\nwritten notes), and analyze the content and context of images.\nTable 1 summarizes the key differences between the previous and\nthe new version regarding healthcare-related and medical\nprompts. It shows that GPT-3 could handle simple prompts with\ngeneral queries, while GPT-4 is able to analyze complex, multi-\nlevel prompts, and provide more sophisticated results such as case\ndescriptions or research paper summaries.\nThe application of GPT-4 in healthcare raises ethical concerns\nthat warrant a regulatory framework. Issues such as transparency,\naccountability, and fairness need to be addressed to prevent\npotential ethical lapses. For instance, healthcare professionals and\npatients should be made aware of the AI’s involvement in the\ndecision-making process and be provided with explanations for\nthe AI’s recommendations.\nMoreover, regulatory oversight can help ensure that AI-driven\nmodels do not perpetuate or exacerbate existing healthcare\ndisparities. By mandating diverseand representative data sources,\nregulators can counteract potential biases within the AI’s training\ndata, thus promoting fairness in the delivery of healthcare services.\nThe use of GPT-4 and ChatGPT in such environments calls for\nrobust regulations to ensure the conﬁdentiality and security of\npatient information. This could include speciﬁc guidelines for data\nanonymization, encryption, and secure storage, as well as measures\nto prevent unauthorized access or misuse of data by third parties.\nAs a sign of wide implementation, medical companies, digital\nhealth services and healthcare organizations have already started\nto implement ChatGPT into their core business. Examples include\nthe Microsoft-owned Nuance as they decided to add GPT-4 AI to\nits medical note-taking tool; and a French startup called Nabla that\nclaimed to be the ﬁrst to build a tool using GPT-3 to help\nphysicians do their paperwork\n12,13.\nAll these examples and challenges prompt regulatory bodies to\nnot only start regulating LLMs as those models are being\ndeployed, but to regulate them differently that AI-technologies\ncurrently on the market.\nTHE REGULATORY CHALLENGES OF LLMS\nMost LLMs have been released globally and no country-speciﬁc\niterations are available requiring a global approach from regulators.\nIt is also not clear what technical category LLMs will fall into from the\nregulatory perspective. However, based on the differences between\nLLMs and prior deep learning methods, a new regulatory category\nmight be needed to address LLM-speciﬁc challenges and risks.\nA regulatory body only has to design regulations for LLMs if\neither the developers of LLMs make claims that their LLM can be\nused for a medical purpose; or if LLMs are developed for, adapted,\nmodiﬁed or directed toward speciﬁcally medical purposes. Even if\ncurrently widespread LLMs won’t fall into either category, the\nmedical alternatives of LLMs speciﬁcally trained on medical data\nand databases probably will.\nFig. 1 Ten examples of use cases of LLMs for medical professionals; and ten examples for patients.\nB. Meskó and E.J. Topol\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   120 \nOne prominent example is Med-PaLM that DeepMind and\nGoogle researchers have published about. In that study, authors\nproposed a framework for human evaluation of model answers\nalong multiple axes including factuality, precision, possible harm,\nand bias. In addition, using a combination of prompting strategies,\ntheir model achieved 67.6% accuracy on the US Medical License\nExam questions, surpassing prior state-of-the-art by over 17%. As\nhuman evaluation reveals key gaps in the responses provided by\nthe LLM, they introduced instruction prompt tuning and the\nresulting model, Med-PaLM, performs encouragingly, but remains\ninferior to clinicians. Since then, GPT-4 could achieve an accuracy\nover 85% on the same exam\n14.\nWith the release of GPT-4 that can analyze not only texts but\nimages, it can be expected that the model will grow to analyze\nuploaded documents, research papers, hand-written notes, sound,\nand video in the near future. (Table2).\nThis underscores the notion that it is not enough to regulate\ncurrent LLM models as the new iterations with those advanced\ncapabilities can be expected to get implemented at a similar rate\nof the previous iterations. Without taking these future additions\ninto consideration, a regulation that focuses on language models\nonly could miss important updates by the time those updates\nbecome widely accessible.\nCompanies with approved devices that decide to implement\nLLMs into their services face an additional challenge. Namely, how\nwill the FDA regulate an AI-based medical technology recently\ninfused with LLM if the technology was already approved for\nmedical uses? Table3 summarizes the regulatory challenges.\nThere have been proposals about regulating LLMs, although those\ncome from outside healthcare. In a working paper, Hacker et al.\nsuggests a novel terminology to capture the AI value chain by\ndifferentiating between developers,d e p l o y e r s ,p r o f e s s i o n a la n dn o n -\nprofessional users, as well as recipients of LLM output. Authors also\nsuggested four strategies to ensure that these models are\ntrustworthy and deployed for the beneﬁt of society at large. In\ndetails, regulation should focus onconcrete high-risk applications,\nand not the pre-trained model itself, and should include (i)\nobligations regarding transparency, (ii) risk management, (iii) non-\ndiscrimination provisions, and(iv) content moderation rules\n15.\nMökander at al pointed out that existing auditing procedures\nfail to address the governance challenges posed by LLMs, and\noffered three contributions toﬁll that gap namely 1) establishing\nthe need to develop new auditing procedures that capture the\nrisks posed by LLMs; 2) outlining a blueprint to audit LLMs in\nfeasible and effective ways by drawing on best practices from IT\ngovernance and system engineering; and 3) discussing the\nlimitations of the prospect of auditing LLMs at all\n16.\nSuch potential solutions could serve as a benchmark for new\nregulations in healthcare. In either case, regulators and lawmakers\nneed to act fast to keep track with the dynamics of the\nunprecedented evolution and progress of LLMs.\nAs a sign of the rising pressure on regulators, in March 2023, a\ngroup of prominent computer scientists and technology industry\nexecutives such as Elon Musk and Steve Wozniak called for“all AI\nlabs to immediately pause for at least 6 months the training of AI\nsystems more powerful than GPT-4”\n17. Their letter mentioned that\n“recent months have seen AI labs locked in an out-of-control race\nto develop and deploy ever more powerful digital minds that no\none – not even their creators– can understand, predict, or reliably\ncontrol. This pause should be public and veriﬁable, and include all\nTable 2. A list of types of content forms that LLMs could analyze now and possible new versions in the future.\nType of content Potential applications Availability\ntext/conversations chatbots, text analysis, documentation Yes\nimage analysis detecting the content and the context of images In 2023\ndocument/PDF analysis analyzing research papers and creating summaries of documents N/A\nsound voice-to-text applications and sound-based interactions N/A\nvideo analyzing the content of videos and creating deepfakes N/A\nTable 1. Differences between the depth and details of prompts for ChatGPT and GPT-4.\nPrompts ChatGPT GPT-4\nPrompt 1 – Diagnosing a patient\nwith ambiguous symptoms\nA patient presents with fatigue, weight loss,\nand occasional dizziness. What are some\npossible causes for these symptoms?\nA 45-year-old male patient presents with a 3-month history of\nprogressive fatigue, unintentional weight loss of 15 pounds,\nand episodes of dizziness. Please provide a differential\ndiagnosis and suggest relevant diagnostic tests.\nPrompt 2 – Treatment\nrecommendations\nWhat are some common treatments for type 2\ndiabetes?\nA 55-year-old female with a recent diagnosis of type 2\ndiabetes has an HbA1c level of 8.5%. Outline a comprehensive\ntreatment plan, including lifestyle modiﬁcations,\npharmacological options, and follow-up monitoring.\nPrompt 3 – Patient education Explain high blood pressure in simple terms. Create a patient-friendly educational handout on\nhypertension, including an overview of the condition, risk\nfactors, symptoms, potential complications, and management\nstrategies.\nPrompt 4 – Reviewing medical\nresearch\nTell me about the beneﬁts of exercise for\nmental health.\nSummarize recent researchﬁndings on the relationship\nbetween physical activity and mental health outcomes,\nincluding potential mechanisms, types of exercise, and\nrecommendations for various populations.\nPrompt 5 – Clinical case scenario Describe a patient with pneumonia. Create a detailed clinical case scenario involving a 65-year-old\npatient presenting with community-acquired pneumonia,\nincluding history of present illness, relevant past medical\nhistory, physical examinationﬁndings, diagnostic test results,\nand treatment plan.\nB. Meskó and E.J. Topol\n4\nnpj Digital Medicine (2023)   120 Published in partnership with Seoul National University Bundang Hospital\nkey actors. If such a pause cannot be enacted quickly, govern-\nments should step in and institute a moratorium.”\nNotable AI experts such as Andrew Ng objected the idea and\ninstead, called for seeking a balance between the huge value AI is\ncreating vs realistic risks. We agree that a moratorium cannot be\nimplemented in practice unless governments step in; and“having\ngovernments pause emerging technologies they don’t understand\nis anti-competitive, sets a terrible precedent, and is awful\ninnovation policy”\n18.\nTo reinforce our concerns, it is worthy of mention that Italy became\nthe ﬁrst Western country to temporarily block ChatGPT in April 2023\ndue to privacy concerns and the lack of proper regulation19.\nCONCLUSIONS\nLLMs offer tremendous promise for the future of healthcare, but\ntheir use also entails risks and ethical challenges. By taking a\nproactive approach to regulation, it is possible to harness the\npotential of AI-driven technologies like LLMs while minimizing\npotential harm and preserving the trust of patients and healthcare\nproviders alike.\nFurthermore, LLMs could also become the ﬁrst category of\nAI-based medical technologies that are regulated by imple-\nmenting patient design, me aning, regulators would ﬁnally\ninvolve patients on the highe st level of decision-making\nensuring that these AI tools that are progressing at an\nincredibly fast pace will be regulated to address real-life\nclinical and patient needs\n20.\nHere we summarize what we can expect regulators to do about\nbringing LLMs to the practice of medicine.\n- Create a new regulatory category for LLMs as those are\ndistinctively different from AI-based medical technologies\nthat have gone through regulation already.\n- Provide a regulatory guidance for companies and healthcare\norganizations about how they can deploy LLMs into their\nexisting products and services.\n- Create a regulatory framework that not only covers text-\nbased interactions but possible future iterations such as\nanalyzing sound or video.\n- Provide a framework for making a distinction between LLMs\nspeciﬁcally trained on medical data and LLMs trained for\nnon-medical purposes.\n- Similar to the FDA ’s Digital Health Pre-Cert Program,\nregulate companies developing LLMs instead of regulating\nevery single LLM iteration.\nReporting summary\nFurther information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nReceived: 7 April 2023; Accepted: 26 June 2023;\nREFERENCES\n1. Introducing ChatGPT. OpenAI,https://openai.com/blog/chatgpt (2022).\n2. Pichai, S. An important next step on our AI journey. Google The Keyword,https://\nblog.google/technology/ai/bard-google-ai-search-updates/ (2023).\n3. Sallam, M. The utility of chatGPT as an example of large language models in\nhealthcare education, research and practice: systematic review on the future\nperspectives and potential limitations. medRxiv, https://doi.org/10.1101/\n2023.02.19.23286155 (2023).\n4 . L i ,J . ,D a d a ,A . ,K l e e s i e k ,J .&E g g e r ,J .C h a t G P Ti nh e a l t h c a r e :at a x o n o m ya n d\nsystematic review. medRxiv, https://doi.org/10. 1101/2023.03.30.23287899\n(2023).\n5. Yaeger, K. A., Martini, M., Yaniv, G., Oermann, E. K. & Costa, A. B. United States\nregulatory approval of medical devices and software applications enhanced by\nartiﬁcial intelligence. Heal. Policy Technol. 8, 192–197 (2019).\n6. Benjamens, S., Dhunnoo, P. & Meskó, B. The state of artiﬁcial intelligence-based\nFDA-approved medical devices and algorithms: an online database. npj Digit.\nMed. 3,1 –8 (2020).\n7. FDA. Software as a Medical Device (SAMD): clinical evaluation. https://\nwww.fda.gov/media/100714/download (2017).\n8. Topol, E. J. High-performance medicin0e: the convergence of human and artiﬁcial\nintelligence. Nat. Med. 25,4 4–56 (2019).\n9. FDA. Arti ﬁcial intelligence and machine learning in software as a medical\ndevice. https://www.fda.gov/me dical-devices/softwa re-medical-device-\nsamd/arti ﬁcial-intelligence-and-machine-learning-software-medical-device\n(2021).\nTable 3. A list of regulatory challenges related to the rise of LLMs.\nRegulatory challenge Short description\nPatient Data Privacy Ensuring that patient data used for training large language models are fully anonymized and protected from\npotential breaches. This poses a signiﬁcant regulatory challenge, as any violation could lead to serious\nconsequences under privacy laws like HIPAA in the US.\nIntellectual Property If an LLM generates content similar to proprietary medical research or literature, it could lead to issues\nregarding intellectual property rights.\nMedical Malpractice Liability Determining who is responsible when an AI ’s recommendations lead to patient harm. Is it the AI developers,\nthe healthcare professionals who used it, or the institutions that adopted it?\nQuality Control & Standardization Regulation is required to ensure the reliability and consistency of AI-generated medical advice, which can vary\nbased on the data used to train the AI.\nInformed Consent Patients need to be informed and give consent when AI tools are used in their healthcare management. This is\nchallenging because it can be difﬁcult for patients to fully understand the implications of AI use.\nInterpretability & Transparency Regulations need to ensure transparency about how decisions are made by the AI. This is particularly\nchallenging with AI models that are often termed as \"black boxes\" due to their complex algorithms.\nFairness and Bias Regulation is needed to prevent biases in AI models, which could be introduced during the training process\nusing patient data. This can lead to disparities in healthcare outcomes.\nData Ownership It can be challenging to de ﬁne and regulate who owns the data that large language models learn from,\nespecially when it comes to patient data.\nOver-reliance on AI Models Over-reliance on AI could lead to decreased human expertise and potential errors if the AI malfunctions or\nprovides incorrect information. Regulations are needed to balance the use of AI and human expertise.\nContinuous Monitoring & Validation Ensuring the continuous performance, accuracy, and validity of AI tools over time and across different\npopulations is a critical regulatory challenge.\nB. Meskó and E.J. Topol\n5\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   120 \n10. Nagendran, M. et al. Artiﬁcial intelligence versus clinicians: systematic review of\ndesign, reporting standards, and claims of deep learning studies.BMJ 368, m689\n(2020).\n11. Lee, P., Bubeck, S. & Petro, J. Beneﬁts, limits, and risks of GPT-4 as an AI chatbot\nfor medicine. N. Engl. J. Med.388, 1233–1239 (2023).\n12. Nuance. Nuance is revolutionizing the contact center with GPT technology(Nuance,\n2023).\n13. Lunden, I. Nabla, a digital health startup, launches Copilot, using GPT-3 to turn\npatient conversations into action(TechCrunch, 2023).\n14. Singhal K., et al. Large language models encode clinical knowledge. Preprint at\nhttps://arxiv.org/abs/2212.13138 (2022).\n15. Hacker, P., Engel, A. & Mauer, M. Regulating ChatGPT and other Large Generative\nAI Models. InProceedings of the 2023 ACM Conference on Fairness, Accountability,\nand Transparency (FAccT '23), 1112–1123 (Association for Computing Machinery,\nNew York, NY, USA, 2023).https://doi.org/10.1145/3593013.3594067.\n16. Mökander, J. et al. Auditing large language models: a three-layered approach. AI\nEthics. https://doi.org/10.1007/s43681-023-00289-2 (2023).\n17. Will Knight, P. D.In sudden alarm, tech doyens call for a pause on ChatGPT(Wired,\n2023).\n18. Ng, A. Andrew Ng ’s Twitter. Twitter https://twitter.com/AndrewYNg/status/\n1641121451611947009 (2023).\n19. McCallum, S. ChatGPT banned in Italy over privacy concerns(BBC, 2023).\n20. Meskó, B. & deBronkart, D. Patient design: the importance of including patients in\ndesigning health care.J. Med. Internet Res.24, e39178 (2022).\nAUTHOR CONTRIBUTIONS\nB.M. and E.T. developed the concept of the manuscript. B.M. drafted the manuscript\nand E.T. contributed to the writing, interpretation of the content, and editing of the\nmanuscript, revising it critically. Both authors approved the completed version.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41746-023-00873-0.\nCorrespondence and requests for materials should be addressed to Bertalan Meskó.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nB. Meskó and E.J. Topol\n6\nnpj Digital Medicine (2023)   120 Published in partnership with Seoul National University Bundang Hospital"
}