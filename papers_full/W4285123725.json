{
  "title": "On the Importance of Effectively Adapting Pretrained Language Models for Active Learning",
  "url": "https://openalex.org/W4285123725",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3089003676",
      "name": "Katerina Margatina",
      "affiliations": [
        "University of Sheffield",
        "Mansfield University"
      ]
    },
    {
      "id": "https://openalex.org/A2153569893",
      "name": "Loïc Barrault",
      "affiliations": [
        "University of Sheffield",
        "Mansfield University"
      ]
    },
    {
      "id": "https://openalex.org/A93365683",
      "name": "Nikolaos Aletras",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2396420927",
    "https://openalex.org/W1845402413",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W2065010255",
    "https://openalex.org/W2951147191",
    "https://openalex.org/W2774918944",
    "https://openalex.org/W3174481471",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4206734067",
    "https://openalex.org/W2951061410",
    "https://openalex.org/W3101345273",
    "https://openalex.org/W2948367246",
    "https://openalex.org/W2964282813",
    "https://openalex.org/W4310738704",
    "https://openalex.org/W2785787385",
    "https://openalex.org/W4206648492",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2951786554",
    "https://openalex.org/W2989499211",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2962892701",
    "https://openalex.org/W2970043232",
    "https://openalex.org/W3156031277",
    "https://openalex.org/W2970450573",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1872312298",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2788686132"
  ],
  "abstract": "Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 825 - 836\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nOn the Importance of Effectively Adapting Pretrained Language Models\nfor Active Learning\nKaterina Margatina♠ Loïc Barrault♣ Nikolaos Aletras♠\n♠University of Sheffield, ♣University of Le Mans\n{k.margatina,n.aletras}@sheffield.ac.uk\nloic.barrault@univ-lemans.fr\nAbstract\nRecent Active Learning (AL) approaches in\nNatural Language Processing (NLP) proposed\nusing off-the-shelf pretrained language mod-\nels (LMs). In this paper, we argue that these\nLMs are not adapted effectively to the down-\nstream task during AL and we explore ways\nto address this issue. We suggest to first adapt\nthe pretrained LM to the target task by contin-\nuing training with all the available unlabeled\ndata and then use it for AL. We also propose a\nsimple yet effective fine-tuning method to en-\nsure that the adapted LM is properly trained\nin both low and high resource scenarios dur-\ning AL. Our experiments demonstrate that our\napproach provides substantial data efficiency\nimprovements compared to the standard fine-\ntuning approach, suggesting that a poor training\nstrategy can be catastrophic for AL.1\n1 Introduction\nActive Learning (AL) is a method for training su-\npervised models in a data-efficient way (Cohn et al.,\n1996; Settles, 2009). It is especially useful in sce-\nnarios where a large pool of unlabeled data is avail-\nable but only a limited annotation budget can be af-\nforded; or where expert annotation is prohibitively\nexpensive and time consuming. AL methods iter-\natively alternate between (i) model training with\nthe labeled data available; and (ii) data selection\nfor annotation using a stopping criterion, e.g. until\nexhausting a fixed annotation budget or reaching a\npre-defined performance on a held-out dataset.\nData selection is performed by an acquisition\nfunction that ranks unlabeled data points by some\ninformativeness metric aiming to improve over ran-\ndom selection, using either uncertainty (Lewis and\nGale, 1994; Cohn et al., 1996; Gal et al., 2017;\nKirsch et al., 2019; Zhang and Plank, 2021), di-\nversity (Brinker, 2003; Bodó et al., 2011; Sener\n1For all experiments in this paper, we have used the code\nprovided by Margatina et al. (2021): https://github.\ncom/mourga/contrastive-active-learning\nand Savarese, 2018), or both (Ducoffe and Pre-\ncioso, 2018; Ash et al., 2020; Yuan et al., 2020;\nMargatina et al., 2021).\nPrevious AL approaches in NLP use task-\nspecific neural models that are trained from scratch\nat each iteration (Shen et al., 2017; Siddhant and\nLipton, 2018; Prabhu et al., 2019; Ikhwantri et al.,\n2018; Kasai et al., 2019). However, these models\nare usually outperformed by pretrained language\nmodels (LMs) adapted to end-tasks (Howard and\nRuder, 2018), making them suboptimal for AL.\nOnly recently, pretrained LMs such as BERT (De-\nvlin et al., 2019) have been introduced in AL set-\ntings (Yuan et al., 2020; Ein-Dor et al., 2020; Shel-\nmanov et al., 2021; Karamcheti et al., 2021; Mar-\ngatina et al., 2021). Still, they are trained at each\nAL iteration with a standard fine-tuning approach\nthat mainly includes a pre-defined number of train-\ning epochs, which has been demonstrated to be\nunstable, especially in small datasets (Zhang et al.,\n2020; Dodge et al., 2020; Mosbach et al., 2021).\nSince AL includes both low and high data resource\nsettings, the AL model training scheme should be\nrobust in both scenarios.2\nTo address these limitations, we introduce a suite\nof effective training strategies for AL (§2). Con-\ntrary to previous work (Yuan et al., 2020; Ein-Dor\net al., 2020; Margatina et al., 2021) that also use\nBERT (Devlin et al., 2019), our proposed method\naccounts for various data availability settings and\nthe instability of fine-tuning. First, we continue\npretraining the LM with the available unlabeled\ndata to adapt it to the task-specific domain. This\nway, we leverage not only the available labeled data\nat each AL iteration, but the entire unlabeled pool.\nSecond, we further propose a simple yet effective\nfine-tuning method that is robust in both low and\nhigh resource data settings for AL.\n2During the first few AL iterations the available labeled\ndata is limited ( low-resource), while it could become very\nlarge towards the last iterations (high-resource).\n825\nWe explore the effectiveness of our approach on\nfive standard natural language understandings tasks\nwith various acquisition functions, showing that it\noutperforms all baselines (§3). We also conduct an\nanalysis to demonstrate the importance of effective\nadaptation of pretrained models for AL (§4). Our\nfindings highlight that the LM adaptation strategy\ncan be more critical than the actual data acquisition\nstrategy.\n2 Adapting & Fine-tuning Pretrained\nModels for Active Learning\nGiven a downstream classification task with C\nclasses, a typical AL setup consists of a pool of\nunlabeled data Dpool, a model M, an annotation\nbudget b of data points and an acquisition function\na(.) for selecting k unlabeled data points for anno-\ntation (i.e. acquisition size) until b runs out. The\nAL performance is assessed by training a model on\nthe actively acquired dataset and evaluating on a\nheld-out test set Dtest.\nAdaptation (TAPT ) Inspired by recent work on\ntransfer learning that shows improvements in down-\nstream classification performance by continuing the\npretraining of the LM with the task data (Howard\nand Ruder, 2018) we add an extra step to the\nAL process by continuing pretraining the LM (i.e.\nTask-Adaptive Pretraining TAPT), as in Gururan-\ngan et al. (2020). Formally, we use an LM, such as\nBERT (Devlin et al., 2019), P(x; W0) with weights\nW0, that has been already pretrained on a large\ncorpus. We fine-tune P(x; W0) with the available\nunlabeled data of the downstream task Dpool, re-\nsulting in the task-adapted LM PTAPT(x; W′\n0) with\nnew weights W′\n0 (cf. line 2 of algorithm 1).\nFine-tuning ( FT+) We now use the adapted\nLM PTAPT(x; W′\n0) for AL. At each iteration i,\nwe initialize our model Mi with the pretrained\nweights W′\n0 and we add a task-specific feedfor-\nward layer for classification with weights Wc on\ntop of the [CLS] token representation of BERT-\nbased PTAPT. We fine-tune the classification model\nMi(x; [W′\n0, Wc]) with all x ∈ Dlab. (cf. line 6 to\n8 of algorithm 1).\nRecent work in AL (Ein-Dor et al., 2020; Yuan\net al., 2020) uses the standard fine-tuning method\nproposed in Devlin et al. (2019) which includes\na fixed number of 3 training epochs, learning rate\nwarmup over the first10% of the steps and AdamW\noptimizer (Loshchilov and Hutter, 2019) without\nAlgorithm 1: AL with Pretrained LMs\nInput: unlabeled data Dpool, pretrained LM\nP(x; W0), acquisition size k, AL\niterations T, acquisition function a\n1 Dlab ← ∅\n2 PTAPT(x; W′\n0) ← Train P(x; W0) on Dpool\n3 Q0 ← RANDOM(.), |Q0| = k\n4 Dlab = Dlab ∪ Q0\n5 Dpool = Dpool \\ Q0\n6 for i ← 1 to T do\n7 Mi(x; [W′\n0, Wc]) ← Initialize from\nPTAPT(x; W′\n0)\n8 Mi(x; Wi) ← Train model on Dlab\n9 Qi ← a(Mi, Dpool, k)\n10 Dlab = Dlab ∪ Qi\n11 Dpool = Dpool \\ Qi\n12 end\nOutput: Dlab\nbias correction, among other hyperparameters.\nWe follow a different approach by taking into\naccount insights from few-shot fine-tuning liter-\nature (Mosbach et al., 2021; Zhang et al., 2020;\nDodge et al., 2020) that proposes longer fine-tuning\nand more evaluation steps during training. 3 We\ncombine these guidelines to our fine-tuning ap-\nproach by using early stopping with 20 epochs\nbased on the validation loss, learning rate 2e − 5,\nbias correction and 5 evaluation steps per epoch.\nHowever, increasing the number of epochs from\n3 to 20, also increases the warmup steps (10% of\ntotal steps4) almost 7 times. This may be problem-\natic in scenarios where the dataset is large but the\noptimal number of epochs may be small (e.g. 2 or\n3). To account for this limitation in our AL setting\nwhere the size of training set changes at each it-\neration, we propose to select the warmup steps as\nmin(10% of total steps, 100). We denote standard\nfine-tuning as SFT and our approach as FT+.\n3 Experiments & Results\nData We experiment with five diverse natural lan-\nguage understanding tasks: question classification\n3In this paper we usefew-shot to describe the setting where\nthere are few labeled data available and thereforefew-shot fine-\ntuning corresponds to fine-tuning a model on limited labeled\ntraining data. This is different than the few-shot setting pre-\nsented in recent literature (Brown et al., 2020), where no\nmodel weights are updated.\n4Some guidelines propose an even smaller number of\nwarmup steps, such as 6% in RoBERTa (Liu et al., 2020).\n826\n2 4 6 8 10 12 14\nAcquired dataset size (%)\n10\n20\n30\n40\n50\n60\n70\n80\n90\nTREC-6\n2 4 6 8 10 12 14\nAcquired dataset size (%)\n20\n30\n40\n50\n60\n70\n80\n90\n100\nDBPEDIA\n2 4 6 8 10 12 14\nAcquired dataset size (%)\n55\n60\n65\n70\n75\n80\n85\n90\n95\nIMDB\n2 4 6 8 10 12 14\nAcquired dataset size (%)\n82\n84\n86\n88\n90\n92\nSST-2\n2 4 6 8 10 12 14\nAcquired dataset size (%)\n87\n88\n89\n90\n91\n92\n93\n94\n95\nAGNEWS\nFigure 1: Test accuracy during AL iterations. We plot the median and standard deviation across five runs.\nDATASETS TRAIN VAL TEST k C\nTREC-6 4.9K 546 500 1% 6\nDBPEDIA 20K 2K 70K 1% 14\nIMDB 22.5K 2.5K 25K 1% 2\nSST-2 60.6K 6.7K 871 1 % 2\nAGNEWS 114K 6K 7.6K 0.5% 4\nTable 1: Datasets statistics for Dpool, Dval and Dtest\nrespectively. k stands for the acquisition size (% of\nDpool) and C the number of classes.\n(TREC -6; V oorhees and Tice (2000)), sentiment\nanalysis (IMDB ; Maas et al. (2011), SST-2 Socher\net al. (2013)) and topic classification ( DBPEDIA ,\nAGNEWS ; Zhang et al. (2015)), including binary\nand multi-class labels and varying dataset sizes (Ta-\nble 1). More details can be found in Appendix A.1.\nExperimental Setup We perform all AL experi-\nments using BERT-base (Devlin et al., 2019) and\nENTROPY , BERT KM, ALPS (Yuan et al., 2020),\nBADGE (Ash et al., 2020) and RANDOM (baseline)\nas the acquisition functions. We pair our proposed\ntraining approach TAPT-FT+ with ENTROPY ac-\nquisition. We refer the reader to Appendix A for\nan extended description of our experimental setup,\nincluding the datasets used (§A.1), the training\nand AL details (§A.2), the model hyperparameters\n(§A.3) and the baselines (§A.4).\nResults Figure 1 shows the test accuracy during\nAL iterations. We first observe that our proposed\napproach (TAPT-FT+) achieves large data efficiency\nreaching the full-dataset performance within the\n15% budget for all datasets, in contrast to the stan-\ndard AL approach (BERT-SFT ). The effectiveness\nof our approach is mostly notable in the smaller\ndatasets. In TREC -6, it achieves the goal accuracy\nwith almost 10% annotated data, while in DBPE -\nDIA only in the first iteration with 2% of the data.\nAfter the first AL iteration in IMDB , TAPT-FT+, it\nachieves only 2.5 points of accuracy lower than the\n827\nperformance when using 100% of the data. In the\nlarger SST-2 and AGNEWS datasets, it is closer to\nthe baselines but still outperforms them, achieving\nthe full-dataset performance with 8% and 12% of\nthe data respectively. We also observe that in all\nfive datasets, the addition of our proposed pretrain-\ning step ( TAPT) and fine-tuning technique ( FT+)\nleads to large performance gains, especially in the\nfirst AL iterations. This is particularly evident in\nTREC -6, DBPEDIA and IMDB datasets, where after\nthe first AL iteration (i.e. equivalent to 2% of train-\ning data) TAPT+FT+ with ENTROPY is 45, 30 and\n12 points in accuracy higher than the ENTROPY\nbaseline with BERT and SFT .\nTraining vs. Acquisition Strategy We finally\nobserve that the performance curves of the vari-\nous acquisition functions considered (i.e. dotted\nlines) are generally close to each other, suggesting\nthat the choice of the acquisition strategy may not\naffect substantially the AL performance in certain\ncases. In other words, we conclude thatthe training\nstrategy can be more important than the acquisi-\ntion strategy. We find that uncertainty sampling\nwith ENTROPY is generally the best performing\nacquisition function, followed by BADGE .5 Still,\nfinding a universally well-performing acquisition\nfunction, independent of the training strategy, is an\nopen research question.\n4 Analysis & Discussion\n4.1 Task-Adaptive Pretraining\nWe first present details of our implementation of\nTAPT (§2) and reflect on its effectiveness in the\nAL pipeline. Following Gururangan et al. (2020),\nwe continue pretraining BERT for the MLM task\nusing all the unlabeled data Dpool for all datasets\nseparately. We plot the learning curves of BERT-\nTAPT for all datasets in Figure 2. We first observe\nthat the masked LM loss is steadily decreasing for\nDBPEDIA , IMDB and AGNEWS across optimization\nsteps, which correlates with the high early AL per-\nformance gains of TAPT in these datasets (Fig. 1).\nWe also observe that the LM overfits in TREC -6\nand SST-2 datasets. We attribute this to the very\nsmall training dataset of TREC -6 and the informal\ntextual style of SST-2. Despite the fact that the\nSST-2 dataset includes approximately 67K of train-\ning data, the sentences are very short (i.e. average\n5We provide results with additional acquisition functions\nin the Appendix B.2 and B.3.\n0 20000 40000 60000 80000 100000\nSteps\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5Loss\nDataset\nTREC-6\nDBPEDIA\nIMDB\nSST-2\nAGNEWS\nFigure 2: Validation MLM loss during TAPT.\n100 1000 10000\nNumber of training samples\n50\n60\n70\n80\n90Accuracy\nEpochs\n3\n10\n20\nAG_NEWS\n100 1000 10000\nNumber of training samples\n50\n55\n60\n65\n70\n75\n80\n85\n90Accuracy\nEpochs\n3\n10\n20\nIMDB\nFigure 3: Few-shot standard BERT fine-tuning.\nlength of 9.4 words per sentence). We hypothesize\nthe LM overfits because of the lack of long and\nmore diverse sentences. We provide more details\non TAPT at the Appendix B.1.\n4.2 Few-shot Fine-tuning\nIn this set of experiments, we aim to highlight that it\nis crucial to consider the few-shot learning problem\nin the early AL stages, which is often neglected\nin literature. This is more important when using\npretrained LMs, since they are overparameterized\nmodels that require adapting their training scheme\nin low data settings to ensure robustness.\nTo illustrate the potential ineffectiveness of stan-\ndard fine-tuning ( SFT ), we randomly undersam-\nple the AGNEWS and IMDB datasets to form low,\nmedium and high resource data settings (i.e. 100,\n1, 000 and 10, 000 training samples), and train\nBERT for a fixed number of 3, 10, and 20 epochs.\nWe repeat this process with 10 different random\nseeds to account for stochasticity in sampling and\nwe plot the test accuracy in Figure 3. Figure 3\nshows that SFT is suboptimal for low data settings\n(e.g. 100 samples), indicating that more optimiza-\ntion steps (i.e. epochs) are needed for the model\nto adapt to the few training samples (Zhang et al.,\n2020; Mosbach et al., 2021). As the training sam-\nples increase (e.g. 1, 000), fewer epochs are of-\nten better. It is thus evident that there is not a\nclearly optimal way to choose a predefined number\n828\nof epochs to train the model given the number of\ntraining examples. This motivates the need to find\na fine-tuning policy for AL that effectively adapts\nto the data resource setting of each iteration (inde-\npendently of the number of training examples or\ndataset), which is mainly tackled by our proposed\nfine-tuning approach FT+ (§2).\n4.3 Ablation Study\nWe finally conduct an ablation study to evaluate\nthe contribution of our two proposed steps to the\nAL pipeline; the pretraining step (TAPT) and fine-\ntuning method (FT+). We show that the addition\nof both methods provides large gains compared\nto standard fine-tuning ( SFT ) in terms of accu-\nracy, data efficiency and uncertainty calibration.\nWe compare BERT with SFT , BERT with FT+ and\nBERT-TAPT with FT+. Along with test accuracy,\nwe also evaluate each model using uncertainty esti-\nmation metrics (Ovadia et al., 2019): Brier score,\nnegative log likelihood (NLL), expected calibration\nerror (ECE) and entropy. A well-calibrated model\nshould have high accuracy and low uncertainty.\nFigure 4 shows the results for the smallest and\nlargest datasets, TREC -6 and AGNEWS respectively.\nFor TREC -6, training BERT with our fine-tuning\napproach FT+ provides large gains both in accu-\nracy and uncertainty calibration, showing the im-\nportance of fine-tuning the LM for a larger number\nof epochs in low resource settings. For the larger\ndataset, AGNEWS , we see that BERT with SFT per-\nforms equally to FT+ which is the ideal scenario.\nWe see that our fine-tuning approach does not de-\nteriorate the performance of BERT given the large\nincrease in warmup steps, showing that our sim-\nple strategy provides robust results in both high\nand low resource settings. After demonstrating\nthat FT+ yields better results than SFT , we next\ncompare BERT-TAPT-FT+ against BERT-FT+. We\nobserve that in both datasets BERT-TAPT outper-\nforms BERT, with this being particularly evident in\nthe early iterations. This confirms our hypothesis\nthat by implicitly using the entire pool of unlabeled\ndata for extra pretraining (TAPT), we boost the per-\nformance of the AL model using less data.\n5 Conclusion\nWe have presented a simple yet effective training\nscheme for AL with pretrained LMs that accounts\nfor varying data availability and instability of fine-\ntuning. Specifically, we propose to first continue\n25\n50\n75Accuracy\n4.9K training data (100%)\n0.05\n0.10Brier\n1.0\n1.5NLL\n0.0\n0.1\n0.2\n0.3ECE\n0 200 400 600\nAcquired dataset size\n1\n2Entropy\nTREC-6\n87.5\n90.0\n92.5\n95.0Accuracy\n114K training data (100%)\n0.02\n0.04\n0.06Brier\n0.2\n0.3\n0.4\n0.5NLL\n0.0\n0.1ECE\n0 5000 10000 15000\nAcquired dataset size\n0.5\n1.0Entropy\nAGNEWS\nFigure 4: Ablation study for TAPT and FT+.\npretraining the LM with the available unlabeled\ndata to adapt it to the task-specific domain. This\nway, we leverage not only the available labeled data\nat each AL iteration, but the entire unlabeled pool.\nWe further propose a method tofine-tune the model\nduring AL iterations so that training is robust in\nboth low and high resource data settings.\nOur experiments show that our approach yields\nsubstantially better results than standard fine-tuning\nin five standard NLP datasets. Furthermore, we find\nthat the training strategy can be more important\nthan the acquisition strategy . In other words, a\npoor training strategy can be a crucial impediment\nto the effectiveness of a good acquisition function,\nand thus limit its effectiveness (even over random\nsampling). Hence, our work highlights how critical\nit is to properly adapt a pretrained LM to the low\ndata resource AL setting.\nAs state-of-the-art models in NLP advance\nrapidly, in the future we would be interested in\nexploring the use of larger LMs, such as GPT-\n3 (Brown et al., 2020) and FLAN (Wei et al.,\n2022). These models have achieved impressive\nperformance in very low data resource settings (e.g.\nzero-shot and few-shot), so we would imagine they\nwould be good candidates for the challenging set-\nting of active learning.\n829\nAcknowledgments\nWe would like to thank Giorgos Vernikos, our col-\nleagues at the Sheffield NLP group for feedback on\nan earlier version of this paper, and all the anony-\nmous reviewers for their constructive comments.\nKM and NA are supported by Amazon through the\nAlexa Fellowship scheme.\nReferences\nJordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy,\nJohn Langford, and Alekh Agarwal. 2020. Deep\nbatch active learning by diverse, uncertain gradient\nlower bounds. In International Conference on Learn-\ning Representations.\nZalán Bodó, Zsolt Minier, and Lehel Csató. 2011. Ac-\ntive learning with clustering. In Proceedings of the\nActive Learning and Experimental Design workshop\nIn conjunction with AISTATS 2010, volume 16, pages\n127–139.\nKlaus Brinker. 2003. Incorporating diversity in active\nlearning with support vector machines. In Proceed-\nings of the International Conference on Machine\nLearning, pages 59–66.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nDavid A. Cohn, Zoubin Ghahramani, and Michael I.\nJordan. 1996. Active learning with statistical mod-\nels. Journal of Artificial Intelligence Research ,\n4(1):129–145.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 4171–4186.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. ArXiv.\nMelanie Ducoffe and Frederic Precioso. 2018. Adver-\nsarial active learning for deep networks: a margin\nbased approach.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim. 2020.\nActive learning for BERT: An empirical study. In\nProceedings of theConference on Empirical Methods\nin Natural Language Processing, pages 7949–7962.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as\na bayesian approximation: Representing model un-\ncertainty in deep learning. In Proceedings of the\nInternational Conference on Machine Learning, vol-\nume 48, pages 1050–1059.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017.\nDeep Bayesian active learning with image data. In\nProceedings of the International Conference on Ma-\nchine Learning, volume 70, pages 1183–1192.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nNeil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and\nMáté Lengyel. 2011. Bayesian active learning for\nclassification and preference learning. ArXiv.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification. In\nProceedings of the Annual Meeting of the Association\nfor Computational Linguistics, pages 328–339.\nFariz Ikhwantri, Samuel Louvan, Kemal Kurniawan,\nBagas Abisena, Valdi Rachman, Alfan Farizki Wicak-\nsono, and Rahmad Mahendra. 2018. Multi-task ac-\ntive learning for neural semantic role labeling on\nlow resource conversational corpus. In Proceedings\nof the Workshop on Deep Learning Approaches for\nLow-Resource NLP, pages 43–50.\nSiddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and\nChristopher Manning. 2021. Mind your outliers! in-\nvestigating the negative impact of outliers on active\nlearning for visual question answering. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7265–7281, Online.\nAssociation for Computational Linguistics.\nJungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li,\nand Lucian Popa. 2019. Low-resource deep entity\nresolution with transfer and active learning. In Pro-\nceedings of the Conference of the Association for\nComputational Linguistic, pages 5851–5861.\n830\nAndreas Kirsch, Joost van Amersfoort, and Yarin Gal.\n2019. BatchBALD: Efficient and diverse batch ac-\nquisition for deep bayesian active learning. In Neural\nInformation Processing Systems, pages 7026–7037.\nDavid D. Lewis and William A. Gale. 1994. A se-\nquential algorithm for training text classifiers. In\nIn Proceedings of the Annual International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nDavid Lowell and Zachary C Lipton. 2019. Practical\nobstacles to deploying active learning. Proceedings\nof the Conference on Empirical Methods in Natu-\nral Language Processing and the International Joint\nConference on Natural Language Processing, pages\n21–30.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan\nHuang, Andrew Y . Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the Annual Meeting of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 142–150.\nKaterina Margatina, Giorgos Vernikos, Loïc Barrault,\nand Nikolaos Aletras. 2021. Active learning by ac-\nquiring contrastive examples. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 650–663, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of fine-tuning\n{bert}: Misconceptions, explanations, and strong\nbaselines. In International Conference on Learning\nRepresentations.\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado,\nD. Sculley, Sebastian Nowozin, Joshua Dillon, Bal-\naji Lakshminarayanan, and Jasper Snoek. 2019. Can\nyou trust your model's uncertainty? evaluating predic-\ntive uncertainty under dataset shift. In Advances in\nNeural Information Processing Systems, volume 32,\npages 13991–14002.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems, pages 8024–8035.\nAmeya Prabhu, Charles Dognin, and Maneesh Singh.\n2019. Sampling bias in deep active classification: An\nempirical study. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\nand the International Joint Conference on Natural\nLanguage Processing, pages 4056–4066.\nOzan Sener and Silvio Savarese. 2018. Active learn-\ning for convolutional neural networks: A core-set\napproach. In International Conference on Learning\nRepresentations.\nBurr Settles. 2009. Active learning literature survey.\nComputer sciences technical report.\nClaude Elwood Shannon. 1948. A mathematical the-\nory of communication. The Bell System Technical\nJournal.\nArtem Shelmanov, Dmitri Puzyrev, Lyubov\nKupriyanova, Denis Belyakov, Daniil Larionov,\nNikita Khromov, Olga Kozlova, Ekaterina Artemova,\nDmitry V . Dylov, and Alexander Panchenko. 2021.\nActive learning for sequence tagging with deep\npre-trained models and Bayesian uncertainty\nestimates. In Proceedings of the 16th Conference\nof the European Chapter of the Association for\nComputational Linguistics: Main Volume , pages\n1698–1712, Online. Association for Computational\nLinguistics.\nYanyao Shen, Hyokun Yun, Zachary Lipton, Yakov\nKronrod, and Animashree Anandkumar. 2017. Deep\nactive learning for named entity recognition. In Pro-\nceedings of the Workshop on Representation Learn-\ning for NLP, pages 252–256.\nAditya Siddhant and Zachary C Lipton. 2018. Deep\nbayesian active learning for natural language pro-\ncessing: Results of a Large-Scale empirical study. In\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing, pages 2904–2909.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the Conference on Empirical Meth-\nods in Natural Language Processing , pages 1631–\n1642.\nN Srivastava, G Hinton, A Krizhevsky, and others. 2014.\nDropout: a simple way to prevent neural networks\nfrom overfitting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nEllen V oorhees and Dawn Tice. 2000. The trec-8 ques-\ntion answering track evaluation. Proceedings of the\nText Retrieval Conference.\n831\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Interna-\ntional Conference on Learning Representations.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing: System Demonstra-\ntions, pages 38–45.\nMichelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-\nGraber. 2020. Cold-start active learning through\nself-supervised language modeling.\nMike Zhang and Barbara Plank. 2021. Cartography ac-\ntive learning. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 395–\n406, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein-\nberger, and Yoav Artzi. 2020. Revisiting few-sample\nbert fine-tuning. ArXiv.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28, pages 649–657. Curran\nAssociates, Inc.\n832\nA Appendix: Experimental Setup\nA.1 Datasets\nWe experiment with five diverse natural language\nunderstanding tasks including binary and multi-\nclass labels and varying dataset sizes (Table 1).\nThe first task is question classification using the six-\nclass version of the small TREC -6 dataset of open-\ndomain, fact-based questions divided into broad\nsemantic categories (V oorhees and Tice, 2000). We\nalso evaluate our approach on sentiment analysis\nusing the binary movie review IMDB dataset (Maas\net al., 2011) and the binary version of the SST-2\ndataset (Socher et al., 2013). We finally use the\nlarge-scale AGNEWS and DBPEDIA datasets from\nZhang et al. (2015) for topic classification. We\nundersample the latter and form a Dpool of 20K ex-\namples and Dval 2K as in Margatina et al. (2021).\nFor TREC -6, IMDB and SST-2 we randomly sample\n10% from the training set to serve as the valida-\ntion set, while for AGNEWS we sample 5%. For\nthe DBPEDIA dataset we undersample both training\nand validation datasets (from the standard splits)\nto facilitate our AL simulation (i.e. the original\ndataset consists of 560K training and 28K valida-\ntion data examples). For all datasets we use the\nstandard test set, apart from the SST-2 dataset that\nis taken from the GLUE benchmark (Wang et al.,\n2019) we use the development set as the held-out\ntest set (and subsample a development set from the\noriginal training set).\nA.2 Training & AL Details\nWe use BERT-BASE (Devlin et al., 2019) and fine-\ntune it ( TAPT §2) for 100K steps, with learning\nrate 2e − 05 and the rest of hyperparameters as in\nGururangan et al. (2020) using the HuggingFace\nlibrary (Wolf et al., 2020). We evaluate the model\n5 times per epoch on Dval and keep the one with\nthe lowest validation loss as in Dodge et al. (2020).\nWe use the code provided by Kirsch et al. (2019)\nfor the uncertainty-based acquisition functions and\nYuan et al. (2020) forALPS , BADGE and BERTKM.\nWe use the standard splits provided for all datasets,\nif available, otherwise we randomly sample a val-\nidation set. We test all models on a held-out test\nset. We repeat all experiments with five different\nrandom seeds resulting into different initializations\nof Dlab and the weights of the extra task-specific\noutput feedforward layer. For all datasets we use as\nbudget the 15% of Dpool. Each experiment is run\non a single Nvidia Tesla V100 GPU.\nA.3 Hyperparameters\nFor all datasets we train BERT-BASE (Devlin et al.,\n2019) from the HuggingFace library (Wolf et al.,\n2020) in Pytorch (Paszke et al., 2019). We train\nall models with batch size 16, learning rate 2e − 5,\nno weight decay, AdamW optimizer with epsilon\n1e − 8. For all datasets we use maximum sequence\nlength of 128, except for IMDB and AGNEWS that\ncontain longer input texts, where we use 256. To\nensure reproducibility and fair comparison between\nthe various methods under evaluation, we run all\nexperiments with the same five seeds that we ran-\ndomly selected from the range [1, 9999].\nA.4 Baselines\nAcquisition functions We compare EN-\nTROPY with four baseline acquisition functions.\nThe first is the standard AL baseline, RANDOM ,\nwhich applies uniform sampling and selects k data\npoints from Dpool at each iteration. The second is\nBADGE (Ash et al., 2020), an acquisition function\nthat aims to combine diversity and uncertainty\nsampling. The algorithm computes gradient\nembeddings gx for every candidate data point\nx in Dpool and then uses clustering to select a\nbatch. Each gx is computed as the gradient of the\ncross-entropy loss with respect to the parameters of\nthe model’s last layer. We also compare against a\nrecently introduced cold-start acquisition function\ncalled ALPS (Yuan et al., 2020). ALPS acquisition\nuses the masked language model (MLM) loss\nof BERT as a proxy for model uncertainty in\nthe downstream classification task. Specifically,\naiming to leverage both uncertainty and diversity,\nALPS forms a surprisal embedding sx for each x,\nby passing the unmasked input x through the BERT\nMLM head to compute the cross-entropy loss for\na random 15% subsample of tokens against the\ntarget labels. ALPS clusters these embeddings to\nsample k sentences for each AL iteration. Last,\nfollowing Yuan et al. (2020), we use BERT KM as\na diversity baseline, where the l2 normalized BERT\noutput embeddings are used for clustering.\nModels & Fine-tuning Methods We evaluate\ntwo variants of the pretrained language model; the\noriginal BERT model, used in Yuan et al. (2020)\nand Ein-Dor et al. (2020)6, and our adapted model\nBERT-TAPT (§2), and two fine-tuning methods;\n6Ein-Dor et al. (2020) evaluate various acquisition func-\ntions, including entropy with MC dropout, and use BERT with\nthe standard fine-tuning approach (SFT ).\n833\nour proposed fine-tuning approach FT+ (§2) and\nstandard BERT fine-tuning SFT .\nMODEL TREC -6 DBPEDIA IMDB SST-2 AGNEWS\nVALIDATIONSET\nBERT 94.4 99.1 90.7 93.7 94.4\nBERT-TAPT 95.2 99.2 91.9 94.3 94.5\nTESTSET\nBERT 80.6 99.2 91.0 90.6 94.0\nBERT-TAPT 77.2 99.2 91.9 90.8 94.2\nTable 2: Accuracy with 100% of data over five runs\n(different random seeds).\nB Appendix: Analysis\nB.1 Task-Adaptive Pretraining ( TAPT ) &\nFull-Dataset Performance\nAs discussed in §2 and §4, we continue training\nthe BERT-BASE (Devlin et al., 2019) pretrained\nmasked language model using the available data\nDpool. We explored various learning rates between\n1e − 4 and 1e − 5 and found the latter to produce\nthe lowest validation loss. We trained each model\n(one for each dataset) for up to 100K optimization\nsteps, we evaluated on Dval every 500 steps and\nsaved the checkpoint with the lowest validation\nloss. We used the resulting model in our ( BERT-\nTAPT) experiments. We plot the learning curves of\nmasked language modeling task (TAPT) for three\ndatasets and all considered learning rates in Figure\n5. We notice that a smaller learning rate facilitates\nthe training of the MLM.\nIn Table 2 we provide the validation and test\naccuracy of BERT and BERT-TAPT for all datasets.\nWe present the mean across runs with three random\nseeds. For fine-tuning the models, we used the\nproposed approach FT+ (§2).\nB.2 Performance of Acquisition Functions\nIn our BERT-TAPT-FT+ experiments so far, we\nshowed results withENTROPY . We have also exper-\nimented with various uncertainty-based acquisition\nfunctions. Specifically, four uncertainty-based ac-\nquisition functions are used in our work: LEAST\nCONFIDENCE , ENTROPY , BALD and BATCH -\nBALD . LEAST CONFIDENCE (Lewis and Gale,\n1994) sorts Dpool by the probability of not pre-\ndicting the most confident class, in descending\norder, ENTROPY (Shannon, 1948) selects sam-\nples that maximize the predictive entropy, and\nBALD (Houlsby et al., 2011), short for Bayesian\nActive Learning by Disagreement, chooses data\n0 20000 40000 60000 80000 100000\nSteps\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5Loss\nLearning rate\n5e-06\n1e-05\n5e-05\n0.0001\nTREC-6\n0 20000 40000 60000 80000 100000\nSteps\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5Loss\nLearning rate\n5e-06\n1e-05\n5e-05\n0.0001\nSST-2\n0 20000 40000 60000 80000 100000\nSteps\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00Loss\nLearning rate\n1e-05\n5e-05\n0.0001\nIMDB\nFigure 5: Learning curves of TAPT for various learning\nrates.\n5 10 15\nAcquired dataset size (%)\n90\n91\n92\n93\n94\n95\nAGNEWS\n114K training data (100%)\n5 10 15\nAcquired dataset size (%)\n86\n87\n88\n89\n90\n91\n92\nIMDB\n22.5K training data (100%)\nFigure 6: Comparison of acquisition functions using\nTAPT and FT+ in training BERT.\npoints that maximize the mutual information be-\ntween predictions and model’s posterior probabil-\nities. BATCH BALD (Kirsch et al., 2019) is a re-\ncently introduced extension of BALD that jointly\nscores points by estimating the mutual informa-\ntion between multiple data points and the model\nparameters. This iterative algorithm aims to find\nbatches of informative data points, in contrast to\nBALD that chooses points that are informative\nindividually. Note that LEAST CONFIDENCE , EN-\nTROPY and BALD have been used in AL for NLP\nby Siddhant and Lipton (2018). To the best of our\n834\nTREC -6 SST-2 IMDB DBPEDIA AGNEWS\nRANDOM 0/0 0/0 0/0 0/0 0/0\nALPS 0/57 0/478 0/206 0/134 0/634\nBADGE 0/63 0/23110 0/1059 0/192 -\nBERTKM 0/47 0/2297 0/324 0/137 0/3651\nENTROPY 81/0 989/0 557/0 264/0 2911/0\nLEAST CONFIDENCE 69/0 865/0 522/0 256/0 2607/0\nBALD 69/0 797/0 524/0 256/0 2589/0\nBATCH BALD 69/21 841/1141 450/104 256/482 2844/5611\nTable 3: Runtimes (in seconds) for all datasets. In each cell of the table we present a tuple i/s where i is the\ninference time and s the selection time. Inference time is the time for the model to perform a forward pass for all the\nunlabeled data in Dpool and selection time is the time that each acquisition function requires to rank all candidate\ndata points and select k for annotation (for a single iteration). Since we cannot report the runtimes for every model\nin the AL pipeline (at each iteration the size of Dpool changes), we provide the median.\nknowledge, BATCH BALD is evaluated for the first\ntime in the NLP domain.\nInstead of using the output softmax probabilities\nfor each class, we use a probabilistic formulation of\ndeep neural networks in order to acquire better cali-\nbrated scores. Monte Carlo (MC) dropout (Gal and\nGhahramani, 2016) is a simple yet effective method\nfor performing approximate variational inference,\nbased on dropout (Srivastava et al., 2014). Gal\nand Ghahramani (2016) prove that by simply per-\nforming dropout during the forward pass in making\npredictions, the output is equivalent to the predic-\ntion when the parameters are sampled from a varia-\ntional distribution of the true posterior. Therefore,\ndropout during inference results into obtaining pre-\ndictions from different parts of the network. Our\nBERT-based Mi model uses dropout layers during\ntraining for regularization. We apply MC dropout\nby simply activating them during test time and we\nperform multiple stochastic forward passes. For-\nmally, we do N passes of every x ∈ Dpool through\nMi(x; Wi) to acquire N different output proba-\nbility distributions for each x. MC dropout for\nAL has been previously used in the literature (Gal\net al., 2017; Shen et al., 2017; Siddhant and Lip-\nton, 2018; Lowell and Lipton, 2019; Ein-Dor et al.,\n2020; Shelmanov et al., 2021).\nOur findings show that all functions provide sim-\nilar performance, except for BALD that slightly\nunderperforms. This makes our approach agnos-\ntic to the selected uncertainty-based acquisition\nmethod. We also evaluate our proposed methods\nwith our baseline acquisition functions, i.e. RAN-\nDOM , ALPS , BERT KM and BADGE , since our\ntraining strategy is orthogonal to the acquisition\nstrategy. We compare all acquisition functions with\nBERT-TAPT-FT+ for AGNEWS and IMDB in Fig-\nure 6. We observe that in general uncertainty-based\nacquisition performs better compared to diversity,\nwhile all acquisition strategies have benefited from\nour training strategy (TAPT and FT+).\nB.3 Efficiency of Acquisition Functions\nIn this section we discuss the efficiency of the\neight acquisition functions considered in this work;\nRANDOM , ALPS , BADGE , BERT KM, ENTROPY ,\nLEAST CONFIDENCE , BALD and B ATCH BALD.\nIn Table 3 we provide the runtimes for all ac-\nquisition functions and datasets. Each AL experi-\nments consists of multiple iterations and (therefore\nmultiple models), each with a different training\ndataset Dlab and pool of unlabeled data Dpool. In\norder to evaluate how computationally heavy is\neach method, we provide the median of all the\nmodels in one AL experiment. We calculate the\nruntime of two types of functionalities. The first is\nthe inference time and stands for the forward pass\nof each x ∈ Dpool to acquire confidence scores for\nuncertainty sampling. RANDOM , ALPS , BADGE\nand BERTKM do not require this step so it is only\napplied of uncertainty-based acquisition where ac-\nquiring uncertainty estimates with MC dropout is\nneeded. The second functionality is selection time\nand measures how much time each acquisition func-\ntion requires to rank and select the k data points\nfrom Dpool to be labeled in the next step of the AL\npipeline. RANDOM , ENTROPY , LEAST CONFI -\nDENCE and BALD perform simple equations to\nrank the data points and therefore so do not require\nselection time. On the other hand, ALPS , BADGE ,\n835\nBERTKM and BATCH BALD perform iterative al-\ngorithms that increase selection time. From all ac-\nquisition functions ALPS and BERT KM are faster\nbecause they do not require the inference step of\nall the unlabeled data to the model. ENTROPY ,\nLEAST CONFIDENCE and BALD require the same\ntime for selecting data, which is equivalent for the\ntime needed to perform one forward pass of the en-\ntire Dpool. Finally BADGE and BATCH BALD are\nthe most computationally heavy approaches, since\nboth algorithms require multiple computations for\nthe selection time. RANDOM has a total runtime of\nzero seconds, as expected.\n836",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8331454396247864
    },
    {
      "name": "Task (project management)",
      "score": 0.7516517043113708
    },
    {
      "name": "Training set",
      "score": 0.5760666728019714
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5265184640884399
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5144136548042297
    },
    {
      "name": "Labeled data",
      "score": 0.5028917193412781
    },
    {
      "name": "Language model",
      "score": 0.49760106205940247
    },
    {
      "name": "Machine learning",
      "score": 0.4803277552127838
    },
    {
      "name": "Active learning (machine learning)",
      "score": 0.4444378614425659
    },
    {
      "name": "Natural language",
      "score": 0.4280416965484619
    },
    {
      "name": "Engineering",
      "score": 0.0824655294418335
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91136226",
      "name": "University of Sheffield",
      "country": "GB"
    }
  ]
}