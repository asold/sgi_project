{
  "title": "Hierarchical Multi-Scale Gaussian Transformer for Stock Movement Prediction",
  "url": "https://openalex.org/W3035414307",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2968813049",
      "name": "Qianggang Ding",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2225567835",
      "name": "Sifan Wu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2070768212",
      "name": "Hao Sun",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2159092766",
      "name": "Jiadong Guo",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2097198191",
      "name": "Jian-Guo",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4238530616",
    "https://openalex.org/W2734986640",
    "https://openalex.org/W2966276668",
    "https://openalex.org/W2962822108",
    "https://openalex.org/W2964413206",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2954731415",
    "https://openalex.org/W2594142095",
    "https://openalex.org/W2807880213",
    "https://openalex.org/W2985427845",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W2905016804",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W2080614264",
    "https://openalex.org/W2613328025",
    "https://openalex.org/W1782977420",
    "https://openalex.org/W2774513877",
    "https://openalex.org/W1985258161",
    "https://openalex.org/W2744043447",
    "https://openalex.org/W3006710048",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2798413829",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Predicting the price movement of finance securities like stocks is an important but challenging task, due to the uncertainty of financial markets. In this paper, we propose a novel approach based on the Transformer to tackle the stock movement prediction task. Furthermore, we present several enhancements for the proposed basic Transformer. Firstly, we propose a Multi-Scale Gaussian Prior to enhance the locality of Transformer. Secondly, we develop an Orthogonal Regularization to avoid learning redundant heads in the multi-head self-attention mechanism. Thirdly, we design a Trading Gap Splitter for Transformer to learn hierarchical features of high-frequency finance data. Compared with other popular recurrent neural networks such as LSTM, the proposed method has the advantage to mine extremely long-term dependencies from financial time series. Experimental results show our proposed models outperform several competitive methods in stock price prediction tasks for the NASDAQ exchange market and the China A-shares market.",
  "full_text": "Hierarchical Multi-Scale Gaussian Transformer for Stock Movement Prediction\nQianggang Ding1;2;\u0003, Sifan Wu2;\u0003, Hao Sun3 , Jiadong Guo1 and Jian Guo1;y\n1Peng Cheng Laboratory\n2Tsinghua University\n3The Chinese University of Hong Kong\nfdqg18, wusf18g@mails.tsinghua.edu.cn, sh018@ie.cuhk.edu.hk, fguojd, guojg@pcl.ac.cn\nAbstract\nPredicting the price movement of ï¬nance securi-\nties like stocks is an important but challenging task,\ndue to the uncertainty of ï¬nancial markets. In this\npaper, we propose a novel approach based on the\nTransformer to tackle the stock movement predic-\ntion task. Furthermore, we present several enhance-\nments for the proposed basic Transformer. Firstly,\nwe propose a Multi-Scale Gaussian Prior to en-\nhance the locality of Transformer. Secondly, we de-\nvelop an Orthogonal Regularization to avoid learn-\ning redundant heads in the multi-head self-attention\nmechanism. Thirdly, we design a Trading Gap\nSplitter for Transformer to learn hierarchical fea-\ntures of high-frequency ï¬nance data. Compared\nwith other popular recurrent neural networks such\nas LSTM, the proposed method has the advantage\nto mine extremely long-term dependencies from\nï¬nancial time series. Experimental results show\nour proposed models outperform several competi-\ntive methods in stock price prediction tasks for the\nNASDAQ exchange market and the China A-shares\nmarket.\n1 Introduction\nWith the development of stock markets all around the world,\nthe overall capitalization of stock markets worldwide has ex-\nceed 68 trillion U.S. dollar by 2018 1. Recent years, more\nand more quantitative researchers get involved in predicting\nthe future trends of stocks, and they help investors make prof-\nitable decisions using state-of-the-art trading strategies. How-\never, the uncertainty of stock prices make it an extremely\nchallenging problem in the ï¬eld of data science.\nPrediction of stock price movement belongs to the area\nof time series analysis which models rich contextual depen-\ndencies using statics or machine learning methods. Tradi-\ntional approaches for stock price prediction are mainly based\non fundamental factors technical indices or statistical time\n\u0003Equal contribution.\nyCorresponding author.\n1https://data.worldbank.org/indicator/CM.MKT.TRAD.CD?end=\n2018&start=2018&view=bar\nseries models, which captures explicit or implicit patterns\nfrom historical ï¬nancial data. However, the performance of\nthose methods are limited by two aspects. Firstly, they usu-\nally require expertise in ï¬nance. Secondly, these methods\nonly capture simple patterns and simple dependence struc-\ntures of ï¬nancial time series. With the rise of artiï¬cial in-\ntelligence technology, more and more researchers attempt to\nsolve this problem using machine learning algorithms, such\nas SVM [Cortes and Vapnik, 1995], Nearest Neighbors [Alt-\nman, 1992], Random Forest[Breiman, 2001]. Recently, since\ndeep neural networks empirically exhibited its powerful capa-\nbilities in solving highly uncertain and nonlinear problems,\nthe stock prediction research based on deep learning tech-\nnique has become more and more popular in recent years and\nshow signiï¬cant advantages over traditional approaches.\nThe stock prediction research based on deep learning tech-\nnique can roughly be grouped to two categories: (1) Fun-\ndamental analysis, and (2) Technical analysis. Fundamen-\ntal analysis constructs prediction signals using fundamen-\ntal information such as news text, ï¬nance report and an-\nalyst report. For example, [Schumaker and Chen, 2009;\nXu and Cohen, 2018; Chen et al., 2019] use natural language\nprocessing approaches to predict stock price movement by\nextracting latent features from market-related texts informa-\ntion,such as news, reports, and even rumors. On the other\nhand, technical analysis predicts ï¬nance market using his-\ntorical data of stocks. One natural choice is the RNN fam-\nily, such as RNN [Rumelhart et al., 1986 ], LSTM [Hochre-\niter and Schmidhuber, 1997 ], Conv-LSTM [Xingjian et al.,\n2015], and ALSTM [Qin et al., 2017]. However, the primary\ndrawback of these methods is that RNN family struggles to\ncapture extremely long-term dependencies [Li et al., 2019 ],\nsuch as the dependencies across several months on ï¬nancial\ntime series.\nRecently, a well-known sequence-to-sequence model\ncalled Transformer [Vaswani et al., 2017] has achieved great\nsuccess on natural machine translation tasks. Distinct from\nRNN-based models, Transformer employs a multi-head self-\nattention mechanism to learn the relationship among different\npositions globally, thereby the capacity of learning long-term\ndependencies is enhanced. Nevertheless, canonical Trans-\nformer is designed for natural language tasks, and therefore\nit has a number of limitations in tackling ï¬nance prediction:\n(1) Locality imperception: the global self-attention mecha-\nProceedings of the Twenty-Ninth International Joint Conference on Artiï¬cial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4640\nnism in canonical Transformer is insensitive to local context,\nwhose dependencies are much important in ï¬nancial time se-\nries. (2) Hierarchy poverty: the point-wise dot-product self-\nattention mechanism lacks the capability of utilizing hierar-\nchical structure of ï¬nancial time series (e.g. learning intra-\nday, intra-week and intra-month features in ï¬nancial time se-\nries independently). Intuitively, addressing those drawbacks\nwill improve the robustness of the model and lead to better\nperformance in the task of ï¬nancial time series prediction.\nIn this paper, we propose a new Transformer-based method\nfor stock movement prediction. The primary highlight of\nthe proposed model is the capability of capturing long-term,\nshort-term as well as hierarchical dependencies of ï¬nancial\ntime series. For these aims, we propose several enhancements\nfor the Transformer-based model: (1) Multi-Scale Gaussian\nPrior enhances the locality of Transformer. (2) Orthogonal\nRegularization avoids learning redundant heads in the multi-\nmead self-attention mechanism. (3) Trading Gap Splitter en-\nables Transformer to learn intra-day features and intra-week\nfeatures independently. Numerical results comparing with\nother competitive methods for time series show the advan-\ntages of the proposed method.\nIn summary, the main contributions of our paper include:\n\u000fWe propose a Transformer-based method for stock\nmovement prediction. To the best of our knowledge, this\nis the ï¬rst work using Transformer model to tackle ï¬nan-\ncial time series forecasting problems.\n\u000fWe propose several enhancements for Transformer\nmodel include Multi-Scale Gaussian Prior, Orthogonal\nRegularization, and Trading Gap Splitter.\n\u000fIn experiments, the proposed Transformer-based method\nsigniï¬cantly outperform several state-of-the-art base-\nlines, such as CNN, LSTM and ALSTM, on two real-\nworld exchange market.\n2 Related Work\nFundamental Analysis Machine learning for fundamental\nanalysis developed with the explosion of ï¬nance alternative\ndata, such as news, location, e-commerce. [Schumaker and\nChen, 2009] proposes a predictive machine learning approach\nfor ï¬nancial news articles analysis using several different tex-\ntual representations. [Weng et al., 2017 ] outlines a novel\nmethodology to predict the future movements in the value of\nsecurities after tapping data from disparate sources. [Xu and\nCohen, 2018] uses a stochastic recurrent model (SRM) with\nan extra discriminator and an attention mechanism to address\nthe adaptability of stock markets. [Chen et al., 2019 ] pro-\nposes to learn event extraction and stock prediction jointly.\nTechnical Analysis On the other hand, technical analy-\nsis methods extract price-volume information from historical\ntrading data and use machine learning algorithms for predic-\ntion. For instances, [Lin et al., 2013 ] proposes an SVM-\nbased approach for stock market trend prediction. Mean-\nwhile, LSTM neural network [Hochreiter and Schmidhuber,\n1997] has been employed to model stock price movement.\n[Nelson et al., 2017 ] proposes an LSTM model to predict\nstock movement based on the technical analysis indicators.\n[Zhang et al., 2017 ] proposes an LSTM model on histori-\ncal data to discover multi-frequency trading. [Wang et al.,\n2019] proposes a ConvLSTM-based Seq2Seq framework for\nstock movement prediction. [Qin et al., 2017 ] proposes an\nAttentive-LSTM model with an attention mechanism to pre-\ndict stock price movement and [Feng et al., 2019] further in-\ntroduces an data augmentation approach with the idea of ad-\nversarial training. However, [Li et al., 2019 ] points out that\nLSTM can only distinguish 50 positions nearby with an effec-\ntive context size of about 200. That means that LSTM-based\nmodels suffer from the difï¬culty in capturing extremely long-\nterm dependencies in time series. To tackle this issue, we pro-\npose a Transformer-based method to better mine the intrinsic\nlong-term and complex structures in ï¬nancial time series.\n3 Problem Formulation\nSince the exact price of a stock is extremely hard to be pre-\ndicted accurately, we follow the setup of [Walczak, 2001 ]\nand predict the stock price movement instead. Usually the\nstock movement prediction is treated as a binary classiï¬ca-\ntion problem â€” e.g., discretizing the stock movement into\ntwo classes (Rise or Fall). Formally, given the stock features\nX = [ xT\u0000\u0001t+1;xT\u0000\u0001t+2;:::; xT] 2 R\u0001t\u0002F (also repre-\nsented as X = [x 1;x2;:::; xN] 2RN\u0002F in the rest of the\npaper for simplicity) in the latest \u0001t time-steps, the predic-\ntion model f\u0012(X) with parameters \u0012can output the predicted\nmovement label y = I(pT+k > pT), where T denotes the\ntarget trading time, F denotes the dimension of stock fea-\ntures and pt denotes the close price at time-stept. Brieï¬‚y, the\nproposed model utilizes the historical data of a stock sin the\nlag [T \u0000\u0001t+ 1;T] (where \u0001tis a ï¬xed lag size) to predict\nthe movement class y (0 for Fall, 1 for Rise) of the future k\ntime-steps.\n4 Proposed Method\nIn this section, we ï¬rst describe the basic Transformer model\nwe designed. Then we introduce the proposed enhancements\nof Transformer for ï¬nancial time series.\n4.1 Basic Transformer for Stock Movement\nPrediction\nIn our work, we instantiate f\u0012(\u0001) with Transformer-based\nmodel. To adapt the stock movement prediction task which\ntakes time series as inputs, we design a variant of Transformer\nwith encoder-only structure which consists of L blocks of\nmulti-head self-attention layers and position-wise feed for-\nward layers (see Figure 1). Given the input time series\nX = [x1;x2;:::; xN] 2RN\u0002F, we ï¬rst add the position en-\ncoding and adopt an linear layer withtanhactivation function\nas follows:\n\u0016X = \u001btanhW(I)\nh [PositionEncoding(X)]: (1)\nThen multi-head self-attention layers take \u0016X as input, and are\ncomputed by\nQh = W(Q)\nh \u0016X; Kh = W(K)\nh \u0016X; Vh = W(V)\nh \u0016X; (2)\nwhere h = 1;:::;H and W(Q)\nh ;W(K)\nh and W(V)\nh are learn-\nable weight matrices for Query, Key and Value, respectively\nProceedings of the Twenty-Ninth International Joint Conference on Artiï¬cial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4641\nð’™1 ð’™2 ð’™3 â€¦ ð’™ð‘\nà´¥ð’™1 à´¥ð’™2 à´¥ð’™3 â€¦ à´¥ð’™ð‘\nð’1 ð’2 ð’3 â€¦ ð’ð‘\nð’›1 ð’›2 ð’›3 â€¦ ð’›ð‘\nÃ—L\nHead-1\nHead-2\nHead-3\nâ€¦\nð’Ž\nâ€¦\nà·œð‘¦\nTemporal Attention\nAggregation\nMulti-Head \nSelf-Attention\nLayers\nFeed-Forward\nLayer\nInput\nPrediction\nð›¼1 ð›¼2 ð›¼3 ð›¼ð‘\nFigure 1: The proposed Basic Transformer overview.\n(refers to [Vaswani et al., 2017 ] for more details). Then the\nattention score matrix ah 2RN\u0002N of the hth head is com-\nputed by\nah = softmax(QhKT\nhpdk\n\u0001M); (3)\nwhere M is a position-wise mask matrix to ï¬lter out temporal\nattention, so as to avoid future information leakage. After-\nwards, the output of the hth head is a weighted sum deï¬ned\nas follows:\n[Oh]i =\nNX\nj=1\n(ah)i;j \u0001[Vh]j: (4)\nThe ï¬nal output of multi-head attention layers is the con-\ncatenation of all heads by O = [O 1;O2;:::; OH]. After-\nward, the position-wise feed forward layer takes O as input\nand transforms it to Z by two fully-connected layers and a\nReLU activation layer. Upon the output zi of the last self-\nattention layer, a temporal attention layer[Qin et al., 2017] is\ndeployed to aggregate the latent features from each position\nas m = PN\ni=1 \u000bizi. Then the scalar prediction score ^y is\ncomputed by a fully-connected layer and a sigmoid transfor-\nmation:\n^y= sigmoid(Wfcm): (5)\nOur ultimate goal is to maximize the log-likelihood between\n^yand yvia the following loss function:\nLCE = (1 \u0000y)log(1 \u0000^y) + ylog(^y) (6)\n4.2 Enhancing Locality with Multi-Scale Gaussian\nPrior\nRecently, Transformer exhibits its powerful capability of ex-\ntracting global patterns in natural language processing ï¬elds.\nHowever, the self-attention mechanism in Transformer con-\nsiders the global dependencies with very weak position in-\nformation. Note that, the position information serves as the\ntemporal variant patterns in time series, which is much im-\nportant. To address it, we incorporate Multi-Scale Gaussian\nPrior into the canonical multi-head self-attention mechanism\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n-40 -35 -30 -25 -20 -15 -10 -5 0 5 10\nHead-1: D=5\nHead-2: D=10\nHead-3: D=20\nHead-4: D=40\nj - i\n[ð!\n(#)]%,'\nFigure 2: Visualization of [B(G)\nh ]i;j in Eq. 8 with window-size set\nD = f5;10;20;40g.\nwith the intuition that the relevance of data in two positions is\ndirectly proportional to the temporal distance between them.\nTo pay more attention to the closer time-steps, we add bi-\nases of Gaussian prior to the attention score matrices based on\nthe assumption that such scores would obey Gaussian distri-\nbutions. Note that this operation is equivalent to multiplying\nthe original attention distribution with a Gaussian distribution\nmask (See [Guo et al., 2019 ] for the proof). In details, we\ntransform Eq.3 to the following form by adding Gaussian bi-\nases:\nah = softmax[(QhKT\nh\npdk\n+ B(G)\nh ) \u0001M]; (7)\nwhere B(G)\nh 2RN\u0002N is a matrix computed by\n[B(G)\nh ]i;j =\n(\nexp(\u0000(j\u0000i)2\n2\u001b2\nh\n) j \u0014i;\n0 j >i:\n(8)\nNote that we allow\u001bhin B(G)\nh are different for different heads\nin the multi-head self-attention layer.\nBesides, we also give an empirical approximation for \u001bh.\nSuppose we want to pay more attention to the Dh closest\ntime-steps, the variance can be empirically set as \u001bh = Dh.\nBy this way, we allow different Dh in different attention\nheads in order to provide Multi-Scale Gaussian Prior.\nIn ï¬nance, the temporal features from last 5-day, 10-day,\n20-day or 40-day are usually considered in trading strategies.\nThat means, for a 4-head self-attention layer, we can empir-\nically assign the window-size set D = f5;10;20;40gto \u001bh\nwith h = 1;:::; 4, respectively as is shown in Figure 2. In\nconclusion, the proposed Multi-Scale Gaussian Prior enables\nTransformer to learn multi-scale localities from ï¬nancial time\nseries.\n4.3 Orthogonal Regularization for Multi-Head\nSelf-Attention Mechanism\nWith the proposed Multi-Scale Gaussian Prior, we let differ-\nent heads learn different temporal patterns in the multi-head\nattention layer. However, some previous research [Tao et al.,\n2018][Li et al., 2018][Lee et al., 2019] claims that canonical\nmulti-head self-attention mechanism tend to learn redundant\nheads. To enhance the diversity between each head, we in-\nduce an orthogonal regularization with regard to the weight\ntensor W(V)\nh in Eq.2. Speciï¬cally, we ï¬rst calculate the\nProceedings of the Twenty-Ninth International Joint Conference on Artiï¬cial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4642\n!ð’™#!ð’™$â€¦!ð’™$%!ð’™$&â€¦!ð’™'$26 Ã—15-min\nIntra-Day Feature\n26 Ã—15-min!ð’™#('â€¦!ð’™#)(26 Ã—15-minâ€¦Intra-Week Feature\n5 Ã—day\n!ð’™#)#â€¦!ð’™#'%26 Ã—15-min\nIntra-Day Featureâ€¦Intra-Week Feature\n5 Ã—day\nIntra-Day FeatureIntra-Day Feature !ð’™â€¦â€¦!ð’™â€¦26 Ã—15-min\nIntra-Day Featureâ€¦Intra-Week Feature\n5 Ã—day\nGlobal Feature\nâ€¦\nð’#ð’$â€¦ð’$%\n!ð’™#!ð’™$â€¦!ð’™$%\nð’$&â€¦ð’'$\n!ð’™$&â€¦!ð’™'$\nð’#('â€¦ð’#)(\n!ð’™#('â€¦!ð’™#)(â€¦ ð’#)#ð’#)$â€¦ð’#'%\n!ð’™#)#!ð’™#)$â€¦!ð’™#'%\nð’$)'â€¦ð’$%(\n!ð’™$)'â€¦!ð’™$%(â€¦ â€¦Block-1\nð’#ð’$ ð’#)(\n!ð’™#!ð’™$ !ð’™#)(\nð’#)#ð’#)$ ð’$%(\n!ð’™#)#!ð’™#)$ !ð’™$%(â€¦Block-2\nð’#ð’$ â€¦ ð’#)(\n!ð’™#!ð’™$ â€¦ !ð’™#)(\nð’#)#ð’#)$ â€¦ ð’,-#\n!ð’™#)#!ð’™#)$ â€¦ !ð’™,-#\nð’,\n!ð’™,Block-3\nâ€¦\nâ€¦â€¦\nâ€¦â€¦130Ã—130\n130Ã—130\n130Ã—130\nâ€¦\nâ€¦\nâ€¦\nâ€¦â€¦\n26Ã—2626Ã—2626Ã—2626Ã—26\n26Ã—26\nðŒ(0)for Block-2\nðŒ(0)for Block-1\n(a) Hierarchical features of NASDAQ 15-minute data\n(b) Hierarchical self-attention mechanism(c) Hierarchical masks\nð’$%\n!ð’™$%\nð’$&â€¦ð’'$\n!ð’™$&â€¦!ð’™'$\nâ€¦\nâ€¦\nð’#'%\n!ð’™#'%\nð’$)'\n!ð’™$)'\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nð’#('\n!ð’™#('\nâ€¦\nâ€¦\nâ€¦\nâ€¦â€¦ â€¦â€¦ â€¦ â€¦ â€¦â€¦\nâ€¦\nâ€¦ â€¦â€¦ â€¦ â€¦\nâ€¦ â€¦\nFigure 3: Trading Gap Splitter overview.\ntensor W(V) = [W(V)\n1 ;W(V)\n2 ;:::; W(V)\nH ] by concatenating\nW(V)\nh of all heads. Note that the size ofW(V) is H\u0002F\u0002dv\nwhere dv denotes the last dimension of Vh. Then we ï¬‚atten\nthe tensor W(V) to a matrix A with size of H\u0002(F\u0003dv), and\nfurther normalize it as eA = A=jjAjj2. Finally, the penalty\nloss is computed by\nLp = jjeAeAT \u0000IjjF; (9)\nwhere jj\u000ejj F denotes the Frobenius norm of a matrix and\nI stands for an identity matrix. We brieï¬‚y add the penalty\nloss to the original loss with a trade-off hyper-parameter \ras\nfollows:\nL= LCE + \rLp: (10)\nFor simplicity in expression, here we omit the number of\nmulti-head self-attention layers in the model. In our exper-\niment, we sum up the penalty losses from each multi-head\nself-attention layer as the ï¬nal penalty loss as follows:\nLp = L(1)\np + L(2)\np + :::+ L(L)\np : (11)\n4.4 Trading Gap Splitter\nAs is known that the input of the model is a continuous time\nseries. However, due to the trading gaps, the input time se-\nries is essentially NOT continuous. Takes the 15-minute data\nfrom NASDAQ stock market2 as an example, of which one\ntrading day contains 26 15-minute time-steps and one trading\nweek contains 5 trading days. This means there are inter-day\nand inter-week trading gaps. However, when the basic Trans-\nformer model is applied to this data, the self-attention layer\n2the NASDAQ Stock Exchange Market is open 5 days per week\nfor 6.5 hours per day\ninside treats all time-steps equally and omit the implicit inter-\nday and inter-week trading gaps. To solve this problem, we\ndesign a new hierarchical self-attention mechanism for the\nTransformer model to learn the hierarchical features of stock\ndata (see Figure 3 (a)).\nTakes a3-block Transformer model as an example, we aim\nto learn the hierarchical features of stock data by the order\nâ€œintra-day!intra-week!globalâ€. In order to do so, we set\ntwo extra position-wise masks to the ï¬rst and second self-\nattention blocks respectively in order to limit the attention\nscopes. Formally, we modify Eq.7 to the following form:\nah = softmax[(QhKT\nhpdk\n+ B(G)\nh ) \u0001M(H) \u0001M]; (12)\nwhere M(H) is an N \u0002N matrix ï¬lled with \u0000inf whose\ndiagonal is composed of continuous sub-matrices ï¬lled with\n0. The M(H) for the ï¬rst and second self-attention blocks are\nshown in Figure 3 (c). Speciï¬cally, the size of sub-matrices in\nM(H) for the ï¬rst block is 26\u000226 since one trading day con-\ntains 26 15-minute time-steps, and the size of sub-matrices\nfor the second block changes to 130 \u0002130 (26 \u00035) since one\ntrading week contains 5 trading days. By this way, the ï¬rst\nand second self-attention blocks will learn the intra-day and\nintra-week features of stock data, respectively. Moreover, for\nthe last self-attention block, we keep the original attention\nmechanism without M(H) to learn global features of stock\ndata. As a result, the Transformer model with the proposed\nhierarchical attention mechanism avoids suffering from the\ntrading gaps. Note that, all attention heads in the same multi-\nhead self-attention layer share the same M(H).\nProceedings of the Twenty-Ninth International Joint Conference on Artiï¬cial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4643\nProperty Data\nDaily 15-min\nMarket NASDAQ China A-shares\nStart Date 2010/07/01 2015/12/01\nEnd Date 2019/07/01 2019/12/01\nTime Frequency 1 day 15 minutes\nTotal Stocks 3243 500\nTotal Records 9749098 7928000\nRising Threshold \frise 0.55% -0.5%\nFalling Threshold \ffall -0.1% 0.105%\nTable 1: Data description.\n5 Experiments\nTo evaluate the proposed methods, we use two stock data: one\nfrom NASDAQ market and the other from China A-shares\nmarket. The details of the two data are listed in Table 1. In\nthe following subsections, we will introduce the data collec-\ntion process and show our empirical results from numerical\nexperiments. We also conduct an incremental analysis to ex-\nplore the effectiveness of each proposed enhancements for\nTransformer.\n5.1 Data Collection\nWe collect the daily quote data of all 3243 stocks from NAS-\nDAQ stock market from July 1 st, 2010 to July 1 st, 2019 and\nthe 15-min quote data of 500 CSI-500 component stocks from\nChina A-shares market from December 1 st, 2015 to Decem-\nber 1st, 2019. We move a lag window with size of N time-\nsteps along these time series to construct candidate exam-\nples. For the NASDAQ data, we construct 5 datasets with\nwindow sizes N = 5; 10;20;40 (denoting 5-, 10-, 20-, 40-\nday, respectively), and the lag strides are all ï¬xed to 1. For\nthe China A-shares data, we also construct 5 datasets with\nwindow sizes N = 5 \u000316;10 \u000316;20 \u000316;40 \u000316 (de-\nnoting 5-, 10-, 20-, 40-day since one trading day contains\n16 15-minute in China A-shares market), and the lag strides\nare all ï¬xed to 16 (i.e. 1 day). The features used in all\ndatasets consist of open;high;low;close and volume, which\nare all adjusted and normalized following [Feng et al., 2019;\nXu and Cohen, 2018]. We initially label both datasets by the\nstrategy mentioned in Section 3.1 (i.e y = I(pT+k > pT)),\nwhere k is set to 1 and 16 (both denote 1 day) for the NAS-\nDAQ data and the China A-shares data, respectively. Further-\nmore, we set two threshold parameter\frise;\ffall to the labels\nin each dataset in order to balance the number of positive and\nnegative samples to roughly 1 : 1 as follows:\ny=\n8\n><\n>:\n1 pT+k\u0000pT\npT\n>\frise;\n\u00001 pT+k\u0000pT\npT\n<\ffall;\nabandon otherwise:\n(13)\nTo avoid the data leakage problem, we strictly follow the\nsequential order to split training/validation/test sets. For in-\nstances, we split the NASDAQ data and the China A-shares\ndata into training/validation/test sets by 8-year/1-year/1-year\nand 3-year/6-month/6-month, respectively.\nMethod\nAccuracy (%)/MCC (\u000210\u00002)\nwith window size of K-day\nK = 5 K = 10 K = 20 K = 40\nNASDAQ Daily Data\nCNN 52.33/3.16 52.02/2.68 51.84/2.28 52.60/2.52\nLSTM 53.86/7.73 53.89/7.72 53.59/7.15 53.81/7.48\nALSTM 54.06/8.35 53.94/7.92 54.05/8.11 54.19/8.56\nB-TFy 54.78/8.48 54.84/8.89 54.90/9.13 56.01/9.45\nMG-TFy 55.10/8.98 56.18/9.74 56.77/10.39 57.30/11.46\nChina A-shares 15-min Data\nCNN 53.53/2.62 52.25/1.80 52.03/1.81 51.61/1.77\nLSTM 56.59/6.42 56.70/6.19 56.18/3.74 54.93/2.98\nALSTM 57.03/8.23 57.42/9.16 55.69/6.65 55.68/6.65\nB-TFy 57.14/9.68 57.42/9.52 57.32/9.14 57.55/10.41\nHMG-TFy 57.36/10.52 57.79/9.98 57.90/10.33 58.70/14.87\nTable 2: The results of comparison experiments. All values are av-\nerage results from 5 repeated experiments. The best results are in\nbold and ydenotes our methods.\n54\n55\n56\n57\n58\n59\n5 10 20 40\nLSTM ALSTM HMG-TF\n0\n5\n10\n15\n20\n5 10 20 40\nLSTM ALSTM HMG-TF\nK K\nMCC(Ã—10âˆ’2)\nAcc(%)\nFigure 4: The accuracy and MCC trends along with the window size\nK-day on the China A-shares data.\n5.2 Evaluation Metrics\nFollowing previous research [Feng et al., 2019; Xu and Co-\nhen, 2018], we evaluate the prediction performance with two\nmetrics: Accuracy (Acc) and Matthews Correlation Coefï¬-\ncient (MCC), which is deï¬ned as below:\nMCC = tp \u0002tn \u0000fp \u0002fn\np\n(tp + fp)(tp + fn)(tn + fp)(tn + fn)\n(14)\nwhere tp;tn;fp;fn denote the number of samples classiï¬ed\nas true positive, true negative, false positive and false nega-\ntive, respectively.\n5.3 Numerical Experiments\nApproaches in comparison. We compare our approaches\nB-TF, MG-TF and HMG-TF with the baselines CNN, LSTM\nand ALSTM:\n\u000fCNN [Selvin et al., 2017 ]: Here we use 1D-CNN with\nthe kernel size of 1 \u00023.\n\u000fLSTM [Nelson et al., 2017]: A variant of recurrent neu-\nral network with feedback connections.\n\u000fAttentive LSTM (ALSTM) [Feng et al., 2019]: A vari-\nant of LSTM model with a temporal attentive aggrega-\ntion layer.\n\u000fBasic Transformer (B-TF): The proposed Basic Trans-\nformer model introduced in Section 4.1.\nProceedings of the Twenty-Ninth International Joint Conference on Artiï¬cial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4644\nMethod Settings Metrics\nMG OR TS Acc. (%) MCC (\u000210 \u00002)\nB-TF 7 7 7 57.55 10.41\nMG-TF\u0003 3 7 7 58.03 12.11\nMG-TF 3 3 7 58.25 12.75\nHMG-TF 3 3 3 58.70 14.87\nTable 3: Experimental results of incremental analysis. MG, OR and\nTS denote Multi-Scale Gaussian Prior, Orthogonal Regularization\nand Trading Gap Splitter, respectively. * denotes wo/ OR.\n\u000fMulti-Scale Gaussian Transformer (MG-TF): The B-TF\nmodel with enhancements of Multi-Gaussian Prior and\nOrthogonal Regularization introduced in Section 4.2\nand Section 4.3, respectively.\n\u000fHierarchical Multi-Scale Gaussian Transformer (HMG-\nTF): The MG-TF model with the enhancement of Trad-\ning Gap Splitter introduced in Section 4.4.\nSettings. We implement B-TF, MG-TF and HMG-TF with\nPyTorch framework on Nvidia Tesla V100 GPU. We use\nAdam optimizer with an initial learning-rate of 1e \u00004. The\nsize of mini-batch is set to 256. The trade-off hyper-\nparameter \ris set to 0:05. All TF-based models have 3 multi-\nhead self-attention blocks each with 4 heads. We train the\nmodel in an end-to-end manner from raw quote data without\nany data augmentation. Since Trading Gap Splitter is less\nappropriate for low-frequency data like daily quote data, we\nonly perform MG-TF on our NASDAQ dataset.\nResults. The performance of comparison experiments on\nboth data are shown in Table 2, from which we have the fol-\nlowing observations:\n\u000fThe proposed approaches B-TF, MG-TF and HMG-\nTF show signiï¬cant gains on both metrics Acc and\nMCC compared with baselines in all cases. It exhibits\nthat Transformer-based approaches have signiï¬cant per-\nformance advantages over RNN- and CNN-based ap-\nproaches.\n\u000fTransformer-based approaches have better capabilities\nof learning long-term dependencies. Especially on\nChina A-shares data, in which the window size of 40-\nday comtains 640 (40*16) time-steps, and it is hard for\nRNN-based approaches to learn the dependencies across\nso many time-steps (see Figure 4). While the advantages\nof the self-attention mechanism enables Transformer-\nbased approaches achieve consistently better perfor-\nmance as the window size becomes larger.\n\u000fThe modiï¬ed Transformer model MG-TF and HMG-\nTF have better performance than the basic Transformer\nmodel. More detailed analysis will be shown in the next\nsection.\n5.4 Incremental Analysis\nTo explore the effectiveness of the proposed components\nMulti-Scale Guassian Prior, Orthogonal Regularization and\nTrading Gap Splitter, we further conduct an incremental anal-\nysis on different settings of the Transformer-based models.\n(a) Daily attention scores\n(b) Weekly attention scores (c) Global attention scores\nFigure 5: Hierarchical attention scores. (a)(b)(c) represents the at-\ntention score matrices learned by Block-1, Block-2 and Block-3, re-\nspectively.\nAs is shown in Table 3, these components all contribute to\nthe performance of Transformer-based method. Moreover,\nwe can further observe that the performance improvement\nmainly beneï¬ts from Multi-Scale Guassian Prior and Trad-\ning Gap Splitter, while the gain fromOrthogonal Regulariza-\ntion is relatively insigniï¬cant.\nBesides, we illustrate the effectiveness of Trading Gap\nSplitter by visualizing the attention score matrix learned by\nHMG-TF model with N = 160 (is equivalent to K = 10-day\nor 2-week). As is shown in Figure 5, the attention scope be-\ncomes larger gradually (16!80!160) from (a) to (c). That\nmeans, with the proposed hierarchical self-attentiom mech-\nanism, the time-steps only attend to those belonging to the\nsame day in the ï¬rst self-attention block, the time-steps only\nattend to those belonging to the same week in the second\nblock, and the attention scopes of time-steps are no limit in\nthe last block.\n6 Discussion & Future Work\nIn this paper, we propose to apply Transformer model for\nstock movement prediction in which the attention mechanism\ncan help to capture extremely long-term dependencies of ï¬-\nnance time series. Furthermore, equipped with the proposed\nenhancements Multi-Scale Gaussian Prior, Orthogonal Reg-\nularization and Trading Gap Splitter, our Transformer-based\nmodel achieves signiï¬cant gains over several state-of-the-art\nbaselines on two real-world market dataset. In the future,\nexcept for the model itself, the following aspects can be in-\nvestigated for further improvements: (1) cross-sectional fea-\ntures of ï¬nancial data can be engaged to improve the model,\n(2) regularization methods can be explored to avoid suffering\nfrom overï¬tting on ï¬nancial data, (3) data augmentation such\nas adversarial and stochastic perturbations can be adopted to\nimprove the robustness of the model. Also, it is signiï¬cant to\ninvestigate the theoretical guarantee for our method.\nProceedings of the Twenty-Ninth International Joint Conference on Artiï¬cial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4645\nReferences\n[Altman, 1992] Naomi S Altman. An introduction to ker-\nnel and nearest-neighbor nonparametric regression. The\nAmerican Statistician, 46(3):175â€“185, 1992.\n[Breiman, 2001] Leo Breiman. Random forests. Machine\nlearning, 45(1):5â€“32, 2001.\n[Chen et al., 2019] Deli Chen, Yanyan Zou, Keiko Hari-\nmoto, Ruihan Bao, Xuancheng Ren, and Xu Sun. Incor-\nporating ï¬ne-grained events in stock movement prediction.\narXiv preprint arXiv:1910.05078, 2019.\n[Cortes and Vapnik, 1995] Corinna Cortes and Vladimir\nVapnik. Support-vector networks. Machine learning,\n20(3):273â€“297, 1995.\n[Feng et al., 2019] Fuli Feng, Huimin Chen, Xiangnan He,\nJi Ding, Maosong Sun, and Tat-Seng Chua. Enhancing\nstock movement prediction with adversarial training. In\nProceedings of the 28th International Joint Conference\non Artiï¬cial Intelligence, pages 5843â€“5849. AAAI Press,\n2019.\n[Guo et al., 2019] Maosheng Guo, Yu Zhang, and Ting Liu.\nGaussian transformer: a lightweight approach for natural\nlanguage inference. In The Thirty-Third AAAI Conference\non Artiï¬cial Intelligence, 2019.\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and\nJÂ¨urgen Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735â€“1780, 1997.\n[Lee et al., 2019] Mingu Lee, Jinkyu Lee, Hye Jin Jang,\nByeonggeun Kim, Wonil Chang, and Kyuwoong Hwang.\nOrthogonality constrained multi-head attention for key-\nword spotting. arXiv preprint arXiv:1910.04500, 2019.\n[Li et al., 2018] Jian Li, Zhaopeng Tu, Baosong Yang,\nMichael R Lyu, and Tong Zhang. Multi-head atten-\ntion with disagreement regularization. arXiv preprint\narXiv:1810.10183, 2018.\n[Li et al., 2019] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou\nZhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. En-\nhancing the locality and breaking the memory bottleneck\nof transformer on time series forecasting. In Advances\nin Neural Information Processing Systems, pages 5244â€“\n5254, 2019.\n[Lin et al., 2013] Yuling Lin, Haixiang Guo, and Jinglu Hu.\nAn svm-based approach for stock market trend prediction.\nIn The 2013 international joint conference on neural net-\nworks (IJCNN), pages 1â€“7. IEEE, 2013.\n[Nelson et al., 2017] David MQ Nelson, Adriano CM\nPereira, and Renato A de Oliveira. Stock marketâ€™s price\nmovement prediction with lstm neural networks. In\n2017 International Joint Conference on Neural Networks\n(IJCNN), pages 1419â€“1426. IEEE, 2017.\n[Qin et al., 2017] Yao Qin, Dongjin Song, Haifeng Chen,\nWei Cheng, Guofei Jiang, and Garrison Cottrell. A dual-\nstage attention-based recurrent neural network for time se-\nries prediction. arXiv preprint arXiv:1704.02971, 2017.\n[Rumelhart et al., 1986] David E Rumelhart, Geoffrey E\nHinton, and Ronald J Williams. Learning representations\nby back-propagating errors. nature, 323(6088):533â€“536,\n1986.\n[Schumaker and Chen, 2009] Robert P Schumaker and\nHsinchun Chen. Textual analysis of stock market pre-\ndiction using breaking ï¬nancial news: The azï¬n text\nsystem. ACM Transactions on Information Systems\n(TOIS), 27(2):12, 2009.\n[Selvin et al., 2017] Sreelekshmy Selvin, R Vinayakumar,\nEA Gopalakrishnan, Vijay Krishna Menon, and KP So-\nman. Stock price prediction using lstm, rnn and cnn-\nsliding window model. In 2017 International Conference\non Advances in Computing, Communications and Infor-\nmatics (ICACCI), pages 1643â€“1647. IEEE, 2017.\n[Tao et al., 2018] Chongyang Tao, Shen Gao, Mingyue\nShang, Wei Wu, Dongyan Zhao, and Rui Yan. Get the\npoint of my utterance! learning towards effective re-\nsponses with multi-head attention mechanism. In IJCAI,\npages 4418â€“4424, 2018.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nÅukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, pages 5998â€“6008, 2017.\n[Walczak, 2001] Steven Walczak. An empirical analysis of\ndata requirements for ï¬nancial forecasting with neural\nnetworks. Journal of management information systems,\n17(4):203â€“222, 2001.\n[Wang et al., 2019] Jia Wang, Tong Sun, Benyuan Liu,\nYu Cao, and Hongwei Zhu. Clvsa: A convolutional lstm\nbased variational sequence-to-sequence model with atten-\ntion for predicting trends of ï¬nancial markets. In Proceed-\nings of the 28th International Joint Conference on Artiï¬-\ncial Intelligence, pages 3705â€“3711. AAAI Press, 2019.\n[Weng et al., 2017] Bin Weng, Mohamed A Ahmed, and\nFadel M Megahed. Stock market one-day ahead move-\nment prediction using disparate data sources. Expert Sys-\ntems with Applications, 79:153â€“163, 2017.\n[Xingjian et al., 2015] SHI Xingjian, Zhourong Chen, Hao\nWang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun\nWoo. Convolutional lstm network: A machine learning ap-\nproach for precipitation nowcasting. InAdvances in neural\ninformation processing systems, pages 802â€“810, 2015.\n[Xu and Cohen, 2018] Yumo Xu and Shay B Cohen. Stock\nmovement prediction from tweets and historical prices. In\nProceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 1970â€“1979, 2018.\n[Zhang et al., 2017] Liheng Zhang, Charu Aggarwal, and\nGuo-Jun Qi. Stock price prediction via discovering\nmulti-frequency trading patterns. In Proceedings of the\n23rd ACM SIGKDD international conference on knowl-\nedge discovery and data mining, pages 2141â€“2149. ACM,\n2017.\nProceedings of the Twenty-Ninth International Joint Conference on Artiï¬cial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4646",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.667262852191925
    },
    {
      "name": "Transformer",
      "score": 0.5322403907775879
    },
    {
      "name": "Locality",
      "score": 0.48671796917915344
    },
    {
      "name": "Stock exchange",
      "score": 0.4798252582550049
    },
    {
      "name": "Stock market",
      "score": 0.4786861538887024
    },
    {
      "name": "Financial market",
      "score": 0.4728287160396576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.438186913728714
    },
    {
      "name": "Trading strategy",
      "score": 0.41678041219711304
    },
    {
      "name": "Machine learning",
      "score": 0.3580127954483032
    },
    {
      "name": "Econometrics",
      "score": 0.35575151443481445
    },
    {
      "name": "Finance",
      "score": 0.2542984187602997
    },
    {
      "name": "Economics",
      "score": 0.11773765087127686
    },
    {
      "name": "Engineering",
      "score": 0.11564844846725464
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Horse",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 187
}