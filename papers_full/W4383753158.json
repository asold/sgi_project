{
  "title": "Vison transformer adapter-based hyperbolic embeddings for multi-lesion segmentation in diabetic retinopathy",
  "url": "https://openalex.org/W4383753158",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096678193",
      "name": "Zijian Wang",
      "affiliations": [
        "Hefei University of Technology",
        "Anhui University of Traditional Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2610005246",
      "name": "Haimei Lu",
      "affiliations": [
        "Anhui Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2438841403",
      "name": "Haixin Yan",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2145794500",
      "name": "Hongxing Kan",
      "affiliations": [
        "Anhui University of Traditional Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2100468826",
      "name": "Li Jin",
      "affiliations": [
        "Anhui University of Traditional Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2096678193",
      "name": "Zijian Wang",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2610005246",
      "name": "Haimei Lu",
      "affiliations": [
        "Anhui Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2438841403",
      "name": "Haixin Yan",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2145794500",
      "name": "Hongxing Kan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100468826",
      "name": "Li Jin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4282559613",
    "https://openalex.org/W4229059038",
    "https://openalex.org/W3154135352",
    "https://openalex.org/W3214497762",
    "https://openalex.org/W4224229807",
    "https://openalex.org/W4285093832",
    "https://openalex.org/W4210655294",
    "https://openalex.org/W4281834132",
    "https://openalex.org/W3114205382",
    "https://openalex.org/W4212818064",
    "https://openalex.org/W3190658604",
    "https://openalex.org/W3197217317",
    "https://openalex.org/W4287181769",
    "https://openalex.org/W4372048987",
    "https://openalex.org/W3011401450",
    "https://openalex.org/W3191514888",
    "https://openalex.org/W4362603432",
    "https://openalex.org/W4205175531",
    "https://openalex.org/W3119307757",
    "https://openalex.org/W4205213185",
    "https://openalex.org/W2980702601",
    "https://openalex.org/W6601412845",
    "https://openalex.org/W3164410012",
    "https://openalex.org/W3177377177",
    "https://openalex.org/W3197418040",
    "https://openalex.org/W3204614423",
    "https://openalex.org/W4220739817",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W4281491335",
    "https://openalex.org/W4315647008",
    "https://openalex.org/W1991266515",
    "https://openalex.org/W6600459194",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2828862258",
    "https://openalex.org/W2948685905",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W4387808640",
    "https://openalex.org/W4226044115"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports\nVison transformer adapter‑based \nhyperbolic embeddings \nfor multi‑lesion segmentation \nin diabetic retinopathy\nZijian Wang 1,3,4, Haimei Lu 2, Haixin Yan 3, Hongxing Kan 1,4 & Li Jin 1*\nDiabetic Retinopathy (DR) is a major cause of blindness worldwide. Early detection and treatment \nare crucial to prevent vision loss, making accurate and timely diagnosis critical. Deep learning \ntechnology has shown promise in the automated diagnosis of DR, and in particular, multi‑lesion \nsegmentation tasks. In this paper, we propose a novel Transformer‑based model for DR segmentation \nthat incorporates hyperbolic embeddings and a spatial prior module. The proposed model is primarily \nbuilt on a traditional Vision Transformer encoder and further enhanced by incorporating a spatial prior \nmodule for image convolution and feature continuity, followed by feature interaction processing \nusing the spatial feature injector and extractor. Hyperbolic embeddings are used to classify feature \nmatrices from the model at the pixel level. We evaluated the proposed model’s performance on \nthe publicly available datasets and compared it with other widely used DR segmentation models. \nThe results show that our model outperforms these widely used DR segmentation models. The \nincorporation of hyperbolic embeddings and a spatial prior module into the Vision Transformer‑based \nmodel significantly improves the accuracy of DR segmentation. The hyperbolic embeddings enable us \nto better capture the underlying geometric structure of the feature matrices, which is important for \naccurate segmentation. The spatial prior module improves the continuity of the features and helps to \nbetter distinguish between lesions and normal tissues. Overall, our proposed model has potential for \nclinical use in automated DR diagnosis, improving accuracy and speed of diagnosis. Our study shows \nthat the integration of hyperbolic embeddings and a spatial prior module with a Vision Transformer‑\nbased model improves the performance of DR segmentation models. Future research can explore the \napplication of our model to other medical imaging tasks, as well as further optimization and validation \nin real‑world clinical settings.\nDiabetic retinopathy is an ocular condition resulting from injury to the retina caused by hyperglycemia in \nindividuals with  diabetes1. The retina is a delicate membrane located at the posterior of the eye that converts \nlight into electrical impulses transmitted to the  brain2. Diabetic retinopathy is classified into two categories: \nNon-Proliferative DR (NPDR) and Proliferative DR (PDR) 3. NPDR is characterized by damage to the small \nblood vessels in the retina, potentially resulting in blood or fluid  leakage4. PDR is defined by the growth of \nabnormal blood vessels on the surface of the retina, which can lead to intraocular bleeding, potentially causing \nretinal detachment and vision  loss5. It is important for individuals with diabetes to undergo regular eye exams \nfor early detection and appropriate treatment of  DR6. The increasing prevalence of DR globally highlights the \nurgent need for automated diagnostic tools to enhance the early detection and prevention of  DR7. Various deep \nneural networks have been employed for studying the diagnosis of DR, specifically in terms of feature extraction, \nimage classification, and lesion detection and  segmentation8–12. Deep neural networks have been shown to be \neffective in medical images analysis, particularly in detecting abnormalities and  lesions13–16. By leveraging these \ntechniques, researchers aim to improve the accuracy and efficiency of DR  diagnosis17.\nIn this paper, the utilization of the most recent and widely acknowledged deep learning technique was \nemployed to enhance the precision of DR lesion segmentation, specifically the Transformer-based model. \nOPEN\n1School of Medicine and Information Engineering, Anhui University of Chinese Medicine, Hefei 230012, \nChina. 2School of Basic Medical Sciences, Anhui Medical University, Hefei 230032, China. 3Hefei University of \nTechnology, Hefei 230009, China.  4 These authors contributed equally: Zijian Wang and Hongxing Kan.  *email: \njinli@ahtcm.edu.cn\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nRecently, the efficacy of the Transformer-based model has demonstrated a trend of surpassing that of convolu-\ntional neural networks and provides state-of-the-art performance in natural image classification, detection, and \n segmentation18. However, it still needs further research in the application of deep learning techniques to medi-\ncal image  processing19. Researchers have pointed out that hyperbolic space may exhibit superior performance \ncompared to the traditional neural network based on Euclidean  space20. Therefore, hyperbolic embeddings were \nintroduced in this paper to conduct the pixel-level classification, and the experimental results demonstrate that \nhyperbolic space can enhance the performance of the model. The following are the contributions of this work:\n• We propose a Transformer-based model named “VTA (Vision Transformer Adapter) + HBE (Hyperbolic \nEmbeddings)” suitable for DR multi-lesion segmentation tasks. Our approach adapts the original Vision \nTransformer model by incorporating a spatial prior module, which leverages convolutional neural networks \nto extract image features. The architecture also incorporates the spatial feature injector and extractor to \nenhance feature interaction.\n• We employ hyperbolic embeddings to classify the feature representation at the pixel level. This solution effec-\ntively addresses the challenges encountered in current implementations of hyperbolic polynomial logistic \nregression, resulting in a more efficient parallel pixel classification method for image segmentation tasks.\n• The proposed model demonstrates superior performance in the segmentation of hard exudates and microa-\nneurysms on the IDRiD dataset, and exceptional performance in the segmentation of microaneurysms and \nsoft exudates on the DDR dataset.\nRelated work\nDeep neural networks in DR lesion segmentation. Despite Deep Neural Networks (DNNs)21,22 being \nprevalent in the segmentation of DR lesions, they still face two major challenges: significant morphological vari-\nations of DR lesions across different degrees and difficulty distinguishing DR lesions from similar  structures23. \nThe challenge in utilizing deep learning models for detecting small-size DR lesions lies in their ability to dif-\nferentiate between normal and abnormal features. The limited size of these lesions, sometimes consisting of only \na few pixels, can significantly impact the model’s accuracy and hinder its ability to make reliable predictions.\nResearchers have put forth numerous proposed enhancements and improvements to address the aforemen-\ntioned challenges. Zhang et al.24 present a feature fusion algorithm that leverages a multilayer attention mecha-\nnism for improved feature layer and channel fusion. The proposed method allows for more accurate selection of \nfeature layers containing small target features, leading to improved preliminary detection of small targets. The \nfindings of the study highlight the efficiency of the algorithm, resulting in a noteworthy increase in the average \naccuracy and sensitivity of microaneurysms detection. Wang et al.25 introduced a semi-supervised collaborative \nlearning model to enhance the precision of DR grading and lesion segmentation. The model leverages attention \nmechanism technology and utilizes low-level guidance to identify lesion features and high-level guidance to \ncreate lesion attention images. These attention images serve as pseudo masks to facilitate the training process of \nthe segmentation model. Xie et al.22 propose a novel and versatile framework to improve the precision of existing \ndeep convolutional neural networks for medical image segmentation (including DR multi-lesion segmentation).\nHyperbolic deep learning. DNNs are characterized by their multi-layer hybrid structure and multiple \nresidual connections, allowing for the potential to model complex functions theoretically and leading to their \ndominance in research areas such as image classification and segmentation. Neural architectures based on \nEuclidean space are optimized primarily for raster data, which limits their ability to effectively handle optimiza-\ntion problems involving structured data in non-Euclidean spaces. Its relying on local proximity can result in the \nincorrect representation of geometric structures and undermine the effectiveness of these architectures for such \ntasks. The Hyperbolic deep learning has gained widespread attention for its ability to effectively represent tree-\nlike structures,  taxonomies26,27,  text28,29, and graph  data30,31. Researchers have put forth a number of hyperbolic \nalternatives for network layers that span from intermediate to classification  layers20,32,33.\nA recent study  by34 has expanded the utilization of hyperbolic space for semantic image segmentation. The \nresearch team has reformulated the hyperbolic multinomial logistic regression approach to ensure tractability. \nThe finding of using hyperbolic space in semantic image segmentation can benefit with higher proficiency, such \nas enhanced zero-shot generalization and improved performance in low-dimensional embeddings. Ganea et al.32 \nbridge the gap between hyperbolic and Euclidean geometry in neural networks and deep learning, opening new \npossibilities in Geometric Deep Learning (GDL). They achieve this by generalizing basic operations, multino -\nmial logistic regression, feed-forward, and gated recurrent neural networks to the Poincaré model of hyperbolic \ngeometry using gyrovector spaces and generalized Möbius transformations. They introduce a unified framework \nthat smoothly parametrizes basic operations and objects in constant negative curvature spaces and demonstrate \nhow Euclidean and hyperbolic spaces can be transformed into each other. The effectiveness of hyperbolic neural \nnetwork layers is demonstrated through experiments on textual entailment and noisy-prefix recognition tasks.\nTransformer in medical images. Vaswani et al.35 introduced Transformers architecture, a transforma-\ntive design that features encoders and decoders as its fundamental components. The encoders employ atten-\ntion mechanisms to consolidate information from input sequences into high-dimensional representations, while \ndecoders are employed to extract these high-dimensional representations to generate target sequences. Since \ntheir inception, Transformer-based architectures have established a strong track record of delivering state-of-\nthe-art performance on a variety of natural language processing and computer vision tasks. The success of Trans-\nformers is attributed to its highly parallelizable, which allows for efficient training on large datasets and fast \ninference during deployment and captures context  effectively36. The impact of Transformers on the field of AI \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\ncontinues to be significant, with ongoing research aimed at further improving their performance and exploring \ntheir applications in new domains.\nRecently, the inception of the Vision Transformer model has sparked a trend in the medical image processing \ncommunity towards the adoption of Transformer-based architectures or hybrid convolutional neural networks \nto enhance model  performance19. Shen et al.37 proposed a novel convolution-and-transformer network, which is \nbuilt on the encoder-decoder architecture and demonstrates efficacy in the segmentation of kidney cysts. Wang \net al.38 were pioneers in applying Transformer-based architecture for the efficient 3D segmentation of brain \ntumors. Their network leverages the encoder to extract volumetric spatial features, which are then transmitted \nto the decoder for upsampling and the generation of a full-resolution segmentation map. Yun et al.39 proposed \na novel Spectral Transformer model for the segmentation of hyperspectral pathology images. The model lever -\nages a sequence-to-sequence prediction procedure to facilitate the learning of contextual features from spectral \nbands, thus enabling more accurate and efficient segmentation. Despite their remarkable performance in various \napplications, there remains potential for further exploration and development of Transformers in the field of \nmedical imaging.\nHyperbolic space\nPoincaré ball model. The Poincaré ball model, named after the mathematician Henri Poincaré, is a math-\nematical representation of hyperbolic geometry. We utilize Euclidean concepts such as distance and angle to \nreason about hyperbolic spaces, making it an effective way to study and comprehend the unique properties of \nhyperbolic geometry, which differ from those of Euclidean geometry. In addition to its utility in visualization, \nthe Poincaré ball model allows for the application of standard Euclidean algorithms to perform geometric cal-\nculations in the hyperbolic plane, making it computationally practical. One of the primary applications of the \nPoincaré ball model is in computer graphics, particularly for representing the conformal structure of surfaces \nand analyzing complex datasets.\nThe Poincaré ball model allows for the depiction of the shape and curvature of a surface without regard to its \nsize or position, making it a useful tool for analyzing and manipulating complex datasets in this field. The grow-\ning attention the Poincaré ball model has received in machine learning and data mining fields due to its ability to \nfacilitate analysis and manipulation of large and complex  datasets33. Eli et al.40 focus on the Poincaré ball model \nand use tangent space formalization to express classification problems, and the proposed algorithm provably \nconverge and are highly scalable as they have complexities comparable to those of their Euclidean counterparts. \nThey demonstrate superior performance accuracy on complex synthetic datasets and real-world datasets. Guo \net al.41 proposed a Poincaré-based heterogeneous graph neural network for sequential recommendation, which \nmodels both sequential pattern information and hierarchical information.\nThe Poincaré ball model is a mathematical model that is used to represent hyperbolic geometry. It is a Rie -\nmannian manifold, denoted as\n(\nBn\nc , gBc\n)\n , where Bn\nc =\n{\nu ∈ Rn : √c � u �< 1\n}\n is the open ball of radius 1√c in \n-dimensional Euclidean space and g Bc is the Riemannian metric defined as:\nwhere |·|  is the l2 norm and �·, ·� is the standard inner product. The curvature of the Poincaré ball model is deter-\nmined by the value of c. When c = 0 , the Poincaré ball model reduces to Euclidean space, i.e. Bn\nc = Rn . In this \ncase, the Riemannian metric becomes the standard inner product, and the Poincaré ball model represents the \nfamiliar geometry of flat spaces. For c > 0 , the Poincaré ball model represents hyperbolic geometry, in which \nthe curvature and radius is determined by the value of c. The Poincaré ball includes two fundamental operator \noperations: Möbius addition and scalar multiplication. These operations correspond to vector addition and \nscalar multiplication in Euclidean spaces, respectively. The Möbius addition is a non-commutative and non-\nassociative operation that extends the concept of vector addition to the Poincaré ball, while scalar multiplication \nextends the concept of scalar multiplication from Euclidean spaces to the Poincaré ball. The Möbius addition \n⊕c of u, v ∈ Bn is defined as:\nThe operation ⊕c hold the following equalities: u⊕c0 = 0⊕cu = u,(−u⊕cu = u⊕c(−u) = 0) . Furthermore, the \noperation ⊕c recovers the Euclidean addition when c approaches zero, i.e. c → 0 ⇒ u⊕cv → u +v . The Möbius \nscalar multiplication ⊗c of vector u ∈ B\\{0} by a scalar v ∈ R is defined according to:\nand v⊗c0 = 0 . The Möbius scalar multiplication operation ⊗c converges to the standard Euclidean scalar multipli-\ncation as the scalar parameter c approaches zero. Mathematically, this can be represented as c → 0 ⇒ v⊗cu = vu . \nThe distance function of u, v ∈ Bn in the Poincare model is given by:\n(1)gBc\nu (·,·) =\n(\nσ c\nu\n)2 �·,·� = 2\n1 − c � u�2 �·,·�,\n(2)u⊕cv =\n(\n1 + 2c�u,v�+ c � v�2 )\nu +\n(\n1 − c � u�2 )\nv\n1 + 2c�u,v�+ c2 � u�2 � v�2 .\n(3)v⊗cu = 1√ctanh (v · tanh -1(√c � u �)) u\n� u �,\n(4)dc(u,v) = 2√ctanh −1 (√c�−u⊕cv�)\n.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nSegmentation networks are typically designed to operate in Euclidean space. However, in order to execute seg-\nmentation within the Poincaré ball, it is required to establish a mapping from the Euclidean tangent space to the \nhyperbolic space. One way to achieve this is through the use of the exponential map, which projects a Euclidean \nvector onto the Poincaré ball with a fixed anchor point. For p ∈ Dn\nc , the exponential map expp : TpDn\nc → Dn\nc is \ngiven by:\nThe exponential map is a mathematical function that can be used to map a tangent vector at a point in a manifold \nto a point on the manifold itself. This projection allows the segmentation network to operate effectively in the \nPoincaré ball while maintaining the geometric properties of the hyperbolic space. The above maps have more \nappealing forms, when p = 0 , namely for v ∈ T 0 Dn\nc \\0,y ∈ Dn\nc \\0:\nHyperbolic embeddings. The problem of image segmentation involves the task of assigning a label to \neach pixel in an input image. The input RGB image is represented as X ∈ Rw×h×3 , where w and h are the image’s \nwidth and height, respectively. A function f(X ) : Rw ×h×3 → Rw ×h×n is used to transform each pixel in the \ninput image to an n-dimensional representation matrix Y ∈ Rw×h×n . A popular technique among contemporary \nmethods for image classification is to process all pixels simultaneously by passing them through a linear layer \nand applying the softmax function to generate a C-dimensional probability distribution for each pixel across all \nC classes, i.e. f(Y ) : Rw ×h×n → Rw ×h×C . The optimization of this approach is typically accomplished by using \ncross-entropy as the objective function. The pipeline design facilitates parallel processing of all pixels, thereby \nmaximizing efficiency and optimizing the model through minimization of cross-entropy loss. The purpose of \nthis study is to examine the application of hyperbolic space in the context of pixel-level classification for image \nsegmentation. The gyroplane represents a hyperplane within the Poincaré ball, based on the geometric interpre-\ntation of hyperbolic multinomial logistic  regression42. Specifically, for p ∈ Dn\nc, a ∈ TpDn\nc\\0 , the Poincaré hyper-\nplane is defined as:\nwhere zij = exp 0(f(X )ij) denote the result of applying the exponential map to the neural network output at pixel \nlocation (i, j), p ∈ Dn\nc is the reference point and a ∈ TpDn\nc is the normal vector of the gyroplane. The set ˜H a,p can \nalso defined as the union of all images of geodesics in the hyperbolic space D n\nc that are orthogonal to vector a and \npass through the point p [27]. The hyperbolic distance between the point z ij and the gyroplane ˜H c\ny of class y, can be \ncomputed as Eq. (8). The Fig. 1 visualized the hyperbolic gyroplane ˜H c\ny and distance to output z ij on the manifold.\n(5)expp (v) = p⊕c\n(\ntanh\n(√c\n/afii9838c\np � v �\n2\n) v√c � v �\n)\n.\n(6)exp0 (v) = tanh\n(√c � v �\n) v√c � v �.\n(7)˜H c\na,p =\n{\nzij∈ Dn\nc :\n⟨\np⊕czij,a\n⟩\n= 0\n}\n,\nFigure 1.  Visualization of the hyperbolic gyroplane ˜H c\ny and distance to output z ij on the manifold.\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nApproximate treatment in hyperbolic space. The task of image segmentation necessitates simultane-\nous per-pixel classification. However, current implementations of hyperbolic multinomial logistic regression are \ncomputationally infeasible. To reduce the memory footprint of explicitly calculating the Möbius addition, we \nleverage an alternative computation for the margin likelihood that eliminates the need for explicit calculation of \nthe Möbius addition. We overwrite the inner product in the numerator and squared norm in the denominator \nof Eq. (8). The overwrite of the inner product \n⟨\npy ⊕czij, ay\n⟩\n is defined as:\nwhere A = 1+2c �py ,zij�+c�zij�2\n1+2c�py ,zij�+c2 �py �2 �zij�2  , B = 1−c�py �2\n1+2c�py ,zij�+c2 �py �2 �zij�2 .\nThe squared norm � p y ⊕c zij �2 of the Möbius addition can be performed efficiently utilizing the following \nmethod:\nConsequently, the logit of per pixel is computed as:\nThe optimization of logit can be achieved through the implementation of the cross-entropy loss function and the \ngradient descent algorithm. Our method employs a approximation of the inner product and squared norm in \nthe calculation of class logits, enabling the possibility of hyperbolic pixel-level classification. This novel approach \neffectively addresses the intractability previously encountered in current implementations of hyperbolic multi-\nnomial logistic regression, and enables more efficient per-pixel classification in parallel for image segmentation \ntasks.\n(8)dc\n�\nzij, ˜H c\ny\n�\n= 1√csinh −1\n\n 2√c\n�\npy⊕czij,ay\n�\n�\n1 − c\n��py⊕czij\n��2 ���ay\n��\n\n.\n(9)�py⊕czij, ay�=� Ap y + Bz ij, ay�=A�p y, a�+ B �zij, a�,\n(10)�py ⊕c zij�2 =\nn∑\nm =1\n(Ap m\ny + Bzm\nij)2\n(11)=\nn∑\nm =1\n(Ap m\ny )2 + Ap m\ny Bzm\nij + (Bzm\nij)2\n(12)= A 2\nn∑\nm =1\n(pm\ny )2 + 2AB\nn∑\nm =1\npm\ny zm\nij + B2\nn∑\nm =1\n(zm\nij)2\n(13)= A 2 �py�2 + 2AB �py, zij�+ B 2 �zij�2 .\n(14)p\n(\nˆy = y | zij\n)\n∝ exp\n(\nζy\n(\nzij\n))\n.\nSpatial Prior \nModule\nEncoder Block 1\nLinear Projection\n0\n2\n1\n3\n14\n15\nEncoder Block N\nSpatial Feature Injector 1\nSpatial Feature Extractor 2\nSpatial Feature Extractor 1\nSpatial Feature Extractor N\nSpatial Feature Injector N\nEncoder Block 2\nSpatial Feature Injector 2\nHyperbolic \nEmbeddings\nHE\nEX\nSE\nMA\nFigure 2.  The overall architecture of the “VTA + HBE” model. The model uses a ViT network for feature \nextraction. A Spatial Prior Module enhances local continuity. Continuous features are input to the feature \ninjector and extractor for interaction. Pixel-level classification employs hyperbolic embeddings, not Euclidean \nspace.\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nModel architecture\nThe overall design of the proposed model is depicted in Fig. 2. The main components of the model, including the \nVision Transformer for feature extraction and the integration of a spatial prior module to enhance image quality \nand capture local image continuity. Feature interaction is facilitated through the utilization of a spatial feature \ninjector and extractor. Hyperbolic embeddings are employed for the final pixel-level classification of features.\nVision transformer encoder. The ViT-Adapter design philosophy is a way of leveraging scalable NLP \nTransformer architectures for vision  tasks43. The benefit of this straightforward design is that it allows us to \nleverage scalable NLP Transformer architectures and their efficient implementations almost immediately. The \nViT-Adapter holds the potential to reconcile the discrepancy between  ViT44 model and vision-specific model for \nsegmentation task while maintaining the versatility of ViT, and could reap the benefits of advanced multi-modal \npre-training techniques.\nThe Transformer receives 1D sequences of token embeddings as input. In order to adapt it for the processing \nof 2D medical images, we reshape the image X ∈ Rw×h×3 into a flattened 2D patches sequence xp ∈ RN ×P2×3 , \nwhere (P , P) is the resolution of each patch, N = w×h\nP2  is the number of patches. The Transformer encoder \nsubsequently comprises sequential layers of Multi-Headed Self-Attention (MSA) and Multi-Layer Perceptron \n(MLP) blocks. The MSA block is responsible for capturing the relationships between different elements in the \ninput sequence. It does this by computing multiple attention heads in parallel, where each head learns a differ -\nent aspect of the input sequence. The MLP block is responsible for applying a non-linear transformation to the \noutput of the MSA block.\nLayer Normalization (LN) is applied after each sub-layer in the Transformer encoder, including the MSA \nand MLP blocks. It normalizes the output of each sub-layer by subtracting its mean and dividing by its standard \ndeviation, which improves the stability of the model during training and allows it to better generalize to new \ndata. The residual function is added to the output of the sub-layer, which helps to mitigate the vanishing gradi-\nent problem and allows the model to learn deeper representations. Layer Normalization is implemented prior \nto each block, and residual connections are employed following each block. The ViT Encoder comprises a total \nof L layers, which can be mathematical represented as:\nFor image segmentation, we divide the encoder layers into uniform encoder block, and use the feature tokens \nfrom the each encoder block to feed into the vision transformer adapter module. The input image for the Trans-\nformer encoder is first processed through patch embeddings, where it is divided into non-overlapping 16 × 16 \npatches.\nVision transformer adapter. Spatial prior module. Recent  studies45,46 have demonstrated that utilizing \nconvolutions with overlapping sliding windows can enhance the ability of transformers to effectively capture lo-\ncal continuity in input images. We present a novel addition to the Transformer encoder layer: the Spatial Prior \nModule (SPM), a convolution-based structure, which downsamples a w × h input image to various scales. The \nSPM module design architecture is shown in Fig. 3. The objective of this module is to concurrently model the lo-\ncal spatial contexts of images alongside the patch embeddings layer while preserving the integrity of the original \narchitecture of the Vision Transformer.\n(15)\ny0 =\n[\nx class; x1\np E; x2\np E;··· ;x N\np E\n]\n+ Epos, E ∈ RP 2·3·D ,Epos ∈ R(N +1)·D\ny′\ni =MSA\n(\nLN\n(\nyl−1\n))\n+ yl−1,\nyi =MLP\n(\nLN\n(\ny′\nl\n))\n+ y′\nl, i= 1 ··· L\n7hh7 conv, 64, /2\n3hh3 max pool\n3hh3 conv, 128, /2\n3hh3 conv, 512, /2\n3hh3 conv, 256, /2\n1hh1 conv, D\n1hh1 conv, D\n1hh1 conv, D\nFlatten & Concat\n…\nDR Image\nFeature Pyramid\nFeature tokens\n1f\n2f\n3f\nFigure 3.  The architecture of the convolution-based Spatial Prior Modules.\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nWe first feed the input image into the spatial prior module, and obtain a feature pyramid f1 ,f2 ,f3 , which \ncontains D-dimensional feature maps (1024 dimension) with resolutions of 1/8, 1/16, and 1/32. Then we flatten \nand concatenate these feature maps into feature tokens Fsp\n1 ∈ R\n(\nHW\n82 +HW\n162 +HW\n322\n)\n×D\n for feature interaction.\nSpatial feature injector. The columnar structure of ViT results in single-scale and low-resolution feature maps, \nwhich negatively impact its performance in segmentation tasks relative to pyramid-structured transformers. To \naddress this challenge, we propose the implementation of feature interaction modules, specifically the Spatial \nFeature Injector (SFI), to enhance communication between the adapter and ViT, thereby improving perfor -\nmance. We partition the transformer encoders of ViT into N equal blocks, the feature bi are from encoder block \ni of ViT model. To incorporate the spatial feature Fsp\ni  into bi , we employ multi-head cross-attention, which can \nbe formulated as:\nwhere γi is a learnable parameter to modulate the balance the output of attention layer and the spatial feature bi , \nwhich is initialized with a value of zero. The Pseudo-codes of the process of spatial feature injector are provided \nin Algorithm 1.\nAlgorithm 1: the process of spatial featurei njector\n1 Set the numbero f transformer encoder blocks to N;\n2 Extract feature maps Fsp\ni fromV iT encoder block i;\n3 Initialize feature maps bi ands patial features Fsp\ni to zero matrices of appropriated imensions;\n4 for i:1t o N do\n5 Generate random featurem aps with size 16x16 andd epth 64;\n6 Generate random spatialf eatures with size 16x16 and depth1 6;\n7 end\n8 Initialize gamma parameters γi to zero;\n9 Deﬁne multi-head cross-attention function;\n10 Compute attention using Eq. (13)f or each transformer encoderb lock i;\n11 ApplyL No peration to the result;\nSpatial feature extractor. Upon incorporating the spatial feature via the SFI module, the output feature bi+1 \nis obtained. Multi-scale feature extractor can enhance the spatial feature and extract the multi-scale feature. \nWe employ a cross-attention layer to facilitate communication between the output feature bi+1 and the spatial \nfeature Fsp\ni  . Subsequently, a Convolutional Feed-Forward Network (CFFN) is introduced following the atten-\ntion layer. Algorithm 2 shows the pseudo-codes of the process of spatial feature extractor, and the process are \nformulated as:\nAlgorithm 2: the process of spatial featuree xtractor\n1 Set the input feature maps Fsp\ni and output feature maps Fsp\ni+1;\n2 Calculate attention between the input feature maps Fsp\ni and theo utputf eature maps bi+1;\n3 Add the Fsp\ni and ˆFsp\ni to obtainane ws patial feature map;\n4 ApplyL Nt o the new spatial feature map Ftemp\ni ;\n5 Add CFFNt o Ftemp\ni to obtain the output feature maps Fsp\ni+1;\nExperiments\nDataset. The IDRiD and DDR datasets are two important datasets in the field of retinal image segmentation, \naimed at advancing the automatic diagnosis of diabetic retinopathy. Both datasets are important open resources \nfor the development and evaluation of machine learning algorithms for diabetic retinopathy lesion detection \nand segementation.\nIDRiD  Dataset47 is a comprehensive resource for the segmentation and grading of retinal images, as part \nof the Retinal Image Challenge 2018. The dataset comprises 81 fundus images for the segmentation task, each \nwith a resolution of 4288 × 2848 pixels. The images are accompanied by four pixel-level annotations for lesions \nof type EX (hard exudates), HE (haemorrhages), MA (microaneurysms), and SE (soft exudates), as applicable. \nSpecifically, there are 81 EX annotations, 80 HE annotations, 81 MA annotations, and 40 SE annotations. The \nIDRiD dataset includes a training set with 54 images and a testing set with 27 images.\n(16)bi = bi + γi Attention(LN( Fi), LN(Fsp\ni )),\n(17)\nˆFsp\ni =Fsp\ni + Attention(LN(F sp\ni ), LN(b i+1))\nFsp\ni+1 =ˆFsp\ni + CFFN(LN( ˆFsp\ni )).\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nDDR  Dataset48 offered by Ocular Disease Intelligent Recognition (ODIR-2019) provides support for lesion \nsegmentation and detection and consists of 13,673 fundus images sourced from 147 healthcare institutions in \n23 provinces in China. The segmentation task utilizes 757 fundus images, equipped with pixel-level annotations \nfor EX, HE, MA, and SE lesions, totaling 486 EX, 601 HE, 570 MA, and 239 SE annotations. The dataset has been \npre-partitioned into training (383 images), validation (149 images), and testing (225 images) subsets.\nModel configurations. Our model is designed to convert each pixel into an n-dimensional representation. \nIn our experiments, the patch size of the ViT model is fixed at 16, D-dimension is set to 1024. The interaction \ntimes N is set to 4, which involves dividing the ViT encoder layers into 4 equal blocks for feature interaction. \nThe width of ViT is set to 768, with a feed-forward network (FFN) size of 3072 and 12 heads. To reduce com-\nputational overhead, the ratio of the CFFN is set to 1/4, with a hidden size of 96 and the adapter has 12 heads.\nImplementation details. Our framework is implemented utilizing the PyTorch platform and executed on \nthree NVIDIA GeForce RTX 3090Ti GPUs, each equipped with 24GB of memory. The newly integrated adapter \nmodules have been randomly initialized and do not employ any pre-trained weights. The initial learning rate is \nset to 0.001. The model is trained for 160 epochs on the ADE20K dataset with a batch size of 2 and then fine-\ntuned on the IDRiD and DDR datasets. The optimization of Euclidean parameters is performed using Stochastic \nGradient Descent (SGD) with a momentum of 0.9 and a polynomial learning rate decay of power 0.9. Hyperbolic \nparameters are optimized utilizing Riemannian Stochastic Gradient Descent (RSGD).\nEvaluation metrics. To evaluate the efficacy of the proposed model, the performance metrics used include \nthe Area-Under-the-Curve (AUC) of the Precision-Recall (PR) curve and the Receiver Operating Characteristic \n(ROC) curve. These metrics have been widely adopted in previous research and competitions on fundus image \nsegmentation. The assessment of the accuracy of true data in predictions is primarily conducted through the \nAUC_PR curve, while the performance of positively predicted data is evaluated using the AUC_ROC curve. \nBoth the AUC_PR and AUC_ROC curves characterize the overall performance of different neural models.\nExperimental results. We trained DeepLab v3+49,  UNet50, UNet++51 and Seg-B/1652 models on the IDRiD \nand DDR datasets using the model training parameters initialized according to the methods described in the \ncorresponding original papers. Our VTA+HBE model was compared with these popular models. Table  1 dis-\nplays the comparison results of the models on the IDRiD test set, showing that the VTA+HBE model achieved \nthe highest performance in MA and SE segmentation predictions, and Seg-B/16’s performance in EX and HE \nsegmentation predictions was comparable. Table 2 displays the comparison results of the models on the DDR \ntest set, showing that the VTA+HBE model achieved the best performance in MA and EX segmentation predic-\ntions, but the EX and HE segmentation results were slightly inferior to those of the Seg-B/16 model.\nAblation studies. We employ the vision transformer with transposed convolution as our baseline, in which \nthe feature sequence generated by each coding block is reshaped, then subjected to upsampling via transposed \nconvolution. The resulting upsampled feature matrix is then reshaped and utilized as the input feature sequence \nTable 1.  Performance comparison with widely-used segmentation methods on the IDRiD dataset. Bold font is \nused to indicate the best result compared to other models.\nMethod\nEX HE MA SE\nAUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC\nDeepLab v3+49 0.8293 0.9942 0.6389 0.9360 0.4024 0.9819 0.5968 0.9374\nUNet50 0.8038 0.9903 0.6034 0.9431 0.4029 0.9820 0.5734 0.9102\nUNet++51 0.8351 0.9940 0.6489 0.9499 0.4043 0.9820 0.5908 0.9326\nSeg-B/1652 0.8601 0.9932 0.6504 0.9503 0.4068 0.9849 0.5849 0.9378\nVTA+HBE (Ours) 0.8640 0.9943 0.6494 0.9487 0.4063 0.9839 0.5978 0.9463\nTable 2.  Performance comparison with widely-used segmentation methods on the DDR dataset. Bold font is \nused to indicate the best result compared to other models.\nMethod\nEX HE MA SE\nAUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC\nDeepLab v3+49 0.5532 0.9730 0.3782 0.9312 0.0334 0.9212 0.2190 0.8671\nUNet50 0.5512 0.9619 0.3727 0.9392 0.0365 0.9378 0.2434 0.8756\nUNet++51 0.5579 0.9723 0.3880 0.9398 0.0443 0.9245 0.2453 0.8767\nSeg-B/1652 0.5435 0.9747 0.3930 0.9367 0.0923 0.9276 0.2687 0.8578\nVTA+HBE (Ours) 0.5607 0.9745 0.3778 0.9403 0.1066 0.9409 0.2694 0.8857\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nfor the next coding block. This baseline preserves the dimensions and size of each feature sequence in our model. \nWe enhanced this baseline with our proposed VTA technique and named the resulting model as “baseline+VTA ” . \nFurther, we added (HBE) to classify the feature matrix at the pixel level, resulting in the “VTA + HBE” model.\nTo demonstrate the efficacy of VTA and HBE, we randomly selected and visualized predictions from the \nIDRiD and DDR test sets. Figure 4 presents a comparison of the segmentation results with the original images \nand ground truths. Segmentation plots of the baseline, baseline + VTA, and VTA + HBE models were used to \nshow the improvement of each component of our network. The green and yellow boxes highlight the areas where \nFigure 4.  Visualization of the efficacy of the VTA and the HBE. We randomly selected and visualized \npredictions from the IDRiD and DDR test sets. The green and yellow boxes highlight the areas where DR \nsegmentation was improved by the VTA and the HBE, respectively. As can be seen from the figure, the HBE \nenhances finer segmentation predictions, while the VTA prevents certain misclassification predictions.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nDR segmentation was improved by the VTA and the HBE, respectively. As evident from the figure, HBE enhances \nfiner segmentation predictions, while VTA prevents certain misclassification predictions. Tables 3 and 4 compare \nthe performance of the baseline, baseline + VTA, and VTA + HBE models on the IDRiD and DDR test sets. The \nresults demonstrate that both VTA and HBE have a positive influence on model accuracy, with VTA showing \nsignificant improvement in all segmentation accuracy metrics. To further analyze the effectiveness of HE, the \ncurvature value were fine-tuned and trained on the IDRiD and DDR dataset, and the model performance is \npresented in Tables 5 and 6. As shown in the table, the performance of the model improves to some extent with \nan increase in the curvature value, but performance decreases rapidly when the curvature value is greater than 2.\nTable 3.  Performance comparison of ablation studies on the IDRiD dataset. Bold font is used to indicate the \nbest result compared to other models.\nAblation studies\nEX HE MA SE\nAUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC\nBaseline 0.7934 0.8956 0.5931 0.9137 0.3823 0.9423 0.5130 0.9024\nBaseline + VTA 0.8603 0.9889 0.6486 0.9497 0.4050 0.9732 0.5903 0.9316\nVTA+HBE 0.8640 0.9943 0.6494 0.9487 0.4063 0.9839 0.5978 0.9463\nTable 4.  Performance comparison of ablation studies on the DDR dataset. Bold font is used to indicate the \nbest result compared to other models.\nAblation studies\nEX HE MA SE\nAUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC\nBaseline 0.4923 0.9081 0.3248 0.8748 0.0830 0.9021 0.2215 0.8353\nBaseline + VTA 0.5598 0.9635 0.3739 0.9392 0.1023 0.9382 0.2594 0.8748\nVTA+HBE 0.5607 0.9745 0.3778 0.9403 0.1066 0.9409 0.2694 0.8857\nTable 5.  Performance comparison of different curvature value on the IDRiD dataset. Bold font is used to \nindicate the best result compared to other models.\nCurvature value\nEX HE MA SE\nAUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC\n0 0.8623 0.9887 0.6465 0.9354 0.4009 0.9712 0.5909 0.9303\n0.2 0.8643 0.9892 0.6459 0.9332 0.4023 0.9729 0.5912 0.9392\n0.4 0.8620 0.9905 0.6485 0.9367 0.4034 0.9829 0.5923 0.9402\n0.8 0.8659 0.9948 0.6487 0.9459 0.4023 0.9810 0.5923 0.9434\n1.0 0.8640 0.9943 0.6494 0.9487 0.4063 0.9839 0.5978 0.9463\n2.0 0.8603 0.9932 0.6492 0.9503 0.4021 0.9804 0.5923 0.9320\n5.0 0.8610 0.9837 0.6421 0.9358 0.4002 0.9723 0.5938 0.9335\n10.0 0.7423 0.8729 0.5293 0.8195 0.3474 0.8538 0.5231 0.9025\nTable 6.  Performance comparison of different curvature value on the DDR dataset. Bold font is used to \nindicate the best result compared to other models.\nCurvature value\nEX HE MA SE\nAUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC AUC_PR AUC_ROC\n0 0.5542 0.9710 0.3720 0.9227 0.0946 0.9203 0.2621 0.8759\n0.2 0.5578 0.9732 0.3750 0.9312 0.1043 0.9401 0.2602 0.8809\n0.4 0.5592 0.9741 0.3729 0.9376 0.1051 0.9372 0.2598 0.8840\n0.8 0.5594 0.9720 0.3769 0.9473 0.1037 0.9391 0.2635 0.8824\n1.0 0.5607 0.9745 0.3778 0.9403 0.1066 0.9409 0.2694 0.8857\n2.0 0.5602 0.9775 0.3784 0.9326 0.1021 0.9391 0.2603 0.8821\n5.0 0.5532 0.9707 0.3742 0.9340 0.1003 0.9289 0.2583 0.8723\n10.0 0.5228 0.9532 0.3494 0.9153 0.0845 0.9102 0.2226 0.7693\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nConclusions\nThe main contribution of this study is the introduction of a hyperbolic space-based Transformer model archi -\ntecture for DR image segmentation. The VTA+HBE model adapts the original Vision Transformer model by \nadding a spatial prior module that leverages convolutional neural networks to extract image features. This allows \nthe model to capture both spatial and semantic information, which is crucial for the accurate segmentation \nof lesions in DR. The VTA+HBE model also incorporates a spatial feature injector and extractor to improve \nfeature interaction. The hyperbolic embeddings for pixel-level classification addresses the challenges faced by \ncurrent implementations of hyperbolic polynomial logistic regression, resulting in more efficient and accurate \nsegmentation of lesions in DR.\nThrough extensive experimentation, we provide compelling evidence of the efficacy of the proposed Trans -\nformer-based model architecture in extracting meaningful features from DR images, while also demonstrating \nthe potential benefits of incorporating hyperbolic space within deep learning frameworks. Given the severe \nconsequences of DR, including vision impairment and blindness, the research community has shown a grow -\ning interest in developing automated DR detection systems. Therefore, the significance of this study lies in its \npotential to aid in the accurate diagnosis of DR by automating the detection of DR lesions.\nIn future research, we will delve deeper into exploring the synergistic integration of hyperbolic space and \ndiverse deep learning architectures, with a specific focus on their application in DR classification and segmenta-\ntion tasks. We aim to advance the field by investigating novel techniques and methodologies that leverage the \nunique properties of hyperbolic space to enhance the performance, interpretability, and generalization capabilities \nof deep learning models when applied to DR classification or segmentation tasks.\nData availibility\nThe datasets used and/or analysed during the current study available from the corresponding author on reason-\nable request.\nReceived: 5 April 2023; Accepted: 6 July 2023\nReferences\n 1. Skouta, A., Elmoufidi, A., Jai-Andaloussi, S. & Ouchetto, O. Hemorrhage semantic segmentation in fundus images for the diagnosis \nof diabetic retinopathy by using a convolutional neural network. J. Big Data 9(1), 1–24 (2022).\n 2. Elsharkawy, M. et al. The role of different retinal imaging modalities in predicting progression of diabetic retinopathy: A survey. \nSensors 22(9), 3490 (2022).\n 3. Das, S., Kharbanda, K., Suchetha, M., Raman, R. & Dhas, E. Deep learning architecture based on segmented fundus image features \nfor classification of diabetic retinopathy. Biomed. Signal Process. Control 68, 102600 (2021).\n 4. Little, K. et al. Common pathways in dementia and diabetic retinopathy: Understanding the mechanisms of diabetes-related \ncognitive decline. Trends Endocrinol. Metab. 33(1), 50–71 (2022).\n 5. Arrigo, A., Aragona, E. & Bandello, F . Vegf-targeting drugs for the treatment of retinal neovascularization in diabetic retinopathy. \nAnn. Med. 54(1), 1089–1111 (2022).\n 6. Palta, H. & Karakaya, M. Image quality assessment of smartphone-based retinal imaging systems. In Smart Biomedical and Physi-\nological Sensor Technology XIV, Vol. 12123, 1212302. SPIE.\n 7. Bilal, A., Zhu, L., Deng, A., Lu, H. & Wu, N. Ai-based automatic detection and classification of diabetic retinopathy using u-net \nand deep learning. Symmetry 14(7), 1427 (2022).\n 8. Abdou, M.A. Literature review: Efficient deep neural networks techniques for medical image analysis. Neural Comput. Appl. 1–22 \n(2022).\n 9. Tang, M. C. S., Teoh, S. S. & Ibrahim, H. Retinal vessel segmentation from fundus images using deeplabv3+. In 2022 IEEE 18th \nInternational Colloquium on Signal Processing & Applications (CSPA), 377–381 (IEEE, 2022).\n 10. Tang, M. C. S. & Teoh, S. S. Blood vessel segmentation in fundus images using hessian matrix for diabetic retinopathy detection. \nIn 2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), 0728–0733 \n(IEEE, 2020).\n 11. Tang, M. C. S., Teoh, S. S., Ibrahim, H. & Embong, Z. A deep learning approach for the detection of neovascularization in fundus \nimages using transfer learning. IEEE Access 10, 20247–20258 (2022).\n 12. Tang, M. C. S., Teoh, S. S., Ibrahim, H. & Embong, Z. Neovascularization detection and localization in fundus images using deep \nlearning. Sensors 21(16), 5327 (2021).\n 13. Suganyadevi, S., Seethalakshmi, V . & Balasamy, K. A review on deep learning in medical image analysis. Int. J. Multimed. Inf. Retr. \n11(1), 19–38 (2022).\n 14. Y ou, C., Zhou, Y ., Zhao, R., Staib, L. & Duncan, J. S. Simcvd: Simple contrastive voxel-wise representation distillation for semi-\nsupervised medical image segmentation. IEEE Trans. Med. Imaging (2022).\n 15. Hatamizadeh, A., Tang, Y ., Nath, V ., Y ang, D., Myronenko, A., Landman, B., Roth, H. R. & Xu, D. Unetr: Transformers for 3d \nmedical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 574–584.\n 16. Tang, M. C. S. & Teoh, S. S. Brain tumor detection from mri images based on resnet18. In 2023 6th International Conference on \nInformation Systems and Computer Networks (ISCON), 1–5 (IEEE, 2023).\n 17. Mateen, M. et al. Automatic detection of diabetic retinopathy: A review on datasets, methods and evaluation metrics. IEEE Access \n8, 48784–48811 (2020).\n 18. Zhang, J., Li, C., Yin, Y ., Zhang, J. & Grzegorzek, M. Applications of artificial neural networks in microorganism image analysis: \nA comprehensive review from conventional multilayer perceptron to popular convolutional neural network and potential visual \ntransformer. Artif. Intell. Rev. 1–58 (2022).\n 19. Shamshad, F ., Khan, S., Zamir, S. W ., Khan, M. H., Hayat, M., Khan, F . S. & Fu, H. Transformers in medical imaging: A survey. \narXiv preprint arXiv: 2201. 09873 (2022).\n 20. Peng, W ., Varanka, T., Mostafa, A., Shi, H. & Zhao, G. Hyperbolic deep neural networks: A survey. IEEE Trans. Pattern Anal. Mach. \nIntell. 44(12), 10023–10044 (2021).\n 21. Rashed-Al-Mahfuz, M. et al. A deep convolutional neural network method to detect seizures and characteristic frequencies using \nepileptic electroencephalogram (eeg) data. IEEE J. Transl. Eng. Health Med. 9, 1–12 (2021).\n 22. Miikkulainen, R. et al. Evolving Deep Neural Networks 293–312 (Elsevier, 2019).\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\n 23. Huang, S., Li, J., Xiao, Y ., Shen, N. & Xu, T. Rtnet: Relation transformer network for diabetic retinopathy multi-lesion segmenta-\ntion. IEEE Trans. Med. Imaging (2022).\n 24. Zhang, L., Feng, S., Duan, G., Li, Y . & Liu, G. Detection of microaneurysms in fundus images based on an attention mechanism. \nGenes 10(10), 817 (2019).\n 25. Zhou, Y ., He, X., Huang, L., Liu, L., Zhu, F ., Cui, S. & Shao, L. Collaborative learning of semi-supervised segmentation and clas-\nsification for medical images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2079–2088.\n 26. Yu, T. & De Sa, C. M. Numerically accurate hyperbolic embeddings using tiling-based models. Adv. Neural Inf. Process. Syst. 32 \n(2019).\n 27. Nickel, M. & Kiela, D. Poincaré embeddings for learning hierarchical representations. Adv. Neural Inf. Process. Syst. 30, (2017).\n 28. Saxena, C., Chaudhary, M. & Meng, H. Cross-lingual word embeddings in hyperbolic space. arXiv preprint arXiv: 2205. 01907  \n(2022).\n 29. Tifrea, A., Bécigneul, G. & Ganea, O. -E. Poincaré glove: Hyperbolic word embeddings. arXiv preprint arXiv: 1810. 06546 (2018).\n 30. Zhang, Y ., Wang, X., Shi, C., Liu, N. & Song, G. Lorentzian graph convolutional networks. In Proceedings of the Web Conference, \n1249–1261 (2021).\n 31. Dai, J., Wu, Y ., Gao, Z. & Jia, Y . A hyperbolic-to-hyperbolic graph convolutional network. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 154–163.\n 32. Ganea, O., Bécigneul, G. & Hofmann, T. Hyperbolic neural networks. Adv. Neural Inf. Process. Syst. 31 (2018).\n 33. Chien, E., Pan, C., Tabaghi, P . & Milenkovic, O. Highly scalable and provably accurate classification in poincaré balls. In 2021 IEEE \nInternational Conference on Data Mining (ICDM), 61–70 (IEEE).\n 34. Atigh, M. G., Schoep, J., Acar, E., van Noord, N. & Mettes, P . Hyperbolic image segmentation. In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, 4453–4462.\n 35. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. & Polosukhin, I. Attention is all you need. \nAdv. Neural Inf. Process. Syst. 30 (2017).\n 36. Gao, Y ., Zhou, M. & Metaxas, D. N. Utnet: A hybrid transformer architecture for medical image segmentation. In Medical Image \nComputing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September \n27-October 1, 2021, Proceedings, Part III 24, 61–71 (Springer).\n 37. Shen, Z., Y ang, H., Zhang, Z. & Zheng, S. Automated Kidney Tumor Segmentation with Convolution and Transformer Network 1–12 \n(Springer, 2022).\n 38. Wang, W ., Chen, C., Ding, M., Yu, H., Zha, S. & Li, J. Transbts: Multimodal brain tumor segmentation using transformer. In Medical \nImage Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September \n27-October 1, 2021, Proceedings, Part I 24, 109–119 (Springer).\n 39. Yun, B., Wang, Y ., Chen, J., Wang, H., Shen, W . & Li, Q. Spectr: Spectral transformer for hyperspectral pathology image segmenta-\ntion. arXiv preprint arXiv: 2103. 03604 (2021).\n 40. Chien, E., Pan, C., Tabaghi, P . & Milenkovic, O. Highly scalable and provably accurate classification in poincaré balls. In 2021 IEEE \nInternational Conference on Data Mining (ICDM), 61–70 (IEEE, 2021).\n 41. Guo, N. et al. Poincaré heterogeneous graph neural networks for sequential recommendation. ACM Trans. Inf. Syst. 41(3), 1–26 \n(2023).\n 42. Vermeer, J. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic plane. Topol. Appl. 152(3), 226–242 \n(2005).\n 43. Chen, Z., Duan, Y ., Wang, W ., He, J., Lu, T., Dai, J. & Qiao, Y . Vision transformer adapter for dense predictions. arXiv preprint \narXiv: 2205. 08534 (2022).\n 44. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., \nGelly, S., Uszkoreit, J. & Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale (2020).\n 45. Jie, S. & Deng, Z. -H. Convolutional bypasses are better vision transformer adapters. arXiv preprint arXiv: 2207. 07039 (2022).\n 46. Wang, W . et al. Pvt v2: Improved baselines with pyramid vision transformer. Comput. Vis. Media 8(3), 415–424 (2022).\n 47. Porwal, P . et al. Indian diabetic retinopathy image dataset (idrid): A database for diabetic retinopathy screening research. Data  \n3(3), 25 (2018).\n 48. Li, T. et al. Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening. Inf. Sci. 501, 511–522 (2019).\n 49. Chen, L. -C., Zhu, Y ., Papandreou, G., Schroff, F . & Adam, H. Encoder-decoder with atrous separable convolution for semantic \nimage segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), 801–818.\n 50. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International Confer-\nence on Medical Image Computing and Computer-Assisted Intervention, 234–241. (Springer).\n 51. Zhou, Z., Rahman Siddiquee, M. M., Tajbakhsh, N. & Liang, J. Unet++: A nested u-net architecture for medical image segmentation. \nIn Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, \nDLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September \n20, 2018, Proceedings 4, 3–11 (Springer).\n 52. Strudel, R., Garcia, R., Laptev, I. & Schmid, C. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/\nCVF International Conference on Computer Vision, 7262–7272.\nAcknowledgements\nWe would like to acknowledge the First Affiliated Hospital of Anhui University of Chinese Medicine for infra-\nstructure and radiologist evaluation support.\nAuthor contributions\nZ.W .: Designed the study, collected and analyzed the data. H.L.: Assisted with the data analysis, contributed to the \ninterpretation of the results, and critically reviewed the manuscript. H.Y .: Provided guidance on the study design, \ncontributed to the data analysis and interpretation, and critically reviewed the manuscript. H.K.: Contributed \nto the study design, provided guidance on the data analysis, and critically reviewed the manuscript. L.J.: Con-\ntributed to the study design, provided guidance on the data analysis, and wrote the manuscript. Each author has \nread and approved the final version of the manuscript and has agreed to be accountable for their contributions.\nFunding\nThis work was supported by the Provincial Natural Science Research Key Project of Anhui Universities (No. \nKJ2020A0443), the Key Project of Humanities and Social Sciences Research in Anhui Universities (No.\nSK2021A0329), the Outstanding Y oung Backbone Talents of Anhui University Visiting and Training Abroad Pro-\nject (No. gxgwfx2019026), the Provincial Teaching Research Key Project of Anhui Province (No. 2020jyxm1018), \n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:11178  | https://doi.org/10.1038/s41598-023-38320-5\nwww.nature.com/scientificreports/\nthe Natural Science Key Project of Anhui University of Chinese Medicine (No. 2020zrzd17), and the Quality \nEngineering Project of Anhui Province (No. 2021xsxxkc140).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to L.J.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7094227075576782
    },
    {
      "name": "Computer science",
      "score": 0.7029470205307007
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6825553178787231
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.518425464630127
    },
    {
      "name": "Computer vision",
      "score": 0.4296983480453491
    },
    {
      "name": "Image segmentation",
      "score": 0.42181485891342163
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I253932293",
      "name": "Anhui University of Traditional Chinese Medicine",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I197869895",
      "name": "Anhui Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I16365422",
      "name": "Hefei University of Technology",
      "country": "CN"
    }
  ]
}