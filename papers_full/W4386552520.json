{
  "title": "Fearless Luminance Adaptation: A Macro-Micro-HierarchicalÂ Transformer for Exposure Correction",
  "url": "https://openalex.org/W4386552520",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5051231286",
      "name": "Gehui Li",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100675904",
      "name": "Jinyuan Liu",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5037377300",
      "name": "Long Ma",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016996735",
      "name": "Zhiying Jiang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5057776894",
      "name": "Xin Fan",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5042370642",
      "name": "Risheng Liu",
      "affiliations": [
        "Dalian University of Technology",
        "Peng Cheng Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3173307371",
    "https://openalex.org/W2780108394",
    "https://openalex.org/W2551717155",
    "https://openalex.org/W2798844427",
    "https://openalex.org/W3204374989",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W4292794831",
    "https://openalex.org/W2468596194",
    "https://openalex.org/W3035731588",
    "https://openalex.org/W2405492408",
    "https://openalex.org/W6744980455",
    "https://openalex.org/W4312644870",
    "https://openalex.org/W4386075792",
    "https://openalex.org/W4304084103",
    "https://openalex.org/W2607202125",
    "https://openalex.org/W4285228564",
    "https://openalex.org/W4214665850",
    "https://openalex.org/W4214902553",
    "https://openalex.org/W4293731547",
    "https://openalex.org/W3147590024",
    "https://openalex.org/W4312594135",
    "https://openalex.org/W3126855404",
    "https://openalex.org/W4210571497",
    "https://openalex.org/W4321794021",
    "https://openalex.org/W3197456663",
    "https://openalex.org/W4285821234",
    "https://openalex.org/W3110670586",
    "https://openalex.org/W4200629530",
    "https://openalex.org/W3174792937",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6755037456",
    "https://openalex.org/W4221107295",
    "https://openalex.org/W3035229960",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3207649350",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W4312926994",
    "https://openalex.org/W2948354154",
    "https://openalex.org/W4212918797",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3011440129",
    "https://openalex.org/W3034347506",
    "https://openalex.org/W3013338555",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W6600281463",
    "https://openalex.org/W4304015075",
    "https://openalex.org/W2897662688",
    "https://openalex.org/W3120540810",
    "https://openalex.org/W2981718299",
    "https://openalex.org/W3204478975",
    "https://openalex.org/W4289537807",
    "https://openalex.org/W4320350643",
    "https://openalex.org/W3201409833",
    "https://openalex.org/W4226185636",
    "https://openalex.org/W4288359455",
    "https://openalex.org/W4230472795",
    "https://openalex.org/W3035687312",
    "https://openalex.org/W3105273993",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3002301267",
    "https://openalex.org/W4312746248",
    "https://openalex.org/W2245625259",
    "https://openalex.org/W2893333553",
    "https://openalex.org/W3099865281",
    "https://openalex.org/W1989550328",
    "https://openalex.org/W4281727859",
    "https://openalex.org/W4313443534",
    "https://openalex.org/W3013313866",
    "https://openalex.org/W2760509339",
    "https://openalex.org/W2949187370",
    "https://openalex.org/W2766497195",
    "https://openalex.org/W2949370505",
    "https://openalex.org/W2997763120",
    "https://openalex.org/W3082630009",
    "https://openalex.org/W4387696030",
    "https://openalex.org/W4390190099",
    "https://openalex.org/W4226445524",
    "https://openalex.org/W4221146853",
    "https://openalex.org/W3035357085"
  ],
  "abstract": "Photographs taken with less-than-ideal exposure settings often display poor\\nvisual quality. Since the correction procedures vary significantly, it is\\ndifficult for a single neural network to handle all exposure problems.\\nMoreover, the inherent limitations of convolutions, hinder the models ability\\nto restore faithful color or details on extremely over-/under- exposed regions.\\nTo overcome these limitations, we propose a Macro-Micro-Hierarchical\\ntransformer, which consists of a macro attention to capture long-range\\ndependencies, a micro attention to extract local features, and a hierarchical\\nstructure for coarse-to-fine correction. In specific, the complementary\\nmacro-micro attention designs enhance locality while allowing global\\ninteractions. The hierarchical structure enables the network to correct\\nexposure errors of different scales layer by layer. Furthermore, we propose a\\ncontrast constraint and couple it seamlessly in the loss function, where the\\ncorrected image is pulled towards the positive sample and pushed away from the\\ndynamically generated negative samples. Thus the remaining color distortion and\\nloss of detail can be removed. We also extend our method as an image enhancer\\nfor low-light face recognition and low-light semantic segmentation. Experiments\\ndemonstrate that our approach obtains more attractive results than\\nstate-of-the-art methods quantitatively and qualitatively.\\n",
  "full_text": "Fearless Luminance Adaptation: A Macro-Micro-Hierarchical\nTransformer for Exposure Correction\nGehui Li\nDalian University of Technology\ngehuili90@gmail.com\nJinyuan Liu\nDalian University of Technology\natlantis918@hotmail.com\nLong Ma\nDalian University of Technology\nmalone94319@gmail.com\nZhiying Jiang\nDalian University of Technology\nzyjiang0630@gmail.com\nXin Fan\nDalian University of Technology\nxin.fan@dlut.edu.cn\nRisheng Liuâˆ—\nDalian University of Technology\nPeng Cheng Laboratory\nrsliu@dlut.edu.cn\nInput IAT LCDP Ours\n +1.5 EV  +1.0 EV     0 EV    -1.0 EV   -1.5 EV\nMSEC\nFigure 1: The row displays a visual comparison of two cutting-edge methods alongside our proposed one, implemented on five\nseparate exposures (ranging from +1.5EV to -1.5EV). It is crucial to emphasize that our method not only precisely corrects color\ndistortions but also ensures consistency of the correction results.\nABSTRACT\nPhotographs taken with less-than-ideal exposure settings often\ndisplay poor visual quality. Since the correction procedures vary\nsignificantly, it is difficult for a single neural network to handle all\nexposure problems. Moreover, the inherent limitations of convolu-\ntions, hinder the models ability to restore faithful color or details on\nextremely over-/under- exposed regions. To overcome these limita-\ntions, we propose a Macro-Micro-Hierarchical transformer, which\nconsists of a macro attention to capture long-range dependencies,\na micro attention to extract local features, and a hierarchical struc-\nture for coarse-to-fine correction. In specific, the complementary\nmacro-micro attention designs enhance locality while allowing\nglobal interactions. The hierarchical structure enables the network\nto correct exposure errors of different scales layer by layer. Further-\nmore, we propose a contrast constraint and couple it seamlessly in\nthe loss function, where the corrected image is pulled towards the\npositive sample and pushed away from the dynamically generated\nnegative samples. Thus the remaining color distortion and loss of\ndetail can be removed. We also extend our method as an image\nâˆ—Corresponding author: Risheng Liu.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0108-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3581783.3612436\nenhancer for low-light face recognition and low-light semantic\nsegmentation. Experiments demonstrate that our approach obtains\nmore attractive results than state-of-the-art methods quantitatively\nand qualitatively.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Computational photography.\nKEYWORDS\nImage restoration, Exposure correction, Low-light enhancement,\nLow-light semantic segmentation, Low-light face detection\nACM Reference Format:\nGehui Li, Jinyuan Liu, Long Ma, Zhiying Jiang, Xin Fan, and Risheng\nLiu. 2023. Fearless Luminance Adaptation: A Macro-Micro-Hierarchical\nTransformer for Exposure Correction. In Proceedings of the 31st ACM In-\nternational Conference on Multimedia (MM â€™23), October 29â€“November 3,\n2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 10 pages. https:\n//doi.org/10.1145/3581783.3612436\n1 INTRODUCTION\nThe ever-increasing interest in photography has intensified the\nneed to capture expansive scenes under diverse lighting conditions.\nNevertheless, image color and detail can be significantly compro-\nmised by sub-optimal and non-uniform illumination. While profes-\nsional photographers often tackle exposure issues with specialized\nequipment and software, these solutions necessitate considerable\nexpertise and can be costly. Moreover, harsh exposure conditions\ncan adversely impact the performance of downstream computer\nvision tasks. Consequently, there is a pressing need for a model\narXiv:2309.00872v2  [cs.CV]  18 Dec 2023\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Gehui Li et al.\ncapable of correcting images with a wide range of exposure errors\nto achieve consistent, normal exposure without sacrificing color\nfidelity or detail.\nExisting models [13, 22, 36, 39, 40, 43â€“45, 53, 56] primarily ad-\ndressing single exposure error scenarios perform inadequately\nwhen confronted with images containing multiple exposure errors.\nPredicated on the assumption that scene lighting is low-light, these\nmethods are inherently ill-equipped to adapt to varying exposure\nstates. In addition, multi-exposure fusion methods[30, 31, 33, 34, 38]\nare powerless when there is only one image of the same scene. These\nlimitation have spurred the development of new approaches[1, 7,\n11, 45, 52] for tackling multiple exposure errors.\nMSEC[1] addresses exposure correction of input images under\ndiverse lighting. LCDP[52] is designed to tackle the challenge of\nmultiple exposure errors within a single image. However, existing\napproaches suffer from two critical shortcomings. First, when pre-\nsented with a set of images with different exposure errors for the\nsame scene, these methods fail to achieve a consistent correction\nstate, leading to considerable discrepancies in the correction out-\ncomes. Second, they struggle to restore distorted details in heavily\noverexposed and underexposed areas, resulting in unbalanced and\nlackluster colors in the corrected images.\nRecognizing the importance of addressing the irreversible color\ndistortion in severely overexposed or underexposed regions, we\nadvocate for the utilization of richer semantic information from\nneighboring regions to compensate for the lost color. By providing\nthe network with enhanced access to multi-scale information, the\nrecovery of color-distorted regions is facilitated. To this end, we\npropose the Macro-Micro Transformer Restorer (MMT Restorer).\nSimultaneously, we introduce a dynamic negative sample generator\nand a contrast constraint function. By incorporating a substantial\nnumber of high-quality negative samples, the network learns a\nmore consistent criterion, ultimately enhancing the consistency\nof input image correction. Furthermore, we present the Macro-\nMicro-Hierarchical Transformer (MMHT), a hierarchical extension\nof MMT Restorer, which refines the correction process from coarse\nto fine, further improving the quality of the corrected images.\nThe contributions of this paper are as follows:\nâ€¢We propose a Macro-Micro-Hierarchical Transformer, which,\nin comparison to existing methods, yields visually appealing\ncolors and consistently corrected outcomes.\nâ€¢Considering the limited capacity of convolutions to attend to\nnon-local regions, we design a Macro-Micro Attention mech-\nanism that effectively restores distorted color by strengthen-\ning global dependencies.\nâ€¢To explore the intrinsic information of over-/under- exposed\nimage and the reference one, we raises a hierarchical con-\ntrastive learning scheme. In this manner, the restored image\nis pulled to the reference image and pushed away from the\nmultiple exposed ones in a hierarchical latent feature spaces.\nâ€¢Extensive experimental results demonstrate our method em-\npirically realizes the remarkable promotion in color and con-\nsistency of corrected images, as well as standard accuracy\nfor low-light segmentation and detection compared with\nexisting advanced networks.\n2 RELATED WORK\n2.1 Methods against Harsh Lighting Condition\nLow-Light Enhancement. Traditional methods[3, 12, 12, 14, 35,\n37, 40, 58, 67, 69] based on Retinex theory have been favored in\nrecent years. These methods mainly decompose the image into\nreflectance map and illumination map, and then do processing on\nthem separately to get the final result. Therefore, the goal is to\npredict the illumination map to obtain a well-exposed target image.\nSome deep learning-based methods[6, 10, 18â€“21, 23, 28, 32, 53, 56,\n65, 71] have also emerged subsequently. All these works are lim-\nited to image correction for underexposure conditions. However,\nmis-exposed images are common in reality, which also contain a\nlarge number of over-exposed and unevenly exposed images. There-\nfore, our goal is to construct a model capable of simultaneously\ncorrecting images with these different exposure levels.\nExposure Correction. MSEC[1] proposes a coarse-to-fine deep\nlearning approach that focuses for the first time on the task of\nsimultaneously correcting overexposed and underexposed images.\nIAT[7] decomposes the Image Signal Processor (ISP) into local and\nglobal image components to correct the image from low-light or\noverexposure conditions and proceeds to solve the problem of low-\nlight image semantic segmentation. LCDP[52] proposes the LCDE\nmodule with Dual-Illumination mechanism to solve the problem\nof having multiple exposure errors on a single image. However,\nexisting methods still have two common problems: inconsistent cor-\nrection results and non-enjoyable colors. In this paper, we propose\na Macro-Micro-Hierarchical Transformer to solve them.\n2.2 Vision Transformers\nA number of image restorers designed using transformer have\nappeared. For example, Uformer[ 55] replaces the building block\nwith its proposed LeWin Transformer Block based on the Encoder-\nDecoder structure of U-Net, which consists of window attention\nwith a sliding window[41] and Locally Enhanced Feed-forward[62].\nRestormer[63] proposes Transformer Block consisting of A Multi-\nDconv Head Transposed Attention with Gated Dconv Feed-forward\nNetwork. But these general image restorers, have a common prob-\nlem that they do not adapt well to the exposure correction problem.\nIn this paper, we propose a Macro-Micro Transformer Block con-\ntaining two mechanisms of complementary Attention to solve this\nproblem. Although Transformers has a stronger model fitting capa-\nbility, it requires far more data than CNNs because of the lack of in-\nductive bias. How to effectively combine the strengths of both is an\nimportant issue. A large number of networks[47, 48, 57, 62, 64, 66]\ndesigned for high level tasks in recent years have organically com-\nbined convolution and transformer. The Coatnet[9] is particularly\nenlightening in its exploration of the arrangement of convolution\nbuilding blocks and transformer building blocks. Using a similar\nexperimental method, we design the final network structure and\nits better adaptation to our task.\n3 METHOD\n3.1 The overall of the proposed method\nOur proposed method commences with a Laplace decomposition\nto obtain four-layer decomposed images, L1-L4, derived from the\nFearless Luminance Adaptation: A Macro-Micro-Hierarchical Transformer for Exposure Correction MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\n(b) Macro-Micro Transformer Block\n(c) Macro & Micro Partition / Reverse\n(a) The overall of the proposed method (d)  Macro & Micro Attention \nFigure 2: (a) The architecture of our MMHT method, which contains MMT Restorer and U-Net Restorer to reconstruct the\nnormal exposure image. (b) Macro-Micro Transformer Block with two complementary attention mechanisms to capture global\ndependencies. (c) Diagram of Micro & Macro Attention Partition. (d) The structure of Micro & Macro Attention.\ninput image. These decomposed images serve as the input terms for\nthe skip-connection. The primary objective of this operation is to\ndecompose the source images into distinct spatial frequency bands,\nthereby enabling the utilization of separate networks for the restora-\ntion of colors and details in specific frequency bands at varying\ndecomposition levels. The L4 image is fed into the network as the\ninitial term. The architecture of the network comprises four stages,\nwith stages 1, 2, and 3 designated for the Macro-Micro Transformer\nRestorer (MMT Restorer) and stage 4 for the U-Net Restorer. In the\nfinal stage, we employ a convolution-based structure as a substitute\nfor the MMT Restorer. This decision is motivated by the fact that\nthe transformer sacrifices its inductive bias while acquiring a global\nperceptual field through the attention structure. Consequently, we\nconstruct the U-Net Restorer by replacing the MMT Block with\nmulti-layer convolutions, while preserving the overall structure of\nthe MMT Restorer. In combination with convolution, our method\nachieves superior correction results. An additional benefit of this\napproach is that implementing convolution on larger sizes reduces\nthe computation complexity. Following each stage, up-sampling is\nperformed to double the width and height. Ultimately, the512 Ã—512\nsize image is up-sampled by BGU[4] to its original size, yielding\nthe final output. A schematic representation of the framework is\ndepicted in Figure 2.\n3.2 Macro-Micro Transformer Restorer\nAs illustrated in Figure 2 (a), the specific structure of our method\nis as follows. Given an input image, its dimensions are initially\nincreased by an Embedding layer to C. Subsequently, the input is\nprocessed through an Encoder consisting of three stages, wherein\nidentical MMT Blocks are stacked repeatedly with ğ¿ğ‘– layers in\neach stage. Upon generating the feature output, the image size\nis downsampled using pooling, and the dimensions are increased\nvia point-wise convolution. Following feature extraction by the\nEncoder, initial semantic features are obtained. These semantic\nfeatures are further refined using the Bottle Block, which serves as\ninput for the Decoder to guide the generation of exposure-corrected\nimages. Prior to feeding the image into the subsequent stage, it\nis upsampled using Pixel-shuffle[ 51] and concatenated with the\nfeatures processed by the Encoder at the same stage via a skip\nconnection. In a similar way, the dimensionality is reduced to 3 by\nthe Projection layer, and the resulting output is added to the input\nimage to produce the exposure-corrected image.\n3.3 Macro-Micro Transformer Block\nIn MMT Restorer, we alternately arrange Macro Attention and\nMicro Attention to obtain global and local features, but with linear\ncomplexity for spatial dimensions. Diagram is shown in Figure 2\n(b). The formulas of spatial attention is as follows.\nAğ‘†ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (Q,K,V)= ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (â„ğ‘’ğ‘ğ‘‘1,...,â„ğ‘’ğ‘ğ‘‘ ğ¶\nğ»ğ‘‘\n), (1)\nğ‘¤â„ğ‘’ğ‘Ÿğ‘’ headğ‘– = Softmax\n\u0012\nQğ‘–Kğ‘‡\nğ‘–âˆšğ»ğ‘‘\n\u0013\nVğ‘–. Qğ‘– = Xğ‘–Wğ‘„\nğ‘– ,Kğ‘– = Xğ‘–Wğ¾\nğ‘– and\nVğ‘– = Xğ‘–Wğ‘‰\nğ‘– âˆˆRğ‘ƒÃ—ğ»ğ‘‘. Wğ‘– denotes the projection weights of theğ‘–ğ‘¡â„\nhead for Qğ‘–,Kğ‘–,Vğ‘–. ğ‘ƒ is is the number of total patches. ğ»ğ‘‘ is head\ndimension. ğ¶ is the total channel size.\n3.3.1 Micro Attention. Conventional transformers employ global\nattention computation, resulting in a quadratic computational com-\nplexity. In contrast, Micro Attention confines attention computation\nto each window, effectively reducing the computational burden. Mi-\ncro Partition: (ğ»\nğ· Ã—ğ·,ğ‘Š\nğ· Ã—ğ·,ğ¶)â†’( ğ»ğ‘Š\nğ·2 ,ğ·2,ğ¶). After partition,\nwe have the following formula.\nMSAğ‘€ğ‘–ğ‘ğ‘Ÿğ‘œ (Q,K,V)=\n\b\nAğ‘†ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (Qğ‘—,Kğ‘—,Vğ‘—)\n\tğ»ğ‘Š\nğ·2\nğ‘—=0 , (2)\nğ‘¤â„ğ‘’ğ‘Ÿğ‘’ Qğ‘—,Kğ‘—,Vğ‘— âˆˆRğ·2 Ã—ğ¶ are local window queries, keys, and\nvalues. Each window contains D Ã—D patches.\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Gehui Li et al.\nDespite this significant reduction in computation, the exclusive\nfocus on the window attention results in the loss of global informa-\ntion modeling capabilities. To address this limitation, we propose\nMacro Attention, characterized by a larger receptive field. This\nnovel approach complements Micro Attention, augmenting the\nacquisition of multi-scale information.\n3.3.2 Macro Attention. Drawing inspiration from the mechanism\nof dilated convolution, we incorporate it into the window division\nof attention in order to expand the receptive field. This adaptation\neffectively enhances the acquisition of global information while\npreserving linear complexity. Each window is inflated such that\nall windows fill the entire feature map without overlap. Macro\nPartition: (ğº Ã—ğ»\nğº,ğº Ã—ğ‘Š\nğº ,ğ¶) â†’ (ğº2,ğ»ğ‘Š\nğº2 ,ğ¶) â†’ (ğ»ğ‘Š\nğº2 ,ğº2,ğ¶).\nAfter partition, we have the following formula.\nMSAğ‘€ğ‘ğ‘ğ‘Ÿğ‘œ (Q,K,V)=\n\b\nAğ‘†ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (Qğ‘—,Kğ‘—,Vğ‘—)\n\tğ»ğ‘Š\nğº2\nğ‘—=0 , (3)\nğ‘¤â„ğ‘’ğ‘Ÿğ‘’ Qğ‘—,Kğ‘—,Vğ‘— âˆˆRğº2 Ã—ğ¶ are dilated window queries, keys, and\nvalues. Each dilated window contains G Ã—G patches.\n3.4 Loss Function\nWe train our model with following loss function:\nLTotal = ğœ†1LMAE +ğœ†2LDEC +ğœ†3LCR, (4)\nwhere LMAE denotes Mean absolute loss, LDEC the Pyramid de-\ncomposition loss and LCR the contrastive regularization.\n3.4.1 Mean absolute loss. ğ¿1 loss is used to characterize the error\nbetween the corrected image and the ground truth.\nLMAE =\n3ğ»ğ‘Šâˆ‘ï¸\nğ‘=1\n|Y(ğ‘)âˆ’T(ğ‘)|, (5)\nwhere H, W are the height and width of the input size of the current\nimage, respectively. T represents the ground truth and Y represents\nthe network corrected image by bilateral interpolation to the origi-\nnal size image. p represents the pixels on the image.\n3.4.2 Pyramid decomposition loss. This loss is used to measure the\nerror between the decomposed corrected image and its ground truth\nlayer by layer. These errors are proportionally merged together to\nform the loss of the decomposed image.\nLDEC =\nğ‘âˆ‘ï¸\nğ‘–=2\n2(ğ‘–âˆ’2)\n3â„ğ‘–ğ‘¤ğ‘–âˆ‘ï¸\nğ‘=1\n\f\fY(ğ‘–)(ğ‘)âˆ’T(ğ‘–)(ğ‘)\n\f\f, (6)\nwhere N represents the number of levels of the pyramid.Y(ğ‘–)stands\nfor i-th corrected pyramid output. T(ğ‘–)represents the i-th gaussian\npyramid layer decomposed from T. â„ğ‘– = 1\n2ğ‘–âˆ’1 â„and ğ‘¤ğ‘– = 1\n2ğ‘–âˆ’1 ğ‘¤.\n3.4.3 Contrastive regularization. As highlighted in existing con-\ntrast learning methods[5], diverse negative samples are vital for\neffective contrast learning. We propose a dynamic negative sample\ngeneration strategy, wherein for each input GT image, K negative\nsamples are generated via gamma correction, with gamma values\nuniformly selected from the range [G1, G2]. Training with mul-\ntiple negative samples enhances the modelâ€™s generalizability to\nunseen exposure error images, crucial for improving performance\nand real-world applicability.\nPush Pull\nPush\nPull\nPush\nPull\nNegative Sample Generator\nNegative Samples\nPush Pull\nNegative Samples\n    Anchor\nï¼ˆCorrected\n        Resultï¼‰\nLatent Feature Space \nLatent Feature Space \nLatent Feature Space \nLatent Feature Space \nPostive \nSample\n(GT)\nFigure 3: Adaptation Contrastive Regularization with Dy-\nnamic Negative Sample Generator\nAs shown in the Figure 3, we formulate a contrastive loss to min-\nimize the distance between the correction result and the positive\nsample while maximizing the distance from the negative sample. A\npre-trained network, R, is employed to extract features and apply\ncontrastive regularization. The rich information obtained through\nfeature extraction enables the contrastive loss to effectively con-\ntribute to the task.\nLCR =\nÃğ‘†\nğ‘ =1 ğ· \u0000ğ‘“ğ‘ ,ğ‘“ +ğ‘ \n\u0001\nÃğ¾\nğ‘˜=1\nÃğ‘†\nğ‘ =1 ğ·\n\u0010\nğ‘“ğ‘ ,ğ‘“ âˆ’\nğ‘˜ğ‘ \n\u0011, (7)\nwhere ğ·(ğ‘¥,ğ‘¦) = âˆ¥ğ‘¥ âˆ’ğ‘¦âˆ¥1, ğ‘“ğ‘  = ğ‘…(ğ‘Œ), ğ‘“+ğ‘  = ğ‘…(ğ‘‡), ğ‘“âˆ’ğ‘  = ğ‘…(ğ¹),\nğ‘  = 1,2,...,ğ‘† . K represents the number of samples generated by the\nnegative sample generator. Both positive and negative samples are\npassed through the pre-trained VGG19 network, and the corrected\nimage is downsampled S times. Y represents the corrected image, T\nrepresents the ground truth and F represents the negative images\ngenerated by Dynamic Negative Sample Generator.\n4 EXPERIMENT\n4.1 Implementation Details\nWe use the ADAM to optimize the network for 150 epochs with a\ninitial learning rate of 1e-4. The weight of the loss function is set\nto ğœ†1 = 1.0, ğœ†2 = 1.0 and ğœ†3 = 0.6. The level of the Laplacian\npyramid decomposition is ğ‘ = 4. The lowest gamma value is\nğº1 = 0.3 and the highest gamma value is ğº2 = 2.8. The number of\nnegative samples generated is ğ¾ = 6. The number of feature maps\nobtained by the feature extractor is ğ‘† = 4. Before training, we do\ndata augmentation and resize the image to 512 Ã—512 (â„Ã—ğ‘¤). We\nconduct all experiments with an NVIDIA 3090TI GPU.\n4.2 Comparisons with State-of-the-Art Methods\nWe evaluate our proposed method by comparing it with existing\nSOTA methods. First, we examine quantitative metrics, employing\nthe widely used peak signal-to-noise ratio (PSNR) and structural\nsimilarity (SSIM) as our evaluation indicators. To assess color per-\nformance, we utilize the FrÃ©chet Inception Distance (FID)[ 16] to\nmeasure the similarity between two sets of images, the Colorfulness\nFearless Luminance Adaptation: A Macro-Micro-Hierarchical Transformer for Exposure Correction MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nTable 1: Quantitative comparisons on MSEC dataset. We compare each method with properly exposed reference image sets\nrendered by five expert photographers. The best results are highlighted with red. The second- and third-best results are\nhighlighted in blue and green, respectively.\nExpert A Expert B Expert C Expert D Expert E AvgMethod PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nWVM[12] 14.488 0.788 15.803 0.699 15.117 0.678 15.863 0.693 16.469 0.704 15.548 0.688\nLIME[14] 11.154 0.591 11.828 0.610 11.517 0.607 12.638 0.628 13.613 0.653 12.150 0.618\nHDR CNN w/PS[10] 15.812 0.667 16.970 0.699 16.428 0.681 17.301 0.687 18.650 0.702 17.032 0.687\nDPED (iPhone)[21] 15.134 0.609 16.505 0.636 15.907 0.622 16.571 0.627 17.251 0.649 16.274 0.629\nDPED (BlackBerry)[21] 16.910 0.642 18.649 0.713 17.606 0.653 18.070 0.679 18.217 0.668 17.890 0.671\nDPE (HDR)[6] 15.690 0.614 16.548 0.626 16.305 0.626 16.147 0.615 16.341 0.633 16.206 0.623\nDPE (S-FiveK)[6] 16.933 0.678 17.701 0.668 17.741 0.696 17.572 0.674 17.601 0.670 17.510 0.677\nRetinexNet[56] 10.759 0.585 11.613 0.596 11.135 0.605 11.987 0.615 12.671 0.636 11.633 0.607\nDeep UPE[53] 13.161 0.610 13.901 0.642 13.689 0.632 14.806 0.649 15.678 0.667 14.247 0.640\nZero-DCE[13] 11.643 0.536 12.555 0.539 12.058 0.544 12.964 0.548 13.769 0.580 12.597 0.549\nRUAS[40] 10.166 0.391 10.522 0.440 9.356 0.411 11.013 0.441 11.574 0.466 10.526 0.430\nURetinex[59] 11.420 0.632 12.230 0.700 11.818 0.672 13.078 0.701 14.066 0.735 12.522 0.688\nDALE[25] 13.294 0.691 14.324 0.757 13.734 0.722 14.256 0.743 14.511 0.763 14.024 0.735\nIAT(local)[7] 16.610 0.750 17.520 0.822 16.950 0.780 17.020 0.780 16.430 0.789 16.910 0.783\nMSEC[1] 19.158 0.746 20.096 0.734 20.205 0.769 18.975 0.719 18.983 0.727 19.483 0.739\nIAT[7] 19.900 0.817 21.650 0.867 21.230 0.850 19.860 0.844 19.340 0.840 20.340 0.844\nLCDPNet[52] 20.574 0.809 21.804 0.865 22.295 0.855 20.108 0.824 19.281 0.822 20.812 0.835\nMMHT(Ours) 20.662 0.816 22.377 0.869 23.049 0.865 20.416 0.831 20.520 0.833 21.405 0.843\n           Input                       ZeroDCE                     DALE                           IAT                          MSEC                         LCDP                         Ours                            GT\nFigure 4: Qualitative comparisons on MSEC dataset. The blue line shows that the pixel values of the column are converted to\nperceived brightness and compared with that of ground truth.\nScore (CF)[15] to gauge the richness of image colors, and additional\nmetrics such as Î”CF and NIQE. Second, we conduct visual com-\nparisons of exposure correction results to further substantiate the\neffectiveness of our method.\n4.2.1 Quantitative Comparisons. The test set of MSEC contains\n1181 sets of images of the same scene with five different exposures,\naccompanied by the correction results of five different experts.\nTable 1 shows that our method outperforms all existing methods in\nterms of PSNR and SSIM on all five expert test sets of MSEC. For\ncolor performance comparisons in Table 2, our method achieves\nthe lowest FID, indicating that the proposed method is capable of\nproducing realistic, natural and consistent images. In addition, the\nlowest NIQE indicates that the corrected images of our method have\nthe best subjective viewability. There are some methods that are\ncloser to our method on CF, but the FIDs of these methods are much\nhigher than ours, which means that they generate a lot of rare colors.\nTherefore, Î”CF is used to measure the difference between the CF\nof the corrected image and that of the expert image. A lower Î”CF\nindicates a more realistic color of the image. OurÎ”CF is significantly\nbetter than all other methods. The performance comparisons are\npresented in the Figure 7 in the form of radar map. Each image of\nthe LCDP dataset has both overexposed and underexposed areas.\nOur method also gains SOTA on LCDP dataset, which shows that\nthe images corrected by our method are in excellent consistency.\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Gehui Li et al.\n               Input                               ZeroDCE                              DALE                                  IAT                                   MSEC                                  LCDP                                  Ours                                     GT\nFigure 5: Compare the color performance of our method with other SOTA methods. Below the image is the RGB histogram.\n            Input                           ZeroDCE                        DALE                           MSEC                              IAT                             LCDP                           Ours                               GT\n+1.5 EV\n-1.5 EV\n+1.5 EV\n +1.5 EV\n +1.5 EV\n +1.5 EV\n +1.5 EV\n-1.5 EV\n -1.5 EV\n -1.5 EV\n -1.5 EV\n -1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\nFigure 6: The consistency of correction results between our method and existing methods is compared. The correction results\nof overexposure and underexposure are concatenated.\nTable 2: Quantitative comparisons of color performance on\nMSEC and LCDP datasets.\nMethods(MSEC)PNSRâ†‘ SSIMâ†‘ FIDâ†“ CFâ†‘ Î”CFâ†“ NIQEâ†“\nRUAS 9.356 0.411 30.125 22.612 19.792 4.643\nURetinex 11.818 0.672 27.451 28.597 13.807 2.654\nMSEC 20.205 0.769 20.264 38.898 3.506 2.838\nIAT 21.230 0.850 12.091 37.188 5.216 3.132\nLCDP 22.295 0.855 12.752 33.293 9.111 2.914\n(MMHT)Ours23.049 0.865 10.223 40.202 2.202 2.339\nMethods(LCDP)PNSRâ†‘ SSIMâ†‘ FIDâ†“ CFâ†‘ Î”CFâ†“ NIQEâ†“\nLIME 17.335 0.686 47.725 32.949 6.077 4.248\nIAT 18.842 0.684 37.516 32.143 6.883 2.975\nRetinexNet 19.250 0.704 52.723 33.541 5.485 4.947\nMSEC 20.377 0.779 45.231 29.005 10.021 3.841\nLCDP 23.239 0.842 28.914 32.975 6.051 2.354\n(MMHT)Ours23.715 0.855 25.147 34.753 4.273 2.142\n          (a) Radar Map for MSEC dataset (b) Radar Map for LCDP dataset\nFigure 7: Radar Map for Performance Comparisons.\n4.2.2 Qualitative Comparisons. Figure 4 presents a comparison of\nimage quality between our method and SOTA methods. For both\nunderexposed and overexposed images, our method accurately cor-\nrects them to the proper exposure state. In Figure 6, we evaluate the\nconsistency of correction results by comparing corrected images\nt-SNE\nFigure 8: t-SNE visualization to show correction consistency.\nproduced by our method with those corrected by existing methods.\nOur methodâ€™s results exhibit remarkable consistency, displaying not\nonly similar color and brightness but also eliminating differences\nin outcomes due to input variations. Furthermore, as illustrated\nin Figure 5, images corrected by our method boast both the most\nvisually appealing color performance and superior detail preser-\nvation. In contrast, other methods either introduce inconsistent\nlighting or generate color distortion. Furthermore, as can be seen\nin Figure 8, after being processed by MMHT, the underexposure\nand overexposure representations tend to be intersected together.\n4.3 Image Enhancement Results\nFor LOL dataset[56], the compared methods include Zero-DCE[13],\nRetiNexNet[56], MBLLEN[42], DRBN[60], KIND++[68], RCT[24],\nIAT[7]. For MIT-Adobe FiveK dataset[2], the compared methods\ninclude White-Box[17], U-Net[49], DPE[6], DPED[21], D-UPE[53],\nD-LPF[46], STAR[70], IAT[7]. The quantitative comparisons with\nexisting methods are shown in Figure 11, which proves that our\nmethod also performs superior in low light scenes.\nFearless Luminance Adaptation: A Macro-Micro-Hierarchical Transformer for Exposure Correction MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\n            Input                         LIME                          RUAS                      ZeroDCE                          IAT                           MSEC                          LCDP                           Ours\nFigure 9: Detection results on DarkFace dataset.\n            Input                        Dannet                       ZeroDCE                          IAT                           MSEC                          LCDP                          Ours                             GT\nFigure 10: Segmentation results on ACDC-Nighttime dataset.\nTable 3: Performance comparisons on high-level tasks. We retrain the detector/segmentator in all cases containing the enhancer.\nTask Dark Face Detector Enhancer + Detector (Finetune)\nMethod HLA[54] REG[29] MEAT[8] LIME[14] ZeroDCE[13] MSEC[1] RUAS[40] LCDP[52] IAT[7] MMHT(Ours)\nmAP 0.607 0.514 0.526 0.644 0.665 0.659 0.642 0.654 0.663 0.675\nTask Nighttime Semantic Segmentator Enhancer + Segmentator (Finetune)\nMethod DANNet[59] CIC[27] GPS-GLASS[26] LIME[14] ZeroDCE[13] MSEC[1] RUAS[40] LCDP[52] IAT[7] MMHT(Ours)\nmIoU 0.398 0.264 0.380 0.447 0.452 0.449 0.448 0.455 0.456 0.461\n(a) PSNR & SSIM of LOL (b) PSNR & SSIM of MIT-Adobe FiveK\nFigure 11: Quantitative comparisons on LOL and MIT-5K.\n4.4 High-level Vision Applications\nTo demonstrate the generalizability of our method, we apply it\nto relevant high level vision tasks, including low-light face and\nlow-light semantic segmentation tasks.\nHere we adopt the DARKFACE[61] and ACDC[50] datasets for\nevaluating low-light detection and segmentation respectively. To\nfully evaluate its performance, we not only compare some of the\ncorrection methods, but also consider specific detection methods\nand segmentation methods. The quantitative comparisons with\nexisting methods are shown in Figure 9 and 10 while quantitative\nresults are in the Table 3. Both quantitatively and quantitatively\ndemonstrate the significance of our method for downstream tasks.\n4.5 Ablation Study\n4.5.1 Experiment on Transformer and Convolution Arrangement\nand the Effect of Laplacian Pyramid Decomposition. To explore the\noptimal arrangement of transformer and convolution, we design an\nablation study for the network used in each stage, with results in\nTable 4. As U-Net is progressively replaced by the MMT Restorer,\nthe quantitative results improve. However, there is a slight drop\nin PSNR and SSIM when the last network is replaced, as removing\nthe convolution entirely eliminates the inductive bias. Combining\nconvolution with transformers yields the better results, with R1,\nR2, R3 as the best pipeline. We also conduct ablation experiments\non Laplacian pyramid decomposition. As shown in the Table 4 and\nFigure 12(a), MMT Restorer performs well even without Laplacian\npyramid decomposition but compromises the coarse-to-fine correc-\ntion, negatively impacting results qualitatively and quantitatively.\n4.5.2 Experiments on the Effect of Macro-Micro Attention. Global\ninformation acquisition is crucial for addressing image exposure\ncorrection issues. However, the micro attention restricts operations\nto a single window and loses long-range dependencies. We intro-\nduce macro attention with a larger receptive field in combination\nwith micro attention to effectively acquire multi-scale information.\nAn ablation study is designed for macro-micro attention, with re-\nsults presented in Table 5. Replacing either micro or macro attention\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Gehui Li et al.\n            (a) w/o Lpls                 (b) w/o Micro Attention        (c) w/o Macro Attention                (d) w/o                                     (e) Ours                                  (f) GT\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\n+1.5 EV\n-1.5 EV\nFigure 12: Visualization results of ablation experiments are shown. The GT images are filled RGB curves while the ablation\nexperimental images are unfilled RGB curves.\nTable 4: Exploring Transformer and Convolution Arrange-\nment and the effect of Laplacian Pyramid Decomposition.\nR1 means replacing the U-Net Restorer with MMT Restorer\nin stage 1 of the pipeline.\nR1 R2 R3 R4 Lpls PSNR â†‘ SSIMâ†‘ Î”CFâ†“ FIDâ†“\n- - - - âœ— 22.785 0.853 2.119 10.331\nâœ— âœ— âœ— âœ— âœ“ 20.517 0.771 3.419 20.964\nâœ“ âœ— âœ— âœ— âœ“ 21.875 0.811 3.331 15.397\nâœ“ âœ“ âœ— âœ— âœ“ 22.269 0.831 2.798 13.778\nâœ“ âœ“ âœ“ âœ— âœ“ 23.049 0.865 2.202 10.223\nâœ“ âœ“ âœ“ âœ“ âœ“ 22.971 0.859 2.229 10.414\nwith the other results in a significant increase in FID and CF with a\ndecrease in PSNR and SSIM. Furthermore, swapping their order also\ninflicts a slight detriment to these metrics. In the Figure 12(c), we\nobserve that omitting macro attention severely impacts the results,\ncausing substantial color distortions and artifacts.\n4.5.3 Experiment on the Effect of Contrastive Regularization. To\nexamine the effect of contrastive loss, we remove the contrastive\nloss in the adjusting attention order. As shown in the Table 5, there\nis a significant reduction in the PSNR and SSIM. Figure 12(d) shows\nthat removing contrastive learning leads to inconsistent correction\nresults for different exposure inputs. Contrastive constraint encour-\nages the model to learn features distinguishing well-exposed im-\nages from exposure-error images. Using abundant negative samples\nforces the model to learn more discriminative features, resulting in\nconsistent exposure correction performance.\nTable 5: Exploring the effect of Macro-Micro Attention and\nContrastive Regularization.\nMic Mac Mic-Mac Mac-Micğ¿ğ¶ğ‘… PSNRâ†‘ SSIMâ†‘ Î”CFâ†“ FIDâ†“\nâœ“ âœ— âœ— âœ— âœ— 22.092 0.823 3.945 16.712\nâœ— âœ“ âœ— âœ— âœ— 22.325 0.836 2.616 12.301\nâœ— âœ— âœ“ âœ— âœ— 22.791 0.843 2.419 11.818\nâœ— âœ— âœ— âœ“ âœ— 22.901 0.851 2.305 10.597\nâœ“ âœ— âœ— âœ— âœ“ 22.225 0.835 3.978 16.645\nâœ— âœ“ âœ— âœ— âœ“ 22.494 0.846 2.511 12.219\nâœ— âœ— âœ“ âœ— âœ“ 22.949 0.862 2.415 11.774\nâœ— âœ— âœ— âœ“ âœ“ 23.049 0.865 2.202 10.223\nFirst Encoding Block Last Decoding Block\n      Macro                     Micro\nInput-N1.5\n      Macro                     MicroInput-P1.5\nOutput-N1.5\nOutput-P1.5\nFigure 13: Visualization of heat maps from the first encoding\nblock and last decoding block.\n4.6 Feature map visualization\nTo further explain the role of multiple attention and hierarchical\nnetwork structure, we visualize the feature map after processing\nof the first block of the encoder in stage 1 and the last block of the\ndecoder in stage 3. The results are presented in the Figure 13. Long-\nrange dependencies appear after macro attention, while short-range\ndependencies appear after micro attention. This aids the network\nin adapting different adjustments based on varying semantics.\n5 CONCLUSION\nIn this paper, we address the issues of inconsistent results and color\ndistortion in exposure correction tasks. To model long-range depen-\ndencies, a Macro-Micro attention mechanism has been developed,\nwhich significantly enhances image color and detail. A contrastive\nconstraint is then established to effectively resolve inconsistent cor-\nrection results. By integrating the advantages of both transformer\nand convolution, a four-stage pipeline is constructed, which further\nimproves the quality of corrected images. Our proposed model is\nassessed on multiple challenging exposure-error datasets and ap-\nplied as an enhancer for high-level tasks. Results demonstrate that\nour approach significantly outperforms state-of-the-art methods,\nindicating its potential applicability in exposure-related tasks.\nACKNOWLEDGMENTS\nThis work is partially supported by the National Key R&D Pro-\ngram of China (No. 2022YFA1004101), the National Natural Science\nFoundation of China (No. U22B2052).\nFearless Luminance Adaptation: A Macro-Micro-Hierarchical Transformer for Exposure Correction MM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada.\nREFERENCES\n[1] Mahmoud Afifi, Konstantinos G Derpanis, Bjorn Ommer, and Michael S Brown.\n2021. Learning multi-scale photo exposure correction. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 9157â€“9167.\n[2] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and FrÃ©do Durand. 2011. Learning\nphotographic global tonal adjustment with a database of input/output image\npairs. In CVPR 2011 . IEEE, 97â€“104.\n[3] Bolun Cai, Xianming Xu, Kailing Guo, Kui Jia, Bin Hu, and Dacheng Tao. 2017.\nA joint intrinsic-extrinsic prior model for retinex. In Proceedings of the IEEE\ninternational conference on computer vision . 4000â€“4009.\n[4] Jiawen Chen, Andrew Adams, Neal Wadhwa, and Samuel W Hasinoff. 2016.\nBilateral guided upsampling. ACM Transactions on Graphics (TOG) 35, 6 (2016),\n1â€“8.\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A\nsimple framework for contrastive learning of visual representations. In Interna-\ntional conference on machine learning . PMLR, 1597â€“1607.\n[6] Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, and Yung-Yu Chuang. 2018.\nDeep photo enhancer: Unpaired learning for image enhancement from pho-\ntographs with gans. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition . 6306â€“6314.\n[7] Ziteng Cui, Kunchang Li, Lin Gu, Shenghan Su, Peng Gao, Zhengkai Jiang, Yu\nQiao, and Tatsuya Harada. 2022. Illumination Adaptive Transformer. arXiv\npreprint arXiv:2205.14871 (2022).\n[8] Ziteng Cui, Guo-Jun Qi, Lin Gu, Shaodi You, Zenghui Zhang, and Tatsuya Harada.\n2021. Multitask aet with orthogonal tangent regularity for dark object detection.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision . 2553â€“\n2562.\n[9] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. 2021. Coatnet: Marrying\nconvolution and attention for all data sizes. Advances in Neural Information\nProcessing Systems 34 (2021), 3965â€“3977.\n[10] Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, RafaÅ‚ K Mantiuk, and Jonas\nUnger. 2017. HDR image reconstruction from a single exposure using deep CNNs.\nACM transactions on graphics (TOG) 36, 6 (2017), 1â€“15.\n[11] F Eyiokur, Dogucan Yaman, Hazim Kemal Ekenel, and Alexander Waibel. 2022.\nExposure Correction Model to Enhance Image Quality. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 676â€“686.\n[12] Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. 2016.\nA weighted variational model for simultaneous reflectance and illumination\nestimation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition. 2782â€“2790.\n[13] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam\nKwong, and Runmin Cong. 2020. Zero-reference deep curve estimation for low-\nlight image enhancement. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition . 1780â€“1789.\n[14] Xiaojie Guo. 2016. LIME: A method for low-light image enhancement. In Pro-\nceedings of the 24th ACM international conference on Multimedia . 87â€“91.\n[15] David Hasler and Sabine E Suesstrunk. 2003. Measuring colorfulness in natural\nimages. In Human vision and electronic imaging VIII , Vol. 5007. SPIE, 87â€“95.\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and\nSepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to\na local nash equilibrium. Advances in neural information processing systems 30\n(2017).\n[17] Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, and Stephen Lin. 2018.\nExposure: A white-box photo post-processing framework. ACM Transactions on\nGraphics (TOG) 37, 2 (2018), 1â€“17.\n[18] Jie Huang, Yajing Liu, Xueyang Fu, Man Zhou, Yang Wang, Feng Zhao, and\nZhiwei Xiong. 2022. Exposure Normalization and Compensation for Multiple-\nExposure Correction. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) . 6043â€“6052.\n[19] Jie Huang, Feng Zhao, Man Zhou, Jie Xiao, Naishan Zheng, Kaiwen Zheng, and\nZhiwei Xiong. 2023. Learning Sample Relationship for Exposure Correction. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). 9904â€“9913.\n[20] Jie Huang, Man Zhou, Yajing Liu, Mingde Yao, Feng Zhao, and Zhiwei Xiong.\n2022. Exposure-Consistency Representation Learning for Exposure Correction. In\nProceedings of the 30th ACM International Conference on Multimedia . 6309â€“6317.\n[21] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc\nVan Gool. 2017. Dslr-quality photos on mobile devices with deep convolutional\nnetworks. In Proceedings of the IEEE International Conference on Computer Vision .\n3277â€“3285.\n[22] Zhiying Jiang, Zhuoxiao Li, Shuzhou Yang, Xin Fan, and Risheng Liu. 2022.\nTarget oriented perceptual adversarial fusion network for underwater image\nenhancement. IEEE Transactions on Circuits and Systems for Video Technology 32,\n10 (2022), 6584â€“6598.\n[23] Feng Zhao Keyu Yan Jinghao Zhang Yukun Huang Man Zhou Zhiwei Xiong\nJie Huang, Yajing Liu. 2022. Deep Fourier-based Exposure Correction Network\nwith Spatial-Frequency Interaction. In Proceedings of the European Conference on\nComputer Vision (ECCV) .\n[24] Hanul Kim, Su-Min Choi, Chang-Su Kim, and Yeong Jun Koh. 2021. Represen-\ntative color transform for image enhancement. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision . 4459â€“4468.\n[25] Dokyeong Kwon, Guisik Kim, and Junseok Kwon. 2020. DALE: Dark region-aware\nlow-light image enhancement. arXiv preprint arXiv:2008.12493 (2020).\n[26] Hongjae Lee, Changwoo Han, and Seung-Won Jung. 2022. GPS-GLASS: Learning\nNighttime Semantic Segmentation Using Daytime Video and GPS data. arXiv\npreprint arXiv:2207.13297 (2022).\n[27] Attila Lengyel, Sourav Garg, Michael Milford, and Jan C van Gemert. 2021. Zero-\nshot day-night domain adaptation with a physics prior. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision . 4399â€“4409.\n[28] Jiawei Li, Jinyuan Liu, Shihua Zhou, Qiang Zhang, and Nikola K Kasabov. 2022.\nLearning a coordinated network for detail-refinement multiexposure image fu-\nsion. IEEE Transactions on Circuits and Systems for Video Technology 33, 2 (2022),\n713â€“727.\n[29] Jinxiu Liang, Jingwen Wang, Yuhui Quan, Tianyi Chen, Jiaying Liu, Haibin Ling,\nand Yong Xu. 2021. Recurrent exposure generation for low-light face detection.\nIEEE Transactions on Multimedia 24 (2021), 1609â€“1621.\n[30] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong,\nand Zhongxuan Luo. 2022. Target-aware dual adversarial learning and a multi-\nscenario multi-modality benchmark to fuse infrared and visible for object detec-\ntion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 5802â€“5811.\n[31] Jinyuan Liu, Xin Fan, Ji Jiang, Risheng Liu, and Zhongxuan Luo. 2021. Learning\na deep multi-scale feature ensemble and an edge-attention guidance for image\nfusion. IEEE Transactions on Circuits and Systems for Video Technology 32, 1 (2021),\n105â€“119.\n[32] Jinyuan Liu, Jingjie Shang, Risheng Liu, and Xin Fan. 2022. Attention-guided\nglobal-local adversarial learning for detail-preserving multi-exposure image\nfusion. IEEE Transactions on Circuits and Systems for Video Technology 32, 8\n(2022), 5026â€“5040.\n[33] Jinyuan Liu, Guanyao Wu, Junsheng Luan, Zhiying Jiang, Risheng Liu, and\nXin Fan. 2023. HoLoCo: Holistic and local contrastive learning network for\nmulti-exposure image fusion. Information Fusion 95 (2023), 237â€“249.\n[34] Jinyuan Liu, Yuhui Wu, Zhanbo Huang, Risheng Liu, and Xin Fan. 2021. Smoa:\nSearching a modality-oriented architecture for infrared and visible image fusion.\nIEEE Signal Processing Letters 28 (2021), 1818â€“1822.\n[35] Risheng Liu, Xin Fan, Ming Zhu, Minjun Hou, and Zhongxuan Luo. 2020. Real-\nworld underwater enhancement: Challenges, benchmarks, and solutions under\nnatural light. IEEE transactions on circuits and systems for video technology 30, 12\n(2020), 4861â€“4875.\n[36] Risheng Liu, Zhiying Jiang, Shuzhou Yang, and Xin Fan. 2022. Twin adversarial\ncontrastive learning for underwater image enhancement and beyond. IEEE\nTransactions on Image Processing 31 (2022), 4922â€“4936.\n[37] Risheng Liu, Zhouchen Lin, Fernando De la Torre, and Zhixun Su. 2012. Fixed-\nrank representation for unsupervised visual learning. In 2012 ieee conference on\ncomputer vision and pattern recognition . IEEE, 598â€“605.\n[38] Risheng Liu, Jinyuan Liu, Zhiying Jiang, Xin Fan, and Zhongxuan Luo. 2020.\nA bilevel integrated model with data-driven layer ensemble for multi-modality\nimage fusion. IEEE Transactions on Image Processing 30 (2020), 1261â€“1274.\n[39] Risheng Liu, Long Ma, Tengyu Ma, Xin Fan, and Zhongxuan Luo. 2022. Learning\nwith nested scene modeling and cooperative architecture search for low-light\nvision. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 5 (2022),\n5953â€“5969.\n[40] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongxuan Luo. 2021. Retinex-\ninspired unrolling with cooperative prior architecture search for low-light image\nenhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 10561â€“10570.\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision . 10012â€“10022.\n[42] Feifan Lv, Feng Lu, Jianhua Wu, and Chongsoon Lim. 2018. MBLLEN: Low-Light\nImage/Video Enhancement Using CNNs.. In BMVC, Vol. 220. 4.\n[43] Long Ma, Dian Jin, Nan An, Jinyuan Liu, Xin Fan, and Risheng Liu. 2023.\nBilevel Fast Scene Adaptation for Low-Light Image Enhancement. arXiv preprint\narXiv:2306.01343 (2023).\n[44] Long Ma, Risheng Liu, Yiyang Wang, Xin Fan, and Zhongxuan Luo. 2022. Low-\nlight image enhancement via self-reinforced retinex projection model. IEEE\nTransactions on Multimedia (2022).\n[45] Long Ma, Tianjiao Ma, Xinwei Xue, Xin Fan, Zhongxuan Luo, and Risheng Liu.\n2022. Practical Exposure Correction: Great Truths Are Always Simple. arXiv\npreprint arXiv:2212.14245 (2022).\n[46] Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, and Gregory\nSlabaugh. 2020. Deeplpf: Deep local parametric filters for image enhancement. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n12826â€“12835.\nMM â€™23, October 29â€“November 3, 2023, Ottawa, ON, Canada. Gehui Li et al.\n[47] Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, and Huchuan Lu. 2019. Depth-\ninduced multi-scale recurrent attention network for saliency detection. In Pro-\nceedings of the IEEE/CVF international conference on computer vision . 7254â€“7263.\n[48] Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren, and Huchuan Lu. 2020.\nA2dele: Adaptive and attentive depth distiller for efficient RGB-D salient object\ndetection. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition. 9060â€“9069.\n[49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional\nnetworks for biomedical image segmentation. In International Conference on\nMedical image computing and computer-assisted intervention . Springer, 234â€“241.\n[50] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2021. ACDC: The adverse\nconditions dataset with correspondences for semantic driving scene understand-\ning. In Proceedings of the IEEE/CVF International Conference on Computer Vision .\n10765â€“10775.\n[51] Wenzhe Shi, Jose Caballero, Ferenc HuszÃ¡r, Johannes Totz, Andrew P Aitken,\nRob Bishop, Daniel Rueckert, and Zehan Wang. 2016. Real-time single image and\nvideo super-resolution using an efficient sub-pixel convolutional neural network.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition .\n1874â€“1883.\n[52] Haoyuan Wang, Ke Xu, and Rynson WH Lau. 2022. Local color distributions prior\nfor image enhancement. In European Conference on Computer Vision . Springer,\n343â€“359.\n[53] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and\nJiaya Jia. 2019. Underexposed photo enhancement using deep illumination\nestimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 6849â€“6857.\n[54] Wenjing Wang, Xinhao Wang, Wenhan Yang, and Jiaying Liu. 2022. Unsupervised\nface detection in the dark. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 45, 1 (2022), 1250â€“1266.\n[55] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu,\nand Houqiang Li. 2022. Uformer: A general u-shaped transformer for image\nrestoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 17683â€“17693.\n[56] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. 2018. Deep retinex\ndecomposition for low-light enhancement.arXiv preprint arXiv:1808.04560 (2018).\n[57] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei\nZhang. 2021. Cvt: Introducing convolutions to vision transformers. InProceedings\nof the IEEE/CVF International Conference on Computer Vision . 22â€“31.\n[58] Jianlong Wu, Zhouchen Lin, and Hongbin Zha. 2019. Essential tensor learning\nfor multi-view spectral clustering. IEEE Transactions on Image Processing 28, 12\n(2019), 5910â€“5922.\n[59] Jun Xu, Yingkun Hou, Dongwei Ren, Li Liu, Fan Zhu, Mengyang Yu, Haoqian\nWang, and Ling Shao. 2020. Star: A structure and texture aware retinex model.\nIEEE Transactions on Image Processing 29 (2020), 5022â€“5037.\n[60] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. 2020. From\nfidelity to perceptual quality: A semi-supervised approach for low-light image\nenhancement. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition . 3063â€“3072.\n[61] Wenhan Yang, Ye Yuan, Wenqi Ren, Jiaying Liu, Walter J Scheirer, Zhangyang\nWang, Taiheng Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, et al. 2020. Advanc-\ning image understanding in poor visibility environments: A collective benchmark\nstudy. IEEE Transactions on Image Processing 29 (2020), 5737â€“5752.\n[62] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.\n2021. Incorporating convolution designs into visual transformers. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision . 579â€“588.\n[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shah-\nbaz Khan, and Ming-Hsuan Yang. 2022. Restormer: Efficient transformer for\nhigh-resolution image restoration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 5728â€“5739.\n[64] Haokui Zhang, Wenze Hu, and Xiaoyu Wang. 2022. EdgeFormer: Improving\nLight-weight ConvNets by Learning from Vision Transformers. arXiv preprint\narXiv:2203.03952 (2022).\n[65] Jinghao Zhang, Jie Huang, Mingde Yao, Man Zhou, and Feng Zhao. 2022.\nStructure- and Texture-Aware Learning for Low-Light Image Enhancement. In\nProceedings of the 30th ACM International Conference on Multimedia . 6483â€“6492.\n[66] Miao Zhang, Weisong Ren, Yongri Piao, Zhengkun Rong, and Huchuan Lu. 2020.\nSelect, supplement and focus for RGB-D saliency detection. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition . 3472â€“3481.\n[67] Qing Zhang, Ganzhao Yuan, Chunxia Xiao, Lei Zhu, and Wei-Shi Zheng. 2018.\nHigh-quality exposure correction of underexposed photos. In Proceedings of the\n26th ACM international conference on Multimedia . 582â€“590.\n[68] Yonghua Zhang, Xiaojie Guo, Jiayi Ma, Wei Liu, and Jiawan Zhang. 2021. Beyond\nbrightening low-light images. International Journal of Computer Vision 129 (2021),\n1013â€“1037.\n[69] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. 2019. Kindling the darkness: A\npractical low-light image enhancer. In Proceedings of the 27th ACM international\nconference on multimedia . 1632â€“1640.\n[70] Zhaoyang Zhang, Yitong Jiang, Jun Jiang, Xiaogang Wang, Ping Luo, and Jinwei\nGu. 2021. Star: A structure-aware lightweight transformer for real-time image en-\nhancement. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 4106â€“4115.\n[71] Naishan Zheng, Jie Huang, Feng Zhao, Xueyang Fu, and Feng Wu. 2022. Un-\nsupervised Underexposed Image Enhancement via Self-Illuminated and Per-\nceptual Guidance. IEEE Transactions on Multimedia (2022), 1â€“16. https:\n//doi.org/10.1109/TMM.2022.3193059",
  "topic": "Luminance",
  "concepts": [
    {
      "name": "Luminance",
      "score": 0.688883900642395
    },
    {
      "name": "Macro",
      "score": 0.6837420463562012
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5928986072540283
    },
    {
      "name": "Transformer",
      "score": 0.5912845134735107
    },
    {
      "name": "Computer science",
      "score": 0.5500704646110535
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37035682797431946
    },
    {
      "name": "Voltage",
      "score": 0.14109694957733154
    },
    {
      "name": "Electrical engineering",
      "score": 0.13850924372673035
    },
    {
      "name": "Engineering",
      "score": 0.1274529993534088
    },
    {
      "name": "Optics",
      "score": 0.12270006537437439
    },
    {
      "name": "Physics",
      "score": 0.09067261219024658
    },
    {
      "name": "Programming language",
      "score": 0.05623328685760498
    }
  ],
  "cited_by": 8
}