{
  "title": "Binary Black-box Evasion Attacks Against Deep Learning-based Static Malware Detectors with Adversarial Byte-Level Language Model",
  "url": "https://openalex.org/W3112370249",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222287049",
      "name": "Ebrahimi, Mohammadreza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1984185994",
      "name": "Zhang Ning",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Hu, James",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Raza, Muhammad Taqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1822436273",
      "name": "Chen, Hsinchun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2966708309",
    "https://openalex.org/W2904109097",
    "https://openalex.org/W2580662672",
    "https://openalex.org/W2943796454",
    "https://openalex.org/W2942795289",
    "https://openalex.org/W2945420045",
    "https://openalex.org/W2963408280",
    "https://openalex.org/W2785844809",
    "https://openalex.org/W2909060985",
    "https://openalex.org/W2973628901",
    "https://openalex.org/W2982725903",
    "https://openalex.org/W2887324343",
    "https://openalex.org/W2618219509",
    "https://openalex.org/W2963106521",
    "https://openalex.org/W3164220323",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2919491917",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W2911919851",
    "https://openalex.org/W3031166524",
    "https://openalex.org/W2784452215",
    "https://openalex.org/W2963382687",
    "https://openalex.org/W2808195131",
    "https://openalex.org/W2986189714",
    "https://openalex.org/W2746600820",
    "https://openalex.org/W2809895662",
    "https://openalex.org/W2963165251",
    "https://openalex.org/W2925469520",
    "https://openalex.org/W2932977083",
    "https://openalex.org/W2799420851"
  ],
  "abstract": "Anti-malware engines are the first line of defense against malicious software. While widely used, feature engineering-based anti-malware engines are vulnerable to unseen (zero-day) attacks. Recently, deep learning-based static anti-malware detectors have achieved success in identifying unseen attacks without requiring feature engineering and dynamic analysis. However, these detectors are susceptible to malware variants with slight perturbations, known as adversarial examples. Generating effective adversarial examples is useful to reveal the vulnerabilities of such systems. Current methods for launching such attacks require accessing either the specifications of the targeted anti-malware model, the confidence score of the anti-malware response, or dynamic malware analysis, which are either unrealistic or expensive. We propose MalRNN, a novel deep learning-based approach to automatically generate evasive malware variants without any of these restrictions. Our approach features an adversarial example generation process, which learns a language model via a generative sequence-to-sequence recurrent neural network to augment malware binaries. MalRNN effectively evades three recent deep learning-based malware detectors and outperforms current benchmark methods. Findings from applying our MalRNN on a real dataset with eight malware categories are discussed.",
  "full_text": "Binary Black-box Evasion Attacks Against Deep Learning-based Static Malware\nDetectors with Adversarial Byte-Level Language Model\nMohammadreza Ebrahimi,1 Ning Zhang,2 James Hu,1 Muhammad Taqi Raza,1 Hsinchun Chen1\n1Artiﬁcial Intelligence Lab, The University of Arizona\n2Covax Data Inc., Arizona\n{ebrahimi, jameshu, taqi, hsinchun}@email.arizona.edu; ning.zhang@covaxdata.com\nAbstract\nAnti-malware engines are the ﬁrst line of defense\nagainst malicious software. While widely used, feature\nengineering-based anti-malware engines are vulnerable\nto unseen (zero-day) attacks. Recently, deep learning-\nbased static anti-malware detectors have achieved suc-\ncess in identifying unseen attacks without requiring fea-\nture engineering and dynamic analysis. However, these\ndetectors are susceptible to malware variants with slight\nperturbations, known as adversarial examples. Gener-\nating effective adversarial examples is useful to re-\nveal the vulnerabilities of such systems. Current meth-\nods for launching such attacks require accessing either\nthe speciﬁcations of the targeted anti-malware model,\nthe conﬁdence score of the anti-malware response, or\ndynamic malware analysis, which are either unrealis-\ntic or expensive. We propose MalRNN, a novel deep\nlearning-based approach to automatically generate eva-\nsive malware variants without any of these restrictions.\nOur approach features an adversarial example gener-\nation process, which learns a language model via a\ngenerative sequence-to-sequence recurrent neural net-\nwork to augment malware binaries. MalRNN effec-\ntively evades three recent deep learning-based malware\ndetectors and outperforms current benchmark methods.\nFindings from applying our MalRNN on a real dataset\nwith eight malware categories are discussed.\nIntroduction\nMalware attacks pose a massive threat to the security of\ncompanies and individuals. The average annual cost of\nmalware attacks has increased to $2.6 million per mid-\nsized company worldwide (Bissell and Ponemon 2018).\nAnti-malware engines are essential to proactively prevent\nthese attacks (Tounsi and Rais 2018). Most anti-malware\nengines mainly rely on signature-based approaches that\nmatch manually-deﬁned patterns against known malicious\nﬁles (Anderson et al. 2018). The success of signature-based\nmethods signiﬁcantly depends on the quality and recency of\nthe pre-deﬁned rules that are often handcrafted by malware\nanalysts. While useful, signature-based engines suffer from\ntwo signiﬁcant deﬁciencies: ﬁrst, they could be ineffective\nin dealing with newly evolved variants of malware, and thus,\nCopyright © 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nvulnerable to ‘unseen’ variants known as zero days (Chen et\nal. 2019a); second, they rely on manually deﬁned rules that\ncannot keep up with the rapid evolution of malware variants.\nDue to the deﬁciencies of signature-based anti-malware\nengines, researchers have presented machine learning-based\nmalware detection. However classic machine learning algo-\nrithms often require manual feature engineering. Recently,\na new stream of Deep Learning (DL)-based malware detec-\ntor has emerged that can consume the whole raw malware\nbinary as input and extract the salient features automati-\ncally, without relying on manually deﬁned rules or feature\nengineering. As a result, successful DL-based anti-malware\nengines have emerged (Raff et al. 2018), (Fleshman et al.\n2019), (Krˇc´al et al. 2018). However, DL-based anti-malware\nengines have shown to be susceptible to small perturba-\ntions in their input, featured by automated attacks known\nas Adversarial Example Generation (AEG) (Demetrio et\nal. 2019). These attacks yield slightly perturbed malware\nvariants that can mislead the DL-based engines into miss-\nclassifying them as benign. Given the crucial role of anti-\nmalware in preventing cyber-attacks and improving the se-\ncurity posture of many organizations, there is a vital need\nto devise automatic ways to protect anti-malware engines\nagainst the AEG attacks.\nAlthough AEG can negatively affect the performance of\nDL-based engines, it can also be utilized to further im-\nprove their performance. Anti-Malware Evasion (AME) has\nemerged as a promising method to automate the AEG pro-\ncess for this purpose (Chen et al. 2019b). Malware variants\nthat successfully evade the DL-based malware detectors can\nbe employed in re-training and improving them. Moreover,\nverifying DL-based anti-malware engines against AEG is a\nviable defense mechanism (Goodfellow, McDaniel, and Pa-\npernot 2018). In effect, automatic emulation of AEG attacks\ncan help strengthen the ability of DL-based engines to detect\nmalware.\nAME methods often rely on additive approaches, which\ninject bytes into the malware binary, known as append at-\ntacks (Suciu, Coull, and Johns 2019). Append attacks are a\nnatural ﬁt for AME because they do not affect the function-\nality of the malware since their injected payload is not exe-\ncuted by the operating system and thus they do not interfere\nwith the malware execution (Castro, Biggio, and Dreo Ro-\ndosek 2019), (Suciu, Coull, and Johns 2019). Nevertheless,\narXiv:2012.07994v1  [cs.CR]  14 Dec 2020\ncurrent approaches for launching these attacks suffer from\ntwo major issues that limit their applicability. First, many\nattacks assume full knowledge about the anti-malware ar-\nchitecture, its parameters, or the conﬁdence level of the anti-\nmalware response. These assumptions do not apply to realis-\ntic attack scenarios in which the information is hidden from\nthe adversary (Hu and Tan 2018). Second, since they often\nrely on brute-force mechanisms to craft new malware vari-\nants, they require a high volume of appended bytes (i.e., pay-\nload) to evade the anti-malware engine (Suciu, Coull, and\nJohns 2019).\nDeep learning methods have shown promise in generat-\ning smaller and more effective perturbations (Kreuk et al.\n2018). Recently, among deep learning methods, deep lan-\nguage models have shown promise in malware analysis by\ntreating the malware binary sequence as characters in a writ-\nten language (Awad, Nassar, and Safa 2018). Generative Re-\ncurrent Neural Network (RNN) is a powerful architecture to\nlearn such language models (Mogren and Johansson 2019).\nMotivated by the importance of ﬁnding the vulnerabilities\nof current DL-based anti-malware engines, we propose a\nnew threat model that utilizes a novel RNN-based method\nto automatically construct adversaries for evading several\nDL-based anti-malware engines simultaneously. To this end,\nwe focus on how to automatically generate evasive malware\nsamples on a large scale. Our study offers a novel approach\nto directly learn a language model on binary executables\nand generate benign-looking content without requiring any\nknowledge of the targeted anti-malware. To our knowledge,\nthe proposed method contributes to the ﬁrst automated at-\ntack against DL-based anti-malware engines without these\nrestrictive assumptions. Furthermore, our approach does not\nrequire expensive dynamic malware analysis. To foster re-\nproducibility, we made the code and the dataset available to\nthe AI-enabled security research community on GitHub at\nhttps://github.com/johnnyzn/MalRNN.\nBackground and Related Work\nAdversarial Example Generation (AEG)\nDeep learning models have been recently shown to fail when\nan adversary carefully modiﬁes their input data with sub-\ntle perturbations. Adversarial examples are instances with\nmeticulous feature perturbations that can cause a target ma-\nchine learning model to make wrong decisions. Automati-\ncally crafting such instances by an adversary against a spe-\nciﬁc class of target machine learning models is an emerging\ntask in artiﬁcial intelligence, referred to as AEG (Goodfel-\nlow, McDaniel, and Papernot 2018). This concept of AEG\nthat we use in this study is not meant to be confused with\nAutomatic Exploit Generation). Verifying machine learning\nmodels against AEG is a crucial defense mechanism that\nnot only helps improve the resistance of these models, but\nalso provides insights for designing better machine learning\nmodels (Goodfellow, McDaniel, and Papernot 2018).\nDepending on the information available to the adversary\nfrom the targeted machine learning model, AEG is car-\nried out under four possible scenarios (Qiu et al. 2019;\nAnderson et al. 2018). In the ﬁrst scenario, known as a\nwhite-box attack, the adversary has full access to the struc-\nture and parameters of the attack target. The second AEG\nscenario is referred to as gray-box AEG and pertains to sit-\nuations in which the parameters of the attacked neural net-\nwork model are not available but the adversary has access to\nthe features that are important for decision making by tar-\nget classiﬁer. The third scenario, called black-box AEG, re-\nlates to when the adversary cannot access the model’s spec-\niﬁcation, features, or parameters; however, it can obtain a\nreal-valued feedback, also known as conﬁdence score, from\nthe attack target. Finally, binary black-box AEG applies to a\nblack-box scenario in which not only no a priori knowledge\nis assumed about the target, but also the adversary does not\nhave access to a real-valued feedback from the attack target.\nInstead, in binary black-box scenario, the adversary can only\nobserve a binary response associated with the success or fail-\nure of the crafted instance in evading the attack target. This\ntype of attack is also known as binary black box (Anderson\net al. 2018). Binary black-box AEG is the most restrictive\nand the most common scenario in real-world (Fleshman et\nal. 2019), since oftentimes the speciﬁcation and conﬁdence\nscore of the attack target are unknown.\nAnti-Malware Evasion\nConducting AEG in the Malware detection domain gives\nrise to anti-malware evasion (AME) attacks, a new stream\nof research that employs AEG to perturb malware sam-\nples and generate variants that evade anti-malware engines\nwhile still preserving the functionality of the original mal-\nware. AME attacks can be categorized based on the type of\nthreat model they implement (i.e., white, gray, black, and\nbinary black-box). Consistent with our goal of proposing\na more realistic AME attack scenario in our study, we ex-\namine the past AME studies that support black-box and bi-\nnary black-box attacks. Among these studies, AME stud-\nies that offer black-box attacks do not require knowing\nthe speciﬁcations of the targeted anti-malware (Demetrio\net al. 2020; Castro, Biggio, and Dreo Rodosek 2019;\nCastro, Schmitt, and Rodosek 2019; Chen et al. 2019a;\nPark, Khan, and Yener 2019; Suciu, Coull, and Johns 2019;\nHu and Tan 2018). These studies employ a wide range of\nmethods such as genetic algorithm (Demetrio et al. 2020),\nrandom perturbations (Castro, Schmitt, and Rodosek 2019;\nChen et al. 2019a), dynamic programming (Park, Khan, and\nYener 2019), and RNN (Hu and Tan 2018). However, these\nmethods heavily rely on the conﬁdence score feedback ob-\ntained from anti-malware engines to craft their perturba-\ntions. The conﬁdence score is a real value ranging between\n0 and 1, which indicates the probability that the input is mal-\nware. This value is interpreted as the conﬁdence of the de-\ncision made by the anti-malware engine. While black-box\nattacks are more realistic than white-box attacks, the conﬁ-\ndence score is internal to the anti-malware engine and thus\nnot visible to the adversary. This issue restricts the usability\nof these methods (Rosenberg et al. 2019).\nBinary black-box attacks, on the other hand, do not re-\nquire observing the anti-malware’s conﬁdence score (Dey\net al. 2019; Fang et al. 2019; Rosenberg et al. 2019;\nAnderson et al. 2018), and thus, are applicable to real at-\ntack scenarios. Nevertheless, most current binary black-box\nAME studies target signature-based anti-malware engines\n(Dey et al. 2019; Fang et al. 2019; Anderson et al. 2018).\nAlthough Rosenberg et al. (Rosenberg et al. 2019) pro-\npose a binary black-box attack on DL-based anti-malware\nengines, their approach requires an API call sequence ob-\ntained from expensive and time-consuming dynamic analy-\nsis of malware binary in a sandbox. We also note that Hu\nand Tan (Hu and Tan 2018) propose a black-box AME at-\ntack that is based on training an RNN. However, similar\nto (Rosenberg et al. 2019), their approach requires a se-\nquence of API calls obtained during the dynamic analy-\nsis in a sandbox. Another practical limitation of the cur-\nrent binary black-box AME methods relates to the brute-\nforce operationalization of append attacks, which requires\na large size of binary content to be appended to the orig-\ninal malware ﬁle (e.g., three times larger than the size\nof the original malware) (Suciu, Coull, and Johns 2019;\nCastro, Schmitt, and Rodosek 2019). This results in generat-\ning abnormally large malware variants that can be detected\nby anti-malware engines due to the suspicious size of the\nresulting malware variant. Furthermore, most AME studies\non attacking DL-based anti-malware engines are designed to\nonly target a speciﬁc anti-malware architecture with certain\nparameter settings. Focusing on evading one speciﬁc archi-\ntecture limits the generalizability of such methods to other\nanti-malware models. We expect that learning a universal\nlanguage model from benign executables can facilitate at-\ntaining more generalizable AME methods.\nGenerative RNN-based Language Models\nConstructing a language model amounts to learning a prob-\nability distribution over a sequence of strings or characters.\nOnce learned, a language model can be used to generate the\nnext element in a given sequence. Neural language models\nwith recurrent architectures have shown promise in gener-\nating high-quality sequences in Natural Language Process-\ning (NLP) tasks (Kim et al. 2016). A generative RNN pro-\ncesses sequential input while preserving temporal patterns\nin the sequence. At each time step t, an RNN takes an in-\nput xt and the current hidden state ht to emit a continu-\nous value. This value is used to generate/predict future el-\nements in the sequence. This generative nature of RNNs\nmakes them suitable for sequence analysis tasks such as\nlanguage modeling (Belletti, Chen, and Chi 2019), where\nthe elements of the input are time-dependent. Once trained,\nRNNs yield effective language models on short natural lan-\nguage text and binary content (Zuo et al. 2018) that are\nable to predict the next element based on a given input se-\nquence. Two major challenges arise in utilizing RNN-based\nlanguage models on malware content. First, using RNN lan-\nguage models for learning long sequences of malware con-\ntent is challenging (Raff et al. 2018) due to the large num-\nber of time steps, which leads to the attenuation of the er-\nror signal during training, widely known as vanishing gra-\ndient problem (Goldberg 2017). Adding gating mechanism\nto the input and output of RNN units can address this is-\nsue and yields an effective variant of RNN, Gated Recur-\nrent Units (GRU) (Goldberg 2017). Generative RNNs re-\nquire the input and output sequences to have the same di-\nmensions. While this is useful in machine learning tasks\nsuch as part of speech tagging, it limits their applicability\nin the malware domain. Among RNN-based architectures\nfor language modeling, sequence-to-sequence models ad-\ndress this issue by adding an additional encoding step be-\nfore feeding the data to the generative RNN. Sequence-to-\nsequence models have recently yielded breakthrough results\nin many sequence analysis tasks such as machine transla-\ntion (Ono, Utiyama, and Sumita 2019) and speech recogni-\ntion (Irie et al. 2019). They can map the input sequence of\na ﬁxed length to a generated output sequence of a different\nlength. Given their recent success in other machine learning\nﬁelds, we expect that sequence-to-sequence RNN-based lan-\nguage models can provide an effective tool to automatically\ngenerate benign-looking adversarial examples for AME ap-\nplications. Accordingly, we propose to construct an RNN\nlanguage model directly on the binary content (as opposed\nto the sequence of API calls in a sandbox) to accomplish ad-\nversarial malware generation in a binary black-box scenario\nwithout requiring dynamic analysis.\nProposed Method (MalRNN )\nAs noted in Section 2, most black-box AME methods rely\non a brute-force approach in which they inject bytes into a\nmalware sample until the generated variant evades the anti-\nmalware. The brute-force property of these methods leads to\ncrafting variants with large payload size that renders AME\nless effective. This issue motivates a threat model that lim-\nits the volume of injected bytes, as opposed to the one that\nallows adding an indeﬁnite length of perturbations.\nThreat Model\nConsistent with (Anderson et al. 2018), we deﬁne the threat\nmodel for launching binary black-box AME attacks against\nstatic anti-malware models. Nevertheless, unlike the threat\nmodel proposed in (Anderson et al. 2018), which targets\nfeature-based anti-malware engines, our threat model fo-\ncuses on launching attacks against DL-based anti-malware\nengines. Three major components of our threat model are:\n• Adversary’s Goal: Automatically crafting mal-\nware variants that are capable of evading DL-based anti-\nmalware.\n• Adversary’s Knowledge: The structure and pa-\nrameters of the anti-malware model are unknown to the\nadversary. Furthermore, the adversary does not have ac-\ncess to the conﬁdence score produced by anti-malware.\nThe only information available to the adversary is whether\nthe generated malware variant can evade the anti-malware\nor not.\n• Adversary’s Capability: Applying functional-\nity preserving append modiﬁcations on malware binary,\nwhile the maximum modiﬁcation size is limited. We fo-\ncus on append modiﬁcations, since they very often do not\ninterfere with the functionality of the malware.\nTo realize this threat model, we propose MalRNN, a byte-\nlevel sequence-to-sequence generative model that learns a\nFigure 1: Abstract view of MalRNN malware evasion architecture\nlanguage model on benign samples and injects benign-\nlooking byte sequences into the original malware binary in\norder to obtain evasive malware variants.\nMalRNN Design\nIn accordance with the above threat model, it can be ex-\npected that that mimicking the patterns of benign executa-\nbles could be a viable attack approach. We incorporate\nthis insight into our design of MalRNN. Speciﬁcally, this\nis achieved through learning a language model on bytes\nthat can generate benign-looking samples. Such a language\nmodel signiﬁcantly contributes to alleviating brute-force\ntrial and error for generating evasive variants. Figure 1 il-\nlustrates the major components of our MalRNN malware\nevasion architecture. We describe each component in the re-\nmaining of this section.\nData Acquisition\nDeveloping MalRNN requires two datasets of binary exe-\ncutables: 1) a malware executable dataset that serves as the\ninitial seed to generate evasive malware variants, and 2) a\nbenign executable dataset to train the language model. To\nobtain the former dataset, we compiled an up-to-date collec-\ntion with recent real malware samples from the last three\nyears. The dataset includes over 6,000 malware binaries\nfrom eight common malware categories. The distribution of\nthe dataset is described later. To obtain the benign executable\ndataset, following (Raff et al. 2018), we collected 4,329\nbenign executables from a clean installation folder of Mi-\ncrosoft Windows. In both datasets, we converted the binary\ninput to hexadecimal characters suitable for processing by a\ncharacter-level language model. Furthermore, to avoid inef-\nﬁcient training with long input byte sequences in malicious\nand benign executables, we employed systematic sampling.\nThis process samples the input binary sequence in ﬁxed in-\ntervals to reduce the input size for generative sequence-to-\nsequence RNN language model.\nGenerative Sequence-to-Sequence RNN Language\nModel\nOur model employs a character-level sequence-to-sequence\nRNN to learn a language model from benign malware bi-\nnaries. We adopt Gated Recurrent Units (GRU) (Goldberg\n2017) as the building block of our sequence-to-sequence\nmodel to alleviate the gradient vanishing problem in pro-\ncessing long sequences,(Dey and Salemt 2017). MalRNN\naims to maximize the adversarial loss (Madry et al. 2017)\nof the anti-malware model, which is formulated in the fol-\nlowing equation:\nmaximize\nδ∈∆\nL(Hθ(x+ δ),y) (1)\nwhere xis the input malware sample, Hθ is the attacked\nDL-based anti-malware model parameterized by θ, and δ ∈\n∆ denotes the allowable perturbations that preserve func-\ntionality (appending byte sequences in our case). The loss\nfunction L represents binary cross-entropy loss in most DL-\nbased anti-malware engines. However, in a (binary) black-\nbox setting the exact loss function from the anti-malware\nis not accessible and thus, cannot be incorporated into the\nmodel’s loss. Accordingly, directly maximization of Eq. 1\nis impractical. The key idea behind our model is that maxi-\nmizing the loss in Eq. 1 translates to minimizing the loss of\nan adversarial model in generating benign-looking samples\nthat can bypass the anti-malware. The middle box in Fig-\nure 1 shows our character-level generative RNN for learning\nsuch an adversarial model serving as a binary content gener-\nator.\nInspired by the recent sequence-to-sequence RNN in lan-\nguage modeling, MalRNN ’s generator consists of two main\nRNN components: An encoder RNN and a decoder RNN.\nThe encoder aims to encapsulate the salient features of the\ninput byte sequence into a feature vector. This vector is ob-\ntained from the ﬁnal hidden state of the last RNN unit in the\nencoder architecture and is fed to the decoder RNN (shown\nin the vertical inner box in Figure 1).\nThe encoder’s current hidden states ht is obtained as a\nfunction of both its previous stateht−1 and the current input\nelement xt. More formally, ht is given by Equation 2:\nht = f(Whht−1 + Wxxt) (2)\nwhere Wh denotes the network weights between the hidden\nunits and and Wx represents the network weights between\nhidden units and the input elements. Function f is a non-\nlinear activation such as tanh(.). The decoder receives the\nfeature vector from the encoder and reconstructs the byte\nsequence that minimizes a cross-entropy loss between the\ngenerated bytes and benign samples ( LRNN) at each time\nstep. Unlike the encoder, the decoder’s hidden state at each\ntime step is only a function of the previous hidden state and\nis given by Equation 3:\nht = f(Whht−1) (3)\nAfter training is complete, the generator learns to append\nbenign-looking binary content to the malware binary in or-\nder to maximize the adversarial loss and construct an evasive\nmalware variant. To craft a candidate malware variant, after\ncompletion of each training iteration, the generated byte se-\nquence from MalRNN’s generator is attached to the original\nmalware. The candidate malware variant is checked against\none or more black-box anti-malware models to assess if it\ncan evade them. The output of the anti-malware is a binary\noutput with 1 and 0 denoting detection and evasion, respec-\ntively. If the generated malware variant successfully evades\nthe detectors, the candidate sample is saved as an evasive\nvariant and will be further processed for ensuring its func-\ntionality.\nSuch a model is suitable for launching binary black-box\nattacks described in our threat model since it depends neither\non the gradients obtained from a differentiable anti-malware\nmodel, as in (Castro, Biggio, and Dreo Rodosek 2019;\nKolosnjaji et al. 2018), nor on the conﬁdence score received\nfrom the anti-malware engine, as in (Chen et al. 2017). This\namounts to achieving an adversary that is agnostic to the tar-\ngeted anti-malware’s deep learning architecture. It is worth\nnoting that, following (Anderson et al. 2018), in order to\ncomply with the binary black-box attack scenario, the con-\nﬁdence score provided by anti-malware architectures was\nmasked to mimic a binary output from anti-malware.\nIn each iteration, MalRNN is trained on a sample of be-\nnign executables and generates a byte sequence that is ap-\npended to the end of the original anti-malware to form a\nnew variant, which subsequently is tested against the tar-\ngeted black-box anti-malware models. In each iteration, the\nRNN is trained on a sample of benign ﬁles, and generates\nthe new bytes based on a given malware sequence. This pro-\ncess repeats until the new variant evades the anti-malware\nor the maximum number of attempts is reached. In case the\nmaximum number of attempts for a speciﬁc input sample is\nreached the model proceeds to the next malware sample.\nWe implemented MalRNN using PyTorch. MalRNN was\nrun on a single Nvidia RTX 2080 GPU with 4,352 CUDA\ncores and 8 GB internal memory. The code is designed to\nrun on both GPU and CPU environments. The data com-\nprises the full testbed including the benign executables for\ntraining the language model and also the malware binary\ndataset. MalrRNN’s speciﬁcations, including the architec-\nture and (hyper) parameter settings are given in Appendix\nA.\nEnsuring the Functionality of Generated Malware\nVariants\nWe used VirusTotal’s API, which supports large-scale mal-\nware analysis, for assessing the functionality of malware\nsamples after modiﬁcation. VirusTotal provides a malware\nbehavior report that includes static and dynamic analysis\nof the malware sample. These reports describe network be-\nhavior, ﬁle access behavior, etc. Using the VirusTotal API,\nwe compare the behavior reports for the modiﬁed evasive\nvariants and original (i.e., unmodiﬁed) malware samples.\nThrough this process, we ensure that the key parts of the\nVirus Total’s report stay the same after modiﬁcation, show-\ning that the modiﬁed malware samples can be executed on\nthe operating system and are fully functional. All 6,037 mal-\nware samples in our dataset were checked to be functional\nafter appending bytes to their overlay. That is, the non-\nfunctional samples in the original dataset (more than 90%)\nwere excluded from the evaluation.\nImplementation and Evaluation\nTestbed and Evaluation Criterion\nWe obtained an academic license of VirusTotal and ex-\ntracted 6,307 recent malware binaries from the past three\nyears (2017-2019) in eight categories, including botnet, ran-\nsomware, spyware, adware, virus, dropper, backdoor, and\nrootkit. Table 1 shows the distribution of the dataset by mal-\nware category. To be able to gain insight into each speciﬁc\nmalware category, we evaluate MalRNN ’s performance on\neach category separately. Utilizing the functionality assess-\nment process described earlier, we checked the functionality\nof all modiﬁed malware binary samples to ensure they retain\ntheir functionality after modiﬁcation.\nTable 1: Breakdown of testbed based on different malware\ncategories\nMalware\nCategory\nExamples # of Malware\nSamples\nAdware eldorado, razy, gator 1,947\nBackdoor lunam, rahack, symmi 678\nBotnet virut, salicode, sality 526\nDropper dunwod, gepys, doboc 904\nRansomware vtﬂooder, msil, bitman 900\nRootkit onjar, dqqd, shipup 53\nSpyware mikey, qqpass, scar 640\nVirus nimda, shodi, hematite 659\nTotal All subtypes 6,307\nAs our attack target, we selected three renowned DL-\nbased static malware detectors. All three are cited frequently\nby security researchers and are made available by authors\nthrough GitHub repositories.\n• MalConv (Raff et al. 2018), is among the most suc-\ncessful DL-based malware detectors, developed through\na collaboration between the Laboratory for Physical Sci-\nences (LPS) and NVIDIA. The model incorporates a deep\nconvolutional neural network architecture that is trained\non approximately half a million malware binaries and\nachieves an area under the ROC curve (AUC) of 98.5%\non an unseen test set.\n• NonNeg (Fleshman et al. 2019) is a successor of Mal-\nConv developed by LPS, which modiﬁes MalConv’s\narchitecture with non-negative weight constraints. The\nmodel was trained on 2 million malware binaries and ob-\ntained the AUC of 95.3% on a holdout sample.\n• ConvNet (Krˇc´al et al. 2018) was developed by Avast re-\nsearch group and features a deeper neural network than\nMalConv and NonNeg, with a total of eight layers. It was\ntrained on 20 million proprietary malware samples from\nAvast and achieved 70.4% AUC.\nBoth MalConv and NonNeg were featured as recent mal-\nware detector architectures in an AME competition hosted\nby Endgame in 2019 (Anderson 2019). It is important to\nnote that all malware samples in our dataset were recog-\nnized as malware by all three anti-malware models. Follow-\ning (Fleshman et al. 2019; Anderson et al. 2018), we adopt\nevasion rate as our evaluation criterion. The evasion rate of\nan AME method against a given anti-malware is deﬁned as\nfollows:\nEvasionRate = |E∩ F|\nN (4)\nwhere E and F denote the sets of evasive and functional\nmodiﬁed malware obtained from the AME method, respec-\ntively.Ndenotes the total number of malware samples given\nas input to the AME method. This statistic yields the efﬁ-\ncacy of a given AME method in evading a malware detector.\nWe use this metric to evaluate MalRNN against other bench-\nmark methods later in this section.\nExperiment Setup\nWe conduct three different experiments. In the ﬁrst experi-\nment, we examine the number of attempts MalRNN requires\nto generate evasive variants. In the second experiment, we\nmeasure the changes of MalRNN’s performance by vary-\ning the append size for each malware category. Finally, in\nthe third experiment, we compare MalRNN ’s performance\non all three malware detectors to that of other AME bench-\nmarks for a ﬁxed append volume (determined in our second\nexperiment). For comparison, we identiﬁed two state-of-the-\nart binary black-box and one black-box AME benchmarks:\n• Random Append (RA) (Suciu, Coull, and Johns\n2019; Castro, Schmitt, and Rodosek 2019): Appends se-\nquences of random bytes to the end of a malware sample\nuntil the evasion occurs.\n• Benign Append (BA) (Castro, Biggio, and Dreo Ro-\ndosek 2019): Appends random sections from benign ﬁles\nto the end of a malware sample until evasion occurs.\n• Enhanced Benign Append (EBA) (Chen et al.\n2019a): Appends speciﬁc byte sequences that lower the\nconﬁdence score of the anti-malware in a brute-force\nmanner.\nIt is worth noting that since EBA requires access to the\nconﬁdence score, it is qualiﬁed as black-box and has an un-\nfair advantage compared to the other two benchmarks and\nour proposed method. The following subsections describe\neach experiment and its corresponding results in detail.\nCan MalRNN Learn to Generate Evasive Variants?\nIt is often desirable to verify if a machine learning model\nlearns during training by monitoring the training loss or\nnumber of iterations required to solve the problem at hand.\nIn order to assess whether MalRNN learns to generate eva-\nsive bytes, we monitor the number of attempts (i.e, itera-\ntions) required for evasion during the training of MalRNN\n(Figure 2).\nFigure 2: Running average of the number of iterations re-\nquired to bypass the anti-malware engine for each sample.\nAs seen in Figure 2, when training starts MalRNN needs\naround 20 attempts to modify a given malware sample such\nthat it can evade the anti-malware. However, as the training\nproceeds, this number signiﬁcantly decreases. As a result,\nat the latest stages of training (after processing almost 300\nmalware samples) the number of required attempts reduces\nto around eight. This behavior is consistent among all eight\ncategories and suggests that MalRNN improves during the\ntraining process and learns to generate evasive content.\nHow Does the Append Size Affect the Evasion\nRate?\nAs noted, very large append sizes can defeat the purpose of\ndeveloping an effective AME method that is able to accom-\nplish an evasion attack through minimal modiﬁcation of the\noriginal malware. As such, in practice, it is crucial to limit\nthe maximum append size of AME methods. To empirically\nobserve the effect of append size on the evasion rate, we\ntrack the changes in evasion rate for various append sizes in\nthe virus category as it is one of the most damaging malware\ntypes. Table 2 summarizes the results.\nTable 2: Evasion rates and number of required training iter-\nations obtained at different append sizes\nA VG\nAppend\nSize (%)\nA VG\nAppend\nSize (KB)\n# of\nEvaded\nSamples\nEvasion\nRate\n# of\nTraining\nIterations\n5 7.5 763 82.4% 14,357\n10 15 862 93.09% 8,588\n20 30 882 95.25% 5,133\n40 66 906 97.84% 3,169\n80 132.8 910 98.27% 3,065\n100 166 912 98.49% 3,205\n120 199.2 919 99.24% 2,840\n180 298.8 921 99.46% 2,406\nTwo major observations are made from Table 2. First, it is\nseen that by appending only 7.5 KB on average to an origi-\nnal malware binary, MalRNN is able to achieve the evasion\nrate of 82.4%. This speaks to the effectiveness of the bytes\ngenerated by the proposed method, as will be thoroughly\ninvestigated in our third experiment. Second, and more im-\nportantly, as the append size increases from 5% to 40% of\nthe original malware size, the evasion rate rapidly increases\nto 97.84%. After this point, the rate of increase almost stabi-\nlizes. Also, the total number of training iterations required to\nevade the anti-malware decreases and exhibits the same be-\nhavior at 40% append size. Figure 3 visualizes this behavior\nby plotting the evasion rate against the changes in append\nsize.\nFigure 3: Evasion rate vs. append volume\nThe 40% append size has been shown as an elbow point,\nwhich denotes where the evasion rate stops to increase sig-\nniﬁcantly. We thus ﬁx the append size for all methods in\nthe benchmark evaluations to 40%. Although our proposed\nmodel yields satisfactory results at much lower append sizes\n(i.e., 5% and 10%), we selected 40% append size in favor\nof the benchmark methods involved in our third experiment.\nMoreover, even though our model implements the black-box\nthreat model, the amount of bytes it appends are compara-\nble to white-box gradient-based attacks in (Kolosnjaji et al.\n2018), which is around 1% to achieve 60-70% evasion rate.\nHow Does MalRNN Compare to the\nState-of-the-Art Black-Box AME Benchmarks?\nWe conduct four benchmark evaluations, each focusing on\nspeciﬁc malware detectors. The ﬁrst three evaluations com-\npare MalRNN ’s ability to conduct targeted AME attacks\non a speciﬁc DL-based malware detector (i.e., MalConv,\nNoNeg, or ConvNet) individually. The last benchmark eval-\nuation targets MalRNN ’s capability to evade all three anti-\nmalware engines simultaneously. That is, the evasion occurs\nonly if the variant can successfully evade all three malware\ndetectors. Such a benchmark evaluation allows us to verify\nMalRNN ’s generalizability to different DL-based models.\nTo provide evasion rates speciﬁc to each category, we con-\nducted benchmark evaluations separately on each malware\ncategory. Table 3 summarizes the results of all four bench-\nmark evaluations.\nFrom Table 3, it is observed that MalRNN outperforms\nall other AME benchmarks in almost all of the categories for\nall three malware detectors. Interestingly, not only does Mal-\nrNN outperform its binary black-box AME counterparts, but\nit also outperforms EBA, which has access to the conﬁ-\ndence scores, with the exception of adware and ransomware\nfor evading MalConv. In addition to comparison with AME\nbenchmarks, it is helpful to measure the performance of\nMalRNN on evading all three DL-based malware detectors\nacross all eight malware types. Figure 4 illustrates our Mal-\nRNN ’s evasion rate for collectively evading all three mal-\nware detectors for each malware type.\nFigure 4: MalRNN ’s averaged evasion rate against three\nDL-based malware detectors across eight malware types\nAs shown in Figure 4, ransomware and botnet have the\nlowest overall evasion rate with 29.33% and 23.57%, respec-\ntively, which may suggest that these categories are less sen-\nsitive to AME append attacks. This could be attributed to the\nfact that ransomware binaries have signiﬁcant sections ded-\nicated to data encryption routines, which could be uniquely\ndistinguished with DL-based classiﬁers. Similarly, botnet bi-\nnaries are often unique in the sense that they incorporate\na considerable amount of code devoted to establishing and\nmaintaining the network of malicious devices on the inter-\nnet. Such unique characteristics can render adversarial mod-\niﬁcations less effective in causing these types of malware\nTable 3: Comparing MalRNN ’s performance on three renowned DL-based anti-malware detectors with black-box AME bench-\nmark methods across eight malware categories\nDetector Method Adware Backdoor Botnet Dropper Ransomware Rootkit Spyware Virus Average\nMalconv\nRA 14.34% 9.88% 8.56% 14.16% 11.78% 13.21% 10.16% 11.53% 12.29%\nBA 49.15% 41.30% 20.34% 41.92% 38.44% 11.32% 35.31% 28.22% 39.43%\nEBA 75.55% 68.29% 46.58% 69.69% 80.22% 56.60% 65.31% 61.76% 69.54%\nMalRNN 68.75% 72.72% 53.66% 90% 64.28% 69.23% 80% 85.71% 73.24%\nNonNeg\nRA 0.67% 0.44% 0.19% 1.00% 0.44% 5.66% 0.63% 0.76% 0.67%\nBA 96.61% 99.41% 99.05% 94.91% 99.00% 90.57% 93.91% 88.47% 96.04%\nEBA 96.10% 94.40% 95.25% 98.78% 96.56% 100% 94.38% 89.38% 95.45%\nMalRNN 99.87% 100% 100% 100% 99.87% 100% 100% 100% 99.97%\nConvNet\nRA 30.71% 25.96% 69.01% 26.77% 10.67% 16.98% 46.88% 54.17% 33.95%\nBA 33.23% 27.43% 66.16% 35.62% 17.67% 35.85% 47.03% 49.92% 36.64%\nEBA 38.46% 35.29% 43.75% 47.83% 24.00% 45.28% 46.3% 51.22% 40.03%\nMalRNN 76.49% 100% 87.1% 69.23% 35.56% 64.15% 73.8% 70.59% 72.03%\nAll Three\nRA 0.00% 0.00% 0.00% 0.55% 0.00% 1.89% 0.00% 0.00% 1.49%\nBA 15.56% 14.31% 5.30% 5.63% 9.33% 5.66% 3.75% 0.00% 8.51%\nEBA 23.52% 23.15% 20.53% 19.58% 23.44% 15.09% 34.69% 22.91% 22.86%\nMalRNN 34.77% 54.28% 23.57% 46.57% 29.33% 41.51% 45.47% 34.75% 38.78%\nto evade. On the contrary, it is also observed that backdoor\nand dropper with 54.28% and 46.57%, respectively, have the\nhighest evasion rate. This suggests that, overall, DL-based\nanti-malware models may be more susceptible to modiﬁ-\ncations of backdoor and dropper samples. This aligns with\nthe fact that backdoor samples often contain malicious bi-\nnary that is embedded into a variety of benign programs to\nbypass regular authentication and provide remote unautho-\nrized access to a system. As a result, their content may be\nsimilar to non-detrimental content that are more likely to\nevade the DL-based anti-malware models. Similarly, drop-\npers are also benign-looking malicious tools that are de-\nsigned to embed other hidden malicious code (e.g., virus)\nto bypass anti-malware engines. Consequently, both back-\ndoor and dropper malware types are difﬁcult to identify for\nDL-based anti-malware models that operate on the entire bi-\nnary content with large portions of benign code. This ﬁnding\naligns with the intuition that crafting adversarial examples\nfor malware executables that are already embedded in be-\nnign executables could be less difﬁcult than other malware\ncategories with larger portions of conspicuously malicious\ncontent (e.g., botnet and ransomware).\nConclusion and Future Work\nRecently, static DL-based malware detectors have shown\npromise in detecting unseen malware without manual rule\ndeﬁnition and feature engineering. However, they can them-\nselves be vulnerable to AME attacks. We can strengthen\nthese anti-malware engines by emulating AME attacks. Au-\ntomating this process is crucial for improving anti-malware\nengines at a higher pace. Current approaches to this end un-\nrealistically assume full or partial knowledge about targeted\nanti-malware. In this study, by treating adversarial malware\ngeneration as language modeling, we developed a novel\nmethod, MalRNN, to craft adversarial examples without re-\nquiring any knowledge of the targeted anti-malware. Mal-\nRNN directly learns a language model on binary executables\nand generates effective benign-looking byte sequences that\ncan evade several DL-based anti-malware models simulta-\nneously. MalRNN neither depends on the gradients of a dif-\nferentiable anti-malware model, nor on the conﬁdence score\nreceived from the anti-malware engine. The results signify\nthe vulnerability of DL-based anti-malware models to ad-\nversarial append attacks and reveal that signiﬁcant future re-\nsearch in this area is needed. Future research is needed for\ndevising more sophisticated AME methods. One promising\ndirection is extending the perturbations from append attacks\nto editing modiﬁcations to help provide more powerful AME\nmethods. However, it should be noted that it is often harder\nto ensure the functionality of the malware variants obtained\nfrom editing modiﬁcations as opposed to additive ones.\nDue to nature of our study, its dual use is crucial to attend\nto. MalRNN contributes to emulating adversarial attacks as\na viable defense mechanism to gain insight on the adver-\nsary’s capabilities. Though the ultimate goal of our study\nis reinforcing the robustness of anti-malware engines, pre-\ncautionary measures should be taken to monitor and prevent\nlarge scale misuse of such AI techniques during the deploy-\nment of technology. Software-as-service deployment is one\nway to provide the monitoring so that the benevolent usage\nof the technology outweighs its malicious usage.\nAcknowledgments\nWe would like to thank Hyrum Anderson from Microsoft for\nvaluable discussions and feedback. We also thank VirusTo-\ntal for providing the malware dataset and granting access to\nthe APIs for functionality assessment. This material is based\nupon work supported by the National Science Foundation\n(NSF) under the grants SaTC-1936370, CICI-1917117, and\nSFS-1921485.\nReferences\n[Anderson et al. 2018] Anderson, H. S.; Kharkar, A.; Filar,\nB.; Evans, D.; and Roth, P. 2018. Learning to evade static pe\nmachine learning malware models via reinforcement learn-\ning. arXiv preprint arXiv:1801.08917.\n[Anderson 2019] Anderson, H. S. 2019. Machine Learning\nStatic Evasion Competition.\n[Awad, Nassar, and Safa 2018] Awad, Y .; Nassar, M.; and\nSafa, H. 2018. Modeling malware as a language. In 2018\nIEEE International Conference on Communications (ICC) ,\n1–6. IEEE.\n[Belletti, Chen, and Chi 2019] Belletti, F.; Chen, M.; and\nChi, E. H. 2019. Quantifying long range dependence in\nlanguage and user behavior to improve rnns. In Proceed-\nings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 1317–1327.\n[Bissell and Ponemon 2018] Bissell, K., and Ponemon, L.\n2018. Annual cost of cybercrime study: Unlocking the value\nof improved cybersecurity protection.\n[Castro, Biggio, and Dreo Rodosek 2019] Castro, R. L.;\nBiggio, B.; and Dreo Rodosek, G. 2019. Poster: Attack-\ning malware classiﬁers by crafting gradient-attacks that\npreserve functionality. In Proceedings of the 2019 ACM\nSIGSAC Conference on Computer and Communications\nSecurity, 2565–2567.\n[Castro, Schmitt, and Rodosek 2019] Castro, R. L.; Schmitt,\nC.; and Rodosek, G. D. 2019. Armed: How automatic\nmalware modiﬁcations can evade static detection? In 2019\n5th International Conference on Information Management\n(ICIM), 20–27. IEEE.\n[Chen et al. 2017] Chen, P.-Y .; Zhang, H.; Sharma, Y .; Yi, J.;\nand Hsieh, C.-J. 2017. Zoo: Zeroth order optimization based\nblack-box attacks to deep neural networks without training\nsubstitute models. In Proceedings of the 10th ACM Work-\nshop on Artiﬁcial Intelligence and Security, 15–26.\n[Chen et al. 2019a] Chen, B.; Ren, Z.; Yu, C.; Hussain, I.;\nand Liu, J. 2019a. Adversarial examples for cnn-based mal-\nware detectors. IEEE Access 7:54360–54371.\n[Chen et al. 2019b] Chen, Y .; Wang, S.; She, D.; and Jana, S.\n2019b. On training robust pdf malware classiﬁers. arXiv\npreprint arXiv:1904.03542.\n[Demetrio et al. 2019] Demetrio, L.; Biggio, B.; Lagorio, G.;\nRoli, F.; and Armando, A. 2019. Explaining vulnerabili-\nties of deep learning to adversarial malware binaries. arXiv\npreprint arXiv:1901.03583.\n[Demetrio et al. 2020] Demetrio, L.; Biggio, B.; Lagorio, G.;\nRoli, F.; and Armando, A. 2020. Efﬁcient black-box opti-\nmization of adversarial windows malware with constrained\nmanipulations. arXiv preprint arXiv:2003.13526.\n[Dey and Salemt 2017] Dey, R., and Salemt, F. M. 2017.\nGate-variants of gated recurrent unit (gru) neural networks.\nIn 2017 IEEE 60th international midwest symposium on cir-\ncuits and systems (MWSCAS), 1597–1600. IEEE.\n[Dey et al. 2019] Dey, S.; Kumar, A.; Sawarkar, M.; Singh,\nP. K.; and Nandi, S. 2019. Evadepdf: Towards evading\nmachine learning based pdf malware classiﬁers. In In-\nternational Conference on Security & Privacy , 140–150.\nSpringer.\n[Fang et al. 2019] Fang, Z.; Wang, J.; Li, B.; Wu, S.; Zhou,\nY .; and Huang, H. 2019. Evading anti-malware engines with\ndeep reinforcement learning. IEEE Access 7:48867–48879.\n[Fleshman et al. 2019] Fleshman, W.; Raff, E.; Sylvester,\nJ.; Forsyth, S.; and McLean, M. 2019. Non-negative\nnetworks against adversarial attacks. arXiv preprint\narXiv:1806.06108.\n[Goldberg 2017] Goldberg, Y . 2017. Neural Network Meth-\nods for Natural Language Processing, volume 10.\n[Goodfellow, McDaniel, and Papernot 2018] Goodfellow, I.;\nMcDaniel, P.; and Papernot, N. 2018. Making machine\nlearning robust against adversarial inputs. Communications\nof the ACM 61(7):56–66.\n[Hu and Tan 2018] Hu, W., and Tan, Y . 2018. Black-box at-\ntacks against rnn based malware detection algorithms. In\nWorkshops at the Thirty-Second AAAI Conference on Artiﬁ-\ncial Intelligence.\n[Irie et al. 2019] Irie, K.; Prabhavalkar, R.; Kannan, A.;\nBruguier, A.; Rybach, D.; and Nguyen, P. 2019. On the\nchoice of modeling unit for sequence-to-sequence speech\nrecognition. Proc. Interspeech 2019 3800–3804.\n[Kim et al. 2016] Kim, Y .; Jernite, Y .; Sontag, D.; and Rush,\nA. M. 2016. Character-aware neural language models. In\nThirtieth AAAI conference on artiﬁcial intelligence.\n[Kolosnjaji et al. 2018] Kolosnjaji, B.; Demontis, A.; Big-\ngio, B.; Maiorca, D.; Giacinto, G.; Eckert, C.; and Roli, F.\n2018. Adversarial malware binaries: Evading deep learning\nfor malware detection in executables. In 2018 26th Euro-\npean Signal Processing Conference (EUSIPCO) , 533–537.\nIEEE.\n[Kreuk et al. 2018] Kreuk, F.; Barak, A.; Aviv-Reuven, S.;\nBaruch, M.; Pinkas, B.; and Keshet, J. 2018. Adversar-\nial examples on discrete sequences for beating whole-binary\nmalware detection. arXiv preprint arXiv:1802.04528.\n[Krˇc´al et al. 2018] Kr ˇc´al, M.; ˇSvec, O.; B´alek, M.; and Jaˇsek,\nO. 2018. Deep Convolutional Malware Classiﬁers Can\nLearn from Raw Executables and Labels Only. In Inter-\nnational Conference on Learning Representations (ICLR).\n[Madry et al. 2017] Madry, A.; Makelov, A.; Schmidt, L.;\nTsipras, D.; and Vladu, A. 2017. Towards deep learn-\ning models resistant to adversarial attacks. arXiv preprint\narXiv:1706.06083.\n[Mogren and Johansson 2019] Mogren, O., and Johansson,\nR. 2019. Character-based recurrent neural networks for\nmorphological relational reasoning. Journal of Language\nModelling 7(1):139–170.\n[Ono, Utiyama, and Sumita 2019] Ono, J.; Utiyama, M.; and\nSumita, E. 2019. Hybrid data-model parallel training\nfor sequence-to-sequence recurrent neural network machine\ntranslation. In Proceedings of The 8th Workshop on Patent\nand Scientiﬁc Literature Translation, 4–12. Dublin, Ireland:\nEuropean Association for Machine Translation.\n[Park, Khan, and Yener 2019] Park, D.; Khan, H.; and Yener,\nB. 2019. Generation & evaluation of adversarial exam-\nples for malware obfuscation. In 2019 18th IEEE Interna-\ntional Conference On Machine Learning And Applications\n(ICMLA), 1283–1290. IEEE.\n[Qiu et al. 2019] Qiu, S.; Liu, Q.; Zhou, S.; and Wu, C. 2019.\nReview of artiﬁcial intelligence adversarial attack and de-\nfense technologies. Applied Sciences 9(5):909.\n[Raff et al. 2018] Raff, E.; Barker, J.; Sylvester, J.; Brandon,\nR.; Catanzaro, B.; and Nicholas, C. K. 2018. Malware de-\ntection by eating a whole exe. In Workshops at the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence.\n[Rosenberg et al. 2019] Rosenberg, I.; Shabtai, A.; Elovici,\nY .; and Rokach, L. 2019. Defense methods against adversar-\nial examples for recurrent neural networks. arXiv preprint\narXiv:1901.09963.\n[Suciu, Coull, and Johns 2019] Suciu, O.; Coull, S. E.; and\nJohns, J. 2019. Exploring adversarial examples in malware\ndetection. In 2019 IEEE Security and Privacy Workshops\n(SPW), 8–14. IEEE.\n[Tounsi and Rais 2018] Tounsi, W., and Rais, H. 2018. A\nsurvey on technical threat intelligence in the age of sophis-\nticated cyber attacks. Computers & security 72:212–233.\n[Zuo et al. 2018] Zuo, F.; Li, X.; Young, P.; Luo, L.; Zeng,\nQ.; and Zhang, Z. 2018. Neural machine translation in-\nspired binary code similarity comparison beyond function\npairs. arXiv preprint arXiv:1808.04706.\nAppendix A - MalRNN Speciﬁcations\nBoth encoder and decoder in MalRNN consist of 100 GRUs\nwith Tanh activation functions as their building blocks. All\nbiases of the GRU units were initialized to zero. The di-\nmension of the emebedding layer (i.e., vocabulary size) in\nthe encoder was set to 256. Also, the size of the fully\nconnected output layer in the decoder was set to 128. Pa-\nrameter settings of MalRNN were ﬁxed throughout all ex-\nperiments. The learning rate and systematic sampling rate\nwere set to 1e−2 and 1e−3, respectively. Also, the batch\nsize was ﬁxed to 10 throughout all experiments. The max-\nimum append size was set to 40% for all benchmark meth-\nods and MalRNN. Lastly, the maximum number of attempts\nwas set to 50 for all benchmark methods and MalRNN. To\nfacilitate reproducability, the code, corresponding dataset,\nand benchmark methods were made available on GitHub at\nhttps://github.com/johnnyzn/MalRNN.",
  "topic": "Evasion (ethics)",
  "concepts": [
    {
      "name": "Evasion (ethics)",
      "score": 0.8268773555755615
    },
    {
      "name": "Byte",
      "score": 0.7870649099349976
    },
    {
      "name": "Malware",
      "score": 0.755056619644165
    },
    {
      "name": "Adversarial system",
      "score": 0.6927148103713989
    },
    {
      "name": "Computer science",
      "score": 0.6909223198890686
    },
    {
      "name": "Black box",
      "score": 0.6203446984291077
    },
    {
      "name": "Computer security",
      "score": 0.5465902090072632
    },
    {
      "name": "Binary number",
      "score": 0.4807169735431671
    },
    {
      "name": "Detector",
      "score": 0.4751662611961365
    },
    {
      "name": "Deep learning",
      "score": 0.47435247898101807
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4233788847923279
    },
    {
      "name": "Machine learning",
      "score": 0.342069149017334
    },
    {
      "name": "Operating system",
      "score": 0.2797468304634094
    },
    {
      "name": "Arithmetic",
      "score": 0.20424485206604004
    },
    {
      "name": "Mathematics",
      "score": 0.11240878701210022
    },
    {
      "name": "Telecommunications",
      "score": 0.08092904090881348
    },
    {
      "name": "Immune system",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Immunology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 19
}