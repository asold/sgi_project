{
    "title": "Medical reasoning in LLMs: an in-depth analysis of DeepSeek R1",
    "url": "https://openalex.org/W4411394707",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3130333390",
            "name": "Birger Moell",
            "affiliations": [
                "KTH Royal Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3023343058",
            "name": "Fredrik Sand Aronsson",
            "affiliations": [
                "Karolinska Institutet",
                "Karolinska University Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2096675247",
            "name": "Sanian Akbar",
            "affiliations": [
                "Stockholm Health Care Services"
            ]
        },
        {
            "id": "https://openalex.org/A3130333390",
            "name": "Birger Moell",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3023343058",
            "name": "Fredrik Sand Aronsson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096675247",
            "name": "Sanian Akbar",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6891853064",
        "https://openalex.org/W4220827941",
        "https://openalex.org/W3086667591",
        "https://openalex.org/W4403442768",
        "https://openalex.org/W6637651015",
        "https://openalex.org/W2078213917",
        "https://openalex.org/W6771739371",
        "https://openalex.org/W4409709519",
        "https://openalex.org/W2060572591",
        "https://openalex.org/W4406779522",
        "https://openalex.org/W2136049664",
        "https://openalex.org/W4399248891",
        "https://openalex.org/W2016056968",
        "https://openalex.org/W2763987884",
        "https://openalex.org/W4403813762",
        "https://openalex.org/W4292219922",
        "https://openalex.org/W2141222241",
        "https://openalex.org/W4396914181",
        "https://openalex.org/W4400949393",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W2889317836",
        "https://openalex.org/W6929467481",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4399399733",
        "https://openalex.org/W4403576262",
        "https://openalex.org/W2992787682",
        "https://openalex.org/W4400307907",
        "https://openalex.org/W6948179215",
        "https://openalex.org/W2346834216",
        "https://openalex.org/W4390918187",
        "https://openalex.org/W1754923559",
        "https://openalex.org/W2603785841",
        "https://openalex.org/W4402335609",
        "https://openalex.org/W2998842167",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W2787934493",
        "https://openalex.org/W1980216664",
        "https://openalex.org/W4210641620",
        "https://openalex.org/W4205164650",
        "https://openalex.org/W2047619334",
        "https://openalex.org/W2023928525",
        "https://openalex.org/W3036159901",
        "https://openalex.org/W2991486269",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4376642690",
        "https://openalex.org/W3009177960",
        "https://openalex.org/W4392491647",
        "https://openalex.org/W4404783225",
        "https://openalex.org/W6809646742",
        "https://openalex.org/W2767879180",
        "https://openalex.org/W2571281625",
        "https://openalex.org/W6891767026",
        "https://openalex.org/W2970370298",
        "https://openalex.org/W4406545364",
        "https://openalex.org/W2101756318",
        "https://openalex.org/W2995701103",
        "https://openalex.org/W4221143046"
    ],
    "abstract": "Introduction The integration of large language models (LLMs) into healthcare holds immense promise, but also raises critical challenges, particularly regarding the interpretability and reliability of their reasoning processes. While models like DeepSeek R1-which incorporates explicit reasoning steps-show promise in enhancing performance and explainability, their alignment with domain-specific expert reasoning remains understudied. Methods This paper evaluates the medical reasoning capabilities of DeepSeek R1, comparing its outputs to the reasoning patterns of medical domain experts. Results Through qualitative and quantitative analyses of 100 diverse clinical cases from the MedQA dataset, we demonstrate that DeepSeek R1 achieves 93% diagnostic accuracy and shows patterns of medical reasoning. Analysis of the seven error cases revealed several recurring errors: anchoring bias, difficulty integrating conflicting data, limited consideration of alternative diagnoses, overthinking, incomplete knowledge, and prioritizing definitive treatment over crucial intermediate steps. Discussion These findings highlight areas for improvement in LLM reasoning for medical applications. Notably the length of reasoning was important with longer responses having a higher probability for error. The marked disparity in reasoning length suggests that extended explanations may signal uncertainty or reflect attempts to rationalize incorrect conclusions. Shorter responses (e.g., under 5,000 characters) were strongly associated with accuracy, providing a practical threshold for assessing confidence in model-generated answers. Beyond observed reasoning errors, the LLM demonstrated sound clinical judgment by systematically evaluating patient information, forming a differential diagnosis, and selecting appropriate treatment based on established guidelines, drug efficacy, resistance patterns, and patient-specific factors. This ability to integrate complex information and apply clinical knowledge highlights the potential of LLMs for supporting medical decision-making through artificial medical reasoning.",
    "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/eight.tnum June /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nOPEN ACCESS\nEDITED BY\nTim Hulsen,\nRotterdam University of Applied Sciences,\nNetherlands\nREVIEWED BY\nHasi Hays,\nUniversity of Arkansas, United States\nHossein Motahari-Nezhad,\nÓbuda University, Hungary\n*CORRESPONDENCE\nBirger Moëll\nbmoell@kth.se\nRECEIVED /two.tnum/two.tnum April /two.tnum/zero.tnum/two.tnum/five.tnum\nACCEPTED /two.tnum/nine.tnum May /two.tnum/zero.tnum/two.tnum/five.tnum\nPUBLISHED /one.tnum/eight.tnum June /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nMoëll B, Sand Aronsson F and Akbar S (/two.tnum/zero.tnum/two.tnum/five.tnum)\nMedical reasoning in LLMs: an in-depth\nanalysis of DeepSeek R/one.tnum.\nFront. Artif. Intell./eight.tnum:/one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Moëll, Sand Aronsson and Akbar. This\nis an open-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nMedical reasoning in LLMs: an\nin-depth analysis of DeepSeek R/one.tnum\nBirger Moëll /one.tnum*, Fredrik Sand Aronsson /two.tnum,/three.tnumand Sanian Akbar /four.tnum\n/one.tnumDivision of Speech, Music and Hearing, School of Electrical Engin eering and Computer Science, KTH\nRoyal Institute of Technology, Stockholm, Sweden, /two.tnumDivision of Speech and Language Pathology,\nDepartment of Clinical Science, Intervention and Technology, K arolinska Institutet, Stockholm,\nSweden, /three.tnumTheme Womens Health and Allied Health Professionals, Section o f Speech and Language\nPathology, Karolinska University Hospital, Stockholm, Sweden, /four.tnumStockholm Health Care Services,\nStockholm, Sweden\nIntroduction: The integration of large language models (LLMs) into healthcar e\nholds immense promise, but also raises critical challenges, part icularly regarding\nthe interpretability and reliability of their reasoning pr ocesses. While models\nlike DeepSeek R/one.tnum-which incorporates explicit reasoning steps-show promise in\nenhancing performance and explainability, their alignment wit h domain-speciﬁc\nexpert reasoning remains understudied.\nMethods: This paper evaluates the medical reasoning capabilities of Deep Seek\nR/one.tnum, comparing its outputs to the reasoning patterns of medical domain experts.\nResults: Through qualitative and quantitative analyses of /one.tnum/zero.tnum/zero.tnum diverseclinical\ncases from the MedQA dataset, we demonstrate that DeepSeek R/one.tnum achieves\n/nine.tnum/three.tnum% diagnostic accuracy and shows patterns of medical reasoning. Analysis of\nthe seven error cases revealed several recurring errors: anchor ing bias, diﬃculty\nintegrating conﬂicting data, limited consideration of alterna tive diagnoses,\noverthinking, incomplete knowledge, and prioritizing deﬁni tive treatment over\ncrucial intermediate steps.\nDiscussion: These ﬁndings highlight areas for improvement in LLM reasoni ng\nfor medical applications. Notably the length of reasoning was imp ortant with\nlonger responses having a higher probability for error. The marked disparity in\nreasoning length suggests that extended explanations may sig nal uncertainty\nor reﬂect attempts to rationalize incorrect conclusions. Shorter responses (e.g.,\nunder /five.tnum,/zero.tnum/zero.tnum/zero.tnum characters) were strongly associated with accuracy, providing\na practical threshold for assessing conﬁdence in model-generat ed answers.\nBeyond observed reasoning errors, the LLM demonstrated sou nd clinical\njudgment by systematically evaluating patient information , forming a diﬀerential\ndiagnosis, and selecting appropriate treatment based on estab lished guidelines,\ndrug eﬃcacy, resistance patterns, and patient-speciﬁc factors. T his ability to\nintegrate complex information and apply clinical knowledge hig hlights the\npotential of LLMs for supporting medical decision-making thr ough artiﬁcial\nmedical reasoning.\nKEYWORDS\nLLM, medical reasoning, DeepSeek R/one.tnum, AI in medicine, reasoning models, medical\nbenchmarking\n/one.tnum Introduction\nThe accelerating adoption of artiﬁcial intelligence (AI) in healthcare, particularly\nlarge language models (LLMs), presents unprecedented opportunities to augment clinical\ndecision-making and potentially improve patient outcomes. Clinical reasoning, the\ncornerstone of medical practice, is a complex cognitive process where practitioners\nintegrate heterogeneous data streams, apply specialized knowledge frameworks, and\nnavigate uncertainty to arrive at diagnostic and therapeutic decisions (\nJay et al., 2024 ;\nSudacka et al., 2023 ). This high-stakes process remains vulnerable to systemic failures, as\nevidenced by research suggesting medical errors contribute to over 250,000 deaths annually\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nin the US, making it the third leading cause of death. Medical error\nincludes unintended acts, execution failures, planning errors, or\ndeviations from care processes that may cause harm (\nMakary and\nDaniel, 2016).\nThese challenges are exacerbated as healthcare systems\nworldwide face mounting pressures from workforce shortages\n(\nWorld Health Organization, 2023 ) and increasing diagnostic\ncomplexity. In this strained environment, LLMs have emerged as\npotential aids to support clinical decision-making by potentially\nreducing cognitive burdens and mitigating error risks. However,\nthe integration of these systems into medical workﬂows demands\nrigorous examination of their reasoning capabilities—not just\ntheir factual knowledge, but their ability to emulate the nuanced\ncognitive processes of expert clinicians while addressing systemic\nvulnerabilities in care delivery.\n/one.tnum./one.tnum Clinical reasoning in healthcare\nClinical reasoning is an essential skill for healthcare\nprofessionals, particularly physicians (\nCrescitelli et al., 2019 ;\nDurning et al., 2024 ). It encompasses all aspects of clinical\npractice, including patient management, treatment decisions, and\nongoing care (\nCrescitelli et al., 2019 ). While extensive research\nhas focused on this area, challenges remain in understanding and\nimplementing eﬀective clinical reasoning (\nYazdani and Abardeh,\n2019).\nA tension exists between explicit, quantitative approaches\nand the inherent limitations of human cognition, leading to\nthe recognition that clinical reasoning involves both analytical\nand non-analytical processes, as described in dual-process theory\n(\nPelaccia et al., 2011 ; Ferreira et al., 2010 ). Understanding how\nclinicians utilize both System 1 (intuitive) and System 2 (analytical)\nreasoning is crucial for evaluating whether LLMs can replicate this\nnuanced cognitive process.\n/one.tnum./one.tnum./one.tnum Theoretical models and cognitive processes\nSeveral theoretical frameworks have shaped our understanding\nof clinical reasoning:\n• Hypothetico-deductive reasoning: Clinicians generate and\ntest hypotheses using clinical data (\nNierenberg, 2020 ). This\nmodel, while foundational, has been reﬁned as research\nindicates clinical reasoning is more domain-speciﬁc and\nknowledge-dependent than initially thought.\n• Script theory: Medical knowledge is organized into “illness\nscripts\"–cognitive frameworks that integrate clinical ﬁndings,\nrisk factors, and pathophysiology (\nGee et al., 2017 ; Charlin\net al., 2000 ). Evaluating LLMs requires assessing their ability\nto form and utilize analogous script-like structures.\n• Dual process theory: This inﬂuential framework describes\ntwo systems of thinking: a fast, intuitive system (Type 1) and\na slower, analytical system (Type 2) (\nGold et al., 2022 ; Custers,\n2013). Clinicians ﬂexibly switch between these modes based\non experience and situation ( Boushehri et al., 2015 ). This\nhighlights the need to evaluate LLMs on both rapid, pattern-\nrecognition tasks and more complex, analytical scenarios.\n• Situated and distributed cognition: Clinical reasoning is\ninﬂuenced by environmental factors, patient interactions, and\nteam dynamics (\nGold et al., 2022 ; Durning and Artino, 2011 ).\nFactors like fatigue and time pressure can impact the process\n(\nTorre et al., 2020 ). This suggests that evaluating LLMs\nshould consider their performance under various contextual\nconstraints.\nClinical reasoning operates through both rapid, intuitive\n(System 1) and slower, analytical (System 2) cognitive processes.\nSystem 1 relies on pattern recognition and experience to\ngenerate immediate diagnostic hypotheses, while System 2 involves\ndeliberate, systematic evaluation of information (\nShimozono et al.,\n2020; Barbosa Chaves et al., 2022 ). Clinicians ﬂexibly switch\nbetween these modes depending on case complexity ( Shimizu and\nTokuda, 2012; Olupeliyawa, 2017).\n/one.tnum./one.tnum./two.tnum Development of expertise\nThe development of clinical reasoning expertise involves a\nprogression from deductive reasoning to the reﬁnement of illness\nscripts, enabling more eﬃcient diagnostic processes (\nShin, 2019 ;\nRadovi´c et al., 2022 ; Lubarsky et al., 2015 ). This involves mastering\ndata gathering, hypothesis generation, diﬀerential diagnosis, and\nmanagement planning (\nWeinstein et al., 2017 ). Assessing an LLM’s\nability to simulate this developmental trajectory could provide\ninsights into its potential for clinical reasoning.\n/one.tnum./one.tnum./three.tnum Diagnostic errors\nDiagnostic errors, often linked to reasoning failures, contribute\nsigniﬁcantly to preventable adverse events (\nMettarikanon and\nTawanwongsri, 2024 ; Zwaan et al., 2010 ). Cognitive errors,\nparticularly biases in information processing, are implicated in a\nmajority of diagnostic errors (\nGraber et al., 2005 ; Mukhopadhyay\nand Choudhari, 2024 ; Schiﬀ et al., 2013 ). Common biases include\nrepresentative heuristic, availability heuristic, and anchoring ( Kim\nand Lee, 2018 ). This underscores the importance of evaluating\nLLMs for susceptibility to similar cognitive biases.\nStructured reﬂection and deliberate analysis can improve\ndiagnostic accuracy ( Moroz, 2017 ). However, the optimal balance\nbetween intuitive and analytical reasoning depends on various\nfactors (\nWelch et al., 2017 ). This suggests that evaluating LLMs\nshould involve tasks that require both rapid, intuitive responses and\nmore deliberate, analytical reasoning.\nThe theoretical frameworks of clinical reasoning will inform\nthe evaluation of DeepSeek R1 by providing a basis for analyzing\nits reasoning chains, identifying potential cognitive biases, and\nassessing its ability to navigate complex clinical scenarios analogous\nto human experts.\n/one.tnum./two.tnum Bridging clinical-cognitive theory and\nLLM computation\n• Hypothetico-deductive reasoning: Similar to the step-wise\n“chain-of-thought prompting (\nWei et al., 2022 ) now used to\nforce models into enumerating intermediate inferences before\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\ncommitting to an answer; tokens in the hidden state act as\nprovisional hypotheses that are pruned or strengthened as new\ncontext is ingested.\n• Illness-script theory : Maps onto the Platonic representation\nhypothesis arguing that large language modules appear to\nlearn the same representations independent of model (\nHuh\net al., 2024 ): every clinical vignette is pulled toward a stable,\ncross-modal “ideal” embedding representing the prototypical\npresentation and guideline-recommended next steps. In this\nsense, both LLM knowledge and human knowledge of the\nsame concept can be argued to be stored in the same\nconceptual place.\n• Dual-process theory : Mirrored in the coexistence of fast,\nimplicit completions (Type 1: zero-shot or few-shot inference)\nand slow, explicit reasoning traces (Type 2: deliberate chains\nor tree-of-thought sampling).\n• Situated/distributed cognition: Corresponds to retrieval-\naugmented generation (RAG) (\nLewis et al., 2020 ) and mixture-\nof-experts (MoE) ( Li and Zhou, 2024 ) systems, where external\nknowledge bases or specialist subnetworks are dynamically\nrouted in–much like clinicians consult colleagues, guidelines,\nor point-of-care tests when confronted with diagnostic\nuncertainty.\n/one.tnum./three.tnum Clinical reasoning by LLMs\nThe rapid evolution of LLMs presents both unprecedented\nopportunities and profound challenges for healthcare applications.\nWhile models like GPT-4 demonstrate remarkable performance\non medical licensing examinations, achieving 87.6% accuracy\non USMLE-style questions (\nNori et al., 2023 ), performance\nmetrics alone provide insuﬃcient evidence for clinical deployment.\nModern medicine requires reasoning that extends beyond factual\nrecall to encompass contextual adaptation, probabilistic weighting\nof competing hypotheses, and adherence to evolving clinical\nguidelines (\nRajpurkar et al., 2022 ). A critical gap persists between\nLLMs’ capacity to generate clinically plausible text and their ability\nto replicate the disciplined reasoning processes that underlie safe\npatient care (\nSinghal et al., 2023 ). Reasoning models such as\nDeepSeek R1 ( DeepSeek-AI et al., 2025 ) output reasoning tokens,\na chain of thought process of thinking in text before giving a text\nresponse. By evaluating reasoning tokens we can evaluate whether\nDeepSeek R1’s (\nDeepSeek-AI et al., 2025 ) reasoning aligns with\nthat of medical experts, particularly in complex clinical scenarios.\nDeepSeek R1 is designed to generate explicit inference chains\nthrough chain-of-thought prompting (\nDeepSeek-AI et al., 2025 ),\noﬀering a degree of interpretability that is crucial for medical\napplications. This paper focuses on DeepSeek R1 because its\narchitecture, which emphasizes explicit reasoning steps, provides a\nunique opportunity to analyze the ﬁdelity of its medical reasoning\nin comparison to human experts. The model is available open\nsource which makes it possible to deploy on site for potential\nhandling of sensitive clinical data.\nThe urgency of this research stems from the accelerating real-\nworld deployment of medical LLMs despite unresolved limitations.\nA 2023 survey found 38% of U.S. health systems piloting\nLLM-based tools (\nHealthcare Information and Management\nSystems Society , HIMSS ), while regulatory approvals for AI\ndiagnostics increased 127% annually since 2020 ( Benjamens et al.,\n2020). The potential risks of deploying LLMs without a thorough\nunderstanding of their reasoning abilities underscore the need for\nthis research. Our work bridges critical gaps by:\n• Establishing validity metrics beyond answer correctness,\nfocusing on medical reasoning ability. We evaluate not\njust *what* the LLM answers, but *how* it arrives at that\nanswer, analyzing the steps in its reasoning process. This\ngoes beyond simple accuracy metrics to assess the quality and\nappropriateness of the reasoning itself.\n• Identifying high-risk error patterns requiring mitigation,\nsuch as anchoring bias, protocol violations, and\nmisinterpretations of lab values. Our analysis of DeepSeek\nR1’s errors reveals speciﬁc cognitive biases and knowledge\ngaps that could lead to patient harm. Identifying these\npatterns is crucial for developing mitigation strategies.\n• Providing a foundation for medically-grounded\narchitectures and training paradigms. By understanding\nthe strengths and weaknesses of current LLM reasoning,\nwe can inform the design of future models that better align\nwith clinical reasoning processes. This includes exploring\ntechniques like retrieval augmented generation (RAG) and\nﬁne-tuning on medical reasoning data.\nAs LLMs transition from experimental tools to clinical assets,\nit is imperative for reasoning transparency equivalent to human\npractitioners. Through systematic evaluation of reasoning chain\nﬁdelity, we lay the groundwork for AI systems that complement\nrather than conﬂict with clinical judgment, harnessing LLMs’\npotential while safeguarding evidence-based medicine.\nOne key beneﬁt of reasoning models over previous LLMs\nis the reasoning as a solution to the black box problem of\nLLM outputs (\nWang Y. et al., 2024 ). By following the models\nreasoning we can evaluate their solutions and see what errors\nin thinking or knowledge led to incorrect outcomes. This has\ngreat potential both from a medical and a technical perspective.\nFrom a medical perspective, the information can be valuable if\ncommon LLM reasoning errors mimic errors that humans make.\nIf so we can use LLM reasoning errors to understand how we\ncan better train physicians to have robust medical reasoning\nskills. From a technical perspective, medical reasoning outputs and\nmedical reasoning errors can be used for reasoning ﬁne-tuning and\nreinforcement learning training (\nDeepSeek-AI et al., 2025 ) as well\nas understanding what data sources might need to be added to the\nmodel to improve performance.\nBy evaluation reasoning we get a more granular understanding\nof both what the model knows and doesn’t know and its reasoning\nprocess and the errors within that reasoning process.\n/one.tnum./four.tnum Current research on medical reasoning\nby LLMs\nResearch has looked into techniques for improving medical\nreasoning in LLMs.\nLucas et al. (2024) showed that prompt\ntechniques could improve the reasoning capabilities of LLMs in\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nthe medical domain while, Wang J. et al. (2024) showed that RAG\njoint training techniques reduced hallucinations and improved\nreasoning capabilities.\nMaharana et al. (2025) found misaligned\nbetween prediction and reasoning with LLMs predicting correctly\nwith faulty reasoning.\nLi et al. (2024) built a multi turn system for\nmedical evaluation of LLMs helpful for assessing clinical reasoning\nability. Recently Open AI releases HealthBench (\nArora et al., 2025 )\na structured evaluation of LLMs in the medical domain focusing\non the quality of outputs.\nLai et al. (2025) trained and evaluated\na Medical Reasoning Vision Language models in a similar style to\nDeepSeek R1. Similarly,\nYu et al. (2025) ﬁne tuned a LLM model\nspeciﬁcally for Medical Reasoning and Wu et al. (2025) created a\ndataset for medical reasoning and ﬁne-tuned a LLM for improved\nmedical reasoning.\nOn the relationship between use of LLMs and reasoning skills\n(\nGoh et al., 2024 ) found no improvement in reasoning skills\nby physicians when using LLMs as a tool in comparison to\nconventional resources. In contrast,\nBorg et al. (2024) found that\na social robot powered by an LLM was useful in clinical training.\n/two.tnum Methodology\n/two.tnum./one.tnum Dataset\n/two.tnum./one.tnum./one.tnum Evaluation corpus\nThe study utilized 100 clinically diverse questions from the\nMedQA benchmark (\nJin et al., 2021 ), a rigorously validated\ndataset derived from professional medical board examinations\nacross multiple countries. MedQA ’s questions follow the United\nStates Medical Licensing Examination (USMLE) format, testing\ndiagnostic reasoning through clinical vignettes requiring:\n• Interpretation of patient histories and physical ﬁndings.\n• Selection of appropriate diagnostic tests.\n• Application of therapeutic guidelines.\n• Integration of pathophysiology knowledge.\nQuestions were selected through random sampling to ensure\na cover of a range of specialties within medicine. The amount of\nquestions ( n = 100) was selected to facilitate human analysis of\nreasoning outputs.\n/two.tnum./two.tnum Model implementation\nWe evaluated DeepSeek-R1 (\nDeepSeek-AI et al., 2025 ), a 671B\nparameter mixture of expert reasoning-enhanced language model\nbuilt through a novel multi-stage training pipeline that combines\nreinforcement learning and ﬁne-tuning on reasoning data. We used\nthe DeepSeek-Reasoner model available through the DeepSeek API\nwith default params. The code used for calling the model including\ndata used and model outputs is available open source on Github.\n/one.tnum\n/one.tnumhttps://github.com/BirgerMoell/medical-reasoning\n/two.tnum./two.tnum./one.tnum System prompt\nPlease analyze this medical question carefully. Consider\nthe relevant medical knowledge, clinical guidelines, and logical\nreasoning needed. Then select the single most appropriate answer\nchoice. Provide your answer as just the letter (A, B, C, or D).\n/two.tnum./three.tnum Error classiﬁcation protocol\n• Step 1: Ground truth alignment check\n– Compare ﬁnal answer to MedQA reference\n• Step 2: Reasoning chain decomposition\n– Break down into diagnostic/treatment decision points\n– Map to clinical reasoning taxonomy\n• Step 3: Expert validation\n– Clinician review all errors and compared them to medical\nreasoning best practice.\n/three.tnum Results\nAuthor S.A who is a active medical professional performed\nanalysis of the medical reasoning of the model. Additional analysis\nfocused on model performance and cognitive errors was done by\nauthors B.M and F.S. The model achieved an overall accuracy of\n93% on the 100 MedQA questions. Our analysis focused on the\nseven cases where the model made an error to identify patterns and\nmechanisms of reasoning failures.\n/three.tnum./one.tnum Reasoning analysis by medical\nprofessional\n/three.tnum./one.tnum./one.tnum Error case /one.tnum: neonatal bilious vomiting\nThe model’s reasoning is hampered by anchoring bias,\ndiﬃculty integrating conﬂicting data, limited consideration of\nalternative diagnoses, overthinking, and a somewhat incomplete\nunderstanding of the embryology involved. It struggles to eﬃciently\nprocess the information and prioritize the most relevant clues,\nhindering its ability to conﬁdently reach the correct diagnosis.\n/three.tnum./one.tnum./two.tnum Error case /two.tnum: respiratory failure\nThe model correctly identiﬁes key information such as age,\nrisk factors, recent surgery and ﬁndings in the pulmonary artery.\nIt excessively focuses on histological composition and ﬁbrous\nremodeling, leading it to weighing other options as more likely.\n/three.tnum./one.tnum./three.tnum Error case /three.tnum: acute limb ischemia\nLimb ischemia is correctly identiﬁed. The model recognizes\natrial ﬁbrillation as a key risk factor for arterial emboli, and\ndiscusses Rutherford classiﬁcations and possible interventions\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\n(surgery vs. thrombolysis). It emphasizes the urgency of\nrevascularization and reasons that surgical thrombectomy\nshould be done because the patient’s presentation suggests an\nembolic source and immediate threat to the limb. It incorrectly\nweighs the deﬁnitive treatment as the answer and skips the\nimportant “next\" step of heparin drip.\n/three.tnum./one.tnum./four.tnum Error case /four.tnum: porphyria cutanea tarda (PCT)\nRecognizes porphyria cutanea tarda (PCT) based on\nphotosensitive blistering, dark urine, and hyperpigmentation. It\nexplains that treatment typically involves phlebotomy or low-dose\nhydroxychloroquine. It dismisses invasive or less relevant options\n(liver transplantation, thalidomide) and incorrectly concludes\nthat hydroxychloroquine (alternative ﬁrst line treatment) is the\nbest next step, largely because the patient’s ferritin level is normal.\nNormally, a professional would reason that phlebomoty (ﬁrst-line\ntreatment) can induce remission even with normal iron stores and\nhydroxychloroquine is used if patient cannot tolerate phlebotomy.\n/three.tnum./one.tnum./five.tnum Error case /five.tnum: enzyme kinetics\nRecognizes hexokinase and glucokinase properties as\ncandidates for an enzyme found in most tissues that phosphorylates\nglucose. It also correctly identiﬁes it as hexokinase rather than\nglucokinase, noting that hexokinase has a low Km (high aﬃnity).\nHowever, it concludes that this enzyme also has a high Vmax,\nleading it to pick the incorrect answer (“Low X and high Y”). The\nLLM’s ﬁnal reasoning step confuses hexokinase’s lower capacity\n(lower Vmax) with a higher capacity, thereby arriving at the wrong\nchoice.\n/three.tnum./one.tnum./six.tnum Error case /six.tnum: preterm PDA management\nIt rightly identiﬁes the continuous murmur as PDA-related\nand distinguishes between drugs that keep the ductus open\n(prostaglandin E1) and those that close it (indomethacin).\nHowever, it overestimates how age limits indomethacin’s use,\nleading it prematurely to favor surgical ligation. In actual clinical\npractice, a stable 5-week-old would still warrant a trial of\npharmacologic closure before considering surgical options.\n/three.tnum./one.tnum./seven.tnum Error case /seven.tnum: niacin ﬂushing\nCorrectly identiﬁes that the patient experiences niacin-induced\nﬂushing after statin intolerance. It recognizes niacin as a likely cause\nof her evening ﬂushing and pruritus, and appropriately considers–\nbut rules out–alternative explanations such as carcinoid syndrome\nand pheochromocytoma, given hints of cancer in the patient’s\nhistory. However, it departs from a typical medical approach by\nconcluding that switching to fenoﬁbrate (which primarily targets\nelevated triglycerides rather than LDL) is the best next step,\nrather than attempting to mitigate the ﬂushing (for example,\nwith NSAIDs) while maintaining niacin therapy. This oversight\nhighlights a gap in its reasoning compared to standard clinical\npractice, where controlling niacin’s side eﬀects is usually preferred\nbefore abandoning a therapy that addresses the patient’s elevated\nLDL cholesterol.\n/three.tnum./one.tnum./seven.tnum./one.tnum Risk scale\nHigh: Foreseeable life- or limb-threat within hours-days.\nModerate: Appreciable morbidity or accelerated disease\nprogression, but sub-acute. Low: Negligible immediate harm;\neﬀects felt only over the long term or not at all, forensic question.\n/three.tnum./two.tnum Detailed error analysis\n/three.tnum./two.tnum./one.tnum Error case /one.tnum: neonatal bilious vomiting\n• Pathway of reasoning :\nBilious Vomit → Duodenal Atresia\n\n \nModel’s Focus\n→ Emergency Laparotomy ← Annular Pancreas\n← Delayed Presentation + Normal Prenatal US\n• Critical failure : Anchoring bias on classic duodenal\nobstruction pattern while ignoring:\n1. 3-week delayed presentation (incompatible with complete\natresia)\n2. Absence of prenatal ultrasound ﬁndings\n• Clinical impact : Risk of delayed annular pancreas diagnosis\n(24–48 h window for surgical intervention)\n/three.tnum./two.tnum./two.tnum Error case /two.tnum: respiratory failure\n• Pathway of reasoning :\nDVT → PE → Fibrosis → Actual Cause → CTEPH\n\n \nModel’s Focus\n• Critical failure: Attributed wall remodeling (eﬀect) as primary\npathology\n• Risk ampliﬁcation : Increased mortality from missed\nvasculitis diagnosis\n/three.tnum./two.tnum./three.tnum Error case /three.tnum: acute limb ischemia\n• Pathway of reasoning :\nIschemic Limb → Direct Surgery\n\n \nModel’s Focus\n→ Reperfusion Injury\n← Heparin Bridge ← Imaging Guidance\n• Critical failure : Bypassed essential anticoagulation and\nimaging steps\n• Risk ampliﬁcation : Increased limb loss probability with\ndelayed anticoagulation\n/three.tnum./two.tnum./four.tnum Error case /four.tnum: porphyria cutanea tarda (PCT)\n• Pathway of reasoning :\nPCT → Phlebotomy Required → Normal Iron Stores\n\n \nModel’s Focus\n→ Hydroxychloroquinine\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\n• Critical failure : Equated serum ferritin with total body iron\nstores\n• Risk ampliﬁcation : Increased risk of cirrhosis from persistent\niron overload\n/three.tnum./two.tnum./five.tnum Error case /five.tnum: enzyme kinetics\n• Pathway of reasoning :\nTissue Distribution → Low Vmax Assumption\n\n \nModel’s Focus\n→ Metabolic Dysregulation\n← Hexokinase Signature ← Low Km/High Vmax\n• Critical failure : Confused hexokinase (high-aﬃnity/high-\ncapacity) with glucokinase kinetics\n• Risk ampliﬁcation : Error in predicting glucose utilization\nrates\n/three.tnum./two.tnum./six.tnum Error case /six.tnum: preterm PDA management\n• Pathway of reasoning :\nPreterm Birth → PDA → Surgical Ligation\n\n \nModel’s Focus\n← Indomethacin Window ← 5-Week Age\n• Critical failure : Overestimated surgical urgency in stable\ninfant\n• Risk ampliﬁcation : Higher complication rate vs medical\nmanagement\n/three.tnum./two.tnum./seven.tnum Error case /seven.tnum: niacin ﬂushing\n• Pathway of reasoning :\nNiacin Use → Flushing → Fenoﬁbrate Switch\n\n \nModel’s Focus\nPGD2 Pathway\n←−−−−−−−− Aspirin Prophylaxis\n• Critical failure : Misattributed prostaglandin-mediated\nﬂushing to rare neoplasms\n• Risk ampliﬁcation : Reduced lipid control eﬃcacy with\nunnecessary agent switch\n/three.tnum./three.tnum Analysis of diagnostic reasoning errors\nWe found recurring patterns of diagnostic reasoning errors. A\nkey ﬁnding across multiple cases was anchoring bias, with ﬁxation\non an initial diagnosis (e.g., duodenal atresia in Case 1, CTEPH\nin Case 2) and subsequently failed to adequately incorporate\nconﬂicting evidence. This was often compounded by conﬁrmation\nbias, with selectively attending to information supporting the\ninitial impression while dismissing contradictory data (e.g., normal\nferritin in the context of suspected PCT in Case 4).\nSeveral cases demonstrated errors related to disease pathway\nunderstanding. In Case 2, feature binding led to misattributing\nwall remodeling as the primary pathology rather than recognizing\nit as a consequence of another underlying condition (vasculitis).\nA similar error in Case 5 involved confusing enzyme kinetics,\nmisidentifying hexokinase as glucokinase, highlighting a lack of\nunderstanding of the speciﬁc biochemical pathways.\nOmission bias was evident in Case 3, where crucial steps like\nanticoagulation and imaging were bypassed in the rush to surgery\nfor acute limb ischemia. This suggests a failure to consider all\nnecessary elements of the diagnostic and treatment pathway. In\ncontrast, Case 6 demonstrated potential commission bias with the\noverestimation of surgical urgency in a stable infant with a PDA,\npotentially exposing the patient to unnecessary risk.\nFinally, Case 7 illustrated an error in attribution, misattributing\nniacin-induced ﬂushing to rare neoplasms instead of recognizing\nit as a prostaglandin-mediated eﬀect. This misattribution led to an\nunnecessary and detrimental change in lipid-lowering medication.\nThese ﬁndings emphasize the importance of recognizing and\nmitigating cognitive biases and ensuring a thorough understanding\nof disease pathways to improve diagnostic accuracy and patient\nsafety. The quantiﬁed risk ampliﬁcations associated with each error\nunderscore the potential clinical impact of these reasoning ﬂaws.\nAnother error we think is important to address is the one\nfound in the ﬁrst Case E1. If you follow the reasoning trace\nof the model it actually decides on A Abnormal migration of\nventral pancreatic bud (correct) but outputs B, Complete failure of\nproximal duodenum to recanalize (false) . The model ﬁrst reason\nand then outputs the answer. Although this only happened a single\ntime, we want to highlight this because it shows that the reasoning\nmight diﬀer from the response. This means that in a clinical setting\nit is wise to have both model reasoning and model output in order\nto minimize the risk of errors. If a clinician would have access to\nboth reasoning and output, the reasoning might help the clinician\nﬁnd the right diagnosis but having only access to the model output\nwould lead to a potential misdiagnosis. This highlights the beneﬁt\nor R1, which shows reasoning patterns, which are hidden in similar\nreasoning models such as O1 and O3 made by Open AI.\n/three.tnum./four.tnum Statistical analysis of reasoning lengths\nin correct vs. incorrect responses\nWe conducted an independent two-sample Welch’s t-test\nto compare the average reasoning length between correct and\nincorrect answers, as the groups exhibited unequal variances,\ncorrect answers ( n = 93) averaged 3,648 characters (SD = 2,132;\nvariance = 4.55 × 10), incorrect answers ( n = 7) averaged 8,118\ncharacters (SD = 4,277; variance = 1.83 ×10).\nThe analysis revealed a statistically signiﬁcant diﬀerence ( t =\n–2.74, p = 0.032), with incorrect answers containing substantially\nlonger reasoning (mean = 8,118 characters) compared to correct\nanswers (mean = 3,648 characters). The eﬀect size was very large:\nCohen’sd = 1.93, indicating that an average incorrect explanation\nis nearly two pooled standard deviations longer than a correct\none. The negative t-value reﬂects the directional diﬀerence, where\nincorrect responses were consistently lengthier.\nThe marked disparity in reasoning length suggests that\nextended explanations may signal uncertainty or reﬂect attempts\nto rationalize incorrect conclusions. Shorter responses (e.g., under\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nTABLE /one.tnum Distribution of reasoning errors in /one.tnum/zero.tnum/zero.tnum clinical cases.\nError type Count Percentage Exemplar\ncase\nProtocol\nmisapplication\n2 2% Acute limb\nischemia\nmanagement\nAnchoring bias 1 1% Neonatal bilious\nvomiting\nEtiology-\nconsequence\nconfusion\n1 1% Pulmonary artery\nﬁbrosis\nLab value\noverinterpretation\n1 1% Porphyria cutanea\ntarda\nIsoform\nmisunderstanding\n1 1% Enzyme kinetics\nOverinvestigation\ntendency\n1 1% Niacin-induced\nﬂushing\n5,000 characters) were strongly associated with accuracy, providing\na practical threshold for assessing conﬁdence in model-generated\nanswers. This metric could enhance user transparency by ﬂagging\nverbose outputs as potential indicators of unreliability.\n/three.tnum./five.tnum Analysis of reasoning success\nAlthough our eﬀort focused on reasoning errors in most cases\nthe model was successful with 93% accuracy. In our analysis of the\nsuccessful cases we found that the medical reasoning of the model\nwas sound.\n/three.tnum./five.tnum./one.tnum Classiﬁcation as medical reasoning\nThe reasoning by the R1 model would likely qualify as\nmedical reasoning. The thought process demonstrates key elements\nof clinical decision-making demonstrated here on case C1 (see\nTable 1):\n/three.tnum./five.tnum./two.tnum Correct case /one.tnum: a /two.tnum/three.tnum-year-old pregnant\nwomen at /two.tnum/two.tnum weeks gestation presents with\nburning upon urination\nThe model identiﬁes that the patient is a pregnant woman at\n22 weeks gestation with signs of a lower urinary tract infection. It\nsystematically evaluates the safety and eﬃcacy of each antibiotic\noption in pregnancy: it rules out ampicillin due to common\nresistance, ceftriaxone because it is overly broad for simple cystitis,\nand doxycycline because it is contraindicated in pregnancy. It\nconcludes that nitrofurantoin is safe and eﬀective in the second\ntrimester, making option D the correct choice.\n• Data synthesis: Systematically reviews the patient’s history,\nsymptoms, and exam ﬁndings.\n• Diﬀerential diagnosis: Rules out pyelonephritis (absence of\nCV A tenderness) and narrows to cystitis.\n• Application of guidelines: Considers pregnancy-speciﬁc risks\nand antibiotic safety proﬁles.\n• Critical appraisal of options: Evaluates drug eﬃcacy,\nresistance patterns, and contraindications.\n• Risk-beneﬁt analysis: Balances fetal safety (e.g., avoiding\ndoxycycline) with maternal treatment eﬃcacy.\n/three.tnum./five.tnum./three.tnum Structured clinical approach\n• Begins with clinical context: Identiﬁes pregnancy as a critical\nfactor inﬂuencing management.\n• Prioritizes diagnosis: Distinguishes cystitis from\npyelonephritis based on exam ﬁndings (no CV A tenderness).\n• Antibiotic stewardship: Avoids overly broad agents\n(ceftriaxone) for uncomplicated cystitis and considers\nresistance patterns (ampicillin’s limitations).\n• Guideline adherence: Correctly applies recommendations for\nnitrofurantoin use in pregnancy (safe in second trimester,\navoided in ﬁrst/third).\n/three.tnum./five.tnum./four.tnum Reasoning process\nThe reasoning follows a hypothetico-deductive model common\nin clinical medicine:\n• Information gathering: Patient demographics, symptoms,\nvital signs, and exam ﬁndings.\n• Problem representation: “Pregnant woman with dysuria, no\nsystemic signs, likely cystitis.”\n• Diﬀerential diagnosis: Prioritizes cystitis over pyelonephritis.\n• Treatment selection:\n– Elimination: Doxycycline (contraindicated).\n– Comparison of remaining options: Ampicillin\n(resistance), ceftriaxone (overly broad), nitrofurantoin\n(guideline-supported).\n– Final decision: Nitrofurantoin, justiﬁed by safety in the\nsecond trimester and eﬃcacy for uncomplicated cystitis.\nWe believe that the structured reasoning approach with high\naccuracy shows the usefulness of DeepSeek R1 in the healthcare\nsector. The sound reasoning combines with an open source model\ngives a clear path forward for integrating this in the healthcare\ndomain.\n/three.tnum./six.tnum Specialty-level accuracy\nMapping the seven erroneous answers (E1–E7) to their\nrespective clinical domains revealed no consistent clustering of\nmistakes. Only seven of the thirty specialties represented in the\n100-item set recorded any error, and in ﬁve of those the domain\ncontained a single question (Physiology, Neonatology) or two\nquestions (Pharmacology), such that a lone miss reduced accuracy\nto 0% or 50%. Among larger categories the system remained highly\nreliable: Pediatrics 86% (6/7 correct), Surgery 80% (4/5), and\nPulmonology 75% (3/4). All remaining 23 disciplines–including\nNeurology (12 items), Infectious Disease (10), and Cardiology\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nTABLE /two.tnum Examples of responses with a focus on incorrect responsesand reasoning.\nQuestion Strengths Weaknesses Diagnosis R/one.tnum answer\nC1. 23-year-old\npregnant woman\nwith UTI\n- Identiﬁes cystitis based on symptoms.\n- Recognizes need for treatment.\n- Rules out inappropriate options.\n- Selects Nitrofurantoin.\n- Spends time on Cephalexin.\n- Could be more concise.\nCystitis Cystitis\nCorrect\nC2. 3-month-old\nwith SIDS\n- Correctly identiﬁes SIDS.\n- Recalls prevention strategies.\n- Evaluates answer choices.\n- Recognizes “Back to Sleep\" campaign.\n- None signiﬁcant. SIDS SIDS Correct\nC3. 20-year-old\nwoman with\nmenorrhagia\n- Interprets lab results.\n- Considers diﬀerentials.\n- Recognizes family history.\n- Identiﬁes vWD.\n- Brieﬂy considers Hemophilia A.\n- Mentions bleeding time.\nVon Willebrand\ndisease\nVon Willebrand\ndisease\nCorrect\nC4. 40-year-old\nzookeeper with\npancreatitis\n- Recalls causes of pancreatitis.\n- Identiﬁes scorpion sting.\n- Considers other options.\n- None signiﬁcant. Scorpion sting Scorpion sting\nCorrect\nE1. 3-week-old\nwith bilious\nvomiting\n- Recognizes bilious vomiting as obstruction.\n- Considers relevant diﬀerentials.\n- Understands embryology.\n- Initially rules out duodenal atresia.\n- Fixates on “complete\" in option B.\n- Overemphasizes malrotation.\n- Repetitive explanation.\nAbnormal\nmigration of ventral\npancreatic bud\nDuodenal atresia\nIncorrect The models\nreason correctly but\ngives out the wrong\nresponse\nE2. 58-year-old\nwoman\npost-surgery\n- Identiﬁes risk factors.\n- Initially leans toward thromboembolism.\n- Considers each option.\n- Understands CTEPH.\n- Gets ﬁxated on histological composition.\n- Repetitive reasoning.\nThromboembolism Pulmonary\nHypertension\nIncorrect\nE3. 68-year-old\nman with leg pain\n- Correctly identiﬁes acute limb ischemia.\n- Recognizes atrial ﬁbrillation as a risk factor.\n- Applies Rutherford classiﬁcations to evaluate\nseverity.\n- Understands that urgent management is\nneeded to salvage limb.\n- Incorrectly prioritizes deﬁnitive treatment over\nimmediate anticoagulation with heparin.\n- Incorrectly states that thrombolysis is\ncontraindicated in embolic events.\nHeparin drip Surgical\nthrombectomy\nIncorrect\nE4. 48-year-old\nwoman with\nphotosensitive\nrash\n- Correctly identiﬁes porphyria cutanea tarda\n(PCT) as the most likely diagnosis.\n- Recognizes the signiﬁcance of family history,\ndark urine, and photosensitivity.\n- Considers other porphyrias (variegate\nporphyria).\n- Appropriately rules out liver transplantion and\nthalomide as standard therapies, understands\nthe role of phlebotomy and hydroxychloroquine\nin PCT treatment.\n-Places excessive emphasis on normal ferritin\nlevels, overlooking that phlebotomy can still\ninduce remission even with normal iron stores.\n- Brieﬂy considers unrelated conditions\n(epidermolysis bullosa, pseudoporphyria).\n- Incorrectly states that thalidomide is used in\nrefractory cases of PCT.\nBegin phlebotomy\ntherapy\nBegin oral\nhydroxychloroquine\ntherapy\nIncorrect\nE5. Enzyme\nKinetics\n- Correctly relates X to Km and Y to Vmax.\n- Correctly identiﬁes the enzyme as hexokinase.\n- Understands the properties of hexokinase (low\nKm).\n- Correctly identiﬁes that the enzyme in\nquestion phosphorylates glucose.\n- Overthinks the Vmax, failing to deﬁnitively\nconclude whether it’s high or low, causing\nconfusion in the ﬁnal step.\n-Confuses the concepts of Vmax and Km,\nincorrectly stating that a low Km indicates a\nhigh Vmax.\n- Incorrectly states that hexokinase has a higher\nVmax than glucokinase and incorrectly states\nthat hexokinase is inhibited by\nglucose-6-phosphate under these experimental\nconditions.\n- It overthinks minor details and loses track of\nthe simpler hallmark diﬀerence\nLow X and low Y Low X and high Y\nIncorrect\nE6. 5-week-old\ninfant with a\nmurmur\n- Correctly identiﬁes PDA as the most likely\ndiagnosis.\n- Recognizes the signiﬁcance of preterm birth.\n- Understands the implications of the\ncontinuous murmur.\n- Considers the infant’s age and feeding changes.\n- Knows the general management options for\nPDA (Indomethacin, surgery).\n- Incorrectly dismisses indomethacin as an\noption based on age alone without considering\nthe full clinical picture\n- Overthinks the feeding changes and weight\ngain.\n- Overthinks age and arrives at the wrong\nﬁrst-line treatment in an otherwise stable infant.\nIndomethacin\ninfusion\nSurgical ligation\nIncorrect\n(Continued)\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nTABLE /two.tnum (Continued)\nQuestion Strengths Weaknesses Diagnosis R/one.tnum answer\nE7. 53-year-old\nwoman with\nﬂushing and\nitching\n- Correctly identiﬁes niacin-induced ﬂushing as\nthe most likely cause.\n- Considers other possibilities (carcinoid,\npheochromocytoma, allergy).\n- Understands the limitations of statins and\nﬁbrates.\n- Recognizes the need for LDL management.\n- Incorrectly prioritizes switching to fenoﬁbrate\nover managing niacin side eﬀects.\n- Overly focuses on the possibility of carcinoid\nsyndrome despite the low likelihood.\n- Fails to recognize that taking aspirin 30\nminutes before niacin can signiﬁcantly reduce\nﬂushing.\nAdminister\nibuprofen\nSwitch niacin to\nfenoﬁbrate\nIncorrect\nTABLE /three.tnum Summary of reasoning errors across cases.\nCase Error type Model answer Key reasoning ﬂaw\nE1. Neonatal vomiting Anchoring bias B (duodenal atresia) Overprioritized textbook presentation despite incompatible\ntimeline\nE2. Respiratory failure Etiology confusion C (Pulmonary hypertension) Misattributed vascular remodeling to primary disease\nE3. Limb ischemia Protocol violation C (surgery) Skipped anticoagulation step in Rutherford IIb\nE4. PCT management Lab misinterpretation D (hydroxychloroquine) Overvalued serum ferritin over hepatic iron\nE5. Enzyme kinetics Isoform confusion C (High Vmax) Confused hexokinase/ glucokinase kinetic proﬁles\nE6. PDA management Therapeutic window error C (Surgery) Misjudged indomethacin eﬃcacy in preterms\nE7. Niacin ﬂushing Overinvestigation D (Fenoﬁbrate) Ignored temporal drug-eﬀect relationship\n(7)–achieved 100% accuracy. Accuracy and errors are detailed in\nTable 2.\nTo test whether the observed distribution deviated from a\nuniform 7% error rate, we applied a χ 2 goodness-of-ﬁt test,\nobtaining χ 2 = 4.8 with p = 0.31; however, the result is tentative\nbecause > 75% of cells contained zero errors, violating standard χ 2\nassumptions. Overall, the data provide no convincing evidence of a\ndiscipline-speciﬁc weakness. The apparent dips are compatible with\nrandom variation in a small sample, and larger, domain-targeted\ntest sets will be required to identify any genuine specialty-level\nperformance gaps.\n/three.tnum./seven.tnumPost-hoc audit of ChatGPT-/four.tnumo\nTo benchmark against a non-research-grade, commercially\ndeployed LLM, we queried ChatGPT-4o via the public web interface\non 28 May 2025, against the seven error vignettes. The system was\ninstructed to reveal its chain of thought inside <thinking> tags\nand then commit to a ﬁnal answer. This was the full prompt:\nPlease answer this question using a format where you ﬁrst\nreason inside <thinking> tags, after thinking you give an output.\nReason through chain of thought.\n/three.tnum./seven.tnum./one.tnum Method\nA single prompt was issued per vignette; no temperature\nor system-level modiﬁcations were possible in the consumer\nUI. A physician reviewer scored the disclosed reasoning for (i)\npresence of clinical problem representation, (ii) generation of\na pathophysiology-grounded diﬀerential, and (iii) adherence to\nguideline logic when selecting a management step.\n/three.tnum./seven.tnum./two.tnum Findings\nSix of seven chains satisﬁed all three criteria, demonstrating\nrecognizable medical reasoning. The sole exception (niacin\nﬂushing) exhibited a sound diagnostic path but erred at the ﬁnal\ntherapeutic choice, mirroring the pattern seen in DeepSeek R1\n(\nTable 3).\n/three.tnum./seven.tnum./three.tnum Implications\nPrompting was suﬃcient to give a non reasoning model\nreasoning steps that could be evaluated for medical reasoning. This\nis promising since it could be a step forward for explainability for\nnon-reasoning LLMs. The model solved 6/7 questions which gives\nan accuracy of 85.71%. This accuracy is hard to compare since the\nsubset is based on questions that DeepSeek R1 failed however it is\nbelow the average accuracy for the entire dataset for DeepSeek R1\n(93%).\nThe close qualitative parity between ChatGPT-4o and\nDeepSeek R1 in terms of medical reasoning suggests that our error\ntaxonomy captures generalizable failure modes of contemporary\nLLMs.\n/four.tnum Discussion\nThis study provides a detailed analysis of the medical reasoning\ncapabilities of DeepSeek R1, revealing both its strengths and\nlimitations in handling complex clinical scenarios. While the model\ndemonstrates high overall diagnostic accuracy (93%), our in-depth\nerror analysis highlights speciﬁc areas where its reasoning leads\nto errors in clinical assessment see\nFigure 1 and Tables 3–6. These\nﬁndings have several important implications for the development\nand deployment of LLMs in healthcare.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nFIGURE /one.tnum\nLength of reasoning and correctness.\n/four.tnum./one.tnum A note on anthropomorphization of\nLLMs\nIn this work we evaluated the reasoning of LLMs and\nhighlighted cognitive errors in its reasoning. There is a speculative\nnature to this since we assign human error mechanism to an\nLLM system. We want to be clear that the bias we found in\nreasoning is dependent on the analysis of the reasoning text and\nwe provide all model reasoning outputs as supplementary material.\nThinking about how we reason and how LLMs reason can be\nfruitful to improve our own reasoning process even though it might\nlead to bias and potential misunderstanding of the technology.\nThe language we use to describe the reasoning and errors is\nmade to help human understanding and we hope that this does\nnot lead to anthropomorphization of these systems. We believe\nthat LLMs should be viewed as tools but language regarding\nhuman cognition can help increase our understanding of their\nfunctioning.\n/four.tnum./two.tnum Opening the black box\nDeep learning models including LLMs have been accused\nof being black box algorithms where the inner workings of the\nmodels are shielded from view (\nWang Y. et al., 2024 ). This\nhas limited their use in high risk areas such as healthcare\nwhere understanding of model outputs is essential for safe\nimplementation. Open reasoning models such as R1 shows a path\nforwards by being transparent regarding reasoning which has\nthe potential of making the model safer to use in a high risk\nsetting.\n/four.tnum./three.tnum Errors in medical reasoning\nErrors that took place were overall a result of thinking errors\nwhere the model focused too much attention on details of a\nproblem and lacked necessary understanding of medical protocols.\nThese errors can be viewed similar to mistakes made by a human\nwith medical knowledge and ability to reason about that knowledge\nmaking a mistake. That is, a doctor misdiagnosing a patient rather\nthan a human without medical knowledge guessing the answer on a\nmedical test. This is an important distinction because the diﬀerence\nbetween the two is years of clinical schooling and medical reasoning\nability. As such we view these errors as promising and believe\nthat training techniques and new reasoning models will enhance\nthis already fairly adequate medical reasoning ability. Our ﬁndings\nthat the length of reasoning was strongly linked to correctness is\ninteresting and can be helpful for improving the usefulness of these\nmodels in a clinical setting. By simply using the length of reasoning\nas a reverse certainty score, we can help a clinician make sense\nof the models reasoning and even automate double checking, by\nrerunning long reasoning attempts with an added prompt that the\nreasoning is likely incorrect.\n/four.tnum./four.tnum Lengths of reasoning and errors\nOne interesting ﬁnding was the strong correlation between the\nlength of output and errors where longer results were more likely\nto be incorrect. We did a qualitative analysis and found that longer\noutputs seemed to suﬀer from “overthinking,\" where the model\ngets confused and uses additional tokens to think even though\nthe thinking is not helpful to improve the quality of the results.\nA concrete ﬁnding from our paper is that showing the reasoning\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nTABLE /four.tnum Distribution of medical questions by specialty.\nSpecialty Number of questions Percentage\nGynecology (OBGYN) 6 6%\nPediatrics 7 7%\nGenetics 7 7%\nCardiology 7 7%\nNeurology 12 12%\nHematology 7 7%\nGastroenterology 7 7%\nPulmonology 4 4%\nNephrology 6 6%\nUrology 3 3%\nInfectious disease 10 10%\nOncology 7 7%\nSurgery 5 5%\nDermatology 3 3%\nEndocrinology 5 5%\nPsychiatry 3 3%\nOrthopedics 2 2%\nEmergency medicine 3 3%\nMedical ethics 1 1%\nBiostatistics/epidemiology 3 3%\nPharmacology 2 2%\nENT (otolaryngology) 4 4%\nPathology 2 2%\nImmunology 1 1%\nToxicology 1 1%\nMetabolic disorders 2 2%\nResearch methods 1 1%\nPhysiology 1 1%\nPatient safety 1 1%\nNeonatology 1 1%\nTotal 100 100%\nlength prominently could be a promising way to help clinician\nevaluate the quality of model reasoning.\n/four.tnum./five.tnum Quality of medical reasoning\nOverall we found that the model made few mistakes in its\nreasoning and the reasoning was medical in nature. The model\ncould reason regarding medical scenarios and overall the reasoning\nof the model was excellent. This is promising because it shows that\nmedical reasoning is possible through LLMs and that the reasoning\nTABLE /five.tnum Risk-ranked clinical impact of reasoning errors.\nCase Error (short\nlabel)\nRisk\nlevel\nPotential patient\nharm if followed\nE1 Anchoring on\nduodenal atresia\nHigh Missed annular pancreas\ndelayed surgery, bowel\nperforation, neonatal sepsis;\nmortality rises hour-to-hour.\nE3 Skipping heparin\nstep\nHigh Thrombus propagation\nduring limb-ischaemia\nwork-up irreversible limb loss\nor systemic emboli within\nhours.\nE4 Overvaluing ferritin Moderate Persistent porphyria lesions\nand hepatic iron overload\nscarring, cirrhosis risk;\nmorbidity high, mortality\nlower.\nE6 Premature PDA\nligation\nModerate Avoidable surgical and\nanesthetic risk when\nindomethacin could suﬃce;\npotential vocal-cord palsy,\nbleeding.\nE2 Treating eﬀect as\ncause\nLow Vasculitis left untreated while\nmanaging “CTEPH”\nunchecked inﬂammation,\nright-heart failure, fatal\npulmonary hemorrhage.\nForensic question. Patient is\nalready diseased.\nE5 Hexokinase/\nglucokinase mix-up\nLow Purely biochemical slip; no\ndirect bedside decision tied to\nit, negligible immediate harm.\nE7 Abandoning niacin\ninstead of ﬁxing\nﬂushing\nLow LDL undertreated for\nmonths-years incremental\nlong-term CV risk; little\nshort-term danger.\nis already functional and can be helpful in the healthcare sector if\nintegrated in a safe way.\n/four.tnum./six.tnum The future of LLMs in healthcare\nAs within other areas of healthcare, expert clinicians time\nbecome a bottleneck when evaluating LLMs. As models improve\nand show signs of medical reasoning it seems worthwhile to use\nLLMs to improve LLMs in healthcare. This seemingly paradoxical\nway of working is actually in line with how large AI labs work\nto improve LLMs (\nAnthropic, 2023 ). A capable LLM model can\nbe used to reﬁne and improve data that can be used to train\nanother LLM and over time data quality improves as well as\nmodel performance. For larger medical datasets where human\nevaluation is simply unfeasible when thousand or millions of\nquestions are evaluated this technique becomes necessary. Having\na gold standard of human evaluation with lesser standards for\nevaluation using LLMs seems to be a possible way forward. As\nin other areas where LLMs are highly performant such as code\ngeneration, we should start to accustom ourself to a world where\nclinicians supervise AI systems that reason independently. In the\nfuture the job of the clinician might be to supervise an AI system\nthat independently gives suggestions for diagnosis and treatment.\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nTABLE /six.tnum Correct vs. incorrect responses by specialty and resulting\naccuracy.\nSpecialty Questions\n(n)\nCorrect Incorrect Accuracy\nGynecology\n(OB/GYN)\n6 6 0 100%\nPediatrics 7 6 1 86%\nGenetics 7 7 0 100%\nCardiology 7 7 0 100%\nNeurology 12 12 0 100%\nHematology 7 7 0 100%\nGastroenterology 7 7 0 100%\nPulmonology 4 3 1 75%\nNephrology 6 6 0 100%\nUrology 3 3 0 100%\nInfectious disease 10 10 0 100%\nOncology 7 7 0 100%\nSurgery 5 4 1 80%\nDermatology 3 2 1 67%\nEndocrinology 5 5 0 100%\nPsychiatry 3 3 0 100%\nOrthopedics 2 2 0 100%\nEmergency\nmedicine\n3 3 0 100%\nMedical ethics 1 1 0 100%\nBiostatistics/\nepidemiology\n3 3 0 100%\nPharmacology 2 1 1 50%\nENT\n(Otolaryngology)\n4 4 0 100%\nPathology 2 2 0 100%\nImmunology 1 1 0 100%\nToxicology 1 1 0 100%\nMetabolic\nDisorders\n2 2 0 100%\nResearch\nMethods\n1 1 0 100%\nPhysiology 1 0 1 0%\nPatient Safety 1 1 0 100%\nNeonatology 1 0 1 0%\nTotal* 100 93 7 93%\n/four.tnum./seven.tnum Improving human medical reasoning\nErrors in medical reasoning by humans leads to thousands\nof deaths and injuries each year ( Makary and Daniel, 2016 ).\nAs such improving clinicians ability to reason might be one of\nthe most important tasks for improving healthcare outcomes.\nThe medical reasoning already available in the R1 model can\ntake years for a clinician to acquire through medical training\nand mentorship and thus using models such as R1 to improve\nclinicians reasoning skills is one potential use of this technology.\nThis is also in line with a human in the loop approach which\nimproves safety while being aligned with regulatory bodies views\non AI in healthcare (\nParliament and of the European Union,\n2024).\n/four.tnum./eight.tnum Improving clinical reasoning\nThe model was evaluated with a simple prompt and could likely\nimprove through several methods.\n1. Retrieval augmented generation (RAG) for improved clinical\nreasoning. By using a RAG system the performance of the\nsystem would likely improve by access to clinical guidelines and\nother medical texts.\n2. Specialization in prompting and documents. In a clinical\ncontext, medical professionals usually reason about a smaller\nsubset of clinical knowledge. By dividing the problem of medical\nreasoning by medical specialty; prompts and knowledge could\nbe used to solve these subproblem more appropriately.\n3. Fine tuning on medical reasoning. Improvements to medical\nreasoning would likely result from ﬁne-tuning on medical\nreasoning data. Recent advancements in reinforcement learning\ntraining for text (\nDeepSeek-AI et al., 2025 ) could be useful in this\nregard.\n/four.tnum./nine.tnum Use in a clinical setting\nAlthough the model had errors, overall the reasoning was\nsound from a medical perspective, as such we believe that these\nmodels can be useful in the medical domain and we think it\nis time for healthcare practitioner to start experimenting with\nthese technologies. As long as healthcare workers are aware of\nlimitations, we believe that use of these systems could help\nimprove patient outcomes. For many clinicians especially in\nspecialized care settings the work can be lonely and there might\nnot be colleagues with similar experience to discuss medical\ndiagnostics. Even though healthcare decisions should always\nbe the responsibility of a human, we believe that reasoning\nmodels such as R1 can help clinicians in their diagnostic\nassessments.\nAs clinicians we need to be creative in ﬁnding safe ways to\nuse this technology in a clinical settings. Both for clinician facing\nand patient facing interfaces there are likely useful ways to use\nthis technology in a way that is helpful for improving health\noutcomes.\n/four.tnum./one.tnum/zero.tnum Extending the evaluation framework:\nbroader applicability and enhanced validity\nOur reasoning analysis framework, applied to DeepSeek R1, is\nbroadly adaptable to all reasoning models that contain reasoning\nFrontiers in Artiﬁcial Intelligence /one.tnum/two.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\ntraces and all LLMs that can be prompted through chain-of-\nthought to show reasoning traces. Our small experiment with GPT-\n4o shows that a simple prompting technique can be used with a\nstandard LLM to allow it to be evaluated in this way.\n/four.tnum./one.tnum/one.tnum Limitations\nThis study has several limitations. First, the evaluation is based\non a limited, albeit diverse, set of clinical cases from a single\ndataset. While MedQA provides a valuable benchmark, it may not\nfully capture the complexity of real-world clinical practice. Second,\nour analysis focuses on one speciﬁc LLM, DeepSeek R1. While\nthis model represents a state-of-the-art approach to reasoning-\nenhanced LLMs, the ﬁndings may not be generalizable to all\nLLMs, especially those with diﬀerent architectures or training\nmethodologies. Third, the expert validation is still subject to the\ninherent limitations of human judgment and potential biases.\nAnother limitation is that we only had a single medical expert\nevaluate the medical reasoning of the model.\n/four.tnum./one.tnum/two.tnum Future research directions\nBuilding on the reasoning failures identiﬁed in this study, we\noutline six targeted avenues to improve the clinical robustness of\nlarge language models (LLMs).\n1. Domain-Speciﬁc ﬁne-tuning with curated medical reasoning\ncorpora\nExtending the work by\nWu et al. (2025) on creating high-\nquality, explanation-rich datasets of medical reasoning, we\nadvocate assembling case collections with chain-of-thought\nmedical reasoning. Fine-tuning domain-specialized LLMs on\nsuch corpora should reduce diagnostic bias and increase factual\ncompleteness.\n2. Retrieval-augmented generation (RAG) over authoritative\nguidelines\nLinking LLMs to continuously updated medical sources via\nlightweight RAG pipelines can ground outputs in best evidence,\nconstrain hallucinations, and expose deviations from established\ncare pathways.\n3. Multi-evaluator frameworks for reliable assessment\nAdopting panels of at least three independent clinicians,\nstandardized rubrics, and explicit inter-rater metrics (e.g.\nCohen’sκ ) will yield more robust estimates of reasoning quality.\nTargeted training on LLM error taxonomies can further align\nevaluators.\n4. Prompt engineering for improvement of reasoning\ncapabilities\nChain-of-thought prompting sparked the rise of reasoning-\noriented models. Systematic exploration of additional\nprompting strategies–especially those requiring no retraining–\ncould further enhance reasoning and should be rigorously\nevaluated.\n5. Hybrid human-AI workﬂows\nAn open research question is how best to integrate these tools\ninto clinical practice. Future systems could auto-ﬂag verbose or\nlow-conﬁdence chains of thought for clinician review, balancing\nautomation with expert oversight.\n6. LLM-as-judge evaluation of reasoning ability\nHuman evaluation is costly, whereas text generation is relatively\ncheap. Leveraging LLM-as-judge techniques (\nCroxford et al.,\n2025) may provide scalable, low-cost assessment of reasoning\nquality–especially when tightly coupled to dataset creation and\nmodel training.\n/five.tnum Conclusion\nThis study shows that DeepSeek R1 is capable of a form of\nmedical reasoning as evaluated by analysis by human evaluation\non a subset ( n = 100) of the MedQA benchmark. The model\nhad an accuracy of 93% and both correct and incorrect cases\nshowed signs of medical reasoning. Using open reasoning models\nin healthcare improves explainability over non-reasoning models\nand we encourage continued investigation of how these models can\nbe used to improve the future of healthcare.\nData availability statement\nThe datasets presented in this study can be found in\nonline repositories. The names of the repository/repositories and\naccession number(s) can be found at:\nhttps://huggingface.co/\ndatasets/birgermoell/medical-reasoning.\nAuthor contributions\nBM: Funding acquisition, Conceptualization, Validation,\nResources, Writing – review & editing, Writing – original\ndraft, Investigation, Supervision, Project administration, Formal\nanalysis, Data curation, Methodology, Software, Visualization.\nFS: Data curation, Project administration, Writing – original\ndraft, Methodology, Formal analysis, Writing – review & editing.\nSA: Formal analysis, Writing – review & editing, Methodology,\nWriting – original draft, Investigation.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research and/or publication of this article.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that Gen AI was used in the creation of\nthis manuscript. Generative AI was used to assist in layout of tables\nin the paper.\nFrontiers in Artiﬁcial Intelligence /one.tnum/three.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAnthropic (2023). Model Card and Evaluations for Claude Models. Technical Report .\nSan Francisco, CA: Anthropic PBC. Available online at: https://www-cdn.anthropic.\ncom/ﬁles/4zrzovbb/website/Bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf\nArora, R. K., Wei, J., Hicks, R. S., Bowman, P., Quiñonero-Ca ndela, J., Tsimpourlas,\nF., et al. (2025). Healthbench: evaluating large language models t owards improved\nhuman health. arXiv [Preprint]. arXiv:2505.08775. doi: 10.48550/arXiv.2505. 08775\nBarbosa Chaves, A., Sampaio Moura, A., de Faria, R. M. D., and C ayres Ribeiro, L.\n(2022). The use of deliberate reﬂection to reduce conﬁrmation bias among orthopedic\nsurgery residents. Sci. Med. 32:42216. doi: 10.15448/1980-6108.2022.1.42216\nBenjamens, S., Dhunnoo, P., and Meskó, B. (2020). The state o f artiﬁcial\nintelligence-based fda-approved medical devices and algorithm s: an online database.\nnpj Digit. Med . 3:118. doi: 10.1038/s41746-020-00324-0\nBorg, A., Jobs, B., Huss, V., Gentline, C., Espinosa, F., Ruiz, M ., et al. (2024).\nEnhancing clinical reasoning skills for medical students: a qua litative comparison\nof LLM-powered social robotic versus computer-based virtual patients within\nrheumatology. Rheumatol. Int. 44, 3041–3051. doi: 10.1007/s00296-024-05731-0\nBoushehri, E., Soltani Arabshahi, K., and Monajemi, A. (2015 ). Clinical reasoning\nassessment through medical expertise theories: past, present and future directions.\nMed. J. Islamic Repub. Iran 29:222.\nCharlin, B., Tardif, J., and Boshuizen, H. (2000). Scripts and medical diagnostic\nknowledge: theory and applications for clinical reasoning instr uction and research.\nAcad. Med. 75, 182–190. doi: 10.1097/00001888-200002000-00020\nCrescitelli, M. E. D., Ghirotto, L., Artioli, G., and Sarli, L. (20 19). Opening the\nhorizons of clinical reasoning to qualitative research. Acta Biomed. 90, 8–16.\nCroxford, E., Gao, Y., First, E., Pellegrino, N., Schnier, M., Caskey, J., et al. (2025).\nAutomating evaluation of AI text generation in healthcare wit h a large language model\n(LLM)-as-a-judge. medRxiv. doi: 10.1101/2025.04.22.25326219\nCusters, E. (2013). Medical education and cognitive contin uum theory: an\nalternative perspective on medical problem solving and clinical re asoning. Acad. Med.\n88, 1074–1080. doi: 10.1097/ACM.0b013e31829a3b10\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., et al. (2025).\nDeepseek-r1: Incentivizing reasoning capability in LLMa via r einforcement learning.\narXiv [Preprint]. arXiv:2501.12948. doi: 10.48550/arXiv.2501. 12948\nDurning, S., and Artino, A. (2011). Situativity theory: a per spective on how\nparticipants and the environment can interact: Amee guide no. 52. Med. Teach . 33,\n188–199. doi: 10.3109/0142159X.2011.550965\nDurning, S., Jung, E., Kim, D.-H., and Lee, Y.-M. (2024). Tea ching\nclinical reasoning: principles from the literature to help improve instruction\nfrom the classroom to the bedside. Korean J. Med. Educ . 36, 145–155.\ndoi: 10.3946/kjme.2024.292\nEuropean Parliament and Council of the European Union (2024). Regulation (EU)\n2024/1689 of the European Parliament and of the Council of 13 June 202 4 Laying\nDown Harmonised Rules on Artiﬁcial Intelligence and Amending Regul ations (EC) no\n300/2008, (EU) no 167/2013, (EU) no 168/2013, (EU) 2018/858 , (EU) 2018/1139 and\n(EU) 2019/2144 and directives 2014/90/eu, (eu) 2016/797 and (EU) 2020/1828 (artiﬁcial\nintelligence act) (text with eea relevance) . ELI. Available online at: http://data.europa.\neu/eli/reg/2024/1689/oj (BG, ES, CS, DA, DE, ET, EL, EN, FR, GA, HR, IT, LV , LT, HU,\nMT, NL, PL, PT, RO, SK, SL, FI, SV).\nFerreira, A. P. R. B., Ferreira, R., Rajgor, D., Shah, J., Men ezes, A., Pietrobon,\nR., et al. (2010). Clinical reasoning in the real world is mediat ed by bounded\nrationality: implications for diagnostic clinical practice gui delines. PLoS ONE 5:e10265.\ndoi: 10.1371/journal.pone.0010265\nGee, W., Anakin, M., and Pinnock, R. (2017). Using theory to in terpret how\nsenior clinicians deﬁne, learn, and teach clinical reasoning. MedEdPublish 6:182.\ndoi: 10.15694/mep.2017.000182\nGoh, E., Gallo, R., Hom, J., Strong, E., Weng, Y., Kerman, H., et a l. (2024). Large\nlanguage model inﬂuence on diagnostic reasoning: a randomize d clinical trial. JAMA\nNetw. Open 7:e2440969. doi: 10.1001/jamanetworkopen.2024.40969\nGold, J., Knight, C. L., Christner, J., Mooney, C. E., Manthey, D., Lang, V. J., et al.\n(2022). Clinical reasoning education in the clerkship years: a cross-disciplinary national\nneeds assessment. PLoS ONE 17:e0273250. doi: 10.1371/journal.pone.0273250\nGraber, M., Franklin, N., and Gordon, R. (2005). Diagnostic err or in internal\nmedicine. Arch. Intern. Med . 165, 1493–1499. doi: 10.1001/archinte.165.13.1493\nHealthcare Information and Management Systems Society (HIM SS) and\nMedscape (2024). Medscape & HIMSS AI Adoption by Health Systems Report\n2024. Available online at: https://cdn.sanity.io/ﬁles/sqo8bpt9/production/\n68216fa5d161adebceb50b7add5b496138a78cdb.pdf\nHuh, M., Cheung, B., Wang, T., and Isola, P. (2024). The platonic representation\nhypothesis. arXiv [Preprint]. arXiv:2405.07987. doi: 10.48550/arXiv.2405. 07987\nJay, R., Davenport, C., and Patel, R. (2024). Clinical reasonin g—the essentials for\nteaching medical students, trainees and non-medical health care professionals. Br. J.\nHosp. Med. 85, 1–8. doi: 10.12968/hmed.2024.0052\nJin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., Szolovi ts, P., et al.\n(2021). What disease does this patient have? A large-scale open d omain question\nanswering dataset from medical exams. Appl. Sci . 11:6421. doi: 10.3390/app111\n46421\nKim, K., and Lee, Y. M. (2018). Understanding uncertainty in medicine:\nconcepts and implications in medical education. Korean J. Med. Educ . 30, 181–188.\ndoi: 10.3946/kjme.2018.92\nLai, Y., Zhong, J., Li, M., Zhao, S., and Yang, X. (2025). Med- r1: reinforcement\nlearning for generalizable medical reasoning in vision-langua ge models. arXiv\n[Preprint]. arXiv:2503.13939. doi: 10.48550/arXiv.2503. 13939\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., et al.\n(2020). “Retrieval-augmented generation for knowledge-int ensive NLP tasks, ” in 34th\nConference on Neural Information Processing Systems (NeurIPS 2020), Vanco uver,\nCanada. 33, 9459–9474. doi: 10.48550/arXiv.2005.11401\nLi, S., Balachandran, V., Feng, S., Ilgen, J., Pierson, E., Koh , P. W. W., et al.\n(2024). “Mediq: question-asking llms and a benchmark for reliable interactive clinical\nreasonin, ” in Advances in Neural Information Processing Systems . 37, 28858–28888.\ndoi: 10.48550/arXiv.2406.00922\nLi, Z., and Zhou, T. (2024). Your mixture-of-experts llm is secr etly an embedding\nmodel for free. arXiv [Preprint]. arXiv:2410.10814. doi: 10.48550/arXiv.2410. 10814\nLubarsky, S., Dory, V., Audétat, M., Custers, E., and Charlin , B. (2015). Using script\ntheory to cultivate illness script formation and clinical reason ing in health professions\neducation. Can. Med. Educ. J . 6, e61–e70. doi: 10.36834/cmej.36631\nLucas, M. M., Yang, J., Pomeroy, J. K., and Yang, C. C. (2024). Reasoning with large\nlanguage models for medical question answering. J. Am. Med. Inf. Assoc . 31, 1964–1975.\ndoi: 10.1093/jamia/ocae131\nMaharana, U., Verma, S., Agarwal, A., Mruthyunjaya, P., Maha patra, D., Ahmed,\nS., et al. (2025). Right prediction, wrong reasoning: uncover ing llm misalignment in ra\ndisease diagnosis. arXiv [Preprint]. arXiv:2504.06581. doi: 10.48550/arXiv.2504. 06581\nMakary, M. A., and Daniel, M. (2016). Medical error—the third leading cause of\ndeath in the us. BMJ 353:i2139. doi: 10.1136/bmj.i2139\nMettarikanon, D., and Tawanwongsri, W. (2024). Analysis of pa tient information\nand diﬀerential diagnosis with clinical reasoning in pre-clin ical medical students. Int.\nMed. Educ. 3, 23–31. doi: 10.3390/ime3010003\nMoroz, A. (2017). Clinical reasoning workshop: cervical spine and shoulder\ndisorders. MedEdPORTAL13:10560. doi: 10.15766/mep_2374-8265.10560\nMukhopadhyay, D., and Choudhari, S. G. (2024). Clinical reason ing skills among\nsecond-phase medical students in west bengal, india: an explora tory study. Cureus\n16:e68839. doi: 10.7759/cureus.68839\nNierenberg, R. (2020). Using the chief complaint driven medic al history:\ntheoretical background and practical steps for student clinic ians. MedEdPublish 9:17.\ndoi: 10.15694/mep.2020.000017.1\nNori, H., King, N., McKinney, S. M., Carignan, D., and Horvit z, E.\n(2023). Capabilities of gpt-4 on medical challenge problems. arXiv [Preprint].\narXiv:2303.13375. doi: 10.48550/arXiv.2303.13375\nOlupeliyawa, A. (2017). Clinical reasoning: implications and str ategies for\npostgraduate medical education. J. Postgrad. Inst. Med . 4:59. doi: 10.4038/jpgim.8174\nPelaccia, T., Tardif, J., Triby, E., and Charlin, B. (2011). An analysis of clinical\nreasoning through a recent and comprehensive approach: the dua l-process theory.\nMed. Educ. Online 16:5890 doi: 10.3402/meo.v16i0.5890\nRadovi´c, M., Petrovic, N., and Tosic, M. (2022). An ontology-driven\nlearning assessment using the script concordance test. Appl. Sci . 12:1472.\ndoi: 10.3390/app12031472\nFrontiers in Artiﬁcial Intelligence /one.tnum/four.tnum frontiersin.org\nMoëll et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/one.tnum/six.tnum/one.tnum/four.tnum/five.tnum\nRajpurkar, P., Chen, E., Banerjee, O., and Topol, E. J. (2022). A I in health and\nmedicine. Nat. Med. 28, 31–38. doi: 10.1038/s41591-021-01614-0\nSchiﬀ, G., Puopolo, A., Huben-Kearney, A., Yu, W., Keohane, C. A ., McDonough,\nP., et al. (2013). Primary care closed claims experience of Massa chusetts malpractice\ninsurers. JAMA Inter. Med . 173, 2063–2068. doi: 10.1001/jamainternmed.2013.\n11070\nShimizu, T., and Tokuda, Y. (2012). Pivot and cluster strateg y: a preventive measure\nagainst diagnostic errors. Int. J. Gen. Med . 5, 917–921. doi: 10.2147/IJGM.S38805\nShimozono, H., Nawa, N., Takahashi, M., Tomita, M., and Tana ka, Y. (2020).\nA cognitive bias in diagnostic reasoning and its remediatio n by the “2-dimensional\napproach\". MedEdPublish 9:123. doi: 10.15694/mep.2020.000123.1\nShin, H. S. (2019). Reasoning processes in clinical reasoning : from the perspective\nof cognitive psychology. Korean J. Med. Educ . 31, 299–308. doi: 10.3946/kjme.2019.140\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., et al.\n(2023). Large language models encode clinical knowledge. Nature 620, 172–180.\ndoi: 10.1038/s41586-023-06291-2\nSudacka, M., Durning, S. J., Georg, C., Huwendiek, S., Konono wicz, A. A., Schlegel,\nC., et al. (2023). Clinical reasoning: what do nurses, physicia ns, and students reason\nabout. J. Interprof. Care 37, 990–998. doi: 10.1080/13561820.2023.2208605\nTorre, D., Durning, S., Rencic, J., Lang, V. J., Holmboe, E., D aniel, M., et al. (2020).\nWidening the lens on teaching and assessing clinical reasonin g: from “in the head\" to\n“out in the world\". Diagnosis 7, 181–190. doi: 10.1515/dx-2019-0098\nWang, J., Yang, Z., Yao, Z., and Yu, H. (2024). JMLR: joint med ical llm and retrieval\ntraining for enhancing reasoning and professional question a nswering capability. arXiv\n[Preprint]. arXiv:2402.17887. doi: 10.48550/arXiv.2402. 17887\nWang, Y., Ma, X., and Chen, W. (2024). “Augmenting black-box llm s with medical\ntextbooks for biomedical question answering (published in ﬁnd ings of EMNLP 2024),\"\nin Findings of the Association for Computational Linguistics: EMNL P (Miami, FL: ACL).\ndoi: 10.18653/v1/2024.ﬁndings-emnlp.95\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E. , et al. (2022). Chain-\nof-thought prompting elicits reasoning in large language models. Adv. Neural Inf.\nProcess. Syst. 35, 24824–24837.\nWeinstein, A., Gupta, S., Pinto-Powell, R. C., Jackson, J., Appel, J .,\nRoussel, D., et al. (2017). Diagnosing and remediating clinica l reasoning\ndiﬃculties: a faculty development workshop. MedEdPORTAL 13:10650.\ndoi: 10.15766/mep_2374-8265.10650\nWelch, P., Plummer, D., Young, L., Quirk, F., Larkins, S., Evan s, R., et al. (2017).\nGrounded theory - a lens to understanding clinical reasoning. MedEdPublish 6:2.\ndoi: 10.15694/mep.2017.000002\nWorld Health Organization (2023). Health workforce snapshot . Geneva: World\nHealth Organization.\nWu, J., Deng, W., Li, X., Liu, S., Mi, T., Peng, Y., et al. (2025) . Medreason:\neliciting factual medical reasoning steps in llms via knowledge g raphs. arXiv [Preprint].\narXiv:2504.00993. doi: 10.48550/arXiv.2504.00993\nYazdani, S., and Abardeh, M. H. (2019). Five decades of resea rch and theorization\non clinical reasoning: a critical review. Adv. Med. Educ. Pract . 10, 703–716.\ndoi: 10.2147/AMEP.S213492\nYu, H., Cheng, T., Cheng, Y., and Feng, R. (2025). Finemedlm-o 1: enhancing the\nmedical reasoning ability of llm from supervised ﬁne-tuning to t est-time training. arXiv\n[preprint] arXiv:2501.09213. doi: 10.48550/arXiv.2501.09 213\nZwaan, L., de Bruijne, M. D., Wagner, C., Thijs, A., Smits, M. , van der\nWal, G., et al. (2010). Patient record review of the incidence, c onsequences,\nand causes of diagnostic adverse events. Arch. Intern. Med . 170, 1015–1021.\ndoi: 10.1001/archinternmed.2010.146\nFrontiers in Artiﬁcial Intelligence /one.tnum/five.tnum frontiersin.org"
}