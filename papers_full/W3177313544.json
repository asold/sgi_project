{
  "title": "AutoFormer: Searching Transformers for Visual Recognition",
  "url": "https://openalex.org/W3177313544",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2347685921",
      "name": "Chen, Minghao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2361504826",
      "name": "Peng Hou-wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748452209",
      "name": "Fu, Jianlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747133689",
      "name": "Ling, Haibin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2964098911",
    "https://openalex.org/W2962933129",
    "https://openalex.org/W780950768",
    "https://openalex.org/W2944779197",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3029385331",
    "https://openalex.org/W2964161337",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W3034528892",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W3109241881",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2994749257",
    "https://openalex.org/W2782417188",
    "https://openalex.org/W2990211757",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W2964515685",
    "https://openalex.org/W2953604046",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W3034512672",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3034657713",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2964212578",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2914611487",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2967733054",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3096533519",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3109946440",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2981698279",
    "https://openalex.org/W2963137684",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3106050345"
  ],
  "abstract": "Recently, pure transformer-based models have shown great potentials for vision tasks such as image classification and detection. However, the design of transformer networks is challenging. It has been observed that the depth, embedding dimension, and number of heads can largely affect the performance of vision transformers. Previous models configure these dimensions based upon manual crafting. In this work, we propose a new one-shot architecture search framework, namely AutoFormer, dedicated to vision transformer search. AutoFormer entangles the weights of different blocks in the same layers during supernet training. Benefiting from the strategy, the trained supernet allows thousands of subnets to be very well-trained. Specifically, the performance of these subnets with weights inherited from the supernet is comparable to those retrained from scratch. Besides, the searched models, which we refer to AutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In particular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy on ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify the transferability of AutoFormer by providing the performance on downstream benchmarks and distillation experiments. Code and models are available at https://github.com/microsoft/AutoML.",
  "full_text": "AutoFormer: Searching Transformers for Visual Recognition\nMinghao Chen1,∗, Houwen Peng2,∗,†, Jianlong Fu2, Haibin Ling1\n1Stony Brook University 2Microsoft Research Asia\nAbstract\nRecently, pure transformer-based models have shown\ngreat potentials for vision tasks such as image classiﬁca-\ntion and detection. However, the design of transformer net-\nworks is challenging. It has been observed that the depth,\nembedding dimension, and number of heads can largely af-\nfect the performance of vision transformers. Previous mod-\nels conﬁgure these dimensions based upon manual craft-\ning. In this work, we propose a new one-shot architecture\nsearch framework, namely AutoFormer, dedicated to vision\ntransformer search. AutoFormer entangles the weights of\ndifferent blocks in the same layers during supernet train-\ning. Beneﬁting from the strategy, the trained supernet al-\nlows thousands of subnets to be very well-trained. Specif-\nically, the performance of these subnets with weights in-\nherited from the supernet is comparable to those retrained\nfrom scratch. Besides, the searched models, which we re-\nfer to AutoFormers, surpass the recent state-of-the-arts such\nas ViT and DeiT. In particular, AutoFormer-tiny/small/base\nachieve 74.7%/81.7%/82.4% top-1 accuracy on ImageNet\nwith 5.7M/22.9M/53.7M parameters, respectively. Lastly,\nwe verify the transferability of AutoFormer by providing\nthe performance on downstream benchmarks and distil-\nlation experiments. Code and models are available at\nhttps://github.com/microsoft/AutoML.\n1. Introduction\nVision transformer recently has drawn signiﬁcant atten-\ntion in computer vision because of its high model capability\nand superior potentials in capturing long-range dependen-\ncies. Building on top of transformers [52], modern state-\nof-the-art models, such as ViT [13] and DeiT [50], are able\nto learn powerful visual representations from images and\nachieve very competitive performance compared to previ-\nous convolutional neural network models [17, 25].\nHowever, the design of transformer neural architectures\nis nontrivial. For example, how to choose the best network\n∗Equal contributions. Work performed when Minghao is an intern of\nMSRA. †Corresponding author: houwen.peng@microsoft.com.\n0 10 20 30 40 50 60 70 80 90\nNumber of Parameters (Millions)\n72\n74\n76\n78\n80\n82\n84ImageNet Top-1 Accuracy (%)\n74.7\n80.1\n81.0\n81.7 82.0 82.3 82.4 82.5\nAutoFormer (Ours)\nDeit\nResNet\nResNeXt\nDenseNet\nFigure 1. The comparison between AutoFormers and transformer-\nbased, convolution-based and architecture-searched models, such\nas DeiT [50], and ResNet [18].\ndepth, embedding dimension and/or head number? These\nfactors are all critical for elevating model capacity, yet ﬁnd-\ning a good combination of them is difﬁcult. As seen in\nFig. 2, increasing the depth, head number and MLP ratio\n(the ratio of hidden dimension to the embedding dimension\nin the multi-layer perceptron) of transformers helps achieve\nhigher accuracy at ﬁrst but get overﬁt after hitting the peak\nvalue. Scaling up the embedding dimension can improve\nmodel capability, but the accuracy gain plateaus for larger\nmodels. These phenomenons demonstrate the challenge of\ndesigning optimal transformer architectures.\nPrevious works on designing vision transformers are\nbased upon manual crafting, which heavily relies on hu-\nman expertise and typically requires a deal of trial-and-\nerror [13, 50, 67]. There are a few works on automat-\ning transformer design using neural architecture search\n(NAS) [45, 55]. However, they are all concentrated on nat-\nural language tasks, such as machine translation, which are\nquite different from computer vision tasks. As a result, it is\nhard to generalize prior automatic search algorithms to ﬁnd\neffective vision transformer architectures.\nIn this work, we present a new architecture search algo-\nrithm, named AutoFormer, dedicated to ﬁnding pure vision\ntransformer models. Our approach mainly addresses two\nchallenges in transformer search. 1) How to strike a good\narXiv:2107.00651v1  [cs.CV]  1 Jul 2021\n18 20 22 24 26 28 30 32 34\nNumber of Parameters (Millions)\n78.5\n79.0\n79.5\n80.0\n80.5\n81.0ImageNet Top-1 Accuracy (%)\nd = 10\nd = 11\nd = 12\nd = 13\nd = 14 d = 15\nd = 16\nd = 18\n0 20 40 60 80\nNumber of Parameters (Millions)\n72\n74\n76\n78\n80\n82ImageNet Top-1 Accuracy (%)\ne = 192\ne = 320\ne = 384\ne = 448e = 512 e = 768\n18 20 22 24 26 28 30\nNumber of Parameters (Millions)\n79.0\n79.2\n79.4\n79.6\n79.8\n80.0\n80.2\n80.4ImageNet Top-1 Accuracy (%)\nr = 3\nr = 4\nr = 5\nr = 6\n2 3 4 5 6 7 8 9\nNumber of Heads\n79.2\n79.3\n79.4\n79.5\n79.6\n79.7\n79.8\n79.9\n80.0ImageNet Top-1 Accuracy (%)\nh = 3\nh = 4\nh = 6\nh = 8\nFigure 2. Adjust a baseline model with different embedding dimension (e), depth (d), MLP ratio (r), and number of heads (h) coefﬁcients\nunder the same training recipe, where MLP ratio( the ratio of hidden dimension to the embedding dimension in the multi-layer perceptron).\nWe set the baseline model with d = 12, r= 4, e= 384, h= 6. Note: number of heads does not affect the model size and complexity if\nwe ﬁx the Q-K-V dimension.\ncombination of the key factors in transformers, such as net-\nwork depth, embedding dimension and head number? 2)\nHow to efﬁciently ﬁnd out various transformer models that\nﬁt different resource constraints and application scenarios?\nTo tackle the challenges, we construct a large search\nspace covering the main changeable dimensions of trans-\nformer, including embedding dimension, number of heads,\nquery/key/value dimension, MLP ratio, and network depth.\nThis space contains a vast number of transformers with di-\nverse structures and model complexities. In particular, it al-\nlows the construction of transformers to use different struc-\ntures of building blocks, thus breaking the convention that\nall blocks share an identical structure in transformer design.\nTo address the efﬁciency issue, inspired by BigNAS [65]\nand slimmable networks [65, 66], we propose a supernet\ntraining strategy called weight entanglement dedicated to\ntransformer architecture. The central idea is to enable dif-\nferent transformer blocks to share weights for their common\nparts in each layer. An update of weights in one block will\naffect all other ones as a whole, such that the weights of\ndifferent blocks are maximally entangled during training.\nThis strategy is different from most one-shot NAS methods\n[16, 8, 59], in which the weights of different blocks are in-\ndependent for the same layer, as visualized in Fig. 5.\nWe observe a surprising phenomenon when using the\nproposed weight entanglement for transformer supernet\ntraining: it allows a large number of subnets in the super-\nnet to be very well-trained, such that the performance of\nthese subnets with weights inherited from the supernet are\ncomparable to those retrained from scratch. This advan-\ntage allows our method to obtain thousands of architectures\nthat can meet different resource constraints while maintain-\ning the same level of accuracy as training from scratch in-\ndependently. We give a detailed discussion in Section 3.4\nexploring the underlying reasons of weight entanglement.\nWe perform a evolutionary search with a model size con-\nstraint over the well-trained supernets to ﬁnd promising\ntransformers. Experiments on ImageNet [11] demonstrate\nthat our method achieves superior performance to the hand-\ncrafted state-of-the-art transformer models. For instance,\nas shown in Fig. 1, with 22.9M parameters, Autoformer\nachieves a top-1 accuracy of 81.7%, being 1.8% and 2.9%\nbetter than DeiT-S [50] and ViT-S/16 [13], respectively. In\naddition, when transferred to downstream vision classiﬁ-\ncation datasets, our AutoFormer also performs well with\nfewer parameters, achieving better or comparable results to\nthe best convolutional models, such as EfﬁcientNet [49].\nIn summary, we make three major contributions in this\npaper. 1) To our best knowledge, this work is the ﬁrst ef-\nfort to design an automatic search algorithm for ﬁnding vi-\nsion transformer models. 2) We propose a simple yet ef-\nfective framework for efﬁcient training of transformer su-\npernets. Without extra ﬁnetuning or retraining, the trained\nsupernet is able to produce thousands of high quality trans-\nformers by inheriting weights from it directly. Such merit\nallows our method to search diverse models to ﬁt different\nresource constraints. 3) Our searched models, i.e., Auto-\nFormers, achieve the state-of-the-art results on ImageNet\namong the vision transformers, and demonstrate promising\ntransferability on downstream tasks.\n2. Background\nBefore presenting our method, we ﬁrst brieﬂy review the\nbackground of the vision transformer and one-shot NAS.\n2.1. Vision Transformer\nTransformer is originally designed for natural language\ntasks [52, 28, 12]. Recent works, such as ViT and DeiT\n[13, 50], show its great potential for visual recognition. In\nthe following, we give the basic pipeline of the vision trans-\nformer, which serves as a base architecture of AutoFormer.\nGiven a 2D image, we ﬁrst uniformly split it into a se-\nquence of 2D patches just like tokens in natural language\nprocessing. We then ﬂatten and transform the patches to\nD-dimension vectors, named patch embeddings, by either\nlinear projection [13] or several CNN layers [67]. A learn-\nable [class] embedding is injected into the head of the se-\nquence to represent the whole picture. Position embeddings\nare added to the patch embeddings to retain positional infor-\nmation. The combined embeddings are then fed to a trans-\nformer encoder described below. At last, a linear layer is\nLiner\nConcate\n          Embedding Choice\n Key Choice  Value Choice\nAttention\nHidden Dim Choice\nHidden Dim ChoiceHidden Dim Choice\n     Embedding Choice\n Query Choice\nAdd Position \nEmbedding to   \nPatch Embedding\nTransformer Block Choice\nTransformer Block Choice\nTransformer Block Choice\nTransformer Block Choice\nClass \nHead\nDynamic Layer of \nTransformer Block\nSplit\nPatch Embedding Choice[class]\nFigure 3. Left: The overall architecture of the AutoFormer supernet. Note that transformer blocks in each layer and depth are dynamic.\nThe parts in solid lines mean they are chosen while those in dashed lines are not.Right: The detailed transformer block in an AutoFormer.\nWe search for the best block of optimal embedding dimension, number of heads, MLP ratio,Q-K-V dim in a layer. For more details about\nthe search space, please refer to section 3.2.\nused for the ﬁnal classiﬁcation.\nA transformer encoder consists of alternating blocks of\nmultihead self-attention (MSA) and multi-layer perceptron\n(MLP) blocks. LayerNorm (LN) [2] is applied before ev-\nery block, and residual connections after every block. The\ndetails of MSA and MLP are given below.\nMultihead Self-Attention (MSA). In a standard self-\nattention module, the input sequence z ∈RN×D will be\nﬁrst linearly transformed to queries Q ∈ RN×Dh, keys\nK ∈ RN×Dh and values V ∈ RN×Dh, where N is the\nnumber of tokens, D is the embedding dimension, Dh is\nthe Q-K-V dimension. Then we compute the weighted\nsum over all values for each element in the sequence. The\nweights or attention are based on the pairwise similarity be-\ntween two elements of the sequence:\nAttention(Q,K,V ) = softmax\n(QKT\n√dh\n)\nV, (1)\nwhere 1√dh\nis the scaling factor. Lastly, a fully connected\nlayer is applied. Multihead self-attention splits the queries,\nkeys and values into different heads and performs self-\nattention in parallel and projects their concatenated outputs.\nMulti-Layer Perceptron (MLP).The MLP block consists\ntwo fully connected layers with an activation function, usu-\nally GELU [19]. In this work, we focus on ﬁnding optimal\nchoices of the MLP ratios in each layer.\n2.2. One-Shot NAS\nOne-shot NAS typically adopts a weight sharing strategy\nto avoid training each subnet from scratch [16, 39]. The\narchitecture search space Ais encoded in a supernet, de-\nnoted as N(A,W), where W is the weight of the supernet.\nW is shared across all the architecture candidates, i.e., sub-\nnets α∈A in N. The search of the optimal architecture α∗\nin one-shot NAS is usually formulated as a two-stage opti-\nmization problem. The ﬁrst-stage is to optimize the weight\nW by\nWA= arg min\nW\nLtrain(N(A,W)), (2)\nwhere Ltrain represents the loss function on the training\ndataset. To reduce memory usage, one-shot methods usu-\nally sample subnets from Nfor optimization. The second-\nstage is to search architectures via ranking the performance\nof subnets α∈A based on the learned weights in WA:\nα∗= arg max\nα∈A\nAccval (N(α,w)) , (3)\nwhere the sampled subnet α inherits weight w from WA,\nand Accval indicates the top-1 accuracy of the architec-\nture α on the validation dataset. Since it is impossible\nto enumerate all the architectures α∈A for evaluation,\nprior works resort to random search [34, 4], evolution al-\ngorithms [43, 16] or reinforcement learning [40, 48] to ﬁnd\nthe most promising one.\n3. AutoFormer\nIn this section, we ﬁrst demonstrate that it is impractical\nto directly apply one-shot NAS for transformer search fol-\nlowing classical weight sharing strategy [16], using differ-\nent weights for different blocks in each layer, because of the\nslow convergence and unsatisfactory performance. Then we\npropose the weight entanglement strategy for vision trans-\nformer to address the issues. Finally, we present the search\nspace and search pipeline.\n0 100 200 300 400 500\nEpochs\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0Train Loss\nWeight Entanglement\nClassical Weight Sharing\n0 100 200 300 400 500\nEpochs\n0\n20\n40\n60\n80Test Accuracy Weight Entanglement\nClassical Weight Sharing\nFigure 4. Left: Comparison of training loss of supernet between\nweight entanglement and classical weight sharing on ImageNet.\nRight: Comparison of Top-1 Accuracy on ImageNet of subnets\nbetween weight entanglement and classical weight sharing during\nsupernet training.\n3.1. One-Shot NAS with Weight Entanglement\nPrior one-shot NAS methods commonly share weights\nacross architectures during supernet training while decou-\npling the weights of different operators at the same layer.\nSuch strategy performs well when used to search architec-\ntures over convolutional neural networks space [16, 6, 8, 21,\n49]. However, in transformer search space, this classical\nstrategy encounters difﬁculties. 1) Slow convergence. As\nshown in the Fig. 4 (left), the training loss of the supernet\nconverges slowly. The reason might be that the independent\ntraining of transformer blocks results in the weights being\nupdated by limited times. 2) Low performance. The perfor-\nmances of subnets inheriting weights from the one-shot su-\npernet, trained under classical weight sharing strategy, are\nfar below their true performances of training from scratch\n(see the right part of Fig. 4). This limits the ranking ca-\npacities of supernet. Furthermore, after the search, it is still\nnecessary to perform additional retraining for the searched\narchitectures since the weights are not fully optimized. In-\nspried by BigNAS [65] and slimmable networks [66, 64],\nwe propose the weight entanglement training strategy ded-\nicated to vision transformer architecture search. The cen-\ntral idea is to enable different transformer blocks to share\nweights for their common parts in each layer. More con-\ncretely, for a subnet α ∈ Awith a stack of l layers, we\nrepresent its structure and weights as\n{\nα= (α(1),...α(i),...α(l)),\nw= (w(1),...w(i),...w(l)),\n(4)\nwhere α(i) denotes the sampled block in the i-th layer and\nw(i) is the block weights. During architecture search, there\nare multiple choices of blocks in each layer. Hence,α(i) and\nw(i) are actually selected from a set of nblock candidates\nbelonging to the search space, which is formulated as\n{\nα(i) ∈{b(i)\n1 ,...b(i)\nj ,...b(i)\nn },\nw(i) ∈{w(i)\n1 ,...w(i)\nj ,...w(i)\nn },\n(5)\nwhere b(i)\nj is a candidate block in the search space and w(i)\nj\nis its weights. The weight entanglement strategy enforces\nInput\nBlock 1 Block 2 Block 3\nOutput\nInput\nOutput\nBlock 2\nBlock 3\n(a) Classical Weight Sharing (b) Weight Entanglement\nOn Off\nd\ndim= dim=\ndk+dim=\ndk−\ndim=\ndk+\nd\ndim=\ndim=\ndk−\n   Block 1 \nFigure 5. Classical weight sharing versus weight entanglement.\nthat different candidate blocks in the same layer to share as\nmany weights as possible. This requires that, for any two\nblocks b(i)\nj and b(i)\nk in the same layer, we have\nw(i)\nj ⊆w(i)\nk or w(i)\nk ⊆w(i)\nj . (6)\nSuch within layer weight sharing makes the weight updates\nof w(i)\nj and w(i)\nk entangled with each other. The training of\nany block will affect the weights of others for their inter-\nsected portion, as demonstrated in Fig. 5. This is different\nfrom the classical weight sharing strategy in one-shot NAS,\nwhere the building blocks in the same layer are isolated. In\nother words, in classical weight sharing, for any two blocks\nb(i)\nj and b(i)\nk , we have w(i)\nj ∩w(i)\nk = ∅.\nNote that the proposed weight entanglement strategy is\ndedicated to work on homogeneous building blocks, such\nas self-attention modules with different numbers of heads,\nand multi-layer perceptron with different hidden dimen-\nsions. The underlying reason is that homogeneous blocks\nare structurally compatible, such that the weights can share\nwith each other. During implementation, for each layer, we\nneed to store only the weights of the largest block among the\nnhomogeneous candidates. The remaining smaller build-\ning blocks can directly extract weights from the largest one.\nEquipped with weight entanglement, one-shot NAS is\ncapable of searching transformer architectures in an efﬁ-\ncient and effective fashion, as demonstrated in Fig. 4.\nCompared with classical weight sharing methods, our\nweight entanglement strategy has three advantages. 1)\nFaster convergence. Weight entanglement allows each\nblock to be updated more times than the previous indepen-\ndent training strategy. 2) Low memory cost. We now only\nneed to store the largest building blocks’ parameters for\neach layer, instead of all the candidates in the space. 3)Bet-\nter subnets performance. We found that the subnets trained\nwith weight entanglement could achieve performance on\npar with those of training from scratch.\n3.2. Search Space\nWe design a large transformer search space that includes\nﬁve variable factors in transformer building blocks: embed-\nding dimension, Q-K-V dimension, number of heads, MLP\nratio, and network depth, as detailed in Tab. 1 and Fig. 3. All\nSupernet-tiny Supernet-small Supernet-base\nEmbed Dim (192, 240, 24) (320, 448, 64) (528, 624, 48)\nQ-K-V Dim (192, 256, 64) (320, 448, 64) (512, 640, 64)\nMLP Ratio (3.5, 4, 0.5) (3, 4, 0.5) (3, 4, 0.5)\nHead Num (3, 4, 1) (5, 7, 1) (8, 10, 1)\nDepth Num (12, 14, 1) (12, 14, 1) (14, 16, 1)\nParams Range 4 – 9M 14 – 34M 42 – 75M\nTable 1. The search space of AutoFormer. We set up three super-\nnets to satisfy different resource constraints. Tuples of three val-\nues in parentheses represent the lowest value, highest, and steps.\nNote: the Q-K-V dimensions, numbers of head and MLP ratios\nare varied across layers.\nthese factors are important for model capacities. For exam-\nple, in attention layers, different heads are used to capture\nvarious dependencies. However, recent works [36, 53, 9]\nshow that many heads are redundant. We thereby make the\nattention head number elastic so that each attention mod-\nule can decide its necessary number of heads. On the other\nhand, since different layers have different capacities on fea-\nture representation, the varying hidden dimensions in layers\nmight be better than the ﬁxed sizes when used for construct-\ning new models. Moreover, AutoFormer adds new Q-K-V\ndimension into the search space and ﬁxes the ratio of theQ-\nK-V dimension to the number of heads in each block. This\nsetting makes the scaling factor 1√dh\nin attention calculation\ninvariant to the number of heads, stabilizing the gradients,\nand decouples the meaning of different heads. Besides, we\nuse MLP ratio and embedding dimension together to de-\ncide the hidden dimension in each block, which enlarges\nthe search space than using ﬁxed values.\nFollowing one-shot NAS methods, we encode the search\nspace into a supernet. That is, every model in the space is\na part/subset of the supernet. All subnets share the weights\nof their common parts. The supernet is the largest model\nin the space, and its architecture is shown in Fig. 3. In par-\nticular, the supernet stacks the maximum number of trans-\nformer blocks with the largest embedding dimension,Q-K-\nV dimension and MLP ratio as deﬁned in the space. During\ntraining, all possible subnets are uniformly sampled, and the\ncorresponding weights are updated.\nAccording to the constraints on model parameters, we\npartition the large-scale search space in to three parts and\nencode them into three independent supernets, as elaborated\nin Tab. 1. Such partition allows the search algorithm to\nconcentrate on ﬁnding models within a speciﬁc parameter\nrange, which can be specialized by users according to their\navailable resources and application requirements.\nOverall, our supernets contains more than1.7×1016 can-\ndidate architectures covering a wide range of model size.\n3.3. Search Pipeline\nOur search pipeline includes two sequential phases.\nPhase 1: Supernet Training with Weight Entangle-\nment. In each training iteration, we uniformly sample\na subnet α = (α(1),...α(i),...α(l)) from the per-deﬁned\nsearch space and update its corresponding weights w =\n(w(1),...w(i),...w(l)) in the supernet’s weight WA while\nfreezing the rest. Detailed algorithm is given in supplemen-\ntary materials, Appendix A.\nPhase 2: Evolution Search under Resource Constraints.\nAfter obtaining the trained supernet, we perform an evo-\nlution search on it to obtain the optimal subnets. Subnets\nare evaluated and picked according to the manager of the\nevolution algorithm. Our objective here is to maximize the\nclassiﬁcation accuracy while minimizing the model size. At\nthe beginning of the evolution search, we pick N random\narchitecture as seeds. The top k architectures are picked\nas parents to generate the next generation by crossover and\nmutation. For a crossover, two randomly selected candi-\ndates are picked and crossed to produce a new one during\neach generation. For mutation, a candidate mutates its depth\nwith probability Pd ﬁrst. Then it mutates each block with a\nprobability of Pm to produce a new architecture.\n3.4. Discussion\nWhy does weight entanglement work? We conjecture\nthat there are two underlying reasons. 1) Regularization\nin training. Different from convolution neural networks,\ntransformer has no convolution operations at all. Its two\nbasic components, MSA and MLP, employ only fully con-\nnected layers. Weight entanglement could be viewed a reg-\nularization training strategy for transformer, to some extent,\nsimilar to the effects of dropout [47, 54, 30]. When sam-\npling the small subnets, corresponding units cannot rely on\nother hidden units for classiﬁcation, which hence reduces\nthe reliance of units. 2) Optimization of deep thin subnets.\nRecent works [3, 57] show that deep transformer is hard to\ntrain, which coincides with our observation in Fig. 2. This\nis because the gradients might explode or vanish in deep\nthin networks during backpropagation. Increasing the width\nof or “overparameterizing” the network will help the opti-\nmization [33, 14, 1, 71]. Our weight entanglement training\nstrategy helps to optimize the thin subnets in a similar way.\nThe gradients backwarded by wide subnets will help to up-\ndate the weights of thin subnets. Besides, the elastic depth\nsevers similar effects to stochastics depth [23] and deep su-\npervision [29], which supervise the shallow layers as well.\n4. Experiments\nIn this section, we ﬁrst present the implementation de-\ntails and evolution search settings. We then analyze the\nproposed weight entanglement strategy and provide a large\nnumber of well-trained subnets sampled from supernets to\ndemonstrate its efﬁcacy. At last, we present the perfor-\nmance of AutoFormer evaluated on several benchmarks\nSearch Method Inherited RetrainParams\nRandom Search - 79.4% 23.0M\nClassical Weight Sharing + Random Search 69.7% 80.1% 22.9M\nWeight Entanglement + Random Search 81.3% 81.4% 22.8M\nClassical Weight Sharing + Evolution Search (SPOS[16])71.5% 80.4% 22.9M\nWeight Entanglement + Evolution Search (Ours)81.7% 81.7% 22.9M\nTable 2. Comparison of different search methods. The supernets are trained\nfor 500 epochs, while the subnets are retrained for 300 epochs. Random\nsearch are performed three times and the best performance is reported.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nSearch Interation\n79.5\n80.0\n80.5\n81.0\n81.5Top1 Accuracy (%)\nRandom\nEvolution\nFigure 6. The performance of subnets inheriting weights\nfrom supernet during search. Top 50 candidates until the\ncurrent iteration are depicted at each search iteration.\nEpochs Optimizer Batch Size LR LR scheduler\n500 AdamW 1024 1 e-3 cosine\nWeight Warmup Label Drop Repeated\nDecay Epochs Smoothing Path Augmentation\n5e-2 20 0.1 0.1 \u0017\nTable 3. Supernet training settings. LR refers to learning rate.\nModel Model Size Inherited Finetune Retrain\nAutoFormer-T 5.7M 74.7% 74.9% 74.9%\nAutoFormer-S 22.9M 81.7% 81.8% 81.7%\nAutoFormer-B 53.7M 82.4% 82.6% 82.6%\nTable 4. Comparison of subnets with inherited weights, ﬁne-tuned\n(40 epochs) and trained from-scratch (300 epochs).\nwith comparisons with state-of-the-art models designed\nmanually or automatically.\n4.1. Implementation Details\nSupernet Training. We train the supernets using a sim-\nilar recipe as DeiT [50]. The details are presented in\nTab. 3. Data augmentation techniques, including RandAug-\nment [10], Cutmix [68], Mixup [69] and random erasing,\nare adopted with the same hyperparameters as in DeiT [50]\nexcept the repeated augmentation [20]. Images are split into\npatches of size 16x16. All the models are implemented us-\ning PyTorch 1.7 and trained on Nvidia Tesla V100 GPUs.\nEvolutionary Search. The implementation of evolution\nsearch follows the same protocol as in SPOS [16]. For a\nfair comparison, we reserve the ImageNet validation set for\ntesting and subsample 10,000 training examples (100 im-\nages per class) as the validation dataset. We set the pop-\nulation size to 50 and number of generations to 20. Each\ngeneration we pick the top 10 architectures as the parents\nto generate child networks by mutation and crossover. The\nmutation probability Pd and Pm are set to 0.2 and 0.4.\n4.2. Ablation Study and Analysis\nThe Efﬁcacy of Weight Entanglement. We compare\nAutoFormer with random search and SPOS [16] (classi-\ncal weight sharing) baselines to demonstrate the effective-\nness of weight entanglement. For random search, we ran-\n16 18 20 22 24 26 28 30 32\nNumber of Parameters (Millions)\n79.5\n80.0\n80.5\n81.0\n81.5\n82.0ImageNet Top-1 Accuracy (%)\nAutoFormer (Ours)\nRandom Sampled (Ours)\nDeiT-S\nRegNetY-8GF\nResNeXt\nFigure 7. Top-1 accuracy on ImageNet of AutoFormer and 1000\nsampled high-performing architectures from the supernet-small\nwith weight inherited from the supernet.\ndomly pick up architectures from the search space to meet\nthe model size constraints. For SPOS [16], we adapt it to\nthe transformer search space deﬁned in Tab. 1 and keep the\nremaining conﬁgurations to be consistent with the original\nmethod. In other words, in each layer of the SPOS super-\nnet, the transformer blocks with different architecture argu-\nments do not share weights. For example, in an MLP block,\nthere are multiple choices of hidden dimensions. Each MLP\nchoice has its own weights, being independent of each other.\nAfter training the SPOS supernet, we apply the same evolu-\ntion process to ﬁnd the most promising architecture candi-\ndate and retrain it using the same setting as our AutoFormer.\nTable 2 presents the comparisons on ImageNet. We can\nobserve that: 1) After retraining,random search and SPOS\nare 2.3% and 1.3% inferior to our methods indicating the\nsuperiority of the proposed methods. 2) Without retrain-\ning, i.e., inheriting weights directly from the supernet, the\nweight entanglement training strategy can produce signiﬁ-\ncantly better results than the classical weight sharing. The\nentangled weight can produce well-trained subnets, which\nare very close to the ones retrained from scratch. We con-\njecture the relatively inferior performance of SPOS in trans-\nformer space is mainly due to insufﬁcient training. We\nalso observe that if we train the supernet in SPOS for more\nepochs, the performance can be slowly improved. However,\nits training cost is largely higher than our proposed weight\nentanglement strategy. Fig. 6 plots the accuracy over the\nnumber of architectures sampled from the trained supernet\nduring search. Top 50 candidates are depicted at each gen-\nTable 5. AutoFormer performance on ImageNet with comparisons to state-of-the-arts. We group the models according to their parameter\nsizes. Our AutoFormer consistently outperforms existing transformer-based visual models, being comparable to CNN models. †: reported\nby [50], ⋆: reported by [63].\nModels Top-1 Acc. Top-5 Acc. #Parameters FLOPs Resolution Model Type Design Type\nMobileNetV3Large1.0 [21] 75.2% - 5.4M 0.22G 2242 CNN Auto\nEfﬁcietNet-B0[49] 77.1% 93.3% 5.4M 0.39G 2242 CNN Auto\nDeiT-tiny [50] 72.2% 91.1% 5.7M 1.2G 2242 Transformer Manual\nAutoFormer-tiny (Ours) 74.7% 92.6% 5.7M 1.3G 2242 Transformer Auto\nResNet50⋆ [18] 79.1% - 25.5M 4.1G 2242 CNN Manual\nRegNetY-4GF†[41] 80.0% - 21.4M 4.0G 2242 CNN Auto\nEfﬁcietNet-B4 [49] 82.9% 95.7% 19.3M 4.2G 3802 CNN Auto\nBoTNet-S1-59 [46] 81.7% 95.8% 33.5M 7.3G 2242 CNN + Trans Manual\nT2T-ViT-14 [67] 81.7% - 21.5M 6.1G 2242 Transformer Manual\nDeiT-S [50] 79.9% 95.0% 22.1M 4.7G 2242 Transformer Manual\nViT-S/16 [13] 78.8% - 22.1M 4.7G 3842 Transformer Manual\nAutoFormer-small (Ours) 81.7% 95.7% 22.9M 5.1G 2242 Transformer Auto\nResNet152⋆ [18] 80.8% - 60M 11G 2242 CNN Manual\nEfﬁcietNet-B7 [49] 84.3% 97.0% 66M 37G 6002 CNN Auto\nViT-B/16 [13] 79.7% - 86M 18G 3842 Transformer Manual\nDeit-B [50] 81.8% 95.6% 86M 18G 2242 Transformer Manual\nAutoFormer-base (Ours) 82.4% 95.7% 54M 11G 2242 Transformer Auto\neration. It is clear that the evolution search on the supernet\nis more effective than the random search baseline.\nSubnet Performance without Retraining. We surprisingly\nobserve that there are a large number of subnets achieving\nvery good performance when inheriting weights from the\nsupernets, without extra ﬁnetuning or retraining . The blue\npoints shown in Fig. 7 represents the 1000 high-performing\nsubsets sampled from the supernet-S. All these subsets can\nachieve top-1 accuracies ranging from 80.1% to 82.0%, ex-\nceeding the recent DeiT [50] and RegNetY [41]. Such re-\nsults amply demonstrate the effectiveness of the proposed\nweight entanglement strategy for one-shot supernet train-\ning. Tab. 4 shows that if we further ﬁnetune or retrain the\nsearched subnets on ImageNet, the performance gains are\nvery small, even negligible. This phenomenon illustrates\nthe weight entanglement strategy allows the subsets to be\nwell-trained in supernets, leading to the facts that searched\ntransformers do not require any retraining or ﬁnetuning and\nthe supernet itself serves good indicator of subnets’ ranking.\n4.3. Results on ImageNet\nWe perform the search of AutoFormer on ImageNet and\nﬁnd multiple transformer models with diverse parameter\nsizes. All these models inherit weights from supernets di-\nrectly, without extra retraining and other postprocessing.\nThe performance are reported in Tab. 5 and Fig. 1. It is clear\nthat our AutoFormer model families achieve higher accura-\ncies than the recent handcrafted state-of-the-art transformer\nmodels such as ViT [13] and DeiT [50]. In particular, us-\ning ∼23M parameters, our small model,i.e. AutoFormer-S,\nachieves a top-1 accuracy of 81.7%, being 1.8% and 2.9%\nbetter than DeiT-S and ViT-S/16, respectively.\nCompared to vanilla CNN models, AutoFormer is also\ncompetitive. As visualized in Fig. 1, our AutoFormers\nperform better than the manually-designed ResNet [18],\nResNeXt [62] and DenseNet [22], demonstrating the poten-\ntials of pure transformer models for visual representation.\nHowever, transformer-based vision models, including\nAutoFormer, now are still inferior to the models based on\ninverted residual blocks [44], such as MobileNetV3 [21]\nand EfﬁcientNet [49]. The reason is that inverted residu-\nals are optimized for edge devices, so the model sizes and\nFLOPs are much smaller than vision transformers.\n4.4. Transfer Learning Results\nClassiﬁcation. We transfer Autoformer to a list of\ncommonly used recognition datasets: 1) general classiﬁca-\ntion: CIFAR-10 and CIFAR-100 [27]; 2) ﬁne-grained clas-\nsiﬁcation: Stanford Car [26], FLowers [37] and Oxford-\nIII Pets [38]. We follow the same training settings as\nDeiT [50], which take ImageNet pretrained checkpoints and\nﬁnetune on new datasets. Tab. 6 shows the results in terms\nof top-1 accuracy: 1) Compared to state-of-the-art Con-\nvNets, AutoFormer is close to the best results with a negligi-\nble gap with fewer parameters; 2) Compared to transformer-\nbased models, AutoFormer achieves better or comparable\nresults on all datasets, with much fewer parameters ( ∼4x).\nIn general, our AutoFormer consistently achieve compara-\nble or better accuracy with an order of magnitude fewer pa-\nrameters than existing models, including ResNet [51], Reg-\nNet [51], EfﬁcientNet [49], ViT [13], and DeiT [50] .\nDistillation. AutoFormer is also orthogonal to knowl-\nTable 6. AutoFormer results on downstream classiﬁcation datasets. ↑ 384 denotes ﬁne-tuning with 384×384 resolution.\nModel #Param FLOPs ImageNet CIFAR-10 CIFAR-100 Flowers Cars Pets Model Type Design Type\nGraﬁt ResNet-50 [51] 25M 12.1G 79.6 - - 98.2 92.5 - CNN Manual\nGraﬁt RegNetY-8GF [51] 39M 23.4G 79.6 - - 99.0 94.0 - CNN Manual\nEfﬁcientNet-B5 [49] 30M 9.5G 83.6 98.7 91.1 98.5 - - CNN Auto\nViT-B/16 [13] 86M 55.4G 77.9 98.1 87.1 89.5 - 93.8 Trans Manual\nDeiT-B↑ 384 [50] 86M 55.4G 83.1 99.1 90.8 98.5 93.3 - Trans Manual\nAutoFormer-S↑ 384 23M 16.5G 83.4 99.1 91.1 98.8 93.4 94.9 Trans Auto\nedge distillation (KD) since we focus on searching for\nan efﬁcient architecture while KD focuses on better train-\ning a given architecture. Combining KD with Auto-\nFormers by distilling hard labels from a RegNetY-32GF\n[41] teacher could further improve the performance from\n74.7%/81.7%/82.4% to 75.7%/82.4%/82.9%, respectively.\n5. Related Work\nVision Transformer. Transformer is originally proposed\nfor language modeling [52], and recently applied in com-\nputer vision. It has shown promising potentials on a vari-\nety of tasks, such as recognition, detection and segmenta-\ntion [7, 13, 35]. A straightforward approach for using trans-\nformer in vision is to combine convolutional layers with the\nself-attention module [52, 58]. There has been progress in\nthis direction in recent works, such as [42, 70, 56, 24].\nMost recently, Dosovitskiyet al. introduce Vision Trans-\nformer (ViT) [13], a pure transformer architecture for visual\nrecognition, without using any convolutions. ViT stacks\ntransformer blocks on linear projections of non-overlapping\nimage patches. It presents promising results when trained\nwith an extensive image dataset (JFT-300M, 300 million\nimages) that is not publicly available. The most recent\nDeiT [50, 67] models verify that large-scale data is not re-\nquired. Using only Imagenet can also produce a competitive\nconvolution-free transformer. However, existing visions of\ntransformer models are all built upon manual design, which\nis engineering-expensive and error-prone. In this work, we\npresent the ﬁrst effort on automating the design of vision\ntransformer with neural architecture search.\nNeural Architecture Search. There has been an increas-\ning interest in NAS for automating network design [15, 25].\nEarly approaches search a network using either reinforce-\nment learning [73, 72] or evolution algorithms [61, 43].\nMost recent works resort to the one-shot weight sharing\nstrategy to amortize the searching cost [34, 40, 5, 16]. The\nkey idea is to train a over-parameterized supernet model,\nand then share the weights across subnets. Among them,\nSPOS is simple and representative [16]. In each iteration, it\nonly samples one random path and trains the path using one\nbatch data. Once the training process is ﬁnished, the subnets\ncan be ranked by inheriting the shared weights. However,\nmost weight-sharing methods need an additional retraining\nstep after the best architecture is identiﬁed [16, 31, 59].\nRecent works, OFA [6], BigNAS [65] and slimmable\nnetworks [66, 64] alleviate this issue by training a once-\nfor-all supernet. Despite the fact that AutoFormer shares\nsimilarities with these methods in the concept of training a\none-for-all supernet, these methods are designed to search\nfor convolutional networks rather than vision transform-\ners, which have very different architecture characteristics.\nSpeciﬁcally, AutoFormer considers the design of multi-\nhead self-attention and MLP, which is unique to transformer\nmodels, and gives dedicated design of search dimensions as\nelaborated in Sec. 3.2. Moreover, BigNAS adopts several\nwell-crafted techniques, such as sandwich training, inplace\ndistillation, regularization, etc. OFA proposes a progres-\nsively shrinking approach by progressively distilling the full\nnetwork to obtain the smaller subnets. By contrast, Auto-\nFormer is simple and efﬁcient, achieving once-for-all train-\ning without these techniques, and could be easily extended\nto search for other vision transformer variants.\nFor transformers, there are few studies applying NAS\nto improve their architectures [45, 55]. These approaches\nmainly focus on natural language processing tasks. Among\nthem, the most similar one to us is HAT [55]. In addi-\ntion to the difference between tasks, HAT requires an ad-\nditional retraining or ﬁnetuning step after the search, while\nAutoFormer does not, which is the key difference. An-\nother difference is the search space. HAT searches for an\nencoder-decoder Transformer structure, while ours is a pure\nencoder one. There are two concurrent works, i.e., Boss-\nNAS [32] and CvT [60], exploring different search space\nfrom ours. BossNAS searches for CNN-transformer hy-\nbrids, while ours for pure transformers. CvT proposes a\nnew architecture family and searches for the strides and ker-\nnel size of them. Due to the difference of search space, we\ndo not compare them in this work.\nIn summary, our work differs from previous works in\ntwo main aspects. First, we propose a simple and efﬁ-\ncient search methodology dedicated to the structure of vi-\nsion transformers, which has not been well studied. Second,\nthe proposed weight entanglement training strategy enables\nthe subnets in the supernet to be well trained, such that the\nsubnets can inherit weights from the one-shot supernet di-\nrectly, without extra ﬁnetuning or retraining. To our best\nknowledge, this is the ﬁrst work to train a once-for-all trans-\nformer in architecture search.\n6. Conclusion\nIn this work, we propose a new one-shot architec-\nture search method, AutoFormer, dedicated to transformer\nsearch. AutoFormer is equipped with the training strategy,\nWeight Entanglement. Under this strategy, the subnets in\nthe search space are almost fully trained. Extensive ex-\nperiments demonstrate the proposed algorithm can improve\nthe training of supernet and ﬁnd promising architectures.\nOur searched AutoFormers achieve state-of-the-art results\non ImageNet among vision transformers. Moreover, Aut-\noFormers transfer well to several downstream classiﬁcation\ntasks and could be further improved by distillation. In future\nwork, we are interested in further enriching the search space\nby including convolutions as new candidate operators. Ap-\nplying weight entanglement to convolution network search\nor giving the theoretical analysis of the weight entangle-\nment are other potential research directions.\nReferences\n[1] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A conver-\ngence theory for deep learning via over-parameterization. In\nICML, 2019. 5\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3\n[3] Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, and\nYonghui Wu. Training deeper neural machine translation\nmodels with transparent attention. In EMNLP, 2018. 5\n[4] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay\nVasudevan, and Quoc Le. Understanding and simplifying\none-shot architecture search. In ICML, 2018. 3\n[5] Andrew Brock, Theodore Lim, James M Ritchie, and Nick\nWeston. Smash: one-shot model architecture search through\nhypernetworks. In ICLR, 2018. 8\n[6] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and\nSong Han. Once for all: Train one network and specialize it\nfor efﬁcient deployment. In ICLR, 2020. 4, 8\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 8\n[8] Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Jixiang Li. Fair-\nnas: Rethinking evaluation fairness of weight sharing neural\narchitecture search. arXiv preprint arXiv:1907.01845, 2019.\n2, 4\n[9] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christo-\npher D Manning. What does bert look at? an analysis of\nbert’s attention.ACL, 2019. 5\n[10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In CVPR Workshops, 2020. 6\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019. 2\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR, 2021. 1, 2, 7,\n8\n[14] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.\nGradient descent provably optimizes over-parameterized\nneural networks. In ICLR, 2018. 5\n[15] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.\nNeural architecture search: A survey. JMLR, 2019. 8\n[16] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\nZechun Liu, Yichen Wei, and Jian Sun. Single path one-shot\nneural architecture search with uniform sampling. ECCV,\n2020. 2, 3, 4, 6, 8\n[17] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-\njing Xu, Yixing Xu, et al. A survey on visual transformer.\narXiv preprint arXiv:2012.12556, 2020. 1\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 1, 7\n[19] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016. 3\n[20] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoeﬂer, and Daniel Soudry. Augment your batch: better\ntraining with larger batches. CVPR, 2020. 6\n[21] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In ICCV, 2019. 4, 7\n[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In CVPR, 2017. 7\n[23] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. InECCV,\n2016. 5\n[24] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 8\n[25] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021. 1, 8\n[26] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei.\nCollecting a large-scale dataset of ﬁne-grained cars. 2013. 7\n[27] Alex Krizhevsky et al. Learning multiple layers of features\nfrom tiny images. 2009. 7\n[28] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. Albert: A lite\nbert for self-supervised learning of language representations.\nICLR, 2020. 2\n[29] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\nZhang, and Zhuowen Tu. Deeply-supervised nets. In AIS-\nTATS, 2015. 5\n[30] Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang,\nXiaodan Liang, Liang Lin, and Xiaojun Chang. Block-\nwisely supervised neural architecture search with knowledge\ndistillation. In CVPR, 2020. 5\n[31] Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang,\nXiaodan Liang, Liang Lin, and Xiaojun Chang. Blockwisely\nsupervised neural architecture search with knowledge distil-\nlation. In CVPR, 2020. 8\n[32] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng,\nBing Wang, Xiaodan Liang, and Xiaojun Chang. Boss-\nnas: Exploring hybrid cnn-transformers with block-wisely\nself-supervised neural architecture search. arXiv preprint\narXiv:2103.12424, 2021. 8\n[33] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom\nGoldstein. Visualizing the loss landscape of neural nets.\nNeurIPS, 2017. 5\n[34] Liam Li and Ameet Talwalkar. Random search and repro-\nducibility for neural architecture search. In UAI, 2019. 3,\n8\n[35] Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen\nXiong, Rui Hu, and Raquel Urtasun. Polytransform: Deep\npolygon transformer for instance segmentation. In CVPR,\n2020. 8\n[36] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\nheads really better than one? In NeurIPS, 2019. 5\n[37] Maria-Elena Nilsback and Andrew Zisserman. Automated\nﬂower classiﬁcation over a large number of classes. In\nICVGIP, 2008. 7\n[38] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In CVPR, 2012. 7\n[39] Houwen Peng, Hao Du, Hongyuan Yu, Qi Li, Jing Liao, and\nJianlong Fu. Cream of the crop: Distilling prioritized paths\nfor one-shot neural architecture search. NeurIPS, 2020. 3\n[40] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff\nDean. Efﬁcient neural architecture search via parameters\nsharing. In ICML, 2018. 3, 8\n[41] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll ´ar. Designing network design\nspaces. In CVPR, 2020. 7, 8\n[42] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 8\n[43] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\nLe. Regularized evolution for image classiﬁer architecture\nsearch. In AAAI, 2019. 3, 8\n[44] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In CVPR, 2018. 7\n[45] David So, Quoc Le, and Chen Liang. The evolved trans-\nformer. In International Conference on Machine Learning .\nPMLR, 2019. 1, 8\n[46] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021. 7\n[47] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overﬁtting. JMLR, 2014. 5\n[48] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\nMark Sandler, Andrew Howard, and Quoc V Le. Mnas-\nnet: Platform-aware neural architecture search for mobile.\nIn CVPR, 2019. 3\n[49] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. In ICML,\n2019. 2, 4, 7, 8\n[50] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv’e J’egou. Train-\ning data-efﬁcient image transformers & distillation through\nattention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 6, 7,\n8\n[51] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze,\nMatthieu Cord, and Herv ´e J ´egou. Graﬁt: Learning ﬁne-\ngrained image representations with coarse labels. arXiv\npreprint arXiv:2011.12982, 2020. 7, 8\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 8\n[53] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich,\nand Ivan Titov. Analyzing multi-head self-attention: Spe-\ncialized heads do the heavy lifting, the rest can be pruned. In\nACL, 2019. 5\n[54] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and\nRob Fergus. Regularization of neural networks using drop-\nconnect. In ICML, 2013. 5\n[55] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng\nZhu, Chuang Gan, and Song Han. Hat: Hardware-aware\ntransformers for efﬁcient natural language processing. arXiv\npreprint arXiv:2005.14187, 2020. 1, 8\n[56] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. In ECCV,\n2020. 8\n[57] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li,\nDerek F Wong, and Lidia S Chao. Learning deep transformer\nmodels for machine translation. In ACL, 2019. 5\n[58] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 8\n[59] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient con-\nvnet design via differentiable neural architecture search. In\nCVPR, 2019. 2, 8\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-\ning convolutions to vision transformers. arXiv preprint\narXiv:2103.15808, 2021. 8\n[61] Lingxi Xie and Alan Yuille. Genetic cnn. In ICCV, 2017. 8\n[62] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017. 7\n[63] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-\nscale conv-attentional image transformers, 2021. 7\n[64] Jiahui Yu and Thomas S Huang. Universally slimmable net-\nworks and improved training techniques. In ICCV, 2019. 4,\n8\n[65] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender,\nPieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xi-\naodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling\nup neural architecture search with big single-stage models.\nNeurIPS, 2020. 2, 4, 8\n[66] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and\nThomas Huang. Slimmable neural networks. ICLR, 2019.\n2, 4, 8\n[67] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 1, 2, 7, 8\n[68] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiﬁers with localizable\nfeatures. In ICCV, 2019. 6\n[69] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 6\n[70] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020. 8\n[71] Denny Zhou, Mao Ye, Chen Chen, Tianjian Meng, Mingxing\nTan, Xiaodan Song, Quoc Le, Qiang Liu, and Dale Schuur-\nmans. Go wide, then narrow: Efﬁcient training of deep thin\nnetworks. In ICML, 2020. 5\n[72] Barret Zoph and Quoc V Le. Neural architecture search with\nreinforcement learning. ICLR, 2016. 8\n[73] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition. In CVPR, 2018. 8",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7980718612670898
    },
    {
      "name": "Computer science",
      "score": 0.7499971985816956
    },
    {
      "name": "Embedding",
      "score": 0.5999354720115662
    },
    {
      "name": "Scratch",
      "score": 0.5866539478302002
    },
    {
      "name": "Transferability",
      "score": 0.518608570098877
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4673246443271637
    },
    {
      "name": "Architecture",
      "score": 0.450946569442749
    },
    {
      "name": "Machine learning",
      "score": 0.41880857944488525
    },
    {
      "name": "Programming language",
      "score": 0.1401212513446808
    },
    {
      "name": "Engineering",
      "score": 0.10399246215820312
    },
    {
      "name": "Voltage",
      "score": 0.07197436690330505
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ]
}