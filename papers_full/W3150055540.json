{
  "title": "Recommending metamodel concepts during modeling activities with pre-trained language models",
  "url": "https://openalex.org/W3150055540",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3036703292",
      "name": "Martin Weyssow",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A43152936",
      "name": "Houari Sahraoui",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A578966534",
      "name": "Eugene Syriani",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A3036703292",
      "name": "Martin Weyssow",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A43152936",
      "name": "Houari Sahraoui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A578966534",
      "name": "Eugene Syriani",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2912153701",
    "https://openalex.org/W2792792583",
    "https://openalex.org/W1524064724",
    "https://openalex.org/W2157427137",
    "https://openalex.org/W3161222848",
    "https://openalex.org/W2989737153",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2626440528",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4251315550",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3011564318",
    "https://openalex.org/W2042330180",
    "https://openalex.org/W197609290",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W3089878122",
    "https://openalex.org/W2094352021",
    "https://openalex.org/W3082775754",
    "https://openalex.org/W3035907598",
    "https://openalex.org/W2156448859",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W1606004093",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2955704984",
    "https://openalex.org/W3174414731",
    "https://openalex.org/W2094489875",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6601141708",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3105398568",
    "https://openalex.org/W2995333547",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3008088841",
    "https://openalex.org/W3211531381",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2399902329",
    "https://openalex.org/W1994170042",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2404647912",
    "https://openalex.org/W4287684944",
    "https://openalex.org/W2997275048",
    "https://openalex.org/W3021206621",
    "https://openalex.org/W2395124743",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2052033513",
    "https://openalex.org/W3034549508",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2113697305",
    "https://openalex.org/W3095317586",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W4385245566"
  ],
  "abstract": null,
  "full_text": "Noname manuscript No.\n(will be inserted by the editor)\nRecommending Metamodel Concepts during Modeling\nActivities with Pre-Trained Language Models\nMartin Weyssow · Houari Sahraoui · Eugene Syriani\nReceived: 2 April 2021 / Accepted: 15 December 2021\nAbstract The design of conceptually sound metamod-\nels that embody proper semantics in relation to the\napplication domain is particularly tedious in Model-\nDriven Engineering. As metamodels deﬁne complex re-\nlationships between domain concepts, it is crucial for a\nmodeler to deﬁne these concepts thoroughly while being\nconsistent with respect to the application domain. We\npropose an approach to assist a modeler in the design of\nmetamodel by recommending relevant domain concepts\nin several modeling scenarios. Our approach does not\nrequire knowledge from the domain or to hand-design\ncompletion rules. Instead, we design a fully data-driven\napproach using a deep learning model that is able to\nabstract domain concepts by learning from both struc-\ntural and lexical metamodel properties in a corpus of\nthousands of independent metamodels. We evaluate our\napproach on a test set containing 166 metamodels, un-\nseen during the model training, with more than 5000\ntest samples. Our preliminary results show that the\ntrained model is able to provide accurate top-5 lists\nof relevant recommendations for concept renaming sce-\nnarios. Although promising, the results are less com-\npelling for the scenario of the iterative construction\nof the metamodel, in part because of the conservative\nstrategy we use to evaluate the recommendations.\nM. Weyssow\nDIRO, Universit´ e de Montr´ eal, Montreal, Canada\nE-mail: martin.weyssow@umontreal.ca\nH. Sahraoui\nDIRO, Universit´ e de Montr´ eal, Montreal, Canada\nE-mail: sahraouh@iro.umontreal.ca\nE. Syriani\nDIRO, Universit´ e de Montr´ eal, Montreal, Canada\nE-mail: syriani@iro.umontreal.ca\n1 Introduction\nModel-Driven Engineering (MDE) is an increasingly\npopular software development methodology that has\nproven to be valuable in real-world environments and\nfor the development of large-scale systems [4, 28]. In\n2014, Whittle et al. found in their study that the use of\nMDE in practice was even more common than the MDE\ncommunity thought [44]. With the ever-increasing com-\nplexity of software systems, MDE provides a framework\nof practice that facilitates the development and main-\ntenance of such complex systems. Nevertheless, prac-\ntitioners may be reluctant to the use of such technol-\nogy, which requires considerable human eﬀorts during\nthe early stage of the system’s development [3]. A re-\ncent work highlighted the need for Intelligent Modeling\nAssistants (IMA) to better support the development\nof MDE-based systems [30]. Given the aforementioned\ndiﬃculties in MDE in conjunction with the tremendous\namount of available data, these previous works have\nprompted researchers to focus on the development of\ndata-driven recommender systems. Therefore, the de-\nsign of eﬃcient IMAs remains one of the key challenges\nin MDE to ease the cognitive load of modelers and\nspread the discipline further by reducing the eﬀort and\ncost associated to its application.\nOne promising way to assist modelers is to recom-\nmend proper names to software elements that are con-\nsistent with the considered application domain. A previ-\nous study showed that a consistent naming of elements\nplays a critical role in the quality of the metamod-\nels [27]. We believe that metamodels elements should\nbe deﬁned using domain concepts consistent with the\napplication domain to (1) design understandable and\nconsistent systems, (2) ease the communication with\narXiv:2104.01642v3  [cs.SE]  21 Feb 2022\n2 Martin Weyssow et al.\nthe many involved stakeholders, and (3) facilitate the\nmaintenance of the systems.\nThere have been a few research contributions that\nfocused on the development of recommender systems\nfor the auto-completion of models or metamodels. Early\napproaches have focused on logic and rule-based sys-\ntems with model transformation to recommend partial\nmodels [32, 37, 38]. More recent works have focused on\nthe extraction of knowledge from the application do-\nmain through textual data and on the utilization of\npre-trained word embedding models1 [1,8]. On the one\nhand, the early approaches require to manually design\nrules and deﬁne rather complex approaches. On the\nother hand, previous knowledge-based and natural lan-\nguage processing (NLP) approaches require the extrac-\ntion of textual knowledge about the domain that are\nnot always available in practice or that are rather te-\ndious to extract with acuteness.\nIn this paper, we propose a learning-based approach\nto recommend relevant domain concepts to a modeler\nduring a metamodeling activity by training a deep learn-\ning model on thousands of independent metamodels.\nOur approach intends not only to be multi-objective\nby enabling the recommendation of concepts related to\nclasses, attributes and references, but also to be eas-\nily adaptable to diﬀerent end-tasks. Additionally, our\napproach neither requires to extract knowledge about\nthe application domain nor hand-designed features or\nrules. More speciﬁcally, we extract both structural and\nlexical information from more than 10 000 metamodels\nand train a state-of-the-art deep neural language model\nsimilar to BERT [11]. The trained model can then be\nused to recommend relevant domain concepts to the\nmodeler given a speciﬁc modeling context. The goal of\nthe system is to provide a relatively short-ranked list\nof suggestions that will help the modeler through the\ndesign of a metamodel.\nWe evaluate the proposed approach using 166 meta-\nmodels unseen during the training of our language model.\nThe results of our experiments show that our trained\nmodel is able to learn meaningful domain concepts and\ntheir relationships among thousands of metamodels. It\nis also able to recommend relevant classes, attributes,\nand associations concepts in three modeling scenarios.\nIn the two ﬁrst scenario, we show that our model is able\nto recommend relevant domain concepts to rename ele-\nments of an already designed metamodel. In the second\none, we show that our model can also be used to rec-\nommend domain concepts during the whole process of\n1 In this paper, the term “model” refers to a machine learn-\ning model rather than an instance of a metamodel as custom-\nary in the MDE literature. All MDE artifacts we operate on\nare metamodels.\nthe design of a metamodel. Finally, we discuss our ap-\nproach with both modeling scenarios using two speciﬁc\nmetamodels and highlight interesting results and sev-\neral opportunities to improve our approach.\nThe contributions of this paper are as follows:\n– A novel representation of Ecore metamodels encod-\ning their structural and lexical information to learn\ndomain concepts with deep learning models.\n– A publicly available repository containing encoded\nmetamodels, all the data and our trained model.\n– The demonstration of the applicability of the ap-\nproach to design intelligent modeling assistants for\ncommon modeling scenarios.\nThe rest of the paper is structured as follows. In Sec-\ntion 2, we review the deep learning model that we use\nin this work. Next, we go through a motivating exam-\nple and discuss how to transform the metamodels into\ntrees to train our learning model. Section 4 details our\noverall framework. Section 5 presents the experimen-\ntal setting of our evaluation and the addressed research\nquestions. We report the results of our experiments, the\nlimitations of our approach and opportunities of im-\nprovements in Section 6. We discuss the related work\nin Section 7. Finally, we conclude and discuss future\nwork opportunities in Section 8.\n2 Pre-trained language models\nTraditional language models such as recurrent neural\nnetworks [6] take textual data as input and try to pre-\ndict the next word given the previous one as training\nobjective (i.e., causal language modeling). Fig. 2 illus-\ntrates this architecture with long-short term memory\ncells (LSTM) [17] which are widely used in the liter-\nature and helps the recurrent neural network to learn\nlong-range dependencies from the input. Nowadays, the\npre-trained language models architectures based ontrans-\nformers and attention mechanism [42] are generally fa-\nvored in a lot of situations as they result in state-of-\nthe-art performance in a lot of tasks.\nOver the last few years, progresses in natural lan-\nguage processing (NLP) have dramatically increased, in\nparticular, thanks to the design of pre-trained bidirec-\ntional language models such as BERT [11], GPT [33],\nGPT2 [34], GPT3 [7], XLM [24] or XLNet [47]. They\nhave the ability to learn high-level contextual represen-\ntations of text in a self-supervised learning fashion. Be-\nsides, another advantage of these architectures is that\none can adapt the neural network to a speciﬁc end-task\nby ﬁne-tuning it.\nIn this work, we propose to reuse a pre-trained lan-\nguage model architecture without the pre-training. That\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 3\nInput The cat someis eating food .\nRandom \nMasking The cat someis [MASK] food\nRoBERTa  \nLanguage Model\n.\nPrediction 0% ... 15% 5% \neatingwalking zoo\nFig. 1: RoBERTa – Masked language modeling with the input sentence: The cat is eating some food .\nLSTM LSTM LSTM LSTM \nx0 x1 x2\n<s>Input\nLSTM\nLayer\nOutput\ny0 y1 y2\n...\nxn\nyn\n</s>\nThe\nEmbedding\nLayer\nActivation\nLayer\nThe\ncat\ncat is\nfood\nFig. 2: The architecture of a recurrent neural network\nwith long-short term memory cells and with the input\nsentence: <s> The cat is eating some food </s>\nis, we do not use an existing pre-trained language model\nand ﬁne-tune it for our task. Instead, we train one us-\ning our data to learn contextual embeddings of meta-\nmodel concepts. More speciﬁcally, we use a RoBERTa\narchitecture and masked language modeling as training\nobjective [25].\nRoBERTa is a language model architecture that im-\nproves BERT [11]. Its main objective is to reduce the\ncomputational costs of BERT and improve the mask-\ning algorithm. Fig. 1 illustrates the masked language\nmodeling objective 2. The masked language modeling\nobjective consists of masking a percentage of the input\nwords (i.e., typically 15%). The objective for the model\nis to predict each masked word. Additionally, RoBERTa\narchitecture improves the pre-training data representa-\ntion by tokenizing them using byte-level pair encoding\n(similarly to GPT-2 ). The advantage of such tokeniza-\ntion algorithm is that it produces an open-vocabulary\nand allows encoding rare and unknown words. The us-\nage of pre-trained language models in software engineer-\ning for program understanding tasks has shown to have\nsome signiﬁcant impacts thanks to their ability to learn\n2 The illustration is inspired from the following blog: http:\n//jalammar.github.io/illustrated-bert/\npowerful embedding spaces and to assist developers in\ncomplex tasks [15,18,20].\n3 Motivation and metamodel representation\nIn this section, we ﬁrst present a motivating example\nto illustrate the importance of developing recommender\nsystems to assist modelers in the design of metamodels.\nThen, we discuss how to transform metamodels into\nstructures that can be used to train language models\npresented in Section 2.\n3.1 Motivating example\nTo illustrate the rationale behind this work and the\nneed for intelligent modeling assistants (IMAs), let us\nconsider the motivating example in Fig. 3 where a mod-\neler is designing the metamodel of ﬁnite state machines\n(FSM).\nLet us also consider that the modeler is working in\na modeling environment, such as Eclipse that provides\na range of Ecore elements that can be dragged and\ndropped to build the metamodel. These elements are\ndeﬁned in the abstract syntax of Ecore and are manip-\nulated through a concrete syntax as depicted in Fig. 3.\nThus, such modeling environments are easily able to en-\nsure the syntactical correctness of the metamodel un-\nder design by checking its conformance to the Ecore\nabstract syntax. However, the existing modeling envi-\nronments generally do not incorporate tools to help the\nmodeler to design pure semantic aspects of the meta-\nmodel. Here, we refer to the semantic as the ontological\nmeaning of the metamodel but also as the meaning of\neach elements, the relationships among them and their\nmeaning in the application domain. Given the example\nin Fig. 3, the ontological meaning of the metamodel can\nbe mapped to a concrete description in the application\n4 Martin Weyssow et al.\nFSM State \n- name: EString\nT ransition \n- event: EStringstates\ninitialState1..1\n0..*\nFinalState \nsource\ntarget1..1\n1..1\ntransitions\n0..*\nFig. 3: Motivating example – ﬁnite state machine metamodel.\ndomain of FSMs. Each single element of the metamodel\nhas a meaning that depends on its instantiation in the\nmetamodel. For example, a FSM is composed of states\nwhich can eventually be ﬁnal states. The concept of\n“State” depends on how it is deﬁned and how it inter-\nacts with other elements of the metamodel. In practice,\na modeler deﬁnes implicitly the semantic of the meta-\nmodel by instantiating domain concepts (e.g., the con-\ncept of FSM or State) and deﬁning how these concepts\nshould be related.\nManipulating these domain concepts in an adequate\nway is tedious for the modeler as (1) she may not be\nan expert of the application domain, (2) metamodels\ncan rather become complex, and (3) the concepts she\nmanipulates during a modeling activity must be han-\ndled so that the metamodel embodies correct seman-\ntics for the application domain. In the FSM example,\nit makes sense to have a class called “ Transition” that\nreferences another class “ State” via two associations:\none for the source and one for the target state of the\ntransition. In fact, all the aforementioned concepts are\nwell-deﬁned in the theory of FSM and the names chosen\nin the provided example are consistent since they refer\nto concepts coming from the application domain. Addi-\ntionally, for more complex metamodels that could either\nbecome large or mix application domains, it is crucial\nto have coherent and conceptually consistent usage of\ndomain concepts to guarantee a certain quality of the\nmetamodel.\nTherefore, our goal is to alleviate the modeler’s bur-\nden by developing an IMA that could provide relevant\nrecommendations of domain concepts for a given meta-\nmodel. In fact, we envision that both structural and tex-\ntual regularities can be abstracted from existing meta-\nmodels using a NLP learning model as follows: (1) the\nstructural regularities can help the model to determine\nwhat type of metamodel element is likely to come af-\nter a speciﬁed context and (2) the model textual regu-\nMET AMODEL \nCLASS \n«cls.name»\nA TTRIBUTES ASSOCIA TIONS \n«attr1.type»\n«attr1.name»\n«attrn.type»\n«attrn.name»\n... «assoc1.target»\n«assoc1.name»\n«assocm.target»\n«assocm.name»\n... \nNAME \nFig. 4: General structure of a metamodel tree\nlarities can help the model to determine a meaningful\nembedding space which consists of a vector space that\nencodes semantic properties of the metamodel domain\nconcepts. These regularities can then be leveraged to\nbuild on a recommender system that is able to adapt\nitself to the current modeling context, i.e., by providing\nrelevant recommendations, ranked by likelihood, given\na metamodel under design.\n3.2 Metamodels as trees\nSince we rely on a NLP learning model, the data needs\nto be available in a textual format. We propose a way\nto extract meaningful information from the metamodels\nby transforming them into trees that capture both lex-\nical and structural information about the metamodels.\nIn view of the target modeling tasks, we extract meta-\nmodel elements that support the task(s) while ignoring\nelements that may not be relevant and that could in-\ntroduce noise during the training of our model.\nFig. 4 depicts the general tree structure of a trans-\nformed metamodel. The transformation assumes that\na metamodel contains classes, their attributes and the\nassociations. The resulting tree is rooted by a generic\nnode representing the metamodel. Then, each class in\nthe metamodel deﬁnes a sub-tree containing its own\ninformation rooted in a CLS node. From left to right,\nwe ﬁrst have a node NAME that references a leaf node\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 5\nMET AMODEL \nCLS CLS \nASSOCS NAME A TTRS NAME \nState \nname initialState states \nEString State State FSM \nCLS \nNAME \nT ransition \nA TTRS \nevent \nEString \nASSOCS \ntarget \nState \nFig. 5: Tree structure of the partial FSM metamodel.\nwith the class name. Secondly, the ATTRS node contains\ntwo nodes for each attribute of the class: one for its\ntype and the leaf is for its name. Similarly, the ASSOCS\nnode contains nodes representing all associations (in-\ncluding compositions) with the current class as source.\nWe create two nodes per association: one for the name\nof the target class and the other for the name of the\nassociation. For both attributes and associations, we\nalso encode their types to provide additional semantic\ninformation to the learning model. The rationale for\ndeﬁning this tree structure is that it speciﬁes a hierar-\nchy between metamodel elements that can be learned\nby a language model. Fig. 5 shows the tree structure of\nthe non-shaded part of the metamodel in Fig. 3. This\ntree representation is similar to the one used by Bur-\ngueno et al. [9] with which the authors learned model\ntransformations using neural networks. Our tree-based\nrepresentation diﬀers from theirs as they transform in-\nstances of models into tree representations that encode\ninformation such as the values of the attributes that are\ninstance-speciﬁc. Note that our representation does not\ncapture composition hierarchies explicitly.\nOur approach does not depend on how trees are\ndeﬁned. For instance, it is possible to ignore the tar-\nget of associations or to include generalization rela-\ntions. Additionally, the order in which elements appear\nin the tree could be altered. In this work, we choose\nto work with the “natural” order of elements as they\nappear in the metamodel. If, for example, metamod-\nels are provided in Ecore, we assume that the order in\nwhich their elements appear in Ecore ﬁle reﬂects (par-\ntially) the order in which a modeler would have de-\nsigned and saved the metamodel. In addition to that,\neven though we present a tree-based representation for\nMOF-like metamodels, our approach can generalize to\nother kind of metamodels which can be expressed as\nrelational graphs.\nFinally, the tree representation of the metamodel\nallows us to train a language model by ﬂattening its\nstructure into a textual sequential representation. We\ndiscuss this aspect of the training phase in the next\nsection.\n4 Learning metamodel domain concepts\nTo tackle the problem of recommending relevant do-\nmain concepts to the modeler during a metamodeling\nactivity, we envision that the high-level concepts ma-\nnipulated during such an activity ( e.g., the concept of\nFSM in Fig. 3) can be abstracted from a dataset of\ndesigned metamodels. Deep learning, and particularly\nself-supervised and unsupervised learning techniques,\nlearn representations that encode semantic information\nabout the world: in our case, metamodels. We foresee\nthat pre-trained language models can be leveraged to\nlearn domain concepts of metamodels. Such a trained\nlearning model can be used to perform predictive mod-\neling task to help a modeler to design conceptually\nsound metamodels. In this work, we present our ap-\nproach on two practical applications, but our model\ncan easily be adapted for other tasks by ﬁne-tuning it.\nIn this section, we present a framework to learn the\naforementioned domain concepts and exploit them to\nrecommend relevant concepts to the modeler by lever-\naging contextual information speciﬁc to the current mod-\neling activity. Fig. 6 depicts the 3-phased architecture\nof our approach. We explain each phase in what follows.\n4.1 Data Extraction and Model Training\nIn order to train eﬃciently pre-trained language model,\nwe need to gather data relevant to the target objec-\ntive. In general, the more data we have, both in term\nof diversity and quantity, the better it is to allow the\nmodel to generalize on unseen examples. Model reposi-\ntories [5, 16, 31] are rich with metamodels, often stored\nin XMI, that can be crawled. In Fig. 6, the crawled\nmetamodels enter the pipeline through the Data Ex-\ntraction and Model Training process.\nGiven the metamodels in a serialized format, the\nﬁrst step consists of transforming these metamodels\ninto a format that can be given as input to a lan-\nguage model. In order to achieve this step, we deﬁne in\n1.1 an Ecore-to-text Generator component. The role\nof this component is to transform metamodels into tex-\ntual tree structures as presented in Section 3.2. The\nmost straightforward way to perform this transforma-\ntion is to deﬁne a model-to-text transformation. For\neach metamodel, the transformation outputs its corre-\nsponding tree structure representation 1.2 as depicted\nin Fig. 7 for the non-shaded part of the FSM meta-\nmodel presented in Fig. 3. Even though the language\nmodel takes sequential data as input, such a represen-\ntation allows the model to learn from both structural\n6 Martin Weyssow et al.\nMetamodel Repositories\n1.3\nMetamodels  \nas Trees\nLanguage \nModel \n1. Training objective \n2. Offline model training \n3. Hyperparameter tuning\n1.2\n1.1\n Data Extraction and Model Training1\nEcore-to-text  \nGenerator \nTest Samples Generation 2\nSampling Strategy\nTest Samples  \nGenerator \n     ( <MODEL> ( <CLS> ( <NAME> FSM ) \n     ( <ASSOCS> ( State states ) ( State initialState ) ) )   \n     ( <CLS> ( <NAME> State ... ) ))\nground-truth\ncontext\nground-truth\ncontext\n     ( <MODEL> ( <CLS> ( <NAME> FSM ) \n     ( <ASSOCS> ( State states ) ( State initialState ) ) )   \n     ( <CLS> ( <NAME> State ... ) ))\nTrained Language \nModel \nTest Sample  \nContext\n3.2\n2.1\n2.2\nuses infer\ncontext3.1\nMost-Likely Suggestions \n(top-k) \nEffectiveness \nMetrics\nground-truth\n3.3\nPrediction and Effectiveness3\nTest set\nTrain set\nFig. 6: Overall framework of the approach.\nand lexical information contained within a metamodel.\nThat is, the structural aspect of the metamodel can still\nbe implicitly learned by the language model as the tex-\ntual format incorporates syntactical elements that allow\nit to contain hierarchical and structural information of\nthe metamodel. Also, our approach could be used with\nthe text-based representation of metamodels proposed\nas part of the Eclipse Emfatic framework [13].\nNext, the metamodels are separated into a training\nset and a test set. The latter is left aside for the next\nsteps of the approach. Before training a model, the data\ncan pass through a NLP pipeline that could include sev-\neral processes such as text normalization or tokeniza-\ntion. In practice, applying tokenization techniques such\nas subword tokenization or byte-pair encoding [39] be-\nfore training a language model helps reducing the size of\nthe vocabulary and produces an open-vocabulary that\nprovides the ability to a trained model to generalize\nbetter at testing time. At step 1.3 , we train a lan-\nguage model using the training set in an oﬄine mode.\nPrior to training, we deﬁne a training objective that the\nlanguage model will try to fulﬁll during the training in\norder to update its parameters and learn from the data.\nFor instance, the training objective could be a masked\nlanguage modeling objective similar to BERT [10] or a\npermutation language modeling objective used in XL-\nNet [46]. Finally, the hyperparameters of the language\nmodel are tuned using either a validation set extracted\nfrom the training set or k-fold cross-validation.\n4.2 Test Samples Generation\nBefore evaluating the trained model on a useful model-\ning task, we deﬁne a process which aims at generating\ntest samples for the deﬁned target task. Here, we em-\nphasize the fact that the modeling task should reﬂect an\nactual use case that makes sense in the real world in or-\nder to evaluate the eﬀectiveness of the trained model in\na relevant modeling scenario. This important aspect is\nmaterialized by the Sampling Strategy 2.1 which must\nbe conceived in a thoughtful way to concretise a mod-\neling scenario as close as possible to a real-world use\ncase. In fact, as we will see further, the sampling of a\nparticular test data point corresponds to a state of a\nmetamodel at a speciﬁc time. Depending on the mod-\neling task, the sampling is not always straightforward\nand requires some thought.\nGiven a sampling strategy, theTest Samples Gener-\nator generates test samples using the test set that cor-\nresponds to the strategy 2.2 . For the sake of clarity,\nwe showcase one possible sampling strategy in Fig. 8.\nThis particular sampling represents a renaming model-\ning scenario. That is, given a fully-designed metamodel,\nwe obfuscate one element of the metamodel (i.e., ground-\ntruth) and consider all the other elements as its context.\nFor a trained model, the goal would be to predict the\nground-truth considering that context as input. In the\nreal-world, this scenario could be considered to rename\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 7\n  ( <METAMODEL>  \n    ( <CLS> ( <NAME> FSM ) ( <ASSOCS> ( State states ) ( State initialState ) ) )   \n    ( <CLS> ( <NAME> State ) ( <ATTRS> ( EString name ) ) ) \n    ( <CLS> ( <NAME> Transition ) ( <ATTRS> ( EString event ) ) ( <ASSOCS> ( State target ) ) ) \n  )\nFig. 7: Textual tree structure of the partial FSM metamodel (see Fig. 3).\nelements of a metamodel that do not represent well the\nmanipulated concepts for a particular application do-\nmain. In Section 5, we describe three modeling scenar-\nios on which we evaluate our approach.\n4.3 Prediction and Eﬀectiveness\nThe last process of our framework concerns the evalu-\nation of a trained language model. For the model to be\nable to provide meaningful recommendations, we need\nto leverage the contextual information of each test sam-\nple, as discussed previously. Given a test sample, we\nprovide its context to the language model 3.1 . Then,\nthe model outputs a list of candidates that are likely to\nmatch the obfuscated element (i.e., ground-truth). At\nstep 3.2 , this list is retrieved using a parameter k that\ndetermines the size of the list. Finally, at step 3.3 , we\ncompute eﬀectiveness metrics to evaluate the ability of\nthe model to provide relevant recommendations given\nthe current modeling scenario.\nTo summarize, the trained model can be seen as\na recommender system that takes as input contextual\ndata about the modeling activity and outputs a ranked-\nlist of most-likely recommendations that are assessed\nthrough the usage of widely-used eﬀectiveness metrics.\n5 Experimental setting\nTo evaluate our approach, we articulate our research\nquestions around three simulated real-world modeling\nscenarios. The goal of our experiments is the following:\nGoal: Determine to what extent a state-of-the-art\npre-trained language model can provide meaning-\nful domain concepts recommendations to a modeler\nduring a modeling activity to reduce its cognitive\nload and ease the design of complex metamodels.\nIn this section, we formulate the research questions\nand follow with detailed discussions about our dataset,\nhow we addressed the research questions, the model\ntraining and the evaluation metrics.\n5.1 Research Questions\nThe experiments are divided into two parts. Each of\nthe following research question aims at assessing the\nusefulness of the aforementioned model in a distinct\nmodeling scenario:\n– RQ1 – Scenario 1:Given an already designed meta-\nmodel, is our approach able to recommend relevant\ndomain concepts to rename metamodel elements?\nIn this scenario, we assume the metamodel if com-\npletely deﬁned. The objective is then to evaluate\nour model on its capability to recommend useful\ndomain concepts to rename speciﬁc elements of the\nmetamodel such as classes, attributes, and associ-\nations by considering all of the information of the\nmetamodel as a global context.\n– RQ2 – Scenario 2:Does considering a local con-\ntext made of the metamodel elements close to the\none being renamed increases the eﬀectiveness of the\nsystem?\nThis scenario is similar to the one in RQ1 except\nthat the sampling strategy diﬀers. By considering a\nglobal context we may introduce noisy contextual\ndata that could reduce the precision of our model.\nTherefore, we investigate whether considering a lo-\ncal context made only of metamodel elements close\nto the one being renamed can improve the eﬀective-\nness of our model.\n– RQ3 – Scenario 3:While incrementally designing\na metamodel, is our approach able to recommend\nrelevant domain concepts at each increment, while\nmaintaining a prediction context made of the ele-\nments in a previous increment?\nThis modeling scenario is substantially more com-\nplex than the previous one. It aims at simulating\nhow a modeler designs a complete metamodel in a\nmodeling editor. Here, sampling of the test data is\nmore complex. It allows us to report on the evolu-\ntion of the eﬀectiveness of our model as the meta-\nmodel is designed progressively.\nThe last research question provides an in-depth eval-\nuation of our approach on two speciﬁc metamodels.\n8 Martin Weyssow et al.\ncontext ground- \ntruth \nState \n  ( <METAMODEL>  \n    ( <CLS> ( <NAME> FSM ) ( <ASSOCS> ( State states ) ( State initialState ) ) )   \n    ( <CLS> ( <NAME>              ) ( <ATTRS> ( EString name ) ) ) \n    ( <CLS> ( <NAME> Transition ) ( <ATTRS> ( EString event ) ) ( <ASSOCS> ( State target ) ) ) \n  )\n?\nFig. 8: Excerpt of the generation of a test sample in a renaming scenario.\nTable 1: Training set used in the experiments. Iden-\ntiﬁers corresponds to the total number of classiﬁers,\nattributes and associations in the dataset. Types is\nthe number of unique identiﬁers. Hapax Legomena is\nthe number of identiﬁers that appear only once in the\ndataset.\n# Identiﬁers Types # Hapax Legomena\nTrain Set 267.370 58.027 31.911 (55%)\n– RQ4 – Use Cases:What are the advantages and\nthe limitations of the latter?\nWe choose one domain-speciﬁc and one general pur-\npose metamodel in our test set and perform a qual-\nitative analysis to identify speciﬁc cases where our\napproach performs well or not. We highlight its lim-\nitations as well as some opportunities of improve-\nment.\n5.2 Datasets\nWe used the MAR dataset3 which contains around 17 000\nEcore metamodels crawled from Github and AtlanMod\nZoo [26]. The former consitute the training dataset and\nthe latter is the test set. Before extracting tree-structured\ndata from the metamodels, we remove metamodels con-\ntaining less than 2 and more than 15 classes. With too\nfew classes, our pre-trained language model would not\nbe able to learn meaningful information from the meta-\nmodels due to a lack of context. Conversely, two many\nclasses would lead to considering very large trees, result-\ning in an intractable and computationally costly train-\ning phase. We end up with 10 000 metamodels for train-\ning, 1 112 for the validation of the hyperparameters of\nthe pre-trained language model and 166 for testing.\nAfter ﬁltering the dataset, we extract trees as de-\nscribed in Section 3.2. We use Xtend 4 combined with\nEclipse EMF API 5 to programmatically transform the\n3 http://mar-search.org/experiments/models20/\n4 https://www.eclipse.org/xtend/\n5 https://www.eclipse.org/modeling/emf/\n0\n5\n10\n15\n20\n25\n30\n35\n40\nNumber of test \nmetamodel\nNumber of elements in the metamodel\nFig. 9: Distribution of the metamodels in the test set\nper intervals of number of elements\nmetamodels into trees using a model-to-text transfor-\nmation. Table 1 reports the number of identiﬁers (i.e.,\nclasses, attributes, and associations), the number of types\n(i.e., unique identiﬁers) and the number of hapax legom-\nena (i.e., identiﬁers that appear only once). As we can\nobserve, we have a high proportion of hapax legomena\n(i.e., more than 50% of the unique identiﬁers) in the\nresulting dataset. Considering those identiﬁers that ap-\npear very rarely in a corpus could lead to learning a\nlot of noise. To cope with this issue, we apply a byte-\npair encoding tokenization algorithm [39]. The objec-\ntive of this process is to decompose identiﬁers into sub-\nword units (or merges). The tokenizer keeps the most-\ncommon merges in a vocabulary. Thanks to byte-pair\nencoding, the pre-trained language model is able to gen-\nerate identiﬁers unseen during training or rare identi-\nﬁers by composing from the merges.\nIn Fig. 9, we report the distribution of our test meta-\nmodels in term of their number of elements, where an\nelement is either a class, an attribute, or an association.\nThis ﬁgure shows that our test set is diverse, which en-\nables the evaluation of our approach in a various range\nof situations.\n5.3 How we address RQ1 – RQ2\nBoth RQ1 and RQ2 rely on a common end-task which is\nabout renaming metamodel elements. For instance, the\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 9\n? State \n- name: EString\nT ransition \n- event: EStringstates\ninitialState1..1\n0..*\nFinalState \nsource\ntarget\n1..1\n1..1\ntransitions0..*\nFig. 10: Example of the local context sampling strategy.\ntask could make sense in a context where a metamodel\nwould have been deﬁned with rather vague concepts. In\nthis particular situation, we believe that our approach\ncould be valuable to help the modeler to rename the\nelements of the metamodel in a meaningful way.\nWe refer to the sampling strategy of RQ1 as aglobal\ncontext sampling because we sample each test data (i.e.,\nan element to be renamed) by considering the whole\nmetamodel as its context. We discussed this sampling\nin Section 4.2 as one possible sampling strategy that\ncan be applied in our approach.\nAlternatively, we refer to the sampling strategy of\nRQ2 as a local context sampling . The rationale for in-\nvestigating this research question is that we conjecture\nthat a global context sampling does not allow the ap-\nproach to scale to medium-large metamodels. Consider-\ning a global context could include elements of the meta-\nmodels that are somehow unrelated to the element of\ninterest and could thus result in imprecise predictions.\nAs a consequence, we investigate whether a local con-\ntext sampling could increase the eﬀectiveness of our pre-\ntrained language model by limiting the context of the\nelement to be renamed to only elements that are close\nto it in the metamodel. In Fig. 10, we illustrate this\nsampling strategy on the FSM example. The context\ncorresponds to the non-shaded part of the metamodel\nand the mask is “FSM”. The local context contains the\nmetamodel elements that are directly connected with\nan association to the element that needs to be pre-\ndicted. In the case of attributes, we keep the elements\nthat are linked with the class of the attribute in the\ncontext.\n5.4 How we address RQ3\nTo address RQ3 thoroughly, we seek to simulate the\nconstruction of the metamodel in a real modeling en-\nvironment. For instance, we take into account the fact\nthat two classes must exist before a modeler creates an\nassociation between them. Fig. 11 illustrates the sam-\npling on the FSM metamodel.\nTo initiate the sampling, we start with an empty\nmetamodel and ﬁrst retrieve a root class, i.e., one that\ndoes not have any incoming association (like the FSM\nclass). If the metamodel does not contain such an ele-\nment, we choose the class that has the least number of\nincoming associations as root. Note that the root class\nis not predicted as our model requires contextual data\nto provide recommendations. We set the root class as\nour current class. The rest of the sampling process is\niterative:\n1. Iterate over all the class that are linked to the cur-\nrent class and choose one randomly.\n2. Generate a test sample where the ground-truth is\nthe name of the chosen class and generate one test\nsample for each of its attributes.\n3. Create one test sample for each association that\nlinks the current class and the chosen one.\n4. Choose the next current class that can be reached\n(by an association) from the current one. If none can\nbe reached, the next current class is unvisited yet\nand has the least number of incoming associations.\n5. Loop back to step 1 and stop the process when the\nmetamodel is fully re-constructed.\nAs we progress in the construction of the meta-\nmodel, the context that can be used to predict the next\nelement increases. In this scenario, we consider the con-\ntext to be all the elements that were previously con-\nstructed. Therefore, we suspect that the eﬀectiveness\nof the trained model will increase as the construction\nevolves thanks to the growth of the context that pro-\nvides more contextual information about the current\nmodeling activity. Nonetheless, we emit certain reserves\non this assumption as the growing size of the context\nmay also introduce noise as it may contain metamodel\nelements far from the one being that do not contribute\nto make the model accurate given a current modeling\ncontext.\n5.5 How we address RQ4\nThe objective of RQ4 is to report qualitative results\non two conceptually diﬀerent metamodels drawn from\nour test set. The ﬁrst one is a Petri nets metamodel\nand the second is a Java metamodel. The former uses\nrather broad concepts that seem to be less domain-\nspeciﬁc and more widely-used in the deﬁnition of com-\nmon metamodels than the latter metamodel. Therefore,\nthis RQ allows us to compare the eﬀectiveness of our ap-\nproach depending to which extent the test metamodel\nis domain-speciﬁc.\nWe report several top-5 recommendations lists pro-\nvided by our trained model for both metamodels to\n10 Martin Weyssow et al.\nFSM \n(a)\nFSM State \n- name: EString (b)\nFSM State \n- name: EStringstates\ninitialState1..1\n0..* (c)\nFSM State \n- name: EString\nT ransition \n- event: EStringstates\ninitialState1..1\n0..*\n(d)\nFSM State \n- name: EString\nT ransition \n- event: EStringstates\ninitialState1..1\n0..*\nsource\ntarget1..1\n1..1\ntransitions0..* (e)\nFig. 11: Illustration of the constructive test sampling\nshowcase the ability of our model to recommend con-\ncepts close to the application domain of the test meta-\nmodel. We chose to report top-5 recommendations as it\nallows us to analyze qualitatively the model in a quiet\nrestrictive scenario and as reporting more recommenda-\ntions does not aﬀect the conclusions of the evaluation.\nTo achieve this, we use the modeling scenario of RQ1\n– RQ2 that gives us the best performance. Finally, we\nevaluate the scenario of RQ3 on both metamodels.\n5.6 Model Training\nPre-trained learning models contain various hyperpa-\nrameters that need to be optimized to avoid phenomena\nsuch as overﬁtting the data or low generalization per-\nformances. To train our models, we divided our training\nset into a training and validation set with a 90/10 ra-\ntio. We monitored the model losses on both training\nand validation sets after each training iteration, where\none iteration means that the model has gone through\nall the training samples. The losses enable us to monitor\nthe evolution of the training of the model over the itera-\ntions and help us determining when to stop the training\nphase to avoid phenomena such as overﬁtting. Due to\nthe wide range of hyperparameters and the complexity\nof the model, we were not able to optimize all of them.\nWe report the hyperparameters values in Appendix B.\nWe trained a RoBERTa language model [25] us-\ning a masked language modeling objective with Hug-\ngingface’s implementation [45]. We use a Linux-Gentoo\nServer with 4 x Nvidia GeForce RTX 2080 GPUs, an\nAMD Ryzen Threadripper 3970X CPU and 126 GB\nof RAM. The training is independent from the testing\nphase as it is done oﬄine and the optimal model is saved\non a hard disk.\nDuring the training phase, the model masks ran-\ndomly 15% of the tokens in the input and attempts\nto reconstruct them (i.e., masked language modeling\nobjective). We did not ﬁne-tune the model for any sce-\nnario, i.e., during the training phase, the model does\nnot learn how to adapt itself to each speciﬁc scenario.\nThis ﬁne-tuning step can be achieved by resampling all\nthe training sample according to each speciﬁc scenario\nand resuming the training from a previous checkpoint.\nAdditionally, the model we provide can be ﬁne-tuned\nfor other tasks that rely on diﬀerent objectives. For in-\nstance, it could be ﬁne-tuned on a translation objective\nthat translates metamodel requirements into an actual\nmetamodel. We discuss some of these opportunities in\nthe future work section.\n5.7 Eﬀectiveness Metrics\nWe measure the performance of our model on predic-\ntive tasks that aim to provide a relevant recommenda-\ntion list for a given target. The recommendation is a\nlist of strings ranked in decreasing order of relevance.\nTo evaluate quantitatively our model, we use two com-\nmonly used metrics in very similar tasks such as code\ncompletion [19, 41, 43], namely the Recall@ k and the\nMean Reciprocal Rank (MRR). We consider a recom-\nmendation to be correct only if it matches exactly the\nground-truth.\nThe Recall@k for a set of test samples is the num-\nber of times the ground-truth (i.e., target) appears in\na recommendation list of size k, on average. Therefore,\na greater value of Recall@ k is preferable as the system\nwould make more accurate recommendations.\nThe MRR for a set of test samples of size T is the\ninverse of the rank of the correct recommendation in\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 11\nthe list, on average:\nMRR = 1\n|T|\n|T|∑\ni=1\n1\nranki\nThe MRR is complementary to the Recall@k as it\ngives an idea of the ability of the model to suggest rel-\nevant tokens at the top of the list. For example, if on\naverage, the correct recommendation appears at rank\n2, the MRR is 0.5. Thus, a MRR close to 1 results in a\nperfect system.\nNote that in all our experiments, the modeling tasks\nare rather complex and restrictive for the model as it\nhas to predict the complete ground-truth and we do not\nconsider a partially correct recommendation to be par-\ntially correct. As a consequence, only recommendations\nthat contain the complete ground-truth have a positive\nimpact on the eﬀectiveness of the system.\n6 Results\nIn this section, we present the results of our experiments\nand answer the research questions.\n6.1 RQ1 – Metamodel Renaming with Global Context\nSampling\nWe start our experiments by determining whether our\npre-trained language model is able to recommend mean-\ningful domain concepts (class, attribute, and associ-\nation identiﬁers) when considering a global sampling\nstrategy. The upper part of Table 2 shows the top-1\naccuracy, recall, MRR when varying the size of the rec-\nommendation lists. As discussed previously, we consider\na recommendation to be accurate when it contains an\nelement that exactly matches the expected identiﬁer\n(ground-truth).\nThe ﬁrst observation that we can draw is that our\napproach is able to predict the correct ground-truth\n(i.e., top-1) when providing only one recommendation\ncandidate. For the three prediction tests, the top-1 score\nis above 30% and reaches almost 40% for the prediction\nof classes. This already shows that our model is able to\nlearn domain concepts from our training set and is able\nto generalize to some extent to unseen metamodels.\nSecondly, we can observe a common increasing trend\nof the eﬀectiveness of the approach as we increase the\nnumber of candidates in the predictions, i.e., Recall@k\nincreases when k increases. However, the MRR does\nnot increase like the recall, which is not surprising. In-\ndeed, the MRR@5 of 0 .42, 0.35 and 0 .39 indicate that\nthe majority of the correct recommendations generally\n0\n0,1\n0,2\n0,3\n0,4\n0,5\n0,6\n0,7\n0,8\n0,9\n1\nRecall@10\nPrediction occurrence in the training set\nclasses attributes associations\nFig. 12: [RQ1] – Comparison of the Recall@10 obtained\nw.r.t the occurrences of the recommended concepts in\nthe training set.\nappear among the three ﬁrst candidates. Consequently,\nthe recommendations that account for the increasing in\nthe Recall@10 and Recall@20 provide the correct can-\ndidate at a low rank in the list. Thus, even though in-\ncreasing the number of candidates increases the recall,\nit is still able to produce relatively accurate top-5 rec-\nommendations. This is valuable for modelers to keep\nthe auto-completion list short.\nFinally, in Fig. 12 we analyze the evolution of the\nRecall@10 for each type of recommendation depending\non the occurrences of the recommended concepts in the\ntraining set. As we can observe, the model is not able\nto recommend hapax legomena (see Table 1) and un-\nseen concepts. However, the eﬀectiveness keeps increas-\ning as the interval of occurrence increases reaching 80 –\n90% Recall@10 for the most-common concepts. Thus,\nuncommon concepts at test time are responsible for a\nsigniﬁcant drop in the eﬀectiveness of the model as the\naverage Recall@10 reported in Table 2 does not exceed\n48%. In addition, this level of accuracy is obtained with\nat least 11 occurrences of the concept, which is already\nlow considering the size of our training set.\nOverall, we notice that the model is slightly more\naccurate when predicting classes. One might think that\nclasses are more diﬃcult to predict than attributes and\nassociations because they embody higher-level concepts\nthan the latter two. This shows that domain concepts of\nhigher level and some of their relationships are proba-\nbly more recurrent among metamodels than local/low-\nlevel concepts. Thus, our pre-trained language model is\nable to learn meaningful representations of these high-\nlevel concepts. Based on the number of test samples\n(i.e., more than 5 000), we can state the following.\n12 Martin Weyssow et al.\nTable 2: [RQ1 - RQ2] – Results in term of Top-1 score, Recall@k and MRR@k.\nMetrics\nScenario Test Top-1 R@5 MRR@5 R@10 MRR@10 R@20 MRR@20 Size\nFull Context\nClasses 39.79 % 45.45 % 0.42 47.97 % 0.42 52.40 % 0.42 1 626\nAttributes 30.64 % 43.02 % 0.35 46.79 % 0.36 50.94 % 0.36 1 325\nAssociations 35.41 % 42.99 % 0.39 45.18 % 0.39 46.88 % 0.39 3 296\nLocal Context\nClasses 27.13 % 41.97 % 0.33 45.88 % 0.33 50.71 % 0.33 1 408\nAttributes 24.47 % 37.72 % 0.30 42.31 % 0.30 46.11 % 0.30 1 132\nAssociations 31.79 % 42.69 % 0.36 44.85 % 0.36 46.38 % 0.37 3 284\n0,2\n0,25\n0,3\n0,35\n0,4\n0,45\n0,5\n0,55\ntop-1 top-5 top-10 top-20\nRecall\nclasses (full) attributes (full) associations (full)\nclasses (local) attributes (local) associations (local)\nFig. 13: [RQ1 - RQ2] – Comparison of the level of recalls\nobtained for each prediction test (full: global context\nsampling = RQ1; local: local context sampling = RQ2)\nAnswer to RQ1:Our approach can recommend\nrelevant domain concepts, that were seen at least\ntwice during the training phase, to rename meta-\nmodel elements when it considers a global context\nto provide predictions.\nAs we will illustrate it in the answer to RQ4, we\ncan improve the quality of the recommendations if we\nconsider partial or semantically equivalent concepts.\n6.2 RQ2 – Metamodel Renaming with Local Context\nSampling\nIn this second experiment, we compare the performance\nof both global and local context sampling strategies.\nFig. 13 shows the evolution of the recall obtained for\neach strategy when the number of recommended can-\ndidates increases.\nFor the three prediction tests, the local context sam-\npling has a lower recall rate than the global context\nsampling strategy. These results contradict our con-\njecture that most of the semantics of an element in a\nmetamodel can be explained locally and that consider-\ning other elements might introduce noise to the model.\nInstead, the results suggest that considering all the ele-\nments of the metamodel helps to make the model more\naccurate.\nAnswer to RQ2: Reducing the context to ele-\nments local to the one being predicted does not\nimprove the eﬀectiveness of our apporach.\nInstead, a global context sampling strategy allows\nthe model to provide more accurate recommendations\ndue to the additional context.\n6.3 RQ3 – Incremental Construction of a Metamodel\nWe evaluate the capacity of our system to recommend\nrelevant domain concepts to a modeler during the con-\nstruction of a metamodel. As we progress in the con-\nstruction of a metamodel, we are able to infer more\ncontext to the trained model. Therefore, we evaluate\nthe impact of the context on the eﬀectiveness as it re-\nﬂects the eﬃciency of the system through the whole\ndesign process.\nIn Fig. 14, we report the distribution of the test set\nin this sampling scenario for diﬀerent ranges of con-\ntext, where the context size corresponds to the number\nof classes, associations, and attributes available (or al-\nready designed). On the one hand, it shows that the\nmajority of the test samples are distributed within a\ncontext size ∈ [1, 40]. On the other hand, it shows that\nour test set is relatively diverse in size and allows us\nto evaluate our approach under fairly broad range of\nsituations.\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 13\n0\n100\n200\n300\n400\n500\n600\n700\n[1, 10] [11, 20] [21, 30] [31, 40] [41, 50] [51, 60] [61, 70] [71, 80] [81, 250]\nclasses attributes associations\nFig. 14: [RQ3] – Distribution of each type of prediction in the test set according to the number of elements in the\navailable context (i.e., metamodel elements that have already been designed).\n0\n0,1\n0,2\n0,3\n0,4\n0,5\n0,6Recall@10\nclasses attributes associations\nFig. 15: [RQ3] – Evolution of the Recall@10 with re-\nspect to the size of the context\nIn Fig. 15, we report the evolution of the Recall@10\naccording to the size of the context for the three pre-\ndiction tests. Surprisingly, the eﬀectiveness on classes\nis substantially lower than that obtained in RQ1 and\nRQ2. This suggests that classes are in reality more dif-\nﬁcult to predict than associations and attributes. Also,\nproviding smaller context i.e., from [1 , 10] to [21 , 30])\nresults in a higher eﬀectiveness than for larger context\nsizes. This result is not necessarily consistent with those\nobtained in RQ1, but as the test sampling are totally\ndiﬀerent (which results in totally diﬀerent test sam-\nples), a thorough comparison of both modeling scenar-\nios is not feasible. However, given the results obtained\nin RQ1 (i.e., 45% Recall@5 for the prediction classes),\nwe can hypothetize that the context generated in this\nmodeling scenario is not appropriate or suﬃcient for\nthe prediction of classes.\nIn general, we can observe that increasing the con-\ntext size hinders the results. In the case of attributes\nand associations, the Recall@10 is higher for context\nsizes between [1, 50] and decreases as the context grows.\nHowever, for the context sizes between [51 , 250], the\nnumber of test samples decreases and may not be rep-\nresentative enough, which can partly explain the drop\nin the eﬀectiveness.\nAnswer to RQ3: When incrementally con-\nstructing metamodels, we conclude that for low\ncontext ranges, the system is relatively eﬃcient to\npredict associations and attributes, but is less eﬃ-\ncient to predict classes.\nHere again, these results must be taken in the con-\ntext of an exact match between the predicted and the\nexpected concepts. A more relaxed comparison taking\ninto account partial and semantically-equivalent con-\ncepts would result in better predictions as illustrated\nin Section 6.4.\n6.4 RQ4 – Use Cases\nIn this last experiment, we consider the Petri nets and\nJava metamodels from our test set. We ﬁrst evaluate\nthe metamodel renaming scenario with global context\nsampling which gave us the best performance on the\nwhole test set in RQ1. Then, we evaluate the incremen-\ntal construction scenario of RQ3 on both metamodels.\nIn Fig. 17 and 18, we report several top-5 recom-\nmendations provided by our model using the sampling\nof RQ1 for both test metamodels. The dashed boxes\nrepresent the mask used for the tests.At ﬁrst, we can\nobserve that the recommendations for the Petri nets\nmetamodel are very relevant and perfect in the three\n14 Martin Weyssow et al.\n0\n0,1\n0,2\n0,3\n0,4\n0,5\n0,6\n0,7\n0,8\n0,9\n1\n[1, 10] [11, 20] [21, 30] [31, 40]\nRecall@10\nclasses attributes associations\nFig. 16: [RQ4] – Evolution of the Recall@10 with re-\nspect to the size of the context for the Petri nets use\ncase in the modeling scenario of RQ3.\ngiven examples. Moreover, the selected test samples are\nrepresentative of the overall results obtained on this\nmetamodel. In fact, we obtain 100% Recall@5 for both\nclasses and attributes, and 82% Recall@5 for the asso-\nciations. Besides, the top-5 lists show that the model\nis able to recommend domain concepts that are related\nto Petri nets. This provides the modeler with relevant\nconcepts applicable in the current modeling activity.\nFor the Java metamodel (Fig. 18), the results show a\ndecrease in eﬀectiveness. We obtain 60%, 33% and 22%\nRecall@5 for the classes, attributes and associations,\nrespectively. The expected recommendation is not al-\nways in the top-5 list. Nevertheless, we note that all\nrecommendations are concepts close to the application\ndomain related to Java. For instance, in Test #1, the\nmodel is able to identify that the JavaProject class rep-\nresents a high-level concept that could be matched with\nthe concepts of Program or Module. Overall, the recom-\nmendations appear to be coherent with the application\ndomain and still provide useful domain concepts to the\nmodeler.\nConsidering the modeling scenario of RQ3, we re-\nport in Fig. 16 the evolution of the Recall@10 with\nrespect to the context size for the Petri nets meta-\nmodel. It is a medium-sized metamodel and the trends\nin the results corroborate with those obtained in RQ3,\nexcept that the Recall@10 is much higher for this spe-\nciﬁc metamodel. This can be explained by the fact that\nthe Petri nets metamodel was deﬁned using relatively\ngeneral concepts (e.g., NamedElement, Place, Transition,\nExecution) that are frequently used in the design of\nother metamodels present in our training set. However,\nthe results for the Java metamodel in this scenario are\nmuch lower. We envision that in a modeling activity\nsimilar to the one of RQ3, providing the model with\nadditional contextual information could help improv-\ning the results. In fact, as shown in Fig. 18, when we\nprovide the model with a global context, it is able to\nprovide relevant recommendations. Therefore, we be-\nlieve that in an incremental construction scenario, this\nlack of context could be bridged by taking into account\ndata source other than metamodels, e.g., requirement\nspeciﬁcations.\nAnswer to RQ4:Our model performs well in\na renaming scenario for two metamodels which use\nboth general and speciﬁc domain concepts. For an\nincremental construction process, the model pro-\nvides meaningful domain concepts in the ﬁrst use\ncase where the modeler manipulates general con-\ncepts, but lacks in eﬀectiveness in the second use\ncase where the concepts are more domain-speciﬁc.\n6.5 Threats to Validity\nWe identiﬁed some threats to the validity of our evalua-\ntion and attempted to address them during its design. A\nﬁrst external threat to the validity relates to the chosen\nrepresentation of the metamodels and how the trees are\norganized. We chose to transform the metamodels into\ntrees by following the “natural order” in which the el-\nements of the metamodels appear in the corresponding\nEcore serializations. We did not compare this approach\nwith other tree structures that could eventually result\nin a better eﬀectiveness of the system. Nevertheless, we\nenabled the approach to be general enough to use dif-\nferent structures of the trees with slight adaptations.\nThe design of our modeling scenarios results in the\nmain threat to construct validity. We attempted to ar-\nticulate our research questions to match realistically\nsimulated modeling scenarios. In the ﬁrst one (i.e., RQ1\n– RQ2), we believe that the two sampling strategies are\nreasonable since they are applicable in a real situation\nwhere a metamodel is already designed. In the second\none (RQ3), we mitigated the threat as best as we could\nby implementing a sampling algorithm to get as close as\npossible to a real-world situation. Since there is an inﬁ-\nnite number of possibilities to design a metamodel (due\nto human choices or modeling constraints), we believe\nthat the proposed simulated scenarios are reasonable to\nproperly evaluate our approach.\nNext, an internal threat to the validity that could\nprevent from strictly replicating of our experiments in-\nvolves the training of our pre-trained language model\nand the choice of the hyperparameters. The initializa-\ntion of neural networks parameters (i.e., weights) is ran-\ndom and two models trained with the same hyperpa-\nrameters could lead to slight variations in their accu-\nracy. Also, the choice of hyperparameters is crucial to\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 15\nLocatedElement \n- location: EString \nNamedElement \n- name: EString \nArc \n- weight: EInt \nElement \nT ransition \nPlaceT oT ransition T ransitionT oPlace \nPlace \nPetriNet \nelements0..* net1..1arcs0..*\noutgoingArc0..*\nfrom1..1\noutgoingArc0..*\nfrom1..1\nMarking to1..1 to1..1\nincomingArc\n1..* 0..*\nincomingArc\nT oken Mouvement \nExecution \nplacedAt1..1 tokens0..* marking1..1\nfire1..1 source1..1\ntarget1..1\nexec1..1\nmarkings0..*\nexec1..1mouvements0..*\nexec0..*\nnet1..1\nTest #1\nTest #2\nTest #3\nTest #1 Test #2 Test #3\nGround-Truth PetriNet weight outgoingArc\nTop-5\n1. PetriNet 1. weight 1. outgoingArc\n2. LocatedElement 2. marking 2. incomingArc\n3. Model 3. token 3. linksToPredecessor\n4. Petrinet 4. probability 4. generatedEvents\n5. Graph 5. kind 5. incomings\nFig. 17: [RQ4] – Excerpt of the renaming of a classiﬁer, an attribute and an association on the Petri nets use case\nmetamodel using the full context sampling approach (RQ1).\noptimize the model to improve its generalization to un-\nseen data. To mitigate this threat, we tuned the hyper-\nparameters of the RoBERTa architecture using ranges\nof values widely used in the literature. We reported the\noptimal model conﬁgurations in Appendix B.\nFinally, another important aspect that we consid-\nered is the representativeness of our data. We trained\nour model on 10 000 metamodels crawled from open-\nsource repositories with a rather broad range of sizes\nand various application domains. Regarding the eval-\nuation, we use a completely separate dataset from the\none used for training, with metamodels covering a va-\nriety of application domains and of various sizes.\n6.6 Discussion\nWe investigated the idea of learning domain concepts\nfrom metamodels covering a various range of applica-\ntion domains using a state-of-the-art pre-trained lan-\nguage model architecture. We regard this work as an\ninitial investigation that presents some limitations but\nalso that our experiments highlight interesting initial\nresults in the context of assisting a modeler in a model-\ning activity. We identiﬁed a number of lessons learned\nas well as opportunities to improve our approach in fu-\nture work.\nRegarding the results of our experiments, we showed\nthat our model is less eﬃcient on domain-speciﬁc meta-\nmodels (e.g., Java metamodel), which causes a drop in\nthe eﬀectiveness. These results could also be explained\nby the fact that some concepts do not appear often in\nthe training data, which may result in the inability of\nthe model to predict them. We envision that this lack\nof eﬃciency of our approach on these types of meta-\nmodels could be overcome in two ways. One option is\nto consider knowledge data (e.g., requirements, mod-\neling environment data, ontologies) in our framework,\nsimilarly to related work [1, 8]. Alternatively, we could\nrecommend generic terms and let the modeler decide\n16 Martin Weyssow et al.\nNamedElement \n- name: EString \nJavaElement ImportDeclaration PackageDeclaration JavaProject \nPackageFragment CompilationUnit \nField Method \n- returnType: EString \n- parametersName: EString \n- parametersType: EString\nT ype \n- elementName: EString \ndeclarations0..1 importDeclaration0..*\nownedType\n0..*\nownedSuperType\n0..*\n0..*\nnestedType\nownedCompilationUnit0..*\nownedPackageFragment\n0..*methods0..* fields0..*\nTest #1\nTest #2\nTest #3\nTest #4\nTest #5Test #6\nTest #1 Test #2 Test #3\nGround-Truth JavaProject ImportDeclaration elementName\nTop-5\n1. Model 1. ImportDeclaration 1. name\n2. DomainModel 2. NamespaceDeclaration 2. visibility\n3. Root 3. Elements 3. type\n4. Module 4. rules 4. returnType\n5. Program 5. ModelingConcept 5. types\nTest #4 Test #5 Test #6\nGround-Truth returnType methods nestedType\nTop-5\n1. name 1. methods 1. type\n2. methodName 2. method 2. returnType\n3. parameterType 3. Methods 3. types\n4. returnType 4. deﬁnitions 4. superType\n5. elementName 5. parameters 5. parameters\nFig. 18: [RQ4] – Excerpt of the renaming of two classiﬁers, two attributes and two associations on the Java use\ncase metamodel using the full context sampling approach (RQ1).\nof the best combination of terms to deﬁne her speciﬁc\nconcepts.\nThrough the modeling scenario of RQ3, we simu-\nlated a real-world incremental construction of a meta-\nmodel and highlighted mitigated results. We plan to\nfurther investigate this particular scenario in future work\nby conducting a case study on the usability of our ap-\nproach in a real modeling environment, with real users.\nAdditionally, we are convinced that considering the user\nin the loop is important in this process and could lead\nto signiﬁcant improvements of our approach. We foresee\nthat reinforcement learning or search-based techniques\nwith user interactions could be applicable in this sce-\nnario.\nTo evaluate the accuracy of our predictions, we used\nan exact match strategy. In the real world, modelers\ncan use diﬀerent names for the same concepts, espe-\ncially when these names are combinations of multiple\nterms. In the future, we plan to use semantic similarity\nmetrics based on, for example WordNet 6, to compare\nthe predicted and the expected concepts. We believe\nthat this strategy will allow a better evaluation of our\napproach.\nIn our proposed representation of metamodels as\ntrees, we purposely did not consider all the elements\nof the metamodels as we discussed that some of them\nmay not be relevant considering our modeling tasks.\n6 https://wordnet.princeton.edu/\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 17\nNevertheless, we argue that including more elements\nin our representations (e.g., generalizations, enumera-\ntions, cardinalities) could be valuable in particular in\nthe context of modeling scenarios similar to the ones\ntackled in this work. As discussed previously, we de-\nsigned our approach to be independent from the rep-\nresentation of the metamodels. Therefore, we plan to\nexplore alternative representations of the metamodels\nin future work and compare them on real modeling sce-\nnarios without necessarily having to alter our approach.\nIn this work, we used a RoBERTa pre-trained lan-\nguage model architecture to learn metamodels domain\nconcepts. In all of our experiments, we evaluated the\ntrained model without ﬁne-tuning procedure. In spite\nof that, the reported results are still promising and per-\nforming a ﬁne-tuning step beforehand the evaluation\ncould lead to an increasing in the eﬀectiveness of the\napproach. Additionally, the pre-trained language model\nthat we released can be ﬁne-tuned for a broad range of\nother tasks.\nOn the learning model used in this work, we worked\nwith a state-of-the-art bidirectional language model that\ndeals with sequential data as input. As discussed in Sec-\ntion 4, we had to ﬂatten the trees into sequences of to-\nkens to train the model. Nevertheless, we concede that\nmetamodels are not sequential and that one may think\nthat a language model may not be suitable given the\nproblem addressed in this paper. However, through our\nexperiments we have shown that such language model is\nstill able to learn meaningful domain concepts. Never-\ntheless, other model architectures, such as graph neural\nnetworks based on graph-structured representation of\nthe input, might be a relevant alternative to better learn\nfrom the structural aspect of the metamodels [21,36].\nWe proposed a fully data-driven approach to design\nan intelligent modeling assistant. As opposed to source\ncode, the amount of available data in MDE is signiﬁ-\ncantly lower. We believe that if we had more datasets\nof metamodels at our disposal, we would be able to\nsigniﬁcantly improve the eﬀectiveness of our approach.\nHowever, there is no indication that we will be able\nto gather more MDE data in a near future and in the\nsame order of magnitude as that available for source\ncode. Consequently, we conjecture that to tackle this\nlack of data, we need to design hybrid approaches as\nmentioned previously in this section.\n7 Related work\nIntelligent modeling assistants are similar to recommen-\ndation systems for software engineering [30,35] and aim\nat supporting a modeler through a modeling activity by\nproviding the modeler with relevant recommendations\nto the modeling context. We present a summary of the\nexisting related work in modeling assistants for MDE\nwith a focus on intelligent assistants to the complete of\nmodels and metamodels.\nThe early works on model completion focused on the\ndesign of systems based on rules and logic program-\nming [37, 38]. The authors focused on the completion\nof partial models by deriving constraint logic programs\nfrom models using model transformations. The deﬁned\napproaches are rather complex as they involve many\nmodel transformations and a Prolog engine. In com-\nparison, our approach does not require to deﬁne any\nconstraints and model transformations manually to pro-\nvide relevant recommendations as it is fully data-driven.\nRabbi et al. [32] designed an approach to rewrite partial\nmodels that ensures their conformance to their meta-\nmodels. They deﬁned the completion rules using model\ntransformations. In this work, we propose an approach\nthat leverages the structural and lexical properties of\nmetamodels in a large corpus by means of a language\nmodel that does not require any pre-deﬁned rule.\nKuschke et al. [23] deﬁned an approach that cap-\ntures editing operations during modeling activities to\nrecommend further modeling activities. Their approach\nis based on a catalog of common UML modeling activi-\nties to produce the recommendations. They extended\ntheir work by allowing more control from the mod-\neler [22]. In contrast, our system is independent from\nany modeling environment as it does not rely on data\nextracted from the latter. Moreover, we deﬁned a gen-\neral approach that is not restricted to the completion\nof speciﬁc cases that need to be deﬁned manually.\nMost recent works have focused on gathering knowl-\nedge data by extracting meaningful information from\ntextual or structured data (e.g., requirements, applica-\ntion domain, models/metamodels). Elkamel et al. [14]\ndesigned a system to recommend UML classes during a\nmodeling activity. They deﬁned a clustering-based ap-\nproach whose objective is to ﬁnd similar UML classes to\nthe current modeling context. Although the approach\nis interesting, it lacks evidence whether it can scale to\nlarge datasets. Alternatively, Stephan [40] proposed a\nsimilarity-based approach to guide the modeling pro-\ncess by considering elements from clone models to the\npartial model being designed as possible recommenda-\ntions. Yet, as we discussed in Section 6.6, we envision\nthat clustering-based could be an interesting direction\nto develop an alternative approach based on semantic\nsimilarity for the completion of metamodels.\nAgt-Rickauer et al. [1] proposed a domain model-\ning recommender system based on knowledge data of\ndomain-speciﬁc terms and their relationships. The sys-\ntem combines various sources of data containing on-\n18 Martin Weyssow et al.\ntologies, concepts, and terms that allow the completion\nof named elements in a model. In another work, Agt-\nRickauer et al. [2] used a frequency-based approach to\nrecommend concepts in domain models. Their approach\nrequires a large amount of background knowledge data\nin order to generalize to a certain range of domains. In\ncontrast, our approach does not rely on ontologies or vo-\ncabularies that are diﬃcult to obtain in an exhaustive\nway. Instead, we learn implicitly the relationships be-\ntween the domain concepts instantiated in thousands of\nmetamodels using a pre-trained language model which\nmakes our work independent from any application do-\nmain.\nMore recently, Di Rocco et al. [12] proposed a GNN-\nbased approach to recommend classes and class mem-\nbers in models. Similarly to our work, they found that\nclasses are diﬃcult to recommend due to the broad\nrange of concepts used by modelers. Burgueno et al. [8]\ndesigned a NLP-based approach to assist a modeler\nbased on textual knowledge and general knowledge data.\nThey also enabled their assistant to take into consider-\nation the user feedback by monitoring its interaction\nwith the assistant. However, the proposed approach re-\nquires to extract relevant sources of general knowledge\nto gap the lack of contextual knowledge; as it may not\nalways be accurate when a modeling activity requires\nknowledge about a speciﬁc application domain. In our\napproach, we do not require to gather contextual data\nabout the metamodel domain to provide relevant rec-\nommendations. Instead, we learn metamodel domain\nconcepts using a pre-trained language model architec-\nture. Nevertheless, we argue that our approach presents\nsome limitations and extending it to allow processing\ngeneral knowledge data or contextual data (e.g., re-\nquirements) would undoubtedly improve its eﬀective-\nness. Additionally, integrating the user feedback simi-\nlarly to Burguenoet al.’s work is an interesting opportu-\nnity for future work as discussed in Section 6.6. Finally,\nMussbacher et al. [29] designed a framework to evaluate\nintelligent modeling assistants on their ability to learn\nfrom data and their usefulness to an actual modeler.\nAs a future work, we plan on assessing our intelligent\nmodeling assistant using their assessment grid.\n8 Conclusion\nIn this paper, we presented a novel approach to learn\nmetamodel domain concepts using a RoBERTa pre-\ntrained language model architecture based on tree rep-\nresentations of the metamodels. With this model, we\nbuilt a recommender system that is able to assist a\nmodeler during a modeling activity by recommending\nmeaningful domain concepts relevant to the modeling\ncontext.\nAs our ﬁrst contribution, we presented an approach\nto transform a metamodel into a tree structure that\nallows to capture both structural and the lexical in-\nformation contained within the metamodel. Then, we\nhave introduced our overall framework to learn domain\nconcepts based on the aforementioned representations\nusing a state-of-the-art pre-trained language model ar-\nchitecture.\nWe evaluated our approach on three simulated mod-\neling scenarios and demonstrated the applicability of\nour approach in these scenarios. The experiments high-\nlight promising results and suggest that our pre-trained\nlanguage model can learn domain concepts from a dataset\nof independent metamodels without requiring additional\ndata about the application domains. The model and\ndata are publicly available (see Appendix A).\nAlthough we provide compelling evidence that it is\npossible to recommend meaningful concepts during the\nmodeling activities, there is still room for improvement\nto better assist modelers. We intend to develop more\ncomplex intelligent modeling assistant that would con-\nsider the human in the loop and her feedback during a\nmodeling activity. We will explore other model architec-\ntures such as graph neural networks and compare their\neﬀectiveness on real modeling tasks. Finally, we plan on\nimplementing a hybrid approach that considers knowl-\nedge data, such as requirements, to improve the eﬀec-\ntiveness of our approach on domain-speciﬁc modeling\nactivities.\nReferences\n1. Agt-Rickauer, H., Kutsche, R.D., Sack, H.: Automated\nrecommendation of related model elements for domain\nmodels. In: International Conference on Model-Driven\nEngineering and Software Development, pp. 134–158.\nSpringer (2018)\n2. Agt-Rickauer, H., Kutsche, R.D., Sack, H.: Domore-a rec-\nommender system for domain modeling. In: MODEL-\nSWARD, pp. 71–82 (2018)\n3. Atkinson, C., K¨ uhne, T.: A tour of language customiza-\ntion concepts. Advances in Computers 70, 105–161\n(2007)\n4. Baker, P., Loh, S., Weil, F.: Model-driven engineering\nin a large industrial context—motorola case study. In:\nInternational Conference on Model Driven Engineering\nLanguages and Systems, pp. 476–491. Springer (2005)\n5. Basciani, F., Di Rocco, J., Di Ruscio, D., Di Salle, A.,\nIovino, L., Pierantonio, A.: Mdeforge: an extensible web-\nbased modeling platform. In: 2nd International Work-\nshop on Model-Driven Engineering on and for the Cloud,\nCloudMDE 2014, Co-located with the 17th International\nConference on Model Driven Engineering Languages and\nSystems, MoDELS 2014, vol. 1242, pp. 66–75. CEUR-WS\n(2014)\nRecommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models 19\n6. Bengio, Y., Ducharme, R., Vincent, P., Janvin, C.: A neu-\nral probabilistic language model. J. Mach. Learn. Res.\n3(null), 1137–1155 (2003)\n7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\nG., Askell, A., et al.: Language models are few-shot learn-\ners. Advances in neural information processing systems\n33, 1877–1901 (2020)\n8. Burgue˜ no, L., Claris´ o, R., Li, S., G´ erard, S., Cabot, J.:\nA NLP-based architecture for the autocompletion of par-\ntial domain models (2020). URLhttps://hal.archives-\nouvertes.fr/hal-03010872. Working paper or preprint\n9. Burgue˜ no, L., Cabot, J., G´ erard, S.: An lstm-based\nneural network architecture for model transformations.\nIn: 2019 ACM/IEEE 22nd International Conference\non Model Driven Engineering Languages and Sys-\ntems (MODELS), pp. 294–299 (2019). DOI 10.1109/\nMODELS.2019.00013\n10. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805\n(2018)\n11. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding (2019)\n12. Di Rocco, J., Di Sipio, C., Di Ruscio, D., T. Nguyen, P.:\nA gnn-based recommender system to assist the speciﬁ-\ncation of metamodels and models. https://github.com/\nMDEGroup/MORGAN/blob/main/main.pdf\n13. Eclipse Foundation, Inc.: Eclipse Emfatic. https://www.\neclipse.org/emfatic/\n14. Elkamel, A., Gzara, M., Ben-Abdallah, H.: An uml\nclass recommender system for software design. In: 2016\nIEEE/ACS 13th International Conference of Computer\nSystems and Applications (AICCSA), pp. 1–8 (2016).\nDOI 10.1109/AICCSA.2016.7945659\n15. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong,\nM., Shou, L., Qin, B., Liu, T., Jiang, D., et al.: Code-\nbert: A pre-trained model for programming and natural\nlanguages. arXiv preprint arXiv:2002.08155 (2020)\n16. France, R., Bieman, J., Cheng, B.H.: Repository for\nmodel driven development (remodd). In: International\nConference on Model Driven Engineering Languages and\nSystems, pp. 311–317. Springer (2006)\n17. Hochreiter, S., Schmidhuber, J.: Long short-term mem-\nory. Neural computation 9, 1735–80 (1997). DOI\n10.1162/neco.1997.9.8.1735\n18. Kanade, A., Maniatis, P., Balakrishnan, G., Shi, K.: Pre-\ntrained contextual embedding of source code. arXiv\npreprint arXiv:2001.00059 (2019)\n19. Karampatsis, R.M., Babii, H., Robbes, R., Sutton, C.,\nJanes, A.: Big code != big vocabulary: Open-vocabulary\nmodels for source code. Proceedings of the ACM/IEEE\n42nd International Conference on Software Engineering\n(2020). DOI 10.1145/3377811.3380342. URL http://dx.\ndoi.org/10.1145/3377811.3380342\n20. Karampatsis, R.M., Sutton, C.: Scelmo: Source code\nembeddings from language models. arXiv preprint\narXiv:2004.13214 (2020)\n21. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation\nwith graph convolutional networks. arXiv preprint\narXiv:1609.02907 (2016)\n22. Kuschke, T., M¨ ader, P.: Pattern-based auto-completion\nof uml modeling activities. In: Proceedings of the 29th\nACM/IEEE international conference on Automated soft-\nware engineering, pp. 551–556 (2014)\n23. Kuschke, T., M¨ ader, P., Rempel, P.: Recommending\nauto-completions for software modeling activities. In:\nInternational Conference on Model Driven Engineering\nLanguages and Systems, pp. 170–186. Springer (2013)\n24. Lample, G., Conneau, A.: Cross-lingual language model\npretraining. CoRR abs/1901.07291 (2019). URL http:\n//arxiv.org/abs/1901.07291\n25. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen,\nD., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach\n(2019)\n26. L´ opez, J.A.H., Cuadrado, J.S.: Mar: A structure-based\nsearch engine for models. In: Proceedings of the 23rd\nACM/IEEE International Conference on Model Driven\nEngineering Languages and Systems, pp. 57–67 (2020)\n27. L´ opez-Fern´ andez, J.J., Guerra, E., De Lara, J.: Assessing\nthe quality of meta-models. In: MoDeVVa@ MoDELS,\npp. 3–12. Citeseer (2014)\n28. Mohagheghi, P., Gilani, W., Stefanescu, A., Fernandez,\nM.A.: An empirical study of the state of the practice\nand acceptance of model-driven engineering in four in-\ndustrial cases. Empirical Software Engineering 18(1),\n89–116 (2013)\n29. Mussbacher, G., Combemale, B., Abrah˜ ao, S., Bencomo,\nN., Burgue˜ no, L., Engels, G., Kienzle, J., K¨ uhn, T.,\nMosser, S., Sahraoui, H., et al.: Towards an assessment\ngrid for intelligent modeling assistance. In: Proceedings of\nthe 23rd ACM/IEEE International Conference on Model\nDriven Engineering Languages and Systems: Companion\nProceedings, pp. 1–10 (2020)\n30. Mussbacher, G., Combemale, B., Kienzle, J., Abrah˜ ao,\nS., Ali, H., Bencomo, N., B´ ur, M., Burgue˜ no, L., Engels,\nG., Jeanjean, P., et al.: Opportunities in intelligent mod-\neling assistance. Software and Systems Modeling 19(5),\n1045–1053 (2020)\n31. NaoMod Research Group: Atlanmod Modeling Tools.\nhttps://www.atlanmod.org/\n32. Rabbi, F., Lamo, Y., Yu, I., Kristensen, L.M.: A\ndiagrammatic approach to model completion. In:\nAMT@MoDELS (2015)\n33. Radford, A.: Improving language understanding by gen-\nerative pre-training. OpenAI blog (2018)\n34. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I.: Language models are unsupervised multi-\ntask learners. OpenAI blog 1(8), 9 (2019)\n35. Robillard, M., Walker, R., Zimmermann, T.: Recommen-\ndation systems for software engineering. IEEE software\n27(4), 80–86 (2009)\n36. Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M.,\nMonfardini, G.: The graph neural network model. IEEE\ntransactions on neural networks 20(1), 61–80 (2008)\n37. Sen, S., Baudry, B., Precup, D.: Partial model comple-\ntion in model driven engineering using constraint logic\nprogramming. In: 17th International Conference on Ap-\nplications of Declarative Programming and Knowledge\nManagement (INAP 2007) and 21st Workshop on (Con-\nstraint), p. 59 (2007)\n38. Sen, S., Baudry, B., Vangheluwe, H.: Domain-speciﬁc\nmodel editors with model completion. In: H. Giese (ed.)\nModels in Software Engineering, pp. 259–270. Springer\nBerlin Heidelberg, Berlin, Heidelberg (2008)\n39. Sennrich, R., Haddow, B., Birch, A.: Neural machine\ntranslation of rare words with subword units. In: Pro-\nceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npp. 1715–1725. Association for Computational Linguis-\ntics, Berlin, Germany (2016). DOI 10.18653/v1/P16-\n20 Martin Weyssow et al.\n1162. URL https://www.aclweb.org/anthology/P16-\n1162\n40. Stephan, M.: Towards a cognizant virtual software mod-\neling assistant using model clones. In: 2019 IEEE/ACM\n41st International Conference on Software Engineering:\nNew Ideas and Emerging Results (ICSE-NIER), pp. 21–\n24. IEEE (2019)\n41. Svyatkovskiy, A., Lee, S., Hadjitoﬁ, A., Riechert, M.,\nFranco, J.V., Allamanis, M.: Fast and memory-eﬃcient\nneural code completion. In: 2021 IEEE/ACM 18th In-\nternational Conference on Mining Software Repositories\n(MSR), pp. 329–340. IEEE (2021)\n42. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A.N., Kaiser,  L., Polosukhin, I.: At-\ntention is all you need. Advances in neural information\nprocessing systems 30 (2017)\n43. Weyssow, M., Sahraoui, H., Fr´ enay, B., Vanderose, B.:\nCombining code embedding with static analysis for\nfunction-call completion (2020). URL https://arxiv.\norg/abs/2008.03731\n44. Whittle, J., Hutchinson, J., Rounceﬁeld, M.: The state\nof practice in model-driven engineering. IEEE Software\n31(3), 79–85 (2014). DOI 10.1109/MS.2013.65\n45. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz,\nM., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jer-\nnite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame,\nM., Lhoest, Q., Rush, A.M.: Huggingface’s transformers:\nState-of-the-art natural language processing (2020)\n46. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhut-\ndinov, R., Le, Q.V.: Xlnet: Generalized autoregressive\npretraining for language understanding. arXiv preprint\narXiv:1906.08237 (2019)\n47. Yang, Z., Dai, Z., Yang, Y., Carbonell, J.G., Salakhut-\ndinov, R., Le, Q.V.: Xlnet: Generalized autoregres-\nsive pretraining for language understanding. CoRR\nabs/1906.08237 (2019). URL http://arxiv.org/abs/\n1906.08237\nA Replication Package\nWe make our code, datasets and models publicly available to\nease the replication of our experiments and to help researchers\nthat are interested in extending our work:\nhttps://github.com/martin-wey/metamodel-concepts-bert\nThe data and models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.5579980\nB Model Hyperparameters\nTable 3: Hyperparameters (HP) used for the training\nof our model.\nHP Value\nNumber of layers 12\nHidden size 768\nFFN inner hidden size 3072\nAttention heads 12\nDropout 0.1\nAttention dropout 0.1\nHidden activation gelu\nPositional embedding absolute\nBatch size 32\nMax epochs 100\nVocabulary byte-level BPE\nVoocabulary size 30.000\nVocabulary cut-oﬀ 2",
  "topic": "Metamodeling",
  "concepts": [
    {
      "name": "Metamodeling",
      "score": 0.953597903251648
    },
    {
      "name": "Computer science",
      "score": 0.8499841690063477
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6716445088386536
    },
    {
      "name": "Software engineering",
      "score": 0.5768681168556213
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5535342693328857
    },
    {
      "name": "Relation (database)",
      "score": 0.5133172869682312
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.49288296699523926
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4868854582309723
    },
    {
      "name": "Model-driven architecture",
      "score": 0.4777073860168457
    },
    {
      "name": "Modeling language",
      "score": 0.4728204011917114
    },
    {
      "name": "Domain model",
      "score": 0.42337560653686523
    },
    {
      "name": "Machine learning",
      "score": 0.34697550535202026
    },
    {
      "name": "Programming language",
      "score": 0.33327537775039673
    },
    {
      "name": "Natural language processing",
      "score": 0.33142316341400146
    },
    {
      "name": "Domain knowledge",
      "score": 0.313140332698822
    },
    {
      "name": "Unified Modeling Language",
      "score": 0.3102054297924042
    },
    {
      "name": "Data mining",
      "score": 0.24217239022254944
    },
    {
      "name": "Software",
      "score": 0.089729905128479
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    }
  ]
}