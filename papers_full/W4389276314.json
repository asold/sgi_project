{
  "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
  "url": "https://openalex.org/W4389276314",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4273657883",
      "name": "Hazem Abdelazim",
      "affiliations": [
        "October University of Modern Sciences and Arts"
      ]
    },
    {
      "id": "https://openalex.org/A2212839637",
      "name": "Mohamed Tharwat",
      "affiliations": [
        "October University of Modern Sciences and Arts"
      ]
    },
    {
      "id": "https://openalex.org/A2515400989",
      "name": "Ammar Mohamed",
      "affiliations": [
        "October University of Modern Sciences and Arts"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6763361438",
    "https://openalex.org/W3146163893",
    "https://openalex.org/W4288079856",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W4327716446",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4386394330",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W4386185606",
    "https://openalex.org/W4310923309",
    "https://openalex.org/W4385570706",
    "https://openalex.org/W4312642432",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2295781714",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4383426701",
    "https://openalex.org/W2970960342",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "In recent times, Retrieval Augmented Generation (RAG) models have garnered considerable attention, primarily due to the impressive capabilities exhibited by Large Language Models (LLMs). Nevertheless, the Arabic language, despite its significance and widespread use, has received relatively less research emphasis in this field. A critical element within RAG systems is the Information Retrieval component, and at its core lies the vector embedding process commonly referred to as “semantic embedding”. This study encompasses an array of multilingual semantic embedding models, intending to enhance the model’s ability to comprehend and generate Arabic text effec-tively. We conducted an extensive evaluation of the performance of ten cutting-edge Multilingual Semantic embedding models, employing a publicly available ARCD dataset as a benchmark and assessing their performance using the average Recall@k metric. The results showed that the Microsoft E5 sentence embedding model outperformed all other models on the ARCD dataset, with Recall@10 exceeding 90%.",
  "full_text": "(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 11, 2023\nSemantic Embeddings for Arabic Retrieval\nAugmented Generation (ARAG)\nHazem Abdelazim\nSchool of Computing and\nDigital Technology\nESLSCA University\nCairo, EGYPT\nMohamed Tharwat\nSchool of Computing and\nDigital Technology\nESLSCA University\nCairo, EGYPT\nAmmar Mohamed\nSchool of Computing and\nDigital Technology\nESLSCA University\nCairo, EGYPT\nAbstract—In recent times, Retrieval Augmented Generation\n(RAG) models have garnered considerable attention, primarily\ndue to the impressive capabilities exhibited by Large Language\nModels (LLMs). Nevertheless, the Arabic language, despite its\nsignificance and widespread use, has received relatively less\nresearch emphasis in this field. A critical element within RAG\nsystems is the Information Retrieval component, and at its\ncore lies the vector embedding process commonly referred to\nas “semantic embedding”. This study encompasses an array of\nmultilingual semantic embedding models, intending to enhance\nthe model’s ability to comprehend and generate Arabic text effec-\ntively. We conducted an extensive evaluation of the performance\nof ten cutting-edge Multilingual Semantic embedding models,\nemploying a publicly available ARCD dataset as a benchmark and\nassessing their performance using the average Recall@k metric.\nThe results showed that the Microsoft E5 sentence embedding\nmodel outperformed all other models on the ARCD dataset, with\nRecall@10 exceeding 90%\nKeywords—Arabic NLP; large language models; retrieval aug-\nmented generation; semantic embedding\nI. I NTRODUCTION\nRetrieval Augmented Generation (RAG), introduced by\nFacebook Researchers in 2020 [1], is a pivotal AI framework\nfacilitating information retrieval for Generative AI models,\nthereby enhancing their accuracy and capabilities. RAG em-\npowers Large Language Models (LLMs) by granting them\naccess to external knowledge sources, augmenting the content\ngeneration process. This dual functionality entails retrieval,\nwherein RAG meticulously selects pertinent information from\nprovided sources and generation, whereby LLMs craft contex-\ntually relevant responses based on user input.\nThe advantages of RAG are multi-fold. Firstly, it bolsters\nthe performance by grounding LLMs with factual, up-to-\ndate information from external knowledge repositories. Fur-\nthermore, RAG maintains contextual relevance in responses,\ncontributing to a more engaging user experience in conversa-\ntional AI applications. Its scalability is noteworthy, as RAG\nmodels seamlessly handle copious volumes of information,\nproving invaluable for data-intensive tasks. Additionally, the\nadaptability of RAG models allows fine-tuning for specific\napplications [2], rendering them versatile across diverse data\nand use cases. Customizability is another hallmark, permitting\nRAG models to specialize in particular domains or subjects\nthrough customization and fine-tuning on specific knowledge\nbases. Due to the importance of such a framework for enter-\nprises, extensive research is currently being pursued to discover\nnew algorithms and techniques to enhance the performance\nof such models bounded by the context-window limitations\nof LLMs. Although there is ongoing research to expand the\nwindow size for LLM to be able to ingest more data in\nthe prompt, the use of techniques like RAG is still of great\npractical importance, not only on homogeneous unstructured\ndata but also on heterogeneous data [3].\nIn principle, at the heart of the information retrieval module\nis the semantic embedding module which converts a piece of\ntext, whether a query or a context text chunk to a numeric\nfeature vector that embodies all semantic features of the text.\nThe development of word and sentence embeddings is a\nrelatively recent area of research in natural language processing\n(NLP) and information retrieval.\nMost of the semantic models are English language-centred;\nhowever, in recent years, Multilingual embedding models were\nreleased [4]. There are lots of benchmarks to test the perfor-\nmance of multilingual embeddings [5], which are aggregate but\nvery few focus on language-specific performance, and on the\nArabic language in particular. This is the main impetus behind\nthe current research work, which focuses on ten different state-\nof-the-art embedding models that are capable of embedding\nArabic language.All the models are tested using publicly\navailable ARCD (Arabic Reading Comprehension Dataset) [6]\nand the metric used is average Recall@k for different values\nof k. A comparative performance is conducted taking into\nconsideration the embedding size for each model.\nThe rest of the paper is organized as follows: Section II,\nthe Retrieval Augmented generation pipeline is presented, as\nwell as the positioning of the semantic embedding and the\ninformation retrieval component within the pipeline. Section\nIII explores related research work in this field, focusing on\nrecent developments. Section IV overviews the 10 semantic\nembedding models that are used in the experiments will\nbe covered. Section V discusses the 10 embedding models\non a standard dataset that are used in for Arabic Reading\ncomprehension (ARCD) and their evaluation using Recall@k\nperformance metric. Also, the impact of the embedding di-\nmension size is analyzed in the comparative results. Section\nVI concludes the paper.\nwww.ijacsa.thesai.org 1328 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 11, 2023\nFig. 1. Retrieval augmented generation.\nII. RAG: R ETRIEVAL AUGMENTED GENERATION\nThe Retrieval Augmented generation pipeline, shown in\nFig. 1 is as follows:\nA. Phase I: Information retrieval\n1) Given a corpus of unstructured text containing docu-\nments\n2) Given a user text query\n3) A semantic embedding model is identified\n4) The query is embedded into a feature vector of n\ndimension (semantic embedding)\n5) The corpus is segmented into m text chunks (either\ndisjoint or overlapping)\n6) Each text chunk is embedded into a feature vector of\nn dimension using the same embedding model used\nin embedding the search query, as shown in Fig. 2.\n7) The m- m-vectors are indexed in a Vector DB store\n8) Cosine similarity, euclidean or inner product score is\ncomputed between the embedded vector of the query\n9) 9. Top k relevant chunks are retrieved, which com-\nprise a context for the next phase\nB. Phase 2: LLM Comprehension and Response\nIn this phase, a suitable LLM is identified and selected,\nwhether an open source model (more than 100 LLM models\nare currently available ) , like LLaMA (7b/13b/70b), Falcon,\nGPT neoX, Bloom, vicuna , FlanT5 , etc.) . However, not all of\nthem support the Arabic Language. The Current Arabic LLM\nmodels are:\n• OpenAI GPT-turbo-3.5\n• Open AI GPT 4.0\n• Google Bard\n• Microsoft Bing Chat (on top of openAIGPT3.5)\n• Google PaLM2 (vertex-ai)\n• Jais (UAE Arabic Language Model)\nFig. 2. Semantic embedding.\nHowever, not all of them provide APIs for programmatic tasks,\nwhich are provided at cost like (OpenAI GPT3.5-turbo/GPT4.0\nGoogle PalM - vertex-ai). After an Arabic LLM is identified.\nA prompt is constructed with two variable components: the\nretrieved Top k text chunks as a context and the input research\nquery. The prompt instructs the Arabic LLM to find an answer\nto the search query from within the retrieved context. This\narchitecture is widely used in Enterprises for domain-specific\ndeployment of generative AI LLMs. A fundamental component\nof the overall process is the information retrieval component,\nas depicted in Fig. 1. The overall efficiency of the system is\nhighly dependent on the performance of the IR component.\nIf the IR component fails to retrieve relevant portions from\nthe corpus, the LLM will not find the proper answer or,\neven worse, may hallucinate with wrong answers confidently\ndepending on the LLM model settings (like temperature) as\nwell as the prompt engineering. Ensuring an efficient and\naccurate semantic embedding is of paramount importance for\nan effective and practical RAG system.\nIII. R ELATED WORK\nIn the context of Arabic language processing, a lot of\nresearch was done in Natural Language Understanding, taking\ninto consideration the different dialectical nature of Arabic\nwww.ijacsa.thesai.org 1329 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 11, 2023\nlanguage [7], in addition to research work on Arabic text clas-\nsification [8] as well as Arabic text similarity using statistical\ntechniques [9]. However, relatively fewer research studies were\nconducted on semantic embeddings for Arabic text. FastText\nfor Arabic Word Embeddings [11] proved to be an effective\nmethod for generating Arabic word embeddings. These embed-\ndings capture subword information, making them valuable for\nmorphologically rich languages like Arabic. Researchers have\nalso explored Word2Vec-based approaches for Arabic word\nembeddings [12].\nMultilingual FastText: Multilingual embeddings have\ngained attention for their ability to handle multiple languages\nsimultaneously through incorporating language-specific infor-\nmation while sharing a common subword vocabulary across\nlanguages [4]. Researchers have also explored cross-lingual\nembeddings that facilitate knowledge transfer between lan-\nguages. A new method is proposed that aligns word em-\nbeddings across languages, enabling multilingual applications\n[13].\nSentence-BERT: Sentence embeddings, which capture the\nsemantic meaning of entire sentences, have gained popular-\nity [10] and demonstrated superior performance in various\nsentence-level tasks in monolingual settings. Multilingual sen-\ntence embeddings [14] have been explored for cross-lingual\napplications based on training sentence embeddings for mul-\ntiple languages using a shared model. The research work in\n[9] provided a foundational framework for evaluating critical\nsemantic embedding APIs that play a pivotal role in search\nand broader information access initiatives. The author in [9]\naddresses the challenge of the limited accessibility of in-\ncreasingly large language models by examining the utilization\nof semantic embedding APIs for information retrieval. Their\ninvestigation focused on assessing the capabilities of these\nAPIs in domain generalization and multilingual retrieval using\nbenchmark datasets like BEIR and MIRACL. The study reveals\nthat re-ranking BM25 results using these APIs proves to be\ncost-effective and most effective in English contexts, offering\nan alternative to the conventional practice of using them as\ninitial retrievers. For non-English retrieval, the authors suggest\na hybrid model with BM25 as the most effective approach,\nalbeit at a higher cost.\nFor using embeddings in downstream tasks, the authors\nin [17] focused on Arabic sentiment analysis, particularly\non social media platforms like Twitter and Facebook, which\nhave become vital for understanding user opinions and prefer-\nences. Sentiment analysis, however, faces challenges in natural\nlanguage processing (NLP). Recent advancements in deep\nlearning have demonstrated superior performance in NLP-\nrelated tasks compared to traditional statistical and lexical-\nbased approaches. A comparative analysis of classic and\ncontextualized word embeddings for sentiment analysis was\nconducted utilizing both trained and pre-trained versions of the\nfour most commonly used word embedding techniques: GloVe,\nWord2Vec, FastText, and ARBERT. Deep learning architec-\ntures, namely, BiLSTM and CNN, are employed for sentiment\nclassification, and experiments are conducted on benchmark\ndatasets, including HARD, Khooli, AJGT, ArSAS, and ASTD.\nThe results reveal that, in general, embeddings generated by\none technique outperform their pre-trained counterparts, with\ncontextualized transformer-based embedding BERT achieving\nthe highest performance, highlighting the significance of word\nembeddings in Arabic sentiment analysis.\nAn Arabic reading comprehension dataset (ARCD) [6]\naddressed the challenge of open-domain Arabic question and\nanswering (QA) with Wikipedia as the knowledge source.\nMainly the scarcity of labelled QA datasets and the need for\nefficient Arabic machine reading comprehension and retrieval.\nTo overcome the lack of Arabic QA datasets, they introduced\nthe Arabic Reading Comprehension Dataset (ARCD), gener-\nated by crowd-workers from Wikipedia articles and a ma-\nchine translation of the Stanford Question Answering Dataset\n(Arabic-SQuAD). Their open-domain QA system, SOQAL,\nincluded two components; the first is a hierarchical TF-IDF\ncomponent and a neural reading comprehension component\nbased on the pre-trained BERT transformer. Experiments on\nARCD demonstrate the effectiveness of their approach, with\nthe BERT-based reader achieving a 61.3 F1 score and SOQAL\nachieving a 27.6 F1 score in open-domain Arabic question\nanswering.\nIn the next session, we will dive more into the semantic\nembedding models used in the current research.\nIV. S EMANTIC EMBEDDING MODELS\nSentence and paragraph embeddings are crucial tools in in-\nformation retrieval (IR), enabling systems to comprehend and\nretrieve text based on semantic meaning. These embeddings\nencode the meaning of sentences and paragraphs into fixed-\nsize vectors [16], [18], allowing for semantic search, document\nretrieval, question answering, duplicate detection, clustering,\nsummarization, recommender systems, cross-lingual search,\nand contextual understanding. By representing queries and\ndocuments in a continuous vector space, these embeddings\nenhance the accuracy of IR tasks by retrieving relevant con-\ntent, even when keyword matching falls short in capturing\nthe nuances of user intent or dealing with extensive and\nunstructured text collections. In the Question and Answering\n(QA) setting under study: Given a complete dataset of records\n(context-paragraphs (cps), question, ground truth answer), the\nIR problem is to retrieve the most relevant cps to this query.\nIn the current work, since our focus is on the Arabic language,\nwe explored ten embedding models that have multilingual\nembedding features. The query is embedded, resulting in a\nfixed-size feature vector, and each of the context paragraphs\n(cps) is also embedded. The result of embedding is a feature\nvector that embodies the semantic features of the text (question\nor context paragraph). A cosine similarity distance metric is\ncalculated between the query, and all the semantic features of\nall the cps is given by\ncosine similarity (A, B) =\nPn\ni=1 Ai · BipPn\ni=1 A2\ni ·\npPn\ni=1 B2\ni\n(1)\nThe cosine similarity metric is a value between [0,1]; the\nhigher score implies a higher similarity. The essence here\nis that “most probably the answer of the query lies in the\n‘context-paragraph’ cp with the highest similarity with the\ninput query”. This assumption, which is mostly adopted in\ncurrent QA systems, works well in the majority of situations\nwww.ijacsa.thesai.org 1330 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 11, 2023\nwith much higher performance than keyword search and re-\ntrieval. The following multilingual embedding models were\ninvestigated in this research work.\nA. Mpnet: Paraphrase-Multilingual-mpnet-base-v2\nMpnet [10] is based on SBERT (Sentence-BERT), which\nis a modification of the pre-trained BERT network that uses\nsiamese and triplet network structures to derive semantically\nmeaningful sentence embeddings, allowing for efficient com-\nparisons using cosine similarity. The original work BERT [18]\nand RoBERTa [19] have achieved state-of-the-art performance\non sentence-pair regression tasks such as semantic textual\nsimilarity (STS). However, they require both sentences to\nbe fed into the network, leading to significant computational\noverhead. Siamese networks are a type of neural network\narchitecture that can learn to compare two inputs and measure\ntheir similarity or dissimilarity. BERT is a pre-trained language\nmodel that can encode sentences into fixed-length vectors,\nbut it requires both sentences to be fed into the network si-\nmultaneously, which is inefficient for large-scale applications.\nSentence-BERT (SBERT) is a modification of BERT that uses\nSiamese networks to derive sentence embeddings that can\nbe compared using cosine similarity. This way, SBERT can\ncompute the similarity of two sentences without processing\nthem together, reducing the computational cost and enabling\nsemantic similarity search and clustering. SBERT can produce\nmore accurate and consistent embeddings than BERT, as it\nfine-tunes the model on specific similarity tasks.\nB. Google LaBSE\nWhile BERT has proven to be a powerful approach for\nacquiring monolingual sentence embeddings that excel in\ntasks related to semantic similarity and embedding-based\ntransfer learning, the realm of BERT-based cross-lingual sen-\ntence embeddings was relatively uncharted. A comprehensive\nLanguage-agnostic BERT Sentence Embedding (LaBSE) [20],\ndeveloped by Google researchers with training across 112\nlanguages, including Arabic, using the Tatoeba dataset [13].\nThis is the multilingual SBERT model used in this research\nwork. The embedding dimension is 768.\nC. Openai Ada-embedding\nOpenai research team [21] delved into the significance of\ntext embeddings, essential for tasks such as semantic search\nand text similarity assessment, transcending traditional applica-\ntions. Unlike previous methods that tailored models for specific\nuse cases, they introduced a more unified approach, empha-\nsizing extensive contrastive pre-training on unsupervised data.\nThis strategy produced top-quality vector representations with\na wider context window for both text and code, a breakthrough\nvalidated across various benchmarks, including MSMARCO\n[15], Natural Questions, TriviaQA, and code search. Their\nfindings underscored the versatility of these unsupervised text\nembeddings, demonstrating their potential to excel in state-of-\nthe-art performance across diverse domains, from linear-probe\nclassification to large-scale semantic search.\nD. Cohere Multilingual Embedding\nCohere’s multilingual text understanding model [16] works\nby mapping text to a semantic vector space, where texts with\nsimilar meanings are positioned close to each other. This al-\nlows for a variety of valuable use cases in multilingual settings,\nsuch as search, content aggregation and recommendation, and\nzero-shot cross-lingual text classification. To train the model,\nCohere collected a dataset of nearly 1.4 billion question/answer\npairs across tens of thousands of websites in hundreds of\nlanguages. This dataset is unique because it contains questions\nactually asked by speakers of said languages, allowing the\nmodel to capture language- and country-specific nuances.\nE. Meta SONAR:Language-Agnostic Representations\nMeta introduced SONAR, a novel fixed-size sentence\nembedding space with support for multiple languages and\nmodalities [22]. SONAR’s single text encoder, spanning 200\nlanguages. Meta stipulated that SONAR outperforms existing\nsentence embeddings like LASER3 and LabSE in multilingual\nsimilarity search tasks. It extends its capabilities to speech\nsegments by employing language-specific speech encoders\ntrained in a teacher-student framework, surpassing existing\nspeech encoders in similarity search tasks. SONAR also pro-\nvides a text decoder for 200 languages, facilitating text-to-\ntext and speech-to-text machine translation, including zero-\nshot language and modality combinations [22]. In our findings,\nwe found that this is an overstatement when applied to Arabic\nLanguage, as described in section 4.\nF . Microsoft E5 - (Small-base-large)\nMicrosoft researchers [23] presented E5, a family of ad-\nvanced text embeddings designed for versatile applications\nacross various tasks. E5 stands for EmbEddings from bidirec-\ntional Encoder representations. These embeddings are trained\nusing a contrastive approach applied to a large, curated text pair\ndataset called CCPairs. E5 text embedding models are suitable\nfor tasks like retrieval, clustering, and classification, where\na single-vector representation of text is required. It exhibits\nrobust performance in both zero-shot and fine-tuned settings.\nThe authors extensively evaluated 56 datasets using BEIR and\nMTEB [5] benchmarks. In zero-shot scenarios, E5 surpasses\nthe strong BM25 baseline in the BEIR retrieval benchmark,\nand when fine-tuned, it achieved the best results in the MTEB\nbenchmark at the time of the publication (Dec. 7th. 2022),\noutperforming existing embedding models with significantly\nfewer parameters.\nG. HuggingFace DistillBert v1,v2\nHuggingFace researchers [24], proposed DistilBERT, a\nsmaller and more efficient language representation model\nderived from BERT. As transfer learning from large-scale pre-\ntrained models gains prominence in Natural Language Process-\ning (NLP), the challenge lies in deploying these large models\non resource-constrained devices or under tight computational\nbudgets. DistilBERT is pre-trained using knowledge distillation\ntechniques, reducing the model size by 40% while retaining\n97% of its language understanding capabilities and achieving\na 60% increase in speed. To leverage the inductive biases\nfrom larger models, they introduced a triple loss mechanism\nwww.ijacsa.thesai.org 1331 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 11, 2023\nthat combines language modelling, distillation, and cosine-\ndistance losses. DistilBERT proves to be cost-effective for pre-\ntraining and demonstrates its suitability for on-device compu-\ntations through proof-of-concept experiments and comparative\non-device studies. In our analysis, we explored distils-base-\nmultilingual-cased-v1 and v2 , denoted in the experiments as\nhf1 and hf2\nV. E XPERIMENTAL RESULTS\nA. ARCD Dataset\nThe dataset used in the benchmark analysis is ARCD\n(Arabic Reading Comprehension Dataset) [6]. Crowdsourced\n1,395 questions with the corresponding context paragraph and\nground truth answers. The research involved curation and\ncrowdsourcing, focusing on 155 randomly selected articles\nfrom the top 1000 most viewed articles on Arabic Wikipedia in\n2018. These articles spanned a wide range of topics, including\nreligious figures, historical figures, sports celebrities, countries,\nand companies. To ensure the appropriateness of the content, a\nmanual filter was applied to remove any adult material. In total,\nthe project collected 1,395 questions based on 465 paragraphs\nextracted from the 155 selected articles. Fig. 3 shows a typical\nrecord from the ARCD dataset. Following the pipeline in Fig.\nFig. 3. ARCD record example.\n1, the knowledge base (corpus) is constructed based on the\nconcatenation of all Context paragraphs (CPs) of all 1395\nquestions. Each question and each context paragraph CP is\nembedded using one of the ten models under study. Faiss\n(Facebook AI Similarity Search ) python library is used for\nVector DB indexing and search.\nB. Recall@k Performance Metric\nAverage Recall@k metric is used, where k is the top model\nretrieval hits with values [1,2,3,4,5,10,15]. The performance\nresults are shown in Fig. 4 and Table I.\nThe results showed that the Microsoft E5 Family of models\nhad a superior overall performance for all k values, where\nthe top performer was E5 - ML - Large. the second was ada\nopenAI embedding, and worth noting here that E5 is a free,\nopen-source model, while Openai ada is at a cost (0.0001$/1k\ntokens)\nTABLE I. R ECALL @K FOR VARIOUS MODELS\nsonar e5s e5b e5l hf1 hf2 cohere mpnet ada LaBSE\nk = 1 11% 21% 21% 23% 15% 14% 15% 14% 18% 15%\nk = 2 23% 43% 42% 46% 27% 26% 28% 27% 36% 29%\nk = 3 35% 63% 62% 68% 40% 38% 40% 38% 53% 44%\nk = 4 40% 71% 69% 75% 46% 44% 44% 44% 59% 50%\nk = 5 44% 76% 74% 79% 52% 49% 48% 49% 64% 55%\nk = 10 56% 87% 87% 91% 66% 63% 60% 61% 77% 70%\nk = 15 63% 90% 90% 93% 71% 67% 64% 65% 79% 78%\nEmbedding 1024 384 768 1024 512 512 768 768 1536 768\nFig. 4. Average % Recall@k performance.\nC. Embedding Dimensions and Model Score\nFig 5 shows the embedding dimension of each model and\nthe top model had 1024, while Openai ada had the maximum\nembedding dimension of 1536. What’s interesting is that the\nsecond top performer, E5-small, has an embedding dimension\nof 384, which is quite impressive. Naturally, the higher the\nembedding dimension, the higher the capacity to capture better\nsemantic context, which impacts storage and Latency. A simple\nformula is used to capture the trade-off between model retrieval\naccuracy and embedding dimension in an overall score:\nmodel score = Avg. Recall@k ∗ 1000\nEmbedding Dimension (2)\nFig. 5. Embedding dimensions.\nwww.ijacsa.thesai.org 1332 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 11, 2023\nFig. 6. Overall model score.\nThe overall model score in Fig. 6 shows a clear superior\nperformance for the E5-ml-small model with 384 embedding\ndimensions, and hence highly recommended for Arabic Lan-\nguage Semantic Information retrieval\nVI. C ONCLUSION\nIn this study, we explored the application of Retrieval\nAugmented Generation (RAG) models in the realm of the\nArabic language, an area that while linguistically rich, often\nreceives less attention in this field. Our focus was particularly\non the Information Retrieval component, with a keen eye on\nthe processes of semantic embedding. For our evaluation, we\nutilized a range of advanced Multilingual Semantic embedding\nmodels, employing the ARCD dataset as a benchmark for\nour assessments. The knowledge corpus is generated from the\nconcatenations of ARCD question contexts. Questions and\ncontexts are embedded using the 10 models understudy, and\nRecall@k metric is used in the evaluation, where k represents\nthe top retrieval hits based on the cosine similarity distance,\nand facebook AI similarity search (faiss) library.\nThe results indicated that the Microsoft E5 Family of mod-\nels, especially E5-ML-Large (e5l), consistently outperformed\nthe other models across different retrieval hit levels (k values).\nNotably, the open-source nature of E5 models makes them\nparticularly appealing for a wide range of applications. The\nsecond-best performer was the Ada OpenAI embedding, albeit\nfor 0.0001$/1k tokens. Furthermore, we observed that embed-\nding dimensions play a crucial role in model performance.\nHigher embedding dimensions, such as the 1536 of OpenAI\nAda, offer improved semantic context capture but come with\nstorage and latency implications. To account for this tradeoff,\nwe introduced an overall model score that combines model re-\ntrieval accuracy and embedding dimension. The E5-ML-Small\nmodel (e5s) , with an embedding dimension of 384, emerged\nas the top performer in this balanced evaluation. In light of\nthese findings, we highly recommend the adoption of the E5-\nML-Small model for Arabic Language Semantic Information\nRetrieval, as it strikes an excellent balance between retrieval\naccuracy and resource efficiency.\nThe superior performance of the e5 family of models is\nattributed to their unique approach to data preparation and\ntraining. Unlike conventional methods that rely on small-\nscale, human-annotated data or large-scale, noisy datasets, the\ne5 models utilize a specially curated dataset called CCPairs\n(Colossal Clean text Pairs), which is derived from diverse\nsemi-structured sources.\nThis research contributes to the broader exploration of\nRAG models for Arabic language processing and information\nretrieval, shedding light on valuable avenues for future appli-\ncation of Arabic Language Understanding and Generation.\nVII. D ISCUSSIONS AND FUTURE WORK\nA critical aspect warranting further investigation in Re-\ntrieval Augmented Generation (RAG) systems and semantic\nembeddings pertains to the dimensionality of the context win-\ndow, or embedding size. While reduced embedding dimensions\nare advantageous for computational efficiency and data storage,\nthey pose challenges in terms of model performance, particu-\nlarly when processing extensive contexts. Such contexts often\nexceed the embedding dimension limits, leading to truncation\nwhich may adversely impact the model’s effectiveness.\nMoreover, the choice of tokenizer algorithm, inherently\nlinked to the language being analyzed, presents another vari-\nable influencing RAG systems’ performance. Tokenizer algo-\nrithms vary significantly, and their compatibility and efficiency\ncan differ across languages. This variability underscores the\nnecessity for extensive research into the implications of dif-\nferent tokenizer algorithms, especially in the context of spe-\ncific languages. Such an investigation could provide valuable\ninsights into optimizing RAG systems for diverse linguistic\nenvironments.\nFuture research should also encompass the exploration of\ncontemporary methodologies in the realm of RAG systems,\nnotably re-ranking strategies and cross-encoder architectures,\nfrom a language-specific perspective. This exploration is es-\nsential given the evolving nature of large language models\nand their application across various downstream tasks. In\nconducting such studies, it will be critical to employ nuanced,\nlanguage-sensitive metrics, such as Mean Average Precision\n(MAP), Mean Reciprocal Rank (MRR), and normalized Dis-\ncounted Cumulative Gain at k (ndcg@k). These metrics would\noffer a more refined evaluation of the models’ capabilities in\nhandling language-specific nuances and complexities.\nThrough these focused areas, we are aim to address the\ninterplay between linguistic characteristics and the technical\ndimensions of RAG systems, thereby enhancing their applica-\nbility and efficiency in diverse linguistic contexts.\nREFERENCES\n[1] Lewis, Patrick & Perez, Ethan & Piktus, Aleksandara & Petroni, Fabio\n& Karpukhin, et al. “Retrieval-Augmented Generation for Knowledge-\nIntensive NLP Tasks”. Advances in neural information processing in\nsystems (2020)\n[2] Krishna CS. Prompt Generate Train (PGT): A framework for few-shot\ndomain adaptation, alignment, and uncertainty calibration of a retriever\naugmented generation (RAG) model for domain specific open book\nquestion-answering. arXiv preprint arXiv:2307.05915. 2023 Jul 12.\n[3] Yu, W.. “Retrieval-augmented Generation across Heterogeneous Knowl-\nedge. North American Chapter of the Association for Computational\nLinguistics (2022).\nwww.ijacsa.thesai.org 1333 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 11, 2023\n[4] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and\nTomas Mikolov. 2018. Learning Word Vectors for 157 Languages.\nIn Proceedings of the Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\n[5] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.\n2023. MTEB: Massive Text Embedding Benchmark. In Proceedings\nof the 17th Conference of the European Chapter of the Association\nfor Computational Linguistics, pages 2014–2037, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\n[6] Mozannar, H., El Hajal, K., Maamary, E., & Hajj, H. (2019). Neural\nArabic Question Answering. Proceedings of the Fourth Arabic Natural\nLanguage Processing Workshop, 108–118. Florence, Italy, August 1,\n2019. © 2019 Association for Computational Linguistics.\n[7] Muhammad Khalifa, Hesham Hassan and Aly Fahmy, “Zero-resource\nMulti-dialectal Arabic Natural Language Understanding” International\nJournal of Advanced Computer Science and Applications(IJACSA),\n12(3), 2021. http://dx.doi.org/10.14569/IJACSA.2021.0120369\n[8] Alroobaea R. An Empirical Deep Learning Approach for Arabic News\nClassification. International Journal of Advanced Computer Science and\nApplications. 2023;14(6).\n[9] Al-Mahmoud RH, Sharieh A. NGram Approach for Semantic Similarity\non Arabic Short Text. International Journal of Advanced Computer\nScience and Applications. 2022;13(11).\n[10] Reimers, Nils, and Iryna Gurevych. ”Sentence-BERT: Sentence Em-\nbeddings using Siamese BERT-Networks.” Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing.\nNovember 2019.\n[11] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.\n2017. Bag of Tricks for Efficient Text Classification. In Proceedings\nof the 15th Conference of the European Chapter of the Association\nfor Computational Linguistics: V olume 2, Short Papers, pages 427–431,\nValencia, Spain. Association for Computational Linguistics.\n[12] Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey\nDean. ”Distributed representations of words and phrases and their com-\npositionality.” Advances in neural information processing in systems, pp.\n3111-3119. 2013.\n[13] Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc Barrault, and\nAntoine Bordes. 2017. Supervised Learning of Universal Sentence Rep-\nresentations from Natural Language Inference Data. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Language Processing,\npages 670–680, Copenhagen, Denmark. Association for Computational\nLinguistics.\n[14] S. Gouws and A. Søgaard. 2015. Simple task-specific bilingual word\nembeddings. In NAACL-HLT., pages 1386–1390\n[15] Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Ma-\njumder, R., Deng, L. (2016). ”MS MARCO: A Human Gener-\nated MAchine Reading COmprehension Dataset.” Retrieved November\n2016, from https://www.microsoft.com/en-us/research/publication/ms-\nmarco-human-generated-machine-reading-comprehension-dataset/\n[16] Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur,\nDavid Alfonso-hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. 2023.\nEvaluating Embedding APIs for Information Retrieval. In Proceedings\nof the 61st Annual Meeting of the Association for Computational\nLinguistics (V olume 5: Industry Track), pages 518–526, Toronto, Canada.\nAssociation for Computational Linguistics.\n[17] Sabbeh, Sahar & Fasihuddin, Heba. (2023). A Comparative Analysis of\nWord Embedding and Deep Learning for Arabic Sentiment Classification.\nElectronics. 12. 1425. 10.3390/electronics12061425\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n2019. BERT: Pre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, V olume 1 (Long and Short\nPapers), pages 4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n[19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. ”RoBERTa: A Robustly Optimized BERT Pretraining Approach.”\nCoRR, vol. abs/1907.11692.\n[20] Mohammed Alsuhaibani. Deep Learning-based Sentence\nEmbeddings using BERT for Textual Entailment. International\nJournal of Advanced Computer Science and Applications,\n14(8), 2023. doi: 10.14569/IJACSA.2023.01408108. URL:\nhttp://dx.doi.org/10.14569/IJACSA.2023.01408108\n[21] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael\nHan, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim,\nChris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna\nEloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr,\nFelipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak\nKhan, Toki Sherbakov, Joanne Jang, Peter Welinder, Lilian Weng.\n”Text and Code Embeddings by Contrastive Pre-Training.” 2022.\n[Link](https://arxiv.org/abs/2201.10005)\n[22] Duquenne, Paul-Ambroise (MetaAI & Inria), Schwenk, Holger\n(MetaAI), Sagot, Beno ˆıt (Inria). ”SONAR: Sentence-Level Multimodal\nand Language-Agnostic Representations.”, August 2023. Meta publisher\n[23] Wang, Liang, Nan Yang, Xiaolong Huang,etal , microsoft., ”Text\nEmbeddings by Weakly-Supervised Contrastive Pre-training.” 2022\n[24] Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf: Distil-\nBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\nCoRR abs/1910.01108 (2019).\nwww.ijacsa.thesai.org 1334 | P a g e",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8609837293624878
    },
    {
      "name": "Embedding",
      "score": 0.7165584564208984
    },
    {
      "name": "Natural language processing",
      "score": 0.6026815176010132
    },
    {
      "name": "Sentence",
      "score": 0.5635613799095154
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.561386227607727
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5360410809516907
    },
    {
      "name": "Metric (unit)",
      "score": 0.5297343134880066
    },
    {
      "name": "Information retrieval",
      "score": 0.4622032642364502
    },
    {
      "name": "Process (computing)",
      "score": 0.4451785385608673
    },
    {
      "name": "Programming language",
      "score": 0.07905492186546326
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I895027328",
      "name": "October University of Modern Sciences and Arts",
      "country": "EG"
    }
  ],
  "cited_by": 9
}