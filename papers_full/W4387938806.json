{
  "title": "Large Language Models and Academic Writing: Five tiers of engagement",
  "url": "https://openalex.org/W4387938806",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2165453681",
      "name": "Martin Bekker",
      "affiliations": [
        "University of the Witwatersrand"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6814396456",
    "https://openalex.org/W6723587114",
    "https://openalex.org/W2940680165",
    "https://openalex.org/W4380985699",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4365450342",
    "https://openalex.org/W7057237628",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4321610465",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W4382361534"
  ],
  "abstract": "In light of the mass adoption of Large Language Models assistance to academic writing, five tiers of LLM support for academic writing are introduced, each offering a different level of writing support, and each entering the writing (and thought-) process at a different stage. Tiers range from no AI-assistance to complete AI-coproduction. Regarding guidelines for publications and assessment-setters, this piece advocates the most utility, and the most practical and morally defensible position, not at the extremes, yet towards the lower end of the continuum. In addition, it claims that, with some intentionality, the principles of ownership (plus responsibility) and transparency (sharing of prompts) can, and ought to be maintained.",
  "full_text": "Large Language Models and Academic Writing:  \nFive tiers of engagement  \n \n                * \nMartin Bekker \nUniversity of the Witwatersrand \n \nIntroduction \n \nAgainst a backdrop of the rapidly expanding use of large language models (LLMs) across \ndiverse domains, this discussion breaks LLM usage into tiers of use, offering practical guidance \nto cautiously embrace the benefits of this significant new tool.  \n \n2023 will be remembered as the year of LLMs, which, led by their brash posterchild, ChatGPT, \nhave changed the world forever. LLM-assisted writing will indelibly alter many writing tasks, \noffering speed and efficiency, and even automating-away many tasks.  \n \nHowever, academic, scientific, and intellectual integrity are at risk: not only due to mistakes \nthat may creep in via the au tomation of writing, but also —and more importantly —owing to \nthe loss of  the ability to construct well- crafted arguments, ostensibly through the dulling of \nscholars’ reasoning via the outsourcing of thinking that LLMs could engender . Moreover, \nethical principles around intellectual process and ownership ought to be protected against the \nvague accountability of black-box algorithms\n1 vis-a-vis published or submitted work (King \n2023, Li et al 2023).   \n \nAcademic journals, along with university and high-school curricula developers and assessment \nsetters, need immediate yet thoughtful guidelines (rules and standards) for using LLMs and AI \nin the scientific process. This short commentary proposes a five-tier system that stipulates \npermissions and prohibitions around the use of LLMs in the academic writing process.  \n \nBelow, I recapitulate what has changed, what—amid all the apparent changes—has stayed the \nsame, and introduce the five tiers of LLM-assistance to academic writing, motivating the lines \nof distinction and suggesting appropriate uses. \n \n  \nWhat has changed \n \nIn existence for over 40 years, Language Models are probabilistic models of a human language \nthat can generate likelihoods of a series of words, based on text corpora  on which they have \nbeen trained (Rosenfeld 2000). Over the last decade, the size of the training text corpora and \nthe number of weights between concepts held within the models have increased, necessitating \naffixing ‘Large’ to recent models, now known as LLMs (Bender et al . 2021). ChatGPT was \nreleased in November 2022, combining the then-most advanced LLM with a chatbot-interface, \nsimplifying the prompting (requesting) and serving (receiving responses) process. With their \npromise of speed and efficiency, ChatGPT and other LLMs have had an immediate impact on \nthe academe; demonstrating the ability to automate the writing of reports, research and \n \n1 Eslami and colleagues suggest the phrase “algorithmic opacity”, developing and clarifying the concept (2023)  \n\nliterature papers, exams,  and computer code, among others, to various degrees of human \nability, with each iteration showing improvement.  \n \nContemporary LLMs represent a break with the past, based on (1) the speed and scale of \ninformation processing, (2) an unprecedented function of research process assistance (includes \nresearch summary and data manufacture), and (3) the potential for the outsourcing of thought. \nThe first of these three innovations often accompanies new technological tools. However, the \nscale and speed at which LLMs perform information processing tasks have now surpassed the \nhuman performance of certain tasks within the so-called information economy (Bubeck et al. \n2023), suggesting a quantum leap in functioning and utility.  \n \nThe second change is tied to th e very nature of the way LLMs  can process information. T he \nability of transformer models —the deep learning architecture  behind LLMs—to manipulate \ntext (or language, or indeed anything that can be represented as a language)  has made LLMs \nsuperlative at summarising texts, style transfers (ranging from translation to mere tone tweaks) \nand spelling and grammar corrections. Moreover, in addition to LLMs, several other AI-related \ntools that assist in the research process have recently been introduced (with many more to \nfollow in their wake ). With these, t wo particular functions spring to mind: first, research \nsummarising tools (e.g., Elicit, Perplexity and Consensus) that can ‘find’ work (published but \nunknown to the scholar) , ‘understand’ discourses, identify res earch gaps , and assist with \nliterature reviews. The second are those that can manufacture artificial (or synthetic) data. Here, \na scholar might give instructions regarding what a dataset should contain, and, in the absence \nof this being available (as secon dary data) or impossible to gather (for, say, ethical reasons), \nsuch data can be ‘created’ instantly.  \n \nThe third change relates to the potential for the wholesale contracting out, or ‘outsourcing’  of \nthought to a (non- human) algorithm. While  cheating is  nothing new,\n2 LLMs present the \nacademy with a new level of concern. That is, from students to scientists, writers are now able \nto turn to LLMs to author academic work from conceptualisation through research and writing \n(Kulesz 2023), putting in peril the principle of scientific advancement through human \nreasoning (or sound thinking as articulated through writing).  \n \nPerils notwithstanding, t he immediate benefit of such a tool —one that can fix the register, \ngrammar, and punctuation of a  text in seconds and apparently at no cost to the user —is \nimmediate and clear. This is especially the case for those writing in a second-language, which \ncase applies to the vast majority of academic scholars, who must publish in English. This \n‘levelling of the playing field’ is to be welcomed by the academic community  (King 2023, Li \net al. 2023, Rillig et al. 2023) . Academics often recite that good writing is indistinguishable \nfrom good thinking; the corollary of this is that clear, high-quality writing helps not only non-\nnative speakers get the recognition they deserve, but benefits humanity if readers access \nknowledge in clear, correct, and accessible language.  \n \nThe immediate drawback accompanying this new class of technology is that, sadly, many \nthings that the academy has long battled to  counter—dishonesty, cheating and plagiarism —\nhave almost instantly become much harder to detect , owing to the increas ed volume and \nsophistication of the breaches. Moreover, LLMs are known to routinely produce credible \n \n2 Academics have long been aware of student essays drafted by ‘essay mills’, computer code written by friends \nor copied from online repositories, or even a human double sitting for an exam on behalf of a less -prepared \nstudent. \nuntruths (‘hallucinations’3) and omit attributions of their source or training data (plagiarism) . \nCombined with the outsourcing of thought itself , such concerns render the use of LLMs for \nacademic work potentially disingenuous  at best, and at worst, in violation  of the norms of \nscientific research. With all of this in mind, the need for practical guidance through LLM’s \nethical minefield is clear.   \n \n \n \nWhat has remained constant \n \nIt is comforting that, despite the impressive and daunting changes wrought by the advent of \nLLMs, in reality, most scientific principles endure. First, the three values of beneficence, \nautonomy, and justice, all tied to non -maleficence and the avoidance of suffering (National \nCommission for the Protection of Human Subjects of Biome dical and Behavioural Research \n1979) stand firm; in this sense, right is still right, and wrong remains wrong. Similarly, cheating \nand plagiarism remain anathema to the spirit of science, while openness, reproducibility, and \nthe sharing (non-obscuring or gatekeeping) of data , keeping in mind all the caveats of harm , \nstill stand as ideals.  \n \nAlso holding firm are pe er review as a well-established principle before publication , and the \nless-formal review-by-peers; that is, the shaping and improving of ideas (and writing) based \non conversations, correspondence, arguments and counsel. Technical h elp—whether in the \nform of word processors, spell checkers, pocket calculators and software programs, or human \neditors and proofreaders—remains accepted and welcomed. \n  \n \nThe five-tier system: an ethical guide to using LLM  \n \nThe present scramble to incorporate LLM use in academic work (or to find ways to ban it) \nimplies that conversations about ethical guidelines and AI -use standards a re timeous and \nvaluable. Given the promise and peril of the new, but guided by the three values  (justice, \nautonomy and beneficence), I propose a 5-tier system to simplify thinking around permissions \nand prohibitions related to using LLM s for academic writing. While representing increasing \n‘levels’ of LLM-support that progress along a seeming continuum, the tiers in fact represent \nparadigmatically different types of mental undertakings.  \n \n \nTier 1: Use ban \n \nThe first level comprises a complete ban on LLM-based support. This means that no LLM tools \nmay be used in the preparation of the academic text. This tier therefore implies the highest \nlevel of human authorship and research authenticity.  \n \nGiven its draconian nature, coupled with the likelihood of inadvertent violations (e.g., the \nspelling and grammar checks employed by ‘ordinary’ word processors use a form of AI, and \ncommon word processors will soon be incorporating several other dimensions of LLMs), this \ntier is the most inviting of being flouted.  Difficulty to enforce, lack of benefit and abundance \n \n3 Other elegant renditions are found in Rillig and coauthors’ writing about LLMs’ “simulated authority” (2023), \nwhile Spitale et al. talks of “compelling misinformation” (2023). \nof drawbacks (e.g., a step backwards in terms of present practice, given for example the \nubiquity of automatic spelling and grammar checks) make such a tier likely only to be used in \nvery specific circumstances, such as proctored university examinations or other formal testing \nconditions.   \n \nTier 2: Proofing tool \n \nHere, human-written text may be submitted to an LLM, accompanied by a prompt instructing \nthe model to fix spelling, grammar, register, tone, and style (in the manner that products such \nas Grammarly might do). A proofing tool can be instructed to catch (and recommend remedies \nfor) tone and style variations, identify problematic or misused words, and highlight direct \ntranslations.   \n \nThe point here, of course, is that the work is presented at the end of the writing process—once \nthe experimental and argumentat ive thinking is complete. Tier 2 does not outsource the \nthinking (or pain) that goes into the drafting process ; rather, it takes fleshed-out thoughts and \ncosmetically enhances (or translates) them in much the same manner as would a private or in-\nhouse proofing team (or academic translation service). \n \nFacilitating word-perfect (or near enough) text before submission, this level of usage can ‘level \nthe playing field’ for non-native speakers and early-career researchers; that is, allowing those \nauthors to display their arguments and findings to best advantage, smoothing over linguistic \nobjections (tacit or implicit), and thus allowing work to be judged on merit alone. Of course, it \nmay also enable ‘lazy’ writing (perhaps on the part of native English writers, knowing that \nsloppy writing will be fixed by an algorithm ). Nonetheless, clarity and universality are \nsignificant in matters of standards, and thus, under this tier, any author might make use of this \nLLM proofing assistance.  \n \nTier 3: Copy Editing tool \n \nEditing and proofing are distinguished by their sequential place in the preparation of texts, but \nalso in the tasks they perform: this distinction is echoed in the differences between tiers 2 and \n3. Given tier 3 permissions, an author may ask for an  LLM to alter text beyond correcting \nlinguistic mistakes and aligning stylistic requirements. Shortening wordy text (e.g., getting that \nblasted abstract from 300 to 150 words), expanding for clarity and rephrasing for precision are \ntasks every academic writer has laboured over and would likely welcome assistance with. Other \nareas of assistance at this tier would be  checking citations for accuracy, style , and \nappropriateness—all things an LLM editorial function\n4 can do (and, in most cases, that paid \nin-house editors do to ensure the quality of publications).  \n \nAn editorial function can ensure a text is well -organised, making changes to a text’s structure \nby reordering paragraphs , or highlighting missing arguments. This tier  will also include the \nassistance that tier 2 permits, including spelling, grammar, tone and style corrections, flagging \nunusual words, nonsensical or confusing text, and guiding smooth transitions between \nparagraphs.  \n \nThis tier would best be used during and after the writing process, and would likely be used \niteratively, perhaps once a substantial section has been written. \n \n4 See Table 2 for prompt examples.  \n \nEfficient editing (which can be a tedious and expensive journey ) and the clarity of resultant \ntexts are among the chief benefits of this tier. If correctly applied, critical thinking (and \nlaboratory work or experimentation) will have preceded this step that in principle simply allows \nfor a near-flawless write-up. Nonetheless, as with tier 2, intellectual thoroughness and writing \nrigour may be compromised, ostensibly making this a compromise that must be accepted. This \ntier raises the point that copyediting is, and should be, regarded as an intellectual contribution \n(although copyeditors are seldom credited in scholarly journal articles in the way they may be \nin the publication of books); underscoring the observation that perhaps all forms of support \nought to be acknowledged in academic writing.   \n \n \nTier 4: Drafting consultant \n \nTier 4 speaks to a process of human- LLM ‘co-creation’\n5. In addition to the support permitted \nunder tiers 2 and 3, this tier permits an iterative back- and-forth of ideas as one might do with \na co-author, up to the  point of (and including) the LLM suggesting the omission of certain \narguments, suggesting alternative ‘interpretations’, or requesting that one rerun experiments or \ncheck back to confirm previous findings.  \n \nThat is, in this tier the author can interact w ith an LLM to plan a research write- up and shape \nand develop an argument, including requesting sample lines (e.g., instructing the LLM to \n‘compose an opening line’). Thus, this is not merely a tool offering a ‘substantive edit’, but a \ntool that can ensur e one’s evidence backs up one’s argument (that is, where an LLM might \neven contribute to shaping that argument at earlier stages ), and where this is not the case, can \nprovide suggestions on how to remedy such gaps.  \n \nUnlike the previous tiers, tier 4 implies  LLM engagement before the writing process \ncommences, followed by iterative ‘reporting back’ sessions with the LLM as the writing \nadvances. Potential benefits include the support such a routine would lend to early- career \nresearchers: hand-holding and sense-checking their writing process and arguments, while also \noffering suggestions and criticisms throughout the process to ensure quality. While the \napproach allowed by this tier is likely to significantly speed up the writing process, it does \nappear to tip t he scales in terms of potential risks. Hallucinations and biases (subtle or not ), \nboth artefacts of LLMs, are more likely to manifest in co -created works. Also, LLMs may \nestablish ways to obscure poor research behind apparently brilliant writing. It also follows that \nan overreliance on LLMs —here operating well beyond argumentative and stylistic \nconsiderations—would constitute a loss of skills the mastery of which  would be expected in \nprofessional scientists or academics.  \n \n \nTier 5: No limits \n \nThe fifth tier allows any LLM-assistance at any stage. This includes brainstorming avenues of \nresearch, discussing and suggesting hypotheses and ideas, and even allowing the LLM to write \ntext on the author’s behalf. Such a no-holds-barred approach also includes interpreting results, \nsummarising other scholarly work, and suggesting the implication of findings. In other words, \nTier 5 permits the outsourcing of thought.  \n \n5 Kulesz (2023) suggests “augmented writing”, King (2023) simply “coauth oring”. \n \nThis tier is likely to be useful for instructing students on AI literacy and AI usage, and more \ngenerally demonstrating the dangers of stochastic models . This tier  has limited value to \nscholarly, peer-reviewed publications , since the principle of aut horship and the requisite of \noriginality (at least as currently conceived) would likely be violated by , for example, a \nsystematic review conducted solely by an AI agent.   \n \nOverview of tiers  \n \nWith their growing levels of permissions, the tiers represent not only increasing degrees of \nLLM-support, but also increasing levels of LLM-dependence. Table 1 illustrates the tiers, and \ntypical use-case points of entry, alongside the most obvious advantages and disadvantages of \neach.  \n \nAs the tiers ‘progress’, so do t he apparent speed and efficiency of tasks, as do the  dangers of \nLLM hallucination and manipulation—both significant and sensible concern s, given the \nopaque nature of LLM’s stochastic processes and governance, not to mention the monopolising \ntendencies among big tech in general. Added to this is the arguably less immediately pernicious \nloss of academic integrity that would attend the outsourcing of thinking, but that may come to \nrepresent a threat to human ity’s overall ability to undertake quality scientific research, in the \nunlikely scenario where LLM-reliance is left unchecked and unregulated . Moreover, real and \nunconscionable human exploitation (Perrigo 2023) and high environmental costs  (Strubell et \nal. 2019), both present in current LLM models, cannot be discounted or wished away.  \n \nTable 1: Summary of LLM permission tiers, when they come into the writing process, and their most obvious \nbenefits and risks.  \n \nTier Effect / Type \nof tool \nPlace in the \nwriting \nprocess  \nMost obvious benefit Most obvious risk \n1 Ban n/a Ensures 100% human \nauthorship and does not \ncompromise academic \nintegrity \nInevitably flouted except under \nexam conditions \n2 Proofing After Increased efficiency, \nreduced cost \nMay subtly alter meaning or \nobscure intentions \n3 Editing  During Produce well-organised, \nword-perfect writing \nLanguage may be bland, may \nfoster laziness \n4 Co-creation Start Offers an alternative to \nhuman partnership–making \ninterpretative and \ninstructive suggestions, \nerror checks  \nAuthorship opaque, high risk of \nintroducing hallucinations and \nbiases,  inexplicability, loss of \nautonomy, loss of critical \nreasoning, outsourcing of thought \n5 All All  High-speed academic \nwriting, maximum support \nfor inexperienced \nacademics \nAs per tier 4, but more extreme   \n \n \n \n \n \nOther AI tools available to the would-be academic author \n \nWhile not part of the writing process, and thus adjacent to the proposed tiers, other academic \ntools draw on new advances in LLM or other recent machine learning technologies. The ethical \nand intellectual considerations of their use overlap with those of LLMs, as do the efficiencies \nthey offer researchers and writers.  \n \nThe first is a class of research summarising tools already introduced in this paper. These can \nperform scans of literature on a topic (e.g., Perplexity), provide one -line summations of \nresearch papers (e.g. , Elicit), give the apparent scientific consensus on a topic (e.g., \nConsensus), or even create a literature review ‘at the touch of a button’. Such tools can provide \nexcellent assistance in exploring new fields of research (or fields related to one’s own work)\n6, \nbut are not  substitutes for the process of critically reading to inform and order one’s own \nintuitions and conclusions, gradually bringing new ideas into relation with one’s own thought-\nscape. Moreover, such models may play a role in reifying conventional wisdoms, and in so \ndoing, drown out marginal voices (the latter which may also be thought of as ‘majority voices’, \nconsidering that most people, including academics, are non- Western, non- white, and non-\nanglophone, despite the outsized influence of Western unive rsities on the global scientific \ncommunity). \n \nThe second non- writing LLM -based tool is ‘ automatic data analysis ’ (e.g. , Langchain), \nwhereby datasets can be loaded up to LLM for statistical analysis. In one dimension, the use \nof such technologies is equivalent to that of a pocket calculator: a logical time-saver, provided \nthe user understands what the LLM is doing. To wit, for at least the past three decades, \nscientists have routinely used multiple regressions, typically executed by statistical software or \na coding routine in a software library. Use of statistical software (not least the writing up of \nresults) requires a basic knowled ge of statistics and data analysis. Subject to new \ndevelopments, danger enters when the process of statistical analysis is not understood by the \nscientist, but regarded, crudely, as magic (i.e., it cannot be explained). In all, the use of LLM \ninterfaces for statistical analysis will likely become commonplace, to the benefit of science in \ngeneral, with the qualification that scientists will still be required to understand at least ‘the \nbare bones’ of statistical analysis.  \n \nThe third application of AI tools, now in the form of machine learning, is the creati on of \nsynthetic data (e.g., Statice). Here, one can ask for data containing certain characteristics and \nof a given (potentially vast) size, or create data for teaching or illustrative purposes (including \ninstances of data unavailability or  cases where ethical  or legal considerations proscribe the \ngathering of such data, such as sensitive healthcare data with patient identifiers information ). \nSuch datasets will play an increasing role in teaching and testing, and as long as users keep in \nmind that the data in hand is fabricated, will be of great advantage in several domains  (and is \npresently used in the training of self-driving cars, models for financial service security and the \npharmaceutical industries).  \n      \nIt is reasonable to expect that more AI-based tools will join the arsenal of the scientist. While \ncaution (based, as ever, on the beneficence, autonomy, and fairness and the avoidance of \nmaleficence principles) regarding the tools’ creation, application and implications  must be \nexercised, many of these tools will provide important and progressive  support to scientific \nadvancements (Rillig et al 2023), and should be embraced.  \n \n6 See Jansen et al (2023) for a discussion of areas in which LLMs might support survey -based research. \n \n \nLLM tools and safety principles \n \nReturning to LLM tools and how they can support the academic writing process, two inviolable \nprinciples merit further reflection: ownership and transparency. AI raises complex questions \nabout these two principles; however, for the time being, the 5-tier system, plus the appropriate \nuse of supplementary material, may help to clarify questions around authorship, responsibility, \nand where we should stand vis-à-vis the place of the thinking process.  \n \nOwnership is the tenet that a submitted or published work and all of it s contents remain the \nresponsibility of a human author, and that the author is the only accountable party for mistakes \nor other consequences emanating from the work (Li et al. 2023). Any academic will be familiar \nwith examples of authors choosing to omit t heir names from a publication (despite having \ncontributed to the scholarship) where they do not fully agree with the contents, or feel unable \nto be held responsible for arguments contained in the work. Yet the broader point here is that \nthe owner of ideas, the agent bearing the risk, and the agent deciding to be listed as author is a \nwork—is, so far, a human one. In our scientific pursuit, in ‘advancing on Chaos and the Dark’ \nit is the human—often individual—thinker that toils, that weighs, that risks (Emmerson 1987). \nIntellectual progress and the advancement of human thinking assumes the hitherto generally \nunspoken assumption that human thought ought not  to be outsourced to non- human entities. \nDifferently put, and evoking Chesterton (1908), you cannot make science without a soul. Even \nIn the scenario where a human and an LLM ‘co -create’ a work—the responsibility for the \ncontent still must rest somewhere, and, in the spirit of science, this would most obviously be \nthe human author.\n7 The idea of blaming an LLM for mistakes appears disingenuously evasive \nand points to a concerning ambiguity over authorship. Certainly, under the first three tiers, as \nsuggested here, authorship and therefore ownership rests with the human writer. The corollary \nof this is that the scientific value of papers, books and artefacts produced under tiers 4 and 5 \nare de facto limited, and are likely to remain so: should this change, we will have to rethink \nauthorship and academic credit anew. \n \nWhile it is possible  that humans will not forever be regarded as the sole culpable parties of \ntheir publications, recent work\n8 drawing on AI advances continues to confirm the principle of \nhuman authorship. Of course, acknowledgement of AI tools used in research (and \nacknowledgements in general) is categorically different from named authors to a work; that is, \ncontribution does not imply attribution.  \n \nThe second principle is that of  transparency. Transparency—that is, showing one’s wor k and \nthought process —lies at the heart of scientific accountability, reproducibility, peer \naccessibility, and public trust (c.f. King 2023). LLMs are by their nature opaque (Bender et al. \n2021, Eslami et al 2023 ); that does not mean they cannot be used, but rather that we must be \nopen about when and how we use them. The alternative scenario is that readers have to guess \nwhether LLMs have been used in the producing of text (Li et al. 2023) –  here, mistrust is a \nsolvent of credibility.  \n \n \n7 Similarly, a recent US court ruling heard that ‘the guiding human hand’ is a ‘bedrock requirement’ to \nauthorship (Thaler v Perlmutter, 2023) \n8 Note for instance that despite the success of AlphaFold being based on Google DeepMind’s algorithms, the \ncelebrated Nature paper (Jumper et al. 2021) lists only the human authors  \nI s uggest that authors  who use LLM assistance  include a way for their readers to see the \nprompts (and responses)  they used in preparing a text . While the American Psychological \nAssociation referencing standard has issued a protocol for referencing ChatGPT, possibly a \nbetter way of referencing LLM contributions would be to provide a way for readers to access \nthe entire series of human-LLM interactions, including every prompt and response. A hyperlink \nto an online repository (such as GitHub  or Google documents) may risk being too fleeting a \nbase, while a text file as supplementary material might suffice in ‘showin g one’s working’ in \nthe same way as sharing one’s routine of commands to statistical software, or sharing a \ncodebase when programming.  \n \nTable 2: Examples of initial prompts per permission tiers 2 and 3 (these are intentionally kept straightforward \nand unsophisticated) \nTier Sample prompts \n2 a) The following is section [x] from my paper, which I aim to submit to [name of journal]. Proofread \nthe section and suggest corrections for spelling, grammar, tone, and style errors. Ensure the text is \nclear and free from any direct translations. Also, review the section for tone and style variations, and \nnote any such pointwise. Finally, identify problematic or misused words, list each, and provide a \nrecommendation for replacement. \nb) Make recommendations to improve the overall readability and coherence of the text. “[text here]”  \n3 a) Edit the below to shorten words to 2000 words while preserving content, intention and clarity.  \nb) Check the citations in the following document for accuracy, style, and appropriateness. List any \nnecessary corrections pointwise.  \nc) Make suggestions for the reorganising of paragraphs in the document below to improve the \noverall structure and flow of the argument. Briefly motivate each modification.  \n \n \nDiscussion: AI hype and despair \n \nThere is nothing emergent—in the complex adaptive systems theory sense—in the working of \na pocket calculator. Calculators’ answers are consistent, predictable, replicable, and regular. In \nthis sense, LLMs are not like calculators: their massive size and black -box nature appear to \nhave given them, at least by most accounts, emergent capabilities. T he general public (and \ntechnical!) discourse has tended to label this not as ‘emergent’ but rather as ‘generative’ —\nwhich is, all told, succumbing to the language of AI hype. 2023 may well be marked as a high \npoint of AI -optimism, not least owing to the abilities and potential of LLMs. However, the \nability to distinguish helpful innovation from unhelpful hyperbole (whether AI-saviourism or \nAI catastrophising) is important in order to keep humankind’s present problems and struggles \nin perspective, recognise our immediate moral duties, and rationally analyse the extent to which \na new class of tools can help (or hinder) scientific progress and human betterment.   \n \nFor scientists across e very branch of knowledge, mental panic and ossification remain our \nnemeses. We gain most by seeing neither cataclysmic doom nor total redemption in \ntechnology, but instead recalibrating a new technology’s value based on what it can change, \nand what it can’t.  \n \nAmong the proposed tiers, tier 1 is techno- pessimistic. This tier assumes  that technology per \nse represents a threat to knowledge production and human capabilities. This position is, to my \nmind, untenable for an academic journal already heavily reliant on LLMs (e.g., in the form of \nspell-checkers), not to mention calculator -like technology. In contrast, tiers 2 and 3 may be \nregarded as technology- embracive. Optimistic about the efficiencies LLMs bring to human \nknowledge and scientific advancement, these advocate for adoption, remove the first-language \nbarriers so often inhibiting the global dissemination of great ideas, and may even expedite the \nwriting-up process. If permitted some rumination, one might suggest that t iers 4 and 5 are  \nleaning towards AI -hype; both of these believing that LLMs are either on the path to true \ncognitive supremacy and should thus be employed at all costs (the slave will soon become the \nbenign master), or, alternatively, taking up the despairing position that LLMs will soon be  so \nubiquitous that any resistance to their use is bound to fail9.  \n \n \nConclusion \n \nFive tiers of LLM support for academic writing have been introduced, each offering a different \nlevel of writing support, and each entering the writing (and thought -) process at a different \nstage. With some intentionality, the principle s of ownership ( plus responsibility) and \ntransparency (sharing of prompts) can, and ought to be maintained.   \n \n \n \n \n \nReferences  \nBender, E.M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On \nthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the \n2021 ACM conference on fairness, accountability, and transparency (pp. 610-623). \nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., ... \nNori, H. (2023). Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv \npreprint arXiv:2303.12712. \nChesterton, G. K. (1908). Orthodoxy. San Francisco: Ignatius Press/John Lane \nCompany. \nEmerson, R.W. (1987). Self-Reliance (1841). Essays and Lectures, ed. Joel Porte \n(New York: Library of America, 1983), 261. \nEslami, M., Vaccaro, K., Lee, M.K., Elazari Bar On, A., Gilbert, E. and Karahalios, \nK., (2019, May). User attitudes towards algorithmic opacity and transparency in online \nreviewing platforms. In Proceedings of the 2019 CHI Conference on Human Factors in \nComputing Systems (pp. 1-14). \nJansen, B.J., Jung, S.G. and Salminen, J., (2023). Employing large language models \nin survey research. Natural Language Processing Journal, 4, p.100020. \nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., \nTunyasuvunakool, K., Bates, R., Žídek, A., Potapenko, A. and Bridgland, A., (2021). Highly \naccurate protein structure prediction with AlphaFold. Nature, 596(7873), pp.583-589. \n \n9 One might argue that King (2023) and Jansen et al. (2023), on whose insigh t this note draws, leans towards \nthe possibility of a tier 5 future, where AI will become colleagues and coauthors.  \nKing, M.R., (2023). A Place for Large Language Models in Scientific Publishing, \nApart from Credited Authorship. Cellular and Molecular Bioengineering, pp.1-4. \nKulesz, O. (2023). \"The impact of Large Language Models on the publishing sectors: \nBooks, academic journals, newspapers.\" Master's Thesis, Linnaeus University, Sweden. \nLi, H., Moon, J.T., Purkayastha, S., Celi, L.A., Trivedi, H. and Gichoya, J.W., (2023). \nEthics of large language models in medicine and medical research. The Lancet Digital \nHealth, 5(6), pp.e333-e335. \nNational Commission for the Protection of Human Subjects of Biomedical and \nBehavioral Research. (1979). The Belmont report: Ethical principles and guidelines for the \nprotection of human subjects of research. Available at: \nhttp://www.hhs.gov/ohrp/regulations-\nand-policy/belmont-report. \nPerrigo, B. (2023). Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per \nHour to Make ChatGPT Less Toxic. Time. \nRillig, M.C., Ågerstrand, M., Bi, M., Gould, K.A. and Sauerland, U., (2023). Risks \nand benefits of large language models for the environment. Environmental Science & \nTechnology, 57(9), pp.3464-3466. \nRosenfeld, R. (2000). Two decades of statistical language modeling: Where do we go \nfrom here? Proceedings of the IEEE, 88(8), 1270-1278. \nSpitale, G., Biller-Andorno, N. and Germani, F., (2023). AI model GPT-3 (dis) \ninforms us better than humans. arXiv preprint arXiv:2301.11924. \nStrubell, E., Ganesh, A., & McCallum, A. (2019). Energy and Policy Considerations \nfor Deep Learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for \nComputational Linguistics (pp. 3645-3650). \nThaler, S. v. Perlmutter, S. (2023). Register of Copyrights and Director of the U.S. \nCopyright Office, et al, Civil Action No. 22-1564 (BAH) (18.08.2023). U.S. District Court \nfor the District of Columbia. \n* Layer cake graphic available from www.freevector.com\n under the non-commercial \n“standard usage” term. ",
  "topic": "Coproduction",
  "concepts": [
    {
      "name": "Coproduction",
      "score": 0.7070122957229614
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.680090069770813
    },
    {
      "name": "Writing process",
      "score": 0.5535651445388794
    },
    {
      "name": "Intentionality",
      "score": 0.5422424077987671
    },
    {
      "name": "Academic writing",
      "score": 0.5125093460083008
    },
    {
      "name": "Process (computing)",
      "score": 0.4399877190589905
    },
    {
      "name": "Computer science",
      "score": 0.4309435188770294
    },
    {
      "name": "Public relations",
      "score": 0.3155522346496582
    },
    {
      "name": "Psychology",
      "score": 0.30064913630485535
    },
    {
      "name": "Mathematics education",
      "score": 0.3000907897949219
    },
    {
      "name": "Political science",
      "score": 0.27761924266815186
    },
    {
      "name": "Epistemology",
      "score": 0.15294647216796875
    },
    {
      "name": "Computer security",
      "score": 0.11285656690597534
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}