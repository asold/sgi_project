{
    "title": "OMPGPT: A Generative Pre-trained Transformer Model for OpenMP",
    "url": "https://openalex.org/W4391420745",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2106377209",
            "name": "Chen Le",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745487629",
            "name": "Bhattacharjee Arijit",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224410081",
            "name": "Ahmed Nesreen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287073131",
            "name": "Hasabnis, Niranjan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2003699468",
            "name": "Oren, Gal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4283936198",
            "name": "Vo, Vy",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2819737013",
            "name": "Jannesari, Ali",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385585457",
        "https://openalex.org/W4386269029",
        "https://openalex.org/W4289827540",
        "https://openalex.org/W1988888548",
        "https://openalex.org/W4388581500",
        "https://openalex.org/W4391558516",
        "https://openalex.org/W4390188475",
        "https://openalex.org/W4386268988",
        "https://openalex.org/W6846665566",
        "https://openalex.org/W4401996408",
        "https://openalex.org/W4396790347",
        "https://openalex.org/W4388186316",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4281763794"
    ],
    "abstract": null,
    "full_text": "OMPGPT: A Generative Pre-trained\nTransformer Model for OpenMP\nLe Chen†1, Arijit Bhattacharjee†1, Nesreen Ahmed2, Niranjan Hasabnis2, Gal\nOren3, Vy Vo2, and Ali Jannesari 1 ⋆\nAbstract. Large language models (LLMs)such as ChatGPT have signif-\nicantly advanced the field of Natural Language Processing (NLP). This\ntrend led to the development of code-based large language models such\nas StarCoder, WizardCoder, and CodeLlama, which are trained exten-\nsively on vast repositories of code and programming languages. While the\ngeneric abilities of these code LLMs are helpful for many programmers\nin tasks like code generation, the area of high-performance computing\n(HPC) has a narrower set of requirements that make a smaller and more\ndomain-specific model a smarter choice. This paper presents OMPGPT,\na novel domain-specific model meticulously designed to harness the in-\nherent strengths of language models for OpenMP pragma generation.\nFurthermore, we leverage prompt engineering techniques from the NLP\ndomain to create Chain-of-OMP, an innovative strategy designed to en-\nhance OMPGPT’s effectiveness. Our extensive evaluations demonstrate\nthat OMPGPT outperforms existing large language models specialized\nin OpenMP tasks and maintains a notably smaller size, aligning it more\nclosely with the typical hardware constraints of HPC environments. We\nconsider our contribution as a pivotal bridge, connecting the advantage\nof language models with the specific demands of HPC tasks.\nKeywords: Large Language model · OpenMP · HPC\n1 Introduction\nRecent advancements in transformer-based [24] large language models (LLMs)\nhave revolutionized artificial intelligence and machine learning. These models\nhave shown remarkable performance in natural language processing (NLP) tasks,\nleading to the development of code-based LLMs such as StarCoder [15], Wizard-\nCoder [18], and CodeLlama [20], which are specifically designed for programming\nlanguage tasks. However, applying these models to High-Performance Comput-\ning (HPC) tasks presents unique challenges.\n– Training data diversity: Advanced LLMs like GPT3 and CodeLlama are\ntrained on both natural language (NL) and programming languages (PL),\nenabling them to interpret NL prompts and generate appropriate PL code.\nIn contrast, models like Starcoder, trained solely on code, struggle with NL\nprompts and are limited to code generation tasks.\n⋆ †: Equal contribution. 1: ISU. 2: Intel. 3: Technion\ncontact: lechen@iastate.edu\narXiv:2401.16445v3  [cs.SE]  22 Jun 2024\n2 Chen et al.\nPrompt\n\"What OpenMP\npragma should I use to\nparallelize this code: \"\nAnswer\n\"To parallelize the code\nsnippet you've provided\nusing OpenMP... \"\n\"Explanation: ... \"\nPrompt\n#pragma omp for\n#pragma omp parallel\n...\nFine-tuning with OMP codebaseTraining with HPC codebase\n#pragma omp parallel for \\\nreduction(+:sum)\nAnswer\n#pragma omp parallel\nAnswer\nFine-tuning\nData\nTraining\nData\n#pragma omp for\n...\nAnswer\nPrompt\n\"#pragma omp\" #pragma omp parallel\nFig. 1.\n (black): Traditional LLMs require extensive NL and PL training data for\ngenerating OpenMP pragmas, leading to an increased complexity and larger model\nsize. Users usually carefully craft prompts and interpret outputs to obtain accurate\nOpenMP pragmas.\n (blue): OMPGPT is tailored for HPC code, with nearly half\nof its training data being OpenMP code. It aligns OpenMP pragmas with their scope\nduring training to match GPT model instincts. OMPGPT not only addresses the limi-\ntations of current LLMs but also benefits from theChain-of-OMP prompt engineering\ntechnique.\n– Performance consistency. Using NL as input can lead to variability in LLM\noutputs due to different phrasing of the same question by users, posing chal-\nlenges for consistent performance and post-processing.\n– Output processing. LLMs typically return answers in NL, necessitating ad-\nditional effort to extract relevant information for practical use.\nTo address these challenges, we propose the OMPGPT model and the chain-\nof-OMP prompt engineering method for automatic parallelization via OpenMP\npragma generation. As illustrated in Figure 1, OMPGPT is a domain-specific\nmodel trained on an extensive HPC dataset of C and C++ code, converting\nOpenMP pragma generation into a code generation problem. This approach\nfollows the instinct of LLMs and eliminates the need for training NL data to un-\nderstand the task objective. The Chain-of-OMP method enhances OMPGPT’s\nperformance by incrementally refining prompts with preconditions for OpenMP\npragma generation, aligning with the structure of OpenMP pragmas. The con-\ntributions of this work are as follows:\n– We introduce OMPGPT, a compact 0.76B domain-specific language model\n(smallest among our baseline models) tailored for OpenMP pragma genera-\ntion with competitive performance to larger LLMs.\n– We propose a novel OpenMP clause-based prompting technique, Chain-of-\nOMP, which enhances OMPGPT by providing targeted hints.\n– Our comprehensive evaluation demonstrates OMPGPT’s superior perfor-\nmance in OpenMP pragma generation compared to the state-of-the-art mod-\nels MonoCoder [11] and GPT-3.5 and highlights the effectiveness of the\nChain-of-OMP method in boosting OMPGPT’s capabilities.\nOMPGPT: A Generative Pre-trained Transformer Model for OpenMP 3\n2 Background\n2.1 Generative Pre-trained Transformers and Code LLMs\nThe emergence of Generative Pre-trained Transformers (GPT) has revolution-\nized Natural Language Processing (NLP) and extended its influence to program-\nming languages, with models like GPT-3.5 capable of generating source code.\nGPT models are autoregressive, generating text sequentially from left to right,\nwhich enables them to produce contextually appropriate responses in natural\nlanguage tasks. This led to the development of Large Language Models (LLMs)\nfor code (Code LLMs), which are able to understand and generate programming\ncode. Code LLMs are trained on extensive datasets containing code snippets and\nare designed to assist in code completion, quality improvement, and streamlining\nthe software development process. However, challenges remain in ensuring the\nmodels can generate contextually and functionally meaningful code.\nStarCoder [15] is a 15B parameter model trained for code generation or\ncompletion. The training dataset, the Stack [13], has 1 trillion tokens sourced\nfrom a large collection of permissively licensed GitHub repositories. CodeLlama\n[20] is a code-generating LLM based on Llama2 [22] by Meta, specialized for code\nby training with code-specific datasets. It has parameter sizes of 7B, 13B, and\n34B, trained with 500B tokens of code data. Most code LLMs (e.g., StarCoder)\nhave been trained on raw code data without instruction fine-tuning. However,\nthe recent WizardCoder [18], enables Code LLMs with instruction fine-tuning\nby adapting the Evol-Instruct methods for coding tasks, and it was shown to\nimprove the performance of code generation.\n2.2 LLMs on code-related tasks for HPC\nLLM models show potential in HPC tasks by leveraging their ability to capture\ncomplex code patterns and boost efficiency. Although not specifically designed\nfor HPC, their adaptability makes them valuable for code-related HPC chal-\nlenges [23]. HPC predominantly uses C, C++, and Fortran for low-level control\nand parallelism optimization capabilities. Focusing on these primary languages\nallows smaller, more efficient models tailored to specific HPC tasks with direct\ninputs. Recently, research in applying LLMs to HPC has been emerging. For\nexample, LM4HPC [4] stands as the first attempt to adapt LLMs to the HPC\ndomain. Building upon this, a subsequent study by Ding et al. [8] introduced\na Llama-based Question and Answer model specifically tailored for HPC tasks.\nDespite these advancements, these early efforts predominantly rely on existing\nLLMs, indicating a nascent stage in developing HPC-focused language models.\nOne important HPC task is sequential code parallelization. OpenMP (Open\nMulti-Processing) [7] is the mainstream API that supports multi-platform shared-\nmemory multiprocessing programming in C, C++, and Fortran. It enables de-\nvelopers to write parallel code using multiple cores on a single machine. Various\nworks [6,5] have applied machine learning techniques to predict OpenMP prag-\nmas for sequential code parallelization.\n4 Chen et al.\n2.3 Prompt Engineering\nPrompt engineering is a key technique in LLMs for crafting input prompts that\nelicit specific outputs. Chain-of-thought (CoT) [25] is a prominent method where\na series of intermediate reasoning steps enhances an LLM’s complex reasoning\nability. Instead of a single prompt, CoT guides the model through phases of\nsmall hints, helping it grasp the deeper meaning of the query. This approach has\nsignificantly improved LLM performance in areas like arithmetic and common\nsense reasoning.\n3 Approach\n3.1 OMPGPT Design\nAs highlighted in Section 1, existing Large Language Models (LLMs) encounter\nsignificant limitations when applied to HPC tasks, particularly in the realm of\nOpenMP pragma generation. In designing OMPGPT, we consider several criteria\nto ensure its suitability and efficacy for OpenMP code generation:\n1. Training Data Relevance: The quality of the training dataset is essential\nto any language model. OMPGPT is trained on HPC code in the most\ncommon programming languages in the HPC field.\n2. Model Compatibility: The OMPGPT model size is aligned with the hard-\nware configurations prevalent in most HPC clusters.\n3. Flexibility and Adaptability : OMPGPT is designed to be flexible and\nadaptable, capable of handling a variety of OpenMP pragma generation tasks\nwithout the need to craft prompts\n4. Performance Efficiency: OMPGPT is expected to outperform previous\nsmall language models and be competitive compared to advanced LLMs.\n5. User Accessibility : Recognizing the varied nature of HPC ecosystems,\nOMPGPT is designed for a diverse array of users, regardless of their OpenMP\nknowledge background.\n3.2 OMPGPT Training & Inference\nDataset. HPCorpus, introduced by Kadosh et al. [10], is an extensive HPC\ndatabase derived from GitHub repositories containing C, C++, and Fortran\ncode. Notably, approximately 45% of the repositories use some form of parallel\nprogramming. The primary parallel programming mode is OpenMP, making\nHPCorpus suitable for training models for OpenMP tasks. We use C and C++\ncode from HPCorpus for our model’s training and fine-tuning, with 144,522 C\ncode repositories and 150,481 C++ code repositories, totaling 8,781,759 C code\nfunctions and 68,233,984 C++ code functions (a total of 72.39 GB). Figure 2\nshows the distribution of OpenMP pragmas in the HPCorpus OpenMP subset\nused for OMPGPT fine-tuning. We allocate 10% of the data as a test set for\nboth training and fine-tuning to maintain evaluation integrity.\nData Processing. Our data pre-processing for OMPGPT includes the follow-\ning:\nOMPGPT: A Generative Pre-trained Transformer Model for OpenMP 5\nparallel\nbarrier\nHPCorpus\nfor\nprivate\nsimd\nshared\ndefault\ntarget\ncritical\natomic\nreduction\ndistributeteamscollapse\nparallel for\n22.3% \n19.3% \n14.5% \n7.3% \n6.4% \n6% \n4.6% \n4.1% \n3.1% \n2.4% \n2.2% \n2.1% 2% 1.9% 1.9% \nFig. 2.Distribution of OpenMP Prag-\nmas in the HPCorpus OpenMP subset\nwith the top 15 most frequently occur-\nring pragmas.\nAnswer (a)\n \nOMPGPT\nPrompt (a)\n\"#pragma omp for\"\nAnswer (b)\n \n\"#pragma omp\"\nPrompt (b) OMPGPT\nFig. 3.Basic prompt for OMPGPT. (a):\nprompt for code generation. (b): prompt for\nOpenMP pragma generation.\n1. Trimming natural language: OMPGPT is trained solely on code. There-\nfore natural language text such as comments are removed.\n2. OpenMP Pragma Positioning : Our approach, unlike previous studies,\nplaces OpenMP pragmas after their scopes, leveraging GPT models’ instinct\nto overcome the constraints of current LLMs. This strategy aligns with the\nfindings of the previous work in [19] but diverges by training OMPGPT\nfrom scratch to generate OpenMP pragmas, a capability not present in\nconventional pre-trained models. This unique training methodology tailors\nOMPGPT to the demands of OpenMP pragma generation and enhances its\nability to understand and process the HPC-specific code structures.\n3. Filtering code by size: Following practices used in previous research, such\nas PolyCoder [26], we filter out large code segments (more than 100 tokens\nor greater than 1 MB) from HPCorpus.\nHardware. In all experiments, we utilized a single node on an HPC cluster\nequipped with a dual-socket AMD EPYC Rome 7402 (24 cores/socket) and 512\nGB DDR4-3200 RAM, along with 4 x NVIDIA A100 40GB HBM2e Tensor Core\nGPUs connected via NVLink3 to each other.\nOMPGPT Training: We leveraged the GPT-Neo 2.7B [2] model architecture\nas a foundation. However, we have tailored the model to better suit our needs\nby downsizing the number of layers to 8 while maintaining 32 attention heads\nper layer. The model features a hidden dimension of 2560. We employ the Star-\nCoder tokenizer for tokenization, which utilizes a vocabulary comprising 49,152\ntokens. This configuration results in a total parameter count of 0.76 billion for\nOMPGPT, making it more compact than our baseline models.\nOMPGPT Inference. We convert the OpenMP pragma generation problem\ninto a code generation task by replacing the OpenMP pragmas in training. Fig-\nure 3 (a) illustrates inference with OMPGPT for code generation. Originating\nfrom a GPT-based model, OMPGPT inherits the capability for generation tasks\ngiven input code. OMPGPT is trained on pre-processed OpenMP code where\npragmas are moved to the end of loops. When prompted with a prefix like\n6 Chen et al.\n#pragma omp, as shown in Figure 3 (b), OMPGPT demonstrates its special-\nized ability to generate relevant OpenMP pragmas. This highlights OMPGPT’s\naptitude for intuitively continuing code sequences tailored to the syntax and\nstructure of OpenMP directives, showcasing practical utility for real-world pro-\ngramming scenarios.\n#pragma omp\nPrompt OMPGPT\n OMPGPT\n#pragma omp for schedule(dynamic, 10)\nAnswer\n \nChain\nNChain\nStart Directive/Clause #1     Directive/Clause #N      Control Variables    \n#pragma omp for private\nChain\n \nFig. 4.Chain-of-OMP. Start phase: using the standard OpenMP pragma generation\nprompts for OMPGPT. Directive/clause generation phase: the first generated Direc-\ntive/clause is augmented to the previous input as the input for the next chain. This\nphase is updated till it number of chains reaches parameter Nchain. The final Control\nvariable generation phase generates the last component to complete the chain-of-OMP\nand generate the OpenMP pragma.\n3.3 Chain-of-OMP\nChain-of-thought (CoT) [25] facilitates complex reasoning in NLP tasks by guid-\ning LLMs through intermediate steps. Inspired by the ”think step by step”\nmethodology, we developed Chain-of-OMP, a novel prompt engineering tech-\nnique to enhance OMPGPT in generating OpenMP pragmas.\nWe consider an OpenMP pragma as comprising three major components:\n– Directive Prefix ( #pragma omp): This universal prefix signals the presence\nof an OpenMP directive to the compiler. Within our model, this prefix is\nused in prompts to orient OMPGPT towards generating OpenMP pragmas,\nsteering it away from generic code generation. We represent it as <prefix>.\n– Directives and Clauses. This segment outlines the type of parallel or work-\nsharing construct being utilized and details its specific behavior. We repre-\nsent them as <dc>.\n– Control Structure. A control structure typically immediately follows the di-\nrectives. It is essential for the behavior of directives like #omp parallel\nreduction. We represent it as <cs>.\nIn OpenMP syntax, these three components are listed sequentially (i.e.,\n<prefix><dc><cs>), perfectly fitting the instinct of LLMs. Leveraging this,\nChain-of-OMP operates as an automated sequence of prompts, as depicted in\nFigure 4. The process begins with standard OpenMP pragma generation prompts\nOMPGPT: A Generative Pre-trained Transformer Model for OpenMP 7\n(i.e., <prefix>). The output from the first stage is then refined, retaining only\nthe generated directive/clause, which is subsequently passed to the next OMPGPT\nclient to generate the following component. In other words, we only retain\n<prefix><dc> part of the generated output and ignore the rest. This retained\npart is then fed as the prompt for the next stage. In general, the components\nof an OpenMP pragma expand incrementally, with one element added at each\ninference stage, until it reaches a user-defined limit, Nchain. The input In for a\nchain component chainn is defined by Equation 1.\nIn = concat(In−1, first generated directive/clause in O n−1) (1)\nwhere In, On stands for the input and output of chainn. concat stands for the con-\ncatenate operation. OpenMP pragmas have a different number of components.\nConsequently, we need a different number of chains to generate the complete\nOpenMP pragma. We let the user set the maximum number of chains limit by\nsetting Nchain. The default value of Nchain is 256 to avoid an infinity loop. The\nvalue aligns with the commonly used maximum output length of LLMs. The\ndesign of Chain-of-OMP has the following advantages:\n– Mimicking Expert User Inquiry: This approach closely replicates the query-\ning process employed by experienced OpenMP users. Instead of requesting a\ncomplete pragma with a basic prompt (using just#pragma omp), skilled users\noften provide more specific information, such as directives (e.g., #pragma\nomp for), to refine their inquiry about the remaining parts of the pragma.\nChain-of-OMP embodies this nuanced approach, leading to more targeted\nand accurate pragma generation.\n– Enhancing Performance Across Various LLMs: By selectively retaining the\ninitial directive/clauses and discarding the rest, Chain-of-OMP effectively\nnarrows the search space for subsequent chains. This approach is akin to the\nstrategy in the classic Monty Hall problem, where a ”pick again” method\ntheoretically increases the chances of accuracy. Such a strategy is anticipated\nto improve OpenMP pragma generation accuracy not just for OMPGPT but\nfor other LLMs as well.\n– Automation of the Process: A key strength of Chain-of-OMP is its fully\nautomated chain generation capability. This contrasts traditional chain-of-\nthought techniques, where users typically need to craft prompts for each\nstep manually. Users usually do not need to specify the value of Nchain, as\nthe model will stop when it predicts there is an end of a pragma. Explicitly\ndefining Nchain is also an option for expert users who want the model to run\nn chain stages. The automation in Chain-of-OMP streamlines the process,\nmaking it more efficient and user-friendly.\n3.4 Fine-tuning\nDue to the complexity of OpenMP pragmas, previous works either train an ML\nmodel or fine-tune a language model for specific tasks covering a limited selection\nof pragmas. In our work, we have employed a strategic fine-tuning approach to\n8 Chen et al.\ndemonstrate the enhanced performance of OMPGPT after fine-tuning and to\nfacilitate a comprehensive comparison with baseline models.\nWe fine-tune the pre-trained OMPGPT model using the AdamW optimizer [17],\na variant of the Adam optimizer known for its effectiveness in large models and\ndatasets. This process incorporates a linear warm-up phase over the initial 100\nsteps. The warm-up phase gradually increases the learning rate from zero to the\ninitial learning rate set for training, helping to stabilize the model’s learning\nprocess in its early stages. Following the warm-up, we implement a linear decay\nin the learning rate for the remaining steps. This decay approach gradually re-\nduces the learning rate, allowing for finer adjustments to the model’s weights as\nit converges toward optimal performance.\n4 Evaluation\n4.1 Model Perplexity\nTask Definition. This subsection evaluates the general knowledge of OpenMP\ncode possessed by the base model. We assess its ability to generate code using\nperplexity score, a common metric in language processing. Perplexity essentially\nmeasures how surprised a model is by the next word in a sequence. Lower per-\nplexity indicates the model can better predict upcoming elements and thus has\na better understanding of the language. In this context, we use perplexity to\ngauge the model’s grasp of OpenMP code structure.\nEvaluation Setup. As described in Section 3.2, we use 10% of the HPCorpus\ndataset as a test set unknown to OMPGPT. We calculate OMPGPT’s perplexity\n(PP) using the following Equation 2.\nPP = exp\n \n− 1\nN\nNX\ni=1\nlog P(wi|w1, w2, . . . , wi−1)\n!\n(2)\nWhere P(wi, w1, w2, . . . , wi−1) is the probability of the first word wi occurring,\ngiven the sequence of the subsequent words w1, w2, . . . , wi−1. We use prompt\n(a) in Figure 3 for general code generation and calculate the perplexity. We\ncompare the perplexity result of OMPGPT with MonoCoder and other open-\nsource language models.\nBaselines. Table 1 compares perplexity scores on OpenMP code for various\nTable 1.Perplexity Comparison across\nOpen-source Language Models.\nModel Size (B) C C++\nOMPGPT 0.76 3.54 3.66\nMonoCoder 0.89 3.51 3.69\nPolyCoder 2.7 2.33 2.99\nGPT-Neo 2.7 3.69 2.87\nGPT-J 6 2.82 2.47\nCodeX 12 2.55 1.95\nStarCoder 15.5 1.71 2.01\nGPT-NeoX 20 2.37 2.32\nopen-source code LLMs. These models\nhave different parameter sizes and were\nall trained with extensive parallel learning\n(PL) data, enabling their code generation\ncapabilities. However, most of them are\ngeneric models and are included here for\nperplexity evaluation only. MonoCoder\nstands out as the only model specifically\nfocused on OpenMP. Additionally, it was\nOMPGPT: A Generative Pre-trained Transformer Model for OpenMP 9\ntrained and tested with the HPCorpus\ndataset, making it a well-suited baseline\nfor our later investigation of OpenMP\npragma generation.\nResults. As shown in Table 1, OMPGPT\nachieves competitive scores (3.54 for C\nand 3.66 for C++) despite its smaller size (0.76B) compared to models like\nPolyCoder (2.7B) or GPT-J (6B). While larger models like StarCoder (15.5B)\ngenerally achieve lower perplexity, OMPGPT demonstrates efficiency by com-\npetently bridging this gap with larger models. This trend suggests a correlation\nbetween model size and perplexity, but the efficiency of OMPGPT is notable,\ngiven its significantly smaller size. It competently bridges the gap with larger\nmodels, indicating a promising direction for efficient model design in the code\ngeneration tasks.\n4.2 OpenMP Pragma Generation with OMPGPT Base Model\nTask Definition. OMPGPT is trained with the preprocessed OpenMP code\nin HPCorpus. The capability of generating any OpenMP pragma distinguishes\nOMPGPT from most existing code-oriented Large Language Models (LLMs).\nThis task evaluates OMPGPT’s performance on the 15 most prevalent OpenMP\npragmas found in the HPCorpus test set to assess this capability.\nEvaluation Setup. We followed the Prompt (b) in Figure 3, where the in-\nput code is followed by the hint #pragma omp to signal to OMPGPT that we\nare performing the pragma prediction instead of generic code generation. Eval-\nuation metrics focus on accuracy, specifically gauging the congruence between\nOMPGPT’s outputs and the original OpenMP pragmas.\nWe employed a strict matching criterion, wherein a generated pragma is\nconsidered correct only if it precisely matches the corresponding pragma in the\ntest dataset. For instance, although the following two OpenMP pragmas are\nfunctionally equivalent, they are not deemed correct under this strict criterion:\n#pragma omp parallel for reduction(+:sum) private(var)\n#pragma omp parallel for private(var) reduction(+:sum)\nResults. Figure 5 compares OMPGPT with its base model GPT-Neo. Notably,\nOMPGPT outperforms GPT-Neo, indicating the performance gain is achieved\nas OMPGPT successfully learned the task of OpenMP pragma generation. How-\never, as shown in the figure, the strict matching criterion results in lower scores,\nhighlighting the challenge of achieving perfect accuracy in this task.\n4.3 OpenMP Pragma Generation using Chain-of-OMP\nTask Definition. As shown in Section 4.2, OMPGPT’s base model can generate\nOpenMP pragmas, but accuracy is limited due to the complexities of OpenMP\nsyntax. This section evaluates our proposed Chain-of-OMP method, assessing\nits effectiveness in improving pragma generation compared to basic prompts.\n10 Chen et al.\n13.13%\n8.16%\n14.21%\n11.93%\n8.33% 7.39%\n1.15% 2.89%\n7.65%\n6.38%\n11.71%\n3.29% 2.46%\n5.55%\n1.53%1.02% 0.98% 0.86% 0.45% 0.27% 0.11% 0.01% 0.01% 0.00% 0.00% 0.04% 0.00% 0.01% 0.00% 0.00%\nFig. 5.Pragma Generation Accuracy. Blue: OMPGPT. Red: GPT-Neo.\nThis comparison will highlight Chain-of-OMP’s ability to address these syntactic\nchallenges.\nEvaluation Setup. We evaluate chain-of-OMP for four specific OpenMP clauses:\nfor schedule, collapse, teams, and target. These pragmas are well-aligned\nwith the three-component structure outlined earlier. We compare Chain-of-OMP’s\naccuracy with the base prompt to quantify its performance improvement.\nResults. Table 2 showcases the accuracy gains achieved by Chain-of-OMP.\nTable 2.Chain-of-OMP effectiveness evalu-\nation with OMPGPT.\nBasic Prompt Chain-of-OMP\nfor schedule 1.3% 7.8%\ncollapse 0.1% 2.4%\nteams 0.1% 0.3%\ntarget 2.9% 16.9%\nFor for schedule, Chain-of-OMP\nsignificantly improves accuracy from\n1.3% to 7.8%, representing a sub-\nstantial 600% increase. Similar im-\nprovements are observed forcollapse\n(0.1% to 2.4%, a 2300% increase)\nand target (2.9% to 16.9%, a 580%\nincrease). The smaller improvement\nfor teams (0.1% to 0.3%) is due to\nlimited training data for this specific\npragma.\n4.4 Fine-tuning OMPGPT for Specific Pragma Generation\nTask Definition. As discussed in Section 3.4, fine-tuning is necessary due to\nthe complexity of OpenMP pragmas. We fine-tuned OMPGPT for three sub-\ntasks: private, reduction, and SIMD. We evaluate its performance against two\nbaselines: MonoCoder (fine-tuned for pragma generation) and GPT-3.5 (not fine-\ntuned due to time/cost constraints). We also applied Chain-of-OMP with the\nfine-tuned OMPGPT for each sub-task, setting the number of chains Nchain to\n2 based on the evaluation clauses’ structure.\nEvaluation Setup. We used the test set from our fine-tuning dataset for each\nsub-task. We employed precision, recall, F1-score, and accuracy metrics. The\nevaluation consisted of two subtests:\n1. Subtest 1 (Clause Matching): This subtest checks if the predicted clause\nmatches the expected clause.\nOMPGPT: A Generative Pre-trained Transformer Model for OpenMP 11\n2. Subtest 2 (Stricter Matching): This stricter subtest requires both the pre-\ndicted clause and its control structure to match the expected ones.\nFor both GPT-3.5 and Monocoder, we used the same test set employed for\nOMPGPT to ensure a fair comparison. Results for MonoCoder are reported from\nits original paper since it uses the same dataset splits as OMPGPT. We note\nthat some entries in Tables 3 and 4 were unavailable for MonoCoder since they\nwere not reported in their paper.\nTable 3.Fine-tuning results for private, reduction, and SIMD on the first test (Subset\n1). P = Precision, R = Recall, F1 = F1 Score, Acc = Accuracy. Note Monocoder does\nnot support SIMD.\nModel private reduction SIMD\nP R F1 Acc P R F1 Acc P R F1 Acc\nOMPGPT 0.94 0.91 0.92 0.96 0.92 0.91 0.915 0.98 0.88 0.82 0.85 0.93\nMonoCoder 0.89 0.83 0.86 0.94 0.99 0.81 0.891 0.98 - - - -\nGPT-3.5 0.4 0.16 0.23 0.77 0.53 0.57 0.549 0.92 0.51 0.55 0.53 0.84\nResults. Tables 3 and 4 show the performance on the two subtests for pri-\nvate, reduction, and SIMD clauses (MonoCoder doesn’t support SIMD). Over-\nall, OMPGPT consistently outperforms both MonoCoder and GPT-3.5 models\nin the two subtests. As expected, the performance drops for all models in the\nsecond subtest (clause and control structure). However, notably, the Chain-of-\nOMP prompt improves the accuracy of OMPGPT in predicting both the clause\nand control structure correctly in the second subtest.\nTable 4. Fine-tuning results for private, reduction, and SIMD on the second test\n(Susbet 2). Note Monocoder does not support SIMD.\nModel private reduction SIMD\nP R F1 Acc P R F1 Acc P R F1 Acc\nOMPGPT 0.74 0.61 0.67 0.51 0.78 0.71 0.74 0.57 0.64 0.69 0.66 0.42\nChain-of-OMP 0.76 0.77 0.76 0.64 0.77 0.78 0.77 0.60 0.66 0.73 0.69 0.58\nMonoCoder - - - 0.48 - - - 0.52 - - - -\nGPT-3.5 0.23 0.34 0.27 0.14 0.71 0.55 0.62 0.51 0.61 0.55 0.58 0.40\nThe effectiveness of Chain-of-OMP is showcased by analyzing two selected\nOpenMP pragmas, as detailed in Section 4.3. Furthermore, we extended our\nevaluation to include a smaller set comprising ten samples of the top 15 most\ncommon pragmas in this test. This testing led to two significant observations:\n– Consistent Performance Enhancement: Chain-of-OMP consistently main-\ntained or improved performance across all test sets. In no instance did its\napplication lead to a performance decline.\n12 Chen et al.\n– Notable Improvement in Majority of Tasks: Impressively, Chain-of-OMP im-\nproved performance in 11 out of the 15 pragma generation tasks, demonstrat-\ning its substantial efficacy in enhancing OpenMP pragma generation across\nvarious tasks.\n5 Related Work\nRecent works have explored language models (LMs) for HPC tasks as described\nin [11]. Studies have fine-tuned existing code LLMs for tasks such as race de-\ntection [8] and generating parallel code like MPI routines [19]. The latter work\nalso explored the task of generating accurate OpenMP pragmas. In contrast, [12]\nformulated predicting OpenMP pragmas as a multi-label classification task.\nOne of the most recent studies utilizing LLMs for OpenMP pragma gen-\neration is MonoCoder [11]. This work fine-tunes a domain-specific language\nmodel on the HPCorpus dataset specifically for OpenMP pragma generation.\nOMPGPT stands out from MonoCoder due to its novel training approach. By\nstrategically repositioning OpenMP pragmas during training, OMPGPT aligns\nwith the left-to-right processing inherent to GPT models. This approach, along\nwith the proposed Chain-of-OMP prompting technique, leads to consistent per-\nformance improvements over MonoCoder.\nChain-of-thought prompting improves LM reasoning ability, showing promise\non math word problems, commonsense reasoning [25], and summarizing software\ncomponents [21]. This prompting strategy has also enhanced code generation\nfrom models like ChatGPT [14,16] and bug reproduction from reports [9]. Most\nrelevantly, it has been applied to correcting compilable code [3].\n6 Conclusion and Future Work\nWhile large language models have transformed natural language processing.\nthey struggle with domain-specific problems like HPC tasks. This work presents\nOMPGPT, a 0.76B domain-specific model trained for OpenMP pragma gener-\nation on HPC data. Our evaluation shows that OMPGPT outperforms larger\nLLMs like GPT-3.5. Moreover, our novel chain-of-OMP prompting technique as-\nsists OMPGPT in improving accuracy through step-by-step prompts. Our find-\nings suggest that smaller LLMs can achieve excellent performance on specific\ntasks with proper domain training and prompting techniques. This paves the\nway for more accessible and efficient LLMs.\nFuture work includes evaluating Chain-of-OMP on other OpenMP clauses,\nextending context windows beyond loop snippets, training LLMs from scratch\non our data with Chain-of-OMP prompting, and exploring this approach for\nother HPC tasks [1].\n7 Acknowledgement\nThis project was funded by NSF (#2211982) and Intel Labs. We would like to\nthank them for their generous support. Additionally, we extend our gratitude to\nOMPGPT: A Generative Pre-trained Transformer Model for OpenMP 13\nthe Research IT team 1 of Iowa State University for their continuous support in\nproviding access to HPC clusters for conducting the experiments of this project.\nReferences\n1. Bhattacharjee, A., Daley, C.S., Jannesari, A.: Openmp offload features and strate-\ngies for high performance across architectures and compilers. In: 2023 IEEE Inter-\nnational Parallel and Distributed Processing Symposium Workshops (IPDPSW).\npp. 564–573 (2023)\n2. Black, S., Gao, L., Wang, P., Leahy, C., Biderman, S.: Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow. If you use this software, please\ncite it using these metadata 58, 2 (2021)\n3. Chen, L., Bhattacharjee, A., Ahmed, N.K., Hasabnis, N., Oren, G., Lei, B., Jan-\nnesari, A.: CompCodeVet: A Compiler-guided Validation and Enhancement Ap-\nproach for Code Dataset. arXiv preprint arXiv:2311.06505 (2023)\n4. Chen, L., Lin, P.H., Vanderbruggen, T., Liao, C., Emani, M., de Supinski, B.:\nLm4hpc: Towards effective language model application in high-performance com-\nputing. In: International Workshop on OpenMP. pp. 18–33. Springer (2023)\n5. Chen, L., Mahmud, Q.I., Jannesari, A.: Multi-view learning for parallelism discov-\nery of sequential programs. In: 2022 IEEE International Parallel and Distributed\nProcessing Symposium Workshops (IPDPSW). pp. 295–303. IEEE (2022)\n6. Chen, L., Mahmud, Q.I., Phan, H., Ahmed, N., Jannesari, A.: Learning to paral-\nlelize with openmp by augmented heterogeneous ast representation. Proceedings\nof Machine Learning and Systems 5 (2023)\n7. Dagum, L., Menon, R.: Openmp: an industry standard api for shared-memory\nprogramming. IEEE Computational Science and Engineering 5(1), 46–55 (1998)\n8. Ding, X., Chen, L., Emani, M., Liao, C., Lin, P.H., Vanderbruggen, T., Xie, Z.,\nCerpa, A., Du, W.: Hpc-gpt: Integrating large language model for high-performance\ncomputing. In: Proceedings of the SC’23 Workshops of The International Confer-\nence on High Performance Computing, Network, Storage, and Analysis. pp. 951–\n960 (2023)\n9. Feng, S., Chen, C.: Prompting Is All You Need: Automated Android Bug Replay\nwith Large Language Models. In: Proceedings of the 46th IEEE/ACM International\nConference on Software Engineering. pp. 1–13 (2024)\n10. Kadosh, T., Hasabnis, N., Mattson, T., Pinter, Y., Oren, G.: Quantifying openmp:\nStatistical insights into usage and adoption. In: 2023 IEEE High Performance Ex-\ntreme Computing Conference (HPEC). pp. 1–7. IEEE (2023)\n11. Kadosh, T., Hasabnis, N., Vo, V.A., Schneider, N., Krien, N., Capota, M., Wasay,\nA., Ahmed, N., Willke, T., Tamir, G., et al.: Domain-specific code language models:\nUnraveling the potential for hpc codes and tasks. arXiv preprint arXiv:2312.13322\n(2023)\n12. Kadosh, T., Schneider, N., Hasabnis, N., Mattson, T., Pinter, Y., Oren, G.: Ad-\nvising OpenMP Parallelization via A Graph-Based Approach with Transformers.\nIn: International Workshop on OpenMP. pp. 3–17. Springer (2023)\n13. Kocetkov, D., et al.: The stack: 3 tb of permissively licensed source code. Preprint\n(2022)\n14. Li, J., Li, G., Li, Y., Jin, Z.: Structured chain-of-thought prompting for code gen-\neration. arXiv preprint arXiv:2305.06599 (2023)\n1 https://researchit.las.iastate.edu/\n14 Chen et al.\n15. Li, R., Allal, L.B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M.,\nAkiki, C., Li, J., Chim, J., et al.: Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161 (2023)\n16. Liu, C., Bao, X., Zhang, H., Zhang, N., Hu, H., Zhang, X., Yan, M.: Improving\nchatgpt prompt for code generation. arXiv preprint arXiv:2305.08360 (2023)\n17. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017)\n18. Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., Jiang,\nD.: Wizardcoder: Empowering code large language models with evol-instruct. arXiv\npreprint arXiv:2306.08568 (2023)\n19. Nichols, D., Marathe, A., Menon, H., Gamblin, T., Bhatele, A.: Modeling parallel\nprograms using large language models. arXiv preprint arXiv:2306.17281 (2023)\n20. Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.E., Adi, Y., Liu,\nJ., Remez, T., Rapin, J., et al.: Code llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950 (2023)\n21. Rukmono, S.A., Ochoa, L., Chaudron, M.R.: Achieving high-level software compo-\nnent summarization via hierarchical chain-of-thought prompting and static code\nanalysis. In: 2023 IEEE International Conference on Data and Software Engineer-\ning (ICoDSE). pp. 7–12. IEEE (2023)\n22. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n23. Valero-Lara, P., Huante, A., Lail, M.A., Godoy, W.F., Teranishi, K., Balaprakash,\nP., Vetter, J.S.: Comparing llama-2 and gpt-3 llms for hpc kernels generation. arXiv\npreprint arXiv:2309.07103 (2023)\n24. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n25. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems 35, 24824–24837 (2022)\n26. Xu, F.F., Alon, U., Neubig, G., Hellendoorn, V.J.: A systematic evaluation of large\nlanguage models of code. In: Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming. pp. 1–10 (2022)"
}