{
  "title": "Exploiting prosodic breaks in language modeling with random forests",
  "url": "https://openalex.org/W359161088",
  "year": 2008,
  "authors": [
    {
      "id": "https://openalex.org/A5057885012",
      "name": "Yi Su",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5110017489",
      "name": "Frederick Jelinek",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W61566120",
    "https://openalex.org/W94155879",
    "https://openalex.org/W1971500911",
    "https://openalex.org/W1493946344",
    "https://openalex.org/W1582730572",
    "https://openalex.org/W1597422500",
    "https://openalex.org/W6606697926",
    "https://openalex.org/W6607533129",
    "https://openalex.org/W180516261",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W133973857",
    "https://openalex.org/W2169998051",
    "https://openalex.org/W1585666902",
    "https://openalex.org/W2086699924",
    "https://openalex.org/W6683103995",
    "https://openalex.org/W2056250865",
    "https://openalex.org/W2110372834",
    "https://openalex.org/W2099345940",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W162654330",
    "https://openalex.org/W2155693943",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W187516025",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W272647413",
    "https://openalex.org/W1600591519",
    "https://openalex.org/W2114858359",
    "https://openalex.org/W2165487751",
    "https://openalex.org/W4234159136",
    "https://openalex.org/W180685247"
  ],
  "abstract": "We propose a novel method of exploiting prosodic breaks in language modeling for automatic speech recognition (ASR) based on the random forest language model (RFLM), which is a collection of randomized decision tree language models and can potentially ask any questions about the history in order to predict the future.We demonstrate how questions about prosodic breaks can be easily incorporated into the RFLM and present two language models which treat prosodic breaks as observable and hidden variables, respectively.Meanwhile, we show empirically that a finer grained prosodic break is needed for language modeling.Experimental results showed that given prosodic breaks, we were able to reduce the LM perplexity by a significant margin, suggesting a prosodic N -best rescoring approach for ASR.",
  "full_text": "Exploiting Prosodic Breaks in Language Modeling with Random Forests\nYi Su and Frederick Jelinek\nCenter for Language and Speech Processing\nDepartment of Electrical and Computer Engineering\nThe Johns Hopkins University, Baltimore, Maryland, USA\n{suy; jelinek}@jhu.edu\nAbstract\nWe propose a novel method of exploiting prosodic breaks in lan-\nguage modeling for automatic speech recognition (ASR) based\non the random forest language model (RFLM), which is a col-\nlection of randomized decision tree language models and can\npotentially ask any questions about the history in order to pre-\ndict the future. We demonstrate how questions about prosodic\nbreaks can be easily incorporated into the RFLM and present\ntwo language models which treat prosodic breaks as observ-\nable and hidden variables, respectively. Meanwhile, we show\nempirically that a ﬁner grained prosodic break is needed for\nlanguage modeling. Experimental results showed that given\nprosodic breaks, we were able to reduce the LM perplexity by a\nsigniﬁcant margin, suggesting a prosodicN-best rescoring ap-\nproach for ASR.\n1. Introduction\nProsody refers to a wide range of suprasegmental properties\nof spoken language units, including tone, intonation, rhythm,\nstress and so on. It has been used for a number of spoken lan-\nguage processing tasks, such as disﬂuency and sentence bound-\nary detection [1], topic segmentation [2], spoken language pars-\ning [3], among others. We are mainly interested in using\nprosody to improve automatic speech recognition (ASR). As\na separate knowledge source, prosody has been helpful in all\nthree major components of a modern ASR system: the acous-\ntic model [4, 5], the pronunciation model [6] and the language\nmodel [7, 8]. (For a comprehensive review of prosody models\nin ASR, see [9].) New opportunities of using prosody emerged\nafter the availability of a prosodically labeled speech corpus\n[10], where tones and breaks were hand-labeled with a subset of\nToBI labeling scheme [11]. In this work, we focus on prosodic\nbreaks.\nThe random forest language model (RFLM) [12] is a pow-\nerful model which consistently outperforms then-gram lan-\nguage model in terms of both perplexity and word error rate\nin several state-of-the-art ASR systems [13, 14]. Based on de-\ncision trees, the RFLM has the potential of integrating informa-\ntion from various sources besides the history words by simply\nasking new questions, analogous to the maximum entropy lan-\nguage model by using new features [15, 16]. We propose two\nprosodic language models based on the RFLM and demonstrate\ntheir performance in perplexity by contrasting them to a base-\nlinen-gram language model using the same information.\nThe rest of the paper is organized as follows: in Section 2\nwe present our proposed models. In Section 3, we brieﬂy re-\nview the RFLM. Experimental setup and results are presented\nin Section 4. Discussion of future work appears in Section 5\nand conclusions in Section 6.\n2. Prosodic Language Models\n2.1. Granularity of Prosodic Breaks\nThe ToBI-labeled speech corpus [10] makes it possible to inves-\ntigate the use of prosodic breaks in two aspects: automatic de-\ntection/classiﬁcation and statistical modeling. Although some\nresearchers argued against this intermediate phonological layer\n[17], we believe that 1) supervised training of prosodic classi-\nﬁers can help us understand the usefulness of various proposed\nprosodic features; 2) symbolic prosodic breaks are easier to ﬁt\ninto the currentn-gram based language modeling approach than\ncontinuous-valued prosodic features. As an example of the sec-\nond point, in direct modeling of prosodic features like the pause\nlength in [18], simple quantization like binning was used.\nA decision tree classiﬁer was built to predict three types of\nbreaks, namely,1,4 and p, with an accuracy of83.12% in [3]\nfor parsing speech. In [19], the1 and p labels were further col-\nlapsed into one. While this granularity of prosodic breaks has\nbeen suitable for their tasks, we believe a ﬁner granularity is\nneeded for language modeling. So we used the quantized pos-\nterior probabilityP(1|features), which has12 possible values,\nfrom the decision tree classiﬁer of [3] in our experiments. (See\nSection 4.2 for details.)\n2.2. Language Models with Prosodic Breaks\nLetW, S= w0s0w1s1w2s2 · · · wmsm be a sequence of words\nand prosodic breaks in their temporal order, whereW =\nw0w1w2 · · · wm is the sentence of length(m + 1) and S =\ns0s1s0 · · · sm is the sequence of prosodic breaks for the sen-\ntenceW, wheresi denotes the break betweenwi and wi+1, for\nall0 ≤ i < m.\nFirst we would like to estimate the joint probability\nP(W, S) as ann-gram LM of the(word, break)-tuples:\nP(W, S) =\nmY\ni=0\nP(wi, si|wi−1\n0 , si−1\n0 )\n≈\nmY\ni=0\nP(wi, si|wi−1\ni−n+1, si−1\ni−n+1). (1)\nThis joint model is immediately usable if our goal of ASR\nis the simultaneous recognition of words and prosodic breaks\n[20, 5]:\n(W, S)∗ = arg max\nW,S\nP(W, S|A)\n= arg max\nW,S\nP(A|W, S)P(W, S)\n≈ arg max\nW,S\nP(A|W)P(W, S), (2)\nSpeech Prosody 2008\nCampinas, Brazil\nMay 6-9, 2008\nISCA Archive\nhttp://www.isca-speech.org/archive\n91Speech Prosody 2008, Campinas, Brazil\n10.21437/SpeechProsody.2008-21\nwhere A stands for the acoustic features and we approximate\nP(A|W, S) with a usual acoustic modelP(A|W) for the sake\nof simplicity1.\nIf we stick to the original formulation of ASR, we can esti-\nmate the language modelP(W) as follows:\nP(W) =\nX\nS\nP(W, S)\n=\nX\nS\nmY\ni=0\nP(wi, si|wi−1\ni−n+1, si−1\ni−n+1). (3)\nThis computation can be carried out efﬁciently by a simple for-\nward pass of the forward-backward algorithm [21].\nFor either (1) or (3), we need to compute the probabil-\nityP(wi, si|wi−1\ni−n+1, si−1\ni−n+1). We propose the following two\nmethods:\n• Letti = ( wi, si), for all0 ≤ i ≤ m. We have\nP(wi, si|wi−1\ni−n+1, si−1\ni−n+1) = P(ti|ti−1\ni−n+1). (4)\nThen we can build ann-gram LM or RFLM of ti’s,\nwhose vocabulary is the Cartesian product of the word\nvocabulary and the prosodic break vocabulary.\n• Alternatively, we can decompose the probability as fol-\nlows:\nP(wi, si|wi−1\ni−n+1, si−1\ni−n+1)\n= P(wi|wi−1\ni−n+1, si−1\ni−n+1)\n·P(si|wi\ni−n+1, si−1\ni−n+1). (5)\nThen we can build twon-gram LMs or RFLMs for pre-\ndicting the word and the break, respectively.\nIn the second method, however, when a history consists of\nboth words and prosodic breaks, there isn’t a natural order of\nbacking off. Previous work either chose it heuristically (e.g.,\n[22, 23]) or tried to ﬁnd the optimal back-off path or combina-\ntion of paths [24, 25]. We propose to handle this problem grace-\nfully with the RFLM, which we will describe in the following\nsection.\n3. Random Forest Language Models\nA RFLM [12] is a collection of randomized decision tree lan-\nguage models (DTLMs) [26], which deﬁne equivalence clas-\nsiﬁcation of histories. The RFLM generalizes the DTLM by\naveraging multiple DTLMs, which, in turn, generalizes then-\ngram LM by having a sophisticated equivalence classiﬁcation.\nThe LM probability of a RFLM is deﬁned as follows:\nPRF (w|h) = 1\nM\nMX\nj=1\nPDTj (w|h)\n= 1\nM\nMX\nj=1\nP(w|ΦDTj (h)), (6)\nwhere h is the history andΦDTj (·) is a decision tree. The ques-\ntions that have been used so far care only about the identity of\nthe words in a history position. Ifwi is the word we want to\npredict, then the question takes the following form:\n1Another way to justify this approximation is that in this paper,\nwe only consider breaks, among many other prosodic features, and\nprosodic breaks have a relatively weak inﬂuence on the acoustics.\nIs the wordwi−k, k≥ 1 in a set of wordsS?\nBecause in the normaln-gram situation we know no more than\nthe words in the history, these questions are almost all we can\nask.\nNow if we have more information about the history, we can\neasily enlarge our inventory to include questions of the follow-\ning form:\nDoes the featuref about the history take its value\nin a set of valuesS?\nAs long as the feature values are categorical, we can use the\nsame decision tree building algorithm as before. This makes the\nRFLM an ideal model framework for integrating information\nfrom various sources. For example, if we are given the prosodic\nbreaks between words in the history, we can ask questions like:\nDoes the prosodic breaksi−1 take its value in the\nset of values{0.7, 0.8, 0.9}?\nNote that from the decision tree’s point of view,wi−k is just\nanother feature which happens to take its value in the vocabu-\nlary. Only when it is informative for the prediction do we want\nto ask questions about it. As numerous LMs, like then-gram\nLM or the maximum entropy LM, have proven, the immediately\nprevious wordwi−1 is the single most important/informative\nand the most easily obtainable feature, followed by, probably,\nthe previous of previous wordwi−2.\n4. Experiments\n4.1. Data and Setup\nWe used the ToBI-labeled Switchboard data from [10]. Fol-\nlowing [3], we divided our data into training (665, 790 words),\ndevelopment (51, 326 words) and evaluation (49, 494 words,\n55, 529 counting the end of sentence symbols). Due to the rela-\ntively small size of the corpus, our LMs would only consider up\nto two words and two breaks in the history, if not speciﬁed oth-\nerwise. We built100 trees for each RFLM and the smoothing\nmethod for both regularn-gram LMs and RFLMs was always\nthe modiﬁed Kneser-Ney [27]. The vocabulary size was10k.\n4.2. Granularity of Prosodic Breaks\nThe decision tree classiﬁer in [3] provided three degrees of\ngranularity: two-level (break or not), three-level (ToBI in-\ndices1, 4 and p) and continuous-valued (quantized into 12\nvalues,0.0, 0.1, . . . ,1.0 and −1.0). We built three RFLMs\nforP(wi|wi−1, wi−2, si−1, si−2), where the breakssi−1 and\nsi−2 took values of different granularity. The baseline was the\nword trigram LM,P(wi|wi−1, wi−2), with modiﬁed Kneser-\nNey smoothing.\nTable 1:Granularity of Prosodic Breaks\nModel\ntwo-level\nthree-level\ncont.-valued\nKN.3gm\n66.1\n66.1\n66.1\nRF-100\n65.5\n65.4\n56.2\nFrom Table 1, we concluded that the ToBI indices were\nnot ﬁne-grained enough for the purpose of language modeling.\nHenceforth our experiments used the continuous-valued breaks.\n92Speech Prosody 2008, Campinas, Brazil\n4.3. Feature Selection by RFLM\nAs we mentioned before, from a RFLM’s point of view, the var-\nious variables in the history,wi’s orsi’s, are just features. The\nmodel chooses any one of them simply because it has strong\ncorrelation, or large mutual information, with the future word.\nSo by asking the RFLMnotto use one of the variables in the\nhistory, we can ﬁnd out how valuable that feature is. This kind\nof feature engineering was also used in maximum entropy LMs\n[15, 16].\nWe built RFLMs forP(wi|wi−1, wi−2, si−1, si−2) then\nmasked out one of the features in order to see how much it con-\ntributed.\nTable 2:Feature Selection by RFLM\nHistory\nPerplexity\nwi−1,wi−2,si−1,si−2\n56.2\nwi−1,wi−2,si−1\n55.9\nwi−1,wi−2,si−2\n63.9\nwi−1,wi−2\n62.3\nAs we had expected, Table 2 showed that the break be-\ntween the immediately previous word and the future word,si−1,\nhelped the prediction, while the break between the previous and\nthe previous of the previous,si−2, did not. Adding the lat-\nter actually hurt the perplexity a little bit, although that might\nchange if we had more data. Similar experiments can be done\nforP(si|wi, wi−1, wi−2, si−1, si−2). We skipped the detail\nbut the conclusion was that the most useful features for predict-\ning a break were its previous two words,wi and wi−1, which\nwas consistent with our intuition.\nWe also point out here that this kind of experiments would\nnot have been so easy to carry out in the case of regularn-\ngram LMs with modiﬁed Kneser-Ney smoothing. You have to\nspecify the back-off order and search the best value for some of\nthe discount parameters.\n4.4. Main Perplexity Results\nHaving selected the features, we put the two components to-\ngether following (5) to getP(wi, si|wi−1, wi−2, si−1, si−2)\nand called it the “decomp.” (decomposition) method in Ta-\nble 3. For comparison, we also followed (4) to get the same\nquantity with a trigram LM of(word, break)-tuples and called\nit the “tuple 3gm” method in Table 3. For each method, we con-\ntrasted the modiﬁed Kneser-Ney-smoothedn-gram LM (“KN”\ncolumn) with the RFLM (“RF” column).\nTable 3:Main Perplexity Results\nModel\nMethod\nKN\nRF\nP(W, S)\ntuple 3gm\n358\n306\ndecomp.\n274\n251\nP(W)\ntuple 3gm\n69.3\n67.2\n= P\nS P(W, S)\ndecomp.\n66.8\n64.2\nP(W)\nword 3gm\n66.1\n62.3\nAs shown in Table 3, the best perplexity resulted from\nthe decomposition method using the RFLM in both the model\nP(W, S), where the prosodic breaks were given, and the model\nP(W) = P\nS P(W, S), where the prosodic breaks were hid-\nden.\nIf we knew nothing about the prosodic breaks, we could\nstill build a trigram LM with the modiﬁed Kneser-Ney smooth-\ning or the RF. We called it the “word 3gm” method and put the\nperplexity results in the last row of Table 3. We observed that al-\nthough our best number for the modelP(W) = P\nS P(W, S)\nwas better than a modiﬁed Kneser-Ney-smoothed trigram LM,\nit was outperformed by the basic RFLM, as shown in the bot-\ntom right corner of Table 3. The reason was that in the model\nP(W) = P\nS P(W, S), we were trying to predict a prosodic\nbreak from its proceeding words and breaks, which correlated\npoorly with it, instead of from its corresponding acoustic fea-\ntures.\nTherefore we concluded that given prosodic breaks, we\ncould successfully reduce the LM perplexity by a signiﬁcant\nmargin with the RFLM and the decomposition formula (5).\n5. Discussion\nGiven that we could build a good LM when the prosodic breaks\nwere provided (Table 2) but could not when they were not (Ta-\nble 3 modelP(W) = P\nS P(W, S)), it is clear that we should\nget the prosodic breaks from the acoustics, instead of predict-\ning them from words. In fact, the prediction of prosodic breaks\nfrom words was so bad that it killed the gain we had from using\nthem to improve the word prediction. Therefore we propose the\nfollowing procedure of using prosodic breaks in an ASR sys-\ntem:\n• Generate anN-best list of hypotheses from a standard\nASR system;\n• For each hypothesis, align the words with the acoustics\nusing the Viterbi algorithm; ﬁnd out the regions between\nwords and predict their prosodic breaks from the acoustic\nfeatures using a prosody classiﬁer;\n• Rescore the N-best list with the modelQ\ni P(wi|wi−1\ni−n+1, si−1\ni−n+1).\nNote that the modelQ\ni P(wi|wi−1\ni−n+1, si−1\ni−n+1) is not a “pure”\nLM anymore since thesi’s come from the acoustics. However,\nbecause the acoustic features used to predict the breaks are dif-\nferent from those used to predict the words, we expect the new\ninformation would help choose a better hypothesis through the\nprosodically-informed LM.\n6. Conclusions\nWe have presented our method that uses the RFLM to build\nLMs strengthened by prosodic break information. We showed\nthat the ToBI break indices were not ﬁne-grained enough for\nthe task of language modeling. Using quantized posterior prob-\nabilities from a decision tree classiﬁer as ﬁne-grained prosodic\nbreaks, we could reduce the perplexity by a signiﬁcant margin.\nWe also demonstrated that the RFLM was an ideal framework\nfor incorporating various information like prosodic breaks into\nthe existing LM in a principled way.\n7. Acknowledgments\nWe would like to thank Zak Shafran, Markus Dreyer and the\nwhole CLSP Workshop’05 PSSED team for preparing and shar-\ning the data.\n8. References\n[1] A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf,\nD. Hakkani, M. Plauche, G. T¨ur, and Y . Lu, “Automatic\n93Speech Prosody 2008, Campinas, Brazil\ndetection of sentence boundaries and disﬂuencies based\non recognized words,” inProceedings of ICSLP-1998,\nvol. 5, 1998, pp. 2247–2250.\n[2] J. Hirschberg and C. H. Nakatani, “Acoustic indicators\nof topic segmentation,” inProceedings of ICSLP-1998,\n1998.\n[3] J. Hale, I. Shafran, L. Yung, B. Dorr, M. Harper, A. Kras-\nnyanskaya, M. Lease, Y . Liu, B. Roark, M. Snover, and\nR. Stewart, “PCFGs with syntactic and prosodic indica-\ntors of speech repairs with syntactic and prosodic indi-\ncators of speech repairsctic and prosodic indicators of\nspeech repairs,” inProceedings of the joint conference of\nthe International Committee on Computational Linguis-\ntics and the Association for Computational Linguistics\n(COLING/ACL) , 2006, pp. 161–168.\n[4] I. Shafran and M. Ostendorf, “Acoustic model clustering\nbased on syllable structure,”Computer Speech and Lan-\nguage, vol. 17, no. 4, pp. 311–328, 2003.\n[5] K. Chen, S. Borys, M. Hasegawa-Johnson, and J. Cole,\n“Prosody dependent speech recognition with explicit du-\nration modelling at intonatinal phrase boundaries,” inPro-\nceedings of INTERSPEECH-2003, 2003, pp. 393–396.\n[6] J. E. Fosler-Lussier, “Dynamic pronunciation models for\nautomatic speech recognition,” Ph.D. dissertation, Univer-\nsity of California, Berkeley, CA, USA, 1999.\n[7] A. Stolcke, E. Shriberg, D. Hakkani-T¨ur, and G. T¨ur,\n“Modeling the prosody of hidden events for improved\nword recognition,” inProceedings of Eurospeech-1999,\n1999.\n[8] K. Hirose, N. Minematsu, and M. Terao, “Statistical lan-\ngage modeling with prosodic boundaries and its use for\ncontinuous speech recognition,” inProceedings of ICSLP-\n2002, 2002.\n[9] M. Ostendorf, I. Shafran, and R. Bates, “Prosody models\nfor conversational speech recognition,” inProceedings of\nthe 2nd Plenary Meeting and Symposium on Prosody and\nSpeech Processing, 2003, pp. 147–154.\n[10] M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,\nL. Carmichael, and W. Byrne, “A prosodically la-\nbeled database of spontaneous speech,” inProceedings\nof ISCA Tutorial and Research Workshop on Prosody\nin Speech Recognition and Understanding, 2001, pp.\n119–121.\n[11] K. Silverman, M. Beckman, J. Pitrelli, M. Osten-\ndorf, C. Wightman, P. Price, J. Pierrehumbert, and\nJ. Hirschberg, “ToBI: A standard for labeling english\nprosody,” inProceedings of ICSLP-1992, 1992, pp. 867–\n870.\n[12] P. Xu and F. Jelinek, “Random forests in language model-\ning,” inProceedings of EMNLP 2004, D. Lin and D. Wu,\nEds. Barcelona, Spain: Association for Computational\nLinguistics, 2004, pp. 325–332.\n[13] P. Xu and L. Mangu, “Using random forest language mod-\nels in the IBM RT-04 CTS system,” inProceedings of\nINTERSPEECH-2005 , 2005, pp. 741–744.\n[14] Y . Su, F. Jelinek, and S. Khudanpur, “Large-scale random\nforest language models for speech recognition,” inPro-\nceedings of INTERSPEECH-2007, vol. 1, 2007, pp. 598–\n601.\n[15] R. Rosenfeld, “A maximum entropy approach to adap-\ntive statistical language modelling,”Computer Speech and\nLanguage, vol. 10, pp. 187–228, 1996.\n[16] S. Khudanpur and J. Wu, “Maximum entropy techniques\nfor exploiting syntactic, semantic and collocational de-\npendencies in lanugage modeling,”Computer Speech and\nLanguage, vol. 14, no. 4, pp. 355–372, 2000.\n[17] E. Shriberg and A. Stolcke, “Prosody modeling for au-\ntomatic speech understanding: An overview of recent re-\nsearch at sri,” inProceedings of ISCA Workshop on Speech\nRecognition and Understanding, 2001, pp. 13–16.\n[18] D. Vergyri, A. Stolcke, V . R. R. Gadde, L. Ferrer, and\nE. Shriberg, “Prosodic knowledge sources for automatic\nspeech recognition,” inProceedings of ICASSP-2003,\n2003.\n[19] M. Dreyer and I. Shafran, “Exploiting prosody for\nPCFGs with latent annotations,” inProceedings of\nINTERSPEECH-2007 , 2007.\n[20] P. A. Heeman and J. F. Allen, “Speech repairs, intona-\ntional phrases, and discourse markers: Modeling speak-\ners’ utterances in spoken dialogue,”Computational Lin-\nguistics, vol. 25, no. 4, pp. 527–571, 1999.\n[21] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, “A maxi-\nmization technique occurring in the statistical analysis of\nprobabilistic functions of markov chains,”The Annals of\nMathematical Statistics, vol. 41, no. 1, pp. 164–171, 1970.\n[22] C. Chelba and F. Jelinek, “Structured language modeling,”\nComputer Speech and Language, vol. 14, no. 4, pp. 283–\n332, 2000.\n[23] E. Charniak, “Immediate-head parsing for language mod-\nels,” inProceedings of the 39th Annual Meeting on Asso-\nciation for Computational Linguistics. Morristown, NJ,\nUSA: Association for Computational Linguistics, 2001,\npp. 124–131.\n[24] J. A. Bilmes and K. Kirchhoff, “Factored language mod-\nels and generalized parallel backoff,” inProceedings of\nHLT/NAACL 2003 , 2003, pp. 4–6.\n[25] K. Duh and K. Kirchhoff, “Automatic learning of lan-\nguage model structure,” inProceedings of COLING-2004,\n2004.\n[26] L. R. Bahl, P. F. Brown, P. V . de Souza, and R. L. Mercer,\n“A tree-based statistical language model for natural lan-\nguage speech recognition,”IEEE Transactions on Acous-\ntics, Speech, and Signal Processing, vol. 37, no. 7, pp.\n1001–1008, 1989.\n[27] S. F. Chen and J. Goodman, “An empirical study of\nsmoothing techniques for language modeling,”Computer\nSpeech and Language, vol. 13, pp. 359–394, 1999.\n94Speech Prosody 2008, Campinas, Brazil",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9768457412719727
    },
    {
      "name": "Computer science",
      "score": 0.8150509595870972
    },
    {
      "name": "Language model",
      "score": 0.7737595438957214
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6443855166435242
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5588515996932983
    },
    {
      "name": "Random forest",
      "score": 0.5401964783668518
    },
    {
      "name": "Natural language processing",
      "score": 0.5050068497657776
    },
    {
      "name": "Spoken language",
      "score": 0.5012140274047852
    },
    {
      "name": "Decision tree",
      "score": 0.4696803689002991
    },
    {
      "name": "Speech recognition",
      "score": 0.4300033748149872
    },
    {
      "name": "Identification (biology)",
      "score": 0.41741085052490234
    },
    {
      "name": "Machine learning",
      "score": 0.21209126710891724
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}