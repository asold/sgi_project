{
  "title": "Relative molecule self-attention transformer",
  "url": "https://openalex.org/W3207161423",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4286983793",
      "name": "Maziarka, Łukasz",
      "affiliations": [
        "Jagiellonian University"
      ]
    },
    {
      "id": "https://openalex.org/A4286983794",
      "name": "Majchrowski, Dawid",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4286983795",
      "name": "Danel, Tomasz",
      "affiliations": [
        "Jagiellonian University"
      ]
    },
    {
      "id": "https://openalex.org/A4286983796",
      "name": "Gaiński, Piotr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226501789",
      "name": "Tabor, Jacek",
      "affiliations": [
        "Jagiellonian University"
      ]
    },
    {
      "id": "https://openalex.org/A4286983798",
      "name": "Podolak, Igor",
      "affiliations": [
        "Jagiellonian University"
      ]
    },
    {
      "id": "https://openalex.org/A4286983799",
      "name": "Morkisz, Paweł",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4222994269",
      "name": "Jastrzębski, Stanisław",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962398326",
    "https://openalex.org/W3113150977",
    "https://openalex.org/W2766761250",
    "https://openalex.org/W3110901318",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4362664882",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W6725181839",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W3120715532",
    "https://openalex.org/W2947427925",
    "https://openalex.org/W2806547269",
    "https://openalex.org/W6628094813",
    "https://openalex.org/W4319762935",
    "https://openalex.org/W4242308659",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4285600549",
    "https://openalex.org/W4382239609",
    "https://openalex.org/W2735246657",
    "https://openalex.org/W3199234580",
    "https://openalex.org/W4283389995",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W4283794957",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W3109892317",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3100157108",
    "https://openalex.org/W3131204112",
    "https://openalex.org/W3118691860",
    "https://openalex.org/W2787998955",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963711743",
    "https://openalex.org/W2785720803",
    "https://openalex.org/W3073845700",
    "https://openalex.org/W3035684957",
    "https://openalex.org/W3112760268",
    "https://openalex.org/W2788775653",
    "https://openalex.org/W2977944219",
    "https://openalex.org/W2996443485",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2279490987",
    "https://openalex.org/W3116202926",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W3036737467",
    "https://openalex.org/W2976093982",
    "https://openalex.org/W2996604169",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W3101620381",
    "https://openalex.org/W2135732933",
    "https://openalex.org/W2948990653",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W3207161423",
    "https://openalex.org/W2096541451"
  ],
  "abstract": null,
  "full_text": "Maziarka et al. Journal of Cheminformatics            (2024) 16:3  \nhttps://doi.org/10.1186/s13321-023-00789-7\nRESEARCH Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nJournal of Cheminformatics\nRelative molecule self-attention transformer\nŁukasz Maziarka1*†, Dawid Majchrowski2, Tomasz Danel1†, Piotr Gaiński1,3, Jacek Tabor1, Igor Podolak1, \nPaweł Morkisz2 and Stanisław Jastrzębski4 \nAbstract \nThe prediction of molecular properties is a crucial aspect in drug discovery that can save a lot of money and time \nduring the drug design process. The use of machine learning methods to predict molecular properties has become \nincreasingly popular in recent years. Despite advancements in the field, several challenges remain that need to be \naddressed, like finding an optimal pre-training procedure to improve performance on small datasets, which are \ncommon in drug discovery. In our paper, we tackle these problems by introducing Relative Molecule Self-Attention \nTransformer for molecular representation learning. It is a novel architecture that uses relative self-attention and 3D \nmolecular representation to capture the interactions between atoms and bonds that enrich the backbone model \nwith domain-specific inductive biases. Furthermore, our two-step pretraining procedure allows us to tune only a few \nhyperparameter values to achieve good performance comparable with state-of-the-art models on a wide selection \nof downstream tasks.\nScientific contribution \nA novel graph transformer architecture for molecular property prediction is introduced. The task-agnostic method-\nology for pre-training this model is presented, improving target task performance with minimal hyperparameter \ntuning. A rigorous exploration of the design space for the self-attention layer is conducted to identify the optimal \narchitecture.\nKeywords Molecular property prediction, Molecular self-attention, Neural networks pre-training\nIntroduction\nPredicting molecular properties is central to applications \nsuch as drug discovery or material design. Without accu -\nrate prediction of properties such as toxicity, a  promis -\ning drug candidate is likely to fail clinical trials. Many \nmolecular properties cannot be feasibly computed (simu-\nlated) from first principles as their complexity scales with \nat least the 4th power of the number of atoms. It makes \ncomputation infeasible for even moderately large sys -\ntems. Moreover, complex molecular properties, such as \npredicting the yield of chemical reactions, are still beyond \nthe reach of what is typically referred to as computational \nchemistry methods [1]. Instead, these properties have to \nbe extrapolated from an often small experimental data -\nset [ 2, 3]. The prevailing approach is to train a machine \nlearning model such a random forest [4] or a graph neu -\nral network [5] from scratch to predict the desired prop -\nerty for a new molecule [6].\nMachine learning is moving away from training mod -\nels purely from scratch. In natural language processing \n(NLP), advances in large-scale pretraining [7, 8] and the \n†Łukasz Maziarka and Tomasz Danel have done this research in part while \nworking at Ardigen.\n*Correspondence:\nŁukasz Maziarka\nlukasz.maziarka@ii.uj.edu.pl\n1 Faculty of Mathematics and Computer Science, Jagiellonian University, \nŁojasiewicza 6, 30-348 Cracow, Poland\n2 NVIDIA, 2788 San Tomas Expy, Santa Clara, CA 95051, USA\n3 Ardigen, Podole 76, 30-394 Cracow, Poland\n4 Molecule.one, Al. Jerozolimskie 96, 00-807 Warsaw, Poland\nPage 2 of 14Maziarka et al. Journal of Cheminformatics            (2024) 16:3 \ndevelopment of the Transformer [9] architecture have \nculminated in large gains in data efficiency across mul -\ntiple tasks because pretrained models usually need less \ndata to produce similar results as models trained from \nscratch [10]. Instead of training models purely from \nscratch, the models in NLP are commonly first pre -\ntrained on large unsupervised corpora. The chemistry \ndomain might be on the brink of an analogous revolu -\ntion, which could be especially transformative due to \nthe high cost of obtaining large experimental datasets. \nIn particular, recent work has proposed Molecule Atten -\ntion Transformer (MAT), a Transformer-based architec -\nture adapted to processing molecular data [11, 12] and \npretrained using self-supervised learning for graphs [13]. \nSeveral works have shown further gains by improving \nnetwork architecture or the pretraining tasks [14–16].\nHowever, pretraining has not yet led to such transform-\native data-efficiency gains in molecular property predic -\ntion. For instance, non-pretrained models with extensive \nhandcrafted featurization tend to achieve very competi -\ntive results [17]. We reason that architecture might be a \nkey bottleneck. In particular, most Transformers for mol-\necules do not encode the three-dimensional structure of \nthe molecule [14, 16], which is a key factor determining \nmany molecular properties. On the other hand, perfor -\nmance has been significantly boosted in other domains \nby enriching the Transformer architecture with proper \ninductive biases [18– 27]. Motivated by this perspec -\ntive, we methodologically explore the design space of the \nself-attention layer, a key computational primitive of the \nTransformer architecture, for molecular property pre -\ndiction. In particular, we explore variants of relative self-\nattention, which has been shown to be effective in various \ndomains such as protein design and NLP [19, 21].\nOur main contribution is a new self-attention layer for \nmolecular graphs. We tackle the aforementioned issues \nwith Relative Molecule Self-Attention Transformer \n(R-MAT), our pre-trained transformer-based model, \nshown in Fig.  1. We propose Relative Molecule Self-\nAttention, a novel variant of relative self-attention, which \nallows us to effectively fuse distance and graph neighbor -\nhood information (see Fig.  2). We perform pretraining \nusing local atom context masking and global graph-based \nprediction, which results in one strong architecture for \nwhich we only tune a range of learning rate values. Our \nmodel achieves competitive performance across a wide \nrange of tasks. Satisfyingly, R-MAT outperforms more \nspecialized models without using extensive handcrafted \nfeaturization or adapting the architecture specifically to \nperform well on quantum prediction benchmarks. The \nimportance of effectively representing distance and other \nrelationships in the attention layer is evidenced by large \nperformance gains compared to MAT.\nMethods\nBackground\nTransformers\nThe  Transformer architecture was introduced by Vaswani \net al. [9] and has since become the standard architecture for \nNLP tasks. The model uses a self-attention mechanism to \nprocess the input sequence, allowing it to capture long-term \ndependencies without the need for recurrent layers. This \nhas resulted in improved performance and faster training \nFig. 1 Relative Molecule Self-Attention Transformer uses a novel \nrelative self-attention block tailored to molecule property prediction. \nIt fuses three types of features: distance embedding, bond \nembedding, and neighborhood embedding\nPage 3 of 14\nMaziarka et al. Journal of Cheminformatics            (2024) 16:3 \n \ntimes compared to traditional NLP models. Originally it \nwas trained for machine translation tasks. However since its \ninception, numerous successors of the Transformer model \nhave been developed, such as BERT [7] or GPT [28], which \nshowed that a properly pretrained Transformer can obtain \nstate-of-the-art on a wide selection of NLP tasks.\nPretraining coupled with the efficient Transformer \narchitecture [9] unlocked state-of-the-art performance \nalso in molecular property prediction [12, 14–16, 29, \n30]. First applications of deep learning did not offer large \nimprovements over more standard methods such as ran -\ndom forests [31–33]. Consistent improvements were \nin particular enabled by more efficient architectures \nadapted to this domain [17, 34, 35]. In this spirit, our goal \nis to further advance modeling for any chemical task by \nredesigning self-attention for molecular data.\nEncoding efficiently the relation between tokens in \nself-attention has been shown to substantially boost \nthe performance of Transformers in vision, language, \nmusic, and biology [19– 25]. The vanilla self-attention \nincludes absolute encoding of position, which can hin -\nder learning when the absolute position in the sen -\ntence is not informative. 1 Relative positional encoding \nfeaturizes the relative distance between each pair of \ntokens, which led to substantial gains in the language \nand music domains [22, 36].\nOn the other hand, a Transformer can be perceived as a \nfully-connected (all vertices are connected to all vertices) \nGraph Neural Network with trainable edge weights given \nby a self-attention [37]. From a practical perspective, the \nempirical success of the Transformer stems from its abil -\nity to learn highly complex and useful patterns.\nMolecular self‑attention\nIn this section, we give a short background on the prior \nworks on adapting self-attention for molecular data and \npoint out their potential shortcomings.\nText Transformers. Multiple works have applied the \nTransformer directly to molecules encoded as text using \nthe SMILES representation [14, 15, 29, 30, 38]. SMILES is \na linear encoding of a molecule into a string of characters \naccording to a deterministic ordering algorithm [39, 40]. \nFor example, the SMILES encoding of carbon dioxide is \nC(=O)=O.\nAdding a  single atom can completely change the \nordering of atoms in the SMILES encoding. Hence, the \nrelative positions of individual characters are not easily \nrelated to their proximity in the graph or space. This is \nin contrast to natural language processing, where the dis-\ntance between two words in the sentence can be highly \ninformative [19, 22, 25]. We suspect this makes the use of \nself-attention in SMILES models less effective. Another \nreadily visible shortcoming is that the graph structure \nand distances between molecule atoms are either com -\npletely encoded or thrown out.\nGraph Transformers. Several works have proposed \nTransformers that operate directly on a graph [12, 16, \n41]. The GROVER and the U2GNN models take as \ninput a molecule encoded as a graph [16, 41]. In both \nof them, the self-attention layer does not have a direct \naccess to the information about the graph. Instead, \nthe information about the relations between atoms \nFig. 2 The Relative Molecule Self-Attention layer is based on the following features: a neighborhood embedding one-hot encodes graph distances \n(neighborhood order) from the source node marked with an arrow; b bond embedding one-hot encodes the bond order (numbers next to the \ngraph edges) and other bond features for neighboring nodes; c distance embedding uses radial basis functions to encode pairwise distances \nin the 3D space. These features are fused according to Eq. (5)\n1 This arises for example when input is an arbitrary chunk of the text [22] \n(e.g. in the next sentence prediction task used in BERT pretraining).\nPage 4 of 14Maziarka et al. Journal of Cheminformatics            (2024) 16:3 \n(existence of a  bond or distance in the graph) is indi -\nrectly encoded by a  graph convolutional layer that is \nrun in GROVER within each layer, and in U2GNN only \nat the beginning. Similarly to Text Transformers, Graph \nTransformers also do not take into account the dis -\ntances between atoms.\nStructured Transformer introduced by Ingraham et al. \n[21] uses relative self-attention that operates on amino \nacids in the task of protein design, while we focus on clas-\nsifiers in the context of molecular property prediction. Its \nself-attention, similarly to our work, provides the model \nwith information about the three-dimensional structure \nof the molecule. As R-MAT encodes the relative dis -\ntances between pairs of atoms, Structured Transformer \nalso uses relative distances between modeled amino acids \nand their position in the sequence. However, it encodes \nthem in a  slightly different way. We incorporate their \nideas and extend them to enable the processing of molec-\nular data.\nMolecule Attention Transformer. Our work is closely \nrelated to Molecule Attention Transformer (MAT), \na  transformer-based model with self-attention tailored \nto processing molecular data [12]. In contrast to most of \nthe aforementioned models, MAT incorporates distance \ninformation in its self-attention module. MAT stacks N \nMolecule Self-Attention blocks followed by a mean pool -\ning and a prediction layer.\nFor a  D-dimensional sequence embedding X ∈ RN×D , \nthe standard self-attention operation is defined as\nwhere Q = XWQ , K= XWK , and V= XWV . Molecule \nSelf-Attention extends Eq. (1) to include additional infor-\nmation about bonds and distances between atoms in the \nmolecule as\nwhere /afii9838a , /afii9838d , /afii9838g are the weights given to individual parts \nof the attention module, g is a  function given by either \na softmax, or an element-wise g(d) = exp(−d) , A is the \nadjacency matrix (with A(i,j) = 1 if there exists a  bond \nbetween atoms i and j and 0 otherwise) and D is the dis -\ntance matrix, where D(i,j) represents the distance between \nthe atoms i and j in the 3D space. Ultimately, Molecule \nAttention Transformer incorporates the interatomic dis -\ntances and atom adjacency by calculating the weighted \naverage of the classical self-attention, a function of atoms’ \ndistance, and a function of atoms’ neighborhood in its \nMolecule Self-Attention layer.\n(1)A(X) = Softmax\n(\nQKT\n√\ndk\n)\nV,\n(2)\nA(X) =\n(\n/afii9838aSoftmax\n(\nQKT\n√\ndk\n)\n+ /afii9838d g(D) + /afii9838gA\n)\nV,\nSelf-attention can relate input elements in a highly flex-\nible manner. In contrast, there is little flexibility in how \nMolecule Self-Attention can use the information about \nthe distance between two atoms. The strength of the \nattention between two atoms depends monotonically \non their relative distance. However, molecular proper -\nties can depend in a  highly nonlinear way on the dis -\ntance between atoms. This has motivated works such as \nKlicpera et  al. [35] to explicitly model the interactions \nbetween atoms, using higher-order terms.\nRelative positional encoding\nIn natural language processing, a vanilla self-attention \nlayer does not take into account the positional information \nof the input tokens (i.e. if we permute the layer input, the \noutput will stay the same). In order to add the positional \ninformation into the input data, the  vanilla transformer \nencodes the absolute position of the input tokens and adds \nits embeddings into the input token embeddings before \npassing this data into the self-attention layers. On the \nother hand, self-attention with relative positional encoding \n[19] adds the embedding of the relative distance between \neach pair of tokens directly into the self-attention layer, \nwhich leads to substantial gains in the learned task. In our \nwork, we use relative self-attention to encode the infor -\nmation about the relative neighborhood, distances, and \nphysicochemical features between all pairs of atoms in the \ninput molecule (See Fig. 2).\nSuccessors\nSince the initial version of this paper was made public, \nseveral researchers have adopted their own versions of \nmolecular self-attention to solve molecular property pre -\ndiction tasks [42, 43], for some datasets even surpassing \nthe results of our model. Choukroun et al. [42] proposed \na model with a different self-attention mechanism, more \nsimilar to Maziarka et  al. [12], that, trained with their \ncustom data augmentation, outperforms R-MAT in the \nQM9 task. Wu et al. [43] proposed Molformer – an archi-\ntecture that exploits both molecular 3D geometry and its \nmotifs. Their model surpasses R-MAT in the QM7, BBBP , \nand BACE tasks.\nAtom relation embedding\nOur core idea to improve Molecule Self-Attention is to \nadd flexibility in how it processes graph and distance \ninformation. Specifically, we adapt positional relative \nencoding to processing molecules [19, 20, 22, 25], which \nwe note was already hinted at by Shaw et  al. [19] as \na high-level future direction. The key idea in these works \nis to enrich the self-attention block to efficiently repre -\nsent information about the relative positions of items in \nthe input sequence.\nPage 5 of 14\nMaziarka et al. Journal of Cheminformatics            (2024) 16:3 \n \nWhat reflects the relative position of two atoms in \na molecule? Similarly to MAT, we delineate three inter -\nrelated factors: (1) their relative distance, (2) their \ndistance in the molecular graph, and (3) their physi -\nochemical relationship (e.g. whether they are within the \nsame aromatic ring). We will also enrich our self-atten -\ntion with this information. However, instead of mode -\nling it as a weighted average, like in Molecule Attention \nTransformer, we allow the network to learn how to use \nthis information by itself.\nIn the next step, we depart from Molecule Self-\nAttention [12] and introduce new factors to the rela -\ntion embedding. Given two atoms, represented by \nvectors xi, xj ∈ RD  , we encode their relation using an \natom relation embedding bij ∈ RD ′\n . This embedding will \nthen be used in the relative self-attention module after \na projection layer.\nIn the next step, we describe three components that \nare concatenated to form the embedding bij.\nNeighborhood embeddings. First, we encode the \nneighborhood order between two atoms as a 6-dimen -\nsional one hot encoding, with information about how \nmany other vertices are between nodes i  and j  in the \noriginal molecular graph (see Fig.  2). The list of neigh -\nborhood features is presented in Table 1 .\nBond embeddings. Finally, we featurize each bond \nto reflect the physical relation between pairs of atoms \nthat might arise from, for example, being part of the \nsame aromatic structure in the molecule. Molecular \nbonds are embedded in as a 7-dimensional vector fol -\nlowing Coley et al. [44], described in Table  2. When the \ntwo atoms are not connected by a true molecular bond, \nall 7 dimensions are set to zeros. We note that while \nthese features can be easily learned in pretraining, we \nhypothesize that this featurization might be highly use -\nful for training R-MAT on smaller datasets.\nDistance embeddings. As we discussed earlier, we \nhypothesize that a much more flexible representation of \nthe distance information should be facilitated in MAT. \nTo achieve this, we use a radial basis distance encoding \nproposed by Klicpera et al. [35]:\nwhere d is the 3D Euclidean distance between two atoms, \nc is the predefined cutoff distance, n ∈{ 1,... ,Nemb } and \nNemb is the total number of radial basis functions that we \nuse. To improve the differentiability, the obtained num -\nbers are multiplied by the polynomial envelope function\nwith p = 6 , resulting in the final distance embedding.\nThis results in the distance embedding given by a whole \nvector (with Nemb dimensions), instead of just one num -\nber, like in the case of Molecule Attention Transformer.\nRelative molecule self‑attention\nEquipped with the embedding bij , which is a concatena -\ntion of neighborhood, distance, and bond embeddings, \nfor each pair of atoms in the molecule, we now use it to \ndefine a novel self-attention layer that we refer to as Rela-\ntive Molecule Self-Attention.\nFirst, mirroring the key-query-value design in the \nvanilla self-attention (c.f. Eq.  (1)), we transform bij into \na key and value specific vectors bV\nij, bK\nij using two neural \nnetworks φV and φK . Each neural network consists of \ntwo layers. A  hidden layer, shared between all attention \nheads and the output layer, that create a separate relative \nembedding for different attention heads.\nConsider Eq. (1) in index notation:\nwhere the unnormalized attention is \neij= (xiW Q )(xjW K )T  . By analogy, in Relative Molecule \nSelf-Attention, we compute e ij as\nen (d) =\n√\n2\nc · sin (nπ\nc d)\nd ,\nu(d) = 1 − (p + 1)(p + 2)\n2\n(d\nc\n)p\n+ p(p + 2)\n(d\nc\n)p+1\n− p(p + 1)\n2\n(d\nc\n)p+2\n,\n(3)A(X)i =\nn∑\nj=1\nSoftmax\n( eij√dz\n)T\n(xjW V ),\nTable 1 Featurization used to embed neighborhood order in \nR-MAT\nIndices Description\n0 i = j\n1 Atoms i and j are connected with a bond\n2 In the shortest path between atoms i and j there is one atom\n3 In the shortest path between atoms i and j there are two \natoms\n4 In the shortest path between atoms i and j there are three \nor more atoms\n5 Any of the atoms i or j is a dummy node\nTable 2 Featurization used to embed molecular bonds in R-MAT\nIndices Description\n0 − 3 Bond order as one-\nhot vector of 1, 1.5, \n2, 3\n4 Is aromatic\n5 Is conjugated\n6 Is in a ring\nPage 6 of 14Maziarka et al. Journal of Cheminformatics            (2024) 16:3 \nwhere u, v ∈ RD′\n are trainable vectors. We then define \nRelative Molecule Self-Attention operation:\nIn other words, we enrich the self-attention layer with \natom relations embedding. Inspired by the text trans -\nformer advancements, we add content-dependent posi -\ntional bias, global content bias, and global positional bias \n[20, 22] (that are calculated based on bK\nij ) to the layer in \nthe phase of attention weights calculation. Then, dur -\ning calculation of the attention weighted average, we \nalso include the information about the other embedding \nbV\nij . This variant of relative self-attention allows us to \nmodel the interaction of query, key, and relative position \nembeddings simultaneously, which was not possible with \nthe original relative self-attention proposed by Shaw et al. \n[19]. The self-attention modules of MAT and R-MAT are \ncompared in Fig. 3.\nRelative molecule self‑attention transformer\nFinally, we use Relative Molecule Self-Attention to con -\nstruct Relative Molecule Self-Attention Transformer \n(R-MAT). The key changes compared to MAT are: (1) \nthe use of Relative Molecule Self-Attention, (2) extended \natom featurization, and (3) extended pretraining proce -\ndure. Figure 1 illustrates the R-MAT architecture.\nThe input is embedded as a matrix of size  Natom × 36 \nwhere each atom of the input is embedded following \nColey et al. [45] and Pocha et al. [45] (see the details in \nAdditional file  1). We process the input using N stacked \nRelative Molecule Self-Attention attention layers. Each \nattention layer is followed by a position-wise feed-for -\nward Network (similar as in the classical transformer \nmodel [9]), which consists of 2 linear layers with a leaky-\nReLU nonlinearity between them.\nAfter processing the input using attention layers, we \npool the representation into a constant-sized vector. We \nreplace simple mean pooling with an attention-based \npooling layer. After applying N self-attention layers, we \nuse the following self-attention pooling [46] in order to \nget the graph-level embedding of the molecule:\n(4)\neij= (xiW Q )(xjW K )T\n  \nvanilla self-attention\n+ (xiW Q )bK\nij\n  \ncontent-dependent\npositional bias\nfor query\n+ (xjW K )bK\nij\n  \ncontent-dependent\npositional bias\nfor key\n+ uT (xjW K )  \nglobal content\nbias\n+ vT bK\nij\n  \nglobal positional\nbias\n,\n(5)A(X)i =\nn∑\nj=1\nSoftmax\n( eij√dz\n)T\n(xjW V + bV\nij). where H is the hidden state obtained from self-attention \nlayers, W 1 ∈ RP×D and W 2 ∈ RS×P are pooling attention \nweights, with P equal to the pooling hidden dimension \nand S equal to the number of pooling attention heads. \nFinally, the graph embedding g is then passed to the two-\nlayer MLP , with leaky-ReLU activation, in order to make \nthe prediction.\nPretraining. We used a two-step pretraining proce -\ndure. In the first step, our network is trained with the \ncontextual property prediction task proposed by Rong \net  al. [16], where we mask not only selected atoms but \nalso their neighbors. The goal of the task is to predict the \nwhole atom context. This task is much more demand -\ning for the network than the classical masking approach \npresented by Maziarka et  al. [12] since the network has \nto encode more specific information about the masked \natom neighborhood. Furthermore, the size of the con -\ntext vocabulary is much bigger than the size of the atoms \nvocabulary in the MAT pretraining approach. The second \ntask is a graph-level prediction proposed by Fabian et al. \n[15] in which the goal is to predict a  set of real-valued \ndescriptors of physicochemical properties. We present \nmore detailed information about the pretraining proce -\ndure and ablations in Additional file 1.\nOther details. Similarly to Maziarka et  al. [12], we \nadd an artificial dummy node to the input molecule. The \ndistance of the dummy node to any other atom in the \nmolecule is set to the maximal cutoff distance, and the \nedge connecting the dummy node with any other atom \nhas its unique index. Moreover, the dummy node has \nits own index in the input atom embedding. We calcu -\nlate distance information in a similar manner as Mazi -\narka et  al. [12]. The 3D molecular conformations that \nare used to obtain distance matrices are calculated using \nUFFOptimizeMolecule function from the RDKit pack -\nage [47] with the default parameters. Finally, we consider \na variant of the model extended with 200 RDKit features \nas in Rong et  al. [16]. The features are concatenated to \nthe final embedding g and processed using a prediction \nM L P.\nP =Softmax (W2 tanh(W1HT)),\ng =Flatten(PH),\nPage 7 of 14\nMaziarka et al. Journal of Cheminformatics            (2024) 16:3 \n \nResults and discussion\nSmall hyperparameter budget\nThe drug discovery pipelines focus on fast iterations \nof compound screenings and adjusting the models to \nnew data incoming from the laboratory. In particu -\nlar, some approaches focus on the fast adaptation to \nthe dataset by employing automated ML and reducing \nhands-on time [48]. We start by comparing in this set -\nting R-MAT to DMPNN [17], MAT [12] and GROVER \n[16], representative state-of-the-art models on popu -\nlar molecular property prediction tasks. We followed \nthe evaluation in Maziarka et  al. [12], where the only \nchangeable hyperparameter is the learning rate, which \nwas checked with 7 different values.\nThe BBBP and Estrogen- β datasets use scaffold splits, \nwhile all the other datasets use random splits. Splits \nwere proposed by Maziarka et  al. [12]. For every data -\nset we calculate scores based on 6 different splits, we \nreport the mean test score based on the hyperparameters \nthat obtained the best validation score, in parentheses \nwe include the standard deviation. In this and the next \nexperiments, we denote models extended with additional \nRDKit features (see Section Relative Molecule Self-Atten -\ntion Transformer) as GROVER rdkit and R-MAT rdkit . More \ndot product distancea djacency VQ\nKT\nself-attention\ndot product neighborhood, bond,\nand distance embedding\nglobal content\nand positional bias\nV+bVQ\nKT\nself-attention\nQK K\n(a) MAT self-attention\n(b) R-MAT self-attention\nFig. 3 Comparison between MAT and R-MAT self-attention modules. The self-attention block comprises scaling and applying softmax. In MAT, \ndistance and adjacency matrices are outside the self-attention block, while in R-MAT all matrices are mixed within the self-attention. Moreover, all \natom-pair embeddings are collected in one matrix that is also multiplied by queries and keys\nPage 8 of 14Maziarka et al. Journal of Cheminformatics            (2024) 16:3 \ninformation about the models and datasets used in this \nbenchmark is given in Additional file 1.\nTable 3 shows that R-MAT outperforms other methods \nin 3 out of 6 tasks. For comparison, we also cite repre -\nsentative results of other methods from Maziarka et  al. \n[12]. Satisfyingly, we observe a  marked improvement \non the solubility prediction tasks (ESOL and FreeSolv). \nUnderstanding solubility depends to a  large degree on \na detailed understanding of spatial relationships between \natoms. This suggests that the improvement in perfor -\nmance might be related to better utilization of the dis -\ntance or graph information.\nLarge hyperparameter budget\nIn contrast to the previous setting, we test R-MAT against \na similar set of models but using a large-scale hyperpa -\nrameter search (300 different hyperparameter combina -\ntions). This setting has been proposed in Rong et al. [16]. \nFor comparison, we include results under small (7 differ -\nent learning rates) hyperparameter budget. All datasets \nuse a scaffold split. Scores are calculated based on 3 dif -\nferent data splits. While the ESOL and FreeSolv datasets \nare the same as in the previous paragraph, here they use \na scaffold split, and the labels are not normalized (unlike \nin the previous paragraph). Additional information about \nthe models and datasets used in this benchmark are given \nin Additional file 1.\nTable 4 summarizes the experiment. The results show \nthat for the large hyperparameter budget R-MAT outper-\nforms other methods in 2 tasks and along with GROVER \nare the best in one more task. Overall in this setting our \nmethod achieves comparable results to GROVER, having \nthe same median rank and being slightly worse in terms \nof mean rank. On the other hand, for small hyperparam -\neters budget R-MAT achieves the best results, both in \nterms of the mean and the median ranks (see the details \nin Additional file 1).\nLarge‑scale experiments\nFinally, to better understand how R-MAT performs in a \nsetting where pretraining is likely to less influence results, \nwe include results on  the QM9 dataset [52]. QM9 is a \nquantum mechanics benchmark that encompasses the \nprediction of 12 simulated properties across around 130 \nk small molecules with at most 9 heavy (non-hydrogen) \natoms. The molecules are provided with their atomic 3D \npositions for which the quantum properties were initially \ncalculated. For these experiments, we used a learning rate \nequal to 0.015 (we selected this learning rate value as it \nreturned the best results for α dataset among 4 differ -\nent learning rates that we tested: {0.005,0.01,0.015,0.02}). \nWe present additional information about the dataset and \nmodels used in this benchmark in Additional file 1.\nFigure  4 compares R-MAT performance with various \nmodels. More detailed results could be found in Addi -\ntional file  1. R-MAT achieves highly competitive results, \nwith state-of-the-art performance on 4 out of the 12 \ntasks. We attribute higher variability of performance to \nthe limited small hyperparameter search we performed.\nExploring the design space of self‑attention layer\nAchieving strong empirical results hinged on a methodo -\nlogical exploration of the design space of different vari -\nants of the self-attention layer. We document here this \nexploration and relevant ablations. We present here the \nexperiments for different relative attention features and \nTable 3 Results on molecule property prediction benchmark from Maziarka et al. [12]\nWe only tune the learning rate for models in the first group. First two datasets are regression tasks (RMSE), other datasets are classification tasks (ROC AUC). For \nreference, we include results for non-pretrained baselines (SVM, RF, GCN [49], and DMPNN [17]) from [12]. We also include SVM rdkit and RF rdkit as two baseline \nmethods with added RDKit features. The best results for each task are shown in bold. A rank plot for these experiments is included in Additional file 1\nESOL ↓ FreeSolv ↓ BBBP ↑ Estrogen‑β ↑ MetStablow ↑ MetStabhigh ↑\nPretrained models\nMAT 0.278(0.020) 0.265(0.042) 0.737(0.009) 0.773(0.012) 0.862(0.025) 0.884(0.030)\nGROVER 0.303(00.048) 0.270(0.033) 0.726(0.007) 0.758(0.006) 0.892(0.031) 0.887(0.019)\nGROVER rdkit 0.288(0.021) 0.308(0.058) 0.726(0.003) 0.788(0.009) 0.873(0.033) 0.881(0.039)\nR-MAT 0.252(0.030) 0.232(0.071) 0.745(0.010) 0.788(0.007) 0.887(0.028) 0.880(0.027)\nR-MAT rdkit 0.246(0.024) 0.239(0.066) 0.746(0.007) 0.791(0.010) 0.884(0.032) 0.886(0.031)\nNon-pretrained models\nSVM 0.479(0.055) 0.461(0.077) 0.723(0.000) 0.772(0.000) 0.893(0.030) 0.890(0.029)\nSVM rdkit 0.279(0.024) 0.285(0.049) 0.741(0.001) 0.781(0.001) 0.895(0.029) 0.884(0.031)\nRF 0.534(0.073) 0.524(0.098) 0.721(0.003) 0.791(0.012) 0.892(0.026) 0.888(0.030)\nRF rdkit 0.289(0.035) 0.337(0.026) 0.743(0.002) 0.807(0.003) 0.903(0.025) 0.886(0.028)\nGCN 0.369(0.032) 0.299(0.068) 0.695(0.013) 0.730(0.006) 0.884(0.033) 0.875(0.036)\nDMPNN 0.297(0.046) 0.252(0.044) 0.709(0.001) 0.776(0.006) 0.885(0.026) 0.889(0.018)\nPage 9 of 14\nMaziarka et al. Journal of Cheminformatics            (2024) 16:3 \n \ndifferent choices of maximum neighborhood order. We \nalso defer most results to the Additional file  1, where we \npresent experiments for different self-attention variants, \ndistance encoding and bond features. We perform all \nexperiments on the ESOL, FreeSolv, and BBBP datasets \nwith 3 different scaffold splits. We did not use any pre -\ntraining for these experiments. We follow the same fine-\ntuning methodology as in Section Small hyperparameter \nbudget.\nImportance of different sources of information in \nself-attention. The self-attention module in R-MAT \nincorporates three auxiliary sources of information: (1) \ndistance information, (2) graph information (encoded \nusing neighborhood order), and (3) bond features. In \nTable  5(a), we show the effect on the performance of \nablating each of these elements. In this experiment, we \nrepeat the calculations for three different data splits \nand five different random seeds to make the results \nless prone to random noise, e.g. due to the random \nweight initialization. We find that all components, \nincluding the distance matrix, are crucial for achiev -\ning optimal performance of R-MAT. The use of all \ninformation sources results in the best performance \nacross all tested datasets. The performance for the \nsmallest FreeSolv dataset is considerably better when \nmore information sources are included. The same \ntrend is observed in the larger ESOL regression task, \nalbeit with less noticeable differences. For the BBBP \nbinary classification task, all results seem comparable, \nbut interestingly, all variants without inter-atomic dis -\ntances achieve better results.\nMaximum neighborhood order. We take a  closer \nlook at how we encode the molecular graph. Maziarka \net  al. [12] used a  simple binary adjacency matrix to \nencode the edges. We enriched this representation by \nadding one-hot encoding of the neighborhood order. \nFor example, the order of 3 for a pair of atoms means \nthat there are two other vertices on the shortest path \nbetween this pair of atoms. In R-MAT we used 4 as the \nmaximum order of neighborhood distance. That is, we \nencoded as separate features if two atoms are 1, 2, 3 or \n4 hops away in the molecular graph. In Table  5 (b) we \nablate this choice. The result suggests that R-MAT per -\nformance benefits from including separate features for \nall the considered orders.\nCloser comparison to molecule attention transformer\nOur main motivation for improving self-attention \nin MAT was to make it easier to represent attention \nTable 4 Results on the benchmark from Rong et al. [16]\nModels are fine-tuned under a large hyperparameters budget. Additionally, models fine-tuned with only tuning the learning rate are presented in the last group. The \nlast two datasets are classification tasks (ROC AUC), the remaining datasets are regression tasks (MAE for QM7 and RMSE for the other datasets). For reference, we \ninclude results for non-pretrained baselines (GraphConv [50], Weave [51] and DMPNN [17]) from Rong et al. [16]. We also include RF rdkit as a baseline method with \nadded RDKit features. A rank plot for these experiments is included in Additional file 1. The best scores for each task over all models are shown in bold, and the best \nscores for the models for which only the learning rate was tuned are underlined\nESOL ↓ FreeSolv ↓ Lipo ↓ QM7 ↓ BACE ↑ BBBP ↑\nFull hyperparameter tuning\nRF rdkit 0.942(0.196) 2.625(0.509) 0.739(0.038) 124.3(3.5) 0.884(0.030) 0.928(0.025)\nGraphConv 1.068(0.050) 2.900(0.135) 0.712(0.049) 118.9(20.2) 0.854(0.011) 0.877(0.036)\nWeave 1.158(0.055) 2.398(0.250) 0.813(0.042) 94.7(2.7) 0.791(0.008) 0.837(0.065)\nDMPNN 0.980(0.258) 2.177(.914) 0.653(0.046) 105.8(13.2) 0.852(0.053) 0.919(0.030)\nGROVER rdkit 0.888(0.116) 1.592(0.072) 0.563(0.030) 72.5(5.9) 0.878(0.016) 0.936(0.008)\nR-MAT rdkit 0.786(0.133) 2.044(0.662) 0.574(0.028) 68.692(1.123) .871(0.028) 0.936(0.020)\nLearning rate only tuning\nMAT 0.853(0.159) 1.744(0.425) 0.608(0.017) 102.8(2.94) 0.846(0.025) 0.920(0.039)\nGROVER 0.927(0.110) 2.262(0.407) 0.604(0.015) 82.623(3.833) 0.867(0.022) 0.908(0.053)\nGROVER rdkit 0.924(0.129) 20.096(0.496) 0.593(0.029) 84.625(4.174) 0.873(0.031) 0.931(0.021)\nR-MAT 0.801(0.132) 1.912(0.364) 0.585(0.029) 77.248(2.819) 0.858(0.041) 0.931(0.016)\nR-MAT rdkit 0.819(0.145) 2.057(0.434) 0.580(0.019) 70.929(3.568) 0.858(0.021) 0.920(0.021)\nFig. 4 Rank plot of scores obtained on the QM9 benchmark, which \nconsists of 12 different quantum property prediction tasks\nPage 10 of 14Maziarka et al. Journal of Cheminformatics            (2024) 16:3 \npatterns that depend in a more complex way on the dis -\ntance and graph information. We qualitatively explore \nhere whether R-MAT achieves this goal, comparing its \nattention patterns to that of MAT.\nWe compared attention patterns learned by the pre -\ntrained MAT (weights obtained from Maziarka et  al. \n[12]) and R-MAT. We observed that long-range atom \nrelations are better captured by our model. We dem -\nonstrate this finding for a  selected molecule from the \nESOL dataset. Figure  5 shows that different heads of \nRelative Molecule Self-Attention are focusing on dif -\nferent atoms in the input molecule. We can see that \nself-attention strength is concentrated on the input \natom (head 5), on the closest neighbors (heads 0 and \n11), on the second-order neighbors (head 7), on the \ndummy node (head 1) or on some substructure that \noccurs in the molecule (heads 6 and 10 are concen -\ntrated on atoms 1 and 2). In contrast, self-attention \nin MAT focuses mainly on the input atoms and their \nclosest neighbors, the information from other regions \nof the molecule is not strongly propagated. This likely \nhappens due to the construction of the Molecule Self-\nAttention in MAT (c.f. Eq. (2 )), where the output atom \nrepresentation is calculated from equally weighted \nmessages based on the adjacency matrix, distance \nmatrix, and self-attention. Due to its construction, it is \nmore challenging for MAT than for R-MAT to learn to \nattend to a distant neighbor.\nAs Relative Molecule Self-Attention Transformer is an \nextension of Molecule Attention Transformer [12], we \nperform a more strict comparison of these models. To this \nend, we compare MAT with R-MAT using three different \npretraining strategies: no pretraining, masking pretrain -\ning (following the original MAT model), and contextual \n+ graph level pretraining (presented in this paper). For \nthis comparison, we use the small hyperparameter budget \nbenchmarks used in the MAT paper (and in this paper, in \nSection Small hyperparameter budget).\nThe results  of the comparison between MAT and \nR-MAT are presented in Table  6. R-MAT, on average, \nobtains better results than the standard MAT. Moreo -\nver, the more complicated the pretraining is, the better \nR-MAT is compared to MAT. In the case of no pretrain -\ning, R-MAT outperforms MAT on 3 out of 6 tasks, the \nscores for one task are equal, and R-MAT is outper -\nformed by MAT on 2 out of 6 tasks. In the case of the \nmasked pretraining, R-MAT achieves better scores, \noutperforming MAT on 4 out of 6 tasks. Finally, in the \ncontextual + graph level pretraining setting, R-MAT out-\nperforms MAT on 5 out of 6 tasks.\nLimitations\nAlthough R-MAT has shown promising results, there \nare a few limitations to our approach that should be \nconsidered. Firstly, our model is E(3)-invariant thanks \nto the use of inter-atomic distances, but it lacks the \nability to recognize mirror images (enantiomers), \nwhich might be crucial for some tasks such as bind -\ning affinity prediction. Secondly, our model uses only \none sampled molecular conformation for the predic -\ntion, thereby missing out on the entire range of other \npossible conformations for molecules that are highly \nTable 5 Ablations of relative molecule self-attention; other ablations are included in the Additional file 1\nBBBP ↑ ESOL ↓ FreeSolv ↓\n(a) Test set performances of R-MAT for different relative attention \nfeatures.\n R-MAT 0.872(0.042) 0.400 (0.044 ) 0.430(0.056)\n Distance 0.877(0.062) 0.407(0.037) 0.484(0.037)\n Neighborhood 0.872(0.055) 0.402(0.027) 0.493(0.046)\n Bond features 0.871(0.057) 0.403(0.026) 0.460(0.025)\n Only distance 0.870(0.038) 0.418(0.036) 0.504(0.072)\n Only neighborhood 0.886(0.038) 0.406(0.032) 0.483(0.042)\n Only bond features 0.894(0.049) 0.407(0.034) 0.494(0.018)\nBBBP ↑ ESOL ↓ FreeSolv ↓\n(b) Test set performances of R-MAT for different choices of maxi-\nmum neighborhood order.\n R-MAT 0.908(0.039) 0.378(0.027) 0.438(0.036)\n Max order = 1 0.847(0.081) 0.372(0.018) 0.461(0.049)\n Max order = 2 0.890(0.068) 0.382(0.040) 0.519(0.036)\n Max order = 3 0.873(0.053) 0.455(0.005) 0.492(0.055)\nPage 11 of 14\nMaziarka et al. Journal of Cheminformatics            (2024) 16:3 \n \nflexible. Finally, our model is currently limited to pre -\ndicting properties for small to medium-sized mole -\ncules and may not be suitable for larger, more complex \nmolecules. R-MAT, like many other transformers, is \ncomputationally intensive and requires memory quad -\nratic with the input molecule size. These limitations \nprovide opportunities for future research to address \nthese challenges and improve upon our results.\n(a) Molecule\n(b) R-MAT( c) MAT\nFig. 5 Visualization of the learned self-attention for each of all attention heads in the second layer of pretrained R-MAT (left) and all attention heads \nin pretrained MAT (right), for a molecule from the ESOL dataset. The top Figure visualizes the molecule and its adjacency and distance matrices. The \nself-attention pattern in MAT is dominated by the adjacency and distance matrix, while R-MAT seems capable of learning more complex attention \npatterns\nTable 6 Results of the direct comparison between R-MAT and MAT, for different pre-training settings\nWe underline the best scores for every pretraining setting\nESOL ↓ FreeSolv ↓ BBBP ↑ Estrogen‑β ↑ MetStablow ↑ MetStabhigh ↑\nNo pretraining MAT 0.278(0.019) 0.283(0.043) 0.727(0.008) 0.751(0.005) 0.857(0.025) 0.872(0.051)\nR-MAT 0.273(0.046) 0.272(0.015) 0.727(0.015) 0.786(0.014) 0.844(0.050) 0.833(0.042)\nMasking pretraining MAT 0.278(0.020) 0.265(0.042) 0.737(0.009) 0.773(0.012) 0.862(0.025) 0.884(0.030)\nR-MAT 0.253(0.085) 0.264(0.028) 0.714(0.090) 0.789(0.015) 0.880(0.022) 0.870(0.042)\nR-MAT pretraining MAT 0.298(0.024) 0.246(0.042) 0.729(0.006) 0.782(0.021) 0.879(0.024) 0.882(0.030)\nR-MAT 0.252(0.030) 0.232(0.071) 0.745(0.010) 0.788(0.007) 0.887(0.028) 0.880(0.027)\nPage 12 of 14Maziarka et al. Journal of Cheminformatics            (2024) 16:3 \nConclusions\nTransformer has been successfully adapted to vari -\nous domains by incorporating into its architecture a \nminimal set of inductive biases. In a similar spirit, we \nmethodologically explored the design space of the self-\nattention layer and identified a highly effective Relative \nMolecule Self-Attention.\nRelative Molecule Self-Attention Transformer, a \nmodel based on Relative Molecule Self-Attention, \nachieves state-of-the-art or very competitive results \nacross a  wide range of molecular property prediction \ntasks. R-MAT is a highly versatile model, showing com -\npetitive results in both quantum property prediction \ntasks, as well as on biological datasets. We also show \nthat R-MAT is easy to train and requires tuning only \nthe learning rate to achieve competitive results, which \ntogether with open-sourced weights and code, makes \nour model highly accessible.\nRelative Molecule Self-Attention encodes an induc -\ntive bias to consider relationships between atoms that \nare commonly relevant to a chemist, but on the other \nhand, leaves flexibility to unlearn them if needed. Relat -\nedly, Vision Transformers learn global processing in early \nlayers despite being equipped with a locality inductive \nbias [18]. Our empirical results show in a new context \nthat picking the right set of inductive biases is key for \nself-supervised learning to work well. We also show that \nRelative Molecule Self-Attention will help improve other \nmodels for molecular property prediction.\nLearning useful representations for molecular property \nprediction is far from being solved. Achieving state-of-\nthe-art results, while less dependent on them, still relied \non using certain large sets of handcrafted features both in \nfine-tuning and pretraining. At the same time, these fea -\ntures are beyond doubt learnable from data. Developing \nmethods that will push representation learning towards \ndiscovering these and better features automatically from \ndata is an exciting challenge for the future.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13321- 023- 00789-7.\nAdditional file 1. Additional experiments, supplementary tables and \nfigures.\nAcknowledgements\nThe authors thank NVIDIA for supporting this research with the computational \nresources required to complete this work.\nAuthor contributions\nLM and SJ derived the concept LM wrote most of the code and performed \npreliminary experiments. DM wrote the code and conducted most of the \nexperiments. LM created all tables and experiment-related figures. LM, TD \nand SJ wrote the paper. TD prepared figures with the visualisation of Relative \nMolecule Self-Attention Transformer and Relative Molecule Self-Attention. JT, \nIP , PM. provided feedback and critical input. All authors read and approved the \nfinal manuscript.\nFunding\nThe work of Ł. Maziarka was supported by the National Science Centre \n(Poland) grant no. 2019/35/N/ST6/02125. The work of T. Danel was supported \nby the National Science Centre (Poland) grant no. 2020/37/N/ST6/02728. \nStanisław Jastrzębski thanks FNP START stipend and IPUB project at Jagiellon-\nian University for supporting this work.\nAvailability of data and materials\nWe open-source R-MAT weights and code as part of the HuggingMolecules \npackage [53] at: https:// github. com/ gmum/ huggi ngmol ecules. We also share \nall datasets and data splits that we used in our experiments at: https:// osf. io/ \nrgva4/.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 24 May 2023   Accepted: 28 November 2023\nReferences\n 1. Rommel JB (2021) From prescriptive to predictive: An interdisciplinary \nperspective on the future of computational chemistry. arXiv preprint \narXiv: 2103. 02933\n 2. Chan HS, Shan H, Dahoun T, Vogel H, Yuan S (2019) Advancing drug \ndiscovery via artificial intelligence. Trends Pharmacol Sci 40(8):592–604\n 3. Bender A, Cortés-Ciriano I (2021) Artificial intelligence in drug discovery: \nwhat is realistic, what are illusions? part 1: Ways to make an impact, and \nwhy we are not there yet. Drug Discovery Today 26(2):511–524\n 4. Korotcov A, Tkachenko V, Russo DP , Ekins S (2017) Comparison of deep \nlearning with multiple machine learning methods and metrics using \ndiverse drug discovery data sets. Mol Pharm 14(12):4462–4475\n 5. Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural mes-\nsage passing for quantum chemistry. In: International Conference on \nMachine Learning. PMLR, pp 1263–1272\n 6. Wieder O, Kohlbacher S, Kuenemann M, Garon A, Ducrot P , Seidel T, \nLanger T (2020) A compact review of molecular property prediction with \ngraph neural networks. Drug Disc Today: Technol 37:1–12\n 7. Devlin J, Chang M, Lee K, Toutanova K (2019) BERT: pre-training of deep \nbidirectional transformers for language understanding. In: Proceedings of \nthe 2019 Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, 2–7 June, 2019, Volume 1 (Long and \nShort Papers), pp 4171–4186\n 8. Howard J, Ruder S (2018) Universal language model fine-tuning for text \nclassification. In: Gurevych, I., Miyao, Y. (eds.) Proceedings of the 56th \nAnnual Meeting of the Association for Computational Linguistics, ACL \n2018, Melbourne, Australia, 15–20 July, 2018, Volume 1: Long Papers, pp \n328–339\n 9. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser \nL, Polosukhin I (2017) Attention is all you need. In: Guyon I, von Luxburg \nU, Bengio S, Wallach HM, Fergus R, Vishwanathan SVN, Garnett R (eds) \nAdvances in Neural Information Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017, 4–9 Dec, 2017, \nLong Beach, CA, USA, pp 5998–6008\n 10. Wang A, Pruksachatkun Y, Nangia N, Singh A, Michael J, Hill F, Levy O, \nBowman SR (2019) Superglue: A stickier benchmark for general-purpose \nlanguage understanding systems. In: Wallach HM, Larochelle H, Beygel-\nzimer A, d’Alché-Buc F, Fox EB, Garnett R (eds.) Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference on Neural Information \nProcessing Systems 2019, NeurIPS 2019, 8-14 Dec, 2019, Vancouver, BC, \nCanada, pp 3261–3275\nPage 13 of 14\nMaziarka et al. Journal of Cheminformatics            (2024) 16:3 \n \n 11. Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzębski S (2020) Mol-\necule attention transformer. arXiv preprint arXiv: 2002. 08264\n 12. Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzebski S (2019) \nMolecule-augmented attention transformer. NeurIPS 2020 Workshop \non Graph Representation Learning\n 13. Hu W, Liu B, Gomes J, Zitnik M, Liang P , Pande VS, Leskovec J (2020) \nStrategies for pre-training graph neural networks. In: 8th International \nConference on Learning Representations, ICLR 2020, Addis Ababa, \nEthiopia, 26–30 Apr, 2020\n 14. Chithrananda S, Grand G, Ramsundar B (2020) Chemberta: large-scale \nself-supervised pretraining for molecular property prediction. arXiv \npreprint arXiv: 2010. 09885\n 15. Fabian B, Edlich T, Gaspar H, Segler M, Meyers J, Fiscato M, Ahmed M \n(2020) Molecular representation learning with language models and \ndomain-relevant auxiliary tasks. arXiv preprint arXiv: 2011. 13230\n 16. Rong Y, Bian Y, Xu T, Xie W, Wei Y, Huang W, Huang J (2020) Self-super -\nvised graph transformer on large-scale molecular data. In: Larochelle \nH, Ranzato M, Hadsell R, Balcan M, Lin H (eds.) Advances in Neural \nInformation Processing Systems 33: Annual Conference on Neural \nInformation Processing Systems 2020, NeurIPS 2020, 6–12 Dec 2020, \nVirtual\n 17. Yang K, Swanson K, Jin W, Coley C, Eiden P , Gao H, Guzman-Perez A, \nHopper T, Kelley B, Mathea M et al (2019) Analyzing learned molecu-\nlar representations for property prediction. J Chem Inform Model \n59(8):3370–3388\n 18. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unter -\nthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, \nHoulsby N (2021) An image is worth 16x16 words: Transformers for \nimage recognition at scale. In: 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Austria, 3–7 May 2021\n 19. Shaw P , Uszkoreit J, Vaswani A (2018) Self-attention with relative posi-\ntion representations. In: Walker MA, Ji H, Stent A (eds) Proceedings of \nthe 2018 Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies, NAACL-\nHLT, New Orleans, Louisiana, USA, 1–6 June 2018, Volume 2 (Short \nPapers), pp 464–468\n 20. Dai Z, Yang Z, Yang Y, Carbonell JG, Le QV, Salakhutdinov R (2019) \nTransformer-XL: Attentive language models beyond a fixed-length \ncontext. In: Korhonen A, Traum DR, Màrquez L (eds.) Proceedings of the \n57th Conference of the Association for Computational Linguistics, ACL \n2019, Florence, Italy, July 28- 2 Aug, 2019, Volume 1: Long Papers, pp \n2978–2988\n 21. Ingraham J, Garg VK, Barzilay R, Jaakkola TS (2019) Generative models \nfor graph-based protein design. In: Wallach HM, Larochelle H, Beyg-\nelzimer A, d’Alché-Buc F, Fox EB, Garnett R (eds) Advances in Neural \nInformation Processing Systems 32: Annual Conference on Neural \nInformation Processing Systems 2019, NeurIPS 2019, 8–14 Dec 2019, \nVancouver, BC, Canada, pp 15794–15805\n 22. Huang Z, Liang D, Xu P , Xiang B (2020) Improve transformer models \nwith better relative position embeddings. In: Cohn T, He Y, Liu Y (eds) \nFindings of the Association for Computational Linguistics: EMNLP 2020, \nOnline Event, 16-20 Nov 2020, vol EMNLP 2020, pp 3327–3335\n 23. Romero DW, Cordonnier J (2021) Group equivariant stand-alone \nself-attention for vision. In: 9th International Conference on Learning \nRepresentations, ICLR 2021, Virtual Event, Austria, 3–7 May 2021\n 24. Khan S, Naseer M, Hayat M, Zamir SW, Khan FS, Shah M (2022) \nTransformers in vision: a survey. ACM computing Surveys (CSUR) \n54(10s):1–41\n 25. Ke G, He D, Liu T-Y (2021) Rethinking positional encoding in language \npre-training. In: International Conference on Learning Representations\n 26. Chen L, Lu K, Rajeswaran A, Lee K, Grover A, Laskin M, Abbeel P , Srinivas \nA, Mordatch I (2021) Decision transformer: reinforcement learning via \nsequence modeling. Adv Neural Inform Process Syst 34:15084–15097\n 27. Born J, Manica M (2023) Regression transformer enables concurrent \nsequence regression and generation for molecular language model-\nling. Nature Machine Intell 5(4):432–444\n 28. Radford A, Narasimhan K, Salimans T, Sutskever I, et al (2018) Improving \nlanguage understanding by generative pre-training\n 29. Wang S, Guo Y, Wang Y, Sun H, Huang J (2019) SMILES-BERT: Large \nscale unsupervised pre-training for molecular property prediction. In: \nProceedings of the 10th ACM International Conference on Bioinformat -\nics, Computational Biology and Health Informatics. BCB ’19\n 30. Honda S, Shi S, Ueda HR (2019) Smiles transformer: Pre-trained molecu-\nlar fingerprint for low data drug discovery. arXiv preprint arXiv: 1911.  \n04738\n 31. Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, \nLeswing K, Pande V (2018) Moleculenet: a benchmark for molecular \nmachine learning. Chem Sci 9(2):513–530\n 32. Jiang D, Wu Z, Hsieh C-Y, Chen G, Liao B, Wang Z, Shen C, Cao D, Wu \nJ, Hou T (2021) Could graph neural networks learn better molecular \nrepresentation for drug discovery? a comparison study of descriptor-\nbased and graph-based models. J Cheminfirm 13(1):1–23\n 33. Robinson M, Glen R, Lee A (2020) Validating the validation: reanalyz-\ning a large-scale comparison of deep learning and machine learning \nmodels for bioactivity prediction. J Computer-Aided Mol Design \n34:717–730\n 34. Mayr A, Klambauer G, Unterthiner T, Steijaert M, Wegner JK, Ceulemans \nH, Clevert D-A, Hochreiter S (2018) Large-scale comparison of machine \nlearning methods for drug target prediction on chembl. Chem Sci \n9(24):5441–5451\n 35. Klicpera J, Groß J, Günnemann S (2020) Directional message passing \nfor molecular graphs. In: 8th International Conference on Learning \nRepresentations\n 36. Shang C, Liu Q, Chen K-S, Sun J, Lu J, Yi J, Bi J (2018) Edge attention-\nbased multi-relational graph convolutional networks. arXiv preprint \narXiv:  1802. 04944\n 37. Veličković P (2023) Everything is connected: Graph neural networks. \narXiv preprint arXiv: 2301. 08210\n 38. Schwaller P , Laino T, Gaudin T, Bolgar P , Hunter CA, Bekas C, Lee AA \n(2019) Molecular transformer: A model for uncertainty-calibrated \nchemical reaction prediction. ACS central science\n 39. Weininger D (1988) Smiles, a chemical language and information \nsystem. 1. introduction to methodology and encoding rules. J Chem \nInf Comput Sci 28(1):31–36\n 40. Jastrzębski S, Leśniak D, Czarnecki WM (2016) Learning to smile (s). \narXiv preprint arXiv: 1602. 06289\n 41. Nguyen DQ, Nguyen TD, Phung D (2019) Unsupervised universal self-\nattention network for graph classification. CoRR abs/1909.11855\n 42. Choukroun Y, Wolf L (2022) Geometric transformer for end-to-end \nmolecule properties prediction. In: Raedt LD (ed) Proceedings of the \nThirty-First International Joint Conference on Artificial Intelligence, \nIJCAI 2022, Vienna, Austria, 23–29 July 2022, pp 2895–2901\n 43. Wu F, Radev D, Li SZ (2023) Molformer: Motif-based transformer on 3d \nheterogeneous molecular graphs. In: Proceedings of the AAAI Confer -\nence on Artificial Intelligence, vol 37, pp 5312–5320\n 44. Coley CW, Barzilay R, Green WH, Jaakkola TS, Jensen KF (2017) Convolu-\ntional embedding of attributed molecular graphs for physical property \nprediction. J Chem Inform Model 57(8):1757–1772\n 45. Pocha A, Danel T, Podlewska S, Tabor J, Maziarka Ł (2021) Compari-\nson of atom representations in graph neural networks for molecular \nproperty prediction. In: 2021 International Joint Conference on Neural \nNetworks (IJCNN). IEEE, pp 1–8\n 46. Lin Z, Feng M, dos Santos CN, Yu M, Xiang B, Zhou B, Bengio Y (2016) A \nstructured self-attentive sentence embedding. In: International Confer -\nence on Learning Representations\n 47. Landrum G (2016) Rdkit: Open-source cheminformatics software\n 48. Li Y, Hsieh C-Y, Lu R, Gong X, Wang X, Li P , Liu S, Tian Y, Jiang D, Yan \nJ et al (2022) An adaptive graph learning method for automated \nmolecular interactions and properties predictions. Nature Machine \nIntell 4(7):645–651\n 49. Duvenaud D, Maclaurin D, Aguilera-Iparraguirre J, Gómez-Bombarelli R, \nHirzel T, Aspuru-Guzik A, Adams RP (2015) Convolutional networks on \ngraphs for learning molecular fingerprints. In: Cortes C, Lawrence ND, \nLee DD, Sugiyama M, Garnett R (eds) Advances in Neural Information \nProcessing Systems 28: Annual Conference on Neural Information \nProcessing Systems 2015, 7–12 Dec 2015, Montreal, Quebec, Canada, \npp 2224–2232\n 50. Kipf TN, Welling M (2017) Semi-supervised classification with graph \nconvolutional networks. In: International Conference on Learning \nRepresentations\nPage 14 of 14Maziarka et al. Journal of Cheminformatics            (2024) 16:3 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 51. Kearnes S, McCloskey K, Berndl M, Pande V, Riley P (2016) Molecular \ngraph convolutions: moving beyond fingerprints. J Computer-aided \nMol Design 30(8):595–608\n 52. Ramakrishnan R, Dral PO, Rupp M, Von Lilienfeld OA (2014) Quantum \nchemistry structures and properties of 134 kilo molecules. Sci Data \n1(1):1–7\n 53. Gaiński P , Maziarka Ł, Danel T, Jastrzebski S (2022) Huggingmolecules: \nAn open-source library for transformer-based molecular property \nprediction (student abstract). In: Proceedings of the AAAI Conference \non Artificial Intelligence, vol 36, pp 12949–12950\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.599103569984436
    },
    {
      "name": "Transformer",
      "score": 0.49496549367904663
    },
    {
      "name": "Data science",
      "score": 0.36492452025413513
    },
    {
      "name": "Electrical engineering",
      "score": 0.1305113136768341
    },
    {
      "name": "Engineering",
      "score": 0.10956060886383057
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126596746",
      "name": "Jagiellonian University",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210127875",
      "name": "Nvidia (United States)",
      "country": "US"
    }
  ]
}