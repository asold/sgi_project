{
  "title": "SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation",
  "url": "https://openalex.org/W3162696321",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2004947742",
      "name": "Bing Li",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097981924",
      "name": "Cheng Zheng",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2004383988",
      "name": "Silvio Giancola",
      "affiliations": [
        "King Abdullah University of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1983562721",
      "name": "Bernard Ghanem",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2004947742",
      "name": "Bing Li",
      "affiliations": [
        "University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097981924",
      "name": "Cheng Zheng",
      "affiliations": [
        "University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2004383988",
      "name": "Silvio Giancola",
      "affiliations": [
        "University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1983562721",
      "name": "Bernard Ghanem",
      "affiliations": [
        "University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6682849275",
    "https://openalex.org/W6685069058",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6687484953",
    "https://openalex.org/W6779125415",
    "https://openalex.org/W2724892359",
    "https://openalex.org/W2158131535",
    "https://openalex.org/W6730722863",
    "https://openalex.org/W764651262",
    "https://openalex.org/W6677174981",
    "https://openalex.org/W6790382439",
    "https://openalex.org/W2951142428",
    "https://openalex.org/W6634713522",
    "https://openalex.org/W3047167401",
    "https://openalex.org/W2088692258",
    "https://openalex.org/W3043718281",
    "https://openalex.org/W2798976292",
    "https://openalex.org/W2887479417",
    "https://openalex.org/W2981760444",
    "https://openalex.org/W3110399695",
    "https://openalex.org/W2974621909",
    "https://openalex.org/W6760224077",
    "https://openalex.org/W3039666984",
    "https://openalex.org/W6765299845",
    "https://openalex.org/W6761127344",
    "https://openalex.org/W3100388886",
    "https://openalex.org/W6746192494",
    "https://openalex.org/W2990100797",
    "https://openalex.org/W3044777232",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W6778520545",
    "https://openalex.org/W181676623",
    "https://openalex.org/W303181297",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2997188997",
    "https://openalex.org/W6640195962",
    "https://openalex.org/W2750772846",
    "https://openalex.org/W3046305366",
    "https://openalex.org/W3013965544",
    "https://openalex.org/W2938428612",
    "https://openalex.org/W2737996100",
    "https://openalex.org/W6674868227",
    "https://openalex.org/W6684838134",
    "https://openalex.org/W3140046250",
    "https://openalex.org/W6910779650",
    "https://openalex.org/W2993567085",
    "https://openalex.org/W1808303360",
    "https://openalex.org/W3107587338",
    "https://openalex.org/W2113221323",
    "https://openalex.org/W2900731076",
    "https://openalex.org/W3097139398",
    "https://openalex.org/W6640300118",
    "https://openalex.org/W3045125647",
    "https://openalex.org/W6634902294",
    "https://openalex.org/W2968798208",
    "https://openalex.org/W6777239595",
    "https://openalex.org/W6788305448",
    "https://openalex.org/W2737234477",
    "https://openalex.org/W3175172372",
    "https://openalex.org/W1578985305",
    "https://openalex.org/W4394671432",
    "https://openalex.org/W2563098792",
    "https://openalex.org/W3109027771",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W2962771259",
    "https://openalex.org/W2168538937",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W4287592659",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W3199986655",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3106932526",
    "https://openalex.org/W3109154950",
    "https://openalex.org/W3107668149",
    "https://openalex.org/W3175645231",
    "https://openalex.org/W2981983525",
    "https://openalex.org/W2971686478",
    "https://openalex.org/W2963231572",
    "https://openalex.org/W3175649884",
    "https://openalex.org/W3023852305",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2153101587",
    "https://openalex.org/W3034321406",
    "https://openalex.org/W4297043848",
    "https://openalex.org/W3109646990",
    "https://openalex.org/W2770902190",
    "https://openalex.org/W2954258401",
    "https://openalex.org/W2964156315",
    "https://openalex.org/W3116959466",
    "https://openalex.org/W2560474170",
    "https://openalex.org/W3010090948",
    "https://openalex.org/W1932072062",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W3186096360",
    "https://openalex.org/W3142353750",
    "https://openalex.org/W3035641096",
    "https://openalex.org/W3182475810",
    "https://openalex.org/W2963652593",
    "https://openalex.org/W3034314779",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W2098500213",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2989637772",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3178825327",
    "https://openalex.org/W3088360840",
    "https://openalex.org/W2171857946",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W1578285471",
    "https://openalex.org/W2751023760",
    "https://openalex.org/W3109908659",
    "https://openalex.org/W4287662905",
    "https://openalex.org/W2984858187",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3107284470",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W4246130809",
    "https://openalex.org/W3109379931",
    "https://openalex.org/W4295312788"
  ],
  "abstract": "We propose a novel scene flow estimation approach to capture and infer 3D motions from point clouds. Estimating 3D motions for point clouds is challenging, since a point cloud is unordered and its density is significantly non-uniform. Such unstructured data poses difficulties in matching corresponding points between point clouds, leading to inaccurate flow estimation. We propose a novel architecture named Sparse Convolution-Transformer Network (SCTN) that equips the sparse convolution with the transformer. Specifically, by leveraging the sparse convolution, SCTN transfers irregular point cloud into locally consistent flow features for estimating spatially consistent motions within an object/local object part. We further propose to explicitly learn point relations using a point transformer module, different from exiting methods. We show that the learned relation-based contextual information is rich and helpful for matching corresponding points, benefiting scene flow estimation. In addition, a novel loss function is proposed to adaptively encourage flow consistency according to feature similarity. Extensive experiments demonstrate that our proposed approach achieves a new state of the art in scene flow estimation. Our approach achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene Flow respectively, which significantly outperforms previous methods by large margins.",
  "full_text": "SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation\nBing Li, Cheng Zheng, Silvio Giancola, Bernard Ghanem\nKing Abdullah University of Science and Technology\n{bing.li, cheng.zheng, silvio.giancola, Bernard.Ghanem}@kaust.edu.sa\nAbstract\nWe propose a novel scene Ô¨Çow estimation approach to cap-\nture and infer 3D motions from point clouds. Estimating 3D\nmotions for point clouds is challenging, since a point cloud is\nunordered and its density is signiÔ¨Åcantly non-uniform. Such\nunstructured data poses difÔ¨Åculties in matching correspond-\ning points between point clouds, leading to inaccurate Ô¨Çow\nestimation. We propose a novel architecture named Sparse\nConvolution-Transformer Network (SCTN) that equips the\nsparse convolution with the transformer. SpeciÔ¨Åcally, by\nleveraging the sparse convolution, SCTN transfers irregular\npoint cloud into locally consistent Ô¨Çow features for estimat-\ning continuous and consistent motions within an object/local\nobject part. We further propose to explicitly learn point rela-\ntions using a point transformer module, different from exiting\nmethods. We show that the learned relation-based contextual\ninformation is rich and helpful for matching corresponding\npoints, beneÔ¨Åting scene Ô¨Çow estimation. In addition, a novel\nloss function is proposed to adaptively encourage Ô¨Çow consis-\ntency according to feature similarity. Extensive experiments\ndemonstrate that our proposed approach achieves a new state\nof the art in scene Ô¨Çow estimation. Our approach achieves an\nerror of 0.038 and 0.037 (EPE3D) on FlyingThings3D and\nKITTI Scene Flow respectively, which signiÔ¨Åcantly outper-\nforms previous methods by large margins.\nIntroduction\nUnderstanding 3D dynamic scenes is critical to many real-\nworld applications such as autonomous driving and robotics.\nScene Ô¨Çow is the 3D motion of points in a dynamic scene,\nwhich provides low-level information for scene understand-\ning (Vedula et al. 1999; Liu, Qi, and Guibas 2019; Geiger\net al. 2013). The estimation of the scene Ô¨Çow can be a build-\ning block for more complex applications and tasks such as\n3D object detection (Shi et al. 2020), segmentation (Thomas\net al. 2019) and tracking (Qi et al. 2020). However, many\nprevious scene Ô¨Çow methods estimate the 3D motion from\nstereo or RGB-D images. With the increasing popularity of\npoint cloud data, it is desirable to estimate 3D motions di-\nrectly from 3D point clouds.\nRecent methods e.g.,(Wu et al. 2020; Liu, Qi, and Guibas\n2019; Wei et al. 2021; Puy, Boulch, and Marlet 2020; Wang\net al. 2021; Li et al. 2021) propose deep neural networks to\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\npc1\npc2\n(a) point clouds (b) FLOT (c) Ours\nFigure 1: Illustrating the advantage of our SCTN in fea-\nture extraction, where Ô¨Årst and second rows indicate the two\npoint clouds, (b) and (c) visualize their features extracted by\nFLOT and our SCTN, respectively. Circles‚Éù or triangles ‚ñ≥\nin (b)(c) indicate a pair of corresponding points between the\npoint clouds, respectively. ‚Éù and ‚ñ≥are not corresponding\nto each other, however, their features extracted by FLOT are\nimproperly similar, which are less discriminative and would\nlead to inaccurate predicted Ô¨Çows. In contrast, our SCTN\nextracts locally consistent while discriminative features.\nlearn scene Ô¨Çow from point clouds in an end-to-end way,\nwhich achieves promising estimation performance. How-\never, estimating scene Ô¨Çow from point clouds is still chal-\nlenging. In particular, existing methods (Liu, Qi, and Guibas\n2019; Puy, Boulch, and Marlet 2020) extract feature for each\npoint by aggregating information from its local neighbor-\nhood. However, such extracted features are not discrimina-\ntive enough to matching corresponding points between point\nclouds (see Figure 1), leading to inaccurate Ô¨Çow estima-\ntion, since the feature extraction of these methods ignore two\nfacts. First, the density of points is signiÔ¨Åcantly non-uniform\nwithin a point cloud. It is non-trivial to learn point features\nthat are simultaneously favorable for both points from dense\nregions and those from sparse regions. Second, due to Lidar\nand object motions, the density of points within an object\noften varies at the temporal dimension, leading that the ge-\nometry patterns of corresponding local regions are inconsis-\ntent between consecutive point clouds. As a result, extracted\nfeatures, that are aggregated from only local regions with-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n1254\nout modeling point relations, is insufÔ¨Åcient for scene Ô¨Çow\nestimation.\nAnother challenge lies in that most previous methods have\nto train a model on the synthetic dataset to estimate scene\nÔ¨Çow for real-world data. However, there exists domain shift\nbetween the synthetic dataset and the real-world one. For\nexample, most objects in the synthetic dataset are rigid and\nundergo rigid motions, while real-world data contains many\nnon-rigid objects whose motions are consistent within local\nobject parts, rather than the whole object. Consequently, the\nperformance of the trained model is degraded when handling\nreal-world data. Yet, most recent work (Liu, Qi, and Guibas\n2019; Puy, Boulch, and Marlet 2020; Wu et al. 2020) do not\nexplicitly constrain the estimated Ô¨Çows. We would like to\nenforce the predicted Ô¨Çows in a local region of an object to\nbe consistent and smooth in 3D space.\nIn this work, we resort to feature representations and\nloss functions to estimate accurate scene Ô¨Çow from point\nclouds. In particular, we explore novel feature represen-\ntations (information) that would help to infer an accurate\nand locally consistent scene Ô¨Çows. We therefore propose\na Sparse Convolution-Transformer Network (SCTN) which\nincorporates the merits of dual feature representations pro-\nvided by our two proposed modules. In particular, to ad-\ndress the issue of spatially non-uniform and temporally\nvarying density of dynamic point clouds, we propose a\nV oxelization-Interpolation based Feature Extraction (VIFE)\nmodule. VIFE extracts features from voxelized point clouds\nrather than original ones, and then interpolates features for\neach point, which encourages to generate locally consis-\ntent Ô¨Çow features. To furture improve discriminability of ex-\ntracted features from the VIFE module, we propose to ad-\nditionally model point relations in the feature space, such\nthat extracted features capture important contextual infor-\nmation. Inspired by impressive performance of transformer\nin object detection tasks (Carion et al. 2020), we propose a\nPoint Transformer-based Feature Extraction (PTFE) module\nto explicitly learn point relations based on transformer for\ncapturing complementary information.\nIn addition, we propose a spatial consistency loss function\nwith a new architecture that equips stop-gradient for train-\ning. The loss adaptively controls the Ô¨Çow consistency ac-\ncording to the similarity of point features. Our experiments\ndemonstrate that our method signiÔ¨Åcantly outperforms state-\nof-the art approaches on standard scene Ô¨Çow datasets: Fly-\ningThings3D (N.Mayer et al. 2016a) and KITTI Scene Flow\n(Menze, Heipke, and Geiger 2018).\nContributions. Our contributions are fourfold:\n1. Instead of designing convolution kernels, our VIFE mod-\nule leverages simple operators ‚Äì voxelization and inter-\npolation for feature extraction, showing such smoothing\noperator is effective to extract local consistent while dis-\ncriminative features for scene Ô¨Çow estimation.\n2. Our PTFE module shows that explicitly modeling point\nrelations can provide rich contextual information and\nis helpful for matching corresponding points, beneÔ¨Åting\nscene Ô¨Çow estimation. We are the Ô¨Årst to introduce trans-\nformer for scene Ô¨Çow estimation.\n3. We propose a new consistency loss equipping stop-\ngradient-based architecture that helps the model trained\non synthetic dataset well adapt to real data, by controlling\nspatial consistency of estimated Ô¨Çows.\n4. We propose a novel network that outperforms the state-\nof-the-art methods with remarkable margins on both Fly-\ningThings3D and KITTI Scene Flow benchmarks.\nRelated Work\nOptical Flow. Optical Ô¨Çow estimation is deÔ¨Åned as the task\nof predicting the pixels motions between consecutive 2D\nvideo frames. Optical Ô¨Çow is a fundamental tool for 2D\nscene understanding, that have been extensively studied in\nthe literature. Traditional methods (Horn and Schunck 1981;\nBlack and Anandan 1993; Zach, Pock, and Bischof 2007;\nWeinzaepfel et al. 2013; Brox, Bregler, and Malik 2009;\nRanftl, Bredies, and Pock 2014) address the problem of esti-\nmating optical Ô¨Çow as an energy minimization problem, that\ndoes not require any training data. Dosovitskiy et al. (Doso-\nvitskiy et al. 2015) proposed a Ô¨Årst attempt for an end-to-end\nmodel to solve optical Ô¨Çow based on convolution neural net-\nwork (CNN). Inspired by this work, many CNN-based stud-\nies have explored data-driven approaches for optical Ô¨Çow\n(Dosovitskiy et al. 2015; Mayer et al. 2016; Ilg et al. 2017;\nHui, Tang, and Loy 2018; Hui and Loy 2020; Sun et al. 2018;\nTeed and Deng 2020b).\nScene Flow from Stereo and RGB-D Videos. Estimat-\ning scene Ô¨Çow from stereo videos have been studied for\nyears (Chen et al. 2020; V ogel, Schindler, and Roth 2013;\nWedel et al. 2008; Ilg et al. 2018; Jiang et al. 2019; Teed and\nDeng 2020a). Many works estimate scene Ô¨Çow by jointly es-\ntimating stereo matching and optical Ô¨Çow from consecutive\nstereo frames (N.Mayer et al. 2016b). Similar to optical Ô¨Çow,\ntraditional methods formulate scene Ô¨Çow estimation as an\nenergy minimization problem (Huguet and Devernay 2007;\nWedel et al. 2008). Recent works estimate scene Ô¨Çow from\nstereo video using neural networks (Chen et al. 2020). For\nexample, networks for disparity estimation and optical Ô¨Çow\nare combined in (Ilg et al. 2018; Ma et al. 2019). Similarly,\nother works (Quiroga et al. 2014; Sun, Sudderth, and PÔ¨Åster\n2015) explore scene Ô¨Çow estimation from RGB-D video.\nScene Flow on Point Clouds. Inspired by\nFlowNet (Dosovitskiy et al. 2015), FlowNet3D (Liu,\nQi, and Guibas 2019) propose an end-to-end network to\nestimate 3D scene Ô¨Çow from raw point clouds. Different\nfrom traditional methods (Dewan et al. 2016; Ushani et al.\n2017), FlowNet3D (Liu, Qi, and Guibas 2019) is based on\nPointNet++ (Qi et al. 2017b), and propose a Ô¨Çow embedding\nlayer to aggregate the information from consecutive point\nclouds and extract scene Ô¨Çow with convolutional layers.\nFlowNet3D++ (Wang et al. 2020) improves the accuracy\nof FlowNet3D by incorporating geometric constraints.\nHPLFlowNet (Gu et al. 2019) projects point clouds into\npermutohedral lattices, and then estimates scene Ô¨Çow using\nBilateral Convolutional Layers. Inspired by the successful\noptical Ô¨Çow method PWC-Net (Sun et al. 2018), PointPWC\n(Wu et al. 2020) estimates scene Ô¨Çow in a coarse-to-Ô¨Åne\nfashion, introducing cost volume, upsampling, and warping\n1255\nFigure 2: Overall framework of our SCTN approach. Given two consecutive point clouds, the V oxelization-Interpolation Feature\nExtraction (VIFE) extracts features from voxelized point clouds and then projects back the voxel features into point features.\nThese point features are fed into the Point Transformer Feature Extraction (PTFE) module to explicitly learn point relations.\nWith fused features of VIFE and PTFE module, SCTN computes point correlations between the point clouds and predicts Ô¨Çows.\nmodules for the point cloud processing. The most related\nrecent work to our approach is FLOT (Puy, Boulch, and\nMarlet 2020). FLOT addresses the scene Ô¨Çow as a matching\nproblem between corresponding points in the consecutive\nclouds and solve it using optimal transport. Our method\ndiffers from FLOT (Puy, Boulch, and Marlet 2020) in\ntwo aspects. First, our method explicitly explores more\nsuitable feature representation that facilitate the scene Ô¨Çow\nestimation. Second, our method is trained to enforce the\nconsistency of predicted Ô¨Çows for local-region points from\nthe same object, which is ignored in FLOT (Puy, Boulch,\nand Marlet 2020). Recently, Gojcic et al. (Gojcic et al.\n2021) explore weakly supervised learning for scene Ô¨Çow es-\ntimation using labels of ego motions as well as ground-truth\nforeground and background masks. Other works (Wu et al.\n2020; Kittenplon, Eldar, and Raviv 2021; Mittal, Okorn,\nand Held 2020) study unsupervised/self-supervised learning\nfor scene Ô¨Çow estimation on point clouds, proposing\nregularization losses that enforces local spatial smoothness\nof predicted Ô¨Çows. These losses are directly constraining\npoints in a local region to have similar Ô¨Çows, but are not\nfeature-aware.\n3D Deep Learning. Many works have introduced deep\nrepresentation for point cloud classiÔ¨Åcation and segmenta-\ntion (Qi et al. 2017b; Thomas et al. 2019; Zhang, Hua, and\nYeung 2019; Liu et al. 2020; Wu, Qi, and Fuxin 2019; Li\net al. 2018; Lei, Akhtar, and Mian 2020; Hu et al. 2022;\nZhao et al. 2021). Qi et al. (Qi et al. 2017a) propose PointNet\nthat learns point feature only from point positions. Point-\nNet++ (Qi et al. 2017b) extends PointNet by aggregating\ninformation from local regions. Motivated by PointNet++,\nmany works (Zhang, Hua, and Yeung 2019; Thomas et al.\n2019; Lei, Akhtar, and Mian 2020; Li et al. 2018) design\nvarious local aggregation functions for point cloud classiÔ¨Å-\ncation and segmentation. Different from these point-based\nconvolutions, (Wu et al. 2015; Wang et al. 2017) transform\npoint cloud into voxels, such that typical 3D convolution can\nbe applied. However, such voxel-based convolution suffers\nfrom expensive computational and memory cost as well as\ninformation loss during voxelization. Liu et al. (Liu et al.\n2019) combine PointNet (Qi et al. 2017a) with voxel-based\nconvolution to reduce the memory consumption. For the\nsake of efÔ¨Åcient learning, researchers have explored sparse\nconvolution for point cloud segmentation (Xie et al. 2020;\nChoy, Gwak, and Savarese 2019) which shows impressive\nperformance. Tang et al. (Tang et al. 2020) propose to com-\nbine PointNet with sparse convolution for large-scale point\ncloud segmentation. Differently, we not only leverage vox-\nelization and interpolation for feature extraction, but also\nexplicitly model point relations to provide complementary\ninformation.\nMethodology\nProblem DeÔ¨Ånition. Given two consecutive point cloudsPt\nand Pt+1, scene Ô¨Çow estimation is to predict the 3D motion\nÔ¨Çow of each point from Pt to Pt+1. Let pt\ni be the 3D coor-\ndinates of i-th point in Pt = {pt\ni}nP\ni=1. Like previous work\n(Puy, Boulch, and Marlet 2020), we predict scene Ô¨Çow based\non the correlations of each point pair between Pt and Pt+1.\nGiven a pair of points pt\ni ‚ààPt and pt+1\nj ‚ààPt+1, the corre-\nlation of the two points is computed as follows:\nC(pt\ni;pt+1\nj ) =\n(Ft\ni)T ¬∑Ft+1\nj\n‚à•Ft\ni‚à•2‚à•Ft+1\nj ‚à•2\n(1)\nwhere Ft+1\nj is the feature of pt+1\nj , and ‚à•¬∑‚à•2 is the L2 norm.\nPoint feature F is the key to computing the correlation\nC(pt\ni;pt+1\nj ) which further plays an important role in scene\nÔ¨Çow estimation. Hence, it is desirable to extract effective\npoint features that enable point pairs in corresponding re-\ngions to achieve higher correlation values between Pt and\nPt+1. However, different from point cloud segmentation and\nclassiÔ¨Åcation that focus on static point cloud, scene Ô¨Çow es-\ntimation operates on dynamic ones, which poses new chal-\nlenges for feature extraction in two aspects. For example,\nthe input point clouds of scene Ô¨Çow are not only irregular\nand unordered, but also its density is spatially non-uniform\nand temporally varying, as discussed in previous sections.\nOur goal is to extract locally consistent while discrimina-\ntive features for points, so as to achieve accurate Ô¨Çow esti-\nmation. Different from exiting methods directly extracting\n1256\npoint feature from the neighborhood in original point cloud,\nwe proposed two feature extraction modules for scene Ô¨Çow\nestimation. The V oxelization-Interpolation based Feature\nExtraction (VIFE) is proposed to address the issue of point\ncloud‚Äôs non-uniform density. With the features extracted by\nVIFE, Point Transformer based Feature Extraction (PTFE)\nfurther enhance feature discriminability by modeling point\nrelations globally. Since the two kinds of features provide\ncomplementary information, we fuse these features, such\nthat the fused features provide proper correspondences to\npredict accurate scene Ô¨Çows.\nOverview. Figure 2 illustrates the overall framework of\nour approach, that takes two consecutive point cloudsPt and\nPt+1 as inputs and predict the scene Ô¨Çow from Pt to Pt+1.\nFirst, the VIFE module extracts features for voxel points\nfrom voxelized point clouds, and projects back the voxel\nfeatures into the original 3D points to obtain locally consis-\ntent point features. Second, our PTFE module improves the\npoint feature representation by modeling relation between\npoints. Third, with features extracted by VIEF and PTFE\nmodule, we calculate the correlations of points between Pt\nand Pt+1, where a sinkhorn algorithm (Puy, Boulch, and\nMarlet 2020; Cuturi 2013; Chizat et al. 2018) is leveraged to\npredict the Ô¨Çows. We train our method with an extra regular-\nizing loss to enforce spatial consistency of predicted Ô¨Çows.\nVoxelization-Interpolation Based Feature\nExtraction\nAs mentioned in previous sections, the density of consec-\nutive point clouds is spatially non-uniform and temporally\nvarying, posing difÔ¨Åculties in feature extraction. To address\nthe issue, we propose the V oxelization-Interpolation based\nFeature Extraction (VIFE) module. VIFE Ô¨Årst voxelizes the\nconsecutive point clouds into voxels. As illustrated in Fig-\nure 2, the spatial non-uniform distributions and temporally\nvariations of points are reduced to some extent.\nAfter that, VIFE conducts convolutions on voxel points\nrather than all points of the point cloud, and then interpolate\nfeatures for each point. We argue that such simple operators\ni.e.,voxelization and interpolation, ensure points in a local\nneighborhood to have smoother features, ideally leading to\nconsistent Ô¨Çows in space.\nVoxel feature extraction. We then leverage a U-Net\n(Ronneberger, Fischer, and Brox 2015) architecture network\nto extract feature from voxelized point clouds, where con-\nvolution can be many types of point cloud convolutions\nsuch as pointnet++ used in FLOT (Puy, Boulch, and Marlet\n2020). Here, we adopt sparse convolution e.g.,Minkowski\nEngine (Choy, Gwak, and Savarese 2019) for efÔ¨Åciency.\nMore details are available in our supplementary material.\nPoint feature interpolation. We project back the voxel\nfeatures into point feature FS\ni for point pi. In particular, we\ninterpolate the point features from the K closest voxels fol-\nlowing equation (2). Nv(pi) represents the set of K nearest\nneighboring non-empty voxels for the point pi, vk ‚ààRC\nrepresents the feature of k-th closest non-empty voxel and\ndik the Euclidian distance between the point pi and the cen-\nter of the k-th closest non-empty voxel.\nFS\ni =\nP\nk‚ààNv(pi) d‚àí1\nik ¬∑vk\nP\nk‚ààNv(pi) d‚àí1\nik\n(2)\nWe observed that close points are encouraged to have sim-\nilar features, which helps our method generate consistent\nÔ¨Çows for these points. This is favorable for local object parts\nor rigid objects with dense densities and consistent Ô¨Çows\n(e.g.,LiDAR points on a car at close range).\nPoint Transformer Based Feature Extraction\nOur VIFE module adopt aggressive downsampling to obtain\na large receptive Ô¨Åeld and low computation cost. However,\naggressive downsampling inevitably loses some important\ninformation (Tang et al. 2020). In such case, the features of\npoints with large information loss are disadvantageous for\nestimating their scene Ô¨Çow. To address this issue, we ex-\nplicitly exploit point relations as a complementary informa-\ntion on top of the point feature extracted with VIFE. Recent\nwork (Zhu et al. 2021; Zhao, Jia, and Koltun 2020; Carion\net al. 2020) employ transformer and self-attention to model\ninternal relation in the features, achieving impressive per-\nformance in image tasks such as detection and recognition.\nSimilar trend appeared in point cloud classiÔ¨Åcation and seg-\nmentation (Guo et al. 2021; Zhao et al. 2021; Engel, Be-\nlagiannis, and Dietmayer 2020) showing the effectiveness\nof transformer in 3D. Inspired by these work, we resort to\ntransformer for capturing point relation information as the\npoint feature.\nIn an autonomous navigation scenario, point clouds rep-\nresent complete scenes, with small object such as cars and\ntrucks, but also large structure such as buildings and walls.\nThe scene Ô¨Çows in such a large scene do not only depend on\nthe aggregation from a small region, but rather a large one.\nAs a results, we refrain in building a transformer for the lo-\ncal neighborhood as it would restrict the receptive Ô¨Åeld or re-\nquire deeper model (i.e.,increase the memory). Instead, our\ntransformer module learns the relation of each point to all\nother points, such that the transformer can adaptively cap-\nture the rich contextual information from a complete scene.\nFormally, given a point pi, we consider every points in P\nas query and key elements. Our transformer module builds a\npoint feature representation for pi by adaptively aggregating\nthe features of all points based on self-attention:\nFR\ni =\nnPX\nj=1\nAi;j ¬∑gv(FS\nj ;Gj) (3)\nwhere gv is the a learnable function (e.g.,linear function),\nAi;j is an attention deÔ¨Åning a weight of pj to pi, Gj is the\npositional encoding feature of pj.\nAs pointed in literature (Zhao, Jia, and Koltun 2020; Car-\nion et al. 2020), the positional encoding feature can pro-\nvide important information for the transformer. The posi-\ntion encoding in recent transformer work (Zhao et al. 2021)\nencodes the relative point position to neighbors for point\ncloud classiÔ¨Åcation or segmentation. Different from those\ntasks, the task of scene Ô¨Çow is to Ô¨Ånd correspondences be-\ntween consecutive point clouds. Thus, we argue that an ab-\nsolute position provides sufÔ¨Åcient information to estimate\n1257\nPoint Positions Point Features\nùëî!: Linear ùëî\": Linear ùëî#: Linear\nMLP\nSoftMax\nG ùêÖùë∫\nFigure 3: Details of our point transformer based feature ex-\ntraction module (PTFE). ‚äóand ‚äïcorrespond to matrix mul-\ntiplication and addition operations, respectively.\nthe scene Ô¨Çow. Therefore, given a point p, our position en-\ncoding function encodes its absolute position pj:\nGj = \u001e(pj) (4)\nwhere \u001e is a MLP layer. Using absolute positions reduce\ncomputational cost, compared with using relative positions.\nWe calculate an attention Ai;j as the similarity between\nthe features of pi and pj in an embedding space. The simi-\nlarity is estimated using features and position information:\nAi;j ‚àùexp((gq(FS\ni ;Gi))T ¬∑gk(FS\nj ;Gj)\nca\n) (5)\nwhere gq(¬∑;¬∑) and gk(¬∑;¬∑) are the learnable mapping func-\ntions to project feature into an embedding space, and ca is\nthe output dimension of gq(¬∑;¬∑) or gk(¬∑;¬∑). Ai;j is further\nnormalized such that P\nj Ai;j = 1. The architecture of our\ntransformer module is illustrated in Figure 3.\nFlow Prediction\nSince the point feature from our VIFE and PTFE modules\nprovide complementary information, we fuse the two kinds\nof features through skip connection for each point, i.e.,Fi =\nFS\ni + FR\ni . By feeding the fused point features into Eq. 1,\nwe compute the correlations of all pairs C(Pt;Pt+1) =\n{C(pt\ni;pt+1\nj )}between the two consecutive point clouds.\nWith the estimated point correlations, we adopt the\nSinkhorn algorithm to estimate soft correspondences and\npredict Ô¨Çows for Pt, following FLOT.\nTraining Losses\nWe train our model to regress the scene Ô¨Çow in a supervised\nfashion, on top of which we propose a Feature-aware Spatial\nConsistency loss, named ‚ÄúFSC loss‚Äù, that enforces similar\nfeatures to have similar Ô¨Çow. The FSC loss provides a better\ngeneralization and transfer capability between the training\nand testing datasets.\nSupervised loss. We deÔ¨Åne in Equation (6) our super-\nvised loss Es that minimize the L1-norm difference be-\ntween the estimated Ô¨Çow and the ground truth Ô¨Çow for the\n{ùëù! \t, ùëù\" }\n30\n25\nz direction\npoint cloud 1\n20\n15\n10\n30\n25\nz direction\npoint cloud 2\n20\n15\n10\nùêπ! \t, ùêπ\"\nùêπ! \t, ùêπ\" Similarity ùë†(ùêπ! , ùêπ\" )\nFlow\nEstimator ùë¢! \t, ùë¢\"\nFSC Loss\nstop-grad\nshare\nFeature\nExtractor\nFeature\nExtractor\n30\n25\nz direction\npoint cloud 1\n20\n15\n10\n30\n25\nz direction\npoint cloud 2\n20\n15\n10\npoint cloud 1\npoint cloud 2\nFigure 4: Stop gradient for the FSC loss. We extract the\nÔ¨Çow from a neighboring point as well as the similarity be-\ntween their features. We optimize the FSC loss without\nback-propagating the gradients to the features from the sim-\nilarity branch to avoid degenerate cases.\nnon-occluded points. u‚àó\ni and ui are respectively the ground-\ntruth and predicted motion Ô¨Çow for the point pi ‚ààP and\nmi is a binary indicator for the non-occlusion of this point,\ni.e.,mi = 0if pi is occluded, otherwise mi = 1.\nEs =\nNX\ni\nmi‚à•ui ‚àíu‚àó\ni ‚à• (6)\nFeature-aware Spatial Consistency (FSC) loss.Given a\nlocal region, points from the same object usually has consis-\ntent motions, resulting in similar Ô¨Çows. To model such phe-\nnomena, we propose a consistency loss that ensures points\nwithin an object/local object part to have similar predicted\nÔ¨Çows. Yet, object annotations are not necessarily available.\nInstead, we propose to control Ô¨Çow consistency according to\nfeature similarity. That is, given a local region, if two points\nare of larger feature similarity, they are of the higher proba-\nbility that belongs to the same object. In particular, given a\npoint pi with predicted Ô¨Çow ui and its local neighborhood\nN(pi), we enforce the Ô¨Çow uj of pj ‚ààN(pi) to be similar\nto ui, if the feature Fi of pj is similar to Fj of pj. Formally,\nwe deÔ¨Åne the FSC loss as follows:\nEc =\nNX\ni=1\n1\nK\nX\npj‚ààN(pi)\ns(Fi;Fj) ¬∑‚à•ui ‚àíuj‚à•2 (7)\nwhere the similarity function s(Fi;Fj) of pi and pj is de-\nÔ¨Åned as 1‚àíexp(‚àí(Fi)T ¬∑Fj=\u001c) with \u001c being a temperature\nhyper-parameter, Kis the number of points in N(pi).\nA naive implementation of the FSC loss would inevitably\nlead to degenerate cases. In particular, the FSC loss is a prod-\nuct of two objectives: (i) a similarity s(Fi;Fj) between the\nfeatures Fi and Fj and (ii) a difference ‚à•ui ‚àíuj‚à•1 between\ntheir Ô¨Çows. The scope of this loss is to train the Ô¨Çows to be\nsimilar if they have similar features. However, to minimize\nthe FSC loss, the model would make the features Fj and Fi\nbe orthogonal (i.e.,F j ¬∑Fi = 0), such that s(Fj;Fi) = 0\n(i.e.,Ec = 0). Obviously, it is against our aim.\nTo circumvent this limitation, we propose a stop-gradient\nfor the FSC loss, taking inspiration form recent advances in\n1258\nDataset Method EPE3D(m) # Acc3DS \" Acc3DR \" Outliers #\nFlyingThings3D\nFlowNet3D (Liu, Qi, and Guibas 2019) 0.114 0.412 0.771 0.602\nHPLFlowNet (Gu et al. 2019) 0.080 0.614 0.855 0.429\nPointPWC (Wu et al. 2020) 0.059 0.738 0.928 0.342\nEgoFlow (Tishchenko et al. 2020) 0.069 0.670 0.879 0.404\nFLOT (Puy, Boulch, and Marlet 2020) 0.052 0.732 0.927 0.357\nSCTN (ours) 0.038 0.847 0.968 0.268\nKITTI\nFlowNet3D (Liu, Qi, and Guibas 2019) 0.177 0.374 0.668 0.527\nHPLFlowNet (Gu et al. 2019) 0.117 0.478 0.778 0.410\nPointPWC (Wu et al. 2020) 0.069 0.728 0.888 0.265\nEgoFlow (Tishchenko et al. 2020) 0.103 0.488 0.822 0.394\nFLOT (Puy, Boulch, and Marlet 2020) 0.056 0.755 0.908 0.242\nSCTN (ours) 0.037 0.873 0.959 0.179\nTable 1: Comparison with the state-of-the-art on FlyingThings3D and KITTI. Best results in bold. Our proposed model SCTN\nreaches highest performances in all metrics.\n(a) Input point clouds\n (b) Ground-truth Ô¨Çows\n (c) FLOT\n (d) Ours\nFigure 5: Qualitative comparison results. Green points indicate the Ô¨Årst point cloud in (a), and blue points indicate the second\npoint cloud in (a)(b)(c)(d). In (b)(c)(d), green points are the ones in the Ô¨Årst point cloud warped by correctly predicted Ô¨Çows,\nwhile red points are the ones warped by incorrect Ô¨Çows (the Ô¨Årst point cloud + incorrect scene Ô¨Çow whose EPE3D >0.1m).\nself-supervised learning (Chen and He 2020). As illustrated\nin Figure 4, our architecture stops the propagation of the gra-\ndient in the branch extracting the feature similarity. By such\narchitecture, our FSC loss avoids optimizing the features,\nwhile optimizing solely the Ô¨Çows similarities‚à•uj ‚àíui‚à•1 for\nneighboring points with similar features.\nExperiments\nDataset. We conduct our experiments on two datasets\nthat are widely used to evaluate scene Ô¨Çow. FlyingTh-\nings3D (N.Mayer et al. 2016a) is a large-scale synthetic\nstereo video datasets, where synthetic objects are selected\nfrom ShapeNet (Chang et al. 2015) and randomly assigned\nvarious motions. We generate 3D point clouds and ground\ntruth scene Ô¨Çows with their associated camera parameters\nand disparities. Following the same preprocessing as in (Puy,\nBoulch, and Marlet 2020; Gu et al. 2019; Wu et al. 2020), we\nrandomly sample 8192 points and remove points with cam-\nera depth greater than 35 m. We use the same 19640=3824\npairs of point cloud (training/testing) used in the related\nworks (Puy, Boulch, and Marlet 2020; Gu et al. 2019; Wu\net al. 2020). KITTI Scene Flow (Menze, Heipke, and Geiger\n2018; Choy, Gwak, and Savarese 2019) is a real-world Lidar\nscan dataset for scene Ô¨Çow estimation from the KITTI au-\ntonomous navigation suite. Following the preprocessing of\n(Gu et al. 2019), we leverage 142 point cloud pairs of 8192\npoints for testing. For a fair comparison, we also remove\nground points by discarding points whose height is lower\nthan ‚àí1:4m, following the setting of existing methods (Puy,\nBoulch, and Marlet 2020; Wu et al. 2020; Gu et al. 2019).\nEvaluation Metrics. To evaluate the performance of our\napproach, we adopt the standard evaluation metrics used in\nthe related methods (Puy, Boulch, and Marlet 2020; Wu et al.\n2020), described as follows: TheEPE3D (m) (3D end-point-\nerror) is calculated by computing the average L2 distance\nbetween the predicted and GT scene Ô¨Çow, in meters. This is\nour main metric. The Acc3DS is a strict version of the accu-\nracy which estimated as the ratio of points whose EPE3D <\n0.05 m or relative error<5%. The Acc3DR is a relaxed accu-\nracy which is calculated as the ratio of points whose EPE3D\n<0.10m or relative error <10%. The Outliers is the ratio of\npoints whose EPE3D >0.30m or relative error >10%.\nImplementation Details. We implement our method in\nPyTorch (Paszke et al. 2019). We train our method on Fly-\ningThing3D then evaluate on FlyingThing3D and KITTI.\nWe minimize a cumulative loss E = Es + \u0015Ec with \u0015 =\n0:30 a weight that scale the losses. We use the Adam opti-\nmizer (Kingma and Ba 2014) with an initial learning rate of\n10‚àí3, which is dropped to 10‚àí4 after the 50th epoch. First,\nwe train for 40 epochs only using the supervised loss. Then\nwe continue the training for 20 epochs with both the super-\nvision loss and the FSC loss, for a total on 60 epochs. We\n1259\nVIFE PTFE FSC loss EPE3D(m) # Acc3DS \"\n3 0.045 0.835\n3 3 0.042 0.853\n3 3 0.040 0.863\n3 3 3 0.037 0.873\nTable 2: Ablation for SCTN. We further analyse the perfor-\nmances of our three components: VIFE, PTFE and FSC loss\non KITTI. We highlight in bold the best performances.\nuse a voxel size of resolution 0:07m.\nRuntime. We evaluate the running time of our method.\nTable 3 reports the evaluated time compared with recent\nstate-of-the-art methods. FLOT (Puy, Boulch, and Marlet\n2020) is the most related work to our method, since we both\nadopt point-wise correlations to generate predicted Ô¨Çows.\nOur method consumes lower running time than FLOT, al-\nthough the transformer module is equipped.\nQuantitative Evaluation\nWe compare our approach with recent deep-learning-based\nmethods including FlowNet3D (Liu, Qi, and Guibas 2019),\nHPLFlowNet (Gu et al. 2019), PointPWC (Wu et al. 2020)\nand FLOT (Puy, Boulch, and Marlet 2020). These methods\nare state-of-the-art in scene Ô¨Çow estimation from point cloud\ndata and do not leverge any additional labels such as ground-\ntruth ego motions or instance segmentation.\nResults on FlyingThings3D. We train and evaluate our\nmodel on the FlyThings3D datasets. As shown in Table 1,\nour method outperforms all methods in every metrics by a\nsigniÔ¨Åcant margin. It is worth noting that our method obtains\nan EPE3D metric below4cm, with a relative improvement of\n26:9% and 35:5% over the most recent methods FLOT (Puy,\nBoulch, and Marlet 2020) and PointPWC (Wu et al. 2020),\nrespectively. The performance shows that our method is ef-\nfective in predicting Ô¨Çows with high accuracy.\nResults on KITTI without Fine-tune. Following the\ncommon practice (Puy, Boulch, and Marlet 2020; Wu et al.\n2020), we train our model on FlyingThings3D and directly\ntest the trained model on KITTI Scene Flow dataset, with-\nout any Ô¨Åne-tuning, to evaluate the generalization capabil-\nity of our method. We report in Table 1 the highest accu-\nracy of scene Ô¨Çow estimation on KITTI Scene Flow dataset\nfor our SCTN method. Again, we reduce the EPE3D met-\nric below 4cm, with a 33.9% relative improvement over\nFLOT (Puy, Boulch, and Marlet 2020). In the Acc3DS met-\nrics, our method outperforms both FLOT (Puy, Boulch, and\nMarlet 2020) and PointPWC (Wu et al. 2020) by13:5% and\n16:6% respectively. These results highlight the capability of\nour method to generalize well on real-world datasets.\nQualitative Evaluation\nTo qualitatively evaluate the quality of our scene Ô¨Çow pre-\ndictions, we visualize the predicted scene Ô¨Çow and ground-\ntruth one in Figure 5. Since FLOT (Puy, Boulch, and Marlet\n2020) is most related to our method, we compare the quali-\ntative performances of our SCTN with FLOT (Puy, Boulch,\nMethod Runtime (ms)\nFLOT (Puy, Boulch, and Marlet 2020) 389.3\nSCTN (ours) 242.7\nTable 3: Running time comparisons. The runtime of FLOT\nand our SCTN are evaluated on a single GTX2080Ti GPU.\nWe used the ofÔ¨Åcial implementation of FLOT.\nand Marlet 2020).\nPoints in a local region from the same object usually have\nsimilar ground-truth Ô¨Çows. Yet, FLOT introduces predic-\ntion errors in local regions, highlighting the inconsistency in\nthe scene Ô¨Çow predictions. For example, FLOT inaccurately\npredicts scene Ô¨Çow for some regions in the background, even\nthough those points have similar Ô¨Çows, as shown in Figure 5.\nIn contrast, our method is more consistent in the prediction\nfor points in the same object, achieving better performance,\ne.g.,for the background objects with complex structure.\nAblation Study\nTo study the roles of the proposed VIFE, PTFE and FSC\nloss, we ablate each proposed component of our model and\nevaluate their performance on KITTI. For all the experi-\nments, we follow the same training procedure than in the\nmain results. Table 2 reports the evaluation results.\nVIFE module. Table 2 shows that our approach with\nthe sole VIEF convolution module already outperforms the\nstate-of-the-art methods listed in Table 1. Different from\nexisting methods directly applying convolution on origi-\nnal point clouds, our VIFE extracts feature from voxelized\npoint cloud, which reduces the non-uniform density of point\ncloud, while ensuring that points in a local region have con-\nsistent features, to some extent. The results show that such\nfeatures are favorable for scene Ô¨Çow estimation.\nPTFE module. Compared with only using VIFE module,\nadding PTFE improves both metrics on KITTI as reported in\nthe third row in Table 2. For example, EPE3D is improved\nby 11.1%, compared with only using the VIEF module. Our\nPIFE module explicitly learns point relations, which pro-\nvides rich contextual information and helps to match cor-\nresponding points even for objects with complex structures.\nFSC loss. Table 2 shows that adding the FSC loss helps\nto achieve better scene Ô¨Çow estimation on KITTI. Our FSC\nloss improves the generalization capability of our method.\nConclusion\nWe present a Sparse Convolution-Transformer Network\n(SCTN) for scene Ô¨Çow estimation. Our SCTN leverages the\nVIFE module to transfer irregular point cloud into locally\nsmooth Ô¨Çow features for estimating spatially consistent mo-\ntions in local regions. Our PTFE module learns rich con-\ntextual information via explicitly modeling point relations,\nwhich is helpful for matching corresponding points and ben-\neÔ¨Åts scene Ô¨Çow estimation. A novel FSC loss is also pro-\nposed for training SCTN, improving the generalization abil-\nity of our method. Our approach achieves state-of-the-art\nperformances on FlyingThings3D and KITTI datasets.\n1260\nAcknowledgments\nThis work was supported by the King Abdullah University\nof Science and Technology (KAUST) OfÔ¨Åce of Sponsored\nResearch through the Visual Computing Center (VCC) fund-\ning. We thank Hani Itani for his constructive suggestions and\nhelp.\nReferences\nBlack, M. J.; and Anandan, P. 1993. A framework for the\nrobust estimation of optical Ô¨Çow. In ICCV, 231‚Äì236. IEEE.\nBrox, T.; Bregler, C.; and Malik, J. 2009. Large displace-\nment optical Ô¨Çow. In CVPR, 41‚Äì48. IEEE.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213‚Äì229.\nChang, A. X.; Funkhouser, T.; Guibas, L.; Hanrahan, P.;\nHuang, Q.; Li, Z.; Savarese, S.; Savva, M.; Song, S.; Su,\nH.; Xiao, J.; Yi, L.; and Yu, F. 2015. ShapeNet: An\nInformation-Rich 3D Model Repository. Technical Report\narXiv:1512.03012 [cs.GR], arXiv preprint.\nChen, X.; and He, K. 2020. Exploring Simple Siamese Rep-\nresentation Learning. arXiv:2011.10566.\nChen, Y .; Van Gool, L.; Schmid, C.; and Sminchisescu, C.\n2020. Consistency Guided Scene Flow Estimation. In\nECCV, 125‚Äì141. Springer.\nChizat, L.; Peyr¬¥e, G.; Schmitzer, B.; and Vialard, F.-X. 2018.\nScaling algorithms for unbalanced optimal transport prob-\nlems. Mathematics of Computation, 87(314): 2563‚Äì2609.\nChoy, C.; Gwak, J.; and Savarese, v. 2019. 4D Spatio-\nTemporal ConvNets: Minkowski Convolutional Neural Net-\nworks. In CVPR, 3075‚Äì3084.\nCuturi, M. 2013. Sinkhorn distances: lightspeed computa-\ntion of optimal transport. In NeurIPS, volume 2, 4.\nDewan, A.; Caselitz, T.; Tipaldi, G. D.; and Burgard, W.\n2016. Rigid scene Ô¨Çow for 3d lidar scans. In IROS, 1765‚Äì\n1770. IEEE.\nDosovitskiy, A.; Fischer, P.; Ilg, E.; Hausser, P.; Hazirbas,\nC.; Golkov, V .; Van Der Smagt, P.; Cremers, D.; and Brox,\nT. 2015. Flownet: Learning optical Ô¨Çow with convolutional\nnetworks. In ICCV, 2758‚Äì2766.\nEngel, N.; Belagiannis, V .; and Dietmayer, K. 2020. Point\nTransformer. arXiv:2011.00931.\nGeiger, A.; Lenz, P.; Stiller, C.; and Urtasun, R. 2013. Vision\nmeets robotics: The kitti dataset. The International Journal\nof Robotics Research, 32(11): 1231‚Äì1237.\nGojcic, Z.; Litany, O.; Wieser, A.; Guibas, L. J.; and Birdal,\nT. 2021. Weakly Supervised Learning of Rigid 3D Scene\nFlow. In CVPR, 5692‚Äì5703.\nGu, X.; Wang, Y .; Wu, C.; Lee, Y . J.; and Wang, P. 2019.\nHPLFlowNet: Hierarchical Permutohedral Lattice FlowNet\nfor Scene Flow Estimation on Large-scale Point Clouds. In\nCVPR.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.;\nand Hu, S.-M. 2021. PCT: Point Cloud Transformer.\narXiv:2012.09688.\nHorn, B. K.; and Schunck, B. G. 1981. Determining optical\nÔ¨Çow. ArtiÔ¨Åcial intelligence, 17(1-3): 185‚Äì203.\nHu, W.; Pang, J.; Liu, X.; Tian, D.; Chia-Wen, L.; and An-\nthony, V . 2022. Graph signal processing for geometric data\nand beyond: Theory and applications. IEEE Trans. Multi-\nmedia.\nHuguet, F.; and Devernay, F. 2007. A variational method for\nscene Ô¨Çow estimation from stereo sequences. In ICCV, 1‚Äì7.\nIEEE.\nHui, T.-W.; and Loy, C. C. 2020. LiteFlowNet3: Resolving\nCorrespondence Ambiguity for More Accurate Optical Flow\nEstimation. In ECCV, 169‚Äì184. Springer.\nHui, T.-W.; Tang, X.; and Loy, C. C. 2018. LiteÔ¨Çownet:\nA lightweight convolutional neural network for optical Ô¨Çow\nestimation. In CVPR, 8981‚Äì8989.\nIlg, E.; Mayer, N.; Saikia, T.; Keuper, M.; Dosovitskiy, A.;\nand Brox, T. 2017. Flownet 2.0: Evolution of optical Ô¨Çow\nestimation with deep networks. In CVPR, 2462‚Äì2470.\nIlg, E.; Saikia, T.; Keuper, M.; and Brox, T. 2018. Occlu-\nsions, motion and depth boundaries with a generic network\nfor disparity, optical Ô¨Çow or scene Ô¨Çow estimation. InECCV,\n614‚Äì630.\nJiang, H.; Sun, D.; Jampani, V .; Lv, Z.; Learned-Miller, E.;\nand Kautz, J. 2019. Sense: A shared encoder network for\nscene-Ô¨Çow estimation. In ICCV, 3195‚Äì3204.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKittenplon, Y .; Eldar, Y . C.; and Raviv, D. 2021. Flow-\nStep3D: Model Unrolling for Self-Supervised Scene Flow\nEstimation. In CVPR, 4114‚Äì4123.\nLei, H.; Akhtar, N.; and Mian, A. 2020. Spherical kernel for\nefÔ¨Åcient graph convolution on 3d point clouds. TPAMI.\nLi, R.; Lin, G.; He, T.; Liu, F.; and Shen, C. 2021. HCRF-\nFlow: Scene Flow From Point Clouds With Continuous\nHigh-Order CRFs and Position-Aware Flow Embedding. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 364‚Äì373.\nLi, Y .; Bu, R.; Sun, M.; Wu, W.; Di, X.; and Chen, B.\n2018. PointCNN: Convolution on \u001f-transformed points. In\nNeurIPS, 828‚Äì838.\nLiu, X.; Qi, C. R.; and Guibas, L. J. 2019. FlowNet3D:\nLearning Scene Flow in 3D Point Clouds. In CVPR.\nLiu, Z.; Hu, H.; Cao, Y .; Zhang, Z.; and Tong, X. 2020.\nA closer look at local aggregation operators in point cloud\nanalysis. In ECCV, 326‚Äì342. Springer.\nLiu, Z.; Tang, H.; Lin, Y .; and Han, S. 2019. Point-V oxel\nCNN for EfÔ¨Åcient 3D Deep Learning. In NeurIPS.\nMa, W.-C.; Wang, S.; Hu, R.; Xiong, Y .; and Urtasun, R.\n2019. Deep Rigid Instance Scene Flow. In CVPR.\nMayer, N.; Ilg, E.; Hausser, P.; Fischer, P.; Cremers, D.;\nDosovitskiy, A.; and Brox, T. 2016. A large dataset to train\nconvolutional networks for disparity, optical Ô¨Çow, and scene\nÔ¨Çow estimation. In CVPR, 4040‚Äì4048.\nMenze, M.; Heipke, C.; and Geiger, A. 2018. Object Scene\nFlow. JPRS.\n1261\nMittal, H.; Okorn, B.; and Held, D. 2020. Just Go With the\nFlow: Self-Supervised Scene Flow Estimation. In CVPR.\nN.Mayer; E.Ilg; P.H ¬®ausser; P.Fischer; D.Cremers;\nA.Dosovitskiy; and T.Brox. 2016a. A Large Dataset\nto Train Convolutional Networks for Disparity, Optical\nFlow, and Scene Flow Estimation. In CVPR.\nN.Mayer; E.Ilg; P.H ¬®ausser; P.Fischer; D.Cremers;\nA.Dosovitskiy; and T.Brox. 2016b. A Large Dataset\nto Train Convolutional Networks for Disparity, Optical\nFlow, and Scene Flow Estimation. In CVPR.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. arXiv preprint arXiv:1912.01703.\nPuy, G.; Boulch, A.; and Marlet, R. 2020. FLOT: Scene\nFlow on Point Clouds Guided by Optimal Transport. In\nECCV.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet:\nDeep learning on point sets for 3d classiÔ¨Åcation and segmen-\ntation. In CVPR, 652‚Äì660.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nNet++: Deep Hierarchical Feature Learning on Point Sets in\na Metric Space. In NeurIPS, 5099‚Äì5108.\nQi, H.; Feng, C.; Cao, Z.; Zhao, F.; and Xiao, Y . 2020. P2B:\nPoint-to-box network for 3D object tracking in point clouds.\nIn CVPR, 6329‚Äì6338.\nQuiroga, J.; Brox, T.; Devernay, F.; and Crowley, J. 2014.\nDense semi-rigid scene Ô¨Çow estimation from rgbd images.\nIn ECCV, 567‚Äì582. Springer.\nRanftl, R.; Bredies, K.; and Pock, T. 2014. Non-local total\ngeneralized variation for optical Ô¨Çow estimation. In ECCV,\n439‚Äì454. Springer.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-\nvolutional networks for biomedical image segmentation. In\nMICCAI, 234‚Äì241. Springer.\nShi, S.; Guo, C.; Jiang, L.; Wang, Z.; Shi, J.; Wang, X.; and\nLi, H. 2020. Pv-rcnn: Point-voxel feature set abstraction for\n3d object detection. In CVPR, 10529‚Äì10538.\nSun, D.; Sudderth, E. B.; and PÔ¨Åster, H. 2015. Layered\nRGBD scene Ô¨Çow estimation. In CVPR, 548‚Äì556.\nSun, D.; Yang, X.; Liu, M.-Y .; and Kautz, J. 2018. PWC-\nNet: CNNs for Optical Flow Using Pyramid, Warping, and\nCost V olume. InCVPR.\nTang, H.; Liu, Z.; Zhao, S.; Lin, Y .; Lin, J.; Wang, H.; and\nHan, S. 2020. Searching EfÔ¨Åcient 3D Architectures with\nSparse Point-V oxel Convolution. InECCV.\nTeed, Z.; and Deng, J. 2020a. RAFT-3D: Scene\nFlow using Rigid-Motion Embeddings. arXiv preprint\narXiv:2012.00726.\nTeed, Z.; and Deng, J. 2020b. Raft: Recurrent all-pairs Ô¨Åeld\ntransforms for optical Ô¨Çow. In ECCV, 402‚Äì419. Springer.\nThomas, H.; Qi, C. R.; Deschaud, J.-E.; Marcotegui, B.;\nGoulette, F.; and Guibas, L. J. 2019. Kpconv: Flexible and\ndeformable convolution for point clouds. In ICCV, 6411‚Äì\n6420.\nTishchenko, I.; Lombardi, S.; Oswald, M. R.; and Pollefeys,\nM. 2020. Self-Supervised Learning of Non-Rigid Residual\nFlow and Ego-Motion. arXiv preprint arXiv:2009.10467.\nUshani, A. K.; Wolcott, R. W.; Walls, J. M.; and Eustice,\nR. M. 2017. A learning approach for real-time temporal\nscene Ô¨Çow estimation from lidar data. In ICRA, 5666‚Äì5673.\nIEEE.\nVedula, S.; Baker, S.; Rander, P.; Collins, R.; and Kanade,\nT. 1999. Three-dimensional scene Ô¨Çow. In ICCV, volume 2,\n722‚Äì729. IEEE.\nV ogel, C.; Schindler, K.; and Roth, S. 2013. Piecewise rigid\nscene Ô¨Çow. In ICCV, 1377‚Äì1384.\nWang, H.; Pang, J.; Lodhi, M. A.; Tian, Y .; and Tian, D.\n2021. FESTA: Flow Estimation via Spatial-Temporal At-\ntention for Scene Point Clouds. In CVPR, 14173‚Äì14182.\nWang, P.-S.; Liu, Y .; Guo, Y .-X.; Sun, C.-Y .; and Tong, X.\n2017. O-CNN: Octree-based Convolutional Neural Net-\nworks for 3D Shape Analysis. ACM Trans. Graph., 36(4):\n72:1‚Äì72:11.\nWang, Z.; Li, S.; Howard-Jenkins, H.; Prisacariu, V .; and\nChen, M. 2020. FlowNet3D++: Geometric Losses For Deep\nScene Flow Estimation. In WACV.\nWedel, A.; Rabe, C.; Vaudrey, T.; Brox, T.; Franke, U.; and\nCremers, D. 2008. EfÔ¨Åcient dense scene Ô¨Çow from sparse or\ndense stereo data. In ECCV, 739‚Äì751. Springer.\nWei, Y .; Wang, Z.; Rao, Y .; Lu, J.; and Zhou, J. 2021. PV-\nRAFT: Point-V oxel Correlation Fields for Scene Flow Esti-\nmation of Point Clouds. In CVPR.\nWeinzaepfel, P.; Revaud, J.; Harchaoui, Z.; and Schmid, C.\n2013. DeepFlow: Large displacement optical Ô¨Çow with deep\nmatching. In ICCV, 1385‚Äì1392.\nWu, W.; Qi, Z.; and Fuxin, L. 2019. Pointconv: Deep convo-\nlutional networks on 3d point clouds. In CVPR, 9621‚Äì9630.\nWu, W.; Wang, Z. Y .; Li, Z.; Liu, W.; and Fuxin, L. 2020.\nPointPWC-Net: Cost V olume on Point Clouds for (Self-) Su-\npervised Scene Flow Estimation. In ECCV, 88‚Äì107.\nWu, Z.; Song, S.; Khosla, A.; Yu, F.; Zhang, L.; Tang, X.;\nand Xiao, J. 2015. 3d shapenets: A deep representation for\nvolumetric shapes. In CVPR, 1912‚Äì1920.\nXie, S.; Gu, J.; Guo, D.; Qi, C. R.; Guibas, L.; and Litany,\nO. 2020. PointContrast: Unsupervised Pre-training for 3D\nPoint Cloud Understanding. In ECCV.\nZach, C.; Pock, T.; and Bischof, H. 2007. A duality based\napproach for realtime tv-l 1 optical Ô¨Çow. In Joint pattern\nrecognition symposium, 214‚Äì223. Springer.\nZhang, Z.; Hua, B.-S.; and Yeung, S.-K. 2019. ShellNet:\nEfÔ¨Åcient Point Cloud Convolutional Neural Networks Using\nConcentric Shells Statistics. In ICCV.\nZhao, H.; Jia, J.; and Koltun, V . 2020. Exploring Self-\nAttention for Image Recognition. In CVPR.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P.; and Koltun, V . 2021.\nPoint Transformer. In ICCV.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In ICLR.\n1262",
  "topic": "Point cloud",
  "concepts": [
    {
      "name": "Point cloud",
      "score": 0.8372206687927246
    },
    {
      "name": "Computer science",
      "score": 0.6934421062469482
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.5380231142044067
    },
    {
      "name": "Transformer",
      "score": 0.5320528149604797
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5196329355239868
    },
    {
      "name": "Optical flow",
      "score": 0.48551464080810547
    },
    {
      "name": "Algorithm",
      "score": 0.4481852054595947
    },
    {
      "name": "Computer vision",
      "score": 0.4012913703918457
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3932020664215088
    },
    {
      "name": "Artificial neural network",
      "score": 0.16530513763427734
    },
    {
      "name": "Image (mathematics)",
      "score": 0.15837544202804565
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210099236",
      "name": "Kootenay Association for Science & Technology",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I71920554",
      "name": "King Abdullah University of Science and Technology",
      "country": "SA"
    }
  ]
}