{
  "title": "SciFive: a text-to-text transformer model for biomedical literature",
  "url": "https://openalex.org/W3168090480",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2125495496",
      "name": "Long N. Phan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Anibal, James T.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228224306",
      "name": "Tran, Hieu",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Chanana, Shaurya",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Bahadroglu, Erol",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Peltekian, Alec",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5021705478",
      "name": "Altan-Bonnet Gregoire",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950279864",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2170189740",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W3156216837",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2765742249",
    "https://openalex.org/W2154142897",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2734608416",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2071879021",
    "https://openalex.org/W2898355739",
    "https://openalex.org/W2885185669"
  ],
  "abstract": "In this report, we introduce SciFive, a domain-specific T5 model that has been pre-trained on large biomedical corpora. Our model outperforms the current SOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation, relation extraction, natural language inference, and question-answering. We show that text-generation methods have significant potential in a broad array of biomedical NLP tasks, particularly those requiring longer, more complex outputs. Our results support the exploration of more difficult text generation tasks and the development of new methods in this area",
  "full_text": "Bioinformatics\ndoi.10.1093/bioinformatics/xxxxxx\nAdvance Access Publication Date: Day Month Y ear\nManuscript Category\nData and text mining\nSciFive: a text-to-text transformer model\nfor biomedical literature\nLong N. Phan 1,2,†\n, James T. Anibal2,†\n, Hieu Tran3, Shaurya Chanana4,\nErol Bahadıro ˘ glu2, Alec Peltekian1,2, Grégoire Altan-Bonnet 2\n1Department of Computer Sciences, Case Western University, Cleveland OH, USA.\n2ImmunoDynamics Section, Laboratory of Integrative Cancer Immunology, National Cancer Institute, Bethesda MD, USA.\n3University of Science, Vietnam National University, Ho Chi Minh City, Vietnam.\n4Natural Products Branch, National Cancer Institute, Bethesda MD, USA.\n†These authors contributed equally to this work.\nAssociate Editor: XXXXXXX\nReceived on XXXXX; revised on XXXXX; accepted on XXXXX\nAbstract\nMotivation: In 2019, researchers from Google released the Text-to-Text Transfer Transformer (T5) trained\non the \"Colossal Clean Crawled Corpus\" (C4) This approach achieved state-of-the-art (SOTA) results\non a diverse range of tasks related to natural language processing (NLP). In the last decade, NLP in\nbiomedicine has become more prominent (i.e. text mining of scientiﬁc literature, analysis of electronic\nhealth records). This development has created a need for NLP methods trained on corpora of biomedical\nliterature containing the dense technical language characteristic of scientiﬁc writing. In this report, we\nintroduce a T5-based model that has been successfully shifted into the biomedical domain.\nResults: In this report, we introduce SciFive, a domain-speciﬁc T5 model that has been pre-trained\non large biomedical corpora. Our model outperforms the current SOTA methods (i.e. BERT, BioBERT,\nBase T5) on tasks in named entity relation, relation extraction, natural language inference, and question-\nanswering. We show that text-generation methods have signiﬁcant potential in a broad array of biomedical\nNLP tasks, particularly those requiring longer, more complex outputs. Our results support further research\ninto biomedical text generation and the development of new methods in this area.\nAvailability: All checkpoints and pre-trained weights of SciFive are publicly available at\nhttps://console.cloud.google.com/storage/browser/sciﬁve. The sources code for self-supervised and ﬁne-\ntuned models is in https://github.com/justinphan3110/SciFive\nContact: gregoire.altan-bonnet@nih.gov\n1 Introduction\nBiomedical literature is widely accessible to the scientiﬁc community\nthrough databases such as Pubmed, PMC, and ScienceDirect. Within\nseconds, researchers can access millions of journal articles relating to\nan input query. Text generation tasks such as document summarization\nand question answering can allow researchers to quickly obtain important\ninformation from a large collection of papers, yet current methods\ngenerally underperform in these areas. Thus, new NLP methods are needed\nto parse the increasingly immense amounts of information.\n1.1 Related Work\nThe introduction of the transformer (Vaswani et al., 2017) marked\na signiﬁcant achievement for natural language processing. This is\ndemonstrated by the success of transformer-based architectures such as\nBERT (Devlin et al., 2018), which, at the time of publication, achieved\nstate-of-the-art (SOTA) results on common NLP tasks. Furthermore, the\nBERT model has been extended for domain-speciﬁc tasks in NLP. Domain-\nspeciﬁc language (i.e. biomedical language) is often challenging for NLP\nmodels because of the signiﬁcant differences in vocabulary compared\nto standard langauge corpora such as Wikipedia. To solve this problem,\nBERT models have been pre-trained for domain-speciﬁc tasks. With this\n© The Author 2021. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com 1\narXiv:2106.03598v1  [cs.CL]  28 May 2021\n2 Phan et al.\napproach, SOTA results were achieved in areas such as clinical notes,\nbiomedical literature, and general scientiﬁc literature.\n2 Approach\nBERT (Devlin et al., 2018) is not a uniﬁed transfer learning method\nbecause BERT-style models can only produce a single prediction for a\ngiven input. These models are simply not designed for text generation tasks\nsuch as question-answering or summarization. The text-to-text transfer\ntransformer (T5) model proposed by Raffel et al.(2019) overcomes this\nlimitation by outputting a string of text for each input, allowing for both\nquestion-answering, summarization and other tasks where a single output\nis generally insufﬁcient. In this report, we introduce SciFive, a pretrained,\ndomain-speciﬁc adaptation of the T5 model that is intended for tasks\nrelating to biomedical literature. We here outline two primary contributions\nof our work.\n(1) Our model achieves SOTA results on a variety of common\nclassiﬁcation tasks in biomedical NLP, including named entity recognition\n(NER) and relation-extraction (RE).\n(2) Second, our model can be extended to tasks requiring extended\noutputs and achieves superior results on BioAsq question-answering\nchallenges when compared to BioBERT, the current SOTA method to the\nbest of our knowledge (Lee et al., 2019)\n3 Unlabeled Dataset\nIn this section we will describe our biomedical unlabeled datasets which\nare used in the transfer learning pre-training stage. These large datasets\novercome the drawback of overﬁtting when building a language model in\nthe biomedical domain. (Ruder, 2017). For SciFive, we use two different\ncorpora of biomedical language in order to generalize our model within\nthe domain.\nPubMed Abstract 1: The PubMed database contains more than 32\nmillions citations and abstracts of biomedical literature. For the purpose\nof model pre-training, we use only the abstracts.\nPubMed Central (PMC)2: PMC is a corpus of free full-text articles in\nthe domain of biomedical and life sciences. We hypothesize that training\nthe language model with full-text articles can improve the learning in\nbiomedical context while still containing a generalized representation of\nnatural language overall.\n4 Methods\nHere, we describe our approach to implementing the SciFive model, which\nretains the original structure and parameters of the T5 model (Raffelet al.,\n2019).\n4.1 T5\nThe text-to-text transfer transformer (T5) model (Raffel et al., 2019) is\nhighly similar to the transformer-based encoder-decoder model introduced\nby Vaswani et al.(2017). Each encoder block consists of a self-attention\nlayer and a feed-forward neural network. Each decoder block consists of a\nself-attention layer, an encoder-decoder attention layer, and a feedforward\nneural network. There are, however, minor differences between T5\nand the transformer-based encoder-decoder model. For example, layer\nnormalization is applied between the components of each encoder block\n1 https://pubmed.ncbi.nlm.nih.gov\n2 https://www.ncbi.nlm.nih.gov/pmc\nand each decoder block. Compared to BERT (Devlin et al., 2018), the\naddition of the decoder block allows T5 to generate outputs that are\nsequences of text. T5 is pre-trained with self-supervision through a learning\nobjective called span-based language masking. (Raffel et al., 2019).\n4.2 SciFive\nSciFive follows the sequence-to-sequence encoder-decoder architecture\nproposed by Vaswani et al. (2017) and the T5 framework 3 released by\nRaffel et al. (2019). The original T5 work implemented ﬁve different\nmodel sizes - Small, Base, Large, 3B, and 11B. Due to limited computing\nresources, we will use only the base and large model for this study. The base\nand large models have 220 million parameters and 770 million parameters\nrespectively.\nTable 1. Corpus combinations for SciFive\nModel Corpus Combination\nT5 Raffel et al.(2019) C4\nSciFive(+pubmed) C4+pubmed\nSciFive(+pmc) C4+pmc\nSciFive(+pubmed+pmc) C4+pubmed+pmc\nWe ﬁrst initialized SciFive with the pre-trained weights from the base\nT5 model. We then re-trained SciFive on various combinations of the C4\ncorpus (Dodge et al., 2021), a corpus of PubMed abstracts, and a corpus of\nPMC full-text articles. We trained SciFive for extra 200k steps to optimize\nthe pre-trained weights from T5 in the context of biomedical literature. We\nalso trained a large version of the SciFive model, using 1.2 millions steps\n(200k additional steps compared to the regular model). With the provided\nTPU v2-8 on Google Colab, we used the self-supervised training setting\nrecommended by Raffel et al.(2019) with a batch size of 128 for the base\nmodel and 64 for the large model. We used a learning rate of 0.001 and\nsequence length 1024 tokens for both input and target as we noticed that\nunlabeled biomedical text during self-supervised training is long. For the\npurpose of generalization of biomedical text, we train SciFive on various\ncombinations of biomedical corpus as describe in Table 1.\n4.3 Input/Output Representation\n<M> expression and NF - kappa <M> CD28 requires reactive <M> by 5 - lipoxygenase\nIL - 2 gene expression and NF - kappa B activation through CD28 requires reactive\noxygen production by 5 - lipoxygenase\nIL - 2 gene <M> B activation through <M> oxygen production <M>\nFig. 1. An illustration on Span-based mask language modeling. For the input sentence,\nthe set of tokens \"IL\",\"-\",\"2\", \"kappa\", \"B\",...\"oxygen\", \"production\" is randomly chosen\nfor corruption, where consecutive tokens are counted as spans and replaced by a sentinel\nunique masked token <M>. The output sequence then consists of the concatenation of the\ndropped-out spans, sentinel tokens used to replace them in the input and the ﬁnal sentinel\ntoken.\n3 https://github.com/google-research/text-to-text-transfer-transformer\nSciFive: a text-to-text transformer model for biomedical literature 3\nConsistent with the original T5 model Raffel et al. (2019), SciFive\nconverts all of the biomedical tasks into a text-to-text format. During self-\nsupervised training, a text input sequence is given and the model will try\nto learn a target input going through a learning objective called span-based\nmask language modeling. Spans of text are randomly masked and the\ntarget sequence is predicted as a concatenation of the same sentinel tokens\nand the real masked spans. An illustration of span-based mask learning\nobjective is in Figure 1.\nDuring supervised training, a sequence of text for both input and target\nis given to the model for the purpose of learning to generate text. For\nexample, when performing Named-entity recognition (NER), we generate\nthe target sequence by prepending and appending a special token to the\nnamed entities in a sentence. The target sequence for Question Answering\ntask is the text corresponding to the answer for a given question (the\nquestion text is the input).\n4.4 Vocabulary\nFor every pre-trained language model (LMs), vocabulary plays a crucial\nrole, as these models attempt to derive effective contextualized word vector\nrepresentations from the training corpus. For SciFive, we use the Sentence\nPiece model (Kudo and Richardson, 2018) as a base vocabulary model.\nSentence Piece is used in all of our SciFive models because it extracts sub-\nwords that contain the semantic meaning of a sequence. This overcomes\nthe drawbacks of word-level tokenization and eliminates the need for an\nimmense vocabulary set.\n4.5 Multi-Task Learning\nSciFive is trained with a maximum likelihood objective using \"teacher\nforcing\" (Raffel et al., 2019) for all tasks, thereby enabling multi-task\nlearning. During supervised ﬁne-tuning, a task-speciﬁc token is prepended\nto the input sequence. In one example, we leverage this type of training for\nthe Named-entity recognition task. We believe that this strategy will boost\nperformance for biomedical NER by using the attention of each named\nentity across all the tasks. Figure 2 illustrates multi-task learning for our\nNER tasks.\n4.6 Fine-Tuning SciFive\nWe ﬁne-tuned SciFive on ﬁve categories of biomedical NLP tasks.\n(1) Named entity recognition (NER) involves predicting a predeﬁned\ncategory that describes a proper noun. For example “Lupus” may be\nclassiﬁed as “Disease\".\n(2) Relation Extraction (RE) involves identifying relationships within\ntext (i.e. gene-disease).\n(3) Natural Language Inference involves determining the validity of a\nhypothesis (i.e., True, False).\n(4) Document Classiﬁcation involves assigning a document to a\ncategory based on the text.\n(5) Question answering involves generating an answer if given a\nquestion and a sequence of text containing the answer to that question.\nWe ﬁne-tuned in both multi-tasking and single-task learning using the\nﬁnal checkpoints of our SciFive model, 200k steps for both base and large\nmodels. Similar to the setting during self-supervised training on TPU v2-8,\nwe choose the batch size of 128 and 64 for the base and large respectively\nwith learning rate 0.001. The input and output speciﬁcation setting for\neach task is described in Table 2.\n5 Results\nWe tested SciFive on 7 NER tasks, 5 RE asks, 1 inference task, 1 document\nclassiﬁcation task, and 3 question answering tasks. We then compared the\nSciFive results with the current SOTA on these tasks.\n5.1 Data\nWe describe here the datasets and the preprocessing techniques we used.\nIn most cases, we use the same preprocessing procedure as the current\nbaseline models (i.e. BioBERT from Lee et al. (2019) and BlueBERT\nfrom Peng et al.(2019)).\n5.1.1 Named Entity Recognition\nWe tested SciFive on 7 datasets commonly used for biomedical NER:\nNCBI disease (Do˘ ganet al., 2014), BC5CDR disease (Li et al., 2016),\nBC5CDR chemical (Liet al., 2016), BC4CHEMD (Krallingeret al., 2015),\nBC2GM (Smith et al., 2008), JNLPBA (Collier and Kim, 2004), and\nSpecies800 Paﬁlis et al.(2013). We follow the processing pipeline and the\ntrain/valid/test split similiar to Lee et al. (2019). For all NER tasks, we\nevaluate the performance of SciFive based on precision (P), recall (R), and\nF-1 score (F).\n5.1.2 Relation Extraction\nWe tested SciFive on 2 RE tasks: CHEMPROT (Islamaj Do˘ ganet al., 2019)\nand DDI (Herrero-Zazo et al., 2013). We follow the same preprocessing\ntechnique as Peng et al. (2019). We also evaluate the F1-scores of each\nclass in the two relation extraction corpus.\n5.1.3 Natural Language Inference\nTo assess the NLI capabilities of SciFive, we use the MedNLI datasets from\nMIMIC-III (Romanov and Shivade, 2018) with the same preprocessing\ntechnique and training/testing sets.\n5.1.4 Document Classiﬁcation\nWe use SciFive to classify documents from the HoC dataset (Bakeret al.,\n2015), evaluating the F1 score on the sample average in the same manner\nas Zhang et al.(2017).\n5.1.5 Question Answering\nQuestion Answering (QA) is perhaps the most important component of our\nassessment, as we expect a text-to-text model to vastly outperform BERT-\nlike models in this area. We test SciFive on the factoid questions from the\nBioASQ 4b, 5b, and 6b challenges Tsatsaroniset al.(2015). To preprocess\nthe BioASQ data, we use the same approach as Lee et al.(2019).\nUsing the same approach as the original T5, (Raffel et al., 2019),\nSciFive converts all problems into a text-to-text format. Therefore, we\ncannot use the same evaluation procedure as BioBERT. (Lee et al.,\n2019). BioBERT determines the ﬁnal answer for a question by taking\nthe highest scoring answer across all the snippets of text corresponding\nto that question. Our model outputs a sequence of text, not a probability\ndistribution, so we cannot determine our \"best\" answer in the same way as\nBioBERT. This key difference prevents us from evaluating strict accuracy\nas done by Lee et al. (2019), so we evaluate only the lenient accuracy\nfor each task. For a single question, SciFive answers questions using a\nsequence of text rather than probabilities for the start and end of the answer.\nSciFive uses each piece of context to answer that question individually. If\nSciFive answers correctly using one or more of the contextual snippets, we\nsay SciFive has answered the question correctly according to the lenient\naccuracy metric.\nTo evaluate our results, we rely on an expert assessment. SciFive\noutputs full-sentence answers that often do not correspond to the exact\nBioASQ answer provided for a given question, but, in many cases, these\n4 Phan et al.\nSciFive \nncbi_ner: Identification of APC2 , a homologue of the\nadenomatous polyposis coli tumour suppressor .\nbc5cdr_disease_ner: Selegiline - induced postural\nhypotension in Parkinson ' s disease : a longitudinal study on\nthe effects of drug withdrawal .\nbc5cdr_chem_ner: Selegiline - induced postural\nhypotension in Parkinson ' s disease : a longitudinal study\non the effects of drug withdrawal .\nSelegiline - induced entity* postural hypotension *entity in entity*\nParkinson ' s disease *entity : a longitudinal study on the effects of\ndrug withdrawal .\nentity* Selegiline *entity - induced postural hypotension in Parkinson ' s\ndisease : a longitudinal study on the effects of drug withdrawal \nIdentification of APC2 , a homologue of the entity*\nadenomatous polyposis coli tumour *entity suppressor .\n... ...\nFig. 2. An illustration about Multi-task learning in Name-entity Recognition Tasks\nTable 2. The input and target sequence length settings for each Self-supervised Learning, Name-entity Recognition, Relational Extraction, and Question Answering\ntask\nTask Dataset Entity type Number of entities Task Type Input Length Target Length\nSelf-Supervise Learning\nPubMed 1024 1024\nPMC 1024 1024\nPubMed+PMC 1024 1024\nName-entity Recognition\nNCBI Disease Disease 6881\nMulti-Task 512 512\nBC5CDR Disease Disease 19,665\nBC5CDR Chem Disease 12,694\nBC4CHEMD Chemical 15,411\nBC2HM Chemical 79,842\nJNLPBA Gene 20,703\nSpecies-800 Species 3708\nRelational Extraction Chemprot Protein-chemical 10,031 Single-Task 256 16\nDDI Biomedical relation 4,920 Single-Task 256 16\nDocument classiﬁcation HoC Biomedical Documents 1,580 Single-Task 256 64\nInference MedNLI Clinical pairs 14,049 Single-Task 256 12\nQuestion Answering\nBioASQ4-factoid Biomedical QA 488 Single-Task 512 128\nBioASQ5-factoid Biomedical QA 636 Single-Task 512 128\nBioASQ6-factoid Biomedical QA 779 Single-Task 512 128\nNotes: The number of entities is the sum of annotations, relations, documents, pairs, and question & answer pairs for each correspond task in the train, valid, and test sets.\nThe statistics from Lee et al.(2019), Peng et al.(2019), Habibi et al.(2017), and Zhu et al.(2018)\nanswers are still scientiﬁcally correct. For a meaningful assessment of\nQ/A results, the scientiﬁc accuracy must be considered rather than the\nphrasing of the answer. Table 4 shows several examples of SciFive answers\ncompared to BioBERT answers. It can be easily seen from these examples\nthat SciFive provides clearer, more complete answers than BioBERT.\n5.2 Experimental Results\nIn Table 5, we show the results of SciFive compared to the SOTA\napproaches. For NER, RE, NLI, and documentation classiﬁcation, we\ncompare the F1 scores obtained by SciFive to the F1 scores obtained by the\nSOTA method pre-BioBERT, BioBERT Leeet al.(2019), BlueBERT Peng\net al.(2019), BERT Devlinet al.(2018), and T5 Raffelet al.(2019). For the\nBioASQ tasks (Table 3), we compare the lenient accuracy of base SciFive\nonly with base T5 and base BioBERT due to the time required for thorough\nexpert assessment. It should be noted that BioBERT was the winner of these\nBioASQ challenges. We achieved SOTA results on 3/7 NER tasks, 2/2 RE\ntasks, 1/1 NLI tasks, and 3/3 question answering tasks (Table 5). We also\nachieved a near-SOTA result on the HoC document classiﬁcation task.\nBased on these results, we emphasize the following point: SciFive (both\nbase and large model) competitive results on classiﬁcation tasks while\nTable 3. Expert assessment result on Question Answering tasks (Lenient\nAccuracy)\nTask BioBERT T5 SciFive\n(PubMed+PMC)\nSciFive\n(PMC)\nSciFive\n(Pubmed)\nBioAsq 4b 57.14 85.06 87.66 85.71 88.31\nBioAsq 5b 64.83 86.21 86.21 88.28 88.28\nBioAsq 6b 57.52 75.82 75.16 79.08 72.55\nalso providing SOTA results on text generation tasks such as question-\nanswering. This is a signiﬁcant improvement over BERT-based models,\nwhich demonstrates weaker performances on question-answering tasks.\n6 Discussion\nWe used SciFive to explore the role of text generation models in broad-\nspectrum biomedical NLP, achieving SOTA results on a variety of tasks.\nThis is particularly true for question answering, where SciFive achieved\nSOTA results. Both T5 and SciFive signiﬁcantly outperformed BioBERT,\nhighlighting the value of text generation models in biomedical NLP.\nHowever, question answering is relatively simplistic compared to other\nSciFive: a text-to-text transformer model for biomedical literature 5\nTable 4. Example of answer generated from SciFive and BioBERT for QA tasks\nTask Question Text Answer\n4b\nWhat was the purpose of the\nFANTOM4 project?\nBioBert Mammalian Genomes 4 (FANTOM4)\nSciFive\nthe international functional annotation of the mammalian genomes 4\n(fantom4) research collaboration set out to better understand the\ntranscriptional network that regulates macrophage differentiation\nWhat is the RESID database? BioBert RESID\nSciFive\nthe resid database of protein modiﬁcations is a comprehensive collection\nof annotations and structures for protein modiﬁcations and cross-links\nincluding pre-, co-, and post-translational modiﬁcations.\n5b\nWhat is the role of gamma-secreatase\ncomplex in Alzheimer’s Disease?\nBioBert APH-1a\nSciFive it cleaves a precursor to create the amyloid beta peptide\nWhat is the function of BAX? BioBert mitochondrial\nSciFive\nbax, a central cell death regulator, is an indispensable gateway to mitochondrial\ndysfunction and a major proapoptotic member of the b-cell lymphoma 2 (bcl-2)\nfamily\n6b\nWhat is the function of the gene\nMDA5?\nBioBert RIG-1\nSciFive\nmelanoma differentiation-associated gene 5 (mda5) is a pattern recognition\nreceptor that recognizes cytoplasmic viral double-stranded rna (dsrna) and\ninitiates rapid innate antiviral responses.\nWhat is the function of HDAC\nproteins?\nBioBert Histone deacetylase\nSciFive histone deacetylases (hdacs) prevent the relaxation of chromatin, and\npositively or negatively regulate transcription.\ntext generation tasks. To fully examine the potential of text generation\nmodels in the context of domain-speciﬁc literature, SciFive will be applied\nto tasks such as document summarization and abstract generation.\nFrom our results, it can be seen that the SOTA results are split between\nthe various versions of SciFive. While we expected the Pubmed+PMC\nmodel to have the best performances given the mixture of abstracts and\nfull text articles, our results show that further study is needed to understand\nthe optimal nature of biomedical corpora.\n7 Conclusion\nIn this manuscript, we introduce SciFive, a domain-speciﬁc text-to-text\nmodel trained speciﬁcally for tasks involving biomedical literature. SciFive\nis effective for NER, RE, NLI, and question answering tasks, achieving\nSOTA or near-SOTA results in all cases. This outcome supports our\nconclusion that text-to-text (text generation) models are highly versatile\nand broadly applicable within domain-speciﬁc contexts. These models can\nbe used for common tasks and tasks which require a longer sequence of\ntext as an output (i.e. question answering). Our results suggest the need for\nfurther study of domain-speciﬁc text generation models applied to more\ndifﬁcult tasks such as a document summarization and abstract generation.\nFunding\nThis work has been supported by the Cancer Research Training Award\n(CRTA) through the National Cancer Institute to JTA and EB. This research\nwas supported by the Intramural Research Program of the NIH.\n6 Phan et al.\nTable 5. Test results in biomedical named entity recognition, relation extraction, document classiﬁcation, and inteference tasks\nBase Large\nMetrics SOTA Bert (base) T5 BlueBERT BioBert\nSciFive\n(PMC\n+PubMed)\nSciFive\n(PubMed)\nSciFive\n(PMC) T5 BlueBERT BioBert\nSciFive\n(PMC\n+PubMed)\nSciFive\n(PubMed)\nSciFive\n(PMC)\nNER\nDisease\nNCBI disease\nP 84.12 87.18 - 88.22 88.28 86.28 88.65 87.48 - 87.70 88.10 88.52 87.64\nR 87.19 89.93 - 91.25 89.30 89.71 90.14 90.14 - 89.90 90.14 89.82 89.30\nF 88.60 85.63 88.54 - 89.71 88.79 87.96 89.39 88.78 - 88.79 89.11 89.17 88.46\nBC5CDR Disease\nP 81.97 85.95 - 86.47 86.67 86.53 86.48 84.28 - - 86.73 86.30 87.01\nR 82.48 87.73 - 87.84 88.01 88.37 87.99 87.38 - - 88.46 87.67 88.24\nF 86.23 82.41 86.83 86.6 87.15 87.33 87.44 87.23 86.31 83.8 - 87.59 86.98 87.62\nDrug/chem\nBC5CDR Chemical\nP 90.94 93.30 - 93.68 93.89 94.01 94.09 93.44 - 93.18 94.13 93.98 93.86\nR 91.38 93.92 - 93.26 94.80 94.69 94.28 95.02 - 92.09 95.39 95.36 95.37\nF 93.31 91.16 93.61 93.5 93.47 94.34 94.35 94.18 94.22 91.7 92.63 94.76 94.66 94.61\nBC4CHEMD\nP 91.19 90.57 - 92.80 92.50 92.71 92.01 91.19 - 93.00 92.89 92.19 91.98\nR 88.92 88.90 - 91.92 91.53 91.35 91.87 88.76 - 92.35 91.17 91.73 91.15\nF 91.14 90.04 89.73 - 92.36 92.01 92.02 92.07 89.96 - 92.67 92.03 91.96 91.56\nGene/protein\nBC2GM\nP 81.17 82.43 - 84.32 84.44 84.97 83.66 82.63 - 84.78 84.20 83.81 83.95\nR 82.42 82.17 - 85.12 83.89 82.89 83.04 82.10 - 85.25 83.48 83.39 83.20\nF 81.69 81.79 82.29 - 84.72 84.16 83.92 84.29 82.36 - 85.01 83.84 83.60 83.57\nJNLPBA\nP 69.57 69.35 - 72.24 70.36 70.91 70.65 71.04 - - 71.08 71.36 77.68\nR 81.20 80.61 - 83.56 80.96 80.96 81.99 81.31 - - 81.62 81.46 77.42\nF 78.58 74.94 74.56 - 77.49 75.29 75.60 75.89 75.83 - - 75.99 76.08 77.55\nSPECIES Species-800\nP 69.35 72.18 - 72.80 73.47 73.84 72.68 72.69 - - 72.55 73.08 74.09\nR 74.05 76.59 - 75.36 79.33 79.45 79.83 76.84 - - 77.33 78.08 78.71\nF 74.98 71.63 74.32 - 74.06 76.29 76.55 76.08 74.66 - - 74.86 75.50 76.33\nRE\nChemProt\nP 74.80 76.02 81 77.02 82.59 84.24 82.35 84.04 - - 81.99 81.31 83.58\nR 56.00 71.60 89.01 75.90 91.21 93.96 92.31 86.81 - - 95.06 95.60 95.06\nF 64.10 73.74 84.82 72.5 76.46 86.68 88.83 87.04 85.41 74.4 - 88.04 87.88 88.95\nDDI\nP - - 82.68 - - 81.96 83.15 82.75 83.87 - - 84.22 83.88 83.00\nR - - 81.41 - - 83.04 83.15 82.33 82.84 - - 82.84 83.45 84.27\nF 72.9 - 82.04 79.4 - 82.50 83.15 82.54 83.35 79.9 - 83.52 83.67 83.63\nDoC HoC\nP - - 85.55 - - 86.27 86.18 86.08 86.02 - - 86.11 86.35 86.36\nR - - 85.42 - - 86.29 86.17 86.20 85.95 - - 86.21 86.31 86.39\nF* 81.5 - 85.22 85.3 - 85.99 85.89 85.83 85.68 87.3 - 85.87 86.03 86.08\nNLI MedNLI Acc 73.5 - 83.90 84.0 - 84.88 85.30 84.25 83.8 83.8 - 86.57 86.36 86.08\nNotes: P for Precision; R for Recall; F for F1 score; F* is F1 score on sample average. Best scores are in bold, second best scores are underlined. Baseline result and SOTA from Leeet al.(2019) and Peng et al.(2019)\nSciFive: a text-to-text transformer model for biomedical literature 7\nReferences\nBaker, S., Silins, I., Guo, Y ., Ali, I., Högberg, J., Stenius, U., and Korhonen, A.\n(2015). Automatic semantic classiﬁcation of scientiﬁc literature according to the\nhallmarks of cancer. Bioinformatics, 32(3), 432–440.\nCollier, N. and Kim, J.-D. (2004). Introduction to the bio-entity recognition task at\nJNLPBA. InProceedings of the International Joint Workshop on Natural Language\nProcessing in Biomedicine and its Applications (NLPBA/BioNLP), pages 73–78,\nGeneva, Switzerland. COLING.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. (2018). BERT: pre-training of deep\nbidirectional transformers for language understanding. CoRR, abs/1810.04805.\nDodge, J., Sap, M., Marasovic, A., Agnew, W., Ilharco, G., Groeneveld, D., and\nGardner, M. (2021). Documenting the english colossal clean crawled corpus.\nCoRR, abs/2104.08758.\nDo˘ gan, R. I., Leaman, R., and Lu, Z. (2014). Ncbi disease corpus: A resource\nfor disease name recognition and concept normalization. Journal of Biomedical\nInformatics, 47, 1 – 10.\nHabibi, M., Weber, L., Neves, M., Wiegandt, D., and Leser, U. (2017). Deep\nlearning with word embeddings improves biomedical named entity recognition.\nBioinformatics (Oxford, England), 33, i37–i48.\nHerrero-Zazo, M., Segura-Bedmar, I., Martínez, P., and Declerck, T. (2013). The\nddi corpus: An annotated corpus with pharmacological substances and drug–drug\ninteractions. Journal of Biomedical Informatics, 46(5), 914–920.\nIslamaj Do˘ gan, R., Kim, S., Chatr-aryamontri, A., Wei, C.-H., Comeau, D. C.,\nAntunes, R., Matos, S., Chen, Q., Elangovan, A., Panyam, N. C., Verspoor, K.,\nLiu, H., Wang, Y ., Liu, Z., Altınel, B., Hüsünbeyi, Z. M., Özgür, A., Fergadis, A.,\nWang, C.-K., Dai, H.-J., Tran, T., Kavuluru, R., Luo, L., Steppi, A., Zhang, J., Qu,\nJ., and Lu, Z. (2019). Overview of the BioCreative VI Precision Medicine Track:\nmining protein interactions and mutations for precision medicine.Database, 2019.\nbay147.\nKrallinger, M., Rabal, O., Leitner, F., Vazquez, M., Salgado, D., lu, Z., Leaman,\nR., Lu, Y ., Ji, D., Lowe, D., Sayle, R., Batista-Navarro, R., Rak, R., Huber, T.,\nRocktäschel, T., Matos, S., Campos, D., Tang, B., Xu, H., and Valencia, A. (2015).\nThe chemdner corpus of chemicals and drugs and its annotation principles.Journal\nof Cheminformatics, 7, S2.\nKudo, T. and Richardson, J. (2018). Sentencepiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text processing.CoRR,\nabs/1808.06226.\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., and Kang, J. (2019).\nBiobert: a pre-trained biomedical language representation model for biomedical\ntext mining. CoRR, abs/1901.08746.\nLi, J., Sun, Y ., Johnson, R., Sciaky, D., Wei, C.-H., Leaman, R., Davis, A. P.,\nMattingly, C., Wiegers, T., and lu, Z. (2016). Biocreative v cdr task corpus: a\nresource for chemical disease relation extraction. Database, 2016, baw068.\nPaﬁlis, E., Frankild, S., Fanini, L., Faulwetter, S., Pavloudi, C., Vasileiadou, A.,\nArvanitidis, C., and Jensen, L. (2013). The species and organisms resources for\nfast and accurate identiﬁcation of taxonomic names in text. PLoS ONE, 8.\nPeng, Y ., Yan, S., and Lu, Z. (2019). Transfer learning in biomedical natural language\nprocessing: An evaluation of BERT and elmo on ten benchmarking datasets.CoRR,\nabs/1906.05474.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li,\nW., and Liu, P. J. (2019). Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. CoRR, abs/1910.10683.\nRomanov, A. and Shivade, C. (2018). Lessons from natural language inference in the\nclinical domain. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 1586–1596, Brussels, Belgium. Association\nfor Computational Linguistics.\nRuder, S. (2017). An overview of multi-task learning in deep neural networks.CoRR,\nabs/1706.05098.\nSmith, L., Tanabe, L., Ando, R., Kuo, C., Chung, I.-F., Hsu, C., Lin, Y ., Klinger, R.,\nFriedrich, C., Ganchev, K., Torii, M., Liu, H., Haddow, B., Struble, C., Povinelli,\nR., Vlachos, A., Baumgartner Jr, W., Hunter, L., Carpenter, B., and Wilbur, W.\n(2008). Overview of biocreative ii gene mention recognition. Genome Biology, 9.\nTsatsaronis, G., Balikas, G., Malakasiotis, P., Partalas, I., Zschunke, M., Alvers, M.,\nWeißenborn, D., Krithara, A., Petridis, S., Polychronopoulos, D., Almirantis, Y .,\nPavlopoulos, J., Baskiotis, N., Gallinari, P., Artieres, T., Ngonga Ngomo, A.-C.,\nHeino, N., Gaussier, E., Barrio-Alvers, L., and Paliouras, G. (2015). An overview\nof the bioasq large-scale biomedical semantic indexing and question answering\ncompetition. BMC Bioinformatics, 16, 138.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,\nL., and Polosukhin, I. (2017). Attention is all you need. CoRR, abs/1706.03762.\nZhang, Y ., Zheng, W., Lin, H., Wang, J., Yang, Z., and Dumontier, M. (2017).\nDrug–drug interaction extraction via hierarchical RNNs on sequence and shortest\ndependency paths. Bioinformatics, 34(5), 828–835.\nZhu, H., Paschalidis, I. C., and Tahmasebi, A. (2018). Clinical concept extraction\nwith contextual word embedding. CoRR, abs/1810.10566.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7706770896911621
    },
    {
      "name": "Text generation",
      "score": 0.7102527618408203
    },
    {
      "name": "Inference",
      "score": 0.6726990342140198
    },
    {
      "name": "Transformer",
      "score": 0.6710879802703857
    },
    {
      "name": "Question answering",
      "score": 0.6559363007545471
    },
    {
      "name": "Natural language processing",
      "score": 0.652576208114624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6415320038795471
    },
    {
      "name": "Biomedical text mining",
      "score": 0.611594021320343
    },
    {
      "name": "Relationship extraction",
      "score": 0.599374532699585
    },
    {
      "name": "Language model",
      "score": 0.5154572129249573
    },
    {
      "name": "Relation (database)",
      "score": 0.49405553936958313
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.44239017367362976
    },
    {
      "name": "Information extraction",
      "score": 0.3002941608428955
    },
    {
      "name": "Text mining",
      "score": 0.28292253613471985
    },
    {
      "name": "Data mining",
      "score": 0.1313241422176361
    },
    {
      "name": "Engineering",
      "score": 0.07535514235496521
    },
    {
      "name": "Mathematics",
      "score": 0.057284414768218994
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}