{
    "title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning",
    "url": "https://openalex.org/W4385570973",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5023202249",
            "name": "Zhen-Ru Zhang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2475478252",
            "name": "Chuanqi Tan",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2103589644",
            "name": "Xu Haiyang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2108884846",
            "name": "Chengyu Wang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2097175565",
            "name": "Jun Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2159562265",
            "name": "Songfang Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4206178588",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3205949070",
        "https://openalex.org/W4285247752",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W3174702398",
        "https://openalex.org/W2946659172",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4297795751",
        "https://openalex.org/W2915816387",
        "https://openalex.org/W2956105246",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W2145755360",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W3174770825"
    ],
    "abstract": "Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient. Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism. Experiments on the SuperGLUE and NER datasets show the effectiveness of APT. In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1239‚Äì1248\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nTowards Adaptive PreÔ¨Åx Tuning\nfor Parameter-EfÔ¨Åcient Language Model Fine-tuning\nZhen-Ru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang,\nJun Huang, Songfang Huang\nAlibaba Group\n{zhangzhenru.zzr,chuanqi.tcq,shuofeng.xhy}@alibaba-inc.com\n{chengyu.wcy,huangjun.hj,songfang.hsf}@alibaba-inc.com\nAbstract\nFine-tuning large pre-trained language mod-\nels on various downstream tasks with whole\nparameters is prohibitively expensive. Hence,\nParameter-efÔ¨Åcient Ô¨Åne-tuning has attracted at-\ntention that only optimizes a few task-speciÔ¨Åc\nparameters with the frozen pre-trained model.\nIn this work, we focus on preÔ¨Åx tuning, which\nonly optimizes continuous preÔ¨Åx vectors (i.e.\npseudo tokens) inserted into Transformer lay-\ners. Based on the observation that the learned\nsyntax and semantics representation varies a lot\nat different layers, we argue that the adaptive\npreÔ¨Åx will be further tailored to each layer than\nthe Ô¨Åxed one, enabling the Ô¨Åne-tuning more\neffective and efÔ¨Åcient. Thus, we propose Adap-\ntive PreÔ¨Åx Tuning (APT) to adjust the preÔ¨Åx\nin terms of both Ô¨Åne-grained token level and\ncoarse-grained layer level with a gate mech-\nanism. Experiments on the SuperGLUE and\nNER datasets show the effectiveness of APT.\nIn addition, taking the gate as a probing, we\nvalidate the efÔ¨Åciency and effectiveness of the\nvariable preÔ¨Åx.\n1 Introduction\nVanilla Ô¨Åne-tuning strategy usually adjusts all the\nparameters to adapt the pre-trained language model\nto downstream tasks. Parameter-efÔ¨Åcient learning\n(He et al. , 2022; Houlsby et al. , 2019; Lester et al. ,\n2021; Guo et al. , 2021; Ben Zaken et al. , 2022) is\nan emerging framework that freezes the pre-trained\nmodel and only tunes a few number of task-speciÔ¨Åc\nparameters for downstream tasks. For instance,\nPreÔ¨Åx tuning ( Li and Liang , 2021; Liu et al. , 2022)\nprepends length-equivalent pseudo preÔ¨Åx tokens,\ni.e. continuous task-speciÔ¨Åc vectors to each layer of\nthe pre-trained model, achieving comparable even\nsuperior performance with only 0.1-3% parameters.\nIn previous works, the length of preÔ¨Åx tokens\n(or the number of trainable parameters) is usually\nthe same at each layer. However, a potential ob-\nservation lies in that the structure information and\nAdd & Norm\nFeedForward\nAdd & Normùêø\t√ó\nHidden States\nMulti-HeadAttention\nùêæùëÉ! ùëÉ\" ùëâùëÑ\nùëÉ! ùëÉ\"\nùíä ‚àí ùüè\nLayer\nùíä Layer\nPrefix Input\nCLS\nCLS\nScaled Weight ùúÜ#\nùõº#$‚ãØùõº#%\n√ó\n√ó\nGated Weight ùõº#\n√ó √ó\nFigure 1: An illustration of the proposed approach APT\nwhere the left is the internal structure of Transformer\nwith inserted preÔ¨Åxes, and the right is the schematic of\npreÔ¨Åx gate mechanism.\nrepresentational capacity embedded in each layer\nare prone to be inconsistent ( Jawahar et al. , 2019).\nIt is generally considered that the bottom layers of\nthe language model tend to capture concrete and\nshallow phrase-level features, while the top layers\nconcerns more with abstract semantic information\n(Tenney et al. , 2019). Based on the perspective, we\nassume adaptive preÔ¨Åx can grab the emphasis more\nÔ¨Çexibly to adapt to various downstream tasks.\nIn light of above motivation, we investigate the\nadaptive preÔ¨Åx in this work. We propose Adaptive\nPreÔ¨Åx Tuning (APT) with an adaptive gate mech-\nanism at both Ô¨Åne-grained token level and coarse-\ngrained layer level. SpeciÔ¨Åcally, as shown in Fig-\nure 1, for Ô¨Åne granularity, APT scores each individ-\nual preÔ¨Åx token via gated weight assignment. Then,\nthe scaled weight is utilized to balance the inserted\ntask-speciÔ¨Åc preÔ¨Åx tokens and original input tokens\nfor current layer at coarse-grained level.\nExtensive experiments against preÔ¨Åx tuning on\nthe sentence and token classiÔ¨Åcation tasks in full\ndata and low resources setting validate the effec-\ntiveness of APT. In addition, the gate learned from\nAPT could be served as a probing for the number\nof necessary parameters in different layers, guiding\nus to directly apply variable preÔ¨Åx to the origi-\nnal preÔ¨Åx tuning. The probing experiment further\ndemonstrates the effectiveness of adaptive preÔ¨Åx.\n1239\n2 Related Works\nSince Ô¨Åne-tuning the whole model is prohibitively\nexpensive, parameter-efÔ¨Åcient language model Ô¨Åne-\ntuning becomes a lightweight alternative that only\noptimizes a small number of parameters while keep-\ning most pre-trained parameters frozen ( He et al. ,\n2022). Adapter tuning ( Houlsby et al. , 2019) in-\nserts two tunable task-speciÔ¨Åc modules after multi-\nhead attention and feed-forward network, achieving\ncomparable performance with only 2-4% of the pa-\nrameters. Prompt tuning ( Lester et al. , 2021) and\nPreÔ¨Åx-Tuning ( Li and Liang , 2021) only train soft\nprompts by adding preÔ¨Åx tokens to the input or\nhidden states. Recently,\nLiu et al. (2022) extend\nthe preÔ¨Åx tuning to the natural language under-\nstanding tasks, which matches the performance of\nÔ¨Åne-tuning with only 0.1%-3% tuned parameters.\nFurthermore, with an overlap of our motivations\nthat each layer of the pre-trained language model\nfocuses on different aspects of feature for various\ntasks ( Jawahar et al. , 2019; Clark et al. , 2019b) and\nextra parameters are probably not necessary for cer-\ntain tasks ( Houlsby et al. , 2019; Fan et al. , 2020;\nR√ºckl√© et al. , 2021), Adaptable Adapters ( Moosavi\net al. , 2022) selects beneÔ¨Åcial adapter layers and\nlearns task-speciÔ¨Åc activation function for down-\nstream tasks to make adaptor dynamic for each\ntask and layer. In addition to different frameworks\n(adapter versa preÔ¨Åx tuning), our key difference\nfrom their work lies in that we aim to dynamically\nÔ¨Ålter required information at each layer in a soft\nway, while they choose whether to add trainable\nmodules at the layer level in a hard manner.\n3 Methodology\n3.1 PreÔ¨Åx Tuning\nAs preÔ¨Åx tuning is an extension on Transformer\n(Vaswani et al. , 2017), we Ô¨Årst recap the structure\nof Transformer. Transformer is the block consist-\ning of multi-head attention concatenated by multi-\nple single self-attention functions and a fully con-\nnected feed-forward network. Formally speaking,\nthe Transformer block is calculated as follows:\nAttn(Q, K, V ) = softmax(QKT\n‚àö\nd\nV ) (1)\nFFN(x) = ReLU(xW1 + b1)W2 + b2 (2)\nPreÔ¨Åx tuning prepends pseudo preÔ¨Åx tokens\nof length l to each layer of the language model,\nwhich is implemented by concatenating inserted\nkeys and values matrix with original corresponding\nitems in each multi-head attention. SpeciÔ¨Åcally, let\nPk, Pv ‚àà Rl√ód be the keys and values of the en-\ngaged preÔ¨Åx separately, where l denotes the length\nof preÔ¨Åx and d corresponds to the dimension, thus\nself-attention function can be reformatted as:\nAttn(Q, K‚Ä≤, V ‚Ä≤) = softmax(Q(K‚Ä≤)T\n‚àö\nd\nV ‚Ä≤) (3)\nwhere K‚Ä≤ = [Pk; K], V ‚Ä≤ = [Pv; V ]\nHere, [; ]donates concatenation function.\n3.2 Adaptive PreÔ¨Åx Tuning\nThe length of preÔ¨Åx is usually a manually set hyper-\nparameter for each task and Ô¨Åxed in distinct layers\nof the model. However, existing work demonstrates\neach layer of the language model pays attention to\ndifferent aspects of the input feature. We assume\nthe preÔ¨Åx in Ô¨Åxed length is insufÔ¨Åcient to tailor dif-\nferent layers and tasks. To dynamically customize\nthe preÔ¨Åx at each layer, APT performs a gate mech-\nanism via Ô¨Åne-grained gated weight assignment\nand coarse-grained scaled weight speciÔ¨Åcation.\nSpeciÔ¨Åcally, to capture the diversity of informa-\ntion utilization at different layers, we go deep into\nthe token level at the Ô¨Åne-grained granularity. The\ntoken-level gate can inspire us on how many train-\nable parameters (i.e. pseudo tokens in preÔ¨Åx tun-\ning) are required for this layer, which will be dis-\ncussed in Section 4.4. Thus, APT yields the gated\nweights of l pseudo tokens at each layer. We use\nthe hidden states to represent the information en-\ncoded in the layer and calculate the gated weights\nŒ±i = [Œ±i1, Œ± i2, . . . , Œ± il] for i-th layer as:\nŒ±i = sigmoid(hi‚àí1Wi) (4)\nHere, hi‚àí1 is the d-dimensional hidden states from\nthe previous layer, and Wi ‚àà Rd√ól corresponds to\nthe parameters to be learned.\nBesides, we also design a coarse-level gate to\nbalance the information brought from task-speciÔ¨Åc\npreÔ¨Åx tokens and original input tokens by learning\na layer-level weight. A learnable scaled weight\nŒªi is added to the representation of pseudo preÔ¨Åx\ntokens at the i-th layer.\nWith the above strategy, the keys-values pair\nPi = [Pik, Piv] derived from pseudo preÔ¨Åx tokens\nin i-th layer is updated to ÀÜPi as:\nÀÜPi = ŒªiŒ±i ‚äô [Pik, Piv] (5)\n1240\nModel SuperGLUE NER\nBoolQ COPA RTE WiC WSC Avg. CoNLL03 CoNLL04 OntoNotes Avg.\nBERT-base\n(110M)\nFT 72.9 67.0 68.4 71.1\n63.5 68.6 - - - -\nPT-2 72.5 67.4 71.3 69.5 65.4 69.2 89.3 82.6 87.1 86.3\nAPT 72.6 70.0 72.7 71.2 66.9 70.7 89.7 84.1 87.2 87.0\nBERT-large\n(335M)\nFT 77.7 69.0 70.4 74.9\n68.3 72.1 92.8 85.6 89.2 89.2\nPT-2 75.8 73.0 78.3 75.1 68.3 74.1 90.2 84.5 86.4 87.0\nAPT 76.0 79.0 79.4 75.1 70.2 75.9 90.7 85.8 88.6 88.4\nRoBRETa-large\n(355M)\nFT 86.9 94.0 86.6 75.6 63.5 81.3\n92.6 88.8 89.8 90.4\nPT-2 84.8 93.0 89.5 73.4 63.5 80.8 92.8 88.4 89.8 90.3\nAPT 84.8 94.0 89.9 74.6 68.3 82.3 92.7 89.0 89.8 90.5\nDeBERTa-xlarge\n(750M)\nFT - - - - - - 93.1 89.1 90.4\n90.9\nPT-2 - - - - - - 93.1 86.5 90.4 90.0\nAPT - - - - - - 93.0 89.1 90.5 90.8\nTable 1: The results on SuperGLUE development set and NER test set in full data setting. The metric of SuperGLUE\nis accuracy and other is micro-f1 score. Results for FT and PT-2 on BERT-large, RoBRETa-large and DeBERTa-\nxlarge are token from ( Liu et al. , 2022). Results for FT on BERT-base are from ( Liu et al. , 2021). (FT: vanilla\nÔ¨Åne-tuning; PT-2: P-Tuning v2; APT: Adaptive PreÔ¨Åx Tuning; bold: the best score; underline : the second best)\nSetting Method BoolQ COPA RTE WiC WSC Avg.\nBERT-base\n(16-shot)\nFT 47.2 7.5 54.06.5 49.42.7 50.32.3 46.26.8 49.4\nPT-2 52.4 7.2 54.23.3 50.83.1 48.23.3 48.54.3 50.8\nAPT 55.76.5 57.42.7 53.14.4 53.72.2 55.23.8 55.0\nBERT-large\n(16-shot)\nFT 57.39.7 52.02.4 49.52.7 50.00.0 38.72.2 49.5\nPT-2 50.3 5.7 58.25.3 49.93.4 49.32.2 48.14.2 51.2\nAPT 51.7 3.5 60.06.3 53.94.6 51.84.8 55.42.3 54.6\nBERT-base\n(32-shot)\nFT 48.1 9.4 52.26.4 49.52.7 49.40.9 60.43.8 51.9\nPT-2 50.1 5.5 55.03.2 53.83.4 52.04.1 51.54.6 52.5\nAPT 53.55.3 57.62.2 56.51.6 54.83.9 54.66.5 55.4\nBERT-large\n(32-shot)\nFT 47.6 11.9 45.03.6 48.42.2 50.00.0 47.313.2 47.6\nPT-2 45.5 5.1 57.46.9 51.32.3 53.32.1 46.07.1 50.7\nAPT 49.95.9 62.05.0 55.53.6 54.92.8 49.04.4 54.3\nTable 2: The mean std experimental results within 5 random seeds on SuperGLUE development set in 16-shot and\n32-shot setting where all metrics are accuracy. bold: the best score.\n‚äô is the element-wise multiplication. Accordingly,\nthe calculation of the self-attention function in APT\nis similar to Eq.( 3) without further elaboration.\n4 Experiments\n4.1 Experimental Setup\nWe conduct 5 NLU tasks on SuperGLUE ( Wang\net al. , 2019) benchmark including BoolQ ( Clark\net al. , 2019a), COPA ( Roemmele et al. , 2011),\nRTE ( Wang et al. , 2018), WiC ( Pilehvar and\nCamacho-Collados, 2019) and WSC ( Levesque\net al. , 2012) as well as 3 Named Entity Recognition\n(NER) tasks including CoNLL03 ( Tjong Kim Sang\nand De Meulder , 2003), CoNLL04 ( Carreras and\nM√†rquez, 2004), and OntoNotes 5.0 ( Weischedel\net al. , 2013). With BERT-base / large ( Devlin et al. ,\n2019) and RoBERTa-large ( Liu et al. , 2019) instan-\ntiated by HuggingFace Transformers ( Wolf et al. ,\n2020), we compare APT with vanilla Ô¨Åne-tuning\nand P-Tuning v2 ( Liu et al. , 2022) which is an im-\nplementation of the preÔ¨Åx tuning, conÔ¨Ågured with\nhyper-parameters public in the released code 1. We\nalso verify our method with DeBERTa-xlarge ( He\net al. , 2020) on NER tasks following P-Tuning v2.\n4.2 Results\nWe report the main results in Table 1. For BERT-\nbase, we can observe that APT achieves 1.5% and\n0.7% improvements over P-Tuning v2 on Super-\nGLUE and NER tasks, respectively. For BERT-\nlarge, APT outperforms P-Tuning v2 by 1.8% on\nSuperGLUE tasks and 1.4% on NER tasks. For\nRoBERTa-large, APT surpasses P-Tuning v2 by\n1.5% on SuperGLUE tasks and 0.2% on NER tasks.\nOn NER tasks with DeBERTa-xlarge, APT is supe-\n1https://github.com/THUDM/P-tuning-v2\n1241\nSetting SuperGLUE NER\nBoolQ COPA RTE WiC WSC Avg. CoNLL03 CoNLL04 OntoNotes Avg.\nAPT 72.6 70.0 72.7 71.2 66.9 70.7 89.7 84.1 87.2 87.0\nw/o token-level Œ± 72.6 69.0 69.9 70.8 65.8 69.6 89.5 83.7 87.2 86.8\nw/o layer-level Œª 72.1 67.4 71.3 69.6 65.4 69.1 89.0 82.6 86.9 86.2\nw/o hidden states h 72.0 68.8 68.7 70.2 64.6 68.9 89.1 83.6 87.1 86.6\nTable 3: Ablation study on BERT-base for two different level gate mechanisms and the hidden states from the\nprevious layer. bold: the best score.\nModel SuperGLUE NER\nBoolQ COPA RTE WiC WSC Avg. CoNLL03 CoNLL04 OntoNotes Avg.\nPT-2 72.5 67.4 71.3 69.5 65.4 69.2 89.3 82.6 87.1 86.3\nPT-2* 72.6 68.8 71.9 70.0 65.8 69.8 89.3 83.0 87.2 86.5\nPT-2+ 72.8 65.4 69.1 71.1 65.8 68.8 89.4 83.2 87.1 86.6\nAPT 72.6 70.0 72.7 71.2 66.9 70.7 89.7 84.1 87.2 87.0\nTable 4: Comparison between PT-2 and PT-2 ‚àó, PT-2 + and APT on BERT-base. (PT-2: P-Tuning v2; PT-2 ‚àó: PT-2\nwith variable preÔ¨Åx; PT-2 +: PT-2 with enlarged preÔ¨Åx)\nrior to P-Tuning v2 by an average of 0.8%. Com-\npared with vanilla Ô¨Åne-tuning, APT is comparable\nor even better on part of tasks. In addition, we\nexplore the experimental performance under low\nresource settings on SuperGLUE benchmark. As\nshown in Table 2, APT is a better few-shot learner\nthan P-Tuning v2, which exceeds 4.2%, 3.4% in\n16-shot setting, and 2.9%, 3.6% in 32-shot setting\nfor BERT-base and BERT-large, respectively.\n4.3 Ablation Study\nWe conduct an ablation study in order to explore\nthe separate effect of token-level gated weight Œ±,\nlayer-level scaled weight Œª and the hidden states h\nfrom the previous layer which is used to calculate\ntoken-level gated weight Œ± in Eq.( 4). As shown in\nTable 3, it can be found that removing any strategy\nhurts the performance to varying degrees, demon-\nstrating that they are all advantageous. SpeciÔ¨Åcally,\nthe beneÔ¨Åcial effect of\nŒª for APT is slightly greater\nthan Œ± overall. Besides, it is effective and meaning-\nful to introduce the context (i.e. the hidden states h\nfrom the previous layer) when obtaining the gated\nweight, especially for SuperGLUE tasks.\n4.4 Discussion\nWhat is preÔ¨Åx weight distribution learned by\nAPT? The gate mechanism for preÔ¨Åx serves as\nthe key strategy of the proposed APT, where the\nlearned preÔ¨Åx weight distribution turns out to be a\ncritical point. Figure 2 illustrates the gate weights\nof the pseudo preÔ¨Åx token for COPA and CoNLL04,\n0 2 4 6 8 10 12 14\nPseudo Prefix Token Index\n0246810121416182022\nModel Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) COPA\n0 30 60 90 120\nPseudo Prefix Token Index\n01234567891011\nModel Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 (b) CoNLL04\nFigure 2: Visualization of the learned weights of the\npreÔ¨Åx token for SuperGLUE task COPA on BERT-large\nand NER task CoNLL04 on BERT-base, with darker\ncolors indicating higher weights.\nrespectively. It can be found that CoNLL04 is con-\ncerned with bottom layers in the language model\nwhich are regarded as phrase-level features, while\nCOPA pays more attention to the higher layers, in-\ndicating semantic information. The observation is\nconsistent with the characteristics of corresponding\ntasks. NER is a token-level task while COPA is a\ncausal reasoning task sensitive to the semantics of\nsentences, which reminds us that it is worth placing\nvarious preÔ¨Åx tokens on speciÔ¨Åc layers according\nto the task properties.\nDoes variable preÔ¨Åx work better than Ô¨Åxed one?\nTo verify the effectiveness of adaptive preÔ¨Åx under\nthe proposed architecture, we wonder if the learned\nratio at each layer can be directly transferred to\nP-Tuning v2. Taking the gate as a probing indica-\ntor, we reset the preÔ¨Åx length of P-Tuning v2 from\nÔ¨Åxed to variable in different layers based on the ob-\n1242\nservation of the learned ratio (e.g. the distribution\nshown in Figure 2). From the comparison between\nPT-2 and PT-2 ‚àó in Table 4, we demonstrate that\nthe variable preÔ¨Åx with less trainable parameters\nsurprisingly outperforms the original implementa-\ntion in Ô¨Åxed preÔ¨Åx. Nonetheless, it is also worth\nnoting that there is still a gap between P-Tuning\nv2 with variable preÔ¨Åx and APT, where the latter\ncontinuously adjusts the weight of preÔ¨Åx during\nthe training phase while the former only initializes\nwith a one-time mask probing.\nWhether the adaptive structure beneÔ¨Åts the Ô¨Åne-\ntuning? Compared to P-Tuning v2, APT learns\nextra gated and scaled weights. To Ô¨Ågure it out\nwhether the improvement of APT is brought from\nmore trainable parameters or the adaptive model\nstructure, we adjust the hyper-parameter, i.e., en-\nlarge the preÔ¨Åx length of P-Tuning v2 by 1.5 times\nto align the number of parameters with our APT. As\nshown in the comparison between PT-2 + and APT\nof Table 4, we observe that APT still outperforms\nenlarged P-Tuning v2 with 1.9%, 0.4% on aver-\nage for SuperGLUE and NER tasks respectively,\nvalidating the superiority of the gate mechanism.\n5 Conclusion\nIn this paper, we investigate preÔ¨Åx tuning and as-\nsume that adaptive preÔ¨Åx is probably more efÔ¨Åcient\nand effective than Ô¨Åxed preÔ¨Åx. Firstly, we propose\nAPT that leverages the token-level and the layer-\nlevel gate mechanism which achieves an improve-\nment of performance over original preÔ¨Åx tuning.\nThen, we illustrate the weight distribution learned\nby APT and take it as a probe, which validates the\nvariable preÔ¨Åx can work better than the Ô¨Åxed one.\nThe above experiments and analysis demonstrate\nthat the adaptive preÔ¨Åx can be served as a promis-\ning strategy for parameter-efÔ¨Åcient Ô¨Åne-tuning.\nLimitations\nThe proposed approach in this paper also suffers\nfrom certain limitations, i.e. we adapt APT on the\nencoder model and lack design for the other archi-\ntectures such as decoder-only and encoder-decoder.\nIn addition, it is better to generalize the key idea to\nother parameter-efÔ¨Åcient learning approaches. A\nuniÔ¨Åed solution for existing work may be worth\nexploring in the future.\nReferences\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. BitFit: Simple parameter-efÔ¨Åcient Ô¨Åne-tuning\nfor transformer-based masked language-models . In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1‚Äì9, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nXavier Carreras and Llu√≠s M√†rquez. 2004. Introduction\nto the CoNLL-2004 shared task: Semantic role la-\nbeling. In Proceedings of the Eighth Conference on\nComputational Natural Language Learning (CoNLL-\n2004) at HLT-NAACL 2004, pages 89‚Äì97, Boston,\nMassachusetts, USA. Association for Computational\nLinguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. BoolQ: Exploring the surprising\ndifÔ¨Åculty of natural yes/no questions . In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924‚Äì2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019b. What does BERT\nlook at? an analysis of BERT‚Äôs attention . In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276‚Äì286, Florence, Italy. Association for Com-\nputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout . In International Conference on Learn-\ning Representations.\nDemi Guo, Alexander Rush, and Yoon Kim. 2021.\nParameter-efÔ¨Åcient transfer learning with diff prun-\ning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n4884‚Äì4896, Online. Association for Computational\nLinguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nuniÔ¨Åed view of parameter-efÔ¨Åcient transfer learning .\nIn International Conference on Learning Representa-\ntions.\n1243\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efÔ¨Åcient transfer learning for NLP . In\nProceedings of the 36th International Conference\non Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pages 2790‚Äì2799.\nPMLR.\nGanesh Jawahar, Beno√Æt Sagot, and Djam√© Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651‚Äì3657, Florence, Italy. Association for\nComputational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efÔ¨Åcient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045‚Äì3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth international conference on the principles of\nknowledge representation and reasoning.\nXiang Lisa Li and Percy Liang. 2021. PreÔ¨Åx-tuning:\nOptimizing continuous prompts for generation . In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582‚Äì\n4597, Online. Association for Computational Lin-\nguistics.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to Ô¨Åne-tuning\nacross scales and tasks\n. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61‚Äì68,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nNaÔ¨Åse Sadat Moosavi, Quentin Delfosse, Kristian Kerst-\ning, and Iryna Gurevych. 2022. Adaptable Adapters .\nIn Proceedings of the 2022 Annual Conference of\nthe North American Chapter of the Association for\nComputational Linguistics, Seattle, W A, USA. Asso-\nciation for Computational Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. WiC: the word-in-context dataset for evalu-\nating context-sensitive meaning representations . In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1267‚Äì1273,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI spring symposium: logical formal-\nizations of commonsense reasoning, pages 90‚Äì95.\nAndreas R√ºckl√©, Gregor Geigle, Max Glockner, Tilman\nBeck, Jonas Pfeiffer, Nils Reimers, and Iryna\nGurevych. 2021.\nAdapterDrop: On the efÔ¨Åciency\nof adapters in transformers . In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7930‚Äì7946, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline . In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593‚Äì\n4601, Florence, Italy. Association for Computational\nLinguistics.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition . In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142‚Äì\n147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz\nKaiser, and Illia Polosukhin. 2017.\nAttention is all\nyou need . In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language understand-\ning systems . In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding . In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353‚Äì355, Brussels, Belgium. Association for Com-\nputational Linguistics.\n1244\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, Mohammed El-Bachouti, Robert Belvin,\nand Ann Houston. 2013. OntoNotes Release 5.0 .\nAbacus Data Network.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020.\nTrans-\nformers: State-of-the-art natural language processing .\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38‚Äì45, Online. Association\nfor Computational Linguistics.\nA Experimental Details\nDatasets In the full data setting, all train-dev-test\nsplits follow P-Tuning v2 ( Liu et al. , 2022). For low\nresources setting, to generate k-shot (k = 16, 32)\ndatasets on SuperGLUE, the Ô¨Åxed set of random\nseed [11,21,42,87,100] is utilized to sample in-\nstances in training and development set, while the\nentire development set is treated as test set, where\nthe average performance is reported in Table 2.\nExperimental Setting We grid search the learn-\ning rate over [5e-3, 7e-3, 1e-2, 1e-4], training epoch\nover [20, 40, 60, 80, 100, 120], batch size over [8,\n16, 32], and random seeds over [11, 21, 42, 87,\n100]. For a fair comparison, the preÔ¨Åx length uti-\nlized by APT is consistent with P-Tuning v2. In\nlow resources setting, the batch size we used is 2.\nIn Eq.( 4), we take the hidden states of the Ô¨Årst input\ntoken as representation in previous layer.\nExperimental Computation We use the pre-\ntrained model BERT-base with 110M parameters,\nBERT-large with 335M parameters, RoBERTa-\nlarge with 355M parameters and DeBERTa-xlarge\nwith 750M parameters. We conduct experiments\non NVIDIA V100 or A100 GPUs for each task.\nB Further Ablation Results\nWe demonstrate further ablation results on BERT-\nlarge and RoBERTa-large as shown in Table 5. It\ncan be found that the beneÔ¨Åcial impact of the three\nstrategies and the observation is consistent with\nBERT-base in Section 4.3 in general.\n20 40 60 80 100\nprefix length\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\nacc\nAPT\nPT-2\n(a) COPA\n30 40 50 60 70 80 90 100\nprefix length\n0.64\n0.65\n0.66\n0.67\n0.68\nacc\nAPT\nPT-2 (b) WSC\nFigure 3: The performance of APT and PT-2 on COPA\nand WSC in a range of preÔ¨Åx length on BERT-large.\nC PreÔ¨Åx Length\nThe preÔ¨Åx length is an important hyper-parameter\nfor preÔ¨Åx tuning and APT. Figure 3 illustrates the\nperformance of APT and P-Tuning v2 with differ-\nent preÔ¨Åx lengths over a range. It can be observed\nthat APT is superior to P-Tuning v2 in most preÔ¨Åx\nlength settings, indicating that APT has a relatively\nwider range of preÔ¨Åx length to achieve better per-\nformance.\nD ScientiÔ¨Åc Artifacts\nWe use datasets involving SuperGLUE ( Wang\net al. , 2019) benchmark including BoolQ ( Clark\net al. , 2019a), COPA ( Roemmele et al. , 2011),\nRTE ( Wang et al. , 2018), WiC ( Pilehvar and\nCamacho-Collados, 2019) and WSC ( Levesque\net al. , 2012) as well as 3 Named Entity Recognition\n(NER) tasks including CoNLL03 ( Tjong Kim Sang\nand De Meulder , 2003), CoNLL04 ( Carreras and\nM√†rquez, 2004), and OntoNotes 5.0 ( Weischedel\net al. , 2013). The pre-trained model we used are\nBERT-base / large ( Devlin et al. , 2019), RoBERTa-\nlarge ( Liu et al. , 2019) and DeBERTa-xlarge ( He\net al. , 2020). We use HuggingFace Transformers\n(Wolf et al. , 2020) and P-Tuning v2 ( Liu et al. ,\n2022) as the codebase implemented by PyTorch\n2. They are all open-source and we only use for\nacademic research which is consistent with their\nintended use.\n2https://pytorch.org/\n1245\nModel Setting SuperGLUE NER\nBoolQ COPA RTE WiC WSC Avg. CoNLL03 CoNLL04 OntoNotes Avg.\nBERT-large\nAPT 76.0 79.0 79.4 75.1 70.2 75.9 90.7 85.8 88.6 88.4\nw/o token-level Œ± 75.8 77.0 77.3 74.8 68.3 74.6 91.1 84.4 88.5 88.0\nw/o layer-level Œª 75.4 74.0 76.9 74.6 68.3 73.8 90.7 83.7 88.4 87.6\nw/o hidden states h 74.7 76.0 75.8 74.6 68.3 73.9 91.2 84.0 88.6 87.9\nRoBERTa-large\nAPT 84.8 94.0 89.9 74.6 68.3 82.3 92.7 89.0 89.8 90.5\nw/o token-level Œ± 84.3 88.0 88.1 73.0 65.4 79.8 92.2 88.7 89.5 90.1\nw/o layer-level Œª 84.7 88.0 86.3 72.1 64.4 79.1 92.0 88.7 89.8 90.2\nw/o hidden states h 83.9 91.0 87.0 72.9 64.4 79.8 92.2 88.7 89.4 90.1\nTable 5: Ablation experiments on BERT-large and RoBERTa-large for two different level gate mechanisms and the\nhidden states from the previous layer. bold: the best score.\n1246\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nsection limitations\n‚ñ° A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nsection abstract and section 1 introduction\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nsection 4.1\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nsection 4.1\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nsection D ScientiÔ¨Åc Artifacts\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nsection D ScientiÔ¨Åc Artifacts\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use open-source datasets and do not change datasets for a fair comparison.\n‚ñ° B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n‚ñ°\u0017 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nIt can be found in the cited paper.\nC ‚ñ°\u0013 Did you run computational experiments?\nsection 4 Experiments\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nTable 1 and section appendix A Experimental Computation\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1247\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nsection appendix A Experimental Details\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nTable 2 report the mean and std results.\n‚ñ°\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe follow the existing work and keep consistent with them.\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1248"
}