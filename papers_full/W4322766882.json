{
  "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
  "url": "https://openalex.org/W4322766882",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100660086",
      "name": "Ning Ding",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5102894085",
      "name": "Yujia Qin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5059211235",
      "name": "Guang Yang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5049088062",
      "name": "Fuchao Wei",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5042856545",
      "name": "Zonghan Yang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5101474800",
      "name": "Yusheng Su",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5010557815",
      "name": "Shengding Hu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100398894",
      "name": "Yulin Chen",
      "affiliations": [
        "Tsinghua University",
        "Tsinghua–Berkeley Shenzhen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5044330945",
      "name": "Chi-Min Chan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5025388406",
      "name": "Weize Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5017575883",
      "name": "Jing Yi",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5091743314",
      "name": "Weilin Zhao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100775958",
      "name": "Xiaozhi Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100320723",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5022672030",
      "name": "Hai-Tao Zheng",
      "affiliations": [
        "Tsinghua University",
        "Tsinghua–Berkeley Shenzhen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5100641283",
      "name": "Jianfei Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100355692",
      "name": "Yang Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5044791875",
      "name": "Jie Tang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5003324011",
      "name": "Juanzi Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5046448314",
      "name": "Maosong Sun",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3207663303",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4287891024",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3205717164",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W3176693010",
    "https://openalex.org/W3175557894",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3188542058",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3210129272",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W3217756540",
    "https://openalex.org/W2140246545",
    "https://openalex.org/W4212774754"
  ],
  "abstract": "Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.",
  "full_text": "Nature Machine Intelligence | Volume 5 | March 2023 | 220–235 220\nnature machine intelligence\nAnalysis\nhttps://doi.org/10.1038/s42256-023-00626-4\nParameter-efficient fine-tuning of large-scale \npre-trained language models\nNing Ding    1,2,4, Yujia Qin1,2,4, Guang Yang1, Fuchao Wei1, Zonghan Yang1, \nYusheng Su1,2, Shengding Hu1,2, Yulin Chen3, Chi-Min Chan1, Weize Chen1,2, \nJing Yi1,2, Weilin Zhao1,2, Xiaozhi Wang1, Zhiyuan Liu    1,2 , Hai-Tao Zheng    3 , \nJianfei Chen1, Yang Liu1, Jie Tang1,2, Juanzi Li1 & Maosong Sun    1,2 \nWith the prevalence of pre-trained language models (PLMs) and the \npre-training–fine-tuning paradigm, it has been continuously shown that \nlarger models tend to yield better performance. However, as PLMs scale \nup, fine-tuning and storing all the parameters is prohibitively costly and \neventually becomes practically infeasible. This necessitates a new branch \nof research focusing on the parameter-efficient adaptation of PLMs, which \noptimizes a small portion of the model parameters while keeping the rest \nfixed, drastically cutting down computation and storage costs. In general, \nit demonstrates that large-scale models could be effectively stimulated by \nthe optimization of a few parameters. Despite the various designs, here we \ndiscuss and analyse the approaches under a more consistent and accessible \nterm ‘delta-tuning’ , where ‘delta’ a mathematical notation often used to \ndenote changes, is borrowed to refer to the portion of parameters that are \n‘changed’ during training. We formally describe the problem and propose a \nunified categorization criterion for existing delta-tuning methods to explore \ntheir correlations and differences. We also discuss the theoretical principles \nunderlying the effectiveness of delta-tuning and interpret them from the \nperspectives of optimization and optimal control. Furthermore, we provide \na holistic empirical study on over 100 natural language processing tasks and \ninvestigate various aspects of delta-tuning. With comprehensive study and \nanalysis, our research demonstrates the theoretical and practical properties \nof delta-tuning in the adaptation of PLMs.\nWith the revolutionary development in computing hardware, tradi-\ntional statistical methods for modelling natural language have yielded \ntheir place to deep learning1 that heavily relies on tensor computation \nand huge data volume. Modern natural language processing (NLP) \nuses deep neural networks to implicitly model language distribution \nand capture language representations2–4. A standard pipeline involves \nencoding language into discrete tokens (tokenization) as model input, \nchoosing a proper model architecture, designing corresponding tasks \nand training the network with the given corpora. Among these deep \nneural architectures, the transformer neural network 4 produces \nstate-of-the-art performances on a series of NLP applications. Sub -\nsequently, the advancement in pre-trained language models (PLMs) \nusing deep transformers as their foundation has ushered in a new era \nof NLP. PLMs typically use heavily over-parameterized transformers \nas the base architecture and model natural language in bidirectional5, \nautoregressive6,7 or sequence-to-sequence8 manners on large-scale \nReceived: 13 April 2022\nAccepted: 2 February 2023\nPublished online: 2 March 2023\n Check for updates\n1Department of Computer Science and Technology, Tsinghua University, Beijing, China. 2Beijing Academy of Artificial Intelligence, Beijing, China. \n3Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China. 4These authors contributed equally: Ning Ding, Yujia Qin. \n e-mail: liuzy@tsinghua.edu.cn; zheng.haitao@sz.tsinghua.edu.cn; sms@tsinghua.edu.cn\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 221\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nsuch model adaptations. Compared with fine-tuning, delta-tuning \nmakes model adaptation a considerably low-cost process. For instance, \nresearchers find that the optimization problem of the adaptations for \nbig models could be reparameterized into a low-dimensional ‘intrinsic \nsubspace’16,17 and various NLP tasks could be handled by tuning only \nvery few parameters in the subspace. The empirical evidence takes us \none step closer to understanding how pre-trained models work and \nmay even spawn new theoretical questions that are worth exploring.\nThis Analysis attempts to comprehensively analyse recent \nadvances in delta-tuning to establish a deeper understanding of \nthis branch of methods (Methods). We formally describe the prob -\nlem and categorize delta-tuning methods into addition-based, \nspecification-based and reparameterization-based methods as illus-\ntrated in Fig. 4 , then we comprehensively introduce the technical \ndetails and empirical conclusions of each method. T o better under-\nstand the inner connections among the delta-tuning methods and the \nmechanisms of model adaptation, we develop theoretical analyses of \ndelta-tuning by proposing theoretical frameworks from two differ-\nent perspectives: optimization and optimal control. Our theoretical \ndiscussion is summarized as follows.\n 1. Optimization. Based on the knowledge of a low intrinsic dimen-\nsion in a large PLM, we show that delta-tuning is essentially a \nsubspace-optimization method with respect to the solution \nspace or functional space. The discussion justifies the designs of \nthe existing delta-tuning methods and explains some phenom-\nena in the experiments.\n 2. Optimal control. Inspired by the relationship between deep \nlearning and optimal control theories, we interpret delta-tuning \nas seeking optimal controllers for PLMs. We propose an optimal \ncontrol framework that unifies different delta-tuning approach-\nes. Our analysis provides theoretical references for the novel de-\nsign of delta-tuning methods.\nIn terms of empirical studies, we carry out extensive and system-\natic experiments (Results) on over 100 NLP tasks to rigorously explore \nthe performances, combinability, the power of scale, transferability \nand so on. Our main findings are summarized as follows.\n 1. Performance. Delta-tuning yields consistent and non-trivial per-\nformance on more than 100 NLP tasks, showing that it is an ef-\nfective and lightweight alternative to conventional fine-tuning. \nAmong several representative delta-tuning methods, no single \nalgorithm predominantly outperforms the others.\n 2. Convergence. Training stability is also one of our focuses. Al-\nthough the convergence of delta-tuning is generally not as fast \nas that of full parameter fine-tuning, we find that it is more sensi-\ntive to the delta structures than the number of tunable param-\neters. Meanwhile, the larger the model is, the faster the training \nconverges.\n 3. Efficiency. In terms of computational efficiency, which is the \noriginal motivation for the methods, delta-tuning could sub -\nstantially improve computational and storage efficiency while \nachieving decent results, highlighting the promising practical \nvalue of adapting super-large PLMs.\n 4. Combinability. Combining multiple delta-tuning methods is \nmore effective than a single method in most cases, despite that \nthe optimal combination may vary for different PLM backbones, \ndownstream tasks and data scales. This finding implies the ex -\nistence of an optimal delta structure, and it is likely that such a \nstructure cannot be obtained artificially, but could be generated \nautomatically.\n 5. Power of scale. The power of scale (that is, both the performance \nand convergence are improved when the size of the PLM increas-\nes) is observed in all of the delta-tuning methods, even in unregu-\nlated neural modules. In other words, when the model size is large \nenough, only optimizing a random portion of parameters can \nachieve comparable performance to conventional fine-tuning.\nunsupervised corpora. Then for downstream tasks, task-specific \nobjectives are introduced to fine-tune the PLMs for model adapta-\ntion. Notably, the increasing scale of PLMs (measured by the number \nof parameters) seems to be an irreversible trend, as constant empiri-\ncal results show that larger models (along with more data) almost \ncertainly lead to better performance. For example, with 175 billion \nparameters, Generative Pre-trained Transformer 3 (GPT-3)9 generates \nnatural language of unprecedented quality and can conduct various \ndesired zero-shot tasks with satisfactory results given appropriate \nprompts. Subsequently, a series of large-scale models such as Gopher10, \nMegatron-Turing Natural Language Generation (NLG)11 and Pathways \nLanguage Model (PaLM)12 have repeatedly shown effectiveness on a \nbroad range of downstream tasks.\nAs the model scales, how to efficiently and effectively adapt \nlarge models to particular downstream tasks becomes an intriguing \nresearch issue. Although in-context learning has shown promising \nperformance for PLMs such as GPT-3, fine-tuning still overtakes it under \nthe task-specific setting. However, the predominant approach, full \nparameter fine-tuning, which initializes the model with the pre-trained \nweights, updates all the parameters and produces separate instances \nfor different tasks, becomes impractical when dealing with large-scale \nmodels. In addition to the cost of deployment and computation, storing \ndifferent instances for different tasks is extremely memory intensive. \nT o further explore the practical application rate of large models (PLMs \nwith over 1 billion parameters), we randomly select 1,200 published \nresearch papers from the recent six NLP conferences (200 for each \nvenue), including Annual Meeting of the Association for Computational \nLinguistics (ACL) 2022, ACL 2021, Conference on Empirical Methods \nin Natural Language Processing (EMNLP) 2021, Annual Conference of \nthe North American Chapter of the Association for Computational \nLinguistics (NAACL) 2021, ACL 2020 and EMNLP 2020. Then we manu-\nally count the usage of PLMs in these peer-reviewed works, focusing on \nonly the experimental part of the papers. According to the statistics in \nExtended Data Table 1, although the use of PLMs has become increas-\ningly popular, only about 0.5–4% of research papers practically adopt \nlarge PLMs in the experiments. One of the reasons for their unpopular-\nity is the unaffordable cost of deploying and experimentally validating \nlarge PLMs.\nIn fact, large PLMs with billions of parameters could be effec -\ntively driven by optimization of a few parameters, and a branch of \nparameter-efficient methods for model tuning arises. Although each \nof these approaches proposes distinct designs on the structure and \nlocation of trainable parameters in PLMs, they essentially tune a ‘delta’ \nin the adaptation phase, which refers to a small fraction of trainable \nparameters that can be placed anywhere in the PLM. We thus unify them \nunder a more accessible term ‘delta-tuning’ that captures the essence \nof this branch of methods more precisely. In general, delta-tuning \nupdates only a small number of parameters (inherently in the model \nor additionally introduced) while freezing the remaining parameters \nthat account for the vast majority. Adapter tuning13 is among the earli-\nest approaches to steer pre-trained models with a limited number of \nparameters. It inserts adapter modules with bottleneck architecture \nbetween layers in PLMs and only these inserted modules get updated \nduring fine-tuning. BitFit14 updates the bias terms in PLMs while freez-\ning the remaining modules. Low rank adaptation (LoRA)15 decomposes \nattention weight update into low-rank matrices to reduce the number \nof trainable parameters. The delta-tuning methods enable efficient \ntuning and practical usage for large pre-trained models and often \nachieve comparable results to the standard fine-tuning. For example, \nthe vanilla fine-tuning of GPT-3 needs to update about 175,255 million \nparameters, which is almost infeasible in both industry and academia. \nHowever, if we tune only the injected low-rank decomposition matrices \nin each transformer layer15, only 37.7 million parameters will be involved \nin backpropagation. Delta-tuning not only provides a promising way \nto adapt large PLMs but also sheds light on the mechanisms behind \nNature Machine Intelligence | Volume 5 | March 2023 | 220–235 222\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nTable 1 | Overall (test) performance of over 100 NLP tasks comparing PT , PF, LR, AP and FT\nTask PT (BASE) PT (LARGE) PF LR AP FT\nRatio of tunable parameters 0.03% 0.01% 7.93% 0.38% 2.38% 100%\nClassification/sentiment analysis\nGLUE-SST2 92.20 94.95 92.66 94.04 93.35 94.27\nROTTEN_TOMATOES 88.36 91.84 89.96 89.30 89.20 89.77\nFINANCIAL_PHRASEBANK 97.18 98.36 98.36 97.94 97.95 98.36\nPOEM_SENTIMENT 54.18 70.31 85.38 86.80 82.52 83.26\nYELP_POLARITY 95.47 98.18 97.78 97.37 97.30 97.92\nAVG. OF SENTIMENT ANALYSIS 85.48 90.73 92.83 93.09 92.06 92.72\nClassification/emotion\nEMO 69.91 71.47 73.31 76.13 74.88 75.69\nEMOTION 89.19 88.73 88.29 88.63 88.98 89.25\nTWEET_EVAL-HATE 53.00 42.23 44.67 48.16 47.88 51.33\nTWEET_EVAL-IRONY 58.02 69.73 76.00 76.75 73.88 7 7.43\nTWEET_EVAL-OFFENSIVE 75.94 78.87 80.94 80.97 80.59 82.05\nTWEET_EVAL-SENTIMENT 28.90 72.79 71.78 71.31 71.90 71.98\nTWEET_EVAL-STANCE_ABORTION 32.59 61.42 61.47 63.20 62.61 61.72\nTWEET_EVAL-STANCE_ATHEISM 56.28 67.58 71.54 71.77 71.27 74.41\nTWEET_EVAL-STANCE_CLIMATE 47.61 52.43 52.86 55.92 59.06 57.38\nTWEET_EVAL-STANCE_FEMINIST 29.65 51.63 56.27 57.41 58.57 58.51\nTWEET_EVAL-STANCE_HILLARY 41.34 63.18 62.15 65.40 61.74 66.41\nAVG. OF EMOTION 52.95 65.46 67.21 68.70 68.31 69.65\nClassification/hate-speech detection\nETHOS-DISABILITY 46.99 100.00 93.81 93.81 100.00 93.81\nETHOS-GENDER 63.84 77.08 77.44 79.91 79.91 74.48\nETHOS-NATIONAL_ORIGIN 44.30 81.77 81.77 87.95 84.72 84.72\nETHOS-RACE 84.36 97.06 94.54 97.21 94.27 97.21\nETHOS-RELIGION 93.02 93.02 96.35 93.02 96.35 96.64\nETHOS-DIRECTED_VS_GENERALIZED 76.86 86.64 94.76 92.29 94.94 94.94\nHATE_SPEECH_OFFENSIVE 73.27 79.08 75.22 75.21 75.06 75.04\nHATE_SPEECH18 75.57 74.45 79.42 79.59 80.86 80.93\nHATEXPLAIN 50.98 67.62 66.06 68.03 68.11 68.02\nAVG. OF HATE SPEECH DETECTION 67.69 84.08 84.37 85.22 86.02 85.09\nClassification/natural language inference\nANLI 25.85 44.96 43.88 45.27 49.19 50.54\nGLUE-MNLI 35.43 86.12 82.21 83.74 83.90 86.39\nGLUE-QNLI 52.34 93.01 87.48 92.02 91.58 92.57\nGLUE-RTE 45.32 79.14 72.66 79.14 78.42 80.58\nSCITAIL 91.02 95.47 93.04 93.80 94.04 94.77\nSUPERGLUE-RTE 50.36 84.89 73.38 79.14 82.01 78.42\nSICK 40.10 88.82 87.91 88.69 88.88 89.15\nSUPERGLUE-CB 75.00 78.57 100.00 100.00 96.43 96.43\nAVG. OF NATURAL LANGUAGE INFERENCE 51.93 81.37 80.07 82.73 83.06 83.61\nClassification/fact checking\nCLIMATE_FEVER 15.47 33.42 38.03 39.35 37.48 41.57\nLIAR 13.23 28.87 26.46 28.67 27.08 28.20\nHEALTH_FACT 39.15 45.60 50.38 52.05 51.21 54.19\nTAB_FACT 46.65 50.16 52.53 56.86 53.42 57.34\nAVG. OF FACT CHECKING 28.63 39.51 41.85 44.23 42.30 45.36\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 223\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nTask PT (BASE) PT (LARGE) PF LR AP FT\nClassification/paraphrase\nGLUE-QQP 84.65 86.21 84.62 86.87 85.93 89.13\nMEDICAL_QUESTIONS_PAIRS 46.56 91.80 85.25 88.52 90.16 87.21\nPAWS 49.60 91.27 92.07 93.39 92.91 93.60\nGLUE-MRPC 67.65 88.24 87.25 87.25 87.25 89.71\nAVG. OF PARAPHRASE 62.12 89.38 87.3 89.01 89.06 89.91\nClassification/topic\nAG_NEWS 91.37 93.61 93.42 94.63 94.60 95.19\nClassification/binary\nBOOLQ 61.28 77.43 77.55 80.00 78.47 81.77\nMC_TACO 76.25 88.39 86.02 88.13 86.81 87.34\nAVG. OF BINARY 68.77 82.91 81.79 84.07 82.64 84.56\nClassification/other\nADE_CORPUS_V2-CLASSIFICATION 41.76 94.42 93.25 94.47 93.91 94.27\nDISCOVERY 0.18 18.83 16.67 18.98 18.41 25.88\nGLUE-COLA 0.00 55.60 50.95 49.40 44.66 51.53\nSMS_SPAM 95.80 97.46 97.14 97.14 97.46 97.11\nSUPERGLUE-WIC 50.16 68.34 64.89 68.65 70.53 71.79\nWIKI_QA 48.78 73.97 64.10 72.15 70.75 74.41\nCIRCA 13.51 77.39 80.16 82.38 82.93 84.69\nONESTOP_ENGLISH 22.53 98.23 100.00 100.00 100.00 100.00\nTREC 90.80 91.51 91.38 93.38 93.36 94.81\nTREC-FINEGRAINED 80.63 88.18 90.04 91.44 90.00 91.27\nAVG. OF OTHER CLASSIFICATION 44.42 76.39 74.86 76.80 76.2 78.58\nQuestion answering/closed-book question answering\nFREEBASE_QA 1.90 6.71 2.63 3.75 5.86 23.52\nLAMA-CONCEPTNET 15.25 26.12 22.63 34.96 43.62 70.28\nLAMA-GOOGLE_RE 11.78 14.08 12.60 18.82 23.73 24.88\nLAMA-SQUAD 3.23 16.13 12.90 9.68 3.23 9.68\nLAMA-TREX 59.13 63.68 63.91 66.21 67.23 69.12\nNUMER_SENSE 50.53 56.75 53.30 56.27 53.97 57.32\nSEARCH_QA 7.14 19.17 8.70 10.17 9.72 19.26\nWEB_QUESTIONS 11.90 19.58 15.87 18.78 20.63 25.40\nHOTPOT_QA 65.95 76.41 73.76 76.13 74.65 78.45\nAVG. OF CLOSED-BOOK QA 25.20 33.18 29.59 32.75 33.63 41.99\nQuestion answering/multiple-choice question answering\nCOSMOS_QA 7.30 10.98 9.91 10.78 10.85 11.32\nDREAM 49.19 71.83 58.70 61.00 59.53 62.42\nHELLASWAG 23.82 70.28 24.76 32.82 27.60 41.90\nOPENBOOKQA 44.80 54.40 50.20 52.20 53.80 57.00\nQASC 19.22 47.73 33.26 37.80 33.05 43.63\nQUAREL 54.89 54.71 57.25 59.78 57.61 62.50\nQUARTZ-NO_KNOWLEDGE 65.43 68.88 68.49 67.09 66.96 69.39\nQUARTZ-WITH_KNOWLEDGE 64.03 85.97 71.56 74.23 73.72 76.28\nRACE-HIGH 34.51 60.09 42.82 59.52 58.92 65.95\nRACE-MIDDLE 47.21 74.65 62.67 68.31 65.46 70.61\nSUPERGLUE-COPA 53.60 56.00 58.40 56.40 60.40 59.20\nWINO_GRANDE 48.42 58.20 50.79 61.20 50.47 67.19\nCOMMONSENSE_QA 58.43 76.76 58.43 62.52 60.72 61.21\nTable 1 (continued) | Overall (test) performance of over 100 NLP tasks comparing PT , PF, LR, AP and FT\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235 224\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nTask PT (BASE) PT (LARGE) PF LR AP FT\nSCIQ 96.95 98.53 98.08 98.42 98.19 98.30\nWIQA 36.10 65.27 63.67 77.99 64.44 79.82\nAVG. OF MULTIPLE-CHOICE QA 46.93 63.62 53.93 58.67 56.11 61.78\nQuestion answering/long-form question answering\nELI5-ASKH 11.26 11.70 12.64 11.99 11.45 13.00\nELI5-ASKS 14.79 15.54 15.09 15.25 15.01 15.28\nELI5-ELI5 14.19 15.38 15.23 14.59 14.43 14.75\nAVG. OF LONG-FORM QA 13.41 14.21 14.32 13.94 13.63 14.34\nQuestion answering/machine reading comprehension\nSUPERGLUE-RECORD 44.67 73.82 61.62 64.66 62.08 67.20\nMULTI_NEWS 18.09 19.23 18.81 19.44 19.10 19.80\nADVERSARIAL_QA 34.10 54.60 43.17 46.40 45.35 48.56\nAVG. OF READING COMPREHENSION 32.29 49.22 41.20 43.50 42.18 45.19\nConditional generation/summarization\nSAMSUM 39.35 45.12 43.38 45.00 44.68 45.73\nXSUM 21.35 26.56 23.84 25.87 26.07 29.90\nAVG. OF SUMMARIZATION 30.35 35.84 33.61 35.44 35.38 37.82\nConditional generation/other\nSPIDER 3.29 6.38 7.74 9.67 8.70 6.77\nWIKI_BIO 42.39 44.03 44.84 45.36 46.19 47.09\nWIKI_SPLIT 79.80 80.10 79.91 80.09 80.05 80.34\nAVG. OF OTHER GENERATION 41.83 43.50 44.16 45.04 44.98 44.73\nOther/linguistic phenomenon\nBLIMP-ANAPHOR_GENDER_AGREEMENT 100.00 100.00 100.00 100.00 100.00 99.00\nBLIMP-ELLIPSIS_N_BAR_1 49.00 100.00 100.00 100.00 100.00 100.00\nBLIMP-SENTENTIAL_NEGATION 54.00 100.00 100.00 100.00 100.00 100.00\n_NPI_SCOPE\nBLIMP-ANAPHOR_NUMBER_AGREEMENT 49.00 100.00 100.00 100.00 100.00 100.00\nBLIMP-DETERMINER_NOUN_AGREEMENT 46.00 100.00 100.00 100.00 100.00 100.00\n_WITH_ADJ_IRREGULAR_1\nBLIMP-EXISTENTIAL_THERE 53.00 100.00 100.00 100.00 100.00 100.00\n_QUANTIFIERS_1\nBLIMP-IRREGULAR_PAST 100.00 100.00 100.00 100.00 100.00 100.00\n_PARTICIPLE_ADJECTIVES\nBLIMP-WH_QUESTIONS_OBJECT_GAP 55.00 100.00 100.00 100.00 100.00 100.00\nAVG. OF LINGUISTIC PHENOMENON 63.25 100.00 100.00 100.00 100.00 99.88\nOther/generate explanation\nCOS_E 12.41 14.82 13.90 14.05 14.31 13.46\nOther/slot filling\nADE_CORPUS_V2-DOSAGE 78.57 89.29 82.14 85.71 82.14 82.14\nADE_CORPUS_V2-EFFECT 59.15 61.35 63.25 62.52 60.91 62.66\nAVG. OF SLOT FILLING 68.86 75.32 72.70 74.12 71.53 72.40\nOther/other\nACRONYM_IDENTIFICATION 93.35 96.68 96.12 96.12 95.57 96.12\nASLG_PC12 15.78 44.07 47.71 73.72 80.65 92.92\nCRAWL_DOMAIN 68.16 76.91 73.04 73.00 72.76 75.12\nPROTO_QA 21.16 37.66 24.57 27.87 26.17 34.47\nAVG. OF OTHER TASKS 49.61 63.83 60.36 67.68 68.79 74.66\nAVG. OF ALL TASKS 49.80 67.18 65.08 67.31 66.80 69.27\nWe experiment all methods on T5BASE, with the best performance highlighted in bold, and also report the performance of PT on T5LARGE.\nTable 1 (continued) | Overall (test) performance of over 100 NLP tasks comparing PT , PF, LR, AP and FT\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 225\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n12,8006,4003,2001,600800400200100\n25,600\nSteps\n0.99 0.97\n0.92\n0.88\n0.84\n0.79\n0.94\n0.89\n0.84\n0.79\nEM\nClassification-F1\nClassification-F1Classification-F1Classification-F1\nEM\nEM EM ACC\nacronym_identification ag_news anli\ncircaboolqaslg_pc12\ncommonsense_qa crawl_domain discovery\ndream freebase_qa glue-mnli\nglue-qnli glue-qqp glue-sst2\nACCACCACC\nACC\nACC ACC\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\nPF\nFT\nAP\nLR\n0.54\n0.41\n0.28\n0.15\n0.02\n0.90\n0.69\n0.48\n0.26\n0.050.36\n0.48\n0.60\n0.72\n0.840.97\n0.72\n0.48\n0.24\n0\n0.69\n0.52\n0.36\n0.19\n0.02\n0.68\n0.51\n0.34\n0.17\n0\n0.94\n0.82\n0.7\n0.57\n0.45\n0.93\n0.77\n0.62\n0.46\n0.3 0.44\n0.57\n0.71\n0.85\n0.98\n0.29\n0.44\n0.60\n0.75\n0.90\n0\n0.06\n0.13\n0.19\n0.25\n0\n0.06\n0.12\n0.17\n0.23\n0.18\n0.33\n0.48\n0.62\n0.77\nFig. 1 | The performance of T5BASE with different delta-tuning methods  \n(LR, AP and PF) and fine-tuning (FT) at different training steps. Note we apply \nearly stopping to all methods. We choose three metrics: (1) exact match (EM), \nwhich measures the percentage of correctly predicted answers that exactly \nmatch the ground-truth answer; (2) classification F1, which is calculated as the \nharmonic mean of precision and recall; and (3) accuracy (ACC), which measures \nthe percentage of correctly predicted instances out of all instances. The \nperformance of PT is omitted as it lags far behind other tuning methods in both \nconvergence and performance. The convergence rate of these tuning methods is \nranked as: FT > AP ≈ LR > PF .\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235 226\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\n 6. Transferability. Existing delta-tuning methods could well sup -\nport knowledge transfer, showing non-trivial transferability \namong downstream tasks of similar categories. The finding sug-\ngests that we could establish a common platform to share and \nmigrate these lightweight delta objects (that is the portion of \nthe fine-tuned parameters).\nWe discuss the practicality and applications of delta-tuning from \nvarious perspectives in Supplementary Section 6, including efficient \ntraining and shareable checkpoints, multi-task learning, catastrophic \nforgetting mitigation and model-as-service. Hopefully, this Analysis \nwill inspire research to advance the efficient adaptation of large lan -\nguage models.\nResults\nAs an effective engine to stimulate large-size PLMs, delta-tuning pre-\nsents an enormous practical potential for various real-world appli -\ncations. We carried out systematic experiments to gain a deeper \nunderstanding of the attributes of different mainstream delta-tuning \nmethods. Specifically, (1) we first conduct thorough comparisons \namong four representative delta-tuning methods and fine-tuning, \ncovering the performance, convergence and the efficiency analysis. \n(2) We explore the combinability of three representative delta-tuning \nmethods by comparing the performance under both the full-data and \nlow-resource settings. We also explore the effects of manual templates \nand compare the generalization gap of different delta-tuning methods. \nFurthermore, we investigate (3) the scaling law and (4) the transfer-\nability of delta-tuning methods among different downstream tasks. \nThe implementation details and tasks are described in Supplementary \nSections 3 and 4.\nPerformance, convergence and efficiency\nExperimental setting. We evaluate vanilla fine-tuning (FT) and four \nrepresentative delta-tuning methods, including prompt-tuning (PT), \nprefix-tuning (PF), LoRA (LR) and adapter (AP). We follow the common \npractice for each delta-tuning implementation, and the training details \nare provided in Supplementary Section 3.1.\nT o cover broad and diverse NLP tasks, we select over 100 repre-\nsentative tasks from Huggingface datasets18. The selected tasks include \ntext classification (for example, sentiment analysis and natural lan -\nguage inference), question answering (for example, machine reading \ncomprehension and multi-choice question answering), conditional \ngeneration (for example, summarization and dialogue) and so on. We \nlist the task details of each category in Supplementary Table 4. T o han-\ndle different tasks with a single text-to-text PLM, we process the input \nand output of each task into the same sequence-to-sequence format. \nT5BASE and T5LARGE are two PLMs with the T5 architecture released by \nref. 8. We choose T5BASE (ref. 8) as the mainly evaluated PLM backbone \nfor different tuning methods, and we also report the performance of \nPT with T5\nLARGE (ref. 8).\nPerformance analysis. The overall results are listed in Table 1, from \nwhich we observe the following.\n 1. In general, despite the substantial reduction of tunable param-\neters, different delta-tuning methods are almost comparable \nto FT in performance in most cases. This demonstrates the po -\ntential of driving large-scale PLMs through parameter-efficient \nadaptation.\n 2. Despite having different design elements, PF , LR and AP are \ncomparable to each other in performance. Specifically, each can \nshow dominant performance (even better than FT) over others \non certain tasks. According to the average results, the perfor-\nmances of all the methods are ranked as FT > LR > AP > PF > PT. \nInterestingly, the performance of the delta-tuning methods is \nnot consistent with their number of tunable parameters, that is, \nat least on small PLMs, more tunable parameters do not neces-\nsarily lead to better performance, and the design of the struc-\nture for delta-tuning may play a greater role.\n 3. PT lags far behind other delta-tuning methods in most cases, \ndespite being the easiest method to implement (that is, without \nmodifying the internal structure of the model). Another inter-\nesting finding is that, better PT performance is observed when \nthe model size is enlarged to T5\nLARGE, which is aligned with previ-\nous findings on the power of scale for prompt-tuning 19. Howev-\ner, as we show later, other delta-tuning methods also exhibit far \nbetter performance when the scale of the backbone PLM grows \nextremely large. The phenomenon implies that when the model \nsize increases sharply, the design of the structure may become \nless important for delta-tuning methods.\nConvergence analysis.  In Fig. 1 , Extended Data Fig. 1 and Supple -\nmentary Fig. 3, we visualize the performance of different delta-tuning \nmethods (LR, AP and PF) and fine-tuning (FT) at different training steps \nto compare their convergence rate. We also report the convergence \nrate with respect to training time in Extended Data Fig. 2. As PT lags \nfar behind other tuning methods in convergence, we do not visualize \nit in the figures. However, as mentioned in Methods, PT is the easiest \nmethod to implement and it is the desirable method to theoretically \nand empirically study the convergence issue across different sizes of \nPLMs. Our findings are summarized as follows.\n 1. The convergence rate of these tuning methods is ranked as: \nFT > AP ≈ LR > PF . Overall, FT converges the fastest.\n 2. We also find empirically that, (1) within a reasonably broad range, \nthe performance and convergence of each delta-tuning method \nare not sensitive to the number of tunable parameters, but more \nsensitive to the structures of the methods, and (2) with the scale \nof PLM growing larger, the convergence of delta-tuning is also \naccelerated (see ‘The power of scale for delta-tuning’ section).\nT o summarize, our experiments yield similar conclusions \nin convergence and overall performance. These conclusions are  \n10.0\n7.5\n5.0\n2.5\n0\n30.0\n22.5\n15.0\n7.5\n0 0\n25.0\n50.0\n75.0\n100.0\n0 1 8\nBatch size\nT5 BASE\nGPU memory (GB)\n32 64\n0.90.80.8 0.90.90.9\n1.61.61.4\n4.4\n7.3\n4.03.8\n3.3\n5.8\n6.87.2\n9.8\n3.73.3\n0 1 8\nBatch size\n32 64 0 1 8\nBatch size\nFT\nAP\nLR\nBF\n32 64\nT5 LARGE T5 XL\n11.0 11.6\n13.5\n2.82.82.8 2.92.93.0\n4.84.74.4\n11.010.59.3\n15.9\n18.319.3\n21.1\n28.1\n42.5\n10.710.710.7 11.211.211.1\n16.116.015.8\n33.132.630.5\n55.654.652.3\n66.5\n90.5\n48.5\n43.6\nFig. 2 | GPU memory consumed by each delta-tuning method and fine-tuning. We choose three T5 models with different scales to assess the GPU memory.  \nAll evaluations are conducted on NVIDIA A100 GPUs.\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 227\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nwell supported by the fact that we used the same experimental and \nimplementation set-up, the same model selection strategy and \ndiverse tasks.\nEfficiency analysis. Here we study the efficiency of delta-tuning from \nthe perspectives of memory efficiency and computation efficiency. For \nmemory efficiency, to validate the efficiency of graphics processing \nTable 2 | Results of combining different delta-tuning methods\nPrompt ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓\nBitFit ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✓\nAdapter ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓\nTunable parameters 0% 1.75% 0.09% 1.84% 0.003% 1.76% 0.09% 1.85%\nRoBERTaLARGE, full data, without manual templates\nCoLA(Matt.) 4.6 66.61.6 63.50.6 65.90.5 42.72.3 63.11.5 63.70.9 64.40.9\nSST-2(acc) 50.9 95.80.1 95.60.1 95.70.2 95.30.2 95.70.1 95.30.2 95.50.1\nMRPC(F1) 1.4 92.70.2 91.90.4 93.00.4 85.40.5 92.00.5 92.20.5 92.90.3\nSTS-B(Pear.) -6.2 91.40.1 90.70.2 90.50.1 83.02.8 90.50.4 90.30.7 90.90.1\nQQP(F1.) 6.4 83.50.1 83.50.0 84.40.0 77.20.4 84.30.0 83.60.1 84.40.0\nMNLI(acc) 34.2 88.60.2 88.00.2 89.00.1 77.92.5 88.90.1 88.00.2 88.90.1\nQNLI(acc) 50.6 93.70.3 93.40.3 94.20.1 86.20.5 94.20.1 93.20.3 94.40.1\nRTE(acc) 47.7 86.80.5 86.21.0 84.50.5 74.40.5 84.10.8 85.71.5 84.71.1\nAverage 23.7 87.40.4 86.60.4 87.10.2 77.71.2 86.60.4 86.50.6 87.00.3\nRoBERTaLARGE, full data, with manual templates\nCoLA(Matt.) 2.2 66.91.1 64.20.5 65.51.0 37.820.8 64.71.3 64.80.7 64.91.0\nSST-2(acc) 83.6 96.30.2 96.10.1 96.20.2 95.70.2 95.80.1 95.90.1 95.80.2\nMRPC(F1) 61.9 92.20.4 92.70.6 92.70.2 84.20.5 91.80.2 92.20.4 92.00.4\nSTS-B(Pear.) -3.3 91.30.5 90.90.1 90.70.2 79.61.3 91.90.3 90.80.4 90.10.6\nQQP(F1) 49.7 83.60.1 83.60.0 84.60.1 77.00.7 84.30.0 83.70.0 84.40.2\nMNLI(acc) 50.9 88.60.1 87.70.1 88.70.1 80.20.2 88.70.1 88.00.1 88.90.1\nQNLI(acc) 50.8 93.60.1 93.10.2 93.80.1 86.60.4 93.80.1 93.00.1 93.80.1\nRTE(acc) 51.3 86.90.2 86.21.0 86.00.7 78.30.3 84.60.5 86.41.5 84.70.9\nAverage 43.4 87.40.3 86.80.3 87.30.3 77.43.0 86.90.3 86.90.4 86.80.4\nRoBERTaLARGE, 16 shot, without manual templates\nCoLA(Matt.) 4.6 19.69.6 15.117.0 17.711.4 3.50.6 21.411.5 20.819.6 21.513.4\nSST-2(acc) 50.9 92.70.4 92.70.6 93.10.6 74.90.6 91.70.8 92.20.5 91.60.7\nMRPC(F1) 1.4 78.24.4 69.81.6 81.20.0 6.24.1 74.67.1 69.36.5 77.45.4\nSTS-B(Pear.) -6.2 66.52.5 67.58.0 71.02.5 10.73.5 63.31.6 64.75.6 69.68.6\nQQP(F1) 6.4 55.95.8 55.16.8 54.64.2 52.41.4 58.37.2 55.14.8 58.56.1\nMNLI(acc) 34.2 58.14.5 64.63.4 62.74.1 35.30.6 61.43.9 61.45.1 61.03.8\nQNLI(acc) 50.6 60.23.0 69.71.9 59.81.7 52.81.0 60.24.9 60.94.0 61.67.0\nRTE(acc) 47.7 55.01.6 54.50.8 54.92.9 50.10.7 58.22.5 54.62.4 58.73.4\nAverage 23.7 60.84.0 61.15.0 61.93.4 35.71.6 61.24.9 59.96.1 62.56.0\nRoBERTaLARGE, 16 shot, with manual templates\nCoLA(Matt.) 2.2 10.515.0 4.65.0 9.210.2 1.41.7 10.24.2 5.92.5 5.95.5\nSST-2(acc) 83.6 93.10.3 92.90.1 92.10.1 90.90.6 91.90.4 92.00.4 92.20.6\nMRPC(F1) 61.9 77.21.4 74.54.9 81.20.0 72.14.4 76.81.3 76.12.4 81.20.0\nSTS-B(Pear.) -3.3 65.84.7 69.36.0 71.04.1 12.08.0 61.75.7 71.36.4 67.12.8\nQQP(F1) 49.7 66.60.5 67.80.5 66.34.1 53.41.0 66.91.9 68.61.2 67.12.9\nMNLI(acc) 50.9 68.01.4 69.43.3 68.90.4 53.22.5 67.11.8 67.12.0 68.10.3\nQNLI(acc) 50.8 69.51.1 70.23.4 68.12.4 59.40.5 69.92.5 72.53.9 70.42.3\nRTE(acc) 51.3 70.63.6 67.35.1 73.02.0 56.34.6 70.42.3 69.23.5 72.42.8\nAverage 43.4 65.23.5 64.53.5 66.22.9 49.82.9 64.42.5 65.32.8 65.62.2\nPerformance of RoBERTaLARGE on GLUE datasets. We report the average result of multiple random seeds on the validation set. A tick symbol denotes that the component is included in the \ncombination and a cross symbol denotes that it is excluded in the combination. The best performance of each dataset is highlighted in bold.\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235 228\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nAdapter (MNLI)\n Adapter (QNLI)\n Adapter (SST-2)\nLoRA (MNLI)\n LoRA (QNLI)\n LoRA (SST-2)\nPre/f_ix-tuning (MNLI)\n Pre/f_ix-tuning (QNLI)\n Pre/f_ix-tuning (SST-2)\nLast-layer tuning (MNLI)\n Last-layer tuning (QNLI)\n Last-layer tuning (SST-2)\nSelective-module tuning (MNLI)\n Selective-module tuning (QNLI)\n Selective-module tuning (SST-2)\n10,0008,0006,000\nSteps\n4,0002,0000 10,0008,0006,000\nSteps\n4,0002,000010,0008,0006,000\nSteps\n4,0002,0000\n10,0008,0006,000\nSteps\n4,0002,000010,0008,0006,000\nSteps\n4,0002,000010,0008,0006,000\nSteps\n4,0002,0000\n10,0008,0006,000\nSteps\n4,0002,0000 10,0008,0006,000\nSteps\n4,0002,0000 10,0008,0006,000\nSteps\n4,0002,0000\n10,0008,0006,000\nSteps\n4,0002,000010,0008,0006,000\nSteps\n4,0002,000020,00015,000\nSteps\n10,0005,0000\n10,0008,0006,000\nSteps\nACC\n4,0002,0000 10,0008,0006,000\nSteps\n4,0002,0000 8,0006,000\nSteps\n4,0002,0000\na b c\nd e f\ng h i\nj k l\nm n o\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nACC\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)XXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)XXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\nXXL\nBASE\nSMALL\nXXL (fine-tune)\n0.96\n0.72\n0.48\n0.24\n0 0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\n0.96\n0.72\n0.48\n0.24\n0\n0.96\n0.72\n0.48\n0.24\n0\n0.96\n0.72\n0.48\n0.24\n0\n0.96\n0.72\n0.48\n0.24\n0\nFig. 3 | The power of scale of delta-tuning methods. a–o, We perform all delta-\ntuning methods on different scales of T5: T5SMALL(), T5BASE() and T5XXL(). We report \nthe performance of Adapter in (a–c), LoRA in (d–f), Prefix-tuning in (g–i),  \nLast-layer tuning in (j–l), and Selective-module tuning in (m–o). From this figure, \nwe can observe that with the scale of T5 increasing, all delta-tuning methods \ncould converge faster and achieve better performance on MNLI, QNLI and SST-2.\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 229\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nunit (GPU) memory for delta-tuning, in Fig. 2, we conduct experiments \nto compare the GPU memory consumed by different delta-tuning \nmethods and fine-tuning across different PLM scales. T5XL is the PLM \nwith the T5 architecture released by ref. 8. Specifically, we choose three \nscales of the T5 model, that is, T5BASE, T5LARGE and T5XL, and test the peak \nGPU memories under different batch sizes. The static GPU memories, \nwhich leave out the intermediate tensors such as hidden states, are \ndrawn on Batchsize=0. We use a NVIDIA A100 GPU (maximum GPU \nmemory 39.58 GB) and library OpenDelta for these experiments. For \nthe cases that consume more GPU memory than a single A100, we \nparallelize the model across multiple GPUs, which does not introduce \nadditional memory consumption. We observe from the figure that \nunder small batch sizes (for example, 1 and 8), delta-tuning saves up \nto 3/4 GPU memory; under large batch sizes (for example, 32 and 64), \ndelta-tuning saves about 1/2–1/3 GPU memory. This demonstrates that \ndelta-tuning saves GPU memory by alleviating the need for gradient \ncomputations for most of the parameters. Given the fact that small \nbatch sizes are preferred when utilizing big models, delta-tuning has \ngreat potential to apply to large-scale PLMs. Furthermore, among the \ninvestigated methods, BitFit is the most memory efficient.\nIn addition, although delta-tuning may converge slower than \ntraditional fine-tuning, the computations of the tunable parameters \nin the optimizer are greatly reduced, which speeds up training. We \ncompare the forwards time and the backwards time of prompt-tuning, \nBitFit, adapter tuning and fine-tuning in Extended Data Fig. 3, varying \nthe input length. For a fair comparison, we keep the batch size the same. \nFrom the results, we can see that:\n 1. The structure of the delta-tuning methods could have a con-\nsiderable impact on the time of a single forwards or backwards \nprocess. By greatly reducing the computations of the tunable \nparameters, the backwards time of delta-tuning methods is \nshorter than fine-tuning.\n 2. As the adapter injects additional neural modules to each layer \nof the transformer model, the path of data flow becomes longer \nand further leads to inference latency (longer forwards time).\nCombinations of delta-tuning methods\nConsidering that different delta-tuning methods are compatible \nwith each other, which means they could be applied on the same PLM \ntogether, we investigate whether such a combination would bring addi-\ntional benefits. Specifically, we evaluate both simultaneous combi-\nnation and sequential combination. We choose three representative \ndelta-tuning methods, including prompt-tuning, BitFit and adapter, \nto explore the effects of their combinations. The training details are \ndescribed in Supplementary Section 3.2.\nSimultaneous combination. We first explore the effects of directly \napplying all the three delta-tuning methods simultaneously. RoBERTaLARGE \nis the PLM released by ref. 20 and GLUE21 is the official benchmark for lan-\nguage understanding ability evaluation. The experiments are conducted \nusing RoBERTaLARGE on eight tasks of GLUE (full-data setting), and we \nreport the performance on the official development sets. We also test \nthe performance of RoBERTaLARGE under the few-shot setting, where we \nrandomly sample 16 training examples per label to construct the new \ntraining set and development set, respectively. Similar to prompt-based \nfine-tuning22, we insert a natural language prompt template into the input \ntext for each task, and the detailed implementations are described in \nSupplementary Section 3.2.\nWe list the results of simultaneous combination for RoBERTaLARGE in \nTable 2 (the results of T5BASE are listed in Extended Data Table 2, with dis-\ncussions in Supplementary Section 3.2), from which we conclude that:\n 1. Under both the full-data setting and few-shot setting, introduc-\ning adapter into the combination almost always conduces to \nthe average performance across GLUE tasks no matter whether \nthere exist manual templates.\n 2. Introducing prompt-tuning into the combination generally \nharms the average performance, showing that prompt-tuning \nmay not be compatible with the other two delta-tuning methods.\n 3. Introducing BitFit into the combination generally improves the \naverage performance.\n 4. Manual templates could substantially improve the zero-shot \nperformance (from 23.7 to 43.4) by narrowing the gap between \ndownstream tuning and pre-training. Under the few-shot set-\nting, manual templates could also help boost the average per-\nformance evidently. However, when the training supervision is \nabundant (full-data setting), manual templates only show mar-\nginal improvements.\nSequential combination. In addition to the simultaneous combina -\ntion, we further investigate the compatibility when the above three \ndelta-tuning methods (prompt-tuning, BitFit and adapter) are sequen-\ntially introduced. Specifically, we split the whole tuning process into \nthree stages. During each stage, we train an individual delta-tuning \nmethod for 6,000 steps; in the following stages, we freeze the tuned \nparameters in the previous stages and optimize only the newly intro-\nduced delta parameters. SST-2 (ref. 23) is the dataset that evaluates \nthe sentiment analysis ability. We experiment with RoBERTa LARGE on \nSST-2 with and without manual templates. The results are visualized \nin Extended Data Fig. 4, from which it is derived that:\n 1. Under certain cases, the performance can be improved with the \ninvolvement of subsequent delta-tuning methods.\n 2. However, there does not exist an optimal sequential combina-\ntion strategy that could dominate other combination strategies \nunder different settings.\nGeneralization gap.  In addition, we report the generalization gap \n(train performance − dev performance) for RoBERTa LARGE under the \nfull-data setting, with the results shown in Extended Data Table 3. It \nis derived that:\n 1. The gap of a single delta-tuning method is always smaller than \nfine-tuning, which means over-parameterization may help bet-\nter memorize (overfit) training samples. Among all the delta- \ntuning methods, prompt-tuning tends to have the smallest gen-\neralization gap. Considering that each delta-tuning method \ncould already generalize well and achieve non-trivial perfor-\nmance on the development set, overfitting the training set may \nnot be the prerequisite for good generalization.\n 2. In general, combining delta-tuning methods would enlarge the \ngeneralization gap, even to the extent that is comparable to \nfine-tuning, despite tuning far fewer parameters. This suggests \nthat, for the investigated tasks, memorizing the training set may \nnot require employing all of the parameters; in other words, a \nsmall model capacity during downstream adaptation may be \nenough for good memorization.\n 3. Utilizing manual templates generally would not influence the \ngeneralization gap.\nConclusion.  The above experiments indicate that different \ndelta-tuning methods have distinct functionalities for the optimization \nΘ/uni2032 =\nPre-trained PLM\nΘ =\nΘ/uni2032 =\nΘ/uni2032 =\nAddition\nSpecification\nReparameterization\nFrozen parameters Tunable parameters\nDelta-tuning\nΘ /uni2192 Θ/uni2032 \nFig. 4 | The categorization criterion of delta-tuning. Here Θ denotes the pre-\ntrained parameters and Θ′ represents the well-tuned parameters.\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235 230\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nof PLMs; thus, combining them is generally conducive to the down -\nstream performance. However, as shown in the above results, the opti-\nmal combination of delta-tuning methods may vary considerably under \ndifferent settings. That being said, it would be interesting to explore \nthe mechanisms behind the inductive biases brought by different \ndelta-tuning methods under different cases in the future. Besides, we \nalso encourage future research explorations to systematically report \nthe performance of their proposed delta-tuning methods on various \nPLM backbones under different settings thoroughly.\nThe power of scale for delta-tuning\nWith the scale of the backbone PLM growing, prompt-tuning becomes \nmore and more competitive in performance, and would even achieve \ncomparable performance to fine-tuning for a PLM with over 10 bil -\nlion parameters19, and the convergence speed of prompt-tuning ben-\nefits from the scaling law. In this section, we explore whether other \ndelta-tuning methods also exhibit the power of scale. MNLI and QNLI \nare two natural language inference dataset, and T5SMALL and T5XXL are \ntwo PLMs with the T5 architecture released by ref. 8. Specifically, we \nexperiment on the task of MNLI, QNLI and SST-2, and choose three \nPLMs (T5SMALL, T5BASE and T5XXL) of increasing sizes, and evaluate the \nperformance of five representative delta-tuning methods (adapter, \nLoRA, prefix-tuning, last-layer tuning and selective-module tuning). We \ndescribe the percentages of the tuned parameters for each method in \nall scales of the PLM in Supplementary Table 3. The training details are \nprovided in Supplementary Section 3.3. The results are visualized in \nFig. 3. From Fig. 3a–i, we observe that with the scale of the PLM growing, \nboth the performance and the convergence of all delta-tuning methods \nare greatly improved. All delta-tuning methods tend to show compa-\nrable performance to fine-tuning, even for a small-scale PLM (T5\nBASE).\nOn the basis of the existing results, we further design two \ndelta-tuning methods: last-layer tuning and selective-module tun -\ning. For last-layer tuning, we optimize the last layer in the T5 encoder; \nfor selective-module tuning, we randomly choose some modules (for \nexample, the feed-forward layer, query/key/value matrix in the atten-\ntion layer, or a layer norm) in the T5 model to be tunable. The results \nare visualized in Fig. 3j–l,m–o, from which we could conclude that:\n 1. Both methods show promising results, especially when the \nscale of the PLM is extremely large, with selective-module tun-\ning slightly better than last-layer tuning. These results suggest \nthat confining the optimization within a specific layer may not \nbe a good strategy (for example, the case of prompt-tuning and \nlast-layer tuning).\n 2. Furthermore, randomly choosing modules across different lay -\ners could achieve excellent performance when the scale of PLMs \ngrows extremely large.\nIn general, the above results imply that the power of scale may be \na common phenomenon for delta-tuning. We hypothesize the exist -\nence of such a phenomenon is because larger PLMs generally have \nsmaller intrinsic dimensionalities16; therefore, merely tuning minimal \nparameters could obtain a strong enough representation ability to \nachieve non-trivial performance in downstream tasks; furthermore, \nthe over-parameterization and large-scale pre-training may make \nPLMs more unlikely to get stuck in a local optimum during downstream \noptimization, and thus the convergence is accelerated.\nTask-level transferability evaluation\nRecent studies24–26 have demonstrated that prompt-tuning has excel-\nlent cross-task transferability. In this subsection, we explore the \ncross-task transferability of four delta-tuning methods (prompt-tuning, \nprefix-tuning, adapter and LoRA) with 12 tasks of 5 different types (senti\n-\nment analysis, natural language inference, paraphrase identification, \nquestion answering and summarization). We transfer the trained delta \nparameters to the unseen target tasks. More training and dataset details \nare provided in Supplementary Section 3.4.\nIn experiments, we report their relative performance (zero-shot \ntransferring performance and original performance). The results \nare shown in Extended Data Fig. 5, from which we can observe \n \nthat:\n 1. For the tasks belonging to the same category, transferring tuned \nparameters among them generally performs well; for the tasks \nof different types, transferring delta parameters among them \ngenerally achieves poor performance.\n 2. We also find that transferring tuned parameters from the text \ngeneration tasks such as question answering and summariza-\ntion can achieve non-trivial performance on sentiment analysis, \nindicating that text generation might be a complex task that in-\ncludes the knowledge required to solve the sentiment analysis \ntasks. In general, the above results demonstrate that it is promis-\ning to utilize trained delta parameters for similar tasks through \nknowledge transfer.\nConclusion\nThis Analysis focuses on parameter-efficient methods, that is, \ndelta-tuning, for PLMs. We first describe the problem and provide a \ncategorization to survey the development of delta-tuning systemati-\ncally. Captivated by the empirical evidence, we propose two frameworks \nto theoretically discuss delta-tuning from the optimization and optimal \ncontrol perspectives. Our discussion sheds light on the theoretical \nreferences of a novel design for delta-tuning methods and hopefully \ncould inspire a deeper understanding of model adaptation for PLMs. \nEmpirically, we conduct extensive experiments across 100+ NLP tasks \nto fairly evaluate and explore the combinatorial property, influence \nof scale and transferability for delta-tuning. In terms of performance, \ndelta-tuning can be slightly behind or comparable to fine-tuning on a \nwide range of tasks, and the gap shrinks as the model scales; in terms of \nefficiency, delta-tuning could considerably reduce storage space and \nmemory usage, as well as accelerate backpropagation. In summary, \ndelta-tuning shows considerable potential to stimulate large PLMs, \nand we hope that the paradigm can be further theoretically studied \nand empirically practiced.\nMethods\nDelta-tuning is developed on the success of PLMs, which use deep \ntransformers as the base structure and adopts pre-training objectives \non large-scale unlabelled corpora. For more information about PLMs \nand transformers, see Supplementary Section 1 or related surveys 27 \nand original papers4,5,8,9.\nGiven a pre-trained model Θ = {w1, w2, ..., wN} and training data 𝒟𝒟, \nthe objective of PLM adaptation is to produce the adapted model \nϴ′ ={ w′\n1,w′\n2,..., w′\nM}, where wi is the model parameter. Define ΔΘ as the \nchange in the adapted model Θ′ compared with Θ, including the change \nin values and the number of elements. In vanilla fine-tuning, N = M and \nΔϴ = ∇fϴ(𝒟𝒟𝒟 is the update value of all parameters in Θ with respect to \ntraining data, where fΘ represents the resulting loss of applying model \nΘ to training data D. Note that in this case, we omit the small set of \nparameters brought by extra classification heads for downstream tasks. \nWhile in delta-tuning, ΔΘ refers to the modification of a small number \nof parameters. Empirically, |ΔΘ| = |Θ| in vanilla fine-tuning, while for \ndelta-tuning, |ΔΘ| ≪ |Θ|, where |⋅| indicates the number of parameters \ninvolved.\nT o organize them under a unified framework, we categorize the \ndelta-tuning methods into three groups according to the operations \non the delta parameters (as illustrated in Fig. 4 ): addition-based, \nspecification-based and reparameterization-based approaches.\n•\t Addition-based methods introduce extra trainable neu-\nral modules or parameters that do not exist in the original \nmodel or process. In addition-based methods, M ≥ N and \nΔΘ = {w\nN+1, wN+2, ..., wM}.\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 231\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\n•\t Specification-based methods specify certain parameters in the \noriginal model or process become trainable, whereas others are \nfrozen. Denote the set of trainable parameters as \n𝒲𝒲, then \nΔΘ = {Δw1, Δw2, ..., ΔwN}. When wi ∈𝒲𝒲 , Δwi is the incremental \nvalue from wi to w′\ni, else, Δwi = 0.\n•\t Reparameterization-based methods reparameterize existing \nparameters to a parameter-efficient form by transformation. \nDenote the set of parameters to be reparameterized as \n𝒲𝒲, and \nsuppose that each wi ∈𝒲𝒲  is reparameterized with new param-\neters R(wi𝒟={ u1,u2,..., uNi }, then Δϴ =( ϴ⧵𝒲𝒲𝒟𝒲𝒲𝒲 , where \n𝒲𝒲={ uj|∃wi ∈𝒲𝒲, uj ∈ R(wi𝒟}.\nAddition-based methods\nWith the above definition in mind, addition-based methods intro -\nduce additional parameters to the neural network. In this section, we \nintroduce two branches of representative addition-based methods, \nadapter-based tuning and prompt-based tuning.\nAdapter-based tuning . As a seminal work in delta-tuning, \nadapter-based methods inject small-scale neural modules (adapters) \nto the transformer layers and only tune these adapters for model adap-\ntation. Although such a strategy leaves an open choice of adapter \nstructures, a simple instantiation13 achieves impressive performance \nand has become the most widely used baseline in recent research. \nSpecifically, one adapter module contains a down-projection and an \nup-projection. For an input feature h ∈ℝ d, a down-projection projects \nthe input to a r-dimensional space with a parameter matrix Wd ∈ℝ d×r, \nafter which a nonlinear function f (⋅) is applied. Then the up-projection \nWu maps the r -dimensional representation back to d -dimensional \nspace. Added with a residual connection, the complete computation \ncould be written as h← f(hW\nd)Wu+h.\nIn each block, the adapter modules are separately inserted after \nthe multi-head self-attention and the feed-forward network sublayers, \nwhich reduces the tunable parameters per layer to 2 × (2dr (projection-\nmatrices) + d (residualconnection) + r (biasterm)). Practically, about \n0.5–8% of parameters of the whole model 13 could be involved in the \ntuning process under such a strategy.\nAlthough an adapter works with much fewer tunable parameters \nthan vanilla fine-tuning, some work attempts a more rigorous saving \nstrategy by introducing inductive biases into the structure of the \nadapter layer. For example, Compacter28 proposes to use a combination \nof hypercomplex multiplication and parameter sharing. The hyper -\ncomplex multiplication parameterizes the original linear layer as the \nsum of the Kronecker products of two small matrices. Taking the \ndown-projection as an example, Wd = ∑\nn\ni=1 Ai ⊗Bi, where A ∈ℝ n×n and \nB ∈ℝ\nd\nn\n×\nr\nn .\nTheir method reduces the parameter complexity of the normal \nadapter layer from 𝒪𝒪(dr𝒟 to 𝒪𝒪(d+r𝒟 without harming the performance. \nIt also shows that a simple low-rank decomposition of the linear layer \nleads to comparable performance with the adapter layer, that is, \nWd = ABT, where A ∈ℝ d×n, B ∈ℝ r×n and n ≪ min(d,r𝒟, where the super-\nscript T means matrix transposition.\nAs an addition-based approach, adapter-based tuning has the \nadvantage of placing multiple adapter instances on a pre-trained \nmodel simultaneously, which can benefit many application scenarios. \nFor example, multi-task learning 29,30 is an advantageous setting for \nadapter-based methods, inserted with adapter modules in parallel with \nthe self-attention module, PLMs could demonstrate impressive rep -\nresentational capacity in the multi-task setting. In contrast to directly \nconducting multi-task learning on adapters, adapterFusion 31 first \npre-trains task-specific adapters and then combines the representa -\ntions of the pre-trained adapters to leverage the cross-task knowledge \nand enhance the performance of transfer learning.\nIn terms of computational efficiency, the training of adapters \ncould be 60% faster than vanilla fine-tuning while the inference is only \n4–6% slower. In addition, the computational cost could be further \nreduced dynamically by removing adapters from lower transformer \nlayers32. Research also shows that adapter-based fine-tuning demon-\nstrates better robustness than fine-tuning. Specifically, adapter-based \nfine-tuning could perform better than vanilla fine-tuning on few-shot \nand cross-lingual scenarios 33 and is more robust under adversarial \nattacking34. We provide a comparison of different adapters, as well as \nother delta-tuning methods in Extended Data Table 4.\nT o sum up, adapters are lightweight additional neural modules \nthat could be trained in a task-specific style, which could be regarded \nas ‘encapsulation’ of task information (in fact, this perspective can be \napplied to all the ‘deltas’). Although in an ideal world, adapters could \nbe freely shared and reused by researchers, in practice, sharing and \nreusing such modules face substantial obstacles. Taking the first step, \nAdapterHub35 provides a feasible platform and toolkit to deploy adapt-\ners inside the transformer-based models.\nPrompt-based tuning.  Instead of injecting neural modules to the \ntransformer model, prompt-based methods wrap the original input \nwith additional context. As a strategy to stimulate PLMs by mimick -\ning pre-trained objectives in the downstream tasks, prompt-based \nlearning has achieved promising performance in various NLP tasks36,37, \nespecially in low-data settings. The introduction of the technique and \nimplementations of prompt-based learning have already been compre-\nhensively presented in other literature38,39. In this paper, we primarily \nfocus on the parameter-efficient attribute of prompt-based learning \n(only prefixes or prompts are optimized) and pay less attention to the \nsettings where the models and prompts are simultaneously optimized.\nAn important seminal work of this branch of research is \nprefix-tuning40, which prepends trainable continuous tokens (prefixes) \nto the input and hidden states of each transformer layer. Each prefix is \ndrawn from a newly initialized trainable parameter matrix P, whereas \nother parameters of the pre-trained model remain unchanged during \ntraining. During generation, if an activation h i is in a prefix position, \nit is the direct copy of the corresponding trainable parameter; other-\nwise, the activation is computed by the model as hi = LM(zi, h<i), where \ni is the position index, z  is the input and LM stands for the language \nmodel. It is worth noting that the paradigm could be applied to both \nautoregressive and encoder–decoder models. Such a strategy could \nbe effectively applied to natural language understanding with differ-\nent scales of models\n41.\nCompared with prefix-tuning, which adds tunable prefixes to \nevery intermediate transformer layer, prompt-tuning 19 proposes a \nmore simplified strategy that only adds soft prompts to the input layer. \nSimilar to prefix-tuning, the newly introduced prompts are not param-\neterized by the pre-trained model but an additional parameter matrix. \nAnd during training, the parameters of soft prompts are updated by \ngradient descent while the model parameters keep frozen. As the model \nsize increases, the performance gap between prompt-tuning and full \nparameter fine-tuning is narrowed. In particular, when the model scales \nto T5XXL with 11 billion parameters, prompt-tuning yields comparable \nperformance on SuperGlue with fine-tuning. This strategy also exhibits \nsensitivity to the length and initialization of the soft prompts. Prompts \ncould also be injected in the pre-training stage to seek a satisfying ini-\ntialization point42. Moreover, similar to other methods, prompt-tuning \nalso demonstrates transferability across tasks 24,26, which suggests \nthat appropriate initialization could be substantially beneficial for \ndownstream tasks.\nThe training curse of prompt-based methods.  Although \nprompt-based methods exhibit a promising future for the adaptation \nof large pre-trained models, especially as prompt-tuning does not need \nto modify anything inside the neural network, there still exist unsolved \nchallenges. In practice, prompt-tuning is difficult to optimize, and \ngenerally, this phenomenon becomes more apparent as the volume \nNature Machine Intelligence | Volume 5 | March 2023 | 220–235 232\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nof data and the size of the model decreases. Even though soft prompts \ncan be trained successfully, they converge slower than full parameter \nfine-tuning and other delta-tuning methods during training. In our \nexperiments, we validate the phenomenon across different datasets \n(‘Performance, convergence and efficiency’ section), indicating that \nit is an interesting topic to train soft prompts to converge stably in \nvarious situations.\nSpecification-based methods\nSpecification-based methods fine-tune a few inherent parameters \nwhile leaving the majority of parameters unchanged in model adapta-\ntion. This approach does not seek to change the internal structure of a \nmodel but to optimize a small number of internal parameters to solve \nparticular tasks. In general, such specifications could be implemented \nbased on heuristics or training supervision.\nHeuristic specification. Specification-based methods do not intro -\nduce any new parameters to the model, but directly specify part of \nthe parameters to be optimized. The idea is simple but surprisingly \neffective; an early study43 only fine-tunes one-fourth of the final lay -\ners of BERT and RoBERTa and could produce 90% of the performance \nof full parameter fine-tuning. BitFit14 empirically proves that by only \noptimizing the bias terms inside the model and freezing other param-\neters, the model could still reproduce over 95% performance on several \nbenchmarks. Empirical results in BitFit also show that even if we use a \nsmall random set of parameters for delta-tuning (which obviously will \ndegrade the performance), the model could still yield passable results \non the GLUE benchmark. Unfortunately, the work only applies this trick \nto small-scale models, and there is no guarantee that randomly choos-\ning some parameters to be tuned would remain competitive for larger \nmodels. Another valuable observation is that different bias terms may \nhave different functionalities during model adaptation.\nLearn the specification. Rather than manually or heuristically specify \nwhich parameters to update, one alternative is to ‘learn’ such specifi-\ncations. Following the definition in this section, diff pruning 44 repa-\nrameterizes the fine-tuned model parameters Θ ′ as the summation \nof the pre-trained parameters Θ and the difference vector ΔΘ, that is, \nΘ′ = Θ + ΔΘ, where |Θ| = |Θ′|. Hence, the key issue is to encourage the \ndifference vector to be as sparse as possible; this work regularizes the \nvector by a differentiable approximation to the L 0-norm penalty to \nachieve the goal of sparsity. Practically, because new parameters to be \noptimized are introduced in the learning phase, diff pruning takes up \nmore GPU memory than full parameter fine-tuning, which may estab-\nlish barriers in the application on large PLMs. The masking method45 \nlearns selective masks for PLMs to only update the critical weights for \nparticular tasks. T o learn such a set of masks, a binary matrix associated \nwith the model weights is introduced, where each value is generated \nby a thresholding function. During backpropagation, the matrix is \nupdated by a noisy estimator.\nReparameterization-based methods\nReparameterization-based methods transform the adaptive param -\neters during optimization into parameter-efficient forms. This branch \nof delta-tuning is typically motivated by the hypothesis that PLM adap-\ntations towards most downstream tasks are inherently low rank, and \ncould thus be equivalently completed in a parameter-efficient way.\nIntrinsic dimensions of PLM adaptation.  Previous work 16 has \nempirically shown that the full parameter fine-tuning process of \npre-trained models can be reparameterized into optimization within \na low-dimensional subspace, that is, fine-tuning has a low intrinsic \ndimension46, which measures the minimum number of parameters \nneeded to reach satisfactory performance. In experiments, they \nfind that a relatively low-dimensional (for example, thousands) \nreparameterization could achieve over 85% fine-tuning performance. In \nthis sense, PLMs may serve as general compression frameworks, which \ncompress the optimization complexity from high dimensions to low \ndimensions. They also demonstrate that larger PLMs generally have \nsmaller intrinsic dimensions, and the process of pre-training implicitly \nreduces the PLM’s intrinsic dimension. Taking inspiration from these \nobservations, reparameterization-based delta-tuning methods are \nproposed, which reparameterize (a part of) original model parameters \nwith low-dimensional proxy parameters and only optimize the proxy \nparameters and thus reduce the computation and memory cost.\nIntrinsic rank of weight differences.  LoRA15 hypothesizes that the \nchange of weights during model tuning has a low intrinsic rank. On the \nbasis of this hypothesis, it is proposed to optimize the low-rank decom-\nposition for the change of original weight matrices in the self-attention \nmodules. In deployment, the optimized low-rank decomposition matri-\nces are multiplied to obtain the delta of self-attention weight matrices. \nIn this way, LoRA could match the fine-tuning performance on the GLUE \nbenchmark. They demonstrate the effectiveness of their methods on \nPLMs of various scales and architectures.\nIntrinsic space of multiple adaptations.  Furthermore, intrinsic \nprompt-tuning17 makes a stronger hypothesis that the adaptations \nto multiple tasks could be reparameterized into optimizations within \nthe same low-dimensional intrinsic subspace. Instead of resorting to a \nrandom subspace16, they try to find a common subspace shared by vari-\nous NLP tasks, which is implemented through decomposing the trained \nsoft prompts of multiple NLP tasks into the same low-dimensional \nnonlinear subspace, and then learn to adapt the PLM to unseen tasks \nor data by only tuning parameters in the subspace. Experiments \nshow that in a 250-dimensional subspace found with 100 random \ntasks, by only tuning 250 free parameters, 97% and 83% of the full \nprompt-tuning performance can be recovered for 100 seen tasks (using \ndifferent training data) and 20 unseen tasks, respectively. This provides \nstrong evidence for their universal reparameterization hypothesis \nand may inspire future work. Moreover, this work also shows that the \nlow-dimensional reparameterization can substantially improve the sta-\nbility of prompt-tuning. Their method could also be leveraged as a tool \nfor analysing the similarity and differences between various NLP tasks.\nTheoretical perspectives of delta-tuning\nAre these methods essentially doing the same thing? We are interested \nin the theoretical principles behind delta-tuning. A PLM can usually be \neffectively adapted to various downstream tasks with a smaller cost \ncompared with pre-training, which leads to theoretical issues that \nare worth exploring in depth. We adopt two frameworks to introduce \ntheoretical insights into delta-tuning from the perspectives of optimi-\nzation and optimal control.\nOptimization perspective.  As training neural networks is an opti -\nmization process, the mechanism of delta-tuning can be analysed \nfrom the perspective of optimization. In general, it is challenging and \ntime-consuming to solve large-scale and high-dimensional optimiza-\ntion problems. However, in the fine-tuning of a large PLM, empirical \nstudy16 reveals that there exists a low intrinsic dimension; thus, some \ncustomized optimization schemes can benefit from this property and \nbe quite efficient in practice. One promising scheme is the subspace \noptimization47 that seeks an acceptable solution in a low-dimensional \nsubspace. It manipulates a small number of variables and is more eco-\nnomical than the optimization in the whole space. In fact, delta-tuning \ncan be viewed as a subspace-optimization method.\nThere are two approaches to applying subspace optimization \nand thus the delta-tuning can roughly fall into two categories. One \nis tuning model parameters in the solution subspace. It exploits a \nlow-dimensional manifold that can approximately represent the \nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 233\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nwhole model parameters, and the optimization trajectory follows \nthis manifold. Some delta-tuning methods can be categorized into \nthis approach, for example, LoRA 15, BitFit14 and diff pruning 44. The \nother approach seeks a surrogate of the original objective function in \na small functional subspace and uses the minimizer of the surrogate \nfunction as the approximate final solution. It can provide some expla-\nnations of the rationales of some popular delta-tuning methods such \nas prompt-tuning19 and prefix-tuning40. A complete discussion can be \nfound in Supplementary Section 2.1.\nOptimal control perspective.  We draw inspiration from optimal \ncontrol theories to better understand the functionality of delta-tuning. \nIn addition to their parameter efficiency, the essence of delta-tuning \nlies in regularizing the layer-wise hidden-state transformation process \nalong forwards propagation. The forward propagation of hidden states \nh between layer j and j + 1 in the PLM, with the guidance of the delta \nparameters δ(j) at the jth layer, can be written as 𝒢𝒢(j)\nθ (h(j),δ(j)) . With the \nparameters θ in the PLM fixed, the transformation function 𝒢𝒢(j)\nθ  defines \nthe altered forwards propagation at the jth layer with the learnable δ(j). \nThe detailed formulations and instantiations of 𝒢𝒢(j)\nθ  for different \ndelta-tuning methods, including Prefix-tuning, Adapter, LoRA and \nBitFit, are listed in Supplementary Section 2.2. In this way, the tuned \ndelta parameters are interpreted as the optimal controllers that steer \nthe PLMs to work in different realistic settings.\nThe optimal control perspective instructs the novel design of \ndelta-tuning. For example, robust prefix-tuning 48 tunes additional \nlayer-wise prefix parameters during inference. The layer-wise propaga-\ntion of hidden states is thus guided towards correct outputs. Another \nwork49 leveraged inference-time bias-term tuning to mitigate bias and \ntoxicity in natural language generation. The number of bias terms to be \ntuned is determined by the extent of modification of the hidden-state \ntransformation in an adaptive manner. Finally, by applying the theo-\nries of controller design 50,51, we expect more delta-tuning methods \nproposed with theoretical guarantees and better exploitation of the \npower of PLMs52.\nData availability\nDatasets used in this study are freely available at https://github.com/\nINK-USC/CrossFit and https://huggingface.co/datasets/glue.\nCode availability\nThe source code of this study is publicly available on GitHub at https://\ngithub.com/thunlp/OpenDelta. It is also available at https://zenodo.\norg/record/7340282.\nReferences\n1. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, \n436–444 (2015).\n2. Hochreiter, S. & Schmidhuber, J. ürgen. Long short-term memory. \nNeural Comput. 9, 1735–1780 (1997).\n3. Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic \nlanguage model. In Advances in Neural Information Processing \nSystems. 13 (2000).\n4. Vaswani, A. et al. Attention is all you need. In Advances in Neural \nInformation Processing Systems. 30 (2017).\n5. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training \nof deep bidirectional transformers for language understanding. \nIn Proc. the 2019 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies. 1, 4171–4186 (2019).\n6. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving \nlanguage understanding by generative pre-training. OpenAI Blog. \nhttps://cdn.openai.com/research-covers/language-unsupervised/\nlanguage_understanding_paper.pdf (2018).\n7. Radford, A. et al. Language models are unsupervised multitask \nlearners. OpenAI Blog. https://d4mucfpksywv.cloudfront.net/\nbetter-language-models/language-models.pdf (2019).\n8. Raffel, C. et al. Exploring the limits of transfer learning with  \na unified text-to-text transformer. J. Mach. Learn. Res. 21,  \n5485–5551 (2020).\n9. Brown, T. et al. Language models are few-shot learners. In \nAdvances in Neural Information Processing Systems. 33,  \n1877–1901 (2020).\n10. Rae, J. W. et al. Scaling language models: methods, analysis & \ninsights from training Gopher. Preprint at arXiv https://arxiv.org/\nabs/2112.11446 (2021).\n11. Smith, S. et al. Using deepspeed and megatron to train \nMegatron-Turing NLG 530b, a large-scale generative language \nmodel. Preprint at arXiv https://arxiv.org/abs/2201.11990 (2022).\n12. Chowdhery, A. et al. PaLM: scaling language modeling with \npathways. Preprint at arXiv https://arxiv.org/abs/2204.02311 (2022).\n13. Houlsby, N. et al. Parameter-efficient transfer learning for NLP. In \nInternational Conference on Machine Learning. (eds Chaudhuri, K. \n& Salakhutdinov, R.) 2790–2799 (2019).\n14. Zaken, E. B., Ravfogel, S. & Goldberg, Y. Bitfit: simple \nparameter-efficient fine-tuning for transformer-based masked \nlanguage-models. In Proc. the 60th Annual Meeting of the \nAssociation for Computational Linguistics. 2, 1–9 (2022).\n15. Hu, E. J. et al. LoRA: low-rank adaptation of large language models. \nIn International Conference on Learning Representations (2022).\n16. AAghajanyan, A., Gupt, S. & Zettlemoyer, L. Intrinsic \ndimensionality explains the effectiveness of language model \nfine-tuning. In Proc. the 59th Annual Meeting of the Association \nfor Computational Linguistics and the 11th International Joint \nConference on Natural Language Processing. 1, 7319–7328 (2021).\n17. Qin, Y. et al. Exploring low-dimensional intrinsic task subspace via \nprompt tuning. Preprint at arXiv https://arxiv.org/abs/2110.07867 \n(2021).\n18. Lhoest, Q. et al. Datasets: a community library for natural \nlanguage processing. In Proc. the 2021 Conference on Empirical \nMethods in Natural Language Processing: System Demonstrations. \n175–184 (2021).\n19. Lester, B., Al-Rfou, R. & Constant, N. The power of scale for \nparameter-efficient prompt tuning. In Proc. the 2021 Conference \non Empirical Methods in Natural Language Processing. 3045–3059 \n(2021).\n20. Liu, Y. et al. Roberta: a robustly optimized BERT pretraining \napproach. Preprint at arXiv https://arxiv.org/abs/1907.11692 (2019).\n21. Wang, A. et al. GLUE: a multi-task benchmark and analysis \nplatform for natural language understanding. In International \nConference on Learning Representations (2019).\n22. Schick, T. & Schütze, H. Exploiting cloze-questions for few-shot \ntext classification and natural language inference. In Proc. the \n16th Conference of the European Chapter of the Association for \nComputational Linguistics. 255–269 (2021).\n23. Socher, R. et al. Recursive deep models for semantic \ncompositionality over a sentiment treebank. In Proc. the 2013 \nConference on Empirical Methods in Natural Language Processing. \n1631–1642 (2013).\n24. Su, Y. et al. On transferability of prompt tuning for natural \nlanguage understanding. In Proc. the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies. 3949–3969 (2022).\n25. Williams, A., Nangia, N. & Bowman, S. A broad-coverage \nchallenge corpus for sentence understanding through inference. \nIn Proc. the 2018 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies. 1, 1112–1122 (2018).\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235 234\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\n26. Vu, T., Lester, B., Constant, N., Al-Rfou, R. & Cer, D. Spot: better \nfrozen model adaptation through soft prompt transfer. In Proc. \nthe 60th Annual Meeting of the Association for Computational \nLinguistics. 1, 5039–5059 (2022).\n27. Han, X. et al. Pre-trained models: Past, present and future. AI \nOpen 2, 225-250. https://www.sciencedirect.com/science/article/\npii/S2666651021000231 (2021).\n28. Mahabadi, R. K., Henderson, J. & Ruder, S. Compacter: efficient \nlow-rank hypercomplex adapter layers. In Advances in Neural \nInformation Processing Systems. 34, 1022–1035 (2021).\n29. Stickland, A. C. & Murray, I. BERT and pals: projected attention \nlayers for efficient adaptation in multi-task learning. In \nInternational Conference on Machine Learning. 5986–5995 (2019).\n30. Mahabadi, R. K., Ruder, S., Dehghani, M. & Henderson, J. \nParameter-efficient multi-task fine-tuning for transformers via \nshared hypernetworks. In Proc. the 59th Annual Meeting of  \nthe Association for Computational Linguistics and the 11th \nInternational Joint Conference on Natural Language Processing.  \n1, 565–576 (2021).\n31. Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K. & Gurevych, I. \nAdapterFusion: non-destructive task composition for transfer \nlearning. In Proc. the 16th Conference of the European Chapter of \nthe Association for Computational Linguistics. 487–503 (2021).\n32. Rücklé, A. et al. AdapterDrop: in the efficiency of adapters in \ntransformers. In Proc. the 2021 Conference on Empirical Methods \nin Natural Language Processing. 7930–7946 (2021).\n33. He, R. et al. On the effectiveness of adapter-based tuning for \npretrained language model adaptation. In Proc. the 59th Annual \nMeeting of the Association for Computational Linguistics and \nthe 11th International Joint Conference on Natural Language \nProcessing. 1, 2208–2222 (2021).\n34. Han, W., Pang, B. & Wu, Y. N. Robust transfer learning with \npretrained language models through adapters. In Proc. the 59th \nAnnual Meeting of the Association for Computational Linguistics \nand the 11th International Joint Conference on Natural Language \nProcessing. 2, 854–861 (2021).\n35. Pfeiffer, J. et al. AdapterHub: a framework for adapting \ntransformers. In Proc. the 2020 Conference on Empirical  \nMethods in Natural Language Processing: System Demonstrations. \n46–54 (2020).\n36. Gao, T., Fisch, A. & Chen, D. Making pre-trained language  \nmodels better few-shot learners. In Proc. the 59th Annual Meeting \nof the Association for Computational Linguistics and the 11th \nInternational Joint Conference on Natural Language Processing. 1, \n3816–3830 (2021).\n37. Hu, S. et al. Knowledgeable prompt-tuning: incorporating \nknowledge into prompt verbalizer for text classification. In  \nProc. the 60th Annual Meeting of the Association for \nComputational Linguistics. 1, 2225–2240 (2021).\n38. Liu, P. et al. Pre-train, prompt, and predict: a systematic survey \nof prompting methods in natural language processing. ACM \nComput. Surv. 55, 1–35 (2023).\n39. Ding, N. et al. Openprompt: an open-source framework for \nprompt-learning. In Proc. the 60th Annual Meeting of the \nAssociation for Computational Linguistics: System Demonstrations. \n105–113 (2022).\n40. Li, X. L. & Liang, P. Prefix-tuning: optimizing continuous  \nprompts for generation. In Proc. the 59th Annual Meeting  \nof the Association for Computational Linguistics and the 11th \nInternational Joint Conference on Natural Language Processing. 1, \n4582–4597 (2021).\n41. Liu, X. et al. P-tuning: prompt tuning can be comparable to \nfine-tuning universally across scales and tasks. In Proc. the 60th \nAnnual Meeting of the Association for Computational Linguistics. 2, \n61–68 (2022).\n42. Gu, Y., Han, X., Liu, S. & Huang, M. Ppt: pre-trained prompt  \ntuning for few-shot learning. In Proc. the 60th Annual  \nMeeting of the Association for Computational Linguistics. 1, \n8410–8423 (2022).\n43. Lee, J., Tang, R. & Lin, J. What would elsa do? Freezing layers \nduring transformer fine-tuning. Preprint at arXiv https://arxiv.org/\nabs/1911.03090 (2019).\n44. Guo, D., Rush, A. & Kim, Y. Parameter-efficient transfer learning \nwith diff pruning. In Proc. the 59th Annual Meeting of the \nAssociation for Computational Linguistics and the 11th  \nInternational Joint Conference on Natural Language Processing. 1, \n4884–4896 (2021).\n45. Zhao, M., Lin, T., Mi, F., Jaggi, M. & Schütze, H. Masking as an \nefficient alternative to finetuning for pretrained language models. \nIn Proc. the 2020 Conference on Empirical Methods in Natural \nLanguage Processing. 2226–2241 (2020).\n46. Li, C., Farkhoor, H., Liu, R. & Yosinski, J. Measuring the intrinsic \ndimension of objective landscapes. In International Conference on \nLearning Representations (2018).\n47. Liu, X., Wen, Z. & Yuan, Y.-X. Subspace methods for nonlinear \noptimization. CSIAM Trans. Appl. Math. 2, 585–651 (2021).\n48. Yang, Z. & Liu, Y. On robust prefix-tuning for text classification.  \nIn International Conference on Learning Representations  \n(2022).\n49. Yang, Z., Yi, X., Li, P., Liu, Y. & Xie, X. Unified detoxifying and \ndebiasing in language generation via inference-time adaptive \noptimization. Preprint at arXiv https://arxiv.org/abs/2210.04492 \n(2022).\n50. Boyd, S. P. & Barratt, C. H. Linear Controller Design: Limits of \nPerformance Vol. 7 (Citeseer, 1991).\n51. Ang, K. H., Chong, G. & Li, Y. PID control system analysis,  \ndesign, and technology. IEEE Trans. Control Syst. Technol. 13, \n559–576 (2005).\n52. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. & Neubig, G. Towards \na unified view of parameter-efficient transfer learning. In \nInternational Conference on Learning Representations (2022).\nAcknowledgements\nThis work is supported by the National Key Research and \nDevelopment Program of China (No. 2020AAA0106500), National \nNatural Science Foundation of China (No. 62276154 and No. \n62011540405), Beijing Academy of Artificial Intelligence (BAAI)  \nand the Institute for Guo Qiang at Tsinghua University. We thank  \nJ. He, P. Liu, T. Sun, C., L. Wang, C. Fang, X. Han and R. Shao for their \nsuggestions and help with the paper.\nAuthor contributions\nN.D., Y.Q. and Z.L. initiated and organized the research. N.D. drafted \nthe abstract, the main text and Section Methods. S.H., X.W., W.Z. and \nY.Q. added contents to Section Methods. F.W., Z.Y., N.D., Y.Q., S.H. and \nJ.C. discussed the scope and content of the theoretical discussion. \nF.W. developed the optimization framework, and Z.Y. and Y.L. proposed \nthe optimal control framework. N.D. verified the formula derivation. \nY.Q. led the empirical study part. Y.Q., G.Y., Y.C., Y.S., W.C., J.Y., \nC.-M.C. and N.D. drafted Section Results. Y.Q., G.Y., W.C., J.Y. and S.H. \nconducted the experiments for overall performance and combination \nin Section Results. Y.S. and C.-M.C. conducted and wrote experiments \nfor transferability and power of scale in Section Results. S.H. and \nY.Q. drafted the application part. Z.L., H.-T.Z, Y.L., J.T., J.L. and M.S. \nadvised the project, suggested the theoretical and empirical study \nand participated in the discussion. N.D. and Y.Q. participated in all the \nsections and proofread the whole paper.\nCompeting interests\nThe authors declare no competing interests.\nNature Machine Intelligence | Volume 5 | March 2023 | 220–235\n 235\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-023-00626-4.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-023-00626-4.\nCorrespondence and requests for materials should be addressed to \nZhiyuan Liu, Hai-Tao Zheng or Maosong Sun.\nPeer review information Nature Machine Intelligence  \nthanks Dieuwke Hupkes and the other, anonymous,  \nreviewer(s) for their contribution to the peer review of  \nthis work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons license, and \nindicate if changes were made. The images or other third party \nmaterial in this article are included in the article’s Creative Commons \nlicense, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons license \nand your intended use is not permitted by statutory regulation \nor exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this license, visit \nhttp://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Fig. 1 | The performance of T5BASE with different delta-tuning methods at different training steps. The performance of T5BASE with different delta-\ntuning methods (LR, AP, PF) and fine-tuning (FT) at different training steps.\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Fig. 2 | The performance of T5BASE with different delta-tuning methods at different training time. The performance of T5BASE.with different delta-\ntuning methods (LR, AP, PF) and fine-tuning (FT) at different training time (seconds).\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Fig. 3 | Time consumption for fine-tuning (FT) and different delta-tuning methods. Time consumption for fine-tuning (FT) and different delta-\ntuning methods, including BitFit (BF), adapter (AP) and prompt-tuning (PT). We report the results with different input length.\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Fig. 4 | The performance of RoBERTaLARGE when sequentially applying different delta-tuning methods. The performance of RoBERTaLARGE when \ndifferent delta-tuning methods (adapter (AP), BitFit (BF) and prompt-tuning (PT)) are applied sequentially. The experiments are conducted on SST-2.\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Fig. 5 | Zero-shot transferring performance of four delta-\ntuning methods using T5BASE. Zero-shot transferring performance of four \ndelta-tuning methods using T5BASE. We report relative performance (zero-shot \ntransferring performance / original performance) (%) on the target tasks \n(columns) when delta parameters are transferred from the source tasks (rows). \nColours of the task names indicate the task types. Blue: sentiment analysis. \nGreen: natural language inference. Orange: paraphrase identification. Brown: \nquestion answering. Purple: summarization.\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Table 1 | Statistics of the usage of different sizes of pre-trained models\nThe usage of models of different sizes in research published in NLP conferences, the statistic is based on 1000 randomly selected papers. Large PLMs are defined as PLMs with over 1 billion \nparameters.\n\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Table 2 | Performance for T5BASE on GLUE datasets\nPerformance of T5BASE on GLUE datasets. We report the average result of multiple random seeds on the validation set. ✓ denotes the component is included in the combination and ✗ denotes \nit is excluded in the combination.\n\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Table 3 | Generalization gap for RoBERTaLARGE on GLUE datasets\nThe experiments of generalization gap for RoBERTaLARGE on GLUE datasets. We report the average result (train performance - dev performance) of multiple random seeds. ✓ denotes the \ncomponent is included in the combination and ✗ denotes it is excluded in the combination.\n\nNature Machine Intelligence\nAnalysis https://doi.org/10.1038/s42256-023-00626-4\nExtended Data Table 4 | Comparison between different delta-tuning methods\nComparison between different delta-tuning methods. we use underline to denote tunable parameters and modules. [:] is the concatenation operation; dh means the hidden dimension of the \nTransformer model; dm is the intermediate dimension between down-projection and up-projection, where dm is far smaller than dh. Compacter utilize hypercomplex matrix multiplication and \nlow-rank decomposition to reduce the amount of parameters; AdapterDrop randomly dropout adapters in the first n layers and also bring down backpropagation time; Prefix-Tuning adds \nprefixes of n past key-values.\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7597874999046326
    },
    {
      "name": "Fine-tuning",
      "score": 0.5871682167053223
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5839609503746033
    },
    {
      "name": "Categorization",
      "score": 0.5403663516044617
    },
    {
      "name": "Computation",
      "score": 0.5105298757553101
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5012094974517822
    },
    {
      "name": "Language model",
      "score": 0.47768479585647583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41421768069267273
    },
    {
      "name": "Machine learning",
      "score": 0.4104207158088684
    },
    {
      "name": "Algorithm",
      "score": 0.20066779851913452
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210114105",
      "name": "Tsinghua–Berkeley Shenzhen Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ],
  "cited_by": 624
}