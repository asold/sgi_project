{
  "title": "Knowledge Aware Emotion Recognition in Textual Conversations via Multi-Task Incremental Transformer",
  "url": "https://openalex.org/W3116679303",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2320125067",
      "name": "Duzhen Zhang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2241378952",
      "name": "Xiuyi Chen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2096822672",
      "name": "Shuang Xu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2074677540",
      "name": "Bo Xu",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951508633",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2749002090",
    "https://openalex.org/W2962830617",
    "https://openalex.org/W2761590056",
    "https://openalex.org/W2963695965",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2739716023",
    "https://openalex.org/W2963873807",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2804780446",
    "https://openalex.org/W2963544536",
    "https://openalex.org/W2806966100",
    "https://openalex.org/W2963686995",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2963789888",
    "https://openalex.org/W2962796276",
    "https://openalex.org/W2891359673",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2154359981",
    "https://openalex.org/W2805005636",
    "https://openalex.org/W2740550900",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2909285558",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2905807898",
    "https://openalex.org/W4285723986",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2963520511",
    "https://openalex.org/W2963829073",
    "https://openalex.org/W2964300796",
    "https://openalex.org/W2950009015",
    "https://openalex.org/W2970431814",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2805662932"
  ],
  "abstract": "Emotion recognition in textual conversations (ERTC) plays an important role in a wide range of applications, such as opinion mining, recommender systems, and so on. ERTC, however, is a challenging task. For one thing, speakers often rely on the context and commonsense knowledge to express emotions; for another, most utterances contain neutral emotion in conversations, as a result, the confusion between a few non-neutral utterances and much more neutral ones restrains the emotion recognition performance. In this paper, we propose a novel Knowledge Aware Incremental Transformer with Multi-task Learning (KAITML) to address these challenges. Firstly, we devise a dual-level graph attention mechanism to leverage commonsense knowledge, which augments the semantic information of the utterance. Then we apply the Incremental Transformer to encode multi-turn contextual utterances. Moreover, we are the first to introduce multi-task learning to alleviate the aforementioned confusion and thus further improve the emotion recognition performance. Extensive experimental results show that our KAITML model outperforms the state-of-the-art models across five benchmark datasets.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 4429–4440\nBarcelona, Spain (Online), December 8-13, 2020\n4429\nKnowledge Aware Emotion Recognition in Textual Conversations\nvia Multi-Task Incremental Transformer\nDuzhen Zhang1,2, Xiuyi Chen1,2*, Shuang Xu1 and Bo Xu1,2\n1Institute of Automation, Chinese Academy of Sciences (CASIA). Beijing, China\n2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences\n{zhangduzhen2019,chenxiuyi2017,shuang.xu,xubo}@ia.ac.cn\nAbstract\nEmotion recognition in textual conversations (ERTC) plays an important role in a wide range of\napplications, such as opinion mining, recommender systems, and so on. ERTC, however, is a\nchallenging task. For one thing, speakers often rely on the context and commonsense knowledge\nto express emotions; for another, most utterances contain neutral emotion in conversations, as a\nresult, the confusion between a few non-neutral utterances and much more neutral ones restrains\nthe emotion recognition performance. In this paper, we propose a novel Knowledge Aware In-\ncremental Transformer with Multi-task Learning (KAITML) to address these challenges. Firstly,\nwe devise a dual-level graph attention mechanism to leverage commonsense knowledge, which\naugments the semantic information of the utterance. Then we apply the Incremental Transformer\nto encode multi-turn contextual utterances. Moreover, we are the ﬁrst to introduce multi-task\nlearning to alleviate the aforementioned confusion and thus further improve the emotion recog-\nnition performance. Extensive experimental results show that our KAITML model outperforms\nthe state-of-the-art models across ﬁve benchmark datasets.\n1 Introduction\nEmotion recognition in textual conversations (ERTC), which aims to identify the emotion of each utter-\nance from the transcript of a conversation, has become a popular research topic in recent years. ERTC can\nbe widely used in various scenarios, such as opinion mining of comments in social media (Chatterjee et\nal., 2019), emotion analysis of customers in artiﬁcial customer service, and others. In addition, it can also\nbe applied to chat robots to analyze the user’s emotional state in real time and generate emotion-aware\nresponses (Poria et al., 2019b; Zhou et al., 2018a; Huang et al., 2018).\nTruth Prediction\nOthers Angry Sad Happy\nOthers 4424 101 60 92\nAngry 54 237 6 1\nSad 44 11 192 3\nHappy 88 0 2 194\nTable 1: A confusion matrix of the emotion recognition results on the EmoContext test dataset (Chatterjee\net al., 2019) from Knowledge-Enriched Transformer (Zhong et al., 2019b), which is the current state-\nof-the-art model. We notice that there are barely miss-classiﬁcations among the non-neutral categories\n(Angry, Sad, and Happy). Most of the errors, shown in the bold font, correspond to the confusion between\na few non-neutral categories and much more neutral category (Others).\nHowever, there are several challenges when analyzing emotion in natural conversations. Firstly, unlike\nvanilla emotion recognition of sentences (Wang and Manning, 2012; Seyeditabari et al., 2018), ERTC\nrequires comprehensively considering the context in the conversation. Secondly, knowledge plays an\n*The ﬁrst two authors contributed equally.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/\n4430\nimportant role in ERTC as speakers often express emotions relying on the context and commonsense\nknowledge (Zhong et al., 2019b). Moreover, most utterances contain neutral emotion in conversations,\nand the heavily imbalanced class distribution can easily lead to the confusion between a few non-neutral\nutterances (e.g., happy, sad, and angry, etc.) and much more neutral ones (e.g., neutral or others), which\nrestrains the emotion recognition performance. Table 1 shows a confusion matrix of emotion recognition\nresults from the current state-of-the-art model and there appears a serious confusion between a few non-\nneutral categories and much more neutral category.\nSome prior studies have been conducted to model contextual information for emotion recognition in\nconversations (Poria et al., 2017; Majumder et al., 2019). These methods ﬁrst adopt convolutional neural\nnetworks (CNN) to extract utterance-level features and then use context-level recurrent neural networks\n(RNN) to model the contextual utterances in conversation. However, RNN and CNN have difﬁculty\nmodeling long-distance dependencies (Vaswani et al., 2017), which may be useful in ERTC. Zhong et\nal. (2019b) uses a context-aware affective graph attention mechanism to incorporate external knowledge\nfor ERTC. However, they don’t consider various relations in external knowledge base, which may cause\nthe loss of semantic information. In addition, to the best of our knowledge, no existing work considers\nthe confusion between a few non-neutral utterances and much more neutral ones.\nIn this paper, We propose a novel Knowledge Aware Incremental Transformer with Multi-task Learn-\ning (KAITML) to address the aforementioned challenges. Firstly, we enhance the background and se-\nmantic information of the given utterance to facilitate ERTC with the retrieved relevant knowledge graphs\nfrom a large-scale commonsense knowledge base. Speciﬁcally, we propose a dual level graph attention\nmechanism to encode these relevant knowledge graphs, which consists of a node-level attention to learn\nthe importance of different neighboring nodes and a relation-level attention to learn the importance of\ndifferent relations to the current node. Then we apply the Incremental Transformer (Li et al., 2019) to\nincrementally encode multi-turn contextual utterances, which could capture the intra-utterance and inter-\nutterance correlations by the self-attention (Cheng et al., 2016) and context-attention (Zhang et al., 2018)\nmodules, respectively. Moreover, we introduce multi-task learning to alleviate the confusion between\na few non-neutral utterances and much more neutral ones, as shown in Table 1. Speciﬁcally, we ﬁrst\nfocus on the binary classiﬁcation, “non-neutral” versus “neutral”, and then classiﬁes the “non-neutral”\nones into ﬁne-grained emotion categories. These two auxiliary tasks are jointly trained with the original\nemotion recognition task.\nIn summary, this paper makes the following contributions:\n•We devise a dual-level graph attention mechanism to support better understanding of utterances\nfor ERTC by considering various relations in external knowledge base. Furthermore, we apply the\nIncremental Transformer to model multi-turn contextual utterances and recognize emotions.\n•We are the ﬁrst to introduce multi-task learning with two auxiliary tasks to alleviate the aforemen-\ntioned confusion and thus further improve emotion recognition performance.\n•Experimental results show that our proposed KAITML model outperforms the state-of-the-art mod-\nels across ﬁve benchmark datasets in F1 score. In addition, context, commonsense knowledge and\nmulti-task learning are all beneﬁcial to the emotion recognition performance.\n2 Related Work\nEmotion recognition in conversations has grabbed much attention from researchers in the past few years\ndue to the proliferation of publicly available conversational dataset (Poria et al., 2019a; Chatterjee et\nal., 2019; Li et al., 2017; Zhou et al., 2018a) and its widespread applications in opinion mining, recom-\nmender systems, emotion-aware dialogues generation, and so on (Poria et al., 2019b). Some of the deep\nlearning-based models have been proposed for emotion recognition in conversations, in only textual and\nmultimodal settings (containing textual, acoustic, and visual information).\nPoria et al. (2017) proposes a long short-term memory network (LSTM) (Hochreiter and Schmidhu-\nber, 1997) based model to capture contextual correlations from the utterances of a user-generated video\n4431\nfor multimodal sentiment classiﬁcation. Hazarika et al. (2018b) proposes conversational memory net-\nwork (CMN) that exploits distinct memory units for each speaker to model emotional dynamics and\ndetect emotion in a dyadic conversation. Later, Hazarika et al. (2018a) improves upon this approach with\ninteractive conversational memory network (ICON), which utilizes the interactive memory unit to hierar-\nchically model the self- and inter-speaker emotional inﬂuences for emotion recognition in conversational\nvideos. Majumder et al. (2019) proposes the DialogueRNN model that exploits three gated recurrent\nunits (GRU) (Cho et al., 2014) to capture speaker information, context and emotional information of the\npreceding utterances, respectively. They achieve the state-of-the-art performance on several multimodal\nconversation datasets. Compared to these gated RNNs and CNNs based models, we apply the Incre-\nmental Transformer (Li et al., 2019) to incrementally encode multi-turn contextual utterances, where the\nshorter path of information ﬂow in the self-attention (Cheng et al., 2016) and context-attention (Zhang\net al., 2018) modules in the Incremental Transformer allows our model to exploit contextual information\nmore efﬁciently.\nRecently, a considerable literature has grown up around the theme of incorporating external knowledge\nin generative conversation systems, including question answering systems (Hao et al., 2017; Mihaylov\nand Frank, 2018), open-domain dialogue systems (Young et al., 2018; Zhou et al., 2018b; Zhong et\nal., 2019a), and task-oriented dialogue systems (He et al., 2019; Madotto et al., 2018; Chen et al.,\n2019). Zhong et al. (2019b) proposes a Knowledge-Enriched Transformer (KET) achieving the state-of-\nthe-art performance on multiple textual conversation datasets, where contextual utterances are encoded\nusing hierarchical self-attention and commonsense knowledge is incorporated using a context-aware\naffective graph attention mechanism. However, they ignore various relations in external knowledge\nbase, which may cause the loss of semantic information. By contrast, our dual-level graph attention\nmechanism, can take advantage of the various relations in external knowledge base to better augment the\nsemantic information of the utterances.\n3 Our Proposed KAITML Model\n3.1 Task Deﬁnition and Overview\nLet ⟨X(i)\nj ,Y (i)\nj ⟩,i = 1,...N,j = 1,...Ni be a collection of ⟨utterance,label⟩pairs in a given conversa-\ntion dataset, where N denotes the number of conversations and Ni denotes the number of utterances in\nthe ith conversation. The objective of the task is to maximize the following function:\narg max\nθ\nN∏\ni=1\nNi∏\nj=1\nP(Y(i)\nj |X(i)\nj ,X(i)\nj−1,...,X (i)\nj−M; θ). (1)\nwhere X(i)\nj denotes target utterance, Y(i)\nj denotes the emotion label of target utterance, θ denotes the\nmodel parameters we need to optimize and X(i)\nj−1,...,X (i)\nj−M denote contextual utterances. Here, we\nlimit the number of contextual utterances to M. We follow (Su et al., 2018; Zhong et al., 2019b) to\ndirectly discard early contextual utterances. Similar to (Zhong et al., 2019b; Poria et al., 2017), we clip\nand pad each utterance X(i)\nj to a ﬁxed K number of tokens. The overview of our KAITML model and\ndetailed architecture of model components are presented in Figure 1.\n3.2 Knowledge Interpreter\nCommonsense knowledge is fundamental to understanding conversations (Zhou et al., 2018b). We use\nConceptNet (Speer et al., 2017) as a external commonsense knowledge base in our model. ConceptNet is\na large-scale multilingual semantic graph where concepts are nodes in the graph and relations are edges,\nwhich describes general human knowledge in natural language. Each ⟨concept1,relation,concept 2⟩\ntriple is termed an assertion. At present, ConceptNet comprises 5.9M assertions, 3.1M concepts and 38\nrelations for English.\nThe knowledge interpreter is designed to facilitate the understanding of an utterance. It takes as input\nan utterance X(i)\nn = x1x2...xK,n = j−M,...,j and retrieves a few relevant knowledge graphs G(i)\nn =\n4432\nX(i)\nj−M X(i)\nj−1 X(i)\nj\nKnowledge \nInterpreter\nKnowledge \nInterpreter\nKnowledge \nInterpreter\n…\nKnowledge \nGraphs\n…\nDual-level \nGraph  \nAttention\nKnowledge \nGraphs\n…\nDual-level \nGraph  \nAttention\nKnowledge \nGraphs\n…\nDual-level \nGraph  \nAttention\nSelf-Attentive \nEncoder\nIncremental \nEncoder\nIncremental \nEncoder\n…\n…\nKnowledge-Enriched \nEmbedding  \nSelf-Attention\nFeed-Forward\nL ×\nKnowledge-Enriched \nEmbedding\nSelf-Attention\nFeed-Forward\nL ×\nContext-Attention\n(a) (b)\nE(i)\nj−M E(i)\nj−1 E(i)\nj\nC(i)[L]\njC(i)[L]\nj−1C(i)[L]\nj−2C(i)[L]\nj−M\nX(i)\nn = x1x2 . . . xK\nKnowledge \nGraphs\nDual-level \nGraph  \nAttention\n…\n…\ng 1\ng K\ng k\n(c)\nx1 xk xK\ng1\n… …\nToken Embedding\n…\ngk\n…\ngK\nGraph \n vectors\n⨁\nf\nPositional \nEncoding\ne1 ek eK… …\nEnriched Embedding\nE(i)\nj−M\nC(i)[L]\nj−M\nC(i)[L]\nn−1\nC(i)[L]\nn\nE(i)\nn\nE(i)\nn\nIncremental Transformer\nTask 1\nTask 2\nTask 3\nMulti-task Learning\nFigure 1: The top is the overview of KAITML and the bottom is the detailed architecture of model\ncomponents. (a) Self-Attentive Encoder. (b) Incremental Encoder. (c) Knowledge Interpreter.\n{g1,g2,...,g K}where each token in the utterance corresponds to a graph, as shown in Figure 1 (c). In\ngeneral, the knowledge interpreter uses each token xk,k = 1,...,K (non-stopword) in an utterance X(i)\nn\nas the key node to retrieve a graph gk comprising its immediate neighbors from ConceptNet, as shown\nin the red box in Figure 1 (c). For each gk, we remove nodes that are stopwords or not in our vocabulary.\nEach retrieved graph gk consists of a key node (the red dots) and its neighboring nodes (different colors\ndenote different relations), where each node cis converted into a vector representation c ∈Rd, where d\ndenotes the size of vector. Then, the knowledge interpreter computes the graph vector gk ∈Rd of the\nretrieved graph gk using the dual-level graph attention mechanism.\nWe use a token embedding layer to convert each tokenxk in X(i)\nn into a vector representationxk ∈Rd.\nTo encode positional information, the position encoding (Vaswani et al., 2017) is added as follows:\nxk = Embed(xk) +Pos(xk). (2)\nFinally, the knowledge-enriched token embedding ek can be obtained via a linear transformation:\nek = W[xk; gk]. (3)\nwhere [; ]denotes concatenation and W ∈Rd×2d denotes a model parameter. All Ktokens in X(i)\nn form\na knowledge-enriched utterance embeddingE(i)\nn ∈RK×dthat is then fed to the Incremental Transformer,\nas shown in Figure 1.\nDual-level Graph Attention Mechanism\nThe dual-level graph attention mechanism is designed to generate a representation for a retrieved knowl-\nedge graph, inspired by (Velickovic et al., 2018), which will be used to augment the semantics of each\ntoken in an utterance. Compared to (Velickovic et al., 2018), our graph attention considers not only all\nnodes in a graph but also relations between nodes. The dual-level graph attention mechanism, including\nnode-level and relation-level attentions, can learn the importance of different neighboring nodes as well\nas the importance of different relations to a key node.\n4433\nNode-level Attention. The node-level attention takes as input the node vectors F(gk) = {cr\ns},r =\n1,...Rk,s = 1,...Nr in the retrieved knowledge graph gk, where Rk denotes the number of relations in\ngk and Nr denotes the number of nodes in the rth relation, to produce relation vectors tr as follows:\ntr =\nNr∑\ns=1\nαr\nscr\ns, (4)\nαr\ns = exp(xk ·cr\ns)\nNr∑\nh=1\nexp(xk ·cr\nh)\n. (5)\nRelation-level Attention. The relation-level attention takes as input the relation vectors tr,r =\n1,...Rk, to produce a graph vector gk as follows:\ngk =\nRk∑\nr=1\nβrtr, (6)\nβr = exp(xk ·tr)\nRk∑\nh=1\nexp(xk ·th)\n. (7)\nIf |gk| = 0, where |gk|denotes the number of nodes in gk, we set gk to the average of all node\nvectors (Zhong et al., 2019b).\n3.3 Incremental Transformer\nWe apply the Incremental Transformer (Li et al., 2019) to encode multi-turn contextual utterances, as\nshown in Figure 1, which contains Self-Attentive Encoder and Incremental Encoder.\nSelf-Attentive Encoder\nThe Self-Attentive Encoder is a transformer encoder as described in (Vaswani et al., 2017), which en-\ncodes the ﬁrst utterance.\nAs shown in Figure 1 (a), the Self-Attentive Encoder contains a stack of L identical layers. Each\nlayer has two sub-layers. The ﬁrst sub-layer is a multi-head self-attention (MultiHead) (Vaswani et al.,\n2017). MultiHead(Q,K,V ) is a multi-head attention function that takes a query matrix Q, a key\nmatrix K, and a value matrix V as input. In current case, Q = K = V . That’s why it’s called self-\nattention. And the second sub-layer is a simple, position-wise fully connected feed-forward network\n(FFN). This FFN consists of two linear transformations with a ReLU activation in between,FFN (x) =\nmax(0,xW1 + b1)W2 + b2, where W1,b1,W2,b2 denote model parameters. (Vaswani et al., 2017)\nFormally, for the ﬁrst knowledge-enriched utterance embedding E(i)\nj−M ∈RK×d, its representation\nC(i)[L]\nj−M ∈RK×d is computed as follows:\nA(i)[l]\nj−M = MultiHead(C(i)[l−1]\nj−M ,C(i)[l−1]\nj−M ,C(i)[l−1]\nj−M ), (8)\nC(i)[l]\nj−M = FFN(A(i)[l]\nj−M). (9)\nwhere l = 1,...,L , C(i)[0]\nj−M = E(i)\nj−M, A(i)[l]\nj−M ∈ RK×d is the hidden state computed by multi-head\nattention at the lth layer, C(i)[l]\nj−M ∈RK×d denotes the representation of E(i)\nj−M after llayer. The residual\nconnection and layer normalization are omitted in the presentation for simplicity. More details can be\nfound in (Vaswani et al., 2017).\n4434\nIncremental Encoder\nThe Incremental Encoder is a variant of the transformer encoder with an additional context-\nattention (Zhang et al., 2018) module, which encodes multi-turn utterances using an incremental encod-\ning scheme. It takes the output of previous utterances and current utterance as input, and use attention\nmechanism to incrementally model relevant context.\nAs shown in Figure 1 (b), the Incremental Encoder contains a stack of Lidentical layers. Each layer\nhas three sub-layers. For each knowledge-enriched utterance embedding E(i)\nn ∈RK×d,n = j−M +\n1,...,j , its representation C(i)[L]\nn ∈RK×d is computed as follows:\nThe ﬁrst sub-layer is a multi-head self-attention:\nA(i)[l]\nn = MultiHead(C(i)[l−1]\nn ,C(i)[l−1]\nn ,C(i)[l−1]\nn ), (10)\nwhere l= 1,...L, C(i)[l−1]\nn ∈RK×d is the output of the previous layer and C(i)[0]\nn = E(i)\nn .\nThe second sub-layer is a multi-head context-attention:\nB(i)[l]\nn = MultiHead(A(i)[l]\nn ,C(i)[L]\nn−1 ,C(i)[L]\nn−1 ), (11)\nwhere C(i)[L]\nn−1 ∈RK×d is the representation of the previous utterances after Llayers.\nThe third sub-layer is a position-wise fully connected feed-forward network:\nC(i)[l]\nn = FFN(B(i)[l]\nn ). (12)\nFinally, C(i)[L]\nj ∈RK×d is the representation of relevant context (including target utterance), as shown\nin Figure 1, which is then fed into a max-pooing layer to learn discriminative features among positions\nand derive the ﬁnal representation O(i)\nj ∈Rd:\nO(i)\nj = MaxPooling(C(i)[L]\nj ). (13)\n3.4 Multi-task Learning\nWe introduce multi-task learning to alleviate the confusion between a few non-neutral categories (e.g.,\nhappy, sad, and angry, etc.) and much more neutral category (e.g., neutral or others) and thus further\nimprove emotion recognition performance, as shown in Figure 1, which contains three different tasks.\nTask 1 is the original emotion recognition task, which predicts the emotion label, including non-neutral\ncategories and neutral category, of target utteranceX(i)\nj . Its loss on one sample ⟨X(i)\nj ,Y (i)\nj ⟩is computed\nas follows:\nloss1 = −\nq∑\nt=1\nY1\n(i)\njt log ˆY1\n(i)\njt , (14)\nˆY1\n(i)\nj = softmax(O(i)\nj W1 + b1). (15)\nwhere W1 ∈ Rd×q and b1 ∈ Rq denotes model parameters, q denotes the number of categories,\nˆY1\n(i)\nj ∈Rq denotes the predicted probability distribution of task 1, and Y1\n(i)\nj ∈Rq (one-hot vector,\nthe corresponding category position is 1, and the remaining positions are 0) denotes the ground-truth\nprobability distribution of task 1.\nTask 2 focuses on the binary classiﬁcation, “non-neutral” versus “neutral”, which determines whether\nthe target utterance X(i)\nj is “non-neutral” or “neutral”. Its loss on one sample ⟨X(i)\nj ,Y (i)\nj ⟩is computed\nas follows:\nloss2 = −\n2∑\nt=1\nY2\n(i)\njt log ˆY2\n(i)\njt , (16)\nˆY2\n(i)\nj = softmax(O(i)\nj W2 + b2). (17)\n4435\nwhere W2 ∈Rd×2 and b2 ∈R2 denotes model parameters, ˆY2\n(i)\nj ∈R2 denotes the predicted probability\ndistribution of task 2, and Y2\n(i)\nj ∈R2 (one-hot vector) denotes the ground-truth probability distribution\nof task 2.\nTask 3 classiﬁes the “non-neutral” into ﬁne-grained emotion categories. Its loss on one sample\n⟨X(i)\nj ,Y (i)\nj ⟩is computed as follows:\nloss3 = 1\n2\nq−1∑\nt=1\n(Y3\n(i)\njt −ˆY3\n(i)\njt )2, (18)\nˆY3\n(i)\nj = sigmoid(O(i)\nj W3 + b3). (19)\nwhere W3 ∈Rd×(q−1) and b3 ∈Rq−1 denotes model parameters, q−1 denotes the number of non-\nneutral categories, ˆY3\n(i)\nj ∈Rq−1 denotes the predicted output of task 3, and Y3\n(i)\nj ∈Rq−1 denotes the\nground-truth output of task 3 (when Y(i)\nj is neutral category, Y3\n(i)\nj is a vector of all zeros, otherwise it’s\na one-hot vector).\nDuring training, the total loss of our model is deﬁned as:\nLoss= loss1 + λ1loss2 + λ2loss3\n1 +λ1 + λ2\n. (20)\nwhere λ1,λ2 ∈[0,1] are weight coefﬁcients of loss2,loss3, respectively.\n4 Experimental Setting\n4.1 Datasets and Evaluations\nWe evaluate our model on the following ﬁve benchmark datasets. Some of datasets, such as MELD,\nIEMOCAP, EmoryNLP, are multimodal conversation datasets containing textual, acoustic, and visual\ninformation. In this paper, we recognize emotion in conversations only based on textual information.\nThe statistics and evaluation metrics of these datasets are drawn in Table 2.\nDataset #Conv. (Train/Val/Test) #Utter. (Train/Val/Test) #Classes Evaluation\nEC 30160/2755/5509 90480/8265/16527 4 Micro-F1(exclude “others”)\nDailyDialogue 11118/1000/1000 87170/8069/7740 7 Micro-F1(exclude “neutral”)\nMELD 1038/114/280 9989/1109/2610 7 Weighted-F1\nEmoryNLP 659/89/79 7551/954/984 7 Weighted-F1\nIEMOCAP 100/20/31 4810/1000/1523 6 Weighted-F1\nTable 2: Dataset descriptions.\nEmoContext(EC) (Chatterjee et al., 2019): Short dialogues composed of three turns comes from social\nmedia. Its emotion labels include happy, sad, angry and others.\nDailyDialogue (Li et al., 2017): Daily communications written by human. Its emotion labels include\nanger, disgust, fear, joy, sadness, surprise and neutral.\nMELD (Poria et al., 2019a): Scripts collected from the Friends TV series. Its emotion labels are the\nsame as the ones used in DailyDialogue.\nEmoryNLP (Zahiri and Choi, 2018): Scripts collected from the Friends TV series as well. Its emotion\nlabels include sad, mad, scared, powerful, peaceful, joyful and neutral, which are different from MELD.\nIEMOCAP (Busso et al., 2008): Two-way emotional conversation. Its emotion labels include happiness,\nsadness, anger, frustrated, excited and neutral.\nThe evaluation metric of each dataset is the same as the one used in (Zhong et al., 2019b).\n4436\n4.2 Baselines\nWe compare our proposed model with the following baselines:\ncLSTM: It ﬁrst adopt a bidirectional LSTM to extract utterance-level features and then use a context-\nlevel unibidirectional LSTM to model the contextual utterances.\nCNN (Kim, 2014): A single-layer CNN is trained on utterance-level without context.\nCNN+cLSTM (Poria et al., 2017): It ﬁrst adopt an CNN to extract utterance-level features and then\napply a context-level unibidirectional LSTM to learn context representations.\nBERT BASE (Devlin et al., 2019): Base version of Bert. It takes as input each utterance with its context\nas a single text.\nDialogueRNN (Majumder et al., 2019): It exploits three gated recurrent units (GRU) to capture speaker\ninformation, context and emotional information of the preceding utterances, respectively.\nKET (Zhong et al., 2019b): It’s the state-of-the-art model for ERTC, where contextual utterances are\nencoded using hierarchical self-attention and commonsense knowledge is incorporated using a context-\naware affective graph attention mechanism.\n4.3 Hyper-parameter Settings\nWe use Adam optimizer (Kingma and Ba, 2015) to train our model with learning rate of 0.0001 and a\nbatch size of 64 in all datasets. We set the class weights in cross-entropy loss as the ratio of the class\ndistribution in the validation set to the class distribution in the training set for each dataset (Zhong et\nal., 2019b). Thus, we can tackle the mismatch in class distribution between validation set and training\nset. The initial token and node embeddings are pre-trained with GloVe (Pennington et al., 2014). The\ndetailed hyper-parameter settings for KAITML are presented in Table 3.\nDataset d p f M L h λ1 λ2\nEC 300 400 1 2 2 4 1.0 0.7\nDailyDialogue 300 400 3 6 3 4 1.0 1.0\nMELD 300 400 1 6 1 4 1.0 0.7\nEmoryNLP 100 200 1 6 2 4 0.9 0.5\nIEMOCAP 300 400 1 6 1 4 0.9 0.6\nTable 3: Hyper-parameter settings for KAITML. d: token/node embedding size. p: hidden size in FFN.\nf: minimum token frequency in vocabulary.M: context length. L: number of encoder layers. h: number\nof heads in MultiHead. λ1,λ2: weight coefﬁcients of loss2,loss3, repectively.\n5 Result Analysis\nModel EC DailyDialogue MELD EmoryNLP IEMOCAP\ncLSTM 69.13 49.90 49.72 26.01 34.84\nCNN (Kim, 2014) 70.56 49.34 55.86 32.59 52.18\nCNN+cLSTM (Poria et al., 2017) 72.62 50.24 56.87 32.89 55.87\nBert BASE (Devlin et al., 2019) 69.46 53.12 56.21 33.15 61.19\nDialogueRNN (Majumder et al., 2019) 74.05 50.65 56.27 31.70 61.21\nKET (Zhong et al., 2019b) 73.48 53.37 58.18 34.39 59.56\nKAITML (ours) 75.39 54.71 58.97 35.59 61.43\nTable 4: Performace comparisons on the ﬁve test sets (%). Bold font denotes the best performance.\n5.1 Comparison with Baselines\nTable 4 shows the performance of different models on 5 benchmark datasets. We can see that our model\noutperforms all the baselines, on all the datasets, which shows the effectiveness of our proposed model\n4437\nfor ERTC. Through paired t-test, there were signiﬁcant differences between our proposed model and all\nbaselines (p≤0.05). Note that all the results of baselines are directly cited from (Zhong et al., 2019b).\nThe state-of-the-art KET model performs best overall among all baselines. And our KAITML model\nsurpasses the KET model by around 1.5% performance on all the dataset tested. To explain this gap\nin performance, it’s signiﬁcant to understand the nature of these models. KAITML and KET both in-\ncorporate external commonsense knowledge and model contextual information based on transformer for\nERTC. This is a key limitation in other baseline models, as external commonsense knowledge can en-\nrich the background and semantic information of utterances and the self-attention module in transformer\nallows model to exploit contextual information more efﬁciently than CNNs and gated RNNs in other\nbaseline models. As for the difference of performance between KAITML and KET, we believe that\nthis is due to the difference of graph attention mechanism and multi-task learning. That KET doesn’t\nconsider various relations in external knowledge base may cause the loss of semantic information. By\ncontrast, KAITML tries to overcome this issue by using a dual-level graph attention mechanism, which\ncan exploit the various relations in external knowledge base and thus support better understanding of\nutterances. In addition, the multi-task learning in KAITML can alleviate the confusion between a few\nnon-neutral utterances and much more neutral ones and thus further improve the emotion recognition\nperformance.\n5.2 Ablation Study\ncontext knowledge multi-task EC DailyDialogue MELD EmoryNLP IEMOCAP\n! ! ! 74.97 56.76 54.22 38.10 50.83\n% ! ! 68.92 54.84 52.68 37.85 49.25\n! % ! 73.90 55.87 53.09 34.75 49.16\n! ! % 73.46 55.57 53.83 36.84 49.19\nTable 5: Ablation results on ﬁve validation sets (%). Context, commonsense knowledge and multi-task\nlearning are all beneﬁcial to the emotion recognition performance.\nTo comprehensively study the impact of context, knowledge and multi-task learning, we remove them\none at a time and investigate their contribution on all datasets. As expected, following Table 5, context,\nknowledge and multi-task learning are all essential to the strong performance of our model on all datasets\nand their combination achieves the best performance. Note that removing knowledge has a greater im-\npact on small datasets (i.e., EmoryNLP and IEMOCAP) than big datasets (i.e., EC, DailyDialogue and\nMELD), which is expected because external commonsense knowledge can help model understand utter-\nances, especially when there is insufﬁcient data. Moreover, compared to other datasets, the performance\nof the EC drops a lot, around 6%, when removing context. The reason may be that there are more short\nutterances on EC, like “ok”, “yes”, whose emotion depends on the context it appears in. With multi-task\nlearning, we observed that the confusion between non-neutral categories and neutral category is allevi-\nated in the confusion matrix and the performance improves by about 1.2% on all datasets on average.\n5.3 Error Analysis\nBy analyzing our predicted emotion labels, we found that the model error is mainly caused by the follow-\ning aspects. Firstly, misclassiﬁcations are often among similar emotion classes in the confusion matrix,\nlike ‘happy’ and ‘excited’, ‘angry’ and ‘frustrated’. Secondly, the performance of emotion classes with\nsmall amount data available is poor, like ‘fear’ and ‘disgust’ in DailyDialogue dataset. Thirdly, some of\ndatasets, such as MELD, IEMOCAP, EmoryNLP, that we use in our experiment are multimodal. And\nwe found that acoustic, and visual modality provide key information to recognize emotions in a few\nutterances (e.g., ‘okay’, ‘yes’, etc.) while our proposed KAITML model considers only textual modality.\n4438\n6 Conclusion and Future Work\nIn this paper, we propose a novel Knowledge Aware Incremental Transformer with Multi-task Learning\n(KAITML) for emotion recognition in textual conversations, which can effectively incorporate contex-\ntual information and commonsense knowledge, and alleviate the confusion between a few non-neutral ut-\nterances and much more neutral ones. Moreover, extensive experimental results show that our KAITML\nmodel outperforms state-of-the-art models across ﬁve benchmark dataset. Future work will focus on the\nfollowing directions: 1) how to differentiate similar emotions, 2) how to recognize emotion using limited\ndata, 3) how to incorporate multimodal information for emotion recognition in conversations.\nAcknowledgements\nWe would like to thank the anonymous reviewers for their insightful comments. This work was supported\nby the Key Programs of the Chinese Academy of Sciences (ZDBS-SSW-JSC006) and the Strategic Pri-\nority Research Program of the Chinese Academy of Sciences (Grant No.XDB32070000).\nReferences\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang,\nSungbok Lee, and Shrikanth S. Narayanan. 2008. IEMOCAP: interactive emotional dyadic motion capture\ndatabase. Lang. Resour. Evaluation, 42(4):335–359.\nAnkush Chatterjee, Umang Gupta, Manoj Kumar Chinnakotla, Radhakrishnan Srikanth, Michel Galley, and Puneet\nAgrawal. 2019. Understanding Emotions in Text Using Deep Learning and Big Data. Comput. Hum. Behav.,\n93:309–317.\nXiuyi Chen, Jiaming Xu, and Bo Xu. 2019. A working memory model for task-oriented dialog response gen-\neration. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2687–2693.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long Short-Term Memory-Networks for Machine Reading.\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016, pages 551–561.\nKyunghyun Cho, Bart van Merrienboer, C ¸ aglar G¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Ma-\nchine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process-\ning, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the\nACL, pages 1724–1734.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirec-\ntional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186.\nYanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu, and Jun Zhao. 2017. An End-to-End\nModel for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge. In\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancou-\nver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 221–231.\nDevamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik Cambria, and Roger Zimmermann. 2018a. ICON:\nInteractive Conversational Memory Network for Multimodal Emotion Detection. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November\n4, 2018, pages 2594–2604.\nDevamanyu Hazarika, Soujanya Poria, Amir Zadeh, Erik Cambria, Louis-Philippe Morency, and Roger Zimmer-\nmann. 2018b. Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018,\nVolume 1 (Long Papers), pages 2122–2132.\n4439\nJunqing He, Bing Wang, Mingming Fu, Tianqi Yang, and Xuemin Zhao. 2019. Hierarchical Attention and\nKnowledge Matching Networks With Information Enhancement for End-to-End Task-Oriented Dialog Systems.\nIEEE Access, 7:18871–18883.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735–\n1780.\nChenyang Huang, Osmar R. Za ¨ıane, Amine Trabelsi, and Nouha Dziri. 2018. Automatic Dialogue Generation\nwith Expressed Emotions. In Proceedings of the 2018 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 2 (Short Papers), pages 49–54.\nYoon Kim. 2014. Convolutional Neural Networks for Sentence Classiﬁcation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha,\nQatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1746–1751.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A Manually\nLabelled Multi-turn Dialogue Dataset. In Proceedings of the Eighth International Joint Conference on Natural\nLanguage Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long\nPapers, pages 986–995.\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng, Qian Li, and Jie Zhou. 2019. Incremental Transformer with\nDeliberation Decoder for Document Grounded Conversations. In Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 12–21.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Mem2Seq: Effectively Incorporating Knowledge\nBases into End-to-End Task-Oriented Dialog Systems. In Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long\nPapers, pages 1468–1478.\nNavonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander F. Gelbukh, and Erik Cam-\nbria. 2019. DialogueRNN: An Attentive RNN for Emotion Detection in Conversations. In The Thirty-Third\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial\nIntelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelli-\ngence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 6818–6825.\nTodor Mihaylov and Anette Frank. 2018. Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehen-\nsion with External Commonsense Knowledge. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers ,\npages 821–832.\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Rep-\nresentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL,\npages 1532–1543.\nSoujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder, Amir Zadeh, and Louis-Philippe\nMorency. 2017. Context-Dependent Sentiment Analysis in User-Generated Videos. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30\n- August 4, Volume 1: Long Papers, pages 873–883.\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea.\n2019a. MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations. In Proceed-\nings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages 527–536.\nSoujanya Poria, Navonil Majumder, Rada Mihalcea, and Eduard H. Hovy. 2019b. Emotion Recognition in Con-\nversation: Research Challenges, Datasets, and Recent Advances. IEEE Access, 7:100943–100953.\nArmin Seyeditabari, Narges Tabari, and Wlodek Zadrozny. 2018. Emotion Detection in Text: a Review. CoRR,\nabs/1806.00674.\n4440\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017. ConceptNet 5.5: An Open Multilingual Graph of General\nKnowledge. In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017,\nSan Francisco, California, USA, pages 4444–4451.\nShang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen. 2018. How Time Matters: Learning Time-Decay Attention\nfor Contextual Spoken Language Understanding in Dialogues. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 2133–2142.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA,\nUSA, pages 5998–6008.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li `o, and Yoshua Bengio. 2018.\nGraph Attention Networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancou-\nver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.\nSida I. Wang and Christopher D. Manning. 2012. Baselines and Bigrams: Simple, Good Sentiment and Topic\nClassiﬁcation. In The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of\nthe Conference, July 8-14, 2012, Jeju Island, Korea - Volume 2: Short Papers, pages 90–94.\nTom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou, Subham Biswas, and Minlie Huang. 2018. Augmenting\nEnd-to-End Dialogue Systems With Commonsense Knowledge. InProceedings of the Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-\n18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018, pages 4970–4977.\nSayyed M. Zahiri and Jinho D. Choi. 2018. Emotion Detection on TV Show Transcripts with Sequence-Based\nConvolutional Neural Networks. In The Workshops of the The Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, New Orleans, Louisiana, USA, February 2-7, 2018, pages 44–52.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018. Improv-\ning the Transformer Translation Model with Document-Level Context. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018 ,\npages 533–542.\nPeixiang Zhong, Di Wang, and Chunyan Miao. 2019a. An Affect-Rich Neural Conversational Model with Biased\nAttention and Weighted Cross-Entropy Loss. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence,\nAAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth\nAAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, Jan-\nuary 27 - February 1, 2019, pages 7492–7500.\nPeixiang Zhong, Di Wang, and Chunyan Miao. 2019b. Knowledge-Enriched Transformer for Emotion Detection\nin Textual Conversations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 165–176.\nHao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018a. Emotional Chatting Machine:\nEmotional Conversation Generation with Internal and External Memory. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018, pages 730–739.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018b. Commonsense\nKnowledge Aware Conversation Generation with Graph Attention. InProceedings of the Twenty-Seventh Inter-\nnational Joint Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden , pages\n4623–4629.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8218077421188354
    },
    {
      "name": "Transformer",
      "score": 0.6477068066596985
    },
    {
      "name": "Utterance",
      "score": 0.6463680267333984
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6404015421867371
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.5852839946746826
    },
    {
      "name": "Natural language processing",
      "score": 0.5385855436325073
    },
    {
      "name": "ENCODE",
      "score": 0.5074135661125183
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4584154784679413
    },
    {
      "name": "Domain knowledge",
      "score": 0.18076452612876892
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}