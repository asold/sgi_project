{
  "title": "STDFormer: Spatial-Temporal Motion Transformer for Multiple Object Tracking",
  "url": "https://openalex.org/W4362501010",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2129114651",
      "name": "Mengjie Hu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2110744601",
      "name": "Xiaotong Zhu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2096795783",
      "name": "Haotian Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2128614332",
      "name": "Shixiang Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101066039",
      "name": "Chun Liu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2038458401",
      "name": "Qing Song",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6809649412",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W3149485574",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W6839744764",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6810827814",
    "https://openalex.org/W6801013943",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3166285241",
    "https://openalex.org/W3006651016",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W3126235906",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W2618529757",
    "https://openalex.org/W3130778992",
    "https://openalex.org/W2903855329",
    "https://openalex.org/W6803363630",
    "https://openalex.org/W6798838024",
    "https://openalex.org/W3115517346",
    "https://openalex.org/W6748176785",
    "https://openalex.org/W6760212410",
    "https://openalex.org/W6799133303",
    "https://openalex.org/W6736160952",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W3108316907",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3165926952",
    "https://openalex.org/W3179985441",
    "https://openalex.org/W2920942303",
    "https://openalex.org/W3106763294",
    "https://openalex.org/W6802459829",
    "https://openalex.org/W3018207438",
    "https://openalex.org/W3167949052",
    "https://openalex.org/W3084173793",
    "https://openalex.org/W2766984662",
    "https://openalex.org/W2889935068",
    "https://openalex.org/W4313117614",
    "https://openalex.org/W4312906297",
    "https://openalex.org/W2016135469",
    "https://openalex.org/W3027919498",
    "https://openalex.org/W6840663683",
    "https://openalex.org/W6846111883",
    "https://openalex.org/W6788023325",
    "https://openalex.org/W4312689495",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W6810826495",
    "https://openalex.org/W2113926057",
    "https://openalex.org/W3207452968",
    "https://openalex.org/W2894642126",
    "https://openalex.org/W3174690341",
    "https://openalex.org/W3195775608",
    "https://openalex.org/W4319299988",
    "https://openalex.org/W2963063317",
    "https://openalex.org/W3094000868",
    "https://openalex.org/W2900871370",
    "https://openalex.org/W2252355370",
    "https://openalex.org/W4304479854",
    "https://openalex.org/W6795368335",
    "https://openalex.org/W3035727180",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W4205537101",
    "https://openalex.org/W3095753995",
    "https://openalex.org/W6780383567",
    "https://openalex.org/W3086436251",
    "https://openalex.org/W3174101598",
    "https://openalex.org/W2511791013",
    "https://openalex.org/W2603203130",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2579024533",
    "https://openalex.org/W4313072323",
    "https://openalex.org/W6775253321",
    "https://openalex.org/W2964019074",
    "https://openalex.org/W6678664622",
    "https://openalex.org/W2891254924",
    "https://openalex.org/W3205100603",
    "https://openalex.org/W2119539043",
    "https://openalex.org/W2020934227",
    "https://openalex.org/W2525726571",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W6696672603",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2981393651",
    "https://openalex.org/W2605035112",
    "https://openalex.org/W4285357233",
    "https://openalex.org/W3104778224",
    "https://openalex.org/W3104218139",
    "https://openalex.org/W4307415560",
    "https://openalex.org/W4310467366",
    "https://openalex.org/W3197804339",
    "https://openalex.org/W4226078731",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4312473433",
    "https://openalex.org/W4286904999",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3035578948",
    "https://openalex.org/W4283748233",
    "https://openalex.org/W2124781496",
    "https://openalex.org/W3099887740",
    "https://openalex.org/W3171485246",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W3184439416",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4319866011",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W3012922853",
    "https://openalex.org/W2291627510",
    "https://openalex.org/W4386076204",
    "https://openalex.org/W2951585248",
    "https://openalex.org/W3185016686"
  ],
  "abstract": "Mainstream multi-object tracking methods exploit appearance information and/or motion information to achieve interframe association. However, dealing with similar appearance and occlusion is a challenge for appearance information, while motion information is limited by linear assumptions and is prone to failure in nonlinear motion patterns. In this work, we disregard appearance clues and propose a pure motion tracker to address the above issues. It dexterously utilizes Transformer to estimate complex motion and achieves high-performance tracking with low computing resources. Furthermore, contrastive learning is introduced to optimize feature representation for robust association. Specifically, we first exploit the long-range modeling capability of Transformer to mine intention information in temporal motion and decision information in spatial interaction and introduce prior detection to constrain the range of motion estimation. Then, we introduce contrastive learning as an auxiliary task to extract reliable motion features to compute affinity and introduce bidirectional matching to improve the affinity computation distribution. In addition, given that both tasks are dedicated to narrowing the embedding distance between the motion features of the tracked object and the detection features, we design a joint-motion-and-association framework to unify the above two tasks in one framework for optimization. The experimental results achieved with three benchmark datasets, MOT17, MOT20 and DanceTrack, verify the effectiveness of our proposed method. Compared with state-of-the-art methods, the proposed STDFormer sets a new state-of-the-art on DanceTrack and achieves competitive performance on MOT17 and MOT20. This demonstrates the advantage of our method in handling associations under similar appearance, occlusion or nonlinear motion. At the same time, the significant advantages of the proposed method over Transformer-based and contrastive learning-based methods suggest a new direction for the application of Transformer and contrastive learning in MOT. In addition, to verify the generalization of STDFormer in unmanned aerial vehicle (UAV) videos, we also evaluate STDFormer on VisDrone2019. The results show that STDFormer achieves state-of-the-art performance on VisDrone2019, which proves that it can handle small-scale object associations in UAV videos well. The code is available at https://github.com/Xiaotong-Zhu/STDFormer.",
  "full_text": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 1\nSTDFormer: Spatial-Temporal Motion Transformer\nfor Multiple Object Tracking\nMengjie Hu, Xiaotong Zhu, Haotian Wang, Shixiang Cao, Chun Liu and Qing Song\nAbstract—Mainstream multi-object tracking methods exploit\nappearance information and/or motion information to achieve\ninterframe association. However, dealing with similar appearance\nand occlusion is a challenge for appearance information, while\nmotion information is limited by linear assumptions and is\nprone to failure in nonlinear motion patterns. In this work, we\ndisregard appearance clues and propose a pure motion tracker\nto address the above issues. It dexterously utilizes Transformer\nto estimate complex motion and achieves high-performance\ntracking with low computing resources. Furthermore, contrastive\nlearning is introduced to optimize feature representation for\nrobust association. Specifically, we first exploit the long-range\nmodeling capability of Transformer to mine intention information\nin temporal motion and decision information in spatial inter-\naction and introduce prior detection to constrain the range of\nmotion estimation. Then, we introduce contrastive learning as\nan auxiliary task to extract reliable motion features to compute\naffinity and introduce bidirectional matching to improve the\naffinity computation distribution. In addition, given that both\ntasks are dedicated to narrowing the embedding distance between\nthe motion features of the tracked object and the detection\nfeatures, we design a joint-motion-and-association framework to\nunify the above two tasks in one framework for optimization. The\nexperimental results achieved with three benchmark datasets,\nMOT17, MOT20 and DanceTrack, verify the effectiveness of our\nproposed method. Compared with state-of-the-art methods, the\nproposed STDFormer sets a new state-of-the-art on DanceTrack\nand achieves competitive performance on MOT17 and MOT20.\nThis demonstrates the advantage of our method in handling asso-\nciations under similar appearance, occlusion or nonlinear motion.\nAt the same time, the significant advantages of the proposed\nmethod over Transformer-based and contrastive learning-based\nmethods suggest a new direction for the application of Trans-\nformer and contrastive learning in MOT. In addition, to verify the\ngeneralization of STDFormer in unmanned aerial vehicle (UA V)\nvideos, we also evaluate STDFormer on VisDrone2019. The re-\nsults show that STDFormer achieves state-of-the-art performance\non VisDrone2019, which proves that it can handle small-scale\nobject associations in UA V videos well. The code is available at\nhttps://github.com/Xiaotong-Zhu/STDFormer.\nIndex Terms —Multi-object tracking, joint-motion-and-\nassociation, spatial-temporal Transformer, contrastive learning,\nbidirectional matching.\nI. INTRODUCTION\nThis work was supported in part by the National Key Research and\nDevelopment Program of China under Grant 2021YFF0500900.\nMengjie Hu, Xiaotong Zhu, Haotian Wang, Chun Liu and Qing Song are\nwith the School of Artificial Intelligence, Beijing University of Posts and\nTelecommunications, Beijing, China (e-mail: mengjie.hu@bupt.edu.cn,\n17zxt826@bupt.edu.cn, lingxin@bupt.edu.cn, chun.liu@bupt.edu.cn,\npriv@bupt.edu.cn).\nShixiang Cao is with Beijing Institute of Space Mechanics & Electricity,\nBeijing 100081, China (e-mail: cshixiang 0110@126.com).\n(a) Linear\n (b) Temporal\n(c) Temporal + Spatial\n (d) Temporal + Spatial + Detection\nFig. 1. Illustration of the idea of STDFormer. (a) is linear motion\nestimation, and (b) (c) (d) is nonlinear motion estimation. (a) The motion\nprediction based on the linear model has a large deviation from the true\ntrajectory of the nonlinear motion. (b) Temporal-based motion estimation can\nmore accurately determine the purpose of nonlinear motion accurately by\nmining historical motion information, and eventually improve motion predic-\ntion (green). However, targets may collide in space when motion estimation\nonly considers temporal information ( yellow and blue). (c) Conflicts can be\navoided by adding spatial information based on temporal prediction, which is\nmore consistent with the social attributes for people to flee from danger. (d)\nDetection constraints can further modify our predictions to be more biased\ntoward ground truth, and can also indicate avoidance directions for conflicting\ntrajectories (dashed red lines).\nM\nULTI-OBJECT tracking (MOT) plays an essential role\nin computer vision. It is widely used in video surveil-\nlance, autonomous driving, motion recognition and crowd\nbehavior analysis. The goal of multi-object tracking is to\nfind objects of interest in a video sequence and match the\nsame objects frame-by-frame. This task can be divided into\ntwo subtasks: object detection and data association. Accord-\ning to the combination of the two subtasks, the mainstream\nmethods can be separated into two paradigms: a) tracking-\nCopyright ©2023 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from\nthe IEEE by sending an email to pubs-permissions@ieee.org.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 2\nby-detection [1]–[4] and b) joint-detection-and-tracking [5]–\n[9]. Researchers [6] have discovered that tracking loss and\ndetection loss are incompatible and even somewhat impair\ndetection performance when training a single backbone net-\nwork jointly. Therefore, we believe that tracking-by-detection,\nwhich decouples the two subtasks, is a better solution. Thanks\nto the rapid development of object detection, an increasing\nnumber of tracking-by-detection methods have started to use\npowerful detectors that are already in place to implement\nhigh-performance tracking. Compared with object detection,\ndata association methods have developed relatively slowly.\nMoreover, existing data association methods still have some\nlimitations in multi-object tracking. To further improve the\nperformance of multi-object tracking, more work is needed to\ndevelop data association methods.\nMost of the data association techniques currently in use rely\non appearance and motion data, with the former predominating\nand the latter typically employed as a secondary association.\nUnfortunately, appearance information performs poorly for\noccluded, blurred or similar objects due to noisy detections\nor similar appearances. Inspired by trajectory prediction [ ?],\n[10], [12], we recognize that if a high-performance motion\nmodel can perfectly predict the object trajectory, the model can\nmitigate the false associations caused by these aforementioned\nproblems, as shown in Figure 1. Moreover, some recent studies\nhave demonstrated that accurate tracking can be achieved\nby relying on motion information alone. Hence, we aim to\nconstruct a powerful motion model for robust data association.\nIn past studies on MOT tasks, the motion information\nwas encoded either by conventional filtering or data-driven\nmethods. Compared with conventional filtering [1], [4], [13]–\n[18], data-driven methods [19]–[23] have attracted less at-\ntention since most studies typically use motion models as\nan auxiliary clue. However, data-driven motion models have\ninherent advantages in handling nonlinear and sophisticated\nmotion patterns which can not be expressed by filtering-based\nmethods. Among data-driven methods, RNNs make up the\nmajority of existing model frameworks since they are capable\nof handling long-range motion context dependencies. As the\nRNN blocks propagate, the relationship between two long-\nrange motion information becomes very weak. RNN-based\nmethods have limitations for modeling long-range motion. In\nsummary, our motivation is to explore a new framework for\nconstructing a powerful motion model for robust association\nunder similar appearance, occlusion, or nonlinear motion.\nRecent literature in sequence modeling confirmed that\nTransformer outperforms RNNs in long-range modeling and\nparallel computing due to the attention mechanism. Conse-\nquently, we leverage the Transformer architecture to model\nlong-range motion information. On the other hand, to achieve\naccurate trajectory prediction, we deeply mine the influence\nof spatial-temporal motion information and observations, as\nshown in Figure 1. Contemporary trajectory prediction meth-\nods emphasize that the motion of an object is determined\nby multiple factors. As shown in Figure 1(b), the temporal\ninformation contains the motion intention of the object. In-\ntention dependence is the dominant factor for the movement\nof an object in any scene. That is, each object has its own\nintended destination. Affected by social neighbors and the\nphysical environment, objects make temporary changes during\ntheir progress. As shown in Figure 1(c), the spatial interaction\nconstrains the movement distribution of the object in the next\nstep. To obtain more accurate motion prediction, we also\nintroduce the detections of the latest frame as prior knowledge\nto further restrict the object’s potential positions, as shown in\nFigure 1(d).\nTo accomplish the aforementioned goals, we propose a\nTransformer-based multi-object tracking model with joint\nSpatial-Temporal motion and prior D etection, called STD-\nFormer. STDFormer performs object motion prediction and\naffinity matrix calculation between detections and tracks by\nutilizing spatial-temporal constraints as well as potential de-\ntection. As shown in Figure 2, our model employs a parallel\nframework for jointly learning motion prediction and asso-\nciation, referred to as joint-motion-and-association. Unlike\njoint-detection-and-tracking, our approach achieves win-win\ncooperation because the jointly optimized feature space is\nexceptionally harmonious for both tasks. We believe that the\nmotion prediction module in our framework can implicitly\nutilize the association information encoded in the interactive\nfeatures of the current frame, making the feature represen-\ntations for the motion prediction module close to the real\ndetection feature representations. Specifically, the network\nconsists of four components. Among them, our core design\nis to propose the STD (Spatial-Temporal-Detection) module\nin feature interaction, which utilizes the attention mechanism\nof Transformer to effectively realize the interaction of spatial,\ntemporal and detection information. As for the token mecha-\nnism, we propose a learnable trajectory token, which obtains\ninformation about the entire trajectory by aggregating the\nfeatures of all tracking boxes for a single trajectory in temporal\nattention. Each trajectory token represents an object and is\nused in spatial attention, detection attention and affinity calcu-\nlation. For the association task branch, discriminative features\nand effective affinity calculation are crucial. However, existing\nmethods optimize the affinity matrix between detections and\ntracks are very dependent on high-quality detection results,\nwhereas missed and coarse detections seriously damage fea-\nture representation learning. To enhance the association task,\nwe introduce contrastive learning for the first time in MOT\ntasks to learn motion representations, and utilize bidirectional\nmatching to optimize the final affinity calculation.\nTo summarize, our contributions are as follows:\n• We present a parallel framework for motion prediction\nand affinity calculation, named STDFormer. STDFormer\nimproves the performance of both tasks via joint opti-\nmization.\n• To the best of our knowledge, we are the first to explicitly\nuse a Transformer architecture to model long-range mo-\ntion information in MOT without considering appearance\ninformation.\n• We generate a learnable trajectory token by aggregating\nprevious tracking box features in temporal attention to\nefficiently switch to spatial attention, detection attention\nand affinity calculation.\n• We initially apply contrastive learning in MOT tasks\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 3\nto learn motion representation, and we further leverage\nbidirectional matching to improve affinity calculation.\nII. RELATED WORK\nIn this paper, with the aid of the existing detectors, we\nformulate multi-object tracking problem as a motion track-\ning model and utilize the Transformer architecture to mine\nlong-range context dependency. Additionally, we employ con-\ntrastive learning to learn the highly discriminative feature\nrepresentation and compute the matching similarity during\nthe training process. Thus, we review the most relevant tasks\nof multi-object tracking, which are motion modeling, Trans-\nformer tracking, and contrastive learning.\nA. Motion Modeling\nMotion models aim to exploit motion information to predict\nspatial-temporal variations in trajectories, and infer spatial\naffinity between predictions and detections. The existing re-\nsearch methods of motion prediction can be roughly divided\ninto two categories: traditional filtering-based methods and\ndata-driven methods.\nThe common idea behind filtering-based motion models\nis to apply a Bayesian estimation framework. Following\nBayesian methodology, different Bayesian filtering techniques\nhave been developed for different scenarios. As one of the\nmost classic Bayesian filters, the vanilla Kalman filter [24]\nwith the assumption of uniform motion has gradually become\nthe most popular motion model in multi-object tracking tasks\n[1], [4], [5], [16], [17], [25]. However, it is not suitable for\nnonlinear motion. To solve this problem, [26], [27] utilized\nthe local linearization method based on the Kalman filter\nto address nonlinear motion, and particle filters [28] use a\nlarge number of random sampling points to approximate the\nposterior probability density function. The former methods\nhave a large tracking error when applied to highly nonlinear\nmotion; in contrast, the latter is close to optimal Bayesian\nestimation when the number of sampling points tends to\ninfinity and can correctly handle nonlinear motion estimation.\nConsidering the target interaction and variable number of\ntargets in MOT, in some studies [29], [30] more advanced\nparticle filter variants were applied, for example, Reversible-\nJump Markov Chain Monte Carlo (RJMCMC) particle filter\n[31]. Nonetheless, particle filter-based methods have seldom\nbeen applied in MOT, given that a large number of sampled\nparticles leads to a sharp increase in computation.\nAs traditional filtering-based methods cannot describe the\nmotion pattern of objects accurately, more complex data-\ndriven motion methods have been proposed to achieve more\ncomplicated state prediction. In the existing multi-object track-\ning literature, data-driven approaches have mainly been based\non RNN architecture to mine the temporal information of\nmotion. [32] introduced recurrent networks to simulate a\nBayesian filter for motion estimation. After [32], different\ntypes and combinations of RNNs were used [19]–[23] to\nrealize deterministic or probability distribution prediction of\nobject motion. However, the relationship between long-range\nmotion information will become very weak as RNN blocks\npropagate. In addition, graph models are also considered for\nmodeling object motion. [33] proposed two modules, box\nembedding and tracklet embedding, which used the attention\nand reconstruction mechanisms of deep graph convolutional\nnetworks to model local and global motion information, re-\nspectively. It should be pointed out that the global motion\nmodeling in [33] was based on tracklets. It did not establish\nconnections to box nodes of non-adjacent frames, which made\nit unable to mine long-range motion information from low-\nlevel features.\nRecently, Transformer has been shown to perform better\nthan RNNs in long-range modeling and parallel computing.\nCompared with the graph models, Transformer does not need\nto pre-design graph. It can directly build a fully connected\ngraph through self-attention. Inspired by this, our approach\nadopts the Transformer architecture for long-range modeling\nof motion information to extract more accurate motion features\nand mine more useful motion cues.\nB. Transformer In Tracking\nTransformers have achieved great success in natural lan-\nguage processing and have gradually been applied to fields\nsuch as computer vision in recent years. The existing\nTransformer-based MOT methods can be categorized as short-\nterm models or long-term models, depending on the range of\ninformation.\nShort-term models only consider the local information of\nadjacent frames to learn and infer object trajectories. [34]\nleverages features from previously detected objects as queries\nto discover associated objects in subsequent frames. [35]\nintroduces pixel-level dense queries with Transformers and\nproposes a dual decoder to output center heatmap and object\nsize, as well as tracking displacements in adjacent frames.\n[36] uses previously detected results as references to aggregate\nthe corresponding features from the combined features of the\nadjacent frames. For each reference, [36] then concurrently\npredicts the one-to-one track state.\nIn contrast, long-term models have access to longer-range\ninformation beyond two frames, which theoretically can obtain\nmore accurate results by using more contextual cues. [37],\n[38] achieve detection and data association synchronously by\nintegrating object and autoregressive track queries as input to\nthe Transformer decoder in the next time step. These methods\nimplicitly apply long-term temporal information by propagat-\ning track queries frame-by-frame. [39], [40] integrate long-\nterm temporal information by focusing on all past embeddings\nfor each individual object and use this information to predict\nthe appropriate embedding for the current time step. [41]\nconstructs a spatial map for objects appearing in each frame\nof the past T frames, and leverages Transformer architecture\nto jointly learn the spatial and temporal relationships of\nsmall trajectories as well as candidate trajectories for efficient\nassociation. However, these methods which explicitly utilize\nmulti-frame information, usually require constructing a large\nspatial-temporal memory to store past observations of tracked\nobjects, which consumes expensive storage and computing\nresources.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 4\nAll of the aforementioned approaches, which explicitly use\nmulti-frame information, take full advantage of Transformer’s\nlong-term modeling capabilities. The high resource cost of this\ntechnique is due to the storage of high-dimensional visual\nfeatures and motion features. To address this problem, our\nmethod only stores and encodes low-dimensional position\ninformation of objects in the past multiple frames and discards\nappearance information.\nC. Contrastive Learning\nContrastive learning is an effective representation learning\nmethod. It has strong discriminative power to distinguish the\nsame object from other objects by pushing away negative\nembedding distances and narrowing the positive ones. This\ntechnology has recently obtained amazing results in various\nfields, such as computer vision [42]–[44], natural language\nunderstanding [45]–[47], and text-image matching. Contrastive\nlearning applied specifically to multi-object tracking has re-\nceived less attention. The few relevant studies [48], [49] also\nfocused on appearance features, which can be learned effec-\ntively by contrastive learning. However, for motion features,\ndetermining how to design positive and negative samples\neffectively is very important. Recently, in the field of trajectory\nprediction, [50] was the first to learn motion representation by\ncontrastive learning. [50] proposes a social sampling strategy.\nIt constructs the positive event from the ground-truth location\nof the primary agent and the negative events from the regions\nof other neighbors, given that one location cannot be occupied\nby multiple agents at the same time. Our work is inspired by\nthe safety of dealing with the sampling strategy proposed by\n[50].\nOur work attempts to aggregate the historical features of the\ntracks to obtain the trajectory information of the objects in the\ncurrent frame and compute the similarity with the detections.\nBoth subtasks require the trajectory token features to be as\nclose as possible to the motion embedding of objects in the\ncurrent frame. Therefore, our method takes all the objects\nappearing in the current frame as samples. The samples be-\nlonging to the same objects as the historical tracks are positive\nsamples, and the samples belonging to different objects are\nnegative samples. This method enables the trajectory token\nfeatures of tracks to effectively learn the behavioral intentions\nof objects. To the best of our knowledge, we are the first to\nutilize the contrastive idea for motion representation learning\nin MOT.\nIII. METHODOLOGY\nIn this section, we first introduce the overall framework of\nSTDFormer (Figure 2). Then, we provide a detailed descrip-\ntion of the model, training and inference process.\nA. Overview\nGiven a sequence of video frames, the goal of MOT is to\ndetect and associate the targets frame-by-frame. In this paper,\nwe address the problem of online multi-object tracking in a\nscene by following a tracking-by-detection paradigm.\nBefore introducing the pipeline of our tracking algo-\nrithm, we define some symbolic expressions. Specifically, let\nDt = {d1, . . . , dN } denote the detections of N objects at\nframe T. Each detection di = [ x, y, w, h] is represented\nas the center point and size of the bounding box. Let\nTt−1 = {Tt−1:t−k\n1 , . . . , Tt−1:t−k\nM } denote the trajectories of\nM tracked objects at frame t−1. Every trajectory Tt−1:t−k\nj =\n[Tt−1\nj , . . . , Tt−k\nj ] consists of the tracking boxes from the j-th\ntracked item over the previous k frames in reverse order of\ntime. Note that if the length of a trajectory Tj at frame t is\ns (s < k), we need to pad Tt−1:t−k\nj . Specifically, we assume\nthat the state of the short trajectory Tj is standing still until\nthe trajectory initialization, so we repeat the initial tracking\nbox Tt−s\nj of the trajectory k − s times and regard them as\nhypothetical tracking boxes for Tj from frame t-k to frame\nt-s+1:\nTt−1:t−k\nj = [\ns\nz }| {\nTt−1\nj , . . . , Tt−s\nj ,\nk−s\nz }| {\nTt−s\nj , . . . , Tt−s\nj ] (1)\nThe j-th tracked object’s tracking box at frame t Tt\nj =\n[x, y, w, h] takes the same definition as detection, where\nt ∈ {t − 1, . . . , t− k}. Based on the definition, the tracking\nboxes of all tracked objects at frame t can be represented as\nTt = {Tt\n1 , . . . , Tt\nM }. Let ∆t−1:t = {δ1, . . . , δM } denote the\ntracked objects’ displacements between frame t−1 and frame\nt. Each tracked object’s displacementδt−1:t\nj = [dx, dy, dw, dh]\nis expressed as the change in the center point and size of\nthe tracking box between two frames. Let At\nN:M denote the\naffinity matrix at frame t, which indicates the similarity of\ndetections and tracked objects.\nThe workflow of STDFormer contains two stages: 1) At\nframe t, we apply a high-performance detector [51] to identify\nand locate all targets Dt; 2) the proposed STDFormer takes\nobject trajectories Tt−1 up to time t − 1 and detections Dt at\ntime t as input and outputs each tracked object’s displacements\n∆t−1:t as well as the affinity matrix At\nN:M . During training,\nwe introduce an auxiliary task to learn more accurate motion\nfeature representation. This stage is similar to the second stage,\nexcept that the input now uses tracking boxes Tt from the\nt-th frame instead of detections Dt. During inference, we\npropose a step-by-step association strategy, which utilizes an\naffinity matrix to match first and then adopts the IoU similarity\nbetween detections and predicted tracking boxes to match the\nremaining detections and tracks.\nSpecifically, as shown in Figure 2, STDFormer consists of\nfour main components: 1) a feature extraction module that\nencodes the current frame’s detection information along with\nthe tracked objects’ historical motion data from the previous\nframes; 2) a feature interaction module that takes advantage of\nTransformer’s attention mechanism to aggregate trajectories’\nspatial-temporal features and detection features; 3) a motion\nprediction head that generates displacements of tracked objects\nin adjacent frames, based on the difference between tracking\nbox features in the previous frame and aggregated features\nof the trajectories; and 4) an affinity calculation head that\ncomputes the affinity matrix between detections and tracked\nobjects for data association.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 5\nFig. 2. The pipeline of the proposed joint-motion-and-association framework, STDFormer , where ⊗ represents concatenate features. The solid line is\nthe data flow for training and testing. It takes the tracking boxes of the existing trajectories in the past k frames and the detection boxes of the current frame\nas input and outputs the localization of the trajectories in the current frame and the association probability matrix between the detections and the trajectories.\nIn training, we add an auxiliary task, which inputs the ground truth of the tracking boxes in the current frame to the network instead of the detection boxes,\nand its data flow ( red dotted line ) is consistent with the detection data flow ( green solid line ). In testing, the predicted tracking boxes are used to update\nthe trajectory set as input for the next frame ( brown dashed line ). The pipeline consists of four components: feature extraction, feature interaction, motion\nprediction head, and affinity calculation head (the detector component is omitted). Feature interaction is the core component of the whole method, which is\nused to mine the spatial-temporal information of the trajectories and interact with detection.\n(a) Feature Extraction Module\n(b) Embedding Layer\nFig. 3. Feature extraction module. The center points and sizes of the\ninput tracking boxes and detection boxes are denoted as (xc, yc, w, h).\nThe embedding layer maps each dimension of the low-dimensional input\nto the high-dimensional space and connects them into embedding vectors\n(R4 ⇒ R4d). The MLP layer fuses and reduces the dimension of the\nembedding vectors of each dimension (R4d ⇒ Rd), and finally outputs\nthe extracted feature vector Ht−1 and Ot.\nB. Feature Extraction\nBoth historical motion information and detection informa-\ntion represent spatial information. Therefore, they can be\nembedded into the same feature space by a shared feature\nextractor. As shown in Figure 3, taking the input object\ntrajectories up to time t−1 Tt−1 ∈ RM×k×4 and detections at\ntime t Dt ∈ RN×4, we utilize an embedding layer to extract\ntheir features. The embedding layer is a lookup table that can\nbe trained. It creates a weight matrix W ∈ Rr×d, where r\nis the embedding vector size and d is the embedding vector\ndimension. This layer takes only positive integers (indices) as\ninput and converts them into fixed-size embedding vectors.\nSpecifically, the embedding layer converts each integer i into\nthe i-th row of the embedding weight matrix. As shown in\nFigure 3(b), in this work, we take the center point coordinates\nand bounding box size values of trajectory and detections as\na set of positive integer indices, retrieve the corresponding\nembedding vectors from the embedding weight matrix and\nconcatenate them as the output of the embedding layer. We\ndefine the extracted features for trajectories and detections as\nHt−1\nemb ∈ RM×k×4d and Ot\nemb ∈ RN×4d, where d indicates\nthe embedded dimension. The calculation process of the\nembedding layer is as follows:\nxemb = W[xc] (2)\nyemb = W[yc] (3)\nwemb = W[w] (4)\nhemb = W[h] (5)\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 6\nFig. 4. Feature interaction module. STD concatenates the learnable trajectory tokens QLTT and the extracted features of the historical tracking boxes\nHt−1:t−k as input, and outputs the iteratively updated trajectory tokens QLTT after three spatial-temporal attention submodules. Each spatial-temporal\nattention submodule is composed of a temporal attention layer and a spatial attention layer, and realizes feature interaction based on a self-attention mechanism.\nThen, the updated trajectory tokens QLTT and the extracted feature of the detection boxes Ot are fed to the three detection attention layers, and QLTT is\nused as the query vector to realize the cross-attention with Ot. Finally, the STD outputs the updated trajectory tokens QLTT .\nHt−1\nemb/Ot\nemb = concat(xemb, yemb, wemb, hemb) (6)\nFurthermore, we use a 1-layer Multi-Layer Perceptron\n(MLP) to fuse the features of the last dimension and map\nthem to the final embedded representation, Ht−1 ∈ RM×k×d\nand Ot ∈ RN×d.\nIn summary, our embedding layer maps low-dimensional\nspatial data to high-dimensional feature space, which obtains\nmore discriminative feature representations and is propagated\nto the next feature interaction module.\nC. Transformer for Feature Interaction\nIn this subsection, we demonstrate how to leverage Trans-\nformer’s attention mechanism for feature interaction: feature\ninteraction using S patial attention, T emporal attention, and\nDetection attention, STD for short.\nAfter feature extraction, Ht−1 =\n{Ht−1:t−k\n1 , . . . , Ht−1:t−k\nM } and Ot = {O1, . . . , ON } are\npassed through the STD module to implement feature\ninteraction. As shown in Figure 4, the STD module consists\nof three different types of attention submodules: a) temporal\nattention, b) spatial attention, c) detection attention. In fact,\nthe three attention modules all follow the same encoder\nlayer paradigm [52], except that the query and key are used\nas input. We first briefly introduce the tokens involved in\nthe three submodules, and then elaborate on the detailed\ndifferences of each module.\nToken Mechanism. The token embeddings are delivered as\ninput to the attention module with a 1D sequence format. In\nthe STD module, tokens can be classified into the following\nthree categories according to the source of embedding:\n• detection token: embedding of a detection at frame t Oi ∈\nOt, where Oi ∈ Rd, i ∈ {1, . . . , N}.\n• track token: embedding of a history tracking box for a\nsingle tracked object at frame t−1 over the past k frames\nHp\nj ∈ Ht−1:t−k\nj = {Ht−1\nj , . . . , Ht−k\nj }, where Hp\nj ∈ Rd,\nj ∈ {1, . . . , M}, p ∈ {t − 1, . . . , t− k}.\n• trajectory token: learnable embedding of a tracked object\nat frame t−1. It aggregates the historical tracking boxes’\nfeatures of a single object, whose state at the output\nof the attention module implicitly serves as the tracked\nobject’s tracking box embedding at frame t. We define\nthe Learnable Trajectory Token of the j-th tracked object\nas QLTT (j) ∈ Rd.\nTemporal Attention. The long-range motion information\nof a tracked object implicitly indicates its motion trend. For\neach tracked object, we independently execute the interaction\nof temporal motion features over the previous k frames.\nTherefore, considering that each tracking target has a temporal\nattention layer, the temporal attention module dynamically\nadjusts the number of parallel temporal attention layers along\nwith the tracking target. As illustrated in Figure 4, we construct\nthe encoded query and key vectors for the intra-track temporal\nattention of the j-th tracked object at frame t − 1 in a self-\nattention manner as follows:\n• query: embeddings of a tracklet token and k\ntrack tokens of the target in the past k frames\n{QLTT (j), Ht−1\nj , . . . , Ht−k\nj }, where j ∈ {1, . . . , M}.\n• key: similar to the query.\nIn particular, position embeddings with sinusoidal format\n[53] are added to the above embeddings to retain temporal\ninformation. DIn contrast to the standard Transformer, we only\nadd them once to the relevant temporal embeddings of the fea-\nture extraction module output. The inputs of temporal attention\nbecome time-dependent by adding the position embeddings to\nthe temporal embeddings, which is essential for STD mining\nmovement trends.\nSpatial Attention. The interaction of spatial features re-\nflects social interaction. In social interactions, targets’ deci-\nsions frequently follow logical social norms. Regarding spatial\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 7\ninformation, we exploit the interaction mechanism of self-\nattention to measure the relative spatial position between the\ntargets and their neighbors, which assists them in making\nmovement decisions. Thus, the query and key vectors for the\ninter-track spatial attention module are as follows:\n• query: embeddings of the tracklet tokens of all the targets\nat frames t − 1 {QLTT (1), . . . , QLTT (M)}.\n• key: similar to the query.\nDetection Attention. The detections provide the potential\ndistribution position of targets in the current frame, which can\nbe used as prior knowledge to constrain the possible position\nof the tracking targets in the current frame. In the detection\nattention module, we focus on the relative spatial position\nbetween the tracked objects’ prediction and the detections.\nUnlike the spatial attention module, we aim to narrow the\nfeature representation of matched pairs in the embedding space\nby measuring relative positions and performing the interaction\nbetween detection and tracking in a cross-attention manner. Its\nquery and key vectors are as follows:\n• query: embeddings of the tracklet tokens of all the targets\nat frames t − 1 {QLTT (1), . . . , QLTT (M)}.\n• key: embeddings of all the detection tokens at frame t\n{O1, . . . , ON }.\nAs demonstrated in Figure 4, we interleave the temporal\nattention and spatial attention modules by L times in the STD\nmodule to aggregate the spatial-temporal motion features of\nthe tracklets. Then, we iterate the detection attention module\nL times to make the aggregated motion features interact with\nthe detection features of the current frame. After feature\ninteraction, we have M trajectory tokens for tracked objects\n{QLTT (1), . . . , QLTT (M)}.\nD. Motion Prediction Head\nThe motion prediction task aims to forecast the position of\neach tracked object in the current frame. However, inspired\nby [54], [55], directly training a model to adapt to shapes of\nvarious objects is a challenging task, which results in poor\nperformance in precise localization. In contrast, predicting the\ncandidate box offset is simpler. Thus, STDFormer takes the\nposition of the target in the previous frame as prior information\nand predicts the change in its position in the current frame\ninstead of directly regressing the position of the target in the\ncurrent frame.\nTo improve the learning ability of the model, we assess the\nobject motion distribution using several benchmark datasets in\nAppendix A and finally propose two motion prediction head\nvariants based on the data distribution characteristics:\n1) Linear Motion Prediction Head (LMPH): This variant\ndirectly predicts the displacement δ of each tracked\nobject between the adjacent frames, which is suitable\nfor datasets with simple motion patterns and small offset\nvariance;\n2) Exponential Motion Prediction Head (EMPH): This vari-\nant scales the bounding box of the tracked object in the\nprevious frame by predicting an exponential adjustment\nfactor ζ to obtain the tracking box in the current frame,\n(a) Linear Motion Prediction Head (LMPH)\n(b) Exponential Motion Prediction Head (EMPH)\nFig. 5. Motion prediction head , where ⊖ represents feature addition and\n⊕ represents vector addition. There are two variants of this module: (a)\nLinear Motion Prediction Head (LMPH): This variant subtracts the trajectory\ntoken features after the feature interaction module and the tracking box\nfeatures of the previous frame after the feature extraction module. Then,\nthe subtracted features are passed to a five-layer MLP, which outputs the\ndisplacement δ of each tracked target between adjacent frames. Finally, the\ndisplacement is added to the tracking box of the previous frame to obtain\nthe tracking box prediction of each tracked target in the current frame. (b)\nExponential Motion Prediction Head (EMPH): This variant first concatenates\nthe trajectory token features after the feature interaction module and the\ntracking box features of the previous frame after feature extraction module.\nThen, the concatenated features are passed to a five-layer MLP, which outputs\nthe exponential adjustment factor ζ of each tracked target between adjacent\nframes. Finally, the exponential adjustment factor is exponentially processed,\nand multiplied by the tracking box of the previous frame to obtain the tracking\nbox prediction of each tracked target in the current frame.\nwhich is suitable for datasets with complex motion\npatterns and large offset variance.\nThe quantitative results in Section IV-C further verify the\nsuperiority of the two variants in different scenarios. More\ndetails about object motion analysis on benchmark datasets\ncan be found in Appendix A. The detailed motion prediction\nhead architecture is shown in Figure 5.\nLinear Motion Prediction Head. As depicted in Figure\n5, to learn the displacement, we first utilize this simple\nsubtraction between trajectory token features from the feature\ninteraction module and the motion features from feature ex-\ntraction module for each tracked object as the displacement\nfeature Bj:\nBj = QLTT (j) − Ht−1\nj , (7)\nwhere QLTT (j) is the implicit motion feature of tracked object\nj at frame t and Ht−1\nj is the motion feature of tracked object\nj at frame t − 1.\nAfter obtaining the displacement features, we propose a\nfive-layer MLP with a nonlinear operator that takes the dis-\nplacement feature Bj as input and produces a scalar value\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 8\nδj that represents the displacement of the tracking target j\nbetween frame t − 1 and frame t:\nδj = MLP (Bj). (8)\nFinally, to obtain the position prediction eTt\nj of tracked object\nj at frame t, we perform simple additive post-processing:\neTt\nj = Tt−1\nj + δj. (9)\nExponential Motion Prediction Head. As shown in Figure\n5(b), the main difference from LMPH is the processing of\ninput and output. Instead of subtracting, we first concatenate\ntrajectory token features from the feature interaction module\nand the motion features from the feature extraction module\nfor each tracked object. Then, the concatenated features are\nalso fed to a five-layer MLP and MLP outputs a scalar as\nan exponential adjustment factor ζ. Finally, an exponential\noperation is performed on this factor to obtain a scaling factor\nthat is multiplied by the bounding box of the previous frame to\nobtain the position of the tracking target in the current frame.\nThe specific calculation process is as follows:\nζ = MLP (concat(QLTT (j), Ht−1\nj )), (10)\neTt\nj = exp(ζ) · Tt−1\nj (11)\nBox Loss. We describe motion prediction as a tracking box\nregression task. L1 loss, the most popular regression loss, is\nsensitive to bounding box size. To alleviate this problem, we\nemploy a linear combination of smooth L1 loss [56] and scale-\ninvariant generalized IoU loss [57] to optimize the prediction\nof tracking boxes:\nLbox(Tt\nj , eTt\nj ) = λl1Ll1(Tt\nj , eTt\nj ) + λiouLiou(Tt\nj , eTt\nj ), (12)\nwhere λl1, λiou ∈ R are hyperparameters, eTt\nj is the estimated\ntracking box of tracked object j at frame t, Tt\nj is the ground\ntruth of the tracking box of tracked object j at frame t, Ll1 is\nthe smooth L1 loss and Liou is the GIoU loss.\nE. Affinity Calculation Head\nThe goal of the affinity calculation head is to compute\nthe pairwise similarity scores of detected and tracked objects\nfor data association. The process of this module is shown\nin Figure 6. Given the features of N detections from the\nfeature extraction module and the trajectory token features of\nM tracked objects from the feature interaction module, we\ncalculate the affinity matrix at frame t At\nN:M as the inner\nproduct between each pair of detection features Oi and tracked\nobject trajectory token features QLTT (j):\nAt\nN:M (i, j) = Oi · QLTT (j), (13)\nwhere i ∈ {1, . . . , N}, j ∈ {1, . . . , M}.\nBidirectional Matching. A well-performing affinity matrix\nshould satisfy bidirectional optimal matching. In other words,\nwhen detection-to-track or track-to-detection pairs identify\nthe best match, symmetrical track-to-detection or detection-\nto-track scores should be the highest. As a result, we use\nthe dual-softmax proposed by [58], which modifies the ini-\ntial affinity matrix by introducing a prior probability matrix\nePprior ∈ RN×M generated in the cross direction. We can filter\nout the challenging cases with a high detection-to-track affinity\nscore but a low track-to-detection affinity score by acalculating\nthe dot product between the prior probability matrix and the\ninitial affinity matrix.\nSpecifically, we successively apply softmax on the two\ndimensions of At\nN:M to obtain the probability eP ∈ RN×M\nof soft mutual nearest neighbor matching:\nePprior = softmax(At\nN:M /τ, dim= 1), (14)\neP(i, j) = softmax( ePprior · At\nN:M , dim= 0)ij (15)\nwhere i ∈ {1, . . . , N}, j ∈ {1, . . . , M} and τ is a temperature\nparameter.\nLabel Assignment. Unlike box loss, the ground truth of\nthe affinity matrix cannot be given directly or indirectly by the\ndataset when computing affinity loss, because the performance\nof the chosen detector greatly influences the detection directly\ntied to the affinity matrix. For this reason, we refer to [59]\nand use the Hungarian algorithm to find an optimal bipartite\nmatching between predicted detection and ground truth tracked\nobjects for label assignment. In contrast to [59], our approach\ntakes only the box loss into account when calculating the\nmatching cost and ignores the categorization loss. For more\nimplementation details, please refer to [59].\nAffinity Loss. After the label assignment, we obtain the\nground truth affinity matrix. Each row and column of the\nmatrix is either a one-hot vector, or an all-zero vector, where\n1 to indicates a matching detection-track pair at that location\nand 0 indicates a mismatch. We can also treat the ground truth\naffinity matrix as a probability matrix, denoted as P.\nDuring training, we set a max objects hyperparameter, Z,\nto achieve parallel training. When the number of tracked or\ndetected targets as input is less than the max objects, we pad\nthe input with 0. In other words, the dimension of our output\nmust be Z ×Z. When M < Zand/or N < Z, the output will\nbe extended to dimension Z×Z by padding with 0. Obviously,\nthe number of zeros in P is much greater than the number of\nones. To address the imbalance between positive and negative\nsamples, we use focal loss [60] as affinity loss:\nLaff ( eP(i, j)) = −α(1 − eP(i, j))γ log( eP(i, j)), (16)\nwhere α and γ are hyperparameters.\nF . Training\nTo summarize the above, whether focusing on motion\nprediction or affinity calculation, the primary task is to learn\nthe motion feature representation of tracked objects in the\ncurrent frame. However, as mentioned in Section III-E, the\nperformance of the affinity calculation head is highly depen-\ndent on the prediction of the detector. Missed, wrong, and\nrough detections cause serious damage to the motion feature\nrepresentation due to the feature interaction. In this regard, we\nbelieve that the ideal situation of the current frame is that no\nnew targets appear, no old targets are lost, and each tracked\ntarget can find its accurate localization in the current scene.\nMotivated by this assumption, we introduce an auxiliary task\nto discover more trustworthy motion feature representations.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 9\nFig. 6. Affinity calculation head. Given the trajectory token features QLTT\nafter the feature interaction module and the detection box features Ot after\nthe feature extraction module, this module first calculates the affinity matrix\nAt\nN:M between the detection and tracks based on the inner product. The\naffinity matrix At\nN:M is then fed to the bidirectional matching submodule to\nobtain a detection and tracking association probability matrix eP.\nThe most intuitive approach is to follow the STD pipeline\nabove, using the ground truth tracked boxes Tt at frame\nt instead of detections Dt as input, and retrain the STD.\nHowever, as shown in Figure 2, instead of training twice, we\nachieve parallel training by adding an auxiliary branch.\nSpecifically, the auxiliary task shares the feature processing\nflow of detection in STD and outputs a track-to-track affinity\nmatrix whose ground truth is the identity matrix. We define\nthis track-to-track affinity matrix at frame t as At\nM:M :\nAt\nM:M (i, j) = QLTT (i) · Vj, (17)\nwhere Vj is the embedding of the j-th ground truth tracked\nboxes Tt\nj at frame t from the feature extraction module.\nIn fact, our auxiliary task subtly introduces the idea of\ncontrastive learning. Contrastive learning requires that each\nsample has one positive sample and several negative samples.\nLike [50], we take the aggregated historical motion infor-\nmation of each tracked object as a sample, and we hope\nto make the feature representation of the sample close to\nthe motion feature representation of the corresponding target\nin the current frame through contrastive learning. Therefore,\nwe regard the ground truth position of the tracked object in\nthe current frame as a positive sample, and the ground truth\nposition of the rest of the tracked objects in the current frame\nas a negative sample. Obviously, it is easy to meet the above\nrequirements in the setting of our auxiliary task. Furthermore,\n[50] pointed out that such a sampling strategy can effectively\nnot only narrow the feature representation of similar samples,\nbut also push the samples away from the negative sample\npoints, avoiding location conflicts and eventually effectively\nintroducing security considerations.\nContrastive Loss. After obtaining the track-to-track affinity\nmatrix at frame t as At\nM:M , we also perform the dual-softmax\noperation (Equation 14, 15) on it. Unlike Equation 16, the\naffinity loss of our auxiliary task replaces the focal loss with\ncross entropy loss which is called Dual Softmax Loss (DSL)\nin [58]. It can be regarded as a variant of InfoNCE Loss, a\ncontrastive loss that is frequently employed. This contrastive\nloss optimization seeks to learn well-performing historical\naggregated representations by maximizing the mutual informa-\ntion between historical aggregated representations and ground\ntruth motion representations. Specifically, the contrastive loss\nof the auxiliary task is calculated as follows:\nP r(i, j) = softmax(At\nM:M /τ, dim= 1), (18)\nLaux = − 1\nM\nMX\ni\nlog exp(Q(\nLTT i) · V +\ni · P r(i, i))\nPM\nj=1 exp(Q(\nLTT i) · Vj · P r(i, j))\n,\n(19)\nwhere P ris the prior matrix of At\nM:M (same as Equation 14),\nτ is a temperature hyperparameter.\nTotal Loss. We jointly train the motion prediction, affinity\ncalculation and auxiliary task branches by adding the losses\n(i.e., Equations 12,16,19) together. In particular, we leverage\nthe uncertainty loss proposed in [61] to automatically balance\nmultitask learning:\nLtask = w1Lbox + w2Laff , (20)\nLtotal = 1\n2( 1\new3\nLtask + 1\new4\nLaux + w3 + w4), (21)\nwhere w1 and w2 are fixed hyperparameters that balance\nthe motion prediction task and the affinity calculation task,\nrespectively, and w3 and w4 are learnable parameters that\nbalance the above two main tasks and the auxiliary task.\nFinally, it should be emphasized that the auxiliary task is only\nused during training.\nG. Inference\nDuring inference, we process the video stream online in\na frame-by-frame manner. We construct a temporal history\nbuffer of T frames to store each tracked object’s tracking\nboxes. Following [4], we divide all detection boxes into high\nscore detection boxes and low score detection boxes according\nto the detection score threshold θdet. For each individual frame\nt, given all the high score detections for the current frame and\na sequence of the tracked objects’ historical tracking boxes\nfor the previous frame, STD takes them as input for inference\nand outputs an affinity matrix with probability format (after\ndual softmax ) as well as the tracked objects’ tracking boxes\nprediction in the current frame.\nData Association. After network inference, we perform\nassociation by using the network outputs. We follow the\nstandard online tracking paradigm to associate boxes. In the\nfirst frame, we first initialize some tracklets based on the high\nscore detection boxes. In the following frames, we link the\ndetection boxes to the existing tracklets according to a step-\nby-step association strategy as follows:\n1) high score detection boxes are associated with all ex-\nisting tracklets, including tracked, lost, and unconfirmed\ntracklets;\n2) low score detection boxes are associated with the un-\nmatched tracklets in the previous step;\n3) the high score detection boxes are associated with all\nthe remaining unmatched tracklets.\nIn each step of the above association process, we obtain a\ncost matrix C by performing a weighted sum operation on the\ncenter point distance cost Ccdist, spatial overlap rate (GIoU)\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 10\ncost Cgiou and affinity cost Caff between the prediction boxes\nand the detection boxes. The calculation process is as follows:\nC = λcdistCcdist + λgiouCgiou + λaff Caff (22)\nWe found that a large amount of noise and false detections\nin low score detection boxes can seriously damage the track-\ning performance through ablation experiments. Therefore, we\nonly feed high score detection boxes to STDFormer, which\ncauses the model to not output an affinity matrix between the\nprediction boxes and the low score detection boxes and step\n2 of the above association does not consider the affinity cost.\nAfter obtaining the cost matrix, we feed it to the Hungarian\nalgorithm and filter matching pairs that are far away according\nto a fine-grained spatial overlap rate threshold design.\nIn addition, determining the birth and death of a trajectory\nis also an important part of MOT. When a detection box fails\nto match an existing tracked object, we can assume that a\nnew target has entered the scene and constructing a unique\nidentification is necessary. To avoid false positive detection\nboxes, we initialize a new trajectory only if the object exists\nfor more than three consecutive frames (except the first frame).\nOn the other hand, when the tracking object does not find\na matching detection box in the current scene, we need to\ndistinguish whether the object is lost or has left the current\nscene. We propose a boundary judgment method. Specifically,\nwhen the center point of the predicted tracking box is less\nthan x pixels from the image frame boundary, we consider the\nobject to have already left the current scene and destroy the\ncorresponding identity of the tracking object. In contrast, when\nthe object is far from the image frame boundary, we consider\nthe object to be only temporarily lost. Furthermore, when the\nobject is lost in fewer than s frames, we fill the tracklet of\nthe lost tracked object with the predicted tracking box, and\nreactivate this tracked object if the object reappears. When\nthe object is missing more than s frames, we also consider\nthe tracking object to have left the current scene and destroy\nits identity.\nIV. EXPERIMENTS\nIn this section, to assess the performance of the proposed\napproach, we describe experiments conducted on MOTChal-\nlenge. First, we introduce evaluation datasets, evaluation met-\nrics and implementation details. Then, we compare the pro-\nposed method with state-of-the-art methods and show quan-\ntitative results on MOTChallenge. In addition, we provide a\nqualitative analysis of the results. Finally, we demonstrate the\neffectiveness of each module through ablative studies.\nA. Datasets and Metrics\nDatasets. We evaluate STDFormer on the MOT17 [62],\nMOT20 [63] and DanceTrack [64] datasets in accordance with\nthe ”private detection” protocol. All three datasets are related\nto pedestrian tracking. The difference is that the pedestrian\nmotion patterns of MOT17 and MOT20 are relatively simple\nand close to linear motion, while the motion of DanceTrack\nis complex and highly nonlinear. The details of these datasets\nare as follows:\n1) MOT17: This dataset includes seven scenes of indoor\nand outdoor public places with pedestrians. The videos\nof each scene are divided into two segments for training\nand testing. Specifically, this dataset contains 14 video\nsequences, 7 of which are used for training and 7 for\ntesting. The training datasets consist of 15948 frames\nwith a total of 1638 identities and 336891 labeled boxes.\nThe test datasets consist of 17757 frames with a total of\n2355 identities and 564228 labeled boxes.\n2) MOT20: This dataset consists of 8 video sequences from\n3 different scenes, half of which are used as the training\ndataset and half as the testing dataset. The training\ndataset consists of 15948 frames with a total of 1638\nidentities and 1336920 labeled boxes. The test dataset\nconsists of 4479 frames with a total of 1501 identities\nand 765465 labeled boxes. Obviously, compared with\nMOT17, the pedestrian density in the scene of MOT20\ndatasets is higher, and the average crowd density reaches\n246 pedestrians per frame. All sequences were shot from\nabove.\n3) DanceTrack: This dataset includes 100 videos (40 train-\ning videos, 25 validation videos and 35 test videos)\ncovering group dance, kung fu, gymnastics, and other\nactivities. It contains 990 unique instances with an aver-\nage length of 52.9 s, 105k frames and 877k high-quality\nbounding boxes by 20 FPS annotation. In addition,\nthe targets in DanceTrack are very clear and in close\nrange, so object detection generally does not limit the\nalgorithm, and emphasis is placed on evaluating the data\nassociation performance of the algorithm. Furthermore,\nsince the targets in DanceTrack have similar or even\nidentical appearances and there are a large number of\nocclusions, position interleaving, and complex nonlinear\nmotion patterns among the targets, the dataset encour-\nages the algorithm to mine matching cues other than\nappearance, such as motion trajectories.\nNote that MOT17 and MOT20 are both benchmark datasets\nfor MOTChallenge and the most commonly used benchmarks\nfor multi-object tracking. Both only have training and testing\nsets, while validation sets are not available. Therefore, in the\nablation experiments, we follow the experimental setup in most\nof the literature [1], [4], [5], [13]. We divide each video in the\nMOT17 training set into two equal parts. The first half is used\nfor training, and the second half is used for validation. In\naddition, we also evaluate STDFormer on the VisDrone2019\n[65] dataset to verify the effectiveness of the proposed method\nin UA V videos. More details about STDFormer in unmanned\naerial vehicle videos can be found in Appendix B.\nMetrics. We use the CLEAR MOT Metrics [66], [67]\nand HOTA [68] to quantitatively evaluate the overall tracking\naccuracy. All the metrics are listed as follows:\n• MOTA(↑): Multi-Object tracking accuracy.\n• IDF1(↑): ID F1 score.\n• HOTA(↑): Higher order tracking accuracy.\n• MT(↑): Mostly tracked targets.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 11\n• ML(↓): Mostly lost targets.\n• FP(↓): The total number of false positives.\n• FN(↓): The total number of false negatives (missed tar-\ngets).\n• AssA(↑): Association accuracy.\n• ID Sw.(↑): Number of identity switches.\n• Frag(↑): The total number of times a trajectory is frag-\nmented.\n(↑) means that the higher the score is, the better the\nperformance. (↓) means that the lower the score is, the better\nthe performance.\nB. Implementation Details\nWe implemented our proposed method in PyTorch and\ntrained it on a server equipped with 4 NVIDIA TITAN Xp\nGPUs, Intel(R) Core(TM) i7-6800K CPU, and 32 GB memory.\nFor fair comparison and high-speed inference, we inferred on\na single Tesla V100 GPU and a single Tesla A100 GPU, which\nare widely used in baseline methods [4], [34], [38], [40].\nOn the MOT17 test set, we achieved approximate real-time\ntracking on a single Tesla V100 GPU with 24 FPS running\nspeed and 2860 MB GPU memory. We achieved up to 28 FPS\nrunning speed on a single Tesla A100 GPU. For detection, we\nadopted YOLOX-X [51] as the detector and the input image\nsize was 1440 × 800. YOLOX-X is widely used by several\nrecent state-of-the-art MOT algorithms based on the tracking-\nby-detection paradigm [4], [13]–[15] because of its balance\nbetween accuracy and speed. The training and inference of the\ndetector stand on the giants’ shoulders, which exactly adhere\nto the strategy of [14]. Next, we focus on the implementation\ndetails of STDFormer.\nTraining schemes. We trained STDFormer using AdamW\noptimizer [69] with an initial learning rate of 10−3 and weight\ndecay of 10−1. For better optimization results, we employed\na cosine annealing scheduler [70] with warm-up and restart\nfor AdamW [69]. Due to the sparseness and scale difference\nof these three datasets, we set different training epochs and\nbatch sizes. For MOT17, the batch size was set to 128 and the\ntraining epoch was set to 1500 with a total training time of\n9h. For MOT20, the batch size was set to 64 and the training\nepoch was set to 750 with a total training time of 26h. For\nDanceTrack, the batch size was set to 512 and the training\nepoch was set to 1270 with a total training time of 35h.\nHyperparameter setting. In this work, we set k = 15 ,\nr = 2500, d = 128 and L = 3 for model building. We let\nθdet = 0.6, λcdist = 0.1, λgiou = 2.0, λaff = 2.0 and x =\n50 for data association. In addition, the hyperparameters for\nthe construction of the box loss were: λl1 = 5 and λiou =\n2. The hyperparameters for the construction of the affinity\nloss were: α = 0.25 and γ = 2.0. The hyperparameters for\nthe construction of the total loss were: w1 = 1.0 and w2 =\n1.0. The temperature hyperparameters for dual-softmax was:\nτ = 1000. The effect of the hyperparameter k, L and x were\nconducted in the ablative studies.\nC. Quantitative Results\nWe compared the proposed method with several state-of-\nthe-art methods on the MOT17, MOT20 and DanceTrack test\nsets. Table I, Table II and Table III present an overview of the\ncomparative results on these three datasets. To better prove\nthe effectiveness of our method’s core design, we grouped\nand compared the state-of-the-art methods according to the\nmethod correlation. Considering that our proposed method is\na motion-based method that takes full advantage of the Trans-\nformer’s superiority and extracts more discriminative motion\nfeatures through comparative learning, we divide the compared\nmethods into four groups: 1) benchmark methods, which are\nthe SOTA method released on the benchmark dataset in recent\nyears except for the methods mentioned in the following three\ngroups; 2) motion-based tracking methods, which only rely\non the motion features of objects and discard appearance\ncues; 3) Transformer-based tracking methods, which utilize a\nregular Transformer framework; and 4) contrastive learning-\nbased tracking methods, which use contrastive learning to\nlearn more discriminative features of the tracked objects. Note\nthat although the methods in the first group are not closely\nrelated to our method, they are all representative and often\nused for comparison in MOT. To prove the competitiveness\nof our method, this group of comparison is indispensable.\nIn addition, since DanceTrack is an MOT benchmark dataset\nreleased in 2022, there were fewer methods available for\ncomparison.\nFor a more comprehensive comparison, we tested and\ncompare two different configurations of STDFormer on\neach benchmark dataset: STDFormer-LMPH and STDFormer-\nEMPH. Their main difference was the motion prediction head.\nThe former uses the Linear Motion Prediction Head (LMPH),\nwhile the latter uses the Exponential Motion Prediction\nHead (EMPH). The performance of STDFormer-LMPH and\nSTDFormer-EMPH was comparable on MOT17 and MOT20.\nIn terms of the three main evaluation metrics of MOTA,\nIDF1 and HOTA, the overall performance of STDFormer-\nLMPH was slightly better than that of STDFormer-EMPH.\nOn DanceTrack, STDFormer-EMPH was significantly better\nthan STDFormer-LMPH based on all evaluation metrics. This\nwas mainly due to the difference in the offset distribution\nand the exponential adjustment factor distribution on the three\nbenchmark datasets. More detailed analysis is provided in\nAppendix A. To better compare with other methods, we chose\nSTDFormer-LMPH as the comparison method on MOT17 and\nMOT20, and we chose STDFormer-EMPH as the comparison\nmethod on DanceTrack. The following will be referred to as\nSTDFormer.\nThe quantitative results show that STDFormer achieved\nstate-of-the-art performance on the DanceTrack test set, while\nSTDFormer achieved comparable performance to that other\nstate-of-the-art methods on the MOT17 and MOT20 test sets.\nThe significant gains of STDFormer on DanceTrack demon-\nstrated its superiority in handling complex nonlinear motion\nestimation. Furthermore, the analysis of the group comparison\nwas as follows:\n1) Benchmark tracking: Compared with benchmark\nmethods on three benchmark datasets, the proposed\nmethod achieved state-of-the-art performance based on\nmost evaluation metrics. In terms of the three main\nevaluation metrics of MOTA, IDF1 and HOTA, the pro-\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 12\nTABLE I\nCOMPARISON OF OUR METHOD WITH STATE -OF-THE -ART MOT ALGORITHMS UNDER THE ”PRIVATE DETECTOR ” PROTOCOL ON THE MOT17 TEST SET .\nTHE BEST RESULTS OF THE FOUR GROUPS OF METHODS ARE MARKED IN YELLOW , GREEN , BLUE AND RED , RESPECTIVELY .\nMethods MOTA↑ IDF1↑ HOTA↑ MT↑ ML↓ FP↓ FN↓ AssA↑ ID Sw.↓ Frag↓\nTubeTK [71] 63.0 58.6 48.0 31.2% 19.9% 27060 177483 45.1 4137 5727\nCenterTrack [72] 67.8 64.7 52.2 34.6% 24.6% 18498 160332 51.0 3039 6102\nTraDes [6] 69.1 63.9 52.7 36.4% 21.5% 20892 150060 50.8 3555 4833\nMAT [73] 69.5 63.1 53.8 43.8% 18.9% 30660 138741 51.4 2844 3726\nSOTMOT [74] 71.0 71.9 - 42.7% 15.3% 39537 118983 - 5184 -\nGSDT [75] 73.2 66.5 - 41.7% 17.5% 26397 120666 - 3891 -\nFairMOT [5] 73.7 72.3 59.3 43.2% 17.3% 27507 117477 58.0 3303 8073\nCSTrack [76] 74.9 72.6 59.3 41.5% 17.5% 23847 114303 57.9 3567 7668\nSGT [77] 76.3 72.4 60.6 47.9% 11.7% 25983 102984 58.6 4578 6960\nSTDFormer-LMPH 78.4 73.1 60.9 49.6% 12.7% 29514 87132 58.4 5091 5853\nBenchmark\nSTDFormer-EMPH 78.8 71.5 59.9 49.7% 13.1% 24627 89862 56.6 4998 5379\nByteTrack [4] 80.3 77.7 63.1 53.2% 14.5% 25491 83721 62.0 2196 2277\nOCSORT [13] 78.0 77.5 63.2 41.0% 20.9% 15129 107055 63.2 1950 2040\nBoT-SORT [15] 80.6 79.5 64.6 - - 22524 85398 - 1257 -\nSTDFormer-LMPH 78.4 73.1 60.9 49.6% 12.7% 29514 87132 58.4 5091 5853\nMotion-based\nSTDFormer-EMPH 78.8 71.5 59.9 49.7% 13.1% 24627 89862 56.6 4998 5379\nTransTrack [34] 75.2 63.5 54.1 55.3% 10.2% 50517 86442 47.6 3603 4872\nTrackFormer [37] 74.1 68.0 57.3 47.3% 10.4% 34602 108777 54.1 2829 4221\nTransCenter [35] 73.2 62.2 54.5 40.8% 18.5% 23112 123738 49.7 4614 9519\nMOTR [38] 71.9 68.4 57.2 38.9% 24.1% 21123 135561 55.8 2115 3897\nMO3TR-PIQ [39] 77.6 72.9 60.3 - - 21045 102531 - 2847 -\nMeMOT [40] 72.5 69.0 56.9 43.8% 18.0% 37221 115248 55.2 2724 -\nGTR [78] 75.3 71.5 59.1 - - 26793 109854 57.0 2859 -\nTR-MOT [36] 76.5 72.6 59.7 - - - - - - -\nSTC [79] 75.8 70.9 59.8 53.6% 7.6% 44952 87039 - 4533 -\nSTDFormer-LMPH 78.4 73.1 60.9 49.6% 12.7% 29514 87132 58.4 5091 5853\nTransformer-based\nSTDFormer-EMPH 78.8 71.5 59.9 49.7% 13.1% 24627 89862 56.6 4998 5379\nQDTrack [49] 68.7 66.3 53.9 40.6% 21.9% 26589 146643 52.7 3378 8091\nSemi-TCL [48] 73.3 73.2 59.8 41.3% 18.7% 22944 124980 59.4 2790 8010\nSTDFormer-LMPH 78.4 73.1 60.9 49.6% 12.7% 29514 87132 58.4 5091 5853Contrastive learning-based\nSTDFormer-EMPH 78.8 71.5 59.9 49.7% 13.1% 24627 89862 56.6 4998 5379\nTABLE II\nCOMPARISON OF OUR METHOD WITH STATE -OF-THE -ART MOT ALGORITHMS UNDER THE ”PRIVATE DETECTOR ” PROTOCOL ON THE MOT20 TEST SET .\nTHE BEST RESULTS OF THE FOUR GROUPS OF METHODS ARE MARKED IN YELLOW , GREEN , BLUE AND RED , RESPECTIVELY .\nMethods MOTA↑ IDF1↑ HOTA↑ MT↑ ML↓ FP↓ FN↓ AssA↑ ID Sw.↓ Frag↓\nMLT [80] 48.9 54.6 43.2 30.9% 22.1% 45660 216803 44.1 2187 3067\nFairMOT [5] 61.8 67.3 54.6 68.8% 7.6% 103440 88901 54.7 5243 7874\nCSTrack [76] 66.6 68.6 54.0 50.4% 15.5% 25404 144358 54.0 3196 7632\nGSDT [75] 67.1 67.5 53.6 53.1% 13.2% 31913 135409 52.7 3131 9875\nSOTMOT [74] 68.6 71.4 - 64.9% 9.7% 57064 101154 - 4209 -\nSGT [77] 72.8 70.5 56.9 64.3% 12.6% 25165 112897 55.3 2649 5486\nSTDFormer-LMPH 76.2 72.1 60.2 71.5% 8.6% 32336 87542 57.7 3166 5245\nBenchmark\nSTDFormer-EMPH 75.8 72.3 60.0 67.4% 12.0% 22056 100991 58.0 2329 4228\nByteTrack [4] 77.8 75.2 61.3 69.2% 9.5% 26249 87594 59.6 1223 1460\nOCSORT [13] 75.7 76.3 62.4 45.5% 12.9% 19067 105894 62.5 942 1086\nBoT-SORT [15] 77.7 76.3 62.6 - - 22521 86037 - 1212 -\nSTDFormer-LMPH 76.2 72.1 60.2 71.5% 8.6% 32336 87542 57.7 3166 5245\nMotion-based\nSTDFormer-EMPH 75.8 72.3 60.0 67.4% 12.0% 22056 100991 58.0 2329 4228\nTransTrack [34] 65.0 59.4 48.9 50.1% 13.4% 27191 150197 45.2 3608 11352\nTrackFormer [37] 68.6 65.7 54.7 53.6% 14.6% 20348 140373 53.0 1532 2474\nTransCenter [35] 58.5 49.6 43.5 48.6% 14.9% 64217 146019 37.0 4695 9581\nMO3TR-PIQ [39] 72.3 69.0 57.3 - - 12738 128439 - 2200 -\nMeMOT [40] 63.7 66.1 54.1 57.5% 14.3% 47882 137983 55.0 1938 -\nTR-MOT [36] 67.1 59.1 50.4 - - - - - - -\nSTC [79] 73.0 67.5 56.3 67.0% 11.8% 30215 107701 - 2011 -\nSTDFormer-LMPH 76.2 72.1 60.2 71.5% 8.6% 32336 87542 57.7 3166 5245\nTransformer-based\nSTDFormer-EMPH 75.8 72.3 60.0 67.4% 12.0% 22056 100991 58.0 2329 4228\nSemi-TCL [48] 65.2 70.1 55.3 61.3% 10.5% 61209 114709 56.3 4139 8508\nSTDFormer-LMPH 76.2 72.1 60.2 71.5% 8.6% 32336 87542 57.7 3166 5245Contrastive learning-based\nSTDFormer-EMPH 75.8 72.3 60.0 67.4% 12.0% 22056 100991 58.0 2329 4228\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 13\nTABLE III\nCOMPARISON OF OUR METHOD WITH STATE -OF-THE -ART MOT ALGORITHMS ON THE DANCE TRACK TEST SET . THE BEST RESULTS OF THE FOUR\nGROUPS OF METHODS ARE MARKED IN YELLOW , GREEN , BLUE AND RED , RESPECTIVELY .\nMethods MOTA↑ IDF1↑ HOTA↑ DetA↑ AssA↑\nCenterTrack [72] 86.8 35.7 41.8 78.1 22.6\nFairMOT [5] 82.2 40.8 39.7 66.7 23.8\nTraDes [6] 86.2 41.2 43.3 74.5 25.4\nSTDFormer-LMPH 91.6 55.0 52.8 80.4 34.8\nBenchmark\nSTDFormer-EMPH 91.7 60.5 57.8 80.5 41.7\nByteTrack [4] 89.6 53.9 47.7 71.0 32.1\nOCSORT [13] 89.4 54.2 55.1 80.3 38.0\nSTDFormer-LMPH 91.6 55.0 52.8 80.4 34.8Motion-based\nSTDFormer-EMPH 91.7 60.5 57.8 80.5 41.7\nTransTrack [34] 88.4 45.2 45.5 75.9 27.5\nMOTR [38] 79.7 51.5 54.2 73.5 40.2\nSTDFormer-LMPH 91.6 55.0 52.8 80.4 34.8Transformer-based\nSTDFormer-EMPH 91.7 60.5 57.8 80.5 41.7\nQDTrack [49] 83.0 44.8 45.7 72.1 29.2\nSTDFormer-LMPH 91.6 55.0 52.8 80.4 34.8Contrastive learning-based\nSTDFormer-EMPH 91.7 60.5 57.8 80.5 41.7\nposed method achieved significant improvement com-\npared with the baseline method. For example, 2.1%,\n0.5% and 0.3% improvements for MOT17, 3.4%, 0.7%\nand 3.3% improvements for MOT20, and 4.9%, 13.8%\nand 9.5% improvements for DanceTrack. The better per-\nformance demonstrated the effectiveness of the proposed\nmethod in MOT.\n2) Motion-based tracking: Compared with motion-based\nmethods, the proposed method achieved state-of-the-\nart performance on DanceTrack, while achieving per-\nformance comparable to that of other state-of-the-art\nalgorithms on MOT17 and MOT20. On DanceTrack,\nthe proposed method achieved the best performance\nbased on all evaluation metrics, especially in the three\nimportant association metrics of IDF1, HOTA and AssA,\nwhich were improved by 6.3%, 2.7% and 3.7%, respec-\ntively. This demonstrated that the proposed method sig-\nnificantly outperformed baselines on complex nonlinear\nobject motion modeling and achieved robust associa-\ntion under similar appearance, occlusion, or nonlinear\nmotion. On MOT17 and MOT20, the proposed method\ndid not achieve state-of-the-art performance on the main\nevaluation metrics of MOTA, IDF1 and HOTA. How-\never, we achieve the best or second-best performance\nin MT and ML. For example, on the MOT17 test set\n(Table I), the proposed method ranked first with an\nML of 12.7% and second with an MT of 49.6%. On\nthe MOT20 test set (Table II), the proposed method\nachieved the best performance in terms of both MT\n(71.5%) and ML (8.6%). The higher MT and lower ML\nvalue indicated that our method was better at recovering\nobjects from occlusion or drift than filter-based methods.\nNote that methods in this group follow the tracking-\nby-detection paradigm and use the same detector [51].\nIn the motion part, ByteTrack uses standard Kalman\nfiltering and trajectory interpolation while both OC-\nSORT and BoT-SORT modify Kalman filtering to adapt\nto more complex situations. Furthermore, BoT-SORT\nalso models camera motion.\n3) Transformer-based tracking: Compared with exist-\ning Transformer-based methods, the proposed method\nachieved the best tracking performance on MOT17,\nMOT20 and DanceTrack. On DanceTrack, STDFormer\nachieves 91.7 MOTA, 60.5 IDF1, 57.8 HOTA, 80.5\nDetA and 41.7 AssA, surpassing the baseline methods\nby 3.3%, 9.0%, 3.6%, 4.6% and 1.5%. Unlike the\nbaseline methods, the proposed method only encodes\nthe object position information without considering the\nobject appearance. This showed that the new paradigm\nof Transformer-based methods had a great advantage\nin tracking under nonlinear motion. On MOT17 and\nMOT20, the three main evaluation metrics of MOTA,\nIDF1 and HOTA and the associated accuracy metric of\nAssA ranked first with 78.4, 73.1, 60.9, 58.4 and 76.2,\n72.1, 60.2, 57.7 respectively. In more detail, on MOT17\ntest set (Table I), our method increased by 0.8%, 0.2%,\n0.6% and 1.4% on MOTA, IDF1, HOTA and AssA,\nrespectively. In the crowded MOT20 test set (Table\nII), the benefits of the proposed method were more\nsignificant. MOTA, IDF1, HOTA and AssA increased by\n3.9%, 3.1%, 2.9% and 2.7%, respectively. Comparing\nTable I and Table II, we found that the performance\nof most Transformer-based methods on the MOT20 test\nset decreased significantly. In contrast, the performance\nloss of our method on MOT20 was much lower, and\nsome metrics (MT and ML) even increase. In addition,\n71.5% MT and 8.6% ML were much better than the\nsecond-best record on the list, increasing by 14.0% and\n4.8%, respectively. These results demonstrated that our\nmethod could effectively handle the occlusion problem\nin dense scenes. In summary, the good performance\nshowed that the idea of using Transformer to encode\nlow-dimensional position information for multi-object\ntracking was effective. Note that since Transformer has\nbeen applied in MOT in the past two years, there\nhave been relatively few related studies. To make a\nfull comparison, we only paid attention to whether the\nTransformer architecture is applied in the pipeline when\nselecting the methods, regardless of whether the tracking\nparadigm, detector or association strategy was closely\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 14\nrelated to the proposed method and whether the Trans-\nformer architecture was applied in the detection part or\nin the association part or even in both parts. In this work,\none of our motivations was to explore a new Transformer\napplication paradigm in MOT, trying to achieve better\ntracking performance with fewer computing and storage\nresources, so the comparison of this group of methods is\nmeaningful. In addition, the selected methods were all\npublished in the past two years and are comparable to a\ncertain extent.\n4) Contrastive learning-based tracking: Compared with\nsome existing contrastive learning-based studies, the\nproposed method outperformed the baseline by a large\nmargin on MOT17, MOT20 and DanceTrack. On Dance-\nTrack, our method improved MOTA, IDF1, HOTA,\nDetA and AssA by 8.7%, 15.7%, 12.1%, 8.4% and\n12.5%, respectively, compared with the baseline method.\nThe large performance advantage demonstrated that\nour contrastive learning strategy could learn more dis-\ncriminative feature representations for complex nonlin-\near object motion tracking. On MOT17, our method\nachieved state-of-the-art performance with 78.4 MOTA,\n60.9 HOTA, 49.6% MT, 12.7% ML, 87132 FN and\n25917 Frag. We achieved the second-best values in\nterms of 73.1 IDF1 and 58.4 AssA, slightly lower\nthan the performance of Semi-TCL [48]. On MOT20,\nour method outperformed other methods based on all\nproposed metrics. Furthermore, like the comparison re-\nsults of Transformer-based methods, our method did not\nfluctuate much in tracking performance on MOT17 and\nMOT20 compared to other contrastive learning-based\nmethods. This indicated that our contrastive learning\nstrategy could learn more reliable feature representations\nto keep tracking or recover lost objects from occlu-\nsion or drift. Note that the contrastive learning-based\nbaseline methods adopt selection criteria similar to the\nTransformer-based baseline methods.\nD. Qualitative Analysis\nVisualization. To more intuitively show that STDFormer\ncan achieve more robust association and more stable tracking\nthan the baseline mentioned in IV-C under similar appearance,\nocclusion and nonlinear motion, we provide some visualization\nresults of difficult cases on DanceTrack that STDFormer was\nable to handle but ByteTrack was not (Figure 7). Specif-\nically, we selected samples from diverse scenes, including\npop dance, gymnastics and street dance. Objects in pop\ndance videos (dancetrack0013, dancetrack0017) have frequent\ncrossover. Objects in gymnastics videos (dancetrack0054,\ndancetrack0059) exhibit diverse body gestures, frequent pose\nvariation and complicated motion patterns. Street dance videos\n(dancetrack0084, dancetrack0093) present difficult scenes in\nlow lighting apart from frequently occluded objects. Addi-\ntionally, the objects in these videos have similar appearances\ndue to similar or even identical clothes. As shown in Figure\n7, ByteTrack caused ID switching due to object occlusion\nor complex nonlinear motion, especially the object with id\n918 on dancetrack0093 (Figure 7(k)) experienced multiple ID\nswitches. In contrast, STDFormer did not exhibit any identity\nconversion and effectively preserved the identity. This demon-\nstrated that the proposed method could effectively improve\nmulti-object tracking under occlusion and nonlinear motion\nsituations.\nIn Figure 8, we also show the tracking results of Bytetrack\nand the proposed method on MOT17. Although STDFormer\ndid not achieve superior tracking performance over motion-\nbased methods on MOT17, the visualization results showed\nthat it still had an advantage in solving the occlusion problem.\nLimitations. STDFormer has several limitations, which are\nthe problems we will focus on in future work:\n1) Linear motion noise/Detector noise: We focus on\nimproving multi-object tracking under occlusion and\nnonlinear motion and pay insufficient attention to the\nnoise interference existing in linear motion prediction.\nAffected by detection noise, STDFormer easily inter-\nprets bad detection results as nonlinear motion signals\nand accumulates errors. We believe that it is necessary\nto explore a reasonable correction mechanism for STD-\nFormer to address the incorrect values in the prediction\nprocess and enhance the smoothness of the trajectory.\n2) Tracking in dynamic scenes: Our method only models\nthe object motion without considering the influence\nof coordinate system transformation caused by camera\nmotion in the dynamic scene (e.g., MOT17-06, MOT17-\n12 and MOT17-14). Motion prediction produces large\ndeviations in dynamic scenes. In particular, because the\ncamera motion is ignored, it is easy to misjudge the\nobject leaving the scene as still remaining in the current\nscene and make the wrong association. Therefore, in\nthe face of multi-object tracking in dynamic scenes,\nSTDFormer needs to model camera motion additionally.\n3) Tracking at the edge: Compared with the objects in\nthe center of the scene, the object motion modeling\nat the edge is more complex. Objects at edges away\nfrom the camera direction are small and dense. Only\nrelying on the center point distance and GIoU matching\nof the object bounding boxes for association is prone\nto ID switching. In addition, due to the small motion\ndisplacement of the objects at the edge, the object\nafter the ID switching may continue to maintain the\nwrong ID in the future. False tracking will interfere with\nSTDFormer’s judgment of the object’s motion intention,\nespecially ID switch with a object wandering at the edge\nor a new object entering the scene. It is difficult to judge\nwhether the object will continue to leave the scene, stop\nat the edge or even re-enter the scene according to the\ntrajectory trend, so the wrong tracking is difficult to be\ncorrected. On the other hand, for the object close to the\ncamera direction and gradually away from the edge of\nthe scene, its motion offset is large. As mentioned in\nAppendix A, the object motion offset at some edges on\nMOT17 and MOT20 can reach tens or even hundreds\n(e.g., MOT17-08, MOT20-06 and MOT20-08), which is\nhighly volatile and difficult to predict compared with the\noffset at the center of the scene. These are issues with\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 15\n(a) ByteTrack: dancetrack0013\n (b) STDFormer: dancetrack0013\n(c) ByteTrack: dancetrack0017\n (d) STDFormer: dancetrack0017\n(e) ByteTrack: dancetrack0054\n (f) STDFormer: dancetrack0054\n(g) ByteTrack: dancetrack0059\n (h) STDFormer: dancetrack0059\n(i) ByteTrack: dancetrack0084\n (j) STDFormer: dancetrack0084\n(k) ByteTrack: dancetrack0093\n (l) STDFormer: dancetrack0093\nFig. 7. Visualization results of STDFormer and ByteTrack on DanceTrack. ByteTrack leads to ID switch due to object occlusion or complex nonlinear\nmotion, but STDFormer effectively preserves the identity. To be precise, the problem occurs with ByteTrack’s objects: (a) ID switch between #82 and #86;\n(c) ID switch between #97 and #101; (e) ID switch between #535 and #537; (g) ID switch between #576 and #579; (i) ID switch between #692 and #693;\n(k) ID switch multiple times between #917 and #918. We pick samples from different scenes, including pop dance, gymnastics, and pop dance.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 16\n(a) ByteTrack: MOT17-01\n(b) STDFormer: MOT17-01\nFig. 8. Visualization results of STDFormer and ByteTrack on MOT17-\n01. ByteTrack switches IDs between #4 and #6 due to object occlusion, while\nSTDFormer effectively preserves identities based on motion trends.\nmotion-based methods that should be studied in future\nwork.\nIn summary, the quantitative experiments and visualization\nresults on MOT17, MOT20 and DanceTrack demonstrate that\nSTDFormer is an effective and powerful tracker. It focuses on\nimproving the nonlinear motion modeling of objects to mine\nobject motion intention and decision information to achieve\nrobust association under similar appearance, occlusion, and\nnonlinear motion. Meanwhile, STDFormer achieves state-of-\nthe-art performance compared to Transformer-based and con-\ntrastive learning-based methods. This points to a new feasible\ndirection for the application of Transformer and contrastive\nlearning in MOT.\nE. Ablative Studies\nIn this subsection, we evaluate different components of\nSTDFormer on the MOT17 validation set using private detec-\ntion and show the individual contributions of key components\nand strategies to facilitate tracking performance. Note that\nto optimize the association performance of STDFormer, we\nuse the three main association metrics of HOTA, IDF1 and\nAssA as the judgment basis in the following experiments to\ndetermine the best parameter values and component design.\nEffect of historical motion memory length. STDFormer\nuses long-range motion information of tracked objects to mine\ntheir behavioral intent. Too long trajectory information may\nhave redundancy or noise effects, while too short trajectory\ninformation may not be enough to reflect the potential motion\nawareness of the objects. To choose an appropriate length of\ntemporal information, we use trajectory histories of different\nlengths to train the model and infer on a single V100 GPU.\nTable IV shows the effect of different track history lengths k\nfrom 5 to 30 on tracking.\nTABLE IV\nCOMPARISONS ON DIFFERENT LENGTH OF HISTORICAL MOTION MEMORY .\nk MOTA↑ IDF1↑ HOTA↑ AssA↑ ID Sw.↓ Time(ms)↓\n5 89.07 82.793 75.544 72.748 433 11.34\n10 88.647 82.637 75.602 73.02 462 11.52\n15 89.091 83.787 76.257 73.975 430 12.14\n20 88.746 82.379 75.35 72.485 469 12.38\n25 88.842 82.744 75.228 72.176 468 12.69\n30 89.297 82.947 75.474 72.603 401 13.12\nThe results support our analysis that raising k can encourage\nassociation performance until the bottleneck is encountered.\nAccording to our experimental results, HOTA, IDF1, and\nAssA performed the best when k was increased to 15. This\ncan be explained by the fact that adding historical motion\ninformation can alleviate the influence of short-term motion\nnoise on motion direction estimation. However, continuing\nto increase k above 15 restrains association performance.\nLong-range movement behavior causes misjudgment of the\ncurrent movement intention. We also focused on the effect of\nincreasing history motion length on inference time in addition\nto association performance. As k varies from 5 to 30, the\nassociation time increased from 11.34 ms to 13.12 ms and\nthe running speed of STDFormer decreased from 24.8 FPS\nto 23.7 FPS with detection time around 29 ms. As can be\nseen, increasing the history motion length had a negligible\nimpact on the overall inference time. Therefore, we mainly\nconsidered the association performance. The results showed\nthat our method had the best association performance when\nk=15, so we finally chose the historical motion memory length\nof 15 frames. This setting improved by 0.84%, 0.66% and\n0.955% over the second place in HOTA, IDF1 and AssA,\nrespectively. Moreover, the association performance changed\nsharply in the neighborhood of the optimal value of k, and\nthe remaining values of k changed gently for the association\nperformance of the proposed method, which further showed\nthat 15 frames was a very suitable historical motion memory\nlength.\nEffect of prior detection. We tried to constrain the range\nof motion prediction by incorporating the high score detection\nboxes of the current frame in the process of modeling object\nmotion. As shown in Table V, we verified the effectiveness\nof prior detection by adding or not adding a detection branch.\nIn addition, we explored the choice of prior detection. ”w/o”\nmeans that no detection information was input to STDFormer.\n”w/ high score” indicates that only high score detections were\ninput to STDFormer. ”w/ all” indicates that both high score\ndetection boxes and low score detection boxes were input to\nSTDFormer.\nWe found that STDFormer achieved the best association\nperformance by adding high score detection boxes in the\nprocess of predicting object motion, especially in the three\nmain association evaluation metrics of HOTA, IDF1 and\nAssA. It shows that adding high confidence prior information\nimposes effective constraints on motion estimation, thereby\nimproving the association accuracy of STDFormer. On the\nother hand, although STDFormer had significantly better\nassociation performance after adding prior information, its\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 17\n(a)\n (b)\n (c)\nFig. 9. Several different feature interaction designs. (a) Intra-feature interaction: Temporal features, spatial features, and detection features sequentially\nimplement self-interaction, and there is no interaction between different types of features. (b) Joint spatial-temporal-detection interaction: Three types of\nfeatures interact in each submodule. (c) Joint spatial-temporal and separate detection interaction: First realize the interaction of spatial-temporal features in\neach submodule, and then implement the self-interaction of detection separately.\nTABLE V\nCOMPARISONS WITH AND WITHOUT PRIOR DETECTION .\nprior detection MOTA↑ IDF1↑ HOTA↑ AssA↑ ID Sw.↓\nw/o 89.306 82.011 74.854 71.285 421\nw/ high score 89.091 83.787 76.257 73.975 430\nw/ all 82.017 72.854 70.492 65.506 336\nTABLE VI\nCOMPARISONS ON DIFFERENT FEATURE INTERACTION MODE .\ninteraction mode MOTA↑ IDF1↑ HOTA↑ AssA↑ ID Sw.↓\na 87.222 75.585 71.617 66.263 715\nb 88.968 82.029 74.783 71.423 489\nc 89.091 83.787 76.257 73.975 430\ndetection metric MOTA was lower than the setting without\nprior detection. This is because noise and false detection boxes\nin high score detection boxes led to a decrease in localization\naccuracy and an increase in FN and FP. In addition, there were\nmore false detection boxes and noise in low score detection\nboxes. Using low score detection boxes as prior information\nnot only failed to improve the association accuracy, but also\ngreatly diminished its localization accuracy, which leads to\na significant decline in STDFormer’s tracking performance.\nTherefore, although there may be some occluded objects in\nlow score detection boxes and considering that all detection\nboxes can help reduce missing objects, fragmented trajectories\nand ID switching, we do not recommend inputting all detection\nboxes into STDFormer. In summary, prior detection helped to\nimprove the performance of association, but it was limited by\nthe impact of detection accuracy to some extent. Considering\neverything together, we chose to use high score detection\nboxes as the prior information available to STDFormer.\nEffect of feature interaction mode. Feature interaction\nis the core module of the proposed method. The interaction\npatterns of temporal features, spatial features and detection\nfeatures are crucial for the model to extract effective motion\ncues. Figure 9 shows several different feature interaction\ndesigns. Mode a iteratively encodes temporal features, spatial\nfeatures and detection features in sequence, and there is no\nobvious interaction among different types of features. Mode b\ninterleaves the three types of features by L times, interacting\nwith the three features in each iteration. Mode c is the feature\ninteraction method proposed by our method, which first inter-\nleaves the spatial-temporal motion features of the objects to\nobtain trajectory token features, and then interactively encodes\nthe trajectory token features and detection features.\nTable VI displays the comparison results of different feature\ninteraction modes. They show that the feature interaction\nmethod adopted by STDFormer was superior in both local-\nization and association. Compared with the split temporal\nattention and spatial attention in Mode a , the interleaved\ncoding of spatial-temporal attention in Mode b and Mode c\nwas more effective in mining the motion information of the\nobjects. In addition, compared with the addition of detection\ninformation in the process of extracting trajectory motion\ninformation in Mode b , the use of detection information in\nMode c to fine-tune the extracted spatial-temporal motion\ninformation was clearly better for making correct decisions.\nEffect of bidirectional matching optimization. STD-\nFormer achieves bidirectional optimal matching of affinity\nmatrices by introducing dual-softmax. As shown in Table VII,\nwe demonstrate the effectiveness of our proposed bidirectional\noptimization method by comparing three different matching\nstrategies. Psoftmax indicates performing single-dimensional\nsoftmax matching processing on the affinity matrix. The\ncalculation process is as follows:\nPsoftmax = softmax(At\nN:M /τ, dim= 2) (23)\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 18\nTABLE VII\nCOMPARISONS ON DIFFERENT MATCHING STRATEGIES .\nmatching strategies MOTA↑ IDF1↑ HOTA↑ AssA↑ ID Sw.↓\nPsoftmax 88.768 80.564 74.089 70.287 507\nPdsl 88.851 81.457 74.272 70.527 494\nOurs 89.091 83.787 76.257 73.975 430\nTABLE VIII\nCOMPARISONS WITH AND WITHOUT THE CONTRASTIVE LEARNING TASK .\ncontrastive learning MOTA↑ IDF1↑ HOTA↑ AssA↑ ID Sw.↓\n- 88.733 80.184 74.052 69.966 496\n✓ 89.091 83.787 76.257 73.975 430\nPdsl indicates that the bidirectional softmax matching pro-\ncess in LoFTR is used for the affinity matrix. The calculation\nprocess is as follows:\nPdsl = softmax(At\nN:M /τ, dim= 1)\n·softmax(At\nN:M /τ, dim= 2) (24)\nOurs is the bidirectional softmax matching strategy we\napplied, and the calculation process is shown in Equation\nEquations 14 and 15.\nThe experimental results demonstrated that STDFormer’s\nbidirectional matching method provided the best optimization\neffect. The comparative analysis of the three matching strate-\ngies not only showed that bidirectional matching was better\nthan single-directional matching but also demonstrated that\nthe sequential softmax was better than the parallel softmax\noptimization in two dimensions of the affinity matrix.\nEffect of the contrastive learning task. To learn the\naccurate motion feature representation of the trajectory predic-\ntion branch, we introduce an auxiliary task based on contrast\nlearning. Table VIII shows the effects of adding and not adding\nauxiliary tasks for model learning.\nThe results were consistent with our previous analysis. The\naddition of auxiliary tasks improved the MOTA, IDF1 and\nHOTA metrics, greatly improving the association accuracy and\nlocalization accuracy of the model. We also verified that the\nmodel learned a more accurate motion feature expression with\nthe constraints of contrastive learning.\nEffect of association strategies. The proposed stepwise\nassociation strategy has two novel designs compared to con-\nventional association methods:\n1) I ncorporating A ffinity C ost (IAC): Compared with ex-\nisting motion association methods, in addition to the\nposition distance and/or detection box overlap ratio, we\nalso use the affinity cost of detection feature represen-\ntation and motion feature representation as one of the\nassignment criteria.\n2) B oundary Judgment Filtering (BJF): It is used to distin-\nguish whether the disappearing target is temporarily lost\nor the target has left the current screen effectively.\nAs shown in Table IX, we separately verified the individual\ncontributions of each component by different combinations\nof association strategies. The results showed that both of our\nproposed association strategies were effective in promoting the\nassociation accuracy of tracking, especially the IAC strategy.\nTABLE IX\nCOMPARISONS ON DIFFERENT ASSOCIATION STRATEGIES .\nassociation strategies\nIAC BJF MOTA↑ IDF1↑ HOTA↑ AssA↑ ID Sw.↓\n- - 88.76 81.364 74.485 70.825 483\n✓ - 89.083 83.494 76.082 73.624 446\n- ✓ 88.786 80.951 74.338 70.634 476\n✓ ✓ 89.091 83.787 76.257 73.975 430\nTABLE X\nCOMPARISONS ON DIFFERENT BOUNDARY JUDGMENT THRESHOLDS .\nx MOTA↑ IDF1↑ HOTA↑ AssA↑ ID Sw.↓\n0 89.083 83.494 76.082 73.624 446\n5 89.1 83.671 76.141 73.756 431\n10 89.096 83.674 76.13 73.739 430\n20 89.094 83.749 76.205 73.869 428\n30 89.102 83.756 76.211 73.877 427\n40 89.085 83.761 76.208 73.878 431\n50 89.091 83.787 76.257 73.975 430\n60 89.094 83.747 76.229 73.919 431\nFurthermore, we also explored the optimal boundary judg-\nment threshold x through experiments. Table X showed the\neffect of different boundary judgment thresholds from 0 to\n60. The results suggested that boundary judgment filtering\nenabled our method to obtain the highest HOTA, IDF1 and\nAssA when x = 50. Therefore, we finally set the boundary\njudgment threshold to 50. As can be seen, this setting had little\ndifference in association metric compared with the boundary\njudgment threshold belonging to the range of 20 to 60 and\nwas superior to no boundary judgment or when the border\njudgment range was close to 0 pixels. For example, it improved\nHOTA, IDF1 and AssA by 0.293%, 0.175% and 0.351%\nrespectively compared with the boundary judgment threshold\nof 0.\nV. CONCLUSION\nIn this paper, we proposed STDFormer for online multi-\nobject tracking by jointly performing motion estimation and\ndata association. STDFormer mines motion cues contained\nin temporal motion and spatial interaction of targets by the\nattention mechanism of Transformer. We also introduced de-\ntection constraints considering prior knowledge. STDFormer\nachieves switching between temporal attention and spatial\nattention/detection attention by representing the tracked tar-\ngets as the embeddings of dynamically updated and aggre-\ngated temporal information. To learn accurate motion feature\nrepresentations, we introduced an auxiliary task based on\ncontrastive learning to mitigate the influence of detection\nnoise. In addition, we employed bidirectional matching to\nimprove the one-to-many/many-to-one matching problem of\nsingle-directional matching. Evaluation on the MOTChallenge\nbenchmark datasets demonstrated the superiority of our pro-\nposed method. The proposed method achieved significant\nimprovement over existing Transformer-based and contrastive\nlearning-based methods, although it still has some limitations\nin simple motion modes and dynamic scenes. In future work,\nwe will conduct in-depth research to empower the STDFormer\nmodel by introducing camera motion information.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 19\n(a) X-Offset on MOT17\n (b) Y-Offset on MOT17\n (c) W-Offset on MOT17\n (d) H-Offset on MOT17\n(e) X-Offset on MOT20\n (f) Y-Offset on MOT20\n (g) W-Offset on MOT20\n (h) H-Offset on MOT20\n(i) X-Offset on DanceTrack\n (j) Y-Offset on DanceTrack\n (k) W-Offset on DanceTrack\n (l) H-Offset on DanceTrack\nFig. 10. Percentage bar graph of Offset on benchmark datasets. For visualization purposes, we do not show the offset percentage bars below 1% separately.\n(a)-(d): The percentages of the center point coordinate x, center point coordinate y, bounding box width w, and bounding box height h of the target in adjacent\nframes on the MOT17 training set are displayed in turn; (e)-(h): The percentages of the center point coordinate x, center point coordinate y, bounding box\nwidth w, and bounding box height h of the target in adjacent frames on the MOT20 training set are displayed in turn; (i)-(l): The percentages of the center\npoint coordinate x, center point coordinate y, bounding box width w, and bounding box height h of the target in adjacent frames on the DanceTrack training\nset are displayed in turn.\nAPPENDIX A\nOBJECT MOTION ANALYSIS ON BENCHMARK DATASETS\nThe convergence speed and performance of the model are\nclosely related to the data distribution. To make STDFormer\nconverge quickly and locate the tracked objects’ positions\naccurately, we analyzed the object motion on different bench-\nmark datasets.\nWe reported the offset distribution of tracked objects’\nbounding boxes between adjacent frames on the MOT17,\nMOT20 and DanceTrack training sets in Figure 10, specifically\nincluding the offset distribution of object center point coor-\ndinates, bounding box height and width. The variance of the\noffset distribution on DanceTrack was large, while the variance\nof the offset distribution on MOT17 and MOT20 was small.\nWe conducted an in-depth analysis to assess this.\nMOT17 and MOT20. Except for the center point coordi-\nnate x, the offset value in the interval [-2,2] accounted for more\nthan 90% of MOT17. Due to the dense scene and overhead\nperspective, the offset on MOT20 was smaller, most of which\nwas in the [-1,1] interval. Furthermore, the percentages of 0\noffsets far exceeded the others on both MOT17 and MOT20.\nThis is because the video frame rate of MOT17 and MOT20\nis high and the pedestrian motion pattern is simple in static\nscenes, so the percentage of no obvious change in position\nbetween adjacent frames is large. At the same time, there were\nalso some offsets as high as dozens or even hundreds. These\nlarge offsets were mostly distributed in dynamic scenes with\ncamera motion on MOT17. A small number of large offsets\noccurred in the static scenes on MOT17 and MOT20, which\nusually appeared when pedestrians approached the camera and\nleft the scene. Overall, object motion in MOT17 and MOT20\nscenes was relatively simple, and the main challenges were\ncamera motion in dynamic scenes and object motion at the\nedge of the scene.\nDanceTrack. Compared with MOT17 and MOT20, the off-\nset on DanceTrack was larger, mostly in the [-10,10] interval.\nThe percentage of large offsets with an absolute offset value\nabove 10 cannot be ignored, and some even reached 300 or\neven 400, while the 0 offsets were much smaller than those\non MOT17 and MOT20. In addition, the continuity of the\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 20\n(a) X-Factor on MOT17\n (b) Y-Factor on MOT17\n (c) W-Factor on MOT17\n (d) H-Factor on MOT17\n(e) X-Factor on MOT20\n (f) Y-Factor on MOT20\n (g) W-Factor on MOT20\n (h) H-Factor on MOT20\n(i) X-Factor on DanceTrack\n (j) Y-Factor on DanceTrack\n (k) W-Factor on DanceTrack\n (l) H-Factor on DanceTrack\nFig. 11. Percentage bar graph of Exponential Adjustment Factor on benchmark datasets. (a)-(d): The percentages of the center point coordinate x,\ncenter point coordinate y, bounding box width w, and bounding box height h of the target in adjacent frames on the MOT17 training set are displayed in\nturn; (e)-(h): The percentages of the center point coordinate x, center point coordinate y, bounding box width w, and bounding box height h of the target in\nadjacent frames on the MOT20 training set are displayed in turn; (i)-(l): The percentages of the center point coordinate x, center point coordinate y, bounding\nbox width w, and bounding box height h of the target in adjacent frames on the DanceTrack training set are displayed in turn.\noffset was not strong. For example, the offset was consistently\n0 and suddenly changed to 30 at a certain point in time.\nThis is because DanceTrack is full of dance videos and the\ntarget movement range is large and the movement pattern is\nnonlinear. The object motion of DanceTrack is more complex,\nand the main challenge was more abrupt motion and nonlinear\nmotion.\nAccording to the analysis of Figure 10, it is feasible to\ndirectly predict the offset of object motion on MOT17 and\nMOT20. However, it is difficult to directly predict the offset\non DanceTrack because the model did not easily converge.\nTherefore, we considered another commonly used method of\nregressing the bounding box based on the candidate box.\nSpecifically, instead of directly outputting the offsets of the\ntwo bounding boxes, we obtained the bounding box of the\ncurrent frame by predicting an exponential adjustment factor to\nscale the bounding box of the previous frame. This exponential\nadjustment factor is calculated as follows:\nζx = log( xt\nxt−1\n) (25)\nζy = log( yt\nyt−1\n) (26)\nζw = log( wt\nwt−1\n) (27)\nζh = log( ht\nht−1\n) (28)\nSimilarly, we report the exponential adjustment factor dis-\ntribution of tracked objects’ bounding boxes between adjacent\nframes on the MOT17, MOT20 and DanceTrack training sets\nin Figure 11, specifically including the exponential adjustment\nfactor distribution of object center point coordinates, bounding\nbox height and width. The distributions of the exponential ad-\njustment factors on the three benchmark datasets were similar,\nand the variances were all extremely small. The values of the\nexponential adjustment factors were nearly or completely in\nthe [-0.5,0.5] interval, and the model was easy to regress.\nIn summary, given that the variance of the offset distribution\nand the variance of the exponential adjustment factor on\nMOT17 and MOT20 were both small, the model converged\nwell, and the performance of the model was not much\ndifferent whether it is directly predicting the offset or the\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 21\nexponential adjustment factor. For further comparison, the\nperformance of the model based on offset regression may be\nbetter due to the larger variance of the offset distribution and\nmore discrimination. The variance of the offset distribution\non DanceTrack was much larger than the variance of the\nexponential adjustment factor and the model based on the\nexponential adjustment factor converged better. To a certain\nextent, the performance of the model was also significantly\nbetter than that of the model based on offset regression. The\nabove conclusions are supported by the quantitative results in\nSection IV-C. Therefore, STDFormer-LMPH is recommended\nwhen dealing with simple motion scenes, and STDFormer-\nEMPH is recommended when dealing with complex nonlinear\nmotion scenes.\nAPPENDIX B\nSTDF ORMER IN UNMANNED AERIAL VEHICLE VIDEOS\nRecently, multi-object tracking techniques based on un-\nmanned aerial vehicle (UA V) videos have received a lot\nof attention. Compared with the targets captured in natural\nscenes, the targets captured by the mobile UA V platform are\nsmaller in scale and have higher density in crowded scenes.\nDue to the differences of targets in different scenes, some\nmulti-object tracking algorithms applied to natural scenes\ncan not be directly used in UA V videos. To demonstrate\nthe generalizability of our method in different scenarios, we\nevaluate the performance on a UA V-captured MOT dataset.\nSpecifically, we compare the proposed method with the state-\nof-the-art methods on the VisDrone2019 dataset.\nA. VisDrone2019 Dataset\nVisDrone2019 is a large-scale benchmark dataset for facil-\nitating object detection and tracking research on UA V videos.\nIt was collected from 14 different cities in China by the\nAISKYEYE team at Lab of Machine Learning and Data\nMining, Tianjin University, China. This dataset contains 96\nsequences. 56 sequences are used as a training set for training\nthe algorithms. 7 sequences are used as a validation set to\nverify the performance of algorithms. 17 sequences are used\nas a test-development set for public evaluation. 16 sequences\nare used as a test-challenge set for the workshop competition.\nThe AISKYEYE team has annotated the targets with bounding\nboxes, categories, and tracking ids in each frame. Note that\nthe VisDrone2019 dataset has ten categories, but we generally\nonly consider five categories (pedestrian, car, van, truck and\nbus) in the evaluation of multi-object tracking methods.\nB. Implementation Details\nSTDFormer’s experimental environment on the Vis-\nDrone2019 dataset was the same as IV-B. We used the training\nset together with the validation set for training and evaluated\nour method on the VisDrone2019 test-development set using\nthe official VisDrone MOT toolkit.\nUnlike single-class tracking (pedestrian) on MOT17,\nMOT20 and DanceTrack datasets, tracking on VisDrone2019\ndataset is a multi-class tracking task. Therefore, we did some\nadditional designs to avoid id switching between different\ncategories of targets. The main modifications were as follows:\n• We retrained a multi-class detector. The original detector\nwas a single-class detector, which only needed to distin-\nguish whether the object was a pedestrian or not. The\nnew detector was extended to 5 classes (pedestrian, car,\nvan, truck and bus).\n• We added object category information to STDFormer’s\ninput. Specifically, we converted the input (xc, yc, w, h)\nto (xc, yc, w, h, c), where c referred to the category id of\nthe target.\n• We added category matching cost in matching. Specifi-\ncally, we filtered out some false matches by setting the\ncost of matching pairs that did not belong to the same\nclass to infinity.\nC. Comparison with State-of-the-arts\nTo further demonstrate its effectiveness, the proposed\nmethod is compared with previous SOTA methods and bench-\nmark methods on the VisDrone2019 dataset. As shown in Ta-\nble XI, STDFormer sets a new state-of-the-art, outperforming\nthe baseline by a large margin in UA V videos. This shows that\nour method generalizes well to UA V videos. Specifically, we\nachieved 45.9 MOTA, 57.1 IDF1 and 77.9 MOTP on the Vis-\nDrone2019 test-development set, which surpassed the second\nplace by 9.8%, 6.1% and 1.8% respectively. STDFormer has\nlittle difference with other methods in detection metrics like\nMOTP, but it has improved significantly in association metrics\nlike IDF1 and finally achieved a large increase in tracking\naccuracy (MOTA). It can be seen that the proposed method\ncan achieve a large leap in association performance on the\nbasis of slightly improving the detection performance, which\nreflects that compared with other methods, STDFormer can\nbetter handle the association problem of small-scale targets in\nUA V videos. Note that, as shown in Figure 12, the variance of\nthe offset distribution on VisDrone2019 is much larger than the\nvariance of the exponential adjustment factor and the model\nbased on the exponential adjustment factor converged better.\nThus, we finally choose STDFormer-EMPH as the comparison\nmethod.\nHowever, while our method outperforms existing methods,\nthe tracking performance based on UA V videos is still far\ninferior to that in natural scenes. An important influencing\nfactor is that the images on VisDrone2019 were all captured\nby a moving unmanned aerial vehicle platform, and its camera\nmovement is more obvious than in natural scenes. In a dy-\nnamic scene, we need to consider the impact of camera motion\non it in addition to its own motion when modeling the object’s\nmotion. Therefore, we believe that our method will achieve\ngreater performance gains in multi-object tracking tasks for\nUA V videos after adding camera motion compensation.\nREFERENCES\n[1] A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online\nand realtime tracking,” in 2016 IEEE international conference on image\nprocessing (ICIP). IEEE, 2016, pp. 3464–3468.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 22\nTABLE XI\nCOMPARISON OF OUR METHOD WITH STATE -OF-THE -ART MOT ALGORITHMS ON THE VISDRONE 2019 TEST -DEV SET . THE BEST RESULTS OF THE\nMETHODS ARE MARKED IN RED .\nMethods MOTA↑ MOTP↑ IDF1↑ MT↑ ML↓ FP↓ FN↓ IDs↓ FM↓\nMOTDT [81] -0.8 68.5 21.6 87 1196 44548 185453 1437 3609\nSORT [1] 14.0 73.2 38.0 506 545 80845 112954 3629 4838\nIOUT [82] 28.1 74.7 38.9 467 670 36158 126549 2393 3829\nGOG [83] 28.7 76.1 36.4 346 836 17706 144657 1387 2237\nMOTR [38] 22.8 72.8 41.4 272 825 28407 147937 959 3980\nTrackFormer [37] 25 73.9 30.5 385 770 25856 141526 4840 4855\nUA VMOT [84] 36.1 74.2 51.0 520 574 27983 115925 2775 7396\nSTDFormer-EMPH 45.9 77.9 57.1 684 538 21288 101506 1440 3471\n(a) X-Offset on VisDrone2019\n (b) Y-Offset on VisDrone2019\n (c) W-Offset on VisDrone2019\n (d) H-Offset on VisDrone2019\n(e) X-Factor on VisDrone2019\n (f) Y-Factor on VisDrone2019\n (g) W-Factor on VisDrone2019\n (h) H-Factor on VisDrone2019\nFig. 12. Percentage bar graph of Offset and Percentage bar graph of Exponential Adjustment Factor on VisDrone2019 datasets. (a)-(d): The offset\npercentages of the center point coordinate x, center point coordinate y, bounding box width w, and bounding box height h of the target in adjacent frames\non the VisDrone2019 training set are displayed in turn; (e)-(h): The exponential adjustment factor percentages of the center point coordinate x, center point\ncoordinate y, bounding box width w, and bounding box height h of the target in adjacent frames on the VisDrone2019 training set are displayed in turn.\n[2] H. Sheng, Y . Zhang, J. Chen, Z. Xiong, and J. Zhang, “Heterogeneous\nassociation graph fusion for target association in multiple object track-\ning,” IEEE Transactions on Circuits and Systems for Video Technology,\nvol. 29, no. 11, pp. 3269–3280, 2018.\n[3] H. Sheng, Y . Zhang, Y . Wu, S. Wang, W. Lyu, W. Ke, and Z.\nXiong, “Hypothesis testing based tracking with spatio-temporal joint\ninteraction modeling,” IEEE Transactions on Circuits and Systems for\nVideo Technology, vol. 30, no. 9, pp. 2971–2983, 2020.\n[4] Y . Zhang, P. Sun, Y . Jiang, D. Yu, Z. Yuan, P. Luo, W. Liu, and X.\nWang, “Bytetrack: Multi-object tracking by associating every detection\nbox,” arXiv preprint arXiv:2110.06864, 2021.\n[5] Y . Zhang, C. Wang, X. Wang, W. Zeng, and W. Liu, “Fairmot:\nOn the fairness of detection and re-identification in multiple object\ntracking,” International Journal of Computer Vision, vol. 129, no. 11,\npp. 3069–3087, 2021.\n[6] J. Wu, J. Cao, L. Song, Y . Wang, M. Yang, and J. Yuan, “Track to\ndetect and segment: An online multi-object tracker,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2021, pp. 12 352–12 361.\n[7] P. Bergmann, T. Meinhardt, and L. Leal-Taixe, “Tracking without\nbells and whistles,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019, pp. 941–951.\n[8] S. You, H. Yao, and C. Xu, “Multi-object tracking with spatial-temporal\ntopology-based detector,” IEEE Transactions on Circuits and Systems\nfor Video Technology, vol. 32, no. 5, pp. 3023–3035, 2021.\n[9] Z. Wang, L. Zheng, Y . Liu, Y . Li, and S. Wang, “Towards real-time\nmulti-object tracking,” in European Conference on Computer Vision.\nSpringer, 2020, pp. 107–122.\n[10] K. Chen, X. Song, and X. Ren, “Modeling social interaction and\nintention for pedestrian trajectory prediction,” Physica A: Statistical\nMechanics and its Applications, vol. 570, p. 125790, 2021.\n[11] X. Li, Y . Liu, K. Wang, Y . Yan, and F.-Y . Wang, “Multi-target track-\ning with trajectory prediction and re-identification,” in 2019 Chinese\nAutomation Congress (CAC). IEEE, 2019, pp. 5028–5033.\n[12] X. Weng, Y . Yuan, and K. Kitani, “Ptp: Parallelized tracking and\nprediction with graph neural networks and diversity sampling,” IEEE\nRobotics and Automation Letters, vol. 6, no. 3, pp. 4640–4647, 2021.\n[13] J. Cao, X. Weng, R. Khirodkar, J. Pang, and K. Kitani, “Observation-\ncentric sort: Rethinking sort for robust multi-object tracking,” arXiv\npreprint arXiv:2203.14360, 2022.\n[14] Y . Du, Y . Song, B. Yang, and Y . Zhao, “Strongsort: Make deepsort\ngreat again,” arXiv preprint arXiv:2202.13514, 2022.\n[15] N. Aharon, R. Orfaig, and B.-Z. Bobrovsky, “Bot-sort: Robust asso-\nciations multi-pedestrian tracking,” arXiv preprint arXiv:2206.14651,\n2022.\n[16] H. Nodehi and A. Shahbahrami, “Multi-metric re-identification for\nonline multi-person tracking,” IEEE Transactions on Circuits and\nSystems for Video Technology, vol. 32, no. 1, pp. 147–159, 2021.\n[17] Y .-G. Lee, Z. Tang, and J.-N. Hwang, “Online-learning-based hu-\nman tracking across non-overlapping cameras,” IEEE Transactions\non Circuits and Systems for Video Technology, vol. 28, no. 10, pp.\n2870–2883, 2017.\n[18] Y . Du, J. Wan, Y . Zhao, B. Zhang, Z. Tong, and J. Dong, “Giaotracker:\nA comprehensive framework for mcmot with global information\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 23\nand optimizing strategies in visdrone 2021,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2021, pp.\n2809–2819.\n[19] N. Ran, L. Kong, Y . Wang, and Q. Liu, “A robust multi-athlete\ntracking algorithm by exploiting discriminant features and long-term\ndependencies,” in International Conference on Multimedia Modeling.\nSpringer, 2019, pp. 411–423.\n[20] A. Sadeghian, A. Alahi, and S. Savarese, “Tracking the untrackable:\nLearning to track multiple cues with long-term dependencies,” in\nProceedings of the IEEE international conference on computer vision,\n2017, pp. 300–311.\n[21] X. Wan, J. Wang, and S. Zhou, “An online and flexible multi-object\ntracking framework using long short-term memory,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops, 2018, pp. 1230–1238.\n[22] K. Fang, Y . Xiang, X. Li, and S. Savarese, “Recurrent autoregressive\nnetworks for online multi-object tracking,” in 2018 IEEE Winter\nConference on Applications of Computer Vision (W ACV). IEEE, 2018,\npp. 466–475.\n[23] F. Saleh, S. Aliakbarian, H. Rezatofighi, M. Salzmann, and S. Gould,\n“Probabilistic tracklet scoring and inpainting for multiple object track-\ning,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2021, pp. 14 329–14 339.\n[24] R. E. Kalman et al., “Contributions to the theory of optimal control,”\nBol. soc. mat. mexicana, vol. 5, no. 2, pp. 102–119, 1960.\n[25] N. Wojke, A. Bewley, and D. Paulus, “Simple online and realtime\ntracking with a deep association metric,” in 2017 IEEE international\nconference on image processing (ICIP). IEEE, 2017, pp. 3645–3649.\n[26] G. L. Smith, S. F. Schmidt, and L. A. McGee, Application of statistical\nfilter theory to the optimal estimation of position and velocity on board\na circumlunar vehicle. National Aeronautics and Space Administration,\n1962.\n[27] S. J. Julier and J. K. Uhlmann, “New extension of the kalman filter\nto nonlinear systems,” in Signal processing, sensor fusion, and target\nrecognition VI, vol. 3068. Spie, 1997, pp. 182–193.\n[28] F. Gustafsson, F. Gunnarsson, N. Bergman, U. Forssell, J. Jansson,\nR. Karlsson, and P.-J. Nordlund, “Particle filters for positioning,\nnavigation, and tracking,” IEEE Transactions on signal processing, vol.\n50, no. 2, pp. 425–437, 2002.\n[29] J. Xiang, N. Sang, J. Hou, R. Huang, and C. Gao, “Multitarget tracking\nusing hough forest random field,” IEEE Transactions on Circuits and\nSystems for Video Technology, vol. 26, no. 11, pp. 2028–2042, 2015.\n[30] Z. Weng, J. Yang, Q. Zhang, and Z. Guo, “Multi-target tracking based\non motion estimation and rj-mcmc particle filter,” in Proceedings of\nthe 2018 International Conference on Information Science and System,\n2018, pp. 161–165.\n[31] Z. Khan, T. Balch, and F. Dellaert, “Mcmc-based particle filtering for\ntracking a variable number of interacting targets,” IEEE transactions\non pattern analysis and machine intelligence, vol. 27, no. 11, pp.\n1805–1819, 2005.\n[32] A. Milan, S. H. Rezatofighi, A. Dick, I. Reid, and K. Schindler, “Online\nmulti-target tracking using recurrent neural networks,” in Thirty-First\nAAAI conference on artificial intelligence, 2017.\n[33] G. Wang, R. Gu, Z. Liu, W. Hu, M. Song, and J.-N. Hwang, “Track\nwithout appearance: Learn box and tracklet embedding with local and\nglobal motion patterns for vehicle tracking,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2021, pp.\n9876–9886.\n[34] P. Sun, J. Cao, Y . Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang, and\nP. Luo, “Transtrack: Multiple object tracking with transformer,” arXiv\npreprint arXiv:2012.15460, 2020.\n[35] Y . Xu, Y . Ban, G. Delorme, C. Gan, D. Rus, and X. Alameda-\nPineda, “Transcenter: Transformers with dense queries for multiple-\nobject tracking,” arXiv preprint arXiv:2103.15145, 2021.\n[36] M. Chen, Y . Liao, S. Liu, F. Wang, and J.-N. Hwang, “Tr-mot: Multi-\nobject tracking by reference,” arXiv preprint arXiv:2203.16621, 2022.\n[37] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, “Track-\nformer: Multi-object tracking with transformers,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 8844–8854.\n[38] F. Zeng, B. Dong, T. Wang, X. Zhang, and Y . Wei, “Motr: End-\nto-end multiple-object tracking with transformer,” arXiv preprint\narXiv:2105.03247, 2021.\n[39] T. Zhu, M. Hiller, M. Ehsanpour, R. Ma, T. Drummond, I. Reid, and\nH. Rezatofighi, “Looking beyond two frames: End-to-end multi-object\ntracking using spatial and temporal transformers,” IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2022.\n[40] J. Cai, M. Xu, W. Li, Y . Xiong, W. Xia, Z. Tu, and S. Soatto, “Memot:\nMulti-object tracking with memory,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022, pp.\n8090–8100.\n[41] P. Chu, J. Wang, Q. You, H. Ling, and Z. Liu, “Transmot: Spatial-\ntemporal graph transformer for multiple object tracking,” arXiv preprint\narXiv:2104.00194, 2021.\n[42] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple frame-\nwork for contrastive learning of visual representations,” in International\nconference on machine learning. PMLR, 2020, pp. 1597–1607.\n[43] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, “Momentum contrast\nfor unsupervised visual representation learning,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 9729–9738.\n[44] T. Park, A. A. Efros, R. Zhang, and J.-Y . Zhu, “Contrastive learning\nfor unpaired image-to-image translation,” in European conference on\ncomputer vision. Springer, 2020, pp. 319–345.\n[45] S. Arora, H. Khandeparkar, M. Khodak, O. Plevrakis, and N. Saun-\nshi, “A theoretical analysis of contrastive unsupervised representation\nlearning,” arXiv preprint arXiv:1902.09229, 2019.\n[46] L. Logeswaran and H. Lee, “An efficient framework for learning\nsentence representations,” arXiv preprint arXiv:1803.02893, 2018.\n[47] M. Pagliardini, P. Gupta, and M. Jaggi, “Unsupervised learning of\nsentence embeddings using compositional n-gram features,” arXiv\npreprint arXiv:1703.02507, 2017.\n[48] W. Li, Y . Xiong, S. Yang, M. Xu, Y . Wang, and W. Xia, “Semi-\ntcl: Semi-supervised track contrastive representation learning,” arXiv\npreprint arXiv:2107.02396, 2021.\n[49] J. Pang, L. Qiu, X. Li, H. Chen, Q. Li, T. Darrell, and F. Yu, “Quasi-\ndense similarity learning for multiple object tracking,” in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recogni-\ntion, 2021, pp. 164–173.\n[50] Y . Liu, Q. Yan, and A. Alahi, “Social nce: Contrastive learning\nof socially-aware motion representations,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2021, pp.\n15 118–15 129.\n[51] Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Yolox: Exceeding yolo\nseries in 2021,” arXiv preprint arXiv:2107.08430, 2021.\n[52] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, “Loftr: Detector-\nfree local feature matching with transformers,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition,\n2021, pp. 8922–8931.\n[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[54] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\nobject detection with region proposal networks,” Advances in neural\ninformation processing systems, vol. 28, 2015.\n[55] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 7263–7271.\n[56] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international\nconference on computer vision, 2015, pp. 1440–1448.\n[57] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S.\nSavarese, “Generalized intersection over union: A metric and a loss for\nbounding box regression,” in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2019, pp. 658–666.\n[58] X. Cheng, H. Lin, X. Wu, F. Yang, and D. Shen, “Improving video-\ntext retrieval by multi-stream corpus alignment and dual softmax loss,”\narXiv preprint arXiv:2109.04290, 2021.\n[59] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean conference on computer vision. Springer, 2020, pp. 213–229.\n[60] T. -Y . Lin, P. Goyal, R. Girshick, K. He and P. Doll ´ar, “Focal Loss\nfor Dense Object Detection,” in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 2980–2988.\n[61] A. Kendall, Y . Gal, and R. Cipolla, “Multi-task learning using un-\ncertainty to weigh losses for scene geometry and semantics,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7482–7491.\n[62] A. Milan, L. Leal-Taix ´e, I. Reid, S. Roth, and K. Schindler,\n“Mot16: A benchmark for multi-object tracking,” arXiv preprint\narXiv:1603.00831, 2016.\n[63] P. Dendorfer, H. Rezatofighi, A. Milan, J. Shi, D. Cremers, I. Reid, S.\nRoth, K. Schindler, and L. Leal-Taix´e, “Mot20: A benchmark for multi\nobject tracking in crowded scenes,” arXiv preprint arXiv:2003.09003,\n2020.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 24\n[64] P. Sun, J. Cao, Y . Jiang, Z. Yuan, S. Bai, K. Kitani, and P. Luo,\n“Dancetrack: Multi-object tracking in uniform appearance and diverse\nmotion,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 20 993–21 002.\n[65] P. Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, and H. Ling, “Detection\nand tracking meet drones challenge,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 44, no. 11, pp. 7380–7399,\n2021.\n[66] K. Bernardin and R. Stiefelhagen, “Evaluating multiple object tracking\nperformance: the clear mot metrics,” EURASIP Journal on Image and\nVideo Processing, vol. 2008, pp. 1–10, 2008.\n[67] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, “Perfor-\nmance measures and a data set for multi-target, multi-camera tracking,”\nin European conference on computer vision. Springer, 2016, pp. 17–35.\n[68] J. Luiten, A. Osep, P. Dendorfer, P. Torr, A. Geiger, L. Leal-Taix ´e,\nand B. Leibe, “Hota: A higher order metric for evaluating multi-object\ntracking,” International journal of computer vision, vol. 129, no. 2, pp.\n548–578, 2021.\n[69] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\narXiv preprint arXiv:1711.05101, 2017.\n[70] ——, “Sgdr: Stochastic gradient descent with warm restarts,” arXiv\npreprint arXiv:1608.03983, 2016.\n[71] B. Pang, Y . Li, Y . Zhang, M. Li, and C. Lu, “Tubetk: Adopting tubes\nto track multi-object in a one-step training model,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 6308–6318.\n[72] X. Zhou, V . Koltun, and P. Kr ¨ahenb¨uhl, “Tracking objects as points,”\nin European Conference on Computer Vision. Springer, 2020, pp.\n474–490.\n[73] S. Han, P. Huang, H. Wang, E. Yu, D. Liu, and X. Pan, “Mat: Motion-\naware multi-object tracking,” Neurocomputing, vol. 476, pp. 75–86,\n2022.\n[74] L. Zheng, M. Tang, Y . Chen, G. Zhu, J. Wang, and H. Lu, “Improving\nmultiple object tracking with single object tracking,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2021, pp. 2453–2462.\n[75] Y . Wang, K. Kitani, and X. Weng, “Joint object detection and multi-\nobject tracking with graph neural networks,” in 2021 IEEE International\nConference on Robotics and Automation (ICRA). IEEE, 2021, pp. 13\n708–13 715.\n[76] C. Liang, Z. Zhang, X. Zhou, B. Li, S. Zhu, and W. Hu, “Rethinking the\ncompetition between detection and reid in multiobject tracking,” IEEE\nTransactions on Image Processing, vol. 31, pp. 3182–3196, 2022.\n[77] J. Hyun, M. Kang, D. Wee, and D.-Y . Yeung, “Detection recovery in\nonline multi-object tracking with sparse graph tracker,” in Proceedings\nof the IEEE/CVF Winter Conference on Applications of Computer\nVision (W ACV), January 2023, pp. 4850–4859.\n[78] X. Zhou, T. Yin, V . Koltun, and P. Kr¨ahenb¨uhl, “Global tracking trans-\nformers,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 8771–8780.\n[79] A. Galor, R. Orfaig, and B.-Z. Bobrovsky, “Strong-transcenter: Im-\nproved multi-object tracking based on transformers with dense repre-\nsentations,” arXiv preprint arXiv:2210.13570, 2022.\n[80] Y . Zhang, H. Sheng, Y . Wu, S. Wang, W. Ke, and Z. Xiong, “Multiplex\nlabeling graph for near-online tracking in crowded scenes,” IEEE\nInternet of Things Journal, vol. 7, no. 9, pp. 7892–7902, 2020.\n[81] L. Chen, H. Ai, Z. Zhuang, and C. Shang, “Real-time multiple\npeople tracking with deeply learned candidate selection and person re-\nidentification,” in 2018 IEEE international conference on multimedia\nand expo (ICME). IEEE, 2018, pp. 1–6.\n[82] E. Bochinski, V . Eiselein, and T. Sikora, “High-speed tracking-by-\ndetection without using image information,” in 2017 14th IEEE inter-\nnational conference on advanced video and signal based surveillance\n(A VSS). IEEE, 2017, pp. 1–6.\n[83] H. Pirsiavash, D. Ramanan, and C. C. Fowlkes, “Globally-optimal\ngreedy algorithms for tracking a variable number of objects,” in CVPR\n2011. IEEE, 2011, pp. 1201–1208.\n[84] S. Liu, X. Li, H. Lu, and Y . He, “Multi-object tracking meets moving\nuav,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022, pp. 8876–8885.\nMengjie Hu received a Ph.D. degree from Beihang\nUniversity, Beijing, China, in 2017. She is now\na lecturer at the Beijing University of Posts and\nTelecommunications. Her current research interests\nare primarily in computer vision and machine learn-\ning, especially visual object tracking and 3D scene\nunderstanding.\nXiaotong Zhu received a B.S. degree from Beijing\nUniversity of Posts and Telecommunications, Bei-\njing, China, in 2021. She is currently pursuing a\nMaster’s degree from Beijing University of Posts and\nTelecommunications, Beijing, China. Her research\ninterests include multi-object tracking and trajectory\nprediction.\nHaotian Wang received a B.S. degree from Bei-\njing University of Posts and Telecommunications,\nBeijing, China, in 2022. He is currently pursuing\nan M.S. degree at Beijing University of Posts and\nTelecommunications, Beijing, China. His current re-\nsearch interests include object detection and object\ntracking, especially multi-object tracking in satellite\nvideos.\nShixiang Cao received his Ph.D. in remote sensing\nfrom Beihang University, Beijing, China, in 2014.\nNow, he works as senior engineer at the Beijing\nInstitute of Space Mechanics & Electricity. His\ncurrent research interests include optical detective\nsensor design, motion analysis, remote sensing im-\nage processing and algorithm implementation from\nthe computer vision community.\nChun Liu received a B.S. in mechanical design,\nmanufacturing and automation, and an M.S. degrees\nin mechatronic engineering from China University\nof Geosciences, Beijing, China, in 2006 and 2009,\nrespectively, and a Ph.D. in measurement and control\ntechnology from the University of Kassel, Kassel,\nGermany, in 2014. She is currently a Lecturer with\nthe School of Artificial Intelligence, Beijing Univer-\nsity of Posts and Telecommunications, Beijing. Her\nresearch interests include deep learning, intelligent\ncomputation, computer vision, and data mining.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. Y , NOVEMBER 2022 25\nQing Song received her Ph.D. from Tianjin Univer-\nsity, Tianjin, China, in 2006. She is currently a scien-\ntific researcher at the Beijing University of Posts and\nTelecommunications (BUPT), where she is engaged\nin computer vision technology study. She is the\nfounder of Pattern Recognition and Intelligent Vision\nLaboratory (PRIV). She oversees many national,\nprovincial and ministerial projects and enterprise\ncooperation projects. She has published more than\n100 academic papers in international journals and\nconferences.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3263884\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7583034038543701
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6964436173439026
    },
    {
      "name": "Computer vision",
      "score": 0.6175135374069214
    },
    {
      "name": "Exploit",
      "score": 0.5792623162269592
    },
    {
      "name": "Motion estimation",
      "score": 0.5144665241241455
    },
    {
      "name": "Video tracking",
      "score": 0.46246641874313354
    },
    {
      "name": "Object detection",
      "score": 0.4195820093154907
    },
    {
      "name": "Embedding",
      "score": 0.41512274742126465
    },
    {
      "name": "Match moving",
      "score": 0.410357266664505
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3737180233001709
    },
    {
      "name": "Motion (physics)",
      "score": 0.31113898754119873
    },
    {
      "name": "Object (grammar)",
      "score": 0.22420817613601685
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    }
  ],
  "cited_by": 37
}