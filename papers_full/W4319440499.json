{
    "title": "MyopiaDETR: End-to-end pathological myopia detection based on transformer using 2D fundus images",
    "url": "https://openalex.org/W4319440499",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5101746400",
            "name": "Manyu Li",
            "affiliations": [
                "Nanchang University"
            ]
        },
        {
            "id": "https://openalex.org/A5101474792",
            "name": "Shichang Liu",
            "affiliations": [
                "Shaanxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5100380112",
            "name": "Zihan Wang",
            "affiliations": [
                "Nanchang University"
            ]
        },
        {
            "id": "https://openalex.org/A5100353783",
            "name": "Xin Li",
            "affiliations": [
                "Shaanxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5080173773",
            "name": "Zezhong Yan",
            "affiliations": [
                "Nanchang University"
            ]
        },
        {
            "id": "https://openalex.org/A5080870519",
            "name": "Renping Zhu",
            "affiliations": [
                "Nanchang University",
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5101634419",
            "name": "Zhijiang Wan",
            "affiliations": [
                "Nanchang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6748733227",
        "https://openalex.org/W3018757597",
        "https://openalex.org/W2889985731",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3000295996",
        "https://openalex.org/W3157594656",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W1975429831",
        "https://openalex.org/W2928165649",
        "https://openalex.org/W3176659256",
        "https://openalex.org/W4287071612",
        "https://openalex.org/W3193396633",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2130665584",
        "https://openalex.org/W2068651555",
        "https://openalex.org/W2917575296",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2107910786",
        "https://openalex.org/W6785652829",
        "https://openalex.org/W4296391946",
        "https://openalex.org/W4312443924",
        "https://openalex.org/W2901350546",
        "https://openalex.org/W6800709815",
        "https://openalex.org/W6681566559",
        "https://openalex.org/W4280573064",
        "https://openalex.org/W3084438349",
        "https://openalex.org/W2144506857",
        "https://openalex.org/W2901158775",
        "https://openalex.org/W2570343428",
        "https://openalex.org/W4293584584",
        "https://openalex.org/W2963037989",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2953602099",
        "https://openalex.org/W2156163116",
        "https://openalex.org/W2970733489",
        "https://openalex.org/W3127935012",
        "https://openalex.org/W2003759790",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W1966944283",
        "https://openalex.org/W71155377",
        "https://openalex.org/W3176580834",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W3106250896",
        "https://openalex.org/W4214627427",
        "https://openalex.org/W2787928077",
        "https://openalex.org/W4226082258",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3099319035",
        "https://openalex.org/W2145221928"
    ],
    "abstract": "Background Automated diagnosis of various retinal diseases based on fundus images can serve as an important clinical decision aid for curing vision loss. However, developing such an automated diagnostic solution is challenged by the characteristics of lesion area in 2D fundus images, such as morphology irregularity, imaging angle, and insufficient data. Methods To overcome those challenges, we propose a novel deep learning model named MyopiaDETR to detect the lesion area of normal myopia (NM), high myopia (HM) and pathological myopia (PM) using 2D fundus images provided by the iChallenge-PM dataset. To solve the challenge of morphology irregularity, we present a novel attentional FPN architecture and generate multi-scale feature maps to a traditional Detection Transformer (DETR) for detecting irregular lesion more accurate. Then, we choose the DETR structure to view the lesion from the perspective of set prediction and capture better global information. Several data augmentation methods are used on the iChallenge-PM dataset to solve the challenge of insufficient data. Results The experimental results demonstrate that our model achieves excellent localization and classification performance on the iChallenge-PM dataset, reaching AP 50 of 86.32%. Conclusion Our model is effective to detect lesion areas in 2D fundus images. The model not only achieves a significant improvement in capturing small objects, but also a significant improvement in convergence speed during training.",
    "full_text": "fnins-17-1130609 February 2, 2023 Time: 14:56 # 1\nTYPE Original Research\nPUBLISHED 07 February 2023\nDOI 10.3389/fnins.2023.1130609\nOPEN ACCESS\nEDITED BY\nXin Huang,\nRenmin Hospital of Wuhan University, China\nREVIEWED BY\nYu Lin Zhong,\nJiangxi Provincial People’s Hospital, China\nYanggang Feng,\nBeihang University, China\nDongsheng Wang,\nJiangsu University of Science and Technology,\nChina\n*CORRESPONDENCE\nRenping Zhu\nxgczrp@ncu.edu.cn\nZhijiang Wan\nwandndn@gmail.com\n†These authors have contributed equally to this\nwork and share ﬁrst authorship\nSPECIALTY SECTION\nThis article was submitted to\nVisual Neuroscience,\na section of the journal\nFrontiers in Neuroscience\nRECEIVED 23 December 2022\nACCEPTED 23 January 2023\nPUBLISHED 07 February 2023\nCITATION\nLi M, Liu S, Wang Z, Li X, Yan Z, Zhu R and\nWan Z (2023) MyopiaDETR: End-to-end\npathological myopia detection based on\ntransformer using 2D fundus images.\nFront. Neurosci.17:1130609.\ndoi: 10.3389/fnins.2023.1130609\nCOPYRIGHT\n© 2023 Li, Liu, Wang, Li, Yan, Zhu and Wan. This\nis an open-access article distributed under the\nterms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with\nthese terms.\nMyopiaDETR: End-to-end\npathological myopia detection\nbased on transformer using 2D\nfundus images\nManyu Li 1†, Shichang Liu2†, Zihan Wang1, Xin Li2, Zezhong Yan1,\nRenping Zhu1,3,4* and Zhijiang Wan1,3*\n1School of Information Engineering, Nanchang University, Jiangxi, China, 2School of Computer Science,\nShaanxi Normal University, Xi’an, China,3Industrial Institute of Artiﬁcial Intelligence, Nanchang University,\nJiangxi, China, 4School of Information Management, Wuhan University, Hubei, China\nBackground: Automated diagnosis of various retinal diseases based on fundus\nimages can serve as an important clinical decision aid for curing vision loss.\nHowever, developing such an automated diagnostic solution is challenged by the\ncharacteristics of lesion area in 2D fundus images, such as morphology irregularity,\nimaging angle, and insufﬁcient data.\nMethods: To overcome those challenges, we propose a novel deep learning\nmodel named MyopiaDETR to detect the lesion area of normal myopia (NM),\nhigh myopia (HM) and pathological myopia (PM) using 2D fundus images provided\nby the iChallenge-PM dataset. To solve the challenge of morphology irregularity,\nwe present a novel attentional FPN architecture and generate multi-scale feature\nmaps to a traditional Detection Transformer (DETR) for detecting irregular lesion\nmore accurate. Then, we choose the DETR structure to view the lesion from the\nperspective of set prediction and capture better global information. Several data\naugmentation methods are used on the iChallenge-PM dataset to solve the challenge\nof insufﬁcient data.\nResults: The experimental results demonstrate that our model achieves excellent\nlocalization and classiﬁcation performance on the iChallenge-PM dataset, reaching\nAP50 of 86.32%.\nConclusion: Our model is effective to detect lesion areas in 2D fundus images. The\nmodel not only achieves a signiﬁcant improvement in capturing small objects, but\nalso a signiﬁcant improvement in convergence speed during training.\nKEYWORDS\nmyopia detection, fundus images, attentional FPN, detection transformer (DETR),\ndichotomous graph matching\nIntroduction\nRetinal diseases are one of the main causes of vision loss, and severe retinal diseases can\nalso cause irreversible damage to vision. Medical research has found that the deformation of\nthe front of the eyeball varies with the degree of myopia (Wong et al., 2014), these changes\nmay be related to the complications of ocular diseases, the complications of pathological\nmyopia (PM) are considered to be the main cause of visual impairment and blindness. Due to\nchanges in the environment and lifestyle, the incidence of high myopia-related diseases has been\nincreasing year by year (Hsu et al., 2004; Iwase et al., 2006; Y amada et al., 2010; You et al., 2011;\nFrontiers in Neuroscience 01 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 2\nLi et al. 10.3389/fnins.2023.1130609\nFurtado et al., 2012). As a common eye disease, it aﬀects 20 to 40% of\nadults (Iwase et al., 2006) and has become a global burden of public\nhealth, 35% of myopic patients are high myopia (HM) (Y amada\net al., 2010), which will develop into pathological myopia. The PM\nis characterized by excessive and progressive elongation of the globe,\nwhich is now considered to be the most visually impaired and blind\ncause. Therefore, timely diagnosis and regular review for PM are very\nimportant.\nNowadays, people pay more attention to their health, and the\ndemand for medical services is also increasing. Although the number\nof ophthalmologists in the developed countries is growing (Hsu et al.,\n2004; You et al., 2011; Furtado et al., 2012; Sakaguchi et al., 2019),\nthere is still a big gap in the demand for ophthalmologists. Due to\nthe long training time for cultivating doctors, the underdeveloped\nregions will still face the problem of shortage of medical resources in\nthe next few decades. With the development of imaging technology,\nmyopia-related complications have been identiﬁed (Lu et al., 2018;\nPeng et al., 2019; Nazir et al., 2020; Cui et al., 2021; Zhang et al.,\n2021; Muthukannan and Glaret Subin, 2022). At present, fundus\nimaging is an important basis for the diagnosis of various ophthalmic\ndiseases. Most retinal diseases can be avoided with early and timely\ntreatment. Therefore, early detection and early treatment are of\ngreat signiﬁcance for the cure of retinal diseases. However, analysing\nmedical images relies on the extensive medical experience of doctors,\nwhich is laborious and time-consuming. Thus, designing a reliable\nand accurate automatic detection method for fundus images is crucial\nto the prevention and treatment of diseases.\nMany studies utilize deep learning techniques to diagnose eye\ndiseases. For instance, Nazir et al. (2020) proposed a FRCNN\nalgorithm with fuzzy k-means for automatic detecting three types of\nretinal diseases at early stage. Vyas et al. used common convolutional\nneural network for dry eye disease (DED) detection based on\nTear Film Breakup Time (TBUT) videos, the approach shows\nhigh performance in classifying TBUT frames and detecting DED.\nMuthukannan and Glaret Subin (2022) designed a CNN that\noptimized by ﬂower pollination for feature extraction, increased the\nspeed and the accuracy of the network for detecting four types of\neye diseases. However, most eﬀorts in the existed deep learning\nfocused on applying existing techniques to the myopia detection task\nrather than proposing new ones speciﬁcally suited to the domain.\nThe standard well-known network architectures were designed for\nthe data collected in natural scenes (e.g., natural images) and do\nnot take the peculiarities of the myopia images’ characteristics into\naccount. Therefore, research is necessary to understand how these\narchitectures can be optimized for myopia data.\nDetection Transformer (DETR) is a new paradigm for end-to-\nend object detection. DETR always failed in detecting small object\nand it has a slow convergence speed. Since DETR only utilizes the\nfeature maps (32 ×down sampling) from the last layer of backbone,\nwhich leads to a large semantic loss of small objects, thus DETR\nperforms poorly on small object detection. Additionally, in the\ndecoder structure of DETR, self-attention is computed for all input\npixels, so the model is presented with computational complexity\nin square level, which further results in slow convergence speed.\nTo solve the problems of the two aspects, Deformable DETR (Zhu\net al., 2020) improves the performance of small object detection and\naccelerates the convergence speed by limiting the range of computed\nattention using multi-scale feature maps. Conditional DETR (Meng\net al., 2021) uses the conditional spatial query explicitly to ﬁnd the\nextremity region of the object to reduce the searching range of object\nand accelerate the convergence.\nFigure 1 shows some examples selected from the iChallenge-\nPM dataset. The ﬁgure shows the typical image characteristics of\nthe fundus images, the green background indicates normal myopia\n(NM), the purple background represents high myopia (HM), and the\nyellow background delegates pathological myopia (PM). The black\nmask on the right side of myopia image is atrophy area, which\nvarious a lot in shape or to some extend very similar. The white oval\narea in yellow background is eye’s optical disk region and the lesion\narea appears randomly. These characteristics challenge the model\nperformance of the deep learning methods. The speciﬁc challenges\nare illustrated as follows:\n(1) Morphology Irregularity: As for the NM and HM shown\nin Figure 1, the lesion area which in green and purple background\nis irregular and similar, its area only occupies a small part of the\nlocation, which makes the model troubling to learn its morphological\nfeatures and the most of the rest area is background.\n(2) Imaging Angle: From all images in Figure 1 , it is obvious\nthat the optical disk region hava a tendency to the left of the image,\nthis man-made imaging method may mislead our model, so the\ndiﬀerences brought by the imaging angle require the learning ability\nof the model for location correlation demanding.\n(3) Insuﬃcient Data: The iChallenge-PM dataset only contains\n1,200 images, around 600 images for PM and 600 images for Non-\nPM (NM+HM). Fewer images and the strong ﬁtting ability of neural\nnetwork make it easy to overﬁt, which will reduce the generalization\nability of the model.\nIn this paper, a novel deep learning model named MyopiaDETR\nis proposed for detecting the lesion of NM, HM, and PM using 2D\nfundus images. Our base model adopts scalable Swin Transformer as\nbackbone, which is ﬂexible in depth. When it comes to morphology\nirregularity, it is worth noting that the lesion tissue is not only\nin irregular shape, but also distributed in a small area of the\nwhole fundus image, most of the pixels are redundant, and an\nimpure background will adversely aﬀect the prediction results. To\naddress those problems, we propose a novel Attentional feature\npyramid networks (FPN) architecture that can purify the feature\nrepresentation during the aggregation of feature maps, speciﬁcally,\nobject queries are added to FPN (Lin et al., 2017) levels with positional\nencoding to execute multi-head self-attention, which are used to\ngive more activation weight to the object regions, produce a larger\ngap between object and background. Our attentional FPN solves\nthe problem that traditional DETR cannot utilize multi-scale feature\nmaps, resulting in poor performance in detecting small objects in\nthe fundus image. Since most of the regions in the fundus image\nare background or useless regions, we want our model to focus\non the ophthalmic disease regions of interest so as to reduce the\ncomputational complexity.\nAs for the imaging angle, unlike the traditional object detection\nmethod, which ﬁrst generates many candidate boxes on the image,\nand then adjusts the oﬀset of the boxes according to the calculated\nloss between the predicted results and the real labels. This method\nhas strong traces of artiﬁcial design, which is diﬀerent from the\nway humans observe objects, and must require post-processing\nmethods like Non-maximum suppression (NMS) (Neubeck and Van\nGool, 2006), whose drawbacks are inevitable in the face of large\noverlapping areas between real labels. The DETR (Carion et al.,\n2020) structure, on the other hand, views the object detection from\nthe perspective of set prediction, calculates the loss through the\ndichotomous graph matching method, and the Transformer structure\nhas global information, which is more in line with the way humans\nFrontiers in Neuroscience 02 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 3\nLi et al. 10.3389/fnins.2023.1130609\nFIGURE 1\nFundus images selected from iChallenge-PM dataset. The dataset contains three types of 2D fundus images (NM, HM, PM).\nobserve objects. Therefore, we choose the DETR structure for further\noptimization in the face of sensitive location information.\nFinally for the insuﬃcient data mentioned above, Ghiasi et al.\n(2021) used data augmentation to address the insuﬃciency of\nthe training data for classifying PM. Almahairi et al. (2018) used\nCycleGAN with cycle consistency to generate more realistic and\nreliable images for training, a res-guided U-Net is constructed for\nsegmentation, they achieved superior result on PM detection. We\nadopt variety of strong data augmentation to enrich iChallenge-PM\ndataset while training our model. Unlike the common two-branch\nhead design, inspired by DETR, we use a feed forward network that\ntakes the output from Transformer decoder, which will produce the\nbox coordinates and a matrix for classiﬁcation if the query has an\nobject, no post-processing is needed.\nThe main contributions of this paper are summarized as follows:\n(1) Inspired by DETR, we present a novel post-processing free\nobject detector that uses fundus image data for pathological myopia\ndiagnosis. For speciﬁc, we design an attentional FPN that uses\nobject queries on feature maps of each level of FPN, the self-\nattention mechanism increases the feature intensity gap between\nforeground and background. Due to the architecture of DETR, it\ncan well solve the challenge of morphology irregularity. To the best\nof our knowledge, this is the ﬁrst work using object detection for\npathological myopia diagnosis based on iChallenge-PM dataset.\n(2) Several data augmentation methods are used on the\niChallenge-PM dataset to accelerate model convergence and enhance\nmodel robustness.\n(3) Extensive experiments are conducted on iChallenge-\nPM dataset for discriminating NM, HM, and PM. The results\ndemonstrate the superiority of our method than other state-of-the-art\n(SOTA) object detectors.\nThe rest of this paper is organized as follows. Section “Related\nworks” introduces related works of the deep learning based retinal\ndisease analysis methods. Section “Materials and methods” illustrates\nthe details of our proposed MyopiaDETR model, which comprizes\nof Swin Transformer (Liu et al., 2021) backbone, attentional FPN,\nTransformer encoder, and decoder, shared feed forward network for\nspeciﬁc retinal disease analysis tasks. Section “Experimental results”\ndescribes the experimental results of ablation studies and comparison\nstudies. Finally, section “Discussion” has a discussion about our\nmethod and section “Conclusion” presents the conclusions of this\npaper and expounds ideas of future work.\nRelated works\nDeep learning based object detection\nObject detection is a popular task in computer vision and is\nwidely applied in many real-world scenes such as autonomous\ndriving, video surveillance, remote sensing, and medical diagnosis.\nThe main task of object detection is to locate and classify the\ntarget of interest from an image. In the context of the rapid\ndevelopment of computing power, deep learning has been researched\nand applied as never before. In the trajectory of vision model,\nAlexNet (Krizhevsky et al., 2017) opened a new era of computer\nvision by using convolutional neural network years ago, making\nCNN architecture the mainstream approach of deep learning for\nmany years. Object detectors can be divided into anchor-based\nand anchor-free model based on whether or not anchor is used\nduring the detection pipeline. The anchor based models can be\nfurther divided into two-stage and one-stage detector. One-stage\nmodel predicts bounding boxes on grid while two-stage model uses\na proposal network to generate candidate boxes, and then uses a\nsecond network to reﬁne the result. The advantage of one-stage\ndetectors is that it can complete localization and classiﬁcation by\ngoing through the network once, hence the one-stage detectors can\noﬀer signiﬁcant advantages in terms of speed, such as SSD (Liu\net al., 2016) and YOLO (Redmon et al., 2016; Redmon and Farhadi,\n2017, 2018; Bochkovskiy et al., 2020) series. Two-stage models\nsacriﬁce speed for obtaining high accuracy, most of the mainstream\ndetectors with high performance adopt two-stage methods, such as\nFaster R-CNN (Ren et al., 2015) and Cascade R-CNN (Cai and\nVasconcelos, 2018). With the trend of Transformer architecture\ngradually unifying natural language processing (NLP) and computer\nvision (CV), Vision Transformer (Dosovitskiy et al., 2020) (ViT) are\ngradually dominating visual tasks. The excellent relational modeling\ncapability of self-attention mechanism is bringing feature extraction\nto a new era. Pyramid Vision Transformer brought pyramid structure\nFrontiers in Neuroscience 03 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 4\nLi et al. 10.3389/fnins.2023.1130609\nFIGURE 2\nOverall architecture of our MyopiaDETR.\nFIGURE 3\nThe architecture of transformer encoder and decoder.\nin to Transformer, making it seamlessly accessible to a variety of\ndownstream tasks (e.g., object detection, semantic segmentation).\nSwin Transformer (Swin-T) proposed a vision transformer with\nsliding window operation and hierarchical design, achieved state-of-\nthe-art in many tasks. The hierarchical design makes feature fusion\neasier. Swin Transformer is an improved version based on ViT, which\nhas a similar structure of CNN. The hierarchical structure of Swin\nTransformer is more suitable to be applied to many downstream\ntasks. While ViT is a straight structure, it does not change the\ndimension of input feature map. In addition, the resolution of fundus\nimage is generally large, and many rich semantic features will be lost\nif the image shape is resized to a range that is acceptable to ViT.\nFrontiers in Neuroscience 04 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 5\nLi et al. 10.3389/fnins.2023.1130609\nFIGURE 4\nThe visualized results after data augmentations. Subplot (A) is the sample of fundus images in iChallenge-PM dataset, the white grid line added to the\noriginal graph is to better show the effect of different augmentation. Subplot (B–E) represents elastic transform, grid distortion, random rotate, and grid\nmask, respectively.\nTABLE 1 The effects of attentional FPN.\nModel AP50 APS APM APL Epochs FPS MS/SS\nDETR 120e 62.53 29.52 65.93 96.18 120 19.31 SS\nDETR 200e 71.21 39.12 74.89 97.62 200 19.31 SS\nDETR 300e 71.87 39.69 75.47 97.81 300 19.31 SS\nDETR + FPN OOM ñ ñ ñ ñ ñ SS\nUP-DETR 76.08 44.13 81.77 98.03 300 16.21 SS\nDeformable DETR 80.53 58.92 82.55 98.76 120 17.86 MS\nConditional DETR 84.29 65.35 83.73 99.10 120 16.60 MS\nOurs 85.18 68.71 83.66 99.34 120 8.73 MS\nBold values represent the best metric values achieved by our method and other comparison methods.\nSwin Transformer can receive larger image resolution, which means\nthe Swin Transformer can do better in processing images with large\nresolution than ViT model. Thus, we choose Swin Transformer as our\nfeature extraction backbone network.\nDeep learning based eye diseases\ndetection\nThe object in medical images usually have small sizes and\ncertain morphological features. Many studies utilize deep learning\ntechniques to diagnose eye diseases. Early work such as (Liu et al.,\n2010) developed a system called PAMELA (Pathological Myopia\nDetection by Peripapillary Atrophy) that automatically identiﬁes\npathological myopia in retinal fundus images. (Wen et al., 2020)\nplaced the key research on the distinction between pathological\nmyopia and high myopia, a two-branch network is proposed, where\nthe ﬁrst branch distinguishes between normal and abnormal, while\nthe other branch classiﬁes pathological myopia and high myopia.\nSpeciﬁcally, the previous studies on iChallenge-PM dataset have been\nrelated to image classiﬁcation and instance segmentation. Cui et al.\n(2021) used data augmentation to address the insuﬃciency of the\ntraining data for classifying PM. Zhang et al. (2021) used CycleGAN\nwith cycle consistency to generate more realistic and reliable images\nfor training. A res-guided U-Net is also constructed for segmentation,\nthey achieved superior result on PM detection. Our work ﬁrst use\nan object detection method for classifying and locating the atrophy\nbased on retinal fundus images. From the previous studies, we can\nknow that the majority of automatic diagnose method uses CNN\narchitecture for feature extraction, and to the best of our knowledge,\nour work is the ﬁrst that uses Transformer architecture as backbone\nand also the ﬁrst post-processing-free end-to-end detector in myopia\ndiagnosis.\nAttention mechanism\nWith the trend of Transformer architecture gradually unifying\nNLP and CV (Gumbs et al., 2021), Vision Transformer is gradually\ndominating the visual tasks. In the evolution of vision attention\nmechanism, common method can be embedded into CNN for\nbuilding more relevant feature, such as soft-attention. Soft-attention\nis a continuous distribution problem, focusing more on spatial or\nchannel, it can be divided into spatial attention and channel attention.\nNon-local ﬁrst utilized the idea of Transformer in computer vision\nmodel, it takes the approach of doing attention on the feature maps\nof the intermediate layers, which greatly avoids the computational\ncost. Self-attention based model, such as ViT and Swin Transformer,\nhas excellent capabilities in extracting relationships between image\npatches, building the connections of those that are mostly relevant\nto each other. The pioneer work DETR is worth noting, it applies\nTransformer architecture as an end-to-end object detector, the\npost-processing-free design is more compatible with human visual\npatterns and also avoid the unstable performance of NMS. In the\nretinal fundus image scenario, the cluttered tissue makes it more\ndiﬃcult to extract the feature of atrophy, so we believe that the\nself-attention mechanism can help the model to better capture the\ndiﬀerence between foreground and background.\nFrontiers in Neuroscience 05 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 6\nLi et al. 10.3389/fnins.2023.1130609\nTABLE 2 Performance improvement on various augmentations.\nOriginal\nimage\nElastic\ntransform\nGrid\ndistortion\nRandom\nrotate\nGrid\nmask\nAP50\n√ 80.45\n√ √ 81.81\n√ √ √ 82.33\n√ √ √ √ 85.11\n√ √ √ √ √ 85.18\nBold values represent the best metric values achieved by our method and other comparison\nmethods.\nMaterials and methods\nDiagnosing myopia by detecting lesions based on fundus images\nrequires suﬃcient data for the deep learning model to have a steady\nperformance (Peng et al., 2019; Virmani et al., 2019). The iChallenge-\nPM dataset released by Baidu encourage the data-driven methods to\nautomatically detect fundus lesion, it contains three types of fundus\ndiseases images and lesion masks, as illustrate in Figure 1, previous\nstudies try to design general deep learning based methods to address\nthe fundus disease identiﬁcation and localization problems, which\ncontain image classiﬁcation and segmentation that are insuﬃcient\nand slow, respectively. In order to implement object detection on\nthis dataset, we transform the mask of lesions into bounding box by\nobtaining the length and width of the mask. It can be clearly found\nfrom the sample images that the region occupied by the disease lesion\nis only a small part of the fundus image, and the rest of the tissue\nwithout lesions can be considered as redundant information, which\nwill be detrimental to the feature extraction and representation of\nthe model. That’s why we choose DETR as our baseline model, detail\nimprovements are as follows.\nOverall architecture\nInspired by DETR, the overall architecture is illustrated as\nFigure 2, the main components are: backbone network, attentional\nFPN, Transformer Encoder-Decoder, and a detection head, in which\nthe backbone is responsible for feature extraction, the feature maps of\nthree stages with diﬀerent resolutions are fed into attentional FPN for\nfeature aggregation. The outputs of attentional FPN are further sent\nto the encoder-decoder architecture, which consists of multi-head\nself-attention mechanism, layer normalization and a feed forward\nneural network, the details are similar to the DETR. Positional\nencoding is also adopted for retaining positional information of the\nfeature blocks, while the object queries are used for information\naggregation, give more attention to the positions where objects are\nlikely to appear. The detection head is in charge of classifying the\noutput of the decoder and ﬁnally getting the detection result. In\nparticular, the yellow circles in the Figure 2 represent self-attention\nmodule, which calculates the similarity while fusing feature maps.\nThe red circles represent multiple layers of features for fusion. Small\nsquares of diﬀerent colors in object queries represent diﬀerent query\nobjects. As the feature map ﬂows from encoder to decoder, the feature\nrepresentation becomes clearer and the purple squares become\ndarker. The ﬁnal feed forward network (FFN) is shared to calculate\nthe object box position and category attributes of the object query.\nDuring the training phase, the image batch is fed into the backbone\nto obtain feature maps of four stages and reduce the computational\noverhead, three relatively small feature maps are selected and fed into\nthe detection neck for integration. In the upsampling process of the\nneck network, self-attention operation is added to obtain features\nwith better feature representation ability. Then the aggregation of the\nneck feature is sent to the following encoder part. In the detection\nhead, FFN takes the output of decoder as input and utilizes bipartite\nmatching loss (i.e., Hungarian Maximum Matching algorithms) to\ncalculate the corresponding loss values. During inference phase, the\nlearned object query generates box candidates through Transformer\ndecoder to select boxes with larger conﬁdence as the ﬁnal prediction\nresult.\nModel backbone\nIn the overall model architecture, the backbone is in charge\nof feature extraction of the image. The mainstream architecture\nof backbone are mainly divided into CNN [AlexNet, ResNet (He\net al., 2016), Res2Net (Gao et al., 2019), ResNeXt (Xie et al.,\n2017), ConvNeXt (Liu et al., 2022)] and Transformer (e.g., ViT,\nSwin Transformer). The translational invariance and localization of\nCNNs provide inductive bias, makes CNN models converge faster,\nhowever, the ﬁxed receptive ﬁeld limits the global view of convolution\noperation. The positional encoding enables Transformer based\nnetwork to obtain better capabilities in learning global dependencies.\nSwin Transformer ﬁrst splits the image into small patches and then\nfeed each patch as a token into Transformer encoder, the core idea is\nto calculate the similarity between patches for training an attention-\nintensive network without convolution operations. Compare with\nthe Swin Transformer, the CNNs possess inductive bias, and their\nconvergence speed is relatively fast. The advantages of inductive\nbias are reﬂected at two aspects: (1) the convolutional kernel size\nis generally ﬁxed which result in high local correlation existed in\nfeature maps; (2) the feature maps generated by the CNNs have\ncharacteristics of translational invariance, which indicates the output\nof the convolutional layer does not change no matter where the object\nappears in the image.\nSince our neck network also uses the architecture of the self-\nattention mechanism, Swin Transformer is selected as our backbone\nso as to keep the consistency of the features representation. Moreover,\nthe Transformer architecture has good parallel computing capability\nand global view characteristics. When we input a fundus image with\nthe shape of H ×W ×C, it will ﬁrst pass through a patch partition\nmodule with the purpose of descending and chunking the image.\nThe output gets a sequence of N ×(P2 ×C) spreading 2D image\nblocks, where N is the number of image blocks, P2 is the area of each\npatch, H and W are the height and width of the image, respectively,\nand C is the number of image channels. Here we set P to 4, and\nN is computed by H/4 ×W/4. In summary, an input image with\nthe shape H ×W ×C passes through the patch partition module\nand outputs a tensor of H/4 ×W/4 ×48, which can be understood\nas a total of H/4 ∗W/4 image patches, each of which is composed\nof a 48-dimensional token. The W-MSA and SW-MSA modules\nhelp the Swin Transformer to improve its ability of extracting the\nglobal features in the fundus image. The W-MSA module restricts\nthe receptive ﬁeld of the model by only applying the self-attentive\nmechanism within each patch. The SW-MSA model adds a cyclic shift\noperation to the W-MSA for extracting the features between patches.\nFrontiers in Neuroscience 06 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 7\nLi et al. 10.3389/fnins.2023.1130609\nFIGURE 5\nThe comparison between traditional object detection algorithm Faster R-CNN and MyopiaDETR. Subplot (A) shows the test image, subplot (B) shows the\nPM location labels, subplot (C) shows the output of the Faster R-CNN algorithm, and subplot (D) shows the output of our MyopiaDETR.\nAs illustrated by Figure 2. The Swin Transformer backbone network\nhas four stages corresponding to four feature maps of diﬀerent sizes.\nThe feature map sizes are H/4 ×W/4 ×C, H/8 ×W/8 ×2C,\nH/16 ×W/16 ×4C, H/32 ×W/32 ×8C. The feature map of each\nlayer will be fed to the subsequent attentional FPN structure for\nfurther processing.\nAttentional FPN\nDue to the high computational complexity of the Transformer\narchitecture, which is of O(n2) level, only the feature map of the\nlast layer (32 times down sampling) is utilized in the original DETR,\nresulting in the loss of small object feature information and has the\nlimitation of single scale in feature representation. To address the\nabove issues, we added the attentional FPN architecture to utilize all\nthe features that output from the backbone network. The output of\nthe ﬁrst layer of the attentional FPN, which is also the last stage output\nfrom the backbone, is down sampled by 32 times. And then, for each\nneck level, the feature size is upsampled by 2 times. As the structure\nillustrated in Figure 2, the feature size of each neck level is 16 ×16,\n32 ×32, and 64 ×64 when the input size is 512 ×512. Self-attention\nis aggregate to the FPN to focus on the regions of interest in each layer\nof the feature map. The feature map of each layer will be sliced into\n8 times ×8 times pixels blocks, and the results computed between\nthe blocks are used as weights for the output, the ﬁnal computation\nis used to activate the part of interest in the feature map. Speciﬁcally,\nwe add an auxiliary head after the attentional FPN to distinguish the\nforeground from the background, and only the foreground region\nwill be input to the follow-up Transformer structure. The attentional\nFPN outputs the ﬁnal blocks while recording the sparse encoding\nmatrix of them, which is fed to the Transformer Decoder to map the\nfeatures back to the original image.\nSince each feature map is partitioned into 8 times ×8 times\npixels blocks, there will be a total of 64 pixel blocks after the slice.\nLet’s deﬁne the input x = ( x1, x2,..., x64). The input elements will\nbe passed through an embedding layer W to obtain a multiset of\none-dimensional vectors, denoted as a = (a1, a2,..., a64). Meanwhile,\nthree learnable matrices are also set as WQ, WK , WV , represent the\nquery matrix, the key matrix, and the value matrix, respectively.\nIn particular for a single input xi, the proportion of its weights is\ncalculated by the following formula:\nqi =WQWxi =WQai (1)\nki =WK Wxi =WK ai (2)\nvi =WV Wxi =WV ai (3)\nαi,64 = qT\ni ·ki√\ndq,k\n,˜αi,64 =Softmax(αi,64) (4)\nbi =\n64∑\ni=1\n˜αi,64·vi (5)\nThe subscript (i, 64) of α in the above equation (4), represents that\nthe similarity of the i-th input patch is currently being calculated,\nand there are a total of 64 patches to be calculated. The purpose of\ndividing by root dq,k when calculating α is for normalization to avoid\ngradient vanish and dq,k means the dimension of q and k vector. The\nsoftmax function is to map the sum of the weight ratios to 1, which is\nconvenient for calculating bi.\nFrontiers in Neuroscience 07 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 8\nLi et al. 10.3389/fnins.2023.1130609\nTransformer encoder-decoder\nThe structure of Transformer encoder-decoder is shown in\nFigure 3, details are as follows:\nTransformer encoder\nThe input requirement of Transformer encoder module is a\nsequence, so when getting the feature map output by attentional\nFPN, let the original feature map shape be C ×H ×W, we need\nto descend the channel dimension of the feature map ﬁrst, and then\nﬂatten the H and W dimension. Finally, we get the feature map with\nthe shape of C ×L, where L equals to H ×W. Each encoder block\nhas a uniﬁed structure: multi-head self-attention, Add and Norm and\nfeed forward network. As for Add and Norm structure, Add stands\nfor residual connection to prevent network degradation, Norm is\nthe layer normalization, which normalizes the activation values of\neach layer. Because of the invariance of Transformer Architecture,\nwe add the ﬁxed spatial positional encoding to the attention layer to\ncomplement the location information.\nTransformer decoder\nThe architecture of the decoder part is the same as the traditional\nTransformer, but the diﬀerence is that each decoder module of\nour model decodes N inputs from encoder in parallel. Because the\ndecoder structure is also permutation-invariant, the N inputs should\nbe diﬀerent so that diﬀerent results can be generated. The meaning\nof object queries is similar to that of anchor in traditional object\ndetection methods, and it is learnable. We input them to each multi-\nhead self-attention module, which will eventually be decoded into\nobject boxes location information and category information by the\nFFN structure, which is described in section “Detection head”.\nTo sum up, the features extracted from the backbone are\npassed through the multi-head self-attention module in the encoder\nstructure along with the spatial positional encoding. After that, the\nN outputs of encoder and object queries are fed to the decoder part.\nFinally, the ﬁnal object boxes and category information is output by\nthe multiple multi-head self-attention and decoder-encoder attention\nand the FNN structure.\nDetection head\nThe role of detection head in DETR is to predicting a ﬁxed set of\nobject detections for each input image, rather than using a sliding\nwindow or anchor-based approach in traditional object detection\nmodels. After obtaining the output of the decoder, the ﬁnal result\nis processed by a 3-layer perceptron and a linear projection layer,\nwhere the perceptron is comprized of a ReLU activation function.\nFNN computes the position of the box. The linear layer computes the\ncategory to which it belongs by softmax function. Since our predicted\nset is composed of N boxes, but in fact N is much larger than the\nactual number of objects present in the image, we mark the object\nquery with no detected object as a background class. In particular,\nour FFN share the same weights and are calculated equally for all\nN object queries.\nLoss function\nThe loss function calculation is performed in two steps. First,\nﬁnding the optimal pairwise loss between the ensemble prediction\nTABLE 3 Comparisons between different backbones.\nBackbone AP50 APS APM APL\nSwin-S 85.18 68.71 83.66 99.34\nSwin-B 86.23 69.76 84.65 99.35\nSwin-L 85.62 69.08 83.72 99.34\nResNet-50 82.25 65.91 80.34 98.72\nResNet-101 84.67 67.21 82.76 99.12\nResNet-152 86.32 69.65 84.78 99.21\nand the ground-truth label in the Hungarian algorithm alignment,\nthe index of the set of solutions is set to ˆσ, as illustrated in formula\n(6), where yiis the set of prediction, ˆyiis the ground-truth, both of\nthem need to be stretched to a ﬁxed length by adding None value,\nwhere length = max(len( yi), len( ˆyi)). The LMATCH is deﬁned as the\ngap between the predicted set and the ground-truth labels in the case\nof the ﬁrst group pairing.\nˆσ =argmin\n∑\nLMATCH(yi,ˆyi). (6)\nSecond, the index value ˆσ that calculated in the ﬁrst steps is used to\ncalculate the classiﬁcation loss and the predicted bounding box loss,\nci is the class label, and the Hungarian loss is calculated as:\nLHungarian(y,ˆy)=\nN∑\ni=1\n[−logˆpˆσ(i)(ci)+Lbox(bi,ˆbˆσ(i))] (7)\nwhere bounding box loss uses weighted IoU and L1 loss, λiou and λL1\nare the weight of IoU loss and L1 loss, respectively. The box loss is\ncalculated as:\nLbox(bi,ˆbˆσ(i))=λiouLiou(bi,ˆbˆσ(i))+λL1||bi −ˆbˆσ(i)||1 (8)\nExperimental results\nDataset description\niChallenge-PM\nMyopia has become a global public health burden. As the\nrefractive error of myopia increases, high myopia will progress\nto pathological myopia, causing irreversible visual damage to the\npatient. Therefore, early diagnosis and regular follow-up are very\nimportant. With this challenge, the iChallenge competition jointly\norganized by Baidu Brain and Zhongshan Eye Center of Sun Y at-\nsen University provides iChallenge-PM, a dataset on pathological\nmyopia, which includes 1,200 annotated retinal fundus images from\nnon-pathological myopic subjects and pathological myopic patients\n(about 50%). There are 400 training data, validation data, and\ntest data sets each.\nData augmentation\nWe adopt several data augmentation methods to address the\nchallenges of imaging angle and insuﬃcient data. Subplot (b) in\nFigure 4 shows the elastic transform, which was proposed by Simard\net al. (2003) and made great progress on the MNIST handwritten\ndataset, and the method is gradually applied to medical image\nprocessing and has been widely used [e.g., Mottl et al. (2002), Gulshad\net al. (2021)], for the parameter settings we set the alpha to 50 and the\nFrontiers in Neuroscience 08 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 9\nLi et al. 10.3389/fnins.2023.1130609\nFIGURE 6\nRetinal fundus image and the corresponding feature heatmap generated by different backbones, the heatmap of the feature learned by ResNet50,\nResNet101, and Swin-S from left to right.\nsigma to 5. Subplot (c) is grid distortion, its eﬀect is similar to that\nof the elastic transform, which is a non-rigid body transformation.\nSubmap (d) is random rotation data augmentation, which aims to\nincrease the diversity of imaging angles to solve the challenge of\nimaging angle, and the rotation angle is set from−180 to 180 degrees.\nSubplot (e) is the grid mask data augmentation proposed by Chen\net al. (2020), which randomly masks a number of block locations on\nthe image and ﬁlls them with 0 pixel values, this data augmentation\ncan mask part of the positive samples with certain probability, thus\npreventing the model from overﬁtting to simple local features, for\nthe parameter settings we set the ratio to 0.3 and the x_holes and\ny_holes to be set randomly between 5 and 7. The gains from each\nenhancement will be presented in the ablation study part.\nEvaluation metrics\nFor evaluating the performance of our detection method on\niChallenge-PM dataset, we use mAP as the evaluation metrics, which\nis the mean average precision of all categories. AP θ is calculated\nas the area enclosed by the Precision (P) and Recall (R) and the\ncoordinate axis at an IoU (Intersection over Union of predicted box\nand ground-truth) threshold of θ, as illustrated below:\nIoU(A, B)=A ⋂B\nA ⋃B (9)\nP = TP\nTP +FP (10)\nR = TP\nTP +FN (11)\nwhere TP (True Positive) is the number of IoU between the predicted\nbox and the ground-truth label that is greater than or equal to\nthe threshold θ, while FP (False Positive) means the number of\nIoU between the predicted box and the ground-truth label that is\nless than the threshold θ. FN (False Negative) means no positive\nobject is detected. Finally, we can get mAP by averaging AP θ at\ndiﬀerent thresholds.\nImplementation details\nOur model is implemented using MMDetection object detection\nalgorithm library based on Pytorch1.8 deep learning framework using\nfour NVIDIA RTX 3090 GPUs. We pre-trained our model on COCO\ndataset for 36 epochs and ﬁne-tuned on iChallenge-PM for 100\nepochs with a mini-batch size of 16 due to the limited data amount.\nThe learning rate is initiated to 0.001, and we use CosineAnnealingLR\nto decay the learning rate with 5 epochs warm-up. The AdamW\noptimizer is used to optimize the hyper-parameters. For evaluating\nmodel performance, the IoU threshold and conﬁdence threshold are\nset to 0.5 and 0.01, respectively. NMS is adopted as post-process\nmethod. As for the data augmentation implementation approach, we\nadopt the image processing algorithms based on opencv-python and\nAlbumentations library (Buslaev et al., 2020).\nAblation study\nTo verify the eﬀectiveness of our attentional FPN, we set up a\nset of ablation experiments. The backbone network used as the base\nmodel was Swin-Small, DETR and its variants are selected to test the\ngains we obtained by adding attentional FPN. In object detection,\ntaking COCO object deﬁnition as an example (Kisantal et al., 2019),\nwe deﬁne a small object as an object which pixel number is less\nthan 32 ×32, a medium object is an object which pixel number is\nbetween 32∗32 and 96∗96, and a large object is an object which pixel\nnumber is larger than 96 ∗96. AP S means object area smaller than\n32 ×32 pixel points, AP M means object area between 32 ×32 and\n96 ×96 pixel points, AP L means object area bigger than 96 ×96\npixel points. As can be seen from Table 1, the DETR works well for\nlarge target detection, but suﬀers from a major shortcoming in small\nobject detection. If the FPN is added directly to the DETR structure,\nFrontiers in Neuroscience 09 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 10\nLi et al. 10.3389/fnins.2023.1130609\nsince the Tranformer has an O(n 2) computational complexity and\nlimited computational resources, this method will result in an out\nof memory (OOM) error. In contrast, by adding a self-attention\nmechanism to the FPN, only a small number of features need to\nbe fed into the Transformer structure, and our model achieves\n29.02% improvement in detecting small objects, and obtains 8.19\nand 1.53% improvement in detecting medium and large object,\nrespectively. It is worth noting that Deformable DETR, Conditional\nDETR and our method achieve more signiﬁcant improvement with\nmulti-scale (MS) than the other models with single-scale (SS) design.\nThe result indicates that multi-scale learning allows greater variety\nof features. With Attentional FPN, our model not only achieves\na signiﬁcant improvement in capturing small objects, but also a\nsigniﬁcant improvement in convergence speed during training. In\na nutshell, our model not only achieves a signiﬁcant improvement\nin capturing small objects, but also a signiﬁcant improvement in\nconvergence speed during training. Note that all experiments adopt\nthe same data augmentation methods.\nFurthermore, it is important to carefully evaluate the impact\nof data augmentation on model performance, which can provide\ninsights into the factors that contribute to model performance.\nIn Table 2 , we focus on the AP boost from the augmentation\nmethod, presenting in a cumulative manner. As can be seen evidently,\nRandom Rotate brings the most signiﬁcant performance gains\nbecause it simulates a situation that is similar to the characteristics\nof the original data (i.e., the lesion sites tend to appear in diﬀerent\ndirections), this greatly expands the training sample. The other\naugmentation methods also achieved considerable improvements,\nproving that the increment of data volume by the data augmentation\nmethods is eﬀective to improving model performance.\nComparison study\nSubplot (a) in Figure 5 shows the test image, subplot (b)\nshows the PM location labels, subplot (c) shows the output of the\nFaster R-CNN algorithm, and subplot (d) shows the output of our\nMyopiaDETR. The detection result in the ﬁrst row of subplot (c)\nfails to detect the small object compared to the ground truth, and\nthe traditional object detection algorithm has limited learning of\ncomplex morphological features. The yellow box in the second row\nof subplot (c) is the model’s false detection, and if the threshold of\npost-processing of NMS is adjusted down, it will lead to the purple\nor green box being removed by the post-processing algorithm, and\nthe detection ability is still very poor for irregular PM regions. Our\nMyopiaDETR does not have these problems, and not only can handle\nthe detection of irregular PM, but also do not have to worry about the\nfalse removal of detection boxes caused by post-processing.\nIn addition, we compare the eﬀect of diﬀerent backbone networks\non the performance of our model. As shown in Table 3 , in the\nsmall to medium sized network architecture with a similar number of\nparameters, Swin Transformer shows better performance, we believe\nthis is because the feature extracted by Swin Transformer is consistent\nwith the operations in attentional FPN. Due to the morphology\nirregularity, we need a feature extraction network with strong\ncapability to capturing global context feature. Swin Transformer has\nbetter feature extraction capability than ResNet because of its global\nﬁeld of view. Thus, the Swin Transformer avoids the problem of\nmorphology irregularity and shows a better detection performance\nthan the ResNet. However, because the Transformer architecture\nlacks inductive bias, as Swin Transformer becomes deeper, it requires\na lot more data and the model performance is somewhat weakened.\nCompared to the CNN structure, the global feature representation\ncapability of Transformer is more prominent, and Swin Transformer\nperformance has a signiﬁcant advantage over the CNN structure in\nthe case of small model structures.\nFigure 6 shows the 8 ×down sampled feature maps of ResNet-\n50, ResNet-101, and Swin-S. As the CNN is more localized, it will\nto some extent activate non-focal regions, such as the cross-focused\nsymbols in the ﬁgure. The Swin Transformer structure, on the other\nhand, has a global ﬁeld of view and can focus on more important\ninformation (e.g., lesion borders as well as slice edge contours). As\nshown in Figure 6 , Swin Transformer has a better understanding\nof the global information and can pay more attention to the global\ncontour information. ResNet, on the other hand, has a strong\nfeature extraction capability, but it only pays attention to the local\ncontour information and has more activation within the local contour\ninformation. In contrast, we do not need to pay attention to all the\ninformation within the local contour information when dealing with\nthe ophthalmic disease region, thus highlighting the superiority of\nSwin Transformer in feature extraction.\nDiscussion\nThe sources of novelty in our work are: (1) Ordinary deep\nlearning-based object detection methods utilize many modules with\nobvious traces of artiﬁcial design, that is, a paradigm that does not\nmatch the way human vision ﬁnds objects. (2) DETR proposes a\nnew object detection paradigm based on ensemble prediction, which\ndirectly predicts all objects in an image without post-processing,\nwhich is more in line with human visual habits. (3) Due to the high\ncomputational complexity of Transformer and the fact that DETR\nonly uses the output features of the last layer of the model, which\ncontains rich semantic information but is weak in the representation\nof local information, the performance in small object detection is\npoor, so we propose attentional FPN for feature aggregation, which\nuses all the output features of the backbone network, signiﬁcantly\nimprove the performance in small object detection.\nHowever, although our proposed new method solves the problem\nof small object detection as well as purifying the background features,\nthere are still some drawbacks. The use of Swin Transformer as\nbackbone makes the training time much longer than traditional\nDETR model. Speciﬁcally, in DETR, each query is responsible for a\npart of the location and size of the object in an image, which requires\nall objects in all images in the training set to be well apportioned to\ndiﬀerent queries, so more epochs are needed. The data amount is\nvery important for deep Transformer architecture model. Our follow-\nup research is considering to replace the backbone network with an\noptimized architecture to adapt to scenarios with small data volumes.\nConclusion\nIn this paper, we propose attentional FPN and use a new\nparadigm for object detection to solve the eye disease detection\nchallenges based on 2D fundus images. The experimental results\nshow that our attentional FPN can be adapted to other deep learning\narchitectures with only a small increment in computational cost to\nachieve a signiﬁcant accuracy improvement. Several augmentation\nFrontiers in Neuroscience 10 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 11\nLi et al. 10.3389/fnins.2023.1130609\nmethods are utilized to improve the data volume and make our model\nachieve considerable performance improvements, proving that the\nincrement of data volume by the data augmentation methods is\neﬀective to detecting lesion area in 2D fundus images. Our model\nnot only achieves a signiﬁcant improvement in capturing small\nobjects, but also a signiﬁcant improvement in convergence speed\nduring training.\nData availability statement\nPublicly available datasets were analyzed in this study. This data\ncan be found here: https://aistudio.baidu.com/aistudio/datasetdetail/\n177172.\nAuthor contributions\nML, SL, and ZJW contributed to the statistical analyses and wrote\nthe manuscript. ZHW, ZY , and XL contributed to the data collection\nand ﬁgures plot. RZ provided the technique and wrote the guidance.\nAll authors read and approved the ﬁnal manuscript, contributed to\ndata collection and article, and approved the submitted version.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the reviewers.\nAny product that may be evaluated in this article, or claim that may\nbe made by its manufacturer, is not guaranteed or endorsed by the\npublisher.\nReferences\nAlmahairi, A., Rajeshwar, S., Sordoni, A., Bachman, P., and Courville, A. (2018).\n“Augmented cyclegan: Learning many-to-many mappings from unpaired data, ” in\nProceedings of the international conference on machine learning (PMLR), Stockholm,\n195ñ204.\nBochkovskiy, A., Wang, C.-Y., and Liao, H.-Y. M. (2020). Yolov4: Optimal speed and\naccuracy of object detection. arXiv [Preprint]. arXiv:2004.10934.\nBuslaev, A., Iglovikov, V. I., Khvedchenya, E., Parinov, A., Druzhinin, M., and Kalinin,\nA. A. (2020). Albumentations: Fast and ﬂexible image augmentations. Information\n11:125. doi: 10.3390/info11020125\nCai, Z., and Vasconcelos, N. (2018). “Cascade R-CNN: Delving into high quality\nobject detection, ” in Proceedings of the IEEE conference on computer vision and\npattern recognition , (Salt Lake City: IEEE), 6154ñ6162. doi: 10.1109/CVPR.201\n8.00644\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.\n(2020). “End-to-end object detection with transformers, ” inProceedings of the European\nconference on computer vision, (Cham: Springer), 213ñ229. doi: 10.1007/978-3-030-\n58452-8_13\nChen, P., Liu, S., Zhao, H., and Jia, J. (2020). Gridmask data augmentation. arXiv\n[Preprint]. arXiv:2001.04086.\nCui, J., Zhang, X., Xiong, F., and Chen, C.-L. (2021). Pathological myopia image\nrecognition strategy based on data augmentation and model fusion. J. Healthc. Eng.\n2021:5549779. doi: 10.1155/2021/5549779\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale.\narXiv [Preprint]. arXiv:2010.11929.\nFurtado, J. M., Lansingh, V. C., Carter, M. J., Milanese, M. F., Peña, B. N., Ghersi,\nH. A., et al. (2012). Causes of blindness and visual impairment in Latin America. Surv.\nOphthalmol. 57, 149ñ177.\nGao, S.-H., Cheng, M.-M., Zhao, K., Zhang, X.-Y., Y ang, M.-H., and Torr, P. (2019).\nRes2net: A new multi-scale backbone architecture. IEEE Trans. Pattern Anal. Mach.\nIntell. 43, 652ñ662. doi: 10.1109/TPAMI.2019.2938758\nGhiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk, E. D., et al. (2021).\n“Simple copy-paste is a strong data augmentation method for instance segmentation, ”\nin Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\nNashville, TN, 2918ñ2928. doi: 10.1109/CVPR46437.2021.00294\nGulshad, S., Sosnovik, I., and Smeulders, A. (2021). Built-in elastic transformations for\nimproved robustness. arXiv [Preprint]. arXiv:2107.09391.\nGumbs, A. A., Frigerio, I., Spolverato, G., Croner, R., Illanes, A., Chouillard, E., et al.\n(2021). Artiﬁcial intelligence surgery: How do we get to autonomous actions in surgery?\nSensors 21:5526. doi: 10.3390/s21165526\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, Las Vegas, 770ñ778.\nHsu, W.-M., Cheng, C.-Y., Liu, J.-H., Tsai, S.-Y., and Chou, P. (2004). Prevalence and\ncauses of visual impairment in an elderly Chinese population in Taiwan: The Shihpai eye\nstudy. Ophthalmology 111, 62ñ69. doi: 10.1016/j.ophtha.2003.05.011\nIwase, A., Araie, M., Tomidokoro, A., Y amamoto, T., Shimizu, H., Kitazawa, Y., et al.\n(2006). Prevalence and causes of low vision and blindness in a Japanese adult population:\nThe Tajimi study. Ophthalmology 113, 1354ñ1362. doi: 10.1016/j.ophtha.2006.04.022\nKisantal, M., Wojna, Z., Murawski, J., Naruniec, J., and Cho, K. (2019). Augmentation\nfor small object detection. arXiv [Preprint]. arXiv:1902.07296.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). Imagenet classiﬁcation with\ndeep convolutional neural networks. Commun. ACM60, 84ñ90. doi: 10.1145/3065386\nLin, T.-Y., Dollar, P., Girshick, R., He, K., Hariharan, B., and Belongie, S. (2017).\n“Feature pyramid networks for object detection, ” inProceedings of the IEEE conference on\ncomputer vision and pattern recognition, Honolulu, HI, 2117ñ2125. doi: 10.1109/CVPR.\n2017.106\nLiu, J., Wong, D. W., Lim, J. H., Tan, N. M., Zhang, Z., Li, H., et al. (2010). Detection of\npathological myopia by Pamela with texture-based features through an SVM approach.\nJ. Healthc. Eng.1, 1ñ11.\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., et al. (2016). “SSD:\nSingle shot multibox detector, ” in Proceedings of the European conference on computer\nvision, eds B. Leibe, J. Matas, N. Sebe, and M. Welling (Cham: Springer), 21ñ37.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021). “Swin transformer:\nHierarchical vision transformer using shifted windows, ” inProceedings of the IEEE/CVF\ninternational conference on computer vision, Nashville, TN, 10012ñ10022. doi: 10.7717/\npeerj-cs.1093\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. (2022). “A\nconvnet for the 2020s, ” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, New Orleans, LA, 11976ñ11986. doi: 10.1109/CVPR52688.2022.\n01167\nLu, W., Tong, Y., Yu, Y., Xing, Y., Chen, C., and Shen, Y. (2018). Applications of\nartiﬁcial intelligence in ophthalmology: General overview. J. Ophthalmol.2018:5278196.\nMeng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y., et al. (2021). “Conditional detr\nfor fast training convergence, ” inProceedings of the IEEE/CVF international conference on\ncomputer vision, Montreal, QC, 3651ñ3660.\nMottl, V., Kopylov, A., Kostin, A., Yermakov, A., and Kittler, J. (2002). “Elastic\ntransformation of the image pixel grid for similarity based face identiﬁcation, ”\nin Proceedings of the 2002 international conference on pattern recognition , Vol. 3,\n(Manhattan, NY: IEEE), 549ñ552.\nMuthukannan, P., and Glaret Subin, P. (2022). Optimized convolution neural network\nbased multiple eye disease detection. Comput. Biol. Med. 146:105648. doi: 10.1016/j.\ncompbiomed.2022.105648\nNazir, T., Irtaza, A., Javed, A., Malik, H., Hussain, D., and Naqvi, R. A. (2020). Retinal\nimage analysis for diabetes-based eye disease detection using deep learning. Appl. Sci.\n10:6185. doi: 10.3390/app10186185\nFrontiers in Neuroscience 11 frontiersin.org\nfnins-17-1130609 February 2, 2023 Time: 14:56 # 12\nLi et al. 10.3389/fnins.2023.1130609\nNeubeck, A., and Van Gool, L. (2006). “Eﬃcient non-maximum suppression, ” in In\nProceedings of the 18th international conference on pattern recognition (ICPR’06), Vol. 3,\n(Piscataway, NJ: IEEE), 850ñ855. doi: 10.1109/ICPR.2006.479\nPeng, Y., Dharssi, S., Chen, Q., Keenan, T. D., Agron, E., Wong, W. T., et al. (2019).\nDeepseenet: A deep learning model for automated classiﬁcation of patient-based age-\nrelated macular degeneration severity from color fundus photographs. Ophthalmology\n126, 565ñ575. doi: 10.1016/j.ophtha.2018.11.015\nRedmon, J., and Farhadi, A. (2017). “YOLO9000: Better, faster, stronger, ” inProceedings\nof the IEEE conference on computer vision and pattern recognition , Honolulu, HI,\n7263ñ7271. doi: 10.1109/CVPR.2017.690\nRedmon, J., and Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv\n[Preprint]. arXiv:1804.02767.\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). “You only look once:\nUniﬁed, real-time object detection, ” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, Las Vegas, 779ñ788. doi: 10.1109/CVPR.2016.91\nRen, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN: Towards real-time\nobject detection with region proposal networks. Adv. Neural Inf. Process. Syst.1:28.\ndoi: 10.1109/TPAMI.2016.2577031\nSakaguchi, A., Wu, R., and Kamata, S.-I. (2019). “Fundus image classiﬁcation for\ndiabetic retinopathy using disease severity grading, ” in Proceedings of the 2019 9th\ninternational conference on biomedical engineering and technology, Tokyo, 190ñ196.\nSimard, P. Y., Steinkraus, D., and Platt, J. C. (2003). “Best practices for convolutional\nneural networks applied to visual document analysis, ” in Proceedings of the 7th\ninternational conference on document analysis and recognition, Edinburgh, 3.\nVirmani, J., Singh, G. P., Singh, Y., and Kriti. (2019). PNN-based classiﬁcation of retinal\ndiseases using fundus images. Sens. Health Monit.5, 215ñ242.\nWen, Y., Chen, L., Qiao, L., Deng, Y., Dai, S., Chen, J., et al. (2020). “On automatic\ndetection of central serous chorioretinopathy and central exudative chorioretinopathy in\nfundus images, ” inProceedings of the 2020 IEEE international conference on bioinformatics\nand biomedicine (BIBM), (Manhattan, NY: IEEE), 1161ñ1165. doi: 10.1109/BIBM49941.\n2020.9313274\nWong, T. Y., Ferreira, A., Hughes, R., Carter, G., and Mitchell, P. (2014). Epidemiology\nand disease burden of pathologic myopia and myopic choroidal neovascularization: An\nevidence-based systematic review. Am. J. Ophthalmol.157, 9ñ25. doi: 10.1016/j.ajo.2013.\n08.010\nXie, S., Girshick, R., Dollar, P., Tu, Z., and He, K. (2017). “Aggregated residual\ntransformations for deep neural networks, ” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, Manhattan, NY , 1492ñ1500. doi: 10.1109/CVPR.\n2017.634\nY amada, M., Hiratsuka, Y., Roberts, C. B., Pezzullo, M. L., Y ates, K., Takano, S.,\net al. (2010). Prevalence of visual impairment in the adult Japanese population by cause\nand severity and future projections. Ophthalmic Epidemiol. 17, 50ñ57. doi: 10.3109/\n09286580903450346\nYou, Q. S., Xu, L., Y ang, H., Wang, Y. X., and Jonas, J. B. (2011). Five-year incidence of\nvisual impairment and blindness in adult Chinese: The Beijing eye study. Ophthalmology\n118, 1069ñ1075. doi: 10.1016/j.ophtha.2010.09.032\nZhang, Z., Ji, Z., Chen, Q., Yuan, S., and Fan, W. (2021). Joint optimization of\ncyclegan and cnn classiﬁer for detection and localization of retinal pathologies on color\nfundus photographs. IEEE J. Biomed. Health Inf.26, 115ñ126. doi: 10.1109/JBHI.2021.3\n092339\nZhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable\nDETR: Deformable transformers for end-to-end object detection. arXiv [Preprint].\narXiv:2010.04159.\nFrontiers in Neuroscience 12 frontiersin.org"
}