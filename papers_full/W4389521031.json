{
  "title": "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
  "url": "https://openalex.org/W4389521031",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3019274440",
      "name": "Luke Gessler",
      "affiliations": [
        "Georgetown University"
      ]
    },
    {
      "id": "https://openalex.org/A2096245607",
      "name": "Nathan Schneider",
      "affiliations": [
        "Georgetown University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4377864537",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3200936406",
    "https://openalex.org/W3168767730",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W4385573670",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4285290042",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3098466758",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2991265431",
    "https://openalex.org/W3004117589",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W1595256356",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3174413662",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W3105601320",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4287887329",
    "https://openalex.org/W3173854146",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W2132481658",
    "https://openalex.org/W1614298861"
  ],
  "abstract": "A line of work on Transformer-based language models such as BERT has attempted to use syntactic inductive bias to enhance the pretraining process, on the theory that building syntactic structure into the training process should reduce the amount of data needed for training. But such methods are often tested for high-resource languages such as English. In this work, we investigate whether these methods can compensate for data sparseness in low-resource languages, hypothesizing that they ought to be more effective for low-resource languages. We experiment with five low-resource languages: Uyghur, Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic inductive bias methods produce uneven results in low-resource settings, and provide surprisingly little benefit in most cases.",
  "full_text": "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 238–253\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n238\nSyntactic Inductive Bias in Transformer Language Models:\nEspecially Helpful for Low-Resource Languages?\nLuke Gessler Nathan Schneider\nDepartment of Linguistics\nGeorgetown University\n{lg876, nathan.schneider}@georgetown.edu\nAbstract\nA line of work on Transformer-based language\nmodels such as BERT has attempted to use syn-\ntactic inductive bias to enhance the pretraining\nprocess, on the theory that building syntactic\nstructure into the training process should re-\nduce the amount of data needed for training.\nBut such methods are often tested for high-\nresource languages such as English. In this\nwork, we investigate whether these methods\ncan compensate for data sparseness in low-\nresource languages, hypothesizing that they\nought to be more effective for low-resource lan-\nguages. We experiment with five low-resource\nlanguages: Uyghur, Wolof, Maltese, Coptic,\nand Ancient Greek. We find that these syn-\ntactic inductive bias methods produce uneven\nresults in low-resource settings, and provide\nsurprisingly little benefit in most cases.\n1 Introduction\nMany NLP algorithms rely on high-quality pre-\ntrained word representations for good performance.\nPretrained Transformer language models (TLMs)\nsuch as BERT/mBERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), XLM-R (Conneau\net al., 2020), and ELECTRA (Clark et al., 2020)\nprovide state-of-the-art word representations for\nmany languages. However, these models require\non the order of tens of millions of tokens of train-\ning data in order to achieve a minimum of quality\n(Micheli et al., 2020; Warstadt et al., 2020), a data\nrequirement that most languages of the world can-\nnot practically satisfy.\nThere are at least two basic approaches to ad-\ndressing this issue. The first, which is at least as old\nas BERT, exploits multilingual transfer to reduce\nthe data requirements for any individual language.\nThe second aims to reduce TLMs’ data require-\nments by modifying their architectures and algo-\nrithms. For example, Gessler and Zeldes (2022)\nmore effectively train low-resource monolingual\nTLMs with as few as 500K tokens by reducing\nmodel size and adding supervised pretraining tasks\nwith part-of-speech tags and syntactic parses.\nWe take up the latter direction in this work, look-\ning specifically at whether the addition of syntactic\ninductive bias (SIB) during the pretraining pro-\ncedure may help improve TLM quality in low-\nresource, monolingual settings. Specifically, we\nexamine two methods which have been proposed\nfor high-resource settings: the two syntactic con-\ntrastive loss functions of Zhang et al. (2022b), and\nthe modified self-attention algorithm of Li et al.\n(2021), wherein a modified self-attention mecha-\nnism, restricted so that tokens may only attend to\ntokens that are syntactically “local”, complements\nthe standard self-attention mechanism.\nAt a high level, SIB is of interest in the context of\nTLMs because of how crucial self-attention is for\nTLMs’ syntactic knowledge. In studies on an En-\nglish TLM, BERT, Htut et al. (2019) and Clark et al.\n(2019) show that while syntactic relations are not\ndirectly recoverable from self-attention patterns,\nmany self-attention heads seem to be sensitive to\nparticular syntactic relations, such as that of a direct\nobject or or a subject. But self-attention is com-\npletely unbounded: during pretraining, the model\nhas to learn from scratch how to decide which other\ntokens in an input sequence a token should attend\nto. We therefore observe that if SIB could be ef-\nfectively applied, then presumably self-attention\nweights would converge more quickly and learn\nmore effectively, since their behavior has been ob-\nserved to be so heavily syntactic in nature.\nMoreover, we expect that this effect would be\ngreater for low-resource languages, where the com-\nparative lack of data is known to hamper models’\nability to form robust linguistic representations.\nWe find additional motivation for our interest in\nSIB given the nearly universal view held by lin-\nguists that the human mind does not start with the\nequivalent of a totally unconstrained self-attention\n239\nmechanism: for example, psycholinguists such\nas Hawkins (2014) have extensively documented\nprocessing-related constraints on syntax, and Gen-\nerative linguists such as Ross (1967) have observed\nthat many syntactic constructions which might have\nbeen possible are in fact not attested in English or\nany other language, and postulate that these con-\nstructions are at least in some cases “impossible”\nbecause of biologically-determined properties of\nthe human mind. Our goal is therefore to give our\nmodels something like the constraints the human\nmind has in order to help them learn more effec-\ntively with less data.\nWe use a standard BERT-like TLM architecture\nas our base model, though we heavily reduce model\nsize, following the results of Gessler and Zeldes\n(2022) which showed that this is beneficial in low-\nresource monolingual settings. We pretrain TLMs\nfor five low-resource languages—Wolof, Coptic,\nMaltese, Uyghur, and Ancient Greek—varying\nwhich SIB methods are used. We then use Univer-\nsal Dependencies (UD) (Nivre et al., 2016) syntac-\ntic parsing and WikiAnn (Pan et al., 2017) named\nentity recognition as representative downstream\ntasks that allow us to assess the quality of our mod-\nels. Additionally, we evaluate our models using\nPrOnto (Gessler, 2023), a suite of downstream task\ndatasets for low-resource languages. We find that\nthese SIB methods are not very effective in low-\nresource languages, with small gains in some tasks\nand degradations or no effects in others. This is sur-\nprising given the intuition that SIB ought to help\nmore in low-resource settings, and we speculate\nthat other methods for SIB may be more effective\nin low-resource settings.\nWe summarize our contributions as follows:\n1. We conduct what is, to the best of our knowl-\nedge, the first work examining whether SIB is help-\nful for pretraining low-resource Transformer LMs.\n2. We reimplement SynCLM (Zhang et al., 2022b),\nSLA (Li et al., 2021), and MicroBERT (Gessler\nand Zeldes, 2022) in plain PyTorch and make it\nopenly accessible.1\n3. We present evidence from seven downstream\nevaluation tasks wherein the two SIB methods we\nexamine are basically ineffective in our experimen-\ntal settings, yielding only scattered and small gains.\n1Our code is publicly available at https://github.com/\nlgessler/lr-sib .\n2 Previous Work\nPretrained word representations have been essential\ningredients for NLP models for at least a decade,\nbeginning with static word embeddings such as\nword2vec (Mikolov et al., 2013b,a), GloVe (Pen-\nnington et al., 2014), and fastText (Bojanowski\net al., 2017). Contextualized word representations\n(McCann et al., 2018; Peters et al., 2018; Devlin\net al., 2019) from Transformer-based (Vaswani\net al., 2017) models have since overtaken them.\nThroughout this period, high-resource languages\nhave received the majority of attention, and al-\nthough interest in low-resource settings has in-\ncreased in the past few years, there remains a large\ngap (in terms of linguistic resources, pretrained\nmodels, etc.) between low- and high-resource lan-\nguages (Joshi et al., 2020).\n2.1 Multilingual Models\nThe first modern multilingual TLM was mBERT,\ntrained on 104 languages (Devlin et al., 2019).\nmBERT and other models that followed it, such as\nXLM-R (Conneau et al., 2020), demonstrated that\nmultilingual pretrained TLMs are capable of good\nperformance not on just languages represented in\ntheir training data, but also in some zero-shot set-\ntings (cf. Pires et al. 2019; Rogers et al. 2020,\namong others). But this is not without a cost: it\nhas been shown (Conneau et al., 2020) that when\na TLM is trained on multiple languages, the lan-\nguages compete for parameter capacity in the TLM,\nwhich effectively places a limit on how many lan-\nguages can be included in a multilingual model be-\nfore performance significantly degrades for some\nor all of the model’s languages. Indeed, the lan-\nguages which had proportionally less training data\nin XLM-R’s training set tended to perform more\npoorly (Wu and Dredze, 2020).\nA possible solution to this difficulty is to adapt\npretrained TLMs to a given target language, rather\nthan trying to fit the target language into an\never-growing list of languages that the model is\npretrained on. One popular method for doing\nthis involves expanding the TLM’s vocabulary\nwith additional subword tokens (e.g. BPE tokens\nfor RoBERTa-style models), which has been ob-\nserved to improve tokenization and reduce out-of-\nvocabulary rates (Wang et al., 2020; Artetxe et al.,\n2020; Chau et al., 2020; Ebrahimi and Kann, 2021),\nleading to downstream improvements in model per-\nformance. But these and other approaches struggle\n240\nwhen a language is very far from any other lan-\nguage that a multilingual TLM was pretrained on.\nMultilingual models like XLM-R which are\ntrained on over 100 languages could be described\nas massively multilingual models. A more recent\ntrend is to train multilingual models on just a few\nto a couple dozen languages, especially in low-\nresource settings. For example, Ogueji et al. (2021)\ntrain an mBERT on data drawn from 11 African\nlanguages, totaling only 100M tokens (cf. BERT’s\n3.3B), and find that their model outperforms mas-\nsively multilingual models such as XLM-R, pre-\nsumably because the African languages in ques-\ntion were quite unrelated to most of the languages\nXLM-R was trained on.\n2.2 Monolingual Models\nThere has been comparatively little work explor-\ning pretraining monolingual low-resource TLMs\nfrom scratch, and this lack of interest is likely ex-\nplainable by the fact that monolingual TLMs re-\nquire copious training data in order to be effective.\nSeveral studies have examined the threshold under\nwhich monolingual models significantly degrade,\nand all find that using standard methods, more data\nthan is available in “low-resource” settings (defi-\nnitionally, if we take “low-resource” to mean ‘no\nmore than 10M tokens’) is required in order to ef-\nfectively train a monolingual TLM. Martin et al.\n(2020) find at least 4GB of text is needed for near-\nSOTA performance in French, and Micheli et al.\n(2020) show further for French that at least 100MB\nof text is needed for “well-performing” models on\nsome tasks. Warstadt et al. (2020) train English\nRoBERTa models on datasets ranging from 1M\nto 1B tokens and find that while models acquire\nlinguistic features readily on small datasets, they\nrequire more data to fully exploit these features in\ngeneralization on unseen data.\nGessler and Zeldes (2022) is the only work we\nare aware of which attempts to develop a method\nfor training “low-resource” (<10M tokens in train-\ning data) monolingual TLMs. They extend the\ntypical MLM pretraining process with multitask\nlearning on part-of-speech tagging and UD syntac-\ntic parsing, and also radically reduce model size to\n1% of BERT-base, yielding fair performance gains\non two syntactic evaluation tasks. They find that\ntheir monolingual approach generally outperforms\nmultilingual methods for languages that are not rep-\nresented in the training set of a multilingual TLM\n(mBERT, in their study).\n2.3 Syntactic Inductive Bias\nOther work has investigated the syntactic capa-\nbilities of TLMs, and whether these capabilities\ncould be enhanced with additional inductive bias.\nIn an influential study, Hewitt and Manning (2019)\nfind that structures that resemble undirected syntac-\ntic dependency graphs are recoverable from TLM\nhidden representations using a simple “structural\nprobe”, consisting of a learned linear transforma-\ntion and a minimum spanning tree algorithm for\ndetermining tokens’ syntactic dependents based on\nL2 distance. Kim et al. (2020) find similar results\nwith a non-parametric, distance-based approach\nusing both hidden representations and attention dis-\ntributions. Both of these works attempt to find\nsyntactic representations within a TLM without\never exposing a TLM to a human-devised represen-\ntation. The quality of the recovered trees is usually\npoor relative to those obtainable from a syntactic\nparser, though their quality is consistently higher\nthan random baselines.\nSome works have attempted to provide\nmodels with direct access to human-devised\nrepresentations—e.g., a syntactic parse provided\nin the Universal Dependencies formalism, which\nmay have been produced by a human or by an au-\ntomatic parser. Zhou et al. (2020) extend BERT\nby adding dependency and constituency parsing\nas additional supervised tasks during pretraining.\nBai et al. (2021) assume that inputs are paired with\nparses, and use the parses to generate masks which\nrestrict an ensemble of self-attention modules to at-\ntend only to syntactic children, parents, or siblings.\nXu et al. (2021) use dependency parses to bias self-\nattention so that self-attention between tokens is\nweighted proportionally to the tokens’ distance in\nthe parse. In this paper, we examine the methods\nof Li et al. (2021) and Zhang et al. (2022b), which\nwe describe below.\nIn sum, there are very many ways in which one\ncould encourage a TLM to either learn a human\nrepresentation of syntax, or to come up with (or re-\nveal) its own. To our knowledge, none of the works\non SIB have been examined in a low-resource TLM\npretraining setting.\n3 Approach\nThis work investigates whether methods for SIB\nthat have succeeded in high-resource monolingual\nTLM pretraining settings could also be useful in\nanalogous low-resource settings. As we have seen,\n241\nmonolingual TLMs tend to have very poor qual-\nity when less than ≈10M tokens of training data\nare available for pretraining, and moreover, it has\nbeen observed that at least one dimension of this\npoor quality is models’ inability to make grammat-\nical generalizations without a large ( ≈1B tokens,\nWarstadt et al. 2020) pretraining dataset. Since it is\n(almost definitionally) difficult to get more data in\nlow-resource settings, it is especially important to\nfind other ways of improving model quality. It is\ntherefore worthwhile to examine whether supply-\ning some kind of SIB could help a low-resource\nTLM form better linguistic representations.\nAs discussed in §2.3, there are many ways to\nintroduce SIB into a TLM. In this work, we look\nspecifically at two methods: SynCLM (Zhang et al.,\n2022b) and SLA (Li et al., 2021), which is also\nused by Zhang et al. Li et al. (2021) extend the self-\nattention module with “local attention”, wherein\ntokens may only attend to tokens which are ≤k\nedges away in the dependency parse tree. Zhang\net al. (2022b) devise two contrastive loss functions\nwhich are intended to encourage tokens to attend to\nsibling and child tokens, and in their experiments,\nthey find success in combining these with SLA. A\nconcise description of the details of each method\nis available in Appendix A.Both of these methods\nhave only been evaluated on English, and both as-\nsume a UD syntactic parse as an additional input\nfor each input sequence and use the parse in dif-\nferent ways to attempt to guide the model to better\nsyntactic representations.\nWe use these two SIB methods with the model\nof Gessler and Zeldes (2022), MicroBERT, as a\nfoundation. MicroBERT is a BERT-like model\nthat has been scaled down to 1% of BERT-base,\nand that optionally employs part-of-speech tagging\nand syntactic parsing as auxiliary pretraining tasks.\nAs shown by experiments on 7 low-resource lan-\nguages conducted by Gessler and Zeldes (2022),\nMicroBERT performs much better than an unmodi-\nfied BERT-baseTLM, so we adopt it as our base-\nline model for most experiments in this work.\nWe now state our two main research questions:\n• (RQ1) Do these SIB methods improve model\nquality when applied to a low-resource lan-\nguage?\n• (RQ2) Are there any gains complementary\nwith the part-of-speech tagging component of\nMicroBERT for training low-resource mono-\nlingual TLMs?\nLanguage Unlabeled UD NER\nWolof 517,237 9,581 10,800\nCoptic 970,642 48,632 –\nMaltese 2,113,223 44,162 15,850\nUyghur 2,401,445 44,258 17,095\nAnc. Greek 9,058,227 213,999 –\nTable 1:Token count for each dataset by language from\nGessler and Zeldes (2022), sorted in order of increasing\nunlabeled token count.\n4 Methods\n4.1 Data and Evaluation\nWe reuse the datasets and evaluation setup of\nGessler and Zeldes (2022), using five of their seven\n“truly”2 low-resource languages’ datasets. Each\nlanguage’s data includes a large collection of un-\nlabeled pretraining data sourced from Wikipedia,\nas well as two datasets for downstream tasks for\nevaluation: UD treebanks for syntactic parsing,\nand WikiAnn (Pan et al., 2017) for named entity\nrecognition (NER). We refer readers to Gessler and\nZeldes’ paper for further details on these datasets\nand the models for UD parsing and NER. In ad-\ndition, we assess models on all five tasks in the\nPrOnto benchmark (Gessler, 2023), which will be\ndescribed below.\n4.2 Models\nWe reimplement the MicroBERT model of Gessler\nand Zeldes (2022), as well as the work of Zhang\net al. (2022b) and Li et al. (2021). In all cases, we\nreuse code wherever possible and closely check\nimplementation details and behavior in order to\nensure correctness. As a foundation, we use the\nBERT implementation provided in HuggingFace’s\ntransformers package (Wolf et al., 2020), and we\nalso use AI2 Tango3 for running experiments. We\nobtain all of our parses for the unlabeled portions\nof our datasets automatically using Stanza (Qi et al.,\n2020), following Zhang et al.\nIn order to answer our research questions, for\neach language, we examine the following condi-\ntions:\n1. MBERT – plain multilingual BERT\n(bert-base-multilingual-cased). A baseline;\nnumbers taken from Gessler and Zeldes.\n2The Indonesian and Tamil Wikipedias were larger than\nGessler and Zeldes’ cutoff of 10M tokens for “low resource”,\nand Indonesian and Tamil are also included in mBERT’s pre-\ntraining data. We exclude them for the purposes of this study\nin the interest of examining these five truly low-resource lan-\nguages in more depth.\n3https://github.com/allenai/tango\n242\nModel Wolof Coptic Maltese Uyghur An. Gk. Avg.\nMBERT 76.40 14.43 78.18 46.30 72.30 57.52\nMBERT -VA 72.94 82.11 72.69 42.97 65.89 67.32\nµB-M 77.71 88.47 81.40 59.97 81.94 77.90\nµB-MP 75.88 87.90 80.88 59.42 81.15 77.05\nµB-MT 77.29 88.32 81.06 59.79 81.42 77.58\nµB-MPT 77.05 88.38 80.07 58.94 81.35 77.16\nµB-MPT-SLA 76.25 87.87 79.52 58.37 80.77 76.56\nµB-MX 77.74 88.00 81.25 61.23 82.02 78.05\nµB-MXP 77.90 88.63 82.21 60.62 81.34 78.14\nµB-MXT 77.30 88.34 81.87 60.44 82.11 78.01\nµB-MXPT 78.19 88.48 81.30 61.41 81.80 78.24\nµB-MXPT -SLA 76.89 87.90 80.87 59.35 81.17 77.24\nTable 2:Labeled attachment score (LAS) by language and model combination for UD parsing evaluation. Results\nfor MBERT and MBERT -VA are taken from Gessler and Zeldes (2022).\nModel Wolof Maltese Uyghur Avg.\nMBERT 83.79 73.71 78.40 78.63\nMBERT -VA 79.37 78.11 77.03 78.17\nµB-M 83.40 82.98 86.70 84.36\nµB-MP 86.38 84.16 87.44 86.00\nµB-MT 87.16 89.46 87.33 87.98\nµB-MPT 88.89 86.83 87.67 87.80\nµB-MPT-SLA 86.38 84.85 84.81 85.35\nµB-MX 77.65 86.09 89.75 84.49\nµB-MXP 81.45 87.74 87.41 85.54\nµB-MXT 85.94 84.67 87.98 86.19\nµB-MXPT 87.06 84.37 87.53 86.32\nµB-MXPT -SLA 83.72 85.35 88.07 85.71\nTable 3:Span-based F1 score by language and model\ncombination for NER evaluation.\n2. MBERT -VA – MBERT , but with vocabulary aug-\nmentation. A baseline; numbers taken from Gessler\nand Zeldes.\n3. µB-M – plain MicroBERT trained only using\nMLM. We obtain our own numbers to verify the\ncorrectness of our implementation.\n4. µB-MP, µB-MT, µB-MPT – MicroBERT with\neither one or both of the SynCLM loss functions:\nP indicates the phrase-guided loss, and T indicates\nthe tree-guided loss.\n5. µB-MPT-SLA – µB-MPT , with the addition of\nSLA. We follow Zhang (2022) in using SLA only\nin conjunction with both contrastive losses.\n6. µB-MX, µB-MXP , µB-MXT , µB-MXPT , µB-\nMXPT -SLA – the conditions in (3–5), but with the\naddition of part-of-speech tagging (X) as an auxil-\niary pretraining task. This is done using the same\nmethods of Gessler and Zeldes: PoS tagging is\nonly performed on gold-tagged data from the UD\ntreebank, and tagged sequences are mixed into the\npretraining data at a 1 to 8 ratio.\nRevisiting our research questions, we intend for\nthe conditions in (3–5) to provide evidence for\n(RQ1), and for the additional information from the\nconditions in (6) to provide evidence for (RQ2).\n5 Results\nParsing Our results for UD syntactic parsing are\ngiven in Table 2. While all models beat the multi-\nlingual baselines, neither SynCLM nor SLA seems\nto improve model quality. In the -M variant models,\nthe top-performing model is always the one trained\nwith plain masked language modeling. This is not\nso for the -MX variant models, where the -MXP and\n-MXPT models do slightly better on average, though\nthis difference is small enough to be within the\nrange of experimental noise. Surprisingly, -MPT-\nSLA models do worst of all. Finally, comparing\n-M variants to their -MX counterparts, we do find\nthat in all cases the -MX counterpart is better on\naverage, and that the difference is about 1% LAS.\nNER Our results for WikiAnn NER are given\nin Table 3. Considering the -M variant models\nfirst, we see that in all cases the model trained\nusing only MLM performs the worst, and the-MPT-\nSLA variant, while always no better than the -MP,\n-MT, and -MPT variants, also outperforms the plain\nMLM model. The -MP, -MT, and -MPT variants\ndo best with a difference of up to 4 points F1 on\naverage.\nTurning now to the -MX variants, while it is still\ntrue that on average the plain MLM model per-\nforms worst and the non-SLA SynCLM models\nperform best, there is more variation within indi-\nvidual languages. The best model for Uyghur is\nthe plain MLM model, and for Maltese, the plain\nMLM model outperforms µB-MXT and µB-MXPT .\nConsidering now all the NER results, two pat-\nterns are worth noticing. First, unlike in parsing,\na -MX variant does not always outperform its -M\ncounterpart: for example, µB-MP for Wolof is bet-\nter than µB-MXP by a difference of 5 points F1.\nWe can see further that the -M models beat the -MX\n243\nNon-pronominal Mention Count Same Sense All 5\nModel An. Grk. Coptic Uyghur Wolof Avg. An. Grk. Coptic Uyghur Wolof Avg. Avg.\nµB-M* 52.59 50.75 49.37 51.47 51.04 60.58 61.32 60.65 59.78 60.58 68.65\nµB-MX* 56.81 53.34 51.19 59.24 55.14 60.95 61.30 61.51 63.08 61.71 70.01\nMBERT 57.36 49.52 51.46 57.35 53.92 65.34 52.79 62.73 66.49 61.84 67.92\nµB-M 56.68 52.52 52.72 53.78 53.93 58.51 56.65 57.97 58.54 57.92 68.25\nµB-MP 56.13 51.98 54.39 54.41 54.23 58.41 58.15 59.54 58.95 58.76 68.40\nµB-MT 50.41 48.98 49.37 51.47 50.06 58.48 58.08 57.99 57.03 57.90 66.88\nµB-MPT 53.68 48.98 51.74 51.47 51.47 53.36 54.19 59.32 58.07 56.23 66.39\nµB-MX 57.49 53.07 54.39 53.57 54.63 56.71 56.01 58.88 58.18 57.44 68.39\nµB-MXP 54.09 53.34 54.39 53.78 53.90 55.61 55.02 59.47 58.47 57.14 67.84\nµB-MXT 53.95 51.02 49.37 51.47 51.45 57.44 56.37 59.56 57.93 57.83 66.89\nµB-MXPT 52.72 51.71 50.91 51.47 51.70 57.19 56.17 56.81 58.14 57.08 67.30\nTable 4:Accuracy by language and model combination for two tasks in PrOnto: the Non-pronominal Mention\nCount, and Same Sense tasks. For non-baseline models, an underline indicates the best performance for a language–\ntask combination for a particular model variant (-M or -MX), and boldface indicates the best performance across\neither model variant. Scores for MBERT , µB-M*, and µB-MX* are taken from Gessler (2023)—the asterisk indicates\nthat the latter two models are not our implementation but the one provided in Gessler and Zeldes (2022), which is\nreported in Gessler (2023). Rightmost column contains an average over all languages and tasks for a given model.\nResults for PrOnto’s other three tasks are given in Appendix D.\nmodels on average by about 4 points F1. This indi-\ncates that when combined with SLA and SynCLM,\nthe PoS tagging pretraining task does not appear\nto be helpful for dimensions of model quality that\nare implicated in NER. Second, the addition of\n-SLA never results in a gain relative to any of the\nSynCLM models, except for Uyghur, where it pro-\nduces a gain of 0.09, which is within the range of\nexperimental noise.\nPrOnto We run our SynCLM models4 on all five\ntasks of PrOnto (Gessler, 2023) on all languages\nexcept Maltese, which is not represented in PrOnto\nbecause of the lack of an open-access Maltese\nBible. For each language in PrOnto, a dataset for\nfive sequence classification tasks is available which\nwas constructed by aligning New Testament verses\nfrom the target language with the English verse in\nOntoNotes (Hovy et al., 2006) and projecting an-\nnotations from English to the target language. All\n5 tasks are sequence classification tasks. Each task\nrequires a model to predict a certain grammatical\nor semantic property—these are, respectively: the\nnumber of referential noun phrases in a sequence;\nwhether the subject of a sentence contains a proper\nnoun; the sentential mood of a sentence; whether\ntwo input sequences both contain a usage of a verb\nsense; and whether two input sequences both con-\ntain a usage of a verb sense with the same number\nof arguments. We refer readers to the PrOnto pub-\nlication for further details.\nResults from two of the five tasks are given in\n4It was not possible to run our SLA models on PrOnto due\nto considerable implementation effort that would have been\nrequired, so we omit those models from this evaluation.\nTable 4.5 Broadly, we may observe that the -MPT\nand -MXPT models never perform best within a\nlanguage, with either variant being in many cases\nworse by a few absolute points compared to other\nmodels. Looking at -M-family models, -MP is the\nclear winner, doing a little better than -M and much\nbetter than -MT or -MPT on both tasks. By contrast,\nfor -MX-family models, the -MXP variant does a\nbit worse on average than -MX, and for the Same\nSense task, the -MXT model does a bit better than\n-MXP . Looking to the rightmost column in Table 4,\nwe can see that when we average accuracy scores\nfor a model across all languages and all 5 tasks\nin PrOnto, the -MP model has the highest score\noverall, with -MX and -M very close behind and all\nother model variants quite a ways behind.\nOverall, it seems that for the PrOnto tasks, of\nall the syntactic bias methods we have tried, only\nthe use of the phrase-based contrastive loss (-MP)\nor the tree-based contrastive loss in combination\nwith PoS tagging (-MXT ) showed much improve-\nment over the baselines. In individual language–\ntask combinations, models sometimes had multiple-\npoint performance differences over others, but\nwhen considered in aggregate, only -MP shows any\nimprovement over -M and -MX—by 0.15% and\n0.01% accuracy, respectively.\n6 Discussion\nConsidering first whether SynCLM and SLA\nyield benefits for low-resource monolingual TLMs\n5We omit results from the other 3 from the main body for\nspace reasons—see Appendix D for these results.\n244\n(RQ1), we have found positive evidence from the\nWikiAnn NER experiments, and weak positive evi-\ndence from the PrOnto experiments. It is true that\nthe same methods did not produce measurable gain\nfor the UD parsing task, but this is in line with\nprevious findings for these two methods, where on\nsome downstream evaluations, gain was very small\nor slightly negative—we return to this matter in the\nfollowing paragraph. For the question of whether\nthese benefits are complementary with the PoS tag-\nging pretraining strategy introduced in Gessler and\nZeldes (2022) (RQ2), we do not find consistent\nevidence in any of our experiments that both PoS\ntagging and SynCLM or SLA yield complemen-\ntary benefits. The only positive evidence we find\nfor this is in the PrOnto experiments, where the\n-MXT model variant does better than -MX in some\ntask–language combinations, though worse overall.\nThe difference in the way model variants be-\nhaved in these seven evaluation tasks is striking,\nand it is difficult to understand why models exhib-\nited these different behaviors. It is worth compar-\ning these results with those reported by the Syn-\nCLM authors (Zhang et al., 2022b). For many\nof the GLUE tasks that they assess their models\non (their Table 3), there is little or no improve-\nment from adding -P, -T, or -PT-SLA . For exam-\nple, considering their models based on RoBERTa-\nbase, none of their model variants outperform the\nMLM-only baseline for the QQP (Quora Ques-\ntion Pairs2), STS (Semantic Textual Similarity),\nor MNLI-m (Multi-Genre Natural Language In-\nference, matched). This situation is more or less\nanalogous to the one we observed in our experi-\nments for the UD parsing downstream task, where\nthe addition of SynCLM and SLA had basically no\neffect.\nOn the other hand, the GLUE task with the great-\nest gain, CoLA (Corpus of Linguistic Acceptabil-\nity), shows a difference of only 1.7% Matthews\ncorrelation coefficient, and a couple of other tasks\nlike SST (Stanford Sentiment Treebank), show an\nimprovement of only 0.3% accuracy. It would be\nnaïve to directly compare percentage points of dif-\nferent metrics in totally different experimental set-\ntings and make conclusions about effect sizes, we\nnevertheless point out that we observe improve-\nments of 1–4% F1 in our NER experiments for -M\nmodels. In light of this, we consider our results to\nbe broadly in line with the trend for previous works’\nresults on English: there is no improvement that\nis wholly consistent across evaluations, and only\nmodest gains for the benchmarks that do improve.\nIn summary, we find that SynCLM and SLA\nproduce uneven results in low-resource settings,\nthough we also find that when they do succeed, they\ncan yield gains that appear greater than anything\nobserved for high-resource languages: we saw that\nwhen we take a pure MLM pretraining regimen as\na base and add SynCLM and/or SLA, we are able\nto improve the quality of pretrained TLMs by 1 to 4\nabsolute points F1 in NER. While a similar benefit\nwas not observed for UD parsing, it is also true that\nthere was a noticeable degradation on UD parsing\nin only a couple cases, and in most cases simply\nhad no effect.\n7 English Experiments\nOne might have expected SIB to be a knockout suc-\ncess for low-resource languages given the intuitive\nfeeling that at lower data volumes, additional bias\nought to be more helpful. We considered reasons\nwhy our attempts to do this might not have panned\nout—perhaps, for example, tree structure matters\nmost for highly analytic languages like English, or\nperhaps the tasks used to evaluate English in GLUE\nare more sensitive to high-level sentence structure,\nor perhaps sensitivity to syntax is only advanta-\ngeous given a base model with sufficiently rich\ndistributional information. Here, we consider an-\nother possible explanation: that the inductive bias\nwith these methods only helps given high-quality\nsyntactic parses. An obvious difference between\nEnglish and the languages we have examined in\nthis study is that UD parsers for English generally\nachieve much higher performance given the size\nand annotation quality of English UD treebanks.\nThis is a potentially consequential difference, given\nthat both the SynCLM and SLA methods rely on\nUD parse trees as inputs. In addition, the models\nwe have developed here differ from common kinds\nof English BERTs in that they are much smaller\nand were trained on much less data, and it is pos-\nsible that the SynCLM and SLA methods might\nhave interactions with these two variables of model\nconstruction.\nIn order to investigate whether parse tree qual-\nity, model size, and pretraining data size might\nbe consequential for these SIB methods, we run\nseveral additional experiments on English datasets.\nWe choose English because its status as a high-\nresource language allows us control over several\n245\nindependent variables which we do not have control\nover in low-resource settings, namely data quantity,\nsyntactic parse quality, and model size.6 We can\nframe an additional research question that we wish\nto answer:\n• (RQ3) Are SynCLM and SLA sensitive to\nparse tree quality, model size, or pretraining\ndataset size?\nFor our English dataset, we use AMALGUM\n(Gessler et al., 2020) as our source of pretraining\ndata. AMALGUM contains around 2M tokens and\ncontains automatic parses with quality that exceeds\nwhat can normally be obtained from a standard\nparser. For downstream evaluation, we use the\nEnglish Web Treebank (Silveira et al., 2014), which\ncontains around 250K tokens, and the English split\nof WikiAnn, downsampled to around 50K tokens\nin order to bring it closer to the quantities for our\nother 3 languages (cf. Table 1). In addition, we\nuse a 100M subset of BERT’s pretraining data as a\nlarger source of unlabeled pretraining data.\nWe frame these additional conditions for English,\nextending our model naming scheme from above:\n1. -NP – syntax trees are taken from Stanza in the\nsame way as before.\n2. -HQP – syntax trees are taken from AMAL-\nGUM’s annotations, made by a high quality parser.\n3. -BD – pretraining is done using the big dataset\ninstead of AMALGUM.\n4. -BD-BM – like -BD, and in addition, the model\nsize is set to half of BERT-base (6 layers instead of\n12).\nEvidence from these conditions could tell us more\nabout how and when SynCLM and SLA can suc-\nceed in low-resource scenarios. We pretrain these\nmodels as we did in our main experiments and\nevaluate them on UD parsing and WikiAnn NER.\nA full description of our results is given in Ap-\npendix B, and we give a description of our key\nfinding here: that SynCLM and SLA are not very\nsensitive to parse quality or model size, but are\nsensitive to quantity of pretraining data. The insen-\nsitivity to parse quality may come as a surprise, and\nwe reason that this is actually understandable, since\nboth methods focus mostly on low-height subtrees\n(often corresponding to phrase- or sub-phrase-level\nconstituents) which are more likely to be correct\neven when overall parse quality is bad. We find\n6Model size is not controllable in low-resource settings in\nthe sense that, as Gessler and Zeldes (2022) argued, mono-\nlingual low-resource TLMs exhibit severe degradations when\nthey get too large.\nevidence for sensitivity to data size in the fact that\nSynCLM and SLA provide gains of up to 1% F1\nfor the NER evaluation in the two low-data condi-\ntions, while in the higher-data conditions, all but\none of the bias-enhanced models lead to degrada-\ntions relative to the baseline. In sum, we take this\nto show that lower parse quality is not the major\nreason for the ineffectiveness of SynCLM and SLA\nin low-resource settings.\n8 Conclusion\nIn this work, we have taken two methods for SIB\nthat have succeeded in English, SynCLM and SLA,\nand we have investigated whether they may also\nbe beneficial in low-resource monolingual settings.\nWe find that in most cases these methods do not\nresult in an improvement in model quality as mea-\nsured on seven tasks. Further, in our auxiliary ex-\nperiments on English, we found evidence suggest-\ning that the lower quality of parses in low-resource\nsettings is probably not what is driving the ineffec-\ntiveness of these SIB methods.\nConsidering all of our results, we conclude that\nthese two specific methods—SynCLM and SLA—\nare not well suited to supporting the pretraining\nof language models in low-resource settings, but\nwe also view it as a yet open question whether any\nmethod for SIB could succeed in this role. There\nare some reasons why SynCLM and SLA might\nhave been unhelpful. First of all, recall the fact\nthat SynCLM limits its application to only short\nsubtrees (no taller than 3 nodes). This would mean\nthat most of the time, the contrastive loss func-\ntions would only be operating on basic phrase-level\nconstituents, such as noun phrases, and not higher,\nclause-level phenomena such as relations between\nthe main clause’s predicate and its arguments. If it\nwere the case that the former kind of syntax is rel-\natively easy for models to learn even with limited\ndata, and that the latter kind of syntax is what is\nhard and therefore where SIB really ought to help,\nthen we would expect to see the results we found in\nthis work, where neither method did much to help.\nTherefore, while we find little reason to be opti-\nmistic about these two particular methods in low-\nresource settings, we don’t view the evidence in\nthis paper as an indictment of SIB in low-resource\nsettings in general, and suggest that SIB methods\nwhich are better able to provide bias for higher,\nclause-level syntactic dependencies may produce\nbetter results for low-resource languages.\n246\nAcknowledgments\nWe thank Amir Zeldes for very helpful comments\non this work.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nFan Bai, Alan Ritter, and Wei Xu. 2021. Pre-train or\nAnnotate? Domain Adaptation with a Constrained\nBudget. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5002–5015, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with Multilingual BERT, a Small Corpus,\nand a Small Treebank. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1324–1334, Online. Association for Computational\nLinguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAbteen Ebrahimi and Katharina Kann. 2021. How to\nAdapt Your Pretrained Multilingual Model to 1600\nLanguages. arXiv:2106.02124 [cs].\nDominik Maria Endres and Johannes E Schindelin.\n2003. A new metric for probability distribu-\ntions. IEEE Transactions on Information theory ,\n49(7):1858–1860.\nLuke Gessler. 2023. Pronto: Language model evalua-\ntions for 859 languages.\nLuke Gessler, Siyao Peng, Yang Liu, Yilun Zhu, Shab-\nnam Behzad, and Amir Zeldes. 2020. AMALGUM –\na free, balanced, multilayer English web corpus. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 5267–5275, Marseille,\nFrance. European Language Resources Association.\nLuke Gessler and Amir Zeldes. 2022. MicroBERT: Ef-\nfective training of low-resource monolingual BERTs\nthrough parameter reduction and multitask learning.\nIn Proceedings of the The 2nd Workshop on Multi-\nlingual Representation Learning (MRL), pages 86–\n99, Abu Dhabi, United Arab Emirates (Hybrid). As-\nsociation for Computational Linguistics.\nJohn A. Hawkins. 2014. Cross-Linguistic Variation\nand Efficiency. Oxford University Press. Publication\nTitle: Cross-Linguistic Variation and Efficiency.\nJohn Hewitt and Christopher D. Manning. 2019. A\nStructural Probe for Finding Syntax in Word Repre-\nsentations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. OntoNotes:\nthe 90% solution. In Proceedings of the Human Lan-\nguage Technology Conference of the NAACL, Com-\npanion Volume: Short Papers , NAACL-Short ’06,\npages 57–60, New York, New York. Association for\nComputational Linguistics.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do attention heads\nin BERT track syntactic dependencies? CoRR,\nabs/1911.12246.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\n247\nTaeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo\nLee. 2020. Are pre-trained language models aware\nof phrases? simple but strong baselines for gram-\nmar induction. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nZhongli Li, Qingyu Zhou, Chao Li, Ke Xu, and Yunbo\nCao. 2021. Improving BERT with syntax-aware local\nattention. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n645–653, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric de la\nClergerie, Djamé Seddah, and Benoît Sagot. 2020.\nCamemBERT: a Tasty French Language Model. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7203–\n7219, Online. Association for Computational Lin-\nguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2018. Learned in Translation: Con-\ntextualized Word Vectors. arXiv:1708.00107 [cs].\nArXiv: 1708.00107.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training data\nvolume for compact language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n7853–7858, Online. Association for Computational\nLinguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Efficient Estimation of Word Repre-\nsentations in Vector Space. arXiv:1301.3781 [cs].\nArXiv: 1301.3781.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013b. Distributed Represen-\ntations of Words and Phrases and their Composition-\nality. arXiv:1310.4546 [cs, stat]. ArXiv: 1310.4546.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajiˇc, Christopher D. Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, Reut Tsarfaty, and Daniel Zeman.\n2016. Universal Dependencies v1: A Multilingual\nTreebank Collection. In Proceedings of the Tenth In-\nternational Conference on Language Resources and\nEvaluation (LREC’16), pages 1659–1666, Portorož,\nSlovenia. European Language Resources Association\n(ELRA).\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall Data? No Problem! Exploring the Viabil-\nity of Pretrained Multilingual Language Models for\nLow-resourced Languages. In Proceedings of the 1st\nWorkshop on Multilingual Representation Learning,\npages 116–126, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global Vectors for Word\nRepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow Multilingual is Multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computational\nLinguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 101–108, Online. As-\nsociation for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What we know about\nhow BERT works. arXiv:2002.12327 [cs]. ArXiv:\n2002.12327 version: 3.\nJohn Robert Ross. 1967. Constraints on Variables in\nSyntax. Doctoral Dissertation, Massachusetts Insti-\ntute of Technology.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor, John\nBauer, and Christopher D. Manning. 2014. A gold\nstandard dependency corpus for English. In Pro-\nceedings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC-2014).\n248\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. CoRR, abs/1807.03748.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. Advances in Neural Information Process-\ning Systems, 30.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and\nDan Roth. 2020. Extending Multilingual BERT to\nLow-Resource Languages. arXiv:2004.13640 [cs].\nArXiv: 2004.13640.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020. Learning Which Fea-\ntures Matter: RoBERTa Acquires a Preference for\nLinguistic Generalizations (Eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n217–235, Online. Association for Computational Lin-\nguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are All Languages\nCreated Equal in Multilingual BERT? In Proceed-\nings of the 5th Workshop on Representation Learning\nfor NLP , pages 120–130, Online. Association for\nComputational Linguistics.\nZenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun\nShou, Ming Gong, Wanjun Zhong, Xiaojun Quan,\nDaxin Jiang, and Nan Duan. 2021. Syntax-enhanced\npre-trained model. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5412–5422, Online. Association for\nComputational Linguistics.\nBryan Zhang. 2022. Improve MT for search with se-\nlected translation memory using search signals. In\nProceedings of the 15th Biennial Conference of the\nAssociation for Machine Translation in the Americas\n(Volume 2: Users and Providers Track and Govern-\nment Track), pages 123–131, Orlando, USA. Associ-\nation for Machine Translation in the Americas.\nRui Zhang, Yangfeng Ji, Yue Zhang, and Rebecca J.\nPassonneau. 2022a. Contrastive data and learning\nfor natural language processing. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies: Tutorial Abstracts,\npages 39–47, Seattle, United States. Association for\nComputational Linguistics.\nShuai Zhang, Wang Lijie, Xinyan Xiao, and Hua Wu.\n2022b. Syntax-guided contrastive learning for pre-\ntrained language model. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2022 ,\npages 2430–2440, Dublin, Ireland. Association for\nComputational Linguistics.\nJunru Zhou, Zhuosheng Zhang, Hai Zhao, and Shuail-\niang Zhang. 2020. LIMIT-BERT : Linguistics In-\nformed Multi-Task BERT. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 4450–4461, Online. Association for Computa-\ntional Linguistics.\n249\nFigure 1:Figure 1 from Li et al. (2021). The standard\nself-attention mechanism is complemented by another\nself-attention mechanism in which tokens may only at-\ntend to tokens close to it in a parse tree. A gated unit\nwith learnable parameters interpolates the two attention\ndistributions before the distribution is combined with\nthe Value representation.\nA Summary of SLA and SynCLM\nOur approach critically relies on two previous re-\nsults, which we summarize here.\nA.1 Syntax-aware Local Attention\nLi et al. (2021) introduce Syntax-aware Local At-\ntention (SLA), a variation on a standard TLM\nself-attention mechanism that retains standard self-\nattention and complements it with a separate self-\nattention mechanism where each token may only\nattend to “syntactically local” tokens.\nRecall that BERT and most other TLMs use\nscaled dot-product attention in every attention head,\nwhere the attention distribution A can be computed\nwith query and key representations Q and K, d is\nthe size of an individual attention head’s hidden\nrepresentation, and the attention head’s output O is\nthe product of A and the value representation V:\nA =softmax(QK⊺\n√\nd\n) (1)\nO =A V (2)\nNow, assume an input sequenceW =w1,..., wn\nwith an unlabeled dependency parse H =h1,..., hn\nwhere hi indexes token wi’s syntactic head. Define\nsyntactic distance between two words, D(wi,wj),\nas the length of the shortest path between the two\nwords in the parse:\nD(wi,wj) ∶=SHORTEST -PATH(H,i, j) (3)\nTo account for the fact that parses may be inaccu-\nrate (e.g. if they come from an automatic parser),\ndefine windowed syntactic distance like so:7\nD′(wi,wj) = min\nk∈{i−1,i,i+1}\nD(wk,wj) (4)\n7If k /∈[1,n], exclude it from the min.\nThis can be viewed as sacrificing precision for re-\ncall: a decision to give tokens a better chance of\nbeing able to attend to truly local tokens (given\nthe imperfection of parser outputs), though at the\ncost of sometimes allowing attention on tokens that\ntruly are not local.\nNow, define a mask matrix M that will mask a\ntoken iff a token j has windowed syntactic distance\nover a certain threshold δ relative to token i:\nmi j =\n⎧⎪⎪⎨⎪⎪⎩\n0 if D′(wi,wj) ≤δ\n−∞ otherwise (5)\nWe can now define syntax-aware local attention by\nmodifying Equation 1 so that M is added to the\ninner term in order to force an attention score of 0\nfor masked tokens:\nAℓ =softmax(QK⊺\n√\nd\n+M) (6)\nSyntax-aware local attention (SLA) is used\nalongside the normal, “global” self-attention. To\ncombine the two after they have been computed,\nintroduce a gated unit for each Transformer block\nwith new parameters Wg and bg to compute gi for\neach word wi using the word’s hidden representa-\ntion hi, where σ is the sigmoid function:\ngi =σ(Wghi +bg) (7)\nNow, usegi to interpolate both the normal attention\ndistribution ai and the local attention distributionaℓ\ni\nat each position i in the sequence to yield the final\nattention distribution ˆA and final attention head\noutput ˆO:\nˆA =\nn\n⊕\ni=1\ngiai +(1−gi)aℓ\ni (8)\nˆO = ˆAV (9)\nIn the original work, the SLA method is evalu-\nated on various benchmarks on English and consis-\ntently achieves measurable improvements in model\nquality. Parses are obtained using Stanza (Qi et al.,\n2020), which for English are of quite high qual-\nity (labeled attachment score is in the mid-80s for\nEnglish datasets). We refer readers to the original\npublication for further details. See Figure 1 for an\noverview.\n250\nFigure 2:Figure 1 from Zhang et al. (2022b). P and Ni represent the positive sample and the ith negative sample,\nrespectively. The phrase-based contrastive loss on the left is intended to make the representations of syntactic\nsiblings more similar, and the tree-based contrastive loss on the right is intended to make the representations of\nsyntactic children and parents more similar.\nA.2 SynCLM\nZhang et al. (2022b) present the Syntax-guided\nContrastive Language Model (SynCLM), a BERT-\nlike TLM that characteristically uses two novel\ncontrastive loss functions and also uses SLA (cf. ap-\npendix A.1). Intuitively, a contrastive learning ob-\njective requires each instance to have one or more\npositive and negative “samples”, and attempts to\nmaximize the instance’s similarity to positive sam-\nples and minimize its similarity to negative samples\n(Zhang et al., 2022a). SynCLM uses a popular loss\nfunction for this, InfoNCE (van den Oord et al.,\n2018):\nL =−log\nexp(sim(q,q+)\nτ )\nexp(sim(q,q+)\nτ )+∑K\ni=0 exp(sim(q,q−\ni )\nτ )\n(10)\nq, q+, and q− are the representations of the instance,\na positive sample, and a negative sample, respec-\ntively, and τ ∈(0,1) is a temperature hyperparame-\nter, set to 0.1 for SynCLM. sim is a similarity func-\ntion, such as cosine similarity or KL-divergence.\nThe loss terms obtained from this equation are sim-\nply added to the loss obtained from masked lan-\nguage modeling. We review only the contrastive\nobjective functions here, and refer readers to Figure\n2 and the original paper for further details.\nThe two SynCLM contrastive learning objec-\ntives are distinguished by how they formulate sim.\nThe first, “phrase-guided” objective aims to make\nattention distributions more similar for words in\nthe same phrase. Given a token t, sample a positive\ntoken t+ such that t and t+ have a lowest com-\nmon ancestor ta whose corresponding subtree (the\n“phrase”) is no more than 2 in height. Now sam-\nple k negative tokens t−\n1 ,..., t−\nk outside the phrase,\ni.e. who do not have ta as an ancestor. Define\nsimphrase using Jensen–Shannon Divergence (En-\ndres and Schindelin, 2003), a similarity metric for\nprobability distributions:\nsimphrase =−JSD(a ∥ a′) (11)\nHere, a is the attention distribution for t, and a′\nis the attention distribution for either a positive\nor a negative sample. This equation is used to\ncalculate similarities for a given attention head and\nlayer—in SynCLM’s implementation, only the last\nlayer is used, and simphrase is averaged across all\nattention heads in the last layer before being used\nwith Equation 10 for the final loss computation.\nThe “tree-guided” objective proceeds similarly.\nA token ti is sampled which forms the root of the\npositive tree, T +. Next, up to three tokens t−\n1 ,..., t−\nk\nare sampled such that each t−\ni is not in T + but is\nadjacent to a token in T +. A new negative subtree\nT −\ni is formed for each t−\ni such that a random non-\nroot token in T + has been removed from T + along\nwith its children, and the subtree rooted at t−\ni has\n251\ntaken its place.\nWe may now define tree similarity as follows,\nwhere T is a positive or a negative subtree and za\nis the hidden representation of token a:\nsimtree =cossim(zi,∑tj∈Tchild ei jzj)\nwhere Tchild =T ∖{ti}\nei j = exp(zi ⋅zj)\n∑tk∈Tchild exp(zi ⋅zk)\n(12)\nInformally, we are taking the dot product of the root\nof the subtree with all other tokens in the subtree,\nsoftmaxing this dot product, using it to produce\na weighted sum of all hidden representations of\ntokens in the subtree, and taking the cosine similar-\nity between this weighted sum and the root of the\nsubtree. The closer these tokens’ representations\nare in the hidden space, the higher this similarity\nmeasure will be. Again, SynCLM uses only the\nlast TLM layer for this objective, and this similar-\nity measure is used with Equation 10. Note that in\na preprocessing step, parses are modified so that\nsubword tokens are syntactic children of the head\ntoken of the word they belong to.8\nB English Experiments\nParsing Parsing results are given in Table 5. First\nnote that as before, there is little difference in\nmodel quality across all the SynCLM conditions,\nproviding more evidence that the SynCLM losses\nare not helpful for UD parsing. Next, as could be\nexpected, the model trained with 100M tokens that\nis half the size of BERT-base performs best. What\nis surprising, however, is that of the remaining 3\nmodels, the model with the standard parser per-\nforms best. Since all three of these variants are\nalike in model hyperparameters, this must be ex-\nplainable in terms of properties of the three datasets.\nIt could be that AMALGUM’s very deliberate con-\nstruction from eight genres in equal proportion\ncould have led to serendipitously good performance\non the parsing task, but it is impossible to know\nwithout further experimentation.\nAt any rate, whatever the differences in these\nthree variants might be caused by that lies in the\ndata, we still have a firm answer for our most im-\nportant question: for English UD parsing, SynCLM\n8We have elided various implementation details here, such\nas hyperparameters which control how many sample sets to\nobtain per input sequence, or maximum token count for a\nsubtree. Please refer to our code or Zhang et al. (2022b)’s\ncode for these details.\nand SLA methods appear not to be sensitive to data\nquantity or parse quality. The latter might be sur-\nprising, but it is worth remembering that the authors\nof these methods designed their algorithms in ways\nthat may mitigate the deleterious effects of lower-\nquality syntactic parses. SLA uses windowed syn-\ntactic distance (cf. Equation 4 in Appendix A) for\nthe express purpose of accommodating bad parses,\nand the SynCLM losses place low limits on tree\nheight, which would help in accommodating bad\nparses since edges at the local, phrase level are\noften more reliable than edges at the clausal or\ninter-clausal level.\nNER Results on NER are given in Table 6. Sur-\nprisingly, the same half-sized BERT model that\nwas trained on 100M tokens and did best in the\nparsing evaluation does very poorly in the NER\ntask. We suspect that this may be due to the fact\nthat larger models can show greater instability in\nfine-tuning setups (Rogers et al., 2020). As with\nparsing, we see that the -NP model performs best\namong the MicroBERT-sized models, which we as-\ncribe to differences in properties of the pretraining\ndatasets.\nWhat is most interesting in the NER results is\nthat for the two low-data conditions, -NP and -HQP ,\nwe see about a 1% gain in the -MPT condition rel-\native to the MLM-only baseline. This gain is not\nseen in the higher-data conditions, where none of\nthe SynCLM combinations lead to a better model\nexcept for µB-MPT -BD, with a gain of 0.45% F1.\nComplicating this picture, though, is that in the\nlow-data settings, the -MP and -MT variants often\nunderperform relative to the baseline. Still, these\nresults seem to indicate at least that the SynCLM\nloss functions may be less effective in improving\nmodel quality as quantity of pretraining data in-\ncreases. We can see that this holds both for the\nhalf-sized BERT model as well as the MicroBERT-\nsized model, indicating that model size does not\nmatter.\nDiscussion Returning to RQ3, these results in-\ndicate that SynCLM and SLA are not especially\nsensitive to parse quality, and are also not sensitive\nto model size, but are sensitive to quantity of pre-\ntraining data. As discussed above, the insensitivity\nto parse quality is understandable, as the dimen-\nsions in which a parse may be bad are less relevant\nfor these methods because of the way they use the\nparse trees. The sensitivity to pretraining data quan-\ntity is intuitive if we consider these two methods as\n252\nModel -NP -HQP -BD -BD-BM Avg.\nµB-M 86.79 85.60 85.81 87.83 86.51\nµB-MP 86.89 85.36 85.91 87.73 86.47\nµB-MT 86.51 85.83 85.93 87.10 86.34\nµB-MPT 86.57 85.39 85.83 86.99 86.19\nµB-MPT-SLA 86.61 85.42 85.62 86.53 86.05\nAvg. 86.67 85.52 85.82 87.23\nTable 5:Labeled attachment score (LAS) for English.\nModel -NP -HQP -BD -BD-BM Avg.\nµB-M 60.07 58.79 57.18 51.15 56.80\nµB-MP 59.99 55.29 54.46 50.96 55.18\nµB-MT 56.92 55.65 57.58 49.52 54.92\nµB-MPT 61.54 59.32 55.63 49.98 56.62\nµB-MPT-SLA 61.49 56.05 59.51 43.90 55.24\nAvg. 60.00 57.02 56.87 49.10\nTable 6:Span-based F1 score by language and model combination for NER evaluation.\nsources of inductive bias: an inductive bias ought\nto be pushing a model towards learning something\nthat they would have learned if there were more\ntraining data available, and so we should expect\nthat if we consider a modification to be an induc-\ntive bias, its influence should wane as the quantity\nof data increases.\nIn sum, these findings support our conclusion\nthat SynCLM and SLA are at least in some respects\nwell-suited to aid the pretraining of TLMs in low-\nresource settings, as we have found that even when\nparse quality is worse than ideal, SynCLM and\nSLA still perform about as well as when they have\nthe highest quality parses.\nC Limitations\nThe goal of this paper is to make progress towards\nmore effective TLMs for low-resource languages\nusing syntactic inductive bias. We believe we have\npresented compelling evidence that two approaches\nto this problem seem not to be very effective for\nlow-resource languages. But it is important to point\nout that we have tested the methods on only 5 lan-\nguages. We believe that this forms an informa-\ntive picture for low-resource languages in general\nbecause these languages are quite different from\none another along typological and phylogenetic di-\nmensions, but in principle, it is conceivable that\nother low-resource languages could exhibit behav-\niors that are very different from the ones we have\nseen in this paper. Moreover, we have had to re-\nimplement the methods at the center of this work,\nand while we have done everything we can to as-\ncertain that these re-implementations have been\nfaithful and without error, tensor programming is\nerror-prone work, and it is not impossible that we\nmay have introduced a bug somewhere which criti-\ncally affected the experimental results in this work.\nD Other PrOnto Results\nProper Noun Subject\nModel An. Grk. Coptic Uyghur Wolof\nµB-M* 76.32 78.76 81.30 90.36\nµB-MX* 81.11 80.78 78.45 90.36\nMBERT 81.42 75.50 80.35 91.65\nµB-M 79.88 79.22 80.35 80.15\nµB-MP 79.72 79.38 80.82 77.97\nµB-MT 79.57 75.66 81.14 77.72\nµB-MPT 76.32 79.53 77.02 77.72\nµB-MX 81.27 81.40 80.67 81.84\nµB-MXP 78.79 78.91 79.71 79.42\nµB-MXT 76.32 80.47 73.53 77.72\nµB-MXPT 80.80 80.16 79.40 77.72\nTable 7:Accuracy by language and model combination\nfor Proper Noun Subject in PrOnto. Scores for MBERT ,\nµB-M*, and µB-MX* are taken from Gessler (2023)—\nthe asterisk indicates that the latter two models are not\nour implementation but the one provided in Gessler and\nZeldes (2022), which is reported in Gessler (2023).\n253\nSentence Mood\nModel An. Grk. Coptic Uyghur Wolof\nµB-M* 90.18 89.75 89.96 90.36\nµB-MX* 91.56 89.75 90.10 90.36\nMBERT 91.70 91.55 91.23 91.65\nµB-M 91.98 91.69 91.51 90.36\nµB-MP 90.73 91.97 91.23 89.72\nµB-MT 90.59 90.30 89.25 90.36\nµB-MPT 90.73 90.30 89.96 90.36\nµB-MX 90.59 92.24 91.80 90.58\nµB-MXP 91.56 91.97 90.81 90.58\nµB-MXT 91.42 90.03 89.96 90.36\nµB-MXPT 90.73 90.03 89.96 90.36\nTable 8:Accuracy by language and model combination\nfor Sentence Mood in PrOnto. Scores for MBERT , µB-\nM*, and µB-MX* are taken from Gessler (2023)—the\nasterisk indicates that the latter two models are not our\nimplementation but the one provided in Gessler and\nZeldes (2022), which is reported in Gessler (2023).\nSame Argument Count\nModel An. Grk. Coptic Uyghur Wolof\nµB-M* 61.80 62.70 61.78 61.05\nµB-MX* 61.71 61.58 62.12 63.46\nMBERT 50.87 51.24 50.78 54.46\nµB-M 59.72 56.94 59.23 56.65\nµB-MP 58.57 57.61 59.99 58.38\nµB-MT 58.44 57.26 59.43 56.10\nµB-MPT 53.13 56.01 59.56 56.32\nµB-MX 57.06 55.92 60.10 56.05\nµB-MXP 58.60 56.18 59.87 56.21\nµB-MXT 58.03 56.47 59.67 56.69\nµB-MXPT 58.36 58.01 57.88 57.54\nTable 9:Accuracy by language and model combination\nfor Same Argument Count in PrOnto. Scores forMBERT ,\nµB-M*, and µB-MX* are taken from Gessler (2023)—\nthe asterisk indicates that the latter two models are not\nour implementation but the one provided in Gessler and\nZeldes (2022), which is reported in Gessler (2023).",
  "topic": "Maltese",
  "concepts": [
    {
      "name": "Maltese",
      "score": 0.8352117538452148
    },
    {
      "name": "Computer science",
      "score": 0.7378689050674438
    },
    {
      "name": "Transformer",
      "score": 0.7142356038093567
    },
    {
      "name": "Inductive bias",
      "score": 0.6206415891647339
    },
    {
      "name": "Natural language processing",
      "score": 0.5718343257904053
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5541627407073975
    },
    {
      "name": "Process (computing)",
      "score": 0.4725087583065033
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42216771841049194
    },
    {
      "name": "Linguistics",
      "score": 0.35240963101387024
    },
    {
      "name": "Programming language",
      "score": 0.13601714372634888
    },
    {
      "name": "Task (project management)",
      "score": 0.11939382553100586
    },
    {
      "name": "Multi-task learning",
      "score": 0.10718724131584167
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I184565670",
      "name": "Georgetown University",
      "country": "US"
    }
  ],
  "cited_by": 2
}