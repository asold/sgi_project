{
  "title": "Automatic Scoring of Creative Problem-Solving with Large Language Models: A Comparison of Originality and Quality Ratings",
  "url": "https://openalex.org/W4387429080",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4292734406",
      "name": "Simone Luchini",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A4299020864",
      "name": "Nadine T. Maliakkal",
      "affiliations": [
        "University of Nebraska at Omaha"
      ]
    },
    {
      "id": "https://openalex.org/A5093022271",
      "name": "Paul V DiStefano",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A2101423419",
      "name": "John D. Patterson",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A3134031488",
      "name": "Roger Beaty",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A4207442100",
      "name": "Roni Reiter-Palmon",
      "affiliations": [
        "University of Nebraska at Omaha"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4200130072",
    "https://openalex.org/W2038333915",
    "https://openalex.org/W6691588457",
    "https://openalex.org/W106053583",
    "https://openalex.org/W2461822810",
    "https://openalex.org/W2488503856",
    "https://openalex.org/W2151189140",
    "https://openalex.org/W2903759034",
    "https://openalex.org/W2981731882",
    "https://openalex.org/W3081924543",
    "https://openalex.org/W6853159330",
    "https://openalex.org/W4293064565",
    "https://openalex.org/W2115881395",
    "https://openalex.org/W3164994912",
    "https://openalex.org/W6672294001",
    "https://openalex.org/W6757576389",
    "https://openalex.org/W4290636900",
    "https://openalex.org/W2802595084",
    "https://openalex.org/W4233620411",
    "https://openalex.org/W6787190696",
    "https://openalex.org/W6681698864",
    "https://openalex.org/W4391561116",
    "https://openalex.org/W2080775075",
    "https://openalex.org/W2910282252",
    "https://openalex.org/W3044750169",
    "https://openalex.org/W2110121268",
    "https://openalex.org/W6769430610",
    "https://openalex.org/W6784358232",
    "https://openalex.org/W6996532349",
    "https://openalex.org/W2563974675",
    "https://openalex.org/W7070887812",
    "https://openalex.org/W2083007012",
    "https://openalex.org/W6783724175",
    "https://openalex.org/W2995523160",
    "https://openalex.org/W2972680241",
    "https://openalex.org/W2608811215",
    "https://openalex.org/W6763121668",
    "https://openalex.org/W2037263885",
    "https://openalex.org/W4200423504",
    "https://openalex.org/W2044474488",
    "https://openalex.org/W2129863227",
    "https://openalex.org/W2902077037",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3183248212",
    "https://openalex.org/W2158997610",
    "https://openalex.org/W6745023567",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3088344655",
    "https://openalex.org/W2086438570",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W1972499682",
    "https://openalex.org/W6903949333",
    "https://openalex.org/W4311140946",
    "https://openalex.org/W4386745708",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2938752664",
    "https://openalex.org/W2025631714",
    "https://openalex.org/W2029183335",
    "https://openalex.org/W1997872094",
    "https://openalex.org/W2065964397",
    "https://openalex.org/W3120237739",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2019602329",
    "https://openalex.org/W2141403362",
    "https://openalex.org/W2004736108",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W4283321795",
    "https://openalex.org/W3162966461",
    "https://openalex.org/W4388210637",
    "https://openalex.org/W2797152899",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4253277532",
    "https://openalex.org/W2763088512",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4306646496",
    "https://openalex.org/W2251703179",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2086542212",
    "https://openalex.org/W4234180827",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3092557781",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W4384644509",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2019443768",
    "https://openalex.org/W4385963769",
    "https://openalex.org/W4361020574",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4378364251",
    "https://openalex.org/W2908149518",
    "https://openalex.org/W2147983157",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1788836429"
  ],
  "abstract": "Creative problem-solving is a naturalistic form of creative thinking involving the generation of solutions that are not only original but also of high quality (i.e., plausible and effective). Naturalistic tasks that evaluate both originality and quality are vital for the promotion of creativity in real-world settings—yet scoring such tasks remains challenging, due to costly human labor required to manually rate task responses. Past work has shown that large language models (LLMs) can be trained to predict human originality ratings of responses to tests of divergent thinking. In the present research, we extend this work to creative problem-solving, examining whether both originality and quality can be automatically scored for a naturalistic creativity task. We gathered data from 10 studies, amounting to 3,235 participants who completed a creative problem-solving task (CPST). We then fine-tuned two open-source LLMs, RoBERTa and GPT-2, to predict human ratings of originality and quality on the CPST, and compared their performance to two other scoring methods: elaboration (i.e., word count) and semantic distance. We found that RoBERTa and GPT-2 models predict solution quality (RoBERTa, r = .83; GPT-2, r = .83) better than solution originality (RoBERTa, r = .79; GPT-2, r = .80). Moreover, we found that both models outperformed elaboration and semantic distance methods and generalized to new CPST items not present in their training set. We therefore show for the first time that naturalistic creativity tasks can be automatically scored for both originality and quality. Open access is provided to the models and training data.",
  "full_text": " \nAutomatic Scoring of Creative Problem-Solving with Large Language Models: A \nComparison of Originality and Quality Ratings \nSimone Luchini1, Nadine T. Maliakkal2, Paul V. DiStefano1, John D. Patterson1, Roger E. \nBeaty1, & Roni Reiter-Palmon2  \n1 Pennsylvania State University, USA \n2 University of Nebraska Omaha, USA \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n \n \n  \nAuthor Note \nR.E.B. is supported by grants from the National Science Foundation [DRL-1920653; DUE- \n2155070]. \nCorrespondence should be addressed to Simone Luchini or Roger E. Beaty, 140 Moore \nBuilding, University Park, PA 16802. Email: skl5875@psu.edu.  \n \nAbstract \nCreative problem-solving is a naturalistic form of creative thinking involving the generation of \nsolutions that are not only original but also of high quality (i.e., plausible and effective). \nNaturalistic tasks that evaluate both originality and quality are vital for the promotion of creativity \nin real-world settings—yet scoring such tasks remains challenging, due to costly human labor \nrequired to manually rate task responses. Past work has shown that large language models \n(LLMs) can be trained to predict human originality ratings of responses to tests of divergent \nthinking. In the present research, we extend this work to creative problem-solving, examining \nwhether both originality and quality can be automatically scored for a naturalistic creativity task. \nWe gathered data from 10 studies, amounting to 3,243 participants who completed a creative \nproblem-solving task (CPST). We then fine-tuned two open-source LLMs, RoBERTa and GPT-\n2, to predict human ratings of originality and quality on the CPST, and compared their \nperformance to two other scoring methods: elaboration (i.e., word count) and semantic distance. \nWe found that RoBERTa and GPT-2 models predict solution quality (RoBERTa, r = .83; GPT-2, \nr = .83) better than solution originality (RoBERTa, r = .79; GPT-2, r = .80). Moreover, we found \nthat both models outperformed elaboration and semantic distance methods and generalized to \nnew CPST items not present in their training set. We therefore show for the first time that \nnaturalistic creativity tasks can be automatically scored for both originality and quality. Open \naccess is provided to the models and training data. \nKeywords: Automated Scoring; Creativity; Creative Problem-Solving; Large Language Models; \nNaturalistic Creativity Assessment \n \n  \n \nAutomatic Scoring of Creative Problem-Solving with Large Language Models: A \nComparison of Originality and Quality Ratings \nCreativity measurement has undergone a rapid evolution, originating in frequency-based \nmethods (Torrance, 1972) and subjective human ratings (Amabile, 1982; Silvia et al., 2008), \nand more recently, automated scoring techniques (Beaty & Johnson, 2021; Beaty et al., 2021; \nDumas & Dunbar, 2014; Dumas et al., 2020). Automated creativity scoring leverages advances \nin natural language processing (NLP), avoiding the limitations of human scoring, such as \nintensive labor costs or disagreement between raters (Barbot, 2018; Forthmann et al., 2017; \nReiter-Palmon et al., 2019). So far, these methods have mostly been applied to scoring \ndivergent thinking tasks, such as the alternate uses tasks, and they have focused exclusively on \nscoring the originality of ideas (Acar et al., 2023; Beaty et al., 2022; Beaty & Johnson, 2021; \nHeinen & Johnson, 2018; Johnson et al., 2022). Real-world creative problem-solving, however, \noften requires both originality and quality (i.e.,  plausible and effective) to ensure effective \nimplementation of creative solutions (Reiter-Palmon, 2020). Yet no methods currently exist for \nautomatic scoring of such naturalistic creativity tasks. In the present study, we extend research \non automatic creativity scoring to real-world creative problem-solving (Reiter-Palmon, Robinson-\nMorral et al., 2012; Reiter-Palmon et al., 1998, 2009), aiming to develop more predictive \nmethods for evaluating the originality and quality of creative solutions. \nAmong the most notable advancements in the scoring of creative products or ideas is \nthe development of the consensual assessment technique (CAT; Amabile, 1982; 1996; Baer, et \nal., 2004). Having undergone decades of refinement, the CAT is considered a gold standard for \ncreativity assessment across many domains (Baer & McKool, 2009). The main principle guiding \nthe CAT is that the combined opinions of several experts from a particular field will approximate \nthe true creative value of a product or idea (Kaufman et al., 2009). However, a clear limitation of \nthe CAT is that of high labor costs, exemplified by the difficulty of gathering domain experts to \nrate numerous responses to a creativity test (Kaufman et al., 2005). Although mixed evidence \n \nsuggests that novice raters may perform comparably to domain experts (Kaufman et al., 2005; \n2013; Silvia et al., 2008), issues such as high labor costs and inter-rater disagreement are \nunavoidable with human raters, illustrating the need for a shift towards automated creativity \nscoring (Paulus et al., 1970). \nAutomated scoring of creativity dates back to early text-mining approaches developed by \nPaulus et al. (1970). These first attempts used linguistic variables such as elaboration (i.e., word \ncount) as predictors for the creativity of an idea (Paulus & Renzulli, 1968). Automated creativity \nscoring was only later pursued with the introduction of semantic distance, an aspect of \noriginality reflecting remote relationships between concepts. The distributional semantics theory \nholds that a word’s meaning can be extracted by looking at the context in which it appears \n(Firth, 1957). Essentially, words that appear in the same context (e.g., sentence) are thought to \nhold similar meanings, leading to the assumption that word meanings can be extracted from \ndistributions of co-occurrences between words in natural language (e.g., books; Lenci, 2018). \nJust as the words “dog” and “cat” can be expected to co-occur in text, because of their semantic \nrelatedness, the words “dog” and “plane” will be less likely to co-occur, due to their semantic \nunrelatedness. Such comparisons between word meanings can then be mathematically \nquantified with word embeddings—vector representations of words in a high-dimensional space \n(Landauer et al., 1998). Several methods exist for the generation of word embeddings, with \nsome early approaches involving latent semantic analysis (LSA; Deerwester et al., 1990) and \ncomputational models such as Word2Vec (Mikolov, Chen, et al., 2013) or GloVe (Pennington et \nal., 2014). \nThe application of distributional semantics to creativity assessment is consistent with the \nassociative theory of creativity, which holds that creative ideas arise from the combination of \nsemantically “distant” concepts from memory (Beaty & Kenett, 2023; Kenett, 2019; Mednick, \n1962). In this way, it was proposed that the originality of an idea or creative product could be \ndetermined by quantifying the semantic distance—derived from the cosine of the angle between \n \npairs of word embeddings in vector space—of the concepts composing it (Beaty & Johnson, \n2021). Research has demonstrated that semantic distance predicts human ratings of originality \non the alternate uses task (AUT), a widely adopted creative thinking task that requires \nparticipants to produce unusual uses for everyday objects (Beaty et al., 2022; Dickson et al., \n2022; Dumas & Runco, 2018; Forthmann et al., 2022; Maio et al., 2020; Patterson et al., 2023). \nFor the AUT, semantic distance is computed between the everyday object posed to participants \n(e.g., box) and all words in the participant’s response (e.g., seat cushion). \nRecently, divergent semantic integration (DSI) was developed to compute semantic \ndistance in longer text responses, such as short stories (Johnson et al., 2022). DSI computes \nthe semantic distance between all possible pairs of words using embeddings from pre-trained \nlarge language models (LLMs), a class of deep neural network models trained on vast textual \ndata for understanding and generating language. These models are sensitive to the context in \nwhich words appear and thus words with multiple meanings—also known as polysemy (e.g., \ncomputer-server, restaurant-server)—making them more suitable for evaluating long-form \ncreative responses than previous semantic distance approaches that do not consider context \n(e.g., LSA; Jawahar et al., 2019). Indeed, Johnson et al. (2022) found that word embeddings \nextracted from LLMs allowed for the best predictions of human-scored creativity ratings, \nexplaining up to 72% of the variance in human ratings. Of note, the model also demonstrated \ngood predictions when faced with responses from non-native English speakers, compared to \nthose from native English speakers, providing support for its generalizability. \nImportantly, semantic distance metrics are \"unsupervised,” text-mining approaches, \ncalculating probability metrics without human involvement in the process. Other models, \nleveraging supervised learning, are being increasingly adopted for their superior performance \nacross a range of tasks (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2018; Raffel et al., \n2020; Vaswani et al., 2017; Yang et al., 2019). Supervised learning involves training a machine \nlearning model on a labeled dataset (i.e., the “training set”), such as responses to a task and \n \noriginality scores, so that the model can learn to predict new labels it has not seen before. In a \nfirst attempt at applying supervised learning to automate creativity scoring, researchers trained \nseveral machine learning classifiers on a variety of text features (e.g., word embeddings, word \nlength, word count) and compared their performance at predicting human creativity ratings on \nthe AUT (Buczak et al., 2022). Researchers observed how their best model would explain \nbetween 33% and 56% of the variance in the AUT scores. This work built directly on past \nresearch employing word embeddings/semantic distance by expanding on the types of models \nand data that were employed in the prediction of creativity. \nLLMs have recently shown performance far superior to semantic distance on the AUT \n(Organisciak et al., 2022; Stevenson et al., 2022). LLMs have garnered attention more broadly \nfor achieving human-level performance across several complex tasks, from language translation \nand text generation to sentiment analysis and question-answering (Rajpurkar et al., 2016; \nRoemmele et al., 2011; Socher et al., 2013). LLMs typically undergo significant pre-training on \ngeneral language data (i.e., large volumes of text), and are later “fine-tuned” on smaller, specific \ndatasets (e.g., creative responses and human ratings) to boost their performance on related \ntasks (e.g., creativity scoring; Liu et al., 2021). Fine-tuning is a supervised approach that allows \nLLMs to build on their vast knowledge obtained during pretraining by adjusting their internal \nparameters through exposure to many examples (e.g., AUT responses and human ratings). \nLLMs such as the Generative Pre-trained Transformer (GPT) are large and complex, with \nmillions to billions of parameters, which also tend to increase exponentially with every new \nversion of the model (e.g., GPT-2, GPT-3, GPT-4). Thus, despite their impressive capabilities \nacross a variety of benchmarks (Vaswani et al., 2017), LLMs have poor interpretability (i.e., they \nare a “black box”; Barredo Arrieta et al., 2020; Dale, 2021; Gunning et al., 2019; Kojima et al., \n2022), unlike semantic distance, which has a clearer interpretation. While it may be possible to \ninterpret the semantic distance score between two words as their dissimilarity within a \n \nconceptual space, it would be complicated to draw similar conclusions about how LLMs draw \npredictions regarding the creativity of an idea. \nNevertheless, compared to semantic distance, the performance gains of LLMs are \nsubstantial. When trained on 27,000 AUT responses, Organisciak et al. (2022) found that GPT-\n3 robustly predicted human creativity ratings of responses it had not seen before (r = .81) \ncompared to semantic distance (r = .20). Organisciak and colleagues offered practical insights \ninto how fine-tuning could be applied to other creativity tasks, and the requirements for reliable \nprediction of human ratings. For example, as the size of the training dataset was reduced, the \nperformance of their models decreased. Yet strong performance (r = .75) was found to require \nabout 6,000 responses; this suggests that successful fine-tuning can be achieved without very \nlarge datasets that are difficult to obtain and manually score. Organisciak et al. also \ndemonstrated that few-shot learning—when a model uses its pre-existing knowledge along with \nonly a small amount of examples to complete a task—can be applied to AUT scoring, with GPT-\n4 demonstrating good predictions of human ratings (r = .70) when provided only 20 example \nresponses.  \nThe Present Research \nAutomatic creativity scoring has largely focused on scoring the originality of ideas, with \nmost work involving divergent thinking tasks such as the AUT (Beaty & Johnson, 2021; Johnson \net al., 2022; Organisciak et al., 2022; Stevenson et al., 2022). To our knowledge, no such work \nhas investigated approaches to automatically scoring the quality of ideas. Importantly, originality \nand quality are distinct components of creativity with unique relationships to other constructs \n(Morral-Robinson & Reiter-Palmon, 2013; Reiter-Palmon et al., 1997; 2009). Further, divergent \nthinking tasks, such as the AUT, often have little resemblance to problems encountered in real \nlife. Creative problem-solving tasks, in contrast, involve the production of a solution that is then \nevaluated for its originality and quality. These tasks can range from navigating social dilemmas \n(e.g., resolving a social dispute) to addressing job-related matters (e.g., crafting a marketing \n \nstrategy). Given the broad importance of real-world creative problem-solving (Reiter-Palmon, \nRobinson-Morral et al., 2012; Reiter-Palmon et al., 1998, 2009), it is important to develop \nefficient ways to score such naturalistic tasks, which are limited by the same bottlenecks of \nscoring divergent thinking tasks (e.g., high human labor cost).  \nTo address this issue, we examined data from the creative problem-solving task \n(CPST)—a paradigm involving a naturalistic, real-world expression of creative thinking—and \ntested whether it could be scored automatically by using LLMs. We compiled data from 10 \nstudies, providing us with a total of 3,243 participants who completed the CPST. Responses \nwere rated by human judges for both originality and quality, allowing us to train LLMs to predict \neach metric separately. We thus investigated, for the first time, whether the quality of creative \nresponses can be predicted with automated scoring methods. We fine-tuned two open-source \nLLMs, RoBERTa and GPT-2, on originality and quality human ratings to responses on the \nCPST. To evaluate model performance, predictions were compared to two other scoring \nmethods—elaboration (i.e. word count) and semantic distance —which reliably predict human \ncreativity ratings in divergent thinking tasks but have not yet been evaluated in the context of \ncreative problem-solving. \nMethods \nThe datasets and analysis code are available online on OSF \n(https://osf.io/45veq/?view_only=9fcb23a2564b4190a4ceb1f2f1707129). \nParticipants \nThe data for this project were compiled from a set of 10 previous studies. Of these, \nseven were conducted at the University of Nebraska Omaha and three at the Pennsylvania \nState University. The total dataset spans some published as well as unpublished datasets. The \ncompiled dataset included a total sample of 3,243 participants, with each participant providing a \nsingle response to the CPST (see Procedure). The total amount of responses was thus equal to \nthe number of participants. Each individual dataset included a single version of the CPST \n \npresented to all participants. The task instructions were generally the same, but the problem \npresented to participants varied. Across all 10 datasets, there are a total of six different \nproblems. \nProcedure \nParticipants completed several tasks and questionnaires within each study from which \nthe data was extracted. Each study involved its own unique combination of tasks and order of \npresentation; only the CPST is considered here.  \nCPST \nParticipants were presented with a problem scenario and tasked to generate a single \nsolution to solve it. Each problem scenario (1) described a central character and the everyday \nproblem they were facing (e.g., at school, at work, or in their relationships) and (2) provided \nsufficient context that highlighted several parameters of the problem (e.g., the characters’ \nmotivations, background, relationship with other entities in the scenario). Problem scenarios \nwere intentionally designed to be familiar to participants, allowing them to leverage their own \nexpertise and experiences when generating a solution. Additionally, problem scenarios were \ndetailed enough so participants could attend to any specific aspect(s) of the problem, allowing \nmultiple acceptable approaches and no clear or correct solution. Taken together, the structure \nof the CPST was conducive to generating an original yet high quality (i.e., creative) solution \n(Reiter-Palmon et al., 1998; 2009). \nHere is a breakdown of each dataset, named in terms of the specific problem used (i.e., \nacme, becky, clara, joan, mike, and ralph) and arbitrarily numbered to distinguish different \ndatasets which include the same problem: \n- Becky1: The data were extracted from a study employing Becky’s Problem and included \n365 responses. The data were collected from University of Nebraska Omaha \nundergraduate psychology students. \n \n- Acme1: The data were extracted from a study employing Acme’s Problem and included \n158 responses. The data were collected from University of Nebraska Omaha \nundergraduate psychology students. \n- Clara1: The data were extracted from a study employing Clara’s problem and included \n533 responses. The data were collected from University of Nebraska Omaha and \nCalifornia State University San Bernardino undergraduate psychology students. The \ndata from this study were previously included in an article published in a peer-reviewed \njournal (Reiter-Palmon et al., 2012). \n- Acme2: The data were extracted from a study employing Acme’s problem and included \n215 responses. The data were collected from University of Nebraska Omaha \nundergraduate psychology students. \n- Becky2: The data were extracted from a study employing Becky’s problem and included \n178 responses. The data were collected from University of Nebraska Omaha \nundergraduate psychology students. \n- Mike1: The data were extracted from a study employing Mike’s problem and included \n179 responses. The data were collected from University of Nebraska Omaha \nundergraduate psychology students. \n- Ralph1: The data were extracted from a study employing Ralph’s problem and included \n220 responses. The data were collected from Amazon Mechanical Turk workers. \n- Becky3: This data were extracted from a study employing Becky’s problem and included \n475 responses. The data were collected from Pennsylvania State University \nundergraduate psychology students. \n- Joan1: This data were extracted from a study employing Joan’s problem and included \n470 responses. The data were collected from Pennsylvania State University \nundergraduate psychology students. \n \n- Acme3: The data were extracted from a study employing Acme’s problem and included \n450 responses. The data were collected from Pennsylvania State University \nundergraduate psychology students. \nConsensual Assessment Technique \nResponses were scored by human raters on two metrics: originality and quality. The \nrating procedure was adapted from the CAT (Amabile, 1982) developed by Reiter-Palmon \n(2020). For each data set, all solutions were rated by a set of two to four raters for originality, \nwhile a different set of raters rated the same solutions for quality. Raters underwent a group \ntraining where they were instructed on how to evaluate responses for either originality or quality. \nDuring the training, emphasis was placed on defining either originality or quality to ensure raters \nhad a shared understanding of their assigned metric’s meaning. Raters would further take part \nin a practice round of scoring responses, where each rater would individually score a set of \nresponses before sharing them with the group. Discrepancies between raters would then be \naddressed to ensure a common understanding of the rating scale. A highly original solution was \ndefined as one that is unique and novel, imaginative, and not structured by the problem. A high \nquality solution was defined as one that addressed two or more issues posed by the problem \nand effectively addressed them. Scores were provided on a 5-point Likhert scale (5 = very \noriginal/very high quality; 1 = very unoriginal/very low quality). \nFollowing the training, raters were asked to independently rate 10% of solutions to the \nproblem they were trained on. Inter-rater reliability was evaluated on this 10% set to ensure \nconsistency in ratings. If raters demonstrated poor inter-rater reliability (i.e., intra-class \ncorrelation coefficient (ICC) < 0.60; Shrout & Fleiss, 1979), raters would undergo additional \ntraining sessions until a satisfactory level of inter-rater reliability was achieved. However, across \nstudies, all raters achieved satisfactory inter-rater reliability, so no additional training was \nnecessary. Finally, raters were asked to rate 100% of solutions to the problem they were trained \non; for inter-rater reliability results on 100% of responses across each dataset, see Table 1).  \n \nFactor analysis was used to obtain a single composite score for automated scoring. \nFactor analytic methods, such as confirmatory factor analysis (CFA), have been widely applied \nto improve reliability and validity of subjective creativity scoring methods by identifying common \nvariance across raters (Silvia, 2011). Separately for originality and quality, a single factor score \nwas calculated from each response, by extracting a weighted composite of all individual rater \nscores using CFA. The CFA was constructed as a unidimensional model with each individual \nrater as an observed variable. The analysis was run in R (version 4.2.1), using R studio (version \n2023.03.0) and by employing the lavaan package (version 0.6). \nAutomated Scoring of Quality and Originality \nDSI. To provide a semantic distance baseline to compare to fine-tuning, DSI was \nselected, given its ability to predict human creativity ratings on text responses of similar length \nto the CPST (i.e., short stories; Johnson et al., 2022). DSI uses word embeddings extracted \nfrom pre-trained models to compute pairwise semantic distance between all words in a text. \nBecause DSI is an unsupervised method (i.e., it does not involve adjusting the pre-trained \nmodel weights or parameters for a specific task like creativity scoring), it serves as an ideal \nbenchmark for evaluating the performance of supervised/fine-tuned models. Like other semantic \ndistance measures (Beaty & Johnson, 2021), DSI computes semantic dissimilarity via the \nconverse cosine similarity formula (1 – semantic similarity). To obtain a DSI score, for each text \nresponse to the CPST, the semantic distance values between all possible word pairs in the \nresponse are calculated and then averaged. We used word embeddings extracted from the \nBERT model, given its context sensitivity and reliable performance in predicting human \ncreativity ratings on other tasks (i.e., short stories; Johnson et al., 2022). DSI was computed \nusing the pipeline developed by Johnson et al. (2022). \nFine-Tuned Models. Following best practices in machine learning, responses from each \ndataset were randomly separated into training and a holdout-responses data following a 80/20 \nratio. In this way, the models were used to predict creativity scores for the holdout-responses \n \nonly after the training procedure was completed. This approach ensured that the generalizability \nof the models could be tested on a batch of unseen responses. Further, the Mike problem (i.e., \nthe entire mike1 dataset) was withheld from this splitting procedure and included as a holdout-\nprompt set. This method ensured that model generalizability could also be tested towards \nproblems it was never presented during training. If the models generalize to new responses and \nprompts, they have likely learned something general about how humans evaluate originality and \nquality on this task. This would allow new prompts to be scored by the models, which is \nimportant for intervention and longitudinal research requiring multiple prompts for repeated \ntesting.  \nTwo open-access LLMs were selected for fine-tuning, RoBERTa and GPT-2, which \nbelong to the BERT and GPT families, respectively. RoBERTa and GPT-2 notably differ in \nterms of their architecture and training objectives. For instance, RoBERTa was trained with a \nbidirectional approach (i.e., the model sees the entire sentence when making a prediction) \naimed towards understanding context (i.e., filling in the blanks), while GPT-2 underwent \nunidirectional training (i.e., the model sees only the text up to the part it is trying to predict) for \ngenerating text (i.e., creating sentences). Employing two models with fundamentally different \narchitectures allowed us to get a broader coverage of the LLM space. The two families of \nmodels have further been shown to perform strongly on a variety of tasks (Gilloz et al., 2020). \nAll fine-tuned models employed in the present analysis are open-accessible on the Huggingface \nplatform (https://huggingface.co/). For our analysis, we selected the RoBERTa-base model, \nwhich contains 123 million parameters, and the small version of the GPT-2 model which \ncontains 137 million parameters. These versions were selected in favor of larger variants of the \nsame models given their comparable sizes and the reduced computational resources needed \nfor fine-tuning models with fewer parameters. To extract the model architectures, weights, and \ntokenizers, the “Huggingface Transformers” suite of the “PyTorch” package was used via the \nPython programming language. Fine-tuning was performed twice for each model, separately for \n \noriginality and quality, such that the fine-tuned models would be specialized in the prediction of \neither score. In total, four fine-tuning procedures were employed for the subsequent prediction \nof originality and quality scores by both RoBERTa and GPT-2. All fine-tuning procedures used \nthe same hyperparameter settings, selected based on hyperparameter tuning of a related \ncreativity scoring model which was employed for unpublished work. The learning rate was set to \n0.00005, batch size to 16, and a total of 116 training epochs were implemented. All other \nhyperparameters were left to the default settings associated with each model. \nRoBERTa. The RoBERTa model is an improvement on the BERT model (Liu et al., \n2019), which was developed by Google and open-sourced in 2018 (Devlin et al., 2018). \nRoBERTa is a transformer model (see Vaswani et al., 2017) pre-trained via a self-supervised \napproach, without any human labeling of the natural language data. RoBERTa maintains a \nsimilar architecture to BERT but implements several changes in its pretraining procedure, such \nas an increased amount of data it was trained on (Liu et al., 2019). The base version of the \nmodel contains 123 million parameters and was pre-trained on a collection of data from the \nToronto BookCorpus (800M words), the English portion of Wikipedia (2,500M words), the CC-\nNews English dataset (63M news articles), the OpenWebText dataset of Reddit posts, and the \nStories dataset which contains a subset of the CommonCrawl repository. \nGPT-2. The GPT-2 model was developed by OpenAI and open-sourced in 2019 \n(Radford et al., 2019). GPT-2 is also a transformer model pretrained via a self-supervised \napproach. The full version of the model contains 1.5 billion parameters and was pre-trained on \nthe WebText dataset, which aggregates content from eight million web pages, primarily \nextracted from Reddit outbound links. This broad data scope allowed GPT-2 to generate diverse \nand coherent text passages across various topics.  \n \n \n \n \nResults \nWe first computed descriptive statistics for the CPST (Table 1). Inter-rater reliability for \noriginality and quality across all datasets was found to range from satisfactory to excellent \n(Originality ICCs = .71 - .96; Quality ICCs = .76 - .88). \nOutliers were identified based on Cook’s distance metrics and were thus removed before \nrunning each regression model (Cook, 1977). Thresholds for outlier identification were \ncalculated separately for the holdout-responses set (Cook’s Distance > 0.0066) and holdout-\nprompt set (Cook’s Distance > 0.023), based on the modified Cook’s cutoff equation (Fox, \n2019). Across all datasets, the average word count was 49, and mean ratings were 2.42 for \noriginality and 2.56 for quality. \nTable 1 \nDescriptive Statistics CPST Responses by Individual Datasets \nDataset Number of \nresponses \nMean \nwords per \nresponse \nMean \noriginality \n(SD) \nICC \noriginality \nMean \nquality \n(SD) \nICC \nquality \nAcme1 158 59 3.01 \n(1.10) .96 2.53 \n(0.85) .86 \nAcme2 215 74 3.05 \n(0.91) .83 3.06 \n(1.00) .87 \nAcme3 450 39 2.07 \n(0.78) .71 2.30 \n(0.81) .86 \nBecky1 365 65 2.53 \n(0.86) .81 2.38 \n(0.80) .87 \nBecky2 178 43 2.45 \n(0.91) .84 2.38 \n(0.84) .88 \nBecky3 475 49 2.29 \n(1.00) .87 2.76 \n(0.62) .76 \nClara1 533 53 2.77 \n(0.96) .88 2.49 \n(0.71) .88 \nJoan1 470 44 2.56 \n(0.88) .90 2.61 \n(0.82) .86 \nMike1 179 38 2.60 \n(0.79) .76 2.35 \n(0.84) .84 \nRalph1 220 32 1.89 \n(0.81) .78 2.74 \n(0.75) .85 \nNote. CPST = Creative problem-solving task; ICC = intra-class correlation coefficient. \n \nAutomatic Scoring: DSI and Word Count \nWe ran a series of Pearson’s correlations between DSI and word count across all \nstudies included in our final dataset.  We found a positive correlation between originality and \nquality ratings on the CPST (r = .51), consistent with past work (Arreola & Reiter-Palmon, 2016). \nWe further found a positive correlation between word count and quality (r = .68) and between \nword count and originality (r = .49), potentially indicating that raters emphasized word count of \nsolutions.  \nNext, we computed the correlations between DSI and originality and quality for each \nsolution. Across the full dataset, we found that DSI was moderately correlated with both \noriginality (r = .39) and quality (r = .44). We also found that DSI was correlated with word count \n(r = .56). \nAutomatic Scoring: Fine-Tuned LLMs \nThe fine-tuned LLMs were then used to predict originality and quality scores for each \nresponse, separately for each model. We then ran Pearson’s correlations between the predicted \nand human-rated scores. For originality, the RoBERTa model robustly predicted  \nhuman ratings on the holdout-responses set (r = .79). Further, GPT-2 predictions of originality \nalso correlated strongly with the human ratings (r = .80). We then observed that mean prediction \nerrors were typically higher for human-rated originality above or below two standard deviations \nfrom the mean than for scores between two standard deviations from the mean (Table 2); this \nresult suggests the models were overestimating the least original solutions and underestimating \nthe most original solutions (cf. Patterson et al., 2022). For quality, the RoBERTa and GPT-2 \nquality models showed equally robust predictions of human scores (r = .83), which were \nnumerically higher than those for the originality models. As with originality ratings, the mean \nprediction errors tended to be higher for human quality scores above or below two standard \ndeviations from the mean (Table 2). \n \n \nFigure 1 \nPredicted Human Originality and Quality Scores for RoBERTa and GPT-2 on the \nHoldout-Responses Set \n \n \nNote: Single responses are denoted by the black dots. Ideal performance (r = 1.00) is \ndenoted by the black line, while the orange line is the line of best fit. \n \n \n \n \n\n \nFigure 2 \nPredicted Human Originality and Quality Scores for RoBERTa and GPT-2 on the \nHoldout-Prompt Set.\n \n \nNote: Single responses are denoted by the black dots. Ideal performance (r = 1.00) is \ndenoted by the black line, while the orange line is the line of best fit.  \n  \n\n \n \nTable 2 \nMean Prediction Errors for Human-Rated Originality and Quality Scores \n \n Originality Quality \nDataset ± 2 SD Within 2 SD ± 2 SD Within 2 SD \nRoBERTA \nholdout-\nresponses set \n1.08 0.54 1.02 0.52 \nRoBERTA \nholdout-prompt \nset \n1.1 0.81 1.19 0.49 \nGPT-2 holdout-\nresponses set \n1.13 0.51 1.11 0.48 \nGPT-2 holdout-\nprompt set \n1.06 0.75 0.98 0.42 \nNote. Errors are calculated over factor scores and are averaged in the form of absolute \nvalues, between two standard deviations of the mean and above or below two standard \ndeviations of the mean. \nIn terms of the holdout-prompt set, model performance decreased for both RoBERTa \nand GPT-2 when predicting originality. The correlation between the RoBERTa model and \noriginality was slightly stronger (r = .41) than that between GPT-2 and originality (r = .40). In \nterms of quality, RoBERTa and GPT-2 both demonstrated extremely robust correlations with \nhuman ratings in the holdout-prompt set (r = .89). \n \nDiscussion \nRecent advances in machine learning have allowed for the automation of costly human \ntasks, including manual creativity scoring. Researchers are increasingly using these tools to \nautomatically score verbal creativity tasks, replacing the need for trained human raters and \naccelerating the pace of creativity research. Nevertheless, automated creativity assessment has \nfocused almost exclusively on scoring the originality of responses to the AUT (Beaty & Johnson, \n2021; Beaty et al., 2021; Buczak et al., 2022; Dumas & Dunbar, 2014; Dumas et al., 2020), with \nno methods currently available to evaluate the quality of responses to tasks that require real-\nworld problem solving.  \nIn our analysis, we trained LLMs (i.e., RoBERTa and GPT-2) to predict human-rated \noriginality and quality on the CPST. The models successfully predicted both metrics, yielding \nsubstantially higher correlations than those achieved by the best-performing semantic distance \nmodel (i.e., DSI). The performance of the trained LLMs also extended to an “unseen” prompt—\nproblems the models were not exposed to during training—finding better predictions for quality \nthan originality. Our results demonstrate for the first time that automated scoring methods can \ncapture the quality of responses to real-world creativity tasks, moving beyond the assessment of \noriginality alone.  \nCorrelations between the model predictions and human-rated originality on the trained \nitems were similarly robust for RoBERTa (r = .79) and GPT-2 (r = .80). This effect size was \nconsiderably higher than that of word count (r = .49) and double that between originality and \nDSI/semantic distance scores (r = .39). Interestingly, originality predictions dropped \nconsiderably for new prompts—a test of model generalization beyond its training set—with \nGPT-2 (r = .40) performing slightly worse than RoBERTa (r = .41). Of note, predictions on the \noriginality holdout-prompt set were worse than those achieved when employing word count \nalone. However, this drop in performance was not observed for predictions of quality on the \nholdout-prompt set, where the models performed slightly better than the holdout-responses set. \n \nIndeed, correlations between model predictions of quality for both RoBERTa and GPT on the \nholdout-prompt set were the highest achieved in the entire analysis (r = .89). In other words, \npredictions for both models explained 79% of the variance in human ratings of quality. These \nresults suggest that the models learned something general about how humans evaluate the \nquality of problem solutions that was consistent across problem prompts, but that originality \nevaluation may be more problem-specific. This observed difference may be due in part to the \nstronger relevance of superficial linguistic features such as word count in the prediction of \nquality. We nevertheless found that these models captured a degree of linguistic complexity \nbeyond the simple elaboration of responses. \nFor both originality and quality, we found that model prediction errors—the extent to \nwhich model-predicted scores deviated from human scores—across both the holdout-responses \nand holdout-sets, were lower within two standard deviations than for responses at the extremes. \nThis tendency to underestimate highly creative responses, and overestimate highly uncreative \nresponses, is likely to stem in part from the LLM's inability to extrapolate real-world implications \nfrom text. Further, given that the data for both originality and quality scores tended to be \nnormally distributed, the model was presented with fewer examples of responses associated \nwith the extremes of this distribution. A similar observation was made in a recent attempt to \nautomate the creativity scoring of drawings, indicating that this obstacle may be a pervasive one \nwhen it comes to the automated scoring of creativity (Patterson et al., 2022). While LLMs are \ntrained on vast quantities of natural language data and thus develop internal representations of \nmeaning, they lack the extensive human experience that would allow for the consideration of \nhighly contextualized responses. Further, for originality, the high prediction errors for responses \nfurther from the mean are likely indicative of the way that human raters tend to score originality \nin accordance with response frequency. Given that the models are likely building an internal \nrepresentation of the distribution of responses to the CPST during training, it is possible that \nthey are engaging in a degree of frequency-based analysis when predicting originality scores. If \n \nthis were to be the case, it then follows that they would underperform when presented with \nunseen items, as the models would lack an internal memory of the relevant responses and their \nfrequency. Of note, both RoBERTa and GPT-2 have been shown to underperform on certain \nbenchmark tasks when compared to more recent LLMs (e.g., Wang et al., 2023). Further, larger \nLLMs, with larger pre-training databases tend to acquire emergent abilities that would previously \nappear unobtainable (e.g., reasoning; Hagendorff, 2023; Ichien et al., 2023). The current \nanalysis should therefore be extended to larger models, such as GPT-4, to evaluate whether \npredictions of originality and quality on the CPST can be improved. It remains worth noting that \nat the time of writing this manuscript some of these larger models like GPT-4 are not freely \navailable for use, and researchers would likely incur considerable costs for employing these \nmodels in their work. \nOriginality and quality are two distinct and important criteria for creativity (Reiter-Palmon, \n2020), but automatic scoring methods have so far focused solely on originality. The present \nwork demonstrates for the first time that automated creativity assessment can be successfully \napplied not only for originality, but also for quality. Further, we provide the first evidence that \nautomated scoring can be applied to creative problem-solving tasks – tasks that emulate how \ncreativity comes about in real-world settings such as the classroom or the workplace (Reiter-\nPalmon et al., 2009). Given the projected importance of creativity to the global economy and \nworkforce (Florida, 2014), the development and dissemination of creativity tasks that can be \nefficiently scored is of vital importance. \n \n  \n \nReferences \nAcar, S., Berthiaume, K., Grajzel, K., Dumas, D., Flemister, C. T., & Organisciak, P. \n(2023). Applying automated originality scoring to the verbal form of Torrance tests of creative \nthinking. Gifted Child Quarterly, 67(1), 3-17. \nAmabile, T. M. (1982). Social psychology of creativity: A consensual assessment \ntechnique. Journal of personality and social psychology, 43(5), 997. \nAndreas, J., & Klein, D. (2014, June). How much do word embeddings encode about \nsyntax?. In Proceedings of the 52nd Annual Meeting of the Association for Computational \nLinguistics (Volume 2: Short Papers) (pp. 822-827). \nArnabile, T. (1996). Creativity in context: Update to the social psychology of creativity. \nBoulder, Colorado: Westview. \nArreola, N. J., & Reiter-Palmon, R. (2016). The effect of problem construction creativity \non solution creativity across multiple everyday problems. Psychology of Aesthetics, Creativity, \nand the Arts, 10(3), 287. \nBaer, J., & McKool, S. S. (2009). Assessing creativity using the consensual assessment \ntechnique. In Handbook of research on assessment technologies, methods, and applications in \nhigher education (pp. 65-77). IGI Global. \nBaer, J., Kaufman, J. C., & Gentile, C. A. (2004). Extension of the consensual \nassessment technique to nonparallel creative products. Creativity research journal, 16(1), 113-\n117. \nBarbot, B. (2018). The dynamics of creative ideation: Introducing a new assessment \nparadigm. Frontiers in Psychology, 9. https://doi.org/10.3389/fpsyg.2018.02529 \nBarredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, \nA., Garcia, S., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R., & Herrera, F. (2020). \nExplainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges \n \ntoward responsible AI. Information Fusion, 58, 82–115. \nhttps://doi.org/10.1016/j.inffus.2019.12.012 \nBeaty, R. E., & Johnson, D. R. (2021). Automating creativity assessment with SemDis: \nAn open platform for computing semantic distance. Behavior Research Methods, 53(2), 757–\n780. https://doi.org/10.3758/s13428-020-01453-w \nBeaty, R. E., & Kenett, Y. N. (2023). Associative thinking at the core of creativity. Trends \nin Cognitive Sciences. \nBeaty, R. E., Kenett, Y. N., Hass, R. W., & Schacter, D. L. (2022). Semantic memory \nand creativity: the costs and benefits of semantic memory structure in generating original ideas. \nThinking & Reasoning, 1-35. \nBeaty, R. E., Smeekens, B. A., Silvia, P. J., Hodges, D. A., & Kane, M. J. (2013). A first \nlook at the role of domain-general cognitive and creative abilities in jazz improvisation. \nPsychomusicology: Music, Mind, and Brain, 23(4), 262–268. https://doi.org/10.1037/a0034968 \nBeaty, R. E., Zeitlen, D. C., Baker, B. S., & Kenett, Y. N. (2021). Forward flow and \ncreative thought: Assessing associative cognition and its role in divergent thinking. Thinking \nSkills and Creativity, 41, 100859. https://doi.org/10.1016/j.tsc.2021.100859 \nBeaty, R. E. (2015). The neuroscience of musical improvisation. Neuroscience & \nBiobehavioral Reviews, 51, 108-117. \nBenedek, M., Christensen, A. P., Fink, A., & Beaty, R. E. (2019). Creativity assessment \nin neuroscience research. Psychology of Aesthetics, Creativity, and the Arts, 13(2), 218. \nBuczak, P., Huang, H., Forthmann, B., & Doebler, P. (2022). The machines take over: A \ncomparison of various supervised learning approaches for automated scoring of divergent \nthinking tasks. The Journal of Creative Behavior. Advance online publication. \nhttps://doi.org/10.1002/jocb.559 \n \nChristensen, A. P., Kenett, Y. N., Cotter, K. N., Beaty, R. E., & Silvia, P. J. (2018). \nRemotely close associations: Openness to experience and semantic memory structure. \nEuropean Journal of Personality, 32(4), 480–492. https://doi.org/10.1002/per.2157 \nCook, R. D. (1977). Detection of influential observation in linear regression. \nTechnometrics, 19(1), 15-18. https://doi.org/10.1080/00401706.1977.10489493 \nDale, R. (2021). GPT-3: What’s it good for?. Natural Language Engineering, 27(1), 113-\n118. \nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). \nIndexing by latent semantic analysis. Journal of the American Society for Information Science, \n41(6), 391–407. https://doi.org/10/db4ft5 \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep \nbidirectional transformers for language understanding. arXiv. https://arxiv.org/abs/1810.04805v2 \nDickson, D., Jonczyk, R., Gunay, E., Van Hell, J., & Siddique, Z. (2022, August). \nUtilization of Automatized Creativity Ratings in Linguistically Diverse Populations: Automated \nScores Align with Human Ratings. In 2022 ASEE Annual Conference & Exposition. \nDumas, D., & Dunbar, K. N. (2014). Understanding fluency and originality: A latent \nvariable perspective. Thinking Skills and Creativity, 14, 56–67. https://doi.org/10/f6wb79 \nDumas, D., & Runco, M. (2018). Objectively scoring divergent thinking tests for \noriginality: A re-analysis and extension. Creativity Research Journal, 30(4), 466-468. \nDumas, D., Organisciak, P., & Doherty, M. D. (2020). Measuring divergent thinking \noriginality with human raters and text-mining models: A psychometric comparison of methods. \nPsychology of Aesthetics, Creativity, and the Arts, 15(4), 645–663. https://doi.org/10/ghcsqq \nFink, A., Graif, B., & Neubauer, A. C. (2009). Brain correlates underlying creative \nthinking: EEG alpha activity in professional vs. novice dancers. NeuroImage, 46(3), 854-862. \nFirth, J. R. (1957). A synopsis of linguistic theory 1930–1955. In Studies in linguistic \nanalysis (pp. 1–32). Wiley-Blackwell \n \nFlorida, R. (2014). The creative class and economic development. Economic \ndevelopment quarterly, 28(3), 196-205. \nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, scope, limits, and consequences. \nMinds and Machines, 30(4), 681-694. \nForthmann, B., Beaty, R. E., & Johnson, D. R. (2022). Semantic spaces are not created \nequal—How should we weigh them in the sequel? On composites in automated creativity \nscoring. European Journal of Psychological Assessment. \nForthmann, B., Holling, H., Zandi, N., Gerwig, A., Çelik, P., Storme, M., & Lubart, T. \n(2017). Missing creativity: The effect of cognitive workload on rater (dis-)agreement in subjective \ndivergent-thinking scores. Thinking Skills and Creativity, 23, 129-\n139.https://doi.org/10.101t6/j.tsc.2016.12.005 \nFox, J. (2019). Regression diagnostics: An introduction. Sage publications. \nGibson, C., Folley, B. S., & Park, S. (2009). Enhanced divergent thinking and creativity \nin musicians: a behavioral and near-infrared spectroscopy study. Brain and cognition, 69(1), \n162-169. https://doi.org/10.1016/j.bandc.2008.07.009 \nGillioz, A., Casas, J., Mugellini, E., & Abou Khaled, O. (2020, September). Overview of \nthe Transformer-based Models for NLP Tasks. In 2020 15th Conference on Computer Science \nand Information Systems (FedCSIS) (pp. 179-183). IEEE. \nGunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S., & Yang, G.-Z. (2019). XAI—\nExplainable artificial intelligence. Science Robotics, 4(37), eaay7120. \nhttps://doi.org/10.1126/scirobotics.aay7120 \nGünther, F., Rinaldi, L., & Marelli, M. (2019). Vector-space models of semantic \nrepresentation from a cognitive perspective: A discussion of common misconceptions. \nPerspectives on Psychological Science, 14(6), 1006–1033. \nhttps://doi.org/10.1177/1745691619861372 \n \nHagendorff, T. (2023). Machine psychology: Investigating emergent capabilities and \nbehavior in large language models using psychological methods. arXiv preprint \narXiv:2303.13988. \nHeinen, D. J., & Johnson, D. R. (2018). Semantic distance: An automated measure of \ncreativity that is novel and appropriate. Psychology of Aesthetics, Creativity, and the Arts, 12(2), \n144. \nIchien, N., Stamenković, D., & Holyoak, K. J. (2023). Large Language Model Displays \nEmergent Ability to Interpret Novel Literary Metaphors. arXiv preprint arXiv:2308.01497. \nJawahar, G., Sagot, B., & Seddah, D. (2019, July). What does BERT learn about the \nstructure of language?. In ACL 2019-57th Annual Meeting of the Association for Computational \nLinguistics. \nJennings, K. E. (2010). Developing creativity: Artificial barriers in artificial intelligence. \nMinds and Machines, 20, 489-501. \nJohnson, D. R., Kaufman, J. C., Baker, B. S., Patterson, J. D., Barbot, B., Green, A. E., \n... & Beaty, R. E. (2022). Divergent semantic integration (DSI): Extracting creativity from \nnarratives with distributional semantic modeling. Behavior research methods, 1-34. \nKaufman, J. C., Baer, J., & Cole, J. C. (2009). Expertise, domains, and the consensual \nassessment technique. The Journal of creative behavior, 43(4), 223-233. \nKaufman, J. C., Baer, J., Cropley, D. H., Reiter-Palmon, R., & Sinnett, S. (2013). Furious \nactivity vs. understanding: How much expertise is needed to evaluate creative work?. \nPsychology of Aesthetics, Creativity, and the Arts, 7(4), 332. \nKenett, Y. N. (2019). What can quantitative measures of semantic distance tell us about \ncreativity? Current Opinion in Behavioral Sciences, 27, 11–16. \nhttps://doi.org/10.1016/j.cobeha.2018.08.010 \nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language \nmodels are zero-shot reasoners. arXiv. https://doi.org/10.48550/arXiv.2205.11916 \n \nLake, B. M., & Murphy, G. L. (2021). Word meaning in minds and machines. \nPsychological review. 130(2), 401–431. https://doi.org/10.1037/rev0000297 \nLandauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic \nanalysis. Discourse Processes, 25(2–3), 259–284. https://doi.org/10.1080/01638539809545028 \nLenci, A. (2018). Distributional models of word meaning. Annual review of Linguistics, 4, \n151-171. \nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2021). Pre-train, prompt, \nand predict: A systematic survey of prompting methods in natural language processing. arXiv. \nhttps://doi.org/10.48550/arXiv.2107.13586 \nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: \nA robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. \nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, \nL., & \nMaio, S., Dumas, D., Organisciak, P., & Runco, M. (2020). Is the reliability of objective \noriginality scores confounded by elaboration?. Creativity Research Journal, 32(3), 201-205. \nMednick, S. (1962). The associative basis of the creative process. Psychological \nReview, 69(3), 220–232. https://doi.org/10.1037/h0048850 \nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed \nrepresentations of words and phrases and their compositionality. Advances in neural \ninformation processing systems, 26. \nMorral-Robinson, E., & Reiter-Palmon, R. (2013). The interactive effects of self-\nperceptions and job requirements on creative problem solving. Journal of Creative Behavior, 47, \n200-214. \nOrganisciak, P., Acar, S., Dumas, D., & Berthiaume, K. (2022). Beyond Semantic \nDistance: Automated Scoring of Divergent Thinking Greatly Improves with Large Language \nModels. https://doi.org/10.13140/RG.2.2.32393.31840 \n \nPatterson, J. D., Barbot, B., Lloyd-Cox, J., & Beaty, R. (2022). AuDrA: An Automated \nDrawing Assessment Platform for Evaluating Creativity. \nPatterson, J. D., Merseal, H. M., Johnson, D. R., Agnoli, S., Baas, M., Baker, B. S., \nBarbot, B., Benedek, M., Borhani, K., Chen, Q., Christensen, J. F., Corazza, G. E., Forthmann, \nB., Karwowski, M., Kazemian, N., Kreisberg-Nitzav, A., Kenett, Y. N., Link, A., Lubart, T., . . . \nBeaty, R. E. (2023). Multilingual semantic distance: Automatic verbal creativity assessment in \nmany languages. Psychology of Aesthetics, Creativity, and the Arts, 17(4), 495–507. \nhttps://doi.org/10.1037/aca0000618 \nPaulus, D. H., & Renzulli, J. S. (1968). Computer scoring of creativity tests. Gifted Child \nQuarterly, 12, 79-83. \nPaulus, D. H., Renzulli, J. S., & Archambault, F. X. (1970). Computer Simulation of \nHuman Ratings of Creativity. Final Report. (No. 9-A-032). \nhttps://files.eric.ed.gov/fulltext/ED060658.pdf \nPennington, J., Socher, R., & Manning, C. D. (2014, October). Glove: Global vectors for \nword representation. In Proceedings of the 2014 conference on empirical methods in natural \nlanguage processing (EMNLP) (pp. 1532-1543). \nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language \nunderstanding by generative pre-training. OpenAI. https://openai.com/blog/language-\nunsupervised/ \nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & \nLiu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. \nJournal of Machine Learning Research, 21(140), 1–67. https://jmlr.org/papers/v21/20-074.html \nRajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ questions \nfor machine comprehension of text. arXiv. https://doi.org/10.48550/arXiv.1606.05250 \n \nReiter-Palmon, R., Forthmann, B., & Barbot, B. (2019). Scoring divergent thinking tests: \nA review and systematic framework. Psychology of Aesthetics, Creativity, and the Arts, 13(2), \n144–152. https://doi.org/10.1037/aca0000227 \nReiter-Palmon, R., Illies, M. Y., Kobe Cross, L., Buboltz, C., & Nimps, T. (2009). \nCreativity and domain specificity: The effect of task type on multiple indexes of creative \nproblem-solving. Psychology of Aesthetics, Creativity, and the Arts, 3(2), 73. \nReiter-Palmon, R., Mumford, M. D., & Threlfall, K. V. (1998). Solving everyday problems \ncreatively: The role of problem construction and personality type. Creativity Research Journal, \n11(3), 187-197. \nReiter-Palmon, R., Mumford, M. D., Boes, O. J., & Runco, M. A. (1997). Problem \nconstruction and creativity: The role of ability, cue consistency, and active processing. Creativity \nResearch Journal, 10, 9-24. \nReiter-Palmon, R., Robinson-Morral, E. J., Kaufman, J. C., & Santo, J. B. (2012). \nEvaluation of self-perceptions of creativity: Is it a useful criterion?. Creativity Research Journal, \n24(2-3), 107-114. \nReiter-Palmon, R. (2020). The consensual assessment technique: Refinement and \nfurther development. Creativity at work: A festschrift in honor of Teresa Amabile, 157-165. \nRoemmele, M., Bejan, C. A., & Gordon, A. S. (2011). Choice of plausible alternatives: \nAn evaluation of commonsense causal reasoning. AAAI Spring Symposium: Logical \nFormalizations of Commonsense Reasoning, 90–95. \nRunco, M. A., & Acar, S. (2012). Divergent thinking as an indicator of creative potential. \nCreativity research journal, 24(1), 66-75. \nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: uses in assessing rater \nreliability. Psychological bulletin, 86(2), 420. \nSilvia, P. J., Winterstein, B. P., Willse, J. T., Barona, C. M., Cram, J. T., Hess, K. I., \nMartinez, J. L., & Richard, C. A. (2008). Assessing creativity with divergent thinking tasks: \n \nExploring the reliability and validity of new subjective scoring methods. Psychology of \nAesthetics, Creativity, and the Arts, 2(2), 68–85. https://doi.org/10.1037/19313896.2.2.68 \nSilvia, P. J. (2011). Subjective scoring of divergent thinking: Examining the reliability of \nunusual uses, instances, and consequences tasks. Thinking Skills and Creativity, 6(1), 24-30. \nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., & Potts, C. (2013). \nRecursive deep models for semantic compositionality over a sentiment treebank. Proceedings \nof the 2013 Conference on Empirical Methods in Natural Language Processing, 1631–1642. \nhttps://aclanthology.org/D13-1170 \nStevenson, C., Smal, I., Baas, M., Grasman, R., & van der Maas, H. (2022). Putting \nGPT-3's Creativity to the (Alternative Uses) Test. arXiv preprint arXiv:2206.08932. \nTorrance, E. P. (1972). Predictive validity of the Torrance Tests of Creative Thinking. \nThe Journal of Creative Behavior, 6(4), 236-262. https://doi.org/10.1002/j.2162-\n6057.1972.tb00936.x \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \n& Polosukhin, I. (2017). Attention is all you need. arXiv. http://arxiv.org/abs/1706.03762 \nWang, L., Du, Z., Liu, D., Deng, C., Yu, D., Jiang, H., ... & Tu, Z. (2023). Disco-Bench: A \nDiscourse-Aware Evaluation Benchmark for Language Modelling. arXiv preprint \narXiv:2307.08074. \nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). Xlnet: \nGeneralized autoregressive pretraining for language understanding. Advances in Neural \nInformation Processing Systems, 32. \n \n \n ",
  "topic": "Originality",
  "concepts": [
    {
      "name": "Originality",
      "score": 0.9894698262214661
    },
    {
      "name": "Creativity",
      "score": 0.8462862968444824
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.6644250750541687
    },
    {
      "name": "Task (project management)",
      "score": 0.6530666351318359
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5077337026596069
    },
    {
      "name": "Psychology",
      "score": 0.5017123222351074
    },
    {
      "name": "Elaboration",
      "score": 0.4971192181110382
    },
    {
      "name": "Naturalism",
      "score": 0.4358731508255005
    },
    {
      "name": "Promotion (chess)",
      "score": 0.4112429618835449
    },
    {
      "name": "Computer science",
      "score": 0.3455500602722168
    },
    {
      "name": "Social psychology",
      "score": 0.3280133903026581
    },
    {
      "name": "Epistemology",
      "score": 0.14839622378349304
    },
    {
      "name": "Humanities",
      "score": 0.13321563601493835
    },
    {
      "name": "Management",
      "score": 0.09437265992164612
    },
    {
      "name": "Political science",
      "score": 0.07735404372215271
    },
    {
      "name": "Law",
      "score": 0.06477132439613342
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}