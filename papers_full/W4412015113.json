{
  "title": "Evaluating large language models for renal colic imaging recommendations: a comparative analysis of Gemini, copilot, and ChatGPT-4.0",
  "url": "https://openalex.org/W4412015113",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1977989713",
      "name": "Yavuz Yigit",
      "affiliations": [
        "Queen Mary University of London",
        "Hamad Medical Corporation"
      ]
    },
    {
      "id": "https://openalex.org/A2580222186",
      "name": "Asım Enes Özbek",
      "affiliations": [
        "Kocaeli Üniversitesi"
      ]
    },
    {
      "id": "https://openalex.org/A2587542944",
      "name": "Betul Dogru",
      "affiliations": [
        "Kocaeli Üniversitesi"
      ]
    },
    {
      "id": "https://openalex.org/A3093352567",
      "name": "Serkan Gunay",
      "affiliations": [
        "Hitit Üniversitesi"
      ]
    },
    {
      "id": "https://openalex.org/A2527786991",
      "name": "Baha Alkahlout",
      "affiliations": [
        "Qatar University",
        "Hamad Medical Corporation"
      ]
    },
    {
      "id": "https://openalex.org/A1977989713",
      "name": "Yavuz Yigit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2580222186",
      "name": "Asım Enes Özbek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2587542944",
      "name": "Betul Dogru",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3093352567",
      "name": "Serkan Gunay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2527786991",
      "name": "Baha Alkahlout",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4390590822",
    "https://openalex.org/W4390741581",
    "https://openalex.org/W2049532218",
    "https://openalex.org/W2116199679",
    "https://openalex.org/W2967342127",
    "https://openalex.org/W4307777205",
    "https://openalex.org/W4407445348",
    "https://openalex.org/W4393941607",
    "https://openalex.org/W4399489002",
    "https://openalex.org/W4408360989",
    "https://openalex.org/W4387956115",
    "https://openalex.org/W4389732527",
    "https://openalex.org/W3046238651",
    "https://openalex.org/W3200759624",
    "https://openalex.org/W3082916567"
  ],
  "abstract": null,
  "full_text": "RESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, \nsharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  v e c  o m m  o n s .  o r  g / l i c e n s e s / b y / 4 . 0 /.\nYigit et al. International Journal of Emergency Medicine          (2025) 18:123 \nhttps://doi.org/10.1186/s12245-025-00895-3\nInternational Journal of \nEmergency Medicine\n*Correspondence:\nBaha AlKahlout\nBkahlout@hamad.qa\n1Department of Emergency Medicine, Hamad Medical Corporation,  \nDoha 3050, Qatar\n2Blizard Institute, Queen Mary University, London, UK\n3Department of Emergency Medicine, Kocaeli City Hospital, Kocaeli, \nTurkey\n4Department of Emergency Medicine, Hitit University, Çorum, Turkey\n5College of Medicine, Qatar University, Doha, Qatar\nAbstract\nBackground The field of natural language processing (NLP) has evolved significantly since its inception in the 1950s, \nwith large language models (LLMs) now playing a crucial role in addressing medical challenges.\nObjectives This study evaluates the alignment of three prominent LLMs—Gemini, Copilot, and ChatGPT-4.0—with \nexpert consensus on imaging recommendations for acute flank pain.\nMethods A total of 29 clinical vignettes representing different combinations of age, sex, pregnancy status, likelihood \nof stone disease, and alternative diagnoses were posed to the three LLMs (Gemini, Copilot, and ChatGPT-4.0) between \nMarch and April 2024. Responses were compared to the consensus recommendations of a multispecialty panel. The \nprimary outcome was the rate of LLM responses matching the majority consensus. Secondary outcomes included \nalignment with consensus-rated perfect (9/9) or excellent (8/9) responses and agreement with any of the nine panel \nmembers.\nResults Gemini aligned with the majority consensus in 65.5% of cases, compared to 41.4% for both Copilot \nand ChatGPT-4.0. In scenarios rated as perfect or excellent by the consensus, Gemini showed 69.5% agreement, \nsignificantly higher than Copilot and ChatGPT-4.0, both at 43.4% (p = 0.045 and < 0.001, respectively). Overall, Gemini \ndemonstrated an agreement rate of 82.7% with any of the nine reviewers, indicating superior capability in addressing \ncomplex medical inquiries.\nConclusion Gemini consistently outperformed Copilot and ChatGPT-4.0 in aligning with expert consensus, \nsuggesting its potential as a reliable tool in clinical decision-making. Further research is needed to enhance the \nreliability and accuracy of LLMs and to address the ethical and legal challenges associated with their integration into \nhealthcare systems.\nKeywords Large Language models (LLMs), Natural Language processing (NLP), Renal colic, Imaging \nrecommendations, Gemini, Copilot, ChatGPT-4.0\nEvaluating large language models \nfor renal colic imaging recommendations: \na comparative analysis of Gemini, copilot, \nand ChatGPT-4.0\nYavuz Yigit1,2, Asım Enes Ozbek3, Betul Dogru3, Serkan Gunay4 and Baha AlKahlout1,5*\nPage 2 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \nIntroduction\nThe field of natural language processing (NLP) has its ori-\ngins in the 1950s, when early researchers laid the foun -\ndations for this interdisciplinary field at the intersection \nof linguistics, computer science, and artificial intelligence \n(AI). While the initial capabilities of NLP were lim -\nited and unsuitable for direct application in high-stakes \nmedical decision-making, the field has since under -\ngone a remarkable transformation. These large language \nmodels (LLMs) have been developed through signifi -\ncant advancements in AIand NLP , and have undergone \nextensive training on comprehensive datasets compris -\ning scholarly publications, books, and guidelines. Since \ntheir public introduction, large language models (LLMs) \nhave been employed in various scientific endeavors to \naddress medical challenges, such as optimizing health -\ncare administration and tackling global health issues [ 1, \n2]. These tools hold considerable potential for healthcare \napplications. Nonetheless, the research outcomes remain \ninconclusive due to several influencing factors, including \nthe evaluation of different concepts, precision, and repro-\nducibility [1, 3].\nUrolithiasis-induced flank pain is a common reason for \nemergency department (ED) visits, with the incidence \nof these visits increasing significantly in recent years [ 4, \n5]. Despite this rise, there remains uncertainty among \nED clinicians regarding the optimal imaging modality \nfor diagnosing obstructive kidney disorders. According \nto the latest guidelines from the European Association \nof Urology on urolithiasis, Non-contrast-enhanced CT \n(NCCT) is recommended for confirming the presence \nof stones in patients presenting with acute flank pain, \nfollowing an initial ultrasound assessment [ 6]. NCCT \nis considered a valuable diagnostic tool, particularly \nin cases where the findings from the initial ultrasound \nexamination are inconclusive. However, the guidelines \nlack clear guidance on specific clinical scenarios where \nCT scans may not be necessary for effectively managing \npatients with acute flank pain.\nThe consensus report by the American College of \nEmergency Medicine, the American College of Radiology, \nand the American Association of Urology asserts that CT \nimaging is often unnecessary in various clinical scenar -\nios, and some patients may not require any imaging at all \n[7]. Despite these recommendations, a nationwide study \nof emergency department practices revealed a significant \nincrease in the use of CT scans for patients presenting \nwith suspected obstructive kidney disorders from 2012 to \n2018 [8]. In contrast, the same study showed that ultra -\nsonography was employed in fewer than 3% of ED visits \ninvolving suspected obstructive kidney disorders.\nWhen the literature is examined, there is no study \nshowing to what extent LLMs can be useful in choos -\ning the right imaging method in renal colic patients. \nAlso the association between LLMs and the consensus \nreport, as well as their associated advantages, remains \nunknown. This study hypothesizes that, owing to their \ncapacity to access extensive data, the three most promi -\nnent LLMs (OpenAI’s ChatGPT-4.0, Google’s Gemini, \nand Microsoft’s Copilot) could provide valuable insights \ninto determining the necessity and appropriateness of \nimaging for patients presenting with acute flank pain in \nthe emergency department. Specifically, this study aims \nto evaluate whether the recommendations of these three \nAI chatbots align with the consensus report when applied \nto 29 pre-defined clinical vignettes concerning imaging in \ncases of suspected renal colic.\nMethods\nSince no patient or clinician data is used in the study, \nethics committee approval is not required. The multi -\nspecialty consensus report by the American College of \nEmergency Medicine (ACEP), the American College of \nRadiology (ACR), and the American Association of Urol -\nogy (AUA) provided twenty-nine distinct cases of acute \nflank pain, each representing different combinations of \nage, sex, pregnant status, likelihood of stone disease, and \npossibility of an alternative diagnosis [ 7]. The twenty-\nnine vignettes used in this study were derived from a \nconsensus report developed by ACEP , ACR, and AUA, \nbased on a systematic literature review and a structured \nmodified Delphi process, ensuring that each vignette rep-\nresented a clinically important and evidence-based case. \nDetailed information about the 29 vignettes is shown in \nSupplementary Table 1, which outlines the demographic \nand clinical characteristics included in each scenario.\nBetween March and April 2024, each vignette was pre -\nsented with the same question pattern to the three LLMs, \nwhich include OpenAI’s ChatGPT-4.0, Google’s Gemini, \nand Microsoft’s Copilot. The same question was asked all \nof them ‘Please answer according to the up-to-date data, \nfor this scenario which would be the most appropriate \nmanagement method to perform in an ED: no (further) \nimaging or point-of-care ultrasonography or radiol -\nogy-performed ultrasonography or reduced-radiation \ndose CT, standard CT (non-contrast) or CT with intra -\nvenous contrast. ’ Responses generated by LLMs were \ndocumented on the digital data sheets. The agreement \nbetween the responses of LLMs was assessed. The level of \nconsensus was determined as the number of expert pan -\nelists in agreement for each vignette, as defined by the \noriginal consensus report: perfect (9/9), excellent (8/9), \ngood (6 to 7/9), moderate (5/9), and not reached (< 5/9). \nThe answers covered a range of imaging choices, such \nas no (further) imaging, point-of-care ultrasonography, \nradiology-performed ultrasonography, reduced-radia -\ntion dose CT, standard CT (non-contrast), and CT with \nintravenous contrast. Furthermore, to ensure adherence \nPage 3 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \nto the consensus report, the replies were categorized into \nthree main groups: no further imaging, ultrasonography, \nand CT. This grouping was prominent for consistency \nand comparability, as the LLM’s imaging recommenda -\ntions were consistent with the predefined consensus \nresponses used by the expert panel, facilitating analysis \nand interpretation of results. The responses of AI chat -\nbots were compared both between themselves and with \nthe responses of the consensus members. The reference \nstudy identified 9 consensus members, including repre -\nsentatives from 3 specialty societies: the American Col -\nlege of Emergency Physicians (ACEP), the American \nCollege of Radiology, and the American Urological Asso -\nciation. All consensus members were board-certified \npracticing physicians and were nominated based on their \nprevious work on specialty-specific guidelines [ 7]. Each \nLLM’s answer was directly compared against the consen -\nsus response for each vignette, thereby ensuring align -\nment between the LLMs and the expert panel.\nThe primary outcome measure of the study was to \ncompare the three LLMs for the rate of giving the same \nanswers as the majority of the members of the consen -\nsus on previously defined 29 vignettes about imaging in \nsuspected renal colic. The secondary outcome measures \nof the study were to determine the compatibility of three \nLLMs with these answers in the questions for which con -\nsensus was achieved at perfect and excellent level in the \nconsensus report and to determine the agreement level \nof three LLMs with not the majority consensus, but by \nwhether the answer of the 3 LLMs was consistent with \nthe answer of any of the 9 members.\nStatistical analyses were performed using Jamovi ver -\nsion 2.5.3.0 Computer Software. The agreement between \nthe answers given by the LLMs and the consensus mem -\nbers was manually assessed separately in 3 groups (the \nmajority of answers provided across all questions, the \nconsensus ratings of “perfect” or “excellent. “, the response \nof any of the 9 reviewers that took part in the consensus). \nResults were given as number of correct answers and \npercentage. The agreement between the correct answers \ngiven by the LLMs was evaluated by the chi-square test \nin pairs, separately for each group. To statistically show \nthe alignment of LLMs with a reference cohort, Fleiss’ \nkappa values have been calculated. P values less than 0.05 \nwere considered statistically significant.\nResults\nTable 1 presents the individual responses of each LLM \nand their agreement with the priori consensus. Figure  1 \nillustrates the distribution of imaging recommenda -\ntions (no imaging, CT, or ultrasonography) provided \nby each LLM. When evaluating the compatibility of the \nLLMs with the majority of answers provided across all \nquestions, Gemini matched the consensus members’ \nresponses in 19 questions (65.5%). In contrast, Copilot \nand ChatGPT-4.0 provided the same answers in 12 ques -\ntions (41.4%) (Fig. 2).\nGemini correctly answered 16 out of 23 questions \n(69.5%) where the consensus rated the answers as “per -\nfect” or “excellent. ” In contrast, both Copilot and Chat -\nGPT-4.0 correctly answered 10 out of 23 questions \n(43.4%) in alignment with the consensus ratings of “per -\nfect” or “excellent. ” (Fig.  3). Gemini responses showed \ngreater agreement compared to Copilot ( p = 0.045) and \nChatGPT-4.0 ( p < 0.001), indicating a consistent advan -\ntage in performance. Furthermore, statistical analysis \nrevealed a substantial difference in agreement between \nCopilot and ChatGPT-4.0 (p = 0.001).\nIrrespective of the majority, Gemini had the highest \nlevel of agreement, with 82.7% (24/29), when evaluat -\ning the agreement of LLMs with the response of any of \nthe 9 reviewers that took part in the consensus. Copilot \nresponses agreement with 18 of the 29 clinical vignettes \n(62.1%), whereas ChatGPT-4.0 showed agreement in \n19 cases (65.5%) (Fig.  4). No meaningful difference in \nagreement was observed between Gemini and Copilot \n(p = 0.917). In contrast, ChatGPT-4.0 demonstrated sig -\nnificantly different agreement rates compared to both \nGemini ( p = 0.019) and Copilot ( p = 0.01). Fleiss’ kappa \nvalues for ChatGPT-4.0, Gemini, and Copilot in compari-\nson with the nine expert reviewers were 0.638, 0.698, and \n0.634, respectively—indicating substantial agreement for \nall three models.\nDiscussion\nIn our study, we evaluated the alignment of three LLMs– \nGemini, Copilot, and ChatGPT-4.0– with consensus \nanswers provided by a panel of experts. The findings \nindicate that Gemini consistently demonstrated a higher \ndegree of agreement with the expert consensus com -\npared to the other LLMs examined. Specifically, Gemini \nexcelled in compatibility with both the majority of the \nconsensus members’ answers and with the responses of \nany of the nine reviewers who participated in the consen-\nsus, regardless of majority opinion. Additionally, among \nthe three commonly utilized LLMs, Gemini was the most \naligned in queries where a perfect or excellent consensus \nwas achieved.\nThe extent to which LLMs can be utilized in clinical \ndecision-making processes remains a topic of growing \ninterest. To shed light on this issue, several studies have \nbeen conducted—and continue to be conducted—com -\nparing the performance of various LLMs [ 9–12]. In a \nstudy involving 134 clinical cases, the diagnostic, thera -\npeutic, and management-related decision-making accu -\nracy of three different LLMs was evaluated, and Gemini \nwas found to have the lowest overall performance [ 9]. In \nanother study focused on surgical planning in glaucoma \nPage 4 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \npatients, Gemini demonstrated 32% lower agreement \ncompared to ChatGPT-4, indicating inferior performance \n[10]. Similarly, when assessed as an intraoperative deci -\nsion-support tool in plastic surgery, Gemini again exhib -\nited suboptimal performance relative to ChatGPT-4 [ 11]. \nConversely, another study evaluating the ability of Chat -\nGPT-4 and Gemini to assess diagnosis and treatment \nplans for patients with acute cholecystitis found compa -\nrable performances between the two models [12]. In con-\ntrast to these previous studies, our study found that, in \nthe context of selecting the appropriate imaging modality \nfor patients with renal colic, Gemini produced responses \nthat were more aligned with those of the expert consen -\nsus panel.\nOne potential factor contributing to variability in LLM \nresponses is the influence of different guideline sources \nused during model training. While our study utilized \nthe 2019 consensus report [ 7] as the reference standard, \nit is possible that the UK NICE guidelines, which have \nrecommended low-dose non-contrast CT as the first-line \nimaging modality since January 2019, were included in \nthe training datasets of the evaluated LLMs. This differ -\nence in guideline exposure may have contributed to dis -\ncrepancies in LLM recommendations.\nHowever, this does not undermine the validity of our \nfindings, as our study was specifically designed to assess \nhow well LLMs align with an established expert consen -\nsus rather than to evaluate the absolute correctness of \ntheir responses based on multiple guideline sources. In \nclinical practice, variations in recommendations across \ndifferent guidelines are well-recognized and do not indi -\ncate an inherent flaw in an individual guideline or its \ninterpretation. Future research could further investigate \nhow different LLMs integrate and prioritize diverse clini -\ncal guidelines, providing additional insights into their \ndecision-making processes.\nGemini demonstrated a significantly higher level of \nconcurrence with the consensus, providing responses \nTable 1 Answers and agreements of LLMs with the priori consensus responses\nAnswers of LLMs Agreement of LLMsa Priori consensus degreeb\nQuestion No. Gemini Copilot ChatGPT Gemini Copilot ChatGPT\n1 RDCT POCUS NCCT - + - Moderate\n2 POCUS RDCT NCCT + - - Moderate\n3 RDCT POCUS RDCT + + + Good\n4 RDCT POCUS NCCT - + - Perfect\n5 RDCT POCUS NCCT + + + Excellent\n6 RDCT POCUS RDCT + - + Perfect\n7 RDCT RDCT NCCT - - - Perfect\n8 No imaging NCCT NCCT + - - Perfect\n9 RDCT POCUS NCCT + - + Perfect\n10 RDCT RDCT NCCT + + + Excellent\n11 No imaging POCUS NCCT + - + Excellent\n12 RDCT POCUS NCCT + + + Excellent\n13 RDCT RDCT NCCT + + + Good\n14 RDCT RDCT NCCT + + + Excellent\n15 No imaging NCCT NCCT + + + Moderate\n16 RDCT NCCT NCCT + + + Excellent\n17 RDCT NCCT NCCT + + + Perfect\n18 RDCT NCCT RDCT + + + Perfect\n19 POCUS POCUS RPUS - - - Perfect\n20 No imaging RPUS RPUS - + + Perfect\n21 No imaging RDCT RPUS + - + Excellent\n22 POCUS POCUS RPUS + + + Perfect\n23 No imaging RDCT RPUS + - - Perfect\n24 No imaging POCUS POCUS + - - Perfect\n25 POCUS POCUS POCUS + + + Perfect\n26 No imaging POCUS RPUS + - - Perfect\n27 No imaging No imaging RPUS + + - Excellent\n28 POCUS POCUS POCUS + + + Good\n29 POCUS POCUS RPUS + + + Perfect\n+: Compatible, -: Not compatible RDCT: reduced-radiation dose computer tomography POCUS: point-of-care ultrasonography NCCT: non-contrast computer \ntomography RPUS: radiology-performed ultrasonography a the response of any of the 9 reviewers b According to a consensus report by the American College of \nEmergency Physicians, the American College of Radiology, and the American Urological Association; consensus was defined as perfect (9/9), excellent (8/9), good (6 \nto 7/9), moderate (5/9), and not reached (< 5/9) (7)\nPage 5 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \nFig. 2 Number of compatible and non-compatible answers\n \nFig. 1 Number of recommendations\n \nPage 6 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \nFig. 4 Number of answers compatible with at least one expert\n \nFig. 3 Number of answers according to expert consensus ratings\n \nPage 7 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \nthat were largely similar to those of the majority of con -\nsensus participants. This suggests that Gemini may have \nbetter performance in understanding and appropriately \ninterpreting clinical case examples consistent with expert \nopinions. In contrast, only 41.4% of the questions were \nanswered by ChatGPT-4.0 and Copilot in agreement with \nthe consensus, indicating a less robust alignment with \nexpert guidelines. Notably, Gemini achieved an agree -\nment rate of 82.7% when comparing its overall responses \nwith those of the nine reviewers. These findings indicate \nthat Gemini could be a more credible tool for applica -\ntions requiring strong conformity with expert guidelines \n[7]. In addition, Gemini’s high rate of agreement with \nexpert evaluations suggests that it may assist clinicians \nin making imaging decisions in patients with renal colic.\nThe high level of agreement of the Gemini scale with the \nhighest scoring responses suggests that it may have the \npotential for widespread adoption in professional and \nacademic contexts if supported by future studies. In addi-\ntion, Gemini and other LLMs can have more sensitive \nevaluation capabilities with each new update. With the \nuse of more data sets and the development of more fine \nanalysis capabilities with each new update, more accu -\nrate results can be achieved in the clinical context. Future \nstudies can contribute to the increase of our knowledge \nand experience in this subject by focusing on how the \nreliability and accuracy rates of these LLMs, especially \nGemini, can be improved with new updates.\nAlthough the use of AI-enabled LLMs may attract \nattention with their performance in clinical case assess -\nments, the integration of AI technologies into healthcare \nsystems raises significant ethical and legal concerns that \nwarrant careful consideration [ 13, 14]. As these complex \nmodels become increasingly embedded in critical clinical \ndecision-making processes, it is crucial to meticulously \nassess the multifaceted risks and responsibilities associ -\nated with their potential impact on patient outcomes. A \nprimary and pressing ethical issue pertains to the trans -\nparency and explainability of the decision-making mech -\nanisms within AI systems. Healthcare providers must \nbe able to comprehend and trust the recommendations \nand rationale generated by these AI systems in order \nto maintain patient confidence and ensure appropriate \ntreatment. The lack of explainability can lead to profound \nchallenges in establishing clear lines of accountability, \nmaking it profoundly difficult to determine whether the \nhealthcare professional or the AI system bears respon -\nsibility for an erroneous or suboptimal decision that \ncould significantly impact a patient’s well-being [15]. This \nadaptation is essential for safeguarding patient trust and \nensuring that the integration of AI into healthcare sup -\nports, rather than undermines, the quality and reliability \nof patient care.\nAnother concern regarding the usability of LLMs in \nclinical case evaluations is the legal processes related to \nthe compliance of the use of AI in healthcare with stan -\ndards regarding patient privacy and data protection \n[16]. In Turkiye, the Law on the Protection of Personal \nData (KVKK) imposes strict regulations on the man -\nagement and disclosure of patient data [ 17]. Similarly, \nin the United States, the Health Insurance Portability \nand Accountability Act (HIPAA) sets stringent rules on \npatient data handling [ 18]. AI systems must be designed \nto comply with these requirements in both jurisdictions, \nensuring the safeguarding of patient information against \nunauthorized access and breaches. This highlights the \nimportance of ensuring that AI systems in healthcare set-\ntings are designed to comply with stringent data protec -\ntion regulations—such as KVKK in Türkiye and HIPAA \nin the U.S.—to safeguard patient information from unau-\nthorized access and breaches. Legally, the utilization of \nAI in healthcare must adhere to standards pertaining \nto patient confidentiality and data protection [ 16]. The \nHealth Insurance Portability and Accountability Act in \nthe United States imposes strict rules on the manage -\nment and disclosure of patient data [17]. AI systems must \nbe engineered to adhere to these requirements, guaran -\nteeing the safeguarding of patient information against \nunauthorized access and breaches.\nLimitations\nThis study has several limitations that should be consid -\nered when interpreting its findings. Firstly, the variability \nin the types and phrasing of questions posed to the LLMs \ncan influence their responses, introducing variability that \nmay affect the conclusions. However, the use of a stan -\ndardized set of 29 clinical scenarios helps mitigate this \nvariability, ensuring a consistent basis for comparison.\nSecondly, the study’s generalizability is limited by \nthe specific scenarios and questions presented. While \nthe results may not apply to all medical inquiries, the \nselected scenarios are representative of common clinical \nsituations in emergency departments. This relevance sup-\nports the applicability of the findings within the intended \ncontext. Thirdly, each vignette was presented only once \nin our study. Repetitive testing might increase the quality \nand robustness of the results of the study. Another limita-\ntion is that no power analysis was applied in our study. \nInstead, the study was designed using all vignettes in the \nconsensus report. Finally, Copilot uses the infrastructure \nof ChatGPT-4.0. However, it also integrates the Microsoft \ndatabase into its infrastructure, focusing on producing \nmore balanced, creative and precise answers. Therefore, \nalthough they use similar infrastructures, they use differ -\nent approaches to reach results on the given data, which \ndistinguishes these two LLMs models from each other.\nPage 8 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \nDespite these limitations, the study’s design and sce -\nnario selection provide a strong foundation for its con -\nclusions. Future research can further address these \nlimitations to enhance our understanding of LLMs in \nhealthcare settings.\nConclusion\nThis study demonstrates that Gemini exhibits a higher \ndegree of alignment with expert consensus compared to \nCopilot and ChatGPT-4.0, has superior ability to address \ncomplex medical inquiries compared to other LLMs. \nDespite some limitations, the findings underscore the \npotential of Gemini to be a more reliable tool in clini -\ncal decision-making while also suggesting the need for \nfurther refinement and validation before its widespread \nadoption in clinical practice. Future research should \nfocus on further improving the reliability and accuracy of \nLLMs while addressing ethical and legal challenges.\nArticle summary\nWhy is this topic important?\n  • The evaluation of large language models (LLMs) like \nGemini, Copilot, and ChatGPT-4.0 is crucial as they \nhave the potential to significantly influence clinical \ndecision-making in healthcare.\nWhat does this study attempt to show?\n  • This study attempts to assess the alignment of \nresponses from three prominent LLMs with expert \nconsensus recommendations for imaging in acute \nflank pain scenarios.\nWhat are the key findings?\n  • Gemini demonstrated superior alignment with \nexpert consensus, matching the majority consensus \nin 65.5% of cases, compared to 41.4% for both \nCopilot and ChatGPT-4.0.\n  • Gemini showed a higher agreement rate of 69.5% \nin scenarios rated as perfect or excellent by the \nconsensus, significantly outperforming the other two \nLLMs.\n  • The overall agreement rate of Gemini with any of \nthe nine reviewers was 82.7%, indicating its superior \ncapability in addressing complex medical inquiries.\nHow is patient care impacted?\n  • The use of Gemini can potentially enhance clinical \ndecision-making by providing more reliable and \nexpert-aligned recommendations.\n  • Integration of advanced LLMs like Gemini in \nemergency departments can improve diagnostic \naccuracy and patient management.\nSupplementary Information\nThe online version contains supplementary material available at  h t t p  s : /  / d o i  . o  r \ng /  1 0 .  1 1 8 6  / s  1 2 2 4 5 - 0 2 5 - 0 0 8 9 5 - 3.\nSupplementary Material 1\nSupplementary Material 2\nAuthor contributions\nYavuz Yigit: Conceptualization, Methodology, Investigation, Supervision, Data \nCuration, Writing - Original Draft, Writing - Review & Editing Asım Enes Ozbek: \nConceptualization, Methodology, Supervision, Writing - Review & Editing, \nProject administration, Betul Dogru: Data Curation, Investigation, Serkan \nGunay: Formal analysis, Visualization Project administration, Baha Alkahlout: \nMethodology, Resources, Supervision.\nFunding\nNot applicable.\nData availability\nThe original data set will be availed by the corresponding author on \nreasonable request.\nDeclarations\nEthics approval and consent to participate\nThis study did not involve human subjects, patient data, or interactions with \nhealthcare providers. All analyses were conducted using publicly available \nconsensus recommendations and clinical guidelines. As no patient-\nidentifiable information was used, and no experimental interventions were \nperformed, formal ethics committee approval was not required for this study.\nbetag.\nCompeting interests\nThe authors declare no competing interests.\nEthics declaration\nNot applicable.\nReceived: 23 December 2024 / Accepted: 27 April 2025\nReferences\n1. Günay S, Yiğit Y, Halhalli HC, Tulgar S, Alkahlout BH, Azad AM. AI in patient \neducation: assessing the impact of ChatGPT-4 on conveying comprehensive \ninformation about chest pain. Am J Emerg Med. 2024;77:220–1.\n2. Lekadir K, Quaglio G, Tselioudis Garmendia A, Gallin C. Artificial intelligence \nin healthcare: applications, risks, and ethical and societal impacts. European \nParliament; 2022.\n3. Franc JM, Cheng L, Hart A, Hata R, Hertelendy A. Repeatability, reproducibility, \nand diagnostic accuracy of a commercial large Language model (ChatGPT) \nto perform emergency department triage using the Canadian triage and \nacuity scale. CJEM. 2024;26(1):40–6.\n4. Fwu CW, Eggers PW, Kimmel PL, Kusek JW, Kirkali Z. Emergency department \nvisits, use of imaging, and drugs for urolithiasis have increased in the united \nStates. Kidney Int. 2013;83(3):479–86.\n5. Hyams ES, Korley FK, Pham JC, Matlaga BR. Trends in imaging use during the \nemergency department evaluation of flank pain. J Urol. 2011;186(6):2270–4.\n6. EAU Guidelines. Edn. presented at the EAU Annual Congress Paris 2024.\n7. Moore CL, Carpenter CR, Heilbrun ME, Klauer K, Krambeck A, Moreno C, et \nal. Imaging in suspected renal colic: systematic review of the literature and \nmultispecialty consensus. Ann Emerg Med. 2019;74(3):391–9.\nPage 9 of 9\nYigit et al. International Journal of Emergency Medicine           (2025) 18:123 \n8. Ganesan C, Stedman MR, Liu S, Conti SL, Chertow GM, Leppert JT, et al. \nNational imaging trends for suspected urinary stone disease in the emer-\ngency department. JAMA Intern Med. 2022;182(12):1323–5.\n9. Wang X, Ye H, Zhang S, Yang M, Wang X. Evaluation of the performance of \nthree large Language models in clinical decision support: A comparative \nstudy based on actual cases. J Med Syst. 2025;49(1):23.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 \n7  / s  1 0 9 1 6 - 0 2 5 - 0 2 1 5 2 - 9.\n10. Carlà MM, Gambini G, Baldascino A, Boselli F, Giannuzzi F, Margollicci F, \nRizzo S. Large Language models as assistance for glaucoma surgical cases: a \nChatGPT vs. Google gemini comparison. Graefes Arch Clin Exp Ophthalmol. \n2024;262(9):2945–59.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  0 0 4 1 7 - 0 2 4 - 0 6 4 7 0 - 5.\n11. Gomez-Cabello CA, Borna S, Pressman SM, Haider SA, Forte AJ. Large \nLanguage models for intraoperative decision support in plastic surgery: A \ncomparison between ChatGPT-4 and gemini. Med (Kaunas). 2024;60(6):957.  h \nt t p  s : /  / d o i  . o  r g /  1 0 .  3 3 9 0  / m  e d i c i n a 6 0 0 6 0 9 5 7.\n12. Goglia M, Cicolani A, Carrano FM, Petrucciani N, D’Angelo F, Pace M, Chiarini \nL, Silecchia G, Aurello P . Using large Language models in the diagnosis of \nacute cholecystitis: assessing accuracy and guidelines compliance. Am Surg. \n2025;31348251323719.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 7 7  / 0  0 0 3 1 3 4 8 2 5 1 3 2 3 7 1 9.\n13. Ahun E, Demir A, Yiğit Y, Tulgar YK, Doğan M, Thomas DT, Tulgar S. Percep-\ntions and concerns of emergency medicine practitioners about artificial \nintelligence in emergency triage management during the pandemic: a \nNational survey-based study. Front Public Health. 2023;11:1285390.\n14. Bottomley D, Thaldar D. Liability for harm caused by AI in healthcare: an \noverview of the core legal concepts. Front Pharmacol. 2023;14:1297353.\n15. Markus AF, Kors JA, Rijnbeek PR. The role of explainability in creating trust-\nworthy artificial intelligence for health care: A comprehensive survey of the \nterminology, design choices, and evaluation strategies. J Biomed Inform. \n2021;113:103655.\n16. Murdoch B. Privacy and artificial intelligence: challenges for protecting health \ninformation in a new era. BMC Med Ethics. 2021;22(1):122.\n17. Özkan M, Yıldırım E. Data protection in healthcare: compliance with the \nlaw on the protection of personal data in Turkey. Health Inform Sci Syst. \n2020;8(1):1–10.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 0 7  / s  1 3 7 5 5 - 0 2 0 - 0 0 1 1 7 - 5.\n18. Rights (OCR) O for C. Summary of the HIPAA Security Rule [Internet]. 2009 \n[cited 2024 Jul 3]. Available from:  h t t p s :   /  / w w  w .  h h  s  . g  o  v / h  i p   a a /   f o r  - p r  o f  e s s  i o n   a l \ns /  s e  c u   r i t y  /  l a  w s  - r e g u l a  t i o n s /  i n d e x . h t m l\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.",
  "topic": "Matching (statistics)",
  "concepts": [
    {
      "name": "Matching (statistics)",
      "score": 0.509695291519165
    },
    {
      "name": "Medicine",
      "score": 0.4366220533847809
    },
    {
      "name": "Medical diagnosis",
      "score": 0.41989535093307495
    },
    {
      "name": "Pathology",
      "score": 0.18680506944656372
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I166337079",
      "name": "Queen Mary University of London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I49828101",
      "name": "Hamad Medical Corporation",
      "country": "QA"
    },
    {
      "id": "https://openalex.org/I51826884",
      "name": "Kocaeli Üniversitesi",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I2799360972",
      "name": "Hitit Üniversitesi",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I60342839",
      "name": "Qatar University",
      "country": "QA"
    }
  ],
  "cited_by": 1
}