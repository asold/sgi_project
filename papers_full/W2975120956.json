{
  "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
  "url": "https://openalex.org/W2975120956",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287362874",
      "name": "Zhong, Peixiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111595691",
      "name": "Wang DI",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232502167",
      "name": "Miao Chunyan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2799176105",
    "https://openalex.org/W2895615590",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2740550900",
    "https://openalex.org/W2955807891",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963544536",
    "https://openalex.org/W2586847566",
    "https://openalex.org/W2890961898",
    "https://openalex.org/W2891501508",
    "https://openalex.org/W2931198394",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2905807898",
    "https://openalex.org/W2250645263",
    "https://openalex.org/W2963475460",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2909285558",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962806234",
    "https://openalex.org/W2943963359",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2963520511",
    "https://openalex.org/W1977222384",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W2798357113",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2964099336",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2964022793",
    "https://openalex.org/W2084046180",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2964077278",
    "https://openalex.org/W2805662932",
    "https://openalex.org/W2963695965",
    "https://openalex.org/W2741447225",
    "https://openalex.org/W2154359981",
    "https://openalex.org/W2962796276",
    "https://openalex.org/W2798456655",
    "https://openalex.org/W2949541494",
    "https://openalex.org/W2964301648",
    "https://openalex.org/W2739716023",
    "https://openalex.org/W61273459",
    "https://openalex.org/W2963829073",
    "https://openalex.org/W2808336242",
    "https://openalex.org/W2561658355",
    "https://openalex.org/W2102953093",
    "https://openalex.org/W1948749636",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2806966100",
    "https://openalex.org/W2043181832",
    "https://openalex.org/W2963789888",
    "https://openalex.org/W2158943324",
    "https://openalex.org/W2964300796",
    "https://openalex.org/W2588225831",
    "https://openalex.org/W2962830617",
    "https://openalex.org/W2891103209",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2804780446",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2891359673",
    "https://openalex.org/W2146334809"
  ],
  "abstract": "Messages in human conversations inherently convey emotions. The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks. However, enabling machines to analyze emotions in conversations is challenging, partly because humans often rely on the context and commonsense knowledge to express emotions. In this paper, we address these challenges by proposing a Knowledge-Enriched Transformer (KET), where contextual utterances are interpreted using hierarchical self-attention and external commonsense knowledge is dynamically leveraged using a context-aware affective graph attention mechanism. Experiments on multiple textual conversation datasets demonstrate that both context and commonsense knowledge are consistently beneficial to the emotion detection performance. In addition, the experimental results show that our KET model outperforms the state-of-the-art models on most of the tested datasets in F1 score.",
  "full_text": "Knowledge-Enriched Transformer for Emotion Detection in Textual\nConversations\nPeixiang Zhong1,2, Di Wang1, Chunyan Miao1,2,3\n1Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly\n2Alibaba-NTU Singapore Joint Research Institute\n3School of Computer Science and Engineering\nNanyang Technological University, Singapore\npeixiang001@e.ntu.edu.sg, {wangdi, ascymiao}@ntu.edu.sg\nAbstract\nMessages in human conversations inherently\nconvey emotions. The task of detecting emo-\ntions in textual conversations leads to a wide\nrange of applications such as opinion mining\nin social networks. However, enabling ma-\nchines to analyze emotions in conversations is\nchallenging, partly because humans often rely\non the context and commonsense knowledge\nto express emotions. In this paper, we address\nthese challenges by proposing a Knowledge-\nEnriched Transformer (KET), where contex-\ntual utterances are interpreted using hierarchi-\ncal self-attention and external commonsense\nknowledge is dynamically leveraged using a\ncontext-aware affective graph attention mech-\nanism. Experiments on multiple textual con-\nversation datasets demonstrate that both con-\ntext and commonsense knowledge are consis-\ntently beneﬁcial to the emotion detection per-\nformance. In addition, the experimental results\nshow that our KET model outperforms the\nstate-of-the-art models on most of the tested\ndatasets in F1 score.\n1 Introduction\nEmotions are “generated states in humans that re-\nﬂect evaluative judgments of the environment, the\nself and other social agents” (Hudlicka, 2011).\nMessages in human communications inherently\nconvey emotions. With the prevalence of social\nmedia platforms such as Facebook Messenger, as\nwell as conversational agents such as Amazon\nAlexa, there is an emerging need for machines to\nunderstand human emotions in natural conversa-\ntions. This work addresses the task of detecting\nemotions (e.g., happy, sad, angry, etc.) in textual\nconversations, where the emotion of an utterance\nis detected in the conversational context. Being\nable to effectively detect emotions in conversa-\ntions leads to a wide range of applications rang-\ning from opinion mining in social media platforms\nNo emotion\n socialize \nWhat do you plan to do for your \nbirthday ?\nI want to have a picnic with my friends, \nMum.\nHow about a party at home? That way \nwe can get together and celebrate it.\nOK, Mum. I'll invite my friends  home.\n party  movie \nNo emotion\nHappiness\nHappiness\nContext\nResponse\nFigure 1: An example conversation with annotated la-\nbels from the DailyDialog dataset (Li et al., 2017). By\nreferring to the context, “it” in the third utterance is\nlinked to “birthday” in the ﬁrst utterance. By lever-\naging an external knowledge base, the meaning of\n“friends” in the forth utterance is enriched by associ-\nated knowledge entities, namely “socialize”, “party”,\nand “movie”. Thus, the implicit “happiness” emotion\nin the fourth utterance can be inferred more easily via\nits enriched meaning.\n(Chatterjee et al., 2019) to building emotion-aware\nconversational agents (Zhou et al., 2018a).\nHowever, enabling machines to analyze emo-\ntions in human conversations is challenging, partly\nbecause humans often rely on the context and\ncommonsense knowledge to express emotions,\nwhich is difﬁcult to be captured by machines. Fig-\nure 1 shows an example conversation demonstrat-\ning the importance of context and commonsense\nknowledge in understanding conversations and de-\ntecting implicit emotions.\nThere are several recent studies that model con-\ntextual information to detect emotions in conver-\nsations. Poria et al. (2017) and Majumder et al.\n(2019) leveraged recurrent neural networks (RNN)\nto model the contextual utterances in sequence,\nwhere each utterance is represented by a feature\nvector extracted by convolutional neural networks\n(CNN) at an earlier stage. Similarly, Hazarika\net al. (2018a,b) proposed to use extracted CNN\narXiv:1909.10681v2  [cs.CL]  1 Oct 2019\nfeatures in memory networks to model contextual\nutterances. However, these methods require sepa-\nrate feature extraction and tuning, which may not\nbe ideal for real-time applications. In addition, to\nthe best of our knowledge, no attempts have been\nmade in the literature to incorporate commonsense\nknowledge from external knowledge bases to de-\ntect emotions in textual conversations. Common-\nsense knowledge is fundamental to understand-\ning conversations and generating appropriate re-\nsponses (Zhou et al., 2018b).\nTo this end, we propose a Knowledge-Enriched\nTransformer (KET) to effectively incorporate con-\ntextual information and external knowledge bases\nto address the aforementioned challenges. The\nTransformer (Vaswani et al., 2017) has been\nshown to be a powerful representation learning\nmodel in many NLP tasks such as machine trans-\nlation (Vaswani et al., 2017) and language under-\nstanding (Devlin et al., 2018). The self-attention\n(Cheng et al., 2016) and cross-attention (Bah-\ndanau et al., 2014) modules in the Transformer\ncapture the intra-sentence and inter-sentence cor-\nrelations, respectively. The shorter path of in-\nformation ﬂow in these two modules compared\nto gated RNNs and CNNs allows KET to model\ncontextual information more efﬁciently. In ad-\ndition, we propose a hierarchical self-attention\nmechanism allowing KET to model the hierarchi-\ncal structure of conversations. Our model sepa-\nrates context and response into the encoder and de-\ncoder, respectively, which is different from other\nTransformer-based models, e.g., BERT (Devlin\net al., 2018), which directly concatenate context\nand response, and then train language models us-\ning only the encoder part.\nMoreover, to exploit commonsense knowledge,\nwe leverage external knowledge bases to facili-\ntate the understanding of each word in the utter-\nances by referring to related knowledge entities.\nThe referring process is dynamic and balances\nbetween relatedness and affectiveness of the re-\ntrieved knowledge entities using a context-aware\naffective graph attention mechanism.\nIn summary, our contributions are as follows:\n•For the ﬁrst time, we apply the Transformer\nto analyze conversations and detect emotions.\nOur hierarchical self-attention and cross-\nattention modules allow our model to exploit\ncontextual information more efﬁciently than\nexisting gated RNNs and CNNs.\n•We derive dynamic, context-aware, and\nemotion-related commonsense knowledge\nfrom external knowledge bases and emotion\nlexicons to facilitate the emotion detection in\nconversations.\n•We conduct extensive experiments demon-\nstrating that both contextual information and\ncommonsense knowledge are beneﬁcial to\nthe emotion detection performance. In addi-\ntion, our proposed KET model outperforms\nthe state-of-the-art models on most of the\ntested datasets across different domains.\n2 Related Work\nEmotion Detection in Conversations: Early\nstudies on emotion detection in conversations fo-\ncus on call center dialogs using lexicon-based\nmethods and audio features (Lee and Narayanan,\n2005; Devillers and Vidrascu, 2006). Devillers\net al. (2002) annotated and detected emotions in\ncall center dialogs using unigram topic modelling.\nIn recent years, there is an emerging research trend\non emotion detection in conversational videos and\nmulti-turn Tweets using deep learning methods\n(Hazarika et al., 2018b,a; Zahiri and Choi, 2018;\nChatterjee et al., 2019; Zhong and Miao, 2019; Po-\nria et al., 2019). Poria et al. (2017) proposed a long\nshort-term memory network (LSTM) (Hochreiter\nand Schmidhuber, 1997) based model to capture\ncontextual information for sentiment analysis in\nuser-generated videos. Majumder et al. (2019)\nproposed the DialogueRNN model that uses three\ngated recurrent units (GRU) (Cho et al., 2014) to\nmodel the speaker, the context from the preced-\ning utterances, and the emotions of the preceding\nutterances, respectively. They achieved the state-\nof-the-art performance on several conversational\nvideo datasets.\nKnowledge Base in Conversations: Recently\nthere is a growing number of studies on incorpo-\nrating knowledge base in generative conversation\nsystems, such as open-domain dialogue systems\n(Han et al., 2015; Asghar et al., 2018; Ghazvinine-\njad et al., 2018; Young et al., 2018; Parthasarathi\nand Pineau, 2018; Liu et al., 2018; Moghe et al.,\n2018; Dinan et al., 2019; Zhong et al., 2019),\ntask-oriented dialogue systems (Madotto et al.,\n2018; Wu et al., 2019; He et al., 2019) and ques-\ntion answering systems (Kiddon et al., 2016; Hao\net al., 2017; Sun et al., 2018; Mihaylov and Frank,\n2018). Zhou et al. (2018b) adopted structured\nknowledge graphs to enrich the interpretation of\ninput sentences and help generate knowledge-\naware responses using graph attentions. The graph\nattention in the knowledge interpreter (Zhou et al.,\n2018b) is static and only related to the recognized\nentity of interest. By contrast, our graph attention\nmechanism is dynamic and selects context-aware\nknowledge entities that balances between related-\nness and affectiveness.\nEmotion Detection in Text: There is a trend\nmoving from traditional machine learning meth-\nods (Pang et al., 2002; Wang and Manning, 2012;\nSeyeditabari et al., 2018) to deep learning methods\n(Abdul-Mageed and Ungar, 2017; Zhang et al.,\n2018b) for emotion detection in text. Khanpour\nand Caragea (2018) investigated the emotion de-\ntection from health-related posts in online health\ncommunities using both deep learning features\nand lexicon-based features.\nIncorporating Knowledge in Sentiment Anal-\nysis: Traditional lexicon-based methods detect\nemotions or sentiments from a piece of text based\non the emotions or sentiments of words or phrases\nthat compose it (Hu et al., 2009; Taboada et al.,\n2011; Bandhakavi et al., 2017). Few studies in-\nvestigated the usage of knowledge bases in deep\nlearning methods. Kumar et al. (2018) proposed to\nuse knowledge from WordNet (Fellbaum, 2012) to\nenrich the text representations produced by LSTM\nand obtained improved performance.\nTransformer: The Transformer has been applied\nto many NLP tasks due to its rich representa-\ntion and fast computation, e.g., document machine\ntranslation (Zhang et al., 2018a), response match-\ning in dialogue system (Zhou et al., 2018c), lan-\nguage modelling (Dai et al., 2019) and understand-\ning (Radford et al., 2018). A very recent work\n(Rik Koncel-Kedziorski and Hajishirzi, 2019) ex-\ntends the Transformer to graph inputs and propose\na model for graph-to-text generation.\n3 Our Proposed KET Model\nIn this section we present the task deﬁnition and\nour proposed KET model.\n3.1 Task Deﬁnition\nLet {Xi\nj,Y i\nj },i = 1,...N,j = 1,...Ni be a collec-\ntion of {utterance, label}pairs in a given dialogue\ndataset, where N denotes the number of conversa-\ntions and Ni denotes the number of utterances in\nthe ith conversation. The objective of the task is to\nmaximize the following function:\nΦ =\nN∏\ni=1\nNi∏\nj=1\np(Yi\nj |Xi\nj,Xi\nj−1,...,X i\n1; θ), (1)\nwhere Xi\nj−1,...,X i\n1 denote contextual utterances\nand θ denotes the model parameters we want to\noptimize.\nWe limit the number of contextual utterances to\nM. Discarding early contextual utterances may\ncause information loss, but this loss is negligible\nbecause they only contribute the least amount of\ninformation (Su et al., 2018). This phenomenon\ncan be further observed in our model analysis re-\ngarding context length (see Section 5.2). Similar\nto (Poria et al., 2017), we clip and pad each utter-\nance Xi\nj to a ﬁxed mnumber of tokens. The over-\nall architecture of our KET model is illustrated in\nFigure 2.\n3.2 Knowledge Retrieval\nWe use a commonsense knowledge base Con-\nceptNet (Speer et al., 2017) and an emotion lex-\nicon NRC V AD (Mohammad, 2018a) as knowl-\nedge sources in our model.\nConceptNet is a large-scale multilingual seman-\ntic graph that describes general human knowledge\nin natural language. The nodes in ConceptNet\nare concepts and the edges are relations. Each\n⟨concept1, relation, concept2⟩triplet is an asser-\ntion. Each assertion is associated with a conﬁ-\ndence score. An example assertion is ⟨friends,\nCausesDesire, socialize⟩with conﬁdence score of\n3.46. Usually assertion conﬁdence scores are in\nthe [1,10] interval. Currently, for English, Con-\nceptNet comprises 5.9M assertions, 3.1M con-\ncepts and 38 relations.\nNRC V AD is a list of English words and\ntheir V AD scores, i.e., valence (negative-\npositive), arousal (calm-excited), and dominance\n(submissive-dominant) scores in the [0,1]\ninterval. The V AD measure of emotion is\nculture-independent and widely adopted in Psy-\nchology (Mehrabian, 1996). Currently NRC V AD\ncomprises around 20K words.\nIn general, for each non-stopword token t in\nXi\nj, we retrieve a connected knowledge graphg(t)\ncomprising its immediate neighbors from Con-\nceptNet. For each g(t), we remove concepts that\nare stopwords or not in our vocabulary. We fur-\nther remove concepts with conﬁdence scores less\nEmbedding Layer \n(Section 3.3)\nKB\nWord \nEmbedding\nConcept \nEmbedding\nKB\nWord \nEmbedding\nConcept \nEmbedding\nKB\nWord \nEmbedding\nConcept \nEmbedding\n. . .\nDynamic Context-Aware Affective Graph Attention\nWord \nEmbedding\nConcept \nRepresentation\nWord \nEmbedding\nConcept \nRepresentation\nWord \nEmbedding\nConcept \nRepresentation\nMulti-Head Self-Attention & FF Multi-Head Self-Attention & FF Multi-Head Self-Attention\nContext Response\n. . .\n. . .\nMulti-Head Self-Attention & FF\nMulti-Head Cross-Attention & FF\nSoftmax\nMax Pooling & Linear\nConcatenation\nConcept-Enriched Embedding\nHierarchical Self-Attention \n(Section 3.5)\nKnowledge Retrieval \n(Section 3.2)\nDynamic Context-Aware Affective \nGraph Attention (Section 3.4)\nContext-Response Cross-Attention \n(Section 3.6)\nFigure 2: Overall architecture of our proposed KET model. The positional encoding, residual connection, and\nlayer normalization are omitted in the illustration for brevity.\nthan 1 to reduce annotation noises. For each con-\ncept, we retrieve its V AD values from NRCV AD.\nThe ﬁnal knowledge representation for each to-\nken t is a list of tuples: (c1,s1,VAD(c1)),\n(c2,s2,VAD(c2)), ..., (c|g(t)|,s|g(t)|,VAD(c|g(t)|)),\nwhere ck ∈g(t) denotes the kth connected con-\ncept, sk denotes the associated conﬁdence score,\nand VAD(ck) denotes the V AD values ofck. The\ntreatment for tokens that are not associated with\nany concept and concepts that are not included in\nNRC V AD are discussed in Section 3.4. We leave\nthe treatment on relations as future work.\n3.3 Embedding Layer\nWe use a word embedding layer to convert each\ntoken tin Xi into a vector representation t ∈Rd,\nwhere ddenotes the size of word embedding. To\nencode positional information, the position encod-\ning (Vaswani et al., 2017) is added as follows:\nt = Embed(t) + Pos(t). (2)\nSimilarly, we use a concept embedding layer to\nconvert each conceptcinto a vector representation\nc ∈Rd but without position encoding.\n3.4 Dynamic Context-Aware Affective Graph\nAttention\nTo enrich word embedding with concept represen-\ntations, we propose a dynamic context-aware af-\nfective graph attention mechanism to compute the\nconcept representation for each token. Speciﬁ-\ncally, the concept representation c(t) ∈ Rd for\ntoken tis computed as\nc(t) =\n|g(t)|∑\nk=1\nαk ∗ck, (3)\nwhere ck ∈ Rd denotes the concept embedding\nof ck and αk denotes its attention weight. If\n|g(t)|= 0 , we set c(t) to the average of all con-\ncept embeddings. The attention αk in Equation 3\nis computed as\nαk = softmax(wk), (4)\nwhere wk denotes the weight of ck.\nThe derivation of wk is crucial because it reg-\nulates the contribution of ck towards enriching t.\nA standard graph attention mechanism (Velikovi\net al., 2018) computes wk by feeding t and ck into\na single-layer feedforward neural network. How-\never, not all related concepts are equal in detect-\ning emotions given the conversational context. In\nour model, we make the assumption that important\nconcepts are those that relate to the conversational\ncontext and have strong emotion intensity. To this\nend, we propose a context-aware affective graph\nattention mechanism by incorporating two factors\nwhen computing wk, namely relatedness and af-\nfectiveness.\nRelatedness: Relatedness measures the strength\nof the relation between ck and the conversational\ncontext. The relatedness factor in wk is computed\nas\nrelk = min-max(sk) ∗abs(cos(CR(Xi),ck)),\n(5)\nwhere sk is the conﬁdence score introduced in\nSection 3.2, min-max denotes min-max scaling for\neach token t, abs denotes the absolute function,\ncos denotes the cosine similarity function, and\nCR(Xi) ∈ Rd denotes the context representa-\ntion of the ith conversation Xi. Here we compute\nCR(Xi) as the average of all sentence represen-\ntations in Xi as follows:\nCR(Xi) = avg(SR(Xi\nj−M ),..., SR(Xi\nj)), (6)\nwhere SR(Xi\nj) ∈ Rd denotes the sentence rep-\nresentation of Xi\nj. We compute SR(Xi\nj) via hi-\nerarchical pooling (Shen et al., 2018) where n-\ngram (n≤3) representations in Xi\nj are ﬁrst com-\nputed by max-pooling and then all n-gram repre-\nsentations are averaged. The hierarchical pooling\nmechanism preserves word order information to\ncertain degree and has demonstrated superior per-\nformance than average pooling or max-pooling on\nsentiment analysis tasks (Shen et al., 2018).\nAffectiveness: Affectiveness measures the emo-\ntion intensity of ck. The affectiveness factor in wk\nis computed as\naffk = min-max(||[V(ck)−1/2,A(ck)/2]||2), (7)\nwhere ||.||k denotes lk norm, V(ck) ∈[0,1] and\nA(ck) ∈[0,1] denote the valence and arousal val-\nues of VAD(ck), respectively. Intuitively,affk con-\nsiders the deviations of valence from neutral and\nthe level of arousal from calm. There is no es-\ntablished method in the literature to compute the\nemotion intensity based on V AD values, but em-\npirically we found that our method correlates bet-\nter with an emotion intensity lexicon comprising\n6K English words (Mohammad, 2018b) than other\nmethods such as taking dominance into consider-\nation or taking l1 norm. For concept ck not in\nNRC V AD, we setaffk to the mid value of 0.5.\nCombining both relk and affk, we deﬁne the\nweight wk as follows:\nwk = λk ∗relk + (1 −λk) ∗affk, (8)\nwhere λk is a model parameter balancing the im-\npacts of relatedness and affectiveness on comput-\ning concept representations. Parameter λk can be\nﬁxed or learned during training. The analysis of\nλk is discussed in Section 5.2.\nFinally, the concept-enriched word representa-\ntion ˆt can be obtained via a linear transformation:\nˆt = W[t; c(t)], (9)\nwhere [; ] denotes concatenation and W ∈Rd×2d\ndenotes a model parameter. All mtokens in each\nXi\nj then form a concept-enriched utterance em-\nbedding ˆXi\nj ∈Rm×d.\n3.5 Hierarchical Self-Attention\nWe propose a hierarchical self-attention mecha-\nnism to exploit the structural representation of\nconversations and learn a vector representation\nfor the contextual utterances Xi\nj−1,...,X i\nj−M .\nSpeciﬁcally, the hierarchical self-attention follows\ntwo steps: 1) each utterance representation is\ncomputed using an utterance-level self-attention\nlayer, and 2) a context representation is computed\nfrom M learned utterance representations using a\ncontext-level self-attention layer.\nAt step 1, for each utterance Xi\nn, n=j −1, ...,\nj−M, its representation ˆX\n′i\nn ∈Rm×d is learned\nas follows:\nˆX\n′i\nn = FF(L\n′\n(MH(L( ˆXi\nn),L( ˆXi\nn),L( ˆXi\nn)))),\n(10)\nwhere L( ˆXi\nn) ∈Rm×h×ds is linearly transformed\nfrom ˆXi\nn to form hheads (ds = d/h), L\n′\nlinearly\ntransforms from hheads back to 1 head, and\nMH(Q,K,V ) = softmax(QKT\n√ds\n)V, (11)\nFF(x) = max(0,xW1 + b1)W2 + b2, (12)\nwhere Q, K, and V denote sets of queries, keys\nand values, respectively, W1 ∈ Rd×p,b1 ∈\nRp,W2 ∈Rp×d and b2 ∈Rd denote model pa-\nrameters, and p denotes the hidden size of the\npoint-wise feedforward layer (FF) (Vaswani et al.,\n2017). The multi-head self-attention layer (MH)\nenables our model to jointly attend to information\nfrom different representation subspaces (Vaswani\net al., 2017). The scaling factor 1√ds\nis added to\nensure the dot product of two vectors do not get\noverly large. Similar to (Vaswani et al., 2017),\nboth MH and FF layers are followed by resid-\nual connection and layer normalization, which are\nomitted in Equation 10 for brevity.\nDataset Domain #Conv. (Train/Val/Test) #Utter. (Train/Val/Test) #Classes Evaluation\nEC Tweet 30160/2755/5509 90480/8265/16527 4 Micro-F1\nDailyDialog Daily Communication 11118/1000/1000 87170/8069/7740 7 Micro-F1\nMELD TV Show Scripts 1038/114/280 9989/1109/2610 7 Weighted-F1\nEmoryNLP TV Show Scripts 659/89/79 7551/954/984 7 Weighted-F1\nIEMOCAP Emotional Dialogues 100/20/31 4810/1000/1523 6 Weighted-F1\nTable 1: Dataset descriptions.\nAt step 2, to effectively combine all utter-\nance representations in the context, the context-\nlevel self-attention layer is proposed to hierarchi-\ncally learn the context-level representation Ci ∈\nRM×m×d as follows:\nCi = FF(L\n′\n(MH(L( ˆXi),L( ˆXi),L( ˆXi)))), (13)\nwhere ˆXi denotes [ ˆX\n′i\nj−M ; ...; ˆX\n′i\nj−1], which is the\nconcatenation of all learned utterance representa-\ntions in the context.\n3.6 Context-Response Cross-Attention\nFinally, a context-aware concept-enriched re-\nsponse representation Ri ∈Rm×d for conversa-\ntion Xi is learned by cross-attention (Bahdanau\net al., 2014), which selectively attends to the\nconcept-enriched context representation as fol-\nlows:\nRi = FF(L\n′\n(MH(L( ˆX\n′i\nj ),L(Ci),L(Ci)))), (14)\nwhere the response utterance representation ˆX\n′i\nj ∈\nRm×d is obtained via the MH layer:\nˆX\n′i\nj = L\n′\n(MH(L( ˆXi\nj),L( ˆXi\nj),L( ˆXi\nj))), (15)\nThe resulted representation Ri ∈Rm×d is then\nfed into a max-pooling layer to learn discrimina-\ntive features among the positions in the response\nand derive the ﬁnal representation O ∈Rd:\nO = max pool(Ri). (16)\nThe output probability pis then computed as\np= softmax(OW3 + b3), (17)\nwhere W3 ∈ Rd×q and b3 ∈ Rq denote model\nparameters, and q denotes the number of classes.\nThe entire KET model is optimized in an end-to-\nend manner as deﬁned in Equation 1. Our model\nis available at here1.\n1https://github.com/zhongpeixiang/KET\n4 Experimental Settings\nIn this section we present the datasets, evaluation\nmetrics, baselines, our model variants, and other\nexperimental settings.\n4.1 Datasets and Evaluations\nWe evaluate our model on the following ﬁve emo-\ntion detection datasets of various sizes and do-\nmains. The statistics are reported in Table 1.\nEC (Chatterjee et al., 2019): Three-turn Tweets.\nThe emotion labels include happiness, sadness,\nanger and other.\nDailyDialog (Li et al., 2017): Human written\ndaily communications. The emotion labels in-\nclude neutral and Ekman’s six basic emotions (Ek-\nman, 1992), namely happiness, surprise, sadness,\nanger, disgust and fear.\nMELD (Poria et al., 2018): TV show scripts col-\nlected from Friends. The emotion labels are the\nsame as the ones used in DailyDialog.\nEmoryNLP (Zahiri and Choi, 2018): TV show\nscripts collected from Friends as well. How-\never, its size and annotations are different from\nMELD. The emotion labels include neutral, sad,\nmad, scared, powerful, peaceful, and joyful.\nIEMOCAP (Busso et al., 2008): Emotional dia-\nlogues. The emotion labels include neutral, happi-\nness, sadness, anger, frustrated, and excited.\nIn terms of the evaluation metric, for EC and\nDailyDialog, we follow (Chatterjee et al., 2019) to\nuse the micro-averaged F1 excluding the majority\nclass (neutral), due to their extremely unbalanced\nlabels (the percentage of the majority class in the\ntest set is over 80%). For the rest relatively bal-\nanced datasets, we follow (Majumder et al., 2019)\nto use the weighted macro-F1.\n4.2 Baselines and Model Variants\nFor a comprehensive performance evaluation, we\ncompare our model with the following baselines:\ncLSTM: A contextual LSTM model. An\nutterance-level bidirectional LSTM is used to en-\ncode each utterance. A context-level unidirec-\ntional LSTM is used to encode the context.\nModel EC DailyDialog MELD EmoryNLP IEMOCAP\ncLSTM 0.6913 0.4990 0.4972 0.2601 0.3484\nCNN (Kim, 2014) 0.7056 0.4934 0.5586 0.3259 0.5218\nCNN+cLSTM (Poria et al., 2017) 0.7262 0.5024 0.5687 0.3289 0.5587\nBERT BASE (Devlin et al., 2018) 0.6946 0.5312 0.5621 0.3315 0.6119\nDialogueRNN (Majumder et al., 2019) 0.7405 0.5065 0.5627 0.3170 0.6121\nKET SingleSelfAttn (ours) 0.7285 0.5192 0.5624 0.3251 0.5810\nKET StdAttn (ours) 0.7413 0.5254 0.5682 0.3353 0.5861\nKET (ours) 0.7348 0.5337 0.5818 0.3439 0.5956\nTable 2: Performance comparisons on the ﬁve test sets. Best values are highlighted in bold.\nDataset M m d p h\nEC 2 30 200 100 4\nDailyDialog 6 30 300 400 4\nMELD 6 30 200 100 4\nEmoryNLP 6 30 100 200 4\nIEMOCAP 6 30 300 400 4\nTable 3: Hyper-parameter settings for KET. M: con-\ntext length. m: number of tokens per utterance. d:\nword embedding size. p: hidden size in FF layer. h:\nnumber of heads.\nCNN (Kim, 2014): A single-layer CNN with\nstrong empirical performance. This model is\ntrained on the utterance-level without context.\nCNN+cLSTM (Poria et al., 2017): An CNN is\nused to extract utterance features. An cLSTM is\nthen applied to learn context representations.\nBERT BASE (Devlin et al., 2018): Base version\nof the state-of-the-art model for sentiment classiﬁ-\ncation. We treat each utterance with its context as\na single document. We limit the document length\nto the last 100 tokens to allow larger batch size.\nWe do not experiment with the large version of\nBERT due to memory constraint of our GPU.\nDialogueRNN (Majumder et al., 2019): The state-\nof-the-art model for emotion detection in textual\nconversations. It models both context and speak-\ners information. The CNN features used in Dia-\nlogueRNN are extracted from the carefully tuned\nCNN model. For datasets without speaker in-\nformation, i.e., EC and DailyDialog, we use two\nspeakers only. For MELD and EmoryNLP, which\nhave 260 and 255 speakers, respectively, we addi-\ntionally experimented with clipping the number of\nspeakers to the most frequent ones (6 main speak-\ners + an universal speaker representing all other\nspeakers) and reported the best results.\nKET SingleSelfAttn: We replace the hierarchi-\ncal self-attention by a single self-attention layer\nto learn context representations. Contextual utter-\nances are concatenated together prior to the single\nself-attention layer.\nKET StdAttn: We replace the dynamic context-\naware affective graph attention by the standard\ngraph attention (Velikovi et al., 2018).\n4.3 Other Experimental Settings\nWe preprocessed all datasets by lower-casing and\ntokenization using Spacy 2. We keep all tokens\nin the vocabulary 3. We use the released code\nfor BERT BASE and DialogueRNN. For each\ndataset, all models are ﬁne-tuned based on their\nperformance on the validation set.\nFor our model in all datasets, we use Adam opti-\nmization (Kingma and Ba, 2014) with a batch size\nof 64 and learning rate of 0.0001 throughout the\ntraining process. We use GloVe embedding (Pen-\nnington et al., 2014) for initialization in the word\nand concept embedding layers 4. For the class\nweights in cross-entropy loss for each dataset, we\nset them as the ratio of the class distribution in\nthe validation set to the class distribution in the\ntraining set. Thus, we can alleviate the problem of\nunbalanced dataset. The detailed hyper-parameter\nsettings for KET are presented in Table 3.\n5 Result Analysis\nIn this section we present model evaluation results,\nmodel analysis, and error analysis.\n5.1 Comparison with Baselines\nWe compare the performance of KET against that\nof the baseline models on the ﬁve afore-introduced\ndatasets. The results are reported in Table 2. Note\nthat our results for CNN, CNN+cLSTM and Di-\nalogueRNN on EC, MELD and IEMOCAP are\nslightly different from the reported results in (Ma-\njumder et al., 2019; Poria et al., 2019).\n2https://spacy.io/\n3We keep tokens with minimum frequency of 2 for Daily-\nDialog due to its large vocabulary size\n4We use GloVe embeddings from Magnitude Medium:\nhttps://github.com/plasticityai/magnitude\n0 1 2\n0.72\n0.74\n0.76\n0 5 10\n0.525\n0.550\n0 5 10\n0.500\n0.525\n0.550\n0 5 10\n0.34\n0.36\n0.38\n0 5 10\n0.50\n0.55\n0.0 0.5 1.0\nEC\n0.72\n0.74\n0.76\n0.0 0.5 1.0\nDailyDialog\n0.52\n0.54\n0.56\n0.0 0.5 1.0\nMELD\n0.52\n0.54\n0.56\n0.0 0.5 1.0\nEmoryNLP\n0.34\n0.36\n0.38\n0.0 0.5 1.0\nIEMOCAP\n0.52\n0.54\nFigure 3: Validation performance by KET. Top: different context length ( M). Bottom: different sizes of random\nfractions of ConceptNet.\ncLSTM performs reasonably well on short\nconversations (i.e., EC and DailyDialog), but\nthe worst on long conversations (i.e., MELD,\nEmoryNLP and IEMOCAP). One major reason is\nthat learning long dependencies using gated RNNs\nmay not be effective enough because the gradi-\nents are expected to propagate back through in-\nevitably a huge number of utterances and tokens\nin sequence, which easily leads to the vanishing\ngradient problem (Bengio et al., 1994). In con-\ntrast, when the utterance-level LSTM in cLSTM\nis replaced by features extracted by CNN, i.e.,\nthe CNN+cLSTM, the model performs signiﬁ-\ncantly better than cLSTM on long conversations,\nwhich further validates that modelling long con-\nversations using only RNN models may not be\nsufﬁcient. BERT BASE achieves very competi-\ntive performance on all datasets except EC due to\nits strong representational power via bi-directional\ncontext modelling using the Transformer. Note\nthat BERT BASE has considerably more param-\neters than other baselines and our model (110M\nfor BERT BASE versus 4M for our model), which\ncan be a disadvantage when deployed to devices\nwith limited computing power and memory. The\nstate-of-the-art DialogueRNN model performs the\nbest overall among all baselines. In particular,\nDialogueRNN performs better than our model on\nIEMOCAP, which may be attributed to its detailed\nspeaker information for modelling the emotion dy-\nnamics in each speaker as the conversation ﬂows.\nIt is encouraging to see that our KET model\noutperforms the baselines on most of the datasets\ntested. This ﬁnding indicates that our model is ro-\nbust across datasets with varying training sizes,\ncontext lengths and domains. Our KET vari-\nants KET SingleSelfAttn and KET StdAttn per-\nform comparably with the best baselines on all\ndatasets except IEMOCAP. However, both vari-\nants perform noticeably worse than KET on all\ndatasets except EC, validating the importance\nof our proposed hierarchical self-attention and\ndynamic context-aware affective graph attention\nmechanism. One observation worth mentioning\nis that these two variants perform on a par with\nthe KET model on EC. Possible explanations are\nthat 1) hierarchical self-attention may not be crit-\nical for modelling short conversations in EC, and\n2) the informal linguistic styles of Tweets in EC,\ne.g., misspelled words and slangs, hinder the con-\ntext representation learning in our graph attention\nmechanism.\n5.2 Model Analysis\nWe analyze the impact of different settings on the\nvalidation performance of KET. All results in this\nsection are averaged over 5 random seeds.\nAnalysis of context length: We vary the context\nlength M and plot model performance in Figure 3\n(top portion). Note that EC has only a maximum\nnumber of 2 contextual utterances. It is clear that\nincorporating context into KET improves perfor-\nmance on all datasets. However, adding more con-\ntext is contributing diminishing performance gain\nor even making negative impact in some datasets.\nThis phenomenon has been observed in a prior\nstudy (Su et al., 2018). One possible explanation\nis that incorporating long contextual information\nmay introduce additional noises, e.g., polysemes\nexpressing different meanings in different utter-\nances of the same context. More thorough investi-\ngation of this diminishing return phenomenon is a\nworthwhile direction in the future.\nAnalysis of the size of ConceptNet: We vary the\nsize of ConceptNet by randomly keeping only a\nfraction of the concepts in ConceptNet when train-\nDataset 0 0.3 0.7 1\nEC 0.7345 0.7397 0.7426 0.7363\nDailyDialog 0.5365 0.5432 0.5451 0.5383\nMELD 0.5321 0.5395 0.5366 0.5306\nEmoryNLP 0.3528 0.3624 0.3571 0.3488\nIEMOCAP 0.5344 0.5367 0.5314 0.5251\nTable 4: Analysis of the relatedness-affectiveness\ntradeoff on the validation sets. Each column corre-\nsponds to a ﬁxed λk for all concepts (see Equation 8).\nDataset KET -context -knowledge\nEC 0.7451 0.7343 0.7359\nDailyDialog 0.5544 0.5282 0.5402\nMELD 0.5401 0.5177 0.5248\nEmoryNLP 0.3712 0.3564 0.3553\nIEMOCAP 0.5389 0.4976 0.5217\nTable 5: Ablation study for KET on the validation sets.\ning and evaluating our model. The results are il-\nlustrated in Figure 3 (bottom portion). Adding\nmore concepts consistently improves model per-\nformance before reaching a plateau, validating the\nimportance of commonsense knowledge in detect-\ning emotions. We may expect the performance of\nour KET model to improve with the growing size\nof ConceptNet in the future.\nAnalysis of the relatedness-affectiveness trade-\noff: We experiment with different values of λk ∈\n[0,1] (see Equation 8) for all k and report the re-\nsults in Table 4. It is clear that λk makes a notice-\nable impact on the model performance. Discard-\ning relatedness or affectiveness completely will\ncause signiﬁcant performance drop on all datasets,\nwith one exception of IEMOCAP. One possible\nreason is that conversations in IEMOCAP are\nemotional dialogues, therefore, the affectiveness\nfactor in our proposed graph attention mechanism\ncan provide more discriminative power.\nAblation Study: We conduct ablation study to in-\nvestigate the contribution of context and knowl-\nedge as reported in Table 5. It is clear that both\ncontext and knowledge are essential to the strong\nperformance of KET on all datasets. Note that re-\nmoving context has a greater impact on long con-\nversations than short conversations, which is ex-\npected because more contextual information is lost\nin long conversations.\n5.3 Error Analysis\nDespite the strong performance of our model, it\nstill fails to detect certain emotions on certain\ndatasets. We rank the F1 score of each emotion\nper dataset and investigate the emotions with the\nworst scores. We found that disgust and fear are\ngenerally difﬁcult to detect and differentiate. For\nexample, the F1 score of fear emotion in MELD is\nas low as 0.0667. One possible cause is that these\ntwo emotions are intrinsically similar. The V AD\nvalues of both emotions have low valence, high\narousal and low dominance (Mehrabian, 1996).\nAnother cause is the small amount of data avail-\nable for these two emotions. How to differentiate\nintrinsically similar emotions and how to effec-\ntively detect emotions using limited data are two\nchallenging directions in this ﬁeld.\n6 Conclusion\nWe present a knowledge-enriched transformer to\ndetect emotions in textual conversations. Our\nmodel learns structured conversation represen-\ntations via hierarchical self-attention and dy-\nnamically refers to external, context-aware, and\nemotion-related knowledge entities from knowl-\nedge bases. Experimental analysis demonstrates\nthat both contextual information and common-\nsense knowledge are beneﬁcial to model perfor-\nmance. The tradeoff between relatedness and af-\nfectiveness plays an important role as well. In ad-\ndition, our model outperforms the state-of-the-art\nmodels on most of the tested datasets of varying\nsizes and domains.\nGiven that there are similar emotion lexicons\nto NRC V AD in other languages and ConceptNet\nis a multilingual knowledge base, our model can\nbe easily adapted to other languages. In addition,\ngiven that NRC V AD is the only emotion-speciﬁc\ncomponent, our model can be adapted as a generic\nmodel for conversation analysis.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers for their valuable comments. This re-\nsearch is supported, in part, by the National Re-\nsearch Foundation, Prime Ministers Ofﬁce, Singa-\npore under its AI Singapore Programme (Award\nNumber: AISG-GC-2019-003) and under its NRF\nInvestigatorship Programme (NRFI Award No.\nNRF-NRFI05-2019-0002). This research is also\nsupported, in part, by the Alibaba-NTU Singapore\nJoint Research Institute, Nanyang Technological\nUniversity, Singapore.\nReferences\nMuhammad Abdul-Mageed and Lyle Ungar. 2017.\nEmonet: Fine-grained emotion detection with gated\nrecurrent neural networks. In ACL, volume 1, pages\n718–728.\nNabiha Asghar, Pascal Poupart, Jesse Hoey, Xin Jiang,\nand Lili Mou. 2018. Affective neural response gen-\neration. In ECIR, pages 154–166. Springer.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nAnil Bandhakavi, Nirmalie Wiratunga, Stewart Massie,\nand Deepak Padmanabhan. 2017. Lexicon genera-\ntion for emotion detection from text. IEEE Intelli-\ngent Systems, 32(1):102–108.\nYoshua Bengio, Patrice Simard, Paolo Frasconi, et al.\n1994. Learning long-term dependencies with gradi-\nent descent is difﬁcult. IEEE Transactions on Neu-\nral Networks, 5(2):157–166.\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. 2008. IEMOCAP: Interactive emotional\ndyadic motion capture database. Language Re-\nsources and Evaluation, 42(4):335.\nAnkush Chatterjee, Umang Gupta, Manoj Kumar\nChinnakotla, Radhakrishnan Srikanth, Michel Gal-\nley, and Puneet Agrawal. 2019. Understanding emo-\ntions in text using deep learning and big data. Com-\nputers in Human Behavior, 93:309 – 317.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\nLong short-term memory-networks for machine\nreading. In EMNLP, pages 551–561.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder–decoder\nfor statistical machine translation. In EMNLP, pages\n1724–1734.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nLaurence Devillers, Ioana Vasilescu, and Lori Lamel.\n2002. Annotation and detection of emotion in a\ntask-oriented human-human dialog corpus. In Pro-\nceedings of ISLE Workshop.\nLaurence Devillers and Laurence Vidrascu. 2006.\nReal-life emotions detection with lexical and par-\nalinguistic cues on human-human call center di-\nalogs. In Ninth International Conference on Spoken\nLanguage Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In ICLR.\nPaul Ekman. 1992. An argument for basic emotions.\nCognition & emotion, 6(3-4):169–200.\nChristiane Fellbaum. 2012. Wordnet. The Encyclope-\ndia of Applied Linguistics.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley. 2018. A knowledge-grounded neural\nconversation model. In AAAI.\nSangdo Han, Jeesoo Bang, Seonghan Ryu, and\nGary Geunbae Lee. 2015. Exploiting knowledge\nbase to generate responses for natural language di-\nalog listening agents. In Proceedings of the 16th\nSIGDIAL, pages 129–133.\nYanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He,\nZhanyi Liu, Hua Wu, and Jun Zhao. 2017. An end-\nto-end model for question answering over knowl-\nedge base with cross-attention combining global\nknowledge. In ACL, pages 221–231.\nDevamanyu Hazarika, Soujanya Poria, Rada Mihal-\ncea, Erik Cambria, and Roger Zimmermann. 2018a.\nIcon: Interactive conversational memory network\nfor multimodal emotion detection. In EMNLP,\npages 2594–2604.\nDevamanyu Hazarika, Soujanya Poria, Amir Zadeh,\nErik Cambria, Louis-Philippe Morency, and Roger\nZimmermann. 2018b. Conversational memory net-\nwork for emotion recognition in dyadic dialogue\nvideos. In NAACL, volume 1, pages 2122–2132.\nJunqing He, Bing Wang, Mingming Fu, Tianqi Yang,\nand Xuemin Zhao. 2019. Hierarchical attention and\nknowledge matching networks with information en-\nhancement for end-to-end task-oriented dialog sys-\ntems. IEEE Access, 7:18871–18883.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nYajie Hu, Xiaoou Chen, and Deshun Yang. 2009.\nLyric-based song emotion detection with affective\nlexicon and fuzzy clustering method. In ISMIR,\npages 123–128.\nEva Hudlicka. 2011. Guidelines for designing compu-\ntational models of emotions. International Journal\nof Synthetic Emotions, 2(1):26–79.\nHamed Khanpour and Cornelia Caragea. 2018. Fine-\ngrained emotion detection in health-related online\nposts. In EMNLP, pages 1160–1166.\nChlo´e Kiddon, Luke Zettlemoyer, and Yejin Choi.\n2016. Globally coherent text generation with neu-\nral checklist models. In EMNLP, pages 329–339.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nAbhishek Kumar, Daisuke Kawahara, and Sadao Kuro-\nhashi. 2018. Knowledge-enriched two-layered at-\ntention network for sentiment analysis. In NAACL,\nvolume 2, pages 253–258.\nChul Min Lee and Shrikanth S Narayanan. 2005. To-\nward detecting emotions in spoken dialogs. IEEE\nTransactions on Speech and Audio Processing ,\n13(2):293–303.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. Dailydialog: A man-\nually labelled multi-turn dialogue dataset. In IJC-\nNLP, volume 1, pages 986–995.\nShuman Liu, Hongshen Chen, Zhaochun Ren, Yang\nFeng, Qun Liu, and Dawei Yin. 2018. Knowledge\ndiffusion for neural dialogue generation. In ACL,\npages 1489–1498.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung.\n2018. Mem2seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog sys-\ntems. In ACL, volume 1, pages 1468–1478.\nNavonil Majumder, Soujanya Poria, Devamanyu Haz-\narika, Rada Mihalcea, Alexander Gelbukh, and Erik\nCambria. 2019. Dialoguernn: An attentive rnn for\nemotion detection in conversations. In AAAI.\nAlbert Mehrabian. 1996. Pleasure-arousal-dominance:\nA general framework for describing and measuring\nindividual differences in temperament. Current Psy-\nchology, 14(4):261–292.\nTodor Mihaylov and Anette Frank. 2018. Knowledge-\nable reader: Enhancing cloze-style reading compre-\nhension with external commonsense knowledge. In\nACL, pages 821–832.\nNikita Moghe, Siddhartha Arora, Suman Banerjee, and\nMitesh M Khapra. 2018. Towards exploiting back-\nground knowledge for building conversation sys-\ntems. In EMNLP, pages 2322–2332.\nSaif Mohammad. 2018a. Obtaining reliable human rat-\nings of valence, arousal, and dominance for 20,000\nenglish words. In ACL, pages 174–184.\nSaif M. Mohammad. 2018b. Word affect intensities.\nIn LREC.\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up?: sentiment classiﬁcation using\nmachine learning techniques. In EMNLP, pages 79–\n86. Association for Computational Linguistics.\nPrasanna Parthasarathi and Joelle Pineau. 2018. Ex-\ntending neural generative conversational model us-\ning external knowledge sources. In EMNLP, pages\n690–695.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In EMNLP, pages 1532–1543.\nSoujanya Poria, Erik Cambria, Devamanyu Hazarika,\nNavonil Majumder, Amir Zadeh, and Louis-Philippe\nMorency. 2017. Context-dependent sentiment anal-\nysis in user-generated videos. In ACL, volume 1,\npages 873–883.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2018. Meld: A multimodal multi-party\ndataset for emotion recognition in conversations.\narXiv preprint arXiv:1810.02508.\nSoujanya Poria, Navonil Majumder, Rada Mihalcea,\nand Eduard Hovy. 2019. Emotion recognition in\nconversation: Research challenges, datasets, and re-\ncent advances. arXiv preprint arXiv:1905.02947.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nYi Luan Mirella Lapata Rik Koncel-Kedziorski,\nDhanush Bekal and Hannaneh Hajishirzi. 2019.\nText Generation from Knowledge Graphs with\nGraph Transformers. In NAACL.\nArmin Seyeditabari, Narges Tabari, and Wlodek\nZadrozny. 2018. Emotion detection in text: a re-\nview. arXiv preprint arXiv:1806.00674.\nDinghan Shen, Guoyin Wang, Wenlin Wang, Mar-\ntin Renqiang Min, Qinliang Su, Yizhe Zhang, Chun-\nyuan Li, Ricardo Henao, and Lawrence Carin.\n2018. Baseline needs more love: On simple word-\nembedding-based models and associated pooling\nmechanisms. In ACL, pages 440–450.\nRobyn Speer, Joshua Chin, and Catherine Havasi.\n2017. Conceptnet 5.5: An open multilingual graph\nof general knowledge. In AAAI.\nShang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen.\n2018. How time matters: Learning time-decay at-\ntention for contextual spoken language understand-\ning in dialogues. In NAACL, volume 1, pages 2133–\n2142.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William Co-\nhen. 2018. Open domain question answering us-\ning early fusion of knowledge bases and text. In\nEMNLP, pages 4231–4242.\nMaite Taboada, Julian Brooke, Milan Toﬁloski, Kim-\nberly V oll, and Manfred Stede. 2011. Lexicon-based\nmethods for sentiment analysis. Computational Lin-\nguistics, 37(2):267–307.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nPetar Velikovi, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li, and Yoshua Bengio.\n2018. Graph attention networks. In ICLR.\nSida Wang and Christopher D Manning. 2012. Base-\nlines and bigrams: Simple, good sentiment and topic\nclassiﬁcation. In ACL, pages 90–94. Association for\nComputational Linguistics.\nChien-Sheng Wu, Richard Socher, and Caiming Xiong.\n2019. Global-to-local memory pointer networks for\ntask-oriented dialogue. In ICLR.\nTom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,\nSubham Biswas, and Minlie Huang. 2018. Aug-\nmenting end-to-end dialogue systems with common-\nsense knowledge. In AAAI.\nSayyed M Zahiri and Jinho D Choi. 2018. Emotion de-\ntection on tv show transcripts with sequence-based\nconvolutional neural networks. In Workshops at\nAAAI.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu.\n2018a. Improving the transformer translation model\nwith document-level context. In EMNLP, pages\n533–542.\nYuxiang Zhang, Jiamei Fu, Dongyu She, Ying Zhang,\nSenzhang Wang, and Jufeng Yang. 2018b. Text\nemotion distribution learning via multi-task convo-\nlutional neural network. In IJCAI, pages 4595–\n4601.\nPeixiang Zhong and Chunyan Miao. 2019. ntuer at\nSemEval-2019 task 3: Emotion classiﬁcation with\nword and sentence representations in RCNN. In Se-\nmEval, pages 282–286.\nPeixiang Zhong, Di Wang, and Chunyan Miao. 2019.\nAn affect-rich neural conversational model with bi-\nased attention and weighted cross-entropy loss. In\nAAAI, pages 7492–7500.\nHao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan\nZhu, and Bing Liu. 2018a. Emotional chatting ma-\nchine: Emotional conversation generation with in-\nternal and external memory. In AAAI.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018b. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In IJCAI, pages 4623–4629.\nXiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying\nChen, Wayne Xin Zhao, Dianhai Yu, and Hua Wu.\n2018c. Multi-turn response selection for chatbots\nwith deep attention matching network. In ACL, vol-\nume 1, pages 1118–1127.",
  "topic": "Conversation",
  "concepts": [
    {
      "name": "Conversation",
      "score": 0.8168811798095703
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.7307402491569519
    },
    {
      "name": "Computer science",
      "score": 0.6946913599967957
    },
    {
      "name": "Transformer",
      "score": 0.6064963936805725
    },
    {
      "name": "Emotion detection",
      "score": 0.6050142049789429
    },
    {
      "name": "Natural language processing",
      "score": 0.4368709623813629
    },
    {
      "name": "Task (project management)",
      "score": 0.43229907751083374
    },
    {
      "name": "Context (archaeology)",
      "score": 0.42081573605537415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3833382725715637
    },
    {
      "name": "Emotion recognition",
      "score": 0.2692492604255676
    },
    {
      "name": "Psychology",
      "score": 0.22816067934036255
    },
    {
      "name": "Knowledge extraction",
      "score": 0.2177181839942932
    },
    {
      "name": "Communication",
      "score": 0.11937475204467773
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}