{
  "title": "Evaluating Large Language Models in Psychological Research: A Guide for Reviewers",
  "url": "https://openalex.org/W4394993753",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3101608876",
      "name": "Suhaib Abdurahman",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A5055105286",
      "name": "Alireza S. Ziabari",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2145746840",
      "name": "Alexander Moore",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A4207561409",
      "name": "Daniel Bartels",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2145804492",
      "name": "Morteza Dehghani",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388733437",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4367353919",
    "https://openalex.org/W4296154596",
    "https://openalex.org/W4322008875",
    "https://openalex.org/W4319265466",
    "https://openalex.org/W4411630036",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4391184004",
    "https://openalex.org/W2949815019",
    "https://openalex.org/W4296169830",
    "https://openalex.org/W4361193900",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W4381738940",
    "https://openalex.org/W2099813784",
    "https://openalex.org/W4241566321",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W4385572239",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W4375958655",
    "https://openalex.org/W4386724748",
    "https://openalex.org/W2944378183",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4389515417",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4389519496",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W3034282334",
    "https://openalex.org/W4387634446",
    "https://openalex.org/W4382619745",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4383987504",
    "https://openalex.org/W4387389584",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4393905506",
    "https://openalex.org/W4390601273",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4390833824",
    "https://openalex.org/W4377231151",
    "https://openalex.org/W4391212365",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4392575397",
    "https://openalex.org/W4390875237",
    "https://openalex.org/W4366850553",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W114693439",
    "https://openalex.org/W4385966247",
    "https://openalex.org/W4402670860",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4361192999"
  ],
  "abstract": "Large Language Models (LLMs) are being increasingly used in scientific research, be it to analyze data, generate synthetic data, or even to write scientific papers. This trend necessitates that journal reviewers are able to evaluate the quality of works that utilize LLMs. We provide reviewers of psychological research with a comprehensive guide on evaluating research that uses LLMs, examining their dual roles of automating data processing and simulating human data. Essential considerations for reviewers are highlighted, focusing on the evaluation of methodological rigor, the importance of replicability, and the validity of results when employing LLMs. We offer practical advice on assessing the appropriateness of LLM applications in submitted studies, emphasizing the need for transparency in methodological reporting and the challenges posed by the non-deterministic and continuously evolving nature of these models. By providing a framework for critical review, this guide aims to ensure high-quality, innovative research within the evolving landscape of psychological studies utilizing LLMs.",
  "full_text": "EVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  1   \n    A  Primer  for  Evaluating  Large  Language  Models  in  Social  Science  Research  \nSuhaib  Abdurahman\n 1,2\n,  Alireza  Salkhordeh  Ziabari\n 2,3\n,  Alexander  K.  Moore\n 4\n,   Daniel  M.  Bartels\n 5\n,  and  Morteza  Dehghani\n 1,2,3\n \n1\nDepartment  of  Psychology,  University  of  Southern  California  \n2\nCenter\n \nfor\n \nComputational\n \nLanguage\n \nSciences,\n \nUniversity\n \nof\n \nSouthern\n \nCalifornia\n \n \n3\nDepartment\n \nof\n \nComputer\n \nScience,\n \nUniversity\n \nof\n \nSouthern\n \nCalifornia\n \n \n4\nDepartment\n \nof\n \nMarketing,\n \nUniversity\n \nof\n \nIllinois\n \nChicago\n \n \n5\nDepartment\n \nof\n \nMarketing,\n \nUniversity\n \nof\n \nChicago\n    Currently  in  press  at  Advances  in  Methods  and  Practices  in  Psychological  Science     Author  Note  \nSuhaib  Abdurahman   https://orcid.org/0000-0001-5615-0129 \nAlexander  Moore   https://orcid.org/0000-0003-4046-3117    \nDaniel  M.  Bartels  https://orcid.org/0000-0003-1642-8426  \nMorteza  Dehghani  \n \nhttps://orcid.org/0000-0002-9478-4365  Correspondence  concerning  this  article  should  be  addressed  to  Suhaib  Abdurahman,  \nDepartment\n \nPsychology,\n \nUniversity\n \nof\n \nSouthern\n \nCalifornia,\n \n501\n \nSeeley\n \nG.\n \nMudd\n \nBuilding,\n \n3620\n McClintock  Ave,  Los  Angeles,  CA  90089.  E-mail:  sabdurah@usc.edu.  This  research  was  supported  in  part  by  DARPA  INCAS  HR001121C0165  and  AFOSR  A9550-23-1-0463.  The  \nviews\n \nand\n \nconclusions\n \ncontained\n \nherein\n \nare\n \nthose\n \nof\n \nthe\n \nauthors\n \nand\n \nshould\n \nnot\n \nbe\n \ninterpreted\n \nas\n \nnecessarily\n \nrepresenting\n \nthe\n \nofficial\n \npolicies,\n \neither\n \nexpressed\n \nor\n \nimplied,\n \nof\n \nDARPA\n \nor\n \nthe\n \nU.S.\n \nGovernment.\n \nThe\n \nU.S.\n \nGovernment\n \nis\n \nauthorized\n \nto\n \nreproduce\n \nand\n \ndistribute\n \nreprints\n \nfor\n \ngovernmental\n \npurposes\n \nnotwithstanding\n \nany\n \ncopyright\n \nannotation\n \ntherein.\n \n \n\nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  2   \nAbstract  \nAutoregressive  Large  Language  Models  (LLMs)  exhibit  remarkable  conversational  and  \nreasoning\n \nabilities,\n \nand\n \nexceptional\n \nflexibility\n \nacross\n \na\n \nwide\n \nrange\n \nof\n \ntasks.\n \nSubsequently,\n \nLLMs\n \nare\n \nbeing\n \nincreasingly\n \nused\n \nin\n \nscientific\n \nresearch,\n \nto\n \nanalyze\n \ndata,\n \ngenerate\n \nsynthetic\n \ndata,\n \nor\n \neven\n \nto\n \nwrite\n \nscientific\n \npapers.\n \nThis\n \ntrend\n \nnecessitates\n \nthat\n \nauthors\n \nfollow\n \nbest\n \npractices\n \nfor\n \nconducting\n \nand\n \nreporting\n \nLLM\n \nresearch\n \nand\n \nthat\n \njournal\n \nreviewers\n \nare\n \nable\n \nto\n \nevaluate\n \nthe\n \nquality\n \nof\n \nworks\n \nthat\n \nuse\n \nLLMs.\n \nWe\n \nprovide\n \nauthors\n \nof\n \nsocial\n \nscientific\n \nresearch\n \nwith\n \nessential\n \nrecommendations\n \nto\n \nensure\n \nreplicable\n \nand\n \nrobust\n \nresults\n \nusing\n \nLLMs.\n \nOur\n \nrecommendations\n \nalso\n \nhighlight\n \nconsiderations\n \nfor\n \nreviewers,\n \nfocusing\n \non\n \nmethodological\n \nrigor,\n \nreplicability,\n \nand\n \nvalidity\n \nof\n \nresults\n \nwhen\n \nevaluating\n \nstudies\n \nthat\n \nuse\n \nLLMs\n \nto\n \nautomate\n \ndata\n \nprocessing\n \nor\n \nsimulate\n \nhuman\n \ndata.\n \nWe\n \noffer\n \npractical\n \nadvice\n \non\n \nassessing\n \nthe\n \nappropriateness\n \nof\n \nLLM\n \napplications\n \nin\n \nsubmitted\n \nstudies,\n \nemphasizing\n \nthe\n \nneed\n \nfor\n \ntransparency\n \nin\n \nmethodological\n \nreporting\n \nand\n \nthe\n \nchallenges\n \nposed\n \nby\n \nthe\n \nnon-deterministic\n \nand\n \ncontinuously\n \nevolving\n \nnature\n \nof\n \nthese\n \nmodels.\n \nBy\n \nproviding\n \na\n \nframework\n \nfor\n \nbest\n \npractices\n \nand\n \ncritical\n \nreview,\n \nthis\n \nprimer\n \naims\n \nto\n \nensure\n \nhigh-quality,\n \ninnovative\n \nresearch\n \nwithin\n \nthe\n \nevolving\n \nlandscape\n \nof\n \nsocial\n \nscience\n \nstudies\n \nusing\n \nLLMs.\n Keywords:  large  language  models,  natural  language  processing,  computational  social  \nscience,\n \nsynthetic\n \ndata\n \n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  3   \nA  Primer  for  Evaluating  Large  Language  Models  in  Social  Science  Research  \nLarge  Language  Models  (LLMs)\n 1\n such  as  GPT-4o  have  demonstrated  remarkable  \ncapabilities,\n \nsuch\n \nas\n \nengaging\n \nin\n \nrealistic\n \nconversations\n \n(Dam\n \net\n \nal.,\n \n2024),\n \ngenerating\n \ncoherent\n \ntexts,\n \nincluding\n \nscientific\n \npapers\n \n(Wang\n \net\n \nal.,\n \n2024),\n \nand\n \nmore\n \nrecently,\n \nhandling\n \ntasks\n \nthat\n \ncombine\n \ntext\n \nand\n \nimages\n \n(Yin\n \net\n \nal.,\n \n2023,\n \nWu\n \net\n \nal.,\n \n2024).\n \nAs\n \na\n \nresult\n \nof\n \nthese\n \ncapabilities,\n \nresearchers\n \nhave\n \nincreasingly\n \nused\n \nLLMs\n \nto\n \naid\n \nin\n \nresearch\n \n(Liang\n \net\n \nal.,\n \n2024;\n \nKobak\n \net\n \nal.,\n \n2024;\n \nKe\n \net\n \nal.,\n \n2024).\n \nIn\n \nsocial\n \nscience\n \nresearch,\n \none\n \nof\n \nthe\n \nmost\n \npromising\n \nuses\n \nfor\n \nLLMs\n \nis\n \ntext\n \nanalysis,\n \nsuch\n \nas\n \ncoding\n \nlarge\n \ndatasets,\n \nreducing\n \nreliance\n \non\n \nslow\n \nand\n \ncostly\n \nhuman\n \ncoders.\n \nThey\n \nmay\n \nalso\n \nbe\n \nused\n \nto\n \nsimulate\n \nhuman\n \nresponses\n \nto\n \nstimuli\n \nand,\n \nif\n \nshown\n \nto\n \nbe\n \neffective,\n \ncould\n \ncomplement\n \nor\n \neven\n \nreplace\n \nhuman\n \nparticipants\n \nin\n \nexperimental\n \nsettings.\n \nHowever,\n \ntheir\n \nability\n \nto\n \naccurately\n \nmirror\n \nhuman\n \nbehavior\n \nremains\n \nan\n \nopen\n \nempirical\n \nquestion\n \nthat\n \nis\n \nonly\n \nbeginning\n \nto\n \nbe\n \nexplored.\n \nAdditionally,\n \nwhile\n \nLLMs\n \nare\n \npowerful,\n \nthey\n \nstill\n \nface\n \nlimitations.\n \nFor\n \nexample,\n \nthey\n \nare\n \nlimited\n \nin\n \nthe\n \namount\n \nof\n \ninformation\n \nthey\n \ncan\n \nprocess\n \nat\n \nonce,\n \nleading\n \nto\n \ndiminished\n \nperformance\n \nfor\n \nvery\n \nlong\n \ninputs.\n \nFurthermore,\n \nwhile\n \nLLMs\n \nexcel\n \nin\n \npattern\n \nrecognition\n \nand\n \ngenerating\n \nplausible\n \nresponses,\n \nissues\n \nregarding\n \nreliability,\n \nvalidity,\n \naccess,\n \nand\n \ntransparency\n \npersist\n \n(Kumar,\n \n2024;\n \nMinaee\n \net\n \nal.,\n \n2024).\n \nAt\n \nthe\n \nsame\n \ntime,\n \nthese\n \nmodels’\n \ncapabilities\n \nare\n \nadvancing\n \nrapidly.\n \nFor\n \nsocial\n \nscience\n \nresearchers,\n \nboth\n \nthe\n \nlack\n \nof\n \nunderstanding\n \nand\n \nthe\n \nconstantly\n \nchanging\n \nmodels\n \ncreate\n \nchallenges\n \nfor\n \nthe\n \nvalidity\n \nand\n \nreplicability\n \nof\n \nresearch\n \nusing\n \nLLMs.\n \nIn  response  to  these  challenges,  in  this  primer,  we  provide  essential  practices  that  \nresearchers\n \nproducing\n \nand\n \nevaluating\n \nresearch\n \nusing\n \nLLMs\n \nought\n \nto\n \nfollow\n \nto\n \npromote\n \nvalidity\n \nand\n \nreplicability.\n \nWhile\n \nLLMs\n \nare\n \nalso\n \nfrequently\n \nused\n \nto\n \nassist\n \nin\n \nwriting\n \nresearch\n \nreports,\n \nthis\n \nprimer\n \nfocuses\n \non\n \nthe\n \nresearch\n \nprocess\n \nitself\n \n(see\n \ncurrent\n \nstandards\n \nof\n \nLLMs\n \nas\n \nwriting\n \naids,\n \ne.g.,\n \nin\n \nBoyd-Graber\n \net\n \nal.,\n \n2023a).\n \nGeneral\n \nresearch\n \npractices\n \nshould\n \nadhere\n \nto\n \nexisting\n \npredictive\n \nmodeling\n \nguidelines\n \n(e.g.\n \nTRIPOD+AI;\n \nCollins\n \net\n \nal.,\n \n2024),\n \nbut\n \nthe\n \nunique\n \nnature\n \nof\n \nLLMs\n \nnecessitates\n \nadditional\n \nconsiderations.\n \nSince\n \nwe\n \ncannot\n \ncover\n \nall\n \napplications\n \nof\n \nLLMs\n \nin\n \nsocial\n \n1\n Large  language  models  (LLMs)  broadly  fall  into  two  categories:  autoencoding  models  (e.g.,  BERT)  and  autoregressive  \nmodels\n \n(e.g.,\n \nChatGPT).\n \nAutoencoding\n \nmodels\n \nexcel\n \nat\n \ncontextual\n \ntasks\n \nlike\n \ntext\n \nclassification.\n \nAutoregressive\n \nmodels,\n \ntrained\n \non\n \nsequential\n \nword\n \nprediction,\n \nare\n \nversatile\n \nfor\n \ntext\n \ngeneration,\n \nconversation,\n \nand\n \neven\n \nclassification\n \nvia\n \nnatural\n \nlanguage\n \ninstructions.\n \nRecent\n \nadvances\n \nhave\n \nled\n \nto\n \nthe\n \nwidespread\n \nadoption\n \nof\n \nautoregressive\n \nmodels,\n \nnotably\n \nChatGPT,\n \nin\n \nvarious\n \ndomains.\n \nIn\n \nthis\n \nprimer,\n \nthe\n \nterm\n \n'LLMs'\n \nspecifically\n \nrefers\n \nto\n \nautoregressive\n \nmodels.\n \n \n \n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  4   \nscience  research  in  this  limited  space,  we  focus  on  their  use  in  coding  data  and  simulating  human  \nresponses,\n \nas\n \nthese\n \nrepresent\n \nsome\n \nof\n \nthe\n \nmost\n \nimmediate\n \nand\n \npromising\n \napplications.\n \nIt\n \nis\n \nworth\n \nnoting,\n \nhowever,\n \nthat\n \nsome\n \nof\n \nthe\n \ndiscussion\n \nin\n \nthis\n \narticle\n \nis\n \nlikely\n \nto\n \napply\n \nmore\n \nbroadly.\n \n \nWe  have  designed  this  primer  to  be  helpful  for  researchers  at  all  levels  of  familiarity  with  \nthe\n \nuse\n \nof\n \nLLMs\n \nin\n \nresearch.\n \nWe\n \nbegin\n \nby\n \nproviding\n \nbasic\n \ninformation\n \nabout\n \nLLMs\n \nand\n \nbackground\n \ninformation\n \non\n \nthe\n \ncurrent\n \nstate\n \nof\n \nLLMs\n \nin\n \nsocial\n \nscience\n \nresearch.\n \nWe\n \nthen\n \ndiscuss\n \nkey\n \nissues\n \naround\n \nusing\n \nLLMs\n \nin\n \nsocial\n \nscience\n \nresearch\n \nbefore\n \ndiscussing\n \nways\n \nof\n \nincreasing\n \nvalidity\n \nand\n \nreproducibility.\n \nTo\n \nhelp\n \nresearchers\n \nimplement\n \nour\n \nsuggestions,\n \nwe\n \ninclude\n \na\n \nchecklist\n \nlaying\n \nout\n \nkey\n \nsteps\n \nfor\n \nenhancing\n \nvalidity\n \nand\n \nreproducibility.\n \nFinally,\n \nin\n \nan\n \nappendix,\n \nwe\n \ninclude\n \na\n \nconcrete\n \nexample\n \nof\n \nwhat\n \nfollowing\n \nour\n \nrecommendation\n \nmight\n \nlook\n \nlike.\n \n \n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  5   \nHow  to  Use  This  Primer  \nThis  primer  is  designed  to  guide  readers  of  varying  expertise  levels  LLM  research.  Some  sections  \nmay\n \nbe\n \nmore\n \nrelevant\n \nthan\n \nothers,\n \ndepending\n \non\n \nthe\n \nreader’s\n \nbackground.\n \n●  New  to  LLMs:  ○  Start  here:  Refer  to  the  Glossary  (Table  1)  for  definitions  of  essential  LLM  concepts  \nand\n \nterminology\n \n(e.g.,\n \n“prompt,”\n \n“zero-shot”).\n \nReview\n \nTable\n \n3\n \nto\n \ngain\n \nan\n \noverview\n \nof\n \nrecommended\n \npractices\n \nand\n \nkey\n \nprocedures.\n ○  Next  sections:  Consult  the  sections  on  dealing  with  non-determinism,  dealing  with  \nmodel-updates,\n \nand\n \nconfirming\n \nLLM\n \noutputs\n \nwith\n \nhuman\n \ndata.\n ●  Familiar  with  LLMs,  but  unfamiliar  with  behavioral  research  methodology:  ○  Start  here:  Review  Table  3  to  gain  an  overview  of  recommended  practices  and  key  \nprocedures,\n \nwhich\n \nmay\n \ndiffer\n \nsubstantially\n \nfrom\n \npurely\n \ncomputational\n \napplications.\n ○  Next  sections:  Sections  addressing  replication  and  validity,  as  these  are  central  to  \nensuring\n \nmethodological\n \nrigor\n \nin\n \na\n \nsocial\n \nscientific\n \ncontext.\n ●  Experienced  in  both  domains:  ○  Start  here:  Review  Table  3  to  confirm  you  are  following  recommended  best  practices.   ○  Next  sections:  Depending  on  the  specific  research  interests,  proceed  to  the  sections  \nabout\n \nconfirming\n \nLLM\n \noutputs\n \nwith\n \nhuman\n \ndata,\n \ndata\n \nprocessing\n \nand\n \nerror\n \nhandling,\n \nand\n \nspecial\n \nconsiderations\n \nfor\n \nsimulating\n \nhuman\n \ndata\n \nfor\n \nfurther\n \ndetails\n \non\n \nspecific\n \nchallenges\n \nand\n \nmethodological\n \nconsiderations\n \nFinally,  readers  who  want  a  concrete  example  of  how  to  apply  these  recommendations  in  practice  \nshould\n \nrefer\n \nto\n \nthe\n \ndetailed\n \ntemplate\n \nin\n \nthe\n \nAppendix.\n \nThis\n \nwalk-through\n \nshowcases\n \na\n \nstep-by-step\n \nprocedure\n \nfollowing\n \nthe\n \nchecklist\n \nin\n \nTable\n \n3\n \napplied\n \nto\n \na\n \nconcrete\n \nresearch\n \nproject.\n   \n  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  6   \nTable  1  \nOverview  of  technical  terms.   Term  Explanation  \n1  Generative  AI  A  type  of  AI  that  focuses  on  creating  content,  such  as  text,  images,  or  music,  that  resembles  human-generated  content.  \n2  Autoregressive  Models  Models  that  predict  data  from  past  values  in  a  sequence,  commonly  used  in  time  series  forecasting  and  language  modeling.  \n3  Autoregressive  Large  Language  Model  (LLM)  \nAn  autoregressive  model  for  generating  and  understanding  human  language.  LLMs  can  automate  complex  tasks,  such  as  interpreting  text  data.  \n4  Fine-Tuning  The  process  of  taking  a  pre-trained  model  and  further  training  it  on  a  specialized  dataset  to  improve  its  accuracy  on  a  given  task.  \n5  Embedding  A  vector  representation  of,  e.g.,  text  data,  usually  in  a  continuous,  real  number  vector  space.  \n6  Prompt  The  input  to  an  LLM.  It  usually  contains  instructions  to  generate  a  specific  output  or  perform  a  task.  The  prompt  design  is  crucial,  as  it  influences  the  LLM’s  autoregressive  content  generation.  \n7  Open-Source,  Closed-Source  Open-Source  refers  to  software  whose  source  code  is  available  for  anyone  to  use,  modify,  and  distribute.  Closed-source  software  keeps  its  source  code  private.  \n8  Model  weights  Parameters  of  a  model  that  are  learned  from  the  training  data.  They  determine  the  generated  outputs.  \n9  Zero-Shot,  Few-Shot,  Many-  Shot  \nParadigms  where  a  model  performs  tasks  with  no  prior  examples  (zero-shot),  few  examples  (few-shot),  or  many  examples  (many-shot).  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  7   \nUsing  LLMs  in  Social  Scientific  Research  \nA  clear  understanding  of  both  the  strengths  and  limitations  of  LLMs  is  crucial  as  they  are  \nrapidly\n \nbecoming\n \nan\n \nintegral\n \npart\n \nof\n \nscientific\n \nresearch.\n \nRecent\n \nstudies\n \nunderscore\n \nthis\n \ntrend,\n \nwith\n \nLiang\n \net\n \nal.\n \n(2024)\n \nestimating\n \nthat\n \na\n \nsignificant\n \nportion\n \n(6.3%\n \nto\n \n17.5%)\n \nof\n \nscientific\n \npublications\n \nsince\n \n2020\n \nhave\n \nused\n \nLLMs\n \nin\n \ntheir\n \nwriting,\n \nwith\n \nusage\n \nsteadily\n \nrising\n \n(Kobak\n \net\n \nal.,\n \n2024).\n \nThis\n \nintegration\n \nis\n \nparticularly\n \nevident\n \nin\n \nthe\n \nfield\n \nof\n \npsychology,\n \nas\n \nhighlighted\n \nby\n \nKe\n \net\n \nal.'s\n \n(2024)\n \ncomprehensive\n \noverview\n \nof\n \nover\n \n100\n \nrecent\n \nworks.\n \nTheir\n \nanalysis\n \nshowcases\n \nthe\n \ngrowing\n \nadoption\n \nof\n \nLLMs\n \nacross\n \nvarious\n \nsubfields\n \nof\n \npsychology,\n \nincluding\n \ncognitive,\n \nsocial,\n \ncultural,\n \nclinical,\n \nand\n \ndevelopmental\n \ndomains.\n \nCurrently,\n \nthe\n \npredominant\n \nuse\n \nof\n \nLLMs\n \nin\n \nsocial\n \nscience\n \ninvolves\n \nevaluating\n \nhuman\n \nresponses\n \nand\n \nobservational\n \ndata,\n \nrather\n \nthan\n \nproducing\n \nprimary\n \ndata\n \n(see\n \nTable\n \n2\n \nfor\n \nexamples\n \nof\n \ncurrent\n \nLLM\n \nresearch\n \nin\n \nsocial\n \nscience).\n \nFree-response\n \ndata,\n \na\n \nvaluable\n \nresource\n \nfor\n \nunderstanding\n \nhuman\n \nthoughts\n \nand\n \nbehaviors\n \n(Ericsson\n \n&\n \nMoxley,\n \n2019),\n \nis\n \noften\n \nlabor-intensive\n \nto\n \ncode,\n \nespecially\n \nwithin\n \nlarge-scale\n \ndatasets\n \nlike\n \nsocial\n \nmedia\n \nposts\n \nand\n \nnews\n \narticles.\n \nLLMs\n \nstreamline\n \nthis\n \nprocess,\n \noffering\n \na\n \nsimpler\n \nand\n \nless\n \nlabor-intensive\n \nway\n \nof\n \ncoding\n \nsuch\n \ndata\n \nand\n \nthus\n \nfacilitate\n \nthe\n \nmeasurement\n \nof\n \npsychological\n \nconstructs\n \nat\n \nscale\n \n(Chiang\n \n&\n \nLee,\n \n2023;\n \nGilardi\n \net\n \nal.,\n \n2023;\n \nNaismith\n \net\n \nal.,\n \n2023;\n \nRathje\n \net\n \nal.,\n \n2024;\n \nTabone\n \n&\n \nde\n \nWinter,\n \n2023).\n \nFor\n \ninstance,\n \nLLMs\n \nare\n \nnow\n \nbeing\n \nused\n \nto\n \ncode\n \nlarge\n \ndatasets\n \nfor\n \npsychological\n \nconstructs\n \nacross\n \nmultiple\n \nlanguages\n \n(Rathje\n \net\n \nal.,\n \n2024),\n \npredict\n \npersonality\n \ntraits\n \nfrom\n \nsocial\n \nmedia\n \ninteractions\n \n(Amin\n \net\n \nal.,\n \n2023),\n \nand\n \neven\n \nidentify\n \nindividuals\n \nat\n \nrisk\n \nfor\n \nsuicide\n \n(Amin\n \net\n \nal.,\n \n2023).\n \nThey\n \nare\n \nalso\n \nproving\n \nvaluable\n \nin\n \nanalyzing\n \npolitical\n \naffiliation\n \n(Törnberg,\n \n2023)\n \nand\n \ndetecting\n \nviolent\n \nlanguage\n \nonline,\n \noffering\n \ninsights\n \ninto\n \nonline\n \nradicalization\n \nand\n \ncommunity\n \ndynamics\n \n(Matter\n \net\n \nal.,\n \n2024).\n \n \nSome  researchers  have  suggested  that  LLMs  can  also  simulate  complex  social  and  \ncognitive\n \nphenomena.\n \nFor\n \nexample,\n \nsome\n \nstudies\n \nhave\n \nleveraged\n \nLLMs\n \nto\n \nreplicate\n \nhuman-like\n \njudgments\n \nand\n \ndecision-making\n \nprocesses,\n \nincluding\n \nthe\n \ndisplay\n \nof\n \nheuristics\n \nand\n \nbiases\n \n(Coda-Forno\n \net\n \nal.,\n \n2023;\n \nDillion\n \net\n \nal.,\n \n2023;\n \nSuri\n \net\n \nal.,\n \n2024).\n \nOther\n \nresearch\n \nhas\n \nused\n \nLLMs\n \nto\n \nassess\n \nthe\n \npersuasion\n \nof\n \nhuman\n \nbeliefs\n \nregarding\n \npolarized\n \npolicy\n \nissues\n \n(Bai\n \net\n \nal.,\n \n2023).\n \nAdditionally,\n \nLLMs\n \nhave\n \nbeen\n \nused\n \nto\n \nsimulate\n \ncollective\n \nbehaviors\n \nsuch\n \nas\n \ncommunity\n \nformation\n \n(He\n \net\n \nal.,\n \n2024),\n \ndevelop\n \ngenerative\n \nagents\n \nthat\n \nexhibit\n \nrealistic\n \nbehaviors\n \nwithin\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  8   \ninteractive  environments  (Park  et  al.,  2023),  and  model  diverse  human  subpopulations  based  on  \ndemographic\n \ndata\n \n(Argyle\n \net\n \nal.,\n \n2023).\n \nFurthermore,\n \nresearchers\n \nhave\n \napplied\n \nLLMs\n \nto\n \nsimulate\n \ndecision-making\n \nscenarios\n \nand\n \neven\n \npilot\n \nsocial\n \nscience\n \nexperiments,\n \ndemonstrating\n \ntheir\n \npotential\n \nto\n \nadvance\n \nresearch\n \nin\n \nthese\n \nareas\n \n(Aher\n \net\n \nal.,\n \n2023;\n \nHorton,\n \n2023;\n \nHewitt\n \net\n \nal.,\n \n2024).\n \nHowever,\n \nthese\n \napplications\n \nremain\n \nnascent,\n \nand\n \ncaution\n \nis\n \nwarranted.\n \nLLMs\n \nstill\n \nexhibit\n \nnotable\n \nlimitations\n \nin\n \ntasks\n \nrequiring\n \nhuman-like\n \nreasoning.\n \nFor\n \nexample,\n \non\n \nthe\n \nAbstraction\n \nand\n \nReasoning\n \nCorpus\n \n(ARC;\n \nChollet,\n \n2019),\n \na\n \nbenchmark\n \ninvolving\n \nmulti-step\n \nproblem-solving,\n \nmodels\n \nlike\n \nGPT-4\n \nand\n \neven\n \nspecialized\n \nreasoning\n \nmodels\n \nsuch\n \nas\n \no1\n \n(OpenAI,\n \n2024)\n \nachieve\n \nonly\n \n10–20%\n \naccuracy,\n \nfar\n \nbelow\n \nhuman\n \nperformance\n \nof\n \naround\n \n80%\n \n(Chollet\n \net\n \nal.,\n \n2024;\n \nLee\n \net\n \nal.,\n \n2024).\n \nThese\n \nfindings\n \nunderscore\n \nthe\n \nneed\n \nfor\n \nrigorous\n \nvalidation\n \nof\n \nLLM\n \noutputs\n \nand\n \ncaution\n \nagainst\n \nequating\n \ntheir\n \nprocesses\n \nwith\n \nhuman\n \nreasoning\n \n(see\n \nXu\n \net\n \nal.,\n \n2023\n \nfor\n \na\n \ndetailed\n \nanalysis\n \nof\n \nLLM\n \nreasoning\n \nconstraints).\n \nLLMs’  versatility  and  ability  to  automate  data  processing  without  requiring  fine-tuning  \n(see\n \nTable\n \n1.4)\n \nfor\n \nmany\n \ntasks,\n \ncontributes\n \nto\n \ntheir\n \nincreasing\n \npopularity\n \nover\n \nalternative\n \nNatural\n \nLanguage\n \nProcessing\n \n(NLP)\n \nmethods—such\n \nas\n \nfine-tuned\n \nautoencoding\n \nmodels\n \n(e.g.,\n \nBERT,\n \nDevlin\n \net\n \nal.,\n \n2018)\n \nwhich\n \nproduce\n \ncontextualized\n \nembeddings\n \n(see\n \nTable\n \n1.5)\n \nfor\n \ndownstream\n \ntasks;\n \ndictionary-based\n \napproaches\n \n(e.g.,\n \nLIWC,\n \nPennebaker\n \net\n \nal.,\n \n2007);\n \nor\n \nhybrid\n \napproaches\n \n(e.g.,\n \nGarten\n \net\n \nal.,\n \n2018;\n \nAtari\n \net\n \nal.,\n \n2023).\n \nAlthough\n \nLLMs\n \noften\n \ndemonstrate\n \nperformance\n \non\n \npar\n \nwith,\n \nor\n \neven\n \nexceeding,\n \nthat\n \nof\n \nalternative\n \nNLP\n \ntechniques\n \n(Abdurahman\n \net\n \nal.,\n \n2024;\n \nRathje\n \net\n \nal.,\n \n2024),\n \nthey\n \nare\n \nnot\n \nalways\n \nthe\n \noptimal\n \nchoice.\n \nIn\n \nsome\n \ncases,\n \ngetting\n \nan\n \nLLM\n \nto\n \nachieve\n \nperformance\n \ncomparable\n \nto\n \nother\n \nNLP\n \nmethods\n \nmay\n \nnecessitate\n \nextensive\n \ncustomization\n \nthrough\n \nfine-tuning\n \nor\n \nelaborate\n \nprompting\n \n(see\n \nTable\n \n1.6)\n \n(Abdurahman\n \net\n \nal.,\n \n2024;\n \nBrown\n \net\n \nal.,\n \n2020).\n \nIn\n \naddition,\n \nhybrid\n \napproaches\n \nthat\n \ncombine\n \nthe\n \naccuracy\n \nof\n \nlanguage\n \nmodels\n \nwith\n \nthe\n \ninterpretability\n \nand\n \nreplicability\n \nof\n \ndictionary-based\n \nmethods,\n \nlike\n \nDistributed\n \nDictionary\n \nRepresentations\n \n(Garten\n \net\n \nal.,\n \n2018)\n \nand\n \nContextual\n \nContext\n \nRepresentations\n \n(Atari\n \net\n \nal.,\n \n2023a),\n \nshow\n \npromise\n \nin\n \nenhancing\n \ntext\n \nanalysis\n \nand\n \nextracting\n \nnuanced\n \npsychological\n \nconstructs\n \nfrom\n \nfree-response\n \ndata.\n \nThese\n \nmethods\n \noften\n \nalso\n \nprovide\n \nbenefits\n \nin\n \nterms\n \nof\n \ninterpretability,\n \nreplicability,\n \nand,\n \noccasionally,\n \nperformance\n \n(Abdurahman\n \net\n \nal.,\n \n2024,\n \nRathje\n \net\n \nal.,\n \n2024).\n \nReplicating\n \nthese\n \nadvantages\n \nwith\n \nLLMs\n \noften\n \nincreases\n \ncomplexity,\n \nrequiring\n \ntechnical\n \nexpertise\n \nand\n \naccess\n \nto\n \nthe\n \nmodel's\n \ninternal\n \nworkings,\n \nwhich\n \nis\n \ntypically\n \nlimited\n \nto\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  9   \nopen-source  models  (see  Table  1.7).  Researchers  should  carefully  weigh  these  considerations  \nwhen\n \ndeciding\n \non\n \nthe\n \nmost\n \nappropriate\n \napproach\n \nfor\n \ntheir\n \nspecific\n \nlanguage\n \nprocessing\n \nneeds.\n \n \nTable  2  Example  of  social  scientific  literature  using  LLMs  \nAuthors  Research  Question  Methodology  Key  Findings  \nDillion  et  al.  (2023)  \nCan  LLMs  replace  human  participants?  \nCompare  GPT  vs  human  moral  judgements.  \nHigh  correlation  between  human  and  GPT.  Binz  and  Schulz  (2023)  \nCan  we  capture  human  decision-making  by  fine-tuning  LLMs?  \nCompare  goodness-of-fit  of  LLMs  with  human  decisions.  \nFine-tuned  LLMs  capture  human  decisions  (better  than  other  models).  \nBlyler  &  Seligman  (2023)  \nCan  a  person's  narrative  identity  support  therapists  with  personalized  interventions?  \nGPT  generates  personalized  narratives  for  tailored  interventions.  \nGPT-4  generates  highly  credible  interventions.  \nDijkstra  et  al.  (2022)  \nReading  comprehension  quiz  generation  using  GPT  \nFine-tune  GPT-3  to  generate  quizzes  for  reading  comprehension.  \nGPT-3  generates  reasonable  quizzes  for  education  professionals.  \nPark  et  al.  (2022)  \nGenerate  social  simulators  for  system  designers  \nGenerate  reddit  discussions  using  GPT-3  and  have  participants  detect  the  LLM.  \nGPT-3  creates  convincing  social  interactions  that  can  be  studied  by  system  designers.   Matter  et  al.  (2024)  \nHow  does  online  “incel”  hate-speech  change  over  time?  \nUse  GPT-4  to  classify  online  texts’  hateful  language.  \nGPT-4  accurately  classifies  hateful  language.  Authors  identify  significant  trends  over  time.  Hewitt  et  Can  LLMs  predict  Used  GPT-4  to  predict  GPT-4  predicted  effect  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  10   \nal.  (2024)  results  of  social  science  studies?  \neffect  sizes  given  stimuli  used  in  studies.  \nsizes  accurately.  \nHorton  (2023)  \nCan  LLMs  simulate  human  economic  behavior?  \nUsed  GPT-3  to  simulate  economic  games  and  a  hiring  scenario.  \nGPT-3  can  qualitatively  replicate  diverse  behaviors.  \n \nTo  sum  up,  although  LLMs  are  a  powerful  new  tool  for  evaluating  human  responses,  they  \nmust\n \nbe\n \nused\n \nin\n \na\n \nway\n \nthat\n \nallows\n \nfor\n \nrobust,\n \nreplicable\n \ninferences.\n \nChoosing\n \nan\n \nLLM\n \nas\n \na\n \nstudy\n \ntool\n \nshould\n \nbe\n \na\n \nconscious\n \nand\n \nreasoned\n \ndecision,\n \nanalogous\n \nto\n \njustifying\n \nthe\n \nuse\n \nof\n \nstatistical\n \nmethods.\n \nAs\n \nwith\n \nany\n \nmethodological\n \nchoice,\n \nresearchers\n \nshould\n \nbegin\n \nby\n \nclearly\n \ndefining\n \ntheir\n \nresearch\n \nquestions,\n \nhypotheses,\n \nanalysis\n \nplans,\n \nand\n \ninterpretation\n \nframeworks,\n \nideally\n \nthrough\n \npreregistration\n \nwhere\n \nfeasible.\n \nCritical\n \nsteps\n \ninclude\n \nensuring\n \nfull\n \ntransparency\n \nby\n \nproviding\n \nall\n \nprompts,\n \ncode,\n \ndata,\n \nmodel\n \nversions,\n \nand\n \nsettings\n \nto\n \nenable\n \nreplication,\n \nas\n \nwell\n \nas\n \naddressing\n \nLLM-specific\n \nchallenges\n \nsuch\n \nas\n \noutput\n \nrandomness,\n \nprompt\n \nsensitivity,\n \nand\n \npotential\n \nbiases.\n \nResearchers\n \nshould\n \njustify\n \ntheir\n \nchoice\n \nof\n \nmodel\n \n(e.g.,\n \nstatic\n \nvs.\n \ncontinuously\n \nupdated\n \nsystems)\n \nand\n \ndisclose\n \nhow\n \nrelated\n \ntechnical\n \nconstraints,\n \nsuch\n \nas\n \nproprietary\n \nmodel\n \nchanges,\n \nmight\n \naffect\n \nconclusions.\n \nEqually\n \nimportant\n \nis\n \nvalidating\n \nresults\n \nagainst\n \nground-truth\n \nhuman\n \ndata\n \nor\n \nestablished\n \nbenchmarks\n \nto\n \nconfirm\n \nreliability.\n \nWhile\n \nnot\n \nall\n \nsteps\n \napply\n \nuniversally\n \n(e.g.,\n \npreregistration\n \ndepends\n \non\n \nstudy\n \ndesign),\n \ncore\n \nrequirements\n \nlike\n \ntransparency,\n \nvalidation,\n \nand\n \nacknowledgment\n \nof\n \nlimitations\n \nare\n \ncritical\n \nfor\n \nscientific\n \nintegrity.\n \nReviewers,\n \nwith\n \ntheir\n \ndomain\n \nexpertise,\n \nplay\n \na\n \ncrucial\n \nrole\n \nin\n \nensuring\n \nthese\n \nrecommendations\n \nare\n \napplied\n \nappropriately\n \nbased\n \non\n \nthe\n \nstudy’s\n \ncontext.\n \nBy\n \nestablishing\n \nthese\n \nguidelines,\n \nwe\n \naim\n \nto\n \nimprove\n \nresearch\n \npractices,\n \nensuring\n \nthat\n \nLLM-driven\n \nstudies\n \nare\n \nboth\n \nreplicable\n \nand\n \nscientifically\n \nsound.\n \nSee\n \nFigure\n \n1\n \nfor\n \na\n \nvisual\n \nroadmap\n \nof\n \nthese\n \nconsiderations\n \nand\n \nsee\n \nTable\n \n3\n \nfor\n \na\n \ndetailed\n \nchecklist\n \nof\n \nthe\n \nrecommendations.\n \n \n \n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  11   \nFigure  1  \nOverview  of  the  key  considerations  for  LLM  research  in  social  science.  \n  Note.  First  row  of  rectangles  denotes  the  key  factors  to  consider  when  conducting  research  in  \nsocial\n \nscience\n \nusing\n \nLLMs.\n \nRed\n \nindicates\n \nrequired\n \nsteps;\n \ngreen\n \nindicates\n \noptional\n \nsteps.\n \nSubsequent\n \nrows\n \nof\n \narrows\n \nindicate\n \nthe\n \ndifferent\n \nsteps\n \nfor\n \nthe\n \nrespective\n \ntopics.\n \nParentheses\n \nmark\n \ncorresponding\n \nsections\n \nin\n \nthe\n \nmanuscript.\n \n \n\nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  12   \nTable  3  Checklist  for  LLM  Research   Category  Checklist  Item  Status  Key  Actions/Considerations  Section  \nPre-  registration  \nAre  the  methods,  including  models,  parameters,  and  validation  strategies,  registered?  \nOptional  Ensure  a  public  record  of  planned  methods/analyses  (e.g.,  OSF,  Github).  \n2.5  \n Does  the  preregistration  allow  for  a  full  understanding  of  the  intended  experimental  design,  data  analysis  plan,  and  how  results  will  be  interpreted?  \nOptional  Confirm  clarity  of  hypotheses,  data-analysis  plan,  and  expected  outcomes.  \n2.5  \n Do  final  methods  deviate  from  the  preregistration?  If  so,  are  deviations  transparent  and  justified?  \nOptional  Document  and  explain  any  changes  (e.g.,  updated  prompt  strategies,  different  LLM  versions).  \n2.5  \nProvide  Replication  Materials  \nIs  code,  prompts,  model  parameters,  fine-tuning  data,  study  material  (e.g.,  questionnaires),  human  validation  data  available?  \nRequired  Supply  a  repository  (GitHub/OSF)  with  all  materials.  \n1.1  \n Does  the  code  run  without  errors,  producing  results  consistent  with  the  paper?  \nRequired  Verify  installation  instructions,  version  control,  and  final  outputs.  \n1.1    \n Discuss  strategies  to  account  for  LLM  randomness  \nRequired  Justify  chosen  strategy  (e.g.,  aggregation  of  multiple  runs)  and  provide  all  details  (e.g.,  report  means  and  standard  deviation,  provide  individual  runs)  \n1.2   \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  13   \nModel  Stability  &  Accessibility  \nDoes  the  applied  model  change  over  time?   \nRequired  If  yes ,  record  exact  name  and  query  date;  justify  necessity  of  using  this  model;  discuss  limitations;  optionally  replicate  key  findings  with  a  stable  model  \n1.3  \n Is  the  applied  model  accessible  for  replication?  \nRequired  If  not ,  justify  use  of  this  model;  optionally  replicate  with  an  accessible  model.  \n1.3  \nValidation  &  Justification  \nAre  LLM  outputs  validated  against  human  data  or  other  ground  truth  where  possible?  \nRequired  Report  accuracy  (e.g.,  correlation  with  human  annotations),  discuss  sufficiency  and  limitations,  comparisons  to  alternative  methods.  \n2.1  \n Does  the  research  question  require  robustness  to  different  prompt  strategies  and  model  settings?  \nRequired  1.  If  yes :  Compare  performance  across  various  prompt  strategies/model  settings;  document  any  differences  and  interpret  their  implications.  2.  If  no :  Clearly  document  and  justify  the  selected  prompt  strategy/settings  (e.g.,  based  on  achieved  accuracy).  \n2.2   &  2.3  \n Is  the  data  processing  and  error  handling  clearly  outlined?  \nRequired  Ensure  all  data  handling  is  transparent;  Explain  and  justify  any  exclusions  or  outliers  \n2.4  \n Is  the  data  processing  and  error  handling  biased  toward  the  desired  outcomes?  \nRequired  Analyze  and  discuss  potential  bias  (e.g.,  correlation  of  exclusion  criteria  and  dependent/independent  variables).  \n2.4  \n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  14   \n1  Ensuring  Replicable  Research  with  LLMs   \n1.1  Transparency  and  Accessibility  of  Materials  \nMost  casual  users  of  LLMs  are  accustomed  to  interacting  with  them  through  a  chat  \ninterface\n \nsuch\n \nas\n \nthe\n \none\n \nprovided\n \nby\n \nOpenAI’s\n \nChatGPT.\n \nWhere\n \nLLMs\n \nare\n \nused\n \nto\n \nprocess\n \nlarge\n \namounts\n \nof\n \ndata\n \nor\n \nwhere\n \nmodel\n \nsettings\n \nmust\n \nbe\n \ncontrolled,\n \na\n \nresearcher\n \nwill\n \ntypically\n \ninteract\n \nwith\n \nthe\n \nLLM\n \nthrough\n \na\n \nprogram.\n \nThese\n \nprograms\n \nfeed\n \ndata\n \nto\n \nthe\n \nLLM\n \nalong\n \nwith\n \nrelevant\n \ninstructions\n \nand\n \nthen\n \nprocess\n \nthe\n \noutput.\n \nProviding\n \nenough\n \ninformation\n \nto\n \nallow\n \nthird\n \nparties\n \nto\n \nreplicate\n \nfindings\n \nis\n \na\n \ncore\n \ntenet\n \nof\n \nresearch\n \ninto\n \nhuman\n \nbehavior\n \nand\n \npsychology.\n \nJust\n \nas\n \nit\n \nis\n \nstandard\n \nfor\n \nother\n \nresearch\n \nto\n \nprovide\n \ndata,\n \ncomputer\n \ncode,\n \ninstructions,\n \nmaterials,\n \nand\n \nprocedures\n \nto\n \nreplicate\n \nfindings,\n \nauthors\n \nusing\n \nLLMs\n \nin\n \ntheir\n \nstudies\n \nmust\n \ndo\n \nthe\n \nsame.\n \nIt\n \nis\n \ncrucial\n \nthat\n \nauthors\n \ndisclose\n \nthe\n \nexact\n \ninput\n \nto\n \nthe\n \nLLM,\n \nas\n \nwell\n \nas\n \nany\n \nmodel\n \nsettings,\n \nenabling\n \na\n \nthorough\n \nevaluation\n \nand\n \nreplication\n \nof\n \nthe\n \nstudy's\n \nmethodology.\n \n Ideally,  authors  provide  an  easy  way  to  replicate  the  reported  results,  e.g.,  by  providing  a  \nprogramming\n \nscript\n \nthat\n \ncombines\n \npre-processing,\n \nLLM\n \nqueries,\n \nand\n \npost-processing\n \nand\n \nby\n \nproviding\n \ninstructions\n \nto\n \nrecreate\n \nthe\n \nauthors’\n \nexact\n \nprogramming\n \nenvironment\n \n(e.g.,\n \nvirtual\n \nenvironments\n \nwith\n \nall\n \nnecessary\n \npackages).\n \nSee,\n \nfor\n \nexample,\n \nthe\n \nrequirements\n \nfor\n \nreplication\n \nby\n \nthe\n \nAssociation\n \nfor\n \nComputational\n \nLinguistics\n \n(e.g.,\n \nin\n \nBoyd-Graber\n \net\n \nal.,\n \n2023b),\n \na\n \nmajor\n \nconference\n \nand\n \npublishing\n \nvenue\n \nin\n \nnatural\n \nlanguage\n \nprocessing.\n \nReviewers\n \nare\n \nasked\n \nto\n \nevaluate\n \nthe\n \nreproducibility\n \nof\n \nsubmitted\n \nworks,\n \nincluding\n \nthe\n \nease\n \nof\n \nreplication.\n \nInsufficient\n \ninstructions\n \nthen\n \nlead\n \nto\n \nlow\n \nscoring\n \nof\n \nthe\n \nsubmissions.\n \n Researchers  developing  or  fine-tuning  LLMs  should  consider  fully  documenting  their  \nmodels\n \nusing\n \nestablished\n \nframeworks\n \nlike\n \nmodel\n \ncards\n \n(Mitchell\n \net\n \nal.,\n \n2019)\n \nand\n \ndatasheets\n \nfor\n \ndatasets\n \nused\n \nin\n \ntraining\n \nand\n \nfine-tuning\n \n(Gebru\n \net\n \nal.,\n \n2021).\n \nThese\n \nframeworks\n \nprovide\n \nstructured\n \nprotocols\n \nfor\n \ndetailing\n \na\n \nmodel’s\n \ndevelopment,\n \ntraining\n \ndata,\n \nintended\n \napplications,\n \nlimitations,\n \nand\n \nethical\n \nconsiderations.\n \nReviewers\n \ncan\n \nconsult\n \nthis\n \ndocumentation\n \nfor\n \na\n \nbetter\n \nunderstanding\n \nof\n \nthe\n \nmodel\n \nused\n \nin\n \nresearch.\n \nWhile\n \nthis\n \nprimer\n \nprimarily\n \nfocuses\n \non\n \nLLM\n \nusage,\n \napplying\n \nthese\n \ndocuments\n \nduring\n \nresearch\n \nand\n \nconsulting\n \nthem\n \nduring\n \nthe\n \nreview\n \nprocess\n \ncan\n \npromote\n \ntransparency,\n \nreplicability,\n \nand\n \nresponsible\n \nreporting,\n \nespecially\n \nfor\n \nnew\n \nor\n \nspecialized\n \nmodels.\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  15   \nIt's  important  to  acknowledge  that  the  feasibility  of  replicating  LLM-based  studies  can  \nvary\n \nsignificantly\n \ndepending\n \non\n \nthe\n \nspecific\n \nresources\n \ninvolved.\n \nFor\n \ninstance,\n \nstudies\n \nrelying\n \non\n \nonline\n \nLLM\n \nservices\n \nlike\n \nChatGPT\n \nincur\n \ncosts\n \nproportional\n \nto\n \nusage,\n \nwhich\n \ncan\n \nbe\n \nsignificant\n \ndepending\n \non\n \nthe\n \nnumber\n \nand\n \ncomplexity\n \nof\n \nqueries.\n \nConversely,\n \nstudies\n \nemploying\n \nlocally\n \nrun\n \nLLMs\n \nmay\n \nnecessitate\n \nspecialized\n \nhardware,\n \npotentially\n \nlimiting\n \naccessibility\n \nfor\n \nreplication.\n \nFor\n \na\n \ngood\n \nexample\n \nof\n \na\n \npaper\n \nthat\n \nprovides\n \nextensive\n \ninstructions\n \nto\n \nreplicate\n \na\n \ncomplex\n \nLLM\n \nstudy\n \ndesign,\n \nsee\n \nPark\n \net\n \nal.\n \n(2023),\n \nwho\n \ndeveloped\n \na\n \nframework\n \nto\n \nsimulate\n \neveryday\n \nhuman\n \nbehavior\n \nusing\n \nLLMs.\n \nThe\n \nauthors\n \nprovide\n \nall\n \ncodes,\n \ndata,\n \nand\n \ninstructions\n \nfor\n \nreplication\n \nin\n \na\n \npublically\n \naccessible\n \nonline\n \nrepository.\n \nIn\n \naddition,\n \nthe\n \nauthors\n \nprovide\n \nthe\n \noriginal\n \nfiles\n \nof\n \nthe\n \nsimulation\n \npresented\n \nin\n \ntheir\n \npaper\n \nwhich\n \nallows\n \nresearchers\n \nto\n \n“rewatch”\n \nthe\n \nsimulation\n \nlike\n \na\n \nvideo\n \n(e.g.,\n \nto\n \nsee\n \nagent\n \ninteractions\n \nand\n \nother\n \ndetails)\n \nwithout\n \nhaving\n \nto\n \npay\n \nfor\n \nLLM\n \nqueries.\n \nA\n \ngood\n \nexample\n \nfor\n \npsychological\n \ntext\n \nanalysis\n \nis\n \nRathje\n \net\n \nal.\n \n(2024),\n \nwho\n \nevaluate\n \nGPT's\n \nperformance\n \nacross\n \nmultiple\n \ntext\n \nanalysis\n \ntasks.\n \nThe\n \nauthors\n \nprovide\n \nall\n \nprompts,\n \nvalidation\n \ndata,\n \nexperimental\n \nresults,\n \nand\n \nreplication\n \ncodes,\n \nincluding\n \ncode\n \nfor\n \ncollecting\n \nGPT\n \nresponses\n \nfor\n \ntwo\n \ntasks,\n \nwith\n \nprompts\n \nfor\n \nthe\n \nremaining\n \ntasks\n \navailable\n \nin\n \nthe\n \nsupplementary\n \nmaterials.\n In  summary,  authors  must  ensure  transparency  and  facilitate  replication  by  providing  \ncomprehensive\n \ndetails\n \nof\n \ntheir\n \nmethods\n \nand\n \nmaterials.\n \nThis\n \nenables\n \nreviewers\n \nto\n \nconfirm\n \nthe\n \nintegrity\n \nand\n \nreplicability\n \nof\n \nthe\n \nresearch.\n \nReviewers\n \nmay\n \nthen,\n \nif\n \nfeasible,\n \nattempt\n \nto\n \nreplicate\n \nthe\n \nresults\n \nusing\n \nthe\n \nmaterials\n \nand\n \nlet\n \nfailure\n \nto\n \nreplicate\n \nthe\n \nresults\n \nor\n \ndeviations\n \ninform\n \ntheir\n \nreview\n \ndecisions.\n \n1.2  Dealing  with  Non-Determinism  \nLLMs  pose  unique  challenges  due  to  their  non-deterministic  nature.  An  LLM  may  code  \na\n \npiece\n \nof\n \ntext\n \ndifferently\n \neach\n \ntime\n \nit\n \nis\n \nasked\n \nto,\n \neven\n \nwhen\n \nusing\n \nthe\n \nsame\n \nprompts\n \nand\n \nmodel\n \nsettings.\n \nFor\n \nexample,\n \nAstekin\n \net\n \nal.\n \n(2024)\n \nprovide\n \na\n \ndetailed\n \ncase\n \nstudy\n \non\n \ninconsistencies\n \nin\n \nan\n \nLLM’s\n \noutputs\n \nwhen\n \nresponding\n \nto\n \nidentical\n \nprompts\n \nwith\n \nthe\n \nsame\n \nmodel\n \nsettings.\n \nThey\n \nobserved\n \nthis\n \ntendency\n \nacross\n \nvarious\n \nLLMs\n \n(e.g.,\n \nGPT,\n \nLLaMA,\n \nClaude).\n \nIn\n \ntheir\n \nstudy,\n \nthe\n \nauthors\n \nrepeatedly\n \nprovided\n \nan\n \nLLM\n \nwith\n \nstatus\n \nmessages\n \n(called\n \n“logs”)\n \nand\n \nasked\n \nthe\n \nmodel\n \nto\n \nprocess\n \nit\n \nin\n \nline\n \nwith\n \na\n \ntemplate.\n \nThey\n \nfound,\n \nacross\n \nall\n \nmodels,\n \ninconsistent\n \noutputs\n \neven\n \nwhen\n \nthe\n \nmodel\n \nparameters\n \nare\n \nset\n \nto\n \nminimal\n \noutput\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  16   \nvariability.  \nOf  course,  not  all  LLM  applications  are  unreliable.  Notably,  Rathje  et  al.  (2024)  report  \na\n \nCohen’s\n \nKappa\n \nof\n \nover\n \n.90\n \nwhen\n \nquerying\n \nChatGPT\n \none-day\n \napart\n \nand\n \nacross\n \nprompts\n \ntranslated\n \ninto\n \ndifferent\n \nlanguages,\n \nshowing\n \nthat\n \nin\n \nsome\n \ncases\n \nLLMs\n \ncan\n \nbe\n \nhighly\n \nreliable\n \nand\n \nrobust.\n \nHowever,\n \nespecially\n \nwith\n \nproprietary\n \nmodels\n \nthat\n \nchange\n \nover\n \ntime,\n \nthere\n \nis\n \nno\n \nguarantee\n \nof\n \nlong\n \nterm\n \nreliability.\n \nResearchers\n \nshould\n \nthus\n \nensure\n \nthat\n \ntheir\n \nfindings\n \nare\n \nreliable\n \nand\n \nnot\n \ncoincidental\n \nat\n \nthe\n \ntime\n \nof\n \ntheir\n \nstudies.\n \n \nTo  improve  reproducibility,  researchers  can  use  “seed”  parameters.  These  parameters  \ngovern\n \nthe\n \nrandom\n \nelements\n \nwithin\n \nthe\n \nLLM's\n \noutput\n \ngeneration\n \nprocess,\n \nensuring\n \nconsistent\n \nresults\n \nacross\n \ndifferent\n \nruns.\n \nIt's\n \nanalogous\n \nto\n \nproviding\n \nthe\n \nLLM\n \nwith\n \na\n \nspecific\n \nstarting\n \npoint\n \non\n \na\n \nmap.\n \nBy\n \nsetting\n \na\n \nseed\n \nparameter,\n \nresearchers\n \nessentially\n \nfix\n \nthis\n \nstarting\n \npoint\n \nso\n \nthat\n \nthe\n \nLLM\n \nwill\n \nfollow\n \nthe\n \nsame\n \nroute\n \nand\n \narrive\n \nat\n \nthe\n \nsame\n \ndestination\n \n(i.e.,\n \nproduce\n \nthe\n \nsame\n \noutput)\n \nevery\n \ntime\n \nit\n \nencounters\n \nthe\n \nsame\n \nprompt\n \nwith\n \nthe\n \nsame\n \nseed.\n \nThis\n \nfunctionality\n \nis\n \ntypically\n \navailable\n \nin\n \nlocally\n \nrun\n \nopen-source\n \nmodels.\n \nRecently,\n \neven\n \nsome\n \nproprietary\n \nmodels\n \nhave\n \nbegun\n \nto\n \nincorporate\n \nit,\n \nas\n \nseen\n \nin\n \nOpenAI's\n \ncurrent\n \ntesting\n \nof\n \nseed\n \nparameters\n \n(see\n \nOpenAI’s\n \nbeta\n \ntesting\n \nof\n \nthe\n \nseed\n \nparameter\n \nin\n \nOpenAI,\n \n2023).\n \nHowever,\n \nreports\n \nsuggest\n \nthis\n \nfunction\n \ndoes\n \nnot\n \nalways\n \nwork\n \nwith\n \nthese\n \nmodels,\n \ndue\n \nto\n \nconstraints\n \nin\n \ntheir\n \narchitecture\n \n(e.g.,\n \nsee\n \na\n \ndiscussion\n \non\n \nhow\n \nthe\n \n“Mixture\n \nof\n \nExperts”\n \narchitecture\n \nimpedes\n \ndeterminism\n \nin\n \nPuigcerver\n \net\n \nal.,\n \n2023).\n \nResearchers\n \nshould\n \nthus\n \nverify\n \nwhether\n \nthey\n \ncan\n \nensure\n \nreliable\n \nreplication\n \nusing\n \nseed\n \nparameters\n \nand\n \nprioritize\n \nmodels\n \nthat\n \ncan.\n \nAlternatively,  and/or  for  models  where  the  user  cannot  set  seed  parameters,  researchers  \nmay\n \nrun\n \nthe\n \nexperiment\n \nrepeatedly\n \nand\n \nreport\n \nand\n \ndiscuss\n \nthe\n \nvariation\n \nin\n \noutcomes.\n \nFor\n \nexample,\n \nwhen\n \nusing\n \nLLMs\n \nto\n \ncode\n \nfree-response\n \ndata,\n \neach\n \nresponse\n \ncan\n \nbe\n \ncoded\n \nby\n \nthe\n \nLLM\n \nmultiple\n \ntimes.\n \nMeans\n \nand\n \nstandard\n \ndeviations\n \ncan\n \nbe\n \nreported\n \nfor\n \nscale\n \nratings\n \nand\n \nmajority\n \nvote\n \nand\n \nclass\n \ndistributions\n \ncan\n \nbe\n \nreported\n \nfor\n \ncategorical\n \nratings.\n \nWhen\n \nusing\n \nLLMs\n \nto\n \nsimulate\n \nhuman\n \ndata,\n \nsimulations\n \nshould\n \nbe\n \nrepeated,\n \nand\n \naggregate\n \nresults\n \nreported\n \n(e.g.,\n \nmeans\n \nand\n \nstandard\n \ndeviations\n \nof\n \nnumeric\n \nresults,\n \npercentage\n \nof\n \nsimulations\n \nshowing\n \na\n \ntarget\n \nbehavior\n \nor\n \nother\n \nnon-numeric\n \nobservations).\n \nIn\n \naddition\n \nto\n \nrepetitions,\n \nresearchers\n \ncan\n \nexplore\n \nadjusting\n \nthe\n \n“temperature”\n \nparameter\n \nto\n \nreduce\n \nvariance\n \nin\n \nLLM\n \noutputs.\n \nLower\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  17   \ntemperature  values  (i.e.,  towards  zero)  tend  to  minimize  variability  (Ouyang  et  al.,  2023).  \nHowever,\n \nit\n \nis\n \nimportant\n \nto\n \nnote\n \nthat\n \neven\n \nat\n \na\n \ntemperature\n \nof\n \n0,\n \nnon-determinism\n \nmay\n \nstill\n \noccur\n \ndue\n \nto\n \nfactors\n \nsuch\n \nas\n \nmodel\n \narchitecture,\n \nrandom\n \nseeds,\n \nor\n \noptimization\n \nand\n \nparallelization\n \n(Monniaux,\n \n2008;\n \nXiao\n \net\n \nal.,\n \n2021;\n \nOuyang\n \net\n \nal.,\n \n2023).\n \nAdjusting\n \nthe\n \ntemperature\n \ncan\n \nalso\n \ninfluence\n \nthe\n \naccuracy\n \nand\n \nquality\n \nof\n \noutputs,\n \nas\n \ndiscussed\n \nin\n \ndetail\n \nin\n \nthe\n \nParameters\n \nsection\n \n(2.3.1)\n \nbelow.\n \nWhen\n \nreviewers\n \nuse\n \nan\n \nauthor’s\n \ncode\n \nto\n \nreplicate\n \nresults,\n \nthey\n \nshould\n \nanticipate\n \nminor\n \nvariation\n \nbetween\n \nthe\n \nreported\n \nand\n \nreplicated\n \nresults\n \n(akin\n \nto\n \nwhat\n \nwould\n \nbe\n \nexpected\n \nfor\n \nbootstrapping\n \nand\n \nother\n \nsimulation-based\n \napproaches).\n \nHowever,\n \nsignificant\n \ndiscrepancies\n \nor\n \nreplication\n \nfailures\n \nshould\n \nreduce\n \nconfidence\n \nin\n \nthe\n \nauthors’\n \nclaims.\n \n \nIn  summary,  authors  should  apply  strategies  to  ensure  the  reliability  of  their  results  in  a  \nmanner\n \nthat\n \nreviewers\n \nand\n \nother\n \nresearchers\n \ncan\n \neasily\n \nverify.\n \nReviewers\n \nshould\n \npay\n \nspecial\n \nattention\n \nto\n \nhow\n \nauthors\n \ndeal\n \nwith\n \nrandomness\n \nin\n \nLLM\n \noutputs\n \nand\n \nhow\n \nthey\n \nmake\n \nsure\n \nthat\n \ntheir\n \nresults\n \nare\n \nnot\n \ncoincidental.\n \nSee\n \nan\n \noverview\n \nof\n \nrecommendations\n \nto\n \nensure\n \nreplicable\n \nLLM\n \nresearch\n \nin\n \nTable\n \n4.\n \n1.3  Dealing  with  Model  Updates  \nProprietary  models  that  are  offered  online  (e.g.,  GPT  by  OpenAI;  Claude  by  Anthropic;  \nGemini\n \nby\n \nGoogle)\n \nare\n \nfrequently\n \nupdated,\n \ncausing\n \nunpredictable\n \nchanges\n \nin\n \nbehavior.\n \nImportantly,\n \nwhile\n \nperformance\n \ntypically\n \nincreases\n \nover\n \ntime,\n \nit\n \ncan\n \ndecrease\n \n(L.\n \nChen\n \net\n \nal.,\n 2023) and  newer  models  might  underperform  in  some  areas  compared  to  previous  versions  (Achiam  et  al.,  2023; Coyne  et  al.,  2023; Rathje  et  al.,  2024). For  example,  Chen  et  al.  (2023)  report  how  the  same  GPT-4  model  showed  a  significant  decline  in  its  ability  to  determine  whether  \na\n \nnumber\n \nis\n \na\n \nprime,\n \ndropping\n \nfrom\n \n84%\n \naccuracy\n \nto\n \n51.1%.\n \nwithin\n \nthree\n \nmonths.\n \nThis\n \nlikely\n \nstemmed\n \nfrom\n \nmodel\n \nupdates\n \nthat\n \naffected\n \nso-called\n \nchain-of-thought\n \n(CoT)\n \nprompts–prompts\n \nthat\n \nelicit\n \na\n \nseries\n \nof\n \nintermediate\n \nreasoning\n \nsteps\n \nwhich\n \nhave\n \nbeen\n \nshown\n \nto\n \nimprove\n \nmodel\n \nperformance\n \non\n \nvarious\n \ntasks\n \n(Wei\n \net\n \nal.,\n \n2022).\n \n \nGPT-4  showed  a  strong  ability  to  reason  through  problems  step-by-step  in  response  to  CoT  \nprompts\n \nin\n \nMarch\n \n2023\n \nbut\n \nsignificantly\n \nless\n \nso\n \nin\n \nJune.\n \nThis\n \nwas\n \nalso\n \naccompanied\n \nby\n \na\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  18   \nsignificant  decrease  in  verbosity  of  outputs  in  the  evaluated  experiments  with  the  average  output  \nlength\n \ndropping\n \nfrom\n \n638\n \ncharacters\n \nto\n \n3.9\n \ncharacters.\n \nThe\n \nmodel\n \nupdates\n \ncould\n \nhave\n \nintended\n \nto\n \nreduce\n \noverly\n \nlong\n \nand\n \nexplanatory\n \nresponses\n \n(e.g.\n \ndue\n \nto\n \nuser\n \nfeedback\n \nor\n \nto\n \nreduce\n \noperational\n \ncosts\n \nas\n \noperating\n \ncosts\n \nscale\n \nwith\n \nthe\n \nnumber\n \nof\n \ngenerated\n \nwords).\n \nSee\n \nthe\n \nfollowing\n \nillustrative\n \nexample\n \nfrom\n \nChen\n \net\n \nal.\n \n(2023):\n \nIn\n \nMarch\n \n2023\n \nGPT4\n \nresponded\n \nto\n \nthe\n \nquestion\n \n“\nQ:\n \nIs\n \n17077\n \na\n \nprime\n \nnumber?\n \nThink\n \nstep\n \nby\n \nstep\n \nand\n \nthen\n \nanswer\n \n\"[Yes]\"\n \nor\n \n\"[No]\"\n \nwith:\n \n“ Step  1:  Check  if  the  number  is  even.  If  it  is,  then  it  is  not  prime  (except  for  2).  17077  is  odd,  so  \nwe\n \nproceed\n \nto\n \nthe\n \nnext\n \nstep.\n \nStep\n \n2:\n \nFind\n \nthe\n \nsquare\n \nroot\n \nof\n \nthe\n \nnumber.\n \nThe\n \nsquare\n \nroot\n \nof\n \n17077\n \nis\n \napproximately\n \n130.68.\n \nStep\n \n3:\n \nCheck\n \nif\n \nthe\n \nnumber\n \nis\n \ndivisible\n \nby\n \nany\n \nprime\n \nnumbers\n \nless\n \nthan\n \nor\n \nequal\n \nto\n \nthe\n \nsquare\n \nroot.\n \nThe\n \nprime\n \nnumbers\n \nless\n \nthan\n \nor\n \nequal\n \nto\n \n130.68\n \nare\n \n2,\n \n3,\n \n5,\n \n7,\n \n11,\n \n13,\n \n17,\n \n19,\n \n23,\n \n29,\n \n31,\n \n37,\n \n41,\n \n43,\n \n47,\n \n53,\n \n59,\n \n61,\n \n67,\n \n71,\n \n73,\n \n79,\n \n83,\n \n89,\n \n97,\n \n101,\n \n103,\n \n107,\n \n109,\n \n113,\n \n127.\n \nStep\n \n4:\n \nTest\n \neach\n \nof\n \nthese\n \nprime\n \nnumbers\n \nas\n \na\n \ndivisor.\n \nIf\n \nany\n \nof\n \nthem\n \ndivide\n \n17077\n \nwithout\n \na\n \nremainder,\n \nthen\n \n17077\n \nis\n \nnot\n \nprime.\n \nIf\n \nnone\n \nof\n \nthem\n \ndivide\n \n17077\n \nwithout\n \na\n \nremainder,\n \nthen\n \n17077\n \nis\n \nprime.\n \n17077\n \nis\n \nnot\n \ndivisible\n \nby\n \nany\n \nof\n \nthe\n \nprime\n \nnumbers\n \nlisted\n \nabove\n \nwithout\n \na\n \nremainder.\n \nTherefore,\n \n17077\n \nis\n \na\n \nprime\n \nnumber.\n \nAnswer:\n \n[Yes]\n”.\n \nIn  June  2023  it  simply  replied  with  “ No ”.  Similar  patterns  of  stark  drops  in  performance  were  \nobserved\n \nfor\n \ncode\n \ngeneration\n \nand\n \ninstruction\n \nfollowing.\n \nIt\n \ncould\n \nbe\n \nthat\n \nmodel\n \nupdates\n \nintended\n \nto\n \nfix\n \none\n \nissue,\n \nsuch\n \nas\n \nmaking\n \na\n \ntoo\n \nverbose\n \nmodel\n \nmore\n \nconcise\n \nor\n \na\n \ntoo\n \nconcise\n \nmodel\n \nmore\n \nverbose,\n \naccidentally\n \naffected\n \nperformance\n \non\n \nvarious\n \ntasks.\n \n \nRegarding  performance  across  model  versions,  Rathje  et  al.  (2024)  showed  that  on  some  \nsentiment\n \nanalysis\n \nor\n \noffensive\n \nlanguage\n \ndetection\n \ntasks,\n \nGPT-3.5\n \noutperformed\n \nthe\n \nnewer\n \nand\n \nlarger\n \nGPT-4\n \nor\n \nGPT-4-Turbo\n \nmodels.\n \nIn\n \nsome\n \ncases,\n \nsuch\n \nas\n \nsentiment\n \nanalysis\n \nin\n \nHausa\n \nand\n \nSwahili,\n \nthe\n \nperformance\n \ndropped\n \nsignificantly\n \nfrom\n \nan\n \nF\n1\n \nscore\n \nof\n \n0.59\n \nand\n \n0.56\n \nto\n \n0.399\n \nand\n \n0.488\n \nrespectively.\n  \nThe\n \nnewer\n \nGPT-4-Turbo\n \nalso\n \nperformed\n \nsignificantly\n \nworse\n \non\n \nsome\n \ntasks\n \nthan\n \nthe\n \nolder\n \nGPT-4\n \nand\n \nvice\n \nversa\n \non\n \nother\n \ntasks,\n \nmaking\n \nit\n \ndifficult\n \nto\n \nchoose\n \na\n \nspecific\n \nmodel\n \nwithout\n \ntesting\n \nit\n \non\n \na\n \ngiven\n \ntask\n \nfirst.\n \nCoyne\n \net\n \nal.\n \n(2023)\n \ntested\n \nGPT-3.5\n \nand\n \nGPT-4’s\n \nability\n \nto\n \ncorrect\n \ngrammatical\n \nerrors\n \nin\n \ntexts.\n \nThey\n \nfound\n \nthat\n \nacross\n \nmultiple\n \nprompt\n \nstyles\n \nand\n \nmodel\n \nsettings,\n \nGPT-3.5\n \noutperformed\n \nGPT-4.\n \nIn\n \nsome\n \ncases,\n \nGPT-4\n \nshowed\n \n“over-editing”\n \nwhere\n \nthe\n \nedits\n \nwent\n \nbeyond\n \ncorrecting\n \ngrammar\n \nand\n \nchanged\n \nthe\n \nmeaning\n \nof\n \na\n \nsentence,\n \nsuch\n \nas\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  19   \n(incorrectly)  expanding  the  erroneous  sentence  “If  the  film  doesn’t  arrive  on  time,  it  immediately”  \nto\n \n“If\n \nthe\n \nfilm\n \ndoesn’t\n \narrive\n \non\n \ntime,\n \nit\n \nwill\n \nbe\n \nshown\n \nimmediately.”,\n \ncompared\n \nto\n \nGPT-3.5’s\n \n“If\n \nthe\n \nfilm\n \ndoesn’t\n \narrive\n \non\n \ntime,\n \nit\n \nwill\n \nbe\n \ncancelled\n \nimmediately.”\n).\n \n \nThese  issues  may  likely  be  fixed  over  time,  but  there  is  no  guarantee  that  previously  \nestablished\n \nperformance\n \nor\n \ncapabilities\n \nstill\n \nhold\n \nnor\n \nthat\n \nnewer\n \nversions\n \nare\n \nsuperior\n \n(i.e.,\n \nnot\n \nestablishing\n \nsuperior\n \n“default\n \nmodels”).\n \nAdditionally,\n \nowners\n \nof\n \nthese\n \nmodels\n \nare\n \nnot\n \nrequired\n \nto\n \nand\n \ntypically\n \ndo\n \nnot\n \ndisclose\n \nall\n \nchanges\n \nor\n \nprovide\n \nlong-term\n \naccess\n \nto\n \nprevious\n \nversions.\n \nResults\n \nobtained\n \nfrom\n \nOpenAI’s\n \nGPT-4\n \nwhen\n \nan\n \narticle\n \nis\n \nsubmitted\n \nmay\n \nthus\n \nnot\n \nreplicate\n \nwith\n \nGPT-4\n \nmonths\n \nlater\n \nwhen\n \nit\n \ngoes\n \nto\n \npress,\n \nmaking\n \nresearch\n \nusing\n \nthese\n \nmodels\n \ndifficult\n \nto\n \nbuild\n \nupon.\n \nTable  4  \nRecommendations  to  ensure  replicable  LLM  research.  \nPotential  Issue  Strategy  Details  Social  Science  Analogy  \nInaccessibility/  Intransparency  \nFull  access  to  materials  and  documentation  \nProvide  all  data,  code,  experimental  instructions,  and  model  settings  to  replicate  the  study  findings.  \nProviding  all  data  and  experimental  methods/procedures  in  a  behavioral  study.  \nNon-Determinism  Multiple  codings  and  Simulations;  use  seed  parameters  \nPrompt  LLMs  repeatedly.  Reports  should  include,  e.g.,  means  and  standard  deviations,  or  majority  votes  to  showcase  consistency  or  variability.  Use  seed  parameters  for  reproducible  outputs.  \nCoding  participants’  self-reported  stress  levels  multiple  times  (e.g.,  multiple  annotators)  to  ensure  consistent  categorization.  \nModel  Updates  Document  model  version,  use  stable  models,  or  use  local  open-source  models  \nSpecify  exact  LLM  version  and  query  date.  Prioritize  stable  local  open-source  models  that  can  be  shared  with  others.   \nA  validated  scale  (e.g.,  a  depression  inventory)  is  revised  to  a  new  version.  The  same  participant  responses  might  be  scored  differently,  changing  the  final  outcomes.  \nTo  ensure  replicability,  authors  should  prioritize  models  that  are  stable  over  time  and  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  20   \npermanently  available.  Open-source  models\n 2\n,  which  meet  these  criteria,  are  increasingly  available.  \nExamples\n \ninclude\n \nMeta’s\n \nLLaMA\n \nfamily\n \nof\n \nmodels\n \n(Touvron\n \net\n \nal.,\n \n2023),\n \nthe\n \npen-science-driven\n \nBLOOM\n \n(Le\n \nScao\n \net\n \nal.,\n \n2023),\n \nMistral-AI’s\n \nmodels\n \n(e.g.,\n \nJiang\n \net\n \nal.,\n \n2023),\n \nand,\n \nmore\n \nrecently,\n \nDeepSeek’s\n \nhigh-performing\n \nV3\n \nand\n \nR1\n \nmodels\n \n(Liu\n \net\n \nal.,\n \n2024;\n \nGuo\n \net\n \nal.,\n \n2025).\n \nOpen-source\n \nmodels\n \ngive\n \nresearchers\n \nmore\n \ncontrol\n \nand\n \nallow\n \nfor\n \nreproducibility\n \nin\n \na\n \nway\n \nthat\n \nproprietary\n \nmodels\n \ndo\n \nnot.\n \nA\n \nresearcher\n \ncan\n \ndownload\n \nan\n \nopen-source\n \nmodel’s\n \nweights\n \n(see\n \nTable\n \n1.8)\n \nand\n \nmake\n \nit\n \navailable\n \nfor\n \nvalidation\n \nand\n \nreplication,\n \nsomething\n \nthat\n \nis\n \nimpossible\n \nwith\n \nproprietary\n \nmodels.\n \nFor\n \nexample,\n \nif\n \nan\n \nLLM\n \nis\n \nused\n \nfor\n \ncoding\n \ntext,\n \nproviding\n \nmodel\n \nweights\n \nis\n \nanalogous\n \nto\n \nproviding\n \na\n \ndetailed\n \ncoding\n \nmanual\n \nfor\n \nhuman\n \nraters.\n \n \nAdditionally,  some  companies,  such  as  Mistral  or  Hugging  Face,  offer  Application  \nProgramming\n \nInterfaces\n \n(APIs)\n \nthat\n \nallow\n \nusers\n \nto\n \ninteract\n \nwith\n \nopen-source\n \nmodels\n \nvia\n \ncode,\n \noften\n \nat\n \nlower\n \ncosts\n \nthan\n \nproprietary\n \nmodels.\n \nThis\n \nincreases\n \naccessibility\n \nfor\n \nresearchers\n \nwho\n \ncannot\n \nset\n \nup\n \nmodels\n \nthemselves.\n \nIf\n \nauthors\n \nuse\n \nopen-source\n \nmodels,\n \nthey\n \nshould\n \nshare\n \nthe\n \nmodel\n \nweights\n \n(or\n \nlink\n \nto\n \nplatforms\n \nsuch\n \nas\n \nHuggingFace\n \nthat\n \nstore\n \nthese\n \nweights)\n \nin\n \naddition\n \nto\n \ntheir\n \ncode\n \nand\n \nall\n \nmodel\n \nsettings.\n \n \nOpen  models  vary  in  their  power,  specialize  in  different  tasks,  and  have  different  safety  \ncontrols.\n \nMany\n \nof\n \nthese\n \nmodels\n \ncompete\n \nwith\n \nstate-of-the-art\n \nproprietary\n \nmodels\n \nsuch\n \nas\n \nGPT-3.5,\n \nGPT-4,\n \nor\n \no1.\n \nNote\n \nthat\n \nthe\n \nperformance\n \nlandscape\n \nfor\n \nLLMs\n \nis\n \ncontinuously\n \nevolving,\n \nas\n \nnew\n \nmodels\n \nare\n \ndeveloped\n \nand\n \nexisting\n \nones\n \nare\n \nrefined.\n \nCurrent\n \nperformance\n \nbenchmarks\n \nfor\n \nboth\n \nopen\n \nand\n \nclosed\n \nmodels\n \nare\n \navailable\n \non\n \nplatforms\n \nsuch\n \nas\n \nHuggingFace’s\n \nChatbot\n \nArena\n (LMSYS,  2024) and  Open  LLM  Leaderboard  (Huggingface,  2024a)\n3\n.  As  of  now,  proprietary  models  have  an  edge  in  user  adoption  due  to  their  convenience  and  ease  of  use  through  various  \ncommercial\n \nservices\n \n(e.g.,\n \nchat\n \ninterfaces,\n \nAPIs)\n \nthat\n \nallow\n \nusers\n \nto\n \nutilize\n \nthese\n \nmodels\n \nwithout\n \nmuch\n \ntechnical\n \nknowledge.\n \n \nIn  some  cases,  authors  may  prefer  proprietary  models  for  their  superior  capabilities  or  to  \nexplore\n \nspecific\n \nfeatures\n \nunique\n \nto\n \nthese\n \nmodels.\n \nIn\n \nthese\n \ncases,\n \nauthors\n \nshould\n \njustify\n \nthe\n \n3\n \nThese\n \nplatforms\n \nbenchmark\n \nlanguage\n \nmodels\n \non\n \nvarious\n \ntasks,\n \nakin\n \nto\n \nstandardized\n \ntesting\n \nin\n \npsychology.\n \n2\n Many  “open”  AI  models  provide  publicly  available  weights,  enabling  local  use  and  investigation  of  their  internal  \nworkings,\n \nbut\n \nnot\n \nfull\n \ntraining\n \ncode\n \nor\n \ndata—limiting\n \nreplication\n \nof\n \nthe\n \nmodel’s\n \ncreation.\n \nSince\n \nthis\n \nprimer\n \nfocuses\n \non\n \nrunning\n \nrather\n \nthan\n \ntraining\n \nmodels,\n \nwe\n \ntreat\n \nthese\n \n“open-weight”\n \nmodels\n \nas\n \nopen-source.\n \nFor\n \na\n \nmore\n \nnuanced\n \ndiscussion\n \nof\n \nLLM\n \n\"openness,\"\n \nsee\n \nLiesenfeld\n \net\n \nal.\n \n(2023).\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  21   \ntrade-off  between  higher  performance  and  issues  with  replication.  Reviewers  should  weigh  this  \njustification\n \nin\n \ntheir\n \nevaluation\n \nof\n \nthe\n \npaper.\n \nWhen\n \nusing\n \ncurrent\n \nproprietary\n \nmodels,\n \nit\n \nis\n \nadvisable\n \nto\n \nchoose\n \nmodel\n \nversions\n \nthat\n \nare\n \nstable\n \nover\n \ntime.\n \nOpenAI,\n \nfor\n \nexample,\n \narchives\n \n“snapshots”\n \nof\n \nmodel\n \nversions,\n \nbut\n \nlong-term\n \navailability\n \nand\n \nstability\n \nof\n \nthese\n \nsnapshots\n \nare\n \nuncertain.\n \nIf\n \na\n \nstable\n \nmodel\n \nwasn’t\n \nused,\n \nauthors\n \nshould\n \nreplicate\n \nwith\n \nthe\n \ncurrent\n \nversion\n \nof\n \nthe\n \nmodel\n \nor\n \na\n \ndifferent\n \n(stable)\n \nmodel.\n \nAuthors\n \nshould\n \nfurthermore\n \ndisclose\n \nall\n \ndetails\n \nabout\n \nthe\n \nused\n \nmodel\n \nversions.\n \nFor\n \nexample,\n \ninstead\n \nof\n \nonly\n \nreporting\n \nthe\n \nmodel\n \nas\n \n“gpt-4-turbo”,\n \nthe\n \nfull\n \nname\n \nsuch\n \nas\n \n“gpt-4-0125-preview”\n \nor\n \n“gpt-4-1106-preview”\n \nand\n \nthe\n \ntime\n \nof\n \naccessing\n \nthe\n \nmodel\n \nshould\n \nbe\n \nreported\n \n(analogue\n \nto\n \nproviding\n \nthe\n \nedition\n \nof\n \na\n \npsychological\n \nscale).\n \nDue\n \nto\n \nthe\n \nlack\n \nof\n \nguaranteed\n \nlong-term\n \navailability,\n \nthis\n \nwill\n \nlikely\n \nnot\n \nlead\n \nto\n \nlong-term\n \nreproducibility,\n \nbut\n \nit\n \nwill\n \nhelp\n \nwith\n \ntracing\n \nchanges\n \nin\n \nperformance\n \nor\n \nmodel\n \ncapability\n \nacross\n \ntime.\n \nIn  summary,  authors  should  provide  a  statement  regarding  replicability,  justifying  the  \nchoice\n \nof\n \nmodel,\n \nincluding\n \npotential\n \nlong-term\n \nchanges\n \nto\n \nthe\n \nmodels,\n \nallowing\n \nreviewers\n \nand\n \nreaders\n \nto\n \nassess\n \nthe\n \nreliability\n \nand\n \nrobustness\n \nof\n \nthe\n \nfindings.\n \n \n 2  Validating  Research  that  Utilizes  LLMs  \nIn  all  cases,  authors  should  explain  how  they  assessed  the  reliability  and  validity  of  an  \nLLM’s\n \nresponses.\n \nResearchers\n \nhave\n \nmany\n \ndegrees\n \nof\n \nfreedom\n \nwhen\n \ngenerating\n \nLLM\n \noutputs.\n \nAuthors\n \nshould\n \nthus\n \nexplain\n \nimportant\n \nchoices\n \nregarding\n \ncomputer\n \ncode\n \nand\n \ndata\n \nused\n \n(e.g.,\n \nmodel\n \nsettings,\n \ndata\n \nprocessing)\n \nto\n \ngenerate\n \nresponses.\n \nIn\n \nthe\n \nfollowing,\n \nwe\n \npresent\n \nsome\n \nkey\n \nconsiderations\n \nto\n \nvalidate\n \nand\n \nensure\n \nthe\n \nrobustness\n \nof\n \nLLM\n \nfindings.\n \nSee\n \nan\n \noverview\n \nof\n \nthe\n \nrecommendations\n \nand\n \nappropriate\n \nstrategies\n \nin\n \nTable\n \n5.\n \n2.1  Confirming  LLM  Outputs  with  Human  Data  \nResearchers  utilizing  LLMs  for  text  classification  should  validate  LLM  responses  to  \nensure\n \naccuracy\n \nand\n \nmitigate\n \npotential\n \nbiases\n \nin\n \ntheir\n \nwork.\n \nThis\n \ncan\n \nbe\n \ndone\n \nin\n \neither\n \nof\n \ntwo\n \nways.\n \nFirst,\n \nauthors\n \ncould\n \nrefer\n \nto\n \npast\n \nliterature\n \nthat\n \nvalidated\n \na\n \nmodel’s\n \nperformance\n \non\n \na\n \ngiven\n \ntask.\n \nIt\n \nshould\n \nbe\n \nnoted\n \nthat\n \nmost\n \nof\n \nthe\n \ncommon\n \nproprietary\n \nmodels\n \nchange\n \nover\n \ntime,\n \nso\n \nit\n \ncannot\n \nbe\n \nguaranteed\n \nthat\n \nthe\n \nperformance\n \nof\n \nthe\n \nmodel\n \nhas\n \nnot\n \nchanged\n \nin\n \nthe\n \nmeantime.\n \nFurthermore,\n \nthe\n \nmodel\n \nwould\n \nneed\n \nto\n \nbe\n \nthoroughly\n \nvalidated\n \n(e.g.,\n \nacross\n \ndomains,\n \ntype\n \nof\n \ntexts,\n \nsources)\n \nto\n \nmake\n \nsure\n \nthat\n \nthe\n \nperformance\n \nwill\n \nhold\n \nin\n \na\n \ngiven\n \nresearch\n \ncontext.\n \nFor\n \nnow,\n \nthere\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  22   \nwill  be  few  cases  where  a  model  is  both  stable  over  time  and  thoroughly  validated  so  that  its  \nperformance\n \ncan\n \nsimply\n \nbe\n \nassumed\n \nwithout\n \nany\n \ntest\n \nby\n \nthe\n \nauthors.\n \nHowever,\n \nthis\n \nmight\n \nchange\n \nas\n \nLLMs\n \nimprove,\n \nas\n \nmore\n \nresearch\n \nand\n \npractical\n \napplication\n \nshifts\n \nto\n \nopen-source\n \nmodels\n \n(or\n \nas\n \nproprietary\n \nmodels\n \nprovide\n \npermanently\n \nstable\n \nversions),\n \nand\n \nas\n \nmore\n \nvalidation\n \nstudies\n \nare\n \npublished.\n \n \nSecond,  where  direct  past  validation  is  lacking,  authors  should  validate  the  model’s  \nperformance\n \nfor\n \ntheir\n \nrespective\n \nstudies.\n \nSimilar\n \nto\n \nreporting\n \ninterrater\n \nreliability\n \nacross\n \nhuman\n \nraters,\n \ncoding\n \ntasks\n \nusing\n \nLLMs\n \nshould\n \nreport\n \naccuracy\n \nby\n \ncomparing\n \nLLM\n \nand\n \nhuman\n \ncoding\n \nresponses\n \n(or\n \nother\n \nground-truth\n \ndata)\n \nfor\n \na\n \nsubsample\n \nof\n \nresponses.\n \nFor\n \nexample,\n \nan\n \nauthor\n \nmight\n \nhave\n \nan\n \nLLM\n \nand\n \nthree\n \nhuman\n \nraters\n \ncode\n \nfree\n \nresponse\n \ndata\n \nfrom\n \n100\n \nout\n \nof\n \n1000\n \nparticipant\n \nresponses.\n \nIf\n \nsufficiently\n \nhigh\n \naccuracy\n \nis\n \nreached,\n \nthe\n \nauthor\n \nmay\n \nthen\n \ncode\n \nthe\n \nremaining\n \n900\n \nresponses\n \nusing\n \nonly\n \nthe\n \nLLM.\n \nSee\n \nfor\n \nexample,\n \nMatter\n \net\n \nal.\n \n(2024)\n \nwho\n \nused\n \nan\n \nLLM\n \nto\n \ndetect\n \nhateful\n \nlanguage\n \nin\n \na\n \nlarge\n \ncorpus\n \nof\n \nsocial\n \nmedia\n \nposts.\n \nThey\n \nfirst\n \nmanually\n \nannotated\n \nroughly\n \n3,000\n \nposts\n \nto\n \ntest\n \nthe\n \nLLMs\n \nperformance\n \non\n \ndetecting\n \nhateful\n \nlanguage\n \non\n \nthis\n \nsubset\n \n(i.e.,\n \ncomparing\n \nthe\n \nLLM\n \nannotations\n \nwith\n \nthe\n \nhuman\n \nannotators).\n \nAfter\n \ndetecting\n \na\n \nhigh\n \nperformance\n \nand\n \nagreement\n \nwith\n \nthe\n \nhuman\n \nannotators\n \non\n \nthis\n \nsubset,\n \nthey\n \napplied\n \nthe\n \nLLM\n \non\n \nthe\n \nremaining\n \n45,611\n \nposts\n \nto\n \nuse\n \nin\n \ntheir\n \nanalyses.\n \nIt\n \nshould\n \nbe\n \nnoted\n \nthat\n \nthey\n \nalso\n \nused\n \nthe\n \nmanually\n \nannotated\n \nsubsample\n \nto\n \ncompare\n \nthe\n \nperformance\n \nof\n \ndifferent\n \nLLMs,\n \nagreement\n \nbetween\n \ndifferent\n \nhuman\n \nannotators\n \nand\n \nLLM\n \nmodels,\n \nthe\n \neffect\n \nof\n \nmodel\n \nsettings,\n \nand\n \nthe\n \nefficacy\n \nof\n \ndifferent\n \nprompt\n \ndesigns.\n \nConsidering  that  researchers  have  substantial  flexibility  in  selecting  the  subsample  to  \nevaluate\n \nthe\n \nmodel’s\n \nperformance,\n \nauthors\n \nshould\n \nideally\n \nmake\n \nthese\n \ncomparisons\n \non\n \na\n \npre-defined\n \nand\n \njustified\n \nsubsample\n \nof\n \nthe\n \ndata\n \n(e.g.,\n \nrandom\n \nor\n \nstratified\n \nby\n \nrelevant\n \ngrouping\n \nvariables)\n \nto\n \nguard\n \nagainst\n \nresearchers\n \noversampling\n \nresponses\n \nwhere\n \nhuman\n \nand\n \nLLM\n \ncoding\n \nmatch\n \n(analogous\n \nto\n \n“p-hacking”\n \nwhere\n \nresearchers\n \nrun\n \nmultiple\n \nanalyses\n \nand\n \nonly\n \nreport\n \nsignificant\n \nones;\n \nSimmons\n \net\n \nal.\n \n2011).\n \n \nIdeally,  authors  should  also  investigate  whether  models  are  biased  in  important  ways  by  \nseeing\n \nwhether\n \nfalse\n \nclassifications\n \nby\n \nthe\n \nmodel\n \ncorrelate\n \nwith\n \nrelevant\n \nfeatures\n \nof\n \nthe\n \ntext\n \nor\n \ntask\n \n(e.g.,\n \nruling\n \nout\n \nthat\n \ntweets\n \nby\n \nwomen,\n \ncompared\n \nto\n \nmen,\n \nare\n \nmore\n \nfrequently\n \nmisclassified\n \nin\n \na\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  23   \nstudy  that  investigates  gender  differences).  An  example  of  this  bias  was  demonstrated  by  \nHutchinson\n \net\n \nal.\n \n(2020),\n \nwho\n \nexamined\n \nhow\n \nLLMs\n \nclassified\n \ntexts\n \nrelated\n \nto\n \ndisability\n \nand\n \ndisabled\n \nindividuals.\n \nThey\n \nfound\n \nthat\n \nthese\n \ntexts\n \nwere\n \ngenerally\n \nclassified\n \nas\n \nnegative\n \nand\n \ntoxic\n \nsimply\n \nbecause\n \nthey\n \nwere\n \nabout\n \ndisability,\n \nregardless\n \nof\n \nthe\n \nactual\n \ntoxicity\n \nor\n \nsentiment\n \nof\n \nthe\n \ncontent.\n \nFor\n \ninstance,\n \nthe\n \nsentence\n \n'I\n \nam\n \na\n \nperson\n \nwith\n \nmental\n \nillness'\n \nwas\n \nrated\n \nas\n \ntoxic\n \n(0.62\n \non\n \na\n \nscale\n \nfrom\n \n0-1),\n \ncompared\n \nto\n \nthe\n \nsimilar\n \nsentence\n \n'I\n \nam\n \na\n \ntall\n \nperson,'\n \nwhich\n \nwas\n \nrated\n \nnon-toxic\n \n(0.03).\n \nIf\n \na\n \nresearcher\n \nused\n \nthis\n \nmodel\n \nto\n \nstudy\n \nperceptions\n \nof\n \ndisability\n \nor\n \nhow\n \nthe\n \ndisabled\n \ncommunity\n \ndiscusses\n \nrelevant\n \nissues,\n \nconfounding\n \nthe\n \nindependent\n \nvariable\n \n(e.g.,\n \nconversation\n \ntopic)\n \nwith\n \nthe\n \noutcome\n \n(e.g.,\n \ntoxicity\n \nor\n \nsentiment)\n \ncould\n \nlead\n \nto\n \nmisrepresentations\n \nand\n \npotentially\n \nresult\n \nin\n \nmisinformed\n \ninferences\n \nand\n \npolicies.\n \nThus,\n \nvalidating\n \nthe\n \nabsence\n \nof\n \nrelevant\n \nsystematic\n \nbiases\n \nin\n \nthe\n \noutputs\n \nof\n \nany\n \nmodel,\n \nwhether\n \nit\n \nbe\n \na\n \ngenerative\n \nLLM,\n \nautoencoding\n \nmodel,\n \nor\n \na\n \nclassical\n \nmachine\n \nlearning\n \nmodel\n \nlike\n \nKNN\n \n(Fix\n \n&\n \nHodges,\n \n1951),\n \nis\n \ncrucial\n \nto\n \nensure\n \naccurate\n \nand\n \nreliable\n \nresults.\n \nIn  summary,  authors  should  validate  their  model’s  accuracy  and,  ideally,  ensure  its  biases  \ndo\n \nnot\n \nsystematically\n \nskew\n \ntheir\n \nresults\n \nand\n \nsubsequent\n \ninferences.\n \nValidation\n \ncan\n \nbe\n \ndone\n \nby\n \ncomparing\n \nits\n \noutputs\n \nwith\n \nhuman\n \n(e.g.,\n \njudgments,\n \nreactions,\n \nopinions)\n \nor\n \nother\n \nground-truth\n \ndata\n \n(e.g.,\n \nphysical\n \nmeasurements,\n \ncorrect\n \nanswers\n \nin\n \ntests),\n \nreferring\n \nto\n \nrecent\n \nliterature\n \non\n \nthe\n \nsame\n \ntasks,\n \nand\n \nthoroughly\n \nexamining\n \nmodel\n \nchoice\n \nand\n \napplication.\n \nReviewers\n \nshould\n \nthen\n \ntake\n \nthe\n \nvalidation\n \nof\n \nresults\n \nand\n \ntheir\n \ndiscussions\n \ninto\n \naccount\n \nwhen\n \nevaluating\n \nLLM\n \nresearch.\n \n2.2  Robustness  of  Prompts  \n One  of  the  most  fundamental  choices  when  using  an  LLM  is  how  a  researcher  instructs  it  \nto\n \ngenerate\n \nthe\n \noutput\n \n(i.e.,\n \nprompt\n \nthe\n \nmodel).\n \nLLMs,\n \nlike\n \nhumans,\n \ncan\n \nalter\n \ntheir\n \nresponses\n based  on  prompt  wording  (Sclar  et  al.,  2024;  Abdurahman  et  al.,  2024; Fujita  et  al.,  2022; Lu  et  al.,  2022). For  example,  Sclar  et  al.  (2024)  presented  a  detailed  investigation  of  LLM's  prompt-sensitivity  to  spurious  prompt  features  across  various  tasks.  They  found  stark  differences  \nin\n \nperformance\n \nacross\n \narbitrary\n \n(i.e.,\n \nmeaning\n \npreserving)\n \nprompt\n \ndesign\n \nchoices,\n \nsuch\n \nas\n \ndifferent\n \nseparators\n \nor\n \nspacing\n \nin\n \nan\n \noutput\n \ntemplate.\n \nFor\n \nexample,\n \ninstructing\n \nthe\n \nmodel\n \nto\n \noutput\n \nresults\n \nin\n \nthe\n \nfollowing\n \nformat:\n “Passage:<text>  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  24   \n Answer:<text>”,  instead  of:  \n“\nPassage\n \n<text>\n \nAnswer\n \n<text>”\n \nled\n \nto\n \na\n \n76\n \npercentage-point\n \ndecrease\n \nin\n \naccuracy.\n \nAdditionally,  LLMs  can  learn  from  examples  included  in  a  prompt  through  in-context  \nlearning.\n \nThis\n \napproach\n \nis\n \ndifferentiated\n \ninto\n \nzero-shot,\n \nfew-shot,\n \nor\n \nmany-shot\n \nlearning\n \n(see\n \nTable\n \n1.9)\n \nbased\n \non\n \nthe\n \nnumber\n \nof\n \nexamples,\n \nand\n \nis\n \noften\n \nemployed\n \nby\n \nresearchers\n \nto\n \nincrease\n model  performance  (Brown  et  al.,  2020; Wang  et  al.,  2020). However,  in-context  learning  introduces  yet  another  degree  of  freedom  because  the  choice  of  examples,  and  even  their  order,  can  change  the  model’s  outputs  (Lu  et  al.,  2022). For  example,  Lu  et  al.  (2022)  prompted  models  to  classify  the  sentiment  of  movie  reviews  given  example  classification  of  other  reviews.  They  \nfound\n \nthat\n \nsimply\n \nchanging\n \nthe\n \norder\n \nof\n \nexamples\n \nfrom\n \ne.g.,\n \n“Review:\n \nthe\n \ngreatest\n \nmusicians.\n \nSentiment:\n \npositive.\n \nReview:\n \nredundant\n \nconcept.\n \nSentiment:\n \nnegative“\n \nto\n \n“Review:\n \nredundant\n \nconcept.\n \nSentiment:\n \nnegative.\n \nReview:\n \nthe\n \ngreatest\n \nmusicians.\n \nSentiment:\n \npositive”\n \naffected\n \nperformance,\n \nwith\n \ndifferent\n \nmodels\n \nhaving\n \ndifferent\n \nideal\n \nexample\n \norders.\n \n \nLoya  et  al.  (2023)  investigated  the  impact  of  reasoning  strategies,  like  chain-of-thought  \nreasoning,\n \nin\n \na\n \nreward-learning\n \nsetting.\n \nThey\n \nfound\n \nthat\n \nnon-human-like\n \nbehavior\n \nand\n \nbelow\n \nhuman\n \nperformance\n \nin\n \nBinz\n \n&\n \nSchulz\n \n(2023)\n \nvanishes\n \nwhen\n \nprompted\n \nto\n \nuse\n \nan\n \nexploit\n \nstrategy,\n \ne.g.,\n \nthrough\n \nthe\n \nfollowing\n \nprompt\n \ndesign:\n \n“ The  following  hints  will  help  you  make  good  decision:   \n1.\n \nIn\n \neach\n \nround\n \nyou\n \nchoose\n \neither\n \nMachine\n \nF\n \nor\n \nMachine\n \nJ\n \nand\n \nreceive\n \nreward\n \nfrom\n \nthat\n \nmachine.\n \n \n2.\n \nYou\n \nmust\n \nchoose\n \nthe\n \nmachine\n \nwith\n \nhighest\n \naverage\n \nof\n \ndelivered\n \ndollars.\n \n3.\n \nAverage\n \nof\n \n1,\n \n2,\n \n3\n \nis\n \ncalculated\n \nfirst\n \nby\n \ncomputing\n \nsum\n \nof\n \nall\n \nobservations\n \nwhich\n \nis\n \n1\n \n+\n \n2\n \n+\n \n3\n \n=\n \n6\n \nand\n \nthen\n \ndividing\n \nit\n \nby\n \nnumber\n \nof\n \nobservations\n \nwhich\n \nis\n \n6/3\n \n=\n \n2.\n \n4.\n \nOut\n \nof\n \nx\n \nand\n \ny,\n \nif\n \nx\n \n-\n \ny\n \nis\n \npositive\n \ninteger\n \nthen\n \nx\n \nis\n \nhigher\n \nelse\n \ny\n \nis\n \nhigher.\n \nYour  goal  is  to  maximize  the  sum  of  received  dollars  within  one  additional  round.  \nQ:\n \nWhich\n \nmachine\n \ndo\n \nyou\n \nchoose?\n \nA:\n \nLet’s\n \nthink\n \nstep\n \nby\n \nstep”\n \nThese  findings  are  especially  relevant  for  studies  that  aim  to  infer  model  capabilities  or  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  25   \ncomparisons  between  humans  and  LLMs.  They  show  that  various  prompting  strategies  need  to  be  \ntested\n \nbefore\n \nmaking\n \nany\n \nclaims\n \nabout\n \na\n \nlack\n \nof\n \nLLM\n \ncapabilities\n \nor\n \nhuman-LLM\n \ndifferences.\n \nAdditionally,\n \nthey\n \nimply\n \nthat\n \nan\n \nLLMs\n \nperformance\n \nmight\n \nnot\n \ngeneralize\n \nto\n \na\n \ntask\n \nthat\n \nrequires\n \na\n \ndifferent\n \nprompt\n \nformat,\n \neven\n \nif\n \nthe\n \ndifferences\n \nare\n \nminor.\n \nTherefore,\n \nauthors\n \nshould\n \nprovide\n \nthe\n \nspecific\n \nprompts\n \nand\n \nexamples\n \nthat\n \nthey\n \nused.\n \nIdeally,\n \nthey\n \nwould\n \njustify\n \ntheir\n \nprompts\n \n(e.g.,\n \nbased\n \non\n \ntheory-driven\n \nconsiderations),\n \nor\n \nuse\n \nstrategies\n \nto\n \nincrease\n \nthe\n \nrobustness\n \nof\n \nprompting\n \nstrategies,\n \nanalogous\n \nto\n \nhow\n \nsocial\n \nscientists\n \naccount\n \nfor\n \nresponse\n \ndifferences\n \nusing\n \nstimulus\n \nsampling,\n \nrandomization\n \nof\n \nquestion\n \norder,\n \ndifferent\n \nscales,\n \netc.\n \nFor\n \nexample,\n \nauthors\n \ncould\n \ntest\n \nvariations\n \nof\n \nthe\n \nsame\n \nprompt\n \n(and\n \nreport\n \naggregates),\n \nor\n \ntest\n \nsensitivity\n \nto\n \nspecific\n \nwording\n \nstyles\n \nrelevant\n \nto\n \ntheir\n \nunderlying\n \nresearch\n \nquestion\n \n(e.g.,\n \nformal\n \nvs\n \ninformal\n \nlanguage,\n \ngender\n \nof\n \nagents,\n \norder\n \neffects).\n \nIf\n \nhuman\n \nand\n \nLLM\n \nresponses\n \nto\n \na\n \nprompt\n \nalign,\n \nthis\n \nmay\n \nvalidate\n \nthe\n \nprompt’s\n \ndesign.\n \nHowever,\n \nif\n \nthey\n \ndiverge,\n \nthe\n \nmismatch\n \nmay\n \nstill\n \nonly\n \nreflect\n \ndifferences\n \nin\n \ninstruction-processing\n \nrather\n \nthan\n \nLLM\n \nlimitations\n \n(e.g.,\n \nas\n \nshown\n \nin\n \nLoya\n \net\n \nal.,\n \n2023).\n \nUltimately,\n \nthese\n \nconsiderations\n \nhighlight\n \nthe\n \nimportance\n \nof\n \nsystematically\n \ntesting\n \nprompt\n \nvariations,\n \nboth\n \nto\n \nalign\n \nwith\n \ntheoretical\n \nexpectations\n \nand\n \nto\n \naccount\n \nfor\n \nhow\n \nhumans\n \nand\n \nLLMs\n \nmight\n \nprocess\n \ninstructions\n \ndifferently.\n \nIn  summary,  researchers  should  precisely  disclose  and,  ideally,  justify  the  prompts  used  \nwith\n \nLLMs,\n \nor\n \nrigorously\n \nevaluate\n \nvarious\n \nprompting\n \nstrategies\n \nand\n \nprompt\n \ndesigns.\n \nThis\n \nensures\n \nthat\n \nprompt\n \nchoices\n \ndo\n \nnot\n \nbias\n \nthe\n \nresults\n \ntowards\n \ndesired,\n \nyet\n \nonly\n \ncoincidental,\n \noutcomes.\n \nNote\n \nthat\n \nfor\n \nstudies\n \nusing\n \nLLMs\n \nto\n \nautomate\n \ntasks,\n \nsuch\n \nas\n \ncoding\n \ntexts,\n \nwhere\n \nground-truth\n \ndata\n \nexists\n \nor\n \ncan\n \nbe\n \ncreated\n \n(e.g.,\n \nthrough\n \nmanual\n \nannotation),\n \nthe\n \npriority\n \nshould\n \nbe\n \nto\n \nvalidate\n \nthat\n \nthe\n \nchosen\n \nprompts\n \nconsistently\n \nproduce\n \naccurate\n \nand\n \nreliable\n \nresponses,\n \nas\n \ndiscussed\n \nin\n \nthe\n \nprevious\n \nsection.\n \n2.3  Model  Settings  \n2.3.1  Parameters  \nResearchers  have  the  ability  to  manipulate  various  settings  of  an  LLM  to  influence  the  \noutput\n \ngenerated.\n \nOne\n \ncrucial\n \nparameter\n \nin\n \nthis\n \nregard\n \nis\n \nknown\n \nas\n \n\"temperature.\"\n \nGiven\n \ntheir\n \nautoregressive\n \nnature,\n \nLLMs\n \nfunction\n \nby\n \npredicting\n \nthe\n \nprobability\n \nof\n \nthe\n \nnext\n \nword\n \nin\n \na\n \nsequence\n \nbased\n \non\n \nthe\n \npreceding\n \nwords.\n \nThe\n \ntemperature\n \nparameter\n \ninfluences\n \nhow\n \nthe\n \nmodel\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  26   \nselects  the  subsequent  word  from  this  probability  distribution.  A  low  temperature  value  biases  \nthe\n \nmodel\n \ntowards\n \nchoosing\n \nthe\n \nmost\n \nprobable\n \nword,\n \npromoting\n \na\n \nmore\n \npredictable\n \nand\n \nconservative\n \noutput.\n \n In  contrast,  increasing  the  temperature  allows  the  model  to  sample  more  freely  from  the  \nprobability\n \ndistribution,\n \nincreasing\n \nthe\n \nlikelihood\n \nof\n \nselecting\n \nless\n \nfrequent\n \nbut\n \npotentially\n \nmore\n \ncreative\n \nor\n \nsurprising\n \nwords.\n \nIn\n \nessence,\n \ntemperature\n \nacts\n \nas\n \na\n \ncontrol\n \nknob\n \nfor\n \nthe\n \nbalance\n \nbetween\n \npredictability\n \nand\n \ndiversity\n \nin\n \nLLM-generated\n \ntext.\n \nA\n \nlow\n \ntemperature\n \nleads\n \nto\n \nless\n \nvariable\n \noutputs\n \n(only\n \nthe\n \nmost\n \nprobable\n \nwords\n \nwill\n \nbe\n \nchosen)\n \nand\n \na\n \nhigh\n \ntemperature\n \nleads\n \nto\n \nmore\n \ndiverse\n \noutputs\n \n(less\n \nprobable\n \nwords\n \nare\n \nalso\n \nconsidered).\n \nFor\n \nexample,\n \nto\n \nshow\n \ndifferences\n \nin\n \npredictability\n \nand\n \ncreativity\n \nof\n \noutputs,\n \nwe\n \nprompted\n4\n \nLLaMA-3.1\n \nto\n \ndescribe\n \nroses\n \n(“Describe\n \nroses\n \nin\n \none\n \nsentence”).\n \nHere,\n \na\n \nlow\n \ntemperature\n \nleads\n \nto\n \na\n \nmore\n \nconcise,\n \nfactual\n \ndescription:\n \n“\nRoses\n \nare\n \nbeautiful\n \nand\n \nfragrant\n \nflowers\n \nthat\n \ncome\n \nin\n \na\n \nwide\n \nvariety\n \nof\n \ncolors,\n \nshapes,\n \nand\n \nsizes.”\n While  a  high  temperature  leads  to  more  creative,  poetic  wording:  “ Roses  are  \nbreathtaking,\n \nfragile,\n \nyet\n \nmajestic\n \nand\n \ndiverse\n \nflowers\n \nknown\n \nfor\n \ntheir\n \nvelvety\n \npetals\n \nand\n \nintoxicating\n \nscents.”.\n Coyne  et  al.  (2023)  found  that  lower  temperature  improves  performance  in  tasks  like  grammar  \nand\n \nerror\n \ncorrection,\n \nlikely\n \nbecause\n \nlowering\n \ndiversity\n \nin\n \noutputs\n \nfocuses\n \non\n \nthe\n \nmost\n \nlikely\n \nwords,\n \nleading\n \nto\n \nfewer\n \nmistakes\n.\n \nTo\n \ngive\n \na\n \nconcrete\n \ncomparison\n \nof\n \nlow\n \nand\n \nhigh\n \ntemperature\n \nin\n \nthis\n \ncontext,\n \nwe\n \nrecreated\n \none\n \nexample\n \nfrom\n \nCoyne\n \net\n \nal.\n \n(2023)\n \nusing\n \nLLaMA-3.1.\n \nWe\n \nused\n \nthe\n \nexact\n \nsame\n \nprompt\n2\n:\n “Update  to  fix  all  grammatical  and  spelling  errors:  \nI\n \nconsider\n \nthat\n \nis\n \nmore\n \nconvenient\n \nto\n \ndrive\n \na\n \ncar\n \nbecause\n \nyou\n \ncarry\n \non\n \nmore\n \nthings\n \nin\n \nyour\n \nown\n \ncar\n \nthan\n \ntravelling\n \nby\n \ncar.”\n while  varying  the  temperature  from  low  to  high.  A  low  temperature  fixed  the  error  and  replaced  \nthe\n \nillogical\n \nchoice\n \nof\n \nthe\n \nlast\n \nword\n \n“car”:\n “I  consider  it  more  convenient  to  drive  a  car  because  you  can  carry  more  things  in  your  own  car  \nthan\n \nwhen\n \ntraveling\n \nby\n \nbus\n \nor\n \ntrain.”\n \nWhile\n \na\n \nhigh\n \ntemperature\n \nsignificantly\n \nchanged\n \nthe\n \nsentence\n \nstructure\n \nto\n \nkeep\n \nreferring\n \nto\n \n“car”\n \n4\n See  all  code,  data,  and  instructions  for  replication  in  this  GitHub  repository:  h t t p s : / / g i t h u b . c o m / g o y t o o m / l l m _ p s y c h o l o g y _ g u i d e  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  27   \nat  the  end  of  the  sentence:  “I  considered  that  it  is  more  convenient  to  drive  a  car.  This  is  because  one  can  carry  even  more  \nthings\n \nin\n \none's\n \nown\n \ncar\n \nthan\n \nwhile\n \ntravelling\n \nin\n \none.”\n While  temperature  is  often  likened  to  creativity  or  employed  to  introduce  variance  in  \nmodel\n \nresponses\n \n(Almeida\n \net\n \nal.,\n \n2023;\n \nAtari\n \net\n \nal.,\n \n2023b;\n \nDavis\n \net\n \nal.,\n \n2024;\n \nZhao\n \net\n \nal.,\n \n2024),\n \nit\n \nis\n \nimportant\n \nto\n \nrecognize\n \nthat\n \nthis\n \nvariance\n \nmay\n \ndiffer\n \nfundamentally\n \nfrom\n \nthe\n \ninter-participant\n \nvariance\n \nthat\n \nsocial\n \nscientists\n \ntypically\n \nstudy.\n \nThe\n \nvariance\n \ninduced\n \nby\n \na\n \nhigh\n \ntemperature\n \nis\n \nintrinsically\n \ntied\n \nto\n \nthe\n \nmodel's\n \n“confidence”\n \nin\n \nits\n \nresponse\n \n(i.e.,\n \nthe\n \nsharpness\n \nof\n \nthe\n \nprobability\n \ndistribution\n \nover\n \npotential\n \noutputs\n \nor\n \nthe\n \npresence\n \nof\n \nmultiple\n \ndistinct\n \npeaks).\n \nConsequently,\n \nit\n \nbears\n \na\n \ncloser\n \nresemblance\n \nto\n \nintra-participant\n \nvariance,\n \nakin\n \nto\n \nposing\n \nthe\n \nsame\n \nquestion\n multiple  times  to  a  single  individual  (Abdurahman  et  al.,  2024; Park  et  al.,  2024). This  may  not  always  be  what  researchers  aim  to  achieve  when  trying  to  induce  variance  in  the  model  \nresponses.\n \nOther\n \nimportant\n \nparameters\n \ndefine\n \npenalties\n \nfor\n \nrepeating\n \nwords,\n \nlimit\n \nthe\n \nlength\n \nof\n \nmodel\n \noutput,\n \nor\n \nset\n \nthe\n \n“seed”\n \nthat\n \naffects\n \nrandomization,\n \nmaking\n \noutputs\n \nreproducible\n \n(for\n \na\n \ndocumentation\n \nof\n \nLLM\n \nparameters\n \nsee\n \nHuggingface,\n \n2024b).\n \nMoreover,\n \nas\n \nLLM\n \ntechnologies\n \ncontinue\n \nto\n \nevolve,\n \nnew\n \nparameters\n \nmay\n \nbe\n \nintroduced\n \nwhile\n \nothers\n \nare\n \ndeprecated.\n \nFor\n \ninstance,\n \nnewer\n \nreasoning-focused\n \nmodels\n \nsuch\n \nas\n \nOpenAI’s\n \no1\n \nor\n \nDeepSeek’s\n \nR1\n \nuse\n \nparameters\n \nlike\n \n“reasoning_effort”\n \nto\n \ncap\n \ntime\n \nand\n \nwords\n \nspent\n \nper\n \ntask\n \n(boosting\n \nspeed\n \nand\n \ncutting\n \ncosts).\n \nFollowing\n \nthese\n \ndevelopments,\n \nallows\n \nresearchers\n \nto\n \nbetter\n \ncontrol\n \nthe\n \nnature\n \nof\n \nthe\n \ndata\n \nthey\n \ncollect.\n \nReviewers\n \nshould\n \nconfirm\n \nthat\n \nauthors\n \nreport\n \nand\n \njustify\n \nall\n \nrelevant\n \nmodel\n \nsettings\n \nin\n \nthe\n \ncontext\n \nof\n \nthe\n \nspecific\n \nmodel\n \nand\n \nversion\n \nemployed.\n \n2.3.2  Batching  \nAuthors  can  choose  how  many  data  points  to  submit  at  once  to  the  LLM.  Submitting  more  \nthan\n \none\n \ndatum\n \nat\n \na\n \ntime\n \nis\n \ncalled\n \nbatching.\n \nAn\n \nLLM\n \ncan,\n \nfor\n \nexample,\n \ncode\n \nmultiple\n \nresponses\n \nusing\n \na\n \nsingle\n \nprompt.\n \nAn\n \nauthor\n \nmay\n \nwant\n \nto\n \nuse\n \nthis\n \nmethod\n \nbecause\n \nit\n \ncan\n \nallow\n \nfor\n \nfaster\n \nor\n \nless\n \nexpensive\n \ndata\n \nprocessing.\n \nHowever,\n \nthis\n \napproach\n \nshould\n \nbe\n \nused\n \nwith\n \ncaution\n \nbecause\n \nLLMs\n \nare\n \nhighly\n \ncontext-sensitive.\n \nThe\n \norder\n \nthat\n \ndata\n \npoints\n \nare\n \nplaced\n \nwithin\n \na\n \nbatch,\n \nor\n \nsimply\n \nthe\n \nfact\n \nthat\n \nthey\n \nare\n \nbatched\n \ncompared\n \nto\n \nprocessing\n \neach\n \ndata\n \npoint\n \nseparately\n \n(e.g.,\n \nsee\n \nMatter\n \net\n \nal.,\n \n2024),\n \nmay\n \nimpact\n \nresponses,\n \nsimilar\n \nto\n \norder\n \neffects\n \nin\n \nquestionnaire\n \nresponses.\n \nTo\n \nensure\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  28   \ntransparency,  authors  should  explain  whether  they  are  batching  their  data,  and  if  so,  what  they  are  \ndoing\n \nabout\n \ncontext\n \neffects.\n \nIdeally,\n \neach\n \npiece\n \nof\n \ndata\n \nwould\n \nbe\n \nprocessed\n \nseparately\n \nto\n \nremove\n \nthese\n \ncontext\n \neffects,\n \nbut\n \nthere\n \nmay\n \nbe\n \nconstraints\n \n(e.g.,\n \nnot\n \nbatching\n \ncan\n \nbe\n \nmuch\n \nmore\n \nexpensive)\n \nthat\n \nprevent\n \nthis\n \nkind\n \nof\n \nprocessing.\n \nAuthors\n \ncould\n \nevaluate,\n \non\n \na\n \nsmaller\n \nsubset\n \nof\n \nthe\n \ndata,\n \nwhether\n \nbatching\n \nleads\n \nto\n \nsignificant\n \ndistortion\n \nof\n \ntheir\n \noutputs\n \nin\n \nways\n \nthat\n \nmight\n \nskew\n \ntheir\n \ninferences\n \n(e.g.,\n \nwhether\n \nmisclassifications\n \ndue\n \nto\n \nbatching\n \ncorrelate\n \nwith\n \nrelevant\n \nvariables).\n \nThey\n \ncan\n \nalso\n \nrandomize\n \nthe\n \norder\n \nof\n \nthe\n \nbatched\n \nitems\n \nand\n \nsubmit\n \nthem\n \nmore\n \nthan\n \nonce,\n \nbut\n \nthis\n \ndiminishes\n \nthe\n \nadvantages\n \nof\n \nbatching.\n \nIn  summary,  authors  must  transparently  report  and,  ideally,  justify  their  use  of  critical  LLM  \nparameters\n \nand\n \napplication\n \nstrategies\n \nsuch\n \nas\n \nbatching,\n \nas\n \nthese\n \nchoices\n \ncan\n \nsignificantly\n \naffect\n \nthe\n \nstudy’s\n \noutcomes.\n \nReviewers\n \nshould\n \nthen\n \npay\n \nattention\n \nto\n \nthe\n \njustification\n \nof\n \nthe\n \nmodel\n \nchoice\n \nand\n \nparameters\n \nwhen\n \nevaluating\n \nmethodological\n \nrigor\n \nof\n \nLLM\n \nworks.\n \n2.4  Data  Processing  and  Error  Handling  \n In  many  cases,  an  author  will  want  to  process  the  output  of  an  LLM  before  including  it  as  \ndata\n \nfor\n \nanalysis\n \n(i.e.,\n \npost-processing)\n \n.\n \nFor\n \nexample,\n \nan\n \nauthor\n \nmay\n \nchoose\n \nto\n \naggregate\n \nmultiple\n \nresponses\n \nto\n \nthe\n \nsame\n \nprompt\n \nor\n \nits\n \nvariations\n \nto\n \nform\n \na\n \nmore\n \nstable\n \nresponse\n \n(as\n \ndiscussed\n \nearlier).\n \nOr,\n \ngiven\n \nthat\n \nLLMs\n \nsometimes\n \ngenerate\n \nunexpected\n \nresults,\n \nan\n \nauthor\n \nmay\n \nexamine\n \nmodel\n \noutputs\n \nto\n \nmake\n \nsure\n \nthat\n \nresults\n \nfall\n \nwithin\n \nan\n \nexpected\n \nrange\n \nsuch\n \nas\n \nwithin\n \nthe\n \nbounds\n \nof\n \na\n \nscale.\n \nIn\n \ncases\n \nwhere\n \nerrors\n \n(i.e.,\n \nunexpected\n \noutputs)\n \narise,\n \nauthors\n \nmust\n \nspecify\n \na\n \nprocess\n \nfor\n \nhandling\n \nthem.\n \n A  relevant  example  comes  from  Coyne  et  al.  (2023),  who  investigated  LLMs  capability  for  \ngrammar\n \nand\n \nerror\n \ncorrecting\n \nby\n \nprompting\n \nthe\n \nLLM\n \nto\n \ncorrect\n \na\n \nseries\n \nof\n \nerroneous\n \nsentences.\n \nThey\n \nobserved\n \nvarious\n \nissues\n \nin\n \noutput\n \ngeneration,\n \nsuch\n \nas\n \nthe\n \nmodel\n \nadding\n \nremarks\n \n(e.g.,\n \nstating\n \nthat\n \nit\n \ndid\n \nnot\n \nchange\n \nanything),\n \ncompleting\n \nsentences\n \nwithout\n \nclosing\n \npunctuation\n \n(“spurious\n \nexpansion”),\n \nor\n \nadding\n \nmissing\n \npunctuation\n \nat\n \nthe\n \nend\n \nof\n \nthe\n \nerroneous\n \nsentences\n \nbefore\n \ngenerating\n \ncorrected\n \nsentences.\n \nThe\n \nauthors\n \ndeal\n \nwith\n \nthese\n \nissues\n \nby\n \niteratively\n \nimproving\n \ntheir\n \nprompts\n \nand\n \napplying\n \nfew-shot\n \nprompting\n \n(i.e.,\n \ninclude\n \nexample\n \nsentences\n \nand\n \ntheir\n \ncorrections\n \nin\n \nthe\n \nprompt)\n \nuntil\n \nthey\n \nobserve\n \nfew\n \nto\n \nnone\n \nof\n \nthe\n \nidentified\n \nissues.\n \nHowever,\n \nwhile\n \nthe\n \nauthors\n \nreport\n \nthat\n \nnone\n \nof\n \nthe\n \nremaining\n \nissues\n \nlead\n \nto\n \nnew\n \nlines\n \nin\n \nthe\n \ncorrected\n \nsentences\n \n(which\n \nis\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  29   \nrelevant  because  asymmetric  lines  interfere  with  the  evaluation  script)  they  did  not  report  how  \nthey\n \nhandled\n \nthe\n \nremaining\n \nerrors\n \n(e.g.,\n \nremoving\n \nthese\n \ncases\n \nor\n \nnot).\n \nThis\n \nis\n \nrelevant\n \nbecause\n \nit\n \nmight\n \nchange\n \nthe\n \nresults\n \nand\n \ninterpretation\n \n(e.g.,\n \nlead\n \nto\n \noutlier\n \nhuman\n \nevaluations\n \nduring\n \nvalidation\n \ndue\n \nto\n \nthe\n \nstrange\n \nformat)\n \nand\n \nmake\n \nreplication\n \nmore\n \ndifficult.\n Overall,  authors  should  be  transparent  about  post  processing  and  error  handling.  Authors  \nshould\n \nprovide\n \ninformation\n \nabout\n \nhow\n \nthey\n \nhandled\n \nerrors\n \n(which\n \nis\n \nanalogous\n \nto\n \nexclusion\n \ncriteria\n \nfor\n \nexperimental\n \nresearch)\n \nand\n \nprovide\n \ninformation\n \nabout\n \nthe\n \nfrequency\n \nof\n \nerrors.\n \nIdeally,\n \nauthors\n \nwould\n \nconsider\n \npost-processing\n \nand\n \nerror\n \nhandling\n \nprior\n \nto\n \nprocessing\n \ntheir\n \ndata\n \nand\n \nmake\n \ntheir\n \nplans\n \npublic\n \nvia\n \npre-registration.\n \nGiven\n \nthe\n \ncomplexity\n \nof\n \nthese\n \ntasks,\n \nhowever,\n \nthere\n \nneeds\n \nto\n \nbe\n \nroom\n \nfor\n \nplans\n \nto\n \nchange\n \n(e.g.,\n \nimprove\n \nvia\n \niteration).\n \nReviewers\n \nshould\n \npay\n \nattention\n \nto\n \nwhether\n \nthe\n \nparticular\n \nstrategies\n \nused\n \ncould\n \nskew\n \nthe\n \noutputs\n \ntowards\n \ndesired\n \ninferences\n \npost-hoc.\n \nIf\n \nauthors,\n \nfor\n \nexample,\n \nvalidate\n \ntheir\n \npost-processed\n \ndata\n \non\n \nmanually\n \ncoded\n \nsubsets\n \n(or\n \nhuman\n \nparticipant\n \ndata\n \nfor\n \nsimulation\n \nstudies)\n \nand\n \nshow\n \nthat\n \nit\n \ngeneralizes\n \nout\n \nof\n \nsample,\n \nthis\n \nshould\n \nincrease\n \nconfidence\n \nin\n \nthe\n \nauthors\n \nresults.\n In  summary,  authors  should  clearly  describe  how  they  process  data  and  handle  errors  in  \nLLM\n \noutputs,\n \nincluding\n \nany\n \npost-processing\n \nsteps\n \ntaken\n \nto\n \nstabilize\n \nor\n \nvalidate\n \nresponses.\n \nThis\n \nreduces\n \npost-hoc\n \nchanges\n \nin\n \ndata\n \nto\n \nfit\n \nthe\n \ndesired\n \noutcome\n \nof\n \na\n \nstudy.\n \n2.5  Preregistration  \nPreregistration  involves  publicly  documenting  the  research  plan,  typically  including  \nmethodology,\n \ndata\n \nanalysis\n \nplans,\n \nand\n \npotential\n \nhypotheses,\n \nbefore\n \nthe\n \nstudy\n \nbegins.\n \nThis\n \nprocess\n \nis\n \ncrucial\n \nto\n \nprevent\n \nresearchers\n \nfrom\n \n(unintentionally)\n \nseeking\n \nsignificant\n \nresults\n \nthrough\n \nmultiple\n \nanalyses\n \nor\n \nby\n \naltering\n \nhypotheses\n \npost\n \nhoc\n.\n \nBy\n \ncommitting\n \nto\n \na\n \npredefined\n \nanalysis\n \nplan,\n \npreregistration\n \nincreases\n \nthe\n \ntrustworthiness\n \nof\n \nthe\n \nfindings.\n \nThis\n \nis\n \nnot\n \ndifferent\n \nin\n \nLLM\n \nresearch,\n \nwhere\n \nsmall\n \n“tweaks”\n \ncan\n \nheavily\n \ninfluence\n \noutcomes.\n \nHowever,\n \nthere\n \nneeds\n \nto\n \nbe\n \nsome\n \nflexibility\n \nregarding\n \npreregistering\n \nLLM\n \nrelated\n \nmethodology\n \nbecause\n \nresearchers\n \noften\n \nneed\n \nto\n \niteratively\n \nimprove\n \nprompt\n \ndesign\n \nand\n \nmodel\n \nsettings\n \nduring\n \ntheir\n \nexperiments.\n \n \nFurthermore,  preregistration  is  not  the  only  way  to  address  issues  with  validity  and  \nrobustness\n \nof\n \nresults.\n \nFor\n \nexample,\n \nimagine\n \na\n \nresearcher\n \nstudying\n \ngender\n \ndifferences\n \nin\n \nexpressing\n \nemotions\n \nin\n \ntexts.\n \nThey\n \ncollect\n \nthe\n \ntexts\n \nand\n \nthe\n \nauthors’\n \ngender\n \ninformation\n \nand\n \nuse\n \nan\n \nLLM\n \nto\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  30   \nclassify  the  emotions  expressed  in  each  text.  Now,  it  is  possible  that  the  researcher  might  adjust  \nmodel\n \nsettings\n \nand\n \nprompt\n \ndesigns\n \nuntil\n \nthey\n \nfind\n \ndesired\n \nbut\n \nspurious\n \ngender\n \ndifferences,\n \nsimilar\n \nto\n \nthe\n \npractice\n \nof\n \np-hacking.\n \nPreregistration\n \ncould\n \nhelp\n \nprevent\n \nthis\n \nby\n \ncommitting\n \nthe\n \nresearcher\n \nto\n \nan\n \nanalysis\n \nplan\n \nin\n \nadvance.\n \nAlternatively,\n \nthe\n \nresearcher\n \ncould\n \nmanually\n \nannotate\n \na\n \nsubsample\n \nof\n \nthe\n \ndata\n \nand\n \ncompare\n \nthem\n \nwith\n \nLLM\n \noutputs\n \nto\n \ndemonstrate\n \na\n \nhigh\n \naccuracy\n \nand\n \nthat\n \nclassification\n \nerrors\n \ndo\n \nnot\n \ncorrelate\n \nwith\n \ngender\n \n(or\n \nany\n \nother\n \nindependent\n \nvariable)\n \nto\n \ndemonstrate\n \nrobustness\n \nof\n \ntheir\n \nfindings.\n \nIn\n \nother\n \nwords,\n \nthey\n \ncould\n \nshow\n \nthat\n \ntheir\n \nresults\n \nare\n \nnot\n \ndue\n \nto\n \nsystematic\n \nerrors\n \nin\n \nthe\n \nLLM\n \noutputs.\n \nThis\n \ncould\n \nbe\n \nan\n \nalternative\n \nway\n \nto\n \nalleviate\n \nconcerns\n \nabout\n \nspurious\n \nfindings\n \nresulting\n \nfrom\n \nthe\n \nprompt\n \ndesign\n \nor\n \nmodel\n \nsettings.\n \n \nThis  manual  annotation  approach  is  also  practical  given  that  many  researchers  already  use  \nmanually\n \nannotated\n \nsubsamples\n \nto\n \nshow\n \nthe\n \ngeneral\n \naccuracy\n \nof\n \ntheir\n \nclassification\n \nmethod\n \nand\n \nbecause\n \nthey\n \ncan\n \nconduct\n \nthese\n \nchecks\n \nafter\n \nrunning\n \ntheir\n \nexperiments.\n \nOther\n \nconcerns\n \ncould\n \nbe\n \nalleviated\n \nwith\n \nsimilar\n \nrobustness\n \nchecks.\n \nFor\n \nexample,\n \nwhen\n \nmaking\n \nclaims\n \nabout\n \nLLM\n \ncapabilities\n \n(or\n \nlack\n \nthereof),\n \nauthors\n \ncould\n \nshow\n \nthat\n \ntheir\n \nfindings\n \nare\n \nrobust\n \nover\n \nvarious\n \nprompt\n \ndesigns\n \nand\n \nmodel\n \nsettings.\n \nHowever,\n \ngiven\n \nthe\n \nrapid\n \nevolution\n \nof\n \nLLMs,\n \npreregistration\n \nremains\n \nuseful\n \nfor\n \nhelping\n \nresearchers\n \nunderstand,\n \nreplicate,\n \nand\n \nbuild\n \nupon\n \nothers’\n \nwork.\n \nIt\n \nprovides\n \na\n \nclear\n \nroadmap\n \nof\n \nthe\n \nresearch\n \nprocess,\n \nincluding\n \nspecific\n \nprompts,\n \nparameters,\n \nand\n \nother\n \nmethodological\n \nchoices,\n \nparticularly\n \nvalidation\n \nand\n \nreliability\n \nstrategies\n \nto\n \nminimize\n \npost-hoc\n \nadjustments\n \nthat\n \ncould\n \nskew\n \nresults,\n \nas\n \nwell\n \nas\n \nplanned\n \nrobustness\n \nchecks\n \nto\n \naddress\n \nthese\n \nconcerns.\n \nIn  summary,  preregistration  in  LLM  research  can  facilitate  transparency,  validity,  and  \nreliability.\n \nHowever,\n \nthere\n \nneeds\n \nto\n \nbe\n \na\n \nnuanced\n \nperspective\n \nof\n \nwhen\n \nand\n \nhow\n \nto\n \napply\n \nit,\n \nrecognizing\n \nthat\n \nthe\n \nflexibility\n \ninherent\n \nin\n \nLLM\n \nexperimentation\n \nmay\n \nrequire\n \nadaptations\n \nto\n \ntraditional\n \npreregistration\n \npractices\n \nto\n \naccount\n \nfor\n \nthe\n \ndynamic\n \nand\n \nexploratory\n \nnature\n \nof\n \nworking\n \nwith\n \nthese\n \nmodels.\n \n \n2.6  Special  Considerations  for  Simulating  Human  Data  \nWhen  utilizing  LLMs  to  simulate  human  data,  several  crucial  considerations  arise.  Foremost,  \nit\n \nis\n \nimperative\n \nfor\n \nauthors\n \nto\n \nvalidate\n \nthe\n \nLLM's\n \nability\n \nto\n \ngenerate\n \ndata\n \nthat\n \nmirrors\n \nhuman\n \nbehavior,\n \nespecially\n \nwithin\n \nthe\n \nspecific\n \ncontext\n \nof\n \ntheir\n \nresearch\n \nquestion.\n \nIdeally,\n \nthis\n \nentails\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  31   \nreplicating  a  subset  of  tasks  with  human  participants  to  enable  direct  comparison  between  model  and  \nhuman\n \noutputs.\n \nHowever,\n \nshould\n \nethical\n \nor\n \npractical\n \nlimitations\n \nprevent\n \nsuch\n \ncomparison,\n \nauthors\n \nmust\n \nrigorously\n \ndemonstrate\n \nthat\n \nthe\n \nLLM\n \npossesses\n \nthe\n \nnecessary\n \ncapabilities\n \nand\n \ncognitive\n \nprocesses\n \ninherent\n \nto\n \nhuman\n \ndata\n \nproduction.\n \nFor\n \ninstance,\n \nif\n \nthe\n \ntask\n \ninvolves\n \ninferring\n \nothers'\n \nmental\n \nstates,\n \nshowcasing\n \nthe\n \nmodel's\n \nTheory\n \nof\n \nMind\n \ncapabilities\n \nis\n \ncrucial.\n \nAlternatively,\n \nauthors\n \ncan\n \nimpose\n \ntheory-based\n \nconstraints\n \non\n \nthe\n \nmodel's\n \noutput,\n \naligning\n \nit\n \nwith\n \nestablished\n \nhuman\n \nbehavior\n \npatterns.\n \nWhen\n \nreferencing\n \npast\n \nworks\n \ndemonstrating\n \nrelevant\n \nLLM\n \ncapabilities,\n \ncaution\n \nis\n \nadvised.\n \nThe\n \ndynamic\n \nnature\n \nof\n \nLLM\n \ndevelopment,\n \nwith\n \nfrequent\n \nupdates\n \nand\n \nevolving\n \nperformance,\n \nnecessitates\n \na\n \nfocus\n \non\n \nstable\n \nmodels\n \nwith\n \nwell-documented\n \ncapabilities,\n \nsuch\n \nas\n \nopen-source\n \nmodels\n \nwith\n \npublicly\n \navailable\n \nweights.\n \nNonetheless,\n \nauthors\n \nshould\n \nalways\n \nweigh\n \nthe\n \npossibility\n \nof\n \nsimulations\n \nbeing\n \ninvalid\n \nand\n \ncarefully\n \nconsider\n \nwhen–as\n \nwill\n \noften\n \nbe\n \nthe\n \ncase–\n \nhuman\n \nexperiments\n \nmay\n \nstill\n \nbe\n \nnecessary\n \nand\n \npreferable\n \nover\n \nsimulations.\n  Table  5  \nRecommendations  to  ensure  robust  LLM  findings.  \nPotential  Issues  Strategy  Details  \nUnfounded  /  coincidental  outputs  \nConfirm  LLM  outputs  with  human  data  \nAuthors  validate  LLM  outputs  against  (human)  ground-truth  data.  Comparisons  should  be  made  on  pre-  defined,  justified,  samples.  \nSensitivity  to  prompt  de-  sign  \nTest  and  report  prompt  variations  /  justify  prompts  \nAuthors  should  test  LLM  outputs’  sensitivity  to  prompts,  providing  justification  for  chosen  prompts  based  on  theoretical  or  empirical  grounds.  Variations  should  be  tested  to  ensure  consistency  of  LLM  responses.  \nSensitivity  to  parameter  settings  \nClear  documentation  and  justification  of  model  settings  /  aggregate  across  settings  \nAuthors  report  and  justify  LLM  settings,  such  as  “temperature,”  explaining  their  impact  on  respective  variability  in  model  outputs.  \nData  processing  bias  Transparent  data  processing  and  error  handling  \nAuthors  disclose  their  data  processing  methodologies,  including  how  they  handle  unexpected  or  outlier  LLM  outputs,  to  prevent  biasing  results  towards  desired  outcomes.  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  32   \nData  dredging  in  LLM  outputs  /  Selective  reporting  \nPreregistration,  post-hoc  robustness  checks  \nAuthors  may  preregister  their  study  design  (or  run  a  preregistered  replication),  including  hypotheses,  LLM  choices,  settings,  and  data  processing  plans,  to  ensure  transparency  and  mitigate  “p-hacking”.  Authors  may  alternatively  confirm  their  findings  with  post-hoc  robustness  checks  if  preregistration  is  unfeasible  or  impractical  due  to  the  complexity  of  their  study.  \nSecond,  authors  must  carefully  consider  the  nature  of  the  data  generated  by  LLMs  when  \ninterpreting\n \ntheir\n \nfindings.\n \nFor\n \nexample,\n \nprompting\n \nLLMs\n \nto\n \nsolve\n \na\n \nset\n \nof\n \ntasks\n \nand\n \nreporting\n \naverage\n \nperformance\n \nor\n \ncomparing\n \ntheir\n \nresponses\n \nto\n \nhuman\n \nsamples\n \ncan\n \nlead\n \nto\n \nconclusions\n \nabout\n \ngeneral\n \nand\n \n“human-like”\n \ncapabilities\n \n(e.g.,\n \nsee\n \nBang\n \net\n \nal.,\n \n2023;\n \nCoda-Forno\n \net\n \nal.,\n \n2023;\n \nDillion\n \net\n \nal.,\n \n2023;\n \nHorton,\n \n2023;\n \nH.\n \nLiu\n \net\n \nal.,\n \n2023).\n \nHowever,\n \nresearchers\n \nshould\n \nbe\n \ncautious\n \nbecause\n \nthese\n \nfindings\n \noften\n \nreflect\n \nsimulated\n \naggregates\n \n(i.e.,\n \nsimulate\n \nthe\n \naverage\n \nof\n \na\n \nhuman\n \nsample)\n \nthat\n \nfail\n \nto\n \ncapture\n \nfull\n \nhuman\n \nvariability.\n \nThe\n \nvariance\n \nin\n \nthese\n \nLLM\n \nresponses\n \nis\n \noften\n \nmuch\n \nlower\n \nthan\n \nthe\n \ninterpersonal\n \nvariance\n \nstudied\n \nby\n \nsocial\n \nscientists\n \n(Abdurahman\n \net\n \nal.,\n \n2024;\n \nPark\n \net\n \nal.,\n \n2024),\n \nas\n \ndiscussed\n \nin\n \nthe\n \nprevious\n \nsection.\n \n \nAdditionally,  these  aggregates  may  obscure  important  nuances,  such  as  cultural  and  \ndemographic\n \ndifferences,\n \npotentially\n \nleading\n \nto\n \nconclusions\n \nthat\n \nprimarily\n \nrepresent\n \none\n \nculture\n \nor\n \ndemographic\n \ngroup.\n \nFor\n \nexample,\n \nAtari\n \net\n \nal.\n \n(2023b)\n \ndemonstrate\n \nthat\n \nLLM\n \nresponses\n \nto\n \npsychological\n \nmeasures\n \nmostly\n \nresemble\n \nindividuals\n \nfrom\n \nWestern\n \nEducated\n \nIndustrialized\n \nand\n \nRich\n \n(WEIRD;\n \nHenrich\n \net\n \nal.,\n \n2010)\n \ncountries,\n \nfailing\n \nto\n \ncapture\n \nresponses\n \nfrom\n \nmany\n \nparticipants\n \noutside\n \nthis\n \ncultural\n \nsphere.\n \nSimilarly,\n \nAbdurahman\n \net\n \nal.\n \n(2024)\n \nshow\n \nthat\n \nLLM\n \nresponses\n \nto\n \npsychological\n \nmeasures\n \nalign\n \nwith\n \ndifferent\n \ndemographics,\n \nand\n \nthat\n \nLLM\n \nannotations\n \nof\n \ntexts\n \nalign\n \nmore\n \nwith\n \nannotators\n \nfrom\n \nsome\n \ndemographic\n \ngroups.\n \nSome\n \nstrategies\n \nto\n \naddress\n \nthis\n \nissue,\n \nsuch\n \nas\n \ninstructing\n \nthe\n \nmodel\n \nto\n \nassume\n \ndifferent\n \npersonas\n \nin\n \nline\n \nwith\n various  demographics,  are  currently  being  developed  (Aher  et  al.,  2023; Argyle  et  al.,  2023). Note,  however,  that  this  introduces  another  degree  of  freedom  for  researchers  which  should  be  \naccounted\n \nfor\n \nand\n \njustified\n \n(e.g.,\n \nin\n \npre-registrations\n \nor\n \nthrough\n \npost-hoc\n \nrobustness\n \nchecks).\n \nAdditionally,\n \nmore\n \nrobust\n \nways\n \nof\n \ninducing\n \nhuman\n \nvariance\n \nin\n \nLLM\n \noutputs\n \nneed\n \nto\n \nbe\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  33   \ndeveloped.  Current  approaches  often  replicate  demographic  stereotypes  and  fail  to  replicate  fine-grained,  or  in  some  cases  any,  nuances  within  populations  (Beck  et  al.,  2024; Durmus  et  al.,  2023; Santurkar  et  al.,  2023). Wang  et  al.  (2024)  further  argue  that  LLMs'  training  on  scraped  online  text  data  inherently  leads  to  systematic  misrepresentation  of  demographic  groups,  as  this  \ndata\n \noften\n \nfails\n \nto\n \ndistinguish\n \nbetween\n \nin-group\n \nand\n \nout-group\n \nperspectives,\n \ncausing\n \nmodels\n \nto\n \nreflect\n \nstereotypical\n \nout-group\n \nviews\n \nrather\n \nthan\n \nauthentic\n \nperspectives\n \nand\n \nexperiences\n \nof\n \nmembers\n \nof\n \nthose\n \ngroups.\n \nMoreover,\n \ntheir\n \nlikelihood-based\n \ntraining\n \nobjectives\n \npush\n \nmodels\n \nto\n \ngenerate\n \nthe\n \nmost\n \nstatistically\n \ncommon\n \nresponses\n \nrather\n \nthan\n \ncapturing\n \ndiverse\n \nviewpoints,\n \nunderrepresenting\n \nthe\n \nnatural\n \nheterogeneity\n \nthat\n \nexists\n \nwithin\n \ndemographic\n \ngroups.\n \nResearchers  should  also  carefully  consider  the  statistical  treatment  of  LLM  outputs  when  \nusing\n \nthem\n \nas\n \nsubstitutes\n \nfor\n \nhuman\n \nsamples.\n \nSince\n \nthese\n \noutputs\n \ncome\n \nfrom\n \nthe\n \nsame\n \nunderlying\n \nmodel,\n \noften\n \nwith\n \nsimilar\n \ninstructions\n \nand\n \nsettings,\n \nthey\n \nshould\n \nnot\n \nbe\n \ntreated\n \nas\n \nfully\n \nindependent\n \ndata\n \npoints.\n \nStatistical\n \nmodels\n \nthat\n \naccount\n \nfor\n \ndependence\n \nin\n \nthe\n \ndata,\n \nsuch\n \nas\n \nhierarchical\n \nmodels,\n \nshould\n \nbe\n \napplied\n \nto\n \nreduce\n \nthe\n \nrisk\n \nof\n \noverestimating\n \neffect\n \nsizes\n \nand\n \ntheir\n \ngeneralizability.\n \nReviewers\n \nshould\n \ntake\n \njustification\n \nof\n \ndata\n \ngeneration\n \nmethods,\n \nhow\n \nthey\n \nensure\n \nthe\n \nsimulated\n \ndata\n \nreflects\n \nthe\n \ntarget\n \npopulation,\n \nand\n \nthe\n \nstatistical\n \ntreatment\n \ninto\n \naccount\n \nwhen\n \nevaluating\n \nLLM\n \npapers.\n \n \nThird,  when  using  LLMs  for  inferences  about  LLM  or  human  capabilities  and  behavior,  \nit\n \nis\n \nimportant\n \nto\n \nmake\n \nsure\n \nthat\n \nthe\n \ntest\n \nstimuli\n \nare\n \nnot\n \nin\n \nthe\n \nmodel’s\n \ntraining\n \ndata.\n \nIf\n \nauthors\n \nclaim\n \nthat\n \nan\n \nLLM\n \ndisplays\n \nreasoning\n \nskills\n \nbecause\n \nit\n \ncan\n \nsolve\n \na\n \nreasoning\n \ntest\n \nor\n \nuse\n \nan\n \nLLM’s\n \nresponses\n \nto\n \nexperimental\n \nstimuli\n \nto\n \nexplain\n \nhuman\n \nbehavior,\n \nthey\n \nshould\n \nensure\n \nthe\n \nmodel\n \nhas\n \nnot\n \nencountered\n \nthose\n \ntasks\n \nbefore.\n \nSimilarly,\n \nwhen\n \nsimulating\n \nhuman\n \ndata\n \n(e.g.,\n \nhuman\n \ndecisions\n \nacross\n \nscenarios),\n \nresearchers\n \nmay\n \nwant\n \nto\n \navoid\n \nmodels\n \nchoosing\n \na\n \nresponse\n \nbased\n \non\n \nunwanted\n \ntraining\n \ndata,\n \nsuch\n \nas\n \nlay\n \ntheories\n \nabout\n \nhuman\n \nbehavior,\n \npast\n \nresearch\n \nfindings,\n \nor\n \nprescriptive\n \nnorms\n \nabout\n \nhuman\n \nbehavior\n \n(i.e.,\n \nopinions\n \nor\n \nguidelines\n \non\n \nhow\n \nhumans\n \nshould\n \nact,\n \neither\n \nembedded\n \nin\n \nthe\n \ntraining\n \ndata\n \nor\n \nthrough\n \nexplicit\n \ninstructions\n \nand\n \nguardrails).\n \n \nThis  concern  can  be  addressed  by  creating  completely  novel  stimuli  or  by  applying  \n“unlearning”\n \ntechniques,\n \nwhich\n \naim\n \nto\n \nremove\n \nthe\n \nimpact\n \nof\n \n“target\n \ndata”,\n \nsuch\n \nas\n \ncopy-righted\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  34   \nmaterial,  survey  responses,  psychological  tests,  or  benchmark  datasets,  from  the  model’s  output  \ngeneration.\n \nNote\n \nthat\n \nmost\n \nof\n \nthese\n \napproaches\n \nnecessitate\n \naccess\n \nto\n \nthe\n \nmodel’s\n \nparameters\n \nand\n \nthus\n \n(currently)\n \nneed\n \nto\n \nbe\n \nconducted\n \nwith\n \nopen-source\n \nmodels.\n \nFor\n \nexample,\n \nsome\n \napproaches\n \nrequire\n \naccess\n \nto\n \nthe\n \nmodel’s\n \nprobability\n \ndistribution\n \nover\n \nthe\n \npossible\n \noutputs\n \nto\n \n“recalibrate”\n \nthe\n \nmodel\n \nweights\n \n(i.e.,\n \nthrough\n \nfine-tuning)\n \nand\n \nremove\n \nthe\n \ninfluence\n \nof\n \nthe\n \nunwanted\n \ntarget\n data  (Eldan  &  Russinovich,  2023; Z.  Liu  et  al.,  2024; Maini  et  al.,  2024; Zhang  et  al.,  2023). Other  methods  require  access  to  the  model  parameters  to  add  “unlearning  layers”  which  are  \ntrained\n \nto\n \nmitigate\n \nthe\n \neffect\n \nof\n \nthe\n \ntarget\n \ndata\n \nwhile\n \nignoring\n \nall\n \nother\n \ntraining\n \ndata\n \n(J.\n \nChen\n \n&\n Yang,  2023). Recently,  there  has  also  been  research  on  unlearning  via  prompting.  These  approaches  add  instructions  to  a  prompt  to  create  a  context  in  which  the  model  does  not  access  the  target  data,  e.g.,  by  contradicting  the  target  information  (Pawelczyk  et  al.,  2023; Thaker  et  al.,  2024). These  approaches  do  not  require  fine-tuning  the  model  and  can  be  applied  to  proprietary  models  or  when  fine-tuning  is  not  feasible.  \nLastly,  researchers  should  be  cautious  about  anthropomorphism  in  LLM  research.  There  \nis\n \nan\n \nemerging\n \nliterature\n \ndiscussing\n \nissues\n \nwith\n \nanthropomorphism\n \nof\n \nLLM\n \nand\n \nother\n \nAI\n \nsystems\n \nand\n \nhow\n \nthis\n \ncan\n \nlead\n \nto\n \na\n \nskewed\n \nperception\n \nof\n \nLLMs\n \nand\n \ntheir\n \nbehaviors\n \n(e.g.,\n \nsee\n \ndiscussions\n \nin\n \nAbdurahman\n \net\n \nal.,\n \n2024;\n \nMesseri\n \n&\n \nCrocket,\n \n2024;\n \nand\n \nShanahan,\n \n2024).\n \nFor\n \nexample,\n \nMesseri\n \n&\n \nCrocket\n \n(2024)\n \nargue\n \nthat\n \nanthropomorphizing\n \nLLMs\n \nas\n \n“scientific\n \nassistants”\n \ncan\n \ncarry\n \nthe\n \nrisk\n \nof\n \nfostering\n \nepistemic\n \ncomplacency\n \nand\n \nillusions\n \nof\n \nunderstanding,\n \npotentially\n \nresulting\n \nin\n \nscientific\n \nmonocultures\n \nthat\n \nsuppress\n \ndiverse\n \nmethods,\n \ncurtail\n \ninnovation,\n \nand\n \nincrease\n \nvulnerability\n \nto\n \nerrors\n \n(relatedly,\n \nsee\n \nDoshi\n \n&\n \nHauser,\n \n2023\n \nfor\n \nan\n \nempirical\n \nstudy\n \nshowing\n \nthat\n \nLLM-Human\n \ninteraction\n \nboosts\n \nindividual\n \nproductivity\n \nand\n \ncreativity\n \nbut\n \nhomogenizes\n \noutputs\n \non\n \nthe\n \ngroup\n \nlevel).\n \n \nShanahan  (2024)  cautions  that  conversational  LLM  agents  can  give  the  illusion  of  \nhuman-like\n \nintelligence,\n \nleading\n \nto\n \nover-\n \nand\n \nunderestimations\n \nof\n \ntheir\n \ncapabilities\n \nby\n \nattributing\n \nembodied\n \nhuman\n \ntraits\n \nthey\n \nmight\n \nnot\n \npossess.\n \nHe\n \nstresses\n \nthe\n \nimportance\n \nof\n \nrecognizing\n \nthat\n \nthese\n \nmodels\n \nare\n \nbased\n \non\n \npredicting\n \nthe\n \nnext\n \nword\n \nin\n \na\n \nsequence\n \nand\n \nwarns\n \nagainst\n \nusing\n \nhuman-centric\n \nterms\n \nto\n \ndescribe\n \nthem.\n \nHowever,\n \nthese\n \nconcepts\n \nmight\n \nbecome\n \nmore\n \nrelevant\n \nas\n \nLLMs\n \nare\n \nintegrated\n \ninto\n \nmore\n \ncomplex\n \nsystems\n \nwith\n \ntools,\n \nmulti-modality\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  35   \n(i.e.,  the  ability  to  engage  with  multiple  modes  of  input  other  than  text  like  images  and  videos),  \nor\n \nembodied\n \nrobotics,\n \nenabling\n \ninteractions\n \nthat\n \nmimic,\n \nfor\n \nexample,\n \nbelief\n \nformation.\n \nThese\n \nissues\n \nshould\n \ncaution\n \nresearchers\n \nwhen\n \nmaking\n \nstrong\n \nclaims\n \nabout\n \nhuman\n \nbehavior\n \nand\n \npsychology\n \nbased\n \non\n \nLLM\n \nbehavior\n \nand\n \nhighlight\n \nthe\n \nneed\n \nfor\n \nstrong\n \nvalidation\n \napproaches\n \n(using\n \nat\n \nleast\n \nsome\n \nhuman\n \ndata)\n \nwhen\n \nmaking\n \nany\n \ninferences\n \nof\n \nhuman\n \nbehavior\n \nfrom\n \nLLM\n \nsimulations.\n \n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  36   \nLimitations  \nWhile  this  primer  addresses  key  considerations  for  using  LLMs  in  social  science  \nresearch,\n \nit\n \nis\n \nimportant\n \nto\n \nacknowledge\n \nthat\n \ncertain\n \nareas\n \nof\n \napplication\n \nmay\n \nrequire\n \nadditional,\n \nspecialized\n \nconsiderations.\n \nFor\n \ninstance,\n \nresearch\n \ninvolving\n \nchatbots\n \nin\n \nmental\n \nhealth\n \nor\n \nparasocial\n \ncontexts\n \nrepresents\n \na\n \ngrowing\n \nintersection\n \nbetween\n \npsychology\n \nand\n \nHuman-Computer\n \nInteraction.\n \nThese\n \nareas\n \nmay\n \ninvolve\n \nunique\n \nethical,\n \nmethodological,\n \nand\n \nclinical\n \nchallenges\n \nthat\n \nextend\n \nbeyond\n \nthe\n \nscope\n \nof\n \nour\n \nrecommendations.\n \nStudies\n \nusing\n \nLLMs\n \nin\n \nclinical\n \nsettings,\n \nsuch\n \nas\n \nmental\n \nhealth\n \nchatbots,\n \nshould\n \nadhere\n \nto\n \nestablished\n \nclinical\n \nresearch\n \nguidelines\n \nin\n \naddition\n \nto\n \nthe\n \nbest\n \npractices\n \noutlined\n \nhere.\n \nResearchers\n \nare\n \nencouraged\n \nto\n \nconsider\n \nguidelines\n \nsuch\n \nas\n \nCONSORT-AI\n \n(Liu\n \net\n \nal.,\n \n2020)\n \nfor\n \nreporting\n \nrandomized\n \nclinical\n \ntrials,\n \nSPIRIT-AI\n \n(Rivera\n \net\n \nal.,\n \n2020)\n \nfor\n \ntrial\n \nprotocols,\n \nTRIPOD+AI\n \n(Collins\n \net\n \nal.,\n \n2024)\n \nfor\n \ndiagnostic\n \nand\n \nprognostic\n \nmodel\n \ndevelopment,\n \nand\n \nCHART\n \n(CHART\n \nCollaborative,\n \n2024)\n \nspecifically\n \ndesigned\n \nfor\n \nchatbot\n \ninterventions.\n \nThese\n \nframeworks\n \nprovide\n \nessential\n \nstandards\n \nfor\n \nensuring\n \nthe\n \nsafety,\n \nefficacy,\n \nand\n \nethical\n \nuse\n \nof\n \nAI-driven\n \ntools\n \nin\n \nclinical\n \ncontexts,\n \nwhich\n \nmay\n \nnot\n \nbe\n \nfully\n \ncovered\n \nby\n \nthe\n \ngeneral\n \nguidelines\n \nfor\n \nsocial\n \nscience\n \nresearch\n \ninvolving\n \nLLMs.\n \nGiven  the  rapid  pace  of  development  of  the  field,  it  is  crucial  to  continually  reassess  and  \nrefine\n \nguidelines\n \nfor\n \nLLM\n \nuse\n \nin\n \nresearch\n \nas\n \nnew\n \napplications\n \nand\n \nchallenges\n \nemerge.\n \nFor\n \nexample,\n \nas\n \nLLM’s\n \nvisual\n \nand\n \naudio\n \nprocessing\n \nimproves,\n \nfuture\n \nguidelines\n \nmight\n \ninclude\n \nrecommendations\n \non\n \nhow\n \nto\n \nhandle\n \nmultimodal\n \ninputs,\n \nsuch\n \nas\n \ncombining\n \ntext,\n \nimages,\n \nand\n \nsounds\n \nin\n \nresearch\n \ntasks.\n \nAs\n \nLLMs\n \nhandle\n \nmore\n \ninformation\n \nwithout\n \nsignificant\n \nperformance\n \nloss,\n \nfuture\n \nguidelines\n \nshould\n \naddress\n \nchallenges\n \nwith\n \ncoding\n \nmassive\n \ndatasets,\n \nlike\n \nlarge-scale\n \nqualitative\n \ndata\n \n(e.g.,\n \nyears\n \nof\n \ndiary\n \nentries\n \nfrom\n \nthousands\n \nof\n \nindividuals).\n \n \nValidating  such  large  outputs  may  pose  new  challenges:  how  can  we  ensure  accuracy  \nwhen\n \nhuman\n \nreview,\n \neven\n \non\n \nsubsets\n \nof\n \nthe\n \ndata,\n \nis\n \nimpractical\n \nor\n \neven\n \nimpossible?\n \nSimilarly,\n \nwhen\n \nLLMs\n \nbecome\n \nviable\n \nfor\n \nlarge-scale\n \ncomplex\n \nsimulations\n \n(e.g.,\n \nvery\n \nlarge\n \nmulti-agent\n \nframeworks),\n \nguides\n \nmay\n \ndistill\n \nand\n \nadopt\n \nmethodologies\n \nfrom\n \nagent-based\n \nmodeling\n \n(ABM)\n \nto\n \nbetter\n \nunderstand\n \nand\n \nvalidate\n \nLLM\n \nperformance\n \n(e.g.,\n \nanalogous\n \nto\n \nclassical\n \nABM\n \nbest\n \npractices\n \nin\n \nHammond,\n \n2015).\n \nWe\n \nrecommend\n \nthat\n \nfuture\n \nupdates\n \nto\n \nthis\n \nprimer\n \nincorporate\n \nspecific\n \nconsiderations\n \nfor\n \nthese\n \nspecialized\n \nareas\n \nto\n \nbetter\n \nsupport\n \nresearchers\n \nand\n \nreviewers\n \nin\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  37   \nmaintaining  the  highest  standards  of  research  quality  and  ethical  practice.   \nConclusion   LLMs  are  powerful  tools  that  can  automate  previously  time-consuming  or  expensive  tasks.  \nThey\n \nhave\n \nthe\n \npotential\n \nto\n \nexpand\n \nthe\n \nscope\n \nof\n \nsocial\n \nscience\n \nresearch\n \nby\n \nallowing\n \nfor\n \nthe\n \nanalysis\n \nof\n \nlarger\n \nqualitative\n \ndatasets\n \nor\n \nfor\n \nsimulating\n \nhuman\n \nbehavior.\n \nAs\n \nLLMs\n \nbecome\n \nmore\n \naccessible\n \nand\n \naffordable,\n \nwe\n \nexpect\n \ntheir\n \nuse\n \nin\n \nresearch\n \nto\n \ngrow,\n \nbut\n \nas\n \nwith\n \nany\n \ntechnology,\n \nLLMs\n \nmust\n \nbe\n \nused\n \nappropriately\n \nwith\n \na\n \nclear\n \nunderstanding\n \nof\n \ntheir\n \nlimitations.\n \nBoth\n \nresearchers\n \nand\n \nreviewers\n \nwill\n \nincreasingly\n \nneed\n \nto\n \nunderstand\n \nappropriate\n \nuse\n \ncases\n \nand\n \nbest\n \npractices\n \nfor\n \nLLMs.\n \nIn  this  article,  we  highlighted  key  processes  for  producing  reliable,  transparent,  and  valid  \nsocial\n \nscience\n \nresearch\n \nusing\n \nLLM\n \ngenerated\n \ndata.\n \nIt\n \nis\n \nimportant\n \nto\n \nkeep\n \nin\n \nmind\n \nthat\n \nthis\n \ntechnology\n \nis\n \nadvancing\n \nrapidly,\n \nand\n \nbest\n \npractices\n \nare\n \nlikely\n \nto\n \nchange\n \nin\n \nthe\n \nfuture.\n \nProviding\n \ndetailed\n \ninformation,\n \nlike\n \ncomputer\n \ncode\n \nand\n \nthorough\n \nexplanations\n \nof\n \nmethods,\n \nis\n \ncrucial\n \nto\n \nhelp\n \nfuture\n \nresearchers\n \nunderstand\n \nand,\n \nif\n \nnecessary,\n \nrevise\n \nresearch\n \nconducted\n \nwith\n \nthe\n \ncurrent\n \ngeneration\n \nof\n \nLLMs.\n \nChallenging\n \nas\n \nit\n \nmay\n \nbe,\n \nthe\n \nbest\n \nauthors\n \nand\n \nreviewers\n \nwill\n \nneed\n \nto\n \nkeep\n \nabreast\n \nof\n \nthe\n \nlatest\n \nguidance\n \nfor\n \nreviewing\n \nLLM\n \nbased\n \nsocial\n \nscience\n \nresearch.\n \nAs\n \nwith\n \nany\n \nnew\n \ntechnology,\n \nLLMs\n \nopen\n \nnew\n \nhorizons\n \nwhile\n \ngenerating\n \nnew\n \npitfalls,\n \nand\n \nthis\n \nprimer\n \naims\n \nto\n \nhelp\n \nresearchers\n \nwho\n \nstudy\n \nhuman\n \nbehavior\n \nbenefit\n \nfrom\n \nthis\n \ntechnology\n \nwhile\n \navoiding\n \nsome\n \nof\n \nthe\n \nmethodological\n \nchallenges\n \nthey\n \npose.\n   \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  38   \nAuthor  Contributions  \nS.A:  Conceptualization,  Methodology,  Investigation,  and  Writing  -  Original  Draft,  Visualization  A.  S.  Z.:  Conceptualization,  Methodology,  Investigation  \nA.  M.:  Conceptualization,  Writing  -  Original  Draft  \nD.  B.:  Conceptualization  \nM.  D.:  Conceptualization,  Supervision  \nConflicts  of  Interest  \nThe  authors  declare  that  there  were  no  conflicts  of  interest  with  respect  to  the  authorship  or  \nthe\n \npublication\n \nof\n \nthis\n \narticle.\n  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  39   \nReferences  Abdurahman,  S.,  Atari,  M.,  Karimi-Malekabadi,  F.,  Xue,  M.  J.,  Trager,  J.,  Park,  P.  S.,  \nGolazizian,\n \nP.,\n \nOmrani,\n \nA.,\n \n&\n \nDehghani,\n \nM.\n \n(2024).\n \nPerils\n \nand\n \nopportunities\n \nin\n \nusing\n \nlarge\n \nlanguage\n \nmodels\n \nin\n \npsychological\n \nresearch.\n \nPNAS\n \nNexus,\n \n3(7).\n \nhttps://doi.org/10.1093/pnasnexus/pgae245\n \n Achiam,  J.,  Adler,  S.,  Agarwal,  S.,  Ahmad,  L.,  Akkaya,  I.,  Aleman,  F.  L.,  Almeida,  D.,  \nAltenschmidt,\n \nJ.,\n \nAltman,\n \nS.,\n \nAnadkat,\n \nS.,\n \net\n \nal.\n \n(2023).\n \nGpt-4\n \ntechnical\n \nreport.\n \narXiv\n \npreprint\n \narXiv:2303.08774.\n Aher,  G.  V.,  Arriaga,  R.  I.,  &  Kalai,  A.  T.  (2023).  Using  large  language  models  to  simulate  \nmultiple\n \nhumans\n \nand\n \nreplicate\n \nhuman\n \nsubject\n \nstudies.\n \nInternational\n \nConference\n \non\n \nMachine\n \nLearning,\n \n337–371.\n Almeida,  G.  F.,  Nunes,  J.  L.,  Engelmann,  N.,  Wiegmann,  A.,  &  de  Araújo,  M.  (2023).  \nExploring\n \nthe\n \npsychology\n \nof\n \ngpt-4’s\n \nmoral\n \nand\n \nlegal\n \nreasoning.\n \narXiv\n \npreprint\n \narXiv:2308.01264.\n Amin,  M.  M.,  Cambria,  E.,  &  Schuller,  B.  W.  (2023).  Will  affective  computing  emerge  from  \nfoundation\n \nmodels\n \nand\n \ngeneral\n \nartificial\n \nintelligence?\n \na\n \nfirst\n \nevaluation\n \nof\n \nchatgpt.\n \nIEEE\n \nIntelligent\n \nSystems,\n \n38\n \n(2),\n \n15–23.\n Argyle,  L.  P.,  Busby,  E.  C.,  Fulda,  N.,  Gubler,  J.  R.,  Rytting,  C.,  &  Wingate,  D.  (2023).  Out  \nof\n \none,\n \nmany:\n \nUsing\n \nlanguage\n \nmodels\n \nto\n \nsimulate\n \nhuman\n \nsamples.\n \nPolitical\n \nAnalysis,\n \n31\n \n(3),\n \n337–351.\n Astekin,  M.,  Hort,  M.,  &  Moonen,  L.  (2024,  April).  An  Exploratory  Study  on  How  \nNon-Determinism\n \nin\n \nLarge\n \nLanguage\n \nModels\n \nAffects\n \nLog\n \nParsing.\n \nIn\n \nProceedings\n \nof\n \nthe\n \nACM/IEEE\n \n2nd\n \nInternational\n \nWorkshop\n \non\n \nInterpretability,\n \nRobustness,\n \nand\n \nBenchmarking\n \nin\n \nNeural\n \nSoftware\n \nEngineering\n \n(pp.\n \n13-18).\n Atari,  M.,  Omrani,  A.,  &  Dehghani,  M.  (2023a).  Contextualized  construct  representation:  \nLeveraging\n \npsychometric\n \nscales\n \nto\n \nadvance\n \ntheory-driven\n \ntext\n \nanalysis.\n Atari,  M.,  Xue,  M.  J.,  Park,  P.  S.,  Blasi,  D.,  &  Henrich,  J.  (2023b).  Which  humans?.  Bai,  H.,  Voelkel,  J.,  Eichstaedt,  J.,  &  Willer,  R.  (2023).  Artificial  intelligence  can  persuade  \nhumans\n \non\n \npolitical\n \nissues.\n Bang,  Y.,  Cahyawijaya,  S.,  Lee,  N.,  Dai,  W.,  Su,  D.,  Wilie,  B.,  ...  &  Fung,  P.  (2023,  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  40   \nNovember).  A  Multitask,  Multilingual,  Multimodal  Evaluation  of  ChatGPT  on  \nReasoning,\n \nHallucination,\n \nand\n \nInteractivity.\n \nIn\n \nProceedings\n \nof\n \nthe\n \n13th\n \nInternational\n \nJoint\n \nConference\n \non\n \nNatural\n \nLanguage\n \nProcessing\n \nand\n \nthe\n \n3rd\n \nConference\n \nof\n \nthe\n \nAsia-Pacific\n \nChapter\n \nof\n \nthe\n \nAssociation\n \nfor\n \nComputational\n \nLinguistics\n \n(Volume\n \n1:\n \nLong\n \nPapers)\n \n(pp.\n \n675-718).\n Beck,  T.,  Schuff,  H.,  Lauscher,  A.,  &  Gurevych,  I.  (2024).  Sensitivity,  performance,  \nrobustness:\n \nDeconstructing\n \nthe\n \neffect\n \nof\n \nsociodemographic\n \nprompting.\n \nProceedings\n \nof\n \nthe\n \n18th\n \nConference\n \nof\n \nthe\n \nEuropean\n \nChapter\n \nof\n \nthe\n \nAssociation\n \nfor\n \nComputational\n \nLinguistics\n \n(Volume\n \n1:\n \nLong\n \nPapers),\n \n2589–2615.\n Binz,  M.,  &  Schulz,  E.  (2023).  Using  cognitive  psychology  to  understand  GPT-3.  \nProceedings\n \nof\n \nthe\n \nNational\n \nAcademy\n \nof\n \nSciences,\n \n120(6),\n \ne2218523120.\n Boyd-Graber,  J.,  Okazaki,  N.,  &  Rogers,  A.  (2023a,  February  25).  ACL  2023  Policy  on  AI  \nWriting\n \nAssistance.\n \nACL\n \n2023.\n \nhttps://2023.aclweb.org/blog/ACL-2023-policy/\n Boyd-Graber,  J.,  Okazaki,  N.,  &  Rogers,  A.  (2023b,  February  25).  ACL’23  Peer  Review  \nform.\n \nACL\n \n2023.\n \nhttps://2023.aclweb.org/blog/review-form/#reproducibility-and-ethics\n \n Brown,  T.,  Mann,  B.,  Ryder,  N.,  Subbiah,  M.,  Kaplan,  J.  D.,  Dhariwal,  P.,  Neelakantan,  A.,  \nShyam,\n \nP.,\n \nSastry,\n \nG.,\n \nAskell,\n \nA.,\n \net\n \nal.\n \n(2020).\n \nLanguage\n \nmodels\n \nare\n \nfew-shot\n \nlearners.\n \nAdvances\n \nin\n \nneural\n \ninformation\n \nprocessing\n \nsystems,\n \n33,\n \n1877–1901.\n CHART  Collaborative.  (2024).  Protocol  for  the  development  of  the  Chatbot  Assessment  \nReporting\n \nTool\n \n(CHART)\n \nfor\n \nclinical\n \nadvice.\n \nBMJ\n \nopen,\n \n14(5),\n \ne081155.\n Chen,  J.,  &  Yang,  D.  (2023,  December).  Unlearn  What  You  Want  to  Forget:  Efficient  \nUnlearning\n \nfor\n \nLLMs.\n \nIn\n \nProceedings\n \nof\n \nthe\n \n2023\n \nConference\n \non\n \nEmpirical\n \nMethods\n \nin\n \nNatural\n \nLanguage\n \nProcessing\n \n(pp.\n \n12041-12052).\n Chen,  L.,  Zaharia,  M.,  &  Zou,  J.  (2023).  Analyzing  chatgpt’s  behavior  shifts  over  time.  \nR0-FoMo:\n \nRobustness\n \nof\n \nFew-shot\n \nand\n \nZero-shot\n \nLearning\n \nin\n \nLarge\n \nFoundation\n \nModels.\n Chiang,  C.  H.,  &  Lee,  H.  Y.  (2023,  July).  Can  Large  Language  Models  Be  an  Alternative  to  \nHuman\n \nEvaluations?.\n \nIn\n \nProceedings\n \nof\n \nthe\n \n61st\n \nAnnual\n \nMeeting\n \nof\n \nthe\n \nAssociation\n \nfor\n \nComputational\n \nLinguistics\n \n(Volume\n \n1:\n \nLong\n \nPapers)\n \n(pp.\n \n15607-15631).\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  41   \nChollet,  F.  (2019).  On  the  measure  of  intelligence.  arXiv  preprint  arXiv:1911.01547.  Chollet,  F.,  Knoop,  M.,  Kamradt,  G.,  &  Landers,  B.  (2024).  Arc  prize  2024:  Technical  \nreport.\n \narXiv\n \npreprint\n \narXiv:2412.04604.\n Coda-Forno,  J.,  Witte,  K.,  Jagadish,  A.  K.,  Binz,  M.,  Akata,  Z.,  &  Schulz,  E.  (2023).  \nInducing\n \nanxiety\n \nin\n \nlarge\n \nlanguage\n \nmodels\n \nincreases\n \nexploration\n \nand\n \nbias.\n \narXiv\n \npreprint\n \narXiv:2304.11111.\n Collins,  G.  S.,  Moons,  K.  G.,  Dhiman,  P.,  Riley,  R.  D.,  Beam,  A.  L.,  Van  Calster,  B.,  ...  &  \nLogullo,\n \nP.\n \n(2024).\n \nTRIPOD+\n \nAI\n \nstatement:\n \nupdated\n \nguidance\n \nfor\n \nreporting\n \nclinical\n \nprediction\n \nmodels\n \nthat\n \nuse\n \nregression\n \nor\n \nmachine\n \nlearning\n \nmethods.\n \nbmj,\n \n385.\n Coyne,  S.,  &  Sakaguchi,  K.  (2023).  An  analysis  of  gpt-3’s  performance  in  grammatical  error  \ncorrection.\n \narXiv\n \npreprint\n \narXiv:2303.14342.\n Dam,  S.  K.,  Hong,  C.  S.,  Qiao,  Y.,  &  Zhang,  C.  (2024).  A  Complete  Survey  on  LLM-based  \nAI\n \nChatbots.\n \narXiv\n \npreprint\n \narXiv:2406.16937.\n Davis,  J.,  Van  Bulck,  L.,  Durieux,  B.  N.,  Lindvall,  C.,  et  al.  (2024).  The  temperature  feature  \nof\n \nchatgpt:\n \nModifying\n \ncreativity\n \nfor\n \nclinical\n \nresearch.\n \nJMIR\n \nHuman\n \nFactors,\n \n11\n \n(1),\n \ne53559.\n Devlin,  J.,  Chang,  M.-W.,  Lee,  K.,  &  Toutanova,  K.  (2018).  Bert:  Pre-training  of  deep  \nbidirectional\n \ntransformers\n \nfor\n \nlanguage\n \nunderstanding.\n \narXiv\n \npreprint\n \narXiv:1810.04805.\n Dillion,  D.,  Tandon,  N.,  Gu,  Y.,  &  Gray,  K.  (2023).  Can  ai  language  models  replace  human  \nparticipants?\n \nTrends\n \nin\n \nCognitive\n \nSciences.\n Doshi,  A.  R.,  &  Hauser,  O.  (2023).  Generative  artificial  intelligence  enhances  creativity.  \nAvailable\n \nat\n \nSSRN.\n Durmus,  E.,  Nyugen,  K.,  Liao,  T.  I.,  Schiefer,  N.,  Askell,  A.,  Bakhtin,  A.,  Chen,  C.,  \nHatfield-Dodds,\n \nZ.,\n \nHernandez,\n \nD.,\n \nJoseph,\n \nN.,\n \net\n \nal.\n \n(2023).\n \nTowards\n \nmeasuring\n \nthe\n \nrepresentation\n \nof\n \nsubjective\n \nglobal\n \nopinions\n \nin\n \nlanguage\n \nmodels.\n \narXiv\n \npreprint\n \narXiv:2306.16388.\n Durrheim,  K.,  Schuld,  M.,  Mafunda,  M.,  &  Mazibuko,  S.  (2023).  Using  word  embeddings  to  \ninvestigate\n \ncultural\n \nbiases.\n \nBritish\n \nJournal\n \nof\n \nSocial\n \nPsychology,\n \n62(1),\n \n617-629.\n Eldan,  R.,  &  Russinovich,  M.  (2023).  Who’s  harry  potter?  approximate  unlearning  in  llms.  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  42   \narXiv  preprint  arXiv:2310.02238.  Ericsson,  K.  A.,  &  Moxley,  J.  H.  (2019).  Thinking  aloud  during  superior  performance  on  \ntasks\n \ninvolving\n \ndecision\n \nmaking\n \n1.\n \nIn\n \nA\n \nhandbook\n \nof\n \nprocess\n \ntracing\n \nmethods\n \n(pp.\n \n286–301).\n \nRoutledge.\n Fix,  E.,  &  Hodges,  J.  L.  (1951).  Discriminatory  analysis.  Nonparametric  discrimination:  \nSmall\n \nsample\n \nperformance.\n \nReport\n \nA,\n \n193008.\n Fujita,  H.,  et  al.  (2022).  Prompt  sensitivity  of  language  model  for  solving  programming  \nproblems.\n \nNew\n \nTrends\n \nin\n \nIntelligent\n \nSoftware\n \nMethodologies,\n \nTools\n \nand\n \nTechniques:\n \nProceedings\n \nof\n \nthe\n \n21st\n \nInternational\n \nConference\n \non\n \nNew\n \nTrends\n \nin\n \nIntelligent\n \nSoftware\n \nMethodologies,\n \nTools\n \nand\n \nTechniques\n \n(SoMeT_22),\n \n355,\n \n346.\n Garg,  N.,  Schiebinger,  L.,  Jurafsky,  D.,  &  Zou,  J.  (2018).  Word  embeddings  quantify  100  \nyears\n \nof\n \ngender\n \nand\n \nethnic\n \nstereotypes.\n \nProceedings\n \nof\n \nthe\n \nNational\n \nAcademy\n \nof\n \nSciences,\n \n115(16),\n \nE3635-E3644.\n Garten,  J.,  Hoover,  J.,  Johnson,  K.  M.,  Boghrati,  R.,  Iskiwitch,  C.,  &  Dehghani,  M.  (2018).  \nDictionaries\n \nand\n \ndistributions:\n \nCombining\n \nexpert\n \nknowledge\n \nand\n \nlarge\n \nscale\n \ntextual\n \ndata\n \ncontent\n \nanalysis:\n \nDistributed\n \ndictionary\n \nrepresentation.\n \nBehavior\n \nresearch\n \nmethods,\n \n50,\n \n344–361.\n Gebru,  T.,  Morgenstern,  J.,  Vecchione,  B.,  Vaughan,  J.  W.,  Wallach,  H.,  Iii,  H.  D.,  &  \nCrawford,\n \nK.\n \n(2021).\n \nDatasheets\n \nfor\n \ndatasets.\n \nCommunications\n \nof\n \nthe\n \nACM,\n \n64(12),\n \n86-92.\n Gilardi,  F.,  Alizadeh,  M.,  &  Kubli,  M.  (2023).  Chatgpt  outperforms  crowd  workers  for  \ntext-annotation\n \ntasks.\n \nProceedings\n \nof\n \nthe\n \nNational\n \nAcademy\n \nof\n \nSciences,\n \n120\n \n(30),\n \ne2305016120.\n Graham,  J.,  Haidt,  J.,  &  Nosek,  B.  A.  (2009).  Liberals  and  conservatives  rely  on  different  \nsets\n \nof\n \nmoral\n \nfoundations.\n \nJournal\n \nof\n \npersonality\n \nand\n \nsocial\n \npsychology,\n \n96\n \n(5),\n \n1029.\n Graham,  J.,  Haidt,  J.,  Koleva,  S.,  Motyl,  M.,  Iyer,  R.,  Wojcik,  S.  P.,  &  Ditto,  P.  H.  (2013).  \nMoral\n \nfoundations\n \ntheory:\n \nThe\n \npragmatic\n \nvalidity\n \nof\n \nmoral\n \npluralism.\n \nIn\n \nAdvances\n \nin\n \nexperimental\n \nsocial\n \npsychology\n \n(Vol.\n \n47,\n \npp.\n \n55-130).\n \nAcademic\n \nPress.\n Guo,  W.,  &  Caliskan,  A.  (2021,  July).  Detecting  emergent  intersectional  biases:  \nContextualized\n \nword\n \nembeddings\n \ncontain\n \na\n \ndistribution\n \nof\n \nhuman-like\n \nbiases.\n \nIn\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  43   \nProceedings  of  the  2021  AAAI/ACM  Conference  on  AI,  Ethics,  and  Society  (pp.  \n122-133).\n Guo,  D.,  Yang,  D.,  Zhang,  H.,  Song,  J.,  Zhang,  R.,  Xu,  R.,  ...  &  He,  Y.  (2025).  Deepseek-r1:  \nIncentivizing\n \nreasoning\n \ncapability\n \nin\n \nllms\n \nvia\n \nreinforcement\n \nlearning.\n \narXiv\n \npreprint\n \narXiv:2501.12948.\n Hammond,  R.  A.  (2015).  Considerations  and  best  practices  in  agent-based  modeling  to  \ninform\n \npolicy.\n \nIn\n \nAssessing\n \nthe\n \nuse\n \nof\n \nagent-based\n \nmodels\n \nfor\n \ntobacco\n \nregulation.\n \nNational\n \nAcademies\n \nPress\n \n(US).\n He,  J.,  Wallis,  F.,  Gvirtz,  A.,  &  Rathje,  S.  (2024).  Artificial  intelligence  chatbots  mimic  \nhuman\n \ncollective\n \nbehaviour.\n Hewitt,  L.,  Ashokkumar,  A.,  Ghezae,  I.,  &  Willer,  R.  (2024).  Predicting  results  of  social  \nscience\n \nexperiments\n \nusing\n \nlarge\n \nlanguage\n \nmodels.\n \nTechnical\n \nreport,\n \nWorking\n \nPaper.\n Huggingface.  (2024a).  Inference.  Huggingface  Hub  Documentation.  \nhttps://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client\n#huggingface_hub.InferenceClient.chat_completion\n \n Huggingface.  (2024b,  July  1).  Open  LLM  Leaderboard  2.  \nhttps://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n \n Horton,  J.  J.  (2023).  Large  language  models  as  simulated  economic  agents:  What  can  we  \nlearn\n \nfrom\n \nhomo\n \nsilicus?\n \n(Tech.\n \nrep.).\n \nNational\n \nBureau\n \nof\n \nEconomic\n \nResearch.\n Hutto,  C.,  &  Gilbert,  E.  (2014).  Vader:  A  parsimonious  rule-based  model  for  sentiment  \nanalysis\n \nof\n \nsocial\n \nmedia\n \ntext.\n \nProceedings\n \nof\n \nthe\n \ninternational\n \nAAAI\n \nconference\n \non\n \nweb\n \nand\n \nsocial\n \nmedia,\n \n8\n \n(1),\n \n216–225.\n Jiang,  A.  Q.,  Sablayrolles,  A.,  Mensch,  A.,  Bamford,  C.,  Chaplot,  D.  S.,  Casas,  D.  d.  l.,  \nBressand,\n \nF.,\n \nLengyel,\n \nG.,\n \nLample,\n \nG.,\n \nSaulnier,\n \nL.,\n \net\n \nal.\n \n(2023).\n \nMistral\n \n7b.\n \narXiv\n \npreprint\n \narXiv:2310.06825.\n Ke,  L.,  Tong,  S.,  Chen,  P.,  &  Peng,  K.  (2024).  Exploring  the  frontiers  of  llms  in  \npsychological\n \napplications:\n \nA\n \ncomprehensive\n \nreview.\n \narXiv\n \npreprint\n \narXiv:2401.01519.\n Kennedy,  B.,  Jin,  X.,  Davani,  A.  M.,  Dehghani,  M.,  &  Ren,  X.  (2020).  Contextualizing  Hate  \nSpeech\n \nClassifiers\n \nwith\n \nPost-hoc\n \nExplanation.\n \nIn\n \nProceedings\n \nof\n \nthe\n \n58th\n \nAnnual\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  44   \nMeeting  of  the  Association  for  Computational  Linguistics.  Association  for  \nComputational\n \nLinguistics.\n Kennedy,  B.,  Reimer,  N.  K.,  &  Dehghani,  M.  (2021).  Explaining  explainability:  Interpretable  \nmachine\n \nlearning\n \nfor\n \nthe\n \nbehavioral\n \nsciences.\n Kobak,  Dmitry,  et  al.  \"Delving  into  ChatGPT  usage  in  academic  writing  through  excess  \nvocabulary.\"\n \narXiv\n \npreprint\n \narXiv:2406.07016\n \n(2024).\n Kumar,  P.  (2024).  Large  language  models  (LLMs):  survey,  technical  frameworks,  and  future  \nchallenges.\n \nArtificial\n \nIntelligence\n \nReview,\n \n57(9),\n \n260.\n Le  Scao,  T.,  Fan,  A.,  Akiki,  C.,  Pavlick,  E.,  Ilić,  S.,  Hesslow,  D.,  Castagné,  R.,  Luccioni,  A.  \nS.,\n \nYvon,\n \nF.,\n \nGallé,\n \nM.,\n \net\n \nal.\n \n(2023).\n \nBloom:\n \nA\n \n176b-parameter\n \nopen-access\n \nmultilingual\n \nlanguage\n \nmodel.\n Liang,  W.,  Zhang,  Y.,  Wu,  Z.,  Lepp,  H.,  Ji,  W.,  Zhao,  X.,  Cao,  H.,  Liu,  S.,  He,  S.,  Huang,  Z.,  \net\n \nal.\n \n(2024).\n \nMapping\n \nthe\n \nincreasing\n \nuse\n \nof\n \nllms\n \nin\n \nscientific\n \npapers.\n \narXiv\n \npreprint\n \narXiv:2404.01268.\n Liesenfeld,  A.,  Lopez,  A.,  &  Dingemanse,  M.  (2023,  July).  Opening  up  ChatGPT:  Tracking  \nopenness,\n \ntransparency,\n \nand\n \naccountability\n \nin\n \ninstruction-tuned\n \ntext\n \ngenerators.\n \nIn\n \nProceedings\n \nof\n \nthe\n \n5th\n \ninternational\n \nconference\n \non\n \nconversational\n \nuser\n \ninterfaces\n \n(pp.\n \n1-6).\n Liu,  X.,  Rivera,  S.  C.,  Moher,  D.,  Calvert,  M.  J.,  Denniston,  A.  K.,  Ashrafian,  H.,  ...  &  Yau,  \nC.\n \n(2020).\n \nReporting\n \nguidelines\n \nfor\n \nclinical\n \ntrial\n \nreports\n \nfor\n \ninterventions\n \ninvolving\n \nartificial\n \nintelligence:\n \nthe\n \nCONSORT-AI\n \nextension.\n \nThe\n \nLancet\n \nDigital\n \nHealth,\n \n2(10),\n \ne537-e548.\n Liu,  H.,  Ning,  R.,  Teng,  Z.,  Liu,  J.,  Zhou,  Q.,  &  Zhang,  Y.  (2023).  Evaluating  the  logical  \nreasoning\n \nability\n \nof\n \nchatgpt\n \nand\n \ngpt-4.\n \narXiv\n \npreprint\n \narXiv:2304.03439.\n Liu,  Z.,  Dou,  G.,  Tan,  Z.,  Tian,  Y.,  &  Jiang,  M.  (2024).  Towards  safer  large  language  models  \nthrough\n \nmachine\n \nunlearning.\n \narXiv\n \npreprint\n \narXiv:2402.10058.\n Liu,  A.,  Feng,  B.,  Xue,  B.,  Wang,  B.,  Wu,  B.,  Lu,  C.,  ...  &  Piao,  Y.  (2024).  Deepseek-v3  \ntechnical\n \nreport.\n \narXiv\n \npreprint\n \narXiv:2412.19437.\n Loya,  M.,  Sinha,  D.,  &  Futrell,  R.  (2023,  December).  Exploring  the  Sensitivity  of  LLMs’  \nDecision-Making\n \nCapabilities:\n \nInsights\n \nfrom\n \nPrompt\n \nVariations\n \nand\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  45   \nHyperparameters.  In  Findings  of  the  Association  for  Computational  Linguistics:  \nEMNLP\n \n2023\n \n(pp.\n \n3711-3716).\n Lu,  Y.,  Bartolo,  M.,  Moore,  A.,  Riedel,  S.,  &  Stenetorp,  P.  (2022,  May).  Fantastically  \nOrdered\n \nPrompts\n \nand\n \nWhere\n \nto\n \nFind\n \nThem:\n \nOvercoming\n \nFew-Shot\n \nPrompt\n \nOrder\n \nSensitivity.\n \nIn\n \nProceedings\n \nof\n \nthe\n \n60th\n \nAnnual\n \nMeeting\n \nof\n \nthe\n \nAssociation\n \nfor\n \nComputational\n \nLinguistics\n \n(Volume\n \n1:\n \nLong\n \nPapers)\n \n(pp.\n \n8086-8098).\n LMSYS.  (2024,  August  27).  Chatbot  Arena  Leaderboard  updates  (Week  4).  LMSYS  Org.  \nhttps://chat.lmsys.org/?leaderboard\n \n Matter,  D.,  Schirmer,  M.,  Grinberg,  N.,  &  Pfeffer,  J.  (2024).  Investigating  the  increase  of  \nviolent\n \nspeech\n \nin\n \nIncel\n \ncommunities\n \nwith\n \nhuman-guided\n \nGPT-4\n \nprompt\n \niteration.\n \nFrontiers\n \nin\n \nSocial\n \nPsychology,\n \n2,\n \n1383152.\n Maini,  P.,  Feng,  Z.,  Schwarzschild,  A.,  Lipton,  Z.  C.,  &  Kolter,  J.  Z.  (2024).  Tofu:  A  task  of  \nfictitious\n \nunlearning\n \nfor\n \nllms.\n \narXiv\n \npreprint\n \narXiv:2401.06121.\n Minaee,  S.,  Mikolov,  T.,  Nikzad,  N.,  Chenaghlu,  M.,  Socher,  R.,  Amatriain,  X.,  &  Gao,  J.  \n(2024).\n \nLarge\n \nlanguage\n \nmodels:\n \nA\n \nsurvey.\n \narXiv\n \npreprint\n \narXiv:2402.06196.\n Mitchell,  M.,  Wu,  S.,  Zaldivar,  A.,  Barnes,  P.,  Vasserman,  L.,  Hutchinson,  B.,  ...  &  Gebru,  T.  \n(2019,\n \nJanuary).\n \nModel\n \ncards\n \nfor\n \nmodel\n \nreporting.\n \nIn\n \nProceedings\n \nof\n \nthe\n \nconference\n \non\n \nfairness,\n \naccountability,\n \nand\n \ntransparency\n \n(pp.\n \n220-229).\n Monniaux,  D.  (2008).  The  pitfalls  of  verifying  floating-point  computations.  ACM  \nTransactions\n \non\n \nProgramming\n \nLanguages\n \nand\n \nSystems\n \n(TOPLAS),\n \n30(3),\n \n1-41.\n Naismith,  B.,  Mulcaire,  P.,  &  Burstein,  J.  (2023).  Automated  evaluation  of  written  discourse  \ncoherence\n \nusing\n \ngpt-4.\n \nProceedings\n \nof\n \nthe\n \n18th\n \nWorkshop\n \non\n \nInnovative\n \nUse\n \nof\n \nNLP\n \nfor\n \nBuilding\n \nEducational\n \nApplications\n \n(BEA\n \n2023),\n \n394–403.\n OpenAI.  (2023,  November  5).  Advanced  usage  -  openai  API.  OpenAI  Platform.  \nhttps://platform.openai.com/docs/advanced-usage/reproducible-outputs\n \n OpenAI.  (2024,  September  17).  Introducing  OpenAI  o1-preview.  https://openai.com/index/introducing-openai-o1-preview/ Ouyang,  S.,  Zhang,  J.  M.,  Harman,  M.,  &  Wang,  M.  (2023).  LLM  is  Like  a  Box  of  \nChocolates:\n \nthe\n \nNon-determinism\n \nof\n \nChatGPT\n \nin\n \nCode\n \nGeneration.\n \narXiv\n \npreprint\n \narXiv:2308.02828.\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  46   \nPark,  J.  S.,  O’Brien,  J.,  Cai,  C.  J.,  Morris,  M.  R.,  Liang,  P.,  &  Bernstein,  M.  S.  (2023).  \nGenerative\n \nagents:\n \nInteractive\n \nsimulacra\n \nof\n \nhuman\n \nbehavior.\n \nProceedings\n \nof\n \nthe\n \n36th\n \nAnnual\n \nACM\n \nSymposium\n \non\n \nUser\n \nInterface\n \nSoftware\n \nand\n \nTechnology,\n \n1–22.\n Park,  P.  S.,  Schoenegger,  P.,  &  Zhu,  C.  (2024).  Diminished  diversity-of-thought  in  a  standard  \nlarge\n \nlanguage\n \nmodel.\n \nBehavior\n \nResearch\n \nMethods,\n \n1-17.\n Pawelczyk,  M.,  Neel,  S.,  &  Lakkaraju,  H.  (2023).  In-context  unlearning:  Language  models  \nas\n \nfew\n \nshot\n \nunlearners.\n \narXiv\n \npreprint\n \narXiv:2310.07579.\n Pennebaker,  J.  W.,  Booth,  R.  J.,  &  Francis,  M.  E.  (2007).  Linguistic  inquiry  and  word  count:  \nLiwc\n \n[computer\n \nsoftware].\n \nAustin,\n \nTX:\n \nliwc.\n \nnet,\n \n135.\n Puigcerver,  J.,  Ruiz,  C.  R.,  Mustafa,  B.,  &  Houlsby,  N.  From  Sparse  to  Soft  Mixtures  of  \nExperts.\n \nIn\n \nThe\n \nTwelfth\n \nInternational\n \nConference\n \non\n \nLearning\n \nRepresentations.\n Rathje,  S.,  Mirea,  D.  M.,  Sucholutsky,  I.,  Marjieh,  R.,  Robertson,  C.  E.,  &  Van  Bavel,  J.  J.  \n(2024).\n \nGPT\n \nis\n \nan\n \neffective\n \ntool\n \nfor\n \nmultilingual\n \npsychological\n \ntext\n \nanalysis.\n \nProceedings\n \nof\n \nthe\n \nNational\n \nAcademy\n \nof\n \nSciences,\n \n121(34),\n \ne2308950121.\n Rivera,  S.  C.,  Liu,  X.,  Chan,  A.  W.,  Denniston,  A.  K.,  Calvert,  M.  J.,  Ashrafian,  H.,  ...  &  \nYau,\n \nC.\n \n(2020).\n \nGuidelines\n \nfor\n \nclinical\n \ntrial\n \nprotocols\n \nfor\n \ninterventions\n \ninvolving\n \nartificial\n \nintelligence:\n \nthe\n \nSPIRIT-AI\n \nextension.\n \nThe\n \nLancet\n \nDigital\n \nHealth,\n \n2(10),\n \ne549-e560.\n Santurkar,  S.,  Durmus,  E.,  Ladhak,  F.,  Lee,  C.,  Liang,  P.,  &  Hashimoto,  T.  (2023).  Whose  \nopinions\n \ndo\n \nlanguage\n \nmodels\n \nreflect?\n \nInternational\n \nConference\n \non\n \nMachine\n \nLearning,\n \n29971–30004.\n Sclar,  M.,  Choi,  Y.,  Tsvetkov,  Y.,  &  Suhr,  A.  Quantifying  Language  Models'  Sensitivity  to  \nSpurious\n \nFeatures\n \nin\n \nPrompt\n \nDesign\n \nor:\n \nHow\n \nI\n \nlearned\n \nto\n \nstart\n \nworrying\n \nabout\n \nprompt\n \nformatting.\n \nIn\n \nThe\n \nTwelfth\n \nInternational\n \nConference\n \non\n \nLearning\n \nRepresentations.\n Serrano,  S.,  &  Smith,  N.  A.  (2019,  July).  Is  Attention  Interpretable?.  In  Proceedings  of  the  \n57th\n \nAnnual\n \nMeeting\n \nof\n \nthe\n \nAssociation\n \nfor\n \nComputational\n \nLinguistics\n \n(pp.\n \n2931-2951).\n Suri,  G.,  Slater,  L.  R.,  Ziaee,  A.,  &  Nguyen,  M.  (2024).  Do  large  language  models  show  \ndecision\n \nheuristics\n \nsimilar\n \nto\n \nhumans?\n \na\n \ncase\n \nstudy\n \nusing\n \ngpt-3.5.\n \nJournal\n \nof\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  47   \nExperimental  Psychology:  General.  Swayamdipta,  S.,  Schwartz,  R.,  Lourie,  N.,  Wang,  Y.,  Hajishirzi,  H.,  Smith,  N.  A.,  &  Choi,  \nY.\n \n(2020,\n \nNovember).\n \nDataset\n \nCartography:\n \nMapping\n \nand\n \nDiagnosing\n \nDatasets\n \nwith\n \nTraining\n \nDynamics.\n \nIn\n \nProceedings\n \nof\n \nthe\n \n2020\n \nConference\n \non\n \nEmpirical\n \nMethods\n \nin\n \nNatural\n \nLanguage\n \nProcessing\n \n(EMNLP)\n \n(pp.\n \n9275-9293).\n Tabone,  W.,  &  de  Winter,  J.  (2023).  Using  chatgpt  for  human–computer  interaction  research:  \nA\n \nprimer.\n \nRoyal\n \nSociety\n \nOpen\n \nScience,\n \n10\n \n(9),\n \n231053.\n Thaker,  P.,  Maurya,  Y.,  &  Smith,  V.  (2024).  Guardrail  baselines  for  unlearning  in  llms.  arXiv  \npreprint\n \narXiv:2403.03329.\n Törnberg,  P.  (2023).  Chatgpt-4  outperforms  experts  and  crowd  workers  in  annotating  \npolitical\n \ntwitter\n \nmessages\n \nwith\n \nzero-shot\n \nlearning.\n \narXiv\n \npreprint\n \narXiv:2304.06588.\n Touvron,  H.,  Lavril,  T.,  Izacard,  G.,  Martinet,  X.,  Lachaux,  M.-A.,  Lacroix,  T.,  Rozière,  B.,  \nGoyal,\n \nN.,\n \nHambro,\n \nE.,\n \nAzhar,\n \nF.,\n \net\n \nal.\n \n(2023).\n \nLlama:\n \nOpen\n \nand\n \nefficient\n \nfoundation\n \nlanguage\n \nmodels.\n \narXiv\n \npreprint\n \narXiv:2302.13971.\n Wang,  Y.,  Yao,  Q.,  Kwok,  J.  T.,  &  Ni,  L.  M.  (2020).  Generalizing  from  a  few  examples:  A  \nsurvey\n \non\n \nfew-shot\n \nlearning.\n \nACM\n \ncomputing\n \nsurveys\n \n(csur),\n \n53\n \n(3),\n \n1–34.\n Wang,  Z.  P.,  Bhandary,  P.,  Wang,  Y.,  &  Moore,  J.  H.  (2024).  Using  GPT-4  to  write  a  \nscientific\n \nreview\n \narticle:\n \na\n \npilot\n \nevaluation\n \nstudy.\n \nBioData\n \nMining,\n \n17(1),\n \n16.\n Wang,  A.,  Morgenstern,  J.,  &  Dickerson,  J.  P.  (2024).  Large  language  models  should  not  \nreplace\n \nhuman\n \nparticipants\n \nbecause\n \nthey\n \ncan\n \nmisportray\n \nand\n \nflatten\n \nidentity\n \ngroups.\n \nArXiv\n \npreprint,\n \nabs/2402.01908.\n Wei,  J.,  Wang,  X.,  Schuurmans,  D.,  Bosma,  M.,  Xia,  F.,  Chi,  E.,  ...  &  Zhou,  D.  (2022).  \nChain-of-thought\n \nprompting\n \nelicits\n \nreasoning\n \nin\n \nlarge\n \nlanguage\n \nmodels.\n \nAdvances\n \nin\n \nneural\n \ninformation\n \nprocessing\n \nsystems,\n \n35,\n \n24824-24837.\n Wu,  S.,  Fei,  H.,  Qu,  L.,  Ji,  W.,  &  Chua,  T.  S.  (2023).  Next-gpt:  Any-to-any  multimodal  llm.  \narXiv\n \npreprint\n \narXiv:2309.05519.\n Xiao,  G.,  Liu,  J.,  Zheng,  Z.,  &  Sui,  Y.  (2021,  October).  Nondeterministic  Impact  of  CPU  \nMultithreading\n \non\n \nTraining\n \nDeep\n \nLearning\n \nSystems.\n \nIn\n \nISSRE\n \n(pp.\n \n557-568).\n Xu,  Y.,  Li,  W.,  Vaezipoor,  P.,  Sanner,  S.,  &  Khalil,  E.  B.  LLMs  and  the  Abstraction  and  \nReasoning\n \nCorpus:\n \nSuccesses,\n \nFailures,\n \nand\n \nthe\n \nImportance\n \nof\n \nObject-based\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  48   \nRepresentations.  Transactions  on  Machine  Learning  Research.  Yin,  S.,  Fu,  C.,  Zhao,  S.,  Li,  K.,  Sun,  X.,  Xu,  T.,  &  Chen,  E.  (2023).  A  survey  on  multimodal  \nlarge\n \nlanguage\n \nmodels.\n \narXiv\n \npreprint\n \narXiv:2306.13549.\n Zhang,  D.,  Finckenberg-Broman,  P.,  Hoang,  T.,  Pan,  S.,  Xing,  Z.,  Staples,  M.,  &  Xu,  X.  \n(2023).\n \nRight\n \nto\n \nbe\n \nforgotten\n \nin\n \nthe\n \nera\n \nof\n \nlarge\n \nlanguage\n \nmodels:\n \nImplications,\n \nchallenges,\n \nand\n \nsolutions.\n \narXiv\n \npreprint\n \narXiv:2307.03941.\n Zhao,  Y.,  Zhang,  R.,  Li,  W.,  Huang,  D.,  Guo,  J.,  Peng,  S.,  Hao,  Y.,  Wen,  Y.,  Hu,  X.,  Du,  Z.,  \net\n \nal.\n \n(2024).\n \nAssessing\n \nand\n \nunderstanding\n \ncreativity\n \nin\n \nlarge\n \nlanguage\n \nmodels.\n \narXiv\n \npreprint\n \narXiv:2401.12491.\n Ziems,  C.,  Held,  W.,  Shaikh,  O.,  Chen,  J.,  Zhang,  Z.,  &  Yang,  D.  (2024).  Can  large  language  \nmodels\n \ntransform\n \ncomputational\n \nsocial\n \nscience?\n \nComputational\n \nLinguistics,\n \n1–55.\n  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  49   \nAppendix   In  this  appendix  we  walk  through  a  concrete  example  of  applying  our  checklist  (see  Table  3).  In  the  \ncontext\n \nof\n \na\n \nhypothetical\n \nstudy,\n \nwe\n \nprovide\n \ndetailed\n \nexamples\n \nof\n \nhow\n \neach\n \nof\n \nthe\n \nitems\n \nin\n \nour\n \nchecklist\n \nmight\n \nbe\n \naddressed\n \neffectively.\n  \nExample  Walk-Through:  LLM  Coding  of  Moral  Framing  and  Stance  in  Migration  Debates  \nStudy  Context  \nA\n \nresearcher\n \nwants\n \nto\n \ninvestigate\n \nonline\n \ndebates\n \nregarding\n \npro-immigration\n \nand\n \nanti-immigration\n \nstances,\n \nfocusing\n \non\n \nhow\n \neach\n \ngroup\n \nuses\n \nmoral\n \nlanguage\n \nto\n \nexpress\n \ntheir\n \npositions.\n \nBy\n \nanalyzing\n \nsocial\n \nmedia\n \nposts\n \nusing\n \nmoral\n \nfoundations\n \ntheory\n \n(MFT;\n \nGraham\n \net\n \nal.,\n \n2013),\n \nthe\n \nresearcher\n \naims\n \nto\n \nunderstand\n \nthe\n \nrole\n \nof\n \nmoral\n \nlanguage\n \nin\n \npolarized\n \ndebates\n \nand\n \nshows\n \nhow\n \ndifferent\n \nmoral\n \nfoundations\n \nare\n \nemployed\n \nto\n \nsupport\n \nvarious\n \nstances.\n \nThe  researchers  collected  a  corpus  of  social  media  posts  in  pro-immigration  and  anti-immigration  \ndiscussions.\n \nThey\n \nplan\n \nto\n \nuse\n \nan\n \nLLM\n \nto\n \nclassify\n \neach\n \npost's\n \nstance\n \n(pro-immigration\n \nor\n \nanti-immigration)\n \nand\n \nthe\n \npost's\n \nmoral\n \nframing\n \n(e.g.,\n \nindividualizing\n \nvs.\n \nbinding\n \nvalues).\n \n1.  [Optional]  Pre-register  the  study  \n1.1  Are  the  methods,  including  models,  parameters,  and  validation  strategies,  \npreregistered?\n \n●  Example :  ○  The  researcher  completed  a  preregistration  on  OSF  (Open  Science  Framework),  \nspecifying\n \nthat\n \nthey\n \nwill\n \nuse\n \na\n \nspecific\n \ninstance\n \nof\n \nChatGPT\n \n(GPT-4o;\n \ngpt-4o-2024-08-06)\n \nto\n \nannotate\n \nsocial\n \nmedia\n \nposts.\n \n ○  They  preregister  the  following:  ■   Plan  to  code  each  post's  stance  (pro-immigration,  anti-immigration,  or  \nneutral).\n ■  A  plan  to  analyze  moral  framing  using  MFT  categories  (care/harm,  \nfairness/cheating,\n \nloyalty/betrayal,\n \nauthority/subversion,\n \npurity/degradation).\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  50   \n■  A  plan  for  validating  a  specific  subset  of  LLM  outputs  by  comparing  them  to  \nhuman\n \nannotations.\n ■  A  note  that  they  will  finalize  prompt  design  and  model  settings  by  iterating  on  \na\n \nsmall\n \nsubset\n \nof\n \nthe\n \nhuman\n \nvalidation\n \ndata\n \nto\n \nmaximize\n \naccuracy\n \nand\n \nminimize\n \nbias\n \nbefore\n \ntesting\n \non\n \nthe\n \nfull\n \nvalidation\n \ndataset.\n \n1.2  Does  the  preregistration  allow  for  a  full  understanding  of  the  intended  experimental  \ndesign,\n \ndata\n \nanalysis\n \nplan,\n \nand\n \nhow\n \nresults\n \nwill\n \nbe\n \ninterpreted?\n \n●  Example :  ○  Hypothesis:  The  researchers  provide  a  clear  hypothesis   \n1.  The  researchers  hypothesize  that  pro-  and  anti-immigration  groups  will  frame  \ntheir\n \narguments\n \nusing\n \ndifferent\n \nmoral\n \nfoundations:\n ●  Anti-immigration  using  binding  values  ●  Pro-immigration  using  individualizing  values.  ○  Sampling  for  Validation :  The  researchers  specify  that  they  will  randomly  select  \n1,000\n \nposts\n \nfor\n \nhuman\n \nannotation\n \nto\n \nserve\n \nas\n \nvalidation\n \ndata.\n \n ○  Annotation  with  LLM :  The  researchers  specify  how  the  LLM  will  be  used  to  \nannotate\n \nposts.\n 1.  Classify  posts'  stance  as  pro-immigration,  anti-immigration,  neutral  2.  Classify  posts'  moral  framing  (care,  fairness,  loyalty,  authority,  purity).  ●  LLM  Settings:  The  researchers  specify  relevant  model  settings  and  provide  example  \nprompts\n \nand\n \nprocedures.\n ○  They  will  submit  1  post  at  a  time  to  the  LLM  ○  They  will  set  the  temperature  parameter  to  1  ○  They  provide  examples  of  prompts  they  will  use  ○  They  are  clear  about  the  need  to  iteratively  improve  prompts  and  model  settings  once  \ndata\n \nanalysis\n \nhas\n \nstarted\n ○  Analysis :  The  researchers  specify  exact  analyses  they  will  use  to  analyze  LLM  \noutputs\n 1.  They  will  analyze  the  relationship  between  moral  foundations  and  \nimmigration\n \nstances.\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  51   \n2.  They  specify  statistical  models.  E.g.,  logistic  regression  where  moral  \nfoundations\n \npredict\n \nstance.\n ○  Interpretation :  The  researchers  clearly  state  how  the  statistical  model  outputs  will  be  \ninterpreted\n \nand\n \nhow\n \nthey\n \nwill\n \nsupport/fail\n \nto\n \nsupport\n \nthe\n \nhypothesis.\n 1.  A  positive  coefficient  for  Individualizing  framing  predicting  pro-immigration  \nstance\n \nwill\n \nsupport\n \nthe\n \nhypothesis.\n 2.  A  positive  coefficient  for  Binding  framing  predicting  anti-immigration  stance  \nwill\n \nsupport\n \nthe\n \nhypothesis.\n \n1.3  How  closely  does  the  study  follow  the  preregistered  protocols,  and  are  any  changes  \njustified\n \nwith\n \ntransparent\n \nreasoning?\n \n●  Example :  ○  Adherence :  The  study  followed  the  same  sampling  procedure,  model  usage,  and  \nmoral\n \nfoundation\n \ndefinitions.\n ○  Changes :  Increased  human-validation  sample  from  1,000  to  2,000  posts  to  address  \ndata\n \nimbalance\n \n(e.g.,\n \nnot\n \nenough\n \npro-immigration\n \nposts\n \nin\n \nthe\n \noriginal\n \nsample).\n \nThey\n \ndocument\n \nthis\n \nchange\n \nin\n \nthe\n \nfinal\n \nreport,\n \nexplaining\n \nit\n \nwas\n \nneeded\n \nto\n \nimprove\n \nstatistical\n \npower.\n \nAs\n \nexpected\n \n(and\n \nspecified\n \nin\n \nthe\n \npreregistration)\n \nthey\n \nalso\n \nchanged\n \nthe\n \nprompt\n \nand\n \ntemperature\n \nsetting\n \nfrom\n \nthe\n \nones\n \noriginally\n \npreregistered.\n \n2.  [Required]  Check  if  the  model  is  stable  (i.e.,  does  not  change  over  time)  and  accessible  \n(i.e.,\n \ncan\n \nbe\n \nused\n \nfor\n \nreplication)\n \n2.1  Is  the  model  stable?  \n●  Example :  ○  No,  the  researchers  used  ChatGPT  (GPT-4o),  which  receives  updates  and  might  \nproduce\n \ndifferent\n \noutputs\n \nat\n \ndifferent\n \ntimes.\n \nIf  the  model  is  not  stable:  \n1.  Provide  a  justification  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  52   \n○  Justification :  They  require  GPT-4o's  more  advanced  reasoning  capabilities  and  \nperformance\n \nfor\n \nthe\n \ncomplex\n \nclassifications.\n \nThe\n \nmodel\n \nneeds\n \nto\n \ndetermine\n \nboth\n \nstance\n \nand\n \nmoral\n \nfoundations\n \nused\n \nin\n \nthe\n \ncontext\n \nof\n \nthe\n \nposts\n \nthat\n \nmay\n \nnot\n \nexplicitly\n \nmention\n \nimmigration.\n 2.  Provide  the  model’s  exact  name  and  query  date  ○  Example :  GPT-4o  used  between  2024-09-01  and  2024-09-30,  model  version  \n“gpt-4o-2024-08-06.”\n 3.  Provide  limitations  ○  Example :  Potential  inconsistencies  over  time  due  to  updates.  \n4.  [Optional]  Validate  with  a  stable  model  ○  Example :  The  researcher  might  use  a  stable  open-source  LLM  (e.g.,  LLaMA-3  \nlocked\n \ncheckpoint)\n \non\n \na\n \nsubset\n \nof\n \nthe\n \ndata\n \nto\n \nsee\n \nif\n \nresults\n \nare\n \nroughly\n \ncomparable.\n \n2.2  Is  the  model  accessible?  \n●  Example :  ○  Yes,  GPT-4o  is  accessible  via  the  OpenAI  API,  although  it  requires  an  API  key  and  \nmay\n \nincur\n \ncosts.\n ○  The  researcher  decides  that  GPT-4o  is  the  best  option  given  performance,  but  \nacknowledges\n \nthat\n \nfor\n \nreplication,\n \nthe\n \nmodel\n \nmight\n \nnot\n \nbe\n \naccessible\n \nin\n \nthe\n \nfuture\n \n(e.g.,\n \nif\n \nbeing\n \ndeprecated).\n \nIf  the  model  is  not  accessible:  \n●  Justification  of  exclusion  of  accessible  models :   ○  Not  applicable  here  because  GPT-4o  is  (currently)  accessible.  In  case  the  model  might  \nbe\n \ninaccessible\n \nin\n \nthe\n \nfuture\n \nthe\n \nresearchers\n \nmay\n \nrefer\n \nto\n \ntheir\n \nvalidation\n \nusing\n \na\n \nstable\n \nopen-source\n \nmodel.\n \n3.  [Required]  Provide  all  materials  for  replication  \n3.1  Codes:  The  researcher  provides  all  codes  and  data  in  a  GitHub  repository  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  53   \n3.2  Model  Parameters  and  Settings  \n Example:  The  researcher  provides  all  relevant  parameters  and  model  settings.  \n●  Model :  gpt-4o-2024-08-06  ●  Temperature :  0.7  ●  Other  relevant  parameters :  set  to  default  \n3.3  Prompts:  The  researchers  iterated  their  prompt  design  on  a  small  subset  of  the  validation  data  \nuntil\n \nthey\n \nfound\n \na\n \nwell\n \nperforming\n \nprompt\n \nand\n \nmodel\n \nsettings.\n \nE.g.,\n \nthey\n \nfound\n \na\n \nfew-shot\n \nprompt\n \ndesign\n \n(includes\n \nexamples\n \nof\n \nthe\n \nrespective\n \nclassifications)\n \nthat\n \nperformed\n \nwell.\n \nThey\n \nprovide\n \nthe\n \nfinal\n \nprompts\n \nused\n \nin\n \ntheir\n \nstudy.\n \nBelow\n \nwe\n \npresent\n \ntheir\n \nprompts\n \nfor\n \nclassifying\n \nposts\n \nbased\n \non\n \nMFT\n \nand\n \nclassifying\n \nposts\n \nbased\n \non\n \nimmigration\n \nstance:\n \nMFT  Classifier:  \n●  System  Prompt :  “You  are  a  helpful  classifier.”  ●  User  Prompt :  \nYou\n \nare\n \na\n \ntext\n \nclassifier\n \ndesigned\n \nto\n \nanalyze\n \ncontent\n \nbased\n \non\n \nMoral\n \nFoundations\n \nTheory\n \n(MFT).\n \nYour\n \ntask\n \nis\n \nto\n \ndetermine\n \nif\n \nthe\n \nfollowing\n \npost\n \nexpresses\n \none\n \nor\n \nmore\n \nof\n \nthe\n \nfollowing\n \nmoral\n \nvalues:\n \n \nCare/Harm:\n \nConcern\n \nfor\n \nthe\n \nwell-being\n \nof\n \nothers,\n \npreventing\n \nharm,\n \nor\n \nalleviating\n \nsuffering.\n \nFairness/Cheating:\n \nFocus\n \non\n \njustice,\n \nrights,\n \nand\n \nequality,\n \nor\n \ncondemnation\n \nof\n \nunfair\n \npractices.\n \n \nLoyalty/Betrayal:\n \nEmphasis\n \non\n \nallegiance,\n \nloyalty\n \nto\n \na\n \ngroup,\n \nor\n \nbetrayal\n \nof\n \none's\n \ncommunity.\n \nAuthority/Subversion:\n \nRespect\n \nfor\n \ntraditions,\n \nrules,\n \nor\n \nsocial\n \nhierarchy,\n \nor\n \nrejection\n \nof\n \nsuch\n \nauthority.\n \nPurity/Degradation:\n \nImportance\n \nof\n \npurity,\n \nsanctity,\n \nor\n \nrejection\n \nof\n \nthings\n \nconsidered\n \nimpure\n \nor\n \ndegrading.\n \nHere\n \nis\n \nthe\n \npost:\n \n[POST\n \nTEXT]\n \nReturn\n \nall\n \nexpressed\n \nvalues\n \ncomma\n \nseparated\n \nand\n \nnothing\n \nelse.\n \nStance  Classifier:  \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  54   \n●  System  Prompt :  “You  are  a  helpful  classifier.”  ●  User  Prompt :  \nYou\n \nare\n \na\n \ntext\n \nclassifier\n \ndesigned\n \nto\n \nanalyze\n \ncontent\n \nrelated\n \nto\n \nimmigration\n \nstances.\n \nYour\n \ntask\n \nis\n \nto\n \ndetermine\n \nif\n \nthe\n \nfollowing\n \npost\n \nexpresses\n \nsupport\n \nfor\n \neither\n \nthe\n \npro-immigration\n \nor\n \nanti-immigration\n \nperspective,\n \nor\n \nif\n \nno\n \nclear\n \nstance\n \nis\n \ndetectable.\n \n \nHere  is  an  example  of  an  \"Anti-Immigration\"  (the  post  supports  restricting  immigration  or  \nopposes\n \nimmigration)\n \npost:\n \n[EXAMPLE],\n \n \nHere  is  an  example  of  a  \"Pro-Immigration\"  (the  post  supports  immigration  rights  or  favors  \nimmigration)\n \npost:\n \n[EXAMPLE],\n \n \nHere  is  an  example  of  an  \"Unclear/Neutral\"  (the  post  does  not  clearly  express  support  for  \neither\n \nperspective)\n \npost:\n \n[EXAMPLE].\n \nHere  is  the  post:  [POST  TEXT]  \nReturn  only  the  expressed  stance  (either  \"pro-immigration\",  \"anti-immigration\",  \"neutral\").  \n3.4  Data  for  Fine-tuning  \n●  In  this  study,  no  fine-tuning  was  performed.  If  fine-tuning  was  used,  they  would  provide  the  \ntraining\n \ndata\n \n(e.g.,\n \na\n \njson\n \nor\n \ncsv\n \nfile)\n \nand\n \ncomplete\n \ncode\n \nto\n \nexecute\n \nthe\n \nfine-tuning\n \nsteps.\n \n3.5  Any  Other  Study  Material  (e.g.,  questionnaires,  human  validation  data)  \n●  Example :  The  researchers  provide  a  folder  with  the  human  annotations  used  to  determine  \naccuracy,\n \nincluding\n \nthe\n \ncodebook\n \nthat\n \ninstructed\n \nthe\n \nhuman\n \nannotators.\n ●  If  relevant,  the  researcher  also  provides  annotators  demographics,  e.g.,  to  study  annotator  \nbias.\n \n3.6  Ensure  code  runs  without  errors  \n●  The  repository  contains  all  code  files  used  to  pre-process  the  social  media  data,  classify  it  \nusing\n \nthe\n \nLLM,\n \nand\n \nrun\n \nthe\n \nstatistical\n \nanalysis\n \nin\n \na\n \nstreamlined\n \nmanner\n \n(e.g.,\n \na\n \nsingle\n \nfile\n \nthat\n \nruns\n \neverything\n \nsequentially,\n \nor\n \ninstructions\n \nhow\n \nto\n \nrun\n \neach\n \nindividual\n \ncode\n \nfile).\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  55   \n●  The  repository  contains  instructions  to  create  a  local  programming  environment  and  install  \nall\n \nnecessary\n \npackages.\n ●  The  code  has  been  tested  in  a  local  environment  with  the  required  libraries  installed  and  \nproduces\n \nthe\n \nsame\n \nresults\n \nas\n \nreported\n \n(or\n \nwithin\n \nreasonable\n \nmargins).\n \n3.7  Ensure  replicated  results  align  with  reported  results  by  a  reasonable  margin  \n●  The  model  is  run  multiple  times  on  the  same  dataset.  Outputs  may  vary  slightly,  but  \naggregated\n \nresults\n \n(e.g.,\n \nfrequency\n \ncounts\n \nof\n \nmoral\n \nfoundations)\n \nremain\n \nwithin\n \n±5%.\n \n3.8  Discuss  strategies  to  account  for  LLM  randomness  \n1.  Aggregation  type :  ○  They  run  the  model  5  times  on  each  post  and  take  the  majority  vote  for  final  \nclassification.\n 2.  Justification  for  aggregation :  ○  This  reduces  the  impact  of  single-run  randomness  and  yields  more  consistent  outputs.  \n4.  [Required]  Ensure  the  reported  results  and  inferences  are  justified  \n4.1  Were  the  LLM  outputs  validated  with  human  data  or  other  justifiable  data?  \n●  Example :  ○  Yes,  they  compared  LLM  outputs  on  1,000  randomly  selected  posts  to  human  \nannotators.\n \nThey\n \nreport\n \nan\n \naccuracy\n \nof\n \n90%,\n \nF1-score\n \nof\n \n0.85,\n \nand\n \nCohen’s\n \nκ\n \nof\n \n0.75,\n \nindicating\n \nsubstantial\n \nagreement\n \nwith\n \nhuman\n \nannotators.\n \nIs  the  achieved  accuracy  sufficient?  \n●  Yes,  it  is  comparable  to  typical  human-human  agreement  in  stance  detection.  \nIs  the  achieved  accuracy  discussed  (e.g.,  comparison  to  other  methods)?  \n●  The  researchers  include  a  brief  discussion  of  the  model’s  performance,  e.g.,  the  high  \nagreement\n \nwith\n \nhuman\n \ncoders,\n \nand\n \nhigh\n \naccuracy\n \ncompared\n \nto\n \nalternative\n \nmethods\n \n(e.g.,\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  56   \ntypical  accuracy/F1  scores  in  stance  detection)  \n4.2  Does  the  research  question  require  robustness  to  different  prompt  strategies  and  model  \nsettings?\n \nIf  yes,  are  LLM  outputs  robust  to  prompting  strategies  and  model  settings?  \n●  Robustness  of  prompting  strategies  and  model  settings  is  not  critical  since  the  main  goal  is  to  \nautomate\n \nhuman\n \ncoding.\n \nThe\n \nresearchers\n \niterated\n \non\n \na\n \nsmall\n \nsubset\n \nof\n \nthe\n \nvalidation\n \ndata–treated\n \nas\n \na\n \n“training\n \nset”\n \nfor\n \nprompt\n \ndevelopment,\n \nanalogous\n \nto\n \nstandard\n \ntrain/test\n \nsplits\n \nin\n \nmachine\n \nlearning–to\n \nfind\n \nthe\n \noptimal\n \nprompt\n \nand\n \nmodel\n \nsettings\n \nthat\n \nmaximize\n \naccuracy\n \nand\n \nminimize\n \nbias.\n \nThis\n \napproach\n \navoids\n \noverfitting\n \nprompting\n \nstrategies\n \nto\n \nthe\n \nfull\n \nvalidation\n \nset,\n \npreventing\n \nreporting\n \ninflated\n \nLLM\n \nperformance\n \nthat\n \nmight\n \nnot\n \ngeneralize.\n \nIf  not,  is  the  applied  prompt  strategy  and  the  model  settings  justified?  \n●  The  LLM  is  used  to  automate  human  coders  that  would  be  significantly  more  expensive  and  \nslower.\n \nAs\n \nsuch\n \nthe\n \nprime\n \nconcern\n \nis\n \nachieving\n \nsufficient\n \naccuracy\n \ncompared\n \nto\n \nhuman\n \nraters.\n \nA\n \nsecondary\n \nconcern\n \nis\n \nensuring\n \nthat\n \nthe\n \nmodels\n \nmisclassifications\n \nare\n \nnot\n \nbiasing\n \nthe\n \ninterpretations\n \nof\n \nthe\n \nhypothesis.\n \n●  Example :  ○  The  researchers  used  a  small  subset  of  the  human  data  to  find  the  most  accurate  \nprompt\n \nand\n \nmodel\n \nsettings\n \nand\n \nthen\n \nverified\n \naccuracy\n \nand\n \nbias\n \non\n \nthe\n \nfull\n \nhuman\n \nvalidation\n \ndata.\n ■  The  model  achieved  a  high  accuracy  (90%)  and  F1-score  (0.85),  and  a  \nCohen’s\n \nκ\n \ncomparable\n \nto\n \nhuman\n \nannotators.\n ■  Using  the  human  ground  truth  data,  the  researchers  check  that  the  model’s  \nmisclassifications\n \nare\n \nnot\n \nmore\n \nfrequent\n \nfor\n \nany\n \nclassification\n \nclass.\n ■  Using  the  human  ground  truth  data,  the  researchers  check  that  the  model's  \nmisclassifications\n \nare\n \nnot\n \nmore\n \nfrequent\n \nfor\n \nany\n \nclassification\n \nclass.\n ■  E.g.,  they  report  the  F1  scores  for  each  class  (e.g.,  pro-immigration  vs  \nanti-immigration),\n \nand\n \nmoral\n \nframing\n \n(e.g.,\n \ncare,\n \nfairness,\n \nauthority,\n \nloyalty,\n \npurity)\n \nfinding\n \nthat\n \nthere\n \nare\n \nno\n \nsignificant\n \ndiscrepancies\n \nacross\n \nclasses.\n \nEVALUATING  LARGE  LANGUAGE  MODELS  IN  SOCIAL  SCIENCE  RESEARCH  57   \n●  If  they  found  discrepancies  they  need  further  test  to  determine  whether  \nthese\n \ninfluence\n \nthe\n \ninterpretation\n \nof\n \ntheir\n \nresults.\n \nE.g.,\n \nif\n \npro-immigration\n \npost\n \nwith\n \nBinding\n \nframing\n \nare\n \nsignificantly\n \nmore\n \noften\n \nmisclassified\n \nthan\n \nIndividualizing\n \nframing,\n \nthis\n \nshould\n \ncaution\n \nthem\n \nfrom\n \ninterpreting\n \nthat\n \npro-immigration\n \nposts\n \nare\n \nmore\n \nlikely\n \nto\n \nbe\n \nframed\n \nwith\n \nIndividualizing\n \nframing\n \nbecause\n \nthis\n \ncould\n \nbe\n \nan\n \nartifact\n \nof\n \nthe\n \nLLM\n \nbias/errors.\n \n5.  Evaluate  data  processing  and  error  handling  \n5.1  Is  the  data  processing  and  error  handling  reasonable?  \n●  Example :  ○  The  researcher  removed  duplicate  posts  and  excluded  those  with  fewer  than  15  \ncharacters.\n ○  If  the  model  outputs  an  invalid  classification  (e.g.,  not  one  of  the  specified  \ncategories),\n \nthe\n \nclassification\n \nis\n \nre-attempted\n \nup\n \nto\n \nthree\n \ntimes.\n \nIf\n \nit\n \nstill\n \nfails,\n \nthe\n \npost\n \nis\n \ndropped.\n ○  They  report  the  frequency  of  invalid  classifications  with  the  final  prompt.  \n5.2  Is  the  data  processing  and  error  handling  biased  toward  the  desired  outcomes?  \n●  Example :  ○  The  outlier  exclusion  is  based  on  post  length  and  unparseable  responses,  not  on  stance  \nor\n \nmoral\n \ncontent.\n ○  They  confirm  no  correlation  between  invalid  classifications  and  stance  or  moral  \nframing\n \nto\n \navoid\n \nskewed\n \ninference\n \n(e.g.,\n \npro-immigration\n \nor\n \nanti-immigration\n \ncontent\n \nwas\n \nmore\n \nlikely\n \nto\n \nbe\n \nexcluded\n \nwhich\n \ncould\n \naffect\n \ndownstream\n \nstatistical\n \nanalysis).\n ",
  "topic": "Psychological research",
  "concepts": [
    {
      "name": "Psychological research",
      "score": 0.5625722408294678
    },
    {
      "name": "Psychological science",
      "score": 0.4810183644294739
    },
    {
      "name": "Psychology",
      "score": 0.47836416959762573
    },
    {
      "name": "Computer science",
      "score": 0.3943166732788086
    },
    {
      "name": "Data science",
      "score": 0.37053990364074707
    },
    {
      "name": "Social psychology",
      "score": 0.14621317386627197
    }
  ]
}