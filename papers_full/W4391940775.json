{
  "title": "Can we Soft Prompt LLMs for Graph Learning Tasks?",
  "url": "https://openalex.org/W4391940775",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5090431203",
      "name": "Zheyuan Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100849212",
      "name": "Xiaoxin He",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055761984",
      "name": "Yijun Tian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5068157871",
      "name": "Nitesh V. Chawla",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6601865935",
    "https://openalex.org/W4393147025",
    "https://openalex.org/W4393152682",
    "https://openalex.org/W4221154673"
  ],
  "abstract": "Graph plays an important role in representing complex relationships in\\nreal-world applications such as social networks, biological data and citation\\nnetworks. In recent years, Large Language Models (LLMs) have achieved\\ntremendous success in various domains, which makes applying LLMs to graphs\\nparticularly appealing. However, directly applying LLMs to graph modalities\\npresents unique challenges due to the discrepancy and mismatch between the\\ngraph and text modalities. Hence, to further investigate LLMs' potential for\\ncomprehending graph information, we introduce GraphPrompter, a novel framework\\ndesigned to align graph information with LLMs via soft prompts. Specifically,\\nGraphPrompter consists of two main components: a graph neural network to encode\\ncomplex graph information and an LLM that effectively processes textual\\ninformation. Comprehensive experiments on various benchmark datasets under node\\nclassification and link prediction tasks demonstrate the effectiveness of our\\nproposed method. The GraphPrompter framework unveils the substantial\\ncapabilities of LLMs as predictors in graph-related tasks, enabling researchers\\nto utilize LLMs across a spectrum of real-world graph scenarios more\\neffectively.\\n",
  "full_text": "Can we Soft Prompt LLMs for Graph Learning Tasks?\nZheyuan Liu‚àó\nUniversity of Notre Dame\nzliu29@nd.edu\nXiaoxin He‚àó\nNational University of Singapore\nhe.xiaoxin@u.nus.edu\nYijun Tian‚àó\nUniversity of Notre Dame\nyijun.tian@nd.edu\nNitesh V. Chawla\nUniversity of Notre Dame\nnchawla@nd.edu\nABSTRACT\nGraph plays an important role in representing complex relation-\nships in real-world applications such as social networks, biological\ndata and citation networks. In recent years, Large Language Mod-\nels (LLMs) have achieved tremendous success in various domains,\nwhich makes applying LLMs to graphs particularly appealing. How-\never, directly applying LLMs to graph modalities presents unique\nchallenges due to the discrepancy and mismatch between the graph\nand text modalities. Hence, to further investigate LLMs‚Äô poten-\ntial for comprehending graph information, we introduce Graph-\nPrompter, a novel framework designed to align graph information\nwith LLMs via soft prompts. Specifically,GraphPrompter consists of\ntwo main components: a graph neural network to encode complex\ngraph information and an LLM that effectively processes textual\ninformation. Comprehensive experiments on various benchmark\ndatasets under node classification and link prediction tasks demon-\nstrate the effectiveness of our proposed method. TheGraphPrompter\nframework unveils the substantial capabilities of LLMs as predic-\ntors in graph-related tasks, enabling researchers to utilize LLMs\nacross a spectrum of real-world graph scenarios more effectively1.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíNeural networks; Semantic\nnetworks; ‚Ä¢ Information systems ‚ÜíLanguage models.\nKEYWORDS\nLarge Language Models; Graph Neural Networks; Natural Language\nProcessing; Graph Representation Learning\nACM Reference Format:\nZheyuan Liu, Xiaoxin He, Yijun Tian, and Nitesh V. Chawla. 2024. Can we\nSoft Prompt LLMs for Graph Learning Tasks?. In Companion Proceedings\nof the ACM Web Conference 2024 (WWW ‚Äô24 Companion), May 13‚Äì17, 2024,\nSingapore, Singapore. ACM, New York, NY, USA, 4 pages. https://doi.org/10.\n1145/3589335.3651476\n‚àóEqually contributed.\n1Code is available at https://github.com/franciscoliu/graphprompter.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0172-6/24/05\nhttps://doi.org/10.1145/3589335.3651476\n1 INTRODUCTION\nLarge Language Models (LLMs) have demonstrated significant suc-\ncess across various domains [4, 8, 12, 16, 18, 19], largely attributed to\ntheir extensive knowledge memorized during the pretraining phase\nand their exceptional ability to generalize during the fine-tuning\nprocess on diverse textual datasets [2, 7, 9, 13, 14]. This success has\nspurred interest in combining graph neural networks (GNNs) with\nLLMs to enhance their capabilities in understanding and modeling\ngraphs [6, 11, 15, 21, 23], including implementing LLMs as encoders\nto process features within GNNs [3, 5, 10, 22], and employing LLMs\nas aligners with GNNs to enhance performance [1, 20, 24].\nHowever, directly applying LLMs to graph modalities presents\nunique challenges due to the discrepancy and mismatch between\nthe graph and text modalities. For example, existing works primar-\nily map the graph into text, ignoring the irrelevant information\nand noises pertinent to the graphs. This oversight results in an\ninadequate understanding of crucial structural knowledge within\nthe graphs. Furthermore, the challenge is amplified when dealing\nwith large graphs containing thousands or millions of nodes and\nedges, as this complexity hinders LLMs‚Äô ability to grasp intricate\nstructural information.\nThese limitations motivate our investigation into using a graph\nas a soft prompt encoded by GNNs for LLMs in graph learning\ntasks. The intuition behind this approach is that GNN are more\nproficient in aggregating and transforming information from the\nneighbourhood information, which could provide topological in-\nformation to LLMs. Additionally, it can guide the LLM in selecting\nrelevant information from textual input and control the generation\nprocess for token generation. Specifically, in this work, we aim to\nexamine the efficacy of a soft graph prompt in informing the LLM‚Äôs\npredictions. Hence, a natural yet pivotal research question arise:\nCan we soft prompt LLMs for graph leanring tasks?\nTo answer this question, we introduce GraphPrompter, a novel\nframework that combines the strength of GNNs and LLMs to pro-\ncess and comprehend graph-structured data. In particular, Graph-\nPrompter capitalizes on the frozen LLM as a robust feature extrac-\ntor that taps into its vast pretraining knowledge, thus enabling us\nto avoid extensive task-specific fine-tuning. In parallel, the GNN\nworks on the graph to produce node embeddings, which are later\nconcatenated with a prompt instruction to guide LLMs for graph\nlearning tasks. The LLM, due to its powerful autoregressive nature,\ngenerates a language response based on the fused graph and text\ninformation, effectively turning the LLM into a powerful tool for\ngraph understanding tasks. See Figure 1 for an illustration.\narXiv:2402.10359v2  [cs.LG]  16 Mar 2024\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore. Zheyuan Liu, Xiaoxin He, Yijun Tian, and Nitesh V. Chawla\nLarge Language Model\n(Self-Attention Layers)\nLarge Language Model\n(Text Embeder)\nK-Hop Subgraph\nTitle: Total Text A Comprehensive Dataset For Scene Text\nDetection And Recognition.\nAbstract: Text in curve orientation, despite being one of the\ncommon text orientations in real world environment... \nQuestion: Which arXiv CS sub-category does this paper belong to?\nTextual Attributes\nGNN\nProjection Layer\ncs.CV\nFigure 1: Illustration of the proposed GraphPrompter for\nnode classification task. The process involves extracting a k-\nhop subgraph for each node, feeding it into a GNN followed\nby a projection layer. Simultaneously, the textual attributes\nassociated with each node are processed by the text embedder.\nThe resulting node embedding is then concatenated with text\nembeddings, serving as a soft prompt to guide the LLM for\ngraph learning tasks.\nThis hybrid approach is particularly suitable for graphs with\ntextual attributes (i.e., textual graphs), which requires an under-\nstanding of both the textual content and the graph structure, such\nas identifying the subcategory of an academic paper based on its\ncitation network and abstract. We experimentally demonstrate that\nour framework is capable of prompting LLMs for graph learning\ntasks on five benchmark datasets under node classification and\nlink prediction tasks. It showcases the potential for significant\nadvancements in the use of LLMs for complex data structures be-\nyond traditional text, opening up new avenues for research and\napplication in the realm of AI assistants capable of intricate graph\ncomprehension. Our main contributions are as follows:\n‚Ä¢To the best of our knowledge, this is the very first work\ninvestigating whether LLMs can understand graph learning\ntasks via soft prompting.\n‚Ä¢We propose GraphPrompter, a novel plug-and-play frame-\nwork that first employ GNN to get node representations\nfrom the textual graph. Then the obtained embeddings are\nconcatenated with a prompt instruction to guide LLMs for\ngraph learning tasks.\n‚Ä¢Extensive experiments demonstrate the effectiveness of our\nproposed framework under both node classification and link\nprediction tasks across various graph benchmarks.\n2 METHOD\nTo enhance the alignment of graph knowledge with LLM, we in-\ntroduce GraphPrompter, a plug-and-play pipeline that fuses post-\nprocessed node embeddings with LLMs. This integration is designed\nto leverage the rich semantic context LLMs provide, enhancing the\ninterpretability and utility of graph representations, as illustrated\nin Figure 1.\n2.1 Graph Section\nA graph can be represented as ùê∫ = (ùëâ,ùê∏ ), where ùëâ is a set of\nvertices, and ùê∏is a set of edges. For each nodeùë£ùëñ ‚ààùëâ, we define the\n3-hop subgraph ùê∫ùë†ùëñ as the induced subgraph containing all nodes\nwithin three hops from ùë£ùëñ. Subsequently, a GNN is employed to\ncompute the node embeddings ùëãùëñ for each of these subgraphs ùê∫ùë†ùëñ ,\nas follows:\nùëãùëñ = GNN(ùê∫ùë†ùëñ )‚àà Rùëëùëî , (1)\nwhere ùëëùëî is the hidden dimension of the GNN. Each embedding\nùëãùëñ captures the structural information of the corresponding sub-\ngraph, providing a rich representation of the topological features\nassociated with ùë£ùëñ.\nWe employ a projection layer, specifically, a multilayer percep-\ntron (MLP), to align the node embedding with the vector space of\nthe LLM:\nÀÜùëãùëñ = MLP(ùëãùëñ)‚àà Rùëëùëô , (2)\nwhere ùëëùëô is the hidden dimension of the LLM.\nBoth the GNN and the projection layer are trained in an end-to-\nend manner, ensuring that the embeddings ùëãùëñ are discriminative\nfor the downstream task. The rationale behind this approach is\ntwofold: first, to leverage the GNN‚Äôs inherent capacity to aggregate\nand transform local neighborhood information into meaningful\nembeddings; and second, to enable the subsequent integration with\nthe LLM, which interprets these embeddings as a soft prompt in\nconjunction with textual data.\n2.2 LLM Section\nAfter encoding the graph structure, the GraphPrompter proceeds to\nprocess the textual information associated with each node. This is\nwhere the power of the LLM comes into play, as it is kept frozen\nfor efficient fine-tuning while preserving the LLM‚Äôs pre-trained\nknowledge. For each node ùë£ùëñ, given its associated textual attribute\nùëáùëñ (e.g., title and abstract of a paper in a citation graph), we tokenize\nùëáùëñ using the frozen tokenizer of the LLM as follows:\nùëátokens = Tokenizer(ùëáùëñ). (3)\nThe tokenizer converts the text into a sequence of discrete tokens\nùëátokens in the LLM‚Äôs vocabulary. Subsequently, these tokens are\nthen embedded into a continuous space:\nùëáemb = Embed(ùëátokens)‚àà RùëÄ√óùëëùëô , (4)\nwhere ùëÄ is the sequence length and ùëëùëô is the hidden dimension of\nthe LLM. These embeddings are designed to capture the semantic\nmeaning of the text, complementing the structural information\nprovided by the GNN.\nNext, we concatenate the node embeddings and the text em-\nbeddings, denoted as [ÀÜùëãùëñ,ùëáemb], which will go through the self-\nattention layers in the LLM as usual.\nThe motivation for this step is to ensure that the LLM can process\nthe rich semantic content of the text alongside the graph embed-\ndings. By encoding the text attributes into a compatible format, we\naim to capitalize on the LLM‚Äôs advanced understanding of natural\nlanguage, which is crucial for tasks that require a combination of\nstructural and textual data interpretation, such as node classifica-\ntion in citation networks. Here, we point out the key distinction\nbetween node classification and link prediction: node classification\nCan we Soft Prompt LLMs for Graph Learning Tasks? WWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore.\nCora Citeseer Pubmed\nDataset\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nBaselines Ours\n(a) Dense (Title + Abstract)\nCora Citeseer Pubmed\nDataset\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nBaselines Ours (b) Sparse (Title Only)\nFigure 2: Node classification experiments comparing the\nproposed GraphPrompter with baseline methods (i.e., soft\nprompt tuning and fine-tuning). The ùë• axis shows dataset\ntypes, and the ùë¶ axis displays accuracy scores. Figure 2a il-\nlustrates a dense semantic setting including both the title\nand abstract of a paper within the node embeddings, while\nFigure 2b illustrates a sparse semantic setting with title only.\nevaluates information from a single node, whereas link prediction\nconsiders the attributes of two nodes, specifically their origin and\ndestination.\n3 EXPERIMENTS\nIn this section, we conduct extensive experiments to validate the\neffectiveness of the GraphPrompter. Specifically, our experiments\naim to address the central question: Can we soft prompt LLMs\nfor graph learning tasks?\n3.1 Experiment setups\n3.1.1 Datasets and baseline models.Our experiments mainly\nfocus on node classification and link prediction using various graph\nbenchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv, and Ogbn-products.\nFor Ogbn-products, given its substantial scale of 2 million nodes and\n61 million edges, we have employed a node sampling strategy to\nobtain a subgraph containing 54k nodes and 74k edges. We compare\nour approach with three kinds of baseline models: 1) Pure GNN; 2)\nFrozen LLM, which includes zero shot and soft prompt tuning; and\n3) Tuned LLM with LoRA. We employ the LLAMA2-7B [16] as the\nLLM backbone. For the pure GNN method, we use GAT [17]. In the\nLLM-frozen setting, for zero-shot, we use a frozen LLM for direct\nnode classification/edge prediction using textual attributes of the\nnodes. In soft prompt tuning, we keep the parameters of the LLM\nfrozen and adjust only the prompt. In LLM-tuned, the original LLM\nparameters are updated for downstream tasks by utilizing LoRA.\n3.1.2 Implementation Details.We report the mean of five inde-\npendent runs with different seeds. The experiments are conducted\non 2 A100 GPUs (80GB).\n4 EXPERIMENTS RESULT\nTo answer the central question, we conduct experiments across\nvarious graph benchmark to comprehensively evaluate the effec-\ntiveness of GraphPrompter. The performance is reported in Table 1\nand Table 2.\n4.1 Node Classification Task\nIn Table 1, we present the results of node classification experi-\nments conducted on a variety of benchmark datasets, where both\nMethod Cora Citeseer Pubmed Ogbn-arxiv Ogbn-products\nNode Classification Accuracy (%)‚Üë\nGAT 84.69 70.78 84.09 71.82 70.52\nZero-Shot 43.31 29.22 91.39 44.23 15.05\nSoft Prompt Tuning 70.31 70.97 91.45 71.99 75.14\nFine-tuning + LoRA 75.97 73.45 94.68 74.58 78.99\nSubgraph Prompt Tuning80.17 72.29 93.84 75.04 75.30\nGraphPrompter+ LoRA 80.26 73.61 94.80 75.61 79.54\nTable 1: Node classification result of our proposed Graph-\nPrompter, with a number of baselines under various graph\nbenchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv and Ogbn-\nproducts. For each model, we present the mean accuracy\nfrom five independent runs, with both the title and abstract\nof papers provided as input along with specific instructions.\nBold indicates the best performance and underline indicates\nthe runner-up.\ntitle and abstract information are provided to LLM alongside de-\ntailed instructions. As it can be seen from the table, the integration\nof the proposed method (GraphPrompter) with LoRA consistently\nsurpasses the performance of other baseline methods across di-\nverse benchmarks. In particular, GraphPrompter with LoRA in the\nPubMed and Citeseer datasets with top accuracies of 94.80 % and\n73.61 %, respectively. The naive fine-tuning with LoRA approach\ntypically appears as the runner-up and has small performance gap\nwith GraphPrompter. Though subgraph prompt tuning approach\nhas relatively close accuracy score with naive fine-tuning, it still\nnot as effective as fine-tuning on node classification task. Further-\nmore, we observe that zero-shot generally ranks lowest in perfor-\nmance across all benchmark datasets, with the exception of PubMed\ndataset. This reflects the limitation of LLMs in capturing graph\nknowledge without the auxiliary processing provided by GNNs.\nAdditionally, two distinct experimental setups are illustrated in Fig-\nure 2, where Figure 2a encompasses both title and abstract within\nthe node embeddings, and Figure 2b restricts the node embeddings\nto only the title. The comparative analysis illustrated in the figures\nfurther illustrates the superior average performance of the proposed\nmethod (GraphPrompter), which combines subgraph prompt tuning\nand GraphPrompter, in comparison to other baseline approaches,\nspecifically soft prompt tuning and fine-tuning methodologies.\n4.2 Link Prediction Task\nIn Table 2, we present the results of link prediction experiments\nconducted on a variety of benchmark datasets, where only the title\nis provided to LLM. As seen in Table 2, subgraph prompt tuning\napproach consistently surpasses baseline models in link prediction\ntasks on various benchmarks. It shows remarkable performance\nespecially on the Citeseer dataset, achieving a 93.49 % accuracy\nrate. GraphPrompter + LoRA is usually the runner-up model for the\nmajority case. Notably, similar to node classification task, the Zero-\nShot approach generally scores lowest, underscoring the necessity\nof GNN integration for LLMs to effectively process and understand\ngraph data. It is notable that under both node classification and link\nprediction, GraphPrompter can continue outperforming traditional\nGNN and soft prompting techniques. This observation indicates\nthat GraphPrompter, is particularly adept at capturing and utilizing\nthe complex intersection between graph components and textual\ninformation, leading to a better performance on various graph\nrelated tasks.\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore. Zheyuan Liu, Xiaoxin He, Yijun Tian, and Nitesh V. Chawla\nMethod Cora Citeseer Pubmed Ogbn-arxiv Ogbn-products\nLink Prediction Accuracy (%)‚Üë\nGAT 90.71 87.40 86.18 72.93 65.67\nZero-Shot 21.94 7.69 10.23 34.41 7.47\nSoft Prompt Tuning 86.77 88.87 83.98 69.13 62.78\nFine-tuning + LoRA 87.58 87.91 81.33 70.10 67.31\nSubgraph Prompt Tuning89.15 93.49 87.20 75.28 68.18\nGraphPrompter+ LoRA 90.10 91.67 86.49 73.21 69.55\nTable 2: Link prediction result of our proposed Graph-\nPrompter, with a number of baselines under various graph\nbenchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv and Ogbn-\nproducts. For each model, we present the mean accuracy\nderived from five independent runs conducted on the link\nprediction task, with only the abstract of papers are provided\nas input. Bold indicates the best performance and underline\nindicates the runner-up.\n5 ANALYSIS\nThe experiments on benchmark datasets for node classification and\nlink prediction tasks demonstrate LLMs‚Äô adaptability and efficiency\nin graph learning, enabled by our proposed soft graph prompting\nstrategies. Here, we summarize the key findings:\n‚Ä¢Performance Benchmark: Overall, our proposed method\nranks either as the best or runner-up across all benchmark\ndatasets for both tasks. This consistently high performance\naffirms a positive response to our central question: we can\neffectively soft prompt for LLMs in graph learning tasks.\n‚Ä¢Methodological Superiority : The fine-tuning approach,\nwhile competitive, displayed a marginal performance gap\nwhen compared to GraphPrompter. This indicates that while\ntraditional fine-tuning remains effective, the specialized soft\ngraph prompt tuning strategy designed for graph data offers\na more potent solution for enhancing LLM‚Äôs graph learning\ncapabilities.\n‚Ä¢Challenges with Zero-Shot Learning : The zero-shot ap-\nproach consistently ranked the lowest across all benchmarks\nfor both tasks. This reflects the challenges LLMs face in\ncomprehending graph structure and semantics, highlighting\nthe necessity for tailored approaches like GraphPrompter in\ngraph learning tasks.\n6 CONCLUSION\nIn conclusion, our study reveals the significant potential of leverag-\ning LLMs for interpreting graph structures through a prompt tuning\nmethodology. To facilitate this study, we presentsGraphPrompter,\na novel plug-and-play framework that integrates the capabilities\nof Large Language Models with the structural insights of Graph\nNeural Networks for the task of node classification and link pre-\ndiction in graph data. Specifically, GraphPrompter consists of two\nsections: the graph section utilizes GNN for structural insights,\nwhile the text section applies LLM to interpret node-related textual\ndata. Extensive experiments and in-depth studies demonstrate the\nsuperiority of GraphPrompter across multiple benchmark datasets.\nFuture work could focus on extending our pipeline to other more\ncomplex graph level tasks.\nREFERENCES\n[1] William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad\nKabbara, and Deb Roy. 2023. ConGraT: Self-Supervised Contrastive Pretraining\nfor Joint Graph and Text Embeddings. arXiv preprint arXiv:2305.14321 (2023).\n[2] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian\nTramer, and Chiyuan Zhang. 2022. Quantifying memorization across neural\nlanguage models. arXiv preprint arXiv:2202.07646 (2022).\n[3] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang,\nand Yang Yang. 2023. Graphllm: Boosting graph reasoning ability of large lan-\nguage model. arXiv preprint arXiv:2310.05845 (2023).\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\nJournal of Machine Learning Research (2023).\n[5] Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. 2023. Explanations\nas Features: LLM-Based Features for Text-Attributed Graphs. arXiv preprint\narXiv:2305.19523 (2023).\n[6] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann\nLeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-Augmented\nGeneration for Textual Graph Understanding and Question Answering. arXiv\npreprint arXiv:2402.07630 (2024).\n[7] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al. 2022. Training compute-optimal large language models.\narXiv preprint arXiv:2203.15556 (2022).\n[8] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk\nMichalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, et al. 2022. Solving quantitative reasoning problems with language\nmodels. In NeurIPS.\n[9] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n(2022).\n[10] Costas Mavromatis, Vassilis N Ioannidis, Shen Wang, Da Zheng, Soji Adeshina,\nJun Ma, Han Zhao, Christos Faloutsos, and George Karypis. 2023. Train Your\nOwn GNN Teacher: Graph-Aware Distillation on Textual Graphs. arXiv preprint\narXiv:2304.10668 (2023).\n[11] Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, and Georg\nRehm. 2022. Neighborhood contrastive learning for scientific document repre-\nsentations with citation embeddings. arXiv preprint arXiv:2202.06671 (2022).\n[12] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .\n2022. Training language models to follow instructions with human feedback. In\nNeurIPS.\n[13] Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng\nJiang. 2024. Democratizing Large Language Models via Personalized Parameter-\nEfficient Fine-tuning. arXiv preprint arXiv:2402.04401 (2024).\n[14] Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, and Nitesh V Chawla. 2024.\nTinyLLM: Learning a Small Student from Multiple Large Language Models.arXiv\npreprint arXiv:2402.04616 (2024).\n[15] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang,\nNitesh V Chawla, and Panpan Xu. 2024. Graph neural prompting with large\nlanguage models. In AAAI.\n[16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[17] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[18] Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr.\n2024. Knowledge graph prompting for multi-document question answering. In\nAAAI.\n[19] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng\nWang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models with\ngraph augmentation for recommendation. In WSDM. 806‚Äì815.\n[20] Zhihao Wen and Yuan Fang. 2023. Augmenting Low-Resource Text Classi-\nfication with Graph-Grounded Pre-training and Prompting. arXiv preprint\narXiv:2305.03324 (2023).\n[21] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal,\nAmit Singh, Guangzhong Sun, and Xing Xie. 2021. GraphFormers: GNN-nested\ntransformers for representation learning on textual graph. In NeurIPS.\n[22] Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, and Xuecang\nZhang. 2023. Empower text-attributed graphs learning with large language\nmodels (llms). arXiv preprint arXiv:2310.09872 (2023).\n[23] Yu Zhang, Zhihong Shen, Chieh-Han Wu, Boya Xie, Junheng Hao, Ye-Yi Wang,\nKuansan Wang, and Jiawei Han. 2022. Metadata-induced contrastive learning\nfor zero-shot multi-label text classification. In WWW.\n[24] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and\nJian Tang. 2022. Learning on large-scale text-attributed graphs via variational\ninference. arXiv preprint arXiv:2210.14709 (2022).",
  "topic": "Graph",
  "concepts": [
    {
      "name": "Graph",
      "score": 0.6020992994308472
    },
    {
      "name": "Computer science",
      "score": 0.5926244854927063
    },
    {
      "name": "Modalities",
      "score": 0.5809023976325989
    },
    {
      "name": "ENCODE",
      "score": 0.4411580562591553
    },
    {
      "name": "Data science",
      "score": 0.3641848862171173
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3598126173019409
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32471975684165955
    },
    {
      "name": "Gene",
      "score": 0.09341686964035034
    },
    {
      "name": "Social science",
      "score": 0.07750385999679565
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}