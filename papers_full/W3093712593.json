{
  "title": "Transformer-Based End-to-End Speech Recognition with Local Dense Synthesizer Attention",
  "url": "https://openalex.org/W3093712593",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2750410191",
      "name": "Xu Menglong",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A2320841813",
      "name": "LI Shengqiang",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A3203200532",
      "name": "Zhang, Xiao-Lei",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2981857663",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6774054309",
    "https://openalex.org/W6757585730",
    "https://openalex.org/W3106504817",
    "https://openalex.org/W6776684981",
    "https://openalex.org/W3015966793",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W2963827914",
    "https://openalex.org/W2962778134",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6769806307",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W3015974384",
    "https://openalex.org/W6687566353",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W6746574493",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2976556660",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W3097874139",
    "https://openalex.org/W2939111082",
    "https://openalex.org/W4294619417",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W3007328579",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3024732798",
    "https://openalex.org/W2982413405",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W3006801027",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W4288088457",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964272710",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963240019"
  ],
  "abstract": "Recently, several studies reported that dot-product selfattention (SA) may not be indispensable to the state-of-theart Transformer models. Motivated by the fact that dense synthesizer attention (DSA), which dispenses with dot products and pairwise interactions, achieved competitive results in many language processing tasks, in this paper, we first propose a DSA-based speech recognition, as an alternative to SA. To reduce the computational complexity and improve the performance, we further propose local DSA (LDSA) to restrict the attention scope of DSA to a local range around the current central frame for speech recognition. Finally, we combine LDSA with SA to extract the local and global information simultaneously. Experimental results on the Ai-shell1 Mandarine speech recognition corpus show that the proposed LDSA-Transformer achieves a character error rate (CER) of 6.49%, which is slightly better than that of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less computation than the SATransformer. The proposed combination method not only achieves a CER of 6.18%, which significantly outperforms the SA-Transformer, but also has roughly the same number of parameters and computational complexity as the latter. The implementation of the multi-head LDSA is available at https://github.com/mlxu995/multihead-LDSA.",
  "full_text": "TRANSFORMER-BASED END-TO-END SPEECH RECOGNITION WITH LOCAL DENSE\nSYNTHESIZER ATTENTION\nMenglong Xu, Shengqiang Li, Xiao-Lei Zhang\nCIAIC, School of Marine Science and Technology, Northwestern Polytechnical University, China\n{mlxu, shengqiangli}@mail.nwpu.edu.cn, xiaolei.zhang@nwpu.edu.cn\nABSTRACT\nRecently, several studies reported that dot-product self-\nattention (SA) may not be indispensable to the state-of-the-\nart Transformer models. Motivated by the fact that dense\nsynthesizer attention (DSA), which dispenses with dot prod-\nucts and pairwise interactions, achieved competitive results\nin many language processing tasks, in this paper, we ﬁrst\npropose a DSA-based speech recognition, as an alterna-\ntive to SA. To reduce the computational complexity and\nimprove the performance, we further propose local DSA\n(LDSA) to restrict the attention scope of DSA to a local\nrange around the current central frame for speech recogni-\ntion. Finally, we combine LDSA with SA to extract the\nlocal and global information simultaneously. Experimental\nresults on the Ai-shell1 Mandarin speech recognition cor-\npus show that the proposed LDSA-Transformer achieves\na character error rate (CER) of 6.49%, which is slightly\nbetter than that of the SA-Transformer. Meanwhile, the\nLDSA-Transformer requires less computation than the SA-\nTransformer. The proposed combination method not only\nachieves a CER of 6.18%, which signiﬁcantly outperforms\nthe SA-Transformer, but also has roughly the same number\nof parameters and computational complexity as the latter.\nThe implementation of the multi-head LDSA is available at\nhttps://github.com/mlxu995/multihead-LDSA\nIndex Terms— End-to-end, speech recognition, Trans-\nformer, dense synthesizer attention\n1. INTRODUCTION\nIn recent years, end-to-end (E2E) automatic speech recogni-\ntion (ASR) [1, 2, 3, 4, 5, 6] has been widely studied in the\nASR community due to its simpliﬁed model structure as well\nas its simple training and inference pipelines. Among various\nE2E models, Transformer-based ASR [7, 8, 9, 10, 11] has re-\nceived more and more attention for its high accuracy and efﬁ-\ncient training procedure. The core component of the state-of-\nthe-art Transformer-based models is a so-called self-attention\nmechanism [12], which uses dot products to calculate atten-\ntion weights. Although the content-based dot-product self-\nattention is good at capturing global interactions, it makes the\ncomputational complexity of the self-attention (SA) layer be\nquadratic with respect to the length of the input feature.\nTherefore, there is a need to reduce the complexity of the\nSA layer. Fortunately, several recent studies in natural lan-\nguage processing simpliﬁed the expensive dot-product self-\nattention [13, 14, 15, 16, 17]. Speciﬁcally, In [15], SA was re-\nplaced with a so-called dynamic convolution. It uses an addi-\ntional linear layer to predict normalized convolution weights\ndynamically at each convolution step. In [16], Raganatoet al.\nreplaced all but one attention heads with simple ﬁxed (non-\nlearnable) attention patterns in Transformer encoders. In [17],\nTay et al.proposed dense synthesizer attention (DSA), which\nuses two feed-forward layers to predict the attention weights.\nCompared to SA, DSA completely dispenses with dot prod-\nucts and explicit pairwise interactions. It achieves competi-\ntive results with SA across a number of language processing\ntasks.\nHowever, it is not easy to replace SA by DSA in ASR.\nFirst, the length of the attention weights predicted by DSA\nis ﬁxed. If we apply DSA directly to ASR, then the spectro-\ngram of each utterance has to be padded to the length of the\nlongest utterance of the training corpus, which unnecessar-\nily consumes quite long time and large storage space. More-\nover, the length of the feature in an ASR task is much longer\nthan that in a language model. Predicting attention weights\ndirectly for such a long spectrogram results in a signiﬁcant\nincrease of errors. In addition, like SA, DSA still does not\nhave the ability to extract ﬁne-grained local feature patterns.\nIn this paper, we propose local dense synthesizer atten-\ntion (LDSA) to address the aforementioned three problems\nsimultaneously. In LDSA, the current frame is restricted to\ninteracting with its ﬁnite neighbouring frames only. There-\nfore, the length of the attention weights predicted by LDSA\nis no longer the length of the longest utterance. It is a ﬁxed\nlength controlled by a tunable context width. LDSA not only\nreduces the storage and computational complexity but also\nsigniﬁcantly improves the performance.\nTo evaluate the effectiveness of the LDSA-Transformer,\nwe implemented the DSA-Transformer, LDSA-Transformer,\nand the combination of the LDSA and SA for ASR, where\nwe denote the combined model as hybrid-attention (HA)\nTransformer. Experimental results on the Ai-shell1 Man-\narXiv:2010.12155v3  [cs.SD]  24 Jul 2021\n(a) Self-attention (with 3 heads)\n (b) Dense synthesizer attention\n (c) Local dense synthesizer attention\nFig. 1: Architecture of different attention mechanisms.\ndarin dataset show that the LDSA-Transformer achieves\nslightly better performance with less computation than the\nSA-Transformer. In addition, HA-Transformer achieves a\nrelative character error rate (CER) reduction of 6.8% over the\nSA-Transformer with roughly the same number of parameters\nand computation as the latter.\nThe most related work of LDSA is [18], in which Fujita\net al. applied dynamic convolution [15] to E2E ASR. How-\never, the method [18] is fully convolution-based. It does not\nadopt the SA structure. On the contrary, our model adopts\nthe SA structure instead of the convolution structure. In addi-\ntion, we combine the proposed LDSA with SA by replacing\nthe convolution module in the convolution-augmented Trans-\nformer with LDSA, so as to further model the local and global\ndependencies of an audio sequence simultaneously.\n2. ALGORITHM DESCRIPTION\nIn this section, we ﬁrst brieﬂy introduce the classic dot-\nproduct self-attention and its variant—DSA, and then elabo-\nrate the proposed LDSA.\n2.1. Dot-product self-attention\nThe SA in transformer usually has multiple attention heads.\nAs illustrated in Fig. 1(a), suppose the multi-head SA has h\nheads. It calculates the scaled dot-product attention htimes\nand then concatenates their outputs. A linear projection layer\nis built upon the scaled dot-product attention, which produces\nthe ﬁnal output from the concatenated outputs. Let X ∈\nRT×d be an input sequence, where T is the length of the se-\nquence and dis the hidden size of the SA layer. Each scaled\ndot-product attention head is formulated as:\nAttention(Qi,Ki,Vi) =Softmax\n(QiKT\ni√dk\n)\nVi (1)\nwith\nQi = XWQi , Ki = XWKi , Vi = XWVi (2)\nwhere WQi ,WKi ,WVi ∈Rd×dk denote learnable projec-\ntion parameter matrices for the i-th head, dk = d/his the di-\nmension of the feature vector for each head. The multi-head\nSA is formulated as:\nMultiHead(Q,K,V) =Concat (U1,··· ,Uh) WO (3)\nwhere\nUi = Attention\n(\nXWQi ,XWKi ,XWVi\n) (4)\nand WO ∈Rd×d is the weight matrix of the linear projection\nlayer.\n2.2. Dense synthesizer attention\nAs illustrated in Fig. 1(b), the main difference between DSA\nand SA is the calculation method of the attention weights.\nDense synthesizer attention removes the notion of query-key-\nvalues in the SA module and directly synthesizes the attention\nweights. In practice, DSA adopts two feed-forward layers\nwith ReLU activation to predict the attention weights, which\nis formulated as:\nB = Softmax(σR(XW1)W2) (5)\nwhere σR is the ReLU activation function, and W1 ∈Rd×d\nand W2 ∈Rd×T are learnable weights. The output of DSA\nis calculated by:\nDSA(X) =B(XW3)WO (6)\nwith W3 ∈Rd×d.\n2.3. Proposed local dense synthesizer attention\nMotivated by convolutional neural networks, we propose\nLDSA to address the weaknesses of DSA. LDSA restricts\nthe current frame to interact with its neighbouring frames\nonly. As illustrated in Fig. 1(c), it deﬁnes a hyper-parameter\nc, termed as context width, to control the length of the pre-\ndicted attention weights, and then assign the synthesized\nattention weights to the current frame and its neighboring\nframes, where c = 3 in Fig. 1(c). Attention weights for\nthe other frames outside the context width will be set to 0.\nThe calculation method of B in LDSA is the same as that in\nDSA. However, its time and storage complexities are reduced\nsigniﬁcantly, due to the fact that W2 ∈Rd×c in LDSA. The\noutput of LDSA is calculated by:\nV = XW3 (7)\nYt =\nc−1∑\nj=0\nBt,jVt+j−⌊c\n2 ⌋ (8)\nLDSA(X) =YWO (9)\nBoth DSA and LDSA can be easily extended to a multi-head\nform in a similar way with the dot-product self-attention.\n3. MODEL IMPLEMENTATION\nThis section ﬁrst describes the baseline model, and then\npresents the proposed models.\n3.1. Baseline model: SA-Transformer\nThe SA-Transformer is an improved Speech-transformer [5].\nAs shown in Fig. 2, it consists of an encoder and a decoder.\nThe encoder is composed of a convolution frontend and a\nstack of N = 12identical encoder sub-blocks, each of which\ncontains a SA layer, a convolution layer1 and a position-wise\nfeed-forward layer. For the convolution frontend, we stack\ntwo 3×3 convolution layers with stride 2 for both time dimen-\nsion and frequency dimension to conduct down-sampling on\nthe input features. The decoder is composed of an embedding\nlayer and a stack of M = 6 identical decoder sub-blocks. In\naddition to the position-wise feed-forward layer, the decoder\nsub-block contains two SA layers performing multi-head at-\ntention over the embedded label sequence and the output of\nthe encoder respectively. The output dimension of the SA\nand feed-forward layers are both 320. The number of the at-\ntention heads in each SA layer is 4. Note that we also add\nresidual connection and layer normalization after each layer\nin the sub-blocks.\n3.2. Proposed LDSA-Transformer\nThe LDSA-Transformer has the same decoder as the base-\nline model. It replaces the self-attention mechanism in the\nencoder of the SA-Transformer with LDSA. The number of\n1Unlike Conformer [19], we only added the convolution layer without the\nrelative positional encoding.\nFig. 2: The model architecture of the SA-Transformer.\nheads of LDSA is set to 4. The other layers in the encoder of\nthe LDSA-Transformer are the same as the baseline model.\nAs for the DSA-Transformer, it just changes LDSA in the\nLDSA-transformer to DSA.\n3.3. Proposed HA-Transformer\nThe HA-Transformer is a combination of SA and the pro-\nposed LDSA. Different from the additive operation as [17]\ndid, we combine them in a tandem manner since that LDSA is\nable to extract ﬁne-grained local patterns, which is similar to\n[19]. The difference between the HA- and SA-Transformers\nis that the HA-Transformer uses LDSA to replace the con-\nvolution layers in the baseline model, leaving the rest of the\nSA-Transformer unchanged. For a fair comparison, we set\nc = 15 in HA-Transformer, which equals to the size of the\nconvolution kernel in SA-Transformer.\n4. EXPERIMENTS\n4.1. Experimental setup\nWe evaluated the proposed models on a publicly-available\nMandarin speech corpus Aishell-1 [20], which contains about\n170 hours of speech recorded from 340 speakers. We used the\nofﬁcial partitioning of the dataset, with 150 hours for train-\nFig. 3: Effect of the context width of LDSA on performance.\ning, 20 hours for validation, and 10 hours for testing. For\nall experiments, we used 40-dimension Mel-ﬁlter bank coef-\nﬁcients (Fbank) features as input. The frame length and shift\nwas set to 25 ms and 10 ms respectively. For the output, we\nadopted a vocabulary set of 4230 Mandarin characters and 2\nnon-language symbols, with the 2 symbols denoting unknown\ncharacters and the start or end of a sentence respectively.\nWe used Open-Transformer 2 to build our models. For\nthe model training, we used Adam with Noam learning rate\nschedule (25000 warm steps) [12] as the optimizer. We also\nused SpecAugment [21] for data augmentation. After 80\nepochs training, the parameters of the last 10 epochs were av-\neraged as the ﬁnal model. During inference, we used a beam\nsearch with a width of 5 for all models. For the language\nmodel, we used the default setting of Open-Transformer, and\nintegrated it into beam search by shallow fusion [22]. The\nweight of the language model was set to 0.1 for all experi-\nments.\n4.2. Results\nWe ﬁrst investigated the effect of the context widthcof LDSA\nin the encoder on the development (Dev) set of Alshell-1,\nwhere we ﬁxed the size of the convolution kernel in all ex-\nperiments. Figure 3 shows the CER curve of the model with\nrespect to c. From the ﬁgure, we see that the CER ﬁrst de-\ncreases, and then becomes stable with the increase ofc. Based\non the above ﬁnding, we set c to 31 in all of the following\ncomparisons.\nThen, we compared the attention mechanisms men-\ntioned in Section 2. Table 1 lists the CER and complexity\nof the attention mechanisms. From the table, we see that\nthe LDSA-Transformer signiﬁcantly outperforms the DSA-\nTransformer, and achieves a slightly lower CER than the\nSA-Transformer, which demonstrates the effectiveness of the\nLDSA-Transformer. We also see that the computational com-\n2https://github.com/ZhengkunTian/OpenTransformer\nTable 1: Comparison of models with different attention\nmechanisms on the test set. ( T is the length of input feature,\ncis the context width.)\nMethod Complexity CER\nwithout LM with LM\nSA O(T2) 6.83 6.63\nDSA O(T2) 7.52 7.26\nLDSA O(Tc) 6.65 6.49\nHA O(T(T+c)) 6.38 6.18\nTable 2: CER comparison with the representative ASR sys-\ntems. (with LM)\nModel Dev Test\nTDNN-Chain (Kaldi) [23] - 7.45\nSA-T (Transducer) [24] 8.30 9.30\nLAS [25] - 10.56\nSpeech-Transformer [26] 6.57 7.37\nSA-Transformer (our implement) 5.83 6.63\nLDSA-Transformer 5.79 6.49\nHA-Transformer 5.66 6.18\nplexity of the LDSA scales linearly with T, which is lower\nthan the SA and DSA. Finally, the HA-Transformer achieves\nthe best performance among all comparison methods. Par-\nticularly, it achieves a relative CER reduction of 6.8% over\nthe SA-Transformer, which demonstrates that the LDSA per-\nforms better than the convolution operation in extracting local\nfeatures.\nTo further investigate the effectiveness of the proposed\nmodels, we compared them with several representative ASR\nsystems, which are the TDNN-Chain [23], Transducer [24],\nand LAS [25] in Table 2. From the table, we ﬁnd that the\nTransformer-based models outperform the three comparison\nsystems [23, 24, 25]. Among the Transformer-based models,\nLDSA-Transformer achieves slightly better performance than\nthe SA-Transformer. The HA-Transformer achieves a CER of\n6.18%, which is signiﬁcantly better than the other models.\n5. CONCLUSIONS\nIn this paper, we ﬁrst replaced the common SA in speech\nrecognition by DSA. Then, we proposed LDSA to restrict\nthe attention scope of DSA to a local range around the cur-\nrent central frame. Finally, we combined LDSA with SA to\nextract the local and global information simultaneously. Ex-\nperimental results on Aishell-1 demonstrate that the LDSA-\nTransformer achieves slightly better performance with lower\ncomputational complexity than the SA-Transformer; the HA-\nTransformer further improves the performance of the LDSA-\nTransformer; and all proposed methods are signiﬁcantly bet-\nter than the three representative ASR systems.\n6. REFERENCES\n[1] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai,\nJingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catan-\nzaro, Qiang Cheng, Guoliang Chen, et al., “Deep speech 2: End-to-end\nspeech recognition in english and mandarin,” in International confer-\nence on machine learning, 2016, pp. 173–182.\n[2] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, “Listen,\nattend and spell: A neural network for large vocabulary conversational\nspeech recognition,” in2016 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–\n4964.\n[3] Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates, Yashesh\nGaur Yi Li, Hairong Liu, Sanjeev Satheesh, Anuroop Sriram, and\nZhenyao Zhu, “Exploring neural transducers for end-to-end speech\nrecognition,” in 2017 IEEE Automatic Speech Recognition and Under-\nstanding Workshop (ASRU). IEEE, 2017, pp. 206–213.\n[4] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and\nTomoki Hayashi, “Hybrid ctc/attention architecture for end-to-end\nspeech recognition,” IEEE Journal of Selected Topics in Signal Pro-\ncessing, vol. 11, no. 8, pp. 1240–1253, 2017.\n[5] L. Dong, S. Xu, and B. Xu, “Speech-transformer: A no-recurrence\nsequence-to-sequence model for speech recognition,” in 2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2018, pp. 5884–5888.\n[6] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu, “Syllable-based\nsequence-to-sequence speech recognition with the transformer in man-\ndarin chinese,” Proc. Interspeech 2018, pp. 791–795, 2018.\n[7] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hiro-\nfumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta\nSoplin, Ryuichi Yamamoto, Xiaofei Wang, et al., “A comparative\nstudy on transformer vs rnn in speech applications,” in 2019 IEEE\nAutomatic Speech Recognition and Understanding Workshop (ASRU).\nIEEE, 2019, pp. 449–456.\n[8] Ching-Feng Yeh, Jay Mahadeokar, Kaustubh Kalgaonkar, Yongqiang\nWang, Duc Le, Mahaveer Jain, Kjell Schubert, Christian Fuegen, and\nMichael L Seltzer, “Transformer-transducer: End-to-end speech recog-\nnition with self-attention,” arXiv preprint arXiv:1910.12977, 2019.\n[9] Niko Moritz, Takaaki Hori, and Jonathan Le, “Streaming automatic\nspeech recognition with the transformer model,” inICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2020, pp. 6074–6078.\n[10] Yongqiang Wang, Abdelrahman Mohamed, Due Le, Chunxi Liu, Alex\nXiao, Jay Mahadeokar, Hongzhao Huang, Andros Tjandra, Xiaohui\nZhang, Frank Zhang, et al., “Transformer-based acoustic modeling for\nhybrid speech recognition,” in ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 6874–6878.\n[11] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDer-\nmott, Stephen Koo, and Shankar Kumar, “Transformer transducer: A\nstreamable speech recognition model with transformer encoders and\nrnn-t loss,” in ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp.\n7829–7833.\n[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, “Attention\nis all you need,” inAdvances in neural information processing systems,\n2017, pp. 5998–6008.\n[13] Chiu Chung-Cheng and Raffel Colin, “Monotonic chunkwise atten-\ntion,” in International Conference on Learning Representations, 2018.\n[14] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han, “Lite\ntransformer with long-short range attention,” in International Confer-\nence on Learning Representations, 2019.\n[15] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael\nAuli, “Pay less attention with lightweight and dynamic convolutions,”\nin International Conference on Learning Representations, 2018.\n[16] Alessandro Raganato, Yves Scherrer, and J ¨org Tiedemann, “Fixed en-\ncoder self-attention patterns in transformer-based machine translation,”\narXiv preprint arXiv:2002.10260, 2020.\n[17] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and\nChe Zheng, “Synthesizer: Rethinking self-attention in transformer\nmodels,” arXiv preprint arXiv:2005.00743, 2020.\n[18] Yuya Fujita, Aswin Shanmugam Subramanian, Motoi Omachi, and\nShinji Watanabe, “Attention-based asr with lightweight and dynamic\nconvolutions,” in ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp.\n7034–7038.\n[19] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang,\nJiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu,\net al., “Conformer: Convolution-augmented transformer for speech\nrecognition,” arXiv preprint arXiv:2005.08100, 2020.\n[20] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng, “Aishell-\n1: An open-source mandarin speech corpus and a speech recognition\nbaseline,” in 2017 20th Conference of the Oriental Chapter of the In-\nternational Coordinating Committee on Speech Databases and Speech\nI/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1–5.\n[21] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret\nZoph, Ekin D Cubuk, and Quoc V Le, “Specaugment: A simple data\naugmentation method for automatic speech recognition,” Proc. Inter-\nspeech 2019, pp. 2613–2617, 2019.\n[22] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng\nChen, and Rohit Prabhavalkar, “An analysis of incorporating an exter-\nnal language model into a sequence-to-sequence model,” in2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 1–5828.\n[23] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahremani,\nVimal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur,\n“Purely sequence-trained neural networks for asr based on lattice-free\nmmi.,” in Interspeech, 2016, pp. 2751–2755.\n[24] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, and Zhengqi Wen,\n“Self-attention transducers for end-to-end speech recognition,” Proc.\nInterspeech 2019, pp. 4395–4399, 2019.\n[25] Changhao Shan, Chao Weng, Guangsen Wang, Dan Su, Min Luo,\nDong Yu, and Lei Xie, “Component fusion: Learning replaceable lan-\nguage model component for end-to-end speech recognition system,”\nin ICASSP 2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2019, pp. 5361–5635.\n[26] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai Zhang, and\nZhengqi Wen, “Spike-triggered non-autoregressive transformer for\nend-to-end speech recognition,” arXiv preprint arXiv:2005.07903,\n2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7627182602882385
    },
    {
      "name": "Transformer",
      "score": 0.7277534604072571
    },
    {
      "name": "Speech recognition",
      "score": 0.6157352328300476
    },
    {
      "name": "Computation",
      "score": 0.5171908140182495
    },
    {
      "name": "Pairwise comparison",
      "score": 0.47045958042144775
    },
    {
      "name": "Computational complexity theory",
      "score": 0.44307276606559753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3946264684200287
    },
    {
      "name": "Algorithm",
      "score": 0.1858464479446411
    },
    {
      "name": "Voltage",
      "score": 0.1189805269241333
    },
    {
      "name": "Engineering",
      "score": 0.10820251703262329
    },
    {
      "name": "Electrical engineering",
      "score": 0.08470186591148376
    }
  ]
}