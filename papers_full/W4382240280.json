{
  "title": "Towards Global Video Scene Segmentation with Context-Aware Transformer",
  "url": "https://openalex.org/W4382240280",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2093908790",
      "name": "Yang Yang",
      "affiliations": [
        "Nanjing University of Science and Technology",
        "Ministry of Industry and Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136583780",
      "name": "Yurui Huang",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2142197010",
      "name": "Weili Guo",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135282682",
      "name": "Baohua Xu",
      "affiliations": [
        "Huawei Technologies (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2162887277",
      "name": "Dingyin Xia",
      "affiliations": [
        "Huawei Technologies (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2093908790",
      "name": "Yang Yang",
      "affiliations": [
        "Ministry of Industry and Information Technology",
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136583780",
      "name": "Yurui Huang",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135282682",
      "name": "Baohua Xu",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2162887277",
      "name": "Dingyin Xia",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2784931967",
    "https://openalex.org/W2964677000",
    "https://openalex.org/W3099032148",
    "https://openalex.org/W2171186743",
    "https://openalex.org/W3158783196",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2033431158",
    "https://openalex.org/W3118041515",
    "https://openalex.org/W2950187998",
    "https://openalex.org/W2146007701",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6780876175",
    "https://openalex.org/W2911779594",
    "https://openalex.org/W6799876724",
    "https://openalex.org/W3023633125",
    "https://openalex.org/W2963448607",
    "https://openalex.org/W2805042136",
    "https://openalex.org/W6781994527",
    "https://openalex.org/W4226078030",
    "https://openalex.org/W6704369950",
    "https://openalex.org/W3014391540",
    "https://openalex.org/W6676832644",
    "https://openalex.org/W6641159296",
    "https://openalex.org/W6791972635",
    "https://openalex.org/W2645519508",
    "https://openalex.org/W6674697456",
    "https://openalex.org/W2151617679",
    "https://openalex.org/W2063249218",
    "https://openalex.org/W6676086989",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W3199369707",
    "https://openalex.org/W4280489517",
    "https://openalex.org/W2948242301",
    "https://openalex.org/W2965578190",
    "https://openalex.org/W4304014288",
    "https://openalex.org/W6787254463",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3208431284",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W3034364644",
    "https://openalex.org/W4312942592",
    "https://openalex.org/W4288966168",
    "https://openalex.org/W3114795718",
    "https://openalex.org/W4293400715",
    "https://openalex.org/W1963878379",
    "https://openalex.org/W3113370935",
    "https://openalex.org/W2983918066",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3184549588",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3174415835",
    "https://openalex.org/W2109412002",
    "https://openalex.org/W343636949",
    "https://openalex.org/W3105232955",
    "https://openalex.org/W3091002423",
    "https://openalex.org/W4287555334",
    "https://openalex.org/W3095481265",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W2962677524",
    "https://openalex.org/W2098498971"
  ],
  "abstract": "Videos such as movies or TV episodes usually need to divide the long storyline into cohesive units, i.e., scenes, to facilitate the understanding of video semantics. The key challenge lies in finding the boundaries of scenes by comprehensively considering the complex temporal structure and semantic information. To this end, we introduce a novel Context-Aware Transformer (CAT) with a self-supervised learning framework to learn high-quality shot representations, for generating well-bounded scenes. More specifically, we design the CAT with local-global self-attentions, which can effectively consider both the long-term and short-term context to improve the shot encoding. For training the CAT, we adopt the self-supervised learning schema. Firstly, we leverage shot-to-scene level pretext tasks to facilitate the pre-training with pseudo boundary, which guides CAT to learn the discriminative shot representations that maximize intra-scene similarity and inter-scene discrimination in an unsupervised manner. Then, we transfer contextual representations for fine-tuning the CAT with supervised data, which encourages CAT to accurately detect the boundary for scene segmentation. As a result, CAT is able to learn the context-aware shot representations and provides global guidance for scene segmentation. Our empirical analyses show that CAT can achieve state-of-the-art performance when conducting the scene segmentation task on the MovieNet dataset, e.g., offering 2.15 improvements on AP.",
  "full_text": "Towards Global Video Scene Segmentation with Context-Aware Transformer\nYang Yang1,2,3*, Yurui Huang1‚Ä†, Weili Guo1, Baohua Xu4, Dingyin Xia4\n1Nanjing University of Science and Technology\n2MIIT Key Lab. of Pattern Analysis and Machine Intelligence, NUAA\n3State Key Lab. for Novel Software Technology, NJU\n4HUAWEI CBG Edu AI Lab\n{yyang, huangyurui, wlguo}@njust.edu.cn, {xubaohua1, xiadingyin}@huawei.com\nAbstract\nVideos such as movies or TV episodes usually need to divide\nthe long storyline into cohesive units, i.e., scenes, to facili-\ntate the understanding of video semantics. The key challenge\nlies in finding the boundaries of scenes by comprehensively\nconsidering the complex temporal structure and semantic in-\nformation. To this end, we introduce a novel Context-Aware\nTransformer (CAT) with a self-supervised learning frame-\nwork to learn high-quality shot representations, for generat-\ning well-bounded scenes. More specifically, we design the\nCAT with local-global self-attentions, which can effectively\nconsider both the long-term and short-term context to im-\nprove the shot encoding. For training the CAT, we adopt the\nself-supervised learning schema. Firstly, we leverage shot-\nto-scene level pretext tasks to facilitate the pre-training with\npseudo boundary, which guides CAT to learn the discrimina-\ntive shot representations that maximize intra-scene similar-\nity and inter-scene discrimination in an unsupervised manner.\nThen, we transfer contextual representations for fine-tuning\nthe CAT with supervised data, which encourages CAT to ac-\ncurately detect the boundary for scene segmentation. As a re-\nsult, CAT is able to learn the context-aware shot representa-\ntions and provides global guidance for scene segmentation.\nOur empirical analyses show that CAT can achieve state-of-\nthe-art performance when conducting the scene segmentation\ntask on the MovieNet dataset, e.g., offering 2.15 improve-\nments on AP.\nIntroduction\nWith the development of internet, a significantly increasing\nnumber of videos have been produced and stored. In order\nto reduce manual costs and improve efficiency, intelligent\nvideo understanding has received extensive attention and re-\nsearches (Yang et al. 2018; Zhu et al. 2020). A fundamental\naspect of video semantic understanding is scene segmenta-\ntion (i.e., scene boundary detection) (Rao et al. 2020; Chen\net al. 2021; Huang et al. 2020; Wang et al. 2021), which\nplays an important role in facilitating downstream tasks. For\nexample, students can quickly locate knowledge points ac-\ncording to the segmented educational videos; users can uti-\n*corresponding authors.\n‚Ä†work is done during the internship at HUAWEI CBG Edu AI\nLab\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nlize the interested scenes to retrieve movies with similar\nthemes; video platforms can advertise based on the segmen-\ntation point to obtain higher revenue. Compared with locat-\ning a shot directly using visual cues (Cotsaces, Nikolaidis,\nand Pitas 2006) (here shot represents a set of visually con-\ntinuous frames over an uninterrupted period of time), scene\nsegmentation is a more challenging task, that aims to find the\ntemporal locations of scene with complex temporal structure\nand semantic information (here scene denotes a sequence of\nshots to describe a semantically associated story).\nDespite the great progress in temporal localization, most\nexisting approaches usually focus on localizing certain ac-\ntion from short videos (Lin et al. 2019, 2018; Long et al.\n2020), these methods usually pre-define a list of categories\nthat are visually distinguishable (Rao et al. 2020). However,\nscene segmentation poses significantly more difficult chal-\nlenges: 1) Coarse-grained labels. The input video has only\nbinary boundary labels, without the fine-grained content cat-\negories for each scene as action recognition. 2) High-order\ncoherence. Scene segmentation needs to group the shots by\nconsidering extracted high-order information, i.e., semanti-\ncal coherence, rather than simple visual continuity. To ad-\ndress these challenges, unsupervised approaches (Baraldi,\nGrana, and Cucchiara 2015; Chasanis, Likas, and Galat-\nsanos 2009) were firstly developed, which detected the\nboundary with pairwise similarity comparison or nearest\nshots clustering. Nevertheless, their performance is rela-\ntively low considering the unsupervised setting. Further-\nmore, (Rao et al. 2020; Chen et al. 2021; Das and Das 2020)\nintroduced the scene boundary label for supervised predic-\ntion with contextual shots within the sliding window. How-\never, these methods are limited to using labeled data, without\nconsidering the unlabeled videos. Therefore, self-supervised\napproaches (Chen et al. 2020; Roh et al. 2021) are widely\nresearched by learning effectiveness representation without\nrelying on costly ground-truth annotations. Therefore, the\nself-supervised learning methods (Chen et al. 2021; Mun\net al. 2022; Wu et al. 2022a) have been designed to employ\nthe pre-training protocols for learning spatio-temporal pat-\nterns in video scenes. However, in current self-supervised\nmethods, the strategy of pretext task designs and high-level\ncontextual representation modeling are not well addressed.\nTherefore, in this paper, we develop a Context-Aware\nTransformer (CAT), which takes advantage of the princi-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n3206\nple behind video production that nearby shots should have\nsemantically cohesive story-arch, and the far away shots\nwill have a transition with little similarity. In detail, CAT\ndevelops the local-global self-attention heads to synthesize\nthe complementary information from both long-term and\nshort-term neighbors, rather than encoding the shots with\nonly single-level contextual information. Moreover, in self-\nsupervised training CAT, we propose shot-to-scene level\npretext tasks, i.e., Shot Masking Prediction, Shot Order Pre-\ndiction, Global Scene Matching, and Local Scene Matching,\nthat leverage pseudo-boundaries to capture semantic contex-\ntual representation during pre-training, thus leading to pre-\ncise scene boundary detection in fine-tuning stage with la-\nbeled data. Along this line, we can overcome the limitation\nof modeling videos by learning context-aware shot represen-\ntations, and wisely employing the unlabeled videos. Conse-\nquently, CAT provides global guidance for video scene seg-\nmentation.\nRelated Work\nVideo Scene Segmentation(also known as scene boundary\ndetection) aims at identifying the begin and end locations\nof different scenes with cohesive story-arch in videos. Early\nattempts mainly adopt the unsupervised learning to contrast\nor cluster neighboring shots into scenes. For example, (Rui,\nHuang, and Mehrotra 1998) grouped shots into semantically\nrelated scenes with time-adaptive similarity. (Rasheed and\nShah 2003) utilized the motion content, shot length and\ncolor properties of shots for first pass cluster, then com-\nputed scene dynamics for fine-grained cluster. (Chasanis,\nLikas, and Galatsanos 2009) proposed an improved spec-\ntral clustering method and employed the fast global k-means\nalgorithm for grouping shots. (Baraldi, Grana, and Cuc-\nchiara 2015) introduced a deep siamese network for seg-\nmenting videos into coherent scenes. (Sidiropoulos et al.\n2011; Yang et al. 2021; Baraldi, Grana, and Cucchiara 2015)\nfused multi-modal such as audio and visual features for final\ndetection. However, these methods always rely on manually\ndesigned similarity mechanisms, which are suffered from\nlow performance and efficiency. Therefore, supervised ap-\nproaches are researched, which adopted the boundary label\nfor supervised training. For example, (Rotman, Porat, and\nAshour 2017) formulated the scene detection as a generic\noptimization problem to optimally group shots into scenes.\n(Das and Das 2020) concatenated a shot with its left and\nright contexts for segment boundary prediction. (Rao et al.\n2020) proposed to hierarchically learn shot embedding to\nprovide a top-down scene segmentation with multi-modal\ninformation. Furthermore, to utilize the unlabeled videos,\nself-supervised segmentation approaches are proposed. For\nexample, (Chen et al. 2021) presented a self-supervised shot\nembedding approach to learn a shot representation that max-\nimizes the similarity between nearby shots compared to ran-\ndomly selected shots. (Mun et al. 2022) pre-trained a trans-\nformer encoder with pseudo-boundaries, and then fine-tuned\nthe encoder with labeled data. Nevertheless, these methods\nalways adopted sophisticated model architectures, without\ncarefully considering the contextual information of the long-\nterm video.\nContext-Aware Transformer\nScale\nSoftmax\nLocal Encoder\nScale\nSoftmax\nGlobal Encoder\n‚Ä¶\n‚Ä¶\nùëäùëâùëô ùëâùëô\nùëÑùëô\nTranspose Matrix Multiplication\nHadamard Product Concatenation\nùëäùêæùëô\nùëäùëÑùëô\nM\nùëäùë£\nùê¥ùëô\nùê¥ùëî\nT\nC‚ãÖ\nQ\nLùëéùë¶ùëíùëüùëÅ√ó\nFeed\nForward\nAdd & Norm\nAdd & Norm\nLocal-Global\nEncoder\nEmbedded\nShot\nK V\nùëâùëî\nùêæùëî\nùëÑùëî\nùëäùëâùëî\nùëäùêæùëî\nùëäùëÑùëî\nT\n‚ãÖ\nC\nT\nùêæùëô\nFigure 1: Illustration of Context-Aware Transformer (CAT).\nCAT designs the local-global self-attentions to comprehen-\nsively consider both short-term shots and potential long-\nterm correlated shot information.\nSelf-Supervised Representation Learning attempts to\nlearn representations using unlabeled data by solving pre-\ntext tasks using pseudo-supervised learning. The pseudo-\nlabels are automatically created without requiring labeled\ndata (Jing and Tian 2021). For example, (Pathak et al.\n2016; Vincent et al. 2008) used the pretext tasks of recon-\nstructing corrupted, (Doersch, Gupta, and Efros 2015) pro-\nposed to classify inputs with pseudo-labels. Inspired by self-\nsupervised learning, many approaches are proposed with\nvarious pretext tasks in video understanding tasks. For ex-\nample, (Ahsan, Sun, and Essa 2018) presented a pretext task\nof masked frame modeling to learn temporal dependency be-\ntween frames. (Xu et al. 2019) discovered the spatiotemporal\nrepresentations of the video by predicting the order of shuf-\nfled clips from the video. (Kuang et al. 2021) proposed a\nvideo-level contrastive learning method based on segments\nto formulate positive pairs. However, most methods concen-\ntrated on the classification task by modeling the shot level\npretext tasks, which may be sub-optimal to the video scene\nsegmentation task.\nProposed Method\nConsidering that state-of-the-art methods (Rao et al. 2020;\nChen et al. 2021; Mun et al. 2022) formulate scene seg-\nmentation based on the constituent set of shots (i.e., de-\ntermine whether a shot boundary is a scene boundary), all\ninput videos are first divided into shots with standard shot\ndetection techniques (Sidiropoulos et al. 2011), details are\nin the supplementary. Therefore, given an untrimmed video\n{vt}T\nt=1 with T shots, where vt is the t‚àíth shot. Scene\nsegmentation is to generate a set of boundary label y =\n{yt}T\nt=1, where yt = 1 represents the boundary, otherwise\nyt = 0. To this end, we first introduce a context-aware\nTransformer encoder to model the contextual information,\nand then propose a self-supervised learning scheme with\nshot-to-scene pretext tasks to learn discriminative shot rep-\nresentations for segmentation.\nContext-Aware Transformer\nThe key challenge to encoding the sequential shots is that:\ndifferent shot neighbors have various importance in mod-\n3207\neling contextual information. To overcome this challenge,\nwe design the Transformer with local-global self-attention\nheads to integrate both short-term shots and potential long-\nterm correlated shot information.\nShot Encoder.Following (Rao et al. 2020; Chen et al. 2021;\nWu et al. 2022b; Mun et al. 2022), we employ a shot encoder\nfe to encode a shot by capturing its spatio-temporal patterns.\nGiven a shot vt, the representations can be formulated as:\nfe(vt). Then the encoded shot sequence is sent into the CAT.\nLocal Encoder.To comprehensively encode each shot by\nconsidering the dependencies between shots, we employ\nthe transformer encoder (Vaswani et al. 2017) as the back-\nbone, which can encode the relationships among indepen-\ndent shots by adopting the self-attention mechanism. Specif-\nically, as shown in Figure 1, with the input shots, a video can\nbe denoted as ¬Øv = [ ¬Øv1, ¬Øv2, ¬∑¬∑¬∑ , ¬ØvT ] = fe(v) ¬ØW ‚àà RT√ód,\nwhere d is the hidden dimension, ¬ØW ‚àà Rd1√ód is the learn-\nable matrix. In the self-attention layer, the input representa-\ntions can be used to compute three matrices: Q, K, and V\ncorresponding to queries, keys, and values. Note that local\nself-attention heads only calculate the dot-product similari-\nties between queries and keys of the shot neighbors, i.e., a\nfixed L-size window centered on the shot:\nQl = ¬ØvWQl, K l = ¬ØvWKl, V l = ¬ØvWVl,\nAl = QlK‚ä§\nl\np\ndNl\n¬∑ M Att( ¬Øvl) = œÉ(Al)Vl, (1)\nwhere Ql ‚àà RT√ódNl , Kl ‚àà RT√ódNl , Vl ‚àà RT√ódNl , and\nWQl ‚àà Rd√ódNl , WKl ‚àà Rd√ódNl , WVl ‚àà Rd√ódNl are learn-\nable matrices. Nl denotes the number of local heads. The ac-\ntivation functionœÉ can be used as softmax here.M ‚àà RT√óT\nrepresents the mask matrix with padding, where the shots in\ndefined local window is 1, otherwise is 0.\nGlobal Encoder. To introduce the extra neighbor shot\nas complementary information, we further propose jointly\nmodeling strategy with global self-attention heads, which di-\nrectly determine attention distributions with the dot-product\nsimilarity between fully queries and keys:\nQg = ¬ØvWQg , K g = ¬ØvWKg , V g = ¬ØvWVg ,\nAg = QgK‚ä§\ng\npdNg\nAtt(¬Øvg) = œÉ(Ag)Vg, (2)\nwhere Qg ‚àà RT√ódNg , Kg ‚àà RT√ódNg , Vg ‚àà RT√ódNg , and\nWQg ‚àà Rd√ódNg , WKg ‚àà Rd√ódNg , WVg ‚àà Rd√ódNg are\nlearnable matrices. Ng denotes the number of global heads.\nFinally, local-global self-attention is composed ofN = Nl+\nNg parallel heads, and dNl = dNg = d/N.\nIn summary, the local head attentions are responsible for\ncapturing local dependencies based on local details, i.e.,\nthe potential intra-scene shots, and global head attentions\nare designed to model the long-term dependencies between\nshots, i.e., the potentially missed intra-scene and inter-scene\nshots. The combination of local and global attention enables\nour CAT to dynamically model local shots and capture the\nglobal dependencies of similar shots. Consequently, we can\nacquire output representations of shots, i.e., ÀÜv = ft(¬Øv) =\n[ÀÜv1, ÀÜv2, ¬∑¬∑¬∑ , ÀÜvT ] ‚àà RT√ód, where ft denotes the CAT.\nSelf-Supervised Training\nTo employ unlabeled videos, there are two ways: semi-\nsupervised and self-supervised techniques. A direct way in\nsemi-supervised methods (Arazo et al. 2020) is to use a\nmodel‚Äôs predictions to obtain artificial labels for unlabeled\ndata. A specific variant is the pseudo labeling, which con-\nverts the model predictions of unlabeled data to hard labels\nfor calculating the cross entropy. However, when we used\nsemi-supervised ideas in the early stage, we found that the\nmodel performance was not as good as the supervision ef-\nfect. We find that the reason is that the distributions of un-\nlabeled and labeled data are inconsistent in the MovieNet\ndataset, which leads to the problem of noisy labeling pre-\ndicted by the model trained on labeled data. In detail, we set\nthe supervised data with label 1, and the unsupervised data\nwith label 0, then we train a binary classifier using the repre-\nsentations fe(v). The result of test AUC is 78.6, which indi-\ncates that we can easily distinguish the supervised and unsu-\npervised data, i.e. existing distribution drift problem. There-\nfore, the self-supervised scheme, which first trains a gen-\neralized model using the unsupervised data and then fine-\ntunes the pre-trained model using supervised model, can\nwell overcome this challenge.\nPre-Training Objectives\nShot Masked Modeling (SMM).Inspired by masked lan-\nguage modeling (Vaswani et al. 2017), we adopt the shot\nmasked modeling task that reconstructs the representation\nof masked shots based on the surrounding shots. In detail,\ngiven the input shot features ¬Øv, we randomly mask them\nwith a probability of 15%. For masked shot sets, we learn\nto reconstruct the output representations to their input shot\nfeatures with a regression head. The reconstruction loss can\nbe formulated as:\nLsmm =\nX\ni‚ààDm\n‚à•¬Øvi ‚àí hsmm(ÀÜvi)‚à•2\n2 (3)\nwhere hsmm is a regression head to match the contextualized\nshot representations with input features ¬Øvi. ÀÜvi denotes the\nlearned representation of i-th shot by CAT. Dm denotes the\nset of masked shots.\nShot Order Modeling (SOM).SOM aims at full-scale ex-\nploiting the sequential nature of video input. Inspired by the\nframe order modeling (Li et al. 2020), we randomly select\n15% of the shots to be shuffled, and the SOM is to recon-\nstruct their original timestamps, i.e., s = {sj}|Do|\nj=1 , where\nsj ‚àà {1,2, ¬∑¬∑¬∑ , T}, Do is the set of shuffled shots and |Do|\nrepresents the size. SOM can be formulated as a classifica-\ntion problem, where s is the ground-truth labels of the re-\nordered shots. The objective can be formulated as:\nLsom =\nX\nj‚ààDo\nCE(sj, hsom(ÀÜvj)) (4)\nwhere CE represents the cross-entropy loss, hsom denotes\nthe order predictor with a softmax layer.\nGlobal Scene Matching (GSM).GSM aims to make the\nshot representations similar to its associated scene, while\n3208\nCross Entropy\nOutput Embedding\ncos “ßùë£ùë°, “ßùë£ùëñ > ùúá?\nlabeled shots\ngradient\n0 0 1 1 1 0 0\nparameter transfer\nrandom\nselect\nMasked Shot EmbeddingShot Embedding\nShot Encoder\nContext-Aware \nTransformer \npush\nrandom shuffle\n4 1 0 3 2 5 6\npush\npull pull\npush\ngradient\nFine-tuning Stage\ngradient\nno gradient\nPositive Shot\\Scene Embedding Shuffled Shot Embedding\nunlabeled shots\nGSM\n“ßùë£ùë°\n‚ãØ ‚ãØ‚ãØ‚ãØ\nShot Encoder\nContext-Aware \nTransformer \nPre-training Stage\nLSM SMM SOM\nAnchor Shot Embedding\nPseudo-boundary \ngeneration\nNegative Shot\\Scene Embedding\nIs boundary ?\nmasking\n“ßùë£ùë°\nMLP\nFigure 2: Illustration of self-supervised training. In the pre-training stage, we train the shot encoder and context-aware Trans-\nformer with shot-to-scene pretext tasks using pseudo boundary in an unsupervised manner. Then, we fix the shot encoder, and\nfine-tune the context-aware Transformer with supervised data, including boundary prediction and supervised contrastive losses.\ndissimilar to other scenes. To achieve this purpose, we con-\nstruct a long fixed-size (i,e, P length) window for each\nshot, in which each shot act as the center shot, i.e., ct =\n{¬Øvt‚àí(P‚àí1)/2, ¬∑¬∑¬∑ , ¬Øvt, ¬∑¬∑¬∑ , ¬Øvt+(P‚àí1)/2}. And then we sim-\nply find pseudo-boundaries by measuring the similarity be-\ntween shots, i.e, taking the given shot as the center to spread\nto both sides, and the first shot whose cosine similarity with\nthe center shot is lower the threshold is regarded as a pseudo-\nboundary, i.e., cos(¬Øvj, ¬Øvt) ‚â§ ¬µ. As a result, we divide the\nfixed-size window of CAT output into three non-overlapping\nsub-sequences, i.e., Qleft\nt , Qt, Qright\nt . Algorithm details are\nin the supplementary. The reason for using¬Øv to calculate the\npseudo-boundary is that adopting the output representations\nof CAT to calculate the similarity will create more noise,\nconsidering the integration of contextual information in the\nforward process. Related experiments are in the supplemen-\ntary. Considering the split three sub-sequences as pseudo-\nscenes, we train the model using InfoNCE loss (van den\nOord, Li, and Vinyals 2018):\nLgsm = ‚àí\nX\nÀÜQ‚àà{Qleft\nt ,Qt,Qright\nt }\nlog esim(ÀÜv, ÀÜQ)/œÑ\nesim(ÀÜv, ÀÜQ)/œÑ + P\nQr‚ààNr\nesim(ÀÜv,Qr)/œÑ\nsim(ÀÜv, ÀÜQ) = cos(ÀÜv, mean(ÀÜQ))\n(5)\nwhere ÀÜv is a randomly sampled shot from ÀÜQ, œÑ is a temper-\nature hyperparameter and mean(Q) means scene-level rep-\nresentations, which utilizes the average pooling of shots in\nsub-sequence Q. Nr is the constructed negative scenes using\nthe pseudo-scenes except for ÀÜQ, and other pseudo-scenes in\nthe mini-batch.\nLocal Scene Matching (LSM).Moreover, LSM measures\nthe semantic coherence of the local shots rather than global\nscene, which learns to decide whether the given two shots\nbelong to the same scene. In detail, we use the center shotÀÜvt\nas the anchor and construct a tuple (ÀÜvt, ÀÜvpos, ÀÜvneg), where\nÀÜvpos is sampled from Qt, and ÀÜvneg is sampled from Qleft\nt\nand Qright\nt . The loss is defined as:\nLlsm = ‚àílog esim(ÀÜvt,ÀÜvpos)/œÑ\nesim(ÀÜvt,ÀÜvpos)/œÑ + P\nÀÜvneg‚ààNn\nesim(ÀÜvt,ÀÜvneg)/œÑ\n(6)\nwhere sim denotes cos function. Nn is the constructed neg-\native shots. In summary, GSM and LSM encourage the CAT\nto maximize intra-scene similarity, while minimizing inter-\nscene similarity. The final pre-training loss is defined as:\nL = Lsmm + Lsom + Lgsm + Llsm, (7)\nFine-Tuning for Segmentation\nAs a matter of fact, we have limited videos with bound-\nary labels. Therefore, in the fine-tuning phase, we formulate\nthe video scene segmentation as a binary classification task\nto identify transitional moments. In detail, given a labeled\nvideo v, we develop a scene boundary detection head to in-\nfer the boundary prediction for each shot. Following (Mun\n3209\net al. 2022), we freeze the parameters of the shot encoder\nfe, and fine-tune the ft and boundary detection head. The\nbinary cross-entropy loss can be formulated as:\nLp = ‚àí\nX\nÀÜvt\n[yt log(hpre(ÀÜvt)) + (1‚àí yt) log(1‚àí hpre(ÀÜvt))]\n(8)\nwhere hpre denotes the boundary detection head. In infer-\nring phase, we predict scene boundary when a shot‚Äôs predic-\ntion score is higher than a pre-defined threshold (i.e., 0.5).\nExperiments\nExperimental Setups\nDataset. Considering the availability and scale of video seg-\nmentation datasets, we adopt the MovieNet dataset follow-\ning all current state-of-the-art methods (Rao et al. 2020;\nChen et al. 2021; Wu et al. 2022b; Mun et al. 2022).\nMovieNet (Huang et al. 2020) dataset published 1,100\nmovies where 318 of them are annotated with scene bound-\naries. In detail, most movies in MovieNet have a time du-\nration between 90 to 120 minutes, providing rich informa-\ntion about individual movie stories. The whole annotation\nset is split into Train, Validation, and Test sets with the ratio\nof 10:2:3 on video level following (Huang et al. 2020), the\nscene boundaries are annotated at shot level. The length of\nthe annotated scenes varies from less than 10s to more than\n120s, where the majority last for 10‚àº30s, more details are\nin the supplementary.\nComparison Methods. We compare our method CAT\nwith state-of-the-art segmentation approaches: 1) unsuper-\nvised methods, i.e., GraphCut (Rasheed and Shah 2005),\nSCSA (Chasanis, Likas, and Galatsanos 2009), DP (Han and\nWu 2011), StoryGraph (Tapaswi, B ¬®auml, and Stiefelhagen\n2014), Grouping (Rotman, Porat, and Ashour 2017). 2) su-\npervised methods, i.e., including Siamese (Baraldi, Grana,\nand Cucchiara 2015), MS-LSTM (Huang et al. 2020), and\nLGSS (Rao et al. 2020), and 3) self-supervised methods,\nincluding ShotCoL (Chen et al. 2021), SCRL (Wu et al.\n2022b), and BaSSL (Mun et al. 2022), more details are in\nthe supplementary.\nEvaluation Protocol. Following (Mun et al. 2022), we\nadopt four commonly used metrics: 1) Average Precision\n(AP), 2) AUC, 3) F1, and 2) Miou, which measures the aver-\naged intersection over union (IoU) between predicted scene\nsegments and their closest ground truth scene segments.\nShot Feature Encoding. Considering the input features\nof shots, we construct two modal features following (Rao\net al. 2020; Chen et al. 2021; Mun et al. 2022), i.e., the\nvisual and audio modalities, which are encoded indepen-\ndently with separate encoder networks from the input shots.\nSpecifically, visual modality includes place elements to\ncapture the complex semantic information. Place features\n(2,048 dimensions) are extracted from key-frames in shots\nwith ResNet50 (He et al. 2016). On the other hand, Au-\ndio features (512 dimensions) are obtained by concatenat-\ning STFT (Umesh, Cohen, and Nelson 1999) features in a\nshot with a 16K Hz sampling rate and 512 windowed signal\nlength. Multi-modal experiments are in the supplementary.\nImplementation Details\nFor CAT framework, we choose the 2-layer Transformer\nnetwork with 8 heads, i.e., N = 8, as the encoder net-\nwork architecture. The regression head hsmm is a fully con-\nnected network with three layers, and the order prediction\nhsom and boundary detection head hpre are fully connected\nnetworks with two layers. All weights in the encoder and\nMLP are randomly initialized. For the pre-training stage, we\ncross-validate the number of neighbor shots among L =\n{1, 3, 5, 7}/P = {13, 15, 17, 19} and L = 5/P = 17\nis selected due to its good performance and computational\nefficiency. The optimization method is Adaptive Moment\nEstimation (Adam), and the learning rate is searched in\n{0.5, 0.1, 0.05, 0.01, 0.005, 0.001} to find the best settings\nfor each task. Finally, we set the learning rate as 0.001. The\nhyper-parameter ¬µ = 0.3, œÑ = 0.1. Code is available at\nhttps://github.com/njustkmg/CAT.\nVideo Scene Segmentation\nTable 1 summarizes segmentation results against compar-\nison methods. M.S. represents the MovieScenes dataset\nwith 150 annotated movies, M.S-318 denotes the MovieNet\ndataset with 318 annotated movies, and Eval. means the\ndataset used for supervised fine-tuning after the self-\nsupervised pre-training. Train., Test., and Val. represent\ntraining, testing, and validation sets of MovieNet (Huang\net al. 2020). The segmentation results are adopted directly\naccording to the original paper except for * annotation. ‚Äú-\n‚Äù denotes that the results are not given in original papers.\nThe results reveal that: 1) Supervised methods perform su-\nperior to the unsupervised methods, e.g., the LGSS achieves\nat least 20% improvement in AP. For the reason that su-\npervised data can better guide representation learning. 2)\nSelf-supervised style approaches further improve the per-\nformance, which indicates the advantages of pre-training\nwith unsupervised data. 3) CAT achieves the best perfor-\nmance on various criteria, e.g., CAT outperforms the su-\npervised state-of-the-art method, i.e., LGSS by margins of\n12.45/4.87 in terms of AP/mIOU, outperforms the state-of-\nthe-art self-supervised method, i.e., BaSSL by margins of\n2.15/2.98 /4.91/1.27 in terms of AP/mIOU/F1/AUC, which\nindicate the effectiveness of context-aware Transformer and\npre-training objectives. Besides, CAT using Transformer\nwith only global attention (i.e., CAT with Transformer) per-\nforms worse than CAT, revealing the advantages of using\ncontext-aware attention. 4) To prevent data leakage, we have\nreproduced the performance of self-supervised methods on\nthe training dataset (660 movies) for comparison. Compared\nwith other self-supervised approaches that have performance\ndeclines, CAT can achieve competitive performance with\nless training data, with only a decline of 0.33/0.23/0.58/0.18\nin terms of AP/mIOU/F1/AUC.\nAblation Study\nImpact of individual pretext tasks.To explore the contri-\nbution of each pre-training objective, we train models by\nvarying the usage of pre-training tasks. From Table 2, we\nconclude the following observations: 1) SMM task leads the\n3210\nW/o SSL Dataset AP (‚Üë) mIOU(‚Üë) F1(‚Üë) AUC(‚Üë)\nGraphCut M.S. 14.10 29.70 - -\nSCSA M.S. 14.70 30.50 - -\nDP M.S. 15.50 32.00 - -\nGrouping M.S. 17.60 33.10 - -\nStoryGraph M.S. 25.10 35.70 - -\nSiamese M.S. 28.10 36.00 - -\nMS-LSTM M.S. 46.50 46.20 - -\nLGSS M.S. 47.10 48.80 - -\nLGSS w/o DP M.S. 44.90 46.50 38.52 -\nLGSS w/o DP‚àó M.S-318 44.90 46.50 38.52 87.73\nW/ SSL Dataset AP(‚Üë) mIOU(‚Üë) F1(‚Üë) AUC(‚Üë)Pretrain Data Eval.\nShotCoL Train.+Test.+Val. M.S-318 52.89 - 49.17 -\nSCRL Train.+Test.+Val. M.S-318 54.82 - 51.43 -\nBaSSL Train.+Test.+Val. M.S-318 57.40 50.69 47.02 90.54\nCAT with transformer Train.+Test.+Val. M.S-318 58.35 51.92 50.20 90.59\nCAT Train.+Test.+Val. M.S-318 59.55 53.67 51.93 91.81\nShotCoL Train. only M.S-318 48.21 - 46.52 -\nSCRL Train. only M.S-318 54.55 - 51.39 -\nBaSSL‚àó Train. only M.S-318 53.36 48.32 43.64 89.26\nCAT Train. only M.S-318 59.22 53.44 51.35 91.63\nTable 1: Scene segmentation results. The compared methods are grouped in two, i.e., (a) approaches that do not use self-\nsupervised learning, including unsupervised and supervised methods, and (b) approaches that adopt self-supervised learning\nfollowed by supervised fine-tuning. * denotes our implementations.\nPretext Tasks Evaluation Metric\nSMM SOM GSM LSM AP mIOU AUC F1 SUM\n‚úì 16.61 28.00 69.09 21.99 135.69\n‚úì 39.59 44.34 82.58 38.42 198.93\n‚úì 44.27 42.03 86.21 32.12 204.63\n‚úì 29.38 39.88 77.63 30.98 177.87\nTable 2: Ablation results for different pre-training tasks.\nLocal Window AP mIOU AUC F1\nL=3 58.07 53.10 91.15 51.12\nL=5 59.55 53.67 91.81 51.93\nL=7 59.49 53.62 91.77 51.93\nTable 3: Ablations of the local encoder window size L. The\nbest scores are in bold.\nworst performance, which indicates that context-aware pre-\ntext tasks (i.e., GSM, LSM, and SOM) can consider contex-\ntual relationships well, which is vital for scene segmenta-\ntion. 2) Scene-level task, i.e., GSM achieves the best perfor-\nmance, which indicates the importance of considering intra-\nscene and inter-scene distances.\nSensitivity of Hyperparameters.To explore the effect of\ndifferent hyperparameters: 1) the window size L in the local\nencoder. 2) the number of local and global attention heads\nNl/Ng in context-aware Transformer. 3) the global scene\nContextual Window AP mIOU AUC F1\nP=13 58.73 52.62 91.41 50.44\nP=15 59.27 53.39 91.72 51.44\nP=17 59.55 53.67 91.81 51.93\nP=19 59.14 53.46 91.64 51.59\nTable 4: Ablations of the contextual window length P in\nscene-level tasks. The best scores are in bold.\nlength P in scene-level tasks, we conduct more experiments.\nMore parameter analyses are in the supplementary. Table 3\nexhibits the results of parameterL. The performance first in-\ncreases and then decreases, indicating that a larger window\ncan consider more contextual information, but an oversized\none may introduce noisy information. Table 4 records the\nperformance of parameter P, which is similar to parame-\nter L in that a larger global scene can introduce more con-\ntextual information, but an oversized one will cause worse\n3211\nGTLGSSBASSLCAT\nFigure 3: Comparison of boundary detection results from three approaches: LGSS, BaSSL, and our CAT. The green dividing\nlines indicate the correct boundary, while the yellow lines denote the incorrect ones. ‚ÄúGT‚Äù represents the ground truth boundary.\nLocal-Global Heads AP mIOU AUC F1\nNl = 0, N g = 8 58.35 51.92 90.59 50.20\nNl = 2, N g = 6 59.55 53.67 91.81 51.93\nNl = 4, N g = 4 59.19 53.19 91.66 51.46\nNl = 6, N g = 2 59.10 53.02 91.57 51.08\nNl = 8, N g = 0 58.92 52.27 91.44 50.92\nTable 5: Ablations of the local-global number. The best\nscores are in bold.\nperformance. Table 5 provides the segmentation results us-\ning various local-global heads in CAT. The performance of\nCAT firstly increases, and then decreases. The CAT acquires\nthe best performance when local-global is (Nl = 2, Ng = 6).\nThe reason may be that the local encoder‚Äôs window size is\nlimited, thereby the global shots can provide additional sup-\nplementary information.\nMethods AP mIOU AUC F1\nLGSS (Visual) 39.00 - - -\nLGSS (Audio) 17.50 - - -\nLGSS (Visual+Audio) 43.40 - - -\nShotCoL (Visual) 46.77 - - -\nShotCoL (Audio) 27.92 - - -\nShotCoL (Visual+Audio) 44.32 - - -\nSCRL (Visual) 53.74 - - -\nSCRL (Audio) 29.39 - - -\nSCRL (Visual+Audio) 50.80 - - -\nBaSSL (Visual) 57.40 50.69 90.54 47.02\nBaSSL (Audio) 31.69 41.85 79.98 35.49\nBaSSL (Visual+Audio) 58.39 52.67 91.09 49.97\nCAT (Visual) 59.55 53.67 91.81 51.93\nCAT (Audio) 33.41 42.43 80.79 36.40\nCAT (Visual+Audio) 60.20 55.49 92.17 54.78\nTable 6: Comparison results of the multi-modal experiment\non MovieNet. Backbones of following methods for each\nmodality are the same.\nPerformance of Multi-modal Learning\nFollowing (Rao et al. 2020; Yang et al. 2022; Wu et al.\n2022b; Mun et al. 2022), we experiment with the proposed\nmethod using multi-modal data, i.e., audio and visual modal-\nities. In detail, we adopt the late fusion (i.e., using max pool-\ning of multi-modal predictions) following (Mun et al. 2022).\n‚Äú-‚Äù denotes that the results are not given in the original pa-\nper. Table 6 records the results, we find that the multi-modal\nfusion performs better than the single modality, but the audio\nis a weak modality, which has little promotion.\nVisualization\nTo explore the learning of shot representations, we conduct\nmore experiments. We show segmented cases in Figure 3 to\ndemonstrate the CAT. There are two scenes, we find that a\nshot with clear change is likely to predict a wrong bound-\nary (i.e., yellow lines) by context limited approaches, even\nthough has similar semantics to contextual shots. However,\nCAT can successfully predict the boundary. More visualiza-\ntion cases are in supplementary.\nConclusion\nIn this paper, we study the video segmentation task. With\nthe development of self-supervised learning that adopts both\nthe unsupervised and supervised data into training, we intro-\nduce a novel Context-Aware Transformer (CAT) with a self-\nsupervised learning framework to learn high-quality shot\nrepresentations, for generating well-bounded scene. In de-\ntail, CAT utilizes local-global self-attentions to improve the\nshot encoding. Furthermore, we design shot-to-scene level\npretext tasks for learning shot representations, and then we\ndirect fine-tune the CAT with supervised data. Empirical\nanalyses show that CAT can achieve state-of-the-art perfor-\nmance when conducting the scene segmentation task. In the\nfuture, how to design a more robust multi-modal fusion strat-\negy is an interesting work.\nAcknowledgements\nNational Key RD Program of China (2022YFF0712100),\nNSFC (61906092, 62006118, 62276131), Natural Science\nFoundation of Jiangsu Province of China under Grant\n(BK20200460), Jiangsu Shuangchuang (Mass Innovation\nand Entrepreneurship) Talent Program. Young Elite Scien-\ntists Sponsorship Program by CAST, CAAI-Huawei Mind-\nSpore Open Fund (CAAIXSJLJJ-2021-014B), the Fun-\ndamental Research Funds for the Central Universities\n(NO.NJ2022028, No.30922010317).\n3212\nReferences\nAhsan, U.; Sun, C.; and Essa, I. A. 2018. DiscrimNet: Semi-\nSupervised Action Recognition from Videos using Generative Ad-\nversarial Networks. CoRR, abs/1801.07230.\nArazo, E.; Ortego, D.; Albert, P.; O‚ÄôConnor, N. E.; and McGuin-\nness, K. 2020. Pseudo-Labeling and Confirmation Bias in Deep\nSemi-Supervised Learning. In IJCNN, 1‚Äì8.\nBaraldi, L.; Grana, C.; and Cucchiara, R. 2015. A Deep Siamese\nNetwork for Scene Detection in Broadcast Videos. In MM, 1199‚Äì\n1202. Brisbane, Australia.\nChasanis, V .; Likas, A.; and Galatsanos, N. P. 2009. Scene De-\ntection in Videos Using Shot Clustering and Sequence Alignment.\nIEEE Trans. Multim., 11(1): 89‚Äì100.\nChen, S.; Nie, X.; Fan, D.; Zhang, D.; Bhat, V .; and Hamid, R.\n2021. Shot Contrastive Self-Supervised Learning for Scene Bound-\nary Detection. In CVPR, 9796‚Äì9805. virtual.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. E. 2020. A\nSimple Framework for Contrastive Learning of Visual Representa-\ntions. In ICML, volume 119 of Proceedings of Machine Learning\nResearch, 1597‚Äì1607. Virtual.\nCotsaces, C. I.; Nikolaidis, N.; and Pitas, I. 2006. Video shot detec-\ntion and condensed representation. a review. IEEE Signal Process.\nMag., 23(2): 28‚Äì37.\nDas, A.; and Das, P. P. 2020. Incorporating Domain Knowledge\nTo Improve Topic Segmentation Of Long MOOC Lecture Videos.\nCoRR, abs/2012.07589.\nDoersch, C.; Gupta, A.; and Efros, A. A. 2015. Unsupervised\nVisual Representation Learning by Context Prediction. In ICCV,\n1422‚Äì1430. Santiago, Chile.\nHan, B.; and Wu, W. 2011. Video scene segmentation using a\nnovel boundary evaluation criterion and dynamic programming. In\nICME, 1‚Äì6. Catalonia, Spain.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learn-\ning for Image Recognition. In CVPR, 770‚Äì778. Las Vegas, NV .\nHuang, Q.; Xiong, Y .; Rao, A.; Wang, J.; and Lin, D. 2020.\nMovieNet: A Holistic Dataset for Movie Understanding. InECCV,\nvolume 12349 of Lecture Notes in Computer Science, 709‚Äì727.\nGlasgow, UK.\nJing, L.; and Tian, Y . 2021. Self-Supervised Visual Feature Learn-\ning With Deep Neural Networks: A Survey. IEEE Trans. Pattern\nAnal. Mach. Intell., 43(11): 4037‚Äì4058.\nKuang, H.; Zhu, Y .; Zhang, Z.; Li, X.; Tighe, J.; Schwertfeger, S.;\nStachniss, C.; and Li, M. 2021. Video Contrastive Learning with\nGlobal Context. CoRR, abs/2108.02722.\nLi, L.; Chen, Y .; Cheng, Y .; Gan, Z.; Yu, L.; and Liu, J.\n2020. HERO: Hierarchical Encoder for Video+Language Omni-\nrepresentation Pre-training. In EMNLP, 2046‚Äì2065. virtual.\nLin, T.; Liu, X.; Li, X.; Ding, E.; and Wen, S. 2019. BMN:\nBoundary-Matching Network for Temporal Action Proposal Gen-\neration. In ICCV, 3888‚Äì3897. Seoul, Korea (South).\nLin, T.; Zhao, X.; Su, H.; Wang, C.; and Yang, M. 2018. BSN:\nBoundary Sensitive Network for Temporal Action Proposal Gener-\nation. In ECCV, volume 11208, 3‚Äì21. Munich, Germany.\nLong, F.; Yao, T.; Qiu, Z.; Tian, X.; Luo, J.; and Mei, T. 2020.\nLearning to Localize Actions from Moments. In ECCV, volume\n12348, 137‚Äì154. Glasgow, UK.\nMun, J.; Shin, M.; Han, G.; Lee, S.; Ha, S.; Lee, J.; and Kim, E.\n2022. Boundary-aware Self-supervised Learning for Video Scene\nSegmentation. CoRR, abs/2201.05277.\nPathak, D.; Kr ¬®ahenb¬®uhl, P.; Donahue, J.; Darrell, T.; and Efros,\nA. A. 2016. Context Encoders: Feature Learning by Inpainting.\nIn CVPR, 2536‚Äì2544. Las Vegas, NV .\nRao, A.; Xu, L.; Xiong, Y .; Xu, G.; Huang, Q.; Zhou, B.; and\nLin, D. 2020. A Local-to-Global Approach to Multi-Modal Movie\nScene Segmentation. In CVPR, 10143‚Äì10152. Seattle, W A.\nRasheed, Z.; and Shah, M. 2003. Scene Detection In Hollywood\nMovies and TV Shows. In CVPR, 343‚Äì350. Madison, WI.\nRasheed, Z.; and Shah, M. 2005. Detection and representation of\nscenes in videos. IEEE Trans. Multim., 7(6): 1097‚Äì1105.\nRoh, B.; Shin, W.; Kim, I.; and Kim, S. 2021. Spatially Consistent\nRepresentation Learning. In CVPR, 1144‚Äì1153. virtual.\nRotman, D.; Porat, D.; and Ashour, G. 2017. Optimal Sequen-\ntial Grouping for Robust Video Scene Detection Using Multiple\nModalities. Int. J. Semantic Comput., 11(2): 193‚Äì208.\nRui, Y .; Huang, T. S.; and Mehrotra, S. 1998. Exploring Video\nStructure Beyond the Shots. In ICMCS, 237‚Äì240. Austin, Texas.\nSidiropoulos, P.; Mezaris, V .; Kompatsiaris, I.; Meinedo, H.;\nBugalho, M.; and Trancoso, I. 2011. Temporal Video Segmen-\ntation to Scenes Using High-Level Audiovisual Features. IEEE\nTrans. Circuits Syst. Video Technol., 21(8): 1163‚Äì1177.\nTapaswi, M.; B¬®auml, M.; and Stiefelhagen, R. 2014. StoryGraphs:\nVisualizing Character Interactions as a Timeline. In CVPR, 827‚Äì\n834. Columbus, OH.\nUmesh, S.; Cohen, L.; and Nelson, D. J. 1999. Fitting the Mel\nscale. In ICASSP, 217‚Äì220. Phoenix, Arizona.\nvan den Oord, A.; Li, Y .; and Vinyals, O. 2018. Represen-\ntation Learning with Contrastive Predictive Coding. CoRR,\nabs/1807.03748.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is\nAll you Need. In NeurIPS, 5998‚Äì6008. Long Beach, CA.\nVincent, P.; Larochelle, H.; Bengio, Y .; and Manzagol, P. 2008.\nExtracting and composing robust features with denoising autoen-\ncoders. In ICML, 1096‚Äì1103. Helsinki, Finland.\nWang, Z.; Wu, L.; Li, Z.; Xiong, J.; and Lu, Q. 2021. Overview of\nTencent Multi-modal Ads Video Understanding Challenge. CoRR,\nabs/2109.07951.\nWu, H.; Chen, K.; Luo, Y .; Qiao, R.; Ren, B.; Liu, H.; Xie, W.; and\nShen, L. 2022a. Scene Consistency Representation Learning for\nVideo Scene Segmentation. CoRR, abs/2205.05487.\nWu, H.; Chen, K.; Luo, Y .; Qiao, R.; Ren, B.; Liu, H.; Xie, W.; and\nShen, L. 2022b. Scene Consistency Representation Learning for\nVideo Scene Segmentation. CoRR, abs/2205.05487.\nXu, D.; Xiao, J.; Zhao, Z.; Shao, J.; Xie, D.; and Zhuang, Y . 2019.\nSelf-Supervised Spatiotemporal Learning via Video Clip Order\nPrediction. In CVPR, 10334‚Äì10343. Long Beach, CA.\nYang, Y .; Fu, Z.; Zhan, D.; Liu, Z.; and Jiang, Y . 2021. Semi-\nSupervised Multi-Modal Multi-Instance Multi-Label Deep Net-\nwork with Optimal Transport. IEEE Trans. Knowl. Data Eng.,\n33(2): 696‚Äì709.\nYang, Y .; Wu, Y .; Zhan, D.; Liu, Z.; and Jiang, Y . 2018. Complex\nObject Classification: A Multi-Modal Multi-Instance Multi-Label\nDeep Network with Optimal Transport. In KDD, 2594‚Äì2603. Lon-\ndon, UK.\nYang, Y .; Zhang, J.; Gao, F.; Gao, X.; and Zhu, H. 2022. DOMFN:\nA Divergence-Orientated Multi-Modal Fusion Network for Re-\nsume Assessment. In ACMMM, 1612‚Äì1620. Lisboa, Portugal.\nZhu, Y .; Li, X.; Liu, C.; Zolfaghari, M.; Xiong, Y .; Wu, C.; Zhang,\nZ.; Tighe, J.; Manmatha, R.; and Li, M. 2020. A Comprehensive\nStudy of Deep Video Action Recognition. CoRR, abs/2012.06567.\n3213",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7939278483390808
    },
    {
      "name": "Segmentation",
      "score": 0.6973859667778015
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6966871023178101
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5575557351112366
    },
    {
      "name": "Discriminative model",
      "score": 0.5407907962799072
    },
    {
      "name": "Computer vision",
      "score": 0.4593115448951721
    },
    {
      "name": "Transformer",
      "score": 0.4490519165992737
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}