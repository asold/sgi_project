{
  "title": "Comparing Code Explanations Created by Students and Large Language Models",
  "url": "https://openalex.org/W4365517040",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4227959803",
      "name": "Leinonen, Juho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3172309270",
      "name": "Denny, Paul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4308503988",
      "name": "MacNeil, Stephen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227959802",
      "name": "Sarsa, Sami",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2675690879",
      "name": "Bernstein, Seth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355939496",
      "name": "Kim, Joanne",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284226270",
      "name": "Tran, Andrew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227959804",
      "name": "Hellas, Arto",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2009908982",
    "https://openalex.org/W1035775227",
    "https://openalex.org/W3155107975",
    "https://openalex.org/W2071792242",
    "https://openalex.org/W3129460529",
    "https://openalex.org/W4283705032",
    "https://openalex.org/W1995897216",
    "https://openalex.org/W3174511786",
    "https://openalex.org/W2916970057",
    "https://openalex.org/W3034086986",
    "https://openalex.org/W2162363818",
    "https://openalex.org/W4249379125",
    "https://openalex.org/W4291476001",
    "https://openalex.org/W2807648480",
    "https://openalex.org/W2963562081",
    "https://openalex.org/W4230514184",
    "https://openalex.org/W4308757526",
    "https://openalex.org/W2899498297",
    "https://openalex.org/W2994531689",
    "https://openalex.org/W4323037544",
    "https://openalex.org/W2036177604",
    "https://openalex.org/W1594737854",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W1972978214",
    "https://openalex.org/W1995133614",
    "https://openalex.org/W2783796210",
    "https://openalex.org/W2137619003",
    "https://openalex.org/W1825242443",
    "https://openalex.org/W4323033814",
    "https://openalex.org/W1969704197",
    "https://openalex.org/W2970608058",
    "https://openalex.org/W2129741299",
    "https://openalex.org/W3084854375",
    "https://openalex.org/W2809393522",
    "https://openalex.org/W3032903422",
    "https://openalex.org/W2612388249",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229889443",
    "https://openalex.org/W4310744613"
  ],
  "abstract": "Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To evaluate LLM-created explanations, we compare them with explanations created by students in a large course ($n \\approx 1000$) with respect to accuracy, understandability and length. We find that LLM-created explanations, which can be produced automatically on demand, are rated as being significantly easier to understand and more accurate summaries of code than student-created explanations. We discuss the significance of this finding, and suggest how such models can be incorporated into introductory programming education.",
  "full_text": "Comparing Code Explanations Created by\nStudents and Large Language Models\nJuho Leinonen\nUniversity of Auckland\nAuckland, New Zealand\njuho.leinonen@auckland.ac.nz\nPaul Denny\nUniversity of Auckland\nAuckland, New Zealand\npaul@cs.auckland.ac.nz\nStephen MacNeil\nTemple University\nPhiladelphia, PA, United States\nstephen.macneil@temple.edu\nSami Sarsa\nAalto University\nEspoo, Finland\nsami.sarsa@aalto.fi\nSeth Bernstein\nTemple University\nPhiladelphia, PA, United States\nseth.bernstein@temple.edu\nJoanne Kim\nTemple University\nPhiladelphia, PA, United States\njoanne.kim@temple.edu\nAndrew Tran\nTemple University\nPhiladelphia, PA, United States\nandrew.tran10@temple.edu\nArto Hellas\nAalto University\nEspoo, Finland\narto.hellas@aalto.fi\nABSTRACT\nReasoning about code and explaining its purpose are fundamental\nskills for computer scientists. There has been extensive research\nin the field of computing education on the relationship between a\nstudent‚Äôs ability to explain code and other skills such as writing and\ntracing code. In particular, the ability to describe at a high-level of\nabstraction how code will behave over all possible inputs correlates\nstrongly with code writing skills. However, developing the exper-\ntise to comprehend and explain code accurately and succinctly is a\nchallenge for many students. Existing pedagogical approaches that\nscaffold the ability to explain code, such as producing exemplar code\nexplanations on demand, do not currently scale well to large class-\nrooms. The recent emergence of powerful large language models\n(LLMs) may offer a solution. In this paper, we explore the potential\nof LLMs in generating explanations that can serve as examples to\nscaffold students‚Äô ability to understand and explain code. To evalu-\nate LLM-created explanations, we compare them with explanations\ncreated by students in a large course ( ùëõ ‚âà1000) with respect to\naccuracy, understandability and length. We find that LLM-created\nexplanations, which can be produced automatically on demand, are\nrated as being significantly easier to understand and more accurate\nsummaries of code than student-created explanations. We discuss\nthe significance of this finding, and suggest how such models can\nbe incorporated into introductory programming education.\nCCS CONCEPTS\n‚Ä¢ Social and professional topics ‚ÜíComputing education; ‚Ä¢ Com-\nputing methodologies ‚ÜíNatural language generation .\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nITiCSE 2023, July 8‚Äì12, 2023, Turku, Finland\n¬© 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0138-2/23/07.\nhttps://doi.org/10.1145/3587102.3588785\nKEYWORDS\nnatural language generation, code comprehension, GPT-3, CS1,\ncode explanations, resource generation, large language models\nACM Reference Format:\nJuho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein,\nJoanne Kim, Andrew Tran, and Arto Hellas. 2023. Comparing Code Expla-\nnations Created by Students and Large Language Models. In Proceedings\nof the 2023 Conference on Innovation and Technology in Computer Science\nEducation V. 1 (ITiCSE 2023), July 8‚Äì12, 2023, Turku, Finland. ACM, New\nYork, NY, USA, 8 pages. https://doi.org/10.1145/3587102.3588785\n1 INTRODUCTION\nThe ability to understand and explain code is an important skill for\ncomputer science students to develop [7, 29, 44]. Prior computing\neducation research tends to suggest that proficiency at explaining\ncode develops for novices after lower-level code tracing skills and\nis a prerequisite for higher-level code writing skills [22, 37]. After\ngraduating, students will also be expected to explain their code to\nhiring managers during job interviews, explain code to their peers\nas they onboard new team members, and explain code to themselves\nwhen they first start working with a new code base. However,\nstudents struggle to explain their own code and the ability to explain\ncode is a difficult skill to develop [ 20, 38]. These challenges are\nfurther compounded by the fact that the ability to explain code is\nnot always explicitly included as a learning objective in CS courses.\nLearning by example is an effective pedagogical technique, often\nemployed in programming education [ 2, 48]. However, generat-\ning good examples for certain kinds of resources, such as code\nexplanations, can be time-consuming for instructors. While learn-\nersourcing techniques could be used to generate code explanations\nefficiently by directly involving students in their creation [21, 32],\nthere are known issues relating to quality when learning content\nis sourced from students [1, 9]. In search of a remedy to this prob-\nlem, researchers have explored the potential of ‚Äòrobosourcing‚Äô (i.e.,\nusing AI-based generators to create content or scaffold content\ncreation by humans) learning materials [ 11, 36], including code\narXiv:2304.03938v1  [cs.CY]  8 Apr 2023\nexplanations [24, 25]. At this stage, very little is known about how\nthe quality of AI-generated code explanations compare with code\nexplanations created by instructors or by students, and whether\nthey could be used as a replacement for either.\nWe compare the quality of learnersourced code explanations\nagainst robosourced code explanations to examine the potential of\nlarge language models (LLMs) in generating explanations for stu-\ndents to use as examples for learning. We used LLMs to create code\nexplanations of three functions, and we asked students to create\nexplanations of the same functions. We then measured students‚Äô\nperceptions of the quality of explanations from both sources. To\naid in the interpretation of our results, we elicit from students the\ncharacteristics of a code explanation that they find most useful. The\nfollowing two research questions have guided this work:\nRQ1 To what extent do code explanations created by students\nand LLMs differ in accuracy, length, and understandability?\nRQ2 What aspects of code explanations do students value?\nOur results show that the code explanations generated by LLMs\nand by students are equivalent in terms of ideal length, but that\nthe LLM-generated explanations are perceived as more accurate\nand easier to understand. Although there are benefits for students\nin being actively involved in producing their own explanations,\nwe conclude that LLM-generated explanations can serve as good\nexamples for students in early learn-by-example contexts and can\nbe a viable alternative for learnersourced code explanations.\n2 RELATED WORK\n2.1 Code Comprehension\nCode comprehension skills are important for helping programming\nstudents understand the logic and functionality behind code snip-\npets [39]. Programmers can employ various code comprehension\nstrategies that give them flexibility in the ways they comprehend\nprogramming concepts [43]. Some strategies include trace execu-\ntion [6], explanations [31], and notional machines [15]. These strate-\ngies take time and vary in effectiveness between students [17]. Re-\ngardless, students may face roadblocks, including logical errors [12]\nand syntactical errors [10] when trying to understand code.\nTop-down and bottom-up learning are two approaches to learn-\ning that focus on the big picture and the details, respectively [47].\nTop-down learning starts with the high-level concept and works\nits way down to the specifics, while bottom-up learning begins\nwith the details and gradually works up to the high-level [40]. Both\napproaches can be useful when teaching complex topics, as they\nprovide a way for learners to understand the whole concept by\nunderstanding its parts. In computer science and programming,\nthese two approaches can be used to help learners understand the\nfundamentals of coding and programming [34].\n2.2 Pedagogical Benefits of Code Explanations\nExplanations are vital teaching resources for students. Explanations\nhelp students develop their understanding of how a code snippet ex-\necutes [27], which can help students improve their reasoning about\nwriting their own code [29]. They also reduce stress by breaking\ndown complex concepts [14].\nEarly approaches for code explanation, such as the BRACElet\nproject, provided students with ‚Äòexplain-in-plain-English‚Äô type ques-\ntions to encourage students to explain the purpose of their code at\na higher level of abstraction [46]. This process of explaining one‚Äôs\nown code provided both short and long-term learning benefits for\nstudents [29, 42]. In large classrooms, the process of explaining\ncode can also be a collaborative activity where peers explain code\nto each other. This process can be more informal, such as in the\ncase of pair programming when students explain their code and\ntheir thought process to a partner as they write their code [16].\nEven though explaining code is an important skill and previous\nwork has explored code explanation tasks, students are rarely ex-\nposed to example code explanations, especially ones created by their\npeers. Having easily available example code explanations could help\nexpose students to code explanations, which could support learn-\ning to explain their own code. Having the instructor create such\nexplanations is a time-consuming task. In big classrooms, it would\nbe hard to find the time to provide personalized explanations for\nstudents [41]. Thus, studying if such explanations could be created\nat scale with the help of LLMs is a relevant research topic.\n2.3 Large Language Models in CS Education\nThe recent emergence of AI-based code generation models has\nsparked considerable interest within the field of computing edu-\ncation research [3]. Initial studies in this area have primarily fo-\ncused on evaluating the performance of these models when solving\nprogramming problems commonly encountered in introductory\ncourses. A seminal study in this field, entitled ‚ÄúThe Robots are\nComing‚Äù [13], utilized the Codex model and a private repository\nof programming problems drawn from high-stakes summative as-\nsessments. The results of the study indicated that the solutions\ngenerated by Codex scored approximately 80% on the assessments,\nsurpassing the performance of three-quarters of students when\ncompared to historical course data. Similar work involving a public\ndataset of programming problems found that Codex produced cor-\nrect solutions on its first attempt approximately half of the time,\nincreasing to 80% when repeated attempts and minor adjustments\nto the input prompt were allowed [8].\nIn addition to evaluating performance, a complementary body of\nresearch has investigated the potential of AI-based code-generation\nmodels to generate learning resources. For example, Sarsa et al. ex-\nplored various prompts and approaches for using the Codex model\nto generate code explanations and programming exercises, finding\nthat it frequently produced novel and high-quality resources [36].\nHowever, their evaluation was conducted solely by experts and did\nnot involve the use of resources by students in a practical setting.\nMacNeil et al. used the GPT-3 model to generate explanations of\nshort code fragments which then were presented to students in\nan online e-book alongside the corresponding code [24]. Although\ntheir evaluation was conducted on a small scale with approximately\n50 participants, students found the explanations to be useful when\nthey chose to engage with them. However, as the authors noted,\nthis engagement was lower than anticipated, and the students were\nnot involved in the creation of either the code examples or the\naccompanying explanations.\nThe current study makes a unique contribution by directly com-\nparing code explanations generated by students with those gen-\nerated by AI models. While prior research has demonstrated that\nLLMs can produce explanations of code that are deemed high-\nquality by both experts and novices, this is the first study to inves-\ntigate how students evaluate code explanations generated by their\npeers in comparison to those generated by AI models.\n3 METHOD\n3.1 Context and Data\nOur data for this study was collected in a first-year programming\ncourse at The University of Auckland. Approximately 1000 students\nwere enrolled in the course in 2022 when our study was conducted.\n3.1.1 Data collection. The data was collected during two separate\nlab sessions, each of which ran over a one-week period. At the\ntime of the first lab, when the data collection began, the course\nhad covered the concepts of arithmetic, types, functions, loops and\narrays in the C programming language. The data collection followed\nthe ethical guidelines of the university.\nDuring the first lab, Lab A, students were shown three function\ndefinitions and were asked to summarize and explain the intended\npurpose of each function. During the second lab, Lab B, which was\nconducted two weeks after the first, students were shown a random\nsample of four code explanations for the functions in Lab A. Some\nof these code explanations were selected from the explanations\ngenerated by students during Lab A, and some were generated\nby the large language model GPT-3 [ 4]. Students were asked to\nrate the explanations with respect to accuracy, understandability\nand length. At the end of Lab B, students were invited to provide\nan open-response answer to the following question: ‚ÄúNow that\nyou have created, and read, lots of code explanations, answer the\nfollowing question about what you believe are the most useful\ncharacteristics of a good code explanation: What is it about a code\nexplanation that makes it useful for you?‚Äù\nFigure 1 lists the three functions that were shown to students\nin Lab A. Each function includes a single loop that processes the\nelements of an array that is passed as input to the function, and has\na name that is representative of the algorithm being implemented.\nFor each of the three functions, students were asked to summarize\nand explain the intended purpose of the function. Specifically, they\nwere asked to: ‚Äúlook at the name of the function, the names of the\nvariables being used, and the algorithm the function implements\nand come up with a short description of what you believe is the\nintended purpose of the function‚Äù.\n3.1.2 Data sampling. Figure 2 provides an overview of the process\nused to sample the code explanations used in Lab B. Students who\nparticipated in generating code explanations in Lab A submitted\n963 explanations for each of the three functions. For each of the\nfunctions, we stratified the code explanations into three categories\nbased on their word length: 10th percentile, 10-90th percentile and\n90th percentile. From each of these three categories, we randomly\nselected three explanations, resulting in nine explanations for each\nof the three functions. To these 27 student-generated explanations,\nwe added 27 explanations created by GPT-3, by generating nine\nexplanations for each of the three functions. For Lab B, each student\nwas shown four explanations selected at random from the pool of\n54 explanations. They were asked to rate each of these with respect\nto the following three questions (each on a 5-point scale):\n‚Ä¢This explanation is easy to understand (5-items: Strongly\ndisagree, Disagree, Neutral, Agree, Strongly agree)\n‚Ä¢This explanation is an accurate summary of the code (5-\nitems: Strongly disagree, Disagree, Neutral, Agree, Strongly\nagree)\n‚Ä¢This explanation is the ideal length (5-items: Much too short,\nA little too short, Ideal, A little too long, Much too long)\n3.1.3 Analyses. To answer RQ1 and to quantify differences be-\ntween student-created and LLM-generated code explanations, we\ncompared student responses to the Likert-scale questions between\nthe two sources of code explanations.\nAs Likert-scale response data is ordinal, we used the non-para-\nmetric Mann‚ÄìWhitney U test [26] to test for differences in Likert-\nscale question data between student and LLM code explanations. We\ntested: (1) whether there was a difference in the code explanations\nbeing easy to understand; (2) whether there was a difference in the\ncode explanations being accurate summaries of the code; and (3)\nwhether there was a difference in the code explanations being of\nideal length. Further, we (4) studied the actual length of the code\nexplanations to form a baseline on whether the lengths of code\nexplanations differed between students and GPT-3, which could\nhelp interpret other findings.\nAltogether, we conducted four Mann‚ÄìWhitney U tests. To ac-\ncount for the multiple testing problem, we used Bonferroni cor-\nrected ùëù < 0.05/4 as the threshold of statistical significance. Fol-\nlowing the guidelines of [45] and the broader discussion in [35], we\nuse ùëùvalues as only one source of evidence and outline supporting\nstatistics including two effect sizes ‚Äì Rank-Biserial (RBC) Correla-\ntion [19] and Common-Language Effect Size (CLES) [28] ‚Äì when\npresenting the results of the study.\nTo answer RQ2, i.e., examine what aspects of code explanations\nstudents value, we conduct a thematic analysis of 100 randomly\nselected student responses to the open-ended question ‚ÄúWhat is it\nabout a code explanation that makes it useful for you?‚Äù.\n4 RESULTS\n4.1 Descriptive Statistics\nOverall, a total of 954 students participated in the activity where\nthey assessed the quality of code explanations. The averages and\nmedians for the responses, where Likert-scale responses have been\ntransformed to numeric values, are shown in Table 1, accompanied\nwith the mean code explanation length for both student-created\ncode explanations and LLM-generated code explanations.\nFigure 3 further outlines the distribution of the responses, sepa-\nrately color coding the different responses and allowing a visual\ncomparison of the different response values, which the numerical\noverview shown in Table 1 complements.\n4.2 Differences in Quality of Student- and\nLLM-Generated Code Explanations\nMann-Whitney U tests were conducted to study for differences be-\ntween the student- and LLM-generated code explanations. We used\n \nint LargestValue(int values[], int length)  \n{ \n    int i, max; \n \n    max = values[0]; \n    for (i = 1; i < length; i++) { \n        if (values[i] > max) { \n            max = values[i]; \n        } \n    } \n \n    return max; \n} \nint CountZeros(int values[], int length) \n{ \n    int i, count; \n \n    count = 0; \n    for (i = 0; i < length; i++) { \n        if (values[i] == 0) { \n            count++; \n        } \n    } \n \n    return count; \n} \ndouble AverageNegativeValues(int values[], int length) \n{ \n    int i, sum, count; \n    i = 0; \n    sum = 0; \n    count = 0; \n \n    while (i < length) { \n        if (values[i] < 0) { \n            sum = sum + values[i]; \n            count++; \n        } \n        i++; \n    } \n \n    return (double)sum / count; \n} \n \nFigure 1: The three function definitions, as presented to students in Lab A. Students were asked to construct a short description\nof the intended purpose of each function.\nLab A39~1000~1000~100033333333AverageNegativeValuesCountZerosLargestValueAverageNegativeValuesCountZerosLargestValue992727Lab B4GPT-3student generated explanationsevaluating explanationslength samplinglanguage modelgenerated explanations\nFigure 2: Overview of the generation and sampling of code\nexplanations. In Lab B, each student was allocated four code\nexplanations to evaluate, selected at random from a pool of\n54 code explanations (half of which were generated by stu-\ndents in Lab A, and half of which were generated by GPT-3.)\ntwo-sided tests, assessing for differences in the code explanations\nbeing easy to understand, accurate summaries of the shown code,\nand of ideal length. We further looked for differences between the\nactual length (in characters) of the code explanations.\nThe results of the statistical tests are summarized in Table 2.\nOverall, we observe statistically significant differences between the\nstudent- and LLM-generated code explanations in whether they are\neasy to understand and in whether they are accurate summaries\nof the code. As per Bonferroni correction, there is no statistically\nsignificant difference in student-perceptions of whether the code\nexplanations were of ideal length, and there is no statistically sig-\nnificant difference in the actual length of the code explanations.\nTable 1: Descriptive statistics of student responses on code\nexplanation quality. The responses that were given using a\nLikert-scale have been transformed so that 1 corresponds to\n‚ÄòStrongly disagree‚Äô and 5 corresponds to ‚ÄòStrongly agree‚Äô.\nStudent-generated LLM-generated\nMean Median Mean Median\nEasy to understand 3.75 4.0 4.12 4.0\nAccurate summary 3.78 4.0 4.0 4.0\nIdeal length 2.75 3.0 2.66 3.0\nLength (chars) 811 738 760 731\nTable 2: Mann-Whitney U test results from two-sided com-\nparisons in the quality of the student- and LLM authored\ncode explanations. The ùëà‚àíùë£ùëéùëô stands for the Mann-Whitney\nU test U value, ùëù outlines the probability (uncorrected) that\nthe responses come from the distribution, ùëÖùêµùê∂ is the Rank-\nBiserial Correlation of the data, and ùê∂ùêøùê∏ùëÜ provides the\nCommon-Language Effect Size.\nU-val ùëù ùëÖùêµùê∂ ùê∂ùêøùê∏ùëÜ\nEasy to understand 1421596.0 3.010 ¬∑10‚àí31 0.206 0.397\nAccurate summary 1568575.5 2.866 ¬∑10‚àí12 0.123 0.438\nIdeal length 1860870.5 0.025 -0.040 0.520\nLength (chars) 1779386.0 0.764 0.006 0.497\nOverall, interpreting the common language effect size (CLES)\nfrom Table 2, the proportion of student-generated and LLM-generated\ncode explanation pairs where the student-generated code expla-\nnation is easier to understand is approximately 40%, while the\nproportion of pairs where the LLM-generated code explanation is\neasier to understand is approximately 60%. Similarly, the proportion\nof student-generated and LLM-generated code explanation pairs\nwhere the student-generated code explanation is a more accurate\nsummary is approximately 44%, while the proportion of pairs where\nthe LLM-generated code explanation is a more accurate summary\nis approximately 56%. Although these differences are statistically\nFigure 3: Distribution of student responses on LLM and student-generated code explanations being easy to understand and\naccurate summaries of code.\nsignificant (and visible, as seen in Figure 3), the effect sizes can be\nconsidered weak (ùëÖùêµùê∂ < 0.3).\n4.3 Examples of Good and Bad Explanations\nfrom GPT-3 and Students\nOur quantitative results suggest that LLM-generated explanations\nwere more accurate and easier to understand than those gener-\nated by students. In the thematic analysis related to what students\nvalue in code explanations, we found that the LLM-generated and\nstudent-generated explanations varied in different thematic quali-\nties. These include differences in levels of accuracy, completeness,\nabstraction, and expert vocabulary used. In an open-ended response,\nmost students described a good explanation as one that ‚Äúsuccinctly\nsummarized the function of the code by describing what its inputs\n[and outputs] are, ‚Äù illustrated ‚Äúhow the code works in coding terms\nbut also explains what the code is trying to achieve in basic English\nterms, ‚Äù and was ‚Äúnice and simple but still has a good amount of\nlength to describe the whole code. ‚Äù\nNext, to illustrate the types of explanations that GPT-3 and stu-\ndents created, we present four example explanations that differ\nin terms of student-perceived quality ‚Äì the highest- and lowest-\nrated LLM- and student-created explanations for the LargestValue\nfunction. The examples were ranked based on the average between\naccuracy and understandability.\nThe following LLM-generated explanation was rated the highest\n(understandability = 4.2, accuracy = 4.5, length = 3.5):\n‚ÄúThis code snippet finds the largest value in an integer\narray. It starts by assuming that the first value in\nthe array is the largest. It then loops through the\nremaining values in the array, comparing each value\nto the max. If a value is larger than the max, it becomes\nthe new max. This process is repeated until all values\nin the array have been checked. The largest value is\nthen returned. ‚Äù\nThe following LLM-generated explanation was rated the lowest\n(understandability = 4.1, accuracy = 3.7, length = 2.0):\n‚ÄúThis code snippet returns the largest value in an\narray of integers. ‚Äù\nThe following student-created explanation received the highest\nevaluations (understandability = 4.2, accuracy = 4.5, length = 3.6):\n‚ÄúThe function LargestValue, finds the largest integer\nvalue in the input array, and outputs that value as\ntype int. It takes two inputs:\n* values - an integer array\n* length - the length of that array (number of elements\ninside the array)\nIt sets the first integer value in the array as the original\n‚Äòmax‚Äô value (largest integer in the array) and then\nscans through the input array (left to right), checking\nto find if there is a larger integer value, which if found\nit stores in it‚Äôs output integer variable - ‚Äòmax‚Äô, which\nis returned at the end of the function. ‚Äù\nThe following student-generated explanation received the lowest\nratings (understandability = 3.3, accuracy = 3.3, length = 1.6):\n‚Äúto find the largest value within the array‚Äù\nStudents preferred explanations that had both information re-\nlated to what the function is and how it operates being described\nin a line-by-line format. Many students also agreed that a good\nexplanation explains the inputs and outputs of the code.\nBad explanations were characterized as those that missed some\ndetails of the code while either being too long or too short. For\nexample, an explanation may state at a high level the purpose of\nthe code, but not go into detail about what data structures were\nused, or what inputs are given to the function.\nInterestingly, we found that all of the LLM-generated explana-\ntions started out with the statement ‚ÄúThis code snippet‚Äù or ‚ÄúThe\npurpose of this code snippet‚Äù while the student generated explana-\ntions differed more. This was partially due to the prompting of the\nLLM, where it was asked to explain the purpose of ‚Äúthe following\ncode snippet‚Äù. However, most of the explanations by both students\nand the LLM generally followed a similar structure: function‚Äôs pur-\npose, analysis of the code, and finally the return output.\n4.4 Characterizing Code Explanations\nIn the thematic analysis (n=100), we found that students were al-\nmost evenly split between focusing on specific (n=57) and generic\n(n=65) aspects of the code with some students‚Äô responses including\nboth. When focusing on specific aspects of code students described\nthe need for a line-by-line explanation (21%). Students also focused\non even lower-level details like the names of variables, the input\nand output parameters (36%), and defining terms (8%). Some stu-\ndents asked for additional aspects that were rarely included in code\nexplanations. For example, students requested examples, templates,\nand the thought process behind how the code was written.\nStudents commented extensively about the qualities that make\na good explanation. Length was an important aspect with 40% of\nthe students commenting explicitly on the length of an explanation.\nHowever, there was no clear consensus about the exact length\nthat was ideal. Instead, comments tended to focus on efficiency;\nconveying the most information with the fewest words. Students\nappeared to rate short explanations low, even when the explanation\nwas to the point and might be something that a teacher would\nappreciate. This may be partly due to such explanations giving them\nlittle or no additional information that was not already obvious in\nthe function, e.g. the function name. Students, them being novices,\nlikely preferred more detailed explanations since it helps them\nbetter learn and understand what is actually going on in the code.\n5 DISCUSSION\n5.1 Differences Between Student- and\nLLM-Created Code Explanations\nGithub Copilot and similar tools have made code comprehension\nan even more important skill by shifting the focus from writing\ncode to understanding the purpose of code, evaluating whether the\ncode generated is appropriate, and modifying the code as needed.\nHowever, it is also possible that LLMs can not only help students\nto generate code, but also help them understand it by creating code\nexplanations which can be used as code comprehension exercises.\nWe found that the code explanations created by GPT-3 were rated\nbetter on average in understandability and accuracy compared to\ncode explanations created by students. This suggests that LLM-\ncreated code explanations could be used as examples on courses\nwith the goal of supporting students in learning to read code. There\nwere no differences in either perceived or actual length of student-\nand LLM-created code explanations, so the increased ratings are\nnot due to the LLM creating longer (or shorter) explanations.\nWe believe that code explanations created by LLMs could be a\nhelpful scaffolding for students who are at the stage where they\ncan understand code explanations created by the LLM but are not\nyet skilled enough to create code explanations of their own. LLM-\ncreated code explanations could also be used as examples that could\nhelp students craft code explanations of their own.\nOne downside mentioned in previous work is potential over-\nreliance on LLM support [5, 13]. One way to combat over-reliance\non LLM-created code explanations would be to monitor student\nuse of this type of support (e.g., giving students a limited number\nof tokens [ 30] that would be used as they request explanations\nfrom an LLM) to limit student use of, or reliance, on these tools.\nFor example, students could get a fixed number of tokens to start\nwith and use up tokens by requesting explanations ‚Äì and then earn\ntokens by writing their own hand-crafted code explanations.\n5.2 What Do Students Value in Code\nExplanations?\nWe found in our thematic analysis that students expressed a prefer-\nence for line-by-line explanations. This is also the type of explana-\ntion that LLMs seem to be best at creating [36]. This finding was\nsomewhat surprising as prior work on ‚Äòexplain-in-plain-English‚Äô\ncode explanation tasks has typically rated ‚Äòrelational‚Äô responses\n‚Äì short, abstract descriptions of the purpose of the code ‚Äì higher\nthan ‚Äòmulti-structural‚Äô ‚Äì line-by-line ‚Äì responses. This suggests that\nthere might be a mismatch between instructor and student opin-\nions on what makes a good explanation. It might even be that some\nprior work has ‚Äúunfairly‚Äù rated student multi-structural explana-\ntions lower since students might have possibly been able to produce\nthe more abstract relational explanations, but were thinking longer,\nmore detailed explanations are ‚Äúbetter‚Äù and thus produced those\ntypes of explanations.\nIn the thematic analysis, we also observed that the LLM-created\nexplanations closely followed a standard format. It is possible that\nshowing students LLM-created explanations could help them adopt\na standard format for their own explanations, which would possibly\nhelp make better explanations. This would be similar to prior work\nthat has shown that templates can help designers frame better\nproblems [23] and writers write better emails [18].\n5.3 Limitations\nThere are limitations to our work, which we outline here. First,\nrelated to generalizability, the students in our study were novices.\nThis might affect both the types of explanations they create as well\nas how they rate the explanations created by their peers and GPT-3.\nFor example, prior work has found differences in how students\nand instructors rate learnersourced programming exercises [ 33].\nIt is possible ‚Äì even likely ‚Äì that more advanced students, or e.g.\ninstructors, could create code explanations that would be rated\nhigher than the explanations created by GPT-3. Novices might also\nvalue different types of explanations than more advanced students:\nfor example, it is possible that once students get more experience,\nthey will start valuing more abstract, shorter explanations.\nRelated to the code being explained, we only provided students\ncorrect code in this study. An interesting avenue of future work\nis evaluating student and LLM performance in explaining and de-\ntecting bugs in incorrect code. The functions being explained were\nalso relatively simple. Future work should study explanations for\nmore varied and complex functions.\nIn this exploratory work, we only looked at student percep-\ntions on the quality of the explanations. Future work should study\nwhether there are differences in student learning when using student-\nand LLM-created code explanations.\nWe acknowledge that we analyzed the data in aggregate, i.e.,\nsome students might have only seen LLM-created explanations\nand some only student-created ones. We did a brief analysis of\nthe data for students who saw two LLM-created explanations and\ntwo student-created explanations, and observed similar effects as\nreported in this study, and thus believe aggregating over all students\nis methodologically valid.\nLastly, we used the davinci-text-002 version of GPT-3. A newer\nversion, davinci-text-003, was released in November 2022. Using the\nnewer LLM-model would likely yield at least similar performance,\nif not better.\n6 CONCLUSION\nIn this work, we presented a study where students created code\nexplanations and then evaluated their peers‚Äô code explanations\nas well as code explanations created by GPT-3. We found that\nstudents rated the code explanations created by GPT-3 higher in\nboth accuracy and understandability, even though there were no\ndifferences in the perceived or actual length of the student and LLM-\ncreated code explanations. Further, we found that students preferred\ndetailed explanations over concise high-level explanations.\nOur results suggest that LLM-created code explanations are good,\nand thus could be useful for students who are practicing code read-\ning and explaining. We argue that these skills are becoming even\nmore relevant with the advent of large language model based AI\ncode generators such as GitHub Copilot as the role of software de-\nvelopers in the future will increasingly be to evaluate LLM-created\nsource code instead of writing code from scratch.\nACKNOWLEDGMENTS\nWe are grateful for the grant from the Ulla Tuominen Foundation\nto the first author.\nREFERENCES\n[1] Solmaz Abdi, Hassan Khosravi, Shazia Sadiq, and Gianluca Demartini. 2021.\nEvaluating the Quality of Learning Resources: A Learnersourcing Approach.\nIEEE Transactions on Learning Technologies 14, 1 (2021), 81‚Äì92.\n[2] Siti-Soraya Abdul-Rahman and Benedict du Boulay. 2014. Learning programming\nvia worked-examples: Relation of learning styles to cognitive load. Computers in\nHuman Behavior 30 (2014), 286‚Äì298. https://doi.org/10.1016/j.chb.2013.09.007\n[3] Brett A. Becker, Paul Denny, James Finnie-Ansley, Andrew Luxton-Reilly, James\nPrather, and Eddie Antonio Santos. 2023. Programming Is Hard - Or at Least It\nUsed to Be: Educational Opportunities and Challenges of AI Code Generation. In\nProc. of the 54th ACM Technical Symposium on Computer Science Education V. 1 .\nACM, 500‚Äì506.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. 2021. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021).\n[6] Bas Cornelissen, Andy Zaidman, and Arie van Deursen. 2011. A Controlled\nExperiment for Program Comprehension through Trace Visualization. IEEE\nTransactions on Software Engineering 37, 3 (2011), 341‚Äì355.\n[7] Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor O‚ÄôRourke. 2022. Bring-\ning \"High-Level\" Down to Earth: Gaining Clarity in Conversational Programmer\nLearning Goals. In Proc. of the 53rd ACM Technical Symposium on Computer\nScience Education V. 1 (Providence, RI, USA) (SIGCSE 2022) . ACM, 551‚Äì557.\n[8] Paul Denny, Viraj Kumar, and Nasser Giacaman. 2023. Conversing with Copilot:\nExploring prompt engineering for solving CS1 problems using natural language.\nIn Proc. of the 54th ACM Technical Symposium on Computer Science Education V. 1 .\n1136‚Äì1142.\n[9] Paul Denny, Andrew Luxton-Reilly, and Beth Simon. 2009. Quality of Student\nContributed Questions Using PeerWise. In Proc. of the Eleventh Australasian\nConf. on Computing Education - Volume 95 (Wellington, New Zealand) (ACE ‚Äô09).\nAustralian Computer Society, Inc., AUS, 55‚Äì63.\n[10] Paul Denny, Andrew Luxton-Reilly, and Ewan Tempero. 2012. All Syntax Errors\nAre Not Equal. InProc. of the 17th ACM Annual Conf. on Innovation and Technology\nin Computer Science Education (Haifa, Israel) (ITiCSE ‚Äô12) . ACM, New York, NY,\nUSA, 75‚Äì80. https://doi.org/10.1145/2325296.2325318\n[11] Paul Denny, Sami Sarsa, Arto Hellas, and Juho Leinonen. 2022. Robosourcing\nEducational Resources‚ÄìLeveraging Large Language Models for Learnersourcing.\narXiv preprint arXiv:2211.04715 (2022).\n[12] Andrew Ettles, Andrew Luxton-Reilly, and Paul Denny. 2018. Common logic\nerrors made by novice programmers. In Proc. of the 20th Australasian Computing\nEducation Conf. 83‚Äì89.\n[13] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope-\nnAI Codex on Introductory Programming. In Australasian Computing Education\nConf. ACM, 10‚Äì19.\n[14] Jean M. Griffin. 2016. Learning by Taking Apart: Deconstructing Code by Read-\ning, Tracing, and Debugging. In Proc. of the 17th Annual Conf. on Information\nTechnology Education. ACM, 148‚Äì153.\n[15] Philip J Guo. 2013. Online python tutor: embeddable web-based program vi-\nsualization for cs education. In Proc. of the 44th ACM technical symposium on\nComputer science education . 579‚Äì584.\n[16] Brian Hanks, Sue Fitzgerald, Ren√©e McCauley, Laurie Murphy, and Carol Zander.\n2011. Pair programming in education: a literature review. Computer Science\nEducation 21, 2 (2011), 135‚Äì173. https://doi.org/10.1080/08993408.2011.579808\n[17] Regina Hebig, Truong Ho-Quang, Rodi Jolak, Jan Schr√∂der, Humberto Linero,\nMagnus √Ögren, and Salome Honest Maro. 2020. How do Students Experience\nand Judge Software Comprehension Techniques?. InProc. of the 28th Int. Conf.\non Program Comprehension . 425‚Äì435.\n[18] Julie S Hui, Darren Gergle, and Elizabeth M Gerber. 2018. Introassist: A tool\nto support writing introductory help requests. In Proc. of the 2018 CHI Conf. on\nHuman Factors in Computing Systems . 1‚Äì13.\n[19] Dave S Kerby. 2014. The simple difference formula: An approach to teaching\nnonparametric correlation. Comprehensive Psychology 3 (2014), 11‚ÄìIT.\n[20] Teemu Lehtinen, Aleksi Lukkarinen, and Lassi Haaranen. 2021. Students Struggle\nto Explain Their Own Program Code. InProc. of the 26th ACM Conf. on Innovation\nand Technology in Computer Science Education V. 1 . ACM, 206‚Äì212.\n[21] Juho Leinonen, Nea Pirttinen, and Arto Hellas. 2020. Crowdsourcing Content\nCreation for SQL Practice. In Proc. of the 2020 ACM Conf. on Innovation and\nTechnology in Computer Science Education . 349‚Äì355.\n[22] Raymond Lister, Colin Fidge, and Donna Teague. 2009. Further Evidence of a\nRelationship between Explaining, Tracing and Writing Skills in Introductory\nProgramming. SIGCSE Bull. 41, 3 (2009), 161‚Äì165.\n[23] Stephen MacNeil, Zijian Ding, Kexin Quan, Thomas j Parashos, Yajie Sun, and\nSteven P Dow. 2021. Framing Creative Work: Helping Novices Frame Better\nProblems through Interactive Scaffolding. In Creativity and Cognition . 1‚Äì10.\n[24] Stephen MacNeil, Andrew Tran, Arto Hellas, Joanne Kim, Sami Sarsa, Paul\nDenny, Seth Bernstein, and Juho Leinonen. 2023. Experiences from using code\nexplanations generated by large language models in a web software development\ne-book. In Proc. of the 54th ACM Technical Symposium on Computer Science\nEducation V. 1 . 931‚Äì937.\n[25] Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, and Ziheng\nHuang. 2022. Generating Diverse Code Explanations Using the GPT-3 Large\nLanguage Model. In Proc. of the 2022 ACM Conf. on Int. Computing Education\nResearch - Volume 2 . ACM, 37‚Äì39.\n[26] Henry B Mann and Donald R Whitney. 1947. On a test of whether one of\ntwo random variables is stochastically larger than the other. The annals of\nmathematical statistics (1947), 50‚Äì60.\n[27] Samiha Marwan, Nicholas Lytle, Joseph Jay Williams, and Thomas Price. 2019.\nThe Impact of Adding Textual Explanations to Next-Step Hints in a Novice\nProgramming Environment. In Proc. of the 2019 ACM Conf. on Innovation and\nTechnology in Computer Science Education . ACM, 520‚Äì526.\n[28] Kenneth O McGraw and Seok P Wong. 1992. A common language effect size\nstatistic. Psychological bulletin 111, 2 (1992), 361.\n[29] Laurie Murphy, Sue Fitzgerald, Raymond Lister, and Ren√©e McCauley. 2012.\nAbility to ‚Äôexplain in Plain English‚Äô Linked to Proficiency in Computer-Based\nProgramming. In Proc. of the Ninth Annual Int. Conf. on Int. Computing Education\nResearch. ACM, 111‚Äì118.\n[30] Henrik Nygren, Juho Leinonen, Nea Pirttinen, Antti Leinonen, and Arto Hellas.\n2019. Experimenting with model solutions as a support mechanism. In Proc. of\nthe 1st UK & Ireland Computing Education Research Conf. 1‚Äì7.\n[31] Steve Oney, Christopher Brooks, and Paul Resnick. 2018. Creating Guided Code\nExplanations with Chat.Codes. Proc. ACM Hum.-Comput. Interact. 2, CSCW,\nArticle 131 (nov 2018), 20 pages. https://doi.org/10.1145/3274400\n[32] Nea Pirttinen, Vilma Kangas, Irene Nikkarinen, Henrik Nygren, Juho Leinonen,\nand Arto Hellas. 2018. Crowdsourcing programming assignments with Crowd-\nSorcerer. In Proc. of the 23rd Annual ACM Conf. on Innovation and Technology in\nComputer Science Education . 326‚Äì331.\n[33] Nea Pirttinen and Juho Leinonen. 2022. Can Students Review Their Peers?\nComparison of Peer and Instructor Reviews. In Proc. of the 27th ACM Conf. on\nInnovation and Technology in Computer Science Education Vol 1 .\n[34] Margaret M. Reek. 1995. A Top-down Approach to Teaching Programming.\nIn Proc. of the Twenty-Sixth SIGCSE Technical Symposium on Computer Science\nEducation. ACM, 6‚Äì9.\n[35] Kate Sanders, Judy Sheard, Brett A Becker, Anna Eckerdal, and Sally Hamouda.\n2019. Inferential statistics in computing education research: A methodological\nreview. In Proc. of the 2019 ACM conf. on int. comp. education research . 177‚Äì185.\n[36] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. 2022. Automatic Gen-\neration of Programming Exercises and Code Explanations Using Large Language\nModels. In Proc. of the 2022 ACM Conf. on Int. Computing Education Research -\nVolume 1. ACM, 27‚Äì43.\n[37] Judy Sheard, Angela Carbone, Raymond Lister, Beth Simon, Errol Thompson,\nand Jacqueline L. Whalley. 2008. Going SOLO to Assess Novice Programmers. In\nProc. of the 13th Annual Conf. on Innovation and Technology in Computer Science\nEducation. ACM, 209‚Äì213.\n[38] Simon and Susan Snowdon. 2011. Explaining Program Code: Giving Students the\nAnswer Helps - but Only Just. In Proc. of the Seventh Int. Workshop on Computing\nEducation Research . ACM, 93‚Äì100.\n[39] Leigh Ann Sudol-DeLyser, Mark Stehlik, and Sharon Carver. 2012. Code Com-\nprehension Problems as Learning Events. In Proc. of the 17th ACM Annual Conf.\non Innovation and Technology in Computer Science Education . ACM, 81‚Äì86.\n[40] Ron Sun, Edward Merrill, and Todd Peterson. 2000. Knowledge Acquisition Via\nBottom-up Learning. Knowledge-Based Systems (2000), 249‚Äì291.\n[41] Zahid Ullah, Adidah Lajis, Mona Jamjoom, Abdulrahman Altalhi, Abdullah Al-\nGhamdi, and Farrukh Saleem. 2018. The effect of automatic assessment on\nnovice programming: Strengths and limitations of existing systems. Computer\nApplications in Engineering Education 26, 6 (2018), 2328‚Äì2341.\n[42] Arto Vihavainen, Craig S Miller, and Amber Settle. 2015. Benefits of self-\nexplanation in introductory programming. In Proc. of the 46th ACM Technical\nSymposium on Computer Science Education . 284‚Äì289.\n[43] A. Von Mayrhauser and A.M. Vans. 1995. Program comprehension during soft-\nware maintenance and evolution. Computer 28, 8 (1995), 44‚Äì55.\n[44] Wengran Wang, Yudong Rao, Rui Zhi, Samiha Marwan, Ge Gao, and Thomas W.\nPrice. 2020. Step Tutor: Supporting Students through Step-by-Step Example-\nBased Feedback. In Proc. of the 2020 ACM Conf. on Innovation and Technology in\nComputer Science Education . ACM, 391‚Äì397.\n[45] Ronald L Wasserstein and Nicole A Lazar. 2016. The ASA statement on p-values:\ncontext, process, and purpose. The American Statistician 70, 2 (2016), 129‚Äì133.\n[46] Jacqueline L. Whalley, Raymond Lister, Errol Thompson, Tony Clear, Phil Robbins,\nP. K. Ajith Kumar, and Christine Prasad. 2006. An Australasian Study of Reading\nand Comprehension Skills in Novice Programmers, Using the Bloom and SOLO\nTaxonomies. In Proc. of the 8th Australasian Conf. on Computing Education -\nVolume 52. Australian Computer Society, Inc., AUS, 243‚Äì252.\n[47] Honglin Wu, Fu Zhang, Jingwei Cheng, and Ke Wang. 2019/11. Determine\nTeaching Content using a Bottom-up Approach. In Proc. of the 2nd Int. Conf. on\nHumanities Education and Social Sciences (ICHESS 2019) . Atlantis Press, 597‚Äì600.\n[48] Rui Zhi, Thomas W. Price, Samiha Marwan, Alexandra Milliken, Tiffany Barnes,\nand Min Chi. 2019. Exploring the Impact of Worked Examples in a Novice\nProgramming Environment. In Proc. of the 50th ACM Technical Symposium on\nComputer Science Education . ACM, 98‚Äì104.",
  "topic": "Code (set theory)",
  "concepts": [
    {
      "name": "Code (set theory)",
      "score": 0.6981102228164673
    },
    {
      "name": "Computer science",
      "score": 0.6543591022491455
    },
    {
      "name": "Abstraction",
      "score": 0.653759241104126
    },
    {
      "name": "Tracing",
      "score": 0.6168485879898071
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5507680773735046
    },
    {
      "name": "Mathematics education",
      "score": 0.43043771386146545
    },
    {
      "name": "Programming language",
      "score": 0.32524341344833374
    },
    {
      "name": "Psychology",
      "score": 0.2604250907897949
    },
    {
      "name": "Epistemology",
      "score": 0.10154879093170166
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}