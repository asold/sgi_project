{
    "title": "Toward Software-Equivalent Accuracy on Transformer-Based Deep Neural Networks With Analog Memory Devices",
    "url": "https://openalex.org/W3184497370",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5109465051",
            "name": "Katie Spoon",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5086723230",
            "name": "Hsinyu Tsai",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5100687057",
            "name": "An Chen",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5008815965",
            "name": "Malte J. Rasch",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5028455655",
            "name": "Stefano Ambrogio",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5015811754",
            "name": "Charles Mackin",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5026800272",
            "name": "Andrea Fasoli",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5079173979",
            "name": "Alexander Friz",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5058290381",
            "name": "Pritish Narayanan",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5089303681",
            "name": "Miloš Stanisavljević",
            "affiliations": [
                "IBM Research - Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5014981734",
            "name": "Geoffrey W. Burr",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4241020465",
        "https://openalex.org/W3005874416",
        "https://openalex.org/W2803163155",
        "https://openalex.org/W2338228311",
        "https://openalex.org/W2560615381",
        "https://openalex.org/W1937359183",
        "https://openalex.org/W2968365336",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6605323724",
        "https://openalex.org/W6834712115",
        "https://openalex.org/W2913035851",
        "https://openalex.org/W3005589975",
        "https://openalex.org/W2114924497",
        "https://openalex.org/W2948661249",
        "https://openalex.org/W6768021236",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2026527606",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3176606713",
        "https://openalex.org/W6768851824",
        "https://openalex.org/W6691459498",
        "https://openalex.org/W2604319603",
        "https://openalex.org/W2964447465",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6750615492",
        "https://openalex.org/W6752342493",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W6769243733",
        "https://openalex.org/W2004823737",
        "https://openalex.org/W4253067820",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3101272433",
        "https://openalex.org/W2978670439"
    ],
    "abstract": "Recent advances in deep learning have been driven by ever-increasing model sizes, with networks growing to millions or even billions of parameters. Such enormous models call for fast and energy-efficient hardware accelerators. We study the potential of Analog AI accelerators based on Non-Volatile Memory, in particular Phase Change Memory (PCM), for software-equivalent accurate inference of natural language processing applications. We demonstrate a path to software-equivalent accuracy for the GLUE benchmark on BERT (Bidirectional Encoder Representations from Transformers), by combining noise-aware training to combat inherent PCM drift and noise sources, together with reduced-precision digital attention-block computation down to INT6.",
    "full_text": "ORIGINAL RESEARCH\npublished: 05 July 2021\ndoi: 10.3389/fncom.2021.675741\nFrontiers in Computational Neuroscience | www.frontiersi n.org 1 July 2021 | Volume 15 | Article 675741\nEdited by:\nAlexantrou Serb,\nUniversity of Southampton,\nUnited Kingdom\nReviewed by:\nDamien Querlioz,\nCentre National de la Recherche\nScientiﬁque (CNRS), France\nMatthew Marinella,\nSandia National Laboratories (SNL),\nUnited States\nDaniele Ielmini,\nPolitecnico di Milano, Italy\n*Correspondence:\nKatie Spoon\nkatherine.spoon@colorado.edu\nHsinyu Tsai\nhtsai@us.ibm.com\nReceived: 03 March 2021\nAccepted: 14 May 2021\nPublished: 05 July 2021\nCitation:\nSpoon K, Tsai H, Chen A, Rasch MJ,\nAmbrogio S, Mackin C, Fasoli A,\nFriz AM, Narayanan P , Stanisavljevic M\nand Burr GW (2021) Toward\nSoftware-Equivalent Accuracy on\nTransformer-Based Deep Neural\nNetworks With Analog Memory\nDevices.\nFront. Comput. Neurosci. 15:675741.\ndoi: 10.3389/fncom.2021.675741\nToward Software-Equivalent\nAccuracy on Transformer-Based\nDeep Neural Networks With Analog\nMemory Devices\nKatie Spoon1*, Hsinyu Tsai1*, An Chen1, Malte J. Rasch2, Stefano Ambrogio1,\nCharles Mackin1, Andrea Fasoli1, Alexander M. Friz1, Pritish Narayanan1,\nMilos Stanisavljevic3 and Geoffrey W. Burr1\n1 IBM Research–Almaden, San Jose, CA, United States, 2 IBM T. J. Watson Research Center, Yorktown Heights, NY,\nUnited States, 3 IBM Zurich Research Center, Zurich, Switzerland\nRecent advances in deep learning have been driven by ever-in creasing model sizes, with\nnetworks growing to millions or even billions of parameters . Such enormous models call\nfor fast and energy-efﬁcient hardware accelerators. We stu dy the potential of Analog AI\naccelerators based on Non-Volatile Memory, in particular P hase Change Memory (PCM),\nfor software-equivalent accurate inference of natural lan guage processing applications.\nWe demonstrate a path to software-equivalent accuracy for t he GLUE benchmark\non BERT (Bidirectional Encoder Representations from Trans formers), by combining\nnoise-aware training to combat inherent PCM drift and noise sources, together with\nreduced-precision digital attention-block computation d own to INT6.\nKeywords: analog accelerators, BERT, PCM, RRAM, in-memory computing, DNN, Transformer\n1. INTRODUCTION\nState-of-the-art Deep Neural Networks (DNNs) have now demo nstrated unparalleled accuracy\nperformance across a wide variety of ﬁelds, including image cla ssiﬁcation, speech recognition,\nmachine translation, and text generation (\nLeCun et al., 2015 ). While current models are generally\ntrained and run on general-purpose digital processors such as CP Us and GPUs, the rapid growth in\nboth size and scope of these networks has fostered novel hardw are architectures aiming to optimize\nspeed and energy-eﬃciency, speciﬁcally targeting either neu ral network training or inference ( Sze\net al., 2017 ).\nAmong these, architectures based on Non-Volatile Memory (NV M) are increasingly gaining\ninterest. Such technologies encode weight information in t he conductance states of two-terminal\ndevices — including Resistive RAM (RRAM) ( Wong et al., 2012 ), using modulation of conductive\nﬁlaments between electrodes, or Magnetic RAM (MRAM) ( Matsukura et al., 2015 ), using\nferromagnetic switching between parallel or antiparallel spin polarization. In particular, Phase-\nChange Memory (PCM) ( Burr et al., 2016 ) is based on thermally-driven reversible transitions\nbetween amorphous and crystalline states of a chalcogenide l ayer, leading to low and high\nconductances, respectively ( Figure 1A).\nAnalog accelerators leverage the massive parallelism of NVM- based crossbar arrays to perform\ncomputation at the location of data ( Burr et al., 2017; Ambrogio et al., 2018 ; Figure 1B). This\narchitecture can signiﬁcantly mitigate the Von-Neumann bo ttleneck caused by communication\nbetween the processor and memory, and is particularly eﬃcient for fully-connected neural network\nlayers (Burr et al., 2015 ).\nSpoon et al. Analog AI for Transformer Networks\nA recent development in DNN-based natural language\nprocessing (NLP) is the migration away from recurrence\ntoward Transformer-based models such as BERT (Bidirection al\nEncoder Representations from Transformers) (\nDevlin et al.,\n2018). BERT oﬀers state-of-the-art performance over a wide\nrange of Natural Language Processing (NLP) tasks. While the\nlarge fully-connected layers in these models are computation ally\nexpensive for both conventional hardware and custom digital\naccelerators, they are ideally suited for analog NVM-based\nhardware acceleration. However, NVM devices exhibit many\nconductance instabilities [conductance drift (\nAmbrogio et al.,\n2019), programming and read noise ( Tsai et al., 2019 ), etc.],\nwhich can degrade accuracy, particularly as the time between\nprogramming and inference increases.\nIn this paper, after a brief overview of Transformer-based\nmodels including BERT, we use a device-aware simulation\nframework to develop and assess techniques that can increas e the\ninference accuracy of BERT implemented using PCM devices.\nWe show that these techniques allow these inherently fast and\nenergy-eﬃcient systems to also approach software-equivalen t\naccuracy [as compared to the original BERT implementation\n(\nDevlin et al., 2018 )], despite the signiﬁcant noise and\nimperfections of current PCM devices. Since the high energy-\neﬃciency of analog crossbar-arrays on the fully-connected l ayers\nwill then expose the energy-ineﬃciency in digital computatio n\nof the attention blocks, we explore the impact of quantized\nattention-block computation. We show that the use of reduced\nprecision down to INT6 can provide further energy optimization\nfor Transformer-based models, applicable both to analog NVM-\nbased as well as to other accelerator systems.\n1.1. Transformer Architecture\nThe Transformer architecture (\nVaswani et al., 2017 ) was a pivotal\nchange-point in deep learning and is expected to remain a criti cal\ncore as new models [BERT ( Devlin et al., 2018 ), DistilBERT (Sanh\net al., 2019 ), Albert ( Lan et al., 2020 ), etc.] continue to build upon\nFIGURE 1 |RRAM, MRAM or PCM devices (A) can be organized in crossbar\narrays, or NVM tiles, where weights are encoded using pairs o f devices (B).\nAnalog accelerators composed of multiple NVM tiles and spec ial function units\n(SFU) for digital computation enable end-to-end network in ference (C).\nthe underlying Transformer architecture. Here we describe how\nthe Transformer architecture diﬀers from recurrent DNNs, an d\nhow the basic building blocks of Transformers map to analog\naccelerators.\n1.1.1. Why Transformer?\nRecurrent neural networks (RNNs) have commonly been used\nfor NLP tasks to account for the sequential nature of words an d\nsentences ( Figure 2A). The bottleneck of RNNs is their limited\n“memory” over very long sequences. Transformers (\nVaswani\net al., 2017 ) provide one solution by replacing recurrence with a\nself-attention mechanism. For any given word w in the sequence,\nan attention probability between 0 and 1 is computed between w\nand every other word in the sequence ( Figure 2B), allowing the\nmodel to quantify the relative importance that each word has i n\npredicting w.\n1.1.2. BERT -Base Model Architecture\nBuilding on the initial success of Transformers, BERT was\ndeveloped to generate meaningful encodings of input sequence s\nuseful across a broad range of downstream tasks, such as\nclassiﬁcation, text generation, and machine translation, requiring\nonly a few epochs of subsequent ﬁne-tuning to prepare for the\nspeciﬁc task. BERT consists of 12 layers of a large Transforme r\nencoder ( Figure 3A). In Figure 3B, detailing the main building\nblocks of each encoder layer, dark grey boxes represent train ed\nweight-matrices (fully-connected layers) that can readily be\nmapped to analog crossbar arrays. The attention computations\n(Figure 3C) along with all activation functions (representing a\nsmall fraction of the total operations) are computed in digita l\nprocessing units.\n2. MATERIALS AND METHODS\n2.1. Optimizing Analog Accuracy for BERT\nIn this section, we ﬁrst describe the comprehensive analog til e\nmodel used in this paper to capture realistic PCM crossbar\narray behavior. We then describe our simulation procedure\nand datasets for evaluation before discussing inference ac curacy\nresults. The simulator is implemented using a modiﬁed pytorch\nframework (\nPaszke et al., 2019 ) (including Caﬀe2).\nFIGURE 2 | (A)Recurrent Neural Networks (RNNs) use recurrence to maintai n\n“memory” of the sequence. Hidden states of previous words co ntribute to the\nnext state. (B) In contrast, Transformers compute an attention matrix, wher e\nhigher (darker) probabilities indicate which words are int errelated.\nFrontiers in Computational Neuroscience | www.frontiersi n.org 2 July 2021 | Volume 15 | Article 675741\nSpoon et al. Analog AI for Transformer Networks\nFIGURE 3 | (A)Bidirectional Encoder Representations from Transformers (BERT) with 12 encoder layers. The input to BERT is a sequence of tokens, where each\ntoken is either a word or a word-piece. This sequence is proce ssed through each layer, followed by a pooler to reduce outpu t size and a fully-connected classiﬁer layer.\nFor example, to classify “I want a cat <eos>\" (where <eos> is the end-of-sentence token) as either grammatical (0) or n ot (1), the classiﬁer needs only two outputs.\nEach encoder layer (B) is comprised of two main building blocks: (1) the self-attention block, where the model computes an attention matrix between the in put and\nitself, and (2) a feed-forward networkwith two large fully-connected layers. Dark grey represent s trained weight layers in analog, while (C) shows the attention\nprocessing in digital. The input sequence to the self-atten tion block passes through a trained weight layer split into t hree parts to compute Q (query), K (key), andV\n(value) matrices. To compute attention (C), Q, K, and V are each split into multiple attention heads (for BERT, 12), both to reduce matrix sizes and to allow each to\nlearn slightly different representations of the sequence. [c(i)] A similarity matrix is computed between Q and K, followed by a softmax operation along rows to produce\nvalues between 0 and 1. [c(ii)] These probabilities are then multiplied by V and move to the next analog tile followed by the feed-forward network. [c(iii)] A higher\nprobability (darker shade) in one of the 12 probability (P) m atrices might indicate, for example, that the word “cat” is i mportant for prediction of the word “want”.\n2.1.1. Analog Tile Model\nWeights, in this study, are encoded using a diﬀerential\nconductance pair G+ and G− without any redundancy scheme.\nZero weights are encoded with G+ = G− = 0, therefore\nconsidering both devices at the RESET (lowest) conductance o f\nthe analog device. While, in practice, the minimum conductan ce\ncannot be zero, therefore the accuracy of the zero conductan ce\ncould be limited, the large (100x–1,000x) PCM device on-oﬀ r atio\nensures a fairly good approximation of a zero weight with very\nlow RESET conductance and RESET noise.\nMultiplication in the analog tile is performed by tuning\nthe input voltage pulse-width, to prevent distortions due to\nconductance non-linearities as a function of read voltage\n(\nChang et al., 2019 ). In order to accurately simulate the analog\ncomponents in the analog accelerator system, we include vari ous\nsources of non-ideality in the analog multiply-accumulate\n(MAC) operation, including quantization errors within the\ndigital peripheral circuitry and conductance noise within th e\nanalog NVM devices. In this section, we describe the PCM-\nbased device noise model and optimized design parameters\nwe used to achieve near software-equivalent accuracy infer ence\non BERT.\n2.1.2. Programming Noise, Conductance Drift and 1/f\nRead Noise\nThe inference accuracy attainable in an analog accelerator\nsystem depends strongly on the analog device conductance\nproperties, since these can be noisy and change over time. In\norder to estimate the accuracy characteristics of future an alog\naccelerators, we model these eﬀects by adding programming\nnoise, read noise, and conductance drift to the DNN weights\n(Figure 4A). We aggregate model error over many simulation\ninstances to arrive at the expected inference accuracy for a\ngiven time point. The noise model used here is based on the\nexperimental characterization from\nJoshi et al. (2020), with PCM\ndevices fabricated in a 90 nm technology. The associated open -\nsource simulator ( Rasch et al., 2021 ) includes the following PCM\nstatistical model for inference:\n• Programming noise represents the error incurred when\nencoding the weight in the PCM device. Instead of\nprogramming the correct target, the ﬁnal achieved\nconductance generally shows some error, which is modeled\nbased on the standard deviation of the iteratively programmed\nconductance values measured from hardware (\nJoshi et al.,\n2020):\ngprog = gT + N(0, σprog) ( µ S)\nσprog = γ max(1 : 1731g2\nT + 1.965gT + 0.2635, 0) ( µ S)\nwhere gprog and gT are the programmed and target\nconductances of a PCM device and N(0, σ ) is a normal\ndistribution with standard deviation σ . The parameter γ is\ngenerally equal to 1, except when we explore the performances\nof devices with reduced noise, where γ = 0.5.\n• PCM devices show a common trend for increasing time:\nafter programming, due to the relaxation of the amorphous\nFrontiers in Computational Neuroscience | www.frontiersi n.org 3 July 2021 | Volume 15 | Article 675741\nSpoon et al. Analog AI for Transformer Networks\nFIGURE 4 | (A)Conductance values (G) exhibit variability due to programm ing\nand read noise and decay toward zero over time due to drift noi se. (B) To\nmediate these noise sources, we train the ﬂoating-point mode l with noise in\norder to prepare the model for noisy inference. (C) During inference, the\nweights are programmed, then some time passes before infere nce is\nperformed.\nstate, conductance decays, following an empirical power-law\nfunction expressed as in\nIelmini et al. (2007):\ngdrift(t) = gprog\n( t\ntc\n) − ν\n(µ S)\nwhere gprog is the programmed conductance measured at time\ntc and gdrift(t) is the conductance at time t, while ν represents\nthe drift exponent, or slope on a log-G vs. log-t plot. In\nour simulations, ν is sampled from a normal distribution\nN(µ ν , σν ). Both µ ν and σν , dimensionless, depend on the\ntarget conductance gT and are modeled by ﬁtting experimental\ndata from\nJoshi et al. (2020), with the following expressions:\nµ ν = min(max(− 0.0155log(gT) + 0.0244, 0.049), 0.1)\nσν = min(max(− 0.0125log(gT) − 0.0059, 0.008), 0.045)\n• PCM non-idealities also include instabilities after the\nprogramming stage, such as read noise. Even in the absence\nof programming error or conductance drift, consecutive\nPCM reads lead to slightly diﬀerent conductance evaluations\n(\nAmbrogio et al., 2019 ). Among the multiple causes generating\nread noise, 1 /f noise and random telegraph noise show the\nstrongest contributions, with increased noise on lower-\nfrequency components. Such behavior leads to analog levels’\nintrinsic precision degradation for longer times. The overa ll\ncontribution can be modeled using a normal distribution with\ntime-dependent sigma (\nJoshi et al., 2020 ):\ng(t) = gdrift(t) + N(0, σnG(t)) ( µ S)\nThe standard deviation of the read noise σnG at time t is\nobtained by integrating the power spectral density over the\nmeasurement bandwidth:\nσnG(t) = γ gdrift(t)Qs\n√\nlog\n( t + tread\n2tread\n)\n(µ S)\nwhere tread = 250 ns is the duration of the read pulse. The\nparameter Qs, dimensionless, measured from the PCM devices\nas a function of gT is given by:\nQs = min\n(\n0.0088\ng0.65\nT\n, 0.2\n)\nThe noise model used in this work was calibrated using\na large number of PCM devices to characterize the statistics\nof (1) the weight programming error (due to deviations\nbetween programmed and desired conductance values), (2) the\naccumulated 1/f read noise of their PCM devices, and the\n(3) conductance drift and (4) drift variability as a functio n\nof the programmed conductance value. Details of the device\nmeasurement and modeling methodologies are described in the\nsupplementary information of reference (\nJoshi et al., 2020 ).\n2.1.3. Analog MAC Design and Additional\nNon-Idealities\nWhile weights are encoded using full precision, we include all\nnoise sources, therefore reﬂecting the true analog nature o f\ndevices, we assume that each analog tile receives digital in puts at\nfull precision, scales and quantizes to an integer representa tion,\nthen converts to analog duration using digital to analog\nconverters (DACs). The output of the analog tile is discretiz ed\nusing analog to digital converters (ADCs). Both DAC and ADC\ndiscretize the values in a ﬁxed range symmetrically around z ero.\nWe assume 8 bit precision for DAC and 10 bit for ADC. The\ninput scaling factor for the DAC is initialized using example dat a,\nlearned during training to optimally match the input ranges, a nd\nkept static during inference. Target weight ranges are clipped t o\n− 1.0, . . . , 1.0, where 1.0 corresponds to maximum target device\nconductance, gmax, although programming noise can induce\novershoot. The output ADC range is related to the ADC gain and\na parameter that depends on the ADC design. Here we set it to\n− 10, . . . , 10, which means that 10 “fully on” input lines (each at\n1.0) in conjunction with 10 weights at maximum (also 1.0) wou ld\nsaturate the ADC output. Even though the tiles have 512 rows,\nnot all weights are at their maximum. In typical DNN models,\nmost weights and activations have low values or are near zero .\nIn addition, the random-walk nature of aggregation along th e\nbitlines causes the signal to grow as the square-root of the n umber\nof rows, not linearly. The dynamic range of 10 for the ADC is a\ndesign parameter.\nEach digital output from the ADC is individually scaled and\noﬀset, to map the conductances back to the high-precision digi tal\nFrontiers in Computational Neuroscience | www.frontiersi n.org 4 July 2021 | Volume 15 | Article 675741\nSpoon et al. Analog AI for Transformer Networks\nFIGURE 5 | (A)Without any noise-aware techniques, inference on the Micro soft Research Paraphrase Corpus (MRPC) task decays very qui ckly over time. (B) Drift\ncompensation improves the decay over time signiﬁcantly, but the inference results are still lower than the BERT -base ide al model with no NVM noise. (C)\nHardware-aware (HWA) training with noise added during trai ning helps close the gap, reaching software-equivalent acc uracy for this task even at 1 month of drift.\ndomain (bﬂoat16 precision). These digital scaling factors a re also\nlearned during training and are critical to achieving softw are-\nequivalent accuracy during inference.\nThe analog MAC output is subject to short-term conductance-\ndependent noise that scales with the input current using the\nPCM read noise statistical model. We assume that the analog\nMAC output is subject to further additive Gaussian noise\ncorresponding to 0.5 LSB (least signiﬁcant bit) of the ADC, an d\nuse an approximated IR drop model. The analog tile size is set\nto 512 × 512 which, together with reduced read voltage (e.g., 0.2\nV) ensures negligible IR drop impact; if layers are larger, th ey\nare distributed across multiple tiles and outputs are summed\n(in digital). Activation functions are computed in ﬂoating po int\n32-bit (FP32) format using standard functions.\n2.2. Simulation Procedure–Training and\nInference\nTraining for inference (i.e., hardware-aware training, or HWA)\nis done in software to make the subsequent hardware inferenc e\nmore robust, even in the presence of PCM non-idealities\n(Figure 4B). We apply noise during hardware-aware training,\nspeciﬁcally during the forward propagation. While this helps\nthe subsequent inference even in the presence of drift, this\nnoise during training does not itself incorporate any explici t\ndrift models. The subsequent backward propagation and weight\nupdate components or various scaling factors (described in\nprevious sections) of software training are based on stochas tic\ngradient descent (SGD) and are both carried out at full precis ion\nwithout additional noise.\nThen, during inference, all hardware non-idealities—MAC\ncycle-to-cycle non-idealities, PCM programming noise, rea d\nnoise, 1/f noise, drift, and drift variability—are conside red, and\ndrift compensation is applied as described below.\nWe train 5 models with diﬀerent random seeds and select\nthe best one for inference evaluation. Accuracy can sometim es\nexceed state of the art results for smaller datasets where ru n-to-\nrun variation can be wider, while larger datasets show small er\naccuracy variation. We re-evaluate each model 25 times for\neach inference time point 1 to reduce sampling error during\ninference. We also report the standard error in the tables of\nresults ( Figures 6, 8). We evaluate accuracy at 5 time points\nafter weight programming ( Figure 4C): 1 second, 1 hour, 1 day,\n1 week, and 1 month. Without any correction techniques, the\ninference accuracy drops markedly over time ( Figure 5A).\n2.2.1. Drift Compensation\nAs described in\nAmbrogio et al. (2019) and Joshi et al. (2020)\nand illustrated in Figure 5B, signal loss by PCM conductance\ndrift can be eﬀectively compensated using a global correction-\nfactor calculated from the mean drift over time. To calculat e\nthe drift compensation factor in the simulator, we ﬁrst read\nout the weight matrix of each analog tile by performing the\nnon-ideal MAC operations of the forward pass using one-hot\ninput vectors, summing the values in an absolute manner to\nobtain an initial reference value. Then after applying conduct ance\ndrift and accumulated 1/f noise to the weights up to a certain\ninference time-point, the weights are again read out through the\nsame (non-ideal) MAC operations to produce a delayed reference\nvalue. Drift compensation is applied by adjusting the digital\noutput scale-factor (applied after ADC) by the ratio of the dela yed\nand initial reference values, and applied across the entire tes t set\nfor all simulations of the model at that inference time-point . Once\nthe average drift is compensated, the remaining noise eﬀects ac t\nas a random walk process, as programmed conductances evolve\naway from their intended states. RRAM, FERAM, or any other\ndevice will also exhibit time-dependent conductance change, and\nthese devices can also beneﬁt from the methodology proposed in\nthis work by substituting the corresponding device noise mode ls.\n2.2.2. Hardware-Aware (HWA) Training\nDrift compensation helps with the accuracy decrease over time\nby boosting the signal, but cannot remove the underlying noi se\nsources. In addition to training the static scale factors fo r DAC\ninput and ADC output, we apply a variety of techniques to\nprepare our trained model for noise during inference (\nGokmen\n1For one particular task, Quora Question Pairs (QQP), we use only 5 re peats due\nto large test dataset size.\nFrontiers in Computational Neuroscience | www.frontiersi n.org 5 July 2021 | Volume 15 | Article 675741\nSpoon et al. Analog AI for Transformer Networks\nFIGURE 6 |Inference results for all 8 GLUE tasks and the average score. D ataset training size shown in parentheses below each task na me, and tasks appear in\norder of their size, with smallest on the left. Since each tas k has a different standard accuracy range, shown is the /Delta1accuracy between the results from the BERT -base\nmodel and our noise-aware trained model for two conditions: (i) full noise model applied, and (ii) 50% programming and re ad noise and full drift noise applied (noise\nreduced). For the full noise model, we consider several diff erent time points, ranging from 1 month down to 1 day (with 1 ho ur and 1 second shown for context). The\nrequired time span would depend on the application. The tabl e reports mean values across trials and standard errors of th e mean.\net al., 2019; Joshi et al., 2020 ). A noise model that includes\ndigital periphery noise and additional noise on DNN weights th at\nmimics a scaled version of our programming noise is applied\nduring training, to prepare the network for inference with noi sy\nweights. The standard deviation scale of this additional we ight\nnoise is a hyper-parameter of the HWA training. The eﬀects can\nbe seen in Figure 5C, reaching software-equivalent accuracy for\na single language task only once these HWA training techniqu es\nare applied.\n2.3. Datasets and Training\nWe evaluate our HWA-trained BERT on the General Language\nUnderstanding Evaluation (GLUE) Benchmark (\nWang et al.,\n2019), consisting of 9 primary language tasks (see leaderboard\nat Wang et al., 2020 ). This benchmark is more robust than\nexamining a single task, as it shows the network’s ability to\ngeneralize. For example, one task tests the network’s abilit y to\nidentify a given sentence as grammatical or not. Another tas k\nassesses, given two sentences A and B, whether A is a paraphrase\nof B. We exclude one task, Winograd Natural Language Inferen ce\n(WNLI), just as BERT (\nDevlin et al., 2018 ) did, due to the unusual\nconstruction of the data set and small test set of only 146 sam ples.\nThis leaves 8 tasks:\n• Microsoft Research Paraphrase Corpus ( MRPC) (\nDolan and\nBrockett, 2005)\n• Recognizing Textual Entailment ( RTE) (Bar-Haim et al., 2006;\nDagan et al., 2006; Giampiccolo et al., 2007; Bentivogli et al .,\n2009)\n• Semantic Textual Similarity Benchmark ( STS-B) (Agirre et al.,\n2007)\n• The Corpus of Linguistic Acceptability ( CoLA) ( Warstadt\net al., 2018 )\n• The Stanford Sentiment Treebank ( SST-2) (Socher et al., 2013 )\n• Question Natural Language Inference ( QNLI) ( Rajpurkar\net al., 2016 )\n• Quora Question Pairs ( QQP)\n• Multi-Genre Natural Language Inference ( MNLI) ( Williams\net al., 2018 )\nWe evaluate each task separately by ﬁne-tuning a pretrained\nBERT-base model (\nWolf et al., 2020 ) using our HWA training\ntechniques. We do not train BERT models from scratch\nusing HWA training, but instead perform ﬁne-tuning from\nthe pretrained BERT model checkpoint with these techniques.\nFine-tuning is a technique used in natural language processin g,\nsimilar to transfer learning, where the main model is traine d\nwith a large amount of generic language data and later\nﬁne-tuned for a speciﬁc task (e.g., sentiment classiﬁcation)\nusing a much smaller set of data with limited epochs of\ntraining. This greatly reduces the runtime for the HWA\ntraining when compared to training from scratch. We use a\nmaximum sequence length of 128 for eﬃciency, since the vast\nmajority of data samples are much shorter that the maximum\nBERT sequence length of 512. We report the aggregated\nscore of all 8 tasks, since this is a common metric reported\nfor GLUE (\nWang et al., 2019 ).\nFrontiers in Computational Neuroscience | www.frontiersi n.org 6 July 2021 | Volume 15 | Article 675741\nSpoon et al. Analog AI for Transformer Networks\nFIGURE 7 | (A)While the energy-inefﬁciency in high-precision digital\ncomputation of the attention blocks may currently be a minor issue, the high\nenergy-efﬁciency of analog crossbar-arrays on fully-conne cted layers will\neventually expose this as a problem. (B) Particularly in Transformer-based\nmodels, quantizing the attention block to lower precision g reatly reduces the\narea and energy usage of the multipliers, optimizing the new bottleneck:\nactivation processing in attention. For example, decreasi ng from bﬂoat16\n(“BFL16FMA”) to INT6 (“IMA6”) results in an estimated energy r eduction\nof 91%.\nEach task needs to be ﬁne-tuned diﬀerently, so we scanned a\nvariety of learning parameters for each task: batch size, lea rning\nrate, weight clipping, and dropout. Here we report the accuracy\non the validation data set because the test set is only availa ble\nonline, which might result in a slight overestimation in the\naccuracy scores for the datasets with small validation set. We\nobserve accuracy variation that correlates with the size of the\ndatasets—models trained with smaller datasets exhibit lar ger\nvariation in test accuracy. Therefore, we train 5 models per ta sk\nper condition and choose the best model for inference simulat ion.\n3. RESULTS\n3.1. Results on BERT\nFigure 5C shows an example of an HWA-trained BERT-base\nmodel reaching software-equivalent accuracy and the infer ence\naccuracy evolution over time for the MRPC task. Accuracy resul ts\non all 8 GLUE tasks, reported at times ranging from 1 second to\n1 month after weight programming, are summarized in Figure 6.\nWe show that several tasks reach software-equivalent accur acy at\n1 month and the biggest accuracy drop is ∼ 4% for MNLI. The\naggregate score over all 8 tasks is only 1.29% below the basel ine\nat 1 month. Since there is hope for additional improvement with\nprogress in PCM device technology (\nGiannopoulos et al., 2018 ),\nwe show results for the full drift model but with only 50% of th e\nprogramming and read noise applied during inference, achieved\nby setting the γ factor in the σprog and σnG(t) equal to 0.5. In\nthis way, we reduce the impacts of both programming and read\nnoise contributions. Noise-reduced PCM devices can be expec ted\nto improve many of the tasks by >1% even for inference after\n1 month, and increase the aggregate GLUE score to just 0.6%\nbelow baseline.\n3.2. Attention Quantization\nAttention-based models such as BERT pose unique challenges\nbeyond previously studied models, because of the extensive\nactivation computation in the self-attention block. Amdahl ’s\nlaw implies that when a system bottleneck is greatly improved,\nperformance is invariably limited by something else, no matt er\nhow insigniﬁcant it was to begin with ( Figure 7A). Self-attention\ncomputations in a Transformer model scale quadratically wit h\nsequence length S, and constitute <1% of the number of\noperations for small S, but ∼ 5% at S = 128. If this computation\nis done in digital processing units at full precision, the cost\nin both energy and area for such processing units can become\nthe system bottleneck for Transformers, particularly as seq uence\nlength grows, despite constituting a relatively low fraction of\nthe workload.\nReduction of the precision in the digital computation of this\nself-attention block can also help reduce overall computatio n\ncosts, beyond consideration of the analog performance and\nprecision of just the fully-connected layers. The attention m atrix\nin this case is not mapped into analog crossbar arrays, but\nprocessed in digital multiply-and-add units.\n3.2.1. Attention Computation\nIn the self-attention block, there are two batch matrix-mul tiplies,\none for Q∗ K and one for softmax(Q∗ K)∗ V (Figure 3C(i,ii)). In\nthis paper, we propose to compute batch matrix-multiplication\nwith various integer precisions in order to reduce energy and\narea costs for these attention computation units, while keeping\nsoftmax operations at full precision. When compared to bﬂoat16\nmultiply-and-add (BFLFMA), integer multiply-and-add (IMA)\nunits are much more energy and area eﬃcient. Figure 7B,\nsimulated in a 14 nm FinFET technology, shows a 11.3 × energy\nbeneﬁt and a 4.7 × area beneﬁt from BFLFMA to INT6 (including\na wide-enough adder for multiply-accumulate operations acros s\nthe 64 terms in an attention-head). Next, we explore the impact\nof these attention quantization options on inference accura cy\nin BERT.\n3.3. Results on BERT With Quantized\nAttention\nFigure 8 summarizes GLUE task inference results with our\nanalog tile models for the fully-connected layers with four\ndiﬀerent precision settings—FP32, integer 10 bit (INT10), in teger\n8 bit (INT8), and integer 6 bit (INT6)—for the batch matrix-\nmultiplications in self-attention. The scaling factor used f or\nquantization is initialized from a small set of training dat a\nand then learned during the training process. BERT inference\nperformance is comparable among all four quantization scheme s.\nFor smaller datasets, INT10, INT8 and INT6 quantized attent ion\nmodels sometimes outperform the FP32 versions because of the\nadditional regularization and noise in the attention layers during\ntraining. For the four larger datasets (SST-2, QNLI, QQP , an d\nMNLI), no signiﬁcant diﬀerences in inference accuracy at 1\nmonth were observed down to INT6 quantized attention.\n4. DISCUSSION\nWhile we have clearly demonstrated the potential for iso-\naccuracy with Transformer-based neural networks on fast an d\nenergy-eﬃcient analog hardware, there are numerous areas fo r\nfuture work.\nFrontiers in Computational Neuroscience | www.frontiersi n.org 7 July 2021 | Volume 15 | Article 675741\nSpoon et al. Analog AI for Transformer Networks\nFIGURE 8 |Quantization inference results for all 8 GLUE tasks and the a verage score. Shown is a comparison to our FP32 noise-aware m odel from Figure 6at 1\nmonth of drift for various levels of precision: INT10, INT8, an d INT6, all of which perform at least as well as our full precisi on model for most tasks. On some tasks,\nincluding the aggregate score, the reduced precision seems to serve as additional regularization and performs better t han our FP32 model. The table reports mean\nvalues across trials and standard errors of the mean.\n4.1. Software-Equivalent accuracy\nWe have shown that full software-equivalent accuracy will\nrequire continued improvement in both PCM devices and in\nhardware-aware training techniques. However, we have been\nreasonably conservative in our accuracy report, presenting\nresults at 1 month of inference. We note that some workloads\nmay only require results at 1 day or 1 week of drift,\nfor example when models are weekly updated. We project\nthat current PCM devices can comfortably support software-\nequivalent accuracy on many GLUE tasks on such timescales.\nFor tasks where models are less frequently updated, another\napproach would be to incur slightly more frequent in-\nplace reprogramming of the same model – this would be a\ntradeoﬀ between model availability, the time needed for mod el\nprogramming, device endurance, temperature variation and\nother factors.\n4.2. Model Size\nWhile we have focused on BERT, which has 110 M parameters,\nnew Transformer-based networks are emerging that attempt to\nreduce model size while maintaining accuracy. DistilBERT (\nSanh\net al., 2019 ) uses knowledge distillation to reduce the number of\nparameters in half, and ALBERT ( Lan et al., 2020 ) uses cross-\nlayer parameter reuse, reducing the number of unique paramete rs\nto a fraction of the original. However, we note that these sma ller\nmodels may present a challenge to analog hardware, since fewe r\nunique weights can make models less robust to noise. Hardwar e-\nsoftware co-optimization that can strike a good balance betw een\nmodel size and robustness to PCM-based noise could lead to\nfuture Transformer-based networks that are highly optimize d for\naccuracy, energy-eﬃciency, and speed on Analog-AI hardware .\n5. CONCLUSION\nWe show that despite their various noise sources, PCM-based\nanalog accelerators are a sensible choice for deep learning\nworkloads, even for large natural language processing models\nlike BERT. Our simulation results using a comprehensive noise\nmodel demonstrate that BERT can be expected to be close to\nsoftware-equivalent accuracy even with existing PCM device s.\nOther Transformer-based models with the same building bloc ks\ncan be similarly evaluated with our approach. We have shown\nthat expected improvements in programming noise variability\nprovide a consistent trend toward software-equivalent accu racy.\nFinally, in preparation for high energy eﬃciency on the fully-\nconnected layers, we provide a potential solution to the next\nbiggest energy cost: the activation processing from the atte ntion\nblock. We show that 11.3 × energy improvements should be\nfeasible by quantization to INT6, with no signiﬁcant loss\nin accuracy.\nDATA AVAILABILITY STATEMENT\nThe original contributions presented in the study are includ ed\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author/s.\nAUTHOR CONTRIBUTIONS\nKS, HT, MS, and GB conceived the original ideas. KS, HT,\nAC, and MS implemented and ran the simulations. All authors\ncontributed during data analysis. KS, HT, AC, MR, SA, and GB\ndrafted the manuscript.\nFrontiers in Computational Neuroscience | www.frontiersi n.org 8 July 2021 | Volume 15 | Article 675741\nSpoon et al. Analog AI for Transformer Networks\nREFERENCES\nAgirre, E., Marquez, L., and Wicentowski, R., (2007). Proceedings Fourth\nInternational Workshop on Semantic Evaluations (SemEval) . (Prague).\nAmbrogio, S., Gallot, M., Spoon, K., Tsai, H., Mackin, C., Wesson, M ., et al. (2019).\n“Reducing the impact of phase-change memory conductance drift on th e\ninference of large-scale hardware neural networks, ” in 2019 IEEE International\nElectron Devices Meeting (IEDM) , 6.1.1–6.1.4.\nAmbrogio, S., Narayanan, P., Tsai, H., Shelby, R. M., Boybat, I., d i Nolfo,\nC., et al. (2018). Equivalent-accuracy accelerated neural-network training\nusing analogue memory. Nature 558:60. doi: 10.1038/s41586-018-\n0180-5\nBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magn ini, B.,\net al. (2006). “The second PASCAL recognising textual entailmen t challenge, ”\nin Proceedings Second PASCAL Challenges Workshop on Recognis ing Textual\nEntailment (Venice).\nBentivogli, L., Dagan, I., Dang, H. T., Giampiccolo, D., and Magni ni, B. (2009).\n“The ﬁfth PASCAL recognizing textual entailment challenge, ” in Proceedings\nText Analysis Conference (TAC) (Gaithersburg, MD).\nBurr, G. W., Brightsky, M. J., Sebastian, A., Cheng, H.-Y., Wu, J .-Y., Kim, S., et al.\n(2016). Recent progress in PCM technology. IEEE J. Emerg. Sel. Top. Circ. Sys.\n6, 146–162. doi: 10.1109/JETCAS.2016.2547718\nBurr, G. W., Shelby, R. M., Sebastian, A., Kim, S., Kim, S., Sidler, S., et al. (2017).\nNeuromorphic computing using non-volatile memory. Adv. Phys. X 2, 89–124.\ndoi: 10.1080/23746149.2016.1259585\nBurr, G. W., Shelby, R. M., Sidler, S., di Nolfo, C., Jang, J., Boyb at, I.,\net al. (2015). Experimental demonstration and tolerancing of a large–\nscale neural network (165,000 synapses), using phase–change memory as\nthe synaptic weight element. IEEE Trans. Electron Dev. 62, 3498–3507.\ndoi: 10.1109/TED.2015.2439635\nChang, H. ., Narayanan, P., Lewis, S. C., Farinha, N. C. P., Hosoka wa, K.,\nMackin, C., et al. (2019). Ai hardware acceleration with analog memory :\nmicroarchitectures for low energy at high speed. IBM J. Res. Dev. 63, 8:1–8:14.\ndoi: 10.1147/JRD.2019.2934050\nDagan, I., Glickman, O., and Magnini, B. (2006). “The PASCAL rec ognising textual\nentailment challenge, ” in ML Challenges: Evaluating Predictive Uncertainty,\nvisual Object Classiﬁcation, and Recognising Textual Enta ilment, (Milan), 177–\n190.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). B ERT: pre-training\nof deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805.\nDolan, W. B., and Brockett, C. (2005). “Automatically constructin g a corpus\nof sentential paraphrases, ” in Proceedings International Workshop on\nParaphrasing, (Jeju Island).\nGiampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. (2007). “T he third\nPASCAL recognizing textual entailment challenge, ” in Proceedings ACL-\nPASCAL Workshop on Textual Entailment and Paraphrasing , Prague, 1–9.\nGiannopoulos, I., Sebastian, A., Le Gallo, M., Jonnalagadda, V., Sousa, M., Boon,\nM., et al. (2018). “8-bit precision in-memory multiplication with projec ted\nphase-change memory, ” in 2018 IEEE International Electron Devices Meeting\n(IEDM), 27.7.1–27.7.4.\nGokmen, T., Rasch, M. J., and Haensch, W. (2019). “The marriage of training and\ninference for scaled deep learning analog hardware, ” in 2019 IEEE International\nElectron Devices Meeting (IEDM) , 22–3.\nIelmini, D., Lacaita, A. L., and Mantegazza, D. (2007). Recove ry and drift dynamics\nof resistance and threshold voltages in phase-change memories. IEEE Trans.\nElectron Dev. 54, 308–315. doi: 10.1109/TED.2006.888752\nJoshi, V., Le Gallo, M., Haefeli, S., Boybat, I., Nandakumar, S. R ., Piveteau, C.,\net al. (2020). Accurate deep neural network inference using computa tional\nphase-change memory. Nat. Comm. 11:2473. doi: 10.1038/s41467-020-1\n6108-9\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R . (2020).\nALBERT: a lite BERT for self-supervised learning of language represen tations.\narXiv preprint arXiv:1909.11942 .\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature 521, 436–444.\ndoi: 10.1038/nature14539\nMatsukura, F., Tokura, Y., and Ohno, H. (2015). Control of magneti sm by electric\nﬁelds. Nat. Nanotechnol. 10, 209–220. doi: 10.1038/nnano.2015.22\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan , G., et al. (2019).\nPytorch: an imperative style, high-performance deep learning library. NIPS 32,\n8026–8037.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). “SQu AD: 100,000+\nquestions for machine comprehension of text, ” in Proceedings of the 2016\nConference on Empirical Methods in Natural Language Process ing, Austin, TX,\n2383–2392.\nRasch, M. J., Moreda, D., Gokmen, T., Gallo, M. L., Carta, F., Goldbe rg, C., et al.\n(2021). A ﬂexible and fast pytorch toolkit for simulating training and inference\non analog crossbar arrays. arXiv.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). “DistilB ERT, a distilled\nversion of bert: smaller, faster, cheaper and lighter, ” inNeurIPS EMC2 Workshop\n(Vancouver, BC).\nSocher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D. , Ng, A. Y., et al.\n(2013). “Recursive deep models for semantic compositionality ove r a sentiment\ntreebank, ” in Proceedings of the 2013 Conference on Empirical Methods in\nNatural Language Processing, Seattle, WA, 1631–1642.\nSze, V., Chen, Y.-H., Yang, T.-J., and Emer, J. S. (2017). Eﬃcie nt processing\nof deep neural networks: a tutorial and survey. Proc. IEEE 105, 2295–2329.\ndoi: 10.1109/JPROC.2017.2761740\nTsai, H., Ambrogio, S., Mackin, C., Narayanan, P., Shelby, R. M., R ocki, K., et al.\n(2019). “Inference of long-short term memory networks at software-eq uivalent\naccuracy using 2.5M analog phase change memory devices, ” in 2019 Symposium\non VLSI Technology , T82–T83.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in Neurips (Long Beach, CA).\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2019).\n“GLUE:a multi-task benchmark and analysis platform for natural languag e\nunderstanding, ” inProceedings of ICLR (New Orleans, LA).\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2020). GLUE\nBenchmark. Available online at: gluebenchmark.com/leaderboard\nWarstadt, A., Singh, A., and Bowman, S. R. (2018). Neural netwo rk acceptability\njudgments. arXiv preprint 1805.12471 .\nWilliams, A., Nangia, N., and Bowman, S. (2018). “A broad-coverage challenge\ncorpus for sentence understanding through inference, ” in Proceedings of\nthe 2018 Conference of the North American Chapter of the Associat ion for\nComputational Linguistics: Human Language Technologies, V olume 1 (Long\nPapers), New Orleans, LO.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A ., et al. (2020).\n“Transformers: State-of-the-art natural language processing, ” in Proceedings\nConference Empirical Methods in NLP: System Demonstrations , 38–45.\nWong, H.-S. P., Lee, H.-Y., Yu, S., Chen, Y.-S., Wu, Y., Chen, P. -\nS., et al. (2012). Metal-Oxide RRAM. Proc. IEEE 100, 1951–1970.\ndoi: 10.1109/JPROC.2012.2190369\nConﬂict of Interest: The authors were employed by IBM Research.\nCopyright © 2021 Spoon, Tsai, Chen, Rasch, Ambrogio, Mackin, Fas oli, Friz,\nNarayanan, Stanisavljevic and Burr. This is an open-access a rticle distributed\nunder the terms of the Creative Commons Attribution License (CC BY). The use,\ndistribution or reproduction in other forums is permitted, p rovided the original\nauthor(s) and the copyright owner(s) are credited and that th e original publication\nin this journal is cited, in accordance with accepted academ ic practice. No use,\ndistribution or reproduction is permitted which does not co mply with these terms.\nFrontiers in Computational Neuroscience | www.frontiersi n.org 9 July 2021 | Volume 15 | Article 675741"
}