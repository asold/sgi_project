{
  "title": "Spatial-Channel Transformer Network for Trajectory Prediction on the Traffic Scenes",
  "url": "https://openalex.org/W3124638336",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2349263532",
      "name": "Zhao Jing-wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2960494400",
      "name": "Li Xuanpeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743190254",
      "name": "Xue, Qifan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A163298095",
      "name": "Zhang Wei-gong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2985871763",
    "https://openalex.org/W2991653934",
    "https://openalex.org/W2798930779",
    "https://openalex.org/W2962687116",
    "https://openalex.org/W3035096461",
    "https://openalex.org/W2963001155",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2804827665",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2998415999",
    "https://openalex.org/W2963906196",
    "https://openalex.org/W2766836212",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2424778531",
    "https://openalex.org/W2963353290",
    "https://openalex.org/W2946949757",
    "https://openalex.org/W2010040278"
  ],
  "abstract": "Predicting motion of surrounding agents is critical to real-world applications of tactical path planning for autonomous driving. Due to the complex temporal dependencies and social interactions of agents, on-line trajectory prediction is a challenging task. With the development of attention mechanism in recent years, transformer model has been applied in natural language sequence processing first and then image processing. In this paper, we present a Spatial-Channel Transformer Network for trajectory prediction with attention functions. Instead of RNN models, we employ transformer model to capture the spatial-temporal features of agents. A channel-wise module is inserted to measure the social interaction between agents. We find that the Spatial-Channel Transformer Network achieves promising results on real-world trajectory prediction datasets on the traffic scenes.",
  "full_text": "  \nï€  \nAbstractâ€”Predicting motion of surrounding agents is critical \nto real -world application s of tactical path planning for \nautonomous driving. Due to the complex temporal dependencies \nand social interactions of agents, on-line trajectory prediction is \na challenging  task. With the development of attention \nmechanism in recent years, transformer model has been applied \nin natural language sequence processing  first and then image \nprocessing. In this paper, we present  a Spatial-Channel \nTransformer Network for trajectory prediction with attention \nfunctions. Instead of RNN models, we employ transformer \nmodel to capture the spati al-temporal feature s of agent s. A \nchannel-wise module is inserted to measure the social interaction \nbetween agents. We find that the Spatial-Channel Transformer \nNetwork achieves promising results  on real -world trajectory \nprediction datasets on the traffic scenes. \nI. INTRODUCTION \nAutonomous vehicles need to make safe and efficient \ndecisions to pass through the complex traffic scene s, such as \nchanging lanes, overtaking or decelerating. It can be seen in \nthe existing tactical path planning algorithms [1] that these \nalgorithms require reliable estimate of the future trajectories of \nsurrounding agents, like vehicles and pedestrians . Therefore, \nautonomous vehicles need to have the ability to infer the future \nmovements of surrounding agents. \nTrajectory prediction of multiple agents utilizes the history \ntrajectories and behavior s to forecast their future motion, \nwhich is closely related to the current traffic condition. Due to \nthe uncertainty of behavior, trajectories tend to be highly non-\nlinear during a  long time. Additionally, the potential \ninteractions between surrounding agents affect action \ndecisions. Therefore, it is challenging to model and predict the \ntrajectory of multiple agents on the traffic scenes. \nWith the development of deep neural networks , Long \nShort-Term Memory (LSTM) network has become the \nprimary method for trajectory prediction. LSTM sequentially \nprocesses time series data to characterize the speed, direction \nand motion mode of agents. Besides, social pooling \nmechanisms are designed to simulate the social interaction of \nagents[2, 3] . However social pooling treats agents equally \nthrough pooling operations, which weakens the impact of their \ninteractions. Attention mechanisms [4-6] are appended to \nweigh surrounding agents unequally based on a learning \nprocess. Nonetheless the existing trajectory prediction \napproaches still have two common limitations: \nï‚· Since time dependence is complex to extract [7], \nLSTM has  been criticized for the memory \n \n*Research supported by National Natural Science Foundation of China \nunder Grant 61906038.  \nJingwen Zhao, Xuanpeng Li, Qifan Xue and Weigong Zhang are with the \nSchool of Instrument Science and Engineering, Southeast University, \nmechanism[8] and the ability to model social \ninteraction[9]. \nï‚· The utilization of attention mechanism is too simple \nto properly simulate the interaction between vehicles;  \nRecently, the Transformer model made breakthrough \nprogress in sequence learn ing and sequence generation tasks \nin natural language processing domains[7, 10], which inspires \nus to replace LSTM model with Transformer to extract  time \ndependencies better. In this paper, we propose the Spatial-\nChannel Transformer framework, a novel model with \ncompletely attention mechanism to deal with trajectory \nprediction problems on the traffic scenes. With powerful self-\nattention mechanism and multi-head attention mechanism, the \nEncoder-decoder Transformer Network models agent motions \nby the spatial position and temporal correlation. Additionally, \nwe employ Squeeze-and-Excitation Blocks[11] to capture the \ninteraction between agents with channel-wise attention. The \nunified experiments of multiple classic approaches are \nconducted on the NGSIM dataset with several metrics. We \nalso carry out ablation studies to highlight the effect of \ncomponent modules. \nII. RELATED WORK  \nRNNs and their variants LSTM [12] and GRU [13] have \nachieved great success in sequence prediction tasks such as \nrobot localization[14], and robot decision making[15]. RNNs \nhave also been successfully applied to simulate the motion \npatterns of pedestrians or vehicles with a Seq2Seq \nstructure[16]. In addition, to improve trajectory prediction, \nsocial pooling mechanism [2, 3], attention mechanism [4, 17] \nand graph neural network [6, 18]  are used to simulate \ninteractive behaviors. \n Social LSTM[2]  is one of the first deep learning models \nto study pedestrian trajectory prediction. Social LSTM uses a \npooling mechanism to aggregate the hidden feature output and \npredict the following trajectory. CS-LSTM aims at researching \non vehicle trajectory prediction , which improves social \npooling mechanism with convolution operation [19]. Later \nmodels State -Refinement LSTM (S R-LSTM)[5] extended \nSocial LSTM through visual features and new pooling \nmechanism to improve prediction accuracy. Especially SR -\nNanjing, 210096, China (corresponding author: Xuanpeng Li, phone: \n+8613770666889, e-mail: li_xuanpeng@seu.edu.cn).  \n \nSpatial-Channel Transformer Network for Trajectory Prediction on \nthe Traffic Scenes  \nJingwen Zhao, Xuanpeng Li*, Member, IEEE, Qifan Xue, and Weigong Zhang, Member, IEEE \n  \nLSTM weights the contribution of each pedestrian to others \nthrough a weighting mechanism.  \nConsidering that the structure of the graph network is \nsuitable for modeling interaction, several approaches \nsuccessfully used Graph Neural Networks (GNN) to \ncharacterize the influence among agents, including STGAT \nmodel[6] with GAT module and Social -STGCNN[18] with \nGCN module.  \nAdditionally, generative model is also well applied in \ntrajectory prediction. Based on the assumption of the multi -\nmodal distribution of pedestrian trajectories, Social GAN[3] \nextended Social LSTM to a generative model. Sophie[20] uses \nCNNs to extract features from the entire scene with a two-way \nattention mechanism for each pedestrian. Sophie concatenates \nthe attention output with the visual CNN output, and then uses \na generative model based on LSTMs auto-encoder to generate \nfuture trajectories. \n In recent years, the transformer network has dominated \nthe field of Natural Language Processing[7, 10]. It completely \nabandons the recurrence and focuses on the attention over time \nstep. This architecture has the ability of parallel training and \ncapturing long-term dependency, which can still predict from \ninputs with missing observation data. Recently the transformer \narchitecture has also been successfully applied in pedestrian \ntrajectory prediction. This paper provides a transformer \napplication of trajectory prediction on the traffic scene. \nWithout using the graph structure, we use complete attention \nmechanism to process time -series data and model social \ninteraction, which provides proof of  the validity of the \nattention mechanism on sequence prediction. \nFurthermore, Squeeze and Excitation (SE) Networks[11] \nbring significant improvements in performance of existing \nCNN models for image processing. We are inspired by the \nSqueeze and Excitation module and apply it to the trajectory \nembedding with channel-wise attention to capture interaction \nbetween neighbors. \nIII. MODEL \nIn this section, we introduce the trajectory prediction \nmodel based on spatial -channel attention. The En coder-\ndecoder Transformer Network is used to extract temporal \nsequence features and utilize the past spatial positions of \nmultiple agents to predict future trajectories. The SE module \nis added to capture the impact of agentsâ€™ interaction on the past \nand future trajectory data. \nA.  Problem Formulation \nWe assume that there are ğ‘ agents involved on a traffic \nscene. The input of our model consists of the trajectories of \nagents, represented as  Xğ‘¡ = [X1\nğ‘¡ , X2\nğ‘¡ , X3\nğ‘¡ , â€¦ , Xğ‘\nğ‘¡ ] , where Xğ‘–\nğ‘¡ =\n(ğ‘¥ğ‘–\nğ‘¡, ğ‘¦ğ‘–\nğ‘¡ ) is defined by its x -y coordinates. We observe the \npositions of all the agents from time 1 to ğ‘‡ğ‘œğ‘ğ‘ , and predict their \npositions for time  ğ‘‡ğ‘œğ‘ğ‘ +1 to ğ‘‡ğ‘ğ‘Ÿğ‘’ğ‘‘ . The future trajectories of \nground truth can be represented as  Yğ‘¡ = [Y1\nğ‘¡, Y2\nğ‘¡, Y3\nğ‘¡, â€¦ , Yğ‘\nğ‘¡ ], and \nthe output sequence of our model can be denoted as  YÌ‚ğ‘¡ =\n[YÌ‚1\nğ‘¡, YÌ‚2\nğ‘¡, YÌ‚3\nğ‘¡, â€¦ , YÌ‚ğ‘\nğ‘¡ ]. \nB.  Encoder-Decoder Transformer Network \nThe Encoder -Decoder structure is extensively used in \nsequence transduction models[16]. In this work, the encoder \nnetwork maps input sequence Xğ‘¡ = [X1\nğ‘¡ , X2\nğ‘¡ , X3\nğ‘¡ , â€¦ , Xğ‘\nğ‘¡ ]  to \nlatent state  Zğ‘¡ = [Z1\nğ‘¡ , Z2\nğ‘¡ , Z3\nğ‘¡ , â€¦ , Zğ‘\nğ‘¡ ] . With the given  Zğ‘¡ , the \ndecoder network generates one element of the output sequence \nYÌ‚ğ‘¡ = [YÌ‚1\nğ‘¡, YÌ‚2\nğ‘¡, YÌ‚3\nğ‘¡, â€¦ , YÌ‚ğ‘\nğ‘¡ ]  on every time step. In an auto -\nregressive way, the previously generated element is used as an \nadditional input to generate the next element. \nAs illustrated in Fig.1, the Encoder -Decoder Transformer \nNetwork is composed of several blocks, including Attention \nmodule, Feed -Forward Networks module, and Residual \nDropout module.  \nThe Transformer network m ainly captures the \ndependencies of time -series data and the non -linear \ncharacteristics of spatial data through the Attention module, \nwhich comprises of self-attention mechanism and multi-head \nattention mechanism. \nFigure 1. Structure of the Spatial-Channel Transformer Network. \n \n\n  \nSelf-attention takes query matrix ğ‘„, key m atrix ğ¾, value \nmatrix ğ‘‰, dimension ğ‘‘ğ‘˜ of queries and keys as input, which is \ncalculated by a dot product of ğ‘„ and ğ¾ , and scaled factor of  \n1/âˆšğ‘‘ğ‘˜ . Then a softmax function is applied to obtain the \nweights of values. The scaled operation is better for large value \nof ğ‘‘ğ‘˜ to avoid the softmax function falling into regions where \nit has extremely small gradients. \nï€ \nAttention( , , ) softmax( )\nT\nk\nQKQ K V V\nd\nï€½ ï€  ï€¨ï€±ï€©ï€ \nInstead of learning a single attention function, it has found \nthat it is beneficial to map the queries, keys and values for h \ntimes to learn different contextual information respectively[7]. \nThe self -attention function is performed on each projected \nversion of queries, keys and values in parallel. Then the results \nare concatenated and projected again to obtain the weight of \nfinal values. Therefore, benefitting from the multi -head \nattention operation, the transformer structure can jointly \ngenerate comprehensive latent feature of trajectory data from \ndifferent representation subspaces. \nï€ \n1hMultiHead( , , )=Concat(head , ,head ) OQ K V W\n ï€  ï€¨ï€²ï€©ï€ \nWhere  headğ‘– = Attention(QWğ‘–\nQ, KWğ‘–\nK, VWğ‘–\nV). Wğ‘–\nQ,  \nWğ‘–\nK, Wğ‘–\nV, Wğ‘–\nOare the projections of parameter matrices in \nqueries, keys, values and output. \nBesides, the fully connected Feed -Forward Networks \nconsists of two linear transformations with a ReLU activation, \nwhich is applied to each attention sub-layers. And the addition \nof the Residual Dropout module  is set to improve the \nefficiency of the network. \nï€ \n1 1 2 2Feedforward( ) max(0, )x xW b W bï€½ ï€« ï€« ï€  ï€¨ï€³ï€©ï€ \nC.  Squeeze and Excitation Module \nIn order to deal with social interaction between vehicles, \nwe embed ğ‘ trajectories of vehicles in time into a vector ğ¸ğ‘œğ‘ğ‘  \nwith dimension of  ğ‘ Ã— ğ‘‡ğ‘œğ‘ğ‘  Ã— ğ‘‘ , and use ğ‘ as the number of \nfeature channels of the input tensor, as illustrated in Fig.1. \nWith the SE module for channel -wise attention, the \ninteractions between vehicles are extracted in weights. The SE \nmodule consists of squeeze o peration, excitation operation, \nand scale operation. \nThe squeeze operation manipulates spatial dimensions to \nperform feature compression, turning each two -dimension \nfeature channel into a real number. This real number has a \nglobal receptive field of feature representation of each vehicle \ntrajectory, and the output dimension matches the input feature \nchannel number ğ‘. It characterizes the global distribution of \nthe feature channel, which is consistent with the interaction \nbetween adjacent vehicles. \nThe excitation operation is similar to the gate mechanism \nin the recurrent neural network. The weight is generated for \neach feature channel through the parameter  ğ‘¤ , where the \nparameter ğ‘¤  is learned to explicitly model the correlation \nbetween the feature channels.  \nThe scale operation treats the results of the excitation as \nthe importance of each feature channel, which reweighs the \noriginal input feature in the channel dimension through \nmultiplication. \nï€ \n11\n21\n,\n1( ) ( , )\n( , ) ( ( , )) ( ( ))\n()\nobsTd\nc sq obs obs\nobs ij\nc ex c c c\nobs scale obs c c obs\nz F E E i j Td\ns F z w g z w w w z\nE F E s s E\nï³ ï³ ï¤\nï€½ï€½\nï€½ï€½ ï‚´\nï€½ ï€½ ï€½\nï€½ï€½\nïƒ¥ïƒ¥\n ï€  ï€¨ï€´ï€©ï€ \nIn this work, global average pooling is used as for the \nsqueeze operation. Then two fully connected layers in a \nbottleneck structure estimate the correlation between channels \nand output the same number of weight s as the input feature \nchannels, where Î´ refers to the ReLU function. Through a \nsigmoid gate Ïƒ, the normalized weights ğ‘ ğ‘  between 0 and 1 are \nobtained, which will be multiplied by the original feature to \nrepresent the social interaction between vehicles through the \nchannel-wise attention. \nD.  Implementation Details \nDifferent from the LSTM model, the transformer network \ndiscards the sequential nature of time -series data and models \ntemporal dependencies with the self -attention mechanism. \nTherefore, the input embedding data  ğ¸ğ‘œğ‘ğ‘ \n(ğ‘–,ğ‘¡) consists of spatial \ntrajectory embedding and temporal position embedding. For \nthe trajectory embedding ğ‘’ğ‘œğ‘ğ‘ \n(ğ‘–,ğ‘¡), we expand the dimension of \ntrajectory sequences Xğ‘¡ = [(ğ‘¥(ğ‘–,ğ‘¡), ğ‘¦(ğ‘–,ğ‘¡))] from 2 to ğ· = 512 \nwith MLP network. And the position embedding ğ‘ƒğ‘¡ is defined \nby sine and cosine functions. \nï€ \n( , ) ( , )\n( , ) 1\n/\n( , ) /\n{}\nsin( /10000 )\ncos( /10000 )\ni t i t t\nobs obs\ntD td d\ndD\ntd dD\nE e P\nPp\nt for d evenp\nt for d odd\nï€½\nï€½ï€«\nï€½\nïƒ¬ïƒ¯ï€½ïƒ­ïƒ¯ïƒ® ï€  ï€¨ï€µï€©ï€ \nIn this work, the Encoder -Decoder Transformer Network \nis composed of 8 attention heads. The loss function is defined \nby L2-loss between the predicted output trajectory YÌ‚ğ‘¡ and the \nground truth trajectory Yğ‘¡.Via backpropagation with the Adam \noptimizer, we trained the network with learning rate 0.01 and \ndropout value of 0.1. The training and inference  is \nimplemented using PyTorch. \nIV. EXPERIMENTS \nIn this section, we evaluate our method on the publicly \navailable NGSIM US -101 and I -80 datasets. Each dataset \nconsists of 15 -minute segments of mild, moderate and \ncongested traffic conditions. In the same way of the previous \nwork[19], we split the complete dataset into train, validation \nand test sets. With the local coordinate data provided by each \n  \ndataset, we split the trajectories into segments of 8 seconds, \nwhere we use 3-second history observation and next 5 seconds \nfor prediction. The original sampling frequency of the 8 -\nsecond segments is 10 Hz. We subsampled it by a factor of 2 \nas previous work[19].  \nA. Evaluation Metrics and Baselines \nWe use the following performance metrics to measure the \nevaluation of the algorithms used for predicting the trajectories \nof vehicles. \nï‚· ADE: Average Displacement Error averages \nEuclidean distances between poi nts of the predicted \ntrajectory and the ground truth that have the same \ntemporal distance from their respective start points. \nï€ \n2\nË†|| Y Y ||\nADE\npred\nnn\ntt\nn N t T\npredNT\nïƒïƒ\nï€­\nï€½ ï‚´\nïƒ¥ïƒ¥ ï€  ï€¨ï€¶ï€©ï€ \nï‚· FDE: Final Displacement Error measures the distance \nbetween final predicted position and the ground truth \nposition at the corresponding time point. \nï€ \n2\nË†|| Y Y ||\nFDE ,\nnn\ntt\nnN\npredtTN\nïƒ\nï€­\nï€½ï€½\nïƒ¥ ï€  ï€ ï€¨ï€·ï€©ï€ \nï‚· RMSE: Root Mean Squared Error evaluates the \ntrajectories in terms of the root of the mean squared \nerror between the prediction results and ground truth. \nï€ \n2Ë†(Y Y )\nRMSE\npred\nnn\ntt\nn N t T\npredNT\nïƒïƒ\nï€­\nï€½ ï‚´\nïƒ¥ïƒ¥ ï€  ï€¨ï€¸ï€©ï€ \nWe compare our approach with the following methods. \nï‚· Social LSTM [2]: An LSTM -based network with \nsocial pooling of hidden states to predict pedestrian \ntrajectories in crowds. \nï‚· CS-LSTM[19]: Adding convolutions to the network \nin Social LSTM in order to predict trajectories on the \nhighway. \nï‚· Social GAN[3]: An LSTM-GAN hybrid network with \ngenerative model to predict trajectories for large \nhuman crowds. \nï‚· Social STGCNN [18]:  A Social Spatial -Temporal \nGraph Convolutional Neural Network, which \nsubstitutes the need of aggregation methods by \nmodeling the interactions as a graph. \nï‚· STGAT[6]: A Spatial -Temporal Graph Attention \nnetwork with graph attention mechanism at each time-\nstep to capture the interactions between pedestrians. \nB. Quantitative Results and Analyses \nThe main results are presented in Table 1. We observe that \nSpatial-Channel Transformer Network can achieve similar \nperformance results closed to SOTA models. Compared with \nSocial LSTM and CS -LSTM based on LSTM model, our \nmodel has an improvement on the precision of trajectories  \nTable 1. The ADE, FDE, RMSE performance of Spatial-Channel Transformer Network are presented (in meter) over a 5-\nsecond prediction horizon compared with baseline models on NGSIM datasets. \n Social LSTM CS-LSTM Social STGCNN Social GAN Social STGAT Ours \n1s ADE/FDE 0.29/0.53 0.25/0.48 0.50/0.80 0.16/0.33 0.15/0.33 0.20/0.39 \nRMSE 0.61 0.57 0.72 0.30 0.31 0.38 \n2s ADE/FDE 0.67/1.46 0.62/1.38 0.92/1.71 0.43/0.99 0.43/1.00 0.50/1.10 \nRMSE 1.35 1.28 1.36 0.78 0.80 0.88 \n3s ADE/FDE 1.19/2.78 1.12/2.65 1.40/2.82 0.79/1.90 0.79/1.88 0.88/2.06 \nRMSE 2.23 2.14 2.10 1.37 1.39 1.51 \n4s ADE/FDE 1.84/4.51 1.75/4.33 1.95/4.16 1.23/3.02 1.22/2.94 1.35/3.26 \nRMSE 3.36 3.23 2.96 2.09 2.10 2.27 \n5s ADE/FDE 2.62/6.62 2.51/6.43 2.57/5.71 1.93/4.64 1.89/4.40 1.90/4.66 \nRMSE 4.63 4.59 3.93 3.17 3.10 3.16 \nFigure 2. Trajectory visualization. We list the trajectory prediction results of Social GAN, STGAT and Spatial-channel \nmodel in the same scene for comparison. The red line is represented as the historical observation for input. The ground \ntruth trajectories are shown as points in green, and the blue points make up the results of prediction. \n \n\n  \nprediction. Furthermore, our model possesses the similar \nperformance with Social GAN. Because the transformer \nmodel abandons the recurrent features of LSTM, the \ncumulative error at the prediction end point is less than \nprevious points. STGAT performs a little better than ours, \nwhich proves that Graph Attention Network is promising for \nmodeling interacti on between agents. We then further \ndemonstrate this in the following s ection with visualized \nresults. \nC. Qualitative Results and Analyses \nAs shown in Fig. 2, our qualitative results consist of the \ntrajectory visualization in Social GAN model, STGAT model \nand our spatial-channel model. Our model has the capacity of \ntemporally consistent trajector y prediction. With the given \nobserved trajector y, our model is able to provide proper \ntrajectory results. In addition, our model successfully extracts \nthe social interaction of multiple vehicles to avoid collision. \nCompared with Social GAN and STGAT, we can see the \nSpatial-Channel Transformer has a better performance facing \na sharp turn situation. It proves that the transformer model has \nthe ability of extracting the spati al-temporal feature of long -\nterm sequence through attention mechanism in the face of \nvariable driving behaviors.  \nIt can be seen from the visualization results that prediction \nof velocity and change of driving behavior have become \nimportant indicators to improve the existing results. Auxiliary \ninformation is required for more accurate trajectory prediction, \nwhich is not only affected by interaction of neighbors. For the \nfuture work, additional information about map and rules of the \nroad, should be added to provide extra information for \nprediction. \nD. Ablation Studies \nWe carry out ablation studies to confirm the effectiveness \nof channel-wise attention and the selection impact of social \nneighborhood range. \nAs shown in Table 2, we arrange the amount of \nneighborhood agents at 5, 10 and 15. With adding channel -\nwise attention module, we record the performance of different \nneighborhood sizes. Generally, channel-wise attention has the \ncapacity of modeling interaction of agents with s light lift of \nevery result of single transformer model. One interesting \nfinding is that the expansion of neighborhood ranges from 5 to \n10 will bring out an improvement  to prediction results. \nHowever, the unlimited expansion of the number of neighbors \nwill not make any improvement further. It can be seen that the \nresult declines slightly when the amount s of agents  are \nincreased to 15 . It brings invalid information to the  training \nprocess which is adverse for fitting. Therefore, the number of \nneighbors is also an important parameter to notice when \ndealing with the interaction problem between vehicles in \ntrajectory prediction. \nV. CONCLUSIONS \nIn this paper, we propose a Spatial -Channel Transformer \nNetwork to predict the trajectories of vehicles on the traffic \nscenes. We replace common RNN model with Transformer \nmodel to process long -term dependency data. With spatial \nembedding, the spati al-temporal feature of each agent is \ncaptured by attention mechanism of the transformer model. To \nexplore social interaction, the channel-wise attention module \nis jointly used to achieve fully attention functions of trajectory \nprediction on the NGSIM dataset. It has been proved in the \nexperiments that attention mechanism is powerful to deal with \ntime-series data.  \nSince trajectory prediction relies on the history data, the \nuncontrollability of future trajectories still exists. In the future, \nadditional information could be incorporated into the \nframework to improve our method, such as map data, and other \ndatasets. In addition, the graph structure can also be further \ncombined with existing models to improve the results. \nREFERENCES \n[1] S. Sivaraman and M. M. Trivedi . Dynamic Probabilistic Drivability \nMaps for Lane Chan ge and Merge Driver Assistance.  IEEE \nTransactions on Intelligent Transportation Systems. 15(5):2063â€“2073, \n2014. \n[2] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, F. F. Li, and S. \nSavarese. Social LSTM:  Human Trajectory Prediction in Crowded \nSpaces. In Proceedings of the IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), pages 961-971, 2016. \n[3] A. Gupta, J. Johnson, F. F. Li, S. Savarese, and A. Alahi. Social GAN: \nSocially Acceptable Tra jectories with Generative Adversarial \nNetworks. In Proceedings of the IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), pages 2255-2264, 2018. \n[4] B. Ivanovic and M. Pavone. The Trajectron: Probabilistic Multi-Agent \nTrajectory Modeling Wit h Dynamic Spatiotemporal Graphs . In  \nTable 2. A result of ablation studies with different range of neighbors. In addition, we compare the performance \nwith/without the SE module in order to evaluate its effect of interaction modeling.  \n Original No SE module Original No SE module Original No SE module \nVehicle numbers 5 10 15 \n1s ADE/FDE 0.21/0.41 0.22/0.42 0.20/0.39 0.21/0.40 0.20/0.38 0.20/0.39 \nRMSE 0.39 0.40 0.38 0.38 0.37 0.38 \n2s ADE/FDE 0.52/1.14 0.53/1.15 0.50/1.10 0.51/1.12 0.49/1.09 0.50/1.10 \nRMSE 0.90 0.91 0.88 0.88 0.87 0.88 \n3s ADE/FDE 0.92/2.14 0.93/2.15 0.88/2.06 0.90/2.10 0.88/2.07 0.89/2.08 \nRMSE 1.54 1.56 1.51 1.52 1.51 1.51 \n4s ADE/FDE 1.41/3.37 1.42/3.40 1.35/3.26 1.38/3.30 1.36/3.28 1.37/3.30 \nRMSE 2.32 2.35 2.27 2.29 2.28 2.29 \n5s ADE/FDE 1.97/4.81 1.98/4.85 1.90/4.66 1.93/4.71 1.91/4.70 1.92/4.73 \nRMSE 3.22 3.26 3.16 3.18 3.17 3.19 \n \n  \nProceedings of the IEEE International Conference on Computer Vision \n(ICCV), pages 2375-2384, 2019. \n[5] P. Zhang, W. L. Ouyang, P. F. Zhang, J. R. Xue, and N. N. Zheng. SR-\nLSTM: State Refinement for LSTM towards P edestrian Trajectory \nPrediction. In Proceedings of the IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), pages 12077-12086, 2019. \n[6] Y. F. Huang, H. K. Bi, Z. X. Li, T. L. Mao, and Z. Q. Wang . STGAT: \nModeling Spatial -Temporal Interactions for Hum an Trajectory \nPrediction. In Proceedings of the IEEE International Conference on \nComputer Vision (ICCV), pages 6281-6290, 2019. \n[7] A.Vaswani, N.Shazeer, N.Parmar, J.Uszkoreit, L.Jones, A. N.G omez, \nL.Kaiser and I.Polosukhin. Attention Is All You Need. In Proceedings \nof the 31st Advances in Neural Information Processing Systems (NIPS), \npages 6000-6010, 2017. \n[8] W. J. Luo, B. Yang, and R. Urtasun. Fast and Furious: Real Time End-\nto-End 3D Detection, Tracking and Motion Forecasting with a Single \nConvolutional Net . In  Proceedings of the  IEEE Conference on \nComputer Vision and Pattern Recognition (C VPR), pages 3569-3577, \n2018. \n[9] S. Becker, R. Hug, W. H Ã¼bner, and M. Arens . An Evaluation of \nTrajectory Prediction Approaches and Notes on the TrajNet Benchmark. \narXiv preprint arXiv:1805.07663, 2018. \n[10] J. Devlin, M.  Wei. Chang, K. Lee, and K. Toutanova . BERT: Pre -\ntraining of Deep Bidirectional Transformers for Language \nUnderstanding. In Proceedings of the 2019 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, pages 4171-4186, 2019. \n[11] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu. Squeeze-and-Excitation \nNetworks. In Proceedings of the IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), pages 7132-7141, 2018. \n[12] S. Hochreiter and J. Schmidhuber . Long short-term memory. Neural \nComputation, 9(8):1735-1780, 1997. \n[13] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical Evaluation of \nGated Recurrent Neural Networks on Seque nce Modeling . In \nProceedings of the Advances in Neural Information Processing Systems \n(NIPS) Workshop on Deep Learning, 2014. \n[14] X. Ma, P. Karkus, D. Hsu, and W. S. Lee . Particle Filter Recurrent \nNeural Networks. In Proceedings of the AAAI Conference on Artificial \nIntelligence, 34(04): 5101-5108, 2019. \n[15] P. Karkus, X. Ma, D. Hsu, L. Pack Kaelbling, W. S. Lee, and T. Lozano-\nPerez. Differentiable Algorithm Networks for Composable Robot \nLearning. arXiv preprint arXiv:1905.11602, 2019. \n[16] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning \nwith Neural Networks. In Proceedings of the 27th Advances in Neural \nInformation Processing Systems (NIPS), pages 3104-3112, 2014. \n[17] A. Vemula, K. Muelling, and J. Oh . Social Attention: Modeling \nAttention in Human Crowds. In Proceedings of the IEEE International \nConference on Robotics and Automation (I CRA), pages 4601-4607, \n2018. \n[18] A. Mohamed, K. Qian, M. Elhoseiny, and C. Claudel. Social-STGCNN: \nA Social Spatio -Temporal Graph Convolutional Neu ral Network for \nHuman Trajectory Prediction . In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition  (CVPR), \npages 14424-14432, 2020. \n[19] N. Deo and M. M. Trivedi . Convolutional Social Pooling for Vehicle \nTrajectory Prediction . In Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition Workshops (CVPRW), pages \n1549-1557, 2018. \n[20] A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, S. H. Rezatofighi, \nand S. Savarese . SoPhie: An Attentive GAN for Predictin g Paths \nCompliant to Social and Physical Constraints . In Proceedings of the  \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), \npages 1349-1358, 2019. \n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6971008777618408
    },
    {
      "name": "Computer science",
      "score": 0.6892455816268921
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5326157808303833
    },
    {
      "name": "Trajectory",
      "score": 0.4974670708179474
    },
    {
      "name": "Channel (broadcasting)",
      "score": 0.4274940490722656
    },
    {
      "name": "Computer vision",
      "score": 0.35830986499786377
    },
    {
      "name": "Real-time computing",
      "score": 0.3369859755039215
    },
    {
      "name": "Engineering",
      "score": 0.16112586855888367
    },
    {
      "name": "Computer network",
      "score": 0.1265660524368286
    },
    {
      "name": "Voltage",
      "score": 0.09854364395141602
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}