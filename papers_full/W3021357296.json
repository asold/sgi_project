{
  "title": "Language Model Prior for Low-Resource Neural Machine Translation",
  "url": "https://openalex.org/W3021357296",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5035665045",
      "name": "Christos Baziotis",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5110781707",
      "name": "Barry Haddow",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5038456766",
      "name": "Alexandra Birch",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2886540570",
    "https://openalex.org/W2963174344",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2976132230",
    "https://openalex.org/W2927431361",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2041404167",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2526471240",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2967985939",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963413917",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2986367395",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W2989276524",
    "https://openalex.org/W2963593215",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2979047515",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2550147980",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2154368244",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2970756316",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963681240",
    "https://openalex.org/W2888779557",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3023166997",
    "https://openalex.org/W2970694516",
    "https://openalex.org/W2986562961",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W3004127093"
  ],
  "abstract": "The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM \"disagrees\" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.",
  "full_text": "Language Model Prior for Low-Resource Neural Machine Translation\nChristos Baziotis, Barry Haddow and Alexandra Birch\nInstitute for Language, Cognition and Computation\nSchool of Informatics, University of Edinburgh\n10 Crichton Street, Edinburgh EH8 9AB\nc.baziotis@ed.ac.uk, bhaddow@inf.ed.ac.uk, a.birch@ed.ac.uk\nAbstract\nThe scarcity of large parallel corpora is an\nimportant obstacle for neural machine trans-\nlation. A common solution is to exploit the\nknowledge of language models ( LM) trained\non abundant monolingual data. In this work,\nwe propose a novel approach to incorporate a\nLM as prior in a neural translation model (TM).\nSpeciﬁcally, we add a regularization term,\nwhich pushes the output distributions of the\nTM to be probable under the LM prior, while\navoiding wrong predictions when the TM “dis-\nagrees” with the LM. This objective relates\nto knowledge distillation, where the LM can\nbe viewed as teaching the TM about the tar-\nget language. The proposed approach does\nnot compromise decoding speed, because the\nLM is used only at training time, unlike previ-\nous work that requires it during inference. We\npresent an analysis on the effects that different\nmethods have on the distributions of the TM.\nResults on two low-resource machine transla-\ntion datasets show clear improvements even\nwith limited monolingual data.\n1 Introduction\nNeural machine translation (NMT ) (Sutskever et al.,\n2014; Bahdanau et al., 2015; Vaswani et al., 2017)\nrelies heavily on large parallel corpora (Koehn\nand Knowles, 2017) and needs careful hyper-\nparameter tuning, in order to work in low-resource\nsettings (Sennrich and Zhang, 2019). A popular\napproach for addressing data scarcity is to exploit\nabundant monolingual corpora via data augmenta-\ntion techniques, such as back-translation (Sennrich\net al., 2016). Although back-translation usually\nleads to signiﬁcant performance gains (Hoang et al.,\n2018), it requires training separate models and ex-\npensive translation of large amounts of monolin-\ngual data. However, when faced with lack of train-\ning data, a more principled approach is to consider\nexploiting prior information.\nLanguage models ( LM) trained on target-side\nmonolingual data have been used for years as pri-\nors in statistical machine translation (SMT ) (Brown\net al., 1993) via the noisy channel model. This\napproach has been adopted to NMT , with the neural\nnoisy channel (Yu et al., 2017; Yee et al., 2019).\nHowever, neural noisy channel models face a com-\nputational challenge, because they model the “re-\nverse translation probability” p(x|y). Speciﬁcally,\nthey require multiple passes over the source sen-\ntence xas they generate the target sentence y, or\nsophisticated architectures to reduce the passes.\nLMs have also been used in NMT for re-\nweighting the predictions of translation models\n(TM), or as additional context, via LM-fusion (Gul-\ncehre et al., 2015; Sriram et al., 2018; Stahlberg\net al., 2018). But, as the LM is required during de-\ncoding, it adds a signiﬁcant computation overhead.\nAnother challenge is balancing the TM and the LM,\nwhose ratio is either ﬁxed (Stahlberg et al., 2018) or\nrequires changing the model architecture (Gulcehre\net al., 2015; Sriram et al., 2018).\nIn this work, we propose to use a LM trained\non target-side monolingual corpora as a weakly\ninformative prior. We add a regularization term,\nwhich drives the output distributions of the TM to\nbe probable under the distributions of the LM. This\ngives ﬂexibility to the TM, by enabling it to deviate\nfrom the LM when needed, unlike fusion methods\nthat change the decoder’s distributions, which can\nintroduce translation errors. The LM “teaches” the\nTM about the target language similar to knowledge\ndistillation (Bucila et al., 2006; Hinton et al., 2015).\nThis method works by simply changing the training\nobjective and does not require any changes to the\nmodel architecture. Importantly, the LM is sepa-\nrated from the TM, which means that it is needed\nonly during training, therefore we can decode faster\nthan fusion or neural noisy channel. We also note\nthat this method is not intended as a replacement\nto other techniques that use monolingual data, such\nas back-translation, but is orthogonal to them.\nWe make the following contributions:\n1. We propose a simple and principled way for\nincorporating prior information from LMs in\nNMT , by adding an extra regularization term\n(§ 3). Also, this approach enables fast decod-\ning, by requiring the LM only during training.\n2. We report promising results (§ 4.2) in two low-\nresource translation datasets. We ﬁnd that the\nproposed LM-prior yields clear improvements\neven with limited monolingual data.\n3. We perform an analysis ( § 5) on the effects\nthat different methods have on the output dis-\ntributions of the TM, and show how this can\nlead to translation errors.\n2 Background\nNMT models trained with maximum likelihood es-\ntimation, model directly the probability p(y|x) of\nthe target sentence ygiven the source sentence x:\nˆy= arg max\ny\nlog p(y|x)\nModeling directly p(y|x) requires large amounts\nof parallel sentences to learn a good model and\nNMT lacks a principled way for leveraging mono-\nlingual data. In this section we review approaches\nthat exploit prior information encoded in LMs or\nthe signal from the language modeling task.\nNoisy Channel Model SMT (Koehn, 2010) em-\nploys Bayes’ rule which offers a natural way\nfor exploiting monolingual data, using a target-\nside LM based on the so called “noisy channel”\nmodel (Shannon and Weaver, 1949). Instead\nof directly modeling p(y|x), it models the “re-\nverse translation probability” p(x|y), by rewriting\np(y|x) ∝p(x|y) ×p(y). It selects words that are\nboth a priori likely with p(yi) and “explain well”\nthe input with p(x|yi). This idea has been adopted\nto NMT with neural noisy channel, but it has two\nfundamental limitations. First, during decoding the\nmodel has to alternate between generating the out-\nput and scoring the input (Yu et al., 2017, 2019) or\nperform multiple forward passes (Yee et al., 2019)\nover x. And crucially, since the LM is part of the\nnetwork it has to also be used during inference,\nwhich adds a computational constraint on its size.\nFusion Gulcehre et al. (2015) proposed to incor-\nporate pretrained LMs in NMT , using shallow- and\ndeep-fusion. In shallow-fusion, the LM re-weights\nthe TM’s scores via log-linear interpolation:\nlog p(yt) =(1−β) logpTM (yt|y<t,x)\n+ β log pLM (yt|y<t)\nIn deep fusion, they alter the model architecture to\ninclude the hidden states of a RNN -LM (Mikolov\net al., 2011) as additional features for predicting\nthe next word in the decoder, which are weighted\nwith a controller mechanism (i.e., gating). In both\napproaches, the TM and LM are ﬁrst trained inde-\npendently and are combined later. Sriram et al.\n(2018) extend these ideas with cold-fusion, where\nthey train the TM from scratch with the LM, using\nits logits as features, instead of its LM hidden states.\nStahlberg et al. (2018) simplify this, by training a\nTM together with a ﬁxed LM, using combinations\nof the TM’s andLM’s outputs. By training theTM\nwith the assistance of the LM, the motivation is that\nthe TM will rely on the LM for ﬂuency, whereas the\nTM will be able to focus on modeling the source.\nThey report the best results with the POSTNORM\nmethod, outperforming other LM-fusion techniques.\nPOSTNORM parameterizes p(yt) as follows:\np(yt)=softmax(logpTM (yt|y<t,x)+logpLM (yt))\nIt is practically the same as shallow-fusion, but\nwith the LM used also during training, instead of\nused just in inference, and interpolating with λ=1.\nFusion methods face the same computational\nlimitation as noisy channel, since the LM needs to\nbe used during inference. Also, probability inter-\npolation methods, such as shallow fusion or POST -\nNORM , use a ﬁxed weight for all time-steps, which\ncan lead to translation errors. Gated fusion (Gul-\ncehre et al., 2015; Sriram et al., 2018) is more ﬂexi-\nble, but requires changing the network architecture.\nOther Approaches Transfer-learning is another\napproach for exploiting pretrainedLMs. Ramachan-\ndran et al. (2017), ﬁrst proposed to use LMs\ntrained on monolingual corpora to initialize the\nencoder and decoder of a TM. Skorokhodov et al.\n(2018) extended this idea to Transformer architec-\ntures (Vaswani et al., 2017). This approach requires\nthe TM to have identical architecture to the LM,\nwhich can be a limitation if the LM is huge.\nDomhan and Hieber (2017) used language mod-\neling as extra signal, by training the decoder of a\nTM also as a LM on target-side monolingual data.\nSennrich et al. (2016) replaced the source with a\nNULL token, while training on monolingual data.\nBoth, reported mixed results, with marginal gains.\n3 Language Model Prior\nWe propose to move the LM out of the TM and\nuse it as a prior over its decoder, by employing\nposterior regularization (PR) (Ganchev et al., 2010).\nPR incorporates prior information, by imposing\nsoft constraints on a model’s posterior distributions,\nwhich is much easier than putting Bayesian priors\nover all the parameters of a deep neural network.\nL=\nN∑\nt=1\n−log pTM (yt|y<t,x) (1)\n+ λDKL (pTM (yt|y<t,x) ∥pLM (yt|y<t))\nThe ﬁrst term is the standard translation objective\nLMT and the second is the regularization term LKL ,\nwhich we interpret as a weakly informative prior\nover the TM’s distributionspTM , that expresses par-\ntial information about y. LKL is deﬁned as the\nKullback-Leibler divergence between the output\ndistributions of the TM and the LM, weighted by λ.\nThis formulation gives ﬂexibility to the model,\nunlike probability interpolation, such as in fusion\nmethods. For example, POSTNORM multiplies the\nprobabilities of the LM and TM, which is the same\nas applying a logical AND operation, where only\nwords that are probable under both distributions\nwill receive non-negligible probabilities. This pre-\nvents the model from generating the correct word\nwhen there is a large “disagreement” between the\nTM and the LM, which is inevitable as the LM is not\naware of the source sentence (i.e., unconditional).\nHowever, by using the LM-prior we do not change\nthe outputs of the TM. LKL pushes the TM to stay\non average close to the prior, but crucially, it en-\nables the TM to deviate from it when needed, for\nexample to copy words from the source.\nSecondly, theLM is no longer part of the network.\nThis means that we can do inference using only the\nTM, unlike fusion or neural noisy channel, which\nrequire the LM for both training and decoding. By\nlifting this computational overhead, we enable the\nuse of large pretrained models LMs (BERT; Devlin\net al. (2019), GPT-2; Radford et al. (2019)), without\ncompromising speed or efﬁciency.\n3.1 Relation to Knowledge Distillation\nThe regularization term in Eq.(1) resembles knowl-\nedge distillation (KD) (Ba and Caruana, 2014; Bu-\ncila et al., 2006; Hinton et al., 2015), where the soft\noutput probabilities of a big teacher model are used\nto train a small compact student model, by min-\nhard target label smoothing language model\nFigure 1: Targets with LS and LM-prior.\nimizing their DKL . However, in standard KD the\nteacher is trained on the same task as the student,\nlike in KD for machine translation (Kim and Rush,\n2016). However, the proposed LM-prior is trained\non a different task that requires only monolingual\ndata, unlike TM teachers that require parallel data.\nWe exploit this connection to KD and fol-\nlowing Hinton et al. (2015) we use a softmax-\ntemperature parameter τ ≥1 to control the smooth-\nness of the output distributions pi = exp(si/τ)∑\nj exp(sj /τ)) ,\nwhere si is the un-normalized score of each word i\n(i.e., logit). Higher values of τ produce smoother\ndistributions. Intuitively, this controls how much\ninformation encoded in the tail of the LM’s distri-\nbutions, we expose to the TM. Speciﬁcally, a well\ntrained LM will generate distributions with high\nprobability for a few words, leaving others with\nprobabilities close to zero. By increasing τ we\nexpose extra information to the TM, because we re-\nveal more low-probability words that the LM found\nsimilar to the predicted word.\nWe use τ >1 only for computing the DKL be-\ntween the distributions of the TM and the LM and\nis the same for both. The magnitude of DKL scales\nas 1/τ2, so it is important to multiply its output\nwith τ2 to keep the scale of the LKL loss invariant\nto τ. Otherwise, this would implicitly change the\nweight to λapplied to LKL . Finally, we re-write the\nregularization term of Eq. (1) as follows:\nLKL =τ2DKL (pTM (yt|y<t,x;τ)∥pLM (yt|y<t;τ))\n3.2 Relation to Label Smoothing\nLabel smoothing (LS) (Szegedy et al., 2016) is a\n“trick” widely used in machine translation that also\nuses soft targets. Speciﬁcally, the target distribution\nat each step is the weighted average between the\none-hot distribution yk of the ground-truth label\nand a uniform distribution over all other Klabels,\nparameterized by a smoothing parameter α: yLS\ni =\nyi(1 −α) +α/K. The purpose of LS is to penalize\nconﬁdence (i.e., low-entropy distributions).\nlanguage-pair train dev test\nEnglish-Turkish 192,482 3,007 3,000\nEnglish-German 275,561 3,004 2,998\nTable 1: Dataset statistics after preprocessing.\nWe note that LS differs from the LM-prior in two\nways. First, LS encourages the model to assign\nequal probability to all incorrect words (M ¨uller\net al., 2019), which can be interpreted as a form of\nuninformative prior (Fig. 1). By contrast, the dis-\ntributions of the LM are informative, because they\nexpress the beliefs of the LM at each step. Second,\nLS changes the target distribution (i.e., ﬁrst term\nin Eq. (1)), whereas the LM-prior involves an addi-\ntional term, hence the two methods are orthogonal.\n4 Experiments\nDatasets We use two low-resource language\npairs (Table 1): the English-German (EN-DE)\nNews Commentary v13 provided by WMT (Bo-\njar et al., 2018) 1 and the English-Turkish (EN-TR)\nWMT-2018 parallel data from the SETIMES22 cor-\npus. We use the ofﬁcial WMT-2017 and 2018 test\nsets as the development and test set, respectively.\nAs monolingual data for English and German\nwe use the News Crawls 2016 articles (Bojar et al.,\n2016) and for Turkish we concatenate all the avail-\nable News Crawls data from 2010-2018, which\ncontain 3M sentences. For English and German we\nsubsample 3M sentences to match the Turkish data,\nas well as 30M to measure the effect of stronger\nLMs. We remove sentences longer than 50 words.\nPre-processing We perform punctuation normal-\nization and truecasing and remove pairs, in which\neither of the sentences has more than 60 words or\nlength ratio over 1.5. The text is tokenized with\nsentencepiece (SPM; Kudo and Richardson (2018))\nwith the “unigram” model. For each language we\nlearn a separate SPM model with 16K symbols,\ntrained on its respective side of the parallel data.\nFor English, we train SPM on the concatenation\nof the English-side of the training data from each\ndataset, in order to have a single English vocabulary\nand be able to re-use the same LM.\nModel Conﬁguration In all experiments, we use\nthe Transformer architecture for both the LMs and\n1http://www.statmt.org/wmt18/translation-task.html\n2http://opus.nlpl.eu/SETIMES2.php\nparameter value\nTM LM\nEmbedding size 512 1024\nTransformer hidden size 1024 4096\nTransformer layers 6 6\nTransformer heads 8 16\nDropout (all) 0.3 0.3\nTable 2: Hyperparameters of the TMs and LMs.\nlanguage 3M (PPL ↓) 30M (PPL ↓)\nEnglish 29.70 25.02\nGerman 22.71 19.22\nTurkish 22.78 –\nTable 3: Perplexity scores for LMs trained on each lan-\nguage’s monolingual data, computed on a small held-\nout validation set per language.\nTMs. Table 2 lists all their hyperparameters. For\nthe TMs we found that constraining their capac-\nity and applying strong regularization was cru-\ncial, otherwise they suffered from over-ﬁtting. We\nalso found that initializing all weights with glorot-\nuniform (Glorot and Bengio, 2010) initialization\nand using pre-norm residual connections (Xiong\net al., 2020; Nguyen and Salazar, 2019), improved\nstability. We also tied the embedding and the out-\nput (projection) layers of the decoders (Press and\nWolf, 2017; Inan et al., 2017).\nWe optimized our models with Adam (Kingma\nand Ba, 2015) with a learning rate of 0.0002 and a\nlinear warmup for the ﬁrst 8K steps, followed by\ninverted squared decay and with mini-batches with\n5000 tokens per batch. We evaluated each model\non the dev set every 5000 batches, by decoding\nusing greedy sampling, and stopped training if the\nBLEU score did not increase after 10 iterations.\nFor the LM training we followed the same opti-\nmization process as for the TMs. However, we use\nTransformer-large conﬁguration, in order to obtain\na powerful LM-prior. Crucially, we did not applyLS\nduring the LM pretraining, because, as discussed,\nit pushes the models to assign equal probability\nto all incorrect words (M¨uller et al., 2019), which\nwill make the prior less informative. In Table 3\nwe report the perplexities achieved by each LM on\ndifferent scales of monolingual data.\nWe developed our models in PyTorch (Paszke\net al., 2019) and we used the Transformer imple-\nmentation from JoeyNMT (Kreutzer et al., 2019).\nWe make our code publically available3.\n3github.com/cbaziotis/lm-prior-for-nmt\nMethod DE→EN EN →DE TR →EN EN →TR\ndev test dev test dev test dev test\nBase 22.6 ±0.1 26.9±0.1 18.3±0.3 25.6±0.2 15.9±0.0 16.6±0.3 12.2±0.1 11.2±0.2\nShallow-fusion 23.4 ±0.1 27.8±0.1 18.5±0.2 26.0±0.1 16.5±0.1 17.3±0.3 12.7±0.0 11.5±0.1\nPOSTNORM 20.4±0.2 24.5±0.3 16.6±0.1 22.9±0.3 13.8±0.2 14.8±0.1 11.0±0.1 10.2±0.2\nPOSTNORM + LS 22.0±0.3 26.4±0.2 16.9±0.5 23.3±0.5 15.0±0.1 16.0±0.0 12.5±0.2 11.0±0.2\nBase + LS 23.8±0.6 28.4±0.7 19.2±0.3 27.3±0.3 17.5±0.1 18.4±0.2 13.8±0.2 12.6±0.0\nBase + Prior 24.9±0.0 30.2±0.1 20.5±0.3 29.1±0.7 18.5±0.2 19.5±0.2 15.1±0.1 13.8±0.1\nBase + Prior + LS 25.1±0.3 30.3±0.3 20.8±0.4 29.7±0.7 18.5±0.3 19.5±0.2 15.5±0.1 14.1±0.2\nBase + Prior (30M) 24.9 ±0.1 30.0±0.1 21.0±0.4 29.8±0.3 18.6±0.0 19.5±0.2 – –\nTable 4: BLEU scores of each model. Mean and stdev of 3 runs reported. The top section contains the main results,\nwhere all methods use LMs trained on the same amount of data (3M). The bottom section compares different\nconﬁgurations of the LM-prior. Underlined scores denote gains over the “Base + Prior (3M)” model.\n4.1 Experiments\nWe compare the proposed LM-prior with other\napproaches that incorporate a pretrained LM or\nregularize the outputs of the TM. First, we\nconsider a vanilla NMT baseline without LS.\nNext, we compare with fusion techniques, namely\nshallow-fusion (Gulcehre et al., 2015) and POST -\nNORM (Stahlberg et al., 2018), which in the original\npaper outperformed other fusion methods. We also\nseparately compare with label smoothing (LS), be-\ncause it is another regularization method that uses\nsoft targets. We report detokenized case-sensitive\nBLEU using sacre-BLEU (Post, 2018)4, and decode\nwith beam search of size 5. The LMs are ﬁxed\nduring training for both POSTNORM and the prior.\nWe tune the hyper-parameters of each method\non the DE→EN dev-set. We set the interpolation\nweight for shallow-fusion to β=0.1, the smoothing\nparameter for LS to α = 0.1. For the LM-prior\nwe set the regularization weight to λ=0.5 and the\ntemperature for LKL to τ=2.\n4.2 Results\nFirst, we use in all methods LMs trained on the\nsame amount of monolingual data, which is 3M\nsentences. We used the total amount of available\nTurkish monolingual data (3M) as the lowest com-\nmon denominator. This is done to remove the ef-\nfects of the size of monolingual data from the ﬁnal\nperformance of each method, across language-pairs\nand translation directions. The results are shown in\nthe top section of Table 4. We also report results\nwith recurrent neural networks (RNN ) based on the\nattentional encoder-decoder (Bahdanau et al., 2015)\narchitecture in appendix A.\n4Signature “BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.2”\nOverall, adding the LM-prior consistently im-\nproves performance in all experiments. Speciﬁ-\ncally, it yields up to +1.8 BLEU score gains over\nthe strongest baseline “Base+ LS” (DE→EN and\nEN→DE). This shows that the proposed approach\nyields clear improvements, even with limited mono-\nlingual data (3M). As expected, LS proves to be\nvery effective for mitigating over-ﬁtting in such\nlow-resource conditions. However, simply penaliz-\ning conﬁdence helps up to a point, which is shown\nby the performance gap between “Base+ LS” and\n“Base+prior”. We explore this further next (§ 5).\nShallow-fusion achieves consistent but marginal\nimprovements in all language-pairs. It works by\nmaking small (local) changes to pTM , which pri-\nmarily helps improve ﬂuency when the TM is very\nunsure about what to generate next. Surprisingly,\nwhen training the TM with the POSTNORM objec-\ntive, it barely reaches the baseline. As we show\nin our analysis ( § 5), under POSTNORM the TM\ngenerates very sharp distributions, which is a result\nof how it combines pTM and pLM\n5. We identify two\npotential reasons for this result. First, in (Stahlberg\net al., 2018) POSTNORM was only tested with LS,\nwhich to some extend hid the issue of low-entropy\noutputs. To verify this, we trained POSTNORM\nwith LS. We observed that in this case, the scores\nimprove signiﬁcantly, but it still falls short in com-\nparison with the other methods. Second, we note\nthat the LMs used in the original paper were also\ntrained with LS. We hypothesize that by using an\nLM that emitted smoother distribution, it implic-\nitly down-weighted the contribution of pLM , that is\n5By multiplying the probabilities ofpTM and pLM , or adding\ntheir log-probabilities, very small subset of tokens that have\nnon-negligible probability in both of them, will be assigned\nvery large probability in the ﬁnal distribution\n10K 50K 100K full\nparallel data\n10\n15\n20\n25\n30BLEU\nBase + LS\nBase + Prior\nFigure 2: BLEU scores (mean of 3 runs) on theDE→EN\ntest set with different scales of parallel data, using the\nLM trained on 30M English sentences.\nsimilar to the small weight used in shallow-fusion,\nwhich works better in our experiments.\nStronger LMs Next, we test how different vari-\nations of the LM-prior affect the translation qual-\nity (bottom section of Table 4). First, we lift the\nmonolingual data constraint, in order to evaluate\nthe impact of stronger LM-priors. Speciﬁcally, for\nEnglish and German we use LMs trained on 30M\nsentences. We observe that the stronger LMs yield\nimprovements only in the EN→DE direction. This\ncould partially be explained by the fact that German\nhas richer morphology than English. Therefore, it\nis harder for the decoder to avoid grammatical mis-\ntakes in low-resource settings while translating into\nGerman, and a stronger prior is more helpful for\nX→DE than X→EN.\nHowever, it is still surprising that the stronger\nEnglish LM does not boost performance. We hy-\npothesize that this might be related to the limited\ncapacity of the TMs we used. Speciﬁcally, in theKD\nliterature it has been found that the student’s per-\nformance is affected by the difference between the\ncapacities of the student and teacher networks (Cho\nand Hariharan, 2019; Zhou et al., 2020). In prelim-\ninary experiments we also used big LMs pretrained\non generic large-scale data, such asGPT-2 (Radford\net al., 2019), but we failed to achieve any measur-\nable improvements over the baseline. Besides the\ndiscrepancy in the capacity between the LM and\nthe TM, we suspect that another obstacle in this\ncase is the large vocabulary size used in GPT-2\n(50K symbols). In particular, Sennrich and Zhang\n(2019) showed that in low-resource NMT , using\na very small vocabulary (2K-10K symbols) is the\nmost important factor that affects translation per-\nformance. A potential solution could be to ﬁnetune\nGPT-2 on the small vocabulary of the TM (Zhao\net al., 2019) and then use it as a prior, but we leave\nthis exploration for future work.\nPrior + LS We also evaluate a combination of\nthe LM-prior with LS. We observe that in most ex-\nperiments it has small but additive effects. This\nimplicitly suggests that the two approaches are\ncomplementary to each other. LS smooths the one-\nhot target distribution, which penalizes conﬁdence,\nwhereas the LM-prior helps improve ﬂuency. We\nfurther explore their differences in our analysis\n(§. 5), by showing the effects each method has on\nthe TM’s distributions.\n4.3 Extremely Low-Resource Experiments\nWe also conducted experiments that measure the\neffect of the LM-prior on different scales of parallel\ndata. Speciﬁcally, we emulate more low-resource\nconditions, by training on subsets of the EN→DE\nparallel data. In Fig. 2 we compare theBLEU scores\nof the “Base+LS” and the “Base+Prior (30M)”.\nOverall, we observe that adding the LM-prior\nyields consistent improvements, even with as little\nas 10K parallel sentences. The improvements have\na weak correlation with the size of parallel data.\nWe hypothesize that by exposing the TM to a larger\nsample of target-side sentences, it has the oppor-\ntunity to extract more information from the prior.\nHowever, we anticipate that in more high-resource\nsettings the improvements will start to diminish.\n5 Analysis\nThe main results show that LS, that simply penal-\nizes conﬁdence, is a very effective form of regu-\nlarization in low-resource settings. We conduct\na qualitative comparison to test whether the im-\nprovements from the proposed LM-prior are due to\npenalizing conﬁdence, similar to LS, or from actu-\nally using information from the LM. Speciﬁcally,\nwe evaluate each model on theDE→EN test-set and\nfor each target token we compute the entropy of\neach model’s distribution. In Fig. 3 we plot for\neach model the density6 over all its entropy values.\n6We ﬁt a Gaussian kernel with bandwidth 0.3 on the en-\ntropy values of each model. Density plots are more readable\n0 1 3 5 7\nentropy per token\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndensity\nBase\nBase+LS\nLM\nBase+Prior\nPOSTNORM\nFigure 3: Estimated densities based on each model’s\nentropy on the DE→EN test set.\nFirst, we observe that the un-regularized “Base”\nmodel generates very conﬁdent (low-entropy) dis-\ntributions, which suggests that it overﬁts on the\nsmall parallel data. As expected, the LS regular-\nization successfully makes the TM less conﬁdent\nand therefore more robust to over-ﬁtting. For addi-\ntional context, we plot the entropy density of the\nLM and observe that, unsurprisingly, it is the most\nuncertain, since it is unconditional.\nInterestingly, the model trained with the LM-\nprior emits more conﬁdent distributions than the\n“Base+LS” model, although it also achieves signiﬁ-\ncantly better performance. This clearly shows that\nthe gains cannot be explained just from smoothing\nthe distributions of the TM and suggests that the\nmodel indeed exploits information from the LM.\nNext, we focus on the “Base+ POSTNORM ”\nmodel and observe that it generates the most conﬁ-\ndent predictions. Note that, this ﬁnding aligns with\na similar analysis in the original paper, where it was\nshown that underPOSTNORM the TM generates low-\nentropy distributions. However, even though this\nmethod might improve ﬂuency, it can hurt transla-\ntion quality in certain cases. As described in Sec. 3,\nby multiplying the two distributions, only a small\nsubset of words will have non-zero probability in\nthe ﬁnal distribution. This means that when there\nare “disagreements” between the TM and LM this\ncan lead to wrong predictions. We illustrate this\nwith a concrete example in Fig. 4. Although the\nTM predicted the correct word, the multiplication\nwith the LM distribution caused the model to ﬁnally\nmake a wrong prediction. Also, the ﬁnal distribu-\ntion assigns a relatively high probability to a word\ncompared to plotting overlapping histograms.\nDE: die Republikaner im Kongress drängen auf eine umfassendere\nNeufassung der Ozonregeln.\nEN: Republicans  in  Congress  are  pushing  for  a  broader rewrite  \nof  the  ozone  rules.\nbroader 34.3%\nwider 22.4%\nlarger 7.2%\nTM\nnew 8.9%\nrepeal 7.1%\nbill 3.7%\nLM\nmore 44.8%\nwider 31.8%\nlarger 11.8%\n… … …\nx =\nmore\nFigure 4: Example of failure of probability interpola-\ntion between LM and TM, while translating DE→EN.\n(“more”), which is not among the top predictions\nof neither the LM or the TM. By contrast, the LM-\nprior does not change the TM’s predictions, and the\nmodel has the ﬂexibility to deviate from the prior.\n5.1 LKL Sensitivity Analysis\nThe proposed regularization uses two different\nhyper-parameters in LKL , the weight λthat controls\nthe strength of the regularization, and the temper-\nature τ that controls how much information from\nthe long-tail of the LM to expose to the TM. We\ndo a pairwise comparison between them, in order\nto measure how sensitive the model is to their val-\nues. In Fig. 5 we plot a heatmap of theBLEU scores\nachieved by models trained on the DE→EN dev-set\nwith various combinations.\nOverall, we observe a clear pattern emerging\nof how the LM-prior affects performance, which\nsuggests that (1) using τ > 1 indeed helps the\nTM to acquire more of the knowledge encoded in\nthe prior, and (2) increasing the strength of the\nregularization up to a point yields consistent im-\nprovements. We ﬁnd that the performance is less\nsensitive to the value of τ, compared to λand that\n1 2 5\nKL temperature (τ)\n0.02 0.1 0.2 0.5 1.0\nKL weight (λ)\n29.1 29.6 29.2\n29.7 30.5 30.3\n30.4 30.6 30.4\n29.1 30.7 30.3\n25.2 29.6 30.3\n27\n28\n29\n30\nBLEU\nFigure 5: BLEU scores on the DE-EN dev set of models\ntrained with different λand τ for the LKL . Mean of 3\nruns for each combination reported.\nby setting τ >1, the model becomes also more\nrobust to λ. Our explanation is that for τ >1, the\nTM tries to match a larger part of the LM’s distri-\nbution and focuses less on its top-scoring words.\nTherefore, it is reasonable to observe that in the\nextreme case when we set equal weight to the LMT\nand LKL (λ= 1) and τ = 1the performance starts\nto degrade, because we strongly push the TM to\nmatch only the top-scoring predictions of the LM,\nthat is unconditional. This forces the TM to pay\nless attention to the source sentence, which leads\nto translation errors.\n6 Related Work\nMost recent related work considers large pretrained\nmodels, either via transfer-learning or feature-\nfusion. Zhu et al. (2020); Clinchant et al. (2019);\nImamura and Sumita (2019) explore combina-\ntions of using BERT as initialization for NMT ,\nor adding BERT’s representations as extra fea-\ntures. Yang et al. (2019) address the problem of\ncatastrophic-forgetting while transferring BERT in\nhigh-resource settings, with a sophisticated ﬁne-\ntuning approach. In concurrent work, Chen et al.\n(2019) propose knowledge-distillation using BERT\nfor various text generation tasks, including NMT ,\nby incentivizing the sequence-to-sequence models\nto “look into the future”. However, in our work\nwe address a different problem (low-resourceNMT )\nand have different motivation. Also, we consider\nauto-regressive LMs as priors, which have clear in-\nterpretation, unlike BERT that is not strictly a LM\nand requires bidirectional context. Note that, large\npretrained LMs, such as BERT or GPT-2, have not\nyet achieved the transformative results in NMT that\nwe observe in natural language understanding tasks\n(e.g., GLUE benchmark (Wang et al., 2019)).\nThere are also other approaches that have used\nposterior regularization to incorporate prior knowl-\nedge into NMT . Zhang et al. (2017) exploit lin-\nguistic real-valued features, such as dictionaries or\nlength ratios, to construct the distribution for reg-\nularizing the TM’s posteriors. Recently, Ren et al.\n(2019) used posterior regularization for unsuper-\nvised NMT , by employing an SMT model, which\nis robust to noisy data, as a prior over a neural TM\nto guide it in the iterative back-translation process.\nFinally, LMs have been used in a similar fashion\nas priors over latent text sequences in discrete la-\ntent variable models (Miao and Blunsom, 2016;\nHavrylov and Titov, 2017; Baziotis et al., 2019).\n7 Conclusions\nIn this work, we present a simple approach for in-\ncorporating knowledge from monolingual data to\nNMT . Speciﬁcally, we use a LM trained on target-\nside monolingual data, to regularize the output dis-\ntributions of a TM. This method is more efﬁcient\nthan alternative approaches that used pretrained\nLMs, because it is not required during inference.\nAlso, we avoid the translation errors introduced by\nLM-fusion, because the TM is able to deviate from\nthe prior when needed.\nWe empirically show that while this method\nworks by simply changing the training objective, it\nachieves better results than alternative LM-fusion\ntechniques. Also, it yields consistent performance\ngains even with modest monolingual data (3M sen-\ntences) across all translation directions. This makes\nit useful for low-resource languages, where not\nonly parallel but also monolingual data are scarce.\nIn future work, we intend to experiment with\nthe LM-prior under more challenging conditions,\nsuch as when there is domain discrepancy between\nthe parallel and monolingual data. Also, we would\nlike to explore how to overcome the obstacles that\nprevent us from fully exploiting large pretrained\nLMs (e.g., GPT-2) in low-resource settings.\nAcknowledgments\nThis work was conducted within the scope\nof the Research and Innovation Action\nGourmet, which has received funding from the Eu-\nropean Union’s Horizon 2020 research and innova-\ntion programme under grant agreement No 825299.\nIt was also supported by the UK Engineering\nand Physical Sciences Research Council fellowship\ngrant EP/S001271/1 (MTStretch).\nIt was performed using resources provided by\nthe Cambridge Service for Data Driven Discov-\nery (CSD3) operated by the University of Cam-\nbridge Research Computing Service (http://www.\ncsd3.cam.ac.uk/), provided by Dell EMC and\nIntel using Tier-2 funding from the Engineering\nand Physical Sciences Research Council (capital\ngrant EP/P020259/1), and DiRAC funding from\nthe Science and Technology Facilities Council\n(www.dirac.ac.uk).\nReferences\nJimmy Ba and Rich Caruana. 2014. Do deep nets re-\nally need to be deep? In Proceedings of the Ad-\nvances in Neural Information Processing Systems ,\npages 2654–2662, Montreal, Quebec, Canada.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450, abs/1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe International Conference on Learning Represen-\ntations, San Diego, CA, USA.\nChristos Baziotis, Ion Androutsopoulos, Ioannis\nKonstas, and Alexandros Potamianos. 2019. SEQˆ3:\nDifferentiable sequence-to-sequence-to-sequence\nautoencoder for unsupervised abstractive sentence\ncompression. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 673–681, Minneapolis, Minnesota,\nUSA. Association for Computational Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aur ´elie\nN´ev´eol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 conference\non machine translation. In Proceedings of the Con-\nference on Machine Translation , pages 131–198,\nBerlin, Germany.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn, and\nChristof Monz. 2018. Findings of the conference on\nmachine translation (WMT). In Proceedings of the\nConference on Machine Translation, pages 272–303,\nBelgium, Brussels.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–\n311.\nCristian Bucila, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In Pro-\nceedings of the ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining ,\npages 535–541, Philadelphia, PA, USA.\nYen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu,\nand Jingjing Liu. 2019. Distilling the knowl-\nedge of bert for text generation. arXiv preprint\narXiv:1911.03829.\nJang Hyun Cho and Bharath Hariharan. 2019. On the\nefﬁcacy of knowledge distillation. In Proceedings\nof the IEEE International Conference on Computer\nVision, pages 4794–4802.\nStephane Clinchant, Kweon Woo Jung, and Vassilina\nNikoulina. 2019. On the use of BERT for neural ma-\nchine translation. In Proceedings of the Workshop\non Neural Generation and Translation , pages 108–\n117, Stroudsburg, PA, USA.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 4171–4186, Minneapolis, Minnesota.\nTobias Domhan and Felix Hieber. 2017. Using target-\nside monolingual data for neural machine translation\nthrough multi-task learning. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1500–1505, Copenhagen,\nDenmark.\nKuzman Ganchev, Jennifer Gillenwater, Ben Taskar,\net al. 2010. Posterior regularization for structured\nlatent variable models. Journal of Machine Learn-\ning Research, 11(Jul):2001–2049.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neu-\nral networks. In Proceedings of the Thirteenth\nInternational Conference on Artiﬁcial Intelligence\nand Statistics, volume 9 of Proceedings of Machine\nLearning Research , pages 249–256, Chia Laguna\nResort, Sardinia, Italy. PMLR.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nSerhii Havrylov and Ivan Titov. 2017. Emergence of\nlanguage with multi-agent games: Learning to com-\nmunicate with sequences of symbols. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Proceed-\nings of the Advances in Neural Information Process-\ning Systems, pages 2149–2159.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, abs/1503.02531.\nVu Cong Duy Hoang, Philipp Koehn, Gholamreza\nHaffari, and Trevor Cohn. 2018. Iterative back-\ntranslation for neural machine translation. In Pro-\nceedings of the Workshop on Neural Machine Trans-\nlation and Generation , pages 18–24, Melbourne,\nAustralia.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nKenji Imamura and Eiichiro Sumita. 2019. Recycling a\npre-trained BERT encoder for neural machine trans-\nlation. In Proceedings of the 3rd Workshop on\nNeural Generation and Translation , pages 23–31,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Proceed-\nings of the International Conference on Learning\nRepresentations, Toulon, France.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1317–1327, Austin, Texas,\nUSA.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the International Conference on Learning Repre-\nsentations, San Diego, CA, USA.\nPhilipp Koehn. 2010. Statistical Machine Translation,\n1st edition. Cambridge University Press, New York,\nNY , USA.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the Workshop on Neural Machine Transla-\ntion, pages 28–39, Vancouver, Canada.\nJulia Kreutzer, Joost Bastings, and Stefan Riezler.\n2019. Joey NMT: A minimalist NMT toolkit for\nnovices. To Appear in EMNLP-IJCNLP 2019: Sys-\ntem Demonstrations.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing , pages 66–71,\nBrussels, Belgium.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1412–1421, Lisbon, Portu-\ngal.\nYishu Miao and Phil Blunsom. 2016. Language as\na Latent Variable: Discrete Generative Models for\nSentence Compression. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 319–328, Austin, Texas.\nTomas Mikolov, Stefan Kombrink, Anoop Deoras,\nLukar Burget, and Jan Cernocky. 2011. Rnnlm-\nrecurrent neural network language modeling toolkit.\nIn Proceedings of the ASRU Workshop, pages 196–\n201.\nRafael M ¨uller, Simon Kornblith, and Geoffrey E Hin-\nton. 2019. When does label smoothing help? pages\n4696–4705.\nToan Q. Nguyen and Julian Salazar. 2019. Transform-\ners without tears: Improving the normalization of\nself-attention.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Proceedings of the Advances in Neural\nInformation Processing Systems, pages 8024–8035.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Conference on Ma-\nchine Translation , pages 186–191, Brussels, Bel-\ngium.\nOﬁr Press and Lior Wolf. 2017. Using the output\nembedding to improve language models. In Pro-\nceedings of the Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 157–163, Valencia, Spain.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPrajit Ramachandran, Peter Liu, and Quoc Le. 2017.\nUnsupervised pretraining for sequence to sequence\nlearning. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing ,\npages 383–391, Copenhagen, Denmark.\nShuo Ren, Zhirui Zhang, Shujie Liu, Ming Zhou, and\nShuai Ma. 2019. Unsupervised neural machine\ntranslation with smt as posterior regularization. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 241–248.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics, pages 86–96, Berlin, Germany.\nRico Sennrich and Biao Zhang. 2019. Revisiting low-\nresource neural machine translation: A case study.\nIn Proceedings of the Annual Meeting of the Associ-\nation for Computational Linguistics, pages 211–221,\nFlorence, Italy.\nClaude E Shannon and Warren Weaver. 1949. The\nmathematical theory of communication. Urbana,\n117.\nIvan Skorokhodov, Anton Rykachevskiy, Dmitry\nEmelyanenko, Sergey Slotin, and Anton Ponkratov.\n2018. Semi-supervised neural machine translation\nwith language models. In Proceedings of the AMTA\nWorkshop on Technologies for MT of Low Resource\nLanguages, pages 37–44, Boston, MA. Association\nfor Machine Translation in the Americas.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2018. Cold fusion: Training seq2seq\nmodels together with language models. In Proceed-\nings of Interspeech, pages 387–391.\nFelix Stahlberg, James Cross, and Veselin Stoyanov.\n2018. Simple fusion: Return of the language model.\nIn Proceedings of the Conference on Machine Trans-\nlation, pages 204–211, Belgium, Brussels.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 2818–2826.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the Advances in Neural\nInformation Processing Systems, pages 5998–6008,\nLong Beach, CA, USA.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tie-Yan Liu. 2020. On layer normaliza-\ntion in the transformer architecture.\nJiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi\nZhao, Yong Yu, Weinan Zhang, and Lei Li. 2019.\nTowards making the most of BERT in neural ma-\nchine translation.\nKyra Yee, Yann Dauphin, and Michael Auli. 2019.\nSimple and effective noisy channel modeling for\nneural machine translation. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing and the International Joint Con-\nference on Natural Language Processing , pages\n5700–5705, Hong Kong, China.\nLei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-\nstette, and Tom´as Kocisk´y. 2017. The neural noisy\nchannel.\nLei Yu, Laurent Sartran, Wojciech Stokowiec, Wang\nLing, Lingpeng Kong, Phil Blunsom, and Chris\nDyer. 2019. Putting machine translation in context\nwith the noisy channel model.\nJiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu,\nand Maosong Sun. 2017. Prior knowledge integra-\ntion for neural machine translation using posterior\nregularization. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics,\npages 1514–1523, Vancouver, Canada.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2019. Extreme language model compression\nwith optimal subwords and shared projections.\nChunting Zhou, Jiatao Gu, and Graham Neubig.\n2020. Understanding knowledge distillation in non-\nautoregressive machine translation. In International\nConference on Learning Representations.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tieyan Liu. 2020.\nIncorporating bert into neural machine translation.\nIn International Conference on Learning Represen-\ntations.\nA Appendix\nOur preliminary experiments were conducted with\nrecurrent neural networks (RNN ), because we faced\nstability problems with the Transformer-based TMs.\nWe include those results here for completeness.\nThe experiments were conducted with the 3M\nmonolingual data in all translation directions, there-\nfore they match the top section of the main results\nreported in the paper (Table 4). We observe the\nsame relative performance as with the Transformer-\nbased models, which veriﬁes that the proposed ap-\nproach transfers across architectures. However,\nthe differences are smaller, because the RNN -based\nmodels achieved overall worse BLEU scores and\nperplexities, for the translation and language mod-\neling tasks, respectively.\nModel Conﬁguration We employ the atten-\ntional encoder-decoder (Bahdanau et al., 2015)\narchitecture, using the “global” attention mech-\nanism (Luong et al., 2015). The recurrent cells\nare implemented using Long short-term memory\n(LSTM; Hochreiter and Schmidhuber (1997)) units.\nWe use a bidirectional LSTM encoder and a unidi-\nrectional LSTM decoder. We also tie the embed-\nding and the output (projection) layers of the de-\ncoders (Press and Wolf, 2017; Inan et al., 2017).\nand apply layer normalization (Ba et al., 2016) to\nthe last decoder representation, before the softmax.\nWe did not do any hyperparameter tuning, but\nselected the hyper-parameter values based on Sen-\nnrich and Zhang (2019), while also trying to keep\napproximately the same number of parameters as\ntheir Transformer-based counterparts. Table 5 lists\nall the model hyperparameters. All models were\noptimized with the Adam optimizer (Kingma and\nBa, 2015) with a learning rate of 0.0002 and with\nmini-batches with 2000 tokens per batch.\nparameter value\nTM LM\nEmbedding size (all) 256 512\nEmbedding dropout (all) 0.2 0.2\nEncoder size 512 –\nEncoder layers 2 –\nEncoder dropout 0.2 –\nDecoder size 512 1024\nDecoder layers 2 2\nDecoder dropout 0.2 0.2\nAttention function global –\nTable 5: Hyperparameters of RNN -based TMs and LMs.\nLanguage Models For the LMs we used an iden-\ntical architecture as the decoder of the TM, but\nwith larger size. We also followed the same op-\ntimization process. Table 5 lists all the RNN -LM\nhyperparameters.\nA.1 Language Models\nFor completeness, we include here some additional\ninformation about the training of the LMs. In all ex-\nperiments we paired theTM with LMs with the same\narchitecture, in order to evaluate how the proposed\napproach generalizes across architectures. We train\none LM for each language, on its respective mono-\nlingual corpus. For the Transformer-based LMs\nwe also used a larger corpus for the high resource\nlanguages, as a part of our comparison shown in\nthe main body of the paper. To evaluate the perfor-\nmance of the LMs and to perform early stopping\nwe used a small held-out development set with\n10K sentences. Speciﬁcally, we stopped training\nwhen the perplexity on the development was not\nimproved on for more than 10 epochs. In Table 6\nwe report the perplexities achieved by the LMs on\neach monolingual corpus.\nlanguage model sentences (PPL ↓)\n3M 30M\nEnglish LSTM 37.04 –\nTransformer (big) 29.70 25.02\nGerman LSTM 31.26 –\nTransformer (big) 22.71 19.22\nTurkish LSTM 31.26 –\nTransformer (big) 22.78 –\nTable 6: Perplexity ( PPL ↓) scores for LMs trained\non each language’s monolingual data, computed on a\nsmall held-out validation set per language.\nMethod DE→EN EN →DE TR →EN EN →TR\ndev test dev test dev test dev test\nBase 19.8 ±0.1 24.2±0.2 15.9±0.3 21.7±0.4 13.1±0.1 13.4±0.4 9.9±0.1 9.3±0.1\nShallow-fusion 20.3 ±0.1 24.9±0.3 16.0±0.5 22.1±0.6 13.5±0.2 13.8±0.5 10.2±0.2 9.7±0.1\nPOSTNORM 19.7±0.2 24.0±0.3 15.6±0.1 21.0±0.3 11.9±0.1 12.6±0.3 9.8±0.3 8.8±0.2\nBase + LS 20.6 ±0.1 25.2±0.3 16.2±0.3 22.8±0.2 13.7±0.1 14.4±0.1 10.6±0.1 9.8±0.2\nBase + Prior 20.7±0.3 25.3±0.4 16.5±0.4 23.0±0.7 13.9±0.1 14.5±0.2 10.3±0.2 9.8±0.1\nBase + Prior + LS 20.8±0.2 25.3±0.3 16.9±0.3 23.53±0.2 14.2±0.2 14.8±0.1 10.6±0.2 10.0±0.2\nTable 7: BLEU scores of each RNN -NMT method. Mean and standard deviation of 3 runs reported.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8732264041900635
    },
    {
      "name": "Computer science",
      "score": 0.8028231859207153
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6160905361175537
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5729204416275024
    },
    {
      "name": "Language model",
      "score": 0.5594872236251831
    },
    {
      "name": "Exploit",
      "score": 0.5316759943962097
    },
    {
      "name": "Translation (biology)",
      "score": 0.5210556387901306
    },
    {
      "name": "Inference",
      "score": 0.5162922739982605
    },
    {
      "name": "Artificial neural network",
      "score": 0.51531982421875
    },
    {
      "name": "Machine learning",
      "score": 0.5029582381248474
    },
    {
      "name": "Obstacle",
      "score": 0.4874139130115509
    },
    {
      "name": "Natural language processing",
      "score": 0.42108774185180664
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}