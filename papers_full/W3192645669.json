{
    "title": "Multimodal Transformer Networks for Pedestrian Trajectory Prediction",
    "url": "https://openalex.org/W3192645669",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2167542095",
            "name": "Ziyi Yin",
            "affiliations": [
                "Xi'an Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2131555928",
            "name": "Ruijin Liu",
            "affiliations": [
                "Xi'an Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2940129644",
            "name": "Zhiliang Xiong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2158893243",
            "name": "Zejian Yuan",
            "affiliations": [
                "Xi'an Jiaotong University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970733097",
        "https://openalex.org/W2631888524",
        "https://openalex.org/W2991327061",
        "https://openalex.org/W2769735038",
        "https://openalex.org/W3109597949",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3108653208",
        "https://openalex.org/W2808493349",
        "https://openalex.org/W3034398756",
        "https://openalex.org/W2798754355",
        "https://openalex.org/W3025086266",
        "https://openalex.org/W3109908659",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3045019771",
        "https://openalex.org/W2962839378",
        "https://openalex.org/W2424778531",
        "https://openalex.org/W2991484432",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W3035540643",
        "https://openalex.org/W2561603333",
        "https://openalex.org/W3010214386",
        "https://openalex.org/W3035172263",
        "https://openalex.org/W2968684599",
        "https://openalex.org/W2771583656"
    ],
    "abstract": "We consider the problem of forecasting the future locations of pedestrians in an ego-centric view of a moving vehicle. Current CNNs or RNNs are flawed in capturing the high dynamics of motion between pedestrians and the ego-vehicle, and suffer from the massive parameter usages due to the inefficiency of learning long-term temporal dependencies. To address these issues, we propose an efficient multimodal transformer network that aggregates the trajectory and ego-vehicle speed variations at a coarse granularity and interacts with the optical flow in a fine-grained level to fill the vacancy of highly dynamic motion. Specifically, a coarse-grained fusion stage fuses the information between trajectory and ego-vehicle speed modalities to capture the general temporal consistency. Meanwhile, a fine-grained fusion stage merges the optical flow in the center area and pedestrian area, which compensates the highly dynamic motion of ego-vehicle and target pedestrian. Besides, the whole network is only attention-based that can efficiently model long-term sequences for better capturing the temporal variations. Our multimodal transformer is validated on the PIE and JAAD datasets and achieves state-of-the-art performance with the most light-weight model size. The codes are available at https://github.com/ericyinyzy/MTN_trajectory.",
    "full_text": "Multimodal Transformer Network for Pedestrian Trajectory Prediction\nZiyi Yin1 , Ruijin Liu1 , Zhiliang Xiong2 , Zejian Yuan1\n1Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University, China\n2Shenzhen Forward Innovation Digital Technology Co. Ltd, China\nfyzy19980922, lrj466097290g@stu.xjtu.edu.cn, leslie.xiong@forward-innovation.com,\nyuan.ze.jian@xjtu.edu.cn\nAbstract\nWe consider the problem of forecasting the future\nlocations of pedestrians in an ego-centric view of a\nmoving vehicle. Current CNNs or RNNs are ﬂawed\nin capturing the high dynamics of motion between\npedestrians and the ego-vehicle, and suffer from the\nmassive parameter usages due to the inefﬁciency of\nlearning long-term temporal dependencies. To ad-\ndress these issues, we propose an efﬁcient multi-\nmodal transformer network that aggregates the tra-\njectory and ego-vehicle speed variations at a coarse\ngranularity and that interacts with the optical ﬂow\nin a ﬁne-grained level to ﬁll the vacancy of highly\ndynamic motion. Speciﬁcally, a coarse-grained fu-\nsion stage fuses the information between trajec-\ntory and ego-vehicle speed modalities to capture\nthe general temporal consistency. Meanwhile, a\nﬁne-grained fusion stage merges the optical ﬂow\nin the center area and pedestrian area, which com-\npensates the highly dynamic motion of ego-vehicle\nand target pedestrian. The whole network is only\nattention-based that can efﬁciently model long-\nterm sequences for better capturing the temporal\nvariations. Our multimodal transformer is validated\non the PIE and JAAD datasets and achieves the\nstate-of-the-art performance with the most light-\nweight model size. The codes are available at\nhttps://github.com/ericyinyzy/MTN\ntrajectory.\n1 Introduction\nPedestrian trajectory prediction anticipates the future bound-\ning boxes of pedestrians in an ego-centric view of a mov-\ning vehicle, which is critical for autonomous driving sys-\ntems to avoid possible collisions. It also beneﬁts various\nvisual research ﬁelds such as pedestrians intention estima-\ntion [Schneemann and Heinemann, 2016; Rehderet al., 2018;\nSaleh et al., 2019], video prediction [Wichers et al., 2018;\nOliu et al., 2018; Ye et al., 2019; Wu et al., 2020 ], and\npose forecasting [Mangalam et al., 2020; Adeli et al., 2020;\nCao et al., 2020]. The task requires different visual modal-\nities to capture the highly dynamic motion information be-\ntween pedestrians and ego-vehicle, which is hard to reﬂect in\nthe changes of bounding boxes [Styles et al., 2020]. Addi-\ntionally, how to model the long-term location dependencies\nmore effectively and implement with fewer parameters also\nincreases the challenges.\nExisting approaches have closely studied additional vi-\nsual modalities, which have signiﬁcantly improved the per-\nformance on pedestrian trajectory prediction tasks compared\nto those only trajectory-based methods [Alahi et al., 2016;\nBhattacharyya et al., 2018]. Some methods [Rasouli et al.,\n2019; Malla et al., 2020] utilize image sequence to extract\na semantic prior for guiding the future trajectory, like cross-\ning intention [Rasouli et al., 2019] or predeﬁned action cate-\ngory [Malla et al., 2020]. The semantic priors can provide\ngeneral orientation (e.g. across or along the sidewalk) of\nfuture trajectories whereas it is hard to satisfy the demand\nfor precise locating. Recently, an approach [Makansi et al.,\n2020] exploits scene segmentation to estimate all possible\nend-locations of target pedestrian to predict future trajectory.\nThe performance, however, may degenerate because of the\nlow accuracy of end-locations estimation caused by the lim-\nited perception perspective and the changing scene from the\nego-centric view. A remedy for these drawbacks is to intro-\nduce optical ﬂow to extract motion features for compensating\nthe temporal features in the past trajectory[Styles et al., 2019;\n2020]. Nevertheless, only using optical ﬂow in the bounding\nboxes [Styles et al., 2019; 2020] can not effectively compen-\nsate the motion from the ego-vehicle. It also sustains the in-\nterference from irrelevant motion in the scene.\nApart from that, current RNNs [Rasouli et al., 2019;\nDendorfer et al., 2020] or CNNs [Styles et al., 2019; 2020]\napproaches have been widely applied to relevant tasks and\nhave achieved promising progress. However, CNNs fail to\nmodel the long-term dependencies due to the limited recep-\ntive ﬁeld, and RNNs are usually ﬂawed in extracting local se-\nquence patterns [Wang, 2018] which sometimes contain key\nclues for predicting future. Moreover, in fusion mechanism,\nmost existing networks directly merge the features from dif-\nferent modalities through a simple concatenation. The lack\nof mining characteristics and relations of distinct modalities\nmakes these approaches hardly capture the interaction be-\ntween various granular motion features and produce redun-\ndant parameters that are limited to deploy on vehicle plat-\nforms with less computing resources.\nTo address such limitations, we propose a Multimodal\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1259\nTransformer Network (MTN), which integrates the observed\ntrajectory, ego-vehicle speed and optical ﬂows to predict\nfuture pedestrian trajectory. Owing to the relations be-\ntween observed trajectory of target and ego-vehicle speed se-\nquence, a novel coarse-grained fusion stage ﬁrstly pro-\ncesses the two modalities to produce a hybrid represen-\ntation through a co-attentional mechanism. The inspira-\ntion comes from vision-language tasks [Lu et al., 2019 ].\nNext, a ﬁne-grained fusion stage integrates the hybrid results\nof the former stage with the motion representations of pedes-\ntrians and ego-vehicle. The latter can provide ﬁne-grained\ndynamic motion information and is obtained when we pro-\ncess separated patches of the optical ﬂow in the center area\nand target pedestrians in parallel. This fusion stage can also\navoid interference from the motion of irrelevant objects. Fi-\nnally, MTN outputs future locations of the target in parallel in\none time. The whole network is only attention-based, which\ncan efﬁciently model long-term sequences and better capture\nthe local temporal variations through a coarse-to-ﬁne manner.\nThe effectiveness of our method is evaluated on the two\nlargest datasets with dense pedestrian bounding box annota-\ntions, PIE [Rasouli et al., 2019] and JAAD [Rasouli et al.,\n2017], under the benchmark of [Rasouli et al., 2019]. Exper-\nimental results demonstrate that our method achieves state-\nof-the-art performance with the fewest parameters.\nIn summary, the main contributions of this paper can be\nsummarized as follows:\n1) The introduction of the center area and target pedestrian\noptical ﬂow compensates the highly dynamic motion between\nthe ego-vehicle and pedestrians by dividing them into patches\nand processing in parallel.\n2) The proposed MTN integrates multiple modalities at\ndistinct stages according to their granularity to more effec-\ntively capture of the highly dynamic motion information. In\naddition, the MTN takes advantages of attention-based archi-\ntecture to efﬁciently model long-range temporal dependen-\ncies with much fewer parameters.\n2 Method\nIn this section, we describe the details of our method which\ninclude the optical ﬂow representations, the multimodal\ntransformer architecture, and a warm-up training strategy.\n2.1 Optical Flow Representations\nThe center area and target boxes of optical ﬂows imply ego-\nvehicle and pedestrian motion. Both of them compensate the\nhighly dynamic motion by dividing ﬂows into patches and\napplying a spatial average pooling on them due to the lo-\ncal smoothness. As Fig. 1 shows, for the t-th frame of the\noptical ﬂows, a Region Of Proposal (ROI) \u001et\nego with shape\n(2;Hego;Wego) is cropped at the center. Then, \u001et\nego is split\ninto M patches with equal area, each patch owns the shape of\n(2;bHego\np\nM c;bWegop\nM c) and may contain speciﬁc motion. Next,\nthe i-th patch \u001et;i\nego is operated by a spatial average pooling\nto generate a vector \u001et;i\nego 2R2\u00021. After repeating the above\noperations with a ﬁxed Hego and Wego for each frame of the\noptical ﬂows, M vectors of each frame are concatenated at\nego\n1,2,...,i M/g32\nAvg.\nAvg.\np/g73\ne/g73\n/g94 /g96\n,t j\nped/g73\n   Concat. \n(T\n-1) frames\n/g94 /g96\n,t i\nego/g73\nt\nped/g73\nt\nego/g73\n/g94 /g96\n,t i/g73\n1,2,...,i M/g32\n/g94 /g96\n,t j\nped/g73\n1,2,...,j P/g32\n    Concat. \n(T\n-1) frames\nthe t-th frame optical flow  1,2,...,j P/g32\nFigure 1: Optical ﬂow representations. At each time step, optical\nﬂows from center area and pedestrian area are divided into patches\nand processed in parallel.\nthe ﬁrst dimension and results in the motion representation of\nego-vehicle \u001ee 2R2(T\u00001)\u0002M .\nOptical ﬂow is also exploited to compensate the dynam-\nics of pedestrians, like changing direction rapidly. For\nthe t-th frame of optical ﬂows, a ROI \u001et\nped with shape\n(2;Ht\nped;Wt\nped) is ﬁrstly extracted, where Ht\nped and Wt\nped\nare chosen from the bounding box annotation of the target in\nthe frame t. Next, \u001et\nped is spatially divided into P patchesn\n\u001et;j\nped\no\nj=1;2;:::;P\n, and the motion representation of target\npedestrian \u001ep 2R2(T\u00001)\u0002P is obtained after the same pro-\ncesses like \u001ee. Finally, \u001ee and \u001ep incorporate the ﬁne-grained\ndynamic motion and will be merged by the multimodal trans-\nformer network.\n2.2 Multimodal Transformer Network\nAs is shown in Fig. 2, MTN consists of a coarse-grained fu-\nsion stage and a ﬁne-grained fusion stage. The former stage\nmerges trajectory and speed sequences by a co-attentional\nmechanism. The latter stage fuses the former results and rep-\nresentations from optical ﬂows to estimate the future trajec-\ntories. Following notions are used: Lobs 2RT\u00024 and Sobs 2\nRT\u00021 represent the observed trajectory and the speed se-\nquence, where T is the length of the observation sequence and\nthe 4 dimensions of Lobs are deﬁned by top-left coordinate\nand bottom-right coordinate. \u001ee and \u001ep indicate ﬁne-grained\nmotion representations of ego-vehicle and pedestrians as de-\nscribed in section 2.1.\nCoarse-grained fusion. Ego-vehicle speed is usually\nclosely related to target trajectory. For example, the trajec-\ntory usually changes rapidly when the ego-vehicle is driving\nat a high speed. Due to such property, the coarse-grained fu-\nsion stage combines the observed trajectory with ego-vehicle\nspeed through a co-attentional mechanism and outputs a hy-\nbrid representation which contains the relative motion at a\ncoarse granularity. As is illustrated in the top row of Fig. 2,\nthe coarse-grained fusion stage includes two fully connected\nlayers and three blocks that are linked sequentially. Each\nblock consists of a self-attention module, two cross-attention\nmodules and two feed-forward layers. Giving input trajec-\ntory Lobs and ego-vehicle speed Sobs, the coarse-grained fu-\nsion stage separately sends them into two independent fully\nconnected layers. For Lobs, an initial location representa-\ntion with shape (T;C) is generated by a linear transforma-\ntion and adding the positional embeddings like [Vaswani et\nal., 2017 ] to provide the order of observed locations. For\nSobs, a fully connected layer transforms speed sequence into\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1260\nFFNs\nobsS\nFFNs\n   Self-\nattention\nobsL\npositional \nembedding\npredL\nFine-grained fusion\nMulti-head\n  attention\nmixX\ndX\nCoarse-grained fusion\n3\nAdd &\nNorm\ne/g73\np/g73\neX\npX\n  Cross-\nattention\nMulti-head\n  attention\nMulti-head\n  attention\n3\n  Cross-\nattention FFNsFC\nFC\nFC\nFC\nFC\nTl\nFigure 2: Overall structure of MTN. The MTN is a transformer-based network that consists of two stages to process and interacts modalities\nat different granularity respectively. The \b denotes matrix addition.\na C-dimensional space to capture the overall speed variation\npatterns by producing a vector with shape (1;C). After that,\nthe initial location representation fromLobs is sent into a self-\nattention module to extract the long-term temporal dependen-\ncies. Then two cross-attention modules are utilized to com-\npute cross-correlations between speed and trajectory in a co-\nattentional mechanism. Speciﬁcally, the input of each cross-\nattention module in Fig. 2 is query, key, and value matri-\nces from top to bottom. Co-attentional mechanism computes\nquery matrices from their own modalities whereas calculates\nkey and value matrices from opposite modalities to perform\ncross-attention like [Carion et al., 2020]. Next, two inter-\nmediate representations that contain the trajectory and speed\npatterns are generated through separate feed forward layers.\nAfter transmitting the intermediate representations into the\nremaining blocks, the coarse-grained fusion stage ﬁnally gen-\nerates a coarse-grained motion representationXmix 2RT\u0002C\nby adding up the outputs from the last block.\nFine-grained fusion. Fine-grained fusion stage exchanges\ninformation between the coarse-grained motion representa-\ntion Xmix and ﬁne-grained motion representations of ego-\nvehicle \u001ee and pedestrians \u001ep which compensate for the lack\nof highly dynamic motion. Concretely, the ﬁne-grained fu-\nsion stage contains three blocks and two fully connected lay-\ners. Each block consists of three multi-head attention mod-\nules, an add & norm layer and a feed forward layer. Giv-\ning Xmix, \u001ee and \u001ep as inputs, Xe and Xp are ﬁrstly gen-\nerated by projecting \u001ee and \u001ep into a C-dimensional space\nthrough two independent fully connected layers. The ﬁrst\nblock expects Xmix, Xe, Xp and a trajectory query Xd as\ninputs, where Xd 2RN\u0002C is a sinusoidal embedding since\nthe future locations are ﬁxed chronologically and N is the\nlength of the prediction sequence. Each multi-head attention\nblock takes in charge of interacting Xd with the correspond-\ning representation. Speciﬁcally, the input of each multi-head\nattention module from top to bottom is query, key and value\nmatrices. The attention mechanism mainly extracts the most\ndependent motion information for the query matrix Xd. For\nexample, it can capture ego-vehicle motion more sensitively\nand also suppress interference from other factors such as ir-\nrelevant motion. After that, the output of each multi-head\nattention module are added together with Xd,followed by a\nlayer normalization and a feed-forward layer to generate an\nintermediate representation. Then the intermediate represen-\ntations are delivered into the left blocks with Xe, Xp and\nXmix to proceed as before. Finally, the output of the last\nblock is delivered into a fully connected layer, and added by\nthe last observed location lT to form the trajectory prediction\nresult ^Lpred 2RN\u00024.\n2.3 Training\nAt training stage, we adopt mean squared error (MSE ) loss\nfunction for training our MTN:\nLoss= 1\nN\nNX\nt=1\nk^lT+t \u0000lT+tk2; (1)\nwhere ^lT+t is the t-th location of ^Lpred and lT+t represent\ncorresponding ground truth.\nTo make training converge more stable and faster, we ap-\nply a warm-up training strategy. Speciﬁcally, we remove the\nmodules related to ego-vehicle speed in the coarse-grain fu-\nsion stage and the modules related to optical ﬂow in the ﬁne-\ngrained fusion stage. The remaining components of MTN\nare ﬁrstly pre-trained by only taking Lobs as input for a few\nepochs. Next, MTN is initialized by the pre-trained model\nand completes the training process after speciﬁed epochs.\n3 Experiments\nDatasets. We evaluate MTN on Pedestrian Intention Esti-\nmation (PIE) [Rasouli et al., 2019] and Joint Attention in\nAutonomous Driving (JAAD) [Rasouli et al., 2017] datasets.\nThe PIE consists of 1, 842 pedestrian tracks and 909, 480\nbounding boxes in 37 videos, recorded by a HD (1080\u00021920,\n30 fps) camera from a front-view in Canada during daytime.\nIt also provides dense frame-wise bounding box annotations\nand ego-vehicle information. For a fair comparison, we adopt\nthe same kind of ego-vehicle sensor information e.g. vehicle\nspeed and train/test splits as in [Rasouli et al., 2019]. The\nJAAD includes 2, 856 pedestrian tracks and 82, 032 frames in\n346 video clips. We apply the same train/test split as in [Ra-\nsouli et al., 2019].\nEvaluation metrics. The Mean Squared Error (MSE ) is\nthe commonly used evaluation metric. MSE computes each\ntime step’s average similarity between the predicted bounding\nbox and ground truth. Besides, CMSE and CFMSE are also\nadopted to evaluate similarity over the spatial location and\nlong-term prediction. CMSE represents the MSE between\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1261\nPIE JAAD\nMethod Para. MSE CMSE CFMSE MSE CMSE CFMSE\nB-LSTM - 855 811 3259 1535 1447 5615\nDTP-MOF11.30 665 566 2373 1158 1014 4143\nPIEfull 3.07 559 520 2162 - - -\nPIEtraj 1.24 636 596 2477 1248 1183 4780\nSTED 13.94 461 415 1871 1044 960 4031\nMTNtraj 0.11 581 547 2278 1231 1177 4644\nMTN 0.13 444 414 1627 1005 951 4010\nTable 1: Quantitative comparison on PIE dataset and JAAD\ndatasets. The number of parameters (Para.) is displayed in M\n(million).\nthe center of the predicted bounding box and the ground truth.\nCFMSE is the CMSE at the last time step. All prediction re-\nsults are given in pixels. The parameters of different methods\nalso attend in our comparison to evaluate the deployment po-\ntential.\nImplementation details. Samples of JAAD and PIE are\ngenerated following [Rasouli et al., 2019]. For each sam-\nple, we employed RAFT [Teed and Deng, 2020 ] to extract\noptical ﬂow per frame, and downsample the results by 2\ntimes. The height Hego and width Wego of the center ROI\nare set to be 160 pixels, and the number of patches M and\nP are 64 and 9. Each patch owns the same area. The\nlength of observation sequence T is set to be 15 frames\n(0.5s) and the length of prediction sequence N is 45 frames\n(1.5s). The number of total training epoch is 80, and ten\nepochs are used to warm up parts of the MTN as Sec. 2.3\nstates. The number of batch size is 128 and the Adam op-\ntimizer [Kingma and Ba, 2015 ] is used. All experiments\nare conducted on a single GTX 2080Ti. Since the JAAD\ndataset do not provide odometry information and most sam-\nples have high visibility, we remove the components related\nto the ego-vehicle speed and replace the residual termlT with\nthe locations of linear prediction like [Styles et al., 2019;\n2020]. In the following sections, we take B-LSTM [Bhat-\ntacharyya et al., 2018 ], DTP-MOF [Styles et al., 2019 ],\nPIEfull [Rasouli et al., 2019 ], PIE traj (the baseline ver-\nsion of PIE full which only takes trajectory as input), and\nSTED [Styles et al., 2020] as the comparative state-of-the-\nart methods. For DTP-MOF and STED, the length of input\nand output sequences are changed for a fair comparison. Be-\nsides, the original DTP-MOF only considers the centroid of\nbounding boxes. Thus we change it by training and predict-\ning using bounding boxes. Moreover, we introduce MTNtraj,\na baseline version of MTN which only takes Lobs as input.\nMTNtraj is a simple encoder-decoder structure. The encoder\nis a transformer encoder which contains three blocks. The de-\ncoder is also composed of three blocks and each block con-\nsists of a cross-attention module and a feed-forward layer.\nThen the output of the decoder is sent into a fully connected\nlayer and added by lT to obtain predictions just like MTN.\n3.1 Comparisons with State-of-the-art Methods\nTab. 1 shows the results on PIE and JAAD benchmarks. Com-\npared to state-of-the-art optical ﬂow-based method STED,\nLego Sego Mego Ped MSE C MSE CFMSE\n537 506 2041\nX 477 445 1835\nX 465 433 1771\nX 451 420 1748\nX 453 422 1790\nX X 444 414 1627\nTable 2: Investigation of selecting different areas of optical ﬂow on\nPIE dataset. Lego, Sego and Mego indicate different areas of the\nextracted area. Ped refers to extract from the target pedestrian area.\nMSE\n0\n1000\n2000\n3000\n4000\n5000MSE\n800\n1600\n2400\nMTNSTED traj MTN(Ours) (Ours)DTP-MOF trajPIE\n0 15 30 45\nTime step\nfullPIE\nFigure 3: Visualization of MSE variations with increasing time steps\non JAAD (top) and PIE (bottom) datasets.\nour MTN outperforms it by 17 and 39 MSE on PIE and\nJAAD respectively with only 107 \u0002fewer parameters. With\nthe single trajectory modality, our MTNtraj also outperforms\nthe PIEtraj 55 and 17 MSE with only 11 \u0002fewer parameters.\nAfter introducing optical ﬂows, the MTN further stretches the\nadvantage than PIEfull , which shows the optical ﬂow is bet-\nter than the representation of semantic intention for trajectory\nprediction. Moreover, considering the CMSE and CFMSE\nwhich evaluate the locating and long-term modeling ability,\nour MTN also shows the best performance. Fig. 3 demon-\nstrates detailed comparisons of the MSE with the increasing\ntime-steps. Two pivots are observed: (1) for each dataset, the\nMSE of our method is kept low at all time steps; (2) more im-\nportantly, our method greatly outperforms other approaches\nin terms of the accuracy of long-term prediction. The visual-\nization of trajectory prediction results is shown in Fig. 4. Our\nmethod generates more reasonable predictions under various\nsituations, especially for the dynamic motion of ego-vehicle\nand pedestrians. This is attributed to (1) optical ﬂow provides\nmotion of pedestrians and vehicles in a ﬁne-grained level,\nwhich compensates for the absence highly dynamic informa-\ntion more effectively; (2) the attention mechanism can better\ncapture temporal relations of the sequence in a local-global\nmanner, as we are going to discuss in details in the ablation\nexperiments.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1262\nDTP-MOF PIEfull STED MTN (Ours) Groundtruth   Past\nFigure 4: Qualitative results on PIE (top) and JAAD (bottom) datasets. Each white bounding box illustrates the target location of the ﬁrst\nframe, and each white line shows the observed trajectory. Other colored boxes represent the ﬁnal predicted location and colored lines\ndemonstrate the prediction trajectories of different methods. Images are cropped for better visibility.\n(a)\n(b)\n(c)\nFigure 5: Effect of introducing optical ﬂows of ego-vehicle. (a)\nshows the last observed frame, and the grey, white and green boxes\nshow the interested region, current location and the ﬁnal predicted\nlocation of target pedestrian. (b) illustrates the extracted optical\nﬂows from the interested region. (c) shows the attention map learns\nto block out irrelevant motions and compensate motion caused by\nego-vehicle.\n3.2 Ablation Study\nIn this section, we ﬁrst evaluate the effect of optical ﬂow from\ndisparate regions and different representing models. Then the\nimpact of merging methods in the coarse-grained fusion stage\nis discussed. After that, we analyze how coarse-grained mo-\ntion information guides trajectory prediction from intermedi-\nate attention maps. Finally, we explore the model complexity\nfrom two aspects: (1) number of blocks; (2) selection of em-\nbedding size C, and show some failure cases.\nSelected area of optical ﬂows. Tab. 2 shows the beneﬁts\nfrom different optical ﬂow areas. The use of pedestrian opti-\ncal ﬂow obtain 84 MSE reduction. Also, the optical ﬂow of\nego-vehicle reduces MSE a lot. To evaluate the impact of dif-\nferent height Hego and width Wego of the center ROI, we also\nset three different sizes (large Lego = (260\u0002260), medium\nMego = (160\u0002160), small Sego = (60\u000260)) and the ﬁ-\nnal results show with medium area, the best MSE reduction\nachieves 86. Fig. 5 visualizes the effect of the ﬁne-grained\nmotion representation \u001ee. Fig. 5(a) and (b) show the selected\narea which is ﬁxed to the lower location of image center\nMethod MSE C MSE CFMSE\nConcatenation 567 536 2161\nAddition 556 525 2137\nCo-attention 537 506 2041\nTable 3: Evaluation of different trajectory-speed fusing methods on\nPIE dataset. Components related to optical ﬂows are removed for\nmore rigor.\narea and the captured optical ﬂows of ego-vehicle, respec-\ntively, which contains the ego-vehicle motion and movement\nof other objects in the scene, e.g. the white van at bottom-\nleft. Fig. 5(c) shows the attention map (the darker the map,\nthe lower the attention) ignores the irrelevant movement of\nthe other vehicles to compensate for the real motion caused\nby ego-vehicle.\nInvestigation of optical ﬂow representations. This part\nexplores the different models to represent the ﬁne-grained\nmotion information. A common approach [Styles et al.,\n2019] is to exploit a CNN to extract motion features from\nstacked optical ﬂows. Here we use the Resnet-18 to process\nthem and generate the ﬁne-grained motion representations.\nFollowing DTP [Styles et al., 2020], Resnet-18 is ﬁrstly pre-\ntrained to learn a compensation term of constant velocity as-\nsumption , and the parameters of the pre-trained network is\nﬁxed when training MTN. Test results on PIE dataset of CNN\nstructure are 460 MSE, 429 CMSE and 1851 CFMSE at the\ncost of 11.41Mparameters. Compared to the best applied re-\nsults using our method in Tab. 2, CNN produces more param-\neters without any performance improvement, which appears\nthat a fully connected layer is sufﬁcient to extract the motion\nfeatures from optical ﬂows.\nFusion methods in the coarse-grained fusion stage. We\ndiscuss the performance of different fusion methods between\nego-vehicle speed and target trajectory in the coarse-grained\nfusion stage. In this part, components related to the opti-\ncal ﬂow representations are not applied. (1) Concatenation.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1263\nBlocks Para. MSE C MSE CFMSE\n1 0.05 500 470 1823\n3 0.13 444 414 1627\n5 0.22 474 448 1738\nTable 4: Investigation of the number of blocks in MTN. The\nnumber of parameters (Para.) is displayed in M (million). We set\nthe embedding size C to 32. The coarse-grained fusion stage and\nthe ﬁne-grained fusion stage contain the same number of blocks.\n5\n0 10 20 30 40\n \n10 0 20 30 40\n10\n0\n10\n5\n0\nPrediction    \nObservation\nFigure 6: Attention maps between coarse-grained motion represen-\ntations and the predicted trajectory query in the ﬁrst (top) and third\n(bottom) blocks. The left and right columns visualize the maps from\nthe ﬁrst and fourth attention heads.\nPast trajectory Lobs is concatenated to ego-vehicle speedSobs\nafter a fully connected layer and a 3-layers transformer en-\ncoder, which forms the coarse-grained motion representation\nXmix 2RT\u0002(C+1). There is another fully connected layer\nin the ﬁne-grained fusion stage to project Xmix into a C-\ndimensional space. Such process is similar to [Rasouli et al.,\n2019], except we replace future ego-vehicle speed with the\nobserved speed. (2) Addition. Ego-vehicle speed sequence\nSobs is embedded by a fully-connected layer and then added\nwith the output of the three transformer encoder blocks which\nprocess Lobs. As is shown in Tab. 3, compared with simple\nconcatenation, passing the vehicle speed through a fully con-\nnected layer ameliorates the performance slightly by 11 MSE.\nOur co-attention method further improves the MSE by 19.\nAttention maps between motion representation Xmix and\ntrajectory query Xd. Fig. 6 visualizes attention maps to\nshow how fusion mechanism works. The ﬁrst block (top row)\nmainly focuses on the local relation between Xmix and Xd.\nIn detail, the ﬁrst and fourth heads separately concentrate on\nthe recent and long ago observation. After subsequent blocks,\nthe last block tends to capture global temporal context to sup-\nplement completeness of pedestrian motion by aggregating\nthe whole-time observation.\nNumber of fusion blocks. To investigate the inﬂuence of\nmodel complexity, we change the number of blocks in the\ndistinct fusion stages. As Tab. 4 shows, MTN with only one\nblock owns a high prediction error with 0.05M parameters.\nWhen the number of blocks increases to 3, the prediction er-\nror reduces by 56 at the cost of an increase of 0.08M pa-\nrameters. However, the addition of another two blocks raises\nthe MSE by 30, which is caused by the imbalance between\nthe expressive relations between different modalities and the\nrepresentation capacity of a deeper network.\nEmbedding size Para. MSE C MSE CFMSE\n16 0.03 532 500 2024\n32 0.13 444 414 1627\n64 0.41 439 411 1688\nTable 5: Investigation of different embedding sizes. The number of\nparameters (Para.) is displayed in M (million). The number of\nattention heads and dimensionality of inner layers in FFNs are ﬁxed\nto 4 and 4 \u0002 embedding size.\nFigure 7: Failure cases. The failure forecasts are often caused by\nrandomness of 2D trajectory mutations during prediction period.\nThe same color coding (see Fig. 4) is used.\nEmbedding size C. We also explore the impact of embed-\nding size C. As Tab. 5 illustrates, the improvement of predic-\ntion performance is signiﬁcant (88 MSE reduction) when em-\nbedding size Cis raised from 16 to 32, but larger embedding\nsize 64 does not bring more meaningful beneﬁts. To obtain\nthe best trade-off between prediction error and computational\nresource consumption, we set the embedding size Cas 32.\nFailure cases. Failure cases are shown in Fig. 7, due to ran-\ndomness of 2D trajectory mutations during prediction period.\nFor example, ego-vehicle is braking in the left situation, or\nthe pedestrian is changing his direction in the right case.\n4 Conclusion\nIn this work, we have developed a multimodal transformer\nnetwork to predict pedestrian trajectory by introducing op-\ntical ﬂows to compensate highly dynamic motion between\nego-vehicle and pedestrians. The whole architecture is only-\nattention-based and consists of two specially stages to pro-\ncess and merge coarse-grained and ﬁne-grained modalities.\nThe coarse-grained fusion stage models the temporal similar-\nity between vehicle speeds and pedestrian trajectory to aggre-\ngate a coarse-grained motion representation. The ﬁne-grained\nfusion stage interacts the ﬁne-grained motion representations,\nwhich are extracted from the observed ego-vehicle and pedes-\ntrian optical ﬂows, with the former coarse features to compen-\nsate the highly dynamic motion. This architecture takes the\nadvantage of attention mechanism to model the long-range\ndependencies more efﬁciently than the common convolution\nand recurrent operations, thus achieving a considerable re-\nduction of overall prediction error. In future work, it would\nbe interesting to employ the semantic understanding of traf-\nﬁc scene to further improve the performance by considering\nmore complex interactions with other objects.\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (62088102, 61976170, 91648121).\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1264\nReferences\n[Adeli et al., 2020] Vida Adeli, Ehsan Adeli, Ian Reid,\nJuan Carlos Niebles, and Hamid Rezatoﬁghi. Socially and\ncontextually aware human motion and pose forecasting.\nIEEE Robotics Autom. Lett., 5(4):6033–6040, 2020.\n[Alahi et al., 2016] Alexandre Alahi, Kratarth Goel, Vignesh\nRamanathan, Alexandre Robicquet, Fei-Fei Li, and Silvio\nSavarese. Social LSTM: human trajectory prediction in\ncrowded spaces. In CVPR, pages 961–971, 2016.\n[Bhattacharyya et al., 2018] Apratim Bhattacharyya, Mario\nFritz, and Bernt Schiele. Long-term on-board prediction\nof people in trafﬁc scenes under uncertainty. In CVPR,\npages 4194–4202, 2018.\n[Cao et al., 2020] Zhe Cao, Hang Gao, Karttikeya Man-\ngalam, Qi-Zhi Cai, Minh V o, and Jitendra Malik. Long-\nterm human motion prediction with scene context. In\nECCV (1), volume 12346, pages 387–404, 2020.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection\nwith transformers. In ECCV (1), volume 12346, pages\n213–229, 2020.\n[Dendorfer et al., 2020] Patrick Dendorfer, Aljosa Osep, and\nLaura Leal-Taix ´e. Goal-gan: Multimodal trajectory\nprediction based on goal position estimation. CoRR,\nabs/2010.01114, 2020.\n[Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In ICLR\n(Poster), 2015.\n[Lu et al., 2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and\nStefan Lee. Vilbert: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. In\nNeurIPS, pages 13–23, 2019.\n[Makansi et al., 2020] Osama Makansi, ¨Ozg¨un C ¸ ic ¸ek, Kevin\nBuchicchio, and Thomas Brox. Multimodal future local-\nization and emergence prediction for objects in egocentric\nview with a reachability prior. InCVPR, pages 4353–4362,\n2020.\n[Malla et al., 2020] Srikanth Malla, Behzad Dariush, and\nChiho Choi. TITAN: future forecast using action priors.\nIn CVPR, pages 11183–11193, 2020.\n[Mangalam et al., 2020] Karttikeya Mangalam, Ehsan\nAdeli, Kuan-Hui Lee, Adrien Gaidon, and Juan Carlos\nNiebles. Disentangling human dynamics for pedestrian\nlocomotion forecasting with noisy supervision. In WACV,\npages 2773–2782, 2020.\n[Oliu et al., 2018] Marc Oliu, Javier Selva, and Sergio Es-\ncalera. Folded recurrent neural networks for future video\nprediction. In ECCV (14), volume 11218, pages 745–761,\n2018.\n[Rasouli et al., 2017] Amir Rasouli, Iuliia Kotseruba, and\nJohn K. Tsotsos. Are they going to cross? A benchmark\ndataset and baseline for pedestrian crosswalk behavior. In\nICCV Workshops, pages 206–213, 2017.\n[Rasouli et al., 2019] Amir Rasouli, Iuliia Kotseruba, Toni\nKunic, and John K. Tsotsos. PIE: A large-scale dataset\nand models for pedestrian intention estimation and trajec-\ntory prediction. In ICCV, pages 6261–6270, 2019.\n[Rehder et al., 2018] Eike Rehder, Florian Wirth, Martin\nLauer, and Christoph Stiller. Pedestrian prediction by\nplanning using deep neural networks. In ICRA, pages 1–5,\n2018.\n[Saleh et al., 2019] Khaled Saleh, Mohammed Hossny, and\nSaeid Nahavandi. Real-time intent prediction of pedestri-\nans for autonomous ground vehicles via spatio-temporal\ndensenet. In ICRA, pages 9704–9710, 2019.\n[Schneemann and Heinemann, 2016] Friederike Schnee-\nmann and Patrick Heinemann. Context-based detection of\npedestrian crossing intention for autonomous driving in\nurban environments. In IROS, pages 2243–2248, 2016.\n[Styles et al., 2019] Olly Styles, Arun Ross, and Victor\nSanchez. Forecasting pedestrian trajectory with machine-\nannotated training data. In IV, pages 716–721, 2019.\n[Styles et al., 2020] Olly Styles, Tanaya Guha, and Victor\nSanchez. Multiple object forecasting: Predicting future\nobject locations in diverse environments. In WACV, pages\n679–688, 2020.\n[Teed and Deng, 2020] Zachary Teed and Jia Deng. RAFT:\nrecurrent all-pairs ﬁeld transforms for optical ﬂow. In\nECCV (2), volume 12347, pages 402–419, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998–6008, 2017.\n[Wang, 2018] Baoxin Wang. Disconnected recurrent neural\nnetworks for text categorization. In ACL (1), pages 2311–\n2320, 2018.\n[Wichers et al., 2018] Nevan Wichers, Ruben Villegas, Du-\nmitru Erhan, and Honglak Lee. Hierarchical long-term\nvideo prediction without supervision. In ICML, vol-\nume 80, pages 6033–6041, 2018.\n[Wu et al., 2020] Yue Wu, Rongrong Gao, Jaesik Park, and\nQifeng Chen. Future video synthesis with object motion\nprediction. In CVPR, pages 5538–5547, 2020.\n[Ye et al., 2019] Yufei Ye, Maneesh Singh, Abhinav Gupta,\nand Shubham Tulsiani. Compositional video prediction.\nIn ICCV, pages 10352–10361, 2019.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n1265"
}