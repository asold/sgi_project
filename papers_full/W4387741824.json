{
  "title": "In the Eye of Transformer: Global–Local Correlation for Egocentric Gaze Estimation and Beyond",
  "url": "https://openalex.org/W4387741824",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2806505668",
      "name": "Bolin Lai",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2116419071",
      "name": "Miao Liu",
      "affiliations": [
        "Menlo School",
        "Georgia Institute of Technology",
        "Alpha Omega Alpha Medical Honor Society"
      ]
    },
    {
      "id": "https://openalex.org/A2115497719",
      "name": "Fiona Ryan",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2211579681",
      "name": "James M. Rehg",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2806505668",
      "name": "Bolin Lai",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2116419071",
      "name": "Miao Liu",
      "affiliations": [
        "Alpha Omega Alpha Medical Honor Society",
        "Menlo School",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2115497719",
      "name": "Fiona Ryan",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2211579681",
      "name": "James M. Rehg",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2997592281",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W3202477427",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2979442772",
    "https://openalex.org/W3202742610",
    "https://openalex.org/W3189307944",
    "https://openalex.org/W4312815172",
    "https://openalex.org/W2883284130",
    "https://openalex.org/W3035735638",
    "https://openalex.org/W3203003533",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W6802913412",
    "https://openalex.org/W4313118515",
    "https://openalex.org/W2135957164",
    "https://openalex.org/W2147299944",
    "https://openalex.org/W3043142510",
    "https://openalex.org/W2795307598",
    "https://openalex.org/W3015458609",
    "https://openalex.org/W4292794051",
    "https://openalex.org/W3005401190",
    "https://openalex.org/W4312928145",
    "https://openalex.org/W4312973165",
    "https://openalex.org/W2986102820",
    "https://openalex.org/W3176436964",
    "https://openalex.org/W2468114283",
    "https://openalex.org/W3022565501",
    "https://openalex.org/W2964114039",
    "https://openalex.org/W4387741824",
    "https://openalex.org/W4312977443",
    "https://openalex.org/W2136668269",
    "https://openalex.org/W6778443936",
    "https://openalex.org/W2895299763",
    "https://openalex.org/W4312769131",
    "https://openalex.org/W4281489207",
    "https://openalex.org/W4312878367",
    "https://openalex.org/W3109667662",
    "https://openalex.org/W2963685207",
    "https://openalex.org/W4321021980",
    "https://openalex.org/W4214561053",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W4285176798",
    "https://openalex.org/W2810712262",
    "https://openalex.org/W3121143616",
    "https://openalex.org/W3210279979",
    "https://openalex.org/W4312461822",
    "https://openalex.org/W3172908893",
    "https://openalex.org/W4313160444",
    "https://openalex.org/W1912797782",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W3163079050",
    "https://openalex.org/W2900293887",
    "https://openalex.org/W3207447090",
    "https://openalex.org/W3034287518",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W4312356418",
    "https://openalex.org/W1947031653",
    "https://openalex.org/W2757028014",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2997004687",
    "https://openalex.org/W3194945615",
    "https://openalex.org/W2997304642",
    "https://openalex.org/W2980565715",
    "https://openalex.org/W6808229666",
    "https://openalex.org/W2062000228",
    "https://openalex.org/W4307823382",
    "https://openalex.org/W2893375469",
    "https://openalex.org/W2741156154",
    "https://openalex.org/W4312785900",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3122006940",
    "https://openalex.org/W3105966669",
    "https://openalex.org/W3123364653",
    "https://openalex.org/W3101262641",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W250309892"
  ],
  "abstract": "Abstract Predicting human’s gaze from egocentric videos serves as a critical role for human intention understanding in daily activities. In this paper, we present the first transformer-based model to address the challenging problem of egocentric gaze estimation. We observe that the connection between the global scene context and local visual information is vital for localizing the gaze fixation from egocentric video frames. To this end, we design the transformer encoder to embed the global context as one additional visual token and further propose a novel global–local correlation module to explicitly model the correlation of the global token and each local token. We validate our model on two egocentric video datasets – EGTEA Gaze + and Ego4D. Our detailed ablation studies demonstrate the benefits of our method. In addition, our approach exceeds the previous state-of-the-art model by a large margin. We also apply our model to a novel gaze saccade/fixation prediction task and the traditional action recognition problem. The consistent gains suggest the strong generalization capability of our model. We also provide additional visualizations to support our claim that global–local correlation serves a key representation for predicting gaze fixation from egocentric videos. More details can be found in our website ( https://bolinlai.github.io/GLC-EgoGazeEst ).",
  "full_text": "International Journal of Computer Vision (2024) 132:854–871\nhttps://doi.org/10.1007/s11263-023-01879-7\nIn the Eye of Transformer: Global–Local Correlation for Egocentric\nGaze Estimation and Beyond\nBolin Lai 1 · Miao Liu 1,2 · Fiona Ryan 1 · James M. Rehg 1\nReceived: 1 April 2023 / Accepted: 10 August 2023 / Published online: 18 October 2023\n© The Author(s) 2023\nAbstract\nPredicting human’s gaze from egocentric videos serves as a critical role for human intention understanding in daily activities.\nIn this paper, we present the ﬁrst transformer-based model to address the challenging problem of egocentric gaze estimation.\nWe observe that the connection between the global scene context and local visual information is vital for localizing the gaze\nﬁxation from egocentric video frames. To this end, we design the transformer encoder to embed the global context as one\nadditional visual token and further propose a novel global–local correlation module to explicitly model the correlation of the\nglobal token and each local token. We validate our model on two egocentric video datasets – EGTEA Gaze + and Ego4D.\nOur detailed ablation studies demonstrate the beneﬁts of our method. In addition, our approach exceeds the previous state-of-\nthe-art model by a large margin. We also apply our model to a novel gaze saccade/ﬁxation prediction task and the traditional\naction recognition problem. The consistent gains suggest the strong generalization capability of our model. We also provide\nadditional visualizations to support our claim that global–local correlation serves a key representation for predicting gaze\nﬁxation from egocentric videos. More details can be found in our website ( https://bolinlai.github.io/GLC-EgoGazeEst).\nKeywords Egocentric gaze estimation · Vision transformer · Global–local correlation\n1 Introduction\nRecent ﬁndings in cognitive science have validated the\ncapability of eye movements in reﬂecting the cognitive pro-\ncesses of human (Yarbus, 2013), which are essential for\nunderstanding human intention, modeling interactions across\na group of people as well as reasoning daily activities in var-\nious scenarios (Hayhoe & Ballard, 2005). Recently, more\nattention has been paid to egocentric gaze behavior model-\ning (Huang et al., 2018;L ie ta l . , 2021; Huang et al., 2020;\nCommunicated by Guang Yang.\nB James M. Rehg\nrehg@gatech.edu\nBolin Lai\nbolin.lai@gatech.edu\nMiao Liu\nmiaoliu@meta.com\nFiona Ryan\nfkryan@gatech.edu\n1 Georgia Institute of Technology, Atlanta, GA 30308, USA\n2 Meta AI, Menlo Park, CA 94025, USA\nLiu et al., 2020; Thakur et al., 2021; Zhang et al., 2018).\nSuch an understanding of visual attention and intention from\nthe ﬁrst-person perspective can be valuable for many appli-\ncations, including Augmented Reality (AR), Virtual Reality\n(VR), and Human-Robot Interaction (HRI). However, how to\nmeasure human’s gaze remains a key challenge in this ﬁeld.\nWhile wearable eye trackers are a standard way to obtain\nmeasurements of gaze behavior, they require calibration,\nconsume signiﬁcant power, and add substantial cost and com-\nplexity to wearable platforms. Alternatively, prior works (Li\net al., 2013, 2021; Huang et al., 2018; Soo Park & Shi, 2015;\nHuang et al., 2020; Tavakoli et al., 2019; Thakur et al., 2021;\nAl-Naser et al., 2019; Huang et al., 2020; Naas et al., 2020)\nseek to estimate the visual attention of the camera wearer\nfrom videos captured from a ﬁrst-person perspective. In this\npaper, we address this challenging task of egocentric gaze\nestimation. Moreover, we introduce a novel task of predict-\ning whether there is gaze saccade within the given egocentric\nvideos. This novel task serves as a key step for understanding\nhuman gaze variation and may promise more power-efﬁcient\nAR user experience. The problem setting of egocentric gaze\nestimation and egocentric gaze saccade/ﬁxation prediction\nare introduced in Fig. 1.\n123\nInternational Journal of Computer Vision (2024) 132:854–871 855\nFig. 1 Problem settings of egocentric gaze estimation and gaze sac-\ncade/ﬁxation prediction. Given a sequence of video frames, the goal\nof gaze estimation is to predict where the camera wearer is looking at\nin each frame. The green dots represent the gaze ground truth (from\na wearable eye tracker). In terms of gaze saccade/ﬁxation prediction,\nthe goal is to predict whether a saccade happens within the given input\nvideo. In the showing example, frames with blue edges suggest large\nmovements (saccade), while frames with orange edges suggest gaze\nﬁxation or subtle gaze movements\nThe key challenge in modeling gaze behavior from ego-\ncentric videos is to effectively integrate multiple gaze cues\ninto a holistic analysis of visual attention. Cues include the\nlikelihood that different scene objects are gaze targets (i.e.\nsalience), the relative location of gaze targets within the video\nframe (i.e. center prior), and the patterns of camera move-\nment that are reﬂective of visual attention (i.e. head motions\naccompanying a gaze shift). Prior works on visual saliency\nprediction propose to use two-stream networks (Wang ,\n2015), dilated convolutional layers (Yang et al., 2019)o r\npyramid architectures (Hussain et al., 2022) to enlarge the\nreceptive ﬁeld, yet incorporating a global representation of\nthe input is still missing from the model designs. Recently,\nthe transformer architecture has achieved great success in\nvarious vision tasks by modeling the spatio-temporal corre-\nlation among local visual tokens (Strudel et al., 2021; Lou et\nal., 2021; Liu et al., 2021; Fang et al., 2021; Ren et al., 2022;\nLee et al., 2022; Patrick et al., 2021; Ma et al., 2022; Li et al.,\n2022). Vision transformer shows the potential to effectively\ncapture the global representation, since its receptive ﬁeld can\ncover the entire input space. However, the pairwise compar-\nisons performed by standard Self-Attention (SA) mechanism\nis not optimized for interpreting local video features in the\ncontext of the global scene. Figure 2 presents the key role\nof comparisons between local patches and global context -\nthe gaze target is a salient object pointed at by both the cam-\nera wearer and another person. Such a salient object can not\nbe easily localized by only modeling the correlation of local\npatches.\nTo this end, our paper introduces a novel transformer-\nbased deep model that explicitly embeds global context and\ncalculates spatio-temporal global–local correlation for ego-\ncentric gaze estimation. Speciﬁcally, we design a transformer\nencoder that adopts a global visual token embedding strat-\negy to incorporate the global scene context. The single global\nvisual token is handled together with all local visual tokens by\nstandard self-attention layers in the encoder. We then come up\nwith a novel Global–Local Correlation (GLC) module that\nFig. 2 Example of local correlation and global–local correlation for the\ntask of egocentric gaze estimation. The green dot represents the gaze\nground truth (from a wearable eye tracker) and the image patch that\ncontains the gaze target has red edges. The heatmap overlaied on the\nvideo frame demonstrates the prediction result from our model. Global–\nlocal correlation models the connections between the global context and\neach local patch, making it possible to understand the scene in a holistic\nperspective, e.g., the camera wearer and social partner are pointing at\nthe salient object. In contrast, local-local correlations may not yield an\neffective representation of the scene context\n123\n856 International Journal of Computer Vision (2024) 132:854–871\nhighlights the connection between global and local visual\ntokens by masking out all correlations across local tokens.\nFinally, we adopt a transformer-based decoder to produce\ngaze prediction output. To the best of our knowledge, this\nis the ﬁrst work applying vision transformer to egocentric\ngaze estimation. As shown in the heatmap from Fig. 2, our\nmodel understands the scene in a holistic view and success-\nfully captures the gaze target with the proposed global context\nembedding and global–local correlation module.\nWe exhaustively evaluate our approach on two egocen-\ntric video datasets – EGTEA Gaze+ (Li et al., 2018) and\nEgo4D (Grauman et al., 2022). To begin with, we investi-\ngate different strategies for global context embedding and\nshow the contribution of involving the global token and the\nglobal–local correlation module. We then compare with all\nprior works of egocentric gaze estimation on the two datasets.\nWe also apply our model to the novel saccade/ﬁxation\nprediction problem, and traditional action recognition prob-\nlem to demonstrate the generalization capability of our\nmodel. Our proposed model is easy to incorporate into exist-\ning transformer-based video analysis architectures, so we\nimplement all experiments with two different transformer\nbackbones. Our method improves the performance of both\nbackbones and ﬁnally yields an improvement of +3.9% in\nF1 score over the previous state-of-the-art method for ego-\ncentric gaze estimation. It also boosts the performance on\negocentric action recognition and gaze saccade/ﬁxation pre-\ndiction tasks by a notbale margin. The codes and pretrained\nmodels are publicly available to the research community. In\nsummary, this work makes the following contributions:\n• We introduce the ﬁrst transformer-based approach to\naddress the challenging task of egocentric gaze estima-\ntion, and introduce a novel task of gaze saccade/ﬁxation\nprediction from egocentric video.\n• We utilize a global visual token embedding strategy to\nincorporate global visual context into self-attention, and\nfurther introduce a novel Global–Local Correlation mod-\nule to explicitly model the correlation between global\ncontext and each local visual token.\n• Our novel design obtains consistent improvement on the\nEGTEA Gaze+ (Li et al., 2018) and Ego4D (Grauman\net al., 2022) datasets and outperforms the previous state-\nof-the-art method by +3.9% on EGTEA and +5.6% on\nEgo4D in terms of F1 score. Importantly, this is the ﬁrst\nwork that uses the Ego4D dataset for egocentric gaze esti-\nmation, which serves as important benchmark for future\nresearch in this direction.\n• We provide more insights of our model by applying\nit to saccade/ﬁxation prediction and egocentric action\nrecognition tasks. We also visualize correlation weights\nto show more evidence of the global–local correlation\nmechanism.\nAn early version of this paper (Lai et al., 2022)w a s\naccepted by BMVC 2022 and then invited this special issue.\nThis paper further extends our previous conference version\nin several important aspects. First, we introduce a novel\ntask of recognizing saccade gaze movements from the ego-\ncentric videos. Second, we conduct additional experiments\nusing recent MotionFormer backbone (Patrick et al., 2021).\nOur new results suggest that GLC module can be easily\nplugged into other transformer-based backbones and can\nproduce consistent performance gain on egocentric gaze esti-\nmation. Third, we show our model design can also beneﬁt\nsaccade/ﬁxation prediction performance, suggesting global\ncontext also contributes to the understanding of rapid gaze\nmovements. Finally, we provide more visualizations of gaze\nestimation results and correlation weights in GLC module.\nThis paper is organized in the following order. Section 2\nreviews all related works about egocentric gaze estimation,\nvision transformer and visual saliency modeling, and high-\nlight the difference between prior works and this paper.\nSection 3 elaborates details of the proposed model. Section 4\npresents implementation details for each experiment and the\nexperimental results. Section 5 lists the current limitation of\nour model and promising future works. Section 6 summarizes\nall ﬁndings of this paper.\n2 Related Work\nThe computational analysis of human gaze behavior is a long-\nestablished topic. For example, earlier works consider the\nproblem of eye tracking (Krafka et al., 2016; MacInnes et\nal., 2018; Ye et al., 2012), which addresses the problem of\ntracking the gaze movement based on closeup view of human\nfaces or eyes. Moreover, another topic on gaze target pre-\ndiction(Chong et al., 2020, 2018; Kellnhofer et al., 2019;\nNonaka et al., 2022) aims at predicting the gaze target of a\nsubject from the third-person view. In contrast to these prior\nworks, we address the problem of predicting the gaze target\ndirectly from egocentric videos captured by wearable cam-\neras. In this paper, we mainly discuss the most relevant prior\nworks on egocentric gaze estimation and related works on\ntransformer-based video representation learning and video\nsaliency prediction.\n2.1 Egocentric Gaze Estimation\nPrevious works focuses on analyzing human daily activities\nfrom egocentric videos (Li et al., 2013, 2021; Huang et al.,\n2018; Liu et al., 2020; Soo Park & Shi, 2015; Huang et al.,\n123\nInternational Journal of Computer Vision (2024) 132:854–871 857\n2020; Tavakoli et al., 2019; Zhang et al., 2017; Thakur et al.,\n2021; Al-Naser et al., 2019; Huang et al., 2020; Naas et al.,\n2020; Jia et al., 2022; Liu et al., 2022). Here, we discuss the\nmost relevant works that develop deep models for egocentric\ngaze estimation. Zhang et al. ( 2017) used deep models and\nan adversarial network to forecast egocentric gaze location\nin future video frames, which can also be applied to estimate\ngaze target in current frames by replacing the labels. They\nfurther improve this model by adding another branch to incor-\nporate prior information (Zhang et al., 2018). Huang et al.\n(2018) proposed to explicitly model the temporal attention\ntransition using a LSTM-based architecture and incorporate\nit into saliency-based models for gaze estimation. Tavakoli\net al. ( 2019) investigated the impact of various factors on\negocentric gaze estimation and provided guidance for future\nwork. Another research ﬁeld is to leverage the relation of\nhuman’s action and gaze behavior and model them jointly. Li\net al. ( 2018) sampled a gaze distribution map from the lower\nlayer and used it to selectively pool the features learned by\nthe higher layer. Inspired by this work, Huang et al. ( 2020)\nintroduced a multi-stream network to enable gaze and action\nto serve as contexts for each other.\nIn addition, there exit many works about the variants of\negocentric gaze estimation which expand its applications in\nvarious scenarios. Soo Park and Shi ( 2015) introduced the\nnovel problem of predicting joint attention during social\ninteraction using egocentric videos. Huang et al. ( 2020)\ncollected a new egocentric video dataset and developed a\ngraphical model to detect joint attention. Thakur et al. ( 2021)\nproposed a multi-modal network that uses both video and\ninertial measurement unit data for more accurate egocentric\ngaze estimation. Naas et al. ( 2020) developed a tiling scheme\nfor gaze prediction which enables a more efﬁcient VR con-\ntent delivery.\nAll of these prior works did not embed global context\nexplicitly or model the connection between local and global\nvisual representations as in our model, which could limit\nthe capability of their models. Additionally, we are the ﬁrst\nto develop a transformer-based architecture to address the\nproblem of egocentric gaze estimation.\n2.2 Vision Transformer\nTransformer architecture is ﬁrst proposed by Vaswani et al.\n(2017) and inspires many large language models (Devlin et\nal., 2018; Liu et al., 2019; Brown et al., 2020). Recently,\nvision transformers (Dosovitskiy et al., 2022) have demon-\nstrated superior performance on image classiﬁcation (Dai\net al., 2021; Liu et al., 2021; Wang et al., 2021; Ren et al.,\n2022; Yang et al., 2021; Lee et al., 2022), detection (Dai et\nal., 2021; Carion et al., 2020; Dai et al., 2021; Fang et al.,\n2021), segmentation (Strudel et al., 2021; Wang et al., 2021;\nZheng et al., 2021; Cheng et al., 2022; Zhang et al., 2022),\nsaliency prediction (Ma et al., 2022; Lou et al., 2021;L i ue t\nal., 2021) and video analysis (Arnab et al., 2021;N e i m a r k\net al., 2021; Patrick et al., 2021; Fan et al., 2021;L ie ta l . ,\n2022; Bertasius, Wang and Torresani, 2021; Wang & Tor-\nresani, 2022; Liu et al., 2022). In this section, we focus on\nreviewing previous works that use vision transformers for\npixel-wise visual prediction and video understanding. More\nrelated works of saliency prediction is elaborated in Sect. 2.3.\nStrudel et al. ( 2021) developed the ﬁrst transformer-based\narchitecture for semantic segmentation. Cheng et al. ( 2022)\nfurther uniﬁed semantic, instance, and panoptic segmentation\nin one transformer architecture. Ma et al. ( 2022) expanded\ntransformers to visual saliency forecasting by using self-\nattention to capture the correlation between past and future\nframes. Liu et al. ( 2021) built a transformer-based model\nto detect salient objects on RGB-D images. They fused the\nembeddings of the two modalities by using query from RGB\nframes and key and value from depth images.\nIn terms of video transformer, Bertasius et al. ( 2021)p r o -\nposed TimeSformer for video action recognition, which is the\nﬁrst transformer-based architecture for video understanding.\nA similar idea was also explored by Arnab et al. (\n2021). They\ndownsampled the resolution of input video segment by mul-\ntiple steps before feeding it into transformer layers. Fan et\nal. ( 2021) designed a multiscale video transformer balanc-\ning computational cost and action recognition performance.\nThis architecture was further improved by rearranging the\nlayers in each transformer block (Li et al., 2022). Patrick\net al. ( 2021) proposed the trajectory attention mechanism to\ntrack the same object in each video frame. Liu et al. ( 2022)\nextended the 2D swin-transformer (Liu et al., 2021)t oa3 D\narchitecture for action recognition.\nInspired by these successful applications of transformer\narchitectures, we present the ﬁrst work that uses a vision\ntransformer to address the challenging task of egocentric\ngaze estimation. In addition, we introduce the novel Global–\nLocal Correlation (GLC) module that provides additional\ninsight into video representation learning with self-attention.\nWe implement this module on two video transformer back-\nbones (Fan et al., 2021; Patrick et al., 2021) and conduct\nthorough experiments in this paper.\n2.3 Visual Saliency\nVisual saliency prediction has been well studied in computer\nvision in recent years (Pan et al., 2017; Wang et al., 2017;\nChe et al., 2019; Wu et al., 2020; Kroner et al., 2020;J i a&\nBruce, 2020; Sun et al., 2022; Lou et al., 2021; Wang et al.,\n2021; Chen et al., 2021; Khattar et al., 2021; Bellitto et al.,\n2021; Tsiami et al., 2020; Jiang et al., 2022). Kruthiventi et\nal. (2017) developed a deep neural network with various ker-\nnel sizes to capture saliency features at different scales. Liu et\nal. (2018) calculated the relation weights between each pixel\n123\n858 International Journal of Computer Vision (2024) 132:854–871\nand the remaining pixels to embed the most relevant contex-\ntual features. Zhuge et al. ( 2022) improved the integrity of\ndetected saliency objects by using integrity channel enhance-\nment mechanism and part-whole veriﬁcation module. In\nterms of saliency prediction in videos, Wang et al. ( 2017)\nexpanded image saliency models to videos by incorporat-\ning a new branch to handle temporal information. Wu et al.\n(2020) proposed SalSAC, which shufﬂes features of different\nCNN layers and feeds them to a correlation-based Con-\nvLSTM. Wang et al. ( 2021) used multiple spatio-temporal\nself-attention modules to address the limitation of ﬁxed ker-\nnel size in 3D models and to model long-range temporal\ndependencies. Chen et al. ( 2021) decomposed video saliency\nprediction into spatial pattern capture and spatio-temporal\nreasoning. Lou et al. ( 2021) combined a convolutional net-\nwork and transformer architecture to model the long-range\nspatial context. Liu et al. ( 2023) proposed short-global and\nlong-local attention mechanisms to integrate contexts from\nneighboring frames.\nWhile visual saliency prediction localizes interesting spa-\ntial regions as potential attention targets, egocentric gaze\nestimation seeks to determine the gaze target of the camera\nwearers as they interact with a scene. In saliency prediction,\ncameras typically keep stable and move slowly and salient\nobjects could dominate the view. However, the scene con-\ntext captured from egocentric video is complex and rapidly\nchanging, which requires a gaze estimation model with the\nability of explicitly reasoning about the correlation between\nlocal visual features and global scene context. In our experi-\nment section, we demonstrate that our proposed GLC module\ncan signiﬁcantly beneﬁt gaze estimation performance under\nthis challenging setting.\n3 Method\nGiven an input egocentric video clip with ﬁxed length T and\nspatial dimension H × W , our goal is to predict the gaze\nlocation in each video frame. Following Li et al. ( 2018), we\nconsider the gaze prediction as a probabilistic distribution\ndeﬁned on the 2 D image plane.\nFigure 3 presents an overview of our proposed method. We\nuse the recent multi-scale video transformer (MViT) (Fan\net al., 2021) or MotionFormer (Patrick et al., 2021) archi-\ntecture as the backbone network for video representation\nlearning. We extend the backbone by designing the Visual\nToken Embedding Module to generate the spatio-temporal\ntokens of both local visual patches and global visual context\nand feed them into the standard Multi-Head Self-Attention\nModule. We then utilize a novel Global–Local Correlation\nFig. 3 Architecture of the proposed model. The model consists of four\nmodules – a Visual Token Embedding Module encodes the input into\nlocal tokens and one global token, b Transformer Encoder is composed\nof multiple regular self-attention and linear layers, c Global–Local Cor-\nrelation Module models the correlation of global and local tokens, and\nd Transformer Decoder maps encoded video features from Transformer\nEncoder and GLC to gaze prediction. ⊕ denotes concatenation along\nthe channel dimension\n123\nInternational Journal of Computer Vision (2024) 132:854–871 859\n(GLC) Module to explicitly model the correlation between\nglobal and local visual tokens for gaze estimation. Finally,\nwe make use of the Decoder Network to predict the gaze\ndistribution based on the learned video representation from\nthe GLC module.\n3.1 Transformer Encoder with Global Visual Token\nEmbedding\nVisual Token Embedding. We split the input video sequence\ninto non-overlapping patches with size sT × sH × sW and\nadopt a linear mapping function to project each ﬂattened\npatch into D-dimension vector space. Following MViT (Fan\net al., 2021), this is equivalent to a convolutional layer with a\nstride of s\nT ×sH ×sW and a number of output channels of D.\nThis operation results in N tokens where N = T\nsT × H\nsH × W\nsW .\nIn addition, the learnable positional embedding E ∈ RN×D\nis added to the local tokens. Our key insight is to further\nembed global information into a global visual token using\nconvolutional operations, as illustrated in Fig. 3a. Since there\nis a single global token, it does not require positional embed-\nding.\nIn our experiments, we examine four global visual embed-\nding strategies as demonstrated in Fig. 4. We (a) implement\nmax pooling on input frames directly, and (b) implement max\nFig. 4 Four different approaches of global visual token embedding\npooling on unﬂattened local visual tokens. For (c) and (d), we\nreplace max pooling operations in (a) and (b) with a sequence\nof convolutional layers. Speciﬁcally, for global embedding\nin (d), we use three additional layers to downsample unﬂat-\ntened local tokens to produce a single global token. In (c),\ninput video frames are ﬁrst fed into a convolutional layer\nthat is identical to the layer used for local token embedding.\nThen, the output is passed to a sequence of convolutional\nlayers identical to (d). The experimental results of the four\nstrategies are reported in Sect. 4.2.1 and Table 1. The strat-\negy (d) provides the best gaze estimation performance in our\nexperiments and we thereby use (d) in the ﬁnal version of\nour model.\nMulti-Head Self-Attention Module .T h e N local tokens\nand one global token are fed into a transformer encoder\nconsisting of multiple self-attention blocks. The number of\nlocal tokens is downsampled after each self-attention block,\nwhile the number of global tokens remains 1. Suppose the\ninput of the j-th layer of encoder is X\n(j)\ne =[ x(j)\ni ]\nN j +1\ni=1 ∈\nR(N j +1)×D j , where N j is the number of local tokens, D j is\nthe vector length of each token and x(j)\ni is the i-th row of X(j)\ne\ndenoting the i-th token of size 1 ×D j . For simplicity, we omit\nsubscript and superscript of j and multi-head operations in\nthe following equations. In each self-attention layer, corre-\nlations are calculated in each token pair as shown in Fig. 3b.\nThey are used to reweight values of each token after softmax.\nFormally, we denote the query, key and value matrices of\neach self-attention layer in an encoder block as Q\n(N+1)×D\ne =\n[qi ]N+1\ni=1 , K (N+1)×D\ne =[ ki ]N+1\ni=1 and V (N+1)×D\ne =[ vi ]N+1\ni=1 .\nThe self-attention in transformer encoder is formulated as\nAttention (Qe, Ke, Ve)\n= Sof tmax (Qe K T\ne /\n√\nD)Ve ∈ R(N+1)×D.\n(1)\nFinally, we attach a standard linear layer after the self-\nattention operation.\nTable 1 Evaluation of different\nglobal embedding approaches\nand global–local correlation\nmodule\nMethods EGTEA Gaze+ Ego4D\nF1 Recall Precision F1 Recall Precision\nMViT (Fan et al., 2021) 43.0 57.8 34.2 40.9 57.4 31.7\nMViT + (a) 43.4 58.4 34.5 41.5 56.8 32.6\nMViT + (b) 43.5 59.2 34.4 41.4 57.3 32.4\nMViT + (c) 43.7 58.3 34.9 41.3 57.5 32.2\nMViT + (d) 43.9 59.0 34.9 41.7 57.6 32.7\nMViT + (d) + SA 44.1 58.8 35.3 42.1 58.5 32.9\nMViT + (d) + GLC 44.86 1 .23 5 .34 3 .1 57.0 34.7\n(a)(b)(c)(d) are different global embedding strategies elaborated in Sect. 3.1 and Fig. 4. SA and GLC denote\nregular self-attention and global–local correlation module, respectively. Please refer to Sects. 4.2.1 and 4.2.2\nfor more explanations\n123\n860 International Journal of Computer Vision (2024) 132:854–871\n3.2 Global–Local Correlation\nEven though global information has been explicitly embed-\nded into the global visual token in our model, the transformer\nencoder treats the global and local tokens equivalently as\nshown in Eq. 1 and Fig. 3b. In this case, global–local cor-\nrelation is diluted by correlations among the local tokens,\nlimiting its impact on gaze estimation. In order to address\nthis problem, we propose to increase the available capacity\nto model global–local token interactions. Our solution is a\nnovel Global-Local Correlation module described in Fig. 3c.\nFormally, we denote the global token as the ﬁrst row vector\nof X\ne, i.e., x1. Thus q1, k1 and v1 are the query, key and value\nprojected from the global token, respectively. To explicitly\nmodel the connection between global and local visual fea-\ntures, we only calculate the correlation between each local\ntoken and the global token, i.e., Correlation (x\ni , x1),a sw e l l\nas its self-correlation, i.e., Correlation (xi , xi ). Then corre-\nlation scores are normalized by softmax to further re-weight\nthe values. We exploit a suppression matrix (Liu et al., 2021)\nS\n(N+1)×(N+1) to suppress the correlation of other tokens,\nwhere\nS(N+1)×(N+1) =[ sij ], sij =\n{ 0, if i = j or j = 1\nλ, otherwise. (2)\nWe assign zeros to the diagonal and the ﬁrst column in S\nand set a large value λ for the other elements. We follow the\nempirical choice from the implementation of Liu et al. ( 2021)\nand set λ = 108 in our experiments. Formally, the proposed\nGLC can be formulated as\nGLC (Qe, Ke, Ve)\n= Sof tmax ((Qe K T\ne − S)/\n√\nD)Ve ∈ R(N+1)×D\n(3)\nIn this way, we keep the values on the ﬁrst column and\nthe diagonal, and map them into probability distributions,\nwhile values in other positions are nearly “masked out”\nafter the softmax. Residual connections and linear layers\nare also used in the GLC module as in the regular self-\nattention block. Finally, the output tokens from the GLC\nare concatenated with those from the transformer encoder\nin the channel dimension. We denote outputs of the GLC\nand the last encoder block as X\nGLC\ne ∈ R(N+1)×D and\nX SA\ne ∈ R(N+1)×D. The concatenation can then be formu-\nlated as Xe = X SA\ne ⊕X GLC\ne ∈ R(N+1)×2D. The fused tokens\nXe are subsequently fed into the transformer decoder for gaze\nestimation.\n3.3 Transformer Decoder\nTo produce the gaze distribution with the desired spatio-\ntemporal resolution, we adopt a decoder to upsample the\nencoded features. We utilize a transformer decoder based\non the multiscale self-attention block of MViT (Fan et al.,\n2021). Suppose each decoder layer takes visual features\nX\nd ∈ RT ′H′W ′×D′\nas inputs and the corresponding query,\nkey and value matrices are QT ′H′W ′×D′\nd , K T ′H′W ′×D′\nd and\nV T ′H′W ′×D′\nd . As shown in Fig. 3d, we replace the original\npooling operation for the query matrix with an upsampling\noperation implemented with trilinear interpolation and keep\nthe pooling for the key and value matrices. Following Fan\net al. ( 2021), ˆQ\nd is obtained by applying a deconvolutional\noperation on Qd , while ˆKd and ˆVd are obtained by applying\nconvolutional operations on Kd and Vd . Then, the output of\nself-attention is calculated in the same way as Eq. 1. In addi-\ntion, we keep the skip connection in the self-attention layers\nand replace the pooling operation in skip connections with\ntrilinear interpolation, which produces the upsampled output\nwith dimension ˆT ˆH ˆW × D\n′. Our decoder is composed of 4\ndecoding blocks. Skip connections are used to combine inter-\nmediate features of the encoder with corresponding decoder\nfeatures. Finally, another linear mapping function is used to\noutput the ﬁnal gaze prediction.\n3.4 Network Architecture and Model Training\nWe adopt MViT (Fan et al., 2021) and MotionFormer (Patrick\net al., 2021) as the backbones, with weights initialized from\nKinetics-400 pretraining (Kay et al., 2017). The GLC module\nand decoder are initialized with Xavier initialization (Glorot\nand Bengio, 2010). For MViT, the token embedding stride\nis set as s\nT = 2, sH = 4 and sW = 4 and the embed-\nding dimension is D = 96. The encoder is composed of\n16 self-attention layers that are divided into 4 blocks. The\nnumber of tokens is downsampled at the transition between\ntwo blocks. For MotionFormer, the token embedding stride\nis set as s\nT = 2, sH = 16, sW = 16 and the embedding\ndimension is D = 768. The encoder consists of 12 layers\nwith trajectory self-attention. The number of tokens doesn’t\nchange in the encoder. We build the decoder with 4 decoder\nblocks corresponding to the 4 blocks in the encoder. After\ngetting raw output from decoder, softmax is applied on each\nframe with a temperature τ. This can be formally written as\nˆp\nij = exp(ˆyij /τ)∑\ni,j exp(ˆyij /τ) where ˆyij is the logit at location (i, j)\nfrom the model and ˆpij is probability after softmax. In exper-\niments, τ is empirically set as 2. We use KL-divergence loss\nto capture the difference between labels and predictions. The\nmodel is trained using AdamW (Loshchilov & Hutter, xxxx)\noptimizer with a batch size of 16. We adopt a warm-up train-\ning strategy that increases learning rate from 10\n−6 to 10 −4.\nThen the learning rate decreases in compliance with cosine\nannealing scheme (Loshchilov & Hutter, 2016).\n123\nInternational Journal of Computer Vision (2024) 132:854–871 861\n4 Experiment\nIn this section, we show the experimental setup and detailed\nresults. We ﬁrst elaborate the two datasets used in our experi-\nments, evaluation metrics and data processing details. Second\nwe show exhaustive ablation studies for egocentric gaze esti-\nmation and compare with prior works. Third, we validate\nthe generalization capability of our model by applying it to\ngaze saccade/ﬁxation prediction and egocentric action recog-\nnition. Finally, we visualize the predictions and correlation\nweights in GLC module to provide more insights.\n4.1 Datasets and Metrics\nDatasets. We conducted our experiments on two egocentric\nvideo datasets with gaze tracking data serving as ground truth\n– EGTEA Gaze+ (Li et al., 2018) and Ego4D (Grauman\net al., 2022). The EGTEA Gaze+ dataset is captured under\nthe meal preparation setting, which involves a great deal of\nhand-object interactions. We used the ﬁrst train/test split from\nEGTEA Gaze+ in our experiments (8299 clips for training\nand 2022 clips for testing). The Ego4D dataset includes 27\nvideos of 80 participants totaling 31 h with gaze tracking data\ncaptured under the social setting. We split the long videos into\n5-second video clips and pick clips containing gaze ﬁxation.\nWe used 20 videos (15,310 clips) for training and the other\n7 videos (5202 clips) for testing. Note that we keep using\nthe same train/test split for all the three tasks – egocentric\ngaze estimation, gaze saccade/ﬁxation prediction and action\nrecognition. Importantly, this is the ﬁrst work that uses the\nEgo4D dataset for egocentric gaze estimation, and we have\nmade our split publicly available to drive future research on\nthis topic.\nEvaluation Metrics. Following Li et al. ( 2018, 2021)( t h e\nsource of the EGTEA Gaze+ dataset), we adopt F1 score,\nrecall, and precision as the evaluation metrics for gaze esti-\nmation. Note that we do not consider AUC score as our main\nmetrics, since AUC performance can become saturated due\nto the long-tailed nature of the distribution of gaze in a single\nframe. In terms of saccade/ﬁxation prediction, we primarily\nmeasure the performance by average F1 (average of F1 scores\nof the two categories) and mean class average (following Li et\nal. (2021)) because of the imbalance of saccade and ﬁxation,\nbut regular accuracy metric is also provided for reference.\nFor action recognition, we directly follow prior works (Li et\nal., 2021; Hao et al., 2022) and adopt top-1 accuracy, top-5\naccuracy and mean class accuracy.\nData Processing. At training time of egocentric gaze esti-\nmation, we randomly sample 8 frames from each video with\na sampling interval of 8 as input (i.e. selecting 8 frames\nfrom a 72-frame window with equal spacing). All videos\nare spatially downsampled to 256 in height while keeping\nthe original aspect ratio. We further implement multiple data\naugmentations including random ﬂipping, shifting, and resiz-\ning. We then randomly crop each frame to get an input with\ndimensions 8 × 256 × 256. The output from the decoder is\na downsampled heatmap with dimension 8 × 56 × 56. For\nvisualization, the output heatmap is upsampled to match the\ninput size by trilinear interpolation. At inference time, the\ninput clip is center-cropped. For gaze labels, we generate a\ngaussian kernel centered at the gaze location in each input\nframe with a kernel size of 19 following Chong et al. ( 2020).\nWe use a uniform distribution for frames where gaze is not\ntracked in training and only calculate metrics on frames with\nﬁxated gaze in testing as in the work of Li et al. ( 2018). For\nthe EGTEA Gaze+ (Li et al., 2018) dataset, we determine\nwhich frames to calculate metrics on by using the provided\nlabel of gaze ﬁxations and saccades. On the Ego4D (Grau-\nman et al., 2022) dataset, no label of gaze type is available.\nWe calculate the euclidean spatial distance of gaze between\nadjacent frames and consider the tracked gaze to be a sac-\ncade if the distance is above a threshold, and treat it as ﬁxation\notherwise. We adopt an empirical threshold of 40.\nIn terms of gaze saccade/ﬁxation prediction, we adopt the\nsame data processing settings as gaze estimation. We aggre-\ngate the frame-level labels of gaze type to get the label for\neach video segment. Speciﬁcally, the percentages of saccade\nframes account for 27% and 17% on EGTEA Gaze+ and\nEgo4D, respectively. The video segment is labeled as saccade\nif any sampled frame is annotated as saccade. Otherwise, it’s\nlabeled as gaze ﬁxation. Consequently, the ratio of saccade\nand ﬁxation is 4:1 on EGTEA Gaze+ and 2:1 on Ego4D.\nAs for egocentric action recognition, we only imple-\nment experiments on EGTEA Gaze+ because Ego4D doesn’t\nprovide action labels. The data processing procedures are\nidentical to gaze estimation except that we set the input\ndimensions as 8 × 224 × 224 during training. In testing, the\ninput dimension is 8×256×256 following Li et al. (2021). We\nalso adopt more data augmentation including MixUp (Zhang\net al., 2017), color jittering and random erasing.\n4.2 Experimental Results on Egocentric Gaze\nEstimation\n4.2.1 The Design Choice of Global Visual Embedding\nWe introduce four global context embedding strategies in\nSect. 3.1 and Fig. 4. We investigate the performance of these\nstrategies on MViT model (Fan et al., 2021). As shown in\nTable 1, all four global embedding strategies improve the\nperformance of vanilla MViT model on both the EGTEA\ndataset and the Ego4D dataset. This result supports our claim\nthat global context is essential for gaze estimation. Among\nthe four embedding strategies, (d) achieves the largest per-\nformance improvement on both datasets (+0.9% on EGTEA\nand +0.8% on Ego4D). This indicates that convolutional lay-\n123\n862 International Journal of Computer Vision (2024) 132:854–871\ners and the embedded local tokens can facilitate the learning\nof global context. Thus, we use this strategy in the follow-\ning experiments. Note that all baseline methods use the same\ntransformer decoder.\n4.2.2 Evaluation of Global–Local Correlation\nWe also evaluate the Global–Local Correlation (GLC) mod-\nule of our model. As presented in Table 1, our full model –\nMViT+(d)+GLC outperforms the baseline MViT by +1.8%\non EGTEA dataset and +2.2% on Ego4D dataset. Specif-\nically, the GLC module contributes to a performance gain\nof +0.9% on EGTEA and +1.4% on Ego4D (comparing to\nMViT+(d)). This result suggests that the GLC can break\ndown the mathematical equivalence of global and local\ntokens in regular self-attention, thereby “highlighting” the\nglobal–local connection in the learned representation.\nDoes the Performance Improvement Come from Addi-\ntional Parameters? It is possible that the performance of our\nmodel beneﬁts from additional parameters in the GLC mod-\nule. In Table 1, we report the results of another baseline model\n— MViT+(d)+SA., where we remove the GLC module and\nadd a regular self-attention (SA) layer at the same location.\nInterestingly, the additional SA layer has minor inﬂuence\non the overall performance (+0.2% on EGTEA and +0.4%\non Ego4D). In contrast, our model outperforms this base-\nline by +0.7% on EGTEA and +1.0% on Ego4D. This result\nindicates that the performance boost of our method does not\nsimply come from the additional parameters of GLC. Instead,\nthe explicit modeling of the connection between global and\nlocal visual features is the key factor in the performance gain.\nOn the other hand, the regular SA layer includes both global–\nlocal correlations and local correlations while the proposed\nGLC module only calculates global–local correlations. The\nresults suggest that local correlations may dilute the global\ncontext and thus limit the performance.\n4.2.3 Comparison with Previous State-of-the-Art\nIn addition to these studies to evaluate the components\nof our model, we compare our approach with prior works.\nApart from MViT (Fan et al., 2021), we also plug the\nglobal embedding and GLC modules in another transformer-\nbased architecture – MotionFormer (Patrick et al., 2021).\nResults are presented in Table 2 and Table 3. Note that, for\nAttention Transition (Huang et al., 2018), I3D-R50 (Feicht-\nenhofer et al., 2019), MotionFormer (Patrick et al., 2021)\nand MViT (Fan et al., 2021) from Table 2 and all base-\nlines from Table 3, we initialize the model parameters using\npretrained checkpoints from Kinetics (Kay et al., 2017) and\nﬁnetune the models using the same training set as our method.\nInterestingly, the baseline MViT and MotionFormer easily\noutperform all previous works that use CNN-based architec-\ntures on both the EGTEA dataset and the Ego4D dataset. In\naddition, our method implemented on MotionFormer (GLC-\nMotionFormer) outperforms the best CNN model by +2.3%\non F1, +1.8% on recall and +2.3% on precision for EGTEA,\nand +3.5% on F1, +4.3% on recall and +2.8% on precision\nfor Ego4D. The improvement is more prominent with MViT\nas backbone (GLC-MViT). It surpasses the best CNN model\nby +3.9% on F1, +4.0% on recall and +3.5% on precision for\nEGTEA, and +5.6% on F1, +4.5% on recall and +5.5% on\nprecision for Ego4D. These results demonstrate the superior-\nity of using a transformer-based architecture for egocentric\ngaze estimation as well as the effectiveness and robustness\nof our proposed method.\nWe can also observe MotionFormer lags behind MViT\nby a large margin. This is because MotionFormer directly\nTable 2 Comparison with\nprevious methods on EGTEA\nGaze+\nMethods F1 Recall Precision\nCenter Prior 10.7 32.0 6.4\nGBVS (Harel et al., 2006) 15.7 45.1 9.5\nE g o G a z e( L ie ta l . ,2013) 16.3 16.3 16.3\nSimpleGaze 31.3 41.8 16.1\nDeep Gaze (Zhang et al., 2017) 34.5 43.1 28.7\nGaze MLE (Li et al., 2021) 26.6 35.7 21.3\nJoint Learning (Li et al., 2021) 34.0 42.7 28.3\nAttention Transition (Huang et al., 2018) 37.2 51.9 29.0\nI3D-R50 (Feichtenhofer et al., 2019) 40.9 57.2 31.8\nMotionFormer (Patrick et al., 2021) 42.1 56.4 33.7\nMViT (Fan et al., 2021) 43.0 57.8 34.2\nGLC-MotionFormer 43.2 59.0 34.1\nGLC-MViT 44.86 1 .23 5 .3\nOur complete model is highlighted. The proposed model outperforms previous approaches by a signiﬁcant\nmargin. See Sect. 4.2.3 for more details\n123\nInternational Journal of Computer Vision (2024) 132:854–871 863\nTable 3 Comparison with\nprevious methods on Ego4D Methods F1 Recall Precision\nCenter Prior 14.9 21.9 11.3\nGBVS (Harel et al., 2006) 18.0 47.2 11.1\nAttention Transition (Huang et al., 2018) 36.4 47.6 29.5\nI3D-R50 (Feichtenhofer et al., 2019) 37.5 52.5 29.2\nMotionFormer (Patrick et al., 2021) 38.5 55.0 29.6\nMViT (Fan et al., 2021) 40.9 57.4 31.7\nGLC-MotionFormer 41.0 56.8 32.0\nGLC-MViT 43.15 7 .03 4 .7\nOur complete model is highlighted. The model shows consistent superiority over other state of the arts on all\nmetrics. See Sect. 4.2.3 for more details\ndownsamples the spatial resolution of video frames by 16 in\nthe visual token embedding module, while MViT adopts a\nmulti-scale downsample strategy. The aggressive reduction\nin spatial dimension keeps high-level semantic information\nbut loses low-level spatial features. Nonetheless, our method\ncan still boost the performance of MotionFormer promi-\nnently on the two datasets (+1.1% on EGTEA and +2.5%\non Ego4D). It suggests the proposed method can work as an\neasy-to-use plug-in for other transformer-based models and\nbrings notable gains.\nMoreover, We note that the improvement of our model is\nmore prominent on Ego4D than EGTEA. We speculate that\nthis is because the Ego4D videos with gaze tracking data\nare captured under social interaction scenarios that contain\ninteractions with both people and objects, and thus require the\nmodel to more heavily consider the global–local connections\n(e.g. the visual information about a social partner’s gesture to\nan object) to predict the gaze. Another possible reason is that\nthe Ego4D dataset has more samples to train the transformer-\nbased model.\n4.2.4 Remarks\nVisualization of Predictions. We visualize predictions of\nour model and other previous methods in Fig. 5. Atten-\ntion transition (Huang et al., 2018) tends to overestimate\ngaze area which includes more uncertainty and ambigu-\nity. I3D-R50 (Feichtenhofer et al., 2019), vanilla Motion-\nFormer (Patrick et al., 2021) and vanilla MViT (Fan et al.,\n2021) architectures run into failure modes when there are\nmultiple objects and people in the scene. In contrast, our\nmodel, by explicitly modeling the connection between the\nglobal and local visual tokens, more robustly predicts the ego-\ncentric gaze distribution from the input video clip. We also\nillustrate examples of failure cases of our model in Fig. 6.\nPredicting gaze target near the boundary of the frame or\nin a scene without enough evidence to infer the gaze tar-\nget remains a challenging problem for our model as well as\nprior methods.\nWhat has been learned by the Global–Local Correlation\nmodule? We additionally empirically analyze our proposed\nGLC module. We ﬁrst calculate the correlation of the global\ntoken and each local token, and then normalize the cal-\nculated weights into a probabilistic distribution. A higher\nscore suggests that the GLC captures a stronger connection\nbetween the particular local token and the global context.\nWe reshape and upsample these weight distributions to form\na heatmap, which we overlay with the original input. Since\nthe GLC module applies a multi-head operation, we visual-\nize the results from different heads in Fig. 7. Interestingly,\nthe correlations captured by the GLC heads are quite diverse.\nSpeciﬁcally, on the EGTEA dataset, the maps produced by\nheads 1, 4, 5, and 8 highlight pixels around the gaze point\nwith different uncertainty (which is illustrated by the size\nof highlighted area). The other four heads focus on sur-\nrounding objects and leave gaze areas unattended. As for\nthe Ego4D data, only head 3 captures the wearers’ attention,\nwhile the other heads fully focus on the backgrounds in dif-\nferent aspects. This suggests that our GLC module does learn\nto model human attention by setting different weights from\nlocal to global tokens, capturing many facets of scene infor-\nmation (both around the gaze target and in the background)\nin the multi-headed attention mechanism. Another impor-\ntant ﬁnding is that some heads learn to attend to background\npixels to prevent the model from omitting important scene\ncontext.\n4.3 Experimental Results on Gaze Saccade/Fixation\nPrediction\nApart from gaze estimation, we demonstrate the capability\nof our model in capturing the feature of gaze ﬁxation and\nsaccade. We use the same backbone as egocentric gaze esti-\nmation but replace the decoder with a linear layer for binary\nclassiﬁcation. Binary cross-entropy loss is adopted in train-\ning and the loss weight for saccade and ﬁxation is set as 1:2 to\nbalance the two categories. Results are presented in Table 4.\n123\n864 International Journal of Computer Vision (2024) 132:854–871\nFig. 5 Visualization of gaze estimation. Estimated gaze is represented as a heatmap overlayed on input frames. Green dots denote the ground truth\ngaze location\nVanilla MotionFormer and MViT can both capture the\nfeature of gaze movement on the two datasets. After embed-\nding the global context into a global token, MotionFormer is\nboosted by +0.6% on EGTEA and +0.6% on Ego4D while\nMViT is boosted by +0.8% on EGTEA and +1.1% on Ego4D\nin terms of average F1. The GLC module further improves the\nperformance by a notable margin. Consequently, our method\nleads to an overall improvement over the MotionFormer base-\nline by +1.1% on average F1, +1.1% on mean class accuracy\nand +0.3% on accuracy for EGTEA as well as +1.8% on\naverage F1, +0.9% on mean class accuracy and +2.5% on\naccuracy for Ego4D. Furthermore, it also improves MViT\nby +1.6% on average F1, +1.3% on mean class accuracy,\n+1.7% on accuracy for EGTEA, as well as +3.7% on aver-\nage F1, +4.4% on mean class accuracy, +3.2% on accuracy\nfor Ego4D. Obviously, gains on Ego4D are more promi-\n123\nInternational Journal of Computer Vision (2024) 132:854–871 865\nVideo Frame AttnTransit I3D-R50 MViT GLC (Ours)MotionFormer\nFig. 6 Failure cases for egocentric gaze estimation. Estimated gaze is represented as a heatmap overlayed on input frames. Green dots denote the\nground truth gaze location\nnent than EGTEA which is consistent with the phenomenon\nwe observed in gaze estimation and can be explained with\nthe similar reasons (see Sect. 4.2.3). We also note that the\nimprovement of MotionFormer is much smaller than MViT.\nThe possible reason is that MotionFormer calculates corre-\nlations along the trajectory of each pixel which might ignore\nsome indicators for saccade (e.g. blurry background). The\noverall improvements further validate the capability of our\nmethod in egocentric gaze behavior modeling.\n4.4 Experimental Results on Action Recognition\nIn addition to gaze behavior modeling, we also examine the\napplication of our GLC module to the egocentric video action\nrecognition task, and ﬁnd that our method performs compet-\nitively with methods designed speciﬁcally for this task on\nEGTEA Gaze+. Similar to saccade/ﬁxation prediction, we\nremove the decoder in the gaze estimation model and keep\nonly the visual token embedding, transformer encoder, and\nGLC modules. However, we further investigate two differ-\nent ways to obtain class categories for action recognition:\nadding a class embedding token at the ﬁrst layer of trans-\nformer, or using pooling across all local tokens to obtain a\nﬁnal embedding. Then a fully-connected layer followed by\nsoftmax is used to predict probabilities for each category.\nWe implement both strategies and compare our approaches\nwith previous works in Table 5. We conduct these experi-\nments with two backbones only on EGTEA Gaze+ (Li et al.,\n2018) using the same split as gaze estimation. Note that the\nEgo4D (Grauman et al., 2022) social benchmark does not\ncontain action labels.\nFor vanilla MotionFormer (Patrick et al., 2021) and\nMViT (Fan et al., 2021), class token embedding performs bet-\nter than or comparably with the pooling operation. For both\nstrategies, simply adding global embedding to MotionFormer\nonly results in minor gains in the performance. Likewise,\nadding global embedding to MViT has a minor inﬂuence on\nthe overall performance (−0.2% on top1 accuracy, −0.5% on\ntop5 accuracy and +1.3% on mean class accuracy while using\nthe class token, and −0.4%, on top1 accuracy, −0.2% on top5\naccuracy and −1.1% on mean class accuracy while using\npooling layer). This result suggests that simply embedding\nglobal context into an additional token has minor inﬂuence\non the action recognition performance.\nIn addition, adding our GLC module can only improve\nthe model performance by a small margin when using class\ntoken embedding to predict action classes. We hypothesize\nthat this is because only the class token is input into the lin-\near layer for ﬁnal prediction and re-weighted tokens from\nGLC are left unused. In contrast, when applying global aver-\nage pooling on all local tokens, GLC improves top1, top5\nand mean class accuracy over the counterpart that doesn’t\nuse GLC ( MotionFormer/MViT+Global Token) by +1.7%,\n+1.4% and +1.7% respectively for MotionFormer and +2.2%,\n+0.6% and +3.1% respectively for MViT. Gains over cor-\nresponding MotionFormer baseline are +1.6%, +3.6% and\n+3.4% on the tree metrics while gains over MViT baseline\nare +1.8%, +0.4% and +2.0%. These results indicate our pro-\nposed GLC module is a robust and general design that also\nimproves the action recognition performance. However, the\nimpact on action recognition is smaller compared with ego-\ncentric gaze estimation because our model doesn’t have any\nspeciﬁc design for actoin recognition task.\nWe note that our model achieves a competitive perfor-\nmance for action recognition on EGTEA without additional\ndesign for this speciﬁc task. Our top1 accuracy of 65.3%\nexceeds the model from Wang et al. ( 2020) by +2.2%, and\nis only a −0.2% difference from the recent state-of-the-art\nmethod (Hao et al., 2022) for this benchmark of 66.5%. We\nalso want to emphasize that we conduct these action recogni-\ntion experiments to demonstrate the generalization capability\n123\n866 International Journal of Computer Vision (2024) 132:854–871\nVideo Frame Head 1 Head 2 Head 3 Head 4 Head 5 Head 6 Head 7 Head 8\nFig. 7 Visualization of the eight heads in global–local correlation module for egocentric gaze estimation. The ﬁrst four samples are from EGTEA\nGaze+ and the last four are from Ego4D. Green dots denote gaze location\nTable 4 Results of gaze\nsaccade/ﬁxation prediction on\nEGTEA Gaze+ and Ego4D\ndatasets\nMethods EGTEA Gaze+ Ego4D\nAvg F1 Mean Acc Acc Avg F1 Mean Acc Acc\nMotionFormer (Patrick et al., 2021) 56.9 56.5 75.3 59.3 61.4 60.1\nMotionFormer + Global Token 57.5 57.4 73.8 59.9 60.3 62.4\nMotionFormer + Global Token + GLC 58.0 57.6 75.6 61.1 62.3 62.6\nMViT (Fan et al., 2021) 58.0 57.8 74.4 57.9 58.6 59.8\nMViT + Global Token 58.8 58.0 77.4 59.0 60.1 60.5\nMViT + Global Token + GLC 59.65 9 .1 76.1 61.66 3 .06 3 .0\nAvg F1 denotes average F1 and Mean Acc denotes mean class accuracy. Acc is the regular accuracy metric\n123\nInternational Journal of Computer Vision (2024) 132:854–871 867\nTable 5 Results of action\nrecognition on EGTEA Gaze+ Methods Cls Token Pooling Top1-Acc Top5-Acc Mean Cls Acc\nMFormer (Patrick et al., 2021) ✓ 63.4 85.8 55.3\nMFormer (Patrick et al., 2021) ✓ 64.7 86.5 55.6\nMFormer + Global Token ✓ 63.2 90.1 53.3\nMFormer + Global Token ✓ 64.6 88.7 57.3\nMFormer + Global Token + GLC ✓ 64.3 89.6 56.4\nMFormer + Global Token + GLC ✓ 66.39 0 .15 9 .0\nMViT (Fan et al., 2021) ✓ 64.6 89.2 54.0\nMViT (Fan et al., 2021) ✓ 63.5 88.7 55.3\nMViT + Global Token ✓ 64.4 88.7 55.3\nMViT + Global Token ✓ 63.1 88.5 54.2\nMViT + Global Token + GLC ✓ 64.8 88.7 56.8\nMViT + Global Token + GLC ✓ 65.38 9 .15 7 .3\nWe implemented two methods for classiﬁcation–adding an additional class token or using global average\npooling. We show the generalization capability of the proposed method on two backbones. The complete\nmodels are highlighted\nVideo Frame Head 1 Head 2 Head 3 Head 4 Head 5 Head 6 Head 7 Head 8\nFig. 8 Visualization of the eight heads in global–local correlation module for action recognition on EGTEA Gaze+\nof our proposed GLC module rather than aim to produce\nSOTA results on action recognition.\nAdditionally, we visualize the global–local correlation\nweights of the GLC in Fig. 8. Importantly, the learned global–\nlocal correlation is vastly different from the gaze distribution\nwhen the model is trained for action recognition; in contrast,\na stronger connection between the learned global–local\ncorrelation and gaze distribution can be observed when the\nmodel is trained for gaze estimation (see Fig. 7). How to\ndesign a weakly-supervised model for egocentric gaze esti-\nmation remains an open question.\n123\n868 International Journal of Computer Vision (2024) 132:854–871\n5 Limitations and Future Work\nLimitations. Despite the notable gains from global–local\ncorrelation, there are still some limitations in our transformer-\nbased method. The model requires larger computational cost,\nand therefore may not be feasible for on-device computing\n(e.g. AR/VR). We note that some recent works on network\narchitecture research (Chen et al., 2021) and knowledge\ndistillation(Lin et al., 2022) seek to reduce the computa-\ntional cost of transformer architecture. These works actually\ndemand a dense model with strong performance as a starting\npoint. Therefore, our work may provide a foundational step\nfor designing light-weight models for the egocentric gaze\nestimation setting in the future.\nFuture Work. In this paper, we studied the explicit integra-\ntion of global scene context for egocentric gaze estimation\nand proposed a novel modeling approach for this problem.\nWe also showe the results of our proposed architecture on\ngaze saccade/ﬁxation prediction and egocentric action recog-\nnition to demonstrate our model’s generalization cability. Our\nﬁndings also point to several exciting future research direc-\ntions:\n• Our proposed GLC module has the potential to address\nother video understanding tasks including visual saliency\nprediction in third-person video, active object detection,\nand future forecasting. We plan to study the effect of our\nmethod on those tasks in our future work.\n• Our proposed GLC fails to learn the gaze distribution\nwhen the model is trained to predict the action labels.\nHow to design a weakly supervised model for egocen-\ntric gaze estimation using action labels is an interesting\nproblem.\n• Our transformer-based model requires larger computa-\ntional cost, and hence may not be applied for on-device\ncomputing. We will continue to study how to com-\nbine it with some recent works on network architecture\nresearch (Chen et al., 2021) and knowledge distilla-\ntion (Lin et al., 2022) to reduce the computational cost\nof transformer architecture.\n6 Conclusion\nIn this paper, we develop a transformer-based architecture to\naddress the task of estimating the camera wear’s gaze ﬁxa-\ntion based only on egocentric video frames. Our key insight\nis that our global visual token embedding strategy, which\nencodes global visual information into the self-attention\nmechanism, and our global–local correlation (GLC) mod-\nule, which explicitly reasons about the connection between\nglobal and local visual tokens, facilitate strong representa-\ntion learning for egocentric gaze estimation. Our experiments\non the EGTEA Gaze+ and Ego4D datasets demonstrate the\neffectiveness of our approach. We additionally apply our\nmethod to a novel gaze saccade/ﬁxation prediction task and\nthe traditional action recognition problem. The proposed\nmethod can improve the performance prominently which\nshows its strong generalization capability. We also implement\nthe global token embedding strategy and GLC module in two\nbackbones to show it can serve as an easy-to-use plug-in to\nother transformer-based architecture. We believe our work\nserves as an essential step in analyzing gaze behavior from\negocentric videos and provides valuable insight into learning\nvideo representations with transformer architectures.\nAcknowledgements Portions of this project were supported in part by\nNIH R01MH114999 and by a gift from Meta.\nAuthor Contributions All authors contributed to the study design. Data\npreprocessing, model implementation and experiment design were per-\nformed by BL and ML. The ﬁrst draft of the manuscript was written by\nBL and it was reviewed and edited by ML, FR and JMR. All authors\nread and approved the ﬁnal manuscript.\nFunding Partial ﬁnancial support of this project was provided by NIH\nR01MH114999 and by a gift from Meta.\nData Availability The data used in the experiments are publicly avail-\nable online. EGTEA Gaze+ dataset is available via https://cbs.ic.gatech.\nedu/fpv/. Ego4D dataset is available via https://ego4d-data.org/.\nCode Availability Codes and train/test split for Ego4D are available on\nhttps://bolinlai.github.io/GLC-EgoGazeEst/ .\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nAl-Naser, M., Siddiqui, S.A., Ohashi, H., Ahmed, S., Katsuyki, N.,\nTakuto, S., & Dengel, A. (2019). Ogaze: Gaze prediction in ego-\ncentric videos for attentional object selection. 2019 digital image\ncomputing: Techniques and applications (dicta) (pp. 1–8).\nArnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇ ci´c, M., & Schmid,\nC. (2021). Vivit: A video vision transformer. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision(pp.\n6836–6846).\nBellitto, G., Proietto Salanitri, F., Palazzo, S., Rundo, F., Giordano, D.,\n& Spampinato, C. (2021). Hierarchical domain-adapted feature\nlearning for video saliency prediction. International Journal of\nComputer Vision, 129(12), 3216–3232.\n123\nInternational Journal of Computer Vision (2024) 132:854–871 869\nBertasius, G., Wang, H., & Torresani, L. (2021). Is space-time attention\nall you need for video understanding?. In International Conference\non Machine Learning.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., & Dhariwal,\nP. (2020). Language models are few-shot learners. Advances in\nNeural Information Processing Systems, 33, 1877–1901.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &\nZagoruyko, S. (2020). End-to-end object detection with transform-\ners. In European Conference on Computer Vision(pp. 213–229).\nChe, Z., Borji, A., Zhai, G., Min, X., Guo, G., & Le Callet, P. (2019).\nHow is gaze inﬂuenced by image transformations? dataset and\nmodel. IEEE Transactions on Image Processing, 29, 2287–2300.\nChen, B., Li, P., Li, C., Li, B., Bai, L., Lin, C., & Ouyang, W. (2021).\nGlit: Neural architecture search for global and local image trans-\nformer. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (pp. 12–21).\nChen, J., Li, Z., Jin, Y ., Ren, D., & Ling, H. (2021). Video saliency\nprediction via spatio-temporal reasoning. Neurocomputing, 462,\n59–68.\nCheng, B., Misra, I., Schwing, A.G., Kirillov, A., & Girdhar, R. (2022).\nMasked-attention mask transformer for universal image segmen-\ntation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition(pp. 1290–1299).\nChong, E., Ruiz, N., Wang, Y ., Zhang, Y ., Rozga, A., & Rehg, J.M.\n(2018). Connecting gaze, scene, and attention: Generalized atten-\ntion estimation via joint modeling of gaze and scene saliency.\nIn Proceedings of the European Conference on Computer Vision\n(ECCV) (pp. 383–398).\nChong, E., Wang, Y ., Ruiz, N., & Rehg, J.M. (2020). Detecting\nattended visual targets in video. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp.\n5396–5406).\nDai, X., Chen, Y ., Yang, J., Zhang, P., Yuan, L., & Zhang, L. (2021).\nDynamic detr: End-to-end object detection with dynamic atten-\ntion. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (pp. 2988–2997).\nDai, Z., Cai, B., Lin, Y ., & Chen, J. (2021). Up-detr: Unsupervised pre-\ntraining for object detection with transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (pp. 1601–1610).\nDai, Z., Liu, H., Le, Q. V ., & Tan, M. (2021). Coatnet: Marrying convolu-\ntion and attention for all data sizes. Advances in Neural Information\nProcessing Systems, 34, 3965–3977.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,\nUnterthiner, T., et al. (2022). An image is worth 16x16 words:\nTransformers for image recognition at scale. Iclr.\nFan, H., Xiong, B., Mangalam, K., Li, Y ., Yan, Z., Malik, J., & Feichten-\nhofer, C. (2021). Multiscale vision transformers. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision\n(pp. 6824–6835).\nFang, Y ., Liao, B., Wang, X., Fang, J., Qi, J., Wu, R., & Liu, W. (2021).\nYou only look at one sequence: Rethinking transformer in vision\nthrough object detection. Advances in Neural Information Pro-\ncessing Systems, 34, 26183–97.\nFeichtenhofer, C., Fan, H., Malik, J., & He, K. (2019). Slowfast net-\nworks for video recognition. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (pp. 6202–6211).\nGlorot, X., & Bengio, Y . (2010). Understanding the difﬁculty of train-\ning deep feedforward neural networks. In Proceedings of the\nThirteenth International Conference on Artiﬁcial Intelligence and\nStatistics (pp. 249–256).\nGrauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Gird-\nhar, R., et al. (2022). Ego4d: Around the world in 3000 hours of\negocentric video. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition(pp. 18995–19012).\nHao, Y ., Zhang, H., Ngo, C.-W., & He, X. (2022). Group contextu-\nalization for video recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition(pp. 928–\n938).\nHarel, J., Koch, C. & Perona, P. (2006). Graph-based visual saliency.\nAdvances in neural information processing systems.\n19.\nHayhoe, M., & Ballard, D. (2005). Eye movements in natural behavior.\nTrends in Cognitive Sciences, 9(4), 188–194.\nHuang, Y ., Cai, M., Li, Z., Lu, F., & Sato, Y . (2020). Mutual context\nnetwork for jointly estimating egocentric gaze and action. IEEE\nTransactions on Image Processing, 29, 7795–7806.\nHuang, Y ., Cai, M., Li, Z. & Sato, Y . (2018). Predicting gaze in ego-\ncentric video by learning task-dependent attention transition. In\nProceedings of the European conference on computer vision (eccv)\n(pp. 754–769).\nHuang, Y ., Cai, M., & Sato, Y . (2020). An ego-vision system for\ndiscovering human joint attention. IEEE Transactions on Human-\nMachine Systems, 50(4), 306–316.\nHussain, T., Anwar, A., Anwar, S., Petersson, L., & Baik, S.W. (2022).\nPyramidal attention for saliency detection. In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Work-\nshops (CVPRW) (pp. 2877–2887).\nJia, S., & Bruce, N. D. (2020). Eml-net: An expandable multi-layer\nnetwork for saliency prediction. Image and Vision Computing, 95,\n103887.\nJia, W., Liu, M. & Rehg, J.M. (2022). Generative adversarial network for\nfuture hand segmentation from egocentric video. In Proceedings\nof the European Conference on Computer Vision (ECCV).\nJiang, L., Li, Y ., Li, S., Xu, M., Lei, S., Guo, Y . & Huang, B. (2022).\nDoes text attract attention on e-commerce images: A novel saliency\nprediction dataset and method. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp.\n2088–2097).\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijaya-\nnarasimhan, S., et al. (2017). The kinetics human action video\ndataset. arXiv preprint arXiv:1705.06950\nKellnhofer, P., Recasens, A., Stent, S., Matusik, W. & Torralba, A.\n(2019). Gaze360: Physically unconstrained gaze estimation in\nthe wild. In IEEE International Conference on Computer Vision\n(ICCV).\nKhattar, A., Hegde, S. & Hebbalaguppe, R. (2021). Cross-domain\nmulti-task learning for object detection and saliency estimation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (pp. 3639–3648).\nKrafka, K., Khosla, A., Kellnhofer, P., Kannan, H., Bhandarkar, S.,\nMatusik, W., & Torralba, A. (2016). Eye tracking for everyone.\nIn Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (pp. 2176–2184).\nKroner, A., Senden, M., Driessens, K., & Goebel, R. (2020). Contextual\nencoder-decoder network for visual saliency prediction. Neural\nNetworks, 129, 261–270.\nKruthiventi, S. S., Ayush, K., & Babu, R. V . (2017). Deepﬁx: A fully\nconvolutional neural network for predicting human eye ﬁxations.\nIEEE Transactions on Image Processing, 26(9), 4446–4456.\nLai, B., Liu, M., Ryan, F., & Rehg, J. (2022). In the eye of transformer:\nGlobal-local correlation for egocentric gaze estimation. In British\nMachine Vision Conference.\nLee, Y ., Kim, J., Willette, J., & Hwang, S.J. (2022). Mpvit: Multi-path\nvision transformer for dense prediction. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition.\nLi, Y ., Fathi, A., & Rehg, J.M. (2013). Learning to predict gaze in\negocentric video. In Proceedings of the IEEE International Con-\nference on Computer Vision (pp. 3216–3223).\n123\n870 International Journal of Computer Vision (2024) 132:854–871\nLi, Y ., Liu, M., & Rehg, J. (2021). In the eye of the beholder: Gaze\nand actions in ﬁrst person video. In IEEE Transactions on Pattern\nAnalysis and Machine Intelligence.\nLi, Y ., Liu, M., & Rehg, J.M. (2018). In the eye of beholder: Joint\nlearning of gaze and actions in ﬁrst person video. In Proceedings\nof the European Conference on Computer Vision (ECCV)(pp. 619–\n635).\nLi, Y ., Wu, C.-Y ., Fan, H., Mangalam, K., Xiong, B., Malik, J., &\nFeichtenhofer, C. (2022). Mvitv2: Improved multiscale vision\ntransformers for classiﬁcation and detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (pp. 4804–4814).\nLin, S., Xie, H., Wang, B., Yu, K., Chang, X., Liang, X., & Wang, G.\n(2022). Knowledge distillation via the target-aware transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (pp. 10915–10924).\nLiu, M., Ma, L., Somasundaram, K., Li, Y ., Grauman, K., Rehg, J.M., &\nLi, C. (2022). Egocentric activity recognition and localization on a\n3d map. In Proceedings of the European Conference on Computer\nVision (ECCV).\nLiu, M., Tang, S., Li, Y ., & Rehg, J.M. (2020). Forecasting human-\nobject interaction: joint prediction of motor attention and actions\nin ﬁrst person video. In Proceedings of the European Conference\non Computer Vision (ECCV) (pp. 704–721).\nLiu, N., Han, J., & Yang, M.-H. (2018). Picanet: Learning pixel-wise\ncontextual attention for saliency detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition\n(pp. 3089–3098).\nLiu, N., Nan, K., Zhao, W., Yao, X., & Han, J. (2023). Learning com-\nplementary spatial–temporal transformer for video salient object\ndetection. IEEE Transactions on Neural Networks and Learning\nSystems.\nLiu, N., Zhang, N., Wan, K., Shao, L., & Han, J. (2021). Visual saliency\ntransformer. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (pp. 4722–4732).\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., & Stoyanov, V .\n(2019). Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., & Guo, B. (2021).\nSwin transformer: Hierarchical vision transformer using shifted\nwindows. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (pp. 10012–10022).\nLiu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., & Hu, H.\n(2022). Video swin transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp.\n3202–3211).\nLoshchilov, I., & Hutter, F. (xxxx). Decoupled weight decay regulariza-\ntion. In International Conference on Learning Representations.\nLoshchilov, I., & Hutter, F. (2016). Sgdr: Stochastic gradient descent\nwith warm restarts. In International Conference on Learning Rep-\nresentations.\nLou, J., Lin, H., Marshall, D., Saupe, D., & Liu, H. (2021). Transal-\nnet: Visual saliency prediction using transformers. arXiv preprint\narXiv:2110.03593\nMa, C., Sun, H., Rao, Y ., Zhou, J., & Lu, J. (2022). Video saliency fore-\ncasting transformer. In IEEE Transactions on Circuits and Systems\nfor Video Technology.\nMacInnes, J.J., Iqbal, S., Pearson, J., & Johnson, E.N. (2018). Wearable\neye-tracking for research: Automated dynamic gaze mapping and\naccuracy/precision comparisons across devices. BioRxiv. 299925\nNaas, S.-A., Jiang, X., Sigg, S., & Ji, Y . (2020). Functional gaze predic-\ntion in egocentric video. In Proceedings of the 18th International\nConference on Advances in Mobile Computing & Multimedia(pp.\n40–47).\nNeimark, D., Bar, O., Zohar, M., & Asselmann, D. (2021). Video trans-\nformer network. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (pp. 3163–3172).\nNonaka, S., Nobuhara, S., & Nishino, K. (2022). Dynamic 3d gaze from\nafar: Deep gaze estimation from temporal eye-head-body coordi-\nnation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (cvpr)(p. 2192-2201).\nPan, J., Ferrer, C.C., McGuinness, K., O’Connor, N.E., Torres, J.,\nSayrol, E., & Giro-i Nieto, X. (2017). Salgan: Visual saliency\nprediction with generative adversarial networks. arXiv preprint\narXiv:1701.01081\nPatrick, M., Campbell, D., Asano, Y ., Misra, I., Metze, F., Feichtenhofer,\nC., Henriques, J. F., et al. (2021). Keeping your eye on the ball:\nTrajectory attention in video transformers. Advances in Neural\nInformation Processing Systems, 34, 12493–12506.\nRen, S., Zhou, D., He, S., Feng, J., & Wang, X. (2022). Shunted self-\nattention via multi-scale token aggregation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion.\nSoo Park, H., & Shi, J. (2015). Social saliency prediction. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (pp. 4777–4785).\nStrudel, R., Garcia, R., Laptev, I., & Schmid, C. (2021). Segmenter:\nTransformer for semantic segmentation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (pp.\n7262–7272).\nSun, Y ., Zhao, M., Hu, K., & Fan, S. (2022). Visual saliency prediction\nusing multi-scale attention gated network. Multimedia Systems,\n28(1), 131–139.\nTavakoli, H.R., Rahtu, E., Kannala, J., & Borji, A. (2019). Digging\ndeeper into egocentric gaze prediction. In 2019 IEEE Winter\nConference on Applications of Computer Vision (WACV)(pp. 273–\n282).\nThakur, S.K., Beyan, C., Morerio, P., & Del Bue, A. (2021). Predicting\ngaze from egocentric social interaction videos and imu data. In\nProceedings of the 2021 International Conference on Multimodal\nInteraction (pp. 717–722).\nTsiami, A., Koutras, P., & Maragos, P. (2020). Stavis: Spatio-temporal\naudiovisual saliency network. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp.\n4766–4776).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., & Polosukhin, I. (2017). Attention is all you need. Advances\nin neural information processing systems. 30\nWang, H., Zhu, Y ., Adam, H., Yuille, A., & Chen, L.-C. (2021). Max-\ndeeplab: End-to-end panoptic segmentation with mask transform-\ners. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition(pp. 5463–5474).\nWang, J., & Torresani, L. (2022). Deformable video transformer. In\nProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (pp. 14053–14062).\nWang, L., Lu, H., Ruan, X., & Yang, M.-H. (2015). Deep networks for\nsaliency detection via local estimation and global search. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (pp. 3183–3192).\nWang, W., Shen, J., & Shao, L. (2017). Video salient object detection\nvia fully convolutional networks. IEEE Transactions on Image\nProcessing, 27(1), 38–49.\nWang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., & Shao,\nL. (2021). Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (pp.\n568–578).\nWang, X., Wu, Y ., Zhu, L., & Yang, Y . (2020). Symbiotic attention\nwith privileged information for egocentric action recognition. In\n123\nInternational Journal of Computer Vision (2024) 132:854–871 871\nProceedings of the AAAI Conference on Artiﬁcial Intelligence(V ol.\n34, pp. 12249–12256).\nWang, Z., Liu, Z., Li, G., Wang, Y ., Zhang, T., Xu, L., & Wang, J.\n(2021). Spatio-temporal self-attention network for video saliency\nprediction. IEEE Transactions on Multimedia.\nWu, X., Wu, Z., Zhang, J., Ju, L., & Wang, S. (2020). Salsac: A\nvideo saliency prediction model with shufﬂed attentions and\ncorrelation-based convlstm. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence (V ol. 34, pp. 12410–12417).\nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., & Gao, J. (2021).\nFocal self-attention for local-global interactions in vision trans-\nformers. arXiv preprint arXiv:2107.00641\nYang, S., Lin, G., Jiang, Q., & Lin, W. (2019). A dilated inception\nnetwork for visual saliency prediction. IEEE Transactions on Mul-\ntimedia, 22(8), 2163–2176.\nYarbus, A. L. (2013). Eye Movements and Vision. Springer.\nYe, Z., Li, Y ., Fathi, A., Han, Y ., Rozga, A., Abowd, G.D., & Rehg,\nJ.M. (2012). Detecting eye contact using wearable eye-tracking\nglasses. In Proceedings of the 2012 ACM Conference on Ubiqui-\ntous Computing (pp. 699–704).\nZhang, H., Cisse, M., Dauphin, Y .N., & Lopez-Paz, D. (2017). mixup:\nBeyond empirical risk minimization. In International Conference\non Learning Representations.\nZhang, M., Ma, K. T., Lim, J. H., Zhao, Q., & Feng, J. (2018). Antic-\nipating where people will look using adversarial networks. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 41(8),\n1783–1796.\nZhang, M., Teck Ma, K., Hwee Lim, J., Zhao, Q., & Feng, J. (2017).\nDeep future gaze: Gaze anticipation on egocentric videos using\nadversarial networks. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition(pp. 4372–4381).\nZhang, W., Huang, Z., Luo, G., Chen, T., Wang, X., Liu, W., & Shen, C.\n(2022). Topformer: Token pyramid transformer for mobile seman-\ntic segmentation. Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition(pp. 12083–12093).\nZheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., et al. (2021).\nRethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In Proceedings of the ieee/cvf\nconference on computer vision and pattern recognition(pp. 6881–\n6890).\nZhuge, M., Fan, D.-P., Liu, N., Zhang, D., Xu, D., & Shao, L. (2022).\nSalient object detection via integrity learning. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 45(3), 3738–52.\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7883466482162476
    },
    {
      "name": "Gaze",
      "score": 0.7734118103981018
    },
    {
      "name": "Artificial intelligence",
      "score": 0.63686203956604
    },
    {
      "name": "Fixation (population genetics)",
      "score": 0.6258214116096497
    },
    {
      "name": "Security token",
      "score": 0.6175991296768188
    },
    {
      "name": "Correlation",
      "score": 0.5080718994140625
    },
    {
      "name": "Encoder",
      "score": 0.4564928412437439
    },
    {
      "name": "Computer vision",
      "score": 0.44357430934906006
    },
    {
      "name": "Mathematics",
      "score": 0.12468701601028442
    },
    {
      "name": "Population",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Demography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}