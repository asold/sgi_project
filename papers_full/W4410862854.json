{
    "title": "Large Language Model and Traditional Machine Learning Scoring of Evolutionary Explanations: Benefits and Drawbacks",
    "url": "https://openalex.org/W4410862854",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2104583822",
            "name": "Pan Yunlong",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A269401176",
            "name": "Ross H. Nehm",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2104583822",
            "name": "Pan Yunlong",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A269401176",
            "name": "Ross H. Nehm",
            "affiliations": [
                "Stony Brook University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2073725417",
        "https://openalex.org/W4393119218",
        "https://openalex.org/W6998825787",
        "https://openalex.org/W3204300593",
        "https://openalex.org/W2102716846",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W2171648197",
        "https://openalex.org/W4380084457",
        "https://openalex.org/W2164777277",
        "https://openalex.org/W6859166728",
        "https://openalex.org/W6859621498",
        "https://openalex.org/W4407296154",
        "https://openalex.org/W4319999560",
        "https://openalex.org/W2052929314",
        "https://openalex.org/W4403122979",
        "https://openalex.org/W6786078782",
        "https://openalex.org/W4386266200",
        "https://openalex.org/W4292425807",
        "https://openalex.org/W2161573472",
        "https://openalex.org/W2074360952",
        "https://openalex.org/W4404924486",
        "https://openalex.org/W1976199158",
        "https://openalex.org/W1512098439",
        "https://openalex.org/W2011665208",
        "https://openalex.org/W3037865544",
        "https://openalex.org/W4392389129",
        "https://openalex.org/W4386116667",
        "https://openalex.org/W4249895324",
        "https://openalex.org/W4319068540",
        "https://openalex.org/W4392545710",
        "https://openalex.org/W4391250553",
        "https://openalex.org/W3200809495",
        "https://openalex.org/W4408711855",
        "https://openalex.org/W4385632485",
        "https://openalex.org/W6959923468",
        "https://openalex.org/W4381930693",
        "https://openalex.org/W4361213626",
        "https://openalex.org/W3101804645",
        "https://openalex.org/W3012071850",
        "https://openalex.org/W2969717756",
        "https://openalex.org/W4389500750",
        "https://openalex.org/W3104822645",
        "https://openalex.org/W4392202731",
        "https://openalex.org/W4393161681"
    ],
    "abstract": "Few studies have compared Large Language Models (LLMs) to traditional Machine Learning (ML)-based automated scoring methods in terms of accuracy, ethics, and economics. Using a corpus of 1000 expert-scored and interview-validated scientific explanations derived from the ACORNS instrument, this study employed three LLMs and the ML-based scoring engine, EvoGrader. We measured scoring reliability (percentage agreement, kappa, precision, recall, F1), processing time, and explored contextual factors like ethics and cost. Results showed that with very basic prompt engineering, ChatGPT-4o achieved the highest performance across LLMs. Proprietary LLMs outperformed open-weight LLMs for most concepts. GPT-4o achieved robust but less accurate scoring than EvoGrader (~500 additional scoring errors). Ethical concerns over data ownership, reliability, and replicability over time were LLM limitations. EvoGrader offered superior accuracy, reliability, and replicability, but required, in its development a large, high-quality, human-scored corpus, domain expertise, and restricted assessment items. These findings highlight the diversity of considerations that should be used when considering LLM and ML scoring in science education. Despite impressive LLM advances, ML approaches may remain valuable in some contexts, particularly those prioritizing precision, reliability, replicability, privacy, and controlled implementation.",
    "full_text": null
}