{
  "title": "Chain-of-Thought Reasoning in Tabular Language Models",
  "url": "https://openalex.org/W4389520282",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2120800086",
      "name": "Mingyu Zheng",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1985984607",
      "name": "Hao Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098983616",
      "name": "Jiang Wenbin",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2085002608",
      "name": "Zheng Lin",
      "affiliations": [
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2770769292",
      "name": "Yajuan Lyu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2502654617",
      "name": "Qiao-Qiao She",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2101634296",
      "name": "Weiping Wang",
      "affiliations": [
        "Institute of Information Engineering",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4377130677",
    "https://openalex.org/W3116342879",
    "https://openalex.org/W3168052339",
    "https://openalex.org/W4385569882",
    "https://openalex.org/W3174679944",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4298184221",
    "https://openalex.org/W4378713467",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W3197798882",
    "https://openalex.org/W3184222203",
    "https://openalex.org/W4205508242",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4285602429",
    "https://openalex.org/W4295312788"
  ],
  "abstract": "Tabular mathematical reasoning task requires models to perform multi-step operations including information look-up and numerical calculation, based on heterogeneous data from tables and questions. Existing solutions tend to extend chain-of-thought (CoT) reasoning into powerful large language models (LLMs) to promote multi-hop mathematical reasoning. However, such LLM-based approaches are not a viable solution in the scenario of privatization deployment or limited resources. To address this problem, we revisit small-scale tabular language models (TaLMs) and extend chain-of-thought reasoning into TaLMs for the first time. Specifically, we propose a novel framework, TaCo, which coordinates two TaLMs responsible for CoT generation and answer inference, respectively. Besides, our framework can be combined with an external calculator to enhance accurate numerical calculation. On the TABMWP dataset, TaCo outperforms the state-of-the-art ChatGPT by 9.55% (82.60%→92.15% in accuracy) with much less parameters (0.8B). The code will be released along with the paper.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11006–11019\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nChain-of-Thought Reasoning in Tabular Language Models\nMingyu Zheng1,2†, Yang Hao3, Wenbin Jiang3 , Zheng Lin1,2‡,\nYajuan Lyu3, Qiaoqiao She3, Weiping Wang1\n1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\n2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China\n3Baidu Inc, Beijing, China\n{zhengmingyu,linzheng,wangweiping}@iie.ac.cn\n{haoyang03,jiangwenbin,lvyajuan,sheqiaoqiao}@baidu.com\nAbstract\nTabular mathematical reasoning task requires\nmodels to perform multi-step operations includ-\ning information look-up and numerical calcula-\ntions, based on heterogeneous data from tables\nand questions. Existing solutions tend to extend\nchain-of-thought (CoT) reasoning into power-\nful large language models (LLMs) to promote\nmulti-hop mathematical reasoning. However, it\ncan be extremely difficult to apply such LLM-\nbased approaches under scenarios of privatiza-\ntion deployment or limited resources. To ad-\ndress this problem, we revisit small-scale tabu-\nlar language models (TaLMs) and extend chain-\nof-thought reasoning into TaLMs for the first\ntime. Specifically, we propose a novel frame-\nwork, TaCo, which coordinates two TaLMs\nresponsible for CoT generation and answer\ninference, respectively. Besides, our frame-\nwork can be combined with an external cal-\nculator to enhance accurate numerical calcula-\ntions. On the TABMWP dataset, TaCo outper-\nforms the state-of-the-art ChatGPT by 9.55%\n(82.60%→92.15% in accuracy) with much less\nparameters (0.8B).1\n1 Introduction\nTabular mathematical reasoning task aims at an-\nswering math questions based on heterogeneous\ntabular and textual data, which can provide users\nwith insights from tables containing valuable fig-\nures (Lu et al., 2023b; Zhu et al., 2021; Chen et al.,\n2021b). This task highlights the demand for multi-\nstep mathematical reasoning including information\nlook-up and numerical calculations. For example,\ngiven the table and the question in Figure 1, we\nfirstly need to count how many numbers are in the\ntable, then add all the numbers together to get the\nsum of baskets, and finally compute the mean of\nthe sum.\n1The code will be released at https://github.com/\nSpursGoZmy/TaCo\n†This work was done during an internship at Baidu Inc.\n‡ Corresponding author: Zheng Lin.\nFigure 1: An example from the TABMWP dataset. To\nsolve the problem, the model needs to perform multi-\nstep mathematical reasoning based on the table and the\nquestion.\nConsidering the inherent demand for multi-step\noperations, existing studies tend to extend chain-\nof-thought (CoT) reasoning (Wei et al., 2022;\nWang et al., 2023a; Kojima et al., 2022; Zhang\net al., 2022) into powerful Large Language Mod-\nels (LLMs) (Brown et al., 2020; Chowdhery et al.,\n2022; Thoppilan et al., 2022; Chen et al., 2021a)\nto promote multi-hop mathematical reasoning. As\ndepicted in Figure 2 (b), this paradigm prompts\nLLMs with several in-context examples containing\nCoT demonstrations to elicit intermediate reason-\ning steps before inferring the final answer.\nThough the combo of LLM and CoT has\nachieved great performance, such LLM-based\nmethods may not be a feasible approach in some\nreal-world scenarios. For instance, it is financially\nexpensive to satisfy the high computational require-\nments, the storage capacity and the desired band-\nwidth of LLMs, which makes it a challenge for\n11006\nindividual users or small organizations to utilize\nLLMs in their applications (Strubell et al., 2019;\nBender et al., 2021). In consideration of the data\nsecurity, enterprises may also seek privatization\ndeployments where private data is not allowed to\nbe processed by third-party LLM APIs. What’s\nmore, despite the fact that many pre-trained tab-\nular language models have been developed (Liu\net al., 2022; Herzig et al., 2020; Wang et al., 2021;\nDong et al., 2022), their CoT reasoning ability has\nnot been thoroughly investigated and it could be\ninadequate for solving the tabular mathematical\nreasoning task. As a result, an alternative approach,\nwith lower costs and competitive CoT reasoning\nability, is needed.\nTo accomplish this goal, we revisit small-scale\ntabular language models (TaLMs) and initiatively\nexplore the chain-of-thought reasoning in TaLMs.\nSpecifically, we propose a novel framework named\nTaCo, which coordinates two TaLMs that are re-\nsponsible for CoT generation and answer inference,\nrespectively. Given the input table and question,\nthe first TaLM is fine-tuned to generate interme-\ndiate reasoning steps. Based on the original input\nand generated reasoning steps, the second TaLM\nis fine-tuned to infer the final answer. To alleviate\nthe weakness of TaLMs in solving mathematical\nexpressions, TaCo is also combined with an ex-\nternal calculator which is used to perform math\ncalculations and fix incorrect results in the output\nreasoning steps.\nTo verify the effectiveness of the proposed\nmethod, we conduct comprehensive experiments\non the TABMWP (Lu et al., 2023b) dataset,\nwhich is the latest math word problem benchmark\nover tabular data and provides detailed chain-of-\nthoughts to solve the problem step by step. Ex-\nperimental results reveal that TaCo explores a new\nand promising paradigm for tabular mathematical\nreasoning, which is illustrated in Figure 2 (c). Com-\npared with traditional fine-tuned TaLMs, TaCo im-\nproves the accuracy of recent TAPEX model by\n29.76%. Compared with LLM-based approaches,\nTaCo outperforms the state-of-the-art ChatGPT by\n9.55% (82.60%→92.15%) with much less param-\neters (0.8B). Moreover, we conduct ablation stud-\nies to analyse contributions of different parts in\nthe framework. The detailed error analysis is also\nperformed to provide insights for future improve-\nments.\nTo summarize, we conclude our contributions as\nfollows:\n• To the best of our knowledge, we explore the\nchain-of-thought reasoning in TaLMs for the\nfirst time, and advocate a new and promising\nparadigm for tabular mathematical reasoning,\nespecially under scenarios where LLM-based\nmethods are not feasible.\n• We propose a novel framework, TaCo, which\ncoordinates two TaLMs responsible for CoT\ngeneration and answer inference, respectively.\nIt is also integrated with a calculator to en-\nhance accurate numerical calculations.\n• Our method can boost the performance of\nsmall-scale TaLMs and surpasses the state-\nof-the-art ChatGPT by 9.55% on TABMWP\nbenchmark with much less parameters (0.8B).\n2 Pilot Experiment\nBefore diving into the specific method, we present\na pilot experiment on the TABMWP dataset to an-\nswer two important questions: (i) Do existing pre-\ntrained generative TaLMs possess chain-of-thought\nreasoning ability? (ii) Whether generative TaLMs\ncan benefit from chain-of-thoughts when predict-\ning the final answer. We select the state-of-the-\nart TAPEX model (Liu et al., 2022) for experi-\nments, which is based on the encoder-decoder lan-\nguage model BART (Lewis et al., 2020) and is\nadditionally pre-trained on the tabular data. We\nconsider two model sizes: TAPEX-base (140M)\nand TAPEX-large (400M).\nExperiments are conducted in three different set-\ntings, i.e., vanilla, zero-shot CoT and gold CoT. For\nthe “vanilla” setting, the pre-trained TAPEX model\nf(·) autoregressively generates the answer a based\non the table t and the question q, i.e., a = f(t, q).\nFor the “zero-shot CoT” setting, we follow Ko-\njima et al. (2022) to evaluate the CoT reasoning\nof the TAPEX. Specifically, a trigger sentence p1\nis appended to the question in order to ask the\nTAPEX to output intermediate reasoning steps s,\ni.e., s = f(t, q, p1). Then, given the original input\nand the generated CoT, another trigger sentence p2\nis appended to make the TAPEX output the final\nanswer a, i.e., a = f(t, q, p1, s, p2). For p1, we\ntry various templates such as “Let’s think step by\nstep” and report best results. For p2, we intuitively\nselect “As a result, the answer is” as the trigger\nsentence. For the “gold CoT” setting, we replace\n11007\nFigure 2: Different paradigms for tabular mathematical reasoning.\ngenerated reasoning steps with annotated ones and\nother procedures are same as “zero-shot CoT”.\nPre-trained TaLMs Acc-Dev Acc-Test\nTAPEX-base (vanilla) 15.66 15.69\nTAPEX-large (vanilla) 18.41 18.59\nTAPEX-base (zero-shot CoT) 15.30 15.25\nTAPEX-large (zero-shot CoT) 18.25 17.94\nTAPEX-base (gold CoT) 40.54 39.99\nTAPEX-large (gold CoT) 47.48 48.01\nTable 1: Pilot experimental results of pre-trained\nTAPEX under different settings. “Acc-Dev” and “Acc-\nTest” represents accuracy on the development set and\nthe test set respectively.\nFrom the results in Table 1, we can see that\nthe TAPEX with “zero-shot CoT” setting performs\neven worse than the vanilla one, which shows that\nthe small-scale TAPEX is not a decent zero-shot\nreasoner like LLMs and does not possess CoT rea-\nsoning ability. This is also consistent with find-\nings from previous CoT studies (Wei et al., 2022;\nHo et al., 2023). After inspecting the model out-\nputs, we find that the pre-trained TAPEX model\ncannot follow the instruction to generate reason-\ning steps. In most cases, it directly generates the\nanswer or illogical texts. However, given the anno-\ntated “gold CoT”, the model achieves a remarkable\nperformance gain. For instance, the accuracy of\nTAPEX-large on test set increases from 18.59%\nto 48.01%. This demonstrates that CoT reasoning\nsteps are beneficial to TAPEX when inferring the\ncorrect answer and it encourages us to further elicit\nCoT reasoning ability of TaLMs by finetuning.\n3 Method\nBased on observations in Section 2, we propose\nthe TaCo framework for tabular mathematical rea-\nsoning. It includes two training stages: (i) CoT\ngeneration and (ii) answer inference, where two\ngenerative TaLMs with the same architecture are\nfine-tuned independently with different inputs and\noutputs. In this section, we introduce the frame-\nwork with the TAPEX model as selected backbones,\nbut it should be noted that TaCo is compatible with\narbitrary generative TaLMs to boost their perfor-\nmance. The overview of TaCo framework is illus-\ntrated in Figure 3.\n3.1 CoT Generation\nIn the CoT generation stage, a TAPEX model is\nfine-tuned to generate a solution which consists\nof multiple reasoning steps to solve the problem.\nGiven an input table T with M rows {Ri}M\ni=1\nand N column headers {cj}N\nj=1, the TAPEX will\nlinearize the table into a flattened text sequence\nT∗ = [HEAD] : c1 | ··· |cN [ROW] 1 :\nR1 |[ROW] 2 : R2 | ··· |RM, where [HEAD]\nand [ROW] are special tokens used to indicate the\nregion of column headers and rows, respectively.\nThe number after [ROW] represents different row\nindex and the vertical bar “|” separates headers or\ncells in different columns. For instance, the table\nin Figure 1 will be linearized into the following\nsequence:\ncol : Day | Number of baskets row 1 :\nThursday | 49 row 2 : Friday | 48 ... row\n6 : Tuesday | 49\nThe resulting sequence T∗will be concatenated\nwith the textual context, which includes a question\nQ and a trigger sentence P. Based on the concate-\nnated input, the probability of generating the target\nsolution S is computed as follows:\n11008\nFigure 3: Overview of the TaCo framework, with the table and the question in Figure 1 as a running example.\np(S|T∗, Q, P) =\nL∏\ni=1\npθ(Si|T∗, Q, P, S<i) (1)\nwhere L is the length of target solution. We select\n“Let’s think step by step” as the trigger sentence P\nsince it gives the best performance in pilot experi-\nments.\nAfter generating a potential solution ¯S, we find\nthat ¯S often contains some numerical calculation\nerrors. This is often the case with language models\nbecause TaLMs and even LLMs are not suitable for\nactually solving mathematical expressions (Chen\net al., 2022). Take the generated solution in Figure\n3 as an example. Though the model generates plau-\nsible reasoning steps, calculation results among\nthese steps are all wrong (in red color), e.g., “49 +\n48 + 51 + 54 + 37 + 49 = 312”. Such calculation\nerrors will accumulate to the last reasoning step\nand seriously mislead the answer inference model\ninto predicting the false answer.\nTo mitigate the influence of calculation mistakes,\nwe introduce an arithmetic calculator g(·) to solve\nmathematical expressions of “+,-,×,÷” in the gen-\nerated solution ¯S and output the corrected solu-\ntion ˆS = g( ¯S). Concretely, we extract equation\nstrings in ¯S using regular expressions and calculate\ntheir results using the Python eval function. Since\nmultiple equations may exist in one solution and\none equation could also refer to results of previous\nequations, the calculation result of each equation\nis propagated to the following equations by string\nreplacing. As we can see from Figure 3, original\nwrong results in ¯S are successfully fixed and are\nreplaced with correct results (in green color), e.g.,\n“49 + 48 + 51 + 54 + 37 + 49 = 288”.\n3.2 Answer Inference\nIn answer inference stage, another TAPEX model is\nfine-tuned to generate the final ansewr based on the\noriginal input and the annotated solutionS. Similar\nwith the CoT generation stage, the probability of\ngenerating target answer A is computed by:\np(A|T∗, Q, P, S) =\nN∏\ni=1\npθ(Ai|T∗, Q, P, S, A<i)\n(2)\nwhere N is the length of target answer. During the\ninference phase, the annotated solution is replaced\nwith the corrected solution ˆS to output the pre-\ndicted answer ¯A. Both CoT generation model and\nanswer inference model are trained with a standard\nlanguage modeling objective.\n4 Experiments\n4.1 Dataset and Evaluation Metric\nExperiments are conducted on the TABMWP (Lu\net al., 2023b) dataset, a recent large-scale bench-\nmark which is constructed from grade-level math\ncurricula and contains 38,481 math word problems\nwith the tabular context. Beside the gold answers,\nTABMWP also provides detailed step-by-step solu-\ntions to solve the problems, which can be utilized as\nchain-of-thoughts to finetuning TaLMs. There are\ntwo question-types in the TABMWP: 28,719free-\ntext questions with integer answers (INT) and deci-\nmal answers (DEC), and 9,712 multi-choice ques-\ntions with extractive text answers (EXTR), boolean\ntext answers (BOOL) and other text answers (OTH).\nStatistics of each split are shown in the Table 2. The\ntest set contains 7,686 questions in total. Among\nthem, 74.08% are INT (4,529) and DEC (1165)\nquestions, and 25.92% are DEC (1,165), EXTR\n(987) and OTH (105) questions. Thus, INT and\n11009\nTrain Dev Test Total\n# of questions 23,059 7,686 7,686 38,431\n# of free-text 17,135 5,710 5,694 28,719\n# of multi-choice 5,744 1,976 1,992 9,712\n# of tables 22,620 7,546 7,549 37,644\n# of solutions 21,623 7,365 7,378 35,442\nTable 2: Dataset statistics of TABMWP.\nDEC questions are more essential for the overall ac-\ncuracy. Given the predicted answer and the ground\ntruth, we employ the exact match accuracy as the\nmetric and use the official evaluation script to eval-\nuate the model performance.\n4.2 Implementation Details\nImplementations. Our framework is imple-\nmented with Pytorch (Paszke et al., 2019). We\nmainly employ the TAPEX (Liu et al., 2022) as the\nbackbone TaLM in the proposed framework. We\nalso replace TAPEX with UnifiedQA (Khashabi\net al., 2020) for the ablation study. Various model\nsizes are included to present more valid evaluation\nacross different model capacities. Both CoT gener-\nation model and answer inference model are opti-\nmized by AdamW (Loshchilov and Hutter, 2019).\nWe use validation set for the model selection and\nmanually tune hyper-parameters, and evaluate the\nbest model on the test set. For CoT generation, we\nadopt the beam search decoding with the beam size\nof 3. For answer inference, we adopt the greedy\ndecoding. Hyper-parameter configurations for best-\nperforming models and more implementation de-\ntails are shown in the Table 6 and Table 7.\nBaselines. (1) Pre-trained and Fine-tuned lan-\nguage models: We develop TAPEX (Liu et al.,\n2022) and UnifiedQA (Khashabi et al., 2020) in\nboth pre-trained and fine-tuned settings to predict\nthe final answer. TAPEX is the state-of-the-art\nBART-based (Lewis et al., 2020) TaLM which is\npre-trained on the tabular data to mimic a SQL\nexecutor. UnifiedQA is a T5-based (Raffel et al.,\n2020) QA model which is pre-trained on 8 QA\ndatasets of multiple formats. We consider three\nmodel sizes for UnifiedQA: small (60M), base\n(220M) and large (770M). Given the flattened table\nand question, both TAPEX and UnifiedQA can gen-\nerate the answer text autoregressively. (2) Large\nlanguage models: We consider GPT-3 (Brown\net al., 2020), Codex (Chen et al., 2021a) and Chat-\nGPT with the standard few-shot and zero-shot\nprompting. ChatGPT is based on the gpt-3.5-turbo\nengine. Numbers of in-context examples and se-\nlection strategies for few-shot prompting are listed\nin Table 8. (3) Large language models with CoT\nprompting: Beside standard prompting, we also\nconsider above LLMs with the chain-of-thought\nprompting. PromptPG (Lu et al., 2023b) utilizes\nthe policy gradient method to select in-context\nexamples for test samples when constructing the\nprompt for LLMs. PoT (Chen et al., 2022) pro-\nposes the “program-of-thoughts”, which exploits\nCodex to generate the text and Python program for\nmath computations. The generated program is exe-\ncuted by a program interpreter to output the final\nanswer. The “Heuristic guess” is a baseline from\nthe TABMWP paper. For multi-choice questions, it\nrandomly selects one from the given options with\neven probabilities. For free-text questions, it ran-\ndomly chooses one number from the question or\nthe table as the prediction.\n4.3 Main Results\nTable 3 demonstrates main experimental results on\nthe TABMWP dataset. For TAPEX, UnifiedQA\nand ChatGPT baselines, we report results based on\nour implementation. For other baselines, we report\npublished results from original papers (Lu et al.,\n2023b; Chen et al., 2022).\nFrom the results in Table 3, we can find that: (1)\nWith two TAPEX-large models as backbones, the\nTaCo framework establishes a new state-of-the-art\naccuracy of 92.15% on the TABMWP test set, out-\nperforming the previous best model ChatGPT with\nCoT prompting by 9.55%, which demonstrates the\neffectiveness of the proposed method. Notably,\ncompared with LLMs such as GPT-3 and Codex,\nthe parameters in TaCo framework are much less\n(0.8B), which brings lower costs for application\ndeployments. (2) Compared with LLM-based ap-\nproaches with the standard few-shot prompting,\nfine-tuned TAPEX and UnifiedQA can achieve\ncompetitive results. For instance, the fine-tuned\nTAPEX-large even performs better than GPT-3 and\nCodex. However, when combined with the CoT\nprompting, LLM-based methods are significantly\nbetter than fine-tuned small-scale language models,\nwhich shows that the CoT prompting plays an im-\nportant role in the tabular mathematical reasoning\ntask. By contrast, the TaCo framework extends\nthe CoT reasoning into TaLMs for the first time,\nand improves the performance of TAPEX-base and\nTAPEX-large model by 29.19% and 29.76%, re-\nspectively.\n11010\nModel Acc-Dev Acc-Test\nQuestion Types Answer Types Grades\nFREE MC INT DEC EXTR BOOL OTH 1-6 7-8\nHeuristic baselines\nHeuristic guess - 15.29 6.71 39.81 8.37 0.26 30.80 51.22 26.67 17.55 12.27\nHuman performance - 90.22 84.61 93.32 84.95 83.29 97.18 88.69 96.20 94.27 81.28\nPre-trained LM\nTAPEX-base 15.66 15.69 7.29 39.71 8.63 2.06 34.95 47.11 20.95 18.6 11.81\nTAPEX-large 18.41 18.59 8.80 46.59 10.62 1.72 46.91 48.11 30.48 22.65 13.18\nUnifiedQA-small 10.71 12.18 1.18 43.62 1.37 0.43 38.7 49.78 37.14 15.57 7.65\nUnifiedQA-base 12.10 14.56 4.60 43.02 5.28 1.97 37.08 50.11 38.1 17.14 11.11\nUnifiedQA-large 14.00 14.06 3.37 44.63 4.02 0.86 40.53 50.22 35.24 17.21 9.87\nFine-tuned LM\nTAPEX-base 57.10 56.39 48.33 79.42 56.33 17.25 90.37 67.78 76.19 65.17 44.67\nTAPEX-large 62.28 62.39 55.50 82.08 64.21 21.63 96.47 65.78 77.14 71.32 50.47\nUnifiedQA-small 35.79 34.82 27.99 54.32 33.94 4.89 52.99 53.89 70.48 42.23 24.93\nUnifiedQA-base 51.89 51.08 42.10 76.76 49.83 12.02 89.16 63.33 75.24 59.03 40.48\nUnifiedQA-large 59.35 59.26 51.62 81.12 60.68 16.39 92.20 69.44 77.14 67.11 48.80\nLLM\nGPT-3 (zero-shot) - 56.96 53.57 66.67 55.55 45.84 78.22 55.44 54.29 63.37 48.41\nGPT-3 - 57.13 54.69 64.11 58.36 40.40 75.95 52.41 53.02 63.10 49.16\nCodex - 59.40 - - - - - - - - -\nChatGPT 64.12 65.52 65.84 64.61 66.55 63.09 74.67 54.67 55.24 69.75 59.88\nLLM+CoT\nGPT-3 (zero-shot) - 57.61 54.36 66.92 55.82 48.67 78.82 55.67 51.43 63.62 49.59\nGPT-3 - 62.92 60.76 69.09 60.04 63.58 76.49 61.19 67.30 68.62 55.31\nCodex - 65.20 - - - - - - - - -\nPromptPG - 68.23 66.17 74.11 64.12 74.16 76.19 72.81 65.71 71.20 64.27\nCodex-SC - 75.40 - - - - - - - - -\nPoT - 73.20 - - - - - - - - -\nPoT-SC - 81.80 - - - - - - - - -\nChatGPT 82.49 82.60 80.89 87.50 79.36 86.87 81.86 94.00 84.76 82.68 82.51\nOurs\nTaCo (TAPEX-base) 86.12±0.13 85.58±0.14 85.53 85.74 85.29 86.44 93.31 77.89 81.90 87.43 83.12\nTaCo (TAPEX-large) 92.91±0.17 92.15±0.13 91.69 93.47 92.54 88.41 96.05 91.44 86.67 92.37 91.86\nTable 3: Accuracy (%) on the development set and test set of TABMWP. We also report detailed accuracy on\ndifferent types of questions in test set. FREE: free-text questions; MC: multi-choice questions. INT: integer answers;\nDEC: decimal answers; EXTR: extractive text answers; BOOL: Boolean text answers; OTH: other text answers.\nThe best results are marked in bold. ±stands for standard deviation over 3 repeated experiments. If not otherwise\nspecified, LLM baselines are in few-shot setting. “-SC” represents using self-consistency decoding strategy (Wang\net al., 2023a).\n(3) Among different baselines, the model per-\nformance on free-text questions is obviously worse\nthan that on multi-choice questions, with an aver-\nage difference of 21%. The reason is that, com-\npared with multi-choice questions, free-text ques-\ntions usually require more complicated numerical\ncalculations and also do not directly provide an-\nswer options in the input. The detailed evidence is\npresented in the Appendix B. Nevertheless, from\npre-trained LM to LLM+CoT and to the proposed\nTaCo framework, the performance gap between\ntwo question types gradually decreases. For in-\nstance, the accuracy gap of TaCo (TAPEX-large)\nframework (1.78%) is much lower than that of\nfine-tuned TAPEX-large (26.58%). This shows\nour method can obtain better generalization on two\ntypes of questions. (4) Considering questions of\nvarious answer types, the TaCo framework beats\nother baselines on questions with integer (INT) and\ndecimal (DEC) answers, which may resulted from\nthe utilization of the external calculator. ChatGPT\nwith the CoT prompting outperforms other meth-\nods including the human baseline on questions with\nBoolean text answer, which may contribute to its\ngreat general semantic understanding ability. For\nexample, judging yes/no questions based on previ-\nously generated reasoning steps. (5) Not surpris-\ningly, all the models perform worse on questions\nfrom the grade 7-8 than that from the grade 1-6 due\nto the increasing difficulty. Among them, the pro-\nposed framework achieves the best accuracy than\nother baselines on harder questions from grade 7-8.\n4.4 Ablation Study\nWe conduct ablation experiments to systematically\ninvestigate the effect of the external calculator,\nthe progressive two-stage paradigm and the TaLM\n11011\nSettings Dev Test Average\nDrop↓\nQuestion Types\nFREE MC\nours\nTaCo (base) 86.12 85.58 - 85.53 85.74\nTaCo (large) 92.91 92.15 - 91.69 93.47\nw/o calculator\nQT →S →A(base) 65.21 64.35 21.07 56.23 84.55\nQT →S →A(large) 75.60 74.58 17.44 67.77 93.03\nw/o two-stage paradigm\nQT →SA(base) 78.22 77.66 7.91 77.15 79.12\nQT →SA(large) 84.73 84.25 8.04 83.95 85.14\nQT →AS(base) 75.18 74.34 11.09 71.88 81.38\nQT →AS(large) 81.45 81.41 11.10 80.21 84.84\nw/o two-stage paradigm and calculator\nQT →SA(base) 59.69 59.41 26.30 50.86 83.84\nQT →SA(large) 69.57 68.85 23.32 63.79 83.33\nQT →AS(base) 56.43 54.85 30.21 45.64 81.17\nQT →AS(large) 63.80 63.41 28.93 56.06 84.44\nw/o two-stage paradigm, calculator and solution\nQT →A(base) 57.10 56.39 29.11 48.33 79.42\nQT →A(large) 62.28 62.39 30.20 55.50 82.08\nTable 4: Ablation study of the external calculator and\nproposed two-stage paradigm. “base” and “large” stands\nfor model sizes of TAPEX backbone.\nbackbone in the TaCo framework. QT →S →A\nrepresents the proposed two-stage paradigm, which\nfirstly generates the solution S and then arrives at\nthe final answer A based on the input question Q,\ntable T and generated solution S. QT →SA and\nQT →AS represents one-stage paradigms, which\ngenerate the solution and the answer in different or-\nders, respectively. QT →A stands for the vanilla\nfine-tuning paradigm that directly predicts the an-\nswer.\nEffect of External Calculator. As shown in Ta-\nble 4, there is a drastic performance drop for the\nTaCo framework (e.g., 92.15% →74.58%) when\nremoving the external calculator. With further ob-\nservations, we find that the performance decline\nmainly comes from free-text questions which de-\nmand more numerical calculations. For instance,\nthe accuracy of TaCo (TAPEX-large) plummets\nfrom 91.69% to 67.77%. It demonstrates the great\nsignificance of using the external calculator to re-\nduce calculation errors in the generated solutions.\nOtherwise, the answer inference model is likely to\nbe misled by the incorrect solution and arrives at\nthe wrong answer.\nEffect of Two-stage Paradigm. When we\nchange the two-stage paradigm to one-stage ones,\nthe model performance drops about 9.5%, which\nreveals the contribution of two-stage paradigm. We\nthink it is challenging for single small-scale TaLM\nto generate correct reasoning steps and the final\nanswer simultaneously. As a result, we delegate\nthe CoT generation and the answer inference to\nModel Dev Test Question Types\nFREE MC\nw/ TAPEX\nTAPEX-base 86.12 85.58 85.53 85.74\nTAPEX-large 92.91 92.15 91.69 93.47\nw/ UnifiedQA\nUnifiedQA-small 48.32 48.17 46.45 53.06\nUnifiedQA-base 66.32 65.46 60.70 79.07\nUnifiedQA-large 77.44 76.96 73.50 86.85\nfine-tuned\nUnifiedQA-small 35.79 34.82 27.99 54.32\nUnifiedQA-base 51.89 51.08 42.10 76.76\nUnifiedQA-large 59.35 59.26 51.62 81.12\nTable 5: Experiment results of TaCo framework with\nTAPEX and UnifiedQA as backbone, respectively.\ntwo TaLMs, respectively. More importantly, one-\nstage paradigms cannot fully utilize the corrected\nCoT to change the original (wrong) answer. By\ncontrast, the two-stage paradigm brings a second\nchance to re-contemplate the improved reasoning\nsteps before making the final judgement. The simi-\nlar two-stage paradigm has also been explored in\nrecent works (Press et al., 2023; Zhao et al., 2023),\nwhere they utilize one LLM to generate the CoT to\nbe improved, and then ask the same LLM to infer\nthe final answer based on the improved CoT.\nComparing two one-stage paradigms, we notice\nthat QT →SA performs better than QT →AS.\nThis shows that it may be more suitable for TaLMs\nto infer the final answer according to produced\nreasoning steps, rather than give explanations based\non the predicted final answer. If we remove both\nthe two-stage paradigm and the external calculator,\nthe model performance would suffer a more steep\ndecline. But it is still better than that of traditional\nfine-tuned models in QT →A paradigm, which\nvalidates the value of intermediate reasoning steps\nfor TaLMs.\nEffect of TaLM Backbone. To investigate the\nperformance of TaCo with different backbones, we\nreplace TAPEX with UnifiedQA as the backbone\nmodel. Related experimental results are presented\nin Table 5. When the backbone changes from\nTAPEX to UnifiedQA, the TaCo framework suf-\nfers a sharp performance drop on both free-text and\nmulti-choice questions. For instance, even with\nmore parameters (1.54B), the accuracy of TaCo\nwith UnifiedQA-large on the test set (76.96%) is\nmuch lower than that with TAPEX-large (92.15%),\nwhich indicates the advantages of pre-trained tabu-\nlar language models. Unlike UnifiedQA which is\nsolely pre-trained on the unstructured textual data,\n11012\nFigure 4: Error distributions of different question types.\nTAPEX is additionally pre-trained on the tabular\ndata and thus has a better understanding of table\nstructures. As more powerful generative TaLMs\nemerge, they can be integrated into the TaCo frame-\nwork to improve their performance on the tabular\nmathematical reasoning task.\n4.5 Error Analysis and Case Study\nAs illustrated in Figure 6, for this problem that\ninvolves two multiplication and one addition oper-\nations, the TaCo framework successfully generates\ncorrect intermediate reasoning chains and finally\npredicts the right answer.\nThere are 473 free-text questions (78%) and 130\nmulti-choice questions (22%) for which the TaCo\n(TAPEX-large) gives wrong predictions. We ran-\ndomly selected 100 questions of each type for error\nanalyses. Figure 4 depicts error distributions by\nquestion types. More error instances are presented\nand discussed in Appendix C.\nFor free-text questions, error cases fall into the\nfollowing four categories. (1) Counting operation\n(49%): the question requires the model to count\nnumbers as the final answer, which is challenging\nfor generative language models. (2) Fraction calcu-\nlation (36%): the model fails to conduct fraction-\nrelated calculations such as reducing a fraction,\nwhich may be alleviated with an advanced calcu-\nlator. (3) Wrong formula (11%): the CoT genera-\ntion model outputs wrong formulas in the reason-\ning steps. (4) Function-related problem (4%): the\nmodel fails to solve problems related to the func-\ntion, e.g., compute the slope of the function based\non the table data.\nFor multi-choice questions, error cases can be\ndivided into the following five types. (1) Number\ncomparison (44%): the model cannot determine\nwhich number is larger or smaller. (2) Time cal-\nculation (21%): the model needs to perform time\ncalculation such as compute the elapsed time be-\ntween 9:15 A.M. and 11:20 A.M.. (3) Max/Min\noperation (19%): the question demands finding the\nbiggest or smallest number in a group. (4) False\nCoT (9%): the CoT generation model gives wrong\nor hallucinated reasoning steps, e.g., using numbers\nthat do not exist in the table or the question when\ngenerating formulas. (5) Commonsense (7%): the\ncommonsense knowledge is needed to answering\nthe question, which is a weakness of small-scale\nlanguage models.\n5 Related Work\nCoT prompting for LLMs. By providing a few\nin-context examples (or demonstrations) which\ncontain chain-of-thoughts, CoT prompting can en-\ncourage LLMs to output intermediate reasoning\nsteps before predicting the final answer (Wei et al.,\n2022). Existing CoT studies mainly focus on\ntwo directions. (1) Improving the quality of CoT\ndemonstrations. For instance, selecting better in-\ncontext examples for CoT prompting according to\nthe question diversity (Zhang et al., 2022), the so-\nlution complexity (Fu et al., 2023), or the example\nsimilarity (Rubin et al., 2022). (2) Exploring new\nrepresentations of CoT reasoning steps. Beside the\ntypical natural language format, researchers also\nproposed chain-of-thoughts in other formats. For\ninstance, program-of-thoughts (Chen et al., 2022),\ntree-of-thoughts (Yao et al., 2023a), and graph-of-\nthoughts (Yao et al., 2023b). Among them, the CoT\nin program languages has emerged as a powerful\napproach for LLMs to invoking external tools (Qin\net al., 2023). Recently, Lu et al. (2023a) proposed\nthe Chameleon framework that augments LLMs\nwith various tools like search engines and Python\nexecutors. We treat it as a contemporary work of\nour paper and list its results in the Appendix D.\nPre-trained TaLMs. Inspired by the success of\npre-training on the natural language text, various\nTaLMs are proposed for pre-training on the semi-\nstructured tabular data (Dong et al., 2022). Ex-\nisting TaLMs mainly inherit the architectures of\ntraditional language models and can be classified\n11013\ninto three types. (1) Encoder-based TaLMs like\nTAPAS (Herzig et al., 2020), MATE (Eisensch-\nlos et al., 2021) and TUTA (Wang et al., 2021).\n(2) Encoder-Decoder TaLMs such as TAPEX (Liu\net al., 2022) and STTP (Xing and Wan, 2021). (3)\nDecoder-based TaLMs like TableGPT (Gong et al.,\n2020). In previous studies, TaLMs are usually fine-\ntuned to directly generate final answers or simple\nformulas. By contrast, we are the first to explore the\ncombination of the CoT reasoning and pre-trained\nTaLMs.\n6 Conclusion\nWe extend the CoT reasoning into small-scale\nTaLMs for the first time, and provide an effective\napproach for tabular mathematical reasoning task,\nespecially under scenarios where LLMs are not\naccessible. Specifically, we propose a novel frame-\nwork named TaCo, which coordinates two TaLMs\nresponsible for CoT generation and answer infer-\nence, respectively. By introducing an external cal-\nculator, we further augment TaCo with the accurate\nmath computing ability. With two TAPEX-large\nmodels as backbones, the TaCo outperforms the\nstate-of-the-art ChatGPT on the TABMWP dataset\nby 9.55% (82.60%→92.15%) with much less pa-\nrameters (0.8B).\nLimitations\nThough the proposed method achieves great perfor-\nmance with less parameters, the fine-tuning of the\nCoT generation model and the answer inference\nmodel depends on annotated chain-of-thoughts and\ngold answers. As a result, the chain-of-thought\nreasoning ability of TaCo could be limited to the\ntabular mathematical reasoning task. In the future\nresearch, one can utilize open-source LLMs to gen-\nerate chain-of-thoughts of more diversities and of\nmore table-related tasks (Wang et al., 2023b; Ho\net al., 2023), which may further extend the gener-\nalization ability of TaLMs and reduce the cost of\nmanual annotation.\nIn the aspect of external tools, compared with\nframeworks which enable LLMs to access various\ntools (Shen et al., 2023; Lu et al., 2023a), TaCo\nonly utilizes a calculator to complete common arith-\nmetic calculations, i.e., “+,-,×,÷”. More advanced\nexternal tools may be integrated to enhance the\ncapability of the framework. We believe that the\ntool learning with small-scale language models is a\nvaluable future direction, especially for particular\nscenarios where LLMs are not available.\nEthics Statement\nThis paper proposes a two-stage framework for the\ntabular mathematical reasoning task, and models\nare trained and evaluated on the public TABMWP\ndataset. Thus, the authors foresee no ethical con-\ncerns with the research in this paper.\nAcknowledgements\nThis work was supported by the National Natural\nScience Foundation of China (No. 61976207) and\nthe National Social Science Foundation of China\n(No. 21AZD145).\nReferences\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? . In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a. Evaluat-\ning large language models trained on code.\n11014\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nHaoyu Dong, Zhoujun Cheng, Xinyi He, Mengyu Zhou,\nAnda Zhou, Fan Zhou, Ao Liu, Shi Han, and Dong-\nmei Zhang. 2022. Table pre-training: A survey\non model architectures, pre-training objectives, and\ndownstream tasks. In Proceedings of the Thirty-First\nInternational Joint Conference on Artificial Intel-\nligence, IJCAI-22, pages 5426–5435. International\nJoint Conferences on Artificial Intelligence Organi-\nzation. Survey Track.\nJulian Martin Eisenschlos, Maharshi Gor, Thomas\nMüller, and William W. Cohen. 2021. Mate: Multi-\nview attention for table transformer efficiency.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023. Complexity-based prompting for\nmulti-step reasoning.\nHeng Gong, Yawei Sun, Xiaocheng Feng, Bing\nQin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020.\nTableGPT: Few-shot table-to-text generation with\ntable structure reconstruction and content matching.\nIn Proceedings of the 28th International Conference\non Computational Linguistics , pages 1978–1988,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2023.\nLarge language models are reasoning teachers.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1896–1907, Online. Association\nfor Computational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: Table pre-training via learning a neural SQL\nexecutor. In International Conference on Learning\nRepresentations.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and\nJianfeng Gao. 2023a. Chameleon: Plug-and-play\ncompositional reasoning with large language models.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\nand Ashwin Kalyan. 2023b. Dynamic prompt learn-\ning via policy gradient for semi-structured mathe-\nmatical reasoning. In International Conference on\nLearning Representations (ICLR).\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary.\n11015\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Tool learning with foundation\nmodels.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in nlp.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. Lamda: Language models for dialog appli-\ncations.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions.\nZhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu,\nShi Han, and Dongmei Zhang. 2021. Tuta: Tree-\nbased transformers for generally structured table pre-\ntraining. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery amp; Data Min-\ning, KDD ’21, page 1780–1790, New York, NY , USA.\nAssociation for Computing Machinery.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nXinyu Xing and Xiaojun Wan. 2021. Structure-aware\npre-training for table-to-text generation. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 2273–2278, Online.\nAssociation for Computational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models.\nYao Yao, Zuchao Li, and Hai Zhao. 2023b. Beyond\nchain-of-thought, effective graph-of-thought reason-\ning in large language models.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompting\nin large language models.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-\nSeng Chua. 2021. TAT-QA: A question answering\nbenchmark on a hybrid of tabular and textual content\nin finance. CoRR, abs/2105.07624.\n11016\nA More Implementation Details\nIn our experiments, we employ TAPEX and Uni-\nfiedQA as backbones of TaCo framework. When\nlinearizing the table into flattened sequence, if\nthere exist no column headers in the original ta-\nble, pseudo column headers will be inserted, e.g.,\n’Column header 1’. The hyper-parameter config-\nurations of TAPEX and UnifiedQA backbone and\ntheir model sizes are shown in Table 6 and Table 7,\nrespectively. Our experiments are all performed on\na 32G NVIDIA V100 GPU.\nFor LLM-based baselines, we list numbers of\nfew-shot examples and selection strategies in Table\n8. For ChatGPT baseline, we randomly select 4\nexamples from train set for each question type. For\nfair comparison, we use the same prompt format as\nPromptPG (Lu et al., 2023b) to construct in-context\nexamples, which is demonstrated in Figure 5.\nParameters TAPEX\nbase (140M) large (400M)\nLearning Rate 3e-5 3e-5\nBatch Size 16 32\nWeight Decay 0.01 0.01\nMax Grad Norm 1.0 1.0\nWarmup Linear Linear\nWarmup Fraction 0.1 0.1\nEpochs for Stage 1 20 25\nEpochs for Stage 2 15 20\nTraining Time for Stage 1 3 hours 8 hours\nTraining Time for Stage 2 2 hours 6 hours\nTable 6: Hyper-parameter configurations for TAPEX\nbackbone.\nParameters UnifiedQA\nsmall (60M) base (220M) large (770M)\nLearning Rate 5e-5 5e-5 5e-5\nBatch Size 16 16 48\nWeight Decay 0.01 0.01 0.01\nMax Grad Norm 1.0 1.0 1.0\nWarmup Linear Linear Linear\nWarmup Fraction 0.1 0.1 0.1\nEpochs for Stage 1 15 20 25\nEpochs for Stage 2 15 15 20\nTraining Time for Stage 1 2 hours 8 hours 15 hours\nTraining Time for Stage 2 2 hours 5 hours 12 hours\nTable 7: Hyper-parameter configurations for UnifiedQA\nbackbone.\nMethod # few-shot\nexamples\nSelection\nstrategy Acc-Test\nGPT-3 2 Random selection 57.13\nCodex 4 Manual construction 59.40\nGPT-3+CoT 2 Random selection 62.92\nCodex+CoT 4 Manual construction 65.20\nPromptPG 2 Policy Gradient 68.23\nPoT 4 Manual construction 73.20\nChatGPT 4 Random selection 65.52\nChatGPT+CoT 4 Random selection 82.60\nTable 8: Number of in-context examples and selection\nstrategies of LLM baselines.\nB The complexity of CoT generation\nTable 3 reveals a significant performance difference\nbetween free-text questions and multi-choice ques-\ntions. To shed more light on the TABMWP dataset,\nwe quantitatively analyze the complexity of the\nCoT generation for two question types. Specifi-\ncally, we compute the number of required numer-\nical calculations in the gold CoT (including +, -,\n×, ÷, counting, min, max), the number of reason-\ning steps (we treat each line in the gold CoT as\none reasoning step for simplicity) and the length of\nthe gold CoT. The statistical results in the Table 9\ndemonstrate that, in the TABMWP dataset, the CoT\ngeneration from free-text questions is more com-\nplex than that from multi-choice questions. Based\non our observations, at least 18%multi-choice ques-\ntions (mainly of EXTR and OTH answer types) do\nnot need numerical calculations, but almost allfree-\ntext questions need numerical calculations.\nC Error Instances and More Analysis\nIn this section, we present detailed error instances\nto analyze the weakness of TaCo framework, which\nis shown in Figure 7 to Figure 10. We find that\nmost of errors are caused by the inability of used\nexternal tool and the representation of chain-of-\nthoughts. Take the error instance in Figure 7 as\nan example. To correctly answer the question in\nFigure 7, the model should find numbers from the\ntable which are greater than 53, and then count\nhow many numbers are found. However, as the\nCoT generation model is fine-tuned to generate\nchain-of-thoughts in simple natural language, it\nis difficult for the model to describe the above\nprocess in a short and straightforward expression,\nwhich makes it hard to invoke external tools. If\nwe could represent chain-of-thoughts in program\nlanguages like Python, the solution of this ques-\ntion would be much more clear. For instance,\none can write a line of Python code: “ Ans =\nCount(61,61,65,65,66,70,66,78)”, and imple-\nment a Python function “Count()” as an external\ntool to get the accurate result. The same method-\nology could be applied to error instances which\ndemand other abilities such as fraction calculation,\nmin/max operation and time calculation. Besides,\nlacking commonsense knowledge also increases\nthe difficulty for models to comprehend tables and\nquestions, e.g., reading bus schedule in Figure 10.\n11017\nFigure 5: The format of in-context examples for ChatGPT baseline (ID:19324).\nQuestion Types # of numerical calculations\n(median/mean)\n# of reasoning steps\n(median/mean)\nthe length of CoT\n(median/mean)\nfree-text 2.00/2.15 4.00/5.18 196.00/239.15\nmulti-choice 1.00/1.78 2.00/3.84 180.00/253.21\nTable 9: The quantitative analysis of the complexity of the CoT generation for two question types.\nD Results of Chameleon framework\nRecently, Lu et al. (2023a) proposed a compo-\nsitional reasoning framework named Chameleon,\nwhich treats LLMs as a natural language planner\nto utilize a variety of tools including vision models,\nweb search engines, Python functions and so on.\nAs shown in Table 10, based on the powerful GPT-\n4 and multiple external tools, Chameleon achieves\nthe best accuracy of 98.78% on TABMWP test\nset. However, the proposed TaCo framework still\nachieves a competitive result of 92.15% with less\nparameters.\nWe also apply the same calculator to the output\nof ChatGPT and use regular expressions to extract\nthe final answer from the output. There is a slight\nperformance increase from 82.60% to 83.07%. Af-\nter inspecting error cases of ChatGPT, we found\nthat most errors resulted from wrong reasoning\nsteps rather than calculation mistakes. Compared\nwith small-scale TaLMs, the numerical calculating\nability of ChatGPT is much more better, which\nmay attribute to the potential use of more advanced\nexternal tools behind the ChatGPT system.\nMethod Acc-Test Question Types\nFREE MC\nChatGPT CoT 82.03 78.43 92.32\nChatGPT PoT 89.49 90.24 87.35\nGPT-4 CoT 90.81 88.48 97.49\nGPT-4 PoT 96.93 97.40 95.58\nChameleon (ChatGPT) 93.28 93.13 93.72\nChameleon (GPT-4) 98.78 98.95 98.29\nTaCo (Ours) 92.15±0.13 91.69 93.47\nTable 10: Accuracy of Chameleon on TABMWP test\nset.\n11018\nFigure 6: A correct instance where TaCo generates right solution and answer. (ID:752).\nFigure 7: An error instance of counting operation (ID:449), where TaCo cannot correctly count how many numbers\nsatisfying requirements.\nFigure 8: An error instance of fraction calculation (ID:1711), where TaCo makes mistakes when reducing a fraction.\nFigure 9: An error instance of number comparison (ID:1434), where TaCo cannot correctly judge which is the larger\nnumber between 72.00 and 74.00.\nFigure 10: An error instance of time calculation (ID:2766), where TaCo fails to compute the elapsed time between\n11:00 A.M. and 12:00 P.M.\n11019",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7790666818618774
    },
    {
      "name": "Inference",
      "score": 0.7227132320404053
    },
    {
      "name": "Language model",
      "score": 0.5379641056060791
    },
    {
      "name": "Task (project management)",
      "score": 0.49630647897720337
    },
    {
      "name": "Software deployment",
      "score": 0.4892392158508301
    },
    {
      "name": "Calculator",
      "score": 0.48111361265182495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43595266342163086
    },
    {
      "name": "Code (set theory)",
      "score": 0.41554561257362366
    },
    {
      "name": "Theoretical computer science",
      "score": 0.39278891682624817
    },
    {
      "name": "Programming language",
      "score": 0.341755211353302
    },
    {
      "name": "Software engineering",
      "score": 0.2185555100440979
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}