{
  "title": "Language Models with Transformers",
  "url": "https://openalex.org/W2936497627",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2206262003",
      "name": "Wang, Chenguang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2041018369",
      "name": "Li Mu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227491603",
      "name": "Smola, Alexander J.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2767002384",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2894175714",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2895242880",
    "https://openalex.org/W2785430118",
    "https://openalex.org/W2809899846",
    "https://openalex.org/W2963536136",
    "https://openalex.org/W2963174729",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2771727678",
    "https://openalex.org/W2913732044",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2769653148",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2178031510",
    "https://openalex.org/W2767274188",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2748513770",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2743945814"
  ],
  "abstract": "The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling. In this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",
  "full_text": "Language Models with Transformers\nChenguang Wang Mu Li Alexander J. Smola\nAmazon Web Services\n{chgwang, mli, smola}@amazon.com\nAbstract\nThe Transformer architecture is superior to\nRNN-based models in computational efﬁ-\nciency. Recently, GPT and BERT demon-\nstrate the efﬁcacy of Transformer models\non various NLP tasks using pre-trained lan-\nguage models on large-scale corpora. Sur-\nprisingly, these Transformer architectures\nare suboptimal for language model itself.\nNeither self-attention nor the positional en-\ncoding in the Transformer is able to efﬁ-\nciently incorporate the word-level sequen-\ntial context crucial to language modeling.\nIn this paper, we explore effective Trans-\nformer architectures for language model,\nincluding adding additional LSTM layers\nto better capture the sequential context\nwhile still keeping the computation efﬁ-\ncient. We propose Coordinate Architec-\nture Search (CAS) to ﬁnd an effective archi-\ntecture through iterative reﬁnement of the\nmodel. Experimental results on the PTB,\nWikiText-2, and WikiText-103 show that\nCAS achieves perplexities between 20.42\nand 34.11 on all problems, i.e. on average an\nimprovement of 12.0 perplexity units com-\npared to state-of-the-art LSTMs. The source\ncode is publicly available 1.\n1 Introduction\nModeling the sequential context in language is the\nkey to success in many NLP tasks. Recurrent neu-\nral networks (RNNs) (Mikolov et al., 2010) mem-\norize the sequential context in carefully designed\ncells. The sequential nature of these models, how-\never, makes computation expensive (Merity et al.,\n2017; Yang et al., 2017), and therefore it is difﬁ-\ncult to scale to large corpora.\nThe Transformer architecture (Vaswani et al.,\n2017) replaces RNN cells with self-attention\n1https://github.com/cgraywang/\ngluon-nlp-1/tree/lmtransformer/scripts/\nlanguage_model\nand point-wise fully connected layers, which are\nhighly parallelizable and thus cheaper to compute.\nTogether with positional encoding, Transformers\nare able to capture long-range dependencies with\nvague relative token positions. This results in\na coarse-grained sequence representation at sen-\ntence level. Recent works such as GPT (or GPT-\n2) (Radford et al., 2018, 2019) and BERT (Devlin\net al., 2018) show that the representations learned\non large-scale language modeling datasets are ef-\nfective for ﬁne-tuning both sentence-level tasks,\nsuch as GLUE benchmark (Wang et al., 2018), and\ntoken-level tasks that do not rely on word order de-\npendency in the context, such as question answer-\ning and NER.\nDespite the fact that both GPT and BERT use\nlanguage models for pre-training, neither of them\nachieves state-of-the-art performance in language\nmodeling. Language model aims to predict the\nnext word given the previous context, where ﬁne-\ngrained order information of words in context is\nrequired. Neither self-attention nor positional en-\ncoding in the existing Transformer architecture is\neffective in modeling such information.\nA second challenge (and opportunity) arises\nfrom the fact that we may often have access to\nmodels pre-trained on related, albeit not identical\ntasks. For instance, neither GPT or BERT is tuned\nfor WikiText and neither of them aims to minimize\nperplexity directly. In fact, the architectures may\nnot even be useful directly: BERT provides esti-\nmates of p(wi|context) rather than p(wi|history).\nThis shows that there is a need for us to design\nalgorithms which systematically explore the space\nof networks that can be derived (and adapted) from\nsuch tasks. This generalizes the problem of mak-\ning use of pre-trained word embeddings for related\ntasks, only that in our case we do not have vectors\nbut rather entire networks to deal with.\nLastly, the problem of architecture search per-se\nhas received great interests. However, the size of\n1\narXiv:1904.09408v2  [cs.CL]  17 Oct 2019\nthe datasets where training a single model for GPT\nor BERT can cost in excess of $10,000, makes it\nprohibitively expensive to perform a fully-ﬂedged\nmodel exploration with full retraining. Instead,\nwe propose to use architecture search in a much\nmore restricted (and economical) manner to in-\nvestigate reﬁning a trained architecture. This is\nmuch cheaper. Our pragmatic approach leads to\nimprovements on the state-of-the-art in language\nmodeling. Our contributions are as follows:\n1. We propose a Transformer architecture for\nlanguage model. It works by adding LSTM\nlayers after all Transformer blocks (a result\nof the search algorithm). This captures ﬁne-\ngrained word-level sequential context.\n2. We describe an effective search procedure,\nCoordinate Architecture Search (CAS). This\nalgorithm randomly generates variants of the\nTransformer architecture, based on the cur-\nrent best found architecture. Due to its greedy\nnature, CAS is simpler and faster than previ-\nous architecture search algorithms (Zoph and\nLe, 2016; Pham et al., 2018; Liu et al., 2018).\n3. We show how this can be used to incorpo-\nrate substantial prior knowledge in the form\nof GPT or BERT. Using this information via\nbrute force architecture search would be pro-\nhibitively expensive.\nContributions 2 and 3 are general and apply to\nmany cases beyond NLP. Contribution 1 is ar-\nguably more language speciﬁc. We evaluate\nCAS on three popular language model datasets:\nPTB, WikiText-2 and WikiText-103. The BERT-\nbased CAS achieves in average 12.0 perplex-\nity gains compared to the state-of-the-art LSTM-\nbased language model AWD-LSTM-MoS (Yang\net al., 2017).\n2 Transformers for Language Models\nOur Transformer architectures are based on GPT\nand BERT. We will reuse the pre-trained weights\nin GPT and BERT to ﬁne-tune the language model\ntask. During ﬁne-tuning, we modify and retrain\nthe weights and network used by GPT and BERT\nto adapt to language model task.\n2.1 GPT and BERT\nGPT (Radford et al., 2018) uses a variant of the\nTransformer architecture (Vaswani et al., 2017).\nThat is, it employs a multi-layer Transformer de-\ncoder based language model. The original paper\nprovides a pre-trained architecture with 12-layer\nTransformer decoder-only blocks. Each block has\nhidden size 768 and 12 self-attention heads. The\nweights are trained on BooksCorpus. This allows\nit to generate p(wi|history), one word at a time.\nBERT is a multi-layer bidirectional Trans-\nformer encoder (Devlin et al., 2018). The origi-\nnal paper provides two BERT structures: BERT-\nBase, consists of 12-layer bidirectional Trans-\nformer encoder block with hidden size 768 and\n12 self-attention heads; BERT-Large includes 24-\nlayer bidirectional Transformer encoder blocks\nwith hidden size 1024 and 16 self-attention heads.\nThe weights are trained on BooksCorpus and the\nEnglish Wikipedia. Unless stated otherwise, we\nmean BERT Base when mentioning BERT.\nRelation between GPT and BERT.Both mod-\nels use virtually the same architecture. In fact,\nGPT and BERT-Base even use the same number\nof layers and dimensions. The only difference is\nthat BERT is bidirectional since it tries to ﬁll in in-\ndividual words given their context, whereas GPT\nuses masked self-attention heads.\n2.2 Adapting GPT and BERT for Sub-word\nLanguage Model\nGPT needs little modiﬁcation, unless we want to\nexplore different architectures. After all, it is al-\nready trained as a language model. At a mini-\nmum, during ﬁne-tuning we add a linear layer with\nhidden size equal to the vocabulary size. These\nweights are tuned and fed into the softmax to gen-\nerate a probability distribution of the target word\nover the vocabulary. Masked self-attention en-\nsures that only causal information ﬂow can occur.\nRecall the objective of BERT: masked language\nmodel and next sentence prediction. The masked\nlanguage model uses bidirectional contextual in-\nformation and randomly masks some tokens dur-\ning training. Based on that it tries to infer the iden-\ntity of the masked word. Unfortunately, estimating\np(wi|w1,...w i−1,wi+1,...w n) is not conducive\nto building an effective text generator: We would\nneed to design a Gibbs sampler to sample wi|w−i,\ni.e. wi given its contextw−i iteratively and repeat-\nedly for all ito use a variant of this aspect directly.\nThe next sentence prediction aims to capture\nthe binarized relationship between two sentences.\nAgain, this is not directly useful for LM. We\nthus remove the objective and replace it by a log-\nlikelihood measure during ﬁne-tuning. Similar to\n2\nGPT, we add an output linear layer and replace the\nself-attention heads with masked self-attention to\nprevent leftward information ﬂow.\nNote that GPT and BERT pre-trained weights\nare re-used in the language model ﬁne-tuning pro-\ncess to save the costs of a full retraining. We are\nthus conducting the language model in the sub-\nword level since the sub-word tokenization is used\nin both GPT and BERT. More details will be de-\nscribed in Section 4.\n2.3 Fine-tuning Transformer Weights\nGPT and BERT tune the weights of their respec-\ntive models for the tasks mentioned above. For in-\nstance, BERT doesn’t use windowing by default.\nHence it makes sense to adjust the weights when\nﬁne-tuning for language modeling. However, up-\ndating all weights could lead to overﬁtting since\ndatasets such as WikiText or Penn Tree Bank are\nover an order of magnitude smaller than the data\nused to train GPT and BERT.\nTo address this dilemma we propose to up-\ndate only a subset of layer weights during ﬁne-\ntuning. Since both GPT and BERT have 12 Trans-\nformer blocks, each of which contains a self-\nattention and a point-wise fully connected layer, it\nis not straightforward to choose the subset of lay-\ners whose parameters should be ﬁxed. Instead, we\nwill automatically search the subset which is most\neffective for the language model task. The search\nalgorithm will be discussed in Section 3.\n2.4 Adding an LSTM\nThe positional encoding via a Fourier base in the\nTransformer only provides vague relative posi-\ntion information, forcing the layers to reinvent\ntrigonometry at each layer for speciﬁc word ac-\ncess. This is problematic since LM requires strong\nword-level context information to predict the next\nword. RNNs explicitly model this sequential in-\nformation. We therefore propose to add LSTM\nlayers to the Transformer architecture.\nIn theory we could add LSTM layers anywhere,\neven interleaving them with Transformers. How-\never, LSTMs add signiﬁcant computational efﬁ-\nciency penalties, since they prevent parallel com-\nputation. Our reasoning is analogous to that guid-\ning the design of the SRU (simple recurrent unit)\n(Lei et al., 2018). Hence we propose to add an\nLSTM layer either before all basic Transformer\nblocks or after these blocks. For the former, we\nadd the LSTM layer immediately after the em-\nbedding layer and remove the positional and seg-\nment embedding, because we believe the LSTM\nlayer is able to encode sufﬁcient sequential infor-\nmation. For the latter, we insert the LSTM layer\nbetween the last Transformer block and the output\nlinear layer. We determine the best location for the\nLSTM by automatic search.\n3 Coordinate Architecture Search\nNow that we have the basic components, let’s\nreview the network transformations and the as-\nsociated search procedures to obtain a well-\nperforming architecture.\n3.1 Network Transformations\nTransformations modify a network. A modiﬁca-\ntion could be adding a new layer or ﬁxing the\nparameters during ﬁne-tuning. In Section 2, we\nproposed multiple transformations. Let us de-\nﬁne them formally below with randomization and\npractical constraints.\nAddLinear adds a linear output layer with hidden\nsize equal to the vocabulary size. It then ran-\ndomly initializes its parameters. If such a lin-\near layer already exists, this step is skipped.\nAddLSTM adds an LSTM layer if no such layer\nalready exists. It attaches the LSTM either\nbefore or after all Transformer blocks. For\nthe former we remove both positional embed-\nding and segment embedding. If there exist\nfewer than 3 LSTM layers, we append an-\nother LSTM layer to the LSTM block. We\nrandomly initialize parameters for the newly\nadded layer.\nFixSubset Given nTransformer blocks, pick k∈\n[0,n] uniformly at random. Accordingly pick\nk blocks uniformly at random among the\n{1,...n }layers and ﬁx the parameters for\neach selected block during ﬁne-tuning.\n3.2 Sampling a Search Candidate\nWe need to generate architecture candidates dur-\ning search. To illustrate that restricted search\nis competitive to a full-ﬂedged brute force rein-\nforcement learning (or genetic algorithms) search,\nwe adopt an exceedingly simple procedure: uni-\nform random sampling. At each time we sam-\nple transformations uniformly at random (as per\nAlgorithm 1) from the set of modiﬁcations of a\nbase architecture until termination, as indicated\n3\nembeddingTransformer 0Transformer 1\nembeddingTransformer 0Transformer 1\nembeddingTransformer 0Transformer 1LSTM\nembeddingTransformer 0Transformer 1LSTMLinear + MOSAddLSTMAddLinear\nFixSubsetnet\ncandidateFixed weightsTunableweights\nFigure 1: Search candidate sampling. net is the base architecture and candidate is returned in the next\nstep. Transformers, Embeddings, LSTMs and Linear output transformations are as stated. Lightly shaded\nblocks are variable, dark blocks are ﬁxed. See Algorithm 1 for details.\nembeddingTransformer 0Transformer 1LSTMLinear + MOSnet_bestSample\nembeddingTransformer 0Transformer 1LSTMLinear + MOS\nembeddingTransformer 0Transformer 1LSTMLinear + MOSFine-tune\nValPPL=52\nValPPL=52\nValPPL=52 embeddingTransformer 0Transformer 1LSTMLinear + MOS\nValPPL=49\nembeddingTransformer 0Transformer 0LSTMLinear + MOS\nValPPL=51\nFine-tune embeddingTransformer 0Transformer 1LSTMLinear + MOS\nValPPL=49\nnet_best\nKeep\nSample\nKeep\nReturn\nDiscard\nStepi Stepi+1\ncandidate0\ncandidate1 candidate1\ncandidate0\nFigure 2: Coordinate architecture search. net_best is the best architecture at step i of the search. We\nsample search candidates and keep the one that performs best, as measured by perplexity (Val PPL) on\nthe target dataset after ﬁne-tuning. See details in Algorithm 2.\nAlgorithm 1 Search Candidate Sampling\nInput: Base architecture net\nOutput: A new architecture candidate\n1: candidate ←net\n2: repeat\n3: Sample a tranformation T uniformly from\n{AddLinear,AddLSTM,FixSubset}.\n4: Apply T to candidate\n5: until T = AddLinear\n6: return candidate\nby adding an emissions layer AddLinear. This\nmeans that we have a valid architecture. See Fig-\nure 1 for an example.\n3.3 Coordinate Architecture Search\nWe use a simple greedy strategy for architecture\nsearch. Starting with either GPT or BERT as pre-\ntrained model we repeat the search ntimes. Each\ntime we sample a candidate, then ﬁne-tune it and\nupdate the best candidate if necessary. See Algo-\nrithm 2 for a description and Figure 2 for an illus-\ntration. At each (successful) step we ﬁne-tune the\nvariable parameters of the architecture.\n4 Experiments\nTo illustrate the effectiveness of the Transformer\narchitectures found using coordinate search we\npresent results on both WikiText and Penn Tree-\nBank datasets. We also provide details about its\nspeed relative to existing neural search strategies.\n4.1 Datasets and Evaluation Metric\nWe evaluate the proposed methods on three\nwidely-used language model benchmark datasets.\nPenn TreeBank (PTB): we use the preprocessed\n4\nAlgorithm 2 Coordinate Architecture Search\nInput: Initial architecture net, search steps n,\nﬁne-tuning dataset\nOutput: Best architecture netbest\n1: netbest ←net;\n2: for i= 1to ndo\n3: Draw candidate from netbest using Algo-\nrithm 1.\n4: Fine-tune candidateon dataset\n5: if PPL(candidate) <PPL(netbest) then\n6: netbest ←candidate\n7: end if\n8: end for\n9: return netbest\nversion of (Mikolov et al., 2010), which contains\n100M tokens. WikiText-2 (WT-2)is a small pre-\nprocessed version of Wikipedia, containing 200M\ntokens (Merity et al., 2016). WikiText-103 (WT-\n103) contains 1B tokens of the same origin as WT-\n2. We use the commonly adopted training, valida-\ntion and test splits.\nTo illustrate the ﬂexibility of our approach,\nwe explore two pre-trained Transformers for sub-\nword level language models, i.e., BERT and GPT.\nFor BERT related model architectures, we use\nWordPiece embedding (Wu et al., 2016) to tok-\nenize the training/validation/test split of the PTB,\nWT-2 and WT-103 respectively. The result-\ning sub-word vocabulary size is 30k, denoted as\nBERTVocab. The split word pieces are denoted\nwith ## following (Devlin et al., 2018). For\nthe model architectures based on GPT, the three\ndatasets are tokenized based on bytepair encod-\ning (BPE) (Sennrich et al., 2016), where the sub-\nword vocabulary size is 40k based on (Radford\net al., 2018), denoted as GPTVocab. Note that\nBERT and the WordPiece embedding in BERT are\ntrained on BooksCorpus and Wikipedia, whereas\nGPT and its BPE are trained only on BooksCor-\npus. Note the sub-word level vocabulary size is\ndifferent from the word-level vocabulary size ob-\ntained on the training splits of the datasets. We\nuse perplexity (PPL) to evaluate the sub-word lan-\nguage model results.\n4.2 Training Details\nWe evaluate CAS (Algorithm 2) with both BERT\nand GPT pre-trained as the initial architecture, and\ntrained on all three datasets. The same training\nconﬁguration is used across all datasets. We pick\nn = 10 search steps. In a ﬁne-tuning task, the\nnumber of epochs is 50, the gradients are com-\nputed using truncated back-propagation through\ntime, and ADAM (Kingma and Ba, 2014) is used\nto update parameters. The perplexity on the vali-\ndation dataset is used to choose architectures. We\nreport results on the respective test datasets.\nFor GPT based architectures the hyperparam-\neters of the Transformer decoder and embedding\nblocks are the same as in (Radford et al., 2018). If\nLSTM layers are added, we set the dropouts of the\nLSTM layers to 0.1. DropConnect is not applied.\nAll other LSTM hyperparameters follow (Merity\net al., 2017). The ﬁnal linear layer is with dropout\nrate 0.1. Following (Yang et al., 2017), we use\na mixture of softmax (MoS) to replace the stan-\ndard softmax with 15 components. We set 64 as\nsequence length and 16 as minibatch size. ADAM\nwith learning rate 6.25·10−5 and L2 weight decay\nof 0.01 are used.\nFor BERT based architectures the hyperparam-\neters of the Transformer encoder blocks and the\nembedding blocks are set the same as the original\nimplementation (Devlin et al., 2018). The hyper-\nparameters of the LSTM layers and linear layer are\nthe same with GPT conﬁguration. As with GPT\nwe use MoS with 15 components. We pick 128 as\nsequence length and 16 as minibatch size. ADAM\nwith learning rate10−4, β1 = 0.9, β2 = 0.999 and\nL2 weight decay of 0.01 are used.\nLastly, for AWD-LSTM-MoS with BERT or\nGPT sub-word setting, we largely follow the pa-\nrameter settings in the original implementation\n(Yang et al., 2017). We use NT-ASGD (Merity\net al., 2017) to train 50 epochs on training datasets.\nSince the goal of this work is to discover best-\nperforming language model from the architecture\nperspective, we do not employ post-training meth-\nods such as neural cache model (Grave et al.,\n2016) or dynamic evaluation (Krause et al., 2018).\nWe expect that such methods would potentially\nimprove the perplexity of all models.\n4.3 Comparing CAS to Other Methods\nWe compare CAS, denoted by BERT-CAS and\nGPT-CAS respectively to three other models.\nBERT and GPT. This is straightforward. The\nonly change needed is that we update the last out-\nput layer during ﬁne-tuning.\nA WD-LSTM-MoS-{BERT, GPT}Vocab.This\nis a state-of-the-art language model, based on\n5\nModel\nDatasets\nPTB WT-2 WT-103\nVal Test Val Test Val Test\nAWD-LSTM-MoS-BERTV ocab 43.47 38.04 48.48 42.25 54.94 52.91\nBERT 72.99 62.40 79.76 69.32 109.54 107.30\nBERT-CAS (Our) 39.97 34.47 38.43 34.64 40.70 39.85\nBERT-Large-CAS (Our) 36.14 31.34 37.79 34.11 19.67 20.42\nAWD-LSTM-MoS-GPTV ocab 50.20 44.92 55.03 49.77 52.90 51.88\nGPT 79.44 68.79 89.96 80.60 63.07 63.47\nGPT-CAS (Our) 46.24 40.87 50.41 46.62 35.75 34.24\nTable 1: Performance of Coordinate Architecture Search (CAS). ‘Val’ and ‘Test’ denote validation and\ntest perplexity respectively.\nLSTMs, improving on (Yang et al., 2017) due to\na more careful handling of tokens. For a fair\ncomparison, instead of using word level vocab-\nulary in the original implementation of AWD-\nLSTM-MoS (Yang et al., 2017), we use the sub-\nword vocabularies of BERT and GPT separately.\nOur implementation uses BERTV ocab or GPTV o-\ncab to replace the word based vocabulary used in\nthe original implementation of AWD-LSTM-MoS\n(Yang et al., 2017). Note that on PTB and WT-2,\nboth AWD-LSTM-MoS-BERTV ocab and AWD-\nLSTM-MoS-GPTV ocab outperform the original\nAWS-LSTM-MoS models by 17.8 and 10.6 per-\nplexity points respectively. This is likely due to\nthe change in word vocabulary to a sub-word vo-\ncabulary.\nThe results are shown in Table 1 and illustrated\nin Figure 3. First note that GPT and BERT are sig-\nniﬁcantly worse than AWD-LSTM-MoS. It con-\nﬁrms our hypothesis that neither BERT nor GPT\nare effective tools for language modeling. Ap-\nplying them naively leads to signiﬁcantly worse\nresults compared to AWS-LSTM-MoS on three\ndatasets. It demonstrates that language modeling\nrequires strong capabilities in modeling the word\norder dependency within sentences. However, due\nto the combination of self-attention and positional\nencoding, both GPT and BERT primarily capture\ncoarse-grained language representation but with\nlimited word-level context.\nOn the other hand, the Transformer architec-\ntures picked by CAS (BERT-CAS) outperform\nAWS-LSTM-MoS on all datasets. The aver-\nage test perplexity improvement with BERT pre-\ntrained models is 8.09 and 8.28 with GPT pre-\ntrained models. The results demonstrate that 1)\nCAS is able to locate an effective Transformer\narchitecture for language model; and 2) that the\ncombination of ﬁxing a subset weights and adding\nLSTM layers is capable of capturing the word-\nlevel context.\nFurthermore, we apply CAS to BERT-Large\n(i.e., BERT-Large-CAS). Compare to BERT-CAS,\nthe architectures generated achieve on average\n7.70 perplexity gains, which are competitive re-\nsults with recent approaches such as Transformer-\nXL (Dai et al., 2019) and GPT-2 (Radford et al.,\n2019). This shows that robustness of the CAS\nmethod, which indicates that a stronger pre-\ntrained model would potentially produce a better\nlanguage model.\nIn addition, BERT-CAS outperforms GPT-CAS\non datasets PTB and WT-2, but is worse on WT-\n103. The reason is twofold. First, the GPT’s\nBPE vocabulary is 10k larger than BERT’s Word-\nPiece vocabulary, since the original word vocabu-\nlary size of WT-103 is around 10 times larger com-\npared to PTB and WT-2, thus we infer that BPE\nvocabulary has stronger ability to represent large\nvocabulary. Second, unlike GPT, the pre-trained\nBERT weights are not based on a language mod-\neling objective. Thus BERT based architectures\nmay need more epochs to converge on large cor-\npora. This is likely due to the fact that masking\nis not a part of BERT training. Its introduction\namounts to a more signiﬁcant change in the co-\nvariates, thus requires more adaptation.\n4.4 Ablation Study\nTo elucidate the effects of different model im-\nprovements we compare CAS to the following\nthree variants:\n{BERT, GPT}-CAS-Subset applies Algorithm 2\nwithout adding LSTM layers.\n6\nModel\nDatasets\nPTB WT-2 WT-103\nVal Test Val Test Val Test\nBERT-CAS-Subset 42.53 36.57 51.15 44.96 44.34 43.33\nBERT-CAS-LSTM 40.22 35.32 53.82 47.00 53.66 51.60\nGPT-CAS-Subset 47.58 41.85 54.58 50.08 35.49 35.48\nGPT-CAS-LSTM 47.24 41.61 50.55 46.62 36.68 36.61\nTable 2: Ablation study. Compare CAS with not adding LSTM layers (CAS-Subset) and not updating\nTransformer block parameters (CAS-LSTM).\nTest Perplexity\n0\n20\n40\n60\n80\n100\n120\nPTB WT-2 WT-103\nAWD-LSTM-MoS-GPTVocab GPT\nGPT-CAS-Subset GPT-CAS-LSTM\nGPT-CAS\nTest Perplexity\n0\n20\n40\n60\n80\n100\n120\nPTB WT-2 WT-103\nAWD-LSTM-MoS-BERTVocab BERT\nBERT-CAS-Subset BERT-CAS-LSTM\nBERT-CAS BERT-Large-CAS\nBERT GPT\nFigure 3: Comparison of test perplexities between CAS and other models (left: using BERT pre-trained\nmodels; right: using GPT pre-trained models). In particular, ‘Subset’ indicates variants without LSTMs\nand ‘LSTM’ corresponds to models without updating the transformer blocks.\n{BERT, GPT}-CAS-LSTM applies Algorithm 2\nbut it ﬁxes all Transformer blocks during\nﬁne-tuning.\nSee Table 2 and Figure 3 for details of the results.\nAs can be seen, both CAS-Subset and CAS-LSTM\nimprove signiﬁcantly upon a naive use of BERT\nand GPT. This is to be expected since ﬁne-tuning\nimproves performance. On the smaller dataset,\ni.e. PTB, adding LSTMs is more effective. This\nmight be due to the overﬁtting incurred in updat-\ning Transformers. On the other hand, on the larger\ndatasets, i.e. WT-103, adding an LSTM is less\neffective, which means that adapting the Trans-\nformer parameters for better sentence-level rep-\nresentation is more important. Combing both to-\ngether leads to further improvement. CAS outper-\nforms AWD-LSTM-MoS on all three datasets.\nNext, we unfreeze the pre-trained weights of\nBERT to allow fully ﬁne-tuning including the last\nModel Validation Test\nBERT-All 79.14 67.43\nBERT-CAS 39.97 34.47\nTable 3: Over-ﬁtting example on PTB data. BERT-\nAll: BERT with fully ﬁne-tuning including the last\nlayer. BERT-CAS: BERT with coordinate archi-\ntecture search.\nlinear output layer (BERT-All) on PTB data as an\nexample, to illustrate the over-ﬁtting issue. From\nthe results in Table 3, we can see that, by leverag-\ning CAS, we marginally relieve the over-ﬁtting is-\nsue by ﬁxing a subset of weights of the full Trans-\nformer architecture.\nLet’s look into the details of adding LSTMs.\nThere are 4 cases:\nOnly-LSTM implements a model consisting only\n7\nTest Perplexity\n0\n100\n200\n300\n400\n500\n600\nBERT GPT\nOnly-LSTM None-LSTM\nFirst-LSTM Last-LSTM\nFigure 4: LSTM variants for Penn TreeBank. We\nstudy whether to add LSTMs before or after the\ntransformer layers (or none at all).\nof LSTM layers. To make up for the loss\nof expressiveness due to removing all Trans-\nformer blocks we add a total of 6 LSTM lay-\ners.\nNone-LSTM adds no LSTM layer at all. Instead,\nwe add another stack of Transformer blocks.\nThis effectively doubles the number of blocks\nto 24.\nFirst-LSTM adds LSTM layers only before all\nTransformer blocks.\nLast-LSTM adds LSTM layers only after all\nTransformer blocks.\nThe results are shown in Table 4 and in Figure 4.\nAs can be seen, neither purely transformer blocks\nnor purely LSTM layers are effective for language\nmodeling. The former is likely unsuitable due\nto the comparatively large number of parameters\nrelative to the tuning set. Adding LSTM lay-\ners properly into Transformer architecture signiﬁ-\ncantly improves the perplexity. In addition, adding\nLSTMs before the output linear layer outperforms\nreplacing positional and segment embeddings with\nLSTM layers. These results conﬁrm our intu-\nition and indicate that we need to ﬁrst preserve\nthe coarse-grained representation using ﬁxed sub-\nset weights; subsequently LSTMs can be used to\nmodel the word order dependency.\n4.5 Efﬁciency Analysis\nLastly, we compare CAS with other existing neu-\nral network search methods in terms of search\ncost. The main distinction being that we signif-\nicantly constrain the architectures to be investi-\nConstraints Validation Test\nBERT\nOnly\nLSTM\n107.47 89.82\nNone 491.75 425.32\nFirst 67.85 57.71\nLast 39.97 34.47\nGPT\nOnly\nLSTM\n75.76 66.56\nNone 579.77 510.49\nFirst 70.05 60.82\nLast 46.24 40.87\nTable 4: Effects of different search constraints for\nplacing the LSTM on perplexity on the PTB data.\ngated. This allows us to obtain signiﬁcant com-\nputational savings.\nNAS by (Zoph and Le, 2016) is a reinforcement\nlearning based search method, which uses a\nrecurrent network to generate the model de-\nscriptions of neural networks and minimizes\nthe expected perplexity of the generated ar-\nchitectures on the PTB validation set.\nENAS by (Pham et al., 2018) also leverages a re-\ninforcement learning search method. ENAS’s\nsearch space is the superposition of all pos-\nsible child models in the NAS search space,\nwhich allows parameters to be shared among\nall child models.\nDARTS by (Liu et al., 2018) is a recently pro-\nposed neural architecture search algorithm\nbased on gradient descent.\nWe evaluate the efﬁciency of the methods using\nGPU days. The search costs of NAS, ENAS and\nDARTS are obtained from (Liu et al., 2018).\nThe reported search costs of the above methods\ncompared to CAS are shown in Table 5. As can be\nseen, BERT-CAS is cheaper than all others. The\nresults indicate that by leveraging the prior knowl-\nedge of the design of the neural networks for spe-\nciﬁc tasks, we could only optimize the architec-\ntures in a small conﬁned sub-space, that leads to\nspeed up the search process. For example, BERT-\nCAS is directly based on BERT, applying search\nupon such effective neural networks could facili-\ntate the adaptation to similar tasks.\nThe reason of the search cost of GPT-CAS on\nWT-2 is higher than ENAS is three-fold:\n1. ENAS is directly transferring an architecture\nsearched based on PTB to WT-2. Instead\nwe apply coordinate search to ﬁnd one from\nscratch;\n8\nSearch Method\nSearch Cost\n(GPU days) Method Class\nPTB WT-2\nNAS (Zoph and Le, 2016) 1,000 CPU days n.a. reinforcement\nENAS (Pham et al., 2018) 0.5 0.5 reinforcement\nDARTS (ﬁrst order) (Liu et al., 2018) 0.5 1 gradient descent\nDARTS (second order) (Liu et al., 2018) 1 gradient descent\nBERT-CAS (Our) 0.15 0.38 greedy search\nGPT-CAS (Our) 0.23 0.53 greedy search\nTable 5: Efﬁciency of different search methods on PTB and WT-2.\nModel Parameters\nDatasets\nPTB WT-2 WT-103\nGPT-2\n345M 47.33 22.76 26.37\n762M 40.31 19.93 22.05\n1542M 35.76 18.34 17.48\nBERT-Large-CAS 395M 31.34 34.11 20.42\nTable 6: Compare model parameter size and results with GPT-2. The GPT-2 model size and results are\nfrom (Radford et al., 2019).\nModel Training Data Tokens\nGPT-2 WebText 14.0B\nBERT-Large-CAS\nPTB 0.1B\nWT-2 0.2B\nWT-103 1.0B\nTable 7: Compare training data size with GPT-2.\n2. The model size of GPT-CAS is 149M, which\nis much larger compared to the size 37M\nfrom ENAS;\n3. The GPT vocabulary size is 10k larger com-\npared to the ENAS’s vocabulary.\nWe note that the difference in vocabularies might\naffect the results, since the results of NAS, ENAS\nand DARTS are from the original implementa-\ntions. The original implementations are based on\nbasic word tokenization (such as space splitter)\nof the PTB and WT-2. Instead, we are using the\nsub-word tokenization (WordPiece and BPE re-\nspectively) for BERT and GPT architecture ex-\nploration. However, the vocabulary size after ba-\nsic tokenization processing is similar to the re-\nsults after the sub-word tokenization, which are all\naround 30k-40k. Given that, we consider the per-\nformance comparison as fair 2.\n4.6 Comparison with GPT-2\nWe speciﬁcally compare the proposed model with\nthe recent state-of-the-art language model GPT-\n2 (Radford et al., 2019) on three dimensions: re-\nsults3, parameter size, and scale of the training\ndata. From the results shown in Table 6, we con-\nclude that with comparable size of the models’\nparameters, BERT-Large-CAS outperforms GPT-\n2 (345M) by on average 10.97 PPL on PTB and\nWT-103. More surprisingly, the proposed method\nperforms better than GPT-2 (1542M) which has\naround 4 times more parameters. On WT-103,\nBERT-Large-CAS is better than GPT-2 (762M)\nwhich has around 2 times more parameters. Note\nthat on WT-2, our method performs worse than\nGPT-2, we suspect the reason is that the Web-\nText still contains the texts that are similar to\nthe Wikipedia. WT-2 is quite small in terms of\nscale. In contrast, we regard the results on WT-103\n(50 times larger than WT-2) as a more reasonable\ncomparison with GPT-2.\nThe training data described in Table 7 suggests\n2The PPL results are not comparable since the vocabular-\nies are different (i.e., sub-word versus word level), we omit\nthe comparison here.\n3The results comparison is fair since GPT-2’s vocabulary\nis also based on sub-word tokenization.\n9\nthat, with signiﬁcantly smaller training datasets,\nthe proposed method generates competitive re-\nsults. Once GPT-2 models are released, we expect\nCAS could generalize to the GPT-2 models to ob-\ntain better results for language model task.\n5 Related Work\nArchitecture search has shown promising results\nin tasks such as image classiﬁcation (Zoph and\nLe, 2016; Liu et al., 2017a,b; Real et al., 2018;\nZoph et al., 2018; Liu et al., 2018), object de-\ntection (Zoph et al., 2018) as well as language\nmodeling (Zoph and Le, 2016; Pham et al., 2018;\nLiu et al., 2018) in NLP. Existing neural architec-\nture search studies focus on leveraging different\nmethods to build the neural network from scratch.\nFor example, NAS (Zoph and Le, 2016) uses re-\ninforcement learning to obtain an architecture for\nCIFAR-10 and ImageNet. Designing the archi-\ntecture from scratch using reinforcement learning\nis very costly. Many follow-up studies focus on\nspeeding up the search process by weight-sharing\nacross child models (Pham et al., 2018; Cai et al.,\n2018), by incorporating a particular structure into\nthe search space (Liu et al., 2017a,b), or by en-\nabling weights prediction for each architecture\n(Brock et al., 2017; Baker et al., 2017). Different\nfrom the above methods, the proposed coordinate\nsearch does not involve any controllers.\nRecent studies start to explore using the idea\nof network transformation within reinforcement\nlearning (Cai et al., 2018) or via Bayesian opti-\nmization (Jin et al., 2018) or simple greedy search\n(Elsken et al., 2017). DARTS (Liu et al., 2018) en-\nables gradient descent to optimize the architecture.\nCompared to these methods, the coordinate search\nis more straightforward and more efﬁcient due to\nthe direct incorporation of the pre-deﬁned Trans-\nformer architecture. Notably, the major differ-\nence of the proposed search algorithm compared\nto the existing methods is that we focus on adapt-\ning an existing well-trained Transformer architec-\nture with minimum changes in the task of lan-\nguage model, whereas a majority of the existing\nwork focus on generating variants of RNN cells\nfrom scratchfor better results.\nLanguage models have been studied exten-\nsively in NLP. Neural language models have sup-\nplanted traditional n-gram models in recent years\n(Bengio et al., 2003; Mnih and Hinton, 2007;\nMikolov et al., 2010). Particularly, recurrent neu-\nral networks (Inan et al., 2016; Merity et al., 2017;\nMelis et al., 2017; Krause et al., 2018), such as\nLSTMs have achieved state-of-the-art results on\nvarious benchmark datasets with different regu-\nlarization techniques and post-training methods\n(Grave et al., 2016; Krause et al., 2018). The mix-\nture of softmax (Yang et al., 2017) has helped ad-\ndress the low-rank embedding problem for word\nprediction. We used this in our model, too. It\nprovides some improvement over a more conven-\ntional model.\nThe recently proposed GPT-2 (Radford et al.,\n2019) is a deeper Transformer decoder based lan-\nguage model trained on a 40GB dataset. In con-\ntrast, the proposed model generates competitive\nresults but with signiﬁcantly less training cost and\nsmaller model size. Transformer-XL (Dai et al.,\n2019) is a word level language model that also de-\nlivers good results by incorporating longer con-\ntext. The proposed method is a sub-word level\nlanguage model thus the results are not compara-\nble. We expect to generalize CAS to pre-trained\nTransformer-XL models as well to achieve better\nresults. The adaptive input representations idea\nproposed in (Baevski and Auli, 2018) could be\ncombined with the proposed method to further\nspeed up.\nNetwork transformations were introduced in\nthe context of the transfer learning (Chen et al.,\n2015). The main purpose of the transformations\nis to make networks deeper and wider. Often\nstagewise training accelerates training and archi-\ntecture search. Recent studies (Wei et al., 2016;\nCai et al., 2018; Elsken et al., 2017) focus on ex-\ntending the set of the network transformations to\nhandle additional operations such as non-linear ac-\ntivation functions and skip connections. We in-\nstead introduce simple network modiﬁcations to\nperform modest modiﬁcations of an existing net-\nwork. They allow us to treat a pre-trained Trans-\nformer block in a manner similar to that of a large\npre-trained embedding vector.\n6 Conclusion\nWe study the problem of ﬁnding an effective\nTransformer architecture for language model. We\nidentify the issues of existing Transformer archi-\ntectures, such as BERT and GPT, that are not able\nto capture the strong word-level context required\nin language model. We proposed two approaches\nto address this issue: we ﬁne-tune a subset of pa-\n10\nrameters to improve the coarse-grain representa-\ntions obtained from the pre-trained Transformer\nmodels. Secondly, we add LSTM layers to cap-\nture the ﬁne-grained sequence. We then propose\na coordinate architecture search (CAS) algorithm\nto select an effective architecture based on ﬁne-\ntuning results. It uses a greedy search strategy to\naccelerate architecture search. We experimentally\nshow that CAS outperforms the state-of-the-art\nlanguage models on three language model bench-\nmark datasets.\nAlthough we only show the effectiveness of\nCAS when applying Transformer architectures to\nthe language model task, we feel it is possible to\napply CAS to both other neural network architec-\ntures and ﬁne-tuning other NLP tasks that require\nstrong word-level context as well.\nReferences\nAlexei Baevski and Michael Auli. 2018. Adaptive\ninput representations for neural language mod-\neling. arXiv preprint arXiv:1809.10853.\nBowen Baker, Otkrist Gupta, Ramesh Raskar, and\nNikhil Naik. 2017. Accelerating neural ar-\nchitecture search using performance prediction.\narXiv preprint arXiv:1705.10823.\nYoshua Bengio, Réjean Ducharme, Pascal Vin-\ncent, and Christian Jauvin. 2003. A neu-\nral probabilistic language model. JMLR,\n3(Feb):1137–1155.\nAndrew Brock, Theodore Lim, James M Ritchie,\nand Nick Weston. 2017. Smash: one-shot\nmodel architecture search through hypernet-\nworks. arXiv preprint arXiv:1708.05344.\nHan Cai, Tianyao Chen, Weinan Zhang, Yong\nYu, and Jun Wang. 2018. Efﬁcient architecture\nsearch by network transformation. AAAI.\nTianqi Chen, Ian J. Goodfellow, and Jonathon\nShlens. 2015. Net2net: Accelerating learning\nvia knowledge transfer. CoRR.\nZihang Dai, Zhilin Yang, Yiming Yang,\nWilliam W Cohen, Jaime Carbonell,\nQuoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. 2018. Bert: Pre-\ntraining of deep bidirectional transformers\nfor language understanding. arXiv preprint\narXiv:1810.04805.\nThomas Elsken, Jan Hendrik Metzen, and Frank\nHutter. 2017. Simple and efﬁcient architec-\nture search for convolutional neural networks.\nCoRR.\nEdouard Grave, Armand Joulin, and Nicolas\nUsunier. 2016. Improving neural language\nmodels with a continuous cache. CoRR.\nHakan Inan, Khashayar Khosravi, and Richard\nSocher. 2016. Tying word vectors and word\nclassiﬁers: A loss framework for language mod-\neling. arXiv preprint arXiv:1611.01462.\nHaifeng Jin, Qingquan Song, and Xia Hu.\n2018. Efﬁcient neural architecture search\nwith network morphism. arXiv preprint\narXiv:1806.10282.\nDiederik P Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nBen Krause, Emmanuel Kahembwe, Iain Murray,\nand Steve Renals. 2018. Dynamic evaluation of\nneural sequence models. In ICML, pages 2771–\n2780.\nTao Lei, Yu Zhang, Sida I Wang, Hui Dai, and\nYoav Artzi. 2018. Simple recurrent units for\nhighly parallelizable recurrence. In Proceed-\nings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages\n4470–4481.\nChenxi Liu, Barret Zoph, Jonathon Shlens, Wei\nHua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan\nHuang, and Kevin Murphy. 2017a. Progres-\nsive neural architecture search. arXiv preprint\narXiv:1712.00559.\nHanxiao Liu, Karen Simonyan, Oriol Vinyals,\nChrisantha Fernando, and Koray Kavukcuoglu.\n2017b. Hierarchical representations for ef-\nﬁcient architecture search. arXiv preprint\narXiv:1711.00436.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2018. DARTS: differentiable architecture\nsearch. CoRR.\n11\nGábor Melis, Chris Dyer, and Phil Blunsom.\n2017. On the state of the art of evaluation\nin neural language models. arXiv preprint\narXiv:1707.05589.\nStephen Merity, Nitish Shirish Keskar, and\nRichard Socher. 2017. Regularizing and opti-\nmizing LSTM language models. CoRR.\nStephen Merity, Caiming Xiong, James Bradbury,\nand Richard Socher. 2016. Pointer sentinel\nmixture models. CoRR.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Bur-\nget, Jan ˇCernock`y, and Sanjeev Khudanpur.\n2010. Recurrent neural network based language\nmodel. In Eleventh Annual Conference of the\nInternational Speech Communication Associa-\ntion.\nAndriy Mnih and Geoffrey Hinton. 2007. Three\nnew graphical models for statistical language\nmodelling. In ICML, pages 641–648.\nHieu Pham, Melody Y . Guan, Barret Zoph,\nQuoc V . Le, and Jeff Dean. 2018. Efﬁcient neu-\nral architecture search via parameter sharing. In\nICML, pages 4092–4101.\nAlec Radford, Karthik Narasimhan, Tim Sal-\nimans, and Ilya Sutskever. 2018. Improv-\ning language understanding by generative\npre-training. URL https://s3-us-west-2.\namazonaws. com/openai-assets/research-\ncovers/language-unsupervised/language_\nunderstanding_paper. pdf.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learn-\ners.\nEsteban Real, Alok Aggarwal, Yanping Huang,\nand Quoc V Le. 2018. Regularized evolution\nfor image classiﬁer architecture search. arXiv\npreprint arXiv:1802.01548.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Neural machine translation of rare\nwords with subword units. In ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In NIPS, pages 5998–\n6008.\nAlex Wang, Amapreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel R Bowman.\n2018. Glue: A multi-task benchmark and anal-\nysis platform for natural language understand-\ning. arXiv preprint arXiv:1804.07461.\nTao Wei, Changhu Wang, Yong Rui, and\nChang Wen Chen. 2016. Network morphism.\nIn ICML, pages 564–572.\nYonghui Wu, Mike Schuster, Zhifeng Chen,\nQuoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, et al. 2016. Google’s neural\nmachine translation system: Bridging the gap\nbetween human and machine translation. arXiv\npreprint arXiv:1609.08144.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov,\nand William W. Cohen. 2017. Breaking the\nsoftmax bottleneck: A high-rank RNN lan-\nguage model. CoRR.\nBarret Zoph and Quoc V . Le. 2016. Neural ar-\nchitecture search with reinforcement learning.\nCoRR.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens,\nand Quoc V . Le. 2018. Learning transferable\narchitectures for scalable image recognition. In\nCVPR, pages 8697–8710.\n12",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9238729476928711
    },
    {
      "name": "Transformer",
      "score": 0.8348867893218994
    },
    {
      "name": "Language model",
      "score": 0.8283988237380981
    },
    {
      "name": "Computer science",
      "score": 0.8046388626098633
    },
    {
      "name": "Architecture",
      "score": 0.61053466796875
    },
    {
      "name": "Computation",
      "score": 0.5754784941673279
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4905783236026764
    },
    {
      "name": "Natural language processing",
      "score": 0.44920340180397034
    },
    {
      "name": "Language understanding",
      "score": 0.4241166114807129
    },
    {
      "name": "Programming language",
      "score": 0.21294447779655457
    },
    {
      "name": "Engineering",
      "score": 0.07137435674667358
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    }
  ]
}