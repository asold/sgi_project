{
    "title": "Exploring Prompting Large Language Models as Explainable Metrics",
    "url": "https://openalex.org/W4392669762",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5109690441",
            "name": "Ghazaleh Mahmoudi",
            "affiliations": [
                "Iran University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4319793767",
        "https://openalex.org/W4389519239",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W4389518754",
        "https://openalex.org/W1555168845",
        "https://openalex.org/W4392669714",
        "https://openalex.org/W3159259047",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4389519254",
        "https://openalex.org/W4322760121"
    ],
    "abstract": "This paper describes the IUST NLP Lab submission to the Prompting Large Language Models as Explainable Metrics Shared Task at the Eval4NLP 2023 Workshop on Evaluation & Comparison of NLP Systems. We have proposed a zero-shot prompt-based strategy for explainable evaluation of the summarization task using Large Language Models (LLMs). The conducted experiments demonstrate the promising potential of LLMs as evaluation metrics in Natural Language Processing (NLP), particularly in the field of summarization. Both few-shot and zero-shot approaches are employed in these experiments. The performance of our best provided prompts achieved a Kendall correlation of 0.477 with human evaluations in the text summarization task on the test data.",
    "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 219–227\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nExploring Prompting Large Language Models as Explainable Metrics\nGhazaleh Mahmoudi\nSchool of Computer Engineering\nIran University of Science and Technology\nGh_Mahmoodi@comp.iust.ac.ir\nAbstract\nThis paper describes the IUST NLP Lab sub-\nmission to the Prompting Large Language Mod-\nels as Explainable Metrics Shared Task at the\nEval4NLP 2023 Workshop on Evaluation &\nComparison of NLP Systems. We have pro-\nposed a zero-shot prompt-based strategy for ex-\nplainable evaluation of the summarization task\nusing Large Language Models (LLMs). The\nconducted experiments demonstrate the promis-\ning potential of LLMs as evaluation metrics in\nNatural Language Processing (NLP), particu-\nlarly in the field of summarization. Both few-\nshot and zero-shot approaches are employed\nin these experiments. The performance of our\nbest provided prompts achieved a Kendall cor-\nrelation of 0.477 with human evaluations in the\ntext summarization task on the test data. Code\nand results are publicly available on GitHub 1.\n1 Introduction\nSummarization is crucial for quickly understanding\nlarge textual documents. The goal of text summa-\nrization is to condense lengthy documents into a\nconcise, coherent, and easily understandable for-\nmat while retaining the essential information from\nthe original source. However, assessing the qual-\nity and performance of summarization systems has\nproven to be a challenging task. Commonly used\nevaluation methods for summarization, such as\nROUGE scores (Lin, 2004), have certain limita-\ntions. They fail to capture the overall quality, co-\nherence, and interpretability of summaries. Addi-\ntionally, they rely on human-generated reference\nsummaries, which are time-consuming and subjec-\ntive. Given the limitations of traditional evaluation\napproaches, it is important to explore alternative\nevaluation methods that leverage the capabilities of\nLLMs and offer explainable metrics.\n1https://github.com/ghazaleh-\nmahmoodi/Prompting_LLMs_AS_Explainable_Metrics\nLLMs, such as GPT-3 (Brown et al., 2020) and\nLLaMA (Touvron et al., 2023), have shown re-\nmarkable summarization capabilities. They can\ngenerate coherent and contextually grounded sum-\nmaries. This makes them ideal for evaluation pur-\nposes. LLMs provide both interpretability and in-\nherent summarization abilities. They can generate\nexplanations and reasoning for their predictions,\ngiving evaluators a deeper understanding of system\nstrengths and weaknesses.\nMoreover, LLMs reduce the dependency on gold-\nstandard reference summaries. By using LLMs\nas evaluators, we can generate comparative sum-\nmaries and objectively assess system-generated\nsummaries. This eliminates potential biases from\nhuman references.\nIn summary, using LLMs as explainable metrics\nin summarization evaluation offers several benefits.\nIt overcomes the limitations of traditional methods,\nprovides interpretability, and reduces reliance on\nhuman-generated references. This emerging field\nof research holds promise for a more comprehen-\nsive and objective assessment of summarization\nsystems.\nThe main contribution of this paper is the in-\nvestigation of various prompt-based methods for\nexplainable evaluation of summarization tasks. We\nexplore both few-shot and zero-shot approaches\nin our experiments. The best performance prompt\nfollows the zero-shot strategy and is introduced\nin the paper under the name P1. In this prompt,\nthe criteria for assessing the quality of summaries\nare described (e.g., how well the main idea of the\nmain document is captured in the summary). This\nprompt achieves a Kendall correlation score of\n0.477, outperforming other methods in comparison.\nOur conducted experiments highlight the promising\npotential of LLMs as evaluation metrics in the field\nof NLP, with a specific focus on summarization.\n219\n2 Related Work\nSeveral recent studies have focused on utilizing\nLLMs for the evaluation of several different tasks\nin NLP (e.g., text generation, machine translation,\nand summarization).\nGPTScore (Fu et al., 2023) is a novel frame-\nwork for evaluating generated texts using large\npre-trained language models, particularly GPT-3.\nThe framework leverages the emergent abilities of\ngenerative pre-trained models, such as zero-shot\ninstruction, to score generated texts. GPTScore op-\nerates under the assumption that a large pre-trained\nlanguage model is more likely to assign higher\nprobabilities to high-quality generated text when\nprovided with adequate instruction and context. By\nleveraging the power of GPT-3, GPTScore aims to\nassess the quality of generated text based on the\nmodel’s generative capabilities.\nIn a similar vein, Wang et al. (2023) conducted a\npreliminary survey on using ChatGPT, a variant of\nthe GPT model, as a natural language generation\n(NLG) evaluator. The study explores the potential\nof ChatGPT in evaluating the quality of generated\ntext in various NLG tasks.\nIn the context of translation quality assessment,\nGEMBA (Kocmi and Federmann, 2023) is intro-\nduced as a GPT-based metric that can effectively\nevaluate translations with or without a reference\ntranslation. The evaluation focuses on zero-shot\nprompting and involves comparing four prompt\nvariants in two modes, depending on the availabil-\nity of a reference. Results from the evaluation\ndemonstrate that GEMBA achieves state-of-the-art\naccuracy when compared to MQM-based human\nlabels, as evidenced by the WMT22 Metrics shared\ntask.\nInstructscore (Xu et al., 2023) is an open-source\nand explainable evaluation metric for text genera-\ntion. This model fine-tunes the LLaMA model to\npredict a fine-grained error diagnosis of machine\ntranslated content. This work presents a novel\nframework for explainable text generation evalu-\nation, addressing the limitations associated with\nblack-box metrics and showcasing the potential\nof LLMs to provide meaningful and interpretable\nevaluations.\nG-Eval (Liu et al., 2023), is a framework that\nutilizes LLMs with chain-of-thoughts (CoT) and\na form-filling paradigm to assess the quality of\nNLG outputs, specifically in text summarization\nand dialogue generation tasks. The experiments\ndemonstrate that G-Eval, utilizing GPT-4 as the\nbackbone model, achieves a high Spearman cor-\nrelation of 0.514 with human evaluations in the\ntext summarization task, outperforming previous\nmethods significantly.\n3 Task Description\nThe topic of Eval4NLP shared task (Leiter et al.,\n2023) is to provide explainable metrics for sum-\nmarization and machine translation evaluation by\nprompting LLMs. The main goal is to investigate\ndifferent prompting methods (e.g., zero-shot, few-\nshot, Chain of Thought, Fine-Grained, Majority\nV ote, Self-Refinement), therefore,fine-tuning the\nLLMs is not allowed. Also, a number of LLMs\nare allowed to be used. The shared task has two\ntracks based on the model sizes (One for models\nbigger than 25B parameters, and one for smaller\nmodels).\nThis work has been done on the summarization\ntask and using small models. In the following, the\ndataset and the evaluation metric are explained.\n3.1 Data\nThe Shared Task organizers opted for SummEval\nduring the training and development phase for sum-\nmarization. (Fabbri et al., 2021) introduced Sum-\nmEval as an evaluation benchmark, aiming to com-\npare various methods for assessing summarization.\nThis benchmark entails the assignment of human\nratings on four key dimensions of every summary,\nincluding fluency, coherence, consistency,\nand relevance . SummEval draws upon the\nrenowned CNN/DailyMail dataset proposed by\n(Hermann et al., 2015) for its construction.\nFurthermore, in the testing phase, a new\nreference-free dataset with summary-level quality\nscores is collected for summarization. As source\ndata, sentences and paragraphs were collected from\nEnglish Wikipedia pages created after 15.07.2023.\nTest-phase scores are constructed from fine-grained\nscores.\n3.2 Evaluation Metrics\nTo determine how well LLMs explainable metrics\ncorrelate with humans, We follow the evaluation\nprotocol of the WMT22 metrics shared task. we use\nKendall’s Tau correlation (Freitag et al., 2022). In\naddition to Kendall correlation, Pearson (Mukaka,\n2012) and Spearman (Zar, 2005) are also used in\nthe test phase.\n220\n4 Methodology\nIn this research, We have used pre-trained\norca_mini_v3_7b (tuned Llama2-7b model)\n(Mathur, 2023) on the HuggingFace Transformers2.\nWe employed two strategies, zero-shot and\nfew-shot, for constructing prompts.\nThe zero-shot strategy included the combina-\ntion of evaluation criteria for the quality of summa-\nrization in the form of questions or detailed expla-\nnations provided to the model.\nThere are examples of summarization evaluation\nwritten in the few-shot strategy. In this way, the\nmain document, the summarized document, and\nthe scores received are mentioned precisely.\nTo assess summarization quality via prompting\nan LLM, the following parameters are needed:\n• Prompt variant (from a pre-defined set)\n• Main document Source Text\n• Summary Summary\n4.1 Prompt variants\nFor P1 (Table 2), we formulated the main criteria\nfor assessing summary quality, which were orig-\ninally expressed by humans. In this prompt, the\nfollowing items are mentioned to be checked:\n• Comparing the key points.\n• Capturing the main ideas.\n• Score on a continuous scale from 0 to 100.\n• Meaning of zero score: irrelevant, factually\nincorrect, and not readable summary.\n• Meaning of a hundred score: relevant, factu-\nally correct, good readability summary.\n• Explain the result.\nTo create prompt P2(Table 3), we consulted the\nChatGPT4 Bot and asked what questions would\nbe relevant for evaluating summarization. We then\nmodified and adapted the generated questions ac-\ncordingly. In this prompt, in addition to the items\nmentioned in P1, the following items have been\nadded in the form of questions.\n• The overall length of the summary. Concise\nrepresentation of the original document.\n• Grammatical accuracy and fluency of the sum-\nmary.\n• Evaluate The ranking of information in the\nsummary.\n• Analyze the level of abstraction in the sum-\nmary.\n2https://huggingface.co/pankajmathur/orca_mini_v3_7b\n• Contextual understanding is exhibited by the\nsummary.\nPrompts P1 and P2 also include an Explanation for\nthe model’s output score, thus containing questions\nthat aid in better understanding the received score’s\nreasoning.\nPrompt P3(Table 4) and P4(Table 5) are similar\nto the P1 prompts, and only the wording and the\nway of expression have changed.\nIn P5 (Table 6), we guide the model to calculate\nthe desired score by calculating the similarity of\nthe main and summarized documents. P5 includes\nexamples of how one can calculate the similarity\nof two documents.\nPrompt P6 (Table 7) follows the few-shot strat-\negy, where two examples consisting of the main\ndocument, and the written summary, along with\ntheir respective score, are included in the input\nprompt.\n5 Results and Analysis\nWe experiment with six different distinct prompt\ntypes. One of them is few-shot (P6) and the rest\nare zero-shot. Table 1 shows results for all prompt\nvariants we have experimented with.\nKendall Pearson Spearman\nP1 0.477 0.495 0.619\nP2 0.470 0.468 0.607\nP3 0.472 0.498 0.612\nP4 0.467 0.504 0.610\nP5 0.454 0.543 0.589\nP6 0.283 0.513 0.376\nTable 1: Test phase Segment-level Kendall ( τ) and\nPearson (ρ) and Spearman correlation.\nThe execution of each prompt takes approxi-\nmately 1 hour. If we also include the explanation\nof the results in the output, each execution of the\ntest data takes close to 4 hours.\nBased on the Kendall measure (which serves\nas the primary evaluation metric), the best result\nis associated with P1. This prompt follows the\nzero-shot strategy. In this prompt, some of the\nSummEval criteria are also mentioned. Addition-\nally, P1 achieves the highest value in the Spearman\nmeasure and serves as the final strategy for the test\nphase.\nThe results of P2, P3, and P4 are very close to\neach other. The reason for the difference observed\n221\nPrompt P1:\nScore the effectiveness of the summarization by comparing the key points and overall coherence of\nthe summarized with the main document.\nChecked whether the summary captures the main ideas, maintains the intended tone and style, and\nprovides a concise yet comprehensive overview of the main document.\nScore the summarization with respect to the summarized document on a continuous scale from 0\nto 100, where a score of zero means irrelevant, factually incorrect and good readable and\nscore of 100 means relevant, factually correct, no readability summarized.\nAlso explain your process to get this score to summary.\nAlso please perform error Analysis of given summary.\nWhat should we change to have a better result?\",\nmain document: {main document},\nSummary: {Summary}\",\nScore: gen ’score’ pattern=’(100|[1-9]?[0-9])’,\nExplanation: gen ’explanation’\nTable 2: The best-performing prompt based on zero-shot prompting strategy expecting a score between 0–100.\nis the variation in the way the evaluation method is\nexpressed. In this regard, it can be said that LLMs\nare sensitive to manner of expression. However,\nconsidering the proximity of the Kendall output\nvalue, it can be concluded that they have a low\nsensitivity to the mentioned changes.\nFurthermore, considering the results of P5, it\ncan be stated that introducing scientific methods\nfor examining the similarity between summaries\nand the main document did not effectively guide\nthe model. Instead, criteria such as \"captures the\nmain ideas\" yielded better results.\nContrary to our expectations, P6 (few-shot ap-\nproach) obtains the lowest score in the Kendall mea-\nsure. We expected that the few-shot strategy would\noutperform zero-shot since it allows the model to\nobserve multiple instances of scoring, thereby en-\nhancing its capabilities. However, our experiments\nyielded results contrary to this assumption. There\nmay be several reasons for this result. It is possible\nthat a larger number of input samples would have\nbeen beneficial. Furthermore, the quality of the in-\nput samples might not have been sufficient for the\nmodel to comprehend the problem-solving process\nfully.\nIn conclusion, based on the obtained results, it\ncan be inferred that by explicitly defining evalua-\ntion metrics, language models can be utilized as an\ninterpretable method for evaluating the summariza-\ntion task.\n6 Conclusion\nIn this paper, we have investigated different\nprompts to define explainable evaluation metrics\nfor summarization Using LLMs.\nThe experiments conducted indicate that LLMs\nhave great potential as evaluation metrics in NLP\ntasks, especially summarization. In these experi-\nments, both the few-shot and zero-shot approaches\nwere used. Our best prompt achieved a Kendall\ncorrelation of 0.477 compared to the human score.\nIn future work, other prompt strategies, such as\nthe Chain of Thought, can also be explored. Exper-\niments can also be repeated with existing prompts\nand other Language Models and compare the re-\nsults obtained to determine the effect of the Lan-\nguage Model on changing the quality of the output.\n7 Limitations\nDue to hardware limitations, we were unable to\ninvestigate other eligible models in this series of\nexperiments. In future research, it would be benefi-\ncial to examine the impact of other models on the\nintroduced prompts more extensively. Furthermore,\nthe lack of fine-tuning the model on the defined\ntasks may have an effect on its performance. In\nfuture research, by fine-tuning the model, we can\nexplore its impact on improving the output quality.\n222\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nAlexander R. Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. SummEval: Re-evaluating Summariza-\ntion Evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nKarl Moritz Hermann, Tomáš Koˇciský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend.\nTom Kocmi and Christian Federmann. 2023. Large lan-\nguage models are state-of-the-art evaluators of trans-\nlation quality. In Proceedings of the 24th Annual\nConference of the European Association for Machine\nTranslation, pages 193–203, Tampere, Finland. Euro-\npean Association for Machine Translation.\nChristoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao,\nRotem Dror, and Steffen Eger. 2023. The eval4nlp\n2023 shared task on prompting large language models\nas explainable metrics. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nPankaj Mathur. 2023. orca_mini_v3_7b: An ex-\nplain tuned llama2-7b model. https://https://\nhuggingface.co/psmathur/orca_mini_v3_7b,.\nMM Mukaka. 2012. Statistics corner: A guide to ap-\npropriate use of correlation coefficient in medical\nresearch. Malawi medical journal : the journal of\nMedical Association of Malawi, 24(3):69—71.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nJiaan Wang, Yunlong Liang, Fandong Meng, Zengkui\nSun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,\nand Jie Zhou. 2023. Is chatgpt a good nlg evaluator?\na preliminary study.\nWenda Xu, Danqing Wang, Liangming Pan, Zhenqiao\nSong, Markus Freitag, William Yang Wang, and Lei\nLi. 2023. Instructscore: Towards explainable text\ngeneration evaluation with automatic feedback.\nJerrold H. Zar. 2005. Spearman Rank Correlation. John\nWiley Sons, Ltd.\nA Appendix: Prompt Templates\nBelow, we present our prompt templates utilized in\nthe described experiments in this paper.\n223\nPrompt P2:\nScore the effectiveness of the summarization by comparing the key points and\noverall coherence of the summarized with the main document.\nChecked whether the summary captures the main ideas, maintains the intended\ntone and style, and provides a concise yet comprehensive overview of the main\ndocument.\nScore the summarization with respect to the summarized document on a contin-\nuous scale from 0 to 100, where a score of zero means irrelevant, factually\nincorrect and no readability and score of 100 means relevant, factually\ncorrect, good readable summarized.\nTo calculate Score, first answer the following questions.\nThen, according to the answers to the questions, scored the quality between 0 and\n100.\n1. Assess the overall length of the summary. Does it provide a concise representation\nof the original document without omitting important information?\n2. Examine the grammatical accuracy and fluency of the summary. Are the sentences\nwell-structured, free of errors, and coherent?\n3. Evaluate the ranking of information in the summary. Are the most salient and\ncrucial details given appropriate emphasis and positioned prominently?\n4. Analyze the level of abstraction in the summary. Does it effectively distill complex\nideas and concepts into more accessible and simplified language?\n5. Consider the contextual understanding exhibited by the summary. Does it demon-\nstrate an understanding of the original text beyond simple keyword extraction?\nAlso explain your process to get this score to summary.\nAlso please perform error Analysis of given summary.\nWhat should we change to have a better result?\",\nmain document: {main document},\nSummary: {Summary}\",\nScore: gen ’score’ pattern=’(100|[1-9]?[0-9])’,\nExplanation: gen ’explanation’\nTable 3: Prompt P2\n224\nPrompt P3:\nYour Task is to score the Samaritan quality. The original document is collected from\nEnglish Wikipedia pages created after 15.07.2023.\nScore the effectiveness of the summarization by comparing the key points and overall\ncoherence of the summarized with the main document.\n\"Checked whether the summary captures the main ideas, maintains the intended tone\nand style, and provides a concise yet comprehensive overview of the main document.\nScore the summarization with respect to the summarized document on a contin-\nuous scale from 0 to 100, where a score of zero means irrelevant, factually\nincorrect and not readable and score of 100 means relevant, factually\ncorrect, good readability, grammatical correctness, covers the main\ntopic and key points of the main document article\nSource text: {main document},\nSummary: {Summary}\",\nScore: gen ’score’ pattern=’(100|[1-9]?[0-9])’,\nTable 4: Prompt P3\nPrompt P4:\nScore the effectiveness of the summarization by comparing the key points and overall\ncoherence of the summarized with the main document.\nChecked whether the summary captures the main ideas, maintains the intended tone\nand style, and provides a concise yet comprehensive overview of the main document.\nScore the summarization with respect to the summarized document, on a continuous\nscale from 0 to 100.\nSource text: {main document},\nSummary: {Summary}\",\nScore: gen ’score’ pattern=’(100|[1-9]?[0-9])’,\nTable 5: Prompt P4\n225\nPrompt P5:\nScore the summarization with respect to the summarized document on a continu-\nous scale from 0 to 100 , where a score of zero means irrelevant, factually\nincorrect and not readable and score of 100 means relevant, factually\ncorrect, good readability.\nlet’s think step by step.\nIn other words, this Score should show the similarity between the main document\nand the summarized document.\nFor similarity measurement, It’s possible to compare the main and summarized\ndocument with a similarity measure such as Cosine Similarity.\nword2vec, Bert embedding or n-gram are some of the approaches to calculate\nsimilarity.\nSource text: {main document},\nSummary: {Summary}\",\nScore: gen ’score’ pattern=’(100|[1-9]?[0-9])’,\nTable 6: Prompt P5\n226\nPrompt P6:\nConsider these example that summarization is graded in scale 0 - 100.\n1. Source text: Usain Bolt will compete at the IAAF/BTC World Relays in the Bahamas next month,\nthe Jamaica Athletics Administrative Association has announced. The six-time Olympic gold medallist\nwill compete at the relay championship on May 2 and 3 as part of the Jamaican team. ’I’m happy to\nbe part of the Jamaican team for the IAAF / BTC World Relays in the Bahamas. I am fit, healthy and\nready to run,’ said Bolt. Usain Bolt has confirmed he will be part of Jamaica’s team at the World Relays\nin the Bahamas Bolt reacts as he wins 4x100m gold at the London Olympic Games in 2012 ’I hear the\nmeet was a lot of fun last year and there was a great atmosphere. Jamaica has a long and successful\ntradition in relays and when we put on the national colours we always do our best to make the country\nproud,’ he added. JAAA General Secretary Garth Gayle commented, ’We were extremely pleased that\nUsain was available for selection and that the world’s fastest man will be running for Jamaica. We can\nexpect some sprint magic on the track in the Bahamas on 2nd and 3rd May.’ The full Jamaican team\nlist for the competition will be announced shortly. Bolt insists he always does ’his best to make his\ncountry proud’ while wearing Jamaica colours.\n1. Summary: Jamaican sprinter Usain Bolt has confirmed he will be part of the Jamaican team at the\nIAAF/BTC World Relays in the Bahamas.\n1. Score : 95,\n2. Source text: Referee Mark Clattenburg has been named to take charge of the Manchester derby on\nSunday, despite having sent off three players from United and City this season. City captain Vincent\nKompany was dismissed for two bookable offences during Belgium’s narrow 1-0 defeat of Israel in\ntheir Euro 2016 qualifier on March 31, meaning he is now suspended for the match against Wales in\nJune. And, although Clattenburg has been accused of favouring Louis van Gaal’s side in the past, it’s\nworth noting that the 40-year-old has only sent off two players season in the Premier League this season\nand both have been from United; Tyler Blackcett in the 5-3 defeat by Leicester and Luke Shaw in the\n1-1 draw with West Ham. Mark Clattenburg will officiate the Manchester derby between United and\nCity at Old Trafford The English referee sent off City and Belgium captain Vincent Kompany during\nthe international break Leicester 5-3 Manchester United West Ham 1-1 Manchester United Manchester\nUnited 3-0 Tottenham Manchester City 3-1 West Ham Liverpool 2-1 Manchester City Chelsea 1-1\nManchester City Clattenburg has courted controversy during his career but is generally regarded as one\nof the Premier League’s leading referees alongside Michael Oliver. The champion’s shock 2-1 loss\nto Crystal Palace on Monday saw United move a point above their local rivals to add extra incentive\nfor both sides ahead of the derby at Old Trafford, which could ultimately decide who finishes second\nbehind expected winners Chelsea. While Manuel Pellegrini’s side have struggled since the turn of the\nyear, turning from title challengers to fourth place chases, United are coasting on confidence having\nwon their last five consecutive league games. Clattenburg will be joined on Sunday by assistants Simon\nBeck and Jake Collin, while Jonathan Moss will serve as the fourth official. Clattenburg has shown\nonly two red cards this season, both to United players including Luke Shaw (centre).\n2. Summary: United’s win over Liverpool was their first league win since the 3-0 win over Leicester\non March 31 City’s win over West Ham was their first league win since the 3-0 win over Chelsea on\nMarch 31 Manchester City’s win over West Ham was their first league win since the 3-0 win over\nChelsea on March 31 Manuel Pellegrini’s side are top of the Premier League table, four points clear of\nChelsea, who have a game.\n2. Score : 26.666666666\nfollowing these examples, please score the following input.\nSource text: {main document},\nSummary: {Summary}\",\nScore: gen ’score’ pattern=’(100|[1-9]?[0-9])’,\nTable 7: Prompt P6\n227"
}