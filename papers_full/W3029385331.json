{
  "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing",
  "url": "https://openalex.org/W3029385331",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3083326127",
      "name": "Wang, Hanrui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227545689",
      "name": "Wu, Zhanghao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1907298309",
      "name": "Liu Zhijian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2437544144",
      "name": "Cai Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284357043",
      "name": "Zhu, Ligeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2386622212",
      "name": "Gan, Chuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1972140387",
      "name": "Han Song",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4297775537",
    "https://openalex.org/W2891911973",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W2886851211",
    "https://openalex.org/W3035511173",
    "https://openalex.org/W3096533519",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W3016832937",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W2932077855",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2805493160",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2948197522",
    "https://openalex.org/W2956301977",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W2994749257",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2949941638",
    "https://openalex.org/W2904529841",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2964240726",
    "https://openalex.org/W2964213727",
    "https://openalex.org/W2983981554",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3092618035",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2949541494",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W3117975394",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2954711779",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2967733054",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W3035332806",
    "https://openalex.org/W3106104873",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W3023198000",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2889964337",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W4289143671",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2799001369",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2962915948",
    "https://openalex.org/W2463507112",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2798362442",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3017024317"
  ],
  "abstract": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with $\\textit{arbitrary encoder-decoder attention}$ and $\\textit{heterogeneous layers}$. Then we train a $\\textit{SuperTransformer}$ that covers all candidates in the design space, and efficiently produces many $\\textit{SubTransformers}$ with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized $\\textit{SubTransformer}$ dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT'14 translation task on Raspberry Pi-4, HAT can achieve $\\textbf{3}\\times$ speedup, $\\textbf{3.7}\\times$ smaller size over baseline Transformer; $\\textbf{2.7}\\times$ speedup, $\\textbf{3.6}\\times$ smaller size over Evolved Transformer with $\\textbf{12,041}\\times$ less search cost and no performance loss. HAT code is https://github.com/mit-han-lab/hardware-aware-transformers.git",
  "full_text": "HAT: Hardware-Aware Transformers for\nEfﬁcient Natural Language Processing\nHanrui Wang1, Zhanghao Wu1, Zhijian Liu1, Han Cai1, Ligeng Zhu1,\nChuang Gan2, Song Han1\n1Massachusetts Institute of Technology, 2MIT-IBM Watson AI Lab\n{hanrui,zhwu,zhijian,hancai,ligeng,chuangg,songhan}@mit.edu\nAbstract\nTransformers are ubiquitous in Natural Lan-\nguage Processing (NLP) tasks, but they are dif-\nﬁcult to be deployed on hardware due to the\nintensive computation. To enable low-latency\ninference on resource-constrained hardware\nplatforms, we propose to design Hardware-\nAware Transformers (HAT) with neural archi-\ntecture search. We ﬁrst construct a large de-\nsign space with arbitrary encoder-decoder at-\ntention and heterogeneous layers. Then we\ntrain a SuperTransformer that covers all can-\ndidates in the design space, and efﬁciently\nproduces many SubTransformers with weight\nsharing. Finally, we perform an evolutionary\nsearch with a hardware latency constraint to\nﬁnd a specialized SubTransformer dedicated\nto run fast on the target hardware. Extensive\nexperiments on four machine translation tasks\ndemonstrate that HAT can discover efﬁcient\nmodels for different hardware (CPU, GPU,\nIoT device). When running WMT’14 transla-\ntion task on Raspberry Pi-4, HAT can achieve\n3× speedup, 3.7× smaller size over baseline\nTransformer; 2.7× speedup, 3.6× smaller size\nover Evolved Transformer with 12,041× less\nsearch cost and no performance loss. HAT\nis open-sourced at https://github.com/mit-han-\nlab/hardware-aware-transformers.git.\n1 Introduction\nTransformer (Vaswani et al., 2017) has been widely\nused in natural language processing tasks. By stack-\ning multiple identical encoder/decoder layers with\nattention modules, it provides a signiﬁcant perfor-\nmance improvement over previous convolutional\nor recurrent neural network models (Kim, 2014).\nNevertheless, it is challenging to deploy Trans-\nformers on mobile devices due to the high com-\nputation cost. For instance, in order to translate a\nsentence with only 30 words, a Transformer-Big\nmodel needs to execute 13G FLOPs and takes 20\nSuperTransformer\nEvolutionary Search with \nHardware Constraints\nSpecialized Deployment is Efficient\nHardware \nLatency \nFeedback\nSubTransformers\n12/3/2019QRXQ_VeQVRU_1895988.VYJ\nÀOe:///UVeUV/KaQUXLZaQJ/DRZQORadV/QRXQ_VeQVRU_1895988.VYJ1/1\nCYea[ed b` CaYVlina Cani fYVm [he NV\\n PYVjec[ IoT\n12/3/2019 QRXQ_gSX_1132940.VYg\nÀOe:///UVeUV/KaQUXLZaQg/DRZQORadV/QRXQ_gSX_1132940.VYg 1/1\nCYea[ed b` MiZha Pe[YiZhche]fYVm [he NV\\n PYVjec[\nGPU\n12/3/2019 Electronic DeYice\nÀle:///Users/hanruiZang/DoZnloads/noun_CPU_2880768.sYg1/1\nCYea[ed b` M\\haTTad KhViY\\S ATfYVT [he NV\\U PYVjec[CPU\nDifferent Hardware\n12/6/2019QRXQ_GaVKbRaUG_50324.VYJ\nÀOH:///UVHUV/KaQUXLZaQJ/DRZQORaGV/QRXQ_GaVKbRaUG_50324.VYJ1/1\nCYea[ed b` Da^id SVbVSe^ZkifYVT [he NV\\U PYVjec[\nTinyML Deeper Wider\n（Weight-Sharing）\nFigure 1: Framework for searching Hardware-Aware\nTransformers. We ﬁrst train a SuperTransformer that\ncontains numerous sub-networks, then conduct an evo-\nlutionary search with hardware latency feedback to ﬁnd\none specialized SubTransformer for each hardware.\nseconds on a Raspberry Pi. Such long latency will\nhurt the user experience on edge devices. Thus we\nneed hardware-efﬁcient Transformers (Figure 1).\nThere are two common pitfalls when evaluat-\ning the efﬁciency of a Transformer. (1) FLOPs\ndoes not reﬂect the measured latency . Although\nFLOPs is used as an metric for efﬁciency in prior\narts (Howard et al., 2017; Wu et al., 2020), it is not\na good latency proxy. As in Figure 2 (Right), mod-\nels with the same FLOPs can result in verydifferent\nmeasured latencies; (2) different hardware prefers\ndifferent Transformer architecture.As in Table 1,\nthe Transformer model optimized on one hardware\nis sub-optimal for another because latency is in-\nﬂuenced by different factors on different hardware\nplatforms. For example, the embedding dimension\nhas signiﬁcant impact on the Raspberry Pi latency\nbut hardly inﬂuences the GPU latency (Figure 2).\nInspired by the success of Neural Architecture\nSearch (NAS) (Bender et al., 2018; Guo et al., 2019;\narXiv:2005.14187v1  [cs.CL]  28 May 2020\nTable 2\nChange Hidden \nDim\nChange Hidden \nDim\nChange Embed \nDim\nChange Embed \nDim\nChange #Layers Change #Layers\n159.17185 52.4 159.17185 52.4 159.17185 52.4\n194.56129 50.9 194.56417 51.4 192.51458 96.2\n226.01857 51.3 229.95649 54.2 225.85731 137\n257.47585 51.7 265.34881 51.5 259.20004 174\n288.93313 50.4 300.74113 51.7 292.54277 216\n320.39041 52.5 336.13345 51.5 325.8855 261\n0\n75\n150\n225\n300\n120 180 240 300 360\nTable 2-1\nHidden Dim \nScaling\nChange Hidden \nDim\nEmbed Dim \nScaling\nChange Embed \nDim\nLayer Number \nScaling\nChange #Layers\n159.17185 1012.378561 159.17185 1012.378561 159.17185 1012.378561\n194.56129 1147.812423 194.56417 1114.87281 192.51458 1321.4497\n226.01857 1281.604218 229.95649 1270.467544 225.85731 1630.328098\n257.47585 1451.090652 265.34881 1403.119045 259.20004 1925.809944\n288.93313 1585.657468 300.74113 1635.061976 292.54277 2266.552657\n320.39041 1730.037206 336.13345 1831.49392 325.8855 2580.540898\n600\n1150\n1700\n2250\n2800\n120 180 240 300 360\nHidden Dim Scaling\n Embedding Dim Scaling\n Layer Number Scaling\nTable 2-2\nChange Hidden \nDim\nChange Hidden \nDim\nChange Embed \nDim\nChange Embed \nDim\nChange #Layers Change #Layers\n159.17185 57.81710545 159.17185 57.81710545 159.17185 57.81710545\n194.56129 73.918281 194.56417 62.03208764 192.51458 100.9847422\n226.01857 80.10061979 229.95649 63.28665018 225.85731 150.8454541\n257.47585 83.99975101 265.34881 69.9605157 259.20004 205.2778184\n288.93313 89.46600159 300.74113 70.42247256 292.54277 268.6143021\n320.39041 96.02826834 336.13345 77.27889021 325.8855 295.1284428\n10\n90\n170\n250\n330\n120 180 240 300 360\nFLOPs (M)\nNvidia GPURaspberry Pi ARM CPU  Intel CPU\nHidden & Embedding \ndim have no impact  \non Nvidia GPU latency\nHidden & Embedding \ndim have large impact \non Raspberry Pi ARM \nCPU latency\nHidden & Embedding \ndim have small impact \non Intel CPU latency\nLatency (ms)\nFLOPs (M) FLOPs (M)\nSimilar FLOPs, \nDiﬀerent Latency\nTable 2-1-1\nHidden Dim \nScaling\nChange Hidden \nDim\nEmbed Dim \nScaling\nChange Embed \nDim\nLayer Number \nScaling\nChange #Layers\n159.17185 1012.378561 159.17185 1012.378561 159.17185 1012.378561\n194.56129 1147.812423 194.56417 1114.87281 192.51458 1321.4497\n226.01857 1281.604218 229.95649 1270.467544 225.85731 1630.328098\n257.47585 1451.090652 265.34881 1403.119045 259.20004 1925.809944\n288.93313 1585.657468 300.74113 1635.061976 292.54277 2266.552657\n320.39041 1730.037206 336.13345 1831.49392 325.8855 2580.540898\nFigure 2: Latency of different Transformer models on different hardware. We ﬁnd (1) FLOPs does not reﬂect\nthe real measured latency; (2) Latency inﬂuencing factors of different hardware are contrasting. Thus we need to\nconsider hardware latency feedback to design specialized models for different hardware.\nMeasured On → GPU ARM CPU\nSpecialized For ↓ BLEU Latency Latency\nHAT (GPU) 28.10 147 ms 6491 ms\nHAT (ARM CPU) 28.15 184 ms 6042 ms\nTable 1: BLEU score and measured inference latency\nof HAT on WMT’14 En-De task. The efﬁcient model\nfor GPU is not efﬁcient for ARM CPU and vice versa.\nPham et al., 2018; Cai et al., 2019a), we propose to\nsearch for Hardware-Aware Transformers (HAT)\nby directly involving the latency feedback into the\ndesign loop. In this way, we do not need FLOPs\nas the latency proxy and can search specialized\nmodels for various hardware.\nWe ﬁrst construct a large search space with arbi-\ntrary encoder-decoder attention and heterogeneous\nTransformer layers. Traditional Transformer has an\ninformation bottleneck between the encoder and de-\ncoder. Arbitrary encoder-decoder attention breaks\nthe bottleneck, allowing all decoder layers to attend\nto multiple and different encoder layers instead of\nonly the last one. Thus low-level information from\nthe encoder can also be used by the decoder. Mo-\ntivated by Figure 2, we introduce heterogeneous\nTransformer layers to allow different layers to have\ndifferent architecture adapting various hardware.\nTo perform a low-cost search in such a large\ndesign space, we ﬁrst train a Transformer super-\nnet – SuperTransformer, which contains many Sub-\nTransformers sharing the weights. We train all\nSubTransformers simultaneously by optimizing the\nuniformly sampled SubTransformers from the Su-\nperTransformer. The performance of a SubTrans-\nformer with inherited weights from the SuperTrans-\nformer can provide a good relative performance\napproximation for different architectures trained\nfrom-scratch. Unlike conventional NAS, we only\nneed to pay the SuperTransformer training cost for\nonce and can evaluate all the models in the design\nspace with it. Finally, we conduct an evolutionary\nsearch to ﬁnd the best SubTransformer under the\nhardware latency constraint. Experiments show\nthat HAT can be naturally incorporated with model\ncompression techniques such as quantization and\nknowledge distillation.\nWe evaluate HAT with WMT’14 En-De,\nWMT’14 En-Fr, WMT’19 En-De, and IWSLT’14\nDe-En tasks on Raspberry Pi ARM CPU, Intel\nXeon CPU, and Nvidia TITAN Xp GPU. Com-\npared with previous work (Vaswani et al., 2017; So\net al., 2019; Gu et al., 2019; Wu et al., 2020), HAT\nachieves up to 3× speedup, 3.7× smaller size over\nTransformer-Big without loss of accuracy. With\n12,041× less search cost, HAT outperforms the\nEvolved Transformer with 2.7× speedup and 3.6×\nsmaller size. It also achieves up to 1.9× speedup\nover Levenshtein and Lite Transformers with no\nBLEU score loss. With 4-bit quantization, HAT\ncan further reach 25× model size reduction.\nHAT has three contributions: (1) Hardware-\nAware and Specialization. To our best knowl-\nedge, we are the ﬁrst to directly involve the hard-\nware feedback in the model design, to reduce NLP\nmodel latency for target hardware, instead of rely-\ning on proxy signals (FLOPs). For different hard-\nware platforms, specialized models for low-latency\ninference are explored. (2) Low-cost Neural Ar-\nchitecture Search with a Large Design Space.\nWe propose arbitrary encoder-decoder attention\nto break the information bottleneck; and heteroge-\nneous layer to let different layers alter its capac-\nity. A weight-shared SuperTransformer is trained\nto search for efﬁcient models at a low cost. (3)\nDesign Insights. Based on the search results, we\nreveal some design insights: Attending to multiple\nencoder layers is beneﬁcial for the decoder; GPU\nprefers shallow and wide models while ARM CPU\nprefers deep and thin ones.\nElastic Layer \nNum in Encoder\nElastic Head Num \n(Self Attention)\nElastic Hidden \nDim in FFN\nEncoder Layer 2\nEncoder Layer m\nElastic Embedding Dim\nElastic Head Num \n(Self Attention)\nElastic Hidden \nDim in FFN\nElastic Embedding Dim\nElastic Head Num \n(En-Decoder Attention)\nDecoder Layer n\nElastic Layer \nNum in Decoder\nArbitrary \nEncoder- \nDecoder \nAttention\nconcat\n❶ Train a SuperTransformer by uniformly sampling SubTransformers with weight sharing\n12/3/2019QRXQ_VeQVRU_1895988.VYJ\nÀOe:///UVeUV/KaQUXLZaQJ/DRZQORadV/QRXQ_VeQVRU_1895988.VYJ1/1\nCYea[ed b` CaYVlina CanifYVm [he NV\\n PYVjec[IoT\n12/3/2019 QRXQ_gSX_1132940.VYg\nÀOe:///UVeUV/KaQUXLZaQg/DRZQORadV/QRXQ_gSX_1132940.VYg 1/1\nCYea[ed b` MiZha Pe[YiZhche]fYVm [he NV\\n PYVjec[\nGPU\n12/3/2019Electronic DeYice\nÀle:///Users/hanruiZang/DoZnloads/noun_CPU_2880768.sYg1/1\nCYea[ed b` M\\haTTad KhViY\\S ATfYVT [he NV\\U PYVjec[CPU\nLatency \nPredictor\n❷ Collect Hardware Latency Datasets\n12/3/2019noun_database_2997685.svg\nﬁle:///Users/hanruiwang/Downloads/noun_database_2997685.svg1/1\nLayer Num\nEmbed Dim\nHeads Num\nLatency\n❸ Train a Latency Predictor        \n    for each Hardware\nEvolutionary Search \nEngine\n12/3/2019QRXQ_LQcUHaVH_2500412.VYJ\nÀOH:///UVHUV/KaQUXLZaQJ/DRZQORadV/QRXQ_LQcUHaVH_2500412.VYJ1/1\nCYea[ed b` ASice DeZigUfYVT [he NV\\U PYVjec[\n❹ Evolutionary Search\nLatencySubTransformer \nArchitecture \nVal Loss\nEncoder Layer 1\nDecoder Layer 1\ne.g.  \n4 heads\ne.g.  \n2 heads\nSubTransformer \nArchitecture \nSuperTransformer\nFigure 3: HAT Overview. A large design space is constructed with Arbitrary Encoder-Decoder Attention and\nHeterogeneous Layers. (1) Train a weight-shared SuperTransformer by iteratively optimizing randomly sampled\nSubTransformers. It can provide a performance proxy for SubTransformers. (2) Collect(SubTransformer architec-\nture, latency) data pairs on the target hardware. (3) Train a latency predictor for each hardware to provide fast and\naccurate latency feedback. (4) Perform an evolutionary search with hardware latency constraint to ﬁnd the model\nwith the lowest validation loss. (5) Finally, the searched model is trainedfrom scratch to get the ﬁnal performance.\nEncoder Layer j\nSelf Attention\nDecoder Layer m\nEncoder Layer i\nKj Vj\nKi Vi\nconcat\nEn-Decoder Attention\nQVK\nFigure 4: Arbitrary Encoder-Decoder Attention. Each\nencoder-decoder attention in one decoder layer can at-\ntend to the outputs from multiple encoder layers, fully\nleveraging the features extracted by the encoder.\n2 Proposed Approaches\nAn overview of the HAT framework is shown\nin Figure 3. We ﬁrstly train a SuperTransformer\nwith a large design space. Then, for a given hard-\nware platform, we collect a dataset of (SubTrans-\nformer architecture, measured latency) pairs for\ndifferent models, and train a latency predictor. Fi-\nnally, we conduct an evolutionary search with a\nlatency constraint to ﬁnd an efﬁcient model special-\nized for the target hardware.\n2.1 Design Space\nWe construct a large design space by breaking two\nconventions in the Transformer design: (1) All\ndecoder layers only attend to the last encoder layer;\n(2) All the layers are identical.\nArbitrary Encoder-Decoder Attention. Differ-\nent encoder layers extract features on different ab-\nstraction levels. Conventionally, all the decoder lay-\ners only attend to the last encoder layer. It forms an\ninformation bottleneck that forces all the decoder\nlayers to learn solely from the high abstraction level\nand ignore the low-level information. To break the\nbottleneck, we propose Arbitrary Encoder-Decoder\nAttention to learn the most suitable connections\nbetween the encoder and the decoder. Each de-\ncoder layer can choose multiple encoder layers to\nattend. The key and value vectors from encoder\nlayers are concatenated in the sentence length di-\nmension (Figure 4) and fed to the encoder-decoder\ncross attention module. The mechanism is efﬁcient\nbecause it introduces no additional parameters. The\nlatency overhead is also negligible. For example,\nwith each decoder layer attending to two encoder\nlayers, the latency of Transformer-Base on Nvidia\nTITAN Xp GPU barely increases by 0.4%. It im-\nproves the model capacity by allowing attention to\ndifferent abstraction levels.\nHeterogeneous Transformer Layers. Previous\nTransformers repeat one architecture for all layers.\nIn HAT, instead, different layers areheterogeneous,\nwith different numbers of heads, hidden dim, and\nembedding dim. In attention layers, different heads\nare used to capture various dependencies. However,\nV oita et al. (2019) shows that many heads are re-\ndundant. We thereby make attention head number\nelastic so that each attention module can decide its\nnecessary number of heads.\nIn the FFN layer, the input features are cast to\na higher dimension (hidden dim), followed by an\nSelf Attention\nFC2\nFC1\nFixed Dimension for Q \nvectors(e.g. 512)Max Embedding \nDim (e.g. 640)\nSampled Embedding \nDim (e.g. 512)\nWeight for \nQ Vector\nFFN\nMax Hidden \nDim (e.g. 1024)\nMax Embedding \nDim (e.g. 640)\nWeight for \nFC1 in FFN\nSampled Hidden \nDim (e.g. 768)\nSampled Embedding \nDim (e.g. 512)\nEn-Decoder Attention\nSampled \nEmbedding Dim\nSampled \nHidden Dim Input\nInput\nOutput Output\nFigure 5: Weight Sharing of the SuperTransformer. All\nSubTransformers share the front portion of word em-\nbeddings, and weights in the fully-connected layers.\nactivation layer. Traditionally, the hidden dim is\nset as 2 × or 4× of the embedding dim, but this\nis sub-optimal since different layers need differ-\nent capacities depending on the feature extraction\ndifﬁculty. We hence make the hidden dim elastic.\nMoreover, we also support elastic embedding\ndim of encoder and decoder, but it is consistent\ninside encoder/decoder. The number of encoder &\ndecoder layers are also elastic to learn the proper\nlevel of feature encoding and decoding. Other de-\nsign choices such as the length of Q, K, Vvectors\nin attention modules can be naturally incorporated\nin our framework, which we leave for future work.\n2.2 SuperTransformer\nIt is critical to have a large design space in or-\nder to ﬁnd high-performance models. However,\ntraining all the models and comparing their BLEU\nscores is infeasible. We thus propose SuperTrans-\nformer, a supernet for performance approxima-\ntion, which can judge the performance of a model\nwithout fully training it. The SuperTransformer is\nthe largest model in the search space with weight\nsharing (Pham et al., 2018; Liu et al., 2019; Cai\net al., 2019a). Every model in the search space\n(a SubTransformer) is a part of the SuperTrans-\nformer. All SubTransformers share the weights of\ntheir common parts. For elastic embedding dim,\nall SubTransformers share the front portion of the\nlongest word embedding and corresponding FC\nlayer weights. As in Figure 5, for elastic FFN\nhidden dim, the front part of the FC weights is\nshared. For elastic head number in attention mod-\nules, the whole Q, K, Vvectors (the lengths are\nﬁxed in our design space) are shared by dividing\ninto head number parts. Elastic layer numbers let\nall SubTransformers share the ﬁrst several layers.\nTable 2\nPredicted Latency\n7.589 7.5296678841114\n5.717 5.73590663075445\n5.0745 5.0150016248226\n6.0985 6.01029899716375\n8.247 8.20982375741005\n4.5205 4.49489665031433\n4.9555 4.95552715659142\n4.7605 4.75900384783745\n8.1035 8.02451953291895\n7.376 7.35119462013245\n5.422 5.4501354396343\n8.25 8.19430738687515\n4.0485 3.96039354801178\n4.249 4.23552632331848\n7.6545 7.64323496818545\n7.033 6.88298216462135\n7.4085 7.3863371014595\n7.2775 7.2759775519371\n6.8365 6.8069668710232\n6.8535 6.7940945625305\n4.998 4.96798822283745\n6.1145 6.0429987311363\n4.871 4.85369718074799\n5.311 5.2973562180996\n9.0085 8.97694331407545\n5.6455 5.616979598999\n3.934 3.83237943053246\n5.2995 5.28114092350005\n7.809 7.78207111358645\n8.855 8.77593064308165\n4.313 4.35372364521027\n4.992 5.03139266371725\n7.861 7.81471514701845\n7.9925 7.95118278265\n4.0585 3.96693044900894\n7.111 7.04923966526985\n8.862 8.83656191825865\n6.8315 6.69998919963835\n3.672 3.61612984538079\n5.834 5.7122194468975\n6.23 6.20329195261\n8.5145 8.50716817379\n5.135 5.0798606276512\n9.0265 9.00762835144995\n7.357 7.3238939344883\n4.2805 4.34337705373764\n9.0005 8.9323092997074\n6.415 6.4160676896572\n4.99 4.92809936404228\n6.481 6.4832152426243\n8.8635 8.83288142085075\n7.383 7.4009601175785\n5.413 5.3161016702652\n3.3655 3.47699934244156\n3.802 3.83249413967133\n4.954 4.86786392331124\n4.948 4.89481997489929\n6.7125 6.70555835962295\n3.3545 3.32608613371849\n7.0475 6.96746802330015\n7.64 7.45707643032075\n4.714 4.68118336796761\n8.921 8.85272547602655\n5.372 5.32582753896715\n5.9175 5.90872702002525\n5.974 5.9056848883629\n7.019 6.9966391324997\n9.2805 9.23188439011575\n7.7095 7.6677560508251\n8.337 8.4311276972294\n7.545 7.51033142209055\n6.1775 6.135611563921\n5.9145 5.87931123375895\n6.8605 6.81591963768005\n7.2735 7.27520194649695\n6.078 6.04990461468695\n9.141 9.1181918680668\n4.8365 4.85468298196793\n7.536 7.49554389715195\n8.2435 8.2348293662071\n4.29 4.3167936205864\n7.828 7.7659921348095\n3.5145 3.53200995922089\n5.908 5.8308235704899\n4.513 4.53006964921952\n4.59 4.56262037158013\n3.399 3.38866749405861\n8.619 8.5397402346134\n6.8685 6.85147523880005\n5.097 5.0277101993561\n4.2755 4.16309571266175\n3.468 3.54644849896431\n7.282 7.33746790885925\n7.767 7.7358447909355\n4.9835 4.90737867355347\n6.9875 6.95428070425985\n4.9315 4.95041823387146\n7.584 7.51947075128555\n8.7255 8.68516021966935\n4.2205 4.19166240096092\n4.983 4.95283806324005\n7.6015 7.6359705030918\n7.376 7.3840798139572\n5.068 5.00582936406135\n8.392 8.42607721686365\n6.8665 6.87219136953355\n8.2345 8.23955875635145\n5.0125 4.99876606464386\n3.843 3.77417260408402\n6.566 6.52556720376015\n4.253 4.24432995915413\n7.2855 7.25049513578415\n4.891 4.88125985860825\n7.2715 7.2124939262867\n5.415 5.3595046401024\n4.934 4.92306473851204\n3.5075 3.5542708337307\n9.1575 9.05027770996095\n8.8535 8.82399493455885\n5.073 5.0438419878483\n5.7265 5.7288476228714\n2\n4\n6\n8\n10\n2 4 6 8 10\nReal Latency on  \nRaspberry Pi ARM CPU (s)\nPredicted Latency (s)\nPredicted latency is very \nclose to real latency.\ny = x\nPredicted latency  \n= f (SubTransformer Architecture)\nFigure 6: The latency predictor is very accurate, with\nan average prediction error (RMSE) of 0.1s.\nIn the SuperTransformer training, all possible\nSubTransformers are uniformly sampled, and the\ncorresponding weights are updated. In practice, the\nSuperTransformer only needs to be trained for the\nsame steps as a baseline Transformer model, which\nis fast and low-cost. After training, we can get\nthe performance proxy of sampled models in the\ndesign space by evaluating the corresponding Sub-\nTransformers on the validation set without training.\n2.3 Evolutionary Search for SubTransformer\nGiven a latency requirement, we perform an evo-\nlutionary search to ﬁnd a satisfactory SubTrans-\nformer. There are two ways to evaluate the hard-\nware latency of a SubTransformer: (1) Online mea-\nsurement in which we measure the models during\nthe search process. (2) Ofﬂine, where we train a la-\ntency predictor to provide the latency. We apply the\nofﬂine method here because it is fast and accurate.\nFor the online method, a single sampled SubTrans-\nformer requires hundreds of inferences to get an\naccurate latency, which lasts for minutes and slows\ndown the searching. For the ofﬂine method, we\nencode the architecture of a SubTransformer into\na feature vector, and predict its latency instantly\nwith a multi-layer perceptron (MLP). Trained with\nthousands of real latency data points, the predic-\ntor yields high accuracy (Figure 6). Note that the\npredicted latency is only used in the search pro-\ncess, and we report real measured latency in the\nexperiment section. Compared with deducing a\nclosed-form latency model for each hardware, the\nlatency predictor method is more general and faster.\nWe use an evolutionary algorithm to conduct the\nsearch process. As in Figure 3, the search engine\nqueries the latency predictor for SubTransformer\nlatency, and validates the loss on the validation\nset. The engine only adds SubTransformers with\nlatency smaller than the hardware constraint to the\npopulation. We then train the searched modelsfrom\nscratch to obtain the ﬁnal performance.\n3 Experiments\n3.1 Datasets\nWe conduct experiments on four machine trans-\nlation tasks: WMT’14 En-De, WMT’14 En-Fr,\nWMT’19 En-De, and IWSLT’14 De-En, consisting\nof 4.5M, 36.3M, 43.0M, and 160K pairs of train-\ning sentences, respectively. For WMT’14 En-De,\nwe apply 32K source-target BPE vocabulary, train\non WMT’16, validate on newstest2013 and test on\nnewstest2014, replicating Wu et al. (2019b); For\nWMT’14 En-Fr, we use 40K source-target BPE vo-\ncabulary, validate on newstest2012&2013, and test\non newstest2014, replicating Gehring et al. (2017).\nWMT’19 En-De adopts 49.6K source-target BPE\nvocabulary, validates on newstest2017, and tests\non newstest2018, the same as Junczys-Dowmunt\n(2019). We use 10K joint BPE vocabulary in lower\ncase for IWSLT’14 De-En (Grave et al., 2017).\n3.2 Experiment Setups\nBaselines. Our baseline models are Trans-\nformer (Vaswani et al., 2017), Levenshtein Trans-\nformer (Gu et al., 2019), both with the Ott et al.\n(2019) implementation, Evolved Transformer (So\net al., 2019) and Lite Transformer (Wu et al., 2020).\nEvaluation Metrics. For evaluation, we use\nbeam four and length penalty 0.6 for WMT, and\nbeam ﬁve for IWSLT (Vaswani et al., 2017). All\nBLEUs are calculated with case-sensitive tokeniza-\ntion1, but we also apply the compound splitting\nBLEU2 for WMT, the same as Vaswani et al.\n(2017). We test the model with the lowest valida-\ntion set loss for WMT and the last ten checkpoints\naveraged for IWSLT.\nWe test the latency of the models by measur-\ning translation from a source sentence to a target\nsentence with the same length. The length is the\naverage output length on the test set – 30 for WMT\nand 23 for IWSLT. For each model, we measure\nthe latency for 300 times, remove the fastest and\nslowest 10% and then take the average of the rest\n80%. We conduct experiments on three represen-\ntative hardware platforms: Raspberry Pi-4 with an\nARM Cortex-A72 CPU, Intel Xeon E5-2640 CPU,\nand Nvidia TITAN Xp GPU.\n1https://github.com/moses-smt/mosesdecoder\n2https://github.com/tensorﬂow/tensor2tensor\n3.3 Implementation Details\nSuperTransformer Setups. The SuperTrans-\nformer for WMT has the following design space:\n[512, 640] for embedding dim, [1024, 2048, 3072]\nfor hidden dim, [4, 8] for the head number in all\nattention modules, [1, 2, 3, 4, 5, 6] for decoder\nlayer number. Due to decoder auto-regression, en-\ncoder only accounts for less than 5% of the mea-\nsured latency; thereby, we set the encoder layer\nnumber ﬁxed as 6. For arbitrary encoder-decoder\nattention, each decoder can choose to attend to the\nlast one, two, or three encoder layers. The Super-\nTransformer design space for IWSLT is the same as\nWMT except for [2048, 1024, 512] for hidden dim\nand [4, 2] for head number. We set the Q, K, V\nvector dim ﬁxed as 512. The design space contains\naround 1015 possible SubTransformers and covers\na wide range of model size and latency (largest =\n6×smallest). We train the SuperTransformers of\nWMT for 40K steps and 50K steps for IWSLT.\nHardware-Aware Evolutionary Search Setups.\nThe input of the latency predictor is a feature vec-\ntor of SubTransformer architecture with ten ele-\nments: layer number, embed dim, average hidden\ndim, average self-attention heads, of both encoder\nand decoder; plus average encoder-decoder atten-\ntion heads, and the average number of encoder\nlayers each decoder layer attends. A dataset of\n2000 (SubTransformer architecture, measured la-\ntency) samples for each hardware is collected, and\nsplit into train:valid:test=8:1:1. We normalize the\nfeatures and latency, and train a three-layer MLP\nwith 400 hidden dim and ReLU activation. We\nchoose three-layer because it is more accurate than\nthe one-layer model, and over three layers do not\nimprove accuracy anymore. With the predictor, we\nconduct an evolutionary search for 30 iterations in\nthe SuperTransformer, with population 125, par-\nents population 25, mutation population 50 with\n0.3 probability and crossover population 50.\nTraining Settings. Our training settings are in\nline with Wu et al. (2019b) and Wu et al. (2020).\nFor WMT, we train for 40K steps with Adam\noptimizer and a cosine learning rate (LR) sched-\nuler (Kingma and Ba, 2015; Loshchilov and Hut-\nter, 2017), where the LR is linearly warmed up\nfrom 10−7 to 10−3, and then cosine annealed. For\nIWSLT, we train for 50K steps with inverse square\nroot LR scheduler. The baseline Transformers are\ntrained with the same settings as the searched Sub-\nTransformers for fair comparisons.\nWMT14 En-De latency on Intel CPU\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n125.6744315 24.37 318.8635727 24.58 137.8659119 25.83 25.63 1 0 1 0\n181.4779053 25.85 415.5805737 27.63 204.1926424 27.62 27.13 2 1 1 0\n267.8353677 26.54 630.4540406 28.4 278.7041912 27.9 27.28 3 1 2 0\n303.4228822 27.04 340.1943684 28.1 27.49 4 2 2 0\n357.4422042 27.53 369.6464062 28.2 27.59 5 2 3 0\n415.5805737 27.63 2.03523774811584 450.9188682 28.53 27.93 6 3 3 0\n21 9 12 0\nWMT14 En-De latency on Arm CPU-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n3.320969731 24.37 3.119880478 24.58 3.510976136 25.82 25.60 1 1 0 0\n4.10328116 25.85 7.347423464 27.63 3.99612087 26.91 26.64 2 2 0 0\n4.906528513 26.54 20.54689761 28.4 4.507411629 27.62 27.13 2 1 1 0\n5.675740421 27.04 5.006854216 27.83 27.23 3 2 1 0\n6.494072417 27.53 6.0 28.15 27.61 4 3 1 0\n7.347423464 27.63 2.96925593028383 6.919880971 28.44 27.81 5 2 3 0\n17 11 6 0\nWMT14 En-De latency on titanxp-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n56.4535487 24.37 233.4403444 24.58 57.11806165 25.83 25.63 1 0 1 0\n93.52474536 25.85 245.1151872 27.63 91.17974952 27.62 27.13 2 1 1 0\n131.0418878 26.54 254.7298993 28.4 126.0203572 27.9 27.28 3 1 2 0\n170.571674 27.04 1.47E+02 28.1 27.49 3 1 2 0\n210.3917578 27.53 208.1187446 28.5 27.84 5 2 3 0\n245.1151872 27.63 2.68826344106412\n135.37 14 5 9 0\n24\n25\n26\n27\n28\n29\n100 234 368 501 635\nBLEU Score\n24\n25\n26\n27\n28\n29\n3 7 12 16 21\n     \n                                                                          \n                                                    \n24\n25\n26\n27\n28\n29\n50 101 153 204 255\nWMT14 En-Fr latency on intel CPU-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n144.0132509 37.23 326.9822071 37.13 154.6823412 39.09 36.31 1 1 0 0\n201.8358638 38.89 431.1191042 40.6 208.7727835 40 37.15 2 1 0 1\n259.3422612 39.94 737.3172412 41.2 329.3679456 41.1 38.24 4 2 1 1\n329.1240702 40.21 394.5148965 41.41 38.53 5 2 1 2\n374.3495574 40.46 441.962711 41.66 38.81 6 3 2 1\n431.1191042 40.6\n2.23858226354374 18 9 4 5\n37\n38\n39\n40\n41\n42\n120 275 430 585 740\nWMT14 En-Fr latency on Arm CPU\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n4.079504708 37.23 3.829777499 37.13 4.269129376 38.75 35.95 1 1 0 0\n4.854586174 38.89 8.044359436 40.6 5.33629785 40.14 37.29 2 1 0 1\n5.650409361 39.94 23.15697475 41.2 5.818866263 40.64 37.76 3 2 1 0\n6.465141773 40.21 6.859018068 41.11 38.26 4 3 1 0\n7.269941688 40.46 7.827191518 41.4 38.54 5 3 2 0\n8.044359436 40.6 2.95852921149897 9.065828112 41.8 38.86 6 3 3 0\n21 13 7 1\nBLEU Score\n37\n38\n39\n40\n41\n42\n3 8 13 18 23\nIntel CPU latency (ms)\nHAT (Ours)\n3.0× Faster \n3.7× Smaller\nTransformer-Base\nRaspberry Pi ARM CPU latency (s) Nvidia GPU latency (ms)\n1.6× Faster\nLayer Number Scaling of Transformer (Vaswani et al.) Dimension Scaling of Transformer (Vaswani et al.)\nTransformer-Big\n1.5\n1.5\n1.5 Dimension scaling \ncan hardly reduce latency \non Nvidia GPU\nTransformer-Base\n2.7× Faster\nWMT ’14 En-Fr\n3.0× Faster \n3.6× Smaller\nTransformer-Big\nRaspberry Pi ARM CPU latency (s) Intel CPU latency (ms)\nWMT ’14 En-Fr\n1.6\n2.2× Faster\n1.9\nWMT ’14 En-De WMT ’14 En-De WMT ’14 En-De\n2.0× Faster\nTransformer-Base\nTransformer-Big Transformer-Big\nTransformer-Base\nTransformer-Big\nTransformer-Base\nWMT14 En-De latency on titanxp-1-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n57.03669935 37.23 233.1844045 37.13 69.31571099 39.1 36.31 1 1 0 0\n90.18124064 38.89 234.7934725 40.6 94.92767955 40 37.15 2 1 0 1\n128.0396621 39.94 238.828734 41.2 132.8543258 40.7 37.81 3 2 1 0\n166.6058949 40.21 168.3487089 41.1 38.25 4 2 1 1\n202.3486488 40.46 208.3378961 41.7 38.78 5 2 2 1\n234.7934725 40.6 1.76730016946125\n188.3 15 8 4 3\n37\n38\n39\n40\n41\n42\n50 98 145 193 240\nNvidia GPU latency (ms)\nWMT ’14 En-Fr\n1.8× Faster\n1.9\nTransformer-Big\nTransformer-Base\nDimension scaling \ncan hardly reduce latency \non Nvidia GPU\nFigure 7: Inference latency and BLEU trade-offs of WMT’14 En-De and En-Fr on three hardware platforms. HAT\nconsistently outperforms the baseline Transformers and achieves up to 3× faster inference speed and 3.7× smaller\nsize over Transformer-Big. Speciﬁc latency, BLEU and SacreBLEU (Post, 2018) are in Appendix Table 8.\nIwslt 14 de-en\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n4.32E+01 33.2 45.60 33.44 1 1 0 0\n7.60E+01 34.01 1.96E+02 33.48 74.53 34.2 33.25 2 2 0 0\n1.03E+02 34.11 1.97E+02 34.5 109.04 34.54 33.59 3 2 0 1\n1.33E+02 34.22 2.03E+02 34.7 137.78 34.7 33.75 4 3 0 1\n1.66E+02 34.4 168.77 34.8 33.89 5 4 0 1\n1.97E+02 34.5 1.80667644900954 201.66\n15 12 0 3\n40 81 122 162 203\nIWSLT ’14 De-En\n1.8× Faster\nNvidia GPU latency (ms)\nWMT19 En-de\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n51.94430365 40.37 55.68625701 42.39 41.85 1 0 1 0\n87.99107674 42.64 237.9384397 40.89 93.20854973 44.42 43.9 2 1 0 1\n125.6872312 43.82 245.3763484 45.2 134.5091038 45.38 44.66 3 1 2 0\n245.3763484 45.2 176.1489895 46.23 45.56 4 2 2 0\n204.467315 46.49 45.72 5 3 2 0\n1.82423599197306 237.7964813 46.74 46.04 6 3 3 0\n267.73 21 10 10 1\n40\n41\n42\n43\n44\n45\n46\n47\n40 93 145 198 250\n                                        \n                                                                          \n                                                     \nWMT ’19 En-De\n1.8× Faster\n2.0\nHAT (Ours)\nLayer Number Scaling of Transformer (Vaswani et al.)\nDimension Scaling of Transformer (Vaswani et al.)\nTransformer-Base\nBLEU Score\n33\n34\n35\nTransformer-Base\nFigure 8: Inference latency and BLEU trade-offs of\nWMT’19 and IWSLT’14 tasks on Nvidia GPU.\n4 Results\n4.1 HAT Performance Comparisons\nIn Figure 7, 8 and Appendix Table 8, we com-\npare HAT with Transformer baselines on four tasks.\nThe embedding dims are 512 and 1024 for the\nTransformer-Base and Big, respectively. The hid-\nden dims are 4× and 2× of the embedding dim for\nWMT and IWSLT. The IWSLT models are smaller\nto prevent overﬁtting (Wu et al., 2019b). We ob-\ntain a series of baseline models with layer number\nscaling (yellow) and dimension scaling (blue). We\nset different latency constraints on three hardware\nto get a series of HAT models. HAT consistently\noutperforms baselines with a large gap under dif-\nferent latency constraints. On ARM CPU, HAT is\n3× faster and 3.7× smaller than Transformer-Big\nwith the same BLEU. On Intel CPU, HAT achieves\nover 2× speedup. On Nvidia GPU, the blue dash\nline is nearly vertical, indicating that dimension\nscaling can hardly reduce the latency. In this case,\nHAT can still ﬁnd models with low latency and\nhigh performance.\nWe further compare various aspects of HAT with\nTransformer (Vaswani et al., 2017) and Evolved\nTransformer (So et al., 2019) in Table 2. HAT\nachieves up to 1.6×, 3×, and 3.4× speedup with\nup to 1.4×, 3.7×, and 4× smaller size than base-\nlines. We report FLOPs for translating a 23-token\nsentence for IWSLT and 30 for WMT. We show\nthe overall GPU hours for training the SuperTrans-\nformer and the searched SubTransformer. We also\ncalculate the cloud computing costs with differ-\nent modes: “preemptable” is cheaper ($0.74/h)\nthan “on-demand” ($2.48/h) (Strubell et al., 2019).\nHAT is highly affordable since the total GPU-hour\nis over 12000 × smaller than the Evolved Trans-\nformer, and is even smaller than Transformer-Big\nby virtue of the compact model size.\nIn Table 3, we compare HAT with other latest\nmodels. We scale down all models to have similar\nBLEU scores with Levenshtein for fair compar-\nisons. We adopt the average iteration time of 2.88\nfor decoding (Gu et al., 2019), without limiting\nthe length of the output sentence (12 tokens after\nHardware-\nAware\nHetero.\nLayers Latency #Params FLOPs\n(G) BLEU GPU\nHours\nCO2e\n(lbs)\nCloud\nComp. Cost\nIWSLT’14\nDe-En\nTransformer \u0017 \u0017 3.3s 32M 1.5 34.5 2 5 $12 - $40\nHAT (Ours) \u0013 \u0013 2.1s 23M 1.1 34.5 4 9 $24 - $80\nWMT’14\nEn-Fr\nTransformer \u0017 \u0017 23.2s 176M 10.6 41.2 240 68 $178 - $595\nEvolved Trans. \u0017 \u0017 20.9s 175M 10.8 41.3 2,192,000 626,000 $1.6M - $5.5M\nHAT (Ours) \u0013 \u0013 7.8s 48M 3.4 41.4 216 61 $159 - $534\nHAT (Ours) \u0013 \u0013 9.1s 57M 3.9 41.8 224 64 $166 - $555\nWMT’14\nEn-De\nTransformer \u0017 \u0017 20.5s 176M 10.6 28.4 184 52 $136 - $456\nEvolved Trans. \u0017 \u0017 7.6s 47M 2.9 28.2 2,192,000 626,000 $1.6M - $5.5M\nHAT (Ours) \u0013 \u0013 6.0s 44M 2.7 28.2 184 52 $136 - $456\nHAT (Ours) \u0013 \u0013 6.9s 48M 3.0 28.4 200 57 $147 - $495\nTable 2: Comparisons of latency, model size, FLOPs, BLEU and training cost in terms of CO2 emissions (lbs) and\ncloud computing cost (USD) for Transformer, the Evolved Transformer and HAT. The training cost estimation is\nadapted from Strubell et al. (2019). The training time is for one Nvidia V100 GPU, and the latency is measured on\nthe Raspberry Pi ARM CPU. The cloud computing cost is based on AWS.\nLatency BLEU\nTransformer (Vaswani et al., 2017) 4.3s 25.85\nLevenshtein (Gu et al., 2019) 6.5s 25.20\nEvolved Transformer (So et al., 2019) 3.7s 25.40\nLite Transformer (Wu et al., 2020) 3.4s 25.79\nHAT (Ours) 3.4s 25.92\nTable 3: Raspberry Pi ARM CPU latency and BLEU\ncomparisons with different models on WMT’14 En-De.\nHAT has the lowest latency with the highest BLEU.\nTable 2\nEvolutionary \nSearch\nChange #Layers Random Search Change Hidden \nDim\n1 4.2086252774046\n1 4.20995895779891 1 4.20501325571882\n1 4.21033455385678 1 4.2127913454601\n1 4.2109058685283 1 4.21663792894497\n1 4.21510322436252 1 4.21977265879843\n1 4.21559179893555 1 4.22036301309488\n1 4.21616453234455 1 4.22087132406925\n1 4.21770790046005 1 4.22091740365535\n1 4.21888013860253 1 4.22111343288016\n1 4.21961313897787 1 4.2235132651117\n1 4.22160126030742 1 4.22563879410481\n1 4.22189414145149 1 4.22688939860478\n1 4.22452424569008 1 4.22736345000711\n1 4.22567540424782 1 4.22822945072542\n1 4.22789824687948 1 4.23115445927811\n1 4.22801193896083 1 4.23173960873882\n1 4.22821085602999 1 4.23266554062221\n1 4.22824959008178 1 4.23346188049056\n1 4.2286416569263 1 4.23401140201114\n1 4.22883210354503 1 4.23472173616631\n1 4.22974229499776 1 4.23663678831797\n1 4.23000086620043 1 4.23689933030662\n1 4.23059080075207 1 4.23707836826121\n1 4.23172883808679 1 4.23980319211538\n1 4.23221376927481 1 4.24037225695468\n2 4.20081189551904 2 4.19909210865662\n2 4.20122212891928 2 4.20324496314526\n2 4.20336807430032 2 4.20341804911833\n2 4.20400721973439 2 4.20501325571882\n2 4.20518054081851 2 4.20700501203849\n2 4.20622713254723 2 4.20867958398913\n2 4.20797865211787 2 4.2127913454601\n2 4.20800219980215 2 4.21303321921392\n2 4.20853104469301 2 4.21360629681367\n2 4.2086252774046 2 4.21663792894497\n2 4.20886892248155 2 4.21977265879843\n2 4.20923273210494 2 4.22016856211059\n2 4.20995895779891 2 4.22036301309488\n2 4.2100824803039 2 4.22087132406925\n2 4.21033455385678 2 4.22091740365535\n2 4.21041252565414 2 4.22111343288016\n2 4.2109058685283 2 4.22232740205243\n2 4.21097847598689 2 4.2235132651117\n2 4.21342741836211 2 4.22395409790916\n2 4.21376823436438 2 4.22563879410481\n2 4.2139987917979 2 4.22688939860478\n2 4.21482347283635 2 4.22736345000711\n2 4.21498469682087 2 4.22781052021256\n2 4.21505000911452 2 4.22788982679843\n2 4.21510322436252 2 4.22813651082786\n3 4.19638274820153 3 4.19909210865662\n3 4.19711136644097 3 4.20324496314526\n3 4.19973038102148 3 4.20341804911833\n3 4.19978557746502 3 4.20501325571882\n3 4.20055802545833 3 4.20700501203849\n3 4.20076856945895 3 4.20867958398913\n3 4.20081189551904 3 4.20893087681666\n3 4.20122212891928 3 4.20949034628942\n3 4.20147679649514 3 4.21254630683036\n3 4.20182421088594 3 4.21261505263662\n3 4.20199752352121 3 4.2127913454601\n3 4.20233973307628 3 4.21303321921392\n3 4.20307332948925 3 4.21360629681367\n3 4.20324958873315 3 4.21424634889655\n3 4.20336807430032 3 4.21663792894497\n3 4.20400721973439 3 4.218409990827\n3 4.20448868383021 3 4.2196199892133\n3 4.20514495485286 3 4.21977265879843\n3 4.20518054081851 3 4.22008596472534\n3 4.20525582624903 3 4.22016856211059\n3 4.20589653313383 3 4.2202687468037\n3 4.20622713254723 3 4.22036301309488\n3 4.20664772524957 3 4.22065513030339\n3 4.20751720984987 3 4.22087132406925\n3 4.20768326089979 3 4.22091740365535\n4 4.19373331051663 4 4.19909210865662\n4 4.19595301345706 4 4.20127024846519\n4 4.19605908297252 4 4.20211272668391\n4 4.19630407962778 4 4.20324496314526\n4 4.19638274820153 4 4.20341804911833\n4 4.19639283047206 4 4.20501325571882\n4 4.19658526668123 4 4.20700501203849\n4 4.19661929119616 4 4.20776120751246\n4 4.19676669718128 4 4.20867958398913\n4 4.19711136644097 4 4.20893087681666\n4 4.19722316967064 4 4.20949034628942\n4 4.19731487551844 4 4.21115258613732\n4 4.19753714718926 4 4.21254630683036\n4 4.19772928118216 4 4.21261505263662\n4 4.19798189200842 4 4.2127913454601\n4 4.19833428457276 4 4.21303321921392\n4 4.19840061264887 4 4.21360629681367\n4 4.19840641352225 4 4.21424634889655\n4 4.19905933498125 4 4.21507272570406\n4 4.19909195754849 4 4.21663792894497\n4 4.1991027449903 4 4.21730884066686\n4 4.19916824197168 4 4.218409990827\n4 4.19973038102148 4 4.2196199892133\n4 4.19978557746502 4 4.21977265879843\n4 4.19983073361252 4 4.22008596472534\n5 4.191208830864 5 4.19909210865662\n5 4.19154945378365 5 4.20127024846519\n5 4.19224428256525 5 4.20211272668391\n5 4.19238242058479 5 4.20324496314526\n5 4.19262934806621 5 4.20341804911833\n5 4.19304217548942 5 4.20437635172207\n5 4.19373331051663 5 4.20501325571882\n5 4.19387990219681 5 4.20509241959146\n5 4.19409850529792 5 4.20664634009167\n5 4.19422860940165 5 4.20700501203849\n5 4.19482176759348 5 4.20776120751246\n5 4.19487221252569 5 4.20867958398913\n5 4.19501304530696 5 4.20893087681666\n5 4.19509469406892 5 4.20949034628942\n5 4.19521905606354 5 4.21115258613732\n5 4.19530276997 5 4.21118284973868\n5 4.19533376392735 5 4.21254630683036\n5 4.19535688347191 5 4.21261505263662\n5 4.19543116151487 5 4.2127913454601\n5 4.19559680960977 5 4.21303321921392\n5 4.1957828992773 5 4.21360629681367\n5 4.19595301345706 5 4.21424634889655\n5 4.19605908297252 5 4.21429509805969\n5 4.19630407962778 5 4.21507272570406\n5 4.19630539762651 5 4.21663792894497\n6 4.18920328172961 6 4.19909210865662\n6 4.18929094123736 6 4.20127024846519\n6 4.18934842109272 6 4.20211272668391\n6 4.18941045098189 6 4.20324496314526\n6 4.18962186805172 6 4.20341804911833\n6 4.18963283178637 6 4.20437635172207\n6 4.18995651380517 6 4.20501325571882\n6 4.19013896848257 6 4.20509241959146\n6 4.19110962837375 6 4.20664634009167\n6 4.191208830864 6 4.20680597744078\n6 4.19129141985435 6 4.20700501203849\n6 4.19154945378365 6 4.20776120751246\n6 4.19157852530973 6 4.20867958398913\n6 4.19177299308382 6 4.20893087681666\n6 4.19224428256525 6 4.20949034628942\n6 4.19238242058479 6 4.21115258613732\n6 4.19238242058479 6 4.21118284973868\n6 4.19242717377727 6 4.21254630683036\n6 4.19249503811941 6 4.21261505263662\n6 4.19262934806621 6 4.2127913454601\n6 4.19304217548942 6 4.21303321921392\n6 4.19308106904424 6 4.21360629681367\n6 4.19323409960992 6 4.21424634889655\n6 4.19332200256966 6 4.21429509805969\n6 4.19336021613788 6 4.21473366423513\n7 4.18507683758492 7 4.19909210865662\n7 4.187136760463 7 4.20127024846519\n7 4.18721828330151 7 4.20211272668391\n7 4.18724792568055 7 4.20324496314526\n7 4.18728339411765 7 4.20341804911833\n7 4.18728538370809 7 4.20437635172207\n7 4.18797630046799 7 4.20501325571882\n7 4.18845471721643 7 4.20509241959146\n7 4.18869603690709 7 4.20632378298898\n7 4.18881599997602 7 4.20664634009167\n7 4.18883195867399 7 4.20680597744078\n7 4.18911755304802 7 4.20700501203849\n7 4.18920328172961 7 4.20776120751246\n7 4.18920328172961 7 4.20867958398913\n7 4.18929094123736 7 4.20873130494558\n7 4.18932709805597 7 4.20893087681666\n7 4.18934842109272 7 4.2092275104794\n7 4.18941045098189 7 4.20949034628942\n7 4.18962186805172 7 4.2103080175894\n7 4.18963283178637 7 4.21115258613732\n7 4.18964367799246 7 4.21118284973868\n7 4.18989509674343 7 4.21254630683036\n7 4.18995651380517 7 4.21261505263662\n7 4.19013896848257 7 4.2127913454601\n7 4.19021712496765 7 4.21303321921392\n8 4.18507683758492 8 4.19909210865662\n8 4.1857362399039 8 4.20127024846519\n8 4.18624753106647 8 4.20211272668391\n8 4.18665956936942 8 4.20324496314526\n8 4.1868116848914 8 4.20341804911833\n8 4.18711815737267 8 4.20437635172207\n8 4.187136760463 8 4.20501325571882\n8 4.187136760463 8 4.20509241959146\n8 4.18721519397965 8 4.20528939743956\n8 4.18721828330151 8 4.20632378298898\n8 4.18724004287287 8 4.20664634009167\n8 4.18724792568055 8 4.20680597744078\n8 4.18724792568055 8 4.20700501203849\n8 4.18728339411765 8 4.20776120751246\n8 4.18728339411765 8 4.20822943285672\n8 4.18728538370809 8 4.20867958398913\n8 4.18754048781856 8 4.20873130494558\n8 4.18757433604066 8 4.20893087681666\n8 4.187743350489 8 4.2090708533185\n8 4.187743350489 8 4.2092275104794\n8 4.18775180414964 8 4.20949034628942\n8 4.18779939481708 8 4.2103080175894\n8 4.18784515539712 8 4.2103911942225\n8 4.18787107883706 8 4.21044728052505\n8 4.18797630046799 8 4.21078104481438\n9 4.18451809007324 9 4.19909210865662\n9 4.18507683758492 9 4.20127024846519\n9 4.18529883222416 9 4.20211272668391\n9 4.18556537857836 9 4.20324496314526\n9 4.1857362399039 9 4.20341804911833\n9 4.18582189303142 9 4.20437635172207\n9 4.18591544575641 9 4.20457147429808\n9 4.18611364925938 9 4.20501325571882\n9 4.18618001930997 9 4.20509241959146\n9 4.18623564389323 9 4.20528939743956\n9 4.18624753106647 9 4.20632378298898\n9 4.18624753106647 9 4.20664634009167\n9 4.18624753106647 9 4.20680597744078\n9 4.18624753106647 9 4.20700501203849\n9 4.18624753106647 9 4.20718690425784\n9 4.18633596290469 9 4.20776120751246\n9 4.18656962644986 9 4.20822943285672\n9 4.18660234136096 9 4.20867958398913\n9 4.1866213809859 9 4.20873130494558\n9 4.1866213809859 9 4.20893087681666\n9 4.18663925372024 9 4.2090708533185\n9 4.18665956936942 9 4.2092275104794\n9 4.1868116848914 9 4.20949034628942\n9 4.18689998241127 9 4.2103080175894\n9 4.18707583870014 9 4.2103911942225\n10 4.18451809007324 10 4.19909210865662\n10 4.18451809007324 10 4.20015496129958\n10 4.18451809007324 10 4.20127024846519\n10 4.18460402862724 10 4.20211272668391\n10 4.18507644302479 10 4.20324496314526\n10 4.18507683758492 10 4.20341804911833\n10 4.18508407398558 10 4.20437635172207\n10 4.18529883222416 10 4.2045218856453\n10 4.18529883222416 10 4.20457147429808\n10 4.18533127849858 10 4.20501325571882\n10 4.18537285002533 10 4.20509241959146\n10 4.18541106359354 10 4.20528939743956\n10 4.18543161429982 10 4.20632378298898\n10 4.18552375668223 10 4.20664634009167\n10 4.18556537857836 10 4.20680597744078\n10 4.18556537857836 10 4.20700501203849\n10 4.18556537857836 10 4.20718690425784\n10 4.18556537857836 10 4.20776120751246\n10 4.18556537857836 10 4.20822943285672\n10 4.18565229094034 10 4.20867958398913\n10 4.1857362399039 10 4.20873130494558\n10 4.1857362399039 10 4.20893087681666\n10 4.18582189303142 10 4.2090708533185\n10 4.18591544575641 10 4.2092275104794\n10 4.18611364925938 10 4.20949034628942\n11 4.18280808326508 11 4.19909210865662\n11 4.18280808326508 11 4.20015496129958\n11 4.18312690463889 11 4.20127024846519\n11 4.18365084691027 11 4.20211272668391\n11 4.18399328312754 11 4.20324496314526\n11 4.18415637917394 11 4.20341804911833\n11 4.18421422000984 11 4.20437635172207\n11 4.18435448193816 11 4.2045218856453\n11 4.18451809007324 11 4.20457147429808\n11 4.18451809007324 11 4.20501325571882\n11 4.18451809007324 11 4.20509241959146\n11 4.18451809007324 11 4.20528939743956\n11 4.18451809007324 11 4.20632378298898\n11 4.18451809007324 11 4.20664634009167\n11 4.18451809007324 11 4.20680597744078\n11 4.18451809007324 11 4.20700501203849\n11 4.18451809007324 11 4.20718690425784\n11 4.18451809007324 11 4.20776120751246\n11 4.18457845777294 11 4.20822943285672\n11 4.18460402862724 11 4.20867958398913\n11 4.18460402862724 11 4.20873130494558\n11 4.18471490841831 11 4.20893087681666\n11 4.18489013508996 11 4.2090708533185\n11 4.18504173012836 11 4.2092275104794\n11 4.18507644302479 11 4.20949034628942\n12 4.18280808326508 12 4.19909210865662\n12 4.18280808326508 12 4.20015496129958\n12 4.18280808326508 12 4.20127024846519\n12 4.18309310678616 12 4.20211272668391\n12 4.18309977233387 12 4.20324496314526\n12 4.18309977233387 12 4.20341804911833\n12 4.18312690463889 12 4.20437635172207\n12 4.18328202553376 12 4.2045218856453\n12 4.18351192816537 12 4.20457147429808\n12 4.18356289358115 12 4.20501325571882\n12 4.18364450036863 12 4.20509241959146\n12 4.18364450036863 12 4.20528939743956\n12 4.18365084691027 12 4.20632378298898\n12 4.18365084691027 12 4.20664634009167\n12 4.18399328312754 12 4.20680597744078\n12 4.18415637917394 12 4.20700501203849\n12 4.18421422000984 12 4.20718690425784\n12 4.18435448193816 12 4.20776120751246\n12 4.18435448193816 12 4.20822943285672\n12 4.18435448193816 12 4.20867958398913\n12 4.18445296246731 12 4.20873130494558\n12 4.18451809007324 12 4.20893087681666\n12 4.18451809007324 12 4.2090708533185\n12 4.18451809007324 12 4.2092275104794\n12 4.18451809007324 12 4.20949034628942\n13 4.18220103151977 13 4.19909210865662\n13 4.18279640596425 13 4.20015496129958\n13 4.18280808326508 13 4.20127024846519\n13 4.18280808326508 13 4.20211272668391\n13 4.18280808326508 13 4.20324496314526\n13 4.18280808326508 13 4.20341804911833\n13 4.18280808326508 13 4.20437635172207\n13 4.18295335694652 13 4.2045218856453\n13 4.18301313700348 13 4.20457147429808\n13 4.18308822935138 13 4.20501325571882\n13 4.18308822935138 13 4.20509241959146\n13 4.18309310678616 13 4.20528939743956\n13 4.18309310678616 13 4.20632378298898\n13 4.18309977233387 13 4.20664634009167\n13 4.18309977233387 13 4.20680597744078\n13 4.18309977233387 13 4.20700501203849\n13 4.18312690463889 13 4.20718690425784\n13 4.18312690463889 13 4.20776120751246\n13 4.18328202553376 13 4.20822943285672\n13 4.18328202553376 13 4.20867958398913\n13 4.18328202553376 13 4.20873130494558\n13 4.18328202553376 13 4.20893087681666\n13 4.18350337376599 13 4.2090708533185\n13 4.18351192816537 13 4.2092275104794\n13 4.18351192816537 13 4.20949034628942\n14 4.18220103151977 14 4.19909210865662\n14 4.18220103151977 14 4.20015496129958\n14 4.18220103151977 14 4.20127024846519\n14 4.18220103151977 14 4.20211272668391\n14 4.18220103151977 14 4.20324496314526\n14 4.18220103151977 14 4.20341804911833\n14 4.18220103151977 14 4.20437635172207\n14 4.18279640596425 14 4.2045218856453\n14 4.18279640596425 14 4.20457147429808\n14 4.18279640596425 14 4.20501325571882\n14 4.18279640596425 14 4.20509241959146\n14 4.18279640596425 14 4.20528939743956\n14 4.18279640596425 14 4.2061838820412\n14 4.18279640596425 14 4.20632378298898\n14 4.18279640596425 14 4.20664634009167\n14 4.18280808326508 14 4.20680597744078\n14 4.18280808326508 14 4.20700501203849\n14 4.18280808326508 14 4.20709938746332\n14 4.18280808326508 14 4.20718690425784\n14 4.18280808326508 14 4.20776120751246\n14 4.18280808326508 14 4.20822943285672\n14 4.18280808326508 14 4.20867958398913\n14 4.18280808326508 14 4.20873130494558\n14 4.18280808326508 14 4.20893087681666\n14 4.18280808326508 14 4.2090708533185\n15 4.18220103151977 15 4.19909210865662\n15 4.18220103151977 15 4.20015496129958\n15 4.18220103151977 15 4.20127024846519\n15 4.18220103151977 15 4.20211272668391\n15 4.18220103151977 15 4.20324496314526\n15 4.18220103151977 15 4.20341804911833\n15 4.18220103151977 15 4.20437635172207\n15 4.18220103151977 15 4.2045218856453\n15 4.18220103151977 15 4.20457147429808\n15 4.18220103151977 15 4.20501325571882\n15 4.18220103151977 15 4.20509241959146\n15 4.18220103151977 15 4.20528939743956\n15 4.18220103151977 15 4.2061838820412\n15 4.18220103151977 15 4.20632378298898\n15 4.18279640596425 15 4.20664634009167\n15 4.18279640596425 15 4.20680597744078\n15 4.18279640596425 15 4.20700501203849\n15 4.18279640596425 15 4.20709938746332\n15 4.18279640596425 15 4.20718690425784\n15 4.18279640596425 15 4.20776120751246\n15 4.18279640596425 15 4.20822943285672\n15 4.18279640596425 15 4.20867958398913\n15 4.18279640596425 15 4.20873130494558\n15 4.18279640596425 15 4.20893087681666\n15 4.18279640596425 15 4.2090708533185\n16 4.18220103151977 16 4.19909210865662\n16 4.18220103151977 16 4.20015496129958\n16 4.18220103151977 16 4.20127024846519\n16 4.18220103151977 16 4.20211272668391\n16 4.18220103151977 16 4.20324496314526\n16 4.18220103151977 16 4.20341804911833\n16 4.18220103151977 16 4.20437635172207\n16 4.18220103151977 16 4.2045218856453\n16 4.18220103151977 16 4.20457147429808\n16 4.18220103151977 16 4.20501325571882\n16 4.18220103151977 16 4.20509241959146\n16 4.18220103151977 16 4.20528939743956\n16 4.18220103151977 16 4.2061838820412\n16 4.18220103151977 16 4.20632378298898\n16 4.18220103151977 16 4.20664634009167\n16 4.18220103151977 16 4.20680597744078\n16 4.18220103151977 16 4.20700501203849\n16 4.18220103151977 16 4.20709938746332\n16 4.18220103151977 16 4.20718690425784\n16 4.18220103151977 16 4.20775613699506\n16 4.18220103151977 16 4.20776120751246\n16 4.18220103151977 16 4.20822943285672\n16 4.18220103151977 16 4.20867958398913\n16 4.18220103151977 16 4.20873130494558\n16 4.18220103151977 16 4.20893087681666\n17 4.18220103151977 17 4.19909210865662\n17 4.18220103151977 17 4.20015496129958\n17 4.18220103151977 17 4.20127024846519\n17 4.18220103151977 17 4.20211272668391\n17 4.18220103151977 17 4.20324496314526\n17 4.18220103151977 17 4.20341804911833\n17 4.18220103151977 17 4.20437635172207\n17 4.18220103151977 17 4.2045218856453\n17 4.18220103151977 17 4.20457147429808\n17 4.18220103151977 17 4.20501325571882\n17 4.18220103151977 17 4.20509241959146\n17 4.18220103151977 17 4.20528939743956\n17 4.18220103151977 17 4.2061838820412\n17 4.18220103151977 17 4.20632378298898\n17 4.18220103151977 17 4.20664634009167\n17 4.18220103151977 17 4.20680597744078\n17 4.18220103151977 17 4.20700501203849\n17 4.18220103151977 17 4.20709938746332\n17 4.18220103151977 17 4.20712699827744\n17 4.18220103151977 17 4.20718690425784\n17 4.18220103151977 17 4.20775613699506\n17 4.18220103151977 17 4.20776120751246\n17 4.18220103151977 17 4.20822943285672\n17 4.18220103151977 17 4.20867958398913\n17 4.18220103151977 17 4.20873130494558\n18 4.18220103151977 18 4.19909210865662\n18 4.18220103151977 18 4.20015496129958\n18 4.18220103151977 18 4.20127024846519\n18 4.18220103151977 18 4.20211272668391\n18 4.18220103151977 18 4.20323297523326\n18 4.18220103151977 18 4.20324496314526\n18 4.18220103151977 18 4.20341804911833\n18 4.18220103151977 18 4.20437635172207\n18 4.18220103151977 18 4.2045218856453\n18 4.18220103151977 18 4.20457147429808\n18 4.18220103151977 18 4.20501325571882\n18 4.18220103151977 18 4.20509241959146\n18 4.18220103151977 18 4.20528939743956\n18 4.18220103151977 18 4.2061838820412\n18 4.18220103151977 18 4.20632378298898\n18 4.18220103151977 18 4.20664634009167\n18 4.18220103151977 18 4.20680597744078\n18 4.18220103151977 18 4.20700501203849\n18 4.18220103151977 18 4.20709938746332\n18 4.18220103151977 18 4.20712699827744\n18 4.18220103151977 18 4.20718690425784\n18 4.18220103151977 18 4.20775613699506\n18 4.18220103151977 18 4.20776120751246\n18 4.18220103151977 18 4.20822943285672\n18 4.18220103151977 18 4.2085382055396\n19 4.18220103151977 19 4.19909210865662\n19 4.18220103151977 19 4.20015496129958\n19 4.18220103151977 19 4.20127024846519\n19 4.18220103151977 19 4.20211272668391\n19 4.18220103151977 19 4.20323297523326\n19 4.18220103151977 19 4.20324496314526\n19 4.18220103151977 19 4.20341804911833\n19 4.18220103151977 19 4.20437635172207\n19 4.18220103151977 19 4.2045218856453\n19 4.18220103151977 19 4.20457147429808\n19 4.18220103151977 19 4.20501325571882\n19 4.18220103151977 19 4.20509241959146\n19 4.18220103151977 19 4.20528939743956\n19 4.18220103151977 19 4.2061838820412\n19 4.18220103151977 19 4.20632378298898\n19 4.18220103151977 19 4.20664634009167\n19 4.18220103151977 19 4.20680597744078\n19 4.18220103151977 19 4.20700501203849\n19 4.18220103151977 19 4.20709938746332\n19 4.18220103151977 19 4.20712699827744\n19 4.18220103151977 19 4.20718690425784\n19 4.18220103151977 19 4.20775613699506\n19 4.18220103151977 19 4.20776120751246\n19 4.18220103151977 19 4.20822943285672\n19 4.18220103151977 19 4.2085382055396\n20 4.18220103151977 20 4.19909210865662\n20 4.18220103151977 20 4.20015496129958\n20 4.18220103151977 20 4.20022391697825\n20 4.18220103151977 20 4.20127024846519\n20 4.18220103151977 20 4.20211272668391\n20 4.18220103151977 20 4.20323297523326\n20 4.18220103151977 20 4.20324496314526\n20 4.18220103151977 20 4.20341804911833\n20 4.18220103151977 20 4.20437635172207\n20 4.18220103151977 20 4.2045218856453\n20 4.18220103151977 20 4.20457147429808\n20 4.18220103151977 20 4.20501325571882\n20 4.18220103151977 20 4.20509241959146\n20 4.18220103151977 20 4.20528939743956\n20 4.18220103151977 20 4.2061838820412\n20 4.18220103151977 20 4.20632378298898\n20 4.18220103151977 20 4.20664634009167\n20 4.18220103151977 20 4.20680597744078\n20 4.18220103151977 20 4.20700501203849\n20 4.18220103151977 20 4.20709938746332\n20 4.18220103151977 20 4.20712699827744\n20 4.18220103151977 20 4.20718690425784\n20 4.18220103151977 20 4.20775613699506\n20 4.18220103151977 20 4.20776120751246\n20 4.18220103151977 20 4.20822943285672\n21 4.18220103151977 21 4.19909210865662\n21 4.18220103151977 21 4.20015496129958\n21 4.18220103151977 21 4.20022391697825\n21 4.18220103151977 21 4.20127024846519\n21 4.18220103151977 21 4.20211272668391\n21 4.18220103151977 21 4.20323297523326\n21 4.18220103151977 21 4.20324496314526\n21 4.18220103151977 21 4.20341804911833\n21 4.18220103151977 21 4.20437635172207\n21 4.18220103151977 21 4.2045218856453\n21 4.18220103151977 21 4.20457147429808\n21 4.18220103151977 21 4.20490352602855\n21 4.18220103151977 21 4.20501325571882\n21 4.18220103151977 21 4.20509241959146\n21 4.18220103151977 21 4.20528939743956\n21 4.18220103151977 21 4.2061838820412\n21 4.18220103151977 21 4.20632378298898\n21 4.18220103151977 21 4.20664634009167\n21 4.18220103151977 21 4.20680597744078\n21 4.18220103151977 21 4.20700501203849\n21 4.18220103151977 21 4.20709938746332\n21 4.18220103151977 21 4.20712699827744\n21 4.18220103151977 21 4.20718690425784\n21 4.18220103151977 21 4.20775613699506\n21 4.18220103151977 21 4.20776120751246\n22 4.18220103151977 22 4.19909210865662\n22 4.18220103151977 22 4.20015496129958\n22 4.18220103151977 22 4.20022391697825\n22 4.18220103151977 22 4.20127024846519\n22 4.18220103151977 22 4.20211272668391\n22 4.18220103151977 22 4.20323297523326\n22 4.18220103151977 22 4.20324496314526\n22 4.18220103151977 22 4.20341804911833\n22 4.18220103151977 22 4.20380549037496\n22 4.18220103151977 22 4.20437635172207\n22 4.18220103151977 22 4.2045218856453\n22 4.18220103151977 22 4.20457147429808\n22 4.18220103151977 22 4.20490352602855\n22 4.18220103151977 22 4.20501325571882\n22 4.18220103151977 22 4.20509241959146\n22 4.18220103151977 22 4.20528939743956\n22 4.18220103151977 22 4.2061838820412\n22 4.18220103151977 22 4.20632378298898\n22 4.18220103151977 22 4.20664634009167\n22 4.18220103151977 22 4.20680597744078\n22 4.18220103151977 22 4.20700501203849\n22 4.18220103151977 22 4.20709938746332\n22 4.18220103151977 22 4.20712699827744\n22 4.18220103151977 22 4.20718690425784\n22 4.18220103151977 22 4.20775613699506\n23 4.18220103151977 23 4.19909210865662\n23 4.18220103151977 23 4.20015496129958\n23 4.18220103151977 23 4.20022391697825\n23 4.18220103151977 23 4.20127024846519\n23 4.18220103151977 23 4.20206821694343\n23 4.18220103151977 23 4.20211272668391\n23 4.18220103151977 23 4.20323297523326\n23 4.18220103151977 23 4.20324496314526\n23 4.18220103151977 23 4.20341804911833\n23 4.18220103151977 23 4.20380549037496\n23 4.18220103151977 23 4.20437635172207\n23 4.18220103151977 23 4.2045218856453\n23 4.18220103151977 23 4.20457147429808\n23 4.18220103151977 23 4.20490352602855\n23 4.18220103151977 23 4.20501325571882\n23 4.18220103151977 23 4.20509241959146\n23 4.18220103151977 23 4.20528939743956\n23 4.18220103151977 23 4.20565069698893\n23 4.18220103151977 23 4.2061838820412\n23 4.18220103151977 23 4.20632378298898\n23 4.18220103151977 23 4.20647419434699\n23 4.18220103151977 23 4.20664634009167\n23 4.18220103151977 23 4.20680597744078\n23 4.18220103151977 23 4.20700501203849\n23 4.18220103151977 23 4.20709938746332\n24 4.18220103151977 24 4.19909210865662\n24 4.18220103151977 24 4.20015496129958\n24 4.18220103151977 24 4.20022391697825\n24 4.18220103151977 24 4.20127024846519\n24 4.18220103151977 24 4.20206821694343\n24 4.18220103151977 24 4.20211272668391\n24 4.18220103151977 24 4.20323297523326\n24 4.18220103151977 24 4.20324496314526\n24 4.18220103151977 24 4.20341804911833\n24 4.18220103151977 24 4.20380549037496\n24 4.18220103151977 24 4.20437635172207\n24 4.18220103151977 24 4.2045218856453\n24 4.18220103151977 24 4.20457147429808\n24 4.18220103151977 24 4.20461524528768\n24 4.18220103151977 24 4.20490352602855\n24 4.18220103151977 24 4.20501325571882\n24 4.18220103151977 24 4.20509241959146\n24 4.18220103151977 24 4.20528939743956\n24 4.18220103151977 24 4.20565069698893\n24 4.18220103151977 24 4.2061838820412\n24 4.18220103151977 24 4.20632378298898\n24 4.18220103151977 24 4.20647419434699\n24 4.18220103151977 24 4.20664634009167\n24 4.18220103151977 24 4.20680597744078\n24 4.18220103151977 24 4.20700501203849\n25 4.18220103151977 25 4.19909210865662\n25 4.18220103151977 25 4.20015496129958\n25 4.18220103151977 25 4.20022391697825\n25 4.18220103151977 25 4.20127024846519\n25 4.18220103151977 25 4.20206821694343\n25 4.18220103151977 25 4.20211272668391\n25 4.18220103151977 25 4.20323297523326\n25 4.18220103151977 25 4.20324496314526\n25 4.18220103151977 25 4.20341804911833\n25 4.18220103151977 25 4.20380549037496\n25 4.18220103151977 25 4.20437635172207\n25 4.18220103151977 25 4.2045218856453\n25 4.18220103151977 25 4.20457147429808\n25 4.18220103151977 25 4.20461524528768\n25 4.18220103151977 25 4.20490352602855\n25 4.18220103151977 25 4.20501325571882\n25 4.18220103151977 25 4.20509241959146\n25 4.18220103151977 25 4.20528939743956\n25 4.18220103151977 25 4.20565069698893\n25 4.18220103151977 25 4.20575762278382\n25 4.18220103151977 25 4.2060260663846\n25 4.18220103151977 25 4.2061838820412\n25 4.18220103151977 25 4.20632378298898\n25 4.18220103151977 25 4.20647419434699\n25 4.18220103151977 25 4.20664634009167\n26 4.18220103151977 26 4.19909210865662\n26 4.18220103151977 26 4.20015496129958\n26 4.18220103151977 26 4.20022391697825\n26 4.18220103151977 26 4.20127024846519\n26 4.18220103151977 26 4.20206821694343\n26 4.18220103151977 26 4.20211272668391\n26 4.18220103151977 26 4.20227603260272\n26 4.18220103151977 26 4.20323297523326\n26 4.18220103151977 26 4.20324496314526\n26 4.18220103151977 26 4.20341804911833\n26 4.18220103151977 26 4.20380549037496\n26 4.18220103151977 26 4.20437635172207\n26 4.18220103151977 26 4.2045218856453\n26 4.18220103151977 26 4.20457147429808\n26 4.18220103151977 26 4.20461524528768\n26 4.18220103151977 26 4.20490352602855\n26 4.18220103151977 26 4.20501325571882\n26 4.18220103151977 26 4.20509241959146\n26 4.18220103151977 26 4.20528939743956\n26 4.18220103151977 26 4.20565069698893\n26 4.18220103151977 26 4.20575762278382\n26 4.18220103151977 26 4.2060260663846\n26 4.18220103151977 26 4.2061838820412\n26 4.18220103151977 26 4.20632378298898\n26 4.18220103151977 26 4.20647419434699\n27 4.18220103151977 27 4.19909210865662\n27 4.18220103151977 27 4.20015496129958\n27 4.18220103151977 27 4.20022391697825\n27 4.18220103151977 27 4.20127024846519\n27 4.18220103151977 27 4.20206821694343\n27 4.18220103151977 27 4.20211272668391\n27 4.18220103151977 27 4.20227603260272\n27 4.18220103151977 27 4.20323297523326\n27 4.18220103151977 27 4.20324496314526\n27 4.18220103151977 27 4.20341804911833\n27 4.18220103151977 27 4.20380549037496\n27 4.18220103151977 27 4.20437635172207\n27 4.18220103151977 27 4.2045218856453\n27 4.18220103151977 27 4.20457147429808\n27 4.18220103151977 27 4.20461524528768\n27 4.18220103151977 27 4.20490352602855\n27 4.18220103151977 27 4.20501325571882\n27 4.18220103151977 27 4.20509241959146\n27 4.18220103151977 27 4.20528939743956\n27 4.18220103151977 27 4.20565069698893\n27 4.18220103151977 27 4.20575762278382\n27 4.18220103151977 27 4.2060260663846\n27 4.18220103151977 27 4.2061838820412\n27 4.18220103151977 27 4.20632378298898\n27 4.18220103151977 27 4.20647419434699\n28 4.18220103151977 28 4.19909210865662\n28 4.18220103151977 28 4.20015496129958\n28 4.18220103151977 28 4.20022391697825\n28 4.18220103151977 28 4.20127024846519\n28 4.18220103151977 28 4.20206821694343\n28 4.18220103151977 28 4.20211272668391\n28 4.18220103151977 28 4.20227603260272\n28 4.18220103151977 28 4.20323297523326\n28 4.18220103151977 28 4.20324496314526\n28 4.18220103151977 28 4.20341804911833\n28 4.18220103151977 28 4.20380549037496\n28 4.18220103151977 28 4.20437635172207\n28 4.18220103151977 28 4.2045218856453\n28 4.18220103151977 28 4.20457147429808\n28 4.18220103151977 28 4.20461524528768\n28 4.18220103151977 28 4.20490352602855\n28 4.18220103151977 28 4.20501325571882\n28 4.18220103151977 28 4.20509241959146\n28 4.18220103151977 28 4.20528939743956\n28 4.18220103151977 28 4.20564853110567\n28 4.18220103151977 28 4.20565069698893\n28 4.18220103151977 28 4.20575762278382\n28 4.18220103151977 28 4.2060260663846\n28 4.18220103151977 28 4.2061838820412\n28 4.18220103151977 28 4.20632378298898\n29 4.18220103151977 29 4.19562099530617\n29 4.18220103151977 29 4.19909210865662\n29 4.18220103151977 29 4.20015496129958\n29 4.18220103151977 29 4.20022391697825\n29 4.18220103151977 29 4.20127024846519\n29 4.18220103151977 29 4.20206821694343\n29 4.18220103151977 29 4.20211272668391\n29 4.18220103151977 29 4.20227603260272\n29 4.18220103151977 29 4.20323297523326\n29 4.18220103151977 29 4.20324496314526\n29 4.18220103151977 29 4.20341804911833\n29 4.18220103151977 29 4.20380549037496\n29 4.18220103151977 29 4.20437635172207\n29 4.18220103151977 29 4.2045218856453\n29 4.18220103151977 29 4.20457147429808\n29 4.18220103151977 29 4.20461524528768\n29 4.18220103151977 29 4.20490352602855\n29 4.18220103151977 29 4.20501325571882\n29 4.18220103151977 29 4.20509241959146\n29 4.18220103151977 29 4.20528939743956\n29 4.18220103151977 29 4.20564853110567\n29 4.18220103151977 29 4.20565069698893\n29 4.18220103151977 29 4.20575762278382\n29 4.18220103151977 29 4.2060260663846\n29 4.18220103151977 29 4.2061838820412\n30 4.18220103151977 30 4.19562099530617\n30 4.18220103151977 30 4.19909210865662\n30 4.18220103151977 30 4.20015496129958\n30 4.18220103151977 30 4.20022391697825\n30 4.18220103151977 30 4.20127024846519\n30 4.18220103151977 30 4.20206821694343\n30 4.18220103151977 30 4.20211272668391\n30 4.18220103151977 30 4.20227603260272\n30 4.18220103151977 30 4.20323297523326\n30 4.18220103151977 30 4.20324496314526\n30 4.18220103151977 30 4.20341804911833\n30 4.18220103151977 30 4.20380549037496\n30 4.18220103151977 30 4.20437635172207\n30 4.18220103151977 30 4.2045218856453\n30 4.18220103151977 30 4.20457147429808\n30 4.18220103151977 30 4.20461524528768\n30 4.18220103151977 30 4.20490352602855\n30 4.18220103151977 30 4.20498488096914\n30 4.18220103151977 30 4.20501325571882\n30 4.18220103151977 30 4.20509241959146\n30 4.18220103151977 30 4.20528939743956\n30 4.18220103151977 30 4.20564853110567\n30 4.18220103151977 30 4.20565069698893\n30 4.18220103151977 30 4.20575762278382\n30 4.18220103151977 30 4.2060260663846\n4.18\n4.20\n4.22\n4.24\n1 6 11 15 20\nEvolutionary Search\nRandom Search\nSearch Iterations\nValidation Loss\nFigure 9: Evolutionary search can ﬁnd better SubTrans-\nformers in the SuperTransformer than random search.\ndecoding). HAT runs 1.3× faster than Transformer\nwith higher BLEU; 1.9× faster than Levenshtein\nwith 0.7 higher BLEU. Under similar latency, HAT\nalso outperforms Lite Transformer. These results\ndemonstrate HAT’s effectiveness in lower latency\nscenarios. Our framework can also be adopted to\nspeedup those models.\n4.2 Analysis\nDesign Insights. For all HAT WMT models\nin Figure 7, 10% of all decoder layers attend to\nSubTransformer Latency #Params BLEU\nWMT’14\nEn-De\nLargest 10.1s 71M 28.1\nSearched HAT 6.9s 48M 28.4\nWMT’14\nEn-Fr\nLargest 10.1s 71M 41.4\nSearched HAT 9.1s 57M 41.8\nTable 4: The searched HAT compared with the largest\nSubTransformer in the design space. Larger models do\nnot necessarily have better performance. HAT models\nhave lower latency, smaller size, and higher BLEU.\nthree encoder layers, 40% attend to two encoder\nlayers. That demonstrates the necessity of arbitrary\nencoder-decoder attentions.\nIn Appendix Figure 12, we visualize the mod-\nels specialized for different hardware mentioned in\nTable 1. We ﬁnd that the GPU model is wide but\nshallow; the Raspberry Pi model is deep but thin.\nThe phenomenon echos with our latency proﬁling\n(Figure 2) as GPU latency is insensitive to embed-\nding and hidden dim, but Raspberry Pi is highly\nsensitive. It guides manual designs: on GPU, we\ncan reduce the layer number and increase dimen-\nsion to reduce latency and keep high performance.\nAblation Study. HAT achieves higher BLEU\nwith 1.5× lower latency and 1.5× smaller size com-\npared with the largest SubTransformer (Table 4).\nThis suggests that larger models do not always\nprovide better performance, and demonstrates the\neffectiveness of HAT. We also compare the evo-\nlutionary search with random search (Figure 9).\nEvolutionary search can ﬁnd models with lower\nlosses than random search.\nWMT’14 En-De WMT’14 En-Fr\nInherited\nVal Loss\nInherited\nBLEU\nFrom-\nScratch\nBLEU\nInherited\nVal Loss\nInherited\nBLEU\nFrom-\nScratch\nBLEU\n4.71 24.9 25.8 3.92 37.4 38.8\n4.40 25.8 27.6 3.71 38.0 40.0\n4.07 26.3 28.1 3.48 39.5 41.1\n4.02 26.7 28.2 3.46 39.6 41.4\n4.01 26.9 28.4 3.45 39.7 41.7\nTable 5: The performance of SubTransformers with in-\nherited weights are close to those trained from-scratch,\nand have the same relative performance order.\nTable 2\nHuman Life 11023\nAmerican Life 36156\nUS car including \nfuel\n126000\nEvolved \nTransformer\n626155\nHAT (Ours) 6000\n626,155\n126,000\n36,156\n11,023\nHuman Life \n(Avg. 1 year)\nAmerican Life \n(Avg. 1 year)\nUS Car w/ Fuel \n(Avg. 1 lifetime)\nEvolved \nTransformer\nHAT (Ours) 52 12041×\n0 175K 350K 525K 700K\nCO2 Emission (lbs)\nFigure 10: The search cost measured in pounds of CO2\nemission. Our framework for searching HAT reduces\nthe cost by four orders of magnitude than the Evolved\nTransformer (So et al., 2019).\nSubTransformer Performance Proxy. All Sub-\nTransformers inside the SuperTransformer are uni-\nformly sampled and thus equally trained, so the\nperformance order is well-preserved during train-\ning. We conduct experiments to show the effective-\nness of the SubTransformer performance proxy as\nin Table 5 and Appendix Figure 11. The BLEUs\nof SubTransformers with inherited weights and\nweights trained from-scratch are very close. More\nimportantly, they also have the same relative per-\nformance order. Therefore, we can rely on the\nproxy to search high-performance model architec-\nture, signiﬁcantly reducing the search cost.\nLow Search Cost. As shown in Table 2 and Fig-\nure 10, the search cost of HAT is 12,041× lower\nthan the Evolved Transformer. Although both are\nusing Evolutionary Search, the key difference is\nthat Evolved Transformer needs to train all individ-\nual models and sort their ﬁnal performance to pick\ntop ones; on the contrary, HAT trains all models\ntogether inside SuperTransformer and sorts their\nperformance proxy to pick top ones. The superior\nperformance of HAT proves that the performance\nproxy is accurate enough to ﬁnd good models.\nFinetuning Inherited SubTransformers In sec-\ntion 4.1, we trained each searched SubTransformer\nTask From-Scratch 40K Inherit-Finetune 10K\nWMT’14\nEn-Fr\n41.5 41.7\n40.0 40.2\nWMT’14\nEn-De\n28.0 28.0\n27.5 27.4\nTable 6: The SubTransformer inherited from the Super-\nTransformer can achieve similar or better performance\nthan the same SubTransformer trained from-scratch.\nTraining steps are saved by 4×.\nfrom-scratch in order to conduct fair comparisons\nwith baselines. In practice, we can also directly\nﬁnetune the SubTransformers with the inherited\nweights from the SuperTransformer to further re-\nduce the training cost. With 10K ﬁnetuning steps\n(1/4 of from-scratch training), the inherited Sub-\nTransformers can achieve similar or better perfor-\nmance than trained from-scratch ones (Table 6).\nIn this way, the training cost for a model under a\nnew hardware constraint can be further reduced\nby 4×, since the SuperTransformer training cost is\namortizable among all searched models.\nQuantization Friendly. HAT is orthogonal to\nother model compression techniques such as quan-\ntization. We apply K-means quantization to HAT\nand further reduce the model size. We initialize\ncentroids uniformly in the range of [min, max] of\neach weight matrix and run at most 300 iterations\nfor each of them. Even without any ﬁnetuning,\n4-bit quantization can reduce the model size by\n25× with negligible BLEU loss compared to the\nTransformer-Big baseline (Table 7). Interestingly,\nthe 8-bit model even has 0.1 higher BLEU than the\nfull precision model, indicating the robustness of\nsearched HAT. Compared with the Transformer-\nBase 4-bit quantization baseline, which has 24MB\nmodel size and 38.9 BLEU score, HAT has 2.2\nhigher BLEU with similar model size.\nKnowledge Distillation Friendly. HAT is also\northogonal to knowledge distillation (KD) because\nHAT focuses on searching for an efﬁcient architec-\nture while KD focuses on better training a given\narchitecture. We combine KD with HAT by dis-\ntilling token-level knowledge (top-5 soft labels)\nfrom a high-performance SubTransformer to a low-\nperformance SubTransformer on WMT’14 En-De\ntask. The teacher model has a BLEU of 28.5 and\n49M parameters; the student model has 30M pa-\nrameters. KD can improve the BLEU of the student\nmodel from 25.8 to 26.1.\nBLEU Model Size Reduction\nTransformer Float32 41.2 705MB –\nHAT Float32 41.8 227MB 3 ×\nHAT 8 bits 41.9 57MB 12 ×\nHAT 4 bits 41.1 28MB 25 ×\nTable 7: K-means quantization of HAT models on\nWMT’14 En-Fr. 4-bit quantization reduces the model\nsize by 25 × with only 0.1 BLEU loss compared with\nthe transformer baseline. 8-bit quantization even has\n0.1 higher BLEU than its full precision version.\n5 Related Work\nTransformer. Transformer (Vaswani et al., 2017)\nhas prevailed in sequence modeling (Ng et al.,\n2019; Junczys-Dowmunt, 2018). By stacking iden-\ntical blocks, the model obtains a large capacity\nbut incurs high latency. Recently, a research trend\nis to modify the Transformer to improve the per-\nformance (Chen et al., 2018; Wu et al., 2019b;\nSukhbaatar et al., 2019; Wang et al., 2019). Among\nthem, Wu et al. (2019b) introduced a convolution-\nbased module to replace the attention; Wang et al.\n(2019) proposed to train deep Transformers by\npropagating multiple layers together in the encoder.\nZhang et al. (2018) and Kim et al. (2019) also\nproposed AAN and SSRU to replace the attention\nmechanism. HAT is orthogonal to them and can\nbe combined to search for efﬁcient architecture\nwith those new modules. Another trend is to ap-\nply non- or partially-autoregressive models to cut\ndown the iteration number for decoding (Gu et al.,\n2019; Akoury et al., 2019; Wei et al., 2019; Gu\net al., 2018). Although reducing latency, they some-\ntimes suffer from low performance. Bapna et al.\n(2018) explored using learned linear combinations\nof encoder outputs as decoder inputs, while HAT\nconcatenates the outputs without linear combina-\ntions, thus better preserving the low-level informa-\ntion. Wu et al. (2020) investigated mobile settings\nfor NLP tasks and proposed a multi-branch Lite\nTransformer. However, it relied on FLOPs for efﬁ-\ncient model design, which is an inaccurate proxy\nfor hardware latency (Figure 2). There are also\nworks (Kim and Rush, 2016; Junczys-Dowmunt\net al., 2018; Kim et al., 2019; Yan et al., 2020) us-\ning Knowledge Distillation (KD) to obtain small\nstudent models. Our method is orthogonal to KD\nand can be combined with it to improve the efﬁ-\nciency further. There are also hardware acceler-\nators (Ham et al., 2020; Zhang et al., 2020) for\nattention and fully-connected layers in the Trans-\nformer to achieve efﬁcient processing.\nNeural Architecture Search. In the computer\nvision community, there has been an increasing\ninterest in automating efﬁcient model design with\nNeural Architecture Search (NAS) (Zoph and Le,\n2017; Zoph et al., 2018; Pham et al., 2018; He\net al., 2018). Some applied black-box optimization\nsuch as evolutionary search (Wang et al., 2020b)\nand reinforcement learning (Cai et al., 2019b; He\net al., 2018; Wang et al., 2018, 2020a; Mao et al.,\n2019); Some leveraged backpropagation with dif-\nferentiable architecture search (Liu et al., 2019).\nSome also involved hardware constraints into op-\ntimizations such as MNasNet (Tan et al., 2019),\nProxylessNAS (Cai et al., 2019b), FBNet (Wu et al.,\n2019a) and APQ (Wang et al., 2020b). To reduce\nthe NAS cost, supernet based methods (Pham et al.,\n2018; Bender et al., 2018; Guo et al., 2019) apply\na proxy for sub-network performance and adopt\nsearch algorithms to ﬁnd good sub-networks. For\nNLP tasks, the beneﬁts of the architecture search\nhave not been fully investigated. Recently, So et al.\n(2019) proposed the Evolved Transformer to search\nfor architectures under model size constraints and\nsurpassed the original Transformer baselines. How-\never, it suffered from very high search costs (250\nGPU years), making it unaffordable to search spe-\ncialized models for various hardware and tasks. In\naddition, hardware latency feedback was not taken\ninto account for better case-by-case specializations.\nSince different hardware has distinct architecture\nand features (Cong et al., 2018), feedback from\nhardware is critical for efﬁcient NLP.\n6 Conclusion\nWe propose Hardware-Aware Transformers (HAT)\nframework to solve the challenge of efﬁcient de-\nployments of Transformer models on various hard-\nware platforms. We conduct hardware-aware neu-\nral architecture search in an ample design space\nwith an efﬁcient weight-shared SuperTransformer,\nconsuming four orders of magnitude less cost than\nthe prior Evolved Transformer, and discover high-\nperformance low-latency models. We hope HAT\ncan open up an avenue towards efﬁcient Trans-\nformer deployments for real-world applications.\nAcknowledgment\nWe thank NSF Career Award #1943349, MIT-IBM\nWatson AI Lab, Semi-conductor Research Corpo-\nration (SRC), Intel, and Facebook for supporting\nthis research.\nReferences\nNader Akoury, Kalpesh Krishna, and Mohit Iyyer.\n2019. Syntactically supervised transformers for\nfaster neural machine translation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1269–1281, Florence,\nItaly. Association for Computational Linguistics.\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao,\nand Yonghui Wu. 2018. Training deeper neural\nmachine translation models with transparent atten-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3028–3033, Brussels, Belgium. Association\nfor Computational Linguistics.\nGabriel Bender, Pieter-Jan Kindermans, Barret Zoph,\nVijay Vasudevan, and Quoc Le. 2018. Understand-\ning and simplifying one-shot architecture search. In\nProceedings of the 35th International Conference\non Machine Learning, volume 80 of Proceedings of\nMachine Learning Research, pages 550–559, Stock-\nholmsmssan, Stockholm Sweden. PMLR.\nHan Cai, Chuang Gan, and Song Han. 2019a. Once for\nall: Train one network and specialize it for efﬁcient\ndeployment. arXiv preprint arXiv:1908.09791.\nHan Cai, Ligeng Zhu, and Song Han. 2019b. Proxy-\nlessNAS: Direct neural architecture search on target\ntask and hardware. In International Conference on\nLearning Representations.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 76–86, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nJason Cong, Zhenman Fang, Michael Lo, Hanrui Wang,\nJingxian Xu, and Shaochong Zhang. 2018. Un-\nderstanding performance differences of fpgas and\ngpus. In 2018 IEEE 26th Annual International Sym-\nposium on Field-Programmable Custom Computing\nMachines (FCCM), pages 93–96. IEEE.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1243–1252, International\nConvention Centre, Sydney, Australia. PMLR.\n´Edouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J ´egou. 2017. Efﬁcient\nsoftmax approximation for GPUs. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1302–1310, International\nConvention Centre, Sydney, Australia. PMLR.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations.\nJiatao Gu, Changhan Wang, and Jake Zhao. 2019. Lev-\nenshtein Transformer. arXiv.\nZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\nZechun Liu, Yichen Wei, and Jian Sun. 2019. Sin-\ngle path one-shot neural architecture search with uni-\nform sampling.\nTae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H\nOh, Yeonhong Park, Yoonho Song, Jung-Hun Park,\nSanghee Lee, Kyoung Park, Jae W Lee, et al. 2020.\nAˆ 3: Accelerating attention mechanisms in neural\nnetworks with approximation. In 2020 IEEE In-\nternational Symposium on High Performance Com-\nputer Architecture (HPCA), pages 328–341. IEEE.\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li,\nand Song Han. 2018. Amc: Automl for model com-\npression and acceleration on mobile devices. In Pro-\nceedings of the European Conference on Computer\nVision (ECCV), pages 784–800.\nAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. 2017. Mo-\nbilenets: Efﬁcient convolutional neural networks for\nmobile vision applications.\nMarcin Junczys-Dowmunt. 2018. Microsoft’s submis-\nsion to the wmt2018 news translation task: How i\nlearned to stop worrying and love the data. arXiv\npreprint arXiv:1809.00196.\nMarcin Junczys-Dowmunt. 2019. Microsoft transla-\ntor at wmt 2019: Towards large-scale document-\nlevel neural machine translation. arXiv preprint\narXiv:1907.06170.\nMarcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu\nHoang, Roman Grundkiewicz, and Anthony Aue.\n2018. Marian: Cost-effective high-quality neural\nmachine translation in C++. In Proceedings of the\n2nd Workshop on Neural Machine Translation and\nGeneration, pages 129–135, Melbourne, Australia.\nAssociation for Computational Linguistics.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. arXiv preprint\narXiv:1606.07947.\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany Has-\nsan, Alham Fikri Aji, Kenneth Heaﬁeld, Roman\nGrundkiewicz, and Nikolay Bogoychev. 2019. From\nresearch to production and back: Ludicrously fast\nneural machine translation. In Proceedings of the\n3rd Workshop on Neural Generation and Transla-\ntion, pages 280–288, Hong Kong. Association for\nComputational Linguistics.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In Interna-\ntional Conference on Learning Representations.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2019. DARTS: Differentiable architecture search.\nIn International Conference on Learning Represen-\ntations.\nIlya Loshchilov and Frank Hutter. 2017. SGDR:\nStochastic Gradient Descent with Warm Restarts. In\nInternational Conference on Learning Representa-\ntions.\nHongzi Mao, Parimarjan Negi, Akshay Narayan, Han-\nrui Wang, Jiacheng Yang, Haonan Wang, Ryan Mar-\ncus, Mehrdad Khani Shirkoohi, Songtao He, Vikram\nNathan, et al. 2019. Park: An open platform for\nlearning-augmented computer systems. In Advances\nin Neural Information Processing Systems , pages\n2490–2502.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nFAIR’s WMT19 news translation task submission.\nIn Proceedings of the Fourth Conference on Ma-\nchine Translation (Volume 2: Shared Task Papers,\nDay 1), pages 314–319, Florence, Italy. Association\nfor Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nHieu Pham, Melody Guan, Barret Zoph, Quoc Le, and\nJeff Dean. 2018. Efﬁcient neural architecture search\nvia parameters sharing. In Proceedings of the 35th\nInternational Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Re-\nsearch, pages 4095–4104, Stockholmsmssan, Stock-\nholm Sweden. PMLR.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv preprint arXiv:1804.08771.\nDavid So, Quoc Le, and Chen Liang. 2019. The\nevolved transformer. In Proceedings of the 36th In-\nternational Conference on Machine Learning , vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 5877–5886, Long Beach, California,\nUSA. PMLR.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 331–335, Florence, Italy.\nAssociation for Computational Linguistics.\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasude-\nvan, Mark Sandler, Andrew Howard, and Quoc V .\nLe. 2019. Mnasnet: Platform-aware neural architec-\nture search for mobile. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Conference on Neural Information Pro-\ncessing Systems.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nHanrui Wang, Kuan Wang, Jiacheng Yang, Linxiao\nShen, Nan Sun, Hae-Seung Lee, and Song Han.\n2020a. Gcn-rl circuit designer: Transferable transis-\ntor sizing with graph neural networks and reinforce-\nment learning. In ACM/IEEE 57th Design Automa-\ntion Conference (DAC).\nHanrui Wang, Jiacheng Yang, Hae-Seung Lee, and\nSong Han. 2018. Learning to design circuits. In\nNeurIPS 2018 Machine Learning for Systems Work-\nshop.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nTianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian\nLiu, Hanrui Wang, Yujun Lin, and Song Han. 2020b.\nApq: Joint search for network architecture, pruning\nand quantization policy. In Conference on Computer\nVision and Pattern Recognition.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang\nLin, and Xu Sun. 2019. Imitation learning for non-\nautoregressive neural machine translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1304–\n1312, Florence, Italy. Association for Computational\nLinguistics.\nBichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan\nWang, Fei Sun, Yiming Wu, Yuandong Tian, Peter\nVajda, Yangqing Jia, and Kurt Keutzer. 2019a. Fb-\nnet: Hardware-aware efﬁcient convnet design via\ndifferentiable neural architecture search. In The\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR).\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019b. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. 2020. Lite transformer with long-short range\nattention. In International Conference on Learning\nRepresentations.\nZhongxia Yan, Hanrui Wang, Demi Guo, and Song\nHan. 2020. Micronet for efﬁcient language model-\ning. Journal of Machine Learning Research.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accel-\nerating neural transformer via an average attention\nnetwork. arXiv preprint arXiv:1805.00631.\nZhekai Zhang, Hanrui Wang, Song Han, and William J\nDally. 2020. Sparch: Efﬁcient architecture for\nsparse matrix multiplication. In 2020 IEEE Interna-\ntional Symposium on High Performance Computer\nArchitecture (HPCA), pages 261–274. IEEE.\nBarret Zoph and Quoc V Le. 2017. Neural Architec-\nture Search with Reinforcement Learning. In Inter-\nnational Conference on Learning Representations.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and\nQuoc V Le. 2018. Learning Transferable Architec-\ntures for Scalable Image Recognition. In The IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR).\nTable 1\nValidation Loss From-Scratch \nBLEU\n4.011039 28.4\n4.011109 28.3\n4.011112 28.3\n4.028058 28.2\n4.028403 28.2\n4.045922 28.1\n4.072190 28.1\n4.072542 28.2\n4.075409 28.1\n4.097665 27.9\n4.165567 27.9\n4.165906 28.0\n4.172398 27.8\n4.182698 27.8\n4.405200 27.4\n4.449993 27.4\n4.468512 26.9\n4.704716 25.8\n4.705631 25.8\n4.717223 25.8\n4.40 27.6\n25\n26\n27\n28\n29\n3.90 4.13 4.35 4.58 4.80\nTable 1-1\nValidation Loss From-Scratch \nBLEU\n3.448210 41.8\n3.454411 41.5\n3.454629 41.7\n3.455616 41.4\n3.459664 41.4\n3.477138 41.1\n3.478086 41.1\n3.483934 41.1\n3.487714 41.0\n3.533458 40.7\n3.533737 40.7\n3.547844 40.6\n3.549524 40.6\n3.707655 40.0\n3.707659 40.1\n3.708909 40.1\n3.749576 39.7\n3.916193 39.1\n3.924879 38.8\n3.926471 38.7\n38\n39\n40\n41\n42\n3.30 3.48 3.65 3.83 4.00\nWMT ’14 En-Fr\nWMT ’14 En-De\nInherited SubTransformer \nValidation Loss (Proxy)\nSubTransformer trained \nfrom-scratch BLEU (Target)\nInherited SubTransformer \nValidation Loss (Proxy)\nThe larger the inherited val \nloss, the lower the trained \nfrom-scratch BLEU.\nThe larger the inherited val \nloss, the lower the trained \nfrom-scratch BLEU.\nSubTransformer trained \nfrom-scratch BLEU (Target)\nFigure 11: The validation loss of SubTransformers is\na good performance proxy for BLEU of from-scratch\ntrained SubTransformers. The larger the validation\nloss, the lower the BLEU score.\nA Appendix for “HAT: H ardware-Aware\nTransformers for Efﬁcient Natural\nLanguage Processing”\nA.1 SubTransformer Performance Proxy\nIn Figure 11, we show the relationship between\nthe validation loss of SubTransformers directly in-\nherited from the SuperTransformer, and the BLEU\nscore of the SubTransformers trained from-scratch.\nWe can observe that the larger the validation loss,\nthe lower the BLEU score. Therefore the validation\nloss can be a good performance proxy.\nA.2 Visualizations of Searched Models on\nWMT’14 En-De Task\nWe show the HAT models searched for Raspberry\nPi ARM Cortex-A72 CPU and Nvidia TITAN Xp\nGPU in Figure 12. The searched model for Rasp-\nberry Pi is deep and thin, while that for GPU is\nshallow and wide. The BLEU scores of the two\nmodels are similar: 28.10 for Raspberry Pi CPU,\nand 28.15 for Nvidia GPU.\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\nDecoder Layer 1\nEncoder Layer 1\n512 Embed Dim\nEncoder Layer 2 Encoder Layer 3 Encoder Layer 4 Encoder Layer 5 Encoder Layer 6\nDecoder Layer 2 Decoder Layer 3 Decoder Layer 4\n2048 Hidden \nDim FFN\n2048 Hidden \nDim FFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n512 Embed Dim\nConcat\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\nDecoder Layer 1\nEncoder Layer 1\n640 Embed Dim\nEncoder Layer 2 Encoder Layer 3 Encoder Layer 4 Encoder Layer 5 Encoder Layer 6\nDecoder Layer 2 Decoder Layer 3\n2048 Hidden \nDim FFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n640 Embed Dim\nConcat\nInput Input\nOutput\nOutput\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n4 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n4 Heads \nEn-Decoder \nAttention\nSubTransformer Optimized for Nvidia TITAN Xp GPUSubTransformer Optimized for Raspberry Pi \nARM Cortex-A72 CPU\nFigure 12: SubTransformers optimized for Raspberry Pi ARM CPU and Nvidia GPU on WMT’14 En-De task are\ndifferent. The CPU model has BLEU 28.10, and GPU model has BLEU 28.15.\nA.3 Latency, BLEU and SacreBLEU of\nsearched HAT models.\nIn Table 8, we show the speciﬁc latency numbers,\nBLEU and SacreBLEU (Post, 2018) scores for\nsearched HAT models in Figure 7 and Figure 8.\nTask Hardware Latency BLEU SacreBLEU\nWMT’14\nEn-De\nRaspberry Pi\nARM Cortex-A72\nCPU\n3.5s 25.8 25.6\n4.0s 26.9 26.6\n4.5s 27.6 27.1\n5.0s 27.8 27.2\n6.0s 28.2 27.6\n6.9s 28.4 27.8\nIntel\nXeon E5-2640\nCPU\n137.9ms 25.8 25.6\n204.2ms 27.6 27.1\n278.7ms 27.9 27.3\n340.2ms 28.1 27.5\n369.6ms 28.2 27.6\n450.9ms 28.5 27.9\nNvidia\nTITAN Xp\nGPU\n57.1ms 25.8 25.6\n91.2ms 27.6 27.1\n126.0ms 27.9 27.3\n146.7ms 28.1 27.5\n208.1ms 28.5 27.8\nWMT’14\nEn-Fr\nRaspberry Pi\nARM Cortex-A72\nCPU\n4.3s 38.8 36.0\n5.3s 40.1 37.3\n5.8s 40.6 37.8\n6.9s 41.1 38.3\n7.8s 41.4 38.5\n9.1s 41.8 38.9\nIntel\nXeon E5-2640\nCPU\n154.7ms 39.1 36.3\n208.8ms 40.0 37.2\n329.4ms 41.1 38.2\n394.5ms 41.4 38.5\n442.0ms 41.7 38.8\nNvidia\nTITAN Xp\nGPU\n69.3ms 39.1 36.3\n94.9ms 40.0 37.2\n132.9ms 40.7 37.8\n168.3ms 41.1 38.3\n208.3ms 41.7 38.8\nWMT’19\nEn-De\nNvidia\nTITAN Xp\nGPU\n55.7ms 42.4 41.9\n93.2ms 44.4 43.9\n134.5ms 45.4 44.7\n176.1ms 46.2 45.6\n204.5ms 46.5 45.7\n237.8ms 46.7 46.0\nIWSLT’14\nDe-En\nNvidia\nTITAN Xp\nGPU\n45.6ms 33.4 32.5\n74.5ms 34.2 33.3\n109.0ms 34.5 33.6\n137.8ms 34.7 33.8\n168.8ms 34.8 33.9\nTable 8: Speciﬁc latency numbers, BLEU and Sacre-\nBLEU scores for searched HAT models in Figure 7 and\nFigure 8.",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.7988307476043701
    },
    {
      "name": "Computer science",
      "score": 0.742294192314148
    },
    {
      "name": "Encoder",
      "score": 0.5503175854682922
    },
    {
      "name": "Computation",
      "score": 0.4738108217716217
    },
    {
      "name": "Computer hardware",
      "score": 0.47133827209472656
    },
    {
      "name": "Transformer",
      "score": 0.45367926359176636
    },
    {
      "name": "Machine translation",
      "score": 0.4240991473197937
    },
    {
      "name": "Parallel computing",
      "score": 0.3662693500518799
    },
    {
      "name": "Algorithm",
      "score": 0.25371795892715454
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1899746060371399
    },
    {
      "name": "Operating system",
      "score": 0.16908219456672668
    },
    {
      "name": "Physics",
      "score": 0.08876961469650269
    },
    {
      "name": "Voltage",
      "score": 0.08782333135604858
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}