{
  "title": "Vision Transformer for femur fracture classification",
  "url": "https://openalex.org/W4224216771",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4301159484",
      "name": "Tanzi, Leonardo",
      "affiliations": [
        "Polytechnic University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A2753136442",
      "name": "Audisio Andrea",
      "affiliations": [
        "University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A2748512139",
      "name": "Cirrincione Giansalvo",
      "affiliations": [
        "Université de Picardie Jules Verne"
      ]
    },
    {
      "id": "https://openalex.org/A2474408912",
      "name": "Aprato Alessandro",
      "affiliations": [
        "University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A3192000005",
      "name": "Vezzetti, Enrico",
      "affiliations": [
        "Polytechnic University of Turin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6682885887",
    "https://openalex.org/W2128986278",
    "https://openalex.org/W4247070365",
    "https://openalex.org/W4251731611",
    "https://openalex.org/W2149987707",
    "https://openalex.org/W3094138545",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3033546813",
    "https://openalex.org/W2266464013",
    "https://openalex.org/W3174972748",
    "https://openalex.org/W6768396509",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6761275059",
    "https://openalex.org/W6757010476",
    "https://openalex.org/W3007647093",
    "https://openalex.org/W6629944447",
    "https://openalex.org/W2895650333",
    "https://openalex.org/W2897228760",
    "https://openalex.org/W2733840449",
    "https://openalex.org/W2793251588",
    "https://openalex.org/W3048464064",
    "https://openalex.org/W6743776964",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2554140915",
    "https://openalex.org/W2012091460",
    "https://openalex.org/W2692121892",
    "https://openalex.org/W1637283730",
    "https://openalex.org/W2166507657",
    "https://openalex.org/W2141619730",
    "https://openalex.org/W2892181857",
    "https://openalex.org/W2154461664",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W3133840873",
    "https://openalex.org/W2588978745"
  ],
  "abstract": null,
  "full_text": "VISION TRANSFORMERS FOR FEMUR FRACTURE\nCLASSIFICATION\nSUBMITTED TO ARTIFICIAL INTELLIGENCE IN MEDICINE\nLeonardo Tanzi ∗\nDIGEP\nPolytechnic University of Turin\nTurin, IT\nleonardo.tanzi@polito.it\nAndrea Audisio\nSchool of Medicine\nUniversity of Turin\nTurin, IT\nandrea.audisio@unito.it\nGiansalvo Cirrincione\nLTI\nUniversity of Picardie Jules Verne\nAmiens, FR\nexin@u-picardie.fr\nAlessandro Aprato\nSchool of Medicine\nUniversity of Turin\nTurin, IT\nalessandro.aprato@unito.it\nEnrico Vezzetti\nDIGEP\nPolytechnic University of Turin\nTurin, IT\nenrico.vezzetti@polito.it\nOctober 27, 2021\nABSTRACT\nObjectives: In recent years, the scientiﬁc community has focused on the development of Computer-\nAided Diagnosis (CAD) tools that could improve bone fractures’ classiﬁcation, mostly based on\nConvolutional Neural Network (CNN). However, the discerning accuracy of fractures’ subtypes was\nfar from optimal. This paper proposes a modiﬁed version of a very recent and powerful deep learning\ntechnique, the Vision Transformer (ViT), outperforming CNNs based approaches and consequently\nincrease specialists’ diagnosis accuracy.\nMethods: 4207 manually annotated images were used and distributed, by following the AO/OTA\nclassiﬁcation, in different fracture types, the largest labeled dataset of proximal femur fractures used\nin literature. The ViT architecture was used and compared with a classic CNN and a multistage\narchitecture composed by successive CNNs in cascade. To demonstrate the reliability of this approach,\n1) the attention maps were used to visualize the most relevant areas of the images, 2) the performance\nof a generic CNN and ViT was compared through unsupervised learning techniques and 3) 11\nspecialists were asked to evaluate and classify 150 proximal femur fractures’ images with and without\nthe help of the ViT, then results were compared for potential improvement.\nResults: The ViT was able to correctly predict 83% of the test images. Precision, recall and F1-score\nwere 0.77 (CI 0.64-0.90), 0.76 (CI 0.62-0.91) and 0.77 (CI 0.64–0.89), respectively. The average\nspecialists’ diagnostic improvement was 29% when supported by ViT’s predictions, outperforming\nthe algorithm alone.\nConclusions: This paper showed the potential of Vision Transformers in bone fracture classiﬁcation.\nFor the ﬁrst time, good results were obtained in sub-fractures classiﬁcation, with the largest and\nrichest dataset ever, outperforming the state of the art. Accordingly, the assisted diagnosis yielded the\nbest results, proving once again the effectiveness of a coordinate work between neural networks and\nspecialists.\nKeywords Deep Learning · Vision Transformer · Femur Fracture · Self-Attention · CAD System\n∗Corresponding author.\narXiv:2108.03414v2  [cs.AI]  26 Oct 2021\nPreprint SUBMITTED TO AI IN MEDICINE\n1 Introduction\nMusculoskeletal diseases represent the most common cause for severe, long-term disability worldwide [1]. Due to the\nprogressive aging of the population, the prevalence and incidence of fragility fractures is increasing and will continue\nso in the future [2]. In 2010 the estimated incidence of hip fractures was 2.7 million patients per year globally. In the\nEmergency Department, a pelvis radiograph is performed when a hip fracture is suspected [3]. The correct evaluation\nand classiﬁcation of proximal femur fractures by radiologists strongly affects future patients’ treatment and prognosis.\nThe AO/OTA classiﬁcation is hierarchical and provides a well-deﬁned methodology for assessing fractures correctly,\nenabling physicians to guide treatment and communicate with a standardized language [ 4]. However, the correct\nclassiﬁcation of hip fractures can be demanding for osteopenia, superimposition of soft tissues in obese patients and\ndifﬁcult patients’ positioning resulting in poorer image quality [ 5]. The above mentioned difﬁculties, the stressful\nworking environment of Emergency Departments and the perceived complexity of the classiﬁcation might have affected\nits widespread utilization, limiting evidence-base fracture management and data collection for research. In this context,\nimplementing a CAD (Computer Assisted Diagnosis) system in doctors’ workﬂow might have a direct impact in patients’\noutcome. This idea was demonstrated in a previous work [6] from our research group, where a deep learning [7] based\nmethod was used to classify femur fractures and the performance of physicians with and without its help was compared.\nDeep learning is becoming more and more widely used, giving astonishing results in different ﬁelds of application,\nsuch as surgery [8, 9, 10] and face recognition [11]. In the vision domain, after the introduction of AlexNet [12] on\nthe ImageNet competition in 2017, the applications of Convolutional Neural Networks (CNNs) have been increasing\nfor their ability to capture the spatial dependencies in an image. Nevertheless, CNNs have different limitations. In\nrecent times, a new paradigm called Transformer, introduced formerly for Natural Language Processing (NLP) [13], has\ndemonstrated exemplary performance on a broad range of language tasks, by means of BERT (Bidirectional Encoder\nRepresentations from Transformers) [14] or GPT (Generative Pre-trained Transformer) [15]. Transformer architectures\nare based on a self-attention mechanism that learns the relationships between the elements of a sequence and 1) can\ndeal with complete sequences, thus learning long-range relationships 2) can be easily parallelized 3) can be scaled\nto high-complexity models and large-scale datasets. The discovery of Transformer networks in the NLP domain has\naroused great interest in the computer vision community. However, visual data follow a typical structure, thus requiring\nnew network designs and training schemes. As a result, different authors have proposed their own implementation of a\nTransformer model applied to vision, obtaining great results in object detection [16], segmentation [17], video analysis\n[18], image generation [19], and many more. In a previous publication from our group [ 20], some selected papers\nconcerning the topic of femur fracture classiﬁcation have been reviewed, from basic approaches to main advanced\nsolutions. Initial prior works for detection and classiﬁcation of fractures [ 21, 22] focused on conventional machine\nlearning processes consisting of pre-processing, feature extraction and classiﬁcation phases. Recently, impressive results\nhave been obtained using CNNs. Unfortunately, the majority of the existing works regarding fractures classiﬁcation,\nfocused mainly on the binary classiﬁcation between Broken and Unbroken bones [23, 24, 25], which has a low impact\non doctor diagnosis. To the best of our knowledge, pure CNNs have been applied to classify different types of fractures\njust in two previous papers [26, 27]. More complex architectures were applied in [28], where an encoder with a LSTM\n(Long Short Term Memory) layer obtained very good results even in sub fracture classiﬁcation, but using also radiology\nreports which are not always easy to obtain, and in [ 29], where the authors used spatial transformer and a CNN to\nlocalize and classify the fractures. Nevertheless, results are still non optimal, especially for complex fractures, and a\ngeneralized approach does not exist yet.\nThe novelties introduced by this work are four-fold:\n1. the largest and richest labeled dataset ever for femur fractures classiﬁcation was used, with 4207 images\ndivided in 7 different classes;\n2. the Vision Transformer (ViT) implementation by Google [30] was applied for the classiﬁcation task, surpassing\nthe two baselines of a classic InceptionV3 [31] network and a hierarchical network proposed in [6]. This is the\nﬁrst work where CNN were not involved in the classiﬁcation pipeline;\n3. the attention maps of ViT were visualized and the output of the Transformer’s encoder was clustered in order\nto understand the potentiality of this architecture;\n4. a ﬁnal evaluation was carried out, asking to 11 specialists to classify 150 images by means of an online survey,\nwith and without the help of our system.\nThis paper is structured as follows. Section 2 covers materials and methods that are relevant for the design of this\napproach. Section 3 shows the overall performance and evaluation. Section 4 discusses our ﬁndings, recommendations,\nfuture work and summarizes our conclusions.\n2\nPreprint SUBMITTED TO AI IN MEDICINE\nFigure 1: AO/OTA hierarchical classiﬁcation determined by the localization and conﬁguration of the fracture lines.\nType A, type B and type C fractures concern the trochanteric region, the femoral neck and the femoral head, respectively.\nEach group is then subsequently divided in different levels of subgroups\n2 Materials and methods\n2.1 AO/OTA Proximal Femur Classiﬁcation\nThe proximal femur is labelled as “31”, being the ﬁrst number related to the femur and the second to the proximal\nregion. Then the intertrochanteric region is coded as “A”, the neck of femur “B” and the femoral head as “C”. Then the\nfracture is further described in groups and subgroups depending on the complexity and degree of displacement. The\nclassiﬁcation process adopted in this study is shown in Figure 1.\n3\nPreprint SUBMITTED TO AI IN MEDICINE\n2.2 Patients selection and Dataset\nThis retrospective study was conducted in a Level-I trauma center and was approved by . the appropriate Ethics Review\nBoard. All >18 year-old patients with proximal femur fracture in the Emergency Department between January 2013\nand December 2020 were eligible for enrollment. Exclusion criteria included missing pelvic anteroposterior radiograph\ndocumenting the hip fracture on the hospital’s PACS and pathologic fractures. Demographic data, pelvic anteroposterior\nradiographs with related radiological referral and intraoperative diagnosis were recorded in a computerized dataset. All\ndata was collected anonymously using Synapse 3D (FUJIFILM Corporation) and each image was carefully examined\nfor removal of all labels. Table 1 describes patients’ baseline characteristics. The initial dataset was labelled by a\nsenior trauma surgeon with 18 years of experience, a specialist who has worked speciﬁcally on femur fractures in\nthe past 6 years and was composed by n=2645 images of the entire or half hip bone. The dataset was then reviewed\nusing the included radiological referrals and intraoperative diagnosis. All images with discordant classiﬁcation were\nanalyzed and removed if univocal interpretation by the whole team was not attained.Table.1describes patients’ baseline\ncharacteristics. The initial dataset was labelled by a senior trauma surgeon with 18 years of experience, a specialist who\nhas worked speciﬁcally on femur fractures in the past 6 years and was composed by n=2645 images of the entire or half\nhip bone. The dataset was then reviewed using the included radiological referrals and intraoperative diagnosis. All\nimages with discordant classiﬁcation were analyzed and removed if univocal interpretation by the whole team was not\nattained. The ﬁrst step was the segmentation of the images into left and right hips. Then segmentation of the images into\nleft and right hips was performed. The second step was a cleaning phase, where a total of n=242 images were excluded\nbecause contained prosthesis (n=97), had poor lighting condition (n=47), showed the area around the femur partially\nhidden (n=23) or presented a lateral view (n=72). Moreover, because of the low number of C fractures (n=3) detectable\non pelvic radiographs, this class was excluded. The third step was a cropping phase, where the areas related to the right\nand left femur were selected through a fully-automated cropping method and resized to 224 × 224, considering the fact\nthat some images present only one between the right and left femur. This technique concerned the use of YOLOv3 (You\nOnly Look Once) [32] algorithm for detection of left and right femur. The fourth step was the revision of the YOLOv3\nerrors: n=25 femur was not detected, n=1845 right femur were correctly detected while n=208 wrongly detected as left,\nn=1874 left femur were correctly detected while n=241 wrongly detected as right. After the correction and the manual\ncropping of the not detected images, the ﬁnal dataset was composed by n=2152 left femur and n=2055 right femur,\nwhich have been ﬂipped horizontally. The fractures were then divided in different types and, afterwards, the dataset was\nreviewed by two radiologists from our medical team, to conﬁrm the validity of the ground truth. The ﬁnal number of\nimages was n=4207 manually annotated images divided in different fracture types: 2003 Unbroken femur, 631 type A1,\n329 type A2, 174 type A3, 625 type B1, 339 type B2, 106 type B3. This process is shown in Figure 2 following the\nSTARD 2015 ﬂow diagram [33]. Some real X-Rays for each class taken from our dataset are shown in Figure 3.\n2.3 ViT Conﬁguration\nIn this paper, the ViT proposed in [30] was applied. This architecture focuses on small patches of the image. Each\npatch in the input image is ﬂattened using a linear projection matrix, and a positional embedding is added to it.\nThe Transformer encoder, similarly to the original 2017 version [13], consists of multiple blocks of self-attention,\nnormalization and fully connected layers with residual connections. In each attention block, multiple heads can capture\ndifferent connectivity patterns. The fully connected Multi-Layer Perceptron (MLP, [ 34]) head at the classiﬁcation\noutput provides the desired class prediction. As stated in the original ViT paper, this network typically requires a larger\ndataset than usual, as well as a longer pre-training schedule. For this reason, with only 4207 images would be unfeasible\nto train ViT from scratch. As a ﬁrst solution, a Compact Convolutional Transformer [? ] was applied, a very recent\narchitecture, based on ViT, where the patch extraction phase is substituted by a CNN which took care of the features\nextraction. This solution is proven to usually overcome the big data problem, but, unfortunately, it did not applied\nto this case. For this reason, the four pre-trained ViT architectures were tested. In [30], the proposed conﬁgurations\ndepend on several network parameters (such as the number of neurons of a speciﬁc layer) and patches number. After\nexperimenting with the so-called base-16, base-32, large-16 and large-32 conﬁgurations, the large-16 ViT block was\nselected, which has a multilayer perceptron of 4096 units, 16 heads, 24 layers, a hidden size of 1024 and operates with\nTable 1: Baseline characteristics. Median computed with Interquartile Range (IQR)\n4\nPreprint SUBMITTED TO AI IN MEDICINE\nFigure 2: The STARD 2015 Flow Diagram shows the dataset processing workﬂow\n16 × 16 patches. The comparison between these four conﬁgurations and the CCT are shown for completeness in Table\n2. A dense layer with 4096 neurons was added to this block, with a GELU (Gaussian Error Linear Units) activation\nfollowed by a batch normalization layer and a dropout layer with 0.5 as keep-in parameter. Finally, a Softmax layer\nfor 7-class classiﬁcation was attached. The learning rate was initially set to 1e−4 and reduced by a factor of 0.2 after\n4 epochs of plateau until a minimum of 1e−6. The optimizer used was the Rectiﬁed Adam and the loss function the\ncategorical crossentropy. To cushion the problem of class imbalance, three diverse methods have been tried: the ﬁrst\nassigned different weights to each class during training, where the weights are inversely proportional to the number of\nsamples in the respective class; the other ones applied oversampling or data augmentation, with a rotation range of 10\ndegrees, and both height and width shift from 0.0 to 0.1 fraction of total height or width, in order to obtain the same\nnumber of samples for each class. After testing, oversampling resulted as the most performing choice. The problem\nFigure 3: Some samples of real X-Rays images used for training the neural network after the cleaning and cropping\nphase\n5\nPreprint SUBMITTED TO AI IN MEDICINE\nTable 2: Values of precision, recall and F1-score for the Compact Convolution Transformer and thebase-16, base-32,\nlarge-16, large-32 conﬁgurations. The values are shown with related conﬁdence interval.\nwith data augmentation is the fact that more complex transformations as shearing can not be used, as it may lead to\nthe generation of “fake” fractures. To compare the results with the state of the art, two other approaches were used as\nbaselines. The ﬁrst is an InceptionV3 network, which has been chosen for the results obtained in [6]. The second is\na hierarchical approach, proposed in [6], which consisted in a cascade of three stages: the ﬁrst network recognized\nUnbroken or Broken, the second one classiﬁed the images predicted as Broken by the ﬁrst network as A and B and the\nthird and fourth ones took care of the A and B subgroups.\n2.4 Visualization and Clustering\nFor understanding where the ViT network was focusing during inference, a function was implemented in order to\nvisualize the attention map. Then, unsupervised learning was used to evaluate the ability of the ViT encoder to extract\nfeatures. Three clustering approaches were tested: ﬁrstly, the initial dataset of images resized to 224 × 224 was\nclustered using a Convolutional Autoencoder and the results were used as a baseline. Secondly, the Convolutional\nAutoencoder was substituted by an Autoencoder which took as input a vector of 1024 values, extracted in one case from\nthe InceptionV3 network and in the other from the ViT encoder. In all three cases, the Autoencoder was pre-trained for\n200 epochs. After this, the encoders were extracted from the three architectures and a clustering layer was added at\nthe end. This layer was initialized with the centers found by the kmeans++ function and trained until convergence. It\noutput a vector that represents the probability, calculated with Student’s t-distribution, of the sample belonging to each\ncluster. The clustering performance was measured with accuracy, Normalized Mutual Information (NMI) between the\nground truth and the predicted distribution, where 0 means no mutual information and 1 means a perfect correlation,\nAdjusted Rand Index (ARI), which computes a similarity measure by considering all pairs of samples and counting\npairs that are assigned in the same or different clusters in the predicted and true clustering, and loss.\n2.5 Training, Framework and Evaluation\nFrom the initial dataset, 15% of images for each class where kept apart for testing, resulting in a test set of 91 images of\ntype A1, 94 type A2, 25 type A3, 90 type B1, 49 type B2, 16 type B3, and 282 Unbroken femur, and 15% for validation.\nThe remaining 70% images were used for training. The networks were then trained for 40 epochs using Early Stopping\nwith a patience of 10 epochs. We used Keras [35], an open-source neural-network library written in Python, running\non top of TensorFlow [36], on Windows 10 Pro with NVIDIA Quadro RTX 6000. For each network, the macro and\nweighted accuracy was computed. Then, the performance for single classes was measured using precision, recall and\nF1-score. Performance of the specialists with and without the system was computed using an online survey (Forms,\nMicrosoft Corporation, Redmond USA) and was measured using accuracy. In this case the accuracy was a reliable\nmetric as the dataset used was balanced. Firstly, 11 specialists (7 residents and 4 radiologists) evaluated 150 hips without\nthe help of the neural network. The CAD tool was designed to suggest the fracture classiﬁcation for the proposed image,\nthe conﬁdence level in percentage points and the attention map. This set of images was taken from the test dataset, and\ntherefore not involved in the training process for obtaining comparable results. Fourteen days later, in order to produce\nunbiased results, the evaluation was made again but this time the specialist could consult the prediction of the ViT and\nthe probability that the network assigned to each class.\n6\nPreprint SUBMITTED TO AI IN MEDICINE\n3 Results\n3.1 Baseline Method\nThe aforementioned InceptionV3 model was able to correctly classify 67% of the images and obtained a macro average\naccuracy of 0.52 (CI 0.33-0.72), a precision of 0.57 (CI 0.42-0.72), a recall of 0.53 (CI 0.33-0.72) and a F1-score of\n0.54 (CI 0.36-0.71). The hierarchical network composed by different InceptionV3 network in cascade was able to\ncorrectly classify 61% of the images and obtained a macro average accuracy of 0.41 (CI 0.13-0.68), a precision of\n0.44 (CI 0.21-0.68) , a recall of 0.41 (CI 0.14-0.69) and a F1-score of 0.40 (CI 0.15–0.64). These values are shown in\nTable.3 (a) and (b).\n3.2 ViT\nOur conﬁguration of the ViT was able to correctly classify 83% of the entire test dataset. The macro average accuracy\nobtained was 0.77 (CI 0.62-0.9), while the value of precision, recall and F1-score were 0.77 (CI 0.64-0.90), 0.76 (CI\n0.62-0.91) and 0.77 (CI 0.64–0.89) respectively. These values are shown in Table.3 (c).\n7\nPreprint SUBMITTED TO AI IN MEDICINE\nTable 3: Values of accuracy, precision, recall and F1-score for the two baselines (a) and (b) and for the ViT (c). The\nvalues are shown with related conﬁdence interval. In (c), the improvement given by the ViT compared to the highest\nvalue among the two baselines is shown in parenthesis.\n8\nPreprint SUBMITTED TO AI IN MEDICINE\nTable 4: Values of accuracy, Normalized Mutual Information (NMI), Adjusted Rand Index (ARI) and loss for the three\nclustering approaches.\n3.3 Clustering\nThe results for the three clustering in terms of accuracy, NMI, ARI and loss are shown inTable.4, while a graphical\nrepresentation of confusion matrix and distribution of each approach is shown in Figure 4.\nFigure 4: Confusion matrices and distributions of the clustering given by a Convolutional Autoencoder applied directly\non the images (a) or by an Autoencoder applied to the feature vector extracted from the CNN (b) and extracted from the\nViT Encoder (c). In the right images the cluster labels are shown with the colors presented in the sidebar\n9\nPreprint SUBMITTED TO AI IN MEDICINE\n3.4 CAD System\nThe 7-class evaluation of the type of fracture present in 150 images without the help of ViT, performed by 11 specialists\nand resumed in Table.5, resulted in an average accuracy of 0.58 (CI 0.53 – 0.65) for residents and 0.84 (CI 0.77 – 0.92)\nfor radiologists. Fourteen days later, the same test was performed with the help of ViT, which with this particular set\nof images obtained an accuracy of 0.90 (CI 0.80-0.99). In this case, the accuracy of both residents and radiologists\naugmented to 0.96 (CI 0.92 – 0.99) and 1.00, respectively. The result is an average improvement of 0.29 (CI 0.12 –\n0.37) in accuracy.\nTable 5: Values of accuracy for the 11 specialists who performed the evaluation with and without the CAD (Computer\nAssisted Diagnosis) system. The average values are shown with related Conﬁdence Interval (CI)\n4 Discussion and Conclusion\nThe work introduced in this paper is summarized in Figure 5. Firstly the original images were cropped and ordered\nusing a YOLOv3 algorithm, obtaining a very large dataset of 4207 images divided in 7 classes. Two baselines were\ndeﬁned using InceptionV3 and a hierarchical network which were able to correctly classify 67% and 61% of the test\nsamples, respectively. A ViT based architecture was then applied to correctly classify 83% of the test images. The\naverage value of accuracy, precision, recall and F1-score, resumed in Table.3, improved by a factor of 25%, 20%,\n23% and 23% compared to the best result among the two baselines. The ViT was able to obtain for the ﬁrst time good\nperformance in sub-fracture classiﬁcation. The attention maps in Figure 6 shows how the network focuses on the calcar\nand trochanteric area for the A class, the neck of femur and the cortex of the greater trochanter for the B class, and\nalongside the whole cortex proﬁle of the proximal femur for the Unbroken class. A clustering phase was also used to\ndemonstrate how better the ViT extracts a feature vector compared to the other two approaches, even if it still struggles\nwith some sub-fractures, as shown in Figure 4 (c), where A, B and Unbroken class were correctly clustered but A1, A2,\nA3 and B1, B2, B3 were often mismatched. Nevertheless the accuracy of this clustering was 0.78 compared to 0.22\nand 0.36 of the other two (Table.4). Finally, the evaluation of 7 residents and 4 radiologists, with or without the ViT,\nimproved by a factor of 37% (accuracy: 0.96) and 15% (accuracy: 1.00), respectively. These values seem excessive, but\nit has to be considered the fact that the specialist had access also to the probabilities, returned by ViT, related to each\nclass. This information allowed them to focus much more on the images with uncertain probabilities and to consider\nalso the second class predicted by ViT, in order of probability. On top of that, the best results were achieved through\nthe synergic effect between physicians and the CAD system, resulting even better than the physicians or the algorithm\nalone. As already stated in [37], the performance of algorithms should be complementary to that by doctors rather than\ncomparable or better with respect to them. The majority of the previous work concerning femur fracture classiﬁcation\nfocused on the classiﬁcation between broken and unbroken bones. The clinical signiﬁcance of a CAD system able\nto classify fractures is related to sub-fractures classiﬁcation, often mismatched even by specialist with many year of\nexperience. The two papers which deﬁnes the state of the art in femur sub-fracture classiﬁcation are [28] and [29]. In\nthe ﬁrst, the authors collected 786 anterior-posterior X-ray images together with 459 radiology reports. The dataset\nwas unbalanced, resulting in a very scarce test dataset. For example, just 1, 6, and 8 samples, respectively, were used\nto validate classes B3, B1 and A3. During training, the images were passed through an InceptionV3 encoder which\nextracted a latent representation. This latent representation was then passed to a Fully Connected (FC) classiﬁcation\n10\nPreprint SUBMITTED TO AI IN MEDICINE\nFigure 5: Full pipeline of this work. The images were ﬁrstly passed through a YOLOv3 network which cropped the\nareas related to the left and right femur. A CNN and a hierarchical CNN were then applied to this dataset and the results\nwere used as baselines. Then, a modiﬁed ViT encoder was used for 7-class classiﬁcation and the attention maps were\nanalyzed. Finally, clustering was performed to evaluate the ability of the ViT to extract features\nlayer and to a LSTM based decoder together with textual data from the reports. With this conﬁgurations, the authors\nobtained an average F1-score of 0.50, 0.27 lower than the one obtained by ViT. In addition, the approach proposed\nin this work does not leverage on text annotations, which are usually very hard to collect. In the second paper, the\nauthors used an attention module to automatically locate the proximal femur area, followed by an InceptionV3 network\nfor classiﬁcation. In this case the dataset used was larger, with a total of 1173 X-Ray images, but still unbalanced,\nwith, as the authors also underlined, as little as 15 cases for A3 fractures. For 6 classes classiﬁcation (as the class\nUnbroken was not considered) they obtained an average value of 0.68 for all three metrics used (precision, recall and\nF1-score), compared to the values of 0.77, 0.76 and 0.77, respectively, of ViT. In summary, the results achieved with\nViT outperformed the state of the art with a dataset eight times and four times larger, respectively. This aspect may\nseem a negative side, since it is always better to achieve certain results with as few images as possible. However, given\nthe variety and diversity of fractures’ patterns, a larger dataset could bring to a better generalization. On the other hand,\ntwo main problems have still to be tackled. Firstly, the unbalancing of this dataset was bypassed with oversampling,\nbut a better approach would be to augment the dataset using Generative Adversarial Networks [38] to produce new\nand reliable samples [39]. The problem with GANs is that they might not work well with classes which present a low\nnumber of samples, as, fewer the images available for training, less their ability generalize and create new images\nwill be. For this reason, a solution could be to produce new generic samples, training the architecture with the entire\ndataset, and ask the specialists to classify the generated images. This idea will be discussed in a future paper. Secondly,\neven if in this paper the hierarchical approach yielded very low results, the performance of the ViT could massively\n11\nPreprint SUBMITTED TO AI IN MEDICINE\nFigure 6: Some examples of original images and corresponding attention map for each class\nimprove adapting its architecture to the hierarchical structure of the AO/OTA classiﬁcation. In conclusion, this is the\nﬁrst work where a ViT architecture was applied to femur fracture classiﬁcation. It outperformed the state of the art\napproaches based on CNN. In future work, this method will be improved and extended for even more complex levels of\nsub fractures in the AO/OTA classiﬁcation.\n12\nPreprint SUBMITTED TO AI IN MEDICINE\nReferences\n[1] Anthony D. Woolf and Bruce Pﬂeger. Burden of major musculoskeletal conditions.Bull World Health Organ, 81(9):\n646–656, 2003. ISSN 0042-9686. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2572542/.\n[2] Jean-Yves Reginster and Nansa Burlet. Osteoporosis: a still increasing prevalence. Bone, 38(2 Suppl 1):S4–9,\nFebruary 2006. ISSN 8756-3282. doi:10.1016/j.bone.2005.11.024.\n[3] Martyn Parker and Antony Johansen. Hip fracture. BMJ, 333(7557):27–30, July 2006. ISSN 1756-1833.\ndoi:10.1136/bmj.333.7557.27.\n[4] Journal of Orthopaedic Trauma. Femur. 32:S33–S44, January 2018. ISSN 0890-5339.\ndoi:10.1097/BOT.0000000000001058. URL http://Insights.ovid.com/crossref?an=\n00005131-201801001-00004.\n[5] Matthew W. Kirby and Charles Spritzer. Radiographic Detection of Hip and Pelvic Fractures in the Emer-\ngency Department. American Journal of Roentgenology , 194(4):1054–1060, April 2010. ISSN 0361-803X.\ndoi:10.2214/AJR.09.3295. URL https://www.ajronline.org/doi/full/10.2214/AJR.09.3295. Pub-\nlisher: American Roentgen Ray Society.\n[6] Leonardo Tanzi, Enrico Vezzetti, Rodrigo Moreno, Alessandro Aprato, Andrea Audisio, and Alessandro MassÃ´l.\nHierarchical fracture classiﬁcation of proximal femur X-Ray images using a multistage Deep Learning approach.\nEur J Radiol, 133:109373, December 2020. ISSN 1872-7727. doi:10.1016/j.ejrad.2020.109373.\n[7] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, May 2015. ISSN\n0028-0836, 1476-4687. doi:10.1038/nature14539. URL http://www.nature.com/articles/nature14539.\n[8] Leonardo Tanzi, Pietro Piazzolla, and Enrico Vezzetti. Intraoperative surgery room management: A deep learning\nperspective. The International Journal of Medical Robotics and Computer Assisted Surgery, 16(5):e2136, 2020.\nISSN 1478-596X. doi:https://doi.org/10.1002/rcs.2136. URL https://onlinelibrary.wiley.com/doi/\nabs/10.1002/rcs.2136. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rcs.2136.\n[9] A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. de Mathelin, and N. Padoy. EndoNet: A Deep\nArchitecture for Recognition Tasks on Laparoscopic Videos. IEEE Transactions on Medical Imaging, 36(1):\n86–97, 2017. doi:10.1109/TMI.2016.2593957.\n[10] Leonardo Tanzi, Pietro Piazzolla, Francesco Porpiglia, and Enrico Vezzetti. Real-time deep learning semantic\nsegmentation during intra-operative surgery for 3D augmented reality assistance. Int J CARS, June 2021. ISSN\n1861-6429. doi:10.1007/s11548-021-02432-y. URL https://doi.org/10.1007/s11548-021-02432-y .\n[11] Elena Carlotta Olivetti, Jacopo Ferretti, Giansalvo Cirrincione, Francesca Nonis, Stefano Tornincasa, and Federica\nMarcolin. Deep CNN for 3D Face Recognition. In Caterina Rizzi, Angelo Oreste Andrisano, Francesco Leali,\nFrancesco Gherardini, Fabio Pini, and Alberto Vergnano, editors, Design Tools and Methods in Industrial\nEngineering, pages 665–674, Cham, 2020. Springer International Publishing. ISBN 978-3-030-31154-4.\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classiﬁcation with deep convolutional\nneural networks. Commun. ACM, 60(6):84–90, May 2017. ISSN 00010782. doi:10.1145/3065386. URL\nhttp://dl.acm.org/citation.cfm?doid=3098997.3065386. Number: 6 Reporter: Communications of\nthe ACM.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Å ˛ Aukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, NIPS’17, pages 6000–6010, Red Hook, NY , USA, December 2017. Curran\nAssociates Inc. ISBN 978-1-5108-6096-4.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\ndoi:10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\n[15] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\ngenerative pre-training. 2018.\n[16] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-End Object Detection with Transformers. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-\nMichael Frahm, editors, Computer Vision â ˘A¸ S ECCV 2020, Lecture Notes in Computer Science, pages 213–229,\nCham, 2020. Springer International Publishing. ISBN 978-3-030-58452-8. doi:10.1007/978-3-030-58452-8_13.\n13\nPreprint SUBMITTED TO AI IN MEDICINE\n[17] L. Ye, M. Rochan, Z. Liu, and Y . Wang. Cross-Modal Self-Attention Network for Referring Image Segmentation.\nIn 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 10494–10503,\nLos Alamitos, CA, USA, June 2019. IEEE Computer Society. doi:10.1109/CVPR.2019.01075. URL https:\n//doi.ieeecomputersociety.org/10.1109/CVPR.2019.01075.\n[18] Rohit Girdhar, Joao Joao Carreira, Carl Doersch, and Andrew Zisserman. Video Action Trans-\nformer Network. pages 244–253. IEEE Computer Society, June 2019. ISBN 978-1-72813-293-8.\ndoi:10.1109/CVPR.2019.00033. URL https://www.computer.org/csdl/proceedings-article/cvpr/\n2019/329300a244/1gyrQLoZXws.\n[19] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-Attention Generative Adversarial\nNetworks. arXiv:1805.08318 [cs, stat] , June 2019. URL http://arxiv.org/abs/1805.08318. arXiv:\n1805.08318.\n[20] Leonardo Tanzi, Enrico Vezzetti, Rodrigo Moreno, and Sandro Moos. X-Ray Bone Fracture Classiﬁcation\nUsing Deep Learning: A Baseline for Designing a Reliable Approach. Applied Sciences, 10(4):1507, February\n2020. ISSN 2076-3417. doi:10.3390/app10041507. URL https://www.mdpi.com/2076-3417/10/4/1507.\nNumber: 4 Reporter: Applied Sciences.\n[21] Yu Cao, Hongzhi Wang, Mehdi Moradi, Prasanth Prasanna, and Tanveer F. Syeda-Mahmood. Fracture detection\nin x-ray images through stacked random forests feature fusion. In 2015 IEEE 12th International Symposium on\nBiomedical Imaging (ISBI), pages 801–805, Brooklyn, NY , USA, April 2015. IEEE. ISBN 978-1-4799-2374-8.\ndoi:10.1109/ISBI.2015.7163993. URL http://ieeexplore.ieee.org/document/7163993/.\n[22] Wint War Myint, Hla Myo Tun, and Khin Sandar Tun. Analysis on Detecting of Leg Bone Fracture from\nX-ray Images. IJSRP, 8(9), September 2018. ISSN 2250-3153. doi:10.29322/IJSRP.8.9.2018.p8150. URL\nhttp://www.ijsrp.org/research-paper-0918.php?rp=P817764.\n[23] Robert Lindsey, Aaron Daluiski, Sumit Chopra, Alexander Lachapelle, Michael Mozer, Serge Sicular, Douglas\nHanel, Michael Gardner, Anurag Gupta, Robert Hotchkiss, and Hollis Potter. Deep neural network improves\nfracture detection by clinicians. Proc Natl Acad Sci USA, 115(45):11591–11596, November 2018. ISSN 0027-\n8424, 1091-6490. doi:10.1073/pnas.1806905115. URL http://www.pnas.org/lookup/doi/10.1073/pnas.\n1806905115.\n[24] Jakub Olczak, Niklas Fahlberg, Atsuto Maki, Ali Sharif Razavian, Anthony Jilert, AndrÃl’ Stark, Olof\nSkÃ˝ uldenberg, and Max Gordon. Artiﬁcial intelligence for analyzing orthopedic trauma radiographs: Deep\nlearning algorithmsâ ˘AˇTare they on par with humans for diagnosing fractures? Acta Orthopaedica , 88\n(6):581–586, November 2017. ISSN 1745-3674, 1745-3682. doi:10.1080/17453674.2017.1344459. URL\nhttps://www.tandfonline.com/doi/full/10.1080/17453674.2017.1344459.\n[25] Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang, Kaylie\nZhu, Dillon Laird, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, and Andrew Y . Ng.\nMURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs. arXiv:1712.06957 [physics],\nMay 2018. URL http://arxiv.org/abs/1712.06957. arXiv: 1712.06957.\n[26] Seok Won Chung, Seung Seog Han, Ji Whan Lee, Kyung-Soo Oh, Na Ra Kim, Jong Pil Yoon, Joon Yub Kim,\nSung Hoon Moon, Jieun Kwon, Hyo-Jin Lee, Young-Min Noh, and Youngjun Kim. Automated detection\nand classiﬁcation of the proximal humerus fracture by using deep learning algorithm. Acta Orthopaedica, 89\n(4):468–473, July 2018. ISSN 1745-3674, 1745-3682. doi:10.1080/17453674.2018.1453714. URL https:\n//www.tandfonline.com/doi/full/10.1080/17453674.2018.1453714.\n[27] Amelia JimÃl’nez-SÃ ˛ anchez, Anees Kazi, Shadi Albarqouni, Chlodwig Kirchhoff, Peter Biberthaler, Nassir\nNavab, Diana Mateus, and Sonja Kirchhoff. Towards an Interactive and Interpretable CAD System to Support\nProximal Femur Fracture Classiﬁcation. arXiv:1902.01338 [cs], February 2019. URL http://arxiv.org/\nabs/1902.01338. arXiv: 1902.01338.\n[28] Changhwan Lee, Jongseong Jang, Seunghun Lee, Young Soo Kim, Hang Joon Jo, and Yeesuk Kim. Clas-\nsiﬁcation of femur fracture in pelvic X-ray images using meta-learned deep neural network. Sci Rep, 10(1):\n13694, August 2020. ISSN 2045-2322. doi:10.1038/s41598-020-70660-4. URL https://www.nature.com/\narticles/s41598-020-70660-4 . Bandiera_abtest: a Cc_license_type: cc_by Cg_type: Nature Research\nJournals Number: 1 Primary_atype: Research Publisher: Nature Publishing Group Subject_term: Computer sci-\nence;Diagnosis;Information technology;Orthopaedics Subject_term_id: computer-science;diagnosis;information-\ntechnology;orthopaedics.\n[29] Anees Kazi, Shadi Albarqouni, Amelia Jimenez Sanchez, Sonja Kirchhoff, Peter Biberthaler, Nassir Navab, and\nDiana Mateus. Automatic Classiﬁcation of Proximal Femur Fractures Based on Attention Models. In Qian Wang,\nYinghuan Shi, Heung-Il Suk, and Kenji Suzuki, editors, Machine Learning in Medical Imaging, Lecture Notes\n14\nPreprint SUBMITTED TO AI IN MEDICINE\nin Computer Science, pages 70–78, Cham, 2017. Springer International Publishing. ISBN 978-3-319-67389-9.\ndoi:10.1007/978-3-319-67389-9_9.\n[30] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nImage is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on\nLearning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\n[31] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\nInception Architecture for Computer Vision. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2818–2826, Las Vegas, NV , USA, June 2016. IEEE. ISBN 978-1-4673-8851-1.\ndoi:10.1109/CVPR.2016.308. URL http://ieeexplore.ieee.org/document/7780677/.\n[32] Joseph Redmon and Ali Farhadi. YOLOv3: An Incremental Improvement. arXiv:1804.02767 [cs], April 2018.\nURL http://arxiv.org/abs/1804.02767. arXiv: 1804.02767.\n[33] JÃl’rÃl’mie F Cohen, DaniÃ´nl A Korevaar, Douglas G Altman, David E Bruns, Constantine A Gatsonis, Lotty\nHooft, Les Irwig, Deborah Levine, Johannes B Reitsma, Henrica C W de Vet, and Patrick M M Bossuyt.\nSTARD 2015 guidelines for reporting diagnostic accuracy studies: explanation and elaboration. BMJ Open,\n6(11):e012799, November 2016. ISSN 2044-6055, 2044-6055. doi:10.1136/bmjopen-2016-012799. URL\nhttp://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2016-012799.\n[34] Charu C. Aggarwal. Neural Networks and Deep Learning: A Textbook . Springer International Publishing,\nCham, 2018. ISBN 978-3-319-94462-3 978-3-319-94463-0. doi:10.1007/978-3-319-94463-0. URL http:\n//link.springer.com/10.1007/978-3-319-94463-0 .\n[35] FranÃ ˘gois Chollet and others. Keras. 2015. URL https://keras.io.\n[36] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy\nDavis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael\nIsard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan ManÃl’, Rajat\nMonga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,\nKunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda ViÃl’gas, Oriol Vinyals, Pete Warden,\nMartin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.TensorFlow: Large-Scale Machine Learning\non Heterogeneous Systems. 2015. URL http://tensorflow.org/.\n[37] Kunio Doi. Computer-aided diagnosis in medical imaging: Historical review, current status and future\npotential. Computerized Medical Imaging and Graphics , 31(4):198–211, June 2007. ISSN 0895-6111.\ndoi:10.1016/j.compmedimag.2007.02.002. URL https://www.sciencedirect.com/science/article/\npii/S0895611107000262.\n[38] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative Adversarial Networks. arXiv:1406.2661 [cs, stat], June 2014. URL\nhttp://arxiv.org/abs/1406.2661. arXiv: 1406.2661.\n[39] Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan. Synthetic Data\nAugmentation using GAN for Improved Liver Lesion Classiﬁcation. arXiv:1801.02385 [cs], January 2018. URL\nhttp://arxiv.org/abs/1801.02385. arXiv: 1801.02385.\n15",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7013704180717468
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6647928357124329
    },
    {
      "name": "Computer science",
      "score": 0.6271467208862305
    },
    {
      "name": "Deep learning",
      "score": 0.6097579002380371
    },
    {
      "name": "CAD",
      "score": 0.5745574831962585
    },
    {
      "name": "Machine learning",
      "score": 0.5173060894012451
    },
    {
      "name": "Transformer",
      "score": 0.4686543047428131
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43049055337905884
    },
    {
      "name": "Engineering",
      "score": 0.1088685691356659
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Engineering drawing",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}