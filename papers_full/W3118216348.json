{
    "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    "url": "https://openalex.org/W3118216348",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2618171555",
            "name": "Armen Aghajanyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2125474800",
            "name": "Sonal Gupta",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A334758317",
            "name": "Luke Zettlemoyer",
            "affiliations": [
                "University of Washington",
                "Meta (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3013571468",
        "https://openalex.org/W3022969335",
        "https://openalex.org/W2982272160",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2785533664",
        "https://openalex.org/W3127622310",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3104263050",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2785749378",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2118563516",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2982295985",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2102409316",
        "https://openalex.org/W3037854022",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2913946806"
    ],
    "abstract": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 7319–7328\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7319\nIntrinsic Dimensionality Explains the Effectiveness\nof Language Model Fine-Tuning\nArmen Aghajanyan\nFacebook AI\narmenag@fb.com\nSonal Gupta\nFacebook\nsonalgupta@fb.com\nLuke Zettlemoyer\nFacebook AI\nUniversity of Washington\nlsz@fb.com\nAbstract\nAlthough pretrained language models can be\nﬁne-tuned to produce state-of-the-art results\nfor a very wide range of language understand-\ning tasks, the dynamics of this process are\nnot well understood, especially in the low data\nregime. Why can we use relatively vanilla gra-\ndient descent algorithms (e.g., without strong\nregularization) to tune a model with hundreds\nof millions of parameters on datasets with only\nhundreds or thousands of labeled examples?\nIn this paper, we argue that analyzing ﬁne-\ntuning through the lens of intrinsic dimension\nprovides us with empirical and theoretical intu-\nitions to explain this remarkable phenomenon.\nWe empirically show that common pre-trained\nmodels have a very low intrinsic dimension;\nthere exists a low dimension reparameteriza-\ntion that is as effective for ﬁne-tuning as the\nfull parameter space. For example, by optimiz-\ning only 200 trainable parameters randomly\nprojected back into the full space, we can\ntune a RoBERTa model to achieve 90% of the\nfull parameter performance levels on MRPC.\nFurthermore, we empirically show that pre-\ntraining implicitly minimizes intrinsic dimen-\nsion and, perhaps surprisingly, larger mod-\nels tend to have lower intrinsic dimension af-\nter a ﬁxed number of pre-training updates, at\nleast in part explaining their extreme effective-\nness. Lastly, we connect intrinsic dimensional-\nity with low dimensional task representations\nand compression based generalization bounds\nto provide generalization bounds that are inde-\npendent of the full parameter count.\n1 Introduction\nPre-trained language models (Radford et al., 2019;\nDevlin et al., 2018; Liu et al., 2019; Lewis et al.,\n2019, 2020) provide the defacto initialization for\nmodeling most existing NLP tasks. However, the\nprocess of ﬁne-tuning them on often very small\ntarget task datasets remains somewhat mysterious.\nWhy can we use relatively vanilla gradient descent\nalgorithms (e.g., without strong regularization) to\ntune a model with hundreds of millions of param-\neters on datasets with only hundreds or thousands\nof labeled examples?\nWe propose intrinsic dimensionality as a new\nlens through which ﬁne-tuning can be analyzed\n(Li et al., 2018). An objective function’s intrinsic\ndimensionality describes the minimum dimension\nneeded to solve the optimization problem it de-\nﬁnes to some precision level. In the context of\npre-trained language models, measuring intrinsic\ndimensional will tell us how many free parameters\nare required to closely approximate the optimiza-\ntion problem that is solved while ﬁne-tuning for\neach end task. For example, we will show that 200\nparameters (randomly projected back into the full\nparameter space) are enough to represent the prob-\nlem of tuning a RoBERTa model to within 90%\nof the performance of the full model. More gen-\nerally, we also describe a set of strong empirical\nand theoretical connections between intrinsic di-\nmensionality, number of parameters, pre-training,\nand generalization.\nWe ﬁrst empirically show that standard pre-\ntrained models can learn a large set of NLP tasks\nwith very few parameters and that the process of\npre-training itself implicitly minimizes the intrinsic\ndimension of later tuning for different NLP tasks.\nWe study over a dozen different pre-trained models\nto show that the number of parameters strongly in-\nversely correlates with intrinsic dimensionality, at\nleast in part justifying the extreme effectiveness of\nsuch models. We interpret pre-training as providing\na framework that learns how to compress the aver-\nage NLP task. Finally, we connect intrinsic dimen-\nsional with low dimensional task representations\nand compression-based generalization bounds to\nprovide intrinsic-dimension-based generalization\nbounds independent of the full parameter count,\nfurther justifying why these methods generalize so\nwell in practice across tasks.\n7320\nThe contributions of our paper are the following:\n• We empirically show that common NLP tasks\nwithin the context of pre-trained representa-\ntions have an intrinsic dimension several or-\nders of magnitudes less than the full parame-\nterization.\n• We propose a new interpretation of intrinsic di-\nmension as the downstream ﬁne-tuning task’s\nminimal description length within the frame-\nwork of the pre-trained model. Within this\ninterpretation, we empirically show that the\nprocess of pre-training implicitly optimizes\nthe description length over the average of NLP\ntasks, without having direct access to those\nsame tasks.\n• We measure the intrinsic dimension of a large\nset of recently developed pre-training method,\nand how that larger models tend to have\nsmaller intrinsic dimension.\n• Lastly, we show that compression based gener-\nalization bounds can be applied to our intrinsic\ndimension framework to provide generaliza-\ntion bounds for large pre-trained models in-\ndependent of the pre-trained model parameter\ncount.\n2 Related Work\nCalculating the intrinsic dimension of an objective\nfunction in the context of deep-learning was ﬁrst\nproposed by Li et al. (2018). They analyzed the\nimpact of various architectures on the intrinsic di-\nmensionality of their objective. Our work is a direct\nextension of this approach, focusing on analyzing\npre-trained representations instead.\nThere is a large collection of literature analyzing\npre-trained models from the perspective of capacity.\nFor example, a recent line of work has shown that\npre-trained models such as BERT are redundant\nin their capacity, allowing for signiﬁcant sparsiﬁ-\ncation without much degradation in end metrics\n(Chen et al., 2020; Prasanna et al., 2020; Desai\net al., 2019). Houlsby et al. (2019) showed that ﬁne-\ntuning top layers of pre-trained models is not effec-\ntive and that alternate methods allow ﬁne-tuning\neffectively with a couple of percent of the param-\neters. Furthermore, we can view computing the\nintrinsic dimensionality as a continuous relaxation\nof the sparsiﬁcation problem.\nThere also exist connections between intrinsic\ndimensionality, knowledge distillation, and other\nmodel compression methods. Fundamentally intrin-\nsic dimensionality attempts to ﬁnd the smallest set\nof parameters needed to tune to reach satisfactory\nsolutions, which can be thought of as a sparsiﬁca-\ntion or distillation problem (Hinton et al., 2015;\nChen et al., 2020). Unlike distillation approaches,\nthe approach of intrinsic dimensionality does not\nchange parameter count, sparsity, or architecture\nbut instead looks at the underlying rank of the ob-\njective function (Li et al., 2018). There are also\nconnections between representing multiple tasks\nwithin a pre-trained model and compression which\nwe explore in §5.\nMoreover, standard approaches towards ﬁne-\ntuning seem to have non-trivial effects on the gen-\neralization of pre-trained representations (Agha-\njanyan et al., 2020, 2021). A holistic explanatory\npicture of the successes of ﬁne-tuning has not yet\nbeen painted. A clear understanding of the un-\nderlying mechanisms which lead to the incredible\ngeneralization of ﬁne-tuned pre-trained represen-\ntations is currently missing. Moreover, we still do\nnot understand why various pre-training methodol-\nogy manifests in universally useful representations,\nalthough recent line of works have attempted to\ncover this gap by looking at loss landscapes, and\nthe learned linguistic properties of pre-trained mod-\nels (Hao et al., 2019; Clark et al., 2019a).\n3 Intrinsic Dimensionality of Finetuning\nBackground The intrinsic dimension of an ob-\njective function measures the minimum number\nof parameters needed to reach satisfactory solu-\ntions to the respective objective (Li et al., 2018).\nAlternatively, the intrinsic dimension represents\nthe lowest dimensional subspace in which one can\noptimize the original function to within a certain\nlevel of approximation error. Computing the ex-\nact intrinsic dimensional of the objective function\nis computation intractable; therefore, we resort to\nheuristic methods to calculate an upper bound. Let\nθD = [θ0,θ1,...,θ m] be a set of Dparameters that\nparameterize some model f(·,θ). Instead of opti-\nmizing the empirical loss in the original parame-\nterization (θD), the subspace method ﬁne-tunes the\nmodel via the following re-parameterization in the\nlower-dimensional d-dimensions:\nθD = θD\n0 + P(θd) (1)\n7321\nwhere P : Rd →RD projects from a parameter\nfrom a lower-dimensional dto the higher dimen-\nsional Dand θD\n0 is the original model parameter-\nization. Intuitively, we project using an arbitrary\nrandom projection onto a much smaller space; usu-\nally, a linear projection, we then solve the optimiza-\ntion problem in that smaller subspace. If we reach\na satisfactory solution, we say the dimensionality\nof that subspace is the intrinsic dimension. This\nmethodology was proposed in the seminal paper\nby Li et al. (2018). Concretely Li et al. (2018)\nproposed three different parameteric forms for P;\na random linear dense projection (θdW), random\nlinear sparse projection (θdWsparse) and random lin-\near projection via the Fastfood transform (Le et al.,\n2013).\nWe will primarily use the Fastfood transform,\ndeﬁned as:\nθD = θD\n0 + θdM M = HGΠHB (2)\nThe factorization of M consists of H, a Hadamard\nmatrix, G, a random diagonal matrix with inde-\npendent standard normal entries, B a random di-\nagonal matrix with equal probability ±1 entries,\nand Π a random permutation matrix. Furthermore,\nthe matrix multiplication with a Hadamard ma-\ntrix can be computed in O(Dlog d) via the Fast\nWalsh-Hadamard Transform. Everything except θd\nis ﬁxed; therefore, the optimization problem lies\nonly in d-dimensions.1\nWe use the Fastfood transform due to its compu-\ntational complexity. Speciﬁcally, using Hadamard\nmatrices instead of dense matrices allows us to com-\npute a linear projection signiﬁcantly faster than a\ndense matrix projection. Furthermore, when work-\ning with large models such as RoBERTa, the mem-\nory required to store even a low-dimensional dense\nmatrix to calculate intrinsic dimension is unrea-\nsonable (d= 1000, 330,000,000 ∗1000 ∗4 bytes\n= 1.32 terabytes).\nThe standard method of measuring the intrin-\nsic dimensionality of an objective as proposed by\n(Li et al., 2018) requires searching over various\nd, training using standard SGD over the subspace\nreparameterization θD and selecting the smallest d\nwhich provides us with a satisfactory solution (d90).\n(Li et al., 2018) deﬁned the satisfactory solution as\nbeing 90% of the full training metric. For example,\n1If we place a constraint of M being a binary matrix, we\nrecover the sparsiﬁcation problem; therefore, we can also view\nﬁnding intrinsic dimensionality as a continuous relaxation of\nthe sparsiﬁcation problem.\nif we reach 85% accuracy training a model with all\nof its parameters, the goal is to ﬁnd the smallest d,\nwhich would reach 0.9 ∗85% = 76.5% accuracy;\nwe call this dimension d90.2\nThe way (Li et al., 2018) deﬁne a satisfactory\nsolution reduces the dependence of the dataset size\non the calculation of intrinsic dimension. For a\nsmall dataset, we will generally have worse end\nmetrics; therefore, we have a lower d90 cut-off;\ninversely, a larger dataset will require a more non-\ntrivial d90 cut-off.\nStructure Aware Intrinsic Dimension Due to\nthe large size of pre-trained language models (gen-\nerally in the hundreds of millions of parameters),\nthe only computationally reasonable subspace op-\ntimization method is one that utilizes the Fastfood\ntransform. For example, if we are interested in\nsubspace training with d= 1000 for the RoBERTa-\nLarge model using a dense matrix, we would re-\nquire 1.42 terabytes of memory to store just the\nprojection matrix.\nUnfortunately, the method of ﬁnding the intrinsic\ndimension proposed by (Li et al., 2018) is unaware\nof the layer-wise structure of the function param-\neterized by θ. Existing literature argues that in\nattention-based pre-trained models, individual lay-\ners specialize separately (Clark et al., 2019b); there-\nfore, it is useful to incorporate a notion of structure\nwhen computing d90. We deﬁne Structure-Aware\nIntrinsic Dimension (SAID) as the following\nθD\ni = θD\n0,i + λiP(θd−m)i (3)\nFor mlayers, we trade mparameters from our sub-\nspace parameter θd to allow for layer-wise scal-\ning through jointly learned λ, thus θd becomes\n[θd−m,λ]. This allows the SAID method to focus\na larger capacity of θd−m towards speciﬁc layers\nwhat might carry more relevant information for\nthe task at hand. Conversely, we will refer to the\nlayer unaware method (Equation 2) as the Direct\nIntrinsic Dimension (DID) method.\n4 Intrinsic Dimensionality of Common\nNLP Tasks\n4.1 Sentence Classiﬁcation\nWe ﬁrst empirically calculate the intrinsic dimen-\nsion of various pre-trained models on a set of sen-\ntence prediction tasks from the GLUE Benchmark\n2Initializing θd = 0 we recover the original parameteri-\nzation θD\n0 which in the context of ﬁne-tuning represents the\noriginal weights of the pre-trained model.\n7322\nSAID DID\nModel MRPC QQP MRPC QQP\nBERT-Base 1608 8030 1861 9295\nBERT-Large 1037 1200 2493 1389\nRoBERTa-Base 896 896 1000 1389\nRoBERTa-Large 207 774 322 774\nTable 1: Estimated d90 intrinsic dimension computed\nwith SAID and DID for a set of sentence prediction\ntasks and common pre-trained models.\n(Wang et al., 2018). We focus on analyzing BERT\n(Devlin et al., 2018) and RoBERTa (Liu et al.,\n2019) at both the base and large model sizes.\nWe chose to experiment with MRPC (Dolan and\nBrockett, 2005) and QQP (Iyer et al., 2017) as ref-\nerence examples of small and large tuning datasets.\nMRPC is a binary classiﬁcation task for predict-\ning semantic equivalency for two paraphrases with\nroughly 3700 training samples, while QQP is a\nbinary classiﬁcation task for predicting semantic\nequality of two questions, with roughly 363k sam-\nples. For every dataset and every model, we run\n100 subspace trainings with dranging from 10 to\n10000 on a log scale. For every training run, we do\na small hyperparameter search across four learning\nrates. We initialize every θd to the zero vector to\nallow for our starting point to be the original pre-\ntrained model. Our subspace optimization method\nalso operates over the randomly initialized sentence\nclassiﬁcation head to ensure we have exactly dpa-\nrameters to optimize.\nWe use both the SAID and DID subspace op-\ntimization methods, which we implemented in\nthe Huggingface Transformers library (Wolf et al.,\n2019). We present the results in Figure 1.\n4.2 Analysis\nThe ﬁrst takeaway is the incredible low dimension-\nality of viable solutions. With RoBERTa-Large,\nwe can reach 90% of the full ﬁne-tuning solution\nof MRPC using roughly 200 parameters and 800\nparameters for QQP (Table 1). Recall that our ap-\nproximation of intrinsic dimension is necessarily\ncrude by using random projections and restricting\nthem to the use of Fastfood transform; therefore, it\nis likely that the true intrinsic dimension is much\nlower.\nFurthermore, RoBERTa consistently outper-\nforms BERT across various subspace dimensions d\nwhile having more parameters. We leave a more in-\ndepth analysis of model parameter size on intrinsic\ndimensionality to a later section (§5.2).\nLastly, we see that adding a notion of structure in\nthe computation of intrinsic dimension is beneﬁcial\nwith the SAID method consistently improving over\nthe structure unaware DID method.\n5 Intrinsic Dimension, Pre-Training, and\nGeneralization Gap\nOne interpretation of the intrinsic parameter vector\nis that it encodes the task at hand with respect to the\noriginal pre-trained representations. Therefore, we\ncan interpret das the minimal description length of\nthe task within the framework dictated by the pre-\ntrained representations (Hinton and Zemel, 1993).\nUnder this interpretation of intrinsic dimensional-\nity, we hypothesize that pre-training is implicitly\nlowering the intrinsic dimensionality of the average\nNLP task, and therefore compressing the minimal\ndescription length of those same tasks.\nWhat do we more precisely mean by intrinsic\nparameter encoding a task within the framework\nprovided by the pre-trained representations? Tra-\nditionally, a ﬁnetuned model (e.g. for a classiﬁca-\ntion tasks) simply consists of a classiﬁcation head\ng, parameterized by wg applied to ﬁne-tuned rep-\nresentations f, parameterized by wf per sample\nx. Therefore, to fully describe a task, we need\nto pack together parameterizations and weights\n{g,f,w g,wf}. This model description is com-\npletely decoupled from the original weights of the\npre-trained representation wf0 , therefore to repre-\nsent n classiﬁcation tasks, we need to maintain\nn{wg,wf}; additionally, the task representation\nis incredibly high dimensional. Conversely, ﬁne-\ntuning utilizing SAID in d-dimensions requires\nstoring only θd per task, a single random seed used\nto generate M and the original pre-trained weights\nwf0 . Therefore, we can represent arbitrary NLP\ntasks within a single pre-trained model framework\nwith d+ 1 parameters.\nFor example, in the last section, we represented\nMRPC with roughly 200 parameters, which trans-\nlates to needing less than a kilobyte of data to en-\ncode a complex natural language task within the\nframework provided by RoBERTa.\nWe hypothesize that the better the pre-trained\nmodels are, the fewer bits (description length) are\nneeded to represent the average NLP task, as we\nwill demonstrate empirically in the next section.\n7323\n102 103 104 105\nd\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy\nMRPC Intrinsic Dimension\nModel\nBERT-Base\nBERT-Large\nRoBERTa-Base\nRoBERTa-Large\n102 103 104 105\nd\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy\nQQP Intrinsic Dimension\nModel\nBERT-Base\nBERT-Large\nRoBERTa-Base\nRoBERTa-Large\nFigure 1: Evaluation accuracy on two datasets and four models across a range of dimensionsdfor the DID method.\nThe horizontal lines in each ﬁgure represent the 90% solution of the respective full model.\n5.1 Pre-Training Intrinsic Dimension\nTrajectory\nTo verify our hypothesis of pre-training optimizing\nintrinsic dimension, we retrain a RoBERTa-Base\nfrom scratch and measure the intrinsic dimension of\nvarious NLP tasks at different training checkpoints,\nusing the SAID method. We completely replicate\nthe setting as described by Liu et al. (2019) apart\nfrom only training for a total of 200k steps (in-\nstead of 500k) with half the batch size (1k). To\ncalculate the intrinsic dimension more efﬁciently,\nwe reuse the best learning rates discovered in Sec-\ntion 4 for d < 10000 and use a ﬁxed learning\nrate for anything else. To ﬁnd d90 we do a binary\nsearch across dper each checkpoint, with a mini-\nmum dof 100 and a maximum of 4 million. The\n“full solution” that we use when decidingd90 cut-\noff is computed by ﬁne-tuning the checkpointed\nmodel in the standard way. We compute SAID on\nsix datasets; MRPC, QQP, Yelp Polarity (Zhang\net al., 2015), SST-2 (Socher et al., 2013), MNLI\n(Williams et al., 2018) and ANLI using all rounds\nof data (Nie et al., 2019). Although we focus on\nbench-marking sentence classiﬁcation tasks the se-\nlected set of tasks contains variety, from sentiment\nclassiﬁcation (Yelp Polarity, SST-2) to Natural Lan-\nguage Inference (MNLI, ANLI) to question similar-\nity (QQP).\nWe present our results in Figure 2. The in-\ntrinsic dimensionality of RoBERTa-Base mono-\ntonically decreases as we continue pre-training.\nWe do not explicitly optimize for intrinsic dimen-\nsionality, speciﬁcally during pre-training (the lan-\nguage model does not have access to downstream\ndatasets!), but none-the-less the intrinsic dimension\nof these downstream tasks continues to decrease.\nMore so, tasks that are easier to solve consis-\ntently show lower intrinsic dimensionality across\nall checkpoints, for example, Yelp Polarityvs. the\nnotoriously tough ANLI dataset. The correlation\nbetween challenging tasks for RoBERTa and their\nlarge intrinsic dimension hints at a connection be-\ntween generalization and intrinsic dimension. We\nwill discuss generalization further in Section §5.3.\nGiven our task representation interpretation of\nintrinsic dimensionality, we argue that the large\nscale training of Masked Language Models (MLM)\nlearns generic and distributed enough representa-\ntions to facilitate downstream learning of highly\ncompressed task representations. Furthermore, we\nargue for another perspective of pre-training learn-\ning representations that form a compression frame-\nwork with respect to various NLP tasks.\n5.2 Parameter Count and Intrinsic\nDimension\nWe also measure the relationships between the pa-\nrameter count of arbitrary pre-trained models and\n7324\n40000 60000 80000 100000 120000 140000 160000 180000 200000\nUpdates\n103\n104\n105\n106\nd90\nRoBERTa Pre-Training Intrinsic Dimension Trajectory\nDataset\nMRPC\nQQP\nYelp\nSST-2\nMNLI\nANLI (R1+R2+R3)\nFigure 2: Every 10k updates of RoBERTa-Base that we trained from scratch, we compute d90 for six datasets;\nMRPC, QQP, Yelp Polarity, SST-2, MNLI, and ANLI. If we were unable to compute ad90 for a speciﬁc checkpoint,\nwe do not plot the point, hence some datasets start at later points. Unable to compute means either we could not\nﬁne-tune the full checkpoint to accuracy above majority class or stabilize SAID training.\nthe intrinsic dimension of downstream NLP tasks.\nThe optimal experiment to run would be to ﬁx the\npre-training method, e.g., MLM RoBERTa style,\nvary the architecture size from small to very big,\nand compute the intrinsic dimension of a group of\ntasks at every size of the model. Unfortunately,\nsuch an experiment is computationally infeasible\ndue to the need to train many RoBERTa models.\nInstead, we do an empirical study of many ex-\nisting pre-trained models, regardless of the pre-\ntraining method. We show that the trend is strong\nenough to overcome differences in training method-\nology. We select the following models: BERT\n(Devlin et al., 2018), RoBERTa (Liu et al., 2019),\nBART (Lewis et al., 2019), Electra (Clark et al.,\n2020), Albert (Lan et al., 2019), XLNet (Yang et al.,\n2019), T5 (Raffel et al., 2019), and XLM-R (Con-\nneau et al., 2019). Furthermore, we selected var-\nious sizes of these models, as available publicly\nwithin the HuggingFace Transformers library (Wolf\net al., 2019).\nWe use the MRPC dataset and compute intrinsic\ndimension for every pre-trained model utilizing\nthe same binary search methodology mentioned in\nthe previous section with additional small hyper-\nparameter searches across learning rate (due to the\nwide range of learning rates needed by various\nmodels).\nWe present our results in Figure 3. There is\na strong general trend that as the number of pa-\nrameters increases, the intrinsic dimension of ﬁne-\ntuning on MRPC decreases. We ran this experiment\non other datasets to ensure that this is not an data\nartifact. Our experiments showed the same trend;\nwe refer to the Appendix for all trends per dataset.\nWithin the same window of number of parame-\nters, the pre-training methodology becomes more\nimportant. For example, in the regime of 108 pa-\nrameters, RoBERTa pre-training dominates sim-\nilar sized pre-training methods. However, there\ndoes not seem to be a method that can overcome\nthe limitations induced by the number of parame-\nters. Interpreting these results through the lens of\nlearning a compression framework for NLP tasks\nis straightforward; the more parameters we have in\nthe model, the less we need to represent a task.\n5.3 Generalization Bounds through Intrinsic\nDimension\nWe have shown strong empirical evidence connect-\ning pre-training, ﬁne-tuning, and intrinsic dimen-\nsionality. However, we have yet to argue the con-\nnection between intrinsic dimensionality and gen-\neralization. Given that we have seen pre-training\nminimize intrinsic dimension, we hypothesize that\ngeneralization improves as the intrinsic dimension\ndecreases.\nTo do so, we will empirically experiment with\nthe connections between d90 and evaluation set per-\nformance by looking at various checkpoints from\nour RoBERTa experiments in Section§5.1. We also\nplot the relative generalization gap (delta between\ntrain time performance and test time performance).\nIn Figure 4 we plot the evaluation accuracy’s\nachieved by our pre-training experiment in Sec-\ntion §5.1. A lower intrinsic dimension is strongly\ncorrelated with better evaluation performance. Ad-\nditionally we are interested in measuring relative\ngeneralization gap ( acctrain−acceval\n1−acceval\n) across intrin-\nsic dimension. We select the training accuracy that\nprovides us with the best evaluation metrics when\ncomputing this ﬁgure.\n7325\n108 109\nNumber of Parameters\n101\n102\n103\n104\n105\nd90\n BERT-Base\n BERT-L RoBERTa-B\n RoBERTa-L\n XLM-R-B\n XLM-R\n Electra-B\n XLNet-B\n XLNet-L\n T5-Small\n T5-L\n BART-B\n BART-L\n Albert-B  Albert-L\n Albert-XL\n Albert-XXL\n T5-3B\nFigure 3: We calculate the intrinsic dimension for a large set of pre-trained models using the SAID method on the\nMRPC dataset.\n103 104 105 106\nd90\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Eval Accuracy\nRoBERTa Pre-Training Generalization Study\nDataset\nMRPC\nQQP\nYelp\nSST-2\nMNLI\nANLI (R1+R2+R3)\nFigure 4: The evaluation accuracy of six datasets across various intrinsic dimensionalities. There is a strong general\ntrend that pre-trained models that are able to attain lower intrinsic dimensions generalize better.\nWe present our results in Figure 5. Lower intrin-\nsic dimension once again correlates strongly with a\nsmaller relative generalization gap. If we interpret\nthe intrinsic dimension as a measure of complexity,\nwe expect the generalization gap to decrease with\nintrinsic dimension.\n5.4 Generalization Bounds\nBy applying standard compression based general-\nization bounds, we can provide theoretical backing\nto the empirical connection between intrinsic di-\nmension and generalization (Arora et al., 2018).\nConsider the following deﬁnition of multi-class\nclassiﬁcation loss with an optional margin over our\nsupervised dataset D.\nLγ(f) = P(x,y)∼D\n[\nf(x)[y] ≤γ+ max\nj̸=y\nf(x)[j]\n]\nWhen γ = 0, L0 recovers the standard classiﬁca-\ntion loss. Furthermore, Let ˆLγ(f) be an unbiased\nempirical estimate of the margin loss.\nTheorem 1. Let f be a function which is parame-\nterized by θD as described in Equation 1 with a to-\ntal of dtrainable intrinsic parameters on a dataset\nwith msamples. Then with a high probability, we\ncan state the following asymptotic generalization\nbound\nL0(f) ≤ ˆL0(f) + O\n(√\nd\nm\n)\n(4)\nProof. We defer the proof Section §A.1 in the\nAppendix. We note that this is an extension of\nthe well-known compression based generalization\nbound (Arora et al., 2018).\nThis generalization bound is independent of the\nunderlying parameter count (D) of the pre-trained\nmodel but depends on the ability to compress the\ndownstream task (d). Moreover, given that our pre-\nvious section shows larger models compress better,\nour bounds are aligned with general intuition and\nrecent empirical evidence that larger pre-trained\nmodels generalize better. Explicitly, these bounds\nonly apply to pre-trained methods trained with the\nintrinsic dimension subspace method; research has\nyet to show that standard SGD optimizes in this\nlow dimensional space (although experimentally,\n7326\n103 104 105 106\nd90\n5.0%\n10.0%\n15.0%\n20.0%\n25.0%Relative Generalization Gap\nRoBERTa Pre-Training Generalization Study\nDataset\nMRPC\nQQP\nYelp\nSST-2\nMNLI\nANLI (R1+R2+R3)\nFigure 5: The intrinsic dimension and the respective relative generalization gap across a set of varied tasks.\nthis seems to be conﬁrmed). We leave the theoreti-\ncal contribution of showing SGD optimizes in this\nspace, possibly resembling intrinsic subspace, for\nfuture work.\nWe want to highlight that generalization is not\nnecessarily measured by the pre-trained model’s\nparameter count or measure of complexity but the\npre-trained model’s ability to facilitate the com-\npression of downstream tasks. In some sense, if\nwe want to compress downstream tasks better, we\nmust expect pre-trained representations to have a\nconsiderable measure of complexity.\n6 Conclusion\nIn conclusion, we proposed viewing the vari-\nous phenomena surrounding ﬁne-tuning and pre-\ntraining through the lens of intrinsic dimension-\nality. We empirically showed that common natu-\nral language tasks could be learned with very few\nparameters, sometimes in the order of hundreds,\nwhen utilizing pre-trained representations. We pro-\nvided an interpretation of pre-training as providing\na compression framework for minimizing the av-\nerage description length of natural language tasks\nand showed that pre-training implicitly minimizes\nthis average description length.\nWe continued by doing an empirical study of ex-\nisting pre-training methods and their respective in-\ntrinsic dimension, uncovering the phenomena that\nintrinsic dimensionality decreases as we increase\nthe number of pre-trained representation parame-\nters. This phenomenon provides some intuitions\nto the trend of growing pre-trained representations.\nWe connected intrinsic dimensionality with gener-\nalization by ﬁrst showing that pre-trained models\nwith lower intrinsic dimensions across various tasks\nachieve higher evaluation accuracies and lower rel-\native generalization gaps. Furthermore, we explain\nthese empirical results by applying well-known\ngeneralization bounds to the intrinsic dimension to\nget generalization bounds that grow on the order of\nthe intrinsic dimension, not the parameter count.\nIntrinsic dimensionality is a useful tool for un-\nderstanding the complex behavior of large models.\nWe hope that future work will make explicit theo-\nretical connections between SGD and optimizing\nthe intrinsic dimension as well as explain exactly\nwhy pre-training methods optimize the intrinsic\ndimensionailty of tasks before not seen.\nReferences\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivas-\ntava, Xilun Chen, Luke Zettlemoyer, and Sonal\nGupta. 2021. Muppet: Massive multi-task rep-\nresentations with pre-ﬁnetuning. arXiv preprint\narXiv:2101.11038.\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2020. Better ﬁne-tuning by reducing representa-\ntional collapse. arXiv preprint arXiv:2008.03156.\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and\nYi Zhang. 2018. Stronger generalization bounds\nfor deep nets via a compression approach. arXiv\npreprint arXiv:1802.05296.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained bert networks. Advances in Neural Informa-\ntion Processing Systems, 33.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019a. What does bert\nlook at? an analysis of bert’s attention. arXiv\npreprint arXiv:1906.04341.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019b. What does bert\nlook at? an analysis of bert’s attention. arXiv\npreprint arXiv:1906.04341.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\n7327\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nShrey Desai, Hongyuan Zhan, and Ahmed Aly. 2019.\nEvaluating lottery tickets under distributional shifts.\narXiv preprint arXiv:1910.12708.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of bert.\narXiv preprint arXiv:1908.05620.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nGeoffrey E Hinton and Richard Zemel. 1993. Autoen-\ncoders, minimum description length and helmholtz\nfree energy. Advances in neural information pro-\ncessing systems, 6:3–10.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\narXiv preprint arXiv:1902.00751.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First quora dataset release: Question pairs.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nQuoc Le, Tam ´as Sarl ´os, and Alex Smola. 2013.\nFastfood-approximating kernel expansions in loglin-\near time. In Proceedings of the international confer-\nence on machine learning, volume 85.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020. Pre-training via paraphrasing.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Ja-\nson Yosinski. 2018. Measuring the intrinsic di-\nmension of objective landscapes. arXiv preprint\narXiv:1804.08838.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2019. Ad-\nversarial nli: A new benchmark for natural language\nunderstanding. arXiv preprint arXiv:1910.14599.\nSai Prasanna, Anna Rogers, and Anna Rumshisky.\n2020. When bert plays the lottery, all tickets are\nwinning. arXiv preprint arXiv:2005.00561.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, et al. 2019. Huggingface’s transformers: State-\nof-the-art natural language processing. ArXiv, pages\narXiv–1910.\n7328\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassiﬁcation. arXiv:1509.01626 [cs].\nA Appendix\nA.1 Proofs\nArora et al. (2018) deﬁne (γ,S) compressible us-\ning helper string sas the following.\nDeﬁnition 1. (γ,S) compressible using helper\nstring s\nSuppose GA,s = {gθ,s|θ∈A} is a class of clas-\nsiﬁers indexed by trainable parameters A and ﬁxed\nstrings s. A classiﬁer f is (γ,S)-compressible with\nrespect to GAusing helper string s if there exists\nθ∈A such that for any x∈S, we have for all y\n|f(x)[y] −gθ,s(x)[y]|≤ γ (5)\nRemark 1. If we parameterize f(x; θ) via the in-\ntrinsic dimension approach as deﬁned in Equa-\ntion 1, then f is compressible losslessly using a\nhelper string consisting of the random seed used to\ngenerate the static random projection weights and\nthe initial pre-trained representationθD\n0 . Therefore\nwe say f parameterized by either DID or SAID is\n(0,S) compressible.\nTheorem 2.1 in (Arora et al., 2018) states given\na compression consisting of r discrete states we\nachieve the following generalization bound.\nL0(f) ≤ ˆLγ(f) + O\n(√\ndlog r\nm\n)\n(6)\nWe can trivially represent our parameters θd in a\ndiscrete fashion through discretization, as was done\nin Arora et al. (2018), and the number of states is\ndependent on the level of quantization but is static\nonce chosen (FP32 vs. FP16).\nWe then connect the fact that models trained in\nlow dimensional subspace using SAID/DID meth-\nods are (0, S)-compressible to derive the ﬁnal\nasymptotic bound.\nL0(f) ≤ ˆL0(f) + O\n(√\nd\nm\n)\n(7)"
}