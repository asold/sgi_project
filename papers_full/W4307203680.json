{
  "title": "Using Large Language Models to Enhance Programming Error Messages",
  "url": "https://openalex.org/W4307203680",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227959803",
      "name": "Leinonen, Juho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227959804",
      "name": "Hellas, Arto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227959802",
      "name": "Sarsa, Sami",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4307204193",
      "name": "Reeves, Brent",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3172309270",
      "name": "Denny, Paul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3166080996",
      "name": "Prather, James",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4307204196",
      "name": "Becker, Brett A.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2747427841",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2523566863",
    "https://openalex.org/W4283705032",
    "https://openalex.org/W2604953184",
    "https://openalex.org/W2605202003",
    "https://openalex.org/W2885271276",
    "https://openalex.org/W2916351062",
    "https://openalex.org/W2050390685",
    "https://openalex.org/W2079166885",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W4291476001",
    "https://openalex.org/W3000240292",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3163358064",
    "https://openalex.org/W3184791513",
    "https://openalex.org/W3033595121",
    "https://openalex.org/W2799855324",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W2915219867",
    "https://openalex.org/W3008479641",
    "https://openalex.org/W2884648946",
    "https://openalex.org/W3138942564",
    "https://openalex.org/W4283205649",
    "https://openalex.org/W2617813225",
    "https://openalex.org/W4306294724",
    "https://openalex.org/W1965093370",
    "https://openalex.org/W2791302163",
    "https://openalex.org/W2157148654",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2905087861"
  ],
  "abstract": "A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix the error. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.",
  "full_text": "arXiv:2210.11630v1  [cs.HC]  20 Oct 2022\nUsing Large Language Models to Enhance\nProgramming Error Messages\nJuho Leinonen\nAalto University\nEspoo, Finland\njuho.2.leinonen@aalto.ﬁ\nArto Hellas\nAalto University\nEspoo, Finland\narto.hellas@aalto.ﬁ\nSami Sarsa\nAalto University\nEspoo, Finland\nsami.sarsa@aalto.ﬁ\nBrent Reeves\nAbilene Christian University\nAbilene, Texas, USA\nbrent.reeves@acu.edu\nPaul Denny\nThe University of Auckland\nAuckland, New Zealand\npaul@cs.auckland.ac.nz\nJames Prather\nAbilene Christian University\nAbilene, Texas, USA\njames.prather@acu.edu\nBrett A. Becker\nUniversity College Dublin\nDublin, Ireland\nbrett.becker@ucd.ie\nABSTRACT\nA key part of learning to program is learning to understand pr o-\ngramming error messages. They can be hard to interpret and id en-\ntifying the cause of errors can be time-consuming. One facto r in\nthis challenge is that the messages are typically intended f or an au-\ndience that already knows how to program, or even for program -\nming environments that then use the information to highligh t ar-\neas in code. Researchers have been working on making these er -\nrors more novice friendly since the 1960s, however progress has\nbeen slow. The present work contributes to this stream of res earch\nby using large language models to enhance programming error\nmessages with explanations of the errors and suggestions on how\nto ﬁx the error. Large language models can be used to create us e-\nful and novice-friendly enhancements to programming error mes-\nsages that sometimes surpass the original programming erro r mes-\nsages in interpretability and actionability. These result s provide\nfurther evidence of the beneﬁts of large language models for com-\nputing educators, highlighting their use in areas known to b e chal-\nlenging for students. We further discuss the beneﬁts and dow n-\nsides of large language models and highlight future streams of re-\nsearch for enhancing programming error messages.\nCCS CONCEPTS\n• Social and professional topics → Computing education; • Com-\nputing methodologies → Natural language generation .\nKEYWORDS\nAI; Codex; compiler error messages; large language models; pro-\ngramming error messages; syntax error messages\n1 INTRODUCTION\nProgramming Error Messages (PEMs) can be notoriously diﬃcu lt\nto decipher, especially for novices [31], possibly to the ex tent that\nthey contribute ot the perception that programming is overl y chal-\nlenging [5]. Eye tracking studies reveal that novices read e rror mes-\nsages and spend a substantial amount of programming time try ing\nto understand them [4]. Instructors report that they spend a con-\nsiderable amount of time helping novices with these often cr yptic\nmessages [14, 29, 30, 34]. It is also known that error message pre-\nsentation aﬀects novice programming behaviour [21]. For ov er six\ndecades, researchers have attempted to improve these messa ges,\nand still there is a call for more work on the matter [6]. Some r ecent\nattempts have been made to put error messages into more natur al\nlanguage by focusing on an increase in readability [7, 16]. T his has\nbeen shown to improve student understanding of error messag es\nand the number of successful code corrections [15]. While it is clear\nthat increasing the readability of PEMs is helpful to novice s, doing\nso at scale, and across languages, remains a challenge.\nVery recent work on using large language models in computing\neducation have already produced promising results. One stu dy re-\nported that Codex – built on top of GPT-3 (see Section 2.2) – co uld\nsolve introductory programming problems, and ranked Codex in\nthe top quartile when compared to a cohort of actual students in a\nlarge introductory programming course [17]. Tools like Cod ex are\nable to generate new programming assignments [33] and code e x-\nplanations [27] when provided examples. Such tools demonst rate\nthe impressive interpretive power of very recent large lang uage\nmodels that may have the potential to improve the readabilit y of\ninput text. In this paper, we investigate whether large lang uage\nmodels can be utilized to parse non-compiling code and the pr o-\ngramming errors generated from that code to output PEMs that are\nmore readable than those generated by the compiler/interpr eter.\nRQ1 How well can Codex explain diﬀerent error messages?\nRQ2 What is the quality of the code ﬁx suggestions that Codex\ngenerates?\n2 BACKGROUND\n2.1 Programming Error Messages\nProgramming error messages (PEMs) encompass syntax error m es-\nsages, compiler error messages, and other diagnostic messa ges that\nare produced by compilers or interpreters indicating that t he input\ncode violates the speciﬁcation of a language [8]. Researche rs and\nLeinonen, Hellas, Sarsa, Reeves, Denny, Prather, and Becker\ninstructors have reported PEMs to be a diﬃculty for students since\nat least 1965 [32]. More than ﬁfty years later, PEMs are still a bar-\nrier to progress for those learning to program [6], and this h as led\nto various eﬀorts for improve their usability.\nOne such avenue of work has involved intercepting messages\nbetween the compiler and the user and altering their wording or\npresentation. One of the many known issues with error messag es\ngenerated by compilers and interpreters is poor readabilit y due to\nfactors such as poor use of vocabulary, strange sentence str ucture,\nand the use of jargon [16]. Thus, a large body of work around so -\ncalled ‘enhanced compiler error messages’ has emerged [8]. Diﬀer-\nent approaches to message wording have been reported by vari ous\nauthors, including Barik [3], Becker [9], Denny [13], Kohn [ 22],\nPettit [29], Prather [31], and Karkare [2]. However, althou gh some\nstudies have shown positive eﬀects of rewording messages fo r novices [15?\n], in general the evidence for the eﬀectiveness of enhanced c om-\npiler error messages is not overwhelming. One of the limitat ions\nof prior work in this area is the manual eﬀort that is required to\ngenerate message rewordings and a lack of clear guidance for ad-\ndressing core issues such as readability [16].\nArtiﬁcial intelligence and machine learning approaches ha ve\nbeen used for ﬁnding and repairing errors in programs [1, 18, 19]\nbut only very fundamental approaches have been applied to re -\nsearching PEMs [10]. To our knowledge, no prior work has ex-\nplored the use of large language models for improving PEMs.\n2.2 Large Language Models\nLarge Language Models (LLMs), particularly pre-trained tr ansformer\nmodels, have rapidly become the core technologies of natura l lan-\nguage processing [24]. One such model is OpenAI GPT-3 (third -\ngeneration Generative Pre-trained Transformer) [11]. GPT -3 can\ntranslate between natural languages, compose poetry in the style\nof human poets, generate convincing essays, and more. GPT-3 also\npowers several other tools such as OpenAI Codex which is esse n-\ntially a GPT-3 model that has also been ﬁne-tuned with more th an\n50 million repositories representing the majority of Pytho n code\navailable on GitHub totalling 159 GB of source code [12]. Cod ex is\navailable via the OpenAI API (beta.openai.com) and also pow ers\ntools such as GitHub Copilot (copilot.github.com).\nGiven the recent emergence of these models, little is yet kno wn\nabout the impact they are likely to have on the computing educ a-\ntion landscape. In this context, the few evaluations conduc ted to\ndate have focused on the accuracy of such models for solving t yp-\nical introductory programming problems and on the potentia l for\nthe models to generate learning resources. Early work by Fin nie-\nAnsley et al. assessed the accuracy of Codex by presenting it with\ntypical CS1-type problems, and comparing its performance a gainst\nthat of students. They found that it outperformed most stude nts,\nand was capable of generating a variety of correct solutions to any\ngiven problem [17]. Sarsa et al. investigated the content ge neration\ncapabilities of Codex, by providing input examples as promp ts and\nusing it to generate novel programming problems and code exp la-\nnations [33]. They found that most of the problems generated by\nCodex were novel and sensible, and that the generated code ex pla-\nnations were generally correct and thorough.\nGiven their capability for generating output of human-like qual-\nity from contextual inputs, such as code explanations from c ode,\nthere is potential in applying large language models to the p rob-\nlem of enhancing PEMs.\n3 METHODOLOGY\n3.1 Error Messages and Programs\nFor the present study, we collected Python error messages th at had\nbeen reported as the most unreadable in [16] and [7]. These er ror\nmessages were as follows:\n(1) can’t assign to function call\n(2) invalid token\n(3) illegal target for annotation\n(4) unindent does no match any outer indentation level\n(5) positional argument follows keyword argument\n(6) unexpected EOF while parsing\n(7) EOL while scanning string literal\n(8) EOF while scanning triple-quoted string literal\n(9) (unicodeerror) ‘unicodeescape’ codec can’t decode byt es\nTo control whether the complexity of the program that result s\nin a given error message aﬀects the ability of large language mod-\nels to create useful explanations of the message, we constru cted\nthree example programs that generated each error message. T he\nﬁrst program was very simple, often only a few lines long. The\nsecond incorporated the usage of strings and functions. The third\nincluded the use of libraries (e.g., the PyGame game library , pan-\ndas, scikit-learn) and was more complex. To create the same e rror\nmessages as in the works by [16] and [7], we used Python versio n\n3.6.\n3.2 Generating Programming Error Messages\nProgramming error messages were generated using the Codex m odel\nthat was most recent and performant at the time of analysis, w hich\nwas the code-davinci-002 -model. As the utility of large language\nmodels depends on the used prompts (see e.g., [26]), it is imp or-\ntant to do “prompt engineering” where the performance of diﬀ er-\nent types of prompts is evaluated [26]. We evaluated a number of\nprompts to identify a version that seemed to provide useful e xpla-\nnations. We tried ﬁve diﬀerent prompt messages:\n1. Plain English explanation of why does running the above\ncode cause an error and how to ﬁx the problem\n2. Plain English explanation of why running the above code\ncauses the above error in the output and instructions on how\nto ﬁx the problem\n3. Explanation of why running the above code causes the above\nerror and instructions on how to ﬁx the problem\n4. Why does the code result in an error message? How can the\ncode be ﬁxed?\n5. Why does the above code cause the above error message in\nthe output? How can the code be ﬁxed?\nWe generated explanations with all ﬁve prompts and checked\nwhich version led to the fewest empty responses from Codex. T he\nnumber of empty responses was 4, 6, 7, 16 and 27 out of 81 gen-\nerated explanations respectively for the prompts 1 to 5 abov e. We\nUsing Large Language Models to Enhance Programming Error Messages\nchose the ﬁrst for the analysis as it generated the fewest emp ty re-\nsponses. The structure of the prompt given to the large langu age\nmodel can be seen in the Codex Examples provided later in this\narticle.\nFor each error message (9 error messages) and each program\nleading to an error message (3 programs), we generated three code\nexplanations, one with Codex temperature parameter set to 0 , and\ntwo with temperature set to 0.7. We chose these values as 0 is\nthe minimum for the parameter and leads to least randomness, i.e.\nmost deterministic outputs. The value of 0.7 is the default v alue for\nthe parameter and leads to more random (or “creative”) respo nses,\nand is less deterministic, which is also why we generated two ex-\nplanations for the value of 0.7. This led to a total of 9 × 3 × 3 = 81\nunique combinations of programming error message, program cat-\negory, and temperature value, which we subsequently evalua ted.\n3.3 Analysis\nWe qualitatively analyzed the LLM-produced PEMs. The evalu a-\ntion was performed by two researchers, both of whom have expe ri-\nence from teaching introductory programming. For the evalu ation,\nwe considered the following aspects of the generated PEMs.\n(1) Comprehensible: was the generated content intelligible (i.e.\nproper English, not nonsensical)\n(2) Unnecessary content : did the generated explanation contain\nunnecessary content (e.g., repeating content, comprehens i-\nble but irrelevant content)\n(3) Has explanation: did the content produced by the LLM con-\ntain an explanation of the programming error message\n(4) Explanation correct : did the content produced by the LLM\ncontain a correct explanation of the programming error mes-\nsage\n(5) Has ﬁx : did the generated explanation contain actions or\nsteps that one should take to ﬁx the error\n(6) Fix correct : did the content produced by the LLM contain\ncorrect actions or steps that one should take to ﬁx the error\n(7) Improvement over the original : did the explanation provide\nadded value (from a novice programmer standpoint) when\ncompared to the original programming error message\nThe researchers ﬁrst had a brief discussion to ensure a share d\nunderstanding of the above aspects and jointly evaluated th ree ex-\namples. After the discussion and initial joint evaluation, they sep-\narately analyzed the full set of generated explanations. Fo r each\naspect, the researchers chose either “yes” or “no”. For eval uation,\nthe researchers also had access to the original error messag e as well\nas the program that produced the error message, and consider ed\nalso these when evaluating the LLM generated explanations. To\nexamine the validity of the approach, we calculated inter-r ater re-\nliability between the researchers using Cohen’s kappa. The kappa\nvalue was 0.83 over all the analyzed aspects, indicating alm ost per-\nfect agreement [23].\nTo answer both of our research questions, we report the perce nt-\nage of “yes” answers for the questions outlined above separa tely\nfor each diﬀerent programming error message and separately for\neach combination of program category and temperature value . The\nproportion of “yes” answers is calculated out of the full set of 162\ndata points: 2 raters, each with 81 distinct ratings for the u nique\ncombinations of programming error message (n = 9), program ( n\n= 3), and Codex output (n = 3).\n4 RESULTS\nTable 1 shows the results of the analysis separately for each error\nmessage. Each cell of the table presents the percentage of “y es” an-\nswers to the evaluation question (see Section 3.3 for the que stions)\nfor each of the nine error messages. The cells in the bottom ro w\nof the table show the percentage of “yes” answers across all e rror\nmessages for the evaluation question indicated by the colum n.\nIn general, most error message explanations created by Code x\nwere comprehensible (percentage of “yes” ranging from 67% t o\n100%). A few of the created explanations contained unnecess ary\ncontent such as repeated sentences, extra question marks, e tc –\nthe percentage ranging from 11% for “unexpected EOF while pa rs-\ning” to 56% for “EOF while scanning triple-quoted string lit eral”\nand “(unicodeerror) ‘unicodeescape’ codec can’t decode by tes”.\nIn most cases, Codex successfully created an explanation of the\nerror message (67% to 100% of the time depending on error mes-\nsage), although there were considerable diﬀerences betwee n error\nmessages on whether the explanation was correct. The range o f\ncorrect explanations ranged from 11% for “unexpected EOF wh ile\nparsing” to 83% for “can’t assign to function call”.\nRegarding Codex’s ability to create actionable ﬁxes based o n the\nfaulty source code and the programming error message, we fou nd\nthat in the majority of cases, Codex provided a ﬁx in the gener ated\nexplanation (44% to 89% of cases). However, the ﬁx was correc t\nonly 33% of the time, ranging from 17% of the time for “EOL whil e\nscanning string literal” to 56% for “(unicodeerror) ‘unico deescape’\ncodec can’t decode bytes”.\nAltogether, the evaluators considered that the Codex-crea ted\ncontent, i.e. the explanation of the error message and the pr oposed\nﬁx, were an improvement over the original error message in sl ightly\nover half of the cases (54%). There were some diﬀerences betw een\ndiﬀerent error messages: the content was an improvement onl y\n22% of the time for the “unexpected EOF while parsing” error m es-\nsage, while it was considered an improvement in 78% of the cas es\nfor “can’t assign to function call” and “invalid token”.\nTable 2 shows the results of the analysis separately for diﬀe rent\ncombinations of program category and temperature value. Fr om\nthe table, it is evident that for the task of explaining PEMs a nd cre-\nating suggestions for ﬁxes to the source code that produced t hose\nerrors, using a temperature value of 0 resulted in considera bly bet-\nter outputs, which holds for all three program categories. F or ex-\nample, the output was considered an improvement in over 70% o f\nthe cases with a temperature value of 0, while only up to 50% of\nthe cases with a temperature value of 0.7.\nRegarding diﬀerences between program categories, we can ob -\nserve that Codex seems to perform slightly worse with the pro -\ngrams in the “function with strings” category. However, the varia-\ntions between program categories are not as noticeable as th e dif-\nferences between diﬀerent temperature values or diﬀerent e rror\nmessages.\nLeinonen, Hellas, Sarsa, Reeves, Denny, Prather, and Becker\nRQ1 RQ2\nError message Comprehensible Unnecessary content Has explanation Expla nation correct Improvement Has ﬁx Fix correct\ncan’t assign to function call 100% 17% 94% 83% 78% 72% 28%\ninvalid token 100% 39% 89% 50% 78% 83% 44%\nillegal target for annotation 67% 22% 67% 33% 33% 50% 28%\nunindent does not match any outer indentation level 100% 39% 100% 56% 56% 67% 28%\npositional argument follows keyword argument 89% 22% 89% 61% 56% 78% 39%\nunexpected EOF while parsing 67% 11% 67% 11% 22% 44% 22%\nEOL while scanning string literal 89% 28% 89% 22% 50% 67% 17%\nEOF while scanning triple-quoted string literal 89% 56% 78% 44% 44% 89% 33%\n(unicodeerror) ‘unicodeescape’ codec can’t decode bytes 89% 56% 83% 72% 67% 78% 56%\nAverage over all error messages 88% 32% 84% 48% 54% 70% 33%\nTable 1: Error message analysis for each research question. The cells show the percentage of “yes” answers out of all (“ye s”\nand “no”) answers for the analysis.\nRQ1 RQ2\nProgram category Temperature Comprehensible Unnecessary content Has explanation Expla nation correct Improvement Has ﬁx Fix correct\nSimple 0.0 100% 6% 100% 67% 72% 78% 44%\nFunction with strings 0.0 100% 22% 100% 56% 72% 78% 33%\nLibrary 0.0 100% 28% 100% 78% 78% 72% 44%\nSimple 0.7 83% 31% 78% 47% 42% 64% 31%\nFunction with strings 0.7 89% 42% 86% 36% 39% 75% 25%\nLibrary 0.7 72% 44% 64% 33% 50% 61% 31%\nTable 2: Eﬀect of temperature and program category on Codex p erformance in the task.\n5 DISCUSSION\n5.1 Are Error Message Explanations Useful?\nOur results suggest that using large language models to expl ain\nprogramming error messages (PEMs) is feasible and shows pro mise.\nOverall, the explanation was considered an improvement ove r the\noriginal programming error message in over half of the cases . If\nwe only consider the results from using temperature value 0, which\nwere overall better, over 70% of the Codex outputs were consi dered\nan improvement over the original programming error message .\nThe results are more sobering when it comes to using large lan -\nguage models to generate correct ﬁxes. Although 70% include d a\nﬁx, when a ﬁx was included it was correct only under half of the\ntime (47%). While the ﬁxes created with the temperature valu e of\n0 were better on average compared to those created with a valu e\nof 0.7, they were still correct in only around half of the case s –\n42%, 56%, and 61% of the cases for functions with strings, sim ple\nprograms, and library related programs respectively.\nWe propose that the generated content could be useful to stu-\ndents if it were delivered so that it is clear that the content is\nAI-generated and might not be correct. And even with this ini tial\nexploratory setup, we found some outputs (two examples seen in\nCodex Example 1 and Codex Example 2) that we consider good\nenough to be shown to students without modiﬁcations.\n5.2 Common Pitfalls and Ways Around Them\nTwo examples of outputs where both the explanation and sug-\ngested ﬁx generated by Codex were incorrect are shown in Code x\nExamples 3 and 4. Comparing these incorrect outputs with the cor-\nrect outputs in Examples 1 and 2, we observe that the messages\nCodex Example 1 (input in black, output in red)\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Code\ndef check_password(password, input):\nreturn password == input\ninput = /quotedbl.Varhunter2/quotedbl.Var\nif check_password(/quotedbl.Vars3cr37/quotedbl.Var, input):\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Output\nFile /quotedbl.Varmain.py/quotedbl.Var, line 5\n^\nSyntaxError: unexpected EOF while parsing\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Plain English explanation of why does running\nthe above code cause an error and how to fix the\nproblem\n↩→\n↩→\nThe error is caused because the block of code is\nexpecting another line of code after the colon.↩→\nTo fix the issue, I would add another line of code\nafter the colon.↩→\nseem similarly conﬁdent in their tone, which could potentia lly mis-\nlead students. In both of the examples where the output is inc orrect,\nCodex suggests that the issue is related to indentation. As n ovices\noften struggle with indentation [22, 25], these incorrect s ugges-\ntions could exacerbate this by potentially misleading stud ents and\neven introduce misconceptions related to correct indentat ion.\nIn general, we observed a few common pitfalls that Codex seem ed\nto often struggle with: (1) source code clearly missing a par t of\nthe content (resulting in “unexpected EOF while parsing”, s ee e.g.,\nCodex Example 3), (2) incorrectly capitalized control stat ements\nUsing Large Language Models to Enhance Programming Error Messages\nCodex Example 2 (input in black, output in red)\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Code\nusers_dir_path = /quotedbl.VarC:\\Users/quotedbl.Var\nprint(/quotedbl.VarUsers directory is/quotedbl.Var, users_dir_path)\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Output\nFile /quotedbl.Varmain.py/quotedbl.Var, line 1\nusers_dir_path = /quotedbl.VarC:\\Users/quotedbl.Var\n^\nSyntaxError: (unicode error) /grave.ts1unicodeescape/quotesingle.Var codec\ncan/quotesingle.Vart decode bytes in position 2-3: truncated\n\\UXXXXXXXX escape\n↩→\n↩→\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Plain English explanation of why does running\nthe above code cause an error and how to fix the\nproblem\n↩→\n↩→\nThe error is caused by the backslash character in\nthe string.↩→\nThe backslash character is used to escape characters\nthat would otherwise have a special meaning,\nsuch as newline, backslash itself, or the quote\ncharacter.\n↩→\n↩→\n↩→\nSince the backslash is used for escaping, if you want\nto use it in your string, you need to escape it.↩→\nYou can do that by using a double backslash:\nCodex Example 3 (input in black, output in red)\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Code\nimport pygame\ndisplay = pygame.display.set_mode((640, 400))\npygame.display.set_caption(/quotedbl.VarAwesome game!/quotedbl.Var)\ndef main():\nwhile True:\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Output\nFile /quotedbl.Varmain.py/quotedbl.Var, line 8\n^\nSyntaxError: unexpected EOF while parsing\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Plain English explanation of why does running\nthe above code cause an error and how to fix the\nproblem\n↩→\n↩→\n# The above code is not indented enough to be able to\nbe run by Python.↩→\n# Add more spaces or tabs to the start of each line\nof code to be able to run it.↩→\n(resulting in “illegal target for annotation”, see e.g., Co dex Exam-\nple 4), and (3) missing quotation marks (resulting in either “EOL\nwhile scanning string literal” or “EOF while scanning tripl e-quoted\nstring literal”).\nFor the ﬁrst case, Codex would often suggest to ﬁx the indenta -\ntion of the program, even though the problem was that the impl e-\nmentation was far from complete (as in Codex Example 3). Simi lar\nsuggestions for ﬁxing the indentation were observed for the sec-\nond case as well, even though the problem is in the capitaliza tion.\nCodex Example 4 (input in black, output in red)\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Code\ndef check_password(password, input):\nIf (input == /quotedbl.Vars3cr37/quotedbl.Var): print(/quotedbl.VarYou are in!/quotedbl.Var)\nElse: print(/quotedbl.VarWrong password!/quotedbl.Var)\ninput = /quotedbl.Varhunter2/quotedbl.Var\ncheck_password(/quotedbl.Vars3cr37/quotedbl.Var, input)\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Output\nFile /quotedbl.Varmain.py/quotedbl.Var, line 2\nIf (input == /quotedbl.Vars3cr37/quotedbl.Var): print(/quotedbl.VarYou are in!/quotedbl.Var)\n^\nSyntaxError: illegal target for annotation\n/quotedbl.Var/quotedbl.Var/quotedbl.Var Plain English explanation of why does running\nthe above code cause an error and how to fix the\nproblem\n↩→\n↩→\nThe error is caused by the fact that the code is not\nindented properly. The code should be indented\nby 4 spaces.\n↩→\n↩→\nThis can be seen in Codex Example 4, where the issue is that the\nif-statement is capitalized, but the message claims the iss ue is with\nindentation. For the third case, Codex was often unable to co rrectly\nidentify whether the quotation mark was missing from the beg in-\nning or the end of the string, and sometimes suggested that th e\nissue is related to parentheses instead of missing quotatio n marks.\nIndeed, the program category “function with strings” had th e low-\nest scores overall (see Table 2).\nWhile it was relatively rare, we did observe some outputs tha t\nwere not just incorrect, but even contradictory and confusi ng. In\none case, Codex seems to have focused too much on the “Plain En g-\nlish” portion of the input and started generating irrelevan t content\nrelated to “looking for a plain English explanation”. To add to the\nconfusion, the generated output actually does include a cor rect ex-\nplanation of the problem – “You need to end your string with th ree\nsingle quotes at the end of your string to make it work. ”, but t he\noutput also states that “this is not a correct explanation”.\nAs there were common pitfalls and clear diﬀerences between e x-\nplanation quality, we see one stream of future work in using a two-\ntiered approach for creating explanations. Codex could be r elied\nupon in cases where it is known that it likely performs well, w hile\nin other cases other means could be exercised. One possibili ty is\nusing LLMs to pre-generate explanations of common error mes -\nsages that the instructor could validate (essentially, a “h uman-in-\nthe-loop” approach). Another possibility would be the use o f learn-\nersourcing, where students could ask for help from their pee rs;\nclassic approaches such as discussion forums would also wor k, al-\nthough the response times would be lower when compared to the\nnear-instantaneous feedback from Codex.\n5.3 Explanations and Context\nWhen considering the usefulness of Codex-generated explan ations,\nthey need to be interpreted and evaluated in context. First, the orig-\ninal error messages might be more useful for more experience d\nstudents who have learned to interpret them. The importance of\ncontext was present also in some of the disagreements of the t wo\nLeinonen, Hellas, Sarsa, Reeves, Denny, Prather, and Becker\nresearchers who independently evaluated the error message s; for\nexample, one of the researchers at times considered the erro r mes-\nsage as an improvement if it pointed the students to the corre ct\ndirection, even if the explanation by itself would be faulty .\nThe utility of these explanations also depends on whether st u-\ndents understand the implications of the suggestions. Prio r research\ninto LLMs has shown that when they are used to facilitate the c re-\nation of source code, they may lead students down debugging r ab-\nbit holes [35] or even introduce security ﬂaws [28]. We also s ee\nthe potential for other types of LLM problems. For example, w hat\nif the problem is not with the source code, but an issue with th e\nuser environment – here, a student could ask for help to ﬁx an i s-\nsue, convincing the LLM that an issue exists, and going down a\nrabbit hole when looking for a solution [35].\nDespite the shortcomings, we see the potential of using LLMs\nas a scaﬀold when learning to program and when learning to in-\nterpret error messages. However, as with any instructional scaf-\nfolding, the scaﬀolding should be dismantled at some point [ 20],\nand students must eventually learn to understand the origin al er-\nror messages.\n5.4 Limitations\nThere are limitations to our study, which we outline here. Fi rstly,\nwe used Python 3.6 in the analysis similar to prior work [16]. On\none hand, this allowed us to focus on error messages from the l it-\nerature that had been found to be confusing to students. On th e\nother hand, we acknowledge that newer versions of Python hav e\nincluded improvements to some of the error messages we analy zed.\nFor example, some of the code snippets we used that resulted i n\nan “invalid token” error would have resulted into a “SyntaxE rror:\nleading zeros in decimal integer literals are not permitted ; use an\n0o preﬁx for octal integers” with newer Python versions. We c on-\nsider the latter to be easier to understand for novice progra mmers.\nRegarding the code snippets used in the analysis, they were c re-\nated by the authors and were not student code. It is possible t hat\nthe performance of Codex in explaining error messages for st udent\ncode would be diﬀerent. In our future work, we are interested in\nstudying the error message explanations with student progr ams\nand with student evaluators. In addition, most of the source codes\nwere relatively short. The performance of large language mo dels\nin explaining error messages might be aﬀected by the length o r the\ncomplexity of source code, which future work should examine in\ngreater detail. Similarly, our code snippets only included singular\nerrors – future work could analyze how well large language mo d-\nels can explain error messages when the source code that lead s to\nthose messages contains multiple issues.\nWhen prompting Codex to generate an explanation of the error\nmessage and a ﬁx to the program, we asked for both the explana-\ntion and the ﬁx with a single prompt (“Plain English explanat ion\nof why does running the above code cause an error and how to ﬁx\nthe problem”). Performance could have increased had we aske d for\nthese separately. In addition, we did not give any examples o f good\nerror message explanations and ﬁxes to the code in the prompt –\ni.e. we relied on “zero shot learning” [26]. Prior work has fo und\nthat giving even just a few examples (i.e. “few shot learning ”) can\ndrastically improve the performance of large language mode ls [11].\n6 CONCLUSION\nWe used large language models to try improve programming er-\nror messages (PEMs). We collected Python error messages tha t\nhad been reported as most unreadable in prior work [7, 16] and\ngenerated code examples that produced these error messages . We\nconducted prompt engineering using OpenAI Codex to identif y\nprompts that would produce explanations of the PEMs and acti on-\nable ﬁxes that could be applied to the code examples to ﬁx the\nerror. We evaluated the explanations and ﬁxes created to exa m-\nine whether they have utility in introductory programming c lass-\nrooms. To summarize, we answer our research questions as fol -\nlows.\nRQ1: How well can Codex explain diﬀerent error messages? Over-\nall, the explanations created by Codex were quite comprehen sible\n(88%). Codex produced an output with an explanation to 84% of\nthe provided codes and error messages, but only about half (5 7%)\nof these explanations were deemed correct (48% of all inputs ).\nRQ2: What is the quality of the code ﬁx suggestions that Codex\ngenerates? Although 70% of the outputs had a proposed ﬁx, a lit-\ntle less than half (47%) of those were deemed correct (33% of a ll\ninputs).\nWhile the above results are aggregated over diﬀerent PEMs, p ro-\ngram categories, and Codex temperature values, we found cas es\nwhere Codex seems to perform better. For example, we noticed\nthat the results were better across the board when using the t em-\nperature value of 0. Similarly, we found that there were cert ain\ncases where Codex was more likely to provide faulty explanat ions\nand suggest ﬁxes that are incorrect, and highlighted a poten tial\nway around this by having a two-step system that would look in to\nthe error message and the complexity of the source code befor e\ndeciding whether to use LLMs or other more traditional suppo rt\nmechanisms.\nThe key implications of this work are that programming error\nmessage explanations and suggested ﬁxes generated by LLMs a re\nnot yet ready for production use in introductory programmin g\nclasses, as there are risks that students may interpret pote ntially\nfaulty LLM outputs as coming from an authority, and end up at-\ntempting to ﬁx their programs in ways that do not actually hel p. At\nthe same time, our results show that LLMs could be a useful too l for\nimproving PEMs, although additional eﬀort needs to be taken both\nwhen using LLMs to enhance the error messages and when com-\ning up with ways to produce high-quality enhancements. Enha nc-\ning programming error messages could help students in debug ging\ntheir programs as traditional error messages are often cryp tic and\nhard to understand for novice programmers [15, 16].\nThe present results were obtained with the code-davinci-002\nmodel of OpenAI Codex, which was the most recent and perfor-\nmant Codex model at the time of the study. As LLMs improve\nover time, these results create a baseline that future model per-\nformance can be compared to. Future work should look in more\ndepth into prompt engineering, for example by considering i nclud-\ning the problem statement and perhaps a sample solution into the\ninput, as well as look into applying and evaluating the enhan ced\nprogramming error messages in classroom settings.\nUsing Large Language Models to Enhance Programming Error Messages\nREFERENCES\n[1] Touﬁque Ahmed, Noah Rose Ledesma, and Premkumar Devanbu . 2021. SYNFIX:\nAutomatically Fixing Syntax Errors using Compiler Diagnos tics. arXiv preprint\narXiv:2104.14671 (2021).\n[2] Umair Z. Ahmed, Pawan Kumar, Amey Karkare, Purushottam K ar, Sumit Gul-\nwani, and A.; et al Ahmed, U.; Kumar, P.; Karkare. 2018. Compi lation Er-\nror Repair: For the Student Programs, From the Student Progr ams. In ICSE-\nSEET 2018 : 2018 ACM/IEEE 40th International Conference on S oftware Engi-\nneering : Software Engineering Education and Training : pro ceedings : 30 May\n- 1 June 2018, Gothenburg, Sweden. ACM Press, New York, NY,USA, 78–87.\nhttps://doi.org/10.1145/3183377.3183383\n[3] Titus Barik. 2018. Error Messages as Rational Reconstructions .\nPh. D. Dissertation. North Carolina State University, Rale igh.\nhttps://repository.lib.ncsu.edu/handle/1840.20/35439\n[4] Titus Barik, Justin Smith, Kevin Lubick, Elisabeth Holm es, Jing Feng, Emerson\nMurphy-Hill, and Chris Parnin. 2017. Do Developers Read Com piler Error Mes-\nsages?. In Proceedings of the 39th International Conference on Softwar e Engineer-\ning (Buenos Aires, Argentina) (ICSE ’17) . IEEE Press, Piscataway, NJ, USA, 575–\n585. https://doi.org/10.1109/ICSE.2017.59\n[5] Brett A. Becker. 2021. What Does Saying That ‘Programmin g is Hard’\nReally Say, and About Whom? Commun. ACM 64, 8 (jul 2021), 27–29.\nhttps://doi.org/10.1145/3469115\n[6] Brett A. Becker, Paul Denny, Raymond Pettit, Durell Bouc hard, Dennis J. Bou-\nvier, Brian Harrington, Amir Kamil, Amey Karkare, Chris McD onald, Peter-\nMichael Osera, Janice L. Pearce, and James Prather. 2019. Co mpiler Error Mes-\nsages Considered Unhelpful: The Landscape of Text-Based Pr ogramming Error\nMessage Research. In Proceedings of the Working Group Reports on Innovation\nand Technology in Computer Science Education (Aberdeen, Scotland Uk) (ITiCSE-\nWGR ’19). Association for Computing Machinery, New York, NY,USA, 17 7–210.\nhttps://doi.org/10.1145/3344429.3372508\n[7] Brett A. Becker, Paul Denny, James Prather, Raymond Pett it, Robert Nix, and\nCatherine Mooney. 2021. Towards Assessing the Readability of Programming\nError Messages. In Australasian Computing Education Conference (Virtual, SA,\nAustralia) (ACE ’21). Association for Computing Machinery, New York, NY,USA,\n181–188. https://doi.org/10.1145/3441636.3442320\n[8] Brett A. Becker, Graham Glanville, Ricardo Iwashima, Cl aire McDonnell, Kyle\nGoslin, and Catherine Mooney. 2016. Eﬀective Compiler Erro r Message En-\nhancement for Novice Programming Students. Computer Science Education 26,\n2-3 (2016), 148–175. https://doi.org/10.1080/08993408. 2016.1225464\n[9] Brett A. Becker, Kyle Goslin, and Graham Glanville. 2018 . The Eﬀects of En-\nhanced Compiler Error Messages on a Syntax Error Debugging T est. In Proceed-\nings of the 49th ACM Technical Symposium on Computer Science Education (Balti-\nmore, Maryland, USA) (SIGCSE ’18). Association for Computing Machinery, New\nYork, NY,USA, 640–645. https://doi.org/10.1145/3159450 .3159461\n[10] Brett A Becker and Catherine Mooney. 2016. Categorizin g Compiler e Error Mes-\nsages with Principal Component Analysis. In 12th China-Europe International\nSymposium on Software Engineering Education (CEISEE 2016) , Shenyang, China,\n28-29 May 2016 .\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Giri sh Sastry, Amanda\nAskell, et al. 2020. Language Models are Few-shot Learners. Advances in neural\ninformation processing systems 33 (2020), 1877–1901.\n[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henri que Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating Large Language Model s Trained on Code.\narXiv preprint arXiv:2107.03374 (2021).\n[13] Paul Denny, Andrew Luxton-Reilly, and Dave Carpenter. 2014. Enhancing Syn-\ntax Error Messages Appears Ineﬀectual. In Proceedings of the 19th Conference\non Innovation and Technology in Computer Science Education (Uppsala, Sweden)\n(ITiCSE ’14) . Association for Computing Machinery, New York, NY,USA, 27 3–\n278. https://doi.org/10.1145/2591708.2591748\n[14] Paul Denny, Andrew Luxton-Reilly, Ewan Tempero, and Ja cob Hendrickx. 2011.\nUnderstanding the Syntax Barrier for Novices. In Proceedings of the 16th Annual\nJoint Conference on Innovation and Technology in Computer S cience Education\n(Darmstadt, Germany) (ITiCSE ’11). Association for Computing Machinery, New\nYork, NY,USA, 208–212. https://doi.org/10.1145/1999747 .1999807\n[15] Paul Denny, James Prather, and Brett A Becker. 2020. Err or Message Readability\nand Novice Debugging Performance. In Proceedings of the 2020 ACM Conference\non Innovation and Technology in Computer Science Education . 480–486.\n[16] Paul Denny, James Prather, Brett A. Becker, Catherine M ooney, John Homer,\nZachary C Albrecht, and Garrett B. Powell. 2021. On Designin g Programming Er-\nror Messages for Novices: Readability and Its Constituent F actors. In Proceedings\nof the 2021 CHI Conference on Human Factors in Computing Syst ems (Yokohama,\nJapan) (CHI ’21). Association for Computing Machinery, New York, NY,USA, Ar -\nticle 55, 15 pages. https://doi.org/10.1145/3411764.344 5696\n[17] James Finnie-Ansley, Paul Denny, Brett A Becker, Andre w Luxton-Reilly, and\nJames Prather. 2022. The Robots Are Coming: Exploring the Im plications of\nOpenAI Codex on Introductory Programming. In Australasian Computing Edu-\ncation Conference . 10–19.\n[18] Rahul Gupta, Aditya Kanade, and Shirish Shevade. 2019. Deep Reinforcement\nLearning for Syntactic Error Repair in Student Programs. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , Vol. 33. 930–937.\n[19] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shev ade. 2017. Deepﬁx:\nFixing common C Language Errors by Deep Learning. In Thirty-First AAAI con-\nference on artiﬁcial intelligence .\n[20] Slava Kalyuga. 2009. The Expertise Reversal Eﬀect. In Managing cognitive load\nin adaptive multimedia learning . IGI Global, 58–80.\n[21] Ioannis Karvelas, Annie Li, and Brett A. Becker. 2020. T he Eﬀects of Compilation\nMechanisms and Error Message Presentation on Novice Progra mmer Behavior.\nIn Proceedings of the 51st ACM Technical Symposium on Computer S cience Edu-\ncation (Portland, OR, USA) (SIGCSE ’20). Association for Computing Machinery,\nNew York, NY,USA, 759–765. https://doi.org/10.1145/3328 778.3366882\n[22] Tobias Kohn. 2019. The Error Behind The Message: Findin g the Cause\nof Error Messages in Python. In Proceedings of the 50th ACM Technical\nSymposium on Computer Science Education (Minneapolis, MN, USA) (SIGCSE\n’19). Association for Computing Machinery, New York, NY,USA, 52 4–530.\nhttps://doi.org/10.1145/3287324.3287381\n[23] J Richard Landis and Gary G Koch. 1977. The Measurement o f Observer Agree-\nment for Categorical Data. biometrics (1977), 159–174.\n[24] Hang Li. 2022. Language Models: Past, Present, and Futu re. Commun. ACM 65,\n7 (jun 2022), 56–63. https://doi.org/10.1145/3490443\n[25] David Liu and Andrew Petersen. 2019. Static Analyses in Python Programming\nCourses. In Proceedings of the 50th ACM Technical Symposium on Computer S ci-\nence Education . 666–671.\n[26] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi roaki Hayashi, and\nGraham Neubig. 2021. Pre-train, Prompt, and Predict: A Syst ematic Sur-\nvey of Prompting Methods in Natural Language Processing. arXiv preprint\narXiv:2107.13586 (2021).\n[27] Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstei n, Erin Ross, and Zi-\nheng Huang. 2022. Generating Diverse Code Explanations usi ng the GPT-3\nLarge Language Model. In Proceedings of the 2022 ACM Conference on Interna-\ntional Computing Education Research-Volume 2 . 37–39.\n[28] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan D olan-Gavitt, and\nRamesh Karri. 2022. Asleep at the Keyboard? Assessing the Se curity of GitHub\nCopilot’s Code Contributions. In 2022 IEEE Symposium on Security and Privacy\n(SP). IEEE, 754–768.\n[29] Raymond S. Pettit, John Homer, and Roger Gee. 2017. Do En hanced Com-\npiler Error Messages Help Students? Results Inconclusive. . In Proceedings of the\n2017 ACM SIGCSE Technical Symposium on Computer Science Edu cation (Seattle,\nWashington, USA) (SIGCSE ’17) . Association for Computing Machinery, New\nYork, NY,USA, 465–470. https://doi.org/10.1145/3017680 .3017768\n[30] James Prather, Raymond Pettit, Kayla McMurry, Alani Pe ters, John Homer, and\nMaxine Cohen. 2018. Metacognitive Diﬃculties Faced by Novi ce programmers\nin Automated Assessment Tools. In Proceedings of the 2018 ACM Conference on\nInternational Computing Education Research . 41–50.\n[31] James Prather, Raymond Pettit, Kayla Holcomb McMurry, Alani Peters, John\nHomer, Nevan Simone, and Maxine Cohen. 2017. On Novices’ Int eraction with\nCompiler Error Messages: A Human Factors Approach. In Proceedings of the\n2017 ACM Conference on International Computing Education R esearch (Tacoma,\nWashington, USA) (ICER ’17). Association for Computing Machinery, New York,\nNY,USA, 74–82. https://doi.org/10.1145/3105726.310616 9\n[32] Saul Rosen, Robert A. Spurgeon, and Joel K. Donnelly. 19 65. PUFFT—The Purdue\nUniversity Fast FORTRAN Translator. Commun. ACM 8, 11 (nov 1965), 661–666.\nhttps://doi.org/10.1145/365660.365671\n[33] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. 2022. Automatic Gener-\nation of Programming Exercises and Code Explanations Using Large Language\nModels. In Proceedings of the 2022 ACM Conference on International Comp uting\nEducation Research V. 1 . 27–43.\n[34] Andreas Steﬁk and Susanna Siebert. 2013. An Empirical I nvestigation into Pro-\ngramming Language Syntax. ACM Transactions on Computing Education 13, 4\n(2013), 1–40. https://doi.org/10.1145/2534973\n[35] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassma n. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generatio n Tools Powered by\nLarge Language Models. In CHI Conference on Human Factors in Computing Sys-\ntems Extended Abstracts . 1–7.",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8287001252174377
    },
    {
      "name": "Computer science",
      "score": 0.8209172487258911
    },
    {
      "name": "Programming paradigm",
      "score": 0.5313340425491333
    },
    {
      "name": "Key (lock)",
      "score": 0.4864077568054199
    },
    {
      "name": "Language model",
      "score": 0.4359420835971832
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3909105956554413
    },
    {
      "name": "Programming language",
      "score": 0.363472580909729
    },
    {
      "name": "Computer security",
      "score": 0.08629569411277771
    }
  ]
}