{
    "title": "N-gram Language Modeling using Recurrent Neural Network Estimation",
    "url": "https://openalex.org/W2605133364",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4293488235",
            "name": "Chelba, Ciprian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2275313196",
            "name": "Norouzi, Mohammad",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746056815",
            "name": "Bengio, Samy",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2146502635",
        "https://openalex.org/W10704533",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W1934041838"
    ],
    "abstract": "We investigate the effective memory depth of RNN models by using them for $n$-gram language model (LM) smoothing. Experiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the $n$-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM $n$-gram matches the LSTM LM performance for $n=9$ and slightly outperforms it for $n=13$. When allowing dependencies across sentence boundaries, the LSTM $13$-gram almost matches the perplexity of the unlimited history LSTM LM. LSTM $n$-gram smoothing also has the desirable property of improving with increasing $n$-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low $n$-gram orders. Experiments on the One Billion Words benchmark show that the results hold at larger scale: while LSTM smoothing for short $n$-gram contexts does not provide significant advantages over classic N-gram models, it becomes effective with long contexts ($n &gt; 5$); depending on the task and amount of data it can match fully recurrent LSTM models at about $n=13$. This may have implications when modeling short-format text, e.g. voice search/query LMs. Building LSTM $n$-gram LMs may be appealing for some practical situations: the state in a $n$-gram LM can be succinctly represented with $(n-1)*4$ bytes storing the identity of the words in the context and batches of $n$-gram contexts can be processed in parallel. On the downside, the $n$-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM.",
    "full_text": "arXiv:1703.10724v2  [cs.CL]  20 Jun 2017\nGoogle T ech Report\nN- G R A M LA N G UAG E MO D E L I N G U S I N G RE C U R RE N T\nNE U R A L NE T WO R K ES T I M AT IO N\nCiprian Chelba, Mohammad Norouzi, Samy Bengio\nGoogle\n{ciprianchelba,mnorouzi,bengio}@google.com\nABSTRACT\nW e investigate the effective memory depth of RNN models by us ing them for\nn-gram language model (LM) smoothing.\nExperiments on a small corpus (UPenn Treebank, one million w ords of train-\ning data and 10k vocabulary) have found the LSTM cell with dro pout to be the\nbest model for encoding the n-gram state when compared with feed-forward and\nvanilla RNN models. When preserving the sentence independe nce assumption the\nLSTM n-gram matches the LSTM LM performance for n = 9 and slightly out-\nperforms it for n = 13 . When allowing dependencies across sentence boundaries,\nthe LSTM 13-gram almost matches the perplexity of the unlimited histor y LSTM\nLM.\nLSTM n-gram smoothing also has the desirable property of improvin g with in-\ncreasing n-gram order, unlike the Katz or Kneser-Ney back-off estimat ors. Using\nmultinomial distributions as targets in training instead o f the usual one-hot target\nis only slightly beneﬁcial for low n-gram orders.\nExperiments on the One Billion W ords benchmark show that the results hold at\nlarger scale: while LSTM smoothing for short n-gram contexts does not provide\nsigniﬁcant advantages over classic N-gram models, it becom es effective with long\ncontexts ( n > 5); depending on the task and amount of data it can match fully\nrecurrent LSTM models at about n = 13 . This may have implications when\nmodeling short-format text, e.g. voice search/query LMs.\nBuilding LSTM n-gram LMs may be appealing for some practical situations: th e\nstate in a n-gram LM can be succinctly represented with (n − 1) ∗ 4 bytes storing\nthe identity of the words in the context and batches of n-gram contexts can be\nprocessed in parallel. On the downside, the n-gram context encoding computed by\nthe LSTM is discarded, making the model more expensive than a regular recurrent\nLSTM LM.\n1 I NTRODUC TI ON\nA statistical language model (LM) estimates the prior proba bility values P (W ) for strings of words\nW in a vocabulary V whose size is usually in the tens or hundreds of thousands. T y pically the\nstring W is broken into sentences, or other segments such as utteranc es in automatic speech recog-\nnition which are assumed to be conditionally independent; t he independence assumption has certain\nadvantages in practice but is not strictly necessary.\nApplying the chain rule to a sentence W = w1, w2, . . . , w n we get:\nP (W ) =\nn∏\nk=1\nP (wk|w1, w2, . . . , w k− 1) (1)\nSince the parameter space of P (wk|w1, w2, . . . , w k− 1) is too large, the language model is forced\nto put the context Wk− 1 = w1, w2, . . . , w k− 1 into an equivalence class determined by a function\nΦ( Wk− 1). As a result:\nP (W ) ∼\n=\nn∏\nk=1\nP (wk|Φ( Wk− 1)) (2)\n1\nGoogle T ech Report\nResearch in language modeling consists of ﬁnding appropria te equivalence classiﬁers Φ and methods\nto estimate P (wk|Φ( Wk− 1)).\n1.1 P E RP L E X IT Y A S A ME A S U RE O F LA N G UAG E MO D E L QUA L IT Y\nA commonly used quality measure for a given model M is related to the entropy of the underlying\nsource and was introduced under the name of perplexity (PPL) Jelinek (1997):\nP P L(W, M ) = exp(− 1\nN\nN∑\nk=1\nln [PM (wk|Wk− 1)]) (3)\nT o give intuitive meaning to perplexity, it represents the a verage number of guesses the model needs\nto make in order to ascertain the identity of the next word, wh en running over the test word string\nW = w1 . . . w N from left to right. It can be easily shown that the perplexity of a language model\nthat uses the uniform probability distribution over words i n the vocabulary V equals the size of the\nvocabulary; a good language model should of course have lowe r perplexity, and thus the vocabulary\nsize is an upper bound on the perplexity of a given language mo del.\nV ery likely, not all words in the test data are part of the lang uage model vocabulary. It is common\npractice to map all words that are out-of-vocabulary to a dis tinguished unknown word symbol, and\nreport the out-of-vocabulary (OOV) rate on test data—the ra te at which one encounters OOV words\nin the test sequence W —as yet another language model performance metric besides p erplexity. Usu-\nally the unknown word is assumed to be part of the language mod el vocabulary— open vocabulary\nlanguage models—and its occurrences are counted in the lang uage model perplexity calculation in\nEq. (3). A situation less common in practice is that of closed vocabulary language models where all\nwords in the test data will always be part of the vocabulary V.\n1.2 S M O OT H IN G\nSince the language model is meant to assign non-zero probabi lity to unseen strings of words (or\nequivalently, ensure that the cross-entropy of the model ov er an arbitrary test string is not inﬁnite),\na desirable property is that:\nP (wk|Φ( Wk− 1)) > ǫ > 0, ∀wk, Wk− 1 (4)\nalso known as the smoothing requirement.\nThere are currently two dominant approaches for building LM s:\n1.2.1 n-G RA M LA N G UAG E MO D E L S\nThe most widespread paradigm in language modeling makes a Ma rkov assumption and uses the\n(n − 1)-gram equivalence classiﬁcation, that is, deﬁnes\nΦ( Wk− 1) .= wk− n+1, wk− n+2, . . . , w k− 1 = h (5)\nA large body of work has accumulated over the years on various smoothing methods for n-gram\nLMs. The two most popular smoothing techniques are probably Kneser & Ney (1995) and Katz\n(1987), both making use of back-off to balance the speciﬁcity of long contexts with the reliabil ity of\nestimates in shorter n-gram contexts. Goodman (2001) provides an excellent overv iew that is highly\nrecommended to any practitioner of language modeling.\nApproaches that depart from the nested features used in back -off n-gram LMs have shown excel-\nlent results at the cost of increasing the number of features and parameters stored by the model,\ne.g. Pelemans et al. (2016).\n1.2.2 N E U RA L LA N G UAG E MO D E L S\nNeural networks (NNLM) have emerged in recent years as an alt ernative to estimating and storing\nn-gram LMs. W ords (or some other modeling unit) are represent ed using an embedding vector\nE(w) ∈ Rd. A simple NNLM architecture makes the Markov assumption and feeds the concate-\nnated embedding vectors for the words in the n-gram context to one or more layers each consisting\n2\nGoogle T ech Report\nof an afﬁne transform followed by a non-linearity (typicall y tanh); the output of the last such layer is\nthen fed to the output layer consisting again of an afﬁne tran sform but this time followed by an expo-\nnential non-linearity that is normalized to guarantee a pro per probability over the vocabulary. This\nis commonly named a feed-forward architecture for an n-gram LM (FF-NNLM), ﬁrst introduced\nby Bengio et al. (2001).\nAn alternative is the recurrent NNLM architecture that feed s the embedding of each word E(wk)\none at a time, advancing the state S ∈ Rs of a recurrent cell and producing a new output U ∈ Ru:\n[Sk, Uk] = RNN (Sk− 1, E(wk)) (6)\nS0 = 0\nThis provides a representation for the context Wk− 1 that can be directly plugged into Eq. 2:\nΦ( Wk− 1) = Uk− 1(Wk− 1) (7)\nSimilar to the FF-NNLM architecture, the output U of the recurrent cell is then fed to a soft-max\nlayer consisting of an afﬁne transform O followed by an exponential non-linearity properly normal-\nized over the vocabulary.\nThe recurrent cell RNN (·) can consist of one or more simple afﬁne/non-linearity layer s,\noften called a vanilla RNN architecture, see Mikolov et al. (2010). The LSTM cell due\nto Hochreiter & Schmidhuber (1997) has proven very effectiv e at modeling long range depen-\ndencies and has become the state-of-the-art architecture f or language modeling using RNNs,\nsee Józefowicz et al. (2016).\nIn this work we approximate unlimited history (R)NN models w ith n-gram models in an attempt\nto identify the order n at which they become equivalent from a perplexity point of vi ew . This is a\npromising direction in a few ways:\n• the training data can be reduced to n-gram sufﬁcient statistics, and the target distribution\npresented to the NN n-gram LM in a given context can be a multinomial pmf instead of the\none-hot encoding used in on-line training for (R)NN LMs;\n• unlike many LSTM LM implementations, back-propagation thr ough time for the LSTM\nn-gram need not be truncated at the begining of segments used t o batch the training data;\n• the state in a n-gram LM can be succinctly represented with (n − 1) ∗ 4 bytes storing the\nidentity of the words in the context; this is in stark contras t with the state S ∈ Rs for\nan RNN LM, where s = 1024 or higher, making the n-gram LM much easier to use in\ndecoders such as for ASR/SMT;\n• similar to Brants et al. (2007), batches of n-gram contexts can be processed in parallel to\nestimate a sharded (R)NN n-gram model; this is particularly attractive because it all ows\nscaling both the amount of training data and the NNLM size sig niﬁcantly (100X).\n2 M ETHOD\nAs mentioned in the previous section, the Markov assumption made by n-gram models allows us\nto present to the NN multinomial training targets specifying the full distribution in a give n n-gram\ncontext instead of the usual one-hot target specifying the predicted word occurring in a given co ntext\ninstance. In addition, when using multinomial targets we can either weight each training sample by\nthe context count or simply present each context token encou ntered in the training data along with\nthe conditional multinomial pmf computed from the entire tr aining set.\nW e thus have three main training regimes:\n• context-weighted multinomial targets\n• multinomial targets (context count count(h) = 1 )\n• one-hot targets (context count count(h) = 1 , word count count(h, w) = 1 )\n3\nGoogle T ech Report\nThe loss function optimized in training is the cross-entrop y between the model pmf P (w|h; θ) in\nsome n-gram context h and the relative frequency f(w|h; T ) in the training data T (or development\nD, or test data E) is computed as:\nH(P, T ) = − 1/T\n∑\nh\ncount(h)\n∑\nw\nf(w|h; T ) log P (w|h; θ) (8)\nwhere T is the length of the training data T and P (·; θ) is the n-gram model being evaluated/trained\nas parameterized by θ.\nThe baseline back-off n-gram models (Katz, interpolated Kneser-Ney) are trained b y making a sen-\ntence independence assumption. As a result n-gram contexts at the beginning of the sentence are\npadded to the left to reach the full context length. The same n-gram counting strategy is used when\npreparing the data for the various NN n-gram LMs that we experimented with. Since RNN LMs are\nnormally trained and evaluated without making this indepen dence assumption by passing the LM\nstate across sentence boundaries, we also evaluated the imp act of resetting the RNN LM state at\nsentence beginning.\nW e next detail the various ﬂavors or NN LM implementations we experimented with.\nFor all NN LMs we represent context words using an embedding v ector E(w) ∈ Rd. Unless\notherwise stated, all models are trained to minimize the cro ss-entropy on training data in Eq. (8),\nusing Adagrad (Duchi et al. (2011)) and gradient norm clippi ng (Pascanu et al. (2012)); the model\nparameters are initialized by sampling from a truncated nor mal distribution of zero mean and a given\nstandard deviation.\nTraining proceeds for a ﬁxed number of epochs for every given point on the grid of hyper-parameters\nexplored for a given model type; the best performing model (p arameter values) on development data\nD is retained as the ﬁnal one to be evaluated on test data E in order to report the model perplexity.\nAll models were implemented using T ensorFlow , see Abadi & et al. (2015b).\n2.1 F E E D FO RWA RD n-G RA M LM\nEach word w in the n-gram context h = wk− n+1 . . . w k− 1 is embedded using the mapping E(w);\nthe resulting vectors are concatenated to form a d ·(n − 1) dimensional vector that is ﬁrst fed into a\ndropout layer Srivastava et al. (2014) and then into an afﬁne layer followed by a tanh non-linearity.\nThe output of this so-called “hidden” layer is again fed into a dropout layer and then followed by\nan afﬁne layer O whose output is of the same dimensionality as the vocabulary . An exponential\n“soft-max” layer converts the activations produced by the l ast afﬁne layer into probabilities over the\nvocabulary.\nT o summarize:\nX = concat(E(wk− n+1), . . . , E (wk− 1))\nD(X) = dropout(X; Pkeep)\nY = tanh( H ·D(X) + Hbias)\nD(Y ) = dropout(Y ; Pkeep)\nP (·|wk− n+1 . . . w k− 1) = exp( O ·D(Y ) + Obias) (9)\nThe parameters of the model are the embedding matrix E ∈ Rd× V , the keep probability for dropout\nlayers Pkeep, the afﬁne input layer parameterized by H ∈ Rs× (n− 1)·d, Hbias ∈ Rs and the output\none parameterized by O ∈ RV × s, Obias ∈ RV .\nThe hyper-parameters controlling the training are: number of training epochs, n-gram order, di-\nmensionality of the model parameters d, s, keep probability value, gradient norm clipping value,\nstandard deviation for the initializer and the Adagrad lear ning rate and initial accumulator value.\n2.2 “V A N IL L A” R E CU RRE N T n-G RA M LM\nEach word w in the n-gram context h = wk− n+1 . . . w k− 1 is embedded using the mapping E(w)\nfollowed by dropout and then fed in left-to-right order into the RNN cell in Eq. (6). The ﬁnal\n4\nGoogle T ech Report\noutput of the RNN cell is then fed ﬁrst into a dropout layer and then into an afﬁne layer followed by\nexponential “soft-max”.\nAssuming that we encode the context h = wk− n+1 . . . wk− 1 with an RNN cell deﬁned as follows\n(using the running index l = k − n + 1 . . . k to traverse the context and a dropout layer on the\nembedding D(E(wl)) = dropout(E(wl), Pkeep)):\n[Sl, Ul] = tanh( R ·[Sl− 1, D(E(wl))] + Rbias)\nSk− n = 0 (10)\nwe pick the last output Uk− 1 and feed it into a dropout layer followed by an afﬁne layer and soft-max\noutput:\nD(Uk− 1) = dropout(Uk− 1; Pkeep)\nP (·|wk− n+1 . . . wk− 1) = exp( O ·D(Uk− 1) + Obias) (11)\nThe parameters of the model are the embedding matrix E ∈ Rd× V , the keep probability for dropout\nlayers Pkeep, the RNN afﬁne layer parameterized by R ∈ R(d+s)× 2·s) and Rbias ∈ R2·s and the\noutput one parameterized by O ∈ RV × s and Obias ∈ RV . Note that we choose to use the same\ndimensionality s for both S, U ∈ Rs.\nThe hyper-parameters controlling the training are the same as in the previous section.\n2.3 LSTM R E CU RRE N T n-G RA M LM\nFinally, we replace the “vanilla” RNN cell deﬁned above with a multilayer LSTM cell with dropout.\nSince this was the most effective model, we experimented wit h a few options:\n• forward context encoding: context words h = wk− n+1 . . . wk− 1 are fed in left-to-right\norder in the LSTM cell; the LSTM cell output after the last con text word wk− 1 is then fed\ninto the output layer;\n• reverse context encoding: context words h = wk− n+1 . . . w k− 1 are fed in left-to-right\norder in the LSTM cell; the LSTM cell output after the ﬁrst con text word wk− n+1 is then\nfed into the output layer;\n• stacked output for either of the above: we concatenate the ou tput vectors along the way and\nfeed that into the output layer;\n• bidirectional context encoding: we encode the context twic e, forward and reverse order\nrespectively, using two separate LSTM cells; the two output s are then concatenated and fed\nto the output layer;\n• forward context encoding with incremental loss with/out ex ponential decay as a function\nof the context length\nThe last item above deserves a more detailed explanation. It is possible that the LSTM encoder\nwould beneﬁt from incremental error back-propagation alon g the n-gram context instead of just\none back-propagation step at the end of the context. As such, we modify the loss function to be\nthe cumulative cross-entropy between the relative frequen cy and the model output distribution at\neach step in the for loop feeding the n-gram context into the LSTM cell instead of just the last one .\nThus amounts to targetting a mix of 1 . . . n-gram target distributions; to have better control over\nthe contribution of different n-gram orders to the loss function, we weigh each loss functio n by an\nexponential term exp(− decay ·(n− 1− l)). The decay > 0 value controls how fast the contribution\nto the loss function from lower n-gram orders decays; note that the highest order l = n − 1 has\nweight 1.0 so a very large value decay = ∞ restores the regular training loss function. For this\ntraining regime we only implemented one-hot targets: the am ount of data that needs to be fed to the\nT ensorFlow graph would increase signiﬁcantly for incremen tal multinomial targets.\nThe hyper-parameters controlling the training are: number of training epochs, n-gram order, embed-\nding dimensionality d, LSTM cell output dimensionality s and number of layers, keep probability\nvalue, gradient norm clipping value, standard deviation fo r the initializer. T o match the fully recur-\nrent LSTM LM implemented by the UPenn Treebank T ensorFlow tu torial, we estimated all of our\n5\nGoogle T ech Report\nLSTM n-gram models using gradient descent with variable learning rate: initially the learning rate\nis constant for a few iterations after which it follows a line ar decay schedule. The hyper-parameters\ncontrolling this schedule were not optimized but rather we u sed the same values as in the RNN LM\ntutorial provided with Abadi & et al. (2015a) or the implemen tation in Józefowicz (2016), respec-\ntively.\nPerhaps a bit of a technicality but it is worth pointing out a m ajor difference between error back-\npropagation through time (BPTT) as implemented in either of the above and the error back-\npropagation in the LSTM/RNN n-gram LM: Abadi & et al. (2015a) and Józefowicz (2016) imple-\nment BPTT by segmenting the training data into non-overlapping segments (of length 35 or 20,\nrespectively)1. The error BPTT does not cross the left boundary of such segme nts, whereas the\nLSTM state is of course copied forward. As a result, the ﬁrst w ord in a segment is not really con-\ntributing to training, and the immediately following ones h ave a limited effect. This is in contrast to\nerror back-propagation for the LSTM/RNN n-gram LM: the n-gram window slides over the train-\ning/test data, and error back-propagation covers the entir e n-gram context; the LSTM cell state and\noutput computed for a given n-gram context are discarded once the output distribution is computed.\n3 E XPERIME NT S\n3.1 UP E N N TRE E BA N K CO RP U S\nFor our initial set of experiments we used the same data set as in Abadi & et al. (2015a), with exactly\nthe same training/validation/test set partition and vocab ulary. The training data consists of about\none million words, and the vocabulary contains ten thousand words; the validation/test data contains\n73760/82430 words, respectively (including the end-of-se ntence token). The out-of-vocabulary rate\non validation/test data is 5.0/5.8%, respectively.\nAs an initial batch of experiments we trained and evaluated b ack-off n-gram models using Katz and\ninterpolated Kneser-Ney smoothing. W e also used the medium setting in Abadi & et al. (2015a)\nas an LSTM/RNN LM baseline; since the baseline n-gram models are trained under a sentence\nindependence assumption, we also ran the LSTM/RNN LM baseli ne by resetting the LSTM state at\neach sentence beginning. The results are presented in T able 1.\nModel Order T est PPL\nn-gram, baseline\nKatz, back-off 5 167\nKatz, back-off 9 182\nInterpolated Kneser-Ney, back-off 5 143\nInterpolated Kneser-Ney, back-off 9 143\nLSTM RNN, baseline\nLSTM (medium setting) reset state at <S> 95\nLSTM (medium setting) ∞ 84\nT able 1: UPenn Treebank: baseline back-off n-gram and LSTM perplexity values.\nAs expected Kneser-Ney (KN) is better than Katz, and it does n ot improve with the n-gram order\npast a certain value, in this case n = 5 . This behavior is due to the fact that the n-gram hit ratio\non test data (number of test n-grams that were observed in training) decreases dramatica lly with the\nn-gram order: the percentage of covered n-grams2 for n = 1 . . . 9 is 100, 81, 42, 18, 8.6, 5.0, 3.3,\n2.5, 2.0, respectively.\nThe medium setting for the LSTM LM in Abadi & et al. (2015a) per forms signiﬁcantly better than\nthe KN baseline. Resetting the state at sentence beginning d egrades PPL signiﬁcantly by 13%\nrelative.\n1 W e have evaluated the impact of reducing the segment length d ramatically , e.g. 4 instead of 35. Much to\nour surprise, the LSTM PPL increased modestly , from 84 to 88, see the before last row in T able 1; for the One\nBillion W ords experiments using a segment of length 5 did not change PPL at all.\n2 For the hit ratio calculation the n-grams are not padded to the left of sentence beginning; if we are to count\nhit ratios using padded n-grams, the values are: 100, 81, 44.7, 24.0, 16.5, 13.7, 12.5 , 11.8, 11.5, respectively .\n6\nGoogle T ech Report\nModel Order T est PPL\nTraining T arget multinomial one-hot\nn-gram, baseline\nInterpolated Kneser-Ney, back-off 5 143\nFeed-fwd n-gram\nFeed-fwd n-gram 5 127 128\nFeed-fwd n-gram 9 125 126\nFeed-fwd n-gram 13 125 127\n“V anilla” RNN n-gram\nRNN n-gram 9 127 131\nLSTM RNN n-gram\nLSTM n-gram, forward context encoding 5 103 106\nLSTM n-gram, forward context encoding 9 94 93\nLSTM n-gram, forward context encoding 13 91 90\nLSTM n-gram, reversed context encoding 9 102 107\nLSTM n-gram, bidirectional context encoding 9 100 102\nincremental LSTM n-gram with decay, decay = 2 .0 13 — 91\nLSTM RNN, baseline\nLSTM (medium setting) reset at <S> — 95\nT able 2: UPenn Treebank: perplexity values for neural netwo rk smoothed n-gram LM.\nW e then trained and evaluated various NN-smoothed n-gram LMs, as described in Section 2. The\nresults are presented in T able 2. The best model among the one s considered is by far the LSTM\nn-gram. The most signiﬁcant experimental result is that the L STM n-gram can match and even\noutperform the fully recurrent LSTM LM as we increase the ord er n: n = 9 matches the LSTM LM\nperformance, decreasing the LM perplexity by 34% relative o ver the Kneser-Ney baseline. LSTM\nn-gram smoothing also has the desirable property of improvin g with the n-gram order, unlike the\nKatz or Kneser-Ney back-off estimators, which can be credit ed to better feature extraction from the\nn-gram context.\nMultinomial targets can slightly outperform the one-hot on es although the difference is shrinking as\nwe increase the n-gram order. W eighting the contribution of each context to t he loss function by\nits count did not work; we suspect this is because on-line tra ining does not work well with the Zipf\ndistribution on context counts.\nAmong the various ﬂavors of LSTM models we experimented with , the forward context encoding\nperforms best. The incremental LSTM n-gram with a fairly large decay ( decay = 2 .0) is slightly\nbetter but we do not consider the difference to be statistica lly signiﬁcant (it also entails signiﬁcantly\nmore computation, we need to perform n − 1 back-propagation steps for each input n-gram).\nT o compare with the LSTM RNN LM that does not reset state at sen tence beginning we also trained\nLSTM n-gram models (forward context encoding only) that straddle the sentence beginning. The\nresults are presented in T able 3. Again, we notice that for a l arge enough order the LSTM n-gram\nLM comes very close to matching the fully recurrent LSTM base line.\nModel Order T est PPL\nTraining T arget multinomial one-hot\nLSTM RNN n-gram\nLSTM n-gram, forward context encoding, straddling <S> 5 102 104\nLSTM n-gram, forward context encoding, straddling <S> 9 91 95\nLSTM n-gram, forward context encoding, straddling <S> 13 87 91\nLSTM RNN, baseline\nLSTM (medium setting) ∞ — 84\nT able 3: UPenn Treebank: perplexity values for neural netwo rk smoothed n-gram LM when strad-\ndling the sentence beginning boundary.\n7\nGoogle T ech Report\n3.2 O N E BIL L IO N WO RD S BE N CH M A RK\nIn a second set of experiments we used the corpus in Chelba et a l. (2013), the same as\nin Józefowicz et al. (2016). For the baseline LSTM model we us ed the single machine implementa-\ntion provided by Józefowicz (2016); the LSTM n-gram variant was implemented as a minor tweak\non this codebase and is thus different from the one used in the UPenn Treebank experiments in\nSection 3.1.\nW e experimented with the LSTM conﬁguration in T able 3 of Józe fowicz et al. (2016) for both base-\nline LSTM and n-gram variant, which are also the default settings in Józefo wicz (2016): embed-\nding and projection layer dimensionality was 128, one layer with state dimensionality of 2048.\nTraining used Adagrad with gradient clipping by global norm (10.0) and droput (probability 0.1);\nback-propagation at the output soft-max layer is done using importance sampling as described\nin Józefowicz et al. (2016) with a set 8192 “negative” sample s. An additional set of experiments\ninvestigated the beneﬁts of adding one more layer to both bas eline and n-gram LSTM.\nThe results are presented in T ables 4-5; unlike the UPenn Tre ebank experiments, we did not tune\nthe hyper-parameters for the n-gram LSTM and instead just used the same ones as for the LSTM\nbaseline; as a result the perplexity values for the n-gram LSTM may be slightly suboptimal.\nSimilar to the UPenn Treebank experiments, we examined the e ffect of resetting state at sentence\nboundaries. As expected PPL dit not change signiﬁcantly bec ause the sentences in the training and\ntest data were randomized, see Chelba et al. (2013); in fact m odeling the sentence independence\nexplicitly is slightly beneﬁcial.\nW e observe that on large amounts of data LSTM smoothing for sh ort n-gram contexts does not\nprovide signiﬁcant advantages over classic back-off n-gram models. This may have implications\nfor short-format text, e.g. voice search/query LMs. On the o ther hand, LSTM smoothing becomes\nvery effective with long contexts ( n > 5) approaching the fully recurrent LSTM model perplexity\nat about n = 13 .\nTraining times are signiﬁcantly different between the LSTM baseline and the n-gram variant, with\nthe latter being about an order of magnitude slower due to the fact that the LSTM state is recomputed\nand discarded for every new training sample.\nModel Order T est PPL\nTraining T arget one-hot\nn-gram, baseline\nInterpolated Kneser-Ney, back-off 5 68\nLSTM RNN n-gram\nLSTM n-gram, forward context encoding 5 70\nLSTM n-gram, forward context encoding 9 54\nLSTM n-gram, forward context encoding 13 49\nLSTM RNN, baseline\nLSTM reset at <S> 48\n2-layer LSTM RNN n-gram\nLSTM n-gram, forward context encoding 5 68\nLSTM n-gram, forward context encoding 9 51\nLSTM n-gram, forward context encoding 13 46\n2-layer LSTM RNN, baseline\nLSTM reset at <S> 43\nT able 4: One Billion W ords Benchmark: perplexity values for neural network smoothed n-gram LM\nwhen enforcing the sentence independence.\n4 C ONCLUSIO NS AND FUTURE WORK\nW e investigated the effective memory depth of (R)NN models b y using them for word-level n-gram\nLM smoothing. The LSTM cell with dropout was by far the best (R )NN model for encoding the\nn-gram state.\n8\nGoogle T ech Report\nModel Order T est PPL\nTraining T arget one-hot\nLSTM RNN n-gram\nLSTM n-gram, forward context encoding, straddling <S> 5 70\nLSTM n-gram, forward context encoding, straddling <S> 9 54\nLSTM n-gram, forward context encoding, straddling <S> 13 49\nLSTM RNN, baseline\nLSTM ∞ 49\n2-layer LSTM RNN n-gram\nLSTM n-gram, forward context encoding, straddling <S> 5 68\nLSTM n-gram, forward context encoding, straddling <S> 9 51\nLSTM n-gram, forward context encoding, straddling <S> 13 46\n2-layer LSTM RNN, baseline\nLSTM ∞ 43\nT able 5: One Billion W ords Benchmark: perplexity values for neural network smoothed n-gram LM\nwhen straddling the sentence beginning boundary.\nWhen preserving the sentence independence assumption the L STM n-gram matches the LSTM LM\nperformance for n = 9 and slightly outperforms it for n = 13 . When allowing dependencies across\nsentence boundaries, the LSTM 13-gram almost matches the perplexity of the unlimited histor y\nLSTM LM.\nW e can thus conclude that the memory of LSTM LMs seems to be abo ut 9-13 previous words which\nis not a trivial depth but not that large either.\nCompared to standard n-gram smoothing methods LSTMs have excellent statistical p roperties: they\nimprove with the n-gram order well beyond the point where Katz or Kneser-Ney ba ck-off smoothing\nmethods saturate, proving that they are able to extract rich er features from the same context. Using\nmultinomial targets in training is only slightly beneﬁcial in this setting, although the advantage over\none-hot diminishes with increasing n-gram order.\nExperiments on the One Billion W ords benchmark conﬁrm that n-gram LSTMs can match the per-\nformance of fully recurrent LSTMs at larger amounts of data.\nBuilding LSTM n-gram LMs is attractive due to the fact that the state in a n-gram LM can be\nsuccinctly represented on 4 ·(n − 1) bytes storing the identity of the context words. This is in st ark\ncontrast with the state H ∈ Rh for an RNN LM, where h = 1024 or higher, making the n-gram LM\neasier to use in decoders such as for ASR/SMT . The LM requests in the decoder can be batched,\nmaking the RNN LM operation more efﬁcient on GPUs.\nOn the downside, the LSTM encoding for the n-gram context is discarded and cannot be re-used;\ncaching it for frequent LM states is possible.\nACKNOWLE DG M EN TS\nW e would like to thank Oriol V inyals and Rafał Józefowicz for support with the baseline imple-\nmentation of LSTM LMs for UPenn Treebank in Abadi & et al. (201 5a) and One Billion W ords\nBenchmark in Józefowicz (2016), respectively. W e would als o like to thank Maxim Krikun for\nthorough code reviews and useful discussions.\nREFERENC ES\nM. Abadi and et al. Recurrent neural networks tutorial (lang uage modeling), 2015a. URL\nhttps://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html.\nUPenn Treebank language modeling.\nM. Abadi and et al. T ensorFlow: Large-scale machine learnin g on heterogeneous systems, 2015b.\nURL http://tensorflow.org/. Software available from tensorﬂow .org.\n9\nGoogle T ech Report\nY oshua Bengio, Réjean Ducharme, and Pascal V incent. A neura l probabilistic language model.\n2001. URL http://www.iro.umontreal.ca/~lisa/pointeurs/nips00_lm.ps.\nT . Brants, A. C. Popat, P . Xu, F . J. Och, and J. Dean. Large lang uage models in machine translation.\nIn Proceedings of the 2007 Joint Conference on Empirical Metho ds in Natural Language Pro-\ncessing and Computational Natural Language Learning (EMNL P-CoNLL), pp. 858–867, 2007.\nURL http://www.aclweb.org/anthology/D/D07/D07-1090.\nCiprian Chelba, T omas Mikolov, Mike Schuster, Qi Ge, Thorst en Brants, and Phillipp Koehn.\nOne billion word benchmark for measuring progress in statis tical language modeling. CoRR,\nabs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005.\nJohn Duchi, Elad Hazan, and Y oram Singer. Adaptive subgradi ent methods for online learning and\nstochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011. ISSN 1532-4435. URL\nhttp://dl.acm.org/citation.cfm?id=1953048.2021068.\nJoshua Goodman. A bit of progress in language modeling, exte nded version. T echnical report,\nMicrosoft Research, 2001.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term me mory. Neural Comput., 1997.\nFrederick Jelinek. Information Extraction From Speech And T ext, chapter 8, pp. 141–142. MIT\nPress, 1997.\nRafal Józefowicz. Single machine implementation of LSTM la nguage model on\nOne Billion W ords benchmark using synchronized gradient up dates, 2016. URL\nhttps://github.com/rafaljozefowicz/lm.\nRafal Józefowicz, Oriol V inyals, Mike Schuster, Noam Shaze er, and Y onghui Wu. Ex-\nploring the limits of language modeling. CoRR, abs/1602.02410, 2016. URL\nhttp://arxiv.org/abs/1602.02410.\nS. Katz. Estimation of probabilities from sparse data for th e language model component of a speech\nrecognizer. In IEEE T ransactions on Acoustics, Speech and Signal Processi ng, volume 35, pp.\n400–01, 1987.\nR. Kneser and H. Ney. Improved backing-off for m-gram langua ge modeling. In Proceedings of\nthe IEEE International Conference on Acoustics, Speech and Signal Processing, volume 1, pp.\n181–184, 1995.\nT omas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Interspeech, volume 2, pp. 3, 2010.\nRazvan Pascanu, T omas Mikolov, and Y oshua Bengio. Understa nding the exploding gradient prob-\nlem. CoRR, abs/1211.5063, 2012. URL http://arxiv.org/abs/1211.5063.\nJoris Pelemans, Noam Shazeer, and Ciprian Chelba. Sparse no n-negative matrix language modeling.\nT ransactions of the Association for Computational Linguis tics, 4:329–342, 2016. ISSN 2307-\n387X. URL https://transacl.org/ojs/index.php/tacl/article/view/561.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. Dropout: A simple way to prevent neural netw orks from over-\nﬁtting. Journal of Machine Learning Research , 15:1929–1958, 2014. URL\nhttp://jmlr.org/papers/v15/srivastava14a.html.\n10"
}