{
    "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers",
    "url": "https://openalex.org/W3174544005",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2111247525",
            "name": "Wenhui Wang",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2605948870",
            "name": "Hangbo Bao",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2612910427",
            "name": "Shaohan Huang",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A1974723233",
            "name": "Li Dong",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2171151462",
            "name": "Furu Wei",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3015233032",
        "https://openalex.org/W2964118293",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2997666887",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2561238782",
        "https://openalex.org/W3113747735",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W3035497479",
        "https://openalex.org/W3015609966",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2969601108",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W4288256350",
        "https://openalex.org/W3103884771",
        "https://openalex.org/W2998653236",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2963350559",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1690739335",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4253067820",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W3101248447",
        "https://openalex.org/W4313908941",
        "https://openalex.org/W2970557265",
        "https://openalex.org/W3101731278",
        "https://openalex.org/W3171975879",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3042711927",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3035317797",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2986591322",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3034999214"
    ],
    "abstract": "We generalize deep self-attention distillation in MINILM (Wang et al., 2020) by only using self-attention relation distillation for taskagnostic compression of pretrained Transformers.In particular, we define multi-head selfattention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module.Then we employ the above relational knowledge to train the student model.Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student.Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer.In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MINILM.We conduct extensive experiments on compressing both monolingual and multilingual pre-trained models.Experimental results demonstrate that our models 1 distilled from base-size and large-size teachers (BERT, RoBERTa and XLM-R) outperform the state-of-the-art.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2140‚Äì2151\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n2140\nMINI LMv2: Multi-Head Self-Attention Relation Distillation\nfor Compressing Pretrained Transformers\nWenhui Wang Hangbo Bao Shaohan Huang Li Dong Furu Wei ‚àó\nMicrosoft Research\n{wenwan,t-habao,shaohanh,lidong1,fuwei}@microsoft.com\nAbstract\nWe generalize deep self-attention distillation\nin M INI LM (Wang et al., 2020) by only us-\ning self-attention relation distillation for task-\nagnostic compression of pretrained Transform-\ners. In particular, we deÔ¨Åne multi-head self-\nattention relations as scaled dot-product be-\ntween the pairs of query, key, and value vec-\ntors within each self-attention module. Then\nwe employ the above relational knowledge to\ntrain the student model. Besides its simplic-\nity and uniÔ¨Åed principle, more favorably, there\nis no restriction in terms of the number of\nstudent‚Äôs attention heads, while most previous\nwork has to guarantee the same head num-\nber between teacher and student. Moreover,\nthe Ô¨Åne-grained self-attention relations tend to\nfully exploit the interaction knowledge learned\nby Transformer. In addition, we thoroughly ex-\namine the layer selection strategy for teacher\nmodels, rather than just relying on the last\nlayer as in MINI LM. We conduct extensive ex-\nperiments on compressing both monolingual\nand multilingual pre-trained models. Exper-\nimental results demonstrate that our models 1\ndistilled from base-size and large-size teachers\n(BERT, RoBERTa and XLM-R) outperform\nthe state-of-the-art.\n1 Introduction\nPretrained Transformers (Radford et al., 2018; De-\nvlin et al., 2018; Dong et al., 2019; Yang et al.,\n2019; Joshi et al., 2019; Liu et al., 2019; Bao et al.,\n2020; Radford et al., 2019; Raffel et al., 2019;\nLewis et al., 2019a) have been highly successful for\na wide range of natural language processing tasks.\nHowever, these models usually consist of hundreds\nof millions of parameters and are getting bigger. It\nbrings challenges for Ô¨Åne-tuning and online serv-\n‚àóContact person.\n1Distilled models and code will be publicly available at\nhttps://aka.ms/minilm.\ning in real-life applications due to the restrictions\nof computation resources and latency.\nKnowledge distillation (KD; Hinton et al.\n2015, Romero et al. 2015) has been widely em-\nployed to compress pretrained Transformers, which\ntransfers knowledge of the large model (teacher)\nto the small model (student) by minimizing the\ndifferences between teacher and student features.\nSoft target probabilities (soft labels) and interme-\ndiate representations are usually utilized to per-\nform KD training. In this work, we focus on\ntask-agnostic compression of pretrained Transform-\ners (Sanh et al., 2019; Tsai et al., 2019; Jiao et al.,\n2019; Sun et al., 2019b; Wang et al., 2020). The\nstudent models are distilled from large pretrained\nTransformers using large-scale text corpora. The\ndistilled task-agnostic model can be directly Ô¨Åne-\ntuned on downstream tasks, and can be utilized to\ninitialize task-speciÔ¨Åc distillation.\nDistilBERT (Sanh et al., 2019) uses soft target\nprobabilities for masked language modeling pre-\ndictions and embedding outputs to train the stu-\ndent. The student model is initialized from the\nteacher by taking one layer out of two. Tiny-\nBERT (Jiao et al., 2019) utilizes hidden states\nand self-attention distributions (i.e., attention maps\nand weights), and adopts a uniform function to\nmap student and teacher layers for layer-wise dis-\ntillation. MobileBERT (Sun et al., 2019b) intro-\nduces specially designed teacher and student mod-\nels using inverted-bottleneck and bottleneck struc-\ntures to keep their layer number and hidden size\nthe same, layer-wisely transferring hidden states\nand self-attention distributions. MINI LM (Wang\net al., 2020) proposes deep self-attention distilla-\ntion, which uses self-attention distributions and\nvalue relations to help the student to deeply mimic\nteacher‚Äôs self-attention modules.MINI LM shows\nthat transferring knowledge of teacher‚Äôs last layer\nachieves better performance than layer-wise distil-\n2141\nlation. In summary, most previous work relies on\nself-attention distributions to perform KD training,\nwhich leads to a restriction that the number of at-\ntention heads of student model has to be the same\nas its teacher.\nIn this work, we generalize and simplify deep\nself-attention distillation of MINI LM (Wang et al.,\n2020) by using self-attention relation distillation.\nWe introduce multi-head self-attention relations\ncomputed by scaled dot-product of pairs of queries,\nkeys and values, which guides the student train-\ning. Taking query vectors as an example, in order\nto obtain queries of multiple relation heads, we\nÔ¨Årst concatenate query vectors of different atten-\ntion heads, and then split the concatenated vector\naccording to the desired number of relation heads.\nAfterwards, for teacher and student models with\ndifferent attention head numbers, we can align their\nqueries with the same number of relation heads for\ndistillation. Moreover, using a larger number of re-\nlation heads brings more Ô¨Åne-grained self-attention\nknowledge, which helps the student to achieves a\ndeeper mimicry of teacher‚Äôs self-attention module.\nIn addition, for large-size (24 layers, 1024 hidden\nsize) teachers, extensive experiments indicate that\ntransferring an upper middle layer tends to perform\nbetter than using the last layer as in MINI LM.\nExperimental results show that our monolingual\nmodels distilled from BERT and RoBERTa, and\nmultilingual models distilled from XLM-R outper-\nform state-of-the-art models in different parameter\nsizes. The 6√ó768 (6 layers, 768 hidden size) model\ndistilled from BERT LARGE is 2.0√ófaster, mean-\nwhile, performing better than BERT BASE. The\nbase-size model distilled from RoBERTaLARGE out-\nperforms RoBERTaBASE using much fewer training\nexamples.\nTo summarize, our contributions include:\n‚Ä¢ We generalize and simplify deep self-attention\ndistillation in MINI LM by introducing multi-\nhead self-attention relation distillation, which\nbrings more Ô¨Åne-grained self-attention knowl-\nedge and allows more Ô¨Çexibility for the num-\nber of student‚Äôs attention heads.\n‚Ä¢ We conduct extensive distillation experiments\non different large-size teachers and Ô¨Ånd that\nusing knowledge of a teacher‚Äôs upper middle\nlayer achieves better performance.\n‚Ä¢ Experimental results demonstrate the effec-\ntiveness of our method for different monolin-\ngual and multilingual teachers in base-size\nand large-size.\n2 Related Work\n2.1 Backbone Network: Transformer\nMulti-layer Transformer (Vaswani et al., 2017) has\nbeen widely adopted in pretrained models. Each\nTransformer layer consists of a self-attention sub-\nlayer and a position-wise fully connected feed-\nforward sub-layer.\nSelf-Attention Transformer relies on multi-head\nself-attention to capture dependencies between\nwords. Given previous Transformer layer‚Äôs out-\nput Hl‚àí1 ‚ààR|x|√ódh , the output of a self-attention\nhead Ol,a, a‚àà[1,Ah] is computed via:\nQl,a = Hl‚àí1WQ\nl,a (1)\nKl,a = Hl‚àí1WK\nl,a (2)\nVl,a = Hl‚àí1WV\nl,a (3)\nOl,a = softmax(\nQl,aK‚ä∫\nl,a‚àödk\n)Vl,a (4)\nPrevious layer‚Äôs output is linearly projected to\nqueries, keys and values using parameter matrices\nWQ\nl,a,WK\nl,a,WV\nl,a ‚àà Rdh√ódk , respectively. The\nself-attention distributions are computed via scaled\ndot-product of queries and keys. These weights are\nassigned to the corresponding value vectors to ob-\ntain the attention output. |x|represents the length\nof input sequence. Ah and dh indicate the num-\nber of attention heads and hidden size. dk is the\nattention head size. dk√óAh is usually equal to dh.\n2.2 Pretrained Language Models\nPre-training has led to strong improvements across\na variety of natural language processing tasks.\nPretrained language models are learned on large\namounts of text data, and then Ô¨Åne-tuned to adapt\nto speciÔ¨Åc tasks. BERT (Devlin et al., 2018) pro-\nposes to pretrain a deep bidirectional Transformer\nusing masked language modeling (MLM) objective.\nUNILM (Dong et al., 2019) is jointly pretrained\non three types language modeling objectives to\nadapt to both understanding and generation tasks.\nRoBERTa (Liu et al., 2019) achieves strong perfor-\nmance by training longer steps using large batch\nsize and more text data. MASS (Song et al., 2019),\nT5 (Raffel et al., 2019) and BART (Lewis et al.,\n2142\n‚Ñé1\nùëô‚àí1 ‚Ñé2\nùëô‚àí1 ‚Ñé3\nùëô‚àí1 ‚Ñé4\nùëô‚àí1 ‚Ñé5\nùëô‚àí1\nQueries Keys Values\nLinear\nConcatenate\nSplit\n‚Ä¶\nQ-Q Att-Rel K-K Att-Rel V-V Att-Rel‚Ä¶\nHidden States\n(‚Ñùùë• √óùëë‚Ñé)\nMulti-Head\nAttention Vectors\n(‚Ñùùê¥‚Ñé√ó ùë• √óùëëùëò)\nMulti-Head\nRelation Vectors\n(‚Ñùùê¥ùëü√ó ùë• √óùëëùëü)\nSelf-Attention\nRelations\n(‚Ñùùê¥ùëü√ó ùë• √ó|ùë•|)\nTeacher (Large Model)\nStudent (Small Model)\nQ-Q Att-Rel K-K Att-Rel V-V Att-Rel\nQ-Q Att-Rel K-K Att-Rel V-V Att-Rel\nSelf-Attention Relations (Last or Upper-Middle Layer)\nSelf-Attention Relations (Last Layer)\nRelation Transfer\n(KL-Divergence)\nInput Embedding\nMulti-Head\nSelf-Attention\nFeed Forward\nAdd & Norm\nAdd & Norm\nùêøx\nInput Embedding\nMulti-Head\nSelf-Attention\nFeed Forward\nAdd & Norm\nAdd & Norm\nùëÄx\nDot-Product\nFigure 1: Overview of multi-head self-attention relation distillation. We introduce multi-head self-attention rela-\ntions computed by scaled dot-product of pairs of queries, keys and values to guide the training of students. In order\nto obtain vectors (queries, keys and values) of multiple relation heads, we Ô¨Årst concatenate self-attention vectors of\ndifferent attention heads and then split them according to the desired number of relation heads. We choose to trans-\nfer Q-Q, K-K and V-V self-attention relations to achieve a balance between performance and training speed. For\nlarge-size teacher, we transfer the self-attention knowledge of an upper middle layer of the teacher. For base-size\nteacher, using the last layer achieves better performance.\n2019a) employ a standard encoder-decoder struc-\nture and pretrain the decoder auto-regressively. Be-\nsides monolingual pretrained models, multilingual\npretrained models (Devlin et al., 2018; Lample and\nConneau, 2019; Chi et al., 2019; Conneau et al.,\n2019; Chi et al., 2020) also advance the state-of-the-\nart on cross-lingual understanding and generation.\n2.3 Knowledge Distillation\nKnowledge distillation has been proven to be a\npromising way to compress large models while\nmaintaining accuracy. Knowledge of a single or an\nensemble of large models is used to guide the train-\ning of small models. Hinton et al. (2015) propose\nto use soft target probabilities to train student mod-\nels. More Ô¨Åne-grained knowledge such as hidden\nstates (Romero et al., 2015) and attention distribu-\ntions (Zagoruyko and Komodakis, 2017; Hu et al.,\n2018) are introduced to improve the student model.\nIn this work, we focus on task-agnostic knowl-\nedge distillation of pretrained Transformers. The\ndistilled task-agnostic model can be Ô¨Åne-tuned to\nadapt to downstream tasks. It can also be utilized to\ninitialize task-speciÔ¨Åc distillation (Sun et al., 2019a;\nTurc et al., 2019; Aguilar et al., 2019; Mukher-\njee and Awadallah, 2020; Xu et al., 2020; Hou\net al., 2020; Li et al., 2020), which uses a Ô¨Åne-\ntuned teacher model to guide the training of the\nstudent on speciÔ¨Åc tasks. Knowledge used for dis-\ntillation and layer mapping function are two key\npoints for task-agnostic distillation of pretrained\nTransformers. Most previous work uses soft target\nprobabilities, hidden states, self-attention distribu-\ntions and value-relation to train the student model.\nFor the layer mapping function, TinyBERT (Jiao\net al., 2019) uses a uniform strategy to map teacher\nand student layers. MobileBERT (Sun et al., 2019b)\nassumes the student has the same number of lay-\ners as its teacher to perform layer-wise distilla-\ntion. MINI LM (Wang et al., 2020) transfers self-\nattention knowledge of teacher‚Äôs last layer to the\nstudent last Transformer layer. Different from\nprevious work, our method uses multi-head self-\nattention relations to eliminate the restriction on the\nnumber of student‚Äôs attention heads. Moreover, we\nshow that transferring the self-attention knowledge\nof an upper middle layer of the large-size teacher\nmodel is more effective.\n3 Multi-Head Self-Attention Relation\nDistillation\nFollowing MINI LM (Wang et al., 2020), the key\nidea of our approach is to deeply mimic teacher‚Äôs\nself-attention module, which draws dependencies\nbetween words and is the vital component of Trans-\nformer. MINI LM uses teacher‚Äôs self-attention dis-\ntributions to train the student model. It brings\n2143\nModel Teacher #Param SpeedupSQuAD2MNLI-m QNLI QQP RTE SST MRPC CoLAAvg\nBERTBASE - 109M √ó1.0 76.8 84.5 91.7 91.3 68.6 93.2 87.3 58.9 81.5\nRoBERTaBASE - 125M √ó1.0 83.7 87.6 92.8 91.9 78.7 94.8 90.2 63.6 85.4\nBERTSMALL - 66M √ó2.0 73.2 81.8 89.8 90.6 67.9 91.2 84.9 53.5 79.1\nTruncated BERTBASE - 66M √ó2.0 69.9 81.2 87.9 90.4 65.5 90.8 82.7 41.4 76.2\nTruncated RoBERTaBASE - 81M √ó2.0 77.9 84.9 91.1 91.3 67.9 92.9 87.5 55.2 81.1\nDistilBERT BERTBASE 66M √ó2.0 70.7 82.2 89.2 88.5 59.9 91.3 87.5 51.3 77.6\nTinyBERT BERTBASE 66M √ó2.0 73.1 83.5 90.5 90.6 72.2 91.6 88.4 42.8 79.1\nMINILM BERTBASE 66M √ó2.0 76.4 84.0 91.0 91.0 71.5 92.0 88.4 49.2 80.4\n6√ó384Ours BERTBASE 22M √ó5.3 72.9 82.8 90.3 90.6 68.9 91.3 86.6 41.8 78.2\n6√ó384Ours BERTLARGE 22M √ó5.3 74.3 83.0 90.4 90.7 68.5 91.1 87.8 41.6 78.4\n6√ó384Ours RoBERTaLARGE 30M √ó5.3 76.4 84.4 90.9 90.8 69.9 92.0 88.7 42.6 79.5\n6√ó768Ours BERTBASE 66M √ó2.0 76.3 84.2 90.8 91.1 72.1 92.4 88.9 52.5 81.0\n6√ó768Ours BERTLARGE 66M √ó2.0 77.7 85.0 91.4 91.1 73.0 92.5 88.9 53.9 81.7\n6√ó768Ours RoBERTaLARGE 81M √ó2.0 81.6 87.0 92.7 91.4 78.7 94.5 90.4 54.0 83.8\nTable 1: Results of our students distilled from base-size and large-size teachers on the development sets of GLUE\nand SQuAD 2.0. We report F1 for SQuAD 2.0, Matthews correlation coefÔ¨Åcient for CoLA, and accuracy for other\ndatasets. The GLUE results of DistilBERT are taken from Sanh et al. (2019). The rest results of DistilBERT, Tiny-\nBERT2, BERTSMALL, Truncated BERTBASE and MINI LM are taken from Wang et al. (2020). BERT SMALL (Turc\net al., 2019) is trained using the MLM objective, without using KD. We also report the results of truncated\nBERTBASE and truncated RoBERTa BASE , which drops the top 6 layers of the base model. Top-layer dropping\nhas been proven to be a strong baseline (Sajjad et al., 2020). The Ô¨Åne-tuning results are an average of 4 runs.\nthe restriction on the number of attention heads\nof students, which is required to be the same as\nits teacher. To introduce more Ô¨Åne-grained self-\nattention knowledge and avoid using teacher‚Äôs\nself-attention distributions, we generalize deep\nself-attention distillation in MINI LM and intro-\nduce multi-head self-attention relations of pairs\nof queries, keys and values to train the student. Be-\nsides, we conduct extensive experiments and Ô¨Ånd\nthat layer selection of the teacher model is critical\nfor distilling large-size models. Figure 1 gives an\noverview of our method.\n3.1 Multi-Head Self-Attention Relations\nMulti-head self-attention relations are obtained by\nscaled dot-product of pairs3 of queries, keys and\nvalues of multiple relation heads. Taking query\nvectors as an example, in order to obtain queries\nof multiple relation heads, we Ô¨Årst concatenate\nqueries of different attention heads and then split\nthe concatenated vector based on the desired num-\nber of relation heads. The same operation is also\nperformed on keys and values. For teacher and\nstudent models which uses different number of at-\ntention heads, we convert their queries, keys and\nvalues of different number of attention heads into\n2In addition to task-agnostic distillation, TinyBERT uses\ntask-speciÔ¨Åc distillation and data augmentation to further im-\nprove the model. We report the Ô¨Åne-tuning results of their\npublic task-agnostic model.\n3There are nine types of self-attention relations, such as\nquery-query, key-key, key-value and query-value relations.\nvectors of the same number of relation heads to\nperform KD training. Our method eliminates the\nrestriction on the number of attention heads of stu-\ndent models. Moreover, using more relation heads\nin computing self-attention relations brings more\nÔ¨Åne-grained self-attention knowledge and improves\nthe performance of the student model.\nWe use A1,A2,A3 to denote the queries, keys\nand values of multiple relation heads. The KL-\ndivergence between multi-head self-attention re-\nlations of the teacher and student is used as the\ntraining objective:\nL=\n3‚àë\ni=1\n3‚àë\nj=1\nŒ±ijLij (5)\nLij = 1\nAr|x|\nAr‚àë\na=1\n|x|‚àë\nt=1\nDKL(RT\nij,l,a,t ‚à•RS\nij,m,a,t)\n(6)\nRT\nij,l,a = softmax(\nAT\ni,l,aAT‚ä∫\nj,l,a‚àödr\n) (7)\nRS\nij,m,a = softmax(\nAS\ni,m,aAS‚ä∫\nj,m,a‚àö\nd‚Ä≤r\n) (8)\nwhere AT\ni,l,a ‚ààR|x|√ódr and AS\ni,m,a ‚ààR|x|√ód‚Ä≤\nr (i‚àà\n[1,3]) are the queries, keys and values of a relation\nhead of l-th teacher layer and m-th student layer.\ndr and d‚Ä≤\nr are the relation head size of teacher and\nstudent models. RT\nij,l ‚ààRAr√ó|x|√ó|x| is the self-\nattention relation of AT\ni,l and AT\nj,l of teacher model.\n2144\nModel Teacher#Param SpeedupSQuAD2MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STSAvg\nBERTBASE - 109M 1.0√ó 76.8 84.6/83.4 90.5 71.2 66.4 93.5 88.9 52.1 85.8 79.3\nBERTLARGE - 340M 0.3√ó 81.9 86.7/85.9 92.7 72.1 70.1 94.9 89.3 60.5 86.5 82.1\n6√ó768Ours BERTBASE 66M 2.0 √ó 76.3 83.8/83.3 90.2 70.9 69.2 92.9 89.1 46.6 84.3 78.7\n6√ó768OursBERTLARGE 66M 2.0 √ó 77.7 84.5/84.0 91.5 71.3 69.2 93.0 89.1 48.6 85.179.4\nTable 2: Results of our 6√ó768 students distilled form BERT on GLUE test sets and SQuAD 2.0 dev set. The\nreported results are directly Ô¨Åne-tuned on downstream tasks. We report F1 for SQuAD 2.0, QQP and MRPC,\nSpearman correlation for STS-B, Matthews correlation coefÔ¨Åcient for CoLA and accuracy for the rest.\nRT\nij,l,a ‚ààR|x|√ó|x|is the self-attention relation of\na teacher‚Äôs relation head. RS\nij,m ‚ààRAr√ó|x|√ó|x|is\nthe self-attention relation of student model. For\nexample, RT\n11,l represents teacher‚Äôs Q-Q attention\nrelation in Figure 1. Ar is the number of relation\nheads. If the number of relation heads and attention\nheads is the same, the Q-K relation is equivalent\nto the attention weights in self-attention module.\nŒ±ij ‚àà{0,1}is the weight assigned to each self-\nattention relation loss. We transfer query-query,\nkey-key and value-value self-attention relations to\nbalance the performance and training cost.\n3.2 Layer Selection of Teacher Model\nBesides the knowledge used for distillation, map-\nping function between teacher and student layers\nis another key factor. As in MINI LM, we only\ntransfer the self-attention knowledge of one of the\nteacher layers to the student last layer. Only distill-\ning one layer of the teacher model is fast and effec-\ntive. Different from previous work which usually\nconducts experiments on base-size teachers, we ex-\nperiment with different large-size teachers and Ô¨Ånd\nthat transferring self-attention knowledge of an up-\nper middle layer performs better than using other\nlayers. For BERT LARGE and BERTLARGE-WWM,\ntransferring the 21-th (start at one) layer achieves\nthe best performance. For RoBERTa LARGE and\nXLM-RLARGE, using the self-attention knowledge\nof 19-th layer achieves better performance. For the\nbase-size teacher, we also Ô¨Ånd that using teacher‚Äôs\nlast layer performs better.\n4 Experiments\nWe conduct distillation experiments on different\nteacher models including BERTBASE, BERTLARGE,\nBERTLARGE-WWM, RoBERTaBASE , RoBERTaLARGE ,\nXLM-RBASE and XLM-RLARGE.\n4.1 Setup\nWe use the uncased version for three BERT teacher\nmodels. For the pre-training data, we use English\nWikipedia and BookCorpus (Zhu et al., 2015). We\ntrain student models using256 as the batch size and\n6e-4 as the peak learning rate for 400,000 steps.\nWe use linear warmup over the Ô¨Årst 4,000 steps\nand linear decay. We use Adam (Kingma and Ba,\n2015) with Œ≤1 = 0.9, Œ≤2 = 0.999. The maximum\nsequence length is set to 512. The dropout rate\nand weight decay are 0.1 and 0.01. The number of\nattention heads is 12 for all student models. The\nnumber of relation heads is 48 and 64 for base-size\nand large-size teacher model, respectively. The\nstudent models are initialized randomly.\nFor models distilled from RoBERTa, we use sim-\nilar pre-training datasets as in Liu et al. (2019).\nFor the 12√ó768 student model, we use Adam with\nŒ≤1 = 0.9, Œ≤2 = 0.98. The rest hyper-parameters\nare the same as models distilled from BERT.\nFor multilingual student models distilled from\nXLM-R, we perform training using the same\ndatasets as in Conneau et al. (2019) for 1,000,000\nsteps. We conduct distillation experiments using 8\nV100 GPUs with mixed precision training.\n4.2 Downstream Tasks\nFollowing previous pre-training (Devlin et al.,\n2018; Liu et al., 2019; Conneau et al., 2019) and\ntask-agnostic distillation (Sun et al., 2019b; Jiao\net al., 2019) work, we evaluate the English stu-\ndent models on GLUE benchmark and extractive\nquestion answering. The multilingual models are\nevaluated on cross-lingual natural language infer-\nence and cross-lingual question answering.\nGLUE General Language Understanding Eval-\nuation (GLUE) benchmark (Wang et al., 2019)\nconsists of two single-sentence classiÔ¨Åcation tasks\n(SST-2 (Socher et al., 2013) and CoLA (Warstadt\net al., 2018)), three similarity and paraphrase tasks\n(MRPC (Dolan and Brockett, 2005), STS-B (Cer\net al., 2017) and QQP), and four inference tasks\n(MNLI (Williams et al., 2018), QNLI (Rajpurkar\net al., 2016), RTE (Dagan et al., 2006; Bar-Haim\net al., 2006; Giampiccolo et al., 2007; Bentivogli\n2145\nModel Teacher #ParamSQuAD2MNLI QNLI QQP RTE SST MRPC CoLA STSAvg\nBERTBASE - 109M 76.8 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 82.4\nRoBERTaBASE - 125M 83.7 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2 86.1\n12√ó768Ours BERTLARGE 109M 81.8 86.5 92.6 91.6 76.4 93.3 89.2 62.3 90.5 84.9\n12√ó768Ours RoBERTaLARGE 125M 86.6 89.4 94.0 91.8 83.1 95.9 91.2 65.0 91.3 87.6\nTable 3: Results of our 12√ó768 models on the dev sets of GLUE benchmark and SQuAD 2.0. The Ô¨Åne-tuning\nresults are an average of 4 runs for each task. We report F1 for SQuAD 2.0, Pearson correlation for STS-B,\nMatthews correlation coefÔ¨Åcient for CoLA and accuracy for the rest.\nModel SQuAD2 MNLI-m SST-2Avg\nMINILM (Last Layer)79.1 84.7 91.2 85.0\n+ Upper Middle Layer80.3 85.2 91.5 85.7\n12√ó384Ours 80.7 85.7 92.3 86.2\nTable 4: Comparison of different methods using\nBERTLARGE-WWM as the teacher. We report dev results\nof 12√ó384 student model with 128 embedding size.\net al., 2009) and WNLI (Levesque et al., 2012)).\nExtractive Question Answering The task aims\nto predict a continuous sub-span of the passage\nto answer the question. We evaluate on SQuAD\n2.0 (Rajpurkar et al., 2018), which has been served\nas a major question answering benchmark.\nCross-lingual Natural Language Inference\n(XNLI) XNLI (Conneau et al., 2018) is a\ncross-lingual classiÔ¨Åcation benchmark. It aims to\nidentity the semantic relationship between two\nsentences and provides instances in 15 languages.\nCross-lingual Question Answering We use\nMLQA (Lewis et al., 2019b) to evaluate multi-\nlingual models. MLQA extends English SQuAD\ndataset (Rajpurkar et al., 2016) to seven languages.\n4.3 Main Results\nTable 1 presents the dev results of 6√ó384\nand 6√ó768 models distilled from BERT BASE,\nBERTLARGE and RoBERTaLARGE on GLUE and\nSQuAD 2.0. (1) Previous methods (Sanh et al.,\n2019; Jiao et al., 2019; Sun et al., 2019a; Wang\net al., 2020) usually distill BERTBASE into a 6-layer\nmodel with 768 hidden size. We Ô¨Årst report re-\nsults of the same setting. Our 6√ó768 model outper-\nforms DistilBERT, TinyBERT,MINI LM and two\nBERT baselines across most tasks. Moreover, our\nmethod allows more Ô¨Çexibility for the number of\nattention heads of student models. (2) Both 6√ó384\nand 6√ó768 models distilled from BERTLARGE out-\nperform models distilled from BERT BASE. The\n6√ó768 model distilled from BERTLARGE is 2.0√ó\nfaster than BERT BASE, while achieving better\nperformance. (3) Student models distilled from\nRoBERTaLARGE achieve further improvements. Bet-\nter teacher results in better students. Multi-head\nself-attention relation distillation is effective for\ndifferent large-size pretrained Transformers.\nWe report the results of 6√ó768 students distilled\nfrom BERTBASE and BERTLARGE on GLUE test\nsets and SQuAD 2.0 dev set in Table 2. 6√ó768\nmodel distilled from BERTBASE retains more than\n99% accuracy of its teacher while using50% Trans-\nformer parameters. 6√ó768 model distilled from\nBERTLARGE compares favorably with BERTBASE.\nWe compress RoBERTaLARGE and BERTLARGE\ninto a base-size student model. Dev results of\nGLUE benchmark and SQuAD 2.0 are shown\nin Table 3. Our base-size models distilled from\nlarge-size teacher outperforms BERT BASE and\nRoBERTaBASE . Our method can be adopted to train\nstudents in different parameter size. Moreover, our\nstudent distilled from RoBERTaLARGE uses a much\nsmaller (almost 32√ósmaller) training batch size\nand fewer training steps than RoBERTaBASE . Our\nmethod requires much fewer training examples.\nMost of previous work conducts experiments\nusing base-size teachers. To compare with previ-\nous methods on large-size teacher, we reimplement\nMINI LM and compress BERTLARGE-WWM into a\n12√ó384 student model. Dev results of SQuAD 2.0,\nMNLI-m and SST-2 are presented in Table 4. Our\nmethod also outperforms MINI LM for large-size\nteachers. Moreover, we report results of distill-\ning an upper middle layer instead of the last layer\nfor MINI LM. Layer selection is also effective for\nMINI LM when distilling large-size teachers.\nTable 5 and Table 6 show the results of our stu-\ndent models distilled from XLM-R on XNLI and\nMLQA. For XNLI, the best single model is se-\nlected on the joint dev set of all the languages as\nin Conneau et al. (2019). Following Lewis et al.\n(2019b), we adopt SQuAD 1.1 as training data and\nevaluate on MLQA English development set for\n2146\nModel Teacher#L #H #Paramen fr es de el bg ru tr ar vi th zh hi sw ur Avg\nmBERT - 12 768 170M 82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.066.3XLM-100 - 16 1280 570M83.2 76.7 77.7 74.0 72.7 74.1 72.7 68.7 68.6 72.9 68.9 72.5 65.6 58.2 62.470.7XLM-RBASE - 12 768 277M 85.8 79.7 80.7 78.7 77.5 79.6 78.1 74.2 73.8 76.5 74.6 76.7 72.4 66.5 68.376.2MINILM XLM-RBASE 6 384 107M 79.2 72.3 73.1 70.3 69.1 72.0 69.1 64.5 64.9 69.0 66.0 67.8 62.9 59.0 60.668.0\n6√ó384Ours XLM-RBASE 6 384 107M 78.1 71.4 72.7 69.2 70.8 72.2 69.9 67.5 66.1 68.9 67.5 68.2 64.7 62.7 62.368.86√ó384OursXLM-RLARGE6 384 107M 79.8 72.5 73.7 69.3 70.6 72.3 69.5 66.9 67.5 69.1 67.0 68.7 64.8 62.4 63.069.1\nTable 5: Cross-lingual classiÔ¨Åcation results of our 6√ó384 multilingual models on XNLI. We report the accuracy\non each of the 15 XNLI languages and the average accuracy. #L and #H indicate the number of layers and hidden\nsize.\nModel Teacher#L #H #Paramen es de ar hi vi zh Avg\nmBERT - 12 768 170M 77.7 / 65.2 64.3 / 46.6 57.9 / 44.3 45.7 / 29.8 43.8 / 29.7 57.1 / 38.6 57.5 / 37.357.7 / 41.6XLM-15 - 12 1024 248M74.9 / 62.4 68.0 / 49.8 62.2 / 47.6 54.8 / 36.3 48.8 / 27.3 61.4 / 41.8 61.1 / 39.661.6 / 43.5XLM-RBASE - 12 768 277M 77.1 / 64.6 67.4 / 49.6 60.9 / 46.7 54.9 / 36.6 59.4 / 42.9 64.5 / 44.7 61.8 / 39.363.7 / 46.3MINILM XLM-RBASE 6 384 107M 75.5 / 61.9 55.6 / 38.2 53.3 / 37.7 43.5 / 26.2 46.9 / 31.5 52.0 / 33.1 48.8 / 27.353.7 / 36.6\n6√ó384Ours XLM-RBASE 6 384 107M 76.0 / 62.5 60.5 / 42.4 57.7 / 43.1 48.6 / 30.1 53.3 / 36.5 55.5 / 35.6 54.6 / 32.558.0 / 40.46√ó384OursXLM-RLARGE6 384 107M 76.2 / 62.9 59.2 / 41.7 57.4 / 42.2 47.3 / 29.4 54.1 / 36.9 58.2 / 37.9 57.0 / 34.058.5/40.7\nTable 6: Cross-lingual question answering results of our 6√ó384 multilingual models on MLQA. We report the F1\nand EM (exact match) scores on each of the 7 MLQA languages. #L and #H indicate the number of layers and\nhidden size.\nModel SQuAD2 MNLI-m SST-2Avg\nOurs (Q-Q + K-K + V-V)72.8 82.2 91.5 82.2\n‚Äì Q-Q Att-Rel 71.6 81.9 90.6 81.4\n‚Äì K-K Att-Rel 71.9 81.9 90.5 81.4\n‚Äì V-V Att-Rel 71.5 81.6 90.5 81.2\nQ-K + V-V Att-Rels72.4 82.2 91.0 81.9\nTable 7: Ablation studies of different self-attention re-\nlations. We report results of 6√ó384 student model dis-\ntilled from BERTBASE. The relation head number is 12.\n#Relation Heads 6 12 24 48\n6√ó384model distilled from RoBERTaBASE\nMNLI-m 82.8 82.9 83.0 83.4\nSQuAD 2.0 74.5 75.0 74.975.7\n6√ó384model distilled from BERTBASE\nMNLI-m 81.9 82.2 82.2 82.4\nSQuAD 2.0 71.9 72.8 72.773.0\nTable 8: Results of 6√ó384 model using different num-\nber of relation heads.\nearly stopping. Our 6√ó384 model outperforms\nmBERT (Devlin et al., 2018) with 5.3√óspeedup.\nOur method also performs better than MINI LM,\nwhich further validates the effectiveness of multi-\nhead self-attention relation distillation. Transfer-\nring multi-head self-attention relations can bring\nmore Ô¨Åne-grained self-attention knowledge.\n4.4 Ablation Studies\nEffect of using different self-attention relations\nWe perform ablation studies to analyse the con-\ntribution of different self-attention relations. Dev\nresults of three tasks are illustrated in Table 7. Q-\nQ, K-K and V-V self-attention relations positively\ncontribute to the Ô¨Ånal results. Besides, we also\ncompare Q-Q + K-K + V-V with Q-K + V-V given\nqueries and keys are employed to compute self-\nattention distributions in self-attention module. Ex-\nperimental result shows that using Q-Q + K-K +\nV-V achieves better performance.\nEffect of distilling different teacher layersFig-\nure 2 presents the results of 6√ó384 model distilled\nfrom different layers of BERTBASE, BERTLARGE\nand XLM-RLARGE. For BERTBASE, using the last\nlayer achieves better performance than other layers.\nFor BERTLARGE and XLM-RLARGE, we Ô¨Ånd that\nusing one of the upper middle layers achieves the\nbest performance. The same trend is also observed\nfor BERTLARGE-WWM and RoBERTaLARGE .\nEffect of different number of relation heads\nTable 8 shows the results of 6√ó384 model distilled\nfrom BERTBASE and RoBERTaBASE using different\nnumber of relation heads. Using a larger number of\nrelation heads achieves better performance. More\nÔ¨Åne-grained self-attention knowledge can be cap-\ntured by using more relation heads, which helps the\nstudent to deeply mimic the self-attention module\nof its teacher. Besides, we Ô¨Ånd that the number of\nrelation heads is not required to be a positive multi-\nple of both the number of student and teacher atten-\ntion heads. The relation head can be a fragment of\na single attention head or contains fragments from\n2147\nModel Teacher #Param SpeedupSQuAD2MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STSAvg\nBERTBASE - 109M 1.0√ó 76.8 84.6/83.4 90.5 71.2 66.4 93.5 88.9 52.1 85.8 79.3\nMobileBERT IB-BERTLARGE 25M 1.8 √ó 80.2 84.3/83.4 91.6 70.5 70.4 92.6 88.8 51.1 84.879.812√ó384Ours BERTLARGE-WWM25M 2.7 √ó 80.7 85.9/84.6 91.9 71.4 71.9 93.3 89.2 44.9 85.5 79.9+ More Att-RelsBERTLARGE-WWM25M 2.7 √ó 80.9 85.8/84.8 92.3 71.6 72.0 93.6 89.746.6 86.080.3\nTable 9: Comparison between MobileBERT and the same-size model (12 layers, 384 hidden size and 128 embed-\nding size) distilled form BERTLARGE (Whole Word Masking) on GLUE test sets and SQuAD 2.0 dev set. Following\nMobileBERT (Sun et al., 2019b), the reported results are directly Ô¨Åne-tuned on downstream tasks. We compute\nthe speedup of MobileBERT according to their reported latency.\n(a) BERTBASE as the teacher (b) BERT LARGE as the teacher (c) XLM-R LARGE as the teacher\nFigure 2: 6√ó384 models trained using different BERTBASE (a), BERTLARGE (b) and XLM-RLARGE (c) layers.\nModel Teacher SQuAD2 MNLI-m SST-2\n6√ó384Ours BERTBASE 72.9 82.8 91.3+ More Att-RelsBERTBASE 73.3 82.8 91.6\n6√ó384Ours BERTLARGE 74.3 83.0 91.1+ More Att-RelsBERTLARGE 74.7 83.2 92.4\n6√ó384Ours RoBERTaLARGE 76.4 84.4 92.0+ More Att-RelsRoBERTaLARGE 76.0 84.4 92.1\n6√ó768Ours BERTBASE 76.3 84.2 92.4+ More Att-RelsBERTBASE 76.8 84.4 92.3\n6√ó768Ours BERTLARGE 77.7 85.0 92.5+ More Att-RelsBERTLARGE 78.1 85.2 92.5\n6√ó768Ours RoBERTaLARGE 81.6 87.0 94.5+ More Att-RelsRoBERTaLARGE 81.2 87.3 94.1\nTable 10: Results of introducing more self-attention re-\nlations (Q-K, K-Q, Q-V , V-Q, K-V and V-K relations).\nmultiple attention heads.\n5 Discussion\n5.1 Comparison with MobileBERT\nMobileBERT (Sun et al., 2019b) compresses a spe-\ncially designed teacher model (in the BERTLARGE\nsize) with inverted bottleneck modules into a 24-\nlayer student using the bottleneck modules. Since\nour goal is to compress different large models (e.g.\nBERT and RoBERTa) to small models using stan-\ndard Transformer architecture, we note that our stu-\ndent model can not directly compare with Mobile-\nBERT. We provide results of a student model with\nthe same parameter size for a reference. A public\nlarge-size model (BERTLARGE-WWM) is used as the\nteacher, which achieves similar performance as Mo-\nbileBERT‚Äôs teacher. We distill BERTLARGE-WWM\ninto a student model (25M parameters) using the\nsame training data (i.e., English Wikipedia and\nBookCorpus). The test results of GLUE and dev\nresult of SQuAD 2.0 are illustrated in Table 9.\nOur model outperforms MobileBERT across most\ntasks with a faster inference speed. Moreover, our\nmethod can be applied for different teachers and\nhas much fewer restriction of students.\nWe also observe that our model performs rela-\ntively worse on CoLA compared with MobileBERT.\nThe task of CoLA is to evaluate the grammati-\ncal acceptability of a sentence. It requires more\nÔ¨Åne-grained linguistic knowledge that can be learnt\nfrom language modeling objectives. Fine-tuning\nthe model using the MLM objective as in Mobile-\nBERT brings improvements for CoLA. However,\nour preliminary experiments show that this strategy\nwill lead to slight drop for other GLUE tasks.\n5.2 Results of More Self-Attention Relations\nIn Table 9 and 10, we report results of students\ntrained using more self-attention relations (Q-K, K-\nQ, Q-V , V-Q, K-V and V-K relations). We observe\nimprovements across most tasks, especially for stu-\ndent models distilled from BERT. Fine-grained self-\nattention knowledge in more attention relations im-\nproves our students. However, introducing more\nself-attention relations also brings a higher compu-\n2148\ntational cost. In order to achieve a balance between\nperformance and computational cost, we choose to\ntransfer Q-Q, K-K and V-V self-attention relations\ninstead of all self-attention relations in this work.\n6 Conclusion\nWe generalize deep self-attention distillation in\nMINI LM by employing multi-head self-attention\nrelations to train the student. Our method intro-\nduces more Ô¨Åne-grained self-attention knowledge\nand eliminates the restriction of the number of stu-\ndent‚Äôs attention heads. Moreover, we show that\ntransferring the self-attention knowledge of an up-\nper middle layer achieves better performance for\nlarge-size teachers. Our monolingual and multilin-\ngual models distilled from BERT, RoBERTa and\nXLM-R obtain competitive performance and out-\nperform state-of-the-art methods. For future work,\nwe are exploring an automatic layer selection algo-\nrithm. We also would like to apply our method to\nlarger pretrained Transformers.\nReferences\nGustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao,\nXing Fan, and Edward Guo. 2019. Knowledge\ndistillation from internal representations. CoRR,\nabs/1910.03723.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-\nfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUnilmv2: Pseudo-masked language models for uni-\nÔ¨Åed language model pre-training. arXiv preprint\narXiv:2002.12804.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and\nDanilo Giampiccolo. 2006. The second PASCAL\nrecognising textual entailment challenge. In Pro-\nceedings of the Second PASCAL Challenges Work-\nshop on Recognising Textual Entailment.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nÔ¨Åfth pascal recognizing textual entailment challenge.\nIn In Proc Text Analysis Conference (TAC‚Äô09.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nling Mao, and Heyan Huang. 2019. Cross-lingual\nnatural language generation via pre-training. CoRR,\nabs/1909.10481.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-\nsham Singhal, Wenhui Wang, Xia Song, Xian-\nLing Mao, Heyan Huang, and Ming Zhou. 2020.\nInfoxlm: An information-theoretic framework for\ncross-lingual language model pre-training. CoRR,\nabs/2007.07834.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Ad-\nina Williams, Samuel R Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP).\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment\nchallenge. In Proceedings of the First Inter-\nnational Conference on Machine Learning Chal-\nlenges: Evaluating Predictive Uncertainty Visual\nObject ClassiÔ¨Åcation, and Recognizing Textual En-\ntailment, MLCW‚Äô05, pages 177‚Äì190, Berlin, Hei-\ndelberg. Springer-Verlag.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. In 33rd Conference on Neural\nInformation Processing Systems (NeurIPS 2019).\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pages 1‚Äì9, Prague. Association\nfor Computational Linguistics.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic\nBERT with adaptive width and depth. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\n2149\nMinghao Hu, Yuxing Peng, Furu Wei, Zhen Huang,\nDongsheng Li, Nan Yang, and Ming Zhou. 2018.\nAttention-guided answer distillation for machine\nreading comprehension. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 2077‚Äì2086.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling BERT for natural lan-\nguage understanding. CoRR, abs/1909.10351.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predict-\ning spans. arXiv preprint arXiv:1907.10529.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, San\nDiego, CA.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems (NeurIPS).\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019a. BART: Denoising sequence-to-sequence\npre-training for natural language generation, trans-\nlation, and comprehension. arXiv preprint\narXiv:1910.13461.\nPatrick S. H. Lewis, Barlas Oguz, Ruty Rinott, Sebas-\ntian Riedel, and Holger Schwenk. 2019b. MLQA:\nevaluating cross-lingual extractive question answer-\ning. CoRR, abs/1910.07475.\nJianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng\nXu, Min Yang, and Yaohong Jin. 2020. BERT-EMD:\nmany-to-many layer mapping for BERT compres-\nsion with earth mover‚Äôs distance. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 3009‚Äì3018. Associ-\nation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nSubhabrata Mukherjee and Ahmed Hassan Awadallah.\n2020. Xtremedistil: Multi-stage distillation for mas-\nsive multilingual models. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 2221‚Äì2234. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don‚Äôt know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 2: Short Papers , pages 784‚Äì\n789.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383‚Äì2392, Austin,\nTexas. Association for Computational Linguistics.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2015. Fitnets: Hints for thin deep nets. In\n3rd International Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and\nPreslav Nakov. 2020. Poor man‚Äôs BERT:\nsmaller and faster transformer models. CoRR,\nabs/2004.03844.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631‚Äì1642.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. arXiv preprint\narXiv:1905.02450.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019a.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019 , pages\n4322‚Äì4331.\n2150\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2019b. Mobile-\nbert: Task-agnostic compression of bert by progres-\nsive knowledge transfer.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical BERT models for sequence labeling.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 3630‚Äì\n3634.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. CoRR, abs/1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998‚Äì6008. Curran Asso-\nciates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In Advances in Neural\nInformation Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112‚Äì1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compress-\ning BERT by progressive module replacing. CoRR,\nabs/2002.02925.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In 33rd Conference on\nNeural Information Processing Systems (NeurIPS\n2019).\nCorpus #Train #Dev #Test Metrics\nSingle-Sentence Tasks\nCoLA 8.5k 1k 1k Matthews Corr\nSST-2 67k 872 1.8k Accuracy\nSimilarity and Paraphrase Tasks\nQQP 364k 40k 391k Accuracy/F1\nMRPC 3.7k 408 1.7k Accuracy/F1\nSTS-B 7k 1.5k 1.4k Pearson/Spearman Corr\nInference Tasks\nMNLI 393k 20k 20k Accuracy\nRTE 2.5k 276 3k Accuracy\nQNLI 105k 5.5k 5.5k Accuracy\nWNLI 634 71 146 Accuracy\nTable 11: Summary of the GLUE benchmark.\n#Train #Dev #Test Metrics\n130,319 11,873 8,862 Exact Match/F1\nTable 12: Dataset statistics and metrics of SQuAD 2.0.\nSergey Zagoruyko and Nikos Komodakis. 2017. Pay-\ning more attention to attention: Improving the per-\nformance of convolutional neural networks via at-\ntention transfer. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19‚Äì\n27.\nA GLUE Benchmark\nThe summary of datasets used for the General Lan-\nguage Understanding Evaluation (GLUE) bench-\nmark4 (Wang et al., 2019) is presented in Table 11.\nB SQuAD 2.0\nWe present the dataset statistics and metrics of\nSQuAD 2.05 (Rajpurkar et al., 2018) in Table 12.\nC Hyper-parameters for Fine-tuning\nExtractive Question Answering For SQuAD\n2.0, the maximum sequence length is 384. The\nbatch size is set to 32. We choose learning rates\nfrom {3e-5, 6e-5, 8e-5, 9e-5} and Ô¨Åne-tune the\nmodel for 3 epochs. The warmup ration and weight\ndecay is 0.1 and 0.01.\n4https://gluebenchmark.com/\n5http://stanford-qa.com\n2151\nGLUE The maximum sequence length is 128 for\nthe GLUE benchmark. We set batch size to 32,\nchoose learning rates from {1e-5, 1.5e-5, 2e-5, 3e-\n5, 5e-5} and epochs from { 3, 5, 10} for differ-\nent student models. We Ô¨Åne-tune CoLA task with\nlonger training steps (25 epochs). The warmup\nration and weight decay is 0.1 and 0.01.\nCross-lingual Natural Language Inference\n(XNLI) The maximum sequence length is 128\nfor XNLI. We Ô¨Åne-tune 5 epochs using 128 as the\nbatch size. The learning rates are chosen from\n{5e-5, 6e-5}.\nCross-lingual Question Answering For\nMLQA, the maximum sequence length is 512. We\nÔ¨Åne-tune 3 epochs using 32 as the batch size. The\nlearning rates are chosen from {5e-5, 6e-5}."
}