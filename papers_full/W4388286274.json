{
  "title": "Spatio-Temporal Context Graph Transformer Design for Map-Free Multi-Agent Trajectory Prediction",
  "url": "https://openalex.org/W4388286274",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2166308883",
      "name": "Zhongning Wang",
      "affiliations": [
        "Jilin University",
        "State Key Laboratory of Automotive Simulation and Control"
      ]
    },
    {
      "id": "https://openalex.org/A2129169043",
      "name": "Jianwei Zhang",
      "affiliations": [
        "State Key Laboratory of Automotive Simulation and Control",
        "Jilin University"
      ]
    },
    {
      "id": "https://openalex.org/A2098693886",
      "name": "Jicheng Chen",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A1992769770",
      "name": "Hui Zhang",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226239849",
    "https://openalex.org/W2997958396",
    "https://openalex.org/W2955189650",
    "https://openalex.org/W6769043036",
    "https://openalex.org/W3169575318",
    "https://openalex.org/W4386076672",
    "https://openalex.org/W3180491419",
    "https://openalex.org/W4383172002",
    "https://openalex.org/W3108486966",
    "https://openalex.org/W3034722190",
    "https://openalex.org/W4384284172",
    "https://openalex.org/W4290717520",
    "https://openalex.org/W2963759562",
    "https://openalex.org/W4376464702",
    "https://openalex.org/W3209837334",
    "https://openalex.org/W4366386347",
    "https://openalex.org/W3196864007",
    "https://openalex.org/W4381328400",
    "https://openalex.org/W4317038480",
    "https://openalex.org/W3199132857",
    "https://openalex.org/W4375928836",
    "https://openalex.org/W3016826426",
    "https://openalex.org/W4214895168",
    "https://openalex.org/W4292974216",
    "https://openalex.org/W4379116730",
    "https://openalex.org/W4382119404",
    "https://openalex.org/W4385338529",
    "https://openalex.org/W4378195072",
    "https://openalex.org/W4360993665",
    "https://openalex.org/W6853629474",
    "https://openalex.org/W4285102480",
    "https://openalex.org/W4312731878",
    "https://openalex.org/W4295934546",
    "https://openalex.org/W3214950490",
    "https://openalex.org/W3156216502",
    "https://openalex.org/W4281706128",
    "https://openalex.org/W3035731883",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2896642734",
    "https://openalex.org/W3114563362",
    "https://openalex.org/W2940129212",
    "https://openalex.org/W3107552218",
    "https://openalex.org/W4386958458",
    "https://openalex.org/W4389666389",
    "https://openalex.org/W4389538642",
    "https://openalex.org/W4386078146",
    "https://openalex.org/W6782468546",
    "https://openalex.org/W3204875639",
    "https://openalex.org/W3125605478",
    "https://openalex.org/W4386083117",
    "https://openalex.org/W3028769608",
    "https://openalex.org/W4381556596"
  ],
  "abstract": "Predicting the motion of surrounding vehicles is an important function of autonomous vehicles. However, most of the current state-of-the-art trajectory prediction models rely heavily on map information. In order to overcome the shortcomings of the existing models, our paper proposes a map-free trajectory prediction model and names it TR-Pred (Trajectory Relative two-stream Prediction). The trajectory stream employs LSTM to embedding the trajectory information of each agent. Subsequently, it utilizes graph neural networks (GNN) to extract latent traffic information in the current scenario, such as lane lines, drivable areas, and traffic control conditions. The relative stream utilizes temporal transformer to capture the local relative movement among agents. Subsequently, it employs GNN to extract the interaction information of all target agent. We augment the temporal transformer through refine initialization of its class token. This refined thereby enable enhanced modeling of inter-agent relative motion correlations between the agents. The decoder predicts the target agent by incorporating the historical interaction information among agents with latent traffic information. We validate TR-Pred on the Argoverse dataset, the highD dataset and the rounD dataset. The results show that TR-Pred performs better in the minimum Average Displacement Error compared to the main map-base model use in 2020, 2021. Experiments on Argoverse results show that our framework achieves a 16.3%/19.7%/24.4% improvement in minADE/minFDE(minimum Final Displacement Error)/MR(Miss Rate) compared to CRAT-Pred. The experimental results on highD and rounD show that, compared to the map-free version HiVT, our framework achieves improvements of 18.8%/18.8%/25.0% and 8.3%/8.5%/7.4% in minADE/minFDE/MR, respectively.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1\nSpatio-temporal Context Graph Transformer Design\nfor Map-free Multi-agent Trajectory Prediction\nZhongning Wang, Jianwei Zhang, Jicheng Chen, Member, IEEE, Hui Zhang, Senior Member, IEEE\nAbstract—Predicting the motion of surrounding vehicles is an\nimportant function of autonomous vehicles. However, most of the\ncurrent state-of-the-art trajectory prediction models rely heavily\non map information. In order to overcome the shortcomings of\nthe existing models, our paper proposes a map-free trajectory\nprediction model and names it TR-Pred (Trajectory Relative two-\nstream Prediction). The trajectory stream employs LSTM to em-\nbedding the trajectory information of each agent. Subsequently,\nit utilizes graph neural networks (GNN) to extract latent traffic\ninformation in the current scenario, such as lane lines, drivable\nareas, and traffic control conditions. The relative stream utilizes\ntemporal transformer to capture the local relative movement\namong agents. Subsequently, it employs GNN to extract the inter-\naction information of all target agent. We augment the temporal\ntransformer through refine initialization of its class token. This\nrefined thereby enable enhanced modeling of inter-agent relative\nmotion correlations between the agents. The decoder predicts the\ntarget agent by incorporating the historical interaction informa-\ntion among agents with latent traffic information. We validate\nTR-Pred on the Argoverse dataset, the highD dataset and the\nrounD dataset. The results show that TR-Pred performs better in\nthe minimum Average Displacement Error compared to the main\nmap-base model use in 2020, 2021. Experiments on Argoverse\nresults show that our framework achieves a 16.3%/19.7%/24.4%\nimprovement in minADE/minFDE(minimum Final Displacement\nError)/MR(Miss Rate) compared to CRAT-Pred. The exper-\nimental results on highD and rounD show that, compared\nto the map-free version HiVT, our framework achieves im-\nprovements of 18.8%/18.8%/25.0% and 8.3%/8.5%/7.4% in mi-\nnADE/minFDE/MR, respectively.\nIndex Terms—Autonomous driving, trajectory prediction, mo-\ntion forecasting, machine learning, deep learning, map-free tra-\njectory prediction\nI. I NTRODUCTION\nW\nITH the development and application of advanced\nautonomous vehicles (A Vs) technologies, safety and\nefficiency have become more and more important in A V\nsystems. In complex driving scenes such as intersections\nscenes, roundabouts scenes and highway on-ramp merging\nscenes, there are a large number of agents around A V . While\nencountering those scenarios, A V need to predict the future\ntrajectory of agents around them and speculate on multi-agent\ninteractions. However, because of uncertain future scenarios,\nagents might operate variably under the same scene [1], [2].\nThis work was supported by the National Natural Science Foundation of\nChina under Grant U1964201. (Corresponding author: Jianwei Zhang.)\nZhongning Wang and Jianwei Zhang are with the State Key Laboratory\nof Automotive Simulation and Control, Jilin University, Changchun 130025,\nChina (e-mail: wangzn21@mails.jlu.edu.cn; zhangjw@jlu.edu.cn).\nJicheng Chen and Hui Zhang are with the School of Transportation\nEngineering and Science, Beihang University, Beijing 100191, China (e-mail:\njichengc@buaa.edu.cn; huizhang285@buaa.edu.cn).\nHence, multimodal trajectory prediction (MTP) needs to gen-\nerate various feasible trajectories of an agent. By incorporating\nMTP, the performance of the planning module is greatly\nenhanced, ensuring a more robust and reliable A Vs system. For\nthese reasons, agent trajectory prediction has attracted constant\ninterest in the area of autonomous vehicles.\nWith the development and gradual adoption of high-\ndefinition (HD) maps, more and more companies and in-\nstitutions begin to incorporate HD maps into downstream\ntasks for A Vs. The emergence of datasets like Argoverse\nfurther facilitates related research by the researchers [3]. In\norder to obtain better performance in trajectory prediction,\nresearchers have begin to focus their research on how to\nrepresent HD map better [4]–[7]. Wayformer [8] uses cross-\nattention to encode information such as maps, traffic control,\nand trajectory. LaneGCN [9] works by refining lane centreline\ninformation as nodes of a graph neural network (GNN) and\nusing lane changes as edges of the graph. Vectornet [10]\ninteracts with global information through transformers by\ntreating each lane line, footpath and other element as a node\nof the graph. However, the above models do not have a well-\ndesigned module for extracting trajectories information. Once\nthese models lose the semantic information of the HD map,\nthey either do not work or their prediction accuracy is severely\ndegraded.\nIn practice, the HD map faces enormous challenges. HD\nmap are difficult for numerous applications because of their\nhigh acquisition and update costs and large storage overhead\n[11]. Moreover, the issues concerning precise positioning in\nA Vs also influence the utilization of HD maps. Both high-\nrise buildings and elevated structures can cause signal in-\nterference. Similarly, in tunnels, the loss of satellite signals\nfurther exacerbates this issue, and HD maps are unavailable\nin such situations. Moreover, the sudden failure of HD maps\nafter A V systems have been activated can severely deteriorate\nthe performance of trajectory prediction. This deterioration\nof trajectory prediction can critically impair the safety of\nautomated driving. Therefore, map-based trajectory prediction\nhas some limitations in practice. The key difference between\nmap-free and map-based trajectory prediction is that map-\nfree methods do not rely on HD map and accurate vehicle\nlocalization. It does not need the HD map information as input.\nMap-free methods depend only on target information provided\nby sensors, giving them wider applicability compared to map-\nbased methods. When A Vs drive in scenarios where HD\nmap is unavailable map-based trajectory prediction methods\noften cannot function. Thus, map-free methods have greater\nversatility for A Vs in diverse environments lacking HD map.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 2\nFor these reasons, it becomes necessary to develop a map-\nfree model in trajectory prediction. In such cases, it becomes\nimperative to rely on the constraints derived from the sur-\nrounding agents’ interaction and motion data to constrain the\npredicted trajectories. By using this crucial information, the\nsystem can accurately predict the trajectories of other agents.\nIn this work, we propose a two-stream map-free prediction\nmethod. It can consider the multimodal information of the\ntrajectories, the interaction and the relative motion information\nbetween agents. Because its two streams are the trajectory\nstream and the relative stream, we term it ”Trajectory Relative\ntwo-stream Prediction (TR-Pred).” This work is dedicated to\npredicting the future trajectory of multi-agent scenes without\nmap information. The framework is validated and tested on the\nArgoverseV1.1 dataset. The results indicate that our model\nachieves state-of-the-art (SOTA) performance compared to\nprevious map-free models. In summary, our main contributions\nare:\n• We propose a map-free vehicle trajectory prediction\nmethod. This method represents the historical trajectories\nas a trajectory stream and a relative stream. In each stream\nof the TR-Pred, we use a GNN with the transformer\nmechanism for the information interaction between the\nagents.\n• We propose a new temporal transformer. This module\nuses LSTM encoding to initialize the class token. We\nfind that LSTM encoding helps the temporal transformer\nto focus on temporal motion from the input.\n• TR-Pred achieves SOTA performance in map-free pre-\ndiction method. Meanwhile, TR-Pred outperforms map-\nbased prediction models, which are proposed in the\nprevious two years, in the minADE metric.\nThe rest of this paper is organized as follows: Section II\ndescribes the related work on trajectory prediction research.\nSection III discusses the current research problem and gives\ndetailed definitions. Section IV provides the structure of the\nmodel. Section V provides the ablation experiments, visual-\nization results and comparison results. Finally, conclusions are\npresented in Section VI.\nII. R ELATED WORK\nTrajectory prediction methods include physics-based meth-\nods, classical machine learning-based methods, deep learning-\nbased methods, and reinforcement learning-based methods.\nHowever, current long sequence forecasting predominantly\nleverages deep learning. Among these methods, we have\ncategorized the representation of agents into three forms.\nRasterization-based approaches. These methods use\nBird’s-Eye-View (BEV) image and require complex repre-\nsentation rules to create the rasterization image [12], [13].\nYuning Chai et al. perform feature extraction and state analysis\non different agents from the BEV perspective, and agents\ninteract through multi-level Convolutional Neural Networks\n(CNN) [4]. Linhui Li et al. predict raster map and historical\ntrajectories by constructing a raster map and subsequently em-\nploying a CNN network with a Multilayer Perceptron (MLP)\n[14]. In rasterized BEV maps, one has the flexibility to insert\nand use a variety of information, including both trajectories\nand maps, while using a variety of common image-processing\nbackbone networks. Thomas Gilles et al. obtain a raster heat\nmap containing future trajectory possibilities by encoding a\nrasterized BEV map through a replaceable CNN backbone\nnetwork [15]. However, rasterization techniques have some\nlimitations. For instance, while coarser raster grids may reduce\ncomputational complexity, they can also lead to substantial\nloss of information. Moreover, too detailed rasterization can\nlead to difficulties in data pre-processing and a rapid rise in\ncomputational complexity, leading to the inability to real-time\n[13]. Therefore, people begin to find a new way of processing\ninformation.\nNode-based approaches. This method is inspired by the\ndevelopment of GNN and the problems in Rasterization-\nbased approaches. Researchers use a graph to represent the\ninformation of agents and maps [9], [16]. This way solves\nthe problem of redundant information and high computation\nbrought on by rasterization [17]. Lots of approaches are pro-\nposed, representing each agent as a node and then aggregating\ncontext via GNN, including Graph Convolutional Networks\n(GCNs) [18], Graph Attention Networks (GATs) [19], [20],\nand transformers [21], [22]. Kunpeng Zhang et al. construct\na motion graph of the agent at each moment directly after\nthe video inputs [23]. Dongwei Xu et al. construct a traffic\ntarget map for each moment centered on the target agent and\nsubsequently encode the information through a GNN [24].\nTheodor Westny et al. obtain the target’s future trajectory by\nconstructing a traffic map for each moment, which is decoded\nby attentive Graph-GRU and Kalman filter [25]. However,\nNode-based approaches exhibit two core deficiencies: first,\nthey are incapable of representing the relative motion between\nobjects at each moment in time; second, they possess a relative\nlack of clarity in the representation of nodes other than the\ncenter object.\nVector-based approaches. Vector representations are\ndemonstrated to effectively encode map information. Inspired\nby the OpenDrive [26], researchers use a vector to represent\nthe information of agents. Moreover, the vectorized informa-\ntion is permutation invariant so that it can be easily combined\nwith transformers, GCN, and other GNN mechanisms. So,\nmuch SOTA work is carried out using vectorized information\nin combination with GNN [27]. For instance, The work by\nJiyang Gao et al. pioneered the application of vectorized tra-\njectory and lane boundary information to the task of trajectory\nprediction [10]. Zhibo Wang et al. encode and distillation\nlearns the inputs by vectorized agent history trajectories and\nvectorized center lanes [28].\nThe above-mentioned methods apply to both map-based\napproaches and map-free approaches. However, among all\nSOTA models tend to use a combination of trajectory and\nmap information. Yuxuan Han et al. achieve better results by\ncombining the map with traffic rules and historical vehicle\ntrajectories [29]. After graphing the HD map, Xing Gao et al.\nreach SOTA by combining historical trajectories and HD map\n[30]. The work of Zhou et al., which win in the competitions\nfor the ArgoverseV1.1 and ArgoverseV2, fully accounts for\nmap and trajectories and uses interactions between agents and\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 3\nmap [31]. However, HD maps have started to reveal some\nissues in their usage. Similar to other A Vs systems, researchers\nalso begin investigating how to perform trajectory prediction\nwithout relying on HD maps. Julian Schmidt et al., in their\nwork, achieves good results in the ArgoverseV1.1 competitions\nonly using historical trajectories [32]. Jing lian et al. achieve\nbetter results than crat-pred using only historical trajectories\nthrough GAT, IDCNN, and multi-attention mechanisms [27].\nOur work is related to that of Julian Schmidt et al. [32] and\nZhou et al. [33]. In our framework, the interactions between\neach agent at every time step are fully taken into account,\nand the corresponding traffic rules can be inferred from the\ntrajectories of surrounding agents.\nDue to the multimodality of agent trajectory prediction,\nresearchers tend to make a priori assumptions about the\nprobability of trajectories. The earliest models tend to use\nthe Gaussian mixture model (GMM) [4], [34], [35]. Recently,\nthere have been frameworks that use Laplace distribution for\nfitting [33]. Both models have been shown to perform well in\nmultimodal prediction of trajectories. Also, both probabilistic\nmodels are equally well adapted to different datasets, such as\nArgoverse, Waymo [3], [36]. Therefore, in our work, we use\nthe Laplace distribution.\nIII. P ROBLEM FORMULATION\nIn this paper, there are N traffic participants, so we can use\nPhist = {ρh1 , ρh2 , ..., ρhM , ..., ρhN } (1)\nfor the traffic participants, where Phist denotes the set of his-\ntorical trajectories for all agents in the current prediction task,\nρhN represents the trajectory of the ego vehicle, [ρh1 , ..., ρhM ]\nrepresent the historical trajectories of agents that need to\nbe predicted, M is the number of prediction targets. Each\nhistorical trajectory in Phist can be represented by :\nρhi =\nn\nρic, ρig , ρt\ni, ρt+1\ni , ..., ρTf\ni\no\n, (2)\nwhere ρhi denotes the history trajectory of the ith agent, t\nmeans the start frame of the ith agent, Tf means the end\nframe of the ith agent, and ρic denotes the difference in\ncoordinates between the agent and the ego at frame t, ρig\ndenotes the agent’s position in the global coordinate system\nat frame t. With the introduction of ρic and ρig , our network\nhas Permutation Invariance. Each frame of the ith agent is\nρt\ni = [xt, yt], (3)\nwhere (xt, yt) denotes the centroid position of the agent in the\nlocal coordinate system at timestamp t.\nSome studies have found that outputting a single predicted\ntrajectory can be deemed unreasonable. A single trajectory\ndoes not yet accurately express the likely future trajectory\nof the target and, thus, the target’s intentions. Therefore,\nduring the trajectory prediction task, we conduct multimodal\ntrajectory predictions for all agents at the same moment. So\nwhen we predict the trajectory Pp, we can represent results as\nPp = {ρp1 , ρp2 , ..., ρpM }. (4)\nwhere M is the number of targets for which single object\npredictions can be made. The prediction for the ith agent can\nbe denoted by:\nρpi =\n\b\nρ1\npi, ρ2\npi, ..., ρU\npi\n\t\n, (5)\nwhere U denotes the total number of trajectories we need to\npredict. Each predicted trajectory\nρj\npi = {(x0, y0), (x1, y1), ...,(xH, yH)}. (6)\nwhere j is the jth predicted trajectory, (xi, yi) is the estimated\ncoordinates and H denotes the number of steps we need to\npredict. In the prediction, since the output time step is deter-\nmined according to the dataset or actual usage requirements,\nit is not necessary to predict the time for each step.\nIV. M ETHODOLOGY\nA. Overall Model\nThe general structure for the trajectory prediction network is\nshown in Fig. 1. TR-Pred comprises two streams: a trajectory\nstream and a relative stream. The trajectory stream includes\ntwo modules: the trajectory feature extraction module and the\nglobal trajectory interaction module. The core of the trajectory\nfeature extraction module is an LSTM encoder. The input of\nthe trajectory feature extraction module is a single trajectory.\nThe core of the global trajectory interaction module is a GNN\nmodule using the transformer mechanism. It uses the output\nfrom the trajectory feature extraction module to get U global\ninteractivity feature for each agent. In this module, the main\nrole of the GNN is to infer the latent traffic context of the\ncurrent road from the trajectory of each vehicle. This includes\nthe drivable area, traffic control, and some lane information.\nThis enables the token corresponding to the target vehicle\nto contain information about the relevant traffic states. The\nmain modules in the relative stream are the local encoder\nmodule and the relative motion global interaction module. The\nlocal encoder consists of a graph feature encoder, an Agent\nrelative motion encoder (AR encoder) module and a temporal\ntransformer module. The GNN module uses a transformer\nmechanism containing edge information for local information\naggregation and the temporal transformer for extracting full-\ntime features of the center target. The relative interaction\nmodule is a GNN module using the transformer mechanism.\nIt uses the output from the relative encoder module to get\nthe U global interactivity feature for each agent. In this\nmodule, the primary role of GNN is to aggregate information\nfrom each local scene. Consequently, the model can obtain\nglobal interactive information between each target agent and\nother agents. This enables the target agent token to access\ninformation about the target vehicle’s interaction with other\nvehicles. Finally, an MLP decodes use the inputs from both\nstreams and directly generates the trajectories within the next\nH steps.\nB. LSTM Encoder\nIn order to improve the generalization performance, we\nrotate the trajectory by a suitable angle. So, We use the\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 4\n Predicted H Frame \nDecoder\nVectorized\nSingle Trajectory\nLSTM Encoder\nLn\nL2\nL1\nL3lstm\nlstm\nlstm\nlstm Ln\nL2\nL1\nL3\nLocal Regions\n \n \nRel-Interaction\nL1\nL2\nL3\nLn\nLn\nL2\nL1\nL3\nRel Encoder\nLocal \nEncoder Ln\nL2\nL1\nL3Local \nEncoder\nLocal \nEncoder\nLocal \nEncoder\n \n \nTraj-Interaction \nL1\nL2\nL3\nLn\nPrevious Tf Frame \nHistorical Trajectories \nFig. 1. Architecture of TR-Pred. This figure illustrates the two-stream structure of our model, where data passes through two distinct encoders and contextual\naggregators prior to decoding. Key: Rel encoder: relative encoder; Rel-Interactivity: relative interactivity; Traj-Interactivity: global trajectory interactivity.\nrotated and normalized agent historical trajectory as the LSTM\nencoder input. However, the agent’s trajectory does not always\nstart at timestamp 0. In contrast to other networks [9], [32], we\nincorporate relative moments as inputs so that trajectories that\nare not at the moment 0 can also be added to the input. Since\nthe sampling period of the dataset is not always uniformly\ndistributed, the network cannot simply take in the number of\nsampling steps as input like other networks [32], [33]. We need\nto also incorporate the timestamp as inputs for learning the\nacceleration, deceleration, and other behaviors of the agents.\nThe concrete representation of the input information can be\nrepresented by\nρhist =\nn\nρt\ni, ρt+1\ni , ..., ρTf\ni\no\n, (7)\nwith ρhist denotes the input trajectory, ρj\ni being the relative\nposition at moment j and ρj\ni = [xj; yj]. Based on this infor-\nmation, a single layer LSTM encoder can be used to capture\ntemporal information for the agent itself, and the encoder\nweights are shared. The LSTM can be denoted by:\nht\ni = LSTM (ρj\ni , ht−1\ni , ct−1\ni ) (8)\nwith output ht\ni, hidden state ct−1\ni and previous moment output\nht−1\ni are vector of size dm. dm is 128, and LSTM finally\noutput id hi.\nC. Trajectory Interactivity\nObtaining spatial interactions is a crucial challenge in\ntrajectory prediction. The GAT method cannot flexibly extract\ninteraction information of interest. Therefore, inspired by the\nGraph Transformer [37], we use a transformer with relative\npositional embedding to extract interaction features. Since we\nhave normalized the trajectories of each agent, it is necessary\nto select a key feature point within each trajectory point for\nthe construction of edges in GNN. We select the feature points\nthrough:\nρic =\n\u001a ρt\ni if t < Tf ;\nρTf\ni otherwise. (9)\nSimilar to the Graph Transformer, we extend the transformer\nto add information about the edges between nodes. We use\nMLP Ψrel (·) to obtain edge embeddings eij from agent i to\nagent j. So, we construct information about the edges between\ntwo different trajectories i and j by ρic, ρjc, ∆θij, where ∆θij\ndenotes θi − θj, Roti ∈ R2×2, eij ∈ Rdm, θi and θj denotes\nthe angle of rotation.\neij = Ψrel\n\u0010h\nRot⊤\ni\n\u0000\nρ⊤\nic − ρ⊤\njc\n\u0001\n; cos (∆θij) ; sin (∆θij)\ni\u0011\n.\n(10)\nIn Eq. (11), x ∈ [1, 2, ..., dk], WQTraj\nx ∈ Rdm×dh,\nWKTraj\nx ∈ R2dm×dh, WV Traj\nx ∈ R2dm×dh are the learnable ma-\ntrices, where dh = dm/dk. dk represents the number of\nattention heads used in the transformer.\n˜qx\ni = hiWQTraj\nx ,\n˜kx\nij = [hj; eij] WKTraj\nx ,\n˜vx\nij = [hj; eij] WV Traj\nx ,\n(11)\nWe then put the ˜qx\ni , ˜kx\nij, ˜vx\nij into the scaled dot product\nattention module to obtain the aggregated information. The\nscaled dot product attention module is formulated as follows:\nmx\ni =\nX\nj∈Ni\nsoftmax\n\u0012 ˜qx\ni√dk\n·\n\u0014n\n˜kx⊤\nij\no\nj∈Ni\n\u0015\u0013\n˜vx\nij (12)\nwhere Ni is the set of all trajectories except i, mx\ni ∈ Rdh.\nmtraj\ni = concat(m1\ni , m2\ni , ..., mdk\ni ) + hi (13)\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 5\n            \nLocal Encoder                                                                                                                                              \nAR \nEncoder\nAR \nEncoder\nAR \nEncoder\nLSTM\nInput\nRel Input\n \n \n \n \n \n \nLocal Regions\nTimeLine\nTime Slice\nTemporal Transformer\nMLP\nMLP\nMLP\nAgent Graph\nFig. 2. Architecture of the Local Encoder.\nThe MLP module is followed by the dot product attention\nmodule.\nli = Relu(MLP (mtraj\ni )) + mtraj\ni (14)\nli is vector of dm. Notably, we applied layer normalization\nbefore formulas Eq. (11) and Eq. (14).\nD. Related Encoder\nThe related encoder consists of one or more local encoder\ncomponents in parallel. Fig. 2 provides a detailed illustration\nof the structure of each module in Local Encoder. The local\nencoder has four main modules:\n• Time Slice\n• Graph Build\n• Agent Relative Encoder, AR Encoder\n• Temporal Transformer\nTime Slice is aimed at providing Rotate-Translation In-\nvariance. Rotate-Translation Invariance [33] is essential for\nenabling our network to have the simultaneous output of\ntrajectory information from multiple agents. In the module,\nwe choose the appropriate rotation matrix Rot based on the\ntrajectory of the center agent. Then, rotate both the center\nagent i and the surrounding agents accordingly. So we can be\nrepresented input Γpi by:\nΓpi =\nn\nRot, γ0\npi, γ1\npi, ..., γTf\npi\no\n. (15)\nwhere γt\npi denotes the set of agent local coordinates within\nthe center agent’s region at t time step, the origin of the local\ncoordinate system is situated at the center agent’s position at\ntime 0. In Γpi,\nγt\npi =\nn\nt, Φt\ni,\n\b\nΦt\nij\n\t\nj∈Ai\no\n, (16)\nAi denotes the local coordinates of surrounding agents within\nthe center agent’s region, Φt\ni represents the position displace-\nment of the center agent from time 0 to the current time step\nt, and Φt\nij =\n\u0000\nxi\nj, yi\nj\n\u0001\n, where\n\u0000\nxi\nj, yi\nj\n\u0001\nis the local coordinate\nfor agent j.\nGraph Build consists of two blocks, namely, the center\nagent embedding and the surrounding agent embedding. Two\ndifferent MLPs are used to obtain each embedding. In these\nMLPs,\n\u0000\nΦt\ni − Φt−1\ni\n\u0001\nand\n\u0000\nΦt\nj − Φt−1\nj\n\u0001\nis the motion infor-\nmation of the agent between adjacent moments,\n\u0000\nΦt\nj − Φt\ni\n\u0001\nis the relative distance coordinates between the surrounding\nagent and the center agent at the current moment. ai and aj\nare semantic attributes of agent i and agent j, respectively.\nEach MLP can be represented by:\nzi = Ψc\n\u0010h\nRot⊤\ni\n\u0000\nΦt\ni − Φt−1\ni\n\u0001⊤\n; ai\ni\u0011\n, (17)\nzij = Ψn\n\u0010h\nRot⊤\ni\n\u0000\nΦt\nj − Φt−1\nj\n\u0001⊤\n; Rot⊤\ni\n\u0000\nΦt\nj − Φt\ni\n\u0001⊤\n; aj\ni\u0011\n,\n(18)\nwhere, zt\ni ∈ Rdm, zt\nij ∈ Rdm, Ψc(·) and Ψn(·) are different\nMLPs.\nAR Encode aggregates information of the constructed graph\nthrough the graph transformer module. In this module, we\nuse the center agent embedding to generate the query vector\nand use the surrounding agents to generate the key and value\nvectors. These are denoted as qar\nix , kar\nijx, var\nijx. In Eq. (19),\nWQAR\nx ∈ Rdm×dh, WKAR\nx ∈ Rdm×dh, WV AR\nx ∈ Rdm×dh are\nthe learnable matrices.\nqar\nix = ziWQAR\nx , k ar\nijx = zijWKAR\nx , v ar\nijx = zijWV AR\nx ,\n(19)\nThen, we feed these values into the scaled dot product attention\nmechanism to compute:\nmar\nix =\nX\nj∈Ai\nsoftmax\n\u0012 qar\nix√dk\n·\nh\b\nkar⊤\nijx\n\t\nj∈Ai\ni\u0013\nvar\nijx (20)\nIn the previous research, it is found that the gating unit has\na specific effect enhancement for information extraction [38],\nso we also included the gating unit as the input and the fusion\nof the extracted information.\nmar\ni = concat(mar\ni1 , mar\ni2 , ..., mar\nidk ) (21)\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 6\ngi = sigmoid\n\u0000\n[zi; mar\ni ] Wgate \u0001\n, (22)\nˆzi = gi ⊙ Wself zi + (1 − gi) ⊙ mar\ni , (23)\nwhere Wgate ∈ R2dm×1, Wself ∈ Rdm×dm are the learnable\nmatrices. The MLP module is followed by the dot product\nattention module.\nSi = Relu(MLP (ˆzi)) + ˆzi (24)\nSi is vector of dm. Notably, we apply layer normalization\nbefore formulas Eq. (19) and Eq. (24).\nTemporal Transformer is used to obtain the temporal\nfeatures of the center agents. This is because the input vector\nSi is just the positional feature at a single timestamp. Our\nmodule differs from Detr and Hivt in that we did not use\nlearnable parameters [33], [39]. We use LSTM embedding to\ninitialize class tokens instead of learnable parameters. In Eq.\n(25), we add the positional embedding ( TA) with the Si.\nˆSi = Si + TA, (25)\nwhere ˆSi ∈ R(Tf +1)×dm, TA ∈ R(Tf +1)×dm, TA is a learnable\nparameter.\nqtime\nix = ˆSiWQtime\nx , ktime\nix = ˆSiWKtime\nx , vtime\nix = ˆSiWV time\nx ,\n(26)\nwhere WQtime\nx ∈ Rdm×dh, WKtime\nx ∈ Rdm×dh, WV time\nx ∈\nRdm×dh, are learnable matrix.\n˜Six = softmax\n \nqtime\nix ktime\nix\n⊤\n√dk\n+ Mask\n!\nvtime\nix , (27)\nmuv =\n\u001a −∞ if u < v;\n0 otherwise, (28)\nMask =\n\n\n0 −∞ ··· −∞\n0 0 ··· −∞\n... ... ... ...\n0 0 ··· 0\n\n\n(Tf +1)×(Tf +1)\n(29)\nwhere we use the Mask to make the model pay more attention\nto the information from the previous timestamps. The MLP\nmodule is followed by the dot product attention module.\n˜Si = Relu(MLP (concat( ˜Si1 , ˜Si2 , ..., ˜Sidk ))) + ˆSi (30)\nSame as the AR encoder, we apply layer normalization before\nformulas Eq. (26) and Eq. (30).\nE. Global Interactivity\nRegion features are obtained through the local encoder,\nnecessitating a global interaction module to capture the remote\ndependencies in the scene. Similar to the Trajectory Inter-\naction module, we employ the graph transformer for global\ninformation aggregation. However, in contrast to the trajectory\ninteraction module, each center agent must possess trajectory\ninformation in the final frame T of the historical trajectory.\n \n \n \n \n \n \n \n Predicted H Frame         Decoder\nC\nTraj\nInput\nRel \nInput\nEn\nE2\nE1\nE3\nMLP\nMLP\nMLP\nMLP\nFig. 3. The specific structure of the decoder is shown to obtain the final\noutput trajectory through the inputs of the two modes.\nThe geometric relationship between agent i and agent j can\nbe represented in the following way:\nEij = Ψr\n\u0012\u0014\nRot⊤\ni\n\u0010\nρTf\nj − ρTf\ni\n\u0011⊤\n; cos (∆θij) ; sin (∆θij)\n\u0015\u0013\n.\n(31)\nwhere Ψr(·) is MLP layer, Eij ∈ Rdm.\nqg\nix = ˜SiWQG\nx , kg\nijx = [ ˜Si; Eij]WKG\nx , vg\nijx = [ ˜Si; Eij]WV G\nx ,\n(32)\nwhere WQG\nx ∈ Rdm×dh, WKG\nx ∈ R2dm×dh, WV G\nx ∈\nR2dm×dh, are learnable matrix.\n˜Mix =\nX\nj∈Ni\nsoftmax\n \nqg⊤\nix√dk\n·\nh\b\nkg\nijx\n\t\nj∈Ni\ni!\nvg\nijx (33)\nIn Eq.(34) Ni donate the set of all trajectories except i. It\nis worth noting that the Ni here is sometimes inconsistent\nwith the Ni of the Trajectory Interaction Module, as not all\ntrajectories can be represented with center embeddings.\nLi = Relu(MLP (concat( ˜Mi1 , ˜Mi2 , ..., ˜Midk ))) + ˜Si (34)\nSame as the AR encoder, we apply layer normalization before\nformulas Eq. (32) and Eq. (34).\nF . Decoder\nThe most important characteristic of the future trajectory\nof a vehicle is its multimodality. Therefore, the output of\nmultimodal trajectories is the core of trajectory prediction\ntasks. We use the Laplace distribution to fit the uncertainty of\nthe trajectory. The model predicts trajectories using an MLP\nwhere the inputs are concatenated with the features extracted\nfrom the two streams.\nˆPp = Relu(MLP ([l; L; ˜S])) (35)\nThe model outputs ˆPp a tensor of size [U, M, H,4], where\nU denotes the number of outputs required for multimodal\noutputs, M denotes the number of agents in the current scene\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 7\nFig. 4. This figure demonstrates accurate prediction results for several common driving scenarios, including vehicle acceleration/deceleration, turning, lane\nchanging, and traveling straight. In this depiction, various elements are color-coded for clarity: blue is the ego vehicle, light brown is the neighboring traffic\nparticipant, magenta is the historical trajectory, red is the true trajectory, green is the predicted trajectory, and the dotted line is the center line of the lane\nline. With the exception of the predicted vehicle, all agents drew only the first two seconds of trajectory.\nthat can meet the requirements for predicting a trajectory, and\nH represents the number of steps that need to be predicted\nin the contemporary scene. To aid in training, we use another\nMLP and a softmax function to predict the mixing coefficients\nfor each agent’s mixing model in the shape of [U, M]. The\nspecific model architecture of the decoder is shown in the\nFig. 3.\nG. Training\nThe loss function consists of two parts. One part is a\nregression loss function, and the other part is a classification\nloss function. The negative log-likelihood function of the\nLaplace distribution serves as the regression loss function. The\ncross-entropy loss serves as the classification loss function.\nTheir ratio is 1:1.\nL = Lreg + Lcls , (36)\nIn Eq.(37) P(·|·) denote the Laplace distribution, ˆµt\ni,ˆbt\ni are\ndenoted the location of the optimal trajectory and uncertainty,\nrespectively.\nLreg = − 1\nNH\nNX\ni=1\nTf +HX\nt=Tf +1\nlog P\n\u0012\nRot⊤\ni\n\u0010\nρt\ni − ρTf\ni\n\u0011⊤\n| ˆµt\ni,ˆbt\ni\n\u0013\n.\n(37)\nWe compute Lcls utilizing the cross-entropy loss function.\nIt is worth noting that we only optimize the best-predicted\ntrajectory each time. The best trajectories are selected by 2-\nparadigm selection.\nV. E XPERIMENTS\nA. Experimental Setup\n1) Dataset: Our framework is trained and validated on the\nArgoverse dataset. We submit to Evalai’s public leaderboard 1\n(07/06/2023). The task of Argoverse Motion ForecastingV1.1\nis to predict the target’s trajectory for the next 3 seconds\nfrom the first 2 seconds of the agent’s historical trajectory.\nThe dataset contains 323,557 real-vehicle driving scenarios\nin Miami and Pittsburgh, divided into training, validation,\nand test sets roughly in a ratio of 5:1:2 (train: 205,942;\nval: 39,472; test: 78,143). In addition, experiments are also\nconducted on the highD [40] and RounD [41] datasets. The\nhighD dataset is collected on German highways, with vehicle\nspeeds generally above 80 km/h and maximum speeds ex-\nceeding 160 km/h. The RounD dataset is collected at German\nroundabouts using drones, with more complex traffic situations\nand increased interaction between agent motions compared\nto other scenarios. Since the sampling rates of these two\ndatasets are 25 Hz, which does not meet trajectory prediction\n1https://eval.ai/web/challenges/challenge-page/454/overview\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 8\nrequirements, we resampled them to 12.5 Hz. We predict the\nnext 30 sample points based on the first 20 points, for a total\nprediction time of 4 s. We spilt these two datasets into training,\ntesting, and validation sets. The roundD dataset has 4,366,\n541 and 569 data points in the training, testing, and validation\nsets, respectively. The highD dataset has 11,848, 1,536 and\n1,564 data points in the training, testing, and validation sets,\nrespectively. The spilting ratio for both datasets is 8:1:1.\n2) Metrics: We choose standard metrics for motion pre-\ndiction to evaluate our model. The standard metrics include\nminimum Average Displacement Error (minADE), minimum\nFinal Displacement Error (minFDE), and Miss Rate (MR).\nUsing these metrics, we need to predict K possible trajectories\nfor the target. The minADE measures the accuracy of the\npredicted trajectory by calculating the l2 distance between the\nbest predicted trajectory ˆτt\ni,k and the actual trajectory τt\ni . The\nspecific calculations are as follows\nmin ADEK = 1\nN\n1\nH\nK\nmin\nk=1\nNX\ni=1\nHX\nt=1\n\r\rˆpt\npi,k − pt\npi\n\r\r\n2 . (38)\nThe minFDE is calculated as the minimum final displace-\nment error between the best predicted endpoints and the actual\nendpoints over K predictions.\nminFDE K = 1\nN\nK\nmin\nk=1\nNX\ni=1\n\r\rˆpH\npi,k − pH\npi\n\r\r\n2 (39)\nIn Eq. (38) and Eq. (39), N is the total number of agents, and\nK denotes that we generate K predictions for each agent.\nMR is calculated as the ratio of the predicted endpoints less\nthan 2 meters from the actual endpoint.\n3) Implementation Details: The model is trained with 64\nepochs by the ADAMw optimizer. The hidden layer size\nis 128, the batch size is 128, the initial learning rate is\n10−4, and the weight decay and drop-out rates are 10−4\nand 0.1, respectively. We use the cosine annealing scheduler\nto attenuate the learning rate. In the trajectory stream, we\nuse one layer of the LSTM module and three layers of the\ntrajectory interaction module in the trajectory stream. In the\nrelative stream, we use a layer of local modules, four layers of\ntemporal transformer modules, and three layers of trajectory\ninteraction modules in our relative stream. The number of\nheads for the multi-head attention is 8, and the number of\npredictive modes K is set to 6. The radius of all local regions\nis 50 meters. We train for 7 hours on a single RTX3090, with\none epoch taking around 5 minutes.\nB. Results\n1) Qualitative Results: We present qualitative results for\nTR-pred on the Argoverse validation set. For the sake of an\nintuitive presentation, we have visualized the predictions. We\nhave selected a few representative scenarios for demonstration,\nwhich include vehicle acceleration, deceleration, turning, lane\nchanging, and traveling straight. Interestingly, despite the\nabsence of semantic information from the map, our model\nstill accurately predicts the vehicle’s actions, such as lane\nchanging and lane centering. We can predict the acceleration\nFig. 5. The picture shows an interesting result, which we have specially\nselected. Primarily, it illustrates the capacity of our model to leverage infor-\nmation from the surrounding agents. In this depiction, various elements are\ncolor-coded for clarity: blue is the ego vehicle, light brown is the neighboring\ntraffic participant, magenta is the historical trajectory, red is the true trajectory,\ngreen is the predicted trajectory, and the dotted line is the center line of the\nlane line. With the exception of the predicted vehicle, all agents drew only\nthe first two seconds of trajectory.\nand deceleration of vehicles at intersections even in the ab-\nsence of map and traffic control information, as depicted in\nFig. 4. Our model likewise accurately determines vehicle lane\nchanges without relying on a map. Remarkably, we found that\neven without map information and despite the vehicle having\nno intention of changing lanes, our model infer the possibility\nof making a left or right turn in the current lane based on\nthe trajectories of surrounding vehicles, as shown in Fig. 5.\nThese observations indirectly validate the effectiveness of the\ninteraction module in our model.\n2) Comparison with State-of-the-art: In order to validate\nthe proposed framework, we compare the results obtained on\nthe Argoverse dataset with those of several SOTA models from\nrecent years. We uniformly compare the results obtained by\nthese models on the validation set with the number of predicted\ntrajectories, K set to 6. Our model is compared with the\nfollowing map-free prediction methods:\n• nearest neighbors (NN) baseline [3].\n• Crystal Graph Convolutional Neural Networks with\nMulti-Head Self-Attention (CRAT) [32].\n• Encoding the intrinsic interaction information for vehi-\ncle trajectory prediction (Liannet) [27].\n• Multi-Agent Tensor Fusion for Contextual Trajectory\nPrediction (MATF) [42].\n• Diverse and Admissible Trajectory Forecasting through\nMultimodal Context Understanding (CAM) [43].\n• Towards Safe Autonomy in Hybrid Traffic: Detecting\nUnpredictable Abnormal Behaviors of Human Drivers via\nInformation Sharing (MEATP) [44].\n• A Fast and Map-Free Model for Trajectory Prediction\nin Traffics (F 2net) [45].\n• Learning Lane Graph Representations for Motion Fore-\ncasting (LaneGCN) [9].\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 9\n• Efficient Baselines for Motion Prediction in Au-\ntonomous Driving (EBMP) [46].\n• HiVT: Hierarchical Vector Transformer for Multi-Agent\nMotion Predictions (Hivt) [33].\nAdditionally, our proposed model is also compared with the\nfollowing map-based prediction methods:\n• Planning-Inspired Hierarchical Trajectory Prediction\nVia Lateral-Longitudinal Decomposition for Autonomous\nDriving (PiH) [47].\n• Heatmap Output for future Motion Estimation (HOME)\n[15].\n• Graph-Heatmap Output for future Motion Estimation\n(GOHOME) [17].\n• Target-driven Trajectory Prediction (TNT) [48].\n• End-to-end Trajectory Prediction from Dense Goal Sets\n(DenseTNT) [49].\n• Distributed Representations for Graph-Centric Motion\nForecasting (LaneRCNN) [50].\nThe results in TABLE I and III are from the 15 July 2023\nArgoverse leaderboard. Our results are submitted on 7 July\n2023.\nCompared to conventional map-based approaches, our\nmodel attains superior performance on the minADE metric.\nNevertheless, it exhibits suboptimal results on the minFDE\nand MR metrics. In light of this phenomenon, we conduct an\nanalysis. Regarding the role of maps in trajectory prediction,\nwe put forward the following hypothesis: In most cases, agents\nare traveling near the centerline of the road, except for situa-\ntions such as lane changing or lane shifting. HD maps impose\nconstraints on the agent’s localization, enhancing the precision\nof the agent regarding the horizontal position and ameliorating\nerrors. Moreover, map cues can help enhance the model’s\nunderstanding of the agent’s intention to turn and change\nlanes. However, our model has some advantages over other\nmap-based methods regarding the quality of the generated\ntrajectories. These results indicate that our model exhibits\ncomparable performance to other map-based approaches in\ninferring the agent’s intent. Our model can acquire the agent’s\nintent through limited information and has stronger processing\ncapabilities for the agent’s trajectory information. Specific\nresults are shown in TABLE I and TABLE II.\nTABLE I\nPERFORMANCE EVALUATION OF SEVERAL MAP-BASED PREDICTION\nMETHODS ON THE ARGOVERSE TEST SET\nModel Source Submission minADE minFDE MR(%)\nGOHOME [17] ICRA 2022 0.943 1.45 10.5\nHOME [15] ITSC 2021 0.920 1.36 11.3\nLaneRCNN [50] IROS 2021 0.904 1.45 12.3\nTNT [48] CoRL 2020 0.910 1.45 16.5\nDenseTNT [49] ICCV 2021 0.882 1.28 12.6\nTR-Pred - 2023 0.879 1.50 19.5\nQuantitative results on the Argoverse Motion Forecasting Leaderboard.\nCompared to recent map-free prediction methods, our pro-\nposed model achieves SOTA performance and surpasses other\nmethods on minADE, minFDE, and MR metrics. Specific\nTABLE II\nPERFORMANCE EVALUATION OF SEVERAL MAP-BASED PREDICTION\nMETHODS ON THE ARGOVERSE VALIDATION SET\nModel Source Submission minADE minFDE MR(%)\nTNT [48] CoRL 2020 0.910 1.29 9.0\nLaneRCNN [50] IROS 2021 0.904 1.19 8.2\nDenseTNT [49] ICCV 2021 0.80 1.27 7.0\nPiH [47] TIV 2023 0.70 1.20 11\nTR-Pred - 2023 0.70 1.14 12.2\nThe validation scores reported in the referenced paper are adopted in the\ntable.\nresults are shown in TABLE III. Compared to the recently pro-\nposed method CRAT, our model demonstrates improvements\nof 0.171, 0.369, and 6.3 on the minADE, minFDE, and MR\nmetrics, respectively. In particular, relative to F 2net, another\nconcurrently proposed map-free prediction method, our model\nachieves respective improvements of 5.5%, 5.8%, and 8.9% on\nthe minADE, minFDE, and MR metrics.\nTABLE III\nPERFORMANCE EVALUATION OF SEVERAL MAP-FREE PREDICTION\nMETHODS ON THE ARGOVERSE TEST SET\nModel Source Submission minADE minFDE MR(%)\nNN [3] CVPR 2019 1.71 3.3 53.7\nMATF-GAN∗ [42] CVPR 2019 1.35 2.48 -\nEBMP∗ [46] - 2023 1.26 2.27 -\nCAM∗ [43] ECCV 2020 1.13 2.50 -\nMEATP∗ [44] TCPS 2023 1.13 2.07 -\nCRAT† [32] ICRA 2022 1.05 1.87 25.8\nF2net∗ [45] - 2023 0.93 1.59 21.4\nTR-Pred - 2023 0.879 1.498 19.5\n(1) Quantitative results on the Argoverse Motion Forecasting Leaderboard.\n(2) ∗ indicates no test results on the Leaderboard are provided; we use\ntest results from the paper instead.\n(3) † denotes no Leaderboard test results are given, but we train the model\nusing open-source code. Our results align closely with those state in the\npaper.\nFor the experiments on the highD and rounD datasets,\nthe models used for comparison are modified from their\nofficial git to allow training on these datasets. Our experiments\nresults show that our model achieves the best performance on\nhighway scene (highD dataset) and roundabout scene (RounD\ndataset). As shown in TABLE IV, our model achieves nearly\nzero loss rate on the highD dataset, with minFDE less than\n0.25 m. Compared to the best performing map-free version of\nHIVT, our model improves minADE and minFDE by nearly\n20%. On the more complex traffic conditions of the RounD\ndataset, our model achieves the best performance compared\nto other models on all three metrics of minADE, minFDE,\nand MR. Compared to the well-performing HIVT model, our\nmodel improves these three metrics by 8.3%, 8.5%, and 6.7%\nrespectively. The performance of each model on the rounD\ndataset is presented in TABLE V.\n3) Compare the Results of the Validation Set with the\nResults of the Test Set:During the experiments, we observe\nsome interesting phenomena that indicate that our model has\na stronger learning ability. We compare the results of the\nmap-free prediction method on the validation set. Our model\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 10\nTABLE IV\nPERFORMANCE EVALUATION OF SEVERAL MAP-FREE PREDICTION\nMETHODS ON THE HIGH D TEST SET\nModel Source Submission minADE minFDE MR(%)\nCAM [43] ECCV 2020 0.37 0.74 5.12\nCRAT [32] ICRA 2022 0.23 0.45 2.24\nLaneGCN‡ [9] ECCV 2020 0.17 0.35 0.58\nHiVT‡ [33] CVPR 2022 0.16 0.32 0.60\nTR-Pred - 2023 0.13 0.26 0.45\n‡ denotes a map-based model that can perform map-free predictions by\nexcluding the map module.\nTABLE V\nPERFORMANCE EVALUATION OF SEVERAL MAP-FREE PREDICTION\nMETHODS ON THE ROUN D TEST SET\nModel Source Submission minADE minFDE MR(%)\nCAM [43] ECCV 2020 0.68 1.51 23.77\nCRAT [32] ICRA 2022 0.52 1.20 14.43\nLaneGCN‡ [9] ECCV 2020 0.28 0.69 2.64\nHiVT‡ [33] CVPR 2022 0.24 0.59 2.82\nTR-Pred - 2023 0.22 0.54 2.61\nachieves the best results in terms of minADE and minFDE\nmetrics. However, in the MR metric, our model performs even\nworse than F2net. Specific data is shown in TABLE VI. These\nTABLE VI\nPERFORMANCE EVALUATION OF SEVERAL MAP-FREE PREDICTION\nMETHODS ON THE ARGOVERSE VALIDATION SET\nModel Source Submission minADE minFDE MR(%)\nCRAT∗ [32] ICRA 2022 0.85 1.44 17.3\nLaneGCN‡ [9] - 2020 0.79 1.29 -\nHiVT‡ [33] CVPR 2022 0.77 1.25 14\nLiannet∗ [27] TIV 2023 0.74 1.16 12\nF2net∗ [45] - 2023 0.74 1.18 11.7\nTR-Pred - 2023 0.70 1.14 12.2\nresults diverge from those attained on the test set. The problem\nof performance degradation of our model is minimal. For this\nreason, we specifically look at the difference between the test\nset, validation set, and training set. We find that scenarios\noverlap in the test set training set much more. The test has\nmany different or even new scenarios. Most of the models\nsuffer from more or less overfitting, resulting in models that\ncannot extract valid information when confronted with new\nscenarios. Our model has better generalization and can be used\nto extract more usable information from the data. More helpful\ninformation can be extracted when confronted with unfamiliar\nscenarios. This is due to our effective model design, which\nallows us to extract the features of the scene better. Despite\nutilizing both input modalities similar to F 2Net, our two-\nstream architecture enables superior generalization capabilities\nand performance metrics. The specific difference between the\nvalidation set and test set data is shown in TABLE VII.\n4) Inference Speed: The computational complexity of our\nmodel is approximately 0.139 GFLOPs. It is comparable to\nLaneGCN and about 25% of DenseTNT’s complexity [51].\nTABLE VII\nDIFFERENCE BETWEEN VALIDATION SET AND TEST SET\nModel Source Submission minADE minFDE MR(%)\nCRAT [32] ICRA 2022 0.20 0.43 8.5\nF2net [45] - 2023 0.19 0.41 9.7\nTR-Pred - 2023 0.18 0.36 7.3\nTABLE VIII\nINFERENCE SPEED ON VALIDATION SET\nModel Speed(ms) Input minADEN = 1 N = 2\nDenseTNT [49] 137 274 Map+Traj. 0.73\nLaneGCN [9] 38 78 Map+Traj. 0.79\nHiVT ‡ [33] 45 45 Traj. 0.73\nCRAT [32] 40 80 Traj. 0.85\nTR-Pred 44 44 Traj. 0.70\nN: The number of objects requiring prediction.\nWe evaluate the inference speed of the model on a part\nof the Argoverse validation set using an RTX 3090 GPU.\nThe inference speed of our model for single-agent trajectory\nprediction tasks is comparable to that of conventional models.\nWhen predicting trajectories for multi-agents, the inference\nspeed is much faster than models that cannot inference multi-\nagents trajectories at the same time. The inference speeds are\nshown in Table VIII.\nC. Ablation Studies\nAll ablation experiments are validated at Argoverse. Unless\nspecified, the class token is initialized through the LSTM out-\nput in the temporal transformer. In this subsection, we conduct\ndetailed ablation experiments to validate the effectiveness of\neach module. It is shown in TABLE IX.\nTABLE IX\nABLATION EXPERIMENTS ON THE ARGOVERSE VALIDATION SET\nLSTM TI LE TT RI minADE minFDE MR\n✓ ✓ 0.76 1.24 0.139\n✓ ✓ ✓ ✓ 0.75 1.21 0.133\n✓ ✓ ✓ ✓ 0.74 1.2 0.13\n✓ ✓ ✓ 0.73 1.16 0.126\n✓ ✓ ✓ ✓ 0.72 1.16 0.124\n✓ ✓ ✓ ✓ ✓ 0.70 1.14 0.122\nTI: Tarj-Interaction; LE: Local Encoder; RI: Rel-Interaction; TT: temporal\ntransformer\n1) Trajectory Stream:In the experiments with the trajectory\nstream, our relative stream uses the whole framework for em-\nbodying the adaptation of the model to the relative stream. For\ntrajectory stream’s ablation experiments, we investigated the\neffects of LSTM Encode and Trajectory-Interactions on model\nperformance separately. In the case of the relative stream,\nLSTM can be a further performance improvement for the\nframework. The Global Information Interaction Module can\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 11\nfurther improve the model’s performance because it focuses on\nvehicle interaction information in a larger region. It is shown\nin TABLE X.\nTABLE X\nABLATION EXPERIMENTS FOR TRAJECTORY STREAM ON THE\nARGOVERSE VALIDATION SET\nLSRM Encoder Traj-Int minADE minFDE MR\n0.73 1.16 0.126\n✓ 0.72 1.16 0.124\n✓∗ ✓∗ 0.76 1.24 0.139\n✓ ✓ 0.70 1.14 0.122\n∗ indicates no use of the relative stream.\n2) Relative Stream: In our experiments with relative\nstream, our trajectory stream uses to represent the adaptation\nof the model to the trajectory stream. We demonstrate the\ncontribution of each module to the prediction performance\nby alternately removing one of the components. If maxout\nis used instead of the temporal transformer, we can find that\nthe performance of the model decreases very seriously. This is\nbecause Temporal Transform can fully incorporate the long-\ndistance trajectory information to fuse the information of the\nsurrounding environment at each moment. Like the trajec-\ntory stream, removing the global interaction module leads\nto decreased model performance. This is due to the global\ninteraction module’s superior ability to capture relationships\nbetween different agents. It is shown in TABLE XI.\nTABLE XI\nABLATION EXPERIMENTS FOR RELATIVE STREAM ON THE ARGOVERSE\nVALIDATION SET\nlocal Encoder TT RI minADE minFDE MR\n✓ ✓ 0.75 1.21 0.133\n✓ ✓ 0.74 1.2 0.13\n✓ ✓ ✓ 0.70 1.14 0.122\nTT: temporal transformer; RI: Rel-Interaction\n3) Relative Stream and Trajectory Stream: In this para-\ngraph, our relative stream and trajectory stream utilize a full-\nframework model. It can be observed that a single stream\nexhibits adequate performance. The model’s performance is\nalso enhanced after the fusion between the two streams. This\nphenomenon demonstrates that the framework’s performance\ncan be fully augmented by sharing information between dif-\nferent modalities. It also provides evidence for the validity of\nrepresenting relative motion temporally and prolonged motion\ntrajectory. The specific performance of the model is shown in\nTABLE XII.\n4) Class-token Initialization: In this paragraph, we evaluate\nthe impact of using two different class token initialization\nmethods on model performance. We find that the method\nutilizing LSTM initialization achieved superior performance.\nConsequently, we posit the following conjecture. Learnable\nparameters can provide some a priori knowledge, as in Detr\n[39], and extract more appropriate features. Therefore, this\nTABLE XII\nABLATION EXPERIMENTS FOR TWO-STREAM ON THE ARGOVERSE\nVALIDATION SET\nTraj-Stream Rel-Stream minADE minFDE MR\n✓ 0.76 1.24 0.139\n✓ 0.73 1.16 0.126\n✓ ✓ 0.70 1.14 0.122\nTraj-Stream: trajectory stream; Rel-Stream: relative stream\nform requires extensive training iterations and stacked Trans-\nformer modules to realize its full potential. With limited trans-\nformer stacks and epochs in our model, LSTM initialization\nachieves superior performance by reducing these requirements.\nThe performance of the class-token initialization is shown in\nTABLE XIII.\nTABLE XIII\nABLATION EXPERIMENTS FOR CLASS -TOKEN ON THE ARGOVERSE\nVALIDATION SET\nLearnable-Para LSTM Init minADE minFDE MR\n✓ 0.72 1.15 0.124\n✓ 0.70 1.14 0.122\nLearnable-Para: Learnable parameters\n5) Summaries: This subsection presents ablation studies to\nassess the contribution of each module and its influence on the\nholistic model performance. The results also demonstrate the\neffectiveness and rationality of our modifications to pertinent\nmodel components. The performance of all ablation experi-\nments is shown in TABLE IX.\nVI. C ONCLUSION\nThis paper proposes a two-stream map-free trajectory pre-\ndiction network that achieves excellent prediction results. The\ncore idea of the framework is to correlate context informa-\ntion through two different modal representations of the same\ninformation. Although the two modes of agent trajectory can\nbe independently predicted to achieve acceptable performance,\nthe collective use significantly enhances the overall effective-\nness, showing strong complementarity. In each stream, we\nuse a message encoding followed by a global interaction for\ncontextual messages. The extensive ablation studies further\nvalidate the efficacy of the proposed solutions in enhanc-\ning predictive performance. The context interaction module\nadopts a graph transformer. In the Argoverse dataset, map-\nfree prediction methods achieve for the first time within 20%\n(19.5%) in MR and within 0.9 (0.88) in minADE. This result\npaves the way for potential applications of map-free prediction\nmethods. In the highway and roundabout traffic scenarios, our\nmodel also exhibits good performance. Future research can\nproceed in the following directions: (1) knowledge distillation\ncan be utilized to acquire enhanced constraint information;\n(2) incorporating interactive motions between agents during\nprediction may improve prediction quality.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 12\nREFERENCES\n[1] Y . Huang, J. Du, Z. Yang, Z. Zhou, L. Zhang, and H. Chen, “A\nsurvey on trajectory-prediction methods for autonomous driving,” IEEE\nTransactions on Intelligent Vehicles, vol. 7, no. 3, pp. 652–674, 2022.\n[2] S. Mozaffari, O. Y . Al-Jarrah, M. Dianati, P. Jennings, and A. Mouza-\nkitis, “Deep learning-based vehicle behavior prediction for autonomous\ndriving applications: A review,” IEEE Transactions on Intelligent Trans-\nportation Systems, vol. 23, no. 1, pp. 33–47, 2022.\n[3] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,\nD. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, “Argoverse: 3d\ntracking and forecasting with rich maps,” in 2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 8740–8749,\n2019.\n[4] Y . Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple\nprobabilistic anchor trajectory hypotheses for behavior prediction,”arXiv\npreprint arXiv:1910.05449, 2019.\n[5] Y . Liu, J. Zhang, L. Fang, Q. Jiang, and B. Zhou, “Multimodal motion\nprediction with stacked transformers,” in 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 7573–7582,\n2021.\n[6] Z. Zhou, J. Wang, Y . Li, and Y . Huang, “Query-centric trajectory\nprediction,” in 2023 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 17863–17873, 2023.\n[7] M. Ye, T. Cao, and Q. Chen, “Tpcn: Temporal point cloud networks\nfor motion forecasting,” in 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 11313–11322, 2021.\n[8] N. Nayakanti, R. Al-Rfou, A. Zhou, K. Goel, K. S. Refaat, and\nB. Sapp, “Wayformer: Motion forecasting via simple & efficient at-\ntention networks,” in 2023 IEEE International Conference on Robotics\nand Automation (ICRA), pp. 2980–2987, IEEE, 2023.\n[9] M. Liang, B. Yang, R. Hu, Y . Chen, R. Liao, S. Feng, and R. Urtasun,\n“Learning lane graph representations for motion forecasting,” in 2020\nEuropean Conference Computer Vision (ECCV), pp. 541–556, Springer,\n2020.\n[10] J. Gao, C. Sun, H. Zhao, Y . Shen, D. Anguelov, C. Li, and C. Schmid,\n“Vectornet: Encoding hd maps and agent dynamics from vectorized\nrepresentation,” in 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 11522–11530, 2020.\n[11] G. Elghazaly, R. Frank, S. Harvey, and S. Safko, “High-definition maps:\nComprehensive survey, challenges and future perspectives,” IEEE Open\nJournal of Intelligent Transportation Systems, pp. 1–1, 2023.\n[12] L. Hou, S. E. Li, B. Yang, Z. Wang, and K. Nakano, “Integrated\ngraphical representation of highway scenarios to improve trajectory\nprediction of surrounding vehicles,” IEEE Transactions on Intelligent\nVehicles, vol. 8, no. 2, pp. 1638–1651, 2023.\n[13] J. Hong, B. Sapp, and J. Philbin, “Rules of the road: Predicting driving\nbehavior with a convolutional model of semantic interactions,” in 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 8446–8454, 2019.\n[14] L. Li, X. Wang, D. Yang, Y . Ju, Z. Zhang, and J. Lian, “Real-\ntime heterogeneous road-agents trajectory prediction using hierarchical\nconvolutional networks and multi-task learning,” IEEE Transactions on\nIntelligent Vehicles, pp. 1–15, 2023.\n[15] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,\n“Home: Heatmap output for future motion estimation,” in 2021 IEEE\nInternational Intelligent Transportation Systems Conference (ITSC) ,\npp. 500–507, 2021.\n[16] Y . Sun, T. Xu, J. Li, Y . Chu, and X. Ji, “Mmh-sta: A macro-micro-\nhierarchical spatio-temporal attention method for multi-agent trajectory\nprediction in unsignalized roundabouts,” IEEE Transactions on Vehicu-\nlar Technology, pp. 1–14, 2023.\n[17] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,\n“Gohome: Graph-oriented heatmap output for future motion estimation,”\nin 2022 International Conference on Robotics and Automation (ICRA),\npp. 9107–9114, 2022.\n[18] Y . Wang, J. Wang, J. Jiang, S. Xu, and J. Wang, “Sa-lstm: A trajectory\nprediction model for complex off-road multi-agent systems considering\nsituation awareness based on risk field,” IEEE Transactions on Vehicular\nTechnology, pp. 1–12, 2023.\n[19] Q. Meng, H. Guo, Y . Liu, H. Chen, and D. Cao, “Trajectory prediction\nfor automated vehicles on roads with lanes partially covered by ice\nor snow,” IEEE Transactions on Vehicular Technology, vol. 72, no. 6,\npp. 6972–6986, 2023.\n[20] Y . Cai, Z. Wang, H. Wang, L. Chen, Y . Li, M. A. Sotelo, and Z. Li,\n“Environment-attention network for vehicle trajectory prediction,” IEEE\nTransactions on Vehicular Technology, vol. 70, no. 11, pp. 11216–11227,\n2021.\n[21] X. Chen, H. Zhang, Y . Hu, J. Liang, and H. Wang, “Vnagt: Variational\nnon-autoregressive graph transformer network for multi-agent trajectory\nprediction,” IEEE Transactions on Vehicular Technology, pp. 1–12,\n2023.\n[22] K. Messaoud, I. Yahiaoui, A. Verroust-Blondet, and F. Nashashibi,\n“Attention based vehicle trajectory prediction,” IEEE Transactions on\nIntelligent Vehicles, vol. 6, no. 1, pp. 175–185, 2021.\n[23] K. Zhang, L. Zhao, C. Dong, L. Wu, and L. Zheng, “Ai-tp: Attention-\nbased interaction-aware trajectory prediction for autonomous driving,”\nIEEE Transactions on Intelligent Vehicles, vol. 8, no. 1, pp. 73–83, 2023.\n[24] D. Xu, X. Shang, Y . Liu, H. Peng, and H. Li, “Group vehicle trajectory\nprediction with global spatio-temporal graph,” IEEE Transactions on\nIntelligent Vehicles, vol. 8, no. 2, pp. 1219–1229, 2023.\n[25] T. Westny, J. Oskarsson, B. Olofsson, and E. Frisk, “Mtp-go: Graph-\nbased probabilistic multi-agent trajectory prediction with neural odes,”\nIEEE Transactions on Intelligent Vehicles, pp. 1–14, 2023.\n[26] B. A. team (2017), “Apollo: Open source autonomous driving,” [online]\nAvailable: https://github.com/ApolloAuto/apollo.\n[27] J. lian, S. Li, D. Yang, J. Zhang, and L. Li, “Encoding the intrinsic inter-\naction information for vehicle trajectory prediction,” IEEE Transactions\non Intelligent Vehicles, pp. 1–12, 2023.\n[28] Z. Wang, J. Guo, H. Zhang, R. Wan, J. Zhang, and J. Pu, “Bridging the\ngap: Improving domain generalization in trajectory prediction,” IEEE\nTransactions on Intelligent Vehicles, pp. 1–13, 2023.\n[29] Y . Han, Q. Liu, H. Liu, B. Wang, Z. Zang, and H. Chen, “Tp-frl: An\nefficient and adaptive trajectory prediction method based on the rule and\nlearning-based frameworks fusion,” IEEE Transactions on Intelligent\nVehicles, pp. 1–13, 2023.\n[30] X. Gao, X. Jia, Y . Li, and H. Xiong, “Dynamic scenario representation\nlearning for motion forecasting with heterogeneous graph convolutional\nrecurrent networks,” IEEE Robotics and Automation Letters, vol. 8,\nno. 5, pp. 2946–2953, 2023.\n[31] Z. Zhou, Z. Wen, J. Wang, Y .-H. Li, and Y .-K. Huang, “Qcnext: A next-\ngeneration framework for joint multi-agent trajectory prediction,” arXiv\npreprint arXiv:2306.10508, 2023.\n[32] J. Schmidt, J. Jordan, F. Gritschneder, and K. Dietmayer, “Crat-pred:\nVehicle trajectory prediction with crystal graph convolutional neural net-\nworks and multi-head self-attention,” in 2022 International Conference\non Robotics and Automation (ICRA), pp. 7799–7805, 2022.\n[33] Z. Zhou, L. Ye, J. Wang, K. Wu, and K. Lu, “Hivt: Hierarchical vector\ntransformer for multi-agent motion prediction,” in 2022 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR), pp. 8813–\n8823, 2022.\n[34] X. Zhou, W. Zhao, A. Wang, C. Wang, and S. Zheng, “Spatiotemporal\nattention-based pedestrian trajectory prediction considering traffic-actor\ninteraction,” IEEE Transactions on Vehicular Technology, vol. 72, no. 1,\npp. 297–311, 2023.\n[35] B. Varadarajan, A. Hefny, A. Srivastava, K. S. Refaat, N. Nayakanti,\nA. Cornman, K. Chen, B. Douillard, C. P. Lam, D. Anguelov, and\nB. Sapp, “Multipath++: Efficient information fusion and trajectory\naggregation for behavior prediction,” in 2022 International Conference\non Robotics and Automation (ICRA), pp. 7814–7821, 2022.\n[36] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y . Chai,\nB. Sapp, C. Qi, Y . Zhou, Z. Yang, A. Chouard, P. Sun, J. Ngiam,\nV . Vasudevan, A. McCauley, J. Shlens, and D. Anguelov, “Large scale\ninteractive motion forecasting for autonomous driving : The waymo\nopen motion dataset,” in 2021 IEEE/CVF International Conference on\nComputer Vision (ICCV), pp. 9690–9699, 2021.\n[37] L. Ramp ´aˇsek, M. Galkin, V . P. Dwivedi, A. T. Luu, G. Wolf, and\nD. Beaini, “Recipe for a general, powerful, scalable graph transformer,”\nAdvances in Neural Information Processing Systems (NeurIPS), vol. 35,\npp. 14501–14515, 2022.\n[38] Y . Chai, S. Jin, and X. Hou, “Highway transformer: Self-gating enhanced\nself-attentive networks,” in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics (ACL), pp. 6887–6900,\nAssociation for Computational Linguistics, July 2020.\n[39] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in Eu-\nropean conference on computer vision (ECCV), pp. 213–229, Springer,\n2020.\n[40] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, “The highd dataset:\nA drone dataset of naturalistic vehicle trajectories on german highways\nfor validation of highly automated driving systems,” in 2018 21st\nInternational Conference on Intelligent Transportation Systems (ITSC),\npp. 2118–2125, 2018.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 13\n[41] R. Krajewski, T. Moers, J. Bock, L. Vater, and L. Eckstein, “The round\ndataset: A drone dataset of road user trajectories at roundabouts in\ngermany,” in 2020 IEEE 23rd International Conference on Intelligent\nTransportation Systems (ITSC), pp. 1–6, 2020.\n[42] T. Zhao, Y . Xu, M. Monfort, W. Choi, C. Baker, Y . Zhao, Y . Wang,\nand Y . N. Wu, “Multi-agent tensor fusion for contextual trajectory\nprediction,” in 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 12118–12126, 2019.\n[43] S. H. Park, G. Lee, J. Seo, M. Bhat, M. Kang, J. Francis, A. Jadhav, P. P.\nLiang, and L.-P. Morency, “Diverse and admissible trajectory forecasting\nthrough multimodal context understanding,” in European conference on\ncomputer vision (ECCV), pp. 282–298, Springer, 2020.\n[44] J. Wang, L. Su, S. Han, D. Song, and F. Miao, “Towards safe autonomy\nin hybrid traffic: Detecting unpredictable abnormal behaviors of human\ndrivers via information sharing,” ACM Transactions on Cyber-Physical\nSystems, 2023.\n[45] J. Xiang, J. Zhang, and Z. Nan, “A fast and map-free model for trajectory\nprediction in traffics,” arXiv preprint arXiv:2307.09831, 2023.\n[46] C. G ´omez-Hu´elamo, M. V . Conde, R. Barea, M. Oca ˜na, and L. M.\nBergasa, “Efficient baselines for motion prediction in autonomous driv-\ning,” arXiv preprint arXiv:2309.03387, 2023.\n[47] D. Li, Q. Zhang, Z. Xia, Y . Zheng, K. Zhang, M. Yi, W. Jin, and\nD. Zhao, “Planning-inspired hierarchical trajectory prediction via lateral-\nlongitudinal decomposition for autonomous driving,” IEEE Transactions\non Intelligent Vehicles, pp. 1–12, 2023.\n[48] H. Zhao, J. Gao, T. Lan, C. Sun, B. Sapp, B. Varadarajan, Y . Shen,\nY . Shen, Y . Chai, C. Schmid, et al., “Tnt: Target-driven trajectory\nprediction,” in Conference on Robot Learning (CoRL), pp. 895–904,\nPMLR, 2021.\n[49] J. Gu, C. Sun, and H. Zhao, “Densetnt: End-to-end trajectory prediction\nfrom dense goal sets,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 15303–15312, 2021.\n[50] W. Zeng, M. Liang, R. Liao, and R. Urtasun, “Lanercnn: Distributed\nrepresentations for graph-centric motion forecasting,” in 2021 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS),\npp. 532–539, 2021.\n[51] X. Wang, T. Su, F. Da, and X. Yang, “Prophnet: Efficient agent-centric\nmotion forecasting with anchor-informed proposals,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 21995–22003, 2023.\nZhongning Wang received his B.Sc. degree in\nindustrial design (body engineering) from Jilin Uni-\nversity, China, in 2019. He is currently working\ntoward the M.Sc. degree in vehicle engineering from\nJilin University.\nHis research interests include multi-information\nfusion, trajectory prediction and real-time trajectory\nplanning.\nJianwei Zhang received his B.Sc. degree in vehicle\nengineering from Harbin Institute of Technology,\nchina, in 1996. He received his M.Sc degree and\nPh.D. degree in vehicle engineering from Jilin Uni-\nversity, China, in 1999 and 2003, respectively. He is\ncurrently an Associate Professor with the State Key\nLaboratory of Automotive Simulation and Control,\nJilin University.\nHis research interests include vehicle dynamic\ncontrol, autonomous vehicle control, and driving\nmotor control.\nJicheng Chen (Member, IEEE) received his B.Sc.\ndegree and M.Sc degree in control science and engi-\nneering from Harbin Institute of Technology, China.\nHe received his Ph.D. degree from the Department\nof Mechanical Engineering, University of Victoria,\nVictoria, BC, Canada, in 2021. He is currently a\nPostdoctoral Researcher at the Beihang University\nin China.\nHis main research interests include robust control,\nstochastic model predictive control, and event-based\ncontrol.\nHui Zhang (Senior Member, IEEE) received the\nB.Sc. degree in mechanical design manufacturing\nand automation from the Harbin Institute of Tech-\nnology, Weihai, China, in 2006, the M.Sc. degree\nin automotive engineering from Jilin University,\nChangchun, China, in 2008, and the Ph.D. degree\nin mechanical engineering from the University of\nVictoria, Victoria, BC, Canada, in 2012\nHe was the recipient of the 2017 IEEE Transac-\ntions on Fuzzy Systems Outstanding Paper Award,\nthe 2018 SAE Ralph R. Teetor Educational Award,\nthe IEEE Vehicular Technology Society 2019 Best Vehicular Electronics Paper\nAward, and the 2019 SAE International Intelligent and Connected Vehicles\nSymposium Best Paper Award. He is a Member of SAE International and\nASME.\nThis article has been accepted for publication in IEEE Transactions on Intelligent Vehicles. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIV.2023.3329885\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7787441611289978
    },
    {
      "name": "Trajectory",
      "score": 0.6459131240844727
    },
    {
      "name": "Initialization",
      "score": 0.6217981576919556
    },
    {
      "name": "Transformer",
      "score": 0.5407109260559082
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5251821279525757
    },
    {
      "name": "Graph",
      "score": 0.4473678767681122
    },
    {
      "name": "Theoretical computer science",
      "score": 0.10315641760826111
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}