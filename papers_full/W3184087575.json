{
  "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification",
  "url": "https://openalex.org/W3184087575",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2100794563",
      "name": "Liu Shilong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1842339085",
      "name": "Zhang Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042933658",
      "name": "Yang Xiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117703573",
      "name": "Su Hang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2043742051",
      "name": "Zhu, Jun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W3099518117",
    "https://openalex.org/W2966628364",
    "https://openalex.org/W3095707208",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2963745697",
    "https://openalex.org/W2963676620",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3102424508",
    "https://openalex.org/W2884420041",
    "https://openalex.org/W2982112268",
    "https://openalex.org/W3120129399",
    "https://openalex.org/W3090578762",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2932399282",
    "https://openalex.org/W3167456680",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2746314669",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2997136715",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W1519855838",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3171625335",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3159337199",
    "https://openalex.org/W2037227137",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1567302070",
    "https://openalex.org/W3039406286",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1962468782",
    "https://openalex.org/W2963300078",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2007972815",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2963875806",
    "https://openalex.org/W2410641892",
    "https://openalex.org/W2951005624"
  ],
  "abstract": "This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels.",
  "full_text": "Query2Label: A Simple Transformer Way to Multi-Label Classiﬁcation\nShilong Liu1,2, Lei Zhang2, Xiao Yang1, Hang Su1, Jun Zhu1*\n1 Dept. of Comp. Sci. and Tech., BNRist Center, Institute for AI, Tsinghua-Bosch Joint ML Center\n1 Tsinghua University, Beijing, 100084, China 2 International Digital Economy Academy\n{liusl20, yangxiao19}@mails.tsinghua.edu.cn, leizhang@idea.edu.cn, {suhangss, dcszj}@mail.tsinghua.edu.cn\nAbstract\nThis paper presents a simple and effective approach to\nsolving the multi-label classiﬁcation problem. The pro-\nposed approach leverages Transformer decoders to query\nthe existence of a class label. The use of Transformer is\nrooted in the need of extracting local discriminative fea-\ntures adaptively for different labels, which is a strongly de-\nsired property due to the existence of multiple objects in one\nimage. The built-in cross-attention module in the Trans-\nformer decoder offers an effective way to use label em-\nbeddings as queries to probe and pool class-related fea-\ntures from a feature map computed by a vision backbone\nfor subsequent binary classiﬁcations. Compared with prior\nworks, the new framework is simple, using standard Trans-\nformers and vision backbones, and effective, consistently\noutperforming all previous works on ﬁve multi-label clas-\nsiﬁcation data sets, including MS-COCO, PASCAL VOC,\nNUS-WIDE, and Visual Genome. Particularly, we estab-\nlish 91.3% mAP on MS-COCO. We hope its compact struc-\nture, simple implementation, and superior performance\nserve as a strong baseline for multi-label classiﬁcation tasks\nand future studies. The code will be available soon at\nhttps://github.com/SlongLiu/query2labels.\n1. Introduction\nMulti-label image classiﬁcation aims to gain a compre-\nhensive understanding of objects and concepts in an image\nwhich has wide applications in realistic scenarios includ-\ning image search, personal photo organization, digital as-\nset management, medical image recognition [21], and scene\nunderstanding [41]. Compared with single label classiﬁca-\ntion, multi-label classiﬁcation requires special attention on\ntwo problems: 1) how to handle the label imbalance prob-\nlem, and 2) how to extract features from region of interests.\nThe former problem is because of the one-vs-all strategy,\ni.e., it usually trains a batch of separate binary classiﬁers\n*Corresponding author\nQuery:\nWhere’s umbrella?\nKey\n⊙\nValue\n⊙Image\nSpatial \nFeatures\nQuery:\nWhere’s person?\nLabel Embeddings\n(Query)\nQuery from \nImage Features\nMultiple \nAttention \nMaps\nSum up Features \nfrom Attentional \nRegions\nLearned Label Features\nContain umbrella!Contain person! Label\nInput\nImage\nF Feature\nExtractor\nFigure 1: Illustration of Query2Label. Using cross attention\nfor adaptively feature pooling through focusing on different\nparts (best view in colors).\nwith each designed for recognizing a particular class, which\nmay lead to severely imbalanced numbers of positive and\nnegative samples especially when the number of classes is\nlarge . The latter problem is because of the distributed ob-\njects, i.e., an image often has multiple objects at different\nlocations – a globally pooled feature as normally used in\nsingle label classiﬁcation may dilute the features and make\nit hard to identify small objects.\nIt has witnessed signiﬁcant attempts to solve the afore-\nmentioned issues. To balance positive and negative sam-\nples, many loss functions have been developed, such as\nfocal loss [34], distribution-balanced loss [50], and asym-\nmetric loss [1]. Some works have tried to address the sec-\nond problem by utilizing spatial transformer network [47],\nadopting a global-to-local strategy [19], or using semantic\nlabel embeddings learned from label graph to discover the\nlocations of discriminative features [53]. Compared with\nthe ﬁrst issue, solutions for the second problem are rela-\ntively less mature, requiring either specially designed net-\nwork architectures or additional dependencies on label cor-\nrelation.\nMotivated by the success of Transformer used in com-\narXiv:2107.10834v1  [cs.CV]  22 Jul 2021\nputer vision tasks [3, 17], we present in this paper a simple\nyet effective solution using Transformer decoder to query\nthe existence of a class label. We show that, without bells\nand whistles, the proposed solution leads to new state-of-\nthe-art results and establishes strong baselines for its simple\nimplementation and superior performance. We name the\nproposed solution as Query2Label and illustrate it in Fig.\n1. As shown in the ﬁgure, we use learnable label embed-\ndings as queries to probe and pool class-related features via\nthe cross-attention module in Transformer encoders. The\npooled features are adaptive and more discriminative, lead-\ning to a superior multi-label classiﬁcation performance.\nThe use of Transformer for solving multi-label classi-\nﬁcation is rooted in the need of extracting local discrim-\ninative features adaptively for different labels, which is a\nstrongly desired property due to the existence of multiple\nobjects in one image. While previous works [56, 1] show\nthat it is possible to use the globally average-pooled feature\nfrom the last layer of a convolutional neural network (CNN)\nfor its simplicity in implementation, we argue that this will\nlead to inferior performance due to its discard of rich infor-\nmation in the convolutional feature map. The built-in cross-\nattention mechanism, which is called encoder-decoder at-\ntention in [45], makes Transformer decoder a perfect choice\nfor adaptively extracting desired features. By treating each\nlabel class as a query in a Transformer decoder, we can per-\nform cross-attention to pool related features for the subse-\nquent binary classiﬁcation. The most related work to this\nidea is developed by You et al. [53], which, however, com-\nputes attention using cosine similarity with negative value\nclipping and uses the same feature for both key and value,\ngreatly limiting its capability of learning locally discrimina-\ntive features.\nAnother advantage of Transformer is its multi-head at-\ntention mechanism, which can extract features from differ-\nent parts or different views of an object class and thus is\nmore capable of recognizing objects with occlusions and\nviewpoint changes. By contrast, the cross-modal attention\nin [53] is merely a single-head attention, which is incapable\nof extracting features by parts or views.\nIn this work, we develop a simple two-stage framework\ncalled Query2Label for multi-label classiﬁcation by lever-\naging multi-layer Transformer decoders. In the ﬁrst stage,\nwe use an image classiﬁcation backbone to extract image\nfeatures. The backbone could be either conventional CNN\nmodels such as ResNet [25] or recently developed Vision\nTransformer models [17]. In the second stage, multiple\nTransformer decoder layers are leveraged, using label em-\nbeddings as queries to check the existence of each label by\nperforming multi-head cross-attention to pool object fea-\ntures adaptively for subsequent binary classiﬁcation to pre-\ndict the existence of the corresponding label. Unlike [53] in\nwhich the label embeddings are learned separately to take\ninto account label correlations, we learn the label embed-\ndings end-to-end to maximize the model potential and avoid\nthe risk of introducing spurious correlations. The idea of\nusing learnable label embeddings is inspired by DETR [3].\nBut the queries in DETR are class agnostic, whereas in\nour work each query (or label embedding) uniquely corre-\nsponds to one label class, making it more effective to extract\nclass-related features. For this reason, in this paper, we will\nuse label embedding and query interchangeably.\nTo handle the label imbalance problem, we adopt a sim-\npliﬁed asymmetric loss [1] by using different γ values to\nweight positive and negative samples differently in focal\nloss. We found that this simple asymmetric loss works suf-\nﬁciently well with this Transformer-based framework and\nleads to new state-of-the-art results on several multi-label\nbenchmark data sets, including MS-COCO, PASCAL VOC,\nNUS-WIDE, and Visual Genome.\nOur contribution can be summarized as follows:\n1. We develop a simple Transformer-based two-stage\nframework Query2Label for multi-label classiﬁcation,\nleading to an effective way to query the existence of\na class label. To our best knowledge, this is the ﬁrst\ntime that the Transformer decoder architecture is used\nin classiﬁcation.\n2. We show that, the built-in cross-attention module in\nTransformer decoders can adaptively extract object\nfeatures and the multi-head attention further helps to\ndecouple object representations into multiple parts or\nviews, resulting in both improved classiﬁcation perfor-\nmance and better interpretability.\n3. We verify the effectiveness of the proposed method\nwith comprehensive experiments on several widely\nused data sets: MS-COCO, PASCAL VOC,\nNUSWIDE, and Visual Genome, and establish\nnew state-of-the-art results on all these data sets.\n2. Related Work\n2.1. Multi-Label Classiﬁcation\nMulti-label classiﬁcation task has attracted an increasing\ninterest recently. The proposed methods can be categorized\ninto three main directions as follows:\nImproving loss functions. As shown in Sec. 1, one\nof the key concerns in multi-label classiﬁcation is the\nimbalance of samples due to the use of one-vs-rest bi-\nnary classiﬁer for each category. Wu et al . [50] pro-\nposed a distribution-balanced loss to tackle the long-tailed\nproblem by re-balancing weights and mitigating the over-\nsuppression of negative labels. Ben-Baruch et al. [1] pro-\nposed an asymmetric loss, which uses different γ values to\nweight positive and negative samples in focal loss [34], and\ndiscarding easy negative samples by shifting the prediction\nprobability. In our study, we adopt a simpliﬁed asymmetric\nloss which uses different γvalues for positive and negative\nsamples without prediction probability shift.\nModeling label correlations. For its nature of multi-\nlabels on one image, the co-occurrence of concepts in a\nlarge-scale data set could be mined as prior knowledge for\nsubsequent classiﬁcation. Chen et al. [7] proposed to learn\ncategory-correlated feature vectors by constructing a graph\nbased on the statistical label co-occurrence and explored\ntheir interactions by performing neural graph propagation.\nChen et al . [8] constructed a similar graph but based on\nclass-aware maps, which is calculated by image level fea-\nture and classiﬁcation weights, and constrained the graph\nby label co-occurrence. Rather than using static graph, Ye\net al. [52] updated static graph to dynamic graph by using\na dynamic graph convolutional network(GCN) module for\nrobust representations. While modeling label correlations\ncan introduce additional gains in multi-label classiﬁcation,\nit is also arguable that it may learn spurious correlations\nwhen the label statistics are insufﬁcient. In our study, we\ndirectly learn label embeddings from data and encourage\nthe network to focus on regions of interest to learn better\nfeature representations and capture label relationships im-\nplicitly without graph networks.\nLocating regions of interest. As an image normally has\nmultiple objects, how to locate areas of interest becomes a\nconcern in multi-label classiﬁcation. Early methods [48, 51]\nfound proposals ﬁrst and treated them as single-label clas-\nsiﬁcation. Wang et al. [47] proposed to locate attentional\nregions corresponding to semantic labels by using a spa-\ntial transformer layer [27] and predicted their scores with a\nLong Short-Term Memory (LSTM) sub-network [26]. Gao\net al. [19] proposed a global-to-local discovery method to\nﬁnd proper regions with objects. All of these methods tried\nto ﬁnd local regions to focus, but the discovered bounding\nboxes were coarse and often contained background infor-\nmation as well. You et al. [53] computed cosine similarities\nbetween a label embedding and the feature map to derive an\nattention map after clipping negative values for class fea-\nture learning, which is a step forward. However, the cosine\nsimilarity with negative value clipping is likely to lead to a\nsmoother and none spike attention, making it less effective\nin extracting desired local features (because it will cover\nlarger areas than needed, see the visualized attention in Fig.\n4 in [53]). By contrast, we adopt the built-in cross-attention\nin Transformer as spatial feature selector to extract desired\nfeatures, which is both simple and effective, thanks to the\nmodularized design of Transformer and its readily available\nimplementations in modern deep learning frameworks.\n2.2. Transformer in Vision Tasks\nTransformer [45] was ﬁrst proposed to model long-range\ndependencies in sequence learning problems, and has been\nwidely used in natural language processing tasks [15, 32,\n13, 2, 31, 55]. Recently, Transformer-based models have\nalso been developed for many vision tasks [17, 54, 44, 3,\n57, 4, 22, 23] and shown great potentials. Chen et al. [5]\ntrained a sequence Transformer, named iGPT, to predict\npixels auto-regressively. Dosovitskiy et al. [17] proposed\nVision Transformers (ViT), in which they split an image to\nmultiple patches and feed them into a stacked Transformer\narchitecture for classiﬁcation. Carion et al. [3] designed an\nend-to-end object detection framework named DETR with\ntransformer. Yuan et al . [54] proposed Tokens-To-Token\nVision Transformer (T2T-ViT) to address the patch tok-\nenization problem. Srinivas et al . [44] replaced convolu-\ntional layers in last few ResNet Bottleneck [25] with Multi-\nHead Self-Attention and capture better global dependen-\ncies. More progress of applying Transformer in computer\nvision can be referred to [24] and [28].\nOur approach also uses Transformers, but we leverage\nthe built-in cross-attention in the Transformer decoder to lo-\ncate object features for each label, which is largely different\nfrom most existing works [17, 54, 44, 10] using the self-\nattention mechanism in Transformer encoders to improve\nfeature representation. Our work is inspired by DETR [3],\nbut different in that the queries in DETR are class-agnostic\nand do not have clear semantics, whereas each query in our\nwork uniquely corresponds to a semantic label.\n3. Method\nQuery2Label is a two-stage framework for multi-label\nclassiﬁcation, which uses Transformer decoders to extract\nfeatures with multi-head attentions focusing on different\nparts or views of an object category and learn label em-\nbeddings from data automatically. In this section, we will\npresent our framework ﬁrst (Sec. 3.1), followed with a brief\ndescription to the loss function used (Sec. 3.2).\n3.1. Framework\nGiven an input image x, among a set of categories of in-\nterest, multi-label classiﬁcation is to predict whether each\ncategory is present. A category could be either an object\nclass (e.g. person, dog, table, etc.) or a scene category\n(grass, sky, etc). Assume there areKcategories in total and\nwe denote the corresponding label of xas y = [y1,...,y K],\nwhere yk ∈{0,1},k = 1,...,K , is a discrete binary indica-\ntor. yk = 1if image xhas the k-th category label, otherwise\nyk = 0. Using x as input, our model predicts the proba-\nbilities of the presence of each category, p = [p1,...,p K],\nwhere yk ∈[0,1],k = 1,...,K .\nFig. 2 illustrates the framework of the proposed\nBackbone\n(CNN/ViT/…)\nTransformer Decoder\nLearnable label embeddings\nSpatial\nFeatures\nPerson\n0.99\nCar\n0.02\nUmbrella\n0.97\ncat\n0.03\nmotor\n0.07\nPerson Car Umbrella cat motor\nInput\nImage\nLogits\nLoss Function\nTransformer Decoder\nquery\nquery\nkey&\nvalue\nkey&\nvalue\n}\nxL\nFigure 2: The framework of our proposed Query2Label\n(Q2L). After extracting spatial features of an input image,\neach label embedding is sent to Transformer decoders to\nquery (by comparing the label embedding with features at\neach spatial location to generate attention maps) and pool\nthe desired feature adaptively (by linearly combining the\nspatial features based on the attention maps). The pooled\nfeature is then used to predict the existence of the queried\nlabel.\nQuery2Label (Q2L). For an input image, it ﬁrstly feeds it\ninto a backbone in the ﬁrst stage to extract spatial features.\nThe second stage is composed of two modules: a multi-\nlayer Transformer decoder block for query updating and\nadaptive feature pooling, and a linear projection layer for\ncomputing prediction logits for each category. Note that\nour method is backbone-agnostic. That is, the second stage\ncould be attached to any feature extractor. In this work, we\nmainly use convolutional neural networks as the feature ex-\ntraction backbone, but Transformer-based networks such as\nViT [17] can also be used.\nFeature extracting. Given an image x ∈RH0×W0×3 as\ninput, we extract its spatial features F0 ∈RH×W×d0 using\nthe backbone, where H0 ×W0, H×W represent the height\nand weight of the input image and the feature map respec-\ntively, and d0 denotes the dimension of features. After that,\nwe add a linear projection layer to project the features from\ndimension d0 to dto match with the desired query dimen-\nsion in the second stage and reshape the projected features\nto be F∈ RHW×d.\nQuery updating. After obtaining the spatial features of\nthe input image in the ﬁrst stage, we use label embed-\ndings as queries Q0 ∈RK×dand perform cross-attention to\npool category-related features from the spatial features us-\ning multi-layer Transformer decoders, where Kis the num-\nber of categories. We use the standard Transformer archi-\ntecture, which has a self-attention module, a cross-attention\nmodule, and a position-wise feed-forward network(FFN).\nEach Transformer decoder layer iupdates the queries Qi−1\nfrom the output of its previous layer as follows:\nself-attn: Q(1)\ni = MultiHead( ˜Qi−1, ˜Qi−1,Qi−1),\ncross-attn: Q(2)\ni = MultiHead( ˜Q(1)\ni , ˜F,F),\nFFN: Qi = FFN(Q(2)\ni ), (1)\nwhere the tilde means the original vectors modiﬁed by\nadding position encodings, Q(1)\ni and Q(2)\ni are two interme-\ndiate variables. Both the MultiHead(query, key, value)\nand FFN(x) functions are the same as deﬁned in the stan-\ndard Transformer decoder [45] and we omit their param-\neters for simplicity. As we do not need to perform auto-\nregressive prediction, we do not use attention masks. Thus\nthe M categories can be decoded in parallel in each layer.\nBoth the self-attention and cross-attention modules are\nimplemented using the same MultiHead function. The only\ndifference is where key and value are from. In the self-\nattention module, query, key and value are all from\nlabel embeddings, whereas in the cross-attention module,\nkey and value turn into the spatial features. The process\nof cross-attention can be described more intuitively: each\nlabel embedding Qi−1,k ∈ Rd,k = 1,...,K checks the\nspatial features ˜Fwhere to attend and selects features of in-\nterest to combine. After that, each label embedding obtains\na better category-related feature and updates itself. As a re-\nsult, the label embeddingsQ0 will be updated layer by layer\nand progressively injected with contextualized information\nfrom the input image via cross-attention.\nInspired by DETR, we treat the label embeddings Q0 as\nlearnable parameters. In this way, the embeddings can be\nlearned end to end from data and model label correlations\nimplicitly. The difference between our approach and DETR\nis that our queries are class-speciﬁc and have clear semantic\nmeanings, whereas the queries in DETR are class-agnostic\nand it is hard to predict which query will detect which cate-\ngory of objects.\nFeature Projection. Assuming that we have Llayers in to-\ntal, we will get the queried feature vectors QL ∈RK×d for\nK categories at the last layer. To perform multi-label clas-\nsiﬁcation, we treat each label prediction as a binary classiﬁ-\ncation task and project the feature of each class QL,k ∈Rd\nto a logit value using a linear projection layer followed with\na sigmoid function:\npk = Sigmoid\n(\nWT\nk QL,k + bk\n)\n, (2)\nwhere Wk ∈ Rd, W = [W1,...,W K]T ∈ RK×d, and\nbk ∈R, b= [b1,...,b K]T ∈RK are parameters in the linear\nlayer, and p = [p1,...,p K]T ∈RK are the predicted prob-\nabilities for each category. Note that pis a function which\nmaps an input image xto category prediction probabilities.\nxis omitted for notation simplicity.\n3.2. Loss Function\nThanks to the built-in cross-attention mechanism in\nTransformer decoders, our framework does not require a\nnew loss function. Both the binary cross entropy loss and\nfocal loss [34] work well with our framework. To more ef-\nfectively address the sample imbalance problem, we adopt\na simpliﬁed asymmetric loss [1], which is a variant of focal\nloss with different γvalues for positive and negative values.\nIn our experiments, we found it works the best.\nGiven an input images x, we can predict its category\nprobabilities p= [p1,...,p K]T ∈RK using our framework.\nThen we leverage the following asymmetric focal loss to\ncalculate the loss for each training sample x:\nL= 1\nK\nK∑\nk=1\n{\n(1 −pk)γ+ log(pk), y k = 1,\n(pk)γ− log(1 −pk), y k = 0, (3)\nwhere yk is a binary label to indicate if imagexhas label\nk. The total loss is computed by averaging this loss over all\nsamples in the training data set D. And the optimization is\nperformed using stochastic gradient descent. By default, we\nset γ+ = 0and γ−= 1in our experiments.\n4. Experiments\nTo evaluate the proposed approach, we conduct experi-\nments on several data sets, including MS-COCO [35], PAS-\nCAL VOC [18], NUS-WIDE [11], and Visual Genome [30].\nFollowing previous works, we adopt the average precision\n(AP) on each category and mean average precision (mAP)\nover all categories for evaluation. To better demonstrate the\nperformance of models, we also present the overall preci-\nsion (OP), recall (OR), F1-measure (OF1) and per-category\nprecision (CP), recall (CR), F1-measure (CF1) for further\ncomparison. See appendix for a more formal deﬁnition of\nthese metrics.\n4.1. Implementation Details\nUnless otherwise stated, we will use the settings de-\nscribed below for all experiments. Following ASL [1],\nWe adopt TResNetL [40] as our backbone, as it performs\nbetter than the standard ResNet101 [25] for this task un-\nder similar efﬁciency constraints on GPU. We resize all\nimages to H0 ×W0 = 448 ×448 as the input resolu-\ntion and the size of the output feature from TResNetL is\nH ×W ×d0 = 14×14 ×2432. We set d = d0 = 2432\nin our experiments, hence the size of the ﬁnal output fea-\nture in the ﬁrst stage is 14 ×14 ×2432. The extracted\nfeatures are fed into the second stage module after adding\nposition encodings and reshaping. For the second stage,\nwe utilize one Transformer encoder layer and two Trans-\nformer decoder layers for label feature updating. After the\nlast Transformer decoder, we add a linear projection layer\nto compute logit predictions for all categories.\nNote that the Transformer encoder is mainly used to fur-\nther help fuse global context for better feature representa-\ntion, but it can be removed for more efﬁcient computation.\nIn our experiments, our model works well even with only\none Transformer decoder layer. See more ablations in Sec.\n4.3.\nWe leverage the ImageNet [14] pre-trained model as our\nbackbone, and update the whole model on the target multi-\nlabel classiﬁcation data set. We trained the model for 80\nepochs using the Adam [29] optimizer, with True-weight-\ndecay [38] of 1e−2, (β1,β2) = (0.9,0.9999), and a learn-\ning rate of 1 ×10−4. More implementation and training\ndetails are available in the supplementary materials.\n4.2. Comparison with State-of-the-art Methods\n4.2.1 Performance on MS-COCO\nMS-COCO [35] is a large-scale data set constructed for\nobject detection and segmentation ﬁrstly and has been\nwidely used for evaluating multi-label image classiﬁcation\nrecently. It contains 122,218 images and covers 80 com-\nmon objects, with an average of 2.9 labels per image. No-\ntice that the mAP scores for MS-COCO are highly inﬂu-\nenced by the input resolution. Thus we divide the results\ninto two groups: medium resolution ( H,W ≤ 600) and\nhigh resolution(H,W >600) and report them separately.\nFor the medium resolution, we adopt our standard setting\nand report the comparison between our method and other\nstate-of-the-art methods in Table 1. All the methods are\nevaluated in the resolution of448 ×448, except for SSGRL\nin 576 ×576 and ADD-GCN in 512 ×512. Our proposed\nmethod outperforms all the previous methods in terms of\nmAP, OF1, and CF1, which are the most important met-\nrics, as other metrics can be affected by the chosen thresh-\nold largely. In particular, our Q2L respectively outperforms\nADD-GCN by 2.0%, SSGRL by 3.4%, and ASL by 0.6%.\nThat demonstrates the superiority of our approach.\nFor high resolution( 640 ×640) experiments, we adopt\nTResNetXL [40] as the backbone and remove the self-\nattention module in Transformer decoders for better train-\ning and inference efﬁciency. The results are shown in Table\n2. Our method outperforms the best result in the literature\nand establishes a new state of the art.\n4.2.2 Performance on PASCAL VOC\nPASCAL VOC 2007 and 2012 [18] are two frequently used\ndata sets for multi-label classiﬁcation. Each image in VOC\ncontains one or multiple labels, corresponding to 20 object\ncategories. In order to fairly compare with other methods,\nwe follow the common setting to train our model on the\ntrain-val set and then evaluate its performance on the\nMethod Backbone Resolution mAP All Top 3\nCP CR CF1 OP OR OF1 CP CR CF1 OP OR OF1\nSRN [56] ResNet101 224×224 77.1 81.6 65.4 71.2 82.7 69.9 75.8 85.2 58.8 67.4 87.4 62.5 72.9\nResNet-101 [25] ResNet101 224×224 78.3 80.2 66.7 72.8 83.9 70.8 76.8 84.1 59.4 69.7 89.1 62.8 73.6\nCADM [8] ResNet101 448×448 82.3 82.5 72.2 77.0 84.0 75.6 79.6 87.1 63.6 73.5 89.4 66.0 76.0\nML-GCN [9] ResNet101 448×448 83.0 85.1 72.0 78.0 85.8 75.4 80.3 87.2 64.6 74.2 89.1 66.7 76.3\nKSSNet [36] ResNet101 448×448 83.7 84.6 73.2 77.2 87.8 76.2 81.5 - - - - - -\nMS-CMA [53] ResNet101 448×448 83.8 82.9 74.4 78.4 84.4 77.9 81.0 86.7 64.9 74.3 90.9 67.2 77.2\nMCAR [20] ResNet101 448×448 83.8 85.0 72.1 78.0 88.0 73.9 80.3 88.1 65.5 75.1 91.0 66.3 76.7\nSSGRL [7] ResNet101 576×576 83.8 89.9 68.5 76.8 91.3 70.8 79.7 91.9 62.5 72.7 93.8 64.1 76.2\nC-Trans [33] ResNet101 576×576 85.1 86.3 74.3 79.9 87.7 76.5 81.7 90.1 65.7 76.0 92.1 71.4 77.6\nADD-GCN [52] ResNet101 576×576 85.2 84.7 75.9 80.1 84.9 79.4 82.0 88.8 66.2 75.8 90.3 68.5 77.9\nQ2L-R101(Ours) ResNet101 448×448 84.9 84.8 74.5 79.3 86.6 76.9 81.5 78.0 69.1 73.3 80.7 70.8 75.4\nQ2L-R101(Ours) ResNet101 576×576 86.5 85.8 76.7 81.0 87.0 78.9 82.8 90.4 66.3 76.5 92.4 67.9 78.3\nASL [1] TResNetL 448×448 86.6 87.2 76.4 81.4 88.2 79.2 81.8 91.8 63.4 75.1 92.9 66.4 77.4\nTResNetL [39] TResNetL(22k) 448×448 88.4 - - - - - - - - - - - -\nQ2L-TResL(Ours) TResNetL 448×448 87.3 87.6 76.5 81.6 88.4 78.5 83.1 91.9 66.2 77.0 93.5 67.6 78.5\nQ2L-TResL(Ours)TResNetL(22k) 448×448 89.2 86.3 81.4 83.8 86.5 83.3 84.9 91.6 69.4 79.0 92.9 70.5 80.2\nMlTr-l [10] MLTr-l(22k) 384×384 88.5 86.0 81.4 83.3 86.5 83.4 84.9 - - - - - -\nSwin-L [37] Swin-L(22k) 384×384 89.6 89.9 80.2 84.8 90.4 82.1 86.1 93.6 69.9 80.0 94.3 71.1 81.1\nCvT-w24 [49] CvT-w24(22k) 384×384 90.5 89.4 81.7 85.4 89.6 83.8 86.6 93.3 70.5 80.3 94.1 71.5 81.3\nQ2L-SwinL(Ours) Swin-L(22k) 384×384 90.5 89.4 81.7 85.4 89.8 83.2 86.4 93.9 70.4 80.5 94.8 71.0 81.2\nQ2L-CvT(Ours) CvT-w24(22k) 384×384 91.3 88.8 83.2 85.9 89.2 84.6 86.8 92.8 71.6 80.8 93.9 72.1 81.6\nTable 1: Comparison of our method with known state-of-the-art models on MS-COCO at medium input resolution. The\nbackbones noted with 22k are pretrained on the ImageNet-22k dataset. Among them, mAP, OF1, and CF1 are the primary\nmetrics (shaded in the table) as the others may be affected by the chosen threshold largely. All metrics are in %.\nMethod Architecture Input\nResolution mAP\nASL [1] TResNetXL 640×640 88.4\nTResNet [39] TResNetL(22k) 640×640 89.8\nQ2L-TResXL TResNetXL 640×640 89.0\nQ2L-TResL TResNetL(22k) 640×640 90.3\nTable 2: Comparison of our method with ASL on MS-\nCOCO for high input resolution of 640 ×640. All metrics\nare in %.\ntest set. Following previous works, we also pre-train the\nmodel on COCO for better performance.\nResults on VOC 07. VOC 2007 contains 5,011 images\nas the train-val set, and 4,952 images as the test set.\nResults on VOC 07 are shown in Table 3. We can see that\nour method achieves the best mAP performance among all\nmethods. We also observe the small margin between our\nresults and ADD-GCN [52]. In addition to the difference\nin input image resolution (ours 448 ×448 and ADD-GCN’s\n512×512), the small increase may indicate the limited data\nof VOC 07 and its saturated metric. Nevertheless, there\nmight be some other factors that inﬂuence the results, as\nwe outperform previous works on VOC 12 with a larger\nmargin as shown in Table 4, whose results are reported by\nthe evaluation server. We report results with ImageNet-1K\npretrained backbone only in the main text for a fair compar-\nison, and results with advanced backbones could be found\nin the appendix.\nResults on VOC 12. VOC 2012 consists of 11,540\nimages as the train-val set and 10,991 images as the\ntest set. Results on VOC 12 are shown in Table 4. As all\nthe results are reported by its evaluation server, it is a much\nfairer comparison than a local test. Our method outperforms\nall other methods on all metrics with a large margin.\n4.2.3 Performance on NUS-WIDE\nNUS-WIDE [11] is a real-world web image data set. It con-\ntains 269,648 Flickr images and has been manually anno-\ntated with 81 visual concepts. We follow the steps in [1]\nfor evaluation, and compare the proposed method with pre-\nvious state-of-the-art models in Table 5. As the resolutions\nof NUS-WIDE images are not high enough, we found the\nimprovement of our method is not as signiﬁcant as on MS-\nCOCO and PASCAL VOC. But we still achieve a new state\nof the art on this data set.\n4.2.4 Performance on Visual Genome\nVisual Genome [30] is a data set that contains 108,249 im-\nages and covers80,138 categories. As most categories con-\ntain very few samples, [7] select images with the most fre-\nMethods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmAP\nCNN-RNN [46] 96.7 83.1 94.2 92.8 61.2 82.1 89.1 94.2 64.2 83.6 70.0 92.4 91.7 84.2 93.7 59.8 93.2 75.399.778.6 84.0\nVGG+SVM [42] 98.9 95.0 96.8 95.4 69.7 90.4 93.5 96.0 74.2 86.6 87.8 96.0 96.3 93.1 97.2 70.0 92.1 80.3 98.1 87.089.7\nFev+Lv [51] 97.9 97.0 96.6 94.6 73.6 93.9 96.5 95.5 73.7 90.3 82.8 95.4 97.7 95.9 98.6 77.6 88.7 78.0 98.3 89.090.6\nHCP [48] 98.6 97.1 98.0 95.6 75.3 94.7 95.8 97.3 73.1 90.2 80.0 97.3 96.1 94.9 96.3 78.3 94.7 76.2 97.9 91.590.9\nRDAL [47] 98.6 97.4 96.3 96.2 75.2 92.4 96.5 97.1 76.5 92.0 87.7 96.8 97.5 93.8 98.5 81.6 93.7 82.8 98.6 89.391.9\nRARL [6] 98.6 97.1 97.1 95.5 75.6 92.8 96.8 97.3 78.3 92.2 87.6 96.9 96.5 93.6 98.5 81.6 93.1 83.2 98.5 89.392.0\nSSGRL [7] (576) 99.7 98.4 98.0 97.6 85.7 96.2 98.2 98.8 82.0 98.1 89.7 98.8 98.7 97.0 99.0 86.9 98.1 85.8 99.0 93.795.0\nMCAR [19] 99.799.098.5 98.2 85.4 96.9 97.4 98.9 83.7 95.5 88.8 99.1 98.2 95.1 99.1 84.8 97.1 87.8 98.3 94.894.8\nASL(TResNetL) [1]99.998.4 98.9 98.7 86.8 98.2 98.7 98.5 83.198.3 89.598.8 99.2 98.6 99.3 89.5 99.4 86.899.695.2 95.8\nADD-GCN [52] (576)99.899.098.499.0 86.7 98.1 98.5 98.385.8 98.388.9 98.8 99.0 97.4 99.2 88.3 98.7 90.799.597.0 96.0\nQ2L-TResL(Ours)99.998.999.098.4 87.7 98.6 98.8 99.184.5 98.3 89.2 99.2 99.2 99.2 99.3 90.2 98.8 88.3 99.5 95.596.1\nTable 3: Comparisons of our method with previous state-of-the-art methods on PASCAL VOC 2007, in terms of AP and\nmAP in %. All results are reported at resolution 448 ×448 except for the ADD-GCN and SSGRL, whose resolutions are\nnoted in parentheses. Results with advanced backbones could be found in the appendix.\nMethods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\nVGG+SVM [42] 99.0 89.1 96.0 94.1 74.1 92.2 85.3 97.9 79.9 92.0 83.7 97.5 96.5 94.7 97.1 63.7 93.6 75.2 97.4 87.8 89.3\nFev+Lv [51] 98.4 92.8 93.4 90.7 74.9 93.2 90.2 96.1 78.2 89.8 80.6 95.7 96.1 95.3 97.5 73.1 91.2 75.4 97.0 88.2 89.4\nHCP [48] 99.1 92.8 97.4 94.4 79.9 93.6 89.8 98.2 78.2 94.9 79.8 97.8 97.0 93.8 96.4 74.3 94.7 71.9 96.7 88.6 90.5\nMCAR [19] 99.6 97.1 98.3 96.6 87.0 95.5 94.4 98.8 87.0 96.9 85.0 98.7 98.3 97.3 99.0 83.8 96.8 83.7 98.3 93.5 94.3\nSSGRL [7](576) 99.7 96.1 97.7 96.5 86.9 95.8 95.0 98.9 88.3 97.6 87.4 99.1 99.2 97.3 99.0 84.8 98.3 85.8 99.2 94.1 94.8\nADD-GCN [52](576) 99.8 97.1 98.6 96.8 89.4 97.1 96.5 99.3 89.0 97.7 87.5 99.2 99.1 97.7 99.1 86.3 98.8 87.0 99.3 95.4 95.5\nQ2L-TResL(Ours) 99.9 98.2 99.3 98.1 90.4 97.7 97.4 99.4 92.7 98.7 89.9 99.4 99.5 99.0 99.4 88.4 98.8 89.3 99.6 96.8 96.6\nTable 4: Comparisons of AP and mAP in% of our model and state-of-the-art methods on PASCAL VOC 2012. All results are\nreported at resolution 448×448 except for the ADD-GCN and SSGRL, whose resolution is noted in parentheses. Different\nfrom VOC 07, results in VOC 12 are reported by the evaluation server.\nMethod Backbone mAP CF1 OF1\nMS-CMA [53] ResNet101 61.4 60.5 73.8\nSRN [56] ResNet101 62.0 58.5 73.4\nICME [9] ResNet101 62.8 60.7 74.1\nQ2L-R101(Ours) ResNet101 65.0 63.1 75.0\nBaseline [40] TresNetL 63.1 61.7 74.6\nFocal loss [34] TresNetL 64.0 62.9 74.7\nASL [1] TresNetL 65.2 63.6 75.0\nQ2L-TResL(Ours) TresNetL 66.3 64.0 75.0\nMlTr-l [10] MlTr-l(22k) 66.3 65.0 75.8\nQ2L-CvT(Ours) CvT-w24(22k) 70.1 67.6 76.3\nTable 5: Comparison of our methods to known state-of-the-\nart models on NUS-WIDE. The backbones noted with 22k\nare pretrained on the ImageNet-22k dataset. All metrics are\nin %.\nquent 500 categories, and split the data set into train and test\nsets. They call the new data set VG500. We follow their set-\nting and compare our model with prior methods in Table 6.\nFor a fair comparison, we set the resolution of input im-\nages to 512 ×512, and evaluate our method using both the\nResNet-101 [25] and TResNetL [40] backbones. Although\nthe previous state-of-the-art model SSGRL [7] use larger\nimage resolution ( 576 ×576) than ours ( 512 ×512), our\nmethod outperforms all previous works and achieves a new\nSOTA on VG500. As the number of categories in VG500 is\nmuch larger than other data sets, it becomes more challeng-\ning for a simple spatially average-pooled feature to recog-\nnize all of the objects. Hence the advantages of our method\nare more obvious. The results indicate the importance and\neffectiveness of spatially adaptive feature attention in multi-\nlabel classiﬁcation, particularly when the number of cate-\ngories is large.\nMethod mAP\nResNet-101 [25] 30.9\nResNet-SRN [56] 33.5\nSSGRL(ResNet101) [7] 36.6\nC-Tran(ResNet101) [33] 38.4\nQ2L-R101(Ours) 39.5\nQ2L-TResL-22k(Ours) 42.5\nTable 6: Comparison of our method with prior state-of-the-\nart methods on VG500. All metrics are in %. All results are\nreported at the input resolution of 576 ×576.\n4.3. Ablation Study\nResults on objects of different sizes. To further test\nQ2L’s performance on objects of different sizes, we split the\nMS-COCO [35] test set into three subsets for small ob-\njects, medium objects, and large objects respectively. Fol-\nlowing the common deﬁnition, objects occupying areas less\nthan and equal to 32 ×32 pixels are considered as “small\nobjects”, less than and equal to 96 ×96 pixels are marked\nas “medium objects”, and the others are “large objects”.\nWe compare our Q2L with the baseline TResNetL model.\nThe results are listed in Table 7. Our model outperforms\nbaselines on all three categories, especially on medium ob-\njects. The larger improvement on medium objects demon-\nstrates the superiority of the spatially adaptive feature pool-\ning, which helps collect information that may be diluted by\naverage pooling. For small objects, although our method\nhas made a big step forward, it remains a challenging prob-\nlem to be solved, requiring ﬁner-grained details to be ex-\ntracted from images.\nMethod small medium large\nBaseline(TResNetL) [40] 37.8 74.2 84.2\nOurs(TResNetL+Q2L) 39.5 77.5 86.1\nTable 7: Comparison of improvement on objects with dif-\nferent sizes.\n4.4. Visualization of Attention Maps\nTo further demonstrate the effectiveness of cross-\nattention and adaptive pooling, we visualize some attention\nmaps in Fig. 3. The attention map is analogous to the re-\nceptive ﬁeld size in a raw image. We found that our model\ncan locate the speciﬁed object approximately, especially on\nsome small or medium objects. We also compare our Q2L\nmodel with baseline in Fig. 4. It validates the effective-\nness of the spatial adaptive pooling built into Transformer\ndecoders and shows great potential for providing better in-\nterpretability.\nBeyond single attention maps, we are also interested in\nﬁnding out the role of multi-head attention in this task. For\na given target person, we plot individual attention maps\nin each head and the mean attention map in Fig. 5. We\nﬁnd that different heads are capable of focusing on different\nparts of targets. For person, head-1, head-3, and head-4\nfocus on the shoulder, neck, and head respectively. The at-\ntention maps of head-2 are less informative, as there is no\nclear focus, which may indicate that head-2 is not utilized\nas the other three heads already collect sufﬁcient informa-\ntion for classiﬁcation. Focusing on different parts makes the\nmodel more robust to occlusion and view changes, and pro-\nvides better interpretability for the superiority of our model.\nraw image\n person\n elephant\ncar\n train\n truck\nraw image\nsink\n refrigerator\n clock\n vase\nbowl\n pottedplant\n oven\nraw image\nFigure 3: Visualization of cross-attention maps. We plot\nthe mean of each head’s cross-attention maps, that repre-\nsent similarities of a givenquery and extracted spatial fea-\ntures. Texts above images represent the ground truth labels\n(query) for the raw images. Best view in colors.\nLabel: clock\nPrediction:\nOurs(Q2L): 0.735\nBaseline: 0.422\nLabel: truck\nPrediction:\nOurs(Q2L): 0.864\nBaseline: 0.348\nLabel: bench\nPrediction:\nOurs(Q2L): 0.922\nBaseline: 0.292\nFigure 4: Image examples classiﬁed correctly by Q2L\nbut wrongly by the baseline TResNetL. The middle two\ncolumns are the mean attention maps of Q2L and the en-\nlarged maps on focused regions respectively. The small\nscale of objects makes it difﬁcult for TResNetL to recog-\nnize. Best view in colors.\n5. Conclusion\nIn this paper, we have presented a simple yet effective\nframework Query2Label (Q2L) for multi-label classiﬁca-\ntion, which is developed based on Transformer decoders\nraw image head-1 head-2 head-3 head-4 head-mean\nFigure 5: Visualization of multi-head attention maps for the\ntarget label person. Each column in the middle represents\nan attention map for one head and the rightmost column\naverages the maps of all heads. Best view in colors.\nattached to an image classiﬁcation backbone. The built-in\ncross-attention module in the Transformer decoder archi-\ntecture offers an effective way to used label embeddings to\nquery the existence of a class label and pool class-related\nfeatures. The proposed framework consistently outperforms\nall prior works on several widely used data sets includ-\ning MS-COCO, PASCAL VOC, NUSWIDE, and Visual\nGenome. We hope its simple model architecture and out-\nstanding performance will serve as a strong baseline for fu-\nture research on multi-label image classiﬁcation.\nA. More Implementation Details\nWe adopt the ofﬁcial PyTorch implementation for both\nthe backbone and Transformer modules [45]. Following\nDETR [3], we use 2D sine and cosine encodings to rep-\nresent spatial positions. Each model was trained for 80\nepochs using Adam [29] and 1-cycle policy [43], with a\nmaximal learning rate of 1e−4. For regularization, we use\nCutout [16] with a factor of 0.5 and True-wight-decay [38]\nof 1e−2. Moreover, we normalize input images with mean\n[0,0,0] and std [1,1,1], and use RandAugment [12] for aug-\nmentation. Following common practices, we apply expo-\nnential moving average (EMA) to model parameters with\na decay of 0.9997. To speed up, we use mixed precision\nduring model training. The entire code to reproduce the ex-\nperiments will be made available.\nB. Metrics\nBeyond the average precision (AP) and mean average\nprecision (mAP), we report more metrics in the experi-\nments, including the overall precision (OP), recall (OR),\nF1-measure (OF1) and per-category precision (CP), recall\n(CR), F1-measure (CF1). These metrics are computed as\nfollows:\nOP =\n∑\niMi\nc∑\niMip\n, OR =\n∑\niMi\nc∑\niMig\n,\nCP = 1\nC\n∑\ni\nMi\nc\nMip\n, CR = 1\nC\n∑\ni\nMi\nc\nMig\n,\nOF1 =2 ×OP ×OR\nOP + OR , CF1 =2 ×CP ×CR\nCP + CR ,\n(4)\nwhere Mi\nc is the number of images predicted correctly\nfor the i-th category, Mi\np is the number of images predicted\nfor the i-th category, and Mi\ng is the number of ground truth\nimages for the i-th category. For each image, we assign it\na positive label if its prediction probability is greater than a\nthreshold or negative otherwise. Note that these results may\nbe affected by the chosen threshold. The OF1 and CF1\nare the primary metrics among them, as they consider both\nrecall and precision and are more comprehensive.\nC. Additional results on VOC07\nWe show the results with ImageNet-1k pretrained back-\nbones only in the maintext for a fair comparison on VOC\n07. Additional results are listed in Table 8.\nD. More Visualization Results\nWe provide more visualization results of cross-attention\nmaps on MS-COCO [35]. To visualize the cross-attention\nmaps, we compute the attention values between labels and\nMethod Resolution mAP\nTResNetL 448×448 96.7\nQ2L-TResL(Ours) 448×448 96.9\nQ2L-CvT(Ours) 384×384 97.3\nTable 8: Comparison of our method with prior state-of-the-\nart methods on VOC07. Backbones in all models are pre-\ntrained on ImageNet-22k dataset. All metrics are in %.\npixels. Since each matrix adds up to 1 and each value in the\nmatrix is small, we divide the entire matrix by0.06 and clip\nit between 0 and 1 to get a better visualization result. Then\nfor a target label, we resize its corresponding attention value\nmatrices (for multiple attention heads) to the same size as\nraw images and use the resized matrices as the opacity of\neach pixel in images. We plot the mean-head maps for more\nimages in Fig. 6 and Fig. 7. These ﬁgures provide an intu-\nitive explanation for the effectiveness of our spatial adaptive\npooling and the superiority of our method. In addition, we\nshow more multi-head attention maps of the targetperson\nin Fig. 8. We ﬁnd that different heads are capable of focus-\ning on different parts of targets.\nFigure 6: More visualizations of cross-attention maps. We plot the mean of each head’s cross-attention maps, which repre-\nsent similarities of a given query and the extracted spatial features. Texts above images represent the ground truth labels\n(query) for the raw images. Best view in colors.\nFigure 7: More visualizations of cross-attention maps (continued). Best view in colors.\nFigure 8: More visualizations of multi-head attention maps for the target label person. Each column in the middle repre-\nsents an attention map for one head and the rightmost column averages the maps of all heads. Best view in colors.\nReferences\n[1] Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy,\nItamar Friedman, Matan Protter, and Lihi Zelnik-Manor.\nAsymmetric loss for multi-label classiﬁcation, 2020.\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In Advances in Neural Information\nProcessing Systems, volume 33, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020.\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020.\n[5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In International Conference on Ma-\nchine Learning, pages 1691–1703. PMLR, 2020.\n[6] Tianshui Chen, Zhouxia Wang, Guanbin Li, and Liang Lin.\nRecurrent attentional reinforcement learning for multi-label\nimage recognition. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 32, 2018.\n[7] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and\nLiang Lin. Learning semantic-speciﬁc graph representation\nfor multi-label image recognition. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 522–531, 2019.\n[8] Z. Chen, X. Wei, X. Jin, and Y . Guo. Multi-label image\nrecognition with joint class-aware map disentangling and la-\nbel correlation embedding. In 2019 IEEE International Con-\nference on Multimedia and Expo (ICME) , pages 622–627,\n2019.\n[9] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen\nGuo. Multi-Label Image Recognition with Graph Convo-\nlutional Networks. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\n[10] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, Dong\nShen, Zhongyuan Wang, Nian Shi, and Honglin Liu. Mltr:\nMulti-label classiﬁcation with transformer, 2021.\n[11] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhip-\ning Luo, and Yantao Zheng. Nus-wide: a real-world web im-\nage database from national university of singapore. In Pro-\nceedings of the ACM International Conference on Image and\nVideo Retrieval, page 48, 2009.\n[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmenta-\ntion with a reduced search space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702–703, 2020.\n[13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell,\nQuoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a ﬁxed-length context. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 2978–2988, 2019.\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009.\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina N.\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, 2018.\n[16] Terrance DeVries and Graham W Taylor. Improved regular-\nization of convolutional neural networks with cutout. arXiv\npreprint arXiv:1708.04552, 2017.\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[18] Mark Everingham, S. M. Eslami, Luc Gool, Christopher K.\nWilliams, John Winn, and Andrew Zisserman. The pascal vi-\nsual object classes challenge: A retrospective. International\nJournal of Computer Vision, 111(1):98–136, 2015.\n[19] Bin-Bin Gao and Hong-Yu Zhou. Learning to discover multi-\nclass attentional regions for multi-label image recognition,\n2020.\n[20] Bin-Bin Gao and Hong-Yu Zhou. Multi-label image recog-\nnition with multi-class attentional regions. arXiv preprint\narXiv:2007.01755, 2020.\n[21] Zongyuan Ge, Dwarikanath Mahapatra, Suman Sedai, Rahil\nGarnavi, and Rajib Chakravorty. Chest x-rays classiﬁca-\ntion: A multi-label and ﬁne-grained problem. arXiv preprint\narXiv:1807.07247, 2018.\n[22] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. arXiv preprint arXiv:2012.09688, 2020.\n[23] Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-\nMin Hu. Beyond self-attention: External attention using two\nlinear layers for visual tasks, 2021.\n[24] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-\njing Xu, Yixing Xu, et al. A survey on visual transformer.\narXiv preprint arXiv:2012.12556, 2020.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[26] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997.\n[27] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and\nKoray Kavukcuoglu. Spatial transformer networks. arXiv\npreprint arXiv:1506.02025, 2015.\n[28] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021.\n[29] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014.\n[30] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\nLi Fei-Fei. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International\nJournal of Computer Vision, 123(1):32–73, 2017.\n[31] K. Lagler, M. Schindelegger, J. B ¨ohm, H. Kr ´asn´a, and\nT. Nilsson. Gpt2: Empirical slant delay model for radio\nspace geodetic techniques. Geophysical Research Letters ,\n40(6):1069–1073, 2013.\n[32] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. Albert: A lite\nbert for self-supervised learning of language representations.\nIn ICLR 2020 : Eighth International Conference on Learn-\ning Representations, 2020.\n[33] Jack Lanchantin, Tianlu Wang, Vicente Ordonez, and Yanjun\nQi. General multi-label image classiﬁcation with transform-\ners. arXiv preprint arXiv:2011.14027, 2020.\n[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980–2988, 2017.\n[35] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC. Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In European Conference on Computer Vision, pages\n740–755, 2014.\n[36] Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming\nXiang, and Chunhong Pan. Multi-label image classiﬁca-\ntion via knowledge distillation from weakly-supervised de-\ntection. In Proceedings of the 26th ACM international con-\nference on Multimedia, pages 700–708, 2018.\n[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows, 2021.\n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[39] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi\nZelnik-Manor. Imagenet-21k pretraining for the masses,\n2021.\n[40] Tal Ridnik, Hussam Lawen, Asaf Noy, Emanuel Ben Baruch,\nGilad Sharir, and Itamar Friedman. Tresnet: High per-\nformance gpu-dedicated architecture. Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 1400–1409, 2020.\n[41] Jing Shao, Kai Kang, Chen Change Loy, and Xiaogang\nWang. Deeply learned attributes for crowded scene un-\nderstanding. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4657–4666,\n2015.\n[42] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In ICLR\n2015 : International Conference on Learning Representa-\ntions 2015, 2015.\n[43] Leslie N Smith. A disciplined approach to neural network\nhyper-parameters: Part 1–learning rate, batch size, momen-\ntum, and weight decay. arXiv preprint arXiv:1803.09820 ,\n2018.\n[44] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021.\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n[46] Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang\nHuang, and Wei Xu. Cnn-rnn: A uniﬁed framework for\nmulti-label image classiﬁcation. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n2285–2294, 2016.\n[47] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and\nLiang Lin. Multi-label image recognition by recurrently dis-\ncovering attentional regions. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 464–472,\n2017.\n[48] Yunchao Wei, Wei Xia, Min Lin, Junshi Huang, Bingbing Ni,\nJian Dong, Yao Zhao, and Shuicheng Yan. Hcp: A ﬂexible\ncnn framework for multi-label image classiﬁcation. IEEE\ntransactions on pattern analysis and machine intelligence ,\n38(9):1901–1907, 2015.\n[49] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-\nvolutions to vision transformers, 2021.\n[50] Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, and Dahua\nLin. Distribution-balanced loss for multi-label classiﬁcation\nin long-tailed datasets. In European Conference on Com-\nputer Vision, pages 162–178. Springer, 2020.\n[51] Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao,\nJianxin Wu, and Jianfei Cai. Exploit bounding box annota-\ntions for multi-label object recognition. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 280–288, 2016.\n[52] Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, and Yu\nQiao. Attention-driven dynamic graph convolutional net-\nwork for multi-label image recognition. InEuropean Confer-\nence on Computer Vision, pages 649–665. Springer, 2020.\n[53] Renchun You, Zhiyao Guo, Lei Cui, Xiang Long, Yingze\nBao, and Shilei Wen. Cross-modality attention with seman-\ntic graph embedding for multi-label classiﬁcation. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 34, pages 12709–12716, 2020.\n[54] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021.\n[55] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong\nSun, and Qun Liu. Ernie: Enhanced language representation\nwith informative entities. arXiv preprint arXiv:1905.07129,\n2019.\n[56] Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and\nXiaogang Wang. Learning spatial regularization with image-\nlevel supervisions for multi-label image classiﬁcation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5513–5522, 2017.\n[57] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7117258310317993
    },
    {
      "name": "Discriminative model",
      "score": 0.696765124797821
    },
    {
      "name": "Pascal (unit)",
      "score": 0.6775102615356445
    },
    {
      "name": "Transformer",
      "score": 0.6008001565933228
    },
    {
      "name": "Binary number",
      "score": 0.530666172504425
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5057079792022705
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4961486756801605
    },
    {
      "name": "Contextual image classification",
      "score": 0.4492647349834442
    },
    {
      "name": "Multi-label classification",
      "score": 0.4444705843925476
    },
    {
      "name": "Machine learning",
      "score": 0.3994379937648773
    },
    {
      "name": "Data mining",
      "score": 0.3938446044921875
    },
    {
      "name": "Image (mathematics)",
      "score": 0.19870147109031677
    },
    {
      "name": "Voltage",
      "score": 0.14954370260238647
    },
    {
      "name": "Mathematics",
      "score": 0.12631374597549438
    },
    {
      "name": "Engineering",
      "score": 0.09005290269851685
    },
    {
      "name": "Programming language",
      "score": 0.07829776406288147
    },
    {
      "name": "Arithmetic",
      "score": 0.07623079419136047
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 116
}