{
    "title": "Text Rendering Strategies for Pixel Language Models",
    "url": "https://openalex.org/W4389520317",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5081588948",
            "name": "Jonas F. Lotz",
            "affiliations": [
                "Charles University",
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A5028564314",
            "name": "Elizabeth Salesky",
            "affiliations": [
                "Charles University",
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A5007759955",
            "name": "Phillip Rust",
            "affiliations": [
                "Charles University",
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A5010165733",
            "name": "Desmond Elliott",
            "affiliations": [
                "Charles University",
                "University of Copenhagen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2493916176",
        "https://openalex.org/W3175049034",
        "https://openalex.org/W3207937903",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W4386566790",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2168199177",
        "https://openalex.org/W4287063318",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3018647120",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W3035110948",
        "https://openalex.org/W3105816068",
        "https://openalex.org/W2758915439",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W2996657533",
        "https://openalex.org/W3164045210",
        "https://openalex.org/W2970820321",
        "https://openalex.org/W2964073004",
        "https://openalex.org/W3095771422",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3125516434",
        "https://openalex.org/W4247294974",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W4285596748",
        "https://openalex.org/W4226271314",
        "https://openalex.org/W4379255802",
        "https://openalex.org/W3045462440",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2966610483",
        "https://openalex.org/W2463895987",
        "https://openalex.org/W3098265177",
        "https://openalex.org/W4378509379",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W4291961401",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2890560993",
        "https://openalex.org/W4285272403",
        "https://openalex.org/W4223589941",
        "https://openalex.org/W3154987757",
        "https://openalex.org/W3100198908",
        "https://openalex.org/W4385573328",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W4385573170",
        "https://openalex.org/W3135427360",
        "https://openalex.org/W2518186251",
        "https://openalex.org/W3037575273",
        "https://openalex.org/W4304192731",
        "https://openalex.org/W4387635303",
        "https://openalex.org/W4285483774",
        "https://openalex.org/W4311730904",
        "https://openalex.org/W4285208773",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2956105246",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4385567351",
        "https://openalex.org/W4385567023"
    ],
    "abstract": "Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks. This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model. Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting the connections between image patch- and tokenization-based language models.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10155–10172\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nText Rendering Strategies for Pixel Language Models\nJonas F. Lotz†,‡ Elizabeth Salesky⋆ Phillip Rust† Desmond Elliott†\n†Department of Computer Science, University of Copenhagen\n‡ROCKWOOL Foundation Research Unit\n⋆Johns Hopkins University\njonasf.lotz@di.ku.dk\nAbstract\nPixel-based language models process text ren-\ndered as images, which allows them to han-\ndle any script, making them a promising ap-\nproach to open vocabulary language modelling.\nHowever, recent approaches use text renderers\nthat produce a large set of almost-equivalent\ninput patches, which may prove sub-optimal\nfor downstream tasks, due to redundancy in\nthe input representations. In this paper, we in-\nvestigate four approaches to rendering text in\nthe PIXEL model (Rust et al., 2023), and find\nthat simple character bigram rendering brings\nimproved performance on sentence-level tasks\nwithout compromising performance on token-\nlevel or multilingual tasks. This new rendering\nstrategy also makes it possible to train a more\ncompact model with only 22M parameters that\nperforms on par with the original 86M param-\neter model. Our analyses show that character\nbigram rendering leads to a consistently better\nmodel but with an anisotropic patch embedding\nspace, driven by a patch frequency bias, high-\nlighting the connections between image patch-\nand tokenization-based language models.\n1 Introduction\nThere is a growing movement in NLP towards\ntokenization-free methods (Clark et al., 2022; Xue\net al., 2022; Yu et al., 2023) including pixel-based\nrepresentations of text (Salesky et al., 2021, 2023;\nRust et al., 2023; Tschannen et al., 2023). It has\nbeen shown that these tokenization-free methods\ncan readily handle unseen languages and that they\nare more robust to noise attacks than tokenization-\nbased models. In addition, pixel-based approaches\ncan effectively exploit visual similarities between\ncharacters and scripts because they allow for com-\nplete parameter sharing across all inputs, making\nthem a promising direction for multilingual NLP.\nPrevious work on pixel-based models segments\nthe rendered text into either consecutive patches\n(Rust et al., 2023; Tschannen et al., 2023) or with\n(a) Continuous rendering (CONTINUOUS ):\n(b) Structured rendering (BIGRAMS ):\n(c) Structured rendering (MONO ):\n(d) Structured rendering (WORDS ):\nFigure 1: Examples of rendering strategies for the sen-\ntence “I must be growing small again.” from Carroll\n(1865). Black patches mark the end of a sequence, fol-\nlowing Rust et al. (2023).\na sliding window (Salesky et al., 2021, 2023) as\nin speech processing. Although the proposed ap-\nproaches have the appealing properties of yielding\ncompact and transferable representations, they also\nresult in a very large input space because there\nis no unique way to represent lexical units. As a\nconsequence, pixel-based models could observe\na new set of image representations with every\nnew sentence, which adds redundancy in the input\nspace and is sub-optimal for developing contextual\nlanguage representations. We refer to these\nunstructured rendering strategies as CONTINUOUS\nand illustrate the point qualitatively in Figure 1\nand Figure 2, and quantitatively in Figure 3. In this\nwork, we ask whether structuring the input, which\nleads to more frequent parameter updates through\nnow-unique word representations, would enable\npixel-based models to develop a deeper understand-\ning of context and semantics. We then propose\nrendering strategies structured around providing\nthe model with a compressed input space.\nWe demonstrate how enforcing a BIGRAMS -\nstructured rendering strategy leads to both a\nmore capable and data-efficient model: when\nevaluated on semantic sentence-level tasks, we\nfind that a 22M parameters model performs\n10155\n(a) Most frequent patches with CONTINUOUS rendering:\n(b) Most frequent patches with BIGRAMS rendering:\nFigure 2: A continuous rendering strategy results in\nmany uniquely-valued image patches for similar inputs,\nwhile structured rendering (here, BIGRAMS ) regularises\nand compresses the potential input space.\ncompetitively with the unstructured original at\n86M parameters, and that scaling back up to\n86M parameters narrows the performance gap to\nBERT (Devlin et al., 2019) trained on the same\ndata. In subsequent analyses, we find that the\nadded input structure provokes a clear visual token\nfrequency bias in the learned embedding space.\nWhile also found in BERT , frequency biases have\nbeen shown to degrade the quality of embedding\nspaces when word representations are not only\ndetermined by semantic relations but also by the\nnumber of model updates (Gong et al., 2018;\nGao et al., 2019; Fuster Baggetto and Fresno,\n2022). We show that frequent words have more\ncontext-specific representations than infrequent\nwords, especially in the upper layers. Finally,\nwe show that PIXEL models acquire a non-trivial\nsemantic understanding during pretraining, but that\ntheir sentence representations are easily influenced\nby this frequency bias. We release all models1 and\ncode2 for pretraining and finetuning.\n2 Background: modelling text as images\nWe build upon the general-purpose language en-\ncoder framework presented in Rust et al. (2023):\nPIXEL is a text autoencoder which builds on the\nMasked Autoencoding Vision Transformer (ViT-\nMAE; He et al., 2021) and is similarly pretrained\nwith a masked reconstruction objective. However,\ninstead of patches from natural images of objects\n(Deng et al., 2009), the patches now contain im-\nages of text. To go from text to images of text,\nPIXEL relies on a rendering library (PangoCairo)3\nto produce a sequence-level image which is sliced\ninto image patches of size 16 ×16 pixels. The\nsequence-length maximum of 529 patches approxi-\nmately equals the memory requirements of BERT ,\n1https://huggingface.co/Team-PIXEL\n2https://github.com/xplip/pixel/tree/\nTextRenderingStrategies\n3https://docs.gtk.org/PangoCairo\n0k 2k 4k 6k 8k 10k\nSentences observed\n0k\n50k\n100k\n150k\n200k\n250k# unique patches\ncontinuous bigrams mono words\nFigure 3: Number of unique image patches observed\nas a function of training data sequences. Structured\nrendering results in greater representational efficiency.\nthe closest benchmark for PIXEL . By using the\nGoogle Noto font family which supports the major-\nity of Unicode codepoints,4 the renderer supports\nall languages that can currently be typeset.\nBefore the first layer of the PIXEL model, image\npatches are linearly projected to obtain a sequence\nof patch ‘embeddings’. During pretraining, 25% of\nembeddings are masked in spans of up to 6 patches\nand only the unmasked patches with a prepended\nCLS embedding are passed through the encoder. Af-\nter replacing the masked embeddings amidst the\nencoder outputs, relying on fixed sinusoidal posi-\ntion embeddings for ordering information, the de-\ncoder predicts the pixel values of solely the masked\npatches. To later finetune the encoder on a classi-\nfication task, the decoder can be replaced with a\ntask-specific head and the masking ratio set to 0%.\n3 Structured rendering\nPreviously proposed approaches to rendering\ntext as images render full sequences of text and\nsegment into either consecutive patches (Rust\net al., 2023; Tschannen et al., 2023) or with a\nsliding window (Salesky et al., 2021, 2023). These\nCONTINUOUS strategies result in a significant\nnumber of uniquely-valued patches, many of\nwhich may be observed only once during training.\nWe depict this redundancy in Figure 2 and quantify\nit in Figure 3, showing how similar text inputs\nresult in unique visual representations.\nWe compare four rendering strategies: the orig-\ninal unstructured (CONTINUOUS ), and three struc-\ntured ( WORDS , MONO , BIGRAMS ), as depicted\nin Figure 1. To render WORDS we separate seg-\n4https://fonts.google.com/noto\n10156\nments with additional whitespace5 such that new\nsegments begin at the beginning of the next im-\nage patch, regulating possible spatial variation. BI-\nGRAMS , rendering two characters per image patch,\nis chosen to be widely applicable, without knowl-\nedge of word or morphemic segmentation (Mielke\net al., 2021; Keren et al., 2022). More specifically—\nconsider the word pairs ⟨“grow”, “growing”⟩and\n⟨“growing”, “walking”⟩—the BIGRAMS renderer\nwill produce an overlap of image patches (under-\nlined) for both pairs while the same extent is not\nguaranteed with WORDS -level rendering as it is reg-\nulated by character width. The choice of character\n(n= 2)-grams is motivated by what generally fits\nwithin a 16 ×16 pixels image patch in the setup\nfrom Rust et al. (2023). MONO instead applies\nmonospaced fonts where each character is a fixed\nwidth; depending on font size, this may result in\ncharacter bigram patches without breaks within\ncharacters, but this is not guaranteed. The main\ndifference between BIGRAMS and MONO is that\nMONO simply slides across the sentence, two char-\nacters at the time, yielding two ways to represent a\nword whereas BIGRAMS renders the words and then\npads with whitespace, ensuring unique inputs.6\nAs seen in Figure 3, the structured rendering\nstrategies result in a greatly compressed input\nspace as measured by the number of unique im-\nage patches processed by the model, but Figure 1\nreveals that it comes at the cost of longer sequence\nlengths. While the rendering strategies we propose\nwere not specifically designed for English, they\nmay not equally generalise to other languages or\nscripts. We further discuss the representational effi-\nciencies of these strategies in § A.1 and limitations\nto generalisability under Limitations.\n4 Model scale variants\nRecall from Figure 3 that CONTINUOUS rendering\nproduces a significantly larger set of unique image\npatches compared to other approaches. A conse-\nquence of this is that models must learn to encode\nmany almost-identical visual representations,\nwhich may be wasteful, both in terms of parameters\nand training efficiency. Therefore, we hypothesise\nthat PIXEL models that operate over fewer unique\nimage patches can be scaled down without sacrific-\n5We render whitespace at minimum 3 pixels wide, sometimes\nresulting in a blank patch between tokens in structured inputs.\n6As an example, “be” in Figure 1 is split into 2 image patches\nwith MONO rendering. Depending on the context, it could\nalso be represented in a single image patch.\nModel Enc L-DecL Hid MLP Att |θ|\nBASE 12-8 768 3072 12 86M\nSMALL 12-4 384 1536 6 22M\nTINY 12-2 192 768 3 5.5M\nTable 1: Details of PIXEL model scale variants.\ning performance. While “Base” models and larger\nones are widely used for their strong performance,\nproven scaling laws (Touvron et al., 2021; Zhai\net al., 2021) enable greater experimentation and\nmodel development at smaller scale (Ivgi et al.,\n2022), which is both more environmentally friendly\n(Strubell et al., 2019; Bender et al., 2021; Hersh-\ncovich et al., 2022) and facilitates contributions\nwith limited computational resources.\nWith this in mind, we propose two smaller\narchitectures which we will compare across down-\nstream tasks in § 5. Our BASE model architecture\nis directly adopted from ViT (Dosovitskiy et al.,\n2021) and PIXEL , and we add two more compact\nSMALL and TINY model variants, as described in\nTable 1. The configurations of the smaller models\nare based on the ViT variants presented in Zhai\net al. (2021). Following the scaling experiments in\nHe et al. (2021), indicating that shallow decoders\nof as small as 2 layers can be sufficient for\nViT-MAEs, we apply a scheme of halving the\nnumber of decoder layers at every scale reduction.\n5 Experiments\nWe pretrain SMALL models with the proposed ren-\ndering strategies. The models are then evaluated\non dependency parsing (UDP ) with data from Uni-\nversal Dependencies v2.10 treebanks (Zeman et al.,\n2022; Nivre et al., 2020) and GLUE (Wang et al.,\n2018), exploring the models’ capabilities at syn-\ntactic processing on the word level and semantic\nprocessing on the sentence level.\n5.1 Pretraining\nWe pretrain all models on the English Wikipedia\nand Bookcorpus (Zhu et al., 2015) data used by\nRust et al. (2023) for direct comparison with PIXEL\nand BERT , which results in ∼16.8M training ex-\namples. We follow the suggested hyperparameters\nused for PIXEL with the exception of batch size.\nThe smaller architectures of SMALL and TINY al-\nlow for larger batch sizes, which we double from\n256 examples to 512 and 1024, respectively. We\nthen halve the number of pretraining steps accord-\n10157\nStructure Scale\nUDP GLUE UDP GLUE TyDiQA-GoldP\nRenderer Avg. Avg. Variant |θ| Avg. ∆µ Avg. ∆µ Avg. ∆µ\nCONTINUOUS 76.2 71.0 TINY 5.5M 72.0 −0.3 66.5 +12.7 41.6 +4.9\nBIGRAMS 76.1 75.4 SMALL 22M 76.1 −0.1 75.4 +4.4 50.8 +2.0\nMONO 75.9 74.4 BASE 86M 75.5 −0.6 78.0 +3.9 52.8 +0.5\nWORDS 76.6 74.7 BERT 110M 50.5 — 80.0 — 51.5 —\nTable 2: Structure (left): averaged results for SMALL -models comparing downstream performance on UDP and\nGLUE following the different rendering strategies. Scale (right): averaged results across model scales using the\nBIGRAMS rendering structure. ∆µis the difference in average performance between BIGRAMS and CONTINUOUS\nrendering for a given model scale. BERT results are marked in grey to visually distinguish from pixel-based models.\ningly from 1M to 500k and 250k in order to train\nfor the same number of epochs as PIXEL (∼16\nepochs, but varying slightly due to differing se-\nquence lengths per rendering strategy).\nPretraining BASE takes 8 days on 8 ×40GB\nNvidia A100 GPUs, while in comparison, pretrain-\ning SMALL takes less than 48 hours on 8 ×40GB\nNvidia A100 GPUs, and TINY less than 24\nhours. Loss trajectories for the different rendering\nstrategies are in line with their representational\nefficiency (Figure 3), indicating that structured\nrendering may make the masked reconstruction\ntask more data-efficient, achieving a low loss in\nfewer steps (see § A.2: Figure 10).\n5.2 Finetuning\nTo finetune our models for classification tasks we\nreplace the decoder used for pretraining with a\ntask-specific classification head. We do not search\nfor more optimal hyperparameters than those used\nfor PIXEL with the exception of the learning rate;\nwe find that the more compact architectures often\nbenefit from a slightly higher learning rate.7\nWe follow the same protocol during finetuning\nas done for PIXEL : for word-level tasks we obtain\nthe rendered image patch indices for every word\nand as a consequence, the CONTINUOUS strategy\nbecomes identical to the WORDS structure when\nfinetuning on UDP . § 6.1 further investigates the\nconsequence of a mismatch between how the data\nis structured during pretraining and finetuning.\nWhen finetuning on GLUE the structure follows\nwhat was seen during pretraining for all rendering\nstrategies. Reported performances for BERT and\nPIXEL are taken from Rust et al. (2023).\n7We search the space {1e−5, 3e−5, 5e−5, 7e−5, 9e−5}\nand report the average over 3 seeds.\n5.3 Rendering strategies\nWe present averaged results comparing the render-\ning strategies in the left part of Table 2. Detailed\nresults for each downstream task are presented in\nTable 4 and Table 5 in the appendix. For UDP\nwe find that the WORDS structure slightly outper-\nforms BIGRAMS and MONO on this word-level task.\nWhen comparing the WORDS and CONTINUOUS\nstrategies we get a first hint as to the importance of\nincluding structure during pretraining as well, keep-\ning in mind that the rendering structure is the same\nfor both strategies when finetuning on UDP . For\nGLUE we see a large increase in performance when\nrendering with any structure and especially BI-\nGRAMS . We attribute the difference in performance\nbetween BIGRAMS and MONO to the unique word\nrepresentations with BIGRAMS , as discussed in § 3.\nWe find that BIGRAMS is the best performing\nstructure on average, even slightly outperforming\nthe 86M parameters PIXEL (average UDP : 76.1;\naverage GLUE : 74.1) with only ¼ its model\nparameters. We provide an investigation into the\nmechanisms that enable this improved performance\non GLUE in § 6.4. Next we pretrain TINY and\nBASE model variants with BIGRAMS rendering to\nevaluate performance at different model scales.\n5.4 Model scaling\nThe right part of Table 2 compares the different\nmodel scales all following a BIGRAMS rendering\nstrategy. Detailed results are likewise presented\nin Table 4, Table 5, and Table 6 in the appendix.\nWe find that the TINY configuration performs\ncompetitively on the word-level tasks considering\nits only 5.5M parameters, but has a larger gap\nup to SMALL and BASE on the sentence-level\nGLUE tasks. SMALL proves to be a good trade-off\nbetween scale and performance where it is not\nfar behind BASE on GLUE and even slightly\n10158\noutperforms on UDP .8 BASE comes a step closer\nto closing the gap in performance up to BERT on\nGLUE . Comparing to the performance following\na CONTINUOUS rendering strategy, summarised\nas the difference in average performance (∆µ), it\nis clear that the more compact the model size, the\ngreater the benefit from structured rendering.\nTo verify that BIGRAMS rendering does not\ndegrade the performance on multilingual sentence-\nlevel tasks across different scripts and morpholo-\ngies, we also include results on TyDiQA-GoldP\n(Clark et al., 2020). 9 Again we find that SMALL\nperforms competitively considering its size.\n6 Ablations and supplementary analyses\nIn this section we investigate how BIGRAMS\nrendering changes the model compared to CON -\nTINUOUS . For clarity in what follows, we refer\nto the BASE model with BIGRAMS rendering from\n§ 5.4 as BASE -BIGRAMS and keep referring to the\noriginal model from Rust et al. (2023) as PIXEL .\n6.1 When does rendering structure matter?\nHaving established that a structured rendering strat-\negy leads to improved downstream performance,\nwe further investigate when it is needed: is it suf-\nficient to finetune with structure or does the model\ndevelop strategy-specific features during pretrain-\ning? We analyze this by comparing rendering\nstrategies between pretraining and finetuning.\nThe results in Table 3 for GLUE show that a mis-\nmatch leads to lower downstream performance for\nboth strategies, with BIGRAMS →CONTINUOUS\nbeing the most harmful, perhaps unsurprisingly.\nThis result does not align with the finding for\nUDP in § 5.3 where CONTINUOUS overcomes\nthe change to WORDS -structured rendering. It\nmay indicate that the lower-level UDP tasks are\neasier for PIXEL -based models than the high-level\nGLUE tasks (Lauscher et al., 2020). This is in\nline with the relatively good performance for\nTINY-BIGRAMS on UDP .\nTo emphasize the increase in performance\non semantic tasks with BIGRAMS rendering, we\n8We expect that BASE could prevail and would benefit from a\nwider search for optimal hyperparameters during finetuning.\n9With the CONTINUOUS rendering strategy, answer spans are\nextracted such that the answer may include leading or trailing\ncharacters when there is no exact mapping from a word to an\nimage patch index. Therefore, we did not include TyDiQA-\nGoldP in the comparison in § 5.3. More details can be found\nin Rust et al. (2023). We discuss limitations to answer span\nextraction with BIGRAMS rendering in § A.4.\nRENDERER GLUE\nPretraining Finetuning Avg.\nBIGRAMS BIGRAMS 75.4\nCONTINUOUS CONTINUOUS 71.0\nCONTINUOUS BIGRAMS 61.1\nBIGRAMS CONTINUOUS 53.0\nTable 3: Rendering strategy combinations between pre-\ntraining and finetuning with SMALL models. For GLUE ,\nmatching pretraining structure is most effective.\ndemonstrate that BASE -BIGRAMS outperforms\nPIXEL by 3.6 points on average on MasakhaNER\n(Adelani et al., 2021), a named entity recognition\nbenchmark for 10 African languages. This further\nillustrates the potential of PIXEL -based models\nfor modelling low-resource languages. Detailed\nresults are presented in Table 7 in the appendix. We\nnext turn our attention to how BIGRAMS rendering\nenables better performance on semantic tasks.\n6.2 Contextual representations\nThe extent to which language models capture se-\nmantic information is partly determined by their\nability to contextualise text (Peters et al., 2018).\nWe therefore analyse how capable BASE -BIGRAMS\nis at producing contextualised word representations.\nWe use the Words in Context dataset (WiC; Pile-\nhvar and Camacho-Collados, 2019) of sentences\nthat contain target words (noun or verb) in either\na similar (True) or different (False) context across\nsentence pairs.10 We compute the mean hidden\nstate output over all tokens associated with the\ntarget word to obtain a representation. We infer\nthat there is contextualisation if the model gener-\nates representations of a target word from different\ncontexts with a low cosine similarity compared to\ntarget words in similar contexts. We report this\nindication of contextuality for each layer of the\nmodel, including the input layer, to better under-\nstand the properties of the different layers. Similar-\nities between randomly chosen words from random\nexamples (Random) are included as a baseline.11\nFigure 4a plots the resulting distributions of\nsimilarities. We see that representations of target\nwords from similar contexts have a higher cosine\nsimilarity than from different contexts, though with\n10Target words are not necessarily identical across sentence\npairs and can vary e.g. in conjugation or number.\n11It is not possible to obtain an exact mapping from words to\nneat image patch indices following the CONTINUOUS ren-\ndering strategy so we do not present this analysis for PIXEL .\n10159\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cosine similarity\nSimilar context\nTrue\nFalse\nRandom\n(a) BASE -BIGRAMS\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer depth\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n (b) BERT\nFigure 4: Distributions of cosine similarities for verbs and nouns from the WiC dataset across model layers 0-12,\nlayer 0 being the input layer. Every example presents a target word in either a similar or different context across\na sentence pair. The representation of the target word is computed as the mean hidden state output over the\ncorresponding tokens. We generally see that BASE -BIGRAMS encodes target words in a similar context as more\nsimilar. The median cosine similarity between random words from random sentences are shown as a baseline.\na considerable overlap, and higher for different con-\ntexts than for random. When comparing to BERT in\nFigure 4b, there is a clear difference in the similar-\nity compared to random words. The difference in\nsimilarity between similar and random words grad-\nually increases throughout the BASE -BIGRAMS\nmodel, until the final layers, whereas the difference\nsteadily decreases throughout the model for BERT .\nGiven the shared image patch embedding layer\nin PIXEL -based models, random words are more\nsimilar to each other at the input layer when\nmodelled as images than entries in a vocabulary.\nTaken together, these plots suggest that a PIXEL -\nbased language model is capable of forming con-\ntextualised word representations and that these are\nmore context-specific in upper layers, though not\nas fine-grained as seen for BERT .\n6.3 Token frequency and similarity\nThe degree of cosine similarity between random\nwords observed in Figure 4a encourages us to\nassess the isotropic nature of the model (Etha-\nyarajh, 2019; Rajaee and Pilehvar, 2021). The\nhigh cosine similarities suggest that the word\nrepresentations are not evenly distributed with\nrespect to direction in the embedding space, but\ninstead appear to be anisotropic. When learned\nvector representations populate a narrow cone in\nthe embedding space, this geometric alignment\nleads to an overestimation of their similarity (Gao\net al., 2019), which is not an expected property of\nan expressive word embedding space (Arora et al.,\n2016; Mu and Viswanath, 2018).12\nRecent work has shown that Transformer-based\nlanguage models can develop a representation bias\ndriven by token frequency, where low-frequency to-\nkens are clustered together in the embedding space,\nleading to anisotropy in the model (Gao et al.,\n2019; Fuster Baggetto and Fresno, 2022; Jiang\net al., 2022). This bias leads to poor word contex-\ntualisation because the learned vector positions of\nlow frequency words have not moved far from their\nrandom initialisation. Thus, their embeddings are\nnot sufficiently distinct from unrelated words with\nsimilarly low token frequency (Gong et al., 2018;\nCai et al., 2021). Tokens with a higher frequency,\nand thus more parameter updates, can move further\nin the embedding space from their initialisation\nand become more semantically meaningful. Conse-\nquently, we hypothesise that compressing the input\nspace in the form of structured rendering allows the\nmodel to build more contextualised word represen-\ntations through more frequent parameter updates.\nWe investigate this by sampling inputs that were\nseen during pretraining with high and low fre-\nquency. Specifically, we take the 100 most fre-\n12Following Cai et al. (2021) thisglobal estimate of ansiotropy\ndoes not rule out the possibility of distinct and locally\nisotropic clusters in the embedding space. Ding et al. (2022)\nshow that isotropy calibration methods (Gao et al., 2019;\nWang et al., 2020; Li et al., 2020) do not lead to consistent\nimprovements on downstream tasks when models already\nbenefit from local isotropy. We leave this direction for\nPIXEL to future research.\n10160\n0.2 0.4 0.6 0.8 1.0\nCosine similarity\n0\n1\n2\n3Density\nComparing\nHigh, High\nLow, Low\nHigh, Low\n(a) Words in isolation, PIXEL\n0.2 0.4 0.6 0.8 1.0\nCosine similarity\n0\n2\n4\n6 (b) Words in isolation, BASE -BIGRAMS\n0.2 0.4 0.6 0.8 1.0\nCosine similarity\n0\n2\n4\n6 (c) Words in context, BASE -BIGRAMS\nFigure 5: Distributions of cosine similarities within samples of high-frequency words (High), low-frequency words\n(Low), or between the two samples. Rendering with BIGRAMS structure leads to less directionally aligned vector\nrepresentations of frequent words that have seen more updates during pretraining compared to infrequent words.\nquently occurring words from the Wikipedia cor-\npus that was seen during pretraining and 100 words\nthat occur around 1000 times (rank ≈50k).13 We\nfirst render each word from the two frequency sam-\nples in isolation. We then include a comparison to\nwords in context across 100 unique sentences per\nword with BASE -BIGRAMS .14\nWe plot the distributions of cosine similarities\nbetween representations from the last encoder\nlayer, where we expect embeddings from both\nmodels to be contextualised. Comparing the plots\nfrom the two rendering strategies, summarised in\nFigure 5, the effect of pretraining with a smaller\nset of unique tokens becomes clear: for PIXEL\nthe distribution appears as mixtures with a larger\ndistribution mass at higher values of cosine\nsimilarity from comparing high-frequency words\nto other high-frequency (excluding self-similarity\nfor now) than when comparing low-frequency\nto other low-frequency. For BASE -BIGRAMS the\nfrequent words both in isolation and in-context are\nless directionally aligned with each other compared\nto the infrequent, which is in line with therepresen-\ntation degeneration problemfrom Gao et al. (2019)\nand more frequent updates leading to better con-\ntextualisation. Figure 6 visualises the in-context\nrepresentations in 2 dimensions using t-SNE\n(van der Maaten and Hinton, 2008) and provides\nan additional indication of more frequent words\nhaving less locally compact representations.15\nWe expect that in-context representations from\nPIXEL also qualitatively resembles Figure 5a\nbut cannot easily demonstrate this due to the\n13Excluding punctuation and numbers.\n14Recall from § 6.2 that the CONTINUOUS rendering strategy\nby design makes an exact mapping from words in a sentence\nto neat image patch indices unattainable.\n15Plotting the first 2 singular values from a singular value\ndecomposition gives the same qualitative indications.\naforementioned challenges in aligning patch\nembeddings with CONTINUOUS rendering.\n6.4 Frequency bias and semantic modelling\nWhile there is less evidence of representation\ndegeneration with CONTINUOUS rendering, it is\nlikely that the poorer performance on GLUE in\n§ 5.4 is caused by PIXEL seeing too many different\npatches too few times. This is a direct consequence\nof the multitude of ways that similar inputs can be\nrendered by the CONTINUOUS approach. However,\nthe drop in performance when mismatching the\nrendering strategies in § 6.1 for CONTINUOUS →\nBIGRAMS demonstrates that the model has devel-\noped a set of strategy-specific expectations and\nfeatures that are not easily updated. In fact, the new\nrendering strategy for finetuning introduces a set of\npatches that likely never escape the low-frequency\ndomain and therefore remain poorly contextualised.\nSigns of a token frequency bias has also been\nfound in BERT (Fuster Baggetto and Fresno, 2022).\nWe lastly assess the connection between\nvisual token frequency and downstream semantic\nperformance. With BERT , high-frequency words\nhave the most context-specific representations\n(Ethayarajh, 2019), and upper-layer representations\nof low-frequency words are influenced more by\ntheir context than frequent words (V oita et al.,\n2019). Following Ethayarajh (2019), we see that\nthis applies to BASE -BIGRAMS as well (illustrated\nin Figure 7 and discussed in greater detail in\n§ A.5). We expect that sentences that only vary in\nbeing cased or uncased would result in different\nrepresentations when lowercase appears more\nfrequently (for most words). This demonstrates the\nimpact of observed token frequency on semantic\nmodelling and is in line with observed biases in\nBERT ’s embedding space (Jiang et al., 2022).\n10161\n200\n 0 200200\n0\n200\nHigh freq.\nLow freq.\nFigure 6: t-SNE plot of the output\nembeddings of high- and low-\nfrequency words in context from\nBASE -BIGRAMS . Low-frequency\nwords cluster tightly in this space.\n0 5 10\nLayer depth\n0.0\n0.5\n1.0Cosine similarity\nSimilarity\nSelf\nHigh freq.\nLow freq.\nIntra-sentence\nHigh freq.\nLow freq.\nFigure 7: Self- and intra-sentence\nsimilarity from BASE -BIGRAMS .\nHigh-frequency words are the most\ncontext-specific; low-frequency\nwords are influenced by their context.\n0 5 10\nLayer depth\n0.0\n0.5\n1.0Spearman's correlation\nContinuous\nBigrams\nBigrams, uncased\nFigure 8: Evaluation performance\non STS -B. Uncased sentences yield\nbetter performance than the original\nwith BASE -BIGRAMS ; the effect is\nless clear for PIXEL (not shown).\nWe rely on the Semantic Textual Similarity\nBenchmark (STS -B; Cer et al., 2017) also found in\nGLUE for this assessment. We measure the cosine\nsimilarity between sentence representations16 and\nplot its correlation with the gold standard similarity\nscores as the measure of performance. Figure 8\nproves that both CONTINUOUS and BIGRAMS ren-\ndering during pretraining lead to non-trivial seman-\ntic modelling capabilties. At peak performance,\naround the middle layers, the increase from sim-\nply ensuring that all words are uncased is roughly\nthe same as the increase from PIXEL to BASE -\nBIGRAMS . This resembles how frequent and in-\nfrequent tokens have unequal influence on their\ncontext in BERT (V oita et al., 2019).\nSeeing that BASE -BIGRAMS exhibits similar\nrepresentational traits to that of BERT , future work\ncould aim for more semantically capable PIXEL -\nbased models by generalising advances found for\ntokenizer-based models (Gao et al., 2021).\n7 Related work\nRecent work on pixel-based language modelling\nhas demonstrated how visual language under-\nstanding can be achieved through pixels only (Lee\net al., 2022), observed that the visual similarity of\nlanguages plays an important role in cross-lingual\ntransfer (Rahman et al., 2023), and shown how\nunifying the modalities for text and images allow\na single encoder to perform multimodal tasks\n(Tschannen et al., 2023). By relying on bytes\ndirectly, the unification of modalities can be taken\neven further (Jaegle et al., 2021; Horton et al.,\n2023; Yu et al., 2023). The work most closely\n16Mean hidden state output across all tokens in a sentence,\nexcluding the CLS token and black end-of-sequence token.\nrelated to ours, after Rust et al. (2023), is the work\non machine translation with pixel representations\n(Salesky et al., 2021, 2023). A detailed discussion\nof previous pixel-based approaches can be found in\nRust et al. (2023, § 5). Where PIXEL laid the foun-\ndation for general-purpose language encoding with\npixel-based representations, this work takes the\nfirst step towards hypothesis-driven improvements\nwithout adding additional data (Yang et al., 2019)\nor scaling up the model (Conneau and Lample,\n2019). Though it is possible that competitive\nperformance could be achieved by a model with\nCONTINUOUS rendering by pretraining on more\ndata for more steps (Liu et al., 2019).\nOur addition of BIGRAMS structure resembles\nthe addition of optional but hugely beneficial\n(n = 4) -grams in the character-based CANINE\nmodel (Clark et al., 2022). While character-level\nn-gram models (Wieting et al., 2016; Bojanowski\net al., 2017) have been succeeded by Transformer-\nbased language models, character-level features\nremain valuable as they are less sparse and more\nrobust to misspellings than word n-grams, and\nremain useful for especially morphologically rich\nlanguages (Garrette and Baldridge, 2013; Kulmizev\net al., 2017). Previous work have hypothesised that\ncharacter-level models would be more suitable than\nsubword-based for modelling morphologically-rich\nlanguages (Tsarfaty et al., 2020; Keren et al.,\n2022), but a semantically capable design has\nproven non-obvious (Ma et al., 2020; Keren et al.,\n2022; Nzeyimana and Niyongabo Rubungo, 2022;\nSun et al., 2023). We see potential for future\nwork with pixel-based language models exploring\nappropriate strategies for learning morphological\npatterns (Klein and Tsarfaty, 2020; Seker and\nTsarfaty, 2020; Soulos et al., 2021).\n10162\n8 Conclusion\nWe evaluate four text rendering strategies to ad-\ndress the problem of redundancy in the input space\nof PIXEL -based language models. Consequently,\nmore frequent parameter updates lead to better\ncontextualised language representations. We find\nthat rendering two characters per image patch\n(BIGRAMS ) is a good trade-off between efficiency\nand generalisability, resulting in substantial im-\nprovements on downstream semantic and sentence-\nlevel tasks; contributing to open-vocabulary NLP\nwith limited computational resources.\nFurther analyses reveal how the added ren-\ndering structure provokes clear representational\nsimilarities to what has been found in BERT .\nWe see potential in future work generalising\nimprovements found for tokenization-based\nmasked language models to PIXEL -based masked\nlanguage models. Furthermore, considering that\nthe Vision Transformer has also been applied\nto speech modelling (Huang et al., 2022), and\nthat patch representation has been suggested to\nbe a critical component for the success of ViTs\n(Trockman and Kolter, 2023), we see potential for\nimage patches as the basis for unifying modalities.\nLimitations\nWhile the rendering strategies we propose here are\nwell-suited to English, not all equally generalise to\nother languages or scripts. WORDS rendering relies\non word boundaries which may not be readily avail-\nable or well-defined for many languages which do\nnot mark word or sentence boundaries with whites-\npace such as Thai or polysynthetic languages such\nas Inuktitut. MONO and BIGRAMS are more gen-\neral approaches, but may affect the rendering of\npositional characters such as diacritics or correct\ncontextual forms based on where boundaries are\ncreated. For both approaches, it may be necessary\nto modulate font size across languages to ensure\ncharacter pairs fit into a single patch, especially\nwhen rendering with diacritics. MONO provides\nfurther representational efficiency compared to BI-\nGRAMS by fixing character width, but comes at\nthe cost of more limited language coverage; many\nscripts cannot be made fixed-width and fewer than\n10 have mono fonts available. CONTINUOUS ren-\ndering provides a more general approach which\nmust be balanced with learning efficiency.\nAcknowledgements\nJonas F. Lotz is funded by the ROCKWOOL Foun-\ndation (grant 1242). Elizabeth Salesky is supported\nby the Apple Scholars in AI/ML fellowship. Phillip\nRust is funded by the Novo Nordisk Foundation\n(grant NNF 20SA0066568). This work was sup-\nported by a research grant (VIL53122) from VIL-\nLUM FONDEN.\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine Lig-\nnos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Is-\nrael Abebe Azime, Shamsuddeen H. Muhammad,\nChris Chinenye Emezue, Joyce Nakatumba-Nabende,\nPerez Ogayo, Aremu Anuoluwapo, Catherine Gitau,\nDerguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-\nmam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,\nRubungo Andre Niyongabo, Jonathan Mukiibi, Ver-\nrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,\nGerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane MBOUP, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021. MasakhaNER: Named entity\nrecognition for African languages. Transactions\nof the Association for Computational Linguistics,\n9:1116–1131.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A latent variable model\napproach to PMI-based word embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385–399.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embedding\nspace: Clusters and manifolds. In International Con-\nference on Learning Representations.\n10163\nLewis Carroll. 1865. Alice’s Adventures in Wonderland.\nMacmillan.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022. Canine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 10:73–91.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li,\nand Li Fei-Fei. 2009. ImageNet: A Large-Scale\nHierarchical Image Database. In CVPR09.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYue Ding, Karolis Martinkus, Damian Pascual, Si-\nmon Clematide, and Roger Wattenhofer. 2022. On\nisotropy calibration of transformer models. In Pro-\nceedings of the Third Workshop on Insights from Neg-\native Results in NLP, pages 1–9, Dublin, Ireland.\nAssociation for Computational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nAlejandro Fuster Baggetto and Victor Fresno. 2022. Is\nanisotropy really the cause of BERT embeddings\nnot being semantic? In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n4271–4281, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and\nTieyan Liu. 2019. Representation degeneration prob-\nlem in training natural language generation models.\nIn International Conference on Learning Representa-\ntions.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDan Garrette and Jason Baldridge. 2013. Learning a\npart-of-speech tagger from two hours of annotation.\nIn Proceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 138–147, Atlanta, Georgia. Association for\nComputational Linguistics.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: Frequency-agnostic\nword representation. arXiv preprint.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Doll’ar, and Ross B. Girshick. 2021. Masked\nautoencoders are scalable vision learners. 2022\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 15979–15988.\nDaniel Hershcovich, Nicolas Webersinke, Mathias\nKraus, Julia Bingler, and Markus Leippold. 2022.\nTowards climate awareness in NLP research. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2480–\n2494, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nMaxwell Horton, Sachin Mehta, Ali Farhadi, and Mo-\nhammad Rastegari. 2023. Bytes are all you need:\nTransformers operating directly on file bytes. arXiv\npreprint.\nPo-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,\nMichael Auli, Wojciech Galuba, Florian Metze, and\nChristoph Feichtenhofer. 2022. Masked autoen-\ncoders that listen. In NeurIPS.\nMaor Ivgi, Yair Carmon, and Jonathan Berant. 2022.\nScaling laws under the microscope: Predicting trans-\nformer performance from small scale experiments.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 7354–7371, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\n10164\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste\nAlayrac, Carl Doersch, Catalin Ionescu, David Ding,\nSkanda Koppula, Andrew Brock, Evan Shelhamer,\nOlivier J. H’enaff, Matthew M. Botvinick, Andrew\nZisserman, Oriol Vinyals, and João Carreira. 2021.\nPerceiver io: A general architecture for structured\ninputs & outputs. arXiv preprint.\nTing Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang,\nDeqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen\nHuang, Denvy Deng, and Qi Zhang. 2022. Prompt-\nBERT: Improving BERT sentence embeddings with\nprompts. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8826–8837, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nOmri Keren, Tal Avinari, Reut Tsarfaty, and Omer\nLevy. 2022. Breaking character: Are subwords good\nenough for mrls after all? arXiv preprint.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How adequate are word-pieces for mod-\nelling complex morphology? In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 204–209, Online. Association for Computa-\ntional Linguistics.\nArtur Kulmizev, Bo Blankers, Johannes Bjerva, Malvina\nNissim, Gertjan van Noord, Barbara Plank, and Mar-\ntijn Wieling. 2017. The power of character n-grams\nin native language identification. In Proceedings of\nthe 12th Workshop on Innovative Use of NLP for\nBuilding Educational Applications, pages 382–389,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,\nFangyu Liu, Julian Martin Eisenschlos, Urvashi\nKhandelwal, Peter Shaw, Ming-Wei Chang, and\nKristina Toutanova. 2022. Pix2struct: Screenshot\nparsing as pretraining for visual language understand-\ning. arXiv preprint.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin\nWang, and Guoping Hu. 2020. CharBERT: Character-\naware pre-trained language model. In Proceedings\nof the 28th International Conference on Computa-\ntional Linguistics, pages 39–50, Barcelona, Spain\n(Online). International Committee on Computational\nLinguistics.\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky,\nColin Raffel, Manan Dey, Matthias Gallé, Arun Raja,\nChenglei Si, Wilson Y . Lee, Benoît Sagot, and Sam-\nson Tan. 2021. Between words and characters: A\nbrief history of open-vocabulary modeling and tok-\nenization in nlp. arXiv preprint.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-top:\nSimple and effective postprocessing for word repre-\nsentations. In International Conference on Learning\nRepresentations.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 4034–4043, Marseille,\nFrance. European Language Resources Association.\nAntoine Nzeyimana and Andre Niyongabo Rubungo.\n2022. KinyaBERT: a morphology-aware Kin-\nyarwanda language model. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5347–5363, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. WiC: the word-in-context dataset for evalu-\nating context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1267–1273,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMd Mushfiqur Rahman, Fardin Ahsan Sakib, Fahim\nFaisal, and Antonios Anastasopoulos. 2023. To token\nor not to token: A comparative study of text repre-\nsentations for cross-lingual transfer. arXiv preprint.\nSara Rajaee and Mohammad Taher Pilehvar. 2021. A\ncluster-based approach for improving isotropy in con-\ntextual embedding space. In Proceedings of the 59th\n10165\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 575–584, Online. Association\nfor Computational Linguistics.\nPhillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Eliza-\nbeth Salesky, Miryam de Lhoneux, and Desmond El-\nliott. 2023. Language modelling with pixels. ICLR.\nElizabeth Salesky, David Etter, and Matt Post. 2021.\nRobust open-vocabulary translation from visual text\nrepresentations. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 7235–7252, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nElizabeth Salesky, Neha Verma, Philipp Koehn, and\nMatt Post. 2023. Multilingual pixel representations\nfor translation and effective cross-lingual transfer.\narXiv preprint.\nAmit Seker and Reut Tsarfaty. 2020. A pointer net-\nwork architecture for joint morphological segmen-\ntation and tagging. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4368–4378, Online. Association for Computational\nLinguistics.\nPaul Soulos, Sudha Rao, Caitlin Smith, Eric Rosen,\nAsli Celikyilmaz, R. Thomas McCoy, Yichen Jiang,\nColeman Haley, Roland Fernandez, Hamid Palangi,\nJianfeng Gao, and Paul Smolensky. 2021. Structural\nbiases for improving transformers on translation into\nmorphologically rich languages. In Proceedings of\nthe 4th Workshop on Technologies for MT of Low\nResource Languages (LoResMT2021), pages 52–67,\nVirtual. Association for Machine Translation in the\nAmericas.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJimin Sun, Patrick Fernandes, Xinyi Wang, and Gra-\nham Neubig. 2023. A multi-dimensional evaluation\nof tokenizer-free multilingual pretrained models. In\nFindings of the Association for Computational Lin-\nguistics: EACL 2023, pages 1725–1735, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Hervé Jé-\ngou. 2021. Training data-efficient image transform-\ners & distillation through attention. In ICML, pages\n10347–10357.\nAsher Trockman and J Zico Kolter. 2023. Patches are\nall you need? Transactions on Machine Learning\nResearch. Featured Certification.\nReut Tsarfaty, Dan Bareket, Stav Klein, and Amit Seker.\n2020. From SPMRL to NMRL: What did we learn\n(and unlearn) in a decade of parsing morphologically-\nrich languages (MRLs)? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7396–7408, Online. Association\nfor Computational Linguistics.\nMichael Tschannen, Basil Mustafa, and Neil Houlsby.\n2023. Image-and-language understanding from pix-\nels only. 2023 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR).\nLaurens van der Maaten and Geoffrey E. Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9:2579–2605.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nLingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,\nGuangtao Wang, and Quanquan Gu. 2020. Improv-\ning neural language generation with spectrum control.\nIn International Conference on Learning Representa-\ntions.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016. Charagram: Embedding words and\nsentences via character n-grams. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1504–1515, Austin,\nTexas. Association for Computational Linguistics.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291–306.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nLili Yu, Dániel Simig, Colin Flaherty, Armen Agha-\njanyan, Luke Zettlemoyer, and Mike Lewis. 2023.\n10166\nMegabyte: Predicting million-byte sequences with\nmultiscale transformers. arXiv preprint.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia\nAckermann, Noëmi Aepli, Hamid Aghaei, Željko\nAgi´c, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy\nAjede, Gabriel˙e Aleksandraviˇci¯ut˙e, Ika Alfina, Avner\nAlgom, Erik Andersen, Lene Antonsen, Katya\nAplonova, Angelina Aquino, Carolina Aragon,\nGlyd Aranes, Maria Jesus Aranzabe, Bilge Nas\nArıcan, ⁀Hórunn Arnardóttir, Gashaw Arutie, Jes-\nsica Naraiswari Arwidarasti, Masayuki Asahara,\nDeniz Baran Aslan, Cengiz Asmazo ˘glu, Luma\nAteyah, Furkan Atmaca, Mohammed Attia, Aitz-\niber Atutxa, Liesbeth Augustinus, Elena Badmaeva,\nKeerthana Balasubramani, Miguel Ballesteros, Esha\nBanerjee, Sebastian Bank, Verginica Barbu Mititelu,\nStarkaður Barkarson, Rodolfo Basile, Victoria Bas-\nmov, Colin Batchelor, John Bauer, Seyyit Talha Bedir,\nKepa Bengoetxea, Yifat Ben Moshe, Gözde Berk,\nYevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ah-\nmad Bhat, Erica Biagetti, Eckhard Bick, Agn ˙e\nBielinskien˙e, Kristín Bjarnadóttir, Rogier Blok-\nland, Victoria Bobicev, Loïc Boizou, Emanuel\nBorges Völker, Carl Börstell, Cristina Bosco, Gosse\nBouma, Sam Bowman, Adriane Boyd, Anouck Brag-\ngaar, Kristina Brokait˙e, Aljoscha Burchardt, Marie\nCandito, Bernard Caron, Gauthier Caron, Lauren\nCassidy, Tatiana Cavalcanti, Gül¸ sen Cebiro˘glu Ery-\ni˘git, Flavio Massimiliano Cecchini, Giuseppe G. A.\nCelano, Slavomír ˇCéplö, Neslihan Cesur, Savas\nCetin, Özlem Çetino ˘glu, Fabricio Chalub, Shweta\nChauhan, Ethan Chi, Taishi Chika, Yongseok Cho,\nJinho Choi, Jayeol Chun, Juyeon Chung, Alessan-\ndra T. Cignarella, Silvie Cinková, Aurélie Collomb,\nÇa˘grı Çöltekin, Miriam Connor, Daniela Corbetta,\nMarine Courtin, Mihaela Cristescu, Philemon Daniel,\nElizabeth Davidson, Mathieu Dehouck, Martina\nde Laurentiis, Marie-Catherine de Marneffe, Vale-\nria de Paiva, Mehmet Oguz Derin, Elvis de Souza,\nArantza Diaz de Ilarraza, Carly Dickerson, Arawinda\nDinakaramani, Elisa Di Nuovo, Bamba Dione, Pe-\nter Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira\nDroganova, Puneet Dwivedi, Hanne Eckhoff, Sandra\nEiche, Marhaba Eli, Ali Elkahky, Binyam Ephrem,\nOlga Erina, Tomaž Erjavec, Aline Etienne, Wograine\nEvelyn, Sidney Facundes, Richárd Farkas, Feder-\nica Favero, Jannatul Ferdaousi, Marília Fernanda,\nHector Fernandez Alcalde, Jennifer Foster, Cláu-\ndia Freitas, Kazunori Fujita, Katarína Gajdošová,\nDaniel Galbraith, Federica Gamba, Marcos Gar-\ncia, Moa Gärdenfors, Sebastian Garza, Fabrício Fer-\nraz Gerardi, Kim Gerdes, Filip Ginter, Gustavo\nGodoy, Iakes Goenaga, Koldo Gojenola, Memduh\nGökırmak, Yoav Goldberg, Xavier Gómez Guino-\nvart, Berta González Saavedra, Bernadeta Grici ¯ut˙e,\nMatias Grioni, Loïc Grobol, Normunds Gr ¯uz¯ıtis,\nBruno Guillaume, Céline Guillot-Barbance, Tunga\nGüngör, Nizar Habash, Hinrik Hafsteinsson, Jan Ha-\njiˇc, Jan Hajiˇc jr., Mika Hämäläinen, Linh Hà M˜y, Na-\nRae Han, Muhammad Yudistira Hanifmuti, Takahiro\nHarada, Sam Hardwick, Kim Harris, Dag Haug,\nJohannes Heinecke, Oliver Hellwig, Felix Hennig,\nBarbora Hladká, Jaroslava Hlaváˇcová, Florinel Hoci-\nung, Petter Hohle, Jena Hwang, Takumi Ikeda, An-\nton Karl Ingason, Radu Ion, Elena Irimia, O. lájídé\nIshola, Kaoru Ito, Siratun Jannat, Tomáš Jelínek,\nApoorva Jha, Anders Johannsen, Hildur Jónsdóttir,\nFredrik Jørgensen, Markus Juutinen, Sarveswaran\nK, Hüner Ka¸ sıkara, Andre Kaasen, Nadezhda\nKabaeva, Sylvain Kahane, Hiroshi Kanayama, Jenna\nKanerva, Neslihan Kara, Ritván Karahó ˇga, Boris\nKatz, Tolga Kayadelen, Jessica Kenney, Václava\nKettnerová, Jesse Kirchner, Elena Klementieva,\nElena Klyachko, Arne Köhn, Abdullatif Köksal,\nKamil Kopacewicz, Timo Korkiakangas, Mehmet\nKöse, Natalia Kotsyba, Jolanta Kovalevskait ˙e, Si-\nmon Krek, Parameswari Krishnamurthy, Sandra\nKübler, O ˘guzhan Kuyrukçu, Aslı Kuzgun, Sooky-\noung Kwak, Veronika Laippala, Lucia Lam, Lorenzo\nLambertino, Tatiana Lando, Septina Dian Larasati,\nAlexei Lavrentiev, John Lee, Phuong Lê H `ông,\nAlessandro Lenci, Saran Lertpradit, Herman Leung,\nMaria Levina, Cheuk Ying Li, Josie Li, Keying Li,\nYuan Li, KyungTae Lim, Bruna Lima Padovani, Kris-\nter Lindén, Nikola Ljube ˇsi´c, Olga Loginova, Ste-\nfano Lusito, Andry Luthfi, Mikko Luukko, Olga\nLyashevskaya, Teresa Lynn, Vivien Macketanz,\nMenel Mahamdi, Jean Maillard, Aibek Makazhanov,\nMichael Mandl, Christopher Manning, Ruli Ma-\nnurung, B ¨us ¸ra Mar s ¸an, Cătălina Mărănduc, David\nMareˇcek, Katrin Marheinecke, Stella Markantonatou,\nHéctor Martínez Alonso, Lorena Martín Rodríguez,\nAndré Martins, Jan Ma ˇsek, Hiroshi Matsuda, Yuji\nMatsumoto, Alessandro Mazzei, Ryan McDonald,\nSarah McGuinness, Gustavo Mendon c ¸a, Tatiana\nMerzhevich, Niko Miekka, Karina Mischenkova,\nMargarita Misirpashayeva, Anna Missil ¨a, Cătălin\nMititelu, Maria Mitrofan, Yusuke Miyao, AmirHos-\nsein Mojiri Foroushani, Judit Molnár, Amirsaeid\nMoloodi, Simonetta Montemagni, Amir More, Laura\nMoreno Romero, Giovanni Moretti, Keiko Sophie\nMori, Shinsuke Mori, Tomohiko Morioka, Shigeki\nMoro, Bjartur Mortensen, Bohdan Moskalevskyi,\nKadri Muischnek, Robert Munro, Yugo Murawaki,\nKaili M ¨u¨urisep, Pinkey Nainwani, Mariam Nakhlé,\nJuan Ignacio Navarro Hor ˜niacek, Anna Nedoluzhko,\nGunta Neˇspore-B¯erzkalne, Manuela Nevaci, Luong\nNguy˜ên Thị, Huy `ên Nguy ˜ên Thị Minh, Yoshihiro\nNikaido, Vitaly Nikolaev, Rattima Nitisaroj, Alireza\nNourian, Hanna Nurmi, Stina Ojala, Atul Kr. Ojha,\nAdédayò Olúòkun, Mai Omura, Emeka Onwueg-\nbuzia, Noam Ordan, Petya Osenova, Robert ¨Ostling,\nLilja Øvrelid, S ¸aziye Bet ¨ul ¨Ozates ¸, Merve ¨Ozc ¸elik,\nArzucan ¨Ozg¨ur, Balkız ¨Ozt¨urk Bas ¸aran, Teresa Pac-\ncosi, Alessio Palmero Aprosio, Hyunji Hayley Park,\nNiko Partanen, Elena Pascual, Marco Passarotti, Ag-\nnieszka Patejuk, Guilherme Paulino-Passos, Giulia\nPedonese, Angelika Peljak- Łapi´nska, Siyao Peng,\nCenel-Augusto Perez, Natalia Perkova, Guy Perrier,\nSlav Petrov, Daria Petrova, Andrea Peverelli, Jason\nPhelan, Jussi Piitulainen, Tommi A Pirinen, Emily\nPitler, Barbara Plank, Thierry Poibeau, Larisa Pono-\nmareva, Martin Popel, Lauma Pretkalni n ¸a, Sophie\nPrévost, Prokopis Prokopidis, Adam Przepiórkowski,\nTiina Puolakainen, Sampo Pyysalo, Peng Qi, An-\ndriela R ¨a¨abis, Alexandre Rademaker, Mizanur Ra-\n10167\nhoman, Taraka Rama, Loganathan Ramasamy, Carlos\nRamisch, Fam Rashel, Mohammad Sadegh Rasooli,\nVinit Ravishankar, Livy Real, Petru Rebeja, Siva\nReddy, Mathilde Regnault, Georg Rehm, Ivan Ri-\nabov, Michael Rie ßler, Erika Rimkut ˙e, Larissa Ri-\nnaldi, Laura Rituma, Putri Rizqiyah, Luisa Rocha,\nEiríkur R ¨ognvaldsson, Mykhailo Romanenko, Rudolf\nRosa, Valentin Ro s, ca, Davide Rovati, Ben Ro-\nzonoyer, Olga Rudina, Jack Rueter, Kristján Rú-\nnarsson, Shoval Sadde, Pegah Safari, Beno ˆıt Sagot,\nAleksi Sahala, Shadi Saleh, Alessio Salomoni,\nTanja Samardˇzi´c, Stephanie Samson, Manuela San-\nguinetti, Ezgi Sanıyar, Dage S ¨arg, Baiba Saul ¯ıte,\nYanin Sawanakunanon, Shefali Saxena, Kevin Scan-\nnell, Salvatore Scarlata, Nathan Schneider, Sebastian\nSchuster, Lane Schwartz, Djamé Seddah, Wolfgang\nSeeker, Mojgan Seraji, Syeda Shahzadi, Mo Shen,\nAtsuko Shimada, Hiroyuki Shirasu, Yana Shishk-\nina, Muh Shohibussirri, Dmitry Sichinava, Janine\nSiewert, Einar Freyr Sigurðsson, Aline Silveira,\nNatalia Silveira, Maria Simi, Radu Simionescu,\nKatalin Simkó, Mária Šimková, Kiril Simov, Maria\nSkachedubova, Aaron Smith, Isabela Soares-Bastos,\nShafi Sourov, Carolyn Spadine, Rachele Sprugnoli,\nVivian Stamou, Stein ⁀hór Steingrímsson, Antonio\nStella, Milan Straka, Emmett Strickland, Jana Str-\nnadová, Alane Suhr, Yogi Lesmana Sulestio, Umut\nSulubacak, Shingo Suzuki, Daniel Swanson, Zsolt\nSzántó, Chihiro Taguchi, Dima Taji, Yuta Takahashi,\nFabio Tamburini, Mary Ann C. Tan, Takaaki Tanaka,\nDipta Tanaya, Mirko Tavoni, Samson Tella, Is-\nabelle Tellier, Marinella Testori, Guillaume Thomas,\nSara Tonelli, Liisi Torga, Marsida Toska, Trond\nTrosterud, Anna Trukhina, Reut Tsarfaty, Utku Türk,\nFrancis Tyers, Sumire Uematsu, Roman Untilov,\nZdeˇnka Urešová, Larraitz Uria, Hans Uszkoreit, An-\ndrius Utka, Elena Vagnoni, Sowmya Vajjala, Rob\nvan der Goot, Martine Vanhove, Daniel van Niek-\nerk, Gertjan van Noord, Viktor Varga, Uliana Ve-\ndenina, Eric Villemonte de la Clergerie, Veronika\nVincze, Natalia Vlasova, Aya Wakasa, Joel C. Wal-\nlenberg, Lars Wallin, Abigail Walsh, Jing Xian Wang,\nJonathan North Washington, Maximilan Wendt,\nPaul Widmer, Shira Wigderson, Sri Hartati Wi-\njono, Seyi Williams, Mats Wirén, Christian Wit-\ntern, Tsegay Woldemariam, Tak-sum Wong, Alina\nWróblewska, Mary Yako, Kayo Yamashita, Naoki\nYamazaki, Chunxiao Yan, Koichi Yasuoka, Marat M.\nYavrumyan, Arife Betül Yenice, Olcay Taner Yıldız,\nZhuoran Yu, Arlisa Yuliawati, Zdenˇek Žabokrtský,\nShorouq Zahra, Amir Zeldes, He Zhou, Hanzhi Zhu,\nAnna Zhuravleva, and Rayan Ziane. 2022. Universal\ndependencies 2.10. LINDAT/CLARIAH-CZ digital\nlibrary at the Institute of Formal and Applied Linguis-\ntics (ÚFAL), Faculty of Mathematics and Physics,\nCharles University.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby,\nand Lucas Beyer. 2021. Scaling vision transformers.\n2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1204–1213.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n10168\nA Appendix\nA.1 Representational efficiency\n0 500 1000 1500\nLength [Patches]\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Proportion\nRenderer\ncontinuous\nbigrams\nmono\nwords\nFigure 9: Distributions of sequence lengths (in patches) resulting from different rendering strategies.\nAs seen in Figure 1, structured rendering compresses the input space by reducing the positions characters\nmay be observed in. This dramatically affects the number of unique inputs observed in a fixed number of\nsequences, as quantified in Figure 3. Concretely, the 10 most frequently observed image patches after\nprocessing 100,000 sequences from English Wikipedia are shown in Figure 2; with continuous rendering\nall are positional variants of the same subword, while with structured rendering each represents different\nwords or morphemes. However, instituting word- or subword-level structure with whitespace padding\nincreases sequence lengths compared to unstructured rendering as quantified in Figure 9.\nA.2 Pretraining loss curves\n0k 100k 200k 300k 400k 500k\nTraining steps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Loss\nRenderer\ncontinuous\nbigrams\nmono\nwords\nFigure 10: Pretraining loss for SMALL models with different rendering strategies, indicating that structured rendering\nmay make the masked reconstruction task more data efficient, reaching a low loss in fewer steps.\n10169\nA.3 Detailed experimental results\nENG ARA COP HIN JPN KOR TAM VIE ZHO AVG\nBERT 90.6 77.7 13.0 75.9 73.8 30.2 15.2 49.4 28.8 50.5\nPIXEL 88.7 77.3 83.5 89.2 90.7 78.5 52.6 50.5 73.7 76.1\nTINY-CONTINUOUS 78.9 74.6 80.0 87.9 89.9 75.1 48.3 46.2 69.5 72.3\nStructure\nSMALL-CONTINUOUS 87.2 77.2 83.4 88.9 91.0 78.8 53.8 51.9 73.5 76.2\nSMALL-BIGRAMS 87.9 75.4 84.1 88.9 90.8 79.4 53.9 50.9 73.9 76.1\nSMALL-MONO 88.3 76.8 83.4 88.9 91.0 79.0 50.5 51.3 73.8 75.9\nSMALL-WORDS 88.0 77.2 83.9 89.3 91.2 78.7 53.7 53.3 74.2 76.6\nScale\nTINY-BIGRAMS 82.9 70.6 79.1 86.2 90.0 76.2 44.9 47.6 69.8 72.0\nSMALL-BIGRAMS 87.9 75.4 84.1 88.9 90.8 79.4 53.9 50.9 73.9 76.1\nBASE-BIGRAMS 89.6 77.7 81.4 88.6 90.8 78.1 49.8 49.4 73.9 75.5\nTable 4: Test set LAS results for dependency parsing on a selection of Universal Dependencies treebanks (UDP ).\nMNLI-M/MM QQP QNLI SST -2 COLA STS -B MRPC RTE WNLI AVG\nBERT 84.0 / 84.2 87.6 91.0 92.6 60.3 88.8 90.2 69.5 51.8 80.0\nPIXEL 78.1 / 78.9 84.5 87.8 89.6 38.4 81.1 88.2 60.5 53.8 74.1\nTINY-CONTINUOUS 36.7 / 37.0 76.6 72.9 87.2 2.1 25.1 82.4 58.5 59.2 53.8\nStructure\nSMALL-CONTINUOUS 72.2 / 73.6 84.8 86.2 88.3 19.1 81.7 84.6 61.4 57.7 71.0\nSMALL-BIGRAMS 77.3 / 78.1 85.7 87.8 90.4 42.3 84.3 87.8 63.5 56.3 75.4\nSMALL-MONO 77.4 / 77.6 84.7 86.8 89.4 42.3 82.4 86.9 57.5 58.9 74.4\nSMALL-WORDS 76.7 / 77.3 84.5 86.6 89.9 44.6 80.5 87.4 62.8 56.3 74.7\nScale\nTINY-BIGRAMS 60.8 / 61.9 79.6 81.7 87.2 15.6 77.9 83.0 59.4 57.7 66.5\nSMALL-BIGRAMS 77.3 / 78.1 85.7 87.8 90.4 42.3 84.3 87.8 63.5 56.3 75.4\nBASE-BIGRAMS 81.1 / 81.4 87.6 89.7 90.4 53.3 86.6 90.2 63.5 56.3 78.0\nTable 5: Validation set performance on GLUE . The reported metrics are F1 score for QQP and MRPC , Matthew’s\ncorrelation for COLA , Spearman’s ρfor STS -B, and accuracy for the rest.\n10170\nENG ARA BEN FIN IND KOR RUS SWA TEL AVG\nBERT 68.5 58.0 43.2 58.3 67.1 12.4 53.2 71.3 48.2 51.5\nPIXEL 59.6 57.3 36.3 57.1 63.6 26.1 50.5 65.9 61.7 52.3\nTINY-CONTINUOUS 42.6 45.0 12.4 45.3 48.1 13.2 36.7 46.8 45.7 36.6\nSMALL-CONTINUOUS 57.1 53.3 20.3 57.5 62.9 22.3 51.1 65.3 58.1 48.8\nScale\nTINY-BIGRAMS 43.3 45.5 19.0 50.3 48.2 14.9 45.4 52.7 56.4 41.6\nSMALL-BIGRAMS 50.8 53.2 37.1 59.1 57.5 20.1 52.8 62.4 64.2 50.8\nBASE-BIGRAMS 53.8 53.1 46.5 59.6 60.3 18.8 54.1 64.1 65.7 52.8\nTable 6: Validation set F1 scores for TyDiQA-GoldP. Average (AVG) scores exclude ENG (Clark et al., 2020). With\nsome rendering structures, answer span extraction adversely affects results (see discussion at § A.4).\nAMH HAU IBO KIN LUG LUO PCM SWA WOL YOR AVG\nBERT 0 86.6 83.5 72.0 78.4 73.2 87.0 83.3 62.2 73.8 62.7\nPIXEL 47.7 82.4 79.9 64.2 76.5 66.6 78.7 79.8 59.7 70.7 70.6\nBASE-BIGRAMS 50.1 85.6 82.2 68.4 78.4 72.5 82.8 82.4 64.4 74.8 74.2\nTable 7: Test set F1 scores on MasakhaNER (Adelani et al., 2021). We follow the implementation of Rust et al.\n(2023) and render each word at the start of a new image patch.\nA.4 TyDiQa-GoldP\nThe CONTINUOUS rendering strategy used for PIXEL , in which words often overlap in an image patch,\nleads to extracted answer spans that potentially include leading or trailing characters that should not be\npart of the answer. BIGRAMS rendering adressess this issue by yielding clear word boundaries in the\ninput representations.\nHowever, theBIGRAMS rendering strategy poses new challenges to extracting answer spans for TyDiQA-\nGoldP. While the task is simplified compared to the primary task by removing language tracks that lack\nwhitespace,17 we find that a surprisingly high number of “words” are a string of comma-separated words\nor concatenations of characters and letters that should be delimited by whitespace. By design we consider\nand render these as one unit when we only split by whitespace. An example of a single “unit” from the\ntraining split highlights this issue more clearly: “oikeudet[1]Lääni[1]1Vilna523,0501387Vilnan”18 where\nthe expected answer is “Vilna” and highlighted in bold. In such an instance, a PIXEL BIGRAMS model\nwill predict the whole unit, resulting in a lower performance. Furthermore, some of these “words” in\nthe training data are more than a thousand characters long and therefore do not fit within the maximum\nsequence length of 529 patches.\n17https://github.com/google-research-datasets/tydiqa/blob/master/gold_passage_baseline/README.md\n18id = finnish-1438027099681899178-6\n10171\nA.5 Measuring self-similarity and intra-sentence similarity\nWe follow Ethayarajh (2019) and measure the degree of self-similarity and intra-sentence similarity for\nthe words in the two frequency samples from § 6.3. Self-similarity is computed as the cosine similarity\nbetween the same word in different sentences and a high degree therefore indicates that representations\nvary little across contexts. For intra-sentence similarity we compute the cosine similarity between a word\nrepresentation and the sentence representation (mean hidden state output across all tokens excluding the\nCLS token and black end-of-sequence token).19 This captures how aligned the representation of a word\nis with the sentence as a whole. If a word has both a low degree of self-similarity and intra-sentence\nsimilarity, we infer that the word has a context-specific representation that is still distinct from the other\nwords in that sentence. If self-similarity is low but intra-sentence similarity is high, this alludes to the\nword simply being contextualised by aligning its representation with the other words in that sentence. We\nsummarise these two measures in Figure 7 and find that, just like in Figure 4a, the upper layers produce\nmore context-specific representations as seen by the lower self-similarity, and that high-frequency words\nare the most context-specific. This is in line with Ethayarajh (2019) who finds that stopwords, being some\nof the most frequently observed words in the pretraining data, have some of the most context-specific\nrepresentations. The measure of intra-sentence similarity reveals that the contextualised representation\nof low-frequency words is more similar to that of its context, with high-frequency words having more\nnuance where words do not necessarily mean the same just because they appear in the same sentence.\n19Ethayarajh (2019) average over every word-sentence combination for a given sentence, not just a single word.\n10172"
}