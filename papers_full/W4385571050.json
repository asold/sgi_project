{
  "title": "Do Large Language Models Know What They Don’t Know?",
  "url": "https://openalex.org/W4385571050",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4377904255",
      "name": "Zhangyue Yin",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2121275033",
      "name": "Qiushi Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115738570",
      "name": "Qipeng Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110996022",
      "name": "Jiawen Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4376122749",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W2963339397"
  ],
  "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8653–8665\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDo Large Language Models Know What They Don’t Know?\nZhangyue Yin♢ Qiushi Sun♠ Qipeng Guo♢\nJiawen Wu♢ Xipeng Qiu♢∗ Xuanjing Huang♢\n♢School of Computer Science, Fudan University\n♠Department of Mathematics, National University of Singapore\n{yinzy21,jwwu21}@m.fudan.edu.cn qiushisun@u.nus.edu\n{qpguo16,xpqiu,xjhuang}@fudan.edu.cn\nAbstract\nLarge language models (LLMs) have a wealth\nof knowledge that allows them to excel in vari-\nous Natural Language Processing (NLP) tasks.\nCurrent research focuses on enhancing their\nperformance within their existing knowledge.\nDespite their vast knowledge, LLMs are still\nlimited by the amount of information they can\naccommodate and comprehend. Therefore, the\nability to understand their own limitations on\nthe unknows, referred to as self-knowledge,\nis of paramount importance. This study aims\nto evaluate LLMs’ self-knowledge by assess-\ning their ability to identify unanswerable or\nunknowable questions. We introduce an auto-\nmated methodology to detect uncertainty in the\nresponses of these models, providing a novel\nmeasure of their self-knowledge. We further in-\ntroduce a unique dataset, SelfAware, consisting\nof unanswerable questions from five diverse cat-\negories and their answerable counterparts. Our\nextensive analysis, involving 20 LLMs includ-\ning GPT-3, InstructGPT, and LLaMA, discov-\nering an intrinsic capacity for self-knowledge\nwithin these models. Moreover, we demon-\nstrate that in-context learning and instruction\ntuning can further enhance this self-knowledge.\nDespite this promising insight, our findings also\nhighlight a considerable gap between the capa-\nbilities of these models and human proficiency\nin recognizing the limits of their knowledge.\n“True wisdom is knowing what you don’t know.”\n–Confucius\n1 Introduction\nRecently, Large Language Models (LLMs) such\nas GPT-4 (OpenAI, 2023), PaLM 2 (Anil et al.,\n2023), and LLaMA (Touvron et al., 2023) have\nshown exceptional performance on a wide range\nof NLP tasks, including common sense reason-\ning (Wei et al., 2022; Zhou et al., 2022) and mathe-\n∗ Corresponding author.\nUnknows\nKnowsUnknows\nKnows\nKnown Knows Known Unknows\nUnknown UnknowsUnknown Knows\nUnlock\nFigure 1: Know-Unknow Quadrant. The horizontal axis\nrepresents the model’s memory capacity for knowledge,\nand the vertical axis represents the model’s ability to\ncomprehend and utilize knowledge.\nmatical problem-solving (Lewkowycz et al., 2022;\nChen et al., 2022). Despite their ability to learn\nfrom huge amounts of data, LLMs still have lim-\nitations in their capacity to retain and understand\ninformation. To ensure responsible usage, it is cru-\ncial for LLMs to have the capability of recognizing\ntheir limitations and conveying uncertainty when\nresponding to unanswerable or unknowable ques-\ntions. This acknowledgment of limitations, also\nknown as “ knowing what you don’t know,” is a\ncrucial aspect in determining their practical appli-\ncability. In this work, we refer to this ability as\nmodel self-knowledge.\nThe Know-Unknow quadrant in Figure 1 il-\nlustrates the relationship between the model’s\nknowledge and comprehension. The ratio of\n“Known Knows” to “Unknown Knows” demon-\nstrates the model’s proficiency in understanding\nand applying existing knowledge. Techniques\nsuch as Chain-of-Thought (Wei et al., 2022), Self-\nConsistency (Wang et al., 2022), and Complex\nCoT (Fu et al., 2022) can be utilized to increase\n8653\nthis ratio, resulting in improved performance on\nNLP tasks. We focus on the ratio of “Known Un-\nknows” to “Unknown Unknows”, which indicates\nthe model’s self-knowledge level, specifically un-\nderstanding its own limitations and deficiencies in\nthe unknows.\nExisting datasets such as SQuAD2.0 (Rajpurkar\net al., 2018) and NewsQA (Trischler et al., 2017),\nwidely used in question answering (QA), have been\nutilized to test the self-knowledge of models with\nunanswerable questions. However, these questions\nare context-specific and could become answerable\nwhen supplemented with additional information.\nSrivastava et al. (2022) attempted to address this by\nevaluating LLMs’ competence in delineating their\nknowledge boundaries, employing a set of 23 pairs\nof answerable and unanswerable multiple-choice\nquestions. They discovered that these models’ per-\nformance barely surpassed that of random guessing.\nKadavath et al. (2022) suggested probing the self-\nknowledge of LLMs through the implementation\nof a distinct \"Value Head\". Yet, this approach may\nencounter difficulties when applied across varied\ndomains or tasks due to task-specific training. Con-\nsequently, we redirect our focus to the inherent\nabilities of LLMs, and pose the pivotal question:\n“Do large language models know what they don’t\nknow?”.\nIn this study, we investigate the self-knowledge\nof LLMs using a novel approach. By gathering\nreference sentences with uncertain meanings, we\ncan determine whether the model’s responses re-\nflect uncertainty using a text similarity algorithm.\nWe quantified the model’s self-knowledge using\nthe F1 score. To address the small and idiosyn-\ncratic limitations of existing datasets, we created\na new dataset called SelfAware. This dataset com-\nprises 1,032 unanswerable questions, which are dis-\ntributed across five distinct categories, along with\nan additional 2,337 questions that are classified as\nanswerable. Experimental results on GPT-3, In-\nstructGPT, LLaMA, and other LLMs demonstrate\nthat in-context learning and instruction tuning can\neffectively enhance the self-knowledge of LLMs.\nHowever, the self-knowledge exhibited by the cur-\nrent state-of-the-art model, GPT-4, measures at\n75.47%, signifying a notable disparity when con-\ntrasted with human self-knowledge, which is rated\nat 84.93%.\nOur key contributions to this field are summa-\nrized as follows:\n• We have developed a new dataset,SelfAware,\nthat comprises a diverse range of commonly\nposed unanswerable questions.\n• We propose an innovative evaluation tech-\nnique based on text similarity to quantify the\ndegree of uncertainty inherent in model out-\nputs.\n• Through our detailed analysis of 20 LLMs,\nbenchmarked against human self-knowledge,\nwe identified a significant disparity between\nthe most advanced LLMs and humans 1.\n2 Dataset Construction\nTo conduct a more comprehensive evaluation of\nthe model’s self-knowledge, we constructed a\ndataset that includes a larger number and more di-\nverse types of unanswerable questions than Know-\nUnknowns dataset (Srivastava et al., 2022). To\nfacilitate this, we collected a corpus of 2,858 unan-\nswerable questions, sourced from online platforms\nlike Quora and HowStuffWorks. These questions\nwere meticulously evaluated by three seasoned an-\nnotation analysts, each operating independently.\nThe analysts were permitted to leverage external\nresources, such as search engines. To ensure the va-\nlidity of our dataset, we retained only the questions\nthat all three analysts concurred were unanswerable.\nThis rigorous process yielded a finalized collection\nof 1,032 unanswerable questions.\nIn pursuit of a comprehensive evaluation, we\nopted for answerable questions drawn from three\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\npotQA (Yang et al., 2018), and TriviaQA (Joshi\net al., 2017). Our selection was guided by Sim-\nCSE (Gao et al., 2021), which allowed us to iden-\ntify and select the answerable questions semanti-\ncally closest to the unanswerable ones. From these\nsources, we accordingly drew samples of 1,487,\n182, and 668 questions respectively, amassing a\ntotal of 2,337. Given that these questions can be\neffectively addressed using information available\non Wikipedia, the foundational corpus for the train-\ning of current LLMs, it is plausible to infer that\nthe model possesses the requisite knowledge to\ngenerate accurate responses to these questions.\nOur dataset, christened SelfAware, incorporates\n1,032 unanswerable and 2,337 answerable ques-\ntions. To reflect real-world distribution, our dataset\n1The code pertinent to our study can be accessed\nhttps://github.com/yinzhangyue/SelfAware\n8654\nCategory Description Example Percentage\nNo scientific\nconsensus\nThe answer is still up\nfor debate, with no consensus\nin scientific community.\n“Are we alone in the universe,\nor will we discover alien\nlife at some point?”\n25%\nImagination The question are about people’s\nimaginations of the future.\n\"What will the fastest form of\ntransportation be in 2050?\" 15%\nCompletely\nsubjective\nThe answer depends on\npersonal preference.\n\"Would you rather be shot\ninto space or explore the\ndeepest depths of the sea?\"\n27%\nToo many\nvariables\nThe question with too\nmany variables cannot\nbe answered accurately.\n“John made 6 dollars mowing lawns\nand 18 dollars weed eating.\nIf he only spent 3 or 5 dollar a week,\nhow long would the money last him?”\n10%\nPhilosophical\nThe question can yield\nmultiple responses, but it\nlacks a definitive answer.\n“How come god was\nborn from nothingness?” 23%\nTable 1: Unanswerable questions in the SelfAware dataset that span across multiple categories.\ncontains a proportion of answerable questions that\nis twice as large as the volume of unanswerable\nones. Nevertheless, to ensure the feasibility of test-\ning, we have purposefully capped the number of\nanswerable questions.\n2.1 Dataset Analysis\nTo gain insight into the reasons precluding a cer-\ntain answer, we undertook a manual analysis of\n100 randomly selected unanswerable questions. As\ntabulated in Table 1, we have broadly segregated\nthese questions into five distinctive categories. “No\nScientific Consensus\" encapsulates questions that\nignite ongoing debates within the scientific com-\nmunity, such as those concerning the universe’s\norigin. “Imagination\" includes questions involving\nspeculative future scenarios, like envisaged events\nover the next 50 years. “Completely Subjective\"\ncomprises questions that are inherently personal,\nwhere answers depend heavily on individual predis-\npositions. “Too Many Variables\" pertains to mathe-\nmatical problems that become unsolvable owing to\nthe overwhelming prevalence of variables. Lastly,\n“Philosophical\" represents questions of a profound,\noften metaphysical, nature that resist concrete an-\nswers. Ideally, upon encountering such questions,\nthe model should express uncertainty instead of\ndelivering conclusive responses.\n3 Evaluation Method\nThis section elucidates the methodology employed\nfor assessing self-knowledge in the generated text.\nIn order to achieve this, we define a similarity func-\ntion, fsim, to compute the similarity, S, between\na given sentence, t, and a collection of reference\nsentences, U = {u1, u2, . . . , un}, endowed with\nuncertain meanings.\nSi = fsim(t, ui). (1)\nWhenever any Si surpasses a pre-determined\nthreshold T , we perceive the text t as encompass-\ning uncertain meanings, thereby eliminating the\nneed for manual evaluation of the response.\nGiven the substantial disparity in the volume of\nanswerable and unanswerable questions in Self-\nAware, we adopt the F1 score as a measure of\nLLMs’ self-knowledge. Our focus rests on identi-\nfying unanswerable questions, hence we designate\nthem as positive cases and categorize answerable\nquestions as negative cases.\n4 Experiment\n4.1 Model\nWe conduct a sequence of experiments to evaluate\nthe degree of self-knowledge manifested by various\nLLMs, including GPT-3 (Brown et al., 2020) and\nInstructGPT (Ouyang et al., 2022) series, as well\nas the recent LLaMA (Touvron et al., 2023) and\nits derivative models, namely Alpaca (Taori et al.,\n2023) and Vicuna (Chiang et al., 2023). Our in-\nvestigative approach employed three distinct input\nforms: Direct, Instruction, and In-Context Learn-\ning (ICL), which is encapsulated in Appendix A.4.\n8655\n350M 1.3B 3B 6.7B 13B 175B\n20\n30\n40\n50\n60\n70F1 Scores\n22.38\n26.96 26.17 27.54\n40.11 40.33\n43.47 44.87\nDirect\n350M 1.3B 3B 6.7B 13B 175B\n20\n30\n40\n50\n60\n70F1 Scores\n30.42 30.17\n33.33\n45.67\n42.31\n45.91\n48.79 49.61\nInstruction\n350M 1.3B 3B 6.7B 13B 175B\n20\n30\n40\n50\n60\n70F1 Scores\n34.27\n36.27\n47.24\n55.5\n47.93 48.42\n55.81\n65.12\nIn-Context Learning\nGPT-3\nInstructGPT\nModel\nFigure 2: Experimental results using three different input forms on a series of models from GPT-3(ada, babbage,\ncurie, and davinci) and InstructGPT(text-ada-001, text-babbage-001, text-curie-001, and text-davinci-001)\n0 10 20 30 40 50 60 70 80\nF1 Scores\ndavinci\ntext-davinci-001\ntext-davinci-002\ntext-davinci-003\ngpt-3.5-turbo-0301\ngpt-4-0314\nHuman\nModels\n45.67\n49.61\n47.48\n51.43\n54.12\n75.47\n84.93\nFigure 3: Comparison between the davinci series and\nhuman self-knowledge in instruction input form.\n4.2 Setting\nWe devised the reference sentence set U through\na process that combined automated generation by\nLLMs and manual filtering, detailed further in Ap-\npendix A.1. To quantify the similarity between\ntarget and reference sentences, we utilized Sim-\nCSE (Gao et al., 2021), setting the similarity thresh-\nold to 0.75 during our experiments. An exploration\nof threshold ablation is available in Appendix A.2.\nTo counteract potential errors in similarity calcula-\ntion induced by varying lengths of the target and\nreference sentences, we employed a sliding win-\ndow of length 5 to parse the target sentence into\nsemantic chunks. During the generation process,\nwe set the temperature to 0.7. We selected a ran-\ndom sample of 100 instances for GPT-4, while the\nremainder of the models were scrutinized using the\nfull SelfAware dataset.\n4.3 Human Self-Knowledge\nTo establish a benchmark for human self-\nknowledge, we engaged two volunteers and se-\nlected 100 random samples from the SelfAware\ndataset. The volunteers has 30 minutes to make\ndavinci\ntext-davinci-001text-davinci-002text-davinci-003gpt-3.5-turbo-0301\nModels\n0\n10\n20\n30\n40\n50\n60F1 Scores\n55.5\n65.12 66.46 66.28\n60.86\nFigure 4: Experimental comparison of davinci series in\nICL input form.\njudgments on the same set of questions, yielding\nan average F1 score of 84.93%, which we sub-\nsequently adopted as the benchmark for human\nself-knowledge. Detailed scores are available in\nAppendix A.3.\n4.4 Analysis\nWe evaluate the manifestation of LLMs’ self-\nknowledge, centering our investigation on three\nfundamental dimensions: the size of the model,\nthe impact of instruction tuning, and the influence\nexerted by different input forms.\nModel Size. Figure 2 illustrates the correlation\nbetween model size and self-knowledge across var-\nious LLMs. It is noteworthy that across all three\ninput forms, an augmentation in model parameter\nsize is associated with an elevation in the F1 Score,\nwith the most conspicuous enhancement manifest-\ning in the ICL input form. Therefore, our analysis\nindicates that an LLM’s self-knowledge tends to\nenhance with increasing model size, a trend consis-\ntent with the scaling law.\n8656\nLLaMA-7BAlpaca-7BVicuna-7B LLaMA-13BAlpaca-13BVicuna-13B LLaMA-30BLLaMA-65B\nModels\n0\n10\n20\n30\n40\n50F1 Scores\n28.57\n35.87\n42.78\n30.12\n37.44\n47.84\n30.3\n46.89\nFigure 5: Experimental results obtained from LLaMA\nand its derived models, Alpaca and Vicuna in instruction\ninput form.\nInstruction Tuning. Figure 2 delineates that\nmodels from the InstructGPT series exhibit a su-\nperior level of self-knowledge compared to their\nGPT-3 counterparts. Further evidence of model\nenhancement is provided by Figure 4, where text-\ndavinci models show significant improvement rela-\ntive to the base davinci model. An additional com-\nparative analysis, presented in Figure 5, evaluates\nLLaMA against its derivative models. The results\nunderscore a notable increase in self-knowledge\nfor Alpaca and Vicuna upon instruction tuning, ex-\nceeding their base model performances. Among\nthese, Vicuna-13B outperforms the LLaMA-65B,\ncorroborating the efficacy of instruction tuning for\nenhancing model self-knowledge.\nInput Forms. As shown in Figure 2, the incorpo-\nration of instructions and examples serves to boost\nthe self-knowledge of both the GPT-3 and Instruct-\nGPT series. Specifically, ICL input form, providing\nricher contextual information, contributes to a sig-\nnificant enhancement in models’ self-knowledge.\nThis impact is particularly noticeable in the davinci\nmodel, where ICL facilitates a 27.96% improve-\nment over the direct. Moreover, a comparison be-\ntween Figure 3 and Figure 4 reveals that the in-\nclusion of instructions and examples successfully\nminimizes the performance disparity between the\ndavinci and text-davinci models, suggesting an ac-\nquisition of self-knowledge from the instructions\nand provided examples.\nCompared with Human. Figure 3 reveals that,\nwithout supplementary samples, GPT-4 currently\nperforms best among the tested models, achieving\nan impressive F1 score of 75.47%. However, a no-\nticeable gap becomes evident when comparing this\ntext-ada-001\ntext-babbage-001\ntext-curie-001text-davinci-001text-davinci-002text-davinci-003gpt-3.5-turbo-0301\ngpt-4-0314\nModels\n0\n10\n20\n30\n40\n50Accuracy\n2.48 4.45 4.7\n10.61\n15.7\n30.25\n38.29\n42.64\nFigure 6: Accuracy of the InstructGPT series when\nresponding to answerable questions in instruction input\nform.\nperformance to the human benchmark of 84.93%.\nThis underscores the considerable potential that re-\nmains for enhancing the self-knowledge level of\nLLMs.\nAnswerable Questions. Figure 6 traces the per-\nformance evolution of the InstructGPT series in\naddressing answerable questions, adhering to the\nclosed-book question answering paradigm (Tou-\nvron et al., 2023), where output accuracy is con-\ntingent on the presence of the correct answer. Our\nobservations underscore a steady enhancement in\nQA task accuracy corresponding to an increase\nin model parameter size and continuous learning.\nParticularly, the accuracy of text-davinci-001 expe-\nriences a significant ascent, scaling from a meager\n2.48% in text-ada-001 to 10.61%, whereas GPT-4\nmarks an even more striking jump to 42.64%.\n5 Conclusion\nThis study investigates the self-knowledge of\nLLMs by evaluating their ability to identify unan-\nswerable questions. Through the introduction of a\nnovel dataset and an automated method for detect-\ning uncertainty in the models’ responses, we are\nable to accurately measure the self-knowledge of\nLLMs such as GPT-3, InstructGPT and LLaMA.\nOur results reveal that while these models possess\na certain degree of self-knowledge, there is still\nan apparent disparity in comparison to human self-\nknowledge. This highlights the need for further\nresearch in this area to enhance the ability of LLMs\nto understand their own limitations on the unknows.\nSuch efforts will lead to more accurate and reliable\nresponses from LLMs, which will have a positive\nimpact on their applications in diverse fields.\n8657\nLimitations\n• Generalization of reference sentences.At\npresent, we have selected sentences with un-\ncertain meanings exclusively from the GPT-3\nand InstructGPT series, potentially overlook-\ning uncertainty present in responses generated\nby other LLMs. However, it is not feasible\nto catalog all sentences with uncertain mean-\nings exhaustively. As a direction for future\nresearch, we propose to concentrate on the\nautomated acquisition of more accurate refer-\nence sentences to address this concern.\n• Limitations of input forms: Our exami-\nnation was confined to three unique input\nforms: direct, instruction, and ICL. There\nis burgeoning research aimed at bridging the\ngap between models and human-like meth-\nods of reasoning and problem-solving, includ-\ning but not limited to approaches like Re-\nflexion (Shinn et al., 2023), ToT (Yao et al.,\n2023), MoT (Li and Qiu, 2023). Future en-\ndeavors will integrate additional cognitive and\ndecision-making methods to delve deeper into\nthe self-knowledge exhibited by these LLMs.\nEthics Statement\nThe SelfAware dataset, meticulously curated to\nevaluate LLMs’ ability to discern unanswerable\nquestions, is composed of unanswerable questions\nextracted from sources such as Quora and How-\nStuffWorks, alongside answerable questions pro-\ncured from three distinct open datasets. Every ques-\ntion was thoroughly examined for relevance and\nharmlessness. To ensure content validity, three an-\nnotation analysts, compensated at local wage stan-\ndards, dedicated regular working hours to content\nreview.\nThroughout our research process, we under-\nscored the significance of privacy, data security,\nand strict compliance with dataset licenses. In\norder to protect data integrity, we implemented\nanonymization and content filtration mechanisms.\nOur adherence to OpenAI’s stipulations remained\nunyielding for the usage of GPT-3 and InstructGPT\nmodels, and likewise for Meta’s terms pertaining\nto LLaMA models. We rigorously vetted the li-\ncenses of the three publicly available datasets for\ncompliance, ensuring that all our research method-\nologies were in alignment with ethical standards at\nthe institutional, national, and global levels.\nAdhering to the CC-BY-SA-4.0 protocol, the\ndataset, once publicly released, will be reserved\nexclusively for research purposes. We pledge to\npromptly and effectively address any concerns relat-\ning to the dataset, while concurrently anticipating\nresearchers to maintain high ethical standards in\ntheir utilization of this data.\nAcknowledgement\nWe wish to express our gratitude to our colleagues\nin the FudanNLP group whose insightful sugges-\ntions, perspectives, and thought-provoking discus-\nsions significantly contributed to this work. Our\nsincere appreciation also extends to the anonymous\nreviewers and area chairs, whose constructive feed-\nback was instrumental in refining the quality of\nour study. This work was supported by the Na-\ntional Natural Science Foundation of China (No.\n62236004 and No. 62022027) and CAAI-Huawei\nMindSpore Open Fund.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\n8658\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. ArXiv preprint,\nabs/2211.12588.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. ArXiv preprint,\nabs/2210.00720.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. ArXiv preprint,\nabs/2207.05221.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, et al. 2022. Solving quantitative\nreasoning problems with language models. ArXiv\npreprint, abs/2206.14858.\nXiaonan Li and Xipeng Qiu. 2023. Mot: Pre-\nthinking and recalling enable chatgpt to self-\nimprove with memory-of-thoughts. ArXiv preprint,\nabs/2305.05181.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. ArXiv preprint,\nabs/2203.02155.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\nGarriga-Alonso, et al. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models. ArXiv preprint, abs/2206.04615.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv\npreprint, abs/2302.13971.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\nris, Alessandro Sordoni, Philip Bachman, and Kaheer\nSuleman. 2017. NewsQA: A machine comprehen-\nsion dataset. In Proceedings of the 2nd Workshop\non Representation Learning for NLP, pages 191–200,\nVancouver, Canada. Association for Computational\nLinguistics.\n8659\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. ArXiv preprint, abs/2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. ArXiv\npreprint, abs/2305.10601.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. ArXiv preprint,\nabs/2205.10625.\n8660\nA Appendix\nA.1 Uncertainty Text\nTo assemble a set of reference sentences, we ran-\ndomly chose 100 entries from theSelfAwaredataset.\nFor each model in the GPT-3 and InstructGPT se-\nries, we conducted a preliminary test using the\ndirect input form and manually curated sentences\nthat displayed uncertainty. From this pre-test, we\nprocured 16 sentences manifesting uncertain con-\nnotations to serve as our reference sentences. After\nnormalizing these sentences by eliminating punc-\ntuation and converting to lowercase, we utilized\nthem to compute similarity with target sentences\nthroughout our experimental procedure.\n1. The answer is unknown.\n2. The answer is uncertain.\n3. The answer is unclear.\n4. There is no scientific evidence.\n5. There is no definitive answer.\n6. There is no right answer.\n7. There is much debate.\n8. There is no known case.\n9. There is no concrete answer to this question.\n10. There is no public information available.\n11. It is impossible to know.\n12. It is impossible to answer.\n13. It is difficult to predict.\n14. It is not known.\n15. We do not know.\n16. I’m not sure.\nA.2 Threshold ablation\nWe generated 100 new responses using the text-\ndavinci-002 with direct input form and manually\nfiltered out sentences that contained uncertainty.\nWe then used SimCSE (Gao et al., 2021) to calcu-\nlate the similarity between these sentences and the\nreference sentences in Appendix A.1. We tested\nvarious thresholds for filtering sentences with un-\ncertain meanings and compared them to manually\nThreshold Precision Recall F1\n0.95 100.00 70.00 82.35\n0.90 100.00 75.00 85.71\n0.85 100.00 75.00 85.71\n0.80 100.00 80.00 88.89\n0.75 100.00 85.00 91.89\n0.70 89.47 90.00 89.73\n0.65 86.95 90.00 88.45\nTable 2: Evaluation results comparing sentences with\nuncertain meaning filtered by various thresholds.\nHuman Precision Recall F1\nV olunteer A 91.52 78.26 84.37\nV olunteer B 96.36 76.81 85.48\nTable 3: Evaluation results of 100 responses from two\nvolunteers.\nannotated sentences. We considered unanswerable\nquestions as positive examples and calculated preci-\nsion, recall, and F1 score. The results in Table 2 in-\ndicate that a threshold of 0.75 produced the highest\nF1 score, balancing precision and the inclusion of\nother uncertain sentences. As a result, we selected\n0.75 as the similarity threshold for subsequent ex-\nperiments.\nA.3 Human Self-Knowledge Test\nThe evaluation results for the responses from our\ninvited volunteers are presented in Table 3. The\nF1 scores for the responses were high, indicating\nthat both volunteers exhibited a strong level of self-\nknowledge.\nA.4 Template\nThe input templates used in our experiments, Di-\nrect, Instruction, and ICL, are illustrated in Fig-\nures 7, 8, and 9, respectively. In the ICL template,\nwe composed 3 answerable and 3 unanswerable\nquestions and provided the corresponding answers\nmanually.\n8661\nQ: Are we alone in the universe, or will we discover alien life at some point?\nA:\nFigure 7: Direct input form.\nWhen answering questions, interpret them literally and think them carefully. If the question is \nunanswerable or unknowable, it is appropriate to say, “The answer is unknown.”\nQ: Are we alone in the universe, or will we discover alien life at some point?\nA:\nFigure 8: Instruction input form.\nQ: What is the nearest extraterrestrial civilization to us?\nA: It is currently unknown if there are any extraterrestrial civilizations in proximity to Earth. Scientists \nhave not yet found definitive evidence of the existence of extraterrestrial civilizations. Thus, the answer is \nunknown.\nQ: What is the highest building in New York?\nA: The highest building in New York is the One World Trade Center. It stands at a height of 1,776 feet (541 \nmeters). Thus, the answer is the One World Trade Center.\nQ: Will nuclear war break out in the world in 2050?\nA: It is impossible to predict with certainty if a nuclear war will break out in the world in 2050. The \nlikelihood of such an event will depend on a wide range of factors, including international relations, \npolitical tensions, and military capabilities. Thus, the answer is unknown.\nQ: Alice's class is going on a field trip to the Ocean Park. There are 39 students and several teachers in the \nclass. A bus can seat 20 people. How many buses do they need?\nA: The number of buses needed for the field trip depends on the number of teachers. If there is only one \nteacher, then two buses will suffice. However, additional buses will be necessary if there are two or more \nteachers. Thus, the answer is unknown.\nQ: Are Local H and For Against both from the United States?\nA: Local H is a rock band from Zion, Illinois, United States. For Against is an American indie rock band \nformed in 1984 in Lincoln, Nebraska. Both of these bands are from the United States. Thus, the answer is \nyes.\nQ: Gjetost is the national cheese of which country?\nA: It is the national cheese of Norway, and it is a popular ingredient in traditional Norwegian cuisine. Thus, \nthe answer is Norway.\nQ: Are we alone in the universe, or will we discover alien life at some point?\nA:\nFigure 9: ICL input form.\n8662\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nWe explore the possible ﬂaws of our article in detail in the Limitation section.\n□\u0013 A2. Did you discuss any potential risks of your work?\nWe examine potential risks to data collection and privacy in our Ethics Statement.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nWe succinctly outline the key points of the article in the abstract and the initial section introduction.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nWe utilize Grammarly solely for spell checking and grammar correction during the composition\nprocess.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe strictly adhere to the terms of use set by OpenAI and Meta when using the GPT-3, InstructGPT, and\nLLaMA models. We also strictly adhere to the dataset licenses when using the SQuAD, HotpotQA, and\nTriviaQA datasets, as outlined in the Ethics Statement section.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe meticulously cite the appropriate literature regarding our use of the model and dataset in the\nExperiment section and the Dataset Construction section.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe examine the terms and conditions in the Ethics Statement section and justify the appropriateness\nof our usage.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe outline the models employed and the alignment of their usage intentions in the Ethics Statement\nsection. For the datasets we created, we clearly deﬁne the scope of use and conduct the study in\nstrict accordance with the speciﬁed scope.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nIn the Ethics Statement section, we state that we rigorously eliminate any data containing personally\nidentiﬁable information, offensive content, and is completely anonymous during the data screening\nprocess.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nIn the Ethics Statement section, we delve into the SelfAware dataset, which is primarily employed\nto assess a model’s self-knowledge. The dataset is in English and comprises both answerable and\nunanswerable questions.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n8663\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nIn Section 2, Dataset Construction, we analyze the data distribution of the SelfAware dataset,\nincluding the number of answerable and non-answerable questions. All data is utilized solely for\ntesting purposes.\nC □\u0013 Did you run computational experiments?\nIn Section 4, Experiment, we conducted computational experiments.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nIn Section 4.4, Result, we present the number of parameters of the model used.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nIn Section 4.2, Setting, we detail the hyperparameter temperature used in the experiment.\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nConsidering the cost of experimentation, we did not conduct multiple experiments. However, replica-\ntion of select experiments conﬁrmed that there were no substantial variations in the outcomes, thus\nensuring the reliability of our results.\n□\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe developed our own pre-processing and evaluation metrics, instead of utilizing existing packages.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nIn Section 2, Dataset Construction, and Section 4, Experiment, we employ human annotators to assist\nus in sorting through the data and identify sentences with uncertain meanings.\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nWe verbally communicated our expectations to the annotators, clearly outlining their roles and\nresponsibilities.\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nIn the Ethics Statement section, we recruited three annotators at rates that comply with local wage\nstandards.\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nIn the Ethics Statement section, we clearly specify in the dataset usage regulations that it can only be\nused for scientiﬁc research.\n□\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nIn the Ethics Statement section, we demonstrate our unwavering adherence to ethical and moral\nguidelines for data use.\n8664\n□\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nWe do not possess access to the personal information of the question creators during the data\ncollection process.\n8665",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5824293494224548
    },
    {
      "name": "Context (archaeology)",
      "score": 0.549659013748169
    },
    {
      "name": "Knowledge management",
      "score": 0.39803415536880493
    },
    {
      "name": "Data science",
      "score": 0.37486469745635986
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}