{
  "title": "Detecting AI-Generated Code Assignments Using Perplexity of Large Language Models",
  "url": "https://openalex.org/W4393157398",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2030984501",
      "name": "Xu, Zhenyu",
      "affiliations": [
        "Texas Tech University"
      ]
    },
    {
      "id": "https://openalex.org/A4224559693",
      "name": "Sheng, Victor S.",
      "affiliations": [
        "Texas Tech University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3101891351",
    "https://openalex.org/W6809838524",
    "https://openalex.org/W6605862995",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4361866100",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W3165784750",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Large language models like ChatGPT can generate human-like code, posing challenges for programming education as students may be tempted to misuse them on assignments. However, there are currently no robust detectors designed specifically to identify AI-generated code. This is an issue that needs to be addressed to maintain academic integrity while allowing proper utilization of language models. Previous work has explored different approaches to detect AI-generated text, including watermarks, feature analysis, and fine-tuning language models. In this paper, we address the challenge of determining whether a student's code assignment was generated by a language model. First, our proposed method identifies AI-generated code by leveraging targeted masking perturbation paired with comperhesive scoring. Rather than applying a random mask, areas of the code with higher perplexity are more intensely masked. Second, we utilize a fine-tuned CodeBERT to fill in the masked portions, producing subtle modified samples. Then, we integrate the overall perplexity, variation of code line perplexity, and burstiness into a unified score. In this scoring scheme, a higher rank for the original code suggests it's more likely to be AI-generated. This approach stems from the observation that AI-generated codes typically have lower perplexity. Therefore, perturbations often exert minimal influence on them. Conversely, sections of human-composed codes that the model struggles to understand can see their perplexity reduced by such perturbations. Our method outperforms current open-source and commercial text detectors. Specifically, it improves detection of code submissions generated by OpenAI's text-davinci-003, raising average AUC from 0.56 (GPTZero baseline) to 0.87 for our detector.",
  "full_text": "Detecting AI-Generated Code Assignments Using Perplexity of Large Language\nModels\nZhenyu Xu, Victor S. Sheng\nDepartment of Computer Science, Texas Tech University\nzhenxu@ttu.edu, victor.sheng@ttu.edu\nAbstract\nLarge language models like ChatGPT can generate human-\nlike code, posing challenges for programming education as\nstudents may be tempted to misuse them on assignments.\nHowever, there are currently no robust detectors designed\nspecifically to identify AI-generated code. This is an issue\nthat needs to be addressed to maintain academic integrity\nwhile allowing proper utilization of language models. Pre-\nvious work has explored different approaches to detect AI-\ngenerated text, including watermarks, feature analysis, and\nfine-tuning language models. In this paper, we address the\nchallenge of determining whether a student’s code assign-\nment was generated by a language model. First, our pro-\nposed method identifies AI-generated code by leveraging tar-\ngeted masking perturbation paired with comprehensive scor-\ning. Rather than applying a random mask, areas of the code\nwith higher perplexity are more intensely masked. Second,\nwe utilize a fine-tuned CodeBERT to fill in the masked por-\ntions, producing subtle modified samples. Then, we integrate\nthe overall perplexity, variation of code line perplexity, and\nburstiness into a unified score. In this scoring scheme, a\nhigher rank for the original code suggests it’s more likely\nto be AI-generated. This approach stems from the observa-\ntion that AI-generated codes typically have lower perplex-\nity. Therefore, perturbations often exert minimal influence\non them. Conversely, sections of human-composed codes\nthat the model struggles to understand can see their perplex-\nity reduced by such perturbations. Our method outperforms\ncurrent open-source and commercial text detectors. Specifi-\ncally, it improves detection of code submissions generated by\nOpenAI’s text-davinci-003, raising average AUC from 0.56\n(GPTZero baseline) to 0.87 for our detector.\nIntroduction\nThe emergence of large language models has profoundly im-\npacted software development, providing invaluable tools for\nprogrammers. These models possess the capability to au-\ntonomously generate code, which, while beneficial in many\nrespects, poses significant challenges to programming edu-\ncation. There’s a growing concern that students may exploit\nthese models by merely providing a problem description and\ntest cases. As a result, they could potentially complete code\nassignments without ever typing a single line of code them-\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nselves. Given the innate proficiency of these language mod-\nels, they can effortlessly produce multiple, unique solutions\nto students’ basic assignments within mere seconds. More-\nover, the generated code often includes annotations and is\nconstructed with the expertise reminiscent of seasoned pro-\ngrammers. Such accessibility allows students to merely copy\nand paste the code, followed by a direct submission. This\nease of misuse gravely threatens the integrity and objectives\nof programming education.\nDespite the rising challenges, most current detectors fo-\ncus on pinpointing AI-generated text, leaving a gap in tools\nspecifically designed for AI-generated code. In this paper,\nwe bridge this gap by introducing a dedicated AI code de-\ntector. We then rigorously test its efficiency, comparing it\nagainst renowned open-source and commercial alternatives\nwithin a coding context.\nIn the prior work DetectGPT (Mitchell et al. 2023), it\nshows a very inspiring hypothesis and perturbation method\non the textual context. DetectGPT posits that subtle modi-\nfications to AI-generated text generally result in lower log\nprobability under the model than the original text. Con-\nversely, slight alterations to human-written text can yield ei-\nther higher or lower log probabilities than the original. This\nsuggests that each token produced by the model often resides\nat the pinnacle of the log probability function curve.\nThe concept of code naturalness implies that a similar be-\nhavior should manifest within code contexts. Building upon\nthese foundational insights, we refine and validate this per-\nspective for code environments. Our hypothesis emphasizes\nthat minor perturbations can distinctly differentiate between\nAI-generated and human-authored code. The core of our ex-\nperiments revolves around this principle: slight modifica-\ntions to AI-generated code are more likely to increase its\nperplexity under the model compared to the original code.\nOn the other hand, alterations to human-written code can\neither raise or lower its perplexity relative to the original.\nFurthermore, we propose another hypothesis: Code gener-\nated by LLMs exhibits more consistent line-by-line perplex-\nity compared to human-authored code. Our findings affirm\nthis perspective.\nWe harness these two observations from the code context\nto construct our AIGCode detector. Our strategy seeks to\ndiscern whether a given piece of code is AI-generated by\nemploying a two-fold approach: perturbation and scoring.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23155\n• Perturbation Mechanism: This involves using a mask\nmodeling task to subtly modify segments of code that ex-\nhibit higher perplexity (PPL).\n• Scoring Mechanism: The scoring is influenced by three\nmetrics: perplexity of the code, standard deviation of per-\nplexity across individual lines, and the code’s burstiness.\nThe foundational premise of our methodology is that AI-\ngenerated code inherently possesses a low score, gauged by\nthe combined metrics of overall perplexity, standard devia-\ntion of perplexity across code lines, and burstiness. Conse-\nquently, post-perturbation, it becomes challenging to gener-\nate samples that score lower than the original AI-generated\ncode. In contrast, human-authored code might be perceived\nas more ambiguous by the model, so minor tweaks might\nyield an ”optimized” version that bears a score lower than\nthe original code.\nWe articulate our contributions in this paper as follows:\n• We validate the hypothesis, previously observed in\nmodel-generated text, within the code context: pertur-\nbations tend to elevate the model’s perplexity for AI-\ngenerated code but can diminish it for human-written\ncode.\n• Building on the foundation of DetectGPT, we have ad-\ndressed the gap in AI-generated code detection by en-\nhancing current zero-shot detection method and devel-\noping a specialized detector for AI-generated code. Em-\npirical results show that our improved detector surpasses\nboth leading open-source and commercial alternatives in\nthe code domain.\n• We delve into the robustness of AI-generated code and\nexplore various adversarial tactics, including refined\nprompting, temperature adjustments, rewrites, and code\nblending.\nBackground\nIn this section, we delve into the domain of Pre-trained\nLarge Language Models specifically tailored for coding lan-\nguages. We introduce metrics to evaluate the proximity of\ncode to model-generated content.\nPre-trained Large Language Models on Codes\nPre-trained language models are transforming natural lan-\nguage processing (NLP) with their strong performance on\ntasks like translation and text summarizing. This has led\nto interest in applying these models to programming lan-\nguages. By training on source code and documentation,\nthen fine-tuning for specific programming tasks, these mod-\nels can assist with code completion, bug finding, and code\ngeneration. They leverage their pre-existing knowledge to\nunderstand and generate code. Notable research projects\ndemonstrate the potential of pre-trained models for program-\nming. Models like CodeBERT (Feng et al. 2020), CodeT5\n(Wang et al. 2021), PolyCoder (Xu et al. 2022), and PaLM-\nCoder (Chowdhery et al. 2022) show early success in adapt-\ning large language models for code. By pre-training on code\nand documentation, they learn about programming. Fine-\ntuning then enables applications like automated code com-\npletion and error correction. Though still an emerging field,\nthese models represent progress toward using the knowledge\nwithin pre-trained language models to comprehend, gener-\nate, and reason about code.\nIn our approach, we utilize Large Language Models on\nCodes (LLMCs) during the perturbation and scoring phase,\nspecifically for tasks such as tokenizing, perplexity calculat-\ning, and mask-filling.\nPerplexity and Burstiness\nPerplexity and burstiness are two key metrics for assessing\nthe quality of code generated by large language models for\ncode (LLMCs).\nPerplexity Perplexity is a measure of how well a probabil-\nity model predicts a sample. In the context of code, it’s used\nto evaluate the predictability of the next token or line of code\nbased on a given context. A lower perplexity indicates that\nthe model’s predictions are generally more accurate, while a\nhigher perplexity suggests the model finds the content more\nunpredictable.\nBurstiness Burstiness in the context of code refers to the\nclustering or frequent appearance of certain patterns, identi-\nfiers, or constructs in a specific section of the code. For ex-\nample, in a code module dealing with database operations,\nthere might be a burst of commands and identifiers related\nto database queries. Analyzing burstiness in code can help\nin understanding patterns, detecting anomalies, or assessing\ncode quality.\nApproach\nProblem Description\nWe are addressing the problem of detecting AI-generated\ncode submissions and preventing the misuse of large lan-\nguage models in education. Our method mainly comprises\nthree processes: perturbation, scoring, and prediction. Un-\nlike the random perturbation and score ranking of Detect-\nGPT, we optimize the masking process, and fine-tune mask-\nfilling models on different code languages. Moreover, we\nalso propose a new scoring algorithm for the ranking of orig-\ninal code and perturbed codes. Algorithm 1 provides a de-\ntailed description of our evaluation strategy and detection\nprocess.\nPerturbation\nAs shown in Figure 1, the perturbation process includes\nmasking and mask-filling, which produces several samples\nwith minor modifications. The degree of modification de-\npends on our mask percentage.\nMasking In contrast to the indiscriminate masking tech-\nnique applied by DetectGPT to textual data, our strategy\nadopts a more nuanced approach. Initially, we compute the\nPerplexity (PPL) for each line of code. Based on these cal-\nculations, a weight for masking is assigned, contingent on\nthe PPL value of each line. Subsequently, random masking\nis applied. Hence, sections of code with elevated PPL values\nare subjected to more extensive masking, leading to more\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23156\nFigure 1: Illustration of the perturbation process. Weights are assigned to code segments based on line-level perplexity, higher\nweight means more allocated masks, followed by mask-filling task for slight modifications.\nprofound modifications. This approach facilitates the gen-\neration of samples that align more harmoniously with the\nmodel’s comprehension, thereby optimizing the number of\nrequired samples. It’s pertinent to note that we abstain from\ndirectly masking tokens that exhibit low log probability. This\ndecision stems from the observation that models frequently\nassign low probabilities to specialized tokens, such as ’#’ or\nindentations integral to code formatting, which, in essence,\nare not informative for our purpose. Additionally, our ap-\nproach offers a distinct advantage over DetectGPT’s random\nmasking strategy: it enhances the likelihood of generating\nsamples with reduced PPL values, implying that fewer per-\nturbed samples are requisite.\nMask-Filling We have fine-tuned the CodeBERT model\nusing code corpora from six distinct programming lan-\nguages. Then CodeBERT is employed to fill the masks intro-\nAlgorithm 1: AI-Generated Code Detection\n1: procedure DETECT (C)\n2: for each line Li in C do // For every line in the code\n3: PPL(Li) ← e− 1\nN\nPN\nj=1 log p(wj|w1,w2,...,wj−1) //\nCompute PPL for the line\n4: end for\n5: W(Li) ← Function of PPL(Li) // Assign mask\nweights based on PPL\n6: M ← Mask(C, W) // Mask the code based on\nweights\n7: C′ ← FillMask(M) // Fill the masked parts\n8: S ← α ×PPL + β ×Std(PPL) +γ ×B // Compute\nthe score\n9: R ← Rank(S, C′) // Rank the score in comparison\nwith perturbed codes\n10: P ← count(S>S(C))\ncount(C′) // Calculate prediction probabil-\nity\n11: if P >0.97 then // Threshold check\n12: return 1 // AI generated\n13: else\n14: return 0 // Human generated\n15: end if\n16: end procedure\nduced in the input code. Instead of relying solely on the top-\nscoring candidate we utilize Nucleus Sampling. This method\nensures a broader spectrum of token candidates, thereby\ngenerating more varied samples.\nScoring and Prediction\nWe employ a composite strategy to evaluate code submis-\nsions, factoring in three distinct metrics: code perplexity\n(PPL), standard deviation of PPL across code lines, and code\nburstiness. Each metric is assigned a specific weight to com-\npute an overall score. We juxtapose the scores of the original\ncodes with their perturbed counterparts. A higher ranking\nfor the original code often indicates a higher likelihood of\nit being AI-generated. The rationale behind this is that AI-\ngenerated codes inherently occupy positions at the bottom\nof the perplexity curve. During the perturbation process, it\nis challenging to produce a sample more preferable to the\nmodel than the AI-generated one, given its grammatical ac-\ncuracy, coherence, and lower burstiness. Hence, perturbed\nversions rarely exhibit a lower score than the AI-generated\nsamples. Conversely, human-written code might have di-\nverse elements that the model finds perplexing. Even minor\nmodifications can lead to ’optimized’ samples with a score\nlower than their original versions. This process is shown in\nFigure 2.\nExperiments Design\nDataset Preparation\nIn this section, we will introduce our data collection process\nand dataset we created.\nCodeNet Dataset The CodeNet dataset (Puri et al. 2021)\nby IBM contains 14 million code samples from 4000 coding\nproblems and covers over 50 programming languages, in-\ncluding C++, Python, and Java. Each sample provides infor-\nmation on problem description, size, memory use, and CPU\nrun time. It also includes human-written solutions to these\nproblems, showcasing different coding strategies. Building\non CodeNet, we refined the data and collect AI-generated\ncodes for each programming problems.\nAIGCode Dataset The AIGCode dataset is derived from\nIBM’s CodeNet dataset and is specifically designed to re-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23157\nFigure 2: Illustration of the scoring and prediction procedure. The scoring considers overall perplexity, standard deviation of\nperplexity across code lines, and burstiness. A lower score suggests a higher likelihood of the code being generated by a model.\nflect the nature of student submissions in educational pro-\ngramming tasks. We concentrated our efforts on six pro-\ngramming languages that are frequently used in academic\nenvironments: C, C++, C#, Java, JavaScript, and Python. To\nensure the quality and uniformity of our dataset, we applied\na data cleanup process. We filtered codes based on specific\ncriteria: they needed to have a line length between 10 and\n100, an alphanumeric character fraction exceeding 0.25, and\nwe eliminated all comments and duplicate files. After this\nrigorous cleanup, we utilized 80% of the resultant CodeNet\ndata to fine-tune CodeBERT. Simultaneously, 10% was re-\nserved for validation purposes, and the remaining 10%, en-\ncompassing roughly 400 programming problems, was des-\nignated a segment of our test set.\nBuilding on these 400 programming challenges, we em-\nployed the OpenAI’s text-davinci-003 (OpenAI 2022) model\nfor two main tasks: text-to-code generation and code transla-\ntion. The former involves generating code from problem de-\nscriptions, while the latter translates code from one program-\nming language to another. All AI-generated codes that suc-\ncessfully passed at least one test case were integrated into the\ntest set. This resulting dataset comprises 5,214 AI-generated\ncodes. To maintain balance, an equal number of human-\nwritten codes were selected at random from CodeNet, en-\nsuring they matched in terms of language and programming\nproblem.\nDetector and Baseline\nIn this section, we introduce a range of AI-generated content\ndetectors, from open-source to commercial solutions, that\nserve as baselines. Each detector possesses its own distinc-\ntive strategies and mechanisms. These detectors are com-\npared with our AIGCode Detector on the AI-generated code\ndataset to benchmark performance.\nGPT2-Detector Leveraging the RoBERTa (Liu et al.\n2019) architecture, the GPT2-Detector (Solaiman et al.\n2019) is fine-tuned specifically to identify outputs from the\n1.5B-parameter GPT-2 model. This detector is trained on\noutputs generated using a combination of temperature and\nnucleus sampling. This training methodology ensures it gen-\neralizes well across outputs produced by various sampling\nmethods.\nDetectGPT DetectGPT (Mitchell et al. 2023) introduces\na novel approach to discern machine-generated text through\nthe analysis of probability curvature. Remarkably, it by-\npasses the need for dedicated classifiers or assembling\ndatasets of real or generated texts. Using out-of-the-box\nmask-filling models, like T5 (Raffel et al. 2020) and mT5\n(Xue et al. 2020), it gauges the curvature of a model’s out-\nput probability distribution. In practice, DetectGPT exhibits\nsuperior discriminative capabilities over other zero-shot de-\ntection methods.\nRoBERTa-QA Derived from the RoBERTa language\nmodel, RoBERTa-QA (Guo et al. 2023) is designed for text\nclassification, particularly in question-answering contexts.\nBy accepting paired inputs, namely a question and its corre-\nsponding answer, the model can discern AI-generated con-\ntent with higher accuracy. A distinctive token is employed to\nmerge the question and its answer, optimizing the classifica-\ntion process.\nGPTZero Renowned as a premier AI detector, GPTZero\n(Tian and Cui 2023) specializes in determining if content, be\nit a sentence, paragraph, or entire document, originates from\na large language model, for instance, ChatGPT. The detec-\ntion strategy relies on metrics like perplexity and burstiness.\nHaving been trained on a vast corpus encompassing both\nhuman and AI-generated English texts, GPTZero boasts of\nserving millions of users worldwide, spanning various sec-\ntors such as education, publishing, and legal.\nThe Writer AI Detector This freely accessible tool aims\nto discern AI-generated content by identifying specific tex-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23158\ntual patterns that AI models frequently produce (Writer.com\n2023). The detection criterion encompasses various aspects,\nfrom recurrent phrasal patterns and sentence structures to the\noverarching tone of the content.\nResearch Questions\nRQ1: How Does AIGCode Detector Compare With Cur-\nrent Open-Source and Commercial Text Detectors? To\nensure that the AIGCode Detector stands up to the de-\nmands of current technology, we benchmark its performance\nagainst a selection of five renowned open-source and com-\nmercial text detectors.\nRQ2: How Does the Perplexity Calculation Influence the\nPerformance of AIGCode Detector? The perplexity cal-\nculation model serves as the foundation for our AIGCode\nDetector and can significantly impact its efficacy.\nRQ3: How Robust Is the AIGCode Detector Against\nAttacks? In real-world applications, codes generated by\nLarge Language Models (LLMs) often undergo modifica-\ntions before submission. Users might apply regular rewrites\nto AI-generated codes or adjust certain hyper-parameters of\nLLMs to bypass detection. We designed a variety of muta-\ntion methods to try to bypass the detector.\nMetrics\nTo comprehensively assess the effectiveness, we used fol-\nlowing metrics:\nAUC. The AUC (Area Under the Receiver Operating\nCharacteristic Curve) score of a detector measures its abil-\nity to differentiate between AI-generated text (positive class)\nand human-written text (negative class). A score closer to\n1.0 signifies effective differentiation, while a score around\n0.5 indicates the detector’s performance is equivalent to ran-\ndom chance.\nFPR. FPR (False Positive Rate) measures the proportion\nof human-written code that is incorrectly identified as AI-\ngenerated by the detector.\nFNR. FNR (False Negative Rate) quantifies the propor-\ntion of AI-generated code that the detector mistakenly clas-\nsifies as human-written.\nBypass Rate. For the robustness test, we assess the abil-\nity of our detector to identify mutated AI-generated codes\nby BypassRate = Nbypassed\nNtotal\n. Here, the numerator denotes\nthe count of AI-generated codes that, post-mutating, eluded\nthe detector’s scrutiny, while the denominator signifies the\noverall count of AI-generated codes subjected to mutating\nwith the aim of circumventing the detector.\nHyper-Parameters\nFor fine-tuning CodeBERT, we employed 5.8G of code\nsourced from CodeNet, spanning 6 distinct programming\nlanguages. The training process lasted for 500,000 steps, us-\ning a batch size of 32, and was executed on two GTX 4090\nGPUs. From this process, we obtained 6 distinct CodeBERT-\ntuned models, each tailored for a specific programming lan-\nguage. We conduct mask-filling task using CodeBERT with\na masking percentage of 5%. For calculating perplexity, we\nmainly leveraged the OpenAI’s text-davinci-003 model. In\ncontrast to the DetectGPT method which required 500 sam-\nples, our approach significantly reduced the sample require-\nment to just 50. This efficiency is attributed to our strategic\nmask selection mechanism. For AI-generated codes using\ntext-davinci-003, the temperature was set to 0.6, and top\np\nto 0.1. Higher temperature and top p values increased the er-\nror rate, while lower values tended to produce more similar\nanswers.\nMain Results\nIn this section, we will present experiment results along with\nresearch questions in an attempt to answer each question.\nRQ1: Performance of AIGCode Detector Against\nExisting Detectors\nTo evaluate the efficacy of the AIGCode Detector, we bench-\nmark its performance against five prominent open-source\nand commercial text detectors. Our comparisons are con-\nducted on the AIGCode dataset. Within the AIGCode De-\ntector framework, we employ CodeBERT as the mask-filling\nmodel and text-davinci-003 as the primary scoring model.\nFor those detectors giving probability, we choose its best\nperformance threshold. For detectors having requirement of\ninput length, we truncate and use the prior tokens as input.\nResults is shown in Table 1. AIGCode Detector has rela-\ntively high AUC on all programming languages, as well as\nlow FPR and FNR.\nRQ2: Influence of Models Used to Calculate\nPerplexity\nTo investigate the role of perplexity calculation models in\nthe effectiveness of the AIGCode Detector, we utilized a va-\nriety of Large Language Models including GPT2-xl (Rad-\nford et al. 2019), GPT-J (Wang and Komatsuzaki 2021),\nGPT-NeoX (Black et al. 2022), and text-davinci-003 (Ope-\nnAI 2022), to compute perplexity scores. Additionally, we\nexamined how alterations in the masking percentage of\nCodeBERT can impact the detector’s performance when\naligned with different scoring models. In Figure 3, we chose\nthe C language subset as our test data. Figure 3 gives in-\nsights into how different perplexity calculation models per-\nform in terms of AUC as the masking in the test code varies.\nIt particularly highlights the heightened sensitivity of large\nparameter models like ’text-davinci-003,’ which are trained\non specialized code corpora.\nIn the Table 2, we employed these models to compute per-\nplexity and set 15% mask percentage. It shows models with\nlarger parameters have a better grasp of code semantics and,\nas a result, perform better in detection.\nRQ3: Robustness Against Attacks\nWe evaluated the robustness of the AIGCode Detector\nagainst several adversarial attack methods designed specifi-\ncally to target such detectors. We selected 560 AI-generated\nC codes from the AIGCode dataset, all of which were accu-\nrately detected by both GPTZero and the AIGCode Detec-\ntor. Subsequently, we employed four mutation strategies to\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23159\nC C++ C# Java JavaScript Python\nDetectors AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR\nGPT2-Detector 0.64 0.83 0.03 0.68 0.84 0.05 0.46 0.92 0.13 0.44 0.91 0.11 0.39 0.90 0.26 0.38 0.89 0.25\nDetectGPT 0.56 0.00 1.00 0.42 0.00 1.00 0.49 0.00 1.00 0.43 0.00 1.00 0.51 0.00 1.00 0.49 0.00 1.00\nRoBERTa-QA 0.68 0.00 1.00 0.53 0.00 1.00 0.48 0.00 1.00 0.64 0.00 1.00 0.52 0.00 1.00 0.60 0.00 1.00\nWriter 0.78 0.13 0.91 0.62 0.15 0.98 0.52 0.13 0.91 0.54 0.01 0.96 0.56 0.10 0.89 0.51 0.08 0.97\nGPTZero 0.90 0.06 0.20 0.83 0.20 0.68 0.29 0.18 0.96 0.28 0.18 0.88 0.41 0.08 0.90 0.59 0.00 1.00\nCGCode Detector 0.95 0.16 0.08 0.88 0.13 0.02 0.86 0.12 0.07 0.82 0.15 0.05 0.81 0.21 0.15 0.92 0.14 0.03\nTable 1: Performance of Different Detectors Across Six Programming Languages.\nC C++ C# Java JavaScript Python\nDetectors AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR AUC FPR FNR\nGPT2-xl 0.79 0.28 0.12 0.75 0.30 0.15 0.73 0.29 0.14 0.72 0.27 0.13 0.71 0.28 0.16 0.69 0.32 0.17\nGPT-J 0.82 0.25 0.18 0.76 0.27 0.19 0.76 0.24 0.20 0.77 0.26 0.21 0.79 0.25 0.22 0.78 0.24 0.23\nGPT-NeoX 0.87 0.26 0.24 0.83 0.27 0.25 0.80 0.26 0.26 0.74 0.25 0.25 0.75 0.27 0.27 0.83 0.28 0.28\nTable 2: Performance for Different Perplexity Calculation Models of AIGCode Detector.\nFigure 3: Illustration of the AUC vs. Mask Percentage for\nVarious Perplexity Calculation Models. Model text-davinci-\n003 has a greater sensitivity on codes compared to other\nmodels. Consequently, text-davinci-003 will consistently re-\nsults in an increase in the perplexity of perturbed samples\nwhile its mask percentage higher than 15%, which make al-\nmost all perturbed samples’ PPL is larger than original code.\ngenerate four derivative subsets, with the primary objective\nof challenging and potentially evading detection by the said\ndetectors.\nRegular Rewrite: To enhance the human-like quality of\nAI-generated code, we employed two primary strategies.\nFirstly, using a curated lexicon of function and variable\nnames, we systematically replaced 80% AI-chosen names\nwith alternatives from our list. Secondly, guided by specific\nstylistic conventions, we made targeted character replace-\nments.\nSampling Techniques: These techniques dictate the out-\nput randomness and determinism of AI-generated content,\nimpacting its resemblance to human-generated content. We\nset Temperature Sampling (Top-k) to 0.9, Nucleus Sampling\n(Top-p) to 0.5 to create more creative and random answers.\n• Temperature Sampling/Top-k:By varying the sampling\ntemperature, we can influence the randomness of the out-\nput. A higher temperature (closer to 2) produces more\nrandomness, whereas a lower value (closer to 0) results\nin more deterministic outputs.\n• Nucleus Sampling/Top-p: This approach selects tokens\nbased on the cumulative probability. Using a smaller per-\ncentage, like 0.1, the output is constrained to the top 10%\nprobable tokens, leading to more deterministic and co-\nherent text.\nSmarter Prompts: Effective prompting plays a pivotal\nrole in steering LLMs towards desired outputs. Utilizing\nmore precise and cleverly formulated prompts can induce\nLLMs to generate outputs that are close to human-produced\ncontent. We add several prefix-prompts for code generation\ntask, such as: ”Please generate a code answer written in a\nmore casual, human-like style with high randomness as if\na beginner programmer or a hobbyist wrote it. I want the\ncode to appear less formal and more like something some-\none might quickly jot down.”\nCode Blending: Integrating AI-generated segments with\nhuman-authored parts can craft a seamless blend that chal-\nlenges detection. Instead of entirely AI-generated or human-\nwritten content, merging segments from both can produce a\ncode that retains human-like irregularities while benefiting\nfrom AI’s efficiency. We truncated half of the human-written\ncode and prompted the model with the program specification\nto complete the remaining portion.\nTable 3 indicates ”Sampling Techniques” and ”Code\nBlending” strategy significantly challenge detectors, result-\ning in high bypass rates. The variability introduced by in-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23160\nDetector Mutation Set Bypass Rate\nGPTZero\nRegular Rewrite 0.22\nSampling Techniques 0.32\nSmarter Prompts 0.20\nCode Blending 0.87\nAIGCode Detector\nRegular Rewrite 0.13\nSampling Techniques 0.37\nSmarter Prompts 0.17\nCode Blending 0.75\nTable 3: Bypass Rates for Different Detectors and Mutation\nSets.\ncreased temperature in sampling and the hybrid nature of\ncode blending effectively mask AI-generated code patterns,\nmaking detection difficult.\nRelated Work\nCode Generation Using Large Language Models\nAI chat-bots like ChatGPT and Claude, while designed for\nnatural language interactions, have shown proficiency in\ncode-related tasks due to their training on vast datasets of\nsource code and documentation. This enables them to handle\ntasks such as code completion, bug fixing, and code synthe-\nsis. In a more specialized vein, OpenAI’s Codex, an offshoot\nof GPT-3 (Brown et al. 2020), is tailored for programming\ntasks and shines in code generation and comprehension. A\nprime example of its application is GitHub Copilot (GitHub\n2021), an AI-assisted coding tool co-developed by GitHub,\nOpenAI, and Microsoft. It intelligently offers real-time code\nsuggestions, drawing from the user’s coding context or a de-\nscriptive comment, and is adept at understanding both the\nactive file and its associated files. Similarly, tools like Code-\nGen (Nijkamp et al. 2022), Amazon CodeWhisperer (Ama-\nzon Web Services 2022), and CodeGeeX (Zheng et al. 2023)\nharness the power of AI to generate code based on natural\nlanguage inputs.\nAI-Generated Text Detectors\nClassical machine learning techniques, such as bag-of-\nwords combined with logistic regression, have been foun-\ndational, with studies like those by Solaiman et al. (So-\nlaiman et al. 2019) employing them to differentiate GPT-2\noutputs from human writing. Log probability-based meth-\nods have been introduced, with Solaiman (Solaiman et al.\n2019) using TGM to evaluate total log probability and GLTR\n(Gehrmann, Strobelt, and Rush 2019) offering statistical\ntechniques to pinpoint differences between GPT-2 and hu-\nman content. The DetectGPT (Mitchell et al. 2023) em-\nploys log probabilities produced by the targeted model and\nrandom perturbations from a generic pre-trained language\nmodel (like T5). Additionally, leveraging pre-trained lan-\nguage models through fine-tuning has emerged as a domi-\nnant strategy. For instance, GROVER (Zellers et al. 2019)\nutilizes a linear classifier on its base model to outclass other\ndetectors, emphasizing the importance of public availability\nof such generators for research. Meanwhile, the fine-tuned\nRoBERTa model (Uchendu et al. 2020) achieves approx-\nimately 95% accuracy in detecting GPT-2-generated web\npages, benefiting from its inherent bidirectional representa-\ntions. Kirchenbauer et al. (Kirchenbauer et al. 2023) propose\na watermarking framework for large language models. This\nmethod embeds signals into generated text that remain algo-\nrithmically detectable, yet are imperceptible to humans.\nDiscussion\nIn this section, we recognize the inherent limitations of our\ncurrent methodology and explore the potential avenues for\nfuture research.\nLimitations: A key limitation is the dependency on a\nsingle generative model (i.e. OpenAI’s text-davinci-003), a\nmainstream GPT-3.5 variant trained on code. We haven’t\ntested our approach across a diverse set of code genera-\ntion models. This singular reliance could introduce biases.\nAs the DetectGPT research suggests, when the generation\nand PPL calculation models are identical, detection accu-\nracy might be slightly improved. Another limitation is our\nbalanced dataset, the balanced dataset provides a fair base-\nline, illustrating model performance without the influence of\nclass imbalance. In contrast, the unbalanced dataset reflects\npotential scenarios the model may encounter in real-world\nsettings, where data is often imbalanced.\nFuture Work: To address the limitations identified, fu-\nture research will focus on evaluating the proposed ap-\nproach across a broader spectrum of code generation mod-\nels, mitigating potential biases. We also plan to delve deeper\ninto scenarios with real-world data imbalances, refining\nour methodology to ensure consistent and reliable detec-\ntion accuracy. Besides, our approach currently depends on\ncode rewriting, which is both time-consuming and resource-\nintensive. To address these challenges, we’re considering the\nadoption of supervised models. The goal is to train a super-\nvised model that can efficiently detect AI-generated code by\nrecognizing specific perplexity patterns instead of just rely-\ning on code embeddings. By integrating deep learning with\nlog probability techniques, we anticipate improved detection\nof AI-generated code.\nConclusion\nIn conclusion, we present the AIGCode Detector, an inno-\nvative tool adept at pinpointing AI-generated code assign-\nments using perplexity analysis and targeted perturbations.\nOur evaluations highlight its superiority over current detec-\ntors. Our tool not only serves as a strong defense for aca-\ndemic integrity but also promotes the judicious use of AI\nin programming education. In future pursuits, we anticipate\nrefining this tool by exploring its efficiency across a wider\nrange of AI models and real-world applications.\nReferences\nAmazon Web Services. 2022. AWS CodeWhisperer. Code\nsuggestions system by Amazon Web Services using a deep\nlearning AI trained on AWS code and documentation.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23161\nBlack, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.;\nGolding, L.; He, H.; Leahy, C.; McDonell, K.; Phang, J.;\net al. 2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nFeng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;\nShou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. Codebert: A\npre-trained model for programming and natural languages.\narXiv preprint arXiv:2002.08155.\nGehrmann, S.; Strobelt, H.; and Rush, A. M. 2019. Gltr: Sta-\ntistical detection and visualization of generated text. arXiv\npreprint arXiv:1906.04043.\nGitHub. 2021. GitHub Copilot. Code auto-completion sys-\ntem developed by GitHub using their private dataset of pub-\nlic GitHub repositories and trained via Codex from OpenAI.\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\nYue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-\nman Experts? Comparison Corpus, Evaluation, and Detec-\ntion. arXiv preprint arxiv:2301.07597.\nKirchenbauer, J.; Geiping, J.; Wen, Y .; Katz, J.; Miers, I.;\nand Goldstein, T. 2023. A watermark for large language\nmodels. arXiv preprint arXiv:2301.10226.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMitchell, E.; Lee, Y .; Khazatsky, A.; Manning, C. D.; and\nFinn, C. 2023. Detectgpt: Zero-shot machine-generated\ntext detection using probability curvature. arXiv preprint\narXiv:2301.11305.\nNijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou,\nY .; Savarese, S.; and Xiong, C. 2022. Codegen: An open\nlarge language model for code with multi-turn program syn-\nthesis. arXiv preprint arXiv:2203.13474.\nOpenAI. 2022. text-davinci-003. https://platform.openai.\ncom/docs/guides/gpt/completions-api. Accessed: 2023-10-\n01.\nPuri, R.; Kung, D. S.; Janssen, G.; Zhang, W.; Domeni-\nconi, G.; Zolotov, V .; Dolby, J.; Chen, J.; Choudhury, M.;\nDecker, L.; et al. 2021. Project codenet: A large-scale ai for\ncode dataset for learning a diversity of coding tasks. arXiv\npreprint arXiv:2105.12655, 1035.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the Limits of Transfer Learning with a Unified Text-to-\nText Transformer. Journal of Machine Learning Research,\n21(140): 1–67.\nSolaiman, I.; Brundage, M.; Clark, J.; Askell, A.; Herbert-\nV oss, A.; Wu, J.; Radford, A.; Krueger, G.; Kim, J. W.;\nKreps, S.; et al. 2019. Release strategies and the social im-\npacts of language models.arXiv preprint arXiv:1908.09203.\nTian, E.; and Cui, A. 2023. GPTZero: Towards detection of\nAI-generated text using zero-shot and supervised methods.\nUchendu, A.; Le, T.; Shu, K.; and Lee, D. 2020. Authorship\nattribution for neural text generation. In Proceedings of the\n2020 conference on empirical methods in natural language\nprocessing (EMNLP), 8384–8395.\nWang, B.; and Komatsuzaki, A. 2021. GPT-J-6B: A 6\nBillion Parameter Autoregressive Language Model. https:\n//github.com/kingoflolz/mesh-transformer-jax. Accessed:\n2023-10-10.\nWang, Y .; Wang, W.; Joty, S.; and Hoi, S. C. 2021. Codet5:\nIdentifier-aware unified pre-trained encoder-decoder mod-\nels for code understanding and generation. arXiv preprint\narXiv:2109.00859.\nWriter.com. 2023. The Writer AI Detector. https://writer.\ncom/ai-content-detector/. Accessed: 2023-10-01.\nXu, F. F.; Alon, U.; Neubig, G.; and Hellendoorn, V . J. 2022.\nA systematic evaluation of large language models of code. In\nProceedings of the 6th ACM SIGPLAN International Sympo-\nsium on Machine Programming, 1–10.\nXue, L.; Constant, N.; Roberts, A.; Kale, M.; Al-Rfou, R.;\nSiddhant, A.; Barua, A.; and Raffel, C. 2020. mT5: A\nmassively multilingual pre-trained text-to-text transformer.\narXiv preprint arXiv:2010.11934.\nZellers, R.; Holtzman, A.; Rashkin, H.; Bisk, Y .; Farhadi,\nA.; Roesner, F.; and Choi, Y . 2019. Defending against neu-\nral fake news. Advances in neural information processing\nsystems, 32.\nZheng, Q.; Xia, X.; Zou, X.; Dong, Y .; Wang, S.; Xue,\nY .; Wang, Z.; Shen, L.; Wang, A.; Li, Y .; et al. 2023.\nCodegeex: A pre-trained model for code generation with\nmultilingual evaluations on humaneval-x. arXiv preprint\narXiv:2303.17568.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23162",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9867448210716248
    },
    {
      "name": "Code (set theory)",
      "score": 0.6709035634994507
    },
    {
      "name": "Computer science",
      "score": 0.6575804948806763
    },
    {
      "name": "Natural language processing",
      "score": 0.6175268292427063
    },
    {
      "name": "Language model",
      "score": 0.5069875717163086
    },
    {
      "name": "Programming language",
      "score": 0.4655841886997223
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43848273158073425
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12315562",
      "name": "Texas Tech University",
      "country": "US"
    }
  ]
}