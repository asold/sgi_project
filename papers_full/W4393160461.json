{
    "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
    "url": "https://openalex.org/W4393160461",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2304771752",
            "name": "Changmao Li",
            "affiliations": [
                "University of California, Santa Cruz"
            ]
        },
        {
            "id": "https://openalex.org/A2141750911",
            "name": "Jeffrey Flanigan",
            "affiliations": [
                "University of California, Santa Cruz"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225905768",
        "https://openalex.org/W4309201735",
        "https://openalex.org/W4377161543",
        "https://openalex.org/W6803096969",
        "https://openalex.org/W6662198291",
        "https://openalex.org/W3138815606",
        "https://openalex.org/W6809918409",
        "https://openalex.org/W6793601707",
        "https://openalex.org/W6810418643",
        "https://openalex.org/W1972978214",
        "https://openalex.org/W4226242393",
        "https://openalex.org/W6793897959",
        "https://openalex.org/W4388092201",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W3002104146",
        "https://openalex.org/W3212893438",
        "https://openalex.org/W4221162039",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W4388788574",
        "https://openalex.org/W4388444775",
        "https://openalex.org/W4386908094",
        "https://openalex.org/W4385565080",
        "https://openalex.org/W4387994765",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4287270166",
        "https://openalex.org/W4389518805",
        "https://openalex.org/W4385573341",
        "https://openalex.org/W4385567093",
        "https://openalex.org/W4389519044",
        "https://openalex.org/W4289494028",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4385572867",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4285107714",
        "https://openalex.org/W4389518953",
        "https://openalex.org/W4385965989",
        "https://openalex.org/W4378942772",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4385572404",
        "https://openalex.org/W4385734111",
        "https://openalex.org/W4226278401"
    ],
    "abstract": "Large language models (LLMs) offer impressive performance in various zero-shot and few-shot tasks. However, their success in zero-shot or few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined. This paper investigates how zero-shot and few-shot performance of LLMs has changed chronologically over datasets released over time, and over LLMs released over time. Utilizing GPT-3 series models and several other recent open-sourced LLMs, and controlling for dataset difficulty, we find that datasets released prior to the LLM training data creation date perform surprisingly better than datasets released post the LLM training data creation date. This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets prior to the LLMs' training data creation date. Additionally, we utilize training data inspection, training data extraction, and a membership inference attack, which reveal further evidence of task contamination. Importantly, we find that for tasks with no possibility of task contamination, LLMs rarely demonstrate statistically significant improvements over simple majority baselines, in both zero and few-shot settings.",
    "full_text": "Task Contamination: Language Models May Not Be Few-Shot Anymore\nChangmao Li, Jeffrey Flanigan\nUniversity of California, Santa Cruz\nchangmao.li@ucsc.edu, jmflanig@ucsc.edu\nAbstract\nLargelanguagemodels(LLMs)offerimpressiveperformance\nin various zero-shot and few-shot tasks. However, their suc-\ncess in zero-shot or few-shot settings may be affected by task\ncontamination, a potential limitation that has not been thor-\noughly examined. This paper investigates how zero-shot and\nfew-shot performance of LLMs has changed chronologically\noverdatasetsreleasedovertime,andoverLLMsreleasedover\ntime. Utilizing GPT-3 series models and several other recent\nopen-sourcedLLMs,andcontrollingfordatasetdifficulty,we\nfind that datasets released prior to the LLM training data cre-\nation date perform surprisingly better than datasets released\npost the LLM training data creation date. This strongly indi-\ncates that, for many LLMs, there exists task contamination\non zero-shot and few-shot evaluation for datasets prior to the\nLLMs‚Äô training data creation date. Additionally, we utilize\ntraining data inspection, training data extraction, and a mem-\nbershipinferenceattack,whichrevealfurtherevidenceoftask\ncontamination.Importantly,wefindthatfortaskswithnopos-\nsibility of task contamination, LLMs rarely demonstrate sta-\ntisticallysignificantimprovementsoversimplemajoritybase-\nlines, in both zero and few-shot settings.\n1 Introduction\nRecently there has been much interest in few-shot methods,\nin particular in-context learning (ICL, Brown et al. 2020)\nwithlargelanguagemodels.In-contextlearninghastheben-\nefit of yielding excellent performance while requiring very\nlittle data, sometimes relying on only a few examples for\nthe task. These promising results have led to an explosion\nof work on in-context learning methods across a wide vari-\netyoftasks(SchickandSch√ºtze2021a,b;Poesiaetal.2022;\nHuetal.2022b),includingprompttuningmethods(Qinand\nEisner2021;Lester,Al-Rfou,andConstant2021),chain-of-\nthought methods (Wei et al. 2022; Wang, Deng, and Sun\n2022; Wang et al. 2023; Aiyappa et al. 2023), tool-based\nmethods (Schick et al. 2023; Yang et al. 2023).\nHowever,alongwiththisexplosionofworkinICL,many\nhaveraisedconcernsaboutdatacontamination(Brownetal.\n2020; Jacovi et al. 2023), that is, prior knowledge of data\nor a task which is thought to be unseen by the model. Data\ncontamination can happen in multiple ways. One common\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ndatasets prior datasets post\n0\n20\n40\n60\n80\n100Percentage larger than majority baseline\nzero-shot\nfew-shot\nzero-shot stat. sig.\nfew-shot stat. sig.\nFigure 1: Percentage of datasets with accuracy higher than\nthe majority baseline for datasets released prior and post\nLLM training data collection date, for both zero-shot (blue,\nleft) and few-shot (green, right). Results are across all mod-\nels and all datasets. On datasets released post training data\ncollection date for the LLM, the LLM is much less likely to\nimproveuponthesimplemajoritybaseline. Stat. sig.(darker)\nis the percent of datasets for which the performance above\nmajority baseline is significant at the 99% confidence level.\ncontaminant is test data contamination, the inclusion of\ntest data examples and labels in the pre-training data. An-\nother contaminant for zero or few-shot methods, which we\ncalltaskcontamination,istheinclusionoftasktrainingex-\namples in the pre-training data, effectively making the eval-\nuation no longer zero or few-shot.1\nSimply evaluating the scope of this contamination is dif-\nficult to do (Magar and Schwartz 2022; Jacovi et al. 2023).\n1Zero-shotevaluationisevaluationwhereamodelhasseenzero\nexamples for the task. Few-shot, orùëÅ-shot, whereùëÅ is a small\nnumber,iswherethemodelhasseen ùëÅ examplesforthetask.Prior\nworkhassometimesdefinedzero-shotformulti-classclassification\nas predictingclasses that have never been seen during training, but\nmost recent work does not use this definition.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18471\nClosed models do not release their pre-training data. While\nopen models give the sources, crawling the sites to obtain\nthat data is non-trivial, especially if the data has changed\nfrom when it was crawled. For models that are pre-trained\nonfreelyavailablepre-trainingcorpora,simplygreppingfor\nexamplesinthepre-trainingcorporamaynotbereliabledue\nto differences in data formatting (such as XML vs CVS, etc)\nor differences in text normalization and tokenization.\nIn this paper we empirically measure the scope of task\ncontamination for few-shot methods across various models\nand tasks. To the best of our knowledge, we are the first to\nsystematicallyanalyzethisproblem.Weevaluate12different\nmodels, ranging from closed GPT-3 series models (OpenAI\n2023) to open models including Fairseq MoE (Artetxe et al.\n2022), GPT-J (Wang and Komatsuzaki 2021), Bloom (Scao\net al. 2022), OPT (Zhang et al. 2022) , LLaMA (Touvron\net al. 2023), Alpaca (Taori et al. 2023), and Vicuna (Chiang\netal.2023)on16classificationtasksand1semanticparsing\ntask.\nWeanalyzeeachmodelondatasetscreatedbeforeitstrain-\ning data was crawled on the internet versus datasets created\nafterward. We find that datasets created before the training\ndatawascollectedhaveasignificantlyhigherchanceofhav-\ning performance higher than the majority baseline (Fig. 1).\nWe perform training data inspection and task data extrac-\ntiontolookforpossibletaskcontamination.Importantly,we\nfind that for tasks with no possibility of task contamination,\nmodels rarely demonstrate statistically significant improve-\nmentsoversimplemajoritybaselinesacrossarangeoftasks,\nin both zero and few-shot settings (Fig. 2).\nAsacasestudy,wealsoattempttoconductamembership\ninferenceattackforasemanticparsingtask(SPIDER)forall\nmodelsinouranalysis,andfindastrongcorrelation(R=.88)\nbetween number of extracted examples and the accuracy of\nthe model on the final task (Fig. 6). This is strong evidence\nthat the performance increase in zero-shot performance on\nthis task is due to task contamination.\nAdditionally, we look closely at the GPT-3 series mod-\nels. We find that training examples can be extracted from\nthe GPT-3 models, and that the number of extractable train-\ning examples increased from each version fromdavinci\nto GPT-3.5-turbo, and closely tracks the increase in\nzero-shot performance of the GPT-3 models on that task\n(Fig. 2). This is strong evidence that the increase in perfor-\nmance on this task across GPT-3 models fromdavinci to\nGPT-3.5-turbo is due to task contamination.\n2 Overview\nWe employ four methods of measuring task contamination.\n1. Training data inspection: Search through the training\ndata to find task training examples.\n2. Task data extraction: Extract task data from an exist-\ning model. Extraction is only possible with instruction-\ntuned models. This analysis can also be done for training\ndata or testing data extraction (Sainz et al. 2023b). Note:\nForthepurposesofdetectingtaskcontamination,theex-\ntracted task data need not exactly match existing train-\ning data examples. Any training examples demonstrating\nthetaskindicatepossiblecontaminationforzeroandfew-\nshot learning.\n3. Membership inference: This method only applies to\ngeneration tasks. Check if the model generated content\nfor an input instance is exactly the same as the original\ndataset (Hu et al. 2022a). If there is an exact match, we\ncan infer it is a member of the LLM‚Äôs training data. This\ndiffers from task data extraction because generated out-\nput is checked for an exact match. Exact matches for an\nopen-ended generation task strongly indicate the model\nhas seen those examples during training. The model is\nnot just good, it is psychic: it has knowledge of the exact\nphrasing used in the data. Note: this can only be used for\ngeneration tasks.2\n4. Chronologicalanalysis:forasetofmodelswhosetrain-\ning data has been collected at a range of known times,\nmeasure performance on a dataset with a known re-\nleasedate,andcheckforevidenceofcontaminationusing\nchronological evidence.\nThe first three methods have high precision, but suffer\nfrom low recall. If data is found in the training data for the\ntask,thenitiscertainthatithasseenexamples.Butbecause\nofdataformattingvariations,variationsinkeywordsusedto\ndefinethetask,andthesizeofthedataset,theabsenceofev-\nidence for contamination using the first three methods is not\nevidence of absence.\nThe fourth method, chronological analysis, is high recall,\nbutlowprecision.Iftheperformanceishighduetotaskcon-\ntamination, then a chronological analysis will have a high\nchanceofcatchingit.Butotherfactorscouldalsocontribute\nto increased performance over time, so the precision is low.\nDue to their inherent trade-offs, we employ all four meth-\nodsfordetectingtaskcontamination.Withallfourmethods,\nwefindstrongevidenceoftaskcontaminationforsomecom-\nbinationsofmodelsanddatasets.Webeginwithachronolog-\nical analysis for all models and datasets we tested, since it\nhasthehighestpotentialforcatchingpossiblecontamination\n(¬ß4). We then look for further evidence of task contamina-\ntion using training data inspection (¬ß5), task data extraction\n(¬ß6) and membership inference attack (¬ß7).\n3 Models and Datasets\nModels We experimented with 12 models. Table 1 lists\nthese models, along with the collection dates of the training\ndata and release dates for each model.3 The 12 models we\nusecanbefurthercategorizedintotwobroadgroups:(1)five\nproprietary GPT-3 series models (\"closed\") and (2) seven\nopenmodelswithfreeaccesstotheirweights(\"open\").Com-\nparingmodelsfromthesetwogroupsyieldsvaluableinsights\ninto the difference between proprietary, high-performance\nmodels like those from the GPT-3 series and more acces-\nsible, community-driven open models. More information\n2Exactmatchesfortheinputdonotindicatetaskcontamination\nbecausetheinputtextcouldhavebeenseen,butitneedstobepaired\nwith the output label for task contamination.\n3GPT-3 series training data collection dates are obtained from\nhttps://platform.openai.com/docs/models/overview\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18472\nModel Training data Release\ndavinci Up to Oct 2019 May 2020\ndavinci-001 Up to Oct 2019 Jun 2020\ndavinci-002 Up to Jun 2021 Jan 2022\ndavinci-003 Up to Jun 2021 Nov 2022\nGPT-3.5-T Up to Sep 2021 Mar 2023\n(a) GPT-3 Series LLMs\nModel Training data Release\nFairseq MoE Up to Feb 2019 Dec 2021\nGPT-J Up to 2020 Jun 2021\nOPT Up to Oct 2021 May 2022\nBLOOM Prior Aug 2022 Nov 2022\nLLaMA Up to Aug 2022 Feb 2023\nAlpaca From davinci-003 Mar 2023\nVicuna From ChatGPT Mar 2023\n(b) Open LLMs\nTable 1: Dates for the training data creation and model re-\nlease. davinci-XXX refers totext-davinci-XXX. GPT-\n3.5-T refers toGPT-3.5-turbo-0301.\nabout these models is given in the Appendix of the arXiv\nversion of the paper.\nDatasets Zero-shotandfew-shotevaluationsinvolvemod-\nels making predictions on tasks that they have never seen\nor seen only a few times during training. The key premise\nis that the models have no prior exposure to the particular\ntask at hand, ensuring a fair evaluation of their learning ca-\npacity. Contaminated models, however, give a false impres-\nsionofitszero-orfew-shotcompetency,astheyhavealready\nbeen trained on task examples during pretraining. Detecting\nsuch inconsistencies would be relatively easier in a chrono-\nlogically ordered dataset, where any overlap or anomaly\nwouldstandout.Basedonthisnarrative,wesplitthedatasets\ninto two categories: datasets released before or after Jan-\nuary1st,2021,identifiedas pre-2021datasetsand post-2021\ndatasets. We use this division to analyze the zero-shot or\nfew-shot performance difference between older datasets and\nnewer ones, with the same division applied for all LLMs.\nWe also use the per-LLM divisionpre-collection and post-\ncollection datasets, which distinguishes datasets that the\nmodelwaspossiblytrainedon(pre-collectiondatasets)from\nthedatasetsitcouldnothavebeentrainedon(post-collection\ndatasets). Table 1 presents the creation time of the training\ndata for each model. Information about the datasets can be\nfound in the Appendix,4 while release dates for each dataset\nare listed in Table 2.\n4 Chronological Analysis\nWe start with a chronological analysis. This allows us to de-\ntectpatternsofpossibletaskcontaminationacrosstheLLMs\n4The Appendix is available in the arXiv version of the paper.\nhttps://arxiv.org/abs/2312.16337\nPre-2021 Post-2021\nDataset Year Dataset Year\nRTE 2009 StrategyQA 2021\nWNLI 2011 NewsMTSC-MT 2021\nCOPA 2011 NewsMTSC-RW 2021\nSST-2 2013 NLI4Wills 2022\nMRPC 2015 CREPE 2023\nQNLI 2018 FOMC 2023\nCB 2019 NewsMet 2023\nWiC 2019\nBoolQ 2019\nTable 2: Dataset release year for each dataset, split into pre-\n2021 datasets and post-2021 datasets.\nand datasets we examine.\nAnalysis of Pre- and Post-collection Datasets\nWe perform a global chronological analysis across all\ndatasets and LLMs. We look at the difference between per-\nformanceondatasetsreleasedbeforethetrainingdatacollec-\ntion date for the LLM (pre-collection datasets) versus after\nthe training data collection date (post-collection datasets).\nSpecifically, we focus on whether the model is above the\nmajority baseline.5 In this section we use this measure, in-\nstead of averaging the performance across datasets, to avoid\ndatasets with large performance differences dominating the\nanalysis.\nThe results are shown in Fig. 1. We find that for datasets\nreleasedpriortothecreationoftheLLM,itismorelikelythe\nLLM beats the majority baseline for both zero and few-shot\nsettings. Using the Mann-Whitney U test (Mann and Whit-\nney1947),wefindthedifferenceinthoseabovethemajority\nbaseline between pre- and post-collection populations to be\nstatistically significant at the 99% confidence level for both\nzero and few shot settings.\nForsomedatasetsandmodels,theperformancedifference\nabove the majority baseline is small, so we also perform\nthe same comparison counting datasets for which the results\nabovethemajoritybaselinearestatisticallysignificantatthe\n99% level, calculated using the student t-test (Student 1908)\n(Fig.1,darker).Again,wefindthatfordatasetsreleasedprior\ntothecreationoftheLLM,itisfarmorelikelytheLLMbeats\nthe majority baseline with statistical significance for both\nzero and few-shot settings. Similarly, the Mann-Whitney U\ntest indicates these differences between pre and post are sta-\ntisticallysignificantatthe99%confidencelevelforbothzero\nand few shot settings.\nThese results indicate the possibility of task contamina-\ntion for both open LLMs and GPT-3 series LLMs, with a\nstrongerindicationofcontaminationintheGPT-3serieswith\ndavinci-001 and after.\n5The majority baseline for a classification task is the perfor-\nmance of a model that labels every example with the label that oc-\ncurs most often in the dataset.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18473\ndavinci\n2019(9)\ndavinci-001\n2019(9)\ndavinci-002\n2021(12)\ndavinci-003\n2021(12)\nGPT-3.5-T\n2021(12)\n0\n20\n40\n60\n80\n100Percentage larger than majority baseline\nzero-shot\nfew-shot\nzero-shot stat. sig.\nfew-shot stat. sig.\nT ask Extraction\n0\n20\n40\n60\n80\n100\nPercentage of tasks can be extracted\n(a) GPT-3 series on pre-collection datasets\ndavinci\n2019(7)\ndavinci-001\n2019(7)\ndavinci-002\n2021(4)\ndavinci-003\n2021(4)\nGPT-3.5-T\n2021(4)\n0\n20\n40\n60\n80\n100Percentage larger than majority baseline\nzero-shot\nfew-shot\nzero-shot stat. sig.\nfew-shot stat. sig.\nT ask Extraction\n0\n20\n40\n60\n80\n100\nPercentage of tasks can be extracted (b) GPT-3 series on post-collection datasets\nMoE\n2019(9)\nGPT-J\n2020(9)\nOPT\n2021(12)\nBLOOM\n2022(13)\nLLaMA\n2022(13)\nAlpaca\n2022(13)\nVicuna\n2022(13)\n0\n20\n40\n60\n80\n100Percentage larger than majority baseline\nzero-shot\nfew-shot\nzero-shot stat. sig.\nfew-shot stat. sig.\nT ask Extraction\n0\n20\n40\n60\n80\n100\nPercentage of tasks can be extracted\n(c) Open LLMs on pre-collection datasets\nMoE\n2019(7)\nGPT-J\n2020(7)\nOPT\n2021(4)\nBLOOM\n2022(3)\nLLaMA\n2022(3)\nAlpaca\n2022(3)\nVicuna\n2022(3)\n0\n20\n40\n60\n80\n100Percentage larger than majority baseline\nzero-shot\nfew-shot\nzero-shot stat. sig.\nfew-shot stat. sig.\nT ask Extraction\n0\n20\n40\n60\n80\n100\nPercentage of tasks can be extracted (d) Open LLMs on post-collection datasets\nFigure2:PercentageofdatasetslargerthanmajoritybaselinesforeachLLM(lightcolor),aswellasthepercentageoftasksfor\nwhichtrainingdatacanbeextractedwithaninstructionprompt(Red,seealsoTable4).Darkcoloristhepercentageofdatasets\nsignificantly larger (ùëù= .99) than the majority baseline using the student t-test. The number in parentheses is the total number\nof datasets for that LLM that belongs in pre- or post-collection (e.g. MoE has 7 datasets post its training collection date.) For\ntasks with no possibility of task contamination (post-collection datasets (b) and (d), with no extracted task examples in red),\nmodels rarely demonstrate statistically significant improvements over majority baselines, in both zero and few-shot settings.\nCaveats There are two considerations we need to make in\nthe global chronological analysis.\nFirst, datasets may have become more difficult over time,\nmeaning LLMs are less likely to outperform the majority\nbaseline despite the lack of task contamination. To account\nfor this, we carefully review the tasks and remove tasks\nknown to be difficult for LLMs, such as GSM8K (Cobbe\net al. 2021) and TrackingShuffledObjects (Srivastava et al.\n2022). The remaining datasets all have decent performance\nusing fine-tuned pretrained language models (PLMs), and,\nimportantly,thereisnocorrelationbetweenreleasedateand\nthe performance of fine-tuned PLMs (ùëÖ2 = 0.001) on our\ndatasets, as shown in Fig. 4.\nSecondly, post-collection datasets, despite being released\nafter data collection, may still cause contamination. For ex-\nample, the FOMC dataset (Shah, Paturi, and Chava 2023)\nwas officially released post-collection for the GPT-3 series,\nbut its performance on subsequent versions is notably high.\nThis may be the result of the authors‚Äô preliminary experi-\nmentationwiththeGPT-3series(asstatedintheirpaper),as\nOpenAI may have then utilized their experimental data for\nmodel updates.\nAnalysis of Pre- and Post-collection for Individual\nLLMs\nInthissection,weconsidertheperformanceonpre-andpost-\ncollection datasets for each LLM individually (see Fig. 2).\nWe find the difference in performance between the two cat-\negories to be statistically significant at 95% confidence ac-\ncording to the paired sign test (Dixon and Mood 1946).\nWe plot the percentage of datasets larger than the major-\nity baseline as in the last section, but for each LLM indi-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18474\ndavinci\n2019\ndavinci-001\n2019\ndavinci-002\n2021\ndavinci-003\n2021\nGPT-3.5-turbo\n2021\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0average accuracy\nzero-shot few-shot majority fine-tune\n(a) GPT-3 series on pre-2021 datasets.\ndavinci\n2019\ndavinci-001\n2019\ndavinci-002\n2021\ndavinci-003\n2021\nGPT-3.5-turbo\n2021\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0average accuracy\nzero-shot few-shot majority fine-tune (b) GPT-3 series on post-2021 datasets.\nMoE\n2019\nGPT-J\n2020\nOPT\n2021\nBLOOM\n2022\nLLaMA\n2022\nAlpaca\n2022\nVicuna\n2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0average accuracy\nzero-shot few-shot majority fine-tune\n(c) Open LLMs on pre-2021 datasets.\nMoE\n2019\nGPT-J\n2020\nOPT\n2021\nBLOOM\n2022\nLLaMA\n2022\nAlpaca\n2022\nVicuna\n2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0average accuracy\nzero-shot few-shot majority fine-tune (d) Open LLMs on post-2021 datasets.\nFigure 3: Average performance on datasets pre/post-2021.\nTask Release Year\nAccuracy on fine-tuning LM\n0\n25\n50\n75\n100\n2010 2012 2014 2016 2018 2020 2022\nAccuracy on fine-tuning LM   R¬≤ = 0.001\nFigure 4: Task accuracy of a fine-tuned LLM baseline vs.\ntask release year.ùëÖ2 = .001, which indicates that the task\ndifficulty for our datasets does not increase over time.\nvidually. The results are shown in Fig. 2. We observe that\ntheglobaltrendfromtheprevioussectionhasremainedtrue\nacross models with the full range of dates, further indicat-\ning that the absolute date of the dataset is not the main fac-\ntor, but rather the date of the dataset relative to the training\ndata collection date for the LLM is the more important fac-\ntor. (Note: because of the recency of LLaMA, Alpaca, and\nVicuna,wehavefewerdatasetsinourexperimentsposttheir\ntraining data collection date).\nImportantly, we find that for tasks with no possibility of\ntask contamination, models do not demonstrate statistically\nsignificant improvements over majority baselines, in both\nzeroandfew-shotsettings.Theexceptionis davinci-001\non post-collection datasets, which shows a statistically\nsignificant improvement over one post-collection dataset\n(MTSC-RW,asentimentclassificationdataset),butdoesnot\ngenerate task examples with our prompt (Table 4).\nPerformance over Time\nNext we perform a chronological analysis that examines the\nchangeinaverageperformanceovertimeforbothGPT-3se-\nries and open LLMs (Fig. 3). To also be sensitive to time\nof the datasets, we split our datasets into two sets: datasets\nreleased before or after January 1st, 2021, identified aspre-\n2021datasets andpost-2021datasets, respectively.\nPre-2021Datasets ForopenLLMs,onpre-2021datasets,\nwe see a slight increase over time for open LLMs (Fig. 3c).\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18475\nWe find that the performance hovers around the majority\nbaseline for both zero and few-shot settings, and does not\nincrease very much from LLM data collection dates ranging\nfrom 2019 to 2022.\nFor the GPT-3 series, on the other hand, the trend on pre-\n2021datasetsisparticularlysuspect(Fig.3a).Weseethatfor\npriorGPT-3datasets,theperformancehasincreaseddramat-\nically over time, with laterdavinci models much higher\nthan the majority baseline for both zero and few-shot set-\ntings.ThecomparisontoopenLLMsindicatesthatzeroand\nfewshotevaluationsmayhavetaskcontaminationissuesdue\nto data collected from user inputs.\nPost-2021 Datasets For post-2021 datasets, GPT-3 aver-\nage performance has also increased over time (Fig. 3b), par-\nticularly in the zero-shot setting. This makes sense, as many\nof the post-2021 datasets are released prior the training data\ncollectiondateforthelater davinci models.(Toseewhich\ndatasets are pre- or post- training data collection time, see\nthe line separating pre- and post- collection datasets in Ta-\nble4.)OpenLLMsaverageperformancealsoincreasedover\ntime, but they remain lower than the majority baseline and\nthe GPT-3 series.\nOne could hypothesize that the high performance of the\nGPT-3 series is due to instruction tuning (Ouyang et al.\n2022), however we do not believe this is the case. While we\nobserveanincreaseinperformancefrom davinci-001 to\ndavinci-002 onpre-2021datasets,thereisacorrespond-\ning decrease in performance on post-2021 datasets, which\nwemeasurewiththesigntesttobestatisticallysignificantat\nthe 95%. This demonstrates that the GPT-3 series instruc-\ntion tuning is specific to certain earlier datasets, and sug-\ngestsdatasetcontaminationforzeroandfew-shotevaluation\nof GPT-3 series.\n5 Training Data Inspection\nTosearchfordirectevidenceoftaskcontamination,wecon-\nduct training data inspection on two instruction fine-tuned\nopen LLMs (Alpaca and Vicuna) for all experimented clas-\nsification tasks. We search for task-related instruction pat-\nterns in the training data, and manually inspect them to see\nif they contain task training examples. We then compare the\nperformance to see if more task-specific training examples\nhasboostedperformance.Becausewemustcheckmanually,\nwe can perform this analysis only for the small fine-tuning\ndatasets of Alpaca and Vicuna.\nTable 3 shows the number of task examples on Alpaca\nand Vicuna, as well as the change in performance averaged\noverzeroandfew-shotsettings.Wefindthatperformancehas\nimproved for Alpaca and Vicuna over the original LLaMA\nmodel for tasks with task examples. This indicates that the\nperformance can be improved with small sets of task exam-\nplesinthetrainingdata,whichcancompromisezero-shotor\nfew-shot evaluation.\n6 Task Data Extraction\nWe test for task data contamination by attempting to extract\ntask data from the LLM. Prior work (Sainz et al. 2023b) has\ntestedifthereexiststestingdatacontaminationbyprompting\nDataset\nAlpaca Vicuna\nSST-2 8, +14.6% 0, -1.0%\nMRPC 0, -0.7% 0, -8.0%\nRTE 0, +3.1% 33, +10.6%\nQNLI 0, -0.4% 28, +10.0%\nWNLI 0, -1.4% 33, +7.7%\nCB 0, +9.8% 0, -23.2%\nCOPA ?, 0% ?, +10%\nWiC 0, -4.9% 0, -2.5%\nBoolQ ?, +1.9% ?, +4.0%\nStrategyQA 0, -3.3% 0, +10.3%\nNLI4Wills 0, -13.5% 0, -11.6%\nMTSC-RW ?, +9.6% ?, +11.3%\nMTSC-MT ?, +6.9% ?, +8.0%\nCREPE 0, +24.2% 0, -0.4%\nFOMC 0, -5.7% 1, -5.4%\nNewsMet 4, +7.2% 0, -11.4%\nTable3:Contaminationanalysisfortasks:#ofdatapointsin\nthe Alpaca and Vicuna datasets that match a regular expres-\nsion for the task, andŒî%, the change in performance aver-\nagedacrosszeroandfew-shotsettings.\"?\"meansthereisno\nspecific pattern to match, so we cannot count the number of\nexamples. Regular expressions for each task are listed in the\nAppendixofthearXivpaper. Œî%istheaverageperformance\ndifferenceoverzero-shotandfew-shotcomparedtotheorig-\ninal LLama model.\nanLLMtogenerateexamplesforatask.IftheLLMcangen-\nerate examples that exactly match examples in the test data,\nitisevidencethatthetestsetofthetaskhasbeenseenduring\ntraining by the LLM. Inspired by their method, we adopt a\nsimilarapproachtotestfortaskcontamination.Insteadofat-\ntemptingtogeneratetestdata,wepromptthemodeltogener-\natetrainingexamples,sinceforzero-orfew-shotevaluation,\nthe model should not be trained on any task examples. If an\nLLM can generate training examples based on the prompt,\nthisisevidenceoftaskcontamination.Notewedonotrequire\nan exact match of the generated examples with the training\ndataforthetask,sinceanyexamplesforthetaskseenduring\ntraining indicate possible task contamination.\nTable 4 shows the training data extraction results on all\ntasks across all models. For all pre-collection datasets,\nGPT-3seriesmodelsstartingfrom davinci-001 cangen-\nerate task specific training examples. There are somepost-\ncollection datasetsthat have evidence of contamination for\nthe GPT-3 series. These datasets may have been contami-\nnated if the authors of these datasets experimented with the\nGPT-3 series before releasing the dataset. For example, the\nFOMC paper (Shah, Paturi, and Chava 2023) states they\ntested with the GPT-3 series prior, which could have caused\ncontamination. For the open LLMs, almost no models can\ngenerate training examples of specific tasks except for Vi-\ncuna,whichisfine-tunedontheChatGPTdata.Notemodels\nwithout instruction tuning cannot follow the instructions di-\nrectingthemtogeneratetaskexamples,sothisanalysisisnot\nconclusive for these models.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18476\ndavinci\n2019\ndavinci-001\n2019\ndavinci-002\n2021\ndavinci-003\n2021\nGPT-3.5-turbo\n2021\n0\n10\n20\n30\n40\n50\n60Exact match amount\nspider-dev-EM\nspider-train-EM\nspider-dev-Accuracy\nspider-train-Accuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\n(a) Over GPT-3 series.\nMoE\n2019\nGPT-J\n2020\nOPT\n2021\nBLOOM\n2022\nLLaMA\n2022\nAlpaca\n2022\nVicuna\n2022\n0\n10\n20\n30\n40\n50\n60Exact match amount\nspider-dev-EM\nspider-train-EM\nspider-dev-Accuracy\nspider-train-Accuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy (b) Over recent LLMs.\nFigure 5: The number of generated examples which exactly match the original set and the performance (Accuracy).\n0 10 20 30 40 50\nexact match amount\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8accuracy\ndev-em-acc\nFigure 6: Exact Match Amount vs. Accuracy for Spider on development set.ùëÖ2 = 0.88\nTask Davinci da\nvinci-001 davinci-002 davinci-003 GPT-3.5-T MoE GPT-J OPT Bloom LLaMA Alpaca Vicuna\nRTE ‚ñ° ‚ñ† ‚ñ†\n‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚ñ†\nWNLI ‚ñ° ‚ñ† ‚ñ† ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚ñ†\nCOPA ‚ñ° ‚óã ‚óã ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nSST-2 ‚ñ° ‚óã ‚ñ† ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nMRPC ‚ñ° ‚óã ‚óã ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nQNLI ‚ñ° ‚óã ‚ñ† ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nCB ‚ñ° ‚ñ† ‚ñ† ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nWiC ‚ñ° ‚óã ‚ñ† ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nBoolQ ‚ñ° ‚óã ‚óã ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nStrategyQA ‚ñ° ‚óã ‚óã ‚óã ‚óã ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã\n‚óã\nNewsMTSC-MT ‚ñ° ‚óã ‚óã ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã\n‚ñ†\nNewsMTSC-RW ‚ñ° ‚óã ‚óã ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã\n‚ñ†\nNLI4Wills ‚ñ° ‚óã ‚óã\n‚óã ‚óã ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nCREPE ‚ñ° ‚óã ‚óã ‚óã\n‚óã ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nFOMC ‚ñ° ‚óã ‚ñ† ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nNewsMet ‚ñ° ‚óã ‚óã ‚ñ† ‚ñ† ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚ñ° ‚óã ‚óã\nTable4:Taskexampleextractionresultsontasks,orderedbyreleaseyear.Alineseparatespre-trainingdatacollectiondatasets\n(top) and post- training data collection datasets (bottom) for each LLM.‚ñ† indicates the model can generate training examples\nfor the task. We indicate models with instruction tuning and those without using‚óã and ‚ñ°, respectively.‚óã indicates a model\nwith instruction tuning cannot generate task examples, while‚ñ° indicates a model without instruction tuning cannot generate\ntask examples. Models without instruction tuning cannot follow the instructions directing them to generate task examples.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18477\n7 Membership Inference\nTofurtherexaminetheeffectoftrainingdatacontamination,\nwe apply a Membership Inference Attack (Hu et al. 2022a),\nwhichchecksifmodelgeneratedcontentexactlymatchesthe\nexamples in the dataset. While this test is possible for gen-\neration tasks, it is not possible for classification tasks, since\ninputsmaybeinthetrainingdataofLLMs(andlikelyare,for\nmany datasets), but we do not know for certain if the inputs\nare also paired with the labels without looking at the train-\ningdata.Weuse Spider,asemanticparsingandtext-to-SQL\ngeneration task, (Yu et al. 2018) as our target for analysis.\nFig. 5a and Fig. 5b show how many generated examples\nfrom the sampled training set and full development set are\nexactlythesameoverversionsoftheGPT-3seriesandrecent\nopensourcedLLMs,respectively.Thedatabaseschemasare\nnot in the zero-shot prompts, so if the model can generate\nexactly the same table name or field name as found in the\ntraining or development data, there must be contamination.\nAs shown in Fig. 5, the number of exact matched generated\nexamples increases over time, which indicates the extent of\nthe task contamination onSpider is increasing.\nWe also compute the execution accuracy after adding the\nschema in the prompts, and plot it against the number of ex-\nact matched generations (Fig. 6). We find a strong positive\ncorrelation between the number of exact matched generated\nexamples and execution accuracy (ùëÖ= 0.88), strongly in-\ndicating increased contamination is related to increased per-\nformance. However, we still cannot determine the extent of\nthecontamination‚Äôseffectonperformanceimprovement.We\nleave this for future work.\n8 Take-Aways\nWe now share some takeaways which our experiments have\nbrought to light:\n‚Ä¢ Due to task contamination, closed-sourced models may\ndemonstrate inflated performance in zero-shot or few-\nshot evaluation, and are therefore not trustworthy base-\nlines in these settings, especially those including instruc-\ntion fine-tuning or reinforcement learning with human\nfeedback(RLHF).Theextentofthiscontaminationisstill\nunknown, and we therefore recommend caution.\n‚Ä¢ In our experiments, for tasks with no possibility of task\ncontamination, models rarely demonstrate statistically\nsignificantimprovementsovermajoritybaselines,inboth\nzero and few-shot settings.\n‚Ä¢ The observed increase over time of GPT-3 series mod-\nelsforzero-shotorfew-shotperformanceformanydown-\nstream tasks is likely due to task contamination.\n‚Ä¢ Inspectionfortaskcontaminationoftrainingdataevenfor\nopen-sourced LLMs can be difficult for several reasons.\nFirst,determiningmembershipisdifficultunlessthepro-\ncesseddatasetusedfortrainingtheLLMisreleased(e.g.,\nOPT and LLaMA did not release the data they used to\ntrain the model, but Alpaca and Vicuna did, so we can\nobtain more definite information). Second, we cannot al-\nwaysrelyonthemodeltoreproduceevidenceofcontam-\ninationevenifitexists.Andthird,formattingdifferences\n(suchasCSVandJSON)ofadatasetcomplicateanalysis.\n‚Ä¢ We encourage publicly releasing training datasets to al-\nlow for easier diagnosing of contamination issues.\n9 Related Work\nThe investigation into potential data contamination in large\nlanguagemodels(LLMs)hasrecentlybeengainingattention\nintheresearchcommunity.Brownetal.(2020),intheirwork\nwith GPT-3, presented an in-depth analysis of data contam-\nination. Although they acknowledged the presence of a bug\nthatledtodatacontaminationinmultipledatasets,theirposi-\ntion was that it did not affect the overall performance of the\nmodel. Intriguingly, they noted that contaminated datasets\noutperformedtheuncontaminatedoneswhich,inaway,con-\ntradictedtheiroriginalassertion.MagarandSchwartz(2022)\nextracted training data from GPT-2 and indicated poten-\ntial leaks of private data in the pre-trained language model.\nChang et al. (2023) discovered that OpenAI models were\nmemorizing substantial amounts of copyrighted materials,\nwhich increased concern over data contamination. Aiyappa\net al. (2023) highlighted the severity and scope of data con-\ntaminationproblemsforChatGPTevaluations.Highlighting\nthe need for strategic interventions to address these issues,\nJacovi et al. (2023) proposed several strategies for circum-\nventingtestingdatacontamination.Additionalworkhasfur-\nther looked into test data contamination (Sainz et al. 2023b;\nZhou et al. 2023; Golchin and Surdeanu 2023; Sainz et al.\n2023a; Deng et al. 2023; Oren et al. 2023; Li 2023).\nThe previous work listed above has investigated test data\ncontamination,buthasnotconsideredtaskcontaminationfor\nzero-shot or few-shot settings. Prior work has noticed our\nproposed task contamination problem for zero-shot or few-\nshot learning (Blevins, Gonen, and Zettlemoyer 2023; Bri-\nakou, Cherry, and Foster 2023), but did not systematically\nanalyze it. Our work seeks to add to the existing knowledge\nbyprovidinganexhaustiveevaluationoftaskcontamination\nfor few-shot or zero-shot learning scenarios.\n10 Conclusion and Future Work\nWe investigate task contamination for LLMs, and conduct a\nchronologicalanalysis,trainingdatainspection,trainingdata\nextraction, and a membership inference attack to analyze it.\nWe find evidence that some LLMs have seen task examples\nduring pre-training for a range of tasks, and are therefore no\nlongerzeroorfew-shotforthesetasks.Additionally,wefind\nthat for tasks without the possibility of task contamination,\nmodels rarely demonstrate statistically significant improve-\nments over simple majority baselines, in both zero and few-\nshot settings. We recommend additional research be con-\nducted on task contamination for zero and few-shot settings\nto reveal the extent and impact of the task contamination for\nlarge language models in these settings.\nAcknowledgements\nWe are grateful for valuable feedback from Nilay Patel on\nan earlier version of this draft. We are thankful for the com-\nputingresourcesprovidedbythePacificResearchPlatform‚Äôs\nNautiluscluster,supportedinpartbyNationalScienceFoun-\ndation (NSF) awards CNS-1730158, ACI-1540112, ACI-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18478\n1541349, OAC-1826967, OAC-2112167, CNS-2100237,\nCNS-2120019, the University of California Office of the\nPresident, and the University of California San Diego‚Äôs Cal-\nifornia Institute for Telecommunications and Information\nTechnology/Qualcomm Institute. Thanks to CENIC for the\n100Gbps networks.\nReferences\nAiyappa,R.;An,J.;Kwak,H.;andAhn,Y.-Y.2023. Canwe\ntrust the evaluation on ChatGPT? arXiv:2303.12767.\nArtetxe, M.; Bhosale, S.; Goyal, N.; Mihaylov, T.; Ott, M.;\nand Shleifer, S. 2022. Efficient Large Scale Language Mod-\neling with Mixtures of Experts. InProceedings of the 2022\nConference on Empirical Methods in Natural Language Pro-\ncessing.\nBlevins,T.;Gonen,H.;andZettlemoyer,L.2023. Prompting\nLanguage Models for Linguistic Structure. InProceedings\nof the 61st Annual Meeting of the Association for Computa-\ntional Linguistics.\nBriakou, E.; Cherry, C.; and Foster, G. 2023. Searching for\nNeedles in a Haystack: On the Role of Incidental Bilingual-\nisminPaLM‚ÄôsTranslationCapability. In Proceedings of the\n61st Annual Meeting of the Association for Computational\nLinguistics.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,\nJ.; Dhariwal, P.; Neelakantan, A.; and ... 2020. Language\nModels are Few-Shot Learners.CoRR, abs/2005.14165.\nChang, K. K.; Cramer, M.; Soni, S.; and Bamman, D.\n2023. Speak, Memory: An Archaeology of Books Known\nto ChatGPT/GPT-4. arXiv:2305.00118.\nChiang,W.-L.;Li,Z.;Lin,Z.;Sheng,Y.;Wu,Z.;Zhang,H.;\nZheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nCobbe, K.; Kosaraju, V.; Bavarian, M.; Hilton, J.; Nakano,\nR.; Hesse, C.; and Schulman, J. 2021. Training Verifiers to\nSolve Math Word Problems.CoRR, abs/2110.14168.\nDeng, C.; Zhao, Y.; Tang, X.; Gerstein, M.; and Cohan, A.\n2023. Investigating Data Contamination in Modern Bench-\nmarks for Large Language Models. arXiv:2311.09783.\nDixon, W. J.; and Mood, A. M. 1946. The Statistical\nSign Test.Journal of the American Statistical Association,\n41(236): 557‚Äì566.\nGolchin, S.; and Surdeanu, M. 2023. Time Travel in LLMs:\nTracing Data Contamination in Large Language Models.\narXiv:2308.08493.\nHu,H.;Salcic,Z.;Sun,L.;Dobbie,G.;Yu,P.S.;andZhang,\nX. 2022a. Membership inference attacks on machine learn-\ning: A survey.ACM Computing Surveys (CSUR), 54(11s):\n1‚Äì37.\nHu,Y.;Lee,C.-H.;Xie,T.;Yu,T.;Smith,N.A.;andOsten-\ndorf,M.2022b. In-ContextLearningforFew-ShotDialogue\nStateTracking. InFindings of the Association for Computa-\ntional Linguistics: EMNLP 2022.\nJacovi, A.; Caciularu, A.; Goldman, O.; and Goldberg, Y.\n2023. Stop Uploading Test Data in Plain Text: Practical\nStrategies for Mitigating Data Contamination by Evaluation\nBenchmarks. arXiv:2305.10160.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power\nofScaleforParameter-EfficientPromptTuning. In Proceed-\nings of the 2021 Conference on Empirical Methods in Natu-\nral Language Processing.\nLi, Y. 2023. Estimating Contamination via Perplexity:\nQuantifying Memorisation in Language Model Evaluation.\narXiv:2309.10677.\nMagar,I.;andSchwartz,R.2022.DataContamination:From\nMemorization to Exploitation. InProceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics.\nMann,H.B.;andWhitney,D.R.1947. OnaTestofWhether\none of Two Random Variables is Stochastically Larger than\ntheOther. The Annals of Mathematical Statistics,18(1):50‚Äì\n60.\nOpenAI. 2023. OpenAI Models. https://platform.openai.\ncom/docs/models/.\nOren, Y.; Meister, N.; Chatterji, N.; Ladhak, F.; and\nHashimoto, T. B. 2023. Proving Test Set Contamination in\nBlack Box Language Models. arXiv:2310.17623.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; and Wain-\nwright..., C. 2022. Training language models to follow in-\nstructions with human feedback. In Oh, A. H.; Agarwal, A.;\nBelgrave, D.; and Cho, K., eds.,Advances in Neural Infor-\nmation Processing Systems.\nPoesia,G.;Polozov,A.;Le,V.;Tiwari,A.;Soares,G.;Meek,\nC.;andGulwani,S.2022.Synchromesh:ReliableCodeGen-\nerationfromPre-trainedLanguageModels. In International\nConference on Learning Representations.\nQin,G.;andEisner,J.2021.LearningHowtoAsk:Querying\nLMs with Mixtures of Soft Prompts. InProceedings of the\n2021 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies.\nSainz, O.; Campos, J.; Garc√≠a-Ferrero, I.; Etxaniz, J.; de La-\ncalle,O.L.;andAgirre,E.2023a.NLPEvaluationintrouble:\nOn the Need to Measure LLM Data Contamination for each\nBenchmark. In Bouamor, H.; Pino, J.; and Bali, K., eds.,\nFindings of the Association for Computational Linguistics:\nEMNLP 2023.\nSainz,O.;Campos,J.A.;Garc√≠a-Ferrero,I.;Etxaniz,J.;and\nAgirr, E. 2023b. Did ChatGPT cheat on your test? https:\n//hitz-zentroa.github.io/lm-contamination/blog/.\nScao,T.L.;Fan,A.;Akiki,C.;Pavlick,E.;Iliƒá,S.;Hesslow,\nD.; Castagn√©, R.; Luccioni, A. S.; Yvon, F.; Gall√©, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model.arXiv preprint arXiv:2211.05100.\nSchick, T.; Dwivedi-Yu, J.; Dess√¨, R.; Raileanu, R.; Lomeli,\nM.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.\nToolformer: Language Models Can Teach Themselves to\nUse Tools.\nSchick, T.; and Sch√ºtze, H. 2021a. Exploiting Cloze-\nQuestionsforFew-ShotTextClassificationandNaturalLan-\nguage Inference. InProceedings of the 16th Conference of\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18479\nthe European Chapter of the Association for Computational\nLinguistics: Main Volume.\nSchick, T.; and Sch√ºtze, H. 2021b. Few-Shot Text Genera-\ntion with Natural Language Instructions. InProceedings of\nthe 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing.\nShah, A.; Paturi, S.; and Chava, S. 2023. Trillion Dollar\nWords: A New Financial Dataset, Task & Market Analysis.\nInProceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics.\nSrivastava,A.;Rastogi,A.;Rao,A.;Shoeb,A.A.M.;Abid,\nA.; and Fisch... 2022. Beyond the Imitation Game: Quanti-\nfying and extrapolating the capabilities of language models.\nStudent. 1908. The probable error of a mean.Biometrika,\n1‚Äì25.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stan-\nfordAlpaca:AnInstruction-followingLLaMAmodel. https:\n//github.com/tatsu-lab/stanford_alpaca.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi√®re, B.; Goyal, N.; Hambro, E.;\nAzhar,F.; Rodriguez,A.;Joulin,A.; Grave,E.;andLample,\nG.2023. LLaMA:OpenandEfficientFoundationLanguage\nModels. arXiv:2302.13971.\nWang, B.; Deng, X.; and Sun, H. 2022. Iteratively Prompt\nPre-trainedLanguageModelsforChainofThought. In Pro-\nceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing.\nWang,B.;andKomatsuzaki,A.2021.GPT-J-6B:A6Billion\nParameter Autoregressive Language Model. https://github.\ncom/kingoflolz/mesh-transformer-jax.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.;\nNarang, S.; Chowdhery, A.; and Zhou, D. 2023. Self-\nConsistency Improves Chain of Thought Reasoning in Lan-\nguageModels. InThe Eleventh International Conference on\nLearning Representations.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;\nLe, Q.; and Zhou, D. 2022. Chain of thought prompting\nelicits reasoning in large language models.arXiv preprint\narXiv:2201.11903.\nYang,R.;Song,L.;Li,Y.;Zhao,S.;Ge,Y.;Li,X.;andShan,\nY. 2023. GPT4Tools: Teaching Large Language Model to\nUse Tools via Self-instruction. arXiv:2305.18752.\nYu, T.; Zhang, R.; Yang, K.; Yasunaga, M.; Wang, D.; Li,\nZ.; Ma, J.; Li, I.; Yao, Q.; Roman, S.; Zhang, Z.; and Radev,\nD.2018. Spider:ALarge-ScaleHuman-LabeledDatasetfor\nComplex and Cross-Domain Semantic Parsing and Text-to-\nSQL Task. InProceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; Mi-\nhaylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.;\nKoura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L.\n2022. OPT: Open Pre-trained Transformer Language Mod-\nels. arXiv:2205.01068.\nZhou, K.; Zhu, Y.; Chen, Z.; Chen, W.; Zhao, W. X.; Chen,\nX.; Lin, Y.; Wen, J.-R.; and Han, J. 2023. Don‚Äôt Make Your\nLLManEvaluationBenchmarkCheater. arXiv:2311.01964.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18480"
}