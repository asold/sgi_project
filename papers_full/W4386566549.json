{
  "title": "Exploiting Language Characteristics for Legal Domain-Specific Language Model Pretraining",
  "url": "https://openalex.org/W4386566549",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5114245479",
      "name": "Inderjeet Nair",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5016869559",
      "name": "Natwar Modani",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964236337",
    "https://openalex.org/W2953010755",
    "https://openalex.org/W2536769020",
    "https://openalex.org/W2132069633",
    "https://openalex.org/W2011015278",
    "https://openalex.org/W2971292190",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2921025868",
    "https://openalex.org/W3035668167",
    "https://openalex.org/W2635873040",
    "https://openalex.org/W3135190223",
    "https://openalex.org/W3103769805",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3032232719",
    "https://openalex.org/W565180412",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2781014390",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3198651167",
    "https://openalex.org/W3099950029"
  ],
  "abstract": "Pretraining large language models has resulted in tremendous performance improvement for many natural language processing (NLP) tasks. While for non-domain specific tasks, such models can be used directly, a common strategy to achieve better performance for specific domains involves pretraining these language models over domain specific data using objectives like Masked Language Modelling (MLM), Autoregressive Language Modelling, etc. While such pretraining addresses the change in vocabulary and style of language for the domain, it is otherwise a domain agnostic approach. In this work, we investigate the effect of incorporating pretraining objectives that explicitly tries to exploit the domain specific language characteristics in addition to such MLM based pretraining. Particularly, we examine two distinct characteristics associated with the legal domain and propose pretraining objectives modelling these characteristics. The proposed objectives target improvement of token-level feature representation, as well as aim to incorporate sentence level semantics. We demonstrate superiority in the performance of the models pretrained using our objectives against those trained using domain-agnostic objectives over several legal downstream tasks.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2516–2526\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nExploiting Language Characteristics for Legal Domain-Specific Language\nModel Pretraining\nInderjeet Nair and Natwar Modani\nAdobe Research, India\n{inair,nmodani}@adobe.com\nAbstract\nPretraining large language models has resulted\nin tremendous performance improvement for\nmany natural language processing (NLP) tasks.\nWhile for non-domain specific tasks, such mod-\nels can be used directly, a common strategy\nto achieve better performance for specific do-\nmains involves pretraining these language mod-\nels over domain specific data using objectives\nlike Masked Language Modelling (MLM), Au-\ntoregressive Language Modelling, etc. While\nsuch pretraining addresses the change in vocab-\nulary and style of language for the domain, it is\notherwise a domain agnostic approach. In this\nwork, we investigate the effect of incorporating\npretraining objectives that explicitly tries to ex-\nploit the domain specific language characteris-\ntics in addition to such MLM based pretraining.\nParticularly, we examine two distinct charac-\nteristics associated with the legal domain and\npropose pretraining objectives modelling these\ncharacteristics. The proposed objectives target\nimprovement of token-level feature represen-\ntation, as well as aim to incorporate sentence\nlevel semantics. We demonstrate superiority\nin the performance of the models pretrained\nusing our objectives against those trained using\ndomain-agnostic objectives over several legal\ndownstream tasks.\n1 Introduction\nPre-trained language models exhibit superior per-\nformance in several NLP tasks. Most of the promi-\nnent language models optimized over Masked lan-\nguage modelling with BERT-like (Devlin et al.,\n2019; Liu et al., 2019b; He et al., 2020) archi-\ntecture using large unlabelled corpus to achieve\nstate of the art results across many NLP tasks.\nWhile the sentence-level tasks like paraphrase de-\ntection (El Desouki and Gomaa, 2019) and senti-\nment analysis (Zhang et al., 2018) benchmarks the\ncapability of the model in effectively modeling the\nholistic representation of the sentence(s), the token-\nlevel tasks like named entity recognition (Li et al.,\n2020) attempted to assess the quality of contex-\ntualized token embeddings furnished by the mod-\nels. However, direct application of these models\nto domain-specific downstream tasks yields sub-\noptimal performance (Lee et al., 2020), perhaps\ndue to change in vocabulary and style of language.\nTo overcome this limitation, most commonly\nused approach involves pre-training a language\nmodel over domain specific corpora. For instance,\nPubMedBERT (Gu et al., 2021) and LEGAL-\nBERT (Chalkidis et al., 2020) achieved state-of-\nthe-art results for the biomedical and legal domain\nspecific tasks respectively by pre-training over do-\nmain specific corpus using a domain agnostic objec-\ntive. In this paper, we argue that the performance\nof these models can be further improved by em-\nploying pre-training objectives that exploit the lan-\nguage characteristics of the domain. We examine\ntwo distinct language characteristics of the legal\ndomain, propose pre-training objectives and finally\ndemonstrate superior performance over domain-\nspecific NLP tasks. Legal domain departs from\nthe generic corpora in terms of specialized vocab-\nulary, particularly formal syntax, domain-specific\nknowledge semantics, etc. to the extent that it can\nbe classified as a distinct \"sub-language\" (Tiersma,\n1999; Williams, 2007), which may be addressed by\nMLM based pretraining. In this paper, we study\nthe following additional domain characteristics and\nformulate closely aligned objectives in addition to\ndomain agnostic objectives like MLM:\n1. Templatized language: Legal documents\nconsist of clauses that are often derived from\nreusable text fragments with placeholders.\nThe placeholders are substituted with appro-\npriate replacements for specific documents.\nWe include a pre-training objective for this\ncharacteristic by optimizing the model to dis-\ntinguish the substitutions from the rest of the\ntext. Since, there is no labelled dataset that\nprovides such information, we also outline the\n2516\nprocess to approximately label the data-points\nwith placeholder spans.\n2. Availability of Soft Labels: Contracts and\nlegally enforceable documents can be seg-\nmented into clauses which are sections defin-\ning terms and conditions and important pro-\nvisions. Clauses can be categorized into dis-\ntinct types based on the aspect they address,\nwhich (the categorization) may sometimes be\navailable as a heading/title associated with the\nclauses. This categorization enables us to de-\nfine semantic relations between clauses. For\ninstance, clauses having same type are closer\nin meaning as compared to different typed\nclauses. This fact is instrumental in formulat-\ning an objective to obtain semantically aware\nholistic representation.\nWe leverage these two characteristics to design a\npre-training strategy, and experimentally show that\na language model pretrained using our strategy out-\nperforms domain-specific language model which is\ntrained only on domain-agnostic objectives, such\nas Masked Language Modelling.\nThe rest of the paper is organized as follows.\nIn Section 2, we discuss some prominent frame-\nworks that provisions domain specific pre-trained\nmodels and survey important works in the legal\nAI. In Section 3, we elucidate the details for the\naforementioned legal domain characteristics and\ndescribe the objective formulation and dataset cura-\ntion strategy. In Section 4 discusses the training. In\nSection 5, we briefly describe the baseline models\nused to compare the performance with our pre-\ntrained model for several legal domain tasks, and\ndiscuss the results. Finally, in Section 6, we con-\nclude by explaining the implications of our work\nand discuss its natural extensions.\n2 Related Works\n2.1 Prominent domain adaptation pretraining\napproaches:\nPretrained language models trained over non-\ndomain specific data such as transformers (Vaswani\net al., 2017), BERT (Devlin et al., 2019) and its\nvariants (Liu et al., 2019b; He et al., 2020) has\nresulted in state-of-the-arts results for several non-\ndomain specific natural language processing down-\nstream tasks. Owing to their success, a prominent\napproach to achieve superior results in domain-\nspecific NLP tasks involves training these models\nover domain-specific corpora. For instance, to im-\nprove the performance of the models in biomedical\ndownstream tasks, BIOBERT (Lee et al., 2020),\nClinical BERT (Alsentzer et al., 2019), Clinical\nBIOBERT (Alsentzer et al., 2019) and PubMed-\nBERT (Gu et al., 2021) were pretrained over spe-\nciality corpora closely associated with the biomed-\nical domain using the MLM objective. Recently,\n(Chalkidis et al., 2020) proposed LEGAL-BERT,\na language model pretrained using MLM over do-\nmain specific corpora, to achieve state-of-the-art\nperformance for several legal downstream tasks.\nMost of these methods focus on choosing appropri-\nate corpora for MLM pretraining and the selection\nof optimal hyperparameters in contrast to the ap-\nproach taken in this work. Here, we propose a\nnew direction to adapt a pretrained language model\nby utilizing language characteristics. In particu-\nlar, by studying the language characteristics of the\nlegal domain, we propose pretraining objectives\nthat explicitly tries to learn these characteristics.\nWhile there are other approaches that adapts the\nlanguage model to domain-specific tasks (Rietzler\net al., 2020; Han and Eisenstein, 2019; Gururan-\ngan et al., 2020), our work mainly tries to address\nthe problem of pretraining a language model for a\nparticular domain.\n2.2 Legal Artificial Intelligence (AI)\nLegal AI refers to the application of AI/NLP\ntechniques to solve several tasks in the legal do-\nmain (Zhong et al., 2020). Due to the distinct lan-\nguage characteristics of the legal domain, many le-\ngal domain-specific tasks requires the expertise of\nlegal practitioners for solving them. Furthermore,\nthe complexity of the associated tasks requires a\nsignificant time commitment even for experienced\nlegal professionals. Thus, this motivated the devel-\nopment of legal AI to reduce the tedium in under-\nstanding and solving these legal tasks.\nIn the legal AI, task-specific methods and\ndatasets were proposed for the following tasks:\nLegal Judgement Prediction (Aletras et al.,\n2016), Legal Entity Recognition and Classifica-\ntion (Cardellino et al., 2017), Legal Question An-\nswering (Kim and Goebel, 2017), Automated Legal\nReview (Hendrycks et al., 2021), Legal Text Classi-\nfication (Chalkidis et al., 2021), etc. Instead on im-\nproving task-specific solution approaches, our ob-\njective is to make improvements for several down-\nstream tasks. The objective of this work in very\n2517\nsimilar to that of (Chalkidis et al., 2020), however,\nour solution approach is very different.\n3 Domain Specific Objectives\nWe now describe the legal domain characteristics\nwhich we will use for formulating the objectives.\nFor each of the two objectives, we also describe the\nassociated dataset used for training. We get differ-\nent pretrained language model variants by incorpo-\nrating various subsets of the following objectives\nwhile pretraining.\nThe process of coming up with the right set of\ndomain specific language characteristics requires\nsignificant exposure to the domain. The authors\nhave been investigating several legal domain nat-\nural language processing tasks, and have been in-\nterviewing several practitioners for an extended\nperiod of time. The insights are a result of reading\nmany legal domain documents and the interactions\nwith domain experts. For one to extend our ideas in\nother domains, we expect them to require similar\nlong exposure to the domain in question and op-\nportunities to interact with domain experts. While\nwe believe that the two characteristics identified in\nthis work are not unique only to the legal domain,\none will need to carefully evaluate whether the\nsame characteristics apply to their chosen domain\nas well.\n3.1 Legal Domain as a Templating Language\nContracts include clauses which often use a stan-\ndardized language with some placeholders which\nare substituted with appropriate values (e.g., names,\ndates, amounts, locations, etc) for specific contracts\n(Figure 1). These standardized fragments with\nplaceholders are referred to as templates (Niemeyer\nand Knudsen, 2005) in software engineering par-\nlance. We refer to the tokens in the template-\ngenerated clauses that remain common across con-\ntract documents as static tokens and the values\nfilled into the placeholders as dynamic tokens.\nWe propose a pre-training objective that aims\nto detect the dynamic tokens/spans from text frag-\nments in the legal documents. Using this objective,\nthe language model can generate holistic represen-\ntation for a text-fragment cognizant of the tokens\nforming the dynamic part and the tokens forming\nthe static part. This can also result in better contex-\ntualized token representation for the task of named-\nentity recognition or other entity level tasks.\nIn these Terms the following words shall have the following meanings:\"Goods\" means those goods, products and/or services to be supplied and delivered by Vendor to Purchaser as described in the relevant Order.\"Purchaser\" The person, company, firm, partnership or such other legal entity that places an order for Goods with Vendor and includes Purchaser's divisions, subsidiaries and affiliates.\"Vendor\" means Russel Metals Inc.and its divisions, subsidiaries and affiliates.\nIn these Terms the following words shall have the following meanings:\"Goods\" means those goods, products and/or services to be supplied and delivered by Vendor to Purchaser as described in the relevant Order.\"Purchaser\" The person, company, firm, partnership or such other legal entity that places an order for Goods with Vendor and includes Purchaser's divisions, subsidiaries and affiliates.\"Vendor\" means AJ Forsythand its divisions, subsidiaries and affiliates.\nFigure 1: Clauses generated from same template: The\nabove example is believed to be generated from a stan-\ndardized clause template with a placeholder in place of\nthe text in yellow highlight. Moreover the substituted\ntext is observed to have close correspondence with or-\nganization named-entity.\n3.1.1 Dataset\nOne of the challenges in utilizing this characteris-\ntic in the pre-training objective is the lack of any\nlabelled dataset with such kind of information. To\novercome this limitation, we propose a dataset cura-\ntion strategy that provides data points with dynamic\nspans. The corpus to be labelled was formed by\ncollecting all the clauses present in the LEDGAR\ndataset (Tuggener et al., 2020), which consists of\nover 700,000 provisions in contracts.\nThe data curation strategy mainly consists of\ntwo steps: a) Grouping clauses that have very\nhigh lexical similarity which are believed to be\ngenerated from a single underlying template, b)\nContrasting data points in a pairwise fashion for\nevery group to differentiate the dynamic part from\nthe static using google-diff-match-patch1. Figure\n3 illustrates the pipeline employed for annotating\nthe dataset. Note that, while the contrasting tokens\nbelong to the dynamic part of the underlying text\n(if the grouping was correct), there is inconclusive\nevidence for the rest of the tokens for considering\nthem as static (For instance, Fig 2). This is due\nto the fact that some values can coincidentally be\nsame for different instances of same clause, e.g.,\n1https://github.com/google/\ndiff-match-patch\n2518\nThis Employment Agreement (the “Agreement”) is made as of ${data.date}, by and between${data.organization} (the “Company”), and ${data.person} (“Executive”),subject to the terms and conditions defined in this Agreement.\nThis Employment Agreement (the “Agreement”) is made as of March 7, 2018, by and betweenRockwell Medical, Inc.(the “Company”), and Robert L. Chioini(“Executive”),subject to the terms and conditions defined in this Agreement.\nThis Employment Agreement (the “Agreement”) is made as of July 31, 2018, by and betweenRockwell Medical, Inc.(the “Company”), and Stuart Paul(“Executive”),subject to the terms and conditions defined in this Agreement.\nAssumed Template\nContrasting two text fragments to determine dynamic parts\nFigure 2: Limitation of the contrasting step: The two\ntext fragments below belong to the same cluster and are\nbelieved to be generated from the template shown in the\nleft. However, the process of contrasting annotates only\nsome of the dynamic parts (highlighted in yellow) and\nmisses out some (highlighted in red). Thus, the rest of\nthe text should not be regarded as static in its entirety.\nhiring date for two individuals can be the same, and\ntherefore would not be marked as dynamic token\nby this strategy.\n3.1.2 Objective Formulation\nAfter applying the labelling strategy explained in\nthe previous section, we obtain a token-wise la-\nbelled dataset L = {(Xi,Yi)}M\ni=1. A datapoint\nin Lis a tuple (Xi,Yi), where Xi represents a\ntext-fragment as a sequence of tokens it contains\n(Xi = [xik]|Xi|\nk=1) and Yi is the corresponding se-\nquence of binary labels assigned to each token\nin Xi in the same order ( Yi = [yik]|Yi|\nk=1 where\nyik ∈ {0,1}), i.e. yik = 1 implies that xik be-\nlongs to the dynamic part and yik = 0implies that\nthe corresponding token can belong to any part.\nGiven such labelled dataset, we wish to train\nthe language model Msuch that Mdyn(Xi,xik)\nprovides us with the likelihood of xik being dy-\nnamic. The subscript ‘dyn’ denotes the addition\nof task-specific overhead architecture for detecting\ndynamic spans. We cannot directly apply binary\ncross entropy objective over the token-level predic-\ntions as the negative labels in our case does not\nimply that the corresponding tokens are static. To\novercome this obstacle, we use the framework of\npositive-unlabelled (PU) (Peng et al., 2019) learn-\ning where all the tokens associated with a positive\nlabel are regarded as dynamic and rest, associated\nwith a negative label, are regarded as unlabelled.\nUnder this framework, all the positively labelled\ntokens are collected with their parent text-fragment\nto form the set Xp = {(Xi,xi)}np\ni=1 where xi rep-\nresents a positively labelled token present in the\ntext fragment Xi. This is also repeated for the neg-\natively labelled datapoints to form the unlabelled\nset Xu = {(Xi,xi)}nu\ni=1. PU learning optimizes\nthe model parameters for the detection of dynamic\nparts by minimizing the following objective:\nLPU (Mdyn,Xp,Xu) = 1\nnu\n∑\n(Xu,xu)∈Xu\nl(Mdyn(Xu,xu),0)\n+ πp\nnp\n∑\n(Xp,xp)∈Xp\n(l(Mdyn(Xp,xp),1) −l(Mdyn(Xp,xp),0))\n(1)\nwhere l is a positive-valued loss function that\npenalizes the distance between its arguments and\nπp ∈[0,1] is a hyperparameter. The above objec-\ntive is derived from the following two terms: a term\nthat incentivizes positively labeled instances to be\nclassified as dynamic and a term that penalizes the\nunlabeled instances based on the assumption that\nthe probability of being dynamic is equal to πp\nPeng et al.. This implicitly assumes that the posi-\ntive and unlabeled datapoints are sampled from the\nsame distribution and the probability of a positive\ndatapoint being labeled is independent of its input\nfeatures. In contrast to the binary cross entropy\nobjective, PU learning accounts for the possibility\nthat some of the elements of Xu can be dynamic.\n3.2 Soft Semantic Labels for Clauses\nThe legal essence of many contractual documents\nand agreements is formed by concatenating clauses\nwhich are crucial for defining terms and conditions\nand important provisions. These clauses can of-\nten be categorized which can be used to optimize\nthe model to provide semantic-aware representa-\ntion scheme, and sometimes, such categorization\nis available as a label/title with the clause text.\nFormally, we want to train the language model\nto learn a representation scheme that maps same\ncategory clauses from the data manifold onto metri-\ncally closer points in the mapped space. We believe\nthat by infusing the ability to generate semantic-\naware representation within model, the language\nmodel may offer better performance on sentence-\nlevel tasks.\n3.2.1 Dataset\nWe used the LEDGAR Corpus (Tuggener et al.,\n2020) which is a collection of labelled legal clauses\nand provisions. This corpus was crawled from the\ncontracts present in the website of U.S. Securi-\nties and Exchange Commission (SEC)2. While this\ndataset contains many clause instances with multi-\nple labels, we retain only those clauses from this\n2https://www.sec.gov/\n2519\nLegal CorpusDocuments segmented into text fragments\nSimilarity established between fragments using Minhash-LSH\nClustering them by finding connected componentsPairwise contrasting to get the dynamicpart\nFigure 3: Pipeline for dataset creation for dynamic part identification: The clauses extracted from the LEDGAR\ncorpus were originally obtained by segmenting legal documents into fragments. As clauses having fairly repetitive\nlexical structure are believed to be generated from the same template, the fragments are clustered using Minhash-\nLSH (Broder, 1997; Indyk et al., 1997), followed by finding the connected components. Finally, each pair in a\ncluster is contrasted to annotate what is dynamic among them.\ncorpus which are associated with a single label\n(roughly 83% of the dataset).\n3.2.2 Objective Formulation\nGiven a language model M, Mrep denotes task\nspecific adaptation of the original language model\nto generate representation for a given sentence.\nWe formulate our requirement as a task of met-\nric learning where the goal is to learn a function\nMrep(.) :X→ Rd that maps semantically closer\ninput datapoints onto metrically closer points in\nRd. Here, Xdenotes the domain of input clauses\n/ provisions. Under the triplet-loss formulation,\nevery instance in the training dataset is a triplet\n(xa,xp,xn) where the model tries to make the dis-\ntance between the representations of xa (anchor)\nand xp (positive) smaller than that between xa and\nxn (negative) by atleast a margin m. Mathemati-\ncally, the loss function ltri is defined as follows:\nltri(xa,xp,xn) = [m+D(Mrep(xa),Mrep(xp))\n−D(Mrep(xa),Mrep(xn))]+ (2)\nIn the above equation, D(.,.) :Rd ×Rd →R\ndenotes a metric function measuring distances in\nthe mapped space.\n4 Training Details\nWe tune the parameters of our model using the algo-\nrithm employed for multi-task learning (Liu et al.,\n2019a). This framework optimizes the language\nmodel over multiple tasks. The language model is\nshared across different tasks by employing same\nencoder with shared parameters for all the task-\nspecific overhead architectures. In each iteration of\nmini-batch gradient descent optimization, a task is\nrandomly selected and corresponding task-specific\nmini-batch of data is sampled to apply single step\nof gradient descent using the task-specific objec-\ntive. We curated the dataset for MLM pretraining\nby extracting text fragments from the SEC corpus\nas curated by Chalkidis et al., utilizing newline\ncharacter (\\n) as the delimiter. In our ablation stud-\nies to understand the impact of various terms in the\npretraining objective on downstream performance,\nwe utilized a randomly selected subset of 40,000\ntext fragments to quickly assess the importance of\neach of the terms. Thereafter, we also evaluate\nthe performance of our model when a significantly\nlarger corpora is provided for MLM.\nIn this paper, the parameters of the shared lan-\nguage model are initialized using the weights of\nLEGAL-BERT (12-layer, 768-hidden, 12-heads,\n110M parameters)3, a domain-specific language\nmodel pre-trained using MLM. Thereafter, we in-\nvestigate the performance of the model variants\nlisted in Table 1 by comparing against LEGAL-\nBERT. We do not assess the performance of non-\ndomain specific models such as BERT (Devlin\net al., 2019) as the superiority of LEGAL-BERT\nover BERT was demonstrated in (Chalkidis et al.,\n2020) for some of the legal downstream tasks.\n3Distributed under CC BY-SA 4.0\n2520\nTable 1: Model variants to be assessed in various legal downstream tasks (on top of LEGAL-BERT). Legal Corpus\nfor MLM was collected by randomly sampling 40,000 text fragments from the SEC corpus.\nModel name Description of Additional Pre-training\nLB-PU Dynamic span recognition using PU\nLB-BC Binary classification to identify dynamic tokens\nLB-MLM MLM over legal corpus\nLB-PU-MLM Multi-task training for PU and MLM over legal corpus\nLB-TRI Representation learning task using triplet margin loss\nLB-TRI-MLM Multi-task training for triplet margin loss and MLM over legal corpus\nLB-PU-TRI Multi-task training for PU and triplet margin loss\nLB-PU-TRI-MLM Multi-task training for PU, triplet margin loss and MLM over legal corpus\nTable 2: Comparison between PU learning and Bi-\nnary Classification for token-level tasks in terms of\nF1-Scores (DPI: Dynamic Part Identification)\nModel name CUAD-NER DPI\nLEGAL-BERT 0.7040 0.7107\nLB-PU 0.7355 0.7507\nLB-BC 0.7221 0.6835\nWe used a 8 GPU A10G instance for training the\nmodels. While it took 32 hours to pretrain the\nmodel with best hyperparameter settings when\nonly 40,000 datapoints for MLM is used, the\nmodel instance pretrained over the total SEC cor-\npus (Chalkidis et al., 2020) consumed 800 hours.\nHuggingFace Transformers (Wolf et al., 2020) was\nused for both pretraining and experimental analy-\nsis.\n5 Results and Discussion\nWe begin this section by validating the choice of\nusing PU learning for dynamic part detection in-\nstead of binary token classification objective. In\nthe subsequent subsection, we describe various le-\ngal downstream tasks and their associated data to\nbe used in comparing the performance of the mod-\nels in Table 1. As our models are derived from\nLEGAL-BERT, it is used as a baseline in our empir-\nical analysis and we demonstrate the improvement\nof our model over it for several downstream tasks.\n5.1 PU learning Versus Binary classification\n5.1.1 Impact on downstream performance\nIn this subsection, we compare the performance\nof the model additional pretrained using PU learn-\ning (LB-PU) and binary classification (LB-BC) for\nnamed entity recognition (NER) and dynamic part\nidentification (DPI).\nWe use the NER adaptation of the Contract Un-\nderstanding Atticus Dataset (CUAD) (Hendrycks\net al., 2021). CUAD labels the contracting-party\nassociated with each contract. This is used for\nconstructing a NER dataset with contracting-party\nspan annotations for each datapoint. This dataset\nconsists of 16,636 training, 2,000 validation and a\n10,000 testing samples.\nAs the dataset curated for pretraining the lan-\nguage model for dynamic part identification was\napproximately labeled, we manually annotated few\ntext fragments by specifying the dynamic spans\nusing the definition in section 3.1. This manual\nannotation furnished 132 training instances, 32 de-\nvelopment instances and 50 testing instances. The\nperformance was reported by computing the F1-\nScore between the inferred spans and the ground\ntruth dynamic spans.\nThe results shown in Table 2 justifies the uti-\nlization of PU learning objective. Our hypothesis\nthat training the model to identify dynamic spans\nwill improve its ability in recognizing named en-\ntities has been validated by the improvement in\nNER performance achieved through the use of the\nPU learning objective. This is further validated in\nthe subsequent section through an examination of\nthe feature representations generated by the model\ntrained with/without PU learning. For the subse-\nquent analysis, we disregard any models trained\nusing binary classification objective owing to the\nresults shown in the table. The decrease in the per-\nformance from LEGAL-BERT to LB-BC for NER\nand DPI stems from the fact that a subset of nega-\ntively labelled tokens in some instances are labelled\nas dynamic for other instances. This confuses the\nmodel in learning correct characteristics associated\nwith these tokens, resulting in poor token-level rep-\nresentation.\n2521\nTable 3: Performance for various legal domain task given in terms of F1-Scores for CUAD-NER and DPI tasks,\nmean of F1-Scores for MULTI-EURLEX tasks for Level 1, 2 and 3, and soft F1-Score for Contract-Discovery task\n(Averaged for 5 runs).\nModel name CUAD-NER DPI MULTI- Contract-\nEURLEX Discovery\nLEGAL-BERT 0.7040 0.7107 0.7535 0.4591\nLB-MLM 0.7344 0.7098 0.7525 0.4367\nLB-PU 0.7355 0.7507 0.7488 0.0394\nLB-PU-MLM 0.7427 0.7509 0.7451 0.1701\nLB-TRI 0.7325 0.7380 0.7566 0.4979\nLB-TRI-MLM 0.7462 0.7091 0.7567 0.5051\nLB-PU-TRI 0.7320 0.7454 0.7513 0.5032\nLB-PU-TRI-MLM 0.7479 0.7628 0.7574 0.5119\n(a) LEGAL-BERT (NMI: 0.1837)\n (b) LB-BC (NMI: 0.0021)\n (c) LB-PU (NMI: 0.3812)\nFigure 4: t-SNE projections of the contextualized embeddings obtained from different representation schemes.\nLB-PU visually performs the best in terms of segregating the named entities from the rest of the tokens.\n5.1.2 Better feature representation for\nextracting named entities\nWe provide a qualitative justification for PU learn-\ning leading to better representation for extracting\nnamed entities in this subsection. In this assess-\nment, we extract 30 text sentences from the CUAD-\nNER dataset that contain at least one named en-\ntity within it and compute the contextualized em-\nbeddings for the tokens in it using LEGAL-BERT,\nLB-BC and LB-PU. Thereafter, these embeddings\nare mapped to two dimensional manifold using t-\nSNE (Van der Maaten and Hinton, 2008) algorithm.\nNote that, we compute the embeddings using dif-\nferent representation schemes without fine-tuning\non CUAD-NER to understand the impact of our\ntoken-level objective for distinguishing named en-\ntities from the rest of the tokens.\nFrom Figure 4, we observe that the embeddings\nof the named entities and other tokens are not very\nwell separated for LEGAL-BERT and LB-BC. On\nthe other hand, LB-PU leads to much better segre-\ngation despite not being explicitly trained for the\ntask of named entity recognition. This can be at-\ntributed to the observation that the dynamic part\nof a legal text fragment corresponds to a named\nentity most of the times. Since, LB-PU is explicitly\npretrained for the task of dynamic part detection,\nit furnishes suitable representation scheme for seg-\nregating named entities. While LB-BC is trained\nfor this task, it yields suboptimal representation\nscheme as it does not consider the possibility that\nsome of the unlabelled tokens may be dynamic.\n5.2 Results in various downstream tasks\nApart from CUAD-NER and DPI introduced in\nsec 5.1.1, we consider following additional tasks to\ncompare the performance of different models:\n1. MULTI-EURLEX (Chalkidis et al., 2021):\nThis dataset is meant to assess the perfor-\nmance in the task of Large-Scale Multi-Label\nText Classification (LSMTC). The datapoints\nin this dataset are curated from European leg-\n2522\nislative documents (EUR-LEX) and the labels\na derived from EUROVOC, a set of4.3K Euro-\npean vocabulary labels. This dataset includes\na total of 65K datapoints with the train-test-\nvalidation split of 55K-5K-5K respectively\nand involves fine-grained categorization of the\nlabel-set into 8 levels based on their hierarchy.\nWe compute the performance of the model\nvariants for ’level 1’ (21 labels), ’level 2’ (127\nlabels) and ’level 3’ (567) (but report only the\nmean of these due to space constraint) as the\nother levels are not publicly available.\n2. Contract-Discovery (Borchmann et al.,\n2020): This dataset is used to measure the\nperformance of a model in semantic retrieval,\nwhere the task is to retrieve a span from a tar-\nget document given a few examples (1 to 5)\nof similar clauses. The dataset uses about 600\ntarget documents and is divided into 2 splits:\ndevelopment and test. Each of these splits con-\nsists of 5000 datapoints. The performance is\nevaluated by computing soft F1 metric (Gral-\ni´nski et al., 2019) on the character-level in-\nferred spans, which rewards proportionally to\nthe extent of overlap between predicted and\nground truth character spans. To solve this\nproblem, we use the unsupervised method pro-\nposed by the authors of this task (Borchmann\net al., 2020).\nWe can see from the Table 3 that the model pre-\ntrained using domain specific objectives achieves\nbetter performance than LEGAL-BERT for all\nthe tasks. The models pretrained using only PU\n(LB-PU and LB-PU-MLM) only improves the per-\nformance for token-level tasks like CUAD-NER\nand DPI and achieves poor performance for other\ntasks. As these models only involve objectives\nat the token level, they offer inferior representa-\ntions at the level of sentences / text-fragments as\ncompared to other models which explains the poor\nperformance in tasks like MULTI-EURLEX and\nContract-Discovery. A similar effect is also ob-\nserved for LB-MLM, where the model exhibits\nsuperior performance for some of the token-level\ntasks but exhibits poor performance for sentence\nlevel objective when compared against LEGAL-\nBERT as it does not involve any objective at the\nlevel of sentences. The models trained using\ntriplet objective only (LB-TRI and LB-TRI-MLM)\nachieves better performance than LEGAL-BERT\nfor all the tasks. This justifies the inclusion of the\nobjective for learning semantic-aware representa-\ntion scheme. We also observe that, inclusion of\nMLM for the model variants almost always im-\nproves the downstream performance. This indi-\ncates the usefulness of having domain-agnostic ob-\njective like MLM in the overall objective. The\nmodel pretrained using all the objectives (LB-\nPU-TRI-MLM) achieves best / competitive perfor-\nmance for most of the tasks. It is noteworthy that\neven though the objective of PU learning has no\ndirect relation to tasks such as Contract-discovery\nand MULTI-EURLEX, the inclusion of PU learn-\ning in combination with Triplet loss and MLM\nleads to further improvement in the model’s effec-\ntiveness in those tasks.\nThese results also emphasize the importance of\nMLM apart from the domain-specific objectives.\nHere, the pretraining over MLM was performed\nover a dataset with about 40,000 text fragments.\nWe believe that the performance of these models\ncan be significantly improved by including a suffi-\nciently larger dataset for MLM pretraining which\nis validated in the next subsection.\n5.3 Performance when the size of MLM\ncorpora is varied\nIn this section, we assess the performance of our\nmodel trained using the three objectives (PU + TRI\n+ MLM) when the number of datapoints in the\nMLM corpus is varied. While the experiment per-\nformed in the previous subsection comprised of\nonly 40,000 text fragments, this analysis assesses\nthe model performance when the number of text\nfragments is varied from 1% to 100% of the total\nSEC corpus (Chalkidis et al., 2020).\nThe results shown in Table 4 clearly demonstrate\nthat the downstream performance improves with\nthe number of datapoints in the MLM corpus. Note\nthat, the pretraining corpus for LEGAL-BERT\nalready comprises of the SEC corpus used in our\nanalysis. This fact also confirms the importance of\ninvolving the two objectives along with MLM for\ngetting improved performance.\n6 Conclusion\nIn this paper, we demonstrated a novel approach to\nenhance the performance of domain-specific lan-\nguage model across several specialty downstream\ntasks by exploiting the language characteristics.\nThe objectives presented in this paper may not be\napplicable to all domains, which is a limitation\n2523\nTable 4: Performance for various legal domain task given in terms of F1-Scores for CUAD-NER and DPI tasks,\nmean of F1-Scores for MULTI-EURLEX tasks for Level 1, 2 and 3, and soft F1-Score for Contract-Discovery task\nwhen the number of datapoints in the MLM corpus is varied.\nNumber of training Fraction of the CUAD-NER DPI MULTI- Contract-\ndatapoints for MLM overall SEC corpus EURLEX Discovery\n40,000 5.56 ×10−4 0.7479 0.7628 0.7574 0.5119\n720,000 0.01 0.7483 0.7651 0.7546 0.5210\n7,200,000 0.10 0.7518 0.7662 0.7547 0.5145\n18,000,000 0.25 0.7457 0.7636 0.7471 0.5158\n72,000,000 1.00 0.7523 0.7721 0.7577 0.5216\nof our work, but the idea of formulating objec-\ntives for learning domain-specific characteristics\ncan be applied to other specialty domains (biomed-\nical, programming languages, etc.). Future work\nmight involve studying other characteristics of the\nlegal domain and understanding their impact in\ndownstream performance. We justified the positive\nimpact of such pretraining across several down-\nstream tasks by conducting extensive quantitative\nanalysis.\nWe conclude this section by enumerating the\nnatural extensions of this work for future:\n1. In this work, we emphasized on two charac-\nteristics in the legal domain. However, the\nlegal domain consists of several other domain-\nspecific characteristics. For instance, the con-\ntent in a legal agreement can be structured\ninto different parts (preamble, recitals, list of\nclauses, etc) and the impact of involving a pre-\ntraining objective to infer the structure of a\nlegal document on several tasks is yet to be\nunderstood. Thus, one line of future work may\ninvolve exhaustive study of language charac-\nteristics and understanding their influence in\ndownstream tasks.\n2. In the future, we plan to study the applicability\nof the introduced characteristics in other do-\nmains, such as programming languages where\ntext fragments can be classified into categories\nlike function blocks, variable declaration, etc.\nand contain both static and dynamic elements\nthat can be templatized. This study may\nprovide a thorough evaluation of the cross-\ndomain applicability of these characteristics,\nincluding the assessment of their impact on\ndownstream performance and the ease of cu-\nrating relevant data. We would like to also\nmotivate the researchers in applying the prin-\nciple introduced in this paper for other do-\nmains (biomedical, finance, etc.). This neces-\nsitates careful investigation in order to extract\ndomain-specific characteristics, as well as a\nmechanism for training the language model to\nunderstand these characteristics.\n7 Limitations\nWe now discuss the limitations of our work. The\nfirst limitation (or requirement) is need for sig-\nnificant computational power. As we showed in\nSection 5.3 of our paper, when the corpus size\nfor MLM training is increased from 0.0556% to\n100% of the SEC corpus, while the performance\nimproved by about 1% on multiple tasks, the com-\nputational requirement went up from 32 hours (on\na 8 GPU A10G instance) to 800 hours.\nSecondly, we had built our model on top of a do-\nmain specific pre-trained language model (which\nhad used only MLM objective on a domain specific\ncorpus). In theory, since we do include MLM as\none of the objectives, we should be able to get com-\nparable performance with or without domain spe-\ncific pretrained language model. However, due to\nsignificant cost involved, we did not train a model\nstarting from general domain language model (e.g.,\nBERT or RoBERTa) to compare its performance\nagainst model built on top of domain specific pre-\ntrained language model. Therefore, we cannot\nmake a claim if our proposed method would re-\nsult in comparable performance improvement for\nthe domains where such pre-trained models are not\navailable.\nThird, our method relies on identifying the do-\nmain specific characteristics and building objec-\ntive functions suitable to exploit them. This re-\nquires building domain expertise and/or collaborat-\n2524\ning with domain experts. Since this process cannot\nbe automated, it requires additional cost and human\neffort. Also, good automated data curation strate-\ngies may or may not be feasible for other domain\nspecific characteristics, limiting using usefulness\nfor training large language models.\nFinally, we have only experimented with English\nlanguage corpus. While the data curation strategy\nwe used should be applicable in most other lan-\nguages also for legal domain, the static/dynamic to-\nken classification task particularly may depend on\ngrammatical rules for sentence construction, which\nmay not be similar in all languages.\nHowever, we believe that despite these limita-\ntions, our work points to possibility of improved\nperformance of language models by using domain\nspecific characteristics (beyond MLM based pre-\ntraining), which should open doors for more such\nexplorations and significant advances in the state\nof art.\nReferences\nNikolaos Aletras, Dimitrios Tsarapatsanis, Daniel\nPreo¸ tiuc-Pietro, and Vasileios Lampos. 2016. Pre-\ndicting judicial decisions of the european court of\nhuman rights: A natural language processing per-\nspective. PeerJ Computer Science, 2:e93.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nŁukasz Borchmann, Dawid Wisniewski, Andrzej\nGretkowski, Izabela Kosmala, Dawid Jurkiewicz,\nŁukasz Szałkiewicz, Gabriela Pałka, Karol Kacz-\nmarek, Agnieszka Kaliska, and Filip Grali´nski. 2020.\nContract discovery: Dataset and a few-shot semantic\nretrieval challenge with competitive baselines. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4254–4268, Online.\nAssociation for Computational Linguistics.\nAndrei Z Broder. 1997. On the resemblance and con-\ntainment of documents. In Proceedings. Compres-\nsion and Complexity of SEQUENCES 1997 (Cat. No.\n97TB100171), pages 21–29. IEEE.\nCristian Cardellino, Milagro Teruel, Laura Alonso Ale-\nmany, and Serena Villata. 2017. A low-cost, high-\ncoverage legal named entity recognizer, classifier and\nlinker. In Proceedings of the 16th edition of the In-\nternational Conference on Articial Intelligence and\nLaw, pages 9–18.\nIlias Chalkidis, Manos Fergadiotis, and Ion Androut-\nsopoulos. 2021. MultiEURLEX - a multi-lingual and\nmulti-label legal document classification dataset for\nzero-shot cross-lingual transfer. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6974–6996, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMohamed I El Desouki and Wael H Gomaa. 2019. Ex-\nploring the recent trends of paraphrase detection. In-\nternational Journal of Computer Applications, 975(S\n8887).\nFilip Grali´nski, Anna Wróblewska, Tomasz Stanisławek,\nKamil Grabowski, and Tomasz Górecki. 2019.\nGEval: Tool for debugging NLP datasets and mod-\nels. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 254–262, Florence, Italy.\nAssociation for Computational Linguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4238–4248, Hong Kong,\nChina. Association for Computational Linguistics.\n2525\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer\nBall. 2021. Cuad: An expert-annotated nlp dataset\nfor legal contract review. NeurIPS.\nPiotr Indyk, Rajeev Motwani, Prabhakar Raghavan, and\nSantosh Vempala. 1997. Locality-preserving hashing\nin multidimensional spaces. In Proceedings of the\ntwenty-ninth annual ACM symposium on Theory of\ncomputing, pages 618–625.\nMi-Young Kim and Randy Goebel. 2017. Two-step cas-\ncaded textual entailment for legal bar exam question\nanswering. In Proceedings of the 16th edition of the\nInternational Conference on Articial Intelligence and\nLaw, pages 283–290.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li.\n2020. A survey on deep learning for named entity\nrecognition. IEEE Transactions on Knowledge and\nData Engineering, 34(1):50–70.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4487–4496, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nPatrick Niemeyer and Jonathan Knudsen. 2005. Learn-\ning java. \" O’Reilly Media, Inc.\".\nMinlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, and\nXuan-Jing Huang. 2019. Distantly supervised named\nentity recognition using positive-unlabeled learning.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 2409–\n2419.\nAlexander Rietzler, Sebastian Stabinger, Paul Opitz,\nand Stefan Engl. 2020. Adapt or get left behind: Do-\nmain adaptation through BERT language model fine-\ntuning for aspect-target sentiment classification. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 4933–4941, Marseille,\nFrance. European Language Resources Association.\nPeter M Tiersma. 1999. Legal language. University of\nChicago Press.\nDon Tuggener, Pius von Däniken, Thomas Peetz, and\nMark Cieliebak. 2020. LEDGAR: A large-scale\nmulti-label corpus for text classification of legal pro-\nvisions in contracts. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference,\npages 1235–1241, Marseille, France. European Lan-\nguage Resources Association.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nChristopher Williams. 2007. Tradition and change in\nlegal English: Verbal constructions in prescriptive\ntexts, volume 20. Peter Lang.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nLei Zhang, Shuai Wang, and Bing Liu. 2018. Deep\nlearning for sentiment analysis: A survey. Wiley\nInterdisciplinary Reviews: Data Mining and Knowl-\nedge Discovery, 8(4):e1253.\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang\nZhang, Zhiyuan Liu, and Maosong Sun. 2020. How\ndoes NLP benefit legal system: A summary of legal\nartificial intelligence. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5218–5230, Online. Association\nfor Computational Linguistics.\n2526",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8477683663368225
    },
    {
      "name": "Vocabulary",
      "score": 0.6533273458480835
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6291831731796265
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6280891299247742
    },
    {
      "name": "Language model",
      "score": 0.6218598484992981
    },
    {
      "name": "Natural language processing",
      "score": 0.6158117055892944
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5357531309127808
    },
    {
      "name": "Sentence",
      "score": 0.5234212875366211
    },
    {
      "name": "Exploit",
      "score": 0.4708758592605591
    },
    {
      "name": "Language identification",
      "score": 0.464713454246521
    },
    {
      "name": "Representation (politics)",
      "score": 0.42980891466140747
    },
    {
      "name": "Natural language",
      "score": 0.42454177141189575
    },
    {
      "name": "Linguistics",
      "score": 0.11032578349113464
    },
    {
      "name": "Programming language",
      "score": 0.10436075925827026
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": []
}