{
  "title": "Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View- and Category-Aware Transformers",
  "url": "https://openalex.org/W4382467872",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2115054089",
      "name": "Chengliang Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1895555568",
      "name": "Jie Wen",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099268044",
      "name": "Xiaoling Luo",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096026659",
      "name": "Yong Xu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2115054089",
      "name": "Chengliang Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1895555568",
      "name": "Jie Wen",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099268044",
      "name": "Xiaoling Luo",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096026659",
      "name": "Yong Xu",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4293791262",
    "https://openalex.org/W4212873158",
    "https://openalex.org/W1666447063",
    "https://openalex.org/W6658049321",
    "https://openalex.org/W1981613567",
    "https://openalex.org/W3094527485",
    "https://openalex.org/W4313291134",
    "https://openalex.org/W4304084053",
    "https://openalex.org/W2938834426",
    "https://openalex.org/W6682859755",
    "https://openalex.org/W3107026593",
    "https://openalex.org/W6782336793",
    "https://openalex.org/W3165208855",
    "https://openalex.org/W3022507937",
    "https://openalex.org/W4327668982",
    "https://openalex.org/W6840292131",
    "https://openalex.org/W2296034168",
    "https://openalex.org/W2604114154",
    "https://openalex.org/W6765060606",
    "https://openalex.org/W3174384244",
    "https://openalex.org/W4317877444",
    "https://openalex.org/W3007788930",
    "https://openalex.org/W6752228462",
    "https://openalex.org/W6848590504",
    "https://openalex.org/W6841568594",
    "https://openalex.org/W2903938664",
    "https://openalex.org/W6656842176",
    "https://openalex.org/W6640561700",
    "https://openalex.org/W6755634998",
    "https://openalex.org/W2604675517",
    "https://openalex.org/W6744293681",
    "https://openalex.org/W2787932447",
    "https://openalex.org/W2277230653",
    "https://openalex.org/W4312925669",
    "https://openalex.org/W2605997098",
    "https://openalex.org/W3217784147",
    "https://openalex.org/W2897402090",
    "https://openalex.org/W3083529857",
    "https://openalex.org/W2955273087",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288064731",
    "https://openalex.org/W2808239701",
    "https://openalex.org/W1937059634",
    "https://openalex.org/W4313154187",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W3167456680",
    "https://openalex.org/W2095377654",
    "https://openalex.org/W4289535947",
    "https://openalex.org/W2756626360",
    "https://openalex.org/W2155803963",
    "https://openalex.org/W2141282920",
    "https://openalex.org/W3169047433",
    "https://openalex.org/W4382468418",
    "https://openalex.org/W2025335430",
    "https://openalex.org/W4304091711"
  ],
  "abstract": "As we all know, multi-view data is more expressive than single-view data and multi-label annotation enjoys richer supervision information than single-label, which makes multi-view multi-label learning widely applicable for various pattern recognition tasks. In this complex representation learning problem, three main challenges can be characterized as follows: i) How to learn consistent representations of samples across all views? ii) How to exploit and utilize category correlations of multi-label to guide inference? iii) How to avoid the negative impact resulting from the incompleteness of views or labels? To cope with these problems, we propose a general multi-view multi-label learning framework named label-guided masked view- and category-aware transformers in this paper. First, we design two transformer-style based modules for cross-view features aggregation and multi-label classification, respectively. The former aggregates information from different views in the process of extracting view-specific features, and the latter learns subcategory embedding to improve classification performance. Second, considering the imbalance of expressive power among views, an adaptively weighted view fusion module is proposed to obtain view-consistent embedding features. Third, we impose a label manifold constraint in sample-level representation learning to maximize the utilization of supervised information. Last but not least, all the modules are designed under the premise of incomplete views and labels, which makes our method adaptable to arbitrary multi-view and multi-label data. Extensive experiments on five datasets confirm that our method has clear advantages over other state-of-the-art methods.",
  "full_text": "Incomplete Multi-View Multi-Label Learning via Label-Guided Masked View-\nand Category-Aware Transformers\nChengliang Liu1, Jie Wen1\u0003, Xiaoling Luo1, Yong Xu1,2*\n1 Shenzhen Key Laboratory of Visual Object Detection and Recognition, Harbin Institute of Technology, Shenzhen, China\n2 Pengcheng Laboratory, Shenzhen, China.\nliucl1996@163.com, jiewen pr@126.com, xiaolingluoo@outlook.com, yongxu@ymail.com\nAbstract\nAs we all know, multi-view data is more expressive than\nsingle-view data and multi-label annotation enjoys richer su-\npervision information than single-label, which makes multi-\nview multi-label learning widely applicable for various pat-\ntern recognition tasks. In this complex representation learn-\ning problem, three main challenges can be characterized as\nfollows: i) How to learn consistent representations of sam-\nples across all views? ii) How to exploit and utilize cate-\ngory correlations of multi-label to guide inference? iii) How\nto avoid the negative impact resulting from the incomplete-\nness of views or labels? To cope with these problems, we\npropose a general multi-view multi-label learning framework\nnamed label-guided masked view- and category-aware trans-\nformers in this paper. First, we design two transformer-style\nbased modules for cross-view features aggregation and multi-\nlabel classiÔ¨Åcation, respectively. The former aggregates in-\nformation from different views in the process of extracting\nview-speciÔ¨Åc features, and the latter learns subcategory em-\nbedding to improve classiÔ¨Åcation performance. Second, con-\nsidering the imbalance of expressive power among views, an\nadaptively weighted view fusion module is proposed to obtain\nview-consistent embedding features. Third, we impose a label\nmanifold constraint in sample-level representation learning to\nmaximize the utilization of supervised information. Last but\nnot least, all the modules are designed under the premise of\nincomplete views and labels, which makes our method adapt-\nable to arbitrary multi-view and multi-label data. Extensive\nexperiments on Ô¨Åve datasets conÔ¨Årm that our method has\nclear advantages over other state-of-the-art methods.\nIntroduction\nSince the development of representation learning methods,\ndata analysis technology based on simple single-view data\nhas become more and more difÔ¨Åcult to meet diverse appli-\ncation requirements. In the past few years, data acquisition\ntechnology has Ô¨Çourished, and multi-view data from various\nmedia or with different styles are ubiquitous, providing more\npossibilities for comprehensively and accurately describing\nobservation targets. Simply put, multiple observations, ob-\ntained from different observation angles for the same thing,\ncould be regarded as multi-view data (Li and He 2020; Li,\n*Corresponding authors.\nCopyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nWan, and He 2021; Wang et al. 2022). For example, retinal\nimages captured at four different positions constitute a four-\nview retinal dataset (Luo et al. 2021; Wen et al. 2022; Luo\net al. 2023); the features extracted from the target image with\ndifferent feature extraction operators can also form a multi-\nview dataset (Liu et al. 2022; Hu, Lou, and Ye 2021). More\ntypically, multi-view or multi-modal datasets composed of\nmultimedia data such as text, pictures, and videos have been\nwidely used in many applications such as web page classi-\nÔ¨Åcation (Hu, Shi, and Ye 2020; Lu et al. 2019; Liu et al.\n2023). As a result, multi-view learning emerges as the times\nrequire, and a large number of methods based on subspace\nlearning, matrix factorization, and graph learning have been\nproposed (Zhan et al. 2017; Liu et al. 2017; Wen et al.\n2019). Most of these methods seek to obtain a consistent\nrepresentation of multiple views to characterize the essen-\ntial attributes of objects. On the other hand, since the core of\nmulti-view learning is representation learning, researchers\nusually combine it with downstream tasks to improve the ap-\nplication value and evaluate the learning effect (Chen, Wang,\nand Lai 2022). That is, multi-view learning can be divided\ninto clustering and classiÔ¨Åcation, according to whether su-\npervised information is available. Further, in the single-label\nclassiÔ¨Åcation task, a sample is only labeled with one cate-\ngory, which is obviously against the distribution of informa-\ntion in nature (Zhao et al. 2022). For example, a bird picture\nis likely to contain the category of ‚Äòsky‚Äô or ‚Äòtree‚Äô. There-\nfore, although multi-label classiÔ¨Åcation still faces more chal-\nlenges than single-label classiÔ¨Åcation, its broad application\nprospects are attracting a lot of research enthusiasm. Based\non this, a complex fusion task, i.e., multi-view multi-label\nclassiÔ¨Åcation (MvMlC ) is put on the agenda. Zhang et al.\nproposed a matrix factorization-basedMvMlC model, which\nattempts to enforce alignment of different views in the ker-\nnel space to exploit multi-view complementarity and explore\nmore latent information (Zhang et al. 2018). A Bernoulli\nmixture conditional model is proposed to model label de-\npendencies and employ a variational inference framework\nfor approximate posterior estimation (Sun and Zong 2020).\nResearchers have conducted extensive researches in the\nÔ¨Åeld of MvMlC, however these works assume that all views\nand labels are complete, which is often violated in practice.\nFor example, if a web page contains only text and images,\nthen the video view of the page is not available. To avoid\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n8816\nthe negative effects of missing views as much as possible,\nsome multi-view learning works try to mask unavailable\nviews or restore missing views (Wen et al. 2022; Xu et al.\n2018). Similarly, manual annotation is likely to miss some\ntags due to mistakes or cost, which inevitably weakens the\nsupervision information of multi-label. To solve the issue,\nsome works focusing on incomplete multi-label classiÔ¨Åca-\ntion have been developed in recent years (Zhu, Kwok, and\nZhou 2018; Huang et al. 2019). Although these methods de-\nsigned for incomplete multi-view or incomplete multi-label\nlearning have achieved surprising fruits, most of them are\nnot able to cope with the both incomplete cases simultane-\nously.\nTo this end, we propose a general MvMlC framework,\ntermed Label-guided Masked View- and Category-Aware\nTransformers (LMVCAT), which can handle the multi-view\ndata with arbitrary missing-view and missing-label. In addi-\ntion, unlike traditional methods to learn low-level represen-\ntations of samples, deep neural networks based LMVCAT\nis capable to extract high-level features of samples, which\nis of great beneÔ¨Åt for learning complex multi-category se-\nmantic labels. In recent years, transformer, designed for nat-\nural language processing, has shown its dominance in the\nimage and other Ô¨Åelds, which clearly demonstrates the ef-\nfectiveness of the self-attention mechanism (Vaswani et al.\n2017; Huang et al. 2022b,a). Inspired by this, our LMVCAT\nis also designed based on the transformer with self-attention\nmechanism which can provide a global receptive Ô¨Åeld for\neach view (Lanchantin et al. 2021). Overall, our model\nis composed of four parts: a masked view-aware encoder,\nan adaptively weighted multi-view fusion module, a label-\nguided sample-level graph constraint, and a subcategory-\naware multi-label classiÔ¨Åcation module. SpeciÔ¨Åcally, the\nfour parts of our LMVCAT are motivated by the follow-\ning points: 1) breaking the inter-view barriers to aggre-\ngate multi-view features can fully exploit the complemen-\ntary information of multiple views, so we design a view-\naware module to integrate all views while extracting sample-\nspeciÔ¨Åc high-level representations; 2) different discrimina-\ntion ability of views means the different contributions to\nthe Ô¨Ånal classiÔ¨Åcation, so an adaptively weighted strategy\nthat automatically learns the weight factor of each view is\nneeded (Chen et al. 2022); 3) compared with single-label\ndata, multi-label data naturally enjoys richer label similar-\nity information, so it is of great signiÔ¨Åcance to utilize label\nsmoothness (manifold) to guide sample encoding; 4) it is\nwell known that multi-label data is with non-negligible inter-\nclass correlations, thus we adopt a category-aware module\nthat learns correlations in the subcategory embedding space\nto collaboratively predict labels (Lanchantin et al. 2021).\nApart from those, it needs to be emphasized that we con-\nsider the possibility of missing labels and missing views in\nall components of our model. So it is no doubt that our LMV-\nCAT is a generalMvMlC framework that is comfortable with\nall kinds of multi-view multi-label data. Our contributions\nare summarized as follows:\n‚Ä¢ To the best of our knowledge, this is the Ô¨Årst\nTransformer-based incomplete multi-view multi-label\nlearning framework capable of handling both incomplete\nviews and labels. The proposed masked view-aware self-\nattention module could avoid the missing views‚Äô nega-\ntive effects to information interaction across views. Sim-\nilarly, our category-aware module is designed to mine\nlatent inter-class correlations in the subcategory embed-\nding space to improve the expressiveness.\n‚Ä¢ Label manifold is fully exploited. Although the multi-\nlabel information is fragmentary, we still try our best to\nconstruct an approximate similarity graph based on in-\ncomplete labels to guide the encoding process of sam-\nples, which further strengthens the discrimination power\nof high-level semantic features.\n‚Ä¢ Different views contribute different importance to the\nprediction, so we introduce an adaptively weighted fu-\nsion method to balance this importance instead of sim-\nply adding multiple views. SufÔ¨Åcient experimental re-\nsults conÔ¨Årm the effectiveness of our LMVCAT.\nPreliminaries\nFormulation\nIn this section, we deÔ¨Åne our main problem as follows:\nGiven input multi-view dataset D = fX;Yg, which con-\ntains nsamples. For i-th sample, Xiis composed ofmviews\nwith dimension dv, i.e., Xi =\n\b\nx(v)\ni 2 Rdv\n\tm\nv=1, or for\nv-th view, X(v) =\n\b\nx(v)\ni 2 Rdv\n\tn\ni=1. Yi 2 f0;1gc is a\nrow vector that denotes the label of i-th sample and c is\nthe number of categories. To describe the missing cases, we\nlet W 2f0;1gn\u0002m be the missing-view indicator matrix,\nwhere Wij = 1 represent j-th view of sample iis available,\notherwise Wij = 0. Similarly, we deÔ¨Åne G 2f0;1gn\u0002c as\nthe missing-label indicator matrix, whereGij = 1 represents\nj-th category of sample iis known, otherwise Gij = 0. All\nmissing information of feature X and label Y will be set as\n‚Äò0‚Äô in the data-preparation stage. And our goal of multi-view\nmulti-label learning is to train a model which can correctly\npredict multiple categories for each input sample.\nRelated Works\nA matrix completion based MvMlC method, termed as lr-\nMMC, which forces common subspace to be low rank to sat-\nisfy the assumption of matrix completion (Liu et al. 2015).\nHowever, lrMMC is incapable of adapting to missing-view\nor missing-label data. MVL-IV , an incomplete multi-view\nlearning method, attempts to exploit the connections be-\ntween views (Xu, Tao, and Xu 2015). As another incom-\nplete multi-view single-label learning model, iMSF cleverly\ndivides the incomplete multi-view classiÔ¨Åcation tasks into\nmultiple complete subtasks (Yuan et al. 2012). There is a\nlimitation to both methods that MVL-IV and iMSF only con-\nsider the incompleteness of views. On the contrary, MvEL,\na method aiming to capture the semantic context and the\nneighborhood consistency, could only handle the incomplete\nmulti-label case (Zhang et al. 2013). In recent years, there\nare few approaches to consider double missing cases except\niMvWL(Tan et al. 2018) and NAIML(Li and Chen 2021).\niMvWL simultaneously maps multi-view features and muti-\nlabel information to a common subspace. And a correlation\n8817\nùë£ùëñùëíùë§\t1\n‚àóùëé!\" pred0.90.70.1\n11?1001001000011?11?10001011?1\nùêø!\"\nùêø#\"\nùêø$\"\nVFormer\nCFormer\nLabel-guided graph constraint‚Ä¶\nAdaptively weighted fusion\nlabelùë£ùëñùëíùë§\t2\nùë£ùëñùëíùë§\t3ùë£ùëñùëíùë§\tùëö\nùë£ùëñùëíùë§\t1\n‚Ä¶ùë£ùëñùëíùë§\t2\nùë£ùëñùëíùë§\t3\nùë£ùëñùëíùë§\tùëöX\nùë£ùëñùëíùë§\t1\n‚Ä¶ùë£ùëñùëíùë§\t2\nùë£ùëñùëíùë§\t3\nùë£ùëñùëíùë§\tùëö\nZ‚ààùëÖ!√ó#√ó$!\nùëìùë¢ùë†ùëñùëúùëõ\n‚Ä¶ùëêùëôùëéùë†ùë†\t1\nùëêùëôùëéùë†ùë†\t2ùëêùëôùëéùë†ùë†\tùëê\n‚àóùëé#\"\nW W\nG\nG\nW\nW: Incomplete view indicator is imposed to corresponding module: Incomplete label indicator is imposed to corresponding moduleG\nG\n‚àóùëé$\"\n‚àóùëé%\"\nMLPMLPMLPMLP\nLinearLinearLinearLinear\nZ%\nFigure 1: The main structure of our LMVCAT. The MLP layer is composed of two linear layers, GELU activation function,\nand two dropout layers. And class 1 to class c denote the ccategory tokens. Incomplete view indicator and incomplete label\nindicator are imported to corresponding modules and losses.\nmatrix is introduced to enhance the projection from label\nspace to embedding subspace. NAIML copes with the dou-\nble incomplete problem based on the low-rank hypothesis of\nsub-label matrix, which also implicitly exploits the sub-class\ncorrelations. In addition, because the iMvWL and NAIML\nare well suited to datasets with both view and label incom-\npleteness, we focus on evaluating these two methods in our\ncomparison experiments.\nMethod\nIn this section, we elaborate on each component of\nour model, including view-aware transformer, label-guided\ngraph constraint, multi-view adaptively weighted fusion,\nand category-aware transformer.\nVFormer\nAs we all know, the key to success of multi-view learning\nis the complementarity among views, which are not avail-\nable in single-view data. To this end, we design a view-\naware transformer encoder ( VFormer for short) to aggre-\ngate complementary information during the cross-view in-\nteraction. Before this, it needs to be considered that dif-\nferent views may have different feature dimensions, so for\nconvenience, we map the original multi-view data to the\nembedding space with the same dimension by a stack of\nMultilayer Perceptrons (MLP)\n\b\n\b(v)\n\u0012\n\tm\nv=1, which can also\nbe seen as a preliminary feature extraction operation, i.e.,\n\b(v)\n\u0012 : X(v) 2Rn\u0002dv ! bX\n(v)\n2Rn\u0002de. These embed-\nding features of multiple views are stacked into a feature\nsequence bX 2 Rn\u0002m\u0002de, like a sentence embedding ten-\nsor composed of some word embedding vectors, as the in-\nput tensor of the VFormer. The structure of our VFormer\nis similar to that of the encoder in the typical transformer\n(Vaswani et al. 2017), and the main difference is that we in-\ntroduce a missing view indicator matrix in the calculation\nof multi-head self-attention scores to prevent missing views\nfrom participating in the calculation of attention scores. Our\nmasked multi-head self-attention encoder is characterized as\nfollows:\nFor each sample embedding bXi 2 Rm\u0002de, we project\nit linearly to get its queries, keys, and values by h groups\nprojective matrices, i.e., fWqt;Wkt;Wvtgh\nt=1 with head\nnumber h. To mask the embedding features according to\nmissing-view distribution, we deÔ¨Åne a Ô¨Åll function to Ô¨Åll\nzero value with\u00001e9 and construct a mask matrix of sample\ni: Mi = wT\ni wi 2Rm\u0002m, where wi is i-th row vector of W.\nThen we calculate view correlations At and output Ht:\nAt = softmax\n\u0010\nfill\n\u0000\u0000bXiWq\nt\n\u0001\u0000bXiWk\nt\n\u0001T\nMi\n\u0001\n=\np\ndh\n\u0011\nHt = At\n\u0000bXiWv\nt\n\u0001\n;\n(1)\nwhere dh = de=h, Wqt;Wkt;Wvt 2Rde\u0002dh, and Ht 2\nRm\u0002dh. The masked self-attention mechanism is shown in\nthe Fig. 2, it is worth noting that we Ô¨Åll the attention values\nwith \u00001e9 so that the softmax will ignore the corresponding\n8818\nmissing views when calculating the attention scores. And\nthen, we concatenate all outputs to produce a new embed-\nding feature w.r.t sample i: H = Concat(H1;:::; Ht) 2\nRm\u0002de. To sum up, all views of the same sample will ex-\nchange information during the parallel encoding process in\nour VFormer. As a result, the private information of each\nview is shared to some extent by other views. Other oper-\nations on VFormer are shown in the Fig. 3(a). Finally, our\nVFormer can be formulated as: \u0000 : bX !Z 2Rn\u0002m\u0002de.\nLabel-Guided Graph Constraint\nUnlike single-label samples maintaining an invariant label\ndistance (\np\n2 by Euclidean distance), multi-label samples\nnaturally hold uneven distribution in label space, which of-\nfers the possibility to guide the high-level representation\nlearning based on label similarity. Simply put, the label man-\nifold assumption means that if two samples are similar, their\nlabels should also be similar (Wu et al. 2014). In turn, we\nutilize label similarity to construct a sample-level graph con-\nstraint to guide the representation learning. As shown in Fig.\n3(b), the similarity vector from sample 1 to other samples is\ncalculated to guide the embedding encoding of sample 1.\nBefore that, we deÔ¨Åne the label similarity matrix T:\nT =\n\u0000\nY \fG\n\u0001\u0000\nY \fG\n\u0001T\n:=\n\u0000\nGGT \u0001\n; (2)\nwhere ‚Äò\f‚Äô is Hadamard product and ‚Äò :=‚Äô denotes the divi-\nsion of corresponding elements. Notably, T 2[0;1]n\u0002n is\nnormalized by GGT, where (GGT)ij is referring to the num-\nber of known categories in both Yi and Yj. In other words,\nfor two samples, the larger the number of common positive\ntags, the more similar they are. In addition, the similarity of\ntwo embedding features is calculated in cosine space, i.e.,\nfor sample iand j, their similarity in view vis deÔ¨Åned as:\nS(v)\nij = (\n\nz(v)\ni \u0001z(v)\nj\n\u000b\n\r\rz(v)\ni\n\r\r\r\rz(v)\nj\n\r\r+ 1)=2; (3)\nwhere h\u0001idenotes the vector dot product operation. z(v)\ni and\nz(v)\nj are two embedding features from viewvthat are output\nby VFormer. In order to learn sample neighbor relationships\nfrom labels, we let label similarity be the target and feature\nsimilarity be the learning object. The graph constraint loss\nLgc is formulated as:\nLgc = \u0000 1\n2mN\nmX\nv=1\nnX\ni=1\nnX\nj6 =i\n\u0000\nTij log S(v)\nij\n+\n\u0000\n1 \u0000Tij\n\u0001\nlog\n\u0000\n1 \u0000S(v)\nij\n\u0001\u0001\n(WivWjv );\n(4)\nwhere N = P\ni;jWivWjv denotes the number of available\nsample pairs in view v. We introduce WivWjv to mask the\ncalculation of loss w.r.tmissing views.\nAdaptively Weighted Fusion\nAs mentioned above, VFormer encodes an embedding fea-\nture for each view of each sample ( i.e., Z 2 Rn\u0002m\u0002de).\nIn order to obtain a consistent common representation to\nuniquely describe the corresponding sample, we propose an\nX\"!(#)\nQ#(#)Q%(#)\nH#(#)\nH(#)C\nK#(#)K%(#)V#(#)V%(#)\nX\"! X\"!(%)\nQ#(%)Q%(%)K#(%)K%(%)V#(%)V%(%)\nX\"!(&)\nQ#(&)Q%(&)K#(&)K%(&)V#(&)V%(&)\nH%(#) H#(%)\nH(%)CH%(%) H#(&)\nH(&)CH%(&)\nH\nC:Concatenate:Dot-Product:Scalar-multiplication:Missing view:Invalid data\nFigure 2: The masked multi-head self-attention mechanism\nof VFormer taking two heads as an example.\nAX\"! LLLLLL CA L NMZ!\nLLinearLayerCConcatASelf-AttentionMMLP\n1?11?110?100??1101011??01?1110\n112/3\n1/33/4\nSample12345Incomplete multi-label\n1 12345\n1\nSample similarity of 1\n2\n34 5\n2/3\n1/31 3/4\nCosine embedding space\n(a)\n(b)\nLayernormN\nFigure 3: (a) The brief construction of VFormer taking sam-\nple i as an example; (b) The simple schematic diagram of\nlabel-guided graph constraint.\nadaptively weighted fusion strategy to fuse multi-view em-\nbedding features before Ô¨Ånal classiÔ¨Åcation. The fusion fea-\nture \u0016zi of sample iis deÔ¨Åned as follows:\n\u0016zi =\nmX\nv=1\nea\r\nvz(v)\ni Wiv\nP\nv ea\r\nvWiv\n; (5)\nwhere av is a learnable scalar weight of v-th view and \r\nis a adjustment factor. Apparently simple as Eq. (5) seems,\nit serves two purposes: i) Distinct from other methods to\ntreat all views equally, we Ô¨Çexibly assign each view differ-\nent weighting coefÔ¨Åcients, which help to maintain or high-\nlight the differences of discriminative ability among views.\nii) Unusable views must be ignored in multi-view fusion to\navoid negative effects, so missing-view indicator matrix is\nintroduced in our fusion module. Stack all \u0016zi and we can get\noutput tensor \u0016Z 2Rn\u0002de for next classiÔ¨Åcation.\nCFormer and ClassiÔ¨Åer\nIn the real world, multiple categories of samples are not in-\ndependent of each other. How to leverage the multi-label\ncorrelations to make our model more discriminative is the\n8819\nmain concern in the subsection. Instead of learning a cate-\ngory correlation graph, similar to (Lanchantin et al. 2021),\nwe directly map each category to the feature space and\nleverage the self-attention mechanism to capture the cate-\ngory correlations. In more detail, c class tokens\n\b\nclsi 2\nRde\n\tc\ni=1 are randomly initialized before training and in-\nput to category-aware transformer (CFormerfor short) with\nfusion features of samples (i.e., the input of CFormer is\nf\u0016zi;cls1;:::;cls cgn\ni=1). Similar to the structure of VFormer,\nCFormer allows the fusion features and category tokens to\nshare information, which enjoys two major beneÔ¨Åts: On the\none hand, the view-fusion features aggregate all subcategory\ninformation according to similarities among them, which\nmakes the embedding features encoded by CFormer closer\nto their relevant class tokens. On the other hand, the in-\nformation interaction across categories implicitly promotes\nthe learning of category relevance. It should be pointed out\nthat we do not introduce the missing-label mask here be-\ncause mining the category correlation information requires\nthe participation of all class tokens. And for any sample i,\nthe output tensor of our CFormer is fbzi;cls1\ni;:::;cls c\nig 2\nR(c+1)\u0002de. We split the output into two parts, i.e., consen-\nsus representation fbzigfor the main classiÔ¨Åcation purpose\nand fcls1\ni;:::;cls c\nigfor the representation learning of class\ntokens.\nAs shown in Fig. 1, c+ 1linear classiÔ¨Åers f\tz;f\tigc\ni=1g\nare connected in parallel to the outputs of CFormer, where\nthe classiÔ¨Åer \tz predicts the Ô¨Ånal result Pz 2 [0;1]n\u0002c.\nWhile each other classiÔ¨Åer \ti only predicts the value of\nits own category so we can get another prediction Pc 2\n[0;1]n\u0002c from all class tokens. Finally, we deÔ¨Åne a masked\nbinary cross-entropy function as our multi-label classiÔ¨Åca-\ntion loss:\nLmbce = \u00001\nC\nnX\ni=1\ncX\nj=1\n\u0010\nYij log(Pij)\n+ (1 \u0000Yij) log(1\u0000Pij)\n\u0011\nGij;\n(6)\nwhere C = P\ni;jGi;j denotes the number of available labels\nand P is the prediction. G is introduced to prevent unknown\nlabels from participating in the calculation of loss. As a re-\nsult, we can obtain a main classiÔ¨Åcation loss Lmc and an\nancillary classiÔ¨Åcation loss Lac according to Lmbce for Pz\nand Pc, respectively. Overall, our total loss function is :\nL = Lmc + \u000bLgc + \fLac; (7)\nwhere \u000band \fare penalty coefÔ¨Åcients.\nExperiments\nExperimental Setting\nDatasets. In the experiments, we use Ô¨Åve multi-view multi-\nlabel datasets as same as (Tan et al. 2018; Guillaumin, Ver-\nbeek, and Schmid 2010; Li and Chen 2021): (1) corel5k\n(Duygulu et al. 2002): The corel5k dataset contains 5000\nimage samples with 260 classes and we use 4999 sam-\nples in the experiments. (2) Pascal07 (Everingham et al.\n2009): The popular Pascal07 dataset has 9963 images cov-\nering 20 types of tags. (3) Espgame (V on Ahn and Dabbish\n2004): 20770 samples and 268 categories of Espgame are\nused in our experiments. It is no doubt that it is a large-\nscale database. (4) IAPRTC12 (Grubinger et al. 2006): As\na benchmark database, IAPRTC12 is composed of 20,000\nhigh-quality natural images and we use 19627 images and\n291 tags in the experiments. (5) MirÔ¨Çickr (Huiskes and Lew\n2008): The MirÔ¨Çickr is collected from the social photogra-\nphy site Flickr, which is made up by 25000 images. 38 types\nof annotations are selected in the experiments. For the above\nÔ¨Åve multi-view multi-label datasets, each dataset contains\nsix views, i.e., GIST, HSV , HUE, SIFT, RGB, and LAB.\nData processing. To evaluate the performance of var-\nious multi-view multi-label learning methods on incom-\nplete datasets, following (Tan et al. 2018), we treat the Ô¨Åve\ndatasets as follows to simulate the incomplete case: (1) For\neach view, we randomly remove50% of samples while guar-\nanteeing that at least one view is available for each sample.\n(2) For each category, we randomly select 50% of positive\ntags and negative tags as unknown labels.\nComparison. We select eight top methods to compare to\nour LMVCAT. Six methods, i.e., lrMMC, MVL-IV , MvEL,\niMSF, iMvWL, and NAIML, are introduced in section . In\naddition, we add two advanced methods, named C2AE (Yeh\net al. 2017) and GLOCAL (Zhu, Kwok, and Zhou 2018),\nto expand the evaluation experiments. C2AE is a canonical\ncorrelated autoencoder network, which learns a latent em-\nbedding space to bridge feature representations and label in-\nformation. GLOCAL exploits the global and local label cor-\nrelations on the representation learning, which is also an ap-\nplication of label manifold. However, C2AE and GLOCAL\nare both single-view multi-label classiÔ¨Åcation methods, and\nonly iMvWL and NAIML are designed for the datasets with\nboth missing views and missing labels. Therefore, we have\nto do extra alterations to the rest of the methods. Like (Tan\net al. 2018) and (Li and Chen 2021), average values of avail-\nable views are Ô¨Ålled into unusable views for MvEL and lr-\nMMC. And as to MVL-IV and iMSF, we set missing tags\nto be negative tags. C2AE and GLOCAL are independently\nconducted on each view and the best results are reported.\nAll parameters of these comparison methods are set as rec-\nommended in their papers or codes for a fair comparison.\nEvaluation. Different from (Tan et al. 2018) and (Li and\nChen 2021), we only select AP (Average Precision), RL\n(Ranking Loss), and AUC (adapted Area Under Curve) as\nour metrics due to the weak discrimination of HL (Hamming\nLoss). Note that 1-RL is used instead of RL in our results so\nthat the higher the value, the better the performance.\nThe implementation of our method is based on Mind-\nSpore and Pytorch framework.\nExperimental Results and Analysis\nTable 1 to 3 show the performance of the nine methods on\nÔ¨Åve incomplete datasets (some of the results are quoted from\n(Li and Chen 2021) and (Tan et al. 2018)). Table 4 lists the\nresults on the datasets with complete views and labels. All\nexperiments are repeated 10 times to avoid outlier results as\nmuch as possible, and we list the mean and standard devia-\ntion of 10 tests in these tables. Values in parentheses repre-\nsent the standard deviation. Fig. 4 and Fig. 5 show more re-\n8820\nData lrMMC MVL-IV MvEL iMSF C2AE GLOCAL iMvWL NAIML ours\nCorel5k .240(.002) .240(.001) .204(.002) .189(.002) .227(.008) 0.285(0.004) .283(.007) .309(.004) .382(.004)\nPascal07 .425(.003) .433(.002) .358(.003) .325(.000) .485(.008) 0.496(0.004) .441(.017) .488(.003) .519(.005)\nEspgame .188(.000) .189(.000) .132(.000) .108(.000) .202(.006) 0.221(0.002) .242(.003) .246(.002) .294(.004)\nIaprtc12 .197(.000) .198(.000) .141(.000) .101(.000) .224(.007) 0.256(0.002) .235(.004) .261(.001) .317(.003)\nMirÔ¨Çickr .441(.001) .449(.001) .375(.000) .323(.000) .505(.008) 0.537(0.002) .495(.012) .551(.002) .594(.005)\nTable 1: The AP values of nine methods on the Ô¨Åve datasets with 50% missing-view ratio, 70% training samples, and 50%\nmissing-label ratio. The best resluts are marked in bold.\nData lrMMC MVL-IV MvEL iMSF C2AE GLOCAL iMvWL NAIML ours\nCorel5k .762(.002) .756(.001) .638(.003) .709(.005) .804(.010) 0.840(0.003) .865(.003) .878(.002) .880(.002)\nPascal07 .698(.003) .702(.001) .643(.004) .568(.000) .745(.009) 0.767(0.004) .737(.009) .783(.001) .811(.004)\nEspgame .777(.001) .778(.000) .683(.002) .722(.002) .772(.006) 0.780(0.004) .807(.001) .818(.002) .828(.002)\nIaprtc12 .801(.000) .799(.001) .725(.001) .631(.000) .806(.005) 0.825(0.002) .833(.003) .848(.001) .870(.001)\nMirÔ¨Çickr .805(.000) .804(.001) .746(.001) .665(.001) .821(.003) 0.832(0.001) .836(.002) .850(.001) .865(.003)\nTable 2: The 1-RL values of nine methods on the Ô¨Åve datasets with 50% missing-view ratio, 70% training samples, and 50%\nmissing-label ratio. The best resluts are marked in bold.\nData lrMMC MVL-IV MvEL iMSF C2AE GLOCAL iMvWL NAIML ours\nCorel5k .763(.002) .762(.001) .715(.001) .663(.005) .806(.010) 0.843(0.003) .868(.003) .881(.002) .883(.002)\nPascal07 .728(.002) .730(.001) .686(.005) .620(.001) .765(.010) 0.786(0.003) .767(.012) .811(.001) .834(.004)\nEspgame .783(.001) .784(.000) .734(.001) .674(.003) .777(.006) 0.784(0.004) .813(.002) .824(.002) .833(.002)\nIaprtc12 .805(.000) .804(.001) .746(.001) .665(.001) .807(.005) 0.830(0.001) .836(.002) .850(.001) .872(.001)\nMirÔ¨Çickr .806(.001) .807(.000) .761(.000) .715(.001) .810(.004) 0.828(0.001) .794(.015) .837(.001) .853(.003)\nTable 3: The AUC values of nine methods on the Ô¨Åve datasets with 50% missing-view ratio, 70% training samples, and 50%\nmissing-label ratio. The best resluts are marked in bold.\nAP 1-RL AUC\n0.0\n0.2\n0.4\n0.6\n0.8values\n10%\n30%\n50%\n70%\n(a) different missing-view ratios\nAP 1-RL AUC\n0.0\n0.2\n0.4\n0.6\n0.8values\n10%\n30%\n50%\n70% (b) different missing-label ratios\nFigure 4: The results on Corel5k dataset with (a) different\nmissing-view ratios and a 50% missing-label ratio and (b) a\n50% missing-view ratio and different missing-label ratios.\nsults w.r.tdifferent missing and training sample rates. From\nTable 1 to 4, we can summarize the following observations:\n‚Ä¢ Our method achieves overwhelming advantages on all\nthree metrics of the Ô¨Åve datasets, especially on the most\nrepresentative AP metric. For example, the AP value of\nour model exceeds the second-best NAIML by about 7%\nand 5% on the Corel5k and Espgame datasets respec-\ntively, which veriÔ¨Åes the superiority of our method.\n‚Ä¢ Both iMvWL and NAIML reach relatively better perfor-\nmance compared to the Ô¨Årst four methods, which beneÔ¨Åts\nAP 1-RL AUC\n0.0\n0.2\n0.4\n0.6\n0.8values\n10%\n30%\n50%\n70%\n(a) Corel5k\nAP 1-RL AUC\n0.0\n0.2\n0.4\n0.6\n0.8values\n10%\n30%\n50%\n70% (b) MirÔ¨Çickr\nFigure 5: The results on (a) Corel5k dataset and (b) MirÔ¨Çickr\ndataset with a 50% missing-view ratio, a 50% missing-label\nratio, and different training sample ratios.\nfrom good compatibility to double incomplete data. As a\ndeep method with stronger Ô¨Åtting ability, C2AE performs\nmediocrely due to the lack of multi-view complementary\ninformation. Though GLOCAL is a single-view method,\nthe full exploitation of label correlations helps it achieve\nsurprising results on several datasets.\n‚Ä¢ From Table 4, we can Ô¨Ånd that, although our LMVCAT\nis an incomplete method, it still works well with com-\nplete datasets. SpeciÔ¨Åcally, on the Corel5k dataset, the\nAP value of our method is about 14 percentage points\n8821\nDateset Metric C2AE GLOCAL iMvWL NAIML ours\nCorel5k\nAP .353 .386 .313 .327 .521\n1-RL .870 .891 .884 .890 .928\nAUC .873 .895 .887 .893 .930\nPascal07\nAP .584 .610 .468 .496 .629\n1-RL .835 .866 .763 .795 .878\nAUC .851 .879 .793 .822 .892\nEspgame\nAP .269 .264 .260 .251 .385\n1-RL .832 .804 .817 .825 .876\nAUC .837 .810 .822 .830 .880\nIaprtc12\nAP .316 .330 .250 .267 .436\n1-RL .868 .871 .842 .825 .918\nAUC .869 .877 .843 .830 .918\nMirÔ¨Çickr\nAP .593 .643 .493 .555 .684\n1-RL .854 .876 .806 .847 .905\nAUC .845 .869 .789 .839 .889\nTable 4: The experimental results on the Ô¨Åve datasets with\nfull views, full labels and 70% training samples. The best\nresults are marked in bold.\n(a) Corel5k\n (b) Pascal07\n0.2\n0.4\n0.6\n0.8\n1\n1 2 3 4 5\nùõæ\nAP AUC\n(c) Corel5k\n0.2\n0.4\n0.6\n0.8\n1\n1 2 3 4 5\nùõæ\nAP AUC (d) Pascal07\nFigure 6: The results of different parameter selections on\n(a,c) Corel5k and (b,d) Pascal07 datasets with half the avail-\nable views and known labels and a 70% training sample rate.\nahead of the second-best GLOCAL.\nAs shown in Fig. 4, we respectively plot the histogram\nof three metrics in different missing-view and missing-label\nratios by Ô¨Åxing another ratio on the corel5k dataset. These\nresults verify that both incomplete views and partial labels\nare harmful for efÔ¨Åcient classiÔ¨Åcation. In addition, an inter-\nesting phenomenon is that when the incompleteness ratio is\nrelatively small, missing labels have a greater impact on the\nperformance, and conversely, the negative impact of missing\nviews is greater. As shown in Fig. 5, we conduct experiments\non Corel5k and MirÔ¨Çickr datasets with 50% available views,\nmethod Corel5k Espgame\nAP 1-RL AUC AP 1-RL AUC\nV .347 .871 .874 .284 .822 .827\nV+ A .348 .872 .877 .286 .824 .829\nV+ A+ C .359 .879 .883 .288 .828 .833\nV +A +C +G .382 .880 .883 .294 .828 .833\nw/o view mask .352 .866 .870 .274 .817 .823\nw/o label mask .365 .880 .883 .287 .827 .832\nTable 5: The ablation experiments on two datasets with 50%\nmissing-view ratio, 50% missing-label ratio, and 70% train-\ning samples. Vis the backbone with VFormer; Adenotes\nthe adaptively weighted strategy; Crepresents the CFormer;\nand Gmeans the graph constraint.\n50% known tags and different training sample ratios. It is\nclear that, as the proportion of training samples in the total\nincreases, the performance of our method gradually goes up.\nHyperparameters Study\nThere are three hyperparameters, i.e., \u000b, \f, and \r in our\nLMVCAT. To study the optimal parameters selection, we list\nthe performance of our method on different parameter com-\nbinations in Fig. 6. All datasets used in parameters study are\nwith 50% missing-view ratio, 50% missing-label ratio, and\n70% training samples. As can be seen in Fig. 6 (a) and (b),\nfor Corel5k dataset, the optimal parameters \u000band \f are lo-\ncated in the range of [5;10] and [0:05;0:5], respectively, and\nfor Pascal07 dataset, the optimal parameters \u000band \fare lo-\ncated in the range of [10;100] and [0:001;0:5], respectively.\nAs to \r, obviously, our method is insensitive to it from Fig.\n6 (c) and (d). In our experiments, \ris uniformly set to 2.\nAblation Study\nTo study the effectiveness of each component of our method,\nwe perform experiments with following altered methods.\nFor the complete LMVCAT, we remove graph constraint,\nCFormer, and adaptively weighted module sequentially. Be-\nsides, we take off the missing-view indicator and missing-\nlabel indicator in our model so that the incomplete views and\nweak labels are completely exposed to the network. As can\nbe seen in Table 5, our label-guided graph constraint plays\na key role and all experiments conÔ¨Årm that each component\nof our LMVCAT is beneÔ¨Åcial and necessary.\nConclusion\nIn this paper, we propose a general transformer-based frame-\nwork for MvMlC, which skillfully exploits label manifold to\nguide the representation learning. And its innovative view-\nand category-awareness realize multi-view information in-\nteraction and multi-category discrimination fusion. An adap-\ntively weighted fusion strategy is also introduced to balance\nthe view-speciÔ¨Åc contribution. In addition, missing-view\nand weak label problems are speciÔ¨Åcally avoided throughout\nthe network. Extensive experiments support the conclusion\nof the effectiveness of our method.\n8822\nAcknowledgments\nThis work is supported by Shenzhen Science and Tech-\nnology Program under Grant RCBS20210609103709020,\nGJHZ20210705141812038, Shenzhen Fundamental Re-\nsearch Fund under Grant GXWD20220811173317002,\nand CAAI-Huawei MindSpore Open Fund under Grant\nCAAIXSJLJJ-2022-011C.\nReferences\nChen, M.-S.; Liu, T.; Wang, C.-D.; Huang, D.; and Lai, J.-\nH. 2022. Adaptively-weighted Integral Space for Fast Mul-\ntiview Clustering. In Proceedings of the 30th ACM Interna-\ntional Conference on Multimedia, 3774‚Äì3782.\nChen, M.-S.; Wang, C.-D.; and Lai, J.-H. 2022. Low-rank\nTensor Based Proximity Learning for Multi-view Cluster-\ning. IEEE Transactions on Knowledge and Data Engineer-\ning.\nDuygulu, P.; Barnard, K.; de Freitas, J. F.; and Forsyth, D. A.\n2002. Object recognition as machine translation: Learning a\nlexicon for a Ô¨Åxed image vocabulary. In European Confer-\nence on Computer Vision, 97‚Äì112.\nEveringham, M.; Van Gool, L.; Williams, C. K.; Winn, J.;\nand Zisserman, A. 2009. The pascal visual object classes\n(voc) challenge. International journal of computer vision,\n88: 303‚Äì308.\nGrubinger, M.; Clough, P.; M ¬®uller, H.; and Deselaers, T.\n2006. The iapr tc-12 benchmark: A new evaluation resource\nfor visual information systems. In International Conference\non Language Resources and Evaluation, 1‚Äì11.\nGuillaumin, M.; Verbeek, J.; and Schmid, C. 2010. Multi-\nmodal semi-supervised learning for image classiÔ¨Åcation. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 902‚Äì909.\nHu, S.; Lou, Z.; and Ye, Y . 2021. View-Wise Versus Cluster-\nWise Weight: Which Is Better for Multi-View Clustering?\nIEEE Transactions on Image Processing, 31: 58‚Äì71.\nHu, S.; Shi, Z.; and Ye, Y . 2020. DMIB: Dual-correlated\nmultivariate information bottleneck for multiview cluster-\ning. IEEE Transactions on Cybernetics.\nHuang, C.; Liu, C.; Wen, J.; Wu, L.; Xu, Y .; Jiang, Q.; and\nWang, Y . 2022a. Weakly Supervised Video Anomaly Detec-\ntion via Self-Guided Temporal Discriminative Transformer.\nIEEE Transactions on Cybernetics.\nHuang, C.; Liu, C.; Zhang, Z.; Wu, Z.; Wen, J.; Jiang, Q.;\nand Xu, Y . 2022b. Pixel-Level Anomaly Detection via\nUncertainty-Aware Prototypical Transformer. In Proceed-\nings of the 30th ACM International Conference on Multime-\ndia, 521‚Äì530.\nHuang, J.; Qin, F.; Zheng, X.; Cheng, Z.; Yuan, Z.; Zhang,\nW.; and Huang, Q. 2019. Improving multi-label classiÔ¨Åca-\ntion with missing labels by learning label-speciÔ¨Åc features.\nInformation Sciences, 492: 124‚Äì146.\nHuiskes, M. J.; and Lew, M. S. 2008. The mir Ô¨Çickr retrieval\nevaluation. In ACM International Conference on Multimedia\nInformation Retrieval, 39‚Äì43.\nLanchantin, J.; Wang, T.; Ordonez, V .; and Qi, Y . 2021. Gen-\neral multi-label image classiÔ¨Åcation with transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 16478‚Äì16488.\nLi, L.; and He, H. 2020. Bipartite graph based multi-view\nclustering. IEEE transactions on knowledge and data engi-\nneering, 34(7): 3111‚Äì3125.\nLi, L.; Wan, Z.; and He, H. 2021. Incomplete multi-view\nclustering with joint partition and graph learning. IEEE\nTransactions on Knowledge and Data Engineering.\nLi, X.; and Chen, S. 2021. A Concise yet Effective Model\nfor Non-Aligned Incomplete Multi-view and Missing Multi-\nlabel Learning. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 1‚Äì1.\nLiu, C.; Wen, J.; Luo, X.; Huang, C.; Wu, Z.; and Xu, Y .\n2023. DICNet: Deep Instance-Level Contrastive Network\nfor Double Incomplete Multi-View Multi-Label ClassiÔ¨Åca-\ntion. In AAAI Conference on ArtiÔ¨Åcial Intelligence.\nLiu, C.; Wu, Z.; Wen, J.; Xu, Y .; and Huang, C. 2022. Local-\nized Sparse Incomplete Multi-view Clustering. IEEE Trans-\nactions on Multimedia.\nLiu, M.; Luo, Y .; Tao, D.; Xu, C.; and Wen, Y . 2015. Low-\nRank Multi-View Learning in Matrix Completion for Multi-\nLabel Image ClassiÔ¨Åcation. In AAAI Conference on ArtiÔ¨Å-\ncial Intelligence, volume 29.\nLiu, X.; Zhou, S.; Wang, Y .; Li, M.; Dou, Y .; Zhu, E.; and\nYin, J. 2017. Optimal neighborhood kernel clustering with\nmultiple kernels. In Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence, volume 31.\nLu, X.; Zhu, L.; Cheng, Z.; Nie, L.; and Zhang, H. 2019. On-\nline multi-modal hashing with dynamic query-adaption. In\nProceedings of the 42nd international ACM SIGIR confer-\nence on research and development in information retrieval,\n715‚Äì724.\nLuo, X.; Pu, Z.; Xu, Y .; Wong, W. K.; Su, J.; Dou, X.; Ye, B.;\nHu, J.; and Mou, L. 2021. MVDRNet: Multi-view diabetic\nretinopathy detection by combining DCNNs and attention\nmechanisms. Pattern Recognition, 120: 108104.\nLuo, X.; Wang, W.; Xu, Y .; Lai, Z.; Jin, X.; Zhang, B.; and\nZhang, D. 2023. A deep convolutional neural network for di-\nabetic retinopathy detection via mining local and long-range\ndependence. CAAI Transactions on Intelligence Technology.\nSun, S.; and Zong, D. 2020. Lcbm: a multi-view probabilis-\ntic model for multi-label classiÔ¨Åcation. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 43(8): 2682‚Äì\n2696.\nTan, Q.; Yu, G.; Domeniconi, C.; Wang, J.; and Zhang, Z.\n2018. Incomplete multi-view weak-label learning. In Inter-\nnational Joint Conference on ArtiÔ¨Åcial Intelligence, 2703‚Äì\n2709.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. Neural Information Processing Sys-\ntems, 30.\nV on Ahn, L.; and Dabbish, L. 2004. Labeling images with a\ncomputer game. In SIGCHI Conference on Human Factors\nin Computing Systems, 319‚Äì326.\n8823\nWang, L.; Gong, Y .; Ma, X.; Wang, Q.; Zhou, K.; and Chen,\nL. 2022. IS-MVSNet: Importance Sampling-Based MVS-\nNet. In European Conference on Computer Vision, 668‚Äì683.\nSpringer.\nWen, J.; Zhang, Z.; Fei, L.; Zhang, B.; Xu, Y .; Zhang, Z.;\nand Li, J. 2022. A survey on incomplete multiview cluster-\ning. IEEE Transactions on Systems, Man, and Cybernetics:\nSystems.\nWen, J.; Zhang, Z.; Xu, Y .; Zhang, B.; Fei, L.; and Liu, H.\n2019. UniÔ¨Åed embedding alignment with missing views in-\nferring for incomplete multi-view clustering. In AAAI con-\nference on artiÔ¨Åcial intelligence, volume 33, 5393‚Äì5400.\nWu, B.; Liu, Z.; Wang, S.; Hu, B.-G.; and Ji, Q. 2014.\nMulti-label learning with missing labels. In 2014 22nd In-\nternational Conference on Pattern Recognition, 1964‚Äì1968.\nIEEE.\nXu, C.; Tao, D.; and Xu, C. 2015. Multi-view learning with\nincomplete views. IEEE Transactions on Image Processing,\n24(12): 5812‚Äì5825.\nXu, N.; Guo, Y .; Zheng, X.; Wang, Q.; and Luo, X. 2018.\nPartial multi-view subspace clustering. In ACM Interna-\ntional conference on multimedia, 1794‚Äì1801.\nYeh, C.-K.; Wu, W.-C.; Ko, W.-J.; and Wang, Y .-C. F. 2017.\nLearning Deep Latent Space for Multi-Label ClassiÔ¨Åcation.\nIn AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 31.\nYuan, L.; Wang, Y .; Thompson, P. M.; Narayan, V . A.; Ye, J.;\nInitiative, A. D. N.; et al. 2012. Multi-source feature learn-\ning for joint analysis of incomplete multiple heterogeneous\nneuroimaging data. NeuroImage, 61(3): 622‚Äì632.\nZhan, K.; Zhang, C.; Guan, J.; and Wang, J. 2017. Graph\nlearning for multiview clustering. IEEE transactions on cy-\nbernetics, 48(10): 2887‚Äì2895.\nZhang, C.; Yu, Z.; Hu, Q.; Zhu, P.; Liu, X.; and Wang, X.\n2018. Latent semantic aware multi-view multi-label classi-\nÔ¨Åcation. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, volume 32.\nZhang, W.; Zhang, K.; Gu, P.; and Xue, X. 2013. Multi-view\nembedding learning for incompletely labeled data. In Inter-\nnational Joint Conference on ArtiÔ¨Åcial Intelligence, 1910‚Äì\n1916.\nZhao, X.; Chen, Y .; Liu, S.; and Tang, B. 2022. Shared-\nPrivate Memory Networks for Multimodal Sentiment Anal-\nysis. IEEE Transactions on Affective Computing.\nZhu, Y .; Kwok, J. T.; and Zhou, Z.-H. 2018. Multi-Label\nLearning with Global and Local Label Correlation. IEEE\nTransactions on Knowledge and Data Engineering, 30(6):\n1081‚Äì1094.\n8824",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7293664216995239
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6421645283699036
    },
    {
      "name": "Embedding",
      "score": 0.6405321955680847
    },
    {
      "name": "Inference",
      "score": 0.630779504776001
    },
    {
      "name": "Exploit",
      "score": 0.6058197617530823
    },
    {
      "name": "Machine learning",
      "score": 0.5568978190422058
    },
    {
      "name": "Feature learning",
      "score": 0.481471449136734
    },
    {
      "name": "Labeled data",
      "score": 0.42502155900001526
    },
    {
      "name": "Premise",
      "score": 0.42467573285102844
    },
    {
      "name": "Transformer",
      "score": 0.420393705368042
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3511192202568054
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}