{
    "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
    "url": "https://openalex.org/W4385572950",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2516232129",
            "name": "Terra Blevins",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2671828835",
            "name": "Hila Gonen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A334758317",
            "name": "Luke Zettlemoyer",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3100198908",
        "https://openalex.org/W2987270981",
        "https://openalex.org/W3156194904",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4286955624",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W22168010",
        "https://openalex.org/W3038047279",
        "https://openalex.org/W2300691323",
        "https://openalex.org/W3199373335",
        "https://openalex.org/W3198757395",
        "https://openalex.org/W2963651521",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W3176765167",
        "https://openalex.org/W4226155321",
        "https://openalex.org/W2970529259",
        "https://openalex.org/W2995230342",
        "https://openalex.org/W3208641770",
        "https://openalex.org/W2740840489",
        "https://openalex.org/W4236950558",
        "https://openalex.org/W3035579820",
        "https://openalex.org/W3035137491",
        "https://openalex.org/W2963826397",
        "https://openalex.org/W3122044994",
        "https://openalex.org/W1973923101",
        "https://openalex.org/W3017190214",
        "https://openalex.org/W3037116584",
        "https://openalex.org/W3105788222",
        "https://openalex.org/W3035547806",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3101284630",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2985620815",
        "https://openalex.org/W3105813095",
        "https://openalex.org/W4295168876",
        "https://openalex.org/W3105005398",
        "https://openalex.org/W2947799968",
        "https://openalex.org/W3018647120"
    ],
    "abstract": "The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We investigate when these models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks. Our analysis shows that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs. Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network. Taken together, these insights highlight the complexity of multilingual pretraining and the resulting varied behavior for different languages over time.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3575–3590\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nAnalyzing the Mono- and Cross-Lingual Pretraining Dynamics\nof Multilingual Language Models\nTerra Blevins1 Hila Gonen1,2 Luke Zettlemoyer1,2\n1 Paul G. Allen School of Computer Science & Engineering, University of Washington\n2 Meta AI Research\n{blvns, lsz}@cs.washington.edu\nhilagnn@gmail.com\nAbstract\nThe emergent cross-lingual transfer seen in\nmultilingual pretrained models has sparked\nsignificant interest in studying their behavior.\nHowever, because these analyses have focused\non fully trained multilingual models, little is\nknown about the dynamics of the multilingual\npretraining process. We investigate when these\nmodels acquire their in-language and cross-\nlingual abilities by probing checkpoints taken\nfrom throughout XLM-R pretraining, using a\nsuite of linguistic tasks. Our analysis shows\nthat the model achieves high in-language per-\nformance early on, with lower-level linguis-\ntic skills acquired before more complex ones.\nIn contrast, the point in pretraining when the\nmodel learns to transfer cross-lingually differs\nacross language pairs. Interestingly, we also\nobserve that, across many languages and tasks,\nthe final model layer exhibits significant perfor-\nmance degradation over time, while linguistic\nknowledge propagates to lower layers of the\nnetwork. Taken together, these insights high-\nlight the complexity of multilingual pretraining\nand the resulting varied behavior for different\nlanguages over time.\n1 Introduction\nLarge-scale language models pretrained jointly on\ntext from many different languages (Delvin, 2019;\nLample and Conneau, 2019; Lin et al., 2021) per-\nform very well on various languages and on cross-\nlingual transfer between them (e.g., Kondratyuk\nand Straka, 2019; Pasini et al., 2021). Due to this\nsuccess, there has been a great deal of interest in\nuncovering what these models learn from the mul-\ntilingual pretraining signal (§6). However, these\nworks analyze a single model artifact: the final\ntraining checkpoint at which the model is consid-\nered to be converged. Recent work has also studied\nmonolingual models by expanding the analysis to\nmultiple pretraining checkpoints to see how model\nknowledge changes across time (Liu et al., 2021).\nWe analyze multilingual training checkpoints\nthroughout the pretraining process in order to\nidentify when multilingual models obtain their in-\nlanguage and cross-lingual abilities. The case of\nmultilingual language models is particularly inter-\nesting, as the model learns both to capture individ-\nual languages and to transfer between them just\nfrom unbalanced multitask language modeling for\neach language.\nSpecifically, we retrain a popular multilingual\nmodel, XLM-R (Conneau et al., 2020a), and run a\nsuite of linguistic tasks covering 59 languages on\ncheckpoints from across the pretraining process.1\nThis suite evaluates different syntactic and seman-\ntic skills in both monolingual and cross-lingual\ntransfer settings. While our analysis primarily fo-\ncuses on the knowledge captured in model output\nrepresentations over time, we also consider how\nthe performance of internal layers changes during\npretraining for a subset of tasks.\nOur analysis uncovers several insights into mul-\ntilingual knowledge acquisition. First, while the\nmodel acquires most in-language linguistic infor-\nmation early on, cross-lingual transfer is learned\nacross the entire pretraining process. Second, the\norder in which the model acquires linguistic infor-\nmation for each language is generally consistent\nwith monolingual models: lower-level syntax is\nlearned prior to higher-level syntax and then se-\nmantics. In comparison, the order in which the\nmodel learns to transfer linguistic information be-\ntween specific languages can vary wildly.\nFinally, we observe significant degradation of\nperformance for many languages at the final layer\nof the last, converged model checkpoint. However,\nlower layers of the network often continue to im-\nprove later in pretraining and outperform the final\nlayer, particularly for cross-lingual transfer. These\nobservations indicate that there is not a single time\n1The XLM-Rreplica checkpoints are available at https:\n//nlp.cs.washington.edu/xlmr-across-time.\n3575\nTask Setup Num. Langs (Pairs) ExampleIn-lang. X-lang.\nBPC Masked LM 94 –\nThe [MASK] brown fox jumps\nquick\nPOS Tagging Token Labeling 44 18 →18\nThe quick brown fox jumps\nADJ\nDependency Arc Pred. Token Pair Labeling 44 18 →18\nThe quick brown fox jumps\n✖\n ✔\n \nDependency Arc Class. Token Pair Labeling 44 18 →18\nThe quick brown fox jumps\namod\nXNLI Sent. Pair Labeling 15 15 →15\nThe quick brown fox jumps\nThe fox is fast Entails\nSimAlign Unsupervised Alignment – 1 →6\nThe quick brown fox jumps\nLe renard brun rapide saute\nTable 1: Summary of the linguistic information we probe XLM-Rreplica for throughout pretraining.\nstep (or layer) in pretraining that performs the best\nacross all languages and suggest that methods that\nbetter balance these tradeoffs could improve multi-\nlingual pretraining in the future.\n2 Analyzing Knowledge Acquisition\nThroughout Multilingual Pretraining\nOur goal is to quantify when information is learned\nby multilingual models across pretraining. To\nthis end, we reproduce a popular multilingual pre-\ntrained model, XLM-R – referred to as XLM-\nRreplica – and retain several training checkpoints\n(§2.1). A suite of linguistic tasks is then run on\nthe various checkpoints (§2.2). For a subset of\nthese tasks, we also evaluate at which layer in the\nnetwork information is captured during pretraining.\nSince we want to identify what knowledge is\ngleaned from the pretraining signal, each task is\nevaluated without finetuning. The majority of our\ntasks are tested viaprobes, in which representations\nare taken from the final layer of the frozen check-\npoint and used as input features to a linear model\ntrained on the task of interest (Belinkov et al.,\n2020). Additional evaluations we consider for\nthe model include an intrinsic evaluation of model\nlearning (BPC) and unsupervised word alignment\nof model representations. Each of the tasks in our\nevaluation suite tests the extent to which a training\ncheckpoint captures some form of linguistic infor-\nmation, or a specific aspect of linguistic knowledge,\nand they serve as a proxy for language understand-\ning in the model.\n2.1 Replicating XLM-R\nAnalyzing model learning throughout pretraining\nrequires access to intermediate training check-\npoints, rather than just the final artifact. We repli-\ncate the base version of XLM-R and save a num-\nber of checkpoints throughout the training process.\nOur pretraining setup primarily follows that of the\noriginal XLM-R, with the exception that we use\na smaller batch size (1024 examples per batch in-\nstead of 8192) due to computational constraints.\nAll other hyperparameters remain unchanged.\nXLM-Rreplica is also trained on the same data as\nthe original model, CC100. This dataset consists\nof filtered Common Crawl data for 100 languages,\nwith a wide range of data quantities ranging from\n0.1 GiB for languages like Xhosa and Scottish\nGaelic to over 300 Gib for English. As with XLM-\nR, we train on CC100 for 1.5M updates and save\n39 checkpoints for our analysis, with more frequent\ncheckpoints taken in the earlier portion of training:\nwe save the model every 5k training steps up to\nthe 50k step, and then every 50k steps. Further de-\ntails about the data and pretraining scheme can be\nfound in Conneau et al. (2020a). We compare the\nfinal checkpoint of XLM-R replica to the original\nXLM-Rbase and find that while XLM-Rreplica per-\nforms slightly worse in-language, the two models\nperform similarly cross-lingually (Appendix A).\n2.2 Linguistic Information Tasks\nThe analysis suite covers different types of syntac-\ntic knowledge, semantics in the form of natural\nlanguage inference, and word alignment (Table 1).\nThese tasks evaluate both in-language linguistics\nas well as cross-lingual transfer with a wide variety\nof languages and language pairs. Unless other-\nwise stated (§5), we evaluate the output from the\nfinal layer of XLM-R replica. Additionally, most\ntasks (POS tagging, dependency structure tasks,\n3576\n0 100 200 300\nGiB of Pretraining Data\n0.5\n1.0\n1.5BPC\nLanguage Modeling\n0 100 200 300\nGiB of Pretraining Data\n80\n85\n90\n95Acc.\nPOS\n0 100 200 300\nGiB of Pretraining Data\n88\n90\n92\n94\n96Acc.\nArc Pred.\n0 100 200 300\nGiB of Pretraining Data\n60\n70\n80\n90Acc.\nArc Class.\n0 100 200 300\nGiB of Pretraining Data\n55.0\n57.5\n60.0\n62.5Acc.\nXNLI\nFigure 1: Best in-language performance of XLM-Rreplica on various tasks and languages across all checkpoints.\nand XNLI) are evaluated with accuracy; the MLM\nevaluation is scored on BPC, and SimAlign is eval-\nuated on F1 performance. Appendix A details the\nlanguages covered by each of these tasks and fur-\nther experimental details.\nMLM Bits per Character (BPC) As an intrinsic\nmeasure of model performance, we consider the\nbits per character (BPC) on each training language\nof the underlying MLM. For a sequence s, BPC(s)\nis the (average) negative log-likelihood (NLL) of\nthe sequence under the model normalized by the\nnumber of characters per token; lower is better for\nthis metric. These numbers are often not reported\nfor individual languages or across time for multilin-\ngual models, making it unclear how well the model\ncaptures each language on the pretraining task. We\nevaluate BPC on the validation split of CC100.\nPart-of-Speech (POS) Tagging We probe XLM-\nRreplica with a linear model mapping the repre-\nsentation for each word to its corresponding POS\ntag; words that are split into multiple subword to-\nkens in the input are represented by the average\nof their subword representations. The probes are\ntrained using the Universal Dependencies (UD)\ntreebanks for each language (Nivre et al., 2020).\nFor cross-lingual transfer, we evaluate a subset of\nlanguages that occur in Parallel Universal Depen-\ndencies (PUD; Zeman et al., 2017), a set of parallel\ntest treebanks, to control for any differences in the\nevaluation data.\nDependency Structure We evaluate syntactic de-\npendency structure knowledge with two pair-wise\nprobing tasks: arc prediction, in which the probe\nis trained to identify pairs of words that are linked\nwith a dependency arc; and arc classification ,\nwhere the probe labels a pair of words with their\ncorresponding dependency relation. The two word-\nlevel representations r1 and r2 are formatted as a\nsingle concatenated input vector [r1; r2; r1 ⊙r2],\nfollowing Blevins et al. (2018). This combined\nrepresentation is then used as the input to a linear\nmodel that labels the word pair. Probes for both\ndependency tasks are trained and evaluated with\nthe same set of UD treebanks as POS tagging.\nXNLI We also consider model knowledge of nat-\nural language inference (NLI), where the probe is\ntrained to determine whether a pair of sentences\nentail, contradict, or are unrelated to each other.\nGiven two sentences, we obtain their respective\nrepresentation r1 and r2 by averaging all represen-\ntations in the sentence, and train the probe on the\nconcatenated representation [r1; r2; r1 ⊙r2]. We\ntrain and evaluate the probes with the XNLI dataset\n(Conneau et al., 2018); for training data outside\nof English, we use the translated data provided by\nSingh et al. (2019).\nWord Alignment In the layer-wise evaluation\n(§5), we evaluate how well the model’s internal\nrepresentations are aligned using SimAlign (Sabet\net al., 2020), an unsupervised algorithm for align-\ning bitext at the word level using multilingual repre-\nsentations. We evaluate the XLM-Rreplica training\ncheckpoints with SimAlign on manually annotated\nreference alignments for the following language\npairs: EN-CS (Mare ˇcek, 2008), EN-DE2, EN-FA\n(Tavakoli and Faili, 2014), EN-FR (WPT2003, Och\nand Ney, 2000), EN-HI3, and EN-RO3.\n3 In-language Learning Throughout\nPretraining\nWe first consider the in-language, or monolingual,\nperformance of XLM-Rreplica on different types of\nlinguistic information across pretraining. We find\nthat in-language linguistics is learned (very) early\nin pretraining and is acquired in a consistent or-\nder, with lower-level syntactic information learned\nbefore more complex syntax and semantics. Addi-\ntionally, the final checkpoint of XLM-Rreplica often\n2Gold alignments on EuroParl (Koehn, 2005), http://www-\ni6.informatik.rwth-aachen.de/goldAlignment/\n3 WPT2005, http://web.eecs.umich.edu/ mihalcea/wpt05/\n3577\nFigure 2: Learning progress of XLM-Rreplica on POS\ntagging, up to 200k training steps. Each point represents\nthe step at which the model achieves x% of the best\noverall performance of the model on that task; arrows\nindicate languages that reach the 98% mark after 200k\nsteps.\nexperiences performance degradation compared to\nthe best checkpoint for a language, suggesting that\nthe model is forgetting information for a number\nof languages by the end of pretraining.\n3.1 Monolingual Performance for Different\nLanguages\nFigure 1 presents the overall best performance of\nthe model across time on the considered tasks\nand languages. We observe a large amount of\nvariance in performance on each task. Across\nlanguages, XLM-Rreplica performance ranges be-\ntween 1.86 and 0.36 BPC for language modeling,\n88.3% and 96.5% accuracy for dependency arc\nprediction, 77.67% and 98.3% accuracy for POS\ntagging, 54.7% and 93.3% accuracy for arc classi-\nfication, and 53.8% and 62.9% accuracy for XNLI.\nOverall, these results confirm previous findings that\nmultilingual model performance varies greatly on\ndifferent languages (§6).\n3.2 When Does XLM-R Learn Linguistic\nInformation?\nFigure 2 shows the step at which XLM-R replica\nreaches different percentages of its best perfor-\nmance of the model on POS tagging. Figures for\nthe other tasks are given in Appendix D.\nMonolingual linguistics is acquired early in pre-\ntraining We find that XLM-Rreplica acquires the\nmajority of in-language linguistic information early\nFigure 3: Heatmap of relative performance over time\nfor dependency arc prediction and classification. Lan-\nguages are ordered by performance degradation in the\nfinal training checkpoint.\nin training. However, the average time step for ac-\nquisition varies across tasks. For dependency arc\nprediction, all languages achieve 98% or more of\ntotal performance by 20k training steps (out of\n1.5M total updates). In contrast, XNLI is learned\nlater with the majority of the languages achieving\n98% of the overall performance after 100k train-\ning updates. This order of acquisition is in line\nwith monolingual English models, which have also\nbeen found to learn syntactic information before\nhigher-level semantics (Liu et al., 2021).\nWe also observe that this order of acquisition is\noften maintained within individual languages. 12\nout of 13 of the languages shared across all tasks\nreach 98% of the best performance consistently in\nthe order of POS tagging and arc prediction (which\nare typically learned within one checkpoint of each\nother), arc classification, and XNLI.\nModel behavior later in pretraining varies\nacross languages For some languages and tasks,\nXLM-Rreplica never achieves good absolute perfor-\nmance (Figure 1). For others, the performance of\nXLM-Rreplica decreases later in pretraining, lead-\ning the converged model to have degraded perfor-\nmance on those tasks and languages (Figure 3).\n3578\nWe hypothesize that this is another aspect of\nthe “curse of multilinguality,” where some lan-\nguages are more poorly captured in multilingual\nmodels due to limited model capacity (Conneau\net al., 2020a; Wang et al., 2020), that arises during\nthe training process. We also find that the ranking\nof languages by performance degradation is not cor-\nrelated across tasks. This suggests the phenomenon\nis not limited to a subset of low-resource languages\nand can affect any language learned by the model.\nMore generally, these trends demonstrate that\nthe best model state varies across languages and\ntasks. Since BPC continues to improve on all in-\ndividual training languages throughout pretraining\n(Appendix D), the results also indicate that perfor-\nmance on the pretraining task is not directly tied to\nperformance on the linguistic probes. This is some-\nwhat surprising, given the general assumption that\nbetter pretraining task performance corresponds to\nbetter downstream task performance.\n4 Cross-lingual Transfer Throughout\nPretraining\nAnother question of interest is: when do multilin-\ngual models learn to transfer between languages?\nWe find that cross-lingual transfer is acquired later\nin pretraining than monolingual linguistics and that\nthe step at which XLM-Rreplica learns to transfer a\nspecific language pair varies greatly. Furthermore,\nthough the order in which XLM-Rreplica learns to\ntransfer different linguistic information across lan-\nguages is on average consistent with in-language\nresults, the order in which the model learns to trans-\nfer across specific language pairs for different tasks\nis much more inconsistent.\n4.1 Overall Transfer Across Language Pairs\nWhich languages transfer well? Figure 4 shows\ncross-lingual transfer between different language\npairs; most source languages perform well in-\nlanguage (the diagonal). We observe that some\ntasks, specifically dependency arc prediction, are\neasier to transfer between languages than others;\nhowever, across the three tasks with shared lan-\nguage pairs (POS tagging, arc prediction, and arc\nclassification) we see similar behavior in the extent\nto which each language transfers to others. For\nexample, English and Italian both transfer well to\nmost of the target languages. However, other lan-\nguages are isolated and do not transfer well into\nor out of other languages, even though in some\ncases, such as Japanese, the model achieves good\nin-language performance.\nOn XNLI there is more variation in in-language\nperformance than is observed on the syntactic tasks.\nThis stems from a more general trend that some\nlanguages appear to be easier to transfer into than\nothers, leading to the observed performance con-\nsistency within columns. For example, English\nappears to be particularly easy for XLM-R replica\nto transfer into, with 12 out of the 14 non-English\nsource languages performing as well or better on\nEnglish as in-language.\nCross-lingual transfer is asymmetric We also\nfind that language transfer is asymmetric within\nlanguage pairs (Figure 5). There are different trans-\nfer patterns between dependency arc prediction and\nthe other syntactic tasks: for example, we see that\nKorean is worse relatively as a source language\nthan as the target for POS tagging and arc classi-\nfication, but performs better when transferring to\nother languages in arc prediction. However, other\nlanguages such as Arabic have similar trends across\nthe syntactic tasks. On XNLI, we find that Swahili\nand Arabic are the most difficult languages to trans-\nfer into, though they transfer to other languages\nreasonably well.\nThese results expand on observations in Turc\net al. (2021) and emphasize that the choice of\nsource language has a large effect on cross-lingual\nperformance in the target. However, there are fac-\ntors in play in addition to linguistic similarity caus-\ning this behavior, leading to asymmetric transfer\nwithin a language pair. We further examine these\ncorrelations with overall cross-lingual performance\nand asymmetric transfer in §B.2.\n4.2 When is Cross-lingual Transfer Learned\nDuring Pretraining?\nWe next consider when during pretraining XLM-\nRreplica learns to transfer between languages (Fig-\nure 6; the dotted line indicates the 200k step cutoff\nused in Figure 2 for comparison). Unlike the case\nof monolingual performance, the step at which the\nmodel acquires most cross-lingual signal (98%)\nvaries greatly across language pairs. We also find\nthat (similar to the in-language setting) higher-level\nlinguistics transfer later in pretraining than lower-\nlevel ones: the average step for a language pair\nto achieve 98% of overall performance occurs at\n115k for dependency arc prediction, 200k for POS\ntagging, 209k for dependency arc classification,\n3579\nFigure 4: Overall performance of XLM-Rreplica on each analysis task when transferring from various source to\ntarget languages.\nFigure 5: Heatmap of the asymmetry of cross-lingual transfer in XLM-Rreplica. Each cell shows the difference in\nperformance between language pairs (l1 →l2) and (l2 →l1).\nFigure 6: Cross-lingual learning progress of XLM-Rreplica across pretraining. Each red point represents the step to\n98% of the best performance for a language pair; the purple represents the mean 98% transfer step for the source\nlanguage.\nFigure 7: Degradation of cross-lingual transfer performance of XLM-Rreplica across pretraining. Each blue point\nrepresents the change in performance from the overall best step to the final model checkpoint for a language pair;\nthe navy represents the mean decrease for the source language.\n3580\nFigure 8: Heatmap of XLM-R replica performance for\nJapanese arc classification and Bulgarian XNLI. Addi-\ntional heatmaps are given in Appendix C.\nand 274k for XNLI. In contrast, when the model\nlearns to transfer different linguistic information be-\ntween two specific languages can vary wildly: only\napproximately 21% of the language pairs shared\nacross the four tasks transfer in the expected order.\nWe also investigate the amount to which the\ncross-lingual abilities of XLM-R replica decrease\nover time (Figure 7; more detailed across time re-\nsults for transferring out of English are given in Ap-\npendix D). Similarly to in-language behavior, we\nfind that the model exhibits notable performance\ndegradation for some language pairs (in particular\non POS tagging and dependency arc classification),\nand the extent of forgetting can vary wildly across\ntarget languages for a given source language.\n5 Layer-wise Learning Throughout\nPretraining\nIn the experiments above we show that in many\ncases the final layer of XLM-R replica forgets in-\nformation by the end of pretraining. Motivated\nby this, we investigate whether this information\nis retained in a different part of the network by\nprobing how information changesacross layers dur-\ning pretraining. We find a surprising trend in how\nthe best-performing layer changes over time: the\nmodel acquires knowledge in higher layers early\non, which then propagates to and improves in the\nlower layers later in pretraining.\n5.1 In-language Knowledge Across Layers\nWe first look at the layer-wise performance of\nXLM-Rreplica on a subset of languages for depen-\nFigure 9: Heatmap of XLM-Rreplica cross-lingual per-\nformance by layer for arc classification (JA → EN) and\nSimAlign (EN-CS).\ndency arc classification (CS, EN, HI, and JA) and\nXNLI (BG, EN, HI, and ZH) (Figure 8). We find\nthat the last layer is often not the best one for each\ntask, with lower layers often outperforming the fi-\nnal one. On average, the best internal layer state\noutperforms the final layer of XLM-R replica by\n7.59 accuracy points on arc classification and 2.93\npoints on XNLI.\nWe also observe a trend of lower layers acquiring\nknowledge later in training than the final one. To\ninvestigate this, we calculate the expected best layer\n(i.e., the average layer weighted by performance)\nat each checkpoint and find that it decreases over\ntime, by up to 2.79 layers for arc classification and\n2.49 layers for XNLI (Appendix C), indicating that\nthough the final layer quickly fits to the forms of in-\nlanguage information we test for, this information\nthen shifts to lower layers in the network over time.\n5.2 Cross-lingual Knowledge Across Layers\nNext, we consider how cross-lingual transfer skills\nare captured across layers during pretraining. Every\nother XLM-Rreplica layer is evaluated on the sub-\nsets of languages for arc classification and XNLI in\n§5.1. We also use SimALign to test how well word\nrepresentations at these layers align from English\nto {CS, DE, FA, FR, HI, RO}.We observe simi-\nlar trends with respect to layer performance over\ntime to the in-language results (Figure 9; additional\nresults given in Appendix C). Specifically, we ob-\nserve an average decrease in the expected layer of\n1.10 (ranging from 0.67 to 2.20) on arc classifica-\ntion, 1.02 (ranging from 0.37 to 2.01) on XNLI,\nand 1.66 (ranging from 0.83 to 2.41) on SimAlign.\n3581\nWe also observe that while most layers perform\nrelatively well in-language performance, the lowest\nlayers of XLM-Rreplica (layers 0-4) often perform\nmuch worse than the middle and final layers for\ncross-lingual transfer throughout the pretraining\nprocess – for example, in the case of Japanese to\nEnglish on arc classification. We hypothesize that\nthis is due to better alignment across languages in\nlater layers, similar to the findings in Muller et al.\n(2021).\n6 Related Work\nLinguistic knowledge in multilingual models\nThere have been several different approaches\nto quantifying the linguistic information that is\nlearned by multilingual models. One direction has\nperformed layer-wise analyses to quantify what in-\nformation is stored at different layers in the model\n(de Vries et al., 2020; Taktasheva et al., 2021; Pa-\npadimitriou et al., 2021). Others have examined\nthe extent to which the different training languages\nare captured by the model, finding that some lan-\nguages suffer in the multilingual setting despite the\noverall good performance exhibited by the models\n(Conneau et al., 2020a; Wang et al., 2020).\nCross-lingual transfer in multilingual models\nAnother line of analysis seeks to understand the\ncross-lingual abilities of multilingual models. Chi\net al. (2020) show that subspaces of mBERT rep-\nresentations that capture syntax are approximately\nshared across languages, suggesting that portions\nof the model are cross-lingually aligned. A similar\ndirection of interest is whether multilingual mod-\nels learn language-agnostic representations. Singh\net al. (2019) find that mBERT representations can\nbe partitioned by language, indicating that the rep-\nresentations retain language-specific information.\nSimilarly, other work has shown that mBERT rep-\nresentations can be split into language-specific\nand language-neutral components (Libovick`y et al.,\n2019; Gonen et al., 2020; Muller et al., 2021).\nOther work has investigated the factors that af-\nfect cross-lingual transfer. These factors include\nthe effect of sharing subword tokens on cross-\nlingual transfer (Conneau et al., 2020b; K et al.,\n2020; Deshpande et al., 2021) and which languages\nact as good source languages for cross-lingual trans-\nfer (Turc et al., 2021). Notably, Lauscher et al.\n(2020), K et al. (2020) and Hu et al. (2020) find\nthat multilingual pretrained models perform worse\nwhen transferring to distant languages and low-\nresource languages.\nExamining Pretrained Models Across Time A\nrecent direction of research has focused on probing\nmultiple checkpoints taken from different points in\nthe pretraining process, in order to quantify when\nthe model learns information. These works have\nexamined the acquisition of syntax (Pérez-Mayos\net al., 2021) as well as higher-level semantics and\nworld knowledge over time (Liu et al., 2021) from\nthe RoBERTa pretraining process. Similarly, Chi-\nang et al. (2020) perform a similar temporal anal-\nysis for AlBERT, and Choshen et al. (2022) find\nthat the order of linguistic acquisition during lan-\nguage model training is consistent across model\nsizes, random seeds, and LM objectives.\nMost work on probing pretrained models across\nthe training process has focused on monolingual,\nEnglish models. There are some limited excep-\ntions: Dufter and Schütze (2020) present results\nfor multilingual learning in a synthetic bilingual\nsetting, and Wu and Dredze (2020) examine perfor-\nmance across pretraining epochs for a small num-\nber of languages. However, this paper is the first\nto report a comprehensive analysis of monolingual\nand cross-lingual knowledge acquisition on a large-\nscale multilingual model.\n7 Conclusion\nIn this paper, we probe training checkpoints across\ntime to analyze the training dynamics of the\nXLM-R pretraining process. We find that although\nthe model learns in-language linguistic information\nearly in training – similar to findings on monolin-\ngual models – cross-lingual transfer is obtained all\nthroughout the pretraining process.\nFurthermore, the order in which linguistic in-\nformation is acquired by the model is generally\nconsistent, with lower-level syntax acquired be-\nfore semantics. However, we observe that for in-\ndividual language pairs this order can vary wildly,\nand our statistical analyses demonstrate that model\nlearning speed and overall performance on specific\nlanguages (and pairs) are difficult to predict from\nlanguage-specific factors.\nWe also observe that the final model artifact of\nXLM-Rreplica performs often significantly worse\nthan earlier training checkpoints on many lan-\nguages and tasks. However, layer-wise analysis of\nthe model shows that linguistic information shifts\nlower in the network during pretraining, with lower\n3582\nlayers eventually outperforming the final layer. Al-\ntogether, these findings provide a better understand-\ning of multilingual training dynamics that can in-\nform future pretraining approaches.\n8 Limitations\nWe note some potential limitations of this work.\nWe consider a single pretraining setting (replicat-\ning the training of XLM-Rbase), and the extent to\nwhich our findings transfer to other multilingual\npretraining settings remains an open question. In\nparticular, pretraining a language model with more\nparameters or on different multilingual data could\nlead to other trends, though many of our findings\nare consistent with prior work.\nAdditionally, despite our attempts to use diverse\ndatasets for evaluating these models, the language\nchoices available in annotated NLP data are skewed\nheavily towards Indo-European, especially English\nand other Western European, languages. This\nmeans that many of the low-resource languages\nseen in the pretraining data are unaccounted for in\nthis study. Due to this, we only evaluate word align-\nment between six languages paired with English,\nand a number of the non-English datasets we use\nare translated from English.\nAnother product of limited multilingual re-\nsources is our ability to compare across languages;\nin UD, each treebank is annotated on different do-\nmains with different dataset sizes. This limits the\ncomparisons we can make across probe training\nsettings, though we focus on changes within in-\ndividual languages in this work. To address this\nlimitation, we use the parallel test sets from Paral-\nlel Universal Dependencies for our cross-lingual\ntransfer experiments, which allows us to compare\nperformance on different target languages from the\nsame source language directly.\nAcknowledgements\nWe would like to thank Naman Goyal for his ad-\nvice on retraining XLM-R and Masoud Jalili Sabet\nfor providing the data to run SimAlign. We also\nthank Leo Z. Liu for helpful conversations on this\nwork as well as the anonymous reviewers for their\nthoughtful feedback.\nReferences\nYonatan Belinkov, Sebastian Gehrmann, and Ellie\nPavlick. 2020. Interpretability and analysis in neural\nNLP. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics: Tu-\ntorial Abstracts, pages 1–5, Online. Association for\nComputational Linguistics.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer. 2018.\nDeep RNNs encode soft hierarchical syntax. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 14–19.\nEthan A Chi, John Hewitt, and Christopher D Man-\nning. 2020. Finding universal grammatical relations\nin multilingual bert. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5564–5577.\nCheng-Han Chiang, Sung-Feng Huang, and Hung-Yi\nLee. 2020. Pretrained language model embryology:\nThe birth of albert. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 6813–6828.\nLeshem Choshen, Guy Hacohen, Daphna Weinshall,\nand Omri Abend. 2022. The grammar-learning tra-\njectories of neural language models. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8281–8297, Dublin, Ireland. Association for\nComputational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2475–2485.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6022–6034.\nWietse de Vries, Andreas van Cranenburgh, and Malv-\nina Nissim. 2020. What’s so special about bert’s\nlayers? a closer look at the nlp pipeline in mono-\nlingual and multilingual models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 4339–4350.\nJacob Delvin. 2019. Multilingual BERT\nReadme. https://github.com/\ngoogle-research/bert/blob/master/\nmultilingual.md.\n3583\nAmeet Deshpande, Partha Talukdar, and Karthik\nNarasimhan. 2021. When is bert multilingual? iso-\nlating crucial ingredients for cross-lingual transfer.\narXiv preprint arXiv:2110.14782.\nPhilipp Dufter and Hinrich Schütze. 2020. Identify-\ning elements essential for bert’s multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4423–4437.\nHila Gonen, Shauli Ravfogel, Yanai Elazar, and Yoav\nGoldberg. 2020. It’s not greek to mbert: Inducing\nword-level translations from multilingual bert. In\nProceedings of the Third BlackboxNLP Workshop on\nAnalyzing and Interpreting Neural Networks for NLP,\npages 45–56.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generalisa-\ntion. In International Conference on Machine Learn-\ning, pages 4411–4421. PMLR.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan\nRoth. 2020. Cross-lingual ability of multilingual bert:\nAn empirical study. In International Conference on\nLearning Representations.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In The Tenth Ma-\nchine Translation Summit Proceedings of Conference,\npages 79–86. International Association for Machine\nTranslation.\nDan Kondratyuk and Milan Straka. 2019. 75 languages,\n1 model: Parsing universal dependencies universally.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2779–2795.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499.\nJindˇrich Libovick`y, Rudolf Rosa, and Alexander Fraser.\n2019. How language-neutral is multilingual bert?\narXiv preprint arXiv:1911.03310.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nZeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Ha-\njishirzi, and Noah A Smith. 2021. Probing across\ntime: What does roberta know and when? In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, pages 820–842.\nChaitanya Malaviya, Graham Neubig, and Patrick Lit-\ntell. 2017. Learning language representations for\ntypology prediction. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\nCopenhagen, Denmark.\nDavid Mare ˇcek. 2008. Automatic Alignment of Tec-\ntogrammatical Trees from Czech-English Parallel\nCorpus. Charles University, MFF UK.\nBenjamin Muller, Yanai Elazar, Benoît Sagot, and\nDjamé Seddah. 2021. First align, then predict: Un-\nderstanding the cross-lingual ability of multilingual\nbert. In EACL 2021-The 16th Conference of the Eu-\nropean Chapter of the Association for Computational\nLinguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip\nGinter, Jan Hajic, Christopher D Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the 12th Language Resources and\nEvaluation Conference, pages 4034–4043.\nFranz Josef Och and Hermann Ney. 2000. Improved sta-\ntistical alignment models. In Proceedings of the 38th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 440–447.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of NAACL-HLT\n2019: Demonstrations.\nIsabel Papadimitriou, Ethan A Chi, Richard Futrell, and\nKyle Mahowald. 2021. Deep subjecthood: Higher-\norder grammatical features in multilingual bert. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 2522–2532.\nTommaso Pasini, Alessandro Raganato, Roberto Nav-\nigli, et al. 2021. Xl-wsd: An extra-large and cross-\nlingual evaluation framework for word sense disam-\nbiguation. In Proceedings of the AAAI Conference\non Artificial Intelligence. AAAI Press.\nLaura Pérez-Mayos, Miguel Ballesteros, and Leo Wan-\nner. 2021. How much pretraining data do language\nmodels need to learn syntax? In Proceedings of the\n3584\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1571–1582.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert? In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4996–5001.\nMasoud Jalili Sabet, Philipp Dufter, François Yvon, and\nHinrich Schütze. 2020. Simalign: High quality word\nalignments without parallel training data using static\nand contextualized embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1627–1643.\nJasdeep Singh, Bryan McCann, Richard Socher, and\nCaiming Xiong. 2019. Bert is not an interlingua and\nthe bias of tokenization. In Proceedings of the 2nd\nWorkshop on Deep Learning Approaches for Low-\nResource NLP (DeepLo 2019), pages 47–55.\nEkaterina Taktasheva, Vladislav Mikhailov, and Ekate-\nrina Artemova. 2021. Shaking syntactic trees on the\nsesame street: Multilingual probing with controllable\nperturbations. In Proceedings of the 1st Workshop\non Multilingual Representation Learning, pages 191–\n210.\nLeila Tavakoli and Heshaam Faili. 2014. Phrase align-\nments in parallel corpus using bootstrapping ap-\nproach. International Journal of Information and\nCommunication Technology Research.\nIulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei\nChang, and Kristina Toutanova. 2021. Revisiting the\nprimacy of english in zero-shot cross-lingual transfer.\nCoRR.\nZirui Wang, Zachary C Lipton, and Yulia Tsvetkov.\n2020. On negative interference in multilingual mod-\nels: Findings and a meta-learning treatment. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4438–4450.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual bert? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130.\nDaniel Zeman, Martin Popel, Milan Straka, Jan Hajic,\nJoakim Nivre, Filip Ginter, Juhani Luotolahti, Sampo\nPyysalo, Slav Petrov, Martin Potthast, et al. 2017.\nConll 2017 shared task: Multilingual parsing from\nraw text to universal dependencies. In CoNLL 2017\nShared Task: Multilingual Parsing from Raw Text to\nUniversal Dependencies, pages 1–19. Association for\nComputational Linguistics.\nA Linguistic Probe Details\nTable 2 presents the languages that are included\nin each of the probing tasks. We filter the Roman-\nized versions of languages from the CC100 dataset,\nleaving us with 94 for evaluation.\nA.1 Experimental Setup\nEach evaluation is run on the frozen parameters of\na training checkpoint of XLM-Rreplica. All repre-\nsentations are taken from the final (12th) layer of\nthe encoder, except for the experiments presented\nin §5, which consider the performance of different\nlayers within the model over time.\nFor the linguistic information tasks involving\nprobing, each probe consists of a single linear layer,\ntrained with a batch size of 256 for 50 epochs with\nearly stopping performed on the validation set. The\nprobes therefore consist of a limited number of\nparameters m∗l, where m = 768is the output\ndimension of the model and lis the size of the task\nlabel set. Following Liu et al. (2019), the probes\nare optimized with a learning rate of 1e-3. Each\nprobe is trained on a single Nvidia V100 16GB\nGPU and takes between <1 minute and 6 minutes\nto train (depending on dataset size, which varies\nby language and task). The reported results for\neach probe are the averaged performance across\nfive runs.\nFor SimAlign, we use the default settings pro-\nvided in the SimAlign implementation. 4 We re-\nport word-level alignment performance (instead of\nsub-word alignment) using the itermax alignment\nalgorithm.\nA.2 XLM-R Replication Details\nXLM-Rreplica consists of the same model architec-\nture as XLM-Rbase, with a total of 270M param-\neters. We train the model for 1.5 million updates\non 64 Nvidia V100 32 GB GPUs using the fairseq\n4https://github.com/cisnlp/simalign\nTask Languages\nBPC af, am, ar, as, az, be, bg, bn, br, bs, ca, cs, cy, da,\nde, el, en, eo, es, et, eu, fa, fi, fr, fy, ga, gd, gl,\ngu, ha, he, hi, hr, hu, hy, id, is, it, ja, jv, ka, kk,\nkm, kn, ko, ku, ky, la, lo, lt, lv, mg, mk, ml, mn,\nmr, ms, my, ne, nl, no, om, or, pa, pl, ps, pt, ro,\nru, sa, sd, si, sk, sl, so, sq, sr, su, sv, sw, ta, te, th,\ntl, tr, ug, uk, ur, uz, vi, xh, yi, zh, zh\nUD af, ar, bg, ca, cs, cy, da, de, el, en, es, et, eu, fa,\nfi, fr, ga, gd, he, hi, hr, hu, hy, is, it, ja, ko, la,\nlv, nl, pl, pt, ro, ru, sk, sl, sr, sv, tr, ug, uk, ur,\nvi, zh\nXNLI ar, bg, de, el, en, es, fr, hi, ru, sw, th, tr, ur, vi,\nzh\nTable 2: Table summarizing the languages considered\nfor each task. Languages in bold are also used for the\ncross-lingual setting of the task. UD covers all of the\nlanguages used for POS tagging, dependency arc pre-\ndiction, and dependency arc classification.\n3585\nTask XLM-R base XLM-Rreplica\nIn-lang\nBPC 0.609* 0.652\nPOS 89.65* 87.20\nXNLI 58.08 ∗ 55.73\nX-lang POS 66.01* 64.94\nXNLI 53.26 53.77 ∗\nTable 3: Average performance across languages of\nXLM-Rbase and the final checkpoint of XLM-Rreplica.\nlibrary (Ott et al., 2019). Notably, the language\nsampling alpha for up-weighting less frequent lan-\nguages is set to α = 0.7: this matches the value\nused for the XLM-R, though it was reported as\nα= 0.3 in the original paper.\nA.3 Comparison with XLM-R base\nWe also compare the performance of our retrained\nXLM-Rreplica model against the original XLM-\nRbase on a subset of the tasks in our evaluation\nsuite (Table 3). We find that on average, the original\nXLM-R model achieves better BPC than the repli-\ncated model; this is likely due to the decrease in\nbatch size while retraining the model. The replica\nmodel also performs slightly worse than the orig-\ninal on in-language tasks but comparably cross-\nlingually (and outperforms the original model on\ncross-lingual XNLI).\nB What Factors Affect Multilingual\nLearning?\nThis section presents extended results analyzing\nthe correlations between different factors and the\nin-language and cross-lingual learning exhibited by\nXLM-Rreplica.\nB.1 In-language Correlation Study\nWe consider whether the following factors correlate\nwith various measures of model learning (Table 4):\npretraining data, the amount of text in the CC100\ncorpus for each language; task data, the amount of\nin-task data used to train each probe; and language\nsimilarity to English, which is the highest-resource\nlanguage in the pretraining data. We use the syn-\ntactic distances calculated in Malaviya et al. (2017)\nas our measure of language similarity; these scores\nare smaller for more similar language pairs.\nOverall Performance The amount of pretraining\ndata and in-task training data are strongly corre-\nlated with overall task performance for most of the\nconsidered tasks; this corroborates similar results\nfrom Wu and Dredze (2020). Language similarity\nwith English is also correlated with better in-task\nperformance on all tasks except for dependency\narc prediction, suggesting that some form of cross-\nlingual signal supports in-language performance\nfor linguistically similar languages.\nLearning Progress Measures We also consider\n(1) the step at which XLM-Rreplica achieves 95%\nof its best performance for each language and task,\nwhich measures how quickly the model obtains a\nmajority of the tested linguistic information, and\n(2) how much the model forgets from the best per-\nformance for each language by the final training\ncheckpoint. We find that language similarity to\nEnglish is strongly correlated with how quickly\nXLM-Rreplica converges on BPC and dependency\narc classification. This suggests that cross-lingual\nsignal helps the model more quickly learn lower-\nresource languages on these tasks, in addition to\nimproving overall model performance. However,\nwe observe no strong trends as to what factors af-\nfect forgetting across tasks.\nB.2 Cross-lingual Correlation Study\nTable 5 presents a correlation study of differ-\nent measures for cross-lingual transfer in XLM-\nRreplica. We consider the effect of source and tar-\nget pretraining data quantity, the amount of in-task\ntraining data (in the source language), and the sim-\nilarity between the source and target language on\nthe following transfer measures: overall task per-\nformance, asymmetry in transfer (the difference\nin model performance on l1 →l2 compared to\nl2 →l1), the step at which the model achieves 95%\nor more of overall performance on the language\npair, and forgetting – the (relative) degradation of\noverall performance in the final model checkpoint.\nCorrelations of Transfer with Language Factors\nFor overall cross-lingual performance, we observe\nthat language similarity is highly correlated with\ntask performance for all tasks and is similarly cor-\nrelated with speed of acquisition (the step to 95%\nof overall performance) for three of the four con-\nsidered tasks. This is in line with prior work that\nhas also identified language similarity as a strong\nindicator of cross-lingual performance (Pires et al.,\n2019). However, all considered factors are less\ncorrelated with the other measures of knowledge\nacquisition, such as the asymmetry of transfer and\nthe forgetting of cross-lingual knowledge; this sug-\n3586\nVariable Factors Spearman (ρ)\nBPC POS Arc Pred. Arc Class. XNLI\nTask Perf.\nPretraining Data -0.597** 0.258 0.267 0.411* 0.767**\nTask Data -0.597** 0.462* 0.276 0.527**\nLang Sim. 0.427** -0.315* -0.170 -0.427* -0.779**\nSteps to 95%\nPretraining Data 0.135 -0.290 -0.193 -0.301* -0.239\nTask Data 0.135 -0.065 -0.260 -0.209\nLang Sim. -0.385** 0.156 0.268 0.325* 0.316\nForgetting\nPretraining Data 0.230 0.218 0.437* 0.564*\nTask Data -0.322* -0.338* -0.015\nLang Sim. 0.172 -0.158 -0.181 -0.795**\nTable 4: Correlation study of different factors against measures of in-language knowledge. * p < 0.05, ** p < 0.001\nVariable Factors Spearman (ρ)\nPOS Arc Pred. Arc Class. XNLI\nTask Perf.\nSrc. Pretraining Data 0.113* 0.107 0.117* 0.178*\nTrg. Pretraining Data 0.038 0.144* 0.015 0.625**\nTask Data 0.245** 0.124* 0.129*\nLang Sim. -0.598** -0.575** -0.593** -0.321**\nAsymmetry\nSrc. Pretraining Data 0.116* -0.045 0.140* -0.423*\nTrg. Pretraining Data -0.116* 0.045 -0.140* 0.423*\nTask Data 0.123* -0.016 -0.077\nSteps to 95%\nSrc. Pretraining Data -0.290** -0.023 -0.132* -0.195*\nTrg. Pretraining Data -0.123* -0.066 -0.106 -0.057\nTask Data 0.073 -0.057 0.115*\nLang Sim. 0.475** 0.518** 0.492** 0.076\nForgetting\nSrc. Pretraining Data -0.208** -0.123* 0.000 0.137*\nTrg. Pretraining Data 0.042 0.015 0.122* -0.079\nTask Data 0.009 -0.004 0.078\nLang Sim. 0.165* 0.186* -0.025 0.164*\nTable 5: Correlation study of different factors against measures of cross-lingual transfer. * p < 0.05, ** p < 0.001\ngests that there could be other factors that explain\nthese phenomena.\nInteractions Between Learning Measures We\nalso consider the correlations between the different\nmeasures of model performance on cross-lingual\ntransfer. For example, overall transfer performance\nis strongly correlated (p«0.001) with earlier acqui-\nsition (step to 95% of overall performance) for all\nsyntactic tasks: ρ= −0.50 for both POS tagging\nand dependency arc prediction and −0.55 for arc\nclassification. To a lesser extent, overall transfer\nperformance and model forgetting are negatively\ncorrelated, ranging from ρ = −0.13 to −0.42\nacross considered tasks. This indicates that XLM-\nRreplica forgets less of the learned cross-lingual\nsignal for better-performing language pairs, at the\nexpense of already less successful ones.\nC Expanded Layer-wise Analysis\nThis section expands on the layer-wise analysis of\nXLM-Rreplica presented in §5. Figure 10 gives\nadditional layer-wise heatmaps over time. Fig-\nure 11 shows the expected layer (i.e., average\nlayer weighted by relative performance) of XLM-\nFigure 10: Layer-wise performance heatmaps for Czech\narc classification and Chinese XNLI.\n3587\nFigure 11: The expected best layer for in-language\ndependency arc classification and XNLI over time on\nXLM-Rreplica.\nFigure 12: Additional heatmaps of cross-lingual transfer\nat different layers and timesteps of XLM-Rreplica.\nFigure 13: Change in the expected best layer for word\nalignment via SimAlign over time in XLM-Rreplica\nRreplica at different time steps. The expected layer\ndecreases over time: by 1.79, 1.61, 1.08, and 2.79\nfor CS, EN, HI, and JA respectively on dependency\narc classification; and by 2.49, 2.25, 0.43, and 0.77\nfor BG, EN, HI, and ZH respectively on XNLI.\nWe also provide additional examples of layer-\nwise cross-lingual transfer in Figure 12; we find\nthat for cross-lingual transfer, the best internal layer\noutperforms the best final layer state on average by\n7.67 on arc classification transfer, 3.39 on XNLI,\nand 14.2 F1 on Simalign. Figure 13 shows the\nchange in the expected best layer over time for\nSimAlign.\nD Additional Across Time Analyses\nThis section includes additional results from our\nanalysis of knowledge acquisition during multilin-\ngual pretraining:\n• Figure 14 presents BPC learning curves for\neach language in the CC100 training data.\n• Figure 16 covers the learning progress of\nXLM-Rreplica on dependency arc prediction,\narc classification, and XNLI, expanding on\nthe results in §3.2.\n• Figure 15 gives the relative performance for\nin-language POS and XNLI across training\ncheckpoints discussed in §3.2.\n• Figure 17 presents more detailed results for\nrelative performance over time when trans-\nferring out of English. This expands on the\nsummary figures discussed in §4.2.\n3588\nFigure 14: Learning Curves for BPC in each training\nlanguage. Lines are colored by the amount of pretrain-\ning data available for that language.\nFigure 15: Heatmap of relative performance over time\nfor different languages for POS tagging and XNLI. Lan-\nguages are ordered by the amount of performance degra-\ndation at the final checkpoint.\n3589\nFigure 16: Learning Progress of XLM-Rreplica across training, up to 200k training steps. Each point represents the\nstep at which the model achieves x% of the best overall performance of the model on that task.\nFigure 17: Heatmap of relative performance over time for cross-lingual transfer with English as the source language.\nLanguages are ordered by the amount of performance degradation at the final checkpoint.\n3590"
}