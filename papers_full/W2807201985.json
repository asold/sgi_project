{
    "title": "Self-Normalization Properties of Language Modeling",
    "url": "https://openalex.org/W2807201985",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2751121310",
            "name": "Goldberger, Jacob",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4301202512",
            "name": "Melamud, Oren",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2138204974",
        "https://openalex.org/W2962819663",
        "https://openalex.org/W1558797106",
        "https://openalex.org/W932413789",
        "https://openalex.org/W2950797609",
        "https://openalex.org/W2463033603",
        "https://openalex.org/W2963932686",
        "https://openalex.org/W1520465330",
        "https://openalex.org/W2251682575",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W2740553347",
        "https://openalex.org/W2124059530",
        "https://openalex.org/W2747886799",
        "https://openalex.org/W2295800168",
        "https://openalex.org/W2736525247",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W1591801644"
    ],
    "abstract": "Self-normalizing discriminative models approximate the normalized probability of a class without having to compute the partition function. In the context of language modeling, this property is particularly appealing as it may significantly reduce run-times due to large word vocabularies. In this study, we provide a comprehensive investigation of language modeling self-normalization. First, we theoretically analyze the inherent self-normalization properties of Noise Contrastive Estimation (NCE) language models. Then, we compare them empirically to softmax-based approaches, which are self-normalized using explicit regularization, and suggest a hybrid model with compelling properties. Finally, we uncover a surprising negative correlation between self-normalization and perplexity across the board, as well as some regularity in the observed errors, which may potentially be used for improving self-normalization algorithms in the future.",
    "full_text": "Self-Normalization Properties of Language Modeling\nJacob Goldberger\nFaculty of Engineering\nBar-Ilan University, Israel\njacob.goldberger@biu.ac.il\nOren Melamud\nIBM Research\nYorktown Heights, NY , USA\noren.melamud@ibm.com\nAbstract\nSelf-normalizing discriminative models approximate the normalized probability of a class with-\nout having to compute the partition function. In the context of language modeling, this property is\nparticularly appealing as it may signiﬁcantly reduce run-times due to large word vocabularies. In\nthis study, we provide a comprehensive investigation of language modeling self-normalization.\nFirst, we theoretically analyze the inherent self-normalization properties of Noise Contrastive\nEstimation (NCE) language models. Then, we compare them empirically to softmax-based ap-\nproaches, which are self-normalized using explicit regularization, and suggest a hybrid model\nwith compelling properties. Finally, we uncover a surprising negative correlation between self-\nnormalization and perplexity across the board, as well as some regularity in the observed errors,\nwhich may potentially be used for improving self-normalization algorithms in the future.\n1 Introduction\nThe ability of statistical language models (LMs) to estimate the probability of a word given a context of\npreceding words, plays an important role in many NLP tasks, such as speech recognition and machine\ntranslation. Recurrent Neural Network (RNN) language models have recently become the preferred\nmethod of choice, having outperformed traditional n-gram LMs across a range of tasks (Jozefowicz et\nal., 2016). Unfortunately however, they suffer from scalability issues incurred by the computation of the\nsoftmax normalization term, which is required to guarantee proper probability predictions. The cost of\nthis computation is linearly proportional to the size of the word vocabulary and has a signiﬁcant impact\non both training and testing run-times.\nSeveral methods have been proposed to cope with this scaling issue by replacing the softmax with a\nmore computationally efﬁcient component at train time. 1 These include importance sampling (Bengio\nand et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise\nContrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012). NCE has been applied to train neural\nLMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train\nLSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016), achieving near state-of-\nthe-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016). All the\nabove works focused on reducing the complexity at train time. However, at test time, the assumption was\nthat one still needs to compute the costly softmax normalization term to obtain a normalized score ﬁt as\nan estimate for the probability of a word.\nSelf-normalization was recently proposed to address the test time complexity. A self-normalized dis-\ncriminative model is trained to produce near-normalized scores in the sense that the sum over the scores\nof all words is approximately one. If this approximation is close enough, the assumption is that the costly\nexact normalization can be waived at test time without signiﬁcantly sacriﬁcing prediction accuracy (De-\nvlin et al., 2014). Two main approaches were proposed to train self-normalizing models. Regularized\nsoftmax self-normalization is based on using softmax for training and explicitly encouraging the nor-\nmalization term of the softmax to be as close to one as possible, thus making its computation redundant\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/\n1Alleviating this problem using sub-word representations is a parallel line of research not discussed here.\narXiv:1806.00913v1  [cs.CL]  4 Jun 2018\nat test time (Devlin et al., 2014; Andreas and Klein, 2015; Chen et al., 2016). The alternative approach\nis based on NCE. The original formulation of NCE included a parametrized normalization term Zc for\nevery context c. However, the ﬁrst work that applied NCE to language modeling (Mnih and Teh, 2012)\ndiscovered empirically that ﬁxing Zc to a constant did not affect the performance. More recent studies\n(Vaswani et al., 2013; Zoph et al., 2016; Chen et al., 2015; Oualil and Klakow, 2017) empirically found\nthat models trained using NCE with a ﬁxed Zc, exhibit self-normalization at test time. This behavior is\nfacilitated by inherent self-normalization properties of NCE LMs that we analyze in this work.\nThe main contribution of this study is in providing a ﬁrst comprehensive investigation of self-\nnormalizing language models. This includes a theoretical analysis of the inherent self-normalizing\nproperties of NCE language models, followed by an extensive empirical evaluation of NCE against\nsoftmax-based self-normalizing methods. Our results suggest that regularized softmax models perform\ncompetitively as long as we are only interested in low test time complexity. However, when train time\nis also a factor, NCE has a notable advantage. Furthermore, we ﬁnd, somewhat surprisingly, that larger\nmodels that achieve better perplexities tend to have worse self-normalization properties, and perform\nfurther analysis in an attempt to better understand this behavior. Finally, we show that downstream tasks\nmay not all be as sensitive to self-normalization as might be expected.\nThe rest of this paper is organized as follows. In sections 2 and 3, we provide theoretical background\nand analysis of NCE language modeling that justiﬁes its inherent self-normalizing properties. In Sec-\ntion 4, we review the alternative regularized softmax-based self-normalization methods and introduce a\nnovel regularized NCE hybrid approach. In Section 5, we report on an empirical intrinsic investigation\nof all the methods above, and ﬁnally, in Section 6, we evaluate the compared methods on the Microsoft’s\nSentence Completion Challenge and compare these results with the intrinsic measures of perplexity and\nself-normalization.\n2 NCE as a Matrix Factorization\nIn this section, we review the NCE algorithm for language modeling (Gutmann and Hyvarinen, 2012;\nMnih and Teh, 2012) and focus on its interpretation as a matrix factorization procedure. This analysis is\nanalogous to the one proposed by Melamud et al. (2017) for their PMI language model. Let p(w|c) be\nthe probability of a word wgiven a preceding context c, and let p(w) be the word unigram distribution.\nAssume the distribution p(w|c) has the following parametric form:\npnce(w|c) = 1\nZc\nexp(m(w,c)) (1)\nsuch that m(w,c) = ⃗ w·⃗ c+ bw, where ⃗ wand ⃗ care d-dimensional vector representations of the word w\nand its context c, and Zc is a normalization term.\nWe can use a simple lookup table for the word representation ⃗ w, and a recurrent neural network\n(RNN) model to obtain a low dimensional representation of the entire preceding context ⃗ c. Given a text\ncorpus D, the NCE objective function is:\nS(m) =\n∑\nw,c∈D\n[\nlog σ(m(w,c) −log(p(w)k)) (2)\n+\nk∑\ni=1\nlog(1 −σ(m(wi,c) −log(p(wi)k)))\n]\nsuch that w,c go over all the word-context co-occurrences in the learning corpus D and w1,...,w k are\n‘noise’ samples drawn from the word unigram distribution.σdenotes the sigmoid function.\nLet pce(w,c) = log p(w|c) be the Pointwise Conditional Entropy (PCE) matrix, which is the true log\nprobability we are trying to estimate. Gutmann and Hyvarinen (2012) proved that S(m) ≤S(pce) for\nevery matrix m. The rank of the matrix mis at most d+ 1. Thus, the NCE training goal is ﬁnding the\nbest low-rank decomposition of the PCE matrix in the sense that it minimizes the difference S(pce) −\nS(m). Following Melamud and Goldberger (2017), we can explicitly write this difference as a Kullback-\nLeibler (KL) divergence. The NCE derivation was originally based on sampling wand ceither from the\njoint distribution or from the product of marginals according to a binary r.v. denoted by z. For every\nmatrix m, the conditional distribution of zgiven wand cis:\npm(z=1|w,c) = σ(m(w,c) −log(kp(w))).\nThe difference between the NCE score at the PCE matrix and the NCE score at a given matrix mcan be\nwritten as:\nS(pce) −S(m) = KL(ppce(z|w,c)||pm(z|w,c)) (3)\n=\n∑\nw,c\np(w,c)\n∑\nz=0,1\nppce(z|w,c) log ppce(z|w,c)\npm(z|w,c) .\nThis view of NCE as a matrix factorization instead of a distribution estimation, makes the normaliza-\ntion factor redundant during training, thereby justifying the heuristics of setting Zc = 1 used by Mnih\nand Teh (2012). The crux of the matrix decomposition view of NCE is that although the normaliza-\ntion term is not explicitly included here, the optimal low-dimensional model attempts to approximate\nthe true conditional probabilities, which are normalized, and therefore we expect that it will be almost\nself-normalized. Indeed, in the next section we provide formal guarantees for that.\n3 The NCE Self-Normalization property\nWe now address the test time efﬁciency of language models, which is the focus of this study. As is the\ncase with other language models, at test time, when we use the low-dimensional matrix learned by NCE\nto compute the conditional probability pnce(w|c) (1), we need to compute the normalization factor to\nobtain a valid distribution:\nZc =\n∑\nw\nexp(m(w,c)) =\n∑\nw\nexp(⃗ w·⃗ c+ bw). (4)\nUnfortunately, this computation of Zc is often very expensive due to the typical large vocabulary size.\nHowever, as we next show, for NCE language models this computation may be avoided not only at train\ntime, but also at test time due to self-normalization.\nA matrix mis called self-normalized if ∑\nw(exp(m(w,c)) = 1 for every c. The full-rank optimal LM\nobtained from the PCE matrix pce(w,c) = log p(w|c), is clearly self normalized:\nZc =\n∑\nw\nexp(pce(w,c)) =\n∑\nw\np(w|c) = 1.\nThe NCE algorithm seeks the best low-rank unnormalized matrix approximation of the PCE matrix.\nHence, we can assume that the matrix m is close to the PCE matrix and therefore deﬁnes a LM that\nshould also be close to self-normalized:\n∑\nw\nexp(m(w,c)) ≈\n∑\nw\nexp(pce(w,c)) = 1. (5)\nWe next formally show that if the matrixmis close to the PCE matrix then the NCE model deﬁned by\nmis approximately self-normalized.\nTheorem 1:Assume that for a given context cthere is an 0 <ϵ such that\nlog\n∑\nw∈V\np(w|c) exp(|m(w,c) −log p(w|c)|) ≤ϵ.\nLet Zc = ∑\nw exp(m(w,c)) be the normalization factor. Then |log Zc|≤ ϵ.\nProof:\nlog Zc = log\n∑\nw\nexp(⃗ w·⃗ c+ bw)\n= log\n∑\nw\n(p(w|c) exp(m(w,c) −log p(w|c)))\n≤log\n∑\nw\np(w|c) exp(|m(w,c) −log p(w|c)|) ≤ϵ. (6)\nThe concavity of the log function implies that:\n−log Zc ≤−\n∑\nw\np(w|c)(m(w,c) −log p(w|c)) (7)\n=\n∑\nw\np(w|c)(−(m(w,c) −log p(w|c)))\nThe convexity of the exp function implies that:\n≤log\n∑\nw\np(w|c) exp(−(m(w,c) −log p(w|c)))\n≤log\n∑\nw\np(w|c) exp(|m(w,c) −log p(w|c)|) ≤ϵ\nCombining Eq. (6) and Eq. (7) we ﬁnally obtain that |log Zc|≤ ϵ. 2\nWe can also state a global version of Theorem 1 and its proof is similar.\nTheorem 2:Assume there is an 0 <ϵ such that\nlog\n∑\nw,c\np(w,c) exp(|m(w,c) −log p(w|c)|) ≤ϵ.\nThen |∑\nc p(c) logZc|≤ ϵ.\n4 Explicit Self-normalization\nIn this section, we review the two recently proposed language modeling methods that achieve self-\nnormalization via explicit regularization, and then borrow from them to derive an novel regularized\nversion of NCE.\nThe standard language modeling learning method, which is based on a softmax output layer, is not\nself-normalized. To encourage its self-normalization, Devlin et al. (2014) proposed to add to its training\nobjective function, an explicit penalty for deviating from self-normalization:\nSDev =\n∑\nw,c∈D\n[\n(⃗ w·⃗ c+ bw −log Zc) −α(log Zc)2\n]\n(8)\nwhere Zc = ∑\nv∈V exp(⃗ v·⃗ c+ bv) and αis a constant. The drawback of this approach is that at train\ntime you still need to explicitly compute the costly Zc. Andreas and Klein (2015) proposed a more\ncomputationally efﬁcient approximation of (8) that eliminates Zc in the ﬁrst term and computes the\nsecond term only on a sampled subset D′of the corpus D:\nSAnd =\n∑\nw,c∈D\n(⃗ w·⃗ c+ bw) −α\nγ\n∑\nc∈D′\n(log Zc)2 (9)\nwhere γ <1 is an additional constant that determines the sampling rate, i.e. |D′|= γ|D|. They also\nprovided analysis that justiﬁes computing Zc only on a subset of the corpus by showing that if a given\nLM is exactly self-normalized on a dense set of contexts (i.e. each context cis close to a context c′s.t.\nlog Zc′ = 0) then E|log Zc|is small.\nInspired by this work, we propose a regularized variant of the NCE objective function (2):\nSnce−r(m) = Snce(m) −α\nγ\n∑\nc∈D′\n(log Zc)2 (10)\nThis formulation allows us to further encourage the NCE self-normalization, still without incurring the\ncost of computing Zc for every word in the learning corpus.\n5 Intrinsic Evaluation\nWe report here on an empirical investigation of the self-normalization properties of NCE language mod-\neling as compared to the alternative methods described in the previous sections.\n5.1 Experimental Settings\nWe investigated the following language modeling methods:\n•DEV-LM - the language model proposed by Devlin et al. (2014) (Eq. 8)\n•SM-LM - a standard softmax language model (DEV-LM with α= 0)\n•AND-LM - the light-weight approximation of DEV-LM proposed by Andreas and Klein (2015) (Eq.\n9)\n•NCE-LM - NCE language model (Eq. 2)\n•NCE-R-LM - our light-weight regularized NCE method (Eq. 10)\nFollowing Devlin et al. (2014), to make all of the above methods approximately self-normalized at\ninit time, we initialized their output bias terms to bw = −log |V|, where V is the word vocabulary. We\nset the negative sampling parameter for the NCE-based LMs to k = 100, following Zoph et al. (2016),\nwho showed highly competitive performance with NCE LMs trained with this number of samples, and\nfollowing Melamud et al. (2017) who used the same with PMI language models. We note that in early\nexperiments with PMI LMs, which can be viewed as a close variant of NCE-LMs, we got very similar\nresults for both of these types of models and therefore did not include PMI-LMs in our ﬁnal investigation.\nAll of the compared methods use standard LSTM to represent the preceding (left-side) sequence of\nwords as the context vector ⃗ c, and a simple word embedding lookup table to represent the predicted\nnext word as ⃗ w. The LSTM hyperparameters and training regimen are similar to Zaremba et al. (2014)\nwho achieved strong perplexity results compared to other standard LSTM-based neural language models.\nSpeciﬁcally, we used a 2-layer LSTM with a 50% dropout ratio. During training, we performed truncated\nback-propagation-through-time, unrolling the LSTM for 20 steps at a time without ever resetting the\nLSTM state. We trained our model for 20 epochs using Stochastic Gradient Descent (SGD) with a\nlearning rate of 1, which is decreased by a factor of 1.2 after every epoch starting after epoch 6. We clip\nthe norms of the gradient to 5 and use mini-batch size of 20. All models were implemented using the\nChainer toolkit ((Tokui et al., 2015)).\nWe used two popular language modeling datasets in the evaluation. The ﬁrst dataset, denoted\nPTB, is a version of the Penn Tree Bank, commonly used to evaluate language models. 2 It consists\nof 929K/73K/82K training/validation/test words respectively and has a 10K word vocabulary. The\nsecond dataset, denoted WIKI, is the WikiText-2, more recently introduced by Merity et al. (2016).\nThis dataset was extracted from Wikipedia articles and is somewhat larger, with 2,088K/217K/245K\ntrain/validation/test tokens, respectively, and a vocabulary size of 33K.\nTo evaluate self-normalization, we look at two metrics: (1) µz = E(log(Zc)), which is the mean log\nvalue of the normalization term, across the contexts in the evaluated dataset; and (2) σz = σ(log(Zc)),\nwhich is the corresponding standard deviation. The closer these two metrics are to zero, the more self-\nnormalizing the model is considered to be. We note that a model with an observed |µz| >> 0 on\na dev set, can be ‘corrected’ to a large extent (as we show later) by subtracting this dev µz from the\nunnormalized scores at test time. However, this is not the case for σz. Therefore, from a practical point\nof view, we consider σz to be the more important metric of the two. In addition, we also look at the\nclassic perplexity metric, which is considered a standard intrinsic measure for the quality of the model\npredictions. Importantly, when measuring perplexity, except where noted otherwise, we ﬁrst perform\nexact normalization of the models’ unnormalized scores by computing the normalization term.\n2Available from Tomas Mikolov at: http://www.fit.vutbr.cz/˜imikolov/rnnlm/simple-examples.\ntgz\nNCE-LM SM-LM\nd µz σz perp µz σz perp\nPTB validation set\n30 -0.18 0.11 267.6 2.29 0.97 243.4\n100 -0.19 0.17 150.9 3.03 1.52 145.2\n300 -0.15 0.29 100.1 3.77 1.98 97.7\n650 -0.17 0.37 87.4 4.36 2.31 87.3\nWIKI validation set\n30 -0.20 0.13 357.4 2.57 1.02 322.2\n100 -0.24 0.19 194.3 3.34 1.45 191.1\n300 -0.23 0.27 125.6 4.19 1.73 123.3\n650 -0.23 0.35 110.5 4.67 1.83 110.7\nTable 1: Self-normalization and perplexity results of NCE-LM against the standard softmax language\nmodel, SM-LM. ddenotes the size of the compared models (units).\n5.2 Results\nWe begin by comparing the results obtained by the two methods that do not include any explicit self-\nnormalization component in their objectives, namely NCE-LM and the standard softmax SM-LM. Ta-\nble 1 shows that consistent with previous works, NCE-LM is approximately self-normalized as appar-\nent by relatively low |µz|and σz values. On the other hand, SM-LM, as expected, is much less self-\nnormalized. In terms of perplexity, we see that SM-LM performs a little better than NCE-LM when\nmodel dimensionality is low, but the gap closes entirely at d = 650 . Curiously, while perplexity im-\nproves with higher dimensionality, we see that the quality of NCE-LM’s self-normalization, as evident\nparticularly by σz, actually degrades. This is surprising, as we would expect that stronger models with\nmore parameters would approximate the true p(w|c) more closely and hence be more self-normalized.\nA similar behavior was recorded for SM-LM. We further investigate this in Section 5.3.\nWe also measured model test run-times, running on a single Tesla K20 GPU. We compared run-times\nfor normalized scores that were produced by applying exact normalization versus unnormalized scores.\nFor both SM-LM and NCE-LM, which perform the same operations at test time, we get ∼9 seconds for\nnormalized scores vs. ∼8 seconds for unnormalized ones on the PTB validation set. Run-times on the\nx3 larger Wiki validation set are ∼38 seconds for normalized and ∼24 seconds for unnormalized. We\nsee that the run-time of the unnormalized models seems to scale linearly with the size of the dataset.\nHowever, the normalized run-time scales super-linearly, arguably since it depends heavily on the vocab-\nulary size, which is greater for Wiki than for PTB. With typical vocabulary sizes reaching much higher\nthan Wiki’s 33K word types, this reinforces the computational motivation for self-normalized language\nmodels.\nNext, Table 2 compares the self-normalization and perplexity performance of DEV-LM for varied\nvalues of the constant αon the validation sets. As could be expected, the larger the value of αis, the\nbetter the self-normalization becomes, reaching very good self-normalization for α = 10 .0. On the\nother hand, the improvement in self-normalization seems to occur at the expense of perplexity. This\nis particularly true for the smaller models, but is still evident even for d = 650. Interestingly, as with\nNCE-LM, we see that σz grows (i.e. self-normalization becomes worse) with the size of the model, and\nis negatively correlated with the improvement in perplexity.\nFinally, in Table 3, we compare AND-LM against our proposed NCE-R-LM, using a sampling rate\nof γ = 0 .1 to avoid computing Zc most of the time, and varied values of α. As can be seen, AND-\nLM exhibits relatively bad performance. In particular, to make the model converge when trained on the\nWIKI dataset, we had to follow the heuristic suggested by Chen et al. (2016), applying the following\nconversion to all of AND-LM’s unnormalized scores,x→10 tanh(x/5). In contrast, we see that NCE-\nR-LM is able to use the explicit regularization to improve self-normalization at the cost of a relatively\nsmall degradation in perplexity.\nDEV-LM\nα= 0.1 α= 1.0 α= 10.0\nd µz σz perp µZ σz perp µz σz perp\nPTB validation set\n30 -0.12 0.21 242.6 -0.16 0.09 250.9 -0.13 0.060 307.2\n100 -0.10 0.28 143.3 -0.17 0.11 149.5 -0.12 0.058 182.0\n300 -0.09 0.36 96.3 -0.14 0.14 100.8 -0.16 0.054 121.3\n650 -0.14 0.43 85.0 -0.17 0.18 86.3 -0.11 0.071 99.5\nWIKI validation set\n30 -0.10 0.23 334.1 -0.17 0.08 338.7 -0.15 0.055 389.0\n100 -0.13 0.28 189.4 -0.22 0.13 191.1 -0.15 0.071 228.3\n300 -0.15 0.34 121.9 -0.20 0.17 125.7 -0.13 0.081 143.6\n650 -0.23 0.42 109.1 -0.23 0.20 110.0 -0.12 0.089 116.9\nTable 2: Self-normalization and perplexity results of the self-normalizing DEV-LM for different values\nof the normalization factor α. ddenotes the size of the compared models (units).\nNCE-R-LM AND-LM\nα µz σz perp µz σz perp\nPTB validation set\n0.1 -0.19 0.34 87.1 6.14 0.56 117.5\n1.0 -0.21 0.27 87.2 0.45 0.25 119.4\n10.0 -0.19 0.17 89.8 -0.037 0.079 143.7\n100.0 -0.089 0.086 112.6 -0.024 0.030 209.5\nWIKI validation set\n0.1 -0.23 0.33 111.1 4.85 0.72 201.5\n1.0 -0.24 0.28 107.5 1.02 0.001 1481.3\n10.0 -0.22 0.19 110.8 0.41 0.12 33323.1\n100.0 -0.12 0.099 131.5 0.413 0.000 33278.0\nTable 3: Self-normalization and perplexity results of the self-normalizing DEV-LM for different values\nof the normalization factor α. d= 650 and γ= 0.1.\nSwitching to the test-set evaluation, we propose a simple technique to center the log(Z) values of a\nself-normalizing model’s scores around zero. Let µvalid\nz be E(log(Z)) observed on the validation set at\ntrain time. The probability estimates of the ‘shifted’ model at test time arelog p(w|c) = ⃗ w·⃗ c+bw−µvalid\nz .\nTable 4 shows the results that we get when evaluating the shifted versions of DEV-LM, NCE-R-LM,\nNCE-LM and AND-LM withd = 650 . For each compared model, we chose theαvalue that showed the\nbest self-normalization performance without sacriﬁcing signiﬁcant perplexity performance. Speciﬁcally,\nwe used αDEV-LM = 1.0 and αNCE-R-LM = 10.0 for both PTB and WIKI datasets, and then αAND-LM =\n1.0 and αAND-LM = 0 .1 for the PTB and WIKI datasets, respectively. Following Oualil and Klakow\n(2017), in addition to perplexity, we also report ‘unnormalized perplexity’, which is computed with the\nunnormalized model scores. When the unnormalized perplexity measure is close to the real perplexity,\nthis suggests that the unnormalized scores are in fact nearly normalized.\nAs can be seen, with the shifting method, all models achieve near perfect (zero) µz value, and their\nunnormalized perplexities are almost identical to their respective real perplexities. Also, with the excep-\ntion of AND-LM, the perplexities of all models are nearly identical. Finally, the standard deviation of the\nnormalization term of DEV-LM and NCE-R-LM is notably better than that of NCE-LM and AND-LM.\nDEV-LM and NCE-R-LM perform very similar in all respects. However, we note that NCE-R-LM’s\nadvantage is that during training, it performs sparse computations of the costly normalization term and\ntherefore its training time depends much less on the size of the vocabulary.\nPTB-test WIKI-test\nµz σz perp u-perp µz σz perp u-perp\nDEV-LM -0.001 0.17 83.1 83.0 0.002 0.20 104.1 104.2\nNCE-R-LM 0.002 0.17 85.9 86.0 -0.003 0.19 105.0 104.7\nNCE-LM -0.004 0.35 83.7 83.4 0.003 0.36 104.3 104.6\nAND-LM 0.001 0.30 114.9 115.0 0.018 0.74 185.7 189.1\nTable 4: Self-normalization and perplexity results on test sets for ‘shifted’ models withd= 650. ‘u-perp’\ndenotes unnormalized perplexity.\nPTB-validation WIKI-validation\nd NCE-LM DEV-LM (α= 1.0) NCE-LM DEV-LM (α= 1.0)\n30 -0.33 -0.27 -0.50 -0.26\n100 -0.29 -0.29 -0.53 -0.49\n300 -0.46 -0.41 -0.56 -0.63\n650 -0.50 -0.45 -0.53 -0.64\nTable 5: Pearson’s correlation betweenHc (entropy) and log(Zc) on samples from the validation sets.\nFigure 1: A 2-dimensional histogram of the normalization term of a predicted distribution as a function\nof its entropy, as measured over a sample from NCE-LM predictions on the WIKI validation set. Brighter\ncolors denote denser areas.\n5.3 Analysis\nThe entropy of the distributions predicted by a language model is a measure of how uncertain it is\nregarding the identity of the predicted word. Low-entropy distributions would be concentrated around\nfew possible words, while high-entropy ones would be much more spread out. To more carefully analyze\nthe self-normalization properties of NCE-LM and DEV-LM, we computed the Pearson’s correlation\nbetween the entropy of a predicted distributionHc = −∑\nv p(v|c) logp(v|c) and its normalization term,\nlog(Zc). As can be seen in Table 5, it appears that a regularity exists, where the value of log(Zc) is\nnegatively correlated with entropy. Furthermore, it seems that, to an extent, the correlation is stronger for\nlarger models. To further illustrate this regularity, Figure 1 shows a 2-dimensional histogram of a sample\nof distributions predicted by NCE-LM . We can see there that particularly low entropy distributions can\nbe associated with very high values of log(Zc), deviating a lot from the self-normalization objective of\nlog(Zc) = 0 . Examples for contexts with such low-entropy distributions are: “During the American\nCivil [War]” and “The United [States]”, where the actual word following the preceding context appears\nin parenthesis. This phenomenon is less evident for smaller models, which tend to produce fewer low\nentropy predictions.\nWe hypothesize that the above observations could be a contributing factor to our earlier ﬁnding that\nlarger models have larger variance in their normalization terms, though it seems to account only for some\nof that at best. Furthermore, we hope that this regularity could be exploited to improve self-normalization\nalgorithms in the future.\n2 training iterations 5 training iterations\nacc-n ∆acc perp σz acc-n ∆acc perp σz\nDEV-LM 47.6 +0.4 75.3 0.10 51.9 -0.7 67.5 0.10\nNCE-R-LM 46.3 -0.5 78.3 0.11 51.0 -0.2 70.2 0.10\nNCE-LM 47.6 -0.4 75.3 0.17 51.6 +1.1 67.1 0.14\nSM-LM 47.0 -2.0 73.4 1.15 51.0 -2.3 66.3 1.19\nTable 6: Microsoft Sentence Completion Challenge (MSCC) results for models with d= 650 that were\ntrained with 2 and 5 iterations. ’acc-n’ denotes the accuracy measure obtained when language model\nscores are precisely normalized. ’ ∆acc’ denotes the delta in accuracy when unnormalized scores are\nused instead. ’perp’ and σz denote the mean perplexity and standard deviation of log(Zc) recorded for\nthe 1,040 answer sentences.\n6 Sentence Completion Challenge\nIn Section 5, we’ve seen that there may be some trade-offs between perplexity, self-normalization and\nrun-time complexity of language models. While the language modeling method should ultimately to be\noptimized for each downstream task individually, we follow Mnih and Teh (2012) and use the Microsoft\nSentence Completion Challenge (Zweig and Burges, 2011) as an example use case.\nThe Microsoft Sentence Completion Challenge (MSCC) (Zweig and Burges, 2011) includes 1,040\nitems. Each item is a sentence with one word replaced by a gap, and the challenge is to identify the\nword, out of ﬁve choices, that is most meaningful and coherent as the gap-ﬁller. The MSCC includes a\nlearning corpus of approximately 50 million words. To use this corpus for training our language models,\nwe split it into sentences, shufﬂed the sentence order and considered all words with frequency less than\n10 as unknown, yielding a vocabulary of about 50K word types. We used the same settings described\nin Section 5.1 to train the language models except that due to the larger size of the data, we ran fewer\ntraining iterations. 3 Finally, as the gap-ﬁller, we choose the word that maximizes the score of the entire\nsentence, where a sentence score is the sum of its words’ scores. For a normalized language model this\nscore can be interpreted as the estimated log-likelihood of the sentence.\nThe results of the MSCC experiment appear in Table 6. Accuracy is the standard evaluation metric for\nthis benchmark (simply the proportion of questions answered correctly). We report this metric when per-\nforming the costly test-time score normalization and then the delta observed when using unnormalized\nscores instead. First, we note that given the same number of training iterations, all methods achieved\nfairly similar normalized-scores accuracies, as well as perplexity values. At the same time, we do see\na notable improvement in both accuracies and perplexities when more training iterations are performed.\nNext, with the exception of SM-LM, all of the compared models exhibit good self-normalization proper-\nties, as is evident from the lowσz values. There does not seem to be a meaningful accuracy performance\nhit when using unnormalized-scores for these models, suggesting that this level of self-normalization is\nadequate for the MSCC task. Finally, as expected, SM-LM exhibits worse self-normalization properties.\nHowever, somewhat surprisingly, even in this case, we see a relatively small (though more noticeable) hit\nin accuracy. This suggests that in some use cases, the level of the language model’s self-normalization\nmay have a relatively low impact on the performance of a down-stream task.\n7 Conclusions\nWe reviewed and analyzed the two alternative approaches for self-normalization of language models,\nnamely, using Noise Contrastive Estimation (NCE) that is inherently self-normalized, versus adding\nexplicit self-normalizing regularization to a softmax objective function. Our empirical investigation\ncompared these approaches, and by extending NCE language modeling with a light-weight explicit self-\nnormalization, we also introduced a hybrid model that achieved both good self-normalization and per-\nplexity performance, as well as little dependence of train-time on the size of the vocabulary. To put our\n3We started with a learning rate of 1 and reduced it by a factor of 2 after each iteration beginning with the very ﬁrst one.\nintrinsic evaluation results in perspective, we used the Sentence Completion Challenge as an example\nuse-case. The results suggest that it would be wise to test the sensitivity of the downstream task to\nself-normalization, in order to choose the most appropriate method. Finally, further analysis revealed\nunexpected correlations between self-normalization and perplexity performance, as well as between the\npartition function of self-normalized predictions and the entropy of the respective distribution. We hope\nthat these insights would be useful for improving self-normalizing models in future work.\nReferences\n[Andreas and Klein2015] J. Andreas and D. Klein. 2015. When and why are log-linear models self-normalizing?\nIn NAACL.\n[Bengio and et al2003] Y . Bengio and J. Senecal et al. 2003. Quick training of probabilistic neural nets by impor-\ntance sampling. In AISTATS.\n[Chen et al.2015] X. Chen, X. Liu, M. Gales, and P. C. Woodland. 2015. Recurrent neural network language model\ntraining with noise contrastive estimation for speech recognition. In ICASSP.\n[Chen et al.2016] W. Chen, D. Grangier, and M. Auli. 2016. Strategies for training large vocabulary neural lan-\nguage models. CoRR, abs/1512.04906.\n[Devlin et al.2014] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. 2014. Fast and robust\nneural network joint models for statistical machine translation. In Proceedings of ACL.\n[Gutmann and Hyvarinen2012] M. U. Gutmann and A. Hyvarinen. 2012. Noise-contrastive estimation of unnor-\nmalized statistical models, with applications to natural image statistics. Journal of Machine Learning Research,\n13:307–361.\n[Ji et al.2016] S. Ji, S. Vishwanathan, N. Satish, A. Nadathur, J. Michael, and P. Dubey. 2016. Blackout: Speeding\nup recurrent neural network language models with very large vocabularies. ICLR.\n[Jozefowicz et al.2016] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y . Wu. 2016. Exploring the limits\nof language modeling. arXiv preprint arXiv:1602.02410.\n[Melamud and Goldberger2017] O. Melamud and J. Goldberger. 2017. Information-theory interpretation of the\nskip-gram negative-sampling objective function. In Proceedings of ACL.\n[Melamud et al.2017] O. Melamud, I. Dagan, and J. Goldberger. 2017. A simple language model based on pmi\nmatrix approximations. In EMNLP.\n[Merity et al.2016] S. Merity, C. Xiong, J. Bradbury, and R. Socher. 2016. Pointer sentinel mixture models. arXiv\npreprint arXiv:1609.07843.\n[Minh and Hinton2008] A. Minh and G. E. Hinton. 2008. A scalable hierarchical distributed language model. In\nAdvances in Neural Information Processing Systems.\n[Mnih and Teh2012] A. Mnih and Y . W. Teh. 2012. A fast and simple algorithm for training neural probabilistic\nlanguage models. In ICML.\n[Oualil and Klakow2017] Y . Oualil and D. Klakow. 2017. A batch noise contrastive estimation approach for\ntraining large vocabulary language models. In Interspeech.\n[Tokui et al.2015] S. Tokui, K. Oono, S. Hido, and J. Clayton. 2015. Chainer: a next-generation open source\nframework for deep learning. In Workshop on Machine Learning Systems (LearningSys) in The 29th Annual\nConference on Neural Information Processing Systems.\n[Vaswani et al.2013] A. Vaswani, Y . Zhao, V . Fossum, and D. Chiang. 2013. Decoding with large-scale neural\nlanguage models improves translation. In EMNLP.\n[Zaremba et al.2014] W. Zaremba, I. Sutskever, and O. Vinyals. 2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\n[Zoph et al.2016] B. Zoph, A. Vaswani, J. May, and K. Knight. 2016. Simple, fast noise-contrastive estimation for\nlarge RNN vocabularies. In NAACL.\n[Zweig and Burges2011] Geoffrey Zweig and Christopher JC Burges. 2011. The microsoft research sentence\ncompletion challenge. Technical report, Technical Report MSR-TR-2011-129, Microsoft."
}