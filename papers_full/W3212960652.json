{
    "title": "Improving Large-scale Language Models and Resources for Filipino",
    "url": "https://openalex.org/W3212960652",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4225988164",
            "name": "Cruz, Jan Christian Blaise",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225988165",
            "name": "Cheng, Charibeth",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962911098",
        "https://openalex.org/W3013571468",
        "https://openalex.org/W3171594500",
        "https://openalex.org/W2948902769",
        "https://openalex.org/W3022569409",
        "https://openalex.org/W2890476684",
        "https://openalex.org/W2041532239",
        "https://openalex.org/W2960374072",
        "https://openalex.org/W2954835819",
        "https://openalex.org/W3184333966",
        "https://openalex.org/W3038105747",
        "https://openalex.org/W3208594153",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3100806282",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3105425516"
    ],
    "abstract": "In this paper, we improve on existing language resources for the low-resource Filipino language in two ways. First, we outline the construction of the TLUnified dataset, a large-scale pretraining corpus that serves as an improvement over smaller existing pretraining datasets for the language in terms of scale and topic variety. Second, we pretrain new Transformer language models following the RoBERTa pretraining technique to supplant existing models trained with small corpora. Our new RoBERTa models show significant improvements over existing Filipino models in three benchmark datasets with an average gain of 4.47% test accuracy across the three classification tasks of varying difficulty.",
    "full_text": "Improving Large-scale Language Models and Resources for Filipino\nJan Christian Blaise Cruz and Charibeth Cheng\nCenter for Language Technologies\nCollege of Computer Studies\nDe La Salle University, Manila\n{jan_christian_cruz, charibeth.cheng}@dlsu.edu.ph\nAbstract\nIn this paper, we improve on existing lan-\nguage resources for the low-resource Filipino\nlanguage in two ways. First, we outline the\nconstruction of the TLUniﬁed dataset, a large-\nscale pretraining corpus that serves as an im-\nprovement over smaller existing pretraining\ndatasets for the language in terms of scale\nand topic variety. Second, we pretrain new\nTransformer language models following the\nRoBERTa pretraining technique to supplant\nexisting models trained with small corpora.\nOur new RoBERTa models show signiﬁcant\nimprovements over existing Filipino models in\nthree benchmark datasets with an average gain\nof 4.47% test accuracy across the three classi-\nﬁcation tasks of varying difﬁculty.\n1 Introduction\nUnlike High-resource Languages (HRL) such as\nEnglish, German, and French, Low-resource Lan-\nguages (LRL) suffer from a lack of benchmark\ndatasets, databases, linguistic tools, and pretrained\nmodels that impede the progress of research within\nthose languages.\nDespite the growing success of methods that\nintrinsically learn from little data (Deng et al.,\n2020; Lee et al., 2021), creating “more data” re-\nmains a very signiﬁcant fundamental task in NLP.\nGiven the data-hungry nature of the neural net-\nworks that are prevalent in NLP today, creating new\ndatasets to train from is the most efﬁcient way to\nimprove model performance. In addition, cleverly-\nconstructed datasets also reveal new insights into\nthe models we commonly use, letting us gauge their\ntrue performance and expose hidden weaknesses\n(Maudslay and Cotterell, 2021).\nIn this paper, we improve upon the existing re-\nsources for Filipino, a low-resource language spo-\nken in the Philippines. We create a larger, more\ntopically-varied large-scale pretraining dataset that\nimproves upon the existing WikiText-TL-39 (Cruz\nand Cheng, 2019) that is too small and too topically-\nnarrow to create robust models that perform well\nin modern NLP. We also produce new RoBERTa\npretrained models using our pretraining dataset\nthat supplant existing models trained with less data\n(Cruz and Cheng, 2020).\n2 Resource Creation\nIn this section, we outline our full methodology\nfor resource creation. First, we introduce the con-\nstruction of our new large-scale pretraining dataset.\nNext, we detail the pretraining steps for our new\nRoBERTa models. Lastly, we introduce the task\ndatasets that we use to benchmark performance for\nour new pretrained models.\n2.1 The TLUniﬁed Dataset\nTo effectively pretrain a large transformer for down-\nstream tasks, we require an equally large pretrain-\ning corpus of high-quality Filipino text. We con-\nstruct our pretraining corpus by combining a num-\nber of available Filipino corpora, including:\n• Bilingual Text Data – Bitext datasets are\nused for training Machine Translation mod-\nels and contain crawled and aligned data\nfrom multiple sources. We collected mul-\ntiple bitexts, extracted Filipino text, then\ndeduplicated the extracted data to add to our\npretraining corpus. Datasets we collected\nfrom include bible-uedin (Christodouloupou-\nlos and Steedman, 2015), CCAligned (El-\nKishky et al., 2020), ELRC 2922 1, MultiC-\nCAligned (El-Kishky et al., 2020),ParaCrawl\n2, TED2020 (Reimers and Gurevych, 2020),\nWikiMatrix (Schwenk et al., 2019), tico-19,\nUbuntu, OpenSubtitles, QED, Tanzil, Tatoeba,\nGlobalV oices, KDE4, and WikiMedia (Tiede-\nmann, 2012).\n1https://elrc-share.eu/\n2https://www.paracrawl.eu/\narXiv:2111.06053v1  [cs.CL]  11 Nov 2021\n• OSCAR – The Open Super-Large Crawled\nAggregated Corpus (OSCAR) (Ortiz Suárez\net al., 2019) is a massive dataset obtained\nfrom language identiﬁcation and ﬁltering of\nthe Common Crawl dataset. We use the dedu-\nplicated version of the Filipino (Tagalog) por-\ntion of OSCAR and add it to our pretraining\ncorpus.\n• NewsPH – The NewsPH (Cruz et al., 2021)\ncorpus is a large-scale crawled corpus of Fil-\nipino news articles, originally used in automat-\nically creating the NewsPH-NLI benchmark\ndataset. Since we plan on using an NLI dataset\nderived from NewsPH for benchmarking in\nthis paper, we opted to only use a 60% subset\nof the NewsPH corpus to add to TLUniﬁed.\nSince a large portion of our corpus is crawled and\nartiﬁcially aligned, we expect that out-of-the-box\ndata quality would be low. To clean our dataset,\nwe apply a number of preprocessing ﬁlters to it,\nincluding:\n1. Non-latin Filter – We ﬁlter out sentences\nwhose characters are composed of more than\n15% non-latin letters.\n2. Length Filter – We remove sentences that have\na number of tokens N where 4 <= N <=\n150.\n3. Puncutation Filter – All sentences that have to-\nkens composed of too many succeeding punc-\ntuations (eg. “///”) are all removed.\n4. Average Word Length Filter – If a sentence\nhas tokens that are signiﬁcantly longer than\nthe other tokens in the sentence, we remove\nthe sentence entirely. We ﬁrst take the sum\nof the character lengths of each token, then\ndivide it by the number of tokens to get a ratio\nr. Only sentences with ratio 3 <= r <= 18\nare kept in the corpus.\n5. HTML Filter – All sentences with HTML and\nURL-related tokens (e.g. “.com” or “http://”)\nare removed.\nAfter ﬁltering the dataset, we perform one addi-\ntional deduplication step to ensure that no identical\nlines are found in the dataset. The ﬁnal result is a\nlarge-scale pretraining dataset we call TLUniﬁed.\nWe then train tokenizers using TLUniﬁed, limit-\ning our vocabulary to a ﬁxed 32,000 BPE subwords\nBase Large\nHidden Size 768 1024\nFeedforward Size 3072 4096\nMax Sequence Length 512 512\nAttention Heads 12 16\nHidden Layers 12 24\nDroput 0.1 0.1\nTable 1: Base and Large RoBERTa hyperparameters.\n(Sennrich et al., 2015). Our tokenizers are trained\nwith a character coverage of 1.0. We also do not\nremove casing to ensure that capitalization is kept\nafter tokenization.\n2.2 Pretraining\nWe then pretrain transformer language models that\ncan serve as bases for a variety of downstream tasks\nlater on. For this purpose, we use the RoBERTa\n(Liu et al., 2019) pretraining technique. Previ-\nous pretrained transformers in Filipino (Cruz and\nCheng, 2020; Cruz et al., 2021) used BERT (Devlin\net al., 2018), and ELECTRA (Clark et al., 2020) as\ntheir method of choice.\nWe choose RoBERTa as it retains state-of-the-art\nperformance on multiple NLP tasks while keeping\nits pretraining task simple unlike methods such\nas ELECTRA. As a reproduction study of BERT,\nRoBERTa optimizes and builds up on the BERT\npretraining scheme to improve training efﬁciency\nand downstream performance.\nTwo size variants are trained in this study fol-\nlowing the original RoBERTa paper: a Base model\n(110M parameters) and a Large model (330M pa-\nrameters). Both size variants use the same BPE\ntokenizer trained with TLUniﬁed. Our hyperpa-\nrameter choices also follow the original RoBERTa\npaper closely. A summary of our models’ hyperpa-\nrameters can be found in Table 1.\nDuring training, we construct batches by con-\ntinually ﬁlling them with tokens until we reach a\nmaximum batch size of 8192 tokens. Both variants\nare trained using the Adafactor (Shazeer and Stern,\n2018) optimizer with β2 = 0.98 and a weight de-\ncay of 0.01. The base model is trained for 100,000\nsteps with a learning rate of 6e-4, while the large\nvariant is trained for 300,000 steps with a learning\nrate of 4e-4. We also use a learning rate sched-\nule that linearly warms up for 25,000 steps, then\nlinearly decays for the rest of training. All experi-\nments are done on a server with 8x NVIDIA Tesla\nP100 GPUs.\n2.3 Benchmark Datasets\nWe test the efﬁcacy of our RoBERTa models on\nthree Filipino benchmark datasets:\n• Filipino Hatespeech Dataset – 10,000\ntweets labelled as “hate” and “non-hate” col-\nlected during the 2016 Philippine Presidential\nElections. Originally published in Cabasag\net al. (2019) and benchmarked with modern\nTransformers in Cruz and Cheng (2020).\n• Filipino Dengue Dataset – Low-resource\nmulticlass classiﬁcation dataset with 4000\nsamples that can be one or many of ﬁve labels.\nOriginally published in Livelo and Cheng\n(2018) and benchmarked in Cruz and Cheng\n(2020) using pretrained Transformers.\n• NewsPH-NLI – An automatically-generated\ndataset constructed by exploiting the\n“inverted-pyramid” structure of news articles,\ncausing every sentence to naturally entail\nthe sentence that came before it. Originally\ncreated in Cruz et al. (2021).\nFor this study, we do not use the original\nNewsPH-NLI created in Cruz et al. (2021) as it has\nsigniﬁcant overlap with the subset of the NewsPH\ncorpus that we used for pretraining. We instead\nre-generated a version of NewsPH-NLI (which we\ncall “NewsPH-NLI Medium”) using 40% of the\nNewsPH corpus, using the other 60% as part of the\nTLUniﬁed pretraining data. This ensures that no\ntest data is present in the training data, which will\nsigniﬁcantly inﬂate the benchmark scores.\nPreprocessing for the downstream benchmark\ndatasets is kept simple and non-destructive to\npreserve the linguistic structures and information\npresent in the original data.\nFor the Hatespeech and the Dengue datasets, we\nfollow the preprocessing used in Cruz and Cheng\n(2020), with a number of changes. Since both are\ndatasets composed mainly of tweet data, the fol-\nlowing preprocessing steps are done:\n• Moses detokenization (Koehn and Hoang,\n2010) was applied on all Moses-tokenized\ntext.\n• All HTML meta text and link texts are col-\nlapsed into a special [LINK] token. This is\nto reduce the noise in the dataset as images in\nthe tweets are naturally converted into links.\n• All substrings that start with an @ character\nthat are greater than length 1 are automatically\ntreated as a “mention” and are replaced with\na [MENTION] special token.\n• All substrings that start with a# character that\nare greater than length 1 are automatically\ntreated as a “hashtag” and are replaced with a\n[HASHTAG] special token.\n• We renormalize apostrophes (e.g. it ’s →\nit’s) and punctuation that were spaced out\n(e.g. one - two → one-two) during the\npreprocessing in the Cruz and Cheng (2020)\npaper.\n• Characters that were converted into unicode\n(e.g. &amp;) are converted back into their\nencoded form (e.g. &).\nFor the Dengue dataset, we transform the mul-\ntilabel, multiclass classiﬁcation setup into a multi-\nclass classiﬁcation problem by concatenating an ex-\nample’s labels and converting the resulting binary\nnumber into an integer. For example, a sentence\nwith the labels 1, 1, 0, 1, 1 for absent,\ndengue, healthclasses, mosquito, and\nsick will be converted into 27 (11011 → 27).\nThis results in 32 possible labels and increases the\ndifﬁculty of the task.\nFor the NewsPH-NLI Medium dataset, we opted\nto not do any further preprocessing as the released\ndata from Cruz et al. (2021) is already preprocessed\nand clean.\n2.4 Finetuning Setups\nWe then ﬁnetune for the downstream benchmark\ntasks using our pretrained RoBERTa models. Since\nthe NewsPH-NLI version and the setup of the\nDengue dataset task is different from the previous\nbenchmarking paper, we also ﬁnetuned Tagalog\nBERT (Cruz and Cheng, 2019) and Tagalog ELEC-\nTRA (Cruz et al., 2021) to serve as baseline models\nagainst the new RoBERTa model.\nAll models are trained using the Adafactor\n(Shazeer and Stern, 2018) optimizer with a learning\nrate scheduler that linearly increases from zero after\na ratio of steps-to-total-training-steps has reached,\nthen linearly decays afterwards. We use a batch\nsize of 32 sentences for all models and use a weight\ndecay of 0.1. We opted to use a larger maximum\nsequence length for the Large RoBERTa models as\nit has more capacity due to its deeper encoder stack.\nHatespeech Dengue NewsPH-NLI Med.\nModel Val. Acc Test Acc. Val. Acc Test Acc. Val. Acc Test Acc.\nBERT Base Cased 0.7479 0.7417 0.7720 0.7580 0.8838 0.8874\nELECTRA Base Cased 0.7491 0.7250 0.7400 0.6920 0.9094 0.9106\nRoBERTa Base 0.7866 0.7807 0.8180 0.8020 0.9492 0.9501\nRoBERTa Large 0.7897 0.7824 0.8281 0.8110 0.9499 0.9510\nTable 2: Finetuning results for all Transformer variants on the three benchmark datasets.\nBase Large\nMax. Seq. Length 128 256\nLearning Rate 2e-5 1e-5\nWarmup Ratio 0.1 0.06\nTable 3: Unique ﬁnetuning hyperparameters for Base\nand Large transformer variants.\nHyperparameters that are different between Base\nand Large variants of the pretrained Transformers\nused are found in Table 3.\nWe add the [LINK], [MENTION], and\n[HASHTAG] special tokens during ﬁnetuning for\nthe Hatespeech and Dengue datasets to the vocab-\nularies of the Transformers used, averaging the\nvectors of all subword embeddings in the embed-\nding layer to serve as initialization for the three\nadded tokens.\nDespite our RoBERTa having a full maximum\nsequence length allowance of 512, we opted to use\nsmaller maximum sequence lengths during ﬁne-\ntuning. This speeds up training (approximately 4x\nfor the Base models and 2x for the Large models)\nwhile losing zero information since no sentence or\nsentence pair in any task reaches 256 subwords in\nlength.\nAll experiments are done on a server with 8x\nNVIDIA Tesla P100 GPUs.\n3 Results\nWe report the results for our ﬁnetuning for the three\nbenchmark datasets in terms of validation and test\naccuracy. A summary of the results can be found\non Table 2.\nOur RoBERTa models outperformed both the\nBERT and the ELECTRA models across all tasks.\nFor the Hatespeech task, RoBERTa Large outper-\nformed the best previous model (BERT Base) by\n+4.07% test accuracy. RoBERTa large also had a\ngain in performance in the Dengue dataset (+5.3%\ntest accuracy over BERT Base) and the NewsPH-\nNLI Medium dataset (+4.04% test accuracy over\nELECTRA Base).\nWhile marginally inferior to the Large variant,\nthe Base RoBERTa variant still outperforms the\nbaseline models in all tasks. RoBERTa Base has an\nimprovement of +3.9% against BERT Base on the\nHatespeech task, +4.4% against BERT Base on the\nDengue task, and +3.95% against ELECTRA Base\non the NewsPH-NLI Medium task.\nThe difference in performance between the Base\nand Large RoBERTa variants is marginal in the\ncurrent benchmarks. Large outperforms Base only\nby +0.17% for Hatespeech, +0.9% for Dengue, and\n+0.09% for NewsPH-NLI Medium. We hypoth-\nesize that this is due to the size of the pretrain-\ning dataset. While the size of TLUniﬁed is much\nlarger than the previous WikiText-TL-39, it may\nstill not be enough to make full use of the capacity\nof a Large-variant Transformer. We surmise that\nRoBERTa Large may need to be trained with more\ndata to show signiﬁcant, non-marginal improve-\nments in performance.\nOverall, our new models show signiﬁcant im-\nprovements over older pretrained Filipino Trans-\nformer models. This is likely due to the improved\npretraining corpus, with TLUniﬁed being larger\nand of more varied topics and sources than the\nprevious WikiText-TL-39.\n4 Conclusion\nOur work has two main contributions in terms of\nlanguage resources for the Filipino language. First,\nwe construct TLUniﬁed, a new large-scale pretrain-\ning corpus for Filipino. This is an improvement\nover the much smaller pretraining corpora currently\navailable, boasting much larger scale and topic va-\nriety. Second, we release new pretrained Trans-\nformers using the RoBERTa pretraining method.\nOur new models outperform existing baselines on\nthree different classiﬁcation tasks, with signiﬁcant\nimprovements of +4.07%, +5.03%, and +4.04%\ntest accuracy for the Hatespeech, Dengue, and\nNewsPH-NLI Medium datasets respectively.\nReferences\nNeil Vicente Cabasag, Vicente Raphael Chan,\nSean Christian Lim, Mark Edward Gonzales, and\nCharibeth Cheng. 2019. Hate speech in philippine\nelection-related tweets: Automatic detection and\nclassiﬁcation using natural language processing.\nPhilippine Computing Journal, XIV No, 1.\nChristos Christodouloupoulos and Mark Steedman.\n2015. A massively parallel corpus: the bible in\n100 languages. Language resources and evaluation,\n49(2):375–395.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJan Christian Blaise Cruz and Charibeth Cheng.\n2019. Evaluating language model ﬁnetuning tech-\nniques for low-resource languages. arXiv preprint\narXiv:1907.00409.\nJan Christian Blaise Cruz and Charibeth Cheng.\n2020. Establishing baselines for text classiﬁca-\ntion in low-resource languages. arXiv preprint\narXiv:2005.02068.\nJan Christian Blaise Cruz, Jose Kristian Resabal, James\nLin, Dan John Velasco, and Charibeth Cheng. 2021.\nExploiting news article structure for automatic cor-\npus generation of entailment datasets. In Paciﬁc Rim\nInternational Conference on Artiﬁcial Intelligence ,\npages 86–99. Springer.\nShumin Deng, Ningyu Zhang, Zhanlin Sun, Jiaoyan\nChen, and Huajun Chen. 2020. When low resource\nnlp meets unsupervised language model: Meta-\npretraining then meta-learning for few-shot text clas-\nsiﬁcation (student abstract). In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 13773–13774.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAhmed El-Kishky, Vishrav Chaudhary, Francisco\nGuzmán, and Philipp Koehn. 2020. CCAligned: A\nmassive collection of cross-lingual web-document\npairs. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2020), pages 5960–5969, Online. Associa-\ntion for Computational Linguistics.\nPhilipp Koehn and Hieu Hoang. 2010. Moses. Statis-\ntical Machine Translation System, User Manual and\nCode Guide, page 245.\nHung-yi Lee, Ngoc Thang Vu, and Shang-Wen Li.\n2021. Meta learning and its applications to natural\nlanguage processing. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing: Tutorial Ab-\nstracts, pages 15–20.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nEvan Dennison Livelo and Charibeth Cheng. 2018. In-\ntelligent dengue infoveillance using gated recurrent\nneural learning and cross-label frequencies. In 2018\nIEEE International Conference on Agents (ICA) ,\npages 2–7. IEEE.\nRowan Hall Maudslay and Ryan Cotterell. 2021.\nDo syntactic probes probe syntax? experi-\nments with jabberwocky probing. arXiv preprint\narXiv:2106.02559.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for pro-\ncessing huge corpora on medium to low resource\ninfrastructures. Proceedings of the Workshop on\nChallenges in the Management of Large Corpora\n(CMLC-7) 2019. Cardiff, 22nd July 2019, pages\n9 – 16, Mannheim. Leibniz-Institut für Deutsche\nSprache.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2019. Wiki-\nmatrix: Mining 135m parallel sentences in 1620\nlanguage pairs from wikipedia. arXiv preprint\narXiv:1907.05791.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning ,\npages 4596–4604. PMLR.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In Proceedings of the Eight Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC’12), Istanbul, Turkey. European Lan-\nguage Resources Association (ELRA)."
}