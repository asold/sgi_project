{
  "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
  "url": "https://openalex.org/W4385571002",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103815782",
      "name": "Jianyi Zhang",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A3109225483",
      "name": "Aashiq Muhamed",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2101174865",
      "name": "Aditya Anantharaman",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2109369421",
      "name": "Guoyin Wang",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2143295056",
      "name": "Changyou Chen",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A1894762881",
      "name": "Kai Zhong",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2131789145",
      "name": "Qingjun Cui",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2080000625",
      "name": "Yi Xu",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A3167139966",
      "name": "Belinda Zeng",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2077038638",
      "name": "Trishul Chilimbi",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2111708568",
      "name": "Yiran Chen",
      "affiliations": [
        "Duke University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3102839769",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963140444",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4224875176",
    "https://openalex.org/W4298857601",
    "https://openalex.org/W2885421725",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2981819252",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4285202066",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W3034999720",
    "https://openalex.org/W3204191745",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W2962922117",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3130315600",
    "https://openalex.org/W3131861520",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W3027879771"
  ],
  "abstract": "Jianyi Zhang, Aashiq Muhamed, Aditya Anantharaman, Guoyin Wang, Changyou Chen, Kai Zhong, Qingjun Cui, Yi Xu, Belinda Zeng, Trishul Chilimbi, Yiran Chen. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1128–1136\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained\nLanguage Models\nJianyi Zhang1, Aashiq Muhamed2, Aditya Anantharaman2, Guoyin Wang2,\nChangyou Chen3, Kai Zhong2, Qingjun Cui2, Yi Xu2,\nBelinda Zeng2, Trishul Chilimbi2, Yiran Chen1\n{jianyi.zhang,yiran.chen}@duke.edu, changyou@buffalo.edu,\n{muhaaash,aditanan,guoyiwan,kaizhong,qingjunc,zengb,trishulc}@amazon.com,\n1 Duke University, 2 Amazon 3 University at Buffalo, SUNY\nAbstract\nKnowledge Distillation (KD) (Hinton et al.,\n2015) is one of the most effective approaches\nfor deploying large-scale pre-trained language\nmodels in low-latency environments by trans-\nferring the knowledge contained in the large-\nscale models to smaller student models. Previ-\nous KD approaches use the soft labels and in-\ntermediate activations generated by the teacher\nto transfer knowledge to the student model\nparameters alone. In this paper, we show\nthat having access to non-parametric mem-\nory in the form of a knowledge base with the\nteacher’s soft labels and predictions can fur-\nther enhance student capacity and improve gen-\neralization. To enable the student to retrieve\nfrom the knowledge base effectively, we pro-\npose a new Retrieval-augmented KD frame-\nwork with a loss function that aligns the rela-\ntional knowledge in teacher and student em-\nbedding spaces. We show through extensive\nexperiments that our retrieval mechanism can\nachieve state-of-the-art performance for task-\nspecific knowledge distillation on the GLUE\nbenchmark (Wang et al., 2018a).\n1 Introduction\nLarge pre-trained language models, such as BERT\n(Devlin et al., 2018), RoBERTa (Liu et al., 2019)\nand Electra (Clark et al., 2020) have achieved sig-\nnificant success on several different NLP tasks\n(Ding et al., 2019; Wang et al., 2018a) with fine-\ntuning. However, these models usually contain mil-\nlions and billions of parameters, preventing their\nexecution on resource-restricted devices. To de-\nploy these models, Knowledge distillation (KD)\nis an effective compression technique to derive a\nsmaller student model from a larger teacher model\nby transferring the knowledge embedded in the\nteacher’s network. Previous KD methods typically\nstore knowledge in the student’s parameters and\ntrain the student by minimizing divergence between\nthe student’s and teacher’s output prediction and\nintermediate activation distributions (Park et al.,\n2019; Zhang et al., 2018). However, the student’s\nparametric memory is often limited and cannot be\nquickly expanded or revised. Moreover, after train-\ning, the teacher model’s soft labels and activations,\nwhich contain essential task-specific knowledge,\nare not utilized by the student at inference time.\nTo address the issues mentioned above, we pro-\npose the Retrieval-augmented Knowledge Distil-\nlation (ReAugKD) framework. ReAugKD intro-\nduces a non-parametric external memory in addi-\ntion to the implicit parametric memory of the model\nand uses kNN retrieval to retrieve from this mem-\nory. The key intuition of ReAugKD is to enhance\nthe effective capacity of the student by using an ex-\nternal memory derived from relevant task-specific\nknowledge of the teacher. While this external mem-\nory could include any task-specific knowledge, in\nthis work, it is composed of the soft labels and\nembeddings generated by the teacher model.\nOur framework consists of an inference phase\nand a training phase. In the inference phase, we\naggregate the soft labels of those teacher embed-\ndings in our memory that are most similar to the\nstudent embedding. We demonstrate the efficacy\nof our framework by achieving state-of-the-art re-\nsults on the GLUE benchmark (Wang et al., 2018a)\nwith less than 3% latency overhead over the base-\nline without retrieval augmentation. ReAugKD\nalso comprises a training phase, where we train the\nstudent to retrieve from the external memory effec-\ntively. We train with a novel relational KD loss that\nminimizes the divergence between teacher-teacher\nand teacher-student embedding distributions. We\nnot only observe that training with this loss is nec-\nessary to align the student and teacher embedding\nspaces for retrieval but also that this loss improves\nstudent generalization even in the absence of re-\ntrieval augmentation. This suggests that incorpo-\nrating the ability to retrieve information can signif-\nicantly enhance generalization during the process\n1128\nof knowledge distillation.\nIn summary, our contributions include\n• We propose ReAugKD, a novel framework\nfor knowledge distillation that introduces a non-\nparametric memory to increase the effective stu-\ndent size. We show that retrieving from a memory\ncomposed of training set teacher predictions at in-\nference time can significantly improve generaliza-\ntion on the GLUE tasks.\n•To effectively retrieve from the non-parametric\nmemory, we introduce a novel loss function that\ntransfers the relational knowledge between teacher-\nteacher embedding and teacher-student embedding\ndistribution. This loss function improves student\ngeneralization even in the absence of retrieval aug-\nmentation at inference time.\n•We study the accuracy and latency cost with the\nnumber of neighbors (k) retrieved in ReAugKD.\nReAugKD with approximate kNN introduces a\nsmall overhead of <3% latency increase.\n2 Related Work\nKnowledge distillationKD can be broadly classi-\nfied into task-specific KD, where the student model\nwill be used for the same task as the teacher model\n(Mirzadeh et al., 2020; Jin et al., 2019; Zhang et al.,\n2018; Sun et al., 2019) and task-agnostic KD where\nthe student may be used for a different task, after\nfinetuning on the new task (Jiao et al., 2019; Sun\net al., 2020; Sanh et al., 2019; Wang et al., 2020;\nZhang et al., 2018; Xu et al., 2019). In this work,\nwe show that ReAugKD can be applied to enhance\ntask-specific distillation as well as when finetuning\ntask-agnostic distilled models. Closest to our work\nis RKD (Park et al., 2019) that introduces a loss\nto transfer relational knowledge between teacher-\nteacher embedding and student-student embedding\ndistributions. Our work differs in that we trans-\nfer relational knowledge between teacher-teacher\nembedding and teacher-student embedding distri-\nbution to enhance the student model’s ability to re-\ntrieve from the external memory. MetaDistil (Zhou\net al., 2022) is a strong task-specific distillation\nbaseline that employs meta-learning to better trans-\nfer knowledge to the student. Unlike MetaDistill,\nwe show that ReAugKD can significantly improve\nthe student model’s generalization without retrain-\ning the whole teacher with meta-learning.\nRetrieval-augmented language models There\nhas been growing interest in retrieval-augmented\nmethods for Knowledge-Intensive generative NLP\nTasks, such as text generation and question an-\nswering (Weston et al., 2018; Lewis et al., 2020;\nGuu et al., 2020; Lin et al., 2022), where querying\ntraining examples during inference significantly\nimproves likelihood. Closest to our work is BERT-\nkNN (Kassner and Schütze, 2020) which combines\nBERT with a kNN search over a large datastore\nof an embedded text collection, to improve cloze-\nstyle QA. In our work, we apply retrieval augmen-\ntation to enhance the capacity of student models\nduring KD, and show improvement even on non-\nknowledge intensive tasks like GLUE.\n3 Methodology\n3.1 Training Phase\nOur framework consists of two main phases, the\ntraining phase and the inference phase. The train-\ning phase has two steps. In the first step, we prepare\nthe teacher model for KD by adding a linear projec-\ntion head Lon the top of the teacher model encoder\nthat has been finetuned for a specific downstream\ntask. The input dimension of this projection head\nis the embedding dimension of the teacher. The\noutput dimension is the embedding dimension of\nthe student. We then freeze the other parameters of\nthe teacher model and finetune the parameters in\nLwith supervised contrastive loss (Khosla et al.,\n2020). This step a) reduces the dimension of the\nteacher’s embeddings, to the student model dimen-\nsion for retrieval, and b) uses supervised contrastive\nloss to derive a kNN classifier for BERT that is ro-\nbust to natural corruptions, and hyperparameter\nsettings (Li et al., 2021). Fine-tuning Lalso greatly\nreduces the computational cost compared to retrain-\ning the whole teacher model (Zhou et al., 2022).\nIn the second step, we perform KD by generating\nthe teacher embeddings with Land teacher soft\nlabels using the original teacher’s classifier head\nfor a batch of data. Then, we use the loss function\nwe proposed in Section 3 to train our student model.\n3.2 Loss function\nWe present some mathematical notations to intro-\nduce our loss function. Given a batch of data\n{di},i = 1 ,2,··· ,N, where N is the batch\nsize, we denote the embedding generated by the\nteacher’s projection head as zi and the soft labels\ngenerated by the teacher’s classifier as¯yi. Similarly,\nwe adopt xi,yi to denote the student’s embeddings\nand predictions. Then we construct a probability\ndistribution qi,j over each teacher’s embeddings zj\n1129\nFigure 1: Training and Inference (Testing) phases of Retrieval-augmented Knowledge Distillation (ReAugKD).\nto capture the similarity with respect to an anchor\npoint zi,\nqi,j = exp (zi ·zj)/τ∑N\nk=1 exp (zi ·zk)/τ\n, (1)\nwhere the τ stands for temperature. Note that∑N\nj=1 qi,j = 1 . qi,j reflects the cosine distance\nrelational knowledge among different embeddings\ngenerated by the teacher model in the batch. If zj\nis closer to zi, cosine distance, qi,j will be larger.\nSimilarly, given a student’s embedding xi as an\nanchor point, we formulate another probability dis-\ntribution ¯qi,j over each teacher’s embeddings zj of\nthe data in the batch.\n¯qi,j = exp (xi ·zj)/τ∑N\nk=1 exp (xi ·zk)/τ\n. (2)\nThe ¯qi,j reflects the cosine distance relationship\nbetween different embeddings generated by the\nteacher model and the student’s embedding. Our\nloss function aims to minimize the divergence of\nthese two distributions ¯qi,j and qi,j since the teacher\nmodel is a strong kNN classifier after finetuning\nwith supervised contrastive loss function in the first\nstep of our training. In the ideal case, given a stu-\ndent’s embedding xi, the student retriever should\nretrieve the same set of embeddings as the corre-\nsponding teacher’s embedding zi. We adopt KL\ndivergence to measure that divergence. In addition,\nwe adopt the commonly-used cross-entropy loss\nto calculate the divergence between the student’s\nprediction yi and the teacher’s prediction ¯yi.\nOur loss function can be formulated as\nCE(yi,¯yi) + αKL(qi,j,¯qi,j), (3)\nwhere CE is the cross entropy loss and KLis KL-\ndivergence. αis the hyperparameter controlling the\ntrade-off between the two losses.\n3.3 Inference Phase\nAfter training, we construct a knowledge base (KB)\ncomprising of projected teacher embeddings and\npredictions. Given new data di at inference time,\nwe obtain (xi,yi) using the student model. and use\nthe HNSW algorithm (Malkov and Yashunin, 2018)\nto derive the Knearest teacher’s embeddings and\ntheir corresponding soft labels {(zk,¯yk)}i=1,2,···,K\nfrom the KB. Then we compute the weighted av-\nerage of these soft labels Avg({¯y})i based on ¯qi,k\nAvg({y})i =\nK∑\nk=1\n¯qi,k\n∑K\nk=1 ¯qi,k\n¯yk\nWe derive a new prediction ¯y′\ni for di with\nAvg({¯y})i.\n¯y′\ni = β¯yi + (1 −β)Avg({¯y})i,\nβ is the hyperparameter controlling the trade-off\nbetween the two predictions.\n4 Experimental Results\nWe apply our method to distill BERT-Base (Devlin\net al., 2018) into a 6-layer BERT with a hidden\nsize of 768. We evaluate our proposed approach,\nReAugKD, on the GLUE benchmark (Wang et al.,\n2018a). These datasets can be broadly divided\ninto three families of problems: single-set tasks\nthat include linguistic acceptability (CoLA) and\nsentiment analysis (SST-2), similarity, and para-\nphrasing tasks (MRPC and QQP); inference tasks\n1130\nMethod #Param GLUE\nCoLA\n(8.5k)\nQNLI\n(105k)\nQQP\n(364k)\nRTE\n(2.5k)\nSST-2\n(67k)\nMRPC\n(3.7k) Avg\nBERT-Base (teacher) (Devlin et al., 2018) 110M 58.9 91.2 91.4 71.4 93.0 87.6 82.25\nBERT-6L (student)(Turc et al., 2019) 66M 53.5 88.6 90.4 67.9 91.1 84.4 79.32\nTask-specific Distillation\nKD (Hinton et al., 2015) 66M 54.1 89.2 90.9 67.7 91.2 85.2 79.72\nPKD (Sun et al., 2019) 66M 54.5 89.5 90.9 67.6 91.3 84.7 79.75\nTinyBERT w/o DA (Jiao et al., 2019) 66M 52.4 89.8 90.6 67.7 91.9 86.5 79.82\nRCO (Jin et al., 2019) 66M 53.6 89.7 90.6 67.6 91.4 85.1 79.67\nTAKD (Mirzadeh et al., 2020) 66M 53.8 89.6 90.7 68.5 91.4 85.0 79.83\nRKD (Park et al., 2019) 66M 53.4 89.5 90.9 68.6 91.7 86.1 80.03\nDML (Zhang et al., 2018) 66M 53.7 89.6 90.3 68.4 91.5 85.1 79.77\nProKT (Shi et al., 2020) 66M 54.3 89.7 90.9 68.4 91.3 86.3 80.15\nSFTN (Park et al., 2021) 66M 53.6 89.5 90.4 68.5 91.5 85.3 79.80\nMetaDistil (Zhou et al., 2022) 66M 58.6 90.4 91.0 69.4 92.3 86.8 81.42\nReAugKD (ours) 66M 59.4 90.7 91.24 70.39 92.5 86.3 81.76\nReAugKD w/o retrieval 66M 59.1 90.6 91.21 69.31 92.3 85.8 81.39\nTable 1: Experimental results of ReAugKD and other previous works on the development set of GLUE. Numbers\nunder each dataset indicate the number of training samples. The results of the baselines are from (Zhou et al., 2022).\nWe report Matthew’s correlation coefficient for CoLA and accuracy for other datasets.\nthat include Natural Language Inference (MNLI\nand RTE); and Question Answering (QNLI). We\ncompare our method with vanilla KD (Hinton et al.,\n2015), TAKD (Mirzadeh et al., 2020), RCO (Jin\net al., 2019), RKD (Park et al., 2019), DML (Zhang\net al., 2018), PKD (Sun et al., 2019) ProKT (Shi\net al., 2020), SFTN (Park et al., 2021) and MetaDis-\ntil (Zhou et al., 2022). Following similar setting\nas MetaDistill, we perform a grid search over the\nsets of the weight of KD loss from {0.9, 0.99}, the\npredictions weight β from {0, 0.1, ... 1} and the\ntop-kfrom 1 to 20. We set the student learning rate\nto 2e-5 and the batch size to 64.\nExperimental Results on GLUEWe report the\nexperimental results on the development set of the\nsix GLUE tasks in Table 1. Notably, our method\nachieves start-of-the-art results on five out of the\nsix datasets with an average improvement of0.34%\nover the previous best KD method MetaDistil\n(Zhou et al., 2022). Although MetaDistil achieves\nslightly better performance on the MRPC dataset,\nour method has the advantage of not needing to\nconduct meta-learning on the whole large teacher\nmodel, which significantly increases extra training\ncost in terms of time and memory (Zhou et al.,\n2022). In addition, we also observe a performance\ngain of 0.37% with the retrieval component of\nReAugKD as compared to ReAugKD without re-\ntrieval which verifies the benefit of retrieval aug-\nmentation in our approach. Even without the re-\ntrieval process, the student model trained by our\nMethod QNLI SST-2 CoLAacc time acc time mcc timeReAugKD w/o Retrieval90.645.70s92.3 7.80s 59.1 8.67sReAugKD (k=5)90.72+1.31s92.43+0.199s58.87+0.143sReAugKD (k=10)90.70+1.32s92.54+0.201s59.39+0.147sReAugKD (k=15)90.74+1.33s92.54+0.202s59.35+0.147sReAugKD (k=20)90.72+1.33s92.43+0.204s59.37+0.148s\nTable 2: Analysis of the sensitivity of top k on model\nperformance and retrieval time\ndesigned loss can still achieve comparable perfor-\nmance to MetaDistill on most datasets. Since our\nloss is designed to improve the student retrieval\nfunction, this demonstrates the importance of re-\ntrieval capability in KD.\nNumber of Neighbors Retrieved (k)To under-\nstand the time overhead of retrieval on the student\nmodel’s inference time, we investigate the perfor-\nmance and additional time overhead of the retrieval\nprocess while varying the number of neighbors re-\ntrieved (k) in Table 2. From the results, it is clear\nthat retrieval improves the student model perfor-\nmance with an additional time overhead of less\nthan 3% of the original inference time. The re-\ntrieval process is conducted only on CPU, and does\nnot take up GPU resources during training.\n5 Conclusion\nIn this paper, we present ReAugKD, a knowl-\nedge distillation framework with a retrieval mech-\nanism that shows state-of-the-art performance on\nthe GLUE benchmark. In the future, we plan to\nexpand the knowledge base with more information\nfrom the teacher and extend it to additional tasks.\n1131\nLimitations Our method relies on having access\nto teacher embeddings and prediction which may\nnot always be possible in a black-box distillation\nsetting. Retrieval augmentation also requires main-\ntaining a knowledge base that is memory intensive.\nThe cost of the retrieval process is dependent on the\nsize of the training corpus, which can be a limita-\ntion when dealing with very large training datasets.\nConducting dataset distillation (Wang et al., 2018b)\non the training corpus to further reduce memory\ncost and retrieval time is an important future step\nfor our framework.\nAcknowledgments This work was done when\nJianyi Zhang was an intern at Amazon Search. In\naddition, Jianyi Zhang and Yiran Chen disclose\nsupport from grants CNS-2112562, IIS-2140247,\nand CNS-1822085. We thank Yuchen Bian for the\nvaluable discussion and thank all reviewers for their\nvaluable comments.\nReferences\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMing Ding, Chang Zhou, Qibin Chen, Hongxia Yang,\nand Jie Tang. 2019. Cognitive graph for multi-\nhop reading comprehension at scale. arXiv preprint\narXiv:1905.05460.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929–3938.\nPMLR.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2019.\nTinybert: Distilling bert for natural language under-\nstanding. arXiv preprint arXiv:1909.10351.\nXiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng\nLiu, Ding Liang, Junjie Yan, and Xiaolin Hu. 2019.\nKnowledge distillation via route constrained opti-\nmization. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 1345–\n1354.\nNora Kassner and Hinrich Schütze. 2020. Bert-\nknn: Adding a knn search component to pretrained\nlanguage models for better qa. arXiv preprint\narXiv:2005.00766.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. Advances in Neural\nInformation Processing Systems, 33:18661–18673.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nLinyang Li, Demin Song, Ruotian Ma, Xipeng Qiu, and\nXuanjing Huang. 2021. Knn-bert: fine-tuning pre-\ntrained models with knn classifier. arXiv preprint\narXiv:2110.02523.\nBill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen\nTian, and Xiang Ren. 2022. Unsupervised cross-\ntask generalization via retrieval augmentation. arXiv\npreprint arXiv:2204.07937.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYu A Malkov and Dmitry A Yashunin. 2018. Efficient\nand robust approximate nearest neighbor search us-\ning hierarchical navigable small world graphs. IEEE\ntransactions on pattern analysis and machine intelli-\ngence, 42(4):824–836.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distil-\nlation via teacher assistant. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 5191–5198.\nDae Young Park, Moon-Hyun Cha, Daesin Kim, Bo-\nhyung Han, et al. 2021. Learning student-friendly\nteacher networks for knowledge distillation. Ad-\nvances in Neural Information Processing Systems ,\n34:13292–13303.\nWonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.\n2019. Relational knowledge distillation. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3967–3976.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nWenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li, and\nLei Li. 2020. Learning from deep model via explor-\ning local targets.\n1132\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nTongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and\nAlexei A Efros. 2018b. Dataset distillation. arXiv\npreprint arXiv:1811.10959.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2020. Minilmv2: Multi-head\nself-attention relation distillation for compress-\ning pretrained transformers. arXiv preprint\narXiv:2012.15828.\nJason Weston, Emily Dinan, and Alexander H Miller.\n2018. Retrieve and refine: Improved sequence\ngeneration models for dialogue. arXiv preprint\narXiv:1808.04776.\nYuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang,\nWenrui Dai, Yingyong Qi, Yiran Chen, Weiyao Lin,\nand Hongkai Xiong. 2019. Trained rank pruning for\nefficient deep neural networks. In 2019 Fifth Work-\nshop on Energy Efficient Machine Learning and Cog-\nnitive Computing - NeurIPS Edition (EMC2-NIPS),\npages 14–17.\nYing Zhang, Tao Xiang, Timothy M Hospedales, and\nHuchuan Lu. 2018. Deep mutual learning. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 4320–4328.\nWangchunshu Zhou, Canwen Xu, and Julian McAuley.\n2022. Bert learns to teach: Knowledge distillation\nwith meta learning. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7037–\n7049.\n1133\nA Appendix\nA.1 ReAugKD with task-agnostic distillation\nModel #Param QNLI QQP RTE SST-2 MRPC MNLI-m CoLA Avg\nTeacher Model (24×1024 RoBERTa-large (Liu et al., 2019))\nRoBERTa-large 354M 94.7 92.2 86.6 96.4 90.9 90.2 68 88.43\nDistilled Student Model (6×768 MiniLMv2)\nPretraining Distillation 81M 92.7 91.4 78.7 94.5 90.4 87.0 54.0 83.8\nReAugKD 81M 93.1 91.9 80.5 95.0 90.2 88.5 57.9 85.30\nReAugKD w/o Retrieval 81M 93.0 91.8 79.8 94.9 90.2 88.3 57.2 85.02\nTable 3: Results of our method improving finetuned task performance of MiniLMv2\nPrevious results have demonstrated the effectiveness of our method for task-specific distillation. Our\nmethod can further improve the finetuned performance of task-agnostic distilled models. We adopt\nRoBERTa-large as the teacher model and the MiniLMv2 as the student model to verify the effectiveness\nof our method. Our method can achieve around 2% improvement in performance.\nA.2 Details about training teacher model’s projection head\nWe adopt the Lsup\nout version of the loss function in (Khosla et al., 2020) to finetune the parameters of the\nprojection head, which is\nLsup\nout = −\nN∑\ni=1\n1\nN\n∑\nj∈P(i)\nlog exp (zi ·zj) /τ∑N\nk=1 exp (zi ·zk) /τ\n. (4)\nHere, there are N data samples di in the batch and we denote the embedding generated by the teacher’s\nprojection head for the i-th data di as zi. P(i) here represents the set of all the positive data samples for\ndata di. The data samples from the same class are considered as positive pairs and the data samples from\ndifferent classes are considered as negative pairs. Regarding the use of data augmentation in training the\nprojection head, we chose not to adopt data augmentation as we found that using supervised contrastive\nloss without data augmentation was sufficient to achieve results comparable to the cross-entropy loss used\nin supervised learning. We use the AdamW optimizer with a learning rate of 0.00002. The batch size was\nset to 512, and the temperature for the supervised contrastive loss (SCL) was set to 0.07. We trained the\nmodel 3 epochs.\n1134\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIt lies before the reference.\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe think our work will not have any potential risk.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1135\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3\n□\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nThe packages we used are conﬁdential due to our company’s policy\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1136",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.910679042339325
    },
    {
      "name": "Zhàng",
      "score": 0.6950421333312988
    },
    {
      "name": "Computer science",
      "score": 0.5542216300964355
    },
    {
      "name": "Natural language processing",
      "score": 0.47875064611434937
    },
    {
      "name": "Association (psychology)",
      "score": 0.4755116105079651
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4634518325328827
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4576176702976227
    },
    {
      "name": "Distillation",
      "score": 0.42047154903411865
    },
    {
      "name": "Philosophy",
      "score": 0.1381002962589264
    },
    {
      "name": "Chemistry",
      "score": 0.1155034601688385
    },
    {
      "name": "History",
      "score": 0.08433473110198975
    },
    {
      "name": "Epistemology",
      "score": 0.07712185382843018
    },
    {
      "name": "Physics",
      "score": 0.06340271234512329
    },
    {
      "name": "Chromatography",
      "score": 0.06075626611709595
    },
    {
      "name": "Archaeology",
      "score": 0.05425277352333069
    },
    {
      "name": "Geology",
      "score": 0.05338209867477417
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "China",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I63190737",
      "name": "University at Buffalo, State University of New York",
      "country": "US"
    }
  ]
}