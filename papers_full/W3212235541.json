{
  "title": "Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction",
  "url": "https://openalex.org/W3212235541",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221377233",
      "name": "Cai, Yuanhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2016203565",
      "name": "Lin Jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221377232",
      "name": "Hu, Xiaowan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2361325101",
      "name": "Wang Hao-qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104307075",
      "name": "Yuan Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2633414948",
      "name": "Zhang, Yulun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744068600",
      "name": "Timofte, Radu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742174613",
      "name": "Van Gool Luc",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2097889461",
    "https://openalex.org/W2135364872",
    "https://openalex.org/W3134510327",
    "https://openalex.org/W2163753106",
    "https://openalex.org/W3203925315",
    "https://openalex.org/W2136251662",
    "https://openalex.org/W2084591647",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3035466729",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3204367729",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W1992653101",
    "https://openalex.org/W2951405960",
    "https://openalex.org/W2028349405",
    "https://openalex.org/W2462946880",
    "https://openalex.org/W2964984565",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2170608472",
    "https://openalex.org/W3096654432",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W2097273723",
    "https://openalex.org/W3202550138",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W2963764784",
    "https://openalex.org/W2118103795",
    "https://openalex.org/W2154761409",
    "https://openalex.org/W2010797000",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2511401065",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W2533971697",
    "https://openalex.org/W2770113520",
    "https://openalex.org/W2144412886",
    "https://openalex.org/W1529476013",
    "https://openalex.org/W1975622988",
    "https://openalex.org/W1538237237",
    "https://openalex.org/W2588000623",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2990070112",
    "https://openalex.org/W2983736948",
    "https://openalex.org/W1988386267",
    "https://openalex.org/W3176892444",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2884144629",
    "https://openalex.org/W1986701690",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3041490661",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3173125503"
  ],
  "abstract": "Hyperspectral image (HSI) reconstruction aims to recover the 3D spatial-spectral signal from a 2D measurement in the coded aperture snapshot spectral imaging (CASSI) system. The HSI representations are highly similar and correlated across the spectral dimension. Modeling the inter-spectra interactions is beneficial for HSI reconstruction. However, existing CNN-based methods show limitations in capturing spectral-wise similarity and long-range dependencies. Besides, the HSI information is modulated by a coded aperture (physical mask) in CASSI. Nonetheless, current algorithms have not fully explored the guidance effect of the mask for HSI restoration. In this paper, we propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI reconstruction. Specifically, we present a Spectral-wise Multi-head Self-Attention (S-MSA) that treats each spectral feature as a token and calculates self-attention along the spectral dimension. In addition, we customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to spatial regions with high-fidelity spectral representations. Extensive experiments show that our MST significantly outperforms state-of-the-art (SOTA) methods on simulation and real HSI datasets while requiring dramatically cheaper computational and memory costs. Code and pre-trained models are available at https://github.com/caiyuanhao1998/MST/",
  "full_text": "Mask-guided Spectral-wise Transformer for\nEfficient Hyperspectral Image Reconstruction\nYuanhao Cai 1,2,‚àó, Jing Lin 1,2,* , Xiaowan Hu 1,2, Haoqian Wang 1,2,‚Ä†,\nXin Yuan 3, Yulun Zhang 4, Radu Timofte 4, and Luc Van Gool 4\n1 Shenzhen International Graduate School, Tsinghua University,\n2 Shenzhen Institute of Future Media Technology,3 Westlake University,4 ETH Z¬®urich\nAbstract\nHyperspectral image (HSI) reconstruction aims to re-\ncover the 3D spatial-spectral signal from a 2D measure-\nment in the coded aperture snapshot spectral imaging\n(CASSI) system. The HSI representations are highly similar\nand correlated across the spectral dimension. Modeling the\ninter-spectra interactions is beneficial for HSI reconstruc-\ntion. However, existing CNN-based methods show limita-\ntions in capturing spectral-wise similarity and long-range\ndependencies. Besides, the HSI information is modulated\nby a coded aperture (physical mask) in CASSI. Nonetheless,\ncurrent algorithms have not fully explored the guidance ef-\nfect of the mask for HSI restoration. In this paper, we pro-\npose a novel framework, Mask-guided Spectral-wise Trans-\nformer (MST), for HSI reconstruction. Specifically, we\npresent a Spectral-wise Multi-head Self-Attention (S-MSA)\nthat treats each spectral feature as a token and calculates\nself-attention along the spectral dimension. In addition, we\ncustomize a Mask-guided Mechanism (MM) that directs S-\nMSA to pay attention to spatial regions with high-fidelity\nspectral representations. Extensive experiments show that\nour MST significantly outperforms state-of-the-art (SOTA)\nmethods on simulation and real HSI datasets while requir-\ning dramatically cheaper computational and memory costs.\nhttps://github.com/caiyuanhao1998/MST/\n1. Introduction\nHyperspectral imaging refers to multi-channel imaging\nwhere each channel captures the information at a specific\nspectral wavelength for a real-world scene. Generally, hy-\nperspectral images (HSIs) have more spectral bands than\nnormal RGB images to store richer information and de-\nlineate more detailed characteristics of the imaged scene.\nRelying on this property, HSIs have been widely applied\nto many computer vision related tasks, e.g., remote sens-\ning [5, 34, 59], object tracking [21, 40], medical image pro-\n*Equal Contribution, ‚Ä† Corresponding Author\n10 1 10 2 10 3\nFLOPS (G)\n26\n28\n30\n32\n34\n36PSNR (dB)\nMST-S(0.93M)MST-M(1.50M)\nMST-L(2.03M)\nDIP-HSI(33.85M)\nDGSMP(3.76M)TSA-Net(44.25M)\nDNU(1.19M)\nùúÜ-Net(62.64M)U-Net(31.32M)\nFigure 1. PSNR-Params-FLOPS comparisons with CNN-based\nHSI reconstruction methods. The vertical axis is PSNR (in dB per-\nformance), the horizontal axis is FLOPS (computational cost), and\nthe circle radius is Params (memory cost). Our proposed Mask-\nguided Spectral-wise Transformers (MSTs) outperform previous\nmethods while requiring significantly cheaper FLOPS and Params.\ncessing [3, 33, 37],etc. To collect HSIs, conventional imag-\ning systems with spectrometers scan the scenes along the\nspatial or spectral dimension, usually requiring a long time.\nTherefore, these traditional imaging systems are unsuitable\nfor capturing and measuring dynamic scenes. Recently, re-\nsearchers have used snapshot compressive imaging (SCI)\nsystems to capture HSIs. These SCI systems compress in-\nformation of snapshots along the spectral dimension into\none single 2D measurement [58]. Among current existing\nSCI systems [10,16,32,45,47], the coded aperture snapshot\nspectral imaging (CASSI) [36,45] stands out and forms one\npromising mainstream research direction.\nBased on CASSI, a large number of reconstruction algo-\nrithms have been proposed to recover the 3D HSI cube from\nthe 2D measurement. Conventional model-based methods\nadopt hand-crafted priors such as sparsity [23, 29, 46], total\nvariation [24, 51, 57], and non-local similarity [30, 52, 61]\nto regularize the reconstruction procedure. However, these\nmethods need to tweak parameters manually, resulting in\npoor generalization ability, unsatisfactory reconstruction\nquality, and slow restoration speed. With the development\nof deep learning, HSI reconstruction has witnessed signif-\nicant progress. Deep convolutional neural network (CNN)\napplies a powerful model to learn the end-to-end mapping\nfunction from the 2D measurement to the 3D HSI cube. Al-\nthough impressive results have been achieved, CNN-based\nmethods [20, 35, 36, 39] show limitations in modeling the\ninter-spectra similarity and long-range dependencies. Be-\nsides, the HSIs are modulated by a physical mask in CASSI.\nNonetheless, previous CNN-based methods [35, 36, 38, 48]\nmainly adopt the inner product between the mask and the\nshifted measurement as the input. This scheme corrupts the\ninput HSI information and does not fully explore the guid-\nance effect of the mask, leading to limited improvement.\nIn recent years, the natural language processing (NLP)\nmodel, Transformer [44], has been introduced into com-\nputer vision and outperformed CNN methods in many tasks.\nThe Multi-head Self-Attention (MSA) module in Trans-\nformer excels at capturing non-local similarity and long-\nrange dependencies. This advantage provides a possibil-\nity to address the aforementioned limitations of CNN-based\nmethods in HSI reconstruction. However, directly apply-\ning the original Transformer may be unsuitable for HSI\nrestoration due to the following reasons. Firstly, original\nTransformers learn to capture the long-range dependencies\nin spatial wise but the representations of HSIs are spectrally\nhighly self-similar. In this case, the inter-spectra similar-\nity and correlations are not well modeled. Meanwhile, the\nspectral information is spatially sparse. Capturing spatial\ninteractions may be less cost-effective than modeling spec-\ntral correlations with the same resources.Secondly, the HSI\nrepresentations are modulated by the mask in the CASSI\nsystem. The original Transformer without sufficient guid-\nance may easily attend to many low-fidelity and less infor-\nmative image regions when calculating self-attention. This\nmay degrade the model efficiency.Thirdly, when using the\noriginal global Transformer [15], the computational com-\nplexity is quadratic to the spatial size. This burden is non-\ntrivial and sometimes unaffordable. When using the local\nwindow-based Transformer [31], the receptive fields of the\nMSA module are limited within the position-specific win-\ndows and some highly related tokens may be neglected.\nTo cope with the above problems, we propose a novel\nmethod, Mask-guided Spectral-wise Transformer (MST),\nfor HSI reconstruction. Firstly, in Fig. 2 (a), we observe\nthat each spectral channel of HSIs captures an incomplete\npart of the same scene due to the constraints of the spe-\ncific wavelength. This indicates that the HSI representations\nare similar and complementary along the spectral dimen-\nsion. Hence, we propose a Spectral-wise MSA (S-MSA) to\ncapture the long-range inter-spectra dependencies. Specifi-\ncally, S-MSA treats each spectral channel feature as a token\nand calculates the self-attention along the spectral dimen-\nsion. Secondly, in Fig. 2 (b), a mask is used in the CASSI\nsystem to modulate HSIs. The light transmittance of dif-\nferent positions on the mask varies significantly. This in-\ndicates that the fidelity of the modulated spectral informa-\ntion is position-sensitive. Therefore, we exploit the mask\nas a key clue and present a novel Mask-guided Mechanism\n(MM) that directs the S-MSA module to pay attention to the\nregions with high-fidelity spectral representations. Mean-\nwhile, MM also alleviates the limitation of S-MSA in mod-\neling the spatial correlations of HSI representations. Fi-\nnally, with our proposed techniques, we establish a series of\nextremely efficient MST models that surpass state-of-the-art\n(SOTA) methods by a large margin, as illustrated in Fig. 1.\nOur contributions can be summarized as follows:\n‚Ä¢ We propose a new method, MST, for HSI reconstruc-\ntion. To the best of our knowledge, it is the first attempt\nto explore the potential of Transformer in this task.\n‚Ä¢ We present a novel self-attention, S-MSA, to capture\nthe inter-spectra similarity and dependencies of HSIs.\n‚Ä¢ We customize an MM that directs S-MSA to pay atten-\ntion to regions with high-fidelity HSI representations.\n‚Ä¢ Our MST dramatically outperforms SOTA methods on\nall scenes in simulation while requiring much cheaper\nParams and FLOPS. Besides, MST yields more visu-\nally pleasant results in real-world HSI reconstruction.\n2. Related Work\n2.1. HSI Reconstruction\nTraditional HSI reconstruction methods [18, 23, 29, 30,\n43, 46, 52, 57, 61] are mainly based on hand-crafted priors.\nFor example, GAP-TV [57] introduces the total variation\nprior. DeSCI [30] exploits the low-rank property and non-\nlocal self-similarity. However, these model-based methods\nachieve unsatisfactory performance and generality due to\nthe poor representing capacity. Recently, deep CNNs have\nbeen applied to learn the end-to-end mapping function of\nHSI reconstruction [19, 20, 36, 39, 49] to achieve promis-\ning performance. TSA-Net [36] uses three spatial-spectral\nself-attention modules to capture the dependencies in com-\npressed spatial or spectral dimensions. The additional costs\nare nontrivial while the improvement is limited. DGSMP\n[20] suggests an interpretable HSI restoration method with\nlearned Gaussian Scale Mixture (GSM) prior. These CNN-\nbased methods yield impressive performance but show limi-\ntations in modeling inter-spectra similarity and correlations.\nBesides, the guidance effect of the mask is under-studied.\n2.2. Vision Transformer\nTransformer is firstly proposed by [44] for machine\ntranslation. Recently, Transformer has achieved great suc-\nSpectral(Œª)\nSpatial(x)\nshift\nFeatureEmbedding\nV\n K\n¬∑\n √ó\nTranspose\n√ó\n SpectralSelf-Attention\n(a)HSICharacteristic (c)MS-MSA\n(c1)Spectral-wiseMSA\nF M\nùêªùëä√óùê∂\nShiftasDispersion\n1√ó1\n1√ó1Conv\n 5√ó5\nDepth-wiseConv5√ó5\n¬∑\nElement-wiseMultiplication\nS\nSigmoidActivation\nShiftReversesDispersionshiftshift\nMatrixMultiplication\nMatrixMultiplication\nA\nF‚Ä≤ F‚Ä≤‚Ä≤ Y Y H‚àó\nùêªùëä√óùê∂\nX\nXùêª√óùëä√óùê∂ùëúùë¢ùë°\n1√ó1\n1√ó1\n5√ó5\nS\n+\n¬∑\n(c2)Mask-guidedMechanism\nshift\n shift\nM‚àó\n M‚Ä≤\nQ\nMùêªùëä√óùê∂\nùêªùëä√óùê∂\nùêªùëä\nsoftmax\nùêªùëä√óùê∂\nW\nV\n W\nK\n W\nQ\nùêªùëä√óùê∂\nPositionEmbedding\n+\n1√ó1HSIFeatures:SpatiallySparsewhileSpectrallyCorrelated\n(b)CASSISystem\nW\nùêªùëä√óùê∂\n1√ó1 1√ó11√ó1 1√ó1 1√ó1 1√ó1 1√ó1 1√ó1\nXùëñùëõùêª√óùëä√óùê∂\nœÉ\nFigure 2. Illustration of the proposed method. Our Mask-guided Spectral-wise Multi-head Self-Attention (MS-MSA) is motivated by the\nHSI characteristics and CASSI system. (a) The representations of HSIs are spatially sparse while spectrally correlated. (b) The CASSI\nsystem uses a mask to modulate the HSIs. (c) Our MS-MSA in stage 0 of MST. (c1) S-MSA treats each spectral feature as a token and\ncalculates self-attention along the spectral dimension. (c2) Mask-guided Mechanism directs the Spectral-wise MSA to pay attention to\nspatial regions with high-fidelity HSI representations. Some components are omitted for simplification. Please refer to the text for details.\ncess in many high-level vision tasks, such as image classi-\nfication [2, 15, 17, 31], object detection [1, 13, 60, 64], seg-\nmentation [8, 55, 63], human pose estimation [7, 25, 26, 56],\netc. Due to its promising performance, Transformer has also\nbeen introduced into low-level vision [6,9,11,14,27,28,54].\nSwinIR [27] uses Swin Transformer [27] blocks to build\nup a residual network and achieve SOTA results in image\nrestoration. However, these Transformers mainly aim to\ncapture long-range dependencies of spatial regions. As for\nspectrally self-similar and mask-modulated HSIs, directly\napplying previous Transformers may be less effective in\ncapturing spectral-wise correlations. In addition, the MSA\nmay pay attention to less informative spatial regions.\n3. CASSI System\nA concise CASSI principle is shown in Fig. 2 (b). Given\na 3D HSI cube, denoted by F ‚àà RH√óW√óNŒª, where H,\nW, and NŒª represent the HSI‚Äôs height, width, and number\nof wavelengths, respectively. F is firstly modulated by the\ncoded aperture (physical mask) M‚àó ‚àà RH√óW as\nF‚Ä≤(:, :, nŒª) =F(:, :, nŒª) ‚äô M‚àó, (1)\nwhere F‚Ä≤ denotes the modulated HSIs, nŒª ‚àà [1, . . . , NŒª]\nindexes the spectral channels, and ‚äô denotes the element-\nwise multiplication. After passing through the disperser, F‚Ä≤\nbecomes tilted and is considered to be sheared along the y-\naxis. We use F‚Ä≤‚Ä≤ ‚àà RH√ó(W+d(NŒª‚àí1))√óNŒª to denote the\ntilted HSI cube, where d represents the shifting step. We\nassume Œªc to be the reference wavelength, i.e., F‚Ä≤‚Ä≤(:, :, nŒªc)\nis not sheared along the y-axis. Then we have\nF‚Ä≤‚Ä≤(u, v, nŒª) =F‚Ä≤(x, y+ d(Œªn ‚àí Œªc), nŒª), (2)\nwhere (u, v) represents the coordinate system on the detec-\ntor plane, Œªn denotes the wavelength of the nŒª-th channel,\nand d(Œªn ‚àí Œªc) indicates the spatial shifting for the nŒª-th\nchannel on F‚Ä≤‚Ä≤. Finally, the captured 2D compressed mea-\nsurement Y ‚àà RH√ó(W+d(NŒª‚àí1)) can be obtained by\nY =\nNŒªX\nnŒª=1\nF‚Ä≤‚Ä≤(:, :, nŒª) +G, (3)\nwhere G ‚àà RH√ó(W+d(NŒª‚àí1)) is the imaging noise on the\nmeasurement, generated by the photon sensing detector.\n4. Method\n4.1. Overall Architecture\nThe overall architecture of MST is shown in Fig. 3 (a).\nWe adopt a U-shaped structure that consists of an encoder, a\nbottleneck, and a decoder. MST is built up by Mask-guided\nSpectral-wise Attention Blocks (MSAB). Firstly, we reverse\nthe dispersion process (Eq. (2)) and shift back the measure-\nment to obtain the initialized signal H ‚àà RH√óW√óNŒª as\nH(x, y, nŒª) =Y(x, y‚àí d(Œªn ‚àí Œªc)). (4)\nThen we feed H into the model. Firstly, MST exploits a\nconv3√ó3 (convolution with kernel size = 3) layer to map\nH into feature X0 ‚àà RH√óW√óC. Secondly, X0 under-\ngoes N1 MSABs, a downsample module,N2 MSABs, and a\ndownsample module to generate hierarchical features. The\ndownsample module is a strided conv4√ó4 layer that down-\nscales the feature maps and doubles the channels. There-\nfore, the feature of the i-th stage of the encoder is denoted\nas Xi ‚àà R\nH\n2i √óW\n2i √ó2iC. Thirdly, X2 passes through the\nMSAB MSAB\nMSAB MSAB\nMSAB\n+ Decoder\n(b)MSAB\nshift\nEncoder\nBottleneck\n(a) MST\nHH'0X1X2X'2X'1X'0XRYùêª√óùëä√óùëÅ% ùêª√óùëä√óùê∂ ùêª2√óùëä2√ó2ùê∂ ùêª4√óùëä4√ó4ùê∂\nùêª4√óùëä4√ó4ùê∂\nùêª2√óùëä2√ó2ùê∂ùêª2√óùëä2√ó2ùê∂\nùêª2√óùëä2√ó4ùê∂\nùêª2√óùëä2√ó2ùê∂\nùêª2√óùëä2√ó2ùê∂\nùêª√óùëä√óùê∂ ùêª√óùëä√óùê∂\nùêª√óùëä√ó2ùê∂\nùêª√óùëä√óùê∂\nùêª√óùëä√óùê∂ùêª√óùëä√óùëÅ%\nùêª√ó(ùëä+ùëë(ùëÅ%‚àí1))\nDownSample\nDownSample\nEmbedding\nUpSample\nUpSample\nMapping\nMeasurement\nReconstructedHSIs\ncc\nShift ReversesDispersion ChannelConcatenationcshift 1√ó1Convolution\n+\n+\nùêª√óùëä√óùê∂√óùëÅ/\n√óùëÅ/\n√óùëÅ0\n√óùëÅ0\n√óùëÅ1\n(c)FFN\nLayerNorm\nLayerNorm\nMS-MSA\nFFN\nùëë\nùëë\nùëë\nùëë4\nùëë4\nùëë16conv1√ó1conv1√ó1\nconv1√ó1\nconv1√ó1\nGELU\nDWconv3√ó3\nGELU\nconv1√ó1\nFigure 3. The overall architecture of MST. (a) MST adopts a U-shaped structure that consists of an encoder, a bottleneck, and a decoder.\n(b) MSAB is composed of a Feed-Forward Network (FFN), an MS-MSA, and two layer normalization. (c) The components of FFN.\nbottleneck that consists of N3 MSABs. Subsequently, We\nfollow the spirit of U-Net [42] and design a symmetrical\nstructure as the decoder. In particular, the upsample module\nis a strided deconv2√ó2 layer. The skip connections are ex-\nploited for feature aggregation between the encoder and de-\ncoder to alleviate the information loss caused by the down-\nsample operations. Similarly, the feature of the i-th stage of\nthe decoder is denoted as X‚Ä≤\ni ‚àà R\nH\n2i √óW\n2i √ó2iC. After passing\nthrough the decoder, the feature maps undergo a conv3√ó3\nlayer to generate the residual HSIs R ‚àà RH√óW√óNŒª. Fi-\nnally, the reconstructed HSIs H‚Ä≤ ‚àà RH√óW√óNŒª can be ob-\ntained by the sum of R and H, i.e., H‚Ä≤ = H + R.\nIn implementation, we set C to 28 and change the com-\nbination (N1, N2, N3) to establish a series of MST models\nwith small, medium, and large model sizes and computation\ncosts: MST-S (2,2,2), MST-M (2,4,4), and MST-L (4,7,5).\nThe basic unit of MST is MSAB. As shown in Fig. 3 (b),\nMSAB consists of two layer normalization, a Mask-guided\nSpectral-wise MSA (MS-MSA), and a Feed-Forward Net-\nwork (FFN). The details of FFN are depicted in Fig. 3 (c).\n4.2. Spectral-wise Multi-head Self-Attention\nThe non-local self-similarity is often exploited in HSI re-\nconstruction but is usually not well modeled by CNN-based\nmethods. Due to the effectiveness of Transformer in captur-\ning non-local long-range dependencies and its impressive\nperformance in other vision tasks, we aim to explore the\npotential of Transformer in HSI reconstruction. However,\nthere are two main issues when directly applying Trans-\nformer to HSI restoration. The first problem is that original\nTransformers model long-range dependencies in spatial di-\nmensions. But the HSI representations are spatially sparse\nand spectrally correlated, as shown in Fig. 2 (a). Captur-\ning spatial-wise interactions may be less cost-effective than\nmodeling spectral-wise correlations. Hence, we propose S-\nMSA that treats each spectral feature map as a token and\ncalculates self-attention along the spectral dimension. Fig. 2\n(c1) shows the S-MSA used in stage 0 of MST. The input\nXin ‚àà RH√óW√óC is reshaped into tokens X ‚àà RHW √óC.\nThen X is linearly projected into query Q ‚àà RHW √óC, key\nK ‚àà RHW √óC, and value V ‚àà RHW √óC:\nQ = XWQ, K = XWK, V = XWV, (5)\nwhere WQ, WK, and WV ‚àà RC√óC are learnable pa-\nrameters; biases are omitted for simplification. Subse-\nquently, we respectively split Q, K, and V into N heads\nalong the spectral channel dimension: Q = [Q1, . . . ,QN ],\nK = [K1, . . . ,KN ], and V = [V1, . . . ,VN ]. The dimen-\nsion of each head is dh = C\nN . Please note that Fig. 2 (c1)\ndepicts the situation with N = 1 and some details are omit-\nted for simplification. Different from original MSAs, our\nS-MSA treats each spectral representation as a token and\ncalculates the self-attention for each headj:\nAj = softmax(œÉjKT\nj Qj), headj = VjAj, (6)\nwhere KT\nj denotes the transposed matrix of Kj. Because\nthe spectral density varies significantly with respect to the\nwavelengths, we use a learnable parameterœÉj ‚àà R1 to adapt\nthe self-attention Aj by re-weighting the matrix multiplica-\ntion KT\nj Qj inside headj. Subsequently, the outputs of N\nheads are concatenated in spectral wise to undergo a linear\nprojection and then is added with a position embedding:\nS-MSA(X) =\n\u0000 N\nConcat\nj=1\n(headj)\n\u0001\nW + fp(V), (7)\nwhere W ‚àà RC√óC are learnable parameters, fp(¬∑) is the\nfunction to generate position embedding. It consists of two\ndepth-wise conv3√ó3 layers, a GELU activation, and re-\nshape operations. The HSIs are sorted by the wavelength\nalong the spectral dimension. Therefore, we exploit this\nembedding to encode the position information of different\nspectral channels. Finally, we reshape the result of Eq. (7)\nto obtain the output feature maps Xout ‚àà RH√óW√óC.\nWe analyze the computational complexity of S-MSA and\ncompare it with other MSAs. We only compare the main\ndifference, i.e., the self-attention mechanism in Eq. (6):\nO(S-MSA) =2HW C2\nN , O(G-MSA) = 2(HW )2C,\nO(W-MSA) = 2(M2)2(HW\nM2 )C = 2M2HW C,\n(8)\nwhere G-MSA denotes the original global MSA [15], W-\nMSA denotes the local window-based MSA [31], and M\nrepresents the window size. The computational complexity\nof S-MSA and W-MSA is linear to the spatial size HW .\nThis cost is much cheaper than that of G-MSA (quadratic to\nHW ). Meanwhile, S-MSA treats a whole spectral feature\nmap as a token. Thus, the receptive field of our S-MSA is\nglobal and not limited to position-specific windows.\n4.3. Mask-guided Mechanism\nThe second problem of directly using Transformer for\nHSI restoration is that original Transformers may attend to\nsome less informative spatial regions with low-fidelity HSI\nrepresentations. In CASSI, a physical mask is used to mod-\nulate the HSIs. Thus, the light transmittance of different\npositions on the mask varies. As a result, the fidelity of the\nmodulated spectral information is position-sensitive. This\nobservation motivates us that the mask should be used as\na clue to direct the model to attend to regions with high-\nfidelity HSI representations. In this part, we firstly analyze\nthe usage of the mask in previous CNN-based methods, and\nthen introduce our Mask-guided Mechanism (MM).\nPrevious Mask Usage Scheme. Previous CNN-based\nmethods [35, 36, 38, 48] mainly conduct an inner product\nbetween the initialized HSIs H and the mask M‚àó to gen-\nerate a modulated input. This scheme introduces spatial\nfidelity information but suffers from the following limita-\ntions: (i) This operation corrupts the input HSI representa-\ntions, causes the information loss, and leads to spatial dis-\ncontinuity. (ii) This scheme only operates at the input. The\nguidance effect of the mask in directing the network to pay\nattention to regions with high-fidelity HSI representations is\nnot fully explored. (iii) This scheme does not exploit learn-\nable parameters to model the spatial-wise correlations.\nOur MM. Different from previous methods, our MM pre-\nserves all the input HSI representations and learns to direct\nS-MSA to pay attention to the spatial regions with high-\nfidelity spectral representations. To be specific, given the\nmask M‚àó ‚àà RH√óW shown in Fig. 2 (c2), since the modu-\nlated HSIs are shifted by the disperser of the CASSI system,\nwe firstly shift M‚àó like the dispersion process:\nMs(x, y, nŒª) =M‚àó(x, y+ d(Œªn ‚àí Œªc)), (9)\nwhere Ms ‚àà RH√ó(W+d(NŒª‚àí1))√óNŒª denotes the shifted\nversion of M‚àó. The shifted regions out of the range in\ny-axis on M‚àó are set to 0. Please note that Fig. 2 (c2)\nshows the MM used in stage 0 of MST. To match the scale\nof the feature maps in stage i of MST, Ms needs to pass\nthrough the same downsample operations in Fig. 3 (a). Sub-\nsequently, Ms undergoes a conv1√ó1 layer and then is input\nto two paths. The upper path is an identity mapping to re-\ntain the original fidelity information. The lower path under-\ngoes a conv1√ó1 layer, a depth-wise conv5√ó5 layer, a sig-\nmoid activation, and an inner product with the upper path.\nS-MSA is effective in capturing inter-spectra dependencies\nbut shows limitations in modeling spatial interactions of\nHSI representations. Thus, the lower path is designed to\ncapture the spatial-wise correlations. Then we have\nM‚Ä≤\ns = (W1Ms) ‚äô (1 +Œ¥(fdw(W2W1Ms)), (10)\nwhere W1 and W2 are the learnable parameters of the two\nconv1√ó1 layers, fdw(¬∑) denotes the mapping function of the\ndepth-wise conv5√ó5 layer, Œ¥(¬∑) represents the sigmoid ac-\ntivation, and M‚Ä≤\ns ‚àà RH√ó(W+d(NŒª‚àí1))√óC denotes the in-\ntermediate feature maps. To spatially align the mask atten-\ntion map with the modulated HSIs F‚Ä≤ in the CASSI system\n(Fig. 2 (b)) and the initialized input H of MST (Fig. 3 (a)),\nwe reverse the dispersion process and shift back M‚Ä≤\ns to ob-\ntain the mask attention map M‚Ä≤ ‚àà RH√óW√óC as\nM‚Ä≤(x, y, nŒª) =M‚Ä≤\ns(x, y‚àí d(Œªn ‚àí Œªc), nŒª), (11)\nwhere nŒª ‚àà [1, . . . , C] indexes the spectral channels to\nmatch the dimensions of M‚Ä≤\ns. We reshape M‚Ä≤ into M ‚àà\nRHW √óC to match the dimensions of V. Then we split M\ninto N heads in spectral wise: M = [M1, . . . ,MN ]. For\neach headj, MM conducts its guidance by re-weightingVj\nusing Mj ‚àà RHW √ódh. Hence, when using MM to direct\nS-MSA, the S-MSA module just needs to make a simple\nmodification by re-formulating headj in Eq. (6):\nheadj = (Mj ‚äô Vj)Aj. (12)\nThe subsequent steps of S-MSA remain unchanged. By us-\ning MM, S-MSA can extract non-corrupted HSI representa-\ntions, enjoy the guidance of position-sensitive fidelity infor-\nmation, and adaptively model the spatial-wise interactions.\n5. Experiments\n5.1. Experimental Settings\nFollowing the settings of TSA-Net [36], we adopt 28\nwavelengths from 450 nm to 650 nm derived by spectral\ninterpolation manipulation for HSIs. We perform experi-\nments on both simulation and real HSI datasets.\nSimulation HSI Data. We use two simulation hyperspec-\ntral image datasets, CA VE [41] and KAIST [12]. CA VE\nTwIST [4] GAP-TV [57] DeSCI [30]Œª-net [39] HSSP [49] DNU [50] DIP-HSI [38] TSA-Net [36] DGSMP [20]MST-S MST-M MST-LScenePSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIMPSNR SSIM PSNR SSIM PSNR SSIM\n1 25.16 0.700 26.82 0.754 27.13 0.748 30.10 0.849 31.48 0.858 31.72 0.863 32.68 0.890 32.03 0.892 33.26 0.91534.71 0.930 35.15 0.93735.40 0.9412 23.02 0.604 22.89 0.610 23.04 0.620 28.49 0.805 31.09 0.842 31.13 0.846 27.26 0.833 31.00 0.858 32.09 0.89834.45 0.925 35.19 0.93535.87 0.9443 21.40 0.711 26.31 0.802 26.62 0.818 27.73 0.870 28.96 0.823 29.99 0.845 31.30 0.914 32.25 0.915 33.06 0.92535.32 0.943 36.26 0.95036.51 0.9534 30.19 0.851 30.65 0.852 34.96 0.897 37.01 0.934 34.56 0.902 35.34 0.908 40.54 0.962 39.19 0.953 40.54 0.96441.50 0.96742.48 0.97342.270.9735 21.41 0.635 23.64 0.703 23.94 0.706 26.19 0.817 28.53 0.808 29.03 0.833 29.79 0.900 29.39 0.884 28.86 0.88231.90 0.933 32.49 0.94332.77 0.9476 20.95 0.644 21.85 0.663 22.38 0.683 28.64 0.853 30.83 0.877 30.87 0.887 30.39 0.877 31.44 0.908 33.08 0.93733.85 0.943 34.28 0.94834.80 0.9557 22.20 0.643 23.76 0.688 24.45 0.743 26.47 0.806 28.71 0.824 28.99 0.839 28.18 0.913 30.32 0.878 30.74 0.88632.69 0.911 33.29 0.92133.66 0.9258 21.82 0.650 21.98 0.655 22.03 0.673 26.09 0.831 30.09 0.881 30.13 0.885 29.44 0.874 29.35 0.888 31.55 0.92331.69 0.933 32.40 0.94332.67 0.9489 22.42 0.690 22.63 0.682 24.56 0.732 27.50 0.826 30.43 0.868 31.03 0.876 34.51 0.927 30.01 0.890 31.66 0.91134.67 0.939 35.35 0.94235.39 0.94910 22.67 0.569 23.10 0.584 23.59 0.587 27.13 0.816 28.78 0.842 29.14 0.849 28.51 0.851 29.59 0.874 31.44 0.92531.82 0.926 32.53 0.93532.50 0.941\nAvg 23.12 0.669 24.36 0.669 25.27 0.721 28.53 0.841 30.35 0.852 30.74 0.863 31.26 0.894 31.46 0.894 32.63 0.91734.26 0.935 34.94 0.94335.18 0.948\nTable 1. Quantitative results on 10 scenes in simulation. PSNR and SSIM are reported. MSTs significantly surpass other competitors.\nRGBImageMeasurement\nSpectralDensityCurves\nTwIST\nGAP-TV DeSCIŒª-Net HSSP\nTSA-Net\nDGSMP\nMST-L\n GroundTruth\n462.0nm\n551.5nm\n594.5nm\n636.5nm\n450 500 550 600 650Wavelength (nm)\n0\n0.2\n0.4\n0.6\n0.8\n1Density\n Ground Truth DeSCI, corr: 0.9703 GapTV, corr: 0.9810 HSSP, corr: 0.9896 -net, corr: 0.9685 TSA-net, corr: 0.9903 Twist, corr: 0.9690 DGSMP, corr: 0.9716 MST-L, corr: 0.9947\nFigure 4. Reconstructed simulation HSI comparisons of Scene 5 with 4 out of 28 spectral channels. 7 SOTA algorithms and our MST-L\nare included. The spectral curves (bottom-left) are corresponding to the selected green box of the RGB image. Zoom in for a better view.\ndataset is composed of 32 hyperspectral images at a spatial\nsize of 512√ó512. KAIST dataset consists of 30 hyperspec-\ntral images at a spatial size of 2704 √ó3376. Following the\nschedule of TSA-Net [36], we adopt CA VE as the training\nset. 10 scenes from KAIST are selected for testing.\nReal HSI Data. We use the real HSI dataset collected by\nthe CASSI system developed in TSA-Net [36].\nEvaluation Metrics. We adopt peak signal-to-noise ratio\n(PSNR) and structural similarity (SSIM) [53] as the metrics\nto evaluate the HSI reconstruction performance.\nImplementation Details. We implement MST in Pytorch.\nAll the models are trained with Adam [22] optimizer ( Œ≤1\n= 0.9 and Œ≤2 = 0.999) for 300 epochs. The learning rate\nis set to 4 √ó10‚àí4 in the beginning and is halved every 50\nepochs during the training procedure. When conducting\nexperiments on simulation data, patches at a spatial size\nof 256 √ó256 cropped from the 3D cubes are fed into the\nnetworks. As for real hyperspectral image reconstruction,\nthe patch size is set to 660 √ó660 to match the real-world\nmeasurement. The shifting step d in dispersion is set to 2.\nThus, the measurement sizes are 256 √ó310 and 660 √ó714\nfor simulation and real HSI datasets. The shifting step in\nreversed dispersion is d/4i, i= 0, 1, 2 for the i-th stage of\nMST. The batch size is 5. Random flipping and rotation are\nused for data augmentation. The models are trained on one\nRTX 8000 GPU. The training objective is to minimize the\nRoot Mean Square Error (RMSE) and Spectrum Constancy\nLoss [62] between the reconstructed and ground-truth HSIs.\n5.2. Quantitative Results\nWe compare our MST with several SOTA HSI recon-\nstruction algorithms, including three model-based methods\n(TwIST [4], GAP-TV [57], and DeSCI [30]) and six CNN-\nbased methods ( Œª-net [39], HSSP [49], DNU [50], PnP-\nDIP-HSI [38], TSA-Net [36], and DGSMP [20]). For fair\ncomparisons, all methods are tested with the same settings\nas DGSMP [20]. The PSNR and SSIM results of different\nmethods on 10 scenes in the simulation datasets are listed\nin Tab. 1. The Params and FLOPS (test size = 256√ó256) of\nopen-source CNN-based algorithms are reported in Tab. 2c.\nIt can be observed from these two tables that our MSTs\nsignificantly surpass previous methods by a large margin\non all the 10 scenes while requiring much cheaper mem-\nory and computational costs. More specifically, our best\nmodel, MST-L surpasses DGSMP, TSA-Net, and Œª-net\nby 2.55, 3.72, and 6.65 dB while costing 54.0% (2.03 /\n3.76), 4.6%, and 3.2% Params and 4.4% (28.15 / 646.65),\n25.6%, and 23.9% FLOPS. Surprisingly, even our smallest\nmodel, MST-S outperforms DGSMP, TSA-Net, PnP-DIP-\nHSI, DNU, and Œª-net by 1.63, 2.80, 3.00, 3.52, and 5.73\n471.5nm\n551.5nm\n594.5nm\n636.5nm\nRGBImage\nMeasurementTwISTGAP-TVDeSCIŒª-NetHSSPTSA-NetDGSMPMST-L\nFigure 5. Reconstructed real HSI comparisons of Scene 3 with 4 out of 28 spectral channels. Seven SOTA algorithms and our MST-L are\nincluded. MST-L reconstructs more detailed contents and suppresses more noise. Please zoom in for better visualization performance.\ndB while requiring 24.7%, 2.1%, 2.7%, 78.2%, and 1.5%\nParams and 2.0%, 11.8%, 20.1%, 7.9%, and 11.0% FLOPS.\nTo intuitively show the superiority of our MST, we pro-\nvide PSNR-Params-FLOPS comparisons of different recon-\nstruction algorithms in Fig. 1. The vertical axis is PSNR\n(performance), the horizontal axis is FLOPS (computa-\ntional cost), and the circle radius is Params (memory cost).\nIt can be seen that our MSTs take up the upper-left corner,\nexhibiting the extreme efficiency advantages of our method.\n5.3. Qualitative Results\nSimulation HSI Reconstruction. Fig. 4 visualizes the re-\nconstructed simulation HSIs of Scene 5 with 4 out of 28\nspectral channels using seven SOTA methods and our MST-\nL. Please zoom in for a better view. As can be seen from\nthe reconstructed HSIs (right) and the zoom-in patches of\nthe selected yellow boxes, previous methods are less favor-\nable to restore HSI details. They either yield over-smooth\nresults sacrificing fine-grained structural contents and textu-\nral details, or introduce undesirable chromatic artifacts and\nblotchy textures. In contrast, our MST-L is more capable of\nproducing perceptually-pleasing and sharp images, and pre-\nserving the spatial smoothness of the homogeneous regions.\nThis is mainly because our MST-L enjoys the guidance of\nmodulation information and captures the long-range depen-\ndencies of different spectral channels. In addition, we plot\nthe spectral density curves (bottom-left) corresponding to\nthe picked region of the green box in the RGB image (top-\nleft). The highest correlation and coincidence between our\ncurve and the ground truth demonstrate the spectral-wise\nconsistency restoration effectiveness of our MST.\nReal HSI Reconstruction. We further apply our proposed\napproach to real HSI reconstruction. Similar to [20, 36], we\nre-train our model (MST-L) on all scenes of CA VE [41] and\nKAIST [12] datasets. To simulate real imaging situations,\nwe inject 11-bit shot noise into the measurements during\ntraining. Visual comparisons are shown in Fig. 5. Our MST-\nL surpasses previous algorithms in terms of high-frequency\nstructural detail reconstruction and real noise suppression.\n5.4. Ablation Study\nIn this part, we adopt the simulation HSI datasets [12,41]\nto conduct ablation studies. The baseline model is derived\nby removing our S-MSA and MM from MST-S.\nBreak-down Ablation. We firstly conduct a break-down\nablation experiment to investigate the effect of each com-\nponent towards higher performance. The results are listed\nin Tab. 2a. The baseline model yields 32.29 dB. When we\nsuccessively apply our S-MSA and MM, the model con-\ntinuously achieves 0.89 and 1.08 dB improvements. These\nresults suggest the effectiveness of S-MSA and MM.\nSelf-Attention Scheme Comparison. We compare S-MSA\nwith other self-attentions and report the results in Tab. 2b.\nFor fair comparisons, the Params of models using different\nself-attention schemes are set to the same value (0.70 M).\nPlease note that we downscale the input feature of global\nMSA [15] into 1\n4 size to avoid out of memory. The baseline\nmodel yields 32.29 dB while costing 0.53 M Params and\n7.43 G FLOPS. We respectively apply the global MSA [15],\nlocal window-based MSA [31], Swin W-MSA [31], and our\nS-MSA. The model gains by 0.38, 0.46, 0.57, and 0.89 dB\nwhile adding 4.45, 3.64, 3.64, and 2.93 G FLOPS. Our S-\nMSA yields the most significant improvement but requires\nthe least computational cost. We explain these results by\nthe HSI characteristics that the spectral representations are\nspatially sparse and spectrally highly self-similar. Hence,\ncapturing spatial interactions may be less cost-effective than\nmodeling inter-spectra dependencies. This evidence clearly\nBaseline S-MSA MM PSNR SSIM Params (M)FLOPS (G)\n‚úì 32.29 0.897 0.53 7.43\n‚úì ‚úì 33.18 0.923 0.70 10.36\n‚úì ‚úì ‚úì 34.26 0.935 0.93 12.96\n(a) Break-down ablation study towards higher performance.\nMethod BaselineGlobal MSALocal W-MSASwin W-MSAS-MSA\nPSNR 32.29 32.67 32.75 32.86 33.18SSIM 0.897 0.912 0.916 0.919 0.923Params (M) 0.53 0.70 0.70 0.70 0.70FLOPS (G) 7.43 11.88 11.07 11.07 10.36\n(b) Ablation study of different self-attention mechanisms.\nMethod Œª-net [39]DNU [50]DIP-HSI [38]TSA-Net [36]DGSMP [20]MST-SMST-MMST-L\nPSNR 28.53 30.74 31.26 31.46 32.63 34.26 34.94 35.18\nSSIM 0.841 0.863 0.894 0.894 0.917 0.935 0.943 0.948\nParams (M) 62.64 1.19 33.85 44.25 3.76 0.93 1.50 2.03\nFLOPS (G) 117.98 163.48 64.42 110.06 646.65 12.96 18.07 28.15\n(c) Performance-Params-FLOPS comparisons with open-source SOTA CNN-based methods.\nMethod Input MM PSNRSSIM\nA H 33.18 0.923\nB H‚äôM‚àó 33.57 0.927\nC H ‚úì 34.26 0.935\nD H‚äôM‚àó ‚úì 34.07 0.932\n(d) MM v.s Previous usage of mask.\nTable 2. Ablations. We train models on CA VE [41] and test on KAIST [12] in simulation. PSNR, SSIM, Params, and FLOPS are reported.\nGlobalMSA\n LocalMSA\nSwinMSA\n S-MSA\n GroundTruth\nRGBImage\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6. Visualization of the correlation coefficients among spec-\ntral channels of HSIs reconstructed by models using different\nMSAs. The correlation coefficient map of the model equipped\nwith our S-MSA is the most similar one to that of the ground truth.\nverifies the efficiency superiority of our S-MSA.\nIn addition, we further conduct visual analysis about dif-\nferent MSAs in Fig. 6. Specifically, we visualize the corre-\nlation coefficients between each spectral pair of HSIs recon-\nstructed by models equipped with different MSAs. It can be\nobserved that the correlation coefficient map of the model\nusing our proposed S-MSA is the most similar one to that of\nthe ground truth. These results demonstrate the promising\neffectiveness of our S-MSA in modeling the inter-spectra\nsimilarity and long-range spectral-wise dependencies.\nMask-guided Mechanism. We conduct ablation studies to\ninvestigate the effect of the previous mask usage scheme\ndescribed in Sec. 4.3, our MM, and their interaction. The\nadopted network is the baseline model using S-MSA. The\nresults are reported in Tab. 2d. Method A uses our input\nsetting. Method B exploits the previous scheme that adopts\nH‚äôM‚àó as the input. B achieves a limited improvement due\nto the HSI representation corruption and under-utilization\nof the mask. Method C uses our MM. C yields the most\nsignificant improvement by 1.08 dB, showing the guidance\nadvantage of MM for HSI reconstruction. D exploits both\nthe previous scheme and our MM but degrades by 0.19 dB\nwhen compared to method C. This degradation may stem\nfrom the loss of some input spectral information.\nAdditionally, to intuitively show the advantages of MM,\nwe visualize the feature maps of the last MSAB in MST-S.\nAs depicted in Fig. 7, the top row shows the original RGB\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRGB\nw/oMM\nwithMM\nFigure 7. Visual analysis of the feature maps of the last MSAB in\nMST-S. The top row shows the original RGB images. The middle\nand bottom rows exhibit the feature maps without and with MM.\nThe model using MM pays more high-fidelity attention to details.\nimages. The middle and bottom rows respectively exhibit\nthe feature maps without and with MM. It can be clearly\nobserved that the model without MM generates blurred, dis-\ntorted, and incomplete feature maps while sacrificing some\ndetails, neglecting some scene patches, or introducing un-\npleasant artifacts. In contrast, the model using our MM\npays more accurate and high-fidelity attention to the de-\ntailed contents and structural textures of the desired scenes.\n6. Conclusion\nIn this paper, we propose an efficient Transformer-based\nframework, MST, for accurate HSI reconstruction. Moti-\nvated by the HSI characteristics, we develop an S-MSA to\ncapture inter-spectra similarity and dependencies. More-\nover, we customize an MM module to direct S-MSA to pay\nattention to spatial regions with high-fidelity HSI represen-\ntations. With these novel techniques, we establish a series of\nextremely efficient MST models. Quantitative experiments\ndemonstrate that our method surpasses SOTA algorithms by\na large margin, even requiring significantly cheaper Params\nand FLOPS. Qualitative comparisons show that our MST\nachieves more visually pleasant reconstructed HSIs.\nAcknowledgements: This work is partially supported by\nthe NSFC fund (61831014), the Shenzhen Science and\nTechnology Project under Grant (ZDYBH201900000002,\nCJGJZD20200617102601004), the Westlake Foundation\n(2021B1501-2), and the funding from Lochn Optics.\nReferences\n[1] Nicolas arion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, 2020.\n3\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu Àáci¬¥c, and Cordelia Schmid. Vivit: A video vi-\nsion transformer. arXiv preprint arXiv:2103.15691, 2021. 3\n[3] V . Backman, M. B. Wallace, L. Perelman, J. Arendt, R.\nGurjar, M. Muller, Q. Zhang, G. Zonios, E. Kline, and T.\nMcGillican. Detection of preinvasive cancer cells. Nature,\n2000. 1\n[4] J.M. Bioucas-Dias and M.A.T. Figueiredo. A new twist:\nTwo-step iterative shrinkage/thresholding algorithms for im-\nage restoration. TIP, 2007. 6\n[5] M. Borengasser, W. S. Hungate, and R. Watkins. Hyperspec-\ntral remote sensing: principles and applications. CRC press,\n2007. 1\n[6] Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Yulun Zhang,\nHanspeter Pfister, and Donglai Wei. Learning to generate\nrealistic noisy images via pixel-level noise-aware adversarial\ntraining. In NeurIPS, 2021. 3\n[7] Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin,\nAngang Du, Haoqian Wang, Xinyu Zhou, Erjin Zhou, Xi-\nangyu Zhang, and Jian Sun. Learning delicate local repre-\nsentations for multi-person pose estimation. arXiv preprint\narXiv:2003.04030, 2020. 3\n[8] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-\naopeng Zhang, Qi Tian, and Manning Wang. Swin-unet:\nUnet-like pure transformer for medical image segmentation.\narXiv preprint arXiv:2105.05537, 2021. 3\n[9] Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool.\nVideo super-resolution transformer. arXiv preprint\narXiv:2106.06847, 2021. 3\n[10] Xun Cao, Tao Yue, Xing Lin, Stephen Lin, Xin Yuan, Qiong-\nhai Dai, Lawrence Carin, and David J. Brady. Computational\nsnapshot multispectral cameras: Toward dynamic capture of\nthe spectral world. IEEE Signal Processing Magazine, 2016.\n1\n[11] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. Pre-trained image processing transformer. In\nCVPR, 2021. 3\n[12] Inchang Choi, MH Kim, D Gutierrez, DS Jeon, and G Nam.\nHigh-quality hyperspectral reconstruction using a spectral\nprior. In Technical report, 2017. 5, 7, 8\n[13] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan\nZhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-to-\nend object detection with dynamic attention. In ICCV, 2021.\n3\n[14] Zhuo Deng, Yuanhao Cai, Lu Chen, Zheng Gong, Qiqi\nBao, Xue Yao, Dong Fang, Shaochong Zhang, and Lan Ma.\nRformer: Transformer-based generative adversarial network\nfor real fundus image restoration on a new clinical bench-\nmark. arXiv preprint arXiv:2201.00466, 2022. 3\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 2, 3, 5, 7\n[16] Hao Du, Xin Tong, Xun Cao, and Stephen Lin. A prism-\nbased system for multispectral video acquisition. In ICCV,\n2009. 1\n[17] Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr\nBojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev,\nNatalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al.\nXcit: Cross-covariance image transformers. In NeurIPS,\n2021. 3\n[18] Ying Fu, Yinqiang Zheng, Imari Sato, and Yoichi Sato. Ex-\nploiting spectral-spatial correlation for coded hyperspectral\nimage restoration. In CVPR, 2016. 2\n[19] Xiaowan Hu, Yuanhao Cai, Jing Lin, Haoqian Wang, Xin\nYuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. Hd-\nnet: High-resolution dual-domain learning for spectral com-\npressive imaging. In CVPR, 2022. 2\n[20] Tao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, and\nGuangming Shi. Deep gaussian scale mixture prior for spec-\ntral compressive imaging. In CVPR, 2021. 2, 6, 7, 8\n[21] M. H. Kim, T. A. Harvey, D. S. Kittle, H. Rushmeier,\nR. O. Prum J. Dorsey, and D. J. Brady. 3d imaging spec-\ntroscopy for measuring hyperspectral patterns on solid ob-\njects. ACM Transactions on on Graphics, 2012. 1\n[22] Diederik P. Kingma and Jimmy Lei Ba. Adam: A method\nfor stochastic optimization. In ICLR, 2015. 6\n[23] David Kittle, Kerkil Choi, Ashwin Wagadarikar, and David J\nBrady. Multiframe image estimation for coded aperture\nsnapshot spectral imagers. Applied optics, 2010. 1, 2\n[24] David Kittle, Kerkil Choi, Ashwin Wagadarikar, and\nDavid J. Brady. Multiframe image estimation for coded aper-\nture snapshot spectral imagers. OSA Applied Optics , 2010.\n1\n[25] Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and\nZhuowen Tu. Pose recognition with cascade transformers. In\nCVPR, 2021. 3\n[26] Yanjie Li, Shoukui Zhang, Zhicheng Wang, Sen Yang,\nWankou Yang, Shu-Tao Xia, and Erjin Zhou. Tokenpose:\nLearning keypoint tokens for human pose estimation. In\nICCV, 2021. 3\n[27] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration us-\ning swin transformer. In ICCVW, 2021. 3\n[28] Jing Lin, Yuanhao Cai, Xiaowan Hu, Haoqian Wang, You-\nliang Yan, Xueyi Zou, Henghui Ding, Yulun Zhang, Radu\nTimofte, and Luc Van Gool. Flow-guided sparse transformer\nfor video deblurring. arXiv preprint arXiv:2201.01893 ,\n2022. 3\n[29] Xing Lin, Yebin Liu, Jiamin Wu, and Qionghai Dai. Spatial-\nspectral encoded compressive hyperspectral imaging. TOG,\n2014. 1, 2\n[30] Yang Liu, Xin Yuan, Jinli Suo, David Brady, and Qionghai\nDai. Rank minimization for snapshot compressive imaging.\nTPAMI, 2019. 1, 2, 6\n[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 2, 3, 5, 7\n[32] Patrick Llull, Xuejun Liao, Xin Yuan, Jianbo Yang, David\nKittle, Lawrence Carin, Guillermo Sapiro, and David J\nBrady. Coded aperture compressive temporal imaging. Op-\ntics Express, 2013. 1\n[33] Guolan Lu and Baowei Fei. Medical hyperspectral imaging:\na review. Journal of Biomedical Optics, 2014. 1\n[34] Farid Melgani and Lorenzo Bruzzone. Classification of hy-\nperspectral remote sensing images with support vector ma-\nchines. IEEE Transactions on Geoscience and Remote Sens-\ning, 2004. 1\n[35] Ziyi Meng, Shirin Jalali, and Xin Yuan. Gap-net for snapshot\ncompressive imaging. arXiv preprint arXiv:2012.08364 ,\n2020. 2, 5\n[36] Ziyi Meng, Jiawei Ma, and Xin Yuan. End-to-end low\ncost compressive spectral imaging with spatial-spectral self-\nattention. In ECCV, 2020. 1, 2, 5, 6, 7, 8\n[37] Ziyi Meng, Mu Qiao, Jiawei Ma, Zhenming Yu, Kun Xu, and\nXin Yuan. Snapshot multispectral endomicroscopy. Optics\nLetters, 2020. 1\n[38] Ziyi Meng, Zhenming Yu, Kun Xu, and Xin Yuan. Self-\nsupervised neural networks for spectral snapshot compres-\nsive imaging. In ICCV, 2021. 2, 5, 6, 8\n[39] Xin Miao, Xin Yuan, Yunchen Pu, and Vassilis Athitsos. l-\nnet: Reconstruct hyperspectral images from a snapshot mea-\nsurement. In ICCV, 2019. 2, 6, 8\n[40] Z. Pan, G. Healey, M. Prasad, and B. Tromberg. Face recog-\nnition in hyperspectral images. TPAMI, 2003. 1\n[41] Jong-Il Park, Moon-Hyun Lee, Michael D. Grossberg, and\nShree K. Nayar. Multispectral imaging using multiplexed\nillumination. In ICCV, 2007. 5, 7, 8\n[42] Olaf Ronneberger, Philipp Fischer, Thomas Brox, a, and b.\nU-net: Convolutional networks for biomedical image seg-\nmentation. In MICCAI, 2015. 4\n[43] Jin Tan, Yanting Ma, Hoover Rueda, Dror Baron, and Gon-\nzalo R. Arce. Compressive hyperspectral imaging via ap-\nproximate message passing. IEEE Journal of Selected Topics\nin Signal Processing, 2016. 2\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 2\n[45] Ashwin Wagadarikar, Renu John, Rebecca Willett, and\nDavid Brady. Single disperser design for coded aperture\nsnapshot spectral imaging. Applied Optics, 2008. 1\n[46] Ashwin Wagadarikar, Renu John, Rebecca Willett, and\nDavid Brady. Single disperser design for coded aperture\nsnapshot spectral imaging. Applied optics, 2008. 1, 2\n[47] Ashwin A Wagadarikar, Nikos P Pitsianis, Xiaobai Sun, and\nDavid J Brady. Video rate spectral imaging using a coded\naperture snapshot spectral imager. Optics Express, 2009. 1\n[48] Jiamian Wang, Yulun Zhang, Xin Yuan, Yun Fu, and\nZhiqiang Tao. A new backbone for hyperspectral image re-\nconstruction. arXiv preprint arXiv:2108.07739, 2021. 2, 5\n[49] Lizhi Wang, Chen Sun, Ying Fu, Min H. Kim, and Hua\nHuang. Hyperspectral image reconstruction using a deep\nspatial-spectral prior. In CVPR, 2019. 2, 6\n[50] Lizhi Wang, Chen Sun, Maoqing Zhang, Ying Fu, and Hua\nHuang. Dnu: Deep non-local unrolling for computational\nspectral imaging. In CVPR, 2020. 6, 8\n[51] Lizhi Wang, Zhiwei Xiong, Dahua Gao, Guangming Shi, and\nFeng Wu. Dual-camera design for coded aperture snapshot\nspectral imaging. OSA Applied Optics, 2015. 1\n[52] Lizhi Wang, Zhiwei Xiong, Guangming Shi, Feng Wu, and\nWenjun Zeng. Adaptive nonlocal sparse representation for\ndual-camera compressive hyperspectral imaging. TPAMI,\n2016. 1, 2\n[53] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncell. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 6\n[54] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and\nJianzhuang Liu. Uformer: A general u-shaped transformer\nfor image restoration. arXiv preprint 2106.03106, 2021. 3\n[55] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph\nGonzalez, Kurt Keutzer, and Peter Vajda. Visual transform-\ners: Token-based image representation and processing for\ncomputer vision. arXiv preprint arXiv:2006.03677 , 2020.\n3\n[56] Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang. Trans-\npose: Keypoint localization via transformer. In ICCV, 2021.\n3\n[57] Xin Yuan. Generalized alternating projection based total\nvariation minimization for compressive sensing. In ICIP,\n2016. 1, 2, 6\n[58] Xin Yuan, David J Brady, and Aggelos K Katsaggelos. Snap-\nshot compressive imaging: Theory, algorithms, and applica-\ntions. IEEE Signal Processing Magazine, 2021. 1\n[59] Yuan Yuan, Xiangtao Zheng, and Xiaoqiang Lu. Hyperspec-\ntral image superresolution by transfer learning. IEEE Jour-\nnal of Selected Topics in Applied Earth Observations and\nRemote Sensing, 2017. 1\n[60] Nicolas ZCarion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nRend-to-end object detection with transformers. In ECCV,\n2020. 3\n[61] Shipeng Zhang, Lizhi Wang, Ying Fu, Xiaoming Zhong, and\nHua Huang. Computational hyperspectral imaging based\non dimension-discriminative low-rank tensor recovery. In\nICCV, 2019. 1, 2\n[62] Yuanyuan Zhao, Hui Guo, Zhan Ma, Xun Cao, Tao Yue, and\nXuemei Hu. Hyperspectral imaging with random printed\nmask. In CVPR, 2019. 6\n[63] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers. In CVPR, 2021. 3\n[64] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 3",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.8262351751327515
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6989902257919312
    },
    {
      "name": "Coded aperture",
      "score": 0.6868030428886414
    },
    {
      "name": "Computer science",
      "score": 0.6844848394393921
    },
    {
      "name": "Spectral imaging",
      "score": 0.5636003613471985
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5222209692001343
    },
    {
      "name": "Computer vision",
      "score": 0.48371580243110657
    },
    {
      "name": "Spectral shape analysis",
      "score": 0.42104193568229675
    },
    {
      "name": "Remote sensing",
      "score": 0.14807772636413574
    },
    {
      "name": "Spectral line",
      "score": 0.13110408186912537
    },
    {
      "name": "Detector",
      "score": 0.12311646342277527
    },
    {
      "name": "Physics",
      "score": 0.11478045582771301
    },
    {
      "name": "Geography",
      "score": 0.07928070425987244
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}