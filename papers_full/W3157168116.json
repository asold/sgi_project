{
    "title": "Diverse Image Inpainting with Bidirectional and Autoregressive Transformers",
    "url": "https://openalex.org/W3157168116",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221824371",
            "name": "Yu, Yingchen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202058497",
            "name": "Zhan, Fangneng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2747070008",
            "name": "Wu, Rongliang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2070919263",
            "name": "Pan Jian-xiong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2571661581",
            "name": "Cui Kaiwen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101392955",
            "name": "Lu, Shijian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4297765786",
            "name": "Ma, Feiying",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2358224448",
            "name": "Xie, Xuansong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2232502167",
            "name": "Miao Chunyan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2732026016",
        "https://openalex.org/W2964082390",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3167101089",
        "https://openalex.org/W4226076628",
        "https://openalex.org/W2962974533",
        "https://openalex.org/W2987563462",
        "https://openalex.org/W3034482833",
        "https://openalex.org/W2963270367",
        "https://openalex.org/W3160809604",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3180355996",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963420272",
        "https://openalex.org/W2125761705",
        "https://openalex.org/W2962760235",
        "https://openalex.org/W2165736859",
        "https://openalex.org/W3105163367",
        "https://openalex.org/W1975049209",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3043547428",
        "https://openalex.org/W2738588019",
        "https://openalex.org/W2904785373",
        "https://openalex.org/W3035251567",
        "https://openalex.org/W1993120651",
        "https://openalex.org/W2963712589",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2100415658",
        "https://openalex.org/W2594057160",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W3130440474",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W1834627138",
        "https://openalex.org/W2331128040",
        "https://openalex.org/W2982763192",
        "https://openalex.org/W2962785568",
        "https://openalex.org/W2963981733",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W3035575271",
        "https://openalex.org/W2981682056",
        "https://openalex.org/W2798365772",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3137989542",
        "https://openalex.org/W3115146140",
        "https://openalex.org/W2295936755",
        "https://openalex.org/W3136958399",
        "https://openalex.org/W3127541803",
        "https://openalex.org/W3120387510",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963709863",
        "https://openalex.org/W2171011251",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2963255313",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W3096509145",
        "https://openalex.org/W2907097116",
        "https://openalex.org/W2156235915"
    ],
    "abstract": "Image inpainting is an underdetermined inverse problem, which naturally allows diverse contents to fill up the missing or corrupted regions realistically. Prevalent approaches using convolutional neural networks (CNNs) can synthesize visually pleasant contents, but CNNs suffer from limited perception fields for capturing global features. With image-level attention, transformers enable to model long-range dependencies and generate diverse contents with autoregressive modeling of pixel-sequence distributions. However, the unidirectional attention in autoregressive transformers is suboptimal as corrupted image regions may have arbitrary shapes with contexts from any direction. We propose BAT-Fill, an innovative image inpainting framework that introduces a novel bidirectional autoregressive transformer (BAT) for image inpainting. BAT utilizes the transformers to learn autoregressive distributions, which naturally allows the diverse generation of missing contents. In addition, it incorporates the masked language model like BERT, which enables bidirectionally modeling of contextual information of missing regions for better image completion. Extensive experiments over multiple datasets show that BAT-Fill achieves superior diversity and fidelity in image inpainting qualitatively and quantitatively.",
    "full_text": "Diverse Image Inpainting with Bidirectional and Autoregressive Transformers\nYingchen Yu1* Fangneng Zhan1* Rongliang Wu1 Jianxiong Pan2 Kaiwen Cui1\nShijian Lu1‚Ä† Feiying Ma2 Xuansong Xie2 Chunyan Miao1\n1 Nanyang Technological University 2 DAMO Academy, Alibaba Group\n{yingchen001, ronglian001, kaiwen001}@e.ntu.edu.sg, {fnzhan, shijian.lu, ascymiao}@ntu.edu.sg\n{jianxiong.pjx, feiying.mfy}@alibaba-inc.com, xingtong.xxs@taobao.com\nFigure 1. The proposed BAT-Fill introduces a novel bidirectional autoregressive transformer that captures deep bidirectional contexts\nfor autoregressive generation of diverse contents in image inpainting. Evaluations over multiple public datasets show that BAT-Fill can\ngenerate realistic and reasonable image contents. The three illustrative sample images from top to bottom are selected from the datasets\nCelebA-HQ [21], Places2 [65], and Paris StreetView [32], respectively.\nAbstract\nImage inpainting is an underdetermined inverse prob-\nlem, which naturally allows diverse contents to Ô¨Åll up\nthe missing or corrupted regions realistically. Prevalent\napproaches using convolutional neural networks (CNNs)\ncan synthesize visually pleasant contents, but CNNs suf-\nfer from limited perception Ô¨Åelds for capturing global fea-\ntures. With image-level attention, transformers enable to\nmodel long-range dependencies and generate diverse con-\ntents with autoregressive modeling of pixel-sequence distri-\nbutions. However, the unidirectional attention in autore-\ngressive transformers is suboptimal as corrupted image re-\ngions may have arbitrary shapes with contexts from any di-\nrection. We propose BAT-Fill, an innovative image inpaint-\ning framework that introduces a novel bidirectional autore-\n*Equal contribution\n‚Ä†Corresponding author\ngressive transformer (BAT) for image inpainting. BAT uti-\nlizes the transformers to learn autoregressive distributions,\nwhich naturally allows the diverse generation of missing\ncontents. In addition, it incorporates the masked language\nmodel like BERT, which enables bidirectionally modeling of\ncontextual information of missing regions for better image\ncompletion. Extensive experiments over multiple datasets\nshow that BAT-Fill achieves superior diversity and Ô¨Ådelity\nin image inpainting qualitatively and quantitatively.\n1. Introduction\nAs an ill-posed problem, image inpainting naturally al-\nlows numerous solutions as long as the restored images are\nrealistic and semantically reasonable as illustrated in Fig. 1.\nHowever, it remains a great challenge to synthesize diverse\nwhile realistic contents that maintain integrity and consis-\ntency with the uncorrupted image regions, especially when\narXiv:2104.12335v3  [cs.CV]  1 Jun 2021\nthe corrupted image regions are large and rich in complex\ntextures and structures.\nRecently, GAN-based (generative adversarial network)\ninpainting [32, 50, 30, 26] has achieved remarkable\nprogress by training with reconstruction and adversarial\nlosses over large-scale datasets. However, these methods\nare trained to learn the one-to-one mapping from masked\nimages to complete images, which results in the incapac-\nity of producing diverse inpainting results. In contrast to\ndeterministic inpainting, a few studies [64, 61] attempt for\ndiverse inpainting with variational auto-encoder (V AE) net-\nworks [23], but the inpainting quality is often compro-\nmised while generating complex structural and texture pat-\nterns due to the limited capacity of parametric distribu-\ntions [63]. Instead of parametric distribution modeling like\nV AE-based methods, [33] utilizes a CNN-based conditional\nnetwork to learn an autoregressive distribution for recov-\nering diverse and structural features. As the autoregres-\nsive models are optimized to encode unidirectional context\nonly, which means the informative contexts of valid pix-\nels after the current position are substantially ignored. To\nexplore bidirectional context, [41] adopts the masked lan-\nguage model (MLM) like BERT [11]. However, MLM pre-\ndicts the masked tokens independently which may oversim-\nplify the complex context dependency in the data [37] and\nresult in inconsistency in the generated results.\nIn this paper, we propose a bidirectional and autoregres-\nsive transformer (BAT) that marries the best of autoregres-\nsive modeling and MLM to model deep bidirectional con-\ntexts in an autoregressive manner. In the proposed BAT, we\npermute the input sequence by sorting the valid and missing\npixels and start autoregressive modeling at the position of\nthe Ô¨Årst missing pixel. With all available contexts in front,\nBAT can exploit bidirectional contexts and spatial depen-\ndency simultaneously. In addition, we adopt the two-stage\ncompletion procedure as reported in [41] and develop BAT-\nFill, an image inpainting network that Ô¨Årstly recovers the di-\nverse yet coherent image structures based on the proposed\nBAT and then exploits a CNN-based texture generator to\nup-sample the coarse structures and synthesize texture de-\ntails. Extensive experiments show that BAT-Fill achieves\nsuperior image inpainting performance.\nThe main contributions of this work can be summarized\nin three aspects. First, we adopt the transformers to learns\nan autoregressive distribution for diverse image inpainting,\nwhich effectively improves the modeling capacity for long-\nrange dependencies and global structures. Second, we de-\nsign a novel bidirectional and autoregressive transformer\n(BAT) that captures bidirectional information and estab-\nlishes output dependency simultaneously. Third, extensive\nexperiments over multiple datasets show that the proposed\nmethod achieves superior performance as compared with\nthe state-of-the-art in both inpainting quality and inpainting\ndiversity.\n2. Related Work\n2.1. Image Inpainting\nAs an ill-posed problem, realistic and high-Ô¨Ådelity im-\nage inpainting is a challenging task that has been studied\nfor years. Based on the inpainting outcome, most existing\nimage inpainting methods can be broadly classiÔ¨Åed into two\ncategories including deterministic image inpainting and di-\nverse image inpainting.\n2.1.1 Deterministic Image Inpainting\nTraditional methods address image inpainting challenge\nthrough either image diffusion [5, 1] or using image patches\n[3, 15, 9]. However, diffusion-based methods often in-\ntroduce diffusion-related blurs, which tends to fail while\nthe missing or corrupted image regions are large [6, 2, 4].\nPatch-based methods can work well for the inpainting of\nstationary background with repeating patterns. However,\nthey struggle in completing large missing regions of com-\nplex scenes as the patch-based approach relies heavily on\npatch-wise matching of low-level features.\nGenerative adversarial networks (GANs) [14] have been\ninvestigated extensively in various image synthesise tasks\nsuch as image translation [31, 36, 19, 53, 51, 56, 54], image\nediting [48, 44, 43], image composition [24, 59, 58, 52, 57,\n55], etc. SpeciÔ¨Åcally for image inpainting, Pathaket al. [32]\nÔ¨Årst apply adversarial learning to the image inpainting task.\nTo further improve the adversarial learning within local re-\ngions, Iizuka et al. [18] introduce an extra local discrimina-\ntor to enforce the local consistency. As the local discrim-\ninator uses fully-connected layers and can only deal with\nmissing regions of Ô¨Åxed shapes, Yu et al. [49] inherit the\ndiscriminator from PatchGAN [19] due to its great success\nin image translation. Yan et al. [46] propose patch-swap to\nmake use of distant feature patches for the better inpainting\nquality. Liu et al. [25] design partial convolutions to alle-\nviate the negative inÔ¨Çuence of the masked regions. Yu et\nal. [50] present a novel free-form image inpainting system\nbased on an end-to-end generative network with gated con-\nvolutions. To generate reasonable structures and realistic\ntextures, Nazeri et al. [30] and Xu et al. [45] utilize edge\nmaps as structural guidance for image inpainting, and Ren\net al. [35] instead propose to use edge-preserved smooth im-\nages as structural guidance. Liu et al. [27] propose feature\nequalizations to improve the consistency between structures\nand textures. As aforementioned methods focus on recon-\nstructing the ground truth instead of generating pluralistic\ninpainting, they are constraint to generate a deterministic\ninpainting image for each incomplete image.\nAuto-regressive\nùëÉùë•!!|ùëã\\#,ùëÄ,ùëã$!!\n32√ó32 Masked Imageùêº!\n‚Ä¶\nConcat\nDownSampling\n‚Ä¶\nS1\nS2\nSN\nSampling\n DiverseStructures\nTextureGenerator\nInpainting Resultsùêº\"#$\n‚Ä¶\nBATBi-directional\nFigure 2. Overview of the proposed image inpainting method: Given a Masked Image Im, the proposed BAT -Fill Ô¨Årst Down\nSample it to a lower resolution and feed the down-sampled image to theBidirectional Autoregressive Transformer(BAT ) for the\nrecovery of Diverse Structures. Taking the recovered image structures and image style features (extracted by the Encoder) as inputs,\nthe Texture Generatorsynthesizes high-resolution texture and produces the Inpainting Results Iout.\n2.1.2 Diverse Image Inpainting\nTo achieve pluralistic image inpainting with plausible Ô¨Ålling\ncontents, Zheng et al. [64] propose a V AE-based network\nwith a dual pipeline, which trades off between reconstruct-\ning ground truth and maintaining the diversity of the in-\npainting results. Similarly, Zhao et al. [61] propose a V AE-\nbased model and leverage a reference image to improve the\ndiversity. Although the above methods achieve certain di-\nversities to some extent, completion quality of V AE-based\nmethods is limited due to variational training. Recently,\nZhao et al. [62] propose a co-modulated GAN to incorpo-\nrate the image condition and the stochastic generation of un-\nconditional generative model for diverse inpainting. Penget\nal. [33] introduce a hierarchical vector quantized variational\nauto-encoder (VQ-V AE) to quantize the context representa-\ntion and archive diverse structure generation in an autore-\ngressive way. Sharing a similar framework with us, Wan et\nal. [41] propose to apply transformer for diverse structure\ngeneration using the objective of BERT [11]. In contrast,\nwe propose a novel Bidirectional and Autoregressive Trans-\nformer (BAT) which inherits the advantages of autoregres-\nsive models and bidirectional models and achieves superior\nimage inpainting performance.\n2.2. Transformers in Vision\nTransformer has emerged as a powerful tool to model\nthe interactions between sequences regardless of the rela-\ntive position. Specially, Vaswani et al. [40] employ trans-\nformers for image classiÔ¨Åcation by treating an image as a\nsequence of patches. DETR [7] utilizes transformer de-\ncoder to model object detection as an end-to-end dictionary\nlookup problem with learnable queries, thus removing the\nhand-crafted processes such as Non-Maximal Suppression\n(NMS). Based on DETR, deformable DETR [66] further\nintroduces a deformable attention layer to focus on a sparse\nset of contextual elements which achieves fast convergence\nand better detection performance. Recently, Vision Trans-\nformer (ViT) [12] showed that pure-transformer networks\ncan also achieve excellent image classiÔ¨Åcation performance\nas compared with CNN-based methods. DeiT [39] further\nextends ViT by introducing a novel distillation approach.\nBoTNet [38] replaces the spatial 3 √ó3 convolution layers\nwith multi-head self-attention in certain stages of the orig-\ninal ResNet [16], demonstrating very competitive perfor-\nmance on different visual recognition tasks. Esseret al. [13]\nadapt transformers and VQ-V AE in both conditional and\nunconditional generation tasks, and achieve high-Ô¨Ådelity\nsynthesis of megapixel images.\nInstead of leveraging features of transformers for high-\nlevel tasks or generate pixels autoregressively, we specif-\nically propose a novel Bidirectional and Autoregressive\nTransformer (BAT) for image inpainting, so that the model\ncan learn both bidirectional context and output dependency.\n3. Proposed Method\nAs illustrated in Fig. 2, the proposed BAT-Fill consists of\ntwo major parts including a diverse-structure generator for\nthe reconstruction of coarse image structures and a texture\ngenerator for the generation of Ô¨Åne-grained texture details.\nThe diverse-structure generator incorporates and adapts a\ntransformer architecture that models the distribution of\nglobal structural information and recovers complete and\ncoherent low-resolution structures S1,S2,¬∑¬∑¬∑ ,SN given a\nMasked Image Im as input. Under the guidance of coarse\nstructure Si,i ‚àà[1,N] and corrupted image Im, the Texture\nGenerator synthesizes high-resolution Ô¨Åne-grained texture\nto produce the Inpainting Results Iout. Once the full model\nis trained, we can sample different image structures Si,i ‚àà\n[1,N] by the diverse-structure generator and thus generate\ndiverse inpainting results with the texture generator, more\ndetails to be discussed in the ensuing subsections.\n3.1. Diverse-structure Generator\n3.1.1 Context Representation\nTo relieve the pressure of quadratic complexity incurred in\ntransformer, we adapt the low-resolution image with the\nsize of 32 √ó32 √ó3 to represent the coarse structure. As the\nautoregressive generation requires discrete distribution, the\npixel value should be treated as classes to the model, which\nleads to the dimensionatliy of 2563 for each pixel of the 8-\nbit RGB images. Following Chen et al. [8], a color palette\nis applied to further reduce the dimensionality to 512 while\nfaithfully preserving the main structure of original images,\nwhich is generated byk-means clustering of RGB pixel val-\nues with k=512 from ImageNet [10] dataset.\n3.1.2 Bidirectional and Autoregressive Transformer\nAutoregressive (AR) modeling and masked language mod-\neling (MLM) in BERT [11] are two representative ob-\njectives for exploiting large language corpora in lan-\nguage processing tasks. Given a discrete sequence X =\n{x1,x2,...,x L}where Lis the length of X, AR model is\noptimized by maximizing the unidirectional likelihood:\nlog P(x; Œ∏) = EX\nL‚àë\nt=1\nlog P(xt|X<t; Œ∏), (1)\nwhere Œ∏is the parameters of the model. In contrast, MLM\naims to reconstruct corrupted data with the masked posi-\ntions M = {m1,m2,...,m K}, where K is the number of\nmasked tokens. Each masked position of the corrupted data\nis indicated by a special token [M] following BERT [11].\nDenoting the masked tokens as XM and unmasked tokens\nas X\\M , the objective of MLM can be formulated by:\nlog P(XM |X\\M ; Œ∏) = EX\n‚àë\nmk‚ààM\nlog P(xmk |X\\M ; Œ∏).\n(2)\nAR and MLM differ from two aspects as deÔ¨Åned in Eqs. 1\nand 2. The Ô¨Årst aspect lies with output dependency, where\nMLM predicts the masked tokens separately and indepen-\ndently which may oversimplify the complex context depen-\ndency in the data [37]. As a comparison, AR factorizes the\npredicted tokens with the product rule, which establishes\nthe output dependency and produces better predictions. The\nsecond aspect lies with context dependency, where AR is\nonly conditioned on the tokens up to the current position\n(in a Ô¨Åxed order), while MLM has access to bidirectional\ncontextual information. Therefore, MLM is more suitable\nfor image inpainting as the missing or corrupted image re-\ngions often have arbitrary shapes with rich variation in the\nneighboring background.\nWe propose a novel Bidirectional and Autoregressive\nTransformer (BAT) that inherits the advantages of AR and\nMLM to achieve bidirectional context modeling and output\ndependency simultaneously. The training objective of the\nBAT is formulated by:\nLBAT = EX\n‚àë\nmk‚ààM\nlog P(xmk |X\\M ,M,X <mk ; Œ∏). (3)\nWe Ô¨Årst project all the tokens into a d-dimensional token\nembedding and add a learnable position embedding over\nthe token embedding to preserve the positional information.\nUnlike XLNet [47] which randomly permutes the input se-\nquence to capture the bidirectional context, we permute all\nunmasked tokens X\\Œ† in the front while maintaining the\noriginal order of the masked tokens for better predicting\ntheir positions. Moreover, the positional information of all\nmasked tokens will be conditioned for better modeling of\nthe full input sequence (e.g. the counts and positions of\nmasked tokens in the sequence). The proposed BAT model\nis then adopted to predict the masked tokens as illustrated\nin Fig. 3.\nAs shown in Fig. 3, there is a masked sequence X =\n{x1,[M],[M] ,x4,[M]}with length L = 5 and positions\n2,3,5 being masked. After permutation and inserting\nthe full mask tokens, we have the non-predicted tokens\n(X\\M ,M) = ( x1,x4,[M],[M],[M]) which provides the\nbidirectional context. For the predicted part, we have the in-\nput tokens ([M],x2,x3) to predict their corresponding next\ntokens i.e. (x2,x3,x5). Here we use the mask token in-\nstead of x1 to predict x2 to encourage the leverage of po-\nsitional information. We apply bidirectional modeling [11]\nto non-predicted tokens and autoregressive modeling to the\npredicted tokens to avoid future information leakage. For\nexample, while predicting x3, the model could attend to\nx4 in non-predicted tokens and meanwhile the previously\n‚Äòpredicted‚Äô token x2. Hence, we could capture bidirec-\ntional context and establish output dependency simultane-\nously with the proposed BAT.\n3.1.3 Transformer Architecture\nIn this work, we adapt GPT [34] as our network architec-\nture. The network is a decoder-only transformer that con-\nsists of Nstacked decoder blocks. Given an intermediate\nembedding Hn at the n-th layer, the decoder block can be\n[M][M][M][M]\nx1+p1x4+p4[M]+p2[M]+p3[M]+p5[M]+p2x2+p2x3+p3\n[M]+p5[M]+p3[M]+p2x4+p4x1+p1 [M]+p2x2+p2x3+p3\nnon-predictedpredicted(a) (b)\nSelf-AttentionAttendtobidirectionalcontextAttendtopredictedtokensNotattended\nx2 x3 x5\nx1\np1\nx4 x2 x3\np4 p2 p3 p5 p2 p2 p3\npPositionalEmbeddingxTokenEmbeddingMaskToken[M]\nFigure 3. Illustrations of BAT and its attention mask. (a) All unmasked tokens (i.e. X1, X4) is permuted to the front of the sequence which\nis followed by masked tokens with positional embedding (i.e. p2, p3, p5). It provides bidirectional context for the autoregressive modeling\nof masked tokens (i.e. X2, X3, X5). (b) BAT allows to attend non-predicted tokens and previously predicted tokens for the prediction of\nmasked future tokens. For example, while predicting X3, the model could attend to X1, X4 (non-predicted tokens) and X2 (previously\npredicted token) simultaneously. Meanwhile, the future tokens are not attended to prevent information leakage in autoregressive modeling.\nformulated by:\nHn = Hn + MA(LN(Hn)) (4)\nHn+1 = Hn + MLP(LN(Hn)), (5)\nwhere MA, LN and MLP stand for multihead self-\nattention, layer normalization, and fully-connected layers,\nrespectively. For self-attention, we apply a customized\nmask to the L√óLmatrix of attention logits as illustrated\nin Fig. 3. At the Ô¨Ånal layer of the transformer, a learnable\nlinear projection is employed to map HN to logits, which\nparameterizes the conditional distribution for each pixel.\nDuring inference, we follow the raster-scan order to pre-\ndict each masked token bidirectionally and autoregressively.\nWe adopt a top- Ksampling strategy to randomly sample\nfrom the Kmost likely next words. The predicted token is\nthen concatenated with the input sequence as conditions for\nthe generation of next masked token. This process repeats\niteratively until all the masked tokens are sampled. Finally,\nthe generated discrete sequence can be converted back to\nthe RGB values with the aforementioned color palette.\n3.2. Texture Generator\n3.2.1 Network Architecture\nAs the inpainting diversity can be achieved by sampling the\nreconstructed structures S, we take the advantages of efÔ¨Å-\nciency and texture representation capacity of CNNs to learn\na deterministic mapping between low-resolution structures\nS and high-resolution completed image Iout. The texture\ngenerator thus utilizes CNN layers and adversarial train-\ning to up-sample the reconstructed structures and replen-\nish high-Ô¨Ådelity texture details by leveraging the styles of\nthe valid pixels of input imageIm. In particular, we employ\ntwo encoders to encode the low-resolution structures and in-\nput images into two high-level CNN representations of the\nsame dimension. We then concatenate them together as the\ninput of a few consecutive residual blocks with different di-\nlation rates. Finally, a SPADE [31] generator is employed to\nincorporate the modulated style of input images and gradu-\nally up-sample the texture features to the target resolution.\nMeanwhile, all vanilla convolutions are replaced by gated\nconvolution [50].\n3.2.2 Loss Functions\nThe training of the texture generator is driven by the combi-\nnation of several losses including a reconstruction loss, an\nadversarial loss, and a perceptual loss. For clarity, we de-\nnote the texture generator asGt, the ground truth asIgt, and\nthe completed image as Iout. Firstly, a reconstruction loss\nLrec between Iout and Igt can be measured as follows:\nLrec = ||Iout ‚àíIgt||1,\nBesides, a CNN-based discriminatorDtogether with an ad-\nversarial loss is employed to synthesize Ô¨Åne texture details.\nSpeciÔ¨Åcally, the texture generator Gt and discriminator D\nare jointly trained with hinge loss [19], where the adver-\nsarial losses for the discriminator and generator are deÔ¨Åned\nby:\nLD\nadv = EIgt [ReLU(1 ‚àíD(Igt)] + EIout [ReLU(1 + D(Iout)]\nLGt\nadv = ‚àíEIout [D(Iout)],\nNext, we penalize the perceptual and semantic discrepancy\nvia the perceptual loss [20] with a pretrained VGG-19 net-\nwork:\nLperc =\n‚àë\ni\nŒªi||Œ¶i(Iout) ‚àíŒ¶i(Igt)||1\n+Œªl||Œ¶l(Iout) ‚àíŒ¶l(Igt)||2,\nwhere Œªi are balancing weights, Œ¶i is the activation of i-\nth layer of the VGG-19 model (including relu1 2, relu2 2,\nrelu3 2, relu4 2 and relu5 2), Œ¶l represents the activation\nmaps of relu4 2 layer which mainly extracts semantic fea-\nture. The texture generator is trained by optimizing the\ncombination of aforementioned losses:\nLGt = min\nGt\nmax\nD\n(ŒªrecLrec + ŒªadvLGt\nadv + ŒªpercLperc),\nwhere Œªrec, Œªadv, and Œªperc are empirically set at 1.0, 1.0\nand 0.2, respectively, in our implementation.\n4. Experiments\n4.1. Experimental Settings\n4.1.1 Datasets\nWe conduct experiments over three public datasets that have\ndifferent characteristics as listed:\n‚Äì CelebA-HQ [21]: It is a high-quality version of the\nhuman face dataset CelebA [28] with 30,000 aligned\nface images. We follow the split in [50] that produces\n28,000 training images and 2,000 validation images,\nwhere 1,000 validation images are randomly sampled\nin evaluations.\n‚Äì Places2 [65]: It consists of more than 1.8M natural\nimages of 365 different scenes. We adopt the same 800\nimages from validation set with [41] in evaluations.\n‚Äì Paris StreetView [32]: It is a collection of street view\nimages in Paris, which contains 14,900 training images\nand 100 validation images.\n4.1.2 Compared Methods\nWe compare our method with a number of state-of-the-art\nmethods as listed:\n‚Äì GC [50]: It is also known as DeepFill v2, a two-stage\nmethod that leverages gated convolutions.\n‚Äì EC [30]: It is a two-stage method that Ô¨Årst predicts\nsalient edges to guide the generation.\n‚Äì MEDFE [26]: It is a mutual encoder-decoder that\ntreats features from deep and shallow layers as struc-\ntures and textures of an input image.\n‚Äì PIC [64]: It is a probabilistically principled framework\nthat leverages V AE to generate diverse image inpaint-\ning.\n‚Äì ICT [41]: It is a diverse inpainting framework that\ncombine the merits of transformers and CNNs for\nhigh-Ô¨Ådelity image inpainting.\n4.1.3 Evaluation Metrics\nWe perform evaluations by using Ô¨Åve widely adopted eval-\nuation metrics: 1) Fr ¬¥echet Inception Score (FID) [17] that\nevaluates the perceptual quality by measuring the distri-\nbution distance between the synthesized images and real\nimages; 2) mean ‚Ñì1 error; 3) peak signal-to-noise ratio\n(PSNR); 4) structural similarity index (SSIM) [42] with a\nwindow size of 51; 5) Learned Perceptual Image Patch Sim-\nilarity (LPIPS) [60] that evaluates the diversity of generated\nimages. The average scores of LPIPS are calculated be-\ntween random pairs of sampled inpainting results.\n4.1.4 Implementation Details\nThe proposed method is implemented in PyTorch. The net-\nwork is trained using256√ó256 images with random irregu-\nlar masks [25]. The diverse-structure generator and texture\ngenerator are trained using 256 √ó256 images with random\nirregular masks [25]. We train the diverse-structure genera-\ntor with AdamW [29] with Œ≤1 = 0.9, Œ≤2 = 0.95 and learn-\ning rate of 3e-4 following [8]. For the texture generator, we\nuse Adam optimizer [22] with Œ≤1 = 0 and Œ≤2 = 0.9, and\nset the learning rate at 1e-4 and 4e-4 for the generator and\ndiscriminators, respectively. Learning rate decay is applied\nfor the training of both networks, and the experiments are\nconducted on 4 NVIDIA(R) Tesla(R) V100 GPU.\n4.2. Quantitative Evaluation\nExtensive quantitative evaluations have been conducted\nover the three datasets with irregular masks [25]. The ir-\nregular masks in the experiments are categorized according\nto the mask ratios, and an additional category ‚Äòrandom‚Äô is\nevaluated which randomly samples masks with ratios vary-\ning from 20% to 60%. The performance of the compared\nmethods was acquired by using the publicly available pre-\ntrained models or implementation codes. 1 2 3 4 .\nWe compare the proposed method with both determin-\nistic and diverse image inpainting methods. Note that all\nreference metrics such as ‚Ñì1, SSIM ,and PSNR are in favor\nof deterministic inpainting methods where the prediction is\ndirectly compared with the ground truth. Different from\nPIC [64] that unitizes its discriminator to sort the results,\nour method adapts the top-50 sampling strategy and use\nall random samples for fair comparisons, which means our\nmethod directly generates the stochastic inpainting without\nthe additional Ô¨Åltering.\n1https://github.com/JiahuiYu/generative_\ninpainting\n2https://github.com/knazeri/edge-connect\n3https://github.com/KumapowerLIU/\nRethinking-Inpainting-MEDFE\n4https://github.com/lyndonzheng/\nPluralistic-Inpainting\nInput EC GC PIC2PIC1 PIC3Ground Truth\nOurs1 Ours2 Ours4Ours3 Ours5 Ours6 Ours7\nFigure 4. Qualitative comparisons of the proposed BAT-Fill with the state-of-the-art: BAT-Fill generates more realistic and diverse image\ninpainting over the dataset CelebA-HQ[21] with irregular masks.\nInputEC GC PIC2MEDFE PIC3\nGround TruthICT1 ICT2 Ours1 Ours2 Ours3\nICT3\nPIC1\nInputEC GC PIC2MEDFE PIC3\nGround TruthICT1 ICT2 Ours1 Ours2 Ours3\nICT3\nPIC1\nFigure 5. Qualitative comparison the proposed BAT-Fill with the state-of-the-art: BAT-Fill generates more realistic and diverse image\ninpainting over Places2 [65] with irregular masks.\nTable 2 shows the inpainting performance over the\ndataset Paris StreetView [32]. Compared with determinis-\ntic methods GC, EC, and MEDFE, the proposed method\nachieves the best FID scores over different mask ratios and\nconsistently outperforms the diverse inpainting method PIC\nin both inpainting quality (FID) and inpainting diversity\n(LPIPS). In addition, Table 1 shows the inpainting per-\nformance over CelebA-HQ [21] and Places2 [65]. For\nCelebA-HQ, our method consistently outperforms all com-\npared methods, especially in FID scores. For Places2, our\nmethod achieves comparable performance with determinis-\ntic methods in all evaluation metrics, and it generally out-\nperforms them in FID scores. In addition, the numerical\nresults of BAT-Fill suggest a clear superiority over the di-\nverse inpainting method PIC [64], and better FID scores\nthan ICT [41].\nInputEC GC PIC1MEDFE PIC2\nGround TruthOurs1 Ours3Ours2 Ours4 Ours5 Ours6\nPIC3\nFigure 6. Qualitative comparison the proposed BAT-Fill with the state-of-the-art: BAT-Fill generates more realistic and diverse image\ninpainting over Paris StreetView [32] with irregular masks.\nTable 1. Quantitative comparison of the proposed BAT-Fill with state-of-the-art methods over CelebA-HQ [21] and Places2 [65] validation\nimages (1,000) with irregular masks [25] (‚àódenotes that we trained the model based on ofÔ¨Åcial implementations, ‚Ä†denotes the results are\ncopied from [41]). For each metric, the best score is highlighted in bold, and the best score for diverse inpainting methods ( i.e. PIC [64]\nand Ours) is highlighted in underline.\nMethods Dataset FID‚Üì ‚Ñì1(%)‚Üì PSNR‚Üë SSIM‚Üë\n20-40% 40-60% Random 20-40% 40-60% Random 20-40% 40-60% Random 20-40% 40-60% Random\nEC‚àó[30]\nCelebA-HQ [21]\n9.06 16.45 12.46 2.19 4.71 3.40 26.60 22.14 24.45 0.923 0.823 0.877\nGC [50] 14.12 22.80 18.10 2.70 5.19 3.88 25.17 21.21 23.32 0.907 0.805 0.858\nPIC [64] 10.21 18.92 14.12 2.50 5.65 4.00 25.92 20.82 23.46 0.919 0.780 0.852\nOurs 6.32 12.50 9.33 1.91 4.57 3.18 27.82 22.40 25.21 0.944 0.834 0.890\nEC‚Ä†[30]\nPlaces2 [65]\n25.64 39.27 30.13 2.20 4.38 2.93 26.52 22.23 25.51 0.880 0.731 0.831\nGC‚Ä†[50] 24.76 39.02 29.98 2.15 4.40 2.80 26.53 21.19 25.69 0.881 0.729 0.834\nMEDFE‚Ä†[26] 26.98 45.46 31.40 2.24 4.57 2.91 26.47 22.27 25.63 0.877 0.717 0.827\nPIC‚Ä†[64] 26.39 49.09 33.47 2.36 5.07 3.15 26.10 21.50 25.04 0.865 0.680 0.806\nICT‚Ä†[41] 21.60 33.85 25.42 2.44 4.31 2.67 26.50 22.22 25.79 0.880 0.724 0.832\nOurs 17.78 32.55 22.16 2.15 4.64 2.84 26.47 21.74 25.69 0.879 0.704 0.826\n4.3. Qualitative Evaluations\nFigs. 4, 5, and 6 show the qualitative comparisons be-\ntween BAT-Fill and the state-of-the-art image inpainting\nmethods over the validation set of CelebA-HQ [21], Places2\n[65] and Paris StreetView [32], respectively.\nWe Ô¨Årst evaluate and compare BAT-Fill with EC [30],\nGC [50], and PIC [64] on CelebA-HQ [21] which contains\nfacial images with similar semantics. As shown in Fig. 4,\nthough EC [30] and GC [50] can synthesize complete fa-\ncial images with reasonable semantics, they tend to gen-\nerate distorted facial structures and artifacts in the missing\nregions which degrades inpainting greatly. In addition, EC\n[30] and GC [50] can only generate deterministic inpaint-\ning, which limits their applicability clearly. Both PIC [64]\nand BAT-Fill can generate diverse inpainting. However, the\nPIC generated images share similar makeups and facial fea-\ntures and thus have limited diversity. As a comparison, the\nBAT-Fill generated facial images vary across a wide range\nof makeups and facial features and contain much less arti-\nfacts, demonstrating that BAT-Fill can produce more diverse\nand realistic inpainting.\nNext, we evaluate and compare BAT-Fill with EC [30],\nGC [50], MEDFE [26], and PIC [64] on the datasets Places2\n[65] and Paris StreetView [32] where images have various\nsemantics. In addition, visual comparison with ICT [41] is\nconducted over Places2 [65] dataset. As shown in Fig. 5,\nEC [30], GC [50] and MEDFE [26] tend to generate blurs\nand even corrupted texture in the inpainting images. The\nPIC [64] synthesized images suffer from unreasonable se-\nmantics, obvious artifacts, and limited diversity. Both ICT\n[41] and BAT-Fill achieved realistic image inpainting with\nmuch less artifacts and better diversity compared with other\nmethods. For Paris StreetView [32], BAT-Fill produced\nmore diverse and plausible results than the PIC [64], and\nmeanwhile achieved comparable or even better inpainting\nquality compared with the deterministic methods.\nTable 2. Quantitative comparison of the proposed BAT-Fill with\nstate-of-the-art methods over Paris StreetView [32] validation im-\nages (100) with irregular masks [25] ( ‚àódenotes that we trained\nthe model based on ofÔ¨Åcial implementations). For each metric,\nthe best score is highlighted in bold, and the best score for di-\nverse inpainting methods (i.e. PIC [64] and Ours) is highlighted in\nunderline.\nMetrics Mask\nRatio\nMethods\nEC [30] GC‚àó[50] MEDFE [26] PIC [64] Ours\nFID‚Üì\n20-40%\n42.81 71.02 36.84 56.83 36.19\n‚Ñì1(%)‚Üì 2.63 3.56 2.29 3.43 2.70\nPSNR‚Üë 26.76 23.95 27.64 24.80 26.52\nSSIM‚Üë 0.874 0.796 0.898 0.817 0.864\nLPIPS‚Üë N/A N/A N/A 0.046 0.076\nFID‚Üì\n40-60%\n72.78 98.32 77.26 90.91 64.20\n‚Ñì1(%)‚Üì 5.18 6.31 5.54 7.47 5.83\nPSNR‚Üë 22.77 20.83 22.01 20.12 21.89\nSSIM‚Üë 0.712 0.631 0.704 0.570 0.678\nLPIPS‚Üë N/A N/A N/A 0.127 0.147\nFID‚Üì\nRandom\n55.29 84.16 54.99 72.16 48.19\n‚Ñì1(%)‚Üì 3.63 4.64 3.58 4.94 3.96\nPSNR‚Üë 25.04 22.61 25.24 22.97 24.50\nSSIM‚Üë 0.806 0.727 0.818 0.718 0.786\nLPIPS‚Üë N/A N/A N/A 0.082 0.106\n4.4. Ablation Study\nWe study the effectiveness of the proposed BAT by con-\nducting ablation studies over Paris StreetView [31]. In the\nablation study, we remove the two key components from\nBAT respectively, which result in two models: 1) w/o bidi-\nrectional context, where we will get the same objective with\nthe autoregressive model that predicts the missing tokens\nby conditioning on previous tokens with unidirectional at-\ntention; 2) w/o autoregressive model, where the model is\nequivalent to MLM that independently reconstruct the miss-\ning tokens. To measure the diversity of MLM, we employ\na Gibbs sampling to iteratively sample tokens and place the\npredicted tokens into the original sequence instead of di-\nrectly output all the predicted tokens. For a fair comparison,\nwe apply the same irregular masks (mask ratios 40-60%) on\nthe same low-resolution images (32 √ó32) from the valida-\ntion set of Paris StreetView [31]. After predicting the same\ninputs, the reconstructed structures of each model are eval-\nuated without applying the texture generator.\nAs shown in Table 3, using AR greatly degrades the\nquality of the reconstructed structures, and the high diver-\nsity measured by LPIPS is also largely attributed to the\npoor reconstruction quality. MLM performs reasonably\nwell as it exploits the bidirectional context for inpainting.\nHowever, the proposed BAT clearly outperforms in recon-\nstruction quality which is mainly reÔ¨Çected by FID, and it\nachieves comparable diversity as reÔ¨Çected by LPIPS. This is\nmainly because BAT models the output dependency to align\nTable 3. Ablation study of the proposed BAT over Paris\nStreetView [31] validation set (100) with irregular masks [25] and\nmask ratios of 40%-60%.\nModels FID‚Üì ‚Ñì1(%)‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üë\nw/o bidirectional 59.60 8.599 19.32 0.518 0.0518\nw/o autoregressive49.62 6.38 22.01 0.655 0.0304\nOurs 40.91 5.76 23.01 0.714 0.0301\nthe future predictions with previously predicted tokens and\nimproves the consistency of the reconstructed structures.\nOverall, the ablation study demonstrates that the proposed\nBAT addresses the constraints of the AR and MLM effec-\ntively.\n5. Conclusion\nThis paper presents BAT-Fill, a novel image inpainting\nframework that achieves realistic and diverse inpainting by\nleveraging the autoregressive transformers with their pow-\nerful long-dependency modeling capacity. To improve the\nquality and diversity of inpainting, we propose a novel bidi-\nrectional and autoregressive transformer (BAT) to model\nthe bidirectional context and output dependency simultane-\nously. Extensive experiments show that BAT-Fill achieves\nsuperior image inpainting in terms of both quality and di-\nversity. Moving forward, we will explore the feasibility\nof adapting our idea to other image recovery or generation\ntasks by replacing the non-predicted part of BAT with other\nconditions such as semantic label, edge, and pose.\nReferences\n[1] Coloma Ballester, Marcelo Bertalmio, Vicent Caselles,\nGuillermo Sapiro, and Joan Verdera. Filling-in by joint inter-\npolation of vector Ô¨Åelds and gray levels. IEEE Trans. Image\nProcess., 10(8):1200‚Äì1211, 2001.\n[2] Coloma Ballester, Vicent Caselles, Joan Verdera, Marcelo\nBertalmio, and Guillermo Sapiro. A variational model for\nÔ¨Ålling-in gray level and color images. In ICCV, volume 1,\npages 10‚Äì16, 2001.\n[3] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and\nDan B Goldman. Patchmatch: a randomized correspondence\nalgorithm for structural image editing. ACM Transactions on\nGraphics (TOG), 28(3):1‚Äì11, 2009.\n[4] Marcelo Bertalmio, Andrea L Bertozzi, and Guillermo\nSapiro. Navier-stokes, Ô¨Çuid dynamics, and image and video\ninpainting. In CVPR, volume 1, pages 355‚Äì355, 2001.\n[5] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and\nColoma Ballester. Image inpainting. In Proceedings of the\n27th annual conference on Computer graphics and interac-\ntive techniques, pages 417‚Äì424, 2000.\n[6] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, and\nStanley Osher. Simultaneous structure and texture image in-\npainting. TIP, 12(8):882‚Äì889, 2003.\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020.\n[8] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In International Conference on Ma-\nchine Learning, pages 1691‚Äì1703. PMLR, 2020.\n[9] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B\nGoldman, and Pradeep Sen. Image melding: Combining in-\nconsistent images using patch-based synthesis. ACM Trans-\nactions on graphics (TOG), 31(4):1‚Äì10, 2012.\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248‚Äì255. Ieee, 2009.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[13] Patrick Esser, Robin Rombach, and Bj ¬®orn Ommer. Tam-\ning transformers for high-resolution image synthesis. arXiv\npreprint arXiv:2012.09841, 2020.\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems , pages 2672‚Äì2680,\n2014.\n[15] James Hays and Alexei A Efros. Scene completion using\nmillions of photographs. ACM Transactions on Graphics\n(TOG), 26(3):4‚Äìes, 2007.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. 2016.\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Advances in Neural Information Processing Sys-\ntems, pages 6626‚Äì6637, 2017.\n[18] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.\nGlobally and locally consistent image completion. ACM\nTrans. Graph., 36(4):1‚Äì14, 2017.\n[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In CVPR, pages 1125‚Äì1134, 2017.\n[20] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nEuropean Conference on Computer Vision, pages 694‚Äì711.\nSpringer, 2016.\n[21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. International Conference on Learning Repre-\nsentations, 2018.\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014.\n[23] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013.\n[24] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,\nand Simon Lucey. St-gan: Spatial transformer generative\nadversarial networks for image compositing. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 9455‚Äì9464, 2018.\n[25] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions. In ECCV, pages\n85‚Äì100, 2018.\n[26] Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao\nYang. Rethinking image inpainting via a mutual encoder-\ndecoder with feature equalizations. In ECCV, 2020.\n[27] Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao\nYang. Rethinking image inpainting via a mutual encoder-\ndecoder with feature equalizations. In ECCV, pages 725‚Äì\n741, 2020.\n[28] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In International\nConference on Computer Vision, pages 3730‚Äì3738, 2015.\n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[30] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi,\nand Mehran Ebrahimi. Edgeconnect: Generative image\ninpainting with adversarial edge learning. arXiv preprint\narXiv:1901.00212, 2019.\n[31] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In CVPR, pages 2337‚Äì2346, 2019.\n[32] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros. Context encoders: Feature\nlearning by inpainting. In CVPR, pages 2536‚Äì2544, 2016.\n[33] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Gen-\nerating diverse structure for image inpainting with hierarchi-\ncal vq-vae. arXiv preprint arXiv:2103.10022, 2021.\n[34] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 2019.\n[35] Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H Li,\nShan Liu, and Ge Li. StructureÔ¨Çow: Image inpainting via\nstructure-aware appearance Ô¨Çow. In ICCV, pages 181‚Äì190,\n2019.\n[36] Ashish Shrivastava, Tomas PÔ¨Åster, Oncel Tuzel, Joshua\nSusskind, Wenda Wang, and Russell Webb. Learning\nfrom simulated and unsupervised images through adversarial\ntraining. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2107‚Äì2116, 2017.\n[37] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan\nLiu. Mpnet: Masked and permuted pre-training for language\nunderstanding. arXiv preprint arXiv:2004.09297, 2020.\n[38] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021.\n[39] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020.\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998‚Äì6008, 2017.\n[41] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao.\nHigh-Ô¨Ådelity pluralistic image completion with transform-\ners. arXiv preprint arXiv:2103.14031, 2021.\n[42] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE Transactions on Image Process-\ning, 13(4):600‚Äì612, 2004.\n[43] Rongliang Wu and Shijian Lu. Leed: Label-free expres-\nsion editing via disentanglement. In European Conference\non Computer Vision, pages 781‚Äì798. Springer, 2020.\n[44] Rongliang Wu, Gongjie Zhang, Shijian Lu, and Tao Chen.\nCascade ef-gan: Progressive facial expression editing with\nlocal focuses. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 5021‚Äì\n5030, 2020.\n[45] Shunxin Xu, Dong Liu, and Zhiwei Xiong. E2I: Generative\ninpainting from edge to image. IEEE Trans. Circuit Syst.\nVideo Technol., 2020.\n[46] Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and\nShiguang Shan. Shift-Net: Image inpainting via deep fea-\nture rearrangement. In ECCV, pages 1‚Äì17, 2018.\n[47] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized\nautoregressive pretraining for language understanding.arXiv\npreprint arXiv:1906.08237, 2019.\n[48] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Generative image inpainting with con-\ntextual attention. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5505‚Äì5514,\n2018.\n[49] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S. Huang. Generative image inpainting with contex-\ntual attention. In CVPR, pages 5505‚Äì5514, 2018.\n[50] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Free-form image inpainting with gated\nconvolution. In ICCV, pages 4471‚Äì4480, 2019.\n[51] Fangneng Zhan and Shijian Lu. Esir: End-to-end scene text\nrecognition via iterative image rectiÔ¨Åcation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2059‚Äì2068, 2019.\n[52] Fangneng Zhan, Shijian Lu, Changgong Zhang, Feiying Ma,\nand Xuansong Xie. Adversarial image composition with\nauxiliary illumination. In Proceedings of the Asian Confer-\nence on Computer Vision, 2020.\n[53] Fangneng Zhan, Chuhui Xue, and Shijian Lu. Ga-dan:\nGeometry-aware domain adaptation network for scene text\ndetection and recognition. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 9105‚Äì9115,\n2019.\n[54] Fangneng Zhan, Yingchen Yu, Kaiwen Cui, Gongjie Zhang,\nShijian Lu, Jianxiong Pan, Changgong Zhang, Feiying Ma,\nXuansong Xie, and Chunyan Miao. Unbalanced feature\ntransport for exemplar-based image translation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2021.\n[55] Fangneng Zhan, Yingchen Yu, Rongliang Wu, Changgong\nZhang, Shijian Lu, Ling Shao, Feiying Ma, and Xuansong\nXie. Gmlight: Lighting estimation via geometric distribution\napproximation. arXiv preprint arXiv:2102.10244, 2021.\n[56] Fangneng Zhan and Changgong Zhang. Spatial-aware gan\nfor unsupervised person re-identiÔ¨Åcation. Proceedings of the\nInternational Conference on Pattern Recognition, 2020.\n[57] Fangneng Zhan, Changgong Zhang, Yingchen Yu, Yuan\nChang, Shijian Lu, Feiying Ma, and Xuansong Xie. Emlight:\nLighting estimation via spherical distribution approximation.\nAAAI, 2020.\n[58] Fangneng Zhan, Hongyuan Zhu, and Shijian Lu. Spa-\ntial fusion gan for image synthesis. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 3653‚Äì3662, 2019.\n[59] Gongjie Zhang, Kaiwen Cui, Tzu-Yi Hung, and Shijian Lu.\nDefect-gan: High-Ô¨Ådelity defect synthesis for automated\ndefect inspection. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision , pages\n2524‚Äì2534, 2021.\n[60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, pages 586‚Äì595,\n2018.\n[61] Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen\nZuo, Haibo Chen, Wei Xing, and Dongming Lu. Uctgan:\nDiverse image inpainting based on unsupervised cross-space\ntranslation. In CVPR, pages 5741‚Äì5750, 2020.\n[62] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image comple-\ntion via co-modulated generative adversarial networks. InIn-\nternational Conference on Learning Representations (ICLR),\n2021.\n[63] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards\ndeeper understanding of variational autoencoding models.\narXiv preprint arXiv:1702.08658, 2017.\n[64] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic\nimage completion. In CVPR, pages 1438‚Äì1447, 2019.\n[65] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 2017.\n[66] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020."
}