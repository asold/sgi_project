{
  "title": "Evaluating Language Model Finetuning Techniques for Low-resource Languages",
  "url": "https://openalex.org/W3099781644",
  "year": 2019,
  "authors": [
    {
      "id": null,
      "name": "Cruz, Jan Christian Blaise Bombio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2517104762",
      "name": "Charibeth Cheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2593408211",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2757749329",
    "https://openalex.org/W1976095474",
    "https://openalex.org/W2940237665",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W2963968475",
    "https://openalex.org/W2954835819",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3091905774",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2815470153",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2616864913",
    "https://openalex.org/W2951514583",
    "https://openalex.org/W2963416784",
    "https://openalex.org/W2963887123"
  ],
  "abstract": "The use of the internet as a fast medium of spreading fake news reinforces the need for computational tools that combat it. Techniques that train fake news classifiers exist, but they all assume an abundance of resources including large labeled datasets and expert-curated corpora, which low-resource languages may not have. In this work, we make two main contributions: First, we alleviate resource scarcity by constructing the first expertly-curated benchmark dataset for fake news detection in Filipino, which we call \"Fake News Filipino.\" Second, we benchmark Transfer Learning (TL) techniques and show that they can be used to train robust fake news classifiers from little data, achieving 91% accuracy on our fake news dataset, reducing the error by 14% compared to established few-shot baselines. Furthermore, lifting ideas from multitask learning, we show that augmenting transformer-based transfer techniques with auxiliary language modeling losses improves their performance by adapting to writing style. Using this, we improve TL performance by 4-6%, achieving an accuracy of 96% on our best model. Lastly, we show that our method generalizes well to different types of news articles, including political news, entertainment news, and opinion articles.",
  "full_text": null,
  "topic": "Fake news",
  "concepts": [
    {
      "name": "Fake news",
      "score": 0.7131574153900146
    },
    {
      "name": "Transfer of learning",
      "score": 0.6864476203918457
    },
    {
      "name": "Multi-task learning",
      "score": 0.5676302313804626
    },
    {
      "name": "Computer science",
      "score": 0.527249276638031
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2596489191055298
    },
    {
      "name": "Internet privacy",
      "score": 0.2024579644203186
    },
    {
      "name": "Task (project management)",
      "score": 0.14085015654563904
    },
    {
      "name": "Engineering",
      "score": 0.07268363237380981
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}