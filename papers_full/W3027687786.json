{
  "title": "Exploring Transformers for Large-Scale Speech Recognition",
  "url": "https://openalex.org/W3027687786",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2101202197",
      "name": "Lu Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2359352996",
      "name": "LIU Changliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133789678",
      "name": "Li Jinyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365187764",
      "name": "Gong, Yifan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W2992632249",
    "https://openalex.org/W2911291251",
    "https://openalex.org/W2795138957",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3008191852",
    "https://openalex.org/W2977728428",
    "https://openalex.org/W2976556660",
    "https://openalex.org/W2005708641",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964084166",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2906625520",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2981661615",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W2802023636",
    "https://openalex.org/W2396384435",
    "https://openalex.org/W2998814410"
  ],
  "abstract": "While recurrent neural networks still largely define state-of-the-art speech recognition systems, the Transformer network has been proven to be a competitive alternative, especially in the offline condition. Most studies with Transformers have been constrained in a relatively small scale setting, and some forms of data argumentation approaches are usually applied to combat the data sparsity issue. In this paper, we aim at understanding the behaviors of Transformers in the large-scale speech recognition setting, where we have used around 65,000 hours of training data. We investigated various aspects on scaling up Transformers, including model initialization, warmup training as well as different Layer Normalization strategies. In the streaming condition, we compared the widely used attention mask based future context lookahead approach to the Transformer-XL network. From our experiments, we show that Transformers can achieve around 6% relative word error rate (WER) reduction compared to the BLSTM baseline in the offline fashion, while in the streaming fashion, Transformer-XL is comparable to LC-BLSTM with 800 millisecond latency constraint.",
  "full_text": "Exploring Transformers for Large-Scale Speech Recognition\nLiang Lu, Changliang Liu, Jinyu Li and Yifan Gong\nMicrosoft\n{liang.lu, chanliu, jinyli, yifan.gong}@microsoft.com\nAbstract\nWhile recurrent neural networks still largely deﬁne state-of-the-\nart speech recognition systems, the Transformer network has\nbeen proven to be a competitive alternative, especially in the\nofﬂine condition. Most studies with Transformers have been\nconstrained in a relatively small scale setting, and some forms\nof data argumentation approaches are usually applied to combat\nthe data sparsity issue. In this paper, we aim at understanding\nthe behaviors of Transformers in the large-scale speech recogni-\ntion setting, where we have used around 65,000 hours of train-\ning data. We investigated various aspects on scaling up Trans-\nformers, including model initialization, warmup training as well\nas different Layer Normalization strategies. In the streaming\ncondition, we compared the widely used attention mask based\nfuture context lookahead approach to the Transformer-XL net-\nwork. From our experiments, we show that Transformers can\nachieve around 6% relative word error rate (WER) reduction\ncompared to the BLSTM baseline in the ofﬂine fashion, while\nin the streaming fashion, Transformer-XL is comparable to LC-\nBLSTM with 800 millisecond latency constraint.\nIndex Terms: Speech recognition, Transformer, Transformer-\nXL\n1. Introduction\nState-of-the-art speech recognition systems usually rely on the\nrecurrent neural networks (RNNs) with long short-term mem-\nory (LSTM) [1] units as their backbones. Recently, there have\nbeen an increasing interests in exploring Transformers [2] for\nspeech recognition, inspired by their success in nature language\nprocessing such as machine translation [2] and language mod-\neling [3]. Compared to RNNs, Transformers do not process the\ninput signal in a sequential fashion. Instead, they reply on the\nself-attention mechanism to capture the temporal correlations\namong the sequential signals, which circumvents the expensive\nback-propagation through time (BPTT) [4] algorithm used to\ntrain RNNs. Therefore, Transformers can capture long-term\ncorrelations with much less computation complexity. Another\nadvantage is that it is simpler to parallelize the computations in\nTransformers, which can reduce the time to train deeper models\non a large scale dataset.\nFor speech recognition, Transformers have achieved com-\npetitive recognition accuracy compared to RNN-based coun-\nterparts within both end-to-end [5, 6, 7, 8, 9, 10] and hy-\nbrid [11, 12] frameworks. However, the superior results are\nusually achieved in the ofﬂine condition, while in the stream-\ning fashion, Transformers have shown signiﬁcant degradation\nin terms of accuracy from previous results [5, 12], even in a\ncondition of a large latency constraint. For example, [12] re-\nports around 25% - 40% accuracy degradation compared with\nthe ofﬂine baseline with approximately 2.5 seconds of latency\nwhen using a hybrid model, while in [13], the streaming Trans-\nformer with around 1.2 second latency lagged behind its ofﬂine\nbaseline by approximately 15% - 23% from experiments with\nan sequence-to-sequence (S2S) model. In addition, the stud-\nies of Transformer model mostly focus on a relatively small\ndataset from a single domain such as Librispeech dataset, and\nsome forms of data argumentation approaches are applied to\ntackle the data sparsity issue. It is relatively less well under-\nstood where Transformer stands in a large-scale setting.\nIn this paper, we aim at understanding the behaviors of\nTransformers for large scale speech recognition, and perform-\ning a fair comparison to LSTMs in both ofﬂine and streaming\nconditions. We use around 65,000 hours of training data, and\napply data-parallelization across up to 64 GPUs. We present\nour approaches to address the technical challenges in cross-\nmachine multi-GPU training of deep Transformers, including\nmodel initialization and warmup training. In the ofﬂine con-\ndition, we show that our Transformers can outperform the bi-\ndirectional LSTM (BLSTM) [14] baseline with approximately\n6% relative with similar number of model parameters, while\nin the streaming fashion, we introduce the Transformer-XL [3]\nbased steaming model, which is computationally tractable for\ninference. Our results show that Transformer-XL is on par with\nlatency-controlled BLSTM (LC-BLSTM) [15] with the same\nlatency constraint.\n2. Related Work\nThere have been a few studies on Transformers for end-to-end\nspeech recognition, particularly in the context of the S2S model\nwith attention [5, 6, 7, 16], as well as Transformer Transduc-\ners [8, 17]. In [5, 10], the authors compared RNNs with trans-\nformers for various speech recognition tasks, and obtained com-\npetitive or even better results with Transformers. Within the\nhybrid framework, the authors in [11, 12] also reported very\nstrong results on the Librispeech benchmark with Transformers.\nThe time restricted self-attention investigated in [18] is closely\nrelated to the Transformer-XL network in this work as they\nboth perform chunk-wise training. The key difference is that\nin Transfromer-XL, we also take the hidden states from previ-\nous chunk as feature to capture the long-term information from\nthe past. In terms of streamable Transformers, attention mask\nbased approach to control the left and right context is mostly\nstudied in the previous works, such as in the S2S model [13],\nTransformer-Transducer [17] and the hybrid model [12]. From\nthe reported results, streaming Transformers still lag behind\ntheir ofﬂine counterparts remarkably in terms of the recognition\naccuracy, especially in the low-latency scenarios.\n3. Transformer Model\n3.1. Self-attention with Multiple Heads\nThe attention mechanism is well understood [19]. In [2], the\nauthors introduced a machine translation model based on self-\nattention, which can be represented in the form of the dot-\narXiv:2005.09684v2  [eess.AS]  11 Aug 2020\nproduct attention as:\nAttention(Q,K,V ) =Softmax\n(QKT\n√dk\n)\nV, (1)\nwhere Q,K,V are referred to the query, key and value accord-\ning to [2]. In self-attention, Q,K and V are from the source\nsequence, while in the conventional attention model [19], Qis\nfrom the decoder hidden state, and K is from the encoder hid-\nden state. In Eq (1), dk is the dimension of the model.\nAnother key idea in [2] is the multi-head attention mecha-\nnism, which performs multiple attention operations in parallel\nusing different model parameters. The outputs from different\nattention heads are then concatenated and projected before be-\ning fed into the next layer, which can be shown as:\nMultiHead(Q,K,V ) = [H1,H2,··· ,HN]WO (2)\nwhere Hi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\nwhere N is the number of attention heads, and WQ\ni ,WK\ni ,WV\ni\nare parameters for thei-th attention head, andWO is the projec-\ntion matrix to reduce the dimension of the concatenated vector.\n3.2. Depth-scale Initialization and Warmup Training\nTraining neural networks in a large scale setting requires paral-\nlelization across multiple GPUs. In our experiments, we did not\nexperience convergence issues when training a shallow Trans-\nformer with random initialization on a single machine with 4-8\nGPUs. However, we observed poor convergence or even diver-\ngence when performed parallelization across multiple machines\nwith 32-64 GPUs with a randomly initialized model. To ad-\ndress this problem, we performed warmup training on a single\nmachine with 4 GPUs until the model has seen ∼640 hours of\ntraining data, before switching to cross-machine parallelization.\nThis approach worked well for Transformers with up to 12 lay-\ners in our experiments, however, when we increased the depth\nof the Transformers to 24 layers and beyond, the model did not\nconverge even during the warmup stage. This is primarily due\nto the gradient explosion and vanishing problem in deep Trans-\nformers as investigated in [20], which shows the gradient norms\nof lower layers were signiﬁcantly smaller that those of deeper\nlayers. This indicates that the supervision signal becomes much\nweaker after back-propagation from the top layer to lower lay-\ners. To address this problem, the authors proposed the depth-\nscale model initialization, which normalized the ℓ2-norm of the\nweight parameters by their corresponding depth index. For the\nmodel parameters from the l-th layer, they can be initialized as:\nW ∈Rdi×do ∼U\n(\n−γ√\nl\n, γ√\nl\n)\n,γ =\n√\n6\ndi + do\n, (3)\nwhere di and do are input and output dimension respectively.\nWith depth-scale initialization, we did not observe conver-\ngence issues during warmup training, and were able to train a\nTransformer-XL with 100 layers and over 200 million parame-\nters in the warmup stage. However, we did not train this model\nuntil convergence due to the high computational cost.\n3.3. Pre-Norm vs. Post-Norm\nLayer normalization (LN) [21] has been a de facto in Trans-\nformers for a smooth model convergence during training. In the\nseminal Transformer paper [2], LN is applied after the element-\nwise residual addition, i.e.,\nxl+1 = LN(xl + F(xl,θl)), (4)\nprevious chunk current chunk\n t    t+1     t+2  t+3  t-2  t-1···\na)\nb)\nFigure 1: a) Context expansion for a streaming Transformer\nbased on attention masks. In this example, the attention context\nwindow is [−2,1] for each layer. With 3-layers, the context size\nincreases to [−6,3]. The gray nodes denote the past frames and\nhidden states, and the blue nodes represent the future frames\nand hidden states. b) A Transformer-XL with chunk size as 4.\nThe context size does not changes as the model goes deeper.\nwhere θl denotes all the model parameters in the l-th layer,\nand F(·) denotes any type of non-linear operation. This nor-\nmalization approach, referred to as Post-Norm, is observed to\nresults in poor convergence in machine translation tasks for\ndeeper Transformers [22]. In our experiments, the Post-Norm\napproach worked well for 12-layer Transformers trained with\ndata-parallelization on a single machine with 4-8 GPUs. How-\never, when we run cross-machine parallelization with 32 or 64\nGPUs, we observed divergence during training with Post-Norm.\nAs investigated in [22], Post-Norm poses a high risk of van-\nishing or exploding gradients problem, which is more appar-\nent in the multi-GPU training with a very large minibatch size.\nWe tackled this problem by switching to Pre-Norm as proposed\nin [22], where the LN is applied as\nxl+1 = xl + F(LN(xl),θl). (5)\nThe Pre-Norm approach was also applied in [12]. With this ap-\nproach, we were able to train a 48-layer Transformer-XL model\nwith 64 GPUs without any convergence issues.\n3.4. Streaming Transformer\nASR systems are usually deployed in the streaming fashion\nwhich produces real-time speech transcription with certain la-\ntency constraint. To enable Transformers for streaming ASR,\nwe have to limit the feature context visible to the model in both\ntraining and inference stage. A typical approach is based on\nthe future context lookahead with an attention mask as shown\nin Figure 1.a), where each node can only access limited number\nof activation states from both the future and the past timesteps.\nHowever, this approach suffers from the issue of context ex-\npansion, as the context size increases as the model goes deeper.\nConsequently each layer can only access features from a very\nsmall context window in order to ﬁt into the latency constraint.\nThis hinders the model to unlock the information from the data,\nand possibly explain the inferior recognition accuracy observed\nin this work as well as in [12, 13] when employing this ap-\nproach. Another disadvantage of this method is the high com-\nputational cost, as all the nodes in the context window need to\nbe updated for every decoding step. This results in intractable\ncomputational complexity for real-time inference.\nIn this work, we explore another streaming approach base\non the Transformer-XL network [3]. Unlike the mask-based\nlookahead approach, Transformer-XL consumes the input se-\nquence in a chunk-wise fashion, which means the information\nis fully visible to all the nodes when they reside in the same\nchunk. In order to capture the information from the long-term\nhistory, Transformer-XL takes the hidden states from the previ-\nous chunk as additional feature, while does not pass the gradi-\nents through those hidden states to avoid the memory explosion.\nMore precisely, this operation can be represented as:\n˜Hl\nc−1 = SG(Hl\nc−1), (6)\n˜Kl\nc = Concat[ ˜Hl\nc−1; Kl\nc],˜Vl\nc = Concat[ ˜Hl\nc−1; Vl\nc ], (7)\nHl\nc = MultiHead(Ql\nc, ˜Kl\nc,˜Vl\nc ), (8)\nwhere cdenote the index of the chunk, SG refers to the stop-\ngradient operation, and Concat indicates the concatenation op-\neration of the two matrices along the time axis. The interpreta-\ntion is that in the self-attention layer, the queries are from the\ncurrent chunk, while the keys and values are from the current\nchunk as well as the previous chunk.\nTransformer-XL enjoys two key advantages compared with\nthe mask-based lookahead approach for streaming ASR. Firstly,\nwe can emit the labels of all the acoustic frames within the\nchunk without refreshing the hidden states of self-attention, and\ntherefore signiﬁcantly cut down the computational cost for in-\nference. Secondly, Transformer-XL enables us to train much\ndeeper models. Recall that the self-attention operation has the\nmemory complexity of O(T2), where T is length of the input\nsequence. In Transformer-XL, the length of the chunk can be\nmuch shorter, which saves memory for deeper or larger models.\nAs aforementioned, we were able to train a Transformer-XL\nwith 100 layers without blowing up the GPU memory in the\nwarmup stage. However, Transformer-XL has the disadvantage\nof longer training time due to the chunk-wise processing.\n4. Experiments and Results\n4.1. Experimental setup\nIn our experiments, the models were trained with around 65,000\nhours of anonymized and transcribed Microsoft data, recorded\nin various conditions. We ﬁt the Transformer models into the\nhybrid architecture, where we used the hidden Markov models\n(HMMs) for sequence modeling. The number of tied triphone\nstates is 9404 in our experiments, and the input feature is 80-\ndimension log Mel ﬁlter banks sampled every 20 milliseconds\n(ms) with the frame skipping strategy [23]. The language model\nis a 5-gram with around 100 million (M) ngrams. We used a\ndev set with around 7459 utterances, which are spontaneous\nconversational speech recorded in both close-talk and far-ﬁled\nconditions mostly from non-native speakers. The speaking style\nand acoustic condition of the dev set are not well represented\nby our training data, so that we can avoid over-training by tun-\ning on this dataset. Our eval set is from the far-ﬁeld scenario\nmostly by native speakers, which has around 13,368 utterances,\nand it matches our training data well.\nWe compare Transformers with a bidirectional LSTM\n(BLSTM) based acoustic model in the ofﬂine condition, and\nTable 1: Results of the Transformers with convolution layers in\nthe ofﬂine mode. The number of heads is 4, and the number\nof layers is 12. IC stands for interleaved 1D convolution. All\nmodels have around 50 million (M) model parameters including\nconvolution layers.\nModel IC Size(M) Encode layer dk dev\n\u0017 51.5 Linear 620 34.7\nTransformer \u0013 50.0 Linear 512 20.2\n\u0013 51.5 VGG 512 19.6\n\u0017 52.0 VGG 620 19.4\nTable 2: Results of the 12-layer Transformer model with dif-\nferent number of attention heads. VGG net was used as the\nencoding layer for all the Transformers. N denotes the number\nof attention heads, and dk is the model dimension as in Eq(1).\nModel IC Size (M) N d k dev\n\u0013 50.5 4 512 19.6\n\u0013 50.5 8 512 19.7\n\u0013 50.5 16 512 18.8\nTransformer \u0017 52.0 4 620 19.4\n\u0017 53.5 8 624 18.4\n\u0017 53.5 16 624 18.6\nBLSTM – 55.0 – – 19.5\nan LC-BLSTM [15] in the streaming condition. Both BLSTM\nand LC-BLSTM in our experiments have 6 hidden layers, and\neach layer has 600 hidden units and cells for each direction. For\nLC-BLSTM, the chunk size is set to be 40. For Transformers, to\nlimit the scope of our investigate, we set the dropout ratio to be\n0.1, and the number of hidden units in the feedforward layer to\nbe 2048 in all our experiments. All the models were trained with\nthe cross-entropy criterion. We used the Adam optimizer [24]\nto train all our Transformer and LSTM models. For 12-layer\nTransformers, we set the mini-batch size as 16,000 frames for\neach GPU, and it took around 4 days for the models to con-\nverge with 32 Tesla V100 GPUs, which corresponds to train-\ning the models for 5-6 epochs. The training time for deeper\nTransformers is longer. For Transformers with 24 layers and\nbeyond, we halved the mini-batch size due to the memory con-\nstraint, and tuned the learning rate scheduler accordingly. This\nincreased the training time by roughly 50%. Transformer-XL\nis much more expensive due to the chunk-wise training fashion.\nIts training time is approximately 50% longer compared with a\nvanilla Transformer with the same model structure. For a 48-\nlayer Transformer-XL, the training time is around 3 weeks with\ndata parallelization across 64 Tesla V100 GPUs.\n4.2. Convolution Layers and Attention Heads\nThe self-attention operation cannot maintain the monotonic-\nity of input sequence, which is particularly harmful for time-\nsynchronous acoustic model such as the hybrid model studies\nin this paper. The positional encoding approach in [2] is shown\nto be less effective to the speech recognition problem [12, 25],\nwhile convolutional layers are proven to be more powerful to\nencode the positional information. In Table 1, we compare the\ntwo schemes of using convolution layers in Transformers in the\nofﬂine condition, namely, the interleaved 1D convolution with\nself-attention from our previous study [25], and using the VGG\nnet [26] as the input encoding layer. The kernel size for the\n1D convolution is 3, while the VGG net has 4 layers of 2D\nconvolutions with 3x3 ﬁlters. When the VGG encoder was ap-\nplied, we used features of frame rate as 10ms, and employed\na max-pooling layer to down-sample the features by a factor\nof 2. As shown in Table 1, the Transformer model performed\npoorly without any convolution layers. Both interleaved con-\nTable 3: Results of streaming Transformer models. The number\nof layers is 12.\nModel IC Size (M) N d k Context dev\n\u0013 50.5 16 512 [- ∞, ∞] 18.8\n\u0013 50.5 16 512 [- ∞, 16] 20.6\n\u0013 50.5 16 512 [- ∞, 28] 20.7\n\u0013 50.5 16 512 [- ∞, 40] 20.0\nTransformer \u0017 53.5 8 624 [- ∞, ∞] 18.4\n\u0017 53.5 8 624 [- ∞, 4] 23.0\n\u0017 53.5 8 624 [- ∞, 16] 21.1\n\u0017 53.5 8 624 [- ∞, 28] 21.8\n\u0017 53.5 8 624 [- ∞, 40] 19.8\nTransformer-XL \u0013 50.5 16 512 [-40, 40] 20.4\n\u0017 53.5 8 624 [-40, 40] 21.0\nBLSTM – 55.0 – – [−∞, ∞] 19.5\nLC-BLSTM – 55.0 – – [-1, 40] 20.2\nvolution and VGG net can signiﬁcant improve the accuracy of\nTransformer models. In addition, when applying a VGG as the\nencoder, it is more beneﬁcial to remove the interleaved convolu-\ntions but increase the model dimension of self-attention layers\nif the model size is constrained to be the same.\nTable 2 shows the results of the Transformers with differ-\nent numbers of attention heads. With the interleaved convolu-\ntion, the Transformer with 16 attention heads achieved the low-\nest WER, while for the vanilla Transformer, 8 attention heads\nare sufﬁcient. We did not further increase the number of the at-\ntention heads in our experiments due to the memory constraint.\nCompared to the BLSTM with around 55 million model param-\neters, the Transformer can achieve around 6% relative WER re-\nduction with a similar model size.\n4.3. Results of Streaming Transformers\nThe previous experiments focused on the ofﬂine scenario. In\nthis section, we evaluate the accuracy of Transformers in the\nsteaming condition. We investigated the mask-based lookahead\napproach, as well as the Transformer-XL network discussed in\nsection 3.4. The results are shown in Table 3 with various la-\ntency constraints. For Transformers with attention masks, the\ncontext window in Table 3 refers to the overall accumulated la-\ntency from both convolution and self-attention. For instance,\n[−∞,40] corresponds to looking ahead 3 frames for each self-\nattention layer in a 12-layer Transformer without interleaved\nconvolution, with an additional 4 frames latency from the VGG\nencoder. Since our Transformers operated at the 20 ms frame\nrate, 40 frames correspond to 800 ms latency. For the attention\nmask based Transformer, we did not limit the left context, so it\nis marked as −∞, while the context window as[−∞,∞] refers\nto the ofﬂine system. For Transformer-XL, we set the chunk\nsize as 40, and since the model takes the hidden states from\nthe previous chunk as feature, we denote the context size as\n[−40,40]. Note that there are not overlaps in the chunks during\nboth training and inference, and it emits 40 outputs each time\nduring inference. For LC-BLSTM, the chunk size is also 40,\nand because it takes the previous hidden state as the representa-\ntion of history, arguably, we denote the context size as[−1,40].\nThe chunks in LC-BLSTM are overlapped by 20 frames, so it\nonly emits 20 outputs each time during inference.\nFrom the results in Table 3, we have the following obser-\nvations. First, without the interleaved convolution, the stream-\ning Transformers based on attention masks degrade the recog-\nnition accuracy by 18% - 25% compared to the ofﬂine base-\nline model in the low latency condition. With the interleaved\nconvolution, the accuracy loss is much smaller. This may be\ndue to that the interleaved convolution layers can compensate\nthe reordering effect of self-attention operations, and main-\ntain the monotonicity of the input sequence [25]. When the\nTable 4: Results of deeper Transformer models. Ldenotes the\nmodel depth.\nModel IC Size(M) L Context dev eval\nBLSTM – 55.0 6 [- ∞, ∞] 19.5 12.7\nLC-BLSTM – 55.0 6 [-1, 40] 20.2 12.9\n\u0017 53.5 12 [- ∞, ∞] 18.4 11.9\nTransformer \u0017 97.0 12 [- ∞, ∞] 18.3 –\n\u0017 101.7 24 [- ∞, ∞] 17.8 11.7\n\u0017 53.5 12 [-40, 40] 21.0 12.9\nTranformer-XL \u0017 101.7 24 [-40, 40] 19.1 12.4\n\u0013 50.5 12 [-40, 40] 20.4 12.9\n\u0013 95.5 24 [-40, 40] 19.3 12.6\n\u0013 185.7 48 [-40, 40] 18.5 12.2\nlatency constraint is less tight, the effect of interleaved con-\nvolution layers is diminishing. Second, Transformer-XL still\nlags behind the vanilla Transformer with the context window\nof [−∞,40]. This is not surprising as in Transformer-XL, the\nprevious chunk is only an approximation of the full history. Im-\nproving the strength of Transformer-XL to capture the infor-\nmation from the long-term history in a computationally fea-\nsible manner, such as the Compressive Transformer [27], is\nworth further investigation. Third, the gap between the stream-\ning Transformer (or Transformer-XL) and its ofﬂine model is\nlarger than that between LC-BLSTM and BLSTM. The ofﬂine\nTransformer outperforms BLSTM by a considerable margin,\nwhile Transformer-XL is only comparable with LC-BLSTM in\nterms of WERs. Though the Transformer with attention mask\nas [−∞,40] can outperform LC-BLSTM, it is not computation-\nally feasible during inference. This observation may align well\nwith the argument that Transformers are more powerful to cap-\nture the long-term correlations in sequential signals. In a sce-\nnario with limited feature context, however, Transformers are\nhindered to release their modeling power.\n4.4. Deeper Transformers\nIn Table 4, we show results from deeper Transformers, and re-\nsults from the eval set. We observe similar trend on the eval\nset. We also investigated the tradeoff between increasing the di-\nmension of hidden state in self-attention layers and increasing\nthe depth the model. For the ofﬂine 12-layer Transformer, we\nincreased dk to 960, which resulted in a model with 97 million\nparameters. However, we only achieved very marginal improve-\nment on the dev set. The gain from increasing the model depth\nto 24 layers is more considerable on the dev set, but the gain\non the eval set is still small. It is possible that the model is\noverﬁtted, and increasing the dropout ratio may result in more\ngains. As for Transformer-XL, we can obtain gains in terms of\naccuracy by increasing the depth up to 48 layers. However, the\ngains are not as large as we have expected, and regularizing the\ndeep Transformers may result in further improvements.\n5. Conclusions\nIn this paper, we investigated Transformers for large-scale\nspeech recognition. We presented our approaches to address\nthe issues of scaling up Transformers with cross-machine multi-\nGPU training, and shown that our approach can train a Trans-\nformer with 48 layers and beyond. For streaming Trans-\nformers, we discussed the drawbacks of the attention mask\nbased approach, and studied an alternative method based on\nTransformer-XL, which is much more computationally efﬁcient\nfor inference. We demonstrated that the ofﬂine Transformer\ncan outperform BLSTM by 6% relative with similar number\nof model parameters, while the streamable Transformer-XL is\ncomparable with LC-BLSTM.\n6. References\n[1] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n5998–6008.\n[3] Z. Dai, Z. Yang, Y . Yang, W. W. Cohen, J. Carbonell, Q. V . Le,\nand R. Salakhutdinov, “Transformer-xl: Language modeling with\nlonger-term dependency,” inProc. ICLR, 2019.\n[4] P. J. Werbos et al., “Backpropagation through time: what it does\nand how to do it,” in Proceedings of the IEEE , vol. 78, no. 10,\n1990, pp. 1550–1560.\n[5] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang et al., “A\ncomparative study on transformer vs rnn in speech applications,”\nin arXiv preprint arXiv:1909.06317, 2019.\n[6] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,” in Proc.\nICASSP. IEEE, 2018, pp. 5884–5888.\n[7] M. Sperber, J. Niehues, G. Neubig, S. St ¨uker, and A. Waibel,\n“Self-attentional acoustic models,” in arXiv preprint\narXiv:1803.09519, 018.\n[8] Z. Tian, J. Yi, J. Tao, Y . Bai, and Z. Wen, “Self-attention\ntransducers for end-to-end speech recognition,” in arXiv preprint\narXiv:1909.13037, 2019.\n[9] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-attention networks\nfor connectionist temporal classiﬁcation in speech recognition,” in\nProc. ICASSP. IEEE, 2019, pp. 7115–7119.\n[10] A. Zeyer, P. Bahar, K. Irie, R. Schl ¨uter, and H. Ney, “A compari-\nson of transformer and LSTM encoder decoder models for ASR,”\nin IEEE Automatic Speech Recognition and Understanding Work-\nshop, Sentosa, Singapore, 2019.\n[11] K. J. Han, R. Prieto, K. Wu, and T. Ma, “State-of-the-art speech\nrecognition using multi-stream self-attention with dilated 1d con-\nvolutions,” inarXiv preprint arXiv:1910.00716, 2019.\n[12] Y . Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., “Transformer-\nbased acoustic modeling for hybrid speech recognition,” in Proc.\nICASSP. IEEE, 2020.\n[13] N. Moritz, T. Hori, and J. L. Roux, “Streaming automatic\nspeech recognition with the transformer model,” arXiv preprint\narXiv:2001.02674, 2020.\n[14] A. Graves, N. Jaitly, and A.-r. Mohamed, “Hybrid speech recog-\nnition with deep bidirectional LSTM,” in Proc. ASRU. IEEE,\n2013, pp. 273–278.\n[15] Y . Zhang, G. Chen, D. Yu, K. Yao, S. Khudanpur, and J. Glass,\n“Highway long short-term memory RNNs for distant speech\nrecognition,” inProc. ICASSP, 2016.\n[16] C. Wang, Y . Wu, Y . Du, J. Li, S. Liu, L. Lu, S. Ren, G. Ye, S. Zhao,\nand M. Zhou, “Semantic mask for transformer based end-to-end\nspeech recognition,”arXiv preprint arXiv:1912.03010, 2019.\n[17] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo,\nand S. Kumar, “Transformer transducer: A streamable speech\nrecognition model with transformer encoders and RNN-T loss,”\nin ICASSP. IEEE, 2020, pp. 7829–7833.\n[18] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur,\n“A time-restricted self-attention layer for ASR,” inProc. ICASSP.\nIEEE, 2018, pp. 5874–5878.\n[19] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation\nby jointly learning to align and translate,” inProc. ICLR, 2015.\n[20] B. Zhang, I. Titov, and R. Sennrich, “Improving deep transformer\nwith depth-scaled initialization and merged attention,” inEMNLP,\n2019.\n[21] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” in\nNIPS Deep Learning Symposim, 2016.\n[22] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n“Learning deep transformer models for machine translation,” in\nACL, 2019.\n[23] Y . Miao, J. Li, Y . Wang, S. Zhang, and Y . Gong, “Simplifying\nlong short-term memory acoustic models for fast training and de-\ncoding,” inProc. ICASSP, 2016.\n[24] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” inarXiv preprint arXiv:1412.6980, 2014.\n[25] L. Lu, “A transformer with interleaved self-attention and\nconvolution for hybrid acoustic models,” arXiv preprint\narXiv:1910.10352, 2019.\n[26] K. Simonyan and A. Zisserman, “Very deep convolutional net-\nworks for large-scale image recognition,”Proc. ICLR, 2015.\n[27] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\n“Compressive transformers for long-range sequence modelling,”\narXiv preprint arXiv:1911.05507, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7197585105895996
    },
    {
      "name": "Transformer",
      "score": 0.6602209806442261
    },
    {
      "name": "Initialization",
      "score": 0.544276773929596
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5346863269805908
    },
    {
      "name": "Speech recognition",
      "score": 0.4862426817417145
    },
    {
      "name": "Artificial neural network",
      "score": 0.469806969165802
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3869136571884155
    },
    {
      "name": "Voltage",
      "score": 0.1977117657661438
    },
    {
      "name": "Engineering",
      "score": 0.13002899289131165
    },
    {
      "name": "Electrical engineering",
      "score": 0.10501736402511597
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}