{
  "title": "Probing Multi-modal Machine Translation with Pre-trained Language Model",
  "url": "https://openalex.org/W3176512019",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2746775331",
      "name": "Kong Yawei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047607534",
      "name": "Kai Fan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3107826490",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2980216782",
    "https://openalex.org/W2987734933",
    "https://openalex.org/W2963331233",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2950886580",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2889545026",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2903343986",
    "https://openalex.org/W2963909453",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3034773362",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2509282593",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2593341061",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2963898017",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W2889903020",
    "https://openalex.org/W2964192290",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2888070626",
    "https://openalex.org/W2902031175",
    "https://openalex.org/W2950207430",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034871396",
    "https://openalex.org/W2997908677",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W1905882502"
  ],
  "abstract": "Multi-modal machine translation (MMT) aimed at using images to help disambiguate the target during translation and improving robustness, but some recent works showed that the contribution of visual features is either negligible or incremental.In this paper, we show that incorporating pre-trained (vision) language model (VLP) on the source side can improve the multi-modal translation quality significantly.Motivated by BERT, VLP aims to learn better cross-modal representations that improve target sequence generation.We simply adapt BERT to a cross-modal domain for the vision language pre-training, and the downstream multi-modal machine translation can substantially benefit from the pre-training.We also introduce an attention based modality loss to promote the image-text alignment in the latent semantic space.Ablation study verifies that it is effective in further improving the translation quality.Our experiments on the widely used Multi-30K dataset show increased BLEU score up to 6.2 points compared with the text-only model, achieving the state-of-the-art results with a large margin in the semi-unconstrained scenario and indicating a possible direction to rejuvenate the multi-modal machine translation.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3689–3699\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3689\nProbing Multi-modal Machine Translation with Pre-trained Language\nModel\nYawei Kong, Kai Fan\nAlibaba DAMO Academy\n{yawei.kyw,k.fan}@alibaba-inc.com\nAbstract\nMulti-modal machine translation (MMT)\naimed at using images to help disambiguate\nthe target during translation and improving\nrobustness, but some recent works showed\nthat the contribution of visual features is either\nnegligible or incremental. In this paper, we\nshow that incorporating pre-trained (vision)\nlanguage model (VLP) on the source side can\nimprove the multi-modal translation quality\nsigniﬁcantly. Motivated by BERT, VLP aims\nto learn better cross-modal representations\nthat improve target sequence generation. We\nsimply adapt BERT to a cross-modal domain\nfor the vision language pre-training, and the\ndownstream multi-modal machine translation\ncan substantially beneﬁt from the pre-training.\nWe also introduce an attention based modality\nloss to promote the image-text alignment in\nthe latent semantic space. Ablation study\nveriﬁes that it is effective in further improving\nthe translation quality. Our experiments on\nthe widely used Multi-30K dataset show\nincreased BLEU score up to 6.2 points\ncompared with the text-only model, achieving\nthe state-of-the-art results with a large margin\nin the semi-unconstrained scenario and\nindicating a possible direction to rejuvenate\nthe multi-modal machine translation.\n1 Introduction\nJoint models of language and vision have achieved\nremarkable results, such as in image caption\n(Karpathy and Fei-Fei, 2015) and visual question\nanswering (Antol et al., 2015). Multi-modal ma-\nchine translation (MMT) was ﬁrst introduced as\na shared competition task at the 2016 Conference\non Machine Translation (WMT16) (Specia et al.,\n2016) as an interdisciplinary study to incorporate a\nvisual element into the multilingual translation task.\nThis task continued for three years until WMT18,\nand the ﬁndings presented by the organizers sug-\ngest that the text-only systems remain competi-\ntive, and that the contribution of visual modality\nData used img src tgt examples\nMulti-30K ✓ ✓ ✓ most works\n+external data\n✓ ✓ (Gr¨onroos et al., 2018)\n✓ ✓ (Helcl et al., 2018)\n✓ ✓ (Yin et al., 2020), ours\nTable 1: Different unconstrained scenarios in MMT.\nis not entirely convincing (Specia et al., 2016; El-\nliott et al., 2017; Barrault et al., 2018). Moreover,\nthe experiments in (Elliott, 2018) ﬁnd that a pub-\nlicly available MMT system produces great trans-\nlations with random, incongruent images, further\nundermining the importance of visual features. The\nempirical results have so far raised doubts about\nwhether the visual features can really help MMT,\nand there is evidence pointing to a negative answer.\nWe hypothesize that one reason is the data scale\nof the benchmarking Multi-30K (Elliott et al.,\n2016) – it is likely insufﬁcient for a deep model to\nlearn better cross-modality or cross-lingual repre-\nsentations. However, the pre-training techniques\nsuch as BERT (Devlin et al., 2019) or cross-lingual\nlanguage model (XLM) (Conneau and Lample,\n2019) can capture rich representations of the inputs\nfrom languages and be applied to various down-\nstream tasks by providing context-aware embed-\ndings, leading to remarkable improvements even on\nsmall datasets. Furthermore, the pre-trained vision\nand language model LXMERT (Tan and Bansal,\n2019) pioneers the cross-modality pre-training and\nsets an inﬂuential record in vision and language\nreasoning tasks. These advances lead us to believe\nthat a better cross-modality representation can help\nmulti-modal machine translation as well.\nIn this work, we discuss the unconstrained sce-\nnario of MMT, but unlike previous setting in most\nWMT 2018 submissions (Gr ¨onroos et al., 2018;\nHelcl et al., 2018), we did not include any external\ndata of the parallel source and target textual corpus.\nSince we want to incorporate a pre-trained (vision)\n3690\nlanguage model as an encoder backbone into the\ntransformer architecture (Vaswani et al., 2017) for\nneural machine translation, our used external data\nonly contains the images and the source texts.\nParticularly, our model is initialized with the\nwidely used BERT, and pre-trained on large scale\nimage-text dataset (about six million pairs), expect-\ning to learn a better cross-modality representation\nbetween the image and the source language. Next,\nwe stack a regular transformer decoder on top of\nthe pre-trained (vision) language model and pro-\nceed to the task of MMT. Meanwhile, we design\nanother modality loss in addition to the traditional\nsequential cross entropy loss. The modality loss is\nto minimize the difference between source-target\ncross attention and image-target cross attention. In-\ntuitively, minimizing this loss function can promote\nthe modality alignment among the three possible\npairwise conﬁgurations in the latent semantic space.\nIn other words, differences among (source, target),\n(source, image), and (target, image) alignments can\nbe reduced. Our experimental section also presents\na detailed analysis of how each factor separately\ncontributes to the overall gains.\nIn summary, this paper makes the following con-\ntributions. (1) We propose to integrate a pre-trained\nvision language model into multi-modal machine\ntranslation, aiming at learning and utilizing better\ncross-modality representations. (2) We address the\nimportance of the modality loss which can further\nboost the model performance. (3) We conduct ex-\ntensive experiments on the benchmark Multi-30K\ndataset, and our results outperform strong baselines\nby a large margin.\n2 Related Works\nConstrained ScenarioMost works like (Calixto\net al., 2017; Zhou et al., 2018; Ive et al., 2019; Yao\nand Wan, 2020) in MMT prefer to use Multi-30K\ndataset alone. For example, a standard paradigm\nof MMT explored by many previous works is to si-\nmultaneously learn the vision language interaction\nand the target language generation (Calixto et al.,\n2017; Zhou et al., 2018; Ive et al., 2019; Yang et al.,\n2020). However, training on such a limited dataset,\nthe beneﬁts provided by visual features of these\nmethods are quantitatively marginal w.r.t. auto-\nmatic evaluation metrics BLEU and METEOR.\nUnconstrained Scenario In the submissions of\nWMT 2018 (Gr ¨onroos et al., 2018; Helcl et al.,\n2018) as shown in Table 1, either images / source\ntexts or the source / target texts parallel dataset (or\nback-translation) are added to improve the model\nperformance. However, as they discovered, train-\ning with the large scale parallel textual corpus will\nshift the machine translation model towards the\npure textual domain, further weakening the effect\nof visual features. The additional target data will\nalso make the fair comparison difﬁcult. A special\nunconstrained scenario by (Su et al., 2019b) lever-\nages large monolingual language data to pre-train\nan unsupervised translation model. It considers\nthe cross representation of the source-target in an\nunsupervised manner, but the image domain is still\nisolated without proper training.\nWe will discuss another unconstrained scenario\nthat only allows to use additional images and source\ntexts. Zhu et al. (2019) investigates the represen-\ntation from pre-trained BERT by feeding it into\nall layers of a text-only translation model. This\nwork, to a large extent, encourages us to explore\nhow the (vision) language pre-trained model can\nbeneﬁt the MMT. However, we found that a direct\narchitecture of feeding cross-modality representa-\ntions (from LXMERT) to multi-modal translation\nmodel does not work well.\nTo our best knowledge, Yin et al. (2020) cur-\nrently achieves the state-of-the-art on Multi-30K. It\nemployed a common encoder-decoder framework\nby hard-encoding a multi-modal graph to guide the\nlearning of the image-text cross attention, where\nthe graph structure is annotated by a pre-trained\nvisual grounding model (Yang et al., 2019). The\nexternal data is not explicitly used in this work, but\nthe pre-trained visual grounding model uses BERT\nas part of its backbone. Instead of relying on a\npre-deﬁned graph to prevent the attention between\nthe word and visual feature without connection,\nwe obtain a soft cross attention from large-scale\nvision-language data pre-training. It is also worth\nmentioning that we make the BERT based visual\ngrounding and multi-modal machine translation\ninto an end-to-end trainable architecture.\n3 Our Method\n3.1 Initial Trial\nThe overall architecture of our proposed ap-\nproach is based on the commonly used transformer\n(Vaswani et al., 2017), which is the basic unit of\nmost pre-trained (vision) language model. Our ini-\ntial experiment is to adopt pre-trained (vision) lan-\nguage model as the encoder. The baseline is to train\n3691\n[CLS] a white and [MASK] dog… its mouth [SEP] [MASK] tree… [SEP] … \nTransformer Encoder\nFFNEmbedding Layer\nTransformer Decoder\n[SOS] Ein weiß-braunerHund fängteinenrotenBall in seinemMaul .\nEmbedding Layer\nEin weiß-braunerHund fängteinenrotenBall in seinemMaul .\nFaster R-CNN\nFFN … ……\nbrown dogBinary Classification\nNMT Training withCross Entropy Loss and Modality Loss\nMasked Token Prediction\nVision Language Pre-training\nMulti-modalInput (text withpaired or random image)\nFFN\nx t v y\nSoftmaxLayer SoftmaxLayer\nFigure 1: The overall architecture of our proposed multi-modal NMT with pre-trained vision language model.\nNote that the [MASK] tokens and random images are merely applied during vision language pre-training.\nEncoder visual Test2016 EnDe Test2016 EnFr\nfeature BLEU Meteor BLEU Meteor\nTransformer - 38.3 56.6 59.6 74.6\nBERT - 39.1 57.1 61.0 75.3\nLXMERT ✓ 37.4 55.2 57.7 68.6\nTable 2: BERT/LXMERT are frozen.\na transformer NMT from scratch. The ﬁrst compet-\nitive system is simply BERT, and the second one\nis the pre-trained vision language model LXMERT.\nLXMERT claimed that the initialization with pre-\ntrained BERT will harm the performance of their\ndownstream tasks. Table 2 shows the preliminary\nresults indicating that the pre-trained LXMERT\nas the encoder performs surprisingly worse than\ntext-only BERT. Does the table suggest that the\nvisual features are equally marginalized in MMT\nequipped with pre-trained language model? How-\never, since BERT encoder can bring more improve-\nments, we can abandon LXMERT’s conclusion and\nreturn to the paradigm with BERT initialization.\n3.2 Vision Language Pre-training (VLP)\nIve et al. (2019) ﬁnds that integrating both object-\nbased embedding features and image features into\nthe NMT model results better performance in hu-\nman evaluation on comprehensibility. We therefore\nfavor the object-semantics alignment whose inter-\naction is composed of text embedding, object tag\nembedding and object image features.\nWe visualize the training rationale of the VLP\nin the red dashed box of Figure 1. Suppose that\nan image and its description x are presented as\nthe input, where x represents a sequence of nto-\nkens (x1,...,x n), i.e., the sentence of the source\nlanguage in our following NMT system. We ﬁrst\nprocess the image with the efﬁcient object detection\nmodel Faster-RCNN (Ren et al., 2015) to detect\nthe object regions, box positions, object tags and\nattribute tags. Particularly, two sets of features are\nextracted. One is the image visual features of all\ndetected objects, denoted as v. The other is the\nclassiﬁcation tags of the corresponding objects, de-\nnoted as t, as textual features.\nSince the backbone of our transformer encoder\nis pre-trained BERT, the input text x and object\ntags t are both language tokens that can be easily\nconcatenated. However, there is a dimensionality\nmismatch between the BERT embedding layer and\nthe visual features. For dimension reduction, a\nfully-connected layer is necessary with input v,\nand its task is to learn cross modality transferring.\nThe ﬁnal input fed into the multiple transformer\nlayers of BERT can be written as follows.\nCat [Emb (Cat[x,t]) ,FFN(v)] (1)\nWe now face two similar tasks as BERT.\nTask 1: Masked LMSame as the standard BERT,\nour training objective employs the masking token\nprediction, where 15% of the input text tokens are\nrandomly selected and replaced with the special\ntoken [MASK]. Then, only the masked token will\nbe predicted.\nTask 2: Paired Image PredictionAnalogous to\nthe standard BERT, we pre-train the binarized\npaired image prediction task that mimics predicting\nthe next sentence, where the training data can be\ntrivially generated for each batch. Speciﬁcally, for\na given text input, we choose its paired image or\n3692\n[CLS] a white and brown dogis catching a red ball in its mouth . [SEP] dogtree building grass ball [SEP] \nModel Decoder\nEin weiß-braunerHund\nModel Encoder\n [SOS] Ein weiß-braune\nSoftmaxLayerKv, VvKx, Vx Qy\nSource-Target Cross-Attention Image-Target Cross-Attention\nCosine Similarity Optimized with NMTWell trained in VLP\nFigure 2: The visualization of modality loss for an input sentence-image pair. It exempliﬁes the computational\nﬂow of the modality loss w.r.t. the last layer of the decoder when decoding “Hund” in German.\na random image each with probability 50%. The\noutput vector of the ﬁrst special token [CLS] is\nused as the aggregate multi-modal representation\nfor this classiﬁcation task.\n3.3 Multi-modal NMT\nOnce the vision language model has been fully\ntrained on a large paired image-text dataset, it is rea-\nsonable to assume that the obtained cross-modality\nrepresentations between the source text and the im-\nage are more powerful than those training on the\nlimited Multi-30K. The (key, value) pairs of both\nthe textual and visual features participate in the dot-\nproduct attention of the transformer decoder. But\nthere is another dimensionality mismatch between\nthe BERT output and the decoder hidden size. To\nclose this gap, we append an additional fully con-\nnected layer after the last layer of BERT. In this\nsection, we also introduce a novel modality loss\nthat is potential to beneﬁt the multi-modal repre-\nsentation learning while but incurs only a few extra\nmodel parameters.\nModality Loss To train a multi-modal machine\ntranslation task, i.e., generating the tokens in the\ntarget language y = (y1,...,y m), a common objec-\ntive is the sequential cross entropy loss LXENT =\n−∑m\nj=1 log p(yj|y<jx,v), which is the sum of\nthe negative log-likelihoods of the auto-regressive\ntext generation task. Our proposed auxiliary modal-\nity loss can be intuitively depicted as Figure 2.\nConcretely, when generating the j-th token in\nthe target, the output textual and visual (key, value)\npairs from the encoder are separately used to com-\npute the cross-lingual and cross-modality attention\nwith the query vector of thel-th layer in the decoder.\nThe derived vectors can be written as follows.\nh(l)\nx,j = Softmax\n(\nKxq(l)\nj /\n√\nd\n)\nVx (2)\nwhere dis the hidden size of the model decoder,\nand similar attention holds for visual features\nh(l)\nv,j = Softmax\n(\nKvq(l)\nj /\n√\nd\n)\nVv. Thus, the\nmodality loss can be represented as\nL(l)\nM =\nm∑\nj=1\n(1 −cos(h(l)\nx,j,h(l)\nv,j)) (3)\nwhere the cosine similarity is deﬁned as\ncos(a,b) = a⊤b\n∥a∥,∥b∥. Consequently, the overall\ntraining objective is a weighted combination of two\nloss functions.\nL= LXENT +\nL∑\nl=1\nλ(l)L(l)\nM (4)\nwhere Lis total number of transformer layers in\ndecoder. Empirically, we found that only using\nthe modality loss of the last layer is sufﬁcient to\nimprove the model performance. Intuitively, the\nquery vector will be directly fed into the softmax\nlayer for decoding the target tokens, making the last\nlayer more informative than other remote layers.\nA common method of choosing the weighting\nparameter λis to run cross validation on the held-\nout development data. For the task at hand, this\nis a time-consuming process. We instead discard\nthe layer-wise λ(l) in Eq. (4) and introduce a self-\ntuning module with respect to the generation pro-\ncess of every single target token. Mathematically,\nthe reﬁned modality loss can be formulated as,\n˜L(l)\nM =\nm∑\nj=1\nλ(l)\nj (1 −cos(h(l)\nx,j,h(l)\nv,j)). (5)\nwhere the token level λj is learnable and derived\nfrom a feedforward neural network.\nλ(l)\nj = Sigmoid(w⊤\ny Emb(yj)+w⊤\nx h(l)\nx,j+w⊤\nv h(l)\nv,j)\nwhere wy,wx,wv are three d-dimensional vectors\nshared cross different decoder layers and required\n3693\nAlgorithm 1Training Pipeline\nRequire: Image, source text paired dataDVLP; Im-\nage, source/target text triple data DMMT.\n1: Initialize the transformer encoder of NMT with\npre-trained BERT.\n2: Pre-train the transformer encoder on DVLP\nwith masked language model task and pair im-\nage prediction task.\n3: Extract the image, source text paired from\nDMMT.\n4: Continue the vision language pre-training on\nabove extracted data.\n5: Freeze the transformer encoder, and optimize\nother parameters on DMMT with cross entropy\nloss and modality loss until convergence.\n6: Optimize all model parameters on DMMT with\ncross entropy loss and modality loss until con-\nvergence.\nto jointly optimize with the model parameters, but\nuseless during inference. We expect the model to\ndynamically adjust the weight parameters of the to-\nkens with different importance. For example, there\nis a good chance that the content words also appear\nas detected objects by the Faster R-CNN. If the\nterm w⊤\nv h(l)\nv,j can positively increase its scale for\nsuch words, the corresponding λjs become larger\nand therefore reinforce the maximization of the\ncosine similarity. In contrast, although it happens\nthat a mapping exists between the source and target\nfunctional words, the image-target cross attention\nmay become weak, making it less necessary to pro-\nmote the similarity. The term w⊤\nx h(l)\nx,j is intended\nto model the importance of the source contribution.\nOur results, however, show that in the current ex-\nperiment setup its effect is not quite as signiﬁcant.\n3.4 Two-Stage Training\nWhen BERT is applied to the downstream tasks,\nthe task-speciﬁc module parameters are usually\nplugged into BERT and all the trainable parameters\nare simultaneously ﬁne-tuned (Devlin et al., 2019).\nHowever, we found this is not the optimal strategy\nof training our downstream task – multi-modal ma-\nchine translation. The large number of untrained\nparameters in the transformer decoder almost ac-\ncount for half of the model size. We conjecture that\nthe encoder parameters have already reached a ﬂat\nplateau after the pre-training, and it is difﬁcult to\nset the consistent optimization hyper-parameters\n(such as learning rate, decay rate or warm-up steps)\nfor both the encoder and decoder.\nTherefore, we adopt a two-stage training sched-\nule. In the ﬁrst stage, the encoder parameters are\nfrozen and only the decoder parameters are opti-\nmized w.r.t. the cross entropy and modality loss.\nIn the second stage, all model parameters become\ntrainable and are updated concurrently. This step\nsimulates the regular BERT ﬁne-tuning procedure,\nand its convergence is expected to lead to a better\nperformance. To this end, we have elaborated the\nkey ideas of our proposed method and summarize\nthe training pipeline of the entire model training\nprocess in Algorithm 1.\n4 Experiments\nIn this section, we describe the datasets, the de-\ntailed settings as well as the compared baselines.\n4.1 Datasets and Settings\nMulti-30K We conduct experiments on the Multi-\n30K dataset (Elliott et al., 2016), where each image\nis paired with one English(En) description and hu-\nman translations of German(De) and French(Fr).\nIt has 29,000 instances for training and 1,014 in-\nstances for development. Besides, we evaluate our\nmodel on various testing sets, including the Multi-\n30K 2016 test set, the WMT17 test set and the am-\nbiguous MSCOCO test set, which contain 1,000,\n1,000 and 461 instances, respectively.\nExternal DataWe use about 6 million image and\nEnglish text paired data for our vision language\nmodel pre-training, including MSCOCO (Lin et al.,\n2014), Im2text (Ordonez et al., 2011), visual7w\n(Zhu et al., 2016), VQA 2.0 (Goyal et al., 2017),\nConceptual captions (Sharma et al., 2018), GQA\n(Hudson and Manning, 2019). We ﬁrst process the\nimage with a popular off-the-shelf Faster-RCNN\ntoolkit1 (Ren et al., 2015; Anderson et al., 2018;\nWu et al., 2019). The Faster R-CNN (Ren et al.,\n2015) network is pre-trained on the MSCOCO\ndataset and ﬁne-tuned on the Visual Genome (Kr-\nishna et al., 2017) dataset to detect salient visual\nobjects, where the number of visual objects ranges\nfrom 10 to 100 with the highest prediction prob-\nability and 2048 is the dimension of the ﬂattened\nlast pooling layer in the ResNet (He et al., 2016)\nbackbone. Then, we obtain the position-sensitive\n1https://github.com/airsplay/\npy-bottom-up-attention\n3694\nModel\nEn⇒De\nNotes on external resourcesTest2016 Test2017 MSCOCO\nBLEU Meteor BLEU Meteor BLEU Meteor\nOur text-only 38.3 56.6 30.3 51.0 28.6 47.7 Our own implemented transformer\nDoubly-Att 36.5 55.0 - - - -\nFusion-conv 37.0 57.0 29.8 51.2 25.1 46.0\nTrg-mul 37.8 57.7 ∗ 30.7 52.2 ∗ 26.4 47.4\nV AG 31.6 52.2 - - - - Constrained methods\nVMMT 37.7 56.0 30.1 49.9 25.5 44.8 ResNet features only\nDNetwork 38.0 55.6 - - - -\nMultimodal-Att 38.7 55.7 - - - -\nSemi-unconstrained methods\nVMMT 38.4 58.3 - - - - few Back-translation data\nMultimodal-Att 39.5 56.9 - - - - few Back-translation data\nGraph-Fusion 39.8∗ 57.6 32.2 ∗ 51.9 28.7 ∗ 47.6∗ BERT(en), visual grounding tool\nOur Model 42.7 60.7 35.5 54.9 32.8 52.2 BERT(en), images-en\nWMT 2018 unconstrained methods\nMeMAD 45.1 - 40.8 - 36.9 - images-en, OpenSub en-de/fr\nCUNI 42.7 59.1 - - - - images-en, Bookshop en-de/fr,\nBack-translation\nTable 3: Experimental results on the En⇒De MMT. Our results are highlighted in bold.∗indicates previous SOTA.\nB will be short for BLEU and M will be short for Metoer in other tables.\nModel\nEn⇒Fr\nTest2016 Test2017\nB M B M\nOur Text-only 59.6 74.6 52.7 69.1\nDoubly-Att 59.9 74.1 52.4 68.1\nFusion-conv 53.5 70.4 51.6 68.6\nTrg-mul 54.7 71.3 52.7 69.5 ∗\nV AG 53.8 70.3 - -\nDNetwork 59.8 74.4 - -\nSemi-unconstrained methods\nGraph-Fusion 60.9∗ 74.9∗ 53.9∗ 69.3\nOur Model 65.8 79.1 58.2 73.5\nWMT 2018 unconstrained methods\nMeMAD 68.3 - 62.5 -\nCUNI 62.8 77.0 - -\nTable 4: Experimental results on the En⇒Fr MMT.\nvisual features by concatenating the region features\nand the corresponding positions.\nFor the English text, we follow the same pre-\nprocessing as the open-source BERT toolkit2. The\nBERT base model with hidden size 768 is utilized\nas initialization. Note that unlike (Gr¨onroos et al.,\n2018; Helcl et al., 2018), we never include any ex-\n2https://github.com/huggingface/\ntransformers\nternal data related to the target languages for both\nvision language pre-training and machine transla-\ntion training. For notation simplicity and differ-\nentiating their setting, we deﬁne our scenario as\nsemi-constrained.\n4.2 Baselines\nWe mainly compare with the following repre-\nsentative and competitive frameworks. The con-\nstrained methods include Doubly-Att (Calixto\net al., 2017), Fusion-conv / Trg-mul(Caglayan\net al., 2017), V AG(Zhou et al., 2018), VMMT\n(Calixto et al., 2019) and Multimodal-Att (Yao\nand Wan, 2020). MeMAD and CUNI (Gr¨onroos\net al., 2018; Helcl et al., 2018) mainly discussed\nthe unconstrained scenario of MMT. In addition,\nVMMT and Multimodal-Att attempted to adding\nin-domain back-translation data. We prefer to in-\nclude them into semi-unconstrained methods as\nwell. Graph-Fusion (Yin et al., 2020) uses BERT\nbased visual ground model to hard-code a uniﬁed\nmulti-modal graph and performs semantic interac-\ntions by graph fusion layers, achieving the current\nstate-of-the-art performance.\n3695\n4.3 Main Results\nIn Table 3 and 4, we report the main experimen-\ntal results of our proposed method with previous\nresearch works. All reported numbers of our ap-\nproach are evaluated on the best performed model\nfor the validation set. Note that when optimizing\nthe parameters, we only use the modality loss cal-\nculated from the last layer with learnable token\nlevel λ(6)\nj . In other words, the reported numbers\nare obtained by minimizing LXENT + ˜L(6)\nM . In the\nablation study, we demonstrate this simpliﬁcation\nnot only reduces the computational complexity, but\nalso achieves better result than our initial proposal.\nBoth tables show that our multi-modal trans-\nlation outperforms the existing models and base-\nlines, especially the recent state-of-the-art algo-\nrithm Graph Fusion, which also leveraged the pre-\ntrained BERT based visual grounding model from\nlarge scale paired image-text data. However, it\nonly hard-coded the inferred multi-modal graph by\nvisual grounding to construct the mask matrix of\ncross modality attention in the transformer encoder.\nOne advantage of our work is that we directly build\nour NMT model on top of the pre-trained vision\nlanguage BERT, making the most of pre-trained\ncross modality attention. Another advantage is that\nour end-to-end trainable model can spontaneously\navoid the error accumulation.\nSince our multi-modal translation model is im-\nplemented based on the text-only transformer, we\nalso report the text-only results with our own im-\nplemented transformer for a fair comparison. Our\ntext-only transformer is a surprisingly strong base-\nline and very competitive with most cited works.\nFor English to German translation task, our text-\nonly baseline almost beats all previous works on\nthe ambiguous MSCOCO test set, and is only in-\nferior to two systems on Multi-30K test sets with\nless than 2 BLEU score difference. For English\nto French translation task, only the Graph Fusion\nalgorithm signiﬁcantly outperforms our text-only\ntransformer. In contrast, on the three test sets of\nEnglish to German, our ﬁnal multi-modal transla-\ntion model can on average achieve approximately\n+4.6 BLEU and +4.2 METEOR over the text-only\nbaseline. On the two test sets of English to French,\nthe averaged gains of our model are about +5.85\nand +4.45 on BLEU and METEOR.\nModel Test2016 Test2017 MSCOCO\nB M B M B M\nText-only En⇒De Model\nTransformer 38.3 56.6 30.3 51.0 28.6 47.7\nBERT-NMT 39.4 56.6 29.7 48.6 27.9 46.2\nBERT-enc 1st 39.1 57.1 31.8 51.1 29.5 47.9\nBERT-enc 2nd 40.0 58.7 34.7 53.8 30.6 51.2\nMulti-modal En⇒De Model\nOur Model 42.7 60.7 35.5 54.9 32.852.2\n- LM 41.8 60.0 34.7 54.6 32.3 52.3\nTable 5: Comparison with variants of text-only models.\n1st and 2nd means the 1st and 2nd stage of training.\n4.4 Probing Textual Language Model\nOur implemented text-only transformer only uses\nthe source-target parallel corpus extracted from\nMulti-30K, which overlooks the power of the pre-\ntraining on the source side. Because our multi-\nmodal encoder has been fully pre-trained, we sys-\ntematically compare it with another two text-only\nbaselines. The ﬁrst baseline virtually has the same\narchitecture as multi-modal framework but without\nvision language pre-training, denoted as BERT-enc.\nThe second one is BERT-NMT(Zhu et al., 2019)\nby incorporating the output of BERT into the atten-\ntion module of the transformer. We directly run the\nexperiments with their released codebase3. With-\nout image data, all text-only models only optimize\nthe cross entropy loss, so we also present the result\nof our model without the modality loss.\nAs shown in Table 5, the BERT-NMT is some-\ntimes even worse than the regular transformer.\nWe hypothesize that the existence of too many\nuntrained parameters in the encoder makes the\nmodel difﬁcult to optimize on the limited Multi-\n30K dataset. When we directly use the pre-trained\nBERT as the encoder and train the model with two-\nstage schedule, we observe a consistent improve-\nment on the metrics over the regular transformer,\ni.e., +1.1 BLEU at 1st-stage and +2.7 BLEU at 2nd-\nstage. Thus, we argue that with the proper 2-stage\ntraining strategy, the pre-trained BERT can account\nfor one half of the overall gains in our ﬁnal model.\n4.5 Ablation Study\nTo validate the contribution of each component in\nour approach, we conduct a series of incremental\nexperiments to observe the model performances in\ndifferent scenarios, summarized in Table 6.\n3https://github.com/bert-nmt/bert-nmt\n3696\nMulti-modal Model Test2016 Test2017 MSCOCO Average\nB M B M B M ∆B ∆M\nEnd2End 38.7 58.3 31.6 53.1 29.1 50.0 - -\n1st-Stage 40.0 57.5 32.4 51.5 30.8 49.8 +1.27 -0.87\n+ 2nd-Stage 41.8 60.0 34.7 54.6 32.3 52.3 +3.13 +1.83\n+ Last Layer Modality loss ˜L(6)\nM 42.7 60.7 35.5 54.9 32.8 52.2 +3.90 +2.13\nor + All Layers Modality loss∑6\ni=1 ˜L(6)\nM 41.7 59.9 34.8 54.7 32.0 51.7 +3.07 +1.63\nor + Last Layer Modality loss L(6)\nM (λ(6) = 0.4) 42.1 59.9 34.9 54.6 31.8 51.3 +3.17 +1.46\nTable 6: Ablation study of MMT training on the En⇒De dataset after VLP. Different modality losses are exclusive.\nTwo-Stage TrainingIn previous analysis, we’ve\nseen how the 2-stage training can beneﬁt the text-\nonly model. In Table 6, we present the metrics\nof different multi-modal models. The end-to-end\ntraining, similar to the traditional ﬁne-tuning strat-\negy in (Devlin et al., 2019), optimizes all model\nparameters of the downstream task once the VLP is\nﬁnished. We found it leads even worse result than\noptimizing the decoder alone (i.e., 1st-stage train-\ning) on the metric BLEU. In addition, the result\nafter the 2nd-stage ﬁne-tuning produces signiﬁcant\nperformance increase. We also plot the learning\ncurve of BLEU on development dataset in Figure 3.\nThe apparent gap between two curves conﬁrms the\ncontribution of 2-stage training.\nModality Loss Note that the results in the ﬁrst\nthree lines of Table 6 are achieved by optimizing\nthe cross entropy loss alone. In this study, we will\nverify the effectiveness of the modality loss in 3\ndifferent setups. We found only optimizing the\nmodality loss of the last layer can achieve the best\nperformance. As we discussed before, the query\nvector of the last layer will directly and maximally\ninﬂuence the generation of the target token, while\nthe vectors from remote layers seem not impor-\ntant. We can use the statistics of the learnable λ\nto avoid the time-consuming cross-validation. For\nexample, we set λ as the approximate mean 0.4\nin the original modality loss Eq. (4). Although a\nslightly performance drop appears, we can get rid\nof 3 trainable vectors.\n4.6 Case Studies\nActually, the translation performance of the MMT\nwith vision language model only exceeds about 2\nBLEU scores compared with the NMT with BERT\nlanguage model. So we cannot guarantee that all\nsentences in the testsets can be better translated\nby MMT with VLP. We only exemplify two cases\nwith better translation quality for MMT with VLP,\nepochs\nStart2ndstagetrainingfromthebestmodelondevsetof1ststagetraining\n1st\n2nd\nFigure 3: Learning curve of two-stage training w.r.t.\nBLEU on development set.\nto indicate the potential beneﬁts.\nIn the ﬁrst case, German words “personen” and\n“leute” both mean “people”, where leute is a general\nexpression and can’t be in singular, and “personen”\nis a formal expression when stating how many peo-\nple. In object detection model, the tag “person”\npossibly enhances the NMT model to produce a\nsimilar German word “personen”. In addition, per-\nson is also a German word.\nThe second case comes from the Ambiguous\nCOCO testset. The NMT with BERT language\nmodel cannot miss the translation of the word pizza.\nThe detected object “pizza” may also emphasize\nthe word and help the MMT, though MMT trans-\nlated the rectangular pizza to stein-pizza (stone-\npizza).\n4.7 Discussion\nThe major limitation of our method is that the train-\ning pipeline cannot easily generalize to other source\nlanguages other than English, because the image-\ntext paired data is unavailable in other languages.\nLiu et al. (2020) presented a sequence-to-sequence\ndenoising auto-encoder pre-trained on large-scale\nmonolingual corpora in many languages, and suc-\ncessfully applied to multi-lingual translation. Hope-\nfully, we can explore the similar unsupervised\ncross-lingual or zero-shot transfer learning tech-\nniques, which help adapt the multi-lingual BERT\n3697\nsrc four people relaxing on a grassy hill overlooking a rocky valley .\nref vier personen entspannen auf einem grasbewachsenen h¨ugel mit ausblick auf ein felsiges tal .\nbrt vier leute entspannen sich auf einem grasbewachsenen h¨ugel mit blick auf ein steiniges tal .\nvlp vier personen entspannen sich auf einem grasbewachsenen h¨ugel mit blick auf ein steiniges tal .\nsrc a girl with arms crossed leaning on counter over a rectangular pizza ,\nby a wall calendar and containers .\nref ein m¨adchen mit gekreuzten armen st¨utzt sich auf eine theke mit einer rechteckigen pizza ,\nneben einem wandkalender und beh¨altern .\nbrt ein m¨adchen mit verschr¨ankten armen lehnt sich mit ¨uberkreuzten armen an einer theke\nneben einer wand und kartons .\nvlp ein m¨adchen mit gekreuzten armen lehnt sich ¨uber eine theke neben einer wand ,\nauf der sich ein stein-pizza und beh¨alter steht .\nTable 7: Case Studies\nFigure 4: The image for the second case\nto a vision multi-lingual model. We will leave this\ndirection as our future work. The main purpose\nis not to design a better vision language model\nfor other downstream tasks such as VQA. Note\nthat the contemporary works including ViLBERT\n(Lu et al., 2019) and Oscar (Li et al., 2020) may\nshare the same idea to utilize pre-trained BERT.\nOur idea is mostly enlighten by (Ive et al., 2019).\nAnother different approach is VL-BERT (Su et al.,\n2019a), which required to mask sub-regions of the\nimage and introduced masked ROI classiﬁcation\nloss, rather than mimicking the NSP loss in tradi-\ntional BERT.\n5 Conclusion\nIn this paper, we found the vision language pre-\ntraining on the source side can signiﬁcantly im-\nprove the multi-modal machine translation, even\nwithout additional target corpus. Although the\nmodel architecture is as simple as the regular\nencoder-decoder transformer, our proposed train-\ning pipeline can help the MMT system outperform\nprevious works by a large margin on the Multi-30K\ndataset. The success of the source-image cross-\nmodality representation learning encourages us to\ndesign the modality loss that aims at transferring\nthe pre-trained representations to the target-image\npair. The quantitative analysis also demonstrates\nits effectiveness.\nImpact Statement\nVision language pre-training has achieved great\nsuccess in many NLP tasks. We believe it would\ndeﬁnitely beneﬁt the multi-modal translation and\nexpect this work can indicate a new unconstrained\nscenario.\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nCVPR.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425–2433.\nLo¨ıc Barrault, Fethi Bougares, Lucia Specia, Chiraag\nLala, Desmond Elliott, and Stella Frank. 2018. Find-\nings of the third shared task on multimodal machine\ntranslation. In Proceedings of the Third Conference\non Machine Translation: Shared Task Papers, pages\n304–323.\n3698\nOzan Caglayan, Walid Aransa, Adrien Bardet, Mer-\ncedes Garc ´ıa-Mart´ınez, Fethi Bougares, Lo ¨ıc Bar-\nrault, Marc Masana, Luis Herranz, and Joost van de\nWeijer. 2017. LIUM-CVC submissions for WMT17\nmultimodal translation task. In Proceedings of the\nSecond Conference on Machine Translation, pages\n432–439, Copenhagen, Denmark. Association for\nComputational Linguistics.\nIacer Calixto, Qun Liu, and Nick Campbell. 2017.\nDoubly-attentive decoder for multi-modal neural\nmachine translation. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1913–\n1924.\nIacer Calixto, Miguel Rios, and Wilker Aziz. 2019. La-\ntent variable model for multi-modal translation. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6392–\n6405.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems, pages\n7059–7069.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nDesmond Elliott. 2018. Adversarial evaluation of mul-\ntimodal machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2974–2978.\nDesmond Elliott, Stella Frank, Lo ¨ıc Barrault, Fethi\nBougares, and Lucia Specia. 2017. Findings of the\nsecond shared task on multimodal machine transla-\ntion and multilingual image description. In Proceed-\nings of the Second Conference on Machine Transla-\ntion, pages 215–233.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30k: Multilingual english-\ngerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language, pages 70–\n74.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nv in vqa matter: Elevating the role of image under-\nstanding in visual question answering. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6904–6913.\nStig-Arne Gr ¨onroos, Benoit Huet, Mikko Kurimo,\nJorma Laaksonen, Bernard Merialdo, Phu Pham,\nMats Sj ¨oberg, Umut Sulubacak, J ¨org Tiedemann,\nRaphael Troncy, et al. 2018. The memad submission\nto the wmt18 multimodal translation task. arXiv\npreprint arXiv:1808.10802.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nJindˇrich Helcl, Jind ˇrich Libovick `y, and Du ˇsan Variˇs.\n2018. Cuni system for the wmt18 multimodal trans-\nlation task. arXiv preprint arXiv:1811.04697.\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6700–6709.\nJulia Ive, Pranava Swaroop Madhyastha, and Lucia\nSpecia. 2019. Distilling translations with visual\nawareness. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6525–6538.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n3128–3137.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123(1):32–\n73.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, et al. 2020. Oscar:\nObject-semantics aligned pre-training for vision-\nlanguage tasks. arXiv preprint arXiv:2004.06165.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. arXiv\npreprint arXiv:2001.08210.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. arXiv preprint arXiv:1908.02265.\nVicente Ordonez, Girish Kulkarni, and Tamara L Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In Advances in neural infor-\nmation processing systems, pages 1143–1151.\n3699\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time ob-\nject detection with region proposal networks. In\nAdvances in neural information processing systems,\npages 91–99.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2556–2565.\nLucia Specia, Stella Frank, Khalil Sima’an, and\nDesmond Elliott. 2016. A shared task on multi-\nmodal machine translation and crosslingual image\ndescription. In Proceedings of the First Conference\non Machine Translation: Volume 2, Shared Task Pa-\npers, pages 543–553.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019a. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\narXiv preprint arXiv:1908.08530.\nYuanhang Su, Kai Fan, Nguyen Bach, C-C Jay Kuo,\nand Fei Huang. 2019b. Unsupervised multi-modal\nneural machine translation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 10482–10491.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5103–5114.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYuxin Wu, Alexander Kirillov, Francisco Massa,\nWan-Yen Lo, and Ross Girshick. 2019. Detectron2.\nhttps://github.com/facebookresearch/\ndetectron2.\nPengcheng Yang, Boxing Chen, Pei Zhang, and\nXu Sun. 2020. Visual agreement regularized train-\ning for multi-modal machine translation. In AAAI,\npages 9418–9425.\nZhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing\nHuang, Dong Yu, and Jiebo Luo. 2019. A fast and\naccurate one-stage approach to visual grounding. In\nProceedings of the IEEE International Conference\non Computer Vision, pages 4683–4693.\nShaowei Yao and Xiaojun Wan. 2020. Multimodal\ntransformer for multimodal machine translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4346–\n4350.\nYongjing Yin, Fandong Meng, Jinsong Su, Chulun\nZhou, Zhengyuan Yang, Jie Zhou, and Jiebo Luo.\n2020. A novel graph-based multi-modal fusion en-\ncoder for neural machine translation. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 3025–3035.\nMingyang Zhou, Runxiang Cheng, Yong Jae Lee, and\nZhou Yu. 2018. A visual attention grounding neural\nmodel for multimodal machine translation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3643–\n3653.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tieyan Liu. 2019.\nIncorporating bert into neural machine translation.\nIn International Conference on Learning Represen-\ntations.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7w: Grounded question answering\nin images. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n4995–5004.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7990349531173706
    },
    {
      "name": "Machine translation",
      "score": 0.7795420289039612
    },
    {
      "name": "Modal",
      "score": 0.6193069219589233
    },
    {
      "name": "Translation (biology)",
      "score": 0.58863765001297
    },
    {
      "name": "Natural language processing",
      "score": 0.5529388785362244
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5342774391174316
    },
    {
      "name": "Speech recognition",
      "score": 0.3448382616043091
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 11
}