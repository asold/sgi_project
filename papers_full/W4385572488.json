{
  "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
  "url": "https://openalex.org/W4385572488",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3213389398",
      "name": "Jonas Belouadi",
      "affiliations": [
        "Bielefeld University"
      ]
    },
    {
      "id": "https://openalex.org/A2073897212",
      "name": "Steffen Eger",
      "affiliations": [
        "Bielefeld University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2168020421",
    "https://openalex.org/W4385573691",
    "https://openalex.org/W4385574263",
    "https://openalex.org/W4243859118",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2492156598",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4224629880",
    "https://openalex.org/W3187134297",
    "https://openalex.org/W3120168417",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3175637107",
    "https://openalex.org/W2963047186",
    "https://openalex.org/W4283650355",
    "https://openalex.org/W2970474271",
    "https://openalex.org/W3034807061",
    "https://openalex.org/W2970686438",
    "https://openalex.org/W3169942382",
    "https://openalex.org/W3171639395",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4229053997",
    "https://openalex.org/W1976891289",
    "https://openalex.org/W4385573290",
    "https://openalex.org/W4282921036",
    "https://openalex.org/W2954010681",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964235962",
    "https://openalex.org/W3207091856",
    "https://openalex.org/W2093634890",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3042349250",
    "https://openalex.org/W2741104967",
    "https://openalex.org/W4287854472",
    "https://openalex.org/W2798782765",
    "https://openalex.org/W4386566861",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4206344656",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W3214260033",
    "https://openalex.org/W4294974598",
    "https://openalex.org/W3200704197",
    "https://openalex.org/W3154832031",
    "https://openalex.org/W2115221470",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3029251511",
    "https://openalex.org/W2970091785",
    "https://openalex.org/W3036839309",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3211622531",
    "https://openalex.org/W3135427360"
  ],
  "abstract": "State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7364‚Äì7381\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nByGPT5: End-to-End Style-conditioned Poetry Generation\nwith Token-free Language Models\nJonas Belouadi\njonas.belouadi@uni-bielefeld.de\nSteffen Eger\nsteffen.eger@uni-bielefeld.de\nNatural Language Learning Group (NLLG)\nFaculty of Technology, Bielefeld University\nnl2g.github.io\nAbstract\nState-of-the-art poetry generation systems are\noften complex. They either consist of task-\nspecific model pipelines, incorporate prior\nknowledgeintheformofmanuallycreatedcon-\nstraints,orboth. Incontrast,end-to-endmodels\nwould not suffer from the overhead of having\nto model prior knowledge and could learn the\nnuancesofpoetryfromdataalone,reducingthe\ndegree of human supervision required. In this\nwork, we investigate end-to-end poetry genera-\ntionconditionedonstylessuchasrhyme,meter,\nand alliteration. We identify and address lack\nof training data and mismatching tokenization\nalgorithms as possible limitations of past at-\ntempts. In particular, we successfully pre-train\nByGPT5, a new token-free decoder-only lan-\nguage model, and fine-tune it on a large custom\ncorpus of English and German quatrains anno-\ntated with our styles. We show thatByGPT5\noutperforms other models such asmT5,ByT5,\nGPT-2 and ChatGPT, while also being more\nparameter efficient and performing favorably\ncompared to humans. In addition, we analyze\nits runtime performance and demonstrate that\nit is not prone to memorization. We make our\ncode, models, and datasets publicly available.1\n1 Introduction\nEnd-to-endfine-tuningofpre-trainedlanguagemod-\nels likeGPT-2 (Radford et al., 2019) orT5 (Raf-\nfel et al., 2020a) on downstream tasks has been\nan immensely popular training paradigm for text-\ngenerationinthelastfewyears(Lietal.,2021). End-\nto-end models learn to complete a task by directly\nlearning all steps, without intermediary algorithms\nsuch as hand-crafted rules or post-processing. This\napproachhasproventobehighlyeffectiveonawide\nrange of problems such as dialog generation (Sun\netal.,2022;Yangetal.,2021),summarization(Zhu\netal.,2021;Zhongetal.,2021;Huangetal.,2021),\nand machine translation (Farinha et al., 2022; Tran\n1https://github.com/potamides/uniformers\nThesweetwildstrain,thesuddenstart, A\nWhichshakes theperfumedaltar‚Äôsflame, B\nTomakeitsshrinea sacredname, B\nAndsing itspraisein every heart. A\n‚ÄîByGPT5\nFigure1: GeneratedquatrainwithABBArhymescheme,\nhigh amount of alliterations (green), and iambic meter,\ni.e., unstressed syllable () follows stressed syllable ().\net al., 2020). Nevertheless, all these applications\nhave in common that they only concern themselves\nwith the generation of prosaic texts. Generating\nformal verse poetryon the other hand, with strict\nconstraintson aestheticstyle suchasrhymescheme,\nmeter and alliteration, remains a difficult problem.\nAttemptstoemployend-to-endsolutionsinthiscon-\ntext have so far been unsuccessful (W√∂ckener et al.,\n2021), with some authors even concluding that\nlanguage models cannot pick up such constraints\nfrom data alone (Popescu-Belis et al., 2022). As a\nconsequence,state-of-the-artpoetrygenerationsys-\ntems rely on human guidance by (i) injecting prior\nknowledge2 in the form of hard-coded constraints\nto filter model outputs or modify probability dis-\ntributions or (ii) breaking the whole process down\ninto sophisticated task-specific model pipelines.\nTianandPeng(2022),forexample,proposeason-\nnetgenerationframeworkwithfourdistinctpipeline\nsteps: content planning, rhyme pairs generation,\npolishingforaesthetics,andfinallysketch-to-sonnet\ngeneration. Further, they incorporate prior knowl-\nedgesuchaspronunciationdictionaries,knowledge\nbases, and lexically constrained decoding. Simi-\nlarly,HopkinsandKiela(2017)useWeightedFinite\nState Transducers to monitor whether their poetry\ngeneration systemmeets metric constraints androll\n2We define incorporating prior knowledge as ‚ÄúAny form of\ninfluence on model decisions not learned by the model itself.‚Äù\n7364\nback its state in case of a violation.\nSuch forms of human supervision lead to ram-\nifications that an end-to-end solution would not\nface. Pipelines are susceptible to errors in early\nmodules that propagate and are amplified in sub-\nsequent modules; an effect known as cascading\nof errors (Castro Ferreira et al., 2019). Similarly,\nincorporating prior knowledge depends on the clev-\nerness and intent of themodeler and generally\nbecomes more difficult when heterogeneous con-\nstraints are involved or the number of constraints\nincreases (Garbacea and Mei, 2022). Furthermore,\nstandard text-generation architectures do not lend\nthemselves well for manually applying constraints.\nDuetotheautoregressivegenerationoftokensfrom\nlefttoright,constraintsatarbitrarypositionscannot\nbeimplementedeasilyoronlywithadditionaltrade-\noffs (Garbacea and Mei, 2022). For example, end\nrhymes, which come at the end of a verse, cannot\nbe constrained in isolation due dependencies on\npreviously generated tokens. A commonly applied\nwork-around for this problem is to generate each\nverse in reverse (Lau et al., 2018; Jhamtani et al.,\n2019; Van de Cruys, 2020; Xue et al., 2021a).\nInthiswork,wethusaimtoreducetheamountof\nhumansupervisioninpoetrygenerationandexplore\nviable end-to-end solutions. We hypothesize that\nfailing to do so far has the following root causes:\n(i) lack of available training data. Poetry corpora\nlabeled with aesthetic styles are few and far be-\ntween and we speculate that they do not suffice\nto train a generalized model. (ii) Unfavorable to-\nkenization algorithms. Aesthetic styles of poetry\nsuch as rhyme, meter, and alliteration are often\nexpressed at the character-level while most avail-\nable off-the-shelf pre-trained models operate at the\nsubword-level (Kudo and Richardson, 2018). Xue\net al. (2022) showed that character-level models\n(also known astoken-free models) excel at other\ncharacter-level tasks so we assume that they would\nperform similarly well at poetry generation. Our\nkey contributions are as follows:\n(i) We pre-trainByGPT5, to our knowledge the\nfirst decoder-only transformer for character-\nlevel language modeling.\n(ii) Wecreate QuaTrain,alargemachine-labeled\npoetry corpus of quatrains in German and\nEnglish.\n(iii) By fine-tuningByGPT5 on QuaTrain, we\nshow that it learns character-level styles better\nthan subword-based systems, such asGPT-2\nand mT5, as well as other token-free models\nlike ByT5, while being more parameter effi-\ncientandalsofaringwellcomparedtohumans.\n(iv) We further demonstrate thatByGPT5 exhibits\nfew memorization problems, understands po-\netry better thanGPT-2 and ChatGPT, and\nalsoperformswellontasksthatdonotoperate\nat the character-level.\n2 Background\nIn formal verse poetry, poems have to follow strict\npatternsandrulesoflanguagewhichweterm styles.\nOurgoalistotrainanend-to-endpoetrygeneration\nsystem which learns toadhere tospecified stylesby\nitself. We refer to this asstyle-conditionedpoetry\ngeneration. In our work, we focus on generating\nquatrainsandconditioningonthefollowingdefining\nstyles of formal verse poetry (cf. Figure 1):\nRhyme A rhyme is the repetition of the same\nor similar sounds in the final accented syllables\nof words, which must be preceded by differing\nconsonants (Harmon et al., 2000). If all conditions\nare met, we speak of perfect rhymes, and if some\nof them are violated, for example, because the\nfinal sounds are different or the words are identical,\nwe speak of imperfect rhymes. In a quatrain with\nABABrhymescheme,thefirstandthirdlineendings\nrhyme, as do the second and fourth lines.\nMeter Meterreferstotherhythmicpatternwithin\na verse. In modern poetry, this rhythm is usu-\nally accented-syllabic, that is, the succession of\nstressed ( ) and unstressed syllables () occurs at\nregular intervals (Harmon et al., 2000). The rhyth-\nmic unit is also known as a foot and the meter of a\nverse can thus be described as a sequence of feet.\nIn English poetry, common feet are iambic (),\ntrochaic( ),anapestic( ),anddactylic(\n). For conditioning on meter, we consider all met-\nric feet appearing in our datasets (cf. Appendix A).\nAlliteration Harmon et al. (2000) define allitera-\ntion as the repetition of the same consonant sounds\nor any vowel sounds at the beginning of words or\nsyllablesthatareclosetogetherinaverse. Informal\nverse, alliteration is secondary to rhyme and meter,\nfollows less strict constraints, and is therefore not\nas easily classified. In this work, we thus consider\nthelevelof alliteration instead, which we classify\nas eitherlow, medium, orhigh (cf. ¬ß5).\n7365\nName Size Params Enc/Dec Token-free\nByGPT5\nsmall 73.5m Decoder ‚úì\nbase 139.2m Decoder ‚úì\nmedium 289.1m Decoder ‚úì\nGPT-2 base 124.4m Decoder ‚úó\nmedium 354.8m Decoder ‚úó\nByT5 small 300m Enc-Dec ‚úì\nmT5 small 300m Enc-Dec ‚úó\nTable 1: Pre-trained models we fine-tune.ByGPT5 is a\nnew model developed by us. The GermanGPT-2 model\nweusedoesnotexistinmediumsizeMinixhofer(2020),\nwhich is why we only use a base model there.\n3 Models\nWe induce a range of end-to-end poetry generation\nsystemsforEnglishandGermanbyfine-tuningpre-\ntrained transformer models (Vaswani et al., 2017).\nForconditioningonstyle,weconsidertwoarchitec-\ntural variants‚Äîencoder-decoder transformers (Xue\net al., 2021b, 2022) and decoder-only transform-\ners (Radford et al., 2019; Brown et al., 2020). As\nexplainedin¬ß1,wefocusontoken-freemodels,but\nalso consider subword-level models for compari-\nson. We do not experiment with models with more\nthan 400 million parameters since they exceed the\ncapacity of our available GPU resources.\nEncoder-Decoder For encoder-decoder models,\nweinitializetheencoderwithajointtripleofrhyme\nscheme, meter, and, alliteration level and generate\na quatrain with the decoder. We represent each\nstyle by a special token which we add to the model\nvocabulary. We useByT5 (Xue et al., 2022), a\ntoken-free pre-trained encoder-decoder model, as\na baseline. For comparison with subword-level\napproaches, we fine-tunemT5 (Xue et al., 2021b).\nDecoder-only As the input for encoder-decoder\nmodels is a relatively short sequence of styles, this\ncould lead to an underutilization of the encoder.\nWe thus hypothesize that a decoder-only model,\nwith styles supplied as a prompt string, would be\nbetter suited for our task. On the subword-level,\nmultiple models, such asGPT-2 (Radford et al.,\n2019), are readily available. However, to our best\nknowledge, no such model exists at the character-\nlevel yet, which is why we train our own. Since our\nnew model shares some similarities with theGPT\nfamily of models, but has its origin inByT5 (see\n¬ß4), we refer to it asByGPT5. An overview of all\nmodels we use can be seen in Table 1.\n12500 25000 37500\nTraining Steps\n2¬ï00\n2¬ï25\n2¬ï50\n2¬ï75\n3¬ï00\n3¬ï25\n3¬ï50\nPerplexity\nEnglish\nB/y.pcGPT5(small)\nB/y.pcGPT5(base)\nB/y.pcGPT5(medium)\n12500 25000 37500\nTraining Steps\nGerman\nB/y.pcGPT5(small)\nB/y.pcGPT5(base)\nB/y.pcGPT5(medium)\nFigure 2: Perplexity on the training data when pre-\ntrainingByGPT5 for English and German.\n4 ByGPT5\nFor pre-training our own token-free decoder-only\nmodelByGPT5,westartbymodifyingthearchitec-\nture ofByT5 and discard its encoder component.\nWe then initialize the weights with the decoder of\nByT53 to warm-start the training process (Rothe\netal.,2020;Tangetal.,2022). Werepeatthisforthe\nthreesmallestvariantsof ByT5. BecauseByT5 has\nan asymmetrical architecture, the resulting models\nretain only 25% of its parameters. We refer to their\nmodel sizes as small, base, and medium.\nAs training data, we useOpenWebText2 (Gao\net al., 2021) for English andcc100 (Conneau et al.,\n2020)forGerman. Forhyper-parameters,wefollow\nRadfordetal.(2019)andBrownetal.(2020)anduse\nAdamwithaweightdecayof0.1,abatchsizeof512,\nvaryinglearningratesdependingonmodelsize,and\ntrain on a causal language modeling objective for\n50kstepsfollowingLesteretal.(2021). Weprovide\nloss curves in Figure 2. As might be expected, the\nperplexity of larger models is generally lower than\nthat of smaller counterparts.\n5 Datasets\nWecollectarangeoflabeledandunlabeleddatasets\nof English and German poetry (cf. Table 2). As\nshown, we were able to procure labeled corpora for\nrhymeandmeter,buttheyarefartoosmalltotraina\npoetrygenerationsystem. Instead,weusethebigger\nunlabeledcorpora,astrainingdata,bylabelingthem\nautomatically (Belouadi and Eger, 2023). To make\nfulluseofthedata,weusenotonlyrealquatrainsbut\nalsopseudo-quatrains(any consecutive sequence\n3Byreferringtothisprocessasweightinitialization(rather\nthan continued pre-training), we adopt the terminology of\nRothe et al. (2020). This is intuitively sensible as we reuse\nonlyasubsetofweightsinaverydifferentarchitecture. Conse-\nquently, our model experiences considerably less exposure to\ntraining data compared to its competitors during pre-training.\n7366\nDataset Language Verses R M A\nEPG64 English 1k ‚úì ‚úì ‚úó\nProsodic English 2k ‚úó ‚úì ‚úó\nFORB English 1k ‚úì ‚úì ‚úó\nChicago English 95k ‚úì ‚úó ‚úó\nAnti-K German 4k ‚úì ‚úì ‚úó\nGRC German 41k ‚úì ‚úó ‚úó\nEPG English 2.8m ‚úó ‚úó ‚úó\nDLK German 2.8m ‚úó ‚úó ‚úó\nQuaTrain English 2.7m ‚àó ‚úì ‚Ä† ‚úì ‚Ä† ‚úì ‚Ä†\nGerman 5.9m ‚àó ‚úì ‚Ä† ‚úì ‚Ä† ‚úì ‚Ä†\n‚àó Verses may occur in multiple pseudo-quatrains.\n‚Ä† Labels are obtained from classifiers.\nTable 2: Available poetry datasets for rhyme (R), meter\n(M),andalliteration(A).Unlabeledcorpora(middle)are\norders of magnitude larger than labeled corpora (top),\nand we label them automatically (bottom). Further\ninformation can be found in Appendix A.\nof four lines), amounting to over 660k quatrains\nfor English and 1.4m for German. We refer to this\nnew dataset asQuaTrain, with further statistics\nin Appendix A. In the following, we explain the\nlabeling process for each style.\nFor automatically labeling rhyme and meter, we\nleveragetheavailablelabeleddataandtrainarange\nof classifiers. We evaluate them on held-out gold\ndata and subsequently use the best performing clas-\nsifier for each style (cf. Appendix A). Meter classi-\nfication is a multiclass classification problem with\na single verse as input, while rhyme classification\nis a binary classification problem with two verses\nseparated by a special token as input. We classify\nthe meter of a quatrain by choosing the dominant\nmeter among the verses4, and the rhyme scheme by\ndetermining which verses rhyme and which do not.\nAs no readily available poetry datasets include\nlabels for alliteration, we approach the problem in\na different way. The quantification of the level of\nalliteration in a document is a long known research\nproblem(Skinner,1939;Leavitt,1976;Blain,1987;\nBenner, 2014). Letùë£ùëñ be the atomic units of sound\nin verseùë£, Blain (1987) quantify alliteration as\nallit(ùë£)=\n/‚àöÔ∏Ñummationtext.Ô£∂|ùë£|\nùëñ=1\n/‚àöÔ∏Ñummationtext.Ô£∂|ùë£|\nùëó=ùëñ+1\nf(ùë£ùëñ ,ùë£ ùëó )\nùëó‚àíùëñ\n/‚àöÔ∏Ñummationtext.Ô£∂|ùë£|\nùëñ=1\n/‚àöÔ∏Ñummationtext.Ô£∂|ùë£|\nùëó=ùëñ+1\n1\nùëó‚àíùëñ\n, (1)\nwhere f(¬∑)is a similarity function of two sounds;\nthe default simply testing for equality. Intuitively,\nallit(¬∑)counts alliterative sounds in a verse, applies\n4Sinceinformalversepoetry,ameterismaintainedthrough-\nout a poem, this procedure is meaningful.\na distance penalty, and normalizes the score to\n[0, 1]. To get a score for quatrains, we average\nthe alliteration level of all verses. We consider\ninitial phonemes of words, as well as all further\nstressed phonemes as atomic sound unitsùë£ùëñ, and to\ndeterminephonemesandstress,weuseagrapheme-\nto-phoneme conversion model (Zhu et al., 2022).\nFurther, we conduct an internal study to determine\nseveral intensity thresholds based on a sample of\nquatrains. We classify the alliteration level of a\nquatrainas lowifthescoreisbelow0.05, mediumif\nthe score is below 0.1, andhigh if it is above that.\n6 Experiments\nFor fine-tuning, we use the same hyperparameters\nas in ¬ß4 for all models, but reduce the batch size\nto 128 (for efficiency reasons). We induce separate\nmodelsforeachlanguagein QuaTrain andtrainfor\n10 epochs.5 We conduct both automatic (¬ß6.1) and\nhuman evaluation (¬ß6.2). Examples of generated\nquatrains can be found in Appendix D.\n6.1 Automatic Evaluation\nFor automatic evaluation, we select four com-\nmon rhyme schemes (AABB, ABAB, ABBA, and\nABCB), the most popular meters per language\n(iambus, trochee, anapest, and dactyl for English;\niambus, trochee, and alexandrine for German), and\nall levels of alliteration to create 75 poems per\nmodelforeachpossiblecombination. Tofindoutif\nstyles are properly reflected in generated quatrains\nwe reuse the classifiers from ¬ß5, i.e., we use them\ntoclassifythegeneratedpoemsandseeifthestyles\nmatch. We define the following metrics:\nRhyme Scorecomputes the recall of verses that\nshould rhyme in a quatrain, as well as the\nrecall of verses that should not and takes their\narithmetic average.\nAlliteration Scoreis1ifaquatrainhasthecorrect\nalliteration level, else 0.\nMeter Scoreisthefractionofverseswithcorrectly\nclassified meters.\nCoherence uses BERT for next sentence predic-\ntion(Devlinetal.,2019)toassessdiscoursere-\nlations of verses (Duari and Bhatnagar, 2021;\nShi and Demberg, 2019). The score is the\nfraction of consecutive verse pairs that are\ncorrectly classified to come after one another.\n7367\nB/y.pcGPT5(small)B/y.pcGPT5(base)B/y.pcGPT5\n(medium)\nGPT-2(base)GPT-2\n(medium)\nB/y.pcT5\n(small)\n/m.pcT5\n(small)\n0¬ï4\n0¬ï5\n0¬ï6\n0¬ï7\n0¬ï8\n0¬ï9\nEnglish\nRhyme Score\nAlliteration Score\nMeter Score\nCoherence\nB/y.pcGPT5(small) B/y.pcGPT5(base) B/y.pcGPT5\n(medium)\nGPT-2(base) B/y.pcT5\n(small)\n/m.pcT5\n(small)\nGerman\nRhyme Score\nAlliteration Score\nMeter Score\nCoherence\nFigure 3: Automatic evaluation results for all models on English and German.\nWe provide the scores for each model averaged\nover all generated quatrains (2700 for German and\n3600 for English) in Figure 3. Although all models\nmanage to learn to follow aesthetic styles to some\ndegree, there are noticeable score differences.\nOn rhyme, allByGPT5 variants collectively out-\nperform allGPT-2 models on both languages by\n5%-20%. Similarly,ByT5consistentlyoutperforms\nmT5 by ~5%. This supports our theory that token-\nfree models are better suited for character-level\nstyles. Further,ByGPT5 (small) performs 2%-15%\nbetter thanByT5 (small) which means we can dis-\ncardtheencoderwhilestillimprovingperformance.\nSurprisingly though, base ByGPT5 and GPT-2\nachieve higher scores than their medium variants.\nWhilethismayinitiallysuggestthatlargerdecoders\nprioritize (meaningful) content, whereas smaller\ndecoders focus on style, the high coherence seen\nacross all models weakens this hypothesis. Instead,\nwespeculatethatthismaybeanoverfittingproblem.\nIn particular, smaller models, up to base size, may\nbe better suited for generating shorter texts such\nas quatrains. Another surprising finding is that\nByT5 (small) performs worse thanGPT-2 (base)\non English. We investigate this further in ¬ß6.2.\nIntermsofmeter,allmodelsperformverysimilar\ntooneanother. Whereas ByGPT5 (small)performs\nbestonEnglishbyasmallmargin,itisoutperformed\n5All models converge within the designated number of\nepochs, and throughout each epoch, the ordering of the sys-\ntems remains consistent with our final results from automatic\nevaluation.\nby mT5 (small) on German. This result is not\nsurprising. Since meter is a syllable-level style,\nsubword-levellanguagemodelsalsomanagetopick\nitupreasonablywell(Lauetal.,2018). Interestingly\nthough,onEnglishthescoresaremuchloweroverall\nthan on German. A reason for this may be that the\noccurrenceofdifferentmetersismuchmoreevenly\ndistributed in GermanQuaTrain (cf. Table 6 in\nthe appendix). While in German only about 60%\nof all meters are iambs, in English it is over 80%,\nmaking it difficult for models to learn other meters.\nWe identify further reasons in ¬ß6.2.\nAlliteration is the style that all models are the\nworst at. Our formulation of alliteration levels may\nmake it difficult for models to pick up its semantics.\nStill,ByGPT5 (base) performs the best on English,\nandByGPT5 (small)and ByT5 (small)performthe\nbest on German, suggesting that token-free models\nhave an advantage on this style.\nIn general, small and baseByGPT5 perform the\nbestonallthreestylesinEnglishandontwoofthem\nin German. It appears that they have an advantage\nin terms of tokenization algorithms (outperforming\nsubword-levelGPT-2 andmT5), architecture (out-\nperforming encoder-decoderByT5), and size (the\nmedium variant performs worse).\n6.2 Human Evaluation\nTo further validate the effectiveness of our models,\nwe conduct a human evaluation campaign using\nbest-worst scaling(BWS) as a means of annota-\ntion (Louviere et al., 2015). BWS is a variant of\n7368\n\u00001¬ï0 \u00000¬ï5 0¬ï0 0¬ï5 1¬ï0\n0¬ï00\n0¬ï05\n0¬ï10\n0¬ï15\n0¬ï20\nDensity\nRhyme\n\u00001¬ï0 \u00000¬ï5 0¬ï0 0¬ï5 1¬ï0\n0¬ï0\n0¬ï1\n0¬ï2\n0¬ï3\nHuman Likeness\n\u00001¬ï0 \u00000¬ï5 0¬ï0 0¬ï5 1¬ï0\n0¬ï00\n0¬ï05\n0¬ï10\n0¬ï15\nMeter\nHuman B/y.pcGPT5(base) GPT-2(base) B/y.pcT5(small) /m.pcT5(small)\nFigure 4: Distributions of BWS scores for rhyme, human likeness, and meter annotations through kernel density\nestimation. Scores range from -1 (very bad) to 1 (very good). The ‚Äú‚Ä¢‚Äù markers denote expected values.\ncomparative annotation that produces high-quality\nresults while keeping the number of required an-\nnotations low (Kiritchenko and Mohammad, 2016,\n2017). Annotators are presented with tuples ofùëõ\nitems (usuallyùëõ = 4) and asked to identify the best\nand worst item based on a specified property. By\nsubtracting the fraction of times an item is chosen\nas the best from the fraction of times it is chosen as\nthe worst, real-valued scores ranging from -1 (bad)\nto 1 (good) can be obtained (Orme, 2009).\nIn our annotations, we consider three proper-\nties: rhyme, meter, and human likeness, i.e., the\nlikelihood of a poem being written by a human.\nWe exclude alliteration to reduce the workload on\nour annotators. Similarly, we exclusively evalu-\nate on English (cf. Appendix B for a small-scale\nevaluation in German) and only consider the top-\nperforming model within each model class based\non the results of automatic evaluation. The mod-\nels in question thus areByGPT5 (base), GPT-2\n(base), ByT5 (small), andmT5 (small). Further-\nmore, we only choose from three rhyme schemes\n(AABB, ABAB and ABBA), two meters (iambus\nandtrochee),andonelevelofalliteration(medium),\nand create four poems per system for each possible\ncombination. In addition, we also randomly sam-\nple human quatrains from our datasets that match\nthe constraints and create 120 4-tuples from the\ncombined set of quatrains.\nFour annotators then annotate rhyme and human\nlikeness, whereas meter is evaluated by a single\nexpert annotator only (cf. Appendix B). Since we\nhave multipleannotators working onrhyme and hu-\nman likeness we use thesplit-half reliability(SHR)\nmeasure (Kiritchenko and Mohammad, 2017) to\nassess their consistency. SHR is calculated by split-\ntingtheannotationsintotwosets,computingscores\nfor each set, and then computing their Spearman\nrank correlation coefficient.\nFigure 4 displays a kernel density estimate for\neachproperty,withdistributionsshiftedtotheright\nindicating better performance. On rhymes, we\nobtain an SHR ofùúå = 0.77 which demonstrates\na high agreement between annotators. Human\nrhymes are ranked the highest overall, whereas\nByGPT5 comes in as a close second, followed by\nByT5. mT5 andGPT-2 perform the worst. This is\na bit different from our findings during automatic\nevaluation whereGPT-2 (base) was ranked higher\nthanByT5 (small)onEnglish. Ananalysisof GPT-\n2 generated quatrains revealed a predominance of\nimperfect rhymes as a likely cause. As our rhyme\nclassifier is trained on binary labels it is unable to\ndetectthis,buthumanannotatorsperceivethiskind\nof rhyme as worse.\nWith ùúå = 0.54, the SRH of human likeness is\nnoticeably lower than for rhyme. On the one hand,\nthissuggeststhatthistaskcouldbemoresubjective;\non the other hand, the generated quatrains must\nbe sufficiently human-like for subjectivity to be a\nfactor. Indeed, although humans rank higher than\nByGPT5 which in turn ranks higher thanGPT-2,\nthey all perform noticeably more similar than for\nrhyme. Nonetheless,wecanobservethat ByT5,and\nespeciallymT5 rank a bit lower. Both models were\npre-trained on corrupted spans and have thus never\nseen truly natural text during pre-training (Zhu\netal.,2022;Raffeletal.,2020b;Lesteretal.,2021)\nwhich we believe could be a possible cause.\nThe distributions for meter have large variances\nfor all models, and also humans. This is surprising,\nas it implies that our annotator does not think that\nhumans are superior, even though the automatic\nevaluation of English models was not particularly\n7369\nB/y.pcGPT5(small)B/y.pcGPT5(base)B/y.pcGPT5\n(medium)\nGPT-2(base)GPT-2\n(medium)\nB/y.pcT5\n(small)\n/m.pcT5\n(small)\n0¬ï2\n0¬ï3\n0¬ï4\n0¬ï5\nRhyme Score\nAlliteration Score\nMeter Score\nFigure5: Automaticevaluationoflow-resourcemodels.\nstrong on meter. We hypothesize that even among\nreal English poets, there is a significant amount of\npoetry that does not strictly adhere to metric con-\nstraints, so language models only learn to follow\nthem freely as well. Nonetheless, we can still see\nthat, among models,ByGPT5 is rated highest, fol-\nlowed byGPT-2, mT5, and lastlyByT5, reflecting\nour findings of automatic evaluation. Interestingly,\nByGPT5 also ranks higher than humans.\nOverall, our human evaluation suggests that\nByGPT5 performs best across all properties evalu-\nated, which is consistent with our automatic evalu-\nation. Moreover,ByGPT5 has shown the ability to\nperform at a level comparable to humans, and even\nsurpass human performance in the meter property.\n7 Analysis\nWe continue with a deeper analysis and look into\nlow-resource training (¬ß7.1), quantify memoriza-\ntion (¬ß7.2), evaluate the performance of token-free\nmodels on non-character-based, high-level tasks\n(¬ß7.3), introspect the models‚Äô understanding of\nstyle when predicting tokens (¬ß7.4), and compare\nByGPT5 withChatGPT (¬ß7.5).\n7.1 Low-resource Training\nWe hypothesized that a large training corpus is an\nimportant factor in successfully training an end-to-\nend poetry generation system. We examine this\nhypothesisbyselectinga5%subsetofEnglish Qua-\nTrain (33k quatrains) and re-training our models\nusing the same hyperparameters as in ¬ß6. Figure 5\nshows how well these new low-resource models\nadhere to style constraints, similar to the automatic\nevaluation of full training in Figure 3.\nCompared to full training, all low-resource mod-\nMemorization Emotion\nModel English German German\nByGPT5 (small) 0.0% 0.0% 0.676\nByGPT5 (base) 0.0% 0.04% 0.680\nByGPT5 (medium) 0.0% 0.81% 0.659\nGPT-2 (base) 0.39% 1.81% 0.676\nGPT-2 (medium) 3.64% ‚Äî ‚Äî\nByT5 0.0% 0.0% 0.691\nmT5 0.0% 0.0% 0.696\nTable 3: Extractive memorization rates (English &\nGerman) and recall on emotion generation (German).\nelsarenoticeablyworseatadheringtostyle. Specif-\nically, the performance drops by 15%-40% for\nrhyme, 5%-20% for meter, and 5%-10% for al-\nliteration. In addition, the overall performance\ndifference between all models is much smaller than\nin ¬ß6.1. While these findings support our hypothe-\nsis that training on large datasets is essential, they\nalso reveal thatByGPT5 demonstrates the largest\nimprovements as the dataset size increases (cf. Fig-\nure 3). We therefore theorize that larger datasets\nleadtosubstantialperformancegainsinpoetrygen-\nerationonlywhen coupled with architectures that\nexcel at character-level tasks.\nNevertheless, even in low-resource scenarios,\nByGPT5 (base) outperforms the other models in\nall categories except rhyme, where a few other\nsystems perform similarly. This suggests that the\nconclusionsdrawnin¬ß6.1holdtosomeextenteven\nwhen the available training data is limited.\n7.2 Extractive Memorization\nA common problem of language models, known\nas extractive memorization (EM), is generating\nverbatim copies from the training data during in-\nference (Carlini et al., 2022; Raunak and Menezes,\n2022; Meehan et al., 2020). According to Carlini\net al. (2022) EM occurs when a language model‚Äôs\ncontinuation of a string is part of the data it was\ntrainedon. Sincetheinputstoourlanguagemodels\nare strings of style, this formulation lends itself\nwell to our case: to detect memorization we simply\nhave to check if generated poems appear inQua-\nTrain. In Table 3, we compute the EM rates of the\nquatrains generated in ¬ß6.1. To account for negligi-\nble variations, we do not compare raw strings, but\ncalculate the Ratcliff-Obershelp similarity (Ratcliff\nand Metzener, 1988), and assume that two strings\nare equivalent if their similarity exceeds 0.7.\n7370\nAs can be seen,GPT-2 suffers from memoriza-\ntionthemost. OnEnglish,over3%ofalloutputsof\nGPT-2 (medium) are copied. WhileByGPT5 also\ncopiesquatrainstoanextent,itismuchlessaffected\nin comparison. On English,ByGPT5 (medium)\ndoes not copy anything and on German only 0.81%\nof all outputs. As a general trend, we can see\nthat bigger models tend to copy more data than\nsmaller ones‚Äîa finding shared by others (Carlini\net al., 2022). Interestingly, the encoder-decoder\nmodelsByT5 andmT5 do not seem to be affected\nbythisproblematall,mostlikelybecausestylesare\nnot used as a prompt, but are fed into the encoder\nseparately.\n7.3 Higher-level Styles\nWe also explore how token-free models perform\non higher-level styles which are not character- or\nsubword-level phenomena. In particular, we focus\non emotion usingPo-Emo (Haider et al., 2020), a\ndataset of eight aesthetic emotions in poetry (cf.\nAppendix A). By conditioning our models on these\nemotions, we can assess their ability to understand\nand depict emotion in poetry.\nAsin¬ß5,weleverageautomaticlabeling. Tothat\nend, we train a classifier on GermanPo-Emo as in\nHaider et al. (2020) and reproduce the results. We\nthen classify emotions in GermanQuaTrain and\nretrainoursystemsbyconditioningthemonallemo-\ntionsinaquatrain. Toevaluatehowwellthemodels\ncan discriminate emotions, we condition them on\nevery possible tuple of two distinct emotions and\ngenerate100poemseach(2800intotal),andreport\nthe recall of correctly classified emotions. The re-\nsults in Table 3 show that encoder-decoder models\nscore highest, withmT5 performing best. During\ntraining, conditioning inputs can be long and vari-\nable in size, a scenario for which encoder-decoders\nmay be better suited. Still, decoder-only models\nare not far behind. EspeciallyByGPT5 fares well\nagainstGPT-2, suggesting that token-free models\nare also competitive on higher-level tasks.\n7.4 Token-level Attributions\nTo introspect the decision-making processes of our\nmodels, we visualize their token-level attributions\nwhen generating a quatrain. Token-level attribu-\ntionsexplaintowhichdegreeeachtokenintheinput\nis involved in determining the next output token of\na model, allowing us to reason about what a model\nhas learned. To this end, Ferrando et al. (2022)\ndecompose the attention blocks of transformers\n<ABBA>\n<iambus>\n<high>\nI hold it true,whate‚Äôer befall;\nI feel it when I sorrow most;\n‚ÄôTis better to have loved and lost\nThan never to have loved at all.\n<ABBA>\n<iambus>\n<high>\nI hold it true,whate‚Äôer befall;\nI feel it when I sorrowmost;\n‚ÄôTis better to have loved and lost\nThan never to haveloved at all.\nFigure 6: A famous stanza by Tennyson (1850) with\nvisualized attention fromByGPT5 (top) and GPT-2\n(bottom) when generating the last syllable.\ninto a sum of vectors and define a new measure for\nvisualizingtoken-to-tokeninteractionsbasedonthe\ndistance of each vector to the output (Kobayashi\net al., 2021). We apply this measure on generative\nlanguage models and visualize token-level attribu-\ntions forByGPT5 and GPT-2 when generating the\nlast syllable in a quatrain. Since we have observed\nreoccurring trends, we use a single visualization\nin Figure 6 as a leading example. We provide\nadditional examples in German in Appendix C.\nWe can see thatByGPT5 puts a big emphasis\non the current verse, as well as the styles it was\nconditioned on. Further, possibly in response to\nthe ABBA rhyme scheme, it also heavily stresses\nthe ending of the first verse. Since the model\nalso places a moderate amount of attention on the\nlast consonants in verse three, it also seems to be\naware of which sounds it shouldnot generate in\norder maintain the rhyme scheme. Interestingly,\nit heavily emphasizes the letterv in the last two\nverses. We assume that this corresponds to what\nByGPT5 understands by alliteration, in which case\nitwouldnothaveunderstoodwellatwhichposition\nin a word the same sounds must occur.\nUnlikeByGPT5,GPT-2 does not put any visible\nemphasis on input style tokens, which suggests that\nit does not understand how to handle them very\nwell. Nevertheless it stresses similar aspects to\nByGPT5, although, due to the subword vocabulary,\nat a different level of granularity.\n7.5 Comparison with ChatGPT\nChatGPT (OpenAI,2022)isaconversationallarge\nlanguage model which specializes in dialogue. It\nhasattractedattentionforitsdetailedandexpressive\nanswers, raising the question of how well it per-\nforms in generating poetry. In a small-scale study,\nwe thus askChatGPT to generate quatrains with\n7371\nvarious rhyme schemes (AABB, ABAB, ABBA,\nand ABCB) using its web interface,6 and similarly\ngenerate poems usingByGPT5. We then construct\nrandom pairs of quatrains of each model and want\ntofindoutwhichpoemadheresbettertorhymecon-\nstraints. Since we know the quatrains ofChatGPT\nbeforehand, we use our rhyme scorer of ¬ß6.1 for\nunbiased scoring. Only in 15% of cases does our\nscorer prefer poems ofChatGPT overByGPT5.\nManual investigation showed thatChatGPT tends\ntogeneraterhymesatarbitrarypositions,ratherthan\nadhering to specified rhyme schemes, even when\ngiving examples in the prompt. Our verdict is that\nChatGPT isaviableapproachforpoetrygeneration\nbut notstyle-conditionedpoetry generation.\n8 Related Work\nAs indicated in ¬ß1, competing poetry generation\nsystems usually consist of model pipelines and/or\ninject prior knowledge. Zhang and Lapata (2014),\nfor example, propose a system for modeling qua-\ntrains consisting of three components: one model\nencodespreviousverses,asecondonereducesthem\nto a single context vector, and a third one generates\nverses, one at a time. During decoding, phrases\nwhich violate style constraints are discarded. Simi-\nlarly,Deep-speare (Lau et al., 2018) consists of a\nlanguagemodelthatgeneratesasetofsampleverses\nin reverse order, a model that reinitiates sampling\naslongasrhymeconstraintsarenotmet,andafinal\nmodelthatranksthesamplesaccordingtohowwell\nthey adhere to iambic pentameter. Van de Cruys\n(2020) also induce prior knowledge into a generic\nlanguagemodel,buttheydoitbymodifyingoutput\nprobability distributions directly. Jhamtani et al.\n(2019) put their focus on actuallylearning rhyme\nand train a sonnet and limerick generator through\nadversarial training. The model is hierarchical, i.e.,\nit first generates a sequence of line endings which\nare subsequently completed in reverse. While the\nmodelmanagestolearnthemeaningofrhymetoan\nextent, the authors still filter outputs using pronun-\nciationdictionaries. Moreinlinewithourresearch,\nHopkins and Kiela (2017) train a model on the\nphonetic representation of poetry using the Interna-\ntional Phonetic Alphabet (IPA) as a character-level\nvocabulary. Duringinference,asecondmodeltrans-\nlatessoundsbacktohuman-readabletext. Although\npromising, the model did not generalize well, and\n6After experimenting with prompt engineering and in-\ncontext learning, we settled on a straightforward template:\n‚ÄúGenerate a quatrain with<pattern> rhyme scheme.‚Äù\nan additional model enforces rhythmic constraints\nin their final approach. Ormazabal et al. (2022)\ntargetunsupervised poetry generation by training\na system on prosaic text only and conditioning it\non structural information such as line endings and\nsyllable counts. During inference, the system can\ngenerate poetry when conditioned on rhyming line\nendings and metric syllable counts. Nonetheless,\nsince this information must be crafted manually, it\nis still supervised in a slightly different sense. In\ncontrast, our model is supervised during training\nas it requires poetry to learn from, but unsuper-\nvised during inference as it is able to independently\nincorporate poetic elements.\n9 Conclusion\nIn this work, we implement end-to-end style-\nconditionedpoetrygenerationsystemsforquatrains\nin English and German. Unlike other work, our\nsystems are able to generate poetry without the\nneed for human supervision, except for the use\nof poetic training data. In particular, we present\nByGPT5,anoveltoken-freedecoder-onlylanguage\nmodel, and show that fine-tuning it on a custom\npoetry corpus outperforms other models, such as\nGPT-2, mT5, andByT5, on average, while also\nperforming favorably against human poets in our\nconstrained setting. Our key findings are that (i)\ntokenization algorithms matter, i.e., token-free lan-\nguagemodelsgenerallyperformbetteratgenerating\ncharacter-level styles than subword-level transform-\ners, and (ii) large datasets are crucial for successful\ntraining. We further show that bigger models do\nnot necessarily perform better and that decoder-\nonly architectures work best, i.e., we can discard\nthe encoder ofByT5 (75% of parameters) while\nstill improving downstream performance. We also\ndemonstrate that token-free transformers perform\ncompetitively on tasks not tied to character-level\nstyles, and are less susceptible to memorization\nof the training dataset. In addition, we conduct a\nvisual analysis of token-level attributions during\nquatrain generation that is consistent with human\nperception of styles.\nIn future work, we want to to extend our system\nto other poetic forms such as sonnets, limericks, or\nvillanelles.\n10 Limitations\nA well-known shortcoming of transformers is the\ncomputational complexity in self-attention lay-\n7372\nB/y.pcGPT5(small)B/y.pcGPT5(base)B/y.pcGPT5\n(medium)\nB/y.pcT5\n(small)\n/m.pcT5\n(small)GPT-2(base)GPT-2\n(medium)\n0\n250\n500\n750\n1000\n1250\n1500\n1750Time in [ms]\nFigure7: Inferencetimesforgeneratingasinglequatrain\n(with 177 characters) on an A6000 GPU.\ners (Vaswani et al., 2017). Since the number\nof required calculations grows quadratically with\nthe length of the input, transformers become pro-\nhibitivelyslowonverylongsequences. Anunfortu-\nnatesideeffectofprocessinginputsatthecharacter-\nlevelisthatinternalsequencesbecomemuchlonger,\nso token-free transformers run into these efficiency\nproblems much earlier than subword-based models.\nFigure 7 illustrates this problem by contrasting the\nruntime of all poetry generation systems when gen-\neratingasinglequatrain. Even ByGPT5(small),the\nsmallest model in terms of number of parameters\n(cf. Table 1) and the fastest token-free transformer,\nis only marginally faster thanGPT-2 (medium),\nwhich is almost five times larger. Tay et al. (2022)\npropose a solution to this problem for transformer\nencoder blocks by applying a neural pooling oper-\nation over input embeddings before feeding them\nintothemodel,whichcouldbeextendedtodecoder\nblocksinfuturework. Alternatively,Libovick√Ωetal.\n(2022)proposeatwo-stagedecodingarchitecturein\nwhichthetransformerdecoderoperatesoncharacter\nblocks that an additional LSTM model (Hochreiter\nand Schmidhuber, 1997) decodes into individual\ncharacters.\nAnother shortcoming is that our poetry genera-\ntion systems can only generate a single poetic form,\ni.e., quatrains. In general, poetry is a very diverse\nform of language and stanzas can be of arbitrary\nlength, so this is a serious limitation. In future\nwork, we thus plan to extend our implementation\nof style-conditioning to variable length poems. In\nparticular, one could encode a rhyme scheme not\nas a single special token, but as an arbitrary series\nof letters indicating which verses rhyme with each\nother. Alternatively, our current systems could be\nused to generate longer stanzas through a sliding\nwindow approach, i.e., generating one verse at a\ntime with the last three verse as context.\nFurther, our human evaluation has limitations\ndue to its relatively small scope. We only have a\nlimited number of annotators and only consider a\nsubset of all style combinations. Nevertheless, we\nhave achieved moderately high to high agreement\non all tasks, and we have an additional human\nevaluationofGermanpoetryinAppendixB,which\npoints to the same conclusion.\nLastly,QuaTrain is limited in that it consists\nof pseudo-quatrains, which are not real quatrains\nand often have missing contexts. Nonetheless, as\ncan be seen in Appendix D, models trained on\nQuaTrain are still able to generate meaningful\npoetry. In future work, we plan to improve the\nquality of our dataset by obtaining real quatrains\nfrom additional sources such as the Eighteenth-\nCentury Poetry Archive (Huber, 2022).\nAcknowledgments\nWe thank all reviewers for their valuable feedback,\nhard work, and time. We also thank all annota-\ntors, without whom this work would not have been\npossible. The last author was supported by DFG\ngrant EG 375/5‚Äì1. We further gratefully thank the\nBMBF for its support via the grant Metrics4NLG.\nReferences\nJonas Belouadi and Steffen Eger. 2023. UScore: An\neffective approach to fully unsupervised evaluation\nmetricsformachinetranslation. In Proceedingsofthe\n17thConferenceoftheEuropeanChapteroftheAsso-\nciationforComputationalLinguistics ,pages358‚Äì374,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nDrayton C. Benner. 2014. ‚ÄòThe Sounds of the Psalter:\nComputational Analysis of Soundplay‚Äô.Literary and\nLinguistic Computing, 29(3):361‚Äì378.\nDerrel R. Blain. 1987. A mathematical model for allit-\neration. Style, 21(4):607‚Äì625.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\n7373\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteuszLitwin,ScottGray,BenjaminChess,JackClark,\nChristopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language\nmodels are few-shot learners. InAdvances in Neural\nInformation Processing Systems, volume 33, pages\n1877‚Äì1901. Curran Associates, Inc.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models.\nThiagoCastroFerreira,ChrisvanderLee,EmielvanMil-\ntenburg, and Emiel Krahmer. 2019. Neural data-to-\ntext generation: A comparison between pipeline and\nend-to-end architectures. InProceedings of the 2019\nConference on Empirical Methods in Natural Lan-\nguageProcessingandthe9thInternationalJointCon-\nference on Natural Language Processing (EMNLP-\nƒ≤CNLP), pages 552‚Äì562, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022. Canine: Pre-training an efficient\ntokenization-freeencoderforlanguagerepresentation.\nTransactions of the Association for Computational\nLinguistics, 10:73‚Äì91.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. InPro-\nceedings of the 58th Annual Meeting of the Associa-\ntionforComputationalLinguistics ,pages8440‚Äì8451,\nOnline. Association for Computational Linguistics.\nJavier de la Rosa, √Ålvaro P√©rez, Mirella de Sisto, Laura\nHern√°ndez, Aitor D√≠az, Salvador Ros, and Elena\nGonz√°lez-Blanco. 2021. Transformers analyzing po-\netry: multilingual metrical pattern prediction with\ntransfomer-based language models.Neural Comput-\ning and Applications.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. InProceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSwagata Duari and Vasudha Bhatnagar. 2021. Ffcd: A\nfast-and-frugal coherence detection method.IEEE\nAccess, PP:1‚Äì1.\nAnaCFarinha,M.AminFarajian,MariannaBuchicchio,\nPatrick Fernandes, Jos√É¬© G. C. de Souza, Helena\nMoniz, and Andr√É¬© F. T. Martins. 2022. Findings\nof the wmt 2022 shared task on chat translation. In\nProceedings of the Seventh Conference on Machine\nTranslation, pages 724‚Äì743, Abu Dhabi. Association\nfor Computational Linguistics.\nJavier Ferrando, Gerard I. G√°llego, and Marta R. Costa-\njuss√†. 2022. Measuring the mixing of contextual\ninformation in the transformer. InProceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 8698‚Äì8714.\nLeoGao,StellaBiderman,SidBlack,LaurenceGolding,\nTravis Hoppe, Charles Foster, Jason Phang, Horace\nHe,AnishThite,NoaNabeshima,ShawnPresser,and\nConnor Leahy. 2021. The pile: An 800gb dataset of\ndiverse text for language modeling.\nCristina Garbacea and Qiaozhu Mei. 2022. Why is\nconstrained neural language generation particularly\nchallenging?\nThomas Haider. 2021. Metrical tagging in the wild:\nBuilding and annotating poetry corpora with rhyth-\nmicfeatures. InProceedingsofthe16thConferenceof\nthe European Chapter of the Association for Compu-\ntationalLinguistics: MainVolume,pages3715‚Äì3725,\nOnline. Association for Computational Linguistics.\nThomas Haider, Steffen Eger, Evgeny Kim, Roman\nKlinger,andWinfriedMenninghaus.2020. PO-EMO:\nConceptualization, annotation, and modeling of aes-\nthetic emotions in German and English poetry. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 1652‚Äì1663, Marseille,\nFrance. European Language Resources Association.\nThomas Haider and Jonas Kuhn. 2018. Supervised\nrhyme detection with Siamese recurrent networks. In\nProceedings of the Second Joint SIGHUM Workshop\non Computational Linguistics for Cultural Heritage,\nSocial Sciences, Humanities and Literature, pages\n81‚Äì86, Santa Fe, New Mexico. Association for Com-\nputational Linguistics.\nW. Harmon, C.H. Holman, and W.F. Thrall. 2000.A\nHandbook to Literature. Handbook to Literature\nSeries. Prentice Hall.\nSepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput., 9(8):1735‚Äì\n1780.\nJack Hopkins and Douwe Kiela. 2017. Automatically\ngenerating rhythmic verse with neural networks. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 168‚Äì178, Vancouver, Canada.\nAssociation for Computational Linguistics.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient attentions for long\ndocumentsummarization. In Proceedingsofthe2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1419‚Äì1436, Online.\nAssociation for Computational Linguistics.\n7374\nAlexander Huber. 2022. Eighteenth-century poetry\narchive. [Online; accessed 15-December-2022].\nHarsh Jhamtani, Sanket Vaibhav Mehta, Jaime Car-\nbonell, and Taylor Berg-Kirkpatrick. 2019. Learning\nrhyming constraints using structured adversaries. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-ƒ≤CNLP), pages 6025‚Äì\n6031, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSvetlana Kiritchenko and Saif Mohammad. 2017. Best-\nworst scaling more reliable than rating scales: A case\nstudy on sentiment intensity annotation. InProceed-\ningsofthe55thAnnualMeetingoftheAssociationfor\nComputational Linguistics (Volume 2: Short Papers),\npages 465‚Äì470, Vancouver, Canada. Association for\nComputational Linguistics.\nSvetlana Kiritchenko and Saif M. Mohammad. 2016.\nCapturing reliable fine-grained sentiment associa-\ntions by crowdsourcing and best‚Äìworst scaling. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 811‚Äì817, San Diego, California. Association\nfor Computational Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2021. Incorporating Residual and Nor-\nmalization Layers into Analysis of Masked Language\nModels. InProceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages4547‚Äì4568,OnlineandPuntaCana,Dominican\nRepublic. Association for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66‚Äì71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nJey Han Lau, Trevor Cohn, Timothy Baldwin, Julian\nBrooke, and Adam Hammond. 2018. Deep-speare:\nA joint neural model of poetic language, meter and\nrhyme. InProceedingsofthe56thAnnualMeetingof\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1948‚Äì1958, Melbourne,\nAustralia. Association for Computational Linguistics.\nJayA.Leavitt.1976. Onthemeasurementofalliteration\ninpoetry. ComputersandtheHumanities ,10(6):333‚Äì\n342.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages3045‚Äì3059,OnlineandPuntaCana,Dominican\nRepublic. Association for Computational Linguistics.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021. Pretrained language model for text gen-\neration: A survey. InProceedings of the Thirtieth\nInternational Joint Conference on Artificial Intel-\nligence, ƒ≤CAI-21, pages 4492‚Äì4499. International\nJoint Conferences on Artificial Intelligence Organiza-\ntion. Survey Track.\nJind≈ôich Libovick√Ω, Helmut Schmid, and Alexander\nFraser. 2022. Why don‚Äôt people use character-level\nmachine translation? InFindings of the Association\nfor Computational Linguistics: ACL 2022, pages\n2470‚Äì2485, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nJordan J. Louviere, Terry N. Flynn, and A. A. J. Marley.\n2015. Best-Worst Scaling: Theory, Methods and\nApplications. Cambridge University Press.\nCasey Meehan, Kamalika Chaudhuri, and Sanjoy Das-\ngupta. 2020. A three sample hypothesis test for\nevaluating generative models. InProceedings of the\nTwentyThirdInternationalConferenceonArtificialIn-\ntelligence and Statistics, volume 108 ofProceedings\nof Machine Learning Research, pages 3546‚Äì3556.\nPMLR.\nBenjamin Minixhofer. 2020. GerPT2: German large\nand small versions of GPT2.\nEduard M√∂rike. 1832.Maler Nolten. Novelle in zwei\nTeilen. 2. Teil. G. J. G√∂schen‚Äôsche Verlagshandlung,\nStuttgart.\nOpenAI. 2022. ChatGPT: Optimizing language models\nfor dialogue. [Online; accessed 17-December-2022].\nAitor Ormazabal, Mikel Artetxe, Manex Agirrezabal,\nAitor Soroa, and Eneko Agirre. 2022. PoeLM: A\nmeter- and rhyme-controllable language model for\nunsupervised poetry generation. InFindings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 3655‚Äì3670, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nBryan K. Orme. 2009. Maxdiff analysis : Simple\ncounting , individual-level logit , and hb. Sawtooth\nSoftware, Inc.\nAndreiPopescu-Belis,√ÄlexAtrio,ValentinMinder,Aris\nXanthos, Gabriel Luthier, Simon Mattei, and Anto-\nnio Rodriguez. 2022. Constrained language models\nfor interactive poem generation. InProceedings of\nthe Thirteenth Language Resources and Evaluation\nConference, pages 3519‚Äì3529, Marseille, France.\nEuropean Language Resources Association.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020a. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1‚Äì67.\n7375\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020b. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1‚Äì67.\nJohn W. Ratcliff and David E. Metzener. 1988. Pattern\nmatching: The gestalt approach.Dr. Dobb‚Äôs Journal,\npage 46.\nVikas Raunak and Arul Menezes. 2022. Finding memo:\nExtractive memorization in constrained sequence\ngeneration tasks. InFindings of the Association for\nComputational Linguistics: EMNLP 2022, pages\n5153‚Äì5162, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks.Transactions of the Associa-\ntion for Computational Linguistics, 8:264‚Äì280.\nWei Shi and Vera Demberg. 2019. Next sentence pre-\ndiction helps implicit discourse relation classification\nwithin and across domains. InProceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-ƒ≤CNLP), pages 5790‚Äì5796, Hong Kong,\nChina. Association for Computational Linguistics.\nBurrhus F. Skinner. 1939. The alliteration in shake-\nspeare‚Äôs sonnets: a study in literary behavior.Psy-\nchological Record.\nHaipengSun,JunweiBao,YouzhengWu,andXiaodong\nHe. 2022. BORT: Back and denoising reconstruction\nforend-to-endtask-orienteddialog. In Findingsofthe\nAssociation for Computational Linguistics: NAACL\n2022, pages 2156‚Äì2170, Seattle, United States. Asso-\nciation for Computational Linguistics.\nTianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong\nWen. 2022. Mvp: Multi-task supervised pre-training\nfor natural language generation.\nYi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Prakash\nGupta, Hyung Won Chung, Dara Bahri, Zhen Qin,\nSimon Baumgartner, Cong Yu, and Donald Metzler.\n2022. Charformer: Fast character transformers via\ngradient-based subword tokenization. InThe Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nAlfred Tennyson. 1850.In Memoriam A.H.H.Edward\nMoxon and Co., London.\nYufei Tian and Nanyun Peng. 2022. Zero-shot sonnet\ngeneration with discourse-level planning and aesthet-\nics features. InProceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 3587‚Äì3597, Seattle, United States.\nAssociation for Computational Linguistics.\nChau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. 2020.\nCross-lingual retrieval for iterative self-supervised\ntraining. InAdvances in Neural Information Process-\ning Systems, volume 33, pages 2207‚Äì2219. Curran\nAssociates, Inc.\nHerbert F. Tucker. 2011. Poetic data and the news from\npoems: A \"for better for verse\" memoir.Victorian\nPoetry, 49(2):267‚Äì281.\nTim Van de Cruys. 2020. Automatic poetry generation\nfrom prosaic text. InProceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2471‚Äì2480, Online. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz\nKaiser,andIlliaPolosukhin.2017. Attentionisallyou\nneed. InAdvances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc.\nJ√∂rg W√∂ckener, Thomas Haider, Tristan Miller, The-\nKhang Nguyen, Thanh Tung Linh Nguyen, Minh Vu\nPham, Jonas Belouadi, and Steffen Eger. 2021. End-\nto-end style-conditioned poetry generation: What\ndoes it take to learn from examples alone? In\nProceedings of the 5th Joint SIGHUM Workshop\non Computational Linguistics for Cultural Heritage,\nSocial Sciences, Humanities and Literature, pages\n57‚Äì66, Punta Cana, Dominican Republic (online).\nAssociation for Computational Linguistics.\nLanqingXue,KaitaoSong,DuocaiWu,XuTan,NevinL.\nZhang, Tao Qin, Wei-Qiang Zhang, and Tie-Yan Liu.\n2021a. DeepRapper: Neural rap generation with\nrhyme and rhythm modeling. InProceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConferenceonNaturalLanguageProcessing(Volume\n1: Long Papers), pages 69‚Äì81, Online. Association\nfor Computational Linguistics.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models.Transac-\ntionsoftheAssociationforComputationalLinguistics ,\n10:291‚Äì306.\nLintingXue,NoahConstant,AdamRoberts,MihirKale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021b. mT5: A massively multilingual\npre-trainedtext-to-texttransformer. InProceedingsof\nthe 2021 Conference of the North American Chapter\noftheAssociationforComputationalLinguistics: Hu-\nman Language Technologies, pages 483‚Äì498, Online.\nAssociation for Computational Linguistics.\nYunyiYang,YunhaoLi,andXiaojunQuan.2021. Ubar:\nTowards fully end-to-end task-oriented dialog system\nwith gpt-2.Proceedings of the AAAI Conference on\nArtificial Intelligence, 35(16):14230‚Äì14238.\n7376\nXingxing Zhang and Mirella Lapata. 2014. Chinese\npoetry generation with recurrent neural networks. In\nProceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 670‚Äì680, Doha, Qatar. Association for Compu-\ntational Linguistics.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir\nRadev. 2021. QMSum: A new benchmark for query-\nbased multi-domain meeting summarization. InPro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5905‚Äì5921, Online. Association for Computational\nLinguistics.\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\n2021. MediaSum: A large-scale media interview\ndataset for dialogue summarization. InProceedings\nofthe2021ConferenceoftheNorthAmericanChapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 5927‚Äì5934,\nOnline. Association for Computational Linguistics.\nJian Zhu, Cong Zhang, and David Jurgens. 2022.\nByT5 model for massively multilingual grapheme-\nto-phoneme conversion. InProc. Interspeech 2022,\npages 446‚Äì450.\n7377\nMeter Symbol\niambus\ntrochee\namphibrach\nanapest\ndactyl\nalexandrine\nTable 4: Meters in our dataset we consider for our\nexperiments. An alexandrine consists of iambic feet\nwith a caesura after the sixth syllable.\nModel Rhyme Meter\nCanine-C 98.05 58.49\nXLM-R 97.22 54.65\nmBERT 97.17 49.01\nTable 5: F1-Score on classifying rhyme and meter.\nA Additional poetry corpus statistics\nThe poetry corpora we collect are English Project\nGutenberg (EPG) and Deutsches Lyrik Korpus\n(DLK) (Haider, 2021) for unlabeled poetry, and\nProsodic7, Chicago Rhyme Corpus (Chicago)8,\nFor-better-for-verse (FORB) (Tucker, 2011), Ger-\nman Rhyme Corpus (GRC) (Haider and Kuhn,\n2018), as well as EPG64 and Anti-K (Haider,\n2021) for labeled poetry. We map meters which\nappear less than 25 times in our labeled corpora to\nthe special labelother. The final list of meters we\nconsider can be found in Table 4.\nThe performance of the meter and rhyme clas-\nsifiers we train can be seen in Table 5. For each\nstyle, we perform a 90/5/5 train-valid-test split and\nfine-tunearangeofencoder-onlytransformerswith\nclassification heads jointly on both languages, as\nthis improves performance (de la Rosa et al., 2021;\nHaider and Kuhn, 2018). We test subword-level\nmBERT and XLM-R, as well as character-level\nCanine-C (Clarketal.,2022). Sincecharacter-level\nCanine-C outperforms bothmBERT andXLM-R\non both tasks, we use it as our final classifier.\nDuring automatic labeling, when the rhyme\nscheme cannot be clearly determined (e.g., accord-\ning to the classifier the first verse rhymes with the\nsecond, the second with the third but the first and\nthethirddonotrhyme)ornodominantmeterexists,\nwe discard the quatrain. Frequencies of automatic\nlabels insideQuaTrain can be seen in Table 6.\nBy limitingQuaTrain to quatrains, we not only\nreduce the burden on poetry generation systems\n7https://github.com/quadrismegistus/prosodic\n8https://github.com/sravanareddy/rhymedata\nby considering only a single poetic form, but also\nease the labeling process. As the length of poems\nincreases, the number of verse pairs that have to\nbe classified for rhyme grows super-exponentially,\nwhich quickly becomes intractable.\nThe eight emotions in Po-Emo we train our\nclassifier on arebeauty / joy,sadness,uneasiness,\nvitality, awe / sublime, suspense, humor, andan-\nnoyance. Since an additional emotion,nostalgia,\nalmost never occurs, we follow Haider et al. (2020)\nand omit it from our experiments.\nB Annotator Demographics\nOur human annotators are fluent in English at a C1\norhigherlevelaccordingtotheCommonEuropean\nFramework of Reference for Languages (CEFR).\nThe annotators for rhyme and human likeness are\none male faculty member, two male PhD students,\nonefemaleundergraduatestudent,andthreefemale\nvolunteers from other departments, amounting to\nseven distinct annotators who are all proficient in\nEnglish but may have limited knowledge of poetry.\nTogetfoursetsofannotationsperstyle,wehaveone\nPhDstudentannotatebothstyles. Formeter,wehire\na professional female teacher who is specialized in\nEnglish and music.\nSince none of our annotators speak English as\na native language, we have one PhD student and\none faculty member conduct a small-scale com-\nparative study in their native language, German,\nannotating 30 BWS tuples for rhyme and human\nlikeness. The results in Table 7 confirm the trends\nwe saw in human-evaluation in ¬ß6.2. In terms of\nrhyme,humansperformbest,followedby ByGPT5,\nByT5,mT5,andfinally GPT-2. Onhumanlikeness\nByGPT5 is outperformed by humans but performs\nsimilar toGPT-2. The encoder-decoder models\nmT5 andByT5 perform the worst, likely for similar\nreasons outlined in ¬ß6.2.\nC German Token-level Attributions\nFigure 8 shows token-level attribution scores for\nGerman quatrains. By and large, we observe the\nsame trends as in ¬ß7.4 on English, i.e.,GPT-2\nplaces less attention on style and the emphasized\nparts of the text are less granular.\nD Example Quatrains\nIn Table 8 we list additional example quatrains\nin German and English, generated withByGPT5\n(base).\n7378\nRhyme Meter Alliteration\nLanguage label freq. label freq. label freq.\nGerman\nABCD 19.73% iambus 61.66% low 50.95%\nABAB 15.60% alexandrine 18.06% medium 39.23%\nAABB 13.07% trochee 17.05% high 9.81%\nEnglish\nAABB 19.17% iambus 83.96% medium 54.92%\nABCD 16.40% anapest 7.61% low 28.94%\nABBC 13.07% trochee 4.13% high 16.14%\nTable 6: Distribution of alliteration levels, as well as most frequent meters and rhyme schemes inQuaTrain.\nModel Rhyme Human Likeness\nHuman 0.84 0.89\nByGPT5 0.57 0.55\nGPT-2 0.35 0.55\nByT5 0.45 0.19\nmT5 0.28 0.32\nTable 7: Min-max normalized and averaged\nBWS scores annotated by two native German\nspeakers. The SHR isùúå = 0.83 for rhyme and\nùúå = 0.61 for human likeness.\n<ABBA>\n<trochee>\n<medium>\nFr√ºhling l√§√üt sein blaues Band\nWieder flattern durch die L√ºfte;\nS√º√üe, wohlbekannteD√ºfte\nStreifen ahnungsvoll das Land.\n<ABBA>\n<trochee>\n<medium>\nFr√ºhlingl√§√üt sein blaues Band\nWieder flattern durchdie L√ºfte;\nS√º√üe, wohlbekannte D√ºfte\nStreifenahnungsvoll das Land.\nFigure 8: A well-known stanza by M√∂rike (1832) with\nvisualized attention fromByGPT5 (top) and GPT-2\n(bottom) when generating the last syllable.\nGerman English\nEin ReiterstehtamHafen, A\nDerschautdieFlut nichtan, B\nErh√∂rt dieSchifferschlafen A\nImstillenOzean. B\nWithlanguid smile,thestealingtear retires, A\nAndtheslowfading light on trembling fires! A\nNowshe receivesthegoldencircletround, B\nAndfillsthewoven chamberswitha sound; B\nSchweigendstehndie Burgennieder, A\nUnddie L√ºftesind verhallt, B\nUnddie Trommelnklingenwider, A\nUnddie B√ºchsen knallenhalt. B\nThefirstwho learned thelessonthere A\nHad learned toscoff andscorn tosneer, A\nAndthat thelearnedmighthavebeen B\nAshamelesswoman andaqueen. B\nDerGreiserbebt,dieHand erstarrt, A\nDieKinder schauern vordem Sterben; B\nDieStimme bricht,dieThr√§ne fallt, C\nSiesiehtihmnachmitnassem Beben. B\nThencame thelabourof thedaywithin, A\nThegraybeginning oftheweek, B\nAnddownwe wentwithhopeandterrorsin, A\nAndnota wordtosay andspeak. B\nDieStr√∂mungwiederum durchalleGlieder dringt, A\nUndalles, wasdalebtundwalltund leuchtet,singt. A\nDasteigteinPalmenstrauchaus demerhabnenLichte, B\nErschwimmtaufeiner Fluthundsingetin Gedichte. B\nForthestarsarein thesky; A\nAndthestarshavegonetodie A\nWiththeirsongsof joyandfear, B\nWiththeirmusicand theircheer. B\nTable 8: Additional example poems generated withByGPT5 (base) in German and English.\n7379\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\n9\n‚ñ°\u0017 A2. Did you discuss any potential risks of your work?\nIt is likely that the risks associated with using a poetry generation system trained on texts that are\nalready hundreds of years old may be manageable and may not require explicit addressing.\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\n1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\n3,4\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\n3, Abstract A\n‚ñ°\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAll artifacts are publicly available under permissive licenses without restrictions. Which is why we\ndidn‚Äôt explicitly discuss terms of use.\n‚ñ°\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe do not explicitly discuss intended use of our artifacts, as the use case does not differ from the\nartifacts we derive them from.\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe collect our data from existing datasets which already have quality control performed on them, to\nwhich we reference.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n3,4\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7380\nC ‚ñ°\u0013 Did you run computational experiments?\n3,4,5,6\n‚ñ°\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe report number of parameters used in our models, but not the overall computational budget. Our\nmodels were trained as low priority slurm jobs which were often interrupted which would make such\nestimates inaccurate. We do however discuss computational performance in Section 9.\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n3,5\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5,6\n‚ñ°\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAll these things are documented in our code artifact.\nD ‚ñ°\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n6\n‚ñ°\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nWe detail our annotation process in our own words, not by copying the text of instructions verbatim.\n‚ñ°\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nAppendix C\n‚ñ°\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nThe data we collect from our annotators does not contain sensitive information. We told our\nannotators what we‚Äôre aiming for, but didn‚Äôt explicitly write about these instructions in our paper.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n‚ñ°\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nAppendix C\n7381",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8028088808059692
    },
    {
      "name": "End-to-end principle",
      "score": 0.727817714214325
    },
    {
      "name": "Security token",
      "score": 0.6944730877876282
    },
    {
      "name": "Rhyme",
      "score": 0.6398621797561646
    },
    {
      "name": "Poetry",
      "score": 0.5691322684288025
    },
    {
      "name": "Language model",
      "score": 0.5393549799919128
    },
    {
      "name": "Natural language processing",
      "score": 0.5269489884376526
    },
    {
      "name": "Task (project management)",
      "score": 0.49703672528266907
    },
    {
      "name": "Alliteration",
      "score": 0.464255154132843
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45651933550834656
    },
    {
      "name": "Interpretability",
      "score": 0.4326443076133728
    },
    {
      "name": "Memorization",
      "score": 0.4212402403354645
    },
    {
      "name": "Speech recognition",
      "score": 0.39903199672698975
    },
    {
      "name": "Linguistics",
      "score": 0.19132104516029358
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20121455",
      "name": "Bielefeld University",
      "country": "DE"
    }
  ],
  "cited_by": 10
}