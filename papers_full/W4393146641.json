{
  "title": "OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples",
  "url": "https://openalex.org/W4393146641",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3206907639",
      "name": "Ryuto Koike",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2000996583",
      "name": "Masahiro Kaneko",
      "affiliations": [
        "Tokyo Institute of Technology",
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2030501650",
      "name": "Naoaki Okazaki",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2000996583",
      "name": "Masahiro Kaneko",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Tokyo Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2406990858",
    "https://openalex.org/W2499568709",
    "https://openalex.org/W3021153741",
    "https://openalex.org/W204891295",
    "https://openalex.org/W4287889356",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3101891351",
    "https://openalex.org/W6682799413",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W2152575748",
    "https://openalex.org/W4383987308",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4360891421",
    "https://openalex.org/W4383751019",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4366342860",
    "https://openalex.org/W4353007481",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W4386136032",
    "https://openalex.org/W4288334893",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W4225959163",
    "https://openalex.org/W4378771612",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4377865296",
    "https://openalex.org/W4324299222",
    "https://openalex.org/W4318149317"
  ],
  "abstract": "Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.",
  "full_text": "OUTFOX: LLM-Generated Essay Detection Through In-Context Learning\nwith Adversarially Generated Examples\nRyuto Koike1, Masahiro Kaneko2,1, Naoaki Okazaki1\n1Tokyo Institute of Technology\n2MBZUAI\nryuto.koike@nlp.c.titech.ac.jp, masahiro.kaneko@mbzuai.ac.ae, okazaki@c.titech.ac.jp\nAbstract\nLarge Language Models (LLMs) have achieved human-level\nfluency in text generation, making it difficult to distinguish\nbetween human-written and LLM-generated texts. This poses\na growing risk of misuse of LLMs and demands the devel-\nopment of detectors to identify LLM-generated texts. How-\never, existing detectors lack robustness against attacks: they\ndegrade detection accuracy by simply paraphrasing LLM-\ngenerated texts. Furthermore, a malicious user might attempt\nto deliberately evade the detectors based on detection results,\nbut this has not been assumed in previous studies. In this pa-\nper, we propose OUTFOX, a framework that improves the\nrobustness of LLM-generated-text detectors by allowing both\nthe detector and the attacker to consider each other’s output.\nIn this framework, the attacker uses the detector’s prediction\nlabels as examples for in-context learning and adversarially\ngenerates essays that are harder to detect, while the detec-\ntor uses the adversarially generated essays as examples for\nin-context learning to learn to detect essays from a strong at-\ntacker. Experiments in the domain of student essays show that\nthe proposed detector improves the detection performance on\nthe attacker-generated texts by up to +41.3 points F1-score.\nFurthermore, the proposed detector shows a state-of-the-art\ndetection performance: up to 96.9 points F1-score, beating\nexisting detectors on non-attacked texts. Finally, the proposed\nattacker drastically degrades the performance of detectors\nby up to -57.0 points F1-score, massively outperforming the\nbaseline paraphrasing method for evading detection.\nIntroduction\nLLMs, characterized by their enormous model size and vast\ntraining data, have demonstrated emergent abilities with im-\npressive performance across various tasks (Wei et al. 2022).\nThese abilities include a high degree of language compre-\nhension, fluent generation, and the capacity to handle tasks\nunseen during training through in-context learning (Brown\net al. 2020; Ouyang et al. 2022; Sanh et al. 2022).\nDespite these successes, there are growing concerns about\nthe potential misuse of LLMs. A notable example is in edu-\ncation, where students might copy and paste text generated\nby LLMs, such as ChatGPT (OpenAI 2023c), for their as-\nsignments. This concern has led to the development of detec-\ntors designed to identify LLM-generated text with promising\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: In our OUTFOX framework, there are three steps.\nStep 1⃝: The detector outputs prediction labels to texts in a\ntraining set. Step 2⃝: The attacker uses the detector’s predic-\ntion labels as examples for in-context learning to generate\nmore sophisticated attacks against a training set. Step 3⃝:\nThe detector uses these adversarially generated texts by a\nstrong attacker to detect texts in a test set.\ndetection performance (Kirchenbauer et al. 2023; Mitchell\net al. 2023; OpenAI 2023a; Tang, Chuang, and Hu 2023).\nUnfortunately, existing detectors often perform poorly\nagainst simple attacks (e.g., paraphrasing), as highlighted by\nrecent studies (Sadasivan et al. 2023; Krishna et al. 2023). A\nrecent survey called for developing robust detection methods\nagainst other potential attacks designed to deceive the de-\ntectors (Tang, Chuang, and Hu 2023). Given the human-like\ngenerative abilities of LLMs, there’s the unexplored risk that\nmalicious users might exploit LLMs to create texts specifi-\ncally designed to evade detection.\nMotivated by this need, we propose OUTFOX, a novel\nframework designed to enhance the robustness and appli-\ncability of LLM-generated text detectors. As illustrated in\nFigure 1, OUTFOX introduces an approach where both the\ndetector and the attacker learn from each other’s output. The\nattacker uses the detector’s predictions as examples for in-\ncontext learning to generate more sophisticated attacks. In\ncontrast, the detector uses these adversarially generated texts\nas examples for in-context learning to improve its detection\nabilities against a strong attacker. To validate our approach\nin a realistic domain, we create a dataset of native-speaker\nstudent essay writing to detect LLM-generated essays. Our\ndataset contains 15,400 triplets of essay problem statements,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21258\nstudent-written essays, and LLM-generated essays.\nExperiments show that our OUTFOX detector substan-\ntially improves the detection performance on the attacker-\ngenerated texts by up to +41.3 points F1-score compared\nto without considering attacks. This result empirically sug-\ngests that LLMs, especially ChatGPT, might learn implicit\ndifferences between non-attacked and attacked texts via in-\ncontext examples. Interestingly, our OUTFOX detector per-\nforms consistently as well or even better on non-attacked\ntexts, resulting in the state-of-the-art detection performance\nof up to 96.9 points F1-score on non-attacked texts. This\nresult demonstrates that considering attacks in our detec-\ntor has little negative effect on detection performance on\nnon-attacked texts. Furthermore, our OUTFOX attacker can\ndrastically degrade the performance of detectors, with a\ndecrease of up to -57.0 points F1-score, which massively\noutperforms the baseline paraphrasing method for evading\ndetection. In our analysis, we explore the semantic differ-\nences between non-attacked and attacked essays. The anal-\nysis reveals that our attacker-generated essays are semanti-\ncally closer to human-written essays than non-attacked es-\nsays, leading to success in such effective attacking. For re-\nproducibility, we release our code and dataset.1\nRelated Work\nLLM-Generated Text Detection Tackling the malicious\nuses of LLMs, recent studies have proposed detectors\nto identify LLM-generated texts. These detectors can be\nmainly categorized into watermarking algorithms, statistical\noutlier detection methods, and supervised classifiers. Water-\nmarking algorithms use token-level secret markers in texts\nthat humans cannot recognize for detection (Kirchenbauer\net al. 2023). In order to embed the markers into texts, the\nprobabilities of selected tokens by a hash function are mod-\nified to be higher in text generation at each time step. Our\nwork focuses not on the watermark-enhanced LMs, but on\nLLMs that are openly used in our daily life—hence our cho-\nsen domain of student essays. Statistical outlier detection\nmethods exploit statistical differences in linguistic features\nbetween human-written and LLM-generated texts. These in-\ncludes n-gram frequencies (Badaskar, Agarwal, and Arora\n2008), entropy (Lavergne, Urvoy, and Yvon 2008), per-\nplexity (Beresneva 2016), token log probabilities (Solaiman\net al. 2019), token ranks (Ippolito et al. 2020), and negative\ncurvature regions of the model’s log probability (Mitchell\net al. 2023). Supervised classifiers are models specifically\ntrained to discriminate human-written and LLM-generated\ntexts with labels. The classifiers range from classical meth-\nods (Ippolito et al. 2020; Crothers, Japkowicz, and Vik-\ntor 2023) to neural-based methods (Solaiman et al. 2019;\nBakhtin et al. 2019; Uchendu et al. 2020; Rodriguez et al.\n2022; Guo et al. 2023).\nDetection for Assessing Academic Dishonesty In the ed-\nucational context, LLM-based services (e.g., ChatGPT and\nBard (Google 2023)) have the potential to help students with\ntheir writing. However, many schools consider these ser-\n1https://github.com/ryuryukke/OUTFOX\nvices as academic dishonesty because students can misuse\nthese tools to cheat on assignments (OpenAI 2023b). Based\non this, recent studies have investigated LLM-generated text\ndetection for student assignments, such as argumentative es-\nsays (Liu et al. 2023) and university-level course problems\n(Ibrahim et al. 2023; Vasilatos et al. 2023). Specifically,\nLiu et al. (2023) targeted the essay domain written by non-\nnative English learners. However, considering the human-\nlevel generative capabilities of LLMs, it can be more dif-\nficult to classify LLM-generated and native-written essays\nthan non-native-written ones. Thus, as a more challenging\nsetting, we create a dataset to distinguish native-student-\nwritten from LLM-generated essays.\nAttacking LLM-Generated Text Detection Most recent\nstudies have reported the effectiveness of paraphrasing at-\ntacks, where existing detectors experience a large loss in\naccuracy when given a paraphrased version of an LLM-\ngenerated text (Sadasivan et al. 2023; Krishna et al. 2023).\nFor instance, Krishna et al. (2023) proposed DIPPER, the\n11B document-level paraphraser, controlling output diver-\nsity on vocabulary and content re-ordering. Considering the\nhuman-level generative abilities of LLMs, malicious users\nmight instruct LLMs to adversarially generate texts based\non detection results, but this has not been explored in prior\nwork.\nDefense against Attacking LLM-Generated Text Detec-\ntion The existence of these attacking methods, in turn,\nposes a need for a defense against such attacks. There are\nfew studies specifically about defending against the attacks.\nKrishna et al. (2023) proposed a retrieval-based defense that\ndetects a text that is semantically similar to one of the LLM-\ngenerated responses in an API database. However, it needs\nactive actions by API providers, and based on the nature\nof the method, a false positive rate can be higher with a\nlarger database. In addition, Sadasivan et al. (2023) recently\nshowed that the retrieval-based defense method is vulnera-\nble against recursive paraphrasing. The detection accuracy\nsignificantly drops to 25% after 5 rounds of paraphrasing by\nKrishna et al. (2023)’s paraphraser.\nIn recent concurrent work by Hu, Chen, and Ho (2023),\nthey proposed RADAR, a framework that jointly trains a ro-\nbust LLM-generated-text classifier against the paraphrasing\nattack via adversarial learning. In our OUTFOX framework,\nthe detector and the attacker can learn from each other’s ad-\nversarially generated output via in-context learning, without\nany parameter updates. Thus, the detector in our framework\ncan handle new attacks by simply adding these to in-context\nexamples without additional fine-tuning. Additionally, the\nattacker in our framework is not limited to the paraphrasing\nattack but is directly designed to deceive the detector.\nOUTFOX Framework\nTask Formulation\nOur work focuses on distinguishing LLM-generated essays\nfrom human-written essays. We assume that a training in-\nstance for detecting LLM-generated essays consists of a\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21259\nFigure 2: An illustration of our OUTFOX detector: The detector utilizes the adversarially generated essays as examples for\nin-context learning to learn to detect essays from our OUTFOX attacker.\ntriplet of an essay-problem statement, a human-written es-\nsay, and an LLM-generated essay. In our framework, the\nattacker adversarially generates attacked text, designed to\nfool the detector, while the detector tries to learn from exam-\nples to better distinguish LLM-generated and human-written\ntext. Both the detector and attacker are able to consider\neach other’s outputs by including examples in their input\nprompts for in-context learning. The detector can consider\nessay-label pairs that can consist of human-written,attacked\n(adversarially-generated by an attacker), and non-attacked\n(generated by an LLM, but not adversarially) essays by in-\ncluding them in its input prompt. On the other hand, the at-\ntacker can consider examples of non-attacked text and the\ndetector’s label predictions in its input prompt.\nThe OUTFOX Detector\nFigure 2 illustrates the outline of our OUTFOX detector.\nGiven a target essaye paired with a problem statementp, we\nretrieve the top-k problem statements that are semantically\nclose2 to the target problem statementp from the training set,\ntogether with human-written and LLM-generated essays as-\nsociated with the retrieved problem statements. To generate\nattacks to be considered in detection, we randomly select j\nout of the k problem statements (0 ≤ j ≤ k). Consequently,\nour OUTFOX attacker adversarially generates an essay for\neach selected problem statement. In this way, we obtain k\nhuman-written, (k − j) LLM-generated, and j attacked es-\n2We follow the setting of Prabhumoye et al. (2022), which\nfound that leveraging semantically close examples is effec-\ntive for in-context learning. We use the vector space of\nall problem statements computed by Term Frequency-Inverse\nDocument Frequency (TF-IDF) implemented in scikit-learn:\nhttps://tinyurl.com/scikitlearn-TF-IDF-vectorizer. The closeness is\ncomputed by (1 − s), where s presents the cosine similarity of the\nvectors of two problem statements.\nsays and construct a mixture of these essays with labels\nRdet = {(ei, li)}2k\ni=1. Here, li is Human when the essay ei\nis written by a human, or LM when the essay ei is written by\neither the LLM or attacker. Tagging attacker-generated texts\nwith LM label encourages our detector to learn the implicit\ncharacteristics of attacker-generated texts for detection. Fi-\nnally, the detector predicts a label ˆl for the target essay e\nby using the LLM with the task instruction I and Rdet as a\nprompt,\nˆl = argmax\nl∈{Human,LM}\nP (l | I, Rdet, e) , (1)\nRdet = {(ei, li)}2k\ni=1. (2)\nHere, the task instruction I is “Please classify whether the\ntext is generated by a human or a Language Model (LM).”\nEach example in Rdet is converted into the text format,\n“Text: ei Answer: li.” The target essay e is converted into\n“Text: e. Answer: ” so that the LLM predicts a label as a\ncontinuation of the prompt.\nThe OUTFOX Attacker\nFigure 3 illustrates the workflow of our OUTFOX attacker.\nUnlike the paraphrasing attack, our attacker generates an\nessay from a problem statement so that it fools our detec-\ntor. Given a problem statement p, we first retrieve the top-k\nproblem statements pi (i ∈ {1, . . . , k}) that are semanti-\ncally close 2 to the problem statement p from the training\nset, along with human-written and LLM-generated essays\nei associated with the retrieved problem statements. Then,\nour detector predicts a label ˆli ∈ {Human,LM} on each re-\ntrieved LLM-generated essay ei using in-context examples\nfrom the training set. At this time, our detector does not con-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21260\nFigure 3: An illustration of our OUTFOX attacker: The attacker considers our OUTFOX detector’s prediction labels as examples\nfor in-context learning and adversarially generates essays that are harder to detect.\nsider an attack. 3 Consequently, we create a mixture of the\nretrieved problem statements, the predicted labels, and the\nretrieved LLM-generated essays Ratk = {(pi, ˆli, ei)}k\ni=1.\nFinally, our attacker adversarially generates an essay a as\na continuation for the prompt rendered from Id, Ratk, the\ntask instruction of essay-generation I, and the given prob-\nlem statement p. We use an LLM to predict the next token\nˆat for the previously predicted tokens ˆa1:t−1,\nˆat ≈ argmax\nat∈V\nP (at | Id, Ratk, I, p,ˆa1:t−1) , (3)\nRatk = {(pi, ˆli, ei)}k\ni=1. (4)\nHere, V is the vocabulary of the LLM. The description Id is\n“Here are the results of detecting whether each essay from\neach problem statement is generated by a Human or a Lan-\nguage Model (LM)”. Each example in Ratk is converted to\nthe text format, “Problem Statement: pi. Answer: ˆli. Essay:\nmi.” The task instruction I is “Given the following problem\nstatement, please write an essay detected as Human in N\nwords with a clear opinion”.N is the number of words in the\nhuman-written essay paired with the given problem state-\nment p. The problem statement p is converted into “Problem\nStatement: p. Answer: Human. Essay: ” so that the LLM ad-\nversarially generates an essay to fool the detector as a con-\ntinuation of the prompt, including explicit Human label.\nConstructing a Dataset to Detect\nLLM-Generated Essays\nWe build a dataset for student essay writing, specifically\nto detect LLM-generated essays. There are already some\n3Although our framework theoretically allows the detector and\nattacker to iteratively strengthen each other many times, we focus\non only once.\ndatasets for the purpose of automatically scoring student-\nwritten essays (Hamner et al. 2012; Maggie et al. 2022),\nbut few of them have abundant essay-problem statement\npairs. To get LLM-generated essays, we need essay problem\nstatements. We focus on the essay dataset of Maggie et al.\n(2022), consisting of argumentative essays written by native\nstudents from 6th to 12th grade in the U.S. Firstly, we in-\nstruct ChatGPT to generate a pseudo-problem-statement to\nmimic a setting where a student would produce the supplied\nessay. Afterward, we instruct an instruction-tuned LLM to\ngenerate an essay based on each generated problem state-\nment. For each LLM, our dataset contains 15,400 triplets of\nessay problem statements, student-written essays, and LLM-\ngenerated essays. In our evaluation, we split the dataset into\nthree parts: train/validation/test with 14400/500/500 exam-\nples, respectively. Besides the non-attacked LLM-generated\nessays, to evaluate each attacker in our experiments, we also\nbuild 500 attacked essays associated with problem state-\nments in our test set for each attacker.\nExperiments and Results\nIn our experiments, we investigate the following aspects:\n• How robust is our detector, considering attacks, against\nattacked texts?\n• Does our detector, considering attacks, consistently per-\nform well even on non-attacked texts?\n• Is our attacker stronger than the previous paraphrasing\nattack approach?\nOverall Setup\nEssay Generation Models To generate non-attacked es-\nsays, we instruct the instruction-tuned LMs: ChatGPT (gpt-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21261\nAttacker Detector Metrics (%) ↑\nHumanRec MachineRec AvgRec F1\nDIPPER\nw/o Attacks 98.6 66.2 82.4 79.0\nw/ DIPPER 98.2 79.6 88.9 87.8\nw/ OUTFOX 97.8 72.4 85.1 82.9\nOUTFOX\nw/o Attacks 98.8 24.8 61.8 39.4\nw/ DIPPER 98.6 20.8 59.7 34.0\nw/ OUTFOX 97.2 69.6 83.4 80.7\nTable 1: Comparison of the detection performances of our OUTFOX detector on attacked essays, with and without considering\nattacks: our OUTFOX attack and the DIPPER attack. The DIPPER paraphrases ChatGPT-generated essays for attacking. In the\nrows of “w/o Attacks”, we show the detection performances of our detector, without considering attacks, on attacked essays by\neach attacker.\n3.5-turbo-0301), GPT-3.5 (text-davinci-003), and FLAN-\nT5-XXL4. In each, we set thetemperature parameter to 1.3.\nEvaluation Metrics and Dataset Area Under Receiver\nOperating Characteristic curve (AUROC) can be applied\nonly to the detectors which output real number prediction\nscores. Since our proposed detector outputs a binary label,\nwe employ the F1-score on LLM-generated texts as our\nfirst metric. Our second metric for detection performance\nis AvgRec, following Li et al. (2023). AvgRec is the av-\nerage of HumanRec and MachineRec. In our evaluation,\nHumanRec is the recall for detecting Human-written texts,\nand MachineRec is the recall for detecting LLM-generated\ntexts. We compute a classification threshold for each base-\nline detector on our validation set where the Youden In-\ndex5 (Youden 1950) is maximum in the ROC curve. Finally,\nwe evaluate detectors with these metrics and thresholds on\nour test set: a mixture of 500 human-written and 500 non-\nattacked essays. To evaluate detectors on attacked essays,\nwe swap only LLM-generated essays from non-attacked to\nattacked ones. Additionally, the threshold for each detector\nis fixed on both non-attacked and attacked essays.\nDetection Methods Our OUTFOX detector is based on\nChatGPT (gpt-3.5-turbo-0301). We set the temperature and\ntop\np parameters to 0 in order to eliminate the randomness\nof our detection. Our detector takes retrieved in-context ex-\namples for an essay. In detection without considering at-\ntacks, the in-context examples are 5 human-written and 5\nLLM-generated essays. In detection with considering at-\ntacks, the in-context examples are 5 human-written, 3 at-\ntacked, and 2 LLM-generated essays. Here, regardless of the\nessay generation models to be detected, our detector takes\nChatGPT-generated essays as part of in-context examples.\nAs a comparison with prior work, we compare our detector\nto the following detectors, divided into two groups: statis-\ntical outlier approaches and supervised classifiers. The first\ngroup covers Rank, LogRank, Log Probability, and Detect-\nGPT (as explained in §). The second group contains Ope-\n4https://huggingface.co/google/flan-t5-xxl\n5The Youden Index is the difference between True Positive\nRate (TPR) and False Positive Rate (FPR). The cut-off point in\nthe ROC curve, where the Youden Index is maximum, is the best\ntrade-off between TPR and FPR.\nnAI’s RoBERTa-based GPT-2 classifiers6 (Base, Large) and\nHC3 ChatGPT detector 7. HC3 is the latest corpus targeted\nfor detecting ChatGPT-generated texts (Guo et al. 2023). We\nemploy default parameters for each detection method.8\nAttacking Methods Our OUTFOX attacker is based on\nChatGPT (gpt-3.5-turbo-0301). In generating attacks, we\nconfigure the temperature parameter to 1.3. Our attacker\ntakes retrieved in-context examples for a problem statement.\nThe number of in-context examples is 10. We compare our\nattacker to the paraphrasing attack by the DIPPER (as ex-\nplained §). The DIPPER paraphrases the non-attacked es-\nsays for attacking. We configure both parameters for vocab-\nulary L and content re-ordering O to 60, which are the pa-\nrameters found to produce the strongest attack in Krishna\net al. (2023). Other hyperparameters are set to the defaults.\nResults\nHow Robust Is Our Detector, considering Attacks,\nagainst Attacked Texts? Table 1 presents the difference\nin detection performance of our OUTFOX detector with and\nwithout considering attacks on attacked essays. The attack-\ning models include our OUTFOX attacker and the DIP-\nPER. Throughout all attackers, our detector improves the\ndetection performance when considering attacks. For in-\nstance, on our attacker-generated essays, our detector shows\n+41.3 points F1-score and +21.6 points AvgRec improve-\nments when considering our attack. From this result, we\nempirically observe that our detector learns to detect essays\nfrom attackers via in-context examples. Notably, our detec-\ntor, when considering our attack, shows the performance im-\nprovements on any attacked essay, while our detector con-\nsidering the DIPPER improves only on the attacked essays\nby the DIPPER, but not our attacker. This observation sug-\ngests that our attacker is not merely paraphrasing but may\ngenerate semantically diverse essays from given problem\nstatements. Consequently, our detector is enabled to identify\ntexts that employ a broader spectrum of attacks.\n6https://github.com/openai/gpt-2-output-dataset/tree/master/\ndetector\n7https://huggingface.co/Hello-SimpleAI/chatgpt-detector-\nroberta\n8To adopt DetectGPT, we change onlybuffer size from default\nto 2 in order to escape being stuck in the perturbation step.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21262\nEssay Generator Detector Metrics (%) ↑\nHumanRec MachineRec AvgRec F1\nChatGPT\nw/o Attacks 99.0 94.0 96.5 96.4\nw/ DIPPER 99.2 87.8 93.5 93.1\nw/ OUTFOX 97.8 92.4 95.1 95.0\nGPT-3.5\nw/o Attacks 98.6 95.2 96.9 96.8\nw/ DIPPER 98.8 92.4 95.6 95.5\nw/ OUTFOX 97.6 96.2 96.9 96.9\nFLAN-T5-XXL\nw/o Attacks 98.8 68.2 83.5 80.5\nw/ DIPPER 99.2 72.0 85.6 83.3\nw/ OUTFOX 97.0 73.4 85.2 83.2\nTable 2: Comparison of the detection performances of our OUTFOX detector on non-attacked essays, with and without con-\nsidering the attacks: our OUTFOX attack and the DIPPER attack. In the rows of “w/o Attacks”, we show the detection perfor-\nmances of our detector, without considering attacks, on non-attacked essays.\nDetector Attacker Metrics (%) ↓\nHumanRec MachineRec AvgRec F1\nRoBERTa-base\nNon-attacked 93.8 92.2 93.0 92.9\nDIPPER 93.8 89.2 91.5 91.3\nOUTFOX 93.8 69.2 81.5 78.9\nRoBERTa-large\nNon-attacked 91.6 90.0 90.8 90.7\nDIPPER 91.6 97.0 94.3 94.4\nOUTFOX 91.6 56.2 73.9 68.3\nHC3 detector\nNon-attacked 79.2 70.6 74.9 73.8\nDIPPER 79.2 3.4 41.3 5.5\nOUTFOX 79.2 0.4 39.8 0.7\nOUTFOX\nNon-attacked 99.0 94.0 96.5 96.4\nDIPPER 98.6 66.2 82.4 79.0\nOUTFOX 98.8 24.8 61.8 39.4\nTable 3: Comparison of the detection performances of the detectors on ChatGPT-generated essays, before and after being\nattacked: our OUTFOX attack and the DIPPER attack. In the rows of “Non-attacked”, we show the detection performances of\neach detector on non-attacked essays.\nDoes Our Detector, considering Attacks, Consistently\nPerform Well Even on Non-attacked Texts? Table 2\nshows the difference in the detection performance of our\nOUTFOX detector on non-attacked essays, with and with-\nout considering attacks. Our detector consistently performs\nwell, even on non-attacked essays. The difference is mini-\nmal: an average decrease of only a -0.1 point F1-score and\na -0.32 point AvgRec across all comparisons. Furthermore,\nin non-attacked essays by FLAN-T5-XXL, our detector per-\nforms better than without considering attacks. These results\nempirically show that considering attacks has little negative\neffects on the detection performance of our detector on the\nnon-attacked texts.\nIs Our Attacker Stronger than the Previous Paraphras-\ning Attack Approach? Table 3 provides the detection\nperformance on attacked essays by different attacking ap-\nproaches: our attack and the DIPPER attack. Here, our de-\ntector does not consider any attacks. Our attacker drastically\ndegrades the detection performance of our detector and su-\npervised classifiers the most by up to -57.0 points F1-score\nand -69.2 points MachineRec. In addition, our attacker has a\nmore detrimental impact on the detection performance of all\ndetectors than the DIPPER by up to -39.6 points F1-score.\nWe also find that the DIPPER doesn’t degrade the detection\nperformance much and conversely improves the detection\nperformance, especially on RoBERTa-large. This is partially\nbecause the DIPPER attack is based on paraphrasing and not\ndesigned specifically to attack detectors resulting in the op-\nposite effect.\nComparison with Prior Work\nIn this section, we compare the detection performance of our\nOUTFOX detector, when considering our attack, and prior\ndetectors on non-attacked essays. The prior detectors are di-\nvided into two groups: statistical outlier approaches and su-\npervised classifiers.\nStatistical Outlier Approaches Statistical outlier ap-\nproaches need access to model logits for their detection.\nThus, we contrast our detector with these statistical out-\nlier approaches in detecting essays by FLAN-T5-XXL. As\nshown in Table 4, our detector has a far superior detection\nperformance of 80.5 points F1-score and 83.5 points Av-\ngRec to previous statistical approaches. We find that statis-\ntical outlier approaches tend to aggressively label Human-\nwritten essays as LLM-generated from the contrast between\nthe low HumanRec and the high MachineRec. While the Hu-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21263\nBaseline type Essay Generator Detector Metrics (%) ↑\nHumanRec MachineRec AvgRec F1\nStatistical outlier methods FLAN-T5-XXL\nlog p(x) 2.0 97.6 49.8 66.0\nRank 28.8 86.2 57.5 67.0\nLogRank 12.0 90.6 51.3 65.0\nEntropy 39.4 80.4 59.9 66.7\nDetectGPT 29.8 76.2 53.0 61.9\nOUTFOX 97.0 73.4 85.2 83.2\nSupervised classifiers\nChatGPT\nRoBERTa-base 93.8 92.2 93.0 92.9\nRoBERTa-large 91.6 90.0 90.8 90.7\nHC3 detector 79.2 70.6 74.9 73.8\nOUTFOX 97.8 92.4 95.1 95.0\nGPT-3.5\nRoBERTa-base 93.8 92.0 92.9 92.8\nRoBERTa-large 92.6 92.0 92.3 92.3\nHC3 detector 79.2 85.0 82.1 82.6\nOUTFOX 97.6 96.2 96.9 96.9\nTable 4: Comparison of the detection performances of our OUTFOX detector and prior approaches on non-attacked essays.\nPrior approaches include statistical outlier detectors and supervised classifiers. We compare our detector with statistical outlier\ndetectors on the essays by FLAN-T5-XXL and supervised classifiers on the essays by ChatGPT and GPT-3.5.\nFigure 4: Cosine similarity distributions of non-attacked\nessays and our OUTFOX attacker-generated essays with\nhuman-written essays, respectively.\nmanRec of our detector is high: 98.8 points, partially be-\ncause our detector is based on ChatGPT, which is trained\nwith human feedback, thus avoiding aggressive detection.\nSupervised Classifiers We compare our detector and su-\npervised classifiers in detecting essays generated by each\nChatGPT and GPT-3.5. Table 4 shows that our detector\nhas better detection performance on both essays by Chat-\nGPT and GPT-3.5 than supervised classifiers. We empiri-\ncally find that ChatGPT has the few-shot ability to detect\nLLM-generated texts via labeled in-context examples with-\nout any parameter updates.\nIn summary, our OUTFOX detector shows the state-of-\nthe-art detection performance on non-attacked essays.\nThe Semantic Similarity of Our OUTFOX\nAttacks with Human-Written Essays\nTo explore why our attacker substantially degrades the\ndetectors’ performance, we investigate the difference be-\ntween the semantic similarity of non-attacked essays and our\nattacker-generated essays with human-written essays. We\nfocus on our test set, consisting of 500 human-written, 500\nChatGPT-generated, and 500 attacked essays generated by\nour attacker. For determining semantic similarity between\ntwo essays, we employ a pre-trained BERT model 9 to each\nessay and compute a cosine similarity of the resulting em-\nbeddings. Figure 4 presents cosine similarity distributions of\nnon-attacked and our attacker-generated essays with human-\nwritten essays. A rightward shift implies that our attacker-\ngenerated essays are more semantically similar to human-\nwritten essays than non-attacked essays. From the shift, we\nempirically find that our OUTFOX attacker can generate es-\nsays that are more semantically similar to human-written es-\nsays than the non-attacking normal LLM, leading to diffi-\nculty in detecting attacker-generated essays.\nConclusion\nWe proposed OUTFOX, a framework that improves the ro-\nbustness of the detector against attacks by allowing both the\ndetector and the attacker to consider each other’s outputs as\nexamples for in-context learning. The experiments in the do-\nmain of student essays demonstrate that 1) Our detector can\nlearn to detect essays from attackers via in-context exam-\nples and 2) Notably, considering attacks of our detector has\nlittle negative effect on the detection of non-attacked texts.\nand 3) Our attacker, which is designed specifically to deceive\nthe detector, can evade current LLM-generated text detectors\nmore effectively than the previous paraphrasing attack. Fur-\nthermore, our analysis reveals that our attacker can generate\nan essay that is semantically closer to a human-written es-\nsay than a non-attacked essay, leading to success in effective\nattacking. In future work, we will apply our framework to\nother domains, such as fake news generation and academic\npaper writing.\n9https://huggingface.co/bert-large-cased\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21264\nAcknowledgements\nThese research results were obtained from the commis-\nsioned research (No.22501) by National Institute of Infor-\nmation and Communications Technology (NICT), Japan.\nReferences\nBadaskar, S.; Agarwal, S.; and Arora, S. 2008. Identifying\nReal or Fake Articles: Towards better Language Modeling.\nIn Proceedings of the Third International Joint Conference\non Natural Language Processing: Volume-II.\nBakhtin, A.; Gross, S.; Ott, M.; Deng, Y .; Ranzato, M.; and\nSzlam, A. 2019. Real or Fake? Learning to Discriminate\nMachine from Human Generated Text. arXiv:1906.03351.\nBeresneva, D. 2016. Computer-generated text detection us-\ning machine learning: A systematic review. In 21st Interna-\ntional Conference on Applications of Natural Language to\nInformation Systems, NLDB, 421–426. Springer.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. arXiv:2005.14165.\nCrothers, E.; Japkowicz, N.; and Viktor, H. 2023. Machine\nGenerated Text: A Comprehensive Survey of Threat Models\nand Detection Methods. arXiv:2210.07321.\nGoogle. 2023. Google AI updates: Bard and new AI features\nin Search. Accessed: 2023-05-10.\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\nYue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-\nman Experts? Comparison Corpus, Evaluation, and Detec-\ntion. arXiv:2301.07597.\nHamner, B.; lynnvandev, J. M.; Shermis, M.; and Ark, T. V .\n2012. The Hewlett Foundation: Automated Essay Scoring.\nAccessed: 2023-05-10.\nHu, X.; Chen, P.-Y .; and Ho, T.-Y . 2023. RADAR:\nRobust AI-Text Detection via Adversarial Learning.\narXiv:2307.03838.\nIbrahim, H.; Liu, F.; Asim, R.; Battu, B.; Benabderrahmane,\nS.; Alhafni, B.; Adnan, W.; Alhanai, T.; AlShebli, B.; Bagh-\ndadi, R.; B ´elanger, J. J.; Beretta, E.; Celik, K.; Chaqfeh,\nM.; Daqaq, M. F.; Bernoussi, Z. E.; Fougnie, D.; de Soto,\nB. G.; Gandolfi, A.; Gyorgy, A.; Habash, N.; Harris, J. A.;\nKaufman, A.; Kirousis, L.; Kocak, K.; Lee, K.; Lee, S. S.;\nMalik, S.; Maniatakos, M.; Melcher, D.; Mourad, A.; Park,\nM.; Rasras, M.; Reuben, A.; Zantout, D.; Gleason, N. W.;\nMakovi, K.; Rahwan, T.; and Zaki, Y . 2023. Perception, per-\nformance, and detectability of conversational artificial intel-\nligence across 32 university courses. arXiv:2305.13934.\nIppolito, D.; Duckworth, D.; Callison-Burch, C.; and Eck,\nD. 2020. Automatic Detection of Generated Text is Easi-\nest when Humans are Fooled. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 1808–1822. Online: Association for Computational\nLinguistics.\nKirchenbauer, J.; Geiping, J.; Wen, Y .; Katz, J.; Miers, I.;\nand Goldstein, T. 2023. A Watermark for Large Language\nModels. arXiv:2301.10226.\nKrishna, K.; Song, Y .; Karpinska, M.; Wieting, J.; and Iyyer,\nM. 2023. Paraphrasing evades detectors of AI-generated\ntext, but retrieval is an effective defense. arXiv:2303.13408.\nLavergne, T.; Urvoy, T.; and Yvon, F. 2008. Detecting Fake\nContent with Relative Entropy Scoring. In Proceedings of\nthe ECAI’08 Workshop on Uncovering Plagiarism, Author-\nship and Social Software Misuse, CEUR Workshop Proceed-\nings.\nLi, Y .; Li, Q.; Cui, L.; Bi, W.; Wang, L.; Yang, L.; Shi, S.;\nand Zhang, Y . 2023. Deepfake Text Detection in the Wild.\narXiv:2305.13242.\nLiu, Y .; Zhang, Z.; Zhang, W.; Yue, S.; Zhao, X.; Cheng, X.;\nZhang, Y .; and Hu, H. 2023. ArguGPT: evaluating, under-\nstanding and identifying argumentative essays generated by\nGPT models. arXiv:2304.07666.\nMaggie, A. F.; Benner, M.; Rambis, N.; Baffour, P.; Hol-\nbrook, R.; and ulrichboser, S. C. 2022. Feedback Prize -\nPredicting Effective Arguments. Accessed: 2023-05-10.\nMitchell, E.; Lee, Y .; Khazatsky, A.; Manning, C. D.;\nand Finn, C. 2023. DetectGPT: Zero-Shot Machine-\nGenerated Text Detection using Probability Curvature.\narXiv:2301.11305.\nOpenAI. 2023a. AI Text Classifier. Accessed: 2023-05-10.\nOpenAI. 2023b. Educator considerations for ChatGPT. Ac-\ncessed: 2023-05-10.\nOpenAI. 2023c. Introducing ChatGPT. Accessed: 2023-05-\n10.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and\nLowe, R. 2022. Training language models to follow instruc-\ntions with human feedback. arXiv:2203.02155.\nPrabhumoye, S.; Kocielnik, R.; Shoeybi, M.; Anandkumar,\nA.; and Catanzaro, B. 2022. Few-shot Instruction Prompts\nfor Pretrained Language Models to Detect Social Biases.\narXiv:2112.07868.\nRodriguez, J.; Hay, T.; Gros, D.; Shamsi, Z.; and Srinivasan,\nR. 2022. Cross-Domain Detection of GPT-2-Generated\nTechnical Text. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, 1213–\n1233. Seattle, United States: Association for Computational\nLinguistics.\nSadasivan, V . S.; Kumar, A.; Balasubramanian, S.; Wang,\nW.; and Feizi, S. 2023. Can AI-Generated Text be Reliably\nDetected? arXiv:2303.11156.\nSanh, V .; Webson, A.; Raffel, C.; Bach, S.; Sutawika, L.;\nAlyafeai, Z.; Chaffin, A.; Stiegler, A.; Raja, A.; Dey, M.;\nBari, M. S.; Xu, C.; Thakker, U.; Sharma, S. S.; Szczechla,\nE.; Kim, T.; Chhablani, G.; Nayak, N.; Datta, D.; Chang,\nJ.; Jiang, M. T.-J.; Wang, H.; Manica, M.; Shen, S.; Yong,\nZ. X.; Pandey, H.; Bawden, R.; Wang, T.; Neeraj, T.; Rozen,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21265\nJ.; Sharma, A.; Santilli, A.; Fevry, T.; Fries, J. A.; Teehan,\nR.; Scao, T. L.; Biderman, S.; Gao, L.; Wolf, T.; and Rush,\nA. M. 2022. Multitask Prompted Training Enables Zero-\nShot Task Generalization. In International Conference on\nLearning Representations.\nSolaiman, I.; Brundage, M.; Clark, J.; Askell, A.; Herbert-\nV oss, A.; Wu, J.; Radford, A.; Krueger, G.; Kim, J. W.;\nKreps, S.; McCain, M.; Newhouse, A.; Blazakis, J.;\nMcGuffie, K.; and Wang, J. 2019. Release Strategies and\nthe Social Impacts of Language Models. arXiv:1908.09203.\nTang, R.; Chuang, Y .-N.; and Hu, X. 2023. The Science of\nDetecting LLM-Generated Texts. arXiv:2303.07205.\nUchendu, A.; Le, T.; Shu, K.; and Lee, D. 2020. Author-\nship Attribution for Neural Text Generation. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 8384–8395. Online: Asso-\nciation for Computational Linguistics.\nVasilatos, C.; Alam, M.; Rahwan, T.; Zaki, Y .; and Mani-\natakos, M. 2023. HowkGPT: Investigating the Detection of\nChatGPT-generated University Student Homework through\nContext-Aware Perplexity Analysis. arXiv:2305.18226.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Met-\nzler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.;\nDean, J.; and Fedus, W. 2022. Emergent Abilities of Large\nLanguage Models. arXiv:2206.07682.\nYouden, W. J. 1950. An index for rating diagnostic tests.\nCancer, 32–35.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21266",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.5901055932044983
    },
    {
      "name": "Sociology",
      "score": 0.4690212309360504
    },
    {
      "name": "Psychology",
      "score": 0.3398400545120239
    },
    {
      "name": "Computer science",
      "score": 0.3312832713127136
    },
    {
      "name": "History",
      "score": 0.20986947417259216
    },
    {
      "name": "Archaeology",
      "score": 0.06504929065704346
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114531698",
      "name": "Tokyo Institute of Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ],
  "cited_by": 27
}