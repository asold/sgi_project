{
    "title": "Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT",
    "url": "https://openalex.org/W3087621941",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2004970051",
            "name": "Alexandra Chronopoulou",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2221112761",
            "name": "Dario Stojanovski",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2099684778",
            "name": "Alexander Fraser",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2025768430",
        "https://openalex.org/W2963047628",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2994475016",
        "https://openalex.org/W2913659301",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2963602293",
        "https://openalex.org/W2962824887",
        "https://openalex.org/W2963633299",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W2948902769",
        "https://openalex.org/W2887920589",
        "https://openalex.org/W2970925270",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2952614664",
        "https://openalex.org/W3016145922",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2963347649"
    ],
    "abstract": "Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on the high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2703–2711,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n2703\nReusing a Pretrained Language Model\non Languages with Limited Corpora for Unsupervised NMT\nAlexandra Chronopoulou, Dario Stojanovski, Alexander Fraser\nCenter for Information and Language Processing, LMU Munich, Germany\n{achron, stojanovski, fraser}@cis.lmu.de\nAbstract\nUsing a language model ( LM) pretrained on\ntwo languages with large monolingual data in\norder to initialize an unsupervised neural ma-\nchine translation ( UNMT ) system yields state-\nof-the-art results. When limited data is avail-\nable for one language, however, this method\nleads to poor translations. We present an ef-\nfective approach that reuses an LM that is\npretrained only on a high-resource language.\nThe monolingual LM is ﬁne-tuned on both lan-\nguages and is then used to initialize a UNMT\nmodel. To reuse the pretrained LM, we have\nto modify its predeﬁned vocabulary, to ac-\ncount for the new language. We therefore\npropose a novel vocabulary extension method.\nOur approach, RE-LM, outperforms a com-\npetitive cross-lingual pretraining model (XLM )\nin English-Macedonian (En-Mk) and English-\nAlbanian (En-Sq), yielding more than +8.3\nBLEU points for all four translation directions.\n1 Introduction\nNeural machine translation ( NMT ) has recently\nachieved remarkable results (Bahdanau et al., 2015;\nVaswani et al., 2017), based on the exploitation of\nlarge parallel training corpora. Such corpora are\nonly available for a limited number of languages.\nUNMT has attempted to address this limitation by\ntraining NMT systems using monolingual data only\n(Artetxe et al., 2018; Lample et al., 2018). Top\nperformance is achieved using a bilingual masked\nlanguage model (Devlin et al., 2019) to initial-\nize a UNMT encoder-decoder system (Lample and\nConneau, 2019). The model is then trained us-\ning denoising auto-encoding (Vincent et al., 2008)\nand back-translation (Sennrich et al., 2016a). The\napproach was mainly evaluated by translating be-\ntween high-resource languages.\nTranslating between a high-resource and a low-\nresource language is a more challenging task. In\nthis setting, the UNMT model can be initialized with\na pretrained cross-lingual LM. However, training\nthis UNMT model has been shown to be ineffective\nwhen the two languages are not related (Guzmán\net al., 2019). Moreover, in order to use a pretrained\ncross-lingual LM to initialize a UNMT model, the\ntwo models must have a shared vocabulary. Thus,\na bilingual LM needs to be trained from scratch for\neach language pair, before being transferred to the\nUNMT model (e.g. En-De LM for En-De UNMT ).\nMotivated by these issues, we focus on the ques-\ntion: how can we accurately and efﬁciently trans-\nlate between a high-monolingual-resource (HMR )\nand a low-monolingual-resource (LMR ) language?\nTo address this question, we adapt a monolingual\nLM, pretrained on an HMR language to an LMR\nlanguage, in order to initialize a UNMT system.\nWe make the following contributions: (1) We\npropose REused-LM1 (RE-LM), an effective trans-\nfer learning method for UNMT . Our method reuses\na pretrained LM on an HMR language, by ﬁne-\ntuning it on both LMR and HMR languages. The\nﬁne-tuned LM is used to initialize a UNMT system\nthat translates the LMR to the HMR language (and\nvice versa). (2) We introduce a novel vocabulary\nextension method, which allows ﬁne-tuning a pre-\ntrained LM to an unseen language. (3) We show that\nRE-LM outperforms a competitive transfer learning\nmethod (XLM ) for UNMT on three language pairs:\nEnglish-German (En-De) on a synthetic setup, En-\nMk and En-Sq. (4) We show thatRE-LM is effective\nin low-resource supervised NMT . (5) We conduct\nan analysis of ﬁne-tuning schemes for RE-LM and\nﬁnd that including adapters (Houlsby et al., 2019)\nin the training procedure yields almost the same\nUNMT results as RE-LM at a lower computational\nprice. We also run experiments to identify the con-\ntribution of the vocabulary extension method.\n1We release the code in https://github.com/\nalexandra-chron/relm_unmt.\n2704\nProjection Layer\nLayer k\nLayer 1\nEmbHMR \n... \nLayer k\nLayer 1\n... \nProjection Layer\nEmbH M R EmbL M R \nEncoder\nDecoder\nAttention\n(A) (B) (C) \nFigure 1: RE-LM. (A) LM pretraining. (B) Fine-tuning.\nThe embedding and the projection layer are extended\nusing §3.2 (dark gray) and (C) Transfer to an NMT sys-\ntem. Dashed arrows indicate transfer of weights.\n2 Related Work\nTransfer learning for UNMT . The ﬁeld of UNMT\nhas recently experienced tremendous progress.\nArtetxe et al. (2018); Lample et al. (2018) train\nUNMT models with monolingual data only, using\ndenoising auto-encoding (Vincent et al., 2008) and\nonline back-translation (Sennrich et al., 2016a) as\ntraining objectives. This approach is successful\nfor languages with high-quality, large, comparable\ndata. When these conditions are not met, though,\nUNMT provides near-zero scores (Neubig and Hu,\n2018). UNMT is further improved when initialized\nwith a cross-lingual pretrained model, trained on\nlarge corpora (Lample and Conneau, 2019; Song\net al., 2019). However, many languages have only\nlimited monolingual data available, a setting where\nUNMT is not effective (Guzmán et al., 2019). Sun\net al. (2020), whose work is close to our work in\nmotivation, train a UNMT model for an HMR -LMR\nlanguage pair. Iteratively, every subset (e.g. 10%)\nof HMR and all LMR data is backtranslated and the\npseudo-parallel corpus is added to the training pro-\ncess. Just like XLM , this training procedure needs\nto run from scratch for every new language pair.\nBy contrast, our method ﬁne-tunes a monolingual\npretrained LM for UNMT , so it is computationally\nfaster and simpler.\nVocabulary. Transferring a pretrained model\n(source) to a new model (target) requires the use of\na shared vocabulary (Nguyen and Chiang, 2017).\nKim et al. (2019) propose a linear alignment of\nthe source and target model embeddings using an\nunsupervised dictionary. However, when the em-\nbeddings of the two models do not have enough\noverlapping strings, dictionary induction might fail\n(Søgaard et al., 2018). Lakew et al. (2018) transfer\na source NMT model to a target NMT model (e.g.\nDe-En to Nl-En). To enable transfer, they overwrite\nthe source vocabulary with the target vocabulary.\nBy contrast, we keep the union of the two vocabu-\nlaries. We ﬁne-tune a pretrained monolingual LM\nto an LMR language, to initialize an NMT model.\nThus, we need the vocabularies of both languages.\nAdapters. Residual adapters (Houlsby et al., 2019)\nare feed-forward networks, added to each of to the\noriginal model’s layers. During ﬁne-tuning, the\nmodel parameters are frozen and only the adapters\nare ﬁne-tuned. This can prevent catastrophic for-\ngetting (Goodfellow et al., 2014; Bapna and Fi-\nrat, 2019). Adapters show promising results in\ndomain adaptation (Bapna and Firat, 2019) and\ncross-lingual classiﬁcation (Artetxe et al., 2020).\nMotivated by this, we study the use of adapters\nduring LM ﬁne-tuning in our analysis.\n3 Proposed Approach\nWe describe our method for translation between a\nhigh-resource (HMR ) and a low-resource language\n(LMR ) using monolingual data in this section.\n3.1 RE-LM\nOur proposed approach consists of three steps, as\nshown in Figure 1:\n(A) We train a monolingual maskedLM on the HMR\nlanguage, using all available HMR corpora. This\nstep needs to be performed only once for the HMR\nlanguage. Note that a publicly available pretrained\nmodel could also be used.\n(B) To ﬁne-tune the pretrained LM on the LMR\nlanguage, we ﬁrst need to overcome the vocabulary\nmismatch problem. Fine-tuning without extending\nthe vocabulary is detrimental, as we will show later\nin the analysis. We therefore extend the vocabulary\nof the pretrained model using our proposed method,\ndescribed in §3.2.\n(C) Finally, we initialize an encoder-decoderUNMT\nsystem with the ﬁne-tuned LM. The UNMT model\nis trained using denoising auto-encoding and online\nback-translation for the HMR -LMR language pair.\nFigure 2: Segmentations of Albanian (Sq). We observe\nthat splitting Sq using En BPEs (BPE HMR) results in\nheavily segmented tokens. This problem is alleviated\nusing BPEjoint tokens, learned on both languages.\n2705\n3.2 Vocabulary Extension\nWe propose a novel method that enables adapting a\npretrained monolingual LM to an unseen language.\nWe consider the case of an LM pretrained on an\nHMR language. The training data is split using\nByte-Pair-Encoding (BPE) (Sennrich et al., 2016b).\nWe denote these BPE tokens as BPEHMR and the\nresulting vocabulary as VHMR. We aim to ﬁne-tune\nthe trained LM to an unseen LMR language. Split-\nting the LMR language with BPEHMR tokens would\nresult in heavy segmentation of LMR words (Figure\n2). To counter this, we learn BPEs on the joint\nLMR and HMR corpus (BPEjoint). We then use\nBPEjoint tokens to split the LMR data, resulting in\na vocabulary VLMR. This technique increases the\nnumber of shared tokens and enables cross-lingual\ntransfer of the pretrained LM. The ﬁnal vocabulary\nis the union of the VHMR and VLMR vocabularies.\nWe extend the input and output embedding layer\nto account for the new vocabulary items. The new\nparameters are then learned during ﬁne-tuning.\n4 Experimental Setup\nDatasets. We experiment with two setups. In the\nﬁrst synthetic setup we use En-De. We sample 8M\nEn sentences from NewsCrawl. To simulate an\nLMR language, we gradually sample 0.05M, 0.5M\nand 1M De sentences. We use the WMT dev/test\nsets (Bojar et al., 2016). The second, real-world\nsetup is En-Mk, En-Sq. We use 68M En sentences\nfrom NewsCrawl. For Mk and Sq, we use2.4M Mk\nand 4M Sq, obtained from OSCAR2 (Ortiz Suárez\net al., 2019) and Wikipedia. We randomly select\n3K sentences from SETIMES 3 as dev and 3K as\ntest set. We tokenize data with standard Moses\n(Koehn et al., 2006) scripts. For the low-resource\nsupervised case, we sample 10K, 100K, and 200K\nparallel sentences from SETIMES for Mk and Sq.\nPreprocessing. We train a standard XLM model\n(Lample and Conneau, 2019) as a baseline using\n32K BPE merge operations, learned on the concate-\nnation of sentences sampled randomly from the\ncorpora of each language pair with α = 0.5. For\nRE-LM, we learn 32K BPEs on the HMR corpus\nand extract the initial vocabulary ( VHMR). Then,\nwe learn 32K BPEs on the joint LMR and HMR\ncorpus (BPEjoint). We extend the initial VHMR vo-\ncabulary by the amount of LMR vocabulary items\nthat are not already present in VHMR. To identify\n2https://oscar-corpus.com/\n3http://opus.nlpl.eu/SETIMES.php\nwhether a smaller number of BPE merges would be\nuseful for splitting the LMR language, we conduct\nexperiments varying their number in the analysis.\nModel Conﬁguration. RE-LM is built using the\nXLM codebase4. Each masked LM has a Trans-\nformer architecture with 1024 hidden units, 6 lay-\ners and 8 attention heads. Each NMT model is a\n6-layer encoder-decoder Transformer with 1024\nhidden units and 8 heads. Each LM is trained us-\ning Adam (Kingma and Ba, 2015) with learning\nrate 10−4 and masking follows Devlin et al. (2019).\nDuring UNMT and supervised NMT training, Adam\nwith inverse square root scheduling and a learning\nrate of 10−4 is used. We evaluate NMT models on\nthe dev set every 3000 updates using greedy de-\ncoding. The En LM and each XLM are trained on\n8 NVIDIA GTX 11 GB GPUs for 1 week, with a\nper-GPU batch size of 32. LM ﬁne-tuning and NMT\ntraining models are computationally efﬁcient, using\njust 1 GPU and 32 batch size. We assume that by\nﬁne-tuning the LM on 8 GPUs, we could get even\nbetter results. Final translations are generated us-\ning beam search of size 5. We report de-tokenized\nBLEU using SacreBLEU (Post, 2018)5.\nExperiments. For unsupervised translation, we\ntrain a randomly initialized UNMT model for each\nlanguage pair as a ﬁrst baseline. As a transfer learn-\ning baseline, we use XLM (Lample and Conneau,\n2019), trained on the two languages and transferred\nto a UNMT model. The UNMT models are trained\nusing monolingual data. For supervised transla-\ntion, NMT training is performed using only par-\nallel corpora, without ofﬂine back-translation of\nmonolingual data. The ﬁrst baseline is a randomly\ninitialized NMT system. The second baseline is\nan NMT model initialized with XLM . We compare\nthem to our proposed approach, RE-LM. Both XLM\nand RE-LM are trained on the monolingual corpora\nof both languages of interest. In the analysis, we\nadd adapters (Rebufﬁ et al., 2018) of hidden size\n256 after each self-attention and each feed-forward\nlayer of the pretrained monolingual LM. We freeze\nthe parameters of the pretrained LM and ﬁne-tune\nonly the adapters and the embedding layer.\n5 Results and Analysis\n5.1 Unsupervised Translation\nTable 1 presents our UNMT results, comparing ran-\ndom initialization, XLM and RE-LM.\n4github.com/facebookresearch/XLM/\n5Signature “BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.9”\n2706\nHMR -LMR language pair En-De En-De En-De En-Mk En-Sq\nsize of LMR language 0.05M 0.5M 1M 2.4M 4M\n← → ← → ← → ← → ← →\nrandom 3.9 4.9 3.4 2.6 4.2 4.1 3.5 3.0 6.6 5.6\nXLM 8.1 6.4 19.8 16.0 21.7 18.1 12.2 12.8 16.3 18.8\nRE-LM 10.7 7.5 22.6 19.0 24.3 21.9 22.0 21.1 27.6 28.1\nTable 1: UNMT BLEU scores. The ﬁrst column indicates the pretraining method used. Left arrow ( ←) refers to\ntranslation from the LMR language to En, while right arrow (→) refers to translation from En to theLMR language.\nSynthetic setup. We observe that RE-LM consis-\ntently outperforms XLM . Using 50K De sentences,\nRE-LM has small gains over XLM (+1.1 BLEU in\nEn→De). However, when we scale to slightly more\ndata (500K), the performance of RE-LM is clearly\nbetter than the one of XLM , with +3 En→De BLEU\ngains. With 1M De data, our model surpasses the\nXLM by more than 2.6 BLEU in both directions.\nReal-world setup. Our approach surpasses XLM\nin both language pairs. We observe that RE-LM\nachieves at least +8.3 BLEU over XLM for En-Mk.\nOur model was ﬁrst pretrained on En and then\nﬁne-tuned on both En and Mk. Therefore, it has\nprocessed all En and Mk sentences, obtaining a\ngood cross-lingual representation. However, XLM\nis jointly trained on En and Mk. As a result, it\noverﬁts Mk before processing all En data. RE-\nLM is similarly effective for En-Sq, achieving an\nimprovement of at least +9.3 BLEU over XLM .\nSynthetic vs Real-world setup. The effectiveness\nof RE-LM is pronounced in the real-world setup.\nWe identify two potential reasons. First, for En-\nDe, 8M En is used for LM pretraining, while for\nEn-Mk and En-Sq, 68M En is used. When XLM is\ntrained on imbalanced HMR -LMR data, it overﬁts\nthe LMR language. This is more evident for the En-\nMk (or En-Sq) than for the En-De XLM , perhaps\ndue to the larger data imbalance. Second, in En-\nDe, we use high-quality corpora for both languages\n(NewsCrawl), whereas Mk and Sq are trained on\nlow-quality CommonCrawl data. The fact that RE-\nMLM outperforms XLM for Mk and Sq shows that\nit is more robust to noisy data than the XLM .\n5.2 Low-Resource Supervised Translation\nWe sample10K, 100K and 200K of En-Mk and En-\nSq bi-text and train supervised NMT systems. We\ncompare XLM , RE-LM and random, an NMT model\ntrained from scratch. We observe (Table 2) that RE-\nLM consistently outperforms the baselines when\ntrained on 100K or less for En-Mk and En-Sq. Us-\ning 200K, though, RE-LM yields the same results\nas XLM . We hypothesize that this happens because\nparallel languages En-Mk En-Sq\ndirection ← → ← →\n10K\nrandom 23.4 23.7 25.5 18.9\nXLM 38.7 38.7 44.7 41.4\nRE-LM 40.1 38.9 45.7 42.8\n100K\nrandom 48.4 48.2 51.8 37.4\nXLM 53.7 53.2 57.1 52.0\nRE-LM 54.8 53.4 58.1 52.9\n200K\nrandom 51.3 51.2 55.6 51.4\nXLM 55.0 55.5 60.9 55.1\nRE-LM 55.2 55.3 61.1 54.8\nTable 2: BLEU scores on the dev set using increasing\namounts of parallel data. We show in bold the models\nthat achieve at least +1 BLEU compared to XLM .\nSETIMES is a homogeneous domain. Thus, train-\ning an NMT model with 200K is sufﬁcient for com-\npetitive results, so both pretraining models provide\nsimilar improvements over random.\n5.3 Analysis\nWe experiment with different ﬁne-tuning schemes\nand show results in Table 3. Then, we vary the\nnumber of BPE merges used to split the LMR lan-\nguage using the vocabulary extension method and\nalso show experiments where this method is not\nused at all. The results are presented in Table 4.\nRE-LM. In Table 3, we compare ﬁne-tuning an\nLM only on the LMR language to ﬁne-tuning it on\nboth the HMR and LMR language (rows 1 and 2).\nFine-tuning only on the LMR language provides\nworse BLEU scores because of catastrophic forget-\nting. The negative effect is clear for Mk and Sq,\nwhere ﬁne-tuning only on the LMR results in worse\nBLEU scores than random initialization, shown in\nTable 1. For De, the effect is smaller, perhaps be-\ncause En and De are very similar languages.\nAdapters. We insert adapters to the pretrained LM\nand ﬁne-tune only the adapter and embedding layer.\nWe use the ﬁne-tuned LM to initialize a UNMT sys-\ntem. Adapters are used for both translation direc-\ntions during UNMT training. Results are presented\nin Table 3. Fine-tuning the LM only on the LMR\nlanguage yields at least +3.9 BLEU for En-Sq com-\n2707\nHMR -LMR language pair En-De En-De En-De En-Mk En-Sq\nsize of LMR language 0.05M 0.5M 1M 2.4M 4M\n← → ← → ← → ← → ← →\nLM\nft on LMR 9.4 7.3 20.4 16.8 20.6 17.8 2.7 2.4 4.7 4.7\nft on LMR & HMR (RE-LM) 10.7 7.5 22.6 19.0 24.3 21.9 22.0 21.1 27.6 28.1\n+ adapters ft on LMR (adapter RE-LM) 9.8 7.5 21.3 18.3 23.7 20.0 21.6 19.0 30.2 29.4\n+ adapters ft on LMR & HMR 9.2 7.1 20.6 18.0 23.4 19.9 21.6 20.3 24.6 25.5\nTable 3: Comparison of UNMT BLEU scores obtained using different ﬁne-tuning schemes of the pretrained mono-\nlingual LM. LM refers to the pretrained LM (on HMR data), while ft refers to ﬁne-tuning.\npared to ﬁne-tuning on both (rows 3, 4). En and\nSq are not similar languages and their embeddings\nalso differ. Thus, ﬁne-tuning on both is not help-\nful. By contrast, ﬁne-tuning only on Sq preserves\nthe pretrained model’s knowledge, while adapters\nare trained to encode Sq. For En-De and En-Mk,\nboth approaches provide similar results. En and\nMk do not share an alphabet, so their embeddings\ndo not overlap and both ﬁne-tuning methods are\nequally effective. In En-De, ﬁne-tuning only on De\nis marginally better than ﬁne-tuning on both. We\nhighlight that adapters allow parameter-efﬁcient\nﬁne-tuning. Adapter RE-LM reaches almost the\nsame results as RE-LM, using just a fraction of the\nRE-LM parameters while ﬁne-tuning. Details can\nbe found in the appendix.\nEn-De En-Mk En-Sq\nBPEjoint 0.5M 2.4M 4M\nmerges → ← → ← → ←\n- 8.1 8.0 6.1 6.4 7.2 7.6\n8K 8.3 10.2 14.3 17.3 18.1 16.4\n16K 8.7 14.6 14.9 20.2 27.1 25.5\n32K 22.6 19.0 22.0 21.1 27.6 28.1\nTable 4: UNMT BLEU scores obtained with RE-LM,\nwith (rows 2-4) and without (row 1) extending the vo-\ncabulary of the pretrained LM (VHMR). When extend-\ning the vocabulary, we vary the number of BPE joint\nmerges used to split the LMR data. We note that 32K\nBPEs are used to split the HMR data (BPEHMR).\nBPEjoint new vocabulary items\nmerges Mk Sq De\n8K 5K 5K 0.6K\n16K 10K 10K 2K\n32K 19K 20K 19K\nTable 5: Statistics of the vocabulary extension method.\nWe split the LMR corpus using 8K, 16K, or 32K BPE\nmerges and report the number of new vocabulary items.\nVocabulary Extension. In order to use RE-LM,\nwe extend the vocabulary of each language, as de-\nscribed in §3.2. The intuition is that, since the\npretrained monolingual LM uses BPEs learned ex-\nclusively on the HMR language, these BPEs would\nnot split the LMR corpus in a meaningful way. We\nconduct experiments to clarify the contribution of\nthe vocabulary extension, presented in Table 4. In\nTable 5, we present the amount of vocabulary items\nadded for each of our experimental setups.\nWithout vocabulary extension, the results are\npoor. This is expected, as in the case of Mk for ex-\nample, the HMR language (En) uses Latin alphabet,\nwhereas Mk uses Cyrillic. If the vocabulary of Mk\nis not taken into account, the UNMT model cannot\nprovide accurate results. The same applies for Sq\nand De. We hypothesize that, even though these\nlanguages use Latin script, a lot of their words do\nnot appear in En, therefore extending the initial\nvocabulary to include them is crucial. Using vocab-\nulary extension, we experiment with learning 8K,\n16K or 32K BPEs on the joint corpus. We then use\nthem to split the LMR data. We observe in Table\n4 that even using only 8K BPEs, there is a large\nimprovement in Mk and Sq (more than +8 BLEU).\nFor En-De, the improvement is negligible. This\nmight be the case because, as Table 5 shows, using\n8K merges, only 600 items are added to the initial\nvocabulary, which are not sufﬁcient for represent-\ning De language. This setup for En-De is in fact\nvery similar to not employing vocabulary extension.\nWe notice that adding more vocabulary items (us-\ning more BPE merge operations) is helpful for all\nlanguage pairs, providing improved BLEU scores.\n6 Conclusions\nTraining competitive unsupervised NMT models\nfor HMR -LMR scenarios is important for many real\nlow-resource languages. We proposed RE-LM, a\nnovel approach that ﬁne-tunes a high-resource LM\non a low-resource language and initializes an NMT\nmodel. RE-LM outperformed a strong baseline in\nUNMT , while also improving translations on a low-\nresource supervised setup. In future work, we will\napply our method to languages with corpora from\ndiverse domains and also to other languages.\n2708\nAcknowledgments\nThis project has received funding from the Euro-\npean Research Council under the European Union’s\nHorizon 2020 research and innovation program\n(grant agreement № 640550). This work was also\nsupported by DFG (grant FR 2829/4-1). We thank\nKaterina Margatina and Giorgos Vernikos for their\nvaluable comments and help with the ﬁrst draft of\nthis paper.\nReferences\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018. Unsupervised neural ma-\nchine translation. In International Conference on\nLearning Representations.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In International Con-\nference on Learning Representations.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing and the International\nJoint Conference on Natural Language Processing ,\npages 1538–1548.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aurélie\nNévéol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the conference on ma-\nchine translation. In Proceedings of the Conference\non Machine Translation, pages 131–198.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 4171–4186.\nIan J. Goodfellow, Mehdi Mirza, Xia Da, Aaron C.\nCourville, and Yoshua Bengio. 2014. An empirical\ninvestigation of catastrophic forgetting in gradient-\nbased neural networks. In International Conference\non Learning Representations.\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nﬂores evaluation datasets for low-resource machine\ntranslation: Nepali–english and sinhala–english. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing and the Inter-\nnational Joint Conference on Natural Language Pro-\ncessing, pages 6100–6113.\nDan Hendrycks and Kevin Gimpel. 2017. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. ArXiv.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn Proceedings of the International Conference on\nMachine Learning.\nYunsu Kim, Yingbo Gao, and Hermann Ney. 2019.\nEffective cross-lingual transfer of neural machine\ntranslation models without shared vocabularies. In\nProceedings of the Annual Meeting of the Associ-\nation for Computational Linguistics , pages 1246–\n1257.\nDiederick P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nPhilipp Koehn, Marcello Federico, Wade Shen,\nNicola Bertoldi, Ondrej Bojar, Chris Callison-Burch,\nBrooke Cowan, Chris Dyer, Hieu Hoang, Richard\nZens, et al. 2006. Open source toolkit for statisti-\ncal machine translation: Factored translation models\nand confusion network decoding. In Final Report of\nthe 2006 JHU Summer Workshop.\nSurafel Melaku Lakew, Aliia Erofeeva, Matteo Negri,\nMarcello Federico, and Marco Turchi. 2018. Trans-\nfer learning in multilingual neural machine trans-\nlation with dynamic vocabulary. In International\nWorkshop on Spoken Language Translation.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , page\n7057–7067.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018. Unsupervised ma-\nchine translation using monolingual corpora only.\nIn International Conference on Learning Represen-\ntations.\nGraham Neubig and Junjie Hu. 2018. Rapid adaptation\nof neural machine translation to new languages. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing , pages 875–\n880.\nToan Q. Nguyen and David Chiang. 2017. Transfer\nlearning across low-resource, related languages for\n2709\nneural machine translation. In Proceedings of the In-\nternational Joint Conference on Natural Language\nProcessing, pages 296–301.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous Pipeline for Process-\ning Huge Corpora on Medium to Low Resource In-\nfrastructures. In Workshop on the Challenges in the\nManagement of Large Corpora.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems ,\npages 8024–8035.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Conference on Ma-\nchine Translation: Research Papers , pages 186–\n191.\nOﬁr Press and Lior Wolf. 2017. Using the output\nembedding to improve language models. In Pro-\nceedings of the Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 157–163.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2018. Efﬁcient parametrization of multi-\ndomain deep neural networks. In IEEE Conference\non Computer Vision and Pattern Recognition, pages\n8119–8127.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics, pages 86–96.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1715–1725.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 778–788.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. In Proceed-\nings of the International Conference on Machine\nlearning, pages 5926–5936.\nHaipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama,\nEiichiro Sumita, and Tiejun Zhao. 2020. Self-\ntraining for unsupervised neural machine transla-\ntion in unbalanced training data scenarios. arXiv\npreprint arXiv:2004.04507.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, page 5998–6008.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the International Confer-\nence on Machine Learning, pages 1096–1103.\n2710\nA Appendix\nA.1 Vocabulary Extension\nWe provide more examples of different segmenta-\ntions of Sq, De and Mk using either theBPEHMR or\nthe BPEjoint tokens in Figure 3. We observe that,\nas expected, the Mk sentence is split to the charac-\nter level, as it uses a different alphabet (Cyrillic)\nthan the one that the BPEHMR tokens were learned\non (Latin).\nFigure 3: Segmentation of Sq, De and Mk using\nBPEHMR or BPE joint tokens. Using BPE HMR tokens\nresults in heavily split words.\nA.2 Datasets\nWe report that we remove sentences longer than\n100 words after BPE splitting. We split the data\nusing the fastBPE codebase6.\nA.3 Model Conﬁguration\nWe tie the embedding and output (projection) lay-\ners of both LM and NMT models (Press and Wolf,\n2017). We use a dropout rate of0.1 and GELU acti-\nvations (Hendrycks and Gimpel, 2017). We use the\ndefault parameters of Lample and Conneau (2019)\nin order to train our models unless otherwise spec-\niﬁed. We do not tune the hyperparameters. The\ncode was built with PyTorch (Paszke et al., 2019)\non top of the XLM implementation7. This code was\nused for LM pretraining, LM ﬁne-tuning, UNMT\ntraining, and NMT training.\nLM conﬁguration and training details. RE-LM\napproach pretrains a monolingual language model\nwhereas the XLM approach pretrains a bilingual\nlanguage model. We obtain a checkpoint every\n200K sentences processed by the model. We train\n6https://github.com/glample/fastBPE\n7https://github.com/facebookresearch/XLM/\neach LM using as criterion the validation perplexity\non the LMR language, with a patience of 10.\nThe training details of the two pretraining meth-\nods are presented here:\n• The monolingual LM pretraining required 1\nweek, 8 GPUs and had 137M parameters.\n• The XLM pretraining required 1 week, in 8\nGPUs. The total number of trainable parame-\nters is 138M.\nOur approach also requires an LM ﬁne-tuning step.\nThe runtimes, parameters and GPU details are\nshown in Table 6 under RE-LM ft column. The\nruntimes mentioned refer to the En-Mk language\npair. We note that the LM ﬁne-tuning step is a lot\nfaster than performing XLM pretraining for each\nlanguage pair (note that pretraining ran on 8 GPUs,\nwhereas ﬁne-tuning on a single GPU).\nNMT conﬁguration and training details. The\nparameters and runtimes of the UNMT models we\nused are shown in Table 6 under UNMT columns.\nLikewise, the details of supervisedNMT models are\nshown under sup NMT columns. We get a check-\npoint every 50K sentences processed by the model.\nRegarding the adapter RE-LM training procedure,\nwe note that, different from Houlsby et al. (2019);\nBapna and Firat (2019), we also freeze the layer\nnormalization (Ba et al., 2016) parameters, without\nintroducing new ones.\nA.4 Validation Scores of Results\nIn Tables 7 and 8 we show the dev scores of the\nmain results of our proposed approach ( RE-LM)\ncompared to the baselines. These Tables extend\nTable 1 of the main paper.\nIn Tables 9 and 10, we show the dev scores of\nthe extra ﬁne-tuning experiments we did for the\nanalysis. The Tables correspond to Table 3 of the\nmain paper.\nWe note that the dev scores are obtained using\ngreedy decoding, while the test scores are obtained\nwith beam search of size 5. We clarify that we\ntrain each NMT model using as training criterion\nthe validation BLEU score of the LMR →HMR di-\nrection, with a patience of 10. We speciﬁcally use\nmulti-bleu.perl script from Moses.\n2711\nXLM RE-LM adapter RE-LM random\nUNMT sup NMT ft UNMT sup NMT ft UNMT UNMT sup NMT\nparams 223M 223M 156M 258M 258M 88M 270M 258M 258M\nruntime 48h 10h 60h 72h 10h 44h 20h 18h 15h\nTable 6: Parameters and training runtimes used for each experiment. We note that each of the experiments ran on\na single GPU. ft refers to the ﬁne-tuning of the pretrained monolingual LM. Adapter RE-LM refers to the addition\nof adapters to the LM and the UNMT model.\nlanguages En-De\nsize of LMR 0.05M 0.5M 1M\n← → ← → ← →\ndev test dev test dev test dev test dev test dev test\nrandom 3.2 3.9 4.1 4.9 2.5 3.4 2.3 2.6 3.7 4.2 3.5 4.1\nXLM 5.6 8.1 4.8 6.4 14.5 19.8 12.0 16.0 17.4 21.7 14.6 18.1\nRE-LM 7.4 10.7 4.1 7.5 16.2 22.6 13.8 19.0 17.8 24.3 16.3 21.9\nTable 7: Unsupervised NMT results with dev scores. The ﬁrst column indicates the pretraining method used.\nRandom refers to random initialization, while XLM refers to the method of Lample and Conneau (2019) and RE-\nLM to our proposed approach.\nsize of LMR 2.4M 4M\nMk→En En →Mk Sq→En En →Sq\ndev test dev test dev test dev test\nrandom 3.1 3.5 3.0 3.0 5.8 6.6 5.6 5.6\nXLM 11.8 12.2 12.6 12.8 15.5 16.3 17.3 18.8\nRE-LM 22.0 22.0 19.5 21.1 27.2 27.6 27.6 28.1\nTable 8: Unsupervised NMT BLEU scores with corresponding dev scores for En-Mk, En-Sq.\nlanguages En-De\nsize of LMR 0.05M 0.5M 1M\n← → ← → ← →\ndev test dev test dev test dev test dev test dev test\nLM\nft LMR 6.8 9.4 5.2 7.3 15.1 20.4 12.9 16.8 15.3 20.6 13.3 17.8\nft both (RE-LM) 7.4 10.7 4.1 7.5 16.2 22.6 13.8 19.0 17.8 24.3 16.3 21.9\n+ adapter RE-LM 6.8 9.8 4.8 7.5 15.1 21.3 13.4 18.3 16.9 23.7 15.2 20.0\n+ adapters ft both 6.7 9.2 4.1 7.1 14.8 20.6 13.0 18.0 17.1 23.4 15.0 19.9\nTable 9: Comparison of UNMT BLEU scores obtained using different ﬁne-tuning schemes of the pretrained mono-\nlingual LM with corresponding dev scores for En-De. LM refers to the pretrained LM, trained on HMR data, while\nft refers to ﬁne-tuning. ft both means ﬁne-tuning on the LMR and the HMR language.\nsize of LMR 2.4M 4M\nMk→En En →Mk Sq→En En →Sq\ndev test dev test dev test dev test\nLM\nft LMR 2.6 2.7 2.3 2.4 4.4 4.7 4.2 4.7\nft both (RE-LM) 22.0 22.0 19.5 21.1 27.2 27.6 27.6 28.1\n+ adapter RE-LM 21.4 21.6 20.0 19.0 29.8 30.2 29.3 29.4\n+ adapters ft both 22.7 21.6 22.2 20.3 24.4 24.6 25.4 25.5\nTable 10: Comparison of UNMT BLEU scores obtained using different ﬁne-tuning schemes of the pretrained mono-\nlingual LM with corresponding dev scores for En-Mk and En-Sq. LM refers to the pretrained LM, trained on HMR\ndata, while ft refers to ﬁne-tuning. ft both means ﬁne-tuning on the LMR and the HMR language."
}