{
    "title": "GLU Variants Improve Transformer",
    "url": "https://openalex.org/W3006439205",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4224379215",
            "name": "Shazeer, Noam",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2797328513",
        "https://openalex.org/W2767286248",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2427527485",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2156387975",
        "https://openalex.org/W2091812280"
    ],
    "abstract": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.",
    "full_text": "arXiv:2002.05202v1  [cs.LG]  12 Feb 2020\nGLU V ariants Improve T ransformer\nNoam Shazeer\nGoogle\nnoam@google.com\nF ebruary 14, 2020\nAbstract\nGated Linear Units [ Dauphin et al. , 2016] consist of the component-wise product of two linear pro-\njections, one of which is ﬁrst passed through a sigmoid funct ion. V ariations on GLU are possible, using\ndiﬀerent nonlinear (or even linear) functions in place of si gmoid. W e test these variants in the feed-\nforward sublayers of the T ransformer [ V aswani et al. , 2017] sequence-to-sequence model, and ﬁnd that\nsome of them yield quality improvements over the typically- used ReLU or GELU activations.\n1 Introduction\nThe T ransformer [ V aswani et al. , 2017] sequence-to-sequence model alternates between multi-he ad attention,\nand what it calls \"position-wise feed-forward networks\" (F FN). The FFN takes a vector x (the hidden repre-\nsentation at a particular position in the sequence) and pass es it through two learned linear transformations,\n(represented by the matrices W1 and W2 and bias vectors b1 and b2). A rectiﬁed-linear (ReLU) [ Glorot et al. ,\n2011] activation function applied between the two linear transf ormations.\nFFN(x, W 1 , W 2 , b 1, b 2) = max(0 , xW 1 + b1)W2 + b2 (1)\nF ollowing the T5 codebase [ Raﬀel et al. , 2019] 1, we use a version with no bias:\nFFNReLU (x, W 1 , W 2) = max( xW1 , 0)W2 (2)\nSubsequent work has proposed replacing the ReLU with other n onlinear activation functions such as\nGaussian Error Linear Units, GELU( x) = xΦ( x) [ Hendrycks and Gimpel , 2016], and Swish β (x) = xσ (βx )\n[Ramachandran et al. , 2017].\nFFNGELU(x, W 1 , W 2) = GELU( xW1 )W2\nFFNSwish (x, W 1 , W 2) = Swish 1(xW1 )W2\n(3)\n2 Gated Linear Units (GLU) and V ariants\n[\nDauphin et al. , 2016] introduced Gated Linear Units (GLU), a neural network laye r deﬁned as the component-\nwise product of two linear transformations of the input, one of which is sigmoid-activated. They also suggest\nomitting the activation, which they call a \"bilinear\" layer and attribute to [ Mnih and Hinton , 2007].\nGLU(x, W, V, b, c ) = σ (xW + b) ⊗ (xV + c)\nBilinear(x, W, V, b, c ) = ( xW + b) ⊗ (xV + c) (4)\nW e can also deﬁne GLU variants using other activation functi ons:\n1 Also in the interest of ML fairness.\n1\nReGLU(x, W, V, b, c ) = max(0 , xW + b) ⊗ (xV + c)\nGEGLU(x, W, V, b, c ) = GELU( xW + b) ⊗ (xV + c)\nSwiGLU(x, W, V, b, c, β ) = Swish β (xW + b) ⊗ (xV + c)\n(5)\nIn this paper, we propose additional variations on the T rans former FFN layer which use GLU or one of\nits variants in place of the ﬁrst linear transformation and t he activation function. Again, we omit the bias\nterms.\nFFNGLU (x, W, V, W 2 ) = ( σ (xW ) ⊗ xV )W2\nFFNBilinear (x, W, V, W 2 ) = ( xW ⊗ xV )W2\nFFNReGLU(x, W, V, W 2 ) = (max(0 , xW ) ⊗ xV )W2\nFFNGEGLU (x, W, V, W 2 ) = (GELU( xW ) ⊗ xV )W2\nFFNSwiGLU (x, W, V, W 2 ) = (Swish 1 (xW ) ⊗ xV )W2\n(6)\nAll of these layers have three weight matrices, as opposed to two for the original FFN. T o keep the\nnumber of parameters and the amount of computation constant , we reduce the number of hidden units df f\n(the second dimension of W and V and the ﬁrst dimension of W2 ) by a factor of 2\n3 when comparing these\nlayers to the original two-matrix version.\n3 Experiments on T ext-to-T ext T ransfer T ransformer (T5)\nW e test the FFN variants we have described on the transfer-le arning setup from [\nRaﬀel et al. , 2019]. An\nencoder-decoder transformer model [ V aswani et al. , 2017] is trained on a denoising objective of predicting\nmissing text segments, and subsequently ﬁne-tuned on vario us language understanding tasks.\n3.1 Model Architecture\nW e use the same code base, model architecture, and training t ask as the base model from [ Raﬀel et al. ,\n2019]. The encoder and decoder each consist of 12 layers, with dmodel = 768. F or the attention layers,\nh = 12 and dk = dv = 64. The FFN layers have hidden size df f = 3072. As we describe above, for the\nGLU-variant-based FFN layers, which have thee weight matri ces instead of two, we reduce the hidden layer\nto df f = 2048, so as to maintain the same parameter and operation cou nts as the base model.\nT able 1: Heldout-set log-perplexity for T ransformer model s on the segment-ﬁlling task from [ Raﬀel et al. ,\n2019]. All models are matched for parameters and computation.\nT raining Steps 65,536 524,288\nFFNReLU (baseline) 1.997 (0.005) 1.677\nFFNGELU 1.983 (0.005) 1.679\nFFNSwish 1.994 (0.003) 1.683\nFFNGLU 1.982 (0.006) 1.663\nFFNBilinear 1.960 (0.005) 1.648\nFFNGEGLU 1.942 (0.004) 1.633\nFFNSwiGLU 1.944 (0.010) 1.636\nFFNReGLU 1.953 (0.003) 1.645\n2\n3.2 Pre-T raining and Perplexity Results\nIdentically to [\nRaﬀel et al. , 2019], we pre-train for 524,288 steps on the span-ﬁlling objecti ve on the C4\ndataset. Each training batch consists of 128 examples, each of which has an input of 512 tokens and an\noutput of 114 tokens, the output containing multiple spans o f tokens which were deleted from the input 2 .\nSimilarly to [ Raﬀel et al. , 2019], we use the Adafactor optimizer [ Shazeer and Stern , 2018] and an inverse-\nsquare-root learning-rate schedule. W e also decay the lear ning rate linearly for the ﬁnal 10 percent of the\ntraining steps. Our main departure from [ Raﬀel et al. , 2019] is that we use no dropout during pre-training.\nW e ﬁnd this to produce superior results. W e compute the log-p erplexity on the training objective on a\nheldout shard of C4, which we believe to be a good indicator of model quality . F or each model architecture,\nwe also trained four models for a shorter period (65,536 step s) to measure inter-run variability . The results\nare listed in table 1. The GEGLU and SwiGLU variants produce the best perplexitie s.\n3.3 Fine-T uning\nW e then ﬁne-tune each fully-trained model once on an example s-proportional mixture of the Stanford\nQuestion-Answering Dataset (SQuAD) [ Rajpurkar et al. , 2016] and all the language understanding tasks\nin the GLUE [ W ang et al. , 2018] and SuperGlue [ W ang et al. , 2019] benchmarks. 3 Fine-tuning consists of\n131072 steps with a learning rate of 10 − 3. As in training, the input sequences for each step have a comb ined\nlength of approximately 65,536 tokens. F ollowing [ Raﬀel et al. , 2019], we use a dropout rate of 0 . 1 on the\nlayer outputs, feed-forward hidden-layers and attention w eights. The embedding matrices are ﬁxed during\nﬁne-tuning.\nT ables 2, 3 and 4 show results on the development sets. F or each task, we repor t the best score of any\nof the checkpoints recorded during ﬁne-tuning. While the re sults are noisy , the new GLU-variants perform\nbest on most of the tasks. F or comparison, at the bottom of eac h of the tables we list the reuslts from\n[Raﬀel et al. , 2019]. The model is identical to our FFN ReLU model. Their results are notably worse, which\nwe believe was caused by their use of dropout during pre-trai ning. Also listed are the inter-run standard\ndeviations measured by [ Raﬀel et al. , 2019].\nT able 2: GLUE Language-Understanding Benchmark [ W ang et al. , 2018] (dev).\nScore CoLA SST-2 MRPC MRPC STSB STSB QQP QQP MNLIm MNLImm QNLI R TE\nA verage MCC Acc F1 Acc PCC SCC F1 Acc Acc Acc Acc Acc\nFFNReLU 83. 80 51. 32 94 . 04 93.08 90.20 89. 64 89 . 42 89 . 01 91 . 75 85 . 83 86 . 42 92 . 81 80 . 14\nFFNGELU 83. 86 53. 48 94 . 04 92 . 81 90.20 89. 69 89 . 49 88 . 63 91 . 62 85 . 89 86 . 13 92 . 39 80 . 51\nFFNSwish 83. 60 49. 79 93 . 69 92 . 31 89 . 46 89 . 20 88 . 98 88 . 84 91 . 67 85 . 22 85 . 02 92 . 33 81 . 23\nFFNGLU 84. 20 49. 16 94 . 27 92 . 39 89 . 46 89 . 46 89 . 35 88 . 79 91 . 62 86 . 36 86 . 18 92 . 92 84.12\nFFNGEGLU 84. 12 53. 65 93 . 92 92 . 68 89 . 71 90 . 26 90 . 13 89 . 11 91 . 85 86 . 15 86 . 17 92 . 81 79 . 42\nFFNBilinear 83. 79 51. 02 94.38 92. 28 89 . 46 90 . 06 89 . 84 88 . 95 91 . 69 86.90 87.08 92. 92 81 . 95\nFFNSwiGLU 84. 36 51. 59 93 . 92 92 . 23 88 . 97 90.32 90.13 89.14 91.87 86. 45 86 . 47 92.93 83. 39\nFFNReGLU 84.67 56.16 94.38 92. 06 89 . 22 89 . 97 89 . 85 88 . 86 91 . 72 86 . 20 86 . 40 92 . 68 81 . 59\n[Raﬀel et al. , 2019] 83. 28 53. 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28\nibid. stddev. 0. 235 1. 111 0 . 569 0 . 729 1 . 019 0 . 374 0 . 418 0 . 108 0 . 070 0 . 291 0 . 231 0 . 361 1 . 393\n4 Conclusions\nW e have extended the GLU family of layers and proposed their u se in T ransformer. In a transfer-learning\nsetup, the new variants seem to produce better perplexities for the de-noising objective used in pre-training,\nas well as better results on many downstream language-under standing tasks. These architectures are simple\nto implement, and have no apparent computational drawbacks . W e oﬀer no explanation as to why these\narchitectures seem to work; we attribute their success, as a ll else, to divine benevolence.\n2 Each training step took approximately 0.15 seconds on a 32-c ore TPUv2 cluster.\n3 This departs from [ Raﬀel et al. , 2019], who ﬁne-tuned separately on the diﬀerent tasks. W e chose o ne ﬁne-tuning run for\nsimplicity .\n3\nT able 3: SuperGLUE Language-Understanding Benchmark [ W ang et al. , 2019] (dev).\nScore BoolQ CB CB CoP A MultiRC MultiRC ReCoRD ReCoRD R TE WiC WSC\nA verage Acc F1 Acc Acc F1 EM F1 EM Acc Acc Acc\nFFNReLU 72. 76 80. 15 83 . 37 89 . 29 70 . 00 76 . 93 39 . 14 73 . 73 72 . 91 83 . 39 67 . 71 77 . 88\nFFNGELU 72. 98 80. 64 86 . 24 91.07 74. 00 75 . 93 38 . 61 72 . 96 72 . 03 81 . 59 68 . 34 75 . 96\nFFNSwish 72. 40 80. 43 77 . 75 83 . 93 67 . 00 76 . 34 39 . 14 73 . 34 72 . 36 81 . 95 68 . 18 81 . 73\nFFNGLU 73. 95 80. 95 77 . 26 83 . 93 73 . 00 76 . 07 39 . 03 74 . 22 73 . 50 84 . 12 67 . 71 87.50\nFFNGEGLU 73. 96 81. 19 82 . 09 87 . 50 72 . 00 77.43 41.03 75. 28 74.60 83. 39 67 . 08 83 . 65\nFFNBilinear 73. 81 81.53 82. 49 89 . 29 76.00 76. 04 40 . 92 74 . 97 74 . 10 82 . 67 69.28 78. 85\nFFNSwiGLU 74.56 81. 19 82 . 39 89 . 29 73 . 00 75 . 56 38 . 72 75.35 74. 55 85.20 67. 24 86 . 54\nFFNReGLU 73. 66 80. 89 86.37 91.07 67. 00 75 . 32 40 . 50 75 . 07 74 . 18 84 . 48 67 . 40 79 . 81\n[Raﬀel et al. , 2019] 71. 36 76. 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56\nibid. stddev. 0. 416 0. 365 3 . 237 2 . 560 2 . 741 0 . 716 1 . 011 0 . 370 0 . 379 1 . 228 0 . 850 2 . 029\nT able 4: SQuAD [ Rajpurkar et al. , 2016] v1.1 (dev).\nEM F1\nFFNReLU 83. 18 90 . 87\nFFNGELU 83. 09 90 . 79\nFFNSwish 83. 25 90 . 76\nFFNGLU 82. 88 90 . 69\nFFNGEGLU 83. 55 91 . 12\nFFNBilinear 83.82 91. 06\nFFNSwiGLU 83. 42 91 . 03\nFFNReGLU 83. 53 91.18\n[Raﬀel et al. , 2019] 80. 88 88 . 81\nibid. Standard Deviation 0. 343 0 . 226\nReferences\nY ann N. Dauphin, Angela F an, Michael Auli, and David Grangie r. Language modeling with gated convolu-\ntional networks. CoRR, abs/1612.08083, 2016. URL\nhttp://arxiv.org/abs/1612.08083.\nXavier Glorot, Antoine Bordes, and Y oshua Bengio. Deep spar se rectiﬁer neural networks. In Proceedings\nof the fourteenth international conference on artiﬁcial in tel ligence and statistics , pages 315–323, 2011.\nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities an d stochastic regularizers with gaussian error\nlinear units. CoRR, abs/1606.08415, 2016. URL http://arxiv.org/abs/1606.08415.\nAndriy Mnih and Geoﬀrey Hinton. Three new graphical models f or statistical language modelling. In\nProceedings of the 24th international conference on Machin e learning , pages 641–648, 2007.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sha ran Narang, Michael Matena, Y anqi Zhou,\nW ei Li, and Peter Liu. Exploring the limits of transfer learn ing with a uniﬁed text-to-text transformer.\narXiv e-prints , 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Perc y Liang. Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint arXiv:1606.05250 , 2016.\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching f or activation functions. arXiv preprint\narXiv:1710.05941, 2017.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learn ing rates with sublinear memory cost. arXiv\npreprint arXiv:1804.04235 , 2018.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit , Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n4\nAlex W ang, Amapreet Singh, Julian Michael, F elix Hill, Omer Levy , and Samuel R. Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural la nguage understanding. arXiv preprint\narXiv:1804.07461, 2018.\nAlex W ang, Y ada Pruksachatkun, Nikita Nangia, Amanpreet Si ngh, Julian Michael, F elix Hill, Omer Levy ,\nand Samuel R. Bowman. Superglue: A stickier benchmark for ge neral-purpose language understanding\nsystems. arXiv preprint arXiv:1905.00537 , 2019.\n5"
}