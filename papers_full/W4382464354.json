{
  "title": "DQ-DETR: Dual Query Detection Transformer for Phrase Extraction and Grounding",
  "url": "https://openalex.org/W4382464354",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100794563",
      "name": "Liu Shi-long",
      "affiliations": [
        "Tsinghua University",
        "King Center",
        "Robert Bosch (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2643851808",
      "name": "Huang Shijia",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1967201970",
      "name": "Li Feng",
      "affiliations": [
        "Digital Science (United States)",
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1979946437",
      "name": "Zhang Hao",
      "affiliations": [
        "University of Hong Kong",
        "Digital Science (United States)",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2412358555",
      "name": "Liang Yao-yuan",
      "affiliations": [
        "Tsinghua‚ÄìBerkeley Shenzhen Institute",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2117703573",
      "name": "su hang",
      "affiliations": [
        "Tsinghua University",
        "King Center",
        "Robert Bosch (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2043742051",
      "name": "Zhu Jun",
      "affiliations": [
        "Robert Bosch (United States)",
        "Tsinghua University",
        "King Center"
      ]
    },
    {
      "id": "https://openalex.org/A1842339085",
      "name": "Zhang Lei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6809716307",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W6795736418",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6811025195",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W4221146420",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W4226013992",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W3092198590",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W4221166856",
    "https://openalex.org/W4224680729",
    "https://openalex.org/W3037533539",
    "https://openalex.org/W4221146106",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2963783181",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4214490042",
    "https://openalex.org/W2964022527",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4226392681",
    "https://openalex.org/W4312956471",
    "https://openalex.org/W4312312588",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4286897344",
    "https://openalex.org/W4225544038",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4287183620",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W4309181071",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2963125676",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2983358816",
    "https://openalex.org/W4224257666",
    "https://openalex.org/W2964284374",
    "https://openalex.org/W3171547673",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2987734933",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4214627427",
    "https://openalex.org/W3177271687"
  ],
  "abstract": "In this paper, we study the problem of visual grounding by considering both phrase extraction and grounding (PEG). In contrast to the previous phrase-known-at-test setting, PEG requires a model to extract phrases from text and locate objects from image simultaneously, which is a more practical setting in real applications. As phrase extraction can be regarded as a 1D text segmentation problem, we formulate PEG as a dual detection problem and propose a novel DQ-DETR model, which introduces dual queries to probe different features from image and text for object prediction and phrase mask prediction. Each pair of dual queries are designed to have shared positional parts but different content parts. Such a design effectively alleviates the difficulty of modality alignment between image and text (in contrast to a single query design) and empowers Transformer decoder to leverage phrase mask-guided attention to improve the performance. To evaluate the performance of PEG, we also propose a new metric CMAP (cross-modal average precision), analogous to the AP metric in object detection. The new metric overcomes the ambiguity of Recall@1 in many-box-to-one-phrase cases in phrase grounding. As a result, our PEG pre-trained DQ-DETR establishes new state-of-the-art results on all visual grounding benchmarks with a ResNet-101 backbone. For example, it achieves 91.04% and 83.51% in terms of recall rate on RefCOCO testA and testB with a ResNet-101 backbone.",
  "full_text": "DQ-DETR: Dual Query Detection Transformer for\nPhrase Extraction and Grounding\nShilong Liu1,2*, Shijia Huang3, Feng Li2,4, Hao Zhang2,4,\nYaoyuan Liang5, Hang Su1, Jun Zhu1‚Ä†, Lei Zhang2‚Ä†\n1 Dept. of CST, BNRist Center, Inst. for AI, Tsinghua-Bosch Joint Center for ML, Tsinghua University.\n2 International Digital Economy Academy (IDEA).\n3 The Chinese University of Hong Kong.\n4 The Hong Kong University of Science and Technology.\n5 Tsinghua-Berkeley Shenzhen Institute, Tsinghua University.\n{liusl20,liang-yy21}@mails.tsinghua.edu.cn, {fliay,hzhangcx}@connect.ust.hk, sjhuang@cse.cuhk.edu.hk,\n{suhangss,dcszjmail.tsinghua.edu.cn, {leizhang}@idea.edu.cn\nAbstract\nIn this paper, we study the problem of visual grounding by\nconsidering both phrase extraction and grounding (PEG). In\ncontrast to the previous phrase-known-at-test setting, PEG re-\nquires a model to extract phrases from text and locate ob-\njects from image simultaneously, which is a more practical\nsetting in real applications. As phrase extraction can be re-\ngarded as a1D text segmentation problem, we formulate PEG\nas a dual detection problem and propose a novel DQ-DETR\nmodel, which introduces dual queries to probe different fea-\ntures from image and text for object prediction and phrase\nmask prediction. Each pair of dual queries is designed to have\nshared positional parts but different content parts. Such a de-\nsign effectively alleviates the difficulty of modality alignment\nbetween image and text (in contrast to a single query design)\nand empowers Transformer decoder to leverage phrase mask-\nguided attention to improve the performance. To evaluate the\nperformance of PEG, we also propose a new metric CMAP\n(cross-modal average precision), analogous to the AP metric\nin object detection. The new metric overcomes the ambigu-\nity of Recall@1 in many-box-to-one-phrase cases in phrase\ngrounding. As a result, our PEG pre-trained DQ-DETR es-\ntablishes new state-of-the-art results on all visual grounding\nbenchmarks with a ResNet-101 backbone. For example, it\nachieves 91.04% and 83.51% in terms of recall rate on Ref-\nCOCO testA and testB with a ResNet-101 backbone. Code\nwill be available at https://github.com/IDEA-Research/DQ-\nDETR.\nIntroduction\nVisual grounding aims to locate objects referred to by lan-\nguage expressions or phrases, which closely relates to ob-\nject detection (DET, Fig. 1 (a)) in vision. It has received\nincreasing attention for its potential to benefit other multi-\nmodal tasks like visual question answering (VQA) (Fukui\n*This work was done when Shilong Liu, Yaoyuan Liang, Feng\nLi, Shijia Huang, and Hao Zhang were interns at IDEA.\n‚Ä†Corresponding author.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nObject \nDetection \n(DET)\nTask Input Output Metric\nAP\nReferring \nExpression \nComprehension\n(REC)\nThe ball that is \nin front of a cat\nRecall@1 \nPhrase \nGrounding A cat and two balls\nRecall@1\n(consider only \none object for \neach phrase)\nPhrase \nExtraction \nand Grounding \n(PEG) A cat and two balls\nA cat \ntwo balls\nCMAP\n(cross-modal\naverage precision)\n(a)\n(b)\n(c)\n(d)\ncat \nball\nor\nor\ntwo balls\nFigure 1: Comparisons of different settings. (a) Given an\nimage, object detection (DET) is to locate objects in pre-\ndefined categories. The most popular metric for DET is mAP\n(Lin et al. 2014). (b) Referring expression comprehension\n(REC) aims at locating objects described by an input text. Its\nperformance is normally evaluated by the recall of the most\nconfident outputs. (c) Phrase grounding needs to ground the\nspatial regions described by phrases in an image. Most meth-\nods treat this task as a ranking problem and evaluate it by\nrecall. The phrases in sentences are usually assumed known\nduring inference. (d) We re-emphasize a phrase extraction\nand grounding (PEG) setting in this paper. A key difference\ncompared with phrase grounding is that phrases in PEG are\nunknown during test. We propose a CMAP (cross-modal av-\nerage precision) metric for the PEG task, analogous to mAP\nfor DET. In this paper, we use the term ‚Äúvisual grounding‚Äù\nfor all of three tasks: REC, phrase grounding, and PEG.\net al. 2016) and image retrieval (Karpathy, Joulin, and Fei-\nFei 2014; Radenovic, Tolias, and Chum 2016).\nSome works (Deng et al. 2021; Huang et al. 2022) treat\nthe terms visual grounding,referring expression comprehen-\nsion (REC), and phrase grounding interchangeable. How-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1728\never, they have subtle differences. Both REC and phrase\ngrounding are sub-tasks of visual grounding. REC locates\nobjects with a free-form guided text, as shown in Fig. 1 (b).\nIt has only one category of objects to detect as requested\nby a referring expression, while phrase grounding needs to\nfind all objects mentioned in a caption, as shown in Fig. 1\n(c). Though they have different definitions, phrase ground-\ning can be reformulated as a REC task by extracting phrases\nas referring expressions since phrases are assumed known\nduring test (Deng et al. 2021; Du et al. 2022; Huang et al.\n2022). Some methods (Mu et al. 2021; Liu et al. 2019b)\nuse non-REC solutions for phrase grounding, while they also\ntreat phrases as known during test.\nWe argue it is more practical to treat phrases as unknown\nduring test and study the problem of visual grounding in\nthis paper by considering both phrase extraction and ground-\ning (PEG), as shown in Fig. 1 (d). Solving PEG by devel-\noping a large-scale image-text-paired training dataset with\nboth phrases and objects annotated is prohibitively costly.\nA simple way to extend existing REC models (Wang et al.\n2022; Zhu et al. 2022) to PEG is to develop a two-stage\nsolution: firstly extracting phrases using an NLP tool like\nspaCy (Honnibal and Montani 2017) and then applying a\nREC model. However, such a solution may result in inferior\nperformance (as shown in our Table 2) as there is no inter-\naction between the two stages. For example, an image may\nhave no object or more than one object that corresponds to\nan extracted phrase. Yet most REC models (Miao et al. 2022;\nZhu et al. 2022) predict only one object for each extracted\nphrase. Let alone inaccurate phrase extraction can mislead a\nREC model to predict unrelated objects.\nWe are not the first to propose the PEG setting. Some pre-\nvious works (Karpathy, Joulin, and Fei-Fei 2014; Karpathy\nand Fei-Fei 2014) align image regions and phrases for image\nretrieval. Flickr30k Entities (Plummer et al. 2015) evaluates\nmodels under the scenario in which phrases are unknown as\nwell. They extract noun phrases using NLP tools and penal-\nize recall if the tools extract inaccurate phrases. Despite of\nsuch early explorations, most successors (Bajaj, Wang, and\nSigal 2019; Deng et al. 2021) treat phrase grounding as a\nretrieval task and use ground truth phrases as inputs. Hence\nwe re-emphasize the PEG setting, where we predict object-\nphrase pairs given only a pair of image and text as input,\nwithout assuming phrases as known input. We can reformu-\nlate all three other tasks (DET, REC, and phrase grounding)\nas PEG tasks.\nPEG lifts the importance of phrase extraction, which is of-\nten overlooked in previous task formulations and solutions.\nSome previous works can be used for the PEG task, such\nas MDETR (Kamath et al. 2021) and GLIP (Li et al. 2021),\nwhich use the same query (in a DETR framework) for both\nobject localization and phrase extraction, as shown in Fig.\n2 (c). However, their phrase extraction module requires a\nquery to have an extra capability to perform the challeng-\ning image-text feature alignment, which can interfere with\nthe bounding box regression branch and result in an inferior\nperformance.\nWe note that phrase extraction is to localize a noun phrase\nfrom an input text, which can be regarded as a 1D text seg-\nmentation problem that predicts a 1D text mask for a tar-\nget phrase. Such a problem is analogous to 2D mask pre-\ndiction for an object instance in 2D image segmentation.\nEspecially, inspired by recent progress of DETR-like mod-\nels (e.g., DINO (Zhang et al. 2022), Mask2Former (Cheng\net al. 2022)), we develop a more principled solution DQ-\nDETR, which is a dual query-based 1 DETR-like model for\nPEG. As shown in Fig. 2 (d), our model uses dual queries\nto perform object detection and text mask prediction in one\nDETR framework. The text mask prediction is very simi-\nlar to instance mask prediction as in Mask2Former, hence\nwe can use masked-attention Transformer decoder layers to\nimprove the performance of text mask prediction. In DQ-\nDETR, a pair of dual queries is designed to have shared po-\nsitional parts but different content parts 2. Such a decoupled\nquery design helps alleviate the difficulty of modality align-\nment between image and text, yielding faster convergence\nand better performance.\nTo evaluate models on the PEG setting, we propose a\nnew metric CMAP (cross-modal average precision), which\nis analogous to the AP metric widely used in object detec-\ntion. It measures the accuracy of both phrase extraction and\nobject localization, as shown in Fig. 3 (c). The CMAP met-\nric overcomes the ambiguous issue of the previous Recall@1\nwhen multiple objects correspond to one phrase. Recall@1\nevaluates the accuracy of boxes with the highest confidence.\nHowever, for cases where multiple objects correspond to\none phrase, the metric becomes ambiguous. To deal with\nsuch cases, previous works (Plummer et al. 2015; Li et al.\n2019; Kamath et al. 2021) leveraged two different protocols,\nwhich we denote as A NY-BOX (Fig. 3 (a)) and M ERGED -\nBOXES (Fig. 3 (b)) protocols following MDETR (Kamath\net al. 2021). The ANY-BOX setting treats a prediction as cor-\nrect if any of the ground truth boxes is matched. However, it\ncannot evaluate a model‚Äôs capability of finding all objects in\nan image. The other protocol, M ERGED -BOXES , combines\nall objects for one phrase into a big box for evaluation. While\nbeing able to capture all objects, this protocol cannot mea-\nsure the localization accuracy for every object instance.\nWe summarize our contributions as follows:\n1. By comparing three settings in visual grounding: DET,\nREC, and phrase grounding, we re-emphasize a PEG set-\nting, which is often overlooked in previous works. To\ntake the phrase extraction accuracy into account, we pro-\npose a new cross-modal average precision (CMAP) met-\nric for PEG to measure a combined accuracy for both\nphrase extraction and object localization. The CMAP\nmetric is free of confusion when multiple objects corre-\nspond to one phrase.\n1We use the term ‚Äúquery‚Äù as the input of the Transformer de-\ncoder layers in this paper, following the common practice in the\nTransformer and DETR-related literature (Vaswani et al. 2017;\nMeng et al. 2021; Liu et al. 2022). The definition differs from some\nvisual grounding papers, where ‚Äúquery‚Äù refers to an input text. A\ndetailed explanation of our dual query is available in the appendix.\n2A DETR query consists of two parts: a content part and a po-\nsitional part. More detailed discussions can be referred to (Meng\net al. 2021) and (Liu et al. 2022).\n1729\n(a) DET\nQuery\n(b) REC/\nPhrase grounding\nQuery\n(d) PEG: DQ-DETR \nA ball and a cat\nImage \nQuery\nText \nQuery\n(c) PEG: \nMDETR/GLIP\nQuery\nA ball and a cat\nFigure 2: Comparisons of different models. The ‚Äúquery‚Äù here refers to the input of Transformer decoder.\nTwo pandas lie on a climber.\n(a) Recall@1, ANY-BOX\nor\nTwo pandas lie on a climber.\n(b) Recall@1, MERGED-BOXES\nTwo pandas lie on a climber.\n(c) CMAP(ours)\nTwo pandas \nTwo pandas \nFigure 3: Comparisons of different metrics. We only plot objects corresponding to the phrases ‚ÄúTwo pandas‚Äù for a better\ncomparison. (a) and (b) are used for phrase grounding. (a) The A NY-BOX setting treats a prediction as correct if any of the\nground truth boxes is matched. (b) M ERGED -BOXES combines all objects for one phrase to a big box for evaluation. (c) Our\nmetric CMAP encourages a model to predict all objects and their corresponding phrases.\n2. We interpret noun phrase extraction as a 1D text segmen-\ntation problem and formulate the PEG problem as pre-\ndicting both bounding boxes for objects and text masks\nfor phrases. Accordingly, we develop a novel dual query-\nbased DETR-like model DQ-DETR with several tech-\nniques to improve the performance of phrase extraction\nand object localization.\n3. We validate our methods on several benchmarks and es-\ntablish new state-of-the-art results, including Flickr30k,\nRefCOCO/+/g, and COCO. Our model obtains 76.0%\nCMAP50 and 83.2% Recall@1 at Flickr30k enti-\nties (Plummer et al. 2015). Moreover, we achieve91.04%\nand 83.51% in terms of recall rate on RefCOCO testA\nand testB with a ResNet-101 backbone.\nPEG & CMAP\nWe present the PEG (phrase extraction and grounding) prob-\nlem formulation and the CMAP (cross-modal average preci-\nsion) definition in this section.\nGiven an image-text pair as input, PEG requires a model\nto predict region-phrase pairs from the input image and text\npair, as shown in Fig. 3. The PEG task can be viewed as\na dual detection problem for image box detection and text\nmask segmentation, since noun phrase extraction can be in-\nterpreted as a 1D text segmentation problem.\nTo measure both the text phrase extraction accuracy and\nthe image object localization accuracy, we propose a new\nmetric which is similar to the AP metric used in DET. AP is\ncalculated by integrating the area under a P-R curve. The key\nto plot P-R curves is to decide positive and negative sam-\nples. DET benchmarks like COCO (Lin et al. 2014) lever-\nage IOU (intersection over union) between a predicted box\nand a ground truth box to discriminate positive and nega-\ntive predictions. As we interpret phrase extraction as a 1D\nsegmentation problem, we use dual IOU to choose positive\npredictions. The dual IOU is defined as:\nIOUdual = (IOUbox)0.5 √ó IOUphrase, (1)\nwhere IOUbox is the box IOU and IOUphrase is the phrase\nIOU. We take the square root ofIOUbox to make IOUdual a\ntwo dimensional metric so that its threshold (e.g. 0.5) has a\nsimilar meaning to IOUbox. Following the common prac-\ntice in phrase grounding and REC, we use IOUdual >=\n0.5 as positive samples, and vice versa. We use the term\n‚ÄúCMAP50‚Äù to denote the metric at threshold 0.5.\n1730\nDQ-DETR\nFollowing DETR (Carion et al. 2020) and MDETR (Kamath\net al. 2021), DQ-DETR is a Transformer-based encoder-\ndecoder architecture, which contains an image backbone, a\ntext backbone, a multi-layer Transformer encoder, a multi-\nlayer Transformer decoder, and several prediction heads.\nGiven a pair of inputs (Image, Text), we extract im-\nage features and text features using an image backbone and a\ntext backbone, respectively. The image and text features are\nflattened, concatenated, and then fed into the Transformer\nencoder layers. We then use learnable dual queries for the\ndecoder layers to probe desired features from the concate-\nnated multi-modality features. The image queries and text\nqueries will be used for box regressions and phrase localiza-\ntions, respectively, as shown in Fig. 4 left.\nDual Queries for Dual Detections\nWe propose to decouple the queries for bounding box regres-\nsion and phrase localization in DQ-DETR. However, as the\ndual queries aim to predict paired (Region, Phrase)\nresults, both queries need to focus on the same region of an\nobject in the input image and the same position of a phrase\nin the input text. Hence we propose to share the positional\nparts and decouple the content parts of the queries. As we\nformulate the problem as a dual detection problem for im-\nage box detection and text phrase segmentation, we intro-\nduce two items for the positional queries, i.e., image posi-\ntional queries and text positional queries. More concretely,\nthe image positional queries are formulated as anchor boxes\nlike DAB-DETR (Liu et al. 2022) and then projected to\nhigh dimensions with sine/cosine encoding. The text posi-\ntional queries are formulated as 1D segmentation masks like\nMask2Former (Cheng et al. 2022) and then used for text\nmask-guided attention. The image positional queries are pre-\ndicted by the updated image queries, and the text positional\nqueries are generated by performing dot product between the\nupdated image queries and text features from the encoder\noutput, as shown in Fig. 4 right. Both two positional queries\nwill be shared by the dual queries as positional parts for the\nnext layer. Beyond the positional and content parts, we add\na learnable modality embedding to the features of different\nmodalities. We list the components of our queries in Table\n1.\nText Mask-Guided Attention\nThe 1D segmentation formulation of phrase localization in-\nspires us to propose a text mask-guided attention to let\nqueries focus on phrase tokens of interest, analogous to the\nmask attention in Mask2Former (Cheng et al. 2022). Each\ntext query has a text positional queryMT ‚àà 1Ntext , which is\na binary mask with the same length as the text features. We\nuse the encoder output image-text-concatenated features as\nkeys and values for cross-attention. The binary masks will be\nused as attention masks for the text features in the concate-\nnated features. Text features will be used if their correspond-\ning mask values are ones, otherwise they will be masked out.\nThis operation constrains the attention on the target phrases\nwhile the predicted masks are updated layer by layer to to\nget closer to ground truth masks. We use all-ones masks as\ninputs for the first decoder layer. Other layers will leverage\nthe predicted masks from their previous layers. The final up-\ndated masks are the outputs for phrase localization.\nLoss Functions\nFollowing DETR (Carion et al. 2020) and MDETR (Kamath\net al. 2021), we use bipartite matching to assign ground truth\nobject boxes and text phrases to dual queries during training.\nThe final loss functions can be grouped into boxes losses\nfor images and phrase losses for texts. We use the L1 loss\nand the GIOU (Rezatofighi et al. 2019) loss for bounding\nbox regression. For phrase localization, we use a contrastive\nsoftmax loss.\nFor a text query at the output of the decoder Q(out) ‚àà\nRNq,D, we compute the similarities between this query and\nthe encoder output text features F(enc)\nT ‚àà RNtext,D to pre-\ndict a text segmentation mask. We first linearly project the\ndecoder output Q(out) to get Q = Linear Q(Q(out)) ‚àà\nRNq,D1 . Then we linearly project the encoder output text\nfeature F(enc)\nT to get FT = LinearT (F(enc)\nT ) ‚àà RNtext,D1 .\nThe notations D1 is the dimension of the projected space,\nand LinearQ, LinearT are two linear layers. As some\nqueries may not correspond to any desired phrase, sim-\nilar to a DETR query not matching with any ground\ntruth object, we set an extra learnable no\nphrase to-\nken NoPhraseToken ‚àà RD1 for no phrase queries.\nWe then concatenate the projected text feature and the\nno\nphrase token to get an extended text feature F‚Ä≤\nT =\nConcat(FT , NoPhraseToken) ‚àà RNtext+1,D1 .\nThe final contrastive softmax loss is performed between\nthe projected query features Q ‚àà RNq,D1 and the extended\ntext feature F‚Ä≤\nT ‚àà RNtext+1,D1 . Let Sqi be the set of text\ntoken indices of a target phrase for a given query qi ‚àà Q.\nThe phrase localization loss for query qi is:\nLphrase,i =\nX\nj‚ààSqi\n(‚àílog q‚ä§\ni pj/œÑPNtext+1\nk=0 q‚ä§\ni pk/œÑ\n), (2)\nwhere œÑ is a temperature parameter which is empirically set\nto 0.07 in our experiments, and pj ‚àà F‚Ä≤\nT is a text feature\nor a no phrase token with index j. We down-weight the\nloss by 0.05 when no objects are assigned to the query qi to\nbalance the classes.\nExperiments\nImplementation Details\nModels. We use two commonly used image backbones,\nResNet-50 and ResNet-101 (He et al. 2016) pre-trained on\nImageNet (Deng et al. 2009), for our base setting and pre-\ntraining setting, respectively. Both two pre-trained models\nare provided by PyTorch (Paszke et al. 2017). For the text\nbackbones, we use the pre-trained RoBERTa-base (Liu et al.\n2019a) provided by HuggingFace (Wolf et al. 2019) in our\nexperiments. We set D = 256 and D1 = 64 in our im-\nplementations and use 100 pairs of dual queries. Our mod-\nels use 6 encoder layers and 6 decoder layers. The learning\n1731\nA cat and a ball\nImage\nBackbone\nText\nBackbone\nEncoder Layers x ùëÅenc\nimage features text features\n2. A Decoder Layer1. Overall Framework\nDecoder Layers x ùëÅdec\nA cat and a ball\nimage \nqueries\ntext \nqueries\ntext masksboxes\nimage query\nshare\nText-mask guided\ncross-attention\nanchor box\ntext mask\nadd & norm\nupdated \nanchor box\nupdated \ntext mask\nimage \nfeatures\ntext \nfeatures\ntext query\nFFN & add & norm \nself-attn & \nadd & norm \nQKV\nupdated \nimage query\nupdated \ntext query\ntext mask\n(Encoder outputs)\nFigure 4: The framework of our proposed DQ-DETR model. The left block is the overall framework. The right block presents\nthe detailed structure of a decoder layer in DQ-DETR.\nQueries Image Queries QI ‚àà RNq,D Text Queries QT ‚àà RNq,D\nContent Parts Q(C)\nI ‚àà RNq,D Q(C)\nT ‚àà RNq,D\nImage Positional Parts AI ‚àà RNq,4 AT = AI\nText Positional Parts MI = MT MT ‚àà {0,1}Ntext\nModality Embeddings ModalityTokenI ‚àà RD ModalityTokenT ‚àà RD\nTable 1: The table of components of our dual queries.\nschedules are different for different settings, which will be\ndescribed in each subsection. The initial learning rates for\nthe Transformer encoder-decoder and image backbone are\n1e‚àí4 and 1e‚àí5, respectively. For the text backbone, we use a\nlinear decay schedule from 5e‚àí5 to 0 with a linear warm-up\nin the first 1% steps. To stabilize the bipartite graph match-\ning, we use anchor denoising (Li et al. 2022) in our imple-\nmentations.\nThe Pre-training Setting\nPre-training Task: PEG Following MDETR (Kamath\net al. 2021), we use the combined dataset of Flickr30k,\nCOCO, and Visual Genome for our pre-training. The back-\nbone we used is ResNet-101. We pre-train our model on the\ncombined dataset for 25 epochs and drop the initial learning\nrate by 10 after the 20-th epoch. The pre-training takes about\n100 hours on 16 Nvidia A100 GPUs with4 images per GPU.\nWe then fine-tune the model on different tasks with4 GPUs,\nexcept for the object detection task, which needs 8 GPUs.\nWe compare our model with three baselines in Table 2.\nWe use the state-of-the-art REC model OFA-REC (Wang\net al. 2022) 3 to demonstrate the necessary of our PEG.\nOFA is an unified model pre-trained with more than 50M\nimages and can be used for REC tasks. To adapt it to our\nPEG task, we use spaCy (Honnibal and Montani 2017) to ex-\ntract noun phrases. The results show that OFA-REC+spaCy\nis much worse than the other two end-to-end models in terms\nof the CMAP50 metric. One important reason is the failure\nwhen multiple objects correspond to one phrase. To decou-\nple the effect of phrase extraction and REC, we design an-\nother baseline with spaCy and an ideal REC model named\nGoldREC. GoldREC outputs the ground-truth object whose\ncorresponding phrase is the most similar to input phrases for\nany given input phrase. It shows that inaccurate phrase ex-\ntraction has a large impact on final performance.\n3We use theOFABase provided in https://github.com/OFA-Sys/\nOFA.\n1732\nModel Pre-train Data Epoches CMAP50 R@1\nOFA-REC+spaCy CC, SBU, COCO, VG, OI, O365, YFCC (50M) - 23.2 58.1\nGoldREC+spaCy - - 44.4 100.0\nMDETR COCO, VG, Flickr30k (200k) 50 70.2 82.5\nDQ-DETR (Ours) COCO, VG, Flickr30k (200k) 25 76.0 (+5.8) 83.2\nTable 2: Pre-training result comparison on Flickr30k Entities with baselines.\nMethod Val Test\nR@1 R@5 R@10 R@1 R@5 R@10\nANY-BOX Protocol\nBAN - - - 69.7 84.2 86.4\nVisualBert 68.1 84.0 86.2 - - -\nVisualBert 70.4 84.5 86.3 71.3 85.0 86.5\nMDETR 82.5 92.9 94.9 83.4 93.5 95.3\nDQ-DETR(Ours) 83.2 (+0.7) 93.9 95.6 83.9 94.6 96.2\nMERGED -BOXES Protocol\nCITE - - - 61.9 - -\nFAOG - - - 68.7 - -\nSimNet-CCA - - - 71.9 - -\nDDPN 72.8 - - 73.5 - -\nRefTR - - - 81.2 - -\nSeqTR - - - 81.2 - -\nMDETR 82.3 91.8 93.7 83.8 92.7 94.4\nDQ-DETR(Ours) 83.7 (+1.4) 93.8 95.8 84.3 93.9 95.5\nTable 3: Results on the phrase grounding task on Flickr30k Entities.\nWe also use MDETR (Kamath et al. 2021) as a baseline.\nOur model outperforms MDETR on Flickr30k Entities with\nonly half of the number of training epochs. It outperforms\nMDETR by +5.8 CMAP50, demonstrating the effectiveness\nof decoupling image and text queries. We provide a visual-\nization of these models‚Äô results in Appendix Table 7\nDown-stream Task: Phrase GroundingWe compare our\nDQ-DETR with BAN (Kim, Jun, and Zhang 2018), Vi-\nsualBert (Li et al. 2019), CITE (Plummer et al. 2017),\nFAOG (Yang et al. 2019), SimNet-CCA (Plummer et al.\n2020), DDPN (Yu et al. 2018), RefTR (Li and Sigal 2021),\nSeqTR (Zhu et al. 2022), and MDETR (Kamath et al.\n2021) in Table 3. We fine-tune our pre-trained model on\nFlickr30k (Plummer et al. 2015) for the phrase grounding\ntask. To compare with previous works in the literature, we\nfollow MDETR (Kamath et al. 2021) and evaluate the mod-\nels with Recall@k under two different protocols, ANY-BOX\nand M ERGED -BOXES protocols. For the A NY-BOX proto-\ncol, we evaluate our pre-trained model on the validation and\ntest splits directly. For the M ERGED -BOXES protocol, we\nfine-tune the pre-trained model for 5 epochs. Our model\nintroduces improvements of +0.7 Recall@1 and +1.4 Re-\ncall@1 on the two validation splits, with only half of the\nnumber of pre-training epochs compared with MDETR. We\nalso establish new state-of-the-art results on the two bench-\nmarks with a ResNet-101 backbone.\nDown-stream Task: REC We compare our model with\nstate-of-the-art REC methods on RefCOCO/+/g benchmarks\nafter fine-tuning in Table. 4. We evaluate the models with\nRecall@1. Although our model is not specifically designed\nfor REC tasks, we can convert the REC task to a PEG\nproblem by marking the whole sentence as a phrase cor-\nresponding to its referred object. As there are no ground\ntruth phrases labeled in the dataset, we do not use the text\nmask-guided attention in the fine-tuning process. As we\nhave leveraged all training data of the three REC datasets\nduring pre-training, it is reasonable to fine-tune the models\non a combination of these three datasets. To avoid data leak-\nage, we removed all images appeared in the val/test splits\nof RefCOCO/+/g. This operation removes about 10% of the\ntotal images. As a result, our model outperforms all previ-\nous works with a ResNet-101 backbone and establishes new\nstate-of-the-art results on the RefCOCO/+/g benchmarks.\nDown-stream Task: DET Refer to for more details.\nThe Base Setting & Ablations\nWe use MDETR (Kamath et al. 2021) as our baseline and\nour model is an improvement upon it. As there are only\npre-trained models in the MDETR paper, which is not easy\nfor the community to compare, we design the base setting\non Flickr30 Entities without pre-training in this section. All\nmodels are trained on 4 Nvidia A100 GPUs with a ResNet-\n50 backbone and each GPU contains 4 images.\nResults on Flickr30k Entities and ablations.All mod-\nels for Flickr30k are trained for 24 epochs with a learning\nrate drop at the 20th epoch. We compare our DQ-DETR and\nMDETR in Table 5. Our proposed DQ-DETR outperforms\n1733\nMethod Pre-training RefCOCO RefCOCO+ RefCOCOg\nimage data val testA testB val testA testB val test\nMAttNet None 76.65 81.14 69.99 65.33 71.62 56.02 66.58 67.27\nVGTR None 79.20 82.32 73.78 63.91 70.09 56.51 65.73 67.23\nTransVG None 81.02 82.72 78.35 64.82 70.70 56.94 68.67 67.73\nViLBERT 3.3M - - - 72.34 78.52 62.61 - -\nVL-BERT L 3.3M - - - 72.59 78.57 62.30 - -\nUNITER L 4.6M 81.41 87.04 74.17 75.90 81.45 66.70 74.86 75.77\nVILLA L 4.6M 82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71\nERNIE-ViL L 4.3M - - - 75.95 82.07 66.88 - -\nRefTR 100k 85.65 88.73 81.16 77.55 82.26 68.99 79.25 80.01\nSeqTR 174k 87.00 90.15 83.59 78.69 84.51 71.87 82.69 83.37\nOFA 50M 88.48 90.67 83.30 81.39 87.15 74.29 82.29 82.31\nMDETR 200k 86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89\nDQ-DETR(Ours) 200k 88.63 91.04 83.51 81.66 86.15 73.21 82.76 83.44\nTable 4: Top-1 accuracy comparison on the referring expression comprehension task.\nModel CMAP50 R@1 R@5\nMDETR 61.49 77.46 88.28\nMDETR with 200 queries 56.89 76.29 86.87\nOur baseline for DQ-DETR 66.68 75.44 87.94\n+ text mask attention 68.26 76.58 88.46\n+ dual queries 69.86 78.87 89.39\n+ positional query sharing 70.63 79.16 89.84\nTable 5: Ablation results for DQ-DETR and a comparison\nwith MDETR. All models are trained with a ResNet-50\nbackbone for 24 epochs, with a learning rate drop at the\n20-th epoch. We use the A NY-BOX protocol for Recall@k\nmetrics in this table.\nMDETR with a large margin, e.g., +13.14% CMAP50 and\n+4.70% Recall@1 on Flickr30k entities.\nWe provide the ablations in Table 5 as well. The dual\nquery design introduces a gain of1.60% CMAP50 in our ex-\nperiments, which demonstrates the effectiveness of our dual\nquery design. Moreover, we find the positional query shar-\ning strategy helps improve the results as well, which high-\nlights the necessity of our carefully designed dual queries.\nThe multi-scale design and the text mask-guided attention\nalso help the training of our model, each of which introduces\na gain of 1 ‚àí 2% CMAP50. We train a variant of MDETR\nwith 200 queries for a fair comparison with our models. Sur-\nprisingly, we find the result drops a lot. We speculate that the\ndata imbalance leads to the result, as more queries will ex-\nacerbate the imbalance of classes. In contrast, our model,\nwhich outputs 100 results like the original DETR, will not\nsuffer from this problem. We will leave it as a future work\nto study how to scale up the model with more queries.\nRelated Work\nWe present the related work in the appendix.\nConclusion\nWe have presented an overview of visual grounding\ntasks and identified an often overlooked phrase extraction\nstep. The observation inspires us to re-emphasize a PEG\n(phrase extraction and grounding) task and propose a new\nCMAP (cross-modal average precision) metric. The CMAP\novercomes the ambiguity of Recall@1 in many-box-to-\none-phrase cases in phrase grounding tasks. Moreover, we\npropose a new interpretation of the PEG task as a dual\ndetection problem by viewing phrase localization as a 1D\ntext segmentation problem. With this new interpretation, we\ndeveloped a dual query-based DETR-like model DQ-DETR\nfor phrase grounding. Such a decoupled query design helps\nalleviate the difficulty of modality alignment between\nimage and text, yielding both faster convergence and\nbetter performance. We also proposed a text mask-guided\nattention to constrain a text query to the masked text\ntokens in cross-attention modules. We conducted extensive\nexperiments to verify the effectiveness of our model design.\nAcknowledgments\nThis work was supported by the National Key Research and\nDevelopment Program of China (2020AAA0106302).\nWe thank Yukai Shi, Linghao Chen, Jianan Wang, Ailing\nZeng, and Xianbiao Qi of IDEA CVR groups for their valu-\nable feedbacks. We thank all the reviewers including SPC\nand AC in AAAI 2023 for their kindly suggestions. We\nthank the reviewers of our initial version in NeurIPS 2022\nfor their valuable suggestions, especially the suggestions of\nthe Reviewer btPY , which helped us a lot.\nReferences\nBajaj, M.; Wang, L.; and Sigal, L. 2019. G3raphGround:\nGraph-Based Language Grounding. international confer-\nence on computer vision.\n1734\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213‚Äì229. Springer.\nCheng, B.; Choudhuri, A.; Misra, I.; Kirillov, A.; Girdhar,\nR.; and Schwing, A. G. 2022. Mask2Former for Video In-\nstance Segmentation.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248‚Äì255. Ieee.\nDeng, J.; Yang, Z.; Chen, T.; Zhou, W.; and Li, H. 2021.\nTransVG: End-to-End Visual Grounding with Transformers.\narXiv: Computer Vision and Pattern Recognition.\nDu, Y .; Fu, Z.; Liu, Q.; and Wang, Y . 2022. Visual Ground-\ning with Transformers. In 2022 IEEE International Confer-\nence on Multimedia and Expo (ICME).\nFukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell,\nT.; and Rohrbach, M. 2016. Multimodal Compact Bilinear\nPooling for Visual Question Answering and Visual Ground-\ning. empirical methods in natural language processing.\nF¬®urst, A.; Rumetshofer, E.; Tran, V . H.; Ramsauer, H.; Tang,\nF.; Lehner, J. M.; Kreil, D. P.; Kopp, M. K.; Klambauer, G.;\nBitto-Nemling, A.; and Hochreiter, S. 2021. CLOOB: Mod-\nern Hopfield Networks with InfoLOOB Outperform CLIP.\narXiv: Learning.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 770‚Äì\n778.\nHonnibal, M.; and Montani, I. 2017. spaCy 2: Natural\nlanguage understanding with Bloom embeddings, convolu-\ntional neural networks and incremental parsing. To appear.\nHuang, J.; Qin, Y .; Qi, J.; Sun, Q.; and Zhang, H. 2022.\nDeconfounded Visual Grounding. Proceedings of the AAAI\nConference on Artificial Intelligence, 36(1): 998‚Äì1006.\nKamath, A.; Singh, M.; LeCun, Y .; Synnaeve, G.; Misra, I.;\nand Carion, N. 2021. MDETR-modulated detection for end-\nto-end multi-modal understanding. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n1780‚Äì1790.\nKarpathy, A.; and Fei-Fei, L. 2014. Deep Visual-Semantic\nAlignments for Generating Image Descriptions. IEEE\nTransactions on Pattern Analysis and Machine Intelligence.\nKarpathy, A.; Joulin, A.; and Fei-Fei, L. 2014. Deep Frag-\nment Embeddings for Bidirectional Image Sentence Map-\nping. neural information processing systems.\nKim, J.-H.; Jun, J.; and Zhang, B.-T. 2018. Bilinear Atten-\ntion Networks. neural information processing systems.\nKingma, D. P.; and Ba, J. 2014. Adam: A Method for\nStochastic Optimization. arXiv: Learning.\nLi, F.; Zhang, H.; Liu, S.; Guo, J.; Ni, L. M.; and Zhang,\nL. 2022. DN-DETR: Accelerate DETR Training by Intro-\nducing Query DeNoising. In Computer Vision and Pattern\nRecognition (CVPR).\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-\nW. 2019. VisualBERT: A Simple and Performant Baseline\nfor Vision and Language. arXiv: Computer Vision and Pat-\ntern Recognition.\nLi, L. H.; Zhang, P.; Zhang, H.; Yang, J.; Li, C.; Zhong, Y .;\nWang, L.; Yuan, L.; Zhang, L.; Hwang, J.-N.; et al. 2021.\nGrounded Language-Image Pre-training. arXiv preprint\narXiv:2112.03857.\nLi, M.; and Sigal, L. 2021. Referring Transformer: A One-\nstep Approach to Multi-task Visual Grounding.arXiv: Com-\nputer Vision and Pattern Recognition.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Dollar, P.\n2020. Focal Loss for Dense Object Detection. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 42(2):\n318‚Äì327.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ¬¥ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740‚Äì755. Springer.\nLiu, S.; Li, F.; Zhang, H.; Yang, X.; Qi, X.; Su, H.; Zhu, J.;\nand Zhang, L. 2022. DAB-DETR: Dynamic Anchor Boxes\nare Better Queries for DETR. In International Conference\non Learning Representations.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,\nO.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019a.\nRoBERTa: A Robustly Optimized BERT Pretraining Ap-\nproach. arXiv: Computation and Language.\nLiu, Y .; Wan, B.; Zhu, X.; and He, X. 2019b. Learning\nCross-modal Context Graph for Visual Grounding. arXiv:\nComputer Vision and Pattern Recognition.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight De-\ncay Regularization. In International Conference on Learn-\ning Representations.\nMeng, D.; Chen, X.; Fan, Z.; Zeng, G.; Li, H.; Yuan, Y .; Sun,\nL.; and Wang, J. 2021. Conditional DETR for Fast Training\nConvergence. arXiv preprint arXiv:2108.06152.\nMiao, P.; Su, W.; Wang, L.; Fu, Y .; and Li, X. 2022. Re-\nferring Expression Comprehension via Cross-Level Multi-\nModal Fusion. ArXiv, abs/2204.09957.\nMu, Z.; Tang, S.; Tan, J.; Yu, Q.; and Zhuang, Y . 2021. Dis-\nentangled Motif-aware Graph Learning for Phrase Ground-\ning. national conference on artificial intelligence.\nNagaraja, V . K.; Morariu, V . I.; and Davis, L. S. 2016. Mod-\neling Context Between Objects for Referring Expression\nUnderstanding. arXiv: Computer Vision and Pattern Recog-\nnition.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;\nDeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer,\nA. 2017. Automatic differentiation in pytorch.\nPlummer, B. A.; Kordas, P.; Kiapour, M. H.; Zheng, S.; Pira-\nmuthu, R.; and Lazebnik, S. 2017. Conditional Image-Text\nEmbedding Networks. european conference on computer\nvision.\nPlummer, B. A.; Shih, K. J.; Li, Y .; Xu, K.; Lazeb-\nnik, S.; Sclaroff, S.; and Saenko, K. 2020. Revisiting\n1735\nImage-Language Networks for Open-ended Phrase Detec-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence.\nPlummer, B. A.; Wang, L.; Cervantes, C. M.; Caicedo,\nJ. C.; Hockenmaier, J.; and Lazebnik, S. 2015. Flickr30k\nEntities: Collecting Region-to-Phrase Correspondences for\nRicher Image-to-Sentence Models. International Journal of\nComputer Vision.\nRadenovic, F.; Tolias, G.; and Chum, O. 2016. CNN Im-\nage Retrieval Learns from BoW: Unsupervised Fine-Tuning\nwith Hard Examples. european conference on computer vi-\nsion.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transferable\nVisual Models From Natural Language Supervision. arXiv:\nComputer Vision and Pattern Recognition.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2017. Faster\nR-CNN: Towards Real-Time Object Detection with Region\nProposal Networks. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 39(6): 1137‚Äì1149.\nRezatofighi, H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid, I.;\nand Savarese, S. 2019. Generalized intersection over union:\nA metric and a loss for bounding box regression. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 658‚Äì666.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998‚Äì6008.\nWang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma,\nJ.; Zhou, C.; Zhou, J.; Yang, H.; and Zhou, C. 2022. Uni-\nfying architectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et al.\n2019. Huggingface‚Äôs transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771.\nYang, Z.; Gong, B.; Wang, L.; Huang, W.; Yu, D.; and Luo,\nJ. 2019. A Fast and Accurate One-Stage Approach to Visual\nGrounding. arXiv: Computer Vision and Pattern Recogni-\ntion.\nYu, Z.; Yu, J.; Xiang, C.; Zhao, Z.; Tian, Q.; and Tao, D.\n2018. Rethinking Diversified and Discriminative Proposal\nGeneration for Visual Grounding. international joint con-\nference on artificial intelligence.\nZhang, H.; Li, F.; Liu, S.; Zhang, L.; Su, H.; Zhu, J.; Ni,\nL. M.; and Shum, H.-Y . 2022. DINO: DETR with Improved\nDeNoising Anchor Boxes for End-to-End Object Detection.\narXiv preprint arXiv:2203.03605.\nZhu, C.; Zhou, Y .; Shen, Y .; Luo, G.; Pan, X.; Lin, M.; Chen,\nC.; Cao, L.; Sun, X.; and Ji, R. 2022. Seqtr: A simple yet uni-\nversal network for visual grounding. In Computer Vision‚Äì\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23‚Äì27, 2022, Proceedings, Part XXXV, 598‚Äì615.\nSpringer.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In ICLR 2021: The Ninth Interna-\ntional Conference on Learning Representations.\n1736",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7275834083557129
    },
    {
      "name": "Phrase",
      "score": 0.6990405321121216
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6079136729240417
    },
    {
      "name": "Transformer",
      "score": 0.5160088539123535
    },
    {
      "name": "Natural language processing",
      "score": 0.49017199873924255
    },
    {
      "name": "Ground",
      "score": 0.4622371792793274
    },
    {
      "name": "Precision and recall",
      "score": 0.43001434206962585
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4278336763381958
    },
    {
      "name": "Segmentation",
      "score": 0.4246044456958771
    },
    {
      "name": "Metric (unit)",
      "score": 0.4211846590042114
    },
    {
      "name": "Pairwise comparison",
      "score": 0.4132225513458252
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40859752893447876
    },
    {
      "name": "Data mining",
      "score": 0.3200145363807678
    },
    {
      "name": "Engineering",
      "score": 0.10028040409088135
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210120115",
      "name": "Robert Bosch (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210114105",
      "name": "Tsinghua‚ÄìBerkeley Shenzhen Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2802444338",
      "name": "King Center",
      "country": "US"
    }
  ]
}