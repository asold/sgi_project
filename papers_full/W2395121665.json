{
  "title": "A rule-based language model for reading recognition",
  "url": "https://openalex.org/W2395121665",
  "year": 2009,
  "authors": [
    {
      "id": "https://openalex.org/A2123804940",
      "name": "Jian Cheng",
      "affiliations": [
        "Pearson (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1966181650",
      "name": "Brent Townshend",
      "affiliations": [
        "Pearson (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2401126781",
    "https://openalex.org/W6677994422",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1991133427",
    "https://openalex.org/W3029593813",
    "https://openalex.org/W161232498",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W104463061",
    "https://openalex.org/W2120316500",
    "https://openalex.org/W2343954916"
  ],
  "abstract": "Systems for assessing and tutoring reading skills place unique requirements on underlying ASR technologies.Most responses to a \"read out loud\" task can be handled with a low perplexity language model, but the educational setting of the task calls for diagnostic measures beyond plain accuracy.Pearson developed an automatic assessment of oral reading fluency that was administered in the field to a large, diverse sample of American adults.Traditional N-gram methods for language modeling are not optimal for the special domain of reading tests because N-grams need too much data and do not produce as accurate recognition.An efficient rule-based language model implemented a set of linguistic rules learned from an archival body of transcriptions, using only the text of the new passage and no passage-specific training data.Results from operational data indicate that this rule-based language model can improve the accuracy of test results and produce useful diagnostic information.",
  "full_text": "33Speech and Language Technology in Education (SLaTE 2009)\nSpe\nech and Language Technology in\nEduc\nation (SLaTE2009)\nWroxall Abbey Estate, Warwickshire, UK\nSeptember 3-5, 2009\nISCA Archive\nhttp://www.isca-speech.org/archive\nA Rule-Based Language Model for Reading Recognition\nJian Cheng, Brent Townshend\nKnowledge Technologies, Pearson\n299 S. California Ave, Palo Alto, California 94306, USA\njian.cheng@pearson.com\nAbstract\nSystems for assessing and tutoring reading skills place unique\nrequirements on underlying ASR technologies. Most responses\nto a “read out loud” task can be handled with a low perplexity\nlanguage model, but the educational setting of the task calls for\ndiagnostic measures beyond plain accuracy. Pearson developed\nan automatic assessment of oral reading ﬂuency that was admin-\nistered in the ﬁeld to a large, diverse sample of American adults.\nTraditional N-gram methods for language modeling are not op-\ntimal for the special domain of reading tests because N-grams\nneed too much data and do not produce as accurate recognition.\nAn efﬁcient rule-based language model implemented a set of\nlinguistic rules learned from an archival body of transcriptions,\nusing only the text of the new passage and no passage-speciﬁc\ntraining data. Results from operational data indicate that this\nrule-based language model can improve the accuracy of test re-\nsults and produce useful diagnostic information.\n1. Introduction\nThe U.S. Government’s National Center for Education Statis-\ntics commissioned a reading ﬂuency assessment as part of the\n2003 NAAL (National Assessment of Adult Literacy) called\nthe Fluency Addition to NAAL, in which each respondent read\naloud from lists and passages of text. We focus here on the\npassages. All oral reading responses were digitally recorded\nand subsequently analyzed for measures of accuracy and ﬂu-\nency. For the 2003 NAAL project, 181,420 response recordings\nwere collected. To expedite the scoring of these responses and\nto extract additional information from the responses that human\nraters cannot provide, technology from Ordinate Corporation\n(now the Knowledge Technologies group of Pearson) was used\nto automate and augment the analysis of the oral readings.\nThe evidence that Pearson’s automatic scoring of reading\naccuracy is reliable and valid was presented in [1]. This paper\npresents the methods for building suitable language models for\nautomatic speech recognition (ASR) used in scoring a person’s\nskill in reading passage aloud. A rule-based language model\n(RBLM) was proposed to improve recognition accuracy on\nread-aloud performances, using a set of linguistic rules learned\nfrom a collection of transcriptions of other passages being read\naloud. Contrary to traditional methods, language models for\nnew passages can be built without transcriptions of readings of\nthe same passage. Furthermore, the rule-based recognition pro-\ncess can yield extra diagnostic linguistic information about the\ntest takers’reading habits that can be reported and analysed.\n2. Task analysis\nNAAL reading passages were relatively simple expository or\nnarrative texts of about 150-200 words in length. Each of the\n18,142 adult respondents read aloud 2 out of 8 passages. There\nhas been previous work [2, 3] using ASR in reading passage. In\ngeneral a large number of transcriptions are required to build\nsuitable language models to ensure recognition accuracy. A\ndedicated language model may be built for each passage to cap-\nture the stochastic language structure accurately. The require-\nment for a large number of transcriptions is a disadvantage be-\ncause each time a set of new passages are introduced, many\nspoken responses for each passage needs to be transcribed be-\nfore language models can even be built. Therefore, one goal\nof this research is to ﬁnd a method to build suitable language\nmodels for a new reading passage based only on the text of\nthe new passage without any passage-speciﬁc training data. In-\nstead, information in existing transcriptions of other material\ncan be extracted and applied to new passages. It turns out that\nthese new rule-based language model improve recognition per-\nformance, and consequently improve the accuracy of test re-\nsults. Beyond guiding the recognition process, another goal of\nthe rule-based model development is to identify and aggregate\ninformation about the ﬁne structure of the reader’s performance.\nFor example, knowing the recognition path taken through one or\nmore rule-based language models should indicate which read-\ning skills have been mastered and which still need work.\nFor our speciﬁed passage-reading task, we know the exact\nsentences test takers are expected to say. One of the scores, the\nnumber of words read correctly, is measured based on the dif-\nference between what the test taker says and the expected text.\nThe smaller the difference, the better. This is the big difference\nbetween the ASR task in passage reading and ASR in other ap-\nplications, in which we usually only know that incoming speech\nrelates to a special knowledge domain (a constrained dictionary\nand grammar). The traditional ASR methods for building and\napplying language models are not optimal for the special do-\nmain of reading testing. In addition, signiﬁcant numbers of test\ntakers are non-native speakers with low oral proﬁciency. Their\nresponses may not follow the English grammar and it is impor-\ntant to have the ability to detect such errors.\n3. The language models\nIn a bigram language model, only one previous word wi−1\nwill be used to estimate the likelihoods of the current word.\nA trigram language model considers only previous two words\nwi−1, wi−2. We introduce feasible methods that model much\nlonger sequential dependencies.\nSuppose that the word string a speaker said is W =\nw1, w2, ..., wn, wi ∈ V . V is used to denote a vocabulary.\nThen a priori probability for this word string is\nP(W) =\nnY\ni=1\nP(wi|w1, ..., wi−1).\n10.21437/SLaTE.2009-9\n34Speech and Language Technology in Education (SLaTE 2009)\n!NULL !NULL i !NU\nLL\n!NULL !NULL !NULL won't !NULL !NULL !NULL !NULL take !NULL !NULL that !NULL !NULL!NU\nLL\n!NULL!NULL %SIL\n%SIL\n!NULL %SIL !NULL\n%SIL\n!NULL !NULL %SIL !NULL\n!NULL !NULLi don't know\nuh\n#\nuh\n#\n!NULL !NULL\ndon't\nwant\nthistell\n1 1 1 12 2 2 2\n3\n4\n4\nuh\n#\nuh\n#\n!NULL\nuh\n#\n!NULL\nuh\n#\n...... ............\n......\nto......other answer choices ......other answer choices\n......other don't know ......other don't know\nFigure 1: A simpliﬁed example of the rule-based language model. The passage text is i won’t take that.\nWe hope that we know the value of P(wi|w1, ..., wi−1) af-\nter a speaker produces w1, w2, ..., wi−1. This value can help\nus to recognize the next word. But there are too many argu-\nments in P(wi|w1, ..., wi−1) even for moderate values of i and\nreasonable vocabulary size. It is impossible to estimate every\nP(wi|w1, ..., wi−1) based on previous transcriptions. We need\nto build a language model to estimate them. Suppose that the\nlanguage model we use is L, then the output likelihood of this\nlanguage model can be calculated as\nP(W|L) =\nnY\ni=1\nP(wi|w1, ..., wi−1, L).\nThe advantage of using the language model is that the proba-\nbility that a speaker will choose his ith word does not explic-\nitly depend on the entire history of all his previous words, but\ndepends on the equivalence classes ΦL(w1, ..., wi−1). A dif-\nferent language model determines how to choose the appropri-\nate equivalence classiﬁcation and gives a method to estimate\nP(wi|ΦL(w1, ..., wi−1)). Thus\nP(W|L) =\nnY\ni=1\nP(wi|ΦL(w1, ..., wi−1)).\nWhen the word sequence is sufﬁciently long, the cross-entropy\nH(W) of a language model onW can be simply approximated\nas H(W) = −1/n·log2P(W), and then the preplexity of a\nlanguage model is the reciprocal of the geometric average prob-\nability P(W|L)1/n. The perplexity is a popular measure of a\ntext’s complexity within a language model and can be treated as\nthe effective branching factor. In passage reading, the perplex-\nity within a reasonable language model is usually very low.\n4. The structure of the RBLM\nTo avoid collecting and transcribing many renditions for each\nnew passage, we build language models based on the transcrip-\ntion of a ﬁxed body of similar passages as read by people sim-\nilar to the target population of readers. The basic proposal is\nthat a simple direct graph needs to be built that has a path from\nthe ﬁrst word in the reading passage to the last word. Different\ndirect arcs are required to represent the different classes of er-\nrors made by the readers, such as skipping, repeating, inserting,\nand substituting words. For each arc, a probability is assigned\nto represent the chance that the arc will be chosen. We use a\nknowledge-based approach, which includes a list of linguistic\nrules, such as she may be substituted by he, a single noun may\nbe substituted by a plural noun, to represent some potential arcs.\nThe arc itself can remember which rule it stands for. If such\na graph represents many of the renditions encountered, then it\nshould be a useful language model.\nWe give a simple example to explain the idea. Suppose that\nthe passage text is just the four wordsi won’t take that, which is\nalso the expected sequence of words that a test taker will speak.\nThe test taker may keep silent or say i don’t knowif he can’t\nread this sentence. Also suppose that we have the following\nlinguistic rules in the knowledge base (how to elicit these rules\nwill be discussed in Section 5): won’t may be substituted by\ndon’tor want; take may be substituted by tell; that may be sub-\nstituted by this; to may be inserted at the end ofwon’t; any word\nin the answer choice may be skipped or repeated; mouth noise\n(noted as #) or hesitation (noted as uh) may be inserted at the\nend of any word or at the beginning or end of the sentence; ... .\nWith these assumptions, we can get a simpliﬁed RBLM shown\nin Figure 1.\nFor any passage language model, we also have a leading\nsilence and tail silence (noted as %SIL), which stand for the\npossible silent pause at the beginning or at the end of the sen-\ntence. After applying all the rules, the null nodes can be deleted\nthat have only one parent and one child, and then a direct con-\nnection between the parent and child can be given.\nFrom Figure 1, we can see, for every language model, we\nhave a start null node and an end null node. Every arc from\nthe start null node can stand for a category, such as correct,\nwrong, silence or i don’t know. The category information can\nhelp us ﬁgure out which direction the test taker goes. For some\ncategories, it is possible to have multiple choices, such as other\nanswer choices and other don’t know. The test taker may speak\ni’m sorryinstead of i don’t know. The leading silence follows\nevery category except the silence category itself. If we want to\nknow the relative proportion of the different response choices or\ndon’t know, we can let the corresponding arcs remember which\nchoice they stand for. In the ﬁgure, we omitted some nodes of\nhesitation and mouth noise to simplify the graph. Type 1 arc\nstands for skipping any word. Type 2 arc stands for repeating\n35Speech and Language Technology in Education (SLaTE 2009)\nany word. Type 3 arc stands for skipping a phrase break. Type\n4 arc stands for the situation that test taker will jump from any\nplace to the end of the passage (i.e. stop reading). This situation\nhappens quite often in passage-reading tests because of the time\nconstraint. Since we have the rules of skipping or repeating any\nword, this language model can cover any response whose words\nare in this model.\nOne advantage of this language model is that we can record\nwhat’s really going on. If a test taker speaksi don’t take that, af-\nter matching to this language model to ﬁnd the maximum likeli-\nhood path, we can tell that the rule won’tis substituted by don’t\nand such rule has been ﬁred. Other situations, such as repeat-\ning a word, skipping a word, can be caught easily. During the\nscoring process, we may take advantage of such information to\nscore them differently. For example, the score penalty for re-\npeating a word could be treated as less than that for skipping a\nword.\nThe RBLM essentially is a Markov chain. Every state in\nthe language model corresponds to the equivalence class of pre-\nceeding words as classiﬁed by the language model. The prob-\nability that a speaker will choose his ith word depends on a\ncertain state. Between that state and the start null node a path\nthat covers his previous words should exist.\n5. Extracting linguistic rules\nTo extract linguistic rules, many transcriptions of spoken re-\nsponses to various passages are required. Since using tagged\npassages can give us the information about the linguistic struc-\ntures of passages, we tag all the passage texts. It can be done\nautomatically using different tagging tools. The tag set we used\nis the Penn Treebank Tag set [4]. We also add potential phrase\nbreaks in answers. The linguistic structure can give us ﬂexibil-\nity to add rules that are more general. Some general rules that\ncan be ﬁgured out easily based on the tagged answer choices\nare: NN (noun, singular or mass) becomes NNS (noun, plural);\nVBZ (verb, 3rd person singular present) becomes VBP (verb,\nnon-3rd person singular present); and so on. These rules happen\nquite frequently in the responses of non-native English speak-\ners.\n5.1. Building rules\nAs the ﬁrst step, the following four rules were put in our knowl-\nedge base: any word can be substituted by any word with a\nprobability 0.0000001; any word can be inserted after any word\nwith a probability 0.0000001; any word can be skipped with a\nprobability 0.001; any word can be repeated with a probability\np = 0.001. Then a RBLM was built as discussed in Section 4.\nFor each transcription, we use the Viterbi algorithm [5] to ﬁnd\nthe maximum likelihood path in the language model. This pro-\ncess is similar to ASR decoding. Note that we assign a very low\nprobability to the garbage models (rules permitting any word\nto be replaced by any word and any word to be inserted after\nany word). They are the only rules that allow out-of-vocabulary\nwords to appear, and their probability is ﬁxed to the lowest level\nno matter how we update other probabilities. They will never\nbe ﬁred unless there is no other choice. The garbage rules will\nonly be used during rules building process. When we build the\nRBLM, they will be ignored. By collecting garbage model ﬁr-\ning patterns, such as context and tags in the language model,\nwe can cluster similar cases and propose rules with the ﬁring\nfrequencies for linguists to review. Starting from the most fre-\nquent transcriptions, the most popular rules should be easy to\ndetermine since there are usually only one or two rules used in\nthese transcriptions. On the other hand, test takers make the\nsame mistakes quite often. After the most frequent rules are\nadded to the knowledge base, linguists can more easily identify\nother less frequent ones. To be easy to generalize to new pas-\nsages, we should try to use the general rules (such as rules based\non tags) instead of using the context-dependent rules (such as\nword substitutions).\nBy counting the rule-ﬁring situations, word coverage can be\nunderstood; i.e., the number of times that garbage models were\nﬁred in a transcription divided by the total number of words in\nthe transcription, equal to one minus word coverage. Our goal is\nto introduce reasonable rules to improve word coverage. After\nevery cycle, a few more rules may be added until no signiﬁcant\nimprovement in word coverage is observed. After such a state\nis reached, the rule building process can be ﬁnished.\n5.2. The probabilities of rules\nThere is one problem in our procedure: we need to have the\nprobabilities of the rules that we are trying to estimate to ﬁgure\nout the best path. Fortunately, the expectation-maximization al-\ngorithm [6] can help us to overcome this problem. We can start\nwith a guess at the probabilities of rules (in our case, we assign\na small probability to rules), obtain better estimates, and then\nput them back to run the program again, so that we can obtain\nan even better estimate.\nThe probabilities for different rules are estimated using the\nmaximum likelihood method. The maximum likelihood param-\neter estimate could be obtained by counting. At the individual\npassage level, for every node, there are several out arcs. If the\nmatched path for a transcription includes that node, we increase\nthe count by one for every out arc of that node. However, we\nincrease the ﬁred count by one only for the out arc included in\nthat path. Others are unchanged. After all the transcriptions are\nmatched, the maximum likelihood probability for a rule is the\nﬁred count divided by the visited count.\nThe above discussion is only based on the individual pas-\nsage level. The rule can be generalized to the whole domain we\nare interested in. Its probability is the sum of the ﬁred counts for\nthat rule in any individual passage divided by the sum of the vis-\nited counts. It is possible that certain rules have different proba-\nbilities when they are applied to the different passages. We can\ndistinguish this situation by using chi-squared test 1 to ﬁnd out\nif there is a statistically signiﬁcant difference in the probabilities\nbetween the general and individual levels. If not, we only keep\nthe probabilities for the general level. Otherwise, we also keep\nthe probabilities in the individual level that have signiﬁcant dif-\nference. When we use a rule for a passage, we ﬁrst check if\nthere is an individual-level probability for that rule. If yes, we\nuse that probability. Otherwise, we use the general level.\nThe whole process can be iterated several times using the\nnew estimated probabilities combined with some new rules.\nThis procedure essentially is an expectation-maximization al-\ngorithm, so the ﬁnal estimated probabilities should converge to\na local minimum.\nCurrently we treat hesitation and mouth noise as general\nrules. We only distinguish three different kinds of hesitation\nand mouth noise in the RBLM. They can be inserted at the be-\nginning of the passage, the end of the passage, or at the end of\nany word.\n1When the total number is less than 80, we use Fisher’s exact test.\n36Speech and Language Technology in Education (SLaTE 2009)\n6. Experimental results and analysis\nFrom the 18,142 adult respondents, we randomly selected 2,703\ntest takers as our training set and 1,301 test takers as our test\nset. Each test taker read 2 out of 8 potential passages. All the\npassage responses in the training and test sets were transcribed.\nThus, there were about 677 transcribed responses for each po-\ntential passage in the training set and about 325 in the test set.\nTable 1 lists the percentage of out-of-vocabulary words in\nthe RBLM building process. We can see that it decreased very\nquickly. In the ﬁnal model, there are still around 1.5% out-of-\nvocabulary words. Most of them are caused by wrong passage\nreadings, some unintelligible words, and partial sounds.\nStep 0 1 2 3 Final\nTrainingSet 16.8% 3.4% 1.7% 1.6% 1.47%\nTestingSet 16.7% 3.6% 2.0% 1.9% 1.52%\nTable 1: The percentage of out-of-vocabulary words in the train-\ning and test sets during different iteration steps.\nUsing the transcriptions in the training set, the ﬁnal rules\nand their probabilities were generated. We found that the fol-\nlowing rules were used frequently by the test takers: might is\nsubstituted by may with the probability 0.088; contraction for-\nmat (such as she’s) becomes no contraction format with the\nprobability 0.073; VBZ (verb, 3rd person singular present) is\nsubstituted by VB (verb, base form) with the probability 0.048;\nNNS (noun, plural) is substituted by NN (noun, singular or\nmass) with the probability 0.044; NN is substituted by NNS\nwith the probability 0.010; inserting mouth noise (#) at the be-\nginning and the end of the passage with the probability 0.296\nand 0.090 respectively; the could be inserted in the middle of\na structure like IN JJ NNS with the probability 0.094; mouth\nnoise (#) could happen after every word with the probability\n0.047; repeating two or three words happens more than three\ntimes more often than skipping two or three words; any word\ncould be replaced by its partial words; and so on.\nThe ﬁnal rule set was used to build RBLMs. The aver-\nage perplexity for these models was 1.73. When standard bi-\ngram language models were built for comparison, their average\nperplexity was 2.87. In both cases, we used a non-native tri-\nphone acoustic model to do the speech recognition. This acous-\ntic model was trained on a widely representive sample of non-\nnative spoken materials collected by Pearson. None of the data\nused in these LM experiments had been used for acoustic model\ntraining. In the test set (the total number of words is 348,681),\nwe achieved word error rate (WER) 9.0% by using the RBLM,\ncompared to a WER 12.6% when using a passage-speciﬁc bi-\ngram language model. This suggests that a non-speciﬁc rule-\nbased model extracted from 5600 reading performances pro-\nvides the recognizer with more accurate information than a bi-\ngram model based on around 677 reading performances on the\nspeciﬁc passage under study.\nBy taking advantage of the RBLM, for each passage read-\ning, Pearson provided rule-ﬁring details to the National Center\nfor Educational Statistics for further analysis. By analyzing the\nrule-ﬁring situation in context, test takers’reading errors includ-\ning real substitutions, omissions, insertions, self-corrections,\nand reversals, etc. can be accurately obtained using automatic\nmethods.\nIt is well-known that the children’s speech is substantially\nharder to be recognized than adults’ [7]. We tested the model\nto a children’s oral reading dataset. A total of 164 elementary\nschool students (46 ﬁrst graders, 62 third graders, and 56 ﬁfth\ngraders) were recruited from different parts of the United States,\nfrom a range of ethnic and linguistic backgrounds. Roughly half\nof the students were male and half were female. Each student\ntook the grade-appropriate Benchmark test (3 passages) yield-\ning 492 responses. Each response was 90 seconds in length. All\nthe responses were transcribed. The rules learned from previous\nadult data were used directly to build RBLMs. All the transcrip-\ntions here plus some from similar source were used to build bi-\ngram language models. The average number of transcriptions\nfor a bigram language model is 126. The average perplexty is\n2.22 for RBLMs and 4.22 for bigram language models. WER\nwas 13.1% by using the RBLM, compared to a WER 26.1%\nwhen using a passage-speciﬁc bigram language model.\nThis RBLM has also been successfully applied in a test of\noral English proﬁciency, Versant for English [8] that uses ASR\nand other automatic techniques to score reading, elicited im-\nmitations, and sentence construction tasks, again, without the\nusual transcription requirement for new items.\n7. Conclusions\nWe proposed an RBLM for reading recognition. This model\ncan be built by applying different linguistic rules to passage\ntexts. Our experimental results showed that the recognition per-\nformance of the RBLM was signiﬁcantly better than that of the\nbigram language model in passage reading task. After building\nenough general linguistic rules, the RBLM should be able to be\napplied to other low perplexity recognition domains without the\nrequirement of transcriptions. Thus, it signiﬁcantly reduces the\ntime and cost of human transcribers in developing automated\ntests that score spoken performances. The RBLM opens the\npossibility of automatically catching spoken grammar mistakes\nand making diagnostic suggestions.\n8. Acknowledgements\nThe part of work was performed as a subcontract to Westat, Inc.,\nunder a contract with the U.S. Department of Education.\n9. References\n[1] Balogh, J., Bernstein, J., Cheng, J., and Townshend, B., “Auto-\nmatic evaluation of reading accuracy: assessing machine scores”,\nIn SLaTE 2007, 112-115.\n[2] Mostow, J., Roth, S.F., Hauptmann, A.G., and Kane, M., “A pro-\ntotype reading coach that listens”, In AAAI 1994, 785-792.\n[3] Li, X., Deng, L., Ju, Y., and Acero, A., “Automatic children’s\nreading tutor on hand-held devices”, In Interspeech 2008, 1733-\n1736.\n[4] Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A., “Build-\ning a large annotated corpus of English: The Penn Treebank”,\nComputational Linguistics 19(2):313-330, 1993.\n[5] Viterbi, A., “Error bounds for convolutional codes and an asymp-\ntotically optimum decoding algorithm”, IEEE Trans. on Informa-\ntion Theory IT-13:260-269, 1967.\n[6] Dempster, N. M., Laird, A. P., and Rubin, D. B., “Maximum like-\nlihood from incomplete data via the EM algorithm”, J. R. Statist.\nSoc., B 39:185-197., 1977.\n[7] Li, Q. and Russel, M. An analysis of the causes of increased error\nrates in children’s speech recognition, In ICSLP 2002, 2337-2340.\n[8] Bernstein, J. and Cheng, J., “Logic and validation of a fully au-\ntomatic spoken English test”, In Holland, V. M. and Fisher, F.\nP. (Ed.), The Path of Speech Technologies in Computer Assisted\nLanguage Learning, 174-194, 2007.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8472265005111694
    },
    {
      "name": "Perplexity",
      "score": 0.7804222106933594
    },
    {
      "name": "Fluency",
      "score": 0.7530053853988647
    },
    {
      "name": "Language model",
      "score": 0.6494570374488831
    },
    {
      "name": "Reading (process)",
      "score": 0.6426736116409302
    },
    {
      "name": "Natural language processing",
      "score": 0.6390621066093445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6184672117233276
    },
    {
      "name": "Task (project management)",
      "score": 0.6157548427581787
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.47297704219818115
    },
    {
      "name": "Language identification",
      "score": 0.47202247381210327
    },
    {
      "name": "Rule-based system",
      "score": 0.4512379467487335
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4429628551006317
    },
    {
      "name": "Cache language model",
      "score": 0.4299122095108032
    },
    {
      "name": "Universal Networking Language",
      "score": 0.19632717967033386
    },
    {
      "name": "Natural language",
      "score": 0.17411041259765625
    },
    {
      "name": "Linguistics",
      "score": 0.15543892979621887
    },
    {
      "name": "Comprehension approach",
      "score": 0.10558897256851196
    },
    {
      "name": "Programming language",
      "score": 0.07494089007377625
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1283104182",
      "name": "Pearson (United States)",
      "country": "US"
    }
  ],
  "cited_by": 6
}