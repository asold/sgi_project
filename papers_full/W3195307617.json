{
  "title": "Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition",
  "url": "https://openalex.org/W3195307617",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287193486",
      "name": "Zheng, Xianrui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1851812986",
      "name": "Zhang Chao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2755011070",
      "name": "Woodland, Philip C.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950527759",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3015484572",
    "https://openalex.org/W3096471021",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W101286142",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2076094076",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W2155388323",
    "https://openalex.org/W2513257850",
    "https://openalex.org/W2747917286",
    "https://openalex.org/W2747236259",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2058695628",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2963654251",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2112036188",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W3141961557",
    "https://openalex.org/W2888867175",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3094841848",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2889177077",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2160451571"
  ],
  "abstract": "Language models (LMs) pre-trained on massive amounts of text, in particular bidirectional encoder representations from Transformers (BERT), generative pre-training (GPT), and GPT-2, have become a key technology for many natural language processing tasks. In this paper, we present results using fine-tuned GPT, GPT-2, and their combination for automatic speech recognition (ASR). Unlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct product of the output probabilities is no longer a valid language prior probability. A conversion method is proposed to compute the correct language prior probability based on bidirectional LM outputs in a mathematically exact way. Experimental results on the widely used AMI and Switchboard ASR tasks showed that the combination of the fine-tuned GPT and GPT-2 outperformed the combination of three neural LMs with different architectures trained from scratch on the in-domain text by up to a 12% relative word error rate reduction (WERR). Furthermore, on the AMI corpus, the proposed conversion for language prior probabilities enables BERT to obtain an extra 3% relative WERR, and the combination of BERT, GPT and GPT-2 results in further improvements.",
  "full_text": "ADAPTING GPT, GPT-2 AND BERT LANGUAGE MODELS FOR SPEECH RECOGNITION\nXianrui Zheng, Chao Zhang, Philip C. Woodland\nCambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K.\n{xz396, cz277, pcw}@eng.cam.ac.uk\nABSTRACT\nLanguage models (LMs) pre-trained on massive amounts of text, in\nparticular bidirectional encoder representations from Transformers\n(BERT), generative pre-training (GPT), and GPT-2, have become a\nkey technology for many natural language processing tasks. In this\npaper, we present results using ﬁne-tuned GPT, GPT-2, and their\ncombination for automatic speech recognition (ASR). Unlike unidi-\nrectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the cor-\nrect language prior probability based on bidirectional LM outputs\nin a mathematically exact way. Experimental results on the widely\nused AMI and Switchboard ASR tasks showed that the combination\nof the ﬁne-tuned GPT and GPT-2 outperformed the combination of\nthree neural LMs with different architectures trained from scratch on\nthe in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, on the AMI corpus, the proposed conversion\nfor language prior probabilities enables BERT to obtain an extra 3%\nrelative WERR, and the combination of BERT, GPT and GPT-2 re-\nsults in further improvements.\nIndex Terms— Bidirectional LM, GPT, GPT-2, BERT\n1. INTRODUCTION\nLanguage models (LMs) incorporate linguistic knowledge as the\nprior probabilities of word sequences, and are crucial for state-of-\nthe-art automatic speech recognition (ASR) systems. LMs provide\na way of leveraging additional text data in ASR [1]. Traditional\nn-gram LMs often suffer from data sparsity and are therefore re-\nstricted to use only a small number of previous words ( n ⩽ 5) (i.e.\ncontext) when estimating the prior probability of the next word in\na sentences. A solution is to build LMs with neural network (NN)\nmodels that can more reliably estimate sentence prior probabilities\nusing longer contexts given a certain amount of text training data.\nAlternatively, additional out-of-domain data can be leveraged to\nimprove LM training with limited in-domain data via LM adaptation\nand transfer learning [2–8].\nThe feed-forward NN (FNN) was the ﬁrst NN structure widely\nstudied for language modelling, and can be seen as an NN-based\nn-gram LM [9–12]. Later, recurrent neural network (RNN) models\nand the long short-term memory (LSTM) variant, which can make\npredictions based on the full history, were applied to language mod-\nelling for ASR [13–17].\nUsing an attention-mechanism is an alternative to RNNs for se-\nquence processing [18, 19]. Transformers, a widely used attention-\nbased sequence encoder-decoder model structure, were ﬁrst pro-\nposed for machine translation [20]. The Transformer decoder can\nbe used to build unidirectional LMs for ASR (referred to as Trans-\nformer LMs in this paper) [21]. The generative pre-training (GPT)\nmodel used the Transformer decoder structure to build an unidirec-\ntional LM. The parameters of GPT were ﬁrst pre-trained on very\nlarge general text corpora and released to the public [22]. When\napplied to a speciﬁc downstream natural language processing (NLP)\ntask, GPT is often ﬁne-tuned on a small amount of in-domain data.\nThis process allows the transfer of linguistic knowledge learned\nin pre-training to a task with a small amount of task-speciﬁc data.\nIn contrast to GPT, the bidirectional encoder representation from\nTransformers (BERT) model uses the Transformer encoder structure\nto build a pre-trained bidirectional LM, which leverages both for-\nward and backward context rather than only previous words when\ncomputing probabilities [23]. The success of these models has led\nto the study of many other types of pre-trained LM [24–32].\nDespite the wide-spread application of GPT and BERT in NLP\nand machine learning, there are only a very limited number of studies\non their use in ASR [5–8]. In this paper, we present ASR results ob-\ntained using GPT and GPT-2 that are ﬁne-tuned on in-domain data.\nThe WERs obtained by combining the ﬁne-tuned GPT and GPT-2\nLMs outperformed the combination of an FNN LM, an LSTM LM,\nand a Transformer LM trained only on in-domain data. Meanwhile,\nunlike the unidirectional LMs, simply multiplying the BERT output\nprobabilities over all words in a sentence does not result in a valid\nsentence prior probability. A novel method is proposed in this paper\nthat can convert the output probabilities of a bidirectional LM into\nexact sentence prior probabilities. This method is applied to BERT\nin our experiments, and is compared to a baseline method developed\nfor the same purpose [6, 33].\nThis paper is organised as follows. Section 2 reviews Trans-\nformer, GPT, GPT-2 and BERT. Section 3 presents our methods for\nLM combination and bidirectional LM output probability conver-\nsion. The experimental setup for the language model training and\nspeech recognition experiments on the widely used AMI and Switch-\nboard corpora are given in Sec. 4 and the experimental results are in\nSec. 5. Finally, conclusions are presented in Sec. 6.\n2. TRANSFORMER-BASED LMS\nThis section reviews the Transformer model structure, which is used\nby GPT, GPT-2, BERT, and the Transformer LM.\n2.1. Multi-head attention for Transformer\nThe Transformer model structure proposed in [20] is shown in Fig. 1,\nMultiHead(Q,K,V) = Concat(head1,..., headh)WO\nheadi = Attention(QWQ\ni ,KWK\ni ,VWV\ni ),\nAttention(Q,K,V) = softmax(QKT/\n√\ndk)V\nwhere K, Q, and V refer to the queries, keys, and values of the at-\ntention mechanism; MultiHead(·) and Concat(·) refers to multi-head\nTo appear in Proc. ASRU2021, December 13-17, 2021, Cartagena, Colombia © IEEE 2021\narXiv:2108.07789v2  [cs.CL]  1 Oct 2021\nattention and concatenation respectively; his the number of heads,\nWO\ni ∈Rdmodel×dmodel is a weight matrix of the ith head, and WQ\ni ,\nWK\ni and WV\ni have the same dimensions dmodel ×dk, where dmodel\nis the size of input embeddings and dk = dmodel/h. Attention (·) is\ntermed scaled dot-product attention since it weights the values based\non the dot-product of keys and queries.\nThe difference between Multi-Head Attention and Masked\nMulti-Head Attention is that the former allows the model to see the\nfuture context while the later does not, which are therefore used in\nthe encoder and decoder structures respectively. The Feed Forward\ncomponent consists of two fully-connected (FC) layers with a ReLU\nfunction in between. The Output component converts the output\nfrom the ﬁnal Transformer decoder block into probability distri-\nbutions using an FC layer with a softmax function. A positional\nencoding is added to each input embedding to include the order\ninformation of the input sequence.\nLayer Norm\nLayer Norm\nFeed Forward\nMulti-Head Attention\n+\n+\n+Input    Positional Encoding \nLayer Norm\nLayer Norm\nFeed Forward\nMulti-Head Attention\n+\n+\nOutput\n+Output    Positional Encoding \nLayer Norm\nMasked Multi-Head Attention\n+\n×N\n×N\nFig. 1. Transformer model structure with N encoder blocks (on the\nleft) and N decoder blocks (on the right).\n2.2. GPT\nGPT uses the Transformer decoder structure (shown in the right part\nof Fig. 1) [22]. Since the decoder structure is used alone without\nan encoder, the Multi-Head Attention and Layer Norm (layer nor-\nmalisation [34]) components that are connected to the encoder are\nremoved (in the middle of the right part of Fig. 1). The pre-trained\nGPT model has 12 Transformer blocks with dmodel =768 and 110M\nparameters. The positional encodings are learnt jointly during pre-\ntraining.\nRegarding GPT, its input is a token sequence w{i:i+n}and its\noutput is the probability distributions for the next tokensw{i+1:i+n+1}.\nTokenisation uses the byte pair encoding (BPE) with 40,000 sub-\nword units [35]. The maximum token sequence length is 512. GPT\nis pre-trained on the BooksCorpus dataset [36] for 100 epochs,\nwhich consists of one billion words from unpublished books cover-\ning many topics.\n2.3. GPT-2\nGPT-2 is the successor to GPT, which also uses the Transformer de-\ncoder structure [29]. In contrast to GPT, GPT-2 uses 50,257 BPE\ntokens and places the Layer Norm before the Masked Multi-Head\ncomponent. An additional Layer Norm is added after the ﬁnal block.\nThe maximum sequence length is increased from 512 to 1024. The\nmini-batch size during pre-training is increased from 64 to 512. Four\npre-trained GPT-2 models with different numbers of decoder blocks\nare available. The largest one has 48 blocks with dmodel = 1600 ,\nresulting in a total number of 1.5 billion model parameters.\nThe training dataset for GPT-2 is also different to that for GPT.\nBy gathering outbound links from Reddit with more than three\nkarma, the resulting training dataset has about ten billion words.\n2.4. BERT\nUnlike GPT, GPT-2, and Transformer LM, which use the Trans-\nformer decoder structure, BERT uses the Transformer encoder struc-\nture (see the left part of Fig. 1) [23]. Two FC output layers with\na Layer Norm component in between are placed after the ﬁnal en-\ncoder block. The estimation of the output probability of each to-\nken relies not only on the previous tokens but also on the future\nones, and BERT is therefore a bidirectional LM. Two versions of un-\ncased BERT are available. The small one has 12 encoder blocks with\ndmodel = 768 and the number of parameters is roughly the same as\nGPT, while the large one has 24 encoder blocks with dmodel = 1024\nand 336M parameters.\nTwo tasks are used to pre-train BERT. In the ﬁrst masked LM\ntask, 15% of the tokens are replaced by either the symbol [MASK]\nor a random token. If token iin a sequence is chosen to be replaced,\nit will have an 80% probability of being replaced by[MASK], a 10%\nprobability of being replaced by a random token and a 10% probabil-\nity of remaining unchanged. The goal is to predict the original token.\nThe second task is to predict if a sentence follows another sentence.\nBERT also uses the BooksCorpus dataset for pre-training.\n3. METHODOLOGY\n3.1. Combining unidirectional LMs\nFor all unidirectional LMs, the training objective is to minimise the\nlog perplexity (PPL) in Eqn. (1):\nlog2 PPL = −1\nT log2 P(w1:T )\n= −1\nT\n∑T\nt=1\nlog2 P(wt|w1:t−1), (1)\nwhere w1:T is a word sequence with T tokens. To combine multiple\nLMs for n-best rescoring for ASR, the Covariance Matrix Adapta-\ntion Evolution Strategy (CMA-ES) [37] can be used. CMA-ES is\nused here to optimise a set of LM score scaling factors λk (λk ⩾\n0,∀k) that minimise the development set WER. It samples sets of\n2\nscaling factors from a normal distribution in each iteration and up-\ndates the mean and covariance function based on the WER obtained.\nThe total score of combiningKLMs of a hypothesis is computed by\nAMScore +\n∑K\nk=1\nλk log P(k)(w1:T ), (2)\nwhere P(k)(w1:T ) is the probability estimated by the kth LM and\nAMScore is the acoustic model score.\n3.2. Converting bidirectional LM output probabilities\nIn ASR, the sentence prior probability P(w1:T ) is often calculated\nusing the chain rule of probability by\nP(w1:T ) = P(w1)\n∏T\nt=2\nP(wt|w1:t−1), (3)\nwhere P(wt|w1:t−1) requires only the previous tokens to predict the\ncurrent token and can thus be obtained using a unidirectional LM.\nConsequently, P(wt|w1:t−1) is often obtained by multiplying the\noutput probabilities of all tokens in the sequence that are produced\nby an unidirectional LM. For a bidirectional LM where both the pre-\nvious and future tokens are taken into account, this procedure results\nin\nΛ = P(w1|w2:T )P(w2|w1,w3:T ) ...P (wT |w1:T−1). (4)\nAlthough Λ ̸= P(w1:T ), Λ is sometimes directly used to replace\nP(w1:T ) in decoding [5, 6, 38], which we refer to as the modiﬁed\nmasked LM (MMLM). It was found in [33] that the MMLM output\ndistributions are overly-sharp, and hence should be smoothed either\nby interpolating with the output distributions from other LMs or by\nusing temperature softmax with a temperature factor α(α<1):\nTempSoftmax(z)|i = exp(αzi)/\n∑\nj\nexp(αzj), (5)\nwhere z is the bidirectional LM logit vector. It is assumed in [33]\nthat P(w1:T ) = Λ/Zwhere Zis a common normalisation constant\ncalculated over all possible word sequences and [39] learns similar\nnormalisation terms with noise contrastive estimation. In contrast,\n[40] approximates the exact sequence probability using a forward\nLM, a completion bidirectional LM, and a backward LM.\nNext, the conversion betweenΛ and P(w1:T ) for a bidirectional\nLM is discussed. Based on the deﬁnition of conditional probability,\nP(w1:T ) can be calculated as\nP(w1:T ) = P(w1|w2:T )P(w2:T )\nP(w1:T ) = P(w2|w1,w3:T )P(w1,w3:T )\n...\nP(w1:T ) = P(wT |w1:T−1)P(w1:T−1). (6)\nMultiplying all items in Eqn. (6) together yields\nP(w1:T ) = [ΛP(w2:T )P(w1,w3:T ) ...P (w1:T−1)]\n1\nT . (7)\nEach term P(w1:t−1,wt+1:T ) is the prior probability obtained by\napplying Eqn. (7) again over a token string obtained by removing the\ntth token from w1:T . Therefore, Eqn. (7) provides a recursive pro-\ncedure that converts the bidirectional LM output probabilities into\nthe exact sentence prior probability without using other LMs. This\napproach to ﬁnding the exact sentence prior probability from a bidi-\nrectional LM is presented for the ﬁrst time to the best of the authors’\nknowledge. A link between unidirectional and bidirectional LMs can\nbe found by equating the right hand sides of Eqn. (3) and Eqn. (7)\n(since both equal P(w1:T )). In this paper, w1:T for the bidirectional\nmodel refers to the words in the current sentence and words in other\nsentences can be given as extra contextC, i.e. all probabilities in this\nsection can be further conditioned on C. We omit this extra context\nCfor simplicity.\nRather than applying Eqn. (7) with T2 terms when generating\nP(w1:T ), a more efﬁcient calculation procedure is shown in Fig. 2,\nwhich avoids processing repeated token strings by following a spe-\nciﬁc order of calculation. Although this reduces the amount of com-\nputation to 0.5T2 + 1.5T −1, it is still computationally impractical\nwhen T is large. To apply the conversion in practice, we propose\nan approximation to trade-off between the cost and the context used\nin the bidirectional LM, which selects M (1⩽ M ⩽ T) items in\nEqn. (6) instead of all of them. An example with M =1 is depicted\nas the red path in Fig. 2, which is equivalent to using the bidirectional\nLM as an unidirectional LM when no tokens from neighbouring sen-\ntences are used as extra context. When extra context is provided, the\nprediction for the target token is conditioned on the tokens on the\nright and the extra context.\nP(w2|w3,4)\nP(w2|w1,3,4)\nP(w1|w2,3,4)\nP(w1|w2,4)\nP(w4)\n1\n0\n2\n3\n4\n5\n6\n7\n10\n8\n9\nP(w3|w1,2,4)\nP(w4|w1,2,3)\nP(w3|w1,2)\nP(w1|w3,4)\nP(w3|w4)\nP(w2|w4)\nP(w2|w1)\nP(w1)\nFig. 2. An example of the efﬁcient calculation procedure ofP(w1:4)\nfor a bidirectional LM presented in the form of a ﬁnite state acceptor.\nWords to the left within the current sentence are masked in the red\npath, while words to the right within the current sentence are masked\nin the blue path.\n4. EXPERIMENTAL SETUP\n4.1. Data\nThe training set in AMI corpus has 911k word tokens, the dev set\n(ADev) has 108K and the eval set (AEval) has 102K. A 13K word\nvocabulary was used for NN LMs trained from scratch on AMI.\nFurther experiments used the combined training sets from Switch-\nboard and Fisher transcripts (SWB+Fisher), which has a total of 27M\nwords with 30K words in the vocabulary, and are evaluated sepa-\nrately on the SWB and CallHome (CH) parts of the SWB evaluation\nset eval2000.\n4.2. Acoustic model and 100-best list\nAll WERs were obtained using the factorised time-delayed neural\nnetwork [41] acoustic model with residual connections [42], which\n3\nModel ADev AEval SWB CH\n4-gram 19.9 20.2 8.6 17.0\nFNN LM 19.4 19.5 7.9 15.8\nLSTM LM 18.2 17.9 6.7 13.7\nTransformer LM 18.4 18.4 6.6 13.7\nF ⊕L ⊕T 17.9 17.7 6.5 13.5\nTable 1. %WER on AMI (ADev and AEval) and on eval2000\n(SWB and CH) with different word level LMs trained from scratch\nto rescore the 100-best lists. F ⊕L ⊕T is the combination of the\nFNN, LSTM and Transformer LMs.\nwas trained using the lattice-free maximum mutual information cri-\nterion [43] following Kaldi recipes [44]. Neither data augmenta-\ntion nor speaker adaptation was used for AMI experiments while the\nacoustic model for SWB uses i-vector speaker adaptation and speed\nperturbation1. A statistical 4-gram model was used to produce the\nmost likely decoding hypotheses for each test utterance represented\nby a lattice. The 100-best lists were then extracted for rescoring by\nthe various LMs under investigation.\n4.3. LM Training Procedures\nAll LMs trained from scratch used word tokens and optimised by\nstochastic gradient descent using just the in-domain training data.\nThe pre-trained LMs, GPT, GPT-2 and BERT, were ﬁne-tuned using\nthe Adam optimiser [47] with only 3 epochs on the in-domain text\ndata. All NN LMs were implemented using PyTorch [48] and the\npre-trained models were obtained from [49].\n5. EXPERIMENTAL RESULTS\nIn our experiments with LMs trained from scratch on only in-domain\ndata, the contexts input to the FNN LM and Transformer LM are 5\nwords and 72 words on the left respectively, which minimise their\nperplexities on ADev. Feeding more context words as input to these\ntwo models leads to an increase in perplexity on ADev. A rescored 1-\nbest list was cached during the rescoring process for FNN and Trans-\nformer LMs so that they can have enough context by using words\nfrom the previous sentences. For the LSTM LM, the hidden vector\nof the last word of the rescored best hypothesis is used for rescoring\nthe next sentence n-best hypotheses as if giving LSTM LM unlim-\nited context. The same word embedding size of 256 is used for these\nthree LMs. We compared our AMI LMs with different depths and\nsizes, and found for the best-performing models, the FNN, LSTM,\nand Transformer LMs have 1 FC layer, 1 LSTM layer, and 8 de-\ncoder blocks correspondingly. For our experiments on SWB+Fisher,\nwe used 2 FC layers, 2 LSTM layers, and 24 decoder blocks for the\nFNN LM, LSTM LM, and Transformer LM respectively.\nTable 1 shows the results rescoring the100-best list using a com-\nbination of three different in-domain NN LMs. The best single in-\ndomain NN LM on AMI (LSTM) is 2.3% absolute WER lower than\nthat of the 4-gram on AEval, and the combination of the three NN\nLMs gives an extra 0.2% absolute reduction on AEval. The Trans-\nformer LM gives the best WER on SWB and CH, giving 2.0% and\n3.3% absolute reductions on SWB and CH respectively. The combi-\n1The Kaldi pipelines for AMI and SWB are from [45] and [46] respec-\ntively.\n20 40 60 80 100 120 140 160 180\nContext Length\n26.0\n28.0\n30.0\n32.0\n34.0\n36.0\n38.0Perplexity\nADev\nAEval\nFig. 3. Preplexity on AMI (ADev and AEval) for GPT ﬁne-tuned\nwith different context lengths. The context length is the number of\ntokens the model can see for giving an output.\n20 40 60 80 100 120 140 160 180\nContext Length\n16.0\n16.2\n16.4\n16.6\n16.8\n17.0\n17.2WER (%)\nADev\nAEval\nFig. 4. %WER on AMI (ADev and AEval) of GPT ﬁne-tuned with\ndifferent context lengths.\nnation of three NN LMs further reduces the WER by 0.1% and 0.2%\nabsolute over the Transformer LM on SWB and CH respectively.\n5.1. GPT and GPT-2\nThe same context length is used for both GPT and GPT-2, and a\n1-best list is also cached to serve as context during rescoring. To de-\ncide the length of context, we ﬁne-tune GPT with different context\nlengths ranging from 20 to 180 tokens. Since GPT was pre-trained\non much larger dataset, we believe it can exploit more context than\nthe Transformer LM trained from scratch. Fig. 3 shows that the per-\nplexity of the ﬁne-tuned GPT drops as the context length increases\nand the context length of 180 gives the lowest perplexity on both\nADev and AEval. A similar trend can be found for GPT WERs with\ndifferent context lengths in Fig. 4. The cost is that the time required\nfor rescoring GPT is more than the total time for rescoring the three\nNN LMs trained from scratch.\nWithout ﬁne-tuning, all GPT-2 models shown in Table 2 give\nlower WERs than F ⊕L ⊕T on AEval. The pre-trained 24-block\nGPT-2 outperforms F ⊕L ⊕T by 1.1% absolute WER on AEval.\nAfter ﬁne-tuning, the 24-block GPT-2 gives a 2.0% absolute WER\n4\nModel FT ADev AEval SWB CH\nF ⊕L ⊕T - 17.9 17.7 6.5 13.5\nGPT × 19.2 18.9 - -√ 16.5 16.0 6.3 13.1\nGPT-2 × 17.7 17.3 - -\n(12 blocks) √ 16.4 16.0 - -\nGPT-2 × 17.1 16.6 - -\n(24 blocks) √ 16.2 15.7 6.2 12.8\nGPT ⊕GPT-2 √ 16.0 15.6 6.1 12.7\nTable 2. %WER on AMI (ADev and AEval) and oneval2000 (SWB\nand CH) with GPT/GPT-2. “FT” indicates if the pre-trained model\nis ﬁne-tuned on in-domain data. F ⊕L ⊕T is from Table 1, which\nis trained on in-domain data only. The last line combines GPT with\nthe 24-block GPT-2.\nreduction over F ⊕L ⊕T on AEval. Both GPT and the 24-block\nGPT-2 are adapted to SWB+Fisher and evaluated on eval2000, the\nﬁne-tuned GPT outperforms F ⊕L ⊕T by 0.2% and 0.4% abso-\nlute WER on SWB and CH respectively. GPT-2 further reduces the\nWERs by 0.1% and 0.3% absolute on SWB and CH respectively.\nNext, the complementarity of GPT and GPT-2 are investigated\nby linearly interpolating the scores derived from the 24-block GPT-2\nand GPT. The resulting WERs decreased by 0.2% absolute on ADev\nand 0.1% absolute on AEval, compared to the WER using the ﬁne-\ntuned 24-block GPT-2 only. Combining the ﬁne-tuned GPT and 24-\nblock GPT-2 also showed an extra 0.1% absolute WER reduction on\nSWB and CH over a single 24-block GPT-2, reaching WERs of 6.1%\nand 12.7%2, which gives 2.5% and 4.3% absolute WER reductions\nover 4-grams on SWB and CH. We found that combining the in-\ndomain only word-level NN LMs with the ﬁne-tuned GPT and GPT-\n2 could not achieve a better WER.\n5.2. BERT\nBERT takes much longer to train than uni-directional models since\nevery word needs to be masked once and fed into the model with\nleft and right context. The experiments using BERT start without\nusing context from other sentences, which is the same as the work\nin [6]. Instead of prepending [CLS] and appending [SEP] to each\nsentence as in [5], we only append a period to the end of each sen-\ntence since there is no punctuation in the nbest list. We found that\nadding [CLS] and [SEP] to each sentence during ﬁne-tuning did\nnot improve the rescoring result. Since we will later allow the model\nto use the context from neighbouring sentences, having these two\nspecial tokens for every sentence reduces the number of tokens from\ntranscriptions under the same context length.\nSimilar to the MMLM, our proposed method in Section 3.2 also\nreplaces the word we want to predict by the special token [MASK],\nwhile the attention mask is used to control the context. Table 3 com-\npares our proposed method with MMLM without letting the model\nsee any tokens outside the current sentence. To match the computa-\ntion needed for MMLM, we use the simplest case of our method by\nsetting M = 1, where the attention mask allows BERT to see only\n2These WERs on SWB and CH eval2000 were, at the time that this pa-\nper was submitted, lower than the best result in https://github.com/\nsyhw/wer_are_we (6.8%/14.4% on SWB/CH) when a single acoustic\nmodel is used and trained on the Switchboard 300 hour set only [50].\nMethod α FT ADev AEval\nMMLM\n1 × 23.3 23.2\n1 √ 18.8 18.9\n0.7 √ 18.1 18.1\nOurs (M = 1)\n1 × 25.8 26.0\n1 √ 18.3 18.2\n0.7 √ 18.2 17.9\nOurs (M = 2) 0.7 √ 18.0 17.7\nTable 3. %WER on AMI (ADev and AEval) with the 12-block un-\ncased BERT using different methods to calculate the sentence prob-\nability without further context beyond the current sentence. “FT”\nindicates if the pre-trained model is ﬁne-tuned on in-domain data.\n“Ours” applies Eqn. (7) with different values ofM.\nthe future tokens in the current sentence. Our method with M = 2\ncomputes both the red path and the blue path in Fig 2. When there\nis no ﬁne-tuning, the performance is better using MMLM. This is\nexpected since the MMLM method is better aligned to the original\nMasked LM pre-training task. Compared to MMLM, our methods\ngives lower WERs after ﬁne-tuning, showing a 0.7% absolute WER\nreduction on AEval when using M = 1. We also set α = 0.7 (as\nin [33]) for decoding BERT using the MMLM method and this gives\na 0.8% absolute WER reduction on AEval after ﬁne-tuning, com-\npared to the MMLM result with α = 1. Our method also beneﬁts\nfrom applying smaller α. Compared to MMLM with α = 0.7, set-\nting M = 1 with α = 0 .7 gives a 0.2% absolute WER reduction\non AEval, and M = 2 with α = 0.7 gives a further 0.2% absolute\nreduction.\nUnlike GPT, GPT-2 and the NN LMs trained from scratch,\nwhich only use context from previous sentences, the context for\nBERT can come from both previous and future sentences. A\nrescored 1-best list is cached to serve as the left context. Since\nthe future hypotheses cannot be rescored before the current hy-\npothesis, the right context of the current hypothesis uses the 1-best\nhypotheses from the original 100-best list. For our method with\nM = 1, Table 4 shows that having 50 tokens on the left gives 0.7%\nand 0.6% absolute WER reductions on ADev and AEval respec-\ntively. Increasing the length of the left context to 100 does not give\nfurther WER reductions. However, using 50 tokens on the left and\n20 on the right outperforms using 100 tokens on the left without\ncontext on the right, yielding a 0.3% absolute WER reduction on\nboth ADev and AEval, which is the best result with M = 1.\nCompared to the best result using MMLM, which is when both\nsides have 100 tokens, our method with M = 1 gives a 0.3% abso-\nlute WER reduction on ADev and 0.5% reduction on AEval. Apply-\ning the best setup for our method with M = 1 on M = 2 achieves\nanother 0.2% and 0.1% absolute WER reduction on ADev and AE-\nval respectively. The second line for our method with M = 2 in\nTable 4 shows that if we improve the quality of the right context\nby using the reference transcriptions instead of the 1-best hypothe-\nses from the original 100-best list, the absolute WERs on ADev and\nAEval can be reduced by 0.2%. Future work could investigate ap-\nproaches to improve the quality of the right context without using\nthe reference. Since our method with M = 2 using the highlighted\nsetup in Table 4 gives the best result among all methods for ﬁne-\ntuning BERT, it is then combined with the GPT and the 24-block\nGPT-2. This combination gives 0.1% absolute WER reductions on\nboth ADev and AEval compared to the combination of GPT and\n5\nMethod LC RC ADev AEval\nMMLM\n0 0 18.1 18.1\n50 0 17.7 17.8\n100 0 17.6 17.6\n50 20 17.5 17.6\n100 100 17.5 17.5\nOurs (M = 1)\n0 0 18.2 17.9\n50 0 17.5 17.3\n100 0 17.5 17.3\n50 20 17.2 17.0\n50 50 17.3 17.1\n100 50 17.2 17.1\n100 100 17.2 17.1\nOurs (M = 2)\n0 0 18.0 17.7\n50 20 17.0 16.9\n50 20† 16.8 16.7\nGPT ⊕GPT-2 ⊕BERT 15.9 15.5\nTable 4. %WER on AMI (ADev and AEval) with the ﬁne-tuned 12-\nblock uncased BERT using different methods to calculate the sen-\ntence probability with context. “LC” and “RC” indicates context\nlength on the left and right respectively. αis set to 0.7 for rescoing\nusing BERT models. 20†means 20 future tokens from the reference.\nThe last line combines GPT, 24-block GPT-2 and BERT model using\nthe highlighted setup of our method with M = 2.\nGPT-2 only, showing that bidirectional LMs are complementary to\nunidirectional LMs.\n6. CONCLUSIONS\nThis paper investigates the use of both unidirectional and bidi-\nrectional NN LMs with different model architectures and training\nstrategies for ASR with speech recognition experiments on both the\nwidely used AMI and Switchboard corpora. Regarding unidirec-\ntional LMs, it was shown that ﬁne-tuning the pre-trained GPT and\nGPT-2 LMs on small and medium sized in-domain datasets outper-\nformed the combination of FNN LM, LSTM LM, and Transformer\nLM that are all trained from scratch with only in-domain data. The\nWERs can be further reduced by combining the ﬁne-tuned GPT\nand GPT-2. Compared to the 4-gram baseline, combining GPT and\nGPT-2 gives 4.6% absolute reductions in WER on the AMI eval\nset and 2.5%/4.3% on the Switchboard/Call Home portions of the\neval2000 data. Hence, rather than building new NN LMs with task-\nspeciﬁc data for ASR, it can be better and perhaps faster to ﬁne-tune\nthe existing unidirectional LMs that have been pre-trained on huge\namounts of out-of-domain data. However, these large pre-trained\nmodels require much longer rescoring time, so it is worth con-\nsidering some optimisation methods during inference. Regarding\nbidirectional LMs, a conversion method is proposed that can com-\npute the exact sentence prior probabilities required by ASR based\non the output proabilities produced by a bidirectional LM, such\nas BERT. The lowest WERs were achieved by further combining\nﬁne-tuned GPT, GPT-2, and BERT together.\n7. REFERENCES\n[1] F. Jelinek, “Statistical methods for speech recognition,” MIT\npress, 1997.\n[2] R. Iyer, M. Ostendorf, and H. Gish, “Using out-of-domain\ndata to improve in-domain language models,” IEEE Signal\nProcessing Letters, vol. 4, no. 8, 1997.\n[3] S. R. Gangireddy, P. Swietojanski, P. Bell, and S. Renals, “Un-\nsupervised adaptation of recurrent neural network language\nmodels,” in Proc. Interspeech, 2016.\n[4] M. Ma, M. Nirschl, F. Biadsy, and S. Kumar, “Approaches\nfor neural-network language model adaptation,” in Proc. In-\nterspeech, 2017.\n[5] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, “Masked\nlanguage model scoring,” in Proc. ACL, 2020.\n[6] J. Shin, Y . Lee, and K. Jung, “Effective sentence scoring\nmethod using BERT for speech recognition,” in Proc. ACML,\n2019.\n[7] S.-H. Chiu and B. Chen, “Innovative BERT-based reranking\nlanguage models for speech recognition,” in Proc. SLT, 2021.\n[8] K. Li, Z. Liu, T. He, H. Huang, F. Peng, D. Povey, and S. Khu-\ndanpur, “An empirical study of transformer-based neural lan-\nguage model adaptation,” in Proc. ICASSP, 2020.\n[9] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural\nprobabilistic language model,” Journal of machine learning\nresearch, 2003.\n[10] H. Schwenk, “Continuous space language models,” Computer\nSpeech & Language, vol. 21, no. 3, 2007.\n[11] J. Park, X. Liu, M. Gales, and P. Woodland, “Improved neural\nnetwork based language modelling and adaptation,” in Proc.\nInterspeech, 2010.\n[12] H.-S. Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and F. Yvon,\n“Structured output layer neural network language models for\nspeech recognition,”IEEE Transactions on Audio, Speech, and\nLanguage Processing, vol. 21, no. 1, 2013.\n[13] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and S. Khu-\ndanpur, “Recurrent neural network based language model,” in\nProc. Interspeech, 2010.\n[14] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, and S. Khu-\ndanpur, “Extensions of recurrent neural network language\nmodel,” in Proc. ICASSP, 2011.\n[15] M. Sundermeyer, R. Schluter, and H. Ney, “LSTM neural net-\nworks for language modeling,” in Proc. Interspeech, 2012.\n[16] S. Merity, N. S. Keskar, and R. Socher, “Regularizing and\noptimizing LSTM language models,”arXiv:1708.02182, 2017.\n[17] M. Sundermeyer, I. Oparin, J.-L. Gauvain, B. Freiberg,\nR. Schl¨uter, and H. Ney, “Comparison of feedforward and re-\ncurrent neural network language models,” in Proc. ICASSP,\n2013.\n[18] A. Graves, G. Wayne, and I. Danihelka, “Neural turing ma-\nchines,” arXiv:1410.5401, 2014.\n[19] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” inProc. ICLR,\n2015.\n6\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” in Proc. NIPS, 2017.\n[21] K. Irie, A. Zeyer, R. Schl ¨uter, and H. Ney, “Language model-\ning with deep transformers,” in Proc. Interspeech, 2019.\n[22] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\n“Improving language understanding by generative pre-\ntraining,” OpenAI Blog, 2018.\n[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” arXiv:1810.04805, 2019.\n[24] J. Howard and S. Ruder, “Universal language model ﬁne-\ntuning for text classiﬁcation,” in Proc. ACL, 2018.\n[25] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,\n“RoBERTa: A robustly optimized BERT pretraining ap-\nproach,” arXiv:1907.11692, 2019.\n[26] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and\nR. Salakhutdinov, “Transformer-XL: Attentive language mod-\nels beyond a ﬁxed-length context,” in Proc. ACL, 2019.\n[27] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee, and L. Zettlemoyer, “Deep contextualized word repre-\nsentations,” in Proc. NAACL, 2018.\n[28] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and\nQ. V . Le, “XLNet: Generalized autoregressive pretraining for\nlanguage understanding,” in Proc. NIPS, 2020.\n[29] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, “Language models are unsupervised multitask\nlearners,” OpenAI Blog, 2019.\n[30] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and\nR. Soricut, “ALBERT: A lite BERT for self-supervised learn-\ning of language representations,” in Proc. ICLR, 2020.\n[31] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text transformer,”\nJournal of machine learning research, 2020.\n[32] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter,\nC. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” in\nProc. NIPS, 2020.\n[33] X. Chen, A. Ragni, X. Liu, and M. Gales, “Investigating bidi-\nrectional recurrent neural network language models for speech\nrecognition,” in Proc. Interspeech, 2017.\n[34] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\nin Proc. NIPS, 2016.\n[35] R. Sennrich, B. Haddow, and A. Birch, “Neural machine trans-\nlation of rare words with subword units,” in Proc. ACL, 2016.\n[36] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun,\nA. Torralba, and S. Fidler, “Aligning books and movies: To-\nwards story-like visual explanations by watching movies and\nreading books,” in Proc. ICCV, 2015.\n[37] N. Hansen and A. Ostermeier, “Completely derandomized\nself-adaptation in evolution strategies,” Evolutionary Compu-\ntation, vol. 9, no. 2, 2001.\n[38] E. Arisoy, A. Sethy, B. Ramabhadran, and S. Chen, “Bidirec-\ntional recurrent neural network language models for automatic\nspeech recognition,” in Proc. ICASSP, 2015.\n[39] T. He, Y . Zhang, J. Droppo, and K. Yu, “On training bi-\ndirectional neural network language model with noise con-\ntrastive estimation,” in Proc. ISCSLP, 2016.\n[40] K. Irie, Z. Lei, L. Deng, R. Schl ¨uter, and H. Ney, “Investiga-\ntion on estimation of sentence probability by combining for-\nward, backward and bi-directional LSTM-RNNs,” in Proc. In-\nterspeech, 2018.\n[41] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. Yarmoham-\nmadi, and S. Khudanpur, “Semi-orthogonal low-rank matrix\nfactorization for deep neural networks,” in Proc. Interspeech,\n2018.\n[42] F. Kreyssig, C. Zhang, and P. Woodland, “Improved TDNNs\nusing deep kernels and frequency dependent grid-RNNs,” in\nProc. ICASSP, 2018.\n[43] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, V . Manohar,\nX. Na, Y . Wang, and S. Khudanpur, “Purely sequence-trained\nneural networks for ASR based on lattice-free MMI,” in Proc.\nInterspeech, 2016.\n[44] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlıc/caron.ts1ek, Y . Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, “The Kaldi speech\nrecognition toolkit,” in Proc. ASRU, 2011.\n[45] P. Manakul, M. Gales, and L. Wang, “Abstractive spoken docu-\nment summarization using hierarchical model with multi-stage\nattention diversity optimization,” in Proc. Interspeech, 2020.\n[46] Q. Li, C. Zhang, and P. C. Woodland, “Combining frame-\nsynchronous and label-synchronous systems for speech recog-\nnition,” arXiv preprint arXiv:2107.00764, 2021.\n[47] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” in Proc. ICLR, 2015.\n[48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\nA. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-\njani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chin-\ntala, “PyTorch: An imperative style, high-performance deep\nlearning library,” in Advances in Neural Information Pro-\ncessing Systems, H. Wallach, H. Larochelle, A. Beygelzimer,\nF. dAlch´e-Buc, E. Fox, and R. Garnett, Eds., 2019, vol. 32.\n[49] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davi-\nson, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu,\nT. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush,\n“Transformers: State-of-the-art natural language processing,”\nin Proc. EMNLP, 2020.\n[50] W. Wang, G. Wang, A. Bhatnagar, Y . Zhou, C. Xiong, and\nR. Socher, “An investigation of phone-based subword units for\nend-to-end speech recognition,” in Proc. Interspeech, 2020.\n7",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7595094442367554
    },
    {
      "name": "Language model",
      "score": 0.644074022769928
    },
    {
      "name": "Speech recognition",
      "score": 0.6125446557998657
    },
    {
      "name": "Transformer",
      "score": 0.587323784828186
    },
    {
      "name": "Encoder",
      "score": 0.5253072381019592
    },
    {
      "name": "Word error rate",
      "score": 0.47410067915916443
    },
    {
      "name": "Artificial intelligence",
      "score": 0.417208194732666
    },
    {
      "name": "Natural language processing",
      "score": 0.3769001364707947
    },
    {
      "name": "Electrical engineering",
      "score": 0.07550892233848572
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}