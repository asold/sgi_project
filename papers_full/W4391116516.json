{
  "title": "Exploring the Path from Instructions to Rewards with Large Language Models in Instance-Based Learning",
  "url": "https://openalex.org/W4391116516",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5106024083",
      "name": "Chase McDonald",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A3084329797",
      "name": "Tyler Malloy",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2150649989",
      "name": "Thuy Ngoc Nguyen",
      "affiliations": [
        "University of Dayton"
      ]
    },
    {
      "id": "https://openalex.org/A2152204186",
      "name": "Cleotilde Gonzalez",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3140479524",
    "https://openalex.org/W2139612737",
    "https://openalex.org/W1528459956",
    "https://openalex.org/W2143292369",
    "https://openalex.org/W2346736747",
    "https://openalex.org/W2096130600",
    "https://openalex.org/W2335959470",
    "https://openalex.org/W2144257734",
    "https://openalex.org/W6677916085",
    "https://openalex.org/W2005566784",
    "https://openalex.org/W2114927477",
    "https://openalex.org/W1985000262",
    "https://openalex.org/W4320342509",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W2963262099",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W2613347565",
    "https://openalex.org/W4378498695",
    "https://openalex.org/W2950993296",
    "https://openalex.org/W4384644642",
    "https://openalex.org/W3034769194"
  ],
  "abstract": "A prominent method to model human learning is through experiential learning, where decisions are influenced by the outcomes observed in previous actions. The decisions-from-experience approach often excludes other forms of learning in humans, such as learning from descriptive information. In humans, descriptive information can enhance learning by providing a denser signal, achieved through understanding the relationship between intermediate decisions and their future outcomes, instead of relying solely on observed outcomes. To account for experiential and descriptive information, we propose the use of large language models (LLMs) to convert descriptive information into dense signals that can be used by computational models that learn from experience. Building on past work in cognitive modeling, we utilize task instructions and prompt an LLM to define and quantify the critical actions an agent must take to succeed in the task. In an initial experiment, we test this approach using an Instance-Based Learning cognitive model of experiential decisions in a gridworld task. We demonstrate how the LLM can be prompted to provide a series of actions and relative values given the task instructions, then show how these values can be used in place of sparse outcome signals to improve the model’s learning of the task significantly.",
  "full_text": "Exploring the Path from Instructions to Rewards with Large Language Models in\nInstance-Based Learning\nChase McDonald1, Tyler Malloy1, Thuy Ngoc Nguyen2, Cleotilde Gonzalez1\n1 Carnegie Mellon University\n2 University of Dayton\ncoty@cmu.edu\nAbstract\nA prominent method to model human learning is through\nexperiential learning, where decisions are influenced by the\noutcomes observed in previous actions. The decisions-from-\nexperience approach often excludes other forms of learning\nin humans, such as learning from descriptive information. In\nhumans, descriptive information can enhance learning by pro-\nviding a denser signal, achieved through understanding the\nrelationship between intermediate decisions and their future\noutcomes, instead of relying solely on observed outcomes.\nTo account for experiential and descriptive information, we\npropose the use of large language models (LLMs) to convert\ndescriptive information into dense signals that can be used by\ncomputational models that learn from experience. Building\non past work in cognitive modeling, we utilize task instruc-\ntions and prompt an LLM to define and quantify the critical\nactions an agent must take to succeed in the task. In an initial\nexperiment, we test this approach using an Instance-Based\nLearning cognitive model of experiential decisions in a grid-\nworld task. We demonstrate how the LLM can be prompted to\nprovide a series of actions and relative values given the task\ninstructions, then show how these values can be used in place\nof sparse outcome signals to improve the model’s learning of\nthe task significantly.\nIntroduction\nHuman learning is commonly modeled through decisions\nfrom experience or trial-and-error interactions with an en-\nvironment. In this paradigm, agents often learn tabula rasa\nwith the need to discover all the effects that their actions\ncan have in the environment. There are a number of reasons\nwhy such an approach may be undesirable. For instance, the\nlearning agent will need to explore unsafe actions, and learn-\ning is significantly slower and inefficient in such models\nwhen prior knowledge about the world is not incorporated\ninto the decision making process.\nWhen humans learn a new task, they are capable of learn-\ning from non-experiential sources of information, such as\ninstructions (Walsh and Anderson 2011b). In this work, we\npropose the use of LLMs to parse instructions to extract re-\nward signals that guide experiential learning. Specifically,\nwe show how an LLM can be prompted with task instruc-\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ntions to provide both a set of steps that an agent should com-\nplete to maximize their task success and the values associ-\nated with each step. We then demonstrate how these rewards\ncan be used in place of a sparse outcome signal in a task to\nsignificantly improve learning.\nWe briefly review related concepts and literature, then\ndescribe and discuss our methods, experiment, results, and\npaths for future investigation.\nTemporal Credit Assignment Temporal credit assign-\nment is the process of attributing credit to past actions re-\nsponsible for the experienced rewards or future outcomes.\nThis process is crucial for learning and making decisions in\ndynamic environments over time with feedback delays (Sut-\nton and Barto 2018; Fu and Anderson 2008). Many stud-\nies in reinforcement learning and cognitive science have ex-\nplored different aspects of temporal credit assignment, in-\ncluding integrating hierarchical structures and intrinsic mo-\ntivation to enhance the learning process (Kulkarni et al.\n2016), as well as whether existing credit assignment mecha-\nnisms are well aligned with human behavior (Gershman and\nDaw 2017; Walsh and Anderson 2011a; Nguyen, McDon-\nald, and Gonzalez 2023).\nTemporal credit assignment can be bolstered by providing\nappropriate signals to the agents—in the form of intrinsic\nrewards—that provide supplementary information about the\neffect that actions in a sequence have on observed outcomes.\nThis has received significant attention in the reinforcement\nlearning literature, from explorations of intrinsic rewards for\ncooperative social learning (Hughes et al. 2018) to long-term\ncredit assignment (Zheng et al. 2020).\nIntrinsic Motivation Studies in psychology have shown\nthat individuals do not always solely aim to maximize their\nutility (Dovidio 1984). In such cases, it is often assumed that\nparticipants or agents are intrinsically motivated. Hence, in-\ntrinsic rewards could be an alternative to external rewards,\nparticularly in environments with sparse signals for rewards\nor success (Chentanez, Barto, and Singh 2004). In studying\nboth human intrinsic motivations and modeling these moti-\nvations computationally, a key challenge is in knowing how\nto construct these internal rewards. Among numerous alter-\nnative sources (e.g., from social preferences in behavioral\neconomics (List 2006) to preferences for self-efficacy (Blain\nand Sharot 2021)), past work has shown that in goal-oriented\nAAAI Fall Symposium Series (FSS-23)\n334\nsettings, such supplementary motivations may come from\nsub-goals based on an individual’s understanding and break-\ndown of their overall goal (Huang, Jin, and Zhang 2017).\nWith the understanding that humans have intrinsic mo-\ntivations guided by their preferences and knowledge that\ndrive their behavior, we can follow extensive existing work\nin computational modeling and use intrinsic signals to im-\nprove learning over temporally extended tasks. In order to\ndo so, we must formalize a process that takes information\nabout the task and any relevant world knowledge and dis-\ntills it into concrete intrinsic reward structures. To that end,\nwe utilize task instructions and LLMs: the former provides\nthe context of the task and the latter utilizes natural language\nunderstanding and reasoning to decompose the task into sub-\ngoals for the agent.\nLLMS, Learning, & Decision Making LLMs gained\nprominence through their performance in natural language\nsettings; however, there is a growing body of literature that\ndemonstrates their efficacy in control and decision making\ntasks, and in providing useful quantifications that may be\ndifficult for humans to produce.\nTo the latter point, Park et al. (2023) develop a social sim-\nulation that relies on LLM prompting for agents to select\nactions and communicate with one another. As part of their\nagent architecture, they utilize LLM queries to ask how rel-\nevant and important particular pieces of information are to\nthe decision at hand—they automate these processes that are\noften difficult to formalize in computational experiments by\nsimply querying the LLM. They demonstrate that these val-\nues result in meaningful actions from the agents and emer-\ngent social behavior in a population.\nWu et al. (2023a) develop an approach to provide in-\ntrinsic reward signals to reinforcement learning agents in\nAtari games by via the instruction manuals. They utilize a\nquestion-answering extraction module to summarize game\ninstructions and a reasoning module to determine if particu-\nlar agent interactions are valuable in the current state of the\ngame. Based on the reasoning modules assessment (a “Yes”\nor “No” response if reward should be provided), fixed in-\ntrinsic rewards or penalties are provided to a reinforcement\nlearning agent during training. They demonstrate that such\nan approach can provide significant gains in terms of learn-\ning speed and overall performance when compared to base-\nlines without their instruction-based reward.\nIn our work, we combine the aforementioned ideas\nto build a mechanism for developing goal-based intrin-\nsic rewards that allow an agent to utilize non-experiential\ninformation. This allows for improved temporal credit\nassignment—and in turn, learning—in sequential tasks with\nsparse rewards.\nMethods\nExperimental Task We use a modified version of the\nDoor Key task from the MiniGrid environment (Chevalier-\nBoisvert et al. 2023). The environment is formalized as a\npartially observable Markov Decision Process (POMDP),\nwhich is represented by the tuple (S, A, O, T , R, Ω, γ).\nHere, S represents the state space of the environment, A the\naction space, O the observation space, T : S × A → S\nthe transition function, R : S × A →R the reward func-\ntion, Ω : S → Othe observation function, and γ ∈ [0, 1)\nthe discount factor. In a POMDP, the goal is for a policy, at\nevery step t ∈ {0, ..., T} to select an action at ∈ Aafter\nobserving observation ot ∈ Oto maximize the discounted\nreturn PT\ni=t γi−tri, where rt ∈ R is the reward observed\nafter taking an action and the environment transitioning to a\nnew state.\nIn our modified Door Key task, there is an agent, a key,\na door, and two targets: one green and one purple. In each\nepisode, the agent must navigate through the environment\nto collect either the green or the purple target. The episodes\nterminate either when a target has been collected or the time\nhorizon T has been reached. The action space A is com-\nprised of six actions: Move North, Move South, Move East,\nMove West, Pick Up/Drop, and Toggle. The agent faces one\nof four cardinal directions and navigates the environment\nthrough one of the four Move actions. If the agent attempts\nto move in a direction that it isn’t currently facing, its orien-\ntation will change to the desired direction. If the orientation\nis aligned with the desired movement, the agent will move\none unit in the desired direction—given that they are unob-\nstructed. When an agent is facing and one unit away from\nthe key, it can use the Pick Up/Drop action to pick it up.\nThe same action will drop the key if the space in front of the\nagent is empty. When holding the key, the agent can unlock\nand open the door by using the Toggle action when facing\nthe door. The same action will close the door, but it cannot\nbe locked again. Finally, to collect a target, the agent must\nuse the Toggle action when facing a target.\nObservations of the environment take a simple form: in\neach observation ot, the agent observes its coordinates in the\ngrid, its current direction, an indicator if the agent is holding\na key, and an identifier for the object in the cell directly in\nfront of the agent.\nFor our preliminary investigation, we only use a single\nlayout depicted in Figure 1. It is important to note that the\nobservation representation is not generalizable across grids\n(e.g., the coordinates are only useful insofar as their sur-\nroundings are fixed), and is constructed as a simple repre-\nsentation for our demonstration.\nInstance-Based Learning Theory Actions of the agent in\nthe Door Key task are determined using a cognitive model\nof learning and dynamic decision making based on Instance\nBased Learning Theory (IBLT). This theory is related to the\nACT-R cognitive architecture through the activation func-\ntion, which is ultimately used to predict the estimated utility\nof performing an action in a state based on the utility out-\ncomes of similar past experiences held in declarative mem-\nory (Thomson et al. 2015). We refer to a computational\nmodel implementing IBLT as an IBL model.\nIn IBLT, declarative memory consists of instances k =\n(o, a, x) represented by the observation that describes the\nstate of the environmento, the action performed by the agent\na, and the utility outcome of that action x.\nAgent actions are determined by maximizing the value\nVk,t of an available action a in an instance k performed at\n335\nFigure 1: A rendering of the full game state of the modi-\nfied Door Key task. The blue triangle is the agent pointing\nwest, three units to its east is a key, and a locked door is\nrepresented by the dark gray block. The light gray cells are\nimpassable walls. The purple and green circles represent tar-\ngets, which upon collection earn the agent a reward of 0.4\nand 0.1, respectively.\ntime-step t, calculated using the “blending” function (Gon-\nzalez, Lerch, and Lebiere 2003):\nVk,t =\nnk,tX\ni=1\npi,k,txi,k,t (1)\nwhere nk,t are the previously generated instances held in\nprocedural memory, xi,k,t are the outcomes of those in-\nstances, and pi,k,t is the probability of retrieving an instance\nin memory, calculated by Equation 2.\npi,k,t = exp (Λi,k,t/τ)\nPnk,t\nj=1 exp (Λj,k,t/τ) (2)\nFurther, Λi,k,t is given by Equation 3.\nΛi,k,t = ln\n\u0012 X\nt′∈Ti,k,t\n(t − t′)−d\n\u0013\n+ σ ln 1 − ξi,k,t\nξi,k,t\n(3)\nHere, d and σ are decay and noise parameters, and Ti,k,t ⊂\n{0, ..., t− 1} is the set of previous timesteps where instance\nk was stored in memory. The ξi,k,t term is used to capture\nnoise in the individual differences in memory recall. Be-\ncause of the relationship between noise σ and temperature\nτ in IBLT, the temperature parameter τ is typically set to\nσ\n√\n2. In our experiments, we use all default parameters of\nd = 0.25 and σ = 0.5. We also set the default utility to 1.0\nto encourage exploration through an optimistic prior Sutton\nand Barto (2018). The default utility is used to predict the\nutility of an instance when there are no similar instances in\nmemory to estimate the expected utility.\nA key aspect of applying IBLT to modeling decision mak-\ning is determining the utility outcome of actions, which can\neither be directly provided by the learning environment or\ndetermined by the cognitive modeler. Nguyen, McDonald,\nand Gonzalez (2023) investigate several methods for tem-\nporal credit assignment in IBL models, demonstrating the\nrelative efficacy of each. We adopt the simple method of as-\nsigning outcomes at the end of each episode with their expo-\nnentially discounted future return. Formally, theith choice is\nassigned outcome xi = PT\nt=i γt−irt.\nLLM Reward Model We construct a reward model by\nprompting a pre-trained LLM. For our experiments, we use\nOpenAI’s gpt-3.5-turbo. Drawing on previous suc-\ncesses in utilizing LLMs in control settings (e.g., Wu et al.\n(2023a,b); Park et al. (2023); Ahn et al. (2022)), we query\nthe LLM to produce a plan or sub-goals that are critical for\nsuccess in the task. In contrast to Wu et al. (2023a), where\nthe LLM decides whether or not a fixed reward should be\nprovided to an agent, we also query the LLM to provide\nthe value of the reward. Specifically, we frame our query in\nterms of steps that the agent must achieve in order to reach\nits goal of reward maximization. The full query is as fol-\nlows, with {instructions} being replaced by the text\nprovided in Appendix :\nYour job is to evaluate a game and\ngenerate a step-by-step plan for the\nplayer to achieve the maximum score\npossible.\nThe instructions are:\n⟨begin instructions⟩\n{instructions}\n⟨end instructions⟩\nBased on the instructions, identify the\nmost important steps that the player\nmust meet in order to maximize their\nreward, and provide a value from 0 to 1\nthat represents the value of that step\nto reward maximization.\nThe list of steps should be in the\nfollowing format:\n1. Player State: ⟨step in terms of\nthe state of the game or player\nobservation⟩. Value: ⟨value between 0\nand 1⟩\nDue to the brevity of our task instructions, context length\nis not prohibitive and we are able to input the instructions\nin their entirety along with our reward prompt. In alternative\ncontexts, such summarization techniques may be required,\nas in, e.g., Wu et al. (2023a,b).\nIn order to provide a reward to the agent, we construct an\nenvironment parser that identifies when each step in the plan\nhas been accomplished and provides the associated reward\nto the agent. In an idealized setting, this process would be\nconducted via an additional LLM query, where the environ-\nment state or agent trajectory would be input into the model\nalong with a prompt that elicits an evaluation of each step in\nthe plan. The latency and cost associated with many queries\nmake such an automated process prohibitive.\nWithin each episode, we consider the full set of steps pro-\nvided by the initial LLM query. Once a step has been evalu-\nated to have been completed, the associated value is returned\nto the agent as a reward and that step is no longer considered\nuntil the subsequent episode. An example set of steps and\nvalues, utilized in our experiments, is provided below:\n336\n1. Player State: Identify the location\nof the Green target. Value: 0.2\n2. Player State: Identify the location\nof the Purple target. Value: 0.2\n3. Player State: Identify the location\nof the key. Value: 0.1\n4. Player State: Identify the location\nof the door. Value: 0.1\n5. Player State: Collect the Green\ntarget. Value: 0.3\n6. Player State: Collect the Purple\ntarget. Value: 0.5\n7. Player State: Collect the key.\nValue: 0.2\n8. Player State: Open the door using\nthe Toggle action. Value: 0.3\n9. Player State: Navigate to the Green\ntarget. Value: 0.4\n10. Player State: Navigate to the\nPurple target. Value: 0.6\nThe setting is sufficiently simple that these intrinsic re-\nwards may have been hand-crafted. Our demonstration fo-\ncuses on the efficacy of the elicitation of these steps and\nvalues from an LLM. We also note, in this LLM response,\nthere is redundancy in the steps: steps (1) and (2) are oper-\nationalized in the same way as (9) and (10): identifying the\nlocation of the targets requires the agent to navigate to them.\nIn our implementation, the first occurrence of navigation to\nthe target is validated as identification in (1) and (2), and any\nsubsequent arrivals are validated with steps (9) and (10).\nThe LLM reward model RLLM : S × A →R can be\nformalized as the following. Let the context C by defined as\nthe combination of the task instructions and previously de-\nscribed query. The LLM modelM takes as input the context\nand produces a set of k rewarded steps P = {p1, ..., pk} and\nthe values associated with each step V = {v1, ..., vk}. Fi-\nnally, let E : S ×P → {True, False} be environment parser\nthat takes an input the state s ∈ Sof the POMDP and a\nstep p ∈ Pand returns a boolean indicating whether or not\nthat step has been satisfied. In each environment episode, we\ninitialize the set of remaining steps Premaining := P to be\nthe full set of steps provided by the LLM. At every timestep\nt, we check whether any remaining step has been satisfied\nE(st, pk}∀pk ∈ Premaining. For any pk that is satisfied, the\nreward vk is returned to the agent and pk is removed from\nthe remaining set. That is,Premaining ← Premaining\\{pk}.\nIn our experiments, the reward model entirely replaces the\nenvironmental reward function R when used.\nResults\nFor our experiment, we ran 50 independent trials of an IBL\nmodel learning in two conditions in a single configuration\nof the task, as shown in Figure 1. Our two conditions are\ndefined by the source of reward: either the environmen-\ntal reward dictated by POMDP or the LLM reward. Each\ntrial consists of 200 episodes, each having a maximum of\nT = 250 timesteps. Each episode ends when the maximum\nnumber of timesteps elapses or a target is collected. Our out-\ncome of interest is the environmental reward in both condi-\nFigure 2: The top panel shows the average environment re-\nward earned in each episode by the IBL model that utilizes\nonly environment rewards and the IBL model that utilizes\nonly the LLM reward. The bottom panel shows the LLM\nreward earned by the IBL agent that received that reward\nsignal. The shaded region shows the standard error over 50\nindependent trials.\ntions.\nThe results of the experiment in terms of average reward\nover time are shown in Figure 2. The baseline IBL model\nwith the environmental reward fails to learn to reach a target\nin the vast majority of trials. The baseline model reaches a\nmaximum average reward of 0.008 ± 0.003. On the other\nhand, the IBL model with the LLM reward is able to make\nsignificant improvements over the baseline by increasing the\naverage reward tenfold to 0.084 ± 0.011.\nIn the bottom panel of Figure 2, we can observe how the\nLLM reward signal continuously increases before the en-\nvironment signal makes significant gains: the LLM reward\nprovides a guiding signal to the agent that helps it explore\nthe task space effectively: reaching and picking up the key,\nfinding and opening the door, and locating the target all pro-\nvide a reward that enables the agent to reach higher environ-\nmental rewards than it would otherwise.\nDiscussion\nOur preliminary experiment demonstrates the promise of\nutilizing LLMs for intrinsic reward signals in instance-based\nlearning models solely from task instructions and the envi-\nronment state. Indeed, our results demonstrate a significant\nimprovement in the success of an IBL model when utiliz-\ning the reward scheme defined by our LLM-based reward\nmodel and the value of incorporating non-experiential in-\nformation into the reward scheme. It is important to note\nthat the specifics of our experiment and results are depen-\ndent on the model, model parameters, and the environment;\nhowever, it has nonetheless demonstrated the feasibility of\nour approach.\n337\nFuture Directions Our work leaves many potential future\navenues for inquiry. In particular, we have shown a proof-\nof-concept for using LLMs as a source for a dense, intrinsic\nreward via non-experiential information in cognitive mod-\neling. This initial step was done in a simplistic environ-\nment with a reward scheme that could be easily designed\nby a modeler. The natural next step is to advance to more\ncomplex environments and representations that would prove\nmore burdensome for a modeler to explicitly construct. In\nthe same vein, ablation experiments should be conducted to\ndetermine how valuable the steps are in conjunction with the\nLLM reward values versus predefined rewards (e.g., select-\ning a constant value for all steps identified by the model).\nFurthermore, we use a simple prompting scheme for the\nLLM and we receive relatively simple steps. A more ad-\nvanced prompting chain may be necessary in more complex\nenvironments and may yield more nuanced responses (e.g.,\nin our setting we would likely have seen increased perfor-\nmance if the steps had additional conditions, such as reach-\ning the door only when a key had been picked up).\nAs previously discussed, we manually design an envi-\nronment parser to check if the steps have been met by the\nagent to earn the LLM rewards. As the cost and latency of\nLLM queries decrease, it will likely become more feasible\nto query the model at every step to allow it to determine\nwhich goals have been met, entirely automating the pro-\ncess we’ve described here. Furthermore, as the feasibility of\nper-environment step LLM queries improves, this approach\ncan be further refined to develop new steps as the episodes\nprogress, conditioning them on the agent’s previous behav-\niors to refine the reward structure as the agent learns.\nLLMs have provided a new toolset for computational cog-\nnitive modeling: they will allow us to incorporate new kinds\nof reasoning, understanding, and non-experiential informa-\ntion in settings where it was previously difficult or infeasi-\nble. Our experiment provides evidence of the efficacy of this\ntool in one of many possible use cases.\nTask Instructions\nThe full task instructions provided to the model are:\nYou are playing a game in a\ngrid-based environment. Your\ngoal is to navigate through the\nenvironment and to maximize your\nreward by collecting one of two\ntargets: Purple or Green. There is\none Green and collecting it will\ngive you a reward of 0.1. There\nis one Purple and collecting it\nwill give you a reward of 0.4. To\ncollect either a Green or Purple\ntarget, you must rotate toward that\ntarget and identify a path to it.\nIf you collect one target, you will\nbe unable to collect the other.\nIn the game, there are also walls,\ndoors, and keys. In order to open a\ndoor, you must first find a key and\npick it up, then navigate to the\ndoor and use the Toggle action to\nopen it.\nYou may navigate by moving North,\nSouth, East, or West. When there is\na target directly in front of you\n(one unit away in the direction you\nare facing) you must use the Toggle\naction to collect the target. If\na target is not directly in front\nof you, the collect action does\nnothing. Once a single target is\ncollected, the game is complete and\nyou will restart.\nThere are six possible actions:\nMove North, Move East, Move South,\nMove West, Pick Up/Drop, and\nToggle. You will also be facing one\nof the four cardinal directions.\nIf you are facing a direction,\nselecting the Move action in that\ndirection will advance you a single\nunit in that direction. Otherwise,\nit will change your rotation to\nface that direction and you will\nnot move.\nIn the game, you must identify\ntarget locations through\nexploration. You will only have\naccess to your current position,\nrotation, what item you are holding\n(if any), and an indicator of what\nobject is directly in front of you.\nIdentifying a target (or any other\nobject) requires that you navigate\nto an adjacent cell and face the\ntarget, such that it is directly in\nfront of you.\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes, O.;\nDavid, B.; Finn, C.; Fu, C.; Gopalakrishnan, K.; Hausman,\nK.; et al. 2022. Do as i can, not as i say: Grounding language\nin robotic affordances. arXiv preprint arXiv:2204.01691.\nBlain, B.; and Sharot, T. 2021. Intrinsic reward: potential\ncognitive and neural mechanisms. Current Opinion in Be-\nhavioral Sciences, 39: 113–118.\nChentanez, N.; Barto, A.; and Singh, S. 2004. Intrinsically\nmotivated reinforcement learning. Advances in neural infor-\nmation processing systems, 17.\nChevalier-Boisvert, M.; Dai, B.; Towers, M.; de Lazcano,\nR.; Willems, L.; Lahlou, S.; Pal, S.; Castro, P. S.; and Terry,\nJ. 2023. Minigrid & Miniworld: Modular & Customizable\nReinforcement Learning Environments for Goal-Oriented\nTasks. CoRR, abs/2306.13831.\nDovidio, J. F. 1984. Helping behavior and altruism: An em-\npirical and conceptual overview. Advances in experimental\nsocial psychology, 17: 361–427.\n338\nFu, W.-T.; and Anderson, J. R. 2008. Solving the credit as-\nsignment problem: explicit and implicit learning of action\nsequences with probabilistic outcomes. Psychological re-\nsearch, 72(3): 321–330.\nGershman, S. J.; and Daw, N. D. 2017. Reinforcement learn-\ning and episodic memory in humans and animals: an integra-\ntive framework. Annual review of psychology, 68: 101–128.\nGonzalez, C.; Lerch, J. F.; and Lebiere, C. 2003. Instance-\nbased learning in dynamic decision making. Cognitive Sci-\nence, 27(4): 591–635.\nHuang, S.-c.; Jin, L.; and Zhang, Y . 2017. Step by step: Sub-\ngoals as a source of motivation. Organizational Behavior\nand Human Decision Processes, 141: 1–15.\nHughes, E.; Leibo, J. Z.; Phillips, M.; Tuyls, K.; Due ˜nez-\nGuzman, E.; Garc ´ıa Casta ˜neda, A.; Dunning, I.; Zhu, T.;\nMcKee, K.; Koster, R.; et al. 2018. Inequity aversion im-\nproves cooperation in intertemporal social dilemmas. Ad-\nvances in neural information processing systems, 31.\nKulkarni, T. D.; Narasimhan, K.; Saeedi, A.; and Tenen-\nbaum, J. 2016. Hierarchical deep reinforcement learning: In-\ntegrating temporal abstraction and intrinsic motivation. Ad-\nvances in neural information processing systems, 29.\nList, J. A. 2006. The behavioralist meets the market: Mea-\nsuring social preferences and reputation effects in actual\ntransactions. Journal of political Economy, 114(1): 1–37.\nNguyen, T. N.; McDonald, C.; and Gonzalez, C. 2023.\nCredit assignment: Challenges and opportunities in develop-\ning human-like ai agents. arXiv preprint arXiv:2307.08171.\nPark, J. S.; O’Brien, J. C.; Cai, C. J.; Morris, M. R.;\nLiang, P.; and Bernstein, M. S. 2023. Generative agents:\nInteractive simulacra of human behavior. arXiv preprint\narXiv:2304.03442.\nSutton, R. S.; and Barto, A. G. 2018. Reinforcement learn-\ning: An introduction. MIT press.\nThomson, R.; Lebiere, C.; Anderson, J. R.; and Staszewski,\nJ. 2015. A general instance-based learning framework for\nstudying intuitive decision-making in a cognitive architec-\nture. Journal of Applied Research in Memory and Cognition,\n4(3): 180–190.\nWalsh, M. M.; and Anderson, J. R. 2011a. Learning from\ndelayed feedback: neural responses in temporal credit as-\nsignment. Cognitive, Affective, & Behavioral Neuroscience,\n11: 131–143.\nWalsh, M. M.; and Anderson, J. R. 2011b. Modulation of the\nfeedback-related negativity by instruction and experience.\nProceedings of the National Academy of Sciences, 108(47):\n19048–19053.\nWu, Y .; Fan, Y .; Liang, P. P.; Azaria, A.; Li, Y .; and Mitchell,\nT. M. 2023a. Read and reap the rewards: Learning to play\natari with the help of instruction manuals. arXiv preprint\narXiv:2302.04449.\nWu, Y .; Min, S. Y .; Prabhumoye, S.; Bisk, Y .; Salakhutdinov,\nR.; Azaria, A.; Mitchell, T.; and Li, Y . 2023b. SPRING:\nGPT-4 Out-performs RL Algorithms by Studying Papers\nand Reasoning. arXiv preprint arXiv:2305.15486.\nZheng, Z.; Oh, J.; Hessel, M.; Xu, Z.; Kroiss, M.; Van Has-\nselt, H.; Silver, D.; and Singh, S. 2020. What can learned\nintrinsic rewards capture? In International Conference on\nMachine Learning, 11436–11446. PMLR.\n339",
  "topic": "Experiential learning",
  "concepts": [
    {
      "name": "Experiential learning",
      "score": 0.8900681734085083
    },
    {
      "name": "Task (project management)",
      "score": 0.7341271638870239
    },
    {
      "name": "Computer science",
      "score": 0.6408147811889648
    },
    {
      "name": "Descriptive statistics",
      "score": 0.5348507165908813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5056147575378418
    },
    {
      "name": "Multi-task learning",
      "score": 0.48247888684272766
    },
    {
      "name": "Cognition",
      "score": 0.4782811105251312
    },
    {
      "name": "Cognitive psychology",
      "score": 0.46010860800743103
    },
    {
      "name": "Path (computing)",
      "score": 0.45736536383628845
    },
    {
      "name": "Test (biology)",
      "score": 0.4190133213996887
    },
    {
      "name": "Outcome (game theory)",
      "score": 0.4174841642379761
    },
    {
      "name": "Machine learning",
      "score": 0.39465564489364624
    },
    {
      "name": "Psychology",
      "score": 0.3473416566848755
    },
    {
      "name": "Mathematics education",
      "score": 0.18269848823547363
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematical economics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I127591826",
      "name": "University of Dayton",
      "country": "US"
    }
  ],
  "cited_by": 3
}