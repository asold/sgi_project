{
  "title": "User Retention-oriented Recommendation with Decision Transformer",
  "url": "https://openalex.org/W4324312897",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2380325257",
      "name": "Zhao Kesen",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2126224517",
      "name": "Zou, Lixin",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2102846269",
      "name": "Zhao, Xiangyu",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1569978773",
      "name": "Wang Maolin",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2109288988",
      "name": "Yin Dawei",
      "affiliations": [
        "Baidu (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963619374",
    "https://openalex.org/W2902572901",
    "https://openalex.org/W3116249021",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2123427850",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2512965516",
    "https://openalex.org/W2017107296",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2893890797",
    "https://openalex.org/W4285602653",
    "https://openalex.org/W3034329167",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2171279286",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W2469952266",
    "https://openalex.org/W2767807341",
    "https://openalex.org/W4284702156",
    "https://openalex.org/W2166237624",
    "https://openalex.org/W4306317444",
    "https://openalex.org/W3043826557",
    "https://openalex.org/W2799544270",
    "https://openalex.org/W3153252032",
    "https://openalex.org/W2788295351",
    "https://openalex.org/W4306873598",
    "https://openalex.org/W2963842088",
    "https://openalex.org/W2996959725",
    "https://openalex.org/W3040127368",
    "https://openalex.org/W3170841641",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287177420",
    "https://openalex.org/W2919013397",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2781763969",
    "https://openalex.org/W3102899483",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3003416843",
    "https://openalex.org/W2596180971",
    "https://openalex.org/W3007328579",
    "https://openalex.org/W3102778384",
    "https://openalex.org/W4379527413",
    "https://openalex.org/W2187547424",
    "https://openalex.org/W4287185638",
    "https://openalex.org/W3104966867",
    "https://openalex.org/W4299286960",
    "https://openalex.org/W2948345531",
    "https://openalex.org/W3197104471",
    "https://openalex.org/W4283026930",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2937556626",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W1593271688",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W3105787366"
  ],
  "abstract": "Improving user retention with reinforcement learning~(RL) has attracted\\nincreasing attention due to its significant importance in boosting user\\nengagement. However, training the RL policy from scratch without hurting users'\\nexperience is unavoidable due to the requirement of trial-and-error searches.\\nFurthermore, the offline methods, which aim to optimize the policy without\\nonline interactions, suffer from the notorious stability problem in value\\nestimation or unbounded variance in counterfactual policy evaluation. To this\\nend, we propose optimizing user retention with Decision Transformer~(DT), which\\navoids the offline difficulty by translating the RL as an autoregressive\\nproblem. However, deploying the DT in recommendation is a non-trivial problem\\nbecause of the following challenges: (1) deficiency in modeling the numerical\\nreward value; (2) data discrepancy between the policy learning and\\nrecommendation generation; (3) unreliable offline performance evaluation. In\\nthis work, we, therefore, contribute a series of strategies for tackling the\\nexposed issues. We first articulate an efficient reward prompt by weighted\\naggregation of meta embeddings for informative reward embedding. Then, we endow\\na weighted contrastive learning method to solve the discrepancy between\\ntraining and inference. Furthermore, we design two robust offline metrics to\\nmeasure user retention. Finally, the significant improvement in the benchmark\\ndatasets demonstrates the superiority of the proposed method.\\n",
  "full_text": "User Retention-oriented Recommendation\nwith Decision Transformer\nKesen Zhao\nCity University of Hong Kong\nkesenzhao2-c@my.cityu.edu.hk\nLixin Zouâ€ \nWuhan University\nzoulixin15@gmail.com\nXiangyu Zhaoâ€ \nCity University of Hong Kong\nxianzhao@cityu.edu.hk\nMaolin Wang\nCity University of Hong Kong\nMorinWang@foxmail.com\nDawei Yin\nBaidu Inc.\nyindawei@acm.org\nABSTRACT\nImproving user retention with reinforcement learning (RL) has\nattracted increasing attention due to its significant importance in\nboosting user engagement. However, training the RL policy from\nscratch without hurting usersâ€™ experience is unavoidable due to\nthe requirement of trial-and-error searches. Furthermore, the of-\nfline methods, which aim to optimize the policy without online\ninteractions, suffer from the notorious stability problem in value\nestimation or unbounded variance in counterfactual policy eval-\nuation. To this end, we propose optimizing user retention with\nDecision Transformer (DT), which avoids the offline difficulty by\ntranslating the RL as an autoregressive problem. However, deploy-\ning the DT in recommendation is a non-trivial problem because\nof the following challenges: (1) deficiency in modeling the nu-\nmerical reward value; (2) data discrepancy between the policy\nlearning and recommendation generation; (3) unreliable offline\nperformance evaluation. In this work, we, therefore, contribute a\nseries of strategies for tackling the exposed issues. We first articu-\nlate an efficient reward prompt by weighted aggregation of meta\nembeddings for informative reward embedding. Then, we endow\na weighted contrastive learning method to solve the discrepancy\nbetween training and inference. Furthermore, we design two robust\noffline metrics to measure user retention. Finally, the significant im-\nprovement in the benchmark datasets demonstrates the superiority\nof the proposed method. The implementation code is available at\nhttps://github.com/kesenzhao/DT4Rec.git.\nCCS CONCEPTS\nâ€¢ Information Systems â†’Recommender Systems.\nKEYWORDS\nDecision Transfomer, Sequential Recommender Systems, Prompt,\nContrastive Learning\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9416-1/23/04. . . $15.00\nhttps://doi.org/10.1145/3543507.3583418\nACM Reference Format:\nKesen Zhao, Lixin Zou â€ , Xiangyu Zhaoâ€ , Maolin Wang, and Dawei Yin.\n2023. User Retention-oriented Recommendation with Decision Transformer.\nIn Proceedings of the ACM Web Conference 2023 (WWW â€™23), May 1â€“5, 2023,\nAustin, TX, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/\n3543507.3583418\n1 INTRODUCTION\nSequential recommender systems (SRSs), which model usersâ€™ his-\ntorical interactions and recommend potentially interesting items\nfor users, have received considerable attention in both academia\nand industry due to their irreplaceable role in the real world sys-\ntems, e.g., movie recommendation in Netflix 1, and E-commerce\nrecommendation in Amazon2. The success of SRSs heavily relays\non usersâ€™ engagement on the platform, which, to some extent, is re-\nflected by usersâ€™ immediate feedback, liking clicks [8, 40]. However,\nthese immediate feedback can not completely reveal usersâ€™ prefer-\nences [37]. For example, some items with eye-catching titles and\ncovers but low-quality content may attract usersâ€™ clicks and further\nbreak usersâ€™ trust in the platform [35]. Therefore, it is essential to\noptimize usersâ€™ long-term engagement at the platform [47], liking\nuser retention, which is a preferable indicator of user satisfaction.\nAs the tool for optimizing the long-term/delayed metrics [24],\nreinforcement learning (RL) has been widely studied for optimiz-\ning user retention in recent years [ 6]. Though they are capable\nof exploring and modeling usersâ€™ dynamic interests [39], existing\nRL-based SRSs leave much to be desired due to the offline learn-\ning challenge. Unlike gaming scenarios, where RL agents achieve\ngreat success by trial and error search [29], training from scratch\nin online SRSs is unaffordable due to the risk of losing users by\nrecommending inappropriate items. Therefore, recent attention\nof the whole community has been paid to offline RL-based SRSs.\nHowever, putting offline RL into practice is frustrating in both value-\nbased and policy-based methods. For value-based approaches, the\nnotorious instability problem (i.e., the â€™Deadly Triadâ€™) pushes the\ndevelopment of model-based mothods [ 5]. However, due to the\nvast state space in the recommendation scenario, estimating the\ntransition probability is a problem and further leads to unsatisfac-\ntory performance [42]. For policy-based methods, the unbounded\nvariance of counterfactual policy evaluation drives the community\nto clip or discard the counterfactual weights [3], which might lead\nto inaccurate weight and discourage performance [36].\n1https://www.netflix.com/\n2https://www.amazon.com/\narXiv:2303.06347v1  [cs.IR]  11 Mar 2023\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Kesen Zhao, Lixin Zou â€ , Xiangyu Zhaoâ€ , Maolin Wang, and Dawei Yin\nTo explore the potential of RL-based recommendation, we pro-\npose to optimize the user retention recommendation with Decision\nTransformer [2] (DT), which casts the offline RL as an autoregres-\nsive problem and therefore solves the mentioned offline learning\nchallenges. Specifically, DT is required to generate the recommen-\ndation under a specific reward, i.e., the user retention and state.\nWhen conditional on optimal reward, DT can generate future ac-\ntions that achieve the desired return in the recommendation stages.\nThough DT is promising in the recommendation, applying the DT\nin SRSs is a non-trivial problem. It has the following challenges: (1)\ndeficiency in reward modeling. Reward, as the most crucial part\nof DT, directly affects the quality of the recommendation. However,\nin DT, translating the reward into embedding ignores its partial or-\nder, leading to the deficiency in model training.(2) discrepancy in\nrecommendation generation. Under the DT, the recommenda-\ntion is generated with the maximum reward, which is inconsistent\nwith the diversified reward encountered in the training stages. As\na result, the model is not capable of utilizing the knowledge of data\nwith smaller rewards (depicted in Figure 2). (3) unreliable per-\nformance evaluation. Though DT solves the problem of offline\nlearning, we still need the importance weighted offline evaluation\nto measure the performance of the learned policy, which leads to\nunbounded variance and results in an unreliable evaluation.\nTo handle these problems, we propose a novel framework DT4Rec,\nwhich firstly deploy Decision Transformer for recommendation.\nSpecifically, we generate the reward embedding by auto-weighted\naggregating the meta-embeddings generated from discretized re-\nward value, which maintains partial order relationship between re-\nwards. Then, we introduce weighted contrastive learning to remedy\nthe discrepancy between inference and training, which leverages\nthe smaller reward samples by contrasting the large and small re-\nward samples. Furthermore, we propose two novel reliable metrics,\ni.e., the model-based and the similarity-based user retention score,\nto evaluate the policies all around. Compared with the off-policy\nevaluation methods, it achieves lower variance, i.e., more stability,\non the performance evaluation (depicted in Figure 4). Our main\ncontributions can be summarized as follows:\nâ€¢We propose a novel Decision Transformer-based SRS model,\nDT4Rec, which eliminates the difficulty of offline learning with\nan RL-based recommender system.\nâ€¢We contribute the auto-discretized reward prompt and contrastive\nsupervised policy learning, which effectively cope with the de-\nficiency in reward modeling and discrepancy between training\nand inference respectively.\nâ€¢We design the model-based and the similarity-based user reten-\ntion score. Compared with the off-policy evaluation methods,\nthey can fairly evaluate model performance.\nâ€¢Experiments on two benchmark datasets illustrate the superiority\nof our proposed model.\n2 METHODOLOGY\n2.1 Problem Formulation\nIn a typical sequential recommendation, we are given a sequence of\nğ‘€ user-interested items denoted as ğ‘ˆğ‘¡ = {ğ‘£ğ‘¡\nğ‘–}ğ‘€\nğ‘–=1 in the ğ‘¡-th recom-\nmendation and seek to predict a set of ğ‘ items ğ‘ğ‘¡ = [ğ‘£ğ‘¡,1,...,ğ‘£ ğ‘¡,ğ‘]\nthat the user might be interested in. Formally, we aim at generating\nthe recommendation ğ‘âˆ—\nğ‘¡, maximizing a specific metric Î˜ as\nğ‘âˆ—\nğ‘¡ = arg max\nğ‘ğ‘¡\nÎ˜(ğ‘ğ‘¡|ğ‘ˆğ‘¡). (1)\nAccording to the target of sequential recommender, we could instan-\ntiate Î˜ with the metric of optimizing user immediate feedback (i.e.,\nprediction accuracy) or long-term user engagement (e.g., user re-\ntention). For user retention, we define it as the number of users\nlogging in the platform over the next ğ¾ time intervals (e.g., next ğ¾\ndays, next ğ¾ months)\nğ‘’ğ‘¡ =\nğ¾âˆ‘ï¸\nğ‘˜=1\n1[log-inğ‘¡+ğ‘˜], (2)\nwhere 1 is the indicator function, log-inğ‘¡+ğ‘˜ is a binary function\nthat indicates whether the user logs in the platform or not at the\nnext (ğ‘¡+ğ‘˜)-th timestep.\nMDP Formulation of Sequential Recommendation. To optimize the\nmetric Î˜ with reinforcement learning, we formally introduce the\nsequential recommendation under the framework of the Markov\nDecision Process (MDP), including a sequence of states, actions, and\nrewards. Particularly, the stateğ‘ ğ‘¡ = ğ‘ˆğ‘¡ is defined as the historical\ninteractions. The action ğ‘ğ‘¡ is the recommendation list, which is\ngenerated by a recommendation policy ğœ‹ : ğ‘ ğ‘¡ â†’ğ‘ğ‘¡. Before every\nrecommendation, ğ‘ ğ‘¡ will be updated by appending the userâ€™s inter-\nested items at its end as ğ‘ ğ‘¡ = ğ‘ ğ‘¡âˆ’1 âŠ•{ğ‘ˆğ‘¡ âˆ’ğ‘ˆğ‘¡âˆ’1}. Given a specific\nevaluation metric Î˜, the reward is usually defined as the function\nof the metric. Particularly, for user retention, the reward is set the\nsame as the ğ‘’ğ‘¡. With the formally defined ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡, user systemsâ€™\ninteractions form a trajectory\nğœ = [ğ‘ 1,ğ‘1,ğ‘Ÿ1,...,ğ‘  ğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡]. (3)\n2.2 Decision Transformer based\nRecommendation\nUnlike traditional frameworks that approximate the state-action\nvalue for planning [17] or directly optimize gradient-based policy\nfunction [4], the Decision Transformer translates the RL problem as\nan autoregression problem by directly predicting the desired action\nwith previous rewards, states, and actions (refer to Section 2.7). As\nan supervised learning task, DT avoids the problems of â€˜Deadly\nTraidâ€™ and unbounded variance and shows superiority in offline\nlearning [2]. To instantiate the DT in SRSs, we formally disassemble\nthe DT into the following four blocks (shown in Figure 1):\nâ€¢Embedding Module : The embedding module aims to map the\nrewards, states, and actions into dense vectors. Particularly, be-\nfore translating rawğ‘ ğ‘¡, ğ‘ğ‘¡ and ğ‘Ÿğ‘¡ into embedding, we need to cast\nthe data in the trajectory ordered by reward, state and action to\nsimplify the prediction of action as\nğœâ€²\nğ‘¡ = [bğ‘Ÿ1,ğ‘ 1,ğ‘1,..., bğ‘Ÿğ‘¡,ğ‘ ğ‘¡,ğ‘ğ‘¡], (4)\nwhere bğ‘Ÿğ‘¡ = Ãğ‘‡\nğ‘¡â€²=ğ‘¡ğ‘’ğ‘¡â€² is the cumulative reward, i.e., the return-to-\ngo [2]. ğ‘‡ is the round of total recommendation. Then, embedding\nmodule sequentially converts the raw feature sequence ğœâ€²\nğ‘¡ into\nfeature vector sequence as ğ‰â€²\nğ‘¡ = [bğ’“1,ğ’”1,ğ’‚1,..., bğ’“ğ‘¡,ğ’”ğ‘¡,ğ’‚ğ‘¡]with the\nencoder model (Section 2.4). Particularly, due to the importance of\nreward, we distinguish the embedding module for reward as the\nUser Retention-oriented Recommendation\nwith Decision Transformer WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nEmbedding Module\nEmbedding Module\n Transformer\nDecision Block\nAction Decoder\nContrastive Learning Loss\n Transformer\nDecision Block\nAction Decoder\n Transformer\nDecision Block\nAction Decoder\nContrastive Learning Loss\n Transformer\nDecision Block\nAction Decoder\nAggregation\nMeta \nEmbedding\nEmbedding Module\nPrompt  \nGenerator\nAction \nEncoder\nState \nEncoder\nReZard \nPrompt\nAction \nEncoder\nState \nEncoder\nFigure 1: Framework overview of DT4Rec. Negative samples have the same states and actions as positive samples, while re-\nwards are replaced with different values. Positive and negative samples share the same model parameters.\nreward prompt, which is designed to prompt the DT generating\ndesired recommendation lists.\nâ€¢Decision Block: The decision block is the centre of the model,\nwhich transforms the dense vectors of contextual information\nğ‰â€²\nğ‘¡ âˆ’{ğ’‚ğ‘¡}= [bğ’“1,ğ’”1,ğ’‚1,..., bğ’“ğ‘¡,ğ’”ğ‘¡]into context feature ğ‘¨ğ‘¡ for gen-\nerating the desired response in the next time step, where bğ’“ğ‘¡ is\nthe generated reward prompt.\nâ€¢Action Decoder : Given contextual information ğ‘¨ğ‘¡, action de-\ncoder is expected to generate a sequence of actionË†ğ‘ğ‘¡ that matches\nthe ground truth action ğ‘ğ‘¡.\nâ€¢Supervised Policy Learning : The goal of supervised policy\nlearning is to minimize the difference between the generated\naction Ë†ğ‘ğ‘¡ and ground truthğ‘ğ‘¡ with a specific loss function. There-\nfore, it translates the ordinary RL task into a supervised learning\nproblem. By specifying the optimal reward as the prompt in\ninference, the DT is desired to recommend the items that can\nmaximize user retention.\n2.3 Auto-Discretized Reward Prompt\nReward, as the target of RL, differentiates the performance of dif-\nferent policies through its numerical value. Therefore, the prompts\ngenerated by DT should maintain the partial order relationship\nbetween rewards, i.e., if two rewards are similar, then the Euclidean\ndistance between their generated prompts is smaller.\nTo this end, we propose to generate more efficient prompts by the\nauto-discretizing method, as shown in Figure 1, which consists of\ndiscretizing the numerical value and auto-weighted aggregating the\nmeta-embedding learned by MLP. It shares the embedding between\nsimilar rewards. Specifically, according to the reward value, we con-\nvert it into a weighted score for a batch of ğµlearnable embeddings\nğ‘´ğ‘ âˆˆR, âˆ€ğ‘ âˆˆ[1,ğµ]as\nğ’› = ğœ™(ğ‘¾ğœ(ğ’˜bğ‘Ÿ)+ğ›¼ğœ (ğ’˜bğ‘Ÿ)), (5)\nwhere ğ’˜ âˆˆR1Ã—ğµ and ğ‘¾ âˆˆRğµÃ—ğµ are learnable variables. ğœ is the\nLeaky Relu [11]. ğœ™ is the Softmax function. Given the weighted\nscore ğ’›, the reward embedding bğ’“ is set as the aggregation of meta\nembedding, which can be formulated as\nbğ’“ =\nğµâˆ‘ï¸\nğ‘=1\nğ’›ğ‘ğ‘´ğ‘, (6)\nwhere ğ’›ğ‘ is the ğ‘th element of ğ’›. Since the value of ğ‘Ÿ is directly\nused as the input of the neural network to ensure the partial order\nbetween rewards, the similar bğ‘Ÿ will share the similar embedding as\nlong as the neural network is smooth.\n2.4 State-Action Encoder\nThe action encoder maps actions to vectors. The challenge lies in\nthe dynamic length of recommender action since the user may\ninteract with different numbers of items each time. Therefore, we\nmodel the sequence of interaction sequence with GRU, which has\nshown better performance than LSTM [14] in modeling the dynamic\nitem sequence for recommender systems. Furthermore, compared\nwith the heavy Transformer [19] based methods, GRU is a more\naffordable alternative for balancing the efficiency and effectiveness.\nSpecifically, we set the maximum length of sequences to ğ‘. For\nthose less than ğ‘, we pad the sequence to ğ‘ with 0 vector [32].\nFormally, the sequence information can be extracted as\nğ‘¯ğ‘› = ğºğ‘…ğ‘ˆğ‘’ (ğ’—ğ‘›,ğ‘¯ğ‘›âˆ’1) (7)\nğ’‚ğ‘¡ = ğ‘¯ğ‘, (8)\nwhere ğºğ‘…ğ‘ˆğ‘’ is the recurrent layer, ğ‘ is maximum sequence length,\nğ‘¯ğ‘› is the hidden state. Additionally, we set the embedding of ğ‘ğ‘¡ as\nthe hidden state of last timestep, i.e., ğ’‚ğ‘¡.\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Kesen Zhao, Lixin Zou â€ , Xiangyu Zhaoâ€ , Maolin Wang, and Dawei Yin\n2.5 Transformer Decision Block\nPrior efforts have demonstrated the superiority of Transformer and\nRL in SRS tasks. On the one hand, Transformer equally treats the ele-\nment of sequence through self-attention, which avoids information\nloss of long sequence in RNN and further diminishes the gradient\nvanishing or explosion in modeling long sequence [19, 46, 51]. On\nthe other hand, RL is expert at dynamically modeling usersâ€™ interest\nprofiles and capturing usersâ€™ interest shifts. To get the best of both\nworlds, we propose to learn user interests from their interaction\ntrajectories via Transformer Decision blocks [2].\nSpecifically, for user retention recommendations, we are required\nto model the dynamic contextual information for generating the\nrecommender decision, which is similar to the generative task.\nTherefore, we select the unidirectional Transformer layer as the\nbackbone model for modeling the complex feature interactions,\nwhich has shown substantial advantage over existing methods in\ngenerative task [10]. Furthermore, we employ the skip-connections\nfor mitigating the over-fitting [12] and feed-forward neural layers\nfor linear-mapping of features [20]. Consequently, the contextual\ninformation for recommender decision can be formulated as\neğ‘¨ = ğ¹ğ¹ğ‘ \u0000ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› \u0000ğ‰â€²âˆ’{ğ’‚ğ‘¡}\u0001\u0001 , (9)\nwhere eğ‘¨ is the matrix of predicted action embedding with the ğ‘¡-th\nrow as eğ’‚ğ‘¡ for ğ‘¡ âˆˆ[1,ğ‘‡]. ğ¹ğ¹ğ‘ (ğ‘¥)= ğºğ¸ğ¿ğ‘ˆ(ğ‘¥ğ‘¾1 +ğ’ƒ1)ğ‘¾2 +ğ’ƒ2 is the\nfeed-forward neural layer with skip-connection. Here, ğºğ¸ğ¿ğ‘ˆ [13]\nis commonly used activation function in Transformer models. The\nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› presents the multi-head self-attentive mecha-\nnism defined in [33], which has been proved effective for jointly\nlearning information from different representation subspaces and ,\ntherefore, massively deployed in the recommender systems [18].\n2.6 Action Decoder\nGiven the contextual information of items interacted with at each\ntime step, action decoder aims to decode sequences of items of\ninterest to users with the GRU [7]. The item of current interaction\nis unknown for decoding. We only use previous interactions and\ncontextual information to predict it as\nbğ’—ğ‘› = ğ’—ğ‘› âŠ•eğ’‚ğ‘¡ (10)\neğ’—ğ‘›+1 = ğºğ‘…ğ‘ˆğ‘‘ (eğ’—ğ‘›,bğ’—ğ‘›), (11)\nwhere ğ’—ğ‘› is the embedding of ğ‘£ğ‘›, eğ’—ğ‘›+1 is the prediction for the item\nat ğ‘›+1 place, âŠ•represents concatenate operations. Since there is\nno information for predicting the first item, we use â€˜bosâ€™ token as\nthe start marker and initialize the eğ’—0 randomly. The full sequence\ncan be decoded as\neğ‘½ = decoder (bos,bğ’—1,..., bğ’—ğ‘›,..., bğ’—ğ‘âˆ’1)\n= [eğ’—1,..., eğ’—ğ‘›,..., eğ’—ğ‘], (12)\nwhere eğ‘½ is predicted matrix for interacted items sequence at the\nğ‘¡-th time step, ğ‘ is maximum sequence length.\nNotice that the length of sequences may not be fixed. When\npredicting, we do not know the length of sequences in decoder, so\nwe add an â€˜eosâ€™ token at the end of each sequence as an end marker\nand then still pad it with zeros to a length of ğ‘. The decoder does\nnot continue predicting when it has predicted â€˜eosâ€™.\n2.7 Contrastive Supervised Policy Learning\nIn Decision Transformer, only the maximal reward will be used for\ninference since it is designed to generate the actions with maximal\nreward. Therefore, the samples with small rewards might not be\nfully utilized (validated in Section 3.7). In order to fully exploit the\nknowledge from the samples, we propose to use a weighted con-\ntrastive learning approach, which treats actions with small rewards\nas negative samples to avoid recommending small rewarded actions.\nTherefore, our objective function consists of two parts, CE loss and\nweighted contrastive learning loss.\n2.7.1 Weighted Contrastive Loss. For each sample, we use same\nstate and action, with different rewards, as negative samples, denot-\ning its predicted action embedding aseğ‘½âˆ’. The weighted contrastive\nlearning loss can be formulated as\nLCL = âˆ’\nâˆ‘ï¸\neğ‘½âˆ’âˆˆğš¼\nğœ…\n\u0010\neğ‘½âˆ’\n\u0011\nğ‘“ğ‘ \n\u0010\neğ‘½,eğ‘½âˆ’\n\u0011\n, (13)\nwhere Î¥ is the set of negative samples. ğ‘“ğ‘ (Â·)calculates the similar-\nity of two sequences by averaging of embeddingsâ€™ dot product at\nevery row of the matrices eğ‘½ and eğ‘½âˆ’. ğœ…\n\u0010\neğ‘½âˆ’\n\u0011\nis the weighting hy-\nperparameter set according to the reward value of negative sample,\ni.e., the weight is inversely proportional to reward. The reason is\nthat smaller rewards make user retention lower and we want to be\ndissimilar to those ones.\nBesides the weighted contrastive loss, we also optimize the DT\nwith the original cross-entropy loss as\nbğ’€ = ğœ“\n\u0010\nğ‘¾ğ‘£eğ‘½ +ğ’ƒğ‘£\n\u0011\n(14)\nLğ¶ğ¸ = CE\n\u0010\nbğ’€,ğ’€\n\u0011\n, (15)\nwhere ğ’€ is the label matrix with the one-hot label in every row, bğ’€\nis the corresponding predicted label distribution. Here, CE is the\ncross-entropy loss function, ğœ“ is the softmax function. ğ‘¾ğ‘£ and ğ’ƒğ‘£\nare learnable parameters.\nFinally, the overall loss function is formulated by combining the\ncross-entropy loss and weighted contrastive loss as\nL= Lğ¶ğ¸ +ğ›½Lğ¶ğ¿, (16)\nwhere ğ›½ is a hyper-parameters.\n3 EXPERIMENTS\nIn this section, we conduct experiments on two benchmark datasets,\nIQiYi and ML-1m, to answer the following research questions.\nâ€¢RQ1: How does the performance of our proposed DT4Rec com-\npare with other state-of-the-art baselines?\nâ€¢RQ2: How does our auto-discretized reward prompt contribute\nto reward modeling?\nâ€¢RQ3: How does our contrastive supervised policy learning method\nmitigate the out-of-distribution (OOD) issue between the recom-\nmendation training and inference?\nâ€¢RQ4: How does stability of our designed evaluation method?\nâ€¢RQ5: How does the generalization capability of our DT4Rec?\nUser Retention-oriented Recommendation\nwith Decision Transformer WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nTable 1: Statistics of datasets. UR is the short for average user\nretention.\nDatasets Users Items Interactions UR Density\nIQiYi 3,000,000 4,000,000 71,046,026 4.80 5.8e-6\nML-1m 6,014 3,417 1,000,000 4.13 4.84%\n3.1 Datasets\nWe conduct experiments on two real datasets, iqiyi user reten-\ntion data (IQiYi) and ML-1m. In Table 1, we provide details of two\ndatasets. Explicitly, the IQiYi dataset 3 records about 70 million\nusers interactions for 4 million videos, e.g., views and comments,\nwhich is a very sparse and noise dataset. Therefore, without losing\ngenerality, we select the users with at least 20 interaction records\nfor experiment. Furthermore, we do not distinguish between the\ntypes of interactions and treat them as the identical interactions.\nUser retention is set as the average number of days users log in the\nnext week, i.e., ğ¾ = 7 (the setting used in WSDM Cup 20223). The\nML-1m dataset 4 is a benchmark dataset commonly used in SRSs,\nwhich records 6,014 usersâ€™ scores for 3,417 movies. Since the ML-1m\nrecords span a large time scope, we calculate the user retention in\nthe perspective of month.\n3.2 Evaluation\nWe use the following evaluation metrics to evaluate the perfor-\nmance of the proposed methods on both immediate feedback (i.e.,\nprediction accuracy) and long-term engagement (i.e., user reten-\ntion). Specifically, these metrics are defined as follows.\n3.2.1 Prediction Accuracy. To assess prediction accuracy, we use\nfour widely used common metrics, BLEU [26], ROUGE [23], NDCG,\nHR in sequence prediction problems [31]:\nâ€¢BLEU evaluates the precision of dynamic length recommenda-\ntions in sequence-to-sequence recommendations [27], which is a\nfrequently-used metric in NLP.\nâ€¢ROUGE refers to the recall of dynamic length recommendations,\nwhich is a commonly used metric for sequence-to-sequence rec-\nommender systems [21].\nâ€¢HR@K measures the probability that ground-truth items appear\nin the top-K recommendations.\nâ€¢NDCG@K measures the cumulative gain (CG) scores in the top-K\nrecommendations and considers the influence of position on rec-\nommendations [34], where the CG scores represent the similarity\nbetween items and ground truths.\n3.2.2 User Retention. To evaluate the effectiveness of the generated\nrecommendation sequences, we provide two designed metrics, MB-\nURS and SB-URS, and two common ones, improved user retention\n(IUR) and no return count (NRC) [35].\nâ€¢MB-URS. The model-based user return score evaluates the ef-\nfectiveness of recommended item sequences by directly returning\na user-retention score. Particularly, we train a supervised model\nthat predicts the reward of particular state-action pairs with\nthe 30% validation dataset, which is independent of the training\n3http://challenge.ai.iqiyi.com/\n4https://grouplens.org/datasets/movielens/1m/\ndataset. Therefore, the model is optimized by minimizing the\nMSE loss between predicted rewards and the ground truth.\nâ€¢SB-URS. The similarity-based user return score evaluates the\neffectiveness of recommended item sequences by weighted sum\nover the ground truth user retention score. Specifically, we divide\nthe samples into 8 classes according to their ground truth reward\nand calculate BLEU-1 scores between predicted sequences and\nground truth for each class as their similarity. Then, the SB-URS\nare calculated as follows:\nSB-URS =\nğ¾âˆ‘ï¸\nğ‘˜=0\nğ‘ ğ‘˜ Â·\n\u0012\nğ‘”ğ‘˜ âˆ’ğ¾\n2\n\u0013\nÂ·ğ‘ğ‘˜, (17)\nwhere ğ‘ ğ‘˜ is the similarity of class k.ğ‘”ğ‘˜ is the ground truth reward\nof class ğ‘˜. We want the similarity to be as small as possible for\nsamples with small ground truth reward, and as large as possible\nfor samples with large ground truth reward. ğ‘ğ‘˜ is the number of\nsamples with a reward of ğ‘˜.\nâ€¢Improved user retention (IUR) . It measures the percentage\nof relative usersâ€™ average retention improvement compared to\ntheir logged average retention in the offline data, which directly\nreflects usersâ€™ engagement on the system. And a larger percentage\nof IUR indicates that recommendation lists make more users\nengaged in the system.\nâ€¢No return count (NRC) . It is the percentage of users who left\nthe system after a recommendation was made. A smaller no\nreturn count indicates that recommendation lists keep more users\nengaged in the system.\n3.3 Baselines\nWe compare our model with state-of-the-art methods from different\ntypes of recommendation approaches, including:\nâ€¢BERT4Rec [30] : It uses a bidirectional Transformer to learn\nsequential information. In addition, it utilizes a mask language\nmodel, to increase the difficulty of the task and enhance the\ntraining power of the model.\nâ€¢SASRec [18] : It employs a left-to-right unidirectional Trans-\nformer that captures usersâ€™ sequential preferences.\nâ€¢LIRD [45] : It combines both policy-based gradient and value-\nbased DQN, using an actor-critic (AC) framework. At each recom-\nmendation step, multiple items are recommended, and a simulator\nautomatically learns the optimal recommendation strategy to\nmaximize long-term rewards from users.\nâ€¢TopK [3]: It uses a policy-based RL approach in offline learning\nand addresses the distribution mismatch with importance weight.\nIt assumes that the reward of a set of non-repeating items is equal\nto the sum of rewards of each item and recommends multiple\nitems to the user at each time step. In this paper, we will refer to\nthis model as TopK.\nâ€¢DT4Rec-R: To illustrate the significance of reward in guiding\nthe model, we train a DT model without considering reward,\ndenoted as DT4Rec-R.\n3.4 Hyperparameter Setting\nIn prompt generator, we set bucket numberğµ = 10, skip-connection\nğ›¼ = 0.1. In action encoder and decoder, we set maximum sequence\nlength ğ‘ = 20, RNN layer number as 1. In Decision Transformer,\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Kesen Zhao, Lixin Zou â€ , Xiangyu Zhaoâ€ , Maolin Wang, and Dawei Yin\nTable 2: Overall performance comparison in prediction ac-\ncuracy. The best performance is marked in bold font.\nDataset Model Metric\nBLEUâ†‘ ROUGEâ†‘ NDCGâ†‘ HRâ†‘\nIQiYi\nBERT4Rec 0.7964 0.7693 0.7384 0.7673\nSASRec 0.8009 0.7906 0.7827 0.7815\nDT4Rec 0.8249* 0.8172* 0.8139* 0.8044*\nML-1m\nBERT4Rec 0.3817 0.3806 0.5286 0.2769\nSASRec 0.4052 0.3983 0.5409 0.3123\nDT4Rec 0.4331* 0.4185* 0.5679* 0.3342*\nâ€œ*â€ indicates the statistically significant improvements (i.e.,\ntwo-sided t-test with ğ‘ < 0.05) over the best baseline.\nâ†‘: the higher the better; â†“: the lower the better.\nTable 3: Overall performance comparison in user retention.\nThe best performance is marked in bold font.\nDataset Model Metric\nMB-URSâ†‘ SB-URSâ†‘ IURâ†‘ NRCâ†“\nIQiYi\nDT4Rec-R 5.16 52131 7.5% 2.6%\nTopK 5.41 61045 12.7% 2.0%\nLIRD 5.63 63572 17.3% 1.9%\nDT4Rec 6.05* 72270* 26.0%* 1.4%*\nML-1m\nDT4Rec-R 5.42 13050 31.2% 9.7%\nTopK 5.71 13964 38.3% 6.9%\nLIRD 5.86 14627 41.9% 4.8%\nDT4Rec 5.93* 15562* 43.6%* 3.6%*\nâ€œ*â€ indicates the statistically significant improvements (i.e.,\ntwo-sided t-test with ğ‘ < 0.05) over the best baseline.\nâ†‘: the higher the better; â†“: the lower the better.\nwe set maximum trajectory length T from {10,20,30,40,50}, Trans-\nformer layer number as 2, heads number as 8, embedding size as 128.\nWe choose AdamW as optimizer and set learning rate as 0.01. To\nsave the computation cost, we only save the 30 most recently inter-\nacted items as the state. For other baselines, the hyper-parameters\nare set to their optimal values as recommended by their original au-\nthors or searched within the same ranges as our model. The results\nfor each model are reported under their optimal hyper-parameter\nsettings. The implementation code is available online5.\n3.5 RQ1: Overall Comparison\n3.5.1 Prediction Accuracy. As shown in Table 2, we record all mod-\nelsâ€™ best results on the prediction accuracy task. From Table 2, we\nhave following observations:\nâ€¢Our model outperforms all baselines on both datasets. This illus-\ntrates the effectiveness of our model in optimizing the immediate\nuser feedback. In comparison to the existing methods, our model\ntakes full advantage of RL to model rewards and states of users\nat each time step. This allows our model to dynamically model\n5https://github.com/kesenzhao/DT4Rec.git\nTable 4: Ablation study on IQiYi dataset.\nArchitecture Metric\nSB-URS MB-URS IUR\nDT4Rec 72270 6.05 26.0%\nw/o contrastive 65175 (-9.82%) 5.68 (-6.11%) 18.3% (-29.6%)\nw/o weight 69316 (-4.09%) 5.79 (-4.30%) 20.6% (-20.8%)\nw/o auto-dis 70953 (-1.82%) 5.96 (-1.49%) 24.2% (-6.9%)\nthe userâ€™s interest characteristics to meet the changing needs of\nthe user, so our model can achieve superior performance on the\nprediction accuracy task.\nâ€¢SASRec performs better than BERT4Rec on both datasets. Our\nmodel and SASRec both use a unidirectional Transformer, while\nBERT4Rec uses a bidirectional Transformer. Since our task is an\nend-to-end generation task, the unidirectional Transformer is\nmore suitable for our task.\n3.5.2 User Retention. Table 3 records all modelsâ€™ best results on\nmaximizing user retention task. We can have following conclusions:\nâ€¢Our model outperforms other baselines on both MB-URS and SB-\nURS. DT4Rec outperforms the best base LIRD by a large margin\non both IQiYi and ML-1m.\nâ€¢Traditional offline methods, i.e., TopK, LIRD, still suffer from the\nâ€˜Dead Traidâ€™ problem and the short-sightedness problem caused\nby the discount factor since they perform significantly worse\nthan the proposed method.\nâ€¢In addition, TopK models based on policy gradients are prone\nto converge to suboptimal solutions, and determining the appro-\npriate learning rate can be challenging. Due to the large action\nspace in SRSs, the LIRD model based on the AC framework is\nmore difficult in the estimation of Q values.\nâ€¢Our model, however, learns rewards and states to action map-\npings by sequential modeling, eliminating the need for bootstrap\nand hence avoiding the â€˜Dead Traidâ€™ issue. Unlike traditional RL\nstrategies, our model also does not require estimating Q-values.\nThis circumvents the problem of large action space in SRSs. The\ncomparison with DT4Rec-R illustrates the importance of rewards\nfor learning usersâ€™ long-term preferences.\n3.6 RQ2: Effectiveness of Auto-Discretized\nReward Prompt\nBy comparing the results of DT4Rec and DT4Rec-R, we have demon-\nstrated the significance of reward as a guide for model training. In\nthis subsection, we further show the effectiveness of our improve-\nments for modeling the partial order of reward by ablation study.\nSpecifically, we compare DT4Rec with the model using the naive\nprompt, which is implemented with a single-layer feed-forward\nneural network, i.e., â€˜w/o auto-disâ€™ in Table 4. The experimental\nresults show that the auto-discretized reward prompt is effective.\nWithout the auto-discretized reward prompt, the performance on\nMB-URS and SB-URS drops 1.82% and 1.49% respectively, indicating\nthat the auto-discretized reward prompt can make the generated\nembeddings maintain the partial order relationship between their\ncorresponding reward values.\nUser Retention-oriented Recommendation\nwith Decision Transformer WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nData-B Original\n62000\n64000\n66000\n(a) SB-URS\nData-B Original\n5.2\n5.4\n5.6\n(b) MB-URS\nFigure 2: Comparison between the model trained on the orig-\ninal IQiYi dataset and the date removing the smaller reward\nparts, i.e., the Data-B.\nData-B Original\n67000\n69000\n71000\n(a) SB-URS\nData-B Original\n5.7\n5.8\n5.9\n6.0\n(b) MB-URS\nFigure 3: Comparison between the DT4Rec trained on the\noriginal IQiYi dataset and the date removing the smaller re-\nward, i.e., the Data-B.\n3.7 RQ3: Validity of Contrastive Supervised\nPolicy Learning\nWe further reveal the OOD problems of DT and then powerfully\nillustrate the effectiveness of our improvements through the fol-\nlowing analytical experiments.\nAnalysis of OOD Problem. In the DT model, the model uses the\nknowledge from the samples with high reward values since only\nthe maximum value of reward is input when generating recom-\nmendations. Therefore, the knowledge in the samples with small\nrewards is not incorporated in final recommendation. Specifically,\nwe evaluate the model without contrastive learning on the origi-\nnal dataset and data-B, which removes the samples with rewards\nsmaller than 4. Figure 2 shows the results on IQiYi dataset. From\nFigure 2, we can see that there is negligible disparity in the modelâ€™s\nperformance on the Data-B and the original dataset. This indicates\nthat the model barely uses the knowledge of samples with a large\nreward, which verifies our claim on the OOD problem.\nEffectiveness of Weighted Contrastive Learning Method (RQ3). In\nthis paragraph, we verify the effectiveness of the weighted con-\ntrastive learning from the perspective of improving performance\nand well-using knowledge of small reward samples. For the for-\nmer, we compare the DT4Rec with the model without the contrast\nlearning loss, denoting it as â€˜w/o contrastiveâ€™ to demonstrate the\nsuperiority of weighted contrastive learning. The significant perfor-\nmance drop in Table 4 validates the effectiveness of our weighted\nMB-URS ASB-URS RiB\n0.02\n0.04\n0.06\n(a) IQiYi\nMB-URS ASB-URS RiB\n0.02\n0.04\n0.06\n(b) ML-1m\nFigure 4: Comparison of different evaluation methodsâ€™ vari-\nance on IQiYi and ML-1m datasets.\ncontrastive learning method. For utilizing the knowledge of small\nreward samples, we compare DT4Rec with the variant that does not\nuse the small reward samples in Figure 3. Particularly, we plot orig-\ninal one and the version without using the small reward samplesâ€™\nperformance on SB-URS and MB-URS. From the figure, we clearly\nobserve that the SB-URS and MB-URS of our model on original\nIQiYi dataset are higher than those without using the small reward\nsamples, showing that our model makes full use of the samples\nwith small rewards. The reason is that our model will avoid rec-\nommending item sequences that are similar to the samples with\nsmall rewards, which is consistent with our intuition on weighted\ncontrastive learning method.\n3.8 RQ4: Evaluation Efficiency\nWe illustrate the validity of the MB-URS and SB-URS by comparing\nour methods with the evaluation method proposed by Wu et al .,\ndenoting it as â€˜RiBâ€™. We split the test set into 5 sets and calculate\nthe variance of the performance on these 5 test sets as the indicator\nof evaluation methodsâ€™ stability. For fair comparison in the same\nsize, we replace the SB-URS with the ASB-URS, which evaluate the\naverage similarity-based user return score by dividing the ğ‘ğ‘˜ in\nEquation (17). Figure 4 plots the variance of evaluation methods\non the IQiYi and ML-1m. The results show that the variance of\nour evaluation method is small on both datasets, confirming the\neffectiveness of our proposed methods.\n3.9 RQ5: Model Generalization Analysis\nIn inference stage, instead of using ground truth reward, only the\nmaximal reward is being utilized for generating the recommenda-\ntion based on current states. Consequently, it leads to the mismatch-\ning between training and test since the maximal reward may not\nexist in the training samples. Therefore, it requires the model to\nhave a strong generalization capability, rather than just performing\nimitation learning on original dataset with a certain return. We\nconduct behavior cloning analysis experiments on IQiYi dataset.\nSpecifically, we treat samples with rewards of 6 or 7 as high re-\nward parts and resplit datasets into high reward parts and not-high\nreward parts. Furthermore, we control the high reward samplesâ€™\nproportions of the whole dataset to study modelsâ€™ generalization\ncapability. In Figure 5, we plot DT4Rec and TopKâ€™s MB-URS and\nSB-URS on these datasets with different high reward sample pro-\nportion, i.e., [10, 25, 40, 100]. From the figure, we observe following\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Kesen Zhao, Lixin Zou â€ , Xiangyu Zhaoâ€ , Maolin Wang, and Dawei Yin\n20 40 60 80 100\nhigh reward sample proportion\n4.5\n5.0\n5.5\n6.0\n6.5\n(a) MB-URS\nDT4Rec\nTopK\n20 40 60 80 100\nhigh reward sample proportion\n5\n10\n15\n20\n25\n30\n(b) IUR%\nDT4Rec\nTopK\nFigure 5: Behavior cloning analysis on IQiYi dataset.\nfacts: (1) on both new datasets, our model performs better than\nTopK, indicating a strong generalization capability of proposed\nmethod. (2) DT4Recâ€™s SB-URS is higher than 20%, even only using\n10% high reward samples. It demonstrates the effectiveness of pro-\nposed method in increasing user retention. When the high reward\nsamples is small, the policy evaluation for high reward samples\nwill become inaccurate and hard. However, our model is trained\nin a supervised way, which avoids the high variance of the policy\nevaluation and boosts modelâ€™s performance on test.\n4 RELATED WORK\nWe summarize the earlier literature related to our work, i.e., se-\nquential recommendations and reinforcement learning based rec-\nommender systems as follows.\n4.1 Sequential Recommender Systems\nThe purpose of sequential recommender systems is to address the\norder of interactions [22, 38]. Early methods are mainly based on\nMarkov Chain and Markov Processes. For example, FPMC learns a\ntransition matrix for each user by combining matrix factorization\n(MF) and Markov chains (MC) [28]; Hosseinzadeh et al. capture the\nvariation of user preference by a hierarchical Markov model [16].\nIn addition to MC-based methods, there are also RNN-based meth-\nods. For example, GRU4Rec [ 32] firstly proposes to build RS by\ntaking full use of the RNN model to learn sequential information;\nGRU4RecF proposes the paralleled RNN to leverage item features\nand improve recommendation efficiency [15]. However, none of\nthese studies are for optimizing the long-term engagement, e.g.,\nuser return times [ 9], browsing depth [ 47], which is crucial for\nimproving the quality of recommendations.\n4.2 Reinforcement Learning based\nRecommendation\nSince trial and error search is too costly in recommender systems,\nhere we only present offline reinforcement learning methods. In gen-\neral, RL-based RSs are mainly divided into two categories: model-\nbased and model-free.\nModel-based models. Model-based models represent the whole\nenvironment by learning a fixed model [ 6, 43, 48, 50]. However,\nin recommendation scenario, the modelâ€™s representation of the\nenvironment will be biased due to the dynamic changes in the\nenvironment. If the model is constantly updated to adapt to the\nenvironmental changes, it will incur a huge expense. In addition,\nthe probability of userâ€™s behavior cannot be estimated and the\ntransition probability function becomes difficult to determine.\nModel-free models. In model-free models, the transition prob-\nability function is unknown and not required [6, 49]. DEERS [44]\nsuggests that negative feedback can also have an impact on RSs.\nRML [25] improves on past heuristics by directly optimizing metrics\nfor relevant feedback. TPGR [1] proposes a tree-structured policy-\nbased model to solve the larger discrete action space problem. LIRD\n[45] utilizes actor-critic framework to make list-wise recommenda-\ntions. DeepPage [41] further takes into account the 2-D position\nof the recommended item sequences. Although model-free models\nachieve better results than model-based models, many problems\nremain. First, in RS, due to the large action space, the approximate\nestimation of value function is very difficult, and the reward func-\ntion is hard to determine. Second, it is hard to evaluate policy, and\nthe variance of evaluation methods is often unbounded.\n5 CONCLUSION\nWe present a novel reinforcement learning-based sequential recom-\nmender system, i.e., DT4Rec, in this work. Particularly, it avoids the\ninstability problem and unbounded variance headache by casting\nthe RL as an autoregressive problem. Furthermore, we contribute a\nseries of technologies to the success application of DT4Rec. Specifi-\ncally, the design of an auto-discretized reward prompt effectively\nmodels the numerical value of reward and allows guiding the train-\ning of models with long-term user engagement. The proposed con-\ntrastive supervised policy learning diminishes the inconsistencies\nbetween inference and training of the naive Decision Transformer.\nTo evaluate our model, we propose two stable metrics, i.e., MB-URS\nand SB-URS, which are verified to be more stable than existing ones.\nExtensive experiments conducted on the benchmark datasets have\ndemonstrated the effectiveness of the proposed methods.\nACKNOWLEDGEMENT\nThis research was partially supported by APRC - CityU New Re-\nsearch Initiatives (No.9610565, Start-up Grant for New Faculty of\nCity University of Hong Kong), SIRG - CityU Strategic Interdis-\nciplinary Research Grant (No.7020046, No.7020074), HKIDS Early\nCareer Research Grant (No.9360163), Huawei Innovation Research\nProgram and Ant Group (CCF-Ant Research Fund).\nREFERENCES\n[1] Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang,\nYuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with\ntree-structured policy gradient. InProceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 33. 3312â€“3320.\n[2] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin,\nPieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer:\nReinforcement learning via sequence modeling. Advances in neural information\nprocessing systems 34 (2021), 15084â€“15097.\n[3] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and\nEd H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender\nsystem. In Proceedings of the Twelfth ACM International Conference on Web Search\nand Data Mining . 456â€“464.\n[4] Minmin Chen, Bo Chang, Can Xu, and Ed H Chi. 2021. User response models\nto improve a reinforce recommender system. In Proceedings of the 14th ACM\nInternational Conference on Web Search and Data Mining . 121â€“129.\n[5] Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. 2019. Gen-\nerative adversarial user model for reinforcement learning based recommendation\nsystem. In International Conference on Machine Learning . PMLR, 1052â€“1061.\nUser Retention-oriented Recommendation\nwith Decision Transformer WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\n[6] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang.\n2021. A survey of deep reinforcement learning in recommender systems: A\nsystematic review and future directions. arXiv preprint arXiv:2109.03540 (2021).\n[7] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine translation.\narXiv preprint arXiv:1406.1078 (2014).\n[8] Abhinandan S Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.\nGoogle news personalization: scalable online collaborative filtering. In Proceed-\nings of the 16th international conference on World Wide Web . 271â€“280.\n[9] Nan Du, Yichen Wang, Niao He, Jimeng Sun, and Le Song. 2015. Time-sensitive\nrecommendation from recurrent user activities. Advances in neural information\nprocessing systems 28 (2015).\n[10] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and\nconsequences. Minds and Machines 30, 4 (2020), 681â€“694.\n[11] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier\nneural networks. In Proceedings of the fourteenth international conference on\nartificial intelligence and statistics . JMLR Workshop and Conference Proceedings,\n315â€“323.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[13] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 (2016).\n[14] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).\n[15] BalÃ¡zs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos\nTikk. 2016. Parallel recurrent neural network architectures for feature-rich\nsession-based recommendations. In Proceedings of the 10th ACM conference on\nrecommender systems . 241â€“248.\n[16] Mehdi Hosseinzadeh Aghdam, Negar Hariri, Bamshad Mobasher, and Robin\nBurke. 2015. Adapting recommendations to contextual changes using hierarchical\nhidden Markov models. InProceedings of the 9th ACM Conference on Recommender\nSystems. 241â€“244.\n[17] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,\nHeng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A tractable\ndecomposition for reinforcement learning with recommendation sets. (2019).\n[18] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE,\n197â€“206.\n[19] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma,\nZiyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto,\nXiaofei Wang, et al. 2019. A comparative study on transformer vs rnn in speech\napplications. In 2019 IEEE Automatic Speech Recognition and Understanding Work-\nshop (ASRU). IEEE, 449â€“456.\n[20] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of NAACL-HLT . 4171â€“4186.\n[21] Mikhail A Khovrichev, Marina A Balakhontceva, and Mikhail V Ionov. 2018.\nEvaluating sequence-to-sequence models for simulating medical staff mobility\non time. Procedia Computer Science 136 (2018), 425â€“432.\n[22] Muyang Li, Xiangyu Zhao, Chuan Lyu, Minghao Zhao, Runze Wu, and Ruocheng\nGuo. 2022. MLP4Rec: A Pure MLP Architecture for Sequential Recommenda-\ntions. In 31st International Joint Conference on Artificial Intelligence and the 25th\nEuropean Conference on Artificial Intelligence (IJCAI-ECAI 2022) . International\nJoint Conferences on Artificial Intelligence, 2138â€“2144.\n[23] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries.\nIn Text summarization branches out . 74â€“81.\n[24] Yuanguo Lin, Yong Liu, Fan Lin, Pengcheng Wu, Wenhua Zeng, and Chunyan\nMiao. 2021. A survey on reinforcement learning for recommender systems.arXiv\npreprint arXiv:2109.10665 (2021).\n[25] Ali Montazeralghaem, Hamed Zamani, and James Allan. 2020. A reinforcement\nlearning framework for relevance feedback. InProceedings of the 43rd international\nacm sigir conference on research and development in information retrieval . 59â€“68.\n[26] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics . 311â€“318.\n[27] Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores. In Proceedings of\nthe Third Conference on Machine Translation: Research Papers . 186â€“191.\n[28] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-\nizing personalized markov chains for next-basket recommendation. InProceedings\nof the 19th international conference on World wide web . 811â€“820.\n[29] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, et al . 2016. Mastering the game of Go with deep neural\nnetworks and tree search. nature 529, 7587 (2016), 484â€“489.\n[30] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-\nresentations from transformer. In Proceedings of the 28th ACM international\nconference on information and knowledge management . 1441â€“1450.\n[31] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning\nwith neural networks. Advances in neural information processing systems 27\n(2014).\n[32] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural\nnetworks for session-based recommendations. In Proceedings of the 1st workshop\non deep learning for recommender systems . 17â€“22.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[34] Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu. 2013. A\ntheoretical analysis of NDCG ranking measures. InProceedings of the 26th annual\nconference on learning theory (COLT 2013) , Vol. 8. 6.\n[35] Qingyun Wu, Hongning Wang, Liangjie Hong, and Yue Shi. 2017. Returning\nis believing: Optimizing long-term user engagement in recommender systems.\nIn Proceedings of the 2017 ACM on Conference on Information and Knowledge\nManagement. 1927â€“1936.\n[36] Xin Xin, Tiago Pimentel, Alexandros Karatzoglou, Pengjie Ren, Konstantina\nChristakopoulou, and Zhaochun Ren. 2022. Rethinking Reinforcement Learning\nfor Recommendation: A Prompt Perspective. In Proceedings of the 45th Inter-\nnational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 1347â€“1357.\n[37] Xing Yi, Liangjie Hong, Erheng Zhong, Nanthan Nan Liu, and Suju Rajan. 2014.\nBeyond clicks: dwell time for personalization. In Proceedings of the 8th ACM\nConference on Recommender systems . 113â€“120.\n[38] Chi Zhang, Yantong Du, Xiangyu Zhao, Qilong Han, Rui Chen, and Li Li. 2022.\nHierarchical Item Inconsistency Signal Learning for Sequence Denoising in Se-\nquential Recommendation. InProceedings of the 31st ACM International Conference\non Information & Knowledge Management . 2508â€“2518.\n[39] Weinan Zhang, Xiangyu Zhao, Li Zhao, Dawei Yin, Grace Hui Yang, and Alex\nBeutel. 2020. Deep Reinforcement Learning for Information Retrieval: Fundamen-\ntals and Advances. In Proceedings of the 43rd International ACM SIGIR Conference\non Research and Development in Information Retrieval . 2468â€“2471.\n[40] Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. 2019. Deep reinforcement\nlearning for search, recommendation, and online advertising: a survey. ACM\nSIGWEB Newsletter Spring (2019), 1â€“15.\n[41] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang\nTang. 2018. Deep reinforcement learning for page-wise recommendations. In\nProceedings of the 12th ACM Conference on Recommender Systems . 95â€“103.\n[42] Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and Jiliang Tang. 2020.\nWhole-Chain Recommendations. In Proceedings of the 29th ACM International\nConference on Information & Knowledge Management . 1883â€“1891.\n[43] Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and Jiliang Tang. 2021.\nUsersim: User simulation via supervised generativeadversarial network. In Pro-\nceedings of the Web Conference 2021 . 3582â€“3589.\n[44] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin.\n2018. Recommendations with Negative Feedback via Pairwise Deep Reinforce-\nment Learning. In Proceedings of the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining . ACM, 1040â€“1048.\n[45] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Dawei Yin, Yihong Zhao, and Jiliang\nTang. 2017. Deep Reinforcement Learning for List-wise Recommendations. arXiv\npreprint arXiv:1801.00209 (2017).\n[46] Lixin Zou, Weixue Lu, Yiding Liu, Hengyi Cai, Xiaokai Chu, Dehong Ma, Daiting\nShi, Yu Sun, Zhicong Cheng, Simiu Gu, et al. 2022. Pre-trained Language Model-\nbased Retrieval and Ranking for Web Search. ACM Transactions on the Web 17, 1\n(2022), 1â€“36.\n[47] Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, and Dawei Yin.\n2019. Reinforcement learning to optimize long-term user engagement in recom-\nmender systems. In Proceedings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining . 2810â€“2818.\n[48] Lixin Zou, Long Xia, Pan Du, Zhuo Zhang, Ting Bai, Weidong Liu, Jian-Yun Nie,\nand Dawei Yin. 2020. Pseudo Dyna-Q: A reinforcement learning framework for\ninteractive recommendation. In Proceedings of the 13th International Conference\non Web Search and Data Mining . 816â€“824.\n[49] Lixin Zou, Long Xia, Yulong Gu, Xiangyu Zhao, Weidong Liu, Jimmy Xiangji\nHuang, and Dawei Yin. 2020. Neural interactive collaborative filtering. InProceed-\nings of the 43rd International ACM SIGIR Conference on Research and Development\nin Information Retrieval . 749â€“758.\n[50] Lixin Zou, Long Xia, Linfang Hou, Xiangyu Zhao, and Dawei Yin. 2021. Data-\nEfficient Reinforcement Learning for Malaria Control. arXiv e-prints (2021),\narXivâ€“2105.\n[51] Lixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma, Suqi Cheng, Shuaiqiang\nWang, Daiting Shi, Zhicong Cheng, and Dawei Yin. 2021. Pre-trained language\nmodel based ranking in Baidu search. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining . 4014â€“4022.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8256272077560425
    },
    {
      "name": "Reinforcement learning",
      "score": 0.7636064887046814
    },
    {
      "name": "Counterfactual thinking",
      "score": 0.687880277633667
    },
    {
      "name": "Transformer",
      "score": 0.6023467183113098
    },
    {
      "name": "Inference",
      "score": 0.5368735194206238
    },
    {
      "name": "Machine learning",
      "score": 0.5323204398155212
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5233932137489319
    },
    {
      "name": "Embedding",
      "score": 0.5209806561470032
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.49925756454467773
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.44712457060813904
    },
    {
      "name": "Autoregressive model",
      "score": 0.4381718635559082
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}