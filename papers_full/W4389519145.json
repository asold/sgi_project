{
  "title": "Long Text Classification using Transformers with Paragraph Selection Strategies",
  "url": "https://openalex.org/W4389519145",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1970502999",
      "name": "Mohit Tuteja",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Daniel González Juclà",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3121217868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3186833834",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4296932804",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2144211451",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W4305033829",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3202026671",
    "https://openalex.org/W3136888420",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285261371",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3008736151"
  ],
  "abstract": "In the legal domain, we often perform classification tasks on very long documents, for example court judgements. These documents often contain thousands of words, so the length of these documents poses a challenge for this modelling task. In this research paper, we present a comprehensive evaluation of various strategies to perform long text classification using Transformers in conjunction with strategies to select document chunks using traditional NLP models. We conduct our experiments on 6 benchmark datasets comprising lengthy documents, 4 of which are publicly available. Each dataset has a median word count exceeding 1,000. Our evaluation encompasses state-of-the-art Transformer models, such as RoBERTa, Longformer, HAT, MEGA and LegalBERT and compares them with a traditional baseline TF-IDF + Neural Network (NN) model. We investigate the effectiveness of pre-training on large corpora, fine tuning strategies, and transfer learning techniques in the context of long text classification.",
  "full_text": "Proceedings of the Natural Legal Language Processing Workshop 2023, pages 17–24\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nLong Text Classification using Transformers with Paragraph Selection\nStrategies\nMohit Tuteja\nThomson Reuters Labs\nBangalore, Karnataka, India\nmohit.tuteja@thomsonreuters.com\nDaniel González Juclà\nThomson Reuters Labs\nZug, Switzerland\ndaniel.gonzalezjucla@thomsonreuters.com\nAbstract\nIn the legal domain, we often perform classifi-\ncation tasks on very long documents, for exam-\nple court judgements. These documents often\ncontain thousands of words, so the length of\nthese documents poses a challenge for this mod-\nelling task. In this research paper, we present\na comprehensive evaluation of various strate-\ngies to perform long text classification using\nTransformers in conjunction with strategies to\nselect document chunks using traditional NLP\nmodels. We conduct our experiments on 6\nbenchmark datasets comprising lengthy docu-\nments, 4 of which are publicly available. Each\ndataset has a median word count exceeding\n1,000. Our evaluation encompasses state-of-\nthe-art Transformer models, such as RoBERTa,\nLongformer, HAT, MEGA and LegalBERT and\ncompares them with a traditional baseline TF-\nIDF + Neural Network (NN) model. We in-\nvestigate the effectiveness of pre-training on\nlarge corpora, fine-tuning strategies, and trans-\nfer learning techniques in the context of long\ntext classification.\n1 Introduction\nThe performance of text classification methods has\nimproved significantly over the last decade for text\ninstances containing less than 512 characters. Due\nto the high computational cost of processing longer\ntext instances, this limitation is introduced in the\nmost recent transformer models. To alleviate this\nproblem and improve the classification of long\ntexts, researchers tried to address the root causes of\nthe computational cost and proposed the optimiza-\ntion of the attention mechanism (Vaswani et al.,\n2017), which is a key element of any transformer\nmodel.\nThis research undertakes a comparative study\nof different ways of doing long text, multi-label\nclassification on multiple legal datasets. We take\nexisting approaches from literature and try an al-\nternate approach of our own. The end goal is to\nstrike a balance between model performance and\ncomputational efficiency.\n2 Related Work\nNatural Language Processing (NLP) is a subfield\nof computer science and linguistics that focuses on\nprocessing language, including text. Probably the\nmost basic task within this field is Text Classifica-\ntion, which given text as an input the objective is\nto classify it among a set of categories. To perform\nthis classification, the model needs to extract fea-\ntures from the text and preferably to understand\ncorrelations between its different parts, in a similar\nway that us humans do where we connect different\nentities in a sentence to extract meaning from them.\nSome approaches treated text with a Bag-of-\nWords (BoW) model, as introduced by Harris (Har-\nris, 1954), where grammar and word order are dis-\nregarded and we consider text as a set of words with\ntheir multiplicity. Related to this concept, a popular\nidea to model text which became popular during\nthe era of Statistical NLP (1990s–2010s) is TF-IDF\n(Term Frequency - Inverse Document Frequency).\nFirst described in 1972, TF-IDF (Sparck Jones,\n1972) as its name indicates consists on vectorizing\ntext by reflecting how important a term is to a doc-\nument in a collection or corpus. Its main intuition\nis to divide the occurrences of the term in the text\nby the occurrences of this term in the whole corpus,\nthis way we get the relative importance of a given\nterm.\nIn the decade of the 2010s we dive into the era of\nthe Neural Models for NLP, starting in 2013 with\nthe word2vec embeddings (Mikolov et al., 2013)\nused to train a Recurrent Neural Network (RNN).\nThis idea of training representation of words and\nterms quickly became very popular and since then\nthere have appeared many methods to obtain these\nrepresentations in an unsupervised way to then use\nthem for downstream tasks, for example GloVe\n17\n(Pennington et al., 2014) and fastText (Bojanowski\net al., 2017).\nA big revolution in NLP started with the presen-\ntation of the Transformer architecture by Vaswani\n(Vaswani et al., 2017) and its attention mechanism.\nThis architecture later evolved into models that\nhave also been pre-trained to obtain representations\nof the input tokens, as for example BERT (Devlin\net al., 2018) and RoBERTa (Liu et al., 2019). Legal-\nBERT (Chalkidis et al., 2020) consists in BERT\nheavily pre-trained with legal data, and this spe-\ncialization has been proved to improve the perfor-\nmance of the model in downstream tasks that use\nthis language. Nonetheless, in this study we do\nnot include models pre-trained with legal corpora\nbecause it would include a bias when comparing\nwith other architectures that do not have a public\nversion pre-trained on this type of language.\nThe limitation of these mentioned models is that\ndue to the quadratic computational cost the atten-\ntion mechanism, they allowed up to 512 tokens\nin the input, while legal documents can easily be\nformed by thousands of them. This problem has\nled to two alternatives: truncation methods in or-\nder to process a part of the text, for example the\nfirst 512 tokens, and to derive the classification\nfrom that part and models that contain a more effi-\ncient attention mechanism rather than computing\nthe pair-wise attention between all the tokens in the\ninput.\nMany sparse-attention models have been pro-\nposed, the most popular ones being Longformer\n(Beltagy et al., 2020), BigBird (Zaheer et al., 2020)\nand most recently Mega (Ma et al., 2022) has\nproved to be the best in the benchmark for long se-\nquence modeling \"Long Range Arena\" (Tay et al.,\n2020). This latter one, despite being the best per-\nforming model for long sequences has not become\nvery popular due to not being a public model that\nhas been largely pre-trained. Another popular ap-\nproach to process long text are Hierarchical Atten-\ntion Transformers (HAT) (Pappagari et al., 2019),\nwhich aim at processing the whole text by encod-\ning chunks of typically 512 or less tokens at each\nstep and then encoding these together in a hierar-\nchical way, through cross-segment attention blocks\n(Chalkidis et al., 2022).\nSome research has been done in the direction of\nputting all these different methods with different in-\ntuitions in a fair comparison to solve a multi-label\nclassification task. Some of these studies include\nTextGuide (Fiok et al., 2021) which compared the\nperformance of different truncation methods in se-\nlecting the text to train a Language Model, Park\n(Park et al., 2022) studied and compared different\nefficient LMs and Chalkidis (Chalkidis et al., 2022)\ncompared Longformer to HATs for long document\nclassification. Nonetheless, no work has obtained\ndefinitive and actionable conclusions and more im-\nportantly no other work has compared traditional\napproaches such as TF-IDF vectorization followed\nby a Neural Network (NN) for classification with\ntruncation methods to train popular Language Mod-\nels such as BERT and Sparse-attention models de-\nsigned to process long documents at once.\n3 Research questions\nFew works have focused on widely comparing dif-\nferent text classification approaches, so this method\naims at setting the baseline for choosing an ap-\nproach for multi-label / multi-class text classifica-\ntion given a set of constraints. So, the research\nquestions that will be addressed are:\n• Which is the Truncation method that works\nthe best on input text?\n• Which is the approach that reports the highest\nperformance?\n• Which is the most cost-effective approach?\n• Can we train a cost-effective language model\non very long sequences?\n4 Datasets\nSix datasets with long documents have been cho-\nsen to compare the performance of the different\napproaches:\nCanadian Abridgement Dataset: This dataset\ncontains 369,943 Canadian court judgements and\ntheir classification as per the Abridgement classi-\nfication taxonomy as defined by the editorial team\nfor Checkpoint. The classification follows a hierar-\nchy, starting with 55 broad classes (subject titles).\nThese classes have further sub-categories going up\nto 6 levels. Each unique classification up to the 6th\nlevel is treated as one class. The data belongs to the\nperiod 2000-2021. We have used only those cases\nwhose classification belongs to the latest available\nversion of the taxonomy.\nCheckpoint Tax Type Classification: Thom-\nson Reuters monitors multiple sources (tax courts,\nrulings, official materials etc.) in order to incor-\nporate the latest changes in law or guidance into\nCheckpoint editorial content. Documents from\n18\nDataset Words/Doc # Classes\nCanada A 5,321 11,648\nPosture 50K 2,901 256\nTax Type 708 44\nSCOTUS 5,352 14\nMIMIC-III 2,260 19\nECtHR 2,140 10\nTable 1: Average words per document and number of\nunique classes for each dataset used\nthese sources are called Alerts. During post pro-\ncessing, the editors assign tags to these alerts which\nare known as tax-types which states what type of\ntax a given alert talks about. This is a multi-label\nclassification dataset. We currently have two types\nof alerts - rulings and official materials.\nPosture 50K: (Song et al., 2022) This data-set is\npublicly accessible and has been made available by\nThomson Reuters. It is designed for the purpose of\nidentifying the legal Procedural Postures involved\nin legal motions within a legal case. To illustrate,\na plaintiff might request an appellate court to over-\nturn a specific decision made by a lower court judge\nregarding a motion, which is referred to as an \"On\nAppeal\" procedural posture. The dataset comprises\n50,000 legal opinions, representing real-world legal\ncases in the United States, along with their corre-\nsponding postures. The majority of these cases\nspan from the year 2013 to 2020, with just three\ncases occurring prior to 2013.\nSCOTUS: (Chalkidis et al., 2021b) This consti-\ntutes one of the six datasets within LexGLUE. It\nencompasses court opinions issued by the United\nStates Supreme Court (SCOTUS). The primary\nchallenge within this dataset involves single-label\nmulti-class classification, with the aim of forecast-\ning the pertinent issue area for each court opinion.\nThere are 14 distinct classes that align with spe-\ncific issue areas, collectively encompassing 278\nissues centered around the subject matter of the\nlegal dispute.\nMIMIC-III Dataset: (Johnson et al., 2016)\nContains approx. 50k discharge summaries from\nUS hospitals. Each summary is annotated with one\nor more codes (labels) from the ICD-9 taxonomy.\nThe input of the model is a discharge summary, and\nthe output is the set of the relevant 1st level ICD-9\n(19 in total) codes.\nECtHR (Task B): (Chalkidis et al., 2021a) This\nis one of the six data-sets from LexGLUE. It com-\nprises approximately 11,000 cases sourced from\nthe public database of the European Court of Hu-\nman Rights (ECtHR). For each case, the dataset in-\ncludes a collection of factual paragraphs extracted\nfrom the case description. Each case is associated\nwith specific articles of the European Convention\non Human Rights (ECHR) that were purportedly\nviolated, as determined by the court. In terms of\nmodel input, it consists of the factual paragraph\nlist for a given case, while the model’s output com-\nprises the set of articles that are alleged to have\nbeen violated.\nThe first two datasets are internal from Thomson\nReuters, while the other four are publicly available.\nMimic-III contains medical documents and the rest\ncontain legal text, which is our focus area. Table 1\nshows a summary of the statistics of each dataset.\n5 Experiments\n5.1 Setup\nThe 10 selected methods, which are explained in\nthe next section, are trained for the 6 datasets with\na standardized methodology.\nLearning Rates: We trained each transformer\nmodel on three predefined learning rates to over-\ncome possible variability on the optimal learning\nrate across models and datasets. These learning\nrates were 2e-05, 1e-05 and 5e-06 respectively. For\neach model, we selected the version which pro-\nvided the best results on the respective development\ndataset. We then used this optimal configuration\nto score our test dataset. We used a standardized\nscript for training of all models, starting with the\nsame base model (RoBERTa) wherever possible.\nEvaluation metric: The authors have chosen\nmicro-f1 score as the metric to establish a com-\nparison between the different approaches. This is\nmotivated by it being the most used throughout the\nliterature when comparing models for the task of\nmulti-label classification.\nRepeatability: We ensured that this training is\nalso reproducible by using fixed seeds and used\nthe same batch size for the models across datasets\nand the same batch-size * accumulation-steps of 8\nacross models. Out of the four publicly available\ndatasets, we dropped a few observations only from\nECtHR. These were observations with missing la-\nbels. Aside from that, the train-dev-test datasets\nwere left unchanged and used as-is in each case.\nBase Model selection: We use RoBERTa as the\nbaseline transformer model instead of LegalBERT.\n19\nThis was done due to the following reasons:\n• LegalBERT uses LexGLUE (Chalkidis et al.,\n2021b) datasets during its pretraining process.\nHence we observed abnormal performance\ngains on the ECtHR data-set which was not\nvisible across other datasets.\n• Longformer starts its training from a\nRoBERTa checkpoint so the comparison with\nRoBERTa becomes fair\n• We decided to include MIMIC-III dataset for\nwhich LegalBERT might not be ideal.\nIt is to be noted though that we did observe 1-2%\ngains in F1 scores on legal datasets (other than\nLexGLUE) while making a switch from RoBERTa\nto LegalBERT.\n5.2 Models\nThe following models were evaluated:\n• TF-IDF + Neural Net (NN): Vectorization\nof each document through TF-IDF to then\ntrain a Neural Network to classify these vec-\ntors. Pre-processing included lower-casing,\nremoving punctuation, special characters, dig-\nits and words containing less than 3 charac-\nters. We used TfidfVectorizer from sklearn\nwith ngram range (1,2), and max features\ncapped at 50,000. The neural network used\n1 hidden layer, relu activation, dropout =\n.3 and batch size of 64. We iterated over\n3 learning rates (.001,.002,.01) and 3 layer\nsizes (128,256,512). Early stopping on val-\ncategorical-accuracy with patience = 3 was\nused. The same grid-search approach was\nconsidered for all data-sets and the hyper-\nparameters giving best micro-averaged F1\nscores on the dev dataset were finally selected.\n• RoBERTa First 512 tokens: Truncate the\nfirst 512 tokens of the input text to fine-tune\npre-trained RoBERTa. It is likely that docu-\nments tend to contain important and descrip-\ntive information at the beginning, as for exam-\nple abstracts or introductions.\n• RoBERTa Last 512 tokens:Truncate the last\n512 tokens of the input text to fine-tune pre-\ntrained RoBERTa. It is possible that important\ninformation is found at the end of a document,\nas for example summaries or conclusions.\n• RoBERTa First & Last 256 tokens: Con-\ncatenate the first and the last 256 tokens to\nobtain a chunk of 512 tokens to fine-tune pre-\ntrained RoBERTa. Information at the begin-\nning or at the end of a document e.g. introduc-\ntions or conclusions might give more general\ninformation about the document than text in\nthe middle, specially in a very long document.\n• RoBERTa First & Last 512 tokens: Fine-\ntune a pre-trained RoBERTa model with\nchunks corresponding to first and last 512 of\neach document, by splitting them into two\nsamples with the same labels. At inference\ntime, aggregate the predictions for these two\nchunks per document, which can be done in\ndifferent ways. For the results that we share\nin this paper, for the multi-label datasets, we\ntake the mean probability for each class be-\ntween the two chunks, and then find the best\nclassification threshold. For the multi-class\ncase, we replace the mean by max and select\nthe class with the highest probability.\n• RoBERTa w/ Best paragraph selection:\nThis is a custom approach to obtain a smaller\nand more targeted training dataset. First,\nwe train a traditional NN model on the tf-idf\nvectorization of the full training documents.\nThen for each training document, we do the\nfollowing:\n1. Split the document into a maximum of\n10 chunks of 512 tokens and predict for\neach of them with the previously trained\nmodel. Note: more than 10 chunks can\nbe used, if desired.\n2. Take a dot product between the predic-\ntion (probability) vector for each chunk\nwith the label vector for the document.\nNote: The label vector is already a binary\nvector indicating the presence of a class\nlabel.\n3. Select the chunk having the highest sim-\nilarity with the label vector, which is\nlikely the \"most important chunk\", as it\nhas been classified the best.\nHaving a dataset of the most important chunk\nper document, we now fine-tune a RoBERTa\nmodel for the multi-label classification task.\nAt inference time, it is not possible to select\nthe most important chunk for each document\nbecause we have no labels, so for each docu-\nment, we:\n1. Split the text into a maximum of 10\nchunks and predict for each of them inde-\n20\nFigure 1: Paragraph importance distribution for all dev datasets. We take only those documents where we have\n>=10 chunks for this representation\npendently with the fine-tuned RoBERTa\nmodel.\n2. The predictions for each of these chunks\nare then combined. The combination\ncan be done in multiple ways. One ap-\nproach could be taking a sum/average of\nthe predictions (probabilities) for each\nclass across chunks and then applying\nsome threshold. Another could be tak-\ning the highest probability for each class\nobserved across chunks and then apply-\ning a threshold. There were multiple ap-\nproaches like these that were evaluated.\n3. It has been found empirically that the\nmost effective way to combine the predic-\ntions is to first predict the class that has\nthe highest weighted average confidence.\nThe weights in this case come from the\nparagraph importance distribution given\nin Figure 1 and derived from the model\ntraining data set. Subsequently, we pre-\ndict all the other classes whose average\nconfidence is within a ratio of 0.8 to the\ntop class. For the multi-class classifica-\ntion case, we stop at only 1 prediction.\n• Longformer First 4096 tokens: Fine-tune\na pre-trained Longformer with the first 4096\ntokens of the document.\n• Longformer First & Last 512: tokens: Con-\ncatenate the first and the last 512 tokens to\nobtain a chunk of 1024 tokens to fine-tune\na pre-trained Longformer. Same intuition as\nwith RoBERTa and possible efficiency boost\ncompared to Longformer processing 4096 to-\nkens.\n• HATs First 4096 tokens: Fine-tune a pre-\ntrained HAT with the first 4096 tokens of the\ndocument. The chosen implementation uses\nBERT as a backbone model.\n• HATs First & Last 512 tokens: Concate-\nnate the first and the last 512 tokens to obtain\na chunk of 1024 tokens to fine-tune a pre-\ntrained HAT.\n• MEGA: Moving Average Equipped Gated\nAttention 4096: Fine-tune a pre-trained (by\nus) MEGA with the first 4096 tokens of the\ndocument. Pre-training details can be found\nin appendix A.\n6 Results\n6.1 Paragraph importance\nIn the explanation of the \"RoBERTa w/ Best para\nselection\" approach it has been mentioned that dur-\ning the training phase of the model we select the\nchunk within each document that has been pre-\ndicted the best, which we call the most important.\nIf we select the documents that were split into 10\nchunks (the maximum allowed, 5 chunks at the be-\nginning and 5 at the end) and we plot the histogram\nof which was the most important paragraph, we\nget the results in Figure 1.\nFigure 1 shows how the first paragraph is the\nmost important in the three Thomson Reuters\ndatasets. In MIMIC-III though, the chunks at the\nend have more importance than the ones at the\n21\nFigure 2: Heat-map of testing Micro-F1s per dataset. Inference time for TF-IDF+DNN corresponds to CPU\nevaluation, while the rest have been evaluated on a single GPU.\nbeginning. In the LexGLUE datasets, it is more\nevenly distributed. In the next section, it is seen\nhow thesedistributions relate to the performance\nof the approaches that truncate different parts of\nthe text.\n6.2 Comparison of testing results\nFigure 2 contains the testing Micro-F1 score at in-\nference time for each combination of dataset and\napproach, together with a color coding to mak-\ning it easier to spot the best and worst performing\napproaches for each dataset. These are the main\nobservations:\n• There is not one clear model that performs\nthe best across all datasets, even though some\napproaches perform consistently better than\nothers. In 4 out of the 6 datasets, Longformer\ntrained with the first 4096 tokens provides the\nbest performance, but on the other hand it is\nthe least efficient.\n• TF-IDF + DNN provides a strong baseline,\nas it is not far from the rest of RoBeRTa-based\napproaches and it is hundreds of times more\nefficient (using a CPU, while RoBERTa-based\nare evaluated using a single GPU).\n• Models that truncate first tokens work well\non Thomson Reuters datasets because of the\nimportance of the first chunk, as seen in the\n\"Paragraph Importance\" section,\n• Mimic-III and ECtHR-B are the only two\ndatasets where the first chunk is not the most\nimportant, as it has been seen in the previous\nsection, hence RoBERTa with first 512 tokens\nshows worse performance.\n• RoBERTa concatenating first and last 256\ntokens provides a better performance than\nRoBERTa first 512 on 5 out of 6 datasets, so\nin cases where it is possible to have relevant\ninformation at the end of the document, it can\nbe a better approach.\n• The versions of Longformer and HATcon-\ncatenating first and last 512 tokens provide\na comparable performance to their version\nwith the first 4096 tokens, while being around\n4 times more efficient.\n• Longformer performs better than HAT for 4\nout of 6 datasets, but it is less efficient. Their\nversions concatenating the first and last 512\ntokens also perform very similar.\n• The in-house pretrained MEGA has a con-\nsistently good performance for all datasets\nwithout being the best in any, but it is consider-\nably more efficient than the other two models\nwith 4096 tokens.\n7 Conclusions and Future Work\nWe recommend the following Model training\nGuidelines:\n1. Train a baseline model (TF-IDF + DNN). It is\nthe most efficient and typically reports good\nperformance. This could be a good solution if\n1-2% lower performance compared to SOTA\nis acceptable.\n22\n2. Plot the paragraph importance for your\ndataset, and identify which paragraphs are the\nmost important. In case the Paragraph Im-\nportance has a U shape, like court judgement\ndatasets, the best choice is usually a concate-\nnated model.\n• LegalBERT/RoBERTa concatenate first\nand last 256, if good efficiency is needed.\n• Longformer/HAT concatenate first and\nlast 512, if efficiency is not the priority.\n3. In case Paragraph importance is more uniform,\nbest choice is a Long model, as all paragraphs\nare important:\n• Longformer with first 4096 tokens will\nprobably report the best performance, but\nit is expensive to train.\n• HAT Concat first and last 512 will prob-\nably report a slightly lower performance\nthan Longformer 4096, while being quite\nmore efficient.\n• LegalBERT concat or LegalBERT trun-\ncate will probably still report decent re-\nsults, while being efficient.\nFuture work could be done in this topic by com-\nparing the presented approaches to newer models\nthat have come out after this study has been done.\nSome of these models can be:\n• Llama 2 (Touvron et al., 2023) is a family\nof LLMs released by Meta with a context\nof 4096 tokens. Their extensive pre-training,\ntheir open-source availability and their large\ncontext length makes them a good option to\nbe added to the comparison.\n• LongNet (Ding et al., 2023) is an architec-\nture published by Microsoft Research, whose\nauthors claim to be able to process up to a mil-\nlion tokens. This is a very interesting feature\nsince it would be able to entirely process any\ndocument in our datasets.\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the associa-\ntion for computational linguistics, 5:135–146.\nIlias Chalkidis, Xiang Dai, Manos Fergadiotis, Prodro-\nmos Malakasiotis, and Desmond Elliott. 2022. An\nexploration of hierarchical attention transformers for\nefficient long document classification. arXiv preprint\narXiv:2210.05529.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert: The muppets straight out of law\nschool. arXiv preprint arXiv:2010.02559.\nIlias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapat-\nsanis, Nikolaos Aletras, Ion Androutsopoulos, and\nProdromos Malakasiotis. 2021a. Paragraph-level\nrationale extraction through regularization: A case\nstudy on European court of human rights cases. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 226–241, Online. Association for Computa-\ntional Linguistics.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael\nBommarito, Ion Androutsopoulos, Daniel Martin\nKatz, and Nikolaos Aletras. 2021b. Lexglue: A\nbenchmark dataset for legal language understanding\nin english. arXiv preprint arXiv:2110.00976.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,\nShaohan Huang, Wenhui Wang, Nanning Zheng, and\nFuru Wei. 2023. Longnet: Scaling transformers to\n1,000,000,000 tokens.\nKrzysztof Fiok, Waldemar Karwowski, Edgar Gutierrez-\nFranco, Mohammad Reza Davahli, Maciej Wilam-\nowski, Tareq Ahram, Awad Al-Juaid, and Jozef Zu-\nrada. 2021. Text guide: improving the quality of long\ntext classification by a text selection method based on\nfeature importance. IEEE Access, 9:105439–105450.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1–9.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian\nHe, Liangke Gui, Graham Neubig, Jonathan May,\nand Luke Zettlemoyer. 2022. Mega: moving av-\nerage equipped gated attention. arXiv preprint\narXiv:2209.10655.\n23\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nRaghavendra Pappagari, Piotr Zelasko, Jesús Villalba,\nYishay Carmiel, and Najim Dehak. 2019. Hierarchi-\ncal transformers for long document classification. In\n2019 IEEE automatic speech recognition and under-\nstanding workshop (ASRU), pages 838–844. IEEE.\nHyunji Hayley Park, Yogarshi Vyas, and Kashif Shah.\n2022. Efficient classification of long documents us-\ning transformers. arXiv preprint arXiv:2203.11258.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nDezhao Song, Andrew V old, Kanika Madan, and Frank\nSchilder. 2022. Multi-label legal document classifi-\ncation: A deep learning-based approach with label-\nattention and domain-specific pre-training. Informa-\ntion Systems, 106:101718.\nKaren Sparck Jones. 1972. A statistical interpretation\nof term specificity and its application in retrieval.\nJournal of documentation, 28(1):11–21.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2020. Long\nrange arena: A benchmark for efficient transformers.\narXiv preprint arXiv:2011.04006.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nHyperparameters\nDatasets Bookcorpus + CC\nNews + Wikipedia\n(20220301.en)\nMax sequence\nlength\n256\nTraining batch size 32\nAccumulation steps 16\nTraining steps 77k\nLearning rate 0.001\nWeight decay 0.05\nLearning rate\nwarmup %\n0.006\nEncoder depth 6\nEncoder embedding\ndim\n256\nEncoder Z dim 128\nEncoder hidden dim 512\nDropout 0.1\nActivation function Silu\nTable 2: MEGA pretraining hyperparameters\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in neural information\nprocessing systems, 33:17283–17297.\nA MEGA pretraining\nMoving Average Gated Attention (MEGA) is an\narchitecture that has demonstrated state-of-the-art\nperformance in long sequence modelling, given\nthat it has the best results for the Long Range Arena\nat the time the research has been done.\nAlso at this time there was not any available pre-\ntrained version of MEGA as it was released very\nrecently. Nonetheless, there was a public imple-\nmentation, so we decided to pretrain this model on\nthe Masked Language Modeling task in order to\nlater fine-tune it for the multi-label classification\ntask in the 6 different datasets.\nThe parameters used for MEGA pretraining can\nbe found down in table 2.\n24",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8363310098648071
    },
    {
      "name": "Paragraph",
      "score": 0.7724939584732056
    },
    {
      "name": "Transformer",
      "score": 0.7092557549476624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6579388380050659
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5565638542175293
    },
    {
      "name": "Natural language processing",
      "score": 0.5149293541908264
    },
    {
      "name": "Document classification",
      "score": 0.4905545115470886
    },
    {
      "name": "Machine learning",
      "score": 0.43144968152046204
    },
    {
      "name": "Task (project management)",
      "score": 0.4274410605430603
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4253963530063629
    },
    {
      "name": "Transfer of learning",
      "score": 0.42531779408454895
    },
    {
      "name": "World Wide Web",
      "score": 0.1075516939163208
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "cited_by": 7
}