{
  "title": "AND does not mean OR: Using Formal Languages to Study Language Models’ Representations",
  "url": "https://openalex.org/W3174693310",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2806434176",
      "name": "Aaron Traylor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2906846704",
      "name": "Roman Feiman",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2013784948",
      "name": "Ellie Pavlick",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3035718362",
    "https://openalex.org/W3035097102",
    "https://openalex.org/W4298325241",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W3203259592",
    "https://openalex.org/W2962765587",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W3156509887",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W1699946128",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W3101204082",
    "https://openalex.org/W2986266667"
  ],
  "abstract": "Aaron Traylor, Roman Feiman, Ellie Pavlick. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 158–167\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n158\nAND does not mean OR: Using Formal Languages to Study Language\nModels’ Representations\nAaron Traylor\nDept. of Computer Science\nBrown University\nRoman Feiman\nDept. of Cognitive, Linguistic,\nand Psychological Sciences\nBrown University\n{aaron traylor, roman feiman, ellie pavlick}@brown.edu\nEllie Pavlick\nDept. of Computer Science\nBrown University\nAbstract\nA current open question in natural language\nprocessing is to what extent language models,\nwhich are trained with access only to the form\nof language, are able to capture themeaning of\nlanguage. In many cases, meaning constrains\nform in consistent ways. This raises the pos-\nsibility that some kinds of information about\nform might reﬂect meaning more transparently\nthan others. The goal of this study is to in-\nvestigate under what conditions we can expect\nmeaning and form to covary sufﬁciently, such\nthat a language model with access only to form\nmight nonetheless succeed in emulating mean-\ning. Focusing on propositional logic, we gen-\nerate training corpora using a variety of moti-\nvated constraints, and measure a distributional\nlanguage model’s ability to differentiate logi-\ncal symbols (¬, ∧, ∨). Our ﬁndings are largely\nnegative: none of our simulated training cor-\npora result in models which deﬁnitively differ-\nentiate meaningfully different symbols (e.g.,∧\nvs. ∨), suggesting a limitation to the types of\nsemantic signals that current models are able\nto exploit.\n1 Introduction\nA current open question in natural language pro-\ncessing is to what extent language models (LMs;\nneural networks trained to predict the likelihood of\nword forms given textual context) are capable of\ntruly understanding language. Bender and Koller\n(2020) argue that, since such models are trained ex-\nclusively on the form of language, they cannot pos-\nsibly learn the meaning of language. We argue that\nthe question of whether language models can learn\nmeaning cannot be settleda priori. While language\nmodels only have direct access to form, linguistic\nform often correlates with meaning. The strength\nof the correlation varies across both different as-\npects of language and different tests of linguistic\ncompetence. While several intuitive tests of un-\nderstanding (e.g., demonstrating knowledge of the\nword dog by identifying pictures of dogs) are out\nof scope for LMs, many tasks which NLP aspires\nto solve (e.g., question answering, machine transla-\ntion) operate entirely on natural language input and\noutput. Thus, a relevant question is whether models\nwhich operate only on the forms of language can\nnonetheless learn to differentiate meanings.\nOur goal is to focus on a tractable subproblem\nin order to improve our intuitions about the types\nof distributional signals that LMs can use to extract\ninformation relevant to meaning. We simulate a\nlanguage modeling setup using propositional logic,\nin which we can naturally operationalize form to\nbe strings of symbols in the language and mean-\ning to be truth conditions. We deﬁne the semantic\ntransparency of a text-only training corpus to be\nthe degree to which an LM trained on that cor-\npus learns to differentiate between aspects of form\nthat affect truth conditions and aspects of form that\ndo not. We have two primary research questions.\nFirst, what constraints on corpus generation pro-\nduce greater semantic transparency? And second,\nare any such constraints sufﬁcient for an LM to\nadequately differentiate meanings?\n2 Experimental Design\n2.1 Dataset Generation\nWe consider theform of a sentence to be simply the\nobserved, syntactically-valid strings of characters\nand the meaning to be the truth conditions. Propo-\nsitional logic is a simple language in which we can\ncharacterize both form and meaning. We use the\ngrammar in Table 1, with standard semantics.\nWe focus our analysis on whether the represen-\ntations of logical operators (∧,∨,¬) are inﬂuenced\nby distributional patterns that go beyond their su-\nperﬁcial syntactic similarity evident in the grammar.\nThat is, if a trained LM identiﬁes that the meanings\n159\nS → (S∧S) |(S∨S) |(¬S) |(sym)\n∧→ ∧ 1 |∧2 ···|∧ K\n∨→ ∨ 1 |∨2 ···|∨ L\n¬→ ¬ 1 |¬2 ···|¬ M\nsym → sym1 |sym2 ···| symN\nTable 1: Propositional logic grammar.\nof ∧1 ···∧k are identical to one another, and differ-\nent from the meanings of ∨1 ···∨l, we expect the\nembeddings for the ∧i to be more similar to one\nanother than they are to any of the∨i or the ¬i. We\nconsider a corpus to be semantically transparentif\nan LM trained on the corpus learns semantically-\nclustered representations of the logical operators.\nWe generate four different training corpora, mo-\ntivated by different assumptions one might make\nabout how natural language corpora arise. These\nconstraints are as follows, ordered roughly from\nweakest to strongest:\n1. Syntactic Constraint. Speakers only generate\nsentences which are syntactically well-formed (that\ncan be parsed by a syntactic parser). Here, this\namounts to sampling from the grammar without\nadditional constraints.\n2. Truthfulness Constraint. Speakers of the\nlanguage are constrained to generate sentences\nthat are true in some context, i.e., that evaluate\nto True in at least one possible world. To im-\nplement this, we again sample from the grammar\nbut additionally check with a satisﬁability checker\nand omit sentences which are not satisﬁable. E.g.,\n(sym1 ∧(¬(sym1))) would not appear.\n3. Informativity Constraint. Speakers generate\nsentences not just to state true facts, but to provide\nlisteners with information about a particular state\nof affairs. To simulate such a constraint, we ran-\ndomly sample a set of “target worlds”T and a set\nof “alternative worlds”Asuch that T∩A= ∅. We\nthen generate the shortest sentence ssuch that s\nis true in every world in T and sis false in every\nworld in A. We experiment with several sizes of\nT and A, but report only on |T|= |A|= 2as this\nprovides the right balance of contextual diversity.\nSee Appendix for additional discussion.\n4. Explicit Grounding. We consider a setting\nin which speakers explicitly dictate the full state\nof affairs, without ambiguity. This is not intended\nas a realistic model of how corpora are generated,\nbut rather to provide an upper bound on semantic\ntransparency by giving models a corpus in which\nform is perfectly correlated with meaning. We\ngenerate this corpus in the same way as the Truth-\nfulness corpus, but append an explicit marker of the\ntruth values1of the variables in the sentence, e.g.:\n(sym1 ∧(¬(sym2))) <sep> sym1 T sym2 F.\nSampling Parameters. Each dataset consists of\n100K training and 1K validation sentences. We\nset the number of non-reserved symbols (N in the\nabove grammar) to 5,000, and the number of “syn-\nonyms” of each logical symbol (K,L,M) to be 5.\nThus, a sentence in one of our datasets might look\nlike (sym1 ∧3 (¬4(sym85))), and would be true if\nand only if sym1 is true and sym85 is false 2.\nWe generate sentences using a probabilistic\ncontext-free grammar with the rules shown above.\nThe tree depth d of a generated sentence is con-\ntrolled by a parameter γsuch that P(d|d−1) =γd.\nThe number of unique variables in a sentence3 is\nsampled from a non-zero Poisson distribution pa-\nrameterized by λ. We set λ = 2 and γ = .85 in\nthe reported experiments, but don’t ﬁnd parameter\nchoice affects our conclusions. Note that the In-\nformativity dataset is generated deterministically,\nand thus sampling parameters do not apply and sen-\ntences in that dataset are shorter. Dataset statistics\nand data generation parameter sensitivity are in the\nAppendix.\n2.2 Models and Training\nWe consider LSTM and Transformer LMs of differ-\ning sizes, shown in Table 2. Each model is trained\non one of the above four datasets until convergence\non the associated validation set using early stop-\nping with a patience of 15 epochs. The LMs were\nimplemented in PyTorch (Paszke et al., 2019) and\ntook roughly 5 hours to converge on TitanV , Ti-\ntanRTX, and QuadroRTX GPUs 4. We randomly\ninitialize the embedding layer. Hyperparameter\ndetails can be found in the Appendix. We train 5\nrandom restarts of each setting. Due to the regular\nnature of our synthetic data, we found larger mod-\n1Sampled from the set of satisfying variable assignments.\n2We began by experimenting with many different dataset\nsizes and vocab counts. However, we did not ﬁnd that models\nbehaved differently on larger datasets and so focused on the\nsmaller ones for convenience. See Appendix for results with\ndifferent model sizes.\n3We set a maximum number of variables per sentence in\norder to bound the number of possible variable assignments.\n4Code publicly available at https://github.com/\nattraylor/semantic-transparency-code.\n160\nModel Syntactic Truthfulness Informativity Grounded\nSmall LSTM (192K) 21.2 / 87.7 / 87.7 17.6 / 88.7 / 88.6 21.5 / 99.6 / 99.5 21.2 / 87.5 / 87.5\nMedium LSTM (545K) 17.6 / 90.2 / 90.1 17.5 / 89.6 / 89.5 20.9 / 99.9 / 99.8 8.3 / 89.3 / 86.8\nSmall Trans. (311K) 11.8 / 86.9 / 84.6 12.4 / 87.2 / 85.4 21.7 / 98.4 / 98.2 10.3 / 86.2 / 83.1\nMedium Trans. (377K) 11.4 / 91.3 / 90.6 9.9 / 92.0 / 91.3 18.1 / 99.5 / 99.5 9.1 / 91.7 / 89.8\nTable 2: Summary of language modeling performance. For each model, on each training dataset, we report PPL /\n%Syn / %Semwhere PPL is the perplexity on heldout data (drawn from the same distribution as the training cor-\npus), %Syn is the percentage of generated sentences that are syntactically well formed (i.e., parseable), estimated\non a set of 1,000 generations sampled from the trained model, and % Sem is the percentage of generated sentences\nthat are semantically well formed (i.e., satisﬁable), estimated on the same set of 1,000.\nels overﬁt the training data quickly, and thus focus\non smaller models.\n3 Results and Discussion\nLanguage Modeling Performance. We ﬁrst\nsanity check that the trained models indeed func-\ntion as LMs before evaluating the lexical represen-\ntations. We compute the models’ perplexity on\nheldout data. However, since perplexity is not com-\nparable across conditions (since each constraint\nleads to differently distributed corpora) we also\nsample 1,000 generated sentences from each model\nand compare by measuring whether the sentences\nare 1) syntactically well-formed (i.e., parseable)\nand 2) semantically well-formed (i.e., satisﬁable).\nEven in the case of models trained with the Syntac-\ntic constraint, as seen in Table 2, most of the sen-\ntences produced are nonetheless satisﬁable. We see\nno difference between the Syntactic, Truthfulness,\nand Explicit Grounding conditions on these met-\nrics. (The Informativity numbers are likely higher\ndue to the shorter sentences that result from that\ngenerative process.) The fact that models trained\nonly on satisﬁable sentences nonetheless generate\nsentences which do not abide by such constraints\nsuggests the models fail to encode less overt distri-\nbutional patterns, which depend, for example, on\nrecognizing abstract relations such as “sameness”\nof symbols in order to recognize violations (e.g.,\n(A ∧(¬A)). The failure to capture such properties\nof the data even in this simpliﬁed setting might\nhave negative implications for the models’ ability\nto infer abstract semantic relationships from more\ncomplex natural language corpora.\nRepresentations of Logical Symbols. Again,\nour ﬁrst question is: What constraints on corpus\ngeneration yield the greatest amounts of semantic\ntransparency? We quantify this by measuring how\nwell the embeddings learned by the trained LMs\ncorrespond to our truth-theoretic notions of seman-\ntic equivalence: e.g., are ∧1 and ∧2 more similar\nto one another than ∧1 and ∨1? We use a nearest\nneighbors probing classiﬁer to evaluate whether\nmodels distinguish the operators at the lexical level.\nWe run k-fold cross validation, in each iteration\nchoosing one symbol per class (i.e., one ∧, one ∨,\none ¬) as the class exemplars, and then classifying\nthe remaining points using cosine similarity. We set\nkto 125, so that we observe every symbol combi-\nnation as exemplars. We report accuracy averaged\nacross folds and random restarts.\nProbing classiﬁer results are shown in Figure 1.\nFigure 2 shows an embedding visualization for one\nmodel (Medium Transformer). We ﬁnd that train-\ning on the Syntactic and on the Explicit Grounding\ndataset leads to the least and the most distinguish-\nable operators respectively for all models, and the\nother conditions end up between these values.\nThese results address our ﬁrst question: there is\nsome difference in semantic transparency between\ndifferently constrained datasets. Interestingly, the\nTransformer models perform better in the Truth-\nfulness condition than in the Syntactic condition,\nwhich the LSTMs fail to differentiate. This sug-\ngests that, even if it does not necessarily manifest in\nthe models’ generations (Table 2), the Transformer\narchitecture may nonetheless be capable of picking\nup on some of the more abstract distributional pat-\nterns via which syntax and semantics are correlated.\nFurther work on larger models would be required\nto explore this in depth.\nIn addition, we observe little difference between\nthe quality of the representations learned in the\nInformativity condition and those learned in the\nTruthfulness condition; one exception might be\nin the Medium LSTM, though we cannot conﬁrm\nthat this difference is robustly reproducible. Thus,\n161\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0score\nSmall LSTM\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMedium LSTM\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSmall Transformer\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMedium Transformer\nFigure 1: Each value in this graph represents average classiﬁcation score across 125 iterations of a simple nearest\nneighbor probing classiﬁer averaged across 5 random seeds of the model (625 accuracy numbers per box and\nwhiskers plot). The dotted line is random chance / maximum class accuracy (33%).\nRandom\n Syntactic\nAND synonyms\nOR synonyms\nNOT synonyms\nTruthfulness\n Explicit Grounding\nFigure 2: PCA of the representations created by the\nMedium Transformer model.\nbased on our experiments, there is no evidence that\nInformativity alone yields greater semantic trans-\nparency. However, we note that the experimental\nsetup for Informativity is not directly comparable\nto the others (e.g., sentences are shorter and less\ndiverse than in Truthfulness) and thus further study\nwould be needed to make strong claims, positive or\nnegative.\nFinally, we note that in nearly all cases, models\nare able to differentiate ¬from the other opera-\ntors, likely because it is a unary operator and thus\nsyntactically different from the binary operators.\nThus the difference in accuracy is almost entirely\ndue to whether the representations of ∧and ∨are\ndifferentiated (as shown in Figure 2). This gives a\nnegative answer to our second question concerning\nwhether any constraints are sufﬁcient for an LM\nto adequately differentiate meaning. Apart from\nthe Small Transformer on the Explicit Grounding\ncondition, none of the models can completely dis-\ntinguish between symbols that are similar in form\nbut different in meaning.\n4 Related Work\nIt is an open question whether neural models can\nlearn abstract functions (Marcus, 2001). Our work\nbuilds upon a large body of research intended to\nprobe which aspects of language and meaning are\nbeing captured by large LMs. Most closely re-\nlated is work that assesses whether models can per-\nform symbolic reasoning about language (Kassner\net al., 2020) e.g., quantiﬁers or negation (Talmor\net al., 2020; Ettinger, 2020; Kassner and Sch¨utze,\n2020; Wang et al., 2018) or by measuring the sys-\ntematicity of models’ inferences (Goodwin et al.,\n2020; Kim and Linzen, 2020; Yanaka et al., 2020;\nWarstadt et al., 2019). Such work has tended to\nﬁnd that LMs reason primarily contextually as op-\nposed to abstractly. Our evaluation method– which\nasks whether word embeddings cluster according to\ntheir truth-conditional meaning– is related to recent\nwork which deﬁnes text-only models as “grounded”\nif the learned embedding space is isomorphic to\nthe similarity function deﬁned over a ground-truth\nmeaning representation (Merrill et al., 2021). More\ndistantly related is work on LMs’ ability to reason\nabout numbers (Wallace et al., 2019) or perform\nmulti-hop reasoning (Yang et al., 2018). Prior work\nthat examines neural networks’ ability to perform\nlogical reasoning is superﬁcially related (Evans\net al., 2018). In this way, our work builds on past\nwork that uses synthetic rather than natural lan-\nguage datasets in order to probe model behavior\nin the absence of confounds. Notable examples\nare SCAN for measuring compositionality and gen-\neralization (Lake and Baroni, 2018) and Kassner\net al. (2020) which investigates LM knowledge ac-\nquisition and fact memorization using a synthetic\ndataset of entity-relation tuples.\n162\n5 Conclusion\nUsing propositional logic corpora to simulate a con-\ntrolled language modeling setting, we ask: 1) Do\nproperties of the training corpus affect LMs’ abili-\nties to differentiate the meanings of logical opera-\ntors? and 2) Do any training corpora lead to models\nthat differentiate these meanings to a satisfactory\ndegree? Our results imply a positive answer to (1):\nModels trained on corpora generated with differ-\nent constraints appear to perform differently at the\ntask of separating ∧from ∨. However, these differ-\nences are a function of both data and model. For\nexample, the Transformer architecture seems better\nable to learn from weaker signal (corpora generated\nonly with a Truthfulness constraint), while LSTMs\nrequire more explicit signal (direct access to truth\nvalues). On question (2), our results are largely neg-\native for the syntactically similar operators. Even\nthe most semantically transparent training data did\nnot enable models to separate the representations\nof symbols with similar form but different meaning.\nOnly the Small Transformer trained on the Explicit\nGrounding condition can perfectly differentiate ∧\nfrom ∨at the lexical level, despite the task’s con-\ntrolled nature. However, every model did separate\n¬from both ∧and ∨, illustrating how syntactic\ndifferences can support differentiation of meaning.\nOverall, we contribute a novel framework, based\non syntax and semantics of propositional logic, via\nwhich we can explore questions of the linguistic\ncapabilities and weaknesses of neural LMs. Our\nexperiments represent a ﬁrst step in this line of\nwork, but further work is needed to fully appre-\nciate the implications of these results in natural\nlanguage settings, in particular, how closely the\nconstraints explored here mirror real corpora, and\nhow such learning is inﬂuenced by noise and am-\nbiguity found in human language. One speciﬁc\nlimitation of our experiments is that we constrain\nour analysis to the lexical representations– i.e., we\nassume that differences between the meanings of ∧\nand ∨should be encoded in the lexicon, via context-\ninvariant type embeddings. While this assumption\nis commonplace in formal semantics, neural LMs\nopen the possibility of alternative representations\nof lexical and compositional semantics. Our results\ndo not rule out the possibility that the relevant se-\nmantic distinctions are encoded elsewhere in the\nmodel, above the lexical layer. However, we take\nthe combination of the lexical probing results and\nLM generation results as suggestive but not con-\nﬁrmational evidence of a more general negative\nﬁnding.\n6 Acknowledgements\nWe would like to thank Najoung Kim and the par-\nticipants of the NALOMA 2020 Workshop for their\nthoughtful feedback on early versions of this work.\nThis work was supported by IARPA under the BET-\nTER program, contract number 19051600004.\nReferences\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198. Association\nfor Computational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nRichard Evans, David Saxton, David Amos, Pushmeet\nKohli, and Edward Grefenstette. 2018. Can neural\nnetworks understand logical entailment? In Interna-\ntional Conference on Learning Representations.\nEmily Goodwin, Koustuv Sinha, and Timothy J.\nO’Donnell. 2020. Probing linguistic systematicity.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1958–1969. Association for Computational Linguis-\ntics.\nNora Kassner, Benno Krojer, and Hinrich Sch ¨utze.\n2020. Are pretrained language models symbolic rea-\nsoners over knowledge? In Proceedings of the 24th\nConference on Computational Natural Language\nLearning, pages 552–564. Association for Compu-\ntational Linguistics.\nNora Kassner and Hinrich Sch¨utze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818. Association\nfor Computational Linguistics.\nNajoung Kim and Tal Linzen. 2020. COGS: A com-\npositional generalization challenge based on seman-\ntic interpretation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 9087–9105. Associa-\ntion for Computational Linguistics.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In In-\nternational Conference on Machine Learning, pages\n2873–2882. PMLR.\n163\nGary F Marcus. 2001. The algebraic mind: Integrating\nconnectionism and cognitive science. MIT press.\nWill Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A. Smith. 2021. Provable limitations of acquir-\ning meaning from ungrounded form:what will future\nlanguage models understand? TACL.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know\nnumbers? probing numeracy in embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5307–\n5315, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investi-\ngating BERT’s knowledge of language: Five anal-\nysis methods with NPIs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2877–2887, Hong Kong,\nChina. Association for Computational Linguistics.\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, and\nKentaro Inui. 2020. Do neural models learn sys-\ntematicity of monotonicity inference in natural lan-\nguage? In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 6105–6117. Association for Computa-\ntional Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\npages 2369–2380, Brussels, Belgium. Association\nfor Computational Linguistics.\n164\n7 Appendix\n7.1 Dataset generation parameters\nThere are several parameters involved in the cre-\nation of our synthetic propositional logic datasets:\n• Number of sentences in the training set\n• Number of unique non-reserved variables (N)\n• Number of each operator (K, L, M)\n• Sentence depth parameter (γ)\n• Poisson distribution parameter for unique non-\nreserved variables in sentence (λ)\nIn comparison to dataset sizes for large language\nmodels in modern natural language processing,\nthe dataset size (100k training examples) and vo-\ncabulary size (5k symbols + 5 of each operator)\nof our main experimental results (Figure 1) are\nrather small. We sought to determine whether our\nchoice for dataset size and non-restricted variable\ncount greatly changed the ﬁnal results– do our con-\nclusions change based on these parameters? We\ntrained models on different variations of our initial\nparameters.\nFirst, we swept across training set sizes (20k,\n100k, and 500k examples) and number of symbols\n(500, 5k, 50k) while holding all other parameters\nconstant (γ = .85, λ= 2, K, L, M = 5). We used\nthe Medium Transformer model, which performed\nthe best across our four models, and observed the\nresults of the probing classiﬁer on the embeddings\nafter training separately on each model.\nThe results of the above sweep are shown in\nFigure 3. We do not ﬁnd that the models perform\ndramatically differently on any of the datasets when\ndataset size and number of non-reserved symbols\nare varied.\nWe also experimented with changing the num-\nber of operator synonyms (e.g. ∧1,∧2,...∧K) We\nexperimented with three different sizes– (K, L, M)\n= 5, 25, 100– for each of our 4 datasets. Those re-\nsults are shown in Figure 5, and average frequency\nis shown in Table 3. We found that adding addi-\ntional synonyms of each operator hurt performance–\nlikely because adding additional synonyms of ∧\nand ∨made generalization more challenging, caus-\ning the models’ performance to drop.\nIn a set of earlier experiments, to choose the\nsentence depth (γ) and Poisson distribution (λ) pa-\nrameters, we hyperparameter searched on the Ex-\nplicit Grounding condition across three values of\nK, L, M Syn. Tru. Inf. Grd.\n5 49.7k 49.2k 16.3k 49k\n25 9.94k 9.84k 3.25k 9.81k\n100 2.49k 2.46k 0.81k 2.45k\nTable 3: Average count of each operator across each of\nthe datasets.\neach (nine datasets in total). Speciﬁcally, we tested\nλ = 2,3,5 and γ = .7,.8,.85. We then trained\nthe transformer model once on each of the nine\ndatasets, and the results are shown in Figure 6. We\nchose λ= 2and γ = .85.\n7.2 Informativity dataset information\nWe tested different settings of |T|(number of tar-\nget worlds) and |A|(number of alternative worlds).\nFor |T|= 1,|A|= 1, the best choice of s will\nalways be a single sym or its negation. For ex-\nample, with variables sym1,sym2, we might sam-\nple max variables = 2 and thus T = (sym1 =\nT,sym2 = F),A = (sym1 = F,sym2 = F).\nThe shortest sentence would then be sym1, as\nit sufﬁciently distinguishes T from A. How-\never, with |T|= 1,|A|= 2, we might generate\nT = (sym1 = T,sym2 = F),A = ((sym1 =\nF,sym2 = F),(sym1 = T,sym2 = T)). Now\nthe shortest sentence that can be generated is\n(sym1 ∧1 ¬1(sym2)).\n|T|= 1,|A|= 2 and |T|= 2,|A|= 1 result\nin sentences that are both short and structurally\nnearly identical, although inverted. This is due\nto the truth conditions allowed by each operator.\nWe generate the datasets for each combination and\nreport the results in Table 4. We excluded these\ndatasets because of the simplicity and similarity\nof the sentences. We found that |T|= 2,|A|= 2\nallows for sentences that are much more varied.\n165\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0score\n2.00e+04 ex., 5.00e+02 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2.00e+04 ex., 5.00e+03 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2.00e+04 ex., 5.00e+04 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0score\n1.00e+05 ex., 5.00e+02 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.00e+05 ex., 5.00e+03 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.00e+05 ex., 5.00e+04 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0score\n5.00e+05 ex., 5.00e+02 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n5.00e+05 ex., 5.00e+03 symbs\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n5.00e+05 ex., 5.00e+04 symbs\nFigure 3: Average probing classiﬁer score across example count / number of unique non-variable symbols for the\nMedium Transformer model.\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0score\nSmall LSTM\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMedium LSTM\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSmall Transformer\nSyn. Truth. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMedium Transformer\nFigure 4: This graph contains the same experiments as Figure 1, but is only the accuracy on ∧and ∨, excluding\nthe results of the negation operator.\n166\nInform. 1T/1A Inform. 1T/2A Inform. 2T/1A\nSent. Count Sent. Count Sent. Count\na 4523 (a∧b) 27047 (a∨b) 27236\n¬(a) 4460 (a∧¬(b)) 21474 ¬((a∧b)) 21392\n¬((a∨b)) 21338 (a∨¬(b)) 21260\n(¬(a) ∧b) 21061 (¬(a) ∨b) 21045\n¬(a) 4544 a 4559\na 4536 ¬(a) 4508\nTable 4: All sentences generated for the ﬁrst three Informativity datasets fell into one of these templates. Arbitrary\nsymbols are replaced with aand b. This distinction happens because of the truth conditions that are allowed by the\n∧and ∨operators.\nDataset Sent. Len. Average sym count Average op count Average Unique syms\nSyntactic 28.51 6.19 7.44 2.27\nTruthfulness 28.25 6.14 7.37 2.33\nInform. (2T/2A) 10.92 2.83 2.70 2.20\nExpl. Ground 34.06 8.51 7.40 2.33\nTable 5: Averaged statistics per sentence for the different datasets (training sets). All datasets are 100K training\nexamples and 1k heldout examples.\nModel LR symb dim hidden dim # heads # layers dropout\nSmall LSTM .0001 4 32 1 0.0\nMedium LSTM .0001 32 64 2 0.2\nSmall Transformer .0001 4 32 2 4 0.0\nMedium Transformer 5e-05 32 128 4 4 0.2\nTable 6: Hyperparameters for each model.\n167\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0score\n5 operators\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n25 operators\nSyn. Tru. Inf. Ground.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n100 operators\nFigure 5: Sweep across number of operators using the\nMedium Transformer model.\n 2, \n 0.7\n 2, \n 0.8\n 2, \n 0.85\n 3, \n 0.7\n 3, \n 0.8\n 3, \n 0.85\n 5, \n 0.7\n 5, \n 0.8\n 5, \n 0.85\ndataset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0score\ntransformer\nFigure 6: Sweep across λand γvalues for the Explicit\nGrounding dataset using a Transformer model.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7138455510139465
    },
    {
      "name": "Computational linguistics",
      "score": 0.5349416136741638
    },
    {
      "name": "Joint (building)",
      "score": 0.5083863139152527
    },
    {
      "name": "Natural language processing",
      "score": 0.5055508613586426
    },
    {
      "name": "Linguistics",
      "score": 0.4981374740600586
    },
    {
      "name": "Programming language",
      "score": 0.4598390460014343
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37625443935394287
    },
    {
      "name": "Engineering",
      "score": 0.09700378775596619
    },
    {
      "name": "Philosophy",
      "score": 0.08784249424934387
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ]
}