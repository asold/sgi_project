{
  "title": "Addressing Some Limitations of Transformers with Feedback Memory",
  "url": "https://openalex.org/W3123673616",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221722489",
      "name": "Fan, Angela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3001935590",
      "name": "Lavril, Thibaut",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222857661",
      "name": "Grave, Edouard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214255973",
      "name": "Joulin, Armand",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228082540",
      "name": "Sukhbaatar, Sainbayar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2890177507",
    "https://openalex.org/W1847088711",
    "https://openalex.org/W2790259362",
    "https://openalex.org/W2991324852",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2963601622",
    "https://openalex.org/W2896528354",
    "https://openalex.org/W2988841832",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2951560313",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2173051530",
    "https://openalex.org/W1732222442",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W2768716007",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963641307",
    "https://openalex.org/W2804044248",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2145107163",
    "https://openalex.org/W2997517014",
    "https://openalex.org/W2956480774",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2980433389",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2908802752",
    "https://openalex.org/W2971842688",
    "https://openalex.org/W1544827683"
  ],
  "abstract": "Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.",
  "full_text": "ADDRESSING SOME LIMITATIONS OF TRANSFORMERS\nWITH FEEDBACK MEMORY\nAngela Fan†, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar\nFacebook AI Research, †LORIA\n{angelafan,thibautlav,egrave,ajoulin,sainbar}@fb.com\nABSTRACT\nTransformers have been successfully applied to sequential, auto-regressive tasks de-\nspite being feedforward networks. Unlike recurrent neural networks, Transformers\nuse attention to capture temporal relations while processing input tokens in parallel.\nWhile this parallelization makes them computationally efﬁcient, it restricts the\nmodel from fully exploiting the sequential nature of the input. The representation\nat a given layer can only access representations from lower layers, rather than\nthe higher level representations already available. In this work, we propose the\nFeedback Transformer architecture that exposes all previous representations to all\nfuture representations, meaning the lowest representation of the current timestep\nis formed from the highest-level abstract representation of the past. We demon-\nstrate on a variety of benchmarks in language modeling, machine translation, and\nreinforcement learning that the increased representation capacity can create small,\nshallow models with much stronger performance than comparable Transformers.\n1 I NTRODUCTION\nIn recent years, the Transformer architecture (Vaswani et al., 2017) has brought large improvements\nto a wide range of Natural Language Processing tasks such as machine translation, sentence rep-\nresentation (Devlin et al., 2019), and summarization (Edunov et al., 2019). Transformers are also\nsuccessfully used as an autoregressive model on sequential tasks such as language modeling (Dai et al.,\n2019; Rae et al., 2020) and reinforcement learning (Parisotto et al., 2019). Unlike more traditional\nrecurrent architectures such as RNNs and LSTMs, the Transformer architecture processes a sequence\nin parallel in an order-invariant way. Techniques such as position embeddings (Sukhbaatar et al.,\n2015; Shaw et al., 2018) and attention masking are required to capture input order information. In\nthis work, we focus on several limitations of the Transformer architecture as an autoregressive model\nand present a straightforward solution — Feedback memory. These limitations and our proposed\nsolution target sequential token prediction tasks, such as language modeling or other auto-regressive\ngenerative tasks.\nThe feedforward nature of Transformers makes them efﬁcient on modern hardware, but restricts the\nTransformer from taking full advantage of the input’s sequential property. In particular, the current\nhidden representation of a Transformer only accesses the past representations of lower layers, even\nthough higher level representations of the past have already been computed as an autoregressive\nmodel. At generation, the Transformer generates only one token at a time, so it could access these\nrepresentations for better performance, but does not exploit these at training time due to parallelization.\nHowever, if these past higher level representations could be used at training time, they would enrich\nfuture lower level representations, enabling shallower models to have the same representation power.\nAnother inherent limitation of Transformers on sequential tasks is the lack of recursive computa-\ntion (Dehghani et al., 2018), and the number of transformations possible on the input is bounded by\nthe model depth. Such disadvantages have impact on tasks that require careful tracking of a world\nstate or modeling hierarchical structures (Tran et al., 2018; Hahn, 2020). On the other hand, while\nRNNs can maintain an internal state for an unbounded time while accumulating more computations\nupon it, the size of this internal state is limited by the dimension of the hidden state.\nIn this work, we propose a novel autoregressive model, the Feedback Transformer, that makes all\nprevious hidden representations accessible to the computation of a representation at any depth —\n1\narXiv:2002.09402v3  [cs.LG]  25 Jan 2021\nthe model feeds back previous computations to itself. The feedback allows the model to perform\nrecursive computation, building stronger representations iteratively upon previous states. To achieve\nthis, we modify self-attention to attend to higher level representations rather than lower ones.\nAs shown in Figure 1, the Feedback Transformer merges the hidden states from all layers into a single\nvector for every time step and stores them in a memory. Instead of self-attention, all subsequent\nlayers attend to this memory, which means every previously computed representation is accessible\nby all future layers, mediated by the memory. This allows Feedback Transformers to recursively\ncompute and transform an input as many times as the input length, which is something Transformers\ncannot achieve. While RNNs can perform recursive computation, the amount of information that\nFeedback Transformers can maintain is not limited by the number of layers.\nThere are computational beneﬁts to this straightforward modiﬁcation. First, it uses less memory\nbecause all the layers share a single Feedback memory, thus reducing the memory size by L times,\nwhere L is the number of layers. There is also less computation because we share the key and\nvalue projections during attention computation, which increases the speed of the attention over the\nFeedback Memory. Further, the GPU memory usage is reduced due to the memory sharing — the\noverall model is 2x smaller — allowing the batch size to be increased for computational efﬁciency.\nDuring inference, the increased batch size contributes to substantially faster decoding speeds.\nIn summary, our main contributions are: (1) The Feedback Transformer architecture, which com-\npletely changes the way a Transformer works to access available higher level representations im-\nmediately. (2) We show the Feedback Transformer can achieve state of the art results with smaller,\nshallower models that have faster decoding speed and smaller memory footprint. (3) The Feedback\nTransformer uses substantially less memory during training and inference time.\n2 R ELATED WORK\nSeveral previous works have analyzed the limitations of Transformer architectures, such as the\ninability to process input sequentially (Dehghani et al., 2018) or represent hierarchical structure (Tran\net al., 2018). Hahn (2020) demonstrate that Transformers cannot model structures involving bounded\nrecursion, such as closing parentheses. Pérez et al. (2019) study Transformers in the context of Turing\nmachines, where they must produce unbounded numbers of decoding steps. Various work in probing\nTransformers identiﬁed several limitations where Transformers may not have the computational\ncapacity of recurrent architecture like an LSTM (Hahn, 2020).\nFrom the architectural perspective, our work shares similarities with recurrent networks augmented\nwith external shared memories (Graves et al., 2014; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015).\nFor example, the stack augmented RNN of Joulin & Mikolov (2015) adds an external memory to a\nrecurrent network to keep long term dependencies. Closer to our work, the Neural Turing Machine\nof Graves et al. (2014) models an unconstrained memory that resembles the self-attention layer of a\nTransformer. Further improvements to recurrent networks, such as the Gated Feedback RNN (Chung\net al., 2015), are based on better controlling signal from different layers and extended to feedback\nthrough multiple pathways (Jin et al., 2017). These works are built on recurrent networks with\nadditional components to store long term dependencies.\nOther works have studied modiﬁcations to the Transformer architecture by enriching its structure\nwith components inspired by recurrent networks. For example, Wang et al. (2019) propose adding a\nlocal recurrent sublayer to the Transformer layer to remove the need for position embeddings in the\nmulti-head self-attention layers. Universal Transformer (Dehghani et al., 2018) share the parameters\nbetween the layers of a Transformer, leading a recurrent network in depth. Hao et al. (2019) and\nChen et al. (2018) augment Transformers with a second, recurrent encoder. As opposed to our\nwork, these prior investigations do not change the computational path in a Transformer to reduce the\ndiscrepancy between the training and inference time. Closer to our work, Merity (2019) proposes\nadding a self-attention layer on top of the past outputs from an LSTM cell. However, this approach\nkeeps the recurrent and the self-attention mechanisms decoupled, as opposed to ours which makes\nthe attention mechanism recurrent. In particular, the LSTM layer of Merity (2019) still intrinsically\nhas a bottleneck corresponding to the dimension of the hidden layer.\n2\nLayers\nTime\nMemory\nxt\nyt\nattention\nweighted sum\nFigure 1: The Feedback Transformermerges\npast hidden representations from all layers into\na single vector and stores it in memory.\nFigure 2: Difference between Feedback and\nTransformer. t indicates the timestep and l\nindicates the layer.\n3 M ETHOD\nIn this section, we propose the Feedback Transformer, which provides capacity to build richer\nrepresentations of each timestep t of a sequential modeling task.\n3.1 T RANSFORMER ARCHITECTURES\nWe brieﬂy describe the Transformer (Vaswani et al., 2017). Each layer is composed of a multi-\nhead self-attention sublayer (Attn) followed by a feedforward sublayer ( FF), and each sublayer\nis followed by an add-norm operation that combines a skip-connection (He et al., 2016) and layer\nnormalization (Lei Ba et al., 2016). The l-th layer of a Transformer processes an input sequence of\nvectors Xl = (xl\n1, . . . ,xl\nt) into a sequence of vectors of the same length. First, the self-attention\nsublayer computes a representation for each time step t by taking its related input vector xt along\nwith its past context, {xl\nt−τ, ...,xl\nt−1}:\nzl\nt = Attn(xl\nt, {xl\nt−τ, . . . ,xl\nt−1}).\nWithin the self-attention sublayer,xl\ntis used to form query vectors while its context is used to compute\nkey and value vectors, forming a memory of the past information. Then the feedforward sublayer\nprocesses each vector zl\nt independently, i.e., xl+1\nt = FF(zl\nt). The Transformer layer transforms its\ninput sequence into an output sequence Xl+1 = FF(Attn(Xl)).\nIn practice, a block of steps {xl\nt−M+1, . . . , xl\nt}is computed in parallel during training, where M\ncan be seen as the backpropagation through time (BPTT) length. This makes training Transformers\nefﬁcient on hardware such as GPUs. However, to operate on sequences of unbounded length,\nTransformers require modiﬁcations such as caching and relative position embeddings (Dai et al.,\n2019; Sukhbaatar et al., 2019).\n3.2 L IMITATIONS OF TRANSFORMERS\nPrevious work has analyzed the impact of several limitations of the Transformer architecture, such as\nthe inability to track long sequences and process hierarchical inputs (Hahn, 2020). In this work, we\nfocus on two major limitations of Transformer architectures.\nLimited Access to Higher Level Representations. Layer by layer, Transformers build more\nabstract, high level representations of the input sequence. At each layer, the representations for the\ninput sequence are treated in parallel. As a consequence, a Transformer does not leverage the highest\nlevel representations from the past to compute the current representation, even though these highest\nlevel representations have already been computed for autoregressive models.\nMaintaining a Belief State.Many sequential tasks require models to maintain an internal state for\ntwo main purposes. First, internal states act as memory for recalling past inputs, where Transformers\nexcel because their internal state xl\nt is directly accessible to future steps through self-attention.\n3\nThe second role of an internal state is to act as a belief state that tracks the world state that is not\ndirectly observable in inputs. For example, when inputs are actions taken on a Markov Decision\nProcess, an internal state can apply those changes to the current belief state and correctly predict\nthe outcome. As a feedforward model, Transformer have inherent limitations in this area — only\na ﬁxed number of transformations can be applied to its internal states. Since both Attn and FF\nsublayers contain a ﬁxed number of transformations and there are L layers of them, the total number\nof transformations between the input and output is limited by the depth. This means Transformers\ncannot maintain an internal state for long time if it has to be frequently updated.\n3.3 F EEDBACK TRANSFORMER\nWe propose to change the Transformer architecture by using the most abstract representations from\nthe past directly as inputs for the current timestep. This means that the model does not form its\nrepresentation in parallel, but sequentially token by token. More precisely, we replace the context\ninputs to attention modules with memory vectors that are computed over the past, i.e.,\nzl\nt = Attn(xl\nt, {mt−τ, . . . ,mt−1}),\nwhere memory vectors mt are computed by summing the representations of all layers at time step t:\nmt =\nL∑\nl=0\nSoftmax(wl)xl\nt, (1)\nwhere wl are learnable scalar parameters. Note these scalars are the only new parameters introduced\nby our change, with all else the same as the standard Transformer. Here l = 0corresponds to token\nembeddings. The weighting of different layers by a softmax output gives the model more ﬂexibility\nas it can average them or select one of them.\nThis modiﬁcation of the self-attention input adapts the computation of the Transformer from parallel\nto sequential, summarized in Figure 2. Indeed, it provides the ability to formulate the representation\nxl\nt+1 based on past representations from any layer l′, while in a standard Transformer this is only true\nfor l′< l. This change can be viewed as exposing all previous computations to all future computations,\nproviding better representations of the input. Such capacity would allow much shallower models to\ncapture the same level of abstraction as a deeper architecture. This has several practical advantages,\nas more shallow models have reduced memory footprint and increased decoding speed.\nAn alternative view of such an architecture modiﬁcation is providing the capacity for recursive\ncomputation — outputs from a sublayer can feed back to the same sublayer through the memory.\nThe model can then maintain an internal state for unbounded time. This is a clear advantage over\nTransformers, in which a submodule never looks at its own output. While an RNN can also repeat its\ncomputation on its internal state, its internal state has a limited capacity determined by the number of\nlayers and their hidden dimension. In contrast, the internal state of a Feedback Transformer is its\nwhole memory, which can grow with the input length. This allows the model to keep track of a large\nnumber of things within its internal state.\nWhile our modiﬁcation requires sequential computation, we signiﬁcantly improve training speed by\nsharing the key and value projections Wl\nk and Wl\nv across all layers. This sharing reduces computation\nbecause we need to compute key and value vectors only once instead of computing them per layer\nkl\nt = kt = Wkmt vl\nt = vt = Wvmt.\nFor the same reason, the memory footprint is smaller than a standard Transformer because only one\nset of kt, vt needs to be stored. To be more precise, the memory requirement for processing a single\ntoken is reduced from O(L ×T) to O(T), where L is the number of layers and T is the context size.\nFurther, the reduced memory usage allows the batch size to be increased to recover some of the lost\nparallelism, which improves training speed. Thus, the Feedback Transformer is not much slower\ncompared to the standard Transformer. Note that the same sharing of projections will not make the\nstandard Transformer efﬁcient because those projections are applied to different representations at\neach layer (the key and value vectors will not the same for all layers).\nLastly, we note that the sequential nature of the Feedback Transformer does not affect the performance\nduring generation where one needs to compute one step at a time anyway. The same is true for online\nreinforcement learning where the input must be processed sequentially even during training.\n4\nTask Trans- Feedback\nformer Trans.\nCopy Char 59.1 76.2\nSeq 6.2 23.6\nReverse Char 50.2 74.8\nSeq 5.9 29.2\nCounting Len 50 99.6 99.7\nLen 1K 82.4 95.3\nRandom Walk 68 100\nAlgorithmic 3 vars 33.7 99.1\n5 vars 37.5 92.6\nTable 1: Accuracy on toy tasks. Char is\ncharacter accuracy, Seq is sequence accuracy.\n20 40 60 80 100\nMemory size\n0.5\n0.6\n0.7\n0.8\n0.9\nSuccess rate\nTransformer\nFeedback Transformer\nFigure 3: Results on the Corridor task.The\nTransformer degrades as the memory size de-\ncreases, but the Feedback Transformer main-\ntains performance.\n4 E XPERIMENTS\nWe explore different sequential input tasks in natural language processing and reinforcement learning.\nFirst, we demonstrate the downsides of the standard Transformer architecture on tasks where the\nTransformer performs poorly. We show that the Feedback Transformer is able to overcome challenges\nand retain long memory. Next, we highlight the strength of the Feedback architecture in building\ncomplex, high level representations even with shallow models. We demonstrate that the Feedback\nmodel can achieve signiﬁcantly stronger results than Transformer models, an effect that is exaggerated\nas models get smaller. Finally, we compare the Feedback architecture to the Transformer architecture\nwith other work on standard long-context language modeling tasks. In experiments on large datasets,\nwe use the shared key-value projections to improve training time. Additional experimental details\nand results can be found in the appendix.\n4.1 L IMITATIONS OF TRANSFORMER : I LLUSTRATIVE TASKS\n4.1.1 L IMITED ACCESS TO LONG MEMORY\nFirst, we examine the Transformer’s limited access to long memory on several simple, straightforward\ntasks that illustrate this. Unlike the standard Transformer, the Feedback architecture is able to\nremember information over many timesteps.\nWalking down a Corridor.In this reinforcement learning task, each agent is placed at the start of\na long corridor with either a blue or green object. The agent must look at the object’s color, walk\ndown the corridor, and go through the corresponding colored door at the end. The only task is to\nremember the color and not become distracted by walking down the very long hallway. Results are\nshown in Figure 3 and show that the performance of the Transformer degrades quickly as the memory\nsize shrinks, but the Feedback Transformer maintains strong performance at all memory sizes.\nCopy and Reverse. We experiment next on two algorithmic tasks, copy and reverse (Kaiser &\nSutskever, 2015). We train on sequences of length 40 consisting of integers 0 through 9, and test\non sequences of length 400. Models read the input and then either copy or reverse, which requires\nmemory over the sequence and the ability to track position, as well as generalization capability as\nthe train and test settings are different lengths. We consider two variations of copying and reversing:\neither at the character level or at the sequence level. Results are shown in Table 1. The Feedback\narchitecture has large improvements in accuracy, indicating improved memory and positional tracking.\nCounting. Finally, we experiment on a counting task, where models have a sequence ofA’s in a\nrow, and must output the corresponding quantity of the letter B. The model must count the number of\nthe A’s to output the correct number of B’s. We consider two settings: training on short sequences of\n5\nlengths up to 50 and training on long sequences of lengths up to 1000. We show results in Table 1,\nwhere we demonstrate the Feedback model is much better at counting over long sequences.\n4.1.2 L IMITED STATE UPDATES\nThe complexity of the representations the Transformer is able to formulate is strictly dependent on\nthe depth, as each layer of the Transformer allows for additional nonlinearity. The Transformer, then,\ncan only update its state the same number of times as it has layers. We demonstrate that the Feedback\nTransformer does not have this limitation — in tasks where the model must carefully track and update\nits state, the Feedback architecture is able to update its state at each timestep.\nRandom Walk. We consider a random walk in a small grid where actions are: go forward 1 step,\nleft turn, and right turn. Given a history of actions and the agent’s initial position, it is strictly possible\nto calculate the current position. The task is trivial because a human could write down the current\nlocation and direction and keep updating with each action. However, Transformers cannot do this\nbecause they lack a storage that can be updated with each input. Its hidden state can store this\ninformation, but with each update, that information has to go up one layer.\nAn alternative approach to this task is to solve it all at once given a sequence of actions, which\nis feasible for Transformers since they can access all inputs with their attention. However, this\napproach is challenging because the effect of each action depends on the direction at that point and\nwhether the agent is on the edges, which itself is not known yet. This can be seen in Table 1, where\nthe Transformer struggles and only reaches 68% accuracy. In contrast, the Feedback Transformer\nachieves 100% accuracy, which indicates the ability to track state for a long period of time. Both\nmodels are trained on 10K sequences, each containing 100 random actions and positions.\nAlgorithmic task. A more complex setting where tracking and updating of a state is crucial is\ncode executions. A model needs keep track of all variable values and update them if necessary. To\ndemonstrate this, we create a simple algorithmic task that consists of the following simple statements:\nassignments (e.g. x=5), increments and decrements (e.g. y--), conditionals (e.g. if x==4: y++),\nand print commands (e.g. print(x)). Each task consists of 100 randomly selected statements. We\nconsider two settings with 3 and 5 different variables.\nProcessing of each statement in parallel will not work because conditional statements cannot be\nexecuted without knowing the current variable value, which itself can depend on another conditional.\nAs shown Table 1, Transformers cannot solve this task because every time a variable increment\nor decrement, its value can only be found one layer up in the model, and eventually will be lost.\nDoubling their layers from 4 to 8 does help little, bringing the accuracy to 47.4% on the 3 variable\nversion and 29.1% on the 5 variable version, but their performance is far from perfect. A recurrent\nmodel like LSTM is capable of storing a variable value while updating it, thus perform well on the 3\nvariables version with an accuracy of 82.8%. However, its performance drop to 32.1% when there are\nmore variables because it has to store all their values in a single vector. The Feedback Transformer\ndoes not have this bottleneck, and can access updated variable values from the lowest layer, so it\ngives strong performance on this task.\n4.2 A DVANTAGES OF FEEDBACK ARCHITECTURE\nWe examined two limitations of standard Transformers that we improve upon: limited memory span\nand limited ability to update state. In the Feedback model, we improve on these limitations and now\nanalyze performance on practical tasks including translation and reinforcement learning.\n4.2.1 S TRONG PERFORMANCE WITH SMALL , SHALLOW MODELS\nThe Feedback Transformer is able to create higher level, more abstract representations with fewer\nlayers and less capacity, as a layer can use all of the most recently created representations of previous\ntimesteps. We demonstrate on neural machine translation that the Feedback model performs much\nbetter than Transformers at small, shallow sizes. Note that for sequence to sequence, we use Feedback\nTransformers only in the decoder because the encoder inputs are available simultaneously.\n6\n2000 3000 4000 5000\nDecoding speed (wps)\n27.5\n28.0\n28.5\n29.0\nTest BLEU\nTransformer\nFeedback Transformer\n0 1 2 3 4\nTraining steps ×109\n4\n6\n8\n10\n12Reward\nTransformer\nFeedback Transformer\nFigure 4: (left) Machine Translation onWMT14 En-De, test set BLEU and decoding speed in\nwords-per-second for varying decoder depths. (right) Maze Navigation in Gridworld.We display\naverage reward comparing Feedback Transformer to standard Transformers.\nWe evaluate the performance of the Feedback Transformer on the WMT14 En-De machine trans-\nlation benchmark of 4.5 million pairs. We follow Vaswani et al. (2017) and train onWMT16 using\nnewstest2013 as dev and newstest2014 as test. We learn 32K joint byte pair encodings\n(Sennrich et al., 2016), generate with beam size 5, tuning a length penalty on the dev set. We average\nthe last 10 checkpoints and apply compound splitting and compute tokenized BLEU.\nIn Figure 4 (left), we display results when making the model shallower only — layers are removed\nfrom a Feedback Transformer decoder compared to Transformers. As the decoder becomes shallow,\nthe gap in performance between the two architectures widens. While the 1-layer Transformer model\ncan only reach 27.3, the Feedback Transformer has 28.3 BLEU. Shallow decoders are critical to\nfast inference — reducing to 1-layer improves decoding speed by 4.2x, while only losing 1 BLEU\nwith the Feedback architecture. Such results are useful for practical applications, where the speed of\nproducing a translation is very important. We report decoding speed in tokens per second on 1 GPU.\nWe further experiment with large encoder but shallow decoders. The Feedback Transformer achieves\n29.0 BLEU with 12 layer encoder and 2 layer decoder. As the encoder is parallelized even during\ninference, the increased size of the encoder has negligible impact on decoding speed. To stabilize the\ntraining of deeper models, we use LayerDrop (Fan et al., 2019).\n4.2.2 L ONG MEMORY TRACKS STATE\nWe apply Feedback to a reinforcement learning maze task that requires long memory to optimally\nsolve because agents have limited vision. Note that in such reinforcement learning tasks, the models\nare trained online using A2C, so the input must be processed sequentially even during training\ntime. Thus, the non-parallelized nature of the Feedback Transformer is not a drawback, and training\nFeedback Transformers is as fast as Transformers.\nThe goal is to navigate a procedurally generated random maze where colored objects are placed. One\nof the colors will be randomly selected as a target, and the agent has to reach it for a reward and a new\ntarget. For optimal performance, the agent must remember the maze and object locations. In addition,\nthe agent has turn actions like the Random Walk task, which makes it necessary to keep track of its\nlocation and orientation. As shown in Figure 4 (right), the Feedback Transformer converges to reach\nhigher average reward, compared to Transformers. Results are shown averaged over 10 trials.\n4.3 C OMPARISON TO OTHER ARCHITECTURES\nIn this section, we ﬁrst, we compare Feedback to recurrent architectures such as LSTM, as well as\nhybrid RNN-Transformer architectures, and show that the Feedback is more powerful than recurrence\nalone. Next, we compare our construction of the Feedback Memory with other possible compositions.\nLastly, we compare to other Transformer architectures on competitive benchmarks.\n7\nModel Test\nRecurrent Architectures\nDenseNMT Shen et al. (2018) 25.5\nRNMT+ (Chen et al., 2018) 28.5\nHybrid Architectures\nBiARN (Hao et al., 2019) 28.9\nSRU (Lei et al., 2017) 28.4\nTransformer Architectures\nTransformer (Vaswani et al., 2017)28.4\nTransformer (Ott et al., 2018) 29.3\nFeedback Transformer 29.5\nTable 2: Results on WMT En-De compar-\ning the Feedback Transformer to Recurrent\narchitectures, hybrid Recurrent-Transformer\nmodels, and standard Transformers.\nGHY\u0003ESF\n\u0014\u0011\u0014\u0013\n\u0014\u0011\u0014\u0018\n\u0014\u0011\u0015\u0013\n\u0014\u0011\u0015\u0018\n\u0014\u0011\u0016\u0013\nEDVHOLQH UHFXUUHQW WRS\u0010RQO\\ DOO\nFigure 5: Comparison of different memory com-\nposition strategies on char-PTB. The recurrent\nconnection alone is not as effective as feedback\nconnections from a higher layer.\n4.3.1 C OMPARISON TO RECURRENT ARCHITECTURES\nWe compare the Feedback Transformer architecture to recurrent architectures like LSTMs as well\nas hybrid RNN-Transformer architectures. In Table 2, we display that the Feedback Transformer\nhas stronger performance than the Transformer, RNN, and RNN-Transformer hybrid model. We\nnote that recurrent models address some limitations of Transformer architectures, but the Feedback\nmechanism goes beyond that. By allowing all past representations to be immediately available for\nthe computation of future representations, Feedback is stronger than Recurrence alone — Recurrent\nmodels can only see representations from the previous layer (as depicted in Table 2).\n4.3.2 M EMORY COMPOSITION\nWe next investigate the importance of the speciﬁc memory mechanism of the Feedback architecture\non char-PTB. The Feedback architecture uses all layers when creating the memory, motivated by\nproviding access to the entire past of all computations, but other ways of creating the memory as\npossible. For example, Recurrent architectures have a different memory structure. In multi-layer\nRNNs, each layer has recurrent connections to the same layer, but not to higher layers. This is an\nadvantage of Feedback architectures — even the highest level abstractions are immediately available.\nIn Figure 5, we examine the construction of the Feedback memory, comparing our choice of making\nall computation accessible with recurrent memory that can access all previous layers plus the same\nlayer, and top-only memory that can attend only to the topmost layer. The Feedback Transformer has\nthe best performance, closely matched by top-only memory. This indicates the importance of high\nlevel representations (see Appendix 6.4 for further analysis on this). Note that recurrence alone is\nnot enough for good performance, and thus the Feedback memory provides richer representations\nbeyond the capacity of recurrent networks.\n4.3.3 C OMPARISON TO OTHER TRANSFORMER ARCHITECTURES\nWe examine the performance of Feedback Transformer on long context language modeling bench-\nmarks. We use caching (Dai et al., 2019) and relative position embeddings. Mechanisms applied at\ninference time (Khandelwal et al., 2019; Krause et al., 2019) can further improve all models, so we\ndo not focus on these.\nWikitext-103. We evaluate on word-level language modeling onWikitext-103 (Merity et al.,\n2017). Our Feedback architecture takes 3.5 days to train, compared to the Transformer which\ntakes 1.2 days. We train a small Feedback model, about half the size of Transformer-XL, and ﬁnd\nthat it can match the performance of Transformer-XL, as shown in Table 3. This indicates the\nadditional representational capacity of Feedback memory. If we train a standard Transformer that\nis approximately the same size as our Feedback Transformer, we ﬁnd it has worse performance\n8\nModel Params Test\nBest Existing (Roy et al., 2020) — 15.8\nTrans-XL (Dai et al., 2019) 257M 18.3\nOur Transformer 140M 19.9\nFeedback Transformer 126M 18.3\nTable 3: Results onWikiText-103. We re-\nport perplexity on test.\nModel Params Test\nBest Existing (Rae et al., 2020) 277M 0.97\nTrans-XL (Dai et al., 2019) 277M 0.99\nFeedback Transformer 77M 0.96\nTable 4: Results onEnwiki8. We report bit-\nper-byte on test.\nTask Model Training Speed Inference Speed\nLanguage Modeling Transformer 296K 592\nFeedback Transformer 84.4K 2176\nTranslation Transformer 280K 3190\nFeedback Transformer 126K 5410\nReinforcement Learning Transformer 22.3K —\nFeedback Transformer 22.3K —\nTable 5: Results comparing Training and Inference Speedfor three different tasks. For language\nmodeling, we measure words-per-second on Wikitext-103 ﬁxing model size and attention span. For\ntranslation, we measure words-per-second on WMT En-De, both models with a 6 layer encoder and 2\nlayer decoder. For RL, we measure the training frame-per-second on maze navigation (with 20 CPU\ncores and 1 GPU). All inference speed is reported on 1 GPU.\n(19.9 PPL rather than 18.3). Further, mechanisms like the Routing Transformer can be added to the\nFeedback Transformer as well. We focus on starting with Transformer-XL as a baseline and showing\nwe can match the performance with a much smaller model.\nEnwiki8. Finally, we test our model on character-level language modeling inEnwiki8 (Mahoney,\n2011), containing 100M unprocessed bytes from Wikipedia. We train a relatively small 12-layer\nmodel, that is one third of the size of the Transformer-XL baseline. Since the task requires very long\ncontext, we use adaptive attention span (Sukhbaatar et al., 2019). As shown in Table 4, the Feedback\nTransformer model achieves a new SOTA performance of0.96 bit-per-byte despite its small size.\n4.3.4 T RAINING AND INFERENCE SPEED\nFinally, we compare the training and inference speed of the Feedback Transformer with standard\nTransformer architectures. Results are shown in Table 5. The Feedback Transformer has faster\ninference, because the key-value projection sharing substantially reduces the memory footprint of the\nmodel and reduces computation. Further, shallow Feedback models perform well, so the batch size\ncan be increased. In language modeling, for example, sharing key-value provides almost 3X inference\nspeed improvement. The shallow model size provides the remaining 10% of speed improvement at\ninference time. Finally, note that for certain problems (such as in RL), the data must be processed\nstrictly sequentially anyway and Feedback Transformer is not any slower.\n5 C ONCLUSION\nWe propose a novel reformulation of the Transformer that fully exploits sequential input — the\nincreased representation power and recursive computation of the Feedback Transformer allows\nshallow and small models to have much stronger performance compared to a Transformer of the same\nsize. This architecture addresses two fundamental limitations of Transformers as an autoregressive\nmodel — limited access to long memory and limited ability to update state. We demonstrate on a\nvariety of tasks the advantages of the Feedback architecture to illustrate the strong performance of\nthis straightforward modiﬁcation.\n9\nREFERENCES\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Mike Schuster, Zhifeng Chen, et al. The best of both worlds: Combining\nrecent advances in neural machine translation. arXiv preprint arXiv:1804.09849, 2018.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent\nneural networks. In International conference on machine learning, pp. 2067–2075, 2015.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT (1), 2019.\nSergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations for\nlanguage generation. arXiv preprint arXiv:1903.09722, 2019.\nAngela Fan, David Grangier, and Michael Auli. Controllable abstractive summarization. arXiv\npreprint arXiv:1711.05217, 2017.\nAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\nstructured dropout. arXiv preprint arXiv:1909.11556, 2019.\nEdouard Grave, Armand Joulin, Moustapha Cissé, and Hervé Jégou. Efﬁcient softmax approximation\nfor gpus. In ICML, 2017.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\narXiv:1410.5401, 2014.\nMichael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of\nthe Association for Computational Linguistics, 8:156–171, 2020.\nJie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling\nrecurrence for transformer. arXiv preprint arXiv:1904.03092, 2019.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 770–778, 2016.\nKarl Moritz Hermann, Tomáš Koˇciský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proc. of NIPS, 2015.\nXiaojie Jin, Yunpeng Chen, Zequn Jie, Jiashi Feng, and Shuicheng Yan. Multi-path feedback recurrent\nneural networks for scene parsing. In Thirty-First AAAI Conference on Artiﬁcial Intelligence ,\n2017.\nArmand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent\nnets. In Advances in neural information processing systems, pp. 190–198, 2015.\nŁukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228,\n2015.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,\n2019.\n10\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of trans-\nformer language models. arXiv preprint arXiv:1904.08378, 2019.\nTao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly\nparallelizable recurrence. arXiv preprint arXiv:1709.02755, 2017.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nMatt Mahoney. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text.\nhtml, 2011.\nStephen Merity. Single headed attention rnn: Stop thinking with your head. arXiv preprint\narXiv:1911.11423, 2019.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm\nlanguage models. arXiv preprint arXiv:1708.02182, 2017.\nRichard GM Morris. Spatial localization does not require the presence of local cues. Learning and\nmotivation, 12(2):239–260, 1981.\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\narXiv preprint arXiv:1806.00187, 2018.\nEmilio Parisotto, H. Song, Jack W. Rae, Razvan Pascanu, Çaglar Gülçehre, Siddhant M. Jayakumar,\nMax Jaderberg, Raphael Lopez Kaufman, A. Clark, Seb Noury, M. Botvinick, N. Heess, and Raia\nHadsell. Stabilizing transformers for reinforcement learning. ArXiv, abs/1910.06764, 2019.\nJorge Pérez, Javier Marinkovi´c, and Pablo Barceló. On the turing completeness of modern neural\nnetwork architectures. arXiv preprint arXiv:1901.03429, 2019.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Com-\npressive transformers for long-range sequence modelling. In International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=SylKikSYDH.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse\nattention with routing transformers. arXiv preprint arXiv:2003.05997, 2020.\nAbigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with\npointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In ACL (1), 2016.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\nIn NAACL-HLT (2), 2018.\nYanyao Shen, Xu Tan, Di He, Tao Qin, and Tie-Yan Liu. Dense information ﬂow for neural\nmachine translation. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers), pp. 1294–1303, 2018.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks.\nIn NIPS, 2015.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention\nspan in transformers. In ACL, 2019.\nKe Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling\nhierarchical structure. arXiv preprint arXiv:1803.03585, 2018.\n11\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nZhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. R-transformer: Recurrent neural network\nenhanced transformer. arXiv preprint arXiv:1907.05572, 2019.\n12\n6 A DDITIONAL RESULTS\n6.1 R EINFORCEMENT LEARNING\nMaze Navigation Easy. We experiment with a slightly different version of the Maze Navigation\ntask. Instead of an agent with forward, turn-left and turn-right actions, the agent has no orientation\nand there are only 4 movement actions corresponding to 4 cardinal directions. This makes navigation\neasier because the agent do not need to keep track of its orientation. Further, it is much easier to\ncompute relative locations given a history of actions. This might explain why standard Transformers\nare not far behind Feedback Transformers in performance as shown in Figure 6 (left). We also\ncompare to LSTMs, which performs much worse. See Section 7.2 for more implementation details.\nWater Maze. We modify the Morris Water Maze task (Morris, 1981) to make it more challenging.\nThe maze is deﬁned by a goal position and a mapping of cell to ID — these remain ﬁxed within an\nepisode but change between episodes. The agent receives as an observation the cell IDs of its current\nlocation and the target cell. When the agent ﬁnds the target, it receives +1 reward and is randomly\nteleported. During the same episode, if the agent reaches a previously seen cell, it needs to remember\nhow it reached the target from there to go back. Results are shown averaged over 10 trials (the reward\nis reported averaged over the last 500 episodes for each trial). As shown in Figure 6 (right), the\nFeedback Transformer converges to higher average reward.\n0.0 0.5 1.0 1.5 2.0\nTraining steps ×109\n0\n5\n10\n15\n20Reward\nTransformer\nFeedback Transformer\nLSTM\n0 1 2 3 4 5\nTraining steps ×108\n0\n1\n2\n3\n4\n5\n6Reward\nTransformer\nFeedback Transformer\nLSTM\nFigure 6: Averaged cumulative reward during training on (left) Maze Navigation Easyand (right)\nWater Mazetasks.\n6.2 IWSLT DE-EN\nWe additionally evaluate the Feedback Transformer onIWSLT De-En, a small machine translation\ndataset. We train a small Transformer model with 6 layers. For generation, we use beam size\n5 without checkpoint averaging. Model quality is evaluated using tokenized BLEU. Results are\nshown in Figure 7 (left) and show that for shallower models, the Feedback Transformer has better\nperformance than the standard Transformer.\n6.3 S UMMARIZATION ON CNN-D AILYMAIL\nWe evaluate on the CNN-Dailymail multi-sentence summarization benchmark of 280K news articles\nHermann et al. (2015), modeling the ﬁrst 400 words of the article See et al. (2017). We evaluate\nusing ROUGE Lin (2004). and use 3-gram blocking and tune length Fan et al. (2017). Figure 7\n(right) displays the performance of the Feedback Transformer as the decoder layers are reduced,\nmaking the model shallower only. For all model depths, the Feedback architecture maintains a\nconsistent improvement in ROUGE compared to the standard Transformer. Compared to sentence-\nlevel tasks such as translation, this summarization benchmark requires multi-sentence generation,\nand the increased capacity of the Feedback architecture is beneﬁcial.\n13\n4000 6000 8000 10000 12000 14000\nwords per second\n33.75\n34.00\n34.25\n34.50\n34.75\n35.00\n35.25\nTest BLEU\nTransformer\nFeedback Transformer\n1 2 3 4 5 6\nDecoder Depth\n35.5\n36.0\n36.5\n37.0\n37.5\nTest ROUGE-L\nTransformer\nFeedback Transformer\nFigure 7: Results on (left) the IWSLT De-En dataset, and (right) Summarization on\nCNN-Dailymail, test set ROUGE-L for varying decoder depths.\n1 2 3 4 5 6\nLayer\n1.18\n1.20\n1.22\n1.24\n1.26\n1.28\n1.30\nDev. (bpc)\nSingle layer\nTransformer\nFeedback\nAverage all\nFigure 8: Ablation results on char-PTB: instead of a weighted sum of all layers as Feedback\nmemory, only a single layer is used as memory for all layers. We also include a setting where the\naverage of all layers is used.\n6.4 A BLATION STUDIES ON LANGUAGE MODELS\nWe investigate which layer of a model has the best representation to be used as a Feedback memory.\nIn Feedback Transformers, a weighted sum of all layers is used as the memory, and feeds to all\nlayers. An alternative approach is to manually select one of the layers as the memory and let all\nlayers attend to it. In Figure 8, we explore this approach, using the same 6-layer char-PTB models\nas Section 4.3.2 (top-only memory there corresponds to using the last 6th layer as memory). We can\nsee that representations from higher layers work better as memory, conﬁrming our assumption of the\nimportance of higher level representations. Simply averaging all layers together works reasonably\nwell as well. Interestingly, when all layer attend to the ﬁrst layer output, it works as good as the\nstandard Transformer. The weighted sum approach matches the best performance because it can\nadopt to select any of the layers.\nHere we study how different techniques affect the model performance on WikiText-103. The\nresults shown in Table 6 indicate:\n• Pre-normalization combined with higher learning rates helps the performance, particularly\nfor the standard Transformer.\n• Increasing the context size with adaptive span further improves the performance for both\nmodels.\n• The technique of increasing the BPTT length during training for efﬁciency does not affect\nthe ﬁnal performance.\n• The gap between two model is consistent along those variations.\nNext, we examine the effect of the model depth on performance onchar-PTB and WikiText-103\nThis time, we keep the total number of parameters constant and only vary the number of layers to\n14\nModel Pre-norm + Adapt. Increase dev\nhigher LR span BPTT ppl\nTransformer no no no 22.9\nTransformer no no yes 22.9\nTransformer yes no yes 21.0\nTransformer yes yes no 20.6\nFeedback no no no 19.7\nFeedback no no yes 19.9\nFeedback yes no yes 19.6\nFeedback yes yes yes 19.0\nTable 6: Ablation on WikiText-103 of various modeing choices. Results are shown without\nﬁnetuning.\n2 4 6 8\nModel depth\n1.2\n1.4\n1.6\n1.8\n2.0\nDev. (bpc)\nTransformer\nFeedback Transformer\n2 4 6 8\nModel Depth\n20\n25\n30\n35Dev. (ppl)\nTransformer\nFeedback Transformer\nFigure 9: The performance on (left) char-PTB and (right) Wikitext-103 as a function of the\nmodel depth. The number of parameters is kept constant by increasing the width.\nisolate the effect of depth. This is achieved by proportionally increasing the head dimension and the\nReLU layer size when we decrease the number of layers. The results in Figure 9 demonstrate that\nfor the standard Transformer improves as the depth increase. In contrast, the Feedback architecture\nis much robust reduced depth, even achieving the best performance on char-PTB with only two\nlayers.\n7 A DDITIONAL IMPLEMENTATION DETAILS\n7.1 R ANDOM WALK TASK DETAILS\nWe provide additional details for the random walk toy task we explore. The agent starts at a ﬁxed\nposition of a 8 ×8 grid. Available actions are 1) move one step forward, 2) turn left and 3) turn right.\nAt every time step, the agent randomly picks on of the three actions and executes it. An action would\nbe ignored if it can’t be executed like going out of the grid. After 100 actions, the agent is reset back\nto the initial position.\nThe input to the model is a sequence of actions taken by the agent, and a special symbol if there was\na reset. The output is a sequence of location symbols corresponding to the agent’s location after each\naction. We generate 10k training episodes, totalling 1M tokens.\nWe use the same setup as our language modeling experiments, except now the model predicts separate\noutput tokens rather than a next token. We concatenate all the episodes and feed them to the model\nas a single sequence. The training is done with the negative-log-likelihood loss. See Table 9 for the\nhyperparameters used in the experiment. The attention span is set to 100, so that the models can\nattend to all the information they needs to solve the task.\n15\nvision range\nFigure 10: (left) Maze Navigationtask and (right) Water Mazetask.\nx = 1 ; print x ; x ++ ; print x ; z = 8 ; print z ; print z ; x -- ; if x > z : z -- ; z ++ ;\nprint z ; print x ; print x ; if z < x : z ++ ; x ++ ; z -- ; x -- ; if z > x : z -- ; z ++ ;\nif x > z : z ++ ; if z < 5 : y = 7 ; print x ; if x > z : z ++ ; x ++ ; y = 7 ; if x > 10 : x\n-- ; y -- ; x ++ ; z ++ ; print z ; y -- ; print x ; print x ; z ++ ; y ++ ; y ++ ; if z < 3 :\ny ++ ; if x > 4 : x ++ ; z -- ; x -- ; x -- ; print x ; y ++ ; z ++ ; y -- ; if x > z : z -- ;\nx ++ ; z -- ; print x ; z ++ ; print y ; y ++ ; y -- ; x -- ; print x ; y ++ ; print y ; y --\n; if z < x : x ++ ; if z > 4 : y -- ; z -- ; x ++ ; if y < x : y ++ ; print y ; print z ; z --\n; y -- ; x ++ ; y -- ; y ++ ; if y > 3 : z -- ; y ++ ; if z < 10 : z ++ ; z ++ ; y -- ; z ++ ;\nprint z ; x -- ; y -- ; x -- ; x ++ ; if x < 4 : y -- ; print y ; print z ; if z > x : y -- ;\nprint z ; if y < x : x -- ; print x ; print z ; if x < 4 : z -- ; if z < y : z ++ ; z -- ; x --\n; print x ; if z < x : y ++ ; print x ; print z ; y -- ; if z < 6 : x ++ ; z -- ; END\nTable 7: An example program from the algorithmic task with 3 variables.\n7.2 M AZE NAVIGATION DETAILS\nWe generate random 9 ×9 mazes using Kruskal’s algorithm. Dead ends are eliminated by randomly\nremoving one of the blocks surrounding them. We randomly place 8 target objects with different\ncolors as shown in Figure 10 (left). The agent is given a randomly selected color as a target. If the\nagent manages to reach the correct target, it gets a reward of +1 and a new target color is sampled.\nAn episode ends after 200 steps. The observation includes the 3 ×3 area around the agent and target\ncolor.\nWe train 2-layer Transformers with a hidden size 256 and 4 heads. We set the BPTT to 100 and the\nbatch size to 1024. The reward discount rate is 0.99. The attention span is 200 so the agent can keep\nan entire episode in memory. All agents were trained using A2C with Adam with a learning rate of\n0.0003 and a entropy cost of 0.0005. For the easy version of the task, we use RMSprop with a batch\nsize of 128 and a learning rate of 0.0003. The RMSProp epsilon regularization parameter is set to\n0.01 The LSTM model is a 3-layer LSTM with a hidden size of 256.\n7.3 W ATER MAZE DETAILS\nThe water maze task we designed is depicted visually in Figure 10 (right). The grid size is 15 ×15.\nTo help exploration, the agent can see if the goal is within a3 ×3 area around it. An episode ends\nafter 200 steps. We train for 500M steps (2.5M episodes). We use 2-layer Transformers with hidden\nsize of 64 and 1 head. The attention span is 200 so the agent can put an entire episode in memory.\nAll agents where trained using A2C with RMSprop with entropy cost of 0.0001, RMSProp epsilon\nregularisation parameter of 0.01, batch size of 64, and BPTT 200. Feedback Transformer and\nTransformer baseline were trained with a learning rate of 0.0003. LSTM model is a 2-layer LSTM\nwith hidden size of 64. For LSTM model we used a learning rate of 0.0004.\n7.4 A LGORITHMIC TASK DETAILS\nIn this task, each program consists of 100 simple statements that should be sequentially executed.\nThe available statement types are:\n1. Initialization. Assign an initial value to a variable like x=3. A variable can only be\ninitialized once in each program.\n16\nHyperparameter Summarization WMT En-De IWSLT De-En\nEncoder Layers 6 6 6\nDecoder Layers 6 6 6\nFFN Size 2048 4096 1024\nAttention Heads 8 16 4\nDropout 0.3 0.3 0.3\nHidden Size 512 1024 512\nLearning Rate 0.0005 0.001 0.0005\nTable 8: Hyperparamers for sequence to sequence experiments.\nHyperparameter Random Walk char-PTB Enwik8 WikiText-103 WikiText-103\nAlgorithmic small large\nLayers 4 6 12 4 8\nHidden size (d) 256 384 512 512 1024\nFF size 4d 4d 8d 8d 4d\nHead count (h) 4 4 8 8 8\nHead dim d/h d/h 2d/h 2d/h d/h\nAttention span 100 512 8192* 512 512, 2048*\nDropout rate 0.2 0.5 0.5 0.1 0.3\nEmbed. dropout - - - 0.1 0.2\nBPTT len (M) 64 128 128 256 256\nBatch size (B) 512 2048 1024 512 512\nLearning rate 0.0001 0.0015 0.0015 0.0007 0.0007\nGradient clip 0.1 1.0 0.1 0.1 0.1\nLR warm-up steps 1k 1k 8k 8k 8k\nParameters 3.2M 10.7M 77M 44M 139M\nTable 9: Hyperparamers for language modeling experiments. Here * indicates the adaptive span.\n2. Increment and decrement.Increment or decrement a variable value by 1, like x++ or\ny--.\n3. Print. Output the value of a certain variable like print(y). Only this statement requires\nmodel to make a prediction.\n4. Conditional. Execute the nested statement only if a variable has a certain value, e.g.,\nif x==4: y--. Note that conditional and print statements cannot be nested.\nA program is generated by randomly choosing a statement one after another, but with the following\nconditions: a variable must be initialized before being used, and a variable value have to between 1\nand 10. The training data contains 10k such programs concatenated with a special separator keyword.\nWe generate two version the data with 3 and 5 different variables in them. An example program is\nshown in Table 7. We used the same hyperparameters as the random walk task as show in Table 9.\n7.5 M ACHINE TRANSLATION AND SUMMARIZATION\nWe detail the hyperparameters in Table 8. Summarization experiments are done with the Transformer\nbase architecture size and WMT En-De experiments are done with the Transformer big architecture\nsize. As IWSLT De-En is a smaller dataset, we use a smaller model. For all sequence to sequence\nexperiments, only the decoder is modiﬁed to have the Feedback Transformer architecture.\n7.6 L ANGUAGE MODELING\nIn the language modeling experiments, we added several improvements on top of the original\nTransformer Vaswani et al. (2017) to better adapt to unbounded sequences:\n17\n• Hidden representation caching Dai et al. (2019):Since the input to the model is an un-\nbounded sequence and the model needs to process it in small blocks, hidden representations\nfrom previous blocks are kept in cache so that any token in the current block will the same\ncontext length regardless of its position in the block.\n• Relative position embedding Shaw et al. (2018):Relative position embeddings allow\neach token in a block to be processed in the same way regardless of its absolute position\nin the block. We found that adding shared embeddings to key vectors at every layer to be\neffective.\n• Adaptive attention span Sukhbaatar et al. (2019)Language modeling requires a model\nto have a very long attention span, which is computationally expensive. The adaptive span\nmechanism allows each attention head to learn different attention spans for efﬁciency.\n• Pre-normalization Child et al. (2019): We observed that pre-normalization makes train-\ning more stable for Transformers, which allowed us to use larger batch sizes for better\nparallelization.\nDropouts are applied to attention and ReLU activations. In WikiText-103 models, additional\ndropouts are added to the embedding layer output and the last sublayer output.\nIn Table 9, we present the hyperparameter values used for our experiments. We use the same\nhyperparameters for both Transformers and Feedback Transformers, and optimize them with Adam.\nThe ﬁnal performances are obtained by ﬁnetuning the models with a 10x smaller learning rate.\nDetails on thechar-PTB experiments We trained the models for 15k updates (or earlier if the\nvalidation loss stops decreasing), and funetined them for 1k steps. We varied the depth of the models\nwhile keeping the number of parameters constant. This is achieved by changing the FF size and the\nhead dimension inverse proportionally to the depth.\nDetails on theenwik8 experiments We used an adaptive span limited to 8192 tokens with a loss\nof 0.0000005. The training is done for 100k updates and another 10k steps is used for ﬁnetuning. The\nwarming up BPTT length is used for speeding up the training, where the BPTT length is decreased to\n64 for the ﬁrst half of the training.\nDetails for Training onWikiText-103 We employed the adaptive input Baevski & Auli (2019)\nand the adaptive softmax Grave et al. (2017) techniques for reducing the number of parameters within\nword embeddings. The models are trained for 200k steps and the ﬁnetuned for additional 10k steps.\nWhile most of the models have a ﬁxed attention span of 512, the best performance is achieved by\nextending the attention span to 2048 with adaptive span loss 0.00001.\nAfter training our models, we noticed that our tokenization method differed from others by omitting\nend-of-line (EOL) symbols. Since our dictionary already contained the EOL token, we were able\nﬁnetune our trained models on the data with EOL tokens, rather than training them from scratch. This\nchange alone brought about 1ppl improvement.\n18",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8396342992782593
    },
    {
      "name": "Computer science",
      "score": 0.7606304883956909
    },
    {
      "name": "Reinforcement learning",
      "score": 0.6201353669166565
    },
    {
      "name": "Feed forward",
      "score": 0.5782163143157959
    },
    {
      "name": "Architecture",
      "score": 0.5412859320640564
    },
    {
      "name": "Representation (politics)",
      "score": 0.5395911931991577
    },
    {
      "name": "Artificial neural network",
      "score": 0.5074300169944763
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4423934817314148
    },
    {
      "name": "Machine translation",
      "score": 0.4324137568473816
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3591907024383545
    },
    {
      "name": "Computer engineering",
      "score": 0.3568224012851715
    },
    {
      "name": "Machine learning",
      "score": 0.3410164713859558
    },
    {
      "name": "Control engineering",
      "score": 0.18792197108268738
    },
    {
      "name": "Voltage",
      "score": 0.1598721444606781
    },
    {
      "name": "Electrical engineering",
      "score": 0.10684585571289062
    },
    {
      "name": "Engineering",
      "score": 0.09678614139556885
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 30
}