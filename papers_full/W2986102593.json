{
  "title": "Long-span language modeling for speech recognition",
  "url": "https://openalex.org/W2986102593",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4213920804",
      "name": "Parthasarathy, Sarangarajan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3122319426",
      "name": "Gale William",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101788877",
      "name": "Chen, Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4285705088",
      "name": "Polovets, George",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4295356551",
      "name": "Chang, Shuangyu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2078179100",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2956159074",
    "https://openalex.org/W1768488313",
    "https://openalex.org/W2747467128",
    "https://openalex.org/W2891176389",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2026149468",
    "https://openalex.org/W2091981305",
    "https://openalex.org/W1561253126",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2251849926",
    "https://openalex.org/W2984476536"
  ],
  "abstract": "We explore neural language modeling for speech recognition where the context spans multiple sentences. Rather than encode history beyond the current sentence using a cache of words or document-level features, we focus our study on the ability of LSTM and Transformer language models to implicitly learn to carry over context across sentence boundaries. We introduce a new architecture that incorporates an attention mechanism into LSTM to combine the benefits of recurrent and attention architectures. We conduct language modeling and speech recognition experiments on the publicly available LibriSpeech corpus. We show that conventional training on a paragraph-level corpus results in significant reductions in perplexity compared to training on a sentence-level corpus. We also describe speech recognition experiments using long-span language models in second-pass re-ranking, and provide insights into the ability of such models to take advantage of context beyond the current sentence.",
  "full_text": "LONG-SPAN LANGUAGE MODELING FOR SPEECH RECOGNITION\nSarangarajan Parthasarathy William Gale Xie Chen George Polovets Shuangyu Chang\nMicrosoft, USA\nABSTRACT\nWe explore neural language modeling for speech recognition where\nthe context spans multiple sentences. Rather than encode history\nbeyond the current sentence using a cache of words or document-\nlevel features, we focus our study on the ability of LSTM and Trans-\nformer language models to implicitly learn to carry over context\nacross sentence boundaries. We introduce a new architecture that\nincorporates an attention mechanism into LSTM to combine the ben-\neﬁts of recurrent and attention architectures. We conduct language\nmodeling and speech recognition experiments on the publicly avail-\nable LibriSpeech corpus. We show that conventional training on a\nparagraph-level corpus results in signiﬁcant reductions in perplexity\ncompared to training on a sentence-level corpus. We also describe\nspeech recognition experiments using long-span language models in\nsecond-pass re-ranking, and provide insights into the ability of such\nmodels to take advantage of context beyond the current sentence.\nIndex Terms— LSTM language model, Transformer language\nmodel, long-span language model, speech recognition\n1. INTRODUCTION\nLanguage models used in automatic speech recognition (ASR) sys-\ntems are typically trained on a sentence-level corpus. Intuition sug-\ngests that context beyond the current sentence should inﬂuence next\nword prediction. Some efforts at improving the quality of language\nmodels using long-span contextual information include incorporat-\ning a short-term cache [1], semantic information at the document\nlevel [2], dialog states [3], and the use of word-triggers [4]. Such\nlong-span context dependencies are difﬁcult to model using n-gram\nlanguage models (NGLM) commonly used in ﬁrst-pass ASR de-\ncoding. Neural Network Language Models (NNLM) exploit longer\ncontext better than NGLMs and have demonstrated signiﬁcant im-\nprovement in perplexity [5], and word-error-rate (WER) when used\nin second-pass rescoring [6, 7] and more recently in ﬁrst-pass decod-\ning [8, 9].\nWhile there have been attempts to train NNLMs at the document-\nlevel [10], NNLMs used in ASR are still trained on a sentence-level\ncorpus. There are many reasons for this. Longer context may not\nbe available or be relevant for improving next-word prediction in\ncommercial ASR systems. For instance, in voice search, long dis-\ntance word history may be less relevant than non-lexical features\nsuch as the geographic location of the user [11]. It is also often\ndifﬁcult to obtain training data representing long session contexts in\nmany conversational scenarios. Scenarios where long-span models\nare useful are becoming more pervasive. Transcriptions of talks and\nmeetings, human-to-human conversation, and document creation\nby voice, are some scenarios which will likely beneﬁt greatly from\nlong-span models [12].\nIn this work, we restrict our attention to context which consists\nof word history alone, rather than contexts such as topic of conver-\nsation and other non-lexical information. We study the beneﬁts of\ntraining NNLMs at the paragraph-level, where a paragraph is a se-\nquence of consecutive sentences. Rather than summarize the recent\npast using a word-cache, topic-vectors, etc., we simply concatenate\nsentences with a sentence boundary symbol which is treated as a\nword in the vocabulary. We study two popular NNLM architectures,\nLSTM [13], and Transformer [14, 15], and introduce a new variant\nwhich augments LSTM with an attention layer. We show that all\nthree architectures are able to take advantage of the longer context\nto reduce perplexity as well as WER.\n2. MODEL ARCHITECTURES\nWe study the following popular architectures for neural language\nmodels.\n2.1. Long short-term memory language models\nWe use a standard LSTM-LM architecture [16]. The modeling unit\nis a word. The parameters of the input embedding and output linear\nlayers are tied. In order to decouple the choice of the dimensional-\nity of the LSTM layer and the embedding dimension, the output of\nthe ﬁnal LSTM layer is projected to the embedding dimension. We\nuse noise-contrastive-estimation (NCE) loss during training [17, 18],\nwhich results in approximately self-normalized models. The vocab-\nulary size of our model is 200K which makes training using cross-\nentropy loss infeasible. Inference, especially in a re-ranking setting,\ncan also be efﬁciently implemented with self-normalized models,\nsince the linear transform followed by softmax can be replaced by\na dot-product. Even though we use NCE loss during training, all\nperplexity results reported in this paper are computed using cross-\nentropy to ensure a valid probability distribution.\n2.2. LSTM with attention\nWhile LSTMs are quite adept at representing history through their\nhidden states, we hypothesize that in longer sequences, the contribu-\ntions of earlier words get attenuated. Attention mechanisms on the\nother hand, are able to assign high weight to any words in the his-\ntory if they are relevant to the current context. In this work, we add a\nmulti-headed dot product attention layer [14] over the LSTM hidden\nstates to better utilize the information in long-spanning sequences.\nat = MultAttn(h1, ..., ht) (1)\npt = Proj(Cat(at, ht)) (2)\nMultiAttn is the multi-headed attention function presented in\n[14]. It operates over all hidden state outputs at time t′ ≤ t to pro-\nduce an attention vector at. The attention vector is combined with\nthe hidden state output ht using the concatenation operator Cat and\nis passed to the network’s output linear layerProj.\narXiv:1911.04571v1  [cs.CL]  11 Nov 2019\n2.3. Transformer Language model\nIn recent years, there is an increasing research endeavor to replace\nLSTM with transformer [14] for sequence modeling. Transformer\nhas achieved state-of-the-art in a range of NLP tasks including lan-\nguage models [19, 7]. In transformer, recurrent connection is not\nneeded. Self-attention with multi-head is applied to model the long-\nterm history information. In this work, we investigate the use of\nstandard transformer architecture for language modeling [14], which\nis similar to the structure used in [20]. The relative position embed-\nding was used to model the position information [21] for better per-\nformance. It is worth noting that, when the number of transformer\nlayers is large (e.g. 10), it is crucial to use warm-up step to increase\nthe learning rate gradually with the progress of training [22] in order\nto guarantee the convergence of transformer LMs.\n3. DATASET\nWe use the publicly available LibriSpeech data-set [23] in our ex-\nperiments. Training data for language model consists of a sentence-\nlevel corpus of about 803M words. A vocabulary of 200K words is\nalso speciﬁed. We set up baseline language models on this corpus as\ndistributed, so that we can compare our results to published bench-\nmarks. However, this corpus is not suitable for our experiments with\nparagraph-level, long-span, language models.\nWe created a new paragraph-level corpus as follows. We started\nwith the raw text from the same books that were used to created\nthe standard LibriSpeech LM training corpus. We applied the text\nprocessing scripts in Kaldi [24] to normalize the text exactly as was\ndone to create the standard corpus. We then split the text into se-\nquences of approximately 2000 characters, taking care to split at the\nnearest sentence boundary. We call each such sequence, typically\ncontaining multiple consecutive sentences separated by a boundary\nsymbol <s>, a paragraph. As a sanity check of our text processing,\nwe ensured that we could recreate the sentence-level corpus by split-\nting on sentence boundaries and retaining only unique sentences.\nFurther, by splitting the paragraph-level corpus at sentence bound-\naries, we get a new sentence-level corpus which is exactly matched\nin terms of number of words with the paragraph-level corpus. This\nmakes our comparisons of paragraph and sentence models meaning-\nful. This new corpus has about 880M words. The length distribution\nof sentences and paragraphs in words is shown in Fig. 1. We simi-\nlarly created paragraph-level dev and test sets by joining consecutive\nsentences for evaluating long-span effects.\nFor word-error-rate (WER) evaluations, we need an acoustic\nmodel. Since the focus of this study is long-span language modeling,\nwe did not train an acoustic model on the LibriSpeech audio corpus.\nFor convenience, we took an off-the-shelf acoustic model trained on\n1000s of hours of audio from a variety of Microsoft ASR applica-\ntions. Therefore, the WER reported in this paper are not directly\ncomparable to WER reported in other publications on LibriSpeech.\nSince our WERs are in the same ballpark as previously published\nresults, we believe that any conclusions regarding word-error-rate-\nreductions (WERR) relative to our baseline are still meaningful and\nshould carry-over to other ASR systems.\nFinally, we follow common practice [7] and combine clean and\nother partitions of the dev and test sets for language model evalua-\ntions while keeping them separate for WER evaluations.\nFig. 1. Distribution of sentence and paragraph lengths in words in\nthe LM training data.\n4. EXPERIMENTS\n4.1. Language modeling\nThe goal of these experiments is to study the behavior of LMs\ntrained on a paragraph-level corpus instead of a sentence-level cor-\npus. Therefore, rather than sweep hyper-parameters of the models\nto get the best possible performance in each scenario, we selected a\nmodel size that provides nearly the best performance on this corpus,\nand kept it constant in all the experiments to make fair comparisons.\n4.1.1. Baseline on standard corpus\nWe trained a4-gram NGLM, and a 4×2048:512 LSTM-LM, on the\nstandard LibriSpeech corpus, where 4 is the number of layers, 2048\nis the dimensionality of the LSTM state,512 is the dimensionality of\nthe embedding and also the output dimensionality of the projection\nlayer. The transformer LM consists of 16 transformer layers, where\neach transformer layer contains 768 hidden nodes with 12 heads.\nPerplexity of the LMs on dev and test sets are shown in Table 1.\nThese perplexities are consistent with best reported results for this\nmodels size [7].\nModel dev test\nKN4 144.2 148.9\nLSTM 62.8 65.6\nTable 1. Perplexity of Kneser-Ney smoothed4-gram and LSTM LM\non the standard LibriSpeech LM training corpus.\n4.1.2. Baseline on the paragraph corpus\nWe retrained the models in Section 4.1.1 on the sentence and para-\ngraph level corpus created as described in Section 3. Both sentence-\nlevel and paragraph-level trained models are evaluated on sentence-\nlevel and paragraph-level dev and test sets. Perplexity onsent evalu-\nation sets of sentence-level NGLM and LSTM-LM are directly com-\nparable to the results in Table 1. The slight difference in the training\nset causes the perplexities on the same evaluation sets to be higher\nby about 3 absolute points for NGLM and about 1 absolute point\nfor LSTM-LM. This establishes the new baseline for the rest of our\nexperiments.\nModel sent para\ndev test dev test\nKN4 147.4 153.8\nLSTM-sent 63.5 66.6 60.6 63.0\nLSTM-para 64.4 67.6 50.3 52.1\nLSTMA-sent 62.3 65.4 79.4 83.8\nLSTMA-sent (RA) 64.4 67.0\nLSTMA-para 62.7 65.5 47.2 48.8\nTrans-sent 58.9 61.6 71.3 73.7\nTrans-para 61.6 64.1 48.6 50.6\nTable 2. Perplexity of models trained on sentence and paragraph\ncorpus and evaluated on sentence and paragraph evaluation sets.\nTrans refers to the transformer model described in Section 2.3 and\nLSTMA refers to the LSTM with attention described in Section 2.2.\nLSTMA-sent (RA) row uses LSTMA-sent but imposes restriction on\nthe attention-span during inference.\nHere are some observations from Table 2. First, consider the\nLSTM results. Sentence-level models do carry over context across\nsentence boundaries as evidenced by the lower perplexity in the para\ncolumns relative to sent columns. This is in spite of the fact that sen-\ntence models have never seen sentence boundaries in the middle of a\ntext sequence during training. Perplexity gains are about4% relative.\nThe behavior of LSTMA as well as Trans models is different from\nLSTM models. There is a signiﬁcant increase in perplexity when\nsentence models are evaluated on paragraph data. We hypothesized\nthat this is due to mismatch in the time-span over which the atten-\ntion vector is computed, between training and inference. The mean\nand standard deviation of the sentence length used to train LSTMA-\nsent is approximately 19 and 16 respectively. We recomputed the\nperplexity on the paragraph corpus using the sentence-level LSTMA\nmodel by restricting the attention-span to 35 past words. The results\nare shown in row LSTM-sent (RA). Notice that there is no signiﬁ-\ncant drop in perplexity when restricting attention-span in this way.\nThe behavior of paragraph-level models is consistent across all three\nmodel architectures. Paragraph-level models perform slightly worse\non sentence evaluation data relative to sentence-level models, proba-\nbly due to mismatch between the lengths of the evaluation and train-\ning sequences. The perplexity gains of paragraph-level models on\nparagraph evaluation data are substantial. If we compare matched\nperplexities of conventional sentence model evaluated on sentence\ndata and paragraph model on paragraph data, the relative gains are\nabout 20%, 24%, and 17%, for LSTM, LSTMA, and Trans architec-\ntures respectively.\n4.2. Speech recognition: n-best re-ranking\nAs mentioned in Section 3, we use an off-the-shelf acoustic model\nand the 4-gram sentence-level NGLM, to generate n-best hypotheses\nusing a WFST ASR decoder. We then re-rank the n-best hypotheses\nusing various NNLMs, using log-linear combination of AM, ﬁrst-\npass NGLM, and NNLM likelihoods. It is important to point out\nthat the ﬁrst-pass recognizer still operates at the sentence-level and\nthere is no state carry-over across sentence boundaries. Therefore,\nwe are inherently limited by how much context information can be\ninjected into second-pass re-ranking. The word-error-rate of the top-\nchoice hypothesis of ﬁrst-pass ASR is shown in Table 3.\nThe ﬁrst set of experiments demonstrate the beneﬁts of re-\nranking using NNLMs using traditional sentence-level models. The\nre-ranked top-choice WERas well as relative WER reduction (WERR)\neval set ﬁrst-pass WER\ndev-clean 4.63\ndev-other 10.67\ntest-clean 4.84\ntest-other 10.94\nTable 3. 1-best WERin % of the ﬁrst-pass decoder\nusing three NNLMs described in Section 2, are shown in Table 4.\nThe corresponding results using paragraph LMs are shown in Table\n5. It is clear that re-ranking using NNLMs is effective in reducing\nWER. All three models perform roughly similarly. The Transformer\nLM performs the best by a slight margin. The performance of para-\ngraph LMs is also very close to the performance of sentence LMs\nsince there is no additional context presented to the model.\neval set LSTM-sent LSTMA-sent Trans-sent\nWER WERR WER WERR WER WERR\ndev-clean 2.91 37.13 2.91 37.09 2.78 39.87\ndev-other 7.46 30.09 7.44 30.29 7.24 32.13\ntest-clean 3.16 34.81 3.24 33.12 3.01 37.83\ntest-other 7.68 29.76 7.58 30.72 7.44 32.01\nTable 4. Re-ranked 1-best WER(%) and WERR(%) using sentence-\nlevel NNLMs on sentence data\neval set LSTM-para LSTMA-para Trans-para\nWER WERR WER WERR WER WERR\ndev-clean 2.98 35.58 2.87 37.92 2.79 39.63\ndev-other 7.53 29.39 7.49 29.80 7.35 31.08\ntest-clean 3.21 33.67 3.15 34.97 3.03 37.40\ntest-other 7.67 29.90 7.53 31.10 7.43 32.08\nTable 5. Re-ranked 1-best WER(%) and WERR(%) using paragraph-\nlevel NNLMs on sentence data.\nIn order to determine the extent to which paragraph models can\ntake advantage of additional context, we did a cheating experiment\nwhere we scored the current sentence in the context of the past\nsentences within a paragraph. We used the reference transcripts\nrather than the recognized hypotheses to understand the ability of\nthe NNLMs to use past context. The results are shown in Table 6.\nImprovement in WERR over sentence models is consistent across\nall data sets. The two conditions we care about are the performance\nof the conventional re-ranker using sentence-models in sentence\ncontext (S/S), and the new paragraph models evaluated in paragraph\ncontext (P/P). First-pass ASR still decodes only in sentence context.\nThe WERR of P/P relative to S/S averaged across the data-sets is\n6.6%, 9.2%, and 4.2% for LSTM, LSTMA, and Trans models re-\nspectively. While all architectures are able to take advantage of the\nparagraph context, LSTMA architecture seems to achieve the largest\ngains. The somewhat lower gains of the Transformer model due to\njust longer LM context may be because the absolute WER of S/S\nsystem is still lower than the other two architectures and search error\nin the n-best hypotheses may be limiting the gains. We intend to\nimplement a second-pass lattice decoder to fully take advantage of\nthe perplexity gains offered by longer context shown in Table 2.\nIn practice, ﬁrst-pass ASR generates a sequence of n-best\nsentence-level hypotheses in each session. To obtain paragraph-\nlevel LM scores for the current sentence, we need to determine\neval set LSTM-para LSTMA-para Trans-para\nWER WERR WER WERR WER WERR\ndev-clean 2.71 41.45 2.59 43.99 2.64 43.00\ndev-other 7.06 33.82 6.87 35.60 6.92 35.17\ntest-clean 2.97 38.65 2.90 40.18 2.99 38.34\ntest-other 7.06 35.41 6.99 36.11 6.98 36.18\nTable 6. Re-ranked 1-best WER(%) and WERR(%) using paragraph-\nlevel NNLMs scored in reference paragraph context.\nwhich of the contexts from the previous n-best hypotheses to carry\nover. This would typically be implemented using a beam-search. We\ntried a simple strategy where we carry-over only the context of the\n1-best hypothesis from the previous sentence. The results are shown\nin Table 7. This simple strategy for context carry-over is effective in\nachieving most of the potential gains shown in the cheating results\nin Table 6.\neval set LSTM-para\nWER WERR\ndev-clean 2.80 39.43\ndev-other 7.24 32.17\ntest-clean 2.97 38.65\ntest-other 7.27 33.54\nTable 7. Re-ranked 1-best WER(%) and WERR(%) using paragraph-\nlevel NNLMs scored in 1-best hypothesisparagraph context.\nSince our interest is primarily in studying the effect of longer\ncontext in language modeling, we have ﬁxed the ASR system and\nthe re-ranker parameters to reasonable settings but not tuned them.\nWhile such tuning may lower WER, it is unlikely to change our con-\nclusions about the beneﬁts of using longer context. In order to dis-\nentangle the ASR effects further, we simulated a situation where we\neliminated search error by adding the reference transcription to the\nn-best list used for re-ranking. This involved generating AM like-\nlihoods using forced-alignment of the reference transcription with\nthe audio, and computing the ﬁrst-pass NGLM likelihood for the\nreference. Just to be certain that the behavior of the ASR decoder\nin forced-alignment mode is not subtly different than during normal\ndecoding, we also generated the AM and ﬁrst-pass LM likelihoods\nfor the n-best hypotheses using forced-alignment of each hypothe-\nsis with the audio. The results are shown in Table 8. The baseline\nWER in Table 8 is different from the one in Table 3. The main rea-\nson is out-of-vocabulary words in the reference transcription (OOV).\nFor example, if sentences with OOV are removed from scoring, the\nWER for dev-clean drops from 4.63 to 4.04. The rest of the drop to\n3.86 is explained by search error. The OOV ﬁltering study was only\nfor diagnosis purposes. The results reported in Table 8 use exactly\nthe dev and test sets used in other experiments.\nThe most interesting observation from Table 8 is that LSTM-\npara is much more effective at taking advantage of context beyond\nthe current sentence. For example, WER with paragraph-context for\nLSTM-para is 4.79 relative to 5.68 without beyond-sentence con-\ntext. There is some drop in WER even for LSTM-sent for dev-other\nbut not as signiﬁcant as for LSTM-para.\n5. RELATED WORK\nThere has been signiﬁcant work on long-span neural LMs in the\nlarger NLP community beyond speech recognition [21, 25]. Much\neval condition baseline LSTM-sent LSTM-para\nWER WER WERR WER WERR\ndev-clean (R) 3.86 2.24 41.87 2.31 40.11\ndev-clean (R+C) 2.23 42.16 2.01 47.96\ndev-other (R) 9.47 5.72 39.64 5.68 40.06\ndev-other (R+C) 5.51 41.88 4.79 49.46\nTable 8. Re-ranked 1-best WER(%) and WERR(%) using sentence-\nlevel and paragraph-level NNLMs assuming no search error. (R)\nrefers to the condition where the reference transcription is added to\nthe n-best hypotheses from the ﬁrst-pass. (R+C) refers to the condi-\ntion where the reference transcription is added to the n-best and the\nNNLM scoring uses context from previous sentences in the session.\nof the recent NNLM research treats the entire corpus as a single long\nstring of text and segment it into sequences without regard to sen-\ntence boundary. When presented with a evaluation corpus consisting\nof a collection of sentences or paragraphs, the order of evaluation\naffects the likelihood of each text string and hence the corpus per-\nplexity calculation. In speech recognition, we have typically insisted\non deterministic behavior where the LM likelihood of a sentence is\nnot affected by previous context since the context is reset at the be-\nginning of a sentence. In this work, we still reset the state of our\nNNLMs, except at the paragraph-level instead of a sentence-level.\nAs we have demonstrated, such a model is effective at traditional\nsentence-level scoring when no context information is available, and\nyet take advantage of the longer context when it is available.\nLong-span language modeling ideas explored in the context of\nspeech recognition are more relevant to our work. There have been\nprior attempts at incorporating context beyond the current sentence.\nA conversational LM that conditioned next-word predictions of a\ncurrent sentence on words uttered by a different speaker in the con-\nversation was introduced in [26]. Cache-based models, where a\nmodel trained on the recent past is interpolated with the base LM,\nhave a long history [1]. Exponential and trigger-based language\nmodels also allow mechanisms for injecting long-distance informa-\ntion [4]. A mechanism for injecting a context-vector such as topics\nin an NNLM was introduced in [27]. All of these efforts attempt to\nexplicitly inject long-span information that may be relevant to next-\nword prediction. These approaches can complement and enhance\nour models and will be the subject of future work. The work that\nis most closely related to ours is the work on session-level language\nmodeling for conversational speech [12]. They train a traditional\nsentence-level LSTM-LM and score the current hypothesis in the\ncontext of words in the previous utterance. Our study is focused on\nhow well NNLMs learn long-span context implicitly when simply\npresented with past text during training and evaluation. This effort\nwas motivated by promising results we achieved in an earlier unpub-\nlished work [28] on an independent text corpus.\n6. CONCLUSION\nLanguage models for speech recognition are traditionally trained on\nsentence-level data. We have demonstrated that LSTM, LSTMA,\nand Transformer language models, trained and evaluated on paragraph-\nlevel data, achieves perplexity reduction of about 20%. Such mod-\nels can be effectively trained without any additional architectural\nchanges to the models, or signiﬁcant changes to the training method-\nology. We also demonstrated gains in WERR of 4% − 9%, that can\nbe attributed only to the use of context beyond the current sentence,\nwhen evaluating the paragraph-level models.\nReferences\n[1] R. Kuhn and R. De Mori. “A Cache-Based Natural Lan-\nguage Model for Speech Recognition”. In: IEEE Trans.\nPattern Anal. Mach. Intell.12.6 (June 1990), pp. 570–583.\n[2] Jerome Bellegarda. “Exploiting Latent Semantic Information\nin Statistical Language Modeling”. In: Proceedings of the\nIEEE 88 (Sept. 2000), pp. 1279–1296.\n[3] Wei Xu and Alexander Rudnicky. “Language modeling for\ndialog system.” In: Jan. 2000, pp. 118–121.\n[4] Raymond Lau, Ronald Rosenfeld, and Salim Roukos. “Trigger-\nbased Language Models: A Maximum Entropy Approach”.\nIn: Proceedings of ICASSP - Volume II. Minneapolis, Min-\nnesota, USA, 1993, pp. 45–48.\n[5] Tomas Mikolov et al. “Recurrent neural network based lan-\nguage model”. In: vol. 2. Jan. 2010, pp. 1045–1048.\n[6] X. Liu et al. “Efﬁcient lattice rescoring using recurrent neu-\nral network language models”. In: 2014 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP). May 2014, pp. 4908–4912.\n[7] Kazuki Irie et al. “Language Modeling with Deep Transform-\ners”. In: Proc. Interspeech 2019. 2019, pp. 3905–3909.\n[8] Zhiheng Huang, Geoffrey Zweig, and Benoit Dumoulin.\n“Cache Based Recurrent Neural Network Language Model\nInference for First Pass Speech Recognition”. In: ICASSP.\nIEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), Jan. 2014.\n[9] Eugen Beck et al. LSTM Language Models for LVCSR in\nFirst-Pass Decoding and Lattice-Rescoring. 2019. arXiv:\n1907.01030 [eess.AS].\n[10] Rui Lin et al. “Hierarchical Recurrent Neural Network for\nDocument Modeling”. In: Proceedings of the 2015 Confer-\nence on Empirical Methods in Natural Language Processing.\nLisbon, Portugal: Association for Computational Linguistics,\nSept. 2015, pp. 899–907.\n[11] Fadi Biadsy, Mohammadreza Ghodsi, and Diamantino Ca-\nseiro. “Effectively Building Tera Scale MaxEnt Language\nModels Incorporating Non-Linguistic Signals”. In: INTER-\nSPEECH. 2017.\n[12] Wayne Xiong et al. “Session-level Language Modeling for\nConversational Speech”. In:Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Processing.\nBrussels, Belgium: Association for Computational Linguis-\ntics, Oct. 2018, pp. 2764–2768.\n[13] Martin Sundermeyer, Ralf Schlter, and Hermann Ney.\n“LSTM Neural Networks for Language Modeling”. In: IN-\nTERSPEECH. 2012.\n[14] Ashish Vaswani et al. “Attention is all you need”. In: Ad-\nvances in neural information processing systems . 2017,\npp. 5998–6008.\n[15] Kazuki Irie et al. “Language modeling with deep Trans-\nformers”. In: Proceedings of INTERSPEECH. ISCA. 2019,\npp. 5929–5933.\n[16] Rafal Jozefowicz et al. Exploring the limits of language mod-\neling. 2016. URL : https://arxiv.org/pdf/1602.\n02410.pdf.\n[17] Michael U. Gutmann and Aapo Hyvrinen. “Noise-contrastive\nEstimation of Unnormalized Statistical Models, with Appli-\ncations to Natural Image Statistics”. In: J. Mach. Learn. Res.\n13 (Feb. 2012), pp. 307–361.ISSN : 1532-4435. URL : http:\n//dl.acm.org/citation.cfm?id=2188385.\n2188396.\n[18] Chris Dyer. Notes on Noise Contrastive Estimation and Neg-\native Sampling. 2014. arXiv: 1410.8251 [cs.LG].\n[19] Jacob Devlin et al. “Bert: Pre-training of deep bidirectional\ntransformers for language understanding”. In: arXiv preprint\narXiv:1810.04805 (2018).\n[20] Alec Radford et al. “Improving language understanding by\ngenerative pre-training”. In: (2018). URL : https://s3-\nus- west- 2.amazonaws.com/openai- assets/\nresearch - covers / language - unsupervised /\nlanguage_understanding_paper.pdf.\n[21] Zihang Dai et al. “Transformer-XL: Attentive Language\nModels beyond a Fixed-Length Context”. In: Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics. Florence, Italy: Association for\nComputational Linguistics, July 2019, pp. 2978–2988. DOI :\n10 . 18653 / v1 / P19 - 1285. URL : https : / / www .\naclweb.org/anthology/P19-1285.\n[22] Martin Popel and Ondej Bojar. “Training tips for the trans-\nformer model”. In:The Prague Bulletin of Mathematical Lin-\nguistics 110.1 (2018), pp. 43–70.\n[23] V . Panayotov et al. “Librispeech: An ASR corpus based on\npublic domain audio books”. In: 2015 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP). Apr. 2015, pp. 5206–5210.\n[24] URL : https://github.com/kaldi-asr/kaldi/\ntree/master/egs/librispeech.\n[25] Urvashi Khandelwal et al. “Sharp Nearby, Fuzzy Far Away:\nHow Neural Language Models Use Context”. In: Proceed-\nings of the 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers). Melbourne,\nAustralia: Association for Computational Linguistics, July\n2018, pp. 284–294.\n[26] Gang Ji and Jeff Bilmes. “Multi-speaker Language Model-\ning”. In: Proceedings of HLT-NAACL 2004: Short Papers.\nHLT-NAACL-Short ’04. Boston, Massachusetts: Association\nfor Computational Linguistics, 2004, pp. 133–136. ISBN : 1-\n932432-24-8. URL : http://dl.acm.org/citation.\ncfm?id=1613984.1614018.\n[27] Tomas Mikolov and Geoffrey Zweig. “Context dependent re-\ncurrent neural network language model”. In:2012 IEEE Spo-\nken Language Technology Workshop (SLT)(2012), pp. 234–\n239.\n[28] Jurik Juraska, Sarangarajan Parthasarathy, and William\nGale. Language modeling: Attention mechanisms for ex-\ntending context-awareness of LSTM. 2018. URL : https :\n/ / www . microsoft . com / en - us / research /\npublication/language-modeling-attention-\nmechanisms-for-extending-context-awareness-\nof-lstm/.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8517799973487854
    },
    {
      "name": "Perplexity",
      "score": 0.7592475414276123
    },
    {
      "name": "Language model",
      "score": 0.750526487827301
    },
    {
      "name": "Sentence",
      "score": 0.6408654451370239
    },
    {
      "name": "Natural language processing",
      "score": 0.6130741238594055
    },
    {
      "name": "Speech recognition",
      "score": 0.58669513463974
    },
    {
      "name": "Transformer",
      "score": 0.562609076499939
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5469723343849182
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4403077960014343
    },
    {
      "name": "Context model",
      "score": 0.43948185443878174
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}