{
    "title": "GOProFormer: A Multi-Modal Transformer Method for Gene Ontology Protein Function Prediction",
    "url": "https://openalex.org/W4309288197",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2192401251",
            "name": "Anowarul Kabir",
            "affiliations": [
                "George Mason University"
            ]
        },
        {
            "id": "https://openalex.org/A2289458112",
            "name": "Amarda Shehu",
            "affiliations": [
                "George Mason University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2995514860",
        "https://openalex.org/W3166142427",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W4242765109",
        "https://openalex.org/W4210494137",
        "https://openalex.org/W3035486570",
        "https://openalex.org/W2889454651",
        "https://openalex.org/W3112600195",
        "https://openalex.org/W4213112325",
        "https://openalex.org/W3194145003",
        "https://openalex.org/W2615066396",
        "https://openalex.org/W4236358448",
        "https://openalex.org/W3005335133",
        "https://openalex.org/W3003053789",
        "https://openalex.org/W4220685469",
        "https://openalex.org/W6601894380",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W3118936575",
        "https://openalex.org/W2935275081",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W2103017472",
        "https://openalex.org/W2154139219",
        "https://openalex.org/W2117486996",
        "https://openalex.org/W2227395312",
        "https://openalex.org/W46679369",
        "https://openalex.org/W2966590054",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3112376646"
    ],
    "abstract": "Protein Language Models (PLMs) are shown to be capable of learning sequence representations useful for various prediction tasks, from subcellular localization, evolutionary relationships, family membership, and more. They have yet to be demonstrated useful for protein function prediction. In particular, the problem of automatic annotation of proteins under the Gene Ontology (GO) framework remains open. This paper makes two key contributions. It debuts a novel method that leverages the transformer architecture in two ways. A sequence transformer encodes protein sequences in a task-agnostic feature space. A graph transformer learns a representation of GO terms while respecting their hierarchical relationships. The learned sequence and GO terms representations are combined and utilized for multi-label classification, with the labels corresponding to GO terms. The method is shown superior over recent representative GO prediction methods. The second major contribution in this paper is a deep investigation of different ways of constructing training and testing datasets. The paper shows that existing approaches under- or over-estimate the generalization power of a model. A novel approach is proposed to address these issues, resulting in a new benchmark dataset to rigorously evaluate and compare methods and advance the state-of-the-art.",
    "full_text": null
}