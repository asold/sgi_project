{
  "title": "Automatic Model Selection with Large Language Models for Reasoning",
  "url": "https://openalex.org/W4389520003",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Zhao, James Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347529925",
      "name": "Xie, Yuxi",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2747118769",
      "name": "Kawaguchi, Kenji",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2746867007",
      "name": "He, Junxian",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Xie, Michael Qizhe",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3138392969",
    "https://openalex.org/W4367859573",
    "https://openalex.org/W4385890378",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W1988790447",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4389524372",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4310290453",
    "https://openalex.org/W4362656036",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4283828387",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4214926090",
    "https://openalex.org/W2768348081",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4366565380",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4380136478",
    "https://openalex.org/W4385570291"
  ],
  "abstract": "Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8% and 93.7%. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 758–783\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAutomatic Model Selection with Large Language Models for Reasoning\nJames Xu Zhao1, Yuxi Xie1, Kenji Kawaguchi1, Junxian He2, Michael Qizhe Xie1\n1 National University of Singapore 2 The Hong Kong University of Science and Technology\nxu.zhao@u.nus.edu\nAbstract\nChain-of-Thought (CoT) and Program-Aided\nLanguage Models (PAL) represent two distinct\nreasoning methods, each with its own strengths.\nCoT employs natural language, offering flex-\nibility and interpretability, while PAL utilizes\nprogramming language, yielding more struc-\ntured and rigorous logic. We introduce a model\nselection method to combine the best of both\nworlds by employing a large language model\n(LLM) to dynamically select between them.\nOur theoretical analysis underscores the fea-\nsibility of this method, which is further cor-\nroborated by empirical results. Our proposed\nmethod demonstrates significant performance\nimprovements across eight reasoning datasets\nwith Codex, ChatGPT, and GPT-4. Addition-\nally, our method is complementary to self-\nconsistency; when integrated, it can further\nenhance performance while significantly reduc-\ning computation costs. Moreover, we achieve\nnew state-of-the-art results on GSM8K and\nSV AMP, with respective accuracies of 96.8%\nand 93.7%.1\n1 Introduction\nLarge language models (LLMs) have made impres-\nsive progresses in numerous fields (Devlin et al.,\n2019; Brown et al., 2020; OpenAI, 2023; Chowd-\nhery et al., 2022; Bubeck et al., 2023; Wei et al.,\n2022a) and are often powerful enough to solve\nproblems through a single unified method. While\nconvenient, this approach tends to ignore the dis-\ntinct structures and variations among the problems,\nwhich would benefit from using different methods.\nOn the other hand, in human society, individuals\nwith different skill sets excel in various roles, lead-\ning to a thriving world as a whole.\nIn the case of reasoning, Chain-of-Thought\n(CoT) (Wei et al., 2022b) and Program-Aided Lan-\nguage Models (PAL) (Gao et al., 2022; Chen et al.,\n1Our code, data and prompts are available at https://\ngithub.com/XuZhao0/Model-Selection-Reasoning\n2022) have emerged as two effective methods that\noffer different strengths and weaknesses. Essen-\ntially, CoT decomposes a reasoning problem into a\nseries of intermediate steps using natural language,\nmaking it more general, flexible, and comprehen-\nsible. On the other hand, PAL offers solutions via\nPython functions, with its step-by-step program-\nming code ensuring a more rigorous and structured\nmethodology. The external Python interpreter fur-\nther guarantees the computation precision. Intu-\nitively, combining the strengths of both models and\nselecting between them based on their solutions\nto a specific problem would lead to enhanced per-\nformance. However, without access to the ground\ntruth, choosing a better method itself becomes a\nmachine learning problem.\nIn order to select among multiple solutions, pre-\nvious studies have suggested training a ranker (Ue-\nsato et al., 2022). While training a dedicated\nmodel generally results in improved accuracy, it\ncan also be somewhat cumbersome and entail sig-\nnificant costs. Conversely, large language models\n(LLMs) have demonstrated good calibration and\nhave been used to assess the accuracy of their own\noutputs (Guo et al., 2017; Shinn et al., 2023; Xie\net al., 2023). In light of this, we propose leveraging\nthe in-context learning capabilities of LLMs for\nmodel selection. We direct LLMs to choose be-\ntween two distinct reasoning models and elucidate\ntheir selection rationale.\nWe present a detailed theoretical analysis to val-\nidate our approach. The analysis highlights two\nprimary factors impacting the effectiveness of our\nmethod: (1) the significance of the difference be-\ntween the two models, i.e. the distinct distributions\nof their respective probabilities of correctly solving\nproblems, and (2) the probability of selecting the\ncorrect model. Particularly, a higher overall perfor-\nmance can be attained when there is a substantial\ndifference between the models being considered,\nand when there is a high probability of the correct\n758\nFigure 1: We propose to perform model selection to combine two distinct methods, CoT and PAL . The\nfigure illustrates an example where PAL makes mistakes about crucial information and therefore fails to answer\nthe question correctly. In contrast, CoT manages to correctly answer the same question. Our selection model\nsuccessfully chooses the correct solution and provides a brief explanation to support its choice.\nmodel being selected. Furthermore, our analysis\naffirms that even without an exceptional model se-\nlector, we can still achieve improvement in certain\ncases. This reinforces our decision to simply em-\nploy an LLM for model selection.\nWe evaluate our method across eight reason-\ning tasks, with CoT and PAL serving as the\nbaseline methods. Our method consistently at-\ntains performance improvement, when employing\nCodex (Chen et al., 2021), ChatGPT, and GPT-\n4 as backbone LLMs. Its broad applicability is\nfurther underscored by performance gains with\nopen-source Llama 2 models (Touvron et al., 2023).\nIn the context of multiple sample aggregations,\nour approach significantly reduces the computa-\ntion costs while achieving notable performance en-\nhancements. Moreover, our approach attains new\nstate-of-the-art accuracies of 96.8% and 93.7% on\nGSM8K (Cobbe et al., 2021) and SV AMP (Patel\net al., 2021), respectively.\n2 Automatic Model Selection with Large\nLanguage Models\nIn this study, we examine reasoning tasks using two\nbaseline models: CoT and PAL. To tackle complex\nreasoning tasks, CoT leverages an LLM to gen-\nerate several intermediate reasoning steps before\narriving at a final answer. Due to its reliance on\nnatural language, the reasoning steps are clearly ex-\nplained. Furthermore, natural language supports a\nbroad range of reasoning that may involve common\nsense and confidence in the reasoning steps. But it\nmight struggle with intricate logical relationships.\nIn contrast, PAL takes a structured and accurate ap-\nproach, breaking down reasoning problems using\nPython code. However, the deterministic program\nformat of PAL constrains its generalizability and\nflexibility.\n2.1 Case Study on the Differences between\nCoT and PAL\nFigure 2: Comparative analysis of error cases between\nCoT and PAL on GSM8K.\nTo delve deeper into the distinct strengths of CoT\nand PAL, we analyze 100 instances from GSM8K\nwhere CoT and PAL yield different results: 50\ncases where CoT is correct and PAL is wrong, and\n759\nanother 50 where the opposite is true. All the CoT\nand PAL solutions are generated using ChatGPT.\nThen we divide the error reasons into 5 categories.\nExamples of each category can be found in Ap-\npendix E.\n• Computation Precision: The ability to per-\nform accurate numerical computations.\n• Logical Decomposition: Breaking down com-\nplex problems into manageable parts and han-\ndling logical relationships well.\n• Problem-Solving Flexibility: The adaptability\nin addressing problems using different reason-\ning paradigms, such as forward and backward\nreasoning.\n• Semantic Understanding: Grasping and inter-\npreting the problem accurately without over-\nlooking crucial information.\n• Others: This encompasses miscellaneous er-\nror reasons such as redundant calculations and\nambiguous questions.\nFigure 2 depicts the error distribution across\nthese categories. It reveals notable differences be-\ntween CoT and PAL. PAL, with its external Python\ninterpreter, ensures computation accuracy. It also\nexcels in logical decomposition, effectively break-\ning down problems and employing variables to map\nlogical relationships. On the other hand, CoT is\nmore versatile and flexible, allowing for both for-\nward and backward logical reasoning, while PAL is\nless adept at backward reasoning scenarios. CoT’s\nstrength also lies in its superior grasp of natural\nlanguage, aiding in semantic understanding. This\nanalysis highlights the distinct proficiencies of both\nmodels in reasoning, reinforcing our hypothesis.\n2.2 Method\nGiven the distinct advantages of CoT and PAL,\nit would be beneficial to combine the strengths\nof both. We propose a systematic approach that\ncombines these models through model selection.\nSpecifically, it contains two stages: solution gener-\nation and model selection.\nFor a given reasoning question Q, an LLM is\nprompted to generate reasoning chains for CoT,\nsymbolized as CCoT along with its answer ACoT.\nSimultaneously, the LLM is prompted to derive\nreasoning chains for PAL, denoted as CPAL. Em-\nploying a Python interpreter, we then get its\nanswer APAL. When ACoT and APAL yield\ndifferent results, we move to the model selec-\ntion phase. The LLM is prompted again, pro-\nvided with (Q,CCoT,ACoT,CPAL), and tasked\nto select the better method, alongside generat-\ning a brief explanation. Specifically for Llama\n2 selection prompts, the LLM is prompted with\n(Q,CCoT,ACoT,CPAL,APAL) pairs. This selec-\ntion result, together with the explanation, is rep-\nresented as S. Ultimately, based on S, the final\nanswer is derived as Afinal ∈{ACoT,APAL}.\nDuring the selection phase, LLMs leverage their\nin-context learning abilities for model selection,\ni.e., we present the LLM with a few in-context ex-\nemplar pairs (Q,CCoT,ACoT,CPAL,S). An ex-\nample of the proposed method is illustrated in Fig-\nure 1, excluding the few-shot examples to save\nspace. Specifically, we provide the LLM with two\ndistinct reasoning solutions in a multiple-choice\nquestion format. We expect the LLM to identify\nerrors or gaps within the chains and determine the\ncorrect method. Note that there are very few in-\nstances where the LLM fails to make a choice; in\nthese cases, we randomly select a method. The\neffect and qualitative analysis of the generated ex-\nplanation can be found in Appendix C.\nOwing to the in-context learning capabilities of\nLLMs, we find that they exhibit reasonable accu-\nracy in selecting the appropriate method. Further-\nmore, as our subsequent theoretical analysis in Sec-\ntion 2.3 reveals, a highly accurate method selection\nis not actually necessary for the algorithm to per-\nform well. Our empirical findings corroborate this,\nshowing that even if there are situations where the\nmodel selection is poor, the overall improvement\nremains substantial.\n2.3 Theoretical Analysis\nIn this section, we conduct a theoretical analysis\nto determine under which condition the proposed\nmethod could work (and fail).\nQuantifying error rates Let us denote the error\nrates of the two base methods, m1 and m2, by err1\nand err2, respectively. Without loss of general-\nity, let m1 be a better base method in the overall\nperformance: i.e., err1 ≤err2. For a given ques-\ntion x, we define ρx as the probability of choos-\ning a more accurate method, either m1 or m2, for\nthe given xusing the proposed approach. Define\nR(x) = p(correct |x,m2) −p(correct |x,m1)\nwhere p(correct |x,mi) represents the probabil-\n760\nity of outputting a correct prediction given input x\nwith method mi. Then we can quantify the final\nerror rate errof the proposed method as follows:\nProposition 1. For any methods m1 and m2 with\nany combining probability function ρx,\nerr = err1 −Ex[|R(x)|(ρx −1 {R(x) <0})]\nWe refer readers to Appendix B for the full proof.\nProposition 1 decomposes the possible improve-\nment (or deterioration) over base methods in terms\nof R(x) and ρx. It quantitatively shows when and\nhow we can expect improvement (or deterioration)\nbased on these two factors. For example, to im-\nprove over the best base method m1, Proposition 1\nsuggests us to choose another base methodm2 such\nthat |R(x)|is not too small and ρx is high when\n|R(x)|is large. In other words, it discourages us\nfrom choosing too similar methods as base meth-\nods, because for similar methods, |R(x)|tends to\nbe small and it is challenging to increase ρx even\nwhen |R(x)|is large due to the similarity. This\nprovides a theoretical motivation for us to use CoT\nand PAL, instead of combining CoT with another\nCoT.\nOn the accuracy of selection Define ρto be the\noverall probability of selecting a better method:\nρ= Ex[ρx]. Theorem 1 shows that ρcan be much\nworse than that of a random guess to achieve the\nimprovement over the base methods m1 and m2;\ni.e., err < err1 and err1 ≤err2 can happen with\nρ< 0.5:\nTheorem 1. For any ϵ >0, there exist data dis-\ntribution over x, two base methods (m1,m2), and\ncombining probability (ρx) such that err < err1,\nerr1 ≤err2, and ρ<ϵ .\nWe provide a stronger version of Theorem 1 in\nAppendix A (that implies Theorem 1) and its proof\nin Appendix B.\nTheorem 1 supports our proposition that, de-\nspite not training a new model for the selection\nprocess and with the in-context learning limited\nto a few-shot prompt, it is possible to achieve im-\nprovement, even if we do not achieveρx >0.5 in\nsome instances. Further elaborated in Theorem 2,\nour analysis indicates that even with a weak selec-\ntor, improvements are attainable, especially when\nthe base models exhibit comparable overall per-\nformance. This theoretical analysis offers deeper\ninsights into the conditions and strategies for effec-\ntive performance improvement with the proposed\nmethodology.\n3 Experiments\n3.1 Setup\nDatasets and backbones We conduct experi-\nments on eight datasets that span a range of\narithmetic and symbolic reasoning tasks. 7 of\nthese datasets, including GSM8K (Cobbe et al.,\n2021), SV AMP (Patel et al., 2021), ASDIV (Miao\net al., 2020), SingleOP, SingleEQ, AddSub and\nMultiArith (Koncel-Kedziorski et al., 2016), are\nabout arithmetic reasoning, while Date Under-\nstanding (Srivastava et al., 2022) focuses on\nsymbolic reasoning. To comprehensively eval-\nuate the effectiveness of our approach, we em-\nploy three proprietary LLMs as backbone sys-\ntems: Codex ( code-davinci-002), ChatGPT\n(gpt-3.5-turbo-0301) and GPT-4 (gpt-4-0314).\nWe also evaluate our method with open-source\nLlama 2 models on GSM8K. 2 Unless stated oth-\nerwise, we always utilize the same LLM for both\nbase models as well as for the model selector.\nPrompt design To effectively exploit the in-\ncontext learning abilities of the LLMs, we create\na set of few-shot examples by manually creating\nan error in one model’s reasoning chain. Few-shot\nexamples in our prompts are based on those in pre-\nvious works. For each backbone LLM, we use\na fixed prompt for all the arithmetic benchmarks\nwithout specific customizations. Examples of the\nprompts can be found in Appendix D.\nHyperparameters For the results derived using\ngreedy decoding, we use a temperature of 0 during\nboth solution generation and model selection. For\nexperiments with self-consistency, we follow the\nsettings in prior works by setting the temperature\nat 0.5 for CoT and 0.8 for PAL, during solution\ngeneration. During model selection, the tempera-\nture is set to 0 for a deterministic result. We refrain\nfrom further hyperparameter tuning to emphasize\nthe simplicity and efficacy of our method.\n3.2 Main Results\nThe results of our experiments with greedy de-\ncoding are shown in Table 1. First, we find that\nour proposed method effectively and robustly en-\nhances performance in most settings across various\ndatasets of different difficulties and with different\nbackbone LLMs, simply by combining two base\nmodels. For example, with GPT-4, we achieve\n2Codex results are obtained in February and March, Chat-\nGPT in April and May, GPT-4 in May and June 9-13, Llama 2\nin September 2023.\n761\nBackbone Method Arithmetic Symbolic\nGSM8K SV AMP ASDIV SingleOP SingleEQ AddSub MultiArith Date\nCodex\nCoT 64.4 77.6 80.2 92.7 93.5 88.4 95.7 64.5\nPAL 71.5 79.6 79.1 95.4 96.5 91.9 99.7 77.5\nOurs 74.7 82.2 81.6 96.3 96.9 91.6 99.7 79.4\nChatGPT\nCoT 80.8 83.0 89.3 94.8 97.4 90.4 98.7 69.1\nPAL 79.2 80.3 83.0 90.7 97.6 89.4 96.3 68.3\nOurs 82.6 84.3 89.4 94.8 97.8 90.6 98.7 70.2\nGPT-4\nCoT 94.6 91.9 92.7 97.2 97.2 93.9 98.0 90.0\nPAL 94.0 92.2 90.2 95.2 98.8 94.9 98.5 88.1\nOurs 95.6 93.7 93.5 97.3 98.6 95.7 99.0 90.5\nTable 1: Results comparison (Accuracy %) on 7 arithmetic datasets and 1 symbolic dataset with greedy decoding.\nWe evaluate our methods on Codex, ChatGPT, and GPT-4. The best results are highlighted inbold.\nBackbone Method SC@5 SC@15\nChatGPT\nCoT 85.4 87.4\nPAL 80.9 82.4\nOurs 88.2 (+2.8) 89.2 (+1.8)\nGPT-4\nCoT 95.6 95.8\nPAL 94.7 95.5\nOurs 96.5 (+0.9) 96.8 (+1.0)\nTable 2: Results comparison (Accuracy %) on GSM8K\nwith the integration of the Self-Consistency (SC).SC@5\nand SC@15 represents 5 and 15 sampled paths respec-\ntively. The previous state-of-the-art on GSM8K is\n95.5%, achieved by Zheng et al. (2023a).\nan accuracy of 95.6% on GSM8K and 93.7% on\nSV AMP without self-consistency.\nSecond, our results show a considerable improve-\nment even when one of the base models performs\nmuch worse than the other. For instance, we ob-\nserve a significant 3.2% improvement over PAL’s\n71.5% accuracy on GSM8K, even though CoT has\na lower accuracy of 64.4%.\nThird, our model’s general applicability is fur-\nther underscored by its 1.9% improvement on the\nsymbolic date understanding task when utilizing\nCodex as the backbone. In fact, even if the ac-\ncuracy difference between two base models is as\nlarge as 13% on this task, our proposed method\nstill improves the accuracy from 77.5% to 79.4%.\nAdditionally, our method also achieves respectable\nperformance gains with both ChatGPT and GPT-4\non this task. It serves as evidence of our method’s\napplicability to a spectrum of reasoning tasks, ex-\ntending beyond mathematical reasoning.\nExperiments with self-consistency We aim\nto investigate the relationship between self-\nconsistency (Wang et al., 2022b) and model se-\n3https://openai.com/pricing Oct. 2023\nMethod Acc. Cost # Tokens # Generated\nCoT@15 87.4 33.69 21.56M 2.72M\nPAL@15 82.4 41.64 27.13M 1.90M\nOurs@5 88.2 29.29 18.99M 1.61M\nCoT@40 88.3 89.84 57.32M 7.08M\nPAL@40 83.5 111.04 72.35M 5.07M\nOurs@15 89.2 87.87 56.97M 4.83M\nCoT@80 88.2 179.68 115.04M 14.48M\nPAL@80 83.3 222.08 144.64M 10.16M\nOurs@20 89.5 117.16 75.96M 6.44M\nTable 3: Computation cost comparison with ChatGPT\non GSM8K (1319 data samples). CoT@15 represents\nCoT with self-consistency (SC) with 15 sampled paths.\nCost in USD is calculated based on OpenAI Pricing 3.\n# Tokens denotes the total number of tokens, including\ninputs and generation tokens. # Generated denotes the\nnumber of generated tokens.\nlection with multiple samples, and whether they\ncomplement each other. The results of CoT and\nPAL with self-consistency are obtained by sam-\npling multiple times with the temperature at 0.5\nand 0.8 respectively. For our method with self-\nconsistency, we execute our algorithm repeatedly,\nand aggregate results across all samples through\nmajority voting, arriving at the final answer. As\ndemonstrated in Table 2, we achieve substantial\nimprovements over the self-consistency results of\nboth CoT and PAL.\nEmploying ChatGPT, we attain a pronounced\nperformance improvement with only 5 samples,\nreflecting a 2.8% leap in accuracy. With GPT-4,\neven though both base models already score around\n95%, integrating them with our method leads to a\n96.5% accuracy on GSM8K. Furthermore, we es-\ntablish the new state-of-the-art results on GSM8K\nat 96.8% using 15 samples.\n762\nBaseline LLM Selector LLM AccCoT AccPAL Ours Improvement\nLlama 2 7B Llama 2 7B 15.0 13.7 16.2 +1.2\nLlama 2 7B Llama 2 13B 15.0 13.7 16.8 +1.8\nLlama 2 13B Llama 2 13B 29.0 33.3 35.3 +2.0\nTable 4: Results with Llama 2 7B/13B on a subset of GSM8K. The reference CoT accuracies (Touvron et al., 2023)\nwith Llama 2 7B and 13B are 14.6% and 28.7% respectively. Baseline LLM refers to the backbone LLM used for\nboth CoT and PAL solution generation. Selector LLM refers to the backbone LLM used for model selection.\n3.3 Computation Cost Comparison\nWe extend our analysis to compare the computa-\ntion costs of model selection with multiple samples\nversus using self-consistency alone. As depicted\nin Table 3, our method not only showcases notable\nperformance improvement but also significantly re-\nduces computation costs, underscoring its efficacy\nand efficiency.\nOur method consistently outperforms both CoT\nand PAL with SC in terms of accuracy while de-\nmanding less token computation and cost. For in-\nstance, using only 5 paths, our method achieves\ncomparable performance to CoT@80 while only\ncosts 29 USD, which is merely 16% of expense\nassociated with CoT@80. Moreover, when aggre-\ngating 15 samples, our method surpasses the per-\nformance of CoT@80 at only half the cost.\nFurthermore, our method also elevates the per-\nformance ceilings. While the result of CoT with\nSC plateaus around 88.2% and that of PAL with SC\nsaturates around 83.3%, our method breaks through\nthese ceilings, achieving an impressive 89.5% accu-\nracy using just 20 samples. This result underscores\nthe power and potential of integrating different rea-\nsoning models.\n3.4 Experiments with Llama 2 Models\nTo explore the broad applicability of our method,\nwe conduct experiments with open-source Llama\n2 7B/13B models using a subset of the GSM8K,\ncomprising 600 data samples. We generate CoT\nand PAL solutions with Llama 2 7B/13B models4.\nFor model selection, we use Llama 2 7B/13B Chat\nmodels5. The experiments are carried out by using\ngreedy decoding without tuning hyperparameters.\nAs shown in Table 4, our method consistently\nyields performance enhancements when applied\nto Llama 2 models. Specifically, compared to the\n4https://huggingface.co/meta-llama/Llama-2-7b-hf\nhttps://huggingface.co/meta-llama/Llama-2-13b-hf\n5https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nhttps://huggingface.co/meta-llama/Llama-2-13b-chat-\nhf\nbaseline results on the 13B model, our method\nachieves respectable 2% performance gains, im-\nproving accuracy from 33.3% to 35.3%. On the\nsmaller 7B model, we observe the performance im-\nprovement from 15.0% to 16.2% when utilizing the\n7B model as the selector. Moreover, when employ-\ning the 13B model as the selector, the performance\nis further improved to 16.8%. These results affirm\nthe broad applicability of our method.\n4 Analysis\nIn this section, we provide a few analyses to see\nwhen and how our method works.\n4.1 Influencing Factors\nTo better understand the reasons for the perfor-\nmance improvement across various datasets and\nbackbone LLMs, we present the performance im-\nprovement, and the associated influencing factors\nin Table 5.\nAs demonstrated in our theoretical analysis, the\nperformance improvement is jointly determined by\nthe difference between the two combined meth-\nods, |R(x)|, and the effectiveness of the model\nselection, ρ. The results from Table 5 showcase\na high expected performance difference between\nCoT and PAL, and decent selection accuracy with\nall backbone LLMs, which clarifies the significant\nperformance gains across the datasets.\nFirstly, the ∆Upper Bound of CoT and PAL re-\nflects how differently the two base models behave\nacross questions. A larger ∆Upper Bound signifies\na larger room for potential improvement. Specif-\nically, on GSM8K with ChatGPT, ∆Upper Bound\nstands at 8.6%, even though the accuracies of CoT\nand PAL are closely matched at 80.8% and 79.2%,\nrespectively. Similarly, with GPT-4,∆Upper Bound\nis 2.5% while the accuracies of the two base models\nare close (94.6% vs 94.0%).\nSecondly, across the eight datasets with varied\ncomplexities, each of the three LLMs exhibits com-\nmendable selection accuracy. Notably, both Codex\nand GPT-4 show a relatively high success selection\n763\nBackbone Metric Arithmetic Symbolic\nGSM8K SV AMP ASDIV SingleOP SingleEQ AddSub MultiArith Date\nCodex\n∆UpperBound 10.0 8.1 6.5 1.6 0.9 2.0 0.3 4.6\nSuccess Rate 74.8 72.5 63.7 87.9 88.0 70.0 92.9 87.8\nImprovement +3.2 +2.6 +1.4 +0.9 +0.4 -0.3 0 +1.9\nChatGPT\n∆UpperBound 8.6 6.0 3.4 2.4 1.4 2.5 0.6 9.8\nSuccess Rate 60.4 66.4 69.8 58.0 57.1 60.9 75.0 53.6\nImprovement +1.8 +1.3 +0.1 0 +0.4 +0.2 0 +1.1\nGPT-4\n∆UpperBound 2.5 3.6 2.2 2.4 0.6 1.3 0.5 1.1\nSuccess Rate 72.6 68.7 64.2 57.0 69.2 85.7 100 86.7\nImprovement +1.0 +1.8 +0.8 +0.1 -0.2 +0.8 +0.5 +0.5\nTable 5: We define the following terms: ∆UpperBound = AccUpper Bound−Accm1 , where AccUpper Bound is the\nupper bound accuracy where we assume a perfect model selection andm1 is the stronger one of the two base models.\n∆UpperBound reflects the expected performance difference between the two base models. Success Rate calculates\nthe correct selection rates when either CoT is correct or PAL is correct, i.e., we ignore the cases where both methods\nare either correct or wrong. Improvement is the performance improvement achieved over the performance of the\nstronger base model.\nBackbone ChatGPT GPT-4\nm1 CoT CoT\nm2 PAL CoT′ PAL CCoT\nAccm1 80.8 80.8 94.6 94.6\nAccm2 79.2 79.2 94.0 95.1\nOurs 82.6 80.8 95.6 95.2\nImprovement (+1.8) (+0) (+1.0) (+0.1)\n∆Upper Bound 8.6 7.5 2.5 1.7\nSuccess Rate 60.4 52.2 72.6 58.8\nTable 6: Other model combinations results on GSM8K.\nCoT′ denotes the base CoT model with the temperature\nof 0.1. CCoT denotes ComplexCoT (Fu et al., 2022).\nrate. For example, their success rates on GSM8K\nsurpass 70%, and on the simpler MultiArith dataset,\nthe selection accuracies exceed 90%.\nFurthermore, as detailed in Theorem 1, perfor-\nmance improvements are attainable even without a\nhigh success selection rate. Our empirical results\ncorroborate it. For instance, on the date understand-\ning task employing ChatGPT, despite a modest\nsuccess rate of 53.6%, we still achieve 1.1% per-\nformance enhancement with a large ∆Upper Bound\nof 9.8%, and small overall performance difference\n(69.1% and 68.3%).\n4.2 Combination between Similar Methods\nWe choose CoT and PAL as our two base mod-\nels due to the motivation of combining different\nstrengths of distinct models. We conduct exper-\niments to examine whether the performance im-\nproves when we combine two similar base models.\nWe use two variants of CoT:CoT′where we set the\ntemperature at 0.1, ComplexCoT (Fu et al., 2022)\nwhere we use more complex few-shot examples in\nthe prompt. Both of these methods’ accuracies are\nsimilar or higher than the accuracy of PAL.\nFrom the results in Table 6, we can find that\nmodel selection between CoT and CoT′, or CoT\nand ComplexCoT, does not lead to substantial per-\nformance gains, even though the accuracy of CoT′\nand ComplexCoT is on par with that of PAL. On the\nother hand, model selection between CoT and PAL\nresults in consistent performance improvements.\nTo understand the reasons behind these outcomes,\nwe further investigate the ∆Upper Bound and the\nsuccess selection rate.\nFirstly, ∆Upper Bound of CoT-PAL exceeds\nthat of other combinations, CoT- CoT′ and\nComplexCoT-CoT, despite their employing two\nstronger or equivalent two base models. This obser-\nvation suggests a larger absolute value of the accu-\nracy difference per question for CoT-PAL. It indi-\ncates that CoT and PAL perform more dissimilarly\nthan other model combinations, which represents a\nlarger |R(x)|. As Proposition 1 highlights, without\na substantial |R(x)|, it is unlikely to achieve sig-\nnificant performance gain since the improvement\ncomponent is factored by |R(x)|.\nSecondly, the success selection rate of CoT-PAL\nsurpasses that of other model combinations. It\nmeans that the selector is more likely to select the\ncorrect choice when one solution derives from CoT\nand the other from PAL. In theory, this higher suc-\ncess rate implies that when |R(x)|is high for a\ngiven question x, the success selection probability\nρx for CoT-PAL is higher than others.\nThese findings support our initial motivation and\nhypothesis. We choose CoT and PAL as our two\n764\nCoT LLM PAL LLM Selector LLM AccCoT AccPAL Ours Success Rate ∆UpperBound\nChatGPT ChatGPT ChatGPT 80.8 79.2 82.6 (+1.6) 60.4 8.6\nChatGPT Codex ChatGPT 80.8 71.5 81.2 (+0.4) 70.6 7.0\nChatGPT Codex GPT-4 80.8 71.5 84.4 (+3.6) 84.8 7.0\nTable 7: Results with the combination of different backbone LLMs on GSM8K. CoT LLM refers to the backbone\nLLM used for CoT solution generation.\nMetric Codex ChatGPT GPT-4\nCoT-PAL PAL-CoT CoT-PAL PAL-CoT CoT-PAL PAL-CoT\nAccCoT 64.4 64.4 80.8 80.8 94.6 94.6\nAccPAL 71.5 71.5 79.2 79.2 94.0 94.0\nOurs 69.9(-1.6) 74.7(+3.2) 82.6(+1.8) 81.6(+0.8) 95.6(+1.0) 95.1(+0.5)\nSuccess Rate 56.7 75 60.4 54.6 72.6 63\nRatioCoT 71.9 17.3 89.9 79.7 60.3 53.4\nRatioPAL 28.1 82.7 10.1 20.3 39.7 46.6\nTable 8: The effect of option order for different backbones on GSM8K. CoT-PAL represents CoT and PAL being\nplaced as choice A and B respectively. RatioCoT indicates the ratio of CoT selected out of all the selections. As\nstated previously, during analysis, we ignore the cases where both CoT and PAL are either correct or wrong.\nbase models because they represent distinct rea-\nsoning approaches using natural language and pro-\ngramming language. We expect these models to\nexhibit a significant difference in errors and accura-\ncies, indicated by a high ∆Upper Bound. Moreover,\nwe posit that for a particular question, the consid-\nerable difference in errors between two solutions\nmakes it easier for LLMs to select the correct op-\ntion, leading to a higher success rate compared\nto selecting between two similar base models like\nCoT-CoT′. This holds true even when different\nprompts or temperature settings are used.\n4.3 Combination between Different Backbone\nLLMs\nConsidering that Codex is optimized for code gen-\neration and ChatGPT performs better in natural\nlanguage reasoning, we embark on experiments in-\ntegrating various backbone LLMs, to harness the\nstrengths of distinct LLMs and reasoning models.\nAs shown in Table 7, we achieve a 0.4% perfor-\nmance improvement with ChatGPT as the selector,\ndespite a notable 9.3% accuracy disparity between\nPAL from Codex and CoT from ChatGPT. More\nimpressively, when utilizing GPT-4 as the model\nselector, we get a significant 3.6% performance\nboost, with an 84.8% success selection rate. These\nresults hint the potential benefits when integrating\nmultiple models tailored for specialized domains\nor tasks.\n4.4 The Effect of Option Order\nTo understand the influence of option order in a\nmultiple-choice question format, we perform ex-\nperiments by modifying the option order. The re-\nsults in Table 8 show a performance impact from\nsimply changing the choice order. We attribute this\nto the inherent bias within the LLMs. A similar\nphenomenon is reported by Zheng et al. (2023b).\nEach of the three backbones seems to exhibit a\npreference for the first option. Particularly notice-\nable on Codex, in which when CoT is positioned\nfirst, it is selected in 71.9% of all selections. How-\never, RatioCoT significantly drops to 17.3% when\nputting CoT second. ChatGPT shows a preference\nfor natural language reasoning as it always selects\nCoT solutions more frequently regardless of its po-\nsition in the options. Moreover, when CoT is the\nsecond choice, RatioCoT decreases from 89.9% to\n79.7%, which we speculate demonstrates its prefer-\nence for the first option. GPT-4 has similar biases\nakin to ChatGPT, though it appears fairer. The op-\ntion order also affects the success selection rate and\nfurther influences the overall performance.\nWhile Zhao et al. (2021) attempts to address the\ninherent bias through probability calibration, the\ntask of mitigating the bias in the absence of un-\nderlying token probabilities remains a challenge.\nTherefore, despite the wide use of LLMs in data an-\nnotation and task evaluation(Xu et al., 2023; Wang\net al., 2023; Chiang et al., 2023), addressing and\neliminating inherent biases is a critical area that\ndeserves further exploration.\n765\n5 Related Work\nEnsemble learning. In machine learning, the strat-\negy of combining various models to address a sin-\ngle problem is exemplified in techniques such as\nbagging (Breiman, 1996), boosting (Freund and\nSchapire, 1997; Chen and Guestrin, 2016; Ke et al.,\n2017), and random forest (Ho, 1995; Breiman,\n2001). The underlying idea in these methods is\nthat a group of weak learners can collectively man-\nifest as a strong learner. This concept has also\nfound its place in deep learning through the use of\nensembles. For reasoning, self-consistency sam-\nples diverse reasoning paths and chooses the most\nconsistent answer through majority voting (Wang\net al., 2022b). Wang et al. (2022a) takes it a step\nfurther by introducing rationale-augmented ensem-\nbles, emphasizing rationale sampling in the out-\nput space. However, typically, ensembling places\nequal weights on models through majority voting,\nwhich may restrict the full potential of the diverse\nstrengths that each model offers.\nReasoning. The research community has made\ntremendous progress in the field of reasoning.\nApart from CoT (Wei et al., 2022b) and PAL (Gao\net al., 2022; Chen et al., 2022), Zhou et al. (2022)\nsimplifies complex problems by breaking them\ndown into a series of sub-problems. Kojima et al.\n(2022) shows that by simply adding \"Let’s think\nstep by step\" before each answer, LLMs can be\ncompetent zero-shot reasoners. Creswell et al.\n(2022) alternates between selection and inference\nstages, generating causal reasoning steps to the\nfinal answer. Kazemi et al. (2022) proposes a\nbackward chaining algorithm that breaks reason-\ning down into four sub-models. Paul et al. (2023);\nXie et al. (2023); Yao et al. (2023) refine interme-\ndiate reasoning steps by leveraging another critic\nmodel or the self-evaluation ability. Recently, Zhou\net al. (2023) utilizes GPT-4 Code Interpreter and\ncode-based-self-verification, achieving a 97.0% ac-\ncuracy on GSM8K, but their work is concurrent\nwith ours. More importantly, the contributions of\nthese methods are distinct from our approach, and\nthe progress made by them could potentially be\nseamlessly integrated using our method.\nSelf-Evaluation. LLM calibration studies reveal\nthat the probabilistic predictions made by current\nLLMs closely align with the actual frequencies of\ntoken occurrences, hence producing well-calibrated\npredictions for certain tasks (Guo et al., 2017; Ka-\ndavath et al., 2022; Jiang et al., 2020). As LLMs ex-\nhibit reliable calibration, there is a growing number\nof research emphasizing the use of self-evaluation\nfor verification. Shinn et al. (2023) proposes an\napproach to provide an agent with dynamic mem-\nory and self-reflection capabilities. Madaan et al.\n(2023) proposes a method to generate outputs from\nLLMs and refine its previously generated output\ngiven its own feedback. Different from these works\nwhere the underlying method is the same, in this\nwork, we are interested in combining systems with\ndifferent strengths and weaknesses through self-\nevaluation model selection.\n6 Conclusion\nWe introduce a method that effectively combines\ntwo distinct models, CoT and PAL, by using LLMs\nfor the model selection. Our theoretical analysis\nsupports the feasibility of such model combination,\nwhich is validated by empirical results. Our method\nachieves performance improvements across eight\ndatasets with various backbone LLMs. We reduce\nthe computation cost while attaining notable per-\nformance improvement with self-consistency. We\nset new SOTA results on GSM8K and SV AMP.\nThis research represents a significant step towards\ntapping into the potential of diversity and collabo-\nration among models in LLMs.\nIn our future work, we aim to expand this frame-\nwork to more reasoning tasks and possibly other\ndomains. An intriguing concept involves explor-\ning the use of diverse system instructions to elicit\nvarying model behaviors for model combinations.\nFurthermore, a genuine combination of specialized\nmodels, each with its strengths and expertise, offers\na promising avenue for exploration.\n7 Limitation\nThis work focuses on reasoning tasks, but we be-\nlieve that extending the exploration of model se-\nlection into other domains could yield valuable\ninsights. Further, due to the inherent bias of LLMs,\nour method is affected by the order of options. We\nencourage the community to explore solutions for\nbias mitigation. We also anticipate subsequent stud-\nies to identify more robust and potent model selec-\ntors. Finally, our combination strategy currently\ncomprises only the CoT and PAL base models. Fu-\nture research could investigate the incorporation\nof more base models with distinct strengths, with\nthe aim of creating a more powerful synthesis that\nleverages their unique advantages.\n766\nReferences\nLeo Breiman. 1996. Bagging predictors. Machine\nlearning, 24:123–140.\nLeo Breiman. 2001. Random forests. Machine learning,\n45:5–32.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. ArXiv,\nabs/2005.14165.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter\nLee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general\nintelligence: Early experiments with gpt-4. ArXiv,\nabs/2303.12712.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde, Jared Kaplan, Harrison Ed-\nwards, Yura Burda, Nicholas Joseph, Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Moham-\nmad Bavarian, Clemens Winter, Philippe Tillet, Fe-\nlipe Petroski Such, David W. Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-V oss, William H. Guss, Alex Nichol, Igor\nBabuschkin, S. Arun Balaji, Shantanu Jain, Andrew\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew M. Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv,\nabs/2107.03374.\nTianqi Chen and Carlos Guestrin. 2016. Xgboost: A\nscalable tree boosting system. In Proceedings of\nthe 22nd acm sigkdd international conference on\nknowledge discovery and data mining , pages 785–\n794.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from rea-\nsoning for numerical reasoning tasks. ArXiv,\nabs/2211.12588.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinod-\nkumar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Re-\nwon Child, Oleksandr Polozov, Katherine Lee, Zong-\nwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. ArXiv, abs/2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. ArXiv, abs/2110.14168.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. ArXiv,\nabs/2205.09712.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. ArXiv, abs/1810.04805.\nYoav Freund and Robert E Schapire. 1997. A decision-\ntheoretic generalization of on-line learning and an\napplication to boosting. Journal of computer and\nsystem sciences, 55(1):119–139.\nYao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. ArXiv, abs/2210.00720.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. ArXiv, abs/2211.10435.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International Conference on Machine\nLearning.\nTin Kam Ho. 1995. Random decision forests. In Pro-\nceedings of 3rd international conference on docu-\nment analysis and recognition, volume 1, pages 278–\n282. IEEE.\n767\nZhengbao Jiang, J. Araki, Haibo Ding, and Graham\nNeubig. 2020. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962–977.\nSaurav Kadavath, Tom Conerly, Amanda Askell, T. J.\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zachary Dodds, Nova DasSarma, Eli Tran-\nJohnson, Scott Johnston, Sheer El-Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, John\nKernion, Shauna Kravec, Liane Lovitt, Kamal\nNdousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom B. Brown, Jack Clark, Nicholas Joseph,\nBenjamin Mann, Sam McCandlish, Christopher Olah,\nand Jared Kaplan. 2022. Language models (mostly)\nknow what they know. ArXiv, abs/2207.05221.\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia,\nXinyuan Xu, and Deepak Ramachandran. 2022. Lam-\nbada: Backward chaining for automated reasoning in\nnatural language. ArXiv, abs/2212.13894.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,\nWei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\n2017. Lightgbm: A highly efficient gradient boost-\ning decision tree. Advances in neural information\nprocessing systems, 30.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. ArXiv,\nabs/2205.11916.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. Mawps:\nA math word problem repository. In North Ameri-\ncan Chapter of the Association for Computational\nLinguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. ArXiv, abs/2303.17651.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and devel-\noping english math word problem solvers. ArXiv,\nabs/2106.15772.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nArkil Patel, S. Bhattamishra, and Navin Goyal. 2021.\nAre nlp models really able to solve simple math word\nproblems? In North American Chapter of the Associ-\nation for Computational Linguistics.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-\nriz Borges, Antoine Bosselut, Robert West, and Boi\nFaltings. 2023. Refiner: Reasoning feedback on in-\ntermediate representations. ArXiv, abs/2304.01904.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. 2023.\nReflexion: an autonomous agent with dynamic mem-\nory and self-reflection. ArXiv, abs/2303.11366.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ameet Annasaheb Rahane, Anan-\ntharaman S. Iyer, Anders Andreassen, Andrea San-\ntilli, Andreas Stuhlmuller, Andrew M. Dai, An-\ndrew D. La, Andrew Kyle Lampinen, Andy Zou,\nAngela Jiang, Angelica Chen, Anh Vuong, Ani-\nmesh Gupta, Anna Gottardi, Antonio Norelli, Anu\nVenkatesh, Arash Gholamidavoodi, Arfa Tabassum,\nArul Menezes, Arun Kirubarajan, Asher Mullokan-\ndov, Ashish Sabharwal, Austin Herrick, Avia Efrat,\nAykut Erdem, Ayla Karakacs, Bridget R. Roberts,\nBao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski,\nBatuhan Ozyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk Ek-\nmekci, Bill Yuchen Lin, Blake Stephen Howald,\nCameron Diao, Cameron Dour, Catherine Stinson,\nCedrick Argueta, C’esar Ferri Ram’irez, Chandan\nSingh, Charles Rathkopf, Chenlin Meng, Chitta\nBaral, Chiyu Wu, Chris Callison-Burch, Chris Waites,\nChristian V oigt, Christopher D. Manning, Christo-\npher Potts, Cindy Tatiana Ramirez, Clara Rivera,\nClemencia Siro, Colin Raffel, Courtney Ashcraft,\nCristina Garbacea, Damien Sileo, Daniel H Garrette,\nDan Hendrycks, Dan Kilman, Dan Roth, Daniel\nFreeman, Daniel Khashabi, Daniel Levy, Daniel\nGonz’alez, Danny Hernandez, Danqi Chen, Daphne\nIppolito, Dar Gilboa, David Dohan, D. Drakard,\nDavid Jurgens, Debajyoti Datta, Deep Ganguli, De-\nnis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen,\nDerek Tam, Dieuwke Hupkes, Diganta Misra, Dil-\nyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-\nHo Lee, Ekaterina Shutova, Ekin Dogus Cubuk,\nElad Segal, Eleanor Hagerman, Elizabeth Barnes,\nElizabeth P. Donoway, Ellie Pavlick, Emanuele\nRodolà, Emma FC Lam, Eric Chu, Eric Tang,\nErkut Erdem, Ernie Chang, Ethan A. Chi, Ethan\nDyer, Ethan J. Jerzak, Ethan Kim, Eunice Engefu\nManyasi, Evgenii Zheltonozhskii, Fan Xia, Fate-\nmeh Siar, Fernando Mart’inez-Plumed, Francesca\nHapp’e, François Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nL’opez, Gregor Betz, Guy Gur-Ari, Hana Galija-\nsevic, Han Sol Kim, Hannah Rashkin, Hanna Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHubert Wong, Ian Aik-Soon Ng, Isaac Noble, Jaap\nJumelet, Jack Geissinger, John Kernion, Jacob Hilton,\nJaehoon Lee, Jaime Fernández Fisac, J. Brooker\nSimon, James Koppel, James Zheng, James Zou,\nJan Koco’n, Jana Thompson, Jared Kaplan, Jarema\nRadom, Jascha Narain Sohl-Dickstein, Jason Phang,\n768\nJason Wei, Jason Yosinski, Jekaterina Novikova,\nJelle Bosscher, Jenni Marsh, Jeremy Kim, Jeroen\nTaal, Jesse Engel, Jesujoba Oluwadara Alabi, Ji-\nacheng Xu, Jiaming Song, Jillian Tang, Jane W\nWaweru, John Burden, John Miller, John U. Balis,\nJonathan Berant, Jorg Frohberg, Jos Rozen, José\nHernández-Orallo, Joseph Boudeman, Joseph Jones,\nJoshua B. Tenenbaum, Joshua S. Rule, Joyce Chua,\nKamil Kanclerz, Karen Livescu, Karl Krauth, Karthik\nGopalakrishnan, Katerina Ignatyeva, Katja Markert,\nKaustubh D. Dhole, Kevin Gimpel, Kevin Ochieng’\nOmondi, Kory Wallace Mathewson, Kristen Chia-\nfullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLuca Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Col’on, Luke Metz, Lutfi Kerem\ncSenel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Madotto Andrea, Maheen Saleem Farooqi,\nManaal Faruqui, Mantas Mazeika, Marco Baturan,\nMarco Marelli, Marco Maru, M Quintana, Marie\nTolkiehn, Mario Giulianelli, Martha Lewis, Martin\nPotthast, Matthew Leavitt, Matthias Hagen, M’aty’as\nSchubert, Medina Baitemirova, Melissa Arnaud,\nMelvin Andrew McElrath, Michael A. Yee, Michael\nCohen, Mi Gu, Michael I. Ivanitskiy, Michael Star-\nritt, Michael Strube, Michal Swkedrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Monica Tiwari, Mo-\nhit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh\nGheini, T MukundVarma, Nanyun Peng, Nathan\nChi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas\nCameron, Nicholas S. Roberts, Nicholas Doiron,\nNikita Nangia, Niklas Deckers, Niklas Muennighoff,\nNitish Shirish Keskar, Niveditha Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar\nAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pas-\ncale Fung, Paul Pu Liang, Paul Vicol, Pegah\nAlipoormolabashi, Peiyuan Liao, Percy Liang, Pe-\nter W. Chang, Peter Eckersley, Phu Mon Htut, Pi-\nBei Hwang, P. Milkowski, Piyush S. Patil, Pouya\nPezeshkpour, Priti Oli, Qiaozhu Mei, QING LYU,\nQinlang Chen, Rabin Banjade, Rachel Etta Rudolph,\nRaefer Gabriel, Rahel Habacker, Ram’on Risco\nDelgado, Raphaël Millière, Rhythm Garg, Richard\nBarnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman\nNovak, Roman Sitelew, Ronan Le Bras, Rosanne\nLiu, Rowan Jacobs, Rui Zhang, Ruslan Salakhut-\ndinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan\nTeehan, Rylan Yang, Sahib J. Singh, Saif M. Mo-\nhammad, Sajant Anand, Sam Dillavou, Sam Shleifer,\nSam Wiseman, Samuel Gruetter, Sam Bowman,\nSamuel S. Schoenholz, Sanghyun Han, Sanjeev\nKwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi S. Hamdan, Sharon Zhou, Shashank Srivas-\ntava, Sherry Shi, Shikhar Singh, Shima Asaadi,\nShixiang Shane Gu, Shubh Pachchigar, Shubham\nToshniwal, Shyam Upadhyay, Shyamolima Deb-\nnath, Siamak Shakeri, Simon Thormeyer, Simone\nMelzi, Siva Reddy, Sneha Priscilla Makini, Soo\nhwan Lee, Spencer Bradley Torene, Sriharsha Hat-\nwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon,\nStella Rose Biderman, Stephanie C. Lin, S. Prasad,\nSteven T. Piantadosi, Stuart M. Shieber, Summer\nMisherghi, Svetlana Kiritchenko, Swaroop Mishra,\nTal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq A.\nAli, Tatsuo Hashimoto, Te-Lin Wu, Theo Desbordes,\nTheodore Rothschild, Thomas Phan, Tianle Wang,\nTiberius Nkinyili, Timo Schick, T. N. Kornev, Tim-\nothy Telleen-Lawton, Titus Tunduny, Tobias Ger-\nstenberg, Trenton Chang, Trishala Neeraj, Tushar\nKhot, Tyler O’Brien Shultz, Uri Shaham, Vedant\nMisra, Vera Demberg, Victoria Nyamai, Vikas Rau-\nnak, Vinay Venkatesh Ramasesh, Vinay Uday Prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fe-\ndus, William Saunders, William Zhang, W V ossen,\nXiang Ren, Xiaoyu Tong, Xinyi Wu, Xudong Shen,\nYadollah Yaghoobzadeh, Yair Lakretz, Yang Song,\nYasaman Bahri, Ye Ji Choi, Yichi Yang, Yiding Hao,\nYifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yun-\ntao Bai, Zachary Seid, Zhao Xinran, Zhuoye Zhao,\nZi Fu Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu,\nSahib Singh, and Uri Shaham. 2022. Beyond the imi-\ntation game: Quantifying and extrapolating the capa-\nbilities of language models. ArXiv, abs/2206.04615.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cantón Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V .\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Fran-\ncis Song, Noah Siegel, L. Wang, Antonia Creswell,\nGeoffrey Irving, and Irina Higgins. 2022. Solving\nmath word problems with process- and outcome-\nbased feedback. ArXiv, abs/2211.14275.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Huai hsin Chi, and Denny Zhou. 2022a.\nRationale-augmented ensembles in language mod-\nels. ArXiv, abs/2207.00747.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Huai hsin Chi, and Denny Zhou. 2022b. Self-\n769\nconsistency improves chain of thought reasoning in\nlanguage models. ArXiv, abs/2203.11171.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang,\nCunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie,\nJindong Wang, Xingxu Xie, Wei Ye, Shi-Bo Zhang,\nand Yue Zhang. 2023. Pandalm: An automatic evalu-\nation benchmark for llm instruction tuning optimiza-\ntion. ArXiv, abs/2306.05087.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022a. Emergent abilities of large language models.\nTrans. Mach. Learn. Res., 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and\nDenny Zhou. 2022b. Chain of thought prompting\nelicits reasoning in large language models. ArXiv,\nabs/2201.11903.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-\nYen Kan, Junxian He, and Qizhe Xie. 2023. De-\ncomposition enhances reasoning via self-evaluation\nguided decoding. ArXiv, abs/2305.00633.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. ArXiv,\nabs/2304.01196.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. ArXiv,\nabs/2305.10601.\nTony Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nInternational Conference on Machine Learning.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo\nLi, and Yu Li. 2023a. Progressive-hint prompting\nimproves reasoning in large language models. ArXiv,\nabs/2304.09797.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Haotong\nZhang, Joseph Gonzalez, and Ion Stoica. 2023b.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. ArXiv, abs/2306.05685.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun\nLuo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song,\nMingjie Zhan, and Hongsheng Li. 2023. Solving\nchallenging math word problems using gpt-4 code\ninterpreter with code-based self-verification. ArXiv,\nabs/2308.07921.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Huai hsin\nChi. 2022. Least-to-most prompting enables com-\nplex reasoning in large language models. ArXiv,\nabs/2205.10625.\n770\nA A detailed version of Theorem 1\nIn this appendix, we provide a detailed version of Theorem 1. Whereas Theorem 1 only states the\nexistence of problem instances, Theorem 2 constructs such instances concretely: i.e., Theorem 2 implies\nTheorem 1. Define µx[X] to be the distribution for the expected errors: i.e., an expected error can be\nwritten by Ex∼µx[X],y,f[1 [y̸= f(x)]] for some function f. Define S[X] = {x∈X : R(x) <0}. Let us\ndenote U[X] as the uniform distribution over X. Given any X, we write n= |X|, T = |S[X]|, α= T/n.\nAssume that 1 ≤T <n.\nTheorem 2. Let µx[X] = U[X] and Xbe given such that |X| < ∞. Let ϵ,δ ∈(0,1) and λ ∈(0,1]\nsuch that β = ϵT\nn−T ∈(0,1) and λ ≥1 − β\nϵT(n−T −δ). Let Rand ρx be set such that R(x) = −ϵ\nfor x∈S[X], R(x) = βfor x∈X\\ S[X], (1/T) ∑\nx∈S[X] ρx = λ, and (1/(n−T)) ∑\nx∈X\\S[X] ρx =\nϵ(T/(n−T))(1 −λ)β−1 + δ/(n−T). Then, we have that err <err1, err1 ≤err2, and\nρ= 1 −α+ λ[2α−1] + δ\nn.\nIn particular, whenα≥0.5, we have ρ→0 as α→1 and (δ/(n−T)) →0 (with λ= 1−β\nTϵ(n−T−δ));\nwhen α< 0.5, we have ρ→0 as α→0 and (δ/n) →0 (with λ= 1).\nThe proof of Theorem 2 is presented in Appendix B. Theorem 2 shows that the overall success\nprobability of the selection process can be much worse than a random guess to achieve the improvement\nover the base methods m1 and m2; i.e., err <err1 and err1 ≤err2 can happen with ρ< 0.5. Indeed, it\nis possible to have ρ→0 with the improvement (err <err1 and err1 ≤err2) when the size of Xis large:\nwhen α≥0.5, we can choose λ= 1 −β\nTϵ(n−T −δ) with which err <err1, err1 ≤err2, and ρ→0 as\nα→1 and (δ/(n−T)) →0. When α< 0.5, we can choose λ= 1 with which err <err1, err1 ≤err2,\nand ρ→0 as α→0 and (δ/n) →0. This supports our proposition that, despite not training a new model\nfor the selection process and with the in-context learning limited to a few-shot prompt, it is possible to\nachieve improvement, even if we do not achieveρx >0.5 in some instances.\nTheorem 2 also suggests that if the overall performance of two base methods is similar, captured\nby ϵ, the overall selection process can be weak to achieve some improvement, as long as the success\nselection probability is relatively high when the two methods have very different expected errors (or\naccuracies) for a given question. In essence, Theorem 2 suggests a trade-off: we want |R(x)|to be larger\nwhen deciding which two base methods m1 and m2 to choose, implying that we prefer base methods to\nperform dissimilarly on X. On the other hand, if two base methods exhibit a substantial expected accuracy\ndifference, then the selection process needs to be stronger to improve the performance (i.e., ρneeds to be\nlarger). However, if the expected accuracy difference between the two base methods is relatively small,\nincreasing the power of the selection process is not that necessary to boost performance.\nB Proofs\nB.1 Proof of Proposition 1\nProof. Define acc = 1 −err and acci = 1 −erri for i ∈ {1,2}. Since expected error =\nE[1 [incorrect prediction]] = P(incorrect prediction) = 1 −P(correct prediction), we have that\nacci = p(correct|mi) = Ex[p(correct|x,mi)]\nwhere correct represents the event of the correct prediction. Similarly,\nacc = Ex\n[2∑\ni=1\np(mi|x)p(correct|x,mi)\n]\nwhere p(mi|x) represents the probability of selecting method mi given xvia the proposed method. Thus,\nacc −acc1 = Ex[p(m1|x)p(correct|x,m1) + p(m2|x)p(correct|x,m2) −p(correct|x,m1)]\n771\n= Ex[(p(m1|x) −1)p(correct|x,m1) + p(m2|x)p(correct|x,m2)]\n= Ex[(p(m2|x)p(correct|x,m2) −(1 −p(m1|x))p(correct|x,m1)]\nSince 1 −p(m1|x) = p(m2|x),\nacc −acc1 = Ex[(p(m2|x)p(correct|x,m2) −p(m2|x)p(correct|x,m1)]\n= Ex[(p(m2|x)R(x)] .\nHere, we notice that\np(m2|x) =\n{\nρx if R(x) ≥0\n1 −ρx if R(x) <0\n= 1 {R(x) ≥0}ρx + 1 {R(x) <0}(1 −ρx).\nBy plugging this into the above equation,\nacc −acc1 = Ex[(1 {R(x) ≥0}ρx + 1 {R(x) <0}(1 −ρx))R(x)]\n= Ex[R(x)1 {R(x) ≥0}ρx] + Ex[R(x)1 {R(x) <0}(1 −ρx)]\nSince R(x)1 {R(x) ≥0}= |R(x)|1 {R(x) ≥0}and R(x)1 {R(x) <0}= −|R(x)|1 {R(x) <0}, we\nhave that\nacc −acc1 = Ex[|R(x)|1 {R(x) ≥0}ρx] −Ex[|R(x)|1 {R(x) <0}(1 −ρx)]\n= Ex[|R(x)|((1 {R(x) ≥0}+ 1 {R(x) <0})ρx −1 {R(x) <0})]\nSince (1 {R(x) ≥0}+ 1 {R(x) <0}) = 1 for any x,\nacc −acc1 = Ex[|R(x)|(ρx −1 {R(x) <0})].\nB.2 Proof of Theorem 2\nProof. We first confirm that R(x) and ρx define valid probabilities under the condition of this statement.\nFor R(x), since ϵ∈(0,1) and β ∈(0,1), it defines valid probabilities for methods m1 and m2. For ρx,\nsince λ∈[0,1], it also defines valid probabilities for the case of x∈S[X]. For the case of x∈X\\ S[X],\nsince ϵ(T/(n−T))(1 −λ)β−1 + δ/n≥0, we need to show that ϵ(T/(n−T))(1 −λ)β−1 + δ/n≤1.\nThat is,\nϵ(T/(n−T))(1 −λ)β−1 + δ/(n−T) ≤1\n⇐⇒ϵT(1 −λ)β−1 ≤n−T −δ\n⇐⇒1 −β(n−T −δ)\nϵT ≤λ,\nwhich is satisfied by the condition on λthat λ≥1 − β\nϵT(n−T −δ). Thus, the condition on ρx defines\nthe valid probabilities for both cases of x∈S[X] and x∈X\\ S[X].\nWe now show thaterr <err1. Invoking Proposition 1,\nerr = err1 −Ex[|R(x)|(ρx −1 {R(x) <0})] .\nThus, we have err <err1 if Ex[|R(x)|(ρx −1 {R(x) <0})] >0. This condition can be rewritten as\nEx[|R(x)|(ρx −1 {R(x) <0})] >0\n⇐⇒1\nn\n∑\nx∈X\n[|R(x)|(ρx −1 {R(x) <0})] >0\n772\n⇐⇒\n∑\nx∈X\\S[X]\n|R(x)|ρx +\n∑\nx∈S[X]\n|R(x)|ρx >\n∑\nx∈S[X]\n|R(x)|\n⇐⇒\n∑\nx∈X\\S[X]\n|R(x)|ρx + ϵTλ>ϵT\n⇐⇒β\n∑\nx∈X\\S[X]\nρx >ϵT −ϵTλ = ϵT(1 −λ)\n⇐⇒\n∑\nx∈X\\S[X]\nρx > ϵT(1 −λ)\nβ\nThis is satisfied by the condition on ρthat (1/(n−T)) ∑\nx∈X\\S[X] ρx = ϵ(T/(n−T))(1 −λ)β−1 +\nδ/(n−T) for some δ >0: i.e.,\n∑\nx∈X\\S[X]\nρx = ϵT(1 −λ)\nβ + δ.\nTherefore, we have that err <err1.\nWe now show thaterr1 ≤err2. Similarly to the proof of Proposition 1, we define acci = 1 −erri for\ni∈{1,2}. Then, the inequality err1 ≤err2 holds if acc1 ≥acc2. By using correct to represent the event\nof the correct prediction, this condition can be rewritten as\nacc1 ≥acc2\n⇐⇒\n∑\nx∈X\np(correct|x,m1)] ≥\n∑\nx∈X\np(correct|x,m2)]\n⇐⇒0 ≥\n∑\nx∈X\nR(x) = (n−T)β−ϵT\n⇐⇒ ϵT\nn−T ≥β\nThis is satisfied by β = ϵT\nn−T. Thus, we have that err1 ≤err2.\nUsing these, we now compute the ρas\nρ= Ex∼µx[X][ρx]\n= 1\nn\n∑\nx∈X\\S[X]\nρx + 1\nn\n∑\nx∈S[X]\nρx\n= 1\nn\n(ϵT(1 −λ)\nβ + δ\n)\n+ αλ\n= ϵT(1 −λ)\nβn + αλ+ δ\nn\n= ϵT(1 −λ)(n−T)\nϵTn + αλ+ δ\nn\n= (1 −λ) −α(1 −λ) + αλ+ δ\nn\n= 1 −α+ λ[2α−1] + δ\nn.\nFinally, we prove the asymptotic behavior using this equation. When α< 0.5, by setting λ= 1, we have\nthat\nρ= 1 −α+ λ[2α−1] + δ\nn = 1 −α+ 2α−1 + δ\nn = α+ δ\nn →0\nas α→0 and (δ/n) →0. When α≥0.5, by setting λ= 1 − β\nTϵ(n−T −δ), we have that\nρ= 1 −α+\n(\n1 − β\nTϵ(n−T −δ)\n)\n[2α−1] + δ\nn\n773\n= 1 −α+ 2α−1 −[2α−1] β\nTϵ(n−T −δ) + δ\nn\n= α−[2α−1] β\nTϵ(n−T −δ) + δ\nn\nBy defining Q= β\nTϵ(n−T −δ), we have\nρ= α−[2α−1]Q+ δ\nn.\nHere,\nQ= β\nTϵ(n−T −δ) = ϵT\nn−T\n1\nTϵ(n−T −δ) = 1\nn−T(n−T −δ) = 1 − δ\nn−T\nThus,\nρ= α−[2α−1]\n(\n1 − δ\nn−T\n)\n+ δ\nn\n= α−2α+ 1 +δ2α−1\nn−T + δ\nn\n= 1 −α+ δ\n(2α−1\nn−T + 1\nn\n)\n→0\nas α = T/n →1 and (δ/(n−T)) →0: e.g., by setting δ = ζ(n−T) and take ζ →0, with which\n(δ/(n−T)) = ζ →0.\n774\nC The Effect and Qualitative Analysis of Explanation\nBackbone AccCoT AccPAL Explanation? Ours Acc. Success\nRate\nCodex 64.4 71.5 w/o exp 74.7 (+3.2) 74.9\n64.4 71.5 w/ exp 74.6 (+3.1) 74.2\nChatGPT 80.8 79.2 w/o exp 81.8 (+1.0) 55.9\n80.8 79.2 w/ exp 82.6 (+1.8) 60.4\nGPT-4 94.6 94.0 w/o exp 95.5 (+0.9) 69.9\n94.6 94.0 w/ exp 95.6 (+1.0) 72.6\nTable 9: Accuracy and success selection rate with and without explanation on GSM8K.\nTo perform model selection, we provide explanations in the few-shot examples and ask the LLMs\nto generate explanations after making a choice, as we expect to improve the model’s selection ability\nby pointing out why the other choice is incorrect. To investigate the potential role of explanations in\nenhancing the model’s selection capability, we conduct experiments on GSM8K by excluding explanations\nfrom the answer.\nThe results in Table 9 reveal that for backbone LLMs with more powerful in-context learning abilities,\nsuch as Codex and GPT-4, the inclusion of explanations within the answer seems to have negligible\ninfluence on the model’s performance. In other words, Codex and GPT-4 have a relatively high success\nselection rate even when explanations are not given. However, for ChatGPT, which possesses relatively\nlow few-shot in-context learning capability, the integration of explanations yields a larger improvement\nin the success selection rate. This observation suggests that for backbone LLMs with weaker in-context\nlearning abilities, explanations can serve as a valuable tool to improve the success selection rate.\nQualitative studies on the explanations We are also interested in gaining an intuitive understanding of\nthe algorithm, so we conduct some qualitative analysis on the selection explanations. As displayed in\nTable 10, in the first example, both ChatGPT and GPT-4 can accurately identify that the PAL solution is\ncorrect. However, ChatGPT provides an incorrect explanation, while GPT-4 offers a valid one. In the\nsecond example, which is relatively simpler, both ChatGPT and GPT-4 successfully perform accurate\nmodel selection and provide valid explanations. In the first example, we can see that GPT-4 actually\npossesses exceptional reasoning capabilities and provides reliable explanations.\n775\nQuestion: Courtney attended a concert and reported that the audience was 48 in number. However, Kelly went to the\nsame concert and said that Courtney had made the mistake of overstating the number of people in attendance by 20%.\nIf Kelly was right, how many people really attended the concert?\n(A)\nAnswer:\nCourtney reported 48 people in attendance.\nIf this number is overstated by 20%, then we need to find\nthe actual number of people.\nFirst, we need to find 20% of 48.\n20% of 48 is (20/100) * 48 = 9.6.\nNow, we subtract this number from Courtney’s reported\nnumber to find the actual number of people.\n48 - 9.6 = 38.4.\nSince we cannot have a fraction of a person, we round the\nnumber to the nearest whole number.\nSo, the actual number of people in attendance was approxi-\nmately 38.\nSo the answer is 38.\n(B)\ndef solution ():\ncourtney_report = 48\noverstatement_percentage = 0.20\nactual_attendance =\ncourtney_report /(1 +\noverstatement_percentage )\nresult = int ( actual_attendance )\nreturn result\nChatGPT: (B) can correctly answer the math problem. Because (A) rounds up the result to the nearest whole number,\nwhich is not necessary.\nGPT-4: (B) can correctly answer the math problem. Because (A) incorrectly calculated the actual number of people\nby subtracting the overstatement percentage from the reported number instead of dividing it.\nQuestion: How much does it cost you for lunch today at Subway if you pay $40 for a foot-long fish sub and thrice as\nmuch for a six-inch cold-cut combo sub?\n(A)\nAnswer:\nIf the foot-long fish sub costs $40, then the six-inch\ncold-cut combo sub costs 3 * $40 = $120.\nSo the total cost for both subs is $40 + $120 = $160.\nTherefore, it costs $160 for lunch today at Subway.\n(B)\ndef solution ():\ncost_footlong_fish = 40\ncost_sixinch_coldcut =\ncost_footlong_fish / 3\ntotal_cost = cost_footlong_fish +\ncost_sixinch_coldcut\nresult = total_cost\nreturn result\nChatGPT: (A) can correctly answer the math problem. Because (B) calculates the cost of the six-inch cold-cut combo\nsub as one-third of the cost of the foot-long fish sub instead of three times the cost of the foot-long fish sub.\nGPT-4: (A) can correctly answer the math problem. Because (B) incorrectly calculates the cost of the six-inch cold-cut\ncombo sub as one-third of the foot-long fish sub instead of thrice as much.\nTable 10: In the first example, ChatGPT performs model selection correctly, but gives a wrong explanation, while\nGPT-4 gives a correct explanation. The second example shows where both ChatGPT and GPT-4 select correctly and\ngive the correct explanation.\n776\nD Examples of Prompts\nWe show examples of model selection prompts used on different tasks with different backbones. For\narithmetic reasoning tasks, we employ an 8-shot example for both Codex and Llama 2, and a 5-shot\nexample for ChatGPT and GPT-4. For the date understanding task, we use 6-shot examples for Codex,\nChatGPT, and GPT-4. Few-shot examples in the prompts are based on those in previous works (Wei et al.,\n2022b; Gao et al., 2022; Fu et al., 2022). We only show a few examples for each case. Full prompts can\nbe found in our code.\nMath Problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nQuestion: Which of the following two choices can correctly answer the math problem?\n(A)\ndef solution ():\nmoney_initial = 23\nbagels = 5\nbagel_cost = 3\nmoney_spent = bagels * bagel_cost\nmoney_left = money_initial -\nmoney_spent\nresult = money_left\nreturn result\n(B)\nAnswer:\nOlivia had 23 dollars.\n5 bagels for 3 dollars each will be 5 * 3 = 15 dollars.\nSo she has 23 - 5 = 18 dollars left.\nThe answer is 18.\nAnswer: (A)\nTable 11: An example of 8-shot model selection prompts used on 7 arithmetic datasets with Codex.\nDate Understanding Problem: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\nQuestion: Which of the following two choices can correctly answer the date understanding problem?\n(A)\ndef solution ():\n# If 2015 is coming in 36 hours ,\nthen today is 36 hours before .\ntoday = datetime (2015 , 1, 1) -\nrelativedelta ( hours =36)\n# One week from today ,\none_week_from_today = today +\nrelativedelta ( weeks =1)\n# The answer formatted with %m/%d\n/%Y is\nresult = one_week_from_today .\nstrftime ( '%m/%d/%Y ')\nreturn result\n(B)\nA:\nIf 2015 is coming in 36 hours, then it is coming in 2 days.\n2 days before 01/01/2015 is 12/30/2014, so today is\n12/30/2014.\nSo one week from today will be 01/06/2015.\nSo the answer is 01/06/2015.\nAnswer: (A)\nTable 12: An example of 6-shot model selection prompts used on Date Understanding task with Codex.\n777\nSystem: You are a helpful assistant that can identify the correct answer to the math problem.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other\nuses Python program to answer it. Either of them can correctly answer the math problem. You need to identify which\nchoice can correctly answer the math problem. Here is one example how to do it,\nMath problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n(A)\nAnswer:\nOlivia had 23 dollars.\n5 bagels for 3 dollars each will be 5 * 3 = 15 dollars.\nSo she has 23 - 15 = 8 dollars left.\nSo the answer is 8.\n(B)\ndef solution ():\nmoney_initial = 23\nbagels = 5\nbagel_cost = 3\nmoney_spent = bagels + bagel_cost\nmoney_left = money_initial -\nmoney_spent\nresult = money_left\nreturn result\nWhich of the above two choices can correctly answer the math problem?\n(A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of\nmultiplying them.\nNow it’s your turn. Here is another math problem and two choices.\nMath Problem: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How\nmany golf balls did he have at the end of wednesday?\n(A)\nAnswer:\nMichael started with 58 golf balls.\nThen after losing 23 on tuesday, he had 58 -23 = 35.\nAfter losing 2 more, he had 35 + 2 = 37 golf balls.\nSo the answer is 37.\n(B)\ndef solution ():\ngolf_balls_initial = 58\ngolf_balls_lost_tuesday = 23\ngolf_balls_lost_wednesday = 2\ngolf_balls_left =\ngolf_balls_initial - \\\ngolf_balls_lost_tuesday -\n\\ golf_balls_lost_wednesday\nresult = golf_balls_left\nreturn result\nWhich of the above two choices can correctly answer the math problem?\n(B) can correctly answer the math problem. Because (A) adds 2 more balls after losing 2 more on Wednesday instead\nof subtracting them.\nTable 13: Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with ChatGPT.\n778\nSystem: You are a helpful assistant that can identify the correct answer to the math problem.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other\nuses Python code to answer it. Either of them can correctly answer the math problem. You need to identify which\nchoice can correctly answer the math problem. Here is one example how to do it,\nMath problem: There were nine computers in the server room. Five more computers were installed each day, from\nmonday to thursday. How many computers are now in the server room?\n(A)\nAnswer:\nThere were originally 9 computers.\nFor each of 4 days from monday to thursday, 5 more\ncomputers were added.\nSo 5 * 4 = 20 computers were added.\nSo there are 9 + 20 = 29 computers now.\nSo the answer is 29.\n(B)\ndef solution ():\ncomputers_initial = 9\ncomputers_added = 5\ncomputers_total =\ncomputers_initial +\ncomputers_added\nresult = computers_total\nreturn result\nWhich of the above two choices can correctly answer the math problem?\n(A) can correctly answer the math problem. Because (B) missed the fact that computers were added each day from\nmonday to thursday.\nNow it’s your turn. Here is another math problem and two choices.\nMath Problem: A piece of square paper has a perimeter of 32 centimeters. Nicky’s dog, Rocky, tore off 1/4 of the\npaper. What is the area of the remaining paper?\n(A)\nAnswer:\nA square has 4 equal sides.\nThe perimeter of the square paper is 32 centimeters.\nSo each side of the square is 32 / 4 = 8 centimeters.\nThe area of the whole square paper is side * side = 8 * 8 =\n64 square centimeters.\nRocky tore off 1/4 of the paper.\nSo The area of the remaining paper is 1/4 * 64 = 16 square\ncentimeters.\nSo the answer is 16.\n(B)\ndef solution ():\nperimeter = 32\nfraction_torn = 1 / 4\narea_total = ( perimeter / 4) ** 2\narea_remaining = (1 -\nfraction_torn ) * area_total\nresult = area_remaining\nreturn result\nWhich of the above two choices can correctly answer the math problem?\n(B) can correctly answer the math problem. Because (A) incorrectly calculated the area of the torn-off portion instead\nof the remaining portion.\nTable 14: Two examples of 5-shot model selection prompts used on 7 arithmetic datasets with GPT-4.\n779\nSystem: You are a helpful assistant that can identify the correct answer to the date understanding problem.\nThere are two choices to the same date understanding problem. One uses natural language to answer the question,\nwhile the other uses Python program to answer it. Either of them can correctly answer the date understanding problem.\nYou need to identify which choice can correctly answer the problem. Here is one example how to do it,\nDate Understanding Problem: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n(A)\nAnswer:\nIf 2015 is coming in 36 hours, then it is coming in 2 days.\nAnd 2 days before 01/01/2015 is 12/30/2014, so today is\n12/30/2014.\nSo one week from today will be 01/05/2015.\nSo the answer is 01/05/2015.\n(B)\ndef solution ():\n# If 2015 is coming in 36 hours ,\nthen today is 36 hours before .\ntoday = datetime (2015 , 1, 1) +\nrelativedelta ( hours =36)\n# One week from today ,\none_week_from_today = today +\nrelativedelta ( weeks =1)\n# The answer formatted with %m/%d\n/%Y is\nresult = one_week_from_today .\nstrftime ( '%m/%d/%Y ')\nreturn result\nWhich of the above two choices can correctly answer the date understanding problem?\n(A) can correctly answer the date understanding problem. Because (B) incorrectly calculates the date 36 hours later\ninstead of 36 hours before.\nNow it’s your turn. Here is another date understanding problem and two choices.\nDate Understanding Problem: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the\ndate today in MM/DD/YYYY?\n(A)\nAnswer:\nIf the first day of 2019 was Tuesday, then 01/01/2019 was\na Tuesday.\nAnd today is the first monday, would be 5 days later.\nSo today is 01/06/2019.\nSo the answer is 01/06/2019.\n(B)\ndef solution ():\n# If the first day of 2019 is a\nTuesday , and today is the first\nMonday of 2019 , then today is\n6 days later .\ntoday = datetime (2019 , 1, 1) +\nrelativedelta ( days =6)\n# The answer formatted with %m/%d\n/%Y is\nresult = today . strftime ('%m/%d/%Y ')\nreturn result\nWhich of the above two choices can correctly answer the date understanding problem?\n(B) can correctly answer the problem. Because (A) missed the fact that there are 6 days between the first day of 2019\nand the first Monday of 2019.\nTable 15: Two examples of 6-shot model selection prompts used on Date Understanding with ChatGPT and GPT-4.\n780\nSystem: You are a helpful and brilliant assistant. You are a math expert who can identify the correct answer to the\nmath problem.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other\nuses Python program to answer it. *ONLY ONE* of them can correctly answer the math problem. You need to check\nthese two solutions carefully and identify which choice can correctly answer the math problem.\nHere is the math problem and two solutions to it.\nMath problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n(A)\ndef solution ():\nmoney_initial = 23\nbagels = 5\nbagel_cost = 3\nmoney_spent = bagels * bagel_cost\nmoney_left = money_initial -\nmoney_spent\nresult = money_left\nreturn result\nThe result is 8.\n(B)\nAnswer:\nOlivia had 23 dollars.\n5 bagels for 3 dollars each will be 5 + 3 = 8 dollars.\nSo she has 23 - 8 = 15 dollars left.\nThe answer is 15.\nWhich of the above two choices can correctly answer the math problem? (A) or (B)?\n(A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of\nmultiplying them.\nTable 16: One example of 8-shot model selection prompts on GSM8K with Llama 2 7B as the model selector.\nSystem: You are a helpful and brilliant assistant. You are a math expert who can identify the correct answer to the\nmath problem.\nThere are two choices to the same math problem. One uses natural language to answer the question, while the other\nuses Python program to answer it. *ONLY ONE* of them can correctly answer the math problem. You need to check\nthese two solutions carefully and identify which choice can correctly answer the math problem.\nHere is the math problem and two solutions to it.\nMath problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n(A)\nAnswer:\nOlivia had 23 dollars.\n5 bagels for 3 dollars each will be 5 * 3 = 15 dollars.\nShe has 23 - 15 = 8 dollars left..\nThe answer is 8.\n(B)\ndef solution ():\nmoney_initial = 23\nbagels = 5\nbagel_cost = 3\nmoney_spent = bagels + bagel_cost\nmoney_left = money_initial -\nmoney_spent\nresult = money_left\nreturn result\nThe result is 15.\nWhich of the above two choices can correctly answer the math problem? (A) or (B)?\n(A) can correctly answer the math problem. Because (B) adds the number of bagels to the cost of each bagel instead of\nmultiplying them.\nTable 17: One example of 8-shot model selection prompts on GSM8K with Llama 2 13B as the model selector.\n781\nE Examples of Failure Cases Comparison\nIn Section 2.1, we divide the error reasons of CoT and PAL on GSM8K into 5 categories. Detailed\nexamples of 4 primary categories are presented in the following tables.\nQuestion: It costs $194 per meter to repave a street. Monica’s street is 150 meters long. How much more does it cost\nto repave Lewis’ street, which is 490 meters long?\nAnswer:\nThe cost to repave Monica’s street is 194 * 150 = $29100.\nThe cost per meter is the same for Lewis’ street, so the cost\nto repave it is 194 * 490 = $94860.\nThe difference in cost is $94860 - $29100 = $65760.\nSo it costs $65760 more to repave Lewis’ street. /enc-37\ndef solution ():\nmonica_street_length = 150\nlewis_street_length = 490\ncost_per_meter = 194\nmonica_street_cost =\nmonica_street_length *\ncost_per_meter\nlewis_street_cost =\nlewis_street_length *\ncost_per_meter\ncost_difference =\nlewis_street_cost -\nmonica_street_cost\nresult = cost_difference\nreturn result\n>65960.0 /enc-33\nTable 18: A CoT failure case due to Computation Precision. PAL can easily solve this question with the help of an\nexternal Python interpreter. >65960.0 is obtained with the Python interpreter.\nQuestion:Kim has started his own housekeeping business and is calculating how much profit he will make from his\nclients. He already has 3 clients, but is talking to another 5 potential clients and feels confident enough to include\nthem in his calculations. Each client’s home will need 2 bottles of bleach and a pack of cloths to clean. Bottles of\nbleach will cost $2 each and packs of cloths will cost $5 each. These are his only expenses. He calculates that his total\nincome each week will be $92. Profit is the difference between total income and total expenses, so how much profit, in\ndollars, will Lucas make each week?\nAnswer:\nKim has 3 clients and is talking to 5 more potential clients,\nso he will have 3 + 5 = 8 clients in total.\nEach client needs 2 bottles of bleach, so Kim will need 2 *\n8 = 16 bottles of bleach each week.\nEach client also needs a pack of cloths, so Kim will need 8\n* 5 = 40 packs of cloths each week.\nThe cost of the bleach will be 16 * 2 = $32.\nThe cost of the cloths will be 40 * 5 = $200.\nSo Kim’s total expenses will be 32 + 200 = $232.\nKim’s total income each week will be $92.\nSo his profit will be 92 - 232 = -$140.\nTherefore, Kim will lose $140 each week. /enc-37\ndef solution ():\nnum_clients = 8\nbleach_cost = 2\ncloths_cost = 5\ntotal_expenses = num_clients * (2\n* bleach_cost + cloths_cost )\ntotal_income = 92\nprofit = total_income -\ntotal_expenses\nresult = profit\nreturn result\n>20.0 /enc-33\nTable 19: A CoT failure case due to Logical Decomposition. PAL breaks down the problem into step-by-step\nPython code, and utilizes variables to map logical relationships.\n782\nQuestion:Seth is twice as old as Brooke. In 2 years, the sum of their ages will be 28. How old is Seth?\nAnswer:\nLet’s assume Brooke’s age to be x.\nThen Seth’s age would be 2x (as he is twice as old as\nBrooke).\nIn 2 years, Brooke’s age would be x+2 and Seth’s age would\nbe 2x+2.\nThe sum of their ages in 2 years would be (x+2) + (2x+2) =\n3x+4.\nWe know that the sum of their ages in 2 years will be 28.\nSo, 3x+4 = 28.\nSolving for x, we get x = 8.\nTherefore, Brooke’s age is 8 and Seth’s age is 2x = 16.\nSo the answer is 16. /enc-33\ndef solution ():\nsum_of_ages_in_two_years = 28\nage_difference = 2 * 2 # Seth is\ntwice as old as Brooke\nage_of_seth_in_two_years = (\nsum_of_ages_in_two_years +\nage_difference ) / 2\nage_of_seth =\nage_of_seth_in_two_years - 2\nresult = age_of_seth\nreturn result\n>14.0 /enc-37\nTable 20: A PAL failure case due to Problem-Solving Flexibility. CoT employs both forward and backward\nreasoning paradigms and solves this problem correctly.\nQuestion: Gloria is shoe shopping when she comes across a pair of boots that fit her shoe budget. However, she has to\nchoose between the boots and two pairs of high heels that together cost five dollars less than the boots. If one pair of\nheels costs $33 and the other costs twice as much, how many dollars are the boots?\nAnswer:\nLet’s start by finding the total cost of the two pairs of high\nheels.\nOne pair of heels costs $33, and the other pair costs twice\nas much, which is $33 x 2 = $66.\nSo the total cost of the two pairs of high heels is $33 + $66\n= $99.\nWe also know that the cost of the boots is $5 more than the\ncost of the two pairs of high heels.\nTherefore, the cost of the boots is $99 + $5 = $104.\nSo the answer is $104. /enc-33\ndef solution ():\nheels1 = 33\nheels2 = 2 * heels1\nheels_total = heels1 + heels2\nboots = heels_total - 5\nresult = boots\nreturn result\n>94.0 /enc-37\nTable 21: A PAL failure case due toSemantic Understanding. PAL fails to understand the crucial information ‘five\ndollars less than boosts’, whereas CoT exhibits a more advanced semantic understanding capability.\n783",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9264523983001709
    },
    {
      "name": "Computer science",
      "score": 0.8218059539794922
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6212911605834961
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.5813952088356018
    },
    {
      "name": "Language model",
      "score": 0.5781465172767639
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5630140900611877
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5409955978393555
    },
    {
      "name": "Machine learning",
      "score": 0.46752333641052246
    },
    {
      "name": "Natural language",
      "score": 0.45714643597602844
    },
    {
      "name": "Computation",
      "score": 0.4531084895133972
    },
    {
      "name": "Natural language processing",
      "score": 0.399557888507843
    },
    {
      "name": "Programming language",
      "score": 0.3188241124153137
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}