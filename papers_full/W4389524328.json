{
    "title": "On Bilingual Lexicon Induction with Large Language Models",
    "url": "https://openalex.org/W4389524328",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2508716396",
            "name": "Yaoyiran Li",
            "affiliations": [
                "University of Cambridge",
                "Language Science (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A1927037681",
            "name": "Anna Korhonen",
            "affiliations": [
                "Language Science (South Korea)",
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A1969142033",
            "name": "Ivan Vulić",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4230872509",
        "https://openalex.org/W4224247062",
        "https://openalex.org/W2950797315",
        "https://openalex.org/W4288090629",
        "https://openalex.org/W4385574002",
        "https://openalex.org/W2294774419",
        "https://openalex.org/W4385565879",
        "https://openalex.org/W4385572225",
        "https://openalex.org/W2970037872",
        "https://openalex.org/W4385565080",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W2126725946",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3033843238",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2964266061",
        "https://openalex.org/W3109407175",
        "https://openalex.org/W4386566630",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W3099178230",
        "https://openalex.org/W2962824887",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4221151632",
        "https://openalex.org/W4299579390",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3104180227",
        "https://openalex.org/W3034623056",
        "https://openalex.org/W4226395792",
        "https://openalex.org/W4385572801",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W2963472233",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3166938950",
        "https://openalex.org/W3104723404",
        "https://openalex.org/W4385570133",
        "https://openalex.org/W3175035375",
        "https://openalex.org/W3155266314",
        "https://openalex.org/W3171465070",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W3015504467"
    ],
    "abstract": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9577–9599\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOn Bilingual Lexicon Induction with Large Language Models\nYaoyiran Li Anna Korhonen Ivan Vuli ´c\nLanguage Technology Lab, TAL, University of Cambridge\n{yl711,alk23,iv250}@cam.ac.uk\nAbstract\nBilingual Lexicon Induction (BLI) is a core task\nin multilingual NLP that still, to a large extent,\nrelies on calculating cross-lingual word repre-\nsentations. Inspired by the global paradigm\nshift in NLP towards Large Language Models\n(LLMs), we examine the potential of the lat-\nest generation of LLMs for the development\nof bilingual lexicons. We ask the following re-\nsearch question: Is it possible to prompt and\nfine-tune multilingual LLMs (mLLMs) for BLI,\nand how does this approach compare against\nand complement current BLI approaches? To\nthis end, we systematically study 1) zero-shot\nprompting for unsupervised BLI and 2) few-\nshot in-context prompting with a set of seed\ntranslation pairs, both without any LLM fine-\ntuning, as well as 3) standard BLI-oriented fine-\ntuning of smaller LLMs. We experiment with\n18 open-source text-to-text mLLMs of differ-\nent sizes (from 0.3B to 13B parameters) on two\nstandard BLI benchmarks covering a range of\ntypologically diverse languages. Our work is\nthe first to demonstrate strong BLI capabilities\nof text-to-text mLLMs. The results reveal that\nfew-shot prompting with in-context examples\nfrom nearest neighbours achieves the best per-\nformance, establishing new state-of-the-art BLI\nscores for many language pairs. We also con-\nduct a series of in-depth analyses and ablation\nstudies, providing more insights on BLI with\n(m)LLMs, also along with their limitations.\n1 Introduction and Motivation\nBilingual Lexicon Induction (BLI), also known as\nword translation, is a fundamental research topic\nin multilingual NLP that aims to bridge the lexi-\ncal gap between languages (Ruder et al., 2019). It\nhas a wide range of applications such as machine\ntranslation (Artetxe et al., 2018b; Marchisio et al.,\n2020; Chronopoulou et al., 2021) and cross-lingual\ntransfer learning, especially for low-resource lan-\nguages (Sun et al., 2021; Zhou et al., 2021; Wang\net al., 2022). Over the past decade, state-of-the-art\n(SotA) BLI approaches have been predominantly\nsupported by learning a cross-lingual word embed-\nding (CLWE) space, with which BLI is tackled via\nnearest neighbour retrieval (Artetxe et al., 2018a;\nHeyman et al., 2019; Peng et al., 2021; Li et al.,\n2022a; Marchisio et al., 2022, inter alia).\nMeanwhile, autoregressive text-to-text large lan-\nguage models (LLMs) have emerged as the corner-\nstone of cutting-edge NLP research (Raffel et al.,\n2020; Brown et al., 2020; Ouyang et al., 2022;\nChowdhery et al., 2022). For example, multilin-\ngual LLMs (mLLMs) have shown (sentence-level)\nmachine translation capabilities (Vilar et al., 2022;\nBriakou et al., 2023), although they have not been\npretrained for machine translation in a supervised\nmanner. Motivated by the recent remarkable suc-\ncess of (m)LLMs, in this work we investigate1) the\npotential of prompting and fine-tuning of mLLMs\nfor BLI and 2) how their capabilities compare\nagainst and complement current BLI approaches.\nWe focus on how to expose word-level bilingual\nknowledge and elicit word translations from multi-\nlingual LLMs. To our best knowledge, we are the\nfirst to leverage autoregressive mLLMs for BLI.1\nWe systematically study zero-shot and few-\nshot prompting for BLI with off-the-shelf\nencoder-decoder and decoder-only autoregressive\nmLLMs (Radford et al., 2019; Raffel et al., 2020;\nBrown et al., 2020), respectively. In the few-shot\nscenario, we propose to incorporate in-context ex-\namples from nearest neighbours into the prompts\nto boost the BLI performance. In order to guide the\nmLLMs’ generation, we hand-craft ‘mask-filling-\nstyle’ and ‘ GPT-style’ templates catering to the\ncharacteristics of different LLMs and conduct ex-\ntensive template search for BLI. In addition to pro-\n1We point out that the work of Li et al. (2022a) leverages\nonly the encoder part of mT5 (the decoder is dropped) in one\nof their experiments to extract CLWEs for BLI. In contrast, our\nexperiments with mT5 utilise its full encoder-decoder structure\nand generate the target words autoregressively, completely\ndifferent from CLWE-based approaches.\n9577\nviding a complete and effective pipeline for BLI\nvia prompting off-the-shelf mLLMs, we also in-\nvestigate BLI-oriented fine-tuning with the LLMs’\nown pretraining objectives, aiming at specialising\nmLLMs into ‘few-shot word translators’.\nWe conduct extensive experiments on two stan-\ndard BLI benchmarks, XLING (Glavaš et al., 2019)\nand PanLex-BLI (Vuli ´c et al., 2019), investigat-\ning the word translation capabilities of off-the-\nshelf mLLMs (we adopt 18 models from 5 LLM\nfamilies) in various BLI setups. Our compre-\nhensive comparisons between mLLMs confirm,\nas expected, that 1) different LLM families dis-\nplay varying word translation capabilities and 2)\nstronger BLI performance tends to be associated\nwith larger model sizes. To demonstrate the effec-\ntiveness of our prompt-based approach, we bench-\nmark our method against two SotA CLWE-based\nbaselines. Notably, our approach with LLaMA13B\noutperforms the CLWE-based SotA on the XLING\ndataset by a considerable margin, establishing new\nSotA results on many language pairs in all BLI se-\ntups. Meanwhile, we also identify two limitations\nof BLI with mLLMs: 1) they are less competitive\non the PanLex-BLI benchmark for lower-resource\nlanguages; 2) CLWE-based approaches usually sup-\nport more languages than mLLMs. Finally, we run\na series of insightful ablations and discuss the use-\nfulness of BLI-oriented fine-tuning. In short, our\nwork validates the BLI capabilities of mLLMs and\nproposes new methodology for BLI. We hope that\nthe combination of our comprehensive analyses\nand discussions, including on limitations, will pave\nthe way for the development of stronger BLI sys-\ntems in the future. Our code is publicly available\nat github.com/cambridgeltl/prompt4bli.\n2 Related Work\nBilingual Lexicon Induction. Over the past\ndecade, predominant BLI approaches have relied\non the calculation of cross-lingual word embed-\ndings (CLWEs) where, in the most popular BLI\nvariant, two transformation functions are learned\nto respectively map source and target monolingual\nstatic word embedding spaces into a shared cross-\nlingual space (Xing et al., 2015; Lample et al.,\n2018; Joulin et al., 2018; Artetxe et al., 2018a;\nAlvarez-Melis and Jaakkola, 2018; Patra et al.,\n2019; Mohiuddin et al., 2020; Glavaš and Vuli ´c,\n2020; Peng et al., 2021; Li et al., 2022a; Marchisio\net al., 2022). Then, relying on the learned CLWE\nspace, BLI has been conducted via nearest neigh-\nbour retrieval. A detailed overview of different BLI\nprinciples can be found, e.g., in the work of Ruder\net al. (2019).\nMore recently, researchers have attempted BLI\nby leveraging encoder-only multilingual masked\nlanguage models (mMLMs) such as mBERT (De-\nvlin et al., 2019) and XLM-R (Conneau et al., 2020)\nwhose neural architecture consists of only Trans-\nformer encoders (Vaswani et al., 2017). Gonen\net al. (2020) prompt mBERT with templates where\nthe target word is replaced with a ‘<mask>’ to-\nken, and the language modelling head of mBERT\noutputs a subword token to fill the mask. This\nmethod is theoretically flawed because it cannot\naddress the cases where the target word comprises\ntwo or more subword tokens. Therefore, Gonen\net al. (2020) only evaluate BLI on a small set of\n‘toy’ examples rather than standard BLI datasets. In\nterms of performance, this method lags far behind\ntraditional BLI approaches. A more successful\nway of leveraging mMLMs is to extract decontex-\ntualised word representations from them (Zhang\net al., 2021). The strongest CLWEs for BLI so far\nare learned via a two-stage contrastive approach\ncombining both static (e.g., fastText) and mMLM-\nextracted features (Li et al., 2022a).2\nText-to-Text LLMs. Autoregressive LLMs have\nestablished new state-of-the-art results on many\nNLP tasks. The prominent model groups include\n1) encoder-decoder LLMs such as BART (Lewis\net al., 2020) and T5 (Raffel et al., 2020); 2) Ope-\nnAI’s decoder-only GPT series such as GPT-2 (Rad-\nford et al., 2019), GPT-3 (Brown et al., 2020), and\nInstructGPT (Ouyang et al., 2022); 3) other GPT-\nlike LLMs with specific improvements such as\nChinchilla (Hoffmann et al., 2022), PaLM (Chowd-\nhery et al., 2022), and LLaMA LLM series (Tou-\nvron et al., 2023).\nOur work adopts five families of open-source\ntext-to-text multilingual LLMs for BLI, including\nmT5 (Xue et al., 2021), mT0 (Muennighoff et al.,\n2022), XGLM (Lin et al., 2022), mGPT (Shliazhko\net al., 2022), and LLaMA (Touvron et al., 2023).\nWe introduce each of these in more detail in §3.1.\nUnlike the encoder-only MLMs, text-to-text LLMs\nare theoretically capable of generating words con-\nsisting of arbitrary numbers of subword tokens.\n2Using mMLMs alone still underperforms purely fastText-\nbased methods since mMLMs are contextualised encoders\npretrained for sentence-level tasks (Li et al., 2022a).\n9578\n3 Methodology\nBLI Task: Preliminaries and Terminology.As-\nsuming a bilingual scenario with a source language\nLx and a target language Ly with their respec-\ntive vocabularies denoted as X and Y, the BLI\ntask is typically formulated as a standard informa-\ntion retrieval task (Gaussier et al., 2004; Glavaš\net al., 2019). The goal is to rank the words from Y\nwith respect to their similarity to the input source\nword wx. The vocabulary size for each language\nis typically set to 200k (Li et al., 2022a), covering\nthe most frequent 200k word types in each lan-\nguage. A bilingual lexicon then comprises a set of\none-to-one source and target word translation pairs\n(Mikolov et al., 2013), and we denote a word pair\nas π=(wx,wy) where wx ∈X,wy ∈Y.\nWe assume a set DS of N available seed trans-\nlation pairs, constituting the so-called seed dictio-\nnary, which are used as the training set. Depending\non the number of training pairs, the task is usually\nreferred to as supervised BLI (typically, N ≥5K),\nsemi-supervised BLI (e.g., 0 <N ≤1K), and un-\nsupervised BLI (N = 0) in the literature (Artetxe\net al., 2018a; Zhao et al., 2020; Li et al., 2022a).\nFor convenience, we also refer to the unsupervised\nsetup as zero-shot BLI (N = 0) and denote the\nsetup with a handful of seed translation pairs as\nfew-shot BLI (N >0), corresponding to how we\nprompt mLLMs for BLI (we describe zero-shot and\nfew-shot prompts for BLI later in §3). A test set\nDT , where DS ∩DT = ∅, is used for evaluation.\nIn some cases, a source word may have more\nthan one ground-truth translation (i.e., there exist\ntwo or more word pairs in a BLI dictionary that\nshare the same source word). Following previous\nwork (Lample et al., 2018; Glavaš et al., 2019; Li\net al., 2022a), we consider a prediction correct as\nlong as it is any of the ground-truth translations.\nThe BLI scores are reported based on the standard\nPrecision@K (P@K) BLI measure, where K de-\nnotes the length of the ranked list.\n3.1 Prompting Multilingual LLMs for BLI\nThis study employs five families of mainstream\nmultilingual text-to-text LLMs (mLLMs): mT5,\nmT0, XGLM, mGPT, and LLaMA.3 Based on their\nmodel structures, we group these models into two\n3We also experimented with mBART (Liu et al., 2020).\nHowever, we do not report the results with mBART since its\nBLI performance proved inferior in our preliminary investiga-\ntions.\ncategories; in what follows, we briefly introduce\neach of them and showcase some simple templates\nused for ‘BLI-prompting’ the LLMs.\nThe first category includes mT5 and mT0, two\nencoder-decoder LLM families that leverage the\nfull Transformer architecture (Vaswani et al., 2017).\nEach model family comes in five different sizes,\nand we evaluate all these ten models.\n• mT5 (Xue et al., 2021) is pretrained on the mC4\ndataset covering 101 languages. The LLM lever-\nages a span-corruption objective that tries to re-\nconstruct consecutive spans of dropped-out tokens\nreplaced with special mask tokens.\n• mT0 (Muennighoff et al., 2022) is a multitask-\nfinetuned mLLM based on instruction fine-tuning\nfrom the original mT5 model. The fine-tuning is\nconducted with English prompts on mT0’s xP3\ndataset spanning 46 languages.4\nFor these two encoder-decoder style mLLMs, we\naim to derive prompts such that the first word of the\noutput sequence serves as its guess for wy. Cater-\ning to its span-corruption objective, for mT5 we\npropose to design mask-filling-style English tem-\nplates where ‘<mask>’ tokens are used as place-\nholders for the target words. Here is an example\ntemplate: ‘The Lx word wx in Ly is <mask>. ’,\nwhere Lx, Ly, and wx are placeholders for the\nsource language, target language, and the input\nsource word, respectively.5 When a prompt based\non this template is fed into mT5, its decoder will\nthen output a sequence to fill the mask. Since mT0\nis based on mT5, we found that mask-filling-style\nprompts are also applicable to mT0. However, un-\nlike for mT5, the instruction-tuned mT0 fits tem-\nplates without the ‘<mask>’ token.6 For simplic-\nity, we will denote all such templates without any\n‘<mask>’ tokens as ‘GPT-style templates’.\nThe second model category comprises XGLM,\nmGPT, and LLaMA as three decoder-only LLMs\npretrained with causal LM losses. Our experiments\ninvolve five XGLM and two LLaMA models whose\n4Among all the LLMs covered in our work, mT0 is the\nonly one trained for sentence-level translation tasks (machine\ntranslation is one of the tasks during its multitask fine-tuning).\nHowever, our experimental results reported later indicate that\nthis does not benefit BLI in our prompting setups.\n5The ‘<mask>’ token for mT5 and mT0 is actually ‘<ex-\ntra_id_0>’. Therefore, an example of an actual prompt would\nbe ‘The German word gebouw in French is <extra_id_0>. ’.\n6For instance, ‘The Lx word wx in Ly is’ may prompt\nmT0 to output wy to complete the input sentence, and ‘How\ndo you say wx in Ly?’ would prompt mT0 to generate wy to\nanswer the question.\n9579\nmodel sizes are no larger than 13B parameters,\nwhile mGPT only releases one model of size 1.4B.\nUnlike encoder-decoder LLMs for conditional gen-\neration, the decoder-only causal LLMs first repeat\nthe input sequence in their output, and we construct\nprompts that induce LLMs to produce wy immedi-\nately after the repeated input sequence.\n• XGLM (Lin et al., 2022) offers multilingual\nLLMs similar to GPT-3 (Brown et al., 2020) and is\nreported to outperform GPT-3 of comparable size\nin a series of tasks. The work builds a CC100-XL\ndataset based on Conneau et al. (2020) and Wenzek\net al. (2020), and XGLM is pretrained with a subset\nof it covering 30 languages.\n• mGPT (Shliazhko et al., 2022) reproduces the\nGPT-3 structure and is trained on 60 languages\nusing Wikipedia and C4 data (Raffel et al., 2020).\n• LLaMA (Touvron et al., 2023) is a recently re-\nleased SotA LLM family trained on trillions of\ntokens exclusively from publicly available datasets;\nit supports 20 languages. LLaMA also features its\nefficient implementation, and it adopts a series of\nrecent improvements on normalisation, activation\nfunctions, and positional embeddings.\nOur decoder-only LLMs solely leverage GPT-\nstyle prompts introduced above for mT0, since their\ntokenisers usually do not support ‘<mask>’ tokens.\n3.2 Retrieval-Augmented In-Context\nLearning\nIn §3.1, we presented some simple zero-shot\nprompts (i.e., prompts without in-context exam-\nples) for BLI. However, recent work highlights\nthe few-shot capabilities of modern LLMs (Brown\net al., 2020). Therefore, we also investigate few-\nshot templates for improved BLI performance.7\nWe propose to retrieve the nearest neighbours of\na source word which we use to construct in-context\nsamples to boost BLI performance. More specifi-\ncally, given DS and an input source word wx, we\nextract nword pairs (wx\ni ,wy\ni ) ∈DS,1 ≤i ≤n,\nsuch that wx\ni ,1 ≤ i ≤ n are n nearest neigh-\nbours of wx in the auxiliary static monolingual\nword embedding space of X. This auxiliary space\nis based on pretrained fastText word embeddings\n(Bojanowski et al., 2017)8 and we use the cosine\n7Note again that few-shot in-context learning does not\nrequire any actual fine-tuning of LLMs. The word ‘learning’\nhere only refers to inserting in-context examples into the input\nprompt sequence.\n8See Appendix B for more details about the fastText WEs\nused in our work.\nsimilarity measure for the retrieval. 9 We again\ndesign mask-filling-style and GPT-style few-shot\ntemplates for the mLLMs, as discussed in §3.1.\nSimilar to zero-shot prompts, for few-shot prompts\nwe also extract the first word after removing spe-\ncial tokens (e.g., start-of-sentence, padding, and\n‘<mask>’ tokens) and repeated input sequence (for\ndecoder-only models) as the prediction of wy.\n3.3 Template Design and BLI Inference\nTemplate Design. We hand-craft in total 102 En-\nglish zero-shot and few-shot templates, respectively\nlisted in Tables 10 and 11 of Appendix C. A small\nset of basic templates is fully manually designed,\nand additional variants are then created by modi-\nfying or replacing the punctuation (see the tables).\nFor each LLM, we search for its best zero-shot\ntemplate and best few-shot template on a randomly\nchosen language pair (German, French) and fix the\ntemplate choices for experiments on all other lan-\nguage pairs. The best template choices for each\nLLM are provided in Table 12 (Appendix C).\nBLI Inference. At inference, we adopt beam\nsearch for both encoder-decoder and decoder-only\nLLMs and make the generator return the final beam\nranked by their sequence scores. For each input\nprompt corresponding to wx, we iterate through the\nreturned set of sequences, and for each sequence\nwe extract the word after removing any redundant\nprefix content, as described in §3.2. The first word\nextracted that appears in the target vocabulary is\nreturned as our prediction of wy.\n3.4 BLI-Oriented Fine-Tuning\nThis work predominantly focuses on ‘learningless’\nexperiments based on zero-shot and few-shot in-\ncontext setups with off-the-shelf mLLMs for BLI\nwithout any fine-tuning. As a side experiment, we\nalso aim to fine-tune smaller-scale mLLMs, making\nthem specialise into few-shot word translators with\nour few-shot prompts as input. Our training set is\nstill DS, but we now exclude retrieving an input\nwx itself as an in-context example. We combine\nthe DS of each language pair with which we fine-\ntune encoder-decoder mLLMs with mT5’s span-\ncorruption loss and fine-tune decoder-only LLMs\nwith the standard causal LM objective.\n9For the rare cases where an in-context source word re-\ntrieved may have more than one translation in DS, we only\nkeep the target word with the highest word frequency in fast-\nText’s training data.\n9580\n4 Experimental Setup\nTraining and Evaluation Data. Our experiments\nadopt two standard and publicly available BLI\ndatasets, also used in a body of very recent BLI\nresearch (Vuli´c et al., 2020; Sachidananda et al.,\n2021; Aboagye et al., 2022; Li et al., 2022a,b; Vuli´c\net al., 2023). 1) XLING (Glavaš et al., 2019) pro-\nvides BLI dictionaries covering 8 languages and\n56 BLI directions. Among these 8 languages, 5\nare supported by all of our mLLMs: English (EN),\nFrench (FR), German (DE), Italian (IT), and Rus-\nsian (RU). Therefore, §5 mainly focuses on and\nreports results on all the 20 = 5×4 BLI direc-\ntions for the 5 languages.10 For each language\npair, XLING provides a test set DT of 2K transla-\ntion pairs. It also provides training sets DS of 5K\nand 1K translation pairs, where the former is the\nsuperset of the latter. For brevity, we denote the\ncases |DS|= 5K, |DS|= 1K, and |DS|= 0as\nthe 5K setup, 1K setup, and unsupervised setup,\nrespectively.11 2) PanLex-BLI (Vuli´c et al., 2019)\noffers BLI lexicons spanning 15 lower-resource\nlanguages and all 210 BLI directions. We select\nthree languages that are supported by most of our\nmLLMs: Bulgarian (BG), Catalan (CA), and Hun-\ngarian (HU). The test set size of PanLex-BLI is also\n2K; under the lower-resource assumption, we only\nfocus on unsupervised and 1K BLI setups.\nMain Experiments. In our main experiments, we\nprompt 18 off-the-shelf models from 5 mLLM fam-\nilies mentioned in §3.112 for BLI without any fine-\ntuning13 and systematically evaluate their BLI per-\nformance in three different BLI setups on XLING\nand PanLex-BLI datasets introduced above. In\n5K and 1K setups, 5-shot in-context learning is\nadopted for our mLLMs, while in the unsupervised\nsetup, zero-shot prompts are used. We compare\nthe BLI scores between different mLLMs from the\nperspectives of LLM family and model size, and\nwe also benchmark their performance against two\nSotA CLWE-based baselines, introduced later. Se-\nlected results are summarised in §5.1 while full and\n10However, in the appendix, we also provide the results\nwith the remaining 3 languages: Croatian (HR), Finnish (FI),\nand Turkish (TR).\n11Related work often also refers to the 5K and 1K cases as\nsupervised and semi-supervised BLI setups, respectively (Li\net al., 2022a).\n12A summary concerning the detailed information of each\nmLLM is available in Appendix B.\n13Experiments on BLI-oriented fine-tuning for a selection\nof mLLMs are discussed later.\ndetailed BLI scores are reported in Appendix D.\nSide Experiments. We conduct a series of addi-\ntional experiments to further understand the BLI\ncapabilities of mLLMs. 1) We investigate how the\nBLI performance is related to the number of in-\ncontext examples (5K and 1K setups). 2) As an ab-\nlation study, we validate the usefulness of (our pro-\nposed) in-context samples extracted from nearest\nneighbours by comparing with randomly sampled\nin-context examples. 3) Finally, we fine-tune some\nof our relatively smaller-scale LLMs, including\nmT5base, mT5large, XGLM564M, and XGLM1.7B on\nour 5-shot templated BLI data (XLING) and fur-\nther study the effectiveness of our BLI-oriented\nfine-tuning (5K and 1K setups). The training set\nincludes all XLING language pairs, where the 5K\nand 1K setups have 271,754 and 55,228 training\ninstances respectively.\nHyperparameters. We first introduce our hyper-\nparameters for BLI inference. In our main experi-\nments, we adoptn= 514 while in side experiments\nwe further investigate and compare using different\nnumbers of in-context examples n. Concerning the\ngeneration of output sequences, we adopt a beam\nsize of 5 for all LLMs, and the maximum sequence\nlength is 5 for encoder-decoder models and 5 plus\nthe input sequence length for decoder-only mod-\nels which first repeat the input sequence before\ngenerating new content. As for encoder-decoder\nLLMs, we use an evaluation batch size of 100 for\nsmaller models and 8 for larger models as listed\nin Table 8 (Appendix B). Since the pretraining of\ndecoder-only LLMs usually does not see padding\ntokens, we adopt a batch size of 1.15 Following\nprior work (Li et al., 2022a,b), all our hyperparam-\neters are tuned on (German, French), a randomly\nselected language pair.\nFor ‘BLI-oriented’ fine-tuning, we use the\nXLING data combining all language pairs, and\nthe batch size is 16 for XGLM 1.7B and 32\nfor mT5 base,large and XGLM 564M. We use\nAdamW (Loshchilov and Hutter, 2019) with be-\ntas = (0.9,0.98) and a weight decay of 0.1. The\nlearning rate is 2e-6 for mT5base, 1e-6 for mT5large,\nand 5e-8 for XGLM564M; concerning XGLM1.7B,\n5e-9 is adopted for the 5K setup and 2e-8 for the\n1K setup. All the hyperparameters are tuned on\n14Unless otherwise stated, we report 5-shot BLI results\nthroughout our experiments.\n15We found that a larger batch size may cause a drop in the\nBLI performance.\n9581\nthe same randomly chosen language pair (German,\nFrench). Each LLM is fine-tuned for at most 20\nepochs capped at 12 hours on 1×80GB A100 GPU.\nBaselines. We adopt the following two SotA\nCLWE-based approaches as our baselines; both\nare open-source. We follow their original sug-\ngested hyperparameter choices respectively for 5K\n(supervised), 1K (semi-supervised), and unsuper-\nvised BLI setups, and we re-verify that the hyper-\nparameters recommended are (near-)optimal. The\nCross-domain Similarity Local Scaling (CSLS) re-\ntrieval (Lample et al., 2018) is adopted as recom-\nmended in the baselines.\n• VECMAP (Artetxe et al., 2018a) is one of the\nmost representative BLI approaches based on static\nCLWEs. It induces fastText-based CLWEs in vari-\nous BLI supervision setups, and is notable for its\neffective self-learning mechanism, especially in\nweakly supervised and unsupervised BLI setups.\n• CONTRASTIVE BLI (Li et al., 2022a) refines\nCLWEs with a two-stage contrastive learning pro-\ncedure and reports the currently highest CLWE-\nbased BLI scores on XLING and PanLex-BLI\nin 5K and 1K BLI setups. We adopt its\nstrongest CLWEs derived with both fastText and\nmBERT (Devlin et al., 2019). CONTRASTIVE BLI\ndoes not support unsupervised BLI.\nBLI Evaluation. Following previous work, we\nreport the standard Precision@1 (P@1) scores both\nfor our methods and for baseline methods.16\n5 Results and Discussion\n5.1 Main Results\nComparison between mLLMs. We compare the\naverage BLI scores on 20 XLING BLI directions\nderived from all our 18 models from 5 LLM fam-\nilies in Figure 1. In all ( 5K, 1K, and zero-shot)\nBLI setups, the same general trends are observed.\n1) As expected, within the same mLLM family,\nlarger models usually present stronger BLI ca-\npabilities, although exceptional cases exist (e.g.,\nXGLM7.5B underperforms XGLM 4.5B). 2) For\nencoder-decoder models, we find that mT5 outper-\nforms mT0, showing that the instruction fine-tuning\nof mT0 does not benefit BLI in our experimental\nsetups; 3) LLaMA models achieve the strongest\nBLI performance among our 5 model families.\n16P@1 is the most authoritative metric for BLI. Other mea-\nsures such as P@5 and Mean Reciprocal Rank (MRR) show\nsimilar trends (Lample et al., 2018; Li et al., 2022a).\n[5K Setup]VECMAP CONTRASTIVEBLImT5xxlmT0xxlXGLM4.5B mGPTLLaMA13B\nDE→∗ 47.65 54.02 49.1 35.73 48.43 37.4456.44∗→DE 47.34 53.64 46.37 32.66 46.56 32.4754.78EN→∗ 53.54 60.26 59.36 44.2 61.2 45.69 69.0∗→EN 57.38 60.97 57.34 41.48 56.08 45.1562.35FR→∗ 53.36 58.51 53.46 39.7 54.31 44.0260.81∗→FR 56.74 60.83 57.29 41.9 57.12 43.9765.18IT→∗ 53.52 58.6 52.34 36.73 51.37 40.5758.66∗→IT 55.61 59.88 55.43 40.67 55.86 47.0664.4RU→∗ 46.74 52.13 49.88 34.09 50.08 38.2556.26∗→RU 37.74 48.21 47.72 33.75 49.76 37.3354.45Avg. 50.96 56.71 52.83 38.09 53.08 41.2 60.23\n[1K Setup]VECMAP CONTRASTIVEBLImT5xxlmT0xxlXGLM4.5B mGPTLLaMA13B\nDE→∗ 44.44 51.79 46.02 33.57 46.9 35.9853.82∗→DE 44.0 48.92 43.62 30.58 44.06 31.9 53.36EN→∗ 47.7 55.11 56.19 40.01 59.3 44.55 68.27∗→EN 55.74 59.66 52.86 39.6 53.51 43.18 57.06FR→∗ 48.47 55.33 49.46 37.03 51.08 42.8258.35∗→FR 54.88 58.65 54.51 38.88 56.74 43.963.26IT→∗ 49.08 55.92 48.49 33.78 49.97 39.8656.06∗→IT 53.4 57.08 51.75 38.15 54.19 45.3862.57RU→∗ 43.61 50.17 46.69 32.71 48.67 37.4652.75∗→RU 25.3 44.02 44.1 29.9 47.42 36.31 53.01Avg. 46.66 53.66 49.37 35.42 51.18 40.1357.85\n[Zero-Shot]VECMAP CONTRASTIVEBLImT5xxlmT0xxlXGLM4.5B mGPTLLaMA13B\nDE→∗ 44.44 - 35.76 29.31 38.37 22.65 43.3∗→DE 43.95 - 42.4 31.75 40.76 21.36 47.53EN→∗ 47.76 - 50.94 36.48 51.01 28.6 54.44∗→EN 55.8 - 43.16 38.77 44.75 25.78 52.34FR→∗ 48.24 - 42.37 35.66 45.24 30.73 51.01∗→FR 54.96 - 46.04 32.16 45.79 32.49 53.87IT→∗ 48.97 - 38.84 30.97 39.08 27.4 48.04∗→IT 53.27 - 45.68 37.4 46.81 34.79 53.09RU→∗ 43.63 - 42.05 32.19 40.85 21.49 46.82∗→RU 25.08 - 32.69 24.5 36.45 16.46 36.76Avg. 46.61 - 41.99 32.92 42.91 26.18 48.72\nTable 1: Main results on 20 XLING BLI directions in\n5K, 1K, and zero-shot (unsupervised) setups. Off-the-\nshelf mLLMs are used without any fine-tuning. Average\nP@1×100% scores of each language going to and from\nother 4 languages are reported. ‘-’: CONTRASTIVE BLI\ndoes not support unsupervised BLI.\nXLING: Main Results. In Table 1, we report\nthe BLI performance of the strongest single model\nfrom each LLM family (full results for each of\nthe 18 LLMs are available in Appendix D). Our\nresults on 20 XLING BLI directions reaffirm the\nleading position of LLaMA where the LLaMA13B\nvariant achieves the highest overall average BLI\nscores in all BLI setups, also outperforming CON-\nTRASTIVE BLI , the previous CLWE-based SotA on\nthe same dataset. We speculate that LLMs are more\nadept at few-shot learning: LLaMA 13B outper-\nforms VECMAP by circa 10 P@1 points in few-shot\nsetups, but only by about 2 points in the zero-shot\nsetup. It is also worth mentioning that in 5K and\n1K setups, mT5 xxl and XGLM4.5B beat VECMAP\nalthough they underperform CONTRASTIVE BLI; how-\never, in the zero-shot setup they still cannot match\nVECMAP.\nPanLex-BLI: Main Results. We present our re-\nsults on lower-resource languages from PanLex-\nBLI in Table 2. We also provide here only one\nstrongest model of each LLM family, while the\nfull results from all 18 LLMs are available in Ap-\npendix D). This time, LLaMA13B still outperforms\nother LLMs, but we report here XGLM7.5B instead\nwhich beats XGLM4.5B. Unlike for XLING, we\n9582\n0 2 4 6 8 10 12\nModel Size (B)\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60Avg. Accuracy (P@1×100%)\nmT5\nmT0\nXGLM\nmGPT\nLLaMA\n0 2 4 6 8 10 12\nModel Size (B)\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60Avg. Accuracy (P@1×100%)\nmT5\nmT0\nXGLM\nmGPT\nLLaMA\n0 2 4 6 8 10 12\nModel Size (B)\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60Avg. Accuracy (P@1×100%)\nmT5\nmT0\nXGLM\nmGPT\nLLaMA\nFigure 1: Averaged BLI score versus model size (0.3B to 13B): (left) |DS|=5K; (middle) |DS|=1K; (right) |DS|=0.\nfind that traditional CLWE-based approaches still\noutperform LLM-elicited BLI in general. This may\nreveal that current SotA mLLMs (size≤13B) still\nlack strong word translation capabilities for a large\nnumber of languages and language pairs, even for\nthose they currently cover.\nPut simply, while current mLLMs do exhibit\nstrong performance for arguably high-resource lan-\nguages (from XLING), they still have deficien-\ncies with lower-resource languages as well as\nwith their portability to a much larger number of\nlanguages, currently covered by more traditional\nBLI approaches (Li et al., 2022a). We leave to\nfuture work the investigation of larger mLLMs\n(e.g., LLaMA30B) for BLI with lower-resource lan-\nguages and languages unseen by the mLLMs.\nStatistical Significance. We conduct χ2 test com-\nparing LLaMA13B against the strongest single base-\nline in each BLI setup (i.e.,CONTRASTIVE BLI in few-\nshot setups and VECMAP in the zero-shot setup) on\nthe average BLI performance over 20 XLING and\n6 PanLex-BLI BLI directions respectively, and we\nestimate the p-values as follows. 1) On XLING, p\nis 2.8e-23 in the 5K setup, 8.5e-32 in the 1K setup,\nand 4.3e-9 in the zero-shot setup. 2) For PanLex-\nBLI, pis 1e-4 in the 1K setup and 1.9e-35 in the\nzero-shot setup. The p-values show that our main\nfindings are clearly statistically significant.17\n5.2 Further Analyses\nn-Shot Prompting. To better understand the in-\nfluence of the number of in-context examples,\nwe pick mT5large (an encoder-decoder LLM) and\nLLaMA13B (a decoder-only LLM) and run exper-\niments ranging from 0-shot to 10-shot. Figure 2\ndepicts their average BLI scores on 20 XLING\nBLI directions in 5K and 1K setups, respectively.\nThe results clearly demonstrate the usefulness of\nin-context learning. Even when having only one in-\ncontext example (one-shot), the same model vari-\n17By convention, p <0.05: statistically significant; p <\n1e-3: statistically highly significant.\n[1K Setup]VECMAP CONTRASTIVEBLImT5xxlmT0xxlXGLM7.5B mGPTLLaMA13B\nBG→CA 39.66 43.93 38.67 31.72 40.19 - 41.71CA→BG 33.54 40.06 36.2 22.72 40.23 - 41.53BG→HU 38.77 44.62 36.17 25.46 - 23.61 36.57HU→BG 36.52 43.03 36.98 24.02 - 28.17 43.2CA→HU 35.47 41.44 32.43 22.21 - - 35.3HU→CA 39.88 47.14 37.68 29.59 - - 46.04Avg. 37.31 43.37 36.36 25.95 - - 40.72\n[Zero-Shot]VECMAP CONTRASTIVEBLImT5xxlmT0xxlXGLM7.5B mGPTLLaMA13B\nBG→CA 39.6 - 28.04 28.86 28.5 - 32.77CA→BG 33.6 - 21.47 16.83 20.17 - 27.03BG→HU 39.24 - 27.26 24.07 - 7.23 23.61HU→BG 36.46 - 22.47 16.94 - 9.85 26.5CA→HU 34.09 - 24.59 22.93 - - 24.53HU→CA 37.79 - 25.47 24.48 - - 38.17Avg. 36.8 - 24.88 22.35 - - 28.77\nTable 2: Main results on 6 PanLex-BLI BLI directions\nin 1K and zero-shot (unsupervised) setups. Off-the-shelf\nmLLMs are used without any fine-tuning. P@1×100%\nscores are reported. ‘-’: 1) a language is not supported\nby the LLM; 2) CONTRASTIVE BLI does not support\nunsupervised BLI.\n0 1 2 3 4 5 6 7 8 9 10\nn (shot)\n25\n30\n35\n40\n45\n50\n55\n60Avg. Accuracy (P@1×100%)\nmT5large,| S|=5K\nmT5large,| S|=1K\nLLaMA13B,| S|=5K\nLLaMA13B,| S|=1K\nFigure 2: BLI scores averaged over 20 BLI directions\nfrom XLING with respect to the number of in-context\nexamples n(0 to 10), with mT5large and LLaMA13B in\nboth 5K and 1K BLI setups.\nant outperforms its zero-shot results by ∼5 P@1\npoints. However, with higher values for n (i.e.,\nn≥5), the gains become saturated.\nAblation Study. One key contribution of our work\nis that we propose to extract in-context examples\nfrom nearest neighbours. To validate the effec-\ntiveness of this approach, we conduct an essential\nablation study where we use randomly sampled\nin-context examples instead. As with our main ex-\nperiments, we present average scores on 20 BLI\ndirections from only one best LLM from each LLM\nfamily, and full results on all LLMs are available\nin Appendix D. Our results in Table 3 demonstrate\nthe following. 1) The nearest neighbour-based ‘NN\n(*K)’ scores for every LLM outperform the ‘Ran-\n9583\ndom (*K)’ scores by a salient margin: this shows\nthe effectiveness of in-context examples from near-\nest neighbours. 2) ‘Random (*K)’ outperforms\n‘Zero-Shot’, showing that even randomly picked\nin-context examples can benefit BLI. 3) ‘NN (5K)’\noutperforms ‘NN (1K)’, which means that better\nin-context examples can be retrieved from a larger\n(and more varied) 5K seed dictionary.18 We further\nshow that these findings are statistically significant\nvia χ2 test and report the p-values in Table 4.\nmT5xxl mT0xxl XGLM4.5B mGPTLLaMA13B\nNN (5K) 52.83 38.09 53.08 41.2 60.23NN (1K) 49.37 35.42 51.18 40.13 57.85\nRandom (5K) 46.93 34.97 49.85 38.88 56.85Random (1K) 45.93 33.90 49.92 38.38 56.11\nZero-Shot (Unsupervised) 41.99 32.92 42.91 26.18 48.72\nTable 3: Ablation results. Averaged BLI scores\n(P@1×100%) on 20 XLING BLI directions. Rows 1-2:\n5-shot prompting with in-context examples extracted\nfrom NN in DS of size 5K and 1K. Rows 3-4: 5-shot\nprompting with random in-context examples in DS of\nsize 5K and 1K. Row 5: zero-shot prompting without\nany in-context examples.\np-value mT5xxl mT0xxl XGLM4.5B mGPTLLaMA13B\nNN (5K) vs. Random (5K) 1.3e-60 1.9e-19 2.4e-19 4.7e-11 1.41e-21NN (1K) vs. Random (1K) 9.3e-22 9.1e-6 4.6e-4 6.4e-7 1.1e-6\nRandom (5K) vs. Zero-Shot 1.7e-43 1.8e-9 1.4e-83 1e-311 8.4e-114Random (1K) vs. Zero-Shot 2.4e-28 3.9e-3 3.2e-85 5.8e-289 2.8e-94\nNN (5K) vs. NN (1K) 5.8e-22 1.3e-14 1.2e-7 2.5e-3 1.7e-11\nTable 4: Statistical significance associated with Table 3.\nWe conduct χ2 tests and report p-values.\nBLI-Oriented Fine-Tuning. The fine-tuning ex-\nperiments, due to computational constraints, are\nconducted on four relatively smaller-scale LLMs.\nTable 5 reports each model’s average performance\non 20 XLING BLI directions before and after fine-\ntuning. We run fine-tuning experiments three times\nwith different random seeds and report both mean\nscores and standard deviations. We only observe\nsalient gains on mT5 models and XGLM 564M in\nthe 5K setups. For XGLM1.7B in both setups and\nall models in the 1K setup, the gains are smaller\nor even non-existent. Even in the 5K setup, the\ntuned mT5base still cannot match the off-the-shelf\nmT5large, and the tuned mT5 large underperforms\nmT5xl (cf. Table 18 in Appendix D). This may\nindicate some of the limitations of our proposed\n18We also observe a slight edge of ‘Random (5K)’ over\n‘Random (1K)’, and we speculate this might have to do with\nhow XLING’s test set and seed dictionaries were created. In\nfact, the 1K seed dictionary contains the most frequent 1K\nwords, and the test set words include less frequent 2K words.\nmT5base mT5large XGLM564M XGLM1.7B\nFine-Tuned (5K)36.11±0.53146.68±0.05828.94±0.02640.92±0.005\nOff-The-Shelf (5K) 28.33 43.25 24.51 39.49\nFine-Tuned (1K)25.61±0.30738.48±0.19325.6±0.12339.59±0.017\nOff-The-Shelf (1K) 24.9138.84 24.18 38.86\nTable 5: Comparisons between mLLMs before and\nafter BLI-oriented fine-tuning. Averaged BLI scores\n(P@1×100%) on 20 XLING BLI directions.\nBLI-oriented fine-tuning (i.e., training the mLLMs\non BLI data with their own pretraining objectives)\nand may indicate the following. 1) With the current\ntraining approach and a fixed amount of computa-\ntional budget, one may prioritise adopting off-the-\nshelf larger LLMs (with in-context learning) rather\nthan training smaller-scale ones. 2) In future work,\nother training objectives and strategies should be\ninvestigated for improved BLI performance with\nmLLMs. As another future research avenue, it is\nalso worth extending the training to larger mLLMs\nand adopting novel fine-tuning techniques such as\nprompt tuning (Lester et al., 2021), adapters (Li\net al., 2020, 2023) and LoRA (Hu et al., 2022).\nTemplates. Now, we additionally provide some\npreliminary findings from our template search. 19\n1) Models from the same mLLM family may tend\nto prefer the same template. For example, Table 12\n(Appendix C) shows that all five XGLM models\nprefer the same best zero-shot template and four\nof them share one best few-shot template. This\nphenomenon is to some extent seen also on mT5\n(zero-shot and few-shot), mT0 (zero-shot and few-\nshot), and LLaMA (few-shot). This should be due\nto the same training data, training strategy, and\nmodel architecture adopted for all models in the\nsame LLM family. 2) As already mentioned in\n§3.1, mT0 is compatible with both mask-filling-\nstyle and GPT-style templates: Table 12 shows that\nsome mT0 models prefer templates with ‘<mask>’\nand others do not. 3) Under the ‘GPT-style’ tem-\nplates, decoder-only models all prefer templates for\nsentence completion while some of the instruction-\ntuned mT0 models prefer questions with ‘?’.\n5.3 Further Discussion\nFew-Shot Learning for BLI. Our main results\ndemonstrate that few-shot learning derives consis-\ntent gains over zero-shot prompting. For instance,\nHR→EN and IT→EN saw 345 and 272 cases in\n19Since we conduct template search only on a random lan-\nguage pair, these findings are yet to be verified by future work\nfor other language pairs.\n9584\ntheir test sets respectively where few-shot learning\nmakes the correct prediction but zero-shot learning\nfails (positive cases). There are only 85 and 87\ncases where zero-shot prompting beats few-shot\nprompting (negative cases). We present 8 posi-\ntive examples and 4 negative examples for each\nof HR→EN and IT→EN, comparing five-shot (5K\nsetup) and zero-shot results with LLaMA13B in Ta-\nble 19 (Appendix E). For instance, ‘gušter ( HR)\n→lizard (EN)’ and ‘sezam ( HR) →sesame’ are\ntwo positive cases, their in-context examples being\nfive different animal names and five plant names,\nwhich may help LLaMA 13B to narrow down the\nscope of the target word to animal and plant names\nrespectively. Similarly, ‘valcer (HR) →waltz (EN)’\n(a positive case) is associated with five in-context\nexamples related to either music or dance. How-\never, few-shot learning does not always help. For\nexample, in ‘eventuale (IT) →eventual (EN)’ and\n‘scopre (IT) →discovers (EN)’ translation tasks,\nthe LLM seems to make a mistake due to directly\ncopying one of the words provided in the in-context\nexamples, whereas zero-shot prompting predicts\nthe correct answers.\nBLI for EN and non-EN Languages. It is notewor-\nthy that the volume of data in English for mLLM\npretraining often exceeds that in any other language\n(e.g., mT5, mT0, and LLaMA), and thus mLLMs\nmay be biased, favouring BLI directions involving\nEN. However, we did not identify very clear clues\nindicating that their BLI performance is (heavily)\nbiased. In fact, in the 5K setup (see Table 1), al-\nthough ‘EN→∗’ surpasses ‘non-EN→∗’ in absolute\nBLI scores (for each of our LLMs and also CLWE-\nbased baselines), we meanwhile observe that 1)\nmT0xxl achieves lower average score in ‘ ∗→EN’\nthan ‘∗→FR’, and 2) for LLaMA13B, ‘∗→EN’ lags\nbehind both ‘ ∗→FR’ and ‘ ∗→IT’. Moreover, as\nan example, the LLaMA13B model supports 6 lan-\nguages resulting in 30 BLI directions, and 20 of\nthem are between non-EN languages. LLaMA13B\noutperforms CLWE-based SotA in 16/20 cases and\nin 18/20 cases respectively in the 5K and 1K setups\nfor the non-EN pairs (cf. Tables 13 and 14). How-\never, we again note that this might hold only for\nhigh-resource languages such as the ones covered\nin XLING.\nImpact Statement. Here, we discuss the potential\nimpact of our study on the following two aspects.\n1) On future BLI research. Our work minimises the\ntechnical gap between BLI and prompt-based learn-\ning and opens up new possibilities for BLI research.\nIn fact, LLM prompting provides a generic and\nstraightforward way of leveraging external knowl-\nedge for BLI. While we have demonstrated the\neffectiveness of in-context word translation exam-\nples, external information such as word definition,\nparts-of-speech, spelling, and sentence translation\npairs can also be integrated into text prompts. 2)\nOn NMT and other related fields. Recent work has\nincorporated word translation pairs into text tem-\nplates to prompt LLMs for sentence-level neural\nmachine translation (NMT) and demonstrates that\nthe bilingual lexical ‘hints’ lead to significant gains\nin NMT (Ghazvininejad et al., 2023; Jones et al.,\n2023). While a ground-truth bilingual dictionary\ncan be leveraged, BLI is able to provide word trans-\nlations for language pairs and words not covered in\nexisting bilingual lexica.20 Our work can provide\nstrong word translation pairs for lexicon-enhanced\nMT, and the improved MT may further benefit,\ne.g., the field of cross-lingual transfer learning via\nTRANSLATE -TRAIN /TEST approaches (Conneau\net al., 2018; Li et al., 2023).\n6 Conclusion\nThis paper presents the first study on bilingual lexi-\ncon induction (BLI) with multilingual text-to-text\nlarge language models (mLLMs). We develop the\nmethodology to prompt mLLMs for BLI, conduct\nextensive template search, and systematically ex-\nperiment with 5 representative mLLM families (18\nmodels) on a variety of zero-shot and few-shot BLI\ntasks. Relying on off-the-shelf mLLMs, our ex-\nperiments on the standard XLING dataset offer\nstrong performance in all BLI setups, where our\nproposed few-shot prompting with in-context ex-\namples from nearest neighbours outperforms the\nstrongest CLWE-based SotA by a considerable\nmargin. However, our study also points out that\nprompting-based methods still need to be success-\nfully extended to lower-resource languages. Fi-\nnally, we conduct a series of in-depth analyses\ncovering variants of our few-shot prompting and\npreliminary investigations on BLI-oriented fine-\ntuning. Our key findings and comprehensive anal-\nyses may pave the way for the development of\nstronger mLLM-based BLI systems in the future.\n20Moreover, given an input word, BLI can offer multiple\nplausible translations for the downstream NMT to consider,\nand we speculate this may, to some extent, increase the diver-\nsity of MT output.\n9585\nLimitations\nFirst, most recently released state-of-the-art\nmLLMs are still unable to support as many lan-\nguages as static word embeddings, which currently\nlimits their wider portability. For instance, LLaMA\nsupports 20 languages and XGLM supports 30 lan-\nguages, while fastText provides pretrained static\nWEs for 294 languages that can be used for the\ninduction of static CLWEs.21 Intuitively, this is be-\ncause training LLMs that support more languages\nwould require higher computational costs (with\nmore training data and typically larger model sizes).\nWe hope that researchers in the future can pretrain\nand release mLLMs that support a larger set of\nlinguistically diverse languages, which can thus\nprobably extend the success of our approach to\nmore languages and language families.\nSecond, our work did not investigate open-\nsource LLMs with more than13B parameters22 due\nto a large number of experiments conducted com-\nbined with our limited computing resources, and\nwe did not evaluate any closed-source LLMs. Quite\na few tech companies and AI research labs have\nbeen training LLMs with 100+B and even 500+B\nparameters. We encourage interested readers who\nhave access to adequate computing resources or\nspecific closed-source LLMs to take a step further\nand investigate if larger LLMs can provide an even\nstronger BLI performance than reported in this par-\nticular work, following the recipe presented in this\nwork.\nThird, as also discussed in other BLI work (Li\net al., 2022b), existing BLI datasets did not control\nthe synonyms and polysemy well and to a sufficient\ndetail. In fact, when constructing BLI datasets, it\nis very difficult to collect all correct translations\nfor each source word. Therefore, one limitation\nof BLI evaluation is that it cannot give credit to\ncorrect answers that are not included in the ground-\ntruth translation set, and evaluation is typically con-\nducted out-of-context. Constructing finer-grained\nBLI datasets with the help of qualified annotators\n(e.g., linguists, typologists and bilingual speakers)\nis beyond the scope of this work.\nAcknowledgements\nWe thank the anonymous reviewers and area chairs\nfor their valuable feedback. Yaoyiran Li is sup-\n21fasttext.cc/docs/en/pretrained-vectors.html\n22For instance, LLaMA also offers larger 30B and 65B\nmodel variants.\nported by Grace & Thomas C. H. Chan Cambridge\nInternational Scholarship. Ivan Vuli´c is supported\nby a personal Royal Society University Research\nFellowship ‘Inclusive and Sustainable Language\nTechnology for a Truly Multilingual World’ (no\n221137; 2022–).\nReferences\nPrince Osei Aboagye, Jeff Phillips, Yan Zheng, Junpeng\nWang, Chin-Chia Michael Yeh, Wei Zhang, Liang\nWang, and Hao Yang. 2022. Normalization of lan-\nguage embeddings for cross-lingual alignment. In\nInternational Conference on Learning Representa-\ntions.\nDavid Alvarez-Melis and Tommi Jaakkola. 2018.\nGromov-Wasserstein alignment of word embedding\nspaces. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1881–1890, Brussels, Belgium. Association\nfor Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018a.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 789–798,\nMelbourne, Australia. Association for Computational\nLinguistics.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018b. Unsupervised neural ma-\nchine translation. In International Conference on\nLearning Representations.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nEleftheria Briakou, Colin Cherry, and George F. Foster.\n2023. Searching for needles in a haystack: On the\nrole of incidental bilingualism in palm’s translation\ncapability. ArXiv, abs/2305.10266.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\n9586\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinod-\nkumar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Re-\nwon Child, Oleksandr Polozov, Katherine Lee, Zong-\nwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. ArXiv, abs/2204.02311.\nAlexandra Chronopoulou, Dario Stojanovski, and\nAlexander Fraser. 2021. Improving the lexical abil-\nity of pretrained language models for unsupervised\nneural machine translation. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 173–180, Online.\nAssociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4171–4186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nEric Gaussier, J.M. Renders, I. Matveeva, C. Goutte,\nand H. Dejean. 2004. A geometric view on bilingual\nlexicon extraction from comparable corpora. In Pro-\nceedings of the 42nd Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 526–533,\nBarcelona, Spain.\nMarjan Ghazvininejad, Hila Gonen, and Luke Zettle-\nmoyer. 2023. Dictionary-based phrase-level prompt-\ning of large language models for machine translation.\nArXiv, abs/2302.07856.\nGoran Glavaš, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 710–721,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nGoran Glavaš and Ivan Vuli ´c. 2020. Non-linear\ninstance-based cross-lingual mapping for non-\nisomorphic embedding spaces. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 7548–7555, Online. As-\nsociation for Computational Linguistics.\nHila Gonen, Shauli Ravfogel, Yanai Elazar, and Yoav\nGoldberg. 2020. It’s not Greek to mBERT: Inducing\nword-level translations from multilingual BERT. In\nProceedings of the Third BlackboxNLP Workshop on\nAnalyzing and Interpreting Neural Networks for NLP,\npages 45–56, Online. Association for Computational\nLinguistics.\nGeert Heyman, Bregt Verreet, Ivan Vuli´c, and Marie-\nFrancine Moens. 2019. Learning unsupervised multi-\nlingual word embeddings with incremental multilin-\ngual hubs. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1890–1902, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand L. Sifre. 2022. Training compute-optimal large\nlanguage models. ArXiv, abs/2203.15556.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nAlex Jones, Isaac Caswell, Ishan Saxena, and Orhan\nFirat. 2023. Bilex rx: Lexical data augmentation for\nmassively multilingual machine translation. ArXiv,\nabs/2303.15265.\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov,\nHervé Jégou, and Edouard Grave. 2018. Loss in\ntranslation: Learning bilingual word mapping with a\nretrieval criterion. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\n9587\nProcessing, pages 2979–2984, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio Ran-\nzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In Proceed-\nings of the International Conference on Learning\nRepresentations.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYaoyiran Li, Ching-Yun Chang, Stephen Rawls, Ivan\nVuli´c, and Anna Korhonen. 2023. Translation-\nenhanced multilingual text-to-image generation. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 9174–9193, Toronto, Canada.\nAssociation for Computational Linguistics.\nYaoyiran Li, Fangyu Liu, Nigel Collier, Anna Korhonen,\nand Ivan Vuli´c. 2022a. Improving word translation\nvia two-stage contrastive learning. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4353–4374, Dublin, Ireland. Association for\nComputational Linguistics.\nYaoyiran Li, Fangyu Liu, Ivan Vuli´c, and Anna Korho-\nnen. 2022b. Improving bilingual lexicon induction\nwith cross-encoder reranking. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2022, pages 4100–4116, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nYaoyiran Li, Edoardo Maria Ponti, Ivan Vuli´c, and Anna\nKorhonen. 2020. Emergent communication pretrain-\ning for few-shot machine translation. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 4716–4731, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2022. Few-shot learning with\nmultilingual generative language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 9019–9052,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In Proceedings of the\nInternational Conference on Learning Representa-\ntions.\nSasha Luccioni, Victor Schmidt, Alexandre Lacoste,\nand Thomas Dandres. 2019. Quantifying the carbon\nemissions of machine learning. In NeurIPS 2019\nWorkshop on Tackling Climate Change with Machine\nLearning.\nKelly Marchisio, Kevin Duh, and Philipp Koehn. 2020.\nWhen does unsupervised machine translation work?\nIn Proceedings of the Fifth Conference on Machine\nTranslation, pages 571–583, Online. Association for\nComputational Linguistics.\nKelly Marchisio, Neha Verma, Kevin Duh, and Philipp\nKoehn. 2022. IsoVec: Controlling the relative iso-\nmorphism of word embedding spaces. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 6019–6033,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nTomás Mikolov, Quoc V . Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for machine\ntranslation. CoRR, abs/1309.4168.\nTasnim Mohiuddin, M Saiful Bari, and Shafiq Joty.\n2020. LNMap: Departures from isomorphic assump-\ntion in bilingual lexicon induction through non-linear\nmapping in latent space. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2712–2723, Online. Associ-\nation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Rose Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng Xin Yong,\nHailey Schoelkopf, Xiangru Tang, Dragomir R.\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel\nAlbanie, Zaid Alyafeai, Albert Webson, Edward\nRaff, and Colin Raffel. 2022. Crosslingual gen-\neralization through multitask finetuning. ArXiv,\nabs/2211.01786.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\n9588\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nBarun Patra, Joel Ruben Antony Moniz, Sarthak Garg,\nMatthew R. Gormley, and Graham Neubig. 2019.\nBilingual lexicon induction with semi-supervision\nin non-isometric embedding spaces. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 184–193, Florence,\nItaly. Association for Computational Linguistics.\nXutan Peng, Chenghua Lin, and Mark Stevenson. 2021.\nCross-lingual word embedding refinement by ℓ1\nnorm optimisation. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2690–2701, Online. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard. 2019.\nA survey of cross-lingual word embedding models.\nJournal of Artificial Inteligence Research , 65:569–\n631.\nVin Sachidananda, Ziyi Yang, and Chenguang Zhu.\n2021. Filtered inner product projection for crosslin-\ngual embedding alignment. In International Confer-\nence on Learning Representations.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mgpt: Few-shot learners go multilin-\ngual. ArXiv, abs/2204.07580.\nJimin Sun, Hwijeen Ahn, Chan Young Park, Yulia\nTsvetkov, and David R. Mortensen. 2021. Cross-\ncultural similarity features for cross-lingual transfer\nlearning of pragmatically motivated tasks. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2403–2414, Online.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming\nLuo, Viresh Ratnakar, and George F. Foster. 2022.\nPrompting palm for translation: Assessing strategies\nand performance. ArXiv, abs/2211.09102.\nIvan Vuli´c, Goran Glavaš, Fangyu Liu, Nigel Collier,\nEdoardo Maria Ponti, and Anna Korhonen. 2023.\nProbing cross-lingual lexical knowledge from mul-\ntilingual sentence encoders. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 2089–\n2105, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nIvan Vuli´c, Goran Glavaš, Roi Reichart, and Anna Ko-\nrhonen. 2019. Do we really need fully unsupervised\ncross-lingual embeddings? In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint\nConference on Natural Language Processing, pages\n4407–4418, Hong Kong, China. Association for Com-\nputational Linguistics.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Prob-\ning pretrained language models for lexical semantics.\nIn Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7222–7240, Online. Association for Computational\nLinguistics.\nXinyi Wang, Sebastian Ruder, and Graham Neubig.\n2022. Expanding pretrained models to thousands\nmore languages via lexicon-based adaptation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 863–877,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. In Proceedings\nof the 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1006–1011,\nDenver, Colorado. Association for Computational\nLinguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\n9589\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nJinpeng Zhang, Baijun Ji, Nini Xiao, Xiangyu Duan,\nMin Zhang, Yangbin Shi, and Weihua Luo. 2021.\nCombining static word embeddings and contextual\nrepresentations for bilingual lexicon induction. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 2943–2955, On-\nline. Association for Computational Linguistics.\nXu Zhao, Zihao Wang, Yong Zhang, and Hao Wu. 2020.\nA relaxed matching procedure for unsupervised BLI.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 3036–\n3041, Online. Association for Computational Lin-\nguistics.\nYucheng Zhou, Xiubo Geng, Tao Shen, Wenqiang\nZhang, and Daxin Jiang. 2021. Improving zero-shot\ncross-lingual transfer for multilingual question an-\nswering over knowledge graph. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5822–5834,\nOnline. Association for Computational Linguistics.\n9590\nA Languages\nFamily LanguageCode LLMs\nIE:GermanicEnglish EN mT5,mT0,XGLM,mGPT,LLaMA\nGerman DE mT5,mT0,XGLM,mGPT,LLaMA\nIE:Romance\nCatalan CA mT5,mT0,XGLM,LLaMA\nFrench FR mT5,mT0,XGLM,mGPT,LLaMA\nItalian IT mT5,mT0,XGLM,mGPT,LLaMA\nIE:Slavic\nBulgarianBG mT5,mT0,XGLM,mGPT,LLaMA\nCroatian HR LLaMA\nRussian RU mT5,mT0,XGLM,mGPT,LLaMA\nTurkic Turkish TR mT5,mT0,XGLM,mGPT\nUralic Finnish FI mT5,mT0,XGLM,mGPT\nHungarianHU mT5,mT0,mGPT,LLaMA\nTable 6: Languages covered in our experiments with\ntheir ISO 639-1 codes and the mLLM families that sup-\nport that language, categorized by language family. IE\n= Indo-European.\nB Reproducibility Checklist\n• BLI Data: We adopt two publicly available BLI\ndatasets.23 24\n• Static Word Embeddings : Following the\ndatasets’ own recommendations and other previ-\nous work, we use the XLING-preprocessed fast-\nText WEs trained on Wikipedia25 for XLING data\nand fastText WEs trained on Common Crawl +\nWikipedia26 for PanLex-BLI, and the WEs are\ntrimmed to the most frequent 200K words for each\nlanguage. For fair comparisons, we use the same\nset of fastText WEs both for the retrieval of near-\nest neighbours (in-context examples) and for the\nCLWE-based baselines.\n• Pretrained LLMs and Parameter Counts: All\nthe LLMs used in our experiments are publicly\navailable from the huggingface.co model hub.\nWe summarise their model identifiers and model\nsizes in Table 7. Please refer to each LLM’s own\ncopyright and licence before downloading, using,\nfine-tuning, or redistributing any LLM.\n• Source Code: Our code is publicly available at\nhttps://github.com/cambridgeltl/prompt4b\nli.\n• Computing Infrastructure: We have run our\ncode on Wilkes3, a GPU cluster hosted by Re-\n23https://github.com/codogogo/xling-eval\n24https://github.com/cambridgeltl/panlex-bli\n25https://fasttext.cc/docs/en/pretrained-vecto\nrs.html\n26https://fasttext.cc/docs/en/crawl-vectors.h\ntml\nsearch Computing Services at the University of\nCambridge, where each run leverages a single\nNvidia 80GB A100 GPU and 32×CPU cores.\n• Software: Slurm 20.11.9, Python 3.9.7, PyTorch\n1.10.1+cu113, Transformers 4.28.1.\n• Runtime (Wall Time): We present the average\ninference time on one single BLI direction (i.e.,\ncirca 2,000 word pairs in an XLING test set; the\ntime required for loading the LLM and the dataset\nis not included) for each LLM in Table 8. The per-\nepoch training time for BLI-oriented fine-tuning is\nprovided in Table 9.\n• Hyperparameter Search: As introduced in §3.3\nand §4, our template selection and all our hyperpa-\nrameter search are conducted on a single randomly\nchosen language pair (German, French), following\nprevious work (Li et al., 2022a,b). The learning rate\nfor LLM fine-tuning is selected from [1e−9,5e−\n9,1e−8,2e−8,1e−7,1e−6,2e−6,2e−5,1e−3].\n• Significance: We have discussed the significance\nof our main results and ablation results in the\nlast paragraph of §5.1 and in Table 4 respectively,\nwhich demonstrates that our findings are statisti-\ncally significant.\n• Randomness: Our main experiments are com-\npletely deterministic since we rely on off-the-shelf\nLLMs without any fine-tuning, nearest neighbour\nretrieval for in-context examples (a deterministic\nretrieval algorithm), and the deterministic beam\nsearch. The randomness only exists in two parts of\nour side analysis. First, we use random in-context\nexamples in our ablation study, and we verify our\nfindings with statistical tests in Table 4. Second,\nthe fine-tuning experiments do have randomness,\nand we run fine-tuning three times for each model,\nreporting both average BLI performance and the\nstandard deviation.\n• Carbon Footprint: All the experiments involved\nin this project including hyperparameter tuning,\ntemplate search, BLI inference, and BLI-oriented\nfine-tuning of our LLMs consume circa 1,650\nA100 GPU hours. Based on a publicly available\n‘machine learning emissions calculator’ (Luccioni\net al., 2019)27 and our computational infrastructure,\nwe estimate that our work causes the emission of\ncirca 200kg CO2 equivalents.\n27https://mlco2.github.io/impact/#compute\n9591\nLLM Model ID Number of Parameters\nmT5small \"google/mt5-small\" 300,176,768\nmT5base \"google/mt5-base\" 582,401,280\nmT5large \"google/mt5-large\" 1,229,581,312\nmT5xl \"google/mt5-xl\" 3,742,619,648\nmT5xxl \"google/mt5-xxl\" 12,921,057,280\nmT0small \"bigscience/mt0-small\" 300,176,768\nmT0base \"bigscience/mt0-base\" 582,401,280\nmT0large \"bigscience/mt0-large\" 1,229,581,312\nmT0xl \"bigscience/mt0-xl\" 3,742,619,648\nmT0xxl \"bigscience/mt0-xxl\" 12,921,057,280\nXGLM564M \"facebook/xglm-564M\" 564,463,616\nXGLM1.7B \"facebook/xglm-1.7B\" 1,732,907,008\nXGLM2.9B \"facebook/xglm-2.9B\" 2,941,505,536\nXGLM4.5B \"facebook/xglm-4.5B\" 4,552,511,488\nXGLM7.5B \"facebook/xglm-7.5B\" 7,492,771,840\nmGPT \"sberbank-ai/mGPT\" 1,417,596,928\nLLaMA7B \"huggyllama/llama-7b\" 6,738,415,616\nLLaMA13B \"huggyllama/llama-13b\"13,015,864,320\nTable 7: LLMs used in our experiments with their hugg\ningface.co model IDs and model sizes.\nLLM Batch Size (Inference)0-Shot 5-Shot\nmT5small 100 6 s 7s\nmT5base 100 7 s 8s\nmT5large 100 8 s 12s\nmT5xl 8 45 s 50s\nmT5xxl 8 52 s 83s\nmT0small 100 5 s 6s\nmT0base 100 6 s 7s\nmT0large 100 8 s 14s\nmT0xl 8 46 s 50s\nmT0xxl 8 57 s 78s\nXGLM564M 1 213 s 225s\nXGLM1.7B 1 228 s 237s\nXGLM2.9B 1 343 s 366s\nXGLM4.5B 1 336 s 394s\nXGLM7.5B 1 382 s 461s\nmGPT 1 192 s 210s\nLLaMA7B 1 328 s 463s\nLLaMA13B 1 434 s 636s\nTable 8: Inference time (in seconds) of each LLM with\n0-Shot and 5-Shot prompts respectively.\nLLM Batch Size (Training)5K Setup1K Setup\nmT5base 32 17 min 4min\nmT5large 32 38 min 8min\nXGLM564M 32 24 min 5min\nXGLM1.7B 16 80 min 15min\nTable 9: Per-epoch training time (in minutes) of each\nLLM with 5-Shot prompts in 5K and 1K setups respec-\ntively.\nC Templates\nWe summarise all our zero-shot templates in Ta-\nble 10 and few-shot templates in Table 11: these\n102 templates constitute our ‘template pool’. Each\nof Tables 10 and 11 is split into two parts for mask-\nfilling-style and GPT-style templates respectively\nas introduced in §3.1 and §3.2. In addition, we\nlist the best zero-shot template and the best few-\nshot template for each of our 18 LLMs in Table 12.\nAgain, as already mentioned in §3.3, our template\nselection is conducted on a randomly chosen lan-\nguage pair (German, French) where for few-show\ntemplates, the in-context examples are derived from\na seed dictionary of size 5K. While we do not have\nenough computational resources to calculate and do\nnot have enough space to present the performance\nof each template for each LLM on each XLING\nBLI direction (102 ×18 ×56 = 102,816 scores),\nin the last paragraph of §5.2 we have discussed\nsome preliminary findings only from our template\nsearch.\nD Full BLI Results\nHere we present our full results on both XLING\nand PanLex-BLI. Table 13, 14, and 16 are our re-\nsults on all 56 XLING BLI directions in 5K, 1K,\nand zero-shot (unsupervised) BLI setups respec-\ntively. Table 15 and 17 are results for PanLex-BLI\nlower-resource languages (6 BLI directions) in 1K\nand zero-shot (unsupervised) BLI setups. Note that\nan (m)LLM usually cannot support every language,\nand we use ‘-’ to denote this scenario. Throughout\nthis paper, our expression ‘a language is not sup-\nported by an LLM’ means that the language is not\nused for pretraining the LLM even if the LLM’s to-\nkeniser may still be able to tokenise possibly many\ninput sentences in the language. Table 18 shows\nthe full ablation results for each of our 18 mLLMs.\nE Translation Examples\nTo illustrate how few-shot learning improves\nBLI, we present some of our BLI results with\nLLaMA13B in Table 19 comparing five-shot and\nzero-shot prompting on HR→EN and IT→EN BLI\ntest sets.\n9592\nMask-Filling-Style Templates (Zero-Shot Prompting)\n1 The word 'wx' inLy is: <mask>. 2 The word wxinLy is: <mask>.\n3 The word 'wx' inLy is: <mask> 4 The word wxinLy is <mask>\n5 The LxwordwxinLy is: <mask>. 6 The LxwordwxinLy is <mask>.\n7 The Lxword 'wx' inLy is: <mask>. 8 The Lxword 'wx' inLy is <mask>.\n9 The LxwordwxinLy is: <mask> 10 The LxwordwxinLy is <mask>\n11 TheLxword 'wx' inLy is: <mask> 12 The Lxword 'wx' inLy is <mask>\n13 ' wx' inLy is: <mask>. 14 wxinLy is: <mask>.\n15 ' wx' inLy is: <mask> 16 wxinLy is: <mask>\n17 What is the translation of the word 'wx' intoLy? <mask>. 18 What is the translation of the wordwxintoLy? <mask>.\n19 What is the translation of theLxword 'wx' intoLy? <mask>. 20 What is the translation of theLxwordwxintoLy? <mask>.\n21 The translation of the word 'wx' intoLy is <mask>. 22 The translation of the word wxintoLy is <mask>.\n23 The translation of theLxword 'wx' intoLy is <mask>. 24 How do you say 'wx' inLy? <mask>.\n25 How do you saywxinLy? <mask>. 26 How do you say the Lxword 'wx' inLy? <mask>.\n27 How do you say theLxwordwxinLy? <mask>. 28 Translate the word ' wx' intoLy: <mask>.\n29 Translate the wordwxintoLy: <mask>. 30 Translate the word wxintoLy: <mask>\n31 TranslatewxintoLy: <mask>. 32 Translate the LxwordwxintoLy: <mask>.\n33 Translate theLxwordwxintoLy: <mask> 34 Translate from LxtoLy: wx-> <mask>.\n35 Translate fromLxtoLy: wx-> <mask> 36 Translate from LxtoLy: wx=> <mask>.\n37 Translate fromLxtoLy: wx=> <mask>\nGPT-Style Templates (Zero-Shot Prompting)\n38 The word 'wx' inLy is: 39 The word wxinLy is:\n40 The wordwxinLy is 41 The LxwordwxinLy is:\n42 TheLxwordwxinLy is 43 The Lxword 'wx' inLy is:\n44 TheLxword 'wx' inLy is 45 ' wx' inLy is:\n46 wxinLy is: 47 Translate the word ' wx' intoLy:\n48 Translate the wordwxintoLy: 49 Translate from LxtoLy: wx->\n50 Translate fromLxtoLy: wx=> 51 Translate wxintoLy:\n52 Translate theLxwordwxintoLy: 53 Translate the Lxword 'wx' intoLy:\n54 What is the translation of the word 'wx' intoLy? 55 What is the translation of the word wxintoLy?\n56 The translation of the word 'wx' intoLy is 57 The translation of the word wxintoLy is\n58 The translation of theLxword 'wx' intoLy is 59 The translation of the LxwordwxintoLy is\n60 How do you say 'wx' inLy? 61 How do you say wxinLy?\n62 How do you say 'wx' inLy: 63 How do you say wxinLy:\n64 How do you say theLxword 'wx' inLy? 65 How do you say the LxwordwxinLy?\n66 Q: What is theLy translation ofwxA:\nTable 10: Our 66 templates for zero-shot prompting. These include 37 mask-filling-style templates (template\nIDs: 1 ∼37) and 29 GPT-style templates (template IDs: 38 ∼66). In our experiments, the ‘<mask>’ token is\n‘<extra_id_0>’ for mT5 and mT0.\n9593\nMask-Filling-Style Templates (Few-Shot Prompting)\n67 Translate fromLx toLy: wx1->wy\n1 wx2->wy\n2 wx-> <mask>.\n68 Translate fromLx toLy: wx1->wy\n1, wx2->wy\n2, wx-> <mask>.\n69 Translate fromLx toLy: wx1->wy\n1 wx2->wy\n2 wx-> <mask>\n70 Translate fromLx toLy: wx1->wy\n1, wx2->wy\n2, wx-> <mask>\n71 Translate fromLx toLy: wx1=>wy\n1 wx2=>wy\n2 wx=> <mask>.\n72 Translate fromLx toLy: wx1=>wy\n1, wx2=>wy\n2, wx=> <mask>.\n73 Translate fromLx toLy: wx1=>wy\n1 wx2=>wy\n2 wx=> <mask>\n74 Translate fromLx toLy: wx1=>wy\n1, wx2=>wy\n2, wx=> <mask>\n75 The wordwx1 inLy iswy\n1. The wordwx2 inLy iswy\n2. The wordwx inLy is <mask>.\n76 The Lx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is <mask>.\n77 The Lx word 'wx1' inLy is 'wy\n1'. TheLx word 'wx2' inLy is 'wy\n2'. TheLx word 'wx' inLy is '<mask>'.\n78 The Lx wordwx1 inLy iswy\n1, TheLx wordwx2 inLy iswy\n2, TheLx wordwx inLy is <mask>.\nGPT-Style Templates (Few-Shot Prompting)\n79 Translate fromLx toLy: wx1->wy\n1 wx2->wy\n2 wx->\n80 Translate fromLx toLy: wx1->wy\n1, wx2->wy\n2, wx->\n81 Translate fromLx toLy: wx1=>wy\n1 wx2=>wy\n2 wx=>\n82 Translate fromLx toLy: wx1=>wy\n1, wx2=>wy\n2, wx=>\n83 The wordwx1 inLy iswy\n1. The wordwx2 inLy iswy\n2. The wordwx inLy is\n84 The wordwx1 inLy iswy\n1. The wordwx2 inLy iswy\n2. The wordwx inLy is:\n85 The Lx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is\n86 The Lx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is:\n87 The wordwx1 inLy iswy\n1, The wordwx2 inLy iswy\n2, The wordwx inLy is\n88 The wordwx1 inLy iswy\n1, The wordwx2 inLy iswy\n2, The wordwx inLy is:\n89 The Lx wordwx1 inLy iswy\n1, TheLx wordwx2 inLy iswy\n2, TheLx wordwx inLy is\n90 The Lx wordwx1 inLy iswy\n1, TheLx wordwx2 inLy iswy\n2, TheLx wordwx inLy is:\n91 The word 'wx1' inLy iswy\n1. The word 'wx2' inLy iswy\n2. The word 'wx' inLy is\n92 The word 'wx1' inLy iswy\n1. The word 'wx2' inLy iswy\n2. The word 'wx' inLy is:\n93 The Lx word 'wx1' inLy iswy\n1. TheLx word 'wx2' inLy iswy\n2. TheLx word 'wx' inLy is\n94 The Lx word 'wx1' inLy iswy\n1. TheLx word 'wx2' inLy iswy\n2. TheLx word 'wx' inLy is:\n95 The word 'wx1' inLy iswy\n1, The word 'wx2' inLy iswy\n2, The word 'wx' inLy is\n96 The word 'wx1' inLy iswy\n1, The word 'wx2' inLy iswy\n2, The word 'wx' inLy is:\n97 The Lx word 'wx1' inLy iswy\n1, TheLx word 'wx2' inLy iswy\n2, TheLx word 'wx' inLy is\n98 The Lx word 'wx1' inLy iswy\n1, TheLx word 'wx2' inLy iswy\n2, TheLx word 'wx' inLy is:\n99 The wordwx1 inLy iswy\n1. The wordwx2 inLy iswy\n2. How do you saywx inLy?\n100 The Lx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. How do you say theLx wordwx inLy?\n101 The word 'wx1' inLy iswy\n1. The word 'wx2' inLy iswy\n2. How do you say 'wx' inLy?\n102 The Lx word 'wx1' inLy iswy\n1. TheLx word 'wx2' inLy iswy\n2. How do you say theLx word 'wx' inLy?\nTable 11: Our 36 templates for few-shot prompting. For simplicity, we present only two in-context examples in\neach template. These include 12 mask-filling-style templates (template IDs: 67 ∼78) and 24 GPT-style templates\n(template IDs: 79 ∼102). In our experiments, the ‘<mask>’ token is ‘<extra_id_0>’ for mT5 and mT0.\n9594\nLLM Best Template (Zero-Shot)\nmT5small The word 'wx' inLy is: <mask>.\nmT5base Translate the word 'wx' intoLy: <mask>.\nmT5large TheLx word 'wx' inLy is: <mask>.\nmT5xl TheLx word 'wx' inLy is: <mask>.\nmT5xxl TheLx word 'wx' inLy is: <mask>\nmT0small Translate fromLx toLy: wx=> <mask>.\nmT0base Translate fromLx toLy: wx=> <mask>.\nmT0large Q: What is theLy translation ofwx A:\nmT0xl How do you say theLx word 'wx' inLy?\nmT0xxl How do you say theLx word 'wx' inLy?\nXGLM564M TheLx wordwx inLy is:\nXGLM1.7B TheLx wordwx inLy is:\nXGLM2.9B TheLx wordwx inLy is:\nXGLM4.5B TheLx wordwx inLy is:\nXGLM7.5B TheLx wordwx inLy is:\nmGPT Translate theLx wordwx intoLy:\nLLaMA7B TheLx wordwx inLy is:\nLLaMA13B Translate fromLx toLy: wx=>\nLLM Best Template (Few-Shot)\nmT5small The wordwx1 inLy iswy\n1. The wordwx2 inLy iswy\n2. The wordwx inLy is <mask>.\nmT5base The wordwx1 inLy iswy\n1. The wordwx2 inLy iswy\n2. The wordwx inLy is <mask>.\nmT5large TheLx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is <mask>.\nmT5xl TheLx wordwx1 inLy iswy\n1, TheLx wordwx2 inLy iswy\n2, TheLx wordwx inLy is <mask>.\nmT5xxl TheLx wordwx1 inLy iswy\n1, TheLx wordwx2 inLy iswy\n2, TheLx wordwx inLy is <mask>.\nmT0small The word 'wx1' inLy iswy\n1. The word 'wx2' inLy iswy\n2. How do you say 'wx' inLy?\nmT0base TheLx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is\nmT0large TheLx word 'wx1' inLy is 'wy\n1'. TheLx word 'wx2' inLy is 'wy\n2'. TheLx word 'wx' inLy is '<mask>'.\nmT0xl TheLx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is:\nmT0xxl TheLx wordwx1 inLy iswy\n1, TheLx wordwx2 inLy iswy\n2, TheLx wordwx inLy is:\nXGLM564M The wordwx1 inLy iswy\n1. The wordwx2 inLy iswy\n2. The wordwx inLy is\nXGLM1.7B TheLx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is\nXGLM2.9B TheLx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is\nXGLM4.5B TheLx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is\nXGLM7.5B TheLx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is\nmGPT The Lx wordwx1 inLy iswy\n1. TheLx wordwx2 inLy iswy\n2. TheLx wordwx inLy is\nLLaMA7B TheLx word 'wx1' inLy iswy\n1. TheLx word 'wx2' inLy iswy\n2. TheLx word 'wx' inLy is\nLLaMA13B TheLx word 'wx1' inLy iswy\n1. TheLx word 'wx2' inLy iswy\n2. TheLx word 'wx' inLy is\nTable 12: The best template for each LLM respectively for zero-shot and few-shot prompting. For simplicity, we\npresent only two in-context examples in each template in the few-shot setup.\n9595\n[5K Setup]VECMAPCONTRASTIVEBLImT5smallmT5basemT5largemT5xl mT5xxlmT0smallmT0basemT0largemT0xl mT0xxlXGLM564MXGLM1.7B XGLM2.9B XGLM4.5B XGLM7.5B mGPTLLaMA7B LLaMA13B\nDE→FI 33.59 44.65 7.82 11.79 25.56 31.72 35.63 0.63 4.49 3.91 15.81 27.18 20.55 31.61 35.89 32.97 39.96 19.67 - -FI→DE 38.73 47.03 10.98 17.76 32.79 37.89 41.99 2.52 7.09 6.15 19.81 27.75 16.5 31.48 40.25 40.04 40.15 24.96 - -DE→FR 50.44 55.56 14.08 23.53 39.12 47.1 51.07 2.45 10.28 11.95 28.95 36.83 19.72 34.95 43.09 49.5 48.57 37.04 56.1359.68FR→DE 47.75 53.29 12.78 21.11 37.35 41.54 44.49 2.17 7.71 7.4 21.68 32.95 14.38 27.42 40.82 45.89 42.89 31.45 51.5853.75DE→HR 32.08 42.41 - - - - - - - - - - - - - - - - 39.65 44.08HR→DE 37.24 48.29 - - - - - - - - - - - - - - - - 43.08 46.71DE→IT 50.55 54.77 12.26 22.27 36.67 43.45 48.15 1.41 6.26 7.2 24.36 35.47 18.41 32.81 43.77 48.51 45.23 39.96 53.6357.75IT→DE 47.29 53.8 11.78 19.28 34.88 38.81 43.67 2.58 7.49 7.03 20.31 28.94 14.06 26.1 36.74 41.55 38.4 29.15 47.86 51.52DE→RU 34.38 46.79 8.61 22.33 35.16 41.84 43.24 0.0 0.37 0.26 17.63 31.82 19.77 34.85 41.99 44.08 44.03 32.19 47.5749.45RU→DE 43.32 49.71 10.11 21.9 37.3 41.17 44.21 1.52 4.61 8.38 21.11 29.39 14.56 24.83 40.54 43.84 41.44 31.22 48.3550.39DE→TR 27.18 38.86 7.3 12.31 25.2 29.53 36.41 1.46 4.54 4.49 15.18 21.44 13.15 19.25 28.59 30.41 31.92 22.9 - -TR→DE 29.93 40.95 8.31 13.26 28.75 32.48 38.18 1.6 6.28 5.38 17.2 24.87 11.08 18.8 26.09 32.11 28.86 24.07 - -EN→DE 51.0 57.75 16.8 30.25 45.9 49.35 53.1 3.3 8.55 8.45 25.5 39.35 21.15 38.6 52.5 54.95 50.3 38.05 62.363.45DE→EN 55.24 58.95 19.51 30.26 42.62 49.5 53.94 0.37 9.65 12.0 29.94 38.81 29.11 42.31 47.94 51.64 51.17 40.58 57.28 58.89EN→FI 37.75 47.15 9.6 18.75 34.25 40.8 44.65 1.3 4.35 3.4 19.15 30.25 25.4 39.9 48.1 41.748.6521.3 - -FI→EN 43.51 50.55 10.98 21.97 36.63 43.51 48.82 0.16 4.83 6.36 21.39 34.05 28.64 44.19 50.24 46.1453.1827.59 - -EN→FR 63.1 67.2 23.95 39.5 55.6 62.45 66.8 5.0 15.5 16.95 38.75 50.95 32.35 55.95 63.8 67.8 66.75 50.25 72.9576.25FR→EN 62.75 65.49 27.16 42.42 50.75 57.84 61.41 0.47 14.85 17.95 37.4 47.28 40.87 54.22 57.37 60.99 60.01 50.44 64.8267.3EN→HR 34.05 47.2 - - - - - - - - - - - - - - - - 48.15 54.7HR→EN 39.08 49.08 - - - - - - - - - - - - - - - - 48.13 51.03EN→IT 60.4 65.6 21.45 38.3 54.75 61.75 63.85 2.3 5.9 6.35 34.05 48.0 29.7 51.1 60.65 64.55 62.9 52.4 71.474.0IT→EN 62.17 65.27 20.26 35.35 47.03 54.32 59.43 0.31 10.8 14.42 33.49 43.0 34.68 48.17 53.23 57.73 57.11 48.63 63.05 64.44EN→RU 39.65 50.5 11.85 29.6 45.6 52.8 53.7 0.2 0.35 0.25 22.0 38.5 30.95 46.4 54.0 57.5 55.05 42.05 58.362.3RU→EN 49.35 54.16 12.31 29.39 43.16 51.86 54.58 0.16 4.3 11.05 27.76 36.83 30.12 43.79 52.28 53.95 53.64 40.96 56.158.77EN→TR 32.05 44.75 10.55 20.65 35.45 40.1545.052.65 4.2 3.8 18.6 27.05 16.9 29.35 38.4 40.6 40.85 31.05 - -TR→EN 39.24 44.78 9.42 20.77 32.43 39.51 44.36 0.16 3.83 7.61 18.0 30.4 19.81 33.07 39.24 39.83 42.17 29.71 - -FI→FR 38.26 45.24 8.57 18.5 35.47 42.35 46.51 1.63 6.2 5.83 24.54 32.05 20.91 37.94 46.66 45.4548.7728.69 - -FR→FI 34.3 43.2 6.93 12.42 27.32 34.2 37.61 0.83 3.16 2.64 15.99 25.3 20.49 29.07 39.16 35.28 42.52 19.66 - -FI→HR 31.58 38.31 - - - - - - - - - - - - - - - - - -HR→FI 31.72 39.56 - - - - - - - - - - - - - - - - - -FI→IT 37.99 46.3 10.3 17.5 32.16 39.83 44.51 1.52 6.78 5.25 21.97 30.06 21.86 36.57 44.04 44.0947.0827.75 - -IT→FI 34.32 43.57 7.7 11.73 25.22 32.09 35.09 0.88 4.55 4.29 16.18 23.41 17.26 29.35 36.43 31.47 40.67 18.35 - -FI→RU 34.16 40.99 6.36 16.34 30.53 38.05 41.04 0.05 0.53 0.68 15.55 27.01 19.23 36.1 42.3 39.9944.8224.38 - -RU→FI 33.53 40.91 4.56 13.51 26.72 34.31 35.57 0.37 1.94 2.78 14.14 24.1 16.29 27.45 34.36 34.47 40.81 18.54 - -HR→FR 40.24 49.29 - - - - - - - - - - - - - - - - 46.66 50.34FR→HR 33.21 44.08 - - - - - - - - - - - - - - - - 40.3 46.51HR→IT 40.24 48.97 - - - - - - - - - - - - - - - - 46.87 50.6IT→HR 34.32 44.75 - - - - - - - - - - - - - - - - 38.5 44.75HR→RU 37.98 46.4 - - - - - - - - - - - - - - - - 43.29 46.87RU→HR 39.5 45.47 - - - - - - - - - - - - - - - - 40.07 45.42IT→FR 65.89 67.86 16.43 31.63 49.87 56.59 59.84 2.53 12.56 13.49 34.83 44.03 23.67 41.76 53.44 58.81 55.71 49.04 63.51 66.77FR→IT 64.72 67.2 17.64 33.83 51.37 57.68 60.42 1.5 5.33 5.59 32.64 44.85 23.07 41.54 57.37 60.27 59.18 54.58 65.0868.03RU→FR 47.51 52.7 11.26 24.2 41.12 48.82 51.44 1.31 6.23 9.17 26.51 35.78 21.74 35.57 46.36 52.38 51.13 39.55 55.3258.04FR→RU 38.23 48.06 8.64 23.33 38.18 44.65 47.54 0.0 0.47 0.26 19.61 33.73 25.92 39.89 46.82 50.08 49.77 39.63 51.2254.16RU→IT 46.78 51.96 11.79 26.14 41.59 47.77 49.29 0.89 4.45 7.18 24.83 34.36 23.47 32.58 46.88 50.13 50.6 41.28 53.5457.83IT→RU 38.71 47.49 8.22 22.02 36.9 43.82 46.41 0.1 0.36 0.26 18.66 30.96 22.58 36.9 43.26 47.39 47.75 35.45 46.6151.89TR→FI 28.59 34.77 5.54 8.47 20.18 27.1 29.77 0.43 3.3 2.08 14.11 18.85 13.47 22.79 26.94 25.03 32.8 16.61 - -FI→TR 29.8 35.68 7.67 12.19 22.86 31.036.211.68 5.52 3.84 14.66 20.13 14.61 23.44 31.21 30.22 35.73 20.18 - -TR→FR 36.58 43.88 8.95 17.25 31.79 39.9944.621.38 4.95 5.86 22.63 29.5 15.55 25.83 33.76 37.7 38.82 29.55 - -FR→TR 31.76 42.06 8.48 14.38 30.42 34.97 40.61 2.28 4.81 3.36 16.81 23.49 13.4 22.76 31.71 33.94 34.71 27.78 - -TR→HR 25.99 36.32 - - - - - - - - - - - - - - - - - -HR→TR 27.35 37.09 - - - - - - - - - - - - - - - - - -TR→IT 34.24 42.17 7.35 15.28 30.46 38.1342.551.22 5.32 4.58 19.97 28.12 14.64 24.17 35.46 36.85 37.01 31.63 - -IT→TR 30.7 40.62 8.11 14.21 26.25 31.94 37.26 2.12 5.37 3.36 16.12 22.02 12.61 21.71 31.01 32.09 34.63 25.06 - -TR→RU 26.2 36.16 4.47 13.53 29.39 35.5738.020.0 0.27 0.43 13.63 25.03 13.26 27.32 34.45 34.93 36.47 27.21 - -RU→TR 27.08 35.78 5.97 14.35 27.08 31.6936.091.47 3.25 4.24 14.51 19.91 11.58 19.75 30.23 32.63 35.73 24.62 - -\nTable 13: Full 5-shot BLI results (P@1 ×100%) on 56 XLING BLI directions with 5K seed translation pairs.\nOff-the-shelf LLMs are used without any fine-tuning. ‘-’: a language in the pair is not supported by the LLM.\n9596\n[1K Setup]VECMAPCONTRASTIVEBLImT5smallmT5basemT5largemT5xl mT5xxlmT0smallmT0basemT0largemT0xl mT0xxlXGLM564MXGLM1.7B XGLM2.9B XGLM4.5B XGLM7.5B mGPTLLaMA7B LLaMA13B\nDE→FI 28.33 43.77 6.0 10.8 21.91 28.95 34.12 0.37 3.65 3.6 13.72 23.53 18.78 30.41 35.32 31.56 35.84 18.31 - -FI→DE 35.0 44.93 8.57 13.87 26.38 33.58 36.94 0.53 5.83 3.84 16.5 25.38 16.97 28.74 38.47 35.94 37.2 21.18 - -DE→FR 49.03 54.04 10.12 21.02 35.32 43.56 48.46 1.2 9.6 8.87 24.93 34.38 20.4 36.46 43.77 48.93 47.0 37.66 54.7756.76FR→DE 44.34 48.16 9.57 17.64 32.33 36.37 41.75 0.72 6.47 5.79 18.37 30.21 13.3 25.35 38.23 42.47 39.68 31.4 49.1551.94DE→HR 27.39 40.48 - - - - - - - - - - - - - - - - 36.57 41.78HR→DE 32.82 44.35 - - - - - - - - - - - - - - - - 39.61 43.19DE→IT 48.72 52.53 8.92 16.95 33.23 41.16 44.91 0.89 5.74 6.21 24.26 33.91 19.67 33.91 42.46 45.44 44.03 37.4 51.3855.92IT→DE 44.39 49.66 9.25 16.38 29.15 33.23 40.93 0.83 5.58 5.79 19.79 27.65 13.07 26.3 36.02 39.84 37.62 28.89 45.0650.18DE→RU 25.46 42.83 6.68 20.08 32.45 39.02 39.75 0.05 0.21 0.37 15.6 28.38 19.2 33.39 40.01 43.19 40.38 30.36 45.4947.63RU→DE 39.08 46.99 7.18 20.43 34.15 37.45 41.8 0.26 3.09 6.13 19.02 28.55 14.56 24.46 39.65 41.49 39.03 32.11 44.7448.45DE→TR 23.37 34.85 4.75 9.08 21.28 27.8 32.81 0.83 3.65 3.44 13.2 17.84 12.0 18.78 25.09 27.44 28.9 22.33 - -TR→DE 26.57 37.11 5.96 9.32 23.0 28.43 32.53 0.37 4.58 3.78 13.74 22.2 10.17 17.52 25.03 28.65 27.16 22.26 - -EN→DE 48.2 50.85 15.05 27.1 40.85 46.75 50.0 1.0 7.85 7.7 21.8 35.9 20.85 35.15 51.5 52.45 48.95 35.2 59.062.85DE→EN 54.56 57.75 16.9 28.22 37.19 45.85 50.97 0.21 9.44 9.44 28.12 37.61 28.38 40.74 45.91 50.03 48.41 38.5 54.46 54.98EN→FI 27.95 45.0 6.9 14.6 29.7 36.9 40.45 0.45 3.35 2.9 14.55 25.0 25.3 37.65 46.5 39.5547.3 18.4 - -FI→EN 41.15 48.77 8.88 16.87 28.43 36.0 42.83 0.0 4.1 2.42 17.24 30.27 28.85 41.99 46.77 43.7249.6123.91 - -EN→FR 60.0 62.5 21.25 37.35 51.65 59.6 63.6 2.55 14.2 13.0 33.45 47.3 33.05 53.45 62.2 66.65 65.8 50.1 71.276.0FR→EN 61.41 64.05 22.04 37.09 43.77 53.08 56.54 0.52 14.07 10.97 33.83 45.42 40.25 51.22 55.04 57.63 54.99 48.42 61.1 61.56EN→HR 24.95 42.35 - - - - - - - - - - - - - - - - 45.95 53.4HR→EN 37.45 47.55 - - - - - - - - - - - - - - - - 42.08 45.61EN→IT 57.55 61.05 17.4 33.75 49.35 58.15 59.65 0.9 5.0 5.05 30.55 42.9 29.95 50.65 57.85 63.4 59.8 51.2 69.3573.1IT→EN 60.78 63.67 16.59 31.99 41.24 51.52 54.73 0.21 10.03 7.86 31.16 40.05 34.63 46.67 49.51 54.94 52.14 46.87 56.95 59.22EN→RU 25.05 46.05 11.3 27.2 42.05 49.25 51.5 0.05 0.15 0.15 18.4 33.95 30.55 44.1 52.75 54.7 53.25 41.7 56.6561.15RU→EN 46.2 53.17 8.96 25.3 38.55 45.68 49.19 0.0 3.25 5.03 26.45 35.31 30.59 42.48 49.66 51.44 49.4 38.92 51.65 52.49EN→TR 26.7 41.05 6.0 15.25 29.3 35.942.350.35 3.1 2.8 13.15 20.75 15.25 24.6 33.75 34.6 37.85 27.75 - -TR→EN 37.17 43.24 8.09 15.02 25.99 34.56 40.42 0.16 2.61 2.24 15.65 28.17 18.58 28.65 34.98 36.53 37.27 28.06 - -FI→FR 34.79 43.3 7.3 13.93 27.96 38.05 41.67 1.0 5.1 3.0 21.28 29.53 22.75 37.83 44.98 42.9847.9225.38 - -FR→FI 23.95 40.56 3.67 9.78 24.42 29.33 33.89 0.26 2.79 2.07 12.62 22.87 20.43 31.25 38.49 32.44 39.11 17.54 - -FI→HR 29.9 34.26 - - - - - - - - - - - - - - - - - -HR→FI 27.62 36.14 - - - - - - - - - - - - - - - - - -FI→IT 34.68 42.88 6.67 14.45 26.06 35.21 37.99 0.58 6.04 2.79 18.76 26.75 23.12 37.41 42.2 41.3645.1924.12 - -IT→FI 26.1 41.65 4.6 8.58 21.86 28.79 32.71 0.47 4.5 3.67 13.64 21.19 16.69 29.92 34.99 30.85 37.0 17.21 - -FI→RU 30.27 37.15 4.36 12.93 25.07 33.74 37.52 0.0 0.42 0.21 12.66 24.65 17.71 34.89 40.41 38.3642.9822.39 - -RU→FI 33.11 37.35 2.83 9.8 24.1 30.07 33.05 0.05 1.57 1.83 13.36 21.16 16.61 27.24 34.0 33.2138.0817.18 - -HR→FR 39.14 45.71 - - - - - - - - - - - - - - - - 44.29 47.66FR→HR 27.52 39.68 - - - - - - - - - - - - - - - - 38.33 43.82HR→IT 38.77 46.19 - - - - - - - - - - - - - - - - 44.56 46.92IT→HR 28.68 41.29 - - - - - - - - - - - - - - - - 36.02 43.15HR→RU 36.09 42.14 - - - - - - - - - - - - - - - - 40.08 44.45RU→HR 38.08 41.17 - - - - - - - - - - - - - - - - 39.65 44.58IT→FR 65.06 66.77 15.3 28.27 45.53 53.8 56.64 1.29 11.99 9.46 32.14 40.16 22.89 42.27 54.06 59.53 55.97 48.06 60.72 64.81FR→IT 63.58 65.49 13.45 26.64 45.63 52.97 56.03 0.72 5.38 4.91 30.99 42.47 22.56 44.44 56.18 58.04 57.37 53.91 63.8966.68RU→FR 45.42 51.28 7.81 22.26 38.24 45.89 49.35 0.84 5.87 4.66 25.46 33.68 23.21 36.14 47.15 51.86 49.4 39.76 53.0655.47FR→RU 24.57 43.61 7.09 20.33 34.76 41.96 43.51 0.05 0.16 0.21 16.55 30.01 23.49 38.08 43.82 46.2 46.3 37.56 48.1653.23RU→IT 43.74 49.24 7.96 22.11 37.51 43.27 46.41 0.31 3.2 5.19 23.99 33.32 22.16 35.31 45.99 49.87 47.88 39.03 51.6554.58IT→RU 26.1 43.57 6.51 18.14 33.8 40.36 41.65 0.05 0.31 0.21 15.19 27.24 20.88 36.64 40.98 45.58 44.34 35.61 43.9850.03TR→FI 24.76 32.96 3.94 5.27 16.51 22.95 25.61 0.27 3.04 1.65 11.5 17.84 12.73 22.42 26.14 23.16 29.45 14.27 - -FI→TR 25.8 30.64 4.52 8.36 18.13 23.49 30.06 0.21 3.52 1.94 11.19 18.08 13.82 21.65 28.74 26.1732.7417.34 - -TR→FR 32.85 41.59 6.28 13.84 26.57 36.0 41.37 0.59 3.78 2.5 20.34 28.12 14.91 26.14 33.81 36.42 37.49 30.14 - -FR→TR 25.19 38.44 4.81 10.61 24.88 28.5 34.87 0.52 2.95 2.33 12.52 18.21 13.24 21.31 29.49 29.95 31.66 26.49 - -TR→HR 20.5 32.16 - - - - - - - - - - - - - - - - - -HR→TR 20.67 33.04 - - - - - - - - - - - - - - - - - -TR→IT 31.42 39.19 5.43 10.81 26.2 33.92 38.39 0.53 3.99 2.93 18.0 25.67 12.78 24.92 33.28 35.09 35.57 31.79 - -IT→TR 25.06 37.93 3.98 9.2 22.12 27.03 33.59 0.36 3.15 2.95 13.33 17.73 12.14 19.38 25.68 28.53 29.82 22.95 - -TR→RU 15.55 31.95 3.51 11.87 23.75 32.3735.520.0 0.27 0.0 12.73 23.06 12.41 25.61 31.2 33.87 34.19 25.56 - -RU→TR 18.6 33.05 3.14 10.53 23.78 28.3434.150.1 1.57 2.1 10.63 16.61 10.42 18.07 27.4 30.23 32.16 22.21 - -\nTable 14: Full 5-shot BLI results (P@1 ×100%) on 56 XLING BLI directions with 1K seed translation pairs.\nOff-the-shelf LLMs are used without any fine-tuning. ‘-’: a language in the pair is not supported by the LLM.\n[1K Setup]VECMAPCONTRASTIVEBLImT5smallmT5basemT5largemT5xl mT5xxlmT0smallmT0basemT0largemT0xl mT0xxlXGLM564MXGLM1.7B XGLM2.9B XGLM4.5B XGLM7.5B mGPTLLaMA7B LLaMA13B\nBG→CA 39.66 43.93 4.56 14.25 25.88 34.29 38.67 0.18 3.1 6.19 23.19 31.72 15.83 24.01 35.69 33.06 40.19 - 37.85 41.71CA→BG 33.54 40.06 3.74 11.27 24.48 30.54 36.2 0.11 0.34 0.11 10.03 22.72 15.86 29.8 37.45 33.09 40.23 - 36.6641.53BG→HU 38.77 44.62 2.2 9.66 22.34 31.48 36.17 0.06 1.56 4.98 14.87 25.46 - - - - - 23.61 31.54 36.57HU→BG 36.52 43.03 3.17 11.29 22.64 34.27 36.98 0.0 0.23 0.17 9.97 24.02 - - - - - 28.17 37.5 43.2CA→HU 35.47 41.44 4.64 10.33 19.67 27.18 32.43 0.5 3.92 4.92 12.98 22.21 - - - - - - 30.39 35.3HU→CA 39.88 47.14 6.38 12.65 20.19 34.98 37.68 0.61 5.89 6.88 19.86 29.59 - - - - - - 39.55 46.04Avg. 37.3143.37 4.11 11.58 22.53 32.12 36.36 0.24 2.51 3.88 15.15 25.95 - - - - - - 35.58 40.72\nTable 15: 5-shot BLI results (P@1×100%) on PanLex-BLI with 1K seed translation pairs. Off-the-shelf LLMs are\nused without any fine-tuning. ‘-’: a language in the pair is not supported by the LLM.\n9597\n[Zero-Shot]VECMAPmT5smallmT5basemT5largemT5xl mT5xxlmT0smallmT0basemT0largemT0xl mT0xxlXGLM564M XGLM1.7B XGLM2.9B XGLM4.5B XGLM7.5B mGPTLLaMA7B LLaMA13B\nDE→FI 28.27 6.31 7.25 12.99 23.37 29.63 4.69 4.02 5.63 14.24 22.07 9.39 19.09 26.19 25.6730.1 11.89 - -FI→DE 34.84 7.46 7.83 17.29 22.12 29.53 6.15 5.52 7.36 10.04 23.75 8.99 24.49 31.79 29.4835.2613.14 - -DE→FR 49.199.96 12.73 25.35 34.69 38.03 10.02 6.31 19.72 19.77 30.26 10.75 24.2 29.73 39.44 37.3 27.18 42.41 47.37FR→DE 44.44 10.09 11.23 26.28 34.14 41.33 8.85 7.97 9.0 18.52 31.82 11.43 26.28 33.11 40.82 37.66 22.97 43.1548.11DE→HR 27.49 - - - - - - - - - - - - - - - - 29.06 28.06HR→DE 32.72 - - - - - - - - - - - - - - - - 31.61 35.72DE→IT 48.418.09 12.21 24.47 33.44 37.09 7.62 8.24 9.75 22.12 31.25 11.42 21.96 30.36 38.29 37.09 28.01 42.83 45.96IT→DE 43.93 8.27 9.82 21.65 28.63 36.43 7.29 6.46 8.79 17.67 27.29 9.72 23.2 29.04 34.63 30.23 22.53 38.7644.39DE→RU 25.67 0.42 0.47 20.24 28.33 28.69 0.42 1.72 0.26 14.19 20.76 1.04 19.09 20.6133.85 30.2 13.51 30.46 32.86RU→DE 38.97 1.78 8.17 24.93 33.47 41.33 3.72 4.92 7.75 16.87 28.6 6.71 15.87 32.11 37.82 35.31 15.9843.48 43.11DE→TR 23.79 5.01 6.89 15.28 18.6226.44.43 4.96 5.37 11.53 18.1 4.28 10.12 16.69 19.51 19.82 11.48 - -TR→DE 26.467.08 6.98 13.74 18.58 24.81 7.24 6.6 6.92 10.22 19.33 4.69 14.59 17.73 24.17 21.83 14.38 - -EN→DE 48.45 11.55 11.85 35.35 42.5 50.5 10.6 9.4 10.55 24.0 39.3 15.55 32.05 43.7 49.75 46.4 23.95 52.0554.5DE→EN 54.5115.6 17.37 23.47 36.25 39.23 14.14 11.06 24.62 29.16 34.95 15.86 26.71 36.31 41.89 31.3 21.91 42.57 47.0EN→FI 28.15 5.25 5.85 17.35 29.1 37.15 4.2 4.25 5.25 15.65 27.45 9.3 21.65 34.35 31.137.7 12.1 - -FI→EN 41.048.04 8.67 14.98 23.7 32.0 5.94 5.2 13.45 19.02 27.27 8.72 17.03 39.78 29.48 39.57 14.14 - -EN→FR 60.1 16.95 19.95 43.65 51.95 56.25 16.9 11.55 29.7 22.7 32.25 19.5 38.7 50.95 56.25 52.15 35.9 57.662.7FR→EN 61.5123.64 25.25 31.97 44.9 47.44 21.68 17.74 35.8 37.97 44.49 22.56 38.7 47.08 50.85 39.32 38.13 54.53 57.32EN→HR 24.1 - - - - - - - - - - - - - - - - 35.85 28.6HR→EN 36.3 - - - - - - - - - - - - - - - - 31.83 37.35EN→IT 57.4 8.05 13.25 38.4 46.95 54.2 7.25 9.55 11.05 31.1 44.3 17.8 35.4 47.85 53.95 50.55 37.85 58.960.4IT→EN 60.7814.37 17.57 25.48 37.47 42.58 16.33 12.61 27.91 33.02 37.88 19.12 31.52 39.64 43.82 35.09 26.25 47.39 54.88EN→RU 25.1 0.35 0.4 31.85 38.6 42.8 0.4 1.95 0.25 19.0 30.05 1.6 24.65 37.3544.1 42.3 16.7 42.15 40.15RU→EN 46.41 2.25 12.99 23.83 36.88 43.37 4.71 5.97 23.57 27.29 37.77 8.85 17.65 40.86 42.43 39.65 16.82 46.250.18EN→TR 26.5 4.05 5.3 20.0 26.9535.053.9 4.9 4.65 13.05 24.9 5.2 9.4 21.8 22.35 27.1 10.55 - -TR→EN 36.9 6.82 7.45 13.26 22.47 27.9 5.96 3.73 12.78 17.25 21.88 5.91 15.12 24.81 26.46 25.99 7.14 - -FI→FR 35.215.25 7.04 15.34 24.38 27.69 5.25 2.89 10.35 13.82 19.81 8.25 24.44 33.74 30.16 35.0 16.13 - -FR→FI 22.56 4.76 5.43 11.9 22.81 29.38 3.1 3.26 4.86 14.23 22.56 7.24 19.61 27.57 26.5430.7811.48 - -FI→HR 28.8 - - - - - - - - - - - - - - - - - -HR→FI 27.72 - - - - - - - - - - - - - - - - - -FI→IT 34.79 6.73 6.88 13.61 20.23 26.85 6.2 6.57 6.94 15.03 23.54 9.67 24.28 33.79 28.937.0516.45 - -IT→FI 25.99 5.89 6.61 8.32 18.9726.464.34 3.77 5.58 11.63 18.86 6.82 18.6 24.6 23.57 25.53 12.04 - -FI→RU 31.110.32 0.42 14.87 20.6 24.8 0.37 1.21 0.26 10.61 20.34 0.53 22.23 23.44 26.33 26.33 8.41 - -RU→FI 32.741.0 4.87 12.94 21.06 31.74 0.84 1.68 3.2 12.0 20.53 4.98 9.64 16.34 24.88 29.65 7.39 - -HR→FR 38.82 - - - - - - - - - - - - - - - - 31.51 36.24FR→HR 26.95 - - - - - - - - - - - - - - - - 31.61 31.09HR→IT 39.03 - - - - - - - - - - - - - - - - 34.46 38.51IT→HR 28.84 - - - - - - - - - - - - - - - - 28.58 28.84HR→RU 36.24 - - - - - - - - - - - - - - - - 30.51 31.77RU→HR 38.71 - - - - - - - - - - - - - - - - 34.31 10.9IT→FR 65.229.46 15.66 29.51 41.71 47.86 13.28 9.25 26.46 25.27 35.97 13.44 32.97 38.6 46.87 41.91 42.27 51.47 57.42FR→IT 63.428.28 14.9 35.33 45.42 49.92 7.97 9.73 12.83 32.64 41.85 16.09 33.73 46.2 52.46 49.46 44.75 57.17 60.01RU→FR 45.31 1.73 10.74 28.03 37.98 42.01 4.24 3.82 20.38 21.32 30.17 6.65 16.19 31.85 40.6 38.92 24.62 43.6447.98FR→RU 23.59 0.36 0.41 24.16 31.14 30.78 0.31 1.66 0.26 15.73 24.47 2.33 22.66 28.04 36.83 33.32 17.07 35.838.59RU→IT 43.84 1.52 11.31 25.25 36.09 41.49 3.35 5.61 9.27 21.79 32.22 8.96 14.09 34.36 42.54 38.87 28.5547.2 45.99IT→RU 25.94 0.41 0.41 19.84 26.46 28.48 0.41 1.96 0.26 13.18 22.74 1.5 20.83 20.52 31.01 26.72 18.55 31.5235.45TR→FI 19.914.47 4.74 6.44 13.68 18.69 3.35 3.73 4.42 9.53 15.02 3.51 13.37 16.24 18.64 17.31 9.32 - -FI→TR 20.974.26 5.52 9.56 13.87 19.76 3.21 4.15 5.1 8.57 16.97 4.68 12.45 18.97 17.13 20.55 6.36 - -TR→FR 32.965.11 6.44 14.7 22.36 23.8 4.47 3.51 11.82 12.14 17.31 3.78 15.02 18.21 27.21 24.65 20.29 - -FR→TR 25.04 3.83 5.59 15.57 20.9527.473.21 4.29 4.66 10.5 20.18 3.93 9.36 17.9 20.28 20.28 11.38 - -TR→HR 21.14 - - - - - - - - - - - - - - - - - -HR→TR 20.62 - - - - - - - - - - - - - - - - - -TR→IT 31.845.43 6.39 12.03 20.29 24.81 5.17 5.59 7.77 14.54 20.39 4.9 14.43 20.39 26.2 25.35 21.88 - -IT→TR 25.064.5 5.79 13.64 17.67 24.91 3.41 4.13 4.96 10.23 18.04 4.29 10.59 16.59 18.29 18.6 13.49 - -TR→RU 12.09 0.43 0.48 11.4 19.33 20.98 0.48 1.28 0.27 7.67 16.24 0.16 14.0 11.0822.95 14.32 11.61 - -RU→TR 15.61 1.0 4.71 15.87 20.9527.971.41 2.99 3.46 10.27 16.5 2.2 4.19 16.92 19.02 20.32 8.75 - -\nTable 16: Full zero-shot BLI results (P@1×100%) on 56 XLING BLI directions without any seed translation pair\n(our zero-shot setup is also known as the unsupervised BLI setup for CLWE-based approaches). Off-the-shelf LLMs\nare used without any fine-tuning. ‘-’: a language in the pair is not supported by the LLM.\n[Zero-Shot]VECMAPmT5smallmT5basemT5largemT5xl mT5xxlmT0smallmT0basemT0largemT0xl mT0xxlXGLM564M XGLM1.7B XGLM2.9B XGLM4.5B XGLM7.5B mGPTLLaMA7B LLaMA13B\nBG→CA 39.6 0.82 4.79 17.23 25.06 28.04 2.16 1.4 15.19 15.6 28.86 5.55 7.24 22.72 25.29 28.5 - 32.83 32.77CA→BG 33.6 0.4 0.51 2.66 15.64 21.47 0.4 0.74 0.45 6.86 16.83 0.4 16.26 21.93 19.55 20.17 - 26.35 27.03BG→HU 39.240.93 5.32 14.7 21.3 27.26 1.45 4.63 7.58 12.04 24.07 - - - - - 7.23 24.19 23.61HU→BG 36.460.35 0.29 0.92 17.8 22.47 0.23 0.58 0.23 5.93 16.94 - - - - - 9.85 28.34 26.5CA→HU 34.096.85 8.34 13.31 21.55 24.59 6.8 8.34 7.57 12.54 22.93 - - - - - - 23.7 24.53HU→CA 37.79 6.6 6.27 16.72 24.2 25.47 8.64 4.84 14.03 14.8 24.48 - - - - - - 32.62 38.17Avg. 36.8 2.66 4.25 10.92 20.92 24.88 3.28 3.42 7.51 11.3 22.35 - - - - - - 28.0 28.77\nTable 17: Zero-shot BLI results (P@1 ×100%) on PanLex-BLI without any seed translation pair (our zero-shot\nsetup is also known as the unsupervised BLI setup for CLWE-based approaches). Off-the-shelf LLMs are used\nwithout any fine-tuning. ‘-’: a language in the pair is not supported by the LLM.\nmT5smallmT5basemT5largemT5xl mT5xxlmT0smallmT0basemT0largemT0xl mT0xxlXGLM564M XGLM1.7B XGLM2.9B XGLM4.5B XGLM7.5B mGPTLLaMA7B LLaMA13B\nNN (5K) 14.84 28.33 43.25 49.66 52.83 1.43 6.8 8.28 27.0 38.09 24.51 39.49 49.14 53.08 51.58 41.2 57.33 60.23NN (1K) 11.97 24.91 38.84 45.95 49.37 0.63 6.08 5.85 24.5 35.42 24.18 38.86 47.64 51.18 49.09 40.13 54.62 57.85Random (5K) 11.34 23.86 36.53 42.83 46.93 0.21 5.73 4.16 23.36 34.97 21.17 36.44 46.65 49.85 48.03 38.88 53.32 56.85Random (1K) 10.45 22.45 35.77 42.55 45.93 0.18 5.4 3.75 22.0 33.9 21.37 36.01 46.0 49.92 47.37 38.38 52.56 56.11Zero-Shot (Unsupervised) 7.66 11.33 27.95 37.35 41.99 7.97 7.37 14.41 23.17 32.92 11.04 25.82 35.91 42.91 38.69 26.18 45.46 48.72\nTable 18: Full ablation results with all our18 mLLMs. Avg. BLI scores (P@1×100%) on 20 XLING BLI directions.\nRow 1 ∼2: five-shot prompting with in-context examples extracted from nearest neighbours (NN) in DS of size\n5K and 1K. Row 3 ∼4: five-shot prompting with random in-context examples in DS of size 5K and 1K. Row 5:\nzero-shot prompting without any in-context examples.\n9598\nGround-Truth Translation PairMethod In-Context Examples Prediction\nbrisati (HR)→erase(EN) 0-Shot - break5-Shot izbrisati →delete, kopirati→copy, ispraviti→amend, dopuniti→supplement, pisati→write erase\njestiv (HR)→edible(EN) 0-Shot - joyful5-Shot jesti →eat,ˇcešnjak→garlic, povr´ce→vegetable, grašak→peas, biljka→plant edible\nslap (HR)→waterfall(EN) 0-Shot - slap5-Shot slapovi →falls, kanjon→canyon, jezero→lake, potok→creek, potoci→streams waterfall\nvalcer (HR)→waltz(EN) 0-Shot - valve5-Shot orkestar →orchestra, koncert→concert, balet→ballet, simfonija→symphony, klavir→piano waltz\nšaran (HR)→carp(EN) 0-Shot - shark5-Shot pastrva →trout, riba→fish, ribolovac→fisher, gavran→raven, bizaran→bizarre carp\nsove (HR)→owls(EN) 0-Shot - slept5-Shot sova →owl, životinje→animals, ptica→bird, maˇcke→cats, životinja→animal owls\ngušter (HR)→lizard(EN) 0-Shot - gusher5-Shot štakor →rat, zmija→snake, zvijer→beast, kornjaˇca→turtle, majmun→monkey lizard\nsezam (HR)→sesame(EN) 0-Shot - cumin5-Shot grašak →peas, rajˇcica→tomato, povr´ce→vegetable, rajˇcice→tomatoes, smokva→fig sesame\nodgo¯dena (HR)→postponed(EN) 0-Shot - postponed5-Shot odgoditi →delay, bila→been, trebala→supposed, najave→announcements, provedena→conducted delayed\ntajnost (HR)→secrecy(EN) 0-Shot - secrecy5-Shot krajnost →extreme, odanost→loyalty, odluˇcnost→resolve, tajna→secret, toˇcnost→accuracy discretion\nprognoza (HR)→prognosis(EN) 0-Shot - prognosis5-Shot prognoze→forecasts, dijagnoza→diagnosis, terapija→therapy, lijeˇcenje→treatment, astma→asthma forecasts\notapanje (HR)→dissolution(EN) 0-Shot - dissolution5-Shot doziranje →dosage, kiseline→acids, filtriranje→filtering, dobivanje→obtaining, curenje→leak melting\nsopportare (IT)→endure(EN) 0-Shot - support5-Shot alleviare →ease, portare→bring, superare→exceed, odiare→hate, minimizzare→minimize endure\nsnello (IT)→slender(EN) 0-Shot - quick5-Shot flessibile →flexible, coda→tail, sottile→subtle, leggermente→slightly, aspetto→appearance slender\nhindi (IT)→hindi(EN) 0-Shot - english5-Shot india →india, indiano→indian, tailandese→thai, lingua→tongue, arabo→arabic hindi\nvelo (IT)→veil(EN) 0-Shot - bicycle5-Shot cappello →hat, mantellina→cape, viso→face, cuscino→pillow, vestito→dress veil\nprugna (IT)→plum(EN) 0-Shot - prickly5-Shot ciliegia →cherry, vaniglia→vanilla, zenzero→ginger, marmellata→jam, aglio→garlic plum\npropano (IT)→propane(EN) 0-Shot - methane5-Shot idrogeno →hydrogen, acido→acid, carbonio→carbon, combustibili→fuels, polimero→polymer propane\npomello (IT)→knob(EN) 0-Shot - apple5-Shot maniglia →handle, regolabile→adjustable, maniglie→handles, coperchio→lid, cinghia→strap knob\naltopiano (IT)→plateau(EN) 0-Shot - high5-Shot montagne →mountains, pianure→plains, vallata→dale, pianura→plain, valle→valley plateau\neventuale (IT)→eventual(EN) 0-Shot - eventual5-Shot necessario→necessary, valutare→gauge, possibile→possible, necessaria→needed, richiedere→require possible\nscopre (IT)→discovers(EN) 0-Shot - discovers5-Shot scoprire →discover, crede→believes, chiede→asking, spiega→explains, ragazza→girl discover\nrane (IT)→frogs(EN) 0-Shot - frogs5-Shot topi →mice, animali→animals, rana→frog, mosche→flies, creature→creatures snakes\nsorprese (IT)→surprises(EN) 0-Shot - surprises5-Shot sorpresa→surprise, inaspettato→unexpected, sorprendente→surprising, aspettandosi→expecting, aspettative→expectations surprised\nTable 19: Translation examples on HR→EN and IT→EN. We include here ground truth translation pairs and show\nthe predictions derived from zero-shot and five-shot prompting with LLaMA13B.\n9599"
}