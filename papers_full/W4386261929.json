{
  "title": "PEER: Empowering Writing with Large Language Models",
  "url": "https://openalex.org/W4386261929",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4318466460",
      "name": "Kathrin Seßler",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A1960361089",
      "name": "Tao Xiang",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A4280893796",
      "name": "Lukas Bogenrieder",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A1926909824",
      "name": "Enkelejda Kasneci",
      "affiliations": [
        "Technical University of Munich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2560140854",
    "https://openalex.org/W2955427418",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W47778786",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3201077663",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W1597864774"
  ],
  "abstract": "Abstract The emerging research area of large language models (LLMs) has far-reaching implications for various aspects of our daily lives. In education, in particular, LLMs hold enormous potential for enabling personalized learning and equal opportunities for all students. In a traditional classroom environment, students often struggle to develop individual writing skills because the workload of the teachers limits their ability to provide detailed feedback on each student’s essay. To bridge this gap, we have developed a tool called PEER (Paper Evaluation and Empowerment Resource) which exploits the power of LLMs and provides students with comprehensive and engaging feedback on their essays. Our goal is to motivate each student to enhance their writing skills through positive feedback and specific suggestions for improvement. Since its launch in February 2023, PEER has received high levels of interest and demand, resulting in more than 4000 essays uploaded to the platform to date. Moreover, there has been an overwhelming response from teachers who are interested in the project since it has the potential to alleviate their workload by making the task of grading essays less tedious. By collecting a real-world data set incorporating essays of students and feedback from teachers, we will be able to refine and enhance PEER through model fine-tuning in the next steps. Our goal is to leverage LLMs to enhance personalized learning, reduce teacher workload, and ensure that every student has an equal opportunity to excel in writing. The code is available at https://github.com/Kasneci-Lab/AI-assisted-writing .",
  "full_text": "PEER: Empowering Writing with Large\nLanguage Models\nKathrin Seßler(B) , Tao Xiang, Lukas Bogenrieder, and Enkelejda Kasneci\nTechnical University of Munich, Munich, Germany\nkathrin.sessler@tum.de\nAbstract. The emerging research area of large language models (LLMs)\nhas far-reaching implications for various aspects of our daily lives. In edu-\ncation, in particular, LLMs hold enormous potential for enabling person-\nalized learning and equal opportunities for all students. In a traditional\nclassroom environment, students often struggle to develop individual\nwriting skills because the workload of the teachers limits their ability\nto provide detailed feedback on each student’s essay. To bridge this gap,\nwe have developed a tool called PEER (Paper Evaluation and Empower-\nment Resource) which exploits the power of LLMs and provides students\nwith comprehensive and engaging feedback on their essays. Our goal is\nto motivate each student to enhance their writing skills through positive\nfeedback and speciﬁc suggestions for improvement. Since its launch in\nFebruary 2023, PEER has received high levels of interest and demand,\nresulting in more than 4000 essays uploaded to the platform to date.\nMoreover, there has been an overwhelming response from teachers who\nare interested in the project since it has the potential to alleviate their\nworkload by making the task of grading essays less tedious. By collecting\na real-world data set incorporating essays of students and feedback from\nteachers, we will be able to reﬁne and enhance PEER through model\nﬁne-tuning in the next steps. Our goal is to leverage LLMs to enhance\npersonalized learning, reduce teacher workload, and ensure that every\nstudent has an equal opportunity to excel in writing. The code is avail-\nable at https://github.com/Kasneci-Lab/AI-assisted-writing.\nKeywords: Large Language Models\n· Writing · Personalized\nEducation\n1 Introduction\nThe introduction of transformers-based technologies [ 13] for natural language\nprocessing (NLP) has been a breakthrough that pushed the ﬁeld signiﬁcantly for-\nward. It enabled the development of pre-trained large language models (LLMs)\nwhich can process natural language more eﬀectively and eﬃciently than pre-\nvious approaches [1,10]. The most recent models, like ChatGPT [8], have been\nﬁne-tuned using reinforcement learning with human feedback [9], enhancing their\nability to generate human-like conversations and leading to a wide range of novel\nc⃝ The Author(s) 2023\nO. Viberg et al. (Eds.): EC-TEL 2023, LNCS 14200, pp. 755–761, 2023.\nhttps://doi.org/10.1007/978-3-031-42682-7\n_73\n756 K. Seßler et al.\napplications and use cases in various domains, also in the ﬁeld of education [5].\nSince LLMs are trained to write high quality texts, they can assist users in their\nwriting process [15]. More speciﬁcally, LLM-based tools can help improve writing\nskills already from the very young age up to professional writing.\nDuring their academic years, students are learning various types of essays.\nHowever, in the traditional classroom setting, teachers are not able to provide\ndetailed feedback for each student’s work due to time constraints and heavy\nworkload. Also, feedback is usually only given once (e.g. in the context of graded\nhomework or assessments) without further possibility to enhance the writing\nafterwards and receive an updated feedback, impeding a continuous process of\nimprovement.\nTo tackle this challenge in essay writing education, and hence support both\nlearners and teachers, we have developed an AI-based tutor named PEER, Paper\nEvaluation and Empowerment Resource. The idea behind PEER is to oﬀer com-\nprehensive textual feedback on the learner’s essay, including speciﬁc suggestions\nfor improvement, while being always constructive, speciﬁc and engaging. This\nstands in contrast to previous work, where the focus was often on merely grading\nthe essay rather than oﬀering comprehensive feedback [ 11]. PEER also allows\nstudents to make adjustments to their work and receive updated feedback to\nprovide an ongoing process of improvement. From an educator’s perspective,\nPEER provides an initial structure and suggestions for constructive and thor-\nough feedback that can serve as a basis for further enhancements by the teacher.\nSuch AI-assisted feedback can save a lot of time and energy, reducing hence the\nteacher’s workload and oﬀering more space for interaction with the students.\nExisting work that uses LLMs to improve writing skills often focuses more on\ngeneral strategies and does not have a concrete focus on the diﬀerent essay types\nthat are part of the school curriculum with their demands and challenges [12].\nThe diﬃculty of this project lies in the limited availability of student’s essay\ndata and corresponding feedback of the teachers. Due to privacy reasons, such\ndata is typically not publicly available in vast amounts, making it less likely\nfor large language models to have encountered this type of data during their\npre-training phase. However, the use of LLMs eliminates the need for costly\nhand-crafted features that previously formed the basis of many automated essay\nscoring systems [4]. PEER explores how the capabilities of LLMs can be lever-\naged to assist students and teachers in personalizing essay writing education,\nwhich involves teaching the model to provide reasonable and helpful feedback\ntailored to diﬀerent types of essays.\nWe argue that by interacting with PEER, a learner can gain the necessary\nskills to comprehend the essential elements of good essay writing, and thereby\nenhance their own writing abilities. In order to evaluate the eﬃcacy and implica-\ntions of PEER, we have initiated collaborations with various German educators,\nschools, and academic institutions who expressed interest in our endeavor.\nWhile the current version of PEER has been designed speciﬁcally to hone\nwriting abilities in the German language and for the German educational sys-\ntem, we envision expanding its applicability to encompass other pedagogical\nframeworks and languages.\nPEER: Empowering Writing with Large Language Models 757\n2 PEER\nPEER is a user-friendly, web-based tool designed to analyze students’ essays\nand generate comprehensive feedback that includes concrete suggestions and\nengaging tips to improve writing.\n2.1 Pedagogical Background\nProviding eﬀective feedback is crucial for enhancing students’ learning experi-\nence [3], as it can signiﬁcantly impact their performance and motivation [ 7].\nHowever, traditional classroom settings often do not allow teachers to provide\ncomprehensive and engaging feedback to each student, hindering the learning\nprocess. To address these challenges, PEER adopts two main approaches:\n1. Thorough and Constructive Feedback : PEER provides detailed and con-\nstructive feedback that is always positive, engaging, and helps to motivate\nstudents. The feedback highlights the strengths of the students’ work and pro-\nvides suggestions for improvement. PEER also uses a visual indicator (green\ninstead of red color) to represent feedback that is encouraging and supportive.\n2. Continuous Learning Environment : PEER follows a continuous learning\nprinciple that allows students to receive feedback as many times as they\nrequire without any fear of being discouraged or judged. This approach fosters\na safe and barrier-free learning environment where students can learn at their\nown pace, and feedback is always readily available.\n2.2 Technical Background\nOur approach aims to provide students with comprehensive and constructive\nfeedback on their essays. In the absence of a task-speciﬁc data set, we achieve\nthis by combining zero-shot learning (inference without prior training only based\non semantic information) with Elo ratings (numerical system to compare the\nrelative performance) for diﬀerent prompts. Through collecting feedback from\nthe users on the quality of the output, we can continuously improve our model.\nZero-Shot Learning. Traditionally, machine learning models have been\ntrained to perform on speciﬁc tasks. One signiﬁcant advantage of LLMs is their\nversatility [10], including zero-shot learning, which is the ability to follow textual\ninstructions [1]. Finding the optimal instructions already evolved into an own\nﬁeld of research [6,14] and several heuristics can aid in crafting suitable prompts.\nBy providing the right instructions, LLMs can be guided to generate feedback\nthat closely resembles that of a teacher. For example, a possible prompt can look\nlike the following (translated from German into English):\nT h e following text is a { article type } from a student in the\n{ year } th grade . { extra infos } The t o p i c i s ” { title } ”. Text:\n” { essay } ”. Analyze the text based on the criteria and give feedback\nand suggestions for improvement l i k e a teacher .\n758 K. Seßler et al.\nPrompt Elo Rating. Our approach involves systematically identifying the\noptimal model instructions from a set of prompts through user feedback, utiliz-\ning the Elo rating system [ 2]. When a user requests feedback on an essay, two\ndiﬀerent responses generated from distinct instructional prompts are presented.\nBased on the user’s preference, the ratings for the corresponding instructional\nprompts are updated using the Elo system. By incorporating human feedback\ninto the process, our system continuously improves, guided by the success of rein-\nforcement learning with human feedback deployed to ﬁne-tune ChatGPT [9].\nWeighted Lottery System. When generating new feedback, the two instruc-\ntional prompts that wrap the user’s essay are selected using a weighted lottery\nsystem based on their respective Elo ratings. This ensures that prompts with\nhigher ratings have a greater chance of being chosen from the set, resulting in\nfeedback that is more likely to be of superior quality. At the same time, the use\nof a random selection process ensures that all prompts are evaluated.\n3P r o t o t y p e\nTo make PEER available for all students, we have developed a website and are\ncurrently working on an accompanying application, in order to further reduce\nthe barriers to access and ensure widespread availability (Fig.1).\nFig. 1. (a) The Start Page. In the ﬁrst step, the topic of the essay and the relevant\nmeta data is entered. (b) The Feedback Page. The user is provided with two feedback\ntexts and can mark the preferred one to improve our model.\nIn the ﬁrst step, users can input the topic or title of their essay, along with\nrelevant meta information such as essay type, school year, and school type. Then,\nthey can choose to insert the text manually or upload an image, which is scanned\nusing an OCR and post-processed by GPT-3 [ 1] to remove any artifacts from\nthe image. Next, PEER evaluates the input and generates two feedback texts.\nThe users are then encouraged to indicate which feedback they ﬁnd more useful.\nTo facilitate continuous learning and improvement, the users can modify their\nPEER: Empowering Writing with Large Language Models 759\nessay according to the feedback and request new feedback. This process can be\nrepeated as many times as necessary to enhance the writing skills of the users.\n4 Preliminary Results\nOver 4000 essays have already been uploaded for evaluation, with argumentation\nbeing the most frequently requested category. The platform was primarily used\nby students from middle and upper levels.\nPrompt Evaluation. Based on the Elo scores, prompting the model to approach\nthe task as a friendly teacher and providing it with additional information about\nthe speciﬁc essay type leads to the best results. It enables the model to focus on\nthe relevant characteristics and use the extra information to improve and adapt\nits feedback accordingly.\nFeedback from Teachers. The quantitative results are complemented by feed-\nback from teachers who assessed the tool from a qualitative perspective. Several\nteachers reported to us their experiences of trying PEER themselves as well\nas applying it together with their students in their classrooms. Overall, they\nacknowledged PEER’s usefulness for both students and teachers, highlighting\nits user-friendliness, respectful tone, and timely feedback that facilitates indi-\nvidualized learning. However, they also identiﬁed some areas for improvement.\nFor instance, they noted that the feedback provided by PEER can be too general\nat times, such as suggesting to “use more adjectives.” Additionally, one teacher\npointed out that in the German language, both the male and female forms for\nprofessions are typically used to be inclusive. Unfortunately, PEER currently\ndoes not account for both versions in its output text, and sometimes even marks\nthem as redundant in students’ essays. Other criticisms included missing essay\ntypes on the start page and sometimes small grammar errors in the generated\nfeedback texts. This initial qualitative assessment and teacher feedback is incor-\nporated in the further development of PEER before a more comprehensive and\nlarger user study is conducted.\n5 Conclusion and Future Agenda\nBased on the amount of feedback we have received from teachers so far, it is\nclear that PEER is meeting a need in schools for both learners and teachers.\nHowever, our project is still in its early stages and requires further development,\nmodel ﬁne-tuning and user evaluations. As our objective is to bring PEER to\nschools and establish it as a valuable assistant in the process of learning how to\nwrite an essay, our concrete current and next steps are as follows:\n– Creating a solid data basis consisting of real-world essays and high-quality\nfeedback provided by teachers. This data set will then be used to ﬁne-tune a\nlarge language model for our speciﬁc domain to improve the performance.\n760 K. Seßler et al.\n– Conducting a user study at schools to assess the performance of PEER and\ngather valuable insights into the positive and negative aspects of the tool for\nboth teachers and students.\nGiven the inherent stochastic nature of the underlying model, it is important\nto acknowledge that a fully error-free outcome cannot be guaranteed. Conse-\nquently, and as for all applications based on large language models, users are\nadvised to carefully evaluate the feedback provided and selectively incorporate\nonly the pertinent critiques. This type of critical thinking is not limited to PEER\nbut should be a fundamental aspect of interacting with any generative AI model.\nReferences\n1. Brown, T., et al.: Language models are few-shot learners. In: Advances in Neural\nInformation Processing Systems, vol. 33, pp. 1877–1901 (2020)\n2. Elo, A.E.: The Rating of Chessplayers, Past and Present. Arco Pub., New York\n(1978)\n3. Hattie, J., Timperley, H.: The power of feedback. Rev. Educ. Res. 77(1), 81–112\n(2007)\n4. Hussein, M.A., Hassan, H., Nassef, M.: Automated language essay scoring systems:\na literature review. PeerJ Comput. Sci. 5, e208 (2019)\n5. Kasneci, E., et al.: ChatGPT for good? On opportunities and challenges of large\nlanguage models for education. Learn. Individ. Diﬀ. 103, 102274 (2023)\n6. Liu, J., Shen, D., Zhang, Y., Dolan, W.B., Carin, L., Chen, W.: What makes\ngood in-context examples for GPT-3? In: Proceedings of Deep Learning Inside\nOut (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration\nfor Deep Learning Architectures, pp. 100–114 (2022)\n7. Molloy, E.K., Boud, D.: Feedback models for learning, teaching and performance.\nIn: Handbook of Research on Educational Communications and Technology, pp.\n413–424 (2014)\n8. OpenAI Team: ChatGPT: optimizing language models for dialogue (2022)\n9. Ouyang, L., et al.: Training language models to follow instructions with human\nfeedback. In: Advances in Neural Information Processing Systems, vol. 35, pp.\n27730–27744 (2022)\n10. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language\nmodels are unsupervised multitask learners. OpenAI Blog 1(8), 9 (2019)\n11. Ramesh, D., Sanampudi, S.K.: An automated essay scoring systems: a systematic\nliterature review. Artif. Intell. Rev. 55(3), 2495–2527 (2022)\n12. Schick, T., et al.: PEER: a collaborative language model. arXiv preprint\narXiv:2208.11663 (2022)\n13. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems, vol. 30 (2017)\n14. Wei, J., et al.: Chain-of-thought prompting elicits reasoning in large language mod-\nels. In: Advances in Neural Information Processing Systems (2022)\n15. Yuan, A., Coenen, A., Reif, E., Ippolito, D.: Wordcraft: story writing with large\nlanguage models. In: 27th International Conference on Intelligent User Interfaces,\npp. 841–852 (2022)\nPEER: Empowering Writing with Large Language Models 761\nOpen Access This chapter is licensed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium\nor format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license and indicate if changes were\nmade.\nThe images or other third party material in this chapter are included in the\nchapter’s Creative Commons license, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the chapter’s Creative Commons license and\nyour intended use is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright holder.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6744964122772217
    },
    {
      "name": "Workload",
      "score": 0.6151626110076904
    },
    {
      "name": "Peer feedback",
      "score": 0.5891973376274109
    },
    {
      "name": "Grading (engineering)",
      "score": 0.5287144780158997
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4325733780860901
    },
    {
      "name": "Mathematics education",
      "score": 0.43141692876815796
    },
    {
      "name": "Multimedia",
      "score": 0.38294586539268494
    },
    {
      "name": "Psychology",
      "score": 0.18477007746696472
    },
    {
      "name": "Artificial intelligence",
      "score": 0.17597559094429016
    },
    {
      "name": "Engineering",
      "score": 0.1166735291481018
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ]
}