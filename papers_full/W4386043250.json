{
  "title": "Contrasting Linguistic Patterns in Human and LLM-Generated News Text",
  "url": "https://openalex.org/W4386043250",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4287145520",
      "name": "Muñoz-Ortiz, Alberto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225096619",
      "name": "Gómez-Rodríguez, Carlos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742777054",
      "name": "Vilares, David",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Mu\\~noz-Ortiz, Alberto",
      "affiliations": []
    },
    {
      "id": null,
      "name": "G\\'omez-Rodr\\'iguez, Carlos",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4366999665",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W3139815689",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4386576626",
    "https://openalex.org/W2785674851",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W1515847863",
    "https://openalex.org/W4324098937",
    "https://openalex.org/W4366588626",
    "https://openalex.org/W4307413986",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W3155632693",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W4287120901",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2143995218",
    "https://openalex.org/W2528664121",
    "https://openalex.org/W2964264262",
    "https://openalex.org/W4226379991",
    "https://openalex.org/W4377864601",
    "https://openalex.org/W4378945475",
    "https://openalex.org/W4226471298",
    "https://openalex.org/W4307647693",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W2114719613",
    "https://openalex.org/W3104764318",
    "https://openalex.org/W1984940486",
    "https://openalex.org/W4287889356"
  ],
  "abstract": "We conduct a quantitative analysis contrasting human-written English news text with comparable large language model (LLM) output from six different LLMs that cover three different families and four sizes in total. Our analysis spans several measurable linguistic dimensions, including morphological, syntactic, psychometric, and sociolinguistic aspects. The results reveal various measurable differences between human and AI-generated texts. Human texts exhibit more scattered sentence length distributions, more variety of vocabulary, a distinct use of dependency and constituent types, shorter constituents, and more optimized dependency distances. Humans tend to exhibit stronger negative emotions (such as fear and disgust) and less joy compared to text generated by LLMs, with the toxicity of these models increasing as their size grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting objective language) than human texts, as well as more pronouns. The sexist bias prevalent in human text is also expressed by LLMs, and even magnified in all of them but one. Differences between LLMs and humans are larger than between LLMs.",
  "full_text": "Vol.:(0123456789)\nArtificial Intelligence Review          (2024) 57:265 \nhttps://doi.org/10.1007/s10462-024-10903-2\nContrasting Linguistic Patterns in Human \nand LLM‑Generated News Text\nAlberto Muñoz‑Ortiz1 · Carlos Gómez‑Rodríguez1 · David Vilares1\nAccepted: 6 August 2024 \n© The Author(s) 2024\nAbstract\nWe conduct a quantitative analysis contrasting human-written English news text with \ncomparable large language model (LLM) output from six different LLMs that cover three \ndifferent families and four sizes in total. Our analysis spans several measurable linguistic \ndimensions, including morphological, syntactic, psychometric, and sociolinguistic aspects. \nThe results reveal various measurable differences between human and AI-generated texts. \nHuman texts exhibit more scattered sentence length distributions, more variety of vocab-\nulary, a distinct use of dependency and constituent types, shorter constituents, and more \noptimized dependency distances. Humans tend to exhibit stronger negative emotions (such \nas fear and disgust) and less joy compared to text generated by LLMs, with the toxicity of \nthese models increasing as their size grows. LLM outputs use more numbers, symbols and \nauxiliaries (suggesting objective language) than human texts, as well as more pronouns. \nThe sexist bias prevalent in human text is also expressed by LLMs, and even magnified in \nall of them but one. Differences between LLMs and humans are larger than between LLMs.\nKeywords Large language models · Computational linguistics · Machine-generated text · \nLinguistic biases\n1 Introduction\nLarge language models (LLMs; Radford et  al., 2018; Scao et  al., 2022; Touvron et  al., \n2023) and instruction-tuned variants (OpenAI 2023; Taori et  al. 2023) output fluent, \nhuman-like text in many languages, English being the best represented. The extent to which \nthese models truly understand semantics (Landgrebe and Smith 2021; Søgaard 2022), \nencode representations of the world (Li et al. 2022), generate fake statements (Kumar et al. \n * Alberto Muñoz-Ortiz \n alberto.munoz.ortiz@udc.es\n Carlos Gómez-Rodríguez \n carlos.gomez@udc.es\n David Vilares \n david.vilares@udc.es\n1 Universidade da Coruña, CITIC, Departamento de Ciencias de la Computación y Tecnologías de \nla Información, Campus de Elviña s/n, A Coruña 15071, A Coruña, Spain\n A. Muñoz-Ortiz et al.\n  265  Page 2 of 28\n2023), propagate specific moral and ethical values (Santurkar et al. 2023), or understand \nlanguage based on their training on form rather than meaning (Bender and Koller 2020), is \ncurrently under active debate. Regardless, a crucial factor contributing to the persuasive-\nness of these models lies, in the very first place, in their exceptional linguistic fluency.\nA question is whether their storytelling strategies align with the linguistic patterns \nobserved in human-generated texts. Do these models tend to use more flowery or redun-\ndant vocabulary? Do they exhibit preferences for specific voices or syntactic structures in \nsentence generation? Are they prone to certain psychometric dimensions? However, con-\ntrasting such linguistic patterns is not trivial. Firstly, the creators of these models often \ninsufficiently document the training data used. Even with available information, determin-\ning the extent of the training set’s influence on a sentence or whether it is similar to an \ninput sample remains challenging. Second, language is subject to cultural norms, social \nfactors, and geographic variations, which shape linguistic preferences and conventions. \nThus, to contrast linguistic patterns between humans and machines, it is advisable to rely \non a controlled environment. In this context, attention has primarily been on explicit biases \nlike societal and demographic biases (Liang et al. 2021).\n1.1  Research contributions and objectives\nWe study six generative large language models: Mistral 7B (Jiang et al. 2023), Falcon 7B \n(Almazrouei et al. 2023) and the four models (7B, 13B, 30B and 65B) from the LLaMa \nfamily (Touvron et  al. 2023). We contrast several linguistic patterns against human text \nusing English news text. To do so, we recover human-generated news and ask the models \nto generate a news paragraph based on the headline and first words of the news. We query \nthe New York Times Archive API to retrieve news published after all the models used \nwere released, to guarantee sterilization from the training set. We analyze various linguistic \npatterns: differences in the distribution of the vocabulary, sentence length, part-of-speech \n(PoS) tags, syntactic structures, psychometric features such as the tone of the news articles \nand emotions detectable in the text, and sociolinguistic aspects like gender bias. We depict \nan overview in Fig.  1. We also explore if these disparities change across models of differ -\nent sizes and families. The data and the scripts used in this work are available at https:// \nzenodo. org/ recor ds/ 11186 264.\nFig. 1  We gather contemporary articles from the New York Times API and use their headlines plus the 3 \nfirst words of the lead paragraph as prompts to LLMs to generate news. We use four LLMs from the LLaMa \nfamily (7B, 13B, 30B and 65B sizes), Falcon 7B and Mistral 7B. We then compare both types of texts, \nassessing differences in aspects like vocabulary, morphosyntactic structures, and semantic attributes\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 3 of 28   265 \n2  Related work\nNext, we survey work relevant to the subject of this paper: (i) analyzing inherent linguis-\ntic properties of machine-generated text, (ii) distinguishing between machine- and human-\ngenerated texts, (iii) using LLMs for natural language annotation and data generation.\n2.1  Analysis of linguistic properties of AI‑generated text\nCognitive scientists (Cai et  al. 2023) have exposed models such as ChatGPT to experi-\nments initially designed for humans. They verified that it was able to replicate human pat-\nterns like associating unfamiliar words to meanings, denoising corrupted sentences, or \nreusing recent syntactic structures, among other abilities. Yet, they also showed that Chat-\nGPT tends to refrain from using shorter words to compress meaning, as well as from using \ncontext to resolve syntactic ambiguities. Similarly, Leong and Linzen (2023) studied how \nLLMs are able to learn exceptions to syntactic rules, claiming that GPT-2 and human judg-\nments are highly correlated. Zhou et al. (2023) conducted a thorough comparison between \nAI-created and human-created misinformation. They first curated a dataset of human-cre-\nated misinformation pertaining to the COVID-19 pandemic. Then, they used these rep-\nresentative documents as prompts for GPT-3 to generate synthetic misinformation. By \nanalyzing and contrasting the outputs from both sources, the study revealed notable differ -\nences. AI-made fake news tended to be more emotionally charged, using eye-catching lan-\nguage. It also frequently raised doubts without proper evidence and jumped to unfounded \nconclusions. Very recently, Xu et al. (2023) have shed light on the lexical conceptual repre-\nsentations of GPT-3.5 and GPT-4. Their study demonstrated that these AI language mod-\nels exhibited strong correlations with human conceptual representations in specific dimen-\nsions, such as emotions and salience. However, they encountered challenges when dealing \nwith concepts linked to perceptual and motor aspects, such as visual, gustatory, hand/arm, \nor mouth/throat aspects, among others. With the goal of measuring differences across both \ntypes of texts, Pillutla et al. (2021) introduced MAUVE, a new metric designed to compare \nthe learned distribution of a language generation model with the distributions observed in \nhuman-generated texts. Given the inherent challenge in open-ended text generation, where \nthere is no single correct output, they address the issue of gauging proximity between dis-\ntributions by leveraging the concept of a divergence curve. Following the release of this \nwork as a preprint, other authors have studied the text generated by language models from a \nlinguistic point of view. Martínez et al. (2023) developed a tool to evaluate the vocabulary \nknowledge of language models, testing it on ChatGPT. Other works have also evaluated \nthe lexical abundance of ChatGPT and how it varies with regards to different parameters \n(Martínez et al. 2024). Linguistic analysis is proving to be a valuable tool in understanding \nLLM outputs. In the line of our work, Rosenfeld and Lazebnik (2024) conducted a linguis-\ntic analysis of the outputs from three popular LLMs, concluding that this type of informa-\ntion can be used for LLM attribution on machine-generated texts. Moreover, comparing \nlinguistic measures is common in model benchmarks (Wang et al. 2018).\n2.2  Identification of synthetically‑generated text\nThis research line aims to differentiate texts generated by machines from those authored by \nhumans (Crothers et al. 2023), thus contributing to accountability and transparency in vari-\nous domains. This challenge has been addressed from different angles including statistical, \n A. Muñoz-Ortiz et al.\n  265  Page 4 of 28\nsyntactic (Tang et al. 2024), feature-based methods (Nguyen-Son et al. 2017; Fröhling and \nZubiaga 2021) and neural approaches (Rodriguez et al. 2022; Zhan et al. 2023). Yet, Croth-\ners et al. (2022) recently concluded that except from neural methods, the other approaches \nhave little capacity to identify modern machine-generated texts. Ippolito et  al. (2020) \nobserved two interesting behaviors related to this classification task: (i) that using more \ncomplex sampling methods can help make generated text better at tricking humans into \nthinking it was written by a person, but conversely make the detection for machines more \naccessible and simpler, and (ii) that showing longer inputs help both machines and humans \nto better detect synthetically-generated strings. Munir et al. (2021) showed that it was pos-\nsible to attribute a given synthetically-generated text to the specific LLM model that pro-\nduced it, using a standard machine learning classification architecture that used XLNet \n(Yang et al. 2019) as its backbone. In a different line, Dugan et al. (2020) studied whether \nhumans could identify the fencepost where an initially human-generated text transitions \nto a machine-generated one, detecting the transition with an average delay of 2 sentences. \nThere are also methods that have been specifically designed to generate or detect machine-\ngenerated texts for highly sensible domains, warning about the dangers of language tech-\nnologies. The SCIgen software (Stribling et al. 2005) was able to create semantically non-\nsense but grammatically correct research papers, whose content was accepted at some \nconferences with poor peer-review processes. More recently, Liao et al. (2023) showed that \nmedical texts generated by ChatGPT were easy to detect: although the syntax is correct, \nthe texts were more vague and provided only general terminology or knowledge. However, \nthis is a hard task and methods to detect AI-generated text are not accurate and are suscep-\ntible to suffer attacks (Sadasivan et al. 2023).\n2.3  Natural language annotation and data generation using LLMs\nThe quality of current synthetically-generated text has encouraged researchers to explore \ntheir potential for complementing labor-intensive tasks, such as annotation and evalua-\ntion. For instance, He et al. (2022) generated synthetic unlabeled text tailored for a specific \nNLP task. Then, they used an existing supervised classifier to silver-annotate those sen-\ntences, aiming to establish a fully synthetic process for generating, annotating, and learn-\ning instances relevant to the target problem. Related, Chiang and Lee (2023) investigated \nwhether LLMs can serve as a viable replacement for human evaluators in downstream \ntasks. Some examples of downstream tasks are text classification (Li et al. 2023b), intent \nclassification (Sahu et  al. 2022), toxic language detection (Hartvigsen et  al. 2022), text \nmining (Tang et al. 2023), or mathematical reasoning (Liu et al. 2023b), inter alia. Particu-\nlarly, they conducted experiments where LLMs are prompted with the same instructions \nand samples as provided to humans, revealing a correlation between the ratings assigned by \nboth types of evaluators. Moreover, there is also work to automatically detect challenging \nsamples in datasets. For instance, Swayamdipta et al. (2020) already used the LLMs’ fine-\ntuning phase to identify simple, hard and ambiguous samples. Chong et al. (2022) demon-\nstrated that language models are useful to detect label errors in datasets by simply ranking \nthe loss of fine-tuned data.\nLLMs can also contribute in generating high-quality texts to pretrain other models. Pre-\nvious work has used language models to generate synthetic data to increase the amount of \navailable data using pretrained models (Kumar et al. 2020). Synthetic data is also used to \npretrain and distill language models. Data quality has been shown to be a determinant fac-\ntor for training LLMs. Additional synthetic data can contribute to scale the dataset size to \nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 5 of 28   265 \ncompensate a small model size, getting more capable small models. LLMs have allowed to \ngenerate high-quality, synthetic text that is useful to train small language models (SLMs). \nOne of such cases is Eldan and Li (2023). They generated high quality data with a con-\nstrained vocabulary and topics using GPT-3.5 and 4 to train SLM that show coherence, \ncreativity and reasoning in a particular domain. The Phi models family (Gunasekar et al. \n2023; Li et al. 2023a; Javaheripi et al. 2023) showed the usefulness of synthetic data in \ntraining high-performance but SLMs. The authors used a mixture of high-quality textbook \ndata and synthetically-generated textbooks to train a highly competent SLM. Moreover, \nsynthetic data has been used to create instruction tuning datasets to adapt LLMs’ behav -\nior to user prompts (Peng et al. 2023). Synthetic data can also help prevent LLMs from \nadapting their answers to previous human opinions when they are not objectively correct \n(Wei et al. 2023). However, although useful, synthetically-generated data may harm perfor-\nmance (Shumailov et al. 2023), especially when the tasks or instances at hand are subjec-\ntive (Li et al. 2023b).\nSynthetic datasets provide data whose content is more controllable, as LLMs tend to \nreproduce the structure of the datasets they have been trained on. Most LLMs are trained \ntotally or partially on scraped data from the web, and such unfiltered internet data usu-\nally contain biases or discrimination as they reproduce the hegemonic view (Bender et al. \n2021). Some widely-used huge datasets such as The Pile (Gao et al. 2020) confirm this. \nAuthors extracted co-occurrences in the data that reflect racial, religious and gender ste-\nreotypes, which are also shown in some models. Some datasets are filtered and refined \nto improve the quality of the data. However, they still reproduce the biases in it (Penedo \net al. 2023). Moreover, Dodge et al. (2021) did an extensive evaluation of the data of the \nC4 dataset (Raffel et al. 2020), pointing out filtering certain information could increase the \nbias on minorities. Prejudices in the data are reproduced in the LLMs trained on them, as \nsome studies have pointed out (Weidinger et al. 2021). LLMs show the same biases that \noccur in the datasets, ranging from religious (Abid et al. 2021) to gender discrimination \n(Lucy and Bamman 2021).\n3  Data preparation\nNext, we will examine our data collection process for both human- and machine-generated \ncontent, before proceeding to the analysis and comparison.\n3.1  Data\nWe generate the evaluation dataset relying on news published after the release date of the \nmodels that we will use in this work. This strategy ensures that they did not have exposure \nto the news headlines and their content during pre-training. It is also in line with strategies \nproposed by other authors—such as Liu et  al. (2023) - who take an equivalent angle to \nevaluate LLMs in the context of generative search engines. The reference human-generated \ntexts will be the news (lead paragraph) themselves.\nWe use New York Times news, which we access through its Archive API.1 Particularly, \nwe gathered all articles available between October 1, 2023, and January 24, 2024, resulting \n1 https:// devel oper. nytim es. com/ docs/ archi ve- produ ct/1/ overv iew\n A. Muñoz-Ortiz et al.\n  265  Page 6 of 28\nin a dataset of 13,371 articles. The articles are retrieved in JSON format, and include meta-\ndata such as the URL, section name, type of material, keywords, or publication date. Fig-\nure 2 shows some general information about the topics and type of articles retrieved. We \nare mainly interested in two fields: the headline and the lead paragraph. The lead paragraph \nis a summary of the information presented in the article. We discarded the articles that had \nan empty lead paragraph. The collected articles primarily consist of news pieces, although \naround 26% also include other types of texts, such as reviews, editorials or obituaries.\n3.1.1  Rationale for methodological decisions and technical trade‑offs\nWe opted for a more conservative setup by focusing our study on English, balancing the \ndepth of our analysis with practical constraints. While this choice is common in various \nlanguage-related fields, including cognitive science (Blasi et  al. 2022), it implies inter -\npreting our results with caution when applying them outside the context of the English \nlanguage and the news domain. By analyzing solely English, we can establish a baseline \nfor future studies that incorporate multilingual analysis. Additionally, this initial approach \ncould enable researchers to clearly identify discrepancies between results in English and \nthose in other distinct languages.\nIn addition, our decision was driven by a few logistical reasons. Firstly, the LLMs we \nuse (as detailed in Sect.  3.2) are English-centric. LLaMa’s dataset comprises over 70% \nEnglish content, and Falcon’s even higher at over 80%. With Mistral, the specifics of the \ntraining data were not disclosed, adding an extra layer of complexity. In this context, it is \nworth noting that a model trained predominantly on data from specific demographics or \nregions might develop a bias towards those linguistic patterns, potentially overlooking oth-\ners. The clarity around the influence of diverse linguistic inputs on model performance is \nalso limited, further complicating a fair analysis. Additionally, it is important to note that \nwe used non-instruction-tuned models, which have shown limitations that we mentioned in \nadhering to languages other than English. This reinforces our decision to focus on English \nat this stage, given the technical constraints and the developmental stage of these mod-\nels. Evaluating instruction-tuned models would be interesting and useful, but as a separate \npiece of work with a different focus and contribution. Here, we decided to focus on foun-\ndation models that have not been trained on instruction-tuning datasets in order to evalu-\nate the effects that pretraining processes and model size can have on linguistic patterns. \nFig. 2  Treemaps for the ‘section name’ and ‘type of material’ fields of the crawled articles\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 7 of 28   265 \nIncluding instruction-tuned variants would introduce another layer of training, blurring \nthe effect of the pretraining and size. Given these considerations, we opted for depth over \nbreadth in our analysis to provide a more thorough evaluation, but limited to English.\nWe are also aware that our prompting approach entails certain trade-offs. Lead para-\ngraphs are not written from the headline but from the text of the article. If humans gener -\nated the lead paragraphs from the headline, we hypothesize that they would face the lack \nof relevant information in similar ways as the LLMs do: (i) restricting to the information \ngiven in the headline, offering a shorter lead paragraph with repeated information from the \nheadline, or (ii) generating new information that could fit the data based on prior knowl-\nedge of the topic. We argue that this latter strategy is similar to what LLMs do, and then \nthis would be a preferred comparison. However, as generating human data in that way \nwould be really costly, we opted for our chosen strategy as the data from the lead paragraph \nis highly correlated with that in the headline, even when the lead paragraph is not written \nfrom the headline itself.\n3.2  Generation\nLet H = [h1 ,h2 , ...,hN ] be a set of human-generated texts, such that h i is a tuple of the form \n(ti, si) where ti is a headline and si is a paragraph of text with a summary of the correspond-\ning news. Similarly, we will define M = [m1 ,m2 , ...,mN] as the set of machine-generated \nnews articles produced by a LLM such that m i is also a tuple of the from ( t�\ni , s�\ni ) where t�\ni = ti \nand s�\ni =[ w �\n1 ,w �\n2 , ...,w �\n/uni007C.varsi/uni007C.var] is a piece of synthetic text. For the generation of high-quality \ntext, language models aim to maximize the probability of the next word based on the previ-\nous content. To ensure that the models keep on track with the domain and topic, we initial-\nize the previous content with the headline (the one chosen by the journalist that released \nthe news) and the first three words of the human-generated lead paragraph to help the \nmodel start and follow the topic. 2 Formally, we first condition the model on c i = t�\ni ⋅ si[0∶2] \nand every next word ( i ≥ 3 ) will be predicted from a conditional distribution \nP(w �\ni/uni007C.varci ⋅ s�\ni[3∶t−1]).\nTo generate a piece of synthetic text s/uni2032.var , we condition the models with a prompt that \nincludes the headline and first words, as described above, and we keep generating news text \nuntil the model decides to stop.3 We enable the model to output text without any forced cri-\nteria, except for not exceeding 200 tokens. The length limit serves two main purposes: (i) \nto manage computational resources efficiently,4 and (ii) to ensure that the generated content \nresembles the typical length of human-written lead paragraphs, making it comparable to \nhuman-produced content. We arrived at this limit after comparing the average and standard \ndeviation of the number of tokens between humans and models in early experiments.\n2 During the configuration runs, certain LLM outputs encountered difficulties in adhering to a minimal \ncoherent structure when a minimum number of the body’s words were absent from the prompt. Also note \nthat the LLMs we are using are not instruction-tuned, and thus prompting engineering is not particularly \nsuitable, nor the goal of this work.\n3 During the configuration runs, we explored hyperparameter values that generated fluent and coherent \ntexts: temperature of 0.7, 0.9 top p tokens, and a repetition penalty of 1.1.\n4 We ran the models on 2xA100 GPUs for 3 days to generate all texts. To address memory costs, we use \n8-bit precision.\n A. Muñoz-Ortiz et al.\n  265  Page 8 of 28\n3.3  Selected models\nWe rely on six pre-trained generative language models that are representative within the \nNLP community. These models cover 4 different sizes (7, 13, 30 and 65 billion param-\neters) and 3 model families. We only include different sizes for LLaMa as results within \nthe same family are similar, and larger models need considerably more compute. We \nbriefly mention their main particularities below:\n3.3.1  LLaMa models (LL) (Touvron et al. 2023)\nThe main representative for our experiments will be the four models from the version \n1 of the LLaMa family, i.e. the 7B, 13B, 30B, and 65B models. The LLaMa models \nare trained on a diverse mix of data sources and domains, predominantly in English, as \ndetailed in Table  1. LLaMa is based on the Transformer architecture and integrates sev -\neral innovations from other large language models. In comparison to larger models like \nGPT-3 (Brown et al. 2020), PaLM (Chowdhery et al. 2023), and Chinchilla (Hoffmann \net al. 2022), LLaMa exhibits superior performance in zero and few-shot scenarios. It is \nalso a good choice as a representative example because the various versions, each with \na different size, will enable us to examine whether certain linguistic patterns become \ncloser or more different to humans in larger models.\n3.3.2  Falcon 7B (F7B) (Almazrouei et al. 2023)\nIntroduced alongside its larger variants with 40 and 180 billion parameters, Falcon 1 7B \nis trained on 1.5 trillion tokens from a mix of curated and web datasets (see Table  1). Its \narchitecture relies on multigroup attention (an advanced form of multiquery attention), \nRotary Embeddings (similar to LLaMa), standard GeLU activation, parallel attention, \nMLP blocks, and omits biases in linear layers. We primarily chose this model to com-\npare the results in the following sections with those of its counterpart, LLaMa 7B, and \nto explore whether there are significant differences among models of similar size.\nTable 1  Size and training data of the models used in our experiments\nFamily Size Tokens Data sources\nLLaMa 7B 1T English CommonCrawl (67%), C4 (15%)\n13B 1T GitHub (4.5%), Wikipedia (4.5%)\n30B 1.5T Gutenberg and Books3 (4.5%), ArXiv (2.5%)\n65B 1.5T Stack Exchange (2%)\nFalcon RefinedWeb-English (76%), RefinedWeb-Euro (8%)\n7B 1.5T Gutenberg (6%), Conversations (5%)\nGitHub (3%), Technical (2%)\nMistral 7B Not publicly \ndisclosed\nNot publicly disclosed\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 9 of 28   265 \n3.3.3  Mistral 7B (M7B) (Jiang et al. 2023)\nMistral v0.1 surpasses larger LLaMa models in various benchmarks despite its smaller \nsize. Its distinctive architecture features Sliding Window Attention, Rolling Buffer \nCache, and Prefill and Chunking. The training data for Mistral 7B is not publicly dis-\nclosed, and to fight against data contamination issues, our analysis only includes articles \npublished after the model’s release. The choice of this model as an object of study fol-\nlows the same thinking we used for the Falcon model. We want to see how well Mistral \n7B does and how its new features stack up against models of the same size.\n4  Analysis of linguistic patterns\nIn this section, we compare human- and machine-generated texts. We first inspect the texts \nunder a morphosyntactic lens, and then focus on semantic aspects.\n4.1  Morphosyntactic analysis\nTo compute linguistic representations, we rely on Stanza (Qi et al. 2020) to perform seg-\nmentation, tokenization, part-of-speech (PoS) tagging, and dependency and constituent \nparsing. For these tasks, and in particular for the case of English and news text, the per -\nformance is high enough to be used for applications (Manning 2011; Berzak et al. 2016), \nand it can be even superior to that obtained by human annotations. This also served as an \nadditional reason to focus our analysis on news text, ensuring that the tools we rely on are \naccurate enough to obtain meaningful results.5\nFig. 3  Sentence length distribution in words for the human-written texts and each tested language model. M \nstands for Mistral, F for Falcon and LL for LLaMa\n5 In addition, we performed a t-test on the mean of several of the tested metrics, obtaining that the differ -\nences between humans and LLMs are statistically significant (p-values<0.05; see Appendix A).\n A. Muñoz-Ortiz et al.\n  265  Page 10 of 28\n4.1.1  Sentence length\nFigure 3 illustrates the length distribution for the LLMs in comparison to human-generated \nnews articles. We excluded a few outliers from the plot by ignoring sentences with lengths \nover 80 tokens. The six LLMs exhibit a similar distribution across different sentence \nlengths, presenting less variation when compared to human-generated sentences, which \ndisplay a wider range of lengths and greater diversity. Specifically, the models exhibit \na higher frequency of sentence generation within the 10 to 30 token range compared to \nhumans, whereas humans tend to produce longer sentences with greater frequency. These \nresults might be explained by considering the stochastic nature of the models, which could \nstreamline outputs, thereby reducing the occurrence of extreme events. This may result \nin less variation and more uniform sentence lengths in LLM-generated text, suggesting a \npotential area for further study. Moreover, humans typically use a specific writing style \nbased on the genre. The probabilistic nature of LLMs can blur the distinctions between \nwriting styles and genres, resulting in what is known as register leveling. Register leveling \nrefers to the phenomenon where distinct linguistic features characteristic of different gen-\nres or styles become less pronounced, leading to a more homogenized output. This can \nobscure the unique stylistic elements that typically differentiate journalistic texts from \nother genres, thereby making the text produced by LLMs more uniform regardless of the \nintended register.\n4.1.2  Richness of vocabulary and lexical variation\nWe analyze the diversity of vocabulary used by the LLMs and compare them against \nhuman texts. To measure it, we relied on two metrics: standardized type-token ratio (STTR) \nand the Measure of Textual Lexical Diversity (MTLD; McCarthy and Jarvis, 2010), which \nare more robust to text length than Type-Token Ratio (TTR). We calculated both metrics \nusing lemmatized tokens as they provide a more accurate measure of true lexical diversity. \nTTR is a measure of lexical variation that is calculated by dividing the number of unique \ntokens (types) by the total number of tokens. To obtain the STTR, we first join all the texts \ngenerated by the humans and each model. We divide the text in segments of 1 000 tokens \nand calculate the TTR of each segment. Finally, we obtain the STTR by averaging the TTR \nof every segment. Table 2 shows the value of the STTR for each model.\nFrom these results, it seems that humans use a richer vocabulary than the LLMs studied. \nThe model family that comes closer to human texts is LLaMa, obtaining similar scores for \nevery model size. Then Mistral comes close, and Falcon is last, exhibiting the lowest lexi-\ncal diversity by far according to STTR. These results show that language family is more \nimportant than model size when accounting for vocabulary richness. This would, intui-\ntively, be expected, as training data is largely shared between models of the same family \nand is a main factor when considering lexical diversity.\nTable 2  STTR and MTLD for \nthe articles generated by humans \nand each tested language model\nHuman M7B F7B LL7B LL13B LL30B LL65B\nSTTR 0.491 0.452 0.424 0.460 0.457 0.461 0.466\nMTLD 96.51 86.34 57.37 94.12 91.82 89.76 94.56\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 11 of 28   265 \nWith respect to MTLD, the metric starts from a threshold value for TTR (with 0.72 \nbeing the default that we use here following previous work) and calculates the TTR of a \ntext word by word, adding them sequentially. When the calculated TTR falls under the \nthreshold, a factor is counted and the TTR calculation resets. Then, the total number of \ntokens in the text is divided by the average number of tokens per factor. This process is \nrepeated backwards. The final MTLD score is the average of the forward and reverse cal-\nculations. Results of MTLD for human and LLM text are also shown in Table  2. Results \nfor MLTD are analogous to STTR: human texts exhibit the highest lexical diversity, closely \nfollowed by LLaMa models, then Mistral and, far away, Falcon. This reinforces the claim \nthat language model family is a more relevant factor for vocabulary richness than size.\nTable 3  UPOS frequencies (%) \nin human- and LLM-generated \ntexts\nUPOS H M7B F7B LL7B LL13B LL30B LL65B\nNOUN 19.69 17.85 17.72 17.75 17.44 17.64 17.74\nPUNCT 11.88 10.92 12.14 10.77 10.91 11.43 11.22\nADP 11.36 10.58 10.30 10.75 10.63 10.70 10.69\nVERB 9.97 10.37 9.23 10.26 10.23 10.14 10.29\nPROPN 9.61 8.75 9.44 9.14 9.18 9.52 9.50\nDET 9.04 9.00 10.72 8.65 8.64 8.76 8.63\nADJ 7.58 6.69 6.74 6.86 6.76 6.73 6.77\nPRON 5.32 7.12 6.11 7.08 7.33 6.96 6.93\nAUX 3.81 5.77 6.02 5.65 5.74 5.50 5.41\nADV 3.26 3.41 2.61 3.58 3.68 3.41 3.49\nCCONJ 2.65 2.72 2.52 2.68 2.70 2.61 2.67\nPART 2.43 2.76 2.80 2.64 2.63 2.52 2.58\nNUM 1.77 1.95 1.98 2.02 1.98 2.05 2.02\nSCONJ 1.41 1.84 1.37 1.84 1.85 1.71 1.72\nINTJ 0.12 0.08 0.08 0.08 0.08 0.08 0.09\nSYM 0.09 0.17 0.19 0.19 0.19 0.18 0.18\nX 0.03 0.03 0.02 0.05 0.04 0.06 0.07\nFig. 4  Percentage differences, following Table 3, in the use of each UPOS category for each tested language \nmodel in comparison to humans\n A. Muñoz-Ortiz et al.\n  265  Page 12 of 28\n4.1.3  Part‑of‑speech tag distributions\nTable 3 presents the frequency of universal part-of-speech (UPOS) tags (Petrov et al. 2012) \nfor both human and LLM-generated texts. Figure  4 shows relative differences observed \nacross humans and each model, for a better understanding of the relative use of certain \ngrammatical categories. Overall, the behavior of LLMs and their generated text tends to be \nconsistent among themselves, yet shows differences when compared to human behavior, \ni.e., they exhibit in some cases a greater or lesser use of certain grammatical categories. \nTo name a few, humans exhibit a preference for using certain kinds of content words, such \nas nouns and adjectives. Humans also use punctuation symbols more often (except when \ncompared to Falcon), which may be connected to sentence length, as human users tend to \nrely on longer sentences, requiring more punctuation. Alternatively, the language models \nexhibit a pronounced inclination towards relying on categories such as symbols or num-\nbers, possibly indicating an extra effort by language models to furnish specific data in order \nto sound convincing. Moreover, they write pronouns more frequently; we will analyze \nthis point later from a gender perspective. Comparing LLM families, Mistral and LLaMa \nshow a similar use of grammatical categories, with Mistral being the model that resembles \nhumans the most. Falcon, however, has some strong anomalies in the use of determiners \nand adverbs. Regarding model size, the larger the model, the greater the similarity with \nhumans. Nevertheless, differences between differently-sized models are much smaller than \nbetween models and humans. Similar to sentence length, the stochastic nature of the mod-\nels may account for the differences between human and LLM-generated text.\n4.1.4  Dependencies\n4.1.5  Dependency arc lengths\nTable 4 shows information about the syntactic dependency arcs in human and machine-\ngenerated texts. In this analysis, we bin sentences by length intervals to alleviate the noise \nfrom comparing dependency lengths on sentences of mixed lengths (Ferrer-i-Cancho and \nLiu 2014). Results indicate that dependency lengths and their distributions are nearly iden-\ntical for all the LLMs except Falcon, which uses longer dependencies than the rest of the \nmodels and resembles more the human texts in this respect. This finding holds true for \nevery sentence length bin for Falcon, and for all but the first (length 1–10) in the case of \nhuman texts, so we can be reasonably sure that it is orthogonal to the variation in sentence \nlength distribution between human and LLM texts described earlier. It is also worth noting \nthat, in spite of the similarities between humans and Falcon in terms dependency lengths, \ntheir syntax is not that similar overall: there is a substantial difference in directionality of \ndependencies, with Falcon using more leftward dependencies than both humans and other \nLLMs. The fact that Falcon-generated texts are not really human-like in terms of depend-\nency syntax is further highlighted in the next section, where we consider a metric that nor -\nmalizes dependency lengths.\n4.1.6  Optimality of dependencies\nWe compare the degree of optimality of syntactic dependencies between human texts and \nLLMs. It has been observed in human language that dependencies tend to be much shorter \nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 13 of 28   265 \nthan expected by chance, a phenomenon known as dependency length minimization (Fer -\nrer-i-Cancho 2004; Futrell et al. 2015). This phenomenon, widely observed across many \nlanguages and often hypothesized as a linguistic universal, is commonly assumed to be due \nto constraints of human working memory, which make longer dependencies harder to pro-\ncess  (Liu et  al. 2017). This makes languages evolve into syntactic patterns that reduce \nTable 4  Statistics for dependency arcs in sentences of different lengths for the texts generated by human \nwriters and each tested language model. The meaning of the columns is as follows: (%L, %R) percentage of \nleft and right arcs, ( ̄l  ) average arc length, ( ̄lL  , ̄lR  ) average left and right arc length, ( /u1D70El ) standard deviation of \narc length, ( /u1D70ElL\n , /u1D70ElR\n ) standard deviation of left and right arc length, and number of sentences\nl Model %L %R ̄l ̄lL ̄lR /u1D70El /u1D70ElL\n/u1D70ElR\n# Sent\n1–10 Human 49.40 50.60 2.37 2.89 1.84 1.67 1.90 1.17 4 719\nM7B 50.94 49.06 2.37 2.93 1.83 1.65 1.88 1.16 6 190\nF7B 52.08 47.92 2.39 2.99 1.84 1.62 1.84 1.15 4 596\nLL7B 50.68 49.32 2.37 2.95 1.81 1.65 1.88 1.14 6 114\nLL13B 50.42 49.58 2.37 2.94 1.81 1.65 1.88 1.14 6 711\nLL30B 49.97 50.03 2.37 2.92 1.81 1.65 1.89 1.14 6 808\nLL65B 50.23 49.77 2.36 2.91 1.81 1.64 1.87 1.15 6 652\n11–20 Human 58.36 41.64 3.19 4.62 2.17 3.12 3.87 1.86 6 179\nM7B 59.76 40.24 3.12 4.63 2.10 3.03 3.80 1.74 12 113\nF7B 61.41 38.59 3.20 4.79 2.19 3.06 3.85 1.83 9 265\nLL7B 59.74 40.26 3.11 4.63 2.09 3.03 3.81 1.72 12 361\nLL13B 59.69 40.31 3.12 4.63 2.11 3.03 3.81 1.75 12 762\nLL30B 59.62 40.38 3.12 4.63 2.11 3.03 3.80 1.76 13 039\nLL65B 59.43 40.57 3.13 4.63 2.10 3.04 3.81 1.75 12 767\n21–30 Human 60.40 39.60 3.64 5.52 2.41 4.42 5.71 2.68 6 153\nM7B 61.00 39.00 3.53 5.50 2.26 4.28 5.65 2.33 10 449\nF7B 62.51 37.49 3.62 5.70 2.38 4.32 5.72 2.46 8 222\nLL7B 60.87 39.13 3.51 5.47 2.25 4.26 5.64 2.30 11 014\nLL13B 60.86 39.14 3.53 5.49 2.27 4.27 5.64 2.34 11 017\nLL30B 60.71 39.29 3.53 5.48 2.27 4.26 5.61 2.34 10 810\nLL65B 60.47 39.53 3.53 5.47 2.26 4.28 5.63 2.35 10 884\n31–40 Human 60.84 39.16 3.90 6.07 2.50 5.49 7.32 3.19 4 770\nM7B 60.48 39.52 3.79 5.95 2.38 5.35 7.15 2.98 4 676\nF7B 61.98 38.02 3.89 6.11 2.52 5.35 7.16 3.12 4 064\nLL7B 60.79 39.21 3.78 5.98 2.35 5.34 7.19 2.90 5 790\nLL13B 60.51 39.49 3.79 5.96 2.38 5.33 7.14 2.93 5 280\nLL30B 60.35 39.65 3.81 5.95 2.40 5.33 7.09 2.99 4 949\nLL65B 60.35 39.65 3.79 5.95 2.37 5.31 7.10 2.93 5 430\n+41 Human 60.48 39.52 4.01 6.28 2.53 6.20 8.32 3.58 2 967\nM7B 60.09 39.91 3.95 6.23 2.44 6.24 8.45 3.39 1  415\nF7B 61.77 38.23 4.04 6.43 2.56 6.18 8.46 3.44 1 318\nLL7B 59.83 40.17 3.97 6.25 2.44 6.24 8.41 3.43 2 035\nLL13B 60.47 39.53 3.99 6.29 2.48 6.23 8.41 3.50 1 693\nLL30B 60.21 39.79 3.98 6.24 2.49 6.21 8.34 3.53 1 579\nLL65B 60.08 39.92 3.95 6.22 2.45 6.16 8.33 3.37 1 880\n A. Muñoz-Ortiz et al.\n  265  Page 14 of 28\ndependency length, and language users prefer the options that minimize it when several \npossibilities are available to express an idea. Dependency length minimization can be \nquantified in a robust way (with respect to sentence length, tree topology and other factors) \nby the Ω optimality score introduced in Ferrer-i Cancho et al. (2022). This score measures \nwhere observed dependency lengths sit with respect to random word orders and optimal \nword orders, and is defined as: Ω= D rla−D\nD rla−D min\n , where D is the sum of dependency lengths in \nthe sentence, D rla is the expected sum of lengths, and D min is the optimal sum of lengths for \nthe sentence’s tree structure. For optimally-arranged trees D = D min and Ω takes a value of \n1, whereas for a random arrangement it has an expected value of 0. Negative values are \npossible (albeit uncommon) if dependency lengths are larger than expected by chance.\nFigure 5 displays the distribution of Ω values across sentences for human and LLM-\ngenerated texts. The values were calculated using the LAL library  (Alemany-Puig et  al. \n2021). Results indicate that the distribution of Ω values is almost identical between all of \nthe LLMs, but human texts show noticeably larger values. This means human texts are \nmore optimized in terms of dependency lengths, i.e. they have shorter dependencies than \nexpected by a larger margin than those generated by the LLMs. At a first glance, this might \nseem contradictory with the results in the previous section, which showed that human texts \nhad longer dependencies on average than non-Falcon LLM texts. However, there is no \nreal contradiction as the object of measurement is different, and in fact this is precisely \nthe point of using Ω to refine and complement the previous analysis. While previously we \nmeasured dependency distances in absolute terms, Ω measures them controlling for tree \ntopology, i.e., given a particular tree shape (e.g., a linear tree which is arranged as a chain \nof dependents, or a star tree where one node has all the others as dependents), Ω meas-\nures to what extent the words are arranged in an order that minimizes dependency lengths \nwithin the constraints of that shape. Thus, combining the results from both sections we can \nconclude that while humans produce longer dependencies, this is due to using syntactic \nstructures with different topology, but their word order is actually more optimized to make \ndependencies as short as possible. In turn, we also note that while Falcon’s dependency \nlengths seemed different from the other LLMs (and more human-like) in absolute terms, \nthe differences vanish (with all LLMs including Falcon having almost identical distribu-\ntions, and humans being the outlier) when considering Ω.\nFig. 5  Ω value distribution for the human- and LLM-generated texts\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 15 of 28   265 \nTable 5  Percentage of words generated by humans and each of the tested LLMs that are labeled with a spe-\ncific dependency type (deprel). We only include relations with a frequency surpassing 1% within the human \ntexts\nDeprel H M7B F7B LL7B LL13B LL30B LL65B\npunct 11.88 10.92 12.15 10.78 10.91 11.44 11.23\ncase 11.69 10.81 10.75 10.98 10.76 10.89 10.85\ndet 8.88 8.81 10.59 8.45 8.43 8.56 8.43\namod 6.98 5.57 5.73 5.79 5.60 5.71 5.75\nnsubj 6.09 7.20 6.89 7.00 7.21 7.11 7.02\nobl 5.50 5.24 4.67 5.39 5.31 5.36 5.31\nnmod 4.95 4.45 4.84 4.50 4.40 4.47 4.47\ncompound 4.87 4.04 4.46 4.20 4.13 4.27 4.33\nobj 4.28 4.41 3.91 4.22 4.23 4.19 4.27\nadvmod 3.46 3.63 2.91 3.83 3.98 3.65 3.76\nconj 3.07 2.80 2.71 2.83 2.79 2.73 2.83\nmark 2.65 3.35 2.94 3.27 3.28 3.07 3.12\ncc 2.63 2.73 2.54 2.72 2.73 2.63 2.69\nnmod:poss 2.34 2.21 2.01 2.21 2.19 2.19 2.17\nflat 2.04 1.67 1.72 1.79 1.80 1.92 1.91\naux 1.91 2.74 2.72 2.68 2.71 2.58 2.55\nadvcl 1.80 1.67 1.30 1.69 1.70 1.62 1.67\ncop 1.26 1.98 2.28 1.90 2.02 1.92 1.86\nacl:relcl 1.22 1.33 1.29 1.38 1.29 1.26 1.28\nappos 1.19 0.85 1.07 0.92 0.92 0.99 1.00\nnummod 1.14 1.16 1.16 1.22 1.21 1.23 1.21\nxcomp 1.10 1.40 1.27 1.37 1.36 1.30 1.34\nacl 1.06 0.93 0.84 0.92 0.87 0.88 0.93\nFig. 6  Percentage differences, following Table  5, in the use of dependency relations for each tested lan-\nguage model in comparison to humans\n A. Muñoz-Ortiz et al.\n  265  Page 16 of 28\n4.1.7  Dependency types\nTable 5 lists the frequencies for the main syntactic dependency types in human and machine-\ngenerated texts. We observe similar trends to the previous sections, with LLM texts exhibiting \nsimilar uses of syntactic dependencies among themselves, with Falcon being the most distinct \nmodel, while all of them present differences compared to human-written news. In terms of \nthe LLaMa models - same model in different sizes - larger models are slightly closer to the \nway humans use dependency types. For the full picture, Fig. 6 depicts all relative differences \nin their use (humans versus each LLM), but we briefly comment on a few relevant cases as \nrepresentative examples. For instance, numeric modifier dependencies (nummod) are more \ncommon in LLM-generated texts compared to human texts. This is coherent with the higher \nuse of the numeric tag (NUM) in the part-of-speech tag distribution analysis. Additionally, we \nobserved higher ratios for other dependency types, such as aux (for which the use of auxil-\niary verbs was also significantly higher according to the UPOS analysis), copula and nominal \nsubjects (nsubj). Furthermore, syntactic structures from LLMs exhibit significantly fewer \nsubtrees involving adjective modifiers (amod dependency type) and appositional modifiers \n(appos).\nTable 6  Statistics for constituents \nthat arise in sentences of different \nlengths for the text generated by \nhuman writers and each tested \nLLM. The meaning of the rows \nare: ( ̄l  ) average constituent \nlength, ( /u1D70El ) standard deviation of \nconstituent length, and number \nof sentences\nModel 1–10 11–20 21–30 31–40 +41\n̄l H 4.32 6.37 7.90 9.38 10.60\nM7B 4.39 6.55 8.27 9.77 11.01\nF7B 4.43 6.47 8.03 9.47 10.76\nLL7B 4.40 6.57 8.33 9.89 11.19\nLL13B 4.40 6.55 8.27 9.76 11.01\nLL30B 4.40 6.49 8.21 9.68 10.86\nLL65B 4.36 6.53 8.25 9.73 10.96\n/u1D70El H 2.35 4.64 6.97 9.19 11.24\nM7B 2.35 4.66 6.92 9.13 11.18\nF7B 2.33 4.63 6.80 8.94 11.01\nLL7B 2.35 4.69 6.99 9.24 11.35\nLL13B 2.33 4.68 6.95 9.14 11.19\nLL30B 2.36 4.66 6.94 9.14 11.17\nLL65B 2.34 4.68 6.96 9.17 11.23\n# Sent H 4 679 6 180 6 154 4 770 2 966\nM7B 6 108 12 113 10 448 4 678 1 414\nF7B 4 575 9 266 8 211 4 011 1 318\nLL7B 6 039 12 362 11 014 5 789 2 035\nLL13B 6 627 12 762 11 018 5 279 1 693\nLL30B 6 713 13 044 10 806 4 949 1 579\nLL65B 6 569 12 765 10 844 5 430 1 880\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 17 of 28   265 \n4.1.8  Constituents\n4.1.9  Constituent lengths\nTable 6 shows the comparison between the distribution of syntactic constituent lengths \nacross both types of texts. While human-generated sentences, on average, surpass the \nlength of those generated by LLMs, the average length of a sentence constituent for \nLLMs is observed to be greater than for humans. The standard deviation exhibits similar \nvalues across all models for each sentence length range. Similar to previous sections, \nFalcon 7B also displays the largest differences among language models. Within the \nLLaMa models, we can observe a clear decreasing trend with size which is broken by \nthe 65B model, for which constituent lengths increase again across most of the length \nbins.\nTable 7  Percentage of spans \ngenerated by humans and \nLLMs labeled with a specific \nconstituent type\nOnly constituent types that conform more than 1% of the human’s \ntexts spans are shown\nType H M7B F7B LL7B LL13B LL30B LL65B\nNP 42.91 39.96 41.42 40.17 40.02 40.69 40.54\nVP 18.08 20.59 20.19 20.18 20.29 19.97 20.02\nPP 14.12 12.62 12.81 12.91 12.69 12.94 12.81\nS 11.79 13.40 13.27 13.09 13.31 13.12 13.12\nSBAR 3.64 4.34 3.84 4.34 4.29 4.09 4.15\nADVP 2.39 2.37 1.86 2.50 2.62 2.44 2.49\nADJP 1.97 1.75 1.80 1.78 1.82 1.76 1.79\nNML 1.73 1.40 1.66 1.43 1.43 1.47 1.52\nWHNP 1.40 1.54 1.51 1.59 1.48 1.48 1.50\nFig. 7  Percentage differences, following Table  7, in the use of constituent labels for each tested language \nmodel in comparison to humans\n A. Muñoz-Ortiz et al.\n  265  Page 18 of 28\n4.1.10  Constituent types\nTable 7 and Fig. 7 examine the disparities in constituent types between human- and LLM-\ngenerated texts. A constituent is a unit composed of a word or group of words that works \nas a unit inside of the hierarchical structure of a sentence. Our focus was on constituent \ntypes that occur more than 1% of the times in human texts: noun phrase (NP), verb phrase \n(VP), prepositional phrase (PP), sentence (S), subordinate clause (SBAR), adverbial phrase \n(ADVP), adjectival phrase (ADJP), nominal (NML) and wh-noun phrase (WHNP). We use \nthe annotation scheme of the English Penn Treebank, which has been widely used by the \nNLP community (Marcus et al. 1994).\nComparing humans and LLMs, some outcomes are in the same line of earlier findings: \nhuman-generated content displays heightened use of noun, adjective, and prepositional \nphrases (NP, ADJP, and PP, respectively). On the contrary, there is minimal divergence \nin the frequency of adverb phrases (ADVP) except for Falcon 7B, which shows a great dif-\nference with human and LLM-generated texts, the latter exhibiting a more pronounced \npropensity for verb phrases (VP). Despite the similar frequency of the VERB UPOS tag \nin human and LLM-generated texts, the latter exhibit a more pronounced propensity for \nverb phrases (VP), consistent with the increased use of auxiliary verbs (whose UPOS tag is \nAUX, not VERB) that we saw in previous sections. Finally, we see that language models use \na considerably larger amount of subordinate clauses (SBAR). Regarding model families, \nresults are similar to those of dependencies and POS tags, but when looking at model size, \nprevious trends are less obvious.\n4.2  Semantic analysis\nAs in the previous section, we are relying on blackbox NLP models to accurately analyze \ndifferent semantic dimensions: (i) emotions, (ii) text similarities, and (iii) gender biases, in \nan automated way.\n4.2.1  Emotions\nTo study differences in the emotions conveyed by human- and LLM-generated outputs, \nwe relied on the Hartmann (2022) emotion model. This model is a DistilRoBERTa model \nTable 8  Percentage of articles generated by humans and LLMs that are labeled with different emotions\nModel Emotion\nAnger Disgust Fear Joy Neutral Sadness Surprise\nH 8.04 9.35 10.77 8.30 52.16 8.51 2.87\nM7B 7.29 7.65 8.34 9.80 53.83 9.72 3.37\nF7B 6.11 8.32 8.77 8.53 56.55 8.99 2.73\nLL7B 7.13 7.19 8.68 8.97 55.57 9.43 3.01\nLL13B 7.72 7.41 8.69 9.00 53.95 9.72 3.51\nLL30B 7.39 7.45 8.61 9.54 54.23 9.59 3.19\nLL65B 7.45 8.26 9.25 9.10 53.65 8.80 3.49\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 19 of 28   265 \nfine-tuned on six different datasets tagged with the six basic emotions considered in \n(Ekman, 1992), plus a neutral tag. It has been pretrained on plain text and subsequently \ntrained to generate text embeddings that correspond to emotion labels, contextualized on \nthe full context by Transformers, a modern neural architecture that powers most of the lat-\nest language models. This process goes beyond lexeme matching by contextualizing the \nentire text into a single vector and assigning an emotion to it. It performs well while being \nlightweight and is widely used in the NLP community.\nTable 8 provides the percentage of articles labeled with distinct emotional categories, \nincluding anger, disgust, fear, joy, sadness, surprise, and a special tag \nneutral to denote that no emotion is present in the text.\nFigure 8 depicts the percentage of articles associated with each emotion for each large \nlanguage model used, as compared to human-written texts. As anticipated in journalistic \ntexts, a substantial majority of the lead paragraphs are classified as neutral. This category \naccounts for over 50% of the texts across all models and human-generated samples, with \nthe LLM-generated text demonstrating a slightly higher inclination towards neutrality.\nConcerning the remainder of the samples, human texts demonstrate a greater inclina-\ntion towards negative and aggressive emotions like disgust and fear. However, humans \nand LLMs generated roughly the same amount of angry texts. In contrast, LLMs tend to \ngenerate more texts imbued with positive emotions, such as surprise and especially joy. \nThe LLMs also produce many sad texts, a passive but negative emotion, yet less toxic 6 \nthan emotions such as anger or fear. Across LLaMa models, fear increases as the num-\nber of parameters grows (from LLaMa 13B), making them more akin to human texts. \nSince LLaMa (version 1 models) were not fine-tuned with reinforcement learning with \nhuman feedback, we hypothesize the main source contributing to this issue might be some \nFig. 8  Relative difference of emotion labels of articles generated by different LLMs in comparison to \nhuman texts\n6 We take the toxicity definition from Perspective API (2024), as it is common in other work in the field: “a \nrude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.”\n A. Muñoz-Ortiz et al.\n  265  Page 20 of 28\npre-processing steps used for the LLaMa models, such as removing toxic content from its \ndata. Yet, LLaMa’s technical report (Touvron et al. 2023) mentioned an increase in model \ntoxicity as they scaled up in size despite using the same pre-processing in all cases, which \nis coherent with our findings. When looking at families, Mistral comes closest to express-\ning emotions in a way similar to humans, and Falcon expresses more joy and less anger and \nsurprise than the rest of the models.\nThis difference in emotions may be related to the same regularization effect that affects \nUPOS and sentence length distribution. The LLMs are less able to distinguish between \ndomains and writing styles when they are not trained on instruction tuning datasets, which \ncan account for the observed differences in writing styles.\n4.2.2  Text similarity\nWe conducted an analysis of the cosine semantic similarity between lead paragraphs gener-\nated by various LLMs and their human-authored counterparts. Our objective was to inves-\ntigate the impact of model sizes on the semantic similarity between these texts. To achieve \nso, we used a a state-of-the-art sentence similarity model called all-mpnet-base-v27 \n(Reimers and Gurevych 2019). Figure  9 illustrates the distribution of the similarity scores \nFig. 9  Similarity scores between the sentences generated by the LLMs and human text\nTable 9  Male-to-female ratio \nof pronouns used by the text \ngenerated by humans and each \nLLM\nModel Male–Female ratio Differ-\nence with \nhumans\nH 1.71 –\nM7B 1.74 3.06 %\nF7B 1.64 − 7.54 %\nLL7B 1.86 14.30%\nLL13B 1.89 17.13 %\nLL30B 1.87 15.73 %\nLL65B 1.88 17.04 %\n7 https:// huggi ngface. co/ sente nce- trans forme rs/ all- mpnet- base- v2\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 21 of 28   265 \nobtained from our analysis. Results show that smaller-sized LLMs do not necessarily result \nin a decrease in sentence similarity compared to the human-authored texts. Differences \nacross families are negligible.\n4.2.3  Gender bias\nAlthough related as well in our study with part-of-speech tag distribution, we here sepa-\nrately analyze the proportion between masculine and feminine pronouns used in both \nhuman- and LLM-generated text. Based on the morphological output by Stanza, we find \nthe words that are pronouns have the features Gender=Masc and Gender=Fem, respec-\ntively. Results in Table  9 indicate that the already biased human texts use male pronouns \n1.71 times more frequently than female pronouns. This is exacerbated by all models but \nFalcon 7B, which, although still heavily biased towards male pronouns, reduces the bias \nby 7.5%. LlaMa models, on the contrary, use around 15% more male than female pronouns \nin comparison to humans. This quantity is roughly the same for every size. Mistral 7B lies \nin the middle, with a slight increase of the male–female ratio of 3% with regards to human \ntext.\nThis analysis is limited by the fact that the cause of this disparity could be related to \nthe LLMs writing more generic pronouns than the humans, hence exacerbating the dispar -\nity between male and female references as presented in the news. In English, masculine \npronouns are more widely used as generic pronouns than female or neutral pronouns. This \nwould cause a wider gap between male and female pronouns in the models’ outputs than \nthe already existing male bias caused by men appearing more commonly in news titles than \nwomen.\n5  Conclusion\nThis paper presented a comprehensive study on linguistic patterns in texts produced by \nboth humans and machines, comparing them under controlled conditions. To keep up with \ncurrent trends, we used modern generative models. To ensure the novelty of texts and \naddress memorization concerns, we fed the LLMs headlines from news articles published \nafter the release date of the models. The study revealed that despite generating highly flu-\nent text, these models still exhibited noticeable differences when compared to human-gen-\nerated texts. More precisely, at the lexical level, large language models relied on a more \nrestricted vocabulary, except for LLaMa 65B. Additionally, at the morphosyntactic level, \ndiscernible distinctions were observed between human and machine-generated texts, the \nlatter having a preference for parts of speech displaying (a sense of) objectivity - such as \nsymbols or numbers - while using substantially fewer adjectives. We also observed varia-\ntions in terms of syntactic structures, both for dependency and constituent representations, \nspecifically in the use of dependency and constituent types, as well as the length of spans \nacross both types of texts. In this respect our comparison shows, among other aspects, that \nall tested LLMs choose word orders that optimize dependency lengths to a lesser extent \nthan humans; while they have a tendency to use more auxiliary verbs and verb phrases and \nless noun and prepositional phrases. In terms of semantics, while exhibiting a great text \nsimilarity with respect to the human texts, the models tested manifested less propensity \nthan humans for displaying aggressive negative emotions, such as fear or anger. Mistral \n A. Muñoz-Ortiz et al.\n  265  Page 22 of 28\n7B generated texts whose emotion distributions are more similar to humans than those \nof LLaMa and Falcon models. However, we noted a rise in the volume of negative emo-\ntions with the models’ size. This aligns with prior findings that associate larger sizes with \nheightened toxicity (Touvron et al. 2023). Finally, we detected an inclination towards the \nuse of male pronouns, surpassing the frequency in comparison to their human counter -\nparts. All models except Falcon 7B exacerbated this bias.\nOverall, this work presents the first results and methodology for studying linguistic \ndifferences between English news texts generated by humans and by machines. There is \nplenty of room for improvement in future work. For instance, we could analyze specific \naspects across multiple languages, which would give us a broader understanding of specific \nlinguistic patterns. Additionally, comparing the performance of instruction-based models \ncould provide insights into how different models align with human preferences and handle \nvarious languages. Expanding the analysis to multiple domains could also offer a more \ncomprehensive view of machine-generated text capabilities, revealing their strengths and \nweaknesses in different contexts of our methodology, as well as ways to improve it.\nAppendix A: Statistical analysis of metrics\nWe performed a t-test comparing the human and LLM means for several metrics: sentence \nlength, arc length, and standardized TTR.\nResults show that the differences between the humans and each of the LLMs for these \nmetrics are all statistically significant (p-values < 0.05, see Tables  10, 11, 12). However, \nthese differences are not always statistically significant between models. Specifically, the \ndifferences between LLaMa 7B and LLaMa 30B in the case of STTR, Falcon 7B and \nLLaMa 65B for sentence length, and most of the models for arc length are not statistically \nsignificant. This matches the results, as distributions of these values for humans and LLMs \nare clearly distinct, while they are very similar for all LLMs, mainly in the case of arc and \nsentence length.\nTable 10  P-values of the t-test to compare mean sentence lengths between text generated by humans and \nLLMs\nHuman M7B F7B LL7B LL13B LL30B LL65B\nHuman 1 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001\nM7B <0.001 1 <0.001 <0.001 0.001 0.280 <0.001\nF7B <0.001 <0.001 1 <0.001 <0.001 <0.001 0.085\nLL7B <0.001 <0.001 <0.001 1 <0.001 <0.001 <0.001\nLL13B <0.001 0.001 <0.001 <0.001 1 <0.001 0.003\nLL30B <0.001 0.280 <0.001 <0.001 <0.001 1 <0.001\nLL65B <0.001 <0.001 0.085 <0.001 0.003 <0.001 1\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 23 of 28   265 \nAuthor Contributions Conceptualization: AMO, CGR, DV; Data curation: AMO; Investigation: AMO, \nCGR, DV; Visualization: AMO; Software: AMO; Methodology: AMO, CGR, DV; Project Administration: \nCGR, DV; Software: AMO; Validation: AMO, CGR, DV; Experiments: AMO; Formal analysis: AMO, \nCGR, DV; Writing - original draft: AMO, CGR, DV; Writing - Review & Editing: AMO, CGR, DV; Fund-\ning Adquisition; CGR, DV\nFunding Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature. We \nacknowledge the European Research Council (ERC), which has funded this research under the Horizon \nEurope research and innovation programme (SALSA, grant agreement No 101100615); SCANNER-UDC \n(PID2020-113230RB-C21) funded by MICIU/AEI/10.13039/501100011033; Xunta de Galicia (ED431C \n2020/11); GAP (PID2022-139308OA-I00) funded by MICIU/AEI/10.13039/501100011033/ and by ERDF, \nEU; Grant PRE2021-097001 funded by MICIU/AEI/10.13039/501100011033 and by ESF+ (predoctoral \ntraining grant associated to project PID2020–113230RB-C21); and Centro de Investigación de Galicia \n“CITIC”, funded by the Xunta de Galicia through the collaboration agreement between the Consellería de \nCultura, Educación, Formación Profesional e Universidades and the Galician universities for the reinforce-\nment of the research centres of the Galician University System (CIGUS). Funding for open access charge: \nUniversidade da Coruña/CISUG.\nData Availability The data analyzed on this article has been obtained using the New York Times Archive \nAPI (https://developer.nytimes.com/docs/archive-product/1/overview), gathering all the available articles \nfrom the 1st of October, 2023 to the 24th of January, 2024. We released the code and the used data on: \nhttps://zenodo.org/records/11186264.\nDeclarations \nConflict of interest The authors have no Conflict of interest to declare that are relevant to the content of this \npaper.\nTable 11  P-values of the t-test to compare mean arc lengths between text generated by humans and LLMs\nHuman M7B F7B LL7B LL13B LL30B LL65B\nHuman 1 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001\nM7B <0.001 1 <0.001 0.248 0.463 0.109 0.004\nF7B <0.001 <0.001 1 <0.001 <0.001 <0.001 <0.001\nLL7B <0.001 0.248 <0.001 1 0.674 0.627 0.075\nLL13B <0.001 0.463 <0.001 0.674 1 0.372 0.030\nLL30B <0.001 0.109 <0.001 0.627 0.372 1 0.206\nLL65B <0.001 0.004 <0.001 0.075 0.030 0.206 1\nTable 12  P-values of the t-test to compare mean TTR per segment between text generated by humans and \nLLMs\nHuman M7B F7B LL7B LL13B LL30B LL65B\nHuman 1 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001\nM7B <0.001 1 <0.001 <0.001 <0.001 <0.001 <0.001\nF7B <0.001 <0.001 1 <0.001 <0.001 <0.001 <0.001\nLL7B <0.001 <0.001 <0.001 1 0.010 0.582 <0.001\nLL13B <0.001 <0.001 <0.001 0.010 1 0.003 <0.001\nLL30B <0.001 <0.001 <0.001 0.582 0.003 1 <0.001\nLL65B <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 1\n A. Muñoz-Ortiz et al.\n  265  Page 24 of 28\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAbid A, Farooqi M, Zou J (2021) Persistent anti-muslim bias in large language models. In: Proceedings of \nthe 2021 AAAI/ACM conference on AI, ethics, and society, pp. 298–306\nAlemany-Puig L, Esteban J, Ferrer-i-Cancho R (2021) The linear arrangement library: a new tool for \nresearch on syntactic dependency structures. In: Proceedings of the second Workshop on quantitative \nsyntax (Quasy, SyntaxFest 2021). Association for Computational Linguistics, Sofia, pp. 1–16. https:// \naclan tholo gy. org/ 2021. quasy-1.1\nAlmazrouei E, Alobeidli H, Alshamsi A, Cappelli A, Cojocaru R, Debbah M, Goffinet É, Hesslow D, Lau-\nnay J, Malartic Q et al (2023) The falcon series of open language models. arXiv preprint arXiv: 2311. \n16867\nBender EM, Gebru T, McMillan-Major A, Shmitchell S (2021) On the dangers of stochastic parrots: can \nlanguage models be too big? In: Proceedings of the 2021 ACM conference on fairness, accountability, \nand transparency, pp. 610–623\nBender EM, Koller A (2020) Climbing towards NLU: On meaning, form, and understanding in the age of \ndata. In: Proceedings of the 58th annual meeting of the association for computational linguistics, pp. \n5185–5198. Association for Computational Linguistics. https:// doi. org/ 10. 18653/ v1/ 2020. acl- main. \n463 . https:// aclan tholo gy. org/ 2020. acl- main. 463\nBerzak Y, Huang Y, Barbu A, Korhonen A, Katz B (2016) Anchoring and agreement in syntactic annota-\ntions. In: Proceedings of the 2016 conference on empirical methods in natural language processing. \nAssociation for Computational Linguistics, Austin, pp. 2215–2224. https:// doi. org/ 10. 18653/ v1/ D16- \n1239 . https:// aclan tholo gy. org/ D16- 1239\nBlasi DE, Henrich J, Adamou E, Kemmerer D, Majid A (2022) Over-reliance on English hinders cognitive \nscience. Trends Cogn Sci 26(12):1153–1170\nBrown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell \nA, Agarwal S, Herbert-Voss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler D, Wu J, Winter \nC, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, McCandlish S, Radford \nA, Sutskever I, Amodei D (2020) Language models are few-shot learners In: Larochelle, H., Ranzato, \nM., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in neural information processing systems, vol. \n33, pp. 1877–1901. Curran Associates, Inc https:// proce edings. neuri ps. cc/ paper_ files/ paper/ 2020/ file/ \n1457c 0d6bf cb496 7418b fb8ac 142f6 4a- Paper. pdf\nCai ZG, Haslett DA, Duan X, Wang S., Pickering MJ (2023) Does ChatGPT resemble humans in language \nuse? arXiv preprint arXiv: 2303. 08014\nChiang C-H, Lee H-y (2023) Can large language models be an alternative to human evaluations? In: Pro-\nceedings of the 61st annual meeting of the association for computational linguistics, Vol 1 (long \npapers). Association for Computational Linguistics, Toronto, pp. 15607–15631. https:// aclan tholo gy. \norg/ 2023. acl- long. 870\nChong D, Hong J, Manning C (2022) Detecting label errors by using pre-trained language models. In: Pro-\nceedings of the 2022 conference on empirical methods in natural language processing. Association for \nComputational Linguistics, Abu Dhabi, pp. 9074–9091. https:// aclan tholo gy. org/ 2022. emnlp- main. 618\nChowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, Barham P, Chung HW, Sutton C, Gehr-\nmann S et al (2023) Palm: scaling language modeling with pathways. J Mach Learn Res 24(240):1–113\nCrothers E, Japkowicz N, Viktor H, Branco P (2022) Adversarial robustness of neural-statistical features \nin detection of generative transformers. 2022 International Joint Conference on Neural Networks \n(IJCNN). IEEE, pp 1–8\nCrothers E, Japkowicz N, Viktor HL (2023) Machine-generated text: a comprehensive survey of threat mod-\nels and detection methods. IEEE Access\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 25 of 28   265 \nDodge J, Sap M, Marasović A, Agnew W, Ilharco G, Groeneveld D, Mitchell M, Gardner M (2021) Doc-\numenting large webtext corpora: a case study on the colossal clean crawled corpus. arXiv preprint \narXiv: 2104. 08758\nDugan L, Ippolito D, Kirubarajan A, Callison-Burch C (2020) RoFT: A tool for evaluating human detection \nof machine-generated text. In: Proceedings of the 2020 conference on empirical methods in natural \nlanguage processing: system demonstrations. Association for Computational Linguistics, pp. 189–196. \nhttps:// doi. org/ 10. 18653/ v1/ 2020. emnlp- demos. 25 . https:// aclan tholo gy. org/ 2020. emnlp- demos. 25\nEldan R, Li Y (2023) Tinystories: how small can language models be and still speak coherent English? \narXiv preprint arXiv: 2305. 07759\nFerrer-i-Cancho R (2004) Euclidean distance between syntactically linked words. Phys Rev E 70:056135. \nhttps:// doi. org/ 10. 1103/ PhysR evE. 70. 056135\nFerrer-i-Cancho R, Liu H (2014) The risks of mixing dependency lengths from sequences of different \nlength. Glottotheory 5(2):143–155. https:// doi. org/ 10. 1515/ glot- 2014- 0014\nFerrer-i-Cancho R, Gómez-Rodríguez C, Esteban JL, Alemany-Puig L (2022) Optimality of syntactic \ndependency distances. Phys Rev E 105(1):014308\nFröhling L, Zubiaga A (2021) Feature-based detection of automated language models: tackling GPT-2, \nGPT-3 and Grover. PeerJ Comput Sci 7:443\nFutrell R, Mahowald K, Gibson E (2015) Large-scale evidence of dependency length minimization in 37 \nlanguages. Proc Natl Acad Sci 112(33):10336–10341. https:// doi. org/ 10. 1073/ pnas. 15021 34112\nGao L, Biderman S, Black S, Golding L, Hoppe T, Foster C, Phang J, He H, Thite ANabeshima N et al \n(2020) The pile: an 800gb dataset of diverse text for language modeling. arXiv preprint arXiv: 2101. \n00027\nGunasekar S, Zhang Y, Aneja J, Mendes CCT, Del Giorno A, Gopi S, Javaheripi M, Kauffmann P, Rosa G, \nSaarikivi O et al (2023) Textbooks are all you need. arXiv preprint arXiv: 2306. 11644\nHartmann J (2022) Emotion English DistilRoBERTa-base. https:// huggi ngface. co/j- hartm ann/ emoti on- engli \nsh- disti lrobe rta- base/\nHartvigsen T, Gabriel S, Palangi H, Sap M, Ray D, Kamar E (2022) ToxiGen: a large-scale machine-gen-\nerated dataset for adversarial and implicit hate speech detection. In: Proceedings of the 60th annual \nmeeting of the association for computational linguistics, Vol. 1 (long papers). Association for Com-\nputational Linguistics, Dublin, pp. 3309–3326. https:// doi. org/ 10. 18653/ v1/ 2022. acl- long. 234 . https:// \naclan tholo gy. org/ 2022. acl- long. 234\nHe X, Nassar I, Kiros J, Haffari G, Norouzi M (2022) Generate, annotate, and learn: NLP with synthetic \ntext. Trans Assoc Comput Linguistics 10:826–842. https:// doi. org/ 10. 1162/ tacl_a_ 00492\nHoffmann J, Borgeaud S, Mensch A, Buchatskaya E, Cai T, Rutherford E, Casas DdL, Hendricks LA, Welbl \nJ, Clark A et al (2022) Training compute-optimal large language models. arXiv preprint arXiv: 2203. \n15556\nIppolito D, Duckworth D, Callison-Burch C, Eck D (2020) Automatic detection of generated text is easiest \nwhen humans are fooled. In: Proceedings of the 58th annual meeting of the association for computa-\ntional linguistics. Association for Computational Linguistics, pp. 1808–1822.https:// doi. org/ 10. 18653/ \nv1/ 2020. acl- main. 164 . https:// aclan tholo gy. org/ 2020. acl- main. 164\nJavaheripi M, Bubeck S, Abdin M, Aneja J, Bubeck S, Mendes CCT, Chen W, Del Giorno A, Eldan R, Gopi \nS et al (2023) Phi-2: the surprising power of small language models. Microsoft Research Blog\nJiang AQ, Sablayrolles A, Mensch A, Bamford C, Chaplot DS, Casas Ddl, Bressand F, Lengyel G, Lample \nG, Saulnier L et al (2023) Mistral 7b. arXiv preprint arXiv: 2310. 06825\nKumar S, Balachandran V, Njoo L, Anastasopoulos A, Tsvetkov Y (2023 Language generation models can \ncause harm: So what can we do about it? An actionable survey. In: Proceedings of the 17th conference \nof the European chapter of the association for computational linguistics. Association for Computa-\ntional Linguistics, Dubrovnik, pp. 3299–3321. https:// aclan tholo gy. org/ 2023. eacl- main. 241\nKumar V, Choudhary A, Cho E (2020) Data augmentation using pre-trained transformer models. In: Pro-\nceedings of the 2nd workshop on life-long learning for spoken language systems. Association for Com-\nputational Linguistics, Suzhou, pp. 18–26. https:// aclan tholo gy. org/ 2020. lifel ongnlp- 1.3\nLandgrebe J, Smith B (2021) Making AI meaningful again. Synthese 198:2061–2081\nLeong CS-Y, Linzen T (2023) Language models can learn exceptions to syntactic rules. In: Hunter, T., \nPrickett, B. (eds.) Proceedings of the society for computation in linguistics 2023. Association for Com-\nputational Linguistics, Amherst, pp. 133–144. https:// aclan tholo gy. org/ 2023. scil-1. 11\nLiang PP, Wu C, Morency L-P, Salakhutdinov R (2021) Towards understanding and mitigating social biases \nin language models. In: International conference on machine learning. PMLR, pp 6565–6576\nLiao W, Liu Z, Dai H, Xu S, Wu Z, Zhang Y, Huang X, Zhu D, Cai H, Liu T et al (2023) Differentiate \nChatGPT-generated and human-written medical texts. arXiv preprint arXiv: 2304. 11567\n A. Muñoz-Ortiz et al.\n  265  Page 26 of 28\nLi Y, Bubeck S, Eldan R, Del Giorno A, Gunasekar S, Lee YT (2023a) Textbooks are all you need ii: phi-\n1.5 technical report. arXiv preprint arXiv: 2309. 05463\nLi K, Hopkins AK, Bau D, Viégas F, Pfister H, Wattenberg M (2022) Emergent world representations: \nexploring a sequence model trained on a synthetic task. arXiv preprint arXiv: 2210. 13382\nLiu NF, Zhang T, Liang P (2023) Evaluating verifiability in generative search engines. arXiv preprint arXiv: \n2304. 09848\nLiu H, Xu C, Liang J (2017) Dependency distance: a new perspective on syntactic patterns in natural lan-\nguages. Phys Life Rev 21:171–193. https:// doi. org/ 10. 1016/j. plrev. 2017. 03. 002\nLiu B, Bubeck S, Eldan R, Kulkarni J, Li Y, Nguyen A, Ward R, Zhang Y (2023b) Tinygsm: achieving> \n80% on gsm8k with small language models. arXiv preprint arXiv: 2312. 09241\nLi Z, Zhu H, Lu Z, Yin M (2023b) Synthetic data generation with large language models for text clas-\nsification: potential and limitations. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the \n2023 conference on empirical methods in natural language processing. Association for Computa-\ntional Linguistics, Singapore, pp. 10443–10461. https:// doi. org/ 10. 18653/ v1/ 2023. emnlp- main. 647 \n. https:// aclan tholo gy. org/ 2023. emnlp- main. 647\nLucy L, Bamman D (2021) Gender and representation bias in GPT-3 generated stories. In: Proceedings \nof the third workshop on narrative understanding. Association for Computational Linguistics, Vir -\ntual, pp. 48–55. https:// doi. org/ 10. 18653/ v1/ 2021. nuse-1.5 . https:// aclan tholo gy. org/ 2021. nuse-1.5\nManning CD (2011) Part-of-speech tagging from 97% to 100%: is it time for some linguistics? Interna-\ntional conference on intelligent text processing and computational linguistics. Springer, pp 171–189\nMarcus M, Kim G, Marcinkiewicz MA, MacIntyre R, Bies A, Ferguson M, Katz K, Schasberger B \n(1994) The Penn Treebank: annotating predicate argument structure. In: Human language technol-\nogy: proceedings of a workshop held at plainsboro, NJ, 8–11 March 1994. https:// aclan tholo gy. org/  \nH94- 1020\nMartínez G, Conde J, Reviriego P, Merino-Gómez E, Hernández JA, Lombardi F (2023) How many \nwords does chatgpt know? The answer is chatwords. arXiv preprint arXiv: 2309. 16777\nMartínez G, Hernández JA, Conde J, Reviriego P, Merino E (2024) Beware of words: evaluating the \nlexical richness of conversational large language models. arXiv preprint arXiv: 2402. 15518\nMcCarthy PM, Jarvis S (2010) Mtld, vocd-d, and hd-d: a validation study of sophisticated approaches to \nlexical diversity assessment. Behav Res Methods 42(2):381–392\nMunir S, Batool B, Shafiq Z, Srinivasan P, Zaffar F (2021) Through the looking glass: Learning to \nattribute synthetic text generated by language models. In: Proceedings of the 16th conference of \nthe European chapter of the association for computational linguistics: main volume. Association \nfor Computational Linguistics, pp. 1811–1822. https:// doi. org/ 10. 18653/ v1/ 2021. eacl- main. 155 . \nhttps:// aclan tholo gy. org/ 2021. eacl- main. 155\nNguyen-Son H-Q, Tieu N-DT, Nguyen HH, Yamagishi J, Zen IE (2017) Identifying computer-generated \ntext using statistical analysis. 2017 Asia-Pacific Signal and Information Processing Association \nAnnual Summit and Conference (APSIPA ASC). IEEE, pp 1504–1511\nOpenAI (2023) GPT-4 technical report\nPenedo G, Malartic Q, Hesslow D, Cojocaru R, Cappelli A, Alobeidli H, Pannier B, Almazrouei E, Lau-\nnay J (2023) The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, \nand web data only. arXiv preprint arXiv: 2306. 01116\nPeng B, Li C, He P, Galley M, Gao J (2023) Instruction tuning with GPT-4. arXiv preprint arXiv: 2304.  \n03277\nPerspective API (2024) About the API FAQs. https:// devel opers. persp ectiv  eapi. com/s/ about- the- api- \nfaqs? langu age= en_ US. Accessed 22 May 2024\nPetrov S, Das D, McDonald R (2012) A universal part-of-speech tagset. In: Proceedings of the eighth \ninternational conference on Language Resources and Evaluation (LREC’12). European Language \nResources Association (ELRA), Istanbul, pp. 2089–2096. http:// www. lrec- conf. org/ proce edings/  \nlrec2 012/ pdf/ 274_ Paper. pdf\nPillutla K, Swayamdipta S, Zellers R, Thickstun J, Welleck S, Choi Y, Harchaoui Z (2021) Mauve: \nmeasuring the gap between neural text and human text using divergence frontiers. Adv Neural Inf \nProcess Syst 34:4816–4828\nQi P, Zhang Y, Zhang Y, Bolton J, Manning CD (2020) Stanza: A python natural language processing \ntoolkit for many human languages. In: Proceedings of the 58th annual meeting of the association for \ncomputational linguistics: system demonstrations. Association for Computational Linguistics, pp. 101–\n108. https:// doi. org/ 10. 18653/ v1/ 2020. acl- demos. 14 . https:// aclan tholo gy. org/ 2020. acl- demos. 14\nRadford A, Narasimhan K, Salimans T, Sutskever I et al (2018) Improving language understanding by \ngenerative pre-training\nContrasting Linguistic Patterns in Human and LLM‑Generated…\nPage 27 of 28   265 \nRaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ (2020) Exploring the \nlimits of transfer learning with a unified text-to-text transformer. J Mach Learn Res 21(140):1–67\nReimers N, Gurevych I (2019) Sentence-Bert: sentence embeddings using Siamese Bert-networks. In: \nProceedings of the 2019 conference on empirical methods in natural language processing. Associa-\ntion for Computational Linguistics. https:// arxiv. org/ abs/ 1908. 10084\nRodriguez J, Hay T, Gros D, Shamsi Z, Srinivasan R (2022) Cross-domain detection of GPT-2-generated \ntechnical text. In: Proceedings of the 2022 conference of the North American chapter of the asso-\nciation for computational linguistics: human language technologies. Association for Computational \nLinguistics, Seattle, pp. 1213–1233. https:// doi. org/ 10. 18653/ v1/ 2022. naacl- main. 88 . https:// aclan \ntholo gy. org/ 2022. naacl- main. 88\nRosenfeld A, Lazebnik T(2024) Whose LLM is it anyway? Linguistic comparison and LLM attribution \nfor GPT-3.5, GPT-4 and bard. arXiv preprint arXiv: 2402. 14533\nSadasivan VS, Kumar A, Balasubramanian S, Wang W, Feizi S (2023) Can AI-generated text be reliably \ndetected? arXiv preprint arXiv: 2303. 11156\nSahu G, Rodriguez P, Laradji I, Atighehchian P, Vazquez D, Bahdanau D (2022) Data augmentation for \nintent classification with off-the-shelf large language models. In: Proceedings of the 4th workshop on \nNLP for conversational AI. Association for Computational Linguistics, Dublin, pp. 47–57. https:// doi. \norg/ 10. 18653/ v1/ 2022. nlp4c onvai-1.5 . https:// aclan tholo gy. org/ 2022. nlp4c onvai-1.5\nSanturkar S, Durmus E, Ladhak F, Lee C, Liang P, Hashimoto T (2023) Whose opinions do language mod-\nels reflect? arXiv preprint arXiv: 2303. 17548\nScao TL, Fan A, Akiki C, Pavlick E, Ilić S, Hesslow D, Castagné R, Luccioni AS, Yvon F, Gallé M et al \n(2022) Bloom: a 176b-parameter open-access multilingual language model. arXiv preprint arXiv: \n2211. 05100\nShumailov I, Shumaylov Z, Zhao Y, Gal Y, Papernot N, Anderson R (2023) The curse of recursion: training \non generated data makes models forget. arXiv preprint arXiv: 2305. 17493\nSøgaard A (2022) Understanding models understanding language. Synthese 200(6):443\nStribling J, Krohn M, Aguayo D (2005) Scigen—an automatic CS paper generator\nSwayamdipta S, Schwartz R, Lourie N, Wang Y, Hajishirzi H, Smith NA, Choi Y (2020) Dataset cartogra-\nphy: mapping and diagnosing datasets with training dynamics. In: Proceedings of the 2020 conference \non Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Lin-\nguistics, pp. 9275–9293. https:// doi. org/ 10. 18653/ v1/ 2020. emnlp- main. 746 . https:// aclan tholo gy. org/ \n2020. emnlp- main. 746\nTang R, Chuang Y-N, Hu X (2024) The science of detecting LLM—generated text. Commun ACM \n67(4):50–59. https:// doi. org/ 10. 1145/ 36247 25\nTang R, Han X, Jiang X, Hu X (2023) Does synthetic data generation of LLMs help clinical text mining? \narXiv preprint arXiv: 2303. 04360\nTaori R, Gulrajani I, Zhang T, Dubois Y, Li X, Guestrin C, Liang P, Hashimoto TB (2023) Alpaca: a strong, \nreplicable instruction-following model. Stanford Center for Research on Foundation Models. https://\ncrfm. stanford. edu/2023/03/13/alpaca. html 3(6), 7\nTouvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, Bashlykov N, Batra S, Bhargava P, Bho-\nsale S, Bikel D, Blecher L, Ferrer CC, Chen M, Cucurull G, Esiobu D, Fernandes J, Fu J, Fu W, \nFuller B, Gao C, Goswami V, Goyal N, Hartshorn A, Hosseini S, Hou R, Inan H, Kardas M, Kerkez \nV, Khabsa M, Kloumann I, Korenev A, Koura PS, Lachaux M-A, Lavril T, Lee J, Liskovich D, Lu Y, \nMao Y, Martinet X, Mihaylov T, Mishra P, Molybog I, Nie Y, Poulton A, Reizenstein J, Rungta R, \nSaladi K, Schelten A, Silva R, Smith EM, Subramanian R, Tan XE, Tang B, Taylor R, Williams A, \nKuan JX, Xu P, Yan Z, Zarov I, Zhang Y, Fan A, Kambadur M, Narang S, Rodriguez A, Stojnic R, \nEdunov S, Scialom T (2023) Llama 2: open foundation and fine-tuned chat models\nWang A, Singh A, Michael J, Hill F, Levy O, Bowman SR (2018) Glue: a multi-task benchmark and analy -\nsis platform for natural language understanding. arXiv preprint arXiv: 1804. 07461\nWeidinger L, Mellor J, Rauh M, Griffin C, Uesato J, Huang P-S, Cheng M, Glaese M, Balle B, Kasirzadeh \nA et  al (2021) Ethical and social risks of harm from language models. arXiv preprint arXiv: 2112. \n04359\nWei J, Huang D, Lu Y, Zhou D, Le QV (2023) Simple synthetic data reduces sycophancy in large language \nmodels. arXiv preprint arXiv: 2308. 03958\nXu Q, Peng Y, Wu M, Xiao F, Chodorow M, Li P (2023) Does conceptual representation require embodi-\nment? insights from large language models. arXiv preprint arXiv: 2305. 19103\nYang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV (2019) Xlnet: generalized autoregressive \npretraining for language understanding. Adv Neural Inf Process Syst 32\nZhan H, He X, Xu Q, Wu Y, Stenetorp P (2023) G3detector: general GPT-generated text detector. arXiv \npreprint arXiv: 2305. 12680\n A. Muñoz-Ortiz et al.\n  265  Page 28 of 28\nZhou J, Zhang Y, Luo Q, Parker AG, De Choudhury M (2023) Synthetic lies: understanding AI-generated \nmisinformation and evaluating algorithmic and human solutions. In: Proceedings of the 2023 CHI con-\nference on human factors in computing systems, pp. 1–20\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.6255768537521362
    },
    {
      "name": "Computer science",
      "score": 0.43667319416999817
    },
    {
      "name": "Natural language processing",
      "score": 0.38427990674972534
    },
    {
      "name": "Psychology",
      "score": 0.3513493537902832
    },
    {
      "name": "Sociology",
      "score": 0.32567721605300903
    },
    {
      "name": "Philosophy",
      "score": 0.10739460587501526
    }
  ]
}