{
  "title": "Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning",
  "url": "https://openalex.org/W4385574560",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2224432596",
      "name": "Shuo Xie",
      "affiliations": [
        "Toyota Technological Institute at Chicago",
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2810472222",
      "name": "Jiahao Qiu",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2765613287",
      "name": "Ankita Pasad",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2110822479",
      "name": "Li Du",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2106430836",
      "name": "Qing Qu",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2202636965",
      "name": "Hongyuan Mei",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W4221153612",
    "https://openalex.org/W4225777963",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W4285242720",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W1883346539",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W3155744586",
    "https://openalex.org/W4226380987",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W4313296431",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2395238154",
    "https://openalex.org/W3159913668",
    "https://openalex.org/W3103838460",
    "https://openalex.org/W4287685082",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3154971029",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4226499584",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W4225576969",
    "https://openalex.org/W3098300729",
    "https://openalex.org/W4287626589",
    "https://openalex.org/W4221157341",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2171033594",
    "https://openalex.org/W3205278400",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3065974826",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W191923771",
    "https://openalex.org/W4315641811",
    "https://openalex.org/W4287816361",
    "https://openalex.org/W3202070718",
    "https://openalex.org/W2970712718",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W3199348444",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W4221148569",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W4281638141",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3169280296",
    "https://openalex.org/W3166439572",
    "https://openalex.org/W4221155755",
    "https://openalex.org/W3202352393",
    "https://openalex.org/W3122890974"
  ],
  "abstract": "While transferring a pretrained language model, common approaches conventionally attach their task-specific classifiers to the top layer and adapt all the pretrained layers. We investigate whether one could make a task-specific selection on which subset of the layers to adapt and where to place the classifier. The goal is to reduce the computation cost of transfer learning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its performance.We propose to select layers based on the variability of their hidden states given a task-specific corpus. We say a layer is already \"well-specialized\" in a task if the within-class variability of its hidden states is low relative to the between-class variability. Our variability metric is cheap to compute and doesn't need any training or hyperparameter tuning. It is robust to data imbalance and data scarcity. Extensive experiments on the GLUE benchmark demonstrate that selecting layers based on our metric can yield significantly stronger performance than using the same number of top layers and often match the performance of fine-tuning or adapter-tuning the entire language model.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5750–5768\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nHidden State Variability of Pretrained Language Models\nCan Guide Computation Reduction for Transfer Learning\nShuo Xie∗1,2 Jiahao Qiu3 Ankita Pasad2 Li Du4 Qing Qu3 Hongyuan Mei2\n1University of Chicago 2Toyota Technological Institute at Chicago\n3University of Michigan 4Johns Hopkins University\nshuox@uchicago.edu,hongyuan@ttic.edu\nAbstract\nWhile transferring a pretrained language model,\ncommon approaches conventionally attach their\ntask-specific classifiers to the top layer and\nadapt all the pretrained layers. We investigate\nwhether one could make a task-specific selec-\ntion on which subset of the layers to adapt and\nwhere to place the classifier.The goal is to re-\nduce the computation cost of transfer learning\nmethods (e.g. fine-tuning or adapter-tuning)\nwithout sacrificing its performance.\nWe propose to select layers based on the vari-\nability of their hidden states given a task-\nspecific corpus. We say a layer is already “well-\nspecialized” in a task if the within-class vari-\nability of its hidden states is low relative to the\nbetween-class variability. Our variability met-\nric is cheap to compute and doesn’t need any\ntraining or hyperparameter tuning. It is robust\nto data imbalance and data scarcity. Extensive\nexperiments on the GLUE benchmark demon-\nstrate that selecting layers based on our metric\ncan yield significantly stronger performance\nthan using the same number of top layers and\noften match the performance of fine-tuning or\nadapter-tuning the entire language model.\n1 Introduction\nTransfer learning from a pretrained language\nmodel (PLM) is now the de-facto paradigm in nat-\nural language processing (NLP). The conventional\napproaches of leveraging PLMs include fine-tuning\nall the parameters in the language model (LM)\nand some lightweight alternatives that can decrease\nthe number of tuning parameters such as adapter-\ntuning (Houlsby et al., 2019; Hu et al., 2022; He\net al., 2022) and prefix-tuning (Li and Liang, 2021).\nThese methods have one thing in common: they\nall involve the entire PLM and attach a classifier to\nits top layer. However, PLMs were optimized via\nthe language modeling objective and thus their top\n∗Work done during internship at TTI-Chicago.\nlayers have been specialized in producing represen-\ntations which facilitate optimizing that objective.\nSuch mismatch between the pretraining and fine-\ntuning objectives poses the following questions:\n① Given a pretrained language model and a down-\nstream task, can we measure how “well-specialized”\neach layer has already been in that task, without\nany task-specific tuning?\n② If the answer to ① is yes, can we use the\nlayer-wise “task-specialty” as a guide in improving\nthe computation efficiency of the transfer learning\nmethods such as fine-tuning and adapter-tuning?\nIn this paper, we take a technically principled\napproach to investigate the research questions ①\nand ②. First, we define a metric in section 3.1\nto measure the “task-specialty” of each layer in a\ngiven PLM. Our task-speciality score is inspired\nby the neural collapse (NC) phenomenon which\nhas been widely observed in the computer vision\ncommunity (Papyan et al., 2020): as training con-\nverges, the top-layer representations of the images\nwith the same label form an extremely tight cluster.\nIn our setting, we examine the variability of the\nrepresentations of the linguistic sequences given\nby each layer of the PLM, and define our layer-\nwise task-specialty to be the within-class variability\nnormalized by the between-class variability. Com-\nputing our metric does not require any training or\nhyperparameter tuning. Experiments on the GLUE\nbenchmark demonstrate that it is highly correlated\nwith layer-wise probing performance, thus giving a\nclear “yes” to the question ① above.\nWe propose several layer-selecting strategies in\nsection 3.2 based on our proposed task-specialty\nmetric. Our strategies are complementary to all\nthe major paradigms of transfer learning (such as\nfine-tuning and adapter-tuning) and thus can take\nadvantages of the state-of-the-art at the time: only\nthe selected layers will be tuned (e.g., via fine-\ntuning or using adapters) such that the computation\n5750\ncost of the tuning methods can be further reduced.\nExperiments on the GLUE benchmark demonstrate\nthat our proposed strategies are highly effective: un-\nder comparable computation budget, fine-tuning or\nadapter-tuning the layers selected by our strategies\ncan achieve significantly higher performance than\nusing the layers selected by the widely adopted\nbaseline strategies; it can even often match the per-\nformance of fine-tuning or adapter-tuning the entire\nPLM which takes 500% more computation cost.\nThrough extensive ablation studies, we demon-\nstrate the comparable advantages of our proposed\ntask-specialty metric over potential alternatives\n(such as CCA and mutual information) as well as\nits robustness to data scarcity and data imbalance.\n2 Technical Background\nIn this paper, we focus on classification tasks.\nTechnically, each classification task has a corpus\nof training data {(xn,yn)}N\nn=1 where each xn =\n(xn,1,...,x n,T ) is a sequence of linguistic tokens\nand each yn ∈Y is a discrete class label. Such\ntasks include\n• Sentiment analysis. Each x is a single sequence\nof words such as “This movie is fantastic” and\nyis a sentiment label from {positive,negative}.\nThus, the sentiment analysis can be cast as a\nbinary-class classification problem.\n• Natural language inference. Each x is of the\nform “premise [SEP] hypothesis” such as “Fun\nfor adults and children. [SEP] Fun for only chil-\ndren.” where “[SEP]” is a special separator to-\nken. The label y ∈{yes,neutral,no}indicates\nwhether the premise entails the hypothesis. It is\na three-class classification problem.\nA PLM performs a classification task as follows:\n1. It reads each given sequence xn and embed it\ninto a series of hidden state vectors\nlayer L h(L)\nn,0 h(L)\nn,1 ... h(L)\nn,t ... h(L)\nn,T\n...\nlayer ℓ h(ℓ)\nn,0 h(ℓ)\nn,1 ... h(ℓ)\nn,t ... h(ℓ)\nn,T\n...\nlayer 1 h(1)\nn,0 h(1)\nn,1 ... h(1)\nn,t ... h(1)\nn,T\nwhere h(ℓ)\nn,t denotes the hidden state of tokenxn,t\ngiven by layer ℓand xn,0 = CLS is a special\nclassification (CLS) token.\n2. The top-layer hidden state h(L)\nn,0 of the CLS to-\nken is read by a neural network f followed by a\nsoftmax layer, which gives the probability dis-\n(a) High within-class vari-\nability and low between-class\nvariability.\n(b) Low within-class variabil-\nity and high between-class\nvariability.\nFigure 1: An illustration of our variability-based task-\nspecialty metric with hypothetical data. Each dot de-\nnotes a two-dimensional hidden state vector and its color\ndenotes its target label. Each colored star denotes the\nmean vector of its class.\ntribution over the target label y∈Y:\np(y|xn) = softmaxy(f(h(L)\nn,0 )) (1)\nThe net f is also called “classification head”.\nTransfer learning is to maximize the log\nprobability of the ground-truth label yn—i.e.,\nlog p(yn |xn)—by learning the parameters of the\nclassification head f as well as certain method-\nspecific parameters:\n• Fine-tuning updates all the PLM parameters (Pe-\nters et al., 2018; Devlin et al., 2018).\n• Adapter-tuning inserts adapters (i.e., small neural\nnetworks) into the LM layers and updates the\nnew adapter parameters (Houlsby et al., 2019;\nHu et al., 2022; He et al., 2022).\n• Prefix-tuning augments trainable tokens to the\ninput x and tunes the new token embeddings (Li\nand Liang, 2021; Qin and Eisner, 2021; Ham-\nbardzumyan et al., 2021).\n3 The Method\nOur goal is to answer the research questions ①\nand ② introduced in section 1. That involves find-\ning a layer-specific metric ν(1),...,ν (ℓ),...,ν (L)\nwhere each ν(ℓ) measures the task-specialty of layer\nℓ. Suppose that we use s(ℓ) to denote the task score\nthat we can achieve by letting the classification\nhead read the layer ℓhidden state h(ℓ)\nn,0 of the CLS\ntoken. If ν(ℓ) is highly (positively or negatively)\ncorrelated with s(ℓ), then the answer to question ①\nis yes. To answer question ② involves designing\nν-based strategies that select a subset of layers to\nuse in transfer learning approaches.\nIn this section, we introduce our task-specialty\nmetric ν(ℓ) (section 3.1) along with a few strategies\nfor selecting layers (section 3.2). In section 4, we\nwill empirically demonstrate the effectiveness of\nour proposed metric and strategies.\n5751\n3.1 Hidden State Variability Ratio\nFor a given task, we define our task-specialty\nmetric ν(1),...,ν (L) based on the variability of\nthe hidden state vectors that the PLM produces by\nembedding the training input sequences {xn}N\nn=1.\nWe use hypothetical data to illustrate our intuition\nin Figure 1: after grouped based on the target labels\nyn, the variability of the hidden states within the\nsame group (dots of same color) measures the dif-\nficulty of separating them, while the variability of\nthe mean states (stars of different colors) quantifies\nhow easy it is to tell the different groups apart.\nTechnically, for each layerℓ, we first define the\nsequence-level hidden state h(ℓ)\nn for each input xn\nto be the average of the hidden states of all the\n(non-CLS) tokens h(ℓ)\nn\ndef\n= 1\nT\n∑T\nt=1 h(ℓ)\nn,t. These\nsequence-level states correspond to the dots in Fig-\nure 1. Then we group all the h(ℓ)\nn based on the\ntarget labels yn: G(ℓ)\ny\ndef\n= {h(ℓ)\nn : yn = y}. The\nmean vector of each group is defined as ¯h(ℓ)\ny\ndef\n=\n1\n|G(ℓ)\ny |\n∑\nh∈G(ℓ)\ny\nh and they correspond to the stars\nin Figure 1. The mean vector between classes is\ndefined as ¯h(ℓ) def\n= 1\n|Y|\n∑\ny∈Y¯h(ℓ)\ny . Then the within-\ngroup variability Σ(ℓ)\nw and between-group variabil-\nity Σ(ℓ)\nb are defined using the sequence-level states\nand mean states:\nΣ(ℓ)\nw\ndef\n= 1\n|Y|\n∑\ny∈Y\n1\n|G(ℓ)\ny |\n∑\nh∈G(ℓ)\ny\n(h −¯h(ℓ)\ny )(h −¯h(ℓ)\ny )⊤\nΣ(ℓ)\nb\ndef\n= 1\n|Y|\n∑\ny∈Y\n(¯h(ℓ)\ny −¯h(ℓ))(¯h(ℓ)\ny −¯h(ℓ))⊤\nBoth Σ(ℓ)\nw and Σ(ℓ)\nb are a lot like covariance matri-\nces since they measure the deviation from the mean\nvectors. Finally, we define our task-specialty met-\nric to be the within-group variability Σ(ℓ)\nw scaled\nand rotated by the pseudo-inverse of between-class\nvariability Σ(ℓ)\nb\nν(ℓ) def\n= 1\n|Y|trace\n(\nΣ(ℓ)\nw Σ\n(ℓ)†\nb\n)\n(3)\nThe pseudo-inverse in equation (3) is why we use\nthe average state as our sequence-level representa-\ntion: averaging reduces the noise in the state vec-\ntors and thus leads to stable computation of ν(ℓ).\nWe believe that the layers with small ν(ℓ) are\nlikely to do better than those with large ν(ℓ) when\ntransferred to the downstream task. Our belief\nstems from two key insights.\nRemark-I: neural collapse. Our proposed metric\nis mainly inspired by the neural collapse (NC) phe-\nnomenon: when training a deep neural model in\nclassifying images, one can see that the top-layer\nrepresentations of the images with the same label\nform an extremely tight cluster as training con-\nverges. Extensive theoretical and empirical studies\nshow that a lower within-class variability can in-\ndicate a better generalization (Papyan et al., 2020;\nHui et al., 2022; Galanti et al., 2022). Thus we\nexamine the variability of the layer-wise represen-\ntations of the linguistic sequences and hope that it\ncan measure the task-specialty of each layer of the\ngiven PLM. Our metric is slightly different from\nthe widely accepted neural collapse metric; please\nsee Appendix A.1 for a detailed discussion.\nRemark-II: signal-to-noise ratio. In multivariate\nstatistics (Anderson, 1973), trace\n(\nΣwΣ†\nb\n)\nis able\nto measure the inverse signal-to-noise ratio for clas-\nsification problems and thus a lower value indicates\na lower chance of misclassification. Intuitively, the\nbetween-class variability Σb is the signal which\none can use to tell different clusters apart while the\nwithin-class variability Σw is the noise that makes\nthe clusters overlapped and thus the separation dif-\nficult; see Figure 1 for examples.\nRemark-III: linear discriminant analysis. A low\nνimplies that it is easy to correctly classify the data\nwith linear discriminant analysis (LDA) (Hastie\net al., 2009). Technically, LDA assumes that the\ndata of each class is Gaussian-distributed and it\nclassifies a new data pointh by checking how close\nit is to each mean vector ¯hy scaled by the covari-\nance matrix Σ which is typically shared across\nclasses. Though our metric does not make the\nGaussian assumption, a low ν suggests that the\nclass means ¯hy are far from each other relative to\nthe within-class variations Σw, meaning that the\ndecision boundary of LDA would tend to be sharp.\nActually, our Σw is an estimate to the Gaussian\ncovariance matrix Σ of LDA.\n3.2 Layer-Selecting Strategies\nSuppose that our metric ν can indeed measure\nthe task-specialty of each layer. Then it is natural to\ninvestigate how the knowledge of layer-wise task-\nspecialty can be leveraged to improve the transfer\nlearning methods; that is what the question ② in\nsection 1 is concerned with. Recall from section 2\nthat the major paradigms of transfer learning use\nall the layers of the given PLM by default. We\npropose to select a subset of the layers based on\n5752\nClassification head\n…[CLS] x1 x2 xT\nLayer 1\nLayer 3\nLayer 5\nLayer 2\nLayer 4\n(a) (2,3,5)\nClassification head\n…[CLS] x1 x2\nLayer 1\nLayer 3\nxT\nLayer 5\nLayer 2\nLayer 4 (b) (1,3,3)\nClassification head\n…[CLS] x1 x2\nLayer 1\nLayer 3\nxT\nLayer 5\nLayer 2\nLayer 4 (c) (2,3,3)\nClassification head\n…[CLS] x1 x2 xT\nLayer 1\nLayer 3\nLayer 5\nLayer 2\nLayer 4 (d) (4,5,5)\nFigure 2: We present our strategies with a toy model of L= 5 layers and ℓ∗= 3. The green layers will be tuned\n(e.g. fine-tuned or adapter-tuned) during the task-specific training while the grey layers are not. The white layers are\ndropped from the tuning and inference procedures, thus further reducing the computation and memory cost.\nthe their task-specialty which will benefit all the\nparadigms of transfer learning: they may be able to\nonly use the selected layers yet still achieve strong\nperformance. Only using the selected layers will\nresult in significant reduction of computation cost:\n• In fine-tuning, only the parameters of the selected\nlayers will be updated.\n• In adapter-tuning, adapters are only added to the\nselected layers but not all the layers.\n• In prefix-tuning, we “deep-perturb” fewer layers.\nA smaller number of task-specific parameters\nmeans not only less training cost but also less stor-\nage cost and less inference cost.\nStrategy-I: ℓ∗-down. We use ℓ∗ to denote the\nlayer which achieves the best task-specialty: i.e.,\nℓ∗def\n= argminℓ ν(ℓ). Our first strategy is motivated\nby the following intuition: if layer ℓ∗has already\nbeen well-specialized in the given task, then it may\nsuffice to just mildly tune it along with a few layers\nbelow it. Meanwhile, we may keep the classifi-\ncation head on the top layer L or move it to the\nbest-specialized layer ℓ∗: the former still utilizes\nthe higher layers in training and inference; the latter\ndoes not and thus will result in even less computa-\ntion and memory cost.\nTechnically, we use(ℓbottom,ℓtop,ℓhead) to denote\nthe strategy of selecting the layers ℓbottom,ℓbottom +\n1,...,ℓ top and connect the classification head to\nthe layer ℓhead. Then all the instances of our\nfirst strategy can be denoted as (ℓbottom,ℓ∗,L) or\n(ℓbottom,ℓ∗,ℓ∗) with appropriately chosen ℓbottom.\nFigures 2a–2c illustrate a few specific instances of\nour ℓ∗-down strategy.\nStrategy-II: ℓ∗-up. Alternative to the ℓ∗-down\nstrategy, our second strategy is to select the layers\nabove the best-specialized layerℓ∗and we call itℓ∗-\nup strategy. Intuitively, if layerℓ∗is already well-\nspecialized in the given task, then what we need is\nperhaps just a powerful classification head. That\nis, we can regard the higher layers ℓ∗+ 1,...,L\nalong with the original classification head f as a\nnew “deep” classification head and then tune it to\nbetter utilize the layer ℓ∗representations.\nIn principle, all the instances of our second\nstrategy can be denoted as (ℓ∗+ 1,ℓtop,ℓtop) or\n(ℓ∗+ 1,ℓtop,L) since we may select the layers up\nthrough ℓtop ≤Land move the classification head\nf to layer ℓtop. Figure 2d shows an instance of our\nℓ∗-up strategy.\nNote that our (ℓbottom,ℓtop,ℓhead) notation can\napply to the conventional layer-selecting strategies\nas well. For example, (1,L,L ) denotes the naive\noption of tuning all the layers of the given PLM;\n(L−2,L,L ) denotes a baseline method of only\nselecting the top three layers.\n4 Experiments\nWe evaluated the effectiveness of our task-\nspecialty metric along with our layer-selecting\nstrategies through extensive experiments on the six\nclassification tasks of the GLUE benchmark (Wang\net al., 2019). The tasks are: CoLA, MNLI, MRPC,\nQNLI, QQP, and SST-2.All of them are sequence-\nlevel classification tasks related to natural language\nunderstanding, thus being very different from how\nlanguage models are pretrained.\nWe chose the widely accepted RoBERTa\nmodel (Liu et al., 2019b) to be our PLM and used\nthe pretrained roberta-large instance (355M pa-\nrameters) downloaded from HuggingFace (Wolf\net al., 2020). Our experiments are mainly con-\nducted with this model. We also experimented with\nDeBERTa (He et al., 2020) to investigate whether\nour methods generalize across models:1 those re-\nsults are in Appendix C.2 and are similar to the\nRoBERTa results. Prior work (Mosbach et al.,\n2020a) found that fine-tuning RoBERTa on GLUE\ncould be unstable, so we ran each of our exper-\niments with five random seeds and reported the\nmeans and standard errors. Experiment details (e.g.,\n1Bowman (2022) advocate that it is important to experi-\nment with more than one pretrained models before drawing\nany general conclusions about “pretrained language models”.\n5753\n0 10 20\nOutput layer\n5\n10\n15\n0.2\n0.3\n0.4\n0.5\n(a) CoLA\n0 10 20\nOutput layer\n20\n40\n60\n0.55\n0.60\n0.65 (b) MNLI\n0 10 20\nOutput layer\n3\n4\n5\n6\n0.84\n0.86\n(c) MRPC\n0 10 20\nOutput layer\n2\n4\n6\n8\n10\n0.70\n0.75\n0.80 (d) QNLI\n0 10 20\nOutput layer\n2\n3\n4\n0.750\n0.775\n0.800\n0.825\n(e) QQP\n0 10 20\nOutput layer\n1.0\n1.5\n2.0\n2.5\n0.85\n0.90 (f) SST-2\nFigure 3: The task-specialty metric (blue) and prob-\ning performance (red) of each layer of a pretrained\nRoBERTa model. Each figure is a GLUE task.\nhyperparameters) can be found in Appendix B.\nOur code is implemented in PyTorch (Paszke\net al., 2017) and heavily relies on HuggingFace. It\nwill be released after the paper is published. Imple-\nmentation details can be found in Appendix B.2.\n4.1 Answer to Question ①: Hidden State\nVariability Ratio Measures Task-Specialty\nFor each task, we computed the task-specialty\nν(1),...,ν (24) (defined in section 3.1) for all the 24\nlayers. They are plotted as blue curves in Figure 3:\nas we can see, the middle layers ( 10 ≤ℓ ≤15)\ntend to have the lowest νon most tasks.\nThen we probed the pretrained RoBERTa: for\neach layer ℓ, we trained a classification head (see\nsection 2) that reads the hidden state h(ℓ)\nn and eval-\nuated it on the held-out development set. The prob-\ning performance is plotted as red curves in Figure 3:\nas we can see, the middle layers ( 10 ≤ℓ ≤15)\ntend to achieve the highest scores.\nHow much is the probing performance correlated\nwith the task-specialty metric ν? To answer that\nquestion, we regressed the probing performance on\nν and found that they are highly correlated. All\nthe slope coefficients are negative, meaning that a\nlow ν predicts a high score. All the R2 are high,\nmeaning that a large fraction of the probing perfor-\nmance variation can be explained by the variation\nin the task-specialty score. Remarkably,R2 = 0.97\non QQP. Detailed results (e.g., fitted lines) are in\nFigure 11 of Appendix C.3.\nThis set of experiments answers our question ①\n(section 1): yes, we can measure the task-specialty\nof each layer of a given PLM on a given task and\nour metric νdoesn’t require task-specific tuning.\nFurthermore, we fully fine-tuned a RoBERTa on\neach task and obtained the ν and probing perfor-\nmance of the fine-tuned models. The results are\npresented in Figures 12 and 13 of Appendix B.4.\nAfter full fine-tuning, strong correlation between\nthe probing performance and the task-specialty ν\nis still observed, but now higher layers are ob-\nserved to have lower νand stronger probing perfor-\nmance. That is because the parameters of higher\nlayers have received stronger training signals back-\npropagated from the classification heads, which\naim to specialize the full model on the tasks.\n4.2 Answer to Question ②: Task-Specialty\nHelps Layer Selection\nAs discussed in section 3.2, our task-specialty-\nbased layer-selecting strategies are supposed to\nhelp improve the computation efficiency of transfer\nlearning and they are compatible with all the major\nparadigms of transfer learning. We evaluated our\nstrategies by pairing them with the widely adopted\nfine-tuning and adapter-tuning methods. In this\nsection, we show and discuss our empirical results.\nLayer selection for fine-tuning. For each task, we\nexperimented with our ℓ∗-down and ℓ∗-up strate-\ngies. The best-specialized layers ℓ∗are those with\nthe lowest task-specialty ν(ℓ). They are\nCoLA MNLI MRPC QNLI QQP SST-2\nℓ∗ 18 14 14 13 14 16\nOur experiment results are shown in Figure 4.\nFor the ℓ∗-down strategy, we experimented with\n(ℓbottom,ℓ∗,ℓhead) where ℓbottom ∈{1,ℓ∗−2,ℓ∗−\n1,ℓ∗}and ℓhead ∈{ℓ∗,L}. They are plotted as blue\ndots. For the ℓ∗-up strategy, we experimented with\n(ℓ∗+ 1,L,L ) and they are shown as green dots.\nFor a comparison, we also experimented with the\nconventionally adopted baseline strategies(1,L,L )\nand (ℓbottom,L,L ) with ℓbottom ∈{L−2,L−1,L}.\nThey are red dots. The actual performance scores\nof the dots along with standard errors are in Table 6\nof Appendix C.1.\nAs shown in Figure 4, when we are constrained\nby a budget of tuning only ≤3 layers, our strate-\ngies can almost always lead to significantly higher\nperformances than the baseline strategies. More-\nover, the (ℓbottom,ℓ∗,L) strategies consistently out-\n5754\n(a) CoLA\n (b) MNLI\n(c) MRPC\n (d) QNLI\n(e) QQP\n (f) SST-2\nFigure 4: Task performance vs. the number of selected\nlayers for fine-tuning. The annotation of each dot is its\nstrategy identifier (ℓbottom,ℓtop,ℓhead).\nperform the (ℓbottom,ℓ∗,ℓ∗) strategies across all the\ntasks. It means that the higher layers ℓ∗+ 1,...,L\nare still very useful for performing well on the\ngiven task even if their parameters are not updated:\nthey are perhaps able to help shape the training\nsignals back-propagated through the tuned layers.\nFurthermore, the performance of only tuning\nthe selected layers is often close to that of full\nfine-tuning. Remarkably, on QQP, our (1,ℓ∗,ℓ∗)\nstrategy even matches the performance of full fine-\ntuning yet only uses the bottom 14 layers; that is, it\nreduces the computation and storage cost by more\nthan 40%. Detailed discussion about wall-clock\ntime saving can be found in Appendix C.1.\nBecause most ℓ∗are in the middle of the PLM,\nwe experimented another baseline of directly us-\ning the middle layer ℓmid = 13 for the 24-layer\nRoBERTa-large. This baseline is only implemented\non CoLA and SST-2 because ℓ∗of other tasks is\nalready the same as or very close toℓmid. We found\nthat tuning around layer ℓ∗always outperforms tun-\ning around ℓmid although the improvement is not\nlarge. Result details are in Table 8 of Appendix C.1.\nLayer selection for adapter-tuning. For adapter-\ntuning, we implemented the earliest adapter archi-\ntecture designed by Houlsby et al. (2019). We\nexperimented with the same set of our strategies\nand baseline strategies as in the fine-tuning experi-\nments. The results are plotted in Figure 5. Detailed\n(a) CoLA\n (b) MNLI\n(c) MRPC\n (d) QNLI\n(e) QQP\n (f) SST-2\nFigure 5: Task performance vs. the number of selected\nlayers for adapter-tuning. The annotation of each dot is\nits strategy identifier (ℓbottom,ℓtop,ℓhead).\nnumbers including standard errors are listed in Ta-\nble 7 in Appendix C.1.\nLike for fine-tuning, in most cases, our strategies\nare significantly better than the baseline strategies\nunder the budget of tuning only ≤3 layers. The\n(ℓbottom,ℓ∗,L) strategies also tend to outperform\nthe (ℓbottom,ℓ∗,ℓ∗) strategies.\nWhat’s impressive is that the performance of\nonly adapter-tuning the selected layers is often\ncomparable to that of full adapter-tuning. Surpris-\ningly, on MNLI and QNLI, our (ℓ∗−2,ℓ∗,L) and\n(ℓ∗−1,ℓ∗,L) strategies match the full adapter-\ntuning but only need 12% and 8% of the trainable\nparameters as full adapter-tuning respectively.\nWe also implemented the middle layer baseline\nfor adapter-tuning on CoLA and SST-2. The results\nare listed in Table 9 of Appendix C.1. In most\ncases, ℓ∗outperforms ℓmid with the same number\nof adapted layers.\nInterestingly, Houlsby et al. (2019) also found\nthat not every layer in the PLM is equally impor-\ntant for adapter-tuning. For example, they experi-\nmented with a 12-layer BERT model and found that\nablating the layer-7 adapters will lead to a much\nlarger performance drop than ablating those of the\ntop three layers on the CoLA task.\n4.3 Is Our Metric the Only Option?\nIn this section, we will discuss a few potential\nalternatives to our proposed task-specialty metric\n5755\nν. The introduction of the alternatives will be brief\nand only focused on their definitions and intuitions;\nmore details can be found in Appendix C.7.\nTask ρ(ν,s) ρ(CCA,s) ρ(ϱ,s)\nCoLA -0.901 0.977 -0.914\nMNLI -0.885 0.977 -0.493\nMRPC -0.874 0.934 -0.593\nQNLI -0.915 0.988 -0.514\nQQP -0.984 0.989 -0.723\nSST-2 -0.913 0.986 -0.798\nTable 1: Correlations between different metric (our ν,\nCCA, effective rank) and probing performance s\nCanonical correlation analysis. A potential al-\nternative to our proposed metric is the canoni-\ncal correlation between the hidden states h(ℓ)\nn and\nthe class labels yn. Canonical correlation anal-\nysis (CCA) involves learning a series of projec-\ntion parameters (v1,w1),..., (vJ,wJ) such that\neach (vj,wj) maximizes the correlation between\nv⊤\nj h(ℓ)\nn and w⊤\nj yn under certain constraints: yn is\na one-hot vector with its ynth entry being one.\nNumerical Rank. Another potential alternative\nis the rank-based metric proposed by Zhou et al.\n(2022). It is also inspired by the neural collapse\nphenomenon. The intuition is: if the sequence-level\nrepresentations h(ℓ)\nn of the same G(ℓ)\ny exhibit a low\nvariability, then the matrix H(ℓ)\ny = [ ... h(ℓ)\nn ... ]\nformed by the vectors in G(ℓ)\ny will have a low rank\nsince its columns will be similar. The rank of H(ℓ)\ny\ncan be estimated by ϱ(ℓ)\ny\ndef\n= ∥H(ℓ)\ny ∥2\n∗\n∥H(ℓ)\ny ∥2\nF\nwhere ∥∥∗is\nthe nuclear norm (i.e., the sum of singular values)\nand ∥∥F is the Frobenius norm. The rank-based\nmetric is the average over y: ϱ(ℓ) def\n= 1\n|Y|\n∑\ny∈Yϱ(ℓ)\ny .\nThe correlation between ϱ(ℓ) and the probing per-\nformance is presented in Table 1.\nAnalysis. As we can see in Table 1, our variability-\nbased metric νand CCA score both exhibit a high\ncorrelation (above 0.85) with the probing perfor-\nmance. The correlation of CCA is moderately bet-\nter than ours and it is higher than 0.9 on all the\ntasks. However, CCA optimization is dependent\non the regularization terms (see Appendix C.7) and\nthus requires cross-validation to choose the right set\nof hyperparameters. This makes it more involved\nto compute than the proposed variability metric.\nTechnically, CCA requires a “training procedure”\nto fit a set of parameters ( vj and wj) though the\ncomputation of that part is as light as calculating\nour metric. Both CCA and our metric are scale\ninvariant. Overall, the rank-based metric ϱ has\na lower correlation. That is because it only con-\nsiders the within-group properties but ignores the\nbetween-group properties.\n4.4 Ablation Studies and Analysis\nAveraged state vs. CLS state. The νthat we have\nreported so far are all computed using the average\nhidden state of each xn as defined in section 3.1.\nWe also experimented with the CLS token hidden\nstates—i.e., h(ℓ)\nn\ndef\n= h(ℓ)\nn,0—and found that the com-\nputation of ν became much less stable: e.g., on\nSST-2, the metrics ν(ℓ) of the pretrained RoBERTa\nare below 50 for most of the layers but can jump\nabove 500 for a couple of layers. Intuitively, one\nwould like to trust a ν(ℓ) curve that is at least lo-\ncally smooth. Technically, local smoothness means\na small local second-order derivative, which can\nbe estimated by finite differences. Therefore, we\nmeasure the local smoothness of a ν(ℓ) curve by\nthe following quantity ζ\nζ\ndef\n=\nL−2∑\nℓ=1\n(ν(ℓ+2) −2ν(ℓ+1) + ν(ℓ))2\nThe results2 are presented in Table 2. As we can\nTask ζwith averaged state ζwith CLS state\nCoLA 0.950 0.949\nMNLI 1.347 88.162\nMRPC 1.994 33.038\nQNLI 3.256 13.566\nQQP 1.640 7.629\nSST-2 3.604 58.040\nTable 2: Overall local smoothness ζ.\nsee, the ζ computed using the CLS token states\nis dramatically larger than that of using the aver-\naged states on all the tasks except CoLA. It means\nthat using the averaged states will give us a more\ntrustable ν(ℓ) curve.Thus, we used the average hid-\nden states throughout our experiments; we made\nthis choice before seeing any task performance.\nThe actual ν(ℓ) curves computed using the CLS\nstates are presented in Figure 14 of Appendix C.5.\nRobustness to data imbalance. The datasets of\nGLUE tasks are almost perfectly balanced. So\nit remains a question whether our proposed task-\nspecialty νis still effective when the data is not im-\nbalanced. To answer this question, we synthesized\na series of data-imbalanced experiments on SST-2.\n2We normalized theν(ℓ) values before computing ζso that\nthe ν(ℓ) and ζare more comparable across different choices\nof the sequence-level representation. Our normalization is:\nν(ℓ) ← ν(ℓ)−¯ν\nσ where ¯ν and σ is the mean and standard\ndeviation of the original ν(ℓ).\n5756\n0 10 20\nOutput layer\n0.5\n1.0\n1.5\n2.0\n2.5 p=0.5\np=0.25\np=0.1\np=0.05\n(a) νcurves\n0.05 0.1 0.25 0.5\np\n10\n15\n20\n25\n0.825\n0.850\n0.875\n0.900 (b) ζ(blue) and |ρ|(red)\nFigure 6: Results of data-imbalanced experiments on\nSST-2.\n0 10 20\nOutput layer\n1\n2\nN=40000\nN=20000\nN=5000\nN=200\n(a) νcurves\n200 5000 20000 40000\nN\n25\n50\n75\n100\n0.5\n0.6\n0.7\n0.8\n0.9 (b) ζ(blue) and |ρ|(red)\nFigure 7: Results of data-scarce experiments on SST-2.\nIn each experiment, we randomly sampled N =\n20000 training examples with a portion p from\nthe negative class where p∈{0.5,0.25,0.1,0.05}.\nWe did the same analysis on MNLI; see results in\nAppendix C.6.\nFor each experiment, we plot the ν(ℓ) curve in\nFigure 6a. We also computed the local smoothness\nscore ζ and the absolute value of the correlation\nρ(ν,s) (defined in section 4.3) and plot them in\nFigure 6b. Interestingly, the ζ in the case of p =\n0.25 is as low as that ofp= 0.5, though it becomes\nmuch larger when the data is extremely imbalanced\n(e.g., p< 0.1). Moreover, the ρ(ν,s) is still above\n0.85 even when p= 0.1. Those findings mean that\nour task-specialty metric is reasonably robust to\ndata imbalance. More details are in Appendix C.6.\nRobustness to data scarcity. We also examined\nthe robustness of our metric to data scarcity. On\nSST-2, we conducted a series of experiments\nwith varying number of training samples: N ∈\n40000,20000,5000,200. For each experiment, we\nmade the sampled dataset label-balanced. See the\nresults on MNLI in Appendix C.6.\nLike in the data-imbalanced experiments, we\nplot νand ζand |ρ|in Figure 7. As we can see, for\nN ≥5000, the ν curves almost perfectly overlap\nand the ζ and |ρ|only slightly change with N. It\nmeans that the ν values are trustable as long as\nthey are computed using thousands of examples.\nIn the extremely data-scarce case of N = 200, the\nνcurve becomes not trustable: after all, it will be\nextremely difficult to estimate Σw and Σb with so\nfew samples. However, even in the case of N =\n200, the middle layers tend to have the lowest ν(ℓ)\nwhich agree with the data-adequate cases. More\ndetails are in Appendix C.6.\n5 Related Work\nAnalysis of PLMs. Representations from PLMs\nhave been widely studied using probing methods.3\nThere is evidence showing that pre-trained fea-\ntures from intermediate layers are more transfer-\nable (Tenney et al., 2019; Rogers et al., 2020). Ad-\nditionally, V oita et al. (2019a) show that masked\nlanguage modelling objective introduces an auto-\nencoder like structure in the PLM, meaning the\nbehaviour of the top most layers is similar to the\nbottom layers. Studies have also shown that the\neffects of fine-tuning are non-uniformly spread\nacross different layers (Peters et al., 2019; Mer-\nchant et al., 2020; Liu et al., 2019a; Phang et al.,\n2021). Those findings challenge the default choice\nof tuning the entire PLM for adapting it to down-\nstream tasks. While probing tools have been used\nto study task-specific layer importance (Tamkin\net al., 2020; Mosbach et al., 2020b), the probing\nparadigm is parametric, hard to interpret, and is\nknown to be unreliable (Ravichander et al., 2020;\nBelinkov, 2022; Hewitt and Liang, 2019; V oita and\nTitov, 2020; Pimentel et al., 2020). Instead, we\npropose to measure the layer-wise task-specialty of\nPLMs using a non-parametric tool that quantifies\nthe task-specific variability of the hidden features.\nPLM-based transfer learning. PLMs are widely\nused in the transfer learning setup to improve per-\nformance on a variety of downstream tasks. Typ-\nically, a task-specific classification layer is added\non the top of the network and the entire network is\ntrained to minimize the supervised task loss. Re-\ncently there has been growing interest in parameter-\nefficient alternatives to this approach. A subset\nof these methods add a few new trainable param-\neters to the PLM while the pre-trained weights\nare frozen and are thus kept consistent across\ntasks (Houlsby et al., 2019; Guo et al., 2020; Li and\nLiang, 2021). Another set of methods either repa-\nrameterizes PLMs (Hu et al., 2022) or chooses only\na subset of the PLM parameters (V oita et al., 2019b;\nSajjad et al., 2020; Gordon et al., 2020; Zaken et al.,\n2021), thus reducing the number of trainable pa-\nrameters for transfer learning. In particular, the\n“early exit” methods (Xin et al., 2020, 2021; Zhou\n3In this paper, we focus on the models that are pretrained\nonly with the language modeling objective. Other pretraining\nobjectives such as those of Sun et al. (2019); Wang et al.\n(2021); Raffel et al. (2020) are out of our current scope.\n5757\net al., 2020) allow samples to pass through part of\nPLM if the prediction from a middle layer is trusted\nby the off-ramp following that layer. This method\ncan reduce inference cost but increase training cost\nbecause it adds a classification head to each hidden\nlayer. Our technique can reduce both training and\ninference cost by tuning fewer layers and moving\nclassification head to an intermediate layer.\nIn this work we leverage our proposed metric of\nlayer-wise task-specificity to make an informed de-\ncision to retain/drop and tune/freeze layers on the\ndownstream task. There has been work studying\nthe effect of dropping PLM layers (Sajjad et al.,\n2020; Phang et al., 2021; Tamkin et al., 2020)\nbut their decision is driven by the performance\non the downstream task itself and thus every new\ntask will require a slew of ablation studies to find\nthe applicability of each layer. Whereas, our task-\nspecificity measure is completely parameter-free\nand is also agnostic to the specific transfer learn-\ning approach (fine-tuning, adapter-tuning, prefix-\ntuning) and thus complements the existing methods\non parameter-efficient approaches (He et al., 2022).\nNeural collapse. Our task-specialty metric νis in-\nspired by the neural collapse phenomenon. A surg-\ning line of work has been demystifying the training,\ngeneralization, and transferability of deep networks\nthrough NC (Kothapalli et al., 2022). For train-\ning, recent works showed that NC happens for a\nvariety of loss functions such as cross-entropy (Pa-\npyan et al., 2020; Zhu et al., 2021; Fang et al.,\n2021; Ji et al., 2022), mean-squared error (Mixon\net al., 2020; Han et al., 2022; Zhou et al., 2022;\nTirer and Bruna, 2022), and supervised contrastive\nloss (Graf et al., 2021). For generalization, Galanti\net al. (2022); Galanti (2022) show that NC also hap-\npens on test data drawn from the same distribution\nasymptotically, but not for finite samples (Hui et al.,\n2022); Hui et al. (2022); Papyan (2020) showed\nthat the variability collapse is happening progres-\nsively from shallow to deep layers; Ben-Shaul and\nDekel (2022) showed that test performance can be\nimproved when enforcing variability collapse on\nfeatures of intermediate layers; Xie et al. (2022);\nYang et al. (2022) showed that fixing the classi-\nfier as simplex ETFs improves test performance on\nimbalanced training data and long-tailed classifica-\ntion problems. For transferability, Kornblith et al.\n(2021) showed that there is an inherent tradeoff\nbetween variability collapse and transfer accuracy.\n6 Conclusion\nIn this paper, we present a comprehensive study\non how to measure the task-specialty of each layer\nof a pretrained language model as well as how to\nleverage that knowledge to improve transfer learn-\ning. Our proposed layer-wise task-specialty metric\nis based on the variability of the hidden states of\neach layer given a task-specific corpus. Our metric\nis highly correlated with the layer-wise probing per-\nformance, though it is cheap to compute and does\nnot require any training or hyperparameter tuning.\nWe propose a couple of strategies based on the met-\nric for selecting a subset of the layers to use in the\nPLM-based transfer learning methods. Extensive\nexperiments demonstrate that our strategies can\nhelp fine-tuning and adapter-tuning achieve strong\nperformance under a greatly reduced computation\nbudget. Our strategies are complementary to all the\nmajor paradigms of PLM-based transfer learning\nand thus they will also benefit other methods.\nAcknowledgements\nThis work was supported by a research gift to\nthe last author by Adobe Research. We thank the\nanonymous EMNLP reviewers and meta-reviewer\nfor their constructive feedback. We also thank\nour colleagues at Toyota Technological Institute\nat Chicago as well as Dongji Gao (JHU), Yiyuan\nLi (UNC), Hao Tan (Adobe Research), and Zhihui\nZhu (OSU) for helpful discussion.\nLimitations\nOur main technical limitation is that the pro-\nposed metric only measures the task specificity\nfrom the perspective of variability. Thus, it might\nunderestimate the task specificity if the features has\nother kinds of good spacial structures with large\nwithin-class variability. For example, concentric\nrings are separable but not linearly separable; see\nFig-3 in Hofmann (2006). Although we have seen\nthat our proposed νis a good predictor for the final\nperformance in all our experiments (section 4), it is\nstill possible that, for some tasks and some models,\nthe layers with high ν can actually achieve good\nperformance. Fortunately, such clustering structure\nis rare in the hidden space of deep neural networks.\nAnother technical limitation is that our proposed\nhidden state variability ratio only works for classi-\nfication tasks. An open research question is how to\ngeneralize it to regression or generation tasks.\n5758\nEthics Statement\nIn this work, we introduce a simple yet effective\napproach for substantially reducing the computa-\ntion for transferring PLMs to downstream tasks.\nOur proposed strategies obviate the need for tuning\nthe entire model, which can significantly reduce\nthe cost of computation and memory. Therefore,\nthey can help reduce greenhouse gas emissions and\ncombat climate change.\nHowever, our technical approaches involve pre-\ntrained language models for which a range of ethi-\ncal concerns exist including privacy leakage, data\nbias, and vulnerablility to adversarial attacks.\nReferences\nTheodore W Anderson. 1973. Asymptotically efficient\nestimation of covariance matrices with linear struc-\nture. The Annals of Statistics, 1(1):135–141.\nYonatan Belinkov. 2022. Probing classifiers: Promises,\nshortcomings, and advances. Computational Linguis-\ntics.\nIdo Ben-Shaul and Shai Dekel. 2022. Nearest class-\ncenter simplification through intermediate layers.\narXiv preprint arXiv:2201.08924.\nSamuel Bowman. 2022. The dangers of underclaiming:\nReasons for caution when reporting how nlp systems\nfail. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 7484–7499.\nTijl De Bie and Bart De Moor. 2003. On the regulariza-\ntion of canonical correlation analysis. International\nSymposium on Independent Component Analysis and\nBlind Signal Separation.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nCong Fang, Hangfeng He, Qi Long, and Weijie J Su.\n2021. Exploring deep neural networks via layer-\npeeled model: Minority collapse in imbalanced train-\ning. Proceedings of the National Academy of Sci-\nences (PNAS).\nTomer Galanti. 2022. A note on the implicit bias to-\nwards minimal depth of deep neural networks. arXiv\npreprint arXiv:2202.09028.\nTomer Galanti, András György, and Marcus Hutter.\n2022. On the role of neural collapse in transfer learn-\ning. In Proceedings of the International Conference\non Learning Representations (ICLR).\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. arXiv preprint\narXiv:2002.08307.\nFlorian Graf, Christoph Hofer, Marc Niethammer, and\nRoland Kwitt. 2021. Dissecting supervised con-\nstrastive learning. In Proceedings of the International\nConference on Machine Learning (ICML).\nDemi Guo, Alexander M Rush, and Yoon Kim. 2020.\nParameter-efficient transfer learning with diff prun-\ning. arXiv preprint arXiv:2012.07463.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level Adversarial\nReProgramming. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nX.Y . Han, Vardan Papyan, and David L. Donoho. 2022.\nNeural collapse under MSE loss: Proximity to and\ndynamics on the central path. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nTrevor Hastie, Robert Tibshirani, Jerome H Friedman,\nand Jerome H Friedman. 2009. The elements of statis-\ntical learning: data mining, inference, and prediction.\nSpringer.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nunified view of parameter-efficient transfer learning.\nIn Proceedings of the International Conference on\nLearning Representations (ICLR).\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. DeBERTa: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. arXiv preprint\narXiv:1909.03368.\nMartin Hofmann. 2006. Support vector machines-\nkernels and the kernel trick. Notes, 26(3):1–16.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In Pro-\nceedings of the International Conference on Machine\nLearning (ICML).\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In Proceedings of the Inter-\nnational Conference on Learning Representations\n(ICLR).\nLike Hui, Mikhail Belkin, and Preetum Nakkiran. 2022.\nLimitations of neural collapse for understanding\ngeneralization in deep learning. arXiv preprint\narXiv:2202.08384.\nWenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and\nWeijie J Su. 2022. An unconstrained layer-peeled\nperspective on neural collapse. In Proceedings of the\n5759\nInternational Conference on Learning Representa-\ntions (ICLR).\nSimon Kornblith, Ting Chen, Honglak Lee, and Moham-\nmad Norouzi. 2021. Why do better loss functions\nlead to less transferable features? In Advances in\nNeural Information Processing Systems (NeurIPS).\nVignesh Kothapalli, Ebrahim Rasromani, and Vasudev\nAwatramani. 2022. Neural collapse: A review\non modelling principles and generalization. arXiv\npreprint arXiv:2206.04041.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. arXiv preprint arXiv:1903.08855.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to bert\nembeddings during fine-tuning? arXiv preprint\narXiv:2004.14448.\nDustin G Mixon, Hans Parshall, and Jianzong Pi. 2020.\nNeural collapse with unconstrained features. arXiv\npreprint arXiv:2011.11619.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020a. On the stability of fine-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. arXiv preprint arXiv:2006.04884.\nMarius Mosbach, Anna Khokhlova, Michael A Hed-\nderich, and Dietrich Klakow. 2020b. On the inter-\nplay between fine-tuning and sentence-level probing\nfor linguistic knowledge in pre-trained transformers.\narXiv preprint arXiv:2010.02616.\nVardan Papyan. 2020. Traces of class/cross-class struc-\nture pervade deep learning spectra. Journal of Ma-\nchine Learning Research (JMLR).\nVardan Papyan, XY Han, and David L Donoho. 2020.\nPrevalence of neural collapse during the terminal\nphase of deep learning training. Proceedings of the\nNational Academy of Sciences (PNAS).\nAnkita Pasad, Ju-Chieh Chou, and Karen Livescu. 2021.\nLayer-wise analysis of a self-supervised speech rep-\nresentation model. In IEEE Workshop on Automatic\nSpeech Recognition and Understanding (ASRU).\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer.\n2017. Automatic differentiation in PyTorch.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers).\nMatthew E Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. arXiv preprint\narXiv:1903.05987.\nJason Phang, Haokun Liu, and Samuel R Bowman.\n2021. Fine-tuned transformers show clusters of sim-\nilar representations across layers. arXiv preprint\narXiv:2109.08406.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. arXiv preprint arXiv:2004.03061.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAACL).\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, Peter J Liu, et al. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR).\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. SVCCA: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nAbhilasha Ravichander, Yonatan Belinkov, and Ed-\nuard Hovy. 2020. Probing the probing paradigm:\nDoes probing accuracy entail task relevance? arXiv\npreprint arXiv:2005.00719.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav\nNakov. 2020. On the effect of dropping layers\nof pre-trained transformer models. arXiv preprint\narXiv:2004.03844.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nAlex Tamkin, Trisha Singh, Davide Giovanardi, and\nNoah Goodman. 2020. Investigating transferabil-\nity in pretrained language models. arXiv preprint\narXiv:2004.14975.\n5760\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert\nrediscovers the classical nlp pipeline. arXiv preprint\narXiv:1905.05950.\nTom Tirer and Joan Bruna. 2022. Extended uncon-\nstrained features model for exploring deep neural\ncollapse. arXiv preprint arXiv:2202.08087.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\nConference of the North American Chapter of the\nAssociation for Computational Linguistics (NAACL).\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019b. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. arXiv\npreprint arXiv:2003.12298.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2019. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nWeiran Wang, Raman Arora, Karen Livescu, and Jeff\nBilmes. 2015. On deep multi-view representation\nlearning. In Proceedings of the International Confer-\nence on Machine Learning (ICML).\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics.\nAdina Williams, Ryan Cotterell, Lawrence Wolf-\nSonkin, Damián Blasi, and Hanna Wallach. 2019.\nQuantifying the semantic core of gender systems. In\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. HuggingFace’s transformers: State-of-\nthe-art natural language processing. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nLiang Xie, Yibo Yang, Deng Cai, Dacheng Tao, and\nXiaofei He. 2022. Neural collapse inspired attraction-\nrepulsion-balanced loss for imbalanced learning.\narXiv preprint arXiv:2204.08735.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exit-\ning for accelerating bert inference. arXiv preprint\narXiv:2004.12993.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. Berxit: Early exiting for bert with better fine-\ntuning and extension to regression. In Proceedings\nof the 16th conference of the European chapter of\nthe association for computational linguistics: Main\nVolume.\nYibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li,\nZhouchen Lin, and Dacheng Tao. 2022. Do we really\nneed a learnable classifier at the end of deep neural\nnetwork? arXiv preprint arXiv:2203.09081.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitfit: Simple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nJinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing\nQu, and Zhihui Zhu. 2022. On the optimization land-\nscape of neural collapse under mse loss: Global opti-\nmality with unconstrained features. In Proceedings\nof the International Conference on Machine Learning\n(ICML).\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit.\nAdvances in Neural Information Processing Systems\n(NeurIPS).\nZhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong\nYou, Jeremias Sulam, and Qing Qu. 2021. A geo-\nmetric analysis of neural collapse with unconstrained\nfeatures. In Advances in Neural Information Process-\ning Systems (NeurIPS).\n5761\nA Metric Details\nA.1 Compared to the Neural Collapse Metric\nOur task-specialty metric νdefined in section 3.1\nis a lot like the neural collapse metric proposed\nby Papyan et al. (2020) except that they assume\na balanced dataset. First, they define the ¯h(ℓ) to\nbe global mean vector: i.e., ¯h(ℓ) def\n= 1\nN\n∑N\nn=1 h(ℓ)\nn ,\nbut we define it to be the mean of the within-group\nmean vectors. In data-balanced cases, those two\ndefinitions are equivalent. But when data is imbal-\nanced, our version is better since it prevents the\n¯h(ℓ) from being dominated by the group that has\nthe largest number of samples.\nIf we strictly follow Papyan et al. (2020), then\nour within-class variability Σ(ℓ)\nw and between-class\nvariability Σ(ℓ)\nb will be defined to be\nΣ(ℓ)\nw\ndef\n= 1\nN\n∑\ny∈Y\n∑\nh∈G(ℓ)\ny\n(h −¯h(ℓ)\ny )(h −¯h(ℓ)\ny )⊤\nΣ(ℓ)\nb\ndef\n= 1\n|Y|\n∑\ny∈Y\n(¯h(ℓ)\ny −¯h(ℓ))(¯h(ℓ)\ny −¯h(ℓ))⊤\nThen the within-class variability Σ(ℓ)\nw will also be\ndominated by the group that has the largest number\nof samples. However, our current definition in\nsection 3.1 will scale the ∑\nh(h −¯hy)(h −¯hy))⊤\nterm by |G(ℓ)\ny |before taking the outer sum∑\ny, thus\nbeing more robust to data imbalance.\nTo verify our intuition, we conducted a series\nof data-imbalanced experiments on SST-2 like we\ndid in section 4.4. In each experiment, we ran-\ndomly sampled N = 20000 training examples\nwith a portion p from the negative class where\np ∈{0.5,0.1,0.05}. Then we constructed the ν\ncurves and plot them in Figure 8: solid curves use\nour math formulas in section 3.1 while dashed lines\nuse the formulas that strictly follow Papyan et al.\n(2020). As we can see, when data is balanced, the\nsolid and dashed lines are exactly the same. When\ndata is imbalanced, the solid lines still stay close\nwhile the dashed lines move apart. This figure il-\nlustrates that our formulas are more robust to data\nimbalance.\nB Experiment Details\nFor the pretrained language model, we use the\nimplementation and pretrained weights (roberta-\nlarge with 24 layers and 355M parameters) of Hug-\ngingface (Wolf et al., 2020).We follow previous\nwork (Wang et al., 2019) and use different evalua-\ntion methods for different tasks:\n0 10 20\nOutput layer\n0.5\n1.0\n1.5\n2.0\n2.5\np=0.5\np=0.1\np=0.05\nFigure 8: The ν(ℓ) metric computed using our formulas\n(solid lines) and the formulas of Papyan et al. (2020)\n(dashed lines) in data-imbalanced experiments.\n• On CoLA, we use Matthews correlation coeffi-\ncient.\n• On MRPC and QQP, we use F1 score.\n• On the others, we use classification accuracy.\nB.1 Training Details\nWe only tune learning rate for each task and\nstrategy and specific tuning methods. The number\nof epochs is 10 for full-fine-tuning on MNLI and\nQQP and 20 for all the other experiments. All\nthe other hyperparameters are the same for all the\nexperiments. We use the AdamW optimizer with\na linear learning rate scheduler with 6% warm-up\nsteps. We set the dropout rate to be 0.1. The weight\ndecay is 0. The batch size is 8. We evaluate on\nthe validation set per epoch and report the best\nresult. We run the experiments on Nvidia RTX\nA4000 and GeForce RTX 2080 Ti. We use the\nstandard splits and the datasets can be downloaded\nat https://huggingface.co/datasets/glue.\nB.2 Implementation Details\nOur code is implemented in PyTorch (Paszke\net al., 2017) and heavily relies on HuggingFace. It\nwill be released after the paper is published.\nFor all the experiments that requires a new\ntask-specific classification head, we relied on the\noriginal implementation in RoBERTa of Hugging-\nface (Wolf et al., 2020)4.\nFor adapter-tuning, we implemented the earli-\nest adapter architecture designed by Houlsby et al.\n(2019) and relied on the public implementation 5\n4https://github.com/huggingface/transformers/\nblob/main/src/transformers/models/roberta/\nmodeling_roberta.py\n5https://github.com/jxhe/\nunify-parameter-efficient-tuning\n5762\nprovided by He et al. (2022). The bottleneck dimen-\nsion is 256 and the adapter uses the same initializa-\ntion as BERT (Devlin et al., 2018). The trainable\nparameters to update are the parameters of the in-\nserted adapters, the layer normalization parameters\nin the selected layers, and the parameters of the\nclassification head.\nB.3 Probing Experiments\nIn section 4, we probed both the pretrained and\nfine-tuned models. On each GLUE task, we used a\nfixed learning rate in Table 3 to train a classification\nhead for each layer.\nTask Learning rate\nCoLA 1e-3\nMNLI 1e-3\nMRPC 1e-3\nQNLI 1e-3\nQQP 3e-4\nSST-2 3e-3\nTable 3: Learning rate for probing experiments.\nB.4 Fine-Tuning Experiments\nFor the fine-tuning experiments in section 4.2,\nwe only tuned the learning rate. Ideally, we should\nhave swept a large range of learning rates for all the\nstrategies and found the best learning rate for each\nstrategy; but that would require too much computa-\ntion cost that we couldn’t afford. Our preliminary\nexperiments showed that small learning rates tend\nto work better when the number of trainable pa-\nrameters is large and that large learning rates tend\nto work better when the number of trainable pa-\nrameters is small. Therefore, we set a different\nrange of learning rates for each different strategy\nbased on their numbers of trainable parameters.\nThe ranges that we used are in Table 4. For each\nstrategy, on each task, we chose the best learning\nrate based on the performance on the held-out val-\nidation set. The (ℓbottom,ℓ∗,ℓ∗) and (ℓbottom,ℓ∗,L)\nstrategies use the same learning rate as the conven-\ntional (ℓbottom,L,L ) strategy.\nStrategy Learning rate set\nfull fine-tuning 1e-6, 5e-6, 8e-6, 1e-5, 2e-5\n(ℓbottom,L,L ) 8e-6, 1e-5, 2e-5, 3e-5, 5e-5\n(1,ℓ∗,ℓ∗) 5e-6, 1e-5, 5e-5, 1e-4\n(ℓ∗+ 1,L,L ) 1e-6, 3e-6, 1e-5, 3e-5\nTable 4: Learning rate for fine-tuning experiments.\nB.5 Adapter-Tuning Experiments\nFor the adapter-tuning experiments in sec-\ntion 4.2, we only tuned the learning rate. For the\nsame reason as we discussed in Appendix B.4, we\nset a different range of learning rates for each dif-\nferent strategy based on their numbers of trainable\nparameters. The ranges that we used are in Ta-\nble 5. Again, the (ℓbottom,ℓ∗,ℓ∗) and (ℓbottom,ℓ∗,L)\nstrategies use the same learning rate as the conven-\ntional (ℓbottom,L,L ) strategy.\nStrategy Learning rate set\nfull adapter 1e-5, 3e-5, 1e-4\n(ℓbottom,L,L ) 3e-5, 1e-4, 3e-4\n(1,ℓ∗,ℓ∗) 1e-5, 3e-5, 1e-4\n(ℓ∗+ 1,L,L ) 1e-5, 3e-5, 1e-4\nTable 5: Learning rate for adapter-tuning experiments.\nC More Results\nC.1 Detailed numbers of RoBERTa\nexperiments\nAs mentioned in section 4.1 and section 4.2, the\nmean values and standard errors of finetuning and\nadapter-tuning RoBERTa with different strategies\nare listed in Table 6 and Table 7. The standard error\nof our strategies’ performance is not significantly\nhigher or lower than the baselines. So our strategies\ncan’t help solve the stability issue in fine-tuning\nPLMs.\nAs discussed in section 4.1 and section 4.2, we\nalso fine-tuned and adapter-tuned PLM with the\nmiddle layer baseline on CoLA and SST-2 and\nlisted the results in Table 8 and Table 9.\nAs discussed in section 4.1, we compare the\ncomputation cost and storage cost of some strate-\ngies on MNLI in Table 10. When only keeping\nℓ∗ = 14 layers in the PLM, it reduces inference\ncost and number of parameters by 40%. In gen-\neral, using our method will reduce the computation\nthough the actual saving depends on the implemen-\ntation and the devices; see Table 10 for details of\nour experiments. For example, when using the ℓ∗-\nup strategies, the most optimized implementation\nwould cache the output of the bottom layers and\nreuse them, which will further reduce the training\nand inference cost. But we haven’t implemented it\nyet. So there is still plenty of room to improve the\nefficiency over Table 10 guided by our experimen-\ntal insights.\n5763\nStrategy ∥# of tuned layers CoLA MNLI MRPC QNLI QQP SST-2\nmean std mean std mean std mean std mean std mean std\n(ℓ∗, ℓ∗, ℓ∗)∥1 0.565 0.0198 0.845 0.0024 0.909 0.0054 0.896 0.0031 0.869 0.0007 0.952 0.0021\n(ℓ∗, ℓ∗, L)∥1 0.619 0.0173 0.891 0.0011 0.925 0.0072 0.934 0.0031 0.885 0.0013 0.960 0.0031\n(L, L, L)∥1 0.598 0.0201 0.820 0.0021 0.858 0.0086 0.866 0.0041 0.859 0.0008 0.938 0.0032\n(ℓ∗ −1, ℓ∗, ℓ∗)∥2 0.594 0.0176 0.861 0.0018 0.920 0.0091 0.917 0.0019 0.885 0.0011 0.954 0.0057\n(ℓ∗ −1, ℓ∗, L)∥2 0.612 0.0123 0.892 0.0021 0.928 0.0040 0.926 0.0009 0.889 0.0018 0.957 0.0031\n(L −1, L, L)∥2 0.616 0.0070 0.841 0.0007 0.881 0.0052 0.902 0.0014 0.875 0.0012 0.945 0.0034\n(ℓ∗ −2, ℓ∗, ℓ∗)∥3 0.615 0.0166 0.870 0.0018 0.927 0.0074 0.927 0.0020 0.887 0.0006 0.953 0.0027\n(ℓ∗ −2, ℓ∗, L)∥3 0.611 0.0138 0.892 0.0019 0.930 0.0035 0.928 0.0014 0.888 0.0017 0.954 0.0017\n(L −2, L, L)∥3 0.625 0.0068 0.847 0.0042 0.895 0.0049 0.908 0.0014 0.878 0.0019 0.944 0.0025\n(ℓ∗ + 1, L, L)∥L −ℓ∗ 0.623 0.0080 0.897 0.0015 0.931 0.0024 0.942 0.0014 0.890 0.0018 0.959 0.0022\n(1, ℓ∗, ℓ∗)∥ℓ∗ 0.674 0.0096 0.886 0.0018 0.929 0.0033 0.933 0.0009 0.895 0.0018 0.957 0.0017\n(1, L, L)∥L 0.699 0.0131 0.906 0.0007 0.934 0.0017 0.948 0.0014 0.896 0.0017 0.965 0.0012\nTable 6: Results of finetuning RoBERTa with different stratigies\nStrategy ∥# of tuned layers CoLA MNLI MRPC QNLI QQP SST-2\nmean std mean std mean std mean std mean std mean std\n(ℓ∗, ℓ∗, ℓ∗)∥1 0.420 0.0086 0.652 0.0016 0.848 0.0016 0.788 0.0013 0.758 0.0022 0.914 0.0015\n(ℓ∗, ℓ∗, L)∥1 0.600 0.0105 0.890 0.0012 0.911 0.0059 0.937 0.0020 0.881 0.0008 0.959 0.0012\n(L, L, L)∥1 0.415 0.0069 0.570 0.0054 0.839 0.0013 0.700 0.0048 0.787 0.0045 0.913 0.0031\n(ℓ∗ −1, ℓ∗, ℓ∗)∥2 0.578 0.0047 0.850 0.0015 0.909 0.0024 0.902 0.0016 0.874 0.0011 0.956 0.0019\n(ℓ∗ −1, ℓ∗, L)∥2 0.597 0.0119 0.895 0.0013 0.922 0.0054 0.938 0.0006 0.885 0.0014 0.954 0.0026\n(L −1, L, L)∥2 0.583 0.0194 0.816 0.0029 0.860 0.0035 0.857 0.0022 0.859 0.0025 0.938 0.0047\n(ℓ∗ −2, ℓ∗, ℓ∗)∥3 0.576 0.0168 0.866 0.0007 0.918 0.0058 0.920 0.0004 0.881 0.0009 0.956 0.0029\n(ℓ∗ −2, ℓ∗, L)∥3 0.588 0.0108 0.898 0.0006 0.922 0.0068 0.934 0.0038 0.887 0.0018 0.954 0.0025\n(L −2, L, L)∥3 0.620 0.0229 0.839 0.0019 0.867 0.0069 0.886 0.0020 0.867 0.0030 0.945 0.0037\n(ℓ∗ + 1, L, L)∥L −ℓ∗ 0.619 0.0198 0.894 0.0017 0.931 0.0081 0.942 0.0011 0.886 0.0032 0.962 0.0020\n(1, ℓ∗, ℓ∗)∥ℓ∗ 0.671 0.0093 0.887 0.0019 0.932 0.0080 0.934 0.0011 0.892 0.0023 0.960 0.0015\n(1, L, L)∥L 0.653 0.0510 0.908 0.0017 0.930 0.0015 0.948 0.0021 0.897 0.0010 0.964 0.0006\nTable 7: Results of adapter-tuning RoBERTa with different strategies\nStrategy ∥# of tuned layers CoLA SST-2\nmean std mean std\n(ℓ∗, ℓ∗, ℓ∗)∥1 0.565 0.0198 0.952 0.0021\n(ℓ∗, ℓ∗, L)∥1 0.619 0.0173 0.960 0.0031\n(ℓmid, ℓmid, ℓmid)∥1 0.578 0.0051 0.936 0.0017\n(ℓmid, ℓmid, L∥1 0.607 0.0087 0.957 0.0043\n(ℓ∗ −1, ℓ∗, ℓ∗)∥2 0.594 0.0176 0.954 0.0057\n(ℓ∗ −1, ℓ∗, L)∥2 0.612 0.0123 0.957 0.0031\n(ℓmid −1, ℓmid, ℓmid)∥2 0.599 0.0075 0.948 0.0024\n(ℓmid −1, ℓmid, L)∥2 0.611 0.0193 0.953 0.0014\n(ℓ∗ −2, ℓ∗, ℓ∗)∥3 0.615 0.0166 0.953 0.0027\n(ℓ∗ −2, ℓ∗, L)∥3 0.611 0.0138 0.954 0.0017\n(ℓmid −2, ℓmid, ℓmid)∥3 0.612 0.0122 0.949 0.0010\n(ℓmid −2, ℓmid, L)∥3 0.611 0.0170 0.952 0.0021\nTable 8: Comparison of fine-tuning with middle layer\nbaseline on CoLA and SST-2\nStrategy ∥# of adapted layers CoLA SST-2\nmean std mean std\n(ℓ∗, ℓ∗, ℓ∗)∥1 0.420 0.0086 0.914 0.0015\n(ℓ∗, ℓ∗, L)∥1 0.600 0.0105 0.959 0.0012\n(ℓmid, ℓmid, ℓmid)∥1 0.383 0.0170 0.884 0.0035\n(ℓmid, ℓmid, L∥1 0.594 0.0028 0.956 0.0022\n(ℓ∗ −1, ℓ∗, ℓ∗)∥2 0.578 0.0047 0.956 0.0019\n(ℓ∗ −1, ℓ∗, L)∥2 0.597 0.0119 0.954 0.0026\n(ℓmid −1, ℓmid, ℓmid)∥2 0.586 0.0095 0.946 0.0026\n(ℓmid −1, ℓmid, L)∥2 0.607 0.0108 0.955 0.0040\n(ℓ∗ −2, ℓ∗, ℓ∗)∥3 0.576 0.0168 0.956 0.0029\n(ℓ∗ −2, ℓ∗, L)∥3 0.588 0.0108 0.954 0.0025\n(ℓmid −2, ℓmid, ℓmid)∥3 0.601 0.0061 0.950 0.0045\n(ℓmid −2, ℓmid, L)∥3 0.621 0.0085 0.957 0.0019\nTable 9: Comparison of adapter-tuning with middle\nlayer baseline on CoLA and SST-2\nC.2 Experiments on DeBERTa\nAs discussed in section 4, we also conducted\nexperiements on DeBERTa-base.\nWe computed the task-specialty metric νfor all\nthe 12 layers and plotted with each layer’s prob-\ning performance in Figure 9 as we did in Figure 3.\nFor all the tasks except SST-2, we can observe\nthe same pattern as in RoBERTa: the layers with\nlow νtend to have high probing performance. On\nSST-2, the task-specialty metric isn’t negatively\ncorrelated with the probing performance. This\nmight be an example mentioned in section 6 that\nthe layers with high ν can also achieve good per-\nformance. Because the best layer ℓ∗selected by\nthe metric is already the last layer for SST-2, we\nimplemented our strategies (ℓ∗,ℓ∗,ℓ∗), (ℓ∗,ℓ∗,L)\non all the tasks except SST-2. We compared them\nwith baseline (L,L,L ) and full fine-tuning to see\nwhether the metric can help make fine-tuning more\nefficiently. The results are plotted in Figure 10\nand listed in Table 11. When only fine-tuning 1\nlayer, our strategy (ℓ∗,ℓ∗,L) always achieves the\nbest performance and the baseline (L,L,L ) is al-\nways the worst. On MRPC, QNLI and QQP, the\nperformance of (ℓ∗,ℓ∗,L) is even close to the per-\nformance of full fine-tuning with fewer than 10%\ntuning parameters.\n5764\nStrategy Training time Inference time Total params Trainable params\nfull fine-tuning 2h30min 50s 355362819 355362819\n(1,ℓ∗,ℓ∗) 1h40min 31s 229400579 177399811\n(ℓ∗+ 1,L,L ) 1h30min 50s 355362819 127014915\nTable 10: Computation cost per epoch for RoBERTa-large fine-tuning experiments on MNLI.\nStrategy ∥# tuned layers CoLA ( ℓ∗ = 8) MNLI ( ℓ∗ = 10) MRPC ( ℓ∗ = 9) QNLI ( ℓ∗ = 7) QQP ( ℓ∗ = 7)\nmean std mean std mean std mean std mean std\n(ℓ∗, ℓ∗, ℓ∗)∥1 0.502 0.0137 0.854 0.0008 0.908 0.0083 0.905 0.0023 0.862 0.0004\n(ℓ∗, ℓ∗, L)∥1 0.571 0.0111 0.866 0.0027 0.922 0.0065 0.928 0.0011 0.882 0.0014\n(L, L, L)∥1 0.484 0.0122 0.849 0.0008 0.900 0.0057 0.899 0.0019 0.862 0.0015\n(1, L, L)∥L 0.640 0.0096 0.885 0.0013 0.929 0.0038 0.935 0.0023 0.891 0.0007\nTable 11: Results of fine-tuning DeBERTa with different strategies\n2 4 6 8 10 12\nOutput layer\n5\n10\n15\n0.2\n0.3\n0.4\n(a) CoLA\n2 4 6 8 10 12\nOutput layer\n10\n15\n20\n25\n30\n0.60\n0.65 (b) MNLI\n2 4 6 8 10 12\nOutput layer\n2\n4\n6\n0.83\n0.84\n0.85\n0.86\n(c) MRPC\n2 4 6 8 10 12\nOutput layer\n2\n4\n6\n8\n0.725\n0.750\n0.775\n0.800\n0.825 (d) QNLI\n2 4 6 8 10 12\nOutput layer\n2\n3\n4\n0.78\n0.80\n0.82\n0.84\n(e) QQP\n2 4 6 8 10 12\nOutput layer\n2.5\n3.0\n3.5\n4.0\n0.84\n0.86\n0.88\n0.90 (f) SST-2\nFigure 9: The task-specialty metric (blue) and prob-\ning performance (red) of each layer of a pretrained De-\nBERTa model. Each figure is a GLUE task.\nWe also compared with middle layer baseline\non MNLI. The results are listed in Table 12. ℓmid\nworks better than ℓ∗this time.\nStrategy ∥# of tuned layers MNLI\nmean std\n(ℓ∗, ℓ∗, ℓ∗)∥1 0.854 0.0008\n(ℓ∗, ℓ∗, L)∥1 0.866 0.0027\n(ℓmid, ℓmid, ℓmid)∥1 0.849 0.0009\n(ℓmid, ℓmid, L∥1 0.872 0.0022\nTable 12: Comparison of fine-tuning DeBERTa-base\nwith middle layer baseline on MNLI\n(a) CoLA\n (b) MNLI\n(c) MRPC\n (d) QNLI\n(e) QQP\nFigure 10: Task performance vs. the number of selected\nlayers for fine-tuning DeBERTa-base. The annotation\nof each dot is its strategy identifier (ℓbottom,ℓtop,ℓhead).\nC.3 Task-Specialty vs. Probing Performance\nfor Pretrained Models\nAs discussed in section 4.1, we regressed the\nprobing performance on ν. The regression results\nare in Figure 11.\nC.4 Task-Specialty vs. Probing Performance\nAfter Full Fine-Tuning\nAs discussed in section 4.1, we fully fine-tuned\na RoBERTa on each task and obtained the ν and\nprobing performance of the fine-tuned models. The\nresults are presented in Figures 12 and 13.\nC.5 About Computing Task-Specialty Using\nthe CLS Token States\nAs discussed in section 4.4, we computed ν(ℓ)\nusing the CLS token hidden states and found that\n5765\n5 10 15\n0.2\n0.4\ny = -2.57e-02 x + 0.538\nR2 = 0.811\n(a) CoLA\n20 40 60\n0.50\n0.55\n0.60\n0.65\ny = -3.68e-03 x + 0.694\nR2 = 0.784 (b) MNLI\n3 4 5 6\n0.84\n0.86\ny = -1.06e-02 x + 0.896\nR2 = 0.764\n(c) MRPC\n2 4 6 8 10\n0.65\n0.70\n0.75\n0.80\ny = -2.05e-02 x + 0.872\nR2 = 0.838 (d) QNLI\n2 3 4\n0.750\n0.775\n0.800\n0.825 y = -3.00e-02 x + 0.871\nR2 = 0.968\n(e) QQP\n1.0 1.5 2.0 2.5\n0.80\n0.85\n0.90\ny = -5.98e-02 x + 0.963\nR2 = 0.834 (f) SST-2\nFigure 11: Regressing the per-layer probing perfor-\nmance onto the per-layer task-specialty metric.\nthe ν curves would become less trustable. The\ncurves are in Figure 14.\nC.6 Robustness to Data Imbalance and Data\nScarcity\nAs discussed in section 4.4, we conducted a se-\nries of experiments with SST-2 and MNLI to verify\nhow sensitive our metric is to data imbalance and\ndata scarcity. For each experiment on SST-2, we\nhad to decide on two key quantities: the number\nof training examples N and the portion pthat are\ndrawn from the negative group. In other words,\nwe built a dataset {(xn,yn)}N\nn=1 by sampling pN\nexamples from the negative group and (1 −p)N\nexamples from the positive group. For the data-\nimbalanced experiments, we fixed N = 20000\nand used p ∈{0.5,0.25,0.1,0.05}. For the data-\nscarce experiments, we fixed p = 0 .5 and used\nN ∈{40000,20000,5000,200}.\nFor MNLI, we need to decide the sam-\nple size of each class because it is a clas-\nsification task with three classes. We use\n(n0,n1,n2) to denote the sample size of\nthe three classes. For the data-imbalanced\nexperiments, (n0,n1,n2) is chosen from\n{(10000,10000,10000),(6000,12000,12000),\n(18000,6000,6000),(24000,3000,3000)}. The\ntotal number is always 30000 to avoid the effect\nof data amount. Similarly as in section 4.4, we\nplotted νand ζand |ρ|in Figure 15. Our metric is\n10 20\nOutput layer\n0\n5\n10\n15\n0.2\n0.4\n0.6\n(a) CoLA\n10 20\nOutput layer\n0\n20\n40\n60\n0.6\n0.7\n0.8\n0.9 (b) MNLI\n10 20\nOutput layer\n0\n2\n4\n6\n0.75\n0.80\n0.85\n0.90\n(c) MRPC\n10 20\nOutput layer\n0.0\n2.5\n5.0\n7.5\n10.0\n0.7\n0.8\n0.9 (d) QNLI\n10 20\nOutput layer\n0\n1\n2\n3\n4\n0.825\n0.850\n0.875\n0.900\n0.925\n(e) QQP\n10 20\nOutput layer\n0.0\n0.5\n1.0\n1.5\n2.0\n0.80\n0.85\n0.90\n0.95 (f) SST-2\nFigure 12: The task-specialty metric (blue) and probing\nperformance (red) of each layer of a RoBERTa model\nfinetuned on each task. Each figure is a GLUE task.\nrobust to data imbalance on MNLI.\nFor the data-scarce experiments, (n0,n1,n2) is\nchosen from {(20000,20000,20000),(10000,\n10000,10000),(5000,5000,5000),(1250,1250,\n1250),(300,300,300)}. The sampled dataset is\nalways balanced. We plotted ν and ζ and |ρ|in\nFigure 16. Our metric has the same trend on MNLI\nwith more than a few thousand samples. This con-\nclusion is consistent with the conclusion on SST-2.\nC.7 About Alternatives to Our Task-Specialty\nMetric\nCanonical correlation analysis. As discussed in\nsection 4.3, a potential alternative to our proposed\nmetric is the canonical correlation between the hid-\nden states h(ℓ)\nn and the class labels yn. Hidden state\nvectors h(ℓ)\nn and one-hot vectors yn can be viewed\nas i.i.d. samples from random vectors h(ℓ) and y\nrespectively, whose relationship can be quantified\nby canonical correlation analysis. It maximizes the\ncorrelations between linear projections of paired\nsamples from these random vectors (or “views\"):\nv(ℓ)\n1 ,w(ℓ)\n1 = argmaxv,w corr(v⊤h(ℓ),w⊤y). The\nsubsequent directions vj,wj, maximize the same\ncorrelation subject to each new projection being\nuncorrelated with others in the same view for\n2 ≤i≤J = min{D,|Y|}, where Dis the dimen-\nsion of hidden states. The algorithm thus provides\n5766\n0 5 10 15\n0.0\n0.2\n0.4\n0.6\ny = -3.55e-02 x + 0.630\nR2 = 0.775\n(a) CoLA\n0 20 40 60\n0.4\n0.6\n0.8\ny = -7.54e-03 x + 0.835\nR2 = 0.740 (b) MNLI\n0 2 4 6\n0.75\n0.80\n0.85\n0.90 y = -2.88e-02 x + 0.909\nR2 = 0.959\n(c) MRPC\n0.0 2.5 5.0 7.5 10.0\n0.7\n0.8\n0.9\ny = -2.69e-02 x + 0.922\nR2 = 0.926 (d) QNLI\n0 1 2 3 4\n0.825\n0.850\n0.875\n0.900\n0.925 y = -2.58e-02 x + 0.920\nR2 = 0.985\n(e) QQP\n0.0 0.5 1.0 1.5 2.0\n0.80\n0.85\n0.90\n0.95 y = -8.21e-02 x + 0.947\nR2 = 0.813 (f) SST-2\nFigure 13: Regressing the per-layer probing perfor-\nmance onto the per-layer task-specialty metric for a\nRoBERTa model finetuned on each task.\nJ correlation values. The CCA score is measured\nas the average over all but the last correlation. This\nis based on the assumption that the last direction\nmeasures noise correlations. This assumption is\nconfirmed by our empirical observation that the\nlast correlation values are always close to zero (of\nthe order 1e-2).\nThe CCA score are plotted in blue curves and\nthe probing performance of the pretrained model\nare plotted in red curves in Figure 17. They almost\noverlap under different y-axes.\nIn order to ensure computational stability, the\nsampled auto-covariance matrices of h(ℓ) and y\nare perturbed by small constants, ϵh and ϵy, along\nthe diagonal (De Bie and De Moor, 2003). For\neach GLUE task we sample N class-balanced data-\npoints from the train set. In order to choose the\nregularization parameters and to avoid overfitting,\nwe perform 10-fold cross validation by using eight\nof the ten splits to learn the linear projection ma-\ntrices for different values of ϵh and ϵy. We use\none of the two remaining splits as development set\nand the other one as test set. The correlation for\nthe development set is evaluated using the learned\nprojection matrices and the best performing pair is\nthen used to evaluate the test set score. We repeat\nthis procedure thrice for each of the two samples\nof N data points. We experiment with different\n0 10 20\nOutput layer\n5\n10\n15\n20\n(a) CoLA\n0 10 20\nOutput layer\n200\n300\n400\n500 (b) MNLI\n0 10 20\nOutput layer\n50\n100\n150\n(c) MRPC\n0 10 20\nOutput layer\n250\n500\n750\n1000 (d) QNLI\n0 10 20\nOutput layer\n10\n15\n20\n25\n30\n(e) QQP\n0 10 20\nOutput layer\n0\n500\n1000\n1500 (f) SST-2\nFigure 14: The task-specialty metric computed using\nthe CLS token hidden states. Each figure is a GLUE\ntask.\n0 10 20\nOutput layer\n0.00\n0.25\n0.50\n0.75\n1.00 10000, 10000, 10000\n6000, 12000, 12000\n18000, 6000, 6000\n24000, 3000, 3000\n(a) νcurves\n0.64 0.95 1.05 1.1\nentropy\n4\n6\n8\n10\n0.86\n0.87\n0.88\n0.89 (b) ζ(blue) and |ρ|(red)\nFigure 15: Results of data-imbalanced experiments on\nMNLI.\nvalues of N. Some of the previous work, although\nusing CCA for representation learning, follows the\nsame scoring procedure to choose the regulariza-\ntion parameters and the corresponding projection\nmatrices (Wang et al., 2015).\nCCA has been previously used to measure sim-\nilarity of layer-wise representations within and\nacross neural network models (Raghu et al., 2017),\nand to measure similarity of the layer-wise word-\nlevel representations with off-the-shelf embedding\nmaps (Pasad et al., 2021). Williams et al. (2019)\nuse CCA to correlate grammatical gender and lexi-\ncal semantics by representing the discrete gender\nclass as a one-hot vector.\nNumerical Rank. As discussed in section 4.3, the\nrank-based metric is ϱ(ℓ) def\n= 1\n|Y|\n∑\ny∈Yϱ(ℓ)\ny where\nϱ(ℓ)\ny\ndef\n= ∥H(ℓ)\ny ∥2\n∗\n∥H(ℓ)\ny ∥2\nF\n. Each ϱ(ℓ)\ny can be viewed as mea-\n5767\n0 10 20\nOutput layer\n0.00\n0.25\n0.50\n0.75\n1.00 20000, 20000, 20000\n10000, 10000, 10000\n5000, 5000, 5000\n1250, 1250, 1250\n300, 300, 300\n(a) νcurves\n900 3750 15000 60000\nN\n0\n20\n40\n60\n0.4\n0.6\n0.8 (b) ζ(blue) and |ρ|(red)\nFigure 16: Results of data-scarce experiments on MNLI.\n0 10 20\nOutput layer\n0.2\n0.3\n0.4\n0.5\n0.6\n0.2\n0.3\n0.4\n0.5\n(a) CoLA\n0 10 20\nOutput layer\n0.2\n0.3\n0.4\n0.5\n0.55\n0.60\n0.65 (b) MNLI\n0 10 20\nOutput layer\n0.2\n0.3\n0.4\n0.84\n0.86\n(c) MRPC\n0 10 20\nOutput layer\n0.4\n0.5\n0.6\n0.70\n0.75\n0.80 (d) QNLI\n0 10 20\nOutput layer\n0.50\n0.55\n0.60\n0.65\n0.750\n0.775\n0.800\n0.825\n(e) QQP\n0 10 20\nOutput layer\n0.70\n0.75\n0.80\n0.85\n0.90 (f) SST-2\nFigure 17: The CCA score (blue) and probing perfor-\nmance (red) of each layer of a pretrained RoBERTa\nmodel. Each figure is a GLUE task.\nsuring the sparsity of the singular values {σi}of\nH(ℓ)\ny because ϱ(ℓ)\ny = ∥σ∥2\n1\n∥σ∥2\n2\n. It is an approximation\nof ∥σ∥0, i.e., the rank of matrix H(ℓ)\ny .\nMutual information. Discrete mutual information\n(MI) gives a measure of mutual dependence be-\ntween two discrete random variables. We use MI\ndependence between the sentence representations\nand the corresponding GLUE task labels as a mea-\nsure of layer-wise task specificity. In order to dis-\ncretize continuous-valued sentence representations,\nwe run k-means clustering to obtain discrete clus-\nters, as in V oita et al. (2019a). This measure has\nbeen previously used to measure the phone and\nword content in layer-wise representations of a pre-\ntrained speech model (Pasad et al., 2021).\nIn our experiments, we sampled N class-\nbalanced data-points. We held a tenth of these\nsamples out and ran the k-means clustering algo-\nrithm with C clusters on the sampled data. Then\nthe categorical ID of each held-out sample is de-\nfined to be the ID of the learned cluster that it was\nassigned to. However, after extensive tuning, we\nstill could not obtain any mutual information num-\nbers that look reasonably high: actually, all the\nnumbers were close to zero and they didn’t differ\nmuch. We believe that the difficulty stems from\nthe fact that learning clusters is unsupervised and\nunsupervised learning is known to be difficult. In-\ndeed, if we just use the class labelsyn as the cluster\nIDs, we can observe a neat clustering—that is why\nour proposed metric ν is effective. However, it\nseems extremely difficult to learn that clustering in\nan unsupervised fashion.\n5768",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7840167284011841
    },
    {
      "name": "Hyperparameter",
      "score": 0.6713988184928894
    },
    {
      "name": "Transfer of learning",
      "score": 0.6040394902229309
    },
    {
      "name": "Computation",
      "score": 0.5876877903938293
    },
    {
      "name": "Artificial intelligence",
      "score": 0.578586757183075
    },
    {
      "name": "Machine learning",
      "score": 0.5616214871406555
    },
    {
      "name": "Language model",
      "score": 0.5290065407752991
    },
    {
      "name": "Multi-task learning",
      "score": 0.5241641998291016
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5227106809616089
    },
    {
      "name": "Metric (unit)",
      "score": 0.4704132378101349
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.44624489545822144
    },
    {
      "name": "Layer (electronics)",
      "score": 0.4302651882171631
    },
    {
      "name": "Adapter (computing)",
      "score": 0.4266001284122467
    },
    {
      "name": "Task (project management)",
      "score": 0.4141278862953186
    },
    {
      "name": "Toolbox",
      "score": 0.410084992647171
    },
    {
      "name": "Speech recognition",
      "score": 0.333988219499588
    },
    {
      "name": "Algorithm",
      "score": 0.13926613330841064
    },
    {
      "name": "Engineering",
      "score": 0.0783083438873291
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I160992636",
      "name": "Toyota Technological Institute at Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ],
  "cited_by": 3
}