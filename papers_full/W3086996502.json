{
  "title": "MLMLM: Link Prediction with Mean Likelihood Masked Language Model",
  "url": "https://openalex.org/W3086996502",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3176735390",
      "name": "Louis Clouatre",
      "affiliations": [
        "Canadian Institute for Advanced Research",
        "Polytechnique Montr√©al",
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2952634862",
      "name": "Philippe Trempe",
      "affiliations": [
        "Polytechnique Montr√©al",
        "Canadian Institute for Advanced Research",
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    },
    {
      "id": "https://openalex.org/A65830212",
      "name": "Amal Zouaq",
      "affiliations": [
        "Polytechnique Montr√©al",
        "Canadian Institute for Advanced Research",
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2044221253",
      "name": "Sarath Chandar",
      "affiliations": [
        "Mila - Quebec Artificial Intelligence Institute",
        "Polytechnique Montr√©al",
        "Canadian Institute for Advanced Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250184916",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2774837955",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2964279602",
    "https://openalex.org/W2988828549",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W2964116313",
    "https://openalex.org/W2948433653",
    "https://openalex.org/W2909137510",
    "https://openalex.org/W2499696929"
  ],
  "abstract": "Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best non-entity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4321‚Äì4331\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n4321\nMLMLM: Link Prediction with Mean Likelihood Masked Language\nModel\nLouis Clouatre‚àó‚Ä†, Philippe Trempe‚àó, Amal Zouaq‚àó, Sarath Chandar‚àó‚Ä† ‚àá\n‚àóPolytechnique Montr¬¥eal\n‚Ä†Mila - Quebec AI Institute\n‚àáCanada CIFAR AI Chair\n{philippe.trempe,amal.zouaq,sarath.chandar}@polymtl.ca\n{clouatrl,sarath.chandar}@mila.quebec.ca\nAbstract\nKnowledge Bases (KBs) are easy to query, ve-\nriÔ¨Åable, and interpretable. They however scale\nwith man-hours and high-quality data. Masked\nLanguage Models (MLMs), such as BERT,\nscale with computing power as well as unstruc-\ntured raw text data. The knowledge contained\nwithin these models is however not directly in-\nterpretable. We propose to perform link pre-\ndiction with MLMs to address both the KBs\nscalability issues and the MLMs interpretabil-\nity issues. By committing the knowledge em-\nbedded in MLMs to a KB, it becomes inter-\npretable. To do that we introduce MLMLM,\nMean Likelihood Masked Language Model,\nan approach comparing the mean likelihood of\ngenerating the different entities to perform link\nprediction in a tractable manner. We obtain\nState of the Art (SotA) results on the WN18RR\ndataset and SotA results on the Precision@1\nmetric on the WikidataM5 inductive and trans-\nductive setting. We also obtain convincing re-\nsults on link prediction on previously unseen\nentities, making MLMLM a suitable approach\nto introducing new entities to a KB.\n1 Introduction\n1.1 Context\nKBs have many desirable properties. They are easy\nto query, veriÔ¨Åable, and perhaps most importantly\ninterpretable by humans. They however have one\ncritical shortcoming, they are expensive to build,\nmaking them harder to scale. Indeed, modern KBs\nscale with high-quality data, manual labor, or a\nmix of both. Approaches that scale with available\ncomputation and the massive amounts of unstruc-\ntured data that are being created and accumulated\nhave proven invaluable in the recent deep learning\nboom.\nLarge pretrained MLMs (Devlin et al., 2018;\nLiu et al., 2019) have been shown to scale well\nwith large amounts of unstructured text data as\nwell as with computing power. They also have dis-\nplayed some interesting emergent abilities, such as\nthe ability to perform zero-shot question answering\n(Radford et al., 2019; Brown et al., 2020). This\nability implies that the model parameters contain a\nlarge amount of factual knowledge that it can lever-\nage to answer a wide variety of questions. However,\nthat knowledge is hardly interpretable by humans,\nas it is hidden within the millions to billions of\nparameters of the language model.\nBy using MLMs to completes KBs, we can ad-\ndress both the issue of scalability of KBs and the is-\nsue of the interpretability of MLMs by committing\nknowledge of the latter to an interpretable format\nin the former. The MLM can learn new knowledge\nfrom the large amount of unstructured textual data\nthat keeps being added to the World Wide Web and\nthen be used to continually complete and update the\nKB. This has the very desirable effect of making\nthe link prediction approach scale with both compu-\ntational power and a large quantity of unstructured\ndata, both of which show no sign of slowing down.\n1.2 Problem DeÔ¨Ånition\nGiven an entity and a relation, we want to train an\nMLM to generate all entities completing the KB\ntriple.\nSeveral technical challenges had to be addressed\nto achieve proper link prediction with pretrained\nMLMs. The Ô¨Årst one is tractability. It is well\nknown that inference in the task of Link Prediction\nis extremely costly, to the point where validation\nand test sets are purposefully kept small and most\ndatasets will shy away from containing millions\nof different entities (Wang et al., 2019). While\nsmaller Link Prediction datasets (Dettmers et al.,\n2017; Toutanova and Chen, 2015) are limited to a\nfew thousand entities, a dataset more representa-\ntive of full sized KBs (Wang et al., 2019) would\ncontain upwards of a million potential entities. A\n4322\nmodel that would be used on such a dataset could\nnot realistically require an inference step through\nan MLM for every potential entity completing a\ntriplet. It is necessary to enable link prediction\nwith as little inference to the model as possible,\nas performing inference on pretrained models is\nexpensive. Otherwise, it could result in a model\nwith no practical purposes, even if it obtained better\nleaderboard scores.\nThe second challenge has to do with the infer-\nence outputs format of the MLMs. The length of\nthe output needs to be known at inference time (De-\nvlin et al., 2018; Vaswani et al., 2017), making it\nhard to sample entities of varying lengths from it.\nThe example ‚ÄùHorses like ‚Äù could be completed\nwith the word ‚Äùcarrot‚Äù and ‚Äùlong runs‚Äù, but those\ntwo answers require varying length of masked in-\nputs to be Ô¨Ålled. Work like Petroni et al. (2019)\nis limited to single token outputs, which is useful\nto probe the model for the presence of embedded\nknowledge, but is not usable in practice for tasks\nsuch as link prediction, as the missing entities will\nhave variable lengths. Solutions have to be able to\nsample an MLM for entities of varying lengths to\nhave practical applications.\nFinally, the use of MLMs opens the door to per-\nforming link prediction on entities that have not\nbeen previously seen by the model or the KB. This\npermits the addition of new entities to a KB on\ntop of the link prediction capacities. Some capa-\nbility of such an approach with MLMs was previ-\nously demonstrated (Petroni et al., 2019) and other\nworks have approached the task by generating and\ncomparing entities embedding (Daza et al., 2020;\nGupta et al., 2017; Wang et al., 2019) for differ-\nent KB tasks. By generating entity embeddings\nfrom text, they permit apt representation for pre-\nviously unseen entities that can then be compared\nto other, previously seen entities. Unlike previous\napproaches we forgo the entity embedding step and\nlet the model directly output the entity. We show\nthat our approach yields strong results with unseen\nentities of arbitrary lengths in this task and should\nbe explored further.\n1.3 Contribution\nOur main contributions are summarized here:\n‚Ä¢ We propose MLMLM, a mean likelihood\nmethod to compare the likelihood of differ-\nent text of different token lengths sampled\nfrom an MLM.\n‚Ä¢ We demonstrate the tractability of our ap-\nproach, requiring only one inference step\nthrough the model to perform link prediction\non any numbers of possible entities, some-\nthing which was not previously possible with\nan MLM.\n‚Ä¢ We achieve SotA results on the WN18RR\nbenchmark and the best Precision@1 on both\nthe inductive and transductive setting of the\nWikidataM5 dataset.\n‚Ä¢ We demonstrate that our approach can gen-\neralize to previously unseen entities on all\nbenchmarks.\n2 Background and Related Work\n2.1 Masked Language Models\nPretrained MLMs, popularized by BERT (Devlin\net al., 2018), have seen tremendous success when\napplied to Natural Language Understanding (NLU)\nproblems. They are pretrained on massive amount\nof unsupervised text data. Those models incor-\nporate enormous amounts of language knowledge\nand world knowledge within their weights. This\nlets them be further tuned on challenging NLU\ntasks with great success. Being based on the trans-\nformer (Vaswani et al., 2017) encoder architecture,\nthe output length of the model is equal to the in-\nput length. This makes it challenging to sample\ntext of arbitrary length when using MLMs with-\nout knowing the length of the desired sample in\nadvance.\n2.2 Sampling from MLM\nSampling single words from an MLM is trivial.\nBy adding a mask to the input, we can sample\nlikelihoods for the whole vocabulary. This fea-\nture is used by several pieces of work to complete\nsentences, answer questions and more (Guu et al.,\n2020; Lewis et al., 2020; Petroni et al., 2019). Work\nto generate and evaluate multi-token spans from\nMLM has also yielded interesting results (Wang\nand Cho, 2019; Salazar et al., 2020). We are\nhowever unaware of any other approach to sam-\npling and evaluating multi-token spans of variable\nlength, which is necessary to properly accomplish\nthe task of link-prediction in a single pass through\nthe model.\n4323\n2.3 A Re-evaluation of Knowledge Graph\nCompletion Methods\nRecently, Sun et al. (2020) has found that many of\nthe SotA approaches to link prediction have used\nan inappropriate evaluation protocol. They have\nshown that the evaluation protocol typically used\nin the link prediction approaches assigns a perfect\nscore to a constant output, by putting the correct\nentities on top during a tiebreaker. In essence, un-\nder this evaluation protocol, assigning a likelihood\nof 0 to all entities would yield a perfect reranking\nscore, since the tiebreaker would put the target en-\ntity as the Ô¨Årst prediction. This was shown to yield\nvery inÔ¨Çated scores for many neural network based\nlink prediction approaches (Nathani et al., 2019;\nVu et al., 2019; Nguyen et al., 2017), as several\nof them output a large number of tied scores for\nthe various entities. Entity-embedding based ap-\nproaches (Bala Àázevi¬¥c et al., 2019; Sun et al., 2019;\nDettmers et al., 2018) do not suffer from this issue.\nWhile we have found that our approach does not\nsuffer from this issue despite not being an entity-\nembedding approach, we will use the random evalu-\nation protocol proposed by Sun et al. (2020) for all\nevaluations and compare against approaches that\nused a similar protocol to ensure the validity of\nthe comparisons. This protocol is similar to the\nÔ¨Åltered setting (Bordes et al., 2013), with the differ-\nence that the rank among entities with tied scores\nis randomly assigned.\n2.4 KG-BERT\nKG-BERT (Yao et al., 2019) is an approach to KB\ntasks based on MLM. It successfully demonstrates\nthe potential of leveraging these models‚Äô internal\nknowledge on KB tasks. They train a BERT model\nto classify whether an individual triple fed to the\nmodel is correct or not. In essence, they feed ev-\nery single possible (h, r, ?) or (?, r, t) triple in a\nstring format to the model to obtain all scores to be\nreranked. This can result in millions of inference\nsteps on the MLM for a single triple completion de-\npending on the size of the KB. KG-BERT has many\nadvantages over our proposed approach. It uses the\ntarget entity in the input, thus giving the model\nmore information to use at inference time. The\nproblem that it solves is much simpler, reducing it\nto a simple sequence classiÔ¨Åcation problem. With\na similar setup, it is even likely that KG-BERT\nwould yield better results than our proposed ap-\nproach since it has access to more information and\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000057/uni0000004c/uni00000048/uni00000056\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni00000056/uni0000000c\n/uni00000032/uni00000058/uni00000055/uni00000003/uni00000044/uni00000053/uni00000053/uni00000055/uni00000052/uni00000044/uni00000046/uni0000004b\n/uni0000002e/uni0000002a/uni00000010/uni00000025/uni00000028/uni00000035/uni00000037\nFigure 1: Approach Inference Time. This Ô¨Ågure\nshows the per-entity inference time based on the\ntotal number of entities to be re-ranked, of\nMLMLM and KG-BERT, the most comparable\napproach.\nhas a more straightforward training setup. Unfortu-\nnately, KG-BERT is not tractable on any reasonably\nlarge KB (see Figure 1). For a KB containing mil-\nlions of entities, KG-BERT would require millions\nof inference step through the MLM model for ev-\nery triple completion. In contrast, our approach\nrequires only one inference step through the MLM\nmodel for every triple completion, by generating\nall logits required to obtain the likelihood of any\npotential entity at once. Modern KBs can con-\ntain millions of entities (Wang et al., 2019), which\nwould translate in KG-BERT requiring hours to\ncomplete a single triplet on a GPU.\n3 Methodology\n3.1 Overview\nOur system performs link prediction. It uses MLM\nto generate all possible logits of all tokens required\nto generate all entities, and mean likelihood sam-\npling to rerank all possible entities to complete the\ntriple and perform link prediction. It can also be\nused to sample likelihoods for previously unseen\nentities.\nFigure 2 shows a toy example of our inference\nsetup. Our model outputs logits for the whole vo-\ncabulary of the RoBERTa-Large model. The out-\nputs are simply the logits for all words and sub-\nwords from that vocabulary. This vocabulary is\nexpressive enough to generate any English text.\n4324\nT = \nToken IDPos 1 Pos 2\n\"c\"\n\"r\"\n\"at\"\n0.7 0.4\n-1.2 0.6\n0.8 0.2\nE = \nEntity IDPos 1 Pos 2\n\"cat\"\n\"rat\"\n\"at\"\n\"c\" \"at\"\n\"r\" \"at\"\n\"at\" MASK\nS = \nEntity ID Score\n\"cat\"\n\"rat\"\n\"at\"\nAVG([0.7, 0.2]) = 0.45\nAVG([-1.2, 0.2]) = -0.5\nAVG([0.8]) = 0.8 1\n2\n3\nRank\nFigure 2: Ranking Example. The Ô¨Ågure contains\na minimal example of the ranking system. It\nrepresents the system in a KB containing 3\nsubwords in its vocabulary [‚Äôc‚Äô, ‚Äôr‚Äô, ‚Äôat‚Äô], 3 entities\nto rank [‚Äôcat‚Äô, ‚Äôrat‚Äô, ‚Äôat‚Äô] and a maximum entity\nlength of 2.\nHead entity Definition\nof head entity Relation ?\n(Tail entity)\nLanguage model\nLogits Logits Logits Logits\nLookup \ntable\nFigure 3: Lookup Table Generation For Tail\nEntity Prediction. The Ô¨Ågure shows how the\nlookup table for tail entity prediction is generated.\nA string representation of the head entity and the\nrelation are fed to the masked language model\nwhich outputs logits that represent the likelihood of\nÔ¨Ånding each token at each possible position of the\ntail entity.\nWe decide in advance on a maximum length n for\nthe maximum length of English text to generate.\nWith a vocabulary of roughly 50,000 logits, the\nmodel would output a vector of logits of shape\n[50,000 x n]. Knowing in advance the tokens that\nwould build the string for all possible entities, we\ncan obtain a score for the likelihood of those en-\ntities completing a triplet by averaging the logits\nof those tokens. To evaluate the likelihood of the\nentity ‚Äùbrown dog‚Äù, we would average the logits\nfor the word ‚Äùbrown‚Äù on the Ô¨Årst column of the ma-\ntrix with the logits of the word ‚Äùdog‚Äù on the second\ncolumn of the matrix, ignoring all other columns\nof our logit matrix. Even if the model would never\nhave encountered the entity ‚Äùbrown dog‚Äù it could\nstill produce a score for said entity.\n3.2 Data Pre-processing\nThe data pre-processing pipeline takes a link pre-\ndiction dataset and transforms it into a generic for-\nmat usable by the model. It is required that both\nthe entity and relations have string representations.\nFor every entity in the dataset, we extract an entity\nstring, which uniquely identiÔ¨Åes the entity, and a\ndeÔ¨Ånition string, which is a textual description of\nthe given entity. For every relation, we extract a\nrelation string, which uniquely identiÔ¨Åes and de-\nscribes the relation.\nWe tokenize all strings through the pretrained\nRoBERTa tokenizer (Sennrich et al., 2016) and fur-\nther transform the entity string by adding padding\nto match the longest tokenized entity within the\ndataset. Concretely, in a dataset where the longest\nentity has a length of 4 token ids, the entity string\n‚Äúdog‚Äù would be padded to have the representation\n‚Äúdog ‚Äù and the entity string ‚Äùcat and dog‚Äù would\nhave the representation ‚Äúcat and dog ‚Äù where ‚Äú ‚Äù\nis the padding token. The purpose of this padding\nis to standardise the masked representation of all\nentities, therefore letting the model treat all entities\nin the same manner.\n3.3 Model\nOur approach uses the RoBERTa-Large model (Liu\net al., 2019) for all experiments. We Ô¨Ånetune the\npretrained model on the link prediction datasets to\ngenerate the logits of the unknown entities. As our\napproach does a single call to the model to rerank\nall possible entities, it is acceptable to use the larger\nmodel for better performance. Figure 3 shows the\ninference process for tail entity prediction. Simi-\nlarly, the head entity prediction takes as input the\nhead entity mask, the relation, the tail entity and\nthe tail entity deÔ¨Ånition. We use the relation string,\nthe known entity string and the entity deÔ¨Ånition of\nthe known entity string to make the model generate\nthe logits representing the unknown entity string.\n3.4 Ranking System\nThe ranking system pictured in Figure 4 performs\nlink prediction on a given triple. The MLM outputs\nlogits for all possible token ids and positions for the\nmissing entity to complete the triple. This forms\nthe lookup table T. The link prediction dataset\ncontains a list of all possible entities. The token\nids forming those entities make up E. We obtain\n4325\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 1\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 2\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 3\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 4\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 5\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 6\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 7\nLogit\nLogit\n‚ãÆ\nLogit\nPosition 8\nToken ID \n0\nToken ID \n1\n‚ãÆ\nToken ID \nùëÅùëá\nToken ID\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 1\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 2\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 3\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 4\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 5\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 6\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 7\nToken ID\nToken ID\n‚ãÆ\nToken ID\nPosition 8\nEntity ID \n0\nEntity ID \n1\n‚ãÆ\nEntity ID \nùëÅùê∏\nEntity ID\nùëá =\nùê∏ =\nLookup \ntable\nùëá\nEntity \ntokens\nùê∏\nRanked \nentities\nùëÖ\nLookup\nFOR entity_id:\nFOR (token_id, pos):\nv = T[(token_id, pos)]\nL[(entity_id, pos)] = v\nEntity \ntoken \nlogits\nùêø\nRanking\nFOR entity_id:\nlogits = L[entity_id]\nR[entity_id] = mean(logits)\nR = sort_by_highest_rank(R)\nFigure 4: Ranking System. The Ô¨Ågure details the inner workings of the ranking system which uses the\nlookup table generated by the masked language model to compute the score associated with each possible\nentity. The scored entities are then ranked by highest score.\nthe entity token logits L by matching all token\nids in E with their corresponding values in T. L\nrepresents how likely every token of the entity is to\nbe generated by the MLM at that speciÔ¨Åc position.\nThe mean likelihood1 of each entity is computed by\naveraging L over non-padded token logits.2 This\nvalue is used to determine the ranking of the entity.\nIt provides a proper comparison between entities\nof different lengths.\nConcretely, in our previous ‚Äúcat and dog ‚Äù ex-\nample, we average the output logits for the ‚Äúcat\nand dog‚Äù token ids and positions while ignoring\nthe Ô¨Ånal padded logit. This averaging is done on all\nentities in the dataset completing the triple, yield-\ning the average likelihood assigned by the model\nto all entities.\nEntities are then sorted by highest rank using the\nrandomized setting (Sun et al., 2020). For equal\nscores the tie-breaking is done randomly, to pro-\nduce the ordered list of ranked entities R. We use\nthe Ô¨Åltered setting (Bordes et al., 2013) for eval-\nuation and remove corrupted triples from the list\nof ranked entities, corrupted triples being all other\nknown correct triples.\n1Because the length of non-padded tokens is variable, us-\ning the mean of the logits is the chosen comparison metric for\nre-ranking.\n2By far, the token the model sees most is the padding\ntoken. Counting it would most likely yield a heavy skew\ntowards shorter entities with more padding.\n4 Experimentation\n4.1 Datasets\nThe two datasets used are WN18RR and Wiki-\ndataM5 (Dettmers et al., 2017; Fellbaum, 1998;\nBollacker et al., 2008; Wang et al., 2019), a com-\nmonly used link prediction benchmark and a new,\nlarge scale, link prediction benchmark. Summary\nstats are shown in Table 1.\nTable 1: Datasets statistics.\nWN18RR WikidataM5\n#Entities 40,943 4 ,594,585\n#Relations 11 822\n#Training 86,835 20 ,614,279\n#Validation 3034 5163\n#Test 3134 5133\nMean in-degree 2.12 1\nMedian in-degree 4.49 0\nWN18RR is a dataset composed of WordNet\nsynsets. We use the cleaned synset as the entity\nstring. The synset ‚Äúdog.n.01‚Äù would have a string\nrepresentation of ‚Äúdog noun 1‚Äù which should be\nmore interpretable by the model while remaining a\nunique identiÔ¨Åer. The entity deÔ¨Ånition is the deÔ¨Åni-\ntion of the entity given by WordNet. The relation\nstring is a cleaned representation of the relation\n4326\nstring. The relation ‚Äú member of domain usage‚Äù\nwould be represented with the string ‚Äúmember of\ndomain usage‚Äù. Full examples of inputs and out-\nputs are shown in Listing 1 and Listing 2.\nWikidataM5 is composed of triples based on\nWikidata and the English Wikipedia with aligned\ndescriptions for each entity. We use the entity string\nand deÔ¨Ånitions provided in the benchmark.\n4.2 Metrics\nWe use the Mean Reciprocal Rank (MRR) metric\nto validate our model and select the best model.\nFor all experiments, we also report the Mean Rank\n(MR), the Mean Precision at 1 (MP@1), the Mean\nPrecision at 3 (MP@3), and the Mean Precision at\n10 (MP@10).\n4.3 Training\nThe training setup is a modiÔ¨Åed MLM training,\nwhere we let the model generate the missing en-\ntity. The previously mentioned padding lets us deal\nwith the generation of entities of varying sizes. The\ninput fed to the model for tail entity prediction,\ndepicted in Figure 3, consists of the concatenated\ntoken ids of the head entity, the head entity deÔ¨Å-\nnition, the relation and the tail entity mask. The\nmodel will then generate, in the place of the mask,\nthe missing entity. The input fed to the model for\nhead entity prediction is similar. An example of the\ninput for head entity prediction is found in Listing 1\nand an example for tail entity prediction is found\nin Listing 2.\nWe use the categorical cross-entropy loss to train\nthe language model. The loss only depends on the\nnon-padded token of the generated entity, ignoring\nall other outputs. The target is the actual entity\ncompleting the triple, aligned with the mask in the\ninput. We retain the model with the best validation\nMRR. All experiments are run for 5 random seeds\nand the mean and standard deviation of the results\nare reported.\nFor all experiments, we use the hyperparameters\nand training setup described in Liu et al. (2019),\nwith a total of 25 epochs for the WN18RR dataset\nand 1 epoch for the WikidataM5 dataset.\n4.4 Unseen Entities\nAn alternative version of the dataset is made to\ntest the generalization capacity of our methodol-\nogy to unseen entities. For WN18RR we start by\nrandomly sampling 5% of the entities for the vali-\ndation entities and 5% of the entities for the testing\nentities. Our training set consists of all triples not\ncontaining any of the validation or testing entities.\nOur validation set consists of all triples containing\nthe validation entities. Finally, our test set consists\nof all triples containing the test entities, but not\ncontaining any of the validation entities. The train-\ning is done in the same fashion. The validation\nand testing are only done on entities present in the\nvalidation or test entity list, but are still reranked\nagainst all other possible entities. While WN18RR\nwould only have 2047 test entities in this setting,\nthe reranking would still involve 40,943 entities. If\nthe tail entity is the one present in the test entity list,\nwe will complete the link (h, r,?) and not the link\n(?, r, t). The reported results are therefore only on\nthe performance of previously unseen entities in\nthe KB, compared to all other possible entities. The\nvalidation and test set are rebuilt for every random\nseed, to evaluate our approach on a wider array of\nunseen entities.\nWikidataM5 has an inductive setting which\nclosely resembles our unseen entities setting. The\nmain difference is that the reranking step in the\nvalidation and test only compares with other previ-\nously unseen entities. While there is upwards of 4\nmillion training entities, the validation and testing\nwould only consider roughly 7000 entities. For the\npurpose of proper comparison, the M5 results are\nhowever done within their speciÔ¨Åc setting.\n5 Results and Analysis\n5.1 WN18RR\nWe achieve SotA results on the WN18RR dataset\non all tested metrics with the exception of MR,\nas shown in Table 2. The WN18RR dataset is\nsparse in terms of the KG in-degree connections,\nsee Table 1. Sparseness lends itself naturally to\nleveraging a pretrained model. The amount of in-\nformation that can be extracted from the dataset\non any given entity is then limited, which makes\noutside information all the more valuable.\nWe can observe that the MR metric is relatively\nmuch weaker for our model, compared to its other\nmetrics. This implies that while the model will of-\nten rerank the correct entities to the top, it will also\nsometimes forget certain entities completely, given\nthem ranks in the thousands. This could be ex-\nplained by an issue of disambiguation in the name\nof the entity. While approaches using entity em-\nbeddings (Bala Àázevi¬¥c et al., 2019; Sun et al., 2019;\nDettmers et al., 2018) will have no issue separating\n4327\nTable 2: WN18RR Results\nApproach MRR ‚Üë MR ‚Üì MP@1 ‚Üë MP@3 ‚Üë MP@10 ‚Üë\nConvE 44.4 4950 ‚Äî ‚Äî 50.3\nRotatE 47.3 3343 ‚Äî ‚Äî 57.1\nTuckER 46.1 6324 ‚Äî ‚Äî 51.6\nConvKB 24.9 3433 ‚Äî ‚Äî 52.4\nCapsE 41.5 718 ‚Äî ‚Äî 55.9\nKBAT 41.2 1921 ‚Äî ‚Äî 55.4\nKG-BERT ‚Äî 97 ‚Äî ‚Äî 52.4\nMLMLM 50.17\n¬± 0.18\n1603\n¬± 26.8184\n43.91\n¬± 0.20\n54.18\n¬± 0.28\n61.10\n¬± 0.20\nThe results are reported as <mean> ¬± <standard deviation>. Results for other models are taken from\nSun et al. (2020); Yao et al. (2019).\nListing 1: Example of an error of the model on WN18RR. Shown are the top 3 ranked entities by the model with\nthe score assigned to them. The correct answer, matchmaker noun 1, was ranked 14,108 by the system.\nPrompt : <s><mask><mask><mask><mask><mask><mask><mask><mask>hypernym mediator noun 1\na negotiator who acts as a link between parties</s><pad><pad><pad><pad>\nCorrect answer : matchmaker noun 1<pad><pad><pad><pad> Answer rank 14108\nRank 1 Score 32.0242 : interpreter noun 2<pad><pad><pad>\nRank 2 Score 32.0103 : harmonizer noun 1<pad><pad><pad>\nRank 3 Score 31.8889 : diplomat noun 1<pad><pad><pad>\nTable 3: WikidataM5 Results\nApproach MRR ‚Üë MR ‚Üì MP@1 ‚Üë MP@3 ‚Üë MP@10 ‚Üë\nTransE 25.3 109370 17.0 31.1 39.2\nDKRL 16.0 31566 12.0 18.1 22.9\nKEPLER-Wiki 15.4 14454 0.105 0.174 0.244\nKEPLER-Cond 21.0 20267 17.3 22.4 27.7\nMLMLM 22.3 488161 20.1 23.2 26.4\nA single seed was ran for the WikidataM5 experiments because of the size of the dataset. Results for\nother models are taken from Wang et al. (2019).\nTable 4: WN18RR Unseen Entities Result\nApproach MRR ‚Üë MR ‚Üì MP@1 ‚Üë MP@3 ‚Üë MP@10 ‚Üë\nRandom baseline 0.03\n¬± 0.007\n20541.91\n¬± 87.88\n0.002\n¬± 0.004\n0.002\n¬± 0.004\n0.026\n¬± 0.008\nNon-Ô¨Ånetuned RoBERTa 2.73\n¬± 0.05\n10130.35\n¬± 187.61\n1.54\n¬± 0.07\n2.95\n¬± 0.11\n4.92\n¬± 0.19\nMLMLM 18.42\n¬± 2.66\n3761.50\n¬± 255.4437\n14.16\n¬± 0.81\n21.75\n¬± 1.19\n29.39\n¬± 0.88\nThe results are reported as <mean> ¬± <standard deviation>.\nthe synsets dog.n.01 and dog.n.03 as mean-\ning respectively ‚Äúa member of the genus Canis [...]‚Äù\nand ‚Äúinformal term for a man‚Äù, our model will\nhave to discern between those two meanings only\nby the digit appended to the name. It is probable\nthat the model is often confused about whether it\nshould generate dog noun 1 or dog noun 3,\nhaving only the Ô¨Ånal digit to differentiate both of\n4328\nListing 2: Example of a disambiguation error of the model on WN18RR. Shown are the top 3 ranked entities by\nthe model with the score assigned to them. The correct answer, aid noun 3, was ranked second by the system,\nafter aid noun 1.\nPrompt : <s>grant noun 1 any monetary aid hypernym<mask><mask><mask><mask><mask><\nmask><mask><mask></s><pad><pad><pad><pad>\nCorrect answer : aid noun 3<pad><pad><pad><pad><pad> Answer rank 2\nRank 1 Score 33.7597 : aid noun 1<pad><pad><pad><pad><pad>\nRank 2 Score 33.5948 : aid noun 3<pad><pad><pad><pad><pad>\nRank 3 Score 32.7605 : aid noun 2<pad><pad><pad><pad><pad>\nTable 5: WikidataM5 Inductive setting Results\nApproach MRR ‚Üë MR ‚Üì MP@1 ‚Üë MP@3 ‚Üë MP@10 ‚Üë\nDKRL 23.1 78 5.9 32.0 54.6\nKEPLER-Cond 40.2 28 22.2 51.4 73.0\nMLMLM 28.4 932 22.6 28.5 34.8\nThe results are reported as <mean> ¬± <standard deviation>.\nthem. An example of such an error is shown in\nListing 2, where the model confuses aid.n.01\nand aid.n.03. Follow up work on better rep-\nresentations for entity names could yield stronger\nresults.\nOur model generally has a much easier time pre-\ndicting the tail entity than the head entity. It has\nan MRR of 60.15 on tail entities and an MRR of\n40.09 on head entities. By observing the instances\nwhere our model gives the worst rank to the correct\nanswer, we can understand why. A large number\nof those cases are hypernyms on the head entity.\nAn example of a hypernym relationship would be:\n‚Äúanimal is an hypernym of dog, since all dogs are\nanimals.‚Äù Correctly ranking all possibilities for ‚ÄúX\nis an hypernym of dog.‚Äù is more straightforward for\nthe model than correctly ranking all possibilities for\n‚ÄúAnimal is an hypernym of Y .‚Äù. An example of such\nfailure is shown in Listing 1, where we look for\nthe hypernym of the term mediator. It is clear\nthat the model understands the concept and outputs\nplausible answers in its top 3. A large amount of\nthe model‚Äôs severe failure cases are similar to this\none, where the model will output a plausible hy-\npernym of the tail entity, while completely missing\nthe targeted hypernym. This seems to be the likely\ncause for the weak MR of the approach.\n5.2 WikidataM5\nThe results on WikidataM5 are shown in Table 3.\nMLMLM boasts the best Precision@1 metric by a\nfair margin. Once again, we observe the weakness\nin the MR metric. The implications are that there\nare many possible correct entities that are given no\nweights by the approach.\n5.3 Unknown Entities Experiments\nWe demonstrate the capacity of our approach to\ngeneralize to unknown entities. Results for the\nWN18RR datasets are shown in Table 4.\nFor baselines, we use a random baseline, rerank-\ning the entities randomly, as well as a non-Ô¨Ånetuned\nRoBERTa-large model, that simply generates the\nentity tokens without being Ô¨Ånetuned on the dataset\nÔ¨Årst. We can notice that while our approach out-\nperforms a non-Ô¨Ånetuned benchmark, the non-\nÔ¨Ånetuned RoBERTa model still far outperforms the\nrandom baseline, supporting some of the Ô¨Åndings\nof Petroni et al. (2019) in the capacity of MLM to\nperform unsupervised link prediction.\nThe M5 inductive settings are reported in Ta-\nble 5. We obtain the best Precision@1 metric. The\nweakness in MR is once again visible, supporting\nthe intuition that while the model might generate\nthe correct entity with high conviction, it will of-\nten not give positive score to all plausible entities,\nyielding a much worst average rank.\nWe believe that leveraging MLMs could even-\ntually lead to automatically populating KBs with\nnew entities, as new knowledge and new facts are\ncreated and added to the web.\n5.4 Limitations\nMLMLM comes with several limitations. Our ap-\nproach to padding limits the size of an unknown\n4329\nentity to the size of the longest known entity. While\nit is unlikely to be limiting in practice, it is still a\nweakness of our approach to sampling. The model\nsize can be very prohibitive and specialized hard-\nware such as GPUs is required to run it in a timely\nfashion. The approach however remains tractable\nas it can provide likelihoods for all possible enti-\nties in a single inference call. Compared to entity-\nembedding based methods, our approach needs\nadditional information in the form of meaningful\nstring representations for both entities and relations.\nThe lack of entity disambiguation is also a limit-\ning factor that does not affect other approaches.\nFinally, our approach is liable to forgetting some\nentities, leading to comparatively much worst MR\nthan prior approaches.\n6 Conclusion\nWe have developed a methodology for training\nmasked language models to perform link prediction.\nBy leveraging the natural language understanding\nabilities of these models and the factual knowledge\nembedded within their weights, we have achieved\na tractable approach to link prediction that yields\nstate of the art results on a standard benchmark\nand the best Precision@1 on another competitive\nbenchmark. We have also demonstrated the ability\nof our model to perform link prediction of previ-\nously unseen entities, making our approach suit-\nable to introduce new entities to knowledge bases.\nMore generally, we have introduced an approach\nto sampling text from a masked language model of\nvarying lengths, which can have a wider use case.\nAcknowledgements\nThis research was supported by Apog ¬¥ee Canada,\nCanada First Research Excellence Fund program\nand ¬¥Ecole Polytechnique Startup Fund PIED. SC\nis supported by a Canada CIFAR AI Chair and an\nNSERC Discovery Grant.\n4330\nReferences\nIvana Bala Àázevi¬¥c, Carl Allen, and Timothy M\nHospedales. 2019. Tucker: Tensor factorization\nfor knowledge graph completion. arXiv preprint\narXiv:1901.09590.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: A collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\nSIGMOD International Conference on Management\nof Data, SIGMOD ‚Äô08, page 1247‚Äì1250, New York,\nNY , USA. Association for Computing Machinery.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In C. J. C. Burges, L. Bottou,\nM. Welling, Z. Ghahramani, and K. Q. Weinberger,\neditors, Advances in Neural Information Processing\nSystems 26 , pages 2787‚Äì2795. Curran Associates,\nInc.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nD. Daza, Michael Cochez, and Paul T. Groth. 2020. In-\nductive entity representations from text via link pre-\ndiction. ArXiv, abs/2010.03496.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2017. Convolutional 2d\nknowledge graph embeddings. In AAAI.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d\nknowledge graph embeddings. In Thirty-Second\nAAAI Conference on ArtiÔ¨Åcial Intelligence.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nChristiane Fellbaum, editor. 1998. WordNet: an elec-\ntronic lexical database. MIT Press.\nNitish Gupta, Sameer Singh, and Dan Roth. 2017. En-\ntity linking via joint encoding of types, descriptions,\nand context. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2681‚Äì2690, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nKelvin Guu, Kenton Lee, Z. Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv,\nabs/2002.08909.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus,\nF. Petroni, V . Karpukhin, Naman Goyal, Heinrich\nKuttler, M. Lewis, Wen tau Yih, Tim Rockt ¬®aschel,\nSebastian Riedel, and Douwe Kiela. 2020. Retrieval-\naugmented generation for knowledge-intensive nlp\ntasks. ArXiv, abs/2005.11401.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nDeepak Nathani, Jatin Chauhan, Charu Sharma, and\nManohar Kaul. 2019. Learning attention-based\nembeddings for relation prediction in knowledge\ngraphs. arXiv preprint arXiv:1906.01195.\nDai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc\nNguyen, and Dinh Phung. 2017. A novel embed-\nding model for knowledge base completion based\non convolutional neural network. arXiv preprint\narXiv:1712.02121.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d Alch¬¥e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024‚Äì8035. Curran Asso-\nciates, Inc.\nFabio Petroni, Tim Rockt ¬®aschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP/IJCNLP.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and\nK. Kirchhoff. 2020. Masked language model scor-\ning. In ACL.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715‚Äì\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\n4331\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embed-\nding by relational rotation in complex space. arXiv\npreprint arXiv:1902.10197.\nZhiqing Sun, Shikhar Vashishth, Soumya Sanyal,\nPartha Talukdar, and Yiming Yang. 2020. A Re-\nevaluation of Knowledge Graph Completion Meth-\nods. arXiv e-prints, accepted at ACL 2020 , page\narXiv:1911.03903.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd Workshop on\nContinuous Vector Space Models and their Composi-\ntionality, pages 57‚Äì66, Beijing, China. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nThanh Vu, Tu Dinh Nguyen, Dat Quoc Nguyen, Dinh\nPhung, et al. 2019. A capsule network-based em-\nbedding model for knowledge graph completion and\nsearch personalization. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2180‚Äì2189.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a markov ran-\ndom Ô¨Åeld language model. CoRR, abs/1902.04094.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019. KE-\nPLER: A uniÔ¨Åed model for knowledge embedding\nand pre-trained language representation. CoRR,\nabs/1911.06136.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R‚Äôemi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface‚Äôs trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg-\nbert: Bert for knowledge graph completion. ArXiv,\nabs/1909.03193.",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9051929712295532
    },
    {
      "name": "Computer science",
      "score": 0.751071572303772
    },
    {
      "name": "Scalability",
      "score": 0.6447975635528564
    },
    {
      "name": "Language model",
      "score": 0.5342758297920227
    },
    {
      "name": "Artificial intelligence",
      "score": 0.531583845615387
    },
    {
      "name": "Link (geometry)",
      "score": 0.499603271484375
    },
    {
      "name": "Verifiable secret sharing",
      "score": 0.49859023094177246
    },
    {
      "name": "Embedding",
      "score": 0.4822740852832794
    },
    {
      "name": "Machine learning",
      "score": 0.4651767909526825
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4423748850822449
    },
    {
      "name": "Data mining",
      "score": 0.43188977241516113
    },
    {
      "name": "Natural language processing",
      "score": 0.4294532239437103
    },
    {
      "name": "Database",
      "score": 0.08998781442642212
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}