{
  "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
  "url": "https://openalex.org/W3177318507",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5086419494",
      "name": "Haoyi Zhou",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5013030532",
      "name": "Shanghang Zhang",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5051856627",
      "name": "Jieqi Peng",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100328838",
      "name": "Shuai Zhang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100380469",
      "name": "Jianxin Li",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5101862104",
      "name": "Hui Xiong",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A5083360053",
      "name": "Wancai Zhang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6642591857",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W6677096361",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W2176412452",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W2768008502",
    "https://openalex.org/W2937537592",
    "https://openalex.org/W2054685200",
    "https://openalex.org/W2613328025",
    "https://openalex.org/W6699643055",
    "https://openalex.org/W2549483845",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6742924651",
    "https://openalex.org/W2970100546",
    "https://openalex.org/W6738846887",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W1969852690",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W2757354914",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2773625660",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2963285578",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4302438404",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2765932895",
    "https://openalex.org/W2962850830",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4230715394",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2963358464",
    "https://openalex.org/W2954731415",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2762309767",
    "https://openalex.org/W2319170717",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.",
  "full_text": "Informer: Beyond Efﬁcient Transformer for Long Sequence\nTime-Series Forecasting\nHaoyi Zhou, 1 Shanghang Zhang, 2 Jieqi Peng, 1 Shuai Zhang, 1 Jianxin Li, 1\nHui Xiong, 3 Wancai Zhang 4\n1 SCSE and BDBC, Beihang University, Beijing, China\n2 UC Berkeley, California, US\n3 Rutgers University, New Jersey, US\n4 Beijing Guowang Fuda Science & Technology Development Company\nfzhouhy, pengjq, zhangs, lijxg@act.buaa.edu.cn, shz@eecs.berkeley.edu,fxionghui,zhangwancaibuaag@gmail.com\nAbstract\nMany real-world applications require the prediction of long\nsequence time-series, such as electricity consumption plan-\nning. Long sequence time-series forecasting (LSTF) demands\na high prediction capacity of the model, which is the ability\nto capture precise long-range dependency coupling between\noutput and input efﬁciently. Recent studies have shown the\npotential of Transformer to increase the prediction capacity.\nHowever, there are several severe issues with Transformer\nthat prevent it from being directly applicable to LSTF, includ-\ning quadratic time complexity, high memory usage, and in-\nherent limitation of the encoder-decoder architecture. To ad-\ndress these issues, we design an efﬁcient transformer-based\nmodel for LSTF, named Informer, with three distinctive char-\nacteristics: (i) a ProbSparse self-attention mechanism, which\nachieves O(Llog L) in time complexity and memory usage,\nand has comparable performance on sequences’ dependency\nalignment. (ii) the self-attention distilling highlights dominat-\ning attention by halving cascading layer input, and efﬁciently\nhandles extreme long input sequences. (iii) the generative\nstyle decoder, while conceptually simple, predicts the long\ntime-series sequences at one forward operation rather than\na step-by-step way, which drastically improves the inference\nspeed of long-sequence predictions. Extensive experiments\non four large-scale datasets demonstrate that Informer sig-\nniﬁcantly outperforms existing methods and provides a new\nsolution to the LSTF problem.\nIntroduction\nTime-series forecasting is a critical ingredient across many\ndomains, such as sensor network monitoring (Papadimitriou\nand Yu 2006), energy and smart grid management, eco-\nnomics and ﬁnance (Zhu and Shasha 2002), and disease\npropagation analysis (Matsubara et al. 2014). In these sce-\nnarios, we can leverage a substantial amount of time-series\ndata on past behavior to make a forecast in the long run,\nnamely long sequence time-series forecasting (LSTF). How-\never, existing methods are mostly designed under short-term\nproblem setting, like predicting 48 points or less (Hochreiter\nand Schmidhuber 1997; Li et al. 2018; Yu et al. 2017; Liu\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n2d 4d 6d 8d Time0d\nGround truth\nPrediction\nShort\nLong\n(a) Sequence Forecasting.\n1248 96 192 480\nThe predict sequence length\n2\n4\n6MSE score\n10\n−1\n10\n0\nThe predictions/sec\n(in log scale)\nMSE score\nInference speed (b) Run LSTM on sequences.\nFigure 1: (a) LSTF can cover an extended period than\nthe short sequence predictions, making vital distinction in\npolicy-planning and investment-protecting. (b) The predic-\ntion capacity of existing methods limits LSTF’s perfor-\nmance. E.g., starting from length=48, MSE rises unaccept-\nably high, and the inference speed drops rapidly.\net al. 2019; Qin et al. 2017; Wen et al. 2017). The increas-\ningly long sequences strain the models’ prediction capacity\nto the point where this trend is holding the research on LSTF.\nAs an empirical example, Fig.(1) shows the forecasting re-\nsults on a real dataset, where the LSTM network predicts the\nhourly temperature of an electrical transformer station from\nthe short-term period (12 points, 0.5 days) to the long-term\nperiod (480 points, 20 days). The overall performance gap\nis substantial when the prediction length is greater than 48\npoints (the solid star in Fig.(1b)), where the MSE rises to\nunsatisfactory performance, the inference speed gets sharp\ndrop, and the LSTM model starts to fail.\nThe major challenge for LSTF is to enhance the predic-\ntion capacity to meet the increasingly long sequence de-\nmand, which requires (a) extraordinary long-range align-\nment ability and (b) efﬁcient operations on long sequence in-\nputs and outputs. Recently, Transformer models have shown\nsuperior performance in capturing long-range dependency\nthan RNN models. The self-attention mechanism can re-\nduce the maximum length of network signals traveling paths\ninto the theoretical shortest O(1) and avoid the recurrent\nstructure, whereby Transformer shows great potential for\nthe LSTF problem. Nevertheless, the self-attention mecha-\nnism violates requirement (b) due to its L-quadratic compu-\ntation and memory consumption onL-length inputs/outputs.\nSome large-scale Transformer models pour resources and\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n11106\nyield impressive results on NLP tasks (Brown et al. 2020),\nbut the training on dozens of GPUs and expensive deploying\ncost make theses models unaffordable on real-world LSTF\nproblem. The efﬁciency of the self-attention mechanism and\nTransformer architecture becomes the bottleneck of apply-\ning them to LSTF problems. Thus, in this paper, we seek to\nanswer the question: can we improve Transformer models to\nbe computation, memory, and architecture efﬁcient, as well\nas maintaining higher prediction capacity?\nVanilla Transformer (Vaswani et al. 2017) has three sig-\nniﬁcant limitations when solving the LSTF problem:\n1. The quadratic computation of self-attention.The atom\noperation of self-attention mechanism, namely canonical\ndot-product, causes the time complexity and memory us-\nage per layer to be O(L2).\n2. The memory bottleneck in stacking layers for long in-\nputs. The stack of J encoder/decoder layers makes total\nmemory usage to be O(J \u0001L2), which limits the model\nscalability in receiving long sequence inputs.\n3. The speed plunge in predicting long outputs.Dynamic\ndecoding of vanilla Transformer makes the step-by-step\ninference as slow as RNN-based model (Fig.(1b)).\nThere are some prior works on improving the efﬁciency of\nself-attention. The Sparse Transformer (Child et al. 2019),\nLogSparse Transformer (Li et al. 2019), and Longformer\n(Beltagy, Peters, and Cohan 2020) all use a heuristic method\nto tackle limitation 1 and reduce the complexity of self-\nattention mechanism to O(Llog L), where their efﬁciency\ngain is limited (Qiu et al. 2019). Reformer (Kitaev, Kaiser,\nand Levskaya 2019) also achieves O(Llog L) with locally-\nsensitive hashing self-attention, but it only works on ex-\ntremely long sequences. More recently, Linformer (Wang\net al. 2020) claims a linear complexityO(L), but the project\nmatrix can not be ﬁxed for real-world long sequence in-\nput, which may have the risk of degradation to O(L2).\nTransformer-XL (Dai et al. 2019) and Compressive Trans-\nformer (Rae et al. 2019) use auxiliary hidden states to cap-\nture long-range dependency, which could amplify limitation\n1 and be adverse to break the efﬁciency bottleneck. All these\nworks mainly focus on limitation 1, and the limitation 2&3\nremains unsolved in the LSTF problem. To enhance the pre-\ndiction capacity, we tackle all these limitations and achieve\nimprovement beyond efﬁciency in the proposed Informer.\nTo this end, our work delves explicitly into these three is-\nsues. We investigate the sparsity in the self-attention mecha-\nnism, make improvements of network components, and con-\nduct extensive experiments. The contributions of this paper\nare summarized as follows:\n• We propose Informer to successfully enhance the predic-\ntion capacity in the LSTF problem, which validates the\nTransformer-like model’s potential value to capture in-\ndividual long-range dependency between long sequence\ntime-series outputs and inputs.\n• We propose ProbSparse self-attention mechanism to ef-\nﬁciently replace the canonical self-attention. It achieves\nthe O(Llog L) time complexity andO(Llog L) memory\nusage on dependency alignments.\n• We propose self-attention distilling operation to privi-\nlege dominating attention scores inJ-stacking layers and\nDecoder\nOutputs\nMasked Multi-head\nProbSparse\nSelf-attention\nMulti-head\nAttention\nEncoder\nInputs:    Xen\nConcatenated Feature Map\nInputs:    Xde={Xtoken, X0}\n0 0 0 0 0 0 0\nFully Connected Layer\nMulti-head\nProbSparse\nSelf-attention\nMulti-head\nProbSparse\nSelf-attention\nFigure 2: Informer model overview. Left: The encoder re-\nceives massive long sequence inputs (green series). We re-\nplace canonical self-attention with the proposed ProbSparse\nself-attention. The blue trapezoid is the self-attention distill-\ning operation to extract dominating attention, reducing the\nnetwork size sharply. The layer stacking replicas increase ro-\nbustness. Right: The decoder receives long sequence inputs,\npads the target elements into zero, measures the weighted\nattention composition of the feature map, and instantly pre-\ndicts output elements (orange series) in a generative style.\nsharply reduce the total space complexity to be O((2 \u0000\n\u000f)Llog L), which helps receiving long sequence input.\n• We propose generative style decoder to acquire long se-\nquence output with only one forward step needed, simul-\ntaneously avoiding cumulative error spreading during the\ninference phase.\nPreliminary\nWe ﬁrst provide the LSTF problem deﬁnition. Under the\nrolling forecasting setting with a ﬁxed size window, we have\nthe input Xt = fxt\n1;:::; xt\nLx jxt\ni 2 Rdxgat time t,\nand the output is to predict corresponding sequence Yt =\nfyt\n1;:::; yt\nLy jyt\ni 2Rdy g. The LSTF problem encourages\na longer output’s length Ly than previous works (Cho et al.\n2014; Sutskever, Vinyals, and Le 2014) and the feature di-\nmension is not limited to univariate case (dy \u00151).\nEncoder-decoder architectureMany popular models are\ndevised to “encode” the input representationsXt into a hid-\nden state representations Ht and “decode” an output rep-\nresentations Yt from Ht = fht\n1;:::; ht\nLhg. The inference\ninvolves a step-by-step process named “dynamic decoding”,\nwhere the decoder computes a new hidden state ht\nk+1 from\nthe previous state ht\nk and other necessary outputs from k-th\nstep then predict the (k+ 1)-th sequenceyt\nk+1.\nInput Representation A uniform input representation is\ngiven to enhance the global positional context and local tem-\nporal context of the time-series inputs. To avoid trivializing\ndescription, we put the details in Appendix B.\nMethodology\nExisting methods for time-series forecasting can be roughly\ngrouped into two categories 1. Classical time-series mod-\nels serve as a reliable workhorse for time-series forecast-\n1Related work is in Appendix A due to space limitation.\n11107\ning (Box et al. 2015; Ray 1990; Seeger et al. 2017; Seeger,\nSalinas, and Flunkert 2016), and deep learning techniques\nmainly develop an encoder-decoder prediction paradigm by\nusing RNN and their variants (Hochreiter and Schmidhuber\n1997; Li et al. 2018; Yu et al. 2017). Our proposed Informer\nholds the encoder-decoder architecture while targeting the\nLSTF problem. Please refer to Fig.(2) for an overview and\nthe following sections for details.\nEfﬁcient Self-attention Mechanism\nThe canonical self-attention in (Vaswani et al. 2017) is de-\nﬁned based on the tuple inputs, i.e, query, key and value,\nwhich performs the scaled dot-product as A(Q;K;V) =\nSoftmax(QK>=\np\nd)V, where Q 2RLQ\u0002d, K 2RLK\u0002d,\nV 2RLV \u0002d and dis the input dimension. To further discuss\nthe self-attention mechanism, let qi, ki, vi stand for the i-th\nrow in Q, K, V respectively. Following the formulation in\n(Tsai et al. 2019), the i-th query’s attention is deﬁned as a\nkernel smoother in a probability form:\nA(qi;K;V) =\nX\nj\nk(qi;kj)P\nl k(qi;kl)vj = Ep(kjjqi)[vj] ; (1)\nwhere p(kjjqi) = k(qi;kj)=P\nl k(qi;kl) and k(qi;kj)\nselects the asymmetric exponential kernel exp(qik>\nj =\np\nd).\nThe self-attention combines the values and acquires outputs\nbased on computing the probability p(kjjqi). It requires\nthe quadratic times dot-product computation andO(LQLK)\nmemory usage, which is the major drawback when enhanc-\ning prediction capacity.\nSome previous attempts have revealed that the distribution\nof self-attention probability has potential sparsity, and they\nhave designed “selective” counting strategies on allp(kjjqi)\nwithout signiﬁcantly affecting the performance. The Sparse\nTransformer (Child et al. 2019) incorporates both the row\noutputs and column inputs, in which the sparsity arises\nfrom the separated spatial correlation. The LogSparse Trans-\nformer (Li et al. 2019) notices the cyclical pattern in self-\nattention and forces each cell to attend to its previous one\nby an exponential step size. The Longformer (Beltagy, Pe-\nters, and Cohan 2020) extends previous two works to more\ncomplicated sparse conﬁguration. However, they are limited\nto theoretical analysis from following heuristic methods and\ntackle each multi-head self-attention with the same strategy,\nwhich narrows their further improvement.\nTo motivate our approach, we ﬁrst perform a qualitative\nassessment on the learned attention patterns of the canoni-\ncal self-attention. The “sparsity” self-attention score forms\na long tail distribution (see Appendix C for details), i.e., a\nfew dot-product pairs contribute to the major attention, and\nothers generate trivial attention. Then, the next question is\nhow to distinguish them?\nQuery Sparsity Measurement From Eq.(1), the i-th\nquery’s attention on all the keys are deﬁned as a probabil-\nity p(kjjqi) and the output is its composition with values v.\nThe dominant dot-product pairs encourage the correspond-\ning query’s attention probability distribution away from the\nuniform distribution. If p(kjjqi) is close to a uniform dis-\ntribution q(kjjqi) = 1=LK, the self-attention becomes a\ntrivial sum of values V and is redundant to the residential\ninput. Naturally, the “likeness” between distribution p and\nq can be used to distinguish the “important” queries. We\nmeasure the “likeness” through Kullback-Leibler divergence\nKL(qjjp) = lnPLK\nl=1 eqik>\nl =\np\nd \u0000 1\nLK\nPLK\nj=1 qik>\nj =\np\nd\u0000\nln LK. Dropping the constant, we deﬁne the i-th query’s\nsparsity measurement as\nM(qi;K) = ln\nLKX\nj=1\ne\nqik>\njp\nd \u0000 1\nLK\nLKX\nj=1\nqik>\njp\nd\n; (2)\nwhere the ﬁrst term is the Log-Sum-Exp (LSE) of qi on\nall the keys, and the second term is the arithmetic mean on\nthem. If the i-th query gains a larger M(qi;K), its atten-\ntion probability pis more “diverse” and has a high chance to\ncontain the dominate dot-product pairs in the header ﬁeld of\nthe long tail self-attention distribution.\nProbSparse Self-attention Based on the proposed mea-\nsurement, we have theProbSparse self-attention by allowing\neach key to only attend to the udominant queries:\nA(Q;K;V) =Softmax(QK>\np\nd\n)V ; (3)\nwhere Q is a sparse matrix of the same size of q and it\nonly contains the Top-uqueries under the sparsity measure-\nment M(q;K). Controlled by a constant sampling factor c,\nwe set u = c\u0001ln LQ, which makes the ProbSparse self-\nattention only need to calculate O(ln LQ) dot-product for\neach query-key lookup and the layer memory usage main-\ntains O(LK ln LQ). Under the multi-head perspective, this\nattention generates different sparse query-key pairs for each\nhead, which avoids severe information loss in return.\nHowever, the traversing of all the queries for the measure-\nment M(qi;K) requires calculating each dot-product pairs,\ni.e., quadratically O(LQLK), besides the LSE operation has\nthe potential numerical stability issue. Motivated by this, we\npropose an empirical approximation for the efﬁcient acqui-\nsition of the query sparsity measurement.\nLemma 1. For each query qi 2Rd and kj 2Rd in the\nkeys set K, we have the bound as ln LK \u0014M(qi;K) \u0014\nmaxjfqik>\nj =\np\ndg\u0000 1\nLK\nPLK\nj=1fqik>\nj =\np\ndg+ lnLK. When\nqi 2K, it also holds.\nFrom the Lemma 1 (proof is given in Appendix D.1), we\npropose the max-mean measurement as\nM(qi;K) = max\nj\nfqik>\nj\np\nd\ng\u0000 1\nLK\nLKX\nj=1\nqik>\nj\np\nd\n: (4)\nThe range of Top-u approximately holds in the bound-\nary relaxation with Proposition 1 (refers in Appendix D.2).\nUnder the long tail distribution, we only need to randomly\nsample U = LK ln LQ dot-product pairs to calculate the\nM(qi;K), i.e., ﬁlling other pairs with zero. Then, we se-\nlect sparse Top-u from them as Q. The max-operator in\nM(qi;K) is less sensitive to zero values and is numeri-\ncal stable. In practice, the input length of queries and keys\nare typically equivalent in the self-attention computation, i.e\nLQ = LK = Lsuch that the total ProbSparse self-attention\ntime complexity and space complexity are O(Lln L).\n11108\nScalar\nStamp\nT = t\nT = t + Dx\nL\nd\nConv1d\nL\nd\nEmbedding\n+\nL\nk\nL\nn-heads\nAttention Block 1\nConv1d\nMaxP\nool1d,\npadding=2\nL/2\nk\nL/2\nn-heads\nAttention Block 2\nConv1d\nMaxP\nool1d,\npadding=2\nL/4\nk L/4\nn-heads\nAttention Block 3\nL/4\nd\nFeature\nMap\nFigure 3: The single stack in Informer’s encoder. (1) The horizontal stack stands for an individual one of the encoder replicas\nin Fig.(2). (2) The presented one is the main stack receiving the whole input sequence. Then the second stack takes half slices\nof the input, and the subsequent stacks repeat. (3) The red layers are dot-product matrixes, and they get cascade decrease by\napplying self-attention distilling on each layer. (4) Concatenate all stacks’ feature maps as the encoder’s output.\nEncoder: Allowing for Processing Longer\nSequential Inputs under the Memory Usage\nLimitation\nThe encoder is designed to extract the robust long-range de-\npendency of the long sequential inputs. After the input rep-\nresentation, the t-th sequence input Xt has been shaped into\na matrix Xt\nen 2RLx\u0002dmodel . We give a sketch of the encoder\nin Fig.(3) for clarity.\nSelf-attention Distilling As the natural consequence of\nthe ProbSparse self-attention mechanism, the encoder’s fea-\nture map has redundant combinations of value V. We use\nthe distilling operation to privilege the superior ones with\ndominating features and make a focused self-attention fea-\nture map in the next layer. It trims the input’s time dimension\nsharply, seeing the n-heads weights matrix (overlapping red\nsquares) of Attention blocks in Fig.(3). Inspired by the di-\nlated convolution (Yu, Koltun, and Funkhouser 2017; Gupta\nand Rush 2017), our “distilling” procedure forwards from\nj-th layer into (j+ 1)-th layer as:\nXt\nj+1 = MaxPool\n\u0000\nELU( Conv1d([Xt\nj]AB) )\n\u0001\n; (5)\nwhere [\u0001]AB represents the attention block. It contains the\nMulti-head ProbSparse self-attention and the essential op-\nerations, where Conv1d(\u0001) performs an 1-D convolutional\nﬁlters (kernel width=3) on time dimension with the ELU(\u0001)\nactivation function (Clevert, Unterthiner, and Hochreiter\n2016). We add a max-pooling layer with stride 2 and down-\nsample Xt into its half slice after stacking a layer, which\nreduces the whole memory usage to be O((2 \u0000\u000f)Llog L),\nwhere \u000f is a small number. To enhance the robustness of\nthe distilling operation, we build replicas of the main stack\nwith halving inputs, and progressively decrease the number\nof self-attention distilling layers by dropping one layer at a\ntime, like a pyramid in Fig.(2), such that their output dimen-\nsion is aligned. Thus, we concatenate all the stacks’ outputs\nand have the ﬁnal hidden representation of encoder.\nDecoder: Generating Long Sequential Outputs\nThrough One Forward Procedure\nWe use a standard decoder structure (Vaswani et al. 2017) in\nFig.(2), and it is composed of a stack of two identical multi-\nhead attention layers. However, the generative inference is\nemployed to alleviate the speed plunge in long prediction.\nWe feed the decoder with the following vectors as\nXt\nde = Concat(Xt\ntoken;Xt\n0) 2R(Ltoken+Ly)\u0002dmodel ; (6)\nwhere Xt\ntoken 2 RLtoken\u0002dmodel is the start token, Xt\n0 2\nRLy\u0002dmodel is a placeholder for the target sequence (set\nscalar as 0). Masked multi-head attention is applied in the\nProbSparse self-attention computing by setting masked dot-\nproducts to \u00001. It prevents each position from attending\nto coming positions, which avoids auto-regressive. A fully\nconnected layer acquires the ﬁnal output, and its outsize dy\ndepends on whether we are performing a univariate forecast-\ning or a multivariate one.\nGenerative Inference Start token is efﬁciently applied in\nNLP’s “dynamic decoding” (Devlin et al. 2018), and we ex-\ntend it into a generative way. Instead of choosing speciﬁc\nﬂags as the token, we sample a Ltoken long sequence in the\ninput sequence, such as an earlier slice before the output se-\nquence. Take predicting 168 points as an example (7-day\ntemperature prediction in the experiment section), we will\ntake the known 5 days before the target sequence as “start-\ntoken”, and feed the generative-style inference decoder with\nXde = fX5d;X0g. The X0 contains target sequence’s time\nstamp, i.e., the context at the target week. Then our proposed\ndecoder predicts outputs by one forward procedure rather\nthan the time consuming “dynamic decoding” in the conven-\ntional encoder-decoder architecture. A detailed performance\ncomparison is given in the computation efﬁciency section.\nLoss function We choose the MSE loss function on pre-\ndiction w.r.t the target sequences, and the loss is propagated\nback from the decoder’s outputs across the entire model.\n11109\nMethods Informer Informery LogTrans Reformer LSTMa DeepAR ARIMA Prophet\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1\n24 0.098 0.247 0.092 0.246 0.103 0.259 0.222 0.389 0.114 0.272 0.107 0.280 0.108 0.284 0.115 0.275\n48 0.158 0.319 0.161 0.322 0.167 0.328 0.284 0.445 0.193 0.358 0.162 0.327 0.175 0.424 0.168 0.330\n168 0.183 0.346 0.187 0.355 0.207 0.375 1.522 1.191 0.236 0.392 0.239 0.422 0.396 0.504 1.224 0.763\n336 0.222 0.387 0.215 0.369 0.230 0.398 1.860 1.124 0.590 0.698 0.445 0.552 0.468 0.593 1.549 1.820\n720 0.269 0.435 0.257 0.421 0.273 0.463 2.112 1.436 0.683 0.768 0.658 0.707 0.659 0.766 2.735 3.253\nETTh2\n24 0.093 0.240 0.099 0.241 0.102 0.255 0.263 0.437 0.155 0.307 0.098 0.263 3.554 0.445 0.199 0.381\n48 0.155 0.314 0.159 0.317 0.169 0.348 0.458 0.545 0.190 0.348 0.163 0.341 3.190 0.474 0.304 0.462\n168 0.232 0.389 0.235 0.390 0.246 0.422 1.029 0.879 0.385 0.514 0.255 0.414 2.800 0.595 2.145 1.068\n336 0.263 0.417 0.258 0.423 0.267 0.437 1.668 1.228 0.558 0.606 0.604 0.607 2.753 0.738 2.096 2.543\n720 0.277 0.431 0.285 0.442 0.303 0.493 2.030 1.721 0.640 0.681 0.429 0.580 2.878 1.044 3.355 4.664\nETTm1\n24 0.030 0.137 0.034 0.160 0.065 0.202 0.095 0.228 0.121 0.233 0.091 0.243 0.090 0.206 0.120 0.290\n48 0.069 0.203 0.066 0.194 0.078 0.220 0.249 0.390 0.305 0.411 0.219 0.362 0.179 0.306 0.133 0.305\n96 0.194 0.372 0.187 0.384 0.199 0.386 0.920 0.767 0.287 0.420 0.364 0.496 0.272 0.399 0.194 0.396\n288 0.401 0.554 0.409 0.548 0.411 0.572 1.108 1.245 0.524 0.584 0.948 0.795 0.462 0.558 0.452 0.574\n672 0.512 0.644 0.519 0.665 0.598 0.702 1.793 1.528 1.064 0.873 2.437 1.352 0.639 0.697 2.747 1.174\nWeather\n24 0.117 0.251 0.119 0.256 0.136 0.279 0.231 0.401 0.131 0.254 0.128 0.274 0.219 0.355 0.302 0.433\n48 0.178 0.318 0.185 0.316 0.206 0.356 0.328 0.423 0.190 0.334 0.203 0.353 0.273 0.409 0.445 0.536\n168 0.266 0.398 0.269 0.404 0.309 0.439 0.654 0.634 0.341 0.448 0.293 0.451 0.503 0.599 2.441 1.142\n336 0.297 0.416 0.310 0.422 0.359 0.484 1.792 1.093 0.456 0.554 0.585 0.644 0.728 0.730 1.987 2.468\n720 0.359 0.466 0.361 0.471 0.388 0.499 2.087 1.534 0.866 0.809 0.499 0.596 1.062 0.943 3.859 1.144\nECL\n48 0.239 0.359 0.238 0.368 0.280 0.429 0.971 0.884 0.493 0.539 0.204 0.357 0.879 0.764 0.524 0.595\n168 0.447 0.503 0.442 0.514 0.454 0.529 1.671 1.587 0.723 0.655 0.315 0.436 1.032 0.833 2.725 1.273\n336 0.489 0.528 0.501 0.552 0.514 0.563 3.528 2.196 1.212 0.898 0.414 0.519 1.136 0.876 2.246 3.077\n720 0.540 0.571 0.543 0.578 0.558 0.609 4.891 4.047 1.511 0.966 0.563 0.595 1.251 0.933 4.243 1.415\n960 0.582 0.608 0.594 0.638 0.624 0.645 7.019 5.105 1.545 1.006 0.657 0.683 1.370 0.982 6.901 4.264\nCount 32 12 0 0 0 6 0 0\nTable 1: Univariate long sequence time-series forecasting results on four datasets (ﬁve cases).\nExperiment\nDatasets\nWe extensively perform experiments on four datasets, in-\ncluding 2 collected real-world datasets for LSTF and 2 pub-\nlic benchmark datasets.\nETT (Electricity Transformer Temperature)2: The ETT is\na crucial indicator in the electric power long-term deploy-\nment. We collected 2-year data from two separated counties\nin China. To explore the granularity on the LSTF problem,\nwe create separate datasets as fETTh1, ETTh2gfor 1-hour-\nlevel and ETTm1 for 15-minute-level. Each data point con-\nsists of the target value ”oil temperature” and 6 power load\nfeatures. The train/val/test is 12/4/4 months.\nECL (Electricity Consuming Load)3: It collects the elec-\ntricity consumption (Kwh) of 321 clients. Due to the missing\ndata (Li et al. 2019), we convert the dataset into hourly con-\nsumption of 2 years and set ‘MT\n320’ as the target value.\nThe train/val/test is 15/3/4 months.\nWeather 4: This dataset contains local climatological data\nfor nearly 1,600 U.S. locations, 4 years from 2010 to 2013,\nwhere data points are collected every 1 hour. Each data point\n2We collected the ETT dataset and published it at https://\ngithub.com/zhouhaoyi/ETDataset.\n3ECL dataset was acquired at https://archive.ics.uci.edu/ml/\ndatasets/ElectricityLoadDiagrams20112014.\n4Weather dataset was acquired at https://www.ncei.noaa.gov/\ndata/local-climatological-data/.\nconsists of the target value “wet bulb” and 11 climate fea-\ntures. The train/val/test is 28/10/10 months.\nExperimental Details\nWe brieﬂy summarize basics, and more information on net-\nwork components and setups are given in Appendix E.\nBaselines: We have selected ﬁve time-series forecast-\ning methods as comparison, including ARIMA (Ariyo,\nAdewumi, and Ayo 2014), Prophet (Taylor and Letham\n2018), LSTMa (Bahdanau, Cho, and Bengio 2015), LST-\nnet (Lai et al. 2018) and DeepAR (Flunkert, Salinas, and\nGasthaus 2017). To better explore the ProbSparse self-\nattention’s performance in our proposed Informer, we in-\ncorporate the canonical self-attention variant (Informer y),\nthe efﬁcient variant Reformer (Kitaev, Kaiser, and Levskaya\n2019) and the most related work LogSparse self-attention\n(Li et al. 2019) in the experiments. The details of network\ncomponents are given in Appendix E.1.\nHyper-parameter tuning: We conduct grid search over\nthe hyper-parameters, and detailed ranges are given in Ap-\npendix E.3. Informer contains a 3-layer stack and a 1-layer\nstack (1/4 input) in the encoder, and a 2-layer decoder. Our\nproposed methods are optimized with Adam optimizer, and\nits learning rate starts from1e\u00004, decaying 0.5 times smaller\nevery epoch. The total number of epochs is 8 with proper\nearly stopping. We set the comparison methods as recom-\nmended, and the batch size is 32. Setup: The input of each\ndataset is zero-mean normalized. Under the LSTF settings,\n11110\nMethods Informer Informery LogTrans Reformer LSTMa LSTnet\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1\n24 0.577 0.549 0.620 0.577 0.686 0.604 0.991 0.754 0.650 0.624 1.293 0.901\n48 0.685 0.625 0.692 0.671 0.766 0.757 1.313 0.906 0.702 0.675 1.456 0.960\n168 0.931 0.752 0.947 0.797 1.002 0.846 1.824 1.138 1.212 0.867 1.997 1.214\n336 1.128 0.873 1.094 0.813 1.362 0.952 2.117 1.280 1.424 0.994 2.655 1.369\n720 1.215 0.896 1.241 0.917 1.397 1.291 2.415 1.520 1.960 1.322 2.143 1.380\nETTh2\n24 0.720 0.665 0.753 0.727 0.828 0.750 1.531 1.613 1.143 0.813 2.742 1.457\n48 1.457 1.001 1.461 1.077 1.806 1.034 1.871 1.735 1.671 1.221 3.567 1.687\n168 3.489 1.515 3.485 1.612 4.070 1.681 4.660 1.846 4.117 1.674 3.242 2.513\n336 2.723 1.340 2.626 1.285 3.875 1.763 4.028 1.688 3.434 1.549 2.544 2.591\n720 3.467 1.473 3.548 1.495 3.913 1.552 5.381 2.015 3.963 1.788 4.625 3.709\nETTm1\n24 0.323 0.369 0.306 0.371 0.419 0.412 0.724 0.607 0.621 0.629 1.968 1.170\n48 0.494 0.503 0.465 0.470 0.507 0.583 1.098 0.777 1.392 0.939 1.999 1.215\n96 0.678 0.614 0.681 0.612 0.768 0.792 1.433 0.945 1.339 0.913 2.762 1.542\n288 1.056 0.786 1.162 0.879 1.462 1.320 1.820 1.094 1.740 1.124 1.257 2.076\n672 1.192 0.926 1.231 1.103 1.669 1.461 2.187 1.232 2.736 1.555 1.917 2.941\nWeather\n24 0.335 0.381 0.349 0.397 0.435 0.477 0.655 0.583 0.546 0.570 0.615 0.545\n48 0.395 0.459 0.386 0.433 0.426 0.495 0.729 0.666 0.829 0.677 0.660 0.589\n168 0.608 0.567 0.613 0.582 0.727 0.671 1.318 0.855 1.038 0.835 0.748 0.647\n336 0.702 0.620 0.707 0.634 0.754 0.670 1.930 1.167 1.657 1.059 0.782 0.683\n720 0.831 0.731 0.834 0.741 0.885 0.773 2.726 1.575 1.536 1.109 0.851 0.757\nECL\n48 0.344 0.393 0.334 0.399 0.355 0.418 1.404 0.999 0.486 0.572 0.369 0.445\n168 0.368 0.424 0.353 0.420 0.368 0.432 1.515 1.069 0.574 0.602 0.394 0.476\n336 0.381 0.431 0.381 0.439 0.373 0.439 1.601 1.104 0.886 0.795 0.419 0.477\n720 0.406 0.443 0.391 0.438 0.409 0.454 2.009 1.170 1.676 1.095 0.556 0.565\n960 0.460 0.548 0.492 0.550 0.477 0.589 2.141 1.387 1.591 1.128 0.605 0.599\nCount 33 14 1 0 0 2\nTable 2: Multivariate long sequence time-series forecasting results on four datasets (ﬁve cases).\nwe prolong the prediction windows size Ly progressively,\ni.e., f1d, 2d, 7d, 14d, 30d, 40dgin fETTh, ECL, Weatherg,\nf6h, 12h, 24h, 72h, 168hg in ETTm. Metrics: We use two\nevaluation metrics, including MSE = 1\nn\nPn\ni=1(y \u0000^y)2 and\nMAE = 1\nn\nPn\ni=1 jy \u0000^yjon each prediction window (aver-\naging for multivariate prediction), and roll the whole set with\nstride = 1.Platform: All the models were trained/tested on\na single Nvidia V100 32GB GPU. The source code is avail-\nable at https://github.com/zhouhaoyi/Informer2020.\nResults and Analysis\nTable 1 and Table 2 summarize the univariate/multivariate\nevaluation results of all the methods on 4 datasets. We grad-\nually prolong the prediction horizon as a higher requirement\nof prediction capacity, where the LSTF problem setting is\nprecisely controlled to be tractable on one single GPU for\neach method. The best results are highlighted in boldface.\nUnivariate Time-series Forecasting Under this setting,\neach method attains predictions as a single variable over\ntime series. From Table 1, we can observe that:(1) The pro-\nposed model Informer signiﬁcantly improves the inference\nperformance (wining-counts in the last column) across all\ndatasets, and their predict error rises smoothly and slowly\nwithin the growing prediction horizon, which demonstrates\nthe success of Informer in enhancing the prediction capacity\nin the LSTF problem. (2) The Informer beats its canonical\ndegradation Informerymostly in wining-counts, i.e., 32>12,\nwhich supports the query sparsity assumption in providing\na comparable attention feature map. Our proposed method\nalso out-performs the most related work LogTrans and Re-\nformer. We note that the Reformer keeps dynamic decoding\nand performs poorly in LSTF, while other methods beneﬁt\nfrom the generative style decoder as nonautoregressive pre-\ndictors. (3) The Informer model shows signiﬁcantly better\nresults than recurrent neural networks LSTMa. Our method\nhas a MSE decrease of 26.8% (at 168), 52.4% (at 336) and\n60.1% (at 720). This reveals a shorter network path in the\nself-attention mechanism acquires better prediction capac-\nity than the RNN-based models. (4) The proposed method\noutperforms DeepAR, ARIMA and Prophet on MSE by de-\ncreasing 49.3% (at 168), 61.1% (at 336), and 65.1% (at 720)\nin average. On the ECL dataset, DeepAR performs better\non shorter horizons (\u0014 336), and our method surpasses on\nlonger horizons. We attribute this to a speciﬁc example, in\nwhich the effectiveness of prediction capacity is reﬂected\nwith the problem scalability.\nMultivariate Time-series Forecasting Within this set-\nting, some univariate methods are inappropriate, and LSTnet\nis the state-of-art baseline. On the contrary, our proposed In-\nformer is easy to change from univariate prediction to mul-\ntivariate one by adjusting the ﬁnal FCN layer. From Table 2,\nwe observe that: (1) The proposed model Informer greatly\noutperforms other methods and the ﬁndings 1 & 2 in the uni-\nvariate settings still hold for the multivariate time-series.(2)\nThe Informer model shows better results than RNN-based\nLSTMa and CNN-based LSTnet, and the MSE decreases\n11111\n48 96 168 240 336 480 624 720\nProlong Input Length (Lx, Ltoken)\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nMSE score\nEncoder Input (horizon=48)\nDecoder Token (horizon=48)\nEncoder Input (horizon=168)\nDecoder Token (horizon=168)\n(a) Input length.\n48 96 168 240 480 624 720\nEncoder Input Length (Lx)\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nMSE score\nInformer, factor c=3\nInformer, factor c=5\nInformer, factor c=8\nInformer, factor c=10 (b) Sampling Factor.\n96 168 240 336 480 720\nEncoder Input Length (Lx)\n0.05\n0.10\n0.15\n0.20\n0.25MSE score\nL-scale Dependency\nL/2-scale Dependency\nL/4-scale Dependency\nInformer Dependency (c) Stacking Combination.\nFigure 4: The parameter sensitivity of three components in Informer.\nPrediction length 336 720\nEncoder’s input 336 720 1440 720 1440 2880\nInformer MSE 0.249 0.225 0.216 0.271 0.261 0.257\nMAE 0.393 0.384 0.376 0.435 0.431 0.422\nInformery MSE 0.241 0.214 - 0.259 - -\nMAE 0.383 0.371 - 0.423 - -\nLogTrans MSE 0.263 0.231 - 0.273 - -\nMAE 0.418 0.398 - 0.463 - -\nReformer MSE 1.875 1.865 1.861 2.243 2.174 2.113\nMAE 1.144 1.129 1.125 1.536 1.497 1.434\n1 Informery uses the canonical self-attention mechanism.\n2 The ‘-’ indicates failure for the out-of-memory.\nTable 3: Ablation study of theProbSparse self-attention mechanism.\nMethods\nTraining Testing\nTime Memory Steps\nInformer O(Llog L) O(Llog L) 1\nTransformer O(L2) O(L2) L\nLogTrans O(Llog L) O(L2) 1?\nReformer O(Llog L) O(Llog L) L\nLSTM O(L) O(L) L\n1 The LSTnet is hard to present in a closed form.\n2 The ? denotes applying our proposed decoder.\nTable 4: L-related computation statics of each layer.\n26.6% (at 168), 28.2% (at 336), 34.3% (at 720) in average.\nCompared with the univariate results, the overwhelming per-\nformance is reduced, and such phenomena can be caused by\nthe anisotropy of feature dimensions’ prediction capacity. It\nis beyond the scope of this paper, and we will explore it in\nthe future work.\nLSTF with Granularity Consideration We perform an\nadditional comparison to explore the performance with var-\nious granularities. The sequences f96, 288, 672gof ETTm1\n(minutes-level) are aligned with f24, 48, 168g of ETTh 1\n(hour-level). The Informer outperforms other baselines even\nif the sequences are at different granularity levels.\nParameter Sensitivity\nWe perform the sensitivity analysis of the proposed In-\nformer model on ETTh1 under the univariate setting. Input\nLength: In Fig.(4a), when predicting short sequences (like\n48), initially increasing input length of encoder/decoder de-\ngrades performance, but further increasing causes the MSE\nto drop because it brings repeat short-term patterns. How-\never, the MSE gets lower with longer inputs in predict-\ning long sequences (like 168). Because the longer encoder\ninput may contain more dependencies, and the longer de-\ncoder token has rich local information. Sampling Factor:\nThe sampling factor controls the information bandwidth of\nProbSparse self-attention in Eq.(3). We start from the small\nfactor (=3) to large ones, and the general performance in-\ncreases a little and stabilizes at last in Fig.(4b). It veriﬁes\nour query sparsity assumption that there are redundant dot-\nproduct pairs in the self-attention mechanism. We set the\nsample factor c = 5(the red line) in practice. The Combi-\nnation of Layer Stacking: The replica of Layers is comple-\nmentary for the self-attention distilling, and we investigate\neach stack fL, L/2, L/4g’s behavior in Fig.(4c). The longer\nstack is more sensitive to the inputs, partly due to receiving\nmore long-term information. Our method’s selection (the red\nline), i.e., joining L and L/4, is the most robust strategy.\nAblation Study: How well Informer works?\nWe also conducted additional experiments on ETTh 1 with\nablation consideration.\nThe performance of ProbSparse self-attention mech-\nanism In the overall results Table 1 & 2, we limited the\nproblem setting to make the memory usage feasible for the\ncanonical self-attention. In this study, we compare our meth-\nods with LogTrans and Reformer, and thoroughly explore\ntheir extreme performance. To isolate the memory efﬁcient\nproblem, we ﬁrst reduce settings as fbatch size=8, heads=8,\ndim=64g, and maintain other setups in the univariate case.\nIn Table 3, the ProbSparse self-attention shows better per-\nformance than the counterparts. The LogTrans gets OOM\nin extreme cases because its public implementation is the\n11112\nPrediction length 336 480\nEncoder’s input 336 480 720 960 1200 336 480 720 960 1200\nInformery MSE 0.249 0.208 0.225 0.199 0.186 0.197 0.243 0.213 0.192 0.174\nMAE 0.393 0.385 0.384 0.371 0.365 0.388 0.392 0.383 0.377 0.362\nInformerz MSE 0.229 0.215 0.204 - - 0.224 0.208 0.197 - -\nMAE 0.391 0.387 0.377 - - 0.381 0.376 0.370 - -\n1 Informerz removes the self-attention distilling from Informery.\n2 The ‘-’ indicates failure for the out-of-memory.\nTable 5: Ablation study of the self-attention distilling.\nPrediction length 336 480\nPrediction offset +0 +12 +24 +48 +72 +0 +48 +96 +144 +168\nInformerz MSE 0.207 0.209 0.211 0.211 0.216 0.198 0.203 0.203 0.208 0.208\nMAE 0.385 0.387 0.391 0.393 0.397 0.390 0.392 0.393 0.401 0.403\nInformerx MSE 0.201 - - - - 0.392 - - - -\nMAE 0.393 - - - - 0.484 - - - -\n1 Informerx replaces our decoder with dynamic decoding one in Informerz.\n2 The ‘-’ indicates failure for the unacceptable metric results.\nTable 6: Ablation study of the generative style decoder.\n48 96 168 336 720\nEncoder Input length (Lx)\n1\n2\n3Train time (day)\nLSTnet\nLSTM\nInformer\nInformer†\nLogTrans\nReformer\n48 96 168 336 720\nDecoder predict length (Ly)\n2\n4\n6\n8Inference time (day)\nLSTnet\nLSTM\nInformer\nInformer†\nInformer§\nLogTrans\nReformer\nFigure 5: The total runtime of training/testing phase.\nmask of the full-attention, which still has O(L2) memory\nusage. Our proposed ProbSparse self-attention avoids this\nfrom the simplicity brought by the query sparsity assump-\ntion in Eq.(4), referring to the pseudo-code in Appendix E.2,\nand reaches smaller memory usage.\nThe performance of self-attention distilling In this\nstudy, we use Informer y as the benchmark to eliminate\nadditional effects of ProbSparse self-attention. The other\nexperimental setup is aligned with the settings of uni-\nvariate Time-series. From Table 5, Informer y has fulﬁlled\nall the experiments and achieves better performance after\ntaking advantage of long sequence inputs. The compari-\nson method Informer zremoves the distilling operation and\nreaches OOM with longer inputs (> 720). Regarding the\nbeneﬁts of long sequence inputs in the LSTF problem, we\nconclude that the self-attention distilling is worth adopting,\nespecially when a longer prediction is required.\nThe performance of generative style decoder In this\nstudy, we testify the potential value of our decoder in acquir-\ning a “generative” results. Unlike the existing methods, the\nlabels and outputs are forced to be aligned in the training and\ninference, our proposed decoder’s predicting relies solely on\nthe time stamp, which can predict with offsets. From Ta-\nble 6, we can see that the general prediction performance\nof Informer z resists with the offset increasing, while the\ncounterpart fails for the dynamic decoding. It proves the de-\ncoder’s ability to capture individual long-range dependency\nbetween arbitrary outputs and avoid error accumulation.\nComputation Efﬁciency\nWith the multivariate setting and all the methods’ cur-\nrent ﬁnest implement, we perform a rigorous runtime\ncomparison in Fig.(5). During the training phase, the In-\nformer (red line) achieves the best training efﬁciency among\nTransformer-based methods. During the testing phase, our\nmethods are much faster than others with the generative\nstyle decoding. The comparisons of theoretical time com-\nplexity and memory usage are summarized in Table 4. The\nperformance of Informer is aligned with the runtime experi-\nments. Note that the LogTrans focus on improving the self-\nattention mechanism, and we apply our proposed decoder in\nLogTrans for a fair comparison (the ?in Table 4).\nConclusion\nIn this paper, we studied the long-sequence time-series fore-\ncasting problem and proposed Informer to predict long se-\nquences. Speciﬁcally, we designed the ProbSparse self-\nattention mechanism and distilling operation to handle the\nchallenges of quadratic time complexity and quadratic mem-\nory usage in vanilla Transformer. Also, the carefully de-\nsigned generative decoder alleviates the limitation of tra-\nditional encoder-decoder architecture. The experiments on\nreal-world data demonstrated the effectiveness of Informer\nfor enhancing the prediction capacity in LSTF problem.\n11113\nAcknowledgments\nThis work was supported by grants from the Natural Science\nFoundation of China (U20B2053, 61872022 and 61421003)\nand State Key Laboratory of Software Development Envi-\nronment (SKLSDE-2020ZX-12). Thanks for computing in-\nfrastructure provided by Beijing Advanced Innovation Cen-\nter for Big Data and Brain Computing. This work was also\nsponsored by CAAI-Huawei MindSpore Open Fund. The\ncorresponding author is Jianxin Li.\nEthics Statement\nThe proposed Informer can process long inputs and make\nefﬁcient long sequence inference, which can be applied\nto the challenging long sequence times series forecasting\n(LSTF) problem. The signiﬁcant real-world applications in-\nclude sensor network monitoring (Papadimitriou and Yu\n2006), energy and smart grid management, disease propa-\ngation analysis (Matsubara et al. 2014), economics and ﬁ-\nnance forecasting (Zhu and Shasha 2002), evolution of agri-\necosystems, climate change forecasting, and variations in air\npollution. As a speciﬁc example, online sellers can predict\nthe monthly product supply, which helps to optimize long-\nterm inventory management. The distinct difference from\nother time series problems is its requirement on a high de-\ngree of prediction capacity. Our contributions are not lim-\nited to the LSTF problem. In addition to acquiring long se-\nquences, our method can bring substantial beneﬁts to other\ndomains, such as long sequence generation of text, music,\nimage, and video.\nUnder the ethical considerations, any time-series forecast-\ning application that learns from the history data runs the\nrisk of producing biased predictions. It may cause irrepara-\nble losses to the real owners of the property/asset. Domain\nexperts should guide the usage of our methods, while the\nlong sequence forecasting can also beneﬁt the work of the\ndomain experts. Taking applying our methods to electrical\ntransformer temperature prediction as an example, the man-\nager will examine the results and decide the future power\ndeployment. If a long enough prediction is available, it will\nbe helpful for the manager to prevent irreversible failure in\nthe early stage. In addition to identifying the bias data, one\npromising method is to adopt transfer learning. We have do-\nnated the collected data (ETT dataset) for further research\non related topics, such as water supply management and 5G\nnetwork deployment. Another drawback is that our method\nrequires high-performance GPU, which limits its application\nin the underdevelopment regions.\nReferences\nAriyo, A. A.; Adewumi, A. O.; and Ayo, C. K. 2014. Stock\nprice prediction using the ARIMA model. In The 16th In-\nternational Conference on Computer Modelling and Simu-\nlation, 106–112. IEEE.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural Ma-\nchine Translation by Jointly Learning to Align and Trans-\nlate. In ICLR 2015.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer:\nThe Long-Document Transformer. CoRR abs/2004.05150.\nBox, G. E.; Jenkins, G. M.; Reinsel, G. C.; and Ljung, G. M.\n2015. Time series analysis: forecasting and control. John\nWiley & Sons.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. CoRR abs/2005.14165.\nChild, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.\nGenerating Long Sequences with Sparse Transformers.\narXiv:1904.10509 .\nCho, K.; van Merrienboer, B.; Bahdanau, D.; and Bengio,\nY . 2014. On the Properties of Neural Machine Trans-\nlation: Encoder-Decoder Approaches. In Proceedings of\nSSST@EMNLP 2014, 103–111.\nClevert, D.; Unterthiner, T.; and Hochreiter, S. 2016. Fast\nand Accurate Deep Network Learning by Exponential Lin-\near Units (ELUs). In ICLR 2016.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q. V .; and\nSalakhutdinov, R. 2019. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. arXiv:1901.02860 .\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv:1810.04805 .\nFlunkert, V .; Salinas, D.; and Gasthaus, J. 2017. DeepAR:\nProbabilistic forecasting with autoregressive recurrent net-\nworks. arXiv:1704.04110 .\nGupta, A.; and Rush, A. M. 2017. Dilated convolu-\ntions for modeling long-distance genomic dependencies.\narXiv:1710.01278 .\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation 9(8): 1735–1780.\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer:\nThe Efﬁcient Transformer. In ICLR.\nLai, G.; Chang, W.-C.; Yang, Y .; and Liu, H. 2018. Model-\ning long-and short-term temporal patterns with deep neural\nnetworks. In ACM SIGIR 2018, 95–104. ACM.\nLi, S.; Jin, X.; Xuan, Y .; Zhou, X.; Chen, W.; Wang, Y .-X.;\nand Yan, X. 2019. Enhancing the Locality and Breaking the\nMemory Bottleneck of Transformer on Time Series Fore-\ncasting. arXiv:1907.00235 .\nLi, Y .; Yu, R.; Shahabi, C.; and Liu, Y . 2018. Diffusion Con-\nvolutional Recurrent Neural Network: Data-Driven Trafﬁc\nForecasting. In ICLR 2018.\nLiu, Y .; Gong, C.; Yang, L.; and Chen, Y . 2019. DSTP-RNN:\na dual-stage two-phase attention-based recurrent neural net-\nworks for long-term and multivariate time series prediction.\nCoRR abs/1904.07464.\nMatsubara, Y .; Sakurai, Y .; van Panhuis, W. G.; and Falout-\nsos, C. 2014. FUNNEL: automatic mining of spatially coe-\nvolving epidemics. In ACM SIGKDD 2014, 105–114.\n11114\nPapadimitriou, S.; and Yu, P. 2006. Optimal multi-scale pat-\nterns in time series streams. In ACM SIGMOD 2006, 647–\n658. ACM.\nQin, Y .; Song, D.; Chen, H.; Cheng, W.; Jiang, G.; and Cot-\ntrell, G. W. 2017. A Dual-Stage Attention-Based Recurrent\nNeural Network for Time Series Prediction. In IJCAI 2017,\n2627–2633.\nQiu, J.; Ma, H.; Levy, O.; Yih, S. W.-t.; Wang, S.; and Tang,\nJ. 2019. Blockwise Self-Attention for Long Document Un-\nderstanding. arXiv:1911.02972 .\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap,\nT. P. 2019. Compressive transformers for long-range se-\nquence modelling. arXiv:1911.05507 .\nRay, W. 1990. Time series: theory and methods. Journal of\nthe Royal Statistical Society: Series A (Statistics in Society)\n153(3): 400–400.\nSeeger, M.; Rangapuram, S.; Wang, Y .; Salinas, D.;\nGasthaus, J.; Januschowski, T.; and Flunkert, V . 2017.\nApproximate bayesian inference in linear state space\nmodels for intermittent demand forecasting at scale.\narXiv:1709.07638 .\nSeeger, M. W.; Salinas, D.; and Flunkert, V . 2016. Bayesian\nintermittent demand forecasting for large inventories. In\nNIPS, 4646–4654.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. In NIPS, 3104–\n3112.\nTaylor, S. J.; and Letham, B. 2018. Forecasting at scale.The\nAmerican Statistician 72(1): 37–45.\nTsai, Y .-H. H.; Bai, S.; Yamada, M.; Morency, L.-P.; and\nSalakhutdinov, R. 2019. Transformer Dissection: An Uni-\nﬁed Understanding for Transformer’s Attention via the Lens\nof Kernel. In ACL 2019, 4335–4344.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS, 5998–6008.\nWang, S.; Li, B.; Khabsa, M.; Fang, H.; and Ma, H.\n2020. Linformer: Self-Attention with Linear Complexity.\narXiv:2006.04768 .\nWen, R.; Torkkola, K.; Narayanaswamy, B.; and Madeka,\nD. 2017. A multi-horizon quantile recurrent forecaster.\narXiv:1711.11053 .\nYu, F.; Koltun, V .; and Funkhouser, T. 2017. Dilated residual\nnetworks. In CVPR, 472–480.\nYu, R.; Zheng, S.; Anandkumar, A.; and Yue, Y . 2017. Long-\nterm forecasting using tensor-train rnns. arXiv:1711.00073\n.\nZhu, Y .; and Shasha, D. E. 2002. StatStream: Statistical\nMonitoring of Thousands of Data Streams in Real Time. In\nVLDB 2002, 358–369.\n11115",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6814442276954651
    },
    {
      "name": "Computer science",
      "score": 0.6107996702194214
    },
    {
      "name": "Encoder",
      "score": 0.47053608298301697
    },
    {
      "name": "Sequence (biology)",
      "score": 0.43886998295783997
    },
    {
      "name": "Dependency (UML)",
      "score": 0.4320381283760071
    },
    {
      "name": "Algorithm",
      "score": 0.3386988639831543
    },
    {
      "name": "Artificial intelligence",
      "score": 0.28772953152656555
    },
    {
      "name": "Engineering",
      "score": 0.260364294052124
    },
    {
      "name": "Voltage",
      "score": 0.14187020063400269
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I102322142",
      "name": "Rutgers, The State University of New Jersey",
      "country": "US"
    }
  ]
}