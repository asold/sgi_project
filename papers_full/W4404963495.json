{
  "title": "Performance Evaluation and Implications of Large Language Models in Radiology Board Exams: Prospective Comparative Analysis",
  "url": "https://openalex.org/W4404963495",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2509139511",
      "name": "Boxiong Wei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3150212014",
    "https://openalex.org/W3013578857",
    "https://openalex.org/W2785645041",
    "https://openalex.org/W4387327030",
    "https://openalex.org/W4394877296",
    "https://openalex.org/W2963353367",
    "https://openalex.org/W2976398475",
    "https://openalex.org/W3138961261",
    "https://openalex.org/W3177044444",
    "https://openalex.org/W2890106926",
    "https://openalex.org/W3013681994",
    "https://openalex.org/W4229036772",
    "https://openalex.org/W2783687327",
    "https://openalex.org/W3012043879",
    "https://openalex.org/W2794518994",
    "https://openalex.org/W3011149445",
    "https://openalex.org/W3014301887",
    "https://openalex.org/W2051885079",
    "https://openalex.org/W2162821268",
    "https://openalex.org/W4376640706",
    "https://openalex.org/W4376640806",
    "https://openalex.org/W4392665596",
    "https://openalex.org/W3138769367",
    "https://openalex.org/W4390393653"
  ],
  "abstract": "Abstract Background Artificial intelligence advancements have enabled large language models to significantly impact radiology education and diagnostic accuracy. Objective This study evaluates the performance of mainstream large language models, including GPT-4, Claude, Bard, Tongyi Qianwen, and Gemini Pro, in radiology board exams. Methods A comparative analysis of 150 multiple-choice questions from radiology board exams without images was conducted. Models were assessed on their accuracy for text-based questions and were categorized by cognitive levels and medical specialties using χ 2 tests and ANOVA. Results GPT-4 achieved the highest accuracy (83.3%, 125/150), significantly outperforming all other models. Specifically, Claude achieved an accuracy of 62% (93/150; P &lt;.001), Bard 54.7% (82/150; P &lt;.001), Tongyi Qianwen 70.7% (106/150; P =.009), and Gemini Pro 55.3% (83/150; P &lt;.001). The odds ratios compared to GPT-4 were 0.33 (95% CI 0.18‐0.60) for Claude, 0.24 (95% CI 0.13‐0.44) for Bard, and 0.25 (95% CI 0.14‐0.45) for Gemini Pro. Tongyi Qianwen performed relatively well with an accuracy of 70.7% (106/150; P =0.02) and had an odds ratio of 0.48 (95% CI 0.27‐0.87) compared to GPT-4. Performance varied across question types and specialties, with GPT-4 excelling in both lower-order and higher-order questions, while Claude and Bard struggled with complex diagnostic questions. Conclusions GPT-4 and Tongyi Qianwen show promise in medical education and training. The study emphasizes the need for domain-specific training datasets to enhance large language models’ effectiveness in specialized fields like radiology.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.9363701343536377
    },
    {
      "name": "Computer science",
      "score": 0.37834692001342773
    },
    {
      "name": "World Wide Web",
      "score": 0.09432509541511536
    }
  ]
}