{
  "title": "AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events",
  "url": "https://openalex.org/W4387210603",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2060314455",
      "name": "Li, Yiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136591766",
      "name": "Li Jianfu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230638476",
      "name": "He Jianping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966090508",
      "name": "Tao Cui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4293740214",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4225479391",
    "https://openalex.org/W3135084718",
    "https://openalex.org/W2546821105",
    "https://openalex.org/W4361020574",
    "https://openalex.org/W3169536731",
    "https://openalex.org/W979396035",
    "https://openalex.org/W4379653093",
    "https://openalex.org/W2320856077",
    "https://openalex.org/W1845364899",
    "https://openalex.org/W4362553921",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4322492760",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W1918065319",
    "https://openalex.org/W4287327402",
    "https://openalex.org/W2008361529",
    "https://openalex.org/W3172480024",
    "https://openalex.org/W3178079110",
    "https://openalex.org/W3058517477",
    "https://openalex.org/W2116310281",
    "https://openalex.org/W4386655575",
    "https://openalex.org/W3002783676",
    "https://openalex.org/W4385951095",
    "https://openalex.org/W2136970311",
    "https://openalex.org/W4385474113",
    "https://openalex.org/W4324027515",
    "https://openalex.org/W4385292160",
    "https://openalex.org/W4366241052",
    "https://openalex.org/W4372348374",
    "https://openalex.org/W2781571133",
    "https://openalex.org/W4243640523",
    "https://openalex.org/W2906808750",
    "https://openalex.org/W1975597036",
    "https://openalex.org/W1983077458",
    "https://openalex.org/W3177920269",
    "https://openalex.org/W1998268789",
    "https://openalex.org/W4384921136",
    "https://openalex.org/W4361806442",
    "https://openalex.org/W4385495736",
    "https://openalex.org/W4360979863",
    "https://openalex.org/W2986530058",
    "https://openalex.org/W4212974972",
    "https://openalex.org/W4212893806",
    "https://openalex.org/W4318685230",
    "https://openalex.org/W1976988340",
    "https://openalex.org/W4382132333"
  ],
  "abstract": "Though Vaccines are instrumental in global health, mitigating infectious diseases and pandemic outbreaks, they can occasionally lead to adverse events (AEs). Recently, Large Language Models (LLMs) have shown promise in effectively identifying and cataloging AEs within clinical reports. Utilizing data from the Vaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study particularly focuses on AEs to evaluate LLMs' capability for AE extraction. A variety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2, were evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5 model (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match and 0.816 for relaxed match. The encouraging performance of the AE-GPT underscores LLMs' potential in processing medical data, indicating a significant stride towards advanced AE detection, thus presumably generalizable to other AE extraction tasks.",
  "full_text": "AE-GPT: UsingLargeLanguageModelstoExtract AdverseEventsfromSurveillanceReports-AUseCasewithInfluenzaVaccineAdverseEvents\nYimingLi\n1\n, JianfuLi\n1\n, JianpingHe\n1\n, Cui Tao\n1\n.\n1\n, McWilliamsSchool of Biomedical Informatics, TheUniversityof TexasHealthScienceCenterat Houston, Houston, TX77030, USA\nAbstract\nThoughVaccinesareinstrumental inglobal health, mitigatinginfectiousdiseasesandpandemicoutbreaks, theycanoccasionallyleadtoadverseevents(AEs). Recently, LargeLanguageModels(LLMs)haveshownpromiseineffectivelyidentifyingandcatalogingAEswithinclinicalreports. UtilizingdatafromtheVaccineAdverseEvent ReportingSystem(VAERS)from1990to2016, thisstudyparticularlyfocusesonAEstoevaluateLLMs' capabilityforAEextraction. Avarietyof prevalent LLMs, includingGPT-2, GPT-3variants, GPT-4, andLlama2, wereevaluatedusingInfluenzavaccineasausecase. Thefine-tunedGPT3.5model (AE-GPT)stoodout witha0.704averagedmicroF1scoreforstrict matchand0.816forrelaxedmatch.Theencouragingperformanceof theAE-GPTunderscoresLLMs' potential inprocessingmedical data, indicatingasignificant stridetowardsadvancedAEdetection, thuspresumablygeneralizabletootherAEextractiontasks.\nKeywords\nADE; GPT; Influenzavaccine; largelanguagemodel; largelanguagemodel fine-tuning; Llama;namedentityrecognition; natural languageprocessing; pharmacovigilance; prompt learning;VAERS; zero-shot learning\nIntroduction\nVaccinesareavital component of publichealthandhavebeeninstrumental inpreventinginfectiousillnesses[1]. Nowadays, wepossessvaccinesthat copewithover20life-threateningdiseases, contributingtoenhancedhealthandlongevityforpeopleacrossall agegroups[2].Eachyear, vaccinationssavebetween3.5to5millionindividualsfromfatal diseasessuchasdiphtheria, tetanus, pertussis, influenza, andmeasles[2]. However, vaccine-relatedadverseevents, althoughrare, canoccurafterimmunization. BySeptember2023, VaccineAdverseEvent ReportingSystem(VAERS)hadreceivedmorethan1,791,000vaccineAEreports, ofwhich9.5%wereclassifiedasserious, whichincludeeventsthat result indeath, hospitalization,orsignificant disability[3]â€“[5]. Consequently, vaccineAEscancausearangeof sideeffectsinindividuals, frommild, temporarysymptomstoseverecomplications[6]â€“[9]. Theymayalsogiverisetovaccinehesitancyamonghealthcareprovidersandrecipients[10].\nUnderstandingAEsfollowingvaccinationsisvital insurveillingtheeffectiveimplementationofimmunizationprograms[11]. Suchvigilanceensuresthecontinuoussafetyof vaccinationcampaigns, allowingforprompt responseswhenAEoccurs. Thisnot onlyconducestorecognizingearlywarningsignsbut alsofostershypothesesregardingpotential newvaccineAEsorshiftsinthefrequencyof knownones, ultimatelycontributingtotherefinement anddevelopment of vaccines[12].\nTherefore, theextractionof AEsandAE-relatedinformationwouldplayapivotal roleinadvancingourunderstandingof conditionssuchassyndromesandothersystemdisordersthatcanemergeaftervaccination. VAERSfunctionsasaspontaneousreportingsystemforadverseeventspost-vaccination, servingasthenational earlywarningmechanismtoflagpotentialsafetyissueswithU.S. licensedvaccines[13], [14]. VAERScollectsstructuredinformationsuchasage, medical background, andvaccinetype. It alsoincludesashort narrativefromthereporterstodescribesymptoms, medical history, diagnoses, treatments, andtheirtemporalinformation[13]. Althoughit doesnot establishcausal relationshipsbetweenAEsandvaccines,VAERSdetectspossiblesafetyconcernswarrantingdeeperinvestigationthroughrobustsystemsandstudydesigns[13]. Duet al. employedadvanceddeeplearningalgorithmstodetectnervoussystemdisorder-relatedeventsinthecasesof Guillain-Barresyndrome(GBS)linkedtoinfluenzavaccinesfromtheVAERSreports[15]. Throughtheevaluationof different machinelearninganddeeplearningmethods, includingdomain-specificBERTmodelssuchasBioBERTandVAERSBERT, theirresearchdemonstratedthesuperiorperformanceof deeplearningtechniquesovertraditional machinelearningmethods(ie, conditional randomfieldswithextensivefeatures)[15].\nNowadays, withthepopularityof artificial intelligence(AI)surging, aremarkablebreakthroughhasemerged: thedevelopment of largelanguagemodels(LLMs)[16]â€“[18]. Thesecutting-edgeAI constructshaveredefinedthewaycomputersunderstandandgeneratehumanlanguage,leadingtounprecedentedadvancementsinvariousaspects[19]. Thesemodels, poweredbyadvancedmachinelearningtechniques, havethecapacitytocomprehendcontext, semantics,andnuances, allowingthemtogeneratecoherent andcontextuallyrelevant text [17]. For\nexample, GenerativePre-trainedTransformer(GPT), developedbyOpenAI, representsapioneeringmilestoneintherealmof AI [20]. Built onamassivedataset, GPTisastate-of-the-artlanguagemodel that excelsingeneratingcoherent andcontextuallyrelevant text [21]. Sinceitsinception, GPThasexhibitedexceptional capabilities, rangingfromcraftingimaginativenarrativesandaidingcontent creationtofacilitatinglanguagetranslationandengaginginnaturalconversationswithvirtual assistants[22], [23]. Itsimpact acrossvariousdomainshashighlighteditspotential torevolutionizehuman-computerinteraction, makingsignificant stridestowardsmachinestrulyunderstandingandinteractingwithhumanlanguage [24]. Anothermodel Llama2, availablefreeof charge, however, takesanovel approachbyincorporatingmultimodal learning, seamlesslyfusingtext andimagedata[25]. ThisuniquefeatureenablesLlama2tonot onlycomprehendandgeneratetext withfinessebut alsotounderstandandcontextualizevisual information, settingit apart fromtraditional languagemodelslikeGPT[25].\nLLMsrepresent asignificant leapforwardinnatural languageprocessing(NLP), enablingapplicationsrangingfromtext generationandtranslationtosentiment analysisandChatbotvirtual assistants[26]. Bylearningpatternsfromvast amountsof text data, theseLLMshavethepotential tobridgethegapbetweenhumancommunicationandmachineunderstanding,openingupnewavenuesforcommunication, informationextraction, andproblem-solving[27].Huet al. examinedChatGPT'spotential forclinical namedentityrecognition(NER)inazero-shot context, comparingitsperformancewithGPT-3andBioClinicalBERTonsyntheticclinical notes[28]. ChatGPToutperformedGPT-3, althoughBioClinicalBERTstill performedbetter. ThestudydemonstratesChatGPT'spromisingutilityforzero-shot clinical NERtaskswithout requiringannotation[28].\nInthisstudy, weaimtodevelopAE-GPT, anautomaticadverseevent extractiontool basedonLLMs. WeusetheadverseeventsfollowingInfluenzavaccineasourusecase, whichwasalsostudiedbyDuet al. before[15]. Theinfluenzavaccineisoneof thetopreportedvaccinesinVAERS. Althoughweusetheinfluenzavaccineasausecase, theAE-GPTframeworkshouldpresumablyworkwell forothervaccinetypes. Whileseveral studieshaveexploredtheapplicationof LLMsinexecutingNERtasks, manyhaveprimarilyfocusedonutilizingpretrainedmodelsforzero-shot learning, lackingcomprehensiveperformancecomparisonsbetweenpretrainedLLMs, fine-tunedLLMs, andtraditional languagemodels. Thisresearchaimstoaddressthisgapbyprovidingathoroughexaminationof theLLM'scapabilities, specificallyfocusingonitsperformancewithintheNERtaskandalsoproposingtheadvancedfine-tunedmodelsAE-GPT, whichspecializesinAErelatedentityextraction. Ourinvestigationnot onlyinvolvesutilizingthepretrainedmodel forinferencebut alsoenhancingtheLLM'sNERperformancethroughfine-tuningwiththecustomizeddataset, thusprovidingadeeperunderstandingof itspotential andeffectiveness.\nMaterialsandMethods\nFigure1presentsanoverviewof thestudyframework. Ourinvestigationcommencedwithzero-shot entityrecognition, involvingthedirect input of userpromptsintopretrainedLLMs(GPT&Llama2). Toachieveacomprehensiveassessment of LLMs' effectivenessinclinicalentityrecognition, ouranalysiscoveredarangeof LLMs, namelyGPT-2, GPT-3, GPT-3.5,GPT-4, andLlama2. Furthermore, toenhancetheirperformance, weperformedfine-tuningontheseLLMsusingannotateddata, followedbyutilizinguserpromptstofacilitateresult inference.\nFigure1Overviewof thestudyframework\nDatasourceandusecase\nVAERSfunctionsasanadvancedalert mechanismjointlyoverseenbytheCentersforDiseaseControl andPrevention(CDC)andtheU.S. FoodandDrugAdministration(FDA), playingapivotal roleinidentifyingpotential safetyconcernsassociatedwithFDA-approvedvaccines[29],[30]. Asof Aug2013, VAERShasdocumentedmorethan1,781,000vaccine-relatedAEs[30].\nTheinfluenzavaccineplaysasignificant roleinpreventingmillionsof illnessesandvisitstodoctorsduetoflu-relatedsymptomseachyear[31]. Forexample, inthe2021-2022fluseason,priortotheemergenceof theCOVID-19pandemic, fluvaccinationwasestimatedtohavepreventedaround1.8millionflu-relatedillnesses, resultinginapproximately1,000,000fewermedical visits, 22,000fewerhospitalizations, andnearly1,000fewerdeathsattributedto\n\ninfluenza[32]. Astudyconductedin2021highlightedthat amongadultshospitalizedwithflu,thosewhohadreceivedthefluvaccinehada26%reducedriskof needingintensivecareunit(ICU)admissionanda31%lowerriskof flu-relatedmortalitycomparedtoindividualswhohadnot beenvaccinated[33].\nHowever, influenzavaccinesalsohavebeenassociatedwitharangeof potential adverseeffects, suchaspyrexia, hypoesthesia, andevenrareconditionslikeGBS[34]. Amongthem,GBSranksastheprimarycontributortoacuteparalysisindevelopednations, andcontinuestobethemost frequentlydocumentedseriousadverseevent followingtrivalent influenzavaccinationintheVAERSdatabase, withareport rateof 0.70casesper1millionvaccinations[35]â€“[37]. Thisrareautoimmunedisorder, GBS, affectstheperipheral nervoussystem,characterizedbyrapidlyadvancing, bilateral motorneuronparalysisthat typicallyarisessubsequent toanacuterespiratoryorgastrointestinal infection[35], [38]â€“[40].\nAsausecase, thisstudyfocusesonsymptomdescriptions(referredtoasnarrativesafetyreports)that includeGBSandsymptomsfrequentlylinkedwithGBS. Particularly, ourinterestliesinthesereportsfollowingtheadministrationof diverseinfluenzavirusvaccines, includingFLU3, FLU4, H5N1, andH1N1. Inordertoenableadirect performancecomparisonwithtraditional languagemodels, weemployedtheidentical dataset that wasutilizedbyDuet al. intheirpreviousstudy[15]. Thisdataset comprisesatotal of 91annotatedreports. Inthecontextof understandingthedevelopment of GBSandothernervoussystemdisorders, weexploredsixentitytypesthat collectivelycapturesignificant clinical insightswithinVAERSreports:investigation, nervous_AE, other_AE, procedure, social_circumstances, andtemporal_expression. Investigation, whichreferstolabtestsandexaminations, includingentitieslikeâ€œneurological examâ€andâ€œlumbarpunctureâ€[15]. nervous_AE(e.g., â€œtremorsâ€â€œGuillain-BarrÃ©syndromeâ€)involvessymptomsanddiseasesrelatedtonervoussystemdisorders, whereasother_AE(e.g., â€œcompletebowel incontinenceâ€â€œdiarrheaâ€)isassociatedwithothersymptomsanddiseases[15]. Procedureaddressesclinical interventionstothepatient,includingvaccination, treatment andtherapy, intensivecare, featuringinstancessuchasâ€œflushotâ€andâ€œhospitalizedâ€[15]. Social_circumstancerecordseventsassociatedwiththesocialenvironment of apatient, forexample, â€œsmokingâ€andâ€œalcohol abuseâ€[15]. Temporal_expressionisconcernedwithtemporal expressionswithprepositionslikeâ€œfor3daysâ€andâ€œonFridaymorningâ€[15].\nModels\nTofullyinvestigatetheperformanceof LLMsontheNERtask, GPTmodelsandLlama2will beleveraged.\nGPTGPTrepresentsagroundbreakingadvancement intherealmof NLPandartificial intelligence[41]. DevelopedbyOpenAI, GPTstandsasaremarkableexampleof thetransformativecapabilitiesof large-scaleneural languagemodels[42]. At itscore, GPTisfoundedupontheinnovativeTransformerarchitecture, amodel that hasrevolutionizedthefieldbyeffectivelycapturinglong-termdependencieswithinsequences, makingit exceptionallywell-suitedfortasksinvolvinglanguageunderstandingandgeneration[43], [44]. TheGPTfamilyhasmultipleversions: TheGPT-2model, with1.5billionparameters, iscapableof generatingextensivesequencesof text whileadaptingtothestyleandcontent of arbitraryinputs[45]. Moreover,GPT-2canalsoperformvariousNLPtasks, suchasclassification[45]. Ontheotherhand,GPT-3with175billionparameters, takesthecapabilitiesevenfurther[35]. It'sanautoregressivelanguagemodel trainedwith96layersonacombinationof 560GB+webcorpora, internet-basedbookcorpora, andWikipediadatasets, eachweighteddifferentlyinthetrainingmix[46], [47].GPT-3model isavailableinfourversions:, Davinci, Curie, BabbageandAdawhichdifferintheamount of trainableparametersâ€“175, 13, 6.7and2.7billionrespectively[46], [48]. GPT-4hasgrowninsizebyafactorof 1000, now reachingamagnitudeof 170trillionparameters, asubstantial increasewhencomparedtoGPT-3.5's175billionparameters[49]. Oneof themostnotableimprovementsinGPT-4istheexpandedcontext length. InGPT-3.5, thecontext lengthis2048[49]. However, inGPT-4, it hasbeenelevatedto8192or32768, dependingonthespecificversion, representinganaugmentationof 4to16timescomparedtoGPT-3.5[49]. Intermsof itsgeneratedoutput, GPT-4possessesthecapacitytonot just accommodatemultimodal input, but alsoproduceamaximumof 24000words(equivalent to48pages)[49].Thisrepresentsanincreaseof 8timescomparedtoGPT-3.5, constrainedby3000words(equivalent to6pages)[49].\nTherationalebehindGPT'sdesignstemsfromtheunderstandingthat pre-training, involvingtheunsupervisedlearningof languagepatternsfromvast textual corpora, canprovideastrongfoundationforsubsequent fine-tuningonspecifictasks[42]. ThisenablesGPTtoacquireasophisticatedunderstandingof grammar, syntax, semantics, andevenworldknowledge,essentiallylearningtogeneratecoherent andcontextuallyrelevant text [50].\nGPT'sarchitecturecomprisesmultiplelayersof self-attentionmechanisms, whichallowthemodel toweightheimportanceof different wordsinasentencebasedontheircontextualrelationships[51], [52]. Thisintricatelayering, coupledwiththemodel'sconsiderableparameters, empowersGPTtoprocessandgeneratecomplexlinguisticstructures, makingit aversatiletool forawiderangeof NLPtasks, includingtext completion, translation,summarization, andevencreativewriting[53].\nLLama2Llama2emergesasacutting-edgeadvancement inthedomainof natural languageprocessing,markingasignificant evolutioninthelandscapeof languagemodels[25]. Developedasanextensionof itspredecessor, Llama, thismodel representsaninnovativestepforwardin\nharnessingthepowerof transformersforlanguageunderstandingandgeneration[54]. Thearchitectureof Llama2isfirmlyrootedintheTransformerframework, whichhasrevolutionizedthefieldbyenablingthemodelingof complexdependenciesinsequences.\nTherationalebehindLlama2'sconceptionrestsupontherecognitionthat whilepre-traininglargelanguagemodelsondiversetext corporaisbeneficial, customizingtheirmulti-layerself-attentionarchitectureforlinguisticstructurescanfurtheroptimizetheirperformance[54].\nExperiment setup\nDataset split\nInthisstudy, wepartitionedthedataset intoatrainingset andatest set usingan8:2ratio,where72VAERSreportsweredesignatedforthetrainingset andtheremaining20%forthetestset.\nPretrainedmodel inferenceWefirstlyinferredtheresultsbyusingtheavailablepretrainedLLMs. GPT-2model sourcehasbeenmadepubliclyavailable, weusedTFGPT2LMHeadModel asthepretrainedGPT-2modeltotest itsabilityinthisNERtask[55]. Llama2isalsoanopen-sourceLLM, whichcanbeaccessedthroughMetaAI [54].\nTable1Promptsandhyperparametersof pretrainedmodels\nModel Prompt Temperature\nMaxtokens\nGPT-2\nPleaseextract all namesofinvestigation, nervousAE, otherAE,procedure, social circumstance, andtimestampfromthisnote, andput theminalist\n1.0 1,000\nGPT-3 ada\nAnswerthequestionbasedonthecontext below, andif thequestioncan'tbeansweredbasedonthecontext,say\"I don't know\"\nContext: [note]\n0 150\n---\nQuestion: Pleaseextract all namesof[timestamp/nervous_AE/other_AE/procedure/investigation/socialcircumstance] fromthisnote\nAnswer:\nbabbage\ncurie\ndavinci\nGPT-3.5 Pleaseextract all namesofinvestigation, nervousAE, otherAE,procedure, social circumstance, andtimestampfromthisnote, andput theminalist\n0.8 1,000GPT-4\nLlama\n2-7b-chat \"Pleaseextract all namesof[timestamp/nervousAE/otherAE/procedure/investigation/socialcircumstance] fromthisnote: [note]\n0.6 5122-13b-chat\nToevaluatetheperformanceof thepre-trainedmodels, weconductedseveral experiments,selectingthetemperatureandmaxtokenssettings(showninTable1)that yieldedthebestresults. Weemployedtheprompts(asdepictedinTable1)that adeptlyarticulateourobjective,arecomprehensibletotheLLMs, andadditionallyaidintheefficient extractionof results.\nInferenceforthepretrainedGPTmodelswasexecutedonaserverequippedwith8NvidiaA100GPUs, whereeachGPUprovidedamemorycapacityof 80GB. Meanwhile, thepretrainedLlamamodelswereinferredonaserver, whichincluded5NvidiaV100GPUs, eachofferingamemorycapacityof 32GB.\nModel Fine-tuningFine-tuningtheGPTmodelsisfacilitatedthroughOpenAI ChatGPT'sAPI calls, withtheexceptionthat theGPT-2model'sfine-tuningstemsfromGPT2sQAandthefine-tuningfor\nGPT-4hasnot beenmadeaccessibleyet [56]. ForLlama2models, thefine-tuningprocessbeginswithHuggingFace. Subsequently, themodel'sembeddingsareautomaticallyfine-tunedandupdated. Throughout theprocess, thetemperatureremainsconsistent. Theformatrequirementsfortrainingset templatesdifferamongmodels, dependingonwhethertheyareinstruction-basedornot. Figure2presentsanexampleof thequestionanswering-basedtrainingset usedbyGPT-2, whichinitializeswiththequestionâ€œPleaseextract all thenamesofnervous_AEfromthefollowingnoteâ€. Thequestionisfollowedbytheanswerwithannotationswheretheentities(i.e., GuillainBarreSyndrome, quadriplegic, GBS)andthestartingcharacteroffset (i.e., 0, 141, 212)shouldbeindicated. Thetrainingset endswiththecontext (GuillainBarreSyndrome. Onset onâ€¦). Figure3showsanexampletoillustratethestructuredformat ofthetrainingset tailoredforGPT-3, wheretheprompt andannotationsarenecessitated. Intheprompts, onlytheoriginal report isrequiredbecauseof thepredeterminedNERtemplateembeddedinGPT-3, whiletheannotationsincludetheentitytypesandtheentities. Forinstance, asshowninFigure3, â€œGuillainBarreSyndrome. Onsetâ€¦â€istheoriginal descriptionfromtheVAERSreports. Withintheannotationssection, all theinvolvedentitytypes(nervous_AE, timestamp, investigation, other_AE, andprocedure)arelisted, with'GuillainBarreSyndrome', 'quadriplegic', and'GBS' beingtheentitiesclassifiedundernervous_AE. Figure4showsaninstruction-basedtrainingexampleutilizedfor, GPT-3.5, andLlama2-chat, whichutilizesprompt instructionstoguideandrefinethemodel'sresponses, ensuringmoreaccurateandcontextuallyrelevant outputs. Theprocessof human-machineinteractionisimitated. Inthisscenario, threerolesareidentified: thesystem, user, andassistant. ThesystemoutlinesthetasktobeaccomplishedbyGPT, stipulating-\"Youareanassistant adept at namedentityrecognition.\"Unlikethestructuredformat trainingset, inadditiontotheoriginal VAERSreports,usersarealsorequiredtoclarifythetaskwithaspecificquestionâ€”e.g., \"Pleaseextract all thenervous_AEentitiesinthefollowingnote.\"Theannotationssectiononlyincludetheanticipatedresponsesthat usersexpect GPTtoprovide.\nFigure2Oneexampleof aquestionanswering-basedtrainingset.\nFigure3Oneexampleof thestructuredformat trainingset\n\nFigure4Oneexampleof theinstruction-basedtrainingset.\n\nTable2Promptsandhyperparametersof model fine-tuning\nModel Prompt\nTrainingsetformat\nTemperature\nMaxtokens\nGPT-2\nPleaseextract all thenamesof[timestamp/nervous_AE/other_AE/procedure/investigation/social circumstance] fromthefollowingnote\nQuestionanswering-based\n1.0 1,000\nGPT-3\nada\nJSONformat specifiedbyOpenAI\nStructured 0.8 1,000\nbabbage\ncurie\ndavinci\nGPT-3.5\nPleaseextract all the[timestamp/nervous_AE/other_AE/procedure/investigation/social circumstance] inthefollowingnote: [note]\nInstruction-based\n1.0 4,096\nLlama\n2-7b-chat JSONformat specifiedbyLlama\nInstruction-based\n1.0 4,0962-13b-chat\nFormodel fine-tuning, weselectedtheinitial hyperparametersasoutlinedinTable2. Typically,thesesettingsarebasedondefaults, except forGPT-3, wherenon-default valuesoutperformedthedefaults. Asforprompts, theydifferslightlyduetomodel-specifictrainingset needs.\nFine-tuningforthepretrainedGPTmodelswasexecutedonaserverequippedwith8NvidiaA100GPUs, whereeachGPUprovidedamemorycapacityof 80GB. Meanwhile, thepretrainedLlamamodelswerefine-tunedonaserver, whichincluded5NvidiaV100GPUs, eachofferingamemorycapacityof 32GB.\nPost-processingInthepost-processingstage, weaddressedinstancesof nestedentitiessharingthesameentitytype, asdepictedinFigure5(â€œMusclestrengthâ€vsâ€œMusclestrengthdecreasedâ€œ). Toeffectivelyhandlethis, weadoptedastrategywhereinentitiespossessingthelongest spanswereretained,whilethenestedentitieswereexcludedfromconsideration. IntheexamplesillustratedinFigure5, theinvestigationterm\"Musclestrength\"waseliminated, resultinginnervous_AE\"Musclestrengthdecreased\"forthefinal output. Thisprocedureensuredastreamlinedandaccuraterepresentationof theentitieswithinthegivencontext.\nFigure5Oneexampleof nestedentities\nEvaluationTheperformanceof theLLMswasevaluatedbymetrics, includingprecision, recall, andF1.Theseevaluationswereconductedundertwodistinct matchingcriteria: exact matching, whichrequiredidentical entityboundaries, andrelaxedmatching, whichtookintoconsiderationoverlappingentityboundaries.\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =  \nğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’+ğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =  \nğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’+ğ¹ğ‘ğ‘™ğ‘ ğ‘’ ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’\nğ¹ âˆ’ 1 =  \n2Ã—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™\nResults\nTable3andTable4presentstheNERperformanceacrossvariousLLMsusingstrict F1andrelaxedF1metricsrespectively. Intermsof evaluationmetrics, strict F1requiresanexactmatchinbothcontent andpositionbetweenthepredictedandtruesegments, whileRelaxedF1allowsforpartial matches, providingamorelenient evaluationof model performance. Notably,theGPT-3.5model emergesasthefrontrunnerinthisNERtask. Remarkably, GPT-3, 3.5, and4modelssurpassLLamamodelssignificantly. WithintheGPTmodel family, performancesignificantlyimproveswitheachsuccessiveversionupgrade. GPT-3-davinci, inparticular,achievedthehighest performanceamongall GPT-3models.\nInterestingly, theperformanceof thefine-tunedGPT-3model closelyrivalsthat of GPT-4thoughthefine-tunedGPT-3model outperformsboththepretrainedGPT-3.5andGPT-4models.\n\nTable3NERperformancecomparisononVAERSreportsbystrict F1\nGPT-2\nGPT-3 GPT-3.5 GPT-4\nLlama\nada babbage curie davinci 2-7b-chat 2-13b-chat\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nPretrained\nFine-tuned\nPretrained\nFine-tuned\ninvestigation 0 0 0 0.389 0 0.304 0 0.214 0 0.344 0.241 0.667 0.304 0.097 0.024 0.099 0.114\nnervous_AE 0 0 0 0.319 0 0.277 0 0.356 0 0.459 0.208 0.727 0.458 0.102 0.024 0.173 0.096\nother_AE 0 0 0 0.17 0 0.17 0 0.183 0 0.351 0.165 0.638 0.412 0.234 0.041 0.213 0.042\nprocedure 0 0 0 0.39 0 0.388 0 0.511 0 0.464 0.059 0.716 0.02 0.283 0.066 0.189 0.077\nsocial_circumstance\n0 0 0 0 0 0 0 0 0 0 0.133 0.5 0 0 0 0 0\ntemporal_expression\n0 0 0 0.457 0 0.545 0 0.504 0 0.583 0.252 0.76 0.323 0.202 0.062 0.013 0.053\nMicroaverage\n0 0 0 0.335 0 0.356 0 0.359 0 0.436 0.183 0.704 0.308 0.16 0.046 0.134 0.07\nTables3andTable4showtheNERperformanceforvariousentitytypes, encompassinginvestigation, nervous_AE, other_AE, procedure, social_circumstance, andtemporal_expressionrespectively. Amongthesecategories, temporal_expressionexhibitsthehighest performance, followedbynervous_AEandprocedure.\nHowever, it'sworthnotingthat LLMsencountersignificant challengesinextractingsocialcircumstanceentities, withthefine-tunedGPT-3.5model achievingthehighest F1scoreof 0.5onlyinthiscategory. Acrossall modelsevaluated, GPT-3.5generallydeliversthebestperformance, except forthepretrainedGPT-3.5, whichexcelsinprecisionscoresforinvestigationextraction. Therefore, weproposedthefine-tunedGPT-3.5, andnamedit asâ€œAE-GPTâ€.\nTable4NERperformancecomparisononVAERSreportsbyrelaxedF1\nGPT-2\nGPT-3 GPT-3.5 GPT-4\nLlama\nada babbage curie davinci 2-7b-chat 2-13b-chat\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nFine-tuned\nPretrained\nPretrained\nFine-tuned\nPretrained\nFine-tuned\ninvestigation 0 0 0 0.463 0 0.448 0 0.321 0 0.53 0.289 0.795 0.464 0.13 0.047 0.128 0.21\nnervous_AE 0 0.047 0 0.478 0 0.408 0 0.483 0 0.584 0.308 0.872 0.658 0.277 0.047 0.378 0.192\nother_AE 0 0.021 0 0.243 0 0.243 0 0.286 0 0.412 0.278 0.704 0.486 0.309 0.062 0.295 0.042\nprocedure 0 0.08 0 0.419 0 0.464 0 0.534 0 0.522 0.094 0.743 0.305 0.318 0.077 0.245 0.088\nsocial_circumstance\n0 0 0 0 0 0 0 0.286 0 0 0.133 0.5 0 0 0 0 0\ntemporal_expression\n0 0.075 0 0.705 0 0.743 0 0.628 0 0.729 0.613 0.886 0.673 0.519 0.198 0.093 0.093\nMicroaverage\n0 0.049 0 0.457 0 0.484 0 0.456 0 0.54 0.329 0.816 0.515 0.269 0.101 0.221 0.118\nDiscussion\nOurresearchhasyieldedremarkableinsightsintothecapabilitiesof LLMsinthecontext ofNER. Withaspecificfocusontheperformanceof thesemodels, wehaveachievedsubstantialachievementsthroughout thisstudy. Additionally, wearepleasedtointroducetheadvancedfine-tunedmodelscollectivelyknownasAE-GPT, whichhavedemonstratedexceptionalprowessintheextractionof AErelatedentities. Ourworkshowcasesthesuccessof leveragingpretrainedLLMsforinferenceandfine-tuningthemwithacustomizeddataset, underscoringtheeffectivenessandpotential of thisapproach.\nIntherealmof LLMsforAENERtasks, thefine-tunedGPT-3.5-turbomodel (AE-GPT)notablystoodout, demonstratingsuperiorperformancecomparedtoitscompetingmodels. Interestingly,theprocessof fine-tuningseemedtohaveasignificant effect onthecapabilitiesof certainmodels. Forinstance, boththefine-tunedGPT-3andGPT-3.5showedenhancedperformance,\nevenoutstrippingthemoreadvancedbut unfine-tunedGPT-4. Thissuggeststhat thespecificfine-tuningwithAEdatasetscouldhaveequippedGPTmodelswithamoreprofoundinsight ofthedomain, whereasthegeneric, broadknowledgebaseof thepretrainedGPT-4maynot havebeenasoptimizedforthisparticulartask. However, thisfine-tuningeffect wasnot universallyobserved. Despitesimilarattemptsat enhancement, GPT-2didnot exhibit substantialimprovementswhenfine-tuned. Oneplausibleexplanationisthat GPT-2'sunderlyingarchitectureandtrainingmight havespecializedintasksliketext completionratherthanNERtasks[57]. Itscorestrengthsmaynot alignasseamlesslywiththedemandsof AENER,resultinginfine-tuninglesseffectiveforthismodel. Ontheotherhand, theperformanceofLlamaremainedstagnant acrossbothitsiterationsandfine-tuningattempts. Thiscouldbeindicativeof aplateauinthemodel'slearningcapacityfortheAENERtask, orperhapsthefine-tuningprocessordatadidnot sufficientlyalignwiththemodel'sstrengths. Anotherpossibilityisthat Llama'sarchitectureinherentlylackscertainfeaturesorcapacitieswhichmaketheGPTseriesmoreadaptabletotheAENERtask. Thelimitedsizeof thedataset mayalsocontributetotheoverfitting, whichdegradestheperformance. Furtherinvestigationmight beneededtodiscernthespecificfactorsinfluencingLlama'sperformance.\nComparedtotheworkcarriedout byDuet al., whichfocusedonusingtheconventionalmachinelearning-basedmethodsanddeeplearningbasedmethods[15]. AE-GPToutperformstheproposedmodel inDuâ€™swork(thehighest exact matchmicroaveragedF-1scoreat 0.6802byensemblesof BioBERTandVAERSBERT; highest lenient matchmicroaveragedF-1scoreat 0.8078byLargeVAERSBERT)[15]. AE-GPTâ€™s(thefine-tunedGPT-3.5modelâ€™s)enhancedperformanceinextractingspecificentitieslikeinvestigations, variousadverseevents, socialcircumstances, andtimestampscanbeattributedtoitsvast pretrainingondiversedatasetsanditsinherent architectural advantages, allowingit tocapturebroadercontextual nuances.Meanwhile, theensemblesof BioBERTandVAERSBERT, despitetheirbiomedicalspecialization, might havelimitationsinadaptabilityacrossdiversedatarepresentations, leadingtotheircomparativeunderperformance. However, whenfocusingonprocedureextraction, thedomain-specificnatureof theBioBERTandVAERSBERTensemblemight provideamoreattunedunderstandingof theintricateandcontext-dependent natureof medical procedures.ThisspecificitycouldovershadowGPT-3.5'sbroadadaptability, explainingthelatter'slessereffectivenessinthat particularextractiontask.\nOurstudyembarkedonacomprehensivecomparisonof prevalent largelanguagemodels,encompassingGPT-2, variousversionsof GPT-3, GPT-3.5, GPT-4, andLlama2, specificallyfocusingontheiraptitudetoextract AE-relatedentities. Crucially, bothpretrainedandfine-tunediterationsof thesemodelswerescrutinized. Basedonitsexhaustivenature, thisresearchstandsasoneof themost holisticinquiriestodateintotheperformanceof LLMsintheNERdomain. Furthermore, it carvesanichebyexploringtheimpact of fine-tuningonLLMsforNERtasks, distinguishingoureffortsfromotherexistingresearchandreinforcingthestudy'suniquecontributiontothefield.\nWhileourstudyoffersvaluableinsights, it isnot without itslimitations. Thedataset utilizedinthisresearchisrelativelyconstrained, comprisingonly91VAERSreports. Thislimitedscope\nmight impedethegeneralizabilityof ourfindingstobroadercontexts. Moreover, it'snoteworthythat weprimarilyfocusedonVAERSreports, whichdifferinstructureandcontent fromtraditional clinical reports, potentiallylimitingthedirect applicabilityof ourfindingstoothermedical documentation.\nInourforthcomingendeavors, weaimtoincorporatefine-tuningexperimentswithGPT-4,especiallyasit becomesaccessibleforsuchtasksinthefall of 2023. Thiswill not onlyaddanotherdimensiontoourcurrent findingsbut alsoensurethat ourresearchremainsat thecuttingedge, reflectingthelatest advancementsintheworldof LLMs.\nError Analysis\nWhileAE-GPT(thefine-tunedGPT-3.5model)hasdemonstratedcommendableperformanceinrecognizingamajorityof entitytypes, it exhibitsinherent limitations. Table5showstheerrorstatisticsof AE-GPTacrossvariousentitytypes. Ourclassificationof errortypesremainsconsistent withthat of Duet al., ensuringeasiercomparison[15]. 'Boundarymismatch' denotesdiscrepanciesinthespanrangeof entitiesbetweenmachine-annotatedandhuman-annotatedresults. 'Falsepositive' referstoentitiesidentifiedbytheproposedmodel that aren't present inthegoldstandard, while'falsenegative' indicatesentitiesthemodel failedtoextract. 'Incorrectentitytype' pertainstoinstanceswhere, thoughtheentity'sspanrangeisaccurate, theentityitself hasbeenmisclassified. It isevident that themodel exhibitsapredominant challengeindealingwithboundarymismatch, falsepositivesandfalsenegatives, whichcanbeattributedtoseveral factors. Thequalityandrepresentativenessof thetrainingdataplayasignificant role;inconsistent orlimitedannotationscanleadtomismatchesandincorrect identifications. Theinherent complexityof distinguishingsimilarandpotentiallyoverlappingentitiesaddstothechallenge. Additionally, textual ambiguityandthetrade-off betweenspecializationduringfine-tuningandthegeneralizationfromthemodel'soriginal vast pretrainingcanimpactaccuracy. WhileGPT-3ispowerful, capturingall thenuancesof aspecializedNERtaskcanstillposechallenges.\nInparticular, AE-GPTtendstomissspecificprocedurenames, suchas\"IVimmunoglobulin\"and\"fluvax.\"Likewise, it exhibitsaheightenedlikelihoodof failingtorecognizeentitiesrelatedtosocial circumstances. Thisunderscoresthenecessityforanimprovedandbroaderdomain-specificvocabularywithinGPT-3.5.\nMoreover, AE-GPT frequentlyconfusesgeneral termssuchas\"injection\"and\"vaccinated\"asexact procedurenames, andfailstoextract thereal vaccinenamesfollowingit inthetext Thismisinterpretationresultsinconcurrent falsepositiveandfalsenegativeerrors.\nAnothernoteworthylimitationof AE-GPT isitspronenesstosplittingerrors. Forinstance,considerthenamedphrase\"unabletomovehishands, armsorlegs.\"Themodel oftenerroneouslysegmentsthisinto\"unabletomovehishands'' and\"armsorlegs,\"revealingashortcominginitsgraspof languageunderstanding.\nTable5Statisticsof AE-GPTpredictionerrorsondifferent entitytypes\nBoundaryMismatch(outofhumanannotatedentities)\nFalsePositive(outofmachineannotatedentities)\nFalseNegative(outofhumanannotatedentities)\nIncorrectEntityType(outofmachineannotatedentities)\ninvestigation 13/66, 19.7% 20/90, 22.22% 6/66, 9.1% 5/90, 5.56%\nnervous_AE 28/175, 16% 15/169, 8.88% 30/175, 17.14% 1/169,0.59%\nother_AE 12/169, 7.1% 21/132,15.91%\n63/169, 37.28% 3/132,2.27%\nprocedure 4/156, 2.56% 28/140, 20% 46/156, 29.49% 2/140,1.43%\nsocial_circumstance 0/2, 0% Â½, 50% Â½, 50% 0/2, 0%\ntemporal_expression 21/141,14.89%\n6/130, 4.62% 22/141,15.6% 0/130,0%\nInournext steps, weintendtoimproverareentityextraction, suchassocial circumstances, byleveragingontologiesandterminologiesinthesespecificdomains. WealsoplantoenhancetheembeddingswithinLLMstobroadentheircoverageof theserareentities. Furthermore,expandingourdataset toincludedrugAEsisonouragenda. Wewill alsointroduceclinicalnotesandbiomedical literaturetofurtherenrichthedataset. ThisincreaseddatavolumewillenableLLMstobetterdistinguishnuancesbetweenentityclasses, suchasprocedurevs.investigationandnervous_AEvs. other_AE.\nConclusion\nInconclusion, ourcomprehensiveexplorationof LLMswithinthecontext of NER, includingthedevelopment of ourspecializedAEextractionmodel AE-GPT, hasnot onlyhighlightedtheprofoundimplicationsof ourfindingsbut alsomarksasignificant achievement asthefirst papertoevaluatetherealmof pretrainedandfine-tunedLLMsinNER. Theintroductionof ourspecializedfine-tunedmodel, AE-GPT, exhibitstheabilitytotailorLLMstodomain-specific\ntasks, offeringpromisingavenuesforaddressingreal-worldchallenges, particularlyintheextractionof AErelatedentities. Ourresearchunderlinesthebroadersignificanceof LLMsinadvancingnatural languageunderstandingandprocessing, withimplicationsspanningvariousfields, fromhealthcareandbiomedicinetoinformationretrieval andbeyond. Aswecontinuetoharnessthepotential of LLMsandrefinetheirperformance, weanticipatefurtherbreakthroughsthat will driveinnovationandenhancetheutilityof thesemodelsacrossdiverseapplications.\nAcknowledgments\nThisarticlewaspartiallysupportedbytheNational Instituteof AllergyAndInfectiousDiseasesof theNational Institutesof HealthunderAwardNumbersR01AI130460andU24AI171008.\nEthicsapprovalandconsenttoparticipate\nNot applicable.\nCompetinginterests\nTheauthorsdeclarethat therearenocompetinginterests.\nAuthorcontribution\nCTtooktheleadindesigningtheexperiments, withYL, JLandJHcontributingtotheexperimental design. YLwasresponsibleformodel development. YLconductederroranalysis.YLwasinvolvedindraftingthemanuscript, withCTofferingresearchsupport. CTandJLeditedandreviewedthemanuscript, andall authorsapprovedthefinal manuscript.\nDataavailability\nThedatasetsgeneratedduringand/oranalyzedduringthecurrent studyareavailablefromthecorrespondingauthoronreasonablerequest.\nAbbreviations\nAE adverseeventAI artificial intelligenceCDC CentersforDiseaseControl andPrevention\nFDA FoodandDrugAdministrationGBS Guillain-BarresyndromeGPT GenerativePre-trainedTransformerICU intensivecareunitLLM LargeLanguageModelNER namedentityrecognitionNLP natural languageprocessingVAERS VaccineAdverseEvent ReportingSystem\nReferences\n[1] L. Di Renzoet al., â€œVaccines, MicrobiotaandImmunonutrition: FoodforThought,â€Vaccines(Basel), vol. 10, no. 2, p. 294, Feb. 2022, doi: 10.3390/vaccines10020294.[2] â€œVaccinesandimmunization.â€https://www.who.int/health-topics/vaccines-and-immunization(accessedSep. 14, 2023).[3] â€œAbout VAERS,â€VaccineAdverseEvent ReportingSystem. https://vaers.hhs.gov/about.html[4] â€œCDCWONDER,â€Wide-rangingOnlineDataforEpidemiologicResearch(CDCWONDER).https://wonder.cdc.gov/[5] J. Fraimanet al., â€œSeriousadverseeventsof special interest followingmRNACOVID-19vaccinationinrandomizedtrialsinadults,â€Vaccine, vol. 40, no. 40, pp. 5798â€“5805, Sep.2022, doi: 10.1016/j.vaccine.2022.08.036.[6] â€œPossibleSideeffectsfromVaccines| CDC,â€Apr. 26, 2023.https://www.cdc.gov/vaccines/vac-gen/side-effects.htm(accessedSep. 14, 2023).[7] M. M. McNeil et al., â€œRiskof anaphylaxisaftervaccinationinchildrenandadults,â€JAllergyClinImmunol, vol. 137, no. 3, pp. 868â€“878, Mar. 2016, doi: 10.1016/j.jaci.2015.07.048.[8] P. M. Strebel et al., â€œEpidemiologyof PoliomyelitisintheUnitedStatesOneDecadeaftertheLast ReportedCaseof IndigenousWildVirus-AssociatedDisease,â€Clinical InfectiousDiseases, vol. 14, no. 2, pp. 568â€“579, 1992.[9] A. Babazadehet al., â€œInfluenzaVaccinationandGuillainâ€“BarrÃ©Syndrome: RealityorFear,â€JTransl Int Med, vol. 7, no. 4, pp. 137â€“142, Dec. 2019, doi: 10.2478/jtim-2019-0028.[10]P. Paterson, F. Meurice, L. R. Stanberry, S. Glismann, S. L. Rosenthal, andH. J. Larson,â€œVaccinehesitancyandhealthcareproviders,â€Vaccine, vol. 34, no. 52, pp. 6700â€“6706, Dec.2016, doi: 10.1016/j.vaccine.2016.10.042.[11]S. MachingaidzeandC. S. Wiysonge, â€œUnderstandingCOVID-19vaccinehesitancy,â€NatMed, vol. 27, no. 8, Art. no. 8, Aug. 2021, doi: 10.1038/s41591-021-01459-7.[12]F. Varricchioet al., â€œUnderstandingvaccinesafetyinformationfromtheVaccineAdverseEvent ReportingSystem,â€PediatrInfect DisJ, vol. 23, no. 4, pp. 287â€“294, Apr. 2004, doi:10.1097/00006454-200404000-00002.[13]A. PatriciaWodi, P. Marquez, A. Mba-Jonas, F. Barash, K. Nguon, andP. L. Moro,â€œSpontaneousreportsof primaryovarianinsufficiencyaftervaccination: Areviewof thevaccineadverseevent reportingsystem(VAERS),â€Vaccine, vol. 41, no. 9, pp. 1616â€“1622,Feb. 2023, doi: 10.1016/j.vaccine.2022.12.038.[14]T. T. Shimabukuro, M. Nguyen, D. Martin, andF. DeStefano, â€œSafetymonitoringintheVaccineAdverseEvent ReportingSystem(VAERS),â€Vaccine, vol. 33, no. 36, pp.4398â€“4405, Aug. 2015, doi: 10.1016/j.vaccine.2015.07.035.[15]J. Duet al., â€œExtractingpostmarketingadverseeventsfromsafetyreportsinthevaccineadverseevent reportingsystem(VAERS)usingdeeplearning,â€Journal of theAmerican\nMedical InformaticsAssociation, vol. 28, no. 7, pp. 1393â€“1400, Jul. 2021, doi:10.1093/jamia/ocab014.[16]B. A. yArcas, â€œDoLargeLanguageModelsUnderstandUs?,â€Daedalus, vol. 151, no. 2, pp.183â€“197, May2022, doi: 10.1162/daed_a_01909.[17]J. Chenet al., â€œWhenLargeLanguageModelsMeet Personalization: PerspectivesofChallengesandOpportunities.â€2023.[18]Y. Li, S. Bubeck, R. Eldan, A. D. Giorno, S. Gunasekar, andY. T. Lee, â€œTextbooksAreAllYouNeedII: phi-1.5technical report.â€2023.[19]B. MonteithandM. Sung, â€œUnleashingtheEconomicPotential of LargeLanguageModels:TheCaseof ChineseLanguageEfficiency,â€TechRxiv. Preprint, 2023.https://doi.org/10.36227/techrxiv.23291831.v1[20]K. Chenget al., â€œThePotential of GPT-4asanAI-PoweredVirtual Assistant forSurgeonsSpecializedinJoint Arthroplasty,â€AnnBiomedEng, vol. 51, no. 7, pp. 1366â€“1370, Jul.2023, doi: 10.1007/s10439-023-03207-z.[21]J.-S. LeeandJ. Hsiang, â€œPatent claimgenerationbyfine-tuningOpenAI GPT-2,â€WorldPatent Information, vol. 62, p. 101983, Sep. 2020, doi: 10.1016/j.wpi.2020.101983.[22]S. Biswas, â€œProspectiveRoleof Chat GPTintheMilitary: AccordingtoChatGPT,â€Qeios,Feb. 2023, doi: 10.32388/8WYYOD.[23]R. Imamguluyev, â€œTheRiseof GPT-3: ImplicationsforNatural LanguageProcessingandBeyond,â€International Journal of ResearchPublicationandReviews, vol. 4, pp. 4893â€“4903,Mar. 2023, doi: 10.55248/gengpi.2023.4.33987.[24]DaijuUeda, ShannonLWalston, ToshimasaMatsumoto, RyoDeguchi, Hiroyuki Tatekawa,andYukioMiki, â€œEvaluatingGPT-4-basedChatGPTâ€™sClinical Potential ontheNEJMQuiz,â€medRxiv, p. 2023.05.04.23289493, Jan. 2023, doi: 10.1101/2023.05.04.23289493.[25]K. I. Roumeliotis, N. D. Tselikas, andD. K. Nasiopoulos, â€œLlama2: EarlyAdoptersâ€™Utilizationof Metaâ€™sNewOpen-SourcePretrainedModel,â€Preprints, vol. 2023, 2023, doi:10.20944/preprints202307.2142.v2.[26]Y. Gong, â€œMultilevel LargeLanguageModelsforEveryone.â€2023.[27]T. Hagendorff, â€œMachinePsychology: InvestigatingEmergent CapabilitiesandBehaviorinLargeLanguageModelsUsingPsychological Methods.â€2023.[28]Y. Huet al., â€œZero-shot Clinical EntityRecognitionusingChatGPT.â€2023.[29]M. Gringeri et al., â€œHerpeszosterandsimplexreactivationfollowingCOVID-19vaccination:newinsightsfromavaccineadverseevent reportingsystem(VAERS)databaseanalysis,â€Expert RevVaccines, vol. 21, no. 5, pp. 675â€“684, May2022, doi:10.1080/14760584.2022.2044799.[30]â€œVAERS-Data.â€https://vaers.hhs.gov/data.html (accessedAug. 17, 2023).[31]L. E. Vega-BriceÃ±o, K. AbarcaV, andI. SÃ¡nchezD, â€œ[Fluvaccineinchildren: stateof theart],â€RevChilenaInfectol, vol. 23, no. 2, pp. 164â€“169, Jun. 2006, doi:10.4067/s0716-10182006000200011.[32]â€œBenefitsof FluVaccinationDuring2021-2022FluSeason,â€CentersforDiseaseControlandPrevention, Jan. 25, 2023.https://www.cdc.gov/flu/about/burden-averted/2021-2022.htm(accessedAug. 11, 2023).[33]J. M. Ferdinands, M. G. Thompson, L. Blanton, S. Spencer, L. Grant, andA. M. Fry, â€œDoesinfluenzavaccinationattenuatetheseverityof breakthroughinfections?Anarrativereviewandrecommendationsforfurtherresearch,â€Vaccine, vol. 39, no. 28, pp. 3678â€“3695, Jun.2021, doi: 10.1016/j.vaccine.2021.05.011.[34]J. Du, Y. Cai, Y. Chen, andC. Tao, â€œTrivalent influenzavaccineadversesymptomsanalysisbasedonMedDRAterminologyusingVAERSdatain2011,â€Journal of BiomedicalSemantics, vol. 7, no. 1, p. 13, May2016, doi: 10.1186/s13326-016-0056-2.[35]D. J. Wang, D. A. Boltz, J. McElhaney, J. A. McCullers, R. J. Webby, andR. G. Webster, â€œNoevidenceof alinkbetweeninfluenzavaccinesandGuillainâ€“Barresyndromeâ€“associated\nantigangliosideantibodies,â€InfluenzaOtherRespirViruses, vol. 6, no. 3, pp. 159â€“166, May2012, doi: 10.1111/j.1750-2659.2011.00294.x.[36]A. Cw, J. Bc, andL. Jd, â€œTheGuillain-BarrÃ©syndrome: atruecaseof molecularmimicry,â€Trendsinimmunology, vol. 25, no. 2, Feb. 2004, doi: 10.1016/j.it.2003.12.004.[37]C. Vellozzi, D. R. Burwen, A. Dobardzic, R. Ball, K. Walton, andP. Haber, â€œSafetyof trivalentinactivatedinfluenzavaccinesinadults: Backgroundforpandemicinfluenzavaccinesafetymonitoring,â€Vaccine, vol. 27, no. 15, pp. 2114â€“2120, Mar. 2009, doi:10.1016/j.vaccine.2009.01.125.[38]J. D. Grabenstein, â€œGuillain-BarrÃ©SyndromeandVaccination: UsuallyUnrelated,â€HospPharm, vol. 35, no. 2, pp. 199â€“207, Feb. 2000, doi: 10.1177/001857870003500214.[39]S. Vucic, M. C. Kiernan, andD. R. Cornblath, â€œGuillain-BarrÃ©syndrome: anupdate,â€JClinNeurosci, vol. 16, no. 6, pp. 733â€“741, Jun. 2009, doi: 10.1016/j.jocn.2008.08.033.[40]R. K. Yu, S. Usuki, andT. Ariga, â€œGangliosideMolecularMimicryandItsPathological RolesinGuillain-BarrÃ©SyndromeandRelatedDiseases,â€Infect Immun, vol. 74, no. 12, pp.6517â€“6527, Dec. 2006, doi: 10.1128/IAI.00967-06.[41]R. NatuvaandS. S. S. Puppala, â€œChat GPTâ€”aBoonorBanetoAcademicCardiology?,â€IndianJournal of Clinical Cardiology, p. 26324636231185644, Aug. 2023, doi:10.1177/26324636231185644.[42]W. HouandZ. Ji, â€œGeneTuringtestsGPTmodelsingenomics,â€bioRxiv, p.2023.03.11.532238, Mar. 2023, doi: 10.1101/2023.03.11.532238.[43]M. O. Topal, A. Bas, andI. vanHeerden, â€œExploringTransformersinNatural LanguageGeneration: GPT, BERT, andXLNetâ€.[44]M. Sallam, â€œChatGPTUtilityinHealthcareEducation, Research, andPractice: SystematicReviewonthePromisingPerspectivesandValidConcerns,â€Healthcare, vol. 11, no. 6, p.887, Mar. 2023, doi: 10.3390/healthcare11060887.[45]E. T. R. Schneider, J. V. A. deSouza, Y. B. Gumiel, C. Moro, andE. C. Paraiso, â€œAGPT-2LanguageModel forBiomedical TextsinPortuguese,â€in2021IEEE34thInternationalSymposiumonComputer-BasedMedical Systems(CBMS), Jun. 2021, pp. 474â€“479. doi:10.1109/CBMS52027.2021.00056.[46]A. Olmo, S. Sreedharan, andS. Kambhampati, â€œGPT3-to-plan: ExtractingplansfromtextusingGPT-3.â€2021.[47]A. GokaslanandV. Cohen, â€œOpenWebText Corpus.â€[48]T. Brownet al., â€œLanguageModelsareFew-Shot Learners,â€inAdvancesinNeuralInformationProcessingSystems, CurranAssociates, Inc., 2020, pp. 1877â€“1901. Accessed:Aug. 24, 2023. [Online]. Available:https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html[49]A. Koubaa, â€œGPT-4vs. GPT-3.5: AConciseShowdown,â€Preprints, Mar. 2023, doi:10.20944/preprints202303.0422.v1.[50]P. Budzianowski andI. VuliÄ‡, â€œHello, Itâ€™sGPT-2â€“HowCanI HelpYou?TowardstheUseofPretrainedLanguageModelsforTask-OrientedDialogueSystems.â€2019.[51]B. GhojoghandA. Ghodsi, â€œAttentionmechanism, transformers, BERT, andGPT: tutorialandsurvey,â€2020.[52]M. Vijayasarathi, A. S., andG. Tanuj, â€œApplicationof ChatGPTinmedical scienceandResearch,â€vol. 3, pp. 1480â€“1483, Jul. 2023, doi: 10.5281/zenodo.8170714.[53]K. Roy, Y. Zi, V. Narayanan, M. Gaur, andA. Sheth, â€œKnowledge-InfusedSelf AttentionTransformers.â€2023.[54]H. Touvronet al., â€œLlama2: OpenFoundationandFine-TunedChat Models.â€2023.[55]â€œOpenAI GPT2.â€https://huggingface.co/docs/transformers/model_doc/gpt2(accessedSep.08, 2023).[56]F. Tarlaci, GPT2sQA. 2023. Accessed: Sep. 06, 2023. [Online]. Available:\nhttps://github.com/ftarlaci/GPT2sQA[57]J. Austin, â€œTheBookof EndlessHistory: Authorial Useof GPT2forInteractiveStorytelling,â€inInteractiveStorytelling, R. E. Cardona-Rivera, A. Sullivan, andR. M. Young, Eds., inLectureNotesinComputerScience. Cham: SpringerInternational Publishing, 2019, pp.429â€“432. doi: 10.1007/978-3-030-33894-7_47.",
  "topic": "Adverse effect",
  "concepts": [
    {
      "name": "Adverse effect",
      "score": 0.6724140048027039
    },
    {
      "name": "Outbreak",
      "score": 0.6348143815994263
    },
    {
      "name": "Pandemic",
      "score": 0.5670410394668579
    },
    {
      "name": "Adverse Event Reporting System",
      "score": 0.4791388213634491
    },
    {
      "name": "Medicine",
      "score": 0.4314122498035431
    },
    {
      "name": "Environmental health",
      "score": 0.3744562268257141
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.33437928557395935
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.314123272895813
    },
    {
      "name": "Internal medicine",
      "score": 0.28793731331825256
    },
    {
      "name": "Virology",
      "score": 0.2745439410209656
    },
    {
      "name": "Disease",
      "score": 0.16220903396606445
    }
  ]
}