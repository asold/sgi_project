{
  "title": "Zero-shot evaluation reveals limitations of single-cell foundation models",
  "url": "https://openalex.org/W4409566559",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Kasia Z. Kedzierska",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2504675986",
      "name": "Lorin Crawford",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4317129157",
      "name": "Ava P. Amini",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2493585915",
      "name": "Alex X. Lu",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": null,
      "name": "Kasia Z. Kedzierska",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2504675986",
      "name": "Lorin Crawford",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4317129157",
      "name": "Ava P. Amini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2493585915",
      "name": "Alex X. Lu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W4388218765",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W2460637143",
    "https://openalex.org/W4392168151",
    "https://openalex.org/W4378806473",
    "https://openalex.org/W4384821687",
    "https://openalex.org/W4387122971",
    "https://openalex.org/W4389132297",
    "https://openalex.org/W4391305595",
    "https://openalex.org/W2728842897",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4312197262",
    "https://openalex.org/W4386629205",
    "https://openalex.org/W4391652655",
    "https://openalex.org/W2152692691",
    "https://openalex.org/W2901677030",
    "https://openalex.org/W2984472267",
    "https://openalex.org/W3032847236",
    "https://openalex.org/W3159800610",
    "https://openalex.org/W4362521490",
    "https://openalex.org/W4225598893",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W4388926373",
    "https://openalex.org/W4288350551",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4210587306",
    "https://openalex.org/W2951506174",
    "https://openalex.org/W4280596398",
    "https://openalex.org/W4280598116",
    "https://openalex.org/W2523369352",
    "https://openalex.org/W2526262591",
    "https://openalex.org/W2523419694",
    "https://openalex.org/W2523620612",
    "https://openalex.org/W2551194178",
    "https://openalex.org/W2472063172",
    "https://openalex.org/W6976632921",
    "https://openalex.org/W6920603232",
    "https://openalex.org/W6901871296",
    "https://openalex.org/W6920746209",
    "https://openalex.org/W6911789094"
  ],
  "abstract": "Abstract Foundation models such as scGPT and Geneformer have not been rigorously evaluated in a setting where they are used without any further training (i.e., zero-shot). Understanding the performance of models in zero-shot settings is critical to applications that exclude the ability to fine-tune, such as discovery settings where labels are unknown. Our evaluation of the zero-shot performance of Geneformer and scGPT suggests that, in some cases, these models may face reliability challenges and could be outperformed by simpler methods. Our findings underscore the importance of zero-shot evaluations in development and deployment of foundation models in single-cell research.",
  "full_text": "Zero‑shot evaluation reveals limitations \nof single‑cell foundation models\nKasia Z. Kedzierska1, Lorin Crawford2, Ava P . Amini2 and Alex X. Lu2*   \nBackground\nFoundation models are machine learning models pretrained on huge amounts of data, \nwhere the aim of the pretraining is to enable models to capture universal patterns in \ndata [1–3]. This foundational knowledge can either be used to specialize rapidly towards \nspecific tasks with a small amount of additional training, or used zero-shot where the \nmodel’s internal representation of input data—an “embedding”—is used for downstream \nanalysis with no further task-specific training.\nEmerging research in single-cell biology has garnered great interest in foundation \nmodels, which promise to automate tasks such as cell type identification and gene \nexpression prediction. Advances in pretraining and scaling foundation models mean that \nresearchers are now seeking to understand how large unlabeled datasets can be used \nto initialize models with a general understanding of biology, and initiatives like CELLx -\nGENE [4] are rising to this data demand. These developments have spurred a variety of \nproposed foundation models [5–12], all of which pretrain on large cell datasets.\nCurrent proposed foundation models predominantly rely on fine-tuning, with lim -\nited exploration in zero-shot settings. However, evaluation standards for pretrained \nmodels in other biological domains, including protein sequences and biomedical \nAbstract \nFoundation models such as scGPT and Geneformer have not been rigorously evalu-\nated in a setting where they are used without any further training (i.e., zero-shot). \nUnderstanding the performance of models in zero-shot settings is critical to applica-\ntions that exclude the ability to fine-tune, such as discovery settings where labels are \nunknown. Our evaluation of the zero-shot performance of Geneformer and scGPT \nsuggests that, in some cases, these models may face reliability challenges and could be \noutperformed by simpler methods. Our findings underscore the importance of zero-\nshot evaluations in development and deployment of foundation models in single-cell \nresearch.\nKeywords: Foundation models, Single-cell, Machine learning\nOpen Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nSHORT REPORT\nKedzierska et al. Genome Biology          (2025) 26:101  \nhttps://doi.org/10.1186/s13059‑025‑03574‑x\nGenome Biology\n*Correspondence:   \nlualex@microsoft.com\n1 University of Oxford, Oxford, UK\n2 Microsoft Research, Cambridge, \nMA, USA\nPage 2 of 13Kedzierska et al. Genome Biology          (2025) 26:101 \nimages, argue that zero-shot evaluation is critical to understanding if pretraining \ndevelops a transferrable understanding of biology [13– 16]. Indeed, recent research in \nprotein language models has exposed numerous trivial mechanisms in which transfer \nlearning can appear to boost performance on downstream tasks, but does not actually \nrely upon sophisticated learning from pretraining [17].\nThe significance of zero-shot evaluation is particularly pronounced in single-cell \nbiology, where many tasks are exploratory and lack predefined labels that limit the \nfeasibility of fine-tuning. This context underscores a growing need to focus on robust \nzero-shot performance in the field. Despite its criticality for potential applications, \nzero-shot evaluation remains infrequent among single-cell foundation models.\nIn this work, we perform zero-shot evaluations of two popular proposed single-\ncell foundation models, Geneformer [6 ] and scGPT [7 ] (Fig.  1A). Our evaluations \nare motivated by the authors’ claims that their proposed models not only generate \nrobust cell embeddings [6 ] but also exhibit strong capabilities for generalizing to \nunseen datasets [7 ]. In our work, we test this claim and show that even with a set of \nbenchmarks representing the most favorable setting where datasets consist of tissues \nand are generated using technologies similar to those used to pretrain these models \n(Additional file 2: Figs. S1 and S2), both Geneformer and scGPT underperform sim -\npler methods. We show that zero-shot evaluation of these models exposes vulnerabil -\nities that are not evident if evaluated with fine-tuning alone. Our results highlight the \nimportance of zero-shot evaluation as a critical step in the development and deploy -\nment of foundation models for single-cell biology.\nFig. 1 Evaluation of the cell embedding space generated by the models. A Overview of the evaluation \nsetup. We compare Geneformer and scGPT to scVI, Harmony, and the selection of highly variable genes \n(HVG) on five diverse datasets. B Average BIO score for HVG and embeddings from Harmony, scVI, scGPT, \nand Geneformer. C, D Visualization of the UMAP projections of the Pancreas (16k) dataset using the cell \nembedding space generated by the models. Cells are color-coded by cell type (C) and batch (D). E Average \nbatch score for HVG and embeddings from Harmony, scVI, scGPT, and Geneformer. Dashed line in B and E \nsignifies the median calculated across the datasets\nPage 3 of 13\nKedzierska et al. Genome Biology          (2025) 26:101 \n \nResults and discussion\nBoth scGPT and Geneformer produce cell embeddings intended to project potentially \nnoisy gene expression measurements to a more biologically relevant latent space [18–\n21], and then these cell embeddings are fine-tuned for cell type classification. However, \nthis fine-tuning strategy fails in more exploratory contexts where cell composition in the \ndataset may not be known; in these settings, foundation models must produce robust \ncell embeddings zero-shot. We evaluated the zero-shot performance of scGPT and Gen-\neformer in separating known cell types across multiple datasets (Fig.  1B). Both models \nperform worse than selecting highly variable genes (HVG) and using more established \nmethods such as Harmony and scVI in cell type clustering, as measured by average BIO \n(AvgBio) score (Fig.  1B, Additional file 1: Table S1). Taking into account only the aver -\nage silhouette width (ASW) metric, the established baselines still outperform scGPT \nand Geneformer (Additional file 2: Fig. S3, Additional file 1: Table S1). scGPT’s perfor -\nmance is better on the PBMC (12k) dataset compared to scVI, Harmony, and HVG; but \nit is worse than both scVI and Harmony with respect to AvgBIO score on other datasets \n(Fig. 1B, Additional file 1: Table S1). scGPT is comparable to scVI on the Tabula Sapiens, \nPancreas, and PBMC (12k) datasets and outperforms Harmony on the Tabula Sapiens \ndatasets with respect to ASW score (Additional file 2: Fig. S3, Additional file 1: Table S1). \nNotably, HVG outperforms Geneformer and scGPT across all metrics (Additional file 2: \nFig. S4, Additional file 1: Tables S1 and S2).\nGiven variable performance across datasets, we sought to understand whether this \nvariability could be explained by potential overlap between the datasets used for evalu -\nation versus for pretraining. We find a partial overlap between the Pancreas dataset and \nthe pretraining sets used for Geneformer. Additionally, the Tabula Sapiens and Immune \ndatasets were included in the scGPT pretraining dataset (Additional file  1: Table  S3). \nHowever, scGPT and Geneformer do not consistently outperform baselines on datasets \nalready seen during pretraining, and the only dataset not seen during pretraining where \nscGPT outperforms both baselines is the the PBMC 12k study.\nNext, to evaluate the impact of the pretraining dataset on zero-shot performance in \ncell type clustering, we tested four different variants of scGPT: randomly initialized \nscGPT, and scGPT pretrained on 814,000 kidney cells (scGPT kidney), on 10.3 million \nblood and bone marrow cells (scGPT blood), and on 33 million non-cancerous human \ncells (scGPT human) (Additional file  2: Fig. S5). We note that the smaller models are \ntrained on tissue-specific data, confounding if differences in performance are due to size \nor the composition of dataset. However, at a minimum, scGPT human includes data \nused to pretrain scGPT blood and scGPT kidney. Our analysis indicates that pretrain -\ning provides a clear improvement in cell-type clustering on the PBMC (12k) dataset, and \nthat the median score, calculated across datasets, for the three scGPT models is greater \nthan that of the random baseline (Additional file 2: Fig. S5, Additional file 1: Table S1). \nWe also observe that scGPT human and blood improve over random models for at least \nsome datasets where scGPT kidney fails to, including the Immune and Tabula Sapiens \ndatasets, suggesting that performance may improve with larger pretraining datasets. \nSurprisingly, scGPT human slightly underperforms scGPT blood, even for datasets \ninvolving tissue types beyond blood and bone marrow cells (Additional file  2: Fig. S5, \nAdditional file 1: Table S4).\nPage 4 of 13Kedzierska et al. Genome Biology          (2025) 26:101 \nOverall, our findings demonstrate that scGPT and Geneformer in zero-shot configura-\ntions perform inconsistently compared to cell embeddings derived from HVG or gen -\nerated using scVI or Harmony. Evaluating variants of the scGPT model highlights that \nwhile pretraining confers some benefit, beyond a certain limit, larger and more diverse \ndatasets may no longer confer additional benefits. Additionally, models did not perform \nwell on datasets seen during pretraining, indicating an unclear relationship between the \npretraining objective and cell type clustering.\nWe next evaluated the zero-shot capabilities of these proposed single-cell foundation \nmodels in batch integration (Fig. 1C–E), a common task in single-cell analysis where the \ngoal is to eliminate batch effects from multiple data sources without removing mean -\ningful biological differences [22–24]. We first visualized the embeddings from scGPT, \nGeneformer, and the other baselines on the Pancreas benchmark dataset, which includes \ndata from five different sources [25] (Fig.  1C–D). While Geneformer and scGPT-human \ncan integrate different experiments conducted with the same experimental technique, \nthey generally fail to correct for batch effects between techniques. Qualitatively, Gen -\neformer’s cell embedding space fails to retain information about cell type, and any clus -\ntering is primarily driven by batch effects. While scGPT’s cell embedding space offers \nsome separation between cell types, the primary structure in the dimensionality reduc -\ntion is still driven by batch effects (Fig.  1C–D). Harmony and scVI largely succeed in \nintegrating the Pancreas dataset.\nQuantitative evaluation with batch integration metrics revealed that Geneformer \nunderperforms relative to scGPT, Harmony, scVI, and HVG across most datasets \n(Fig.  1E). scVI and Harmony both outperform scGPT in datasets where the batch is \nrestricted to technical variation (Pancreas and PBMC), but they each are outperformed \nby scGPT on more complex datasets where both technical and biological (i.e., varia -\ntion between donors) batch effects are present (Tabula Sapiens and Immune datasets, \nrespectively; Fig.  1E, Additional file  2: Fig. S6). We note that both the Immune and \nTabula Sapiens datasets were used in pretraining scGPT, underscoring a limitation of \nour evaluation—we cannot disentangle if these improvements are potentially because \nthese datasets were seen in pretraining. Although the evaluation scores for batch mix -\ning vary—with Harmony ranking last for batch integration but second for principal \ncomponent regression (PCR) score, which is expected given that Harmony adjusts the \nPC embeddings to correct batch effects—Geneformer consistently ranks at the bottom \nacross all metrics (Additional file 1: Table S2).\nThe Tabula Sapiens dataset poses significant challenges for Harmony, particularly in \nterms of PCR scores, while the Immune datasets present similar difficulties for scVI \n(Additional file 2: Fig. S6B). Geneformer’s embeddings across all datasets show a higher \nproportion of variance explained by batch effects compared to the original data, indi -\ncating inadequate batch mixing (Additional file 2: Fig. S6B). Consequently, Geneformer \nconsistently ranks last in terms of batch mixing scores (Additional file 1: Table S2), high-\nlighting its limitations in effectively handling batch effects compared to other models.\nSurprisingly, the best batch integration scores for all datasets were achieved by \nselecting HVG (Additional file 1: Table S2). This observation is slightly different than \nour qualitative observations (Fig.  1C–D) and can be explained by differences in our \nranking metrics being calculated in full, rather than reduced, dimensions (Additional \nPage 5 of 13\nKedzierska et al. Genome Biology          (2025) 26:101 \n \nfile 2: Fig. S6A, Additional file 1: Table S5.) However, the proposed foundation models \nunderperform compared to both the Harmony and scVI in full and reduced dimen -\nsions (Additional file 1: Tables S4 and S6).\nWe propose two hypotheses as to why Geneformer and scGPT underperform zero-\nshot compared to the tested baselines. First, it could be that the masked language \nmodel pretraining framework used by both scGPT and Geneformer does not produce \nuseful cell embeddings. Second, it could be that scGPT and Geneformer have failed \nto learn the pretraining task. To understand this distinction, we evaluated the perfor -\nmance of these models on the gene expression reconstruction pretraining task (for \nscGPT, the bin value for each gene; for Geneformer, the gene rankings) (Fig.  2, Addi-\ntional file 2: Fig. S7).\nWe find that both models face challenges in reconstructing gene expression (Fig.  2). \nWithout conditioning on its cell embedding, scGPT predicts the mean value of the \ninput bin across all binned values (Fig.  2A). Predictions improve when conditioned \non cell embeddings, particularly for higher input values (Fig.  2B). Under its masked \nlanguage modeling (MLM) objective, Geneformer predicts the most likely gene at a \ngiven position. Although there is good agreement across many genes, there are cases \nin which Geneformer predicts genes absent from the input (Fig.  2C, Additional file 2: \nFig. S7).\nNotably, scGPT without embeddings underperforms against a naive baseline of pre -\ndicting the mean, and only marginally improves with the cell embeddings (Fig.  2D). \nGeneformer’s gene ranking exhibits modest correlations with the actual true rank -\nings, with median and best correlations of 0.56 and 0.95, respectively, across datasets \n(Fig. 2E, Additional file 2: Fig. S7).\nFig. 2 Performance comparison of scGPT and Geneformer in gene expression reconstruction. A–C \nReconstruction of the expression in the Immune (330k) dataset: A scGPT gene expression prediction (GEP) \nunder the masked language modeling (MLM) objective. B scGPT gene expression prediction from cell \nembeddings (GEPC). C Geneformer MLM output of the predicted expression ranking (y-axis) versus the true \ninput expression ranking (x-axis). D Mean squared error (MSE) comparison for scGPT objectives. Mean and \nstandard deviation range are shown as points and solid lines, respectively, and a median MSE for mean-based \nreconstruction is shown as a dashed line. E Pearson’s correlation of input and predicted expression ranking \nfor both Geneformer and for average gene rankings. Mean and standard deviation range are shown as points \nand solid lines, respectively, and a median correlation for average ranking is shown as a dashed line\nPage 6 of 13Kedzierska et al. Genome Biology          (2025) 26:101 \nConclusions\nThis work presents an evaluation of two single-cell foundation models, Geneformer \nand scGPT, in zero-shot settings. Our findings indicate that both models, in their \ncurrent form, do not consistently outperform simpler baselines and face challenges \nin dealing with batch effects. This is in spite of these models reporting strong per -\nformance when fine-tuned, as exhibited in their original papers, demonstrating that \nzero-shot evaluation can reveal vulnerabilities that are not evident if models are \nexclusively evaluated with fine-tuning.\nOur findings raise questions about the general suitability of MLM for learning sin -\ngle-cell embeddings. While scGPT does not outperform averaged bin prediction and \nstruggles with accurate gene expression prediction in our evaluation datasets, Gen -\neformer shows relative strength in blood datasets but underperforms in others. Addi -\ntionally, we demonstrate that larger pretraining datasets do not always increase the \nperformance of scGPT, and that datasets seen in pretraining still have poor cell type \nclustering performance. These observations suggest that improved pretraining tasks \nthat enhance the representation of genes and gene expression within these models \nmight be a viable path for improvement.\nRecognizing the rapid advancement of the field, our study adopts a focused \napproach. We concentrated on specific models and selected a limited array of datasets \nthat do not exhaustively represent all applications of single-cell analysis (for example, \nour datasets only include transcriptomic data). While a timely benchmark of all pro -\nposed single-cell foundation models and applications is not realistic given the fast-\npaced nature of the field, the aim of our work is to inform the development of future \nmodels by showing the importance of zero-shot evaluation.\nIn fact, some more recent advances occurring at time of our work have already \nsought to address limitations exposed by our analysis, for example, in improving the \nrepresentation of genes by transferring knowledge from proteins [11]. However, in \nmany cases methods do not make their pretraining code publicly available [11, 12]. \nOur results should be viewed as an initial exploration, highlighting specific areas for \nimprovement and further research. We hope that our work helps shape evaluation \npractices in the emerging intersection of single-cell biology and foundation models.\nOne limitation of our analyses was that some of our evaluation datasets were used \nin pretraining, confounding our ability to say if performance trends are general or due \nto prior exposure to specific datasets. While this issue can be mitigated for individual \nmodels with strategies like identifying datasets released after a model was pretrained, \nthis strategy may not permit a comparative analysis of older models with newer ones, \nas newer models may also subsume newer data for pretraining. To tackle this, we \npropose the creation of benchmark tasks and datasets reserved exclusively for model \nevaluation that should never be used to pretrain any future model, as has been done \nfor large language models (LLMs) [26– 29]. These datasets should be a representative \nyet separate subset of the broader available data, ensuring an unbiased and effective \nassessment of model performance. The curation of biologically grounded, standalone \nbenchmark datasets would provide a substantial advancement towards more reliable \nand robust evaluation methods.\nPage 7 of 13\nKedzierska et al. Genome Biology          (2025) 26:101 \n \nBuilding on the analyses presented in this manuscript, it would be beneficial to further \nexplore the specific outputs provided by models like Geneformer and scGPT. Unlike \ntraditional methods that output batch-corrected counts, Geneformer outputs rankings, \nand scGPT generates binned counts. Determining how effectively these models adjust \nfor batch variations is a challenging yet an important investigation—particularly since \ncorrelating their batch correction efficacy with performance in downstream applica -\ntions, such as perturbation predictions, could provide deeper insights into the practical \nutility of the proposed foundation models.\nAnother promising avenue for future research on improving these models, closely \naligned with current trends in the field, is a more in-depth exploration of how they dis -\ncern gene-gene interactions. Careful evaluation of this aspect is essential but requires \nthoughtful consideration and ideally should be paired with targeted in vitro lab exper -\niments. These experiments are crucial for establishing a ground truth, which, in turn, \nenables an accurate assessment of the models’ true capabilities in unraveling complex \nbiological interactions. A collaborative approach that combines computational evalua -\ntion with experimental validation can significantly strengthen and enhance the quality of \nwork in designing these models.\nOverall, foundation models hold significant promise for automating cell type annota -\ntion and gene expression prediction, presenting an opportunity to transform how bio -\nlogical data is analyzed and interpreted. Beyond technical advancements, these models \nhave the potential to democratize scientific research by enabling groups with limited \ncomputational resources to access advanced analytical tools. The challenges identified \nin this study underscore the necessity of a meticulous evaluation of proposed models in \nzero-shot settings, ensuring that models are not only technically sound but also prac -\ntically applicable. We believe that more focused evaluations, particularly in zero-shot \ncontexts, will be instrumental to the methodological development and deployment of \nfoundation models in single-cell research.\nMethods\nModels and baselines\nWe evaluated two proposed foundation models for single-cell transcriptomics: Gene -\nformer [6] and scGPT [7]. We chose these models because they offer pretrained weights \nand have been trained using unsupervised objectives on extensive datasets (ca. 30M \nsingle-cell transcriptomes). Several other possible models did not have publicly avail -\nable weights at the time of evaluation. Here, we provide an overview of Geneformer and \nscGPT, including their practices for extracting cell embeddings (i.e., latent representa -\ntions of single-cells) which we follow for our analyses.\nBoth models accept single-cell gene expression vectors as input but represent data dif -\nferently. The input to the Geneformer model is a ranked list where a gene’s position rep -\nresents its expression relative to the remaining genes in the cell. The model leverages \na BERT-inspired architecture with 6 Transformer layers, each with 4 attention heads. \nGeneformer is trained using a modification of the masked language modeling (MLM) \ntask, where the model is trained to recover randomly selected genes that are masked \nor corrupted. Since genes are ordered by their expression, this effectively predicts gene \nexpression relative to other genes. The model outputs gene embeddings, which are \nPage 8 of 13Kedzierska et al. Genome Biology          (2025) 26:101 \nsubsequently decoded into gene predictions. A cell embedding is calculated by averaging \nover all gene embeddings extracted for that cell. Genefomer was pretrained on 27.4M \nhuman single-cell transcriptomes (excluding malignant and immortalized cells).\nscGPT preprocesses each gene expression vector by independently binning values into \n50 equidistant bins where the lowest bin is the lowest expression and the highest bin \ncorresponds to the highest expression. Next, the binned values and the gene tokens (i.e., \na unique index for each gene) are separately embedded and summed in the embedding \nspace—jointly representing the gene and its binned expression. Like Geneformer, scGPT \nuses an MLM task. However, scGPT directly learns a cell embedding, which is integrated \ninto its pretraining loss of predicting masked genes: scGPT first predicts a masked gene \nexpression bin and a cell embedding from unmasked genes; then, in a second step, it \nfurther iteratively refines masked gene expression using the cell embedding predicted in \nthe first step. This means that scGPT outputs two sets of binned gene predictions in its \npretraining task, first from unmasked genes alone and second from conditioning on the \ncell embedding. In our effort to understand the generalization of the pretraining objec -\ntives, we analyzed both. Finally, compared to Geneformer, scGPT has 3 × the number \nof parameters, using 12 Transformer layers with 8 attention heads. scGPT is available \nin several variants, each pretrained on multiple different datasets. In our analyses, we \nfocused on three variants of scGPT pretrained on 814,000 kidney cells (scGPT kidney), \non 10.3 million blood and bone marrow cells (scGPT blood), and on 33 million non-\ncancerous human cells (scGPT human).\nFor baselines in evaluating cell embeddings, we compared Geneformer and scGPT \nagainst selecting highly variable genes (HVG). We standardize to 2000 HVG across all \nexperiments. In addition, we compared all methods to scVI, a scalable generative model \n[20], and Harmony, a method for adjusting shared embedding space [21], which we \ntrained on each individual dataset. While this means that we deploy scGPT and Gen -\neformer zero-shot but train scVI and Harmony on target data, we reason this set-up \nreflects practical settings where resources are often more readily available to train light -\nweight models than to fine-tune larger ones. Importantly, both Harmony’s and scVI’s \ndesign inherently incorporates batch labels in its training process. This aspect of the \nbaselines leverages its capability to handle batch effects directly. In contrast, Geneformer \nand scGPT are not explicitly pretrained with batch labels. Instead, they aim to learn to \nmitigate batch effects indirectly through exposure to a vast diversity of cells during their \npretraining phase. For the evaluation of the pretraining objective, we used the mean esti-\nmates or average ranking as a reference.\nDatasets\nTo assess the quality of cell embeddings and performance on batch integration tasks, \nwe used five distinct human tissue datasets (Additional file 1: Table S7). These datasets \ninclude samples from the pancreas [25], two sets of peripheral blood mononuclear cells \n(PBMCs) [30, 31], a cross-tissue immune cell atlas [32], and a multi-organ human cell \natlas [33]. Each dataset poses unique challenges relevant to single-cell analysis, such as \nthe distinction between well-defined and less well-defined cell type clusters, the integra -\ntion of different technical batches within the same tissue (Additional file 2: Fig. S2), and \nthe unification of data across multiple tissues (Additional file 2: Fig. S1). The overview of \nPage 9 of 13\nKedzierska et al. Genome Biology          (2025) 26:101 \n \nthe datasets used for pretraining of the models and the datasets used in this manuscript \nare discussed below in Data availability.\nAmong the selected datasets, the Pancreas dataset partially overlapped with the data \nused to pretrain Geneformer (version with GEO ID: GSE84133 was included in the \npretraining). We conducted evaluations using both the complete Pancreas dataset and \nits non-overlapping subset. The results were consistent between the two, leading us to \ninclude the entire Pancreas dataset for simplicity in this evaluation (Additional file  1: \nTables S1 and S5). The Tabula Sapiens and Immune were included in the CellxGene col-\nlection [4] (May 2023 census) used for scGPT pretraining.\nEvaluation metrics\nIn this work, we evaluated the cell embedding space for its ability to separate known \ncell types correctly and to integrate different batches. For our evaluations, we largely fol-\nlowed the approach described in Luecken et al. [25] which included selecting the specific \nscores best fitting to the evaluated task (see the description below). We also evaluated \nthe performance of the models at the pretraining task by evaluating their reconstruction \naccuracy.\nBiological preservation scores\nOne key aspect of evaluating cell embeddings is the degree to which cell types are dis -\ntinct within the embedding space. To assess this, we employ metrics based on Average \nSilhouette Width (ASW) [25] and Average Bio (AvgBIO) scores [7]. Briefly, ASW is com-\nputed by taking the difference of the between-cluster and within-cluster distances and \ndividing this by the larger of the two values. ASW is normalized to a range between 0 \nand 1, where 0 signifies strong within-cluster cohesion, 0.5 indicates overlapping clus -\nters, and 1 denotes well-separated clusters. Higher ASW indicates better performance in \nseparating clusters. AvgBIO is the arithmetic mean of three individual metrics [7]: ASW, \nNormalized Mutual Information (NMI), and Adjusted Rand Index (ARI). NMI and ARI \nare calculated based on Louvain clusters generated directly from the embedding space as \ndescribed in Luecken et al. Briefly, the clustering is calculated across 20 resolutions, the \nresolution for which the clustering reported highest NMI metric is chosen. The cluster -\ning is then generated with the selected resolution and NMI and ARI metrics are then \nreported. AvgBIO is normalized to a unit scale, with higher values indicating better \nalignment between clusters and ground truth labels.\nBatch mixing scores\nTo evaluate the extent of the mixing of the batches, we used a variation of the average \nsilhouette score that we call batch integration score (average silhouette width score with \nrespect to batch averaged across cell types) and the PCR score (as described in [25]). \nBriefly, silhouette scores are calculated for each cell type with respect to the batch \nlabel by taking only its absolute value, where a score of 0 is equivalent to absolute mix -\ning and any deviation from 0 indicates the presence of a batch effect. To keep with the \nused convention, the score is then subtracted from 1, resulting in final scores on a scale \nbetween 0 and 1, where a final score of 0 suggests complete separation of the batches \nand strong batch effect while 1 signifies a perfect batch mixing and integration. The \nPage 10 of 13Kedzierska et al. Genome Biology          (2025) 26:101 \nprincipal component regression (PCR) score compares the proportion of the variance \nthat is explained by the batch variable between the original dataset and the embeddings \nof the model. We additionally defined a counterpart to AverageBIO score—the Average \nBatch Score—which is computed as the arithmetic mean between the PCR and batch \nintegration scores.\nReconstructing gene expression\nIn pretraining, both models select a percentage of genes to mask; thus, this evaluation \nrequired selection of how many genes were masked in an input. To eliminate stochastic-\nity in sampling and to recapitulate the maximally informative setting, we used all  genes \nunmasked as input. We evaluated the models on the same datasets as in the cell embed -\nding tasks. To evaluate the performance of scGPT in its pretraining objective, we used \nthe mean squared error (MSE), as used by the original authors for the model’s loss [7]. \nTo evaluate Geneformer’s performance in its pretraining objective, we measured Pear -\nson’s correlation between the true and predicted rankings. For that, we transformed \nordered outputs into scaled (from 0 to 1) rankings, where the highest expressed genes \nwere assigned a rank of 1. Geneformer can output a sequence of up to 2048 genes and, \nwhen input is passed in batches, the model outputs the sequence of the length equal to \nthat of the longest input. In our evaluations, we limit the output sequence to the length \nof the input sequence.\nSupplementary information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s13059- 025- 03574-x.\nAdditional file 1: Supplementary Tables S1-S7. Table S1: Scores for cell embeddings generated by the models. \nScores calculated based on the whole dataset, as well in 10 repeats of subsampled for 10% of the dataset. Table S2: \nRankings of median scores across datasets. Rankings are based on the median scores obtained from the evalu-\nated datasets. A lower rank indicates better performance, highlighted in blue, while a higher rank signifies poorer \nperformance, highlighted in red. Table S3: Source of the datasets and overlap with the data used for pretraining of \nthe models. Table S4: Average ranking for the evaluated models. Rankings were calculated based on mean scores for \nbiological integrationas well as batch mixing. A lower rank indicates better performance, highlighted in blue, while \na higher rank signifies poorer performance, highlighted in red. Table S5: Scores for UMAP of the cell embeddings \ngenerated by the models. Scores calculated based on the whole dataset, as well in 10 repeats of subsampled for 10% \nof the dataset. Table S6: Average ranking for the UMAP embeddings of the embeddings generated by the evaluated \nmodels. Rankings were calculated based on mean scores for biological integrationas well as batch mixing. A lower \nrank indicates better performance, highlighted in blue, while a higher rank signifies poorer performance, highlighted \nin red.mean scores for biological integrationas well as batch mixing. A lower rank indicates better performance, \nhighlighted in blue, while a higher rank signifies poorer performance, highlighted in red. Table S7: Overview of the \nused datasets.\nAdditional file 2: Supplementary Figs. S1-S9. Fig. S1: Tissue composition of the multi-tissue datasets. The tissue com-\nposition of the collections used for pretraining is depicted in the upper panel: A Geneformer and B scGPT. Below, the \nmulti-tissue datasets utilized in evaluations are visualized: C Immuneand D Tabula Sapiens. Fig. S2: Technology distri-\nbution across datasets. Fig. S3: Proposed single-cell foundation models fail to outperform cell embeddings derived \nfrom HVG or generated using the scVI model. Average BIO score calculated on the highly variable genesof the log \nnormalized input data and on the embeddings extracted from scVI, scGPT, and Geneformer models. Median value \nannotated with a dashed line. A higher score indicates better performance in separating clusters. Fig. S4: UMAP pro-\njection of the embeddings improves cell type separation for scVI. Scores used for assessing cell types separation in \ncell embedding space across raw and UMAP projected embeddings. A Average silhouette widthscore, B Normalized \nmutual informationscore, C Adjusted randscore and D Average BIOscore - an average of the other scores. The higher \nthe score – the better the performance of the model. Fig. S5: Size of the pretraining dataset correlates with the \nperformance at separating the cell types in cell embedding space. Average BIO score calculated on the embeddings \nextracted from selected variations of the scGPT models. The dashed line marks the median score across datasets. Fig. \nS6: UMAP projection of the embeddings results in lower batch integration scores. Scores used for assessing batch \nintegration in cell embedding space across raw and UMAP projected embeddings. A Batch integrations score based \non Average silhouette widthfor batch and label, and B Principal component regressionscore. The higher the score – \nthe better the performance of the model. Fig. S7: Performance of the proposed foundation models with respect to \nreconstructed expression binor ranking. The predicted bins as a function of input bins for scGPT GEP , scGPT GEPCand \nPage 11 of 13\nKedzierska et al. Genome Biology          (2025) 26:101 \n \nthe agreement between input and output rankings for Geneformerfor A Pancreas, B PBMC, C PBMCD ImmuneE \nTabula Sapiensdatasets shown. Fig. S8: Pearson’s correlation of input and Geneformer’s predicted expression rankings \nwith respect to fraction of masked input tokens. Shown here are results for Pancreasand PBMCdatasets. Fig. S9: Pear-\nson’s correlation of input and Geneformer’s predicted expression rankings with respect to fraction of masked input \ntokens. Shown here are results for PBMC, Immuneand Tabula Sapiensdatasets.\nAdditional file 3: Review history.\nAcknowledgements\nWe thank members of the Biomedical Machine Learning (BioML) group at Microsoft Research for insightful comments \nand suggestions on earlier versions of this manuscript. We also thank Zeinab Navidi, Giovanni Palla, and Sevahn Vorper-\nian for helpful discussions.\nPeer review\nKevin Pang, Veronique van den Berghe, and Claudia Feng were the primary editors of this article and managed its edito-\nrial process and peer review in collaboration with the rest of the editorial team.\nReview history\nThe review history is available as Additional file 3.\nAuthors’ contributions\nK.Z.K., L.C., A.P .A., and A.X.L. conceived and designed the project. K.Z.K. wrote the computer code, analyzed the data, and \nwrote the first manuscript draft. L.C., A.P .A., and A.X.L. supervised the research. All authors revised and edited the manu-\nscript. All authors read and approved the final manuscript.\nFunding\nComputational resources used in this research were supported by Microsoft and by the Wellcome Trust Core Award \nGrant Number 203141/Z/16/Z and the NIHR Oxford BRC. The views expressed are those of the author(s) and not neces-\nsarily those of the NHS, the NIHR, or the Department of Health.\nData availability\nThe Pancreas dataset [34–39] was downloaded from Figshare [40]. The PBMC (12k) dataset was accessed via the data.\npbmc_dataset function from the scvi-tools [30] Python package. The PBMC (95k) data [31] was downloaded \nfrom the 10x dataset website at http:// suppo rt. 10xge nomics. com/ single- cell/ datas ets. The Immune dataset was \ndownloaded from the Cross-tissue Immune Cell Atlas website at https:// www. tissu eimmu necel latlas. org/ [32] and the \nTabula Sapiens dataset was downloaded as the TabulaSapiens.h5ad.zip file from the Figshare Tabula Sapiens \nproject https:// figsh are. com/ proje cts/ Tabula_ Sapie ns/ 100973 [41–43]. The code with data downloads and preprocess-\ning steps is available in the GitHub repository.\nCode availability\nThe code used in our analyses is available at https:// github. com/ micro soft/ zero- shot- scfou ndati on [44] and has also \nbeen archived on Zenodo [45]. It is released under the MIT License. In our analyses, we used version 0.1.6 (Sep 14, 2023) \nof the scGPT [7] code, and the version of Geneformer [6] corresponding to commit 5 d0082c (Nov 21, 2023). To com-\npute described metrics, including those involving clustering within the embedding space, we used version 1.0.4 of \nthe scib [25] package. Additionally, we have published a docker image at DockerHub (accessible via kzked ziers ka/ sc_ \nfound ation_ evals) with the full environment setup.\nDeclarations\nEthical approval and consent to participate\nNot applicable.\nCompeting interests\nA.X.L., L.C., and A.P .A. are employees of and hold equity in Microsoft.\nReceived: 23 February 2024   Accepted: 9 April 2025\nReferences\n 1. Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, et al. On the opportunities and risks of foundation \nmodels. arXiv. 2022. ArXiv:2108.07258 [cs]. https:// doi. org/ 10. 48550/ arXiv. 2108. 07258.\n 2. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P , et al. Language models are few-shot learn-\ners. arXiv. 2020. ArXiv:2005.14165 [cs]. https:// doi. org/ 10. 48550/ arXiv. 2005. 14165.\n 3. Ramesh A, Pavlov M, Goh G, Gray S, Voss C, Radford A, et al. Zero-shot text-to-image genera-\ntion. arXiv. 2021. ArXiv:2102.12092 [cs]. https:// doi. org/ 10. 48550/ arXiv. 2102. 12092.\n 4. Program CSCB, Abdulla S, Aevermann B, Assis P , Badajoz S, Bell SM, et al. CZ CELL×GENE Discover: a single-\ncell data platform for scalable exploration, analysis and modeling of aggregated data. bioRxiv. 2023. Pages: \n2023.10.30.563174 Section: New Results. https:// doi. org/ 10. 1101/ 2023. 10. 30. 563174.\nPage 12 of 13Kedzierska et al. Genome Biology          (2025) 26:101 \n 5. Yang F, Wang W, Wang F, Fang Y, Tang D, Huang J, et al. scBERT as a large-scale pretrained deep language model \nfor cell type annotation of single-cell RNA-seq data. Nat Mach Intell. 2022;4(10):852–66. Number: 10 Publisher: \nNature Publishing Group. https:// doi. org/ 10. 1038/ s42256- 022- 00534-z.\n 6. Theodoris CV, Xiao L, Chopra A, Chaffin MD, Al Sayed ZR, Hill MC, et al. Transfer learning enables predictions in \nnetwork biology. Nature. 2023;618(7965):616–24. Number: 7965 Publisher: Nature Publishing Group. https:// doi.  \norg/ 10. 1038/ s41586- 023- 06139-9.\n 7. Cui H, Wang C, Maan H, Pang K, Luo F, Duan N, et al. scGPT: toward building a foundation model for single-cell \nmulti-omics using generative AI. Nat Methods. 2024;21(8):1470–80. Publisher: Nature Publishing Group. https://  \ndoi. org/ 10. 1038/ s41592- 024- 02201-0.\n 8. Hao M, Gong J, Zeng X, Liu C, Guo Y, Cheng X, et al. Large scale foundation model on single-cell transcriptomics. \nbioRxiv. 2023. Pages: 2023.05.29.542705 Section: New Results. https:// doi. org/ 10. 1101/ 2023. 05. 29. 542705.\n 9. Heimberg G, Kuo T, DePianto D, Heigl T, Diamant N, Salem O, et al. Scalable querying of human cell atlases via \na foundational model reveals commonalities across fibrosis-associated macrophages. bioRxiv. 2023. Pages: \n2023.07.18.549537 Section: New Results. https:// doi. org/ 10. 1101/ 2023. 07. 18. 549537.\n 10. Yang X, Liu G, Feng G, Bu D, Wang P , Jiang J, et al. GeneCompass: deciphering universal gene regulatory mecha-\nnisms with knowledge-informed cross-species foundation model. bioRxiv. 2023. Pages: 2023.09.26.559542 \nSection: New Results. https:// doi. org/ 10. 1101/ 2023. 09. 26. 559542.\n 11. Rosen Y, Roohani Y, Agarwal A, Samotorčan L, Consortium TS, Quake SR, et al. Universal cell embeddings: a \nfoundation model for cell biology. bioRxiv. 2023. Pages: 2023.11.28.568918 Section: New Results. https:// doi. org/  \n10. 1101/ 2023. 11. 28. 568918.\n 12. Bian H, Chen Y, Dong X, Li C, Hao M, Chen S, et al. scMulan: a multitask generative pre-trained language model \nfor single-cell analysis. bioRxiv. 2024. Pages: 2024.01.25.577152 Section: New Results. https:// doi. org/ 10. 1101/  \n2024. 01. 25. 577152.\n 13. Ando DM, McLean CY, Berndl M. Improving phenotypic measurements in high-content imaging screens. \nbioRxiv. 2017. Pages: 161422 Section: New Results. https:// doi. org/ 10. 1101/ 161422.\n 14. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, et al. Highly accurate protein structure predic-\ntion with AlphaFold. Nature. 2021;596(7873):583–9. Number: 7873 Publisher: Nature Publishing Group. https://  \ndoi. org/ 10. 1038/ s41586- 021- 03819-2.\n 15. Verkuil R, Kabeli O, Du Y, Wicky BIM, Milles LF, Dauparas J, et al. Language models generalize beyond natural pro -\nteins. bioRxiv. 2022. Pages: 2022.12.21.521521 Section: New Results. https:// doi. org/ 10. 1101/ 2022. 12. 21. 521521.\n 16. Alamdari S, Thakkar N, Berg Rvd, Lu AX, Fusi N, Amini AP , et al. Protein generation with evolutionary diffusion: \nsequence is all you need. bioRxiv. 2023. Pages: 2023.09.11.556673 Section: New Results. https:// doi. org/ 10. 1101/  \n2023. 09. 11. 556673.\n 17. Li FZ, Amini AP , Yue Y, Yang KK, Lu AX. Feature reuse and scaling: understanding transfer learning with protein \nlanguage models. bioRxiv. 2024. Pages: 2024.02.05.578959 Section: New Results. https:// doi. org/ 10. 1101/ 2024.  \n02. 05. 578959.\n 18. Pierson E, Yau C. ZIFA: dimensionality reduction for zero-inflated single-cell gene expression analysis. Genome \nBiol. 2015;16(1):241. https:// doi. org/ 10. 1186/ s13059- 015- 0805-z.\n 19. Prabhakaran S, Azizi E, Carr A, Pe’er D. Dirichlet process mixture model for correcting technical variation in \nsingle-cell gene expression data. JMLR Work Conf Proc. 2016;48:1070–9.\n 20. Lopez R, Regier J, Cole MB, Jordan MI, Yosef N. Deep generative modeling for single-cell transcriptomics. \nNat Methods. 2018;15(12):1053–8. Number: 12 Publisher: Nature Publishing Group. https:// doi. org/ 10. 1038/  \ns41592- 018- 0229-2.\n 21. Korsunsky I, Millard N, Fan J, Slowikowski K, Zhang F, Wei K, et al. Fast, sensitive and accurate integration of \nsingle-cell data with Harmony. Nat Methods. 2019;16(12):1289–96. Publisher: Nature Publishing Group. https://  \ndoi. org/ 10. 1038/ s41592- 019- 0619-0.\n 22. Hie B, Peters J, Nyquist SK, Shalek AK, Berger B, Bryson BD. Computational methods for single-cell RNA sequenc-\ning. Ann Rev Biomed Data Sci. 2020;3(1):339–64. https:// doi. org/ 10. 1146/ annur ev- bioda tasci- 012220- 100601.\n 23. Argelaguet R, Cuomo ASE, Stegle O, Marioni JC. Computational principles and challenges in single-cell data \nintegration. Nat Biotechnol. 2021;39(10):1202–15. Number: 10 Publisher: Nature Publishing Group. https:// doi.  \norg/ 10. 1038/ s41587- 021- 00895-7.\n 24. Heumos L, Schaar AC, Lance C, Litinetskaya A, Drost F, Zappia L, et al. Best practices for single-cell analysis across \nmodalities. Nat Rev Genet. 2023;24(8):550–72. Number: 8 Publisher: Nature Publishing Group. https:// doi. org/ 10.  \n1038/ s41576- 023- 00586-w.\n 25. Luecken MD, Büttner M, Chaichoompu K, Danese A, Interlandi M, Mueller MF, et al. Benchmarking atlas-level \ndata integration in single-cell genomics. Nat Methods. 2022;19(1):41–50. Number: 1 Publisher: Nature Publish-\ning Group. https:// doi. org/ 10. 1038/ s41592- 021- 01336-8.\n 26. Hendrycks D, Burns C, Basart S, Zou A, Mazeika M, Song D, et al. Measuring massive multitask language under -\nstanding. arXiv. 2021. ArXiv:2009.03300 [cs]. https:// doi. org/ 10. 48550/ arXiv. 2009. 03300.\n 27. Rein D, Hou BL, Stickland AC, Petty J, Pang RY, Dirani J, et al. GPQA: a graduate-level Google-proof Q &A bench-\nmark. arXiv. 2023. ArXiv:2311.12022 [cs]. https:// doi. org/ 10. 48550/ arXiv. 2311. 12022.\n 28. Zellers R, Holtzman A, Bisk Y, Farhadi A, Choi Y. HellaSwag: can a machine really finish your sen-\ntence? arXiv. 2019. ArXiv:1905.07830 [cs]. https:// doi. org/ 10. 48550/ arXiv. 1905. 07830.\n 29. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman S. GLUE: a multi-task benchmark and analysis platform for \nnatural language understanding. In: Proceedings of the 2018 EMNLP workshop BlackboxNLP: analyzing and \ninterpreting neural networks for NLP . 2018. pp. 353–5. https:// doi. org/ 10. 18653/ v1/ W18- 5446.\n 30. Gayoso A, Lopez R, Xing G, Boyeau P , Valiollah Pour Amiri V, Hong J, et al. A Python library for probabilistic analy-\nsis of single-cell omics data. Nat Biotechnol. 2022;40(2):163–6. Number: 2 Publisher: Nature Publishing Group. \nhttps:// doi. org/ 10. 1038/ s41587- 021- 01206-w.\nPage 13 of 13\nKedzierska et al. Genome Biology          (2025) 26:101 \n \n 31. Zheng GXY, Terry JM, Belgrader P , Ryvkin P , Bent ZW, Wilson R, et al. Massively parallel digital transcriptional profiling \nof single cells. Nat Commun. 2017;8(1):14049. Number: 1 Publisher: Nature Publishing Group. https:// doi. org/ 10. \n1038/ ncomm s14049.\n 32. Domínguez Conde C, Xu C, Jarvis LB, Rainbow DB, Wells SB, Gomes T, et al. Cross-tissue immune cell analysis reveals \ntissue-specific features in humans. Science. 2022;376(6594):eabl5197. Publisher: American Association for the \nAdvancement of Science. https:// doi. org/ 10. 1126/ scien ce. abl51 97.\n 33. Tabula Sapiens Consortium. The Tabula Sapiens: a multiple-organ, single-cell transcriptomic atlas of humans. Sci-\nence. 2022;376(6594):eabl4896. Publisher: American Association for the Advancement of Science. https:// doi. org/ 10. \n1126/ scien ce. abl48 96.\n 34. Segerstolpe r, Palasantza A, Eliasson P , Andersson EM, Andréasson AC, Sun X, et al. Single-cell transcriptome profiling \nof human pancreatic islets in health and type 2 diabetes. Cell Metab. 2016;24(4):593–607.\n 35. Muraro MJ, Dharmadhikari G, Grün D, Groen N, Dielen T, Jansen E, et al. A single-cell transcriptome atlas of the \nhuman pancreas. Cell Syst. 2016;3(4):385-394.e3.\n 36. Xin Y, Kim J, Okamoto H, Ni M, Wei Y, Adler C, et al. RNA sequencing of single human islet cells reveals type 2 diabe-\ntes genes. Cell Metab. 2016;24(4):608–15.\n 37. Baron M, Veres A, Wolock SL, Faust AL, Gaujoux R, Vetere A, et al. A single-cell transcriptomic map of the human and \nmouse pancreas reveals inter- and intra-cell population structure. Cell Syst. 2016;3(4):346-360.e4.\n 38. Lawlor N, George J, Bolisetty M, Kursawe R, Sun L, Sivakamasundari V, et al. Single-cell transcriptomes identify \nhuman islet cell signatures and reveal cell-type-specific expression changes in type 2 diabetes. Genome Res. \n2017;27(2):208–22.\n 39. Grün D, Muraro MJ, Boisset JC, Wiebrands K, Lyubimova A, Dharmadhikari G, et al. De novo prediction of stem cell \nidentity using single-cell transcriptome data. Cell Stem Cell. 2016;19(2):266–77.\n 40. Luecken M, Buttner M, Danese A, Interlandi M, Müller M, Strobl D, et al. Benchmarking atlas-level data integration in \nsingle-cell genomics - integration task datasets. 2020. https:// doi. org/ 10. 6084/ m9. figsh are. 12420 968. v8.\n 41. Pisco A, Consortium TS. Tabula Sapiens single-cell dataset. figshare. 2021. https:// doi. org/ 10. 6084/ m9. figsh are. 14267 \n219. v5.\n 42. Pisco A, Tan S, Consortium TS. Tabula Sapiens H &E image collection. figshare. 2021. https:// doi. org/ 10. 6084/ m9. figsh \nare. 14962 947. v2.\n 43. Pisco A, Mahmoudabadi G. Tabula Sapiens RNA velocity. figshare. 2021. https:// doi. org/ 10. 6084/ m9. figsh are. 17065 \n466. v1.\n 44. Kedzierska KZ, Crawford L, Amini A, Lu A. Foundation models in single-cell biology: evaluating zero-shot capabilities. \nGitHub. 2024. [Accessed 19 Aug 2024]. https:// github. com/ micro soft/ zero- shot- scfou ndati on.\n 45. Kedzierska KZ, Crawford L, Amini A, Lu A. Foundation models in single-cell biology: evaluating zero-shot capabilities. \nZenodo. 2024. https:// doi. org/ 10. 5281/ zenodo. 15123 462.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7267223000526428
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.7177968621253967
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6158739924430847
    },
    {
      "name": "One shot",
      "score": 0.600570023059845
    },
    {
      "name": "Single shot",
      "score": 0.5091914534568787
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.495552659034729
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.42112863063812256
    },
    {
      "name": "Computer science",
      "score": 0.41101711988449097
    },
    {
      "name": "Biology",
      "score": 0.41060417890548706
    },
    {
      "name": "Machine learning",
      "score": 0.3931886553764343
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38806915283203125
    },
    {
      "name": "Physics",
      "score": 0.1340477168560028
    },
    {
      "name": "Engineering",
      "score": 0.1266571581363678
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ]
}