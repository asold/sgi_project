{
    "title": "Equivariant transformer is all you need",
    "url": "https://openalex.org/W4390273128",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2231520916",
            "name": "Akio Tomiya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111147979",
            "name": "Yuki Nagai",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385583918",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4221165850",
        "https://openalex.org/W4225405705",
        "https://openalex.org/W4381586531",
        "https://openalex.org/W6795489260",
        "https://openalex.org/W2077766762",
        "https://openalex.org/W2557778781",
        "https://openalex.org/W4283068346",
        "https://openalex.org/W3025742964",
        "https://openalex.org/W2781887058",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2530819665",
        "https://openalex.org/W4308514146",
        "https://openalex.org/W4293246336"
    ],
    "abstract": "Machine learning, deep learning, has been accelerating computational physics, which has been used to simulate systems on a lattice. Equivariance is essential to simulate a physical system because it imposes a strong induction bias for the probability distribution described by a machine learning model. This reduces the risk of erroneous extrapolation that deviates from data symmetries and physical laws. However, imposing symmetry on the model sometimes occur a poor acceptance rate in self-learning Monte-Carlo (SLMC). On the other hand, Attention used in Transformers like GPT realizes a large model capacity. We introduce symmetry equivariant attention to SLMC. To evaluate our architecture, we apply it to our proposed new architecture on a spin-fermion model on a two-dimensional lattice. We find that it overcomes poor acceptance rates for linear models and observe the scaling law of the acceptance rate as in the large language models with Transformers.",
    "full_text": "PoS(LATTICE2023)001\nEquivariant Transformer is all you need\nAkio Tomiya0,1,∗ and Yuki Nagai2,3\n0faculty of technology and scienceL international professional university of technologyL SMSM1L umedaL\nkitaMkuL osakaL USPMPPP1L osakaL japan\n1rikenObnl research centerL brookhaven national laboratoryL uptonL 11YWSL nyL usa\n2ccseL japan atomic energy agencyL 1W8MTMTL wakashibaL kashiwaL chiba RWWMP8W1L japan\n3mathematical science teamL riken center for advanced intelligence project (aipIL 1MTM1 nihonbashiL\nchuoMkuL tokyo 1PSMPPRWL japan\neMmail: akio@yukawa.kyoto-u.ac.jp, nagai.yuki@jaea.go.jp\nmachine learningL deep learningL has been accelerating computational physicsL which has been\nused to simulate systems on a latticeN equivariance is essential to simulate a physical system\nbecause it imposes a strong induction bias for the probability distribution described by a maM\nchine learning modelN this reduces the risk of erroneous extrapolation that deviates from data\nsymmetries and physical lawsN howeverL imposing symmetry on the model sometimes occur a\npoor acceptance rate in selfMlearning monteMcarlo HslmcIN on the other handL attention used\nin transformers like gpt realizes a large model capacityN we introduce symmetry equivariant\nattention to slmcN to evaluate our architectureL we apply it to our proposed new architecture on\na spinMfermion model on a twoMdimensional latticeN we ﬁnd that it overcomes poor acceptance\nrates for linear models and observe the scaling law of the acceptance rate as in the large language\nmodels with transformersN\nthe TPth international symposium on lattice field theory (lattice RPRSI\njuly S1st M august TthL RPRS\nfermi national accelerator laboratory\n∗speaker\n© copyright owned by the authorHsI under the terms of the creative commons\nattributionMnoncommercialMnoderivatives TNP international license Hcc byMncMnd TNPIN https://pos.sissa.it/\nPoS(LATTICE2023)001\nequivariant transformer is all you need akio tomiya\n1. Introduction\nlattice qcd is essential for calculating quantum ﬁeld expectations but struggles with the\ncritical slowing down issueL reducing computational eﬃciencyN machine learning methods can\neﬃciently handle this problem and work well with structured data like gauge conﬁgurations [ QL R]N\ntransformer is a neural network originally for dealing with natural languagesL but it has been\napplied to various data [ S5V]N the most crucial feature of transformer is it can deal with global\ncorrelationsL such as modiﬁers in natural languageL which sometimes act from distant placesN\nin computational physics with machine learningL in particular equivariant neural networks\nenhance numerical calculations by capturing input data symmetriesN this improves generalization\non unfamiliar data and may boost computational eﬃciencyN data symmetry ensures alignment with\nphysical laws like momentum conservationN convolutional layersL for instanceL exhibit equivariant\nproperties to spatial translationN this reduces the risk of erroneous extrapolation that deviates from\ndata symmetries and physical laws [ W]N\nin this workL we develop an equivariant transformer for physical systemsN in quantum ﬁeld\ntheoryL local actionOhamiltonians are typically consideredN howeverL when fermionsL described\nby grassmann numbersL are integrated ahead of numerical simulationsL the resulting e;ective\nactionOhamiltonian becomes nonMlocal regarding bosonsN we develop a neural network architecture\nwhich is capable with global correlations from fermions and symmetric under sytemGs symmetriesN\nin this proofMofMprinciple studyL we employ the double exchange modelHdeI in two spacial\ndimensionsN de model is well established model in condensed matter physics and contains fermions\nand spatially ﬁxed classical heisenbergMspinsN the model hamiltonian is invariant under global\noHSI spin rotationN this model is similar to yukawa system with fermions and three component\nscalars on the lattice in particle physicsN to see more detail of this studyL refer [ X]N\n2. Concepts in Machine learning\n2.1 Self-learning Monte-Carlo\nwe review concepts in machine learning to introduce our numerical calculationN the selfM\nlearning monteMcarlo HslmcI is an exact markov chain monteMcarlo HmcmcI algorithm with\nan e;ective model [ Y]N in mcmc for a spin systemL a spin conﬁguration Y is distributed with a\nprobability distribution , (Y)N samples from desired distribution are obtained after many stepsN\nthe detailed balance condition is a suﬃcient condition for convergence of mcmcL which is\n, ({ Y})) ({ Y′}|{ Y}) = , ({ Y′})) ({ Y}|{ Y′})L where ) ({ Y′}|{ Y}) is the transition probability from\na conﬁguration {Y} to another conﬁguration {Y′}N if a probabilistic process described ) ({ Y′}|{ Y})\nwith this conditionL the obtained conﬁgurations distributed according to , ({ Y})N\nin general metropolisMhastings HmhI algorithmL the transition probability is factorised in two\nsubMstepsL ) ({ Y′}|{ Y}) = 6({ Y′}|{ Y}) \u0016({ Y′}, {Y})L where the proposal distribution 6({ Y}′|{ Y})\nis the conditional probability of proposing a conﬁguration {Y′} when a conﬁguration {Y} is givenL\nand the acceptance ratio \u0016({ Y′}, {Y}) is the probability to accept the proposed conﬁguration\n\u0016({ Y′}, {Y′})N the markov chain that has the desired distribution , ({ Y}) is obtained when the\nR\nPoS(LATTICE2023)001\nequivariant transformer is all you need akio tomiya\nFigure 1: schematic ﬁgure of slmcN blue thin arrows the metropolis chain in the inner chain with ,eﬀ N\nred bold arrows indicate the mh test in the outer chain with ,eﬀ and , as eqN H RIN\nacceptance ratio is given as\n\u0016({ Y′}, {Y}) = min\n(\nQ, , ({ Y′})\n, ({ Y})\n6({ Y}|{ Y′})\n6({ Y′}|{ Y})\n)\n. HQI\nthis is the general argument about the mh algorithmN the metropolis test is a reduced versionL\n\u0016({ Y′}, {Y}) = min (Q, , ({ Y′})/, ({ Y})) and this is obtained assuming the reversibility for\n6({ Y′}|{ Y}) = 6({ Y}|{ Y′})N\n2.1.1 Inner Markov chain in SLMC\nslmc is a nested mcmcN the simplest update is the local updateL where a single site in the\nconﬁguration is randomly selected and its spin orientation is changedN we perform the metropolis\nacceptOreject procedure following to the singleMsiteMupdate with ,eﬀ ({ Y})L which is the boltzmann\nweight with an e;ective modelN since the metropolis test with the single cite update satisﬁes the\ndetailed balance conditionL it will converge into ,eﬀ ({ Y})N in slmcL the e;ective model ,eﬀ ({ Y})\ncontains trainable parametersN\n2.1.2 Outer Markov chain in SLMC\nin slmcL the inner markov chain using an e;ective model proposes processes for the outer\nchainN a correction step ensures the target systemGs distributionN in the general mh algorithmL\nthe inner markov updates represent the 6(·|· ′) process in the outer chainL with the acceptance ratio\ndesigned to o;set ,eﬀ and distribute as , (·)N the acceptance ratio in the slmc is given as\n\u0016({ Y′}, {Y}) = min\n(\nQ, , ({ Y′})\n, ({ Y})\n,eﬀ ({ Y})\n,eﬀ ({ Y′})\n)\n, HRI\nwhere , ({ Y}) is the probability weight for the target systemN we remark thatL the second factor\nin the second column in min(Q, ·) is upMsideMdown of the weights for the inner chainN in summaryL\nslmc process can be express as figN QN\n2.2 Equivariance\nthe concept of equivariance plays a pivotal role in machine learning and physicsL o;ering\nsolutions to issues like model overﬁtting and the preservation of physical lawsN its importance\nstems from its capacity to embed symmetries directly into neural networksL ensuring that the\nlearned model respects certain symmetries of the dataN in neural networksL equivariance is usually\nachieved through weight sharingL reducing the number of irrelevant parametersN this is critical\nas an excess of parameters often leads to model overﬁttingL undermining the modelGs ability to\ngeneralize to new dataN\nin physicsL ensuring numerical calculations align with physical laws can be achieved via\nequivariant neural networksN while penalties in the loss function were previously used to impose\nS\nPoS(LATTICE2023)001\nequivariant transformer is all you need akio tomiya\nphysical laws [ QP]L this method is not always reliableN a more dependable approach embeds these\nsymmetries directly into the neural network architecture [ W]N\n3. Lattice setup\nin this studyL we employ semiMclassical double exchangeHdeI model in two dimension for a\ntestbed [QQ5QS]N it is a semiMclassical system and has electrons and classical heisenberg spinsL\n\u001d = −C\nÕ\nU, ⟨8, 9⟩\n( ˆ2†\n8 Uˆ2 9 U+ h.c.) + \u001f\nR\nÕ\n8\nY8 · ˆ28, HSI\nwhere Y8 is the classical heisenberg spins on the 8Mth siteL namely it is normalized oHSI scalar\nﬁeldN ˆ2†\n8 U is the fermionic creation operator at the 8Mth site for fermion with spin U ∈ {↑ , ↓}N a\nsymbol ⟨8, 9 ⟩ indicate the pairs of nearest neighborsN the interaction term with pauli matrices are\ndeﬁned as [ ˆ28]W ≡ ˆ2†\n8 UfW\nUV ˆ28V HW = G, H, I IN \u001f is the interaction strength between the classical\nspins and the electrons and we consider the hopping constant C as the unit of energyN we adopt\nthe periodic boundary condition on #G × #H site systemN the total number of the sites is # ≡\n#G #HN the hamiltonian has $(S) rotational symmetry in the spin sector and discrete translational\ninvarianceNwe want to calculate statistical expectation value with exp[−V\u001d] for HSIN\n3.1 Equivariant Transformer for physical system\nhere we introduce an equivariant transformer for a physical spin systemN input of our transM\nformer is a spin conﬁguration Y and output is modiﬁed spin conﬁgurationN in our formalismL queryL\nkeyL and value are # × S matrices and they are deﬁned as\nYq ≡ ˆ,qY, Yk ≡ ˆ,kY, Yv ≡ ˆ,vY, HTI\nrespectivelyN (U\n8` ≡ Í\n⟨8, 9⟩: , U\n: ( 9 `L , U\n: ∈ R HU = qL kL v and : = P, Q, R, · · · , ˜#IN a symbol\n⟨8, 9 ⟩: picks up :Mth nearest neighbors for sites 8, 9 N this procedure can be regarded as a block spin\ntransformation with ˜# + Q free parametersN in this workL we take ˜# = VN\nconsequentlyL we introduce the following HequivariantI selfMattention block asL\nselfattentionspin (Y) = 4\"Yv, HUI\nwhere 4\" is a # × # matrixN this 4\" is deﬁned by\n[ 4\"\n]\n8 9 = f\n(ÍS\n`=Q (q\n8` (k\n9 `\n/√\nS\n)\nL where 8, 9 are\nindices for spatial position on the latticeN f(·) is a nonlinear activation functionN intuitivelyL the\nargument Í\n` (q\n8` (k\n9 `is a set of R point correlation functions of the blocked spin ﬁeld from a point\n9 to 8 and this is invariant under oHSI rotations since rotation matrices cancel outN in this studyL we\ntake an activation function f(·) as relu functionN\nwe construct the e;ective spin ﬁeld that with multiple attention layersN our neural neural\nnetwork architecture is deﬁned with a residual connectionL\nY(;) ≡ N\n(\nY(;−Q) + selfattentionspin\n)(;) (Y(;−Q) )\n)\n, HVI\nand Y(P) ≡ Y and Yeﬀ ≡ Y(!) N ; is an index for layers and ; = Q, R, · · · , ! N )(;) represents a\nset of network trainable parameters in ;Mth layerN N ( Y) normalizes the spin vector on each lattice\nT\nPoS(LATTICE2023)001\nequivariant transformer is all you need akio tomiya\nFigure 2:HleftI e;ective spin construction using the transformer with an attention blockN yellow is deﬁned\nby eqN H VI[ purple is the attention blockN H rightI blue represents the attention block Hsee main textIN\nsiteL N ( Y8) = Y8/∥ Y8 ∥. we call this network architecture the equivariant transformerL which is\nschematically visualized in figN RN we remark that if all weights , U\n: are P in ;Mth blockL the\nselfMattention block Hindicated by a purple block in figN RI of ; works as an identity operation and it\ndoes nothing since the second term in the argument in H VI is zero Hsee [ X] for detailsIN\nthe longMrange correlation in the de model with slmc is partially considered in the linear\ne;ective model in the literature [ QRL QT]N in this workL we replace a bare spin operator Y8 by the\ne;ective spin operator from the transformer asL\n\u001deﬀ [Y] = −\nÕ\n⟨8, 9⟩=\n\u001feﬀ\n= Yeﬀ\n8 · Yeﬀ\n9 + \u001aP, HWI\nwhere Yeﬀ\n8 is an output the transformer at site 8N a symbol ⟨8, 9 ⟩= represents the =Mth nearest\nneighbor pairN the e;ective spin Yeﬀ is a function of Y and in totalL \u001deﬀ is a function of YN this\ne;ective hamiltonian contains a number of parameters in the transformerN in slmcL \u001feﬀ\n= L \u001aP and\nparameters in Yeﬀ\n8 is determined by using adamw and to increase acceptance ratioN\nin our calculationL we apply the mh test H RI using the e;ective model H WI and the hamiltonian\nHSI in a form where the fermions is traced out by the exact diagonalizationN\n4. Results\nhere we show our resultsN firstL we show results for physical observablesN our results show that\nit is consistent with exact resultsN as depicted in figN SL the selfMlearning monte carlo HslmcI\nmethod with e;ective models accurately replicates results from the original theoryL exhibiting\nantiMferromagnetic order at lower temperaturesN\nthe acceptance rate with the number of layers is shown in figN TL left panelN using e;ective\nmodels trained at ) = P.PUC on a V × V latticeL we set #original\nmc = S × QPT and #eﬀ\nmc = QPPN the\nlinear model slmc has an acceptance ratio of RQE due to the omission of longMrange spinMspin\ninteractionN as observedL the acceptance ratio improves with an increase in attention layersN\nfinallyL we show results for the scaling law of the loss function in figN T Hright panelIN the\nvalue of loss function is estimated from acceptance [ QU]N it is known thatL large language models\nwith transformer show a powerMtype scaling law[ model performance increases in depending on\nthe size of the input data and the number of parameters in the model [ QV]N we ﬁnd thatL our model\nU\nPoS(LATTICE2023)001\nequivariant transformer is all you need akio tomiya\nT[t]\n10-2 10-1 100 101\n|M|\n0.05\n0.10\n0.15\nLinear\nOriginal\n3 layer attention\nT[t]\n10-2 10-1 100 101\n|Ms|\n0.25\n0.50\n0.75\nLinear\nOriginal\n3 layer attention\nFigure 3: magnetization Hleft panelI and staggered magnetization Hright panelI for V × V lattice sitesN for\neach temperatureL we generate R × QPU samples using exact diagonalization Hred starsIL UPPP samples using\nslmc with the linear model Hgreen trianglesI and the e;ective model with attention blocks Hblue circlesIN\nwith equivariant transformer shows the scaling lawN there are no direct relation from our model to\nlarge language models and the origin of the scaling law has to be studied in another workN\nFigure 4: HleftI acceptance ratio in slmcZ blue square for the linear model and red circles for attention\nmodels with ! = Q, R, · · · , VN H rightI mse vsN trainable parametersZ blue square represents the linear model[\nred circles for models with ! = Q, R, · · · , VN fitting excludes the linear model and ! = QN\nAcknowledgments\nthe work of aNtN was partially by jsps kakenhi grant numbers RPkQTTWYL RRhPUQQRL\nand RRhPUQQQN yNnN was partially supported by jsps kakenhi grant numbers RRkQRPURL\nRRkPSUSYL RRhPUQQQ and RRhPUQQTN the calculations were partially performed using the superM\ncomputing system hpe sgiXVPP at the japan atomic energy agencyN this work was partially\nsupported by mext as 0program for promoting researches on the supercomputer fugaku1 Hgrant\nnumber jpmxpQPRPRSPTQQL jpmxpQPRPRSPTPYIN\nReferences\n[Q] kyle cranmerL gurtej kanwarL sébastien racanièreL danilo jN rezendeL and phiala eN shanaM\nhanN advances in machineMlearningMbased sampling motivated by lattice quantum chromodyM\nnamicsN nature revN physNL UHYIZURV5USUL RPRSN\nV\nPoS(LATTICE2023)001\nequivariant transformer is all you need akio tomiya\n[R] yuki nagai and akio tomiyaN gauge covariant neural network for T dimensional nonMabelian\ngauge theoryN march RPRQN\n[S] jN jumperL rN evansL aN pritzelL et alN highly accurate protein structure prediction with\nalphafoldN natureL UYVZUXS5UXYL RPRQN\n[T] alexey dosovitskiyL lucas beyerL alexander kolesnikovL dirk weissenbornL xiaohua zhaiL\nthomas unterthinerL mostafa dehghaniL matthias mindererL georg heigoldL sylvain gellyL\njakob uszkoreitL and neil houlsbyN an image is worth QVxQV wordsZ transformers for image\nrecognition at scaleL RPRQN\n[U] philipp thölke and gianni de fabritiisN torchmdMnetZ equivariant transformers for neural\nnetwork based molecular potentialsN february RPRRN\n[V] simon batznerL albert musaelianL lixin sunL mario geigerL jonathan p mailoaL mordechai\nkornbluthL nicola molinariL tess e smidtL and boris kozinskyN eHSIMequivariant graph neural\nnetworks for dataMeﬃcient and accurate interatomic potentialsN natN communNL QSHQIZRTUSL\nmay RPRRN\n[W] masanobu horieN e(nIMequivariant graph neural networks emulating meshMdiscretized\nphysicsN doctor of philosophy in engineeringL university of tsukubaL march RPRSN\n[X] yuki nagai and akio tomiyaN selfMlearning monte carlo with equivariant transformerL RPRSN\n[Y] junwei liuL yang qiL zi yang mengL and liang fuN selfMlearning monte carlo methodN\nphysical review bL YUHTIL jan RPQWN\n[QP] gN eN karniadakisL iN gN kevrekidisL lN luL et alN physicsMinformed machine learningN nature\nreviews physicsL SZTRR5TTPL june RPRQN acceptedZ SQ march RPRQL publishedZ RT may RPRQN\n[QQ] kipton barros and yasuyuki katoN eﬃcient langevin simulation of coupled classical ﬁelds\nand fermionsN physN revN bL XXHRSIZRSUQPQL december RPQSN\n[QR] junwei liuL huitao shenL yang qiL zi yang mengL and liang fuN selfMlearning monte carlo\nmethod and cumulative update in fermion systemsN physN revN bL YUHRTIZRTQQPTL june RPQWN\n[QS] georgios stratisL phillip weinbergL tales imbiribaL pau closasL and adrian e feiguinN sample\ngeneration for the spinMfermion model using neural networksN physN revN bL QPVHRPIZRPUQQRL\nnovember RPRRN\n[QT] hidehiko kohshiro and yuki nagaiN e;ective ruderman5kittel5kasuya5yosidaMlike interM\naction in diluted doubleMexchange modelZ selfMlearning monte carlo approachN jN physN socN\njpnNL YPHSIZPSTWQQL march RPRQN\n[QU] huitao shenL junwei liuL and liang fuN selfMlearning monte carlo with deep neural networksN\nphysN revN bL YWHRPIZRPUQTPL may RPQXN\n[QV] jared kaplanL sam mccandlishL tom henighanL tom bN brownL benjamin chessL rewon\nchildL scott grayL alec radfordL je;rey wuL and dario amodeiN scaling laws for neural\nlanguage modelsL RPRPN\nW"
}