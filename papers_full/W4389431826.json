{
  "title": "Performance of Large Language Models on a Neurology Board–Style Examination",
  "url": "https://openalex.org/W4389431826",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4288951127",
      "name": "Marc Cicero Schubert",
      "affiliations": [
        "German Cancer Research Center",
        "National Center for Tumor Diseases",
        "University Hospital Heidelberg",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A1992121173",
      "name": "Wolfgang Wick",
      "affiliations": [
        "German Cancer Research Center",
        "National Center for Tumor Diseases",
        "University Hospital Heidelberg",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2117774879",
      "name": "Varun Venkataramani",
      "affiliations": [
        "University Hospital Heidelberg",
        "National Center for Tumor Diseases",
        "German Cancer Research Center",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2117774879",
      "name": "Varun Venkataramani",
      "affiliations": [
        "Heidelberg University",
        "University Hospital Heidelberg",
        "National Center for Tumor Diseases",
        "German Cancer Research Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4280500298",
    "https://openalex.org/W3041796653",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4367175039",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W4385827730",
    "https://openalex.org/W1985233388",
    "https://openalex.org/W1185378561",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W3213380581",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4388933341",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4366974303",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4378509375",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W4318591734",
    "https://openalex.org/W4286233477",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W3153712677",
    "https://openalex.org/W3142164167",
    "https://openalex.org/W2900298334",
    "https://openalex.org/W2116817443",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W2187089797"
  ],
  "abstract": "Importance Recent advancements in large language models (LLMs) have shown potential in a wide array of applications, including health care. While LLMs showed heterogeneous results across specialized medical board examinations, the performance of these models in neurology board examinations remains unexplored. Objective To assess the performance of LLMs on neurology board–style examinations. Design, Setting, and Participants This cross-sectional study was conducted between May 17 and May 31, 2023. The evaluation utilized a question bank resembling neurology board-style examination questions and was validated with a small question cohort by the European Board for Neurology. All questions were categorized into lower-order (recall, understanding) and higher-order (apply, analyze, synthesize) questions based on the Bloom taxonomy for learning and assessment. Performance by LLM ChatGPT versions 3.5 (LLM 1) and 4 (LLM 2) was assessed in relation to overall scores, question type, and topics, along with the confidence level and reproducibility of answers. Main Outcomes and Measures Overall percentage scores of 2 LLMs. Results LLM 2 significantly outperformed LLM 1 by correctly answering 1662 of 1956 questions (85.0%) vs 1306 questions (66.8%) for LLM 1. Notably, LLM 2’s performance was greater than the mean human score of 73.8%, effectively achieving near-passing and passing grades in the neurology board–style examination. LLM 2 outperformed human users in behavioral, cognitive, and psychological–related questions and demonstrated superior performance to LLM 1 in 6 categories. Both LLMs performed better on lower-order than higher-order questions, with LLM 2 excelling in both lower-order and higher-order questions. Both models consistently used confident language, even when providing incorrect answers. Reproducible answers of both LLMs were associated with a higher percentage of correct answers than inconsistent answers. Conclusions and Relevance Despite the absence of neurology-specific training, LLM 2 demonstrated commendable performance, whereas LLM 1 performed slightly below the human average. While higher-order cognitive tasks were more challenging for both models, LLM 2’s results were equivalent to passing grades in specialized neurology examinations. These findings suggest that LLMs could have significant applications in clinical neurology and health care with further refinements.",
  "full_text": null,
  "topic": "Neurology",
  "concepts": [
    {
      "name": "Neurology",
      "score": 0.5945666432380676
    },
    {
      "name": "Cohort",
      "score": 0.5659107565879822
    },
    {
      "name": "Institutional review board",
      "score": 0.48119860887527466
    },
    {
      "name": "Psychology",
      "score": 0.45473676919937134
    },
    {
      "name": "Order (exchange)",
      "score": 0.4304269552230835
    },
    {
      "name": "Medicine",
      "score": 0.3964603841304779
    },
    {
      "name": "Medical education",
      "score": 0.35965490341186523
    },
    {
      "name": "Psychiatry",
      "score": 0.28021591901779175
    },
    {
      "name": "Internal medicine",
      "score": 0.15409323573112488
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ]
}