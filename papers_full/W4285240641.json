{
    "title": "OPI@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text using RoBERTa Pre-trained Language Models",
    "url": "https://openalex.org/W4285240641",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287891339",
            "name": "Rafał Poświata",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4320553414",
            "name": "Michał Perełkiewicz",
            "affiliations": [
                "National Information Processing Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4285209895",
        "https://openalex.org/W2402700",
        "https://openalex.org/W2927148761",
        "https://openalex.org/W2753766265",
        "https://openalex.org/W3133738707",
        "https://openalex.org/W2094553285",
        "https://openalex.org/W2250375872",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3154426640",
        "https://openalex.org/W2953413710",
        "https://openalex.org/W3156047327",
        "https://openalex.org/W2965265935",
        "https://openalex.org/W757697219",
        "https://openalex.org/W2998644122",
        "https://openalex.org/W3033913896",
        "https://openalex.org/W4292937823",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4237337450",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2252031683",
        "https://openalex.org/W3097571385",
        "https://openalex.org/W1567805217",
        "https://openalex.org/W3114950584",
        "https://openalex.org/W3131947689",
        "https://openalex.org/W2940818588",
        "https://openalex.org/W4253360490",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3202915159",
        "https://openalex.org/W2067495470",
        "https://openalex.org/W3164561728",
        "https://openalex.org/W4221145185",
        "https://openalex.org/W4382246105"
    ],
    "abstract": "This paper presents our winning solution for the Shared Task on Detecting Signs of Depression from Social Media Text at LT-EDI-ACL2022. The task was to create a system that, given social media posts in English, should detect the level of depression as 'not depressed', 'moderately depressed' or 'severely depressed'. We based our solution on transformer-based language models. We fine-tuned selected models: BERT, RoBERTa, XLNet, of which the best results were obtained for RoBERTa. Then, using the prepared corpus, we trained our own language model called DepRoBERTa (RoBERTa for Depression Detection). Fine-tuning of this model improved the results. The third solution was to use the ensemble averaging, which turned out to be the best solution. It achieved a macro-averaged F1-score of 0.583. The source code of prepared solution is available at https://github.com/rafalposwiata/depression-detection-lt-edi-2022.",
    "full_text": "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 276 - 282\nMay 27, 2022 ©2022 Association for Computational Linguistics\nOPI@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media\nText using RoBERTa Pre-trained Language Models\nRafał Po´swiata & Michał Perełkiewicz\nNational Information Processing Institute, 00-608 Warsaw, Poland\n{rposwiata, mperelkiewicz}@opi.org.pl\nAbstract\nThis paper presents our winning solution for the\nShared Task on Detecting Signs of Depression\nfrom Social Media Text at LT-EDI-ACL2022.\nThe task was to create a system that, given so-\ncial media posts in English, should detect the\nlevel of depression as ‘not depressed’, ‘moder-\nately depressed’ or ‘severely depressed’. We\nbased our solution on transformer-based lan-\nguage models. We fine-tuned selected models:\nBERT, RoBERTa, XLNet, of which the best re-\nsults were obtained for RoBERTalarge. Then, us-\ning the prepared corpus, we trained our own lan-\nguage model called DepRoBERTa (RoBERTa\nfor Depression Detection). Fine-tuning of this\nmodel improved the results. The third solu-\ntion was to use the ensemble averaging, which\nturned out to be the best solution. It achieved\na macro-averaged F1-score of 0.583. The\nsource code of prepared solution is available\nat https://github.com/rafalposwiata/depression-\ndetection-lt-edi-2022.\n1 Introduction\nDepression (major depressive disorder) is a com-\nmon and serious medical illness that, according to\nWorld Health Organization (WHO), already af-\nfects about 322 million people worldwide (WHO,\n2017). The main symptoms of depression include:\nfeeling sad or having a depressed mood, loss of\ninterest or pleasure, feeling worthless or guilty,\ninsomnia or hypersomnia, thoughts of death and\nsuicidal ideation or suicide attempts (American\nPsychiatric Association, 2013). When diagnosed\nand treated quickly, it can greatly improve qual-\nity of live and in some cases even save it. Such\nrapid detection of depression signs is possible, for\nexample, based on the social media posts of the\nindividual (De Choudhury et al., 2013). Following\nthis assumption, Sampath et al. (2022) organized at\nLT-EDI-ACL2022the Shared Task on Detecting\nSigns of Depression from Social Media Text. The\ntask was to create a system that, given social media\nposts in English, should classify the level of depres-\nsion as ‘not depressed’, ‘moderately depressed’\nor ‘severely depressed’.\nIn this paper we present our solution for this com-\npetition. The paper is organized as follows. Section\n2 describes related work with particular emphasis\non issues of depression detection in social media.\nSection 3 presents the dataset and its modification.\nThe process of developing our solution is explained\nin Section 4. The next section shows performed ex-\nperiments, the results, along with the error analysis.\nFinally, Section 6 concludes this paper.\n2 Related Work\nDe Choudhury et al. (2013) authored one of the\nfirst papers on detecting depression based on so-\ncial media posts. In their work, they collected a\ngroup of Twitter1 users diagnosed with depression\nwhose one-year posts were used to create a statis-\ntical classifier to estimate the risk of depression.\nTsugawa et al. (2015) prepared the dataset in a sim-\nilar way but for Japanese users, and then trained a\nSupport Vector Machines (SVM) classifier to esti-\nmate the presence of active depression. Wolohan\net al. (2018) created a dataset based on Reddit2\nposts in which users were assigned to one group:\ndepressed or control. Then, among other things,\nthey analyzed their posts using the Linguistic In-\nquiry and Wordcount Tool (LIWC) (Pennebaker\net al., 2015). Pirina and Çöltekin (2018) also used\nReddit as a data source and with other datasets they\nverified how training data can affect the quality of a\nSVM-based model to identify depression. Tadesse\net al. (2019) use different types of approaches to\ntext encoding (the LIWC dictionary, Latent Dirich-\nlet Allocation (LDA) topics or N-grams) to explore\nthe users’ linguistic usage in the depressive posts.\nArora and Arora (2019) analyze tweets for depres-\nsion and anxiety by using Multinomial Naive Bayes\n1https://twitter.com\n2https://www.reddit.com\n276\nPID Text Label\ntrain_pid_6035 Happy New Years Everyone : We made it another year not depression\ntrain_pid_35 My life gets worse every year : That’s what it feels like anyway.... moderate\ntrain_pid_8066 Words can’t describe how bad I feel right now : I just want to fall asleep forever. severe\nTable 1: Samples from the dataset.\nand Support Vector Regression (SVR) Algorithm\nas a classifier. Lin et al. (2020) create SenseMood\nsystem to detect depression from tweets based on\nvisual and textual features using Convolutional\nNeural Network (CNN) and BERT language model.\nZogan et al. (2021) propose novel summarization\nboosted deep framework for depression detection\ncalled DepressionNet. Other works worth men-\ntioning include Aswathy et al. (2019); Haque et al.\n(2021); William and Suhartono (2021).\nFor text-based classification, the last few years\nhave been primarily a time of deep learning and\nlarge pre-trained transformer-based language mod-\nels (Min et al., 2021). This kind of solutions\nachieve state-of-the-art results for numerous classi-\nfication tasks (Devlin et al., 2019; Liu et al., 2019;\nChan et al., 2020; Dadas et al., 2020).\n3 Dataset\nThe dataset used in the competition consists of En-\nglish posts from Reddit, where each was annotated\nwith one of the labels: not depression, moder-\nate and severe (Kayalvizhi and Thenmozhi, 2022).\nThe first label indicates a case where no signs of\ndepression were identified. The other two labels\nshow that symptoms in the post indicate moderate\nor severe depression respectively. Example texts\nwith labels from the dataset are presented in Table 1.\nThe dataset was divided into three parts: train, dev,\nand test. Labels for the test part were not provided\nby the organizers, as this one was the part on which\nthe solutions were verified. To verify the quality of\nthe collections used to prepare the solution (train,\ndev), we first verified their diversity by removing\nduplicate records containing the same posts. As a\nresult of this step, we noticed that the train set con-\nsists of a large number of the same examples, and\nthe unique ones are only 2,720 (out of 8,891 total).\nIn the case of the dev set, the difference was much\nsmaller, i.e., 4,481 unique against 4,496 all. It is\ngood practice to make the train set larger than the\ndev or test set. This is especially important when\nusing machine learning or deep learning methods\nwhere the quality of the model directly depends on\nthe number and variety of samples during training.\nTherefore, we decided to use part of the dev set for\ntraining, leaving 1,000 examples for verification\n(we kept the class distribution close to the original\none). As a result, the train set we used in our exper-\niments counted 6,006 unique examples (the final\nnumber is due to the fact that there were overlaps\nbetween the original train and dev sets). The whole\nprocess of preparing the dataset, including class\ndistribution, is shown in Figure 1. What is worth\nnoting is that the dataset is unbalanced, and the\nsevere class is underrepresented.\n4 Our solution\nWe organized the work on our solution into three\nsteps, which will be presented in the following\nsubsections.\n4.1 Fine-tuned Transformer-Based Language\nModels\nFirst, we fine-tune several commonly used English\npre-trained language models. We use the stan-\ndard fine-tuning procedure like Devlin et al. (2019),\nwhich involves training pre-trained language model\nwith classification head on top (a linear layer on\ntop of the pooled output). The following mod-\nels were utilized: BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019) and XLNet (Yang\net al., 2019). Both in base and large version. All\nmodels were downloaded from the Hugging Face\nhub3. The best result on the dev set was achieved\nby RoBERTalarge, which will be further described\nin Section 5.3.\n4.2 Pre-trained and Fine-tuned Domain\nSpecific Transformer-Based Language\nModel\nThe models used in the previous step were pre-\ntrained on general domain corpora (e.g. English\nWikipedia or BooksCorpus). It can be assumed\nthat most of the texts from these corpora did not\nmanifest symptoms of depression. Inspired by Lee\net al. (2019), we decided to pre-train our own\n3https://huggingface.co/models\n277\ntrain dev test\nnot depression1,971 1,830 - \nmoderate 6,019 2,306 - \nsevere 901 360 - \ntotal 8,891 4,496 3,245 \nRemoving  \nduplicates  \n(train and dev)\nOriginal dataset Dataset without duplicates\nin train and dev sets\nFinal dataset\nAdding dev  \nsamples to  \ntrain w/o \nduplicates\ntrain dev test\nnot depression916 1,817 - \nmoderate 1,401 2,304 - \nsevere 403 360 - \ntotal 2,720 4,481 3,245 \ntrain dev test\nnot depression2,255 400 - \nmoderate 3,101 510 - \nsevere 650 90 - \ntotal 6,006 1,000 3,245 \nFigure 1: The process of preparing the dataset including the distribution of classes at each step. The dashes (-) are\ndue to the lack of labels for the test set.\nyensemble = arg max(\nsoftmax( y′\nRoBERTalarge ) +softmax( y′\nDepRoBERTa )\n2 ) (1)\nlanguage model on texts mainly expressing de-\npression. We built a corpus based on the Reddit\nMental Health Dataset (Low et al., 2020) and\na dataset of 20,000 posts from r/depression and\nr/SuicideWatch subreddits4. We filtered the data\nappropriately, leaving mainly those related to de-\npression (31,2%), anxiety (20,5%) and suicide\n(18.1%), which resulted in a corpora consisting\nof 396,968 posts. We used a further pre-training\ntechnique where the model weights were initialized\nwith the RoBERTalarge model weights, since it was\nthe fine-tuning of this particular model that gave the\nbest results in the first step. We called the resulting\nmodel DepRoBERTa (RoBERTa for Depression\nDetection). For more information on the corpus\nstatistics and the pre-training process, we refer you\nto the appendices. Then, as with the models in Sec-\ntion 4.1, we performed DepRoBERTa fine-tuning\non the train set.\n4.3 Ensemble\nIn the last step, we combined the best models\nobtained in the previous steps using ensemble\n4https://www.kaggle.com/xavrig/reddit-dataset-\nrdepression-and-rsuicidewatch\naveraging (Naftaly et al., 1999). This method\ninvolves averaging the predictions from a group\nof models, and its implementation in our case is\npresented in Equation 1. Where y′\nRoBERTalarge\nand y′\nDepRoBERTa are vectors of raw (non-\nnormalized) predictions generated by fine-tuned\nRoBERTalarge and DepRoBERTa, respectively.\nParameter Value\nOptimizer AdamW\nLearning rate 5e-6\nBatch size 16\nDropout 0.1\nWeight decay (L2) 0.1\nEpochs 10\nValidation after no. steps 100\nMax sequence length 300\nTable 2: Hyper-parameters used when fine-tuning mod-\nels.\n278\nModel Accuracy Precision Recall F1-score\nBERTbase 0.627 0.586 0.574 0.579\nBERTlarge 0.606 0.568 0.566 0.566\nRoBERTabase 0.622 0.567 0.573 0.570\nRoBERTalarge 0.664 0.629 0.591 0.605\nXLNetbase 0.654 0.632 0.576 0.590\nXLNetlarge 0.639 0.611 0.597 0.602\nDepRoBERTa 0.661 0.628 0.607 0.616\nEnsemble 0.695 0.663 0.621 0.637\nTable 3: Results of each model on the dev set. Bolded and underlined values indicate the best and second-best\nscores for models from each of the three steps for a given measure.\nModel Accuracy Precision Recall F1-score\nRoBERTalarge 0.614 0.583 0.564 0.552\nDepRoBERTa 0.626 0.575 0.588 0.571\nEnsemble 0.658 0.586 0.591 0.583\nTable 4: Results of submitted models on the test set (official competition results made available by the competition\norganisers). Bolded and underlined values indicate the best and second-best scores for the measure, respectively.\n5 Experiments and Results\n5.1 Experimental Setup\nWe utilized Simple Transformers library (Ra-\njapakse, 2019) to perform experiments, includ-\ning models fine-tuning and pre-training the\nDepRoBERTa model. Used hyper-parameters are\npresented in Table 2. The fine-tuning procedure for\neach model was repeated 5 times using the train and\ndev sets described in Section 3. All experiments\nwere run on a single GPU Tesla V100.\n5.2 Metrics\nThe metrics used during the experiments are accu-\nracy, macro-averaged precision, macro-averaged\nrecall and macro-averaged F1-score across all the\nclasses. The macro-averaged F1-score was the\nmain measure when evaluating solutions.\n5.3 Results\nTable 3 shows the results on the dev set. Among\nthe fine-tuned transformer-based language models,\nRoBERTalarge model was the best in terms of accu-\nracy (0.664) and F1-score (0.605). In the other two\nmeasures, XLNet models were better, respectively\nXLNetbase for precision ( 0.632) and XLNetlarge\nfor recall (0.597). RoBERTalarge was second in\nthese cases. We improved the F1-score by0.011 us-\ning the DepRoBERTafine-tuned model. This was\nmainly due to the high score for the recall measure\n(0.607), as the results for the other measures were\nworse than RoBERTalarge. Ensemble proved to be\nthe best approach by achieving the highest scores\non each measure, having an F1-score of 0.637 (an\nimprovement of 0.021 over DepRoBERTa). Due\nto these results, we have chosen as our official com-\npetition solutions: RoBERTalarge, DepRoBERTa\nand Ensemble. The results they achieved on the\ntest set are presented in Table 4. As expected, En-\nsemble proved to be the best by achieving an F1-\nscore of 0.583. This score gave our team the 1st\nplace among the 31 participating teams.\n5.4 Errors Analysis\nTo be able to evaluate the errors and strengths of our\nmodels, we created the confusion matrices shown\nin Figure 2. Each model specializes in one class,\ni.e. it achieves the best results for a different class.\nRoBERTalarge performs best for the not depres-\nsion class, DepRoBERTafor the severe class, and\nEnsemble for the moderate class. The most com-\nmon mistake is to assign a severe class to a post\noriginally tagged as moderate. A mistake that also\noften occurs is confusion between not depression\nand modereate classes. The analysis was carried\nout on the dev set as the competition organisers did\nnot provide labels for the test set.\n279\nFigure 2: Normalized confusion matrices for RoBERTalarge, DepRoBERTa and their ensemble on the dev set.\n6 Conclusion\nIn this paper, we presented a solution to the Shared\nTask on Detecting Signs of Depression from Social\nMedia Text at LT-EDI-ACL2022. The use of en-\nsemble averaging previously fine-tuned language\nmodels proved to be the best. As part of this work,\nin addition to the models designed for this compe-\ntition, we also prepared a new pre-train language\nmodel, DepRoBERTa. In the future it can be used\nfor other depression detection tasks. We plan to pre-\ntrain it further on a larger corpus of texts expressing\ndepression, as an extension of this work.\nThe code of our solution and pre-\npared models are available online at\nhttps://github.com/rafalposwiata/depression-\ndetection-lt-edi-2022.\nReferences\nAmerican Psychiatric Association. 2013. Diagnostic\nand Statistical Manual of Mental Disorders, Fifth\nEdition. American Psychiatric Association, Arling-\nton, V A.\nPriyanka Arora and Parul Arora. 2019. Mining twitter\ndata for depression detection. In 2019 International\nConference on Signal Processing and Communica-\ntion (ICSC), pages 186–189.\nK S Aswathy, P C Rafeeque, and Reena Murali. 2019.\nDeep learning approach for the detection of depres-\nsion in twitter. In Proceedings of the International\nConference on Systems, Energy Environment (IC-\nSEE).\nA. T. BECK, C. H. WARD, M. MENDELSON,\nJ. MOCK, and J. ERBAUGH. 1961. An Inventory\nfor Measuring Depression. Archives of General Psy-\nchiatry, 4(6):561–571.\nBranden Chan, Stefan Schweter, and Timo Möller. 2020.\nGerman’s next language model. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 6788–6796, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nGlen Coppersmith, Mark Dredze, Craig Harman,\nKristy Hollingshead, and Margaret Mitchell. 2015.\nCLPsych 2015 shared task: Depression and PTSD\non Twitter. In Proceedings of the 2nd Workshop on\nComputational Linguistics and Clinical Psychology:\nFrom Linguistic Signal to Clinical Reality, pages 31–\n39, Denver, Colorado. Association for Computational\nLinguistics.\nSławomir Dadas, Michał Perełkiewicz, and Rafał\nPo´swiata. 2020. Pre-training polish transformer-\nbased language models at scale. In Artificial Intelli-\ngence and Soft Computing, pages 301–314. Springer\nInternational Publishing.\nMunmun De Choudhury, Michael Gamon, Scott Counts,\nand Eric Horvitz. 2013. Predicting depression via\nsocial media. Proceedings of the International AAAI\nConference on Web and Social Media, 7(1):128–137.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAyaan Haque, Viraaj Reddi, and Tyler Giallanza. 2021.\nDeep learning for suicide and depression identifica-\ntion with unsupervised label correction. In Artificial\nNeural Networks and Machine Learning – ICANN\n280\n2021, pages 436–447, Cham. Springer International\nPublishing.\nS Kayalvizhi and D Thenmozhi. 2022. Data set cre-\nation and empirical analysis for detecting signs of de-\npression from social media postings. arXiv preprint\narXiv:2202.03047.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan So, and Jaewoo Kang. 2019.\nBiobert: a pre-trained biomedical language represen-\ntation model for biomedical text mining. Bioinfor-\nmatics (Oxford, England), 36.\nChenhao Lin, Pengwei Hu, Hui Su, Shaochun Li, Jing\nMei, Jie Zhou, and Henry Leung. 2020. Sense-\nMood: Depression Detection on Social Media, page\n407–411. Association for Computing Machinery,\nNew York, NY , USA.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nDavid E. Losada, Fabio Crestani, and Javier Parapar.\n2020. Overview of erisk 2020: Early risk predic-\ntion on the internet. In Experimental IR Meets Mul-\ntilinguality, Multimodality, and Interaction: 11th\nInternational Conference of the CLEF Association,\nCLEF 2020, Thessaloniki, Greece, September 22–25,\n2020, Proceedings, page 272–287, Berlin, Heidel-\nberg. Springer-Verlag.\nDavid E. Losada, Fabio A. Crestani, and Javier Parapar.\n2017. Clef 2017 erisk overview: Early risk prediction\non the internet: Experimental foundations. In CLEF.\nDavid E. Losada, Fabio A. Crestani, and Javier Parapar.\n2018. Overview of erisk 2018: Early risk prediction\non the internet (extended lab overview). In CLEF.\nDavid E. Losada, Fabio A. Crestani, and Javier Parapar.\n2019. Overview of erisk at clef 2019: Early risk\nprediction on the internet (extended overview). In\nCLEF.\nDaniel M Low, Laurie Rumker, John Torous, Guillermo\nCecchi, Satrajit S Ghosh, and Tanya Talkar. 2020.\nNatural language processing reveals vulnerable men-\ntal health support groups and heightened health anxi-\nety on reddit during covid-19: Observational study.\nJournal of medical Internet research, 22(10):e22635.\nBonan Min, Hayley H. Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heinz, and Dan Roth. 2021. Re-\ncent advances in natural language processing via\nlarge pre-trained language models: A survey. ArXiv,\nabs/2111.01243.\nUry Naftaly, Nathan Intrator, and David Horn. 1999.\nOptimal ensemble averaging of neural networks. Net-\nwork: Computation in Neural Systems, 8.\nJavier Parapar, Patricia MartÃn, David E. Losada, and\nFabio Crestani. 2021. Overview of eRisk 2021: Early\nRisk Prediction on the Internet. In Experimental IR\nMeets Multilinguality, Multimodality, and Interaction\nProceedings of the Twelfth International Conference\nof the CLEF Association (CLEF 2021). Springer In-\nternational Publishing.\nJames W. Pennebaker, Ryan Boyd, Kayla Jordan, and\nKate Blackburn. 2015. The development and psycho-\nmetric properties of LIWC2015. University of Texas\nat Austin.\nInna Pirina and Ça ˘grı Çöltekin. 2018. Identifying de-\npression on Reddit: The effect of training data. In\nProceedings of the 2018 EMNLP Workshop SMM4H:\nThe 3rd Social Media Mining for Health Applica-\ntions Workshop & Shared Task, pages 9–12, Brussels,\nBelgium. Association for Computational Linguistics.\nDaniel Preo¸ tiuc-Pietro, Maarten Sap, H. Andrew\nSchwartz, and Lyle Ungar. 2015. Mental illness\ndetection at the world well-being project for the\nCLPsych 2015 shared task. In Proceedings of the 2nd\nWorkshop on Computational Linguistics and Clinical\nPsychology: From Linguistic Signal to Clinical Real-\nity, pages 40–45, Denver, Colorado. Association for\nComputational Linguistics.\nT. C. Rajapakse. 2019. Simple transformers. https:\n//github.com/ThilinaRajapakse/\nsimpletransformers.\nPhilip Resnik, William Armstrong, Leonardo Claudino,\nand Thang Nguyen. 2015. The University of Mary-\nland CLPsych 2015 shared task system. In Proceed-\nings of the 2nd Workshop on Computational Linguis-\ntics and Clinical Psychology: From Linguistic Signal\nto Clinical Reality, pages 54–60, Denver, Colorado.\nAssociation for Computational Linguistics.\nKayalvizhi Sampath, Thenmozhi Durairaj,\nBharathi Raja Chakravarthi, and Jerin Mahibha C.\n2022. Findings of the shared task on Detecting Signs\nof Depression from Social Media. In Proceedings of\nthe Second Workshop on Language Technology for\nEquality, Diversity and Inclusion . Association for\nComputational Linguistics.\nMichael M. Tadesse, Hongfei Lin, Bo Xu, and Liang\nYang. 2019. Detection of depression-related posts in\nreddit social media forum. IEEE Access, 7:44883–\n44893.\nSho Tsugawa, Yusuke Kikuchi, Fumio Kishino, Kosuke\nNakajima, Yuichi Itoh, and Hiroyuki Ohsaki. 2015.\nRecognizing depression from twitter activity. Pro-\nceedings of the 33rd Annual ACM Conference on\nHuman Factors in Computing Systems.\nWHO. 2017. Depression and other common mental\ndisorders: global health estimates. World Health\nOrganization.\n281\nDavid William and Derwin Suhartono. 2021. Text-\nbased depression detection on social media posts:\nA systematic literature review. Procedia Computer\nScience, 179:582–589. 5th International Conference\non Computer Science and Computational Intelligence\n2020.\nJT Wolohan, Misato Hiraga, Atreyee Mukherjee, Zee-\nshan Ali Sayyed, and Matthew Millard. 2018. Detect-\ning linguistic traces of depression in topic-restricted\ntext: Attending to self-stigmatized depression with\nNLP. In Proceedings of the First International Work-\nshop on Language Cognition and Computational\nModels, pages 11–21, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nHamad Zogan, Imran Razzak, Shoaib Jameel, and Guan-\ndong Xu. 2021. Depressionnet: A novel summariza-\ntion boosted deep framework for depression detection\non social media. ArXiv, abs/2105.10878.\nAppendix\nA Previous competitions\nThe Shared Task on Detecting Signs of Depres-\nsion from Social Media Text at LT-EDI-ACL2022\nwas not the first competition to address the topic\nof depression detection. To the best of our knowl-\nedge, the first was theCLPsych 2015 Shared Task:\nDepression and PTSD on Twitter (Coppersmith\net al., 2015). The shared task consisted of three\ntasks, two of which related to depression: identify-\ning depressed users from a control group and dis-\ntinguishing depressed users from those with PTSD\n(post-traumatic stress disorder). The SVM clas-\nsifier and its variants have proven to be the best\nand most popular solution (Resnik et al., 2015;\nPreo¸ tiuc-Pietro et al., 2015). This was followed\nby a series of eRisk competitions as part of the\nCLEF conference (Losada et al., 2017, 2018, 2019,\n2020; Parapar et al., 2021). In the first two edi-\ntions (2017-2018), the problem was defined as an\nearly risk detection task. So, in addition to identi-\nfying depression, the system should be able to do\nso by having the shortest possible list of posts or\nchunks of a user’s posting history. In subsequent\neditions (2019-2021), participants were asked to\ncreate systems that would determine a user’s sever-\nity of depression based on their posts by predicting\ntheir responses to a standard depression question-\nnaire derived from the Beck’s Depression Inventory\n(BDI) (BECK et al., 1961). In the case of eRisk\ncontests, the datasets created were based on Reddit\nposts.\nB Reddit Depression Corpora\nsubreddit # posts %\ndepression 123,824 31.2\nsuicidewatch 71,816 18.1\nanxiety 53,797 13.6\nbpd 21,836 5.5\nlonely 21,399 5.4\nsocialanxiety 19,648 4.9\nfitness 10,000 2.5\njokes 10,000 2.5\nlegaladvice 10,000 2.5\nparenting 10,000 2.5\npersonalfinance 10,000 2.5\nrelationships 10,000 2.5\nhealthanxiety 7,847 2.0\nptsd 7,551 1.9\nbipolarreddit 5,186 1.3\nteaching 4,064 1.0\nTable 5: Statistics of the corpus formed to pre-train\nDepRoBERTa.\nC DepRoBERTa\nParameter Value\nOptimizer AdamW\nLearning rate 4e-5\nBatch size 50\nDropout 0.1\nEpochs 10\nTraining samples 389,028\nValidation samples 7,940\nValidation after no. steps 5,000\nTable 6: Configuration used when pre-training\nDepRoBERTa.\n282"
}