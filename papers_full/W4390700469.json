{
    "title": "Large language models in simplifying radiological reports: systematic review",
    "url": "https://openalex.org/W4390700469",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5093698157",
            "name": "Yaara Artsi",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A2809724045",
            "name": "Vera Sorin",
            "affiliations": [
                "Sheba Medical Center",
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A2155772135",
            "name": "Eli Konen",
            "affiliations": [
                "Tel Aviv University",
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A1498187152",
            "name": "Benjamin S Glicksberg",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2419298921",
            "name": "Girish Nadkarni",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A1945837108",
            "name": "Eyal Klang",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai",
                "Sheba Medical Center",
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A5093698157",
            "name": "Yaara Artsi",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A2809724045",
            "name": "Vera Sorin",
            "affiliations": [
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2155772135",
            "name": "Eli Konen",
            "affiliations": [
                "Tel Aviv University",
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A1498187152",
            "name": "Benjamin S Glicksberg",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2419298921",
            "name": "Girish Nadkarni",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A1945837108",
            "name": "Eyal Klang",
            "affiliations": [
                "Sheba Medical Center"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2989543878",
        "https://openalex.org/W2042958776",
        "https://openalex.org/W1989961189",
        "https://openalex.org/W2907025287",
        "https://openalex.org/W4380082731",
        "https://openalex.org/W3163634736",
        "https://openalex.org/W3025338242",
        "https://openalex.org/W2734388030",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4366989525",
        "https://openalex.org/W3013872196",
        "https://openalex.org/W4377010595",
        "https://openalex.org/W4379883463",
        "https://openalex.org/W4380048615",
        "https://openalex.org/W4386753580",
        "https://openalex.org/W4387356888",
        "https://openalex.org/W4387880927",
        "https://openalex.org/W4387747149",
        "https://openalex.org/W4388594599",
        "https://openalex.org/W4313120307",
        "https://openalex.org/W2100924250",
        "https://openalex.org/W3137874387",
        "https://openalex.org/W4310954861",
        "https://openalex.org/W1990916334",
        "https://openalex.org/W2994242459",
        "https://openalex.org/W2917966016",
        "https://openalex.org/W2068291226",
        "https://openalex.org/W4205164650",
        "https://openalex.org/W3134138028",
        "https://openalex.org/W4323314258",
        "https://openalex.org/W4367307887",
        "https://openalex.org/W2075806609",
        "https://openalex.org/W4379799312",
        "https://openalex.org/W4386351210",
        "https://openalex.org/W4200191097",
        "https://openalex.org/W4379095447",
        "https://openalex.org/W4386565997",
        "https://openalex.org/W3199858151",
        "https://openalex.org/W3205291392"
    ],
    "abstract": "Abstract Objectives Simplifying medical information to make it understandable for patients, specifically in the case of radiology reports, is challenging. It requires time and effort from medical personnel. This systematic review focuses on the application of large language models (LLMs) in generating simplified radiological imaging reports, as well as answering patient inquiries regarding radiological procedures. Materials and Methods The authors searched for studies published up to January 2024. Search terms focused on LLMs generated simplified radiological reports and answers to patient inquiries regarding radiological procedures. MEDLINE was used as a search database. Results Overall, eight studies published between May 2023 and November 2023 were included. All studies showed that LLMs can produce simplified medical information for patients. Four studies (50%) used GPT-3.5, Two studies (25%) conducted a comparative analysis between GPT-3.5 and GPT-4. One study (12.5%) examined Microsoft Bing. One study (12.5%) utilized GPT-4. Four studies (50%) used LLMs to simplify radiological reports. Four studies (50%) used LLMs to answer patient questions regarding radiological procedures. Only two studies (25%) used patients to evaluate the LLMs output. One study (12.5%) compared their initial prompt with optimized prompt. Five studies (62.5%) showed missing, inaccurate and potentially harmful AI outputs. Conclusion LLMs can be used to simplify medical imaging reports and procedures, for improved patient comprehension. However, their limitations cannot be ignored. Further study in this field is essential and more conclusive evidence is needed.",
    "full_text": "Large language models in simplifying radiological reports: systematic \nreview \nYaara Artsi, BMedSC1; Vera Sorin, MD2-4; Eli Konen, MD2-3; Benjamin S. Glicksberg, \nPhD5; Girish Nadkarni, MD5-6; Eyal Klang, MD2-6 \n1Azrieli Faculty of Medicine, Bar-Ilan University, Zefat, Israel. \n2Department of Diagnostic Imaging, Chaim Sheba Medical Center, Israel \n3Tel-Aviv University School of Medicine, Israel \n4DeepVision Lab, Chaim Sheba Medical Center, Israel \n5Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount \nSinai, New York, New York, USA \n6The Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine at \nMount Sinai, New York, New York, USA. \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract \nObjectives \nSimplifying medical information to make it understandable for patients, specifically in \nthe case of radiology reports, is challenging. It requires time and effort from medical \npersonnel. This systematic review focuses on the application of large language \nmodels (LLMs) in generating simplified radiological imaging reports, as well as \nanswering patient inquiries regarding radiological procedures.   \nMaterials and Methods \nThe authors searched for studies published up to January 2024. Search terms \nfocused on LLMs generated simplified radiological reports and answers to patient \ninquiries regarding radiological procedures. MEDLINE was used as a search \ndatabase. \nResults \nOverall, eight studies published between May 2023 and November 2023 were \nincluded. All studies showed that LLMs can produce simplified medical information \nfor patients. Four studies (50%) used GPT-3.5, Two studies (25%) conducted a \ncomparative analysis between GPT-3.5 and  GPT-4. One study (12.5%) examined \nMicrosoft Bing. One study (12.5%) utilized GPT-4.  Four studies (50%) used LLMs to \nsimplify radiological reports. Four studies (50%) used LLMs to answer patient \nquestions regarding radiological procedures. Only two studies (25%) used patients to \nevaluate the LLMs output. One study (12.5%) compared their initial prompt with \noptimized prompt. Five studies (62.5%) showed missing, inaccurate and potentially \nharmful AI outputs.\n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nConclusion \nLLMs can be used to simplify medical imaging reports and procedures, for improved \npatient comprehension. However, their limitations cannot be ignored. Further study \nin this field is essential and more conclusive evidence is needed.  \n \n \nKeywords \nLarge language models, Generative Pre-trained Transformer, Patient Education, \nArtificial intelligence, Radiology  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nIntroduction \n \nReading, understanding, and interpreting radiographic images, reports and \nprocedures is challenging, even for non-radiologist physicians [1, 2]. The task is \nmany more times difficult for patients who have no medical training [3, 4]. Nowadays, \nmedical imaging is an integral part of the clinical decision making process [5, 6, 7]. \nAccording to the patient-centered care approach, patients should be active \nparticipants in their care [4, 8]. Although patients have access to their imaging \nreports, these reports are frequently incomprehensible to the average patient [3, 4]. \nUsing complex medical terminology can create patient anxiety and a perception of \nexacerbated severity of their condition [9]. The lack of access to simplified \ninformation for patients regarding their healthcare highlights a shortcoming of \nmodern healthcare practice.   \nLarge language models (LLMs), such as ChatGPT, can be used to analyze free-text \nand generate human-like responses to various inquiries [10, 11]. It is possible that \nthis technology might hold the key to bridge the gap between patients and \ncomplicated medical imaging reports jargon. However, the possibility of patients \nseeking clarification from AI poses risks. The accuracy and credibility of such models \nis still up for debate and can potentially misinform patients’ diagnoses and outcomes \n[12].\n \n \n \nThe aim of this study is to systematically review the literature on applications of \nLLMs for patient education and simplification of radiological reports and procedures.   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n \nMethods \n \nLiterature search \nFor this retrospective review, we conducted a search to identify studies describing \nLLMs’ applications for patient education. We searched PubMed/MEDLINE for papers \npublished not earlier than 2023 up to January 2024. The following keywords were \nused with Boolean operators AND/OR: large language models, ChatGPT, openAI, \npatient, education. \nWe checked the references list of selected publications for more relevant papers. \nSections as ‘Similar Articles’ (e.g., PubMed) were also inspected for possible \nadditional articles.  \nOur study followed the Preferred Reporting Items for Systematic Reviews and Meta \nAnalyses (PRISMA) guidelines. The study is registered with PROSPERO  \nInclusion and exclusion process \nPublications resulting from the search were initially assessed by one author (YA) for \nrelevant titles and abstracts. Next, full-text papers underwent an independent \nevaluation by two authors (EK and VS).  \nWe included full length studies describing LLMs application for patient education \nfocusing on radiology and imaging reports. We excluded papers published before \n2023, non-English papers and non-original studies (Figure 1). \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nDiscrepancies were discussed and resolved to achieve a consensus. Risk of bias \nand applicability were evaluated using the tailored QUADAS-2 tool (Figure 2).  \n \n \nResults \n \nStudy selection and characteristics  \nThe initial literature search resulted in 729 articles. Eight studies met our inclusion \ncriteria (Figure 1). Six studies were retrospective. One study is cross-sectional \n(descriptive). Majority of the studies used ChatGPT (versions 3.5 or 4) as an AI \nmodel of choice, one study used Microsoft's Bing. The prompts were phrased \ndifferently in each study. One study conducted a comparison between initial and \noptimized prompt. Two studies involved patients' evaluation of the simplified reports \n(Figure 3.).  \n \nDescriptive summary of results \nLyu et al. [13] collected a total of 138 imaging reports, 76 chest CT and 62 brain MRI \n(Table 2). All reports were anonymized. GPT-3.5 was provided with three initial \nprompts requesting simplification of the reports (Table 7). The evaluation focused on \nthree aspects: overall score, completeness, and correctness. The number of places \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nwith missing information and with incorrect information was recorded as well. Two \nradiologists evaluated the results using a 5-point system and word count. Patients \ndid not evaluate the simplified reports. For the chest CT reports, 85.5% of the \ntranslated results (53 of 62) were shorter than the corresponding original reports. \nThe overall length reduction was 26.7%. In brain MRI radiology reports, 72.4% of the \ntranslated results (55 of 76) contained fewer words than the corresponding original \nreports. The overall length reduction was 21.1% (Table 3).  \nNegative performance of ChatGPT was documented as well, showing instances of \ninaccurate, missing or incorrect information (Table 5). They’ve also compared GPT-\n3.5 to GPT-4, with GPT-4 outperforming in all aspects. Lastly, they tested the \ndifference between the original prompt and an optimized prompt. The overall quality \nof translation increased from 55.2% to 77.2%, and the measures on information that \nwere completely omitted, partially translated, and misinterpreted were reduced to \n9.2%, 13.6%, and 0%, respectively (Table 4).  \nLi et al. [14] randomly sampled 100 radiographs (XR), 100 ultrasound (US), 100 CT, \nand 100 MRI radiology reports (Table 2). They prompted GPT-3.5 for simplified \nreports. Mean report length, Flesch reading ease score (FRES), and Flesch-Kincaid \nreading level (FKRL) were calculated for each original report and GPT-3.5 simplified \noutput (Table 3). Patients did not evaluate the simplified reports. Negative GPT-3.5 \nperformance was not detailed. Following simplification by GPT-3.5, all reports had a \nFKRL <8.5 and 77/100 (77%) of XR, 76/100 (76%) of US, 65/100 (65%) of CT, and \n58/100 (58%) of MRI reports had a FKRL <6.5 (Table 3)  \nGrewal et al. [15] tested GPT-4 application in radiology across several fields, \nincluding patient education. GPT-4 generated patient-oriented explanations of \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nradiological findings, and assisted in patient inquiries. Patients did not evaluate the \nsimplified reports. Negative GPT-4 performance was not detailed (Table 3). \nKuckelman et al. [16] selected three common radiologic examinations and \nprocedures: CT, MRI, and bone biopsy. Ten patient questions for each type of \nexamination or procedure were compiled (Table 2). This is the only study that \nutilized Microsoft Bing. The questions were asked on three different chatbot settings \nin two trials, for a total of 360 reviews. Attending radiologist and a fourth-year \nmedical student rated the responses independently for accuracy and completeness \non a 1–3 scale. They used  radiologyinfo.org, an accepted online resource for \ncomparison [17]. The Fleisch-Kincaid level of readability was also examined. Overall, \n336 (93%) ratings were “entirely correct”, and 235 (65%) ratings were “complete”. No \nresponses were rated as “inaccurate/incomplete” by either reviewer. The Fleisch-\nKincaid level of readability was an eighth-grade level (Table 3). Patients did not \nevaluate the simplified reports. Negative Microsoft Bing performance included \nmissing details about bone biopsy procedure (Table 5). \nJeblick et al. [18] wrote three fictitious radiology reports, for knee MRI, head MRI and \nwhole-body CT. The reports were simplified by prompting GPT-3.5. They generated \n15 different simplified reports per original report, 45 in total (Table 2). Many different \nprompt designs were tried. Radiologists evaluated the quality of the simplified reports \nusing a 5-point Likert scale in three categories: factual correctness, completeness \nand potential harm. All simplified reports were factually correct and complete. For \nquality criteria 75% rated “Agree” (Table 3).  Negative GPT-3.5 performance \nincluded incorrect text passages in 23 simplified reports (51%), missing relevant \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \ninformation for 10 simplified reports (22%) and potentially harmful conclusions for 16 \nsimplified reports (36%) (Table 5). Patients did not evaluate the simplified reports.  \n \nScheschenja et al.[19] compiled 133 questions related to three specific interventional \nradiology procedures (Port implantation, percutaneous transluminal angioplasty and \ntranscatheter arterial chemoembolization). They assessed both GPT-3.5 and GPT-4 \nresponses. The chatbot was primed to respond to specific inquiries (Table 2). \nGrading was performed using a 5-point Likert scale. For “completely correct” GPT-\n3.5 scored 30.8%/i1 while GPT-4 scored 35.3%. GPT-4 was found to give significantly \nmore accurate responses than GPT-3 (p/i1 =/i1 0.043) (Table 3). Negative performance \nincluded “mostly incorrect” responses in 5.3% of instances for GPT-3. For GPT-4 just \n2.3%. No response was identified as potentially harmful (Table 5). \n \nGordon et al. [20] assessed GPT-3.5 for accuracy, relevance and readability in \nanswering patient imaging-related questions. They compiled 22 imaging-related \nquestions (Table 2). The categories for the questions included: safety, the radiology \nreport, the procedure, preparation before imaging, meaning of terms and medical \nstaff. Questions were posed to ChatGPT with and without a prompt. Four board-\ncertified radiologists evaluated the answers for accuracy, consistency and relevance. \nTwo patients also reviewed the responses. Readability was assessed by Flesch \nKincaid Grade Level (FKGL). For accuracy GPT-3.5 scored 87% (229/264). \nConsistency of the responses was 86% (76/88). Nearly all responses 99% (261/264) \nwere partially relevant for both prompt and non-prompt questions. The average \nFKGL was high at 13.6. When provided with a prompt, GPT-3.5 performed better in \nall parameters (Table 3). Negative GPT-4 performance was not detailed. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nSchmidt et al. [21] evaluated the ability of GPT-3.5 for simplifying radiological MRI \nfindings. They created five versions of a simplified radiological report using ChatGPT \n3.5 (Table 2). They created different prompts until one prompt was selected for the \nbest outcomes (Table 4). They asked GPT-3.5 for varying levels of complexity: \nsimple, moderate and complex. Two orthopedic surgeons and two radiologists \nevaluated the reports for quality, completeness and comprehensibility using a \nquestionnaire. All simplified reports were evaluated by 20 patients. They used a \npatient-specific questionnaire for comprehension and simplification. The simplified \nradiology reports were factually correct regardless of complexity. The majority of \nparticipants indicated “Agree” with respect to the simplicity and comprehensibility \n(Table 3). Negative performance included missing 53.8% (7/13) or incorrect 23% \n(3/13) information across all simplified findings. For potentially harmful conclusions \nthe simplified reports misinterpreted crucial information 6 times. An incorrect need for \ntherapy was indicated two times, and degeneration was interpreted as injury once \n(Table 5). In addition, patient evaluation showed that while they knew what the text \nwas about, the majority responded that the simplified text did not inform them as well \nas a doctor.  \n \nSome studies presented examples for different prompts they used, as well as \nexamples for the simplification process done by ChatGPT. We included examples \nfrom each study of simplification and prompt generation, presented in Table 6 and \nTable 7, respectively.  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nDiscussion \n \nIn this review we examined LLMs capability to simplify radiological reports and \nprocedures, enhancing patient comprehension and education. All studies reviewed \ndemonstrated LLMs capability in generating simplified, understandable radiological \nreports. \nIn the past, radiology was considered a paraclinical field [22]. Radiology reports were \nwritten for referring physicians and healthcare providers. Nowadays, radiology \nemerges to be more clinical and patient centered [23]. Patients can access their \nimaging reports, but the reports readability is still complex and incomprehensible \n[24]. \n \n \nMaking medicine approachable to patients is a formidable challenge to the medical \ncommunity. Doctors often use complicated medical terms that patients have trouble \nunderstanding [25]. Patients' misperception of medical jargon can lead to confusion, \nstress and overtreatment [26, 27]. The application of advanced AI to explain and \nsimplify imaging reports and procedures could be a step toward accessible and \napproachable medicine.\n \n \nLLMs benefits \nOne of the most limited resources for a medical doctor is time [28, 29]. Working long \nhours, with many tasks and responsibilities, leaves little time to address patients' \ninquiries and concerns [30]. AI continues to evolve, becoming more integrated in \nvarious medical applications [31]. AI performance is fast and efficient [32]. Utilizing \nLLMs chatbots for patient education can save time for both the overworked physician \nand the patient waiting for answers [33].  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n \nAnother important advantage is the chatbot’s ability to simplify complicated medical \ntexts into plain language [34]. Gotlieb et al. showed that several common medical \nphrases are often misunderstood. The interpreted meaning is frequently the exact \nopposite of what is intended [35]. LLMs ability to make medical terms \nunderstandable to patients can alleviate patient concern and anxiety [36]. \n \nLastly, ChatGPT reached over 100 million users in only 2 months [37]. This rapid \nadoption highlights its potential role in improving  accessibility of medical information \nto patients seeking answers. In every study we reviewed, LLMs significantly \nimproved clarity and simplicity of radiological reports. These models may provide the \nsolution to the knowledge gap between patients and their medical information.  \nLLMs drawbacks \n \nWhen patients rely on LLMs for simplifying their medical imaging reports, they also \nneed to be certain of the medical accuracy. A known limitation for LLMs is called \n“hallucination”. This occurs when generative AI misinterprets the given prompt, \nresulting in outputs that lack logical consistency. When relying on AI for accurate \nmedical information, this phenomenon is unacceptable [38]. Also, LLMs can often \nmisinterpret clinical findings [39]. For example, some of the studies we reviewed \npresented AI output that mistook benign findings as malignant. This can lead to \nneedless patient anxiety and interfere with the physician ability to reach the correct \ndiagnosis [40]  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nAnother concern is the patient's medical information safety. Medical imaging reports \nhold private medical information, and can often be the target of cyber-attacks [41].  \nFinally, another consideration is the disparity between different age groups in their \nability to use technology. When applying LLMs for simplifying patients' imaging \nreports, a certain level of technological abilities is needed. Older individuals might \nfind it harder to apply LLMs to obtain readability for their medical reports [42, 43].    \n \nIt is imperative to take into consideration those significant shortcomings and \nchallenges. LLMs should be used with caution while utilized to simplify important \nmedical information. \n \nPrompt engineering \nFor each study we examined the process of crafting the prompts. We noticed a wide \nrange of approaches to writing and designing prompts. Only one study [13] \nconducted prompt optimization, which significantly improved the LLM’s outputs. This \nemphasizes the importance and sensitivity of prompts. Prompt-engineering may be a \ntask that requires specific training, so that the prompt is phrased correctly and the \nquality of the simplified medical report is not impaired.   \n \nLimitations \nOur review has several limitations. Due to heterogeneity in study design and data, a \nmeta-analysis was not performed. Only two studies tested the simplified result with \npatients. Several studies used word count as representation of simplification. \nHowever, a shorter text is not always a guarantee for simplification. This was not \nexamined. Only one study conducted prompt optimization and evaluated its \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \noutcomes. Two studies were at high risk of bias. One study did not present clear \nparameters for evaluation of bias. Additional studies will be needed to further solidify \nthe usefulness of LLMs in simplifying radiological reports and clarifying radiologic \nprocedures. Lastly, we limited our search to PubMed/MEDLINE. We did so due to its \nrelevance in biomedical research. We recognize this choice narrows our review's \nscope. This might exclude studies from other databases, possibly limiting diverse \ninsights. \n \nConclusion \nUtilizing LLMs for simplifying medical imaging reports and procedures is feasible. In \nthe majority of the studies we reviewed, LLMs demonstrated promise in their \ncapability to generate accessible medical imaging reports. However, their use \nwarrants cautious, critical evaluation. Awareness of LLMs limitations is needed in \norder to avoid misuse and harming patients' diagnosis and treatment. Currently, \nfurther research in this field is warranted. Until further advancements are achieved, \nAI should be used with caution when applying it to simplify medical information for \npatients. \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n \n \nDisclosure statement  \nThe authors report there are no competing interests to declare  \nAdditional information  \nFunding  \nThe author(s) reported there is no funding associated with the work featured in this \narticle.  \nAcknowledgments  \nNone  \nDisclaimer  \nNone \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \nReferences  \n \n1. Atsina, Kofi-Buaku et al. “Advanced Imaging Interpretation by Radiologists \nand Nonradiologist Physicians: A Training Issue.” AJR. American journal of \nroentgenology vol. 214,1 (2020): W55-W61. doi:10.2214/AJR.19.21802 \n \n2. Madrigano, Renata Rodrigues et al. “Evaluation of non-radiologist physicians' \nknowledge on aspects related to ionizing radiation in imaging.” Radiologia \nbrasileira vol. 47,4 (2014): 210-6. doi:10.1590/0100-3984.2013.1840 \n \n3. Rosenkrantz, Andrew B, and Eric R Flagg. “Survey-Based Assessment of \nPatients' Understanding of Their Own Imaging Examinations.” Journal of the \nAmerican College of Radiology : JACR vol. 12,6 (2015): 549-55. \ndoi:10.1016/j.jacr.2015.02.006 \n \n4. Martin-Carreras, Teresa et al. “Readability of radiology reports: implications \nfor patient-centered care.” Clinical imaging vol. 54 (2019): 116-120. \ndoi:10.1016/j.clinimag.2018.12.006 \n \n5. Sneider, Michael B, and Corey D Kershaw. “The Importance of Imaging in the \nAssessment of Interstitial Lung Diseases.” Journal of thoracic imaging vol. \n38,Suppl 1 (2023): S2-S6. doi:10.1097/RTI.0000000000000708 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n \n6. Sotoudeh, Houman, and Masoumeh Gity. “The Role of Medical Imaging in \nCOVID-19.” Advances in experimental medicine and biology vol. 1318 (2021): \n413-434. doi:10.1007/978-3-030-63761-3_24 \n \n7. Ballard, David H et al. “The Role of Imaging in Health Screening: Overview, \nRationale of Screening, and Screening Economics.” Academic radiology vol. \n28,4 (2021): 540-547. doi:10.1016/j.acra.2020.03.038 \n \n8. Reynolds, April. “Patient-centered Care.” Radiologic technology vol. 81,2 \n(2009): 133-47. \n \n9. Nickel, Brooke et al. “Words do matter: a systematic review on how different \nterminology for the same condition influences management preferences.” \nBMJ open vol. 7,7 e014129. 10 Jul. 2017, doi:10.1136/bmjopen-2016-014129 \n \n10. Clusmann, Jan et al. “The future landscape of large language models in \nmedicine.” Communications medicine vol. 3,1 141. 10 Oct. 2023, \ndoi:10.1038/s43856-023-00370-1 \n \n11. De Angelis, Luigi et al. “ChatGPT and the rise of large language models: the \nnew AI-driven infodemic threat in public health.” Frontiers in public health vol. \n11 1166120. 25 Apr. 2023, doi:10.3389/fpubh.2023.1166120 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n \n12. Hatherley, Joshua James. “Limits of trust in medical AI.” Journal of medical \nethics vol. 46,7 (2020): 478-481. doi:10.1136/medethics-2019-105935 \n \n13. Lyu, Qing et al. “Translating radiology reports into plain language using \nChatGPT and GPT-4 with prompt learning: results, limitations, and potential.” \nVisual computing for industry, biomedicine, and art vol. 6,1 9. 18 May. 2023, \ndoi:10.1186/s42492-023-00136-5 \n \n14. Li, Hanzhou et al. “Decoding radiology reports: Potential application of \nOpenAI ChatGPT to enhance patient understanding of diagnostic reports.” \nClinical imaging vol. 101 (2023): 137-141. doi:10.1016/j.clinimag.2023.06.008 \n \n15. Grewal, Harpreet et al. “Radiology Gets Chatty: The ChatGPT Saga Unfolds.” \nCureus vol. 15,6 e40135. 8 Jun. 2023, doi:10.7759/cureus.40135 \n \n16. Kuckelman, Ian J., et al. “Assessing AI-Powered Patient Education: A case \nstudy in radiology.” Academic Radiology, 2023, \nhttps://doi.org/10.1016/j.acra.2023.08.020. \n17. Radiology Info for Patients. Computed tomography of the abdomen and \npelvis; MRI spine; bone biopsy. Retrieved \nfrom〈 https://www.radiologyinfo.org/ \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n18. Jeblick, Katharina et al. “ChatGPT makes medicine easy to swallow: an \nexploratory case study on simplified radiology reports.” European radiology, \n10.1007/s00330-023-10213-1. 5 Oct. 2023, doi:10.1007/s00330-023-10213-1 \n19. Scheschenja, Michael et al. “Feasibility of GPT-3 and GPT-4 for in-Depth \nPatient Education Prior to Interventional Radiological Procedures: A \nComparative Analysis.” Cardiovascular and interventional radiology, \n10.1007/s00270-023-03563-2. 23 Oct. 2023, doi:10.1007/s00270-023-03563-\n2 \n20. Gordon, Emile B et al. “Enhancing patient communication with Chat-GPT in \nradiology: evaluating the efficacy and readability of answers to common \nimaging-related questions.” Journal of the American College of Radiology : \nJACR, S1546-1440(23)00775-5. 18 Oct. 2023, doi:10.1016/j.jacr.2023.09.011 \n21. Schmidt, Sebastian et al. “Simplifying radiologic reports with natural language \nprocessing: a novel approach using ChatGPT in enhancing patient \nunderstanding of MRI results.” Archives of orthopaedic and trauma surgery, \n10.1007/s00402-023-05113-4. 11 Nov. 2023, doi:10.1007/s00402-023-05113-\n4 \n \n \n22. Boey, Hong Khim. “The evolution of radiology from paraclinical to clinical.” \nAnnals of the Academy of Medicine, Singapore vol. 38,7 (2009): 653-7. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n23. Itri, Jason N. “Patient-centered Radiology.” Radiographics : a review \npublication of the Radiological Society of North America, Inc vol. 35,6 (2015): \n1835-46. doi:10.1148/rg.2015150110 \n \n24. Patil, Siya et al. “Radiology Reporting in the Era of Patient-Centered Care: \nHow Can We Improve Readability?.” Journal of digital imaging vol. 34,2 \n(2021): 367-373. doi:10.1007/s10278-021-00439-0 \n \n25. Gotlieb, Rachael et al. “Accuracy in Patient Understanding of Common \nMedical Phrases.” JAMA network open vol. 5,11 e2242972. 1 Nov. 2022, \ndoi:10.1001/jamanetworkopen.2022.42972 \n \n26. Chapple, A et al. “Clinical terminology: anxiety and confusion amongst \nfamilies undergoing genetic counseling.” Patient education and counseling \nvol. 32,1-2 (1997): 81-91. doi:10.1016/s0738-3991(97)00065-7 \n27. Nickel, Brooke et al. “Words do matter: a systematic review on how different \nterminology for the same condition influences management preferences.” \nBMJ open vol. 7,7 e014129. 10 Jul. 2017, doi:10.1136/bmjopen-2016-014129 \n \n28. Prasad, Kriti et al. “Time Pressure During Primary Care Office Visits: a \nProspective Evaluation of Data from the Healthy Work Place Study.” Journal \nof general internal medicine vol. 35,2 (2020): 465-472. doi:10.1007/s11606-\n019-05343-6 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n \n29. Moura, Felipe Scipião et al. “Physicians' working time restriction and its \nimpact on patient safety: an integrative review.” Revista brasileira de medicina \ndo trabalho : publicacao oficial da Associacao Nacional de Medicina do \nTrabalho-ANAMT vol. 16,4 482-491. 24 Apr. 2020, \ndoi:10.5327/Z1679443520180294 \n \n30. Dugdale, D C et al. “Time and the patient-physician relationship.” Journal of \ngeneral internal medicine vol. 14 Suppl 1,Suppl 1 (1999): S34-40. \ndoi:10.1046/j.1525-1497.1999.00263.x \n \n31. Rajpurkar, Pranav et al. “AI in health and medicine.” Nature medicine vol. 28,1 \n(2022): 31-38. doi:10.1038/s41591-021-01614-0 \n32. Yin, Jiamin et al. “Role of Artificial Intelligence Applications in Real-Life \nClinical Practice: Systematic Review.” Journal of medical Internet research \nvol. 23,4 e25759. 22 Apr. 2021, doi:10.2196/25759 \n \n33. Priolo, Manuela, and Marco Tartaglia. “The Right to Ask, the Need to Answer-\nWhen Patients Meet Research: How to Cope with Time.” International journal \nof environmental research and public health vol. 20,5 4573. 4 Mar. 2023, \ndoi:10.3390/ijerph20054573 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n34. Kirchner, Gregory J et al. “Can Artificial Intelligence Improve the Readability of \nPatient Education Materials?.” Clinical orthopaedics and related research vol. \n481,11 (2023): 2260-2267. doi:10.1097/CORR.0000000000002668 \n \n35. Gotlieb, Rachael et al. “Accuracy in Patient Understanding of Common \nMedical Phrases.” JAMA network open vol. 5,11 e2242972. 1 Nov. 2022, \ndoi:10.1001/jamanetworkopen.2022.42972 \n \n36. Mueller, P R et al. “Interventional radiologic procedures: patient anxiety, \nperception of pain, understanding of procedure, and satisfaction with \nmedication--a prospective study.” Radiology vol. 215,3 (2000): 684-8. \ndoi:10.1148/radiology.215.3.r00jn33684 \n \n37. Mesko, Bertalan. “The ChatGPT (Generative Artificial Intelligence) Revolution \nHas Made Artificial Intelligence Approachable for Medical Professionals.” \nJournal of medical Internet research vol. 25 e48392. 22 Jun. 2023, \ndoi:10.2196/48392 \n \n38. Sharun, Khan et al. “ChatGPT and artificial hallucinations in stem cell \nresearch: assessing the accuracy of generated references - a preliminary \nstudy.” Annals of medicine and surgery (2012) vol. 85,10 5275-5278. 1 Sep. \n2023, doi:10.1097/MS9.0000000000001228 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n39. Seyyed-Kalantari, Laleh et al. “Underdiagnosis bias of artificial intelligence \nalgorithms applied to chest radiographs in under-served patient populations.” \nNature medicine vol. 27,12 (2021): 2176-2182. doi:10.1038/s41591-021-\n01595-0 \n \n40. Bernstein, Michael H et al. “Can incorrect artificial intelligence (AI) results \nimpact radiologists, and if so, what can we do about it? A multi-reader pilot \nstudy of lung cancer detection with chest radiography.” European radiology \nvol. 33,11 (2023): 8263-8269. doi:10.1007/s00330-023-09747-1 \n \n41. Sorin, Vera et al. “Adversarial attacks in radiology - A systematic review.” \nEuropean journal of radiology vol. 167 (2023): 111085. \ndoi:10.1016/j.ejrad.2023.111085 \n \n42. Köttl, Hanna et al. “\"But at the age of 85? Forget it!\": Internalized ageism, a \nbarrier to technology use.” Journal of aging studies vol. 59 (2021): 100971. \ndoi:10.1016/j.jaging.2021.100971 \n \n43. Kunonga, Tafadzwa Patience et al. “Effects of Digital Technologies on Older \nPeople's Access to Health and Social Care: Umbrella Review.” Journal of \nmedical Internet research vol. 23,11 e25887. 24 Nov. 2021, \ndoi:10.2196/25887 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted January 9, 2024. ; https://doi.org/10.1101/2024.01.05.24300884doi: medRxiv preprint "
}