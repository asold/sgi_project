{
    "title": "Sinkformers: Transformers with Doubly Stochastic Attention",
    "url": "https://openalex.org/W3210498880",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5090284871",
            "name": "Michael E. Sander",
            "affiliations": [
                "Département de mathématiques et applications",
                "Laboratoire de Mathématiques d'Orsay",
                "Université Paris Sciences et Lettres",
                "École Normale Supérieure - PSL"
            ]
        },
        {
            "id": "https://openalex.org/A5042340163",
            "name": "Pierre Ablin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5049123454",
            "name": "Mathieu Blondel",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5058651667",
            "name": "Gabriel Peyré",
            "affiliations": [
                "Département de mathématiques et applications",
                "Laboratoire de Mathématiques d'Orsay",
                "Université Paris Sciences et Lettres",
                "École Normale Supérieure - PSL"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2600297185",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2970791107",
        "https://openalex.org/W3035208336",
        "https://openalex.org/W2143420533",
        "https://openalex.org/W1990283121",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W3167133407",
        "https://openalex.org/W1542343170",
        "https://openalex.org/W2974916071",
        "https://openalex.org/W3158009924",
        "https://openalex.org/W1963816084",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2018304737",
        "https://openalex.org/W3167630251",
        "https://openalex.org/W2962781405",
        "https://openalex.org/W2963302407",
        "https://openalex.org/W3175639804",
        "https://openalex.org/W2963755523",
        "https://openalex.org/W3034609440",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W3131747507",
        "https://openalex.org/W2146508075",
        "https://openalex.org/W3039256091",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3041866211",
        "https://openalex.org/W2071048859",
        "https://openalex.org/W2158131535",
        "https://openalex.org/W3103894541",
        "https://openalex.org/W2598666589",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3127760629",
        "https://openalex.org/W1920022804",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2970181183",
        "https://openalex.org/W2953273646",
        "https://openalex.org/W2963095610",
        "https://openalex.org/W2970900903",
        "https://openalex.org/W2995273672",
        "https://openalex.org/W2902305746",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2523246573",
        "https://openalex.org/W2519388618",
        "https://openalex.org/W2963540976",
        "https://openalex.org/W2964095938",
        "https://openalex.org/W3128847475",
        "https://openalex.org/W2505728881",
        "https://openalex.org/W1984032850",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2805680509",
        "https://openalex.org/W3171952585",
        "https://openalex.org/W2964095333",
        "https://openalex.org/W1585773866",
        "https://openalex.org/W3103322710"
    ],
    "abstract": "Attention based models such as Transformers involve pairwise interactions between data points, modeled with a learnable attention matrix. Importantly, this attention matrix is normalized with the SoftMax operator, which makes it row-wise stochastic. In this paper, we propose instead to use Sinkhorn's algorithm to make attention matrices doubly stochastic. We call the resulting model a Sinkformer. We show that the row-wise stochastic attention matrices in classical Transformers get close to doubly stochastic matrices as the number of epochs increases, justifying the use of Sinkhorn normalization as an informative prior. On the theoretical side, we show that, unlike the SoftMax operation, this normalization makes it possible to understand the iterations of self-attention modules as a discretized gradient-flow for the Wasserstein metric. We also show in the infinite number of samples limit that, when rescaling both attention matrices and depth, Sinkformers operate a heat diffusion. On the experimental side, we show that Sinkformers enhance model accuracy in vision and natural language processing tasks. In particular, on 3D shapes classification, Sinkformers lead to a significant improvement.",
    "full_text": "Sinkformers: Transformers with Doubly Stochastic Attention\nMichael E. Sander Pierre Ablin Mathieu Blondel Gabriel Peyr´ e\nENS and CNRS ENS and CNRS Google Research, Brain team ENS and CNRS\nAbstract\nAttention based models such as Transformers\ninvolve pairwise interactions between data\npoints, modeled with a learnable attention\nmatrix. Importantly, this attention ma-\ntrix is normalized with the SoftMax oper-\nator, which makes it row-wise stochastic.\nIn this paper, we propose instead to use\nSinkhorn’s algorithm to make attention ma-\ntrices doubly stochastic. We call the resulting\nmodel a Sinkformer. We show that the row-\nwise stochastic attention matrices in classical\nTransformers get close to doubly stochastic\nmatrices as the number of epochs increases,\njustifying the use of Sinkhorn normalization\nas an informative prior. On the theoretical\nside, we show that, unlike the SoftMax op-\neration, this normalization makes it possible\nto understand the iterations of self-attention\nmodules as a discretized gradient-ﬂow for the\nWasserstein metric. We also show in the in-\nﬁnite number of samples limit that, when\nrescaling both attention matrices and depth,\nSinkformers operate a heat diﬀusion. On the\nexperimental side, we show that Sinkformers\nenhance model accuracy in vision and natu-\nral language processing tasks. In particular,\non 3D shapes classiﬁcation, Sinkformers lead\nto a signiﬁcant improvement.\n1 Introduction\nThe Transformer (Vaswani et al., 2017), an archi-\ntecture that relies entirely on attention mechanisms\n(Bahdanau et al., 2014), has achieved state of the\nart empirical success in natural language processing\n(NLP) (Brown et al., 2020; Radford et al., 2019; Wolf\nTo appear in the proceedings of the 25th International Con-\nference on Artiﬁcial Intelligence and Statistics (AISTATS)\n2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022\nby the author(s).\net al., 2019) as well as in computer vision (Dosovitskiy\net al., 2020; Zhao et al., 2020; Zhai et al., 2021; Lee\net al., 2019). As the key building block of the Trans-\nformer, the self-attention mechanism takes the follow-\ning residual form (Yun et al., 2019) given a n-sequence\n(x1,x2,...,x n), embedded in dimension d:\nxi ←xi +\nn∑\nj=1\nK1\ni,jWVxj, (1)\nwhere K1 := SoftMax(C) with Ci,j := (WQxi)⊤WKxj\n= x⊤\ni W⊤\nQWKxj. Here, WQ,WK ∈Rm×d and WV ∈\nRd×d are the query, key and value matrices. The Soft-\nMax operator can be seen as a normalization of the\nmatrix K0 := exp(C) as follows: K1\nij := K0\nij/∑n\nl=1 K0\nil\nfor all iand j. Importantly, the matrix K1 is row-wise\nstochastic: its rows all sum to 1.\nIn this work, we propose to take the normalization pro-\ncess further by successively normalizing the rows and\ncolumns of K0. This process is known to provably con-\nverge to a doubly stochastic matrix (i.e., whose rows\nand columns both sum to 1) and is called Sinkhorn’s\nalgorithm (Sinkhorn, 1964; Cuturi, 2013; Peyr´ e et al.,\n2019). We denote the resulting doubly stochastic ma-\ntrix K∞. Intuitively, such a normalization relies on\na democratic principle where all points are matched\none to another with diﬀerent degrees of intensity, so\nthat more interactions are considered than with the\nSoftMax normalization, as shown in Figure 1.\nK0\n K1\n K∞\nFigure 1: Illustration of the diﬀerent normaliza-\ntions of attention matrices. We form two point\nclouds (WQxi)1≤i≤10 (green) and (WKxj)1≤i≤10 (red).\nFor k ∈ {0,1,∞}, the width of the line connecting\nxi to xj is Kk\ni,j. We only display connections with\nKk\ni,j ≥ 10−12. For K0, one interaction dominates.\nFor K1 (SoftMax), one cluster is ignored. For K∞\n(Sinkhorn), all points are involved in an interaction.\narXiv:2110.11773v2  [cs.LG]  24 Jan 2022\nSinkformers: Transformers with Doubly Stochastic Attention\nWe call our Transformer variant where the SoftMax is\nreplaced by Sinkhorn a Sinkformer. Since Sinkhorn’s\nﬁrst iteration coincides exactly with the SoftMax,\nSinkformers include Transformers as a special case.\nOur modiﬁcation is diﬀerentiable, easy to implement\nusing deep learning libraries, and can be executed on\nGPUs for fast computation. Because the set of row-\nwise stochastic matrices contains the set of doubly\nstochastic matrices, the use of doubly stochastic matri-\nces can be interpreted as a prior. On the experimental\nside, we conﬁrm that doubly stochastic attention leads\nto better accuracy in several learning tasks. On the\ntheoretical side, doubly stochastic matrices also give a\nbetter understanding of the mathematical properties\nof self-attention maps.\nTo summarize, we make the following contributions.\n• We show empirically that row-wise stochastic ma-\ntrices seem to converge to doubly stochastic matri-\nces during the learning process in several classical\nTransformers (Figure 2). Motivated by this ﬁnding,\nwe then introduce the Sinkformer, an extension of\nthe Transformer in which the SoftMax is replaced by\nthe output of Sinkhorn’s algorithm. In practice, our\nmodel is parametrized by the number of iterations\nin the algorithm, therefore interpolating between the\nTransformer and the Sinkformer.\n• On the theoretical side, we show that Transformers\nand Sinkformers can be viewed as models acting on\ndiscrete distributions, and we show under a symme-\ntry assumption that Sinkformers can be seen in the\ninﬁnite depth limit as a Wasserstein gradient ﬂow\nfor an energy minimization (Proposition 2). We also\nshow that the classical Transformer with the Soft-\nMax operator cannot be interpreted as such a ﬂow\n(Proposition 3). To the best of our knowledge, this\nis the ﬁrst time such a connection is established.\nWe also prove that in the inﬁnite number of parti-\ncles limit (when ngoes to inﬁnity), the iterations of\nSinkformers converge to the heat equation (Theorem\n1), while the corresponding equation for Transform-\ners is nonlinear and nonlocal (Proposition 4).\n• On the experimental side, we show that Sinkform-\ners lead to a signiﬁcant accuracy gain compared to\nTransformers on the ModelNet 40 3D shapes clas-\nsiﬁcation task. We then demonstrate better perfor-\nmance of Sinkformers on the NLP IMDb dataset for\nsentiment analysis and IWSLT’14 German to En-\nglish neural machine translation tasks. Sinkformers\nalso achieve a better accuracy than Vision Trans-\nformers on image classiﬁcation tasks. Therefore,\nthe proposed method is capable of enhancing the\nperformance of transformers in a wide range of ap-\nplications.\n2 Background and related work\nTransformers. Proposed by Vaswani et al. (2017),\nthe Transformer is a fully attention-based architecture.\nOriginally designed to process sequences for natural\nlanguage processing (NLP), many variants have since\nbeen developed such as Vision Transformers (Dosovit-\nskiy et al., 2020; Zhai et al., 2021), Set Transformers\n(Lee et al., 2019) or Point Cloud Transformers (Zhao\net al., 2020). The Transformer and its variants are\nbased on an encoder-decoder structure, where the de-\ncoder can have a more or less complex form. The\nencoder is fully self -attention based. After embed-\nding and concatenating with positional encoding the\noriginal input sequence, the encoder uses a series of\nresidual blocks that iterates relation (1) followed by a\nfeed forward neural network applied to each xi inde-\npendently. In its most complex form such as in neu-\nral machine translation, the decoder combines a self-\nattention based mechanisms and across attention one,\nmeaning that it is given access to the encoder via an-\nother multi-head attention block.\nSinkhorn and Attention. To the best of our\nknowledge, using Sinkhorn’s algorithm in Transform-\ners has been done once in a diﬀerent context (Tay\net al., 2020). The authors propose to learn eﬃcient\nand sparse attention using a diﬀerentiable algorithm\nfor sorting and rearranging elements in the input se-\nquence. For this purpose, they introduce a sorting\nnetwork to generate a doubly-stochastic matrix (that\ncan be seen as a relaxed version of a permutation ma-\ntrix) and use it to sort the sequence in a diﬀeren-\ntiable fashion. Mialon et al. (2021) propose an em-\nbedding for sets of features in Rd based on Sinkhorn’s\nalgorithm, by using the regularized optimal transport\nplan between data points and a reference set. Nicu-\nlae et al. (2018) use doubly stochastic attention matri-\nces in LSTM-based encoder-decoder networks but they\nuse Frank-Wolfe or active set methods to compute the\nattention matrix. None of these works use Sinkhorn\non self-attention maps in Transformers and provide its\ntheoretical analysis, as we do.\nImpact of bi-normalization. Theoretical proper-\nties of kernels K, which attention is an instance of, can\nalso be studied through the operator f ↦→f−Kf. Bi-\nnormalization of kernels over manifolds have already\nbeen studied in the literature, on uniform measures\n(Singer, 2006), weighted measures (Hein et al., 2007)\nand in a more general setup with associated diﬀusion\noperators (Ting et al., 2011). Milanfar (2013) pro-\nposes to approximate smoothing operators by doubly\nstochastic matrices using Sinkhorn’s updates, leading\nto better performance in data analysis and signal pro-\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr´ e\ncessing. Importantly, the works of Marshall and Coif-\nman (2019) and Wormell and Reich (2021) exactly in-\ntroduce a normalization that is based on Sinkhorn’s\nalgorithm. They prove that this method models a\nLangevin diﬀusion and leads to the approximation of a\nsymmetric operator. They also show that convergence\nto this operator is faster with Sinkhorn normalization\nthan with the SoftMax normalization. In section 5,\nwe adopt a similar point of view with a parametrized\ncost and show that diﬀerent normalizations result in\ndiﬀerent partial diﬀerential equations (PDEs) in the\ninﬁnite number of particles limit.\nInﬁnite depth limit. Studying deep residual neural\nnetworks (ResNets) (He et al., 2016) in the inﬁnites-\nimal step-size regime (or inﬁnite depth limit) has re-\ncently emerged as a new framework for analyzing their\ntheoretical properties. The ResNet equation\nxi ←xi + T(xi) (2)\ncan indeed be seen as a discretized Euler scheme with\nunit step size of the ordinary diﬀerential equation\n(ODE) ˙xi = T(xi) (Weinan, 2017; Chen et al., 2018;\nTeh et al., 2019; Sun et al., 2018; Weinan et al., 2019;\nLu et al., 2018; Ruthotto and Haber, 2019; Sander\net al., 2021). In section 4, we adopt this point of\nview on residual attention layers in order to get a bet-\nter theoretical understanding of attention mechanisms.\nThis is justiﬁed by the fact that, for instance, GPT-3\n(Brown et al., 2020) has 96 layers.\nNeural networks on measures. The self-attention\nmechanism (1) acts on sets {xi}i where the order-\ning of the elements does not matter. An equiva-\nlent way to model such invariant architectures is to\nconsider them as acting on probability measures or\npoint clouds of varying cardinality (De Bie et al.,\n2019; Vuckovic et al., 2021; Zweig and Bruna, 2021).\nSpeciﬁcally, a collection of points ( xi)1≤i≤n, where\nxi ∈Rd, can also be seen as a discrete measure on\nRd: µ := 1\nn\n∑n\ni=1 δxi ∈M(Rd), where M(Rd) is the\nset of probability measures on Rd. A map Tµ then\nacts on µ through F(µ) := 1\nn\n∑n\ni=1 δTµ(xi). One no-\ntable interest of such a point of view is to consider\nthe evolution of non ordered sets of points. Another\nis to consider the mean ﬁeld (or large sample) limit,\nthat is when n →∞, to conduct theoretical analysis\n(Zweig and Bruna, 2021) as when analyzing the SGD\nproperties in the mean-ﬁeld limit (Song et al., 2018).\n3 Sinkformers\nWe now introduce Sinkformers, a modiﬁcation of any\nTransformer by replacing the SoftMax operator in the\nattention modules by Sinkhorn’s algorithm.\nAttention matrices during training. In Trans-\nformers, attention matrices are row-wise stochastic. A\nnatural question is how the sum over columns evolve\nduring training. On 3 diﬀerent models and 3 diﬀerent\nlearning tasks, we calculated the sum over columns of\nattention matrices in Transformers. We ﬁnd out that\nthe learning process makes the attention matrices more\nand more doubly stochastic, as shown in Figure 2.\n0 50000\nSorted columns\n0.1\n1\n10\nSum of coeﬃcients\nVision\nTransformer\n1\n4\n8\n12\n16\n20\n0 5000\nSorted columns\n0.1\n1\n10\nFairseq\nTransformer\n1\n2\n3\n4\n5\n6\n0 10000\nSorted columns\n10−6\n10−3\n1\nPoint Cloud\nTransformer\n10\n30\n50\n70\n110\n160\nFigure 2: Sum over columns of attention matri-\nces at diﬀerent training epochs (color) when training,\nfrom left to right, a ViT on MNIST (section 6.4), a\nfairseq Transformer on IWSLT’14 (section 6.3), and\na Point Cloud Transformer on Model Net 40 (section\n6.1). The majority of columns naturally sum\nclosely to 1.\nThus, row-wise stochastic attention matrices seem to\napproach doubly stochastic matrices during the learn-\ning process in classical Transformers. Therefore, it\nseems natural to impose double stochasticity as a prior\nand study theoretically and experimentally the result-\ning model. A process to obtain such matrices which\nextends the SoftMax is Sinkhorn’s algorithm.\nSinkhorn’s algorithm. Given a matrix C ∈Rn×n,\nand denoting K0 ∈ Rn×n such that K0 = exp( C),\nSinkhorn’s algorithm (Sinkhorn, 1964; Cuturi, 2013;\nPeyr´ e et al., 2019) iterates, starting fromK0:\nKl+1 =\n{ NR(Kl) if l is even\nNC(Kl) if l is odd, (3)\nwhere NR and NC correspond to row-wise and column-\nwise normalizations: ( NR(K))i,j := Ki,j∑n\nl=1 Ki,l\nand\n(NC(K))i,j := Ki,j∑n\nl=1 Kl,j\n. We denote the resulting\nscaled matrix limit K∞ := Sinkhorn(C). Note that\nit is doubly stochastic in the sense that K∞1n = 1n\nand K∞⊤1n = 1n. The operations in (3) are perfectly\nsuited for being executed on GPUs (Charlier et al.,\n2021; Cuturi, 2013).\nSinkformers. For simplicity, we consider a one head\nattention block that iterates equation (1). Note\nthat K1 := SoftMax(C) is precisely the output of\nSinkhorn’s algorithm (3) after 1 iteration. In this\nSinkformers: Transformers with Doubly Stochastic Attention\npaper, we propose to take Sinkhorn’s algorithm sev-\neral steps further until it approximately converges to\na doubly stochastic matrix K∞. This process can\nbe easily implemented in practice, simply by plug-\nging Sinkhorn’s algorithm into self-attention modules\nin existing architectures, without changing the overall\nstructure of the network. We call the resulting drop-in\nreplacement of a Transformer a Sinkformer. It iterates\nxi ←xi +\nn∑\nj=1\nK∞\ni,jWVxj. (4)\nIn the next two sections 4 and 5, we investigate the\ntheoretical properties of Sinkformers. We exhibit con-\nnections with energy minimization in the space of mea-\nsures and the heat equation, thereby proposing a new\nframework for understanding attention mechanisms.\nAll our experiments are described in Section 6 and\nshow the beneﬁts of using Sinkformers in a wide vari-\nety of applications.\nComputational cost and diﬀerentiation. Turn-\ning a Transformer into a Sinkformer simply relies on\nreplacing the SoftMax by Sinkhorn, i.e., substituting\nK1 with K∞. In practice, we use a ﬁnite number\nof Sinkhorn iterations and therefore use Kl, where l\nis large enough so that Kl is almost doubly stochas-\ntic. Doing literations of Sinkhorn takes ltimes longer\nthan the SoftMax. However, this is not a problem in\npractice because Sinkhorn is not the main computa-\ntional bottleneck and because only a few iterations of\nSinkhorn are suﬃcient (typically 3 to 5) to converge to\na doubly stochastic matrix. As a result, the practical\ntraining time of Sinkformers is comparable to regular\nTransformers, as detailed in our experiments.\nSinkhorn is perfectly suited for backpropagation (au-\ntomatic diﬀerentiation), by diﬀerentiating through the\noperations of (3). The Jacobian of an optimization\nproblem solution can also be computed using the im-\nplicit function theorem (Griewank and Walther, 2008;\nKrantz and Parks, 2012; Blondel et al., 2021) instead of\nbackpropagation if the number of iterations becomes a\nmemory bottleneck. Together with Sinkhorn, implicit\ndiﬀerentiation has been used by Luise et al. (2018) and\nCuturi et al. (2020).\nInvariance to the cost function. Recall that in\npractice one has Ci,j = (WQxi)⊤WKxj. An important\naspect of Sinkformers is that their output is unchanged\nif the cost is modiﬁed with non interacting terms, as\nthe next proposition shows.\nProposition 1. Let C ∈Rn×n. Consider, for (f,g) ∈\nRn×Rn the modiﬁed cost function ˜Ci,j := Ci,j+fi+gj.\nThen Sinkhorn(C) = Sinkhorn( ˜C).\nA proof is available in Appendix A. A consequence\nof this result is that one can consider the cost ˜Ci,j :=\n−1\n2 ∥WQxi−WKxj∥2 instead of Ci,j = (WQxi)⊤WKxj,\nwithout aﬀecting K∞. A Transformer using the cost\n˜C is referred to as L2 self-attention, and is Lipschitz\nunder some assumptions (Kim et al., 2021) and can\ntherefore be used as an invertible model (Behrmann\net al., 2019). For instance, we use ˜C in Proposition 4.\n4 Attention and gradient ﬂows\nIn this section, we make a parallel between self-\nattention modules in Sinkformers and gradient ﬂows in\nthe space of measures. We denoteM(Rd) the probabil-\nity measures on Rd and C(Rd) the continuous functions\non Rd. We denote ∇the gradient operator, div the di-\nvergence, and ∆ the Laplacian, that is ∆ = div( ∇).\nResidual maps for attention. We consider a one-\nhead attention block operating with diﬀerent normal-\nizations. We consider the continuous counterparts of\nthe attention matrices seen in the previous section. We\ndenote c(x,x′) := (WQx)⊤WKx′and k0 := exp(c). For\nsome measure µ∈M(Rd), we deﬁne the SoftMax op-\nerator on the costcby k1(x,x′) = SoftMax(c)(x,x′) :=\nk0(x,x′)∫\nk0(x,y)dµ(y) . Similarly, we deﬁne Sinkhorn’s algo-\nrithm as the following iterations, starting from k0 =\nexp(c):\nkl+1(x,x′) =\n\n\n\nkl(x,x′)∫\nkl(x,y)dµ(y) if l is even\nkl(x,x′)∫\nkl(y,x)dµ(y) if l is odd.\nWe denote k∞ := Sinkhorn(c) the resulting limit.\nNote that if µ is a discrete measure supported on a n\nsequence of particles ( x1,x2,...,x n), µ = 1\nn\n∑n\ni=1 δxi,\nthen for all ( i,j), k0(xi,xj) = K0\ni,j, k1(xi,xj) = K1\ni,j\nand k∞(xi,xj) = K∞\ni,j, so that k0, k1 and k∞ are in-\ndeed the continuous equivalent of the matricesK0, K1\nand K∞respectively.\nInﬁnitesimal step-size regime. In order to bet-\nter understand the theoretical properties of attention\nmatrices in Transformers and Sinkformers, we omit\nthe feed forward neural networks acting after each\nattention block. We consider a succession of atten-\ntion blocks with tied weights between layers and study\nthe inﬁnite depth limit where the output is given by\nsolving a neural ODE (Chen et al., 2018). In this\nframework, iterating the Transformer equation (1), the\nResNet equation (2) and the Sinkformer equation (4)\ncorresponds to a Euler discretization with step-size 1\nof the ODEs\n˙xi = Tµ(xi) for all i,\nwhere xi(t) is the position of xi at time t. For an\narbitrary measure µ ∈ M(Rd), these ODEs can be\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr´ e\nequivalently written as a continuity equation (Renardy\nand Rogers, 2006)\n∂tµ+ div(µTµ) = 0. (5)\nWhen Tµ is deﬁned by the ResNet equation (2),\nTµ = T does not depend on µ. It deﬁnes an ad-\nvection equation where the particles do not interact\nand evolve independently. When Tµ is deﬁned by the\nTransformer equation (1) or Sinkformer equation (4),\nTµ has a dependency in µ and the particles interact:\nthe local vector ﬁeld depends on the position of the\nother particles. More precisely we have in this case\nT1\nµ(x) =\n∫\nk1(x,x′)WVx′dµ(x′) for the Transformer\nand T∞\nµ (x) =\n∫\nk∞(x,x′)WVx′dµ(x′) for the Sink-\nformer. It is easily seen that when µ is discrete we\nrecover the operators in equation (1) and (4).\nWasserstein gradient ﬂows. A particular case of\nequation (5) is when Tµ is a gradient with respect to\nthe Wasserstein metric W2. Let F be a function on\nM(Rd). As is standard, we suppose that Fadmits a\nﬁrst variation at all µ: there exists a function δF\nδµ(µ)\nsuch that d\ndεF(µ+ ερ)|ε=0 =\n∫ δF\nδµ(µ)dρfor every per-\nturbation ρ (Santambrogio, 2017). The Wasserstein\ngradient of F at µ is then ∇WF(µ) := ∇(δF\nδµ(µ)).\nThe minimization of Fon the space of measures cor-\nresponds to the PDE (5) with Tµ = −∇WF(µ). This\nPDE can be interpreted as ruling the evolution of the\nmeasure µ of particles initially distributed according\nto some measure µ0, for which the positions x(t) fol-\nlow the ﬂow ˙ x = −∇WF(µ)(x), that minimizes the\nglobal energy F. It corresponds to a steepest descent\nin Wasserstein space (Jordan et al., 1998). In Propo-\nsition 2, we show in the symmetric kernel case that\nSinkformers correspond to a Wasserstein gradient ﬂow\nfor some functional F∞, while Transformers do not.\nParticular case. An example is when Tµ does not\ndepend on µand writes Tµ = −∇Ewhere E : Rd →R.\nUnder regularity assumptions, a solution of (5) then\nconverges to a local minimum of E. This ﬁts in the\nimplicit deep learning framework (Bai et al., 2019),\nwhere a neural network is seen as solving an optimiza-\ntion problem. A typical beneﬁt of implicit models is\nthat the iterates xi do not need to be stored during\nthe forward pass of the network because gradients can\nbe calculated using the implicit function theorem: it\nbypasses the memory storage issue of GPUs (Wang\net al., 2018; Peng et al., 2017; Zhu et al., 2017) during\nautomatic diﬀerentiation. Another application is to\nconsider neural architectures that include an argmin\nlayer, for which the output is also formulated as the\nsolution of a nested optimization problem (Agrawal\net al., 2019; Gould et al., 2016, 2019).\nFlows for attention. Our goal is to determine the\nPDEs (5) deﬁned by the proposed attention maps. We\nconsider the symmetric case, summarized by the fol-\nlowing assumption:\nAssumption 1. WK\n⊤WQ = WQ\n⊤WK = −WV\nAssumption 1 means we consider symmetric ker-\nnels (by imposing WK\n⊤WQ = WQ\n⊤WK), and that\nwhen diﬀerentiating x ↦→ exp(c(x,x′)), we obtain\n−exp(c)WV. We show that, under this assumption,\nthe PDEs deﬁned by k0 and k∞correspond to Wasser-\nstein gradient ﬂows, whereas it is not the case for k1.\nA particular case of imposing WK\n⊤WQ = WQ\n⊤WK is\nwhen WQ = WK. This equality setting is studied by\nKim et al. (2021), where the authors show that it leads\nto similar performance for Transformers. Since impos-\ning WK\n⊤WQ = WQ\n⊤WK is less restrictive, it seems to\nbe a natural assumption. Imposing W⊤\nQWK = −WV is\nmore restrictive, and we detail the expressions for the\nPDEs associated to k0,k1,k∞without this assumption\nin Appendix A. We have the following result.\nProposition 2 (PDEs associated to k0,k1,k∞). Sup-\npose Assumption 1. Let F0 and F∞ : M(Rd) →R\nbe such that F0(µ) := 1\n2\n∫\nk0d(µ⊗µ) and F∞(µ) :=\n−1\n2\n∫\nk∞log(k∞\nk0 )d(µ⊗µ). Then k0, k1 and k∞ re-\nspectively generate the PDEs ∂µ\n∂t + div(µTk\nµ) = 0 with\nT0\nµ := −∇WF0(µ), T1\nµ := −∇[log(\n∫\nk0(·,x′)dµ(x′))]\nand T∞\nµ := −∇WF∞(µ).\nA proof is given in Appendix A. Proposition 2 shows\nthat k0 and k∞ correspond to Wasserstein gradient\nﬂows. In addition, the PDE deﬁned by k1 does not\ncorrespond to such a ﬂow. More precisely, we have the\nfollowing result.\nProposition 3 (The SoftMax normalization does not\ncorrespond to a gradient ﬂow) . One has that T1\nµ =\n−∇[log(\n∫\nk0(·,x′)dµ(x′))] is not a Wasserstein gradi-\nent.\nA proof is given in Appendix A, based on the lack of\nsymmetry of T1\nµ. As a consequence of these results,\nwe believe this variational formulation of attention\nmechanisms for Sinkformers (Proposition 2) provides\na perspective for analyzing the theoretical properties\nof attention-based mechanisms in light of Wasserstein\ngradient ﬂow theory (Santambrogio, 2017). Moreover,\nit makes it possible to interpret Sinkformers as argmin\nlayers, which is promising in terms of theoretical and\nexperimental investigations, and which is not possible\nfor Transformers, according to Proposition 3.\nOur results are complementary to the one of Dong\net al. (2021), where the authors show that, with no\nskip connections and without the feed forward neu-\nral network acting after each attention block, the out-\nput of a Transformer converges doubly exponentially\nSinkformers: Transformers with Doubly Stochastic Attention\nwith depth to a rank-1 matrix. On the contrary,\nwe propose a complementary analysis by taking skip-\nconnections into account, as is standard in Transform-\ners. Precisely because we consider such connections,\nwe end up with very diﬀerent behaviors. Indeed, as\nshown in the next section, our analysis reveals that\nthe relative signs for WK, WQ and WV imply very dif-\nferent behavior, such as aggregation or diﬀusion. The\ndynamics obtained when considering skip connections\nare therefore richer than a rank collapse phenomenon.\n5 Attention and diﬀusion\nIn this section, we use the same notations as in sec-\ntion 4. We consider the mean-ﬁeld limit, where the\nmeasure µ has a density with respect to the Lebesgue\nmeasure. We are interested in how the density of\nparticles evolves for an inﬁnite depth self-attention\nnetwork with tied weights between layers. We con-\nsider Assumption 1 and suppose that W⊤\nKWQ is pos-\nitive semi-deﬁnite. For a bandwidth ε > 0, let\nk∞\nε = Sinkhorn(c/ε), that is the attention kernel\nfor the Sinkformer with the cost c/ε. The mapping\nT∞\nµ,ε : x ↦→ 1\nε\n∫\nk∞\nε (x,x′)WVx′dµ(x′) corresponds to\nthe continuous version of the Sinkformer where we re-\nscale WQWT\nK = −WV by ε. To better understand\nthe dynamics of attention, we study the asymptotic\nregime in which the bandwidth ε→0. In this regime,\none can show that ∀x∈Rd, εT∞\nµ,ε(x) →WVx (details\nin Appendix A). Thus, to go beyond ﬁrst order, we\nstudy the modiﬁed map T\n∞\nµ,ε = T∞\nµ,ε −1\nεWV. A natu-\nral question is the limit of this quantity when ε →0,\nand what the PDE deﬁned by this limit is. We have\nthe following theorem.\nTheorem 1 (Sinkformer’s PDE) . Let µ ∈ M(Rd).\nSuppose that µ is supported on a compact set and has\na density ρ∈C3(Rd). Suppose assumption 1 and that\nW⊤\nKWQ is positive semi-deﬁnite. Then one has in L2\nnorm as ε→0,\nT\n∞\nµ,ε →T\n∞\nµ,0 := −∇ρ\nρ .\nIn this limit, the PDE ∂tρ+ div(ρT\n∞\nµ,0) = 0 rewrites\n∂tρ= ∆ρ. (6)\nA proof is available in Appendix A, making use of The-\norem 1 from Marshall and Coifman (2019). We recover\nin Equation (6) the well-known heat equation.\nWe want to compare this result with the one obtained\nwith the SoftMax normalization. In order to carry a\nsimilar analysis, we make use of a Laplace expansion\nresult (Tierney et al., 1989; Singer, 2006). However,\nthe kernel k1\nε = SoftMax(c/ε) is not suited for using\nLaplace method because it does not always have a limit\nwhen ε → 0. Thus, we consider the modiﬁed cost\nas in Proposition 1, ˜c(x,x′) = −∥WQx−WKx′∥2\n2 . The\nkernel ˜k1\nε = SoftMax(˜c/ε), for which we can now ap-\nply Laplace expansion result, then corresponds to the\nL2 self-attention formulation (Kim et al., 2021). Note\nthat thanks to Proposition 1, ˜k∞\nε = k∞\nε : Sinkorn’s\nalgorithm will have the same output for both costs.\nTo simplify the expressions derived, we assume that\nWQ and WK are in Rd×d and are invertible. Similarly\nto the analysis conducted for Sinkformers, we con-\nsider the mapping T1\nµ,ε : x↦→1\nε\n∫˜k1\nε(x,x′)WVx′dµ(x′).\nWhen ε → 0, we show that ∀x ∈ Rd, εT1\nµ,ε(x) →\n−W⊤\nQWQx(details in Appendix A). Thus, we consider\nT\n1\nµ,ε = T1\nµ,ε+ 1\nεW⊤\nQWQ. We have the following result.\nProposition 4 (Transformer’s PDE) . Let µ ∈\nM(Rd). Suppose that µ is supported on a compact set\nand has a density ρ ∈C1(Rd). Suppose assumption 1\nand that WQ and WK are in Rd×d and are invertible.\nThen one has ∀x∈Rd,\nT\n1\nµ,ε(x) →T\n1\nµ,0(x) := −W⊤\nQW−1\nK\n∇ρ\nρ (W−1\nK WQx).\nIn this limit, the PDE ∂tρ+ div(ρT\n1\nµ,0) = 0 rewrites\n∂tρ= div(W⊤\nQW−1\nK\n∇ρ\nρ (W−1\nK WQ·)ρ) (7)\nA proof is given in Appendix A. While equation (6)\ncorresponds to the heat equation, equation (7) is dif-\nferent. First, it is nonlinear in ρ. Second, it is nonlocal\nsince the evolution of the density at xdepends on the\nvalue of this density at location W−1\nK WQx. Note that\nthe linear and local aspect of Sinkformer’s PDE on\nthe one hand, and the nonlinear and nonlocal aspect\nof Transformer’s PDE on the other hand, remain true\nwithout assuming WQ\n⊤WK = −WV (details in Ap-\npendix A).\n6 Experiments\nWe now demonstrate the applicability of Sinkform-\ners on a large variety of experiments with diﬀerent\nmodalities. We use Pytorch (Paszke et al., 2017) and\nNvidia Tesla V100 GPUs. Our code is open-sourced\nand is available at this address: https://github.\ncom/michaelsdr/sinkformers. All the experimental\ndetails are given in Appendix C.\nPractical implementation. In all our experi-\nments, we use existing Transformer architectures and\nmodify the SoftMax operator in attention modules\nwith Sinkhorn’s algorithm, which we implement in log\ndomain for stability (details in Appendix B).\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr´ e\n6.1 ModelNet 40 classiﬁcation\nThe ModelNet 40 dataset (Wu et al., 2015) is com-\nposed of 40 popular object categories in 3D. Trans-\nformers for point clouds and sets have been applied to\nthe ModelNet 40 classiﬁcation in several works, such\nas Set Transformers (Lee et al., 2019) or Point Cloud\nTransformers (Guo et al., 2021).\nSet Sinkformers. Set Transformers (Lee et al.,\n2019) also have an encoder decoder structure with\ndiﬀerent possibilities for deﬁning attention-based set\noperations. We propose to focus on the architecture\nthat uses Induced Self Attention Block (ISAB), which\nbypasses the quadratic time complexity of Self Atten-\ntion Blocks (SAB). More details about this architec-\nture can be found in (Lee et al., 2019). We reproduce\nthe ModelNet 40 classiﬁcation experiment using 5000\nuniformly sampled points for each shape and use a Set\nTransformer and a Set Sinkformer with two ISAB lay-\ners in the encoder and a decoder composed of a SAB\nand a Pooling by Multihead Attention (PMA) module.\nWhile the reported test accuracy is of 87 .8% using a\nSet Transformer, we obtain as our best accuracy when\nperforming 21 iterations of Sinkhorn algorithm within\nour Sinkformer of 89 .1%. Results are summarized in\nTable 1.\n0 200\nEpoch\n10 %\n15 %\n20 %\n30 %\n40 %\n60 %Test error\n0 200\nEpoch\n1Test loss\n1 it.\n5 it.\n21 it.\n0 200\nEpoch\n1 %\n5 %\n10 %Train error\n0 200\nEpoch\n1Train loss\nFigure 3: Classiﬁcation error and loss on Mod-\nelNet 40 when training a Set Transformer and a\nSet Sinkformer with diﬀerent number of iterations in\nSinkhorn’s algorithm.\nMoreover, we show in Figure 3 the learning curves cor-\nresponding to this experiment. Interestingly, the num-\nber of iterations within Sinkhorn’s algorithm increases\nthe accuracy of the model. Note that we only consider\nan odd number of iterations since we always want to\nhave row-wise stochastic attention matrices to be con-\nsistent with the properties of the SoftMax.\nPoint Cloud Transformers. We also train Point\nCloud Transformers (Guo et al., 2021) on ModelNet\n40. This architecture achieves accuracy comparable\nto the state of the art on this dataset. We compare\nbest and median test accuracy over 4 runs. Results\nare reported in Table 1, where we see that while the\nbest test-accuracy is narrowly achieved for the Trans-\nformer, the Sinkformer has a slightly better median\naccuracy.\nTable 1: Test accuracy for ModelNet 40 over 4\nruns for each model.\nModel Best Median Mean Worst\nSet Transformer 87.8% 86.3% 85.8% 84.7%\nSet Sinkformer 89.1% 88.4% 88.3% 88.1%\nPoint Cloud Transformer93.2% 92.5% 92.5% 92.3%\nPoint Cloud Sinkformer93.1% 92.8% 92.7% 92.5%\n6.2 Sentiment Analysis\nWe train a Transformer (composed of an attention-\nbased encoder followed by a max-pooling layer) and a\nSinkformer on the IMDb movie review dataset (Maas\net al., 2011) for sentiment analysis. This text clas-\nsiﬁcation task consists of predicting whether a movie\nreview is positive or negative. The learning curves are\nshown in Figure 4, with a gain in accuracy when us-\ning a Sinkformer. In this experiment, Sinkhorn’s algo-\nrithm converges perfectly in 3 iterations (the resulting\nattention matrices are doubly stochastic), which corre-\nsponds to the green curve. The Sinkformer only adds a\nsmall computational overhead, since the training time\nper epoch is 4m 02s for the Transformer against 4m\n22s for the Sinkformer.\n6.3 Neural Machine Translation\nWe train a Transformer and its Sinkformer counter-\npart using the fairseq (Ott et al., 2019) sequence\nmodeling toolkit on the IWSLT’14 German to English\ndataset (Cettolo et al., 2014). The architecture used is\ncomposed of an encoder and a decoder, both of depth\n6. We plug Sinkhorn’s algorithm only into the encoder\npart. Indeed, in the decoder, we can only pay attention\nto previous positions in the output sequence. For this\nreason, we need a mask that prevents a straightforward\napplication of Sinkhorn’s algorithm. We demonstrate\nthat even when using the hyper-parameters used to\noptimally train the Transformer, we achieve a similar\nBLEU (Papineni et al., 2002) over 6 runs. We ﬁrst\ntrain a Transformer for 30 epochs. On the evaluation\nset, we obtain a BLEU of 34 .43. We then consider\na Sinkformer with the weights of the trained Trans-\nformer. Interestingly, even this un-adapted Sinkformer\nprovides a median BLEU score of 33 .81. We then di-\nvide the learning rate by 10 and retrain for 5 additional\nSinkformers: Transformers with Doubly Stochastic Attention\n0 10\nEpoch\n0.4\n0.5\nTrain Loss\nTransformer\nSinkformer\n0 10\nEpoch\n0.46\n0.48\n0.50\n0.52\nTest loss\n0 10\nEpoch\n70 %\n80 %\n90 %\n98 %Train Acc\n0 10\nEpoch\n78 %\n80 %\n82 %\n84 %\n86 %Test Acc\nFigure 4: Learning curves when training a Transformer and a Sinkformer on the Sentiment Analysis task on\nthe IMDb Dataset.\nepochs both the Transformer and the Sinkformer to\nobtain a median BLEU of respectively 34.68 and 34.73\n(Table 2). Importantly, the runtime for one training\nepoch is almost the same for both models: 2m 48s\n(Transformer) against 2m 52s (Sinkformer).\nTable 2: Median BLEU score over 6 runs on the\nIWSLT’14 German to English dataset. The score ⋆\nis when evaluating the Sinkformer with the weights of\nthe trained Transformer.\nModel Epoch 30 Epoch 35\nTransformer 34.43 34.68\nSinkformer 33.81⋆ 34.73\n6.4 Vision Transformers\nVision Transformers (ViT) (Dosovitskiy et al., 2020)\nhave recently emerged as a promising architecture for\nachieving state of the art performance on computer\nvision tasks (Zhai et al., 2021), using only attention\nbased mechanisms by selecting patches of ﬁxed size in\nimages and feeding them into an attention mechanism.\n50 150 250\nEpoch\n70 %\n75 %\n80 %\n85 %Accuracy\nTransformer\nSinkformer\nFigure 5: Train\n(dotted) and test\n(plain) accuracy as\na function of the\nnumber of epochs\nwhen training a ViT\nand its Sinkformer\ncounterpart on the\ncats and dogs clas-\nsiﬁcation task (me-\ndian over 5 runs).\nCats and dogs classiﬁcation. We train a ViT\nand its Sinkformer counterpart on a binary cats and\ndogs image classiﬁcation task. The evolution of the\ntrain and test accuracy is displayed in Figure 5. The\nmedian test accuracy is 79 .0% for the Transformer\nagainst 79 .5% for the Sinkformer, whereas the max-\nimum test accuracy is 80 .0% for the Transformer\nagainst 80.5% for the Sinkformer. We also use 3 itera-\ntions in Sinkhorn’s algorithm which leads to a negligi-\nble computational overhead (training time per epoch\nof 3m 25s for the Sinkformer against 3m 20s for the\nTransformer).\nImpact of the patch size on the ﬁnal accuracy.\nWe consider a one-layer and one-head self-attention\nmodule on the MNIST dataset, with no additional\nlayer. The purpose is to isolate the self-attention mod-\nule and study how its accuracy is aﬀected by the choice\nof the patch size. Results are displayed in Figure 6. We\nrecall that a MNIST image is of size 28×28. When tak-\ning only one patch of size 28, both models are equiva-\nlent because the attention matrix is of size 1. However,\nwhen the patch size gets smaller, the two models are\ndiﬀerent and the Sinkformer outperforms the Trans-\nformer.\n1 2 4 7 14 28\nPatch Size\n92\n94\n96Test Accuracy\nTransformer\nSinkformer\nFigure 6: Final\ntest accuracy when\ntraining a one layer\nand one head self\nattention module on\nthe MNIST dataset,\nwith no feedforward\nneural network, when\nvarying the patch\nsize (median over 5\nruns).\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr´ e\nConclusion\nIn this paper, we presented the Sinkformer, a vari-\nant of the Transformer in which the SoftMax, which\nleads to row-wise stochastic attention, is replaced by\nSinkhorn’s algorithm, which leads to doubly stochastic\nattention. This new model is motivated by the empiri-\ncal ﬁnding that attention matrices in Transformers get\ncloser and closer to doubly stochastic matrices during\nthe training process. This modiﬁcation is easily imple-\nmented in practice by simply replacing the SoftMax in\nthe attention modules of existing Transformers with-\nout changing any parameter in the network. It also\nprovides a new framework for theoretically studying\nattention-based mechanisms, such as the interpreta-\ntion of Sinkformers as Wasserstein gradient ﬂows in\nthe inﬁnitesimal step size regime or as diﬀusion op-\nerators in the mean-ﬁeld limit. On the experimental\nside, Sinkformers lead to better accuracy in a variety\nof experiments: classiﬁcation of 3D shapes, sentiment\nanalysis, neural machine translation, and image clas-\nsiﬁcation.\nAcknowledgments\nThis work was granted access to the HPC resources of\nIDRIS under the allocation 2020-[AD011012073] made\nby GENCI. This work was supported in part by the\nFrench government under management of Agence Na-\ntionale de la Recherche as part of the “Investisse-\nments d’avenir” program, reference ANR19-P3IA-0001\n(PRAIRIE 3IA Institute). This work was supported in\npart by the European Research Council (ERC project\nNORIA). We thank Marco Cuturi and D. Sculley for\ntheir comments on a draft of the paper. We thank\nScott Pesme, Pierre Rizkallah, Othmane Sebbouh,\nThibault S´ ejourn´ e and the anonymous reviewers for\nhelpful feedbacks.\nReferences\nAgrawal, A., Amos, B., Barratt, S., Boyd, S., Di-\namond, S., and Kolter, Z. (2019). Diﬀeren-\ntiable convex optimization layers. arXiv preprint\narXiv:1910.12430.\nBahdanau, D., Cho, K., and Bengio, Y. (2014). Neural\nmachine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473 .\nBai, S., Kolter, J. Z., and Koltun, V. (2019). Deep\nequilibrium models. Advances in Neural Informa-\ntion Processing Systems, 32:690–701.\nBehrmann, J., Grathwohl, W., Chen, R. T., Duve-\nnaud, D., and Jacobsen, J.-H. (2019). Invertible\nresidual networks. In International Conference on\nMachine Learning, pages 573–582. PMLR.\nBlondel, M., Berthet, Q., Cuturi, M., Frostig, R.,\nHoyer, S., Llinares-L´ opez, F., Pedregosa, F., and\nVert, J.-P. (2021). Eﬃcient and modular implicit\ndiﬀerentiation. arXiv preprint arXiv:2105.15183 .\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M.,\nKaplan, J., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Lan-\nguage models are few-shot learners. arXiv preprint\narXiv:2005.14165.\nCettolo, M., Niehues, J., St¨ uker, S., Bentivogli, L., and\nFederico, M. (2014). Report on the 11th iwslt eval-\nuation campaign, iwslt 2014. In Proceedings of the\nInternational Workshop on Spoken Language Trans-\nlation, Hanoi, Vietnam , volume 57.\nCharlier, B., Feydy, J., Glaun` es, J., Collin, F.-D., and\nDurif, G. (2021). Kernel operations on the gpu, with\nautodiﬀ, without memory overﬂows. Journal of Ma-\nchine Learning Research, 22(74):1–6.\nChen, T. Q., Rubanova, Y., Bettencourt, J., and Du-\nvenaud, D. K. (2018). Neural ordinary diﬀerential\nequations. In Advances in neural information pro-\ncessing systems, pages 6571–6583.\nCuturi, M. (2013). Sinkhorn distances: Lightspeed\ncomputation of optimal transport. Advances in neu-\nral information processing systems, 26:2292–2300.\nCuturi, M., Teboul, O., Niles-Weed, J., and Vert, J.-\nP. (2020). Supervised quantile normalization for\nlow rank matrix factorization. In International\nConference on Machine Learning, pages 2269–2279.\nPMLR.\nDe Bie, G., Peyr´ e, G., and Cuturi, M. (2019). Stochas-\ntic deep networks. In International Conference on\nMachine Learning, pages 1556–1565. PMLR.\nDong, Y., Cordonnier, J.-B., and Loukas, A. (2021).\nAttention is not all you need: Pure attention\nloses rank doubly exponentially with depth. arXiv\npreprint arXiv:2103.03404.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weis-\nsenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S., et al.\n(2020). An image is worth 16x16 words: Transform-\ners for image recognition at scale. arXiv preprint\narXiv:2010.11929.\nGould, S., Fernando, B., Cherian, A., Anderson, P.,\nCruz, R. S., and Guo, E. (2016). On diﬀerentiating\nparameterized argmin and argmax problems with\napplication to bi-level optimization. arXiv preprint\narXiv:1607.05447.\nGould, S., Hartley, R., and Campbell, D. (2019). Deep\ndeclarative networks: A new hope. arXiv preprint\narXiv:1909.04866.\nSinkformers: Transformers with Doubly Stochastic Attention\nGriewank, A. and Walther, A. (2008). Evaluating\nderivatives: principles and techniques of algorithmic\ndiﬀerentiation. SIAM.\nGuo, M.-H., Cai, J.-X., Liu, Z.-N., Mu, T.-J., Martin,\nR. R., and Hu, S.-M. (2021). Pct: Point cloud trans-\nformer. Computational Visual Media, 7(2):187–199.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep\nresidual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pages 770–778.\nHein, M., Audibert, J.-Y., and Luxburg, U. v. (2007).\nGraph laplacians and their convergence on random\nneighborhood graphs. Journal of Machine Learning\nResearch, 8(6).\nJordan, R., Kinderlehrer, D., and Otto, F. (1998).\nThe variational formulation of the fokker–planck\nequation. SIAM journal on mathematical analysis ,\n29(1):1–17.\nKim, H., Papamakarios, G., and Mnih, A. (2021). The\nlipschitz constant of self-attention. In International\nConference on Machine Learning, pages 5562–5571.\nPMLR.\nKingma, D. P. and Ba, J. (2014). Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKrantz, S. G. and Parks, H. R. (2012). The implicit\nfunction theorem: history, theory, and applications .\nSpringer Science & Business Media.\nLee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S.,\nand Teh, Y. W. (2019). Set transformer: A frame-\nwork for attention-based permutation-invariant neu-\nral networks. In International Conference on Ma-\nchine Learning, pages 3744–3753. PMLR.\nLu, Y., Zhong, A., Li, Q., and Dong, B. (2018). Be-\nyond ﬁnite layer neural networks: Bridging deep\narchitectures and numerical diﬀerential equations.\nIn International Conference on Machine Learning ,\npages 3276–3285. PMLR.\nLuise, G., Rudi, A., Pontil, M., and Ciliberto, C.\n(2018). Diﬀerential properties of sinkhorn approxi-\nmation for learning with wasserstein distance. arXiv\npreprint arXiv:1805.11897.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,\nA. Y., and Potts, C. (2011). Learning word vectors\nfor sentiment analysis. In Proceedings of the 49th\nAnnual Meeting of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 142–150, Portland, Oregon, USA. Association\nfor Computational Linguistics.\nMarshall, N. F. and Coifman, R. R. (2019). Manifold\nlearning with bi-stochastic kernels. IMA Journal of\nApplied Mathematics, 84(3):455–482.\nMialon, G., Chen, D., d’Aspremont, A., and Mairal, J.\n(2021). A trainable optimal transport embedding for\nfeature aggregation and its relationship to attention.\nIn ICLR 2021-The Ninth International Conference\non Learning Representations.\nMilanfar, P. (2013). Symmetrizing smoothing ﬁlters.\nSIAM Journal on Imaging Sciences , 6(1):263–284.\nNiculae, V., Martins, A., Blondel, M., and Cardie, C.\n(2018). Sparsemap: Diﬀerentiable sparse structured\ninference. In International Conference on Machine\nLearning, pages 3799–3808. PMLR.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S.,\nNg, N., Grangier, D., and Auli, M. (2019). fairseq:\nA fast, extensible toolkit for sequence modeling. In\nProceedings of NAACL-HLT 2019: Demonstrations.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J.\n(2002). Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th an-\nnual meeting of the Association for Computational\nLinguistics, pages 311–318.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang,\nE., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L.,\nand Lerer, A. (2017). Automatic diﬀerentiation in\npytorch.\nPeng, C., Zhang, X., Yu, G., Luo, G., and Sun, J.\n(2017). Large kernel matters–improve semantic seg-\nmentation by global convolutional network. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 4353–4361.\nPeyr´ e, G., Cuturi, M., et al. (2019). Computational\noptimal transport: With applications to data sci-\nence. Foundations and Trends® in Machine Learn-\ning, 11(5-6):355–607.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei,\nD., Sutskever, I., et al. (2019). Language models\nare unsupervised multitask learners. OpenAI blog,\n1(8):9.\nRenardy, M. and Rogers, R. C. (2006).An introduction\nto partial diﬀerential equations, volume 13. Springer\nScience & Business Media.\nRuder, S. (2016). An overview of gradient de-\nscent optimization algorithms. arXiv preprint\narXiv:1609.04747.\nRuthotto, L. and Haber, E. (2019). Deep neural net-\nworks motivated by partial diﬀerential equations.\nJournal of Mathematical Imaging and Vision , pages\n1–13.\nSander, M. E., Ablin, P., Blondel, M., and Peyr´ e, G.\n(2021). Momentum residual neural networks. In\nProceedings of the 38th International Conference on\nMachine Learning, volume 139 ofProceedings of Ma-\nchine Learning Research, pages 9276–9287. PMLR.\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr´ e\nSantambrogio, F. (2017). {Euclidean, metric, and\nWasserstein}gradient ﬂows: an overview. Bulletin\nof Mathematical Sciences, 7(1):87–154.\nSinger, A. (2006). From graph to manifold laplacian:\nThe convergence rate. Applied and Computational\nHarmonic Analysis, 21(1):128–134.\nSinkhorn, R. (1964). A relationship between arbitrary\npositive matrices and doubly stochastic matrices.\nThe annals of mathematical statistics , 35(2):876–\n879.\nSong, M., Montanari, A., and Nguyen, P. (2018). A\nmean ﬁeld view of the landscape of two-layers neural\nnetworks. Proceedings of the National Academy of\nSciences, 115:E7665–E7671.\nSun, Q., Tao, Y., and Du, Q. (2018). Stochastic\ntraining of residual networks: a diﬀerential equation\nviewpoint. arXiv preprint arXiv:1812.00174 .\nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan,\nD.-C. (2020). Sparse sinkhorn attention. In In-\nternational Conference on Machine Learning, pages\n9438–9447. PMLR.\nTeh, Y., Doucet, A., and Dupont, E. (2019). Aug-\nmented neural odes. Advances in Neural Informa-\ntion Processing Systems 32 (NIPS 2019) , 32(2019).\nTierney, L., Kass, R. E., and Kadane, J. B. (1989).\nFully exponential laplace approximations to ex-\npectations and variances of nonpositive functions.\nJournal of the american statistical association ,\n84(407):710–716.\nTing, D., Huang, L., and Jordan, M. (2011). An anal-\nysis of the convergence of graph laplacians. arXiv\npreprint arXiv:1101.5435.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser,  L., and Polosukhin,\nI. (2017). Attention is all you need. In Advances in\nneural information processing systems , pages 5998–\n6008.\nVuckovic, J., Baratin, A., and Combes, R. T. d. (2021).\nOn the regularity of attention. arXiv preprint\narXiv:2102.05628.\nWang, L., Ye, J., Zhao, Y., Wu, W., Li, A., Song, S. L.,\nXu, Z., and Kraska, T. (2018). Superneurons: Dy-\nnamic gpu memory management for training deep\nneural networks. In Proceedings of the 23rd ACM\nSIGPLAN Symposium on Principles and Practice\nof Parallel Programming, pages 41–53.\nWeinan, E. (2017). A proposal on machine learning\nvia dynamical systems. Communications in Mathe-\nmatics and Statistics , 5(1):1–11.\nWeinan, E., Han, J., and Li, Q. (2019). A mean-ﬁeld\noptimal control formulation of deep learning. Re-\nsearch in the Mathematical Sciences , 6(1):10.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., De-\nlangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,\nFuntowicz, M., et al. (2019). Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. arXiv preprint arXiv:1910.03771 .\nWormell, C. L. and Reich, S. (2021). Spectral con-\nvergence of diﬀusion maps: Improved error bounds\nand an alternative normalization. SIAM Journal on\nNumerical Analysis, 59(3):1687–1734.\nWu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang,\nX., and Xiao, J. (2015). 3d shapenets: A deep rep-\nresentation for volumetric shapes. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 1912–1920.\nYun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J.,\nand Kumar, S. (2019). Are transformers universal\napproximators of sequence-to-sequence functions?\narXiv preprint arXiv:1912.10077 .\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L.\n(2021). Scaling vision transformers. arXiv preprint\narXiv:2106.04560.\nZhao, H., Jiang, L., Jia, J., Torr, P., and Koltun,\nV. (2020). Point transformer. arXiv preprint\narXiv:2012.09164.\nZhu, J.-Y., Park, T., Isola, P., and Efros, A. A. (2017).\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks. In Proceedings of\nthe IEEE international conference on computer vi-\nsion, pages 2223–2232.\nZweig, A. and Bruna, J. (2021). A functional per-\nspective on learning symmetric functions with neu-\nral networks. In International Conference on Ma-\nchine Learning, pages 13023–13032. PMLR.\nSinkformers: Transformers with Doubly Stochastic Attention\nAppendix\nIn Section A we give the proofs of all the Propositions and the Theorem. In Section B we present the implemen-\ntation details of Sinkformers. Section C gives details for the experiments in the paper.\nA Proofs\nA.1 Invariance to the cost function - Proof of Proposition 1\nProof. We use the variational formulation for Sinkhorn (Peyr´ e et al., 2019):\nSinkhorn(C) = argminK1n=K⊤1n=1n KL(K|K0)\nwith\nKL(K|K0) =\n∑\ni,j\nKi,jlog(Ki,j\nK0\ni,j\n),\nwhere K0\ni,j = exp(Ci,j).\nWe let ˜Ci,j = Cij + fi + gj. We have for K ∈ U := {K|K1n = K⊤1n = 1n} that KL( K|e˜C) =∑\ni,jKi,jlog( Ki,j\neCi,j+fi+gj ). This gives\nKL(K|e\n˜C) =\n∑\ni,j\nKi,j[log( Ki,j\neCi,j\n) −fi −gj] =\n∑\ni,j\nKi,j[log( Ki,j\neCi,j\n)] −\n∑\ni\nfi −\n∑\nj\ngj\nso that\nKL(K|e\n˜C) = KL(K|eC) −\n∑\ni\nfi −\n∑\nj\ngj.\nThis shows that KL( K|e˜C) and KL( K|eC) have the same argmin on U which implies that Sinkhorn(C) =\nSinkhorn( ˜C).\nA.2 PDEs associated with k 0,k1,k∞ - Proof of Proposition 2\nProof. Recall that for p∈{0,1,∞}, we have Tp\nµ(x) =\n∫\nkp(x,x′)WVx′dµ(x′).\nFor h∈C(Rd ×Rd) consider\nH(µ) =\n∫\nh(x,y)dµ(x)dµ(y).\nThen we have (Santambrogio, 2017)\nδH\nδµ(µ) =\n∫\n(h(x,.) + h(.,x))dµ(x).\nWe can now derive the diﬀerent gradient expressions for T0\nµ, T1\nµ and T∞\nµ .\nFor T0\nµ: under Assumption 1, we have that f(x,x′) = eC(x,x′) is symmetric. This gives\nδF\nδµ(µ) =\n∫\nf(.,x′)dµ(x′)\nand by diﬀerentiation under the integral, under suﬃcient regularity assumptions on µ, this gives\n∇W(F0)(x) =\n∫\n∇xf(x,x′)dµ(x′) =\n∫\nf(x,x′)∇xc(x,x′)dµ(x′).\nSince ∇xc(x,x′) = −WVx′, we get\n∇W(F0)(x) = −\n∫\nec(x,x′)WVx′dµ(x′).\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr´ e\nFor µ= 1\nn\n∑n\ni=1 δxi this is exactly\n∇W(F0)(x) = −\nn∑\nj=1\nK0\ni,jWVxj.\nFor T1\nµ: we have\n∇[log(\n∫\nec(.,x′)dµ(x′))](x) =\n∫ ∇xc(x,x′)ec(x,x′)\n∫\nec(x,y)dµ(y) dµ(x′) = −\n∫ ec(x,x′)WVx′\n∫\nec(x,y)dµ(y)dµ(x′).\nFor T2\nµ: one has the dual formulation for F∞(Peyr´ e et al., 2019):\n2F∞(µ) = −max\nf\n∫\nRd\n(f + fc)dµ (8)\nwhere we denote the soft c transform as\nfc(x′) := −log\n(∫\nef(x)+c(x,x′)dµ(x)\n)\n, (9)\nwhich actually depends on µ and c. One has for an optimal pair f = fc (Peyr´ e et al., 2019). In addition, one\nhas k∞(x,x′) = ec(x,x′)+f(x)+f(x′). The Wasserstein gradient of F∞is then\n∇WF∞(µ) = −∇f\nwhere f is an optimal solution of (8) (which is unique up to a constant). The gradient of f can be obtained\nusing (9) and the fact that f = fc:\n∇f(x) = −\n∫\nef(x)+f(x′)+c(x,x′)∇xc(x,x′)dµ(x′) = −\n∫\nk∞(x,x′)∇xc(x,x′)dµ(x′).\nThis ﬁnally gives\n∇WF∞(µ) : x↦→−\n∫\nk∞(x,x′)Wvx′dµ(x′),\nthat is what we wanted to show.\nA.3 The SoftMax normalization does not correspond to a gradient ﬂow - Proof of Proposition 3\nProof. Suppose by contradiction that T1\nµ = −∇[log(\n∫\nk0(·,x′)dµ(x′))] is a Wasserstein gradient. This implies\nthat there exists a function F such that, ∀µ∈M(Rd) and ∀x∈Rd,\nδF\nδµ(µ)(x) = log(\n∫\nk0(·,x′)dµ(x′)).\nWe therefore have\nδ2F\nδµ2 (µ)(x,x′) = k0(x,x′)∫\nk0(x,y)dµ(y),\n∀x,x′ ∈Rd. However, δ2F\nδµ2 (µ) is symmetric for all µ ∈M(Rd). The relationship δ2F\nδµ2 (µ)(x,x′) = δ2F\nδµ2 (µ)(x′,x)\nthen implies that for all µ, x and x′such that k0(x,x′) ̸= 0 we have\n∫\nk0(x,y)dµ(y) =\n∫\nk0(x′,y)dµ(y).\nTaking µ= δy gives k0(x,y) = k0(x′,y), which by symmetry implies that k0 is a constant.\nThis is a contradiction since k0(x,x′) = exp(x⊤W⊤\nQWKx′).\nSinkformers: Transformers with Doubly Stochastic Attention\nA.4 Sinkformer’s PDE - Proof of Theorem 1\nProof. Since W⊤\nQWK is positive-deﬁnite we write it W⊤\nQWK = A2 where Ais positive-deﬁnite. Note that thanks\nto Proposition 1, if κε(x,x′) = exp(−∥x−x′∥2\n2ε ), one has under Assumption 1 that κ∞\nε (Ax,Ax′) = k∞\nε (x,x′). For\nx∈Rd, we have\nT\n∞\nµ,ε(A−1x) = 1\nε(\n∫\nκ∞\nε (x,Ax′)WVx′ρ(x′)dx′−WVA−1x).\nWe perform the change of variable y= Ax′. This gives\nT\n∞\nµ,ε(A−1x) = 1\nε(\n∫\nκ∞\nε (x,y)WVA−1yρ(A−1y)CAdy−WVA−1x),\nwhere CA depends only on A. We then apply Theorem 1 from Marshall and Coifman (2019) with f = WVA−1,\nq(x) = ρ(A−1x) and w= 1\nCA\n, to obtain that\nT\n∞\nµ,ε(A−1·) →2∇f∇(q1/2)\nq1/2 = WVA−1 ∇q\nq\nin L2 norm. Since q(x) = ρ(A−1x) we have obtained that ∇q\nq = A−1 ∇ρ\nρ (A−1·) so that\nT\n∞\nµ,ε(A−1x) →WVA−2 ∇ρ\nρ (A−1x) = WV(W⊤\nQWK)−1 ∇ρ\nρ (A−1x).\nIn other words,\nT\n∞\nµ,ε →WV(W⊤\nQWK)−1 ∇ρ\nρ ,\nwhich is exactly what we wanted to show. Note that when WV = −W⊤\nQWK this gives the expected result. The\ngeneral form for the PDE is then\n∂tρ= div(−WV(W⊤\nQWK)−1 ∇ρ\nρ ×ρ)\nwhich gives\n∂tρ= ∆ρ\nif WV = −W⊤\nQWK.\nA.5 Transformer’s PDE - Proof of Proposition 4\nProof. Let x∈Rd and consider\ngε(x) = εT1\nµ,ε(W−1\nQ x) =\n∫\ne−∥x−WKx′∥2\n2ε WVx′ρ(x′)dx′\n∫\ne−∥x−WKx′∥2\n2ε ρ(x′)dx′\n.\nWe perform the change of variable y= WKx′. This gives:\ngε(x) =\n∫\ne−∥x−y∥2\n2ε WVW−1\nK yρ(W−1\nK y)dy\n∫\ne−∥x−y′∥2\n2ε ρ(W−1\nK y)dy\n.\nUsing the Laplace expansion result from Singer (2006), we obtain that\ngε(x) = WVW−1\nK\nxρ(W−1\nK x) + ε\n2 ∆(xρ(W−1\nK x)) + o(ε)\nρ(W−1\nK x) + ε\n2 ∆(ρ(W−1\nK x)) + o(ε).\nBy doing a Taylor expansion for the denominator, we ﬁnd\ngε(x) = WVW−1\nK (x+ ε\n2\n∆(xρ(W−1\nK x)\nρ(W−1\nK x) + o(ε))(1 −ε\n2\n∆(ρ(W−1\nK x))\nρ(W−1\nK x) + o(ε))\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr´ e\nand\ngε(x) = WVW−1\nK (x+ ε∇(ρ(W−1\nK x))\nρ(W−1\nK x) + o(ε).)\nSince T\n1\nµ,ε = T1\nµ,ε + 1\nεW⊤\nQWQ = 1\nε(gε(WQx) + W⊤\nQWQx) and because WVW−1\nK = −WQ\n⊤WKW−1\nK = −W⊤\nQ we\nhave\nT\n1\nµ,ε = −W⊤\nQW−1\nK\n∇ρ(W−1\nK WQx))\nρ(W−1\nK WQx) + o(1)\nwhich is exactly the expected result.\nB Implementation details\nWe implement Sinkhorn’s algorithm in log domain for stability. Given a matrixK0 ∈Rn×n such that K0\ni,j = eCi,j\nfor some C ∈Rn×n, Sinkhorn’s algorithm (3) approaches (f,g) ∈Rn×Rn such that K∞= diag(ef∞\n)K0diag(eg∞\n)\nby iterating in log domain, starting from g0 = 0n,\nfl+1 = log(1n/n) −log(Kegl\n) if l is even\ngl+1 = log(1n/n) −log(K⊤efl\n) if l is odd.\nThis allows for fast and accurate computations, where log( Kegl\n) and log( K⊤efl\n) are computed using\nlog-sum-exp.\nC Experimental details\nC.1 ModelNet 40 classiﬁcation\nSet Transformers. For our experiments on ModelNet using Set Transformers, we ﬁrst prepossess the Mod-\nelNet 40 dataset. We then uniformly sample 5000 points from each element in the dataset. Our architecture\nis composed of two ISAB layers in the encoder and a decoder composed of a SAB and a Pooling by Multihead\nAttention (PMA) module. For the training, we use a batch-size of 64 and we use Adam (Kingma and Ba, 2014).\nThe training is done over 300 epochs. The initial learning rate is 10 −3 and is decayed by a factor 10 after 200\nepochs.\nPoint Cloud Transformers. For our experiments on ModelNet using Point Clouds Transformers, we uni-\nformly sample 1024 points from each element in the dataset. For the training, we use a batch-size of 32 and we\nuse SGD (Ruder, 2016). The training is done over 300 epochs. The initial learning rate is 10 −4 and is decayed\nby a factor 10 after 250 epochs.\nC.2 Sentiment Analysis\nWe use the code available at the repository nlp-turorial 1, where a pretrained Transformer is ﬁne-tuned on the\nIMDb dataset. In our experiment, we reset the parameters of the pretrained Transformer and train it from\nscratch on the IMDb dataset. We use an architecture of depth 6, with 8 heads. For the training, we use a\nbatch-size of 32 and we use Adam. The training is done over 15 epochs. The initial learning rate is 10 −4 and is\ndecayed by a factor 10 after 12 epochs.\nC.3 Neural Machine Translation\nWe use the Transformer from fairseq and the command for training it on the IWSLT’14 2 dataset. When\nﬁne-tuning a Sinkformer, we simply divide the original learning rate by 10.\n1https://github.com/lyeoni/nlp-tutorial/tree/master/text-classiﬁcation-transformer\n2https://github.com/pytorch/fairseq/blob/main/examples/translation/README.md\nSinkformers: Transformers with Doubly Stochastic Attention\nC.4 Vision Transformers\nCats and dogs classiﬁcation. This experiment is done on the cats and dogs 3 dataset. For this experiment,\nwe use a batch-size of 64 and Adam. We use an architecture of depth 6, with 8 heads, and select a patch-size\nof 16. The training is done over 300 epochs. The initial learning rate is 5 ×10−5 and divided by 10 after 250\nepochs.\nImpact of the patch size on the ﬁnal accuracy. For this experiment, we use a batch-size of 100 and\nAdam. We use an architecture of depth 1, with 1 heads, without non-linearity, and select diﬀerent values for the\npatch-size. The training is done over 45 epochs. The initial learning rate is 1 ×10−3 (resp. 2 ×10−3) for the\nTransformer (resp. Sinkformer) and divided by 10 after 35 epochs and again by 10 after 41 epochs.\n3https://www.kaggle.com/c/dogs-vs-cats/data"
}