{
  "title": "RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models",
  "url": "https://openalex.org/W4385570802",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2807774868",
      "name": "Dave Van Veen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5072177559",
      "name": "Cara Van Uden",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3201947956",
      "name": "Maayane Attias",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2893077245",
      "name": "Anuj Pareek",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4291043079",
      "name": "Christian Bluethgen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2797219821",
      "name": "Malgorzata Polacin",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2100767227",
      "name": "Wah Chiu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4320554146",
      "name": "Jean-benoit Delbrouck",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5102541728",
      "name": "Juan Zambrano Chaves",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4316441464",
      "name": "Curtis Langlotz",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2328836187",
      "name": "Akshay Chaudhari",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2247131048",
      "name": "John Pauly",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4288627341",
    "https://openalex.org/W6903577382",
    "https://openalex.org/W4385574368",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170218295",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3166358520",
    "https://openalex.org/W3166845084",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2108017642",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W3188252230",
    "https://openalex.org/W4206529673",
    "https://openalex.org/W4312349643",
    "https://openalex.org/W3034863243",
    "https://openalex.org/W4385572235",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W1484393529",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W3103616906",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4285236136",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2396881363"
  ],
  "abstract": "Dave Van Veen, Cara Van Uden, Maayane Attias, Anuj Pareek, Christian Bluethgen, Malgorzata Polacin, Wah Chiu, Jean-Benoit Delbrouck, Juan Zambrano Chaves, Curtis Langlotz, Akshay Chaudhari, John Pauly. The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023.",
  "full_text": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 449–460\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nRadAdapt: Radiology Report Summarization via Lightweight\nDomain Adaptation of Large Language Models\nDave Van Veen*, Cara Van Uden*, Maayane Attias, Anuj Pareek,\nChristian Bluethgen, Malgorzata Polacin, Wah Chiu,\nJean-Benoit Delbrouck, Juan Manuel Zambrano Chaves,\nCurtis P. Langlotz, Akshay S. Chaudhari, John Pauly\nStanford University\n{vanveen, cvanuden}@stanford.edu\nAbstract\nWe systematically investigate lightweight\nstrategies to adapt large language models\n(LLMs) for the task of radiology report sum-\nmarization (RRS). Specifically, we focus on\ndomain adaptation via pretraining (on natural\nlanguage, biomedical text, or clinical text) and\nvia discrete prompting or parameter-efficient\nfine-tuning. Our results consistently achieve\nbest performance by maximally adapting to the\ntask via pretraining on clinical text and fine-\ntuning on RRS examples. Importantly, this\nmethod fine-tunes a mere 0.32% of parameters\nthroughout the model, in contrast to end-to-end\nfine-tuning (100% of parameters). Addition-\nally, we study the effect of in-context examples\nand out-of-distribution (OOD) training before\nconcluding with a radiologist reader study and\nqualitative analysis. Our findings highlight the\nimportance of domain adaptation in RRS and\nprovide valuable insights toward developing ef-\nfective natural language processing solutions\nfor clinical tasks.\n1 Introduction\nRadiology reports are comprehensive documents\nthat capture and interpret the results of a radio-\nlogical imaging examination. Reports are often\nstructured into three main sections: (1) a back-\nground section that provides general information\nabout the exam and the patient, e.g. medical his-\ntory (2) a findings section that presents detailed\nexam analysis and results, and (3) an impression\nsection that concisely summarizes the most salient\nfindings. In a typical workflow, a radiologist first\ndictates the detailed findings and then distills them\ninto a concise impression. This impression is the\nmost significant part of a radiology report, as it\ncontains crucial information for clinical decision-\nmaking (Kahn Jr et al., 2009). However, perform-\n*Equal contribution\ning radiology report summarization (RRS) manu-\nally can be labor-intensive and prone to errors (Ger-\nshanik et al., 2011), motivating the importance of\nautomating this task.\nLarge language models (LLMs) have demon-\nstrated remarkable capabilities in natural language\nunderstanding and generation, serving as founda-\ntion models that can be adapted to various domains\nand tasks. However, their sheer size, sometimes\nexceeding 100B parameters, makes training for\ndomain-specific tasks prohibitively expensive in\nterms of computation and training data. We ad-\ndress this challenge by exploring lightweight strate-\ngies for domain adaptation in the context of RRS,\nculminating in the following contributions:\n• We systematically evaluate a variety of LLMs\nand lightweight adaptation methods, achiev-\ning the best performance by pretraining\non clinical text and performing parameter-\nefficient fine-tuning with LoRA (Hu et al.,\n2021). Generated impressions showcase the\neffectiveness of lightweight adaptation strate-\ngies for RRS.\n• We investigate the impact of few-shot prompt-\ning by conducting ablation studies on the num-\nber of in-context examples provided to each\nmodel. Our findings reveal that increased con-\ntext leads to improved performance across al-\nmost all cases, shedding light on the value of\nprompt engineering when adapting LLMs for\nRRS.\n• We evaluate “out-of-distribution” (OOD)\nmodel performance and specifically exam-\nine the model’s ability to generalize to dif-\nferent imaging modalities and anatomies. Our\nresults indicate that anatomy plays a more\ncrucial role than modality, and best perfor-\nmance is achieved when training on a larger\ndataset which encompasses all modalities and\nanatomies.\n449\n• We conduct a reader study with radiologists\nwho provide qualitative insights and quanti-\ntative scores on the model’s correctness, co-\nherence, and ability to capture critical infor-\nmation. While our results are promising, we\nemphasize the need for further improvements\nand evaluation before clinical deployment.\nOverall, our research presents a comprehensive\ninvestigation of lightweight strategies for domain\nadaptation in the context of RRS, offering insights\ninto the effectiveness of different approaches and\nhighlighting the potential of LLMs for this task.\nOur findings contribute toward the advancement of\napplied natural language processing (NLP) to radi-\nology with implications for improving radiologists’\nworkflow and patient care.\n2 Related Work\nIn recent years, transformer-based (Vaswani et al.,\n2017) language models have become ubiquitous\nin NLP due to their state-of-the-art performance\nacross many tasks including language genera-\ntion, question answering, and machine translation.\nTransformer models BERT (Devlin et al., 2018)\nand GPT-2 (Radford et al., 2019) established a new\nparadigm of first training on large amounts of gen-\neral data and then fine-tuning on domain-specific\ndata, as opposed to direct training on domain-\nspecific data. This has led to training transformers\nwith more parameters on increasingly more data,\nresulting in LLMs such as GPT-3 (Brown et al.,\n2020), PaLM (Chowdhery et al., 2022), and the\n“text-to-text transfer transformer,” or T5 (Raffel\net al., 2020).\nHowever, end-to-end fine-tuning LLMs like\nGPT-3 (175B parameters) requires substantial com-\nputational resources, creating a high barrier to en-\ntry. As a result, there has been a growing interest in\nlightweight methods for domain adaptation. One\nsuch method is prompting, in which one provides\ninitial text input to the LLM so it has context for\nthe given task. Performance depends heavily on\nthe provided prompt (Brown et al., 2020), motivat-\ning principled prompting methods. Various works\nhave pursued this in the form of natural language\ninstructions (Liu et al., 2023; Wei et al., 2022) or\nsupplying “in-context” examples of desired output\n(Lampinen et al., 2022). An alternative approach\nconsists of parameter-efficient fine-tuning, where\none freezes existing model weights and inserts a\nsmall number of tunable parameters (Rebuffi et al.,\n2017; Houlsby et al., 2019; Lin et al., 2020). We fo-\ncus on the two highest regarded parameter-efficient\nfine-tuning methods: prefix tuning (Li and Liang,\n2021; Lester et al., 2021) and LoRA (Hu et al.,\n2021), discussed further in Section 3.\nIn addition to methods for prompting and fine-\ntuning, previous work has demonstrated adaptation\nto the medical domain via pretraining on biomedi-\ncal or clinical text. Consider SciFive (Phan et al.,\n2021), Clinical-T5 (Lehman and Johnson, 2023),\nand Med-PaLM (Singhal et al., 2022), which lever-\naged LLMs for text generation on medical tasks.\nBoth SciFive (biomedical) and Clinical-T5 (clini-\ncal) achieved state-of-the art results for their respec-\ntive domains in tasks such as named entity recog-\nnition, natural language inference, and question-\nanswering. Additionally, Med-PaLM achieved suc-\ncess aligning text generation models to clinical\ntasks via methods such as chain-of-thought prompt-\ning, prompt tuning (Lester et al., 2021), and instruc-\ntion tuning (Chung et al., 2022a).\nWithin the clinical domain, we focus specifically\non the task of RRS. Previous work has approached\nthis task with a focus on consistency and factual\ncorrectness (Zhang et al., 2020; Dai et al., 2021;\nMiura et al., 2021; Delbrouck et al., 2022a). To the\nbest of our knowledge, this work is the first to lever-\nage lightweight domain adaptation strategies on\nLLMs for the RRS task. Additionally, most prior\nwork on RRS uses only chest x-rays (Dai et al.,\n2021; Abacha et al., 2021) from large datasets like\nMIMIC-CXR (Johnson et al., 2019a) and CheX-\npert (Irvin et al., 2019). In contrast, our work em-\nploys the MIMIC-III dataset (Johnson et al., 2016),\nwhich contains longer radiology reports from a di-\nverse set of two imaging modalities (MR, CT) and\nseven anatomies (head, chest, etc.), presenting a\nmore difficult summarization task.\n3 Methods\nAs depicted in Figure 1, we investigate adapting\nLLMs to the task of RRS along two axes: (1) mod-\nels pretrained on various combinations of natural\nlanguage, biomedical, and clinical text data, as de-\nscribed in Section 3.1, and (2) various methods\nfor discrete prompting and parameter-efficient fine-\ntuning, as described in Section 3.2.\n450\n C4 C4 C4\nPubMed\nC4\nPubMed\nMIMIC-III, IV\nC4\nMIMIC-III, IV\nModels \nInstruction prompt tuning \nDatasets used to pretrain \n \nMethods \nused for prompting, tuning \nNull\nSummarize the following\nradiology report: \nFindings: ... \nImpression: \nPrefix\nFindings: ... \nImpression: \nFindings: [example] \nImpression: [example] \n... \nFindings: ... \nImpression: \nIn-context\n[tune parameters prefixed to\nmodel] \nFindings: ... \nImpression: \nPrefix tuning LoRA\n{  , , , , }  \n{  , , , , }  \nT5 FLAN-T5 SCI FIVE CLIN -T5-SCI CLIN -T5\ntemplate \n×  \nzero-shot \ndiscrete prompting \nparameter-efficient fine-tuning few-shot \nincreasing domain adaptation via model pretraining (top) and methods for prompting or tuning (bottom) \n[tune parameters injected\nwithin model] \nFindings: ... \nImpression: \nFigure 1: Diagram of experiments. We evaluate every combination of pretrained LLM (top) and lightweight\nadaptation method (bottom). Moving from left to right, the models and methods become increasingly adapted to the\ndownstream clinical task of RRS.\n3.1 Pretrained Models\nTo mitigate variance introduced by different model\narchitectures, we focus this study on T5, i.e. “text-\nto-text transfer transformer,” a highly regarded\nencoder-decoder architecture available for public\nuse (Raffel et al., 2020). T5’s text-to-text frame-\nwork enables the model to be used on any NLP\ntask, and its pretraining over the C4 (Colossal\nClean Crawled Corpus) dataset (Raffel et al., 2020)\nenables excellent performance in transfer learn-\ning. We include results for two architecture sizes:\nbase (223M parameters), and large (738M param-\neters). From hereon we refer to T5 as the origi-\nnal model pretrained on C4 alone. The remaining\nfour models are simply a version of T5 that was\nsubsequently end-to-end fine-tuned on datasets of\nvarious relevance to the RRS task:\n• FLAN-T5 (Chung et al., 2022b) tuned via\ninstruction prompt tuning.\n• SCIFIVE (Phan et al., 2021) tuned on\nthe biomedical text dataset of PubMed,\ni.e. Pubmed Abstract (NCBI, 1996) and\nPubMed Central (NCBI, 2000).\n• CLIN -T5-S CI or Clinical-T5-Sci (Lehman\nand Johnson, 2023) tuned on PubMed and\ntwo clinical text datasets (MIMIC-III (John-\nson et al., 2016) and MIMIC-IV (Johnson\net al., 2020a)).\n• CLIN -T5 or Clinical-T5 (Lehman and John-\nson, 2023) tuned on MIMIC-III and IV alone.\nPlease see the top row in Figure 1 for a visual illus-\ntration of these five models. We acknowledge the\ndifficulty of ranking pretraining datasets’ relevance\nfor a particular downstream task. In the case of\nRRS, it may seem reasonable to assert that clini-\ncal text is more relevant than biomedical text, and\nthat biomedical text is more relevant than general\nnatural language text. However, it becomes more\ndifficult to compareFLAN-T5 ’s instruction tuning,\nwhich drastically improves performance on prompt-\ning benchmarks but has not yet been explored for\nmedical tasks (Longpre et al., 2023). We also note\nthe complication introduced by CLIN -T5-S CI and\nCLIN -T5 being trained on a subset of MIMIC-III,\nthe same dataset used for evaluation; please see\nSection 5.4 for further discussion.\n3.2 Lightweight task adaptation methods\nFor each pretrained model discussed in Section 3.1,\nwe evaluate five lightweight domain adaptation\nmethods for prompting and tuning (Fig. 1, bottom\nrow). Prompting provides adaptation by simply\nsupplying particular tokens to the frozen pretrained\nmodel. In contrast, parameter-efficient fine-tuning\nprovides adaptation by adding a small number of\nparameters to the model and optimizing them for\nthe task, while the original model parameters re-\nmain frozen. Compared to updating all model\nparameters via end-to-end fine-tuning, parameter-\nefficient methods require much less computation\nand training data. We describe these five meth-\nods below in order of increasing adaptation to the\ndownstream RRS task:\n451\nTable 1: We employ parameter-efficient fine-tuning methods for domain adaptation that modify <0.4% of model\nparameters while keeping other parameters frozen.\nTunable parameters Training time (hr)\nModel size Method # % of total per epoch total # epochs\nBase (223M) prefix tuning 0.37M 0.17% 0.98 9.83 10\nLoRA 0.88M 0.39% 1.32 6.60 5\nLarge (738M) prefix tuning 0.98M 0.13% 2.93 29.3 10\nLoRA 2.4M 0.32% 3.85 19.3 5\n1. Null prompting (Zhao and Schütze, 2021) is\na simple discrete (sequence of real natural-\nlanguage tokens), zero-shot prompt. We sup-\nply the radiology report findings section and\nthe basic prompt, “impression:”.\n2. Prefixed prompting (Zhao and Schütze, 2021)\nis a discrete, zero-shot prompt with a brief\ninstruction prepended to the original null\nprompt above. For our instruction we use\n“summarize the following radiology report:”.\nThis provides the model some context for the\nRRS task. We note that a slight modification\nto the prepended instruction may significantly\nchange the generated output for an individ-\nual sample. However, that same modification\ndoes not meaningfully alter quantitative met-\nrics when applied over the entire dataset.\n3. In-context learning (Lampinen et al., 2022)\nis a type of discrete, few-shot prompt. We\nbegin with the null prompt and prepend one,\ntwo, or four task examples using the same\ntemplate. Particular examples are chosen by\ncomputing the k-nearest neighbors (Johnson\net al., 2019b) of the findings section for each\ntraining example in the embedding space of\na PubMedBERT model (Deka et al., 2022).\nThis provides the most relevant examples for\nthe RRS task.\n4. Prefix tuning (Li and Liang, 2021) is a\nparameter-efficient fine-tuning method which\nprepends and optimizes an additional task-\nspecific vector called the prefix as input to\nthe model. For our base and large architec-\ntures, this requires tuning a mere 0.17% and\n0.13% of total parameters, respectively (see\nTable 1). This approach provides the model a\ntask-specific prompt that is very well aligned\nto the downstream task.\n5. LoRA (Hu et al., 2021), or low-rank adapta-\ntion, approximates the fine-tuning process by\ninjecting trainable rank decomposition matri-\nces into each architecture layer. Compared to\nprefix tuning, which requires some portion of\nthe input sequence for adaptation, LoRA ben-\neficially preserves the entire sequence length\nfor the downstream task. For our base and\nlarge architectures, this requires tuning a mere\n0.39% and 0.32% of total parameters, respec-\ntively (see Table 1). Because LoRA modifies\nslightly more parameters than prefix tuning,\nwe characterize this method as having greater\ndomain adaptation.\n4 Experiments\n4.1 Data\nOur primary dataset is MIMIC-III (Johnson et al.,\n2016), which contains 79,790 radiology reports\nacross two imaging modalities and seven anatomies.\nTable 2 contains a dataset overview. Figure 2 con-\ntains an example of a CT head report; recall the\ntask of RRS is to predict the report impressions\nTable 2: Number of reports in MIMIC-III by modality,\nanatomy, and dataset split.\nModality/ Number of reports\nAnatomy Train Val Test\nCT head 25,122 3,140 3,141\nCT abdomen 12,792 1,599 1,599\nCT chest 10,229 1,278 1,280\nMR head 5,851 731 732\nCT spine 4,414 551 553\nCT neck 912 114 115\nMR spine - - 2,822\nCT sinus - - 1,268\nMR abdomen - - 1,062\nMR pelvis - - 254\nMR neck - - 231\n452\nImpressions generated via…\nNull prompt:\nsulcal effacement, and extensive edema. there is a large left mca \ndistribution infarction.: left mca territory..:, there is an evolving left mca\nPrefixed prompt:\n. on , the following is a summary of the radiology report...:...: findings: there \nis an evolving left mca distribution infarction\nIn-context (1) prompt:\nnew infarction. the left MCA territory is noted. no new areas of \nhypoattenuation are identified. the right lateral ventricle is slightly more \nprominent. the left hemisphere.\nPrefix tuning: \nevolving left mca distribution infarction, with extensive edema and mass \neffect. no evidence of hemorrhagic transformation.\nLoRA:\n1. evolving left mca distribution infarction, with extensive edema and mass \neffect, with 6 mm of midline shift and subfalcine herniation. 2. no evidence \nof hemorrhagic transformation.\nFindings:\nthere is an evolving total left mca distribution infarction, with extensive \nedema and mass effect. on today's study, there is at least 6 mm of midline \nshift and associated subfalcine herniation. there is near total effacement of \nthe left lateral ventricle and perhaps minimal dilatation of the contralateral \nright lateral ventricle. no evidence of hemorrhagic transformation at this \ntime. the left proximal mca and intrasylvian branches remain hyperdense. \nno evidence to suggest evolving contralateral infarction. there are air fluid \nlevels in the sphenoid and maxillary sinuses, presumably related to prior \nintubation.\nImpression (reference):\nevolving total left mca distribution infarction, with significant mass effect, \nmidline shift, and subfalcine herniation. no evidence of hemorrhagic \ntransformation.\nincreasing domain adaptation\nKEY\nBlue: factually correct\nGreen: factually correct, novel compared to reference\nOrange: incoherent but potentially relevant\nPink: hallucinated filler text (okay)\nRed: hallucinated medical text (bad)\nFigure 2: Example radiology report. Left: Findings and reference impression. Right: Generated impressions with\nvarious methods for discrete prompting (top) and parameter-efficient fine-tuning (bottom), all using the CLIN -T5-\nLARGE model. Color annotations were provided by a radiologist who specializes in the relevant anatomy (head).\nsection (label) given the findings section (input).\nWe also provide some evaluations on a secondary\nradiology report dataset, MIMIC-CXR (Johnson\net al., 2019a). This is an easier summarization task\nas it contains one modality and anatomy (chest x-\nrays) with generally shorter impression sections\nthan MIMIC-III. PhysioNet (Johnson et al., 2020b)\nand ViLMedic (Delbrouck et al., 2022b) provided\naccess to pre-processed versions of these datasets,\nremoving confidential patient information. We con-\nduct additional processing according to domain\nadaptation strategies discussed in Section 3.2.\n4.2 Evaluation\nFor quantitative evaluation of our generated impres-\nsions, we employ common summarization metrics\nsuch as BLEU and ROUGE-L. Between a given\npair of reference and generated text, BLEU eval-\nuates overlap using a weighted average of 1- to\n4-gram precision, and ROUGE-L evaluates the\nlongest common subsequence overlap. Beyond\nthese token-level syntactic similarity metrics, we\nemploy by also using metrics like BERTScore (via\nHuggingFace), which computes the semantic simi-\nlarity between the reference and generated texts\nusing BERT embeddings (Zhang* et al., 2020).\nLastly, following previous work (Delbrouck et al.,\n2022a), we evaluate our model with F1-RadGraph,\na F-score style metric that measures the factual\ncorrectness, consistency and completeness of gen-\nerated radiology reports compared to the reference.\nF1-RadGraph uses RadGraph (Jain et al., 2021),\na graph dataset of entities and relations present in\nradiology reports.\nFor qualitative evaluation, we include a reader\nstudy (Figure 3) with three board-certified radiol-\nogists. Each evaluated a randomly selected group\nof twenty generated impressions in comparison to\nreference impressions. They responded with ei-\nther 0 (“no”), 5 (“somewhat”), or 10 (“yes”) to the\nfollowing three questions:\n1. Does the generated impression capture critical\ninformation? Consider if the model missed\nany important findings.\nQuestions \nQ1 ) Does the summary capture critical information?\nQ2 ) Is it factually correct?\nQ3 ) Is it coherent? \nno yessomewhat\n0 5 10\nFigure 3: Radiology reader study. Top: Study design.\nBottom: Results via CLIN -T5-L ARGE + LoRA on ran-\ndom samples from the CT head dataset. The model\nscores highest in coherence (Q3) and generally performs\nwell capturing critical information (Q1) in a factually\ncorrect way (Q2). Each entry’s highlight color corre-\nsponds to its location on the above color spectrum.\n453\nFigure 4: Domain adaptation. Left: Adaptation via pretraining on increasingly relevant data (T5, SCIFIVE , CLIN -\nT5-S CI, CLIN -T5 ) generally leads to improved performance for both fine-tuning methods. Note we exclude\nFLAN-T5 , whose degree of domain adaptation is difficult to rank. See Table 5 in the appendix for comprehensive\nresults. Right: Adaptation via increasing number of in-context examples leads to improved performance in most\nmodels. See Section 5.1 for discussion.\n2. Is the generated impression factually correct?\nGiven the text that the model did generate,\nconsider its correctness.\n3. Is the generated impression coherent, i.e. do\nyou find the syntax comprehensible?\n4.3 Experimental details\nNo further model tuning was needed for the null,\nprefix, and in-context discrete prompting exper-\niments; we simply run inference with the pre-\nexisting LLM. When tuning model hyperparam-\neters for prefix tuning and LoRA, we discovered\nthat the same set of hyperparameters achieves best\nperformance across all five models. This seems rea-\nsonable given that each model employs the same\narchitecture. As such we tuned each model with\nthe same set of hyperparameters. Please see Sec-\ntion A.1 in the appendix for details.\n5 Results and Discussion\n5.1 Domain Adaptation\nA recurring theme throughout our results is that\nincreased domain adaptation leads to improved per-\nformance. One axis for domain adaptation is the\nrelevance of pretraining data to the evaluation task,\nin our case RRS on the MIMIC-III dataset. Fig-\nure 4 (left) demonstrates that pretraining the same\narchitecture on increasingly relevant data (T5, SCI-\nFIVE , CLIN -T5-S CI, CLIN -T5) typically improves\nperformance for prefix tuning and LoRA. We do\nnot include FLAN-T5 in this portion of the anal-\nysis, as its relative degree of domain adaptation is\ndifficult to assess.\nThe second axis for domain adaptation is the\namount of context provided either via longer dis-\ncrete prompts (null, prefix, in-context prompting),\nor parameter-efficient fine-tuning (prefix tuning,\nLoRA). For all models, fine-tuning significantly\noutperforms any discrete prompting technique. Dis-\ncrete prompting is still useful, however, especially\nif one only has black-box access to the LLM. There-\nfore, we compare techniques for discrete prompting\nin Figure 4 (right), demonstrating that prompting\nthe model with more in-context examples improves\nperformance for almost every model with the lone\nexception of T5. As T5 is the least domain-adapted\nmodel, perhaps a small number of in-context exam-\nples may actually hurt performance when examples\nare sufficiently out-of-domain. We leave this ques-\ntion to future work. Another component of Figure 4\n(right) is that the remaining four models improve\nat different rates: SCIFIVE seems to level off near\nfour in-context examples, while the others continue\nto steadily increase. This supports our hypothesis\nthat instruction prompt tuning and maximal domain\nadaptation (in this case, to clinical text) improve\nresults seen with in-context learning. This also mo-\ntivates future work in instruction prompt tuning a\nmodel using domain-specific biomedical or clinical\ninstructions.\nTable 5 in the appendix includes an ablation of\nall configurations for domain adaptation using a\nbase architecture (223M parameters). We subse-\nquently take the best model (CLIN -T5) for the best\nmethods (prefix tuning, LoRA) and scale to a large\narchitecture (738M parameters) in Table 3. This\nscaling provides a significant performance boost.\nSimilar to the base architecture, LoRA outperforms\nprefix tuning in all cases. Interestingly, CLIN -T5-\n454\nTable 3: Best results overall. Top: Given that the base architecture (223M parameters) performs best via pretraining\non clinical text ( CLIN -T5 ) and subsequent fine-tuning, we improve performance on MIMIC-III by scaling to\nthe large architecture (738M). Bottom: LoRA also outperforms prefix tuning on the MIMIC-CXR dataset using\nCLIN -T5.\nDataset Method Size BLEU ROUGE-L BERT F1-Radgraph F1-CheXbert\nMIMIC-III\nprefix tuning base 11.9 33.8 89.4 35.4 -\nlarge 14.6 36.7 89.9 38.4 -\nLoRA base 14.5 36.4 89.9 38.0 -\nlarge 16.2 38.7 90.2 40.8 -\nMIMIC-CXR prefix tuning large 16.1 43.4 89.7 41.0 70.2\nLoRA large 18.9 44.5 90.0 41.8 70.9\nBASE + LoRA achieves nearly equivalent perfor-\nmance to CLIN -T5-L ARGE + prefix tuning, exem-\nplifying the benefits of LoRA over prefix tuning.\nFinally, after performing all prior analysis using the\nMIMIC-III dataset, we apply the best combination\n(CLIN -T5-L ARGE + LoRA) on MIMIC-CXR and\ninclude results in Table 3.\n5.2 Out-of-distribution performance\nTable 4 demonstrates out-of-distribution perfor-\nmance using a CLIN -T5-B ASE model prefix tuned\non CT head data. When evaluating on a differ-\nent test set, the model better summarizes reports\non a different modality (MR head) than a differ-\nent anatomy (CT other). This suggests that report\nfinding tokens are more anatomy- than modality-\nspecific, which seems reasonable. Counterintu-\nitively, the model performs worse when shifting\njust the anatomy (CT other) compared to shifting\nthe modality and anatomy (MR other). This could\nbe due to a myriad of reasons, such as MR find-\nings being 15% longer than CT findings and MR\nother containing fewer anatomies (four) than CT\nother (five). Lastly, we find that training CLIN -T5-\nBASE on all data leads to higher performance than\ntraining on CT head alone.\n5.3 Qualitative Evaluation\n5.3.1 Error Analysis\nWe perform a qualitative error analysis of 20 ran-\ndomly selected reference findings, reference im-\npressions, and generated impressions via CLIN -\nT5-L ARGE on the CT head dataset. Please see\nFigure 2 for an example. We describe four types\nof deviations from the reference impressions and\ntheir corresponding colors used in Figure 2:\n1. Factually correct text that is novel compared\nto the reference impression (green). This text\nis present in the reference findings but not in\nthe reference impression.\n2. Incoherent but potentially relevant text (or-\nange). This contains medically relevant infor-\nmation that is also included in the reference,\nbut is presented with incoherent grammar.\n3. Hallucinated filler text (pink). This includes\nextra punctuation or common words such as\n“findings.” These are filler text because they\nare undesirable but do not detract from the\ncorrectness of the generated impression.\n4. Hallucinated medical text (red). This includes\ntext that is either (1) not explicitly included\nTable 4: Out-of-distribution (OOD) performance of CLIN -T5 prefix tuned on CT head. Compared to in-distribution\n(first row), performance suffers increasingly with OOD modalities (second row) and anatomies (third row). Addi-\ntionally, when evaluating CT head, tuning on a larger dataset comprising all modalities/anatomies (bottom row)\nimproves performance compared to tuning on CT head alone (top row).\nDataset OOD\nTrain Test Modality Anatomy BLEU ROUGE-L BERT F1-Radgraph\nCT head CT head 11.4 35.0 89.8 35.1\nCT head MR head ✓ 9.0 27.5 87.8 27.4\nCT head CT other ✓ 2.9 19.5 86.7 16.3\nCT head MR other ✓ ✓ 7.9 24.2 87.2 25.9\nAll CT head N/A N/A 12.6 35.3 89.7 36.4\n455\nin the findings or reference impression, or (2)\nrelevant to the findings but communicated in\na factually incorrect manner. This is the worst\nof the four deviations, as we want the model\nto avoid inferring information which didn’t\noriginate directly from the radiologist.\n5.3.2 Reader study\nThree radiologists evaluated impressions generated\nvia CLIN -T5-L ARGE + LoRA on the CT head\ndataset according to the procedure described in Sec-\ntion 4.2. Results in Figure 3 are encouraging, as\nour generated impressions scored well for all three\nquestions. Additionally, the radiologists shared the\nfollowing observations:\n• Occasionally there is extra information in-\ncluded in the “reference” impression that is\nnot available in the findings, i.e. which the\nmodel has no chance of summarizing.\n• The model may include duplicate or re-\nsummarized when referring to prior studies.\nFor example, the reference will state “area\nof subarachnoid hemorrhage ... which is un-\nchanged since the patient’s prior scan,” while\nthe generated impression merely says “no sig-\nnificant change since the prior study.” This\ndifference is typically an institutional or per-\nsonal preference.\n• The model made an incorrect reference to the\npatient’s prior medical history. The reference\nwas “subtle hypodensity in the left frontal lobe.\ngiven lack of prior studies available for com-\nparison,” but the generated impression was\n“subtle hypodensity ... in the left frontal lobe,\nwhich is consistent with prior studies.”\nThis study provided valuable insights which\ncould not be obtained via quantitative metrics. Fun-\ndamentally we advocate for the use of reader stud-\nies when evaluating report summarization to facili-\ntate more clinically relevant research.\n5.4 Pitfalls\nWe now discuss weaknesses in our analysis which\nmotivate future study. One potential concern is that\nCLIN -T5 and CLIN -T5-S CI were pretrained over\nthe MIMIC-III dataset. This serves our purpose for\nevaluating models with high levels of domain adap-\ntation, but it could result in data leakage if Lehman\nand Johnson (2023) used the test set for pretraining.\nWe also evaluate each model on a second clini-\ncal dataset, MIMIC-CXR, and CLIN -T5 similarly\nperforms the best (Table 6, appendix).\nUltimately though, while we chose CLIN -T5 in\nTable 3 because it achieves the highest scores,\nwe note the comparable performance of models\nwhich did not pretrain with MIMIC-III, such as T5,\nFLAN-T5, and S CIFIVE (see Appendix Table 5).\nAnother weakness is that “domain” and “distri-\nbution” are not rigorously defined. Our intuitive\ncharacterizations could be improved by quantifying\nor visualizing the distance between different distri-\nbutions using various embedding- or graph-based\nmethods (Johnson et al., 2019b; Jain et al., 2021).\nLastly, we made assumptions determining the\nbest configuration of model and prompting method.\nWe first performed a comprehensive evaluation of\nall models and methods using the base architec-\nture (223M parameters) on MIMIC-III (Table 5,\nappendix) and all models with LoRA on MIMIC-\nCXR (Table 6, appendix). From these results we\nchose only the best model (CLIN -T5 ) for scaling\nto the large architecture (738M parameters) due to\ncompute constraints. Future work should include a\ncomprehensive evaluation of all configurations on\nboth architecture sizes and datasets.\n6 Conclusion\nOur research employs innovative lightweight strate-\ngies to adapt LLMs for the task of RRS. We in-\nvestigate how domain adaptation—both via model\npretraining on relevant data and via methods\nfor discrete prompting and parameter-efficient\nfine-tuning—affects downstream RRS task perfor-\nmance. We achieve best performance using a model\npretrained on clinical text ( CLIN -T5 ) and subse-\nquently fine-tuned with RRS samples using LoRA.\nThese compelling results require tuning a mere\n0.32% of model parameters. While further valida-\ntion is required before clinical deployment, we be-\nlieve our findings contribute to the literature and ad-\nvance the potential for improved radiologist work-\nflows and patient care.\n7 Acknowledgements\nOur research is a direct continuation of the radiol-\nogy report summarization track conducted at ACL\nBioNLP 2023 (Delbrouck et al., 2023).\n456\nReferences\nAsma Ben Abacha, Yassine M’rabet, Yuhao Zhang,\nChaitanya Shivade, Curtis Langlotz, and Dina\nDemner-Fushman. 2021. Overview of the mediqa\n2021 shared task on summarization in the medical\ndomain. In Proceedings of the 20th Workshop on\nBiomedical Language Processing, pages 74–85.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nH.W. Chung, L. Hou, S. Longpre, et al. 2022a.\nScaling instruction-finetuned language models.\nhttps://doi.org/10.48550/arXiv.2210.11416.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022b. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416.\nSongtai Dai, Quan Wang, Yajuan Lyu, and Yong Zhu.\n2021. Bdkg at mediqa 2021: system report for the\nradiology report summarization task. In Proceed-\nings of the 20th Workshop on Biomedical Language\nProcessing, pages 103–111.\nPritam Deka, Anna Jurek-Loughrey, et al. 2022. Evi-\ndence extraction to validate medical claims in fake\nnews detection. In International Conference on\nHealth Information Science, pages 3–15. Springer.\nJean-Benoit Delbrouck, Pierre Chambon, Christian\nBluethgen, Emily Tsai, Omar Almusa, and Curtis\nLanglotz. 2022a. Improving the factual correctness\nof radiology report generation with semantic rewards.\nhttps://aclanthology.org/2022.findings-emnlp.319.\nJean-benoit Delbrouck, Khaled Saab, Maya Varma,\nSabri Eyuboglu, Pierre Chambon, Jared Dunnmon,\nJuan Zambrano, Akshay Chaudhari, and Curtis Lan-\nglotz. 2022b. ViLMedic: a framework for research\nat the intersection of vision and language in medical\nAI. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 23–34, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJean-Benoit Delbrouck, Maya Varma, Pierre Cham-\nbon, and Curtis Langlotz. 2023. Overview of the\nradsum23 shared task on multi-modal and multi-\nanatomical radiology report summarization. In Pro-\nceedings of the 22st Workshop on Biomedical Lan-\nguage Processing, Toronto, Canada. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nEsteban F Gershanik, Ronilda Lacson, and Ramin Kho-\nrasani. 2011. Critical finding capture in the impres-\nsion section of radiology reports. In AMIA Annual\nSymposium Proceedings, volume 2011, page 465.\nAmerican Medical Informatics Association.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nEdward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. 2021.\nLora: Low-rank adaptation of large language models.\nJeremy Irvin et al. 2019. Chexpert: A\nlarge chest radiograph dataset with un-\ncertainty labels and expert comparison.\nhttps://doi.org/10.48550/arXiv.1901.07031.\nSaahil Jain, Ashwin Agrawal, Adriel Saporta,\nSteven QH Truong, Du Nguyen Duong, Tan Bui,\nPierre Chambon, Yuhao Zhang, Matthew P Lungren,\nAndrew Y Ng, et al. 2021. Radgraph: Extracting\nclinical entities and relations from radiology reports.\narXiv preprint arXiv:2106.14463.\nAlistair Johnson, Lucas Bulgarelli, Tom Pollard,\nSteven Horng, Leo Anthony Celi, and Roger Mark.\n2020a. Mimic-iv. PhysioNet. Available online at:\nhttps://physionet. org/content/mimiciv/1.0/(accessed\nAugust 23, 2021).\nAlistair Johnson, Tom Pollard, and Roger Mark. 2020b.\nMIMIC-III clinical database.\nAlistair Johnson et al. 2019a. Mimic-cxr,\na de-identified publicly available database\nof chest radiographs with free-text reports.\nhttps://www.nature.com/articles/s41597-019-0322-\n0.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1–9.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019b.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nCharles E Kahn Jr, Curtis P Langlotz, Elizabeth S Burn-\nside, John A Carrino, David S Channin, David M\nHovsepian, and Daniel L Rubin. 2009. Toward\nbest practices in radiology reporting. Radiology,\n252(3):852–856.\n457\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\nChan, Kory Matthewson, Michael Henry Tessler,\nAntonia Creswell, James L McClelland, Jane X\nWang, and Felix Hill. 2022. Can language models\nlearn from explanations in context? arXiv preprint\narXiv:2204.02329.\nE. Lehman and A. Johnson. 2023. Clinical-t5: Large\nlanguage models built using mimic clinical text.\nhttps://doi.org/10.13026/rj8x-v335.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2020. Exploring versatile generative language model\nvia parameter-efficient transfer learning. arXiv\npreprint arXiv:2004.03829.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V . Le,\nBarret Zoph, Jason Wei, and Adam Roberts. 2023.\nThe flan collection: Designing data and methods for\neffective instruction tuning.\nYasuhide Miura, Yuhao Zhang, Emily Bao Tsai, Curtis P.\nLanglotz, and Dan Jurafsky. 2021. Improving fac-\ntual completeness and consistency of image-to-text\nradiology report generation. In NAACL-HLT 2021.\nNCBI. 1996. Pubmed.\nNCBI. 2000. Pubmed central (pmc).\nLong N Phan, James T Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. arXiv\npreprint arXiv:2106.03598.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. Advances in neural informa-\ntion processing systems, 30.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2022. Large language models encode clinical\nknowledge. arXiv preprint arXiv:2212.13138.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nYuhao Zhang, Derek Merck, Emily Bao Tsai, Christo-\npher D. Manning, and Curtis P. Langlotz. 2020. Opti-\nmizing the factual correctness of a summary: A study\nof summarizing radiology reports. In ACL2020.\nMengjie Zhao and Hinrich Schütze. 2021. Discrete\nand soft prompting for multilingual models. arXiv\npreprint arXiv:2109.03630.\n458\nA Appendix\nA.1 Hyperparameters for parameter-efficient\nfine-tuning\nFor the prefix tuning experiments, we tune each\nLLM with the following hyperparameters:\n• Initial learning rate of 1e−2 that linearly de-\ncays to 1e−3 after a 100-step warm-up.\n• Ten epochs maximum with an early stopping\ncriterion if validation loss has not decayed for\nfive consecutive epochs.\n• Batch size of eight (large architecture) or 16\n(base architecture) with four gradient accumu-\nlation steps, rendering an effective batch size\nof 32 or 64, respectively.\nFor the LoRA experiments, we tune each LLM\nwith the following hyperparameters:\n• Initial learning rate of 1e−3 that decays lin-\nearly to 1e−4 after a 100-step warm-up.\n• Five epochs with no early stopping criterion.\n• Batch size of six with four gradient accumula-\ntion steps, rendering an effective batch size of\n24.\nLoRA requires slightly more memory than pre-\nfix tuning, hence we adjusted the effective batch\nsize to comfortably fit on our NVIDIA Quadro\nRTX 8000 GPU. Despite the larger memory foot-\nprint and greater training time per epoch (Table 1),\nLoRA requires fewer epochs to reach convergence\nthan prefix tuning, resulting in 30% less tuning time\noverall. We note the importance of a learning rate\nwarm-up over the first 100 gradient steps, which\nhas been shown beneficial for low-data settings (Li\nand Liang, 2021). We experimented with various\nlearning rate schedulers (step, exponential decay)\nbut found linear decay to give slightly better per-\nformance in terms of validation loss. As discussed\nin Section 4.3, the best set of hyperparameters is\nconstant across each of the five models.\n459\nTable 5: Quantitative evaluation across each model and adaptation method using the base architecture size. Parameter-\nefficient (updating <0.4% of parameters) fine-tuning methods LoRA and prefix tuning drastically outperform discrete\nprompting strategies. Among these fine-tuning methods, the best performing models are those which have been\npretrained on clinical text (CLIN -T5-S CI, CLIN -T5).\nModel Method BLEU ROUGE-L BERT F1-Radgraph\nnull 3.4 14.3 84.1 13.8\nprefix 4.7 19.0 86.1 19.0\nin-context (1) 3.4 15.8 85.4 14.4\nT5 in-context (2) 3.3 15.8 85.4 11.8\nin-context (4) 4.4 16.2 85.5 12.1\nprefix tuning 12.9 29.1 88.4 30.7\nLoRA 13.7 33.9 89.5 35.2\nnull 0.5 11.3 83.0 9.7\nprefix 1.1 14.7 84.7 13.8\nin-context (1) 2.9 17.8 85.6 14.6\nFLAN-T5 in-context (2) 5.3 19.6 86.2 16.6\nin-context (4) 8.6 25.0 87.0 21.6\nprefix tuning 12.1 27.1 87.8 28.0\nLoRA 13.8 34.4 89.5 36.2\nnull 1.0 6.4 80.0 4.2\nprefix 0.3 4.2 78.0 0.7\nin-context (1) 1.8 11.3 82.0 9.7\nSCIFIVE in-context (2) 2.8 12.4 82.9 12.9\nin-context (4) 3.4 12.7 83.6 14.8\nprefix tuning 10.3 28.9 88.4 30.2\nLoRA 13.5 34.6 89.6 36.1\nnull 1.5 7.0 78.7 6.1\nprefix 1.1 5.0 77.9 4.2\nin-context (1) 0.4 9.9 73.3 7.6\nCLIN -T5-S CI in-context (2) 0.9 11.1 76.1 7.3\nin-context (4) 2.4 14.2 76.7 11.8\nprefix tuning 11.7 33.3 89.3 35.0\nLoRA 13.4 36.4 89.9 37.6\nnull 0.8 12.2 69.4 10.7\nprefix 1.0 9.5 78.6 7.1\nin-context (1) 0.3 8.7 66.1 7.7\nCLIN -T5 in-context (2) 0.6 9.6 66.6 8.7\nin-context (4) 2.2 11.5 70.9 13.0\nprefix tuning 11.9 33.8 89.4 35.4\nLoRA 14.8 36.8 89.9 38.2\nTable 6: Quantitative evaluation on MIMIC-CXR with the best adaptation method (LoRA) across each model using\nthe base architecture size. This supports our hypothesis that pretraining with clinical text is beneficial for datasets\nbeyond MIMIC-III.\nModel BLEU ROUGE-L BERT F1-Radgraph F1-CheXbert\nT5 16.9 40.5 89.6 37.6 66.7\nFLAN-T5 16.6 41.0 89.3 38.2 68.7\nSCIFIVE 17.1 42.4 89.5 39.8 69.0\nCLIN -T5-S CI 16.9 42.7 89.5 39.3 69.3\nCLIN -T5 18.1 43.6 89.7 40.1 69.5\n460",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9326754808425903
    },
    {
      "name": "Computer science",
      "score": 0.6739532351493835
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6223403215408325
    },
    {
      "name": "Biomedical text mining",
      "score": 0.581291913986206
    },
    {
      "name": "Domain adaptation",
      "score": 0.5764865875244141
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5393580794334412
    },
    {
      "name": "Natural language processing",
      "score": 0.49724701046943665
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4103473424911499
    },
    {
      "name": "Psychology",
      "score": 0.1270802617073059
    },
    {
      "name": "Text mining",
      "score": 0.09255078434944153
    },
    {
      "name": "Mathematics",
      "score": 0.060130178928375244
    },
    {
      "name": "Neuroscience",
      "score": 0.05742400884628296
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ]
}