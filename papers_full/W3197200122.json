{
  "title": "The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning",
  "url": "https://openalex.org/W3197200122",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100669162",
      "name": "Yujin Tang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5001419875",
      "name": "David Ha",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3114510016",
    "https://openalex.org/W2550182557",
    "https://openalex.org/W3080453421",
    "https://openalex.org/W193141452",
    "https://openalex.org/W2970866842",
    "https://openalex.org/W2948609886",
    "https://openalex.org/W2984074464",
    "https://openalex.org/W3006201994",
    "https://openalex.org/W2947265751",
    "https://openalex.org/W2131294823",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W3115727409",
    "https://openalex.org/W3013735613",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W3116353560",
    "https://openalex.org/W2971239080",
    "https://openalex.org/W2578206533",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2951736998",
    "https://openalex.org/W3198212763",
    "https://openalex.org/W2083007992",
    "https://openalex.org/W3135415126",
    "https://openalex.org/W2535697732",
    "https://openalex.org/W2140135625",
    "https://openalex.org/W3127433878",
    "https://openalex.org/W2890208753",
    "https://openalex.org/W3101438731",
    "https://openalex.org/W2558460151",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W3040182061",
    "https://openalex.org/W2787712361",
    "https://openalex.org/W2907502844",
    "https://openalex.org/W2975357369",
    "https://openalex.org/W185620388",
    "https://openalex.org/W2160121923",
    "https://openalex.org/W3112689365",
    "https://openalex.org/W2195446438",
    "https://openalex.org/W2889731659",
    "https://openalex.org/W2963168530",
    "https://openalex.org/W102487131",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1891300059",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2792839479",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2137825550",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W2612506596",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W2999617596",
    "https://openalex.org/W2584280040",
    "https://openalex.org/W2785693718",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2771807014",
    "https://openalex.org/W1969021871",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2950004691",
    "https://openalex.org/W3154346432",
    "https://openalex.org/W2097815751",
    "https://openalex.org/W3008659862",
    "https://openalex.org/W2089217417",
    "https://openalex.org/W2610992585",
    "https://openalex.org/W3111551570",
    "https://openalex.org/W2952433032",
    "https://openalex.org/W3032554070",
    "https://openalex.org/W3138886030",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io/",
  "full_text": "The Sensory Neuron as a Transformer:\nPermutation-Invariant Neural Networks for\nReinforcement Learning\nYujin Tang†\nGoogle Brain\nyujintang@google.com\nDavid Ha†\nGoogle Brain\nhadavid@google.com\nAbstract\nIn complex systems, we often observe complex global behavior emerge from a\ncollection of agents interacting with each other in their environment, with each\nindividual agent acting only on locally available information, without knowing\nthe full picture. Such systems have inspired development of artiﬁcial intelligence\nalgorithms in areas such as swarm optimization and cellular automata. Motivated\nby the emergence of collective behavior from complex cellular systems, we build\nsystems that feed each sensory input from the environment into distinct, but identi-\ncal neural networks, each with no ﬁxed relationship with one another. We show\nthat these sensory networks can be trained to integrate information received locally,\nand through communication via an attention mechanism, can collectively produce a\nglobally coherent policy. Moreover, the system can still perform its task even if the\nordering of its inputs is randomly permuted several times during an episode. These\npermutation invariant systems also display useful robustness and generalization\nproperties that are broadly applicable. Interactive demo and videos of our results:\nhttps://attentionneuron.github.io/\n1 Introduction\nSensory substitution refers to the brain’s ability to use one sensory modality (e.g., touch) to supply\nenvironmental information normally gathered by another sense (e.g., vision). Numerous studies have\ndemonstrated that humans can adapt to changes in sensory inputs, even when they are fed into the\nwrong channels [4, 5, 24, 63]. But difﬁcult adaptations–such as learning to “see” by interpreting\nvisual information emitted from a grid of electrodes placed on one’s tongue [5], or learning to ride a\n“backwards” bicycle [63]–require months of training to attain mastery. Can we do better, and create\nartiﬁcial systems that can rapidly adapt to sensory substitutions, without the need to be retrained?\nFigure 1: Comparison of visual input intended for the game player, and what our system receives.\nWe partition the visual input from CarRacing (Left) and Atari Pong (right) into a 2D grid of small\npatches, and randomly permute their ordering. Each sensory neuron in the system receives a stream\nof visual input at a particular permuted patch location, and through coordination, must complete the\ntask at hand, even if the visual ordering is randomly permuted again several times during an episode.\n†Equal Contribution\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2109.02869v2  [cs.NE]  29 Sep 2021\nModern deep learning systems are generally unable to adapt to a sudden reordering of sensory inputs,\nunless the model is retrained, or if the user manually corrects the ordering of the inputs for the\nmodel. However, techniques from continual meta-learning, such as adaptive weights [ 2, 35, 64],\nHebbian-learning [51, 52, 56], and model-based [1, 19, 36, 37] approaches can help the model adapt\nto such changes, and remain a promising active area of research.\nIn this work, we investigate agents that are explicitly designed to deal with sudden random reordering\nof their sensory inputs while performing a task. Motivated by recent developments in self-organizing\nneural networks [26, 54, 60] related to cellular automata [13, 16, 17, 57, 77], in our experiments, we\nfeed each sensory input (which could be an individual state from a continuous control environment, or\na patch of pixels from a visual environment) into an individual neural network module that integrates\ninformation from only this particular sensory input channel over time. While receiving information\nlocally, each of these individual sensory neural network modules also continually broadcasts an output\nmessage. Inspired by the Set Transformer [47, 73] architecture, an attention mechanism combines\nthese messages to form a global latent code which is then converted into the agent’s action space. The\nattention mechanism can be viewed as a form of adaptive weights of a neural network, and in this\ncontext, allows for an arbitrary number of sensory inputs that can be processed in any random order.\nIn our experiments, we ﬁnd that each individual sensory neural network module, despite receiving\nonly localized information, can still collectively produce a globally coherent policy, and that such a\nsystem can be trained to perform tasks in several popular reinforcement learning (RL) environments.\nFurthermore, our system can utilize a varying number of sensory input channels in any randomly\npermuted order, even when the order is shufﬂed again several times during an episode.\nPermutation invariant systems have several advantages over traditional ﬁxed-input systems. We ﬁnd\nthat encouraging a system to learn a coherent representation of a permutation invariant observation\nspace leads to policies that are more robust and generalizes better to unseen situations. We show\nthat, without additional training, our system continues to function even when we inject additional\ninput channels containing noise or redundant information. In visual environments, we show that our\nsystem can be trained to perform a task even if it is given only a small fraction of randomly chosen\npatches from the screen, and at test time, if given more patches, the system can take advantage of\nthe additional information to perform better. We also demonstrate that our system can generalize to\nvisual environments with novel background images, despite training on a single ﬁxed background.\nLastly, to make training more practical, we propose a behavioral cloning scheme to convert policies\ntrained with existing methods into a permutation invariant policy with desirable properties.\n2 Related Work\nSelf-organization is a process where some form of global order emerges from local interactions\nbetween parts of an initially disordered system. It is also a property observed in cellular automata\n(CA) [16, 17, 57], which are mathematical systems consisting of a grid of cells that perform com-\nputation by having each cell communicate with its immediate neighbors and performing a local\ncomputation to update its internal state. Such local interactions are useful in modeling complex\nsystems [77] and have been applied to model non-linear dynamics in various ﬁelds [ 13]. Cellular\nNeural Networks [ 15] were ﬁrst introduced in the 1980s to use neural networks in place of the\nalgorithmic cells in CA systems. They were applied to perform image processing operations with\nparallel computation. Eventually, the concept of self-organizing neural networks found its way into\ndeep learning in the form of Graph Neural Networks (GNN) [61, 78].\nUsing modern deep learning tools, recent work demonstrates that neural CA, or self-organized neural\nnetworks performing only local computation, can generate (and re-generate) coherent images [54]\nand voxel scenes [69, 83], and even perform image classiﬁcation [60]. Self-organizing neural network\nagents have been proposed in the RL domain [ 10, 11, 58, 59], with recent work demonstrating\nthat shared local policies at the actuator level [ 42], through communicating with their immediate\nneighbors, can learn a global coherent policy for continuous control locomotion tasks. While existing\nCA-based approaches present a modular, self-organized solution, they are not inherently permutation\ninvariant. In our work, we build on neural CA, and enable each cell to communicate beyond its\nimmediate neighbors via an attention mechanism that enables permutation invariance.\nMeta-learning recurrent neural networks (RNN) [22, 39, 41, 75] have been proposed to approach the\nproblem of learning the learning rules for a neural network using the reward or error signal, enabling\nmeta-learners to learn to solve problems presented outside of their original training domains. The\n2\ngoals are to enable agents to continually learn from their environments in a single lifetime episode,\nand to obtain much better data efﬁciency than conventional learning methods such as stochastic\ngradient descent (SGD). A meta-learned policy that can adapt the weights of a neural network to its\ninputs during inference time have been proposed in fast weights [ 64, 66], associative weights [2],\nhypernetworks [35], and Hebbian-learning [51, 52] approaches. Recently works [45, 62] combine\nideas of self-organization with meta-learning RNNs, and have demonstrated that modular meta-\nlearning RNN systems not only can learn to perform SGD-like learning rules, but can also discover\nmore general learning rules that transfer to classiﬁcation tasks on unseen datasets.\nIn contrast, the system presented here does not use an error or reward signal to meta-learn or ﬁne-tune\nits policy. But rather, by using the shared modular building blocks from the meta-learning literature,\nwe focus on learning or converting an existing policy to one that is permutation invariant, and we\nexamine the characteristics such policies exhibit in a zero-shot setting, without additional training.\nAttention can be viewed as an adaptive weight mechanism that alters the weight connections\nof a neural network layer based on what the inputs are. Linear dot-product attention has ﬁrst\nbeen proposed for meta-learning [65], and versions of linear attention with softmax nonlinearity\nappeared later [32, 50], now made popular with Transformer [73]. The adaptive nature of attention\nprovided the Transformer with a high degree of expressiveness, enabling it to learn inductive biases\nfrom large datasets and have been incorporated into state-of-the-art methods in natural language\nprocessing [8, 20], image recognition [21] and generation [25], audio and video domains [30, 43, 70].\nAttention mechanisms have found many uses for RL [12, 55, 67, 71, 82]. Our work here speciﬁcally\nuses attention to enable communication between arbitrary numbers of modules in an RL agent. While\nprevious work [31, 44, 53, 74, 80, 84] utilized attention as a communication mechanism between\nindependent neural network modules of a GNN, our work focuses on studying the permutation\ninvariant properties of attention-based communication applied to RL agents. Related work [ 49]\nused permutation invariant critics to enhance performance of multi-agent RL. Building [33, 81], Set\nTransformers [47] investigated the use of attention explicitly for permutation invariant problems that\ndeal with set-structured data, which have provided the theoretical foundation for our work.\n3 Method\n3.1 Background\nOur goal is to devise an agent that is permutation invariant (PI) in the action space to the permutations\nin the input space. While it is possible to acquire a quasi-PI agent by training with randomly shufﬂed\nobservations and hope the agent’s policy network has enough capacity to memorize all the patterns,\nwe aim for a design that achieves true PI even if the agent is trained with ﬁx-ordered observations.\nMathematically, we are looking for a non-trivial functionf(x) :Rn ↦→Rm such that f(x[s]) =f(x)\nfor any x∈Rn, and sis any permutation of the indices {1,··· ,n}. A different but closely related\nconcept is permutation equivariance (PE) which can be described by a function h(x) :Rn ↦→Rn\nsuch that h(x[s]) =h(x)[s]. Unlike PI, the dimensions of the input and the output must equal in PE.\nSelf-attentions can be PE. In its simplest form, self-attention is described as y= σ(QK⊤)V where\nQ,K ∈Rn×dq ,V ∈Rn×dv are the Query, Key and Value matrices andσ(·) is a non-linear function.\nIn most scenarios, Q,K,V are functions of the input x ∈Rn (e.g. linear transformations), and\npermuting xtherefore is equivalent to permuting the rows in Q,K,V and based on its deﬁnition it\nis straightforward to verify the PE property. Set Transformer [47] cleverly replaced Qwith a set of\nlearnable seed vectors, so it is no longer a function of input x, thus enabling the output to become PI.\nA simple, intuitive explanation of the PI property of self-attention is available in Appendix A.1.\n3.2 Sensory Neurons with Attention\nTo create PI agents, we propose to add an extra layer in front of the agent’s policy networkπ, which\naccepts the current observation ot and the previous action at−1 as its inputs. We call this new layer\nAttentionNeuron, and Figure 2 gives an overview of our method. Inside AttentionNeuron, we model\nthe observation ot as an arbitrarily ordered, variable-length list of sensory inputs, each of which is\npassed into its own sensory neuron, a neural network module. Each sensory neuron only has partial\naccess to the agent’s observation, at time t, the ith neuron can see only the ith component of the\nobservation ot[i]. Combined with the previous action at−1, each sensory neuron computes messages\nfk(ot[i],at−1) and fv(ot[i]) that are broadcast to the rest of the system. We then use attention to\naggregate these messages into a global latent code, mt, that is PI with respect to the inputs.\n3\nAttentionNeuron\n<latexit sha1_base64=\"MSgGTlk/UUyhm4FlQ17N+lq+FRo=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NLmIphYkTsKpSSxscTEAxK4kL1lDzbs7l1250wI4TfYWGiMrT/Izn/jAlco+JJJXt6bycy8KBXcoOd9O4Wt7Z3dveJ+6eDw6PikfHrWNkmmKQtoIhLdjYhhgisWIEfBuqlmREaCdaLJ3cLvPDFteKIecZqyUJKR4jGnBK0UVOUAq4Nyxat5S7ibxM9JBXK0BuWv/jChmWQKqSDG9HwvxXBGNHIq2LzUzwxLCZ2QEetZqohkJpwtj527V1YZunGibSl0l+rviRmRxkxlZDslwbFZ9xbif14vw7gRzrhKM2SKrhbFmXAxcRefu0OuGUUxtYRQze2tLh0TTSjafEo2BH/95U3Srtf8m1r9oV5pNvI4inABl3ANPtxCE+6hBQFQ4PAMr/DmKOfFeXc+Vq0FJ585hz9wPn8AF/OOLw==</latexit>\nm t\n<latexit sha1_base64=\"01CPmgmVNm06clg1/33U6XNLl9U=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsKpSSxscTEAxK4kL1lDzbs7V1250wI4TfYWGiMrT/Izn/jAlco+JJJXt6bycy8MJXCoOt+O4Wt7Z3dveJ+6eDw6PikfHrWNkmmGfdZIhPdDanhUijuo0DJu6nmNA4l74STu4XfeeLaiEQ94jTlQUxHSkSCUbSSX+2nojooV9yauwTZJF5OKpCjNSh/9YcJy2KukElqTM9zUwxmVKNgks9L/czwlLIJHfGepYrG3ASz5bFzcmWVIYkSbUshWaq/J2Y0NmYah7Yzpjg2695C/M/rZRg1gplQaYZcsdWiKJMEE7L4nAyF5gzl1BLKtLC3EjammjK0+ZRsCN76y5ukXa95N7X6Q73SbORxFOECLuEaPLiFJtxDC3xgIOAZXuHNUc6L8+58rFoLTj5zDn/gfP4AByuOJA==</latexit>\n⇡\n<latexit sha1_base64=\"wRkAxm1exqC1lR893Ex4z6r5Nic=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsKpSSxscTEAxK4kL1lDzbs7V1250wI4TfYWGiMrT/Izn/jAlco+JJJXt6bycy8MJXCoOt+O4Wt7Z3dveJ+6eDw6PikfHrWNkmmGfdZIhPdDanhUijuo0DJu6nmNA4l74STu4XfeeLaiEQ94jTlQUxHSkSCUbSSX6UDrA7KFbfmLkE2iZeTCuRoDcpf/WHCspgrZJIa0/PcFIMZ1SiY5PNSPzM8pWxCR7xnqaIxN8FseeycXFllSKJE21JIlurviRmNjZnGoe2MKY7NurcQ//N6GUaNYCZUmiFXbLUoyiTBhCw+J0OhOUM5tYQyLeythI2ppgxtPiUbgrf+8iZp12veTa3+UK80G3kcRbiAS7gGD26hCffQAh8YCHiGV3hzlPPivDsfq9aCk8+cwx84nz8Fn44j</latexit>\na t\n<latexit sha1_base64=\"sK/ZfLDao9LelKWTm+CFIbSJUic=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NLmIphYkTsKpSSxscTEAxK4kL1lDzbs7V5250wI4TfYWGiMrT/Izn/jAlco+JJJXt6bycy8KBXcoOd9O4Wt7Z3dveJ+6eDw6PikfHrWNirTlAVUCaW7ETFMcMkC5ChYN9WMJJFgnWhyt/A7T0wbruQjTlMWJmQkecwpQSsFVTXA6qBc8WreEu4m8XNSgRytQfmrP1Q0S5hEKogxPd9LMZwRjZwKNi/1M8NSQidkxHqWSpIwE86Wx87dK6sM3VhpWxLdpfp7YkYSY6ZJZDsTgmOz7i3E/7xehnEjnHGZZsgkXS2KM+Gichefu0OuGUUxtYRQze2tLh0TTSjafEo2BH/95U3Srtf8m1r9oV5pNvI4inABl3ANPtxCE+6hBQFQ4PAMr/DmSOfFeXc+Vq0FJ585hz9wPn8AGwGOMQ==</latexit>\no t\n<latexit sha1_base64=\"LQp/IPfW6aQ5SVg/11mFGL8PH2E=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmI5jYSO4olJLExhIT+TBwIXvLAht27y67cybkwq+wsdAYW3+Onf/GBa5Q8CWTvLw3k5l5QSyFQdf9dnIbm1vbO/ndwt7+weFR8fikZaJEM95kkYx0J6CGSxHyJgqUvBNrTlUgeTuY3M799hPXRkThA05j7is6CsVQMIpWeizTfopX3qzcL5bcirsAWSdeRkqQodEvfvUGEUsUD5FJakzXc2P0U6pRMMlnhV5ieEzZhI5419KQKm78dHHwjFxYZUCGkbYVIlmovydSqoyZqsB2Kopjs+rNxf+8boLDmp+KME6Qh2y5aJhIghGZf08GQnOGcmoJZVrYWwkbU00Z2owKNgRv9eV10qpWvOtK9b5aqteyOPJwBudwCR7cQB3uoAFNYKDgGV7hzdHOi/PufCxbc042cwp/4Hz+AKiKj6E=</latexit>\na t \u0000 1\n<latexit sha1_base64=\"YAbI6ZIEj3Ej70SDKI2OYVxVcAQ=\">AAAB73icbVA9SwNBEJ3zM8avqKXNYiJYhbsUmjJgYxnBfMDlCHubvWTJ3u25OyeEkD9hY6GIrX/Hzn/jJrlCEx8MPN6bYWZemEph0HW/nY3Nre2d3cJecf/g8Oi4dHLaNirTjLeYkkp3Q2q4FAlvoUDJu6nmNA4l74Tj27nfeeLaCJU84CTlQUyHiYgEo2ilbkX10feCSr9UdqvuAmSdeDkpQ45mv/TVGyiWxTxBJqkxvuemGEypRsEknxV7meEpZWM65L6lCY25CaaLe2fk0ioDEiltK0GyUH9PTGlszCQObWdMcWRWvbn4n+dnGNWDqUjSDHnClouiTBJUZP48GQjNGcqJJZRpYW8lbEQ1ZWgjKtoQvNWX10m7VvWuq7X7WrlRz+MowDlcwBV4cAMNuIMmtICBhGd4hTfn0Xlx3p2PZeuGk8+cwR84nz/xAI84</latexit>\no t [1]\n<latexit sha1_base64=\"KCpyh/ClZCAOvpdtO+wK7ImaI8o=\">AAAB73icbVBNS8NAEJ34WetX1aOXxVbwVJIctMeCF48V7AekoWy2m3bpZjfuboQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8KOVMG9f9djY2t7Z3dkt75f2Dw6PjyslpR8tMEdomkkvVi7CmnAnaNsxw2ksVxUnEaTea3M797hNVmknxYKYpDRM8EixmBBsr9WpyYAI/rA0qVbfuLoDWiVeQKhRoDSpf/aEkWUKFIRxrHXhuasIcK8MIp7NyP9M0xWSCRzSwVOCE6jBf3DtDl1YZolgqW8Kghfp7IseJ1tMksp0JNmO96s3F/7wgM3EjzJlIM0MFWS6KM46MRPPn0ZApSgyfWoKJYvZWRMZYYWJsRGUbgrf68jrp+HXvuu7f+9Vmo4ijBOdwAVfgwQ004Q5a0AYCHJ7hFd6cR+fFeXc+lq0bTjFzBn/gfP4A8oaPOQ==</latexit>\no t [2]\n<latexit sha1_base64=\"gRoGCrOQbxDdfBGElvPAw7L6pFY=\">AAAB73icbVBNSwMxEJ2tX7V+VT16CbaCp7JbQXssePFYwX7AdinZNNuGZpM1yQpl6Z/w4kERr/4db/4b03YP2vpg4PHeDDPzwoQzbVz32ylsbG5t7xR3S3v7B4dH5eOTjpapIrRNJJeqF2JNORO0bZjhtJcoiuOQ0244uZ373SeqNJPiwUwTGsR4JFjECDZW6lXlwPhXQXVQrrg1dwG0TrycVCBHa1D+6g8lSWMqDOFYa99zExNkWBlGOJ2V+qmmCSYTPKK+pQLHVAfZ4t4ZurDKEEVS2RIGLdTfExmOtZ7Goe2MsRnrVW8u/uf5qYkaQcZEkhoqyHJRlHJkJJo/j4ZMUWL41BJMFLO3IjLGChNjIyrZELzVl9dJp17zrmv1+3ql2cjjKMIZnMMleHADTbiDFrSBAIdneIU359F5cd6dj2VrwclnTuEPnM8f9AyPOg==</latexit>\no t [3]\n<latexit sha1_base64=\"iGqV9/s4Bx1C62nS8Bm3hCw/VAM=\">AAAB73icbVA9SwNBEJ2LXzF+RS1tFhPBKtyl0JQBGyuJYD7gcoS9zV6yZG/33N0TwpE/YWOhiK1/x85/4ya5QhMfDDzem2FmXphwpo3rfjuFjc2t7Z3ibmlv/+DwqHx80tEyVYS2ieRS9UKsKWeCtg0znPYSRXEcctoNJzdzv/tElWZSPJhpQoMYjwSLGMHGSr2qHBj/LqgOyhW35i6A1omXkwrkaA3KX/2hJGlMhSEca+17bmKCDCvDCKezUj/VNMFkgkfUt1TgmOogW9w7QxdWGaJIKlvCoIX6eyLDsdbTOLSdMTZjverNxf88PzVRI8iYSFJDBVkuilKOjETz59GQKUoMn1qCiWL2VkTGWGFibEQlG4K3+vI66dRr3lWtfl+vNBt5HEU4g3O4BA+uoQm30II2EODwDK/w5jw6L86787FsLTj5zCn8gfP5Ax09j1U=</latexit>\no t [ N ]\n<latexit sha1_base64=\"KoMakcMWZITzwwLcacecS++pPSE=\">AAAB73icbVA9SwNBEJ3zM8avqKXNYiJYhbsUmjJgYxnBfEByhL29TbJk7/bcnRPCkT9hY6GIrX/Hzn/jJrlCEx8MPN6bYWZekEhh0HW/nY3Nre2d3cJecf/g8Oi4dHLaNirVjLeYkkp3A2q4FDFvoUDJu4nmNAok7wST27nfeeLaCBU/4DThfkRHsRgKRtFK3UqfhQpNZVAqu1V3AbJOvJyUIUdzUPrqh4qlEY+RSWpMz3MT9DOqUTDJZ8V+anhC2YSOeM/SmEbc+Nni3hm5tEpIhkrbipEs1N8TGY2MmUaB7Ywojs2qNxf/83opDut+JuIkRR6z5aJhKgkqMn+ehEJzhnJqCWVa2FsJG1NNGdqIijYEb/XlddKuVb3rau2+Vm7U8zgKcA4XcAUe3EAD7qAJLWAg4Rle4c15dF6cd+dj2brh5DNn8AfO5w9ngY+G</latexit>\n···\n<latexit sha1_base64=\"KoMakcMWZITzwwLcacecS++pPSE=\">AAAB73icbVA9SwNBEJ3zM8avqKXNYiJYhbsUmjJgYxnBfEByhL29TbJk7/bcnRPCkT9hY6GIrX/Hzn/jJrlCEx8MPN6bYWZekEhh0HW/nY3Nre2d3cJecf/g8Oi4dHLaNirVjLeYkkp3A2q4FDFvoUDJu4nmNAok7wST27nfeeLaCBU/4DThfkRHsRgKRtFK3UqfhQpNZVAqu1V3AbJOvJyUIUdzUPrqh4qlEY+RSWpMz3MT9DOqUTDJZ8V+anhC2YSOeM/SmEbc+Nni3hm5tEpIhkrbipEs1N8TGY2MmUaB7Ywojs2qNxf/83opDut+JuIkRR6z5aJhKgkqMn+ehEJzhnJqCWVa2FsJG1NNGdqIijYEb/XlddKuVb3rau2+Vm7U8zgKcA4XcAUe3EAD7qAJLWAg4Rle4c15dF6cd+dj2brh5DNn8AfO5w9ngY+G</latexit>\n···\nSensory Neurons\n<latexit sha1_base64=\"LQp/IPfW6aQ5SVg/11mFGL8PH2E=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmI5jYSO4olJLExhIT+TBwIXvLAht27y67cybkwq+wsdAYW3+Onf/GBa5Q8CWTvLw3k5l5QSyFQdf9dnIbm1vbO/ndwt7+weFR8fikZaJEM95kkYx0J6CGSxHyJgqUvBNrTlUgeTuY3M799hPXRkThA05j7is6CsVQMIpWeizTfopX3qzcL5bcirsAWSdeRkqQodEvfvUGEUsUD5FJakzXc2P0U6pRMMlnhV5ieEzZhI5419KQKm78dHHwjFxYZUCGkbYVIlmovydSqoyZqsB2Kopjs+rNxf+8boLDmp+KME6Qh2y5aJhIghGZf08GQnOGcmoJZVrYWwkbU00Z2owKNgRv9eV10qpWvOtK9b5aqteyOPJwBudwCR7cQB3uoAFNYKDgGV7hzdHOi/PufCxbc042cwp/4Hz+AKiKj6E=</latexit>\na t \u0000 1\nAttention\n<latexit sha1_base64=\"MSgGTlk/UUyhm4FlQ17N+lq+FRo=\">AAAB7HicbVA9TwJBEJ3DL8Qv1NLmIphYkTsKpSSxscTEAxK4kL1lDzbs7l1250wI4TfYWGiMrT/Izn/jAlco+JJJXt6bycy8KBXcoOd9O4Wt7Z3dveJ+6eDw6PikfHrWNkmmKQtoIhLdjYhhgisWIEfBuqlmREaCdaLJ3cLvPDFteKIecZqyUJKR4jGnBK0UVOUAq4Nyxat5S7ibxM9JBXK0BuWv/jChmWQKqSDG9HwvxXBGNHIq2LzUzwxLCZ2QEetZqohkJpwtj527V1YZunGibSl0l+rviRmRxkxlZDslwbFZ9xbif14vw7gRzrhKM2SKrhbFmXAxcRefu0OuGUUxtYRQze2tLh0TTSjafEo2BH/95U3Srtf8m1r9oV5pNvI4inABl3ANPtxCE+6hBQFQ4PAMr/DmKOfFeXc+Vq0FJ585hz9wPn8AF/OOLw==</latexit>\nm t\n<latexit sha1_base64=\"ECd0QP2n0gRc+PCQTM/2VPPjf3U=\">AAAB+nicbVBNS8NAEN3Ur1q/Uj16WWyFClqSHtRjwYvgpYL9gLaEzXbTLm42YXeilNif4sWDIl79Jd78N27bHLT1wcDjvRlm5vmx4Boc59vKrayurW/kNwtb2zu7e3Zxv6WjRFHWpJGIVMcnmgkuWRM4CNaJFSOhL1jbv7+a+u0HpjSP5B2MY9YPyVDygFMCRvLsYvmmEnlwiomXwpk7OSl7dsmpOjPgZeJmpIQyNDz7qzeIaBIyCVQQrbuuE0M/JQo4FWxS6CWaxYTekyHrGipJyHQ/nZ0+wcdGGeAgUqYk4Jn6eyIlodbj0DedIYGRXvSm4n9eN4Hgsp9yGSfAJJ0vChKBIcLTHPCAK0ZBjA0hVHFzK6YjoggFk1bBhOAuvrxMWrWqe16t3dZKdSeLI48O0RGqIBddoDq6Rg3URBQ9omf0it6sJ+vFerc+5q05K5s5QH9gff4AqjOSRA==</latexit>\nK ( o t ,a t \u0000 1 )\n<latexit sha1_base64=\"+mTQfh9hqIs54xGdrLdDnOW66m4=\">AAAB73icbVA9TwJBEJ3DL8Qv1NJmI5hgQ+4o1JLExhIT+UjgQvaWBTbs3Z67cybkwp+wsdAYW/+Onf/GBa5Q8CWTvLw3k5l5QSyFQdf9dnIbm1vbO/ndwt7+weFR8fikZVSiGW8yJZXuBNRwKSLeRIGSd2LNaRhI3g4mt3O//cS1ESp6wGnM/ZCOIjEUjKKVOuVWRfXxstwvltyquwBZJ15GSpCh0S9+9QaKJSGPkElqTNdzY/RTqlEwyWeFXmJ4TNmEjnjX0oiG3Pjp4t4ZubDKgAyVthUhWai/J1IaGjMNA9sZUhybVW8u/ud1Exze+KmI4gR5xJaLhokkqMj8eTIQmjOUU0so08LeStiYasrQRlSwIXirL6+TVq3qXVVr97VS3c3iyMMZnEMFPLiGOtxBA5rAQMIzvMKb8+i8OO/Ox7I152Qzp/AHzucPiTGO7g==</latexit>\nV ( o t )\n<latexit sha1_base64=\"l0+h0Pak/KzfAfsPiq4FQxAsuZU=\">AAAB6nicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsKtSSxsYQoSAIXsrfMwYa9vcvungm58BNsLDTG1l9k579xgSsUfMkkL+/NZGZekAiujet+O4WNza3tneJuaW//4PCofHzS0XGqGLZZLGLVDahGwSW2DTcCu4lCGgUCH4PJ7dx/fEKleSwfzDRBP6IjyUPOqLHSfbVVHZQrbs1dgKwTLycVyNEclL/6w5ilEUrDBNW657mJ8TOqDGcCZ6V+qjGhbEJH2LNU0gi1ny1OnZELqwxJGCtb0pCF+nsio5HW0yiwnRE1Y73qzcX/vF5qwhs/4zJJDUq2XBSmgpiYzP8mQ66QGTG1hDLF7a2EjamizNh0SjYEb/XlddKp17yrWr1VrzTcPI4inME5XIIH19CAO2hCGxiM4Ble4c0Rzovz7nwsWwtOPnMKf+B8/gBe740k</latexit>\nQ\nFigure 2: Overview of Method. AttentionNeuron is a standalone layer, in which each sensory neuron\nonly has access to a part of the unordered observations ot. Together with the agent’s previous action\nat−1, each neuron generates messages independently using the shared functions fk(ot[i],at−1) and\nfv(ot[i]). The attention mechanism summarizes the messages into a global latent code mt.\nThe operations inside AttentionNeuron can be described by the following two equations. For clarity,\nTable 1 summarizes the notations as well as the corresponding setups we used for the experiments.\nK(ot,at−1) =\n[fk(ot[1],at−1)\n···\nfk(ot[N],at−1)\n]\n∈RN×dfk ,V (ot) =\n[fv(ot[1])\n···\nfv(ot[N])\n]\n∈RN×dfv (1)\nmt = σ\n([QWq][K(ot,at−1)Wk]⊤\n√\ndq\n)\n[V(ot)Wv] (2)\nEquation 1 shows how each of the N sensory neuron independently generates its messages fk and\nfv, which are functions shared across all sensory neurons. Equation 2 shows the attention mecha-\nnism aggregate these messages. Note that although we could have absorbed the projection matrices\nWq,Wk,Wv into Q,K,V , we keep them in the equation to show explicitly the formulation. Equa-\ntion 2 is almost identical to the simple deﬁnition of self-attention mentioned earlier. Following [47],\nwe make our Qmatrix a bank of ﬁxed embeddings, rather than depend on the observation ot.\nNote that permuting the observations only affects the row orders of Kand V, and that applying the\nsame permutation to the rows of both Kand V still results in the same mt which is PI. As long as we\nset constant the number of rows in Q, the change in the input size affects only the number of rows in\nKand V and does not affect the output mt. In other words, our agent can accept inputs of arbitrary\nlength and output a ﬁxed sized mt. Later, we apply this ﬂexibility of input dimensions to RL agents.\nTable 1: In this notation list, we provide the dimensions used in our model for different RL environ-\nments, to give the reader a sense of the relative magnitudes involved in each part of the system.\nDescription Notation CartPole Ant CarRacing Atari Pong\nFull observation space ot R5 R28 R96×96×4 R84×84×4\nIndividual sensory input space ot[i] R1 R1 R6×6×4=144 R6×6×4=144\nNumber of sensory neurons N 5 28 (96 /6)2 = 256 (84 /6)2 = 196\nDimension of action space |A| 1 8 3 6 (one-hot)\nNumber of embeddings in Q M 16 32 1024 400\nProjection matrix for Q Wq R8×32 R8×32 R8×16 R8×32\nProjection matrix for K Wk R8×32 R8×32 R111×16 R114×32\nProjection matrix for V Wv I I R144×16 R144×32\nPost-attention activation function σ(·) tanh tanh softmax softmax\nGlobal latent code mt R16 R32 R1024×16 R400×32\n3.3 Design Choices\nIt is worthwhile to have a discussion on the design choices made. Since the ordering of the input is\narbitrary, each sensory neuron is required to interpret and identify their received signal. To achieve\nthis, we want fk(ot[i],at−1) to have temporal memories. In practice, we ﬁnd both RNNs and\nfeed-forward neural networks (FNN) with stacked observations work well, with FNNs being more\npractical for environments with high dimensional observations.\n4\nIn addition to the temporal memory, including previous actions is important for the input identiﬁcation\ntoo. Although the former allows the neurons to infer the input signals based on the characteristics of\nthe temporal stream, this may not be sufﬁcient. For example, when controlling a legged robot, most\nof the sensor readings are joint angles and velocities from the legs, which are not only numerically\nidentically bounded but also change in similar patterns. The inclusion of previous actions gives each\nsensory neuron a chance to infer the casual relationship between the input channel and the applied\nactions, which helps with the input identiﬁcation.\nFinally, in Equation 2 we could have combined QWq ∈RM×dq as a single learnable parameters\nmatrix, but we separate them for two reasons. First, by factoring into two matrices, we can reduce\nthe number of learnable parameters. Second, we ﬁnd that instead of making Qlearnable, using the\npositional encoding proposed in Transformer [73] encourages the attention mechanism to generate\ndistinct codes. Here we use the row indices in Qas the positions for encoding.\n4 Experiments\nWe experiment on several different RL environments to study various properties of permutation\ninvariant RL agents. Due to the nature of the underlying tasks, we will describe the different\narchitectures of the policy networks used and discuss various different training methods. However,\nthe AttentionNeuron layers in all agents are similar, so we ﬁrst describe the common setups. Hyper-\nparameters and other details for all experiments are summarized in Appendix A.4.\nFor non-vision continuous control tasks, the agent receives an observation vector ot ∈R|O|at time t.\nWe assign N = |O|sensory neurons for the tasks, each of which sees one element from the vector,\nhence ot[i] ∈R1,i = 1,··· ,|O|. We use an LSTM [ 40] as our fk(ot[i],at−1) to generate Keys,\nthe input size of which is 1 +|A|(2 for Cart-Pole and 9 for PyBullet Ant). A simple pass-through\nfunction f(x) =xserves as our fv(ot[i]), and σ(·) is tanh. For simplicity, we ﬁnd Wv = I works\nwell for the tasks, so the learnable components are the LSTM, Wq and Wk.\nFor vision based tasks, we gray-scale and stackk= 4consecutive RGB frames from the environment,\nand thus our agent observes ot ∈RH×W×k. ot is split into non-overlapping patches of size P = 6\nusing a sliding window, so each sensory neuron observesot[i] ∈R6×6×k. Here, fv(ot[i]) ﬂattens the\ndata and returns it, hence V(ot) returns a tensor of shape N ×dfv = N ×(6 ×6 ×4) =N ×144.\nDue to the high dimensionality for vision tasks, we do not use RNNs for fk, but instead use a simpler\nmethod to process each sensory input. fk(ot[i],at−1) takes the difference between consecutive\nframes (ot[i]), then ﬂattens the result, appends at−1, and returns the concatenated vector. K(ot,at−1)\nthus gives a tensor of shape N ×dfk = N ×[(6 ×6 ×3) +|A|] = N ×(108 +|A|) (111 for\nCarRacing and 114 for Atari Pong). We use softmax as the non-linear activation function σ(·), and\nwe apply layer normalization [3] to both the input patches and the output latent code.\n4.1 Cart-pole swing up\nWe examine Cart-pole swing up [ 28, 29, 34, 85] to ﬁrst illustrate our method, and also use it to\nprovide a clear analysis of the attention mechanism. We use CartPoleSwingUpHarder [27], a more\ndifﬁcult version of the task where the initial positions and velocities are highly randomized, leading\nto a higher variance of task scenarios. In the environment, the agent observes[x, ˙x,cos(θ),sin(θ), ˙θ],\noutputs a scalar action, and is rewarded at each step for getting xclose to 0 and cos(θ) close to 1.\nFigure 3: Interactive demo of CartPoleSwingUpHarder.In our web demo2, the user can shufﬂe the\norder of the 5 inputs at any time, and observe how the agent adapts to the new ordering of the inputs.\nWe use a two-layer neural network as our agent. The ﬁrst layer is an AttentionNeuron layer with\nN = 5sensory neurons and outputs mt ∈R16. A linear layer takes mt as input and outputs a scalar\naction. For comparison, we also trained an agent with a two-layer FNN policy with 16 hidden units.\nWe use direct policy search to train agents with CMA-ES [38], an evolution strategies (ES) method.\n2Interactive demos and videos of our results available at https://attentionneuron.github.io/\n5\nTable 2: Cart-pole Tests. For each experiment, we report the average score and the standard deviation\nfrom 1000 test episodes. Our agent is trained only in the environment with 5 sensory inputs.\n5 obs 5 obs (shufﬂed) 10 obs 5 obs + 5 noise\nFNN (trained with 5 obs) 593 ±433 38 ±120 N/A N/A\nFNN (trained with 10 obs) N/A N/A 593 ±433 137 ±242\nOurs (trained with 5 obs) 472 ±426 471 ±426 471 ±425 461 ±410\nOur agent can perform the task and balance the cart-pole from an initially random state. Its average\nscore is slightly lower than the baseline (See column 1 of Table 2) because each sensory neuron\nrequires some time steps in each episode to interpret the sensory input signal it receives. However, as\na trade-off for the performance sacriﬁce, our agent can retain its performance even when the input\nsensor array is randomly shufﬂed, which is not the case for an FNN policy (column 2). Moreover,\nalthough our agent is only trained in an environment with ﬁve inputs, it can accept an arbitrary\nnumber of inputs in any order without re-training3. We test our agent by duplicating the 5 inputs to\ngive the agent 10 observations (column 3). When we replace the 5 extra signals with white noises\nwith σ= 0.1 (column 4), we do not see a signiﬁcant drop in performance.\n16 messages\nt=1000 steps\nWithout Observation Shufﬂing With Observation Shufﬂing\nt=1000 steps\nFigure 4: Permutation invariant outputs. The output (16-dimensional global latent code) from the\nAttentionNeuron layer does not change when we input the sensor array as-is (left) or when we\nrandomly shufﬂe the array (right). Yellow represents higher values, and blue for lower values.\nThe AttentionNeuron layer should possess 2 properties to attain these: its output is permutation\ninvariant to its input, and its output carries task-relevant information. Figure 4 is a visual conﬁrmation\nof the permutation invariant property, whereby we plot the output messages from the layer and their\nchanges over time from two tests. Using the same random seed, we keep the observation as-is in the\nﬁrst test but we shufﬂe the order in the second. As the ﬁgure shows, the output messages are identical\nin the two roll-outs. We also perform a simple linear regression analysis on the outputs (based on the\nshufﬂed inputs) to recover the 5 inputs in their original order. Table 3 shows the R2 values4 from this\nanalysis, suggesting that some important indicators (e.g. ˙xand ˙θ) are well represented in the output.\nTable 3: Linear regression analysis on the output.\nFor each of the N = 5 sensory inputs we have\none linear regression model with mt ∈R16 as the\nexplanatory variables.\nx ˙x cos (θ) sin(θ) ˙θ\nR2 0.354 0 .620 0 .626 0 .233 0 .550\nTable 4: PyBullet Ant results.\nScore\nFNN (teacher) 2700 ±28\nFNN (shufﬂed) 232 ±112\nOurs (ES, shufﬂed) 2576 ±75\nOurs (BC, shufﬂed) 2034 ±948\nOurs (BC, shufﬂed, larger) 2579 ±457\n4.2 PyBullet Ant\nWhile direct policy search methods such as evolution strategies (ES) can train permutation invariant\nRL agents, oftentimes we already have access to pre-trained agents or recorded human data performing\nthe task at hand. Behavior cloning (BC) can allow us to convert an existing policy to a version that is\npermutation invariant with desirable properties associated with it.\nIn Table 4, we train a standard two-layer FNN policy to performAntBulletEnv-v0, a 3D locomotion\ntask in PyBullet [18], and use it as a teacher for BC. For comparison, we also train a two-layer agent\nwith AttentionNeuron for its ﬁrst layer. Both networks are trained with ES. Similar to CartPole, we\nexpect to see a small performance drop due to some time steps required for the agent to interpret\nan arbitrarily ordered observation space. We then collect data from the FNN teacher policy to train\npermutation invariant agents using BC. More details of the BC setup can be found in Appendix A.4.2.\n3Because our agent was not trained with normalization layers, we scaled the output from the AttentionNeuron\nlayer by 0.5 to account for the extra inputs in the last 2 experiments.\n4R2 measures the goodness-of-ﬁt of a model. An R2 of 1 implies that the regression perfectly ﬁts the data.\n6\nThe performance of the BC agent is lower than the one trained from scratch with ES, despite having\nthe identical architecture. This suggests that the inductive bias that comes with permutation invariance\nmay not match the original teacher network, so the small model used here may not be expressive\nenough to clone any teacher policy, resulting in a larger variance in performance. A beneﬁt of\ngradient-based BC, compared to RL, is that we can easily train larger networks to ﬁt the behavioral\ndata. We show that increasing the size of the subsequent layers for BC does enhance the performance.\nAs we will demonstrate next, BC is a useful technique for training permutation invariant agents in\nenvironments with high dimensional visual observations that may require larger networks.\n4.3 Atari Pong\nHere, we are interested in solving screen-shufﬂed versions of vision-based RL environments, where\neach observation frame is divided up into a grid of patches, and like a puzzle, the agent must process\nthe patches in a shufﬂed order to determine a course of action to take. A shufﬂed version of Atari\nPong [7] (See Figure 1, right pair) can be especially hard for humans to play when inductive biases\nfrom human priors [23] that expect a certain type of spatial structure is missing from the observations.\nBut rather than throwing away the spatial structure entirely from our solution, we ﬁnd that convolution\nneural network (CNN) policies work better than fully connected multi-layer perceptron (MLP) policies\nwhen trained with behavior cloning for Atari Pong. In this experiment, we reshape the output mt of\nthe AttentionNeuron layer from R400×32 to R20×20×32, a 2D grid of latent codes, and pass this 2D\ngrid into a CNN policy. This way, the role of the AttentionNeuron layer is to take a list of unordered\nobservation patches, and learn to construct a 2D grid representation of the inputs to be used by a\ndownstream policy that expects some form of spatial structure in the codes. Our permutation invariant\npolicy trained with BC can consistently reach a perfect score of 21, even with shufﬂed screens. The\ndetails of the CNN policy and BC training can be found in Appendix A.4.3.\nTest Occlusion Ratio\nTraining Occlusion Ratio\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n21 -20 -20 -21 -21 -21 -21 -21 -21 -21\n21 7 -5 -10 -16 -19 -21 -21 -21 -21\n21 11 5 -3 -8 -17 -20 -21 -20 -21\n21 12 4 -3 -9 -13 -20 -20 -21 -21\n21 17 8 0 0 -10 -16 -18 -21 -21\n21 15 17 15 15 10 -4 -14 -20 -21\n21 16 11 13 10 7 2 -10 -17 -21\n21 11 6 6 5 -1 -4 -8 -16 -21\n1 -13 -15 -15 -16 -17 -18 -13 -17 -20\n-12 -17 -20 -20 -19 -20 -19 -21 -21 -20\nFigure 5: Mean test scores in Atari Pong, and example of a randomly-shufﬂed occluded observation.\nIn the heat map, each value is the average score from 100 test episodes. For comparison, we show the\noriginal screen (left), and the agent’s observation (right). Discarded patches shown here in black.\nUnlike typical CNN policies, our agent can accept a subset of the screen, since the agent’s input is a\nvariable-length list of patches. It would thus be interesting to deliberately randomly discard a certain\npercentage of the patches and see how the agent reacts. The net effect of this experiment for humans\nis similar to being asked to play a partially occluded and shufﬂed version of Atari Pong (see Figure 5,\nright). During training via BC, we randomly remove a percentage of observation patches. In tests, we\nﬁx the randomly selected positions of patches to discard during an entire episode.\nWe present the results in a heat map in Figure 5 (left), where the y-axis shows the patches removed\nduring training and the x-axis gives the patch occlusion ratio in tests. The diagram shows clear\npatterns for interpretation. Looking horizontally along each row, the performance drops because the\nagent sees less of the screen which increases the difﬁculty. Interestingly, an agent trained at a high\nocclusion rate of 80% rarely wins against the Atari opponent, but once it is presented with the full set\nof patches during tests, it is able to achieve a fair result by making use of the additional information.\nTo gain insights into understanding the policy, we projected the AttentionNeuron layer’s output in a\ntest roll-out to 2D space using t-SNE [72]. In Figure 6, we highlight several groups and show their\n7\ncorresponding inputs. The AttentionNeuron layer clearly learned to cluster inputs that share similar\nfeatures. For example, the 3 sampled inputs in the blue group show the situation when the agent’s\npaddle moved toward the bottom of the screen and stayed there. Similarly, the orange group shows\nthe cases when the ball was not in sight, this happened right before/after a game started/ended. We\nbelieve these discriminative outputs enabled the downstream policy to accomplish the agent’s task.\nt\nFigure 6: 2D embedding of the AttentionNeuron layer’s output in a test episode.We highlight several\nrepresentative groups in the plot, and show the sampled inputs from them. For each group, we show\n3 corresponding inputs (rows) and unstack each to show the time dimension (columns).\n4.4 CarRacing\nWe ﬁnd that encouraging an agent to learn a coherent representation of a deliberately shufﬂed visual\nscene leads to agents with useful generalization properties. Such agents are still able to perform their\ntask even if the visual background of the environment changes, despite being trained only on a single\nstatic background. Out-of-domain generalization is an active area, and here, we combine our method\nwith AttentionAgent [71], a method that uses selective, hard-attention via a patch voting mechanism.\nAttentionAgents in [71] generalize well to several unseen visual environments where task irrelevant\nelements are modiﬁed, but fail to generalize to drastic background changes in a zero-shot setting.\nIn this experiment, we combine the permutation invariant AttentionNeuron layer with the policy\nnetwork used in AttentionAgent. As their hard-attention-based policy is non-differentiable, we train\nthe entire system using ES. We reshape the AttentionNeuron layer’s outputs to adapt for the policy\nnetwork. Speciﬁcally, we reshape the output message to mt ∈R32×32×16 such that it can be viewed\nas a 32-by-32 grid of 16 channels. The end result is a policy with two layers of attention: the ﬁrst layer\noutputs a latent code book to represent a shufﬂed scene, and the second layer performs hard attention\nto select the top K = 10codes from a 2D global latent code book. A detailed description of the\nselective hard attention policy from [71] and other training details can be found in Appendix A.4.4.\nWe ﬁrst train the agent in the CarRacing [46] environment, and report the average score from 100 test\nroll-outs in Table 5. As the ﬁrst column shows, our agent’s performance in the training environment\nis slightly lower but comparable to the baseline method, as expected. But because our agent accepts\nrandomly shufﬂed inputs, it is still able to navigate even when the patches are shufﬂed. Figure 1 (left\npair) gives an illustration, where the right screen is what our agent observes and the left is for human\nvisualization. A human will ﬁnd driving with the shufﬂed observation to be very difﬁcult because we\nare not constantly exposed to such tasks, just like in the “reverse bicycle” example mentioned earlier.\nWithout additional training or ﬁne-tuning, we test whether the agent can also navigate in four modiﬁed\nenvironments where the green grass background is replaced with various images (See Figure 7). As\nTable 5 (from column 2) shows, our agent generalizes well to most of the test environments with only\nmild performance drops while the baseline method fails to generalize. We suspect this is because\n8\nTable 5: CarRacing Test Results.\nTraining Env KOF Mt. Fuji Ukiyoe DS\nAttentionAgent [71] 901 ±54 −81 ±4 −57 ±38 −107 ±50 −56 ±23\nNetRand [48] 480 ±144 20 ±84 356 ±159 533 ±111 −27 ±34\nNetRand + AttentionAgent 885 ±64 −51 ±14 709 ±94 656 ±131 122 ±134\nOurs 801 ±147 646 ±189 503 ±152 661 ±140 171 ±146\nFigure 7: Screenshots of test environments. In each pair of images, the left is for human visualization\nand the right is what our agent sees. From the top left and in the clockwise order, the environments\nare “KOF”, “Mt. Fuji”, “DS” and “Ukiyoe”.\nthe AttentionNeuron layer has transformed the original RGB space to a useful hidden representation\n(represented by mt) that has eliminated task irrelevant information after observing and reasoning\nabout the sequences of (ot,at−1) during training, enabling the downstream hard attention policy to\nwork with an optimized abstract representation tailored for the policy, instead of raw RGB patches.\nWe also compare our method to NetRand [48], a simple but effective technique developed to perform\nsimilar generalization tasks. In the second row of Table 5 are the results of training NetRand on the\nbase CarRacing task. The CarRacing task proved to be too difﬁcult for NetRand, but despite a low\nperformance score of 480 in the training environment, the agent generalizes well to the “Mt. Fuji”\nand “Ukiyoe” modiﬁcations. In order to obtain a meaningful comparison, we combine NetRand\nwith AttentionAgent so that it can get close to a mean score of 900 on the base task. To do that,\nwe use NetRand as an input layer to the AttentionAgent policy network, and train the combination\nend-to-end using ES, which is consistent with our proposed method for this task. The combination\nattains a respectable mean score of 885, and as we can see in the third row of the above table, this\napproach also generalizes to a few of the unseen modiﬁcations of the CarRacing environment.\nOur score on the base CarRacing task is lower than NetRand, but this is expected since our agent\nrequires some amount of time steps to identify each of the inputs (which could be shufﬂed), while the\nNetRand and AttentionAgent agent will simply fail on the shufﬂed versions of CarRacing. Despite\nthis, our method still compares favorably on the generalization performance.\nWe visualize the attentions from the AttentionNeuron layer in Figure 8. In CarRacing, the agent has\nlearned to focus its attention (indicated by the highlighted patches) on the road boundaries which are\nintuitive to human beings and are critical to the task. Notice that the attended positions are consistent\nbefore and after the shufﬂing. More details about this visualization can be found in Appendix A.4.4.\n5 Discussion and Future Work\nIn this work, we investigate the properties of RL agents that can treat their observations as an arbitrarily\nordered, variable-length list of sensory inputs. By processing each input stream independently, and\nconsolidating the processed information using attention, our agents can still perform their tasks even\nif the ordering of the observations is randomly permuted several times during an episode, without\nexplicitly training for frequent re-shufﬂing (See Table 6).\n9\nFigure 8: Attention visualization. We highlight the observed patches that receive the most attention.\nLeft: Training environment. Right: Test environment with unseen background.\nTable 6: Reshufﬂe observations during a roll-out. In each test episode, we reshufﬂe the observations\nevery tsteps. For CartPole, we test for 1000 episodes because of its larger task variance. For the\nother tasks, we report mean and standard deviation from 100 tests. All environments except for Atari\nPong have a hard limit of 1000 time steps per episode. In Atari Pong, while the maximum length of\nan episode does not exist, we observed that an episode usually lasts for around 2500 time steps.\nCartPole PyBullet Ant Atari Pong CarRacing\nt= 25 107 ±146 2053 ±225 −20 ±1 732 ±161\nt= 50 163 ±198 2319 ±188 −20 ±2 772 ±163\nt= 100 242 ±254 2406 ±178 −10 ±12 768 ±167\nt= 200 318 ±310 2493 ±105 −2 ±17 774 ±182\nt= 500 407 ±380 2548 ±87 18 ±9 805 ±158\nNo reshufﬂe 472 ±426 2576 ±75 21 ±0 801 ±147\nApplications By presenting the agent with shufﬂed, and even incomplete observations, we encourage\nit to interpret the meaning of each local sensory input and how they relate to the global context. This\ncould be useful in many real world applications. For example, such policies could avoid errors due\nto cross-wiring or complex, dynamic input-output mappings when being deployed in real robots. A\nsimilar setup to the CartPole experiment with extra noisy channels could enable a system that receives\nthousands of noisy input channels to identify the small subset of channels with relevant information.\nLimitations For visual environments, patch size selection will affect both performance and comput-\ning complexity. We ﬁnd that patches of 6x6 pixels work well for our tasks, as did 4x4 pixels to some\nextent, but single pixel observations fail to work. Small patch sizes also result in a large attention\nmatrix which may be too costly to compute, unless approximations are used [14, 76, 79].\nAnother limitation is that the permutation invariant property applies only to the inputs, and not to the\noutputs. While the ordering of the observations can be shufﬂed, the ordering of the actions cannot.\nFor permutation invariant outputs to work, each action will require feedback from the environment,\nincluding reward information, in order to learn the relationship between itself and the environment.\nSocietal Impact Like most algorithms proposed in computer science and machine learning, our\nmethod can be applied in ways that will have potentially positive or negative impacts to society.\nWhile our small-scale, self-contained experiments study only the properties of agents that are PI to\ntheir observations, and we believe our results do not directly cause harm to society, the robustness\nand ﬂexible properties of the method may be of use for data-collection systems that receive data\nfrom a large variable number of sensors. For instance, one could apply permutation invariant sensory\nsystems to process data from millions of sensors for anomaly detection, which may lead to both\npositive or negative impacts, if used in applications such as large-scale sensor analysis for weather\nforecasting, or deployed in large-scale surveillance systems that could undermine our basic freedoms.\nOur work also provides a way to view the Transformer [73] through the lens of self-organizing neural\nnetworks. Transformers are known to have potentially negative societal impacts highlighted in studies\nabout possible data-leakage and privacy vulnerabilities [9], malicious misuse and issues concerning\nbias and fairness [6], and energy requirements for training them [68].\nFuture WorkAn interesting future direction is to also make the action layer have the same properties,\nand model each “motor neuron” as a module connected using attention. With such methods, it may be\npossible to train an agent with an arbitrary number of legs, or control robots with different morphology\nusing a single policy that is also provided with a reward signal as feedback. We look forward to\nseeing future works that include signals such as environmental rewards to train PI meta-learning\nagents that can adapt to not only changes in the observed environment, but also to changes to itself.\n10\nAcknowledgements\nThe authors would like to thank Rishabh Agarwal, Jie Tan, Yingtao Tian, Douglas Eck, Aleksandra\nFaust and our NeurIPS2021 reviewers for valuable discussion and feedback. The experiments in this\nwork were conducted using virtual machines provided by Google Cloud Platform.\nA Appendix\nA.1 Intuitive explanation of Self-Attention’s permutation invariant property\nHere, we provide a simple, non-rigorous example demonstrating permutation invariant property\nof the self-attention mechanism, to give some intuition to readers who may not be familiar with\nself-attention. For a detailed treatment, please refer to [47].\nAs mentioned in Section 3.1, in its simplest form, self-attention is described as:\ny= σ(QK⊤)V (3)\nwhere Q ∈ RNq×dq ,K ∈ RN×dq ,V ∈ RN×dv are the Query, Key and Value matrices and\nσ(·) is a non-linear function. In this work, Qis a ﬁxed matrix, and K,V are functions of the input\nX ∈RN×din where Nis the number of observation components (equivalent to the number of sensory\nneurons) and din is the dimension of each component. In most settings, K = XWk,V = XWv are\nlinear transformations, thus permuting X therefore is equivalent to permuting the rows in K,V .\nWe would like to show that the outputyis the same regardless of the ordering of the rows of K,V .\nFor simplicity, suppose N = 3,Nq = 2,dq = dv = 1, so that Q∈R2×1,K ∈R3×1,V ∈R3×1:\ny= σ\n((\nq1\nq2\n)\n(k1 k2 k3)\n)(v1\nv2\nv3\n)\n= σ\n((\nq1k1 q1k2 q1k3\nq2k1 q2k2 q2k3\n))(v1\nv2\nv3\n)\n=\n(\nσ(q1k1)v1 + σ(q1k2)v2 + σ(q1k3)v3\nσ(q2k1)v1 + σ(q2k2)v2 + σ(q2k3)v3\n)\n(4)\nThe output y∈R2×1 remains the same when the rows ofK,V are permuted from [1,2,3] to [3,1,2]:\ny= σ\n((\nq1\nq2\n)\n(k3 k1 k2)\n)(v3\nv1\nv2\n)\n= σ\n((\nq1k3 q1k1 q1k2\nq2k3 q2k1 q2k2\n))(v3\nv1\nv2\n)\n=\n(\nσ(q1k3)v3 + σ(q1k1)v1 + σ(q1k2)v2\nσ(q2k3)v3 + σ(q2k1)v1 + σ(q2k2)v2\n)\n(5)\nWe have highlighted the same terms with the same color in Equations 4 and 5 to show the results\nare indeed identical. In general, we have yij = ∑N\nb=1 σ\n(∑dq\na=1 QiaKba\n)\nVbj. Permuting the input\nis equivalent to permuting the indices b(i.e. rows of Kand V), which only affects the order of the\nouter summation and does not affect yij because summation is a permutation invariant operation.\nNotice that in the above example and the proof here we have assumed that σ(·) is an element-wise\noperation—a valid assumption since most activation functions satisfy this condition. 5\nAs discussed in Section 3.2, this formulation lets us convert an observation signal from the RL\nenvironment into a permutation invariant representation y. We can use this representation in place of\nthe actual observation as the input that goes into the downstream policy network of an RL agent.\n5Applying softmax to each row only brings scalar multipliers to each row and the proof still holds.\n11\nA.2 Hyper-parameters\nTable 1 contains the hyper-parameters used for each experiment. We did not employ exhaustive\nhyper-parameter tuning, but have simply selected parameters that can appropriately size our models\nto work with training methods such as evolution strategies, where the number of parameters cannot\nbe too large. As mentioned in the discussion section about the limitations, we tested a small range of\npatch sizes (1 pixel, 4 pixels, 6 pixels), and we ﬁnd that a patch size of 6x6 works well across tasks.\nA.3 Description of compute infrastructure used to conduct experiments\nFor all ES results, we train on Google Kubernetes Engines (GKE) with 256 CPUs (N1 series) for\neach job. The approximate time, including both training and periodic tests, for the jobs are: 3 days\n(CartPole), 5 days (PyBullet Ant ES) and 10 days (CarRacing). For BC results, we train with Google\nComputing Engines (GCE) on an instance that has one V100 GPU. The approximate time, including\nboth training and periodic tests, for the jobs are: 5 days (PyBullet Ant BC), 1 day (Atari Pong).\nA.4 Detailed setups for the experiments\nA.4.1 Training budget\nThe costs of ES training are summarized in the following table. A maximum of 20K generations\nis speciﬁed in the training, but stopped early if the performance converged. Each generation has\n256 ×16 = 4096episode rollouts, where 256 is the population size and 16 is the rollout repetitions.\nThe Pong permutation-invariant (PI) agents were trained using behavior cloning (BC) on a pre-trained\nPPO policy (which is not PI-capable), with 10M training steps.\nEnvironment CartPoleSwingUpHarder PyBullet Ant Atari Pong CarRacing\nNumber of Generations 14,000 12,000 - 4,000\nNote that we used the hyper-parameters (e.g., population size, rollout repetitions) that proved to\nwork on a wide range of tasks from past experience, and did not tune them for each experiment. In\nother words, these settings were not chosen with sample-efﬁciency in mind, but rather for learning a\nworking PI-capable policy using distributed computation within a reasonable wall-clock time budget.\nWe consider two possible approaches when we take sample-efﬁciency into consideration. In the\nexperiments, we have demonstrated that it is possible to simply use state-of-the-art RL algorithms\nto learn a non-PI policy, and then use BC to produce a PI version of the policy. The ﬁrst approach\nis thus to rely on the conventional RL algorithms to increase sample efﬁciency, which is a hot and\non-going topic in the area. On the other hand, we do think that an interesting future direction is to\nformulate environments where BC will fail in a PI setting, and that interactions with the environment\n(in a PI setting) is required to learn a PI policy. For instance, we have demonstrated in PyBullet Ant\nthat the BC method requires the cloned agent to have a much larger number of parameters compared\nto one trained with RL. This is where an investigation in sample-efﬁciency improvements in the RL\nalgorithm explicitly in the PI setting may be beneﬁcial.\nA.4.2 PyBullet Ant\nIn the PyBullet Ant experiment, we demonstrated that a pre-trained policy can be converted into\na permutation invariant one with behavior cloning (BC). We give detailed task description and\nexperimental setups here. In AntBulletEnv-v0, the agent controls an ant robot that has 8 joints\n(|A|= 8), and gets to see an observation vector that has base and joint states as well as foot-ground\ncontact information at each time step (|O|=28). The mission is to make the ant move along a pre-\ndeﬁned straight line as fast as possible. The teacher policy is a 2-layer FNN policy that has 32 hidden\nunits trained with ES. We collected data from 1000 test roll-outs, each of which lasted for 500 steps.\nDuring training, we add zero-mean Gaussian noise (σ= 0.03) to the previous actions. For the student\npolicy, We set up two networks. The ﬁrst policy is a 2-layered network that has the AttentionNeuron\nwith output size mt ∈R32 as its ﬁrst layer, followed by a fully-connected (FC) layer. The second,\nlarger policy is similar in architecture, but we added one more FC layer and expanded all hidden\nsize to 128 to increase its expressiveness. We train the students with a batch size of 64, an Adam\noptimizer of lr= 0.001 and we clip the gradient at maximum norm of 0.5.\n12\nA.4.3 Atari Pong\nIn the Atari game Pong, we append a deep CNN to the AttentionNeuron layer in our agent (student\npolicy). To be concrete, we reshape the AttentionNeuron’s output messagemt ∈R400×32 to mt ∈\nR20×20×32 and pass it to the trailing CNN: [Conv(in=32, out=64, kernel=4, stride=2), Conv(in=64,\nout=64, kernel=3, stride=1), FC(in=3136, out=512), FC(in=512, out=6)]. We use ReLU as the\nactivation functions in the CNN. We collect the stacked observations and the corresponding logits\noutput from a pre-trained PPO agent (teacher policy) from 1000 roll-outs, and we minimize the MSE\nloss between the student policy’s output and the teacher policy’s logits. The learning rate and norm\nclip are the same as the previous experiment, but we use a batch size of 256.\nFor the occluded Pong experiment, we randomly remove a certain percentage of the patches across\na training batch of stacked observation patches. In tests, we sample a patch mask to determine the\npositions to occlude at the beginning of the episode, and apply this mask throughout the episode.\nA.4.4 CarRacing\nIn AttentionAgent [71], the authors observed that the agent generalizes well if it is forced to make\ndecisions based on only a fraction of the available observations. Concretely, [71] proposed to segment\nthe input image into patches and let the patches vote for each other via a modiﬁed self-attention\nmechanism. The agent would then take into consideration only the top K = 10patches that have the\nmost votes and based on the coordinates of which an LSTM controller makes decisions. Because the\nvoting process involves sorting and pruning that are not differentiable, the agent is trained with ES. In\ntheir experiments, the authors demonstrated that the agent could navigate well not only in the training\nenvironment, but also zero-shot transfer to several modiﬁed environments.\nWe need only to reshape the AttentionNeuron layer’s outputs to adapt for AttentionAgent’s policy\nnetwork. Speciﬁcally, we reshape the output message mt ∈R1024×16 to mt ∈R32×32×16 such that\nit can be viewed as a 32-by-32 “image” of 16 channels. Then if we make AttentionAgent’s patch\nsegmentation size 1, the original patch voting becomes voting among the mt’s and thus the output\nﬁts perfectly into the policy network. Except for this patch size, we kept all hyper-parameters in\nAttentionAgent unchanged, we also used the same CMA-ES training hyper-parameters.\nAlthough the simple settings above allows our augmented agent to learn to drive and generalize\nto unseen background changes, we found the car jittered left and right through the courses. We\nsuspect this is because of the frame differential operation in our fk(ot,at−1). Speciﬁcally, even\nwhen the car is on a straight lane, constantly steering left and right allows fk(ot,at−1) to capture\nmore meaningful signals related to the changes of the road. To avoid such jittering behavior, we\nmake mt a rolling average of itself: mt = (1−α)mt + αmt−1,0 ≤α≤1. In our implementation\nα = g([ht−1,at−1]), where ht−1 is the hidden state from AttentionAgent’s LSTM controller and\nat−1 is the previous action. g(·) is a 2-layer FNN with 16 hidden units and a sigmoidoutput layer.\nWe analyzed the attention matrix in the AttentionNeuron layer and visualized the attended positions.\nTo be concrete, in CarRacing, the Query matrix has 1024 rows. Because we have 16 ×16 = 256\npatches, the Key matrix has 256 rows, we therefore have an attention matrix of size 1024 ×256. To\nplot attended patches, we select from each row in the attention matrix the patch that has the largest\nvalue after softmax, this gives us a vector of length 1024. This vector represents the patches each of\nthe 1024 output channels has considered to be the most important. 1024 is larger than the total patch\ncount, however there are duplications (i.e. multiple output channels have mostly focused on the same\npatches). The unique number turns out to be 10 ∼20 at each time step. We emphasize these patches\non the observation images to create an animation.\n13\nReferences\n[1] B. Amos, I. D. J. Rodriguez, J. Sacks, B. Boots, and J. Z. Kolter. Differentiable mpc for\nend-to-end planning and control. arXiv preprint arXiv:1810.13400, 2018.\n[2] J. Ba, G. Hinton, V . Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent\npast. arXiv preprint arXiv:1610.06258, 2016.\n[3] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[4] P. Bach-y Rita, C. C. Collins, F. A. Saunders, B. White, and L. Scadden. Vision substitution by\ntactile image projection. Nature, 221(5184):963–964, 1969.\n[5] P. Bach-y Rita and S. W. Kercel. Sensory substitution and the human–machine interface.Trends\nin cognitive sciences, 7(12):541–546, 2003.\n[6] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic\nparrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency, pages 610–623, 2021.\n[7] G. Brockman, V . Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.\nOpenai gym. arXiv preprint arXiv:1606.01540, 2016.\n[8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[9] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee, A. Roberts, T. Brown,\nD. Song, U. Erlingsson, et al. Extracting training data from large language models. arXiv\npreprint arXiv:2012.07805, 2020.\n[10] M. Chang, S. Kaushik, S. M. Weinberg, T. Grifﬁths, and S. Levine. Decentralized reinforcement\nlearning: Global decision-making via local economic transactions. In International Conference\non Machine Learning, pages 1437–1447. PMLR, 2020.\n[11] N. Cheney, R. MacCurdy, J. Clune, and H. Lipson. Unshackling evolution: evolving soft robots\nwith multiple materials and a powerful generative encoding. ACM SIGEVOlution, 7(1):11–23,\n2014.\n[12] J. Choi, B.-J. Lee, and B.-T. Zhang. Multi-focus attention network for efﬁcient deep reinforce-\nment learning. arXiv preprint arXiv:1712.04603, 2017.\n[13] B. Chopard and M. Droz. Cellular automata, volume 1. Springer, 1998.\n[14] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins,\nJ. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint\narXiv:2009.14794, 2020.\n[15] L. O. Chua and L. Yang. Cellular neural networks: Theory. IEEE Transactions on circuits and\nsystems, 35(10):1257–1272, 1988.\n[16] E. F. Codd. Cellular automata. Academic press, 1968.\n[17] J. Conway. The game of life. Scientiﬁc American, 223(4):4, 1970.\n[18] E. Coumans and Y . Bai. Pybullet, a python module for physics simulation for games, robotics\nand machine learning, 2016.\n[19] M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efﬁcient approach to policy\nsearch. In Proceedings of the 28th International Conference on machine learning (ICML-11),\npages 465–472. Citeseer, 2011.\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[22] Y . Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl2: Fast reinforce-\nment learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.\n14\n[23] R. Dubey, P. Agrawal, D. Pathak, T. L. Grifﬁths, and A. A. Efros. Investigating human priors\nfor playing video games. arXiv preprint arXiv:1802.10217, 2018.\n[24] D. Eagleman. Livewired: The inside story of the ever-changing brain. Canongate Books, 2020.\n[25] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis.\narXiv preprint arXiv:2012.09841, 2020.\n[26] V . Fortuin, M. Hüser, F. Locatello, H. Strathmann, and G. Rätsch. Som-vae: Interpretable\ndiscrete representation learning on time series. arXiv preprint arXiv:1806.02199, 2018.\n[27] D. Freeman, D. Ha, and L. Metz. Learning to predict without looking ahead: World models\nwithout forward prediction. In Advances in Neural Information Processing Systems, volume 32.\nCurran Associates, Inc., 2019. https://learningtopredict.github.io.\n[28] A. Gaier and D. Ha. Weight agnostic neural networks. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc., 2019. https://weightagnostic.\ngithub.io.\n[29] Y . Gal, R. McAllister, and C. E. Rasmussen. Improving PILCO with Bayesian neural network\ndynamics models. In Data-Efﬁcient Machine Learning workshop, ICML, Apr. 2016.\n[30] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman. Video action transformer network. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n244–253, 2019.\n[31] A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y . Bengio, and B. Schölkopf. Recurrent\nindependent mechanisms. In International Conference on Learning Representations, 2021.\n[32] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines.arXiv preprint arXiv:1410.5401,\n2014.\n[33] N. Guttenberg, N. Virgo, O. Witkowski, H. Aoki, and R. Kanai. Permutation-equivariant neural\nnetworks applied to dynamics prediction. arXiv preprint arXiv:1612.04530, 2016.\n[34] D. Ha. Evolving stable strategies. http://blog.otoro.net/, 2017.\n[35] D. Ha, A. Dai, and Q. V . Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n[36] D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in\nNeural Information Processing Systems 31, pages 2451–2463. Curran Associates, Inc., 2018.\nhttps://worldmodels.github.io.\n[37] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent\ndynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.\n[38] N. Hansen. The cma evolution strategy: a comparing review. Towards a new evolutionary\ncomputation, pages 75–102, 2006.\n[39] M. Haruno, D. M. Wolpert, and M. Kawato. Mosaic model for sensorimotor learning and\ncontrol. Neural computation, 13(10):2201–2220, 2001.\n[40] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–\n1780, 1997.\n[41] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001.\n[42] W. Huang, I. Mordatch, and D. Pathak. One policy to control them all: Shared modular policies\nfor agent-agnostic control. In Proceedings of the 37th International Conference on Machine\nLearning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 4455–4464. PMLR, 2020.\n[43] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira. Perceiver: General\nperception with iterative attention. arXiv preprint arXiv:2103.03206, 2021.\n[44] C. Joshi. Transformers are graph neural networks. The Gradient, 2020.\n[45] L. Kirsch and J. Schmidhuber. Meta learning backpropagation and improving it. arXiv preprint\narXiv:2012.14905, 2020.\n[46] O. Klimov. Carracing-v0, 2016.\n15\n[47] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh. Set transformer: A framework for\nattention-based permutation-invariant neural networks. In K. Chaudhuri and R. Salakhutdinov,\neditors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of\nProceedings of Machine Learning Research, pages 3744–3753. PMLR, 09–15 Jun 2019.\n[48] K. Lee, K. Lee, J. Shin, and H. Lee. Network randomization: A simple technique for general-\nization in deep reinforcement learning. arXiv preprint arXiv:1910.05396, 2019.\n[49] I.-J. Liu, R. A. Yeh, and A. G. Schwing. Pic: permutation invariant critic for multi-agent deep\nreinforcement learning. In Conference on Robot Learning, pages 590–602. PMLR, 2020.\n[50] M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural\nmachine translation. arXiv preprint arXiv:1508.04025, 2015.\n[51] T. Miconi, A. Rawal, J. Clune, and K. O. Stanley. Backpropamine: training self-modifying\nneural networks with differentiable neuromodulated plasticity.arXiv preprint arXiv:2002.10585,\n2020.\n[52] T. Miconi, K. Stanley, and J. Clune. Differentiable plasticity: training plastic neural networks\nwith backpropagation. In International Conference on Machine Learning, pages 3559–3568.\nPMLR, 2018.\n[53] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric\ndeep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 5115–5124, 2017.\n[54] A. Mordvintsev, E. Randazzo, E. Niklasson, and M. Levin. Growing neural cellular automata.\nDistill, 2020. https://distill.pub/2020/growing-ca.\n[55] A. Mott, D. Zoran, M. Chrzanowski, D. Wierstra, and D. J. Rezende. Towards interpretable\nreinforcement learning using attention augmented agents. arXiv preprint arXiv:1906.02500,\n2019.\n[56] E. Najarro and S. Risi. Meta-learning through hebbian plasticity in random networks. arXiv\npreprint arXiv:2007.02686, 2020.\n[57] J. Neumann, A. W. Burks, et al. Theory of self-reproducing automata , volume 1102024.\nUniversity of Illinois press Urbana, 1966.\n[58] S. Ohsawa, K. Akuzawa, T. Matsushima, G. Bezerra, Y . Iwasawa, H. Kajino, S. Takenaka, and\nY . Matsuo. Neuron as an agent, 2018.\n[59] J. Ott. Giving up control: Neurons as reinforcement learning agents. arXiv preprint\narXiv:2003.11642, 2020.\n[60] E. Randazzo, A. Mordvintsev, E. Niklasson, M. Levin, and S. Greydanus. Self-classifying mnist\ndigits. Distill, 2020. https://distill.pub/2020/selforg/mnist.\n[61] B. Sanchez-Lengeling, E. Reif, A. Pearce, and A. Wiltschko. A gentle introduction to graph\nneural networks. Distill, 2021. https://distill.pub/2021/gnn-intro.\n[62] M. Sandler, M. Vladymyrov, A. Zhmoginov, N. Miller, A. Jackson, T. Madams, et al. Meta-\nlearning bidirectional update rules. arXiv preprint arXiv:2104.04657, 2021.\n[63] D. Sandlin. The backwards brain bicycle: Un-doing understanding, 2019.\n[64] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent\nnetworks. Neural Computation, 4(1):131–139, 1992.\n[65] J. Schmidhuber. Reducing the ratio between learning complexity and number of time varying\nvariables in fully recurrent nets. In International Conference on Artiﬁcial Neural Networks,\npages 460–463. Springer, 1993.\n[66] J. Schmidhuber. A ‘self-referential’weight matrix. In International Conference on Artiﬁcial\nNeural Networks, pages 446–450. Springer, 1993.\n[67] I. Sorokin, A. Seleznev, M. Pavlov, A. Fedorov, and A. Ignateva. Deep attention recurrent\nq-network. arXiv preprint arXiv:1512.01693, 2015.\n[68] E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning\nin nlp. arXiv preprint arXiv:1906.02243, 2019.\n16\n[69] S. Sudhakaran, D. Grbic, S. Li, A. Katona, E. Najarro, C. Glanois, and S. Risi. Growing 3d arte-\nfacts and functional machines with neural cellular automata. arXiv preprint arXiv:2103.08737,\n2021.\n[70] C. Sun, F. Baradel, K. Murphy, and C. Schmid. Learning video representations using contrastive\nbidirectional transformer. arXiv preprint arXiv:1906.05743, 2019.\n[71] Y . Tang, D. Nguyen, and D. Ha. Neuroevolution of self-interpretable agents. InProceedings of\nthe Genetic and Evolutionary Computation Conference, 2020. https://attentionagent.\ngithub.io.\n[72] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning\nresearch, 9(11), 2008.\n[73] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc., 2017.\n[74] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention\nnetworks. arXiv preprint arXiv:1710.10903, 2017.\n[75] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Ku-\nmaran, and M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763,\n2016.\n[76] S. Wang, B. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear\ncomplexity. arXiv preprint arXiv:2006.04768, 2020.\n[77] S. Wolfram. Cellular automata as models of complexity. Nature, 311(5985):419–424, 1984.\n[78] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip. A comprehensive survey on graph\nneural networks. IEEE transactions on neural networks and learning systems, 2020.\n[79] Y . Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y . Li, and V . Singh. Nyströmformer: A\nnyström-based algorithm for approximating self-attention. arXiv preprint arXiv:2102.03902,\n2021.\n[80] S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim. Graph transformer networks. arXiv preprint\narXiv:1911.06455, 2019.\n[81] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, and A. Smola. Deep sets.\narXiv preprint arXiv:1703.06114, 2017.\n[82] V . Zambaldi, D. Raposo, A. Santoro, V . Bapst, Y . Li, I. Babuschkin, K. Tuyls, D. Reichert,\nT. Lillicrap, E. Lockhart, M. Shanahan, V . Langston, R. Pascanu, M. Botvinick, O. Vinyals,\nand P. Battaglia. Deep reinforcement learning with relational inductive biases. In International\nConference on Learning Representations, 2019.\n[83] D. Zhang, C. Choi, J. Kim, and Y . M. Kim. Learning to generate 3d shapes with generative\ncellular automata. arXiv preprint arXiv:2103.04130, 2021.\n[84] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y . Yeung. Gaan: Gated attention networks for\nlearning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.\n[85] X. Zuo. Pytorch implementation of improving pilco with bayesian neural network dynamics\nmodels, 2018. https://github.com/zuoxingdong/DeepPILCO.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7598284482955933
    },
    {
      "name": "Reinforcement learning",
      "score": 0.6167000532150269
    },
    {
      "name": "Artificial neural network",
      "score": 0.531105101108551
    },
    {
      "name": "Sensory system",
      "score": 0.5303516983985901
    },
    {
      "name": "Artificial intelligence",
      "score": 0.528583288192749
    },
    {
      "name": "Invariant (physics)",
      "score": 0.49932432174682617
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.47538745403289795
    },
    {
      "name": "Cellular automaton",
      "score": 0.4501594305038452
    },
    {
      "name": "Permutation (music)",
      "score": 0.448199599981308
    },
    {
      "name": "Generalization",
      "score": 0.41096171736717224
    },
    {
      "name": "Theoretical computer science",
      "score": 0.40461266040802
    },
    {
      "name": "Machine learning",
      "score": 0.35914507508277893
    },
    {
      "name": "Distributed computing",
      "score": 0.3352010250091553
    },
    {
      "name": "Neuroscience",
      "score": 0.17476439476013184
    },
    {
      "name": "Mathematics",
      "score": 0.12184399366378784
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    }
  ]
}