{
  "title": "Contextual Representation Learning beyond Masked Language Modeling",
  "url": "https://openalex.org/W4223490341",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2112143377",
      "name": "Zhiyi Fu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2952165275",
      "name": "Wangchunshu Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2114806282",
      "name": "Jingjing Xu",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2103794919",
      "name": "Hao Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3092747189",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W2980360762",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4249573750",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2945667196",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Currently, masked language modeling (e.g., BERT) is the prime choice to learn contextualized representations. Due to the pervasiveness, it naturally raises an interesting question: how do masked language models (MLMs) learn contextual representations? In this work, we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs. To address these problems, we propose TACO, a simple yet effective representation learning approach to directly model global semantics. To be specific, TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations. Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over MLM.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2701 - 2714\nMay 22-27, 2022c‚Éù2022 Association for Computational Linguistics\nContextual Representation Learning beyond Masked Language Modeling\nZhiyi Fu1‚àó‚Ä†, Wangchunshu Zhou2‚àó, Jingjing Xu3‚Ä†, Hao Zhou2, Lei Li3‚Ä†\n1Peking University 2ByteDance AI Lab 3University of California, Santa Barbara\nypfzy@pku.edu.cn\n{zhouwangchunshu.7, zhouhao.nlp}@bytedance.com\n{jingjingxu, leili}@cs.ucsb.edu\nAbstract\nHow do masked language models (MLMs)\nsuch as BERT learn contextual representa-\ntions? In this work, we analyze the learn-\ning dynamics of MLMs. We Ô¨Ånd that MLMs\nadopt sampled embeddings as anchors to es-\ntimate and inject contextual semantics to rep-\nresentations, which limits the efÔ¨Åciency and\neffectiveness of MLMs. To address these is-\nsues, we propose TACO, a simple yet effec-\ntive representation learning approach to di-\nrectly model global semantics. TACO extracts\nand aligns contextual semantics hidden in con-\ntextualized representations to encourage mod-\nels to attend global semantics when gener-\nating contextualized representations. Exper-\niments on the GLUE benchmark show that\nTACO achieves up to 5x speedup and up to\n1.2 points average improvement over existing\nMLMs. The code is available at https://\ngithub.com/FUZHIYI/TACO.\n1 Introduction\nIn the age of deep learning, the basis of repre-\nsentation learning is to learn distributional seman-\ntics. The target of distributional semantics can be\nsummed up in the so-called distributional hypoth-\nesis (Harris, 1954): Linguistic items with similar\ndistributions have similar meanings. To model\nsimilar meanings, traditional representation ap-\nproaches (Mikolov et al., 2013; Pennington et al.,\n2014) (e.g., Word2Vec) model distributional seman-\ntics by deÔ¨Åning tokens using context-independent\n(CI) dense vectors, i.e., word embeddings, and di-\nrectly aligning the representations of tokens in the\nsame context. Nowadays, pre-trained language\nmodels (PTMs) (Devlin et al., 2019; Radford et al.,\n2018; Qiu et al., 2020) expand static embeddings\ninto contextualized representations where each to-\nken has two kinds of representations: context-\nindependent embedding, and context-dependent\n‚àóEqual Contribution\n‚Ä†This work is done at ByteDance AI Lab.\nEnc.\nùë•!\n(#)\nùë•%\n(#)\nùë•&\n(#)\n‚àí =\nEmbedding\n=‚àí\n=‚àí\nùë•!\n(')\nùë•%\n(')\nùë•&\n(')\n‚àí =\n=‚àí\n=‚àí\n( , )\n( , ) Positive\nNegative\n‚áí\n‚áíEnc.\nLast HiddenEmbedding Global Anchor from ùë•(#) or ùë•('),,\nFigure 1: Illustration of the proposed token-alignment\ncontrastive objective. It extracts and aligns the global\nsemantics hidden in contextualized representations via\nthe gap between contextualized representations and cor-\nresponding static embeddings.\n(CD) dense representation that stems from its em-\nbedding and contains context information. Al-\nthough language modeling and representation learn-\ning have distinct targets, masked language model-\ning is still the prime choice to learn token represen-\ntations with access to large scale of raw texts (Pe-\nters et al., 2018; Devlin et al., 2019; Raffel et al.,\n2020; Brown et al., 2020).\nIt naturally raises a question: How do masked\nlanguage models learn contextual representa-\ntions? Following the widely-accepted understand-\ning (Wang and Isola, 2020), MLM optimizes two\nproperties, the alignment of contextualized repre-\nsentations with the static embeddings of masked\ntokens, and the uniformity of static embeddings in\nthe representation space. In the alignment property,\nsampled embeddings of masked tokens play as an\nanchor to align contextualized representations. We\nÔ¨Ånd that although such local anchor is essential\nto model local dependencies, the lack of global\nanchors brings several limitations. First, experi-\nments show that the learning of contextual repre-\nsentations is sensitive to embedding quality, which\nharms the efÔ¨Åciency of MLM at the early stage of\n2701\ntraining. Second, MLM typically masks multiple\ntarget words in a sentence, resulting in multiple em-\nbedding anchors in the same context. This pushes\ncontextualized representations into different clus-\nters and thus harms modeling global dependencies.\nTo address these challenges, we propose a novel\nToken-Alignment Contrastive Objective (TACO)\nto directly build global anchors. By combing lo-\ncal anchors and global anchors together, TACO\nachieves better performance and faster convergence\nthan MLM. Motivated by the widely-accepted be-\nlief that contextualized representation of a token\nshould be the mapping of its static embedding on\nthe contextual space given global information, we\npropose to directly align global information hid-\nden in contextualized representations at all posi-\ntions of a natural sentence to encourage models\nto attend same global semantics when generating\ncontextualized representations. Concerning possi-\nble relationships between context-dependent and\ncontext-independent representations, we adopt the\nsimplest probing method to extract global informa-\ntion via the gap between context-dependent and\ncontext-independent representations of a token for\nsimpliÔ¨Åcation, as shown in Figure 1. To be speciÔ¨Åc,\nwe deÔ¨Åne tokens in the same context (text span) as\npositive pairs and tokens in different contexts as\nnegative pairs, to encourage the global information\namong tokens within the same context to be more\nsimilar compared to that from different contexts.\nWe evaluate TACO on GLUE benchmark. Ex-\nperiment results show that TACO outperforms\nMLM with average 1.2 point improvement and 5x\nspeedup (in terms of sample efÔ¨Åciency) on BERT-\nsmall, and with average 0.9 point improvement and\n2x speedup on BERT-base.\nThe contributions of this paper are as follows.\n‚Ä¢ We analyze the limitation of MLM and pro-\npose a simple yet efÔ¨Åcient method TACO to\ndirectly model global semantics.\n‚Ä¢ Experiments show that TACO outperforms\nMLM with up to 1.2 point improvement and\nup to 5x speedup on GLUE benchmark.\n2 Understanding Language Modeling\n2.1 Objective Analysis\nThe key idea of MLM is to randomly replace a\nfew tokens in a sentence with the special token\n[MASK] and ask a neural network to recover the\noriginal tokens. Formally, we deÔ¨Åne a corrupted\nsentence as x1, x2, ¬∑¬∑¬∑, xL, and feed it into a\nTransformers encoder (Vaswani et al., 2017), the\nhidden states from the Ô¨Ånal layer are denoted as\nh1, h2, ¬∑¬∑¬∑, hL. We denote the embeddings of the\ncorresponding original tokens as e1, e2, ¬∑¬∑¬∑, eL.\nThe MLM objective can be formulated as:\nLMLM(x) =‚àí 1\n|M|\n‚àë\ni‚ààM\nlog exp(mi ¬∑ei)\n‚àë|V|\nk=1 exp(mi ¬∑ek)\n(1)\nwhere Mdenotes the set of masked tokens and |V|\nis the size of vocabulary V. mi is hidden state of\nthe last layer at the masked position, and can be\nregarded as a fusion of contextualized representa-\ntions of surrounding tokens. Following the widely-\naccepted understanding (Wang and Isola, 2020),\nEq.1 optimizes: (1) the alignment between contex-\ntualized representations of surrounding tokens and\nthe context-independent embedding of the target\ntoken and (2) the uniformity of representations in\nthe representation space.\nIn the alignment part, MLM relies on sampled\ncontextual-independent embeddings of masked to-\nkens as anchors to align contextualized represen-\ntations in contexts, as shown in Figure 2. Local\nanchor is the key feature of MLM. Therefore, the\nlearning of contextualized representations heavily\nrelies on embedding quality. In addition, multiple\nlocal anchors in a sentence tend to pushing con-\ntextualized representations of surrounding tokens\ncloser to different clusters, encouraging models to\nattend local dependencies where global semantics\nare neglected.\nContextualized RepresentationStatic EmbeddingTokenBoundaryAlignmentContextbank\ndeposit money with the bank\nthe east bankbankthe river\nFigure 2: Illustration of the MLM objective. At the\nalignment part, it uses static embedding of masked to-\nkens to align contextualized representations in the same\ncontext.\n2.2 Empirical Analysis\nTo verify our understanding, we conduct compre-\nhensive experiments to investigate: How does em-\nbedding anchor affect the learning dynamics of\nMLM? We re-train a BERT-small (Devlin et al.,\n2019) model with the MLM objective solely and\nanalyze the changes in its semantic space during\n2702\npre-training. The training details are described in\nAppendix A.\nContextualized representation evaluation. In\ngeneral, if contextualized representations are well\nlearned, the contextualized representations in a\nsame context will have higher similarity than that of\nin different contexts. Naturally, we use the gap be-\ntween intra-sentence similarity and inter-sentence\nsimilarity to evaluate contextual information in con-\ntextualized representations. We call this gap ascon-\ntextual score. The similarity can be evaluated via\nprobing methods like L2 distance, cosine similarity,\netc. We observe similar Ô¨Åndings on different prob-\ning methods and only report cosine similarity here\nfor simpliÔ¨Åcation. Figure 3(b) shows how contex-\ntual score changes during training. Other statistical\nresults are listed in Appendix A.\nEmbedding similarity evaluation. To observe\nhow sampled embeddings affect contextualized\nrepresentation learning, we evaluate the embed-\nding similarity between co-occurrent tokens. Moti-\nvated by the target that co-occurrent tokens should\nhave similar representations, we use the similar-\nity score calculated by cosine similarity between\nco-occurrent words labeled by humans (sampled\nfrom the WordSim353 dataset (Agirre et al., 2009))\nas the evaluation metric. Figure 3(a) shows how\nembedding similarity between co-occurrent tokens\nchanges during training.\nThe learning of contextualized representations\nheavily relies on embeddings similarity.As we\ncan see from Figure 3(a), the embedding similarity\nbetween co-occurrent tokens Ô¨Årst decreases during\nthe earliest stage of pre-training. It is because all\nembeddings are randomly initialized with the same\ndistribution and the uniformity feature in MLM\npushes tokens far away from each other, thus result-\ning in the decrease of embedding similarity. Mean-\nwhile, the contextual score, i.e., the gap between\nintra-context similarity and inter-context similar-\nity in Figure 3(b), does not increase at the earliest\nstage of training. It shows that random embeddings\nprovide little help to learn contextual semantics.\nDuring 5K-10K iterations, only when embeddings\nbecome closer, contextualized representations in\nthe same context begin to have similar features. At\nthis stage, the randomly sampled embeddings from\nthe same sentence, i.e., the same context, usually\nhave similar representations and thus MLM can\npush contextualized tokens closer to each other.\nFigure 3: The learning dynamics of MLM. The top Ô¨Åg-\nure (a) illustrates the similarity between embeddings of\nfrequently co-occurrent tokens (e.g., bank and money).\nThe bottom Ô¨Ågure (b) illustrates the similarity between\ncontextualized representation of tokens from the same\ncontext and different contexts. These Ô¨Ågures show an\nembedding bias problem where only the randomly se-\nlected target embeddings in MLM are similar, contex-\ntualized representations in the same context will be\naligned with similar features.\nWe further verify the effects of embedding qual-\nity in Figure 4. To this end, we train two BERT\nmodels whose embedding matrices are frozen and\ninitialized with the ones from different pre-training\nstage. We can see the model initialized with ran-\ndom embedding fails to teach contextualized repre-\nsentations to attend sentence meanings and repre-\nsentations from different contexts have almost the\nsame similarity. However, the variant with well-\ntrained but frozen embeddings learns to distinguish\ndifferent contexts early at around 4k steps. These\nstatistical observations verify that embedding an-\nchors bring the efÔ¨Åciency and effectiveness prob-\nlem.\nSurprisingly, embedding anchors reduce global\ncontextual information in contextualized repre-\nsentation at the later stage of training. Fig-\nure 3(a) shows that embedding similarity begins\nto drop after 8k steps. It shows that the model\nlearns the speciÔ¨Åc meanings of co-occurrent to-\nkens and begins to push them a little bit far away.\nSince MLM adopts local anchors, these local em-\n2703\nbeddings push contextualized representations into\ndifferent clusters. The contextual score begins to\ndecrease too. This phenomenon proves the embed-\nding bias problem where the learning of contextu-\nalized representations is decided by the selected\nembeddings where the global contextual semantics\nare neglected.\nFigure 4: The impact of embedding quality for the\nlearning of contextualized representations. We train\ntwo BERT-small variants from scratch, whose embed-\nding is either (a) randomly initialized and frozen or (b)\ncopied from normally pre-trained BERT at 250k steps\nand frozen.\n3 Proposed Approach: TACO\nTo address the challenges of MLM, we propose\na new method TACO to combine global anchors\nand local anchors. We Ô¨Årst introduce TC, a token-\nalignment contrastive loss which explicitly models\nglobal semantics in Section 3.1, and combine TC\nwith MLM to get the overall objective for training\nour TACO model in Section 3.2.\n3.1 Token-alignment Contrastive Loss\nTo model global semantics, the objective is ex-\npected to be capable of explicitly capturing infor-\nmation shared between contextualized representa-\ntion of tokens within the same context. Therefore,\na natural solution is to maximize the mutual infor-\nmation of contextual information hidden in contex-\ntualized representations in the same context. To\nextract shared contextual information, we Ô¨Årst de-\nÔ¨Åne a rule to generate contextual representations\nof tokens by combining embeddings and global\ninformation. Formally,\nhi = f(ei,g). (2)\nwhere f is a probing algorithm andei is the embed-\nding and g is the global bias of a concrete context.\nIn this paper, we adopt a straightforward probing\nmethod to get global information hidden in contex-\ntualized representations, where\ngi = hi ‚àíei. (3)\nGiven contextualized representations of an token\nx and its nearby tokens c in the same context, we\nuse gx and gc to represent global semantics hidden\nin these representations. The mutual information\nbetween the two global bias gx and gc is\nI(gx,gc) =\n‚àë\ngx,gc\np(gx,gc) logp(gx|gc)\np(gx) (4)\nAccording to van den Oord et al. 2019, the In-\nfoNCE loss serves as an estimator of mutual infor-\nmation of x and c:\nI(gx,gc) ‚â•log(K) ‚àíL(gx,gc) (5)\nwhere L(gx,gc) is deÔ¨Åned as:\nL(gx,gc) =‚àíE\nÔ£Æ\nÔ£∞log f(gx,gc)\nf(gx,gc) +‚àëK\nk=1 f(gx,gc‚àí\nk\n)\nÔ£π\nÔ£ª\n(6)\nwhere c‚àí\nk is the k-th negative sample ofx and Kis\nthe size of negative samples. Hence minimizing the\nobjective L(gx,gc) is equivalent to maximizing the\nlower bound on the mutual information I(gx,gc).\nThis objective contains two parts: positive pairs\nf(gx,gc) and negative pairsf(gx,gc‚àí\nk\n).\nPrevious study (Chen et al., 2020) has shown that\ncosine similarity with temperature performs well\nas the score function f in InfoNCE loss. Following\nthem, we take\nf(gx,gc) = 1\nœÑ\ngx ¬∑gc\n‚à•gx‚à•‚à•gc‚à• (7)\nwhere œÑ is the temperature hyper-parameter and\n‚à•¬∑‚à• is ‚Ñì2-norm function.\nContextualized representation: To get global\nbias gx and gc following Eq. 3, we adopt the\nwidely-used Transformer (Vaswani et al., 2017)\nas the encoder and take the last hidden states as\n2704\nthe contextualized representations hx and hc. For-\nmally, suppose a batch of sequences {si}where\ni‚àà{1,¬∑¬∑¬∑ ,N}. We feed it into the Transformer\nencoder to obtain contextualized representations,\nhi\n1, hi\n2, ¬∑¬∑¬∑, hi\n|si|where hi\nj ‚ààRd.\nPositive pairs: Given each token x, we randomly\nsample a positive sample c from nearby tokens in\nthe same context (sequence) within a window span\nwhere W is the window size.\nNegative pairs: Given each token x, we ran-\ndomly sample K tokens from other sequences in\nthis batch as negative samples c‚àí\nk .\nTo sum up, the Token-alignment Contrastive\n(TC) loss is applied to every token in a batch as:\nLTC = 1\nN\nN‚àë\ni=1\n1\n|si|\n|si|‚àë\nj=1\nL(gi\nj,gi\njc ) (8)\nwhere N is the number of sequences of this batch;\nsi is the i-th sequence; j and jc are tokens in si\nwhere jc Ã∏= j; gi is the global semantics hidden in\ncontextualized representation of token si. gi\nj and\ngi\njc are generated via:\ngi\nj = hi\nj ‚àíei\nj (9)\ngi\njc = hi\njc ‚àíei\njc (10)\nwhere hi\nj and ei\nj are the contextualized represen-\ntation and static embedding of the anchor token,\nrespectively. hi\njc and ei\njc are the contextualized\nrepresentation and static embedding of the sampled\npositive token in the same context.\n3.2 Training Objective\nAs described before, the token-alignment con-\ntrastive loss LTC is designed to model global de-\npendencies while MLM is able to capture local\ndependencies. Therefore, we can better model con-\ntextualized representations by combining the token-\nalignment contrastive loss LTC and the MLM loss\nto get our overall objective LTACO:\nLTACO = LTC + LMLM (11)\nWe implement it in a multi-task learning manner\nwhere all objectives are calculated within one for-\nward propagation, which only introduces negligible\nextra computations.\n4 Experiments\n4.1 Experimental Settings\nTraining Following BERT (Devlin et al., 2019),\nwe select the BooksCorpus (800M words after\nWordPiece tokenization) (Zhu et al., 2015) and En-\nglish Wikipedia (4B words) as pre-training corpus.\nWe pre-train two variants of BERT models: BERT-\nsmall and BERT-base. All models are equipped\nwith the vocabulary of size 30,522, trained with\n15% masked positions for MLM. The maximum\nsequence length is 256 and batch size is 1,280.\nWe adopt optimizer AdamW (Loshchilov and Hut-\nter, 2019) with learning rate 1e-4. All models are\ntrained until convergence. To be speciÔ¨Åc, the small\nmodel is trained up to 250k steps with a warm-up\nof 2.5k steps. The base model is trained up to 500k\nsteps with a warm-up of 10k steps. For TACO, we\nset the positive sample window size W to 5, the\nnegative sample number Kto 50, and the tempera-\nture parameter œÑ to 0.07 after a slight grid-search\nvia preliminary experiments. More pre-training\ndetails can be found in Appendix A.\nDuring Ô¨Åne-tuning models, we conduct a grid\nsearch over batch sizes of {16, 32, 64, 128}, learn-\ning rates of {1e-5, 2e-5, 3e-5, 5e-5}, and training\nepochs of {4, 6} with an Adam optimizer (Kingma\nand Ba, 2015). We use the open-source pack-\nages for implementation, including HuggingFace\nDatasets1 and Transformers2. All the experiments\nare conducted on 16 GPU chips (32 GB V100).\nEvaluation We evaluate methods on the GLUE\nbenchmark (Wang et al., 2019). SpeciÔ¨Åcally, we\ntest on Microsoft Research Paraphrase Matching\n(MRPC) (Dolan and Brockett, 2005), Quora Ques-\ntion Pairs (QQP)3 and STS-B (Conneau and Kiela,\n2018) for Paraphrase Similarity Matching; Stan-\nford Sentiment Treebank (SST-2) (Socher et al.,\n2013) for Sentiment ClassiÔ¨Åcation; Multi-Genre\nNatural Language Inference Matched (MNLI-m),\nMulti-Genre Natural Language Inference Mis-\nmatched (MNLI-mm) (Williams et al., 2018), Ques-\ntion Natural Language Inference (QNLI) (Ra-\njpurkar et al., 2016) and Recognizing Textual En-\ntailment (RTE) (Wang et al., 2019) for the Natural\nLanguage Inference (NLI) task; The Corpus of\nLinguistic Acceptability (CoLA) (Warstadt et al.,\n2019) for Linguistic Acceptability.\nFollowing Devlin et al. (2019), we exclude\nWNLI (Levesque, 2011). We report F1 scores for\nQQP and MRPC, Spearman correlations for STS-\nB, and accuracy scores for the other tasks. For\nevaluation results on validation sets, we report the\n1https://github.com/huggingface/datasets\n2https://github.com/huggingface/transformers\n3https://www.quora.com/q/quoradata/\nFirst-Quora-Dataset-Release-Question-Pairs\n2705\nApproach MNLI(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\nValidation Set\nMLM-250k 76.9 / 77.4 85.7 86.2 89.0 28.8 85.6 85.9 59.6 75.0\nTACO-50k 76.7 / 76.8 85.2 85.0 87.5 31.3 85.6 87.1 59.1 74.9\nTACO-250k 77.9 / 78.4 86.1 86.5 88.9 34.2 86.1 88.1 59.5 76.2\nTest Set MLM-250k 77.5 / 76.5 68.2 85.6 89.3 27.9 76.9 82.6 60.6 71.7\nTACO-250k 78.0 / 76.9 67.6 86.3 89.5 31.2 77.8 84.4 58.4 72.2\nTable 1: GLUE results on BERT-small. For validation results, we run 4 experiments with different seeds for each\ntask and report the average score. For test results, we report the test scores of the checkpoint performing best on\nvalidation sets. TACO outperforms MLM with 1.2 point improvement and 5√óspeedup on validations sets. On test\nsets, TACO also obtains better results on 6 out of 8 tasks.\naverage score of 4 Ô¨Åne-tunings with different ran-\ndom seeds. For results on test sets, we select the\nbest model on the validation set to evaluate.\nBaselines We mainly compare TACO with MLM\non BERT-small and BERT-base models. In ad-\ndition, we also compare TACO with related con-\ntrastive methods: a sentence-level contrastive\nmethod BERT-NCE and a span-based contrastive\nlearning method INFOWORD, both from Kong\net al. (2020). We directly compare TACO with the\nresults reported in their paper.\n4.2 Results on BERT-Small\nTable 1 and Figure 5 show the results of TACO\non BERT-small. As we can see, compared with\nMLM with 250k training steps ( convergence steps),\nTACO achieves comparable performance with only\n1/5 computation budget. By modeling global de-\npendencies, TACO can signiÔ¨Åcantly improve the\nefÔ¨Åciency of contextualized representation learning.\nIn addition, when pre-trained with the same steps,\nTACO outperforms MLM with 1.2 average score\nimprovement on the validation set.\nIn addition to convergence, we also compare\nTACO and MLM on fewer training data. The re-\nsults are shown in Table 2. We sample 4 tasks with\nthe largest amount of training data for evaluation.\nAs we can see, TACO trained on 25% data can\nachieve competitive results with MLM trained on\nfull data. These results also verify the data efÔ¨Å-\nciency of our method, TACO.\n4.3 Results on BERT-Base\nWe also compare TACO with MLM on base-sized\nmodels, which are the most commonly used mod-\nels according to the download data from Hugging-\nface4 (Wolf et al., 2020). First, from Table 3,\nwe can see that TACO consistently outperforms\n4https://huggingface.co/models\nApproach MNLI QQP QNLI SST-2 Avg.\nMLM-25% 77.8 85.7 85.8 87.2 84.1\nMLM-100% 76.9 85.7 86.2 89.0 84.5\nTACO-25% 77.8 85.7 86.1 88.4 84.5\nTACO-100% 77.9 86.1 86.5 88.9 84.9\nTable 2: TACO pre-trained on a quarter of data\nachieves competitive downstream results with MLM\npre-trained on full data. All results are reported on\nGLUE validation sets with BERT-small. Here we sam-\nple 4 tasks with the largest amount of training data.\nFigure 5: Average GLUE score during pre-training.\nAll results are reported on validation sets with BERT-\nsmall. TACO achieves better results and 5 √óspeedup\nthan MLM.\nMLM under all pre-training computation budgets.\nNotably, TACO-250kachieves comparable perfor-\nmance with MLM-500k, which saves 2x computa-\ntions. Similar results are observed on TACO-100k\nand BERT-250k. These results demonstrate that\nTACO can achieve better acceleration over MLM.\nIt is also a signiÔ¨Åcant improvement compared to\nprevious methods (Gong et al., 2019) focusing on\naccelerating BERT but only with slight speedups.\nIn addition, as shown in Table 4, TACO achieves\ncompetitive results compared to BERT-NCE and\nINFOWORD, two similar contrastive methods.\n2706\nApproach MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\nMLM-100k 80.7 86.4 89.3 90.5 47.4 86.0 85.0 56.6 77.7\nMLM-250k 83.0 87.4 90.4 91.8 48.6 87.1 87.5 57.8 79.2\nMLM-500k 84.2 87.9 91.1 92.1 51.1 87.9 89.8 63.4 80.9\nTACO-100k 81.5 87.4 89.4 90.3 46.4 87.2 87.8 62.8 79.1\nTACO-250k 83.8 87.9 90.2 91.4 50.7 87.9 89.3 63.5 80.6\nTACO-500k 84.6 88.1 90.8 92.3 53.4 88.5 90.7 66.3 81.8\nTable 3: GLUE results on BERT-base. All results are reported on validation sets. We run 6 experiments with\ndifferent hyper-parameter combinations (including random seeds) for each task and report the average score. The\nMNLI-matched score is reported here. TACO outperforms MLM with 0.9 point improvement and 2√óspeedup.\nApproach MNLI(m/mm) QQP QNLI SST-2 Avg.\nBERT-NCE 83.2 / 83.0 70.5 90.9 93.0 84.1\nINFOWORD 83.7 / 82.4 71.0 91.4 92.5 84.2\nTACO 84.5 / 83.5 71.7 91.6 93.2 84.9\nTable 4: TACO achieves the best among contrastive-based methods. All results are reported on GLUE test sets\nwith BERT-base. For each task, we report test results of the checkpoint performing best on validation sets.\n5 Discussion\n5.1 TACO and MLM\nTo better understand how TACO works, we con-\nduct a quantitative comparison on the learning dy-\nnamic for BERT and TACO. Similar to Section 2.2,\nwe plot the Cosine similarity among contextual-\nized representations of tokens in the same context\n(intra-context) and different contexts (inter-context)\nin Figure 6. We Ô¨Ånd that the learning dynamic\nof TACO signiÔ¨Åcantly differs from that of MLM.\nSpeciÔ¨Åcally, for TACO, the intra-context represen-\ntation similarity remains high and the gap between\nintra-context similarity and inter-context similarity\nremains large at the later stage of training. This con-\nÔ¨Årms that TACO can better fulÔ¨Åll global semantics,\nwhich may contribute to the superior downstream\nperformance.\n5.2 Ablation Study\nTACO is implemented as a token-level contrastive\n(TC) loss along with the MLM loss. Therefore, the\nimprovement of TACO might come from two as-\npects, including 1) denser supervision signals from\nthe all-token objective and 2) the beneÔ¨Åts of the\ncontrastive loss to strengthen global dependencies.\nIt is helpful to Ô¨Ågure out which factor is more im-\nportant. To this end, we design two variants for\nablation. One is a concentrated TACO, where the\ncontrastive loss is built on the 15% masked posi-\ntions only, keeping the same density of supervision\nsignal with MLM. The other is an extended MLM,\nwhere not only 15% masked positions are asked\nto predict the original token, so do the rest 85%\nunmasked positions. The extended MLM has the\nsame dense supervision with TACO but loses the\nbeneÔ¨Åts of modeling the global dependencies. The\nresults on small models are shown in Figure 6.\nAs we can see, the performance of TACO de-\ncreases if we sample a part of token positions to\nimplement TC objectives. It shows that more su-\npervision signals beneÔ¨Åt the Ô¨Ånal performance of\nTACO. However, simply adding more supervision\nsignals by predicting unmasked tokens does not\nhelp MLM too much. Even equipped with the ex-\ntra 85% token prediction (TP) loss, MLM+TP does\nnot show signiÔ¨Åcant improvements and it is notice-\nable that the performance of MLM+TP starts to\ndrop after 150k steps. This further conÔ¨Årms the\neffectiveness of TC loss by strengthening global\ndependencies.\n6 Related Work\n6.1 Language Representation Learning\nClassic language representation learning meth-\nods (Mikolov et al., 2013; Pennington et al., 2014)\naims to learn context-independent representation\nof words, i.e., word embeddings. They gener-\nally follow the distributional hypothesis (Harris,\n1954). Recently, the pre-training then Ô¨Åne-tuning\nparadigm has become a common practice in NLP\nbecause of the success of pre-trained language\nmodels like BERT (Devlin et al., 2019). Context-\ndependent (or contextualized) representations are\nthe basic characteristic of these methods. Many\n2707\n0 5k 10k 15k 20k\npretrain checkpoints\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMLM intra-context similarity\nMLM inter-context similarity\nTACO intra-context similarity\nTACO inter-context similarity\n0 50k 100k 150k 200k 250k\npretrain checkpoints\n0.68\n0.70\n0.72\n0.74\n0.76average GLUE score\n 15% MLM + 100% TC (TACO)\n15% MLM + 15% TC\n15% MLM + 85% TP\n15% MLM\nFigure 6: The left Ô¨Ågure (a) shows the intra-context similarity and inter-context similarity change during pre-\ntraining. The right Ô¨Ågure (b) shows two ablations of TACO: a concentrated TACO (15% MLM + 15% TC), where\nthe MLM loss and the TC loss are both built on the same 15% masked positions, and an extended MLM (15%\nMLM + 85% TP), which masks 15% positions but predict original tokens on all positions.\nexisting contextualized models are based on the\nmasked language modeling objective, which ran-\ndomly masks a portion of tokens in a text sequence\nand trains the model to recover the masked tokens.\nMany previous studies prove that pre-training with\nthe MLM objective helps the models learn syntac-\ntical and semantic knowledge (Clark et al., 2019).\nThere have been numerous extensions to MLM. For\nexample, XLNet (Yang et al., 2019) introduced the\npermutated language modeling objective, which\npredicts the words one by one in a permutated or-\nder. BART (Lewis et al., 2020) and T5 (Raffel et al.,\n2020) investigated several denoising objectives and\npre-trained an encoder-decoder architecture with\nthe mask span inÔ¨Ålling objective. In this work, we\nfocus on the key MLM objective and aim to explore\nhow MLM objective helps learn contextualized rep-\nresentation.\n6.2 Contrastive-based SSL\nApart from denoising-based objectives, contrastive\nlearning is another promising way to obtain self-\nsupervision. In contrastive-based self-supervised\nlearning, the models are asked to distinguish the\npositive samples from the negative ones for a given\nanchor. Contrastive-based SSL method was Ô¨Årst\nintroduced in NLP for efÔ¨Åcient learning of word\nrepresentations by negative sampling, i.e., SGNS\n(Word2Vec (Mikolov et al., 2013)). Later, sim-\nilar ideas were brought into CV Ô¨Åeld for learn-\ning image representation and got prevalent, such\nas MoCo (He et al., 2020), SimCLR (Chen et al.,\n2020), BYOL (Caron et al., 2020), etc.\nIn the recent two years, there have been many\nstudies targeting at reviving contrastive learning\nfor contextual representation learning in NLP. For\ninstance, CERT (Fang et al., 2020) utilized back-\ntranslation to generate positive pairs. CAPT (Luo\net al., 2020) applied masks to the original sentence\nand considered the masked sentence and its origi-\nnal version as the positive pair. DeCLUTR (Giorgi\net al., 2020) samples nearby even overlapping spans\nas positive pairs. INFOWORD (Kong et al., 2020)\ntreated two complementary parts of a sentence as\nthe positive pair. However, the aforementioned\nmethods mainly focus on sentence-level or span-\nlevel contrast and may not provide dense self-\nsupervision to improve efÔ¨Åciency. Unlike these\napproaches, TACO regards the global semantics\nhidden in contextualized token representations as\nthe positive pair. The token-level contrastive loss\ncan be built on all input tokens, which provides a\ndense self-supervised signal.\nAnother related work is ELECTRA (Clark et al.,\n2020). ELECTRA samples machine-generated to-\nkens from a separate generator and trains the main\nmodel to discriminate between machine-generated\ntokens and original tokens. ELECTRA implicitly\ntreats the fake tokens as negative samples of the\ncontext, and the unchanged tokens as positive sam-\nples. Unlike this method, TACO does not require\narchitectural modiÔ¨Åcations and can serve as a plug-\nand-play auxiliary objective, largely improving pre-\ntraining efÔ¨Åciency.\n7 Conclusion\nIn this paper, we propose a simple yet effective ob-\njective to learn contextualized representation. Tak-\n2708\ning MLM as an example, we investigate whether\nand how current language model pre-training ob-\njectives learn contextualized representation. We\nÔ¨Ånd that the MLM objective mainly focuses on\nlocal anchors to align contextualized representa-\ntions, which harms global dependencies modeling\ndue to an ‚Äúembedding bias‚Äù problem. Motivated\nby these problems, we propose TACO to directly\nmodel global semantics. It can be easily combined\nwith existing LM objectives. By combining lo-\ncal and global anchors, TACO achieves up to 5√ó\nspeedups and up to 1.2 improvements on GLUE\nscore. This demonstrates the potential of TACO\nto serve as a plug-and-play approach to improve\ncontextualized representation learning.\nAcknowledgement\nWe thank the anonymous reviewers for their help-\nful feedback. We also thank the colleagues from\nByteDance AI Lab for their suggestions on our\nexperiment designing and paper writing.\nReferences\nEneko Agirre, Enrique Alfonseca, Keith Hall, Jana\nKravalova, Marius Pasca, and Aitor Soroa. 2009. A\nstudy on similarity and relatedness using distribu-\ntional and wordnet-based approaches.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya\nGoyal, Piotr Bojanowski, and Armand Joulin. 2020.\nUnsupervised learning of visual features by contrast-\ning cluster assignments. In Advances in Neural\nInformation Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey E. Hinton. 2020. A simple framework\nfor contrastive learning of visual representations. In\nProceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 1597‚Äì1607. PMLR.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of bert‚Äôs attention. CoRR,\nabs/1906.04341.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. In LREC.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171‚Äì4186. Association for Computa-\ntional Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn IWP@IJCNLP.\nHongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan\nDing, and Pengtao Xie. 2020. Cert: Contrastive\nself-supervised learning for language understanding.\narXiv preprint arXiv:2005.12766.\nJohn M Giorgi, Osvald Nitski, Gary D Bader, and\nBo Wang. 2020. Declutr: Deep contrastive learn-\ning for unsupervised textual representations. arXiv\npreprint arXiv:2006.03659.\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2019. EfÔ¨Åcient training\nof BERT by progressively stacking. In ICML, vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 2337‚Äì2346. PMLR.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146‚Äì162.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 9726‚Äì9735. Computer Vi-\nsion Foundation / IEEE.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\n2709\nLingpeng Kong, Cyprien de Masson d‚ÄôAutume, Lei Yu,\nWang Ling, Zihang Dai, and Dani Yogatama. 2020.\nA mutual information maximization perspective of\nlanguage representation learning. In International\nConference on Learning Representations.\nHector J. Levesque. 2011. The winograd schema chal-\nlenge. In AAAI Spring Symposium: Logical Formal-\nizations of Commonsense Reasoning.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871‚Äì7880. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nFuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng\nRen, and Xu Sun. 2020. Capt: Contrastive pre-\ntraining for learningdenoised sequence representa-\ntions. arXiv preprint arXiv:2010.06351.\nTom√°s Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed rep-\nresentations of words and phrases and their com-\npositionality. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States, pages 3111‚Äì\n3119.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2014, October 25-29, 2014,\nDoha, Qatar, A meeting of SIGDAT, a Special Inter-\nest Group of the ACL, pages 1532‚Äì1543. ACL.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227‚Äì2237. Association for\nComputational Linguistics.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nCoRR, abs/2003.08271.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In EMNLP.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2019. Representation learning with contrastive pre-\ndictive coding.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nTongzhou Wang and Phillip Isola. 2020. Understand-\ning contrastive representation learning through align-\nment and uniformity on the hypersphere. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 9929‚Äì9939. PMLR.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTACL.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL-HLT.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\n2710\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, EMNLP 2020 - Demos, On-\nline, November 16-20, 2020, pages 38‚Äì45. Associa-\ntion for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 5754‚Äì5764.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19‚Äì\n27.\n2711\nA Experiment Details\nA.1 Pre-training Hyper-parameters\nAll pre-training approaches involved in experi-\nments use the same pre-training hyper-parameters\nbut do not include BERT-NCE and INFOWORD.\nResults of BERT-NCE and INFOWORD are di-\nrectly cited from the original paper (Kong et al.,\n2020). Following Liu et al. (2019), we do not use\nthe next sentence prediction (NSP) objective and\nuse dynamic masking for MLM with a 15% mask\nratio, where the masked positions are decided on\nthe Ô¨Çy.\nTACO introduces three extra hyper-parameters,\nincluding negative sample size K, positive sample\nwindow size W and temperature œÑ. We set the tem-\nperature œÑ as a small value, 0.07, following Fang\net al. (2020). By searching for the best K out of\n{10, 50} and W out of {3, 5, 10, 50} on the small\nTACO model, we found that TACO withK=50 and\nW=5 performs best, so we also apply these hyper-\nparameter choices for base-sized TACO. The full\nset of pre-training hyper-parameters are listed in\nTable 5. Actually, TACO outperforms MLM under\nmost cases in our preliminary experiments. How-\never, we still also Ô¨Ånd some extreme cases which\nmight harm the effectiveness of TACO. If the size\nof negative samples K is too small, e.g., smaller\nthan 10, the performance of TACO degenerates\nnearly to the level of BERT baseline. Similar con-\nclusions are also mentioned in related works (He\net al., 2020; Chen et al., 2020). Also, if the positive\nwindow size W is too large, e.g., bigger than 50,\nthe performance of TACO degrades, too. We sus-\npect the over-large positive window brings more\nfalse-positive samples, which makes the sequence\nmeaning ambiguous, thus harms the performance.\nA.2 Fine-tuning Details\nFor small-sized models, we Ô¨Åne-tune all saved\ncheckpoints (5k, 10k, 20k, 30k, 40k, 50k, 100k,\n150k, 200k, 250k-step) of different pre-trained\nmodels (TACO and its ablations) with the same\nhyper-parameters on each task. Considering the\nlarge amount of pre-training checkpoints, we just\nadopt the default Ô¨Åne-tuning hyper-parameters and\nrepeat Ô¨Åne-tuning 4 times with different random\nseeds. Then the best performed Ô¨Åne-tuned models\non validation sets are used for testing. This setting\nhelps make a fair comparison among models and\navoids a large amount of grid-search runs. The task-\nspeciÔ¨Åc hyper-parameters for small-sized models\nPre-training Hyper-parameters Small Base\nParameters Shared by\nAll Approaches\nNumber of Layers 4 12\nHidden Size 512 768\nHidden Layer Activation Function gelu gelu\nFFN Inner Hidden Size 2,048 3,072\nAttention Heads 8 12\nAttention Head Size 64 64\nEmbedding Size 512 768\nV ocab Size 30,522 30,522\nMax Position Embeddings 512 512\nMax Sequence Length 256 256\nAttention Dropout 0.1 0.1\nDropout 0.1 0.1\nInitializer Range 0.02 0.02\nLearning Rate Decay Linear Linear\nLearning Rate 1e-4 1e-4\nMax Gradient Norm 1.0 1.0\nAdam œµ 1e-8 1e-8\nAdam Œ≤1 0.9 0.9\nAdam Œ≤2 0.999 0.999\nWeight Decay 0.01 0.01\nBatch Size 1,280 1,280\nTrain Steps 250k 500k\nWarm-up Steps 2,500 10,000\nFP16 True True\nMask Percentage 15 15\nTACO\nOnly\nNegative Sample Size K 50 50\nPositive Sample Window Size W 5 5\nTemperature Parameter œÑ 0.07 0.07\nTable 5: Hyper-parameters during pre-training.\n2712\nFine-tuning Hyper-parameters Small/Base\nParameters Shared by\nAll Models\nMax Sequence Length 128\nAttention Dropout 0.1\nDropout 0.1\nInitializer Range 0.02\nLearning Rate Decay Linear\nMax Gradient Norm 1.0\nAdam œµ 1e-8\nAdam Œ≤1 0.9\nAdam Œ≤2 0.999\nWeight Decay 0.0\nFP16 False\nTable 6: Hyper-parameters during Ô¨Åne-tuning.\nTask Learning Rate Batch Size Train Epochs Warm-up Steps\nMNLI 5e-5 64 6 2,000\nQQP 5e-5 64 6 2,000\nQNLI 5e-5 64 4 200\nSST-2 5e-5 64 4 200\nCoLA 5e-5 32 4 100\nSTS-B 5e-5 32 4 100\nMRPC 5e-5 32 4 100\nRTE 5e-5 32 4 100\nTable 7: Task-speciÔ¨Åc hyper-parameters for small models during Ô¨Åne-tuning.\nare listed in Table 7. The general Ô¨Åne-tuning hyper-\nparameters are listed in Table 6.\nFor base-sized models, we save checkpoints at\n100k, 250k, and 500k steps, respectively. During\nÔ¨Åne-tuning, we also conduct multiple Ô¨Åne-tuning\nruns with different task-speciÔ¨Åc hyper-parameter\ncombinations as shown in Table 8. Concretely, we\nrandomly sample 6 different hyper-parameter com-\nbinations and report the average score for validation\nresults. Then we select the best-performing run of\n500k-step checkpoints (converged) for testing.\nA.3 Statistic Details\nEmbedding Similarity We calculate cosine sim-\nilarity of 20 randomly sampled pairs of fre-\nquently co-occurrent words from the WordSim353\ndataset (Agirre et al., 2009) labeled by human an-\nnotators to plot the average similarity curve in Fig-\nure 3(b). Corresponding embeddings are obtained\nfrom the embedding layer of the BERT model and\nvariant models mentioned in Section 2.2.\nIntra-/Inter-context Similarity For every token\nwi in the corpus, we randomly sample a positive\ntoken wjÃ∏=i within the same context (sentence) and\nanother token wk from other sentences. As men-\ntioned in Section 2.2, we take BERT (Devlin et al.,\n2019) as our encoder to get contextualized represen-\ntations through the last hidden states h. We mainly\nadopt the cosine similarity as the measurement and\ncalculate the average intra-context similarity (be-\ntween hi and hj) and the average inter-context\nsimilarity (between hi and hk) over all tokens in\nthe corpus. It is worth noticing that we do use any\nmasks here when generating a token‚Äôs contextual-\nized representation for statistics.\nOther Measurements We observe the same Ô¨Ånd-\nings for MLM under other measurements, though\nthe statistics before are mainly based on cosine sim-\nilarities. We tried other similarities or distances,\ne.g., L1 distance, L2 distance and L10 distance, to\nevaluate the discrepancy between contextualized\nrepresentations from the same context and different\ncontexts. SpeciÔ¨Åcally, we make intra-context and\ninter-context statistics under speciÔ¨Åc measurement\nat different pre-training checkpoints, then calcu-\nlate the ratio of intra-context measurement over\nthe inter-context one. Table 9 shows the statistical\nresults. As we can see, when the ratio of L1 dis-\ntance decreases, the ratio of cosine similarity and\nthe dot-production similarity increase, vice versa.\nB Extra Experiments\nIn the standard implementation of BERT, the pa-\nrameters of input embeddings are shared with out-\nput embeddings. All experiments and analyses in\nthis paper are based on this assumption. To further\n2713\nTask Learning Rate Batch Size Train Epochs Warm-up Steps\nMNLI {1e-5, 2e-5, 3e-5, 5e-5} {32, 64, 128} {4, 6, 8} {1000, 2000}\nQQP {1e-5, 2e-5, 3e-5, 5e-5} {32, 64, 128} {4, 6, 8} {1000, 2000}\nQNLI {1e-5, 2e-5, 3e-5, 5e-5} {32, 64} {4, 6} {100, 200, 1000}\nSST-2 {1e-5, 2e-5, 3e-5, 5e-5} {16, 32, 64} {4, 6} 200\nCoLA {1e-5, 2e-5, 3e-5, 5e-5} {16, 32, 64} {4, 6} 100\nSTS-B {1e-5, 2e-5, 3e-5, 5e-5} {16, 32, 64} {4, 6} 100\nMRPC {1e-5, 2e-5, 3e-5, 5e-5} {16, 32, 64} {4, 6} 100\nRTE {1e-5, 2e-5, 3e-5, 5e-5} {16, 32, 64} {4, 6, 8} 100\nTable 8: Task-speciÔ¨Åc hyper-parameters for base models during Ô¨Åne-tuning.\nMeasurement / Checkpoint 1k 2k 3k 5k 7.5k 10k 20k 50k 100k 250k\nL1 Distance 0.977 0.925 0.880 0.833 0.769 0.779 0.774 0.797 0.820 0.838\nL2 Distance 0.978 0.927 0.884 0.838 0.778 0.789 0.783 0.803 0.826 0.843\nL10 Distance 0.981 0.928 0.890 0.854 0.802 0.811 0.805 0.822 0.844 0.860\nCosine Similarity 1.093 1.314 1.548 1.890 3.197 3.533 3.591 3.482 3.325 3.174\nDot-production Similarity 1.092 1.313 1.547 1.890 3.189 3.525 3.586 3.480 3.321 3.166\nTable 9: The ratio of intra-context measurement over inter-context measurement during pre-training. We list two\ndistance measurements and three similarity measurements here.\nconÔ¨Årm the effectiveness of TACO, we conduct the\nextra experiments without embedding sharing on\nBERT-small. The results are showed in Table 10.\nIt is unexpected that the variants without embed-\nding sharing perform worse compared their counter-\nparts due to lack of regularization of weight sharing.\nFrom the results, we can see that the TACO without\nembedding sharing performs slightly worse than\nTACO with embedding sharing. However, com-\npared to the MLM, it is still better than MLM than\n0.9 average GLUE score when convergence. These\nresults prove the effectiveness of TACO even when\nembeddings are not sharing.\nApproach MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\nMLM-250k 76.9 / 77.4 85.7 86.2 89.0 28.8 85.6 85.9 59.6 75.0\nTACO-50k 76.7 / 76.8 85.2 85.0 87.5 31.3 85.6 87.1 59.1 74.9\nTACO-50kw/o shared embedding 76.3 / 76.5 85.0 85.2 87.2 32.5 85.1 86.7 58.9 74.6\nTACO-250k 77.9 / 78.4 86.1 86.5 88.9 34.2 86.1 88.1 59.5 76.2\nTACO-250kw/o shared embedding 77.5 / 78.2 86.3 86.2 88.5 35.1 85.8 88.0 59.3 75.9\nTable 10: Results on GLUE validation set with small-size models. For models without embedding sharing, we run\n3 experiments with different random seeds for each task and report the average score.\n2714",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7259534597396851
    },
    {
      "name": "Representation (politics)",
      "score": 0.7127588391304016
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.6700154542922974
    },
    {
      "name": "Natural language processing",
      "score": 0.6204019784927368
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6124826073646545
    },
    {
      "name": "Speedup",
      "score": 0.5904427766799927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5728081464767456
    },
    {
      "name": "Language model",
      "score": 0.4709526002407074
    },
    {
      "name": "Distributional semantics",
      "score": 0.46951454877853394
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32238954305648804
    },
    {
      "name": "Programming language",
      "score": 0.17659211158752441
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I154570441",
      "name": "University of California, Santa Barbara",
      "country": "US"
    }
  ]
}