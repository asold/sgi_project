{
  "title": "Novel Transformer-based Fusion Models for Aero-engine Remaining Useful Life Estimation",
  "url": "https://openalex.org/W4382645144",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5010720671",
      "name": "Qiankun Hu",
      "affiliations": [
        "Nanjing University of Aeronautics and Astronautics"
      ]
    },
    {
      "id": "https://openalex.org/A5044023674",
      "name": "Yong-Ping Zhao",
      "affiliations": [
        "Nanjing University of Aeronautics and Astronautics"
      ]
    },
    {
      "id": "https://openalex.org/A5053608862",
      "name": "Li-Hua Ren",
      "affiliations": [
        "Nanjing University of Aeronautics and Astronautics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2022621390",
    "https://openalex.org/W2134456669",
    "https://openalex.org/W3083956363",
    "https://openalex.org/W2908875359",
    "https://openalex.org/W2617137613",
    "https://openalex.org/W2118020555",
    "https://openalex.org/W2885732902",
    "https://openalex.org/W2110787940",
    "https://openalex.org/W4293065977",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2037250283",
    "https://openalex.org/W2544905596",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3029002268",
    "https://openalex.org/W4361855816",
    "https://openalex.org/W2979101700",
    "https://openalex.org/W2914618626",
    "https://openalex.org/W2910660149",
    "https://openalex.org/W2415594836",
    "https://openalex.org/W3012055358",
    "https://openalex.org/W2120841219",
    "https://openalex.org/W2947621394",
    "https://openalex.org/W3137613462",
    "https://openalex.org/W3147229768",
    "https://openalex.org/W3173407600",
    "https://openalex.org/W4283750241",
    "https://openalex.org/W3215567741",
    "https://openalex.org/W3014146531",
    "https://openalex.org/W2772084711",
    "https://openalex.org/W2897557170",
    "https://openalex.org/W2889347686",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3177862163",
    "https://openalex.org/W3001566134",
    "https://openalex.org/W2992148407",
    "https://openalex.org/W4210562913",
    "https://openalex.org/W3217173838",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3126272279",
    "https://openalex.org/W3216106896",
    "https://openalex.org/W3208991442",
    "https://openalex.org/W2891408454",
    "https://openalex.org/W2035109300",
    "https://openalex.org/W2961350108",
    "https://openalex.org/W2791384746",
    "https://openalex.org/W2744067593",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Remaining Useful Life (RUL) estimation is a crucial technology in prognostic and health management (PHM) for modern aero-engines, as it ensures the reliability and safety of aircraft. With advances in sensor technology, data-driven approaches for RUL estimation have gained significant interest in recent years, especially deep learning-based methods. To further contribute to the field and improve the accuracy of RUL estimation, this paper, proposes novel Transformer-based fusion models for aero-engine RUL estimation. The vanilla Transformer is adapted for RUL estimation by modifying its structure based on the characteristics of aero-engine sensor data. The modified Transformer is then fused with Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) to extract degradation features from multiple aspects. Specifically, the LSTM and CNN layers are incorporated into the decoder and encoder of the Transformer. The effectiveness and superiority of the proposed models are demonstrated through experiments on the C-MAPSS benchmark dataset. The experimental results show that the proposed LSTM-Transformer fusion model outperforms the existing state-of-the-art approaches, with up to 66.53&#x0025; and 84.86&#x0025; improvement in RMSE and score metrics, respectively.",
  "full_text": "Received 8 April 2023, accepted 15 May 2023, date of publication 18 May 2023, date of current version 2 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3277730\nNovel Transformer-Based Fusion Models for\nAero-Engine Remaining Useful Life Estimation\nQIANKUN HU\n , YONGPING ZHAO\n , AND LIHUA REN\nCollege of Energy and Power Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China\nCorresponding author: Yongping Zhao (y.p.zhao@163.com)\nThis research was supported in part by the National Science and Technology Major Project under Grant J2019-I-0010-0010, in part by the\nFundamental Research Funds for the Central Universities under Grant NS2022027, and in part by the Science Center for Gas Turbine\nProject under Grant P2022-B-V-002-001.\nABSTRACT Remaining Useful Life (RUL) estimation is a crucial technology in prognostic and health\nmanagement (PHM) for modern aero-engines, as it ensures the reliability and safety of aircraft. With\nadvances in sensor technology, data-driven approaches for RUL estimation have gained significant interest\nin recent years, especially deep learning-based methods. To further contribute to the field and improve the\naccuracy of RUL estimation, this paper, proposes novel Transformer-based fusion models for aero-engine\nRUL estimation. The vanilla Transformer is adapted for RUL estimation by modifying its structure based\non the characteristics of aero-engine sensor data. The modified Transformer is then fused with Long\nShort-Term Memory (LSTM) and Convolutional Neural Network (CNN) to extract degradation features\nfrom multiple aspects. Specifically, the LSTM and CNN layers are incorporated into the decoder and\nencoder of the Transformer. The effectiveness and superiority of the proposed models are demonstrated\nthrough experiments on the C-MAPSS benchmark dataset. The experimental results show that the proposed\nLSTM-Transformer fusion model outperforms the existing state-of-the-art approaches, with up to 66.53%\nand 84.86% improvement in RMSE and score metrics, respectively.\nINDEX TERMS Aero-engine, prognostic and health management, remaining useful life estimation, trans-\nformer, long short-term memory neural network, convolution neural network.\nI. INTRODUCTION\nPrognostics and health management (PHM) has opened up\nnew avenues for maintenance of mechanical systems through\ncondition-based maintenance (CBM) [1]. By adopting prog-\nnostics, CBM forecasts potential faults beforehand and alters\nmaintenance plans in real-time. It presents a substitute for\npost-failure and planned maintenance. CBM is advantageous\nin preventing prolonged downtime, decreasing maintenance\nexpenses, and enhancing efficiency and financial gain. One of\nthe crucial technologies for CBM is prognostics. Predicting\nthe remaining useful life (RUL) of machines is a difficult\ntask, which aims at predicting the time left before a machine\nreaches the end of its useful life. Over the past few years,\nRUL estimation has become increasingly popular among both\nindustrial and academic circles.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Yongquan Sun\n.\nAero-engines, as a crucial part of an intricate system, are\nengineered to ensure consistent and dependable performance\neven under demanding conditions for prolonged periods. Pre-\nventive maintenance based on schedules and corrective main-\ntenance based on faults can lead to operational disruptions\nin aircraft and a resultant downtime for equipment. Hence,\nit is essential to detect potential faults beforehand and conduct\nnecessary maintenance tasks in accordance with the RUL to\npreserve the durability and reliability of the aircraft.\nThe methods for RUL estimation are typically categorized\nas either model-based or data-driven. Model-based methods\ncan precisely model prognostic systems using ample empiri-\ncal data, which establishes their reliability. Some exemplary\nmodel-based approaches have been proposed for RUL esti-\nmation, such as the Gamma model [2], [3], the semi-Markov\nmodel [4], and the Wiener model [5]. However, with the\nincreasing complexity of machines, the flexibility and scal-\nability of model-based approaches are limited, which makes\n52668\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nthem less suitable for various machine systems. With the\nadvancements in sensor technology, the quantity of data\navailable for condition monitoring (CM) has increased dra-\nmatically, which leads to the emergence of various data-\ndriven approaches. Data-driven approaches aim to uncover\nthe relationship between CM sensor data and the degradation\nof the machines [6]. In contrast to model-based, data-driven\nmethods have superior generalization capabilities and do not\nnecessitate expert knowledge. Therefore, there has been an\nincreasing prevalence of data-driven approaches over the past\nfew years.\nVarious machine learning (ML) algorithms have been com-\nmonly utilized in data-driven approaches to extract valuable\ndegradation information from CM sensor data and estimate\nthe remaining useful life of machines [7], [8], [9]. These\nalgorithms include support vector regression (SVR) [10],\nextreme learning machine (ELM) [11], [12], support vector\nmachine (SVM) [13], [14], etc. Traditional machine learning\nalgorithms typically necessitate appropriate feature engineer-\ning, which entails extracting relevant features and reducing\ndimensionality. However, without the aid of pertinent human\nknowledge, improper features can lead to inadequate per-\nformance. Thankfully, deep learning algorithms are capable\nof automatically extracting abstract feature representations\nof high level from large quantities of sensor measurements,\nand feature engineering is not required. This advantage has\nmotivated a rising number of researchers to exploit deep\nlearning approaches for RUL estimation.\nThe field of machinery prognostics has recently shown\nincreased interest in deep learning-based architectures such\nas convolutional neural networks (CNN) and long short-term\nmemory (LSTM) neural network. LSTM is particularly suit-\nable for analyzing time-series data and has been used by\nseveral scholars for RUL estimation. Wu et al. [15] employed\nbasic LSTM to predict the RUL of aero-engines with high\nprecision. Wang et al. [16] adopted the bidirectional LSTM\n(BiLSTM) for RUL estimation. This approach improved the\naccuracy of RUL estimation by reducing the influence of\nnoise. In a different approach, Liu et al. [17] developed a\nmodel with encoder and decoder structure for RUL esti-\nmation. In the encoder, BiLSTM is combined with CNN\nto extract temporal dependencies and significant features\nfrom the time series data. While in the decoder, fully-\nconnected layers (FC) are used to decode the encoded feature\ninformation, i.e., to estimate the RUL. On the other hand,\nCNNs have the capacity to extract relevant local features and\nhave shown great potential in RUL prediction [18]. Sateesh\nBabu et al. [19] employed a two-dimensional CNN for RUL\nestimation. Li et al. [20] utilized a deep CNN to estimate RUL\nwithout the need for prior human expertise in feature engi-\nneering. Moreover, Li et al. [21] also developed a multi-scale\ndeep CNN with a strong capability for extracting features, and\ndemonstrated its superiority through experiments.\nAlongside RNN and CNN models, the Transformer [22]\nmodel has been used for RUL estimation in recent studies.\nTransformer is a neural network architecture that was\nfirst introduced for machine translation tasks. It uses a\nself-attention mechanism to model the dependencies between\ninput and output sequences. The self-attention mechanism\nallows the model to weigh the importance of each input\nelement when producing the output. This architecture has\nbeen shown to outperform traditional RNN models on sev-\neral natural language processing tasks. Due to its success\nin modeling long-term dependencies and parallel compu-\ntation, the Transformer model has also been explored for\ntime-series data analysis. Recent studies have proposed novel\narchitectures that combine the Transformer model with other\ndeep learning techniques to achieve better performance in\nRUL estimation. For example, Mo et al. [23] combined the\nTransformer encoder with a gated convolutional unit to esti-\nmate aero-engine RUL, which demonstrates superior per-\nformance on C-MAPSS benchmark dataset [24]. Similarly,\nWang et al. [25] proposed a joint deep learning architecture\nfor RUL estimation based on the Transformer encoder and the\ntemporal convolution neural network (TCNN). This approach\nachieved satisfactory results on the C-MAPSS dataset, espe-\ncially under complex working conditions. Zhang et al. [26]\ndesigned a dual-aspect self-attention based on transformer\n(DAST) model for aero-engine RUL estimation, which also\nobtains promising performance. In addition, Ding and Jia [27]\ncreated a convolutional Transformer (CoT) architecture for\nRUL estimation. This structure was validated to be effec-\ntive and superior to other state-of-the-art methods. Finally,\nLiu et al. [28] presented a data-driven framework based on\ndouble attention for aero-engine RUL prediction. This frame-\nwork consisted of a attention-based CNN and a Transformer\nmodel. It outperformed the existing state-of-the-art meth-\nods on the benchmark dataset. In summary, the Transformer\nmodel has demonstrated notable potential in prognostics for\nRUL estimation.\nWhile data-driven approaches have demonstrated impres-\nsive achievements in aero-engine RUL estimation, approaches\nbased on LSTM, CNN and Transformer have limitations.\nCNN-based methods for RUL estimation treat all features\nas equally important and do not weigh them distinctively.\nLSTM-based methods suffer from the problem of long-\nterm dependencies, where gradients tend to vanish when\nusing historical information for the present task. While the\nTransformer has shown promise in RUL estimation due to\nits ability to model long-term dependencies and parallel\ncomputation, its point-wise dot-product self-attention can\nmake it insensitive to local context, potentially leading to\nanomalies in time series.\nTo further explore the potential of the Transformer and\nenhance the accuracy of RUL estimation, this paper conducts\nresearch on Transformer-based fusion models. The proposed\nfusion models combine the strengths of CNN, LSTM and\nTransformer, with the aim of leveraging their complemen-\ntary capabilities to enhance the accuracy of RUL estimation.\nSpecifically, the vanilla Transformer model is adapted for\nVOLUME 11, 2023 52669\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nRUL estimation by modifying its input and output layers, and\nmaking supplementary adjustments to its structure based on\nthe characteristics of aero-engine sensor data. The adapted\nTransformer is then fused with CNN, LSTM and their com-\nbination layers by incorporating them into the encoder and\ndecoder layers of the Transformer, yielding three fusion mod-\nels. These fusion models aim to combine the advantages of\nCNN, LSTM, and Transformer models to enhance the perfor-\nmance of RUL prediction. In these fusion models, CNN and\nLSTM are respectively fused with the encoder and decoder\nof Transformer, which serve as the feature extractor and\ninterpreter respectively. Consequently, this fusion enhances\nthe feature extraction and interpretation capabilities of Trans-\nformer. The fusion of LSTM with the Transformer model\nallows for taking advantage of the strong temporal charac-\nteristics inherent in the LSTM recurrent structure, thereby\naddressing the issue of weak temporal characteristics in the\nTransformer. The fusion of CNN with Transformer makes it\npossible to better explore the local degradation characteristics\nof the degraded trajectory by leveraging the CNN’s ability\nto extract local features. Furthermore, the fusion of CNN,\nLSTM, and Transformer aims to leverage their combined\nfeature extraction capabilities, with the intention of capturing\nas much degradation information as possible and improving\nthe accuracy of RUL estimation. The effectiveness and supe-\nriority of these fusion models are verified through experi-\nments on the aero-engine benchmark dataset. Moreover, the\nRUL estimation performance of the proposed fusion models\nis compared with existing state-of-the-art approaches.\nThe major contributions of this paper are summarized as\nfollows:\n1) Three novel Transformer-based fusion models, the\nLSTM-Transformer, CNN-Transformer, and CNN-\nLSTM-Transformer, are proposed for aero-engine RUL\nestimation. These models are developed by structurally\nadjusting the original Transformer model to accommo-\ndate the RUL estimation scenario, and fusing it with\nLSTM and CNN layers by incorporating them into the\nencoder and decoder.\n2) The structural hyperparameters of the basic Trans-\nformer model are optimized, and with these opti-\nmized hyperparameters, the RUL estimation perfor-\nmance of the proposed fusion models is compared on\nthe aero-engine benchmark dataset. The experiment\nresults show that, in terms of estimation accuracy, the\nLSTM-Transformer is the optimal choice among the\nthree fusion models.\n3) The estimation errors of LSTM-Transformer on the\nbenchmark dataset are analyzed in detail from various\naspects in this study. The analysis reveals the high\nestimation accuracy and early prediction characteristic\nof LSTM-Transformer.\n4) The effectiveness and superiority of LSTM-Transformer\nare experimentally validated on the C-MAPSS bench-\nmark dataset. The experimental results indicate that\nthe LSTM-Transformer fusion model significantly\nimproves the performance of aero-engine RUL esti-\nmation and outperforms existing state-of-the-art\napproaches.\nThe rest of this paper is organized as follows: Section II\nelaborates on the backbone and structures of the proposed\nmodels. Subsequently, the proposed models are experimen-\ntally validated in Section III. Finally, Section IV concludes\nthis paper.\nII. METHODOLOGY\nA. TRANSFORMER\nTransformer [22] is a neural network model based on the\nself-attention mechanism, and has shown significant success\nin various domains, particularly in natural language process-\ning. It has an encoder-decoder structure and is a sequence-\nto-sequence model that accepts a series of input data and\ngenerates a corresponding series of output data. In this paper,\nwe have adapted the Transformer for RUL estimation by\nmodifying the input and output layers and making supple-\nmentary adjustments to its structure based on the charac-\nteristics of aero-engine sensor data. Figure 1 presents the\nFIGURE 1. The adapted Transformer network architecture for remaining\nuseful life estimation.\n52670 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nproposed Transformer-based network architecture for RUL\nestimation. The proposed structure consists of five compo-\nnents: an encoder, a decoder, a dimension replication layer,\nposition encoding and two linear layers before the RUL\noutput. The encoder and decoder in Transformer, are con-\nstructed by stacking identical layers. Each encoder layer\ncomprises a feedforward neural network (FNN) sublayer and\na multi-head attention (MHA) sublayer, while every decoder\nlayer comprises an FNN sublayer and a masked MHA sub-\nlayer, an encoder–decoder MHA sublayer. Every sublayer in\nTransformer employs a residual connection to improve the\nstability of gradient updates. Additionally, layer normaliza-\ntion is adopted in each sublayer to ensure effective training.\nIn the proposed structure, the dimension replication layer\nis used to replicate the input one-dimensional RUL data to\nmatch the dimension of the sequence flowing in the decoder.\nIn the following sections, key components and mechanisms\nof the Transformer will be elaborated upon.\n1) ENCODER\nThe encoder takes the feature sequences as input, and utilize\nthe self-attention mechanism to transform the input into a\nvector that contains the essential feature information. The\nself-attention mechanism is designed to establish connections\nbetween different positions within the sequence, and derives\na novel representation of the input. This mechanism reduces\ndependency on external knowledge and is more efficient in\ncapturing the inherent significance of the data. The mech-\nanism achieves this mainly through the calculation of fea-\nture vector similarity to present relevance as an approach\nto addressing the challenge of capturing long-term depen-\ndencies. For RUL estimation, the self-attention mechanism\ncan filter crucial information from the feature sequences of\naero-engine sensor data and utilizes weights to indicate the\ninformation’s significance, which enables the model to focus\nmore on important information.\nFIGURE 2. The scaled dot-product attention module and the multi-head\nattention module.\nIn the implementation of Transformer, the scaled\ndot-product attention (SDPA) is utilized to compute the\ninput matrix’s attentions. The operations in SDPA comprise\nnormalization and dot-product. Figure 2 presents the structure\nof SDPA and its combination, the MHA module. Firstly,\ndot-product is performed on key K and query matrix Q to\nobtain the attention weights, and subsequently, these weights\nare scaled by the softmax function. Afterwards, the value\nmatrix V is multiplied with the scaled weights and added\ntogether. The calculation procedure is expressed as the fol-\nlowing formula:\nAttention (Q, K, V ) = softmax\n(QKT\n√dk\n)\nV (1)\nwhere Q, K, V ∈ RT ×dk , Q represents the query matrix, K is\nthe key matrix, and V is the value matrix. The three matrices\nare computed by taking the dot product of their corresponding\nweight matrices and feature matrix X:\nQ = Xq × Wq\nK = Xk × Wk\nV = Xv × Wv (2)\nwhere Xq, Xk , Xv ∈ RT ×dmodel are position-encoded feature\nmatrices, and Wq, Wk , Wv ∈ Rdmodel×dk denotes the corre-\nsponding weight matrices.\nBy utilizing the self-attention mechanism, the model can\nprioritize crucial parts of the input sequence. However, a sin-\ngle attention can solely capture significant information within\none representation space. To address this, the MHA mech-\nanism is employed, which maps the separate sets of Q, K,\nand V into multiple spaces with lower dimensions, enabling\ndifferent parts of the sequence to be attended to in each\nmapping. This approach improves the capture of positional\ninformation since each head focuses on distinct parts of the\ninput sequence. The integration of multiple heads generates\na more holistic and inclusive representation. Actually, MHA\ncan be regarded as a variant of self-attention mechanism.\nIt utilizes multi attention heads to extract informations from\ndistinct spaces individually, and merge them as the final\nattention. The formula of MHA is as follows:\nMultiHead\n(\nXq, Xk , Xv\n)\n= Concat (head1, head2, · · ·, headh, ) W O\nwhere headi = Attention\n(\nXqW i\nq, Xk W i\nk , XvW i\nv\n)\n(3)\nwhere i denotes the ith head, h is the number of attention\nheads, W i\nq, W i\nk , W i\nv ∈ Rdmodel×dk are the weight matrices of\nheadi, W O ∈ Rhdk ×dmodel represents the MHA weight matrix,\nand the outputs yielded each self-attention head are spliced\nby the Concat function.\n2) DECODER\nAs shown in Figure 1, the output of the encoder are passed\nto an MHA sublayer of the decoder, which is known as the\nencoder-decoder attention mechanism. It should be noted\nVOLUME 11, 2023 52671\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nthat each decoder layer utilizes the output of the encoder’s\nfinal layer. In this mechanism, the K and V matrices are\nacquired from the encoder’s output, which encapsulate the\nunderlying information of the inputted feature sequence. And\nQ is acquired from the output of the masked MHA sublayer\nin the decoder, which contains the information of the RUL\nlabels. In the encoder, the MHA sublayers only aim to learn\nthe relationships between features at different temporal posi-\ntions. To enhance RUL estimation accuracy, it is essential\nto take into account the relationships between the feature\ninput sequence and the corresponding RUL labels. As seen in\nFigure 1, the RUL labeled data is fed into the masked MHA\nmodule, which then endeavors to capture the relationships\nwithin the data.\nWhen training, the RUL label are included in the train-\ning dataset, and the decoder decodes for every time step in\nparallel. However, in the computation of SDPA, the decoder\ninevitably learns information from future RUL label data at\neach time step, which is unrealistic. To address this issue,\na masking operation is added to the decoder’s MHA module,\nwhich masks the information from future RUL labels. In the\nmasking process, a matrix that has values 1 in the diagonal\nand lower triangle, and 0 in the upper triangle, is added into\nQKT to exclude the future information during computation.\nDuring testing or evaluation, the estimated RUL from the\nproposed model is fed into the decoder as the RUL label data,\nin an auto-regressive manner.\nThe original proposal for the Transformer model was\nin the field of natural language processing, in which both\nfeatures and labels were represented as word vectors with\nidentical dimensions. The identical dimensions expedite the\nstraightforward computation in MHA sublayers. But in RUL\nestimation scenario, the features are composed of multiple\ndimensions while the RUL labels have only one dimension.\nThis dimensionality difference hinders the functioning of the\nencoder-decoder attention mechanism. In order to address\nthis issue, the dimension replication layer is introduced before\nthe positional encoding of decoder to increase the RUL data’s\ndimensionality. This replication process involves copying the\none-dimensional RUL vector y ∈ RT into a matrix Y ∈\nRT ×dmodel with identical columns. Similarly, a fully connected\n(FC) layer with dmodel neurons is employed at the output for\ndimensionality reduction.\n3) POSITIONAL ENCODING\nThe positional information in a sequence plays a crucial role\nin RUL prediction as it represents the sequential order and\nsignificantly impacts the prognostics. However, due to the\nabsence of recursion in RNN and convolution in CNN in\nTransformer, the sequence does not inherently contain tem-\nporal information. Therefore, relative or absolute positional\ninformation of the data needs to be added to the sequence.\nThe Transformer adopts position encoding, to encode the\npositional information into the input/output sequences. Posi-\ntion encoding is a matrix with the same length and dimen-\nsion as the input/output, which can be directly added to the\ninput/output to obtain positional information. In Transformer,\nsine and cosine functions of different frequencies are used for\nposition encoding. The position encoding in Transformer is as\nfollows:\nPE(pos,2i) = sin\n( pos\n10002i/dmodel\n)\nPE(pos,2i+1) = cos\n( pos\n10002i/dmodel\n)\n(4)\nwhere i represents the dimension, dmodel is the feature dimen-\nsion, and pos denotes the position. In addition, to achieve\nsymmetric position encoding, the value of dmodel ought to be\neven.\n4) FEED-FORWARD NEURAL NETWORK\nBesides the attention sublayers, each sublayer within the\nencoder and decoder contains a fully connected feed-forward\nnetwork (FFN) that is applied separately and identically to\neach position. This network comprises two linear transforma-\ntions, with a rectified linear unit (ReLU) activation function\nin between. While the self-attention mechanism might not\nbe able to effectively capture complex processes, FNN can\nenhance the representation ability of the model through a\ntransformation of dimensions. FNN’s formula is expressed as\nfollows:\nFNN (x) = ReLU (xW1 + b1) W2 + b2 (5)\nwhere W1, W2 are weight matrices, b1, b2 are biases, and\nReLU (x) =\n{\nx, x ≥ 0\n0, x < 0. (6)\nB. LSTM\nLSTM (Long Short-Term Memory) [29] is a type of recurrent\nneural network (RNN) architecture that is capable of learning\nlong-term dependencies in sequential data. Figure 3 illus-\ntrates the diagram of LSTM blocks and the information trans-\nfer and update mechanisms among them. Unlike traditional\nRNNs, which have a simple structure and tend to suffer from\nvanishing or exploding gradients, LSTM uses a memory cell,\ninput gate, forget gate, and output gate to allow the network to\nselectively store or discard information. The memory cell is\nthe central component of LSTM and is responsible for storing\ninformation over time. The input gate determines how much\nnew information is added to the memory cell, while the forget\ngate controls the extent to which old information is retained.\nFinally, the output gate regulates how much information is\noutput from the memory cell. The formulas of LSTM are as\nfollows:\nit = σ(Wxixt + Whiht−1 + bi)\nft = σ(Wxf xt + Whf ht−1 + bf )\n˜ct = tanh(Wxcxt + Whcht−1 + bc)\nct = ft ⊙ ct−1 + it ⊙ ˜ct\not = σ(Wxoxt + Whoht−1 + bo)\nht = ot ⊙ tanh(ct ) (7)\n52672 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nFIGURE 3. Diagram of LSTM blocks.\nwhere xt is the input at the current time step t, ht−1 is the\noutput at the previous time step, Ct is the memory cell, ft ,\nit , ˜Ct , and ot are the forget gate, input gate, new memory\ncell, and output gate respectively, W and b are learnable\nparameters, and σ and tanh are sigmoid and hyperbolic tan-\ngent functions, respectively. The equations for LSTM are\nderived from the gating mechanism, which involves a series\nof sigmoid and hyperbolic tangent functions. The sigmoid\nfunctions are used to decide which information to keep or\nforget, while the hyperbolic tangent functions are used to\ncalculate the new information to be added to the memory cell.\nIn this paper, we fuse LSTM layers with the Transformer to\nimprove the RUL estimation performance of the Transformer-\nbased model, which will be elaborated in the subsequent\nsections.\nC. CNN\nConvolutional Neural Networks (CNN) [30] is commonly\nused in various fields of pattern recognition and machine\nlearning, including image and time series data analysis. The\nconvolutional layer is the core of CNN, consisting of con-\nvolution and pooling operations. The convolution in CNN\nis the cross-correlation operation used in time series signal\nprocessing and image processing. One-dimensional convo-\nlution (Conv1D) is commonly used in time series signal\nprocessing, while two-dimensional convolution (Conv2D) is\ncommonly used in image processing. Figure 4(a) illustrates\nthe Conv1D operation. Conv1D is essentially a special case\nof Conv2D, where the width of the convolution kernel is the\nsame as the number of dimensions in the input data, and the\nkernel slides only along the time dimension for convolution.\nIn Conv1D, the convolution kernel traverses the original\ntime series data in the time direction, and the correspond-\ning elements are multiplied and summed to obtain a new\nfeature map, which is reduced to one dimension. Therefore,\nmultiple convolution kernels are needed to extract different\nfeatures and merge them along the dimension direction to\nform a multi-dimensional output time series. The formula for\nConv1D can be expressed as:\nyt =\nk∑\ni=1\nwixt−i+1 + b (8)\nwhere yt is the output of the convolutional layer at time step\nt, k is the size of the convolutional kernel, wi are the weights\nof the kernel, xt−i+1 are the input values within the kernel’s\nrange, and b is the bias term.\nMax pooling is a down-sampling operation that is com-\nmonly used in CNN to reduce the dimensionality of fea-\nture maps. The technique works by partitioning the feature\nmaps into non-overlapping rectangular regions and taking\nthe maximum value in each region. The resulting output\nis a down-sampled version of the input feature maps that\npreserves the most important features. Figure 4(b) depicts the\n1D-maxpooling (MaxPool1D) operation. The MaxPool1D\noperation is used in one-dimensional CNNs and performs\nmax pooling along the time axis of a feature map. The formula\nfor MaxPool1D is:\nMaxPool1D (xi) =\nk−1\nmax\nj=0\nxi×s+j (9)\nwhere xi is the output of the previous convolutional layer\nat time step i, k is the size of the pooling window, and s is\nthe stride. In this paper, we integrate the convolutional layer\nconsisting of Conv1D and MaxPool1D with Transformer for\naero-engine RUL estimation.\nD. TRANSFORMER-BASED FUSION MODELS OF CNN\nAND LSTM\nThe fusion method proposed in this paper aims to structurally\nfuse different deep learning algorithms to explore the effect\nof combining their different feature extraction capabilities.\nCNN has a strong local perception and translation invari-\nance in feature extraction. LSTM, as an RNN structure, has\nstrong temporal characteristics, and the introduction of the\nforget gate can solve long-term dependency issues in fea-\nture extraction. The attention mechanism of Transformer has\nstrong feature extraction capabilities for key features and is\nindependent of the temporal relationship, but this is also one\nof its drawbacks, as it has weak temporal characteristics even\nwith positional encoding. In order to further improve the\nRUL estimation performance of the proposed Transformer-\nbased model, this paper proposes to incorporate LSTM and\nCNN into the decoder and encoder of the Transformer to\nextract features from multiple aspects and obtain better RUL\nestimation accuracy. Based on the Transformer model shown\nVOLUME 11, 2023 52673\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nFIGURE 4. The Conv1D operation and MaxPool1D operation in CNN.\nFIGURE 5. The network structures of proposed Transformer-based fusion models.\nin Figure 1, three types of Transformer-based fusion mod-\nels are proposed in this paper: LSTM-Transformer, CNN-\nTransformer, and CNN-LSTM-Transformer, illustrated in\nFigure 5 (a), (b), and (c), respectively.\nThe LSTM-Transformer fusion model is an extension of\nthe adapted Transformer model for RUL prediction that incor-\nporates two LSTM layers. The first LSTM layer is added\nbefore positional encoding and the Transformer encoder,\ncreating an encoder with an LSTM feature extractor. The\nsecond LSTM layer is added after the Transformer decoder,\ncreating a decoder with an LSTM feature interpreter. The\nuse of LSTM layers as feature extractors and interpreters for\nthe Transformer is intended to take advantage of the strong\ntemporal characteristics of the LSTM recurrent structure to\naddress the issue of weak temporal characteristics in the\nTransformer, resulting in a complementary effect.\nThe CNN-Transformer model shares the same structure as\nthe LSTM-Transformer model, except that the corresponding\nLSTM layers are replaced with CNN layers that include a\nConv1D layer and a MaxPool1D layer. In these CNN lay-\ners, the number of kernels, stride, and zero-padding of the\nConv1D and MaxPool1D layers are specifically set to ensure\nthat the temporal length and feature dimensions of the input\ndata remain unchanged after passing through the CNN layer.\n52674 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nDetailed parameter settings are provided in the subsequent\nexperimental sections. The use of the CNN structure as the\nfeature extractor and interpreter for the Transformer is aimed\nat fully exploring the local degradation characteristics of\nthe degraded trajectory by leveraging the CNN’s ability to\nextract local features, which can improve the accuracy of\nRUL prediction.\nThe CNN-LSTM-Transformer model combines the LSTM\nand CNN layers from the previous two fusion models in\na sequential manner, placing them at the front end of the\nTransformer encoder and the back end of the decoder. Specif-\nically, the input features are first passed through positional\nencoding and then undergo convolution and max-pooling in\nthe CNN layer, followed by feature extraction with the LSTM\nlayer before entering the Transformer encoder. Conversely,\nthe features generated from the Transformer decoder are first\nprocessed by the LSTM layer and then the CNN structure.\nThe parameter settings for the LSTM and CNN structures are\nconsistent with those of the previous two models. By com-\nbining the LSTM and CNN layers as the feature extractor and\nfeature interpreter for the Transformer, it is hoped to leverage\ntheir combined feature extraction capabilities to capture as\nmuch degradation information as possible and improve the\naccuracy of RUL estimation.\nIII. EXPERIMENT\nIn this section, the data preprocessing steps on C-MAPSS\nare firstly introduced, followed by optimizing the hyperpa-\nrameters of the proposed Transformer model. Subsequently,\nthe performance of the proposed fusion models is compared\nand analyzed. Finally, the RUL estimation performance of the\nLSTM-Transformer fusion model is compared with the exist-\ning state-of-the-art methods. All experiments in this paper\nwere conducted on a 64-bit Windows 10 PC with a 64GB\nRAM, an Intel i5-10400 CPU, and an Nvidia RTX2070 GPU.\nThe proposed models were implemented using ‘Python 3.10’\nand the deep learning library ‘PyTorch 1.13.1’. Instead of\nthe traditional mean square error (MSE) function, the scoring\nloss function proposed in [31] was adopted to calculate the\ntraining loss. The weights of neural networks were updated\nby the Adam optimization method [32]. During training, the\ntraining dataset was divided into mini-batches, each con-\ntaining randomly selected 10 samples. Moreover, all the\nsamples in training dataset are divided into 80%/20% train-\ning/validation sub-datasets. The learning rate of Adam was\nset to 0.001, and the training process was set to a maximum\nof 300 epochs. In this paper, each experiment is repeated ten\ntimes to reduce the effect of randomness, and the results are\naveraged. All hyperparameters of the proposed models are\ndetailed in Table 1.\nA. INTRODUCTION TO TURBOFAN ENGINE DATASET\nTo evaluate the proposed models’ performance, a tur-\nbofan engine benchmark dataset [24] is utilized in this\npaper. This dataset is generated by the NASA Commercial\nModular Aero-Propulsion System Simulation (C-MAPSS)\nTABLE 1. Hyperparameters of the proposed fusion models.\nFIGURE 6. Simplified diagram of turbofan engine in C-MAPSS [33]\n(components including low-pressure turbine (LPT), high-pressure turbine\n(HPT), fan spool (N1), N2: core spool (N2), low-pressure compressor\n(LPC), high-pressure compressor (HPC)).\nprogram [33] to simulate the realistic degradation of turbofan\nengines. This dataset has been widely used as the benchmark\ndataset for RUL estimation in PHM. A simplified diagram\nof a turbofan engine in C-MAPSS is presented in Figure 6,\nwhere the core consist of the low/high-pressure compressor,\nthe low/high-pressure turbine, and the combustor. This engine\nis capable of producing a thrust up to 90,000 pounds (nearly\n400,340 newtons), and can operate under a wide range of\nconditions, such as from sea level to an altitude of 40,000 foot\n(12,192 meters), from temperatures of -60 to 103 Fahrenheit\n(-51 to 39 Celsius), and from Mach numbers of 0 to 0.90.\nThe turbofan engine operates in the following way: air is\ndrawn into the engine by the fan, and then passes through the\nlow and high-pressure compressors where it is compressed.\nThen in the burner, fuel is mixed with the compressed air to\ninitiate combustion. Next, the resulting hot exhaust gas flows\nVOLUME 11, 2023 52675\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nthrough the low and high-pressure turbines, and exits through\nthe nozzle, which generates thrust. Meanwhile, the air that\nbypasses the engine core remains cool and flows through the\nfan. The aircraft is propelled forward by the joint effort of the\nengine core and the fan, which generate the engine’s thrust.\nBased on fault modes and operating conditions, the\nC-MAPSS dataset is divided into four sub-datasets (FD001\nto FD004). For instance, FD001 involves one fault mode\n(the degradation of high-pressure compressor) and one oper-\national condition, while FD004 has two fault modes (the\ndegradations of high-pressure compressor and fan) and six\noperational conditions. The details of the C-MAPSS dataset\nare presented in Table 2. Each sub-dataset consists of a\ntraining dataset and a testing dataset. The training dataset\ncomprises time-series data with multiple variables, which\ntrack the status of multiple engine units. The engine units are\ntreated as a group of comparable engines, but with varying\nlifetimes. The engines in the training dataset operate from\na specific initial time until the end of their life time, with\ntheir corresponding RUL values recorded at every time cycle.\nIn contrast, the time-series data in the testing dataset stops\nbefore the engines fails. The goal is to estimate the number\nof operation cycles remaining in the testing dataset before the\nengine failure occurs.\nTABLE 2. Details of the C-MAPSS benchmark dataset [24].\nIn C-MAPSS, the multivariate time series data is collected\nfrom 21 types of sensors with indices ranging from 1 to 21.\nThese sensors measure various parameters such as pressure\nand temperature to monitor the engine’s state. A detailed\ndescription of each of the 21 sensor measurements can be\nfound in Table 3.\nB. DATA PREPROCESSING\n1) FEATURE SELECTION\nThe first step of the data preprocessing is to perform fea-\nture selection on the C-MAPSS dataset. To select the most\ninformative features, FIGURE 7 displays the measurements\nof all 21 sensors from the sub-dataset FD004. The different\nclusters in the figures represent turbofan engines operating in\ndifferent operation conditions. As shown in FIGURE 7, there\nare some sensor measurements that remain constant, and\ncannot provide useful degradation information. Therefore,\nthe constant measurements from seven sensors (including epr,\nfarB, T2, Nf_dmd, P2, P15, and PCNfR_dmd) are discarded.\nIn recent research by Ren et al. [31], a novel feature selection\napproach based on the linear degradation model (LDM) was\nproposed for the C-MAPSS dataset. This approach selects\nsensor measurements with the most significant degradation\ninformation for each sub-dataset in C-MAPSS. In this study,\nTABLE 3. Details of 21 sensor measurements in C-MAPSS [24].\nwe use the selected sensors from [31] to train the proposed\nmodels and perform RUL estimation for testing engines. The\nselected sensor indices are presented in Table 4.\nTABLE 4. Indices of sensors selected for each sub-dataset.\n2) NORMALIZATION\nIn practical prognostic applications, different sensor data gen-\nerally have different scales. If the raw data are directly used\nfor model training, the unequally weighted input will make\nthe algorithm difficult to converge. Therefore, data normal-\nization is necessary before model training, which converts\nthe raw data into the same scale. Additionally, the engineered\nsystems typically operate at different operational conditions.\nThe differences can affect the degradation process of engi-\nneered systems [34], which can be revealed in the sensor\ndata. Therefore, the operational difference should also be\nconsidered while performing data normalization. For these\nreasons, z-score normalization [35] is employed in this paper.\nThis normalization method takes into account the differences\nin operational conditions while normalizing the data. It is\ndefined as follows\nx(m,f )\nnorm = x(m,f ) − µ(m,f )\nσ(m,f ) (10)\nwhere x(m,f ) is the raw data, x(m,f )\nnorm is the normalized data,\nm represents one of the M possible operational conditions,\nf denotes the f th sensor, µ(m,f ) and σ(m,f ) are the mean and\nstandard deviation of the f th sensor from the mth operational\ncondition. In practice, all µ and σ are calculated from the\n52676 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nFIGURE 7. All 21 sensors measurements from the sub-dataset FD004. The abscissa of these figures represents the index of monitoring time\ncycle. The last time cycle (i.e., the failure time) corresponds to index 0. Hence all the previous monitoring time cycles have negative indexes.\nFIGURE 8. Illustration of the data normalization.\ntraining dataset (run-to-fail data), and stored for the normal-\nization of the testing dataset (real-time sensor monitoring\ndata). Figure 8 shows the sensory data of a single turbo-\nfan engine before and after the normalization. As shown in\nFigure 8, the scales of the raw data vary significantly with\neach other. After the normalization, these scales are converted\ninto a normalized range. Besides, the normalized sensory data\ndisplays a clear degradation trend of turbofan engines.\n3) THE PIECE-WISE LINEAR RUL TARGET FUNCTION\nIn addition to the data-preprocessing steps mentioned above,\na piece-wise linear RUL target function [36] is also utilized\nin this paper. Conventionally, RUL is considered to decrease\nlinearly with time. However, the degradation of aero-engines\nis not evident at the early stage of their operational life.\nThus, during the initial period, the RUL can be regarded\nas constant, and when degradation occurs, the RUL can be\nconsidered as a linearly decreasing value. To address this\nissue, the piece-wise linear RUL target function was pro-\nposed in [36]. It limits the maximum value of the RULs in\nthe training dataset, as shown in FIGURE 9. This function\nhelps to prevent the algorithm from overestimating the RUL\nand achieve greater accuracy. The piece-wise RUL target\nfunction is widely recognized as an effective approach as\nVOLUME 11, 2023 52677\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nFIGURE 9. The piece-wise linear RUL target function.\nTABLE 5. RUL limit values on C-MAPSS dataset.\nit significantly enhances the RUL estimation performance.\nTherefore, the RUL target function is utilized in this paper\nto obtain the RUL labels in the training dataset of C-MAPSS.\nIn this paper, the RUL limit values for each sub-dataset are\ndetermined based on [31], in which a novel identification\nmethod is proposed that uses Euclidean distance to determine\nthe RUL limit values. The RUL limit values are presented in\nTable 5.\nC. PROGNOSTIC PERFORMANCE METRICS\nTo evaluate the RUL estimation performance of the proposed\napproach, two popular metrics are adopted in this paper.\nThe first metric is the root mean squared error (RMSE) in\nEquation (11), which is widely used in regression problems.\nThe other one is the RUL scoring function in Equation (12),\nwhich was developed in the 2008 PHM data challenge com-\npetition [24] and is a widely acknowledged metric in prog-\nnostics. A smaller score indicates a better RUL estimation\nperformance. The scoring function penalizes late estimations\n(i.e., when the estimated RUL is larger than the actual) more\nthan early estimations. By contrast, RMSE treats late and\nearly estimations equally.\nRMSE =\n\n√\n1\nn\nn∑\ni=1\nd2\ni (11)\nS =\n\n\n\n\n\n\nn∑\ni=1\n(\ne− di\n13 − 1\n)\n, di < 0\nn∑\ni=1\n(\ne\ndi\n10 − 1\n)\n, di ≥ 0\n(12)\nwhere n is the number of testing data samples and di =\nˆRULi − RULi is the error benchmark the estimated RUL and\nFIGURE 10. Comparison between the RUL scoring function and RMSE.\nthe actual RUL of the ith data sample. The difference between\nthe two metrics is illustrated in FIGURE 10.\nD. HYPERPARAMETER OPTIMIZATION OF THE\nTRANSFORMER-BASED MODEL\nIn this section, we perform hyperparameter optimization for\nthe Transformer-based model and its fusion models. The\nTransformer is widely used due to its excellent performance\nand few hyperparameters. Typically, the hyperparameters that\nneed tuning in Transformer include the number of nodes in\nthe feedforward neural network, the number of heads in the\nMulti-Head Attention (MHA), and the number of repetitions\nof the encoder and decoder. The number of nodes in the\nfeedforward neural network is usually chosen to be the same\nas or twice the input feature dimension, which has little effect\non the model’s performance. The number of heads in MHA is\nusually chosen as a divisor of the feature dimension. In this\npaper, the input feature dimension is 8, so the number of\nheads in MHA can only be 2 or 4. Finally, the number of\nrepetitions of the encoder and decoder is chosen. The original\nTransformer model used 6 repetitions, but considering the\nsmall size of the dataset and feature dimensions in this paper,\nthe number of repetitions is optimized between 1 and 6.\nDue to the large number of models that need training for\nhyperparameter tuning, only FD001 and FD003 are used as\noptimization datasets instead of the entire dataset.\nInitially, the number of heads in MHA is selected between\n2 and 4. With the encoder and decoder layers fixed at 2, the\nTransformer model is trained and the corresponding Score\nvalues on the FD001 and FD003 test sets are calculated,\nas shown in Table 6. The simulation results indicate that\nthe Transformer model with 2 heads in MHA is signifi-\ncantly better than the one with 4 heads. Therefore, the num-\nber of heads in MHA for the Transformer model and the\nTransformer-based fusion model in this paper is set to 2.\nNext, the number of repetitions of the encoder and decoder\nis optimized. With the number of heads in MHA fixed at 2, the\nnumber of repetitions is set from 1 to 6, and the corresponding\n52678 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nTABLE 6. Scores corresponding to different numbers of MHA heads.\nmodel is trained and tested. The performance of the Score\nvalues on the FD001 and FD003 test sets is shown in Table 7\nand Figure 12. From the figure, it is clear that the Score\nvalue drops significantly when the number of repetitions of\nthe encoder and decoder is increased from 1 to 2, indicating\nan improvement in the RUL prediction performance. How-\never, further increasing the number of repetitions does not\nshow a further decrease in the Score value, indicating that\na Transformer with two encoder and decoder layers can fully\nexpress the nonlinear relationship between sensor parameters\nand RUL in the C-MAPSS dataset. Further increasing the\nnetwork complexity will not significantly improve the RUL\nprediction performance but will increase the computational\nburden. Therefore, the number of repetitions of the encoder\nand decoder in the Transformer model and the encoder and\ndecoder structures in the Transformer-based fusion model in\nthis paper are set to 2.\nTABLE 7. Scores corresponding to different numbers of encoder and\ndecoder layers.\nFIGURE 11. Score Curve with the change of number of Layers.\nE. PERFORMANCE COMPARISON AND ANALYSIS OF THE\nPROPOSED MODELS\nThis section provides a comparative analysis of the RUL\nestimation performance between the single models and the\nTransformer-based fusion models proposed in this study.\nThe Score and RMSE values of each model on the four\nsub-datasets of C-MAPSS are shown in Tables 8 and Table 9,\nrespectively. In these tables, the best results achieved by the\nmodels on each sub-dataset are highlighted in red. According\nto these tables, Transformer achieved the highest RUL esti-\nmation accuracy among the three single models. The perfor-\nmance difference between LSTM and Transformer was not\nsignificant, while the performance of CNN was noticeably\nworse than that of the other two models. Therefore, CNN\nmay not be the most suitable model for RUL estimation in\nthe context of aircraft engines. However, after fusing CNN\nand LSTM with the Transformer model, the RUL estimation\nperformance of both of them has been significantly improved.\nAmong the fusion algorithms, LSTM-Transformer achieved\nthe highest RUL estimation accuracy, while the estimation\naccuracy of the CNN-Transformer fusion model was not as\ngood as that of the single Transformer model. These results\nsuggest that CNN has limited performance on the C-MAPSS\ndataset, whether used alone or fused with Transformer. LSTM\nand Transformer have good performance on this dataset,\nand the combination of them can obtain the optimal model\nstructure.\nTable 8 lists the training time of each model after one\niteration on FD001, which reflects the model’s computa-\ntional complexity. Figure 12 presents a comparison of the\nestimation accuracy and computational complexity of each\nmodel. The closer the algorithm is to the bottom left corner\nof the figures, the higher the accuracy and the lower the\ncomputational complexity. From Figure 12, it can be seen that\nthe Score and RMSE values of the Transformer-based fusion\nmodels are relatively low, but their corresponding algorithm\ncomplexity is high. If both computational complexity and\nestimation accuracy are considered, the single Transformer\nmodel located at the bottom left corner of the graph is the opti-\nmal choice. However, if accuracy is given priority, LSTM-\nTransformer has the highest estimation accuracy.\nFrom Figure 12, it can also be observed that the training\ntime is significantly increased after fusing Transformer with\nthe LSTM model, while the increase in training time after fus-\ning Transformer with CNN is relatively small. This is because\nboth Transformer and CNN can perform parallel computation\nin the temporal dimension, while LSTM requires recursive\ncalculation of output according to the time order strictly.\nTherefore, the fusion of Transformer and LSTM models\nalso needs to follow the recursive calculation rules, which\nnaturally leads to a significant increase in calculation time.\nHowever, for the RUL dataset, the data volume itself is not\nparticularly large, and the overall training time is within\nan acceptable range. In practical RUL estimation scenarios,\npeople pay more attention to estimation accuracy than com-\nputational complexity. Therefore, estimation accuracy has the\nhighest priority for model selection. Considering the above\nfactors, LSTM-Transformer can be regarded as the optimal\nmodel among the single models and fusion models proposed\nin this paper.\nVOLUME 11, 2023 52679\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nTABLE 8. The scores of proposed models on C-MAPSS dataset and the training time.\nTABLE 9. The RMSE of proposed models on C-MAPSS dataset.\nFIGURE 12. Comparisons of RUL estimation accuracy and computational complexity for the proposed models.\nF. RUL ESTIMATION RESULTS OF LSTM-TRANSFORMER\nON C-MAPPS\nIn this section, the RUL estimation results and errors of\nthe proposed LSTM-Transformer model on the C-MAPPS\ndataset are presented and analyzed in detail.\nFigure 13 presents the RUL estimation results of the\nLSTM-Transformer model at the end point of the degradation\ntrajectory in the four testing sub-datasets. It can be observed\nthat the estimation results of most degradation trajectories\nare very close to the actual values, except for those with\nRUL exceeding the limit value, whose estimation accuracy is\nrelatively low. However, those degradation trajectories with\nRUL exceeding the limit value are already in the healthy\nstage, and even if the estimation is inaccurate, it would not\nhave a significant impact on the PHM of the engine.\nFigure 14 presents the predicted degradation trajectories\nof some engines in the FD003 test set using the LSTM-\nTransformer model, where the aero-engine numbers are indi-\ncated in the sub-figure titles. As can be seen, the predicted\ndegradation trajectories of all aero-engines maintain high\naccuracy after entering the degradation phase. Furthermore,\nin the late degradation period, the predicted RULs are very\nclose to the actual values and mostly slightly underestimated,\nindicating the tendency of ahead-of-time failure estimation.\nFigure 15 shows the boxplot of the estimation error distri-\nbution of the LSTM-Transformer on the C-MAPSS testing\ndatasets. As can be seen in the figure, the normal errors\ndistributed in the 25% ∼ 75% range of FD001 and FD003\nare almost close to zero, and those of FD002 and FD004\nare mostly within ±10 cycles. There are many outliers with\n52680 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nFIGURE 13. The RUL estimation results of LSTM-Transformer on all testing sub-datasets.\nnegative errors in FD002 and FD004, which correspond to\nactual RUL values exceeding the RUL limiting value. More-\nover, it is noteworthy that both the mean and median of\nthe RUL estimation errors are negative, indicating that the\nLSTM-Transformer model has a clear tendency to predict an\nearlier failure time in the late period of aero-engine degrada-\ntion, where the engines are close to failure. This characteristic\nof the model has significant industrial value, as timely and\naccurate RUL estimation in the late period of aero-engine life-\ntime contributes to enhancing system reliability and safety.\nTo further understand the distribution of RUL estimation\nerrors of the LSTM-Transformer model, a histogram of the\nerrors is presented in Figure 16. In this study, kernel density\nestimation [37] is used to estimate the density function of\nRUL estimation errors. The estimated probability density\nfunction and the 95% confidence interval are illustrated in\nFigure 16. It can be seen from Figure 16 that both the his-\ntogram and probability density function of the estimation\nerrors are centered around zero. In addition, the 95% con-\nfidence intervals in FD001 and FD003 datasets are within\n±10 cycles, indicating that the LSTM-Transformer model has\nhigh accuracy in RUL estimation. After analyzing the error\ndistribution and confidence interval, it can also be concluded\nthat the LSTM-Transformer model exhibits a significant ten-\ndency towards ahead-of-time prediction.\nG. COMPARISON WITH OTHER APPROACHES\nThe C-MAPSS dataset has been widely used as a bench-\nmark dataset in prognostics, and many approaches have\nbeen applied to this dataset. To validate the superior-\nity of our approach, the performance of the proposed\nLSTM-Transformer fusion model on the C-MAPSS dataset\nis compared with other state-of-the-art approaches. These\napproaches are based on CNN, LSTM, Transformer or their\nvarious combinations. Table 10 and Table 11 provide com-\nparisons of RMSE and Score between different approaches,\nwhere N/A indicates that the results were not provided in\nthe original paper. The first and second-best results for each\nsub-dataset are highlighted in red and blue, respectively.\nIn addition, the improvement (IMP) of LSTM-Transformer\nover other approaches is calculated in these tables.\nVOLUME 11, 2023 52681\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nFIGURE 14. Comparisons between the estimated RUL and real RUL of testing engines in sub-dataset FD003.\nFIGURE 15. The RUL estimation errors of LSTM-Transformer on four\ntesting sub-datasets.\nAs shown in Table 10, the proposed LSTM-Transformer\nobtains the 1 st best results on all four sub-datasets in\nterms of RMSE. Compared to the 2 nd best approach,\nLSTM-Transformer demonstrates significant improvements\nin terms of the RMSE metric, with respective improve-\nments of 66.35%, 12.85%, 55.78%, and 19.87% on the four\nsub-datasets in C-MAPSS. For the score metric, as shown in\nTable 11, the proposed approach demonstrated superior per-\nformance across all sub-datasets, outperforming other state-\nof-the-art approaches. Specifically, the approach achieved\nnoticeable improvements on FD001 and FD003, with score\nTABLE 10. RMSE metric comparison with state-of-the-art methods.\nreductions of 84.86% and 78.72%, respectively. Furthermore,\na significant improvement was observed on FD004, where the\nscore was reduced by 51.66%. This result is particularly note-\nworthy as FD004 is considered the most complex sub-dataset\namong the four. These findings highlight the remarkable\nability of the proposed approach to accurately estimate RUL,\neven in challenging scenarios. Overall, the results indi-\ncate the effectiveness of the proposed LSTM-Transformer\napproach and its potential to advance the field of RUL\nestimation.\n52682 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nFIGURE 16. The histogram and probability density function of the RUL estimation error of LSTM-Transformer on testing datasets.\nTABLE 11. Score metric comparison with state-of-the-art methods.\nIV. CONCLUSION\nIn this paper, novel Transformer-based fusion models are\nproposed for aero-engine RUL estimation. Firstly, the\nnetwork structure of the original Transformer model is mod-\nified and adjusted based on the characteristics of aero-engine\nsensor measurements to adapt it to RUL estimation. Next,\nthree Transformer-based fusion models are designed by\nincorporating LSTM, CNN, and their combinations into the\nencoder and decoder of the modified Transformer. This fusion\napproach aims to capture as much degradation information\nas possible from aero-engine feature data. The effective-\nness and superiority of the proposed fusion models are val-\nidated through experiments on the C-MAPSS benchmark\ndataset. The experimental results indicate that among the\nproposed fusion models, LSTM-Transformer achieves the\nbest performance and shows a clear tendency to predict\nan earlier failure in the late period of aero-engine degra-\ndation. Furthermore, LSTM-Transformer outperforms other\nexisting state-of-the-art approaches in terms of estimation\nperformance.\nIn future work, we will investigate more sophisticated\nnetwork structures to further improve the estimation accuracy\nof aero-engine RUL. Additionally, we plan to investigate the\ntransferability of the proposed fusion models to other types of\nmachinery, such as gas turbines or wind turbines, and evaluate\ntheir performance in those domains.\nVOLUME 11, 2023 52683\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\nAPPENDIX A\nNOTATIONS\nQ Query matrix in self-attention mechanism.\nK Key matrix in self-attention mechanism.\nV Value matrix in self-attention mechanism.\nAttention (·) Scaled dot-product attention function.\nXq/k/v Position-encoded feature matrices.\nWq/k/v Corresponding weight matrices of Xq/k/v.\nMultiHead (·) Multi-head attention (MHA) function.\nW O MHA weight matrix.\ny One-dimensional RUL vector.\nY RUL matrix with identical columns.\nPE(·) Position encoding function in Transformer.\nFNN (·) Feed-forward network (FNN) function.\nReLU (·) Rectified linear unit function.\nW1/2 Weight matrices in FNN.\nb1/2 Biases items in FNN.\nσ (·) Sigmoid function.\ntanh (·) Hyperbolic tangent function.\nCt Memory cell in LSTM.\n˜Ct New memory cell in LSTM.\nft Forget gate in LSTM.\nit Input gate in LSTM.\not Output gate in LSTM.\nht Output at time step t in LSTM.\nConv1D One-dimensional convolution function.\nConv2D two-dimensional convolution function.\nMaxPool1D One-dimensional maxpooling function.\nk The convolutional kernel size in convolu-\ntional layer.\nwi The kernel weights in convolutional layer.\nb The bias item in convolutional layer.\nyt The output of the convolutional layer.\nxnorm The z-score normalization.\nx Raw sensor data.\nµ The mean of raw sensor data.\nσ The standard deviation of raw sensor data.\nRMSE (·) Root mean squared error metric function.\nS (·) RUL scoring metric function.\nIQR The interquartile range.\nREFERENCES\n[1] Q. Hu, Y . Zhao, Y . Wang, P. Peng, and L. Ren, ‘‘Remaining useful life esti-\nmation in prognostics using deep reinforcement learning,’’ IEEE Access,\nvol. 11, pp. 32919–32934, 2023.\n[2] Y . Shi, W. Zhu, Y . Xiang, and Q. Feng, ‘‘Condition-based maintenance\noptimization for multi-component systems subject to a system reliability\nrequirement,’’Rel. Eng. Syst. Saf., vol. 202, Oct. 2020, Art. no. 107042.\n[3] K. Le Son, M. Fouladirad, and A. Barros, ‘‘Remaining useful life esti-\nmation on the non-homogenous gamma with noise deterioration based on\nGibbs filtering: A case study,’’ in Proc. IEEE Conf. Prognostics Health\nManage., Denver, CO, USA, Jun. 2012, pp. 1–6.\n[4] D. Tang, J. Cao, and J. Yu, ‘‘Remaining useful life prediction for engi-\nneering systems under dynamic operational conditions: A semi-Markov\ndecision process-based approach,’’ Chin. J. Aeronaut., vol. 32, no. 3,\npp. 627–638, Mar. 2019.\n[5] Z. Zhang, X. Si, C. Hu, and Y . Lei, ‘‘Degradation data analysis and remain-\ning useful life estimation: A review on Wiener-process-based methods,’’\nEur. J. Oper. Res., vol. 271, no. 3, pp. 775–796, Dec. 2018.\n[6] J. Wu, K. Hu, Y . Cheng, H. Zhu, X. Shao, and Y . Wang, ‘‘Data-driven\nremaining useful life prediction via multiple sensor signals and deep long\nshort-term memory neural network,’’ ISA Trans., vol. 97, pp. 241–250,\nFeb. 2020.\n[7] S. V ollert and A. Theissler, ‘‘Challenges of machine learning-based RUL\nprognosis: A review on NASA’s C-MAPSS data set,’’ in Proc. 26th IEEE\nInt. Conf. Emerg. Technol. Factory Autom. (ETFA), Vasteras, Sweden,\nSep. 2021, pp. 1–8.\n[8] Z. Xu and J. H. Saleh, ‘‘Machine learning for reliability engineering and\nsafety applications: Review of current status and future opportunities,’’ Rel.\nEng. Syst. Saf., vol. 211, Jul. 2021, Art. no. 107530.\n[9] P. Vrignat, F. Kratz, and M. Avila, ‘‘Sustainable manufacturing, mainte-\nnance policies, prognostics and health management: A literature review,’’\nRel. Eng. Syst. Saf., vol. 218, Feb. 2022, Art. no. 108140.\n[10] R. Khelif, B. Chebel-Morello, S. Malinowski, E. Laajili, F. Fnaiech,\nand N. Zerhouni, ‘‘Direct remaining useful life estimation based on\nsupport vector regression,’’ IEEE Trans. Ind. Electron., vol. 64, no. 3,\npp. 2276–2285, Mar. 2017.\n[11] K. Javed, R. Gouriveau, and N. Zerhouni, ‘‘A new multivariate approach\nfor prognostics based on extreme learning machine and fuzzy clustering,’’\nIEEE Trans. Cybern., vol. 45, no. 12, pp. 2626–2639, Dec. 2015.\n[12] T. Berghout, L.-H. Mouss, O. Kadri, L. Saïdi, and M. Benbouzid, ‘‘Aircraft\nengines remaining useful life prediction with an adaptive denoising online\nsequential extreme learning machine,’’ Eng. Appl. Artif. Intell., vol. 96,\nNov. 2020, Art. no. 103936.\n[13] P. J. G. Nieto, E. García-Gonzalo, F. S. Lasheras, and F. J. de Cos Juez,\n‘‘Hybrid PSO–SVM-based method for forecasting of the remaining useful\nlife for aircraft engines and evaluation of its reliability,’’ Rel. Eng. Syst.\nSaf., vol. 138, pp. 219–231, Jun. 2015.\n[14] C. Ordóñez, F. S. Lasheras, J. Roca-Pardiñas, and F. J. D. C. Juez,\n‘‘A hybrid ARIMA–SVM model for the study of the remaining useful\nlife of aircraft engines,’’ J. Comput. Appl. Math., vol. 346, pp. 184–191,\nJan. 2019.\n[15] Y . Wu, M. Yuan, S. Dong, L. Lin, and Y . Liu, ‘‘Remaining useful life\nestimation of engineered systems using vanilla LSTM neural networks,’’\nNeurocomputing, vol. 275, pp. 167–179, Jan. 2018.\n[16] J. Wang, G. Wen, S. Yang, and Y . Liu, ‘‘Remaining useful life estimation\nin prognostics using deep bidirectional LSTM neural network,’’ in Proc.\nPrognostics Syst. Health Manage. Conf. (PHM-Chongqing), Chongqing,\nChina, Oct. 2018, pp. 1037–1042.\n[17] H. Liu, Z. Liu, W. Jia, and X. Lin, ‘‘A novel deep learning-based encoder–\ndecoder model for remaining useful life prediction,’’ in Proc. Int. Joint\nConf. Neural Netw. (IJCNN), Budapest, Hungary, Jul. 2019, pp. 1–8.\n[18] X. Song, K. Chen, X. Li, J. Sun, B. Hou, Y . Cui, B. Zhang, G. Xiong,\nand Z. Wang, ‘‘Pedestrian trajectory prediction based on deep convolu-\ntional LSTM network,’’ IEEE Trans. Intell. Transp. Syst., vol. 22, no. 6,\npp. 3285–3302, Jun. 2021.\n[19] G. S. Babu, P. Zhao, and X.-L. Li, ‘‘Deep convolutional neural network\nbased regression approach for estimation of remaining useful life,’’ in\nDatabase Systems for Advanced Applications, vol. 9642, S. B. Navathe,\nW. Wu, S. Shekhar, X. Du, X. S. Wang, and H. Xiong, Eds. Cham,\nSwitzerland: Springer, 2016, pp. 214–228.\n[20] X. Li, Q. Ding, and J.-Q. Sun, ‘‘Remaining useful life estimation in\nprognostics using deep convolution neural networks,’’ Rel. Eng. Syst. Saf.,\nvol. 172, pp. 1–11, Apr. 2018.\n[21] H. Li, W. Zhao, Y . Zhang, and E. Zio, ‘‘Remaining useful life prediction\nusing multi-scale deep convolutional neural network,’’ Appl. Soft Comput.,\nvol. 89, Apr. 2020, Art. no. 106113.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Advances in\nNeural Information Processing Systems, vol. 30, I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.\nRed Hook, NY , USA: Curran Associates, 2017.\n[23] Y . Mo, Q. Wu, X. Li, and B. Huang, ‘‘Remaining useful life estimation\nvia transformer encoder enhanced by a gated convolutional unit,’’ J. Intell.\nManuf., vol. 32, no. 7, pp. 1997–2006, Oct. 2021.\n[24] A. Saxena, K. Goebel, D. Simon, and N. Eklund, ‘‘Damage propagation\nmodeling for aircraft engine run-to-failure simulation,’’ in Proc. Int. Conf.\nPrognostics Health Manage., Denver, CO, USA, Oct. 2008, pp. 1–9.\n[25] H.-K. Wang, Y . Cheng, and K. Song, ‘‘Remaining useful life estimation\nof aircraft engines using a joint deep learning model based on TCNN and\ntransformer,’’Comput. Intell. Neurosci., vol. 2021, pp. 1–14, Nov. 2021.\n52684 VOLUME 11, 2023\nQ. Hu et al.: Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation\n[26] Z. Zhang, W. Song, and Q. Li, ‘‘Dual-aspect self-attention based on trans-\nformer for remaining useful life prediction,’’ IEEE Trans. Instrum. Meas.,\nvol. 71, pp. 1–11, 2022.\n[27] Y . Ding and M. Jia, ‘‘A convolutional transformer architecture for remain-\ning useful life estimation,’’ in Proc. Global Rel. Prognostics Health Man-\nage. (PHM-Nanjing), Nanjing, China, Oct. 2021, pp. 1–7.\n[28] L. Liu, X. Song, and Z. Zhou, ‘‘Aircraft engine remaining useful life\nestimation via a double attention-based data-driven architecture,’’ Rel. Eng.\nSyst. Saf., vol. 221, May 2022, Art. no. 108330.\n[29] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classification\nwith deep convolutional neural networks,’’ Commun. ACM, vol. 60, no. 6,\npp. 84–90, May 2017.\n[31] L.-H. Ren, Z.-F. Ye, and Y .-P. Zhao, ‘‘Long short-term memory neural\nnetwork with scoring loss function for aero-engine remaining useful life\nestimation,’’ Proc. Inst. Mech. Eng., G, J. Aerosp. Eng., vol. 237, no. 3,\npp. 547–560, Mar. 2023.\n[32] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\nJan. 2017, arXiv:1412.6980.\n[33] D. K. Frederick, J. A. DeCastro, and J. S. Litt, ‘‘User’s guide for the\ncommercial modular aero-propulsion system simulation (C-MAPSS),’’\nNASA/ARL, Cleveland, OH, USA, Tech. Memorandum NASA/TM-2007-\n215026, 2007.\n[34] C. Huang, H. Huang, and Y . Li, ‘‘A bidirectional LSTM prognostics\nmethod under multiple operational conditions,’’ IEEE Trans. Ind. Elec-\ntron., vol. 66, no. 11, pp. 8792–8802, Nov. 2019.\n[35] L. Peel, ‘‘Data driven prognostics using a Kalman filter ensemble of neural\nnetwork models,’’ in Proc. Int. Conf. Prognostics Health Manage., Denver,\nCO, USA, Oct. 2008, pp. 1–6.\n[36] F. O. Heimes, ‘‘Recurrent neural networks for remaining useful life estima-\ntion,’’ in Proc. Int. Conf. Prognostics Health Manage., Denver, CO, USA,\nOct. 2008, pp. 1–6.\n[37] E. Parzen, ‘‘On estimation of a probability density function and mode,’’\nAnn. Math. Statist., vol. 33, no. 3, pp. 1065–1076, Sep. 1962.\n[38] G. Sateesh Babu, P. Zhao, and X.-L. Li, ‘‘Deep convolutional neural net-\nwork based regression approach for estimation of remaining useful life,’’\nin Database Systems for Advanced Applications, vol. 9642, S. B. Navathe,\nW. Wu, S. Shekhar, X. Du, X. S. Wang, and H. Xiong, Eds. Cham,\nSwitzerland: Springer, 2016, pp. 214–228.\n[39] L. Wen, Y . Dong, and L. Gao, ‘‘A new ensemble residual convolutional\nneural network for remaining useful life estimation,’’ Math. Biosci. Eng.,\nvol. 16, no. 2, pp. 862–880, 2019.\n[40] S. Zheng, K. Ristovski, A. Farahat, and C. Gupta, ‘‘Long short-term\nmemory network for remaining useful life estimation,’’ in Proc. IEEE Int.\nConf. Prognostics Health Manage. (ICPHM), Dallas, TX, USA, Jun. 2017,\npp. 88–95.\n[41] Y . Liao, L. Zhang, and C. Liu, ‘‘Uncertainty prediction of remaining useful\nlife using long short-term memory network based on bootstrap method,’’\nin Proc. IEEE Int. Conf. Prognostics Health Manage. (ICPHM), Seattle,\nWA, USA, Jun. 2018, pp. 1–8.\n[42] A. Elsheikh, S. Yacout, and M.-S. Ouali, ‘‘Bidirectional handshaking\nLSTM for remaining useful life prediction,’’ Neurocomputing, vol. 323,\npp. 148–156, Jan. 2019.\n[43] Y . Zhang, P. Hutchinson, N. A. J. Lieven, and J. Nunez-Yanez, ‘‘Remaining\nuseful life estimation using long short-term memory neural networks and\ndeep fusion,’’ IEEE Access, vol. 8, pp. 19033–19045, 2020.\n[44] S. Xiang, Y . Qin, J. Luo, H. Pu, and B. Tang, ‘‘Multicellular LSTM-based\ndeep learning model for aero-engine remaining useful life prediction,’’ Rel.\nEng. Syst. Saf., vol. 216, Dec. 2021, Art. no. 107927.\n[45] J. Xia, Y . Feng, C. Lu, C. Fei, and X. Xue, ‘‘LSTM-based multi-layer\nself-attention method for remaining useful life estimation of mechanical\nsystems,’’Eng. Failure Anal., vol. 125, Jul. 2021, Art. no. 105385.\n[46] J. Li, X. Li, and D. He, ‘‘A directed acyclic graph network combined with\nCNN and LSTM for remaining useful life prediction,’’ IEEE Access, vol. 7,\npp. 75464–75475, 2019.\n[47] H. Liu, Z. Liu, W. Jia, and X. Lin, ‘‘Remaining useful life prediction using\na novel feature-attention-based end-to-end approach,’’ IEEE Trans. Ind.\nInformat., vol. 17, no. 2, pp. 1197–1207, Feb. 2021.\n[48] S. Jose, R. H. Ngouna, K. T. P. Nguyen, and K. Medjaher, ‘‘Solving time\nalignment issue of multimodal data for accurate prognostics with CNN-\ntransformer-LSTM network,’’ in Proc. 8th Int. Conf. Control, Decis. Inf.\nTechnol. (CoDIT), Istanbul, Turkey, vol. 1, May 2022, pp. 280–285.\nQIANKUN HUreceived the B.E. degree in power\nengineering of aircraft from the Nanjing Univer-\nsity of Aeronautics and Astronautics, in 2018,\nwhere he is currently pursuing the Ph.D. degree in\naerospace propulsion theory and engineering. His\nresearch interests include reinforcement learning\nand aero-engine fault diagnosis.\nYONGPING ZHAO received the B.E. degree in\nthe thermal energy and power engineering field\nand the M.S. and Ph.D. degrees from the Nanjing\nUniversity of Aeronautics and Astronautics, Nan-\njing, China, in 2004, 2009, and 2013, respectively.\nHe is currently a Professor with the College of\nEnergy and Power Engineering, Nanjing Univer-\nsity of Aeronautics and Astronautics. His research\ninterests include aircraft engine modeling, control\nand fault diagnostics, machine learning, and pat-\ntern recognition. He received the award of the Nominated for the National\nExcellent Doctoral Dissertation Award of China in 2013.\nLIHUA RENreceived the B.E. and Ph.D. degrees\nfrom the Nanjing University of Aeronautics and\nAstronautics. He is currently a Post-Doctoral\nresearcher with the Nanjing University of Aero-\nnautics and Astronautics. His research interests\ninclude machine learning, deep learning, and\naero-engine prognostic and health management.\nVOLUME 11, 2023 52685",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6998633146286011
    },
    {
      "name": "Aero engine",
      "score": 0.6726564168930054
    },
    {
      "name": "Transformer",
      "score": 0.6517679691314697
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6378839612007141
    },
    {
      "name": "Deep learning",
      "score": 0.5784019231796265
    },
    {
      "name": "Encoder",
      "score": 0.5099495649337769
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4906963109970093
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4437119960784912
    },
    {
      "name": "Artificial neural network",
      "score": 0.4431621730327606
    },
    {
      "name": "Sensor fusion",
      "score": 0.417338490486145
    },
    {
      "name": "Long short term memory",
      "score": 0.41724878549575806
    },
    {
      "name": "Machine learning",
      "score": 0.3943392038345337
    },
    {
      "name": "Recurrent neural network",
      "score": 0.27083641290664673
    },
    {
      "name": "Engineering",
      "score": 0.19086387753486633
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}