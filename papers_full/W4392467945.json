{
  "title": "Application of visual transformer in renal image analysis",
  "url": "https://openalex.org/W4392467945",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2144822630",
      "name": "Yuwei YIN",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences",
        "University of Shanghai for Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2160515717",
      "name": "Zhixian Tang",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2140246650",
      "name": "Hua-Chun Weng",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences",
        "University of Shanghai for Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2144822630",
      "name": "Yuwei YIN",
      "affiliations": [
        "University of Shanghai for Science and Technology",
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2160515717",
      "name": "Zhixian Tang",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2140246650",
      "name": "Hua-Chun Weng",
      "affiliations": [
        "University of Shanghai for Science and Technology",
        "Shanghai University of Medicine and Health Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4220734654",
    "https://openalex.org/W4311775689",
    "https://openalex.org/W4313456840",
    "https://openalex.org/W3123540939",
    "https://openalex.org/W2041352300",
    "https://openalex.org/W2766069452",
    "https://openalex.org/W4313452394",
    "https://openalex.org/W4200099596",
    "https://openalex.org/W4229038815",
    "https://openalex.org/W4378714237",
    "https://openalex.org/W6796417832",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W4220799648",
    "https://openalex.org/W3131121492",
    "https://openalex.org/W6600234944",
    "https://openalex.org/W4288050040",
    "https://openalex.org/W4387459634",
    "https://openalex.org/W4323980924",
    "https://openalex.org/W4361208283",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3164746467",
    "https://openalex.org/W4386075611",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W4281659556",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W3178812510",
    "https://openalex.org/W6600605178",
    "https://openalex.org/W4312661927",
    "https://openalex.org/W3212933375",
    "https://openalex.org/W6600005384",
    "https://openalex.org/W4387430177",
    "https://openalex.org/W4394745150",
    "https://openalex.org/W4225016626",
    "https://openalex.org/W4285814354",
    "https://openalex.org/W4315815851",
    "https://openalex.org/W4205988081",
    "https://openalex.org/W4313854855",
    "https://openalex.org/W4313525678",
    "https://openalex.org/W2979808779",
    "https://openalex.org/W4296106323",
    "https://openalex.org/W3204255739",
    "https://openalex.org/W4317639911",
    "https://openalex.org/W3014974815",
    "https://openalex.org/W4312193499",
    "https://openalex.org/W2990230185",
    "https://openalex.org/W3048572146",
    "https://openalex.org/W4211031152",
    "https://openalex.org/W4386160755",
    "https://openalex.org/W2962900979",
    "https://openalex.org/W6603957568",
    "https://openalex.org/W4306678312",
    "https://openalex.org/W4377235560",
    "https://openalex.org/W4378188686",
    "https://openalex.org/W4367051574",
    "https://openalex.org/W4310656841",
    "https://openalex.org/W4281260326",
    "https://openalex.org/W4385647280",
    "https://openalex.org/W4390345094",
    "https://openalex.org/W3213925660",
    "https://openalex.org/W3206685025",
    "https://openalex.org/W4313525578",
    "https://openalex.org/W4319300975",
    "https://openalex.org/W4297094423",
    "https://openalex.org/W4387211248",
    "https://openalex.org/W4327704817",
    "https://openalex.org/W4313641702",
    "https://openalex.org/W4220739817",
    "https://openalex.org/W4375869151",
    "https://openalex.org/W4211136945",
    "https://openalex.org/W4313229413",
    "https://openalex.org/W4324118452",
    "https://openalex.org/W4288809219",
    "https://openalex.org/W3103663607",
    "https://openalex.org/W4322102572",
    "https://openalex.org/W4377970326",
    "https://openalex.org/W4360994866",
    "https://openalex.org/W4362696912",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4311088269",
    "https://openalex.org/W2941964694",
    "https://openalex.org/W4319993048",
    "https://openalex.org/W3021399285",
    "https://openalex.org/W3180576354",
    "https://openalex.org/W4312703232",
    "https://openalex.org/W2783755104",
    "https://openalex.org/W4362694317",
    "https://openalex.org/W4382240309",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W4320085508",
    "https://openalex.org/W4226085666",
    "https://openalex.org/W4392238058",
    "https://openalex.org/W4312568229",
    "https://openalex.org/W4385664412",
    "https://openalex.org/W4385656546",
    "https://openalex.org/W4225567746",
    "https://openalex.org/W4364376993",
    "https://openalex.org/W4211186538",
    "https://openalex.org/W4213099919"
  ],
  "abstract": "Abstract Deep Self-Attention Network (Transformer) is an encoder–decoder architectural model that excels in establishing long-distance dependencies and is first applied in natural language processing. Due to its complementary nature with the inductive bias of convolutional neural network (CNN), Transformer has been gradually applied to medical image processing, including kidney image processing. It has become a hot research topic in recent years. To further explore new ideas and directions in the field of renal image processing, this paper outlines the characteristics of the Transformer network model and summarizes the application of the Transformer-based model in renal image segmentation, classification, detection, electronic medical records, and decision-making systems, and compared with CNN-based renal image processing algorithm, analyzing the advantages and disadvantages of this technique in renal image processing. In addition, this paper gives an outlook on the development trend of Transformer in renal image processing, which provides a valuable reference for a lot of renal image analysis.",
  "full_text": "Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nREVIEW\nYin et al. BioMedical Engineering OnLine           (2024) 23:27  \nhttps://doi.org/10.1186/s12938-024-01209-z\nBioMedical Engineering\nOnLine\nApplication of visual transformer in renal \nimage analysis\nYuwei Yin1,2, Zhixian Tang2* and Huachun Weng1,2* \nAbstract \nDeep Self-Attention Network (Transformer) is an encoder–decoder architectural model \nthat excels in establishing long-distance dependencies and is first applied in natural \nlanguage processing. Due to its complementary nature with the inductive bias of con-\nvolutional neural network (CNN), Transformer has been gradually applied to medical \nimage processing, including kidney image processing. It has become a hot research \ntopic in recent years. To further explore new ideas and directions in the field of renal \nimage processing, this paper outlines the characteristics of the Transformer network \nmodel and summarizes the application of the Transformer-based model in renal image \nsegmentation, classification, detection, electronic medical records, and decision-mak-\ning systems, and compared with CNN-based renal image processing algorithm, analyz-\ning the advantages and disadvantages of this technique in renal image processing. In \naddition, this paper gives an outlook on the development trend of Transformer in renal \nimage processing, which provides a valuable reference for a lot of renal image analysis.\nKeywords: Deep learning, Transformer, Convolutional neural network, Attention \nmechanism, Kidney disease\nBackground\nKidney disease is a series of infections caused by kidney damage in function, morphol -\nogy, or structure. Common kidney diseases include glomerulonephritis, pyelonephritis, \ndiabetic nephropathy, hypertensive nephropathy, kidney stones, etc. Glomerulonephritis \nand diabetic nephropathy are the leading causes of chronic kidney failure. Today, ten \npercent of the world’s population suffers from chronic kidney disease (CKD), which has \nbecome one of the most prevalent and fatal diseases and seriously affects people’s health \n[1]. Kidney stones disease (KSD) is a common disease caused by solid mineral deposits \nthat form in the kidneys [2]. According to the World Health Organization, approximately \n5–10% of the global adult population suffers from kidney stones, with 10% and 14% in \nsome developed countries in Europe and North America, respectively [3]. Meanwhile, \nkidney stones have been on the rise in the past decades. Renal cancer is a common uro -\nlogical malignancy, with more than 4 million new cases diagnosed yearly [4]. Therefore, \nimproving the accuracy of diagnosis and early detection rate of nephrolithiasis is very \nimportant for the treatment and prognosis of patients.\n*Correspondence:   \ntangzx@sumhs.edu.cn; \nWenghc@sumhs.edu.cn\n1 The College of Health Sciences \nand Engineering, University \nof Shanghai for Science \nand Technology, 516 Jungong \nHighway, Yangpu Area, \nShanghai 200093, China\n2 The College of Medical \nTechnology, Shanghai University \nof Medicine & Health Sciences, \n279 Zhouzhu Highway,  Pudong \nNew Area, Shanghai 201318, \nChina\nPage 2 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nWith the development of digital medical technology, medical image processing \ntechnology has also been rapidly developed and has become one of the crucial tools \nin the medical field, especially in diagnosing renal diseases. Several medical imag -\ning modalities exist, such as ultrasonography, computed tomography (CT) [5 ], and \nmagnetic resonance imaging (MRI) [6 ]. However, imaging tests may require longer \nscanning times, and diagnostic images need more time and effort from healthcare \nprofessionals. Long-term fatigue of healthcare workers is likely to result in subjec -\ntive misdiagnosis or underdiagnosis.\nSome studies have shown that using machine learning in medical imaging can \nreduce the possibility of diagnostic errors and thus effectively improve diagnostic \naccuracy [2 ]. Therefore, improving the ability and automation of image analysis is a \nwidespread issue in medical research today. Deep learning, as a branch of machine \nlearning, has been tried to be applied in diagnosing CKD and predicting the decline \nof renal function [7 ], renal insufficiency, and diabetic nephropathy.\nDeep Self-Attention Network (Transformer), as a new type of sequence model, \nhas been widely recognized for its excellent performance in fields such as natural \nlanguage processing [8 ]. Kidney CT/MRI images are sequential structural data with \ncomplex structural correlations between different parts. The transformer can simul -\ntaneously learn the contextual information of other parts of kidney images through \nthe mechanism of multi-attention and capture the global structural relationship of \nthe images more comprehensively and accurately to improve the recognition effect. \nThe focus of current research is how to introduce it into medical image processing, \nespecially in kidney disease. Moreover, the Transformer framework is more general, \nand the trained base model can be used for other renal image analysis tasks, such \nas classification, detection, segmentation, etc. This paper outlines the current stage \nof the Transformer’s application in kidney image classification, segmentation, and \ndetection and compares it with traditional CNN models.\nIntroduction to transformer\nThe Transformer model is the first transduction model that relies exclusively on \nself-attention to compute its input and output representations without recur -\nrent neural networks (RNNs) or CNNs for sequence comparison [8 ]. Compared to \ncommonly used models such as RNNs and CNNs, Transformer has a higher par -\nallel computation capability due to an attentional mechanism that simultaneously \nallows the computation to consider all input words or characters. Moreover, the \nself-attention mechanism can effectively handle long sequential data and improve \nthe modeling ability of long-range dependencies. The transformer abstracts the \nencoder and decoder into individual modules (as shown in Fig.  1). In the encoder, \nthe inputs are mapped to a multidimensional space, and the input representation is \nlearned through the multi-head self-attention mechanism. The feed-forward neural \nnetwork uses the ReLU transform for the nonlinear transformation. In the decoder, \nthe model also uses a standard attentional mechanism to compute the attentional \nweights between the input and its corresponding context for the decoding operation.\nPage 3 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nVision transformer\nVision transformer (ViT) is the application of Transformer models to computer \nvision, especially for image classification tasks. ViT transforms images into sequences \nby segmenting them into different paths and encodes and classifies them using stand -\nard Transformer models. Compared to traditional CNN models, ViT is based on a \nmulti-head self-attention mechanism [9 ], which can adapt to inputs of different sizes \nand shapes, provides better flexibility, and allows migration learning after large-scale \npre-training.\nSelf‑attention\nSelf-attention is a unique mechanism for computing the interaction between two ele -\nments in a sequence. Given a sequence input, each element can be used simultaneously \nas a query, key, and value. The attention function can be described as mapping a query \nand a set of key-value pairs to an output, where the query, key, value, and production \nare vectors. The outcome is computed as a weighted sum of values, where the weights \nassigned to each value are calculated by the compatibility function of the query with the \ncorresponding key [8]. These weights can be used in a weighted sum to add the encoded \nvector at that location.\nFig. 1 Basic structure of transformer\nPage 4 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nMulti‑head attention\nMulti-head attention is a combination of multiple sets of self-attention mechanisms, \neach mapping between a pair of queries, keys, and values, thus allowing the model to \nsimultaneously attend to different representation subspaces from other locations [8 ]. \nIn this process, multiple attention mechanisms form a \"head\", each getting a sepa -\nrate set of queries, keys, and values and generating the corresponding output [10]. It \ncaptures multiple feature representations simultaneously and detects the relationship \nbetween different parts, thus developing more contextual relevance and significantly \nimproving the model’s ability to learn the original drawing.\nOther techniques\nRecent studies have found that multilayer perceptual network (MLP) models excel in \nsolving image tasks without convolution or self-attention mechanisms. Such mod -\nels learn representations only through basic linear algebra operations, which can be \ncomputed repeatedly for different spatial locations and feature channels. Despite the \nlong-term dominance of CNNs and ViT, simple MLP models perform well on specific \nkidney image processing tasks. This demonstrates that MLPs can learn efficient rep -\nresentations, opening up new ideas for deep learning. A typical example is that Saikia \net al. [11] proposed a model MLP-UNet based only on MLP architecture for glomeru -\nlar segmentation tasks. The results show that MLP-UNet performs on PAS-stained \nwhole kidney images comparable to the pre-trained model TransUNet but with a 20% \nreduction in the number of parameters without needing pre-training. The research \nadvancement of MLP models has proposed many novel architectures such as gMLP \n[12], ResMLP [13], ASMLP [14], Cyclemlp [15], etc. Transformer, CNN, and MLP \nperform differently on different tasks, and there is yet to be a unified optimal struc -\nture for deep learning. This section will focus on applying Transformer and its variant \narchitectures to the kidney image processing task to find a network architecture more \nsuitable for a specific task and thus advance the field.\nApplication of transformer in renal image processing\nCurrently, the Transformer mechanism has more applications in renal image process -\ning, mainly including image classification [16], tumor lesion segmentation [17], renal \norgan segmentation [18, 19], etc. In addition, Transformer can achieve prognostic \nassessment of renal diseases [20, 21], provide treatment plans [22], help doctors write \npathology reports [23], construct electronic medical records [24], and so on.\nUsing the Transformer mechanism, the application that can be used in renal image \nprocessing can realize the fast and accurate automated analysis and processing of \nrenal images, improve clinicians’ efficiency and diagnosis level, and bring new oppor -\ntunities and challenges for renal disease research and clinical treatment.\nTransformer applied to kidney image segmentation\nRenal cancer is now considered one of the most common malignant tumors in urol -\nogy, leading to a large number of deaths every year [19]. In the past 30 years, the num -\nber of new cases of renal cancer in China has dramatically increased from 110,700 to \nPage 5 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \n598,300 cases [25]. Traditionally, the lesion areas of renal cancer patients are mainly \nidentified by clinicians’ depiction, which relies heavily on the clinical experience of \ndoctors and is very time-consuming and prone to erroneous judgment. Accurate \nmeasurements from medical images can help doctors make accurate diagnoses and \nprovide timely treatment. Medical image segmentation aims at identifying tumors \nand depicting different sub-regions of an organ from the corresponding background \nby assigning labels of predefined categories to each pixel in a medical image, e.g., CT \n[5] MRI [6]. Therefore, the emergence of automatic medical image segmentation tech -\nniques is crucial to improve the accuracy and efficiency of clinical diagnosis.\nCNN-based and U-Net-based [26] medical image segmentation algorithms have per -\nformed better in recent years. Still, based on the limitations of convolutional operations, \nthey cannot capture long-range relationships. To solve this problem, there have been \nsome research works applying network models based on Transformer with improve -\nments to kidney image segmentation and have achieved good results. In renal image \nsegmentation, more application scenarios are renal organ segmentation, renal lesion \nsegmentation, and automatic target area outlining.\nIn this paper, the goodness of segmentation performance is usually expressed in \nterms of the following metrics. Dice similarity coefficient (DSC): measures the over -\nlap between the segmentation result and the ground truth. Hausdorff distance (HD): \ncomputes the maximum distance between two sets, assessing differences between the \npredicted boundary and the ground truth boundary. IOU (Intersection over Union): cal -\nculates the ratio of the intersection area to the union area of the predicted region and \nground truth, reflecting the degree of overlap. MIoU (mean IOU): represents the average \nIOU values of multiple samples, offering a comprehensive evaluation of model perfor -\nmance. F1 Score: considers both precision and recall, providing a balanced assessment of \nclassification model performance. AUC (area under the ROC curve): reflects the overall \nperformance of a classification model by measuring the relationship between true and \nfalse positive rates at different thresholds. Accuracy (ACC): indicates the model’s overall \nclassification performance. Sensitivity: measures the correct identification rate of posi -\ntive cases. Specificity: measures the correct identification rate of negative cases.\nModels for renal image processing based on transformers typically employ simple \nrandom rotation data augmentation for preprocessing, using cross-entropy as the loss \nfunction and optimization methods such as SGD and Adam. Regularization techniques \ninclude dropout and weight decay. Key hyperparameters encompass the learning rate \n(usually ranging from 1e−4 to 1e−5), batch size (4 to 16), and dropout rate (0.1 to 0.3)\n[27].\nMulti‑organ segmentation of the abdomen\nAccurate kidney organ segmentation can provide clinicians with important informa -\ntion, and the task is often integrated with abdominal multi-organ segmentation. In the \nabdominal multi-organ segmentation task, the algorithm needs to segment all the organs \nin the abdomen at once. The synapse dataset (https: //doi.org/https:// doi. org/ 10. 7303/ \nsyn31 93805) is the most common publicly available dataset for abdominal multi-organ \nsegmentation. Previous researchers usually use CNN for multi-organ segmentation [28]. \nThe algorithm needs to consider global and local information to improve further the \nPage 6 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nsegmentation accuracy, which led to a combined Transformer–CNN model. According \nto the way of combining CNN and Transformer, hybrid Transformer model methods are \nusually classified into three categories (as shown in Fig. 2):\n①Methods based on the encoder of the Transformer model.\n②Methods using the Transformer model between the encoder and decoder.\n③Methods utilizing the decoder based on the Transformer model-based decoder \nmethods.\nThe first class of methods aims at extracting higher-quality coded features by tak -\ning advantage of the Transformer model’s strength in modeling remote dependencies. \nTherefore, some studies have directly adopted a Transformer as an encoder. TransU -\nNet is a successful attempt to introduce a Transformer into medical image segmenta -\ntion tasks [29]. This model employs a Transformer as an encoder, which combines \nthe strengths of a Transformer and U-Net and can extract the global context from the \nlabeled image chunks. At the same time, the Transformer helps to capture spatial rela -\ntions over long-range distances. Compared with V-Net, AttnUNet, and ViT, TransUNet \nperforms better on multi-organ and heart segmentation tasks. In this way, TransUNet \ncan handle large image sizes without the memory constraints of traditional encoder–\ndecoder models. Similarly, there are many models inspired by the U-shaped architecture \ndescribed above. For example, Atek [30] et  al. designed a two-scale encoder (Swin-\nTransformer) U-shaped architecture (SwinT-Unet) to integrate the Shift Window (Swin) \nTransformer module and the Transformer Interactive Fusion (TIF) module. Models \nincorporating hierarchical SwinT modules into the decoder include UNETR [31], Swin-\nUnet [32], TransClaw U-Net [33], MISSFormer [34], and others. In order to aggregate \nfeatures from multiple scales of an image, many methods propose a Transformer model \nblock based on parallel shift windows to improve SwinT. For, Feng et al. [35] proposed \nthe ConvWin-UNet structure, which combines the ConvWin, Transformer, and UNet \nand utilizes the W-MSA (weighted multi-scale aggregation) mechanism and convolu -\ntion operation to accelerate the convergence and enrich the information exchange \nFig. 2 a–c Shows the transformer model-based encoder method, the method of using the transformer \nmodel between encoder and decoder, and the decoder based on the transformer model\nPage 7 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nbetween patches. Using convolutional window operations for each convolutional layer \nin the encoder and decoder, the model achieves an Average DSC of 79.39% and an HD of \n21.39 mm in the Synapse dataset. And to deal with multiple related tasks simultaneously, \nsome methods U-Net based on the introduction of numerous parallel branches, such as \nWang [36] designed a hybrid MT-UNet network; MTM first computes the self-factor \nefficiently by Local–Global Gaussian Weighted Self-Attention (LGG-SA) and then mines \nthe interconnections between data samples by external attention (EA). The MT-UNet \nmodel achieved 78.59% and 90.43% DSC on the Synapse and ACDC datasets, respec -\ntively. Finally, a U-shaped model is constructed for accurate medical image segmenta -\ntion. The method consistently outperforms Trans-Unet and other visual Transformers \nfor complex-shaped organ segmentation (e.g., liver and left kidney).\nUnlike the above approaches, the second class of techniques aims to enhance the \nnetwork’s modeling capability in cross-layer feature transfer by incorporating a Trans -\nformer between the encoder and decoder to improve the performance of the segmenta -\ntion task. For example, Zhou et al. [37] proposed a new 3D Transformer model called \nnnFormer. nnFormer introduces a self-attention mechanism based on local and global \nvolumes to learn 3D volume representations and uses skip attention instead of skip con -\nnections to improve further the performance, which can be used to use less computa -\ntional cost to model global feature relationships efficiently. The model achieved 86.4%, \n86.57%, and 92.06% DSC on the BraTs2016, 2017, Synapse, and ACDC datasets. Outper-\nformed the LeViT-UNet-384s and TransUNet and was more advantageous in segment -\ning the pancreas and the stomach in terms of mean HD and DSC, respectively. Similarly, \nintroducing the cross-attention cross-convolution Transformer module instead of skip \nconnections are DAE-Former [38], DSTUNet [39], and so on.\nThe third class of methods incorporates the Transformer into the encoder. For exam -\nple, the Trans-U model proposed by Guo et al. [40] uses the combined high-resolution \npositional data from CNN features and the global context stored by the Transformer \nto compensate for the loss of feature resolution caused by the Transformer. The DSC \nresult of this model on the Synapse dataset is 76.56%, which is lower than the U-Net \nand attnUNet models. The main reason is that the Transformer cannot extract low-level \nmorphological details in medical images. However, it works well in capturing high-level \nsemantic information that helps classify. For this reason, it is proposed to combine the \nTransformer with U-Net and let the Transformer learn the positional features through \nthe jump connection of U-Net so that the model can utilize the high-level semantics \nas well as consider the low-level morphology and may obtain better results in medical \nimage segmentation.\nUnlike the U-shaped model-based approach mentioned above, to enhance the Trans -\nformer network’s ability in local feature extraction, Wang et al. [41] proposed the use of a \npyramid structure to construct multiscale representations and deal with multiscale vari -\nations, firstly, using a lightweight convolutional layer to extract the low-level features and \nreduce the amount of data, and then, using the Transformer block and the convolution \nblock’s mixture of Transformer blocks and Convolutional blocks to handle high-level \nfeatures. Models with similar ideas include ECT-NAS [42], C2Former [43], CASTformer \n[44], etc. Niu et al. [45] proposed a novel symmetric supervised network based on the \ntraditional two-branch approach, which utilizes a symmetric supervisory mechanism to \nPage 8 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nenhance the supervision of the network training and introduces a Transformer-based \nglobal feature alignment module to improve the global consistency between the two \nbranches. Compared with the baseline SE-Net [46], the method improved by 16.9% \nand 25.98% on the MS-CMRSeg and CHAOS datasets, respectively, and showed sig -\nnificant performance in the multi-organ left and suitable kidney segmentation experi -\nments with 78.46% and 81.45%, respectively. To solve the problem of information loss \nor resolution degradation due to downsampling or cutting of the input image by tra -\nditional Transformer, Themyr et  al. [47] proposed a full-resolution memory (FINE) \nTransformer model, which learns the memory Token by learning the memory Token, \nwhich scales well in terms of memory and computational cost, and allows for localized \nimage segmentation. It scales well and interacts with local image regions and all 3D \nvolumetric regions. FINE has better performance and superiority over CNN and recent \nTransformer model baselines (e.g., CoTr [48] and nnFormer [37]) to focus entirely on \nhigh-resolution images. FINE obtained 87.1% DSC and better segmentation of small and \ncomplex organs such as the pancreas (Pa) and gallbladder (Gb).\nFurthermore, to reduce the dependence on expensive labeled kidney data and to be \nmore efficient in data acquisition, Wang et  al. [49] proposed a cross-teaching semi-\nsupervised medical image segmentation model based on CNN and Transformer mod -\nels, aiming to improve the efficiency of automatic segmentation of multiple organs in \nabdominal CT. However, it was found in the validation on the FLARE2022 challenge \ndataset that the segmentation effect could have been more satisfactory. Although the \nseparation network could segment most organs, the location of organs such as kidneys \nshifted. In contrast, Xin et al. [50] used U-Net, the backbone network of nnU-Net [51], \nas the final prediction network based on the combination of CNN and Transformer. An \naverage DSC of 75.80% was obtained in the FLARE2022 challenge. To perform accurate \norgan segmentation without the need for manual annotation, Wang et al. [52] designed \na self-supervised learning-based framework for one-time kidney organ segmenta -\ntion, which is used to build a network model of global correlation between the refer -\nence samples (VALUE) and the desired segmentation samples (QUERY). Local features \nare extracted using a CNN, and then global features are removed from the local feature \nspace via a Transformer. A semantic dependency embedding method introduces channel \nand spatial standard information into the Transformer to establish global corrections. \nThe experiment compares the model with PANet [53], SENet [54] and SSL-ALPNet \n[55], and the test scenarios include observed organ settings (OO) and unobserved organ \nsettings (UO). The results show that the model outperforms the others in the MICCAI \n2015 CT and ISBI2019 MRI datasets. This demonstrates the effectiveness of using self-\nsupervised learning to train Transformer and Convolutional Hybrid Networks to handle \nbetter OO and UO scenarios in medical image segmentation tasks.\nSegmentation of renal tumors\nRenal tumor segmentation refers to accurately segmenting the tumor region in the kid -\nney from the surrounding normal tissue in medical images to quantitatively identify and \nlocate the location and extent of the renal tumor, which can effectively assist doctors in \ntumor diagnosis, treatment, and monitoring.\nPage 9 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nTo further improve the segmentation and save the running time and memory of the \nalgorithm. Some approaches apply a Transformer to the encoder for feature extrac -\ntion of kidney images. For example, Yu et al. [56] proposed UNesT, which employs a \nsimplified and faster converging Transformer model encoder design to achieve local \ncommunication between positional information by hierarchically aggregating spa -\ntially adjacent patch sequences. The model performs state-of-the-art on the four data -\nsets BTCV, KiTS2015, BraTS2021, and KiTS2021, outperforming the state-of-the-art \nintegrated model SLANT [57] in a whole-brain segmentation task. Some methods \nlearn more straightforward mappings, focusing on normalized pose and size images. \nFor example, Barbera et al. [18] proposed a new CNN architecture that contains three \nconsecutive modules: a regression module, a differentiable module, and a segmenta -\ntion module. The architecture uses a spatial Transformer model network (STN) to \nnormalize the input image to improve the accuracy of subsequent segmentation tasks. \nThe differentiable module automatically localizes the regions of interest to reduce the \nmanual labeling effort. The segmentation module uses a UNet-based architecture, \nand the model achieved good DSC scores (88.01% for kidneys and 87.12% for tumors) \nin the segmentation task for kidneys and tumors on pediatric data and KiTS19 data. \nInspired by the hierarchical structure in the visual Transformer model, Yu et  al. \n[58] proposed a method to segment kidney components using a 3D block aggrega -\ntion Transformer model. They constructed a kidney substructure segmentation data -\nset containing 116 subjects. The model enables localized communication between \nsequential representations without changing the self-attention mechanism. It showed \nadvanced performance in the segmentation task with a DSC metric of 84.67%. Bous -\nsaid et al. [59] used the spatial Transformer model and linear subspace projection to \ncompare segmentation masks in feature space and to characterize global shape prop -\nerties. The authors experimented on a 3D ultrasound dataset of left and right adult \nkidneys from 667 patients and obtained a DSC metric of 92.07%, demonstrating the \nvalidity and accuracy of the method.\nChen et  al. [17] proposed a multi-stage 2.5D semantic segmentation network for \nmulti-stage fine segmentation to address the high cost of computational resources for \nkidney mass segmentation. The first stage uses ResSENormUnet [60] combined with \ndeep residual connectivity and attention mechanism to pre-segment the kidney and pre-\ndict the approximate location and shape. In the second stage, fine segmentation is per -\nformed using the DenseTransUnet [61] network combined with dense connectivity and \nself-attention mechanism to more finely segment the contours of the kidney, tumor and \ncyst. Finally, post-processing operations based on 3D-connected regions remove possi -\nble false-positive segmentation results. The model obtained good DSC for kidney seg -\nmentation (Kidney: 94.3%, Tumor:77.79%, Cyst:70.99%), but the network approach can \nbe improved for segmenting smaller kidneys, tumors, and cysts. To enhance the spatial \nmodeling capability of the network while maintaining the efficient use of computational \nresources, Yang et al. [62] proposed that the EPT-Net network effectively combines the \nedge sensing and Transformer structures and introduces the Dual Position Transformer \nto enhance 3D spatial localization capability. Meanwhile, the Edge Weight Guidance \nmodule extracts edge information without additional network parameters. Good perfor -\nmance is demonstrated on the relabeled KiTS2019 dataset (KiTS19-M).\nPage 10 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nOutlining of the renal target area\nRadiation therapy is one of the most crucial localized treatment modalities for abdomi -\nnal malignancies (e.g., cervical, prostate, pancreatic, renal, and liver cancers). Depict -\ning abdominal organs at risk (OARs) on CT images is essential during radiation therapy \nmanagement [63]. The method currently used in clinical practice is manual contouring \nof CT images, which is often very tedious and time-consuming. The results also vary \ndepending on the skill level of the observer, environment, or equipment type. Deep \nlearning-based automated contouring techniques for segmenting OAR would help elim -\ninate these problems and produce consistent results with minimal time and labor [64].\nTraditionally, there are conditional generative adversarial network (GAN) techniques \nproposed by Seenia et al. [64] for semantic segmentation of OAR in CT images of organs \nsuch as kidneys and Pan et  al. [65] for multi-organ segmentation of abdominal CT \nimages utilizing a V-net-like structure, a U-shaped multilayer perceptron mixer (MLP-\nMixer) and a convolutional neural network (CNN). These methods need to use the image \nfeature information effectively. At the same time, Jiang et al. [66] proposed the MRRN-\nNBSA method incorporating self-attention to segment multiple key OARs of head and \nneck (HN) and abdominal organ (BTCV) datasets. Comparison of MRRN-NBSA with \nUnet using cross-attention (CCA), dual-SA, and transformer-based (UNETR) methods \nshowed that MRRN-NBSA obtained a DSC of HN: 88% and BTCV: 86%. The technique \napplies NBSA in a decoder that incorporates interactions between regional contexts \nwhile extracting non-local attentional information in a fast and memory-efficient man -\nner. Overall, the network extracts relevant feature sets to generate accurate segmenta -\ntion of organs such as kidneys by combining a deep multiresolution residual network \nand nested block (SA) self-attention to take advantage of multiscale features and self-\nattention mechanisms. To address the limitations in global and local information feature \nfusion in the classical TransUnet model decoder, Jiang et al. [67] proposed BiFTransNet, \nwhich introduces the BiFusion module into the decoder stage to achieve effective global \nand local feature fusion by enabling feature integration from various modules. It is used \nin the Synapse dataset to develop automated gastrointestinal image segmentation to \nhelp radiation oncologists accurately target the X-ray beam to the tumor.\nSummary of segmentation algorithms\nA literature search reveals that TransUNet, Swin-Unet, AgDenseU-Net 2.5D, LeViT-\nUNet, ViTBI, UNETR, and HiFormer are the more popular algorithms in the field of \nrenal medical image segmentation at present and show different degrees of advantages \nin kidney image segmentation tasks. We conducted a comprehensive evaluation of the \nabove segmentation algorithms, as shown in Table 1.\nTransformer applied to kidney image classification\nKidney image classification is categorizing kidney image data into different categories or \nlabels. With deep learning technology, kidney images can be automatically analyzed and \nclassified to provide more accurate and faster diagnostic results. This helps to improve \nthe early detection and treatment of kidney diseases. Due to the complexity of mor -\nphological and structural features of kidneys and surrounding tissues, the task of renal \nPage 11 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nTable 1 Comparison of kidney image segmentation algorithm performance\nAlgorithms Datasets Evaluation indicators/results Main views and contributions Limitations\nTransUNet [29] Synapse 2015/ACDC Synapse (DSC: 77.48%; Kidney (R): \n81.87%; Kidney (L):77.02%; HD: \n31.69 mm)/ACDC(DSC: 89.71%)\nTransUNet is the first successful attempt to introduce a \nTransformer into medical image segmentation. Com-\nbining CNN and Transformer in coding\nTransformer leads to a dramatic increase in \nthe number of model parameters\nIB-TransUNet [68] Synapse 2015 DSC: Kidney (R):79.87%\nKidney (L):83.89%\nUsing the UNet model to combine the information \nbottleneck (IB) with the Transformer\nMore advantages in learning small organ \nfeatures\nSwin-Unet [32] Synapse 2015 DSC: 79.13%\nHD: 21.55 mm\nThe information bottleneck block was innovatively \nintroduced in the encoding; a hierarchical Swin \nTransformer model with moving windows is used as \nan encoder to extract contextual features. An asym-\nmetric Swin Transformer model decoder with a patch \nextension layer is designed to perform the upsampling \noperation\nHigher dependency on large and diverse \ndatasets with a large number of parameters \nand complexity\nAgDenseU-Net 2.5D [60] KiTS 2021 DSC:\nKidney: 95%\nTumor: 87.8%\nCyst: 74.6%\nCombining the features of AggRes (which enhances \nfeature representation by aggregating residual con-\nnectivity and attention mechanisms) and DenseU-Net \n(which efficiently performs multi-scale feature fusion)\nHigher computation and memory con-\nsumption, longer training time\nLeViT-UNet [69] Synapse/ACDC Synapse (DSC: 78.53%, Kidney (R): \n80.25%, Kidney (L): 84.61%, HD: \n16.84 mm)/ACDC (DSC: 90.32%)\nUsing LeViT as the encoder of LeViT-UNet, combining \nLeViT Transformer with U-Net\nSome metrics do not reach SOTA, and the \nsegmentation performance is imaged to \nsome extent to reduce the computational \ncomplexity\nViTBIS [70] Synapse 2015 DSC: 80.45% Adding the Concat operator for merging features The dataset is more homogeneous, with \nfewer baselines for comparison\nTransClaw \nU-Net [33]\nSynapse 2015 Synapse (DSC: 78.09%, HD: 26.38 mm) Claw U-Net with Transformer\nCombined/decoder dual-path design\nRelatively homogenous data sets\nAfter-Unet [71] Thorax-85/BCV/SegTHOR \nthorax\nThorax-85 (DSC: 92.32%)/BCV (DSC: \n81.02%)/SegTHOR thorax\n(DSC: 92.10%)\nBoth intra- and inter-slice long-distance cues were \nconsidered to guide segmentation\nAxis information is naturally provided mainly \nfor 3D volume\nTransBTSV2 [19] KiTS 2019/\nBraTS2019/\nBraTS2020/\nLiTS 2017\nKiTS 2019 (DSC: KIdney: 97.37%, Tumor: \n83.69%, Composite: 90.53%)\nNot limited to brain tumor segmentation (BTS) but \nfocuses on general medical image segmentation, \nproviding a powerful and efficient 3D baseline for the \nvolumetric segmentation of medical images\nMainly for 3D medical image segmentation \ntasks\nPage 12 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nTable 1 (continued)\nAlgorithms Datasets Evaluation indicators/results Main views and contributions Limitations\nUNETR [31] BTCV/MSD BTCV (AVG: 89.1%)/MSD (DSC: 71.1%, \nHD95: 8.822 mm)\nThe Transformers encoder utilizes embedded 3D \ncorpora to capture remote dependencies efficiently; \nthe jump-join decoder combines extracted repre-\nsentations of different resolutions and predicts the \nsegmentation output\nMainly for 3D medical image segmentation\nDBT-UNETR [72] BTCV AVG:80.3% An improved SwinUNETR is proposed based on UNETR \nwith Swin Transformer as an alternative to Transformer\nNo significant improvement in performance \ncompared to UNETR\nNnFormer [37] Synapse 2015/\nACDC\nSynapse (DSC: 87.40%)/ACDC(DSC: \n91.78%)\nUtilizing a combination of cross-convolution and self-\nattention operations\nLittle performance gain on the ACDC \ndataset\nHiFormer [73] Synapse 2015 DSC:80.69% Two multi-scale representations were designed based \non the Swin transformer module and CNN encoder, \nand the Double-Level Fusion (DLF) module was \ndesigned to finely fuse the global and local features of \nthe two representations\nSingle dataset\nMPSHT [74] Synapse 2015/\nACDC\nSynapse (DSC: 79.76%, KIdney: 80.77%, \nHD: 21.55 mm)/ACDC(DSC: 91.80%)\nBased on the CNN-Transformer model hybrid model, to \nwhich the asymptotic sampling module is added\nAccuracy of segmentation to be improved\nDSGA-Net [75] Synapse 2015/\nBraTs 2020/\nACDC\nSynapse (DSC: 81.24%)/BraTs2020 (DSC: \n85.82%)/ACDC(DSC: 91.34%)\nAdd a Depth Separable Gating Visual Transformation \n(DSG-ViT) module to the code and propose a Hybrid \nThree-Branch Attention (MTA) module\nConsiderable computational burden; con-\nsumes large amounts of GPU memory\nMedNeXt [76] BTCV/AMOS22/KiTS19/\nBraTS21/AVG\nBTCV (DSC: 88.76%)/AMOS22 (DSC: \n91.77%)/KiTS19 (DSC: 91.02%)/BraTS21 \n(DSC: 91.49%)/AVG (DSC: 88.01%)\nThe use of ConvNeXt 3D and the extension of Con-\nvNeXt blocks to upsampling and downsampling layers \nrepresents a modern deep architecture for medical \nimage segmentation\nDeep Networks Dedicated to Medical Image \nSegmentation\nMESTrans [77] COVID-DS36/GlaS/Syn-\napse/I2CVB\nCOVID-DS36 (DSC: 81.23%)/GlaS (DSC: \n89.95%, IoU: 82.39)/Synapse (DSC: \n77.48%, HD:31.69 mm)/I2CVB (DSC: \n92.3%, IoU: 85.8)\nPropose a Multi-scale Embedding (MEB) and Multi-\nlayer Spatial Attention Transformer structure (SATrans) \nto adjust the sensory field. Propose a Feature Fusion \nModule (FFM) for global learning between shallow and \ndeep features\nThe performance of small organ segmenta-\ntion needs to be improved\nPage 13 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nTable 1 (continued)\nAlgorithms Datasets Evaluation indicators/results Main views and contributions Limitations\nST-Unet [78] Synapse 2015/ISIC 2018 Synapse2015(DSC:78.86%, \nHD:20.37mm)/ISIC 2018(F1:90.94%, \nmIoU:85.26)\nProposing a new Cross-Layer Feature Enhancement \n(CLFE) module for cross-layer feature learning with \nspatial and channel squeezing and excitation modules \nto highlight the saliency of specific regions\nThe accuracy of segmentation needs to be \nimproved\nCOTRNet [79] KiTS 2021 DSC:\nKidney:92.28%\nTumor:55.28%\nCyst:0.50.52%\nUtilizing pre-trained ResNet to develop the encoder, in \naddition to adding deep supervised\nThe accuracy of segmentation for masses \nand tumors needs to be improved\nCS-Unet [80] Synapse 2015 DSC:82.21%\nKidney(R):79.52%\nKidney(L):85.28%\nDesign of convolutional Swin-Transformer (CST) \nmodule that merges convolution with multi-head self-\nattention and feed-forward networks\nFacing the challenge of dealing with long-\nrange dependencies\nPage 14 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nimage classification usually needs to consider different levels of features [81], including \nrenal morphology, size, texture, and so on. Traditional CNN models have limitations in \ndealing with complex kidney morphological and structural features. In contrast, Trans -\nformer can extract multiple sets of feature representations in parallel and incorporate \na fully connected layer to fuse and classify the features, thus improving the model per -\nformance [82]. Therefore, applying Transformer to the renal image classification task \ncan improve the accuracy and sensitivity, especially for the classification of renal cysts, \ntumors, stones, etc., thus helping doctors to understand renal lesions more accurately \nand provide better treatment plans and prognosis assessment.\nClassification network model based on the combination of Transformer and CNN networks\nThe first class of approaches applies the Transformer to an encoder–decoder structure, \nwhere the encoder–decoder consists of multiple identical layers, each containing an \nAttention mechanism and a feed-forward neural network. For example, the MT-ONet \nnetwork [83], combines CNN, hybrid Transformer and LGG-SA into the encoder com -\nponent of the proposed O-Net architecture to improve the classification accuracy. The \nsecond class of approaches uses the Attention mechanism between the encoder and \ndecoder to capture the dependency between input and output. For example, the CTrans-\nPath [84] network uses a new Semi-Supervised Learning (SSL) strategy called Seman -\ntic Relatedness Contrastive Learning (SRCL), which utilizes the local features of CNNs \nmining capability and the global interaction capability of Transformer, which has some \nadvantages in solving small sample data.\nIn diagnostic pathology, whole-slice images are typically huge and often have only \noverall labels and no labels corresponding to specific instances (e.g., cells or lesions). \nThis leads to the fact that traditional supervised learning methods cannot be directly \napplied to this problem. To transform the weakly supervised classification problem into \nan overlooked learning problem, Shao et al. [16]. proposed a new framework called Mul-\ntiple Instance Learning (MIL) to explore the correlation between different instances to \nsolve the weakly supervised classification problem in pathological diagnosis based on \nthe whole section images of the kidney, based on the MIL framework, the paper designs \nA Transformer model-based MIL (i.e., TransMIL), which can efficiently handle unbal -\nanced/balanced and binary/multiple classification with good visualization and interpret -\nability. TransMIL achieved an AUC of 93.09% and TCGA-NSCLC: 96.03% TCGA-RCC: \n98.82% on the CAMELYON16 and TCGA datasets.\nCNNs are more commonly used for renal image classification tasks than Transformer \nmodels; for example, Cicalese et al. [85] proposed an uncertainty-guided Bayesian Clas -\nsification (UGBC) scheme for glomerular and renal level classification tasks. Qadir \net al. [86] used a deep migration learning model based on the DenseNet201 network to \nclassify the tumor, normal cysts and stone regions of the kidney. Aruna et al. [87] used \nnetworks such as CNN and VGG19 to diagnose polycystic kidneys, and the classifica -\ntion task covered cysts, tumors, and stones. Hossain et al. [88] used three classification \nmethods, namely, EAnet, ResNet50, and a customized CNN model, to classify the four \ntypes in CT images of the kidney (cysts, normal, stones, tumors). Chanchal et al. [89] \nproposed the RCCGNet network for fully automated renal cell carcinoma grading from \nrenal histopathology images.\nPage 15 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nSummary of classification algorithms\nIn kidney image classification, algorithms based on CNN or combining CNN and \nTransformer have become a hot research topic. These algorithms utilize the feature \nextraction capability of CNN and the sequence modeling capability of Transformer \nto improve the accuracy and efficiency of kidney image classification. In this paper, \nwe summarize some crucial algorithms, including TransMIL, CTransPath and other \nalgorithms and CNN and DNN-based algorithm models, and their performance \nis summarized and compared in detail in Table  2. This provides an opportunity to \nanalyze their strengths and limitations in depth and provides a reference for future \nresearch and applications.\nMulti‑modal image alignment\nMultimodal image alignment is aligning and matching renal image data from different \nmodalities. By aligning images from other modalities, the correlations and implied \nrelationships between them can be revealed, providing researchers with more infor -\nmation and insight. In clinical practice, doctors often need to refer to renal image data \nfrom multiple modalities simultaneously, such as MRI, CT, and ultrasound images. \nBy aligning these images, the correlation analysis between different modalities can be \nrealized, improving the accuracy of diagnosis and treatment decisions.\nChi et  al. [90] proposed a new depth alignment pipeline for free-breathing 3D CT \nand 2D ultrasound (U/S) kidney scans. The pipeline consists of a feature network and a \n3D–2D CNN-based alignment network. The feature network has hand-textured feature \nlayers to reduce semantic gaps. The alignment network adopts the encoder–decoder \nstructure of feature image mismatch (FIM), is first pre-trained with a retrospective \ndataset and training data generation strategy, i.e., the kidneys are uniformly aligned on \nthe upper and lower axes on the CT images, and then the kidneys are aligned with the \ncenter of mass on the U/S images, and successfully achieves accurate alignment between \nkidneys on CT and U/S images. The pipeline solves the challenge of 3DCT–2DUS kid -\nney alignment during free-breathing with a new network structure and training strategy \nand obtains a DSC of 96.88% and 96.39% in CT and U/S images, respectively.\nOther clinical applications for transformer\nIn addition to intelligent analysis and intelligent diagnosis of medical images, the Trans -\nformer mechanism can also be applied to renal image detection, disease prediction, \nimage alignment, electronic reports related to renal diseases, clinical decision models, \netc. [91]. These renal image processing tasks involve large and complex image data, and \nthe models constructed by traditional convolutional neural networks can hardly meet \nthe actual clinical needs. Using an improved Transformer for kidney image data applica-\ntion is an efficient strategy that can help the medical imaging field accomplish quantita -\ntive analysis and clinical diagnosis of kidney images more accurately [92].\nTransformer application for kidney disease prediction\nThe main clinical applications of renal ultrasonography include ruling out revers -\nible causes of acute kidney injury, such as urinary tract obstruction, or identifying \nPage 16 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nTable 2 Comparison of kidney image classification algorithm performance\nAlgorithms Datasets Evaluation indicators/results Main views and contributions Limitations\nTransMIL [16] CAMELYON16/TCGA-NSCLC/TCGA-RCC AUC: (CAMELYON16: 93.09%, TCGANSCLC: \n96.03%, TCGA-RCC: 98.82%)\nUsing multiple instance learning (MIL) to \nexplore morphological and spatial informa-\ntion in images\nMainly dealing with weakly supervised clas-\nsification in whole-slice image (WSI)-based \npathology diagnosis\nCTransPath [84] TCGA-RCC AUC:99.1% Self-computation of localized window \nattention using Swin-Transformer as a \nbackbone model\nLarge amounts of unlabeled data are \nrequired\nUGBC [85] private dataset ACC (glomerulus: 96.30%, Kidney: 96.60%) Assigning image labels based on kidney-\nlevel classification using a high-throughput \nbatch labeling scheme to exploit label \nnoise immunity associated with deep \nneural networks (DNNs)\nDependence on the accuracy of label \nannotations\nDenseNet201–\nRandom Forest \n[86]\nCT KIDNEY DATASET: Normal-Cyst-Tumor \nand Stone\nACC: 99.44% (cyst: 99.60%, kidney: 98.90%, \ntumor: 100%)\nFeature extraction using deep migration \nlearning model DenseNet-201-Random \nForest\nMore resources are needed to train and use \nboth models simultaneously\nRCCGNet [89] KMC-kidney dataset/BreakHis dataset KMC-kidney (ACC: 90.14%, F1:89.06%)/\nBreakHis (ACC: 90.09%, F1: 88.90%)\nRCCGNet contains a shared channel \nresidual (SCR) block, which shares informa-\ntion between two different layers and \ncomplements each other’s shared data\nThe model integration is complex\nPage 17 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nirreversible CKD to rule out unnecessary tests, such as renal biopsy [93]. Traditional \nmethods of assessing kidney injury have relied on metrics such as kidney length, vol -\nume, cortical thickness, and echogenicity [94]. However, in recent years, advances in \ndeep learning and computer vision have enabled machine learning and artificial intel -\nligence techniques to more accurately and objectively assess kidney images, providing \nmore comprehensive information to diagnose kidney injury and treatment decisions. \nCompared to traditional qualitative or semi-quantitative assessment methods, these \ntechniques can reduce the influence of operator experience and subjective factors and \nprovide more accurate assessment results [95].\nMa et al. [96] used a novel multimodal data model combining Transformer’s bi-direc -\ntional encoder representation and optical gradient boosters to improve CKD prediction. \nThe MD-BERT-LGBM model was used in a CKD prediction experiment using over 3 \n/ls of medical data from 3295 participants and compared with traditional LR, LGBM \nand multimodal disease risk prediction algorithms. The results showed that MD-BERT-\nLGBM is expected to play an essential role in predicting and preventing CKD for clinical \napplications. Zeng et al. [97] constructed a sequential model for the prediction of acute \nkidney injury (AKI) induced by sepsis in the ICU. The attention-based sequential con -\nduction model outperforms logistic regression, XGBoost, and RNN through a compre -\nhensive performance evaluation. Its AUROC is 79.5% and AUPRC is 65.0%. Asif et al. [7] \nproposed a deep migration learning architecture based on the pre-trained VGG19 [98] \nmodel and Inception module, i.e., the architecture of the VGG19 model was customized \nby removing the fully connected layer and placing a randomly initialized plain Inception \nmodule and other coatings. It is used to detect major renal diseases from CT images. The \nexperiments considered two migration learning approaches: feature extractor and fine-\ntuning. An AUC of 99.25% was achieved on 4000 renal CT images. The proposed model \nis of great benefit to urologists in detecting renal diseases. Shickelae et al. [99] designed \na multi-stage end-stage renal disease (ESRD) prediction framework for ESRD based on \nthe Transformer model. The framework was based on nonlinear dimensionality reduc -\ntion, relative Euclidean pixel distance embedding, and spatial self-attention mechanisms \nfor predictive modeling. Researchers developed a deep transformer network for coding \nWSI and predicting future ESRD using a dataset of 56 renal biopsy WSIs from patients \nwith diabetic neuropathy at Seoul National University Hospital. The subjects had an \nAUC of 97% for the prediction of 2-year ESRD. Aboutalebi et al. [21] designed a clini -\ncian assessment-based dataset containing clinical and biochemical data of 1366 patients. \nDifferent machine learning models were developed and trained to predict kidney injury, \nincluding gradient-based augmented tree and deep Transformer architecture.\nTransformer in electronic reporting\nElectronic reporting has also been progressively applied in the medical field. Schuppe \net al. [23] used the large-scale language Transformer model open source artificial intel -\nligence ChatGPT, a patient diagnosed with bilateral renal cell carcinoma who underwent \nright partial and left total nephrectomy as well as episodic biliary atresia (BA) exhibited \nnephrotic syndrome (NS) signs and symptoms article reports were written. Yang et al. \n[24] described a methodology to develop a language model for reporting renal trans -\nplant pathology. The study aimed to answer two predefined questions: what rejection \nPage 18 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \ndid the patient exhibit, and what was the grade of interstitial fibrosis and tubular atrophy \n(IFTA)? For this purpose, a corpus containing 3.4K renal transplant ports and 1.5 million \nwords were used in the paper for pre-training in clinical BERT and fine-tuned with QA \nheaders. Additionally, an extended renal BERT (i.e., exKidneyBERT) model was created, \npre-trained and fine-tuned using the same corpus to capture the complex vocabulary of \na narrow technical domain.\nApplication of transformer in decision‑making systems\nZhang et al. [22] utilized the Decision Transformer model, an offline RL (reinforcement \nlearning) paradigm for continuous time decision-making in the healthcare domain. In \nthe paper, the model was generalized to a continuous-time decision-making scenario, \nconsidered past clinical measurements and treatments, and learned methods for sug -\ngesting future visit times and per-treatment schedules. Experimental results show that \nthe continuous-time decision-making Transformer model can outperform its competi -\ntors. It has clinical utility in improving patients’ health and prolonging their survival by \nlearning high-performance strategies from log data generated using strategies of differ -\nent quality levels.\nOther applications summary\nKidney images play an essential role in clinical applications, and different algorithms \nhave been proposed to achieve kidney image alignment and disease detection. Table  3 \ncompares the performance and usage of several standard algorithms for clinical applica -\ntions of kidney images.\nDiscussion and outlook\nThis paper presents a comprehensive overview of Transformer model-based methods \nused for renal image processing tasks. After extensive comparisons and systematic anal -\nysis, compared with traditional CNNs, the Transformer model-based approach can cap -\nture the correlation between different locations in an image through the self-attention \nmechanism. It can consider global and local contextual information, improving the mod-\nel’s ability to understand and judge images. It shows excellent performance and potential \nto become the backbone network model in the renal disease image processing task.\nIn the clinic, the Transformer model-based approach can provide quantitative image \nanalysis for doctors, thus assisting in the diagnosis and treatment planning of kidney dis-\nease. It has certain advantages in the segmentation and classification of kidney images: \n① compared with other traditional models, the Transformer can effectively deal with \nlong-range dependencies through the self-attention mechanism and can better capture \nthe relationship between each part of the image, thus improving the accuracy of segmen-\ntation and classification; ② transformer model is more suitable for dealing with long \nsequence data and global information. The self-attention mechanism in Transformer \nallows interaction between arbitrary positional information without limiting parameter \nsharing and local sense fields, thus providing greater flexibility; ③ transformer model \ncan be easily extended to handle multimodal data, such as the combination of image and \ntext, which is advantageous in the task of multimodal information.\nPage 19 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nTable 3 Performance comparison of kidney image algorithms for other applications\nAlgorithms Datasets Evaluation \nindicators/results\nMain views and \ncontributions\nUsage\nVGG19 [87] Private dataset ACC: 98% VGG19 uses a \ndeeper network \nstructure and a \nsmall convolutional \nkernel for improved \nfeature extraction\nKidney cysts detec-\ntion\nEANet [88] CT KIDNEY DATASET: \nNormal-Cyst-Tumor \nand Stone\nACC: 83.65% Introduction of \nattention mecha-\nnism, multi-scale \nfeature fusion, \nefficient network \ndesign, cross-layer \nfeature interaction\nKidney cyst classifica-\ntion\nResNet50 [88] CT KIDNEY DATASET: \nNormal-Cyst-Tumor \nand Stone\nACC: 87.92% Having introduced \nResidual Block and \nBatch Normalization\nKidney cyst classifica-\ntion\nMD-BERT-LGBM [96] private dataset ACC: 78.12%\nAUC: 85.15%\nThe model \nintegrates a bi-\ndirectional encoder \nrepresentation of \nthe Transformer with \nan optical gradient \nlifter, a multimodal \ndata model\nCKD disease predic-\ntion\nKidneyRegNet [90] KiTS19/in-house \ndatasets\nKiTS19 (DSC: 96.88%, \nSensitivity: 0.9711, \nSpecificity: 0.9667)/\nin-house (DSC: \n96.39%, Sensitivity: \n0.9736, Specificity: \n0.9560)\nA new depth-align-\nment pipeline for \nfree-breathing 3D \nCT and 2D U/S renal \nscans is proposed\nKidney alignment\nChatGPT [23] NA NA The core algorithm \nis the Transformer, \nwhich combines the \nTransformer model’s \nself-attention \nmechanism with the \nlanguage model’s \ngenerative power\nNelson syndrome \n(NS) pathology report \nwriting\nMulGT [100] TCGA-KICA/TCGA-\nESCA\nKICA (Typing: \nAUC: 98.44%, \nACC: 93.89%, F1: \n93.89%, Staging: \nAUC: 80.22%, ACC: \n74.98%, F1: 72.55%)/\nESCA (Typing: \nAUC: 97.49%, \nACC: 92.81%, F1: \n92.74%, Staging: \nAUC: 71.48%, ACC: \n66.63%, F1: 65.73%)\nA domain knowl-\nedge-driven graph \npooling module \nwas designed to \nsimulate diagnostic \npatterns for different \nanalysis tasks\nWSI task diagnostics\nTransformer [22] DIVAT (Database \nof Kidney Trans-\nplantation Medical \nRecords)\nNA For use in \nmedical fields where \ncontinuous-time \ndecision-making is \nrequired\nMedical decision \nsystem\nTransformer [99] Dataset of 56 renal \nbiopsy WSIs in \npatients with DN\nAUC: 97% A multi-stage ESRD \nprediction frame-\nwork based on the \nTransformer model\nFor encoding WSI \n(whole-slice images) \nand predicting future \nESRDs\nPage 20 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nAlthough the Transformer model has unique advantages and potential in kidney \nimage segmentation and classification tasks, some challenges and limitations must be \naddressed. For example, ① the Transformer may suffer from information loss when \ndealing with long-range dependencies compared to traditional CNNs;② the Trans -\nformer model consumes a large amount of computational resources, including memory \nand computational power, when dealing with large-scale image data. This may limit its \nfeasibility and efficiency in practical clinical applications. ③ Transformer models usu -\nally require a large amount of training data for good generalization ability. However, in \nmedical images, especially kidney images, acquiring large-scale labeled data is a chal -\nlenging task.\nFuture research directions include the more effective integration of CNN and Trans -\nformer, the design of novel Transformer model architectures, the handling of multi-\nmodal data, addressing unstructured data, and leveraging weak supervision and \nself-supervised learning to enhance the performance of clinical applications. The devel -\nopment of versatile and robust Transformer methods will facilitate improved analysis \nand application of clinical data. In the context of implementing this model in real-world \nmedical diagnostics, three key challenges and considerations emerge. Firstly, privacy \nand security of data must be taken into account. Patient’s private data should be appro -\npriately handled and protected to prevent data leakage. Secondly, there is a need for \ndiversity in training data. Currently, clinical sample sizes remain limited, resulting in \nconstrained model generalization to different populations and disease types. Collabora -\ntion with more healthcare organizations is essential to collect large-scale clinical sam -\nples for model training to enhance its quality. It is worth noting that Transformer models \ntypically require substantial training data to achieve strong generalization. However, \nobtaining extensive annotated data, especially in the field of medical imaging, such as \nkidney images, poses a challenging task. Lastly, it is necessary to validate the model’s \nTable 3 (continued)\nAlgorithms Datasets Evaluation \nindicators/results\nMain views and \ncontributions\nUsage\nTransformer [20] Private dataset F1: 96.3%,\nAUC: 98.9%\nPredicting Kidney \nTransplant Function \nUsing the Critical \nMask Tensor of the \nTransformer Dot \nProduct Attention \nMechanism\nPredicting kidney \ntransplant function\nCOVID-Net [21] Private dataset Survival prediction: \nACC:93.55%,\nKidney Injury \nComplications: \nACC:88.05%\nProposing an inter-\npretability-driven \nframework for \nbuilding machine \nlearning models \nto predict survival \nand kidney injury \nin patients with no \ncoronary pneumo-\nnia from clinical and \nbiochemical data\nPredicting survival \nand kidney injury in \npatients with new \ncrown pneumonia\nExKidneyBERT [24] Private dataset OneQA (ACC: 83.3%)\nTwoQA (ACC: 95.8%)\nLinguistic modeling \nof renal transplan-\ntation pathology \nreports\nRenal pathology \nreports\nPage 21 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \nstability across multiple datasets, collecting diverse samples from different healthcare \norganizations for validation to test the model’s robustness in various settings.\nThrough our review, we recognize the crucial importance of preprocessing methods \nin current kidney CT image processing. In our future work, we plan to further optimize \nand propose more effective CT image preprocessing approaches to overcome current \nchallenges and limitations. Our method involves multi-step data preprocessing, includ -\ning voxel size resampling, grayscale normalization, noise reduction, contrast enhance -\nment, histogram equalization, region cropping, and data augmentation techniques. \nThese comprehensive preprocessing steps aim to optimize model input, enhance perfor -\nmance, and improve generalization capabilities.\nConclusion\nIn kidney image analysis, diverse architectures and optimization techniques have sig -\nnificantly improved model performance. Transformer architectures for kidney image \nanalysis are typically optimized in three main aspects: ① hybrid CNN and Transformer \nmodels, such as TransUnet [29]and U-Net variants, are employed to extract local fea -\ntures and learn global dependencies; ② introduction of 3D Transformer architecture, \ne.g., TransBTSV2 [19], focuses on learning CT/MRI 3D structural relationships, prov -\ning advantageous in volumetric image analysis compared to 2D models; ③transformer \nmodel modifications, including attention mechanism updates and depth increase for \nricher feature learning. For instance, the DSGA-Net [74] model introduces a Depth Sep -\narable Gated Visual Transformer (DSG-ViT) module to learn deeper features of kidney \nimages. Multimodal data fusion, exemplified by MD-BERT-LGBM, combines different \nimaging modalities (CT, MRI, ultrasound) and text/label data, enhancing feature charac-\nterization. In summary, to enhance kidney image analysis task performance, appropriate \nmodel architectures need to be selected or modified based on data and task character -\nistics. We have summarized the features and performance of each model, providing a \nvaluable reference resource for advancing and expanding kidney image analysis research.\nAbbreviations\nCNN  Convolutional neural network\nCKD  Chronic kidney disease\nKSD  Kidney stones disease\nCT  Computed tomography\nMRI  Magnetic resonance imaging\nRNNs  Recurrent neural networks\nViT  Vision Transformer\nTIF  Transformer interactive fusion\nW-MSA  Weighted multi-scale aggregation\nLGG-SA  Local–global Gaussian-weighted self-attention\nEA  External attention\nFINE  Full-resolution memory\nSTN  Spatial Transformer model network\nFCN  Full convolutional network\nOARs  Organs at risk\nGAN  Generative adversarial network\nMLP-Mixer  U-shaped multilayer perceptron mixer\nHN  Head and neck datasets\nBTCV  Abdominal organ datasets\nKiTS 2019  Kidney Tumor Segmentation Challenge 2019\nKiTS 2021  Kidney Tumor Segmentation Challenge 2021\nSynapse 2015  Synapse Multimodal MRI Segmentation and Classification Challenge 2015\nACDC  Automated Cardiac Diagnosis Challenge\nThorax-85  Thoracic Disease Screening in Chest Radiographs Dataset and Challenge\nPage 22 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \nBCV  Brain Cancer Vision\nSegTHOR thorax  Segmentation of THoracic Organs at Risk\nBraTS2019  Multimodal Brain Tumor Segmentation Challenge 2019\nBraTS2020  Multimodal Brain Tumor Segmentation Challenge2020\nLiTS 2017  Liver Tumor Segmentation Challenge 2017\nMSD  Medical Segmentation Decathlon\nAMOS22  AMOS Medical Image Analysis Challenge 2022\nBraTS21  Multimodal Brain Tumor Segmentation Challenge 2021\nCOVID-DS36  COVID-19 Diagnosis using Chest X-ray Images Dataset and Challenge\nGlaS  Glasgow Retinal Image Analysis Challenge: Image Registration\nISIC 2018  2018 International Skin Imaging Collaboration: Skin Lesion Analysis Towards Melanoma Detection \nChallenge\nDIVAT  Database of Kidney Transplantation Medical Records\nCAMELYON16  Camelyon16: A Benchmark for Fully Automatic Multi-Path Segmentation of Lymph Nodes\nTCGA   The Cancer Genome Atlas\nSSL  Semi-supervised learning\nSRCL  Semantic relatedness contrastive learning\nMIL  Multiple instance learning\nUGBC  Uncertainty-guided Bayesian Classification\nU/S  Ultrasound\nFIM  Feature image mismatch\nAKI  Acute kidney injury\nESRD  End-stage renal disease\nBA  Biliary atresia\nNS  Nephrotic syndrome\nIFTA  Interstitial fibrosis and tubular atrophy\nMS-CMRSeg  MICCAI 2019 Multi-sequence Cardiac MRI Segmentation Challenge\nCHAOS  ISBI 2019 Combined Healthy Abdominal Organ Segmentation Challenge\nAcknowledgements\nNot applicable.\nAuthor contributions\nYY wrote the original manuscript and the design of the tables and illustrations. ZT and HW collaborated on the design of \nthe study and revised the manuscript. All authors have read and agreed to the published version of the manuscript.\nFunding\nThis study was supported by Grants from the National Natural Science Foundation of China (Grant No. 82170728).\nAvailability of data and materials\nNot applicable.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no conflict of interests regarding the publication of this paper.\nReceived: 20 September 2023   Accepted: 22 January 2024\nReferences\n 1. Kovesdy CP . Epidemiology of chronic kidney disease: an update 2022. Kidney Int Supplements. 2022;12(1):7–11.\n 2. Sassanarakkit S, Hadpech S, Thongboonkerd V. Theranostic roles of machine learning in clinical management of \nkidney stone disease. Comput Struct Biotechnol J. 2023;21:260–6.\n 3. Lemberger U, Pjevac P , Hausmann B, Berry D, Moser D, Jahrreis V, Özsoy M, Shariat SF, Veser JJU. The microbiome of \nkidney stones and urine of patients with nephrolithiasis. Urolithiasis. 2023;51(1):27.\n 4. Huang J, Leung DK-W, Chan EO-T, Lok V, Leung S, Wong I, Lao X-Q, Zheng Z-J, Chiu PK-F, Ng C-F. A global trend \nanalysis of kidney cancer incidence and mortality and their associations with smoking, alcohol consumption, and \nmetabolic syndrome. Eur Urol Focus. 2022;8(1):200–9.\n 5. Nguyen H-G, Fouard C, Troccaz J. Segmentation, separation and pose estimation of prostate brachytherapy seeds \nin CT images. IEEE Trans Biomed Eng. 2015;62(8):2012–24.\n 6. Huo Y, Liu J, Xu Z, Harrigan RL, Assad A, Abramson RG, Landman BA. Robust multicontrast MRI spleen segmenta-\ntion for splenomegaly using multi-atlas segmentation. IEEE Trans Biomed Eng. 2017;65(2):336–43.\nPage 23 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \n 7. Asif S, Wenhui Y, Jinhai S, Ain QU, Yueyang Y, Jin H. Modeling a fine-tuned deep convolutional neural network \nfor diagnosis of kidney diseases from CT images. In 2022 IEEE International Conference on Bioinformatics and \nBiomedicine (BIBM). IEEE; 2022:2571–2576.\n 8. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you \nneed. Adv Neural Inf Process Syst. 2017, 30.\n 9. Naseer MM, Ranasinghe K, Khan SH, Hayat M, Shahbaz Khan F, Yang M-H. Intriguing properties of vision trans-\nformers. Adv Neural Inf Process Syst. 2021;34:23296–308.\n 10. Li F, Lu X, Yuan J. Mha-corocapsule: multi-head attention routing-based capsule network for covid-19 chest \nx-ray image classification. IEEE Trans Med Imaging. 2021;41(5):1208–18.\n 11. Saikia FN, Iwahori Y, Suzuki T, Bhuyan MK, Wang A, Kijsirikul B. MLP-UNet: Glomerulus Segmentation. IEEE \nAccess 2023:1–1.\n 12. Liu H, Dai Z, So D, Le QV. Pay attention to mlps. Adv Neural Inf Process Syst. 2021;34:9204–15.\n 13. Touvron H, Bojanowski P , Caron M, Cord M, El-Nouby A, Grave E, Izacard G, Joulin A, Synnaeve G, Verbeek J. \nResmlp: feedforward networks for image classification with data-efficient training. IEEE Trans Pattern Anal \nMach Intell. 2022;45(4):5314–21.\n 14. Lian D, Yu Z, Sun X, Gao S. As-mlp: an axial shifted mlp architecture for vision. arXiv preprint,  arXiv: 2107. 08391. \n2021.\n 15. Chen S, Xie E, Ge C, Chen R, Liang D, Luo PJ. Cyclemlp: A mlp-like architecture for dense prediction. arXiv \npreprint, arXiv: 2107. 10224. 2021.\n 16. Shao Z, Bian H, Chen Y, Wang Y, Zhang J, Ji X. Transmil: transformer based correlated multiple instance learning \nfor whole slide image classification. Adv Neural Inf Process Syst. 2021;34:2136–47.\n 17. Chen Z, Liu H. 5 D Cascaded semantic segmentation for kidney tumor cyst. In International challenge on \nkidney and kidney tumor segmentation. Springer; 2021: 28–34.\n 18. La Barbera G, Gori P , Boussaid H, Belucci B, Delmonte A, Goulin J, Sarnacki S, Rouet L, Bloch I. Automatic size \nand pose homogenization with Spatial Transformer Network to improve and accelerate pediatric segmenta-\ntion. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). IEEE; 2021:1773–1776\n 19. Li J, Wang W, Chen C, Zhang T, Zha S, Wang J, Yu H. TransBTSV2: towards better and more efficient volumetric \nsegmentation of medical images. arXiv preprint, arXiv: 2201. 12785. 2022.\n 20. Milecki L, Kalogeiton V, Bodard S, Anglicheau D, Correas J-M, Timsit M-O, Vakalopoulou M. Contrastive masked \ntransformers for forecasting renal transplant function. In International conference on medical image comput -\ning and computer-assisted intervention. Springer; 2022:244–254.\n 21. Aboutalebi H, Pavlova M, Shafiee MJ, Florea A, Hryniowski A, Wong A. COVID-Net Biochem: an explainability-\ndriven framework to building machine learning models for predicting survival and kidney injury of COVID-19 \npatients from clinical and biochemistry data. arXiv preprint, arXiv: 2204. 11210. 2022.\n 22. Zhang Z, Mei H, Xu Y. Continuous-Time decision transformer for healthcare applications. In International \nconference on artificial intelligence and statistics. PMLR; 2023:6245–6262.\n 23. Schuppe K, Burke S, Cohoe B, Chang K, Lance RS, Mroch HJC. Atypical Nelson syndrome following right \npartial and left total nephrectomy with incidental bilateral total adrenalectomy of renal cell carcinoma: a chat \ngenerative pre-trained transformer (ChatGPT)-assisted case report and literature review. Cureus. 2023;15(3): \ne36042.\n 24. Yang T. exKidneyBERT: a language model for kidney transplant pathology reports and the crucial role of \nextended vocabularies. Waterloo: University of Waterloo; 2022.\n 25. Xu Q, Zhang T, Xia T, Jin B, Chen H, Yang X. Epidemiological trends of kidney cancer along with attributable risk \nfactors in China from 1990 to 2019 and its projections until 2030: an analysis of the global burden of disease \nstudy 2019. Clin Epidemiol. 2023;15:421–33.\n 26. Ronneberger O, Fischer P , Brox T. U-net: Convolutional networks for biomedical image segmentation. In \nMedical image computing and computer-assisted intervention–MICCAI 2015: 18th International Conference, \nMunich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer; 2015:234-241.\n 27. De Raad K, van Garderen KA, Smits M, van der Voort SR, Incekara F, Oei E, Hirvasniemi J, Klein S, Starmans MP . \nThe effect of preprocessing on convolutional neural networks for medical image segmentation. In 2021 IEEE \n18th International Symposium on Biomedical Imaging (ISBI). IEEE; 2021:655–658\n 28. Liu J, Sun H, Katto J. Learned image compression with mixed transformer-CNN architectures. In Proceedings of \nthe IEEE/CVF conference on computer vision and pattern recognition. 2023:14388–14397.\n 29. Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y, Lu L, Yuille AL, Zhou Y. transunet: transformers make strong encoders \nfor medical image segmentation. arXiv preprint, arXiv: 2102. 04306. 2021.\n 30. Atek S, Mehidi I, Jabri D, Belkhiat DEC. SwinT-Unet: hybrid architecture for medical image segmentation based \non Swin transformer block and Dual-Scale Information. In 2022 7th International conference on image and \nsignal processing and their applications (ISPA); 8–9 May 2022. 2022:1–6.\n 31. Hatamizadeh A, Tang Y, Nath V, Yang D, Myronenko A, Landman B, Roth HR, Xu D. Unetr: Transformers for 3d \nmedical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer \nvision. 2022:574–584.\n 32. Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, Wang M. Swin-unet: Unet-like pure transformer for medical \nimage segmentation. In European conference on computer vision. Springer; 2022:205–218.\n 33. Yao C, Hu M, Li Q, Zhai G, Zhang XP . Transclaw U-Net: Claw U-Net with transformers for medical image \nsegmentation. In 2022 5th International Conference on Information Communication and Signal Processing \n(ICICSP); 26–28 Nov. 2022. 2022:280–284.\n 34. Huang X, Deng Z, Li D, Yuan X. Missformer: an effective medical image segmentation transformer. arXiv pre -\nprint, arXiv: 2109. 07162. 2021.\n 35. Feng X, Wang T, Yang X, Zhang M, Guo W, Wang W. ConvWin-UNet: UNet-like hierarchical vision Transformer \ncombined with convolution for medical image segmentation. Math Biosci Eng. 2023;20(1):128–44.\nPage 24 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \n 36. Wang H, Xie S, Lin L, Iwamoto Y, Han X-H, Chen Y-W, Tong R. Mixed transformer u-net for medical image segmen-\ntation. In ICASSP 2022–2022 IEEE International conference on acoustics, speech and signal processing (ICASSP). \nIEEE; 2022:2390–2394.\n 37. Zhou H-Y, Guo J, Zhang Y, Yu L, Wang L, Yu Y. nnformer: Interleaved transformer for volumetric segmentation. arXiv \npreprint, arXiv: 2109. 03201. 2021.\n 38. Azad R, Arimond R, Aghdam EK, Kazerouni A, Merhof D. Dae-former: dual attention-guided efficient transformer \nfor medical image segmentation. International Workshop on PRedictive Intelligence In MEdicine Cham: Springer \nNature Switzerland 2022:83–95.\n 39. Cai Z, Xin J, Shi P , Wu J, Zheng N. DSTUNet: UNet with efficient dense SWIN transformer pathway for medical \nimage segmentation. In 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI); 28–31 March \n2022. 2022:1–5.\n 40. Guo S, Sheng S, Lai Z, Chen S. Trans-U: transformer enhanced U-Net for medical image segmentation. In 2022 3rd \nInternational conference on computer vision, image and deep learning & international conference on computer \nengineering and applications (CVIDL & ICCEA); 20–22 May 2022. 2022:628–631.\n 41. Wang F, Wang B. Hybrid transformer and convolution for medical image segmentation. In 2022 International con-\nference on image processing, computer vision and machine learning (ICICML); 28–30 Oct. 2022. 2022:156–159.\n 42. Xu S, Quan H. ECT-NAS: searching efficient CNN-transformers architecture for medical image segmentation. In \n2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM); 9–12 Dec. 2021. 2021:1601–1604.\n 43. Wang J, Zhao H, Liang W, Wang S, Zhang Y. Cross-convolutional transformer for automated multi-organs segmen-\ntation in a variety of medical images. Phys Med Biol. 2023;68(3): 035008.\n 44. You C, Zhao R, Liu F, Dong S, Chinchali S, Topcu U, Staib L, Duncan J. Class-aware adversarial transformers for medi-\ncal image segmentation. Adv Neural Inf Process Syst. 2022;35:29582–96.\n 45. Niu Y, Luo Z, Lian S, Li L, Li S, Song H. symmetrical supervision with transformer for few-shot medical image \nsegmentation. In 2022 IEEE International conference on bioinformatics and biomedicine (BIBM); 6–8 Dec. 2022. \n2022:1683–1687.\n 46. Roy AG, Siddiqui S, Pölsterl S, Navab N, Wachinger C. ‘Squeeze & excite’ guided few-shot segmentation of volumet-\nric images. Med Image Anal. 2020;59: 101587.\n 47. Themyr L, Rambour C, Thome N, Collins T, Hostettler A. Memory transformers for full context and high-resolution \n3D Medical Segmentation. In machine learning in medical imaging: 13th International Workshop, MLMI 2022, \nheld in conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings. Springer; 2022:121-130.\n 48. Xie Y, Zhang J, Shen C, Xia Y. Cotr: efficiently bridging cnn and transformer for 3d medical image segmentation. \nIn Medical image computing and computer assisted intervention–MICCAI 2021: 24th international conference, \nStrasbourg, France, September 27–October 1, 2021, Proceedings, Part III 24. Springer; 2021:171-180\n 49. Wang R. A semi-supervised multi-organ segmentation method via cross teaching between CNN and transformer. \narXiv preprint, arXiv: 2112. 04894. 2022.\n 50. Xin R, Wang L. Abdominal multi-organ segmentation using CNN and transformer. In MICCAI Challenge on fast and \nlow-resource semi-supervised abdominal organ segmentation. Springer; 2022: 270–280.\n 51. Isensee F, Jaeger PF, Kohl SA, Petersen J, Maier-Hein KH. nnU-Net: a self-configuring method for deep learning-\nbased biomedical image segmentation. Nat Methods. 2021;18(2):203–11.\n 52. Wang B, Li Q, You Z. Self-supervised learning based transformer and convolution hybrid network for one-shot \norgan segmentation. Neurocomputing. 2023;527:1–12.\n 53. Wang K, Liew JH, Zou Y, Zhou D, Feng J. Panet: few-shot image semantic segmentation with prototype alignment. \nIn proceedings of the IEEE/CVF international conference on computer vision. 2019:9197–9206.\n 54. Qin R, Fu X, Lang P . PolSAR image classification based on low-frequency and contour subbands-driven polarimet-\nric SENet. IEEE J Select Topics Appl Earth Observ Remote Sensing. 2020;13:4760–73.\n 55. Ouyang C, Biffi C, Chen C, Kart T, Qiu H, Rueckert D. Self-supervised learning for few-shot medical image segmen-\ntation. IEEE Trans Med Imaging. 2022;41(7):1837–48.\n 56. Yu X, Yang Q, Zhou Y, Cai LY, Gao R, Lee HH, Li T, Bao S, Xu Z, Lasko TA. Unest: local spatial representation learning \nwith hierarchical transformer for efficient medical segmentation. Med Image Anal. 2022;90: 102939.\n 57. Huo Y, Xu Z, Xiong Y, Aboud K, Parvathaneni P , Bao S, Bermudez C, Resnick SM, Cutting LE, Landman BAJN. 3D \nwhole brain segmentation using spatially localized atlas network tiles. Neuroimage. 2019;194:105–19.\n 58. Yu X, Tang Y, Zhou Y, Gao R, Yang Q, Lee HH, Li T, Bao S, Huo Y, Xu Z. Characterizing renal structures with 3D block \naggregate Transformers. arXiv preprint, arXiv: 2203. 02430. 2022.\n 59. Boussaid H, Rouet L. Shape feature loss for kidney segmentation in 3d ultrasound images. In. BMVC; 2021.\n 60. Sun P , Mo Z, Hu F, Song X, Mo T, Yu B, Zhang Y, Chen Z. Segmentation of kidney mass using AgDenseU-Net 2.5 D \nmodel. Comput Biol Med. 2022;150: 106223.\n 61. Lin A, Chen B, Xu J, Zhang Z, Lu G, Zhang D. Ds-transunet: dual swin transformer u-net for medical image segmen-\ntation. IEEE Trans Instrum Meas. 2022;71:1–15.\n 62. Yang J, Jiao L, Shang R, Liu X, Li R, Xu L. EPT-Net: edge perception transformer for 3D medical image segmentation. \nIEEE Trans Med Imaging. 2023;42:3229–43.\n 63. Liao W, Luo X, He Y, Dong Y, Li C, Li K, Zhang S, Zhang S, Wang G, Xiao J. Comprehensive evaluation of a deep \nlearning model for automatic organs at risk segmentation on heterogeneous computed tomography images for \nabdominal radiation therapy. Int J Radiat Oncol Biol Phys. 2023;117:994–1006.\n 64. Francis S, Jayaraj P , Pournami P , Puzhakkal N. ContourGAN: auto-contouring of organs at risk in abdomen com-\nputed tomography images using generative adversarial network. Int J Imaging Syst Technol. 2023;33:1494–504.\n 65. Pan S, Chang CW, Wang T, Wynne J, Hu M, Lei Y, Liu T, Patel P , Roper J, Yang X. Abdomen CT multi-organ segmenta-\ntion using token-based MLP-Mixer. Med Phys. 2023;50(5):3027–38.\n 66. Jiang J, Elguindi S, Berry SL, Onochie I, Cervino L, Deasy JO, Veeraraghavan H. Nested block self-attention multiple \nresolution residual network for multiorgan segmentation from CT. Med Phys. 2022;49(8):5244–57.\n 67. Jiang X, Ding Y, Liu M, Wang Y, Li Y, Wu Z. BiFTransNet: a unified and simultaneous segmentation network for \ngastrointestinal images of CT & MRI. Comput Biol Med. 2023;165: 107326.\nPage 25 of 26\nYin et al. BioMedical Engineering OnLine           (2024) 23:27 \n \n 68. Li G, Jin D, Yu Q, Qi M. IB-TransUNet: combining information bottleneck and transformer for medical image seg-\nmentation. J King Saud Univ Comput Inf Sci. 2023;35(3):249–58.\n 69. Xu G, Zhang X, He X, Wu X. Levit-unet: make faster encoders with transformer for medical image segmentation. In \nChinese conference on pattern recognition and computer vision (PRCV). Springer; 2023:42–53.\n 70. Sagar A: Vitbis: Vision transformer for biomedical image segmentation. In MICCAI Workshop on distributed and \ncollaborative learning. Springer; 2021:34–45.\n 71. Yan X, Tang H, Sun S, Ma H, Kong D, Xie X. After-unet: axial fusion transformer unet for medical image segmenta-\ntion. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2022:3971–3981.\n 72. Tao H, Mao K, Zhao Y. DBT-UNETR: double branch transformer with cross fusion for 3D medical image seg-\nmentation. In 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM); 6–8 Dec. 2022. \n2022:1213–1218.\n 73. Heidari M, Kazerouni A, Soltany M, Azad R, Aghdam EK, Cohen-Adad J, Merhof D. Hiformer: hierarchical multi-scale \nrepresentations using transformers for medical image segmentation. In Proceedings of the IEEE/CVF winter con-\nference on applications of computer vision. 2023:6202–6212.\n 74. Zhao Y, Li J, Hua Z. MPSHT: multiple progressive sampling hybrid model multi-organ segmentation. IEEE J Transl \nEng Health Med. 2022;10:1–9.\n 75. Sun J, Zhao J, Wu X, Tang C, Wang S, Zhang Y. DSGA-Net: deeply separable gated transformer and attention strat-\negy for medical image segmentation network. J King Saud Univ Comput Inf Sci. 2023;35(5): 101553.\n 76. Roy S, Koehler G, Ulrich C, Baumgartner M, Petersen J, Isensee F, Jaeger PF, Maier-Hein KH. Mednext: transformer-\ndriven scaling of convnets for medical image segmentation. In International Conference on Medical Image \nComputing and Computer-Assisted Intervention. Springer; 2023:405–415.\n 77. Liu Y, Zhu Y, Xin Y, Zhang Y, Yang D, Xu T. MESTrans: multi-scale embedding spatial transformer for medical image \nsegmentation. Comput Methods Programs Biomed. 2023;233: 107493.\n 78. Zhang J, Qin Q, Ye Q, Ruan T. ST-Unet: Swin Transformer boosted U-Net with cross-layer feature enhancement for \nmedical image segmentation. Comput Biol Med. 2023;153: 106516.\n 79. Shen Z, Yang H, Zhang Z, Zheng S. Automated kidney tumor segmentation with convolution and transformer \nnetwork. In International challenge on kidney and kidney tumor segmentation. Springer; 2021: 1–12.\n 80. Liu Q, Kaul C, Wang J, Anagnostopoulos C, Murray-Smith R, Deligianni F. Optimizing vision transformers for medi-\ncal image segmentation. In ICASSP 2023 - 2023 IEEE International conference on acoustics, speech and signal \nprocessing (ICASSP); 4–10 June 2023. 2023:1–5.\n 81. Kalantar-Zadeh K, Jafar TH, Nitsch D, Neuen BL, Perkovic V. Chronic kidney disease. The lancet. \n2021;398(10302):786–802.\n 82. Sun L, Zhao G, Zheng Y, Wu Z. Sensing R: spectral–spatial feature tokenization transformer for hyperspectral image \nclassification. IEEE Trans Geosci Remote Sens. 2022;60:1–14.\n 83. Zheng P . MT-ONet: mixed Transformer O-Net for Medical Image Segmentation. In 2022 International confer-\nence on sensing, measurement & data analytics in the era of artificial intelligence (ICSMD); 30 Nov.-2 Dec. 2022. \n2022:1–4.\n 84. Wang X, Yang S, Zhang J, Wang M, Zhang J, Yang W, Huang J, Han X. Transformer-based unsupervised contrastive \nlearning for histopathological image classification. Med Image Anal. 2022;81: 102559.\n 85. Cicalese PA, Mobiny A, Shahmoradi Z, Yi X, Mohan C, Nguyen HV. Kidney level lupus nephritis classification using \nuncertainty guided Bayesian convolutional neural networks. IEEE J Biomed Health Inform. 2021;25(2):315–24.\n 86. Qadir AM, Abd DF. Kidney diseases classification using hybrid transfer-learning densenet201-based and random \nforest classifier. Kurdistan J Appl Res. 2023:131–144.\n 87. Aruna SK, Deepa N, Devi T. A deep learning approach based on CT images for an automatic detection of polycys-\ntic kidney disease. In 2023 International conference on computer communication and informatics (ICCCI); 23–25 \nJan. 2023. 2023:1–5.\n 88. Hossain MS, Hassan SMN, Al-Amin M, Rahaman MN, Hossain R, Hossain MI. Kidney disease detection from CT \nImages using a customized CNN model and deep learning. In 2023 International conference on advances in intel-\nligent computing and applications (AICAPS); 1–3 Feb. 2023. 2023:1–6.\n 89. Chanchal AK, Lal S, Kumar R, Kwak JT, Kini J. A novel dataset and efficient deep learning framework for automated \ngrading of renal cell carcinoma from kidney histopathology images. Sci Rep. 2023;13(1):5728.\n 90. Yanling C, Yuyu X, Huiying L, Xiaoxiang W, Zhiqiang L, Jiawei M, Guibin X, Weimin H. KidneyRegNet: a Deep Learn-\ning Method for 3DCT-2DUS Kidney Registration during Breathing. arXiv preprint, arXiv: 2305. 13855. 2023.\n 91. Han K, Wang Y, Chen H, Chen X, Guo J, Liu Z, Tang Y, Xiao A, Xu C, Xu Y. A survey on vision transformer. IEEE Trans \nPattern Anal Mach Intell. 2022;45(1):87–110.\n 92. Yuan F, Zhang Z, Fang ZJPR. An effective CNN and Transformer complementary network for medical image seg-\nmentation. Pattern Recogn. 2023;136: 109228.\n 93. Kuo C-C, Chang C-M, Liu K-T, Lin W-K, Chiang H-Y, Chung C-W, Ho M-R, Sun P-R, Yang R-L, Chen K-T. Automation of \nthe kidney function prediction and classification through ultrasound-based kidney imaging using deep learning. \nNPJ Digit Med. 2019;2(1):29.\n 94. Sawhney R, Malik A, Sharma S, Narayan V. A comparative assessment of artificial intelligence models used for early \nprediction and evaluation of chronic kidney disease. Decis Anal J. 2023;6: 100169.\n 95. Ma F, Sun T, Liu L, Jing H. Detection and diagnosis of chronic kidney disease using deep learning-based heteroge-\nneous modified artificial neural network. Futur Gener Comput Syst. 2020;111:17–26.\n 96. Ma D, Li X, Mou S, Cheng Z, Yan X, Lu Y, Yan R, Cao S. Prediction of chronic kidney disease risk using multimodal \ndata. In 2021 The 5th International conference on computer and data analysis. Sanya, China: Association for Com-\nputing Machinery; 2021: 20–25.\n 97. Zeng G, Zhuang J, Huang H, Gao Y, Liu Y, Yu X. Continuous prediction of acute kidney injury from patients with \nsepsis in ICU settings: A sequential transduction model based on attention. In Proceedings of the 2022 Interna-\ntional Conference on Intelligent Medicine and Health. Xiamen, China: Association for Computing Machinery; \n2022: 31–37.\nPage 26 of 26Yin et al. BioMedical Engineering OnLine           (2024) 23:27 \n 98. Carvalho T, De Rezende ER, Alves MT, Balieiro FK, Sovat RB. Exposing computer generated images by eye’s region \nclassification via transfer learning of VGG19 CNN. In 2017 16th IEEE international conference on machine learning \nand applications (ICMLA). IEEE; 2017:866-870\n 99. Shickel B, Lucarelli N, Rao A, Yun D, Moon KC, Seok HS, Sarder P . Spatially aware transformer networks for contex-\ntual prediction of diabetic nephropathy progression from whole slide images. In Medical Imaging 2023: Digital \nand Computational Pathology. SPIE; 2023:129-140\n 100. Zhao W, Wang S, Yeung M, Niu T, Yu L. MulGT: multi-task graph-transformer with task-aware knowledge injection \nand domain knowledge-driven pooling for whole slide image analysis. arXiv preprint, arXiv: 2302. 10574. 2023.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7192440032958984
    },
    {
      "name": "Image processing",
      "score": 0.6749365329742432
    },
    {
      "name": "Transformer",
      "score": 0.5940196514129639
    },
    {
      "name": "Artificial intelligence",
      "score": 0.575555145740509
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4836036264896393
    },
    {
      "name": "Segmentation",
      "score": 0.4576316475868225
    },
    {
      "name": "Encoder",
      "score": 0.4553154408931732
    },
    {
      "name": "Image segmentation",
      "score": 0.44924822449684143
    },
    {
      "name": "Computer vision",
      "score": 0.3708898425102234
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32564663887023926
    },
    {
      "name": "Image (mathematics)",
      "score": 0.25769996643066406
    },
    {
      "name": "Engineering",
      "score": 0.15170082449913025
    },
    {
      "name": "Electrical engineering",
      "score": 0.12162989377975464
    },
    {
      "name": "Voltage",
      "score": 0.09832018613815308
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I148128674",
      "name": "University of Shanghai for Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210135985",
      "name": "Shanghai University of Medicine and Health Sciences",
      "country": "CN"
    }
  ]
}