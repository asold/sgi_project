{
  "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
  "url": "https://openalex.org/W4389524330",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2128504963",
      "name": "Xinwei Wu",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2512786803",
      "name": "Junzhuo Li",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2139263507",
      "name": "Minghui Xu",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2416726853",
      "name": "Wei-Long Dong",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2770339140",
      "name": "Shuangzhi Wu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2031923449",
      "name": "Chao Bian",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2139156831",
      "name": "Deyi Xiong",
      "affiliations": [
        "Tianjin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3170672407",
    "https://openalex.org/W4231844697",
    "https://openalex.org/W3172075261",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2963919731",
    "https://openalex.org/W2889507104",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W4385574113",
    "https://openalex.org/W2621075239",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W155995321",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W4281262808",
    "https://openalex.org/W1977843657",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W3202586005",
    "https://openalex.org/W3197942009",
    "https://openalex.org/W4221165593",
    "https://openalex.org/W2128128412",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4287828446",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W3040639636",
    "https://openalex.org/W4389518968"
  ],
  "abstract": "Pretrained language models have learned a vast amount of human knowledge from large-scale corpora, but their powerful memorization capability also brings the risk of data leakage. Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone number and frequently outputting it. In such cases, model developers need to eliminate specific data influences from the model to mitigate legal and ethical penalties. To effectively mitigate these risks, people often have to spend a significant amount of time and computational costs to retrain new models instead of finding ways to cure the 'sick' models. Therefore, we propose a method to locate and erase risky neurons in order to eliminate the impact of privacy data in the model. We use a new method based on integrated gradients to locate neurons associated with privacy texts, and then erase these neurons by setting their activation values to zero.Furthermore, we propose a risky neuron aggregation method to eliminate the influence of privacy data in the model in batches. Experimental results show that our method can effectively and quickly eliminate the impact of privacy data without affecting the model's performance. Additionally, we demonstrate the relationship between model memorization and neurons through experiments, further illustrating the robustness of our method.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2875–2886\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDEPN: Detecting and Editing Privacy Neurons in Pretrained Language\nModels\nXinwei Wu1, Junzhuo Li2, Minghui Xu1, Weilong Dong1,\nShuangzhi Wu3, Chao Bian3,4, Deyi Xiong1,2∗\n1College of Intelligence and Computing, Tianjin University, Tianjin, China\n2School of New Media and Communication, Tianjin University, Tianjin, China\n3Department of Computer Science and Technology, Tsinghua University, Beijing, China\n4ByteDance Lark AI, Beijing, China\n{wuxw2021,jzli,xuminghui,willowd,dyxiong}@tju.edu.cn,\nwufurui@bytedance.com, bianc18@mails.tsinghua.edu.cn\nAbstract\nLarge language models pretrained on a huge\namount of data capture rich knowledge and\ninformation in the training data. The ability\nof data memorization and regurgitation in pre-\ntrained language models, revealed in previous\nstudies, brings the risk of data leakage. In or-\nder to effectively reduce these risks, we pro-\npose a framework DEPN to Detect and Edit\nPrivacy Neurons in pretrained language mod-\nels, partially inspired by knowledge neurons\nand model editing. In DEPN, we introduce a\nnovel method, termed as privacy neuron detec-\ntor, to locate neurons associated with private\ninformation, and then edit these detected pri-\nvacy neurons by setting their activations to zero.\nFurthermore, we propose a privacy neuron ag-\ngregator dememorize private information in a\nbatch processing manner. Experimental results\nshow that our method can significantly and ef-\nficiently reduce the exposure of private data\nleakage without deteriorating the performance\nof the model. Additionally, we empirically\ndemonstrate the relationship between model\nmemorization and privacy neurons, from multi-\nple perspectives, including model size, training\ntime, prompts, privacy neuron distribution, il-\nlustrating the robustness of our approach.\n1 Introduction\nRemarkable progress has been made in large lan-\nguage models (LLMs) in recent years (Brown et al.,\n2020; Liu et al., 2021; Ouyang et al., 2022; Lee\net al., 2023). However,despite this success, LLMs\nare confronted with privacy and security concerns\nin real-world applications (Guo et al., 2022; Brown\net al., 2022; Li et al., 2023). The primary cause of\nprivacy and security risks is the inherent nature of\nlarge pretrained language models. Previous studies\n(Carlini et al., 2019, 2021; Thakkar et al., 2021;\n∗*Corresponding author.\nHenderson et al., 2018) have demonstrated that\npretrained language models tend to memorize and\nregurgitate a significant portion of the training data,\nincluding atypical data points that appear only once\nin the training data. Additionally, external factors\n(e.g., membership attack) also contribute to these\nrisks. A variety of methods have been explored\nto attack LLMs for training data extraction. For\ninstance, Carlini et al. (2021) have successfully ex-\ntracted personal information from GPT-3’s output,\nwhile Li et al. (2023) have induced the genera-\ntion of personal information by utilizing multi-step\nprompts in ChatGPT. All these show that large pre-\ntrained language models suffer from a serious risk\nof privacy leakage.\nIn order to safeguard privacy, numerous meth-\nods have been proposed. The majority focus on\neither removing sensitive information during the\ndata processing stage (Liu et al., 2017; El Emam\net al., 2009; Zhou et al., 2008; García-Pablos et al.,\n2020), or reducing the extent to which models\nmemorize training data during the training stage\n(Li et al., 2021; Hoory et al., 2021; Plant et al.,\n2021; Coavoux et al., 2018). However, privacy\nbreaches often come to light after the completion\nof model training, rendering previous methods less\neffective. There are also methods proposed in the\npost-processing stage, which involve slight param-\neter retraining to make the model forget privacy\ninformation (Bourtoule et al., 2021; Gupta et al.,\n2021; Neel et al., 2020). Nevertheless, these meth-\nods generally incur high computational complexity,\nmaking it challenging to apply them to complex\nmodel architectures. In practice, model develop-\ners often attempt to prevent language models from\noutputting specific information via blocking or fil-\ntering certain keywords, which, however, does not\ntruly address the underlying issue.\nWe speculate that private information might be\n2875\nstored in specific neurons, just like knowledge neu-\nrons (Geva et al., 2021; Meng et al., 2022; Dai\net al., 2022). This presumption suggests that we\ncould change the model memorization of private\ninformation by detecting and deleting these neu-\nrons (termed as privacy neurons). Therefore, we\npropose a framework DEPN for detecting and edit-\ning privacy neurons. To detect privacy neurons,\nwe introduce a privacy neuron detector that uses\ngradient integration to simultaneously compute the\ncontributions of multiple markers to neuron activa-\ntions. This allows us to estimate an overall privacy\nattribution score for private information. Subse-\nquently, we further propose a privacy neuron editor\nthat simply sets the activations of the top zprivacy\nneurons with the highest privacy scores to zero to\nerase the model memorization of the corresponding\nprivate information. For the scenario of process-\ning multiple sentences at the same time, we also\npresent a privacy neuron aggregator to facilitate\nprivacy information editing in batches.\nExperimental results show that our framework\ncan quickly reduce the risk of private data leakage\nwithout affecting model performance. Compared\nwith other methods, our framework is highly ef-\nficient. Furthermore, we have found that model\nmemorization leads to the aggregation of privacy\nneurons in our experiments, and demonstrated that\nour framework is very suitable for the scenario of\ndeep model dememorization.\nThe main contributions of our work are summa-\nrized as follows:\n• For the first time, we explore model edit-\ning into privacy protection of pretrained lan-\nguage models, provide a new way for privacy\nprotection, and propose DEPN to effectively\neliminate model memorization in the post-\nprocessing stage.\n• We propose the privacy neuron detector to\nlocalize privacy neurons based on gradient\nattribution, and the privacy neuron editor to\ndememorize privacy information in pretrained\nlanguage models.\n• We conduct experiments to demonstrate that\nthe proposed framework is capable of protect-\ning privacy leakage from pretrained language\nmodels.\n2 Preliminary\nPrivacy Definition Privacy preservation has be-\ncome an issue of great concern in the era of pre-\ntrained language models. Protecting privacy first\nrequires specifying the boundaries of privacy. The\ndefinition of privacy is broad. It is closely related\nto its context and discourse (Brown et al., 2022).\nIn any texts about, a specific person can be consid-\nered as private. For the convenience of research, a\nnarrow definition of privacy is usually taken (Sousa\nand Kern, 2023), which treats personal identity in-\nformation as privacy, such as names, ID numbers,\nphone numbers and other related expressions. The\nproposed DEPN can be adapted to the above two\ndefinitions.\nModel Editing Geva et al. (2021) find that the\nfeed-forward network module in Transformer (i.e.,\na two-layer perceptron) can be considered as a key-\nvalue memory, where each key corresponds to a\ntext pattern and each value represents a distribution\nover the vocabulary. Based on this finding, a strand\nof research, (Geva et al., 2021; Meng et al., 2022;\nDai et al., 2022) propose to edit factual knowledge\nencoded in pre-trained LLMs by locating neurons\nrelated to the entities of factual knowledge. The\nbasic idea of localization is to change the param-\neters of neurons, and then observe the changes in\nthe probability of the object entity predicted by the\nmodel. The neurons with greater influence on the\nprobability are more closely related to the object\nentity.\nHowever, these methods have a limitation that\nthey can only observe the probability change of\none token at a time. Semantic units are usually\ncomposed of a sequence of tokens, rather than a\nsingle token. This makes it impossible to use these\nmethods directly.\n3 Methodology\nThe proposed DEPN consists of three components:\nthe privacy neuron detector (§3.2), the privacy neu-\nron editor (§3.3) to erase the model memorization\nof privacy data, and the privacy neuron aggregator\n(§3.4) for privacy preservation in batches.\n3.1 Privacy Prediction Task\nGiven a tuple T = {X,Y}, let Y = {y1,...,y n}\nbe the sequence with private information, X be\nthe the context of the sequence, θbe the parame-\nters of a language model. Given a context X, the\n2876\nFigure 1: The diagram of DEPN . When a language model leaks privacy information, DEPN calculates privacy\nattribution scores using the Privacy Neuron Detector. It then selects the top z privacy neurons with the Privacy\nNeuron Aggregator and eliminates the model memorization of privacy information using the Privacy Editor.\nprobability of the language model yielding a token\nis P(yi|X,θ),yi ∈Y, so the probability of the\nmodel leaking the private sequence is:\nP(Y|X,θ) =\n|Y|∏\ni=1\nP(yi|X,θ) (1)\nTake \"An■ Ka■ is a senior writer at ESPN.com\" as\nprivate sentence containing a person’s name \"An■\nKa■\". Suppose the input to the language model\nis \"_ _ is a senior writer at ESPN.com\", our goal\nis to reduce the probability of privacy leakage, i.e.,\nminimizing the probability of predicting \"An ■\"\nand \"Ka■\" .\n3.2 Privacy Neuron Detector\nAs described in Section 2 factual knowledge is\nfound to be stored in the feed-forward networks\nof Transformer, in the form of key-value memory.\nInspired by this, we speculate that private infor-\nmation might be also encoded in specific neurons.\nModel editing has offered methods to locate and\nedit knowledge-related neurons. However, existing\nmethods can only deal with semantic units com-\nposed of a single token, making them not directly\napplicable to detect and edit mutli-token private\nsequences. To address this issue, we propose a pri-\nvacy attribution method based on gradient integra-\ntion. The proposed privacy attribution can evaluate\nwhich neurons play a key role in the leakage of\nprivate information from language models.\nLet wk\nl be a neuron to be evaluated by the pri-\nvacy attribution method, where lis the layer of the\nneuron in the language model, and kis its position.\nAccording to §3.1, the probability of the model\noutputting private information is:\nP(Y|X,wk\nl ) =\n|Y|∏\ni=1\nP(yi|X,wk\nl = αk\nl) (2)\nwhere αk\nl represents the value of the k-th neuron in\nthe l-ith FFN layer.\nWe gradually change the parameter of the target\nneuron from 0 to the original value of the neuron.\nIn this process, the probability of the output will\naccordingly change. We calculate the cumulative\ngradient of the probability change during this pro-\ncess as the neuron’s contribution (i.e., privacy attri-\nbution score) to the privacy-sensitive output. The\nprivacy attribution score is computed as:\nAtt(wk\nl ) =βk\nl\n∫βk\nl\n0\n∂P(Y|X,αk\nl)\n∂wk\nl\ndαk\nl (3)\nwhere βk\nl is the original value of the neuron wk\nl ,\n∂P(Y|X,αk\nl )\n∂wk\nl\ncalculates the gradient of the model\n2877\noutput with regard to wk\nl . Directly calculating con-\ntinuous integrals is intractable. We follow Dai\net al. (2022) to use Riemann approximation:\nAtt(wk\nl ) =βk\nl\nm\n∑m\nj=1\n∂P(Y|X, j\nmβk\nl )\n∂wk\nl\n(4)\nwhere m = 20 is the number of approximation\nsteps.\nAs P(Y|X,wk\nl ) = ∏|Y|\ni=1 P(yi|X,wk\nl = αk\nl),\nwe have\nAtt(wk\nl ) =\n|Y|∑\ni=1\nβk\nl\nm\n∑m\nj=1\n∂P(yi|X, j\nmβk\nl )\n∂wk\nl\n(5)\nIf the neuron has a great influence on the output of\na private information, the gradient will be signifi-\ncant, and a large integration value will be obtained.\nTherefore, the privacy attribution score can mea-\nsure the neuron’s contribution to the leakage of\nprivacy information, and the greater the privacy\nattribution score, the greater the privacy sensitivity\nof the neuron. We select neurons with the top z\nprivacy attribution score as candidates for editing.\n3.3 Privacy Editor\nAfter detecting the privacy neuron candidates with\nthe privacy neuron detector, we reduce the model\nmemorization of private information by editing.\nParticularly, we use a simple yet effective editing\nstrategy: setting the parameters (activation values)\nof the corresponding neurons to 0, so that the in-\nformation flow will not pass through these privacy\nneurons.\n3.4 Privacy Neuron Aggregator\nAs a number of sentences in the training data of\nLLMs contain private information, the privacy neu-\nron detection and editing can be done over multiple\nsentences in a batch processing way. To erase pri-\nvacy information encoded in the language model\nfrom multiple sentences in the training data, we\npropose the privacy neuron aggregator. When the\ninput is a text batch, we calculate the privacy attri-\nbution score matrix of each sequence in the batch.\nAfter the privacy attribution score calculation, we\nlet each sequence vote for neurons according to\ntheir privacy attribution scores, and select the top z\nneurons with the most votes. These selected neu-\nrons will be edited to erase private information.\nThe hyperparameter zis adjusted according to the\nmodel size, training epochs and other factors. More\ndetails can be found in (§5.1).\n4 Experiments\nWe carried out experiments to examine the effec-\ntiveness of the proposed DEPN on a dataset con-\ntaining private information.\n4.1 Setup\nDataset We used the Enron dataset (Klimt and\nYang, 2004). It consists of employee emails that\nwere publicly disclosed during Enron’s legal inves-\ntigation by the Federal Energy Regulatory Commis-\nsion. It is the largest publicly available collection of\n\"real\" email data, containing over 500,000 emails\nfrom 158 users. 1 We randomly sampled 5% of\nthe data from Enron as the validation dataset to\nevaluate model performance.\nPrivate Information Sampling In our study, we\ncategorized the private information in the Enron\ndataset into two types: private phrases (for the nar-\nrow definition of privacy), such as names and phone\nnumbers, and a batch of randomly sampled sen-\ntences to be edit. Names: We selected 20 unique\nnames that are memorized by language models,\nfound in 126 sentences, such as \"An ■ Ka■ is a\nsenior writer at ESPN.com\". Phone Numbers :\nWe also selected 20 unique LM-memorized phone\nnumbers, such as \"My phone number is 7 1 3 8 5■\n■ ■ ■ ■\". Private texts: We randomly selected\n100 sentences that are not semantically overlap-\nping with each other. In Appendix A.4, we discuss\nhow we determine whether private information is\nmemorized by a language model.\nModel Settings We conducted experiments us-\ning the widely used pretrained model, BERT-base\n(Devlin et al., 2018). The model consists of 12\ntransformer layers, with a hidden state size of 768\nand an internal hidden size of 3072 for the feed-\nforward network (FFN). Our experiments were per-\nformed on NVIDIA Tesla A6000 graphics pro-\ncessors. More training details are show in Ap-\npendix A.1.\nBaselines To demonstrate the effectiveness and\nrobustness of DEPN, we compared it with the fol-\nlowing baseline models. BERT-O:The bert model\nthat has not been trained on the Enron dataset.\nSince the model does not know the privacy infor-\nmation in the dataset, it provides an oracle for as-\nsessing the risk of privacy leakage; BERT-F:The\n1https://www.cs.cmu.edu/~enron/\n2878\nPrivacy Type Models Time↓ Valid-PPL↓ Privacy Leakage Risk\nMetric Value\nPhone Number\nBERT-O - 25.23\nExposure↓\n1.58\nBERT-F 100% 3.07 15.74\nBERT-FE 2.4% 3.11 9.78\nBERT-DP 181.4% 5.43 3.12\nName\nBERT-O - 25.23\nMRR↓\n0.87\nBERT-F 100% 3.07 1.21\nBERT-FE 4.4% 3.11 1.15\nBERT-DP 181.4% 5.43 0.95\nRandom Text\nBERT-O - 25.23\nPPL↑\n10.05\nBERT-F 100% 3.07 2.30\nBERT-FE 4.6% 3.11 3.67\nBERT-DP 181.4% 5.43 8.82\nTable 1: Results of testing the risks of leaking private Phone Numbers, Names, and Texts on different baseline\nmodels, as well as the efficiency of protection. Bold and underlined results indicate the best and second best result,\nrespectively. ↑: the higher the better. ↓: the lower the better.\nbert model trained on the Enron dataset, which cor-\nresponds to the best predictive performance on the\nEnron dataset, but has the greatest risk of privacy\nleakage; BERT-DP: A privacy model trained by\nthe differential privacy gradient descent method\n(Li et al., 2021) on the Enron dataset, which is the\ncommonly used privacy protection method when\nusing private data for training.\nWe applied our proposed DEPN on BERT-Fto\nmake a safe model, which is referred to as BERT-\nFE in following experiments. Our codes are avail-\nable now.2\nMetrics To observe the effect of different privacy\npreserving methods on the model performance, we\nuse the Perplexity of Masked Language Modeling\ntask on the Enron validation dataset (Valid-PPL)\nas the metric.\nDue to the different types of private information,\nwe provide metrics separately for the risk of privacy\nleakage.\nExposure: The exposure (Carlini et al., 2019)\nmetric is commonly used in privacy attacks to mea-\nsure the exposure risk of phone numbers. Given\na number sequence c, a model with parameters θ,\nand the randomness space R, the exposure eθ of c\ncan be calculated as :\neθ = log2 |R|−log2 Rankθ(c). (6)\nMean Reciprocal Rank (MRR): A person’s\nname is usually composed of multiple tokens.\nTherefore, we use the reciprocal average of the\nrank of each target token to measure the model’s\nmemorization of names. Given a prefix Q, a name\n2https://github.com/flamewei123/DEPN\ntoken sequence E= {e1,...,e n}, the length is |E|,\nthe model predicts the rank of the target token as\nrank(ei|Q), and the MRR for the name Eis cal-\nculated as follows:\n∑|E|\ni=1\n1\nRank(ei|Q)\n|E| . (7)\nPerplexity (PPL): When the private text is a\ncomplete sentence, we directly use the perplexity\nas the measure of the model memorization.\n4.2 Main Results\nTable 1 presents our main results, including model\nperformance, privacy leakage risk, and execution\ntime cost. The results demonstrate the competitive-\nness of our framework.\nFor the performance on the Enron validation\ndataset (Valid-PPL), BERT-O, which is not trained\non the Enron dataset, exhibits the poorest perfor-\nmance. BERT-DP trained with DP-SGD does not\nperform well either, due to noise introduced during\nbackpropagation. In contrast, BERT-FE equipped\nwith DEPN performs almost on par with BERT-F\non the validation dataset, indicating that neuron\nerasure minimally impacts model performance.\nRegarding privacy leakage risk metrics, includ-\ning exposure, MRR, and PPL, clearly indicate that\nBERT-FE equipped with DEPN achieve the reduc-\ntion of privacy leakage risk. BERT-F, trained di-\nrectly on private data, exhibits the highest risk. In\ncomparison, DEPN significantly reduces the risk of\nleakage. BERT-O, which has no access to private\ndata, demonstrates the lowest risk across all three\ndata types. The BERT-DP model also exhibits very\nlow risk.\n2879\n(a) Exposures with different number of edited neurons.\n (b) Model performance with different number of edited\nneuron.\nFigure 2: The performance of the model and the risk of privacy leakage with the change trend of the number of\nneurons edited.\nIn terms of execution time cost, we assume that\nthe fine-tuning time of BERT-F on data excluding\nprivacy is 100% (reference time cost). The DEPN\nframework requires less than 5% of the reference\ntime cost, while BERT-DP requires more time due\nto gradient clipping.\nIn conclusion, while differential privacy training\nand fine-tuning with non-private data can mitigate\nprivacy leakage risks, they incur more time and\nmay significantly undermine model performance.\nThe DEPN framework strikes an excellent balance\nbetween performance and privacy protection.\n5 Analysis\nWe further conducted in-depth analyses to demon-\nstrate why DEPN is able to dememorize privacy in\nLLMs from multiple perspectives, including anal-\nyses on the relationship between privacy neurons\nand model memorization, on the robustness as well\nas the cost-effectiveness of DEPN.\n5.1 Effect of the Hyperparameter\nFigure 2 illustrates the impact of the hyperparame-\nter, the number of edited neurons, on the model. We\ncalculate the exposures of the original model BERT-\nF and the enhanced model BERT-FE on 20 phone\nnumbers. In Figure 2(a), the red line represents\nthe average exposure of BERT-F, while the green\nline represents the average exposure of BERT-FE\nwith varying numbers of edited neurons. As the\nnumber of edited neurons increases, the exposure\nsignificantly decreases. In Figure 2(b), the purple\nline represents the PPL of BERT-F on the valida-\ntion set, while the blue line represents the PPL of\nBERT-FE on the validation set with different num-\nbers of edited neurons. As the number of erasures\nincreases, the PPL noticeably increases. Therefore,\nincreasing the number of edited neurons reduces\nthe risk of privacy leakage in the model, but it also\nleads to a decrease in the model performance.\n5.2 Relationship between Memorization And\nPrivacy Neurons\nAs it is widely recognized, privacy data leakage\noften stems from the model’s ability to memorize\nthe training data. In this subsection, we conducted\nexperiments to investigate the relationship between\nmodel memorization and privacy neurons, provid-\ning further evidence for the effectiveness of the\nproposed DEPN.\nImpact of Training Time on Privacy Neuron\nDistribution over Layers Figure 3 depicts the\nevolution of the distribution of privacy neurons\nover layers as the number of training epochs in-\ncreases. Overall, the distribution of privacy neu-\nrons is pyramid-shaped, and most privacy neurons\nidentified by the privacy neuron detector are lo-\ncated in layers 10-12 of BERT-base. Specifically,\nin epoch 1, about 40% of privacy neurons are in\nthe top layer of BERT-base. As training progresses,\nthe proportion of privacy neurons from deep layers\nincreases to 60% by epoch 3 and to 80% by epoch\n6. By the 9-th epoch, the distribution of privacy\nneurons remains largely unchanged compared to\nthe 6-th epoch. This suggests that as the depth\nof model training increases, the memorization of\n2880\n(a) epoch 1\n (b) epoch 3\n(c) epoch 6\n (d) epoch 9\nFigure 3: The distribution of privacy neurons in the bert-base model at different training epochs.\nModels # Edited NeuronsTime Before Editing After Editing Reduction RateValid-PPL ExposureValid-PPL Exposure\nbert-small 100 0.26h 4.09 5.10 4.57 3.39 33.5%\nbert-base 200 1.59h 3.07 15.74 3.11 9.78 37.86%\nbert-large 400 7.66h 2.93 18.10 2.98 7.63 57.84%\nTable 2: The privacy leakage risk reduction rate for models of different sizes.\nprivate data tends to converge.\nIn Appendix A.3, we conducted experiments to\nobserve the changes of privacy leakage risk reduc-\ntion at different training epoch. The results show\nthat when the training time increases, the risk of\nprivacy leakage increases too, and the proposed\nDEPN becomes more effective in privacy preserva-\ntion.\nEffect of the Model Size Table 2 illustrates the\nperformance of the DEPN framework on models\nof different scales. Each model was trained for\n10 epochs using the optimal hyperparameter set-\ntings. Overall, larger models require more time\nto identify privacy neurons and require editing a\ngreater number of privacy neurons for optimal per-\nformance. Larger models tended to show a deeper\nmemory for phone numbers before privacy neu-\nrons are edited, leading to higher exposure. After\nprivacy neuron editing, from the perspective of re-\nduction rate, the exposure of the large model is\nreduced even more. These findings suggest that\nlarger models are more at risk of privacy breaches.\nFortunately, the DEPN framework demonstrates\nbetter performance on larger models compared to\nsmaller ones, offering improved protection against\nprivacy risks.\nSummary of the Relationship between Mem-\norization and Privacy Neurons Based on the\naforementioned experimental findings, we can con-\nclude that the model’s scale, training time, and\nfrequency of privacy data occurrence are all factors\nthat have influence on the model memorization. As\nthe model memorization of privacy data deepens,\nthe aggregation of privacy neurons associated with\nprivacy data becomes more pronounced, which\nmakes the method of locating and eliminating pri-\nvacy neurons more suitable for deep memorization\nscenarios. Therefore, the DEPN framework has\ndemonstrated excellent effectiveness in mitigating\nmodel memorization.\n5.3 Robustness Analysis\nAblation Study We conducted ablation experi-\nments to assess the robustness of the privacy neu-\nron detector by comparing its performance with\ndifferent neuron localization methods on phone\nnumber data. In Table 4, we present the results of\nthese experiments. Specifically, \"KN\" refers to the\nknowledge attribution approach proposed by Dai\net al. (2022), while \"Random\" donates an approach\n2881\nPrivacy Amount# Edited NeuronsTime Before Editing After Editing\nValid-PPL ExposureValid-PPL Exposure\n20 200 0.76h 3.07 15.74 3.11 9.78\n100 500 1.59h 3.07 12.46 3.33 10.47\n1000 2000 17.61h 3.07 8.32 3.81 8.03\nTable 3: Analysis results on the cost-effectiveness of DEPN.\nMethods Before Editing After Editing\nValid-PPL ExposureValid-PPL Exposure\nPND + Editing 3.07 15.54 3.11 9.78\nKN + Editing 3.07 15.54 3.10 10.75\nRandom + Editing3.07 15.54 3.07 12.48\nTable 4: Effect of using different neuron localization\nmethods on results.\nthat randomly selects the same number of neurons\nas our method. Our method PND (privacy neuron\ndetector) achieves superior performance in terms\nof exposure reduction compared to the other meth-\nods. Although the knowledge attribution approach\ngains a good exposure reduction, it is less effec-\ntive than our method due to its attribution being\ntargeted at a single token. The random selection\napproach is also able to decrease privacy exposure\nbut the exposure reduction is not as significant as\nthe KN approach and our detector. These results\nunequivocally demonstrate the effectiveness of our\nmethod for in privacy neuron localization.\nRobustness to Different Prompts We conducted\nexperiments to validate the robustness of DEPN\nto different prompts. We sampled private data\ncontaining phone numbers, all composed of the\nsame prefix, from the training dataset. We then\nperformed privacy attacks during inference using\ndifferent prompts to examine whether changing\nprompts would still result in privacy leakage. Ta-\nble 5 presents the results of these experiments. The\ntraining data consist of phone numbers with the\nsame prefix of ‘Contact me at ***’. We observe\nprivacy risk reduction across all prompts, demon-\nstrating the robustness of DEPN to prompt.\n5.4 Analysis on the Cost-Effectiveness of\nDEPN\nIn this subsection we discuss the limitation of\nDEPN, specifically its dependency on the amount\nof private data to be erased. We conducted an exper-\niment where we used 1,000 private data instances,\neach containing phone numbers, extracted from our\ntraining dataset. DEPN was applied onto the BERT-\nbase model to erase private information. Experi-\nPrompts Original Exposure Exposure\n‘Contact me at ***’ 12.52 9.77 ↓\n‘Contact me at : ***’ 11.20 9.40 ↓\n‘Contact me : ***’ 12.50 9.68 ↓\n‘Call me at ***’ 12.31 11.82 ↓\n‘My phone number is ***’ 13.41 12.96 ↓\n‘You can call me at ***’ 13.04 12.84 ↓\nTable 5: Results with varying prompts during privacy\nattack. ‘Contact me at ***’ is the prefix to the private\nphone numbers in the training data, and the others are\nvarying prompts used in inference.\nment results are shown in Table 3. As the amount\nof private data increases, more neurons need to\nbe edited to achieve better privacy protection, and\nthe performance of the model drops significantly.\nFurthermore, it becomes apparent that, with the\nescalation of private data volume, the reduction in\nprivacy risks gradually diminishes. These obser-\nvations indicate that DEPN excels in remediating\nlanguage models when dealing with a small num-\nber of data leaks, but exhibits weak performance\nwhen confronted with a large batch of private data.\n6 Related Work\nModel Editing To edit incorrect or undesirable\ninformation captured in LLMs, a variety of model\nediting approaches have been proposed, which can\nbe categorized into four strategies. First, the Con-\nstrained Fine-tuning strategy (Zhu et al., 2020)\nupdates LLMs specifically for the target knowl-\nedge, allowing precise modification. Second, the\nMemory-based Editing strategy (Mitchell et al.,\n2022; Dong et al., 2022) maintains a knowledge\ncache that stores new information to replace unde-\nsirable predictions. Third, the Meta-learning-based\nEditing strategy (De Cao et al., 2021; Mitchell et al.,\n2021) introduces editable training based on meta-\nlearning, training model parameters to accommo-\ndate editing. Lastly, the Locating and Editing strat-\negy (Geva et al., 2021; Meng et al., 2022; Dai et al.,\n2022) assumes that knowledge is locally stored in\nLLMs. This strategy locates specific parameters\nassociated with the knowledge and directly edits\nparameters to perform editing.\n2882\nPrivacy Protection To address privacy risks in\nNLP models, various privacy-preserving methods\nhave been proposed, which can be categorized\ninto three main stages of application (Guo et al.,\n2022; Sousa and Kern, 2023): data processing\nstage, pre-training and/or fine-tuning stage, and\npost-processing stage. In the data processing stage,\nmethods involve removing or replacing sensitive\ninformation in the original data (Liu et al., 2017;\nEl Emam et al., 2009; Zhou et al., 2008; García-\nPablos et al., 2020). In the pre-training or fine-\ntuning stage, data privacy can be protected by mod-\nifying the model training process. One approach is\ndifferential privacy stochastic gradient descent (DP-\nSGD) (Li et al., 2021; Hoory et al., 2021), which\nintroduces noise into the clipped gradient to re-\nduce the distinction between gradients and prevent\nmemorization of training data. Another method is\nadversarial training (Plant et al., 2021; Coavoux\net al., 2018), which constrains the model’s learning\nof private information through adversarial training\ntechniques. However, methods used in the data pro-\ncessing stage and in the pre-training or fine-tuning\nstage are not applicable if the privacy leakage is\ndiscovered after the model training is completed.\nMethods used in the post-processing stage focus\non making trained models forget specific data or al-\nter specific parameters to safeguard hidden private\ninformation (Bourtoule et al., 2021; Gupta et al.,\n2021; Neel et al., 2020). These methods are often\nwith high computational cost and cannot be eas-\nily applied to large models. In contrast, proposed\nDEPN can achieve the protection of private infor-\nmation in the post-processing stage with a small\ncomputational overhead.\n7 Conclusion\nIn this paper, we have presented a privacy neuron\ndetecting and editing framework DEPN to address\nprivacy leakage risks in pretrained language mod-\nels. Through the privacy neuron detector based on\nthe privacy attribution scoring method, we accu-\nrately detect risky neurons associated with private\ninformation. The privacy neuron editor effectively\neliminates model memorization of private data. Ex-\nperimental results and in-depth analyses demon-\nstrate the ability of DEPN to reduce privacy risks\nefficiently without degrading model performance.\nOur work explores a novel approach to privacy pro-\ntection and contributes to model de-memorization\nin the post-processing stage.\nLimitations Our current study still has two lim-\nitations. First, although we propose a method to\nprocess private data in batches, we find that too\nmany instances in a batch will reduce the effect of\nmemorization erasure. Second, we use a few types\nof private information in our experiments due to\nthe limited availability of datasets containing pri-\nvate information. We would like to collect more\navailable datasets for our framework in the future.\nEthical Statement In this paper, we use the En-\nron dataset to evaluate the privacy-preserving ef-\nfect of DEPN. This dataset consists of employee\nemails that were publicly disclosed during Enron’s\nlegal investigation by the Federal Energy Regula-\ntory Commission. Since the data comes from real\npersons, we masked sensitive information such as\nspecific names and phone numbers in this paper.\nAcknowledgements\nThe work was partially supported by the research\ncollaboration project between Tianjin University\nand ByteDance(PJ20210625900030) and Zhejiang\nLab (No. 2022KH0AB01). We would like to thank\nthe anonymous reviewers for their insightful com-\nments.\nReferences\nPrajjwal Bhargava, Aleksandr Drozd, and Anna Rogers.\n2021. Generalization in nli: Ways (not) to go be-\nyond simple heuristics. In Proceedings of the Second\nWorkshop on Insights from Negative Results in NLP,\npages 125–135.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A\nChoquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\nZhang, David Lie, and Nicolas Papernot. 2021. Ma-\nchine unlearning. In 2021 IEEE Symposium on Secu-\nrity and Privacy (SP), pages 141–159. IEEE.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model\nto preserve privacy? In Proceedings of the 2022\nACM Conference on Fairness, Accountability, and\nTransparency, pages 2280–2292.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\n2883\nral networks. In 28th USENIX Security Symposium\n(USENIX Security 19), pages 267–284.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nMaximin Coavoux, Shashi Narayan, and Shay B Co-\nhen. 2018. Privacy-preserving neural representations\nof text. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1–10.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,\nZhifang Sui, and Lei Li. 2022. Calibrating factual\nknowledge in pretrained language models. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 5937–5947.\nKhaled El Emam, Fida Kamal Dankar, Romeo Issa,\nElizabeth Jonker, Daniel Amyot, Elise Cogo, Jean-\nPierre Corriveau, Mark Walker, Sadrul Chowdhury,\nRegis Vaillancourt, et al. 2009. A globally optimal k-\nanonymity method for the de-identification of health\ndata. Journal of the American Medical Informatics\nAssociation, 16(5):670–682.\nAitor García-Pablos, Naiara Pérez, and Montse Cuadros.\n2020. Sensitive data detection and classification in\nspanish clinical text: Experiments with bert. In Pro-\nceedings of the 12th Language Resources and Evalu-\nation Conference, pages 4486–4494.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5484–5495.\nShangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu,\nand Tianwei Zhang. 2022. Threats to pre-trained lan-\nguage models: Survey and taxonomy. arXiv preprint\narXiv:2202.06862.\nVarun Gupta, Christopher Jung, Seth Neel, Aaron Roth,\nSaeed Sharifi-Malvajerdi, and Chris Waites. 2021.\nAdaptive machine unlearning. In Advances in Neural\nInformation Processing Systems, volume 34, pages\n16319–16330. Curran Associates, Inc.\nPeter Henderson, Koustuv Sinha, Nicolas Angelard-\nGontier, Nan Rosemary Ke, Genevieve Fried, Ryan\nLowe, and Joelle Pineau. 2018. Ethical challenges\nin data-driven dialogue systems. In Proceedings of\nthe 2018 AAAI/ACM Conference on AI, Ethics, and\nSociety, pages 123–129.\nShlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell,\nAlon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri\nStemmer, Ayelet Benjamini, Avinatan Hassidim, et al.\n2021. Learning and evaluating a differentially pri-\nvate pre-trained language model. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1178–1189.\nBryan Klimt and Yiming Yang. 2004. Introducing the\nenron corpus. In CEAS, volume 45, pages 92–96.\nPeter Lee, Sebastien Bubeck, and Joseph Petro. 2023.\nBenefits, limits, and risks of gpt-4 as an ai chatbot\nfor medicine. New England Journal of Medicine ,\n388(13):1233–1239.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and\nYangqiu Song. 2023. Multi-step jailbreaking privacy\nattacks on chatgpt. arXiv preprint arXiv:2304.05197.\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2021. Large language models can be\nstrong differentially private learners. In International\nConference on Learning Representations.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nZengjian Liu, Buzhou Tang, Xiaolong Wang, and Qing-\ncai Chen. 2017. De-identification of clinical notes\nvia recurrent neural network and conditional random\nfield. Journal of biomedical informatics , 75:S34–\nS42.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in gpt. Advances in Neural Information Pro-\ncessing Systems, 35:17359–17372.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2021. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022. Memory-\nbased model editing at scale. In International Con-\nference on Machine Learning, pages 15817–15831.\nPMLR.\n2884\nSeth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi.\n2020. Descent-to-delete: Gradient-based methods\nfor machine unlearning. In International Conference\non Algorithmic Learning Theory.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nRichard Plant, Dimitra Gkatzia, and Valerio Giuffrida.\n2021. Cape: Context-aware private embeddings for\nprivate language learning. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7970–7978.\nSamuel Sousa and Roman Kern. 2023. How to keep\ntext private? a systematic review of deep learning\nmethods for privacy-preserving natural language pro-\ncessing. Artificial Intelligence Review, 56(2):1427–\n1492.\nOm Dipakbhai Thakkar, Swaroop Ramaswamy, Rajiv\nMathews, and Francoise Beaufays. 2021. Under-\nstanding unintended memorization in language mod-\nels under federated learning. In Proceedings of the\nThird Workshop on Privacy in Natural Language Pro-\ncessing, pages 1–10.\nBin Zhou, Jian Pei, and WoShun Luk. 2008. A brief sur-\nvey on anonymization techniques for privacy preserv-\ning publishing of social network data. ACM Sigkdd\nExplorations Newsletter, 10(2):12–22.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\narXiv preprint arXiv:2012.00363.\n2885\nA Appendix\nA.1 Training Details\nFor BERT-base fine-tuning, we set the hyperpa-\nrameters as follows: 20 training epochs, a learning\nrate of 3e-5 with linear warm-up, and a batch size of\n16. We fine-tuned BERT-baseon the Enron dataset\nusing the Masked Language Modeling task to simu-\nlate training on datasets containing privacy informa-\ntion. Additionally, we pretrained smaller (layer=4,\nhidden size=512, intermediate size=2048) (Bhar-\ngava et al., 2021) and larger ( layer=24, hidden\nsize=1024, intermediate size=4096) BERT mod-\nels 3 to compare the performance of privacy erasure\nat different model scales.\nA.2 Effect of the Frequency of Privacy Data\nOcurrence\nWe also examined the influence of the frequency of\nprivacy data ocurrence in the training set on DEPN.\nAs shown in Table 6, phone numbers with an ocur-\nrence frequency greater than 10 exhibit higher ex-\nposure compared to those with a frequency less\nthan 10, indicating a higher risk of leakage. How-\never, after erasure, the exposure of phone num-\nbers with a frequency greater than 10 is reduced\nby 32.65%, while the exposure of phone numbers\nwith a frequency less than 10 is reduced by 22.58%.\nThese results suggest that our method effectively\nreduces exposure for both high-frequency and low-\nfrequency phone numbers, mitigating the risk of\nprivacy leakage.\nFrequency Original Exposure Exposure\n>=10 23.15 15.59\n<10 8.90 6.89\nTable 6: Frequency of privacy data ocurrence make\nexposure different.\nA.3 Effect of Training Time\nFigure 4 illustrates the changes in exposure of\nphone number data before and after erasing pri-\nvacy neurons in models with different training\nepochs. We conducted experiments using 20 differ-\nent phone numbers and averaged the final results.\nThe blue line represents the exposure of phone\nnumbers before privacy neuron erasing. The blue\nline initially remains low but exhibits a significant\nsurge after the 10-th epoch, indicating that models\n3https://huggingface.co/BERT-large-uncased\nFigure 4: Comparison of privacy leakage risk reduction\nat different training epochs.\nwith longer training time have a more pronounced\nmemorization of the training data. Additionally,\nthe yellow line represents the exposure of phone\nnumbers after privacy neuron erasing. The widen-\ning gap between the two lines indicates that as the\nmodel’s memorization becomes more apparent, the\nproposed DEPN becomes more effective in privacy\npreservation.\nA.4 The Judgement of the Memorization\nIn our experiment, we specifically identify the pri-\nvate data memorized by the language model from\nthe training dataset. To assess whether the model\nhas memorized private data, we employ the con-\ntext of private information as the input to the lan-\nguage model. Subsequently, we calculate the risk\nof private information leakage and classify the in-\nformation with a leakage risk exceeding predefined\nthresholds as having been memorized by the lan-\nguage model. For names, we establish a threshold\nfor memorization as the MRR of less than 1.5. For\nphone numbers, we employ the Exposure values\nexceeding 15 as memorization.\n2886",
  "topic": "Memorization",
  "concepts": [
    {
      "name": "Memorization",
      "score": 0.8979406356811523
    },
    {
      "name": "Computer science",
      "score": 0.8395907878875732
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6602827310562134
    },
    {
      "name": "Language model",
      "score": 0.5951610207557678
    },
    {
      "name": "Data modeling",
      "score": 0.4709293246269226
    },
    {
      "name": "Phone",
      "score": 0.4553258419036865
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43090981245040894
    },
    {
      "name": "Information privacy",
      "score": 0.41244742274284363
    },
    {
      "name": "Machine learning",
      "score": 0.41163209080696106
    },
    {
      "name": "Computer security",
      "score": 0.24753275513648987
    },
    {
      "name": "Database",
      "score": 0.1283213496208191
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 26
}