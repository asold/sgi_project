{
  "title": "Exploring the Potential of Large Language Models in Personalized Diabetes Treatment Strategies",
  "url": "https://openalex.org/W4382792994",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1985984607",
      "name": "Hao Yang",
      "affiliations": [
        "West China Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2108981464",
      "name": "Jiaxi Li",
      "affiliations": [
        "Maternal and Child Health Hospital of Sichuan Province"
      ]
    },
    {
      "id": "https://openalex.org/A2555062769",
      "name": "Siru Liu",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1997816401",
      "name": "Lei Du",
      "affiliations": [
        "Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2412585477",
      "name": "Xiali Liu",
      "affiliations": [
        "Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2103106343",
      "name": "Yong Huang",
      "affiliations": [
        "Ministry of Education Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2611232877",
      "name": "Qingke Shi",
      "affiliations": [
        "Ministry of Education Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2125685373",
      "name": "Jialin Liu",
      "affiliations": [
        "Sichuan University",
        "West China Medical Center of Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A1985984607",
      "name": "Hao Yang",
      "affiliations": [
        "West China Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2108981464",
      "name": "Jiaxi Li",
      "affiliations": [
        "Maternal and Child Health Hospital of Sichuan Province"
      ]
    },
    {
      "id": "https://openalex.org/A2555062769",
      "name": "Siru Liu",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A1997816401",
      "name": "Lei Du",
      "affiliations": [
        "West China Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2412585477",
      "name": "Xiali Liu",
      "affiliations": [
        "West China Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2103106343",
      "name": "Yong Huang",
      "affiliations": [
        "Ministry of Education Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2611232877",
      "name": "Qingke Shi",
      "affiliations": [
        "Ministry of Education Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2125685373",
      "name": "Jialin Liu",
      "affiliations": [
        "West China Medical Center of Sichuan University",
        "Sichuan University",
        "West China Hospital of Sichuan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3019636814",
    "https://openalex.org/W2995999703",
    "https://openalex.org/W2950048237",
    "https://openalex.org/W4366769280",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4223489644",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4391551777",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4362601804",
    "https://openalex.org/W2250389441",
    "https://openalex.org/W4317910576",
    "https://openalex.org/W4380995257"
  ],
  "abstract": "Abstract This study aims to explore the application of a fine-tuned model-based outpatient treatment support system for the treatment of patients with diabetes, and evaluate its effectiveness and potential value. Methods The ChatGLM model was selected as the subject of investigation and trained using the P-tuning and LoRA fine-tuning methods. Subsequently, the fine-tuned model was successfully integrated into the Hospital Information System (HIS). The system generates personalized treatment recommendations, laboratory test suggestions, and medication prompts based on patients’ basic information, chief complaints, medical history, and diagnosis data. Results Experimental testing revealed that the fine-tuned ChatGLM model is capable of generating accurate treatment recommendations based on patient information, while providing appropriate laboratory test suggestions and medication prompts. However, for patients with complex medical records, the model’s outputs may carry certain risks and cannot fully substitute outpatient physicians’ clinical judgment and decision-making abilities. The model’s input data is confined to electronic health record (EHR), limiting the ability to comprehensively reconstruct the patient’s treatment process and occasionally leading to misjudgments of the patient’s treatment goals. Conclusion This study demonstrates the potential of the fine-tuned ChatGLM model in assisting the treatment of patients with diabetes, providing reference recommendations to healthcare professionals to enhance work efficiency and quality. However, further improvements and optimizations are still required, particularly regarding medication therapy and the model’s adaptability.",
  "full_text": "Exploring the Potential of Large Language Models in Personalized \nDiabetes Treatment Strategies \n \nHao Yang 1#, Jiaxi Li 2#, Siru Liu 3, Lei Du 4, Xiali Liu 4, Yong Huang 5, Qingke Shi 5, \nJialin Liu 1,6* \n1. Information Center, West China Hospital, Sichuan University, Chengdu, China \n2. Department of Clinical Laboratory Medicine, Jinniu Maternity and Child Health \nHospital of Chengdu, Chengdu, China \n3.Department of Biomedical Informatics, Vanderbilt University Medical Center, \nNashville, TN, USA \n4. Anesthetic Surgery Center, West China Hospital, Sichuan University, Chengdu, \nChina \n5. Engineering Research Center of Medical Information Technology \n， Ministry of \nEducation \n6. Department of Medical Informatics, West China Medical School, Chengdu, China \n# these authors contributed equally \n* Corresponding author  \nJialin Liu, MD \nDepartment of Medical Informatics, West China West China Hospital, Sichuan \nUniversity, No. 37 Guoxue Road, Chengdu, 610041, China \nPhone: 86 28 85422306 \nFax: 86 28 85582944 \nEmail: Dljl8@163.com\n \nFoundation: \nNational Key Research and Development Program: Construction of a Big Data Cloud Service Plat\nform for Health Examinations(2020YFC2003404) \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \nAbstract: This study aims to explore the application of a fine-tuned model-based \noutpatient treatment support system for the treatment of patients with diabetes, and \nevaluate its effectiveness and potential value. \nMethods: The ChatGLM model was selected as the subject of investigation and \ntrained using the P-tuning and LoRA fine-tuning methods. Subsequently, the \nfine-tuned model was successfully integrated into the Hospital Information System \n(HIS). The system generates personalized treatment recommendations, laboratory test \nsuggestions, and medication prompts based on patients' basic information, chief \ncomplaints, medical history, and diagnosis data. \nResults: Experimental testing revealed that the fine-tuned ChatGLM model is \ncapable of generating accurate treatment recommendations based on patient \ninformation, while providing appropriate laboratory test suggestions and medication \nprompts. However, for patients with complex medical records, the model's outputs \nmay carry certain risks and cannot fully substitute outpatient physicians' clinical \njudgment and decision-making abilities. The model's input data is confined to \nelectronic health record (EHR), limiting the ability to comprehensively reconstruct the \npatient's treatment process and occasionally leading to misjudgments of the patient's \ntreatment goals. \nConclusion: This study demonstrates the potential of the fine-tuned ChatGLM \nmodel in assisting the treatment of patients with diabetes, providing reference \nrecommendations to healthcare professionals to enhance work efficiency and quality. \nHowever, further improvements and optimizations are still required, particularly \nregarding medication therapy and the model's adaptability. \nKey words: ChatGLM; Artificial intelligence; Large language models; Diabetes; \nEHR. \nIntroduction \nDiabetes is a chronic metabolic disorder characterized by elevated blood glucose \nlevels, which over time can cause serious damage to the heart, blood vessels, eyes, \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nkidneys and nervous system. The most common type is type 2 diabetes, which \naccounts for about 90-95% of diabetes cases [1] and affects mainly adults. According \nto the World Health Organization, approximately 422 million people worldwide have \ndiabetes, the majority in low- and middle-income countries, and 1.5 million people \ndie each year as a direct result of diabetes [2]. The incidence and prevalence of \ndiabetes have been steadily increasing over the last few decades [1]. As diabetes \npatients require long-term medication to control blood glucose levels and prevent \ncomplications [3], they can face several challenges during the treatment process, such \nas medication selection, dosage adjustment and management of adverse effects. \nFailure to address these issues in a timely manner can compromise the efficacy of \nmedication and even pose a threat to patients' lives [4]. Therefore, people with \ndiabetes need timely medication advice, health education and nutrition support to help \nthem use their medicines correctly, safely and effectively, thereby improving \nadherence and quality of life. To better serve patients and increase the efficiency of \nhealthcare professionals, we aim to optimize the management of diabetes patients \nthrough the application of artificial intelligence. \nWith the significant success of ChatGPT in tasks related to understanding and \ngenerating human-like responses [5], large language models (LLMs) have attracted \nconsiderable attention. They have shown strong performance in various natural \nlanguage processing (NLP) tasks and the ability to generalize to unfamiliar tasks, \ndemonstrating their potential as a unified solution for natural language understanding, \ntext generation and dialogue. Although ChatGPT has shown promising results in \nmedical document summarization and decision support [7,8], as well as in passing the \nUnited States Medical Licensing Examination (USMLE) Step 1 and 2 [6], the \nexploration of these broad-domain LLMs in the medical field is still relatively limited \n[9]. Currently, there is a lack of specifically trained LLMs in the field of healthcare. \nTo address this gap, we plan to fine-tune a large language model using de-identified \ndata from diabetes patients, with the aim of exploring its application in diabetes \nmanagement. In addition, harnessing the potential of large language models will open \nup new opportunities for medical research and practice, and drive advances and \ninnovation in healthcare technology. \nMethods \nIn this study, the West China Hospital Big Data Integration Platform will be used \nas the data collection source [10]. We will collect EHR data from patients diagnosed \nwith diabetes who visited the outpatient department from January 2022 to February \n2022. The collected data will include information such as the patient's department of \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nvisit, age, gender, chief complaint, present illness history, and diagnosis, which will \nserve as input for the model. In addition, we will obtain data on patients' outpatient \nmedications, laboratory tests, examinations, and physician opinions, which will be \nused as model outputs. Patient data with missing chief complaints and present illness \nhistory will be excluded to ensure data quality and usability. Furthermore, we will \ncollect an additional set of data from 300 patients (visiting in March 2022) as a test set \nto evaluate the performance of the optimized model. Figure 1 illustrates the data \ncollection process. \n \nOutpatient electronic medical \nrecord\nAppointment date (2022-\n01~2022.02)\nN=530637\nPatient with diabetes mellitus\nN=21790\nExclude:\nno present medical history  \n(N=496)\nno chief complaints (N=675)\nTrain data\n(N=20619)\nTest data\nAppointment date (2022-03)\n(diabetes, N=300)\n \nFigure1. The patient selection process \nData Pre-processing  \nPrior to model training, we pre-process the collected data. This includes data \ncleaning, standardization and anonymization. Data cleaning involves the removal of \nmissing values, outliers and duplicates. Standardization ensures that different data \nelements have a consistent format and unit, which facilitates the input and output \nprocessing of the model. To protect patient privacy, we will anonymize the personal \nidentity information of patients. For example, we will use anonymization techniques \nto replace or remove names and identification numbers. \nModel selection \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nChatGLM is an LLM developed by Tsinghua University and Zhipu.AI [11]. \nChatGLM-6B is an open-source dialogue language model based on the General \nLanguage Model (GLM) architecture, specifically designed for question answering in \nboth Chinese and English. It consists of 6 billion parameters [11]. The open-source \nnature of the model is an important consideration when choosing a model. With the \nimplementation of model quantization techniques, it can be deployed locally on \nconsumer-grade graphics cards. The experimental environment for this study utilized \na single GPU, specifically an RTX 3090 with 24GB of memory, and a CPU with 16 \nvCPUs (Intel(R) Xeon(R) Platinum 8350C CPU @ 2.60GHz). These hardware \ncomponents were used to facilitate the training and inference processes of the \nChatGLM-6B model. \nFine-tune \nWe will use ChatGLM as the base model and fine-tune it for specific tasks in the \nfield of diabetes management. For the fine-tuning process, we will employ commonly \nused techniques for LLMs, namely P-tuning [12] and LoRA [13]. These methods have \nbeen proven effective in adapting pre-trained language models to domain-specific \ntasks and improving their performance [14].  \nP-tuning: P-tuning is a method that automatically constructs templates for \nlanguage models to perform downstream tasks [12]. It uses [unused]* tokens as \ncontinuous prompts and optimizes them with labelled data. It can achieve comparable \nor better performance than fine-tuning with much fewer trainable parameters. \nP-tuning v2 is an improved version of P-tuning that applies continuous prompts to \neach layer of the pre-trained model and adapts it to natural language understanding \ntasks [15]. In this study, we used p-tuning v2 as the technical implementation of \nfine-tuning (Figure 2). \n \nh0 h1 hi...\n...\nChatGLM Pretrained Model\n(Most scales, multiple tasks)\nEmbedding\nOutputP-tuning v2\nFigure 2 P-tuning (Each layer of the transformer requires fine-tuning of the \nembedding) \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \n \n \nLoRA： Lora (low-rank adaptation) is a low-rank adaptation technique that allows \nfor a significant reduction in the number of trainable parameters in downstream tasks \nwhile keeping the weights of the pre-trained model unchanged [13]. This method is \nsimilar to matrix factorization, where a trainable low-rank decomposition matrix is \ninjected into each layer of the Transformer structure. By employing a fully connected \nlayer, the dimensionality of the trainable layers is reduced from 'd' to 'r', and then \nmapped back to the original dimension 'd' through another fully connected layer, \nwhere 'r' represents the rank of the matrix, and 'r<<d'. As a result, the computational \ncomplexity of matrix operations is reduced from 'd×d' to 'd×r + r×d', leading to a \nsignificant decrease in the number of parameters. In Lora, both parameters A and B \nare initialized using random Gaussian initialization (Figure 3). \n \n \nPretrained  Weights \nof ChatGLM\nB=0\nX\nh\nd\n \nFigure3. LoRA (Fine-tuning is only required for the matrices B and A)[13] \nModel metrics  \nBLEU: BLEU (Bilingual Evaluation Understudy) measures the effectiveness of a \nmodel by computing the n-gram matching between the machine-generated content and \nthe reference standard text [16].  \n/g1828/g1838/g1831 /g1847 /g3404 /g1828 /g1842 /g3400 /g1857/g1876 /g1868 /g4666 /g3533 /g1875 /g3041\n/g3015\n/g3041/g2880 /g2869\n/g1864/g1867/g1859  /g1842 /g3041 /g4667  \n \nIn the BLEU, the BP (Brevity Penalty) is a penalty factor that addresses the issue \nof shorter machine-generated content compared to the reference text. The weights, w\nn, \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nrepresent the importance assigned to each n-gram matching score. P n refers to the \nprecision of the n-gram matching.  \nROUGE: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is an \nevaluation metric that compares the system-generated content with manually generated \nreference texts. It measures the overlap between the two by counting the number of \nshared basic units, such as n-grams, word sequences, and word pairs, to assess the \neffectiveness of a model [17]. In the context of Chinese NLP tasks, ROUGE Chinese is \nemployed for algorithm optimization. It involves transforming Chinese summaries into \nnumeric IDs and then utilizing dynamic programming to calculate the counts of \nmatching units, such as the longest common subsequence and N-grams. This approach \nallows for the computation of ROUGE-N and ROUGE-L metrics. \n/g1844/g1867 /g1873 /g1859 /g1857 /g3398 /g1840 /g3404\n∑\n/g3020 /g1488 /g3019/g3032 /g3033/g3032/g3045/g3032/g3041/g3030/g3032 /g3020/g3048/g3040/g3040 /g3028/g3045 /g3036/g3032/g3046\n∑\n/g3034/g3045 /g3028 /g3040 /g3289 /g1488/g3020\n/g1829 /g1867/g1873/g1866/g1872 /g3040/g3028 /g3047 /g3030 /g3035 /g4666/g1859/g1870/g1853 /g1865 /g3041 /g4667\n∑\n/g3020 /g1488 /g3019/g3032/g3033 /g3032/g3045/g3032/g3041 /g3030 /g3032/g3020 /g3048/g3040 /g3040 /g3028 /g3045/g3036/g3032 /g3046\n∑\n/g3034/g3045/g3028 /g3040 /g3289 /g1488/g3020\n/g1829/g1867/g1873 /g1866/g1872/g4666 /g1859/g1870/g1853 /g1865 /g3041 /g4667  \n \nIn the Roug-N, S represents the set of generated content, gram\nn represents the n-gram, \nCount_match (gramn) indicates the number of n-grams shared between the reference \nand generated content, and Count (gram n) represents the total number of n-grams in \nthe reference. ROUGE-N is used to measure the overlap at the n-gram level between \nthe automatically generated text and the reference text. It is a commonly used \nevaluation metric. \n \n \n/g1844/g1864 /g1855 /g1871 /g3404 /g1838/g1829 /g1845 /g4666 /g1850, /g1851 /g4667\n/g1865  \n/g1842/g1864/g1855/g1871 /g3404 /g1838/g1829/g1845/g4666/g1850 , /g1851 /g4667\n/g1866  \n/g1844/g1867/g1873/g1859 /g1857 /g3398 /g1838 /g3404 /g1832 /g3039/g3030/g3046 /g3404 /g46661 /g3397 /g2010 /g2870 /g4667/g1844 /g3039/g3030 /g3046 /g1842 /g3039/g3030 /g3046\n/g1844 /g3039/g3030 /g3046 /g3397/g2010 /g2870 /g1842 /g3039/g3030 /g3046\n \n \nIn the Rouge-L, LCS(X, Y) is the length of the longest common subsequence between \nX and Y . m and n are lengths of the reference summary and the automatic summary, \ntypically measured in the number of words. R\nlcs and P lcs denote recall and precision, \nrespectively. Finally, Flcs corresponds to what we refer to as Rouge-L. In this formula, \nβ  is set to a large value, so Rouge-L primarily considers Rlcs. \n  Physician assessment of recommendations \n  After the fine-tuning process, we will evaluate the model using the reserved dataset \nof 300 patients. The evaluation will assess the accuracy and consistency of the model in \nterms of diabetes treatment recommendations. We will compare the model-generated \noutputs and seek evaluations from three endocrinologists based on the following three \ncategories: useful, useless, and harmful. \nUseful: This category refers to cases where the model provides correct treatment \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nrecommendations that are helpful in clinical practice [18]. \nUseless: This category pertains to instances where the content generated by the \nmodel lacks logical coherence or specific guiding significance in certain tasks [19]. \nHarmful: This category addresses potential negative impacts and risks that the model \nmight introduce in its application [20]. For example, it may include erroneous \nmedication or treatment suggestions. \n  Model integrated \nThe fine-tuned model will be integrated into the EHR system (Figure 4) to facilitate \nthe management of diabetes patients by clinical physicians. This integration will \nenable the provision of personalized treatment recommendations for diabetes patients. \nBy integrating the model with the EHR, physicians will have easy access to the \nsystem's output, enhancing their ability to make informed decisions and develop \ntailored management plans for patients with diabetes. \n \nPatient\nDiagnosis history of \npresent illness\nBaisc \ninformation\nChief \ncomplaint\nIn-clinic of consulation\nChatGLM-fintuing\nLoRA P-tuning\nAutomatic collection\nDiagnosis and treatment \nrecommendation\nDoctor issues medical \norders\nEHR\nLLM\nTraining data\n \nFigure 4. ChatGLM fine-tuning based treatment aid model for diabetes \nEthics \nThroughout the research and model development process, we will strictly adhere \nto ethical standards and privacy principles. Patient data will be de-identified to protect \ntheir privacy. We will ensure secure storage and transmission of the data and will only \nuse it for model training and evaluation when necessary. In addition, this study has \nbeen approved by the Ethics Committee of West China Hospital, Sichuan University \n(2021-1450). \nResults \nThe LoRA and P-Tuning training resulted in a training loss curve for both \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nfine-tuning methods (Figure 5).  The results show that the LoRA can converge faster \nand achieve better fine-tuning results than P-Tuning. These results indicate that the \nLoRA fine-tuning method achieves significant improvements in the task of diabetes \nmanagement, enhancing model performance and adaptability. It provides clinicians \nwith more accurate and personalized guidance and recommendations. \n \nFigure 5. Training Loss for LoRA and P-Tuning  \n \nThe results shown in Table 2, the comparison between the fine-tuned models \nand the original model in terms of BLEU and Rouge on the test set are as follows. The \nmodel after LoRA tuning shows a significant improvement over the original model (p \n< 0.05). The model after P-Tuning shows a significant improvement in the BLEU \nscore compared to the original model (p < 0.05), while there is no improvement in the \nRouge score (p > 0.05). \n \nTable 2 Performance analysis of based on BLEU and Rouge scores (mean:std) \nScore Original P-Tuning P (Orignal vs P-Tuning) LoRA  P (Orignal vs Lo RA)\nBLEU-4 42.02(0.81) 50.22(3.47) <0.05 56.54 (3.49) <0.05 \nRouge-1 9.40(0.04) 9.41(0.04) 0.7 33.23 (4.02) <0.05 \nRouge-2 0.58(0.09) 0.49(0.08) 0.7 20.37 (1.66) <0.05 \nRouge-L 6.18(0.07) 6.21(0.11) 1.0 28.75 (2.86) <0.05 \n \n \n    For the same question, the original model, the model after P-tuning and the \nLoRA show the following specific performances: \nExample \nInput:  \n##Department: Endocrinology and Metabolism Medical Unit \n##Gender: Male \n##Age: 65~70 years old \n##Diagnosis: Type 2 diabetes; hyperlipidemia \n) \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \n##Chief complaint： Diabetes follow-up visit \n##Medical history： Diabetes for 2 more years \nReal clinical data \nOutput:  \n##Laboratory tests \nBiochemistry 2: Liver and kidney glucose and fat-a, glycated hemoglobin (HbA1c) \n##Medication: \nGanjing insulin injection (3ml:300 units/pre-filled) 11 U once a day, OuTangning \n(5mg×7 tablets) 5 mg once a day, Atorvastatin calcium tablets (10mg×28 tablets) 10 mg \nonce every night. \nOrigin ChatGLM \nOutput:  \nA patient with diabetes who comes for a follow-up visit needs to undergo a detailed \nmedical history and physical examination in order to better understand the patient’s \ncondition and develop a treatment plan. According to your description, the patient has \ntype 2 diabetes and hyperlipidemia. Diabetes is a chronic disease that requires long-term \nmanagement and treatment. Hyperlipidemia is a common metabolic disorder that can \naffect the patient’s cardiovascular health and increase the risk of cardiovascular disease. \nDuring the follow-up visit, the patient needs to monitor blood glucose levels and adjust \nthem according to the control of blood glucose levels, diet, exercise, etc. In addition, the \npatient also needs to undergo regular blood lipid level checks and develop corresponding \ntreatment plans based on the results of the checks. \nFor diabetes, common treatment options include oral medications, insulin injections, \nblood glucose monitoring, and dietary control. During the treatment process, patients \nneed to actively cooperate with doctors, adhere to dietary and exercise control, and have \nregular follow-up visits to maintain stable and healthy blood glucose levels. \nP-tuning ChatGLM \nOutput: \nAccording to the patient’s diagnosis, he needs to be treated for both type 2 diabetes and \nhyperlipidemia. Here are some treatment recommendations: \n1. Drug treatment: The patient needs to continue drug treatment, including oral \nmedications and insulin. Depending on the patient’s blood glucose and blood lipid levels, \nthe doctor may adjust the drug dosage or switch to another medication. \n2.Dietary control: The patient needs to control his diet, avoid high-sugar and high-fat \nfoods, and increase his exercise. It is recommended that patients eat high-fiber, low-fat, \nand low-sugar foods such as vegetables, fruits, whole grains, legumes, lean meats, fish, \netc. \n3. Lifestyle adjustments: The patient needs to quit smoking, limit alcohol consumption, \ncontrol weight, maintain regular exercise and sleep patterns. \n4. Regular follow-up visits: The patient needs to have regular follow-up visits to monitor \nblood glucose and blood lipid levels and adjust drug dosage or lifestyle in a timely \nmanner. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nWeight control: The patient needs to control his weight and avoid obesity to reduce the \nrisk of hyperlipidemia. \nLoRA ChatGLM \nOutput:  \n##Laboratory tests \nGlycosylated hemoglobin (HbA1c) \nBiochemistry 1: Liver and kidney lipase-a, Fasting insulin, 2-hour postprandial insulin, \nFasting plasma glucose, 2-hour postprandial C-peptide, 2-hour postprandial plasma \nglucose \n##Medication: \nGan Jing insulin injection (3ml:300 units/pre-filled) 11 U once a day, OuTangning \n(5mg×7 tablets) 5 mg once a day \n## Physician's recommendation: \nRegular follow-up is recommended. \n \nRecommended evaluation \nTable 4 shows the results of three endocrinologists' evaluation of the 300 \ntreatment recommendations generated by ChatGLM. The results show that more than \n50% of the recommendations are clinically relevant to diabetes management, while \nmore than 30% are of limited help to physicians. In addition, about 10% of the \nrecommendations carried potential risks, such as incorrect treatment \nrecommendations or overtreatment. There was no significant difference in the results \nof the three physician ratings (p>0.05). \nTable 4.  Physician assessment of recommendations \nDoctor ☺ useful /neutralface useless /frownface harmful P \n#1 187(62.33%) 91(30.33%) 22(7.33%) 0.26 \n#2 172(57.33%) 95(31.67%) 33(11.00%) \n#3 167(55.67%) 108(36%) 25(8.33%) \n \nIntegration with existing EHR \n   In order to seamlessly integrate ChatGLM into existing clinical workflows and \nEHR systems, we conducted internal testing by integrating ChatGLM into the \nhospital's EHR system within an internal test environment. In this internal test \nenvironment, when a physician completes the patient's basic information, chief \ncomplaints, history of present illness and diagnosis, the system automatically collects \nthe information and uses a fine-tuned GLM to generate appropriate treatment \nrecommendations. Physicians can refer to these treatment recommendations and use \nthem as a reference when issuing medical orders and treatment suggestions to patients \n(Figure 6). It is important to note that these treatment recommendations serve as a \nreference for physicians and do not replace their decision-making process. Physicians \nare still required to make a comprehensive assessment based on the specific \ncircumstances of each patient and to make final decisions based on their clinical \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nexperience and expertise. \n \n \nFigure 6. Integration with existing EMR (Regarding the HIS system screenshots, if \nyou require Figure 6, please contact the corresponding author.) \n \nDiscussion \nChatGLM, developed by Tsinghua University and Zhipu.AI, is an open-source \nLLM for natural language understanding and generation. It is a general pre-training \nframework that can be effectively parameterized for different tasks. In our study, we \nused the ChatGLM model and trained it on data from diabetic patients. \nIn terms of fine-tuning, traditional fine-tuning algorithms typically require \nmanual selection of parameters to be fine-tuned, which requires domain knowledge \nand expertise. In addition, LLM typically have a large number of parameters, which is \nan important consideration for GPU computing power and training dataset \nrequirements. Even if only a small fraction of the parameters are frozen, forward and \nbackward propagation computations are still required, and these computations scale \nlinearly with the number of model parameters. As a result, large-scale models require \nrelatively large GPU computing power and training data sets. To address these issues, \nour research has used the P-tuning and LoRA methods for fine-tuning training. \nP-tuning is a prompt-based fine-tuning method that guides the model to produce \nmore accurate outputs by inserting specific markers in the input. Compared to \ntraditional fine-tuning methods, P-tuning makes better use of the knowledge of the \npre-trained model and achieves better results on small datasets [21]. Moreover, \nP-tuning allows control over the style and content of the generated text by adjusting \nthe prompt [22]. \nLoRA is a parameter-based fine-tuning method that improves performance by \nfine-tuning the parameters of the model [23]. Compared to traditional fine-tuning \nmethods, LoRA makes better use of the knowledge of the pre-trained model and \nprovides better interpretability [24]. In addition, LoRA can further improve the \nefficiency of a model by reducing the number of parameters through low-rank \ndecomposition [25]. It is important to note that low-rank decomposition may \nintroduce some loss of information as it approximates the original model parameters \n[13]. Therefore, when applying LoRA or other parameter reduction techniques, it is \nessential to strike a balance between model efficiency and performance to ensure \nadequate information retention and model effectiveness [26]. \nAfter repeated debugging and testing of the model, we have found that for a \nsubset of patients with complex medical records, the model output can be potentially \nharmful and fail to assist healthcare professionals in their treatment. This situation can \noccur for the following reasons: 1) Data imbalance or sample bias: The model may \nhave been trained on an over-representation of certain types of patient records, leading \nto an inadequate understanding of other types of patients. This bias can lead to \ninaccurate or harmful treatment recommendations for certain patients. 2) Unknown or \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nrare scenarios: If the model encounters unfamiliar or infrequent situations during \ntraining, it may struggle to make accurate predictions or appropriate recommendations. \nComplex patient records often contain such unknown scenarios, rendering the model's \noutput ineffective. 3) Limitations of the model: The model may have inherent design \nor training limitations that prevent it from adequately accounting for factors specific \nto complex medical records. As a result, the model's outputs may lack accuracy or \nreliability in these cases. 4) The size of the training data has a direct impact on the \nperformance of large models. Therefore, it is crucial for us to explore methods to \nincrease the scale of our dataset. [27] (supplementary material). \nIt is important to understand that while LLMs often perform well in many \nscenarios, they may have limitations when dealing with complex medical records. \nTherefore, model outputs should not be the sole basis for decision making. Healthcare \nprofessionals should rely on their expertise and clinical judgement, and integrate \nmodel outputs with comprehensive assessments to make informed decisions [27]. \nThis research also has some limitations. Firstly, this work is at an early stage and \ncontains some errors, making it unsuitable for any commercial or clinical use. The \naforementioned experimental environment is limited to the experimental testing phase. \nSecondly, ChatGLM-6B is a GLM with only 6 billion parameters, which is \nsignificantly smaller than larger models such as ChatGPT and GPT-4, which have \nhundreds of billions of parameters. However, due to its smaller size, it does not \nrequire powerful computing resources and can be deployed on an intranet at a lower \ncost. Thirdly, the training data for this study comes from a single medical institution \nand is relatively small in volume. The results of the study require further validation. In \nfuture research, we will consider using larger models and incorporating more training \ndata to improve the robustness of the results. \nConclusion \nWhile ChatGLM has shown promise in assisting with the treatment of diabetic \npatients, thorough validation, scrutiny and regulatory measures must be undertaken to \nensure safety and efficacy before the model can be applied in clinical practice or \ncommercial settings. Future research directions include developing more universally \napplicable medical assistance models, incorporating Reinforcement Learning from \nHuman Feedback (RLHF) techniques for model fine-tuning, and ensuring the model's \napplicability in different hospitals and real-world scenarios. \n \nReference \n1. Artasensi A, Pedretti A, Vistoli G, Fumagalli L. Type 2 Diabetes Mellitus: A \nReview of Multi-Target Drugs. Molecules. 2020 Apr 23;25(8):1987. doi: \n10.3390/molecules25081987. PMID: 32340373; PMCID: PMC7221535.. \n2. WHO|Diabetes Programme. WHO. [(accessed on 27 January 2019)];2019 \nAvailable online: https://www.who.int/diabetes/en/. \n3. Landgraf R, Aberle J, Birkenfeld AL, Gallwitz B, Kellerer M, Klein H, \nMüller-Wieland D, Nauck MA, Reuter HM, Siegel E. Therapy of Type 2 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nDiabetes. Exp Clin Endocrinol Diabetes. 2019 Dec;127(S 01):S73-S92. doi: \n10.1055/a-1018-9106. Epub 2019 Dec 20. PMID: 31860927. \n4. Eghbali-Zarch M, Tavakkoli-Moghaddam R, Esfahanian F, Azaron A, Sepehri \nMM. A Markov decision process for modeling adverse drug reactions in \nmedication treatment of type 2 diabetes. Proc Inst Mech Eng H. 2019 \nAug;233(8):793-811. doi: 10.1177/0954411919853394. Epub 2019 Jun 10. PMID: \n31177917. \n5. Eloundou T, Manning S, Mishkin P , et al. Gpts are gpts: An early look at the \nlabor market impact potential of large language models[J]. arXiv preprint \narXiv:2303.10130, 2023. \n6. Nori H, King N, McKinney S M, et al. Capabilities of gpt-4 on medical challenge \nproblems[J]. arXiv preprint arXiv:2303.13375, 2023. \n7. Liu Z, Y u X, Zhang L, et al. Deid-gpt: Zero-shot medical text de-identification by \ngpt-4[J]. arXiv preprint arXiv:2303.11032, 2023. \n8. Liu S, Wright AP, Patterson BL, et al. Using AI-generated suggestions from \nChatGPT to optimize clinical decision support. J Am Med Inform Assoc. 2023 \nApr 22:ocad072. doi: 10.1093/jamia/ocad072. \n9. Gilson A, Safranek CW, Huang T, et al. How Does ChatGPT Perform on the \nUnited States Medical Licensing Examination? The Implications of Large \nLanguage Models for Medical Education and Knowledge Assessment. JMIR Med \nEduc. 2023 Feb 8;9:e45312. doi: 10.2196/45312. \n10. Wang M, Li S, Zheng T, et al. Big data health care platform with multisource \nheterogeneous data integration and massive high-dimensional data governance \nfor large hospitals: Design, development, and application[J]. JMIR Medical \nInformatics, 2022, 10(4): e36481. \n11. Du Z, Qian Y , Liu X, et al. GLM: General language model pretraining with \nautoregressive blank infilling[C]//Proceedings of the 60th Annual Meeting of the \nAssociation for Computational Linguistics (V olume 1: Long Papers). 2022: \n320-335. \n12. Liu X, Ji K, Fu Y , et al. P-tuning v2: Prompt tuning can be comparable to \nfine-tuning universally across scales and tasks[J]. arXiv preprint \narXiv:2110.07602, 2021. \n13. Hu E J, Shen Y , Wallis P, et al. Lora: Low-rank adaptation of large language \nmodels[J]. arXiv preprint arXiv:2106.09685, 2021. \n14. Lukichev D, Kryanina D, Bystrova A, et al. Parameter-Efficient Tuning of \nTransformer Models for Anglicism Detection and Substitution in \nRussian[C]//Proceedings of the International Conference “Dialogue. 2023, 2023. \n15. Liu X, Ji K, Fu Y , et al. P-tuning v2: Prompt tuning can be comparable to \nfine-tuning universally across scales and tasks[J]. arXiv preprint \narXiv:2110.07602, 2021. \n16. Papineni K, Roukos S, Ward T, et al. Bleu: a method for automatic evaluation of \nmachine translation[C]//Proceedings of the 40th annual meeting of the \nAssociation for Computational Linguistics. 2002: 311-318. \n17. Lin C Y . Rouge: A package for automatic evaluation of summaries[C]//Text \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint \nsummarization branches out. 2004: 74-81. \n18. Eggmann F, Weiger R, Zitzmann N U, et al. Implications of large language \nmodels such as ChatGPT for dental medicine[J]. Journal of Esthetic and \nRestorative Dentistry, 2023. \n19. Kang D, Li X, Stoica I, et al. Exploiting programmatic behavior of llms: \nDual-use through standard security attacks[J]. arXiv preprint arXiv:2302.05733, \n2023. \n20. Dash D, Thapa R, Banda J M, et al. Evaluation of GPT-3.5 and GPT-4 for \nsupporting real-world information needs in healthcare delivery[J]. arXiv preprint \narXiv:2304.13714, 2023. \n21. Liu X, Zheng Y , Du Z, et al. GPT understands, too[J]. arXiv preprint \narXiv:2103.10385, 2021. \n22. Dong Z, Wei P, Lin L. Dreamartist: Towards controllable one-shot text-to-image \ngeneration via contrastive prompt-tuning[J]. arXiv preprint arXiv:2211.11337, \n2022. \n23. Hu Z, Lan Y , Wang L, et al. LLM-Adapters: An Adapter Family for \nParameter-Efficient Fine-Tuning of Large Language Models[J]. arXiv preprint \narXiv:2304.01933, 2023. \n24. Chavan A, Liu Z, Gupta D, et al. One-for-All: Generalized LoRA for \nParameter-Efficient Fine-tuning[J]. arXiv preprint arXiv:2306.07967, 2023. \n25. Fried D, Polajnar T, Clark S. Low-rank tensors for verbs in compositional \ndistributional semantics[C]//Proceedings of the 53rd Annual Meeting of the \nAssociation for Computational Linguistics and the 7th International Joint \nConference on Natural Language Processing (V olume 2: Short Papers). 2015: \n731-736. \n26. Wang G, Cao Q, Yang J, et al. Task-oriented Memory-efficient \nPruning-Adapter[J]. arXiv preprint arXiv:2303.14704, 2023. \n27. Antaki F, Touma S, Milad D, et al. Evaluating the performance of chatgpt in \nophthalmology: An analysis of its successes and shortcomings[J]. Ophthalmology \nScience, 2023: 100324. \n28. Liu J, Wang C, Liu S. Utility of ChatGPT in Clinical Practice. J Med Internet Res \n2023;25:e48568.doi: 10.2196/48568 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 1, 2023. ; https://doi.org/10.1101/2023.06.30.23292034doi: medRxiv preprint ",
  "topic": "Test (biology)",
  "concepts": [
    {
      "name": "Test (biology)",
      "score": 0.5956515073776245
    },
    {
      "name": "Personalized medicine",
      "score": 0.5558569431304932
    },
    {
      "name": "Limiting",
      "score": 0.5343436598777771
    },
    {
      "name": "Process (computing)",
      "score": 0.5294684171676636
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.513717532157898
    },
    {
      "name": "Computer science",
      "score": 0.5103115439414978
    },
    {
      "name": "Medical record",
      "score": 0.509543240070343
    },
    {
      "name": "Adaptability",
      "score": 0.4162454903125763
    },
    {
      "name": "Medical physics",
      "score": 0.3632490634918213
    },
    {
      "name": "Medicine",
      "score": 0.3620823621749878
    },
    {
      "name": "Engineering",
      "score": 0.12530389428138733
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24185976",
      "name": "Sichuan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210089761",
      "name": "West China Hospital of Sichuan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210124723",
      "name": "Maternal and Child Health Hospital of Sichuan Province",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I901861585",
      "name": "Vanderbilt University Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I14308217",
      "name": "Ministry of Education Science and Technology",
      "country": "MW"
    },
    {
      "id": "https://openalex.org/I2800091995",
      "name": "West China Medical Center of Sichuan University",
      "country": "CN"
    }
  ],
  "cited_by": 12
}