{
  "title": "Evaluating large-language-model chatbots to engage communities in large-scale design projects",
  "url": "https://openalex.org/W4392913402",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3033732104",
      "name": "Jonathan Dortheimer",
      "affiliations": [
        "Ariel University"
      ]
    },
    {
      "id": "https://openalex.org/A2804747206",
      "name": "Nik Martelaro",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2068594279",
      "name": "Aaron Sprecher",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2113404867",
      "name": "Gerhard Schubert",
      "affiliations": [
        "Technical University of Munich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2170634217",
    "https://openalex.org/W2076170954",
    "https://openalex.org/W2060736319",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W3038234846",
    "https://openalex.org/W4288080268",
    "https://openalex.org/W2139087511",
    "https://openalex.org/W2281963941",
    "https://openalex.org/W2131477909",
    "https://openalex.org/W2008251417",
    "https://openalex.org/W2994508595",
    "https://openalex.org/W3044483028",
    "https://openalex.org/W4211100811",
    "https://openalex.org/W2164267986",
    "https://openalex.org/W2160338921",
    "https://openalex.org/W1274426204",
    "https://openalex.org/W4383679928",
    "https://openalex.org/W3033703320",
    "https://openalex.org/W2097811905",
    "https://openalex.org/W1989879666",
    "https://openalex.org/W2769178511",
    "https://openalex.org/W4285712492",
    "https://openalex.org/W2753349643",
    "https://openalex.org/W4315853397",
    "https://openalex.org/W4200404694",
    "https://openalex.org/W3187781029",
    "https://openalex.org/W2980650369",
    "https://openalex.org/W2161466446",
    "https://openalex.org/W2079819632",
    "https://openalex.org/W2294756111",
    "https://openalex.org/W2001829221",
    "https://openalex.org/W2922256442",
    "https://openalex.org/W2929038690",
    "https://openalex.org/W1600487731",
    "https://openalex.org/W6778849206",
    "https://openalex.org/W2005470179",
    "https://openalex.org/W2899044477",
    "https://openalex.org/W2753903507",
    "https://openalex.org/W4389523509",
    "https://openalex.org/W2037990632",
    "https://openalex.org/W2761001406",
    "https://openalex.org/W1424980240",
    "https://openalex.org/W2942196293",
    "https://openalex.org/W2008027087",
    "https://openalex.org/W2056713300",
    "https://openalex.org/W2966755844",
    "https://openalex.org/W2783539613",
    "https://openalex.org/W4307472389",
    "https://openalex.org/W4206379883",
    "https://openalex.org/W3031532441",
    "https://openalex.org/W1996076792",
    "https://openalex.org/W1968983388",
    "https://openalex.org/W4206480907"
  ],
  "abstract": "Abstract Recent advances in machine learning have enabled computers to converse with humans meaningfully. In this study, we propose using this technology to facilitate design conversations in large-scale urban development projects by creating chatbot systems that can automate and streamline information exchange between stakeholders and designers. To this end, we developed and evaluated a proof-of-concept chatbot system that can perform design conversations on a specific construction project and convert those conversations into a list of requirements. Next, in an experiment with 56 participants, we compared the chatbot system to a regular online survey, focusing on user satisfaction and the quality and quantity of collected information. The results revealed that, with regard to user satisfaction, the participants preferred the chatbot experience to a regular survey. However, we found that chatbot conversations produced more data than the survey, with a similar rate of novel ideas but fewer themes. Our findings provide robust evidence that chatbots can be effectively used for design discussions in large-scale design projects and offer a user-friendly experience that can help to engage people in the design process. Based on this evidence, by providing a space for meaningful conversations between stakeholders and expanding the reach of design projects, the use of chatbot systems in interactive design systems can potentially improve design processes and their outcomes.",
  "full_text": "Evaluating large-language-model chatbots to\nengage communities in large-scale design\nprojects\nJonathan Dortheimer1 , Nik Martelaro2 , Aaron Sprecher3 and\nGerhard Schubert4\n1School of Architecture, Ariel University, Ariel, Israel; 2Human-Computer Interaction Institute, Carnegie Mellon\nUniversity, Pittsburgh, PA, USA;3Faculty of Architecture, Technion Israel Institute of Technology, Haifa, Israel and\n4TUM School of Engineering and Design, Technical University of Munich, Munich, Germany\nAbstract\nRecent advances in machine learning have enabled computers to converse with humans\nmeaningfully. In this study, we propose using this technology to facilitate design conversations\nin large-scale urban development projects by creating chatbot systems that can automate and\nstreamline information exchange between stakeholders and designers. To this end, we developed\nand evaluated a proof-of-concept chatbot system that can perform design conversations on a\nspecific construction project and convert those conversations into a list of requirements. Next, in\nan experiment with 56 participants, we compared the chatbot system to a regular online survey,\nfocusing on user satisfaction and the quality and quantity of collected information. The results\nrevealed that, with regard to user satisfaction, the participants preferred the chatbot experience\nto a regular survey. However, we found that chatbot conversations produced more data than the\nsurvey, with a similar rate of novel ideas but fewer themes. Our findings provide robust evidence\nthat chatbots can be effectively used for design discussions in large-scale design projects and offer\na user-friendly experience that can help to engage people in the design process. Based on this\nevidence, by providing a space for meaningful conversations between stakeholders and expand-\ning the reach of design projects, the use of chatbot systems in interactive design systems can\npotentially improve design processes and their outcomes.\nIntroduction\nRecent advances in machine learning (ML) have enabled computers to converse with humans in\nmeaningful ways (Brown et al.,2020; OpenAI,2023). In this paper, we argue that this technology\ncan potentially revolutionize the design process, particularly in large-scale urban development\nprojects. Traditionally, in small architecture projects, there is a negotiation between a client (the\nend user) and the architect; during this negotiation, information is exchanged, thereby facilitating\nthe progression of reflection into an agreement among all stakeholders (McDonnell,2009; Oak,\n2009). However, in large-scale projects, the client is typically a governmental agency or developer\nwho is not the end user of the constructed buildings, making it extremely challenging to engage in\nmeaningful conversations with thousands of potential stakeholders who are the end users.\nSince the mid-20th century, such urban development projects have faced extensive criticism\ndue to their disconnect with end users, often resulting in underperforming designs (Alexander,\n1964), the destruction of thriving neighborhoods (Jacobs,1961), and a lack of inclusivity and\ndemocracy (Harvey,1973), which affects marginalized communities (Arnstein,1969). This issue\nhas become increasingly relevant as cities continue to densify, and urban renewal projects\nsignificantly impact various aspects of urban life, including social, economic, and environmental\nfactors. In response to this growing need, numerous participatory design methods have emerged\nsince the 1970s (Simonsen and Robertson,2012), aiming to incorporate diverse perspectives in\narchitectural projects (Luck,2018). By involving end users in the design process, these methods\nfoster a more comprehensive approach to urban development, better suited to address the\ncomplex challenges faced by contemporary cities.\nHowever, despite the widespread agreement on the significance of community participation\nin fostering sustainable development, promoting democratic culture, and creating equitable\ncommunities (Münster et al.,2017; Calderon,2020), the practical implementation of participa-\ntory design in urban design remains challenging. Various factors have hindered effective public\nparticipation, including intra-community politics and power dynamics (Krüger et al.,2019),\nbureaucratic obstacles and red tape (Brabham,2009), knowledge gaps between experts and\ncommunity members (Dortheimer and Margalit,2020), and a pervasive lack of public trust in\npoliticians and local authorities (Giering,2011). Additionally, the considerable time and effort\nArtificial Intelligence for\nEngineering Design, Analysis\nand Manufacturing\nwww.cambridge.org/aie\nResearch Article\nCite this article:Dortheimer J, Martelaro N,\nSprecher A and Schubert G (2024). Evaluating\nlarge-language-model chatbots to engage\ncommunities in large-scale design projects.\nArtificial Intelligence for Engineering Design,\nAnalysis and Manufacturing, 38, e4, 1–16\nhttps://doi.org/10.1017/S0890060424000027\nReceived: 01 June 2023\nRevised: 24 December 2023\nAccepted: 06 February 2024\nKeywords:\nchatbot; urban design; participatory design;\nlarge language model; crowdsourcing\nCorresponding author:\nJonathan Dortheimer;\nEmail: jonathand@ariel.ac.il\n© The Author(s), 2024. Published by Cambridge\nUniversity Press. This is an Open Access article,\ndistributed under the terms of the Creative\nCommons Attribution-ShareAlike licence\n(http://creativecommons.org/licenses/by-sa/\n4.0), which permits re-use, distribution, and\nreproduction in any medium, provided the\nsame Creative Commons licence is used to\ndistribute the re-used or adapted article and\nthe original article is properly cited.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nrequired from participants and organizers can impede the success-\nful execution of participatory design processes.\nIn response to these challenges, researchers and practitioners\nhave explored various strategies to enhance participatory design by\nleveraging digital tools to enable more accessible and inclusive\nparticipation (Luck,2018). The emergence of crowdsourcing tech-\nnologies has significantly strengthened the trend toward participa-\ntory design in recent years (Robertson and Simonsen,2012; Gooch\net al.,2018, Dortheimer et al.,2020). These technologies facilitate\nindividual communication, thereby relieving political pressure on\nparticipants and allowing them to express their opinions freely and,\nwhen necessary, anonymously (Dortheimer et al.,2023). Numerous\nstudies have examined the application of crowdsourcing in archi-\ntecture and urban design, encompassing ideation (Lu et al.,2018),\narchitectural design (Dortheimer,2022), co-creation (Mueller et al.,\n2018; Hofmann et al.,2020), mapping (Borges et al.,2015), and\nopinion-gathering (Hosio et al.,2015; Wang et al.,2021). However,\nsurvey-like methods remain the primary method for collecting\ninformation on an urban scale.\nUnlike surveys, conversations can be more open and flexible,\nfostering an environment that encourages stakeholders to reflect\nupon and share their ideas and experiences. This interactive\napproach to data collection allows for a deeper understanding of\nthe participants’ perspectives and emotions.\nTherefore, we argue that chatbots have the potential to revolu-\ntionize the field of urban design by facilitating meaningful conver-\nsations with a diverse array of stakeholders. Some studies have\nindicated that chatbots may be more effective for gathering\nrespondents’ information than traditional surveys (te Pas et al.,\n2020; Xiao et al.,2020a,b). Furthermore, the engaging nature of\nchatbot-facilitated conversations can help maintain participants’\ninterest and make the interaction more enjoyable. Finally, adopting\nchatbots in urban design can lead to more comprehensive and\naccurate insights, driving more effective decision-making and\nresulting in better-designed urban spaces that cater to the needs\nand preferences of all stakeholders.\nImplementing chatbots in participatory urban design projects\ncan lead to an overwhelming amount of conversational data, which\nposes a significant challenge for human designers. Consequently, it\nis essential to develop a comprehensive framework that not only\nstreamlines chatbots’ effective communication with stakeholders\nbut also enhances the efficient analysis and summarization of the\nvast data collected from these interactions. By doing so, this frame-\nwork would transform chatbots into practical and valuable tools in\nparticipatory urban design processes, equipping urban planners\nwith meaningful and actionable insights while effectively managing\ncommunication and data volume.\nFurthermore, there is a limited understanding of the differences\nin the quality and quantity of information gathered from such\nchatbot frameworks compared to traditional surveys in the context\nof urban design. This research gap warrants a comprehensive\nexamination to inform better the implementation of chatbot inter-\nventions in urban planning processes.\nConsequently, this research aims to investigate the use of chat-\nbots within the realm of participatory urban design and assess their\nefficacy compared to conventional survey methods. The research\nquestion addressed in this study is as follows: “What are\nthe differences between using a chatbot framework and surveys\nto collect information and design ideas in the context of urban\ndesign?”\nTo answer the research question, we develop and test a chatbot\nframework capable of performing design conversations and\nsummarizing such conversations into design requirements. Next,\nin an experiment with 56 participants, we compared the chatbot\nframework to a traditional online survey, focusing on the quality\nand quantity of collected information.\nThe contributions of this paper are as follows: 1) We present\na novel chatbot system and human –artificial intelligence\n(AI) prompt framework for initiating and managing design\nconversations in large-scale urban design projects, contributing\nto the emerging field of AI-assisted participatory design; 2) we\nperform a comprehensive experiment involving 56 participants,\ncomparing the chatbot framework with traditional online sur-\nveys in the context of participatory urban design that offers\ninsight into the differences between chatbot and survey outputs\nregarding the quality and quantity of information collected in\nthe context of urban design, addressing the current research gap;\nand 3) we propose recommendations for effectively using the\nchatbot framework in participatory urban design, based on the\nfindings and observations from the experiment, to facilitate\nmore productive and engaging interactions between stake-\nholders and designers.\nRelated works\nIn this section, we provide an overview of the related work in the\nareas of conversation in the design process, chatbots, design, and\nthe large language models (LLMs) to establish the context for our\nstudy.\nParticipatory design\nProponents of participatory design maintain that a more suit-\nable design fit can be achieved when end users actively partici-\npate in the design process (Reich et al., 1996). This approach\nbrings together individuals with diverse backgrounds, roles, and\nexpertise to collaboratively examine a problem and collectively\ngenerate potential solutions. The concept of incorporating local\nresidents into urban and architectural design planning has been\nin practice for over five decades (Luck,2018). Generally, urban\nplanning research posits that community involvement fosters\ndemocratic values and equitable communities, making it a vital\ncomponent of sustainable development (Münster et al., 2017;\nCalderon, 2020).\nThe role of conversation in the design process\nWhile design frequently involves working with visual representa-\ntions, verbal communication plays a crucial role in the design\nprocess. Verbal conversations with various stakeholders, such as\nclients, contractors, and community representatives, allow design-\ners to understand their needs (Lawson and Loke,1997; Dubberly\nand Pangaro, 2019) and constraints better and negotiate project\nrequirements (McDonnell,2009). Similarly, conversations among\ndesigners facilitate knowledge sharing and negotiation of new ideas,\nallowing design teams to transcend the individual abilities of a\nsingle designer (Arias et al.,2000). Conversations with end users\nand community members help explicate their needs and concerns\nabout a design project, making tacit knowledge explicit from the\nuser’s perspective (Luck,2003). Although verbal communication is\nessential, it is important to consider its challenges and limitations,\nsuch as potential misunderstandings and difficulties translating\nverbal ideas into concrete design elements (Karlgren and Ramberg,\n2 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\n2012). In early architectural discussions between designers and\nclients, clients often concentrate on familiar functional and struc-\ntural aspects of building designs, while designers seek to identify\nproblems and understand the design’s significance to the client\n(Luck and McDonnell,2006). Understanding how project stake-\nholders communicate verbally to express their needs is a valuable\ninsight for developing effective design chatbots.\nChatbots and the automation of conversation\nThe history of chatbots dates back to the first chatbot, ELIZA,\ndeveloped between 1964 and 1966, which was based on a lan-\nguage model that identified keywords and, following a set of\nrules, provided a response (Weizenbaum,1983). Over the years,\nchatbots have evolved significantly, owing to advancements in\nML and AI. There are now three primary types of chatbots: rule-\nbased, retrieval-based, and generative models (Hussain et al.,\n2019). Rule-based chatbots function based on a predefined set\nof rules, while retrieval-based chatbots choose responses from a\npre-built database. However, both are limited by linguistic know-\nledge hard-coded into their software (Shawar and Atwell,2005).\nTherefore, generative models powered by ML techniques have\nshown the most progress in recent years since these models can\nconstruct novel responses, adapting better to various conversa-\ntional situations.\nOverall, building a chatbot that can understand complex con-\nversations and answer appropriately was reported to be a challen-\nging task (Xiao et al.,2020a). However, modern chatbots such as\nApple’s Siri or Amazon’s Echo leverage ML techniques to create\nLLMs and web search results to produce meaningful responses.\nAt the time we completed the work described here, GPT-3 was\nthe largest publicly available LLM that produces human-like text\n(Brown et al.,2020). The model includes 175 billion parameters and\nproduces high-quality texts. The model generates texts based on a\nprovided text prompt. For instance, if a prompt is the beginning of a\nstory, the model would try to predict the continuation of that story.\nSeveral previous applications demonstrated that this model can be\nmeaningfully used for grammar correction, summarizing, answer-\ning questions, parsing unstructured data, classification, and trans-\nlation (Radford et al.,2019; Brown et al.,2020), among other tasks.\nLLMs have also been used to build chatbots to capture self-reported\nuser data chatting (Wei et al.,2023). However, since the model\ncannot reason, solve mathematical and ethical questions, or pass\nthe Turing test (Floridi and Chiriatti,2020), it is regarded as a\nhuman-like text generator rather than a general AI in the strict\nsense.\nTo build useful chatbots, an LLM must be provided with a well-\ndesigned prompt that can steer the LLM to generate topic-relevant\ntext. Designing these prompts, as described by Zamfirescu-Pereira\net al., is akin to“herding AI cats” due to the unpredictable nature of\nLLMs (Zamfirescu-Pereira et al.,2023). The challenges in crafting\ngood prompts are manifold. For instance, adding a new instruction\nto repair specific issues found using a previous prompt might\nunpredictably affect other instructions. In addition, the model\nmay generate “hallucinations,” which are instances of fabricated\ninformation. Furthermore, the GPT-3 model could not acknow-\nledge that it did not know some information. Similarly, a study on\nprompting generative image models for design highlighted the\nunpredictability and challenges of using these models (Dortheimer\net al., 2023b). The impact of prompts on model outputs and\nprompting techniques are active areas of natural language process-\ning research (White et al.,2023).\nChatbots for creativity and design\nSeveral previous studies have utilized human–chatbot interactions\nto generate creative ideas in various design fields (Kulcke,2018;\nCuadra et al.,2021; Shin et al.,2022). For instance, a notable study\ninvestigated how humans converse with a perceived AI during a\nWizard of Oz study where designers prototyped speech interaction\nwith music systems (Martelaro et al.,2020\n). Other studies have\nexplored human–chatbot interactions in spatial design (Kulcke,\n2018; Dortheimer et al., 2023) and ornament design (Cuadra\net al.,2021). Additionally, chatbots have been employed to mediate\nconsensus-building conversations (Shin et al.,2022).\nMoreover, numerous studies have focused on the development\n(Ahmed, 2019) and evaluation (Tavanapour and Bittner, 2018;\nHwang and Won,2021) of chatbots for ideation tasks. Chatbots\nhave also been utilized to facilitate design thinking through the\nempathy map method (Bittner and Shoury,2019). An intriguing\nexample is CharacterChat, a chatbot designed to assist writers in\ncreating fictional characters (Schmitt and Buschek,2021). How-\never, to the best of the authors’ knowledge, no studies have specif-\nically examined the use of chatbots in urban design tasks.\nComparisons of chatbots and surveys\nAnother challenge in human –chatbot communication is the\nhuman behavior and understanding that needs to be taken into\naccount (Nguyen et al.,2022). First, compared to human–human\ninteraction, human–chatbot communication was reported to take\nlonger and include shorter messages (Hill et al.,2015). In addition,\nthere is evidence that human–chatbot messages lack vocabulary\nrichness and can contain profane words (Hill et al.,2015). However,\ncompared to web surveys with open-ended questions, chatbots\nwere reported to produce longer and richer responses from humans\n(Xiao et al., 2020b). Interestingly, some evidence suggests that\nhumans can generate more or better-quality ideas when commu-\nnicating with a chatbot rather than with a human partner (Hwang\nand Won,2021).\nFurthermore, with regard to effectiveness in eliciting informa-\ntion from respondents, several previous reports noted that chatbot\ninterfaces could be more effective (Xiao et al.,2020a,b) and prefer-\nable (te Pas et al.,2020) than surveys. A reason that could underlie\nthis finding is that there is evidence suggesting that people are more\nwilling to share information through chatbots (Lee et al.,2020),\nwhich are generally believed to be useful for collaboration (Kim\net al.,2021). Together with LLMs such as GPT-3, recent research\nhas explored the usability of these models in operating chatbots\n(Wei et al.,2023). In line with this novel technology, in the present\nstudy, we empirically test a chatbot framework that may be more\neffective than survey methods for eliciting meaningful responses\nfrom participants in the context of urban design.\nChatbot design and development\nWe developed a chatbot system with dual functionalities to discuss\nan urban design project. The first function involves conversing with\nusers to gather their responses and insights about the project. The\nsecond function is to analyze and extract a set of design require-\nments from these conversations.\nThe chatbot design and development process can be broken\ndown into several stages. Initially, we experimented with a“mock”\nchatbot to explore design conversations. Next, we constructed a\nprototype utilizing an LLM for the chatbot system. Finally, we\nArtificial Intelligence for Engineering Design, Analysis, and Manufacturing 3\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nenhanced the chatbot’s performance through a series of experi-\nments and testing.\nExploring design conversations\nWe developed a chatbot prototype to investigate the ideal structure\nof human–chatbot design conversations. In order to achieve this,\nwe conducted a “Wizard of Oz experiment” wherein a human\nsubject engaged in a discussion about an architecture project of\ntheir choice with a human-operated chatbot.\nA chat system was created to enable the participants to converse\nwith a human architect, as depicted inFigure 1a. The chatbot was\noperated by a certified and experienced architect, who was\ninstructed to engage in a natural conversation with the participants.\nEach chatbot message was automatically converted into audio using\ntext-to-speech technology and played when the message was dis-\nplayed to the user, facilitating a more authentic interaction. The\nchat was initiated with a predefined user prompt introducing the\nchatbot: “I’m a design bot and would love to speak with you about a\ndesign project of your choice!” The conversations were recorded in\na database. The chat logs were then summarized to create a set of\ndesign requirements for the design project. The participant would\nthen rate these design requirements, as shown inFigure 1b. Parti-\ncipants were later interviewed to gather their experiences and\nsuggestions for improving the dialogue. Finally, the recorded con-\nversations were thoroughly analyzed.\nPreliminary experiment\nAfter the initial test with the Wizard of Oz controlled chatbot, we\nimplemented a preliminary chatbot using the GPT-3 LLM (text-\ndavinci-001) as the backend of the application instead of the human\noperator (see theappendix section “Final chatbot implementation”\nfor further detail). Since GPT-3 did not know when the conversa-\ntion was over and always produced new responses to user inputs, we\nadded a“finish chat” button to allow the participants to conclude\nthe discussion. Alternatively, the chat automatically ended after\nexchanging 50 messages. Then, the discussion log was automatic-\nally summarized into a list of design requirements that resulted\nfrom the discussion transcript. The participants then rated the\ncorrectness of each requirement on a five-point Likert-type scale.\nFinally, the process concluded when the participants provided\nrequirement evaluations.\nIn order to test the new chatbot’s performance, we conducted\na preliminary experiment where participants ( n = 51) were\nasked to discuss an architecture project of their choice with\nthe bot, rate the automatically produced requirement list, and\nanswer a user experience survey. The participants were students\nwho used the chatbot during a design class at a university. Based\non the results of this test, the chatbot was improved and fine-\ntuned to be used later in the controlled experiment. In addition,\nthe chatbot LLM was updated to the newer“text-davinci-002 ”\nfor the conversations and “text-curie-001 ” for design require-\nment summarizing.\nHuman–AI prompt framework\nThe present study generated chatbot responses using the\nimproved “text-davinci-002 ” GPT-3 LLM. The request to the\nLLM included multiple parameters, including a “text prompt”\nthat the model would use as input to predict how the text would\ncontinue. The text prompt is the foundation for the chatbot’s\nability to comprehend and respond to user inputs in a mean-\ningful and contextually relevant manner. It is the most critical\nelement, as the quality and relevance of the chatbot’s responses\nheavily depend on the information and context provided by the\nprompt.\nHowever, a part of the prompt influences how the human\nengages with the chatbot since it sets the stage for their expectations\nfrom the conversation. Designing the optimal prompt is\nFigure 1.Chatbot user interface.\n4 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nchallenging, given the nuances of human language and the need to\naccount for numerous conversational scenarios. Consequently,\nextensive testing and fine-tuning are necessary to identify the\nappropriate balance of context, specificity, and flexibility to achieve\nthe desired performance (Zamfirescu-Pereira et al., 2023).\nOur chatbot prompt contained the context of the conversation,\nincluding a specification of whether it was a conversation between\nan architect and a client, some character descriptions, and conver-\nsion goals. To design a chatbot operated solely by an LLM, we used\nthe terms internal prompt and shared prompt to describe the\nstructure of the “prompt” as the input of the LLM. The key\ndifference between the two prompts was that the shared prompt\nalso acted as a user prompt to provide the human with a shared\nunderstanding of the design situation and expectations from the\nconversation (see Fig. 2). Our chatbot implementation can be\nviewed in theappendix section “Final chatbot implementation.”\nThe internal prompt includes the definitions of the human and\nbot personas and the technical context of the conversation provided\nsolely to the LLM. In our case, we defined the chatbot first as an\narchitect conversing with a client. In the first experiments, we\nnoticed that the chatbot sometimes stopped discussing the design,\nasked to show site photos, presented non-existing sketches, or\nnegotiated for payment and budget.\nIn order to address the observed shortcomings, we modified the\nprompt by specifying that it involves a“conversation between an\narchitect and her client.” We also added to the prompt that the\nconversation focuses on aspects such as aesthetics, functional elem-\nents, and social preferences of the design.\nAdditionally, we noted that providing“The architect is very kind\nand professional” in the prompt led the chatbot to agree with most\nof the participants’suggestions without further discussion. In order\nto generate more engaging and reflective conversations, we\nreplaced the prompt as mentioned above with“The architect is\nchallenging the client with questions to gain a deeper mutual\nunderstanding of the requirements.” This adjustment helped make\nthe chatbot more critical without dismissing the participants’ideas.\nThe shared prompt was also improved between the experiments\nand included the names of the chatbot and the human, relevant\nproject information, and the appropriate context of the conversa-\ntion. We started with a generic prompt,“Hi, I am your architect. Let\nus discuss your architecture project. I would like to know what kind\nof project you had in mind?” This allowed the participants to\ndiscuss any architecture project, which resulted in different kinds\nof architecture project conversations with various qualities.\nWe added personal attributions to the shared prompt by asking\nthe participants to provide their names before the chat started.\nWe chose the chatbot to be female and named it Zaha, in reference\nto the late influential architect Zaha Hadid. To reduce the risk of\nconversations about business issues, we stated that the chatbot is\npart of a design team. This resulted in the following shared prompt\n“Hello username, my name is Zaha, and I am on the design team of\nthe project.”\nFigure 2.Conceptual diagram of the LLM prompt and user prompt, made out of internal and shared prompts.\nArtificial Intelligence for Engineering Design, Analysis, and Manufacturing 5\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nNext, we wanted the chatbot to discuss a single project with\nseveral participants to investigate how to aggregate several conver-\nsations. To this end, we outlined a specific construction project the\nparticipants would be familiar with–a new architecture school\nbuilding on campus. The project was presented with the following\nshared prompt:“The (Technical University of Munich) is planning\nto build a new architecture school instead of the outdated (electrical\nfacility on Theresienstrasse). The building will host the architecture\nschool, and the design will be based on the preferences of the\nstudents and faculty. That’s why we want to ask you about your\nideas for the new building.”\nFinally, we defined the design conversations’ scope and goals\nwith the criteria we thought were important to address. These\nincluded the following: functionality, aesthetics, cultural values,\nand the desired social effect. This was done with the following\nshared prompt: “Let’s discuss the project requirements. What\nspaces should be in the building? How can the building have a\npositive impact on the community and the environment? What\nshould the building look like? What values should the building\nexpress?” We found that the detailed shared and internal prompts\nproduced better conversations that were more focused and pro-\nduced higher-quality chatbot texts that helped the chatbot keep the\ndiscussion on topic and ask relevant questions.\nIn summary, the internal and shared prompt structure was a\npractical, functional approach to designing communication about\nurban design between LLM-based chatbots and humans.\nMethod\nUpon chatbot improvement, we conducted a controlled experiment\nto answer our research question (“What are the differences between\nusing a chatbot framework and using surveys to collect information\nand design ideas in the context of urban design?”). The experiment\nwas designed to evaluate the chatbot’s performance as a design\nrequirement collection tool and compare its performance to a web-\nbased survey.\nAs mentioned, we outlined a hypothetical urban architecture\nproject as a test case, which involved constructing a new architec-\nture school in place of a historic building (in Munich). The project’s\ncontext, situated within an existing neighborhood and incorporat-\ning public spaces, added complexity and relevance to the experi-\nment. We chose this scenario to present a realistic and multifaceted\ndesign challenge, requiring participants to consider various factors\nand constraints in a familiar setting. The nature of this design\nproject was both demanding and engaging for our participants,\nenabling us to assess the chatbot’s performance more effectively.\nThe new school building had to offer the most suitable envir-\nonment for studying and working, enrich the university campus\nwith the best quality and sustainable architecture in mind, serve\naround 1,400 architecture students, and provide them with various\nspaces such as studios, lecture halls, and so forth. These constraints\nwere based on the current architecture school needs. By providing a\nrealistic design problem, we aimed to create an environment where\nparticipants would be more likely to engage with the experiment.\nThe study participants were divided into the experimental and\ncontrol groups (see the“Participants” section for further detail).\nThe experimental group had conversations with the chatbot. The\ncontrol group completed a web-based survey derived from a real-\nistic urban public participation project survey. The adapted survey\nwas built using Google Forms and included the following questions:\n What are your hopes for this project?\n What are your concerns about this project?\n How can the building be more sustainable and have a positive\neffect on the environment?\n Which values should the new building design express?\n How can the building have a positive impact on the community\nand society?\nThe same questions were also provided in the chatbot’s internal\nprompt so that it could discuss these questions with the study\nparticipants.\nParticipants\nWe recruited students and faculty from the architecture depart-\nment. All participants were stakeholders of our hypothetical pro-\nject, possessed learning or teaching experiences, and had a good\nknowledge of the existing facility. The experiment group consisted\nof 35 participants; the control group had 21 participants. No\ncompensation was offered.\nIn the experimental group, there were 12 participants aged 18–\n24 and 13 participants aged 25\n–34. Unfortunately, the remaining\nparticipants did not complete the user experience survey, so we lack\ninformation about their age, gender, and education. Concerning\ngender, 10 participants were male, 13 were female, and two indi-\ncated “other” as gender. As concerns educational attainment, eight\nof the study participants in the experimental group had a master’s\ndegree, while 14 had a bachelor’s degree, and three were bachelor’s\nstudents at the time when the study was conducted.\nIn the control group, which consisted of 21 participants, all\nexcept one were students, with the remaining participant being a\nfaculty member. Regarding age, eight respondents were 18–24,\nwhile 13 were 25–35 years old. Furthermore, most of the partici-\npants (N = 12) were male, while eight were female, and one\nindicated “other” as gender. Finally, regarding educational attain-\nment, six respondents were bachelor’s students, 12 had a bachelor’s\ndegree, and three held a master’s degree.\nAnalyzing and evaluating chatbot responses\nWe evaluated the chatbot performance by manually analyzing the\nconversation transcripts in terms of logic, behavior, and accuracy of\nthe produced requirement list, inspired by Han et al. (2021). This\nanalysis encompassed several stages: response quality, user experi-\nence, and ethical considerations. The quality of responses was\nassessed through a detailed text analysis, which will be elaborated\nupon later. User experience was measured using survey data, while\nethical considerations were evaluated by identifying any potentially\nproblematic chatbot messages containing hate speech or privacy-\nintrusive questions.\nFirst, we checked whether the chatbot messages were coherent\nand relevant in the context of the conversation. A chatbot that does\nnot produce logically sound responses is more likely to be perceived\nas unreliable by users. To this end, we differentiated between the\nfollowing three possible levels:“the message makes perfect sense”\n(level 2),“the message makes sense but is slightly wrong” (level 1),\nand “the message is a mistake” (level 0).\nTo illustrate, consider the following example of a level 0 response\nin a discussion where the participant specifically asked to design a\nfire station with a parking space for three fire trucks:\nH: “Okay, but I asked for space for just 3 trucks”\nC: “But you said 6 trucks”\nThe next example shows a level 1 response when the chatbot\nrecommended cladding the walls with basalt while the participant\nwanted a white bedroom:\n6 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nH: “but basalt is black and I like white, maybe is not such a\ngood idea?”\nC: “I recommend you to paint the basalt white.”\nFinally, consider an example of a level 2 response, which is a\nresponse that makes perfect sense:\nHuman: “What about taking marble or other material that\noriginally is white?”\nChatbot: “Marble is a great idea.”\nNext, in our analysis of conversation dynamics, we sought to\nidentify when the chatbot discussed a specific topic and when it\nchanged it. In doing so, we aimed to identify when these events\nhappened and whether they were in the proper context of the\nconversation. Accordingly, we manually tagged the chatbot\nresponses with labels “changing subject,”“ reflecting,” and “sug-\ngesting ideas” when responses made sense.\nTo illustrate, a reflective conversation was assumed to occur\nwhen the chatbot asked a relevant question in response to a user’s\nidea, which could cause the user to reflect on the idea and develop\nit. For example:\nH: “Hey, I would like a building with community space”\nC: “Excellent, we were thinking the same thing. What\nkind of community space?”\nWhen the chatbot continued asking repetitive questions or\nfixated on a topic instead of changing the conversation subject,\nwe marked such responses as“repetition.” For example:\nC: “What about the other rooms?”\nH: “There are no other rooms”\nC: “How would you like to use the rooms?”\nH: “There are no other rooms”\nFinally, we tallied and assessed the design requirements generated\nby the chatbot and those from the web-based survey. To evaluate the\naccuracy of the design briefs summarizing the design requirements\ncreated by the chatbot, we first matched each conversation with its\ncorresponding design brief. We then labeled each accurately repre-\nsented design brief item as“correct.” Conversely, items that failed to\nexpress a design requirement were labeled as“incorrect”\nor “invalid”\nif they were technically flawed. Lastly, we pinpointed any absent\ndesign brief items and marked them as“missing.”\nComparison between chatbot and survey\nThe comparison between two different methods–chatbots and\nweb-based surveys–was challenging since these methods produce\ndifferent kinds of information. The following three metrics were\nused to analyze the performance of both methods.\nThe first metric was the number of words in an interaction,\nwhich was taken to attest to the quantity of data produced. The\nsecond metric was the number of themes the participant men-\ntioned, which was done by manually analyzing the text and iden-\ntifying the themes. The third metric was novel ideas. Innovative and\nnovel ideas hold a significant value that can elevate the design\nprocess, allowing designers to explore new perspectives and\napproaches.\nNovel ideas were identified by comparing the participant’s\nsuggestions to a list of existing concepts. These ideas were men-\ntioned in the provided project introduction document to the par-\nticipants or popular themes, for example, producing a suitable\nenvironment for studying, quality of architecture, sustainable con-\nstruction, availability of studios, lecture halls, facilitating idea\nexchange or creativity, shared spaces, and accessibility issues. To\nthese themes, we added a list of themes that were most frequently\nmentioned in similar projects or academic discourse but not in the\nproject introduction document. Such themes included having a\npublic space, facilitating connections to the community, generic\nsustainability ideas (e.g., recycling, solar power, water preserving,\nwood construction, green facades or roofs), simplicity, efficiency,\ncafe, coworking spaces, colors, and ordinary materials.\nThe statistical analysis of the data was conducted using the jStat\nstatistical library, implementing Welch’s t-test.\nOnce the experimental and control groups completed the\nexperiment, they filled out a user experience survey that contained\n10 questions about their recent experience. The questions included\nin the study were taken from Ashfaq et al.’s( 2020) subset relevant to\nthe chatbot experiment and web surveys (see Table A1 in the\nappendix section“Survey questions”). The items in the survey were\nrated on a five-point Likert scale. For further analysis, an average\nscore was computed for each survey item. Finally, the participants\nwere asked to share their qualitative opinions about using the\nchatbot or survey.\nResults\nIn the experiment, 751 messages were collected in 35 conversations.\nWe provide example chatbot and survey output in theappendix\nsections “Chatbot conversation example” and “Survey response\nexample.” The participants produced 377 messages, and the chat-\nbot produced 374, with an average of 21.45 messages per conver-\nsation. The average interaction duration was 9.75 minutes\n(min = 2.25, max = 24.74, SD 6.1). The chatbot generated 145 design\nrequirements from 26 conversations since some participants did\nnot click on the“finish chat” button or did not reach 50 messages.\nTwenty-five participants filled out the user experience survey. We\ndecided to keep the conversation transcripts of the participants who\ndid not complete the user experience survey since they are valuable\nto the analysis and might contain failed conversations. However,\nthe analysis did not reveal chatbot conversation failures.\nConversation quality\nThe summary of our conversation analysis results, including the\nchatbot and human participants’evaluation of the preliminary and\ncontrolled experiment, is provided inTable 1. Figure 3shows the\nchatbot’s human word count per message distribution compared to\nthe survey method. The chatbot produced responses of an average\nlength of 24.56 words (SD 22.11), with the longest response being\n134 words long. By contrast, the participants’ responses to the\nchatbot produced significantly fewer words in each message, with\nan average length of 10.51 words (SD 15.42). This result demon-\nstrates a clear LLM performance difference from our preliminary\nexperiments, where the chatbot produced shorter responses\n(M = 10.83, SD 8.41), which caused the participants to respond\nwith shorter messages (M = 4.62, SD 4.68).\nWith regard to the qualitative aspects of the analyzed conver-\nsations, of a total of 374 messages, 297 were marked as most\ncomprehensive (level 2, 86.84%), 34 were marked as lightly flawed\n(level 1, 9.94%), and only 11 did not make sense (level 0, 3.22%) (see\nTable 2).\nThis improvement between the preliminary and the controlled\nexperiments can be attributed to the enhancement in the LLM and\nprompt, which highlights the crucial role of coherent and logical\nresponses in fostering effective interaction. However, there is still a\nneed for improvement in the LLM’s performance.\nArtificial Intelligence for Engineering Design, Analysis, and Manufacturing 7\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nIn their responses to the chatbot, the participants produced an\naverage of 117.91 words in each conversation (SD 75.09), with a\nmaximum of 350 words. The control group respondents who filled\nout the survey produced considerably fewer words per response–\nan average of 59.04 words for each survey record, with a maximum\nof 175 words (SD 35.78). The results show that the chatbot elicited\ntwice as much data from the participants as the survey.\nBehavior analysis\nThe subsequent behavior analysis indicated that the chatbot changed\nthe conversation topic by asking a different question 41.44% of the\ntime (seeTable 3). However, in 5.26% of the messages, the chatbot\ndemonstrated a reflecting behavior and asked the participants about\ntheir ideas. In 5.14% of the messages, the chatbot repeated the\nprevious message, albeit with some (minor) changes. Finally, the\nchatbot did not produce any messages that could be considered hate\nspeech, nor did it commit privacy intrusion.\nThemes and novel ideas\nThe study found no significant difference in the topics generated\nthrough the chatbot or survey,t(38) = 0.49,p = .062, although the\nsurvey (M = 6.90, SD = 1.86) produced more topics on average than\nthe chatbot (M = 4.82, SD = 2.49). Similarly, there was no significant\ndifference in generating novel ideas between the two methods\n(t(38) = 0.36,p = .071). The difference was not statistically signifi-\ncant despite surveys (M = 0.43, SD = 0.62) yielding a little more\nnovel ideas on average than chatbots (M = 0.36, SD = 0.70). These\nfindings suggest that the choice of method, whether chatbot or\nsurvey, does not significantly impact the number of topics or novel\nideas generated.\nSeveral survey response examples are provided below:\n “Modern, sustainable architecture”\n “Opening up to the public and creating spaces for idea exchange\nand collaboration”\n “Space for experimentation, generous connection to public space,\nspace for experimentation”\nBelow is an example of a chatbot conversation:\nH: “well, I think that the spaces should be focused on\ninteraction and collaboration… so the biggest amount of exchange\ncan take place”\nC: “Excellent, so you want the spaces in the building to\nfacilitate interaction and collaboration. Do you have any ideas about\nhow we can achieve that?”\nH: “well it could be achieved through different methods.\nOne, for example, could be the relationship between public and\nprivate spaces - leaning more to the public side. Another method\ncould be through the typology of the floorplan and the construction\nmethods implemented - meaning to have a high degree of\n“permeability” in the architectural concept”\nC: “That makes sense. Are there any other values that\nyou feel are important for the building to express?”\nThe findings can be attributed to the fact that participants can\ncontemplate and offer well-considered responses in the form of a\nlist during a survey. Furthermore, the interactive nature of a chatbot\nmay result in more impulsive responses, potentially generating\nfewer ideas. Additionally, as demonstrated in the above example,\nchatbot conversations tend to be lengthier and demand more effort.\nNumber of words per message\nNumber of interactions\n0\n20\n40\n60\n0 2 4 6 8 1 01 21 41 61 82 02 22 42 62 83 03 23 43 63 84 04 24 44 64 85 05 25 45 65 86 0\nChatbot (count) Survey (count)\nFigure 3.Distribution of the number of human-provided words per message, comparing chatbot and survey responses with a bucket size of two words. Both mediums are similarly\ndistributed, peaking at 4–12 words per message. Notably, the chatbot generated a significantly higher number compared to the surveys.\nTable 1. Summary of human-provided information in chatbot and survey\nItem Chatbot mean (SD) Web survey mean (SD)\nWord count 117.91 (75.09) 59.04 (35.78)\nWords in message 10.51 (15.42) 11.87 (9.72)\nMean messages\nper user\n11.00 (7.69) 5 (0 –The number of survey\nquestions was fixed)\nTopics and ideas\nper interaction\n4.82 (2.49) 6.90 (1.86)\nNovel idea rate 31% (0.60) 36% (0.58)\n8 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nNevertheless, chatbot discussions yielded many ideas, exhibiting a\ncomparable novelty rate, which is promising as chatbot technology\nimproves.\nDesign brief analysis\nThe chatbot generated 26 valid design brief lists with 140 list items.\nThe design briefs contained an average of 5.38 list items per\ndiscussion (SD = 2.49, min = 3, max = 15). No relationship was\nobserved between the length of conversations and the number of\ndesign brief items. Accordingly, a more extended discussion did not\ntranslate into more ideas and topics for the subsequent generation\nof design requirements.\nNext, to measure the system’s performance, we manually ana-\nlyzed conversation transcripts and corresponding design briefs (see\nTable 4). As a result, in addition to 140 requirements captured by\nthe chatbot, we identified 30 further requirements and preferences\nthat were overlooked by the chatbot. Furthermore, out of 140 list\nitems, 130 (92.85%) were correct, 10 were incorrect (7.15%), and\n43 were invalid (12.2%). Most invalid items were texts that were\ngenerated by the chatbot or fragmented sentences.\nUser experience\nAs mentioned earlier, the survey was conducted in both the experi-\nmental and control groups to evaluate service quality, enjoyment,\nusefulness, and ease of use of the chatbot or the survey.\nThe results of the comparative analysis between the chatbot and\nsurvey methods demonstrated that participants rated the user\nexperience of the chatbot more positively than the survey (see\nFig. 4). Participants reported a higher level of enjoyment when\ninteracting with the chatbot than when completing the survey. They\nalso perceived the chatbot’s service quality to be superior. Further-\nmore, the chatbot was deemed considerably easier to use, and\nsatisfaction ratings were higher for the chatbot experience. Inter-\nestingly, the perceived usefulness and continuance intention were\nsimilar for both methods.\nIn conclusion, these findings suggest that the chatbot provides a\nmore engaging and satisfying user experience than traditional\nsurveys, and may encourage increased digital public participation.\nUser feedback\nAccording to the results of the user feedback survey, most partici-\npants found the chatbot system to be a valuable tool for data\ncollection in the early stages of the project. The participants appre-\nciated the chatbot’s conversational nature, which made it easier for\nthem to understand and express their opinions.\nHowever, two participants mentioned that they preferred\nmerely writing down their ideas instead of having a longer conver-\nsation. Furthermore, some other participants noted that the chatbot\nlacked the conversational qualities of real-life interaction. They felt\nthat the chatbot was too quick to agree or thank them for their\ncontribution without providing a meaningful response.\nOne participant admitted having a negative bias toward chatbot\nsystems, mentioning that a real person could still do the job better.\nHowever, they acknowledged that chatbot systems could be helpful\nin certain situations, such as when architects do not have time to\ndiscuss their ideas with stakeholders.\nDiscussion\nThe chatbot tool demonstrated its ability to handle extensive con-\nversations, allowing for significant and focused human-like discus-\nsions while gathering new types of information. In the study,\nparticipants’ input during their interactions with the chatbot was\nautomatically transformed into a valuable list of requirements. This\ninnovative method can enhance participation in large-scale urban\ndesign projects.\nHowever, the findings of this study indicate that although\nchatbot technology can generate meaningful conversations, there\nare still numerous challenges to address in order to guarantee its\nsuccessful implementation in such urban design projects.\nHuman–chatbot prompt design\nIn chatbot prompt design, we face a unique challenge that stems not\nonly from the unpredictable nature of LLMs but also from the\nvariability of human behavior. Previous chatbot research has com-\npared the unpredictability of LLMs to“herding AI cats” due to the\ntendency of LLMs to generate unexpected responses (Zamfirescu-\nPereira et al.,2023). This metaphor is particularly pertinent when\ndesigning an assistant chatbot. However, our chatbot’s primary aim\nis significantly different – to extract specific information from\nhuman users.\nTo achieve this goal, designers must also consider the significant\nvariability in human behavior. For instance, our data show a wide\nrange of responses to the same prompt that resulted in different\nkinds of conversations and a varying number of design require-\nments, demonstrating high variability in human behavior. This\nTable 2. Comparative analysis of chatbot response quality evaluation between\npreliminary experiments and a controlled experiment. It shows that the\nenhanced GPT-3 model, coupled with refined text prompts, improved the\nquality of the generated text\nQuality level Preliminary experiment Controlled experiment\nLevel 2 83.20% 86.84%\nLevel 1 11.60% 9.94%\nLevel 0 5.19% 3.22%\nTable 3. Summary of chatbot behavioral evaluation\nBehavior Preliminary experiment Controlled experiment\nChanging subject 54.70% 41.44%\nReflecting 0% 5.26%\nSuggesting ideas 1.33% 4.39%\nRepetition 3.87% 5.14%\nTable 4. Summary of design brief analysis success\nCriteria\nPreliminary\nexperiment\nControlled\nexperiment\nCorrectness of the produced\nrequirement items\n75.0% 92.85%\nRate of missing requirements 15.95% 17.6%\nArtificial Intelligence for Engineering Design, Analysis, and Manufacturing 9\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nchallenge is further compounded by the chat conversation format’s\ninherently unstructured and open-ended nature.\nTherefore, it is not just about“herding AI cats” but also about\nmanaging “human cats.” This metaphor underscores the need for\ndesigners to account for human behavior’s unpredictability and\nLLMs’ inherent unpredictability. In essence, designers of partici-\npatory chatbot systems, which involve users in the design process,\nmust be prepared to navigate the dual unpredictability of human\ninteraction and LLMs.\nTo address this complexity, we propose a prompt design frame-\nwork for information collection comprising two key components:\nan internal and shared prompt. The internal prompt encapsulates\nproject-specific information and a personality modifier to influence\nthe chatbot’s responses. The shared prompt, on the other hand,\nplays a crucial role in shaping the human–AI interaction by pro-\nviding a clear prompt for the human user. Both prompts must be\nfine-tuned through iterative testing to produce the expected inter-\naction between humans and AI. While testing LLM prompts can be\ndone automatically, human subject experiments are much more\ncomplex but can be automated using crowdsourcing platforms.\nIn conclusion, our proposed framework provides an essential\nfoundation for designing effective human and chatbot prompts,\naddressing the challenges posed by the unpredictable nature of\nboth. Our chatbot design framework distinguishes itself from\nChatbot\nSurvey\n0% 25% 50% 75%\nService Quality: The following software is an effective is\ninterface communicating my \n(a) Service Quality\nChatbot\nSurvey\n0% 25% 50% 75%\nPerceived enjoyment: I enjoyed a using the software\n(b) Perceived enjoyment 1\nChatbot\nSurvey\n0% 25% 50% 75%\nPerceived enjoyment: I was excited using the following software\n(c) Perceived enjoyment 2\nChatbot\nSurvey\n0% 25% 50% 75%\nPerceived usefulness: I find the software useful\n(d) Perceived usefulness 1\nChatbot\nSurvey\n0% 25% 50% 75%\nPerceived usefulness: Using the software helps me to\ncommunicate my ideas more \n(e) Perceived usefulness 2\nChatbot\nSurvey\n0% 25% 50% 75%\nPerceived usefulness: Using the software increases my productivity\n(f) Perceived usefulness 3\nChatbot\nSurvey\n0% 25% 50% 75%\nPerceived ease of use: The interaction with the software is\nclear and understandable\n(g) Perceived ease of use\nChatbot\nSurvey\n0% 25% 50% 75%\nSatisfaction: How do you feel about your overall experience of\nusing the software?\n(h) Satisfaction\nChatbot\nSurvey\n0% 25% 50% 75%\nContinuance intention: I intend to continue using this software\nin the future\n(i) Continuance intention 1\nChatbot\nSurvey\n0% 25% 50% 75%\nContinuance intention: I will strongly recommend others to\nuse the software\n(j) Continuance intention 2\nFigure 4. Chatbot and survey user experience evaluation result comparison in terms of service quality, perceived enjoyment, perceived usefulness, perceived ease of use,\nsatisfaction, and continuance intention.\n10 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nprevious studies by extending the prompt design to the human,\nwhich is essential for fostering clear communication and effectively\nframing the conversation (Wei et al.,2023). However, the design of\nchatbot prompts is still very challenging. It necessitates improved\ntools and methodologies, as they demand extensive trials and\ntesting. Therefore, future research should focus on developing more\nsophisticated tools and methodologies for prompt chatbot design,\nconsidering the unpredictability of human input and AI responses.\nEnhancing stakeholder engagement through enjoyable\nparticipation methods\nIn accordance with prior research, our study demonstrated that the\nparticipants perceived the chatbot interaction as more enjoyable\nthan completing a traditional survey (Kim et al.,2019; Xiao et al.,\n2020b). Previous research emphasizes the significance of stake-\nholder involvement in design projects to achieve successful out-\ncomes (Arnstein, 1969; Münster et al., 2017; Calderon, 2020).\nHowever, urban design and planning initiatives often struggle with\nlow participation rates due to their professional and political nature\n(Brabham, 2009; Giering,2011; Krüger et al.,2019; Dortheimer and\nMargalit, 2020). This creates difficulties in meaningfully engaging\nunderrepresented communities in such projects.\nTo address this challenge, it is essential to comprehend the\nmotivations behind participation. Researchers agree that enjoy-\nment is a crucial factor in driving participation (Lindenberg,\n2001; Malone et al.,2010). A robust connection has been discovered\nbetween enjoyment and increased engagement in a crowdsourcing\nactivity. Individuals who partake in enjoyable crowdsourcing activ-\nities are more likely to invest time and effort, resulting in more\ncontributions and greater satisfaction with the final product (Frey\net al.,2011; Liang et al.,2018).\nOur study results indicate that chatbots are perceived as more\nenjoyable than traditional surveys, which can enhance participation\nand foster greater engagement with diverse communities. Practi-\ntioners should consider this finding when planning large-scale\nparticipatory urban projects, as incorporating chatbots may lead\nto improved involvement and more successful outcomes.\nHowever, it is crucial to consider the potential novelty effect of the\nchatbot. Participants may have enjoyed their first interaction with the\nLLM-based chatbot due to its novelty, which could have influenced\ntheir favorable ratings. Over time, as the novelty wears off, users may\nexperience “chatbot fatigue,” similar to “survey fatigue.” This can\nimpact the long-term effectiveness and user satisfaction of chatbots.\nMore research is needed to understand this potential effect.\nWe propose several strategies to maximize the potential of chat-\nbots in stakeholder involvement in spatial design to make them more\nenjoyable. Firstly, the chatbot persona should be designed to be\nengaging and enjoyable, incorporating elements of humor, empathy,\nand a conversational style in the internal prompt. Secondly, chat\ninterfaces should be made more accessible by integrating chatbots\ninto popular instant messaging platforms, reducing the learning curve\nfor users. Lastly, it is crucial to continually monitor and improve the\nchatbot based on user feedback and performance analysis.\nComparing information quality and quantity in chatbot\nconversations and surveys\nIn examining the quality and quantity of information generated, we\ndiscovered that chatbot conversations yielded more data than\nsurveys despite covering fewer topics. However, the rate of novel\nideas was similar. While the differences observed were not\nstatistically significant, preventing us from definitively stating one\nmethod as superior, our findings suggest that chatbots can generate\ndesign requirements and ideas similar to those obtained from\nsurveys. We recommend that future research delve deeper into this\ncomparison, utilizing larger datasets from chatbot conversations\nand surveys to substantiate these preliminary findings further.\nVarious factors may explain the differences between surveys and\nchatbot conversations concerning information quality and quan-\ntity. Firstly, our chatbot conversations were less structured than a\nsurvey, which may have led to answering fewer questions. Secondly,\nsurveys offer a clear outline of the required input through a series of\npages and questions, whereas chatbot conversations may lack\nclarity regarding completing information collection, allowing users\nto end the conversation when they feel it is over. Lastly, the casual,\nconversational nature of chatbot interactions may result in fewer\ntopics being discussed, as they demand more effort from users and\nneed to be longer to cover all questions.\nThis limitation of current chatbot technology should be con-\nsidered when using chatbots for ideation or design requirement\ncollection and should inform future research. Therefore, to ensure\nthat sufficient issues and novel ideas are generated, we suggest using\nchatbots with a substantial participation group. With small groups,\nthe current chatbot should not be seen as a replacement for surveys\nbut rather as an additional input method. Notwithstanding, accord-\ning to the results of the present study, chatbots can be a valuable tool\nfor engaging stakeholders as they are more enjoyable and, thus, are\nmore likely to be used by a broader pool of stakeholders.\nFuture research\nFuture research should focus on the experimental testing of chat-\nbots in real-world urban design settings to identify new challenges.\nIn addition, further research on improving the proposed chatbot in\nterms of conversation structure and summarization algorithms\nwould also be needed. In particular, future studies could explore\nhow chatbots can communicate design using visual communica-\ntion. Furthermore, considering our findings on both the strengths\nand limitations of using chatbots in design projects, it would be\nmeaningful to examine the combined use of chatbots and surveys to\nleverage the advantages of both approaches.\nLimitations\nThe present study has several limitations. First, the study partici-\npants were architecture students and faculty members proficient in\nverbally expressing design ideas, understanding what buildings\nrequire, and having outstanding novel ideas. This could have led\nto the creation of more topics and a higher topic and novelty rate\nthan lay people.\nThe second limitation is that the participants were aware that\nthe project used in the present study was not real, and thus, their\nideas would not be realized. This could have caused some partici-\npants to have a lighter conversation with the chatbot, knowing there\nwould be no ramifications. Furthermore, it may have led people to\nalso report less in their survey responses.\nThe third limitation concerns certain obscurities in utilizing the\nchatbot’s design requirement generation process. Upon the con-\nversation’s conclusion, participants were required to activate a\n“finish chat” button to compile and generate a list of design\nrequirements. However, some participants failed to execute this\nstep. Upon analysis of the conversations, no causative factors\nrelated to the chatbot’s performance could be identified, leading\nArtificial Intelligence for Engineering Design, Analysis, and Manufacturing 11\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nus to conclude that the issue lies within the user interface. To\naddress this limitation in future chatbot research, we propose\nautomatically generating design requirements once the conversa-\ntion ends, eliminating the need for user-initiated activation.\nConclusion\nChatbots have the potential to transform stakeholder engagement\nin urban design projects. By offering a more engaging and inter-\nactive experience than traditional surveys, they can potentially help\nurban designers connect with more extensive and diverse commu-\nnities. However, the findings of this study do not show a significant\ndifference between chatbots and surveys in generating topics and\nnew ideas. Despite this limitation, the study demonstrates that\nchatbots can successfully automate design conversations in archi-\ntecture and urban design. Participants found the chatbot experience\nenjoyable and stimulating, which could lead to increased public\ninvolvement in participatory design processes.\nThis research suggests a chatbot system and prompt framework\nthat can be utilized in large-scale participatory design projects,\nstreamlining data collection and analysis. The system enables auto-\nmated conversations while providing a summarization mechanism\nto help designers manage the vast amounts of data generated. In\nconclusion, our findings support the effective use of chatbots in\nfacilitating design conversations, highlighting necessary further\nresearch to enhance the data collection capabilities of chatbots,\nmaking them even more beneficial for design processes.\nData availability statement. The data that support the findings will be\navailable.\nFunding statement. This work was supported by the Technical University of\nMunich Global Incentive Fund. The funder had no role in study design, data\ncollection and analysis, publication decision, or manuscript preparation.\nCompeting interest. The authors declare no competing interests.\nReferences\nAbdul-Kader SA and Woods J(2015) Survey on chatbot design techniques in\nspeech conversation systems.International Journal of Advanced Computer\nScience and Applications6(7), 72–80.\nAhmed S (2019) An Architecture for Dynamic Conversational Agents for\nCitizen Participation and Ideation. Master’s thesis, Technische Universität\nMünchen.\nAlexander C(1964) Notes on the Synthesis of Form. Harvard University Press,\nCambridge.\nAlexander C, Ishikawa S and Silverstein M(1977) A Pattern Language: Towns,\nBuildings, Construction. New York: Oxford University Press.\nArias E, Eden H, Fischer G, Gorman A and Scharff E(2000) Transcending the\nindividual human mind–creating shared understanding through collabora-\ntive design. ACM Transactions on Computer-Human Interaction 7(1),\n84–113.\nArnstein SR(1969) A ladder of citizen participation.Journal of the American\nInstitute of Planners35(4), 216–224.\nAshfaq M, Yun J, Yu S and Loureiro SMC(2020) I, chatbot: modeling the\ndeterminants of users’satisfaction and continuance intention of AI-powered\nservice agents.Telematics and Informatics54(July), 101473.\nBittner E and Shoury O(2019) Designing automated facilitation for design\nthinking: a chatbot for supporting teams in the empathy map method.\nhttps://doi.org/10.24251/HICSS.2019.029\nBorges J, Jankowski P and Davis CA(2015) Crowdsourcing for geodesign:\nopportunities and challenges for stakeholder input in urban planning. In\nSluter CR, Cruz CBM and de Menezes PML (eds),Cartography – Maps\nConnecting the World. Lecture Notes in Geoinformation and Cartography.\nCham: Springer, 361–373.\nBrabham DC(2009) Crowdsourcing the public participation process for plan-\nning projects.Planning Theory8(3), 242–262.\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan\nA, Shyam P, Sastry G, Askell A, Agarwal S, Herbert-Voss A, Krueger G,\nHenighan T, Child R, Ramesh A, Ziegler DM, Wu J, Winter C, Hesse C,\nChen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, McCandl-\nish S, Radford A, Sutskever I and Amodei D(2020) Language models are\nfew-shot learners. Preprint,arXiv:2005.14165.\nCalderon C(2020) Unearthing the political: differences, conflicts and power in\nparticipatory urban design.Journal of Urban Design25(1), 50–64.\nCuadra A, Goedicke D and Zamfirescu-Pereira JD(2021) Democratizing\ndesign and fabrication using speech. In CUI 2021 – 3rd Conference on\nConversational User Interfaces . New York: Association for Computing\nMachinery, 1–8.\nDortheimer J(2022) Collective intelligence in design crowdsourcing.Mathem-\natics 10(4), 539.\nDortheimer J and Margalit T(2020) Open-source architecture and questions\nof intellectual property, tacit knowledge, and liability.The Journal of Archi-\ntecture 25(3), 276–294.\nDortheimer J, Neuman E and Milo T(2020, September) A novel crowdsour-\ncing-based approach for collaborative architectural design. In Anthropologic:\nArchitecture and Fabrication in the Cognitive Age— Proceedings of the 38th\neCAADe Conference (Vol. 2, pp. 155–\n164). Berlin, Germany: Education and\nResearch in Computer Aided Architectural Design in Europe.\nDortheimer J, Yang S, Yang Q and Sprecher A(2023) Conceptual architectural\ndesign at scale: a case study of community participation using crowdsourcing.\nBuildings 13(1), 222.\nDortheimer J, Schubert G, Dalach A, Brenner L and Martelaro N(2023b)\nThink AI-side the Box!. eCAADe 41 - Digital Design Reconsidered, Graz,\nAustria, 567–576\nDubberly H and Pangaro P(2019) Cybernetics and design: conversations for\naction. InDesign Cybernetics. Cham: Springer, 85–99.\nFloridi L and Chiriatti M(2020) GPT-3: its nature, scope, limits, and conse-\nquences. Minds and Machines30(4), 681–694.\nFrey K, Lüthje C and Haag S(2011) Whom should firms attract to open\ninnovation platforms? The role of knowledge diversity and motivation.Long\nRange Planning44(5–6), 397–420.\nGiering Scott(2011) Public participation strategies for transit. Vol. 89. Trans-\nportation Research Board.\nGooch D, Barker M, Hudson L, Kelly R, Kortuem G, Van Der Linden J, Petre\nM, Brown R, Klis-Davies A, Forbes H, Mackinnon J, Macpherson R and\nWalton C(2018) Amplifying quiet voices: challenges and opportunities for\nparticipatory design at an urban scale.ACM Transactions on Computer-\nHuman Interaction25(1), 1–34.\nHan X, Zhou M, Turner MJ and Yeh T(2021) Designing effective interview\nchatbots: automatic chatbot profiling and design suggestion generation for\nchatbot debugging. InProceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems (CHI’21). New York: Association for Com-\nputing Machinery, 1\n–15.\nHarvey D(1973) Social Justice and the City, revised edn. Athens, GA: University\nof Georgia Press.\nHill J, Ford WR and Farreras IG(2015) Real conversations with artificial\nintelligence: a comparison between human–human online conversations and\nhuman–chatbot conversations.Computers in Human Behavior49, 245–250.\nHofmann M, Münster S and Noennig JR(2020) A theoretical framework for\nthe evaluation of massive digital participation systems in urban planning.\nJournal of Geovisualization and Spatial Analysis4(1), 3.\nHosio S, Goncalves J, Kostakos V and Riekki J(2015) Crowdsourcing public\nopinion using urban pervasive technologies: lessons from real-life experi-\nments in Oulu.Policy & Internet7(2), 203–222.\nHussain S, Sianaki OA and Ababneh N(2019) A survey on conversational\nagents/chatbots classification and design techniques. In Barolli L,\nTakizawa M, Xhafa F and Enokido T (eds),Web, Artificial Intelligence and\nNetwork Applications. Cham: Springer, 946–956.\nHwang AH-C and Won AS(2021) IdeaBot: investigating social facilitation in\nhuman–machine team creativity. InProceedings of the 2021 CHI Conference\n12 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\non Human Factors in Computing Systems. New York: Association for Com-\nputing Machinery, 1–16.\nJacobs J(1961) Death and Life of Great American Cities. New York: Vintage\nBooks.\nKarlgren K and Ramberg R(2012) The use of design patterns in overcoming\nmisunderstandings in collaborative interaction design. CoDesign 8(4),\n231–246.\nKim S, Eun J, Seering J and Lee J(2021) Moderator chatbot for deliberative\ndiscussion: effects of discussion structure and discussant facilitation.Pro-\nceedings of the ACM on Human-Computer Interaction5,1 –26.\nKim S, Lee J and Gweon G(2019) Comparing data from chatbot and web\nsurveys: effects of platform and conversational style on survey response\nquality. In Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems (CHI ’19). New York: Association for Computing\nMachinery, 1–12.\nKrüger M, Duarte AB, Weibert A, Aal K, Talhouk R and Metatla O(2019)\nWhat is participation?: Emerging challenges for participatory design in\nglobalized conditions.Interactions 26(3), 50–54.\nKulcke M(2018) Design-bot–using half-automated qualitative interviews as\npart of self-communication within the design process. In Kepczynska-\nWalczak A and Bialkowski S (eds),Computing for a Better Tomorrow–\nProceedings of the 36th eCAADe Conference, Vol. 1, Lodz University of\nTechnology, Lodz, Poland, September 19–21, 2018, 103–108.\nLawson B and Loke SM(1997) Computers, words and pictures.Design Studies\n18(2), 171–183.\nLee Y-C, Yamashita N and Huang Y(2020) Designing a chatbot as a mediator\nfor promoting deep self-disclosure to a real mental health professional.\nProceedings of the ACM on Human-Computer Interaction4,1 –27.\nLiang H, Wang M-M, Wang J-J and Xue Y(2018) How intrinsic motivation\nand extrinsic incentives affect task effort in crowdsourcing contests: a medi-\nated moderation model.Computers in Human Behavior81, 168–176.\nLindenberg S (2001) Intrinsic motivation in a new light. Kyklos 54(2–3),\n317–342.\nLu H, Gu J, Li J, Lu Y, Müller J, Wei W and Schmitt G(2018) Evaluating urban\ndesign ideas from citizens from crowdsourcing and participatory design. In\nFukuda T, Huang W, Janssen P, Crolla K and Alhadidi S (eds),CAADRIA\n2018 – 23rd International Conference on Computer-Aided Architectural\nDesign Research in Asia: Learning, Prototyping and Adapting,V o l .2.H o n g\nKong: Association for Computer-Aided Architectural Design Research in Asia,\n297–306.\nLuck R(2003) Dialogue in participatory design.Design Studies24(6), 523–535.\nLuck R(2018) Participatory design in architectural practice: changing practices\nin future making in uncertain times.Design Studies59, 139–157.\nLuck R and McDonnell J(2006) Architect and user interaction: the spoken\nrepresentation of form and functional meaning in early design conversations.\nDesign Studies27(2), 141–166.\nMalone TW, Laubacher R and Dellarocas C(2010) The collective intelligence\ngenome. IEEE Engineering Management Review38(3), 38–52.\nMartelaro N, Mennicken S, Thom J, Cramer H and Ju W(2020) Using remote\ncontrolled speech agents to explore music experience in context. InProceed-\nings of the 2020 ACM Designing Interactive Systems Conference. Eindhoven:\nAssociation for Computing Machinery, 2065–2076.\nMcDonnell J (2009) Collaborative negotiation in design: a study of\ndesign conversations between architect and building users.CoDesign 5(1),\n35–50.\nMueller J, Lu H,\nChirkin A, Klein B and Schmitt G(2018) Citizen design\nscience: a strategy for crowd-creative urban design.Cities 72, 181–188.\nMünster S, Georgi C, Heijne K, Klamert K, Noennig JR, Pump M, Stelzle B\nand van der Meer H(2017) How to involve inhabitants in urban design\nplanning by using digital tools? An overview on a state of the art, key\nchallenges and promising approaches. Procedia Computer Science 112,\n2391–2405 (Knowledge-Based and Intelligent Information & Engineering\nSystems: Proceedings of the 21st International Conference, KES-20176-8\nSeptember 2017, Marseille, France).\nNguyen TH, Waizenegger L and Techatassanasoontorn AA(2022) “Don’t\nneglect the user!” –identifying types of human–chatbot interactions and their\nassociated characteristics.Information Systems Frontiers24(3), 797–838.\nOak A (2009) Performing architecture: talking“architect” and “client” into\nbeing. CoDesign 5(1), 51–63.\nOpenAI (2023) ChatGPT: optimizing language models for dialogue. Available\nat https://openai.com/blog/chatgpt/ (accessed 2 February 2023).\nRadford A, Wu J, Child R, Luan D, Amodei D and Sutskever I(2019)\nLanguage models are unsupervised multitask learners.OpenAI Blog1(8), 9.\nReich Y, Konda SL, Monarch IA, Levy SN and Subrahmanian E(1996)\nVarieties and issues of participation and design. Design Studies 17(2),\n165–180.\nRobertson T and Simonsen J(2012) Challenges and opportunities in contem-\nporary participatory design.Design Issues28(3), 3–9.\nSchmitt O and Buschek D(2021) CharacterChat: supporting the creation of\nfictional characters through conversation and progressive manifestation with\na chatbot. InCreativity and Cognition. New York: Association for Computing\nMachinery, 1–10.\nShawar BA and Atwell ES(2005) Using corpora in machine-learning chatbot\nsystems. International Journal of Corpus Linguistics10(4), 489–516.\nShin J, Hedderich MA, Lucero A and Oulasvirta A(2022) Chatbots facilitating\nconsensus-building in asynchronous co-design. InProceedings of the 35th\nAnnual ACM Symposium on User Interface Software and Technology .\nNew York: Association for Computing Machinery, 1–13.\nSimonsen J and Robertson T(2012) Routledge international handbook of\nparticipatory design. New York, NY, USA: Routledge.\nTavanapour N and Bittner EAC(2018) Automated facilitation for IDEA\nplatforms: design and evaluation of a chatbot prototype. In Jan Pries-Heje,\nSudha Ram and Michael Rosemann (eds),Proceedings of the International\nConference on Information Systems - Bridging the Internet of People, Data,\nand Things, ICIS 2018. San Francisco, CA, USA: Association for Information\nSystems, December 13–16, 2018, https://aisel.aisnet.org/icis2018/general/\nPresentations/8.\nte Pas ME, Werner GMMR, Bouwman RA and Buise MP (2020) User\nexperience of a chatbot questionnaire versus a regular computer question-\nnaire. Prospective Comparative Study8(12), e21982.\nWang Y, Gao S, Li N and Yu S(2021) Crowdsourcing the perceived urban built\nenvironment via social media: the case of underutilized land.Advanced\nEngineering Informatics50, 101371.\nWei J, Kim S, Jung H and Kim Y-H(2023) Leveraging large language models to\npower chatbots for collecting userself-reported data. Preprint,arXiv:2301.05843.\nWeizenbaum J(1983) ELIZA–a computer program for the study of natural\nlanguage communication between man and machine.Communications of the\nACM 26(1), 23–\n28.\nWhite J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, Elnashar A, Spencer-\nSmith J and Schmidt DC(2023) A prompt pattern catalog to enhance\nprompt engineering with ChatGPT. Preprint,arXiv:2302.11382.\nXiao Z, Zhou MX, Chen W, Yang H and Chi C(2020a) If I hear you correctly:\nbuilding and evaluating interview chatbots with active listening skills. InCHI\nConference on Human Factors in Computing Systems (CHI’20). New York:\nAssociation for Computing Machinery, 1–14.\nXiao Z, Zhou MX, Liao QV, Mark G, Chi C, Chen W and Yang H(2020b) Tell\nme about yourself: using an AI-powered chatbot to conduct conversational\nsurveys with open-ended questions. ACM Transactions on Computer-\nHuman Interaction27(3), 1–37.\nZamfirescu-Pereira JD, Wei H, Xiao A, Gu K, Jung G, Lee MG, Hartmann B\nand Yang Q(2023) Herding AI cats: lessons from designing a chatbot by\nprompting GPT-3. InProceedings of the 2023 ACM Designing Interactive\nSystems Conference (DIS ’23). New York: Association for Computing\nMachinery, 2206–2220.\nArtificial Intelligence for Engineering Design, Analysis, and Manufacturing 13\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nAppendix\nA Final chatbot implementation\nThe implementation of the chatbot system was executed using a suite of web\ntechnologies. The system’s backbone was a custom-built NodeJS web server\napplication designed to handle the processing and management of the chatbot’s\noperations. The system also incorporated a MySQL relational database, which was\nutilized to store and manage all conversation data to ensure that all interactions\nwere recorded and could be analyzed. The user interface was developed using a\ncustom HTML and JavaScript application coded using ReactJS.\nThe system used web services for AI functionality. Upon receiving a user’s\nmessage, the server compiled the message into a comprehensive prompt, which\nincluded the entire conversation. This prompt was sent to the GPT-3 LLM via\nthe Open AI web service. The response from the LLM was subsequently returned\nto the application, where it was parsed and cleaned before being sent back to the\nuser interface (seeFig. A1).\nA.1 Internal prompt\nThe internal prompt was provided only to the language model and included the\nconversation context that the human should not be aware of. The straightfor-\nward parts of the prompt were the context of the dialog (e.g., a conversation,\nchat, phone call, or theater play), description of the chatbot’s persona (e.g.,\narchitect, client, project manager, interior designer, or bot), the chatbot’s\ncharacter (e.g., helpful, critical, or rude), and the role of the user (e.g., client,\nstakeholder, or neighbor). These textual descriptions were essential to fine-\ntuning the chatbot behavior. The internal prompt we used for the controlled\nexperiment was as follows:\nThis is a chat between an architect and her client. The\narchitect asks many questions to get a better under-\nstanding of the design project. The architect is help-\nful and clever and goes into detail to help the client to\nbetter express his/her needs. The architect needs to\nknow which rooms the client needs, who is going to use\nthem, as well as the client’s preferred style.\nChat transcript:\nA.2 Shared prompt\nThe shared prompt included predefined messages that the chatbot sent to the\nuser to provide conversation context. These messages were not generated by the\nlanguage model and were considered to be part of the prompt since they\nadditionally provided the conversation context to the language model. The\nshared prompt described the context of our design project (i.e., initial require-\nments and location) and introduced the user and the chatbot (i.e., names,\norganizational belonging, etc.).\nThen, the discussion transcript was provided, including identification of the\nspeakers (e.g., architect, client). The prompt“Architect:” suggested that the\nlanguage model would complete how an architect would continue the conver-\nsation. The example below shows how the shared prompt was used.\nA\nRCHITECT: Hello {participant_name}, my name is Zaha and I\nam on the design team of the project.\nARCHITECT: The (Technical University of Munich) is plan-\nning to build a new architecture school instead of the\noutdated (electrical facility on Theresienstrasse). The\nbuilding will host the architecture school and the design\nwill be based on the preferences of the students and fac-\nulty. That’s why we want to ask you about your ideas for the\nnew building.\nA\nRCHITECT: Let’s brainstorm a little bit. What is your\nopinion on the project, what useful spaces should be in\nthe building? How can the building have a positive impact\non the community and the environment? What should the\nbuilding look like? What values should the building\nexpress?\nA.3 Generating design requirements from a conversation\nOnce the discussion concluded, we used the language model to extract the list of\ndesign requirements. We used the“davinci-instruct-beta” and “text-curie-001”\nmodels, which we found suitable for the task. An example of the prompt is\nshown below.\nHere is a transcript of a conversation between an archi-\ntect and her client:\n[… conversation transcript log…]\n—–\nMake a list summarizing all the detailed requirements\nfrom the discussion:\nFigure A1.Chatbot application structure diagram and information flow between the user, application, and web services.\n14 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nB Survey questions\nC Chatbot conversation example\nThe following is a typical conversation log provided as an example to illustrate\nthe collected data.\nC: “Hello (name removed for privacy), my name is Zaha and I am on\nthe design team of the project.”\nC: “The Technical University of Munich is planning to build a new\narchitecture school instead of the outdated electrical facility on Theresienstrasse.\nThe building will host the architecture school and the design will be based on the\npreferences of the students and faculty. That’s why we want to ask you about your\nideas for the new building.”\nC: “Let’s discuss the project requirements. What spaces should be in\nthe building? How can the building have a positive impact on the community and\nthe environment? What should the building look like? What values should the\nbuilding express?”\nH: “I think this is a great opportunity address some of the issues the\nmain campus has.”\nC: “Yes, the new school will be an important part of the University.”\nHuman: “As a student, I honestly do not feel welcomed in most of the architecture\nfaculty’s buildings. There are very limited spaces to just hang out in between the\nlectures. I think we need a lounge space with some little amenities like\nmicrowaves”\nC: “That sounds like a great idea. The school should be a place where\nstudents want to spend time.”\nH: “Do you know what they are planning to do with the building\nmaterials from the existing building?”\nC: “The plan is to recycle as much of the material from the existing\nbuilding as possible.”\nH: “I do believe the first course of action should not be to demolish the\nbuilding but refurbish it. If that is not possible we should reuse existing materials\non site as much as possible and design based on those resources.”\nC: “That is a very good point. We will definitely consider that during\nthe design process.”\nH: “Recycling is not nearly as effective as reuse, unfortunately”\nC: “Yes, that is true. But we have to work with what we have.”\nH: “Another thing that has always bothered me with the other archi-\ntecture buildings is that they do not feel and look like a design school”\nC: “That is a valid point. How do you think the new school should look?”\nH: “I would like for the new building to have loads of daylight”\nC: “That is a great idea. We will definitely try to incorporate that into\nthe design.”\nH: “And I think it could be a good idea to use parts of this building as a\nprototype for the various research that is done here at TUM”\nC: “That is an excellent suggestion. We will take that into consider-\nation during the design process.”\nH: “Why do you like that suggestion?”\nC: “I think it would be a great way to use the building as a tool for\nresearch and learning.”\nH: “I agree! Architecture students should be inspired by the environ-\nment they’re in and learn from it”\nC: “Absolutely. The new school should be a place where students can\nlearn and be inspired. Thank you for your input, (name removed for privacy).”\nH: “I also believe building as simply as possible, while still complying\nwith the high sustainability standards is a must. Florian Nagler has done a great\ndeal of research into that topic”\nC: “Thank you for your input, (name removed for privacy). We will\ndefinitely consider your suggestions as we move forward with the design of the new\nschool.”\nH: “I would love for it to be built out of timber, because of its ecological\nproperties”\nC: “Thank you for your input, (name removed for privacy). We will\ndefinitely consider your suggestions as we move forward with the design of the new\nschool.”\nH:\n“Do you not want any more input from me?”\nC: “No, I would love to hear more of your thoughts on the project.”\nH: “What do you think of the project?”\nC: “I think it is a great opportunity to address some of the issues on the\nmain campus.”\nH: “Also, I know the Garching campus has a very fast and cool slide and\nI would love it if we get an even faster slide here in the city centre as well”\nC: “That sounds like a great idea. We will definitely consider it during\nthe design process. Thank you for your input, (name removed for privacy).”\nH: “Do you know of the slide in Garching?”\nC: “Yes, I have heard of it.”\nH: “I think the building should reflect the values of the university, and\nnot just be flashy for the sake of it”\nC: “I agree. The new school should express the values of the Univer-\nsity. Thank you for your input, (name removed for privacy).”\nH: “What do you think are the values of the university?”\nC: “I think the University values excellence in research and teaching,\nand I think the new school should reflect that. Thank you for your input, (name\nremoved for privacy).”\nH: “Thank you for putting up with me!”\nC: “You’re welcome. Thank you for your input.”\nThis conversation was summarized to the following design requirements list:\n1. The new building should have lounge spaces with amenities like micro-\nwaves for students to hang out in between lectures.\n2. The existing building materials should be reused as much as possible,\nrather than being demolished and recycled.\n3. The new building should reflect the aesthetic of a design school and inspire\narchitecture students.\n4. The building should have plenty of daylight.\n5. Parts of the building should be used as a prototype for research at TUM.\n6. The building should comply with high sustainability standards and be built\nas simply as possible.\n7. The client suggested using timber for construction due to its ecological\nproperties.\n8. The client suggested incorporating a fast slide, similar to the one at the\nGarching campus.\n9. The building should reflect the values of the university, which include\nexcellence in research and teaching.\nTable A1. A comparison of user experience with the chatbot versus the survey\nConstruct Item\nService quality\n(SQ)\nThe following software has an effective interface for\ncommunicating my needs\nPerceived\nenjoyment (PE)\nI enjoyed using the software\nI was excited to use the following software\nPerceived\nusefulness (PU)\nI find the software useful\nUsing the software helps me to communicate my\nideas more quickly\nUsing the software increases my productivity\nPerceived ease of\nuse (PEU)\nThe interaction with the software is clear and\nunderstandable\nSatisfaction (SAT) How do you feel about your overall experience of\nusing the software?\nContinuance\nintention (CI)\nI intend to continue using this software in the future\nI will strongly recommend others to use the software\nArtificial Intelligence for Engineering Design, Analysis, and Manufacturing 15\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press\nD Survey response example\nThe following is a single survey response provided as an example to illustrate the\ncollected control data.\nWhich spaces should the new school include?\nAnswer: Faculty offices, Workshops, Quiet working spaces, Collaborative\nworking spaces, sleeping pods\nHow can the building be more sustainable and have a positive effect on the\nenvironment?\nAnswer: Solar power, water, and waste recycling promote and enact healthy\nlifestyle (end all-nighter culture producing piles of waste)\nWhich values should the new building design express?\nAnswer: Multifunctionality, accessibility, communication, open space, edu-\ncation as an open-ended process Open to the public, integration of social minor-\nities in the facility management, and possibly the educational process as well\n(homeless people probably understand a lot about minimalism)\nHow can the building have a positive impact on the community and the\nsociety?\nAnswer: The plot is too small. the architecture produced there might be just as\nconfined as the plot is. other than that, the building might end up looking just like\nthe buildings of other faculties where education is a one-way process. architecture\nis a social discipline more than an engineering one.\nWhat are your concerns about this project?“What are your hopes about this\nproject?\nAnswer: More openly talked about if it comes to actually designing the\nbuilding. spark a real discussion among the students and the alumni. Make it a\npublic topic.\n16 Jonathan Dortheimeret al.\nhttps://doi.org/10.1017/S0890060424000027 Published online by Cambridge University Press",
  "topic": "Chatbot",
  "concepts": [
    {
      "name": "Chatbot",
      "score": 0.9503116607666016
    },
    {
      "name": "Computer science",
      "score": 0.6256592273712158
    },
    {
      "name": "Converse",
      "score": 0.6254658102989197
    },
    {
      "name": "Scale (ratio)",
      "score": 0.45515745878219604
    },
    {
      "name": "Engineering design process",
      "score": 0.44605591893196106
    },
    {
      "name": "World Wide Web",
      "score": 0.4242035150527954
    },
    {
      "name": "Conversation",
      "score": 0.42160671949386597
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3833736777305603
    },
    {
      "name": "Knowledge management",
      "score": 0.3515947461128235
    },
    {
      "name": "Engineering",
      "score": 0.2078964114189148
    },
    {
      "name": "Psychology",
      "score": 0.12107226252555847
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Communication",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52170813",
      "name": "Ariel University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ],
  "cited_by": 12
}