{
  "title": "Quantity doesn’t buy quality syntax with neural language models",
  "url": "https://openalex.org/W2971016963",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A1976982077",
      "name": "Marten van Schijndel",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2108518500",
      "name": "Aaron MUELLER",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A817205692",
      "name": "Tal Linzen",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951483775",
    "https://openalex.org/W2625014264",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963614302",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W2793273050",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W2103018059",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W4288255289",
    "https://openalex.org/W2963940534",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2918996109",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W4247868144",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2887020936",
    "https://openalex.org/W2049049032",
    "https://openalex.org/W2964348070",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2886663262",
    "https://openalex.org/W2970853769",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4399650694",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1978662219",
    "https://openalex.org/W2949629417"
  ],
  "abstract": "Marten van Schijndel, Aaron Mueller, Tal Linzen. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 5831–5837,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n5831\nQuantity doesn’t buy quality syntax with neural language models\nMarten van Schijndel\nCornell University\nmv443@cornell.edu\nAaron Mueller\nJohns Hopkins University\namueller@jhu.edu\nTal Linzen\nJohns Hopkins University\ntal.linzen@jhu.edu\nAbstract\nRecurrent neural networks can learn to predict\nupcoming words remarkably well on average;\nin syntactically complex contexts, however,\nthey often assign unexpectedly high probabil-\nities to ungrammatical words. We investigate\nto what extent these shortcomings can be miti-\ngated by increasing the size of the network and\nthe corpus on which it is trained. We ﬁnd that\ngains from increasing network size are min-\nimal beyond a certain point. Likewise, ex-\npanding the training corpus yields diminishing\nreturns; we estimate that the training corpus\nwould need to be unrealistically large for the\nmodels to match human performance. A com-\nparison to GPT and BERT, Transformer-based\nmodels trained on billions of words, reveals\nthat these models perform even more poorly\nthan our LSTMs in some constructions. Our\nresults make the case for more data efﬁcient\narchitectures.\n1 Introduction\nRecurrent neural network language models (LMs)\ncan learn to predict upcoming words with remark-\nably low perplexity (Mikolov et al., 2010; Joze-\nfowicz et al., 2016; Radford et al., 2019). This\noverall success has motivated targeted paradigms\nthat measure whether the LM’s predictions reﬂect\na correct analysis of sentence structure. One such\nevaluation strategy compares the probability as-\nsigned by the LM to a minimal pair of sentences\ndiffering only in grammaticality (Linzen et al.,\n2016). In the following example, the LM is ex-\npected to assign a higher probability to the sen-\ntence when the verb agrees in number with the\nsubject (1a) than when it does not (1b):\n(1) a. The author laughs .\nb. *The author laugh.\nRNN LMs have been shown to favor the grammat-\nical variant in the vast majority of cases sampled\nat random from a corpus (Linzen et al., 2016), but\ntheir accuracy decreases in the presence of dis-\ntracting nouns intervening between the head of the\nsubject and the verb, especially when those nouns\nare in relative clauses (Marvin and Linzen, 2018).\nCan we hope to address these deﬁcits by train-\ning larger and larger networks on larger and larger\ncorpora, relying on the “unreasonable effective-\nness” of massive datasets (Halevy et al., 2009) and\ncomputational power?1 Or would architectural ad-\nvances be necessary to improve our LMs’ syntac-\ntic representations (Kuncoro et al., 2018)?\nThis paper takes a ﬁrst step towards address-\ning this question. We train 125 RNN LMs with\nlong short-term memory (LSTM, Hochreiter and\nSchmidhuber, 1997) units, systematically varying\nthe size of the training corpus and the dimension-\nality of the models’ hidden layer, and track the re-\nlationship between these parameters and the per-\nformance of the models on agreement dependen-\ncies in a range of syntactic constructions (Marvin\nand Linzen, 2018). We also compare our RNNs’\naccuracy to that of GPT (Radford et al., 2018) and\nBERT (Devlin et al., 2019), Transformer-based\nLMs trained on very large corpora.\nWe ﬁnd that model capacity does not consis-\ntently improve performance beyond a minimum\nthreshold. Increased corpus size likewise has a\nmoderate and inconsistent effect on accuracy. We\nestimate that even if training data yielded con-\nsistent improvements, an unreasonable amount of\ndata would be required to match human accuracy.\nWe conclude that reliable and data-efﬁcient learn-\ning of syntax is likely to require external supervi-\nsion signals or a stronger inductive bias than that\nprovided by RNNs and Transformers.\n1http://www.incompleteideas.net/\nIncIdeas/BitterLesson.html\n5832\n2 Language models\nArchitecture: All of the models we trained con-\nsisted of two LSTM layers. We trained models\nwith 100, 200, 400, 800 or 1600 units in each\nhidden layer. Input and output weights were tied\nduring training (Press and Wolf, 2017; Inan et al.,\n2017); consequently, the input embedding had the\nsame dimensionality as the hidden layers.\nTraining data: We trained networks of each\nsize on 2M, 10M, 20M, 40M and 80M words\n(2M = 2 million). We extracted ﬁve disjoint sec-\ntions of the WikiText-103 corpus (Merity et al.,\n2016) for each corpus size; 2 in total, we trained\n125 models (5 layer sizes ×5 corpus sizes ×5\ncorpus subsets).3 We used the WikiText-103 vali-\ndation set for validation.\nVocabulary: To ensure comparability across\ndifferent models trained on different data, we used\nthe same vocabulary for all the models we trained.\nThe vocabulary consisted of an intersection of the\n400k word GloVe vocabulary (Pennington et al.,\n2014) with the 50k words used by GRNN (see be-\nlow); the resulting vocabulary had 28,438 words.\nGRNN: We also report the syntactic perfor-\nmance of a publicly available LSTM LM (Gulor-\ndava et al., 2018, henceforth GRNN). This trained\nmodel has been the focus of a considerable amount\nof analysis work in the past year. The model has\ntwo layers of 650 units each, and was trained on\n80M words.\nComparison with Transformers: Finally, we\nreport results from two publicly available LMs\nbased on non-recurrent self-attention (Transform-\ners; Vaswani et al., 2017): GPT (Radford et al.,\n2018) and BERT (Devlin et al., 2019). Both of\nthese models have been argued to learn powerful\nsyntactic representations (Goldberg, 2019; Wolf,\n2019). We compare our results to those reported\nby Wolf (2019) on a similar challenge set for these\ntwo Transformer models.4\n2We made each of the 40M- and 80M-token training sets\nas disjoint as possible, but since Wikitext-103 only contains\n103M tokens, it was not possible to make them wholly dis-\njoint using Wikitext-103 as the mother corpus.\n3Each model was initialized randomly and was trained to\nconvergence with a dropout of 0.2 using a batch size of 20,\nbackpropagating error for 35 observations. An initial learning\nrate of 20 was gradually annealed.\n4The comparison is not exact because the Transformers\nwere evaluated based on the rank of the two target verbs given\nthe preﬁx, and the LSTMs based on the total log-probability\nGPT is a 12-layer Transformer with 110 million\nparameters (compared to GRNN’s 39 million pa-\nrameters); it was trained on 1 billion words. BERT\nhas a similar architecture to GPT,5 with three dif-\nferences: it is bidirectional, it was trained on 3.3\nbillion words, and it has a different training objec-\ntive than the typical LM: it attempts to predict a\nsingle masked word in a sentence given the words\nboth before and after the target word. For compa-\nrability to the LSTMs and GPT, we examine the\nagreement performance of BERT when only the\nwords before the target are given (in contrast to the\nbidirectional tests reported by Goldberg 2019).\n3 Evaluation\nWe tested each trained model on the constructions\nfrom the Marvin and Linzen (2018) challenge set,\nwhich is based on the agreement paradigm de-\nscribed in the introduction. 6 We replaced the\nverbs used by Marvin and Linzen with the high-\nfrequency verbs is/are, was/were and has/have.\nThis was done to ensure that even the models\ntrained on smaller corpora will have had exposure\nto both forms of the verb in question.\nWe performed statistical tests of our hypothe-\nses using Bayes factors, which quantify the sup-\nport the data provide for a more complex model\ncompared to a simpler one (Rouder et al., 2009).\nWe computed two-sample Bayes factors using\nttestBF from the BayesFactor R package\n(Morey and Rouder, 2018) using default settings.\nOur null hypothesis was that there is no differ-\nence in accuracy between the two sets of models in\nquestion (e.g., all models with 400 units per layer\ncompared to all models with 800 units per layer).\nThe magnitude of the resulting Bayes factorK can\nbe interpreted as follows (Jeffreys, 1961): K <1\nindicates that there is no difference in accuracy be-\ntween the two model groups, and K > 10 pro-\nvides strong evidence that the model groups obtain\ndifferent accuracies.\n4 Results\nIncreasing model size improved syntactic predic-\ntion accuracy up to 400 units per layer; further in-\nof the sentence (including the ﬁnal period); in addition, Wolf\n(2019) did not modify the dataset to use only high frequency\nverbs, as we describe in Section 3.\n5BERT Base showed more accurate syntactic predictions\nthan BERT Large (Goldberg, 2019), which has more param-\neters, so we only consider BERT Base.\n6https://github.com/BeckyMarvin/LM_\nsyneval\n5833\nCorpus size Layer size\n2M →10M 5508.8 100 →200 768.5\n10M →20M 0.1 200 →400 63.5\n20M →40M 12.9 400 →800 0.2\n40M →80M 0.2 800 →1600 0.1\nTable 1: Strength of evidence for improvements in\nagreement prediction accuracy as a result of increasing\ncorpus size averaging across layer size (left) or layer\nsize averaging across corpus size (right), as quantiﬁed\nby Bayes factors. Boldfaced Bayes factors indicate\nstrong evidence of improvement.\ncreases in model size had no effect (see Table 1\nfor the statistical tests). Increasing the amount of\ntraining data impacted accuracy in an inconsistent\nway. Training on 10M tokens produced general\nimprovements across all constructions compared\nto 2M tokens. Doubling the corpus to 20M did not\naffect accuracy, but doubling it again to 40M did.\nThere was no evidence of further improvement be-\ntween 40M and 80M words.\nIn the remainder of this section we analyze the\neffect of increasing model size and training cor-\npus size on the models’ predictions for each con-\nstruction in the data set.7 A subset of the results is\nshown in Fig. 1; for the full results, see Figs. 3, 4\nand 5 in the Appendix.\nLocal number agreement: All models trained\non 10M words or more obtained perfect or near-\nperfect accuracy (mean> 99%) in the cases where\nthe verb was adjacent to its subject: simple agree-\nment (the author has/*have books) and agreement\nwithin a sentential complement ( the mechanics\nsaid the author has/*have books). When trained\non 2M words, the models performed slightly\nworse, but their accuracy was still very high (mean\n95.6%), regardless of model size. Overall, we con-\nclude that the plurality of speciﬁc nouns and the\ngeneralization that a verb has to agree with a noun\ncan be learned very quickly.\nAttractors: Agreement across subject relative\nclauses (the author that likes the guards has/*have\nbooks) and across prepositional phrases ( the au-\nthor next to the guards has/*have books; Fig. 1a)\nbeneﬁted from increasing the hidden layer size to\n400, but showed little improvement when hidden\n7See Table 2 in the Appendix for a Bayes factor analysis\nof the improvement in each construction for each amount of\ntraining data.\nlayer size was increased further. Accuracy in these\nconstructions consistently improved as the amount\nof training data increased.\nObject relative clauses: Expanding the train-\ning corpus improved local agreement within ob-\nject relative clauses ( the movies that the guard\nhas/*have are good; Fig. 1b) for all model sizes,\nbut only improved agreementacross those clauses\n(the movie that the guards like has/*have drama;\nFig. 1c) in models with larger hidden layers.\nLarger hidden layers improved accuracy in object\nrelatives only when a relativizer was present, and\nonly up to about 80% accuracy. When a sentence\nlacked an overt relativizer ( the movies the secu-\nrity guard has/*have are good; Fig. 1d), all mod-\nels performed poorly, with accuracy levelling off\naround 70%.\nCoordination: Perhaps surprisingly, all LSTM\nLMs struggled with agreement in a coordinated\nverb phrase ( the authors laugh and have/*has\nbooks; Fig. 1e), even though this construction does\nnot include distracting nouns between the subject\nand the second verb. In larger models trained on\nmore data, accuracy was higher when the second\nverb was further from the subject ( the authors\nknow many different languages and have/*has\nbooks; Fig. 1f).\nTraining on more than 10M tokens did not im-\nprove accuracy in short VP coordination, even\nwhen the amount of data was multiplied by eight\n(10M →80M: K <1), unlike coordination across\nlong VPs, which beneﬁted from additional data\n(10M →80M: K > 90). These results further\nchallenge the assumption that increased amounts\nof training data will lead to adequately abstract\nsyntactic representations: RNNs show a limited\nability to generalize from instances of a construc-\ntion that have longer constituents to instances with\nshorter constituents.\nReﬂexive anaphora: A reﬂexive pronoun such\nas themselves must have an antecedent with the\nappropriate plurality in the same clause as the\npronoun ( The manager that the architects like\ndoubted himself/*themselves). Accuracy was not\nstrongly affected by the parameters we varied: re-\nﬂexive agreement accuracy across a relative clause\nwas consistently mediocre (61%–76%) regardless\nof model size or the amount of training data.\n5834\n100 200 400 800 1600\nHidden Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Agreement Accuracy\nChance\nGRNN\nGPT\nBERT\nHuman\nThe author next to the security guards\n has/*have books .\nCorpus Size\n2m\n10m\n20m\n40m\n80m\n(a) Prepositional Phrase\n100 200 400 800 1600\nHidden Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Agreement Accuracy\nChance\nGRNNGPT\nBERT\nHuman\nThe movie that the security guard has/*have\n is good .\nCorpus Size\n2m\n10m\n20m\n40m\n80m (b) Object Relative: Within\n100 200 400 800 1600\nHidden Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Agreement Accuracy\nChance\nGRNN\nGPT\nBERT\nHuman\nThe movie that the security guard likes\n has/*have drama. \nCorpus Size\n2m\n10m\n20m\n40m\n80m\n(c) Object Relative: Across\n100 200 400 800 1600\nHidden Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Agreement Accuracy\nChance\nGRNN\nGPT\nBERT\nHuman\nThe movies the security guard likes\n have/*has drama .\nCorpus Size\n2m\n10m\n20m\n40m\n80m (d) Object Relative: Across (no that)\n100 200 400 800 1600\nHidden Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Agreement Accuracy\nChance\nGRNN\nGPT\nBERT\nHuman\nThe authors laugh and have/*has books .\nCorpus Size\n2m\n10m\n20m\n40m\n80m\n(e) VP Coordination (Short)\n100 200 400 800 1600\nHidden Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Agreement Accuracy\nChance\nGRNN\nGPT\nBERT\nHuman\nThe author knows many different foreign languages\n and has/*have books .\nCorpus Size\n2m\n10m\n20m\n40m\n80m (f) VP Coordination (Long)\nFigure 1: LSTM agreement performance in several syntactic constructions. The solid horizontal line indicates\nchance performance. The dashed lines show the performance of GPT and BERT as reported by Wolf (2019), the\nperformance of humans as reported by Marvin and Linzen (2018), and the performance of GRNN. Error bars\nreﬂect standard deviation across the ﬁve models in each category.\nTransformers: Despite having more parameters\nand having been trained on signiﬁcantly larger cor-\npora, the two Transformer models performed ei-\nther as well as or more poorly than our LSTMs\nin seven of the ten subject-verb agreement condi-\ntions. BERT underperformed GPT in several con-\nditions despite having been trained on three times\nas many tokens as GPT.8\n8Goldberg (2019) reports much better results using a\nsetup in which BERT has access to both left and right con-\n5 How much data would be enough?\nHow much training data would be required for an\nLSTM LM to perform at a human level (as re-\nported by Marvin and Linzen 2018) in the condi-\ntext. We hypothesize that the task is made signiﬁcantly sim-\npler when the model knows where the target word is relative\nto the end of the sentence. For example, if the point of pre-\ndiction is at the last word of the sentence, it is also the last\npoint at which the verb agreeing with the main clause subject\ncould possibly occur; the model does not need to detect the\nend of the relative clause to perform the task in this case.\n5835\nGRNN GPT BERT100\n104\n108\n1012\n1016\n1020\n1024\n1028\n1032\nReflexives\nObj Rel Within (no that)\nObj Rels Across\nVP Coord (long)\nVP Coord (short)\n(a) Human-like\nGRNN GPT BERT100\n1012\n1024\n1036\n1048\n1060\n1072\n1084\n1096\nSubj Rel/Prep/Sent Comp\nObj Rels/VP Coord (long)\nObj Rel Within (no that)\nVP Coord (short)\nReflexives/\nObj Rel Across (no that) (b) 99.99%\nFigure 2: Lines depict number of training tokens\nneeded for LSTMs to achieve human-like (left) or\n99.99% accuracy (right) in each syntactic agreement\ncondition, according to our estimates. Bars depict the\namount of data on which each model was trained.\ntions in which our models do not already perform\nat a human level? As a conservative estimate, we\nmeasured the error reduction achieved by doubling\nthe data from 20M to 40M tokens (the largest error\nreduction we observed beyond 2M →10M).9 Un-\nder the assumption that each subsequent doubling\nof training data would produce the same percent\nerror reduction, we predicted the amount of data\nrequired to obtain human-like and 99.99% accu-\nracy (see Fig. 2). 10 We found that every remain-\ning construction would require over 10 billion to-\nkens to achieve human-like performance, and most\nwould require trillions of tokens to achieve perfect\naccuracy – an impractically large amount of train-\ning data, especially for these relatively simple syn-\ntactic phenomena.\n9See Tables 3 and 4 in the Appendix for data requirements\nestimated from other error reduction rates.\n10Human performance on this task is well known to be far\nfrom perfect, with error rates approaching 25% in some con-\ntexts (Bock and Miller, 1991). While modeling human errors\nis of considerable interest to cognitive scientists (Linzen and\nLeonard, 2018), we believe that in most applied contexts it is\ndesirable for the model to make no errors at all.\n6 Discussion\nWe have investigated the effect of network size\nand training corpus size on the quality of the\nsyntactic representations of LSTM LMs, as mea-\nsured by agreement prediction accuracy. Increased\nmodel size had limited beneﬁts; models with 400\nhidden units performed signiﬁcantly better than\nsmaller models, but further increases in network\nsize had no effect. The limited effect of net-\nwork size is consistent with previous ﬁndings on\nsequence labeling tasks (Reimers and Gurevych,\n2017; Greff et al., 2017). We have also shown that\nincreasing the amount of training data is unlikely\nto result in human-like accuracy in all cases.\nWe found a striking difference in agreement ac-\ncuracy between short and long coordinated verb\nphrases: performance on short phrases was poorer.\nWhile RNNs are known to struggle with general-\nizing short patterns to longer sequences, this pat-\ntern constitutes a failure to generalize to shorter\nsequences (cf. Trask et al., 2018); techniques for\nimproving longer distance dependency learning in\nLMs (e.g., Trinh et al., 2018; Dai et al., 2019) are\nunlikely to mitigate this deﬁcit. This suggests that\nchallenge sets should include materials that can\nbe used to ascertain whether the model’s syntac-\ntic representations are robust to syntactically irrel-\nevant factors such as constituent length.\nGPT and BERT, Transformer models trained on\nvery large corpora, did not consistently outper-\nform the LSTMs trained on several orders of mag-\nnitude less data. Other studies suggest that Trans-\nformer models suffer from similar problems as the\nLSTMs we have analyzed. BERT’s agreement ac-\ncuracy decreases as the subject becomes more dis-\ntant from its verb (Bacon and Regier, 2019). Dra-\nmatically increasing the pre-training corpus for a\nBERT-like model from 562M words to 18G words\nonly leads to a modest improvement in its natu-\nral language inference accuracy, from 81.7% to\n82.3% (Baevski et al., 2019). Overall, this body of\nresults points to the limited data efﬁciency of stan-\ndard RNNs and Transformers, and indicates that\nlearning syntax from realistic amounts of data—in\nparticular the amount of data available to humans\nwhen they learn language—may require syntac-\ntically structured architectures or explicit syntac-\ntic supervision (Enguehard et al., 2017; Kuncoro\net al., 2018, 2019; Wilcox et al., 2019).\n5836\nReferences\nGeoff Bacon and Terry Regier. 2019. Does BERT\nagree? Evaluating knowledge of structure depen-\ndence through agreement relations. Technical re-\nport, UC Berkeley.\nAlexei Baevski, Sergei Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. Technical re-\nport, Facebook AI Research.\nKathryn Bock and Carol A. Miller. 1991. Broken\nagreement. Cognitive Psychology, 23(1):45–93.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov.\n2019. Transformer-XL: Attentive language mod-\nels beyond a ﬁxed-length context. Technical report,\nCarnegie Mellon University.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Annual Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics. Association\nfor Computational Linguistics.\n´Emile Enguehard, Yoav Goldberg, and Tal Linzen.\n2017. Exploring the syntactic abilities of RNNs\nwith multi-task learning. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 3–14. Association\nfor Computational Linguistics.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. Technical report, Bar Ilan University.\nKlaus Greff, Rupesh K. Srivastava, Jan Koutn ´ık,\nBas R. Steunebrink, and J¨urgen Schmidhuber. 2017.\nLSTM: A search space odyssey. IEEE Transac-\ntions on Neural Networks and Learning Systems,\n28(10):2222–2232.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Annual Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nAlon Halevy, Peter Norvig, and Fernando Pereira.\n2009. The unreasonable effectiveness of data. IEEE\nIntelligent Systems, 24(2):8–12.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Yoshua\nBengio and Yann LeCun, editors, Proceedings of\nthe Fifth International Conference on Learning Rep-\nresentations. International Conference on Learning\nRepresentations.\nHarold Jeffreys. 1961. Theory of Probability, 3rd edi-\ntion. Oxford University Press, Oxford.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better.\nIn Proceedings of the 2018 Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1426–1436. Association for Computational Linguis-\ntics.\nAdhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen\nClark, and Phil Blunsom. 2019. Scalable syntax-\naware language models using knowledge distilla-\ntion. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 3472–3484, Florence, Italy. Association for\nComputational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nTal Linzen and Brian Leonard. 2018. Distinct pat-\nterns of syntactic agreement errors in recurrent net-\nworks and humans. In Proceedings of the 2018 An-\nnual Meeting of the Cognitive Science Society, pages\n690–695. Cognitive Science Society.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Ellen\nRiloff, David Chiang, Julia Hockenmaier, and\nJun’ichi Tsujii, editors, Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1192–1202. Association\nfor Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Wikitext-103. Technical re-\nport, Salesforce.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Pro-\nceedings of the 11th Annual Conference of the In-\nternational Speech Communication Association (IN-\nTERSPEECH 2010), pages 1045–1048, Makuhari,\nChiba, Japan.\nRichard D. Morey and Jeffrey N. Rouder. 2018.\nBayesFactor: Computation of Bayes Factors for\nCommon Designs. R package version 0.9.12-4.2.\n5837\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. GloVe: Global vectors for\nword representation. In Proceedings of EMNLP.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 2017 Annual Conference of the European\nChapter of the Association for Computational Lin-\nguistics. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of LSTM-networks for sequence tagging. In\nMartha Palmer, Rebecca Hwa, and Sebastian Riedel,\neditors, Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing,\npages 338–348. Association for Computational Lin-\nguistics.\nJeffrey N. Rouder, Paul L. Speckman, Dongchu Sun,\nRichard D. Morey, and Geoffrey Iverson. 2009.\nBayesian t-tests for accepting and rejecting the\nnull hypothesis. Psychonomic Bulletin & Review,\n16(2):225–237.\nAndrew Trask, Felix Hill, Scott E Reed, Jack Rae,\nChris Dyer, and Phil Blunsom. 2018. Neural\narithmetic logic units. In S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information\nProcessing Systems 31, pages 8035–8044. Curran\nAssociates, Inc.\nTrieu H. Trinh, Andrew M. Dai, Minh-Thang Luong,\nand Quoc V . Le. 2018. Learning longer-term depen-\ndencies in RNNs with auxiliary losses. In Proceed-\nings of the 35th International Conference on Ma-\nchine Learning, pages 4965–4974. PMLR 80.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran As-\nsociates, Inc.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019. Structural su-\npervision improves learning of non-local grammat-\nical dependencies. In Proceedings of the 2019 An-\nnual Conference of the North American Chapter of\nthe Association for Computational Linguistics. As-\nsociation for Computational Linguistics.\nThomas Wolf. 2019. Some additional experiments ex-\ntending the tech report “assessing BERT’s syntactic\nabilities” by Yoav Goldberg. Technical report, Hug-\ngingface Inc.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7679735422134399
    },
    {
      "name": "Syntax",
      "score": 0.6992640495300293
    },
    {
      "name": "Natural language processing",
      "score": 0.5764050483703613
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5313580632209778
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.473627507686615
    },
    {
      "name": "Natural language",
      "score": 0.46891674399375916
    },
    {
      "name": "Joint (building)",
      "score": 0.43832308053970337
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4117383360862732
    },
    {
      "name": "Linguistics",
      "score": 0.36973947286605835
    },
    {
      "name": "Engineering",
      "score": 0.0943082869052887
    },
    {
      "name": "History",
      "score": 0.09093227982521057
    },
    {
      "name": "Philosophy",
      "score": 0.057431161403656006
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ]
}