{
  "title": "Anticipative Video Transformer",
  "url": "https://openalex.org/W3169738033",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2322504504",
      "name": "Rohit Girdhar",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2239304286",
      "name": "Kristen Grauman",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963692464",
    "https://openalex.org/W6674904848",
    "https://openalex.org/W6739622702",
    "https://openalex.org/W2608988379",
    "https://openalex.org/W6769024994",
    "https://openalex.org/W6743837088",
    "https://openalex.org/W6781471103",
    "https://openalex.org/W3010874390",
    "https://openalex.org/W6799308665",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2980037812",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W6758736441",
    "https://openalex.org/W2550462002",
    "https://openalex.org/W6776216169",
    "https://openalex.org/W2993447238",
    "https://openalex.org/W4214755140",
    "https://openalex.org/W2963720581",
    "https://openalex.org/W3176051609",
    "https://openalex.org/W2989839235",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W6759455113",
    "https://openalex.org/W2962899219",
    "https://openalex.org/W6780832159",
    "https://openalex.org/W6752237900",
    "https://openalex.org/W6755626774",
    "https://openalex.org/W6794642395",
    "https://openalex.org/W3108892828",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2198618282",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W2619697695",
    "https://openalex.org/W6780196116",
    "https://openalex.org/W3034499084",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2099614498",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W2097073572",
    "https://openalex.org/W2554320282",
    "https://openalex.org/W6740017579",
    "https://openalex.org/W6754048563",
    "https://openalex.org/W6628868384",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W2962728572",
    "https://openalex.org/W2963814513",
    "https://openalex.org/W6955071965",
    "https://openalex.org/W2777985721",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6797361983",
    "https://openalex.org/W6777976589",
    "https://openalex.org/W2600081845",
    "https://openalex.org/W6753596515",
    "https://openalex.org/W6754935294",
    "https://openalex.org/W3124314487",
    "https://openalex.org/W2963482775",
    "https://openalex.org/W6739520758",
    "https://openalex.org/W2472970127",
    "https://openalex.org/W2960747818",
    "https://openalex.org/W3034667697",
    "https://openalex.org/W3210279979",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963570630",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W2799087757",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W6724944384",
    "https://openalex.org/W2422305492",
    "https://openalex.org/W6779965299",
    "https://openalex.org/W3106884574",
    "https://openalex.org/W6747225971",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W6749916090",
    "https://openalex.org/W6780677595",
    "https://openalex.org/W2800438594",
    "https://openalex.org/W3123326873",
    "https://openalex.org/W2109698606",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6600983433",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W6763170755",
    "https://openalex.org/W6682864246",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W2895299763",
    "https://openalex.org/W3015146382",
    "https://openalex.org/W2773514261",
    "https://openalex.org/W2996238012",
    "https://openalex.org/W2706729717",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3096383329",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2795410839",
    "https://openalex.org/W2931316642",
    "https://openalex.org/W3202698164",
    "https://openalex.org/W2962737939",
    "https://openalex.org/W3047740097",
    "https://openalex.org/W2885024018",
    "https://openalex.org/W3159612540",
    "https://openalex.org/W2995849700",
    "https://openalex.org/W2963610939",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2911273949",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2975357369",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2964233791",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3037916678",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W2795093135",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W3109667662",
    "https://openalex.org/W2789198060",
    "https://openalex.org/W3105486778",
    "https://openalex.org/W2966156373",
    "https://openalex.org/W3037927086",
    "https://openalex.org/W2913636123",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2894666173",
    "https://openalex.org/W2070653320",
    "https://openalex.org/W2964048159",
    "https://openalex.org/W1483019628",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2964242760",
    "https://openalex.org/W2950971447",
    "https://openalex.org/W3170218137",
    "https://openalex.org/W3168294587",
    "https://openalex.org/W3118673420",
    "https://openalex.org/W2099320314"
  ],
  "abstract": "We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to predict the next action in a video sequence, while also learning frame feature encoders that are predictive of successive future frames' features. Compared to existing temporal aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies--both critical for the anticipation task. Through extensive experiments, we show that AVT obtains the best reported performance on four popular action anticipation benchmarks: EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins first place in the EpicKitchens-100 CVPR'21 challenge.",
  "full_text": "Anticipative Video Transformer\nRohit Girdhar‚Ä† Kristen Grauman‚Ä†‚Ä°\n‚Ä†Facebook AI Research ‚Ä°University of Texas, Austin\nhttp://facebookresearch.github.io/AVT\nAbstract\nWe propose Anticipative Video Transformer (AVT), an\nend-to-end attention-based video modeling architecture\nthat attends to the previously observed video in order to\nanticipate future actions. We train the model jointly to pre-\ndict the next action in a video sequence, while also learn-\ning frame feature encoders that are predictive of succes-\nsive future frames‚Äô features. Compared to existing tempo-\nral aggregation strategies, AVT has the advantage of both\nmaintaining the sequential progression of observed actions\nwhile still capturing long-range dependencies‚Äîboth crit-\nical for the anticipation task. Through extensive experi-\nments, we show that AVT obtains the best reported per-\nformance on four popular action anticipation benchmarks:\nEpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and\n50-Salads; and it wins first place in the EpicKitchens-100\nCVPR‚Äô21 challenge.\n1. Introduction\nPredicting future human actions is an important task for\nAI systems. Consider an autonomous vehicle at a stop sign\nthat needs to predict whether a pedestrian will cross the\nstreet or not. Making this determination requires modeling\ncomplex visual signals‚Äîthe past actions of the pedestrian,\nsuch as speed and direction of walking, or usage of devices\nthat may hinder his awareness of the surroundings‚Äîand us-\ning those to predict what he may do next. Similarly, imag-\nine an augmented reality (AR) device that observes a user‚Äôs\nactivity from a wearable camera, e.g. as they cook a new\ndish or assemble a piece of furniture, and needs to antici-\npate his next steps to provide timely assistance. In many\nsuch applications, it is insufficient torecognize what is hap-\npening in the video. Rather, the vision system must also\nanticipate the likely actions that are to follow. Hence, there\nis a growing interest in formalizing the activity anticipation\ntask [24, 45, 49, 64, 73, 82] along with development of mul-\ntiple challenge benchmarks to support it [13, 14, 49, 55, 82].\nCompared to traditional action recognition, anticipation\ntends to be significantly more challenging. First of all, it re-\nwash \ntomato \ntime \n‚Ä¶\n‚Ä¶\nturn-off \ntap Head \nBackbone \nTemporal Attention \n(\n‚Üí ‚Üí line width) \nHead \nSpatial Attention \nInput Frames \nFigure 1: Anticipating future actions using A VTinvolves en-\ncoding video frames with a spatial-attention backbone, followed\nby a temporal-attention head that attends only to frames before the\ncurrent one to predict future actions. In this example, it sponta-\nneously learns to attend to hands and objects without being su-\npervised to do so. Moreover, it attends to frames most relevant\nto predict the next action. For example, to predict ‚Äòwash tomato‚Äô\nit attends equally to all previous frames as they determine if any\nmore tomatoes need to be washed, whereas for ‚Äòturn-off tap‚Äô it fo-\ncuses most on the current frame for cues whether the person might\nbe done. Please see ¬ß 5.3 for details and additional results.\nquires going beyond classifying current spatiotemporal vi-\nsual patterns into a single action category‚Äîa task nicely\nsuited to today‚Äôs well-honed discriminative models‚Äîto in-\nstead predict the multi-modal distribution of future activi-\nties. Moreover, while action recognition can often side-step\ntemporal reasoning by leveraging instantaneous contextual\ncues [31], anticipation inherently requires modeling the pro-\ngression of past actions to predict the future. For instance,\nthe presence of a plate of food with a fork may be sufficient\nto indicate the action of eating, whereas anticipating that\nsame action would require recognizing and reasoning over\nthe sequence of actions that precede it, such as chopping,\ncooking, serving, etc. Indeed, recent work [23, 77] finds\nthat modeling long temporal context is often critical for\nanticipation, unlike action recognition where frame-level\nmodeling is often enough [43, 50, 81]. These challenges are\nalso borne out in practice. For example, accuracy for one of\ntoday‚Äôs top performing video models [77] drops from 42%\nto 17% when treating recognition versus anticipation on the\nsame test clips [13]‚Äîpredicting even one second into the\nfuture is much harder than declaring the current action.\nThe typical approach to solving long-term predictive rea-\nsoning tasks involves extracting frame or clip level features\nusing standard architectures [12, 86, 91], followed by ag-\ngregation using clustering [32, 62], recurrence [23, 24, 42],\nor attention [28, 59, 77, 95] based models. Except the recur-\nrent ones, most such models merely aggregate features over\nthe temporal extent, with little regard to modeling the se-\nquential temporal evolution of the video over frames. While\nrecurrent models like LSTMs have been explored for antici-\npation [2, 23, 96], they are known to struggle with modeling\nlong-range temporal dependencies due to their sequential\n(non-parallel) nature. Recent work mitigates this limitation\nusing attention-based aggregation over different amounts\nof the context to produce short-term (‚Äòrecent‚Äô) and long-\nterm (‚Äòspanning‚Äô) features [77]. However, it still reduces\nthe video to multiple aggregate representations and loses its\nsequential nature. Moreover, it relies on careful and dataset-\nspecific tuning of the architecture and the amounts of con-\ntext used for the different aggregate features.\nIn this work, we introduce Anticipative Video Trans-\nformer (A VT), an alternate video modeling architecture that\nreplaces ‚Äúaggregation‚Äù based temporal modeling with a an-\nticipative1 architecture. Aiming to overcome the tradeoffs\ndescribed above, the proposed model naturally embraces\nthe sequential nature of videos, while minimizing the lim-\nitations that arise with recurrent architectures. Similar to\nrecurrent models, A VT can be rolled out indefinitely to pre-\ndict further into the future (i.e. generate future predictions),\nyet it does so while processing the input in parallel with\nlong-range attention, which is often lost in recurrent archi-\ntectures.\nSpecifically, A VT leverages the popular transformer ar-\nchitecture [89, 92] with causal2 masked attention, where\neach input frame is allowed to attend only to frames that\nprecede it. We train the model to jointly predict the next\naction while also learning to predict future features that\nmatch the true future features and (when available) their\nintermediate action labels. Figure 1 shows examples of\nhow A VT‚Äôs spatial and temporal attention spreads over pre-\nviously observed frames for two of its future predictions\n(wash tomato and turn-off tap). By incorporating interme-\ndiate future prediction losses, A VT encourages a predictive\nvideo representation that picks up patterns in how the vi-\nsual activity is likely to unfold into the future. This facet\nof our model draws an analogy to language, where trans-\n1We use the term ‚Äúanticipative‚Äù to refer to our model‚Äôs ability to pre-\ndict future video features and actions.\n2Throughout we use the term ‚Äúcausal‚Äù to refer to the constraint that\nvideo be processed in a forward, online manner, i.e. functions applied at\ntime t can only reference the frames preceding them, akin to Causal Lan-\nguage Modeling (CLM) [51]. This is not to be confused with other uses of\n‚Äúcausal‚Äù in AI where the connotation is instead cause-and-effect.\nformers trained with massive text corpora are now powerful\ntools to anticipate sequences of words ( cf. GPT and vari-\nants [8, 69, 70]). The incremental temporal modeling aspect\nhas been also been explored for action recognition [53], al-\nbeit with convolutional architectures and without interme-\ndiate self-supervised losses.\nWhile the architecture described so far can be applied on\ntop of various frame or clip encoders (as we will show in\nexperiments), we further propose a purely attention-based\nvideo modeling architectureby replacing the backbone with\nan attention-based frame encoder from the recently intro-\nduced Vision Transformer [18]. This enables A VT to at-\ntend not only to specific frames, but also to spatial features\nwithin the frames in one unified framework. As we see\nin Figure 1, when trained on egocentric video, the model\nspontaneously learns to attend to spatial features corre-\nsponding to hands and objects, which tend to be especially\nimportant in anticipating future activities [57].\nIn summary, our contributions are: 1) A VT, a novel\nend-to-end purely attention based architecture for predic-\ntive video modeling; 2) Incorporation of a self-supervised\nfuture prediction loss, making the architecture especially\napplicable to predictive tasks like action anticipation; 3) Ex-\ntensive analysis and ablations of the model showing its ver-\nsatility with different backbone architectures, pre-trainings,\netc. on the most popular action anticipation benchmarks,\nboth from first and third person viewpoints. Specifically,\nwe outperform all published prior work on EpicKitchens-\n553 [13], EpicKitchens-1003 [14], EGTEA Gaze+ [55], and\n50-Salads [82]. Most notably, our method outperforms all\nsubmissions to the EpicKitchens-100 CVPR‚Äô21 challenge4,\nand is ranked #1 on the EpicKitchens-55 leaderboard 5 for\nseen (S1) and #2 on unseen (S2) test sets.\n2. Related Work\nAction anticipation is the task of predicting future ac-\ntions given a video clip. While well explored in third-\nperson video [2, 26, 38, 39, 47, 49, 82, 90], it has re-\ncently gained in popularity for first-person (egocentric)\nvideos [13, 14, 16, 24, 57, 64, 77], due to its applicabil-\nity on wearable computing platforms. Various approaches\nhave been proposed for this task, such as learning represen-\ntations by predicting future features [90, 96], aggregating\npast features [24, 77], or leveraging affordances and hand\nmotion [57, 64]. Our work contributes a new video archi-\ntecture for anticipation, and we demonstrate its promising\nadvantages on multiple popular anticipation benchmarks.\nSelf-supervised feature learning from video methods\nlearn representations from unlabeled video, often to be fine-\n3EpicKitchens-55/100 datasets are licensed under the Creative Com-\nmons Attribution-NonCommercial 4.0 International License.\n4competitions.codalab.org/competitions/25925\n5competitions.codalab.org/competitions/20071\ntuned for particular downstream tasks. Researchers ex-\nplore a variety of ‚Äúfree‚Äù supervisory signals, such as tem-\nporal consistency [21, 41, 44, 94, 99], inter-frame pre-\ndictability [36, 37, 40, 83], and cross-modal correspon-\ndence [3, 48, 83, 84]. A VT incorporates losses that en-\ncourage features predictive of future features (and actions);\nwhile this aspect shares motivation with prior [25, 36, 37,\n58, 60, 75, 78, 83, 84, 90] and concurrent work [96], our\narchitecture to achieve predictive features is distinct (trans-\nformer based rather than convolutional/recurrent [25, 36,\n37, 78, 96]), it operates over raw frames or continuous video\nfeatures as opposed to clustered ‚Äòvisual words‚Äô [84], as-\nsumes only visual data (rather than vision with speech or\ntext [83, 84]), and is jointly trained for action anticipation\n(rather than pre-trained and then fine-tuned for action recog-\nnition [36, 37, 83]).\nLanguage modeling (LM) has been revolutionized with\nthe introduction of self-attention architectures [89]. LM ap-\nproaches can generally be classified in three categories: (1)\nencoder-only [17, 67], which leverage bidirectional atten-\ntion and are effective for discriminative tasks such as clas-\nsification; (2) decoder-only [8, 69], which leverage a causal\nattention [51] attending on past tokens, and are effective for\ngenerative tasks such as text generation; and (3) encoder-\ndecoder [52, 71], which incorporate both a bidirectional en-\ncoder and causal decoder, and are effective for tasks such\nas machine translation. Capitalizing on the analogy be-\ntween action prediction and generative language tasks, we\nexplore causal decoder-only attention architectures in our\nmodel. While language models are typically trained on\ndiscrete inputs (words), A VT trains with continuous video\nfeatures. This distinction naturally influences our design\nchoices, such as an L2 loss for generative training as op-\nposed to a cross entropy loss for the next word.\nSelf-attention and transformers in vision.The general\nidea of self-attention in vision dates back to non-local\nmeans [9], and is incorporated into contemporary network\narchitectures as non-local blocks [10, 56, 93, 95] and gat-\ning mechanisms [30, 46, 62, 97]. While self-attention ap-\nproaches like transformers [89, 92] offer strong results for\nhigh-level vision reasoning tasks [11, 101], more recently,\nthere is growing interest in completely replacing convolu-\ntional architectures with transformers for image recogni-\ntion [18, 85]. For video, prior work has mostly leveraged\nattention architectures [28, 93, 95] on top of standard spa-\ntiotemporal convolutional base architectures [12, 86, 88]. In\ncontrast, A VT is an end-to-end transformer architecture for\nvideo‚Äîto our knowledge the first (concurrent with [4, 7, 19,\n54, 65]). Unlike the concurrent methods [4, 7, 19, 54, 65],\nwhich are bidirectional and address traditional action recog-\nnition, A VT has a causal structure and tackles predictive\ntasks (anticipation). A VT yields the best results to date for\nseveral well-studied anticipation benchmarks.\nPast Frames \n(\n<latexit sha1_base64=\"gCypq3O8UF/yHR3aIxsdKvZUONw=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1u0iznu6VK37Vn4OskiAnFchR75W/un3NsoQrZJJa2wn8FMMJNSiY5NNSN7M8pWxEB7zjqKIJt+Fkfu2UnDmlT2JtXCkkc/X3xIQm1o6TyHUmFId22ZuJ/3mdDOObcCJUmiFXbLEoziRBTWavk74wnKEcO0KZEe5WwobUUIYuoJILIVh+eZU0L6rBVdW/v6zU/DyOIpzAKZxDANdQgzuoQwMYPMIzvMKbp70X7937WLQWvHzmGP7A+/wBot2PHA==</latexit>\n‚åß o\n<latexit sha1_base64=\"gCypq3O8UF/yHR3aIxsdKvZUONw=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1u0iznu6VK37Vn4OskiAnFchR75W/un3NsoQrZJJa2wn8FMMJNSiY5NNSN7M8pWxEB7zjqKIJt+Fkfu2UnDmlT2JtXCkkc/X3xIQm1o6TyHUmFId22ZuJ/3mdDOObcCJUmiFXbLEoziRBTWavk74wnKEcO0KZEe5WwobUUIYuoJILIVh+eZU0L6rBVdW/v6zU/DyOIpzAKZxDANdQgzuoQwMYPMIzvMKbp70X7937WLQWvHzmGP7A+/wBot2PHA==</latexit>\n‚åß o )\nAnticipation\nTime (\n<latexit sha1_base64=\"uGpqdCQ2CPnbHq0Sb1S7xCN2IAE=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJfROZpMxszPLzKwQQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgU31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVpyhpUCaXbERomuGQNy61g7VQzTCLBWtHodua3npg2XMkHO05ZmOBA8phTtE5qdi1mPeyVK37Vn4OskiAnFchR75W/un1Fs4RJSwUa0wn81IYT1JZTwaalbmZYinSEA9ZxVGLCTDiZXzslZ07pk1hpV9KSufp7YoKJMeMkcp0J2qFZ9mbif14ns/FNOOEyzSyTdLEozgSxisxeJ32uGbVi7AhSzd2thA5RI7UuoJILIVh+eZU0L6rBVdW/v6zU/DyOIpzAKZxDANdQgzuoQwMoPMIzvMKbp7wX7937WLQWvHzmGP7A+/wBjaWPDg==</latexit>\n‚åß a\n<latexit sha1_base64=\"uGpqdCQ2CPnbHq0Sb1S7xCN2IAE=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJfROZpMxszPLzKwQQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSgU31ve/vcLa+sbmVnG7tLO7t39QPjxqGpVpyhpUCaXbERomuGQNy61g7VQzTCLBWtHodua3npg2XMkHO05ZmOBA8phTtE5qdi1mPeyVK37Vn4OskiAnFchR75W/un1Fs4RJSwUa0wn81IYT1JZTwaalbmZYinSEA9ZxVGLCTDiZXzslZ07pk1hpV9KSufp7YoKJMeMkcp0J2qFZ9mbif14ns/FNOOEyzSyTdLEozgSxisxeJ32uGbVi7AhSzd2thA5RI7UuoJILIVh+eZU0L6rBVdW/v6zU/DyOIpzAKZxDANdQgzuoQwMoPMIzvMKbp7wX7937WLQWvHzmGP7A+/wBjaWPDg==</latexit>\n‚åß a ) \nAction\nSegment\nObserved video Unobserved video\nFigure 2: Action anticipation problem setup.The goal is to use\nthe observed video segment of length œÑo to anticipate the future\naction œÑa seconds before it happens.\n3. Anticipation Problem Setup\nWhile multiple anticipation problem setups have been\nexplored in the literature [45, 64, 73], in this work we follow\nthe setup defined in recent challenge benchmarks [13, 14]\nand illustrated in Figure 2. For each action segment labeled\nin the dataset starting at time œÑs, the goal is to recognize it\nusing a œÑo length video segment œÑa units before it, i.e. from\nœÑs ‚àí (œÑa + œÑo) to œÑs ‚àí œÑa. While methods are typically\nallowed to use any length of observed segments ( œÑo), the\nanticipation time (œÑa) is usually fixed for each dataset.\n4. Anticipative Video Transformer\nWe now present the A VT model architecture, as illus-\ntrated in Figure 3. It is designed to predict future actions\ngiven a video clip as input. To that end, it leverages a two-\nstage architecture, consisting of a backbone network that\noperates on individual frames or short clips, followed by a\nhead architecture that operates on the frame/clip level fea-\ntures to predict future features and actions. A VT employs\ncausal attention modeling‚Äîpredicting the future actions\nbased only on the frames observed so far‚Äîand is trained\nusing objectives inspired from self-supervised learning. We\nnow describe each model component in detail, followed by\nthe training and implementation details.\n4.1. Backbone Network\nGiven a video clip with T frames, V = {X1, ¬∑¬∑¬∑ , XT }\nthe backbone network, B, extracts a feature representa-\ntion for each frame, {z1, ¬∑¬∑¬∑ , zT } where zt = B(Xt).\nWhile various video base architectures have been pro-\nposed [12, 20, 87, 91] and can be used with A VT as we\ndemonstrate later, in this work we propose an alternate ar-\nchitecture for video understanding based purely on atten-\ntion. This backbone, which we refer to as A VT-b, adopts\nthe recently proposed Vision Transformer (ViT) [18] archi-\ntecture, which has shown impressive results for static image\nclassification.\nSpecifically, we adopt the ViT-B/16 architecture. We\nsplit each input frame into16√ó16 non-overlapping patches.\nWe flatten each patch into a 256D vector, and linearly\nproject them to 768D, which is the feature dimension used\nthroughout the encoder. While we do not need to clas-\nsify each frame individually, we still prepend a learnable\n[class] token embedding to the patch features, whose\nLinear ProjectionsLinear ProjectionsLinear ProjectionsLinear Projections123*0 9‚Ä¶ 123*0 9‚Ä¶ 123*0 9‚Ä¶ 123*0 9‚Ä¶[CLASS]token Patch features + spatial position embedding\nInput Video Frames\nCausal Transformer Decoder\nUnwrap Pizza Plate Pizza Take WrapperCrumple WrapperThrow wrapper\nLinearLinearLinearLinearLinear‚Ñí!\"#\nTransformer EncoderTransformer EncoderTransformer EncoderTransformer Encoder\n0 1 2 3 Past frame features + temporal position embedding\nBackboneHead\n‚Ñí!\"# ‚Ñí!\"# ‚Ñí!\"# ‚Ñí$%&'\n‚Ñí(%)' ‚Ñí(%)' ‚Ñí(%)'\nùêó* ùêó+\nùê≥!\nùê≥!\nùê≥\" ùê≥# ùê≥$\n\"ùê≥\" \"ùê≥#\"ùê≥! \"ùê≥$\nLayerNorm\nMasked \nmulti-head \nattention\nPast frame \nembeddings\n+\nLayerNorm\nMLP\n+\nFuture frame \nembeddings\nLayerNorm\nL \n<latexit sha1_base64=\"NjGIR+4adDImgJVPgwBhEg5se9g=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hd2g6DHgxWME84BkCbOT2WTM7Mwy0yuEkH/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63t7a+sbm1Xdgp7u7tHxyWjo6bVmeG8QbTUpt2RC2XQvEGCpS8nRpOk0jyVjS6nfmtJ26s0OoBxykPEzpQIhaMopOaXRQJt71S2a/4c5BVEuSkDDnqvdJXt69ZlnCFTFJrO4GfYjihBgWTfFrsZpanlI3ogHccVdQtCSfza6fk3Cl9EmvjSiGZq78nJjSxdpxErjOhOLTL3kz8z+tkGN+EE6HSDLlii0VxJglqMnud9IXhDOXYEcqMcLcSNqSGMnQBFV0IwfLLq6RZrQRXFf/+slyr5nEU4BTO4AICuIYa3EEdGsDgEZ7hFd487b14797HonXNy2dO4A+8zx+yrY8o</latexit>\n‚Üí\n<latexit sha1_base64=\"NjGIR+4adDImgJVPgwBhEg5se9g=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hd2g6DHgxWME84BkCbOT2WTM7Mwy0yuEkH/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63t7a+sbm1Xdgp7u7tHxyWjo6bVmeG8QbTUpt2RC2XQvEGCpS8nRpOk0jyVjS6nfmtJ26s0OoBxykPEzpQIhaMopOaXRQJt71S2a/4c5BVEuSkDDnqvdJXt69ZlnCFTFJrO4GfYjihBgWTfFrsZpanlI3ogHccVdQtCSfza6fk3Cl9EmvjSiGZq78nJjSxdpxErjOhOLTL3kz8z+tkGN+EE6HSDLlii0VxJglqMnud9IXhDOXYEcqMcLcSNqSGMnQBFV0IwfLLq6RZrQRXFf/+slyr5nEU4BTO4AICuIYa3EEdGsDgEZ7hFd487b14797HonXNy2dO4A+8zx+yrY8o</latexit>\n‚Üí\nFigure 3:(Left) A VT architecture.We split the T input frames into non-overlapping patches that are linearly projected. We add a learned\n[CLASS] token, along with spatial position embeddings, and the resulting features are passed through multiple layers of multi-head\nattention, with shared weights across the transformers applied to all frames. We take the resulting features corresponding to the [CLASS]\ntoken, append a temporal position encoding and pass it through the Causal Transformer Decoder that predicts the future feature at framet,\nafter attending to all features from 1 ¬∑¬∑¬∑ t. The resulting feature is trained to regress to the true future feature (Lfeat ) and predict the action\nat that time point if labeled ( Lcls), and the last prediction is trained to predict the future action ( Lnext). (Right) Causal Transformer\nDecoder. It follows the Transformer architecture with pre-norm [92], causal masking in attention, and a final LayerNorm [70].\noutput will be used as a frame-level embedding input to\nthe head. Finally, we add learned position embeddings to\neach patch feature similar to [18]. We choose to stick to\nframe-specific spatial position encodings, so that the same\nbackbone model with shared weights can be applied to each\nframe. We will incorporate the temporal position informa-\ntion in the head architecture (discussed next). The result-\ning patch embeddings are passed through a standard Trans-\nformer Encoder [89] with pre-norm [92]. We refer the\nreader to [18] for details of the encoder architecture.\nA VT-b is an attractive backbone design because it makes\nour architecture purely attentional. Nonetheless, in addi-\ntion to A VT-b, A VT is compatible with other video back-\nbones, including those based on 2D CNNs [80, 91], 3D\nCNNs [12, 20, 87], or fixed feature representations based\non detected objects [5, 6] or visual attributes [63]. In ¬ß 5\nwe provide experiments testing several such alternatives.\nFor the case of spatiotemporal backbones, which operate\non clips as opposed to frames, we extract features as zt =\nB(Xt‚àíL, ¬∑¬∑¬∑ , Xt), where the model is trained on L-length\nclips. This ensures the features at framet do not incorporate\nany information from the future, which is not allowed in the\nanticipation problem setting.\n4.2. Head Network\nGiven the features extracted by the backbone, the head\nnetwork, referred to as A VT-h, is used to predict the future\nfeatures for each input frame using a Causal Transformer\nDecoder, D:\nÀÜ z1, ¬∑¬∑¬∑ ,ÀÜ zT = D(z1, ¬∑¬∑¬∑ , zT ). (1)\nHere ÀÜ zt is the predicted future feature corresponding to\nframe feature zt, after attending to all features before and\nincluding it. The predicted features are then decoded into\na distribution over the semantic action classes using a lin-\near classifier Œ∏, i.e. ÀÜ yt = Œ∏(ÀÜ zt). The final prediction, ÀÜ yT ,\nis used as the model‚Äôs output for the next-action anticipa-\ntion task. Note that since the next action segment ( T + 1)\nis œÑa seconds from the last observed frame ( T) as per the\nproblem setup, we typically sample frames at a stride of œÑa\nso that the model learns to predict future features/actions at\nthat frame rate. However, empirically we find the model is\nrobust to other frame rate values as well.\nWe implement D using a masked transformer decoder\ninspired from popular approaches in generative language\nmodeling, such as GPT-2 [70]. We start by adding a tempo-\nral position encoding to the frame features implemented as\na learned embedding of the absolute frame position within\nthe clip. The embedded features are then passed through\nmultiple decoder layers, each consisting of masked multi-\nhead attention, LayerNorm (LN) and a multi-layer percep-\ntron (MLP), as shown in Figure 3 (right). The final output\nis then passed through another LN, akin to GPT-2 [70], to\nobtain the future frame embeddings.\nAside from being visual rather than textual, this model\ndiffers from the original Transformer Decoder [89] in terms\nof the final LN and the masking operation in the multi-head\nattention. The masking ensures that the model only attends\nto specific parts of the input, which in the case of predictive\ntasks like ours, is defined as a ‚Äòcausal‚Äô mask. That is, for the\noutput corresponding to the future after frame t, i.e. ÀÜ zt, we\nset the mask to only attend to z1 ¬∑¬∑¬∑ zt. We refer the reader\nto [70] for details on the masking implementation.\nThis design differs considerably from previous applica-\ntions of language modeling architectures to video, such as\nVideoBERT [84]. It operates directly on continuous clip\nembeddings instead of first clustering them into tokens, and\nit leverages causal attention to allow for anticipative train-\ning (discussed next), instead of needing masked language\nmodeling (MLM) as in BERT [17]. These properties make\nA VT suited for predictive video tasks while allowing for the\nlong-range reasoning that is often lost in recurrent architec-\ntures. While follow-ups to VideoBERT such as CBT [83]\noperate on raw clip features, they still leverage a MLM ob-\njective with bidirectional attention, with the primary goal of\nrepresentation learning as opposed to future prediction.\n4.3. Training A VT\nTo sample training data, for each labeled action segment\nin a given dataset, we sample a clip preceding it and ending\nœÑa seconds before the start of the action. We pass the clip\nthrough A VT to obtain future predictions, and then super-\nvise the network using three losses.\nFirst, we supervise the next-action prediction using a\ncross-entropy loss with the labeled future action,cT+1:\nLnext = ‚àílog ÀÜ yT [cT+1]. (2)\nSecond, to leverage the causal structure of the model, we\nsupervise the model‚Äôs intermediate future predictions at the\nfeature level and the action class level. For the former, we\npredict future features to match the true future features that\nare present in the clip, i.e.\nLfeat =\nT‚àí1X\nt=1\n||ÀÜ zt ‚àí zt+1||2\n2. (3)\nThis loss is inspired from the seminal work by V ondrick\net al . [90] as well as follow ups [36, 37] that show that\nanticipating future visual representations is an effective\nform of self-supervision, though typically for traditional ac-\ntion recognition tasks. Concurrent and recent work adopts\nsimilar objectives for anticipation tasks, but with recur-\nrent architectures [25, 78, 96]. Whereas recent meth-\nods [36, 37, 96] explore this loss with NCE-style [66] ob-\njectives, in initial experiments we found simple L2 loss to\nbe equally effective. Since our models are always trained\nwith the final supervised loss, we do not suffer from poten-\ntial collapse during training that would necessitate the use\nof contrastive losses.\nThird, as an action class level anticipative loss, we lever-\nage any action labels available in the dataset to supervise the\nintermediate predictions, i.e., when the input clip overlaps\nwith any labeled action segments that precede the segment\nto be anticipated.6 Setting ct = ‚àí1 for any earlier frames\nfor which we do not have labels, we incur the following\nloss:\nLcls =\nT‚àí1X\nt=1\nLt\ncls; Lt\ncls =\n(\n‚àílog ÀÜ yt[ct+1] if ct+1 ‚â• 0\n0 otherwise.\n(4)\nWe train our model with\nL = Lnext + Lcls + Lfeat (5)\nas the objective, and refer to it as the anticipative [a] train-\ning setting. As a baseline, we also experiment with a model\ntrained solely with L = Lnext, and refer to it as the naive\n[n] setting, as it does not leverage our model‚Äôs causal atten-\ntion structure, instead supervising only the final prediction\nwhich attends to the full input. As we will show in Table 7,\nthe anticipative setting leads to significant improvements.\n4.4. Implementation Details\nWe preprocess the input video clips by randomly scaling\nthe height between 248 and 280px, and take 224px crops at\ntraining time. We sample 10 frames at 1FPS for most ex-\nperiments. We adopt network architecture details from [18]\nfor the A VT-b backbone. Specifically, we use a 12-head,\n12-layer transformer encoder model that operates on 768D\nrepresentations. We initialize the weights from a model pre-\ntrained on ImageNet-1K (IN1k), ImageNet-21K (IN21k)\nor ImageNet-1K finetuned from ImageNet-21K (IN21+1k),\nand finetune end-to-end for the anticipation tasks. For A VT-\nh, we use a 4-head, 6-layer model that operates on a 2048D\nrepresentation, initialized from scratch. We employ a lin-\near layer between the backbone and head to project the fea-\ntures to match the feature dimensions used in the head. We\ntrain A VT end-to-end with SGD+momentum using 10‚àí6\nweight decay and 10‚àí4 learning rate for 50 epochs, with a\n20 epoch warmup [33] and 30 epochs of cosine annealed de-\ncay. At test time, we employ 3-crop testing, where we com-\npute three 224px spatial crops from 248px input frames, and\n6For example, this would be true for each frame for densely labeled\ndatasets like 50-Salads, and a subset of frames for sparsely labeled datasets\nlike EpicKitchens-55.\nDataset Viewpoint Segments Classes œÑa (s) Metric(s)\nEK100 [14] 1st 90.0K 3,807 1.0 [14] recall\nEK55 [13] 1st 39.6K 2,513 1.0 [13] top-1/5, recall\nEGTEA Gaze+ [55] 1st 10.3K 106 0.5 [57] top-1, cm top-1\n50S [82] 3rd 0.9K 17 1.0 [2] top-1\nTable 1: Datasetsused for evaluation. We use four popular bench-\nmarks, spanning first and third person videos. Class-mean (‚Äòcm‚Äô)\n=‚áí evaluation is done per-class and averaged over classes. Recall\nrefers to class-mean recall@5 from [22]. For all, higher is better.\naverage the predictions over the corresponding three clips.\nThe default backbone for A VT is A VT-b, based on the ViT-\nB/16 architecture. However, to enrich our comparisons with\nsome baselines [23, 24, 77], below also we report perfor-\nmance of only our head model operating on fixed features\nfrom 1) a frame-level TSN [91] backbone pre-trained for\naction classification, or 2) a recent spatiotemporal convo-\nlutional architecture irCSN-152 [87] pre-trained on a large\nweakly labeled video dataset [27], which has shown strong\nresults when finetuned for action recognition. We fine-\ntune that model for action classification on the anticipation\ndataset and extract features that are used by the head for\nanticipation. In these cases, we only train the A VT-h lay-\ners. For all datasets considered, we use the validation set or\nsplit 1 to further optimize the hyperparameters, and use that\nsetup over multiple splits or the held out test sets. Code and\nmodels will be released for reproducibility.\n5. Experiments\nWe empirically evaluate A VT on four popular action an-\nticipation benchmarks covering both first- and third-person\nvideos. We start by describing the datasets and evaluation\nprotocols (¬ß 5.1), followed by key results and comparisons\nto the state of the art (¬ß 5.2), and finally ablations and qual-\nitative results (¬ß 5.3).\n5.1. Experimental Setup\nDatasets and metrics.We test on four popular action antic-\nipation datasets summarized in Table 1. EpicKitchens-100\n(EK100) [14] is the largest egocentric (first-person) video\ndataset with 700 long unscripted videos of cooking activi-\nties totalling 100 hours. EpicKitchens-55 (EK55) [13] is an\nearlier version of the same, and allows for comparisons to\na larger set of baselines which have not yet been reported\non EK100. For both, we use the standard train, val, and test\nsplits from [14] and [23] respectively to report performance.\nThe test evaluation is performed on a held-out set through a\nsubmission to their challenge server. EGTEA Gaze+ [55]\nis another popular egocentric action anticipation dataset.\nFollowing recent work [57], we report performance on the\nsplit 1 [55] of the dataset at œÑa = 0.5s. Finally, 50-Salads\n(50S) [82] is a popular third-person anticipation dataset, and\nHead Backbone Init Verb Noun Action\nRULSTM [14] TSN IN1k 27.5 29.0 13.3\nA VT-h TSN IN1k 27.2 30.7 13.6\nA VT-h irCSN152 IG65M 25.5 28.1 12.8\nA VT-h A VT-b IN1k 28.2 29.3 13.4\nA VT-h A VT-b IN21+1k 28.7 32.3 14.4\nA VT-h A VT-b IN21k 30.2 31.7 14.9\nRULSTM [14] Faster R-CNN IN1k 17.9 23.3 7.8\nA VT-h Faster R-CNN IN1k 18.0 24.3 8.7\nTable 2: EK100 (val) using RGB and detected objects (OBJ)\nmodalities separately. A VT outperforms prior work using the ex-\nact same features, and further improves with our A VT-b backbone.\nPerformance reported using class-mean recall@5.\nwe report top-1 accuracy averaged over the pre-defined 5\nsplits following prior work [77]. Some of these datasets\nemploy top-5/recall@5 criterion to account for the multi-\nmodality in future predictions, as well as class-mean (cm)\nmetrics to equally weight classes in a long-tail distribution.\nThe first three datasets also decompose the action annota-\ntions into verb and nouns. While some prior work [77]\nsupervises the model additionally for nouns and verbs, we\ntrain all our model solely to predict actions, and estimate\nthe verb/noun probabilities by marginalizing over the other,\nsimilar to [23]. In all tables, we highlight the columns show-\ning the metric used to rank methods in the official challenge\nleaderboards. Unless otherwise specified, the reported met-\nrics correspond to future action (act.) prediction, although\nwe do report numbers for verb and nouns separately where\napplicable. Please see Appendix A for further details.\nBaselines. We compare A VT to its variants with differ-\nent backbones and pretrained initializations, as well as to\nthe strongest recent approaches for action anticipation, i.e.\nRULSTM [23, 24], ActionBanks [77], and Forecasting HOI\n(FHOI) [57]. Please see Appendix B for details on them.\nWhile FHOI trains the model end-to-end, RULSTM and\nActionBanks operate on top of features from a model pre-\ntrained for action classification on that dataset. Hence, we\nreport results both using the exact same features as well as\nend-to-end trained backbones to facilitate fair comparisons.\n5.2. Comparison to the state-of-the-art\nEK100. We first compare A VT to prior work using individ-\nual modalities (RGB and Obj [23]) in Table 2 for apples-to-\napples comparisons and to isolate the performance of each\nof our contributions. First, we compare to the state-of-the-\nart RULSTM method using only our A VT (head) model\napplied to the exact same features from TSN [91] trained\nfor classification on EK100. We note this already improves\nover RULSTM, particularly in anticipating future objects\n(nouns). Furthermore, we experiment with backbone fea-\nOverall Unseen Kitchen Tail Classes\nSplit Method Verb Noun Act Verb Noun Act Verb Noun Act\nVal\nchance 6.4 2.0 0.2 14.4 2.9 0.5 1.6 0.2 0.1\nRULSTM [14] 27.8 30.814.0 28.827.2 14.219.8 22.0 11.1\nA VT+ (TSN) 25.5 31.814.8 25.5 23.6 11.5 18.5 25.8 12.6\nA VT+ 28.2 32.015.9 29.523.9 11.921.1 25.8 14.1\nTest\nchance 6.2 2.3 0.1 8.1 3.3 0.3 1.9 0.7 0.0\nRULSTM [14] 25.3 26.711.2 19.4 26.9 9.7 17.6 16.0 7.9\nTBN [100] 21.5 26.8 11.0 20.828.3 12.213.2 15.4 7.2\nA VT+ 25.6 28.812.6 20.922.3 8.8 19.0 22.0 10.1\nChallenge\nIIEMRG 25.3 26.7 11.2 19.4 26.9 9.7 17.6 16.0 7.9\nNUSCVML [76] 21.8 30.612.6 17.9 27.0 10.5 13.6 20.6 8.9\nICL+SJTU [35]36.2 32.213.427.6 24.2 10.132.1 29.911.9\nPanasonic [98] 30.433.514.8 21.1 27.1 10.2 24.6 27.5 12.7\nA VT++ 26.7 32.3 16.721.0 27.6 12.919.3 24.013.8\nTable 3: EK100 val and test setsusing all modalities. We split\nthe test comparisons between published work and CVPR‚Äô21 chal-\nlenge submissions. We outperform prior work including all chal-\nlenge submissions, with especially significant gains on tail classes.\nPerformance is reported using class-mean recall@5. A VT+ and\nA VT++ late fuse predictions from multiple modalities; please see\ntext for details.\ntures from a recent state-of-the-art video model, irCSN-\n152 [87] pretrained on a large weakly supervised dataset,\nIG65M [27]. We finetune this backbone for recognition on\nEK100, extract its features and train A VT-h same as before,\nbut find it to not be particularly effective at the EK100 antic-\nipation task. Next, we replace the backbone with our A VT-b\nand train the model end-to-end, leading to the best perfor-\nmance so far, and outperforming RULSTM by 1.6%. We\nmake the same comparison over features from an object-\ndetector [72] trained on EK100 provided by RULSTM (re-\nferred to as OBJ modality, details in Appendix A), and simi-\nlarly find our method outperforms RULSTM on this modal-\nity as well.\nNote that the fixed features used above can be thought of\nas a proxy for past recognized actions, as they are trained\nonly for action recognition. Hence, A VT-h on TSN or\nirCSN152 features is comparable to a baseline that trains\na language model over past actions to predict future ones.\nAs the later experiments show, end-to-end trained A VT is\nsignificantly more effective, supporting A VT‚Äôs from-pixels\nanticipation as opposed to label-space anticipation.\nFinally, we compare models using all modalities on the\nEK100 val and the held-out test set in Table 3. While RUL-\nSTM fuses models trained on RGB, Flow, and OBJ features\nusing an attention based model (MATT [23]), we simply\nlate fuse predictions from our best RGB and OBJ models\n(resulting model referred to as A VT+), and outperform all\nreported work on this benchmark, establishing a new state-\nof-the-art. Note we get the largest gains on tail classes, sug-\ngesting our model is particularly effective at few-shot antici-\npation. Finally, A VT++ ensembles multiple model variants,\nand outperforms all submissions on the EK100 CVPR‚Äô21\nHead Backbone Init Top-1 Top-5 Recall\nRULSTM [24] TSN IN1k 13.1 30.8 12.5\nActionBanks [77] TSN IN1k 12.3 28.5 13.1\nA VT-h TSN IN1k 13.1 28.1 13.5\nA VT-h A VT-b IN21+1k 12.5 30.1 13.6\nA VT-h irCSN152 IG65M 14.4 31.7 13.2\nTable 4: EK55 using only RGB modalityfor action anticipation.\nA VT performs comparably, and outperforms when combined with\na backbone pretrained on large weakly labeled dataset.\nTop-1 acc. Class mean acc.\nMethod Verb Noun Act. Verb Noun Act.\nI3D-Res50 [12] 48.0 42.134.8 31.3 30.023.2\nFHOI [57] 49.0 45.5 36.6 32.5 32.725.3\nA VT-h (+TSN) 51.7 50.339.8 41.2 41.428.3\nA VT 54.9 52.243.0 49.9 48.335.2\nTable 5: EGTEA Gaze+Split 1 at œÑa =\n0.5s. A VT outperforms prior work by sig-\nnificant margins, especially when trained\nend-to-end with the A VT-b backbone.\nHead Top-1\nDMR [90] 6.2\nRNN [2] 30.1\nCNN [2] 29.8\nActionBanks [77] 40.7\nA VT 48.0\nTable 6: 50-Salads.\nA VT outperforms\nprior work even in\n3rd person videos.\nchallenge leaderboard. Please refer to the workshop pa-\nper [29] for details on A VT++.\nEK55. Since EK100 is relatively new and has few baseline\nmethods reported, we also evaluate A VT on EK55. As be-\nfore, we start by comparing single modality methods (RGB-\nonly) in Table 4. For A VT-h models, we found a slightly\ndifferent set of (properly validated) hyperparameters per-\nformed better for top-1/5 metrics vs. the recall metric, hence\nwe report our best models for each set of results. Here we\nfind A VT-h performs comparably to RULSTM, and outper-\nforms another attention-based model [77] (one of the win-\nners of the EK55 2020 challenge) on the top-1 metrics. The\ngain is more significant on the recall metric, which aver-\nages performance over classes, indicating again that A VT-h\nis especially effective on tail classes which get ignored in\ntop-1/5 metrics. Next, we replace the backbone with A VT-\nb, and find it to perform comparably on top-1/5 metrics, and\noutperforms on the recall metric. Finally, we experiment\nwith irCSN-152 [87] pretrained using IG65M [27] and fine-\ntuned on EK55, and find it to outperform all methods by\na significant margin on top-1/5. We show further compar-\nisons with the state-of-the-art on EK55 in Appendix C.\nEGTEA Gaze+. In Table 5 we compare our method at\nœÑa = 0.5s on the split 1 as in recent work [57]. Even us-\ning fixed features with A VT-h on top, A VT outperforms the\nbest reported results, and using the A VT-b backbone further\nimproves performance. Notably, FHOI leverages attention\non hand trajectories to obtain strong performance, which, as\nwe see in Figure 1, emerges spontaneously in our model.\n50-Salads. Finally, we show that our approach is not lim-\nLosses Backbones\nSetting Lcls LfeatTSN A VT-b\nnaive [n] - - 10.1 13.1\n‚úì - 11.5 14.4\n- ‚úì 13.7 13.0\nanticipative [a]‚úì ‚úì 13.6 14.4\nTable 7: Anticipative training.\nEmploying the anticipative train-\ning losses are imperative to obtain\nstrong performance with A VT.\nReported on EK100/cm recall@5.\n2 4 6 8 10\nObserved segment ( o) (seconds)\n10\n11\n12\n13\n14\n15Class Mean Recall @ 5\nTSN [a]\nTSN [n]\nAVT-b [a]\nAVT-b [n]\nFigure 4: Temporal con-\ntext. A VT effectively lever-\nages longer temporal context,\nespecially in the [a] setting.\nited to egocentric videos and is also effective in third-person\nsettings. In Table 6, we report top-1 performance on 50-\nSalads averaged over standard 5 splits. We observe it out-\nperforms previous RNN [2] and attention [77] based ap-\nproaches by a significant 7.3% absolute improvement, again\nestablishing a new state-of-the-art.\n5.3. Ablations and Analysis\nWe now analyze the A VT architecture, using the RGB\nmodality and EK100 validation set as the test bed.\nAnticipative losses. In Table 7, we evaluate the contribu-\ntion of the two intermediate prediction losses that leverage\nthe causal structure of A VT. We find using those objectives\nleads to significant improvements for both backbones. We\nfind Lcls is more effective for TSN, and Lfeat for A VT-b.\nGiven that both combined work well in both settings, we\nuse both for all experiments. Note that the naive setting\nalso serves as a baseline with A VT-b backbone followed by\nsimple aggregation on top, and shows our proposed losses\nencouraging the predictive structure are imperative to ob-\ntain strong performance. We analyze per-class gains in Ap-\npendix D.1 and find classes like ‚Äòcook‚Äô, which require un-\nderstanding the sequence of actions so far to anticipate well,\nobtain the largest gains in the anticipative setting.\nTemporal context. Next, we analyze the effect of tem-\nporal context. In Figure 4, we train and test the model\nwith different lengths of temporal context, œÑo. We no-\ntice that the performance improves as we incorporate more\nframes of context, with more consistent gains for A VT-b.\nThe gains are especially pronounced when trained using the\nanticipative setting ( 11.2 ‚Üí 14.9 = 3.5 ‚Üë) vs. the naive\n(11.0 ‚Üí 13.1 = 2.1 ‚Üë). This suggests end-to-end trained\nA VT using anticipative losses is better suited at modeling\nsequences of long-range temporal interactions.\nAttention visualization. To better understand how A VT\nmodels videos, we visualize the learned attention in the\nbackbone and head. For the backbone, following prior\nwork [18], we use attention rollout [1] to aggregate atten-\ntion over heads and layers. For the head, since our causal\nmodeling would bias aggregated attention towards the first\nTurn-on\nTap\nWash\nKnife\nWash\nKnife\nWash\nSpoon \n<latexit sha1_base64=\"NxO9TjPSxC8jlwY7HwtqtoScTAk=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1uygSbnvlil/15yCrJMhJBXLUe+Wvbl+zLOEKmaTWdgI/xXBCDQom+bTUzSxPKRvRAe84qqhbEk7m107JmVP6JNbGlUIyV39PTGhi7TiJXGdCcWiXvZn4n9fJML4JJ0KlGXLFFoviTBLUZPY66QvDGcqxI5QZ4W4lbEgNZegCKrkQguWXV0nzohpcVf37y0rNz+MowgmcwjkEcA01uIM6NIDBIzzDK7x52nvx3r2PRWvBy2eO4Q+8zx+yE48m</latexit>\n‚Üí\n<latexit sha1_base64=\"NxO9TjPSxC8jlwY7HwtqtoScTAk=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1uygSbnvlil/15yCrJMhJBXLUe+Wvbl+zLOEKmaTWdgI/xXBCDQom+bTUzSxPKRvRAe84qqhbEk7m107JmVP6JNbGlUIyV39PTGhi7TiJXGdCcWiXvZn4n9fJML4JJ0KlGXLFFoviTBLUZPY66QvDGcqxI5QZ4W4lbEgNZegCKrkQguWXV0nzohpcVf37y0rNz+MowgmcwjkEcA01uIM6NIDBIzzDK7x52nvx3r2PRWvBy2eO4Q+8zx+yE48m</latexit>\n‚Üí 4\nWash\nHand \n<latexit sha1_base64=\"NxO9TjPSxC8jlwY7HwtqtoScTAk=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1uygSbnvlil/15yCrJMhJBXLUe+Wvbl+zLOEKmaTWdgI/xXBCDQom+bTUzSxPKRvRAe84qqhbEk7m107JmVP6JNbGlUIyV39PTGhi7TiJXGdCcWiXvZn4n9fJML4JJ0KlGXLFFoviTBLUZPY66QvDGcqxI5QZ4W4lbEgNZegCKrkQguWXV0nzohpcVf37y0rNz+MowgmcwjkEcA01uIM6NIDBIzzDK7x52nvx3r2PRWvBy2eO4Q+8zx+yE48m</latexit>\n‚Üí\n<latexit sha1_base64=\"NxO9TjPSxC8jlwY7HwtqtoScTAk=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1uygSbnvlil/15yCrJMhJBXLUe+Wvbl+zLOEKmaTWdgI/xXBCDQom+bTUzSxPKRvRAe84qqhbEk7m107JmVP6JNbGlUIyV39PTGhi7TiJXGdCcWiXvZn4n9fJML4JJ0KlGXLFFoviTBLUZPY66QvDGcqxI5QZ4W4lbEgNZegCKrkQguWXV0nzohpcVf37y0rNz+MowgmcwjkEcA01uIM6NIDBIzzDK7x52nvx3r2PRWvBy2eO4Q+8zx+yE48m</latexit>\n‚Üí 1\nDry \nHand \n<latexit sha1_base64=\"NxO9TjPSxC8jlwY7HwtqtoScTAk=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1uygSbnvlil/15yCrJMhJBXLUe+Wvbl+zLOEKmaTWdgI/xXBCDQom+bTUzSxPKRvRAe84qqhbEk7m107JmVP6JNbGlUIyV39PTGhi7TiJXGdCcWiXvZn4n9fJML4JJ0KlGXLFFoviTBLUZPY66QvDGcqxI5QZ4W4lbEgNZegCKrkQguWXV0nzohpcVf37y0rNz+MowgmcwjkEcA01uIM6NIDBIzzDK7x52nvx3r2PRWvBy2eO4Q+8zx+yE48m</latexit>\n‚Üí\n<latexit sha1_base64=\"NxO9TjPSxC8jlwY7HwtqtoScTAk=\">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoseAF48RzAOSJcxOZpMxszPLTK8QQv7BiwdFvPo/3vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0O/NbT9xYodUDjlMeJnSgRCwYRSc1uygSbnvlil/15yCrJMhJBXLUe+Wvbl+zLOEKmaTWdgI/xXBCDQom+bTUzSxPKRvRAe84qqhbEk7m107JmVP6JNbGlUIyV39PTGhi7TiJXGdCcWiXvZn4n9fJML4JJ0KlGXLFFoviTBLUZPY66QvDGcqxI5QZ4W4lbEgNZegCKrkQguWXV0nzohpcVf37y0rNz+MowgmcwjkEcA01uIM6NIDBIzzDK7x52nvx3r2PRWvBy2eO4Q+8zx+yE48m</latexit>\n‚Üí 2\ntime\nFigure 5: Long-term anticipation.A VT can also be used to pre-\ndict further into the future by rolling out predictions autoregres-\nsively. The text on top represents the next action predicted at pro-\nvided frames, followed by subsequently predicted actions, with the\nnumber representing how long that action would repeat.\nfew frames, we visualize the last layer attention averaged\nover heads. As shown in Figure 1, the model spontaneously\nlearns to attend to hands and objects, which has been found\nbeneficial for egocentric anticipation tasks [57]‚Äîbut re-\nquired manual designation in prior work. The temporal at-\ntention also varies between focusing on the past or mostly\non the current frame depending on the predicted future ac-\ntion. We show additional results in Appendix D.2.\nLong-term anticipation.So far we have shown A VT‚Äôs ap-\nplicability in the next-action anticipation task. Thanks to\nA VT‚Äôs predictive nature, it can also be rolled out autore-\ngressively to predict a sequence of future actions given the\nvideo context. We append the predicted feature and run the\nmodel on the resulting sequence, reusing features computed\nfor past frames. As shown in Figure 5, A VT makes reason-\nable future predictions‚Äî‚Äòwash spoon‚Äô after ‚Äòwash knife‚Äô,\nfollowed by ‚Äòwash hand‚Äô and ‚Äòdry hand‚Äô‚Äîindicating the\nmodel has started to learn certain ‚Äòaction schemas‚Äô [68],\na core capability of our causal attention and anticipative\ntraining architecture. We show additional results in Ap-\npendix D.3.\n6. Conclusion and Future Work\nWe presented A VT, an end-to-end attention-based archi-\ntecture for anticipative video modeling. Through extensive\nexperimentation on four popular benchmarks, we show its\napplicability in anticipating future actions, obtaining state-\nof-the-art results and demonstrating the importance of its\nanticipative training objectives. We believe A VT would be a\nstrong candidate for tasks beyond anticipation, such as self-\nsupervised learning [37, 90], discovering action schemas\nand boundaries [68, 79], and even for general action recog-\nnition in tasks that require modeling temporal ordering [34].\nWe plan to explore these directions in future work.\nAcknowledgements: Authors would like to thank An-\ntonino Furnari, Fadime Sener and Miao Liu for help with\nprior work; Naman Goyal and Myle Ott for help with lan-\nguage models; and Tushar Nagarajan, Gedas Bertasius and\nLaurens van der Maaten for feedback on the manuscript.\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying attention\nflow in transformers. In ACL, 2020.\n[2] Yazan Abu Farha, Alexander Richard, and Juergen Gall.\nWhen will you do what?-anticipating temporal occurrences\nof activities. In CVPR, 2018.\n[3] Relja Arandjelovic and Andrew Zisserman. Look, listen\nand learn. In ICCV, 2017.\n[4] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu Àáci¬¥c, and Cordelia Schmid. ViVit: A video\nvision transformer. arXiv preprint arXiv:2103.15691, 2021.\n[5] Gedas Bertasius and Lorenzo Torresani. Classifying, seg-\nmenting, and tracking object instances in video with mask\npropagation. In CVPR, 2020.\n[6] Gedas Bertasius and Lorenzo Torresani. Cobe: Contextual-\nized object embeddings from narrated instructional video.\nIn NeurIPS, 2020.\n[7] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\nIn ICML, 2021.\n[8] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners. In NeurIPS, 2020.\n[9] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-\nlocal algorithm for image denoising. In CVPR, 2005.\n[10] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation\nnetworks and beyond. In ICCV Workshop, 2019.\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In ECCV,\n2020.\n[12] Joao Carreira and Andrew Zisserman. Quo Vadis, Action\nRecognition? A New Model and the Kinetics Dataset. In\nCVPR, 2017.\n[13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nSanja Fidler, Antonino Furnari, Evangelos Kazakos, Da-\nvide Moltisanti, Jonathan Munro, Toby Perrett, Will Price,\nand Michael Wray. Scaling egocentric vision: The epic-\nkitchens dataset. In ECCV, 2018.\n[14] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nAntonino Furnari, Evangelos Kazakos, Jian Ma, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and\nMichael Wray. Rescaling egocentric vision. arXiv preprint\narXiv:2006.13256, 2020.\n[15] Roeland De Geest and Tinne Tuytelaars. Modeling tempo-\nral structure with lstm for online action detection. InWACV,\n2018.\n[16] Eadom Dessalene, Michael Maynord, Chinmaya Devaraj,\nCornelia Fermuller, and Yiannis Aloimonos. Forecast-\ning action through contact representations from first person\nvideo. TPAMI, 2021.\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In NAACL,\n2019.\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale. In ICLR, 2021.\n[19] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichten-\nhofer. Multiscale vision transformers. In ICCV, 2021.\n[20] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nICCV, 2019.\n[21] Basura Fernando, Hakan Bilen, Efstratios Gavves, and\nStephen Gould. Self-supervised video representation learn-\ning with odd-one-out networks. In CVPR, 2017.\n[22] Antonino Furnari, Sebastiano Battiato, and Giovanni\nMaria Farinella. Leveraging uncertainty to rethink loss\nfunctions and evaluation measures for egocentric action an-\nticipation. In ECCV Workshop, 2018.\n[23] Antonino Furnari and Giovanni Maria Farinella. What\nwould you expect? anticipating egocentric actions with\nrolling-unrolling lstms and modality attention. In ICCV,\n2019.\n[24] Antonino Furnari and Giovanni Maria Farinella. Rolling-\nunrolling lstms for action anticipation from first-person\nvideo. TPAMI, 2020.\n[25] Harshala Gammulle, Simon Denman, Sridha Sridharan,\nand Clinton Fookes. Predicting the future: A jointly learnt\nmodel for action anticipation. In ICCV, 2019.\n[26] Jiyang Gao, Zhenheng Yang, and Ram Nevatia. Red: Re-\ninforced encoder-decoder networks for action anticipation.\nIn BMVC, 2017.\n[27] Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-\nscale weakly-supervised pre-training for video action\nrecognition. In CVPR, 2019.\n[28] Rohit Girdhar, Jo Àúao Carreira, Carl Doersch, and Andrew\nZisserman. Video Action Transformer Network. In CVPR,\n2019.\n[29] Rohit Girdhar and Kristen Grauman. Anticipative Video\nTransformer @ EPIC-Kitchens Action Anticipation Chal-\nlenge 2021. In CVPR Workshop, 2021.\n[30] Rohit Girdhar and Deva Ramanan. Attentional pooling for\naction recognition. In NeurIPS, 2017.\n[31] Rohit Girdhar and Deva Ramanan. CATER: A diagnostic\ndataset for Compositional Actions and TEmporal Reason-\ning. In ICLR, 2020.\n[32] Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef\nSivic, and Bryan Russell. ActionVLAD: Learning spatio-\ntemporal aggregation for action classification. In CVPR,\n2017.\n[33] Priya Goyal, Piotr Doll ¬¥ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large mini-\nbatch sgd: training imagenet in 1 hour. arXiv preprint\narXiv:1706.02677, 2017.\n[34] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fr ¬®und, Peter Yianilos, Moritz\nMueller-Freitag, Florian Hoppe, Christian Thurau, Ingo\nBax, and Roland Memisevic. The ‚Äúsomething something‚Äù\nvideo database for learning and evaluating visual common\nsense. In ICCV, 2017.\n[35] Xiao Gu, Jianing Qiu, Yao Guo, Benny Lo, and Guang-\nZhong Yang. Transaction: Icl-sjtu submission to epic-\nkitchens action anticipation challenge 2021. In CVPR\nWorkshop, 2021.\n[36] Tengda Han, Weidi Xie, and Andrew Zisserman. Video rep-\nresentation learning by dense predictive coding. In ICCV\nWorkshop, 2019.\n[37] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-\naugmented dense predictive coding for video representation\nlearning. In ECCV, 2020.\n[38] De-An Huang and Kris M Kitani. Action-reaction: Fore-\ncasting the dynamics of human interaction. InECCV, 2014.\n[39] Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh, and\nAshutosh Saxena. Recurrent neural networks for driver ac-\ntivity anticipation via sensory-fusion architecture. In ICRA,\n2016.\n[40] Dinesh Jayaraman and Kristen Grauman. Learning image\nrepresentations tied to ego-motion. In ICCV, 2015.\n[41] Dinesh Jayaraman and Kristen Grauman. Slow and steady\nfeature analysis: higher order temporal coherence in video.\nIn CVPR, 2016.\n[42] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas\nLeung, Rahul Sukthankar, and Li Fei-Fei. Large-scale\nvideo classification with convolutional neural networks. In\nCVPR, 2014.\n[43] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,\nand Andrew Zisserman. The kinetics human action video\ndataset. arXiv preprint arXiv:1705.06950, 2017.\n[44] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-\nsupervised video representation learning with space-time\ncubic puzzles. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 33, pages 8545‚Äì8552, 2019.\n[45] Kris M Kitani, Brian D Ziebart, James Andrew Bagnell,\nand Martial Hebert. Activity forecasting. In ECCV, 2012.\n[46] Shu Kong and Charless Fowlkes. Low-rank bilinear pooling\nfor fine-grained classification. In CVPR, 2017.\n[47] Hema S Koppula and Ashutosh Saxena. Anticipating hu-\nman activities using object affordances for reactive robotic\nresponse. TPAMI, 2015.\n[48] Bruno Korbar, Du Tran, and Lorenzo Torresani. Co-\noperative learning of audio and video models from self-\nsupervised synchronization. In NeurIPS, 2018.\n[49] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language\nof actions: Recovering the syntax and semantics of goal-\ndirected human activities. In CVPR, 2014.\n[50] Hilde Kuehne, Hueihan Jhuang, Est ¬¥ƒ±baliz Garrote, Tomaso\nPoggio, and Thomas Serre. HMDB: a large video database\nfor human motion recognition. In ICCV, 2011.\n[51] Guillaume Lample and Alexis Conneau. Cross-lingual lan-\nguage model pretraining. In NeurIPS, 2019.\n[52] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: Denois-\ning sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In ACL, 2020.\n[53] Xinyu Li, Bing Shuai, and Joseph Tighe. Directional tem-\nporal modeling for action recognition. In ECCV, 2020.\n[54] Xinyu Li, Yanyi Zhang, Chunhui Liu, Bing Shuai, Yi Zhu,\nBiagio Brattoli, Hao Chen, Ivan Marsic, and Joseph Tighe.\nVidTr: Video transformer without convolutions. arXiv\npreprint arXiv:2104.11746, 2021.\n[55] Yin Li, Miao Liu, and James M Rehg. In the eye of be-\nholder: Joint learning of gaze and actions in first person\nvideo. In ECCV, 2018.\n[56] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and\nThomas S Huang. Non-local recurrent network for image\nrestoration. In NeurIPS, 2019.\n[57] Miao Liu, Siyu Tang, Yin Li, and James Rehg. Forecasting\nhuman object interaction: Joint prediction of motor atten-\ntion and actions in first person video. In ECCV, 2020.\n[58] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao.\nFuture frame prediction for anomaly detection‚Äìa new base-\nline. In CVPR, 2018.\n[59] Xiang Long, Chuang Gan, Gerard de Melo, Jiajun Wu,\nXiao Liu, and Shilei Wen. Attention clusters: Purely atten-\ntion based local feature integration for video classification.\nIn CVPR, 2018.\n[60] Pauline Luc, Camille Couprie, Yann Lecun, and Jakob Ver-\nbeek. Predicting future instance segmentation by forecast-\ning convolutional features. In ECCV, 2018.\n[61] Shugao Ma, Leonid Sigal, and Stan Sclaroff. Learning ac-\ntivity progression in lstms for activity detection and early\ndetection. In CVPR, 2016.\n[62] Antoine Miech, Ivan Laptev, and Josef Sivic. Learnable\npooling with context gating for video classification. In\nCVPR Workshop, 2017.\n[63] Antoine Miech, Ivan Laptev, Josef Sivic, Heng Wang,\nLorenzo Torresani, and Du Tran. Leveraging the present to\nanticipate the future in videos. In CVPR Workshop, 2019.\n[64] Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer,\nand Kristen Grauman. EGO-TOPO: Environment affor-\ndances from egocentric video. In CVPR, 2020.\n[65] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan As-\nselmann. Video transformer network. arXiv preprint\narXiv:2102.00719, 2021.\n[66] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018.\n[67] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. In\nNAACL, 2018.\n[68] Jean Piaget. La naissance de l‚Äôintelligence chez l‚Äôenfant ,\npage 216. 1935.\n[69] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by genera-\ntive pre-training. 2018.\n[70] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsu-\npervised multitask learners. 2019.\n[71] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. JMLR, 2020.\n[72] Shaoqing Ren, Kaiming He, Ross B Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015.\n[73] Nicholas Rhinehart and Kris M Kitani. First-person activity\nforecasting with online inverse reinforcement learning. In\nICCV, 2017.\n[74] Alexander Richard, Hilde Kuehne, and Juergen Gall.\nWeakly supervised action learning with rnn based fine-to-\ncoarse modeling. In CVPR, 2017.\n[75] Cristian Rodriguez, Basura Fernando, and Hongdong Li.\nAction anticipation by predicting future dynamic images.\nIn ECCV Workshop, 2018.\n[76] Fadime Sener, Dibyadip Chatterjee, and Angela Yao. Tech-\nnical report: Temporal aggregate representations. arXiv\npreprint arXiv:2106.03152, 2021.\n[77] Fadime Sener, Dipika Singhania, and Angela Yao. Tem-\nporal aggregate representations for long-range video under-\nstanding. In ECCV, 2020.\n[78] Yuge Shi, Basura Fernando, and Richard Hartley. Action\nanticipation with rbf kernelized feature mapping rnn. In\nECCV, 2018.\n[79] Mike Zheng Shou, Deepti Ghadiyaram, Weiyao Wang,\nand Matt Feiszli. Generic event boundary detection:\nA benchmark for event segmentation. arXiv preprint\narXiv:2101.10511, 2021.\n[80] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos. In\nNeurIPS, 2014.\n[81] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUCF101: A dataset of 101 human actions classes from\nvideos in the wild. CRCV-TR-12-01, 2012.\n[82] Sebastian Stein and Stephen J McKenna. Combining em-\nbedded accelerometers with computer vision for recogniz-\ning food preparation activities. In UbiComp, 2013.\n[83] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia\nSchmid. Contrastive bidirectional transformer for temporal\nrepresentation learning. arXiv preprint arXiv:1906.05743,\n2019.\n[84] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy,\nand Cordelia Schmid. VideoBERT: A joint model for video\nand language representation learning. In ICCV, 2019.\n[85] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv¬¥e J¬¥egou. Training\ndata-efficient image transformers and distillation through\nattention. In ICML, 2021.\n[86] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torre-\nsani, and Manohar Paluri. Learning spatiotemporal features\nwith 3d convolutional networks. In ICCV, 2015.\n[87] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli.\nVideo classification with channel-separated convolutional\nnetworks. In ICCV, 2019.\n[88] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotempo-\nral convolutions for action recognition. In CVPR, 2018.\n[89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017.\n[90] Carl V ondrick, Hamed Pirsiavash, and Antonio Torralba.\nAnticipating visual representations from unlabeled video.\nIn CVPR, 2016.\n[91] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recogni-\ntion. In ECCV, 2016.\n[92] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang\nLi, Derek F Wong, and Lidia S Chao. Learning deep trans-\nformer models for machine translation. In ACL, 2019.\n[93] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018.\n[94] Donglai Wei, Joseph J Lim, Andrew Zisserman, and\nWilliam T Freeman. Learning and using the arrow of time.\nIn CVPR, 2018.\n[95] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan,\nKaiming He, Philipp Krahenbuhl, and Ross Girshick.\nLong-term feature banks for detailed video understanding.\nIn CVPR, 2019.\n[96] Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, and Fei Wu.\nLearning to anticipate egocentric actions by imagination.\nTIP, 2021.\n[97] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy. Rethinking spatiotemporal feature learning\nfor video understanding. In ECCV, 2018.\n[98] Yutaro Yamamuro, Kazuki Hanazawa, Masahiro Shida,\nTsuyoshi Kodake, Shinji Takenaka, Yuji Sato, and Takeshi\nFujimatsu. Submission to epic-kitchens action anticipation\nchallenge 2021. In CVPR Workshop, 2021.\n[99] Ceyuan Yang, Yinghao Xu, Bo Dai, and Bolei Zhou. Video\nrepresentation learning with visual tempo consistency. In\narXiv preprint arXiv:2006.15489, 2020.\n[100] Olga Zatsarynna, Yazan Abu Farha, and Juergen Gall.\nMulti-modal temporal convolutional network for anticipat-\ning actions in egocentric videos. In CVPR Workshop, 2021.\n[101] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer. In ICCV, 2021.\nA. Dataset and Metrics\nWe test on four datasets as described in the main pa-\nper. EpicKitchens-100 (EK100) [14] is the largest egocen-\ntric (first-person) video dataset with 700 long unscripted\nvideos of cooking activities totalling 100 hours. It con-\ntains 89,977 segments labeled with one of 97 verbs, 300\nnouns, and 3807 verb-noun combinations (or ‚Äúactions‚Äù),\nand uses œÑa=1s. The dataset is split in 75:10:15 ratio into\ntrain/val/test sets, and the test set evaluation requires sub-\nmission to the CVPR‚Äô21 challenge server. The evaluation\nmetric used is class-mean recall@5 [22], which evaluates if\nthe correct future class is within the top-5 predictions, and\nequally weights all classes by averaging the performance\ncomputed individually per class. The top-5 criterion also\ntakes into account the multi-modality in the future predic-\ntions. Entries are ranked according to performance on ac-\ntions.\nEpicKitchens-55 (EK55) [13] is an earlier version of the\nEK100, with 39,596 segments labeled with 125 verbs, 352\nnouns, and 2,513 combinations (actions), totalling 55 hours,\nand œÑa = 1s. We use the standard splits and metrics\nfrom [24]. For anticipation, [24] splits the public train-\ning set into 23,493 training and 4,979 validation segments\nfrom 232 and 40 videos respectively. The test evaluation\nis similarly performed on the challenge server. The evalua-\ntion metrics used are top-1/top-5 accuracies and class-mean\nrecall@5 over verb/noun/action predictions at anticipation\ntime œÑa = 1s. Unlike EK100, the recall computation on\nEK55 is done over a subset of ‚Äòmany-shot‚Äô classes as de-\nfined in [23]. While EK55 is a subset of EK100, we use it\nto compare to a larger set of baselines, which have not yet\nbeen reported on EK100.\nPrior work on the EK datasets [23, 77] operate on fea-\ntures from pre-trained models, specifically RGB features\nextracted using a TSN [91] architecture trained for action\nclassification on the train set; Flow features using a TSN\ntrained on optical flow; and OBJ features from a Faster R-\nCNN, whose output is converted into a vector depicting the\ndistribution over object classes for that frame. We refer the\nreader to [23] for details. We use these features for some ex-\nperiments in the paper that use fixed backbones (eg TSN).\nWe use the features as provided in the code release by [23].7\nEGTEA Gaze+ [55] is another popular egocentric ac-\ntion anticipation dataset, consisting of 10,325 action anno-\ntations with 106 unique actions. To be comparable to prior\nwork [57], we report performance on the split 1 [55] of the\ndataset at œÑa = 0.5s using overall top-1 accuracy and mean\nover top-1 class accuracies (class mean accuracy).\nFinally, we also experiment with a popular third-person\naction anticipation dataset: 50-Salads (50S) [82]. It con-\ntains fifty 40s long videos, with 900 segments labeled with\n7https://github.com/fpv-iplab/rulstm\nVerb Noun Action\nMethod Top-1 Top-5 Top-1 Top-5 Top-1 Top-5\nDMR [90] - 73.7 - 30.0 - 16.9\nATSN [13] - 77.3 - 39.9 - 16.3\nED [26] - 75.5 - 43.0 - 25.8\nMCE [22] - 73.4 - 38.9 - 26.1\nFN [15] - 74.8 - 40.9 - 26.3\nRL [61] - 76.8 - 44.5 - 29.6\nEL [39] - 75.7 - 43.7 - 28.6\nFHOI (I3D) [57] 30.7 76.5 17.4 42.6 10.4 25.5\nRULSTM [23, 24] 32.4 79.6 23.5 51.8 15.3 35.3\nImagineRNN [96] - - - - - 35.6\nActionBanks [77] 35.8 80.0 23.4 52.8 15.1 35.6\nA VT+ 32.5 79.9 24.4 54.0 16.6 37.6\nTable 8: EK55 (val) resultsreported in top-1/5 (%) at œÑa = 1.0s.\nThe final late-fused model outperforms all prior work.\none of 17 action classes. We report top-1 accuracy aver-\naged over the pre-defined 5 splits for an anticipation time\nœÑa = 1s, following prior work [2, 77].\nB. Baselines Details\nRULSTM leverages a ‚Äòrolling‚Äô LSTM to encode the past,\nand an ‚Äòunrolling‚Äô LSTM to predict the future, from differ-\nent points in the past. It was ranked first in the EK55 chal-\nlenge in 2019, and is currently the best reported method\non EK100. ActionBanks [77] improves over RULSTM\nthrough a carefully designed architecture leveraging non-\nlocal [93] and long-term feature aggregation [95] blocks\nover different lengths of past features, and was one of\nthe winners of the CVPR‚Äô20 EK55 anticipation challenge.\nForecasting HOI [57] takes an alternate approach, leverag-\ning latest spatio-temporal convnets [87] jointly with hand\nmotion and interaction hotspot prediction.\nC. EpicKitchens-55 Full Results\nWe use the irCSN-152 backbone for comparisons to\nstate-of-the-art on EK55, as that performed the best in Ta-\nble 4 on top-1, the primary metric used in EK55 leader-\nboards. For comparisons using all modalities, in Table 8, we\nlate fuse our best model with the other modalities from [77]\n(resulting model referred to as A VT+). We outperform all\nreported work on the validation set. Finally in Table 9\nwe train our model on train+val, late fuse other modalities\nfrom [77], and evaluate on the test sets on the challenge\nserver. Here as well we outperform all prior work. Note that\nour models are only trained for action prediction, and indi-\nvidual verb/noun predictions are obtained by marginalizing\nover the other. We outperform all prior work on on seen test\nset (S1), and are only second to concurrent work [16] on\nSeen test set (S1) Unseen test set (S2)\nTop-1 Accuracy% Top-5 Accuracy% Top-1 Accuracy% Top-5 Accuracy%\nVerb Noun Act. Verb Noun Act. Verb Noun Act. Verb Noun Act.\n2SCNN [13] 29.76 15.15 4.32 76.03 38.56 15.21 25.23 9.97 2.29 68.66 27.38 9.35\nATSN [13] 31.81 16.22 6.00 76.56 42.15 28.21 25.30 10.41 2.39 68.32 29.50 6.63\nED [26] 29.35 16.07 8.08 74.49 38.83 18.19 22.52 7.81 2.65 62.65 21.42 7.57\nMCE [22] 27.92 16.09 10.76 73.59 39.32 25.28 21.27 9.90 5.57 63.33 25.50 15.71\nP+D [63] 30.70 16.50 9.70 76.20 42.70 25.40 28.40 12.40 7.20 69.80 32.20 19.30\nRULSTM [23, 24] 33.04 22.78 14.39 79.55 50.95 33.73 27.01 15.19 8.16 69.55 34.38 21.10\nActionBanks [77] 37.87 24.10 16.64 79.74 53.98 36.06 29.50 16.52 10.04 70.13 37.83 23.42\nFHOI [57] 34.99 20.86 14.04 77.05 46.45 31.29 28.27 14.07 8.64 70.67 34.35 22.91\nFHOI+obj [57] 36.25 23.83 15.42 79.15 51.98 34.29 29.87 16.80 9.94 71.77 38.96 23.69\nImagineRNN [96] 35.44 22.79 14.66 79.72 52.09 34.98 29.33 15.50 9.25 70.67 35.78 22.19\nEgo-OMG [16] 32.20 24.90 16.02 77.42 50.24 34.53 27.42 17.65 11.81 68.59 37.93 23.76\nA VT+ 34.36 20.16 16.84 80.03 51.57 36.52 30.66 15.64 10.41 72.17 40.76 24.27\nTable 9: EK55 test setresults obtained from the challenge server. A VT outperforms all published work on this dataset on top-5 metric,\nand is only second to [16] on S2 on top-1. Note that [16] leverages transductive learning (using the test set for initial graph representation\nlearning), whereas A VT only uses the train set.\nLosses Backbones\nSetting Lcls Lfeat IG65M A VT-b\nnaive [n] - - 31.4 25.9\n‚úì - 31.4 28.8\n- ‚úì 31.7 23.9\nanticipative [a] ‚úì ‚úì 31.7 30.1\nTable 10: Anticipative training on EK55.Employing the antici-\npative training losses are imperative to obtain strong performance\nwith A VT; similar to as seen in Table 7.\nunseen (S2) for top-1 actions. It is worth noting that [16]\nuses transductive learning, leveraging the test set. A VT is\nalso capable of similarly leveraging the test data with unsu-\npervised objectives (Lfeat ), which could potentially further\nimprove in performance. We leave that exploration to future\nwork. In Table 10, we analyze the effect of different losses\non the final performance on EK55, similar to the analysis\non EK100 in Table 7. We see a similar trend: using the\nanticipative setting performs the best.\nD. Analysis\nD.1. Per-class Gains\nTo better understand the source of these gains, we ana-\nlyze the class-level gains with anticipative training in Fig-\nure 6. We notice certain verb classes show particularly large\ngains across the backbones, such as ‚Äòcook‚Äô and ‚Äòchoose‚Äô.\nWe posit that is because predicting the person will cook an\nitem would often require understanding the sequence of ac-\nmeasure adjust fold unwrap empty soak scrub serve choose cook\nClasses\n0\n50Recall @ 5\nmethod\nbase\nanticipative\nFigure 6: Verb classes that gain the most with causal modeling,\naveraged over the TSN and A VT-b backbones. Actions such as\n‚Äòcook‚Äô and ‚Äòchoose‚Äô show particularly significant gains.\ntions so far, such as preparing ingredients, turning on the\nstove etc., which the anticipative training setting encour-\nages.\nD.2. Attention Visualizations\nIn Figure 8 we show additional visualizations of the spa-\ntial and temporal attention, similar to Figure 1. We also\nshow failure cases, which often involve temporal inaccuracy\n(i.e. when the model anticipates an action too soon or too\nlate) and object recognition errors (predicting ‚Äòwash spoon‚Äô\ninstead of ‚Äòwash fork‚Äô). We also provide attached videos\nto visualize predicted future classes along with the ground\ntruth (GT) future prediction in a video form for EK100 and\nEGTEA Gaze+, at each time step (as opposed to only 2\nshown in these figures).\nD.3. Long-term Anticipation\nIn Figure 9 we show additional visualizations of the\nlong-term anticipation, similar to Figure 5.\n0.5 1.0 1.5 2.0\nWeight on the feat\n12.0\n12.5\n13.0\n13.5\n14.0\n14.5\n15.0Class Mean Recall @ 5\nTSN w/ L2\nTSN w/ NCE\nAVT-b w/ L2\nAVT-b w/ NCE\nFigure 7: Different Lfeat functions and weights. We found\nsimilar or better performance of the simpler L2 metric over NCE\nand use it for all experiments in the paper. The graph here shows\nperformance on EK100 (validation, RGB) at œÑa = 1s, at different\nscalar weights used on this loss during optimization.\nD.4. Lfeat Formulation\nIn Figure 7 we show the performance of A VT with both\nA VT-b and TSN backbones, using two different loss func-\ntions for Lfeat : L2 as used in paper, and InfoNCE [66] ob-\njective as in some recent work [37, 96], at different weights\nused on that loss during training. We find thatL2 is as effec-\ntive or better for both backbones, and hence we use it with\nweight=1.0 for all experiments. While further hyperparam-\neter tuning can potentially lead to further improvements for\nInfoNCE as observed in some concurrent work [96], we\nleave that exploration to future work.\nD.5. Computational complexity\nThe only additional compute in anticipative training as\nopposed to naive is for applying the linear layer to classify\npast frame features for Lcls, since Lfeat simply matches\npast features, which anyway need to be computed for self\nattention to predict the next action. We found GPU memory\nremains nearly same, and runtime was only 1% higher than\na model that only predicts the next action. Also, this ad-\nditional processing is only for training; inference is exactly\nsame irrespective of additional losses.\nwash tomato turn-off tap\npour water turn-off tap\nclose bag open fridge\nclose box close cupboard\nFigure 8: More Qualitative Results.The spatial and temporal attention visualization in EK100, similar to Figure 1. For each input frame,\nwe visualize the effective spatial attention by A VT-b using attention rollout [1]. The red regions represent the regions of highest attention,\nwhich we find to often correspond to hands+objects in the egocentric EpicKitchens-100 videos. The text on the top show future predictions\nat 2 points in the video, along with the temporal attention (last layer of A VT-h averaged over heads) visualized using the width of the\nlines. The green color of text indicates that it matches the GT action at that future frame (or that nothing is labeled at that frame). As seen\nin Figure 1, spatial attention focuses on hands and objects. The temporal attention focuses on the last frame when predicting actions like\n‚Äòturn-off tap‚Äô, whereas more uniformly on all frames when predicting ‚Äòopen fridge‚Äô (as an action like that usually follows a sequence of\nactions involving packing up food items and moving towards the fridge).\nwash plate wash bowl\nturn-on tap put sponge\nmix pasta filter pasta\ntake glass put glass\nturn-on tap turn-off tap\nFigure 8: More Qualitative Results.(Continued) Here we also see some failure cases (the text in black‚Äîdoes not match the labeled\nground truth). Note that the predictions in those failure cases are still reasonable. For instance in the second example the model predicts\n‚Äòturn-on tap‚Äô, while the groundtruth on that frame is ‚Äòwash cloth‚Äô. As we can see in the frame that the water is running, hence the ‚Äòturn-on\ntap‚Äô does happen before the eventual labeled action of ‚Äòwash cloth‚Äô, albeit slightly sooner than when the model predicts.\nGround truth future: wash knife:4 [Not Labeled]:5 insert knife:1 wash fork:4 take cutter:pizza:1\nturn-on tap wash knife wash knife wash spoon:4 wash hand:1 dry hand:2 take knife:13\nGround truth future: [Not Labeled]:1 wash sink:6 [Not Labeled]:1 squeeze cloth:2 [Not Labeled]:2\nwash hand wash sink wash sink wash cloth:1 wash hand:15 dry hand:4\nGround truth future: take plate:1 wash plate:11 put plate:1 take spatula:1 wash spatula:6\nwash cup wash knife wash plate wash bowl:3 put bowl:17\nFigure 9: Long-term anticipation.Additional results continued from Figure 5 on EK100. On top of each frame, we show the future\nprediction at that frame (not the action that is happening in the frame, but what the model predicts will happen next). The following text\nboxes show the future predictions made by the model by rolling out autoregressively, using the predicted future feature. The number next\nto the rolled out predictions denotes for how many time steps that specific action would repeat, according to the model. For example, ‚Äòwash\nspoon: 4‚Äô means the model anticipates the ‚Äòwash spoon‚Äô action to continue for next 4 time steps. On top of the predictions we show the\nlabeled ground truth future actions. As we can observe, A VT makes reasonable future predictions, such as ‚Äòput pan‚Äô would follow ‚Äòwash\npan‚Äô; ‚Äòdry hand‚Äô would follow ‚Äòwash hand‚Äô etc. This suggests the model has picked up on action schemas [68].\nGround truth future: take sponge:1 wash pan:8 [Not Labeled]:1 wash pan:6 put pan:1\nwash pan wash pan wash pan wash pan:3 put pan:17\nGround truth future: [Not Labeled]:1 open fridge:1 insert container:1 close fridge:1 [Not Labeled]:2\nwash top wash top open fridge open fridge:2 put plate:18\nGround truth future: [Not Labeled]:1 wash knife:1 put knife:4 [Not Labeled]:2 put fork:1\nwash knife wash plate wash knife wash knife:1 wash fork:1 take plate:2 put plate:16\nFigure 9: Long-term anticipation.(Continued)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8088658452033997
    },
    {
      "name": "Gaze",
      "score": 0.6819301247596741
    },
    {
      "name": "Encoder",
      "score": 0.6785489320755005
    },
    {
      "name": "Transformer",
      "score": 0.6735543012619019
    },
    {
      "name": "Anticipation (artificial intelligence)",
      "score": 0.6398189067840576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5562114715576172
    },
    {
      "name": "Action recognition",
      "score": 0.4958864152431488
    },
    {
      "name": "Task (project management)",
      "score": 0.43716612458229065
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.43156522512435913
    },
    {
      "name": "Voltage",
      "score": 0.10443708300590515
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 4
}