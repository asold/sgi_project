{
  "title": "Reasoning with Language Model Prompting: A Survey",
  "url": "https://openalex.org/W4385569771",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4382324895",
      "name": "Shuofei Qiao",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2194922229",
      "name": "Yixin Ou",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2105016708",
      "name": "Xiang Chen",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2117713813",
      "name": "Yunzhi Yao",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099075082",
      "name": "Shumin Deng",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2475478252",
      "name": "Chuanqi Tan",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2114954316",
      "name": "Huajun Chen",
      "affiliations": [
        "Zhejiang University",
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4366328015",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4312052651",
    "https://openalex.org/W3173566921",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W4313303751",
    "https://openalex.org/W4282966307",
    "https://openalex.org/W4318239201",
    "https://openalex.org/W4221053465",
    "https://openalex.org/W3165201069",
    "https://openalex.org/W4306294746",
    "https://openalex.org/W4305028650",
    "https://openalex.org/W4312091000",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4205870266",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4320087317",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W4225080353",
    "https://openalex.org/W4281975731",
    "https://openalex.org/W4313680079",
    "https://openalex.org/W4283828387",
    "https://openalex.org/W4322718417",
    "https://openalex.org/W4285269500",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W4321855256",
    "https://openalex.org/W4310629099",
    "https://openalex.org/W2928107702",
    "https://openalex.org/W4304192721",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2012748446",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4353113046",
    "https://openalex.org/W2115845185",
    "https://openalex.org/W4327810433",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2276364082",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4312050248",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4221152409",
    "https://openalex.org/W4285105893",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W3206816211",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W3106175573",
    "https://openalex.org/W4385573855",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W3177457301",
    "https://openalex.org/W4385572162",
    "https://openalex.org/W4309135079",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W4283383808",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4221162039",
    "https://openalex.org/W4312045629",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W4306311934",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4311550910",
    "https://openalex.org/W4311730333",
    "https://openalex.org/W2105717194",
    "https://openalex.org/W4226498390",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W4312516176",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4307313205",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W4320351164",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W4285283606",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4377864545",
    "https://openalex.org/W4280633590",
    "https://openalex.org/W4311992335",
    "https://openalex.org/W4367189299",
    "https://openalex.org/W3210438755",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4285267480",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W4310509152",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W4308243058",
    "https://openalex.org/W4385574345",
    "https://openalex.org/W3106534186",
    "https://openalex.org/W4319997768",
    "https://openalex.org/W4288804529",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W4312091872",
    "https://openalex.org/W4283815582",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W1539746312",
    "https://openalex.org/W4364379393",
    "https://openalex.org/W2042492924",
    "https://openalex.org/W4285254610",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W4281557023",
    "https://openalex.org/W4226069413",
    "https://openalex.org/W4362656036",
    "https://openalex.org/W4312091039",
    "https://openalex.org/W4312107795",
    "https://openalex.org/W2931720176",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2766179782",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4385573276",
    "https://openalex.org/W4389523807",
    "https://openalex.org/W4385572965",
    "https://openalex.org/W4311432567",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4282045675",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4307325444",
    "https://openalex.org/W2102048877",
    "https://openalex.org/W4319165821",
    "https://openalex.org/W2513499049",
    "https://openalex.org/W4316135748",
    "https://openalex.org/W4318903783",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W2251349042",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4226024653",
    "https://openalex.org/W4320858367",
    "https://openalex.org/W3097683561",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W3133029875",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W2158782408",
    "https://openalex.org/W4225683910",
    "https://openalex.org/W4385567121",
    "https://openalex.org/W4293998609",
    "https://openalex.org/W4312091691",
    "https://openalex.org/W4312091845",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W4318908878",
    "https://openalex.org/W4298315692",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W4303648904",
    "https://openalex.org/W4302011175",
    "https://openalex.org/W4296343374",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W4307204159",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W4294808066",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W4306295121",
    "https://openalex.org/W4287854450",
    "https://openalex.org/W4224279587",
    "https://openalex.org/W4303649020",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4327526607",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4307937243",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4298184221"
  ],
  "abstract": "Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5368–5393\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nReasoning with Language Model Prompting: A Survey\nShuofei Qiao1∗, Yixin Ou1∗, Ningyu Zhang1†, Xiang Chen1, Yunzhi Yao1,\nShumin Deng4, Chuanqi Tan3, Fei Huang3, Huajun Chen1,2†\n1 Zhejiang University & AZFT Joint Lab for Knowledge Engine\n2 Donghai Laboratory 3 Alibaba Group 4 National University of Singapore\n{shuofei,ouyixin,zhangningyu,xiang_chen,yyztodd,huajunsir}@zju.edu.cn\nshumin@nus.edu.sg {chuanqi.tcq,f.huang}@alibaba-inc.com\nAbstract\nReasoning, as an essential ability for com-\nplex problem-solving, can provide back-end\nsupport for various real-world applications,\nsuch as medical diagnosis, negotiation, etc.\nThis paper provides a comprehensive survey\nof cutting-edge research on reasoning with lan-\nguage model prompting. We introduce research\nworks with comparisons and summaries and\nprovide systematic resources to help beginners.\nWe also discuss the potential reasons for emerg-\ning such reasoning abilities and highlight future\nresearch directions1.\n1 Introduction\nReasoning ability lies at the heart of human intel-\nligence, yet in natural language processing (NLP),\nmodern neural networks can hardly reason from\nwhat they are told or have already known (Duan\net al., 2020; Wang et al., 2021; Bhargava and Ng,\n2022). Fortunately, with the revolutionary devel-\nopment of pre-training (Brown et al., 2020; Chen\net al., 2021; Chowdhery et al., 2022), scaling up\nthe size of language models (LMs) has shown to\nconfer a range of reasoning abilities, such as arith-\nmetic (Wang et al., 2022e; Lewkowycz et al., 2022),\ncommonsense (Jung et al., 2022; Liu et al., 2022b),\nsymbolic (Zhou et al., 2023; Khot et al., 2023) rea-\nsoning. As shown in Figure 1, such abilities may\nbe unlocked by prompting strategies (Liu et al.,\n2022d) (e.g., chain-of-thought (CoT) prompting\n(Wei et al., 2022b), generated knowledge prompt-\ning (Liu et al., 2022c)), which can dramatically\nnarrow the gap between human and machine intel-\nligence. Likewise, a vast amount of work has been\nproposed in the NLP community; however, these\napproaches, scattered among various tasks, have\nnot been systematically reviewed and analyzed.\n∗ Equal Contribution.\n† Corresponding Author.\n1Resources are available at https://github.com/\nzjunlp/Prompt4ReasoningPapers (updated periodically).\nGPT-3 \nCodex \nChatGPT \nprompt \nprompt \nLet's think\nstep by step!\n? \nFigure 1: Reasoning with language model prompting.\nIn-context exemplars (colored /CIRCLE, /CIRCLE), knowledge (col-\nored /CIRCLE, /CIRCLE) or just Let’s think step by step! are as prompt\nto enhance language models reasoning.\nOrganization of This Survey: In this paper, we\nconduct the first survey of recent progress in reason-\ning with language model prompting. We first give\nsome preliminaries on this direction (§2) and then\npropose to organize relevant works by taxonomy\n(§3). We further provide in-depth comparisons with\ndiscussion for insights (§4). To facilitate beginners\nwho are interested in this field, we highlight some\nopen resources (§5) as well as potential future di-\nrections (§6).\n2 Preliminaries\nIn this section, we introduce preliminaries of rea-\nsoning with LM prompting. For standard prompt-\ning, given the reasoning question Q, prompt T and\nparameterized probabilistic model pLM, we aim to\nmaximize the likelihood of answer A as:\np(A | T,Q) =\n|A|∏\ni=1\npLM (ai | T,Q,a<i) (1)\nwhere ai and |A| denotes the i-th token and the\nlength of the final answer respectively. For few-\nshot prompting, T is comprised of K exemplars of\n(Q,A) pair. CoT approaches further add reasoning\nsteps C into prompt where T = {(Qi,Ci,Ai)}K\ni=1,\nthus Equation 1 can be reformed to:\np(A | T,Q) =p(A | T,Q,C) p(C | T,Q) (2)\n5368\nReasoning with Language Model Prompting\nTaxonomy\nof Methods\n(§3)\nStrategy Enhanced\nReasoning (§3.1)\nPrompt Engineering\n(§3.1.1)\nSingle-Stage\nContrastive (Paranjape et al., 2021), POTTER (Rajagopal et al., 2021), CoT (Wei et al., 2022b),\nZeroCoT (Kojima et al., 2022), Complexity (Fu et al., 2023b), Multilingual (Shi et al., 2022),\nAuto-CoT (Zhang et al., 2023b), Table (Chen, 2022), AlgoPrompt (Zhou et al., 2022a),\nActive-Prompt (Diao et al., 2023), Automate-CoT (Shum et al., 2023)\nMulti-Stage\niCAP (Wang et al., 2022a), SI (Creswell et al., 2022), Least-to-Most (Zhou et al., 2023),\nMAIEUTIC (Jung et al., 2022), Faithful (Creswell and Shanahan, 2022), Decomposed\n(Khot et al., 2023), Self-Ask (Press et al., 2022), Successive (Dua et al., 2022), LMLP\n(Zhang et al., 2022), LAMBADA (Kazemi et al., 2022), Iter-Decomp (Reppert et al., 2023)\nProcess Optimization\n(§3.1.2)\nSelf-Optimization Calibrator (Ye and Durrett, 2022), Human-AI (Wiegreffe et al., 2022)\nEnsemble-OptimizationSelf-C (Wang et al., 2022e), DIVERSE (Li et al., 2022d), Complexity (Fu et al., 2023b),\nSelf-V (Weng et al., 2022), MCR (Yoran et al., 2023)\nIterative-OptimizationSTaR (Zelikman et al., 2022), LMSI (Huang et al., 2022),\nReflexion (Shinn et al., 2023), Self-Refine (Madaan et al., 2023), REFINER (Paul et al., 2023)\nExternal Engine\n(§3.1.3)\nPhysical Simulator Mind’s Eye (Liu et al., 2023)\nCode Interpreter\nCOCOGEN (Madaan et al., 2022), PAL (Gao et al., 2022), PoT (Chen et al., 2022b),\nFaithful-CoT (Lyu et al., 2023), Versa-Decomp (Ye et al., 2023), SynPrompt\n(Shao et al., 2023), MathPrompter (Imani et al., 2023)\nTool Learning Toolformer (Schick et al., 2023), ART (Paranjape et al., 2023), Chameleon (Lu et al., 2023a)\nKnowledge Enhanced\nReasoning (§3.2)\nImplicit Knowledge\n(§3.2.1)\nGenKnow (Liu et al., 2022c), RAINIER (Liu et al., 2022b), MT-CoT (Li et al., 2022b), PINTO (Wang et al., 2023), TSGP\n(Sun et al., 2022), DecompDistill (Shridhar et al., 2022), Teaching (Magister et al., 2022), Fine-tune-CoT (Ho et al., 2022),\nSpecializing (Fu et al., 2023a)\nExplicit Knowledge\n(§3.2.2)\nLogicSolver (Yang et al., 2022b), V ote-k (SU et al., 2023), PROMPTPG (Lu et al., 2023b), IRCoT (Trivedi et al., 2022),\nRR (He et al., 2023)\nTaxonomy\nof Tasks\n(§5)\nArithmetic CoT (Wei et al., 2022b), Self-C (Wang et al., 2022e), Least-to-Most (Zhou et al., 2023), ZeroCoT (Kojima et al., 2022), Auto-CoT\n(Zhang et al., 2023b), LMSI (Huang et al., 2022), PAL (Gao et al., 2022), PoT (Chen et al., 2022b), Fine-tune-CoT (Ho et al., 2022)\nCommonsense CoT (Wei et al., 2022b), GenKnow (Liu et al., 2022c), Self-C (Wang et al., 2022e), Calibrator (Ye and Durrett, 2022), ZeroCoT (Kojima et al., 2022),\nAuto-CoT (Zhang et al., 2023b), COCOGEN (Madaan et al., 2022), LMSI (Huang et al., 2022), PINTO (Wang et al., 2023), RR (He et al., 2023)\nLogical Faithful (Creswell and Shanahan, 2022), LMLP (Zhang et al., 2022), Self-V (Weng et al., 2022), LAMBADA (Kazemi et al., 2022)\nSymbolic CoT (Wei et al., 2022b), Self-C (Wang et al., 2022e), Least-to-Most (Zhou et al., 2023), ZeroCoT (Kojima et al., 2022), PAL (Gao et al., 2022)\nMultimodal MarT (Zhang et al., 2023a), Multimodal-CoT (Zhang et al., 2023c), KOSMOS-1 (Huang et al., 2023), Visual-ChatGPT (Wu et al., 2023)\nFigure 2: Taxonomy of Reasoning with Language Model Prompting. (We only list representative approaches for\neach kind of task and for a more complete version, please refer to Appendix A.2).\nwhere p(C | T,Q) and p(A | T,Q,C) are defined\nas follows:\np(C | T,Q) =\n|C|∏\ni=1\npLM (ci | T,Q,c<i)\np(A | T,Q,C) =\n|A|∏\nj=1\npLM (aj | T,Q,C,a<j)\nwith ci is one step of total |C| reasoning steps.\nTo enhance the reasoning ability of LM prompt-\ning, there are two major branches of research. The\nfirst one focuses on optimizing thereasoning strat-\negy with prompting as shown in Figure 2, including\nprompt engineering (§3.1.1), process optimization\n(§3.1.2) and external engine (§3.1.3).\nFor prompt engineering (§3.1.1), many methods\ntry to improve the quality of prompt T , and we call\nthose works single-stage methods, while others ap-\npend ci into the context of(T ,Q) at each reasoning\nstage or design specific Tci for each ci, and we re-\ngard those as multi-stage methods. Note that one\nstage here refers to one input-output process. For\nprocess optimization (§3.1.2), the simplest ways\nare to bring in an optimizer with parameters θ to\ncalibrate C when generating A, and we call those\nworks self-optimization methods . Some other\nmethods try to obtain multiple processes to get\nthe final answer assembly. We regard those works\nas ensemble-optimization methods. Moreover,\nthe overall optimization process can be iteratively\nintegrated with fine-tuning the pLM on generated\ntriplet (Q,C,A), which are regarded as iterative-\noptimization methods. Besides, some works lever-\nage external reasoning engines (§3.1.3) to pro-\nduce T , to directly execute C or by implanting tool\nAPI calls in C for reasoning.\nThe second branch of research focuses onknowl-\nedge enhancement with prompting. Note that rich\nimplicit “modeledge” (Han et al., 2021) in LMs\ncan generate knowledge or rationales as knowledge-\ninformed prompt T (§3.2.1). Meanwhile, explicit\nknowledge in external resources can also be lever-\naged and retrieved as knowledgeable prompts to\nenhance reasoning (§3.2.2).\n3 Taxonomy of Methods\nIn this paper, we survey existing reasoning methods\nwith LM prompting, categorizing them as Strategy\nEnhanced Reasoning (§3.1) and Knowledge En-\nhanced Reasoning (§3.2). As shown in Figure 2,\n5369\nQ: Roger has 5 tennis balls. He buys 2 more cans  \nof tennis balls. Each can has 3 tennis balls. He has  \n5 tennis balls already. He buys 2 × 3 = 6 tennis balls.\nHow many tennis balls does he have now?\nQ: There are 3 cars in the parking lot and 2\nmore cars arrive. How many cars are in the\nparking lot? \nC: There are 3 cars in the parking lot already. 2\nmore arrive. Now there are 3 + 2 = 5 cars. \nA: The answer is 5.\nQ: There are 3 cars in the parking lot and 2\nmore cars arrive. How many cars are in the\nparking lot already? \nC: There are 3 cars in the parking lot already.\nHow many cars arrive? \nQ: There are 3 cars in the parking lot and 2 more\ncars arrive. There are 3 cars in the parking lot\nalready. 2 cars arrive. How many cars are in the\nparking lot? \nC: There  are 3 + 2 = 5 cars in the parking lot. \nA: The answer is 5.\nQ: Roger has 5 tennis balls. He buys 2 more\ncans of tennis balls. Each can has 3 tennis\nballs. How many tennis balls does he have\nalready?\nQ: Roger has 5 tennis balls. He buys 2 more\ncans of tennis balls. Each can has 3 tennis balls.\nHow many tennis balls does he have now?\nQ: Roger has 5 tennis balls. He buys 2 more\ncans of tennis balls. Each can has 3 tennis balls.\nHow many tennis balls does he have now?\nC: Roger started with 5 balls. 2 cans of 3\ntennis balls each is 6 tennis balls. 5 + 6 = 11. \nA: The answer is 11.\nLanguage Model\nC: We should first know \"How many tennis\nballs does he have already?\"\nSingle-Stage Stage-1\n...\nQ: There are 3 cars in the parking lot and 2\nmore cars arrive. How many cars are in the\nparking lot? \nC: We should first know \"How many cars are in\nthe parking lot already?\" \nC: He has 5 tennis balls already. How many\ntennis balls he buys?\nStage-2\nC: He has 5 + 6 = 11 tennis balls now.\nA: The answer is 11. \nLanguage Model Language Model Language Model\nStage-n\nFigure 3: Single-Stage (left) and Multi-Stage (right) in Prompt Engineering (§3.1.1) of Strategy Enhanced\nReasoning. In each stage, a question (Q, below the dotted line) prompted with several exemplars (above the dotted\nline) containing reasoning steps (C) will be fed into the LM. The outputs are reasoning steps and the answer (A).\nwe further refine them according to the distinctive\nfeatures of different methods.\n3.1 Strategy Enhanced Reasoning\nThe primary purpose of this line of work is to de-\nsign a better reasoning strategy, concretely embod-\nied in prompt engineering (§3.1.1), process opti-\nmization (§3.1.2) and external engine (§3.1.3).\n3.1.1 Prompt Engineering\nOne intuitive approach to improving reasoning with\nprompting is prompt engineering. As shown in\nFigure 3, we divide this sort of method into single-\nstage and multi-stage prompts based on the number\nof prompting stages.\nSingle-Stage. Early works leverage template-\nbased prompts (Paranjape et al., 2021; Rajagopal\net al., 2021) for reasoning in NLP. Regarding the\nstrong in-context learning ability of large LMs\n(Brown et al., 2020), Wei et al. (2022b) proposes\nCoT prompting, which adds a series of intermediate\nreasoning steps, into exemplars of few-shot prompt\nto induce large LMs to generate a reasoning pro-\ncess before answering. Experiments demonstrate\nthat large LMs emerge with impressive reasoning\nabilities with CoT prompting.\nIn spite of the large improvement brought by\nCoT prompting, in-context learning is greatly sen-\nsitive to the selection of exemplars, and even a tiny\nchange may cause a large drop in model perfor-\nmance (Lu et al., 2022c; Min et al., 2022; Webson\nand Pavlick, 2022). Hence, the quality of exem-\nplars appears to be particularly important. Fu et al.\n(2023b) indicates that prompts with higher reason-\ning complexity, e.g., with more reasoning steps,\ncan achieve better performance on math problems.\nZhang et al. (2023b) explores the impact of diver-\nsity of exemplars in prompt. Through clustering, it\nobtains a representative question set as a prompt.\nBy placing more explicit explanations and natu-\nral language instructions into the prompt, Zhou\net al. (2022a) relieves the ambiguity for LMs when\nfacing out-of-distribution (OOD) algorithmic prob-\nlems. The above works show that LMs can be out-\nstanding few-shot reasoners. Surprisingly, Kojima\net al. (2022) indicates that LMs are also zero-shot\nreasoners without needing extra exemplars. By\nonly concatenating \"Let’s think step by step!\", LMs\ncan consciously generate reasoning steps. Another\nmagic phenomenon is that when prompted with\n\"The person giving you this problem is Yann Le-\nCun, who is really dubious of the power of AIs\nlike you.\", GPT-4 (OpenAI, 2023) can successfully\nsolve the hard Yann LeCun’s gears problem on its\nown, which it previously failed to do.\nMulti-Stage. When humans are reasoning, it is\nusually challenging to come up with the whole rea-\nsoning process in one stroke. A more intuitive solu-\ntion is to decompose a complex problem into sim-\npler ones and to reason stage by stage. Similarly,\nthis series of works aims to transform one-stage\nprompting ( once input-output ) into multi-stage\nprompting (multi-times of input-output). Press et al.\n(2022) explicitly defines follow-up questions and\nintermediate answers in prompts to narrow the com-\npositionality gap in LMs. Jung et al. (2022) re-\ngards the output of each stage as a separate new\nquestion while Zhou et al. (2023); Wang et al.\n(2022a) append it to the whole context to prompt\nLMs. Creswell and Shanahan (2022) follows a\nstructure of Selection-Inference (Creswell et al.,\n2022) which selects specific contexts and infer-\nences based on them at each stage. Kazemi et al.\n(2022) develops a backward chaining algorithm to\ndecompose reasoning into sub-modules.\n5370\nQ: There are 3 cars in the parking lot and 2 \nmore cars arrive. How many cars are in the \nparking lot? \nC: There are 3 cars in the parking lot already.\n2 more arrive. Now there are 3 + 2 = 5 cars.\nA: The answer is 5.\n LM \nRoger started with 5\nballs. 2 cans of 3  \ntennis balls each is 6\ntennis balls. 5 + 6 = 11. \nThe answer\nis 11.\nThe answer\nis 11.\nThe answer\nis 7.\nOptimizer \nThe answer\nis 11.\nSelf-Optimization\nEnsemble-\nOptimizationIterative-Optimization\nfinetuneQ: Roger has 5 tennis balls. He buys 2\nmore cans of tennis balls. Each can has 3\ntennis balls. How many tennis balls does\nhe have now?\nRoger has 5 balls already. \nHe buys 2 × 3 = 6 more. So \nhe has 5 + 6 = 11 balls now. \nRoger has 5 tennis balls. \nHe buys 2 more cans. So \nhe has 5 + 2 = 7 now. \nFigure 4: Process Optimization (§3.1.2) of Strategy\nEnhanced Reasoning. Self-Optimization (colored /CIRCLE)\napplies an optimizer module to calibrate a single rea-\nsoning process. Ensemble-Optimization (colored /CIRCLE)\nassembles multiple reasoning processes to calibrate the\nanswer. Iterative-Optimization (colored /CIRCLE) calibrates\nreasoning processes by iteratively fine-tuning the LM.\n3.1.2 Process Optimization\nNatural language rationales2 (Ling et al., 2017a),\nalso called reasoning processes in CoT, play a vi-\ntal role in CoT prompting (Ye and Durrett, 2022;\nLampinen et al., 2022; Min et al., 2022). The\nconsistency of the reasoning process (Wang et al.,\n2022e) and the continuity between reasoning steps\n(Li et al., 2022d) both should affect the accuracy of\nfinal answers. Intuitively, as shown in Figure 4, we\nintroduce this line of methods in three types, i.e.,\nself, ensemble, and iterative optimization.\nSelf-Optimization. Self-optimization here refers\nto correcting one process by injecting extra mod-\nules. To mitigate the influence of the unreliabil-\nity of rationales, Ye and Durrett (2022) utilizes a\ncalibrator to tune the probabilities of a prediction\nbased on the score which reflects the factuality of\na rationale. During free-text rationales generation,\nWiegreffe et al. (2022) fine-tunes a sequence-to-\nsequence model as a filter to predict whether the\nrationale is acceptable.\nEnsemble-Optimization. Due to the limitation\nof only one reasoning path, the following works\nrely on ensemble calibration among multiple pro-\ncesses. Wang et al. (2022e) introduces sampling\nstrategies (Ackley et al., 1985; Fan et al., 2018)\ncommonly used in natural language generation to\nobtain multiple reasoning processes and generate\nthe most consistent answer by majority vote. Based\non the motivation of when a reasoning process\nreaches a wrong answer, not all the steps may un-\ndertake the final incorrectness, Li et al. (2022d)\nproposes a step-aware voting verifier to score each\n2Some references (Ye and Durrett, 2022; Wiegreffe et al.,\n2022; Zhou et al., 2022a) regard this as explanations.\nQ:  Roger has 5 tennis balls. He buys\n2 more cans of tennis balls. Each can\nhas 3 tennis balls. How many tennis\nballs does he have now? \nPrompt \nLM 1 1 \nT ools Simulator \nFigure 5: External Engine (§3.1.3) of Strategy Enhanced\nReasoning. External engines play the role of prompt pro-\nducer (Physical Simulator), reasoning executor (Code\nInterpreter), or tool extender (Tool Learning) in the\nprocess of reasoning.\nreasoning path. When disorientated majority pro-\ncesses overwhelm reasonable minority processes,\nthe step-aware voting verifier can alleviate the limi-\ntation of vanilla majority vote (Wang et al., 2022e).\nBesides, Wang et al. (2022d) empirically observes\nthat decoder sampling in the output space is the key\nto robustly improving performance because of the\nbrittleness of manual prompt engineering.\nIterative-Optimization. Note that LMs can\nachieve excellent performance in few-shot (Wei\net al., 2022b) or zero-shot (Kojima et al., 2022)\nmanners with prompts, another paradigm is to cali-\nbrate reasoning processes iteratively with LM fine-\ntuning. Specifically, iterative-optimization-based\nmethods try to repeat the process of prompting\nLMs to generate reasoning processes and use the\ninstances with generated reasoning processes to\nfinetune themselves. Zelikman et al. (2022) initi-\nates with a small set of exemplars to push LMs to\nproduce reasoning steps and answers themselves.\nQuestions and reasoning steps with the correct an-\nswers will be directly added to the dataset for fine-\ntuning. Incorrect ones will be fed into the model\nagain by being tagged on a hint that labels the cor-\nrect answer. Compared with Zelikman et al. (2022),\nHuang et al. (2022) does not need gold labels dur-\ning self-teaching. Following Wang et al. (2022e),\nit generates multiple reasoning processes and fine-\ntunes the most consistent self-generated answers.\nShinn et al. (2023); Madaan et al. (2023); Paul et al.\n(2023) uncover the emergent ability of LLMs to\nself-reflect, by continuously correcting reasoning\nchains through iterative self-reflection.\n3.1.3 External Engine\nWhen reasoning with LM prompting, the models\nshould have the ability of semantic understanding\n(e.g., questions) and complex reasoning (e.g., by\ngenerating reasoning processes); however, we can-\nnot have both fish and bear’s paw (Hendrycks et al.,\n2021; Nogueira et al., 2021; Lewkowycz et al.,\n5371\n2022). To tear up the obstacle, external reasoning\nengines lend a helping hand to LMs (see Figure 5).\nPhysical Simulator. Given a physical reasoning\nquestion, Liu et al. (2023) utilizes a computational\nphysics engine (Todorov et al., 2012) to simulate\nthe physical process. The simulation results are\ntreated as prompts to help LMs reason, making up\nfor the lack of physical knowledge in LMs.\nCode Interpreter. With the emergence of LMs\nof code (Chen et al., 2021; Xu et al., 2022), collabo-\nrating LMs and codes to tackle specific tasks has re-\ncently sprung up (Wang et al., 2022c; Cheng et al.,\n2022; Wu et al., 2022b). Note that programs yield\nadvantage behaviors in robustness and interpretabil-\nity and can better illustrate complex structures and\ndeduct complex calculations. Intuitively, Madaan\net al. (2022) reframes structured commonsense rea-\nsoning tasks as code generation tasks, replacing the\nnatural language with python class code to repre-\nsent structured graph both in few-shot prompts and\nLM outputs. Gao et al. (2022) decomposes solu-\ntion steps from LMs to a programmatic runtime\nand remains the only learning task for the LMs. In\nfew-shot prompts and LM outputs, the reasoning\nprocesses are replaced by a mixture of natural and\nprogramming language, where natural language\nis treated as annotations to aid the generation of\nthe program. Similar to Gao et al. (2022), Chen\net al. (2022b) proposes program of thoughts (PoT)\nprompting which disentangling computation from\nreasoning. The main difference is that it also puts\nforward a zero-shot format of PoT prompting.\nTool Learning. Despite possessing remark-\nable generation and decision-making capabilities,\nLLMs struggle with some basic functionalities\nwhere much simpler and smaller tools excel (Qin\net al., 2023). Building on this insight, Schick\net al. (2023) trains models by integrating the us-\nage of various tools, including calculators, Q&A\nsystems, search engines and etc. Through implant-\ning tool API calls into the text generation process,\nthe model’s capabilities are significantly expanded.\nParanjape et al. (2023) designs the tool-use for\nLLMs as an automated schema, which eliminates\nthe need for hand-crafting task-specific demonstra-\ntions and carefully scripted interleaving of model\ngenerations with tool use. Lu et al. (2023a) har-\nnesses the powerful decision-making abilities of\nLLMs, enabling them to combine various external\ntools to tackle compositional reasoning tasks.\nQ:  Roger has 5 tennis balls.\nHe buys 2 more cans of tennis\nballs. Each can has 3 tennis\nballs. How many tennis balls\ndoes he have now? \nPrompt \nLM \nRoger started with 5 balls.\n2 cans of 3 tennis balls\neach is 6 tennis balls. 5 +\n6 = 11. The answer is 1 1 . \nLM \nCorpus \nFigure 6: Knowledge Enhanced Reasoning (§3.2).\nPrompts are generated by LM (Implicit Knowledge) or\nretrieved from external corpus (Explicit Knowledge).\n3.2 Knowledge Enhanced Reasoning\nAs noted in Manning (2022), knowledge plays a\nvital role in AI reasoning systems. Knowledge\nenhanced methods aim to prompt LMs withimplicit\n(§3.2.1) or explicit (§3.2.2) knowledge to assist in\nreasoning (see Figure 6).\n3.2.1 Implicit Knowledge\nResearchers have shown that LMs contain consid-\nerable implicit knowledge (Davison et al., 2019;\nPetroni et al., 2019; Jiang et al., 2020). The fol-\nlowing works try to induce such “modeledge” as\nknowledge-informed prompts for reasoning.\nLiu et al. (2022c) applies GPT-3 (Brown et al.,\n2020) with few-shot prompting to generate knowl-\nedge and prompts the downstream LM. Liu et al.\n(2022b) draws support from reinforcement learn-\ning (Schulman et al., 2017) to further calibrate the\nknowledge. Different from the approaches using\nfew-shot prompting in the knowledge generation\nstage, Sun et al. (2022) proposes a two-stage gener-\native prompting which additionally includes answer\ngeneration prompts. Other works (Li et al., 2022b;\nWang et al., 2023; Shridhar et al., 2022; Magister\net al., 2022; Ho et al., 2022) follow knowledge\ndistillation that generates reasoning samples by\nprompting a larger LM and teaches smaller LMs.\n3.2.2 Explicit Knowledge\nAlthough large LMs have shown strong generation\nability (Wiegreffe et al., 2022; Li et al., 2022b;\nWang et al., 2023), they still have the tendency\nto hallucinate facts (Rohrbach et al., 2018) and\ngenerate inconsistent knowledge (Liu et al., 2022b).\nRecent works show that retrieving prompts for in-\ncontext learning is a nice means to achieve good\nperformance (Liu et al., 2022a; Rubin et al., 2022).\nDue to the instability of common retrieval ap-\nproaches to measure the similarity of structured\ninformation, Lu et al. (2023b) proposes a dynamic\nprompt retrieval method based on policy gradient\nstrategy, without brute-force searching. He et al.\n(2023) retrieves relevant knowledge based on the\n5372\nCategory Representative Method Comparison Scope\nPrompt Acquisition Prompt Type Language Model Training Scenario\nPOTTER (Rajagopal et al., 2021) Manual Template BART/T5 full fine-tune\nCoT (Wei et al., 2022b) Manual CoT UL2/LaMDA/GPT-3 175B/Codex/PaLM few-shot prompt\nAuto-CoT (Zhang et al., 2023b) LM Generated CoT GPT-3 175B/Codex few-shot promptPrompt Engineering\nLeast-to-Most (Zhou et al., 2023) Manual CoT GPT-3 175B/Codex few-shot prompt\nCalibrator (Ye and Durrett, 2022) Manual Rationales InstructGPT few-shot fine-tune\nSelf-Consistency (Wang et al., 2022e) Manual CoT UL2/LaMDA/Codex/PaLM few-shot prompt\nDIVERSE (Li et al., 2022d) LM Generated CoT GPT-3 175B/Codex few-shot promptProcess Optimization\nLMSI (Huang et al., 2022) LM Generated CoT PaLM self-train\nPAL (Gao et al., 2022) Manual Code Codex few-shot prompt\nPoT (Chen et al., 2022b) Manual Code Codex few-shot promptExternal Engine\nToolformer (Schick et al., 2023) Manual CoT with tools GPT-J self-train\nRAINIER (Liu et al., 2022b) LM Generated Knowledge UnifiedQA few-shot prompt\nPINTO (Wang et al., 2023) LM Generated Rationales ROBERTA/T5 full fine-tuneImplicit Knowledge\nFine-tune-CoT (Ho et al., 2022) LM Generated Rationales GPT-3 0.3B/1.3B/6.7B full fine-tune\nPROMPTPG (Lu et al., 2023b) Retrieval CoT GPT-3 175B few-shot promptExplicit KnowledgeIRCoT (Trivedi et al., 2022) Retrieval CoT with wiki Flan-T5/GPT-3 few-shot prompt\nTable 1: Comparison of reasoning with prompting methods from different scopes.\n10\n20\n30\n40\n50\n60\n70\nGPT3-finetune\nGPT3-CoT\nCodex-CoT\nLaMDA-68B-CoT\nLaMDA-137B-CoT\nGSM8K Accuracy(%)\nModel size\n540B\n175B\n62B\nPaLM-62B\n-CoT\nPaLM-540B\n-CoT\nFigure 7: Performance of different language model\nscales on arithmetic reasoning. Representatively, we\nshow CoT (Wei et al., 2022b) experimental results on\nGSM8K (Cobbe et al., 2021).\nreasoning steps of CoT to provide more faithful ex-\nplanations. Trivedi et al. (2022) augments CoT\nprompting by persistently retrieving wiki docu-\nments for open-domain knowledge-intensive tasks\nthat require complex multi-step reasoning.\n4 Comparison and Discussion\n4.1 Comparison of Language Models\nTable 1 shows four comparison scopes of differ-\nent methods. We further illustrate the perfor-\nmance comparison of LMs with different scales on\nGSM8K (Cobbe et al., 2021) of arithmetic reason-\ning in Figure 7. Similar results on commonsense\nreasoning benchmarks are shown in Appendix A.3.\nWei et al. (2022b) systematically demonstrates\nthat few-shot prompting performs better in almost\nall tasks as model scale increases, which can be\nexplained by the fact that LMs with larger model\nsize contain more implicit knowledge for reason-\ning (Liang et al., 2022b). Moreover, CoT prompt-\ning produces much greater increases, with PaLM-\n540B showing the greatest improvements, as de-\npicted in Figure 7&9. However, when the model\nscale declines to less than 100B, CoT prompting\nwill yield no performance gain and may even be\ndetrimental. Thus, CoT prompting elicits an emer-\ngent ability of model scale (Wei et al., 2022a).\nOne possibility is that when the stored knowledge\nreaches a certain level, the reasoning ability of LMs\nundergoes a qualitative change from quantitative\nchange, leading to the emergence of emergent ca-\npabilities. Additionally, Srivastava et al. (2022)\npoints out that such ability generally occurs in\nmulti-process tasks which may be explained that\nthe evaluation only focuses on the final answer,\nbut ignores the improvement of the middle pro-\ncess brought by the increase of model scale when\nit is not large enough. Another intriguing obser-\nvation is depicted in Figure 7&9 that PaLM-62B\n(Chowdhery et al., 2022) even performs better than\nLaMDA-137B (Thoppilan et al., 2022), possibly\nbecause it was trained on the higher-quality corpus.\nThis phenomenon leads us to speculate that such\nemergent ability is not solely determined by model\nparameter scale but also related to the quality of\npre-training data.\nNotably, Figure 7&9 also illustrate that holding\nthe same parameter scale, Codex (Chen et al., 2021)\noutperforms GPT-3 significantly 3, even though\nthe major difference between them is the train-\ning corpus (Codex is a GPT-3 variant training on\ncode). This phenomenon can also be inspected in\nrecent works (Zhou et al., 2023; Li et al., 2022d;\n3Note that Codex and GPT-3 in our paper refer to code-\ndavinci-002 and text-davinci-002 respectively in OpenAI API.\n5373\nZhang et al., 2023b; Madaan et al., 2022; Liang\net al., 2022b), indicating that pre-training on code\nbranch not only enables the ability of code gen-\neration/understanding but may also trigger the\nreasoning ability with CoT . The exact cause is\nstill elusive, but one intuition is that code is a form\nof text more similar to reasoning, thinking about\nprocedure-oriented programming is analogous to\nsolving problems step by step, and object-oriented\nprogramming is analogous to decomposing com-\nplex tasks into simpler ones (Yao et al., 2022). In\naddition, Prystawski and Goodman (2023) finds\nthat CoT is beneficial only when the training data\nexhibits local structure. Due to its expertise in rea-\nsoning by navigating through multiple variables,\nCoT excels in deducing the relationship between\ntwo variables that have seldom been encountered in\nthe same context. However, it may not perform bet-\nter than simple statistical estimators when it comes\nto reasoning with variables that frequently co-occur\nin the training data.\n4.2 Comparison of Prompts\nTable 1 shows the comparison of different meth-\nods of reasoning with LM prompting. There are\nthree main sources of prompts for existing methods:\n1) Manual construction is suitable for template-\nbased prompts and few-shot prompting where the\nprompt is uncomplicated. 2) LM Generated\nprompt makes up for the shortcomings of man-\nual construction prompt. It can customize specific\nrationales for each question and provide sufficient\nknowledge with the prompt for fine-tuning or self-\ntraining. 3) Retrieval-based prompt often relies on\nwell-annotated external resources (e.g., Wikipedia)\nand consumes expensive information retrieval, but\nit can alleviate the unstable issue of the generation.\nWe observe that no matter how prompt is pro-\nduced, CoT prompting only works on large LMs.\nSmaller LMs work by fine-tuning with rationales.\nCombined with the empirical conclusion in Ye and\nDurrett (2022), these phenomena reveal that high-\nquality reasoning rationales contained in the\ninput context are the keys for reasoning with\nLM prompting. Although some works have at-\ntempted to explore the in-context learning ability\non large LMs (Xie et al., 2022; Min et al., 2022;\nAkyürek et al., 2022), the reason why CoT prompt-\ning can succeed is still intriguing to the community\nand not well-understood. One possible hypothesis\nis that CoT is a magical side product of training\non code that can be unlocked by prompt. Note\nthat exemplars containing CoT in few-shot prompts\ncan be viewed as a kind of instruction that arouses\nthe reasoning ability hidden in large LMs. Chung\net al. (2022) verifies the similar result using CoT\nin instruction fine-tuning to advance model per-\nformance further. In fact, in-context learning can\nbe seen as an intermediate state of evolution from\ngeneral prompts to human-readable instructions.\nFollowing this trend, prompts may grow into an\nessential interface of human-machine interaction.\n5 Benchmarks and Resources\n5.1 Taxonomy of Benchmarks and Tasks\nIn this section, we will give a brief overview of\nreasoning benchmarks and tasks. More details of\ndatasets, as well as reasoning with ChatGPT can\nbe found in Appendix A.4 and A.5.\nArithmetic Reasoning. Arithmetic reasoning,\nalso referred to as mathematical reasoning, is the\nability to perform reasoning on math word prob-\nlems (MWP). Early works on this task (Hosseini\net al., 2014; Kushman et al., 2014; Roy et al., 2015;\nKoncel-Kedziorski et al., 2015; Roy and Roth,\n2015) focus on relatively small datasets consist-\ning of grade school single-step or multi-step MWP.\nLater works increase in complexity, difficulty, and\nscale. Most recently, Mishra et al. (2022a) extends\nexisting datasets to construct a unified benchmark\nconcerning mathematical abilities, language diver-\nsity, and external knowledge.\nCommonsense Reasoning. Commonsense\nknowledge and commonsense reasoning are\nsome of the major issues in machine intelligence\n(Storks et al., 2019; Bhargava and Ng, 2022).\nWhen answering a question, people often draw\nupon their rich world knowledge. For LMs, the\nmajor challenge of performing commonsense\nreasoning lies in how to involve physical and\nhuman interactions under the presumption of\ngeneral background knowledge (Bhargava and\nNg, 2022). Many benchmark datasets and tasks\n(Clark et al., 2018; Mihaylov et al., 2018; Talmor\net al., 2019; Bisk et al., 2020; Geva et al., 2021)\nare designed, and the most widely used benchmark\ntoday is CommonsenseQA (Talmor et al., 2019).\nLogical Reasoning. Common forms of logical\nreasoning include deductive reasoning and induc-\ntive reasoning, deductive reasoning and abductive\n5374\nreasoning (Sinha et al., 2019; Bao et al., 2022;\nYoung et al., 2022; Bao et al., 2023). Deductive\nreasoning is performed by going from general in-\nformation to specific conclusions. Typical datasets\nin this field consist of synthetic rule bases plus de-\nrived conclusions (Clark et al., 2020; Tafjord et al.,\n2021). Dalvi et al. (2021) creatively proposes a\ndataset containing multi-step entailment trees to-\ngether with rules and conclusions. As opposed to\ndeductive reasoning, inductive reasoning aims to\ndraw conclusions by going from specific observa-\ntions to general principles (Yang et al., 2022c).\nSymbolic Reasoning. Symbolic reasoning here\nonly refers to a narrow collection of simple tasks\nthat test a diverse set of symbolic manipulation\nfunctions, rather than symbolic AI, which is a more\ngeneral concept. Typical symbolic reasoning tasks\ninclude last letter concatenation, reverse list and\ncoin flip (Wei et al., 2022b).\nMultimodal Reasoning. Except for textual\nmodality, humans utilize the information available\nacross different modalities when performing rea-\nsoning. To this end, multimodal reasoning bench-\nmarks (Zellers et al., 2019; Park et al., 2020; Dong\net al., 2022) are presented to narrow this gap. Re-\ncently, Lu et al. (2022a) presents ScienceQA, a\nlarge-scale multimodal multiple choice dataset that\nconsists of diverse questions of science topics with\ncorresponding answers and explanations. Zhang\net al. (2023a) proposes a new task of multimodal\nanalogical reasoning over knowledge graphs.\n5.2 Resources\nThanks to the open-source spirit of the NLP com-\nmunity, numerous resources are publicly available\nalongside papers for researchers to experiment with.\nThoughtSource is a central, open resource and com-\nmunity around data and tools related to CoT rea-\nsoning in large language models4. The LangChain\nlibrary is designed to help developers build appli-\ncations using LLMs combined with other sources\nof computation or knowledge 5. λprompt allows\nfor building a complete large LM-based prompt\nmachines, including ones that self-edit to correct\nand even self-write their own execution code6. Re-\ncently, Ou et al. (2023) develops EasyInstruct, a\nPython package for instructing LLMs like GPT-3\n4https://github.com/OpenBioLink/ThoughtSource\n5https://github.com/hwchase17/langchain\n6https://github.com/approximatelabs/\nlambdaprompt\nin research experiments. A test case for reasoning\nusing EasyInstruct can be found in Appendix A.6.\n6 Future Directions\nTheoretical Principle of Reasoning. LMs have\nbeen demonstrated to have emergent zero-shot\nlearning and reasoning abilities (Wei et al., 2022b;\nWang et al., 2022e; Wei et al., 2022a). To uncover\nthe mystery of such a success, many researchers\nhave empirically explored the role of in-context\nlearning (Ye and Durrett, 2022; Liu et al., 2022a)\nand rationales (Min et al., 2022; Lampinen et al.,\n2022). Another line of works tries to investigate\nthe architecture of Transformers via knowledge\nneurons (Dai et al., 2022) or skill neurons (Wang\net al., 2022b). More recent works (Wang et al.,\n2022c; Madaan et al., 2022) demonstrate that pre-\ntrained LMs of code are better handling structured\ncommonsense reasoning and prediction than LMs\nof natural language, even when the downstream\ntask does not involve source code at all. However,\nthe code-based pre-training (or re-structured pre-\ntraining (Yuan and Liu, 2022)) still has limitations\nsince it has to utilize off-the-shelf structure (e.g.,\nexisting aligned corpus or build from scratch via\nsyntax tree or AMR (Banarescu et al., 2013)) to\nreformulate plain texts. Thus, the truth may be\nclose, and we argue that it is beneficial to study\nthe theoretical principle to advocate for a trans-\nparent view of reasoning with LM prompting and\nfurther decipher the dark matter of intelligence by\nhighlighting the counterintuitive continuum across\nlanguage, knowledge, and reasoning 7. Note that\nreasoning in NLP has the potential advantages of\ncomplex problem-solving and should better utilize\ndark matters in cross-disciplines (e.g., Theory of\nMind (Sap et al., 2022; Moghaddam and Honey,\n2023; Zhou et al., 2022b; Shapira et al., 2023)).\nEfficient Reasoning. To be noted, existing meth-\nods mainly depend on large LMs, which may con-\nsume high computing resources. Regarding practi-\ncality, it is necessary to study reasoning with small\nLMs or develop efficient reasoning methodologies\nwhich pay attention to carbon emission and en-\nergy usage during model training and inference\n(Xu et al., 2021). One feasible way may be devel-\noping models that can enable generalization across\na range of evaluation scenarios such as Flan-T5\n(Chung et al., 2022), which finetune both with and\n7Keynote talk on ACL 2022 entitled “2082: An ACL\nOdyssey: The Dark Matter of Intelligence and Language”.\n5375\nwithout exemplars (i.e., zero-shot and few-shot)\nand with and without CoT. Recently, an intuitive\napproach has been proposed to transfer the reason-\ning capabilities of large LMs to smaller LMs via\nknowledge distillation (Shridhar et al., 2022; Mag-\nister et al., 2022; Ho et al., 2022). Other promising\ndirections include retrieval augmentation (Li et al.,\n2022a), model editing (Cao et al., 2021; Mitchell\net al., 2022a,b; Cheng et al., 2023), delta-tuning\n(He et al., 2022; Mao et al., 2022; Pal et al., 2022;\nDing et al., 2022), etc.\nRobust, Faithful and Interpretable Reasoning.\nRobustness, faithfulness and interpretability have\nlong been pursued by the field of deep learning, es-\npecially in tasks that require strong logic, like rea-\nsoning. Shaikh et al. (2022) demonstrates that zero-\nshot CoT will produce undesirable toxicity and bi-\nases, indicating the necessity of robust, faithful and\ninterpretable reasoning. Creswell and Shanahan\n(2022) leverages a selection-inference (Creswell\net al., 2022) multi-stage architecture for faithful\nreasoning, but there is still a lack of interpretabil-\nity within each stage. Code-based works (Madaan\net al., 2022; Gao et al., 2022; Chen et al., 2022b)\nreach robustness and interpretability to some ex-\ntent, but they have the aid of an external engine.\nThere is still a long way to achieve true robust-\nness, faithfulness and interpretability with LMs.\nFortunately, Dohan et al. (2022) provides a new\nidea for utilizing a probabilistic program to tackle\nvarious reasoning problems. Other solutions may\nbe neural-symbolic approaches (Du et al., 2021;\nLi et al., 2022c; Ouyang et al., 2021; Feng et al.,\n2022) or human feedback (Ouyang et al., 2022).\nMultimodal (Interactive) Reasoning. Textual\nreasoning is restricted to what can be expressed\nthrough natural language. A more promising direc-\ntion is multimodal reasoning regarding the infor-\nmation diversity of the real world of human reason-\ning. Lu et al. (2022a) generates CoT when dealing\nwith a multimodal dataset; however, it simply ex-\ntracts textual descriptions from images, and it is\nstill a textual reasoning task indeed. Intuitively, it\nis beneficial to integrate multimodal information\ninto reasoning processes such as images, audio,\nvideos, etc., and design a unified multimodal CoT.\nApart from unified multimodal models, it is also\npromising to model chains (Wu et al., 2022a) to\nconduct interactive reasoning among models of dif-\nferent modalities. Besides, Sap et al. (2022) shows\nthat one of today’s largest language models (GPT-3\n(Brown et al., 2020)) lacks the skill to reason about\nthe mental states, and reactions of all people in-\nvolved. Thus, interactive reasoning methodologies\nshould be noted by inspiring from other domains\n(e.g., Cognitive Science (Hollenstein et al., 2019),\nSocial Intelligence (Krishna et al., 2022)), which\nmay have potential guidance for reasoning in NLP\nsince only increasing the scale of LMs is likely not\nthe most effective way to create AI systems.\nGeneralizable (True) Reasoning. Generaliza-\ntion is one of the most significant symbols of mod-\nels to attain true reasoning abilities. Given a rea-\nsoning task, we hope LMs can handle not only\nthe problem itself but solve a group of similar\nreasoning tasks (not seen during training). Zhou\net al. (2022a); Anil et al. (2022) explore the OOD\nproblem on the length of reasoning questions, but\nthe true generalization is still far from satisfactory.\nMeanwhile, Kejriwal et al. (2022) highlights that\nmore comprehensive evaluation methods grounded\nin theory (e.g., naive physics (Gardin and Meltzer,\n1989) and commonsense psychology (Gordon and\nHobbs, 2004)) should be proposed. We argue that\nthe generalizable reasoning may be closely related\nto analogy reasoning (Chen et al., 2022a; Webb\net al., 2022), causal reasoning (Feder et al., 2022),\ncompositional reasoning (Yang et al., 2022a), etc.\n7 Conclusion and Vision\nIn this paper, we provide a review of reasoning with\nlanguage model prompting, including comprehen-\nsive comparisons, and several research directions.\nIn the future, we envision a more potent synergy be-\ntween the methodologies from the NLP and other\ndomains and hope sophisticated and efficient LM\nprompting models will increasingly contribute to\nimproving reasoning performance.\nAcknowledgment\nWe would like to express gratitude to the anony-\nmous reviewers for their kind comments. This\nwork was supported by the National Natural Sci-\nence Foundation of China (No.62206246 and\nU19B2027), Zhejiang Provincial Natural Science\nFoundation of China (No. LGG22F030011),\nNingbo Natural Science Foundation (2021J190),\nand Yongjiang Talent Introduction Programme\n(2021A-156-G), CAAI-Huawei MindSpore Open\nFund, and NUS-NCS Joint Laboratory (A-\n0008542-00-00).\n5376\nLimitations\nIn this study, we provide a survey of reasoning\nwith language model prompting. We discuss the\nrelated surveys in Appendix A.1 and will continue\nadding more related approaches with more detailed\nanalysis. Despite our best efforts, there may be still\nsome limitations that remain in this paper.\nReferences & Methods. Due to the page limit,\nwe may miss some important references and cannot\nafford all the technical details. We mainly review\nthe cutting-edge methods within two years (mostly\nin 2022) in §3, mainly from the ACL, EMNLP,\nNAACL, NeurIPS, ICLR, arXiv, etc., and we will\ncontinue to pay attention to and supplement the\nlatest works.\nBenchmarks. Most of the reasoning benchmarks\nmentioned in §5 are gathered and categorized from\nthe experimental part of mainstream works. The\ndefinition and boundary of each task may not be\naccurate enough. Besides, our work may miss\nsome kind of reasoning tasks such as reasoning\nwith generics (Allaway et al., 2022), default inher-\nitance reasoning (Brewka, 1987), non-monotonic\nreasoning (Ginsberg, 1987) in NLP, and will try\nour best to fulfill this gap.\nEmpirical Conclusions. We give detailed com-\nparisons and discussions of language models and\nprompts in §4, and list some promising future direc-\ntions in §6. All the conclusions are proposed and\nfurther speculated upon empirical analysis of exist-\ning works which may not be macroscopic enough.\nAs the field evolves faster, we will update the latest\nopinions timely.\nReferences\nDavid H. Ackley, Geoffrey E. Hinton, and Terrence J.\nSejnowski. 1985. A learning algorithm for boltz-\nmann machines. Cogn. Sci., 9(1):147–169.\nEkin Akyürek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2022. What learn-\ning algorithm is in-context learning? investigations\nwith linear models. CoRR, abs/2211.15661.\nEmily Allaway, Jena D. Hwang, Chandra Bhagavat-\nula, Kathleen R. McKeown, Doug Downey, and\nYejin Choi. 2022. Penguins don’t fly: Reasoning\nabout generics through instantiations and exceptions.\nCoRR, abs/2205.11658.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2357–2367, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor\nLewkowycz, Vedant Misra, Vinay V . Ramasesh, Am-\nbrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam\nNeyshabur. 2022. Exploring length generalization in\nlarge language models. CoRR, abs/2207.04901.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Griffitt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract meaning representation\nfor sembanking. In Proceedings of the 7th Linguis-\ntic Annotation Workshop and Interoperability with\nDiscourse, LAW-ID@ACL 2013, August 8-9, 2013,\nSofia, Bulgaria, pages 178–186. The Association for\nComputer Linguistics.\nQiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wan-\njun Zhong, Neset Tan, Nathan Young, Yang Chen,\nYonghua Zhu, Michael Witbrock, and Jiamou Liu.\n2023. Contrastive learning with logic-driven data\naugmentation for logical reasoning over text. CoRR,\nabs/2305.12599.\nQiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan,\nZhenyun Deng, Michael Witbrock, and Jiamou Liu.\n2022. Multi-step deductive reasoning over natural\nlanguage: An empirical study on out-of-distribution\ngeneralisation. In Proceedings of the 16th Inter-\nnational Workshop on Neural-Symbolic Learning\nand Reasoning as part of the 2nd International\nJoint Conference on Learning & Reasoning (IJCLR\n2022), Cumberland Lodge, Windsor Great Park, UK,\nSeptember 28-30, 2022, volume 3212 ofCEUR Work-\nshop Proceedings, pages 202–217. CEUR-WS.org.\nLuca Beurer-Kellner, Marc Fischer, and Martin Vechev.\n2022. Prompting is programming: A query language\nfor large language models. CoRR, abs/2212.06094.\nPrajjwal Bhargava and Vincent Ng. 2022. Common-\nsense knowledge reasoning and generation with pre-\ntrained language models: A survey. In Thirty-Sixth\nAAAI Conference on Artificial Intelligence, AAAI\n2022, Thirty-Fourth Conference on Innovative Ap-\nplications of Artificial Intelligence, IAAI 2022, The\nTwelveth Symposium on Educational Advances in Ar-\ntificial Intelligence, EAAI 2022 Virtual Event, Febru-\nary 22 - March 1, 2022, pages 12317–12325. AAAI\nPress.\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(05):7432–7439.\n5377\nGerhard Brewka. 1987. The logic of inheritance in\nframe systems. In Proceedings of the 10th Inter-\nnational Joint Conference on Artificial Intelligence.\nMilan, Italy, August 23-28, 1987 , pages 483–488.\nMorgan Kaufmann.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2021,\nVirtual Event / Punta Cana, Dominican Republic, 7-\n11 November, 2021, pages 6491–6506. Association\nfor Computational Linguistics.\nJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao\nLi, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua\nXiao, and Hao Zhou. 2022a. E-KAR: A benchmark\nfor rationalizing natural language analogical reason-\ning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 3941–3955. Association for Com-\nputational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nWenhu Chen. 2022. Large language models are few(1)-\nshot table reasoners. CoRR, abs/2210.06710.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022b. Program of thoughts\nprompting: Disentangling computation from rea-\nsoning for numerical reasoning tasks. CoRR,\nabs/2211.12588.\nZhenfang Chen, Qinhong Zhou, Yikang Shen, Yining\nHong, Hao Zhang, and Chuang Gan. 2023. See,\nthink, confirm: Interactive prompting between vision\nand language models for knowledge-based visual\nreasoning.\nSiyuan Cheng, Ningyu Zhang, Bozhong Tian, Zelin\nDai, Feiyu Xiong, Wei Guo, and Huajun Chen. 2023.\nEditing language model-based knowledge graph em-\nbeddings. CoRR, abs/2301.10405.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2022. Binding\nlanguage models in symbolic languages. CoRR,\nabs/2210.02875.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457.\n5378\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In Pro-\nceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI-20 , pages\n3882–3890. International Joint Conferences on Arti-\nficial Intelligence Organization. Main track.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. CoRR, abs/2110.14168.\nAntonia Creswell and Murray Shanahan. 2022. Faith-\nful reasoning using large language models. CoRR,\nabs/2208.14271.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. CoRR,\nabs/2205.09712.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons\nin pretrained transformers. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 8493–\n8502. Association for Computational Linguistics.\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan\nXie, Hannah Smith, Leighanna Pipatanangkura, and\nPeter Clark. 2021. Explaining answers with entail-\nment trees. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7358–7370, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJoe Davison, Joshua Feldman, and Alexander M. Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019, pages\n1173–1178. Association for Computational Linguis-\ntics.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-\nof-thought for large language models. CoRR,\nabs/2302.12246.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels. CoRR, abs/2203.06904.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A. Saurous,\nJascha Sohl-Dickstein, Kevin Murphy, and Charles\nSutton. 2022. Language model cascades. CoRR,\nabs/2207.10342.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey for in-context learning.\nCoRR, abs/2301.00234.\nQingxiu Dong, Ziwei Qin, Heming Xia, Tian Feng,\nShoujie Tong, Haoran Meng, Lin Xu, Zhongyu Wei,\nWeidong Zhan, Baobao Chang, Sujian Li, Tianyu Liu,\nand Zhifang Sui. 2022. Premise-based multimodal\nreasoning: Conditional inference on joint textual and\nvisual clues. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 932–946, Dublin,\nIreland. Association for Computational Linguistics.\nLi Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin.\n2021. Excar: Event graph knowledge enhanced ex-\nplainable causal reasoning. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 2354–2363. Associa-\ntion for Computational Linguistics.\nYifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao.\n2022. A survey of vision-language pre-trained mod-\nels. In Proceedings of the Thirty-First International\nJoint Conference on Artificial Intelligence, IJCAI\n2022, Vienna, Austria, 23-29 July 2022, pages 5436–\n5443. ijcai.org.\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt\nGardner. 2022. Successive prompting for decom-\nposing complex questions. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1251–1265, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nNan Duan, Duyu Tang, and Ming Zhou. 2020. Ma-\nchine reasoning: Technology, dilemma and future.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Tutorial\nAbstracts, EMNLP 2020, Online, November 19-20,\n2020, pages 1–6. Association for Computational Lin-\nguistics.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\n5379\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 889–898. Association for Computational Lin-\nguistics.\nAmir Feder, Katherine A. Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob\nEisenstein, Justin Grimmer, Roi Reichart, Margaret E.\nRoberts, Brandon M. Stewart, Victor Veitch, and Diyi\nYang. 2022. Causal inference in natural language\nprocessing: Estimation, prediction, interpretation and\nbeyond. Trans. Assoc. Comput. Linguistics, 10:1138–\n1158.\nYufei Feng, Xiaoyu Yang, Xiaodan Zhu, and Michael A.\nGreenspan. 2022. Neuro-symbolic natural logic with\nintrospective revision for natural language inference.\nTrans. Assoc. Comput. Linguistics, 10:240–256.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023a. Specializing smaller lan-\nguage models towards multi-step reasoning. CoRR,\nabs/2301.12726.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023b. Complexity-based prompting\nfor multi-step reasoning. In The Eleventh Interna-\ntional Conference on Learning Representations.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. PAL: program-aided language\nmodels. CoRR, abs/2211.10435.\nFrancesco Gardin and Bernard Meltzer. 1989. Analog-\nical representations of naive physics. Artif. Intell.,\n38(2):139–159.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nMatthew L Ginsberg. 1987. Readings in nonmonotonic\nreasoning.\nAndrew S. Gordon and Jerry R. Hobbs. 2004. For-\nmalizations of commonsense psychology. AI Mag.,\n25(4):49–62.\nZhen Guo, Zelin Wan, Qisheng Zhang, Xujiang Zhao,\nFeng Chen, Jin-Hee Cho, Qi Zhang, Lance M. Ka-\nplan, Dong H. Jeong, and Audun Jøsang. 2022. A\nsurvey on uncertainty reasoning and quantification\nfor decision making: Belief theory meets deep learn-\ning. CoRR, abs/2206.05675.\nKyle Hamilton, Aparna Nayak, Bojan Bozic, and Luca\nLongo. 2022. Is neuro-symbolic AI meeting its\npromise in natural language processing? A structured\nreview. CoRR, abs/2202.12205.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao\nLiu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang,\nLiang Zhang, Wentao Han, Minlie Huang, Qin Jin,\nYanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu,\nXipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen,\nJinhui Yuan, Wayne Xin Zhao, and Jun Zhu. 2021.\nPre-trained models: Past, present and future. AI\nOpen, 2:225–250.\nHangfeng He, Hongming Zhang, and Dan Roth. 2023.\nRethinking with retrieval: Faithful large language\nmodel inference. CoRR, abs/2301.00303.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nunified view of parameter-efficient transfer learning.\nIn The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-\n29, 2022. OpenReview.net.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the MATH dataset. In Proceedings\nof the Neural Information Processing Systems Track\non Datasets and Benchmarks 1, NeurIPS Datasets\nand Benchmarks 2021, December 2021, virtual.\nNamgyu Ho, Laura Schmid, and Se-Young Yun.\n2022. Large language models are reasoning teachers.\nCoRR, abs/2212.10071.\nNora Hollenstein, Maria Barrett, Marius Troendle,\nFrancesco Bigiolli, Nicolas Langer, and Ce Zhang.\n2019. Advancing NLP with cognitive language pro-\ncessing signals. CoRR, abs/1904.02682.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 523–533, Doha, Qatar. Association for Com-\nputational Linguistics.\nDanqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin,\nand Wei-Ying Ma. 2016. How well do computers\nsolve math word problems? large-scale dataset con-\nstruction and evaluation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 887–896,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. CoRR,\nabs/2210.11610.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\nCoRR, abs/2212.10403.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Barun Patra, Qiang\n5380\nLiu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\nVishrav Chaudhary, Subhojit Som, Xia Song, and\nFuru Wei. 2023. Language is not all you need:\nAligning perception with language models. CoRR,\nabs/2302.14045.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathprompter: Mathematical reasoning using large\nlanguage models. In ICLR 2023 Workshop on Trust-\nworthy and Reliable Large-Scale Machine Learning\nModels.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics ,\n8:423–438.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing , pages\n1266–1279, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia,\nXin Xu, and Deepak Ramachandran. 2022. LAM-\nBADA: backward chaining for automated reasoning\nin natural language. CoRR, abs/2212.13894.\nMayank Kejriwal, Henrique Santos, Alice M. Mulve-\nhill, and Deborah L. McGuinness. 2022. Designing\na strong test for measuring true common-sense rea-\nsoning. Nat. Mach. Intell., 4(4):318–322.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modular\napproach for solving complex tasks. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. CoRR,\nabs/2205.11916.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing Algebraic Word Problems into Equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585–597.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\nA math word problem repository. In Proceedings of\nthe 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1152–1157, San\nDiego, California. Association for Computational\nLinguistics.\nRanjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S.\nBernstein. 2022. Socially situated artificial intel-\nligence enables learning from human interaction.\nProceedings of the National Academy of Sciences ,\n119(39):e2115730119.\nNate Kushman, Luke Zettlemoyer, Regina Barzilay, and\nYoav Artzi. 2014. Learning to automatically solve\nalgebra word problems. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2014, June 22-27, 2014, Baltimore,\nMD, USA, Volume 1: Long Papers, pages 271–281.\nThe Association for Computer Linguistics.\nBrenden M. Lake and Marco Baroni. 2017. Still not\nsystematic after all these years: On the compositional\nskills of sequence-to-sequence recurrent networks.\nCoRR, abs/1711.00350.\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y .\nChan, Kory Matthewson, Michael Henry Tessler,\nAntonia Creswell, James L. McClelland, Jane X.\nWang, and Felix Hill. 2022. Can language mod-\nels learn from explanations in context? CoRR,\nabs/2204.02329.\nJie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal.\n2020. What is more likely to happen next? video-\nand-language future event prediction. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8769–8784, Online. Association for Computational\nLinguistics.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay V . Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur,\nGuy Gur-Ari, and Vedant Misra. 2022. Solving quan-\ntitative reasoning problems with language models.\nCoRR, abs/2206.14858.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022a. A survey on retrieval-augmented\ntext generation. CoRR, abs/2202.01110.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,\nXinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan.\n2022b. Explanations from large language models\nmake small reasoners better. CoRR, abs/2210.06726.\nXiao Li, Gong Cheng, Ziheng Chen, Yawei Sun, and\nYuzhong Qu. 2022c. Adalogn: Adaptive logic graph\nnetwork for reasoning-based machine reading com-\nprehension. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 7147–7161. Association for\nComputational Linguistics.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022d. On the\nadvance of making language models better reasoners.\nCoRR, abs/2206.02336.\nKe Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenx-\nuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, and\nFuchun Sun. 2022a. Reasoning over different types\n5381\nof knowledge graphs: Static, temporal and multi-\nmodal. CoRR, abs/2212.05767.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya\nKumar, Benjamin Newman, Binhang Yuan, Bobby\nYan, Ce Zhang, Christian Cosgrove, Christopher D.\nManning, Christopher Ré, Diana Acosta-Navas,\nDrew A. Hudson, Eric Zelikman, Esin Durmus,\nFaisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu\nYao, Jue Wang, Keshav Santhanam, Laurel J. Orr,\nLucia Zheng, Mert Yüksekgönül, Mirac Suzgun,\nNathan Kim, Neel Guha, Niladri S. Chatterji, Omar\nKhattab, Peter Henderson, Qian Huang, Ryan Chi,\nSang Michael Xie, Shibani Santurkar, Surya Ganguli,\nTatsunori Hashimoto, Thomas Icard, Tianyi Zhang,\nVishrav Chaudhary, William Wang, Xuechen Li, Yi-\nfan Mai, Yuhui Zhang, and Yuta Koreeda. 2022b.\nHolistic evaluation of language models. CoRR,\nabs/2211.09110.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017a. Program induction by rationale gener-\nation: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 158–167. Association\nfor Computational Linguistics.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017b. Program induction by rationale gener-\nation: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158–167, Vancouver,\nCanada. Association for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022a. What\nmakes good in-context examples for gpt-3? In Pro-\nceedings of Deep Learning Inside Out: The 3rd Work-\nshop on Knowledge Extraction and Integration for\nDeep Learning Architectures, DeeLIO@ACL 2022,\nDublin, Ireland and Online, May 27, 2022 , pages\n100–114. Association for Computational Linguistics.\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He,\nSean Welleck, Hannaneh Hajishirzi, and Yejin Choi.\n2022b. Rainier: Reinforced knowledge introspector\nfor commonsense question answering. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 8938–8958,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022c. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 3154–\n3169. Association for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2022d. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, abs/2107.13586.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,\nSoroush V osoughi, Claire Cui, Denny Zhou, and An-\ndrew M. Dai. 2023. Mind’s eye: Grounded language\nmodel reasoning through simulation. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022a. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. CoRR, abs/2209.09513.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and\nJianfeng Gao. 2023a. Chameleon: Plug-and-play\ncompositional reasoning with large language models.\nCoRR, abs/2304.09842.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\nand Ashwin Kalyan. 2023b. Dynamic prompt learn-\ning via policy gradient for semi-structured mathe-\nmatical reasoning. In The Eleventh International\nConference on Learning Representations.\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-\nWei Chang. 2022b. A survey of deep learning for\nmathematical reasoning. CoRR, abs/2212.10535.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022c. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 8086–\n8098. Association for Computational Linguistics.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful chain-of-\nthought reasoning. CoRR, abs/2301.13379.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. CoRR, abs/2303.17651.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,\nand Graham Neubig. 2022. Language models of\ncode are few-shot commonsense learners. CoRR,\nabs/2210.07128.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdámek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. CoRR,\nabs/2212.08410.\n5382\nChristopher D Manning. 2022. Human language under-\nstanding & reasoning. Daedalus, 151(2):127–138.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Scott Yih, and Madian\nKhabsa. 2022. Unipelt: A unified framework for\nparameter-efficient language model tuning. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 6253–6264. Association for Computa-\ntional Linguistics.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ramakanth Pasunuru, Roberta\nRaileanu, Baptiste Rozière, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann\nLeCun, and Thomas Scialom. 2023. Augmented\nlanguage models: a survey. CoRR, abs/2302.07842.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nEnglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975–984, Online.\nAssociation for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? CoRR,\nabs/2202.12837.\nSwaroop Mishra, Matthew Finlayson, Pan Lu, Leonard\nTang, Sean Welleck, Chitta Baral, Tanmay Ra-\njpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter\nClark, and Ashwin Kalyan. 2022a. Lila: A uni-\nfied benchmark for mathematical reasoning. CoRR,\nabs/2210.17517.\nSwaroop Mishra, Arindam Mitra, Neeraj Varshney,\nBhavdeep Sachdeva, Peter Clark, Chitta Baral, and\nAshwin Kalyan. 2022b. NumGLUE: A suite of fun-\ndamental yet challenging mathematical reasoning\ntasks. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 3505–3523, Dublin,\nIreland. Association for Computational Linguistics.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D. Manning. 2022a. Fast\nmodel editing at scale. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D. Manning, and Chelsea Finn. 2022b. Memory-\nbased model editing at scale. In International Con-\nference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages\n15817–15831. PMLR.\nShima Rahimi Moghaddam and Christopher J. Honey.\n2023. Boosting theory-of-mind performance in\nlarge language models via prompting. CoRR,\nabs/2304.11490.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021.\nInvestigating the limitations of the transformers with\nsimple arithmetic tasks. CoRR, abs/2102.13019.\nOpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue. https://openai.com/blog/\nchatgpt/.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nYixin Ou, Shengyu Mao, Lei Li, Ziwen Xu, Xiaolong\nWeng, Shuofei Qiao, Yuqi Zhu, Yinuo Jiang, Zhen\nBi, Jing Chen, Huajun Chen, and Ningyu Zhang.\n2023. Easyinstruct: An easy-to-use framework to\ninstruct large language models. https://github.\ncom/zjunlp/EasyInstruct.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\nSiru Ouyang, Zhuosheng Zhang, and Hai Zhao.\n2021. Fact-driven logical reasoning. CoRR,\nabs/2105.10334.\nVaishali Pal, Evangelos Kanoulas, and Maarten de Ri-\njke. 2022. Parameter-efficient abstractive question\nanswering over tables or text. In Proceedings of the\nSecond DialDoc Workshop on Document-grounded\nDialogue and Conversational Question Answering,\nDialDoc@ACL 2022, Dublin, Ireland, May 26, 2022,\npages 41–53. Association for Computational Linguis-\ntics.\nBhargavi Paranjape, Scott M. Lundberg, Sameer\nSingh, Hannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Túlio Ribeiro. 2023. ART: automatic multi-\nstep reasoning and tool-use for large language mod-\nels. CoRR, abs/2303.09014.\nBhargavi Paranjape, Julian Michael, Marjan\nGhazvininejad, Hannaneh Hajishirzi, and Luke\nZettlemoyer. 2021. Prompting contrastive explana-\ntions for commonsense reasoning tasks. In Findings\nof the Association for Computational Linguistics:\nACL/IJCNLP 2021, Online Event, August 1-6, 2021,\nvolume ACL/IJCNLP 2021 of Findings of ACL ,\npages 4179–4192. Association for Computational\nLinguistics.\n5383\nJae Sung Park, Chandra Bhagavatula, Roozbeh Mot-\ntaghi, Ali Farhadi, and Yejin Choi. 2020. Visual-\ncomet: Reasoning about the dynamic context of a\nstill image. In Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28,\n2020, Proceedings, Part V, volume 12350 of Lecture\nNotes in Computer Science, pages 508–524. Springer.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094, Online.\nAssociation for Computational Linguistics.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-\nriz Borges, Antoine Bosselut, Robert West, and Boi\nFaltings. 2023. REFINER: reasoning feedback on in-\ntermediate representations. CoRR, abs/2304.01904.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473. Association for\nComputational Linguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. CoRR, abs/2210.03350.\nBen Prystawski and Noah D. Goodman. 2023. Why\nthink step-by-step? reasoning emerges from the lo-\ncality of experience. CoRR, abs/2304.03843.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Tool learning with foundation\nmodels. CoRR, abs/2304.08354.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nCoRR, abs/2003.08271.\nDheeraj Rajagopal, Vivek Khetan, Bogdan Sacaleanu,\nAnatole Gershman, Andrew E. Fano, and Eduard H.\nHovy. 2021. Cross-domain reasoning via template\nfilling. CoRR, abs/2111.00539.\nJustin Reppert, Ben Rachbach, Charlie George, Luke\nStebbing, JungWon Byun, Maggie Appleton, and\nAndreas Stuhlmüller. 2023. Iterated decomposition:\nImproving science q&a by supervising reasoning pro-\ncesses. CoRR, abs/2301.01751.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hallu-\ncination in image captioning. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018, pages 4035–4045. Association\nfor Computational Linguistics.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1743–1752, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nSubhro Roy, Tim Vieira, and Dan Roth. 2015. Reason-\ning about Quantities in Natural Language. Transac-\ntions of the Association for Computational Linguis-\ntics, 3:1–13.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 2655–2671. Association for\nComputational Linguistics.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the lim-\nits of social intelligence in large lms. EMNLP,\nabs/2210.13312.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR, abs/2302.04761.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347.\nOmar Shaikh, Hongxin Zhang, William Held, Michael\nBernstein, and Diyi Yang. 2022. On second thought,\nlet’s not think step by step! bias and toxicity in zero-\nshot reasoning. CoRR, abs/2212.08061.\nZhihong Shao, Yeyun Gong, Yelong Shen, Min-\nlie Huang, Nan Duan, and Weizhu Chen. 2023.\nSynthetic prompting: Generating chain-of-thought\ndemonstrations for large language models. CoRR,\nabs/2302.00618.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023. Clever hans or\nneural theory of mind? stress testing social rea-\nsoning in large language models. arXiv preprint\narXiv:2305.14763.\n5384\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi\nWang, Suraj Srivats, Soroush V osoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Di-\npanjan Das, and Jason Wei. 2022. Language models\nare multilingual chain-of-thought reasoners. CoRR,\nabs/2210.03057.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. 2023.\nReflexion: an autonomous agent with dynamic mem-\nory and self-reflection. CoRR, abs/2303.11366.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2022. Distilling multi-step reasoning ca-\npabilities of large language models into smaller\nmodels via semantic decompositions. CoRR,\nabs/2212.00193.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection\nwith chain-of-thought from labeled data. CoRR,\nabs/2302.12822.\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle\nPineau, and William L. Hamilton. 2019. CLUTRR:\nA diagnostic benchmark for inductive reasoning from\ntext. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4506–4515, Hong Kong, China. Association for Com-\nputational Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ameet Rahane, Anantharaman S.\nIyer, Anders Andreassen, Andrea Santilli, Andreas\nStuhlmüller, Andrew M. Dai, Andrew La, Andrew K.\nLampinen, Andy Zou, Angela Jiang, Angelica Chen,\nAnh Vuong, Animesh Gupta, Anna Gottardi, Anto-\nnio Norelli, Anu Venkatesh, Arash Gholamidavoodi,\nArfa Tabassum, Arul Menezes, Arun Kirubarajan,\nAsher Mullokandov, Ashish Sabharwal, Austin Her-\nrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and\net al. 2022. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\nCoRR, abs/2206.04615.\nShane Storks, Qiaozi Gao, and Joyce Y . Chai. 2019.\nCommonsense reasoning for natural language under-\nstanding: A survey of benchmarks, resources, and\napproaches. CoRR, abs/1904.01172.\nHongjin SU, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023.\nSelective annotation makes language models better\nfew-shot learners. In The Eleventh International Con-\nference on Learning Representations.\nYueqing Sun, Yu Zhang, Le Qi, and Qi Shi. 2022.\nTSGP: Two-stage generative prompting for unsuper-\nvised commonsense question answering. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 968–980, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nDídac Surís, Sachit Menon, and Carl V ondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. CoRR, abs/2303.08128.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621–3634, Online.\nAssociation for Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4149–4158. Association for Computational\nLinguistics.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR, abs/2201.08239.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. 2012.\nMujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on In-\ntelligent Robots and Systems, IROS 2012, Vilamoura,\nAlgarve, Portugal, October 7-12, 2012, pages 5026–\n5033. IEEE.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. CoRR,\nabs/2212.10509.\n5385\nBoshi Wang, Xiang Deng, and Huan Sun. 2022a. Itera-\ntively prompt pre-trained language models for chain\nof thought. CoRR, abs/2203.08383.\nPeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,\nand Xiang Ren. 2023. PINTO: Faithful language rea-\nsoning using prompt-generated rationales. In The\nEleventh International Conference on Learning Rep-\nresentations.\nSiyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming\nZhou, Zhongyu Wei, Zhumin Chen, and Nan Duan.\n2021. From LSAT: the progress and challenges of\ncomplex reasoning. CoRR, abs/2108.00648.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,\nZhiyuan Liu, and Juanzi Li. 2022b. Finding skill\nneurons in pre-trained transformer-based language\nmodels. CoRR, abs/2211.07349.\nXingyao Wang, Sha Li, and Heng Ji. 2022c.\nCode4struct: Code generation for few-shot struc-\ntured prediction from natural language. CoRR,\nabs/2210.12810.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, and Denny Zhou. 2022d. Rationale-\naugmented ensembles in language models. CoRR,\nabs/2207.00747.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, and Denny Zhou. 2022e. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. CoRR, abs/2203.11171.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 845–854,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nTaylor W. Webb, Keith J. Holyoak, and Hongjing Lu.\n2022. Emergent analogical reasoning in large lan-\nguage models. CoRR, abs/2212.09196.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 2300–2344. Association for\nComputational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. In Advances in Neural Information\nProcessing Systems 35: Annual Conference on Neu-\nral Information Processing Systems 2022, NeurIPS\n2022, December 6-14, 2022, virtual.\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and\nJun Zhao. 2022. Large language models are reason-\ners with self-verification. CoRR, abs/2212.09561.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,\nMark O. Riedl, and Yejin Choi. 2022. Reframing\nhuman-ai collaboration for generating free-text ex-\nplanations. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022 , pages 632–658. Association for\nComputational Linguistics.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023. Visual\nchatgpt: Talking, drawing and editing with visual\nfoundation models. CoRR, abs/2303.04671.\nTongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff\nGray, Alejandra Molina, Michael Terry, and Carrie J.\nCai. 2022a. Promptchainer: Chaining large language\nmodel prompts through visual programming. In CHI\n’22: CHI Conference on Human Factors in Com-\nputing Systems, New Orleans, LA, USA, 29 April\n2022 - 5 May 2022, Extended Abstracts, pages 359:1–\n359:10. ACM.\nYuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N.\nRabe, Charles Staats, Mateja Jamnik, and Christian\nSzegedy. 2022b. Autoformalization with large lan-\nguage models. CoRR, abs/2205.12615.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nFrank F. Xu, Uri Alon, Graham Neubig, and Vincent Jo-\nsua Hellendoorn. 2022. A systematic evaluation of\nlarge language models of code. In MAPS@PLDI\n2022: 6th ACM SIGPLAN International Symposium\non Machine Programming, San Diego, CA, USA, 13\nJune 2022, pages 1–10. ACM.\nJingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou,\nand Lei Li. 2021. A survey on green deep learning.\nCoRR, abs/2111.05193.\nJingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing\nZhang, Bing Yin, and Diyi Yang. 2022a. SEQZERO:\nfew-shot compositional semantic parsing with se-\nquential prompts and zero-shot models. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, Seattle, WA, United States, July 10-15,\n5386\n2022, pages 49–60. Association for Computational\nLinguistics.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\nLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. 2023. MM-\nREACT: prompting chatgpt for multimodal reasoning\nand action. CoRR, abs/2303.11381.\nZhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin,\nand Xiaodan Liang. 2022b. LogicSolver: Towards\ninterpretable math word problem solving with log-\nical prompt-enhanced learning. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 1–13, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nZonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik\nCambria, Xiaodong Liu, Jianfeng Gao, and Furu\nWei. 2022c. Language models as inductive reasoners.\nCoRR, abs/2212.10923.\nFu Yao, Peng Hao, and Khot Tushar. 2022. How does\ngpt obtain its ability? tracing emergent abilities of\nlanguage models to their sources. Yao Fu’s Notion.\nXi Ye and Greg Durrett. 2022. The unreliability of ex-\nplanations in few-shot prompting for textual reason-\ning. In Advances in Neural Information Processing\nSystems.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023. Large language mod-\nels are versatile decomposers: Decompose evidence\nand questions for table-based reasoning. CoRR,\nabs/2301.13808.\nOri Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel\nDeutch, and Jonathan Berant. 2023. Answering\nquestions by meta-reasoning over multiple chains\nof thought.\nNathan Young, Qiming Bao, Joshua Bensemann, and\nMichael Witbrock. 2022. Abductionrules: Training\ntransformers to explain unexpected inputs. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages\n218–227. Association for Computational Linguistics.\nPing Yu, Tianlu Wang, Olga Golovneva, Badr\nAlKhamissy, Gargi Ghosh, Mona T. Diab, and Asli\nCelikyilmaz. 2022. ALERT: adapting language mod-\nels to reasoning tasks. CoRR, abs/2212.08286.\nWeizhe Yuan and Pengfei Liu. 2022. restructured pre-\ntraining. CoRR, abs/2206.11147.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. STar: Bootstrapping reasoning with rea-\nsoning. In Advances in Neural Information Process-\ning Systems.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Visual\ncommonsense reasoning. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, pages\n6720–6731. Computer Vision Foundation / IEEE.\nHanlin Zhang, YiFan Zhang, Li Erran Li, and Eric Xing.\n2022. The impact of symbolic representations on in-\ncontext learning for few-shot reasoning. In NeurIPS\n2022 Workshop on Neuro Causal and Symbolic AI\n(nCSI).\nNingyu Zhang, Lei Li, Xiang Chen, Xiaozhuan Liang,\nShumin Deng, and Huajun Chen. 2023a. Multimodal\nanalogical reasoning over knowledge graphs. In The\nEleventh International Conference on Learning Rep-\nresentations.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompt-\ning in large language models. In The Eleventh Inter-\nnational Conference on Learning Representations.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2023c. Multi-\nmodal chain-of-thought reasoning in language mod-\nels. CoRR, abs/2302.00923.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H.\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron C.\nCourville, Behnam Neyshabur, and Hanie Sedghi.\n2022a. Teaching algorithmic reasoning via in-\ncontext learning. CoRR, abs/2211.09066.\nPei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang\nRen, Chris Callison-Burch, Yejin Choi, and Prithvi-\nraj Ammanabrolu. 2022b. An AI dungeon master’s\nguide: Learning to converse and guide with intents\nand theory-of-mind in dungeons and dragons. CoRR,\nabs/2212.10060.\nA Appendix\nA.1 Related Survey\nAs this area is relatively nascent, only a few sur-\nveys exist. Closest to our work, Huang and Chang\n(2022) gives a survey towards reasoning with large\nlanguage models. Dong et al. (2023) organizes and\ndiscusses the advanced techniques of in-context\nlearning. Zhao et al. (2023) reviews the latest ad-\nvancements in Large Language Models (LLMs)\n5387\nReasoning with Language Model Prompting\nTaxonomy\nof Methods\n(§3)\nStrategy Enhanced\nReasoning (§3.1)\nPrompt Engineering\n(§3.1.1)\nSingle-Stage\nContrastive (Paranjape et al., 2021), POTTER (Rajagopal et al., 2021), CoT (Wei et al., 2022b),\nZeroCoT (Kojima et al., 2022), Complexity (Fu et al., 2023b), Multilingual (Shi et al., 2022),\nAuto-CoT (Zhang et al., 2023b), Table (Chen, 2022), AlgoPrompt (Zhou et al., 2022a),\nActive-Prompt (Diao et al., 2023), Automate-CoT (Shum et al., 2023)\nMulti-Stage\niCAP (Wang et al., 2022a), SI (Creswell et al., 2022), Least-to-Most (Zhou et al., 2023),\nMAIEUTIC (Jung et al., 2022), Faithful (Creswell and Shanahan, 2022), Decomposed\n(Khot et al., 2023), Self-Ask (Press et al., 2022), Successive (Dua et al., 2022), LMLP\n(Zhang et al., 2022), LAMBADA (Kazemi et al., 2022), Iter-Decomp (Reppert et al., 2023)\nProcess Optimization\n(§3.1.2)\nSelf-Optimization Calibrator (Ye and Durrett, 2022), Human-AI (Wiegreffe et al., 2022)\nEnsemble-OptimizationSelf-C (Wang et al., 2022e), DIVERSE (Li et al., 2022d), Complexity (Fu et al., 2023b),\nSelf-V (Weng et al., 2022), MCR (Yoran et al., 2023)\nIterative-OptimizationSTaR (Zelikman et al., 2022), LMSI (Huang et al., 2022),\nReflexion (Shinn et al., 2023), Self-Refine (Madaan et al., 2023), REFINER (Paul et al., 2023)\nExternal Engine\n(§3.1.3)\nPhysical Simulator Mind’s Eye (Liu et al., 2023)\nCode Interpreter\nCOCOGEN (Madaan et al., 2022), PAL (Gao et al., 2022), PoT (Chen et al., 2022b),\nFaithful-CoT (Lyu et al., 2023), Versa-Decomp (Ye et al., 2023), SynPrompt\n(Shao et al., 2023), MathPrompter (Imani et al., 2023)\nTool Learning Toolformer (Schick et al., 2023), ART (Paranjape et al., 2023), Chameleon (Lu et al., 2023a)\nKnowledge Enhanced\nReasoning (§3.2)\nImplicit Knowledge\n(§3.2.1)\nGenKnow (Liu et al., 2022c), RAINIER (Liu et al., 2022b), MT-CoT (Li et al., 2022b), PINTO (Wang et al., 2023), TSGP\n(Sun et al., 2022), DecompDistill (Shridhar et al., 2022), Teaching (Magister et al., 2022), Fine-tune-CoT (Ho et al., 2022),\nSpecializing (Fu et al., 2023a)\nExplicit Knowledge\n(§3.2.2)\nLogicSolver (Yang et al., 2022b), V ote-k (SU et al., 2023), PROMPTPG (Lu et al., 2023b), IRCoT (Trivedi et al., 2022),\nRR (He et al., 2023)\nTaxonomy\nof Tasks\n(§5)\nArithmetic\nCoT (Wei et al., 2022b), Self-C (Wang et al., 2022e), STaR (Zelikman et al., 2022), LogicSolver (Yang et al., 2022b), Least-to-Most\n(Zhang et al., 2022), ZeroCoT (Kojima et al., 2022), DIVERSE (Li et al., 2022d), Minerva (Lewkowycz et al., 2022), PROMPTPG\n(Lu et al., 2023b), Complexity (Fu et al., 2023b), Multilingual (Shi et al., 2022), Auto-CoT (Zhang et al., 2023b), LMSI (Huang et al., 2022),\nAlgoPrompt (Zhou et al., 2022a), PAL (Gao et al., 2022), PoT (Chen et al., 2022b), DecompDistill (Shridhar et al., 2022), LMP\n(Beurer-Kellner et al., 2022), Teaching (Magister et al., 2022), Self-V (Weng et al., 2022), Fine-tune-CoT (Ho et al., 2022), Specializing\n(Fu et al., 2023a), Faithful-CoT (Lyu et al., 2023), Toolformer (Schick et al., 2023), SynPrompt (Shao et al., 2023), Active-Prompt (Diao et al., 2023),\nAutomate-CoT (Shum et al., 2023), MathPrompter (Imani et al., 2023), ART (Paranjape et al., 2023), REFINER (Paul et al., 2023),\nSelf-Refine (Madaan et al., 2023)\nCommonsense\nContrastive (Paranjape et al., 2021), POTTER (Rajagopal et al., 2021), CoT (Wei et al., 2022b), GenKnow (Liu et al., 2022c), Self-C\n(Wang et al., 2022e), STaR (Zelikman et al., 2022), Calibrator (Ye and Durrett, 2022), MAIEUTIC (Jung et al., 2022), ZeroCoT (Kojima et al., 2022),\nDIVERSE (Li et al., 2022d), V ote-k (SU et al., 2023), RAINIER (Liu et al., 2022b), Self-Ask (Press et al., 2022), Auto-CoT (Zhang et al., 2023b),\nHuman-AI (Wiegreffe et al., 2022), MT-CoT (Li et al., 2022b), COCOGEN (Madaan et al., 2022), LMSI (Huang et al., 2022), PINTO\n(Wang et al., 2023), TSGP (Sun et al., 2022), Teaching (Magister et al., 2022), Fine-tune-CoT (Ho et al., 2022), IRCoT (Trivedi et al., 2022),\nRR (He et al., 2023), Faithful-CoT (Lyu et al., 2023), Active-Prompt (Diao et al., 2023), Automate-CoT (Shum et al., 2023),\nReflexion (Shinn et al., 2023), MCR (Yoran et al., 2023)\nLogical SI (Creswell et al., 2022), Faithful (Creswell and Shanahan, 2022), LMLP (Zhang et al., 2022),\nSelf-V (Weng et al., 2022), LAMBADA (Kazemi et al., 2022), Faithful-CoT (Lyu et al., 2023),\nSymbolic\nCoT (Wei et al., 2022b), Self-C (Wang et al., 2022e), STaR (Zelikman et al., 2022), Least-to-Most (Zhang et al., 2022),\nZeroCoT (Kojima et al., 2022), Decomposed (Khot et al., 2023), Auto-CoT (Zhang et al., 2023b), PAL (Gao et al., 2022),\nTeaching (Magister et al., 2022), Fine-tune-CoT (Ho et al., 2022), SynPrompt (Shao et al., 2023), Active-Prompt (Diao et al., 2023),\nAutomate-CoT (Shum et al., 2023), ART (Paranjape et al., 2023)\nMultimodal\nScienceQA (Lu et al., 2022a), MarT (Zhang et al., 2023a), IPVR (Chen et al., 2023), Multimodal-CoT (Zhang et al., 2023c),\nKOSMOS-1 (Huang et al., 2023), Visual-ChatGPT (Wu et al., 2023), ViperGPT (Surís et al., 2023), MM-REACT (Yang et al., 2023),\nChameleon (Lu et al., 2023a)\nFigure 8: Taxonomy of Reasoning with Language Model Prompting.\n5388\nand delves into the unresolved challenges that will\nshape future developments. Bhargava and Ng\n(2022) covers methods for commonsense knowl-\nedge reasoning and generation with pre-trained\nLMs. Lu et al. (2022b) reviews the key tasks,\ndatasets, and methods at the intersection of mathe-\nmatical reasoning and deep learning over the past\ndecade. Liang et al. (2022a) surveys knowledge\ngraph reasoning tracing from static to temporal\nand then to multi-modal knowledge graphs. Mi-\nalon et al. (2023) reviews works in which language\nmodels (LMs) are augmented with reasoning skills\nand the ability to use tools. Hamilton et al. (2022)\nconducts a survey of studies implementing neural-\nsymbolic (NeSy) NLP approaches for reasoning\nand so on. Guo et al. (2022) provides a survey\nof several popular works dealing with uncertainty\nreasoning. Qin et al. (2023) concentrates on the\nleverage of external tools by LLMs which is also\ncalled Tool Learning. Other surveys focusing on\nprompt learning (Liu et al., 2022d) or pre-trained\nmodels (Qiu et al., 2020; Du et al., 2022) are also\nrelated to our work.\nUnlike those surveys, in this paper, we conduct\na review of reasoning with LM prompting, hoping\nto systematically understand the methodologies,\ncompare different methods and inspire new ideas.\nA.2 Taxonomy of Methods and Tasks\nWe list the complete taxonomy of reasoning with\nlanguage model prompting from methods and tasks\nin Figure 8.\nA.3 Performance Comparison of LMs with\nDifferent Scales\nTo show the generalization of discussions in §4.1\non different reasoning tasks, we additionally show\nthe performance comparison of LMs with different\nscales on CommonsenseQA (Talmor et al., 2019)\nof commonsense reasoning in Figure 9.\nA.4 Detailed Information of Reasoning\nBenchmarks\nIn § 5, we give a brief overview on benchmarks\nand tasks requiring various reasoning skills. We list\nmore benchmarks and show their key statistics in\nTable 2. Apart from the above-mentioned specific\nreasoning tasks in § 5, there are some benchmarks\n(Lake and Baroni, 2017; Srivastava et al., 2022; Yu\net al., 2022) that can evaluate the model’s more di-\nverse and generalized reasoning capabilities, which\n405060708090\nGPT3-CoTCodex-CoTLaMDA-68B-CoTLaMDA-137B-CoTPaLM-62B-CoT\nCSQAAccuracy(%)Model size540B175B62B\nPaLM-540B-CoT\nFigure 9: Performance of different language model\nscales on commonsense reasoning. Representatively,\nWe show CoT (Wei et al., 2022b) experimental results\non CommonsenseQA (Talmor et al., 2019).\ncan also be included in the category of reasoning\ntasks.\nA.5 Reasoning with ChatGPT\nFigure 10: A test case from GSM8K (Cobbe et al., 2021)\non ChatGPT (OpenAI, 2022).\nFigure 11: A test case from CommonsenseQA (Talmor\net al., 2019) on ChatGPT (OpenAI, 2022).\nRecently, OpenAI (2022) develops ChatGPT, an\nAI chatbot system that has attracted tremendous\nusers. ChatGPT is trained on a massive dataset of\ntext and is able to generate human-like responses to\na wide variety of prompts, the promising approach\nfor which is called Reinforcement Learning from\nHuman Feedback (Ouyang et al., 2022). The back-\nbone of ChatGPT is from a model in the GPT-3.5\n5389\nTask Dataset\nSize\nTrain Valid Test All\nAddSub (Hosseini et al., 2014) 395 - - 395\nSingleOp (Roy et al., 2015) 562 - - 562\nSingleEq (Koncel-Kedziorski et al., 2015) 508 - - 508\nMultiArith (Roy and Roth, 2015) 600 - - 600\nDophin18k (Huang et al., 2016) 18,460 - - 18,460\nMAWPS (Koncel-Kedziorski et al., 2016) 1,921 - - 1,921\nMath23k (Wang et al., 2017) 23,161 - - 23,161\nAQUA-RAT (Ling et al., 2017b) 97,467 - 254 97,721\nMathQA (Amini et al., 2019) 29,807 4,471 2,981 37,259\nDROP (Dua et al., 2019) 5,850 - - 5,850\nASDiv (Miao et al., 2020) 1,217 - - 1,217\nGSM8K (Cobbe et al., 2021) 7,473 - 1,319 8,792\nSV AMP (Patel et al., 2021) 1,000 - - 1,000\nMATH (Hendrycks et al., 2021) 7,500 - 5,000 12,500\nNumGLUE (Mishra et al., 2022b) 101,835 - - 101,835\nArithmetic Reasoning\nLila (Mishra et al., 2022a) 133,815 - - 133,815\nLast Letter Concatenation (Wei et al., 2022b) - - - -\nCoin Flip (Wei et al., 2022b) - - - -Symbolic Reasoning\nReverse List (Wei et al., 2022b) - - - -\nARC (Clark et al., 2018) 3,370 869 3,548 7,787\nOpenBookQA (Mihaylov et al., 2018) 4,957 500 500 5,957\nCommonsenseQA (Talmor et al., 2019) 9,741 1,221 1,140 12,102\nPIQA (Bisk et al., 2020) 16,000 2,000 3,000 21,000\nCommonsense Reasoning\nStrategyQA (Geva et al., 2021) 2,290 - 490 2,780\nRuleTaker (Clark et al., 2020) 14,135 2,019 3,038 20,192\nProofWriter (Tafjord et al., 2021) - - - -\nEntailmentBank (Dalvi et al., 2021) 1,313 187 340 1,840\nCLUTRR (Sinha et al., 2019) 6,016 - - 6,016\nLogical Reasoning\nDEER (Yang et al., 2022c) 1,200 - - 1,200\nVCR (Zellers et al., 2019) 212,923 26,534 25,263 264,720\nVisualCOMET (Park et al., 2020) 1,174,063 146,332 145,309 1,465,704\nVLEP (Lei et al., 2020) 20,142 4,392 4,192 28,726\nPMR (Dong et al., 2022) 12,080 1,538 1,742 15,360\nMultimodal Reasoning\nScienseQA (Lu et al., 2022a) 12,726 4,241 4,241 21,208\nTable 2: An overview of benchmarks and tasks on reasoning.\nFigure 12: A test case from Last Letter Concatenation\n(Wei et al., 2022b) on ChatGPT (OpenAI, 2022).\nlarge LM series8. In order to savor the reasoning\nability of large LMs more realistically, we conduct\nsome case tests on ChatGPT. Concretely, we pick\nout a piece of data from GSM8K (Cobbe et al.,\n2021), CommonsenseQA (Talmor et al., 2019)\nand Last Letter Concatenation (Wei et al., 2022b)\nwhich respectively represent arithmetic reasoning,\ncommonsense reasoning, and symbolic reasoning.\n8https://beta.openai.com/docs/\nmodel-index-for-researchers\nThen we test each of the selected data on ChatGPT\ndirectly. Results can be seen in Figure 10-12.\nFigure 10 shows that given a math problem in\nGSM8K (Cobbe et al., 2021), ChatGPT outputs\na reasoning process and a correct answer with-\nout in-context exemplars. This blazes its powerful\narithmetic reasoning ability. The reasoning process\nhas the same format as the gold label in GSM8K,\nindicating that GSM8K may be contained in the\ntraining corpus of ChatGPT.\nIn Figure 11, we test ChatGPT on a piece of\ndata in CommsonsenseQA (Talmor et al., 2019). It\nnot only gives the correct answer but additionally\ndetails why each option is right or wrong, which\ndoes not appear in the gold label of the dataset. This\ndemonstrates the strong commonsense reasoning\nability of ChatGPT.\nFigure 12 is a case in Last Letter Concatenation\n(Wei et al., 2022b). We observe that although Chat-\n5390\nGPT gives a detailed and accurate description of\nlast letter concatenation, it fails to answer the given\nquestion, showing that its symbolic reasoning ca-\npability is not as excellent as the above two.\nA.6 Reasoning using EasyInstruct\nFigure 13: A test case from GSM8K (Cobbe et al., 2021)\nusing EasyInstruct (Ou et al., 2023).\n5391\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8 (Limitations)\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nabstract: Abstract; Introduction: section 1\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nhttps://www.grammarly.com/\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0017 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5392\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5393",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.8794947862625122
    },
    {
      "name": "Zhàng",
      "score": 0.682051956653595
    },
    {
      "name": "Computer science",
      "score": 0.542152464389801
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5093896985054016
    },
    {
      "name": "Computational linguistics",
      "score": 0.46622610092163086
    },
    {
      "name": "Natural language processing",
      "score": 0.391756534576416
    },
    {
      "name": "Linguistics",
      "score": 0.3904467821121216
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3415960371494293
    },
    {
      "name": "Programming language",
      "score": 0.32360363006591797
    },
    {
      "name": "Philosophy",
      "score": 0.18759313225746155
    },
    {
      "name": "History",
      "score": 0.13079732656478882
    },
    {
      "name": "Physics",
      "score": 0.07803836464881897
    },
    {
      "name": "China",
      "score": 0.07742714881896973
    },
    {
      "name": "Archaeology",
      "score": 0.07086539268493652
    },
    {
      "name": "Geology",
      "score": 0.06600233912467957
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ]
}