{
  "title": "Evaluating the Robustness of Neural Language Models to Input Perturbations",
  "url": "https://openalex.org/W3198690080",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2317807276",
      "name": "Milad Moradi",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A40952608",
      "name": "Matthias Samwald",
      "affiliations": [
        "Medical University of Vienna",
        "Austrian Research Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2989473642",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W3035441470",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3128686514",
    "https://openalex.org/W3021046385",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2787487383",
    "https://openalex.org/W3020242586",
    "https://openalex.org/W3015001695",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2972539067",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W4294536576",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2989344603",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2508156266",
    "https://openalex.org/W3120061794",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W3087231533",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W3035219095",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3104570147",
    "https://openalex.org/W2963916869",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems' robustness.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1558–1570\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1558\n \n \n \n \nAbstract \nHigh-performance neural language models \nhave obtained state-of-the-art results on a \nwide range of Natural Language \nProcessing (NLP) tasks. However, results \nfor common benchmark datasets often do \nnot reflect model reliability and robustness \nwhen applied to noisy, real-world data. In \nthis study, we design and implement \nvarious types of character-level and word-\nlevel perturbation methods to simulate \nrealistic scenarios in which input texts may \nbe slightly noisy or different from the data \ndistribution on which NLP systems were \ntrained. Conducting comprehensive \nexperiments on different NLP tasks, we \ninvestigate the ability of high-performance \nlanguage models such as BERT, XLNet, \nRoBERTa, and ELMo in handling different \ntypes of input perturbations. The res ults \nsuggest that language models are sensitive \nto input perturbations and their \nperformance can decrease even when small \nchanges are introduced. We highlight that \nmodels need to be further improved and \nthat current benchmarks are not reflecting \nmodel robu stness well. We argue that \nevaluations on perturbed inputs should \nroutinely complement widely -used \nbenchmarks in order to yield a more \nrealistic understanding of NLP systems ’ \nrobustness. \n1 Introduction \nHigh-performance deep neural language models \nsuch as BER T (Devlin et al., 2018 ), XLNet (Z. \n                                                           \n1 https://rajpurkar.github.io/SQuAD-\nexplorer/ \nYang et al., 2019 ), and GPT -2 (Radford et al., \n2019) have brought breakthroughs to a wide range \nof Natural Language Processing (NLP) tasks \nincluding text classification, sentiment analysis, \ntextual entailment, natural language inference , \nmachine translation, and question answering. Their \nimmense ability in capturing various linguistic  \nproperties has led the se state-of-the-art language \nmodels to master different NLP tasks, even \nsurpassing human accuracy on some benchmarks \nsuch as SQuAD1. \nHowever, recent studies have revealed that there \nis a gap between performing well on benchmarks \nand actually working under real -world situations \n(Belinkov and Bisk, 20 18; Ribeiro et al., 2020 ). \nEven a well -trained, high-performance deep \nlanguage model  can be sensitive to negligible \nchanges in the input that cause the model to make \nerroneous decisions (M. Sun et al., 2018 ). This \nraises serious concer ns regarding the \nrobustness/reliability of neural language models \nutilized in real-world applications.  The terms \n‘robustness’ and ‘reliability’ refer to the ability of a \nsystem to perform con sistently well in situations \nwhere changes to input should not cause a change \nin the system’s output, or the system is expected to \nproperly reflect the change and produce a correct \noutcome. \nApplying automatic or human -controlled \nperturbations to textual inputs has been shown to \nbe effective for evaluating the robustness of NLP \nsystems, investigating their vulnerabilities , and \nfinding their bugs. Recently, CheckList (Ribeiro et \nal., 2020 ) provided a framework for behavioral \ntesting of NLP systems  inspired by black -box \nEvaluating the Robustness of Neural Language Models to Input \nPerturbations \n \n \nMilad Moradi, Matthias Samwald \nInstitute for Artificial Intelligence \nMedical University of Vienna, Austria \n{milad.moradivastegani, matthias.samwald}@meduniwien.ac.at \n \n \n \n \n \n1559\n \n \n \n \ntesting in software engineering. CheckList enabled \ngenerating new (perturbed) test samples through \nabstracting different test types aimed at  testing \nlinguistic capabilities. Other studies focused on \nevaluating robustness to perturbed inputs for \nmachine translation (Belinkov and Bisk, 2018; Niu \net al., 2020 ), perturbation sensitivity analysis for \ndetecting unintended model biases (Prabhakaran et \nal., 2019 ), or robustness to adversarial \nperturbations (Alshemali and Kalita, 2020 ; \nEbrahimi et al., 2018; Liang et al., 2018). However, \na comprehensive methodology for evaluating the \nperformance of NLP models under real -world \nconditions is still missing. \nIn a realistic scenario, the input text may contain \ntypos and misspellings that should not cause a ny \nchanges in the NLP system ’s outcome . Minor \ngrammatical errors may appear in the text, but the \nsemantics is still preserved, therefore, the NLP \nsystem is expected to treat the input as it was error-\nfree. Some deliberate or unintentional changes may \nmodify the semantics, and the NLP model is \nexpected to reflect the change s in the outcome. \nThese are only few examples of natural noise in \ntext data that NLP systems should have the ability \nto properly deal with.  \nIn this paper, we design and implement a wide \nrange of character-level and word-level systematic \nperturbations to textual inputs in order to simulate \ndifferent types of noise that a NLP system may face \nin real-world use cases.  Conducting extensive \nexperiments on various NLP tasks, we investigated \nthe ability of four neural language models, i.e. \nBERT, RoBERTa, XLNet, and ELMo, in handling \nslightly perturbed inputs. The results reveal that the \nneural models are unstable to small changes that \ncan be easily handled by humans, e.g. misspellings, \nmissing words, repeated words, synonym s, etc. \nThe systematic input perturbations can expose the \nvulnerabilities of NLP systems and bring more \ninsights into how high -performance models \nbehave when they encounter noisy yet \nunderstandable inputs. This study suggests that the \nperformance of NLP models should not be \noverestimated by only relying on accuracy scores \nobtained on benchmark datasets. \nSimilar to CheckList, our pertur bation \nframework treats NLP systems as black -boxes. \nThis facilitates comparison of different models, \nwithout needing to know the model structure and \ninternals. CheckList focuses on testing linguistic \ncapabilities of NLP systems, e.g. handling \ncoreferences, identifying named entities, semantic \nrole labeling, and vocabulary. On the other hand, \nour perturbation methods aim at evaluating the \nrobustness of NLP systems to  noisy inputs. Our \ninput perturbation framework can act as a \ncomplement to the CheckList testing methodology. \nIn CheckList, many test types rely on creating \nsynthetic samples from scratch by the user, which \nis a time -consuming task, and needs much \ncreativity and effort. Moreover, synthetic samples \nmay suffer from low coverage (Ribeiro et al., \n2020). However, most of the perturbation methods \nintroduced in  this paper do not need human \nintervention; they can  automatically generate \nperturbed samples that still preserve the semantics \nand are sufficiently meaningful to users. \nSome types of perturbation utilized in this work \nwere already tested in previous work on adversarial \nattacks on NLP systems (Zeng et al., 2020; Zhang \net al., 2020 ). However, adversarial p erturbations \nare considered worst-case scenarios that d o not \noccur frequently  in real -world situations, \nrepresenting a very specific type of noise (Fawzi et \nal., 2016). In order to generate effective adversarial \nexamples, most attack methods need to have access \nto the NLP model structure, internal weights, and \nhyperparameters, which may not be possible in \nevery testing scenario (Zhang et al., 2020 ). \nFurthermore, adversarial perturbations should not \nbe perceived by humans (Liang et al., 2018). This \nis a serious challenge, since even small changes to \na text may be easily recognized by the user. \nTo the best of our knowledge, this paper is the \nfirst study that presents empirical results achieved \nwith a comprehensive set o f non -adversarial \nperturbation methods for testing robustness of NLP \nsystems on non -synthetic text . An important \ncontribution of this work is to evaluate the \nrobustness of several high -performance language \nmodels on various NLP tasks using different types \nof character -level and word -level input \nperturbations. Moreover, to ascertain the \nusefulness of the perturbations (i.e. how effectively \nthey can be used to automatically generate \nmeaningful and understandable perturbed \nsamples), we conducted an extensive user study. \n2 NLP tasks \nIn our experiments, we used five datasets covering \nfive different NLP tasks. Table 1 summarizes some \nstatistics of the datasets. A short description of the \ndatasets is given in the following. \n1560\n \n \n \n \nTREC (Li and Roth, 2002 ) is a  Text \nClassification (TC) dataset containing more than \n6,000 questions and 50 different class labels that \nspecify the type of questions. \nStanford Sentiment Treebank (SST) (Socher \net al., 2013) is a Sentiment Analysis (SA) dataset \ncontaining more than 11,000 movie reviews from \n‘Rotten Tomatoes’. Every review is classified into \none of the five classes: very positive , positive, \nneutral, negative, and very negative. \nCoNLL-20032  is a Named Entity Recognition \n(NER) dataset containing news stories from the \nReuters corpus with  more than 200K tokens \nannotated as Person, Organization, Location, \nMiscellaneous, or Other. \nSTS benchmark (Cer et al., 2017) is a Semantic \nSimilarity (SS) dataset comprising of more than 8K \ntext pairs extracted from image captions, news \nheadlines, and user forums. Each pair of sentences \nis assigned a similarity score between 0 and 5. \nWikiQA (WQA) (Y . Yang et al., 2015) is a  \nQuestion Answering (QA) dataset composed of \nmore than 3,000 questions and 29,000 sentences as \nanswers extracted from Wikipedia.  \n3 Language models \nIn our experiments, we utilized fo ur neural \nlanguage models shown to be effective in learning \nbidirectional contexts and obtained state-of-the-art \nresults during recent years: \nBERT (Devlin et al., 2018) is composed of deep \nencoder transformer layers  and uses two \npretraining objectives, i.e. masked language \nmodelling and next sentence prediction. We used \nthe BERTLARGE architecture (along with the cased \nmodel) containing 24 transformer layers, 1024 \nhidden units per layer, 1 6 attention heads per \nhidden unit, and 340 million parameters. \nRoBERTa (Liu et al., 2019 ) uses a model \narchitecture similar to  BERT, but adopts an \noptimized pretraining approach. It was pretrained \non more data, with bigger batch sizes and longer \nsequences than BERT . Furthermore, the next \nsentence prediction objective was removed and a \ndynamic masking strategy replaced the basic \nmasking method.  We used RoBERTa LARGE that \nfurther optimizes the same model as BERTLARGE. \nXLNet (Z. Yang et al., 2019 ) utilizes decoder \ntransformers and adopts a permutation langu age \n                                                           \n2https://github.com/synalp/NER/tree/mast\ner/corpus/CoNLL-2003 \nmodelling approach along with generalized \nautoregressive pretraining.  We used the \nXLNetLARGE model, with the same architecture \nhyperparameters and model size as BERTLARGE.  \nELMo (Peters et al., 2018) is a contextualized \nword representation method that utilizes character \nconvolutions along with shallow concatenation of  \nbackward and forward LSTMs to implement \nbidirectional language modeling.  We used the \noriginal ELMo model composed of two highway \nlayers with an LSTM hidden size of 4096, output \nsize of 512, and a total parameters of 93.6 million. \nThe contextualized embeddings computed by \nELMo were fed into a dense layer containing 128 \nhidden units followed by an output layer with a \nsoftmax activation in the TC and QA tasks, a linear \nactivation in the SA and SS tasks, and CRF layer \nwith a linear activation in the NER task.  \nWe retrieved the pretrained models, fine-tuned \nthem separately on each downstream task using the \ntraining and development sets, and tested them on \nthe test sets. We utilized the Huggingface \ntransformers (Wolf et al., 2020 ) and FARM 3 \nlibraries to implement the transformer -based \nmodels. A complete list of hyperparameter values \nis presented in Appendix A. \n4 Perturbation methods \nWe designed and implemented various character-\nlevel and word -level perturbation methods that \nsimulate different types of noise an NLP system \nmay encounter  in real -world situations. The \nperturbations can be produced for every dataset \nregardless of the underlying language model or \nNLP system being tested.  Table 2 presents an \nexample for every perturbation method.  The \nperturbation methods were implemented in Python \nusing the NLTK library. The source code is \navailable at https://github.com/mmoradi-\niut/NLP-perturbation. \n3https://github.com/deepset-ai/FARM \nDataset Task Train Dev Test Eval. Measure \nTREC TC 5,000 452 500 Micro F1-score \nSST SA 8,544 1,101 2,210 Accuracy \nCoNLL NER 14,041 3,250 3,453 F1-score \nSTS SS 5,749 1,500 1,379 Pearson \nWQA QA 2,117 296 630 F1-score \nTable 1:  The main statistics of the datasets used to \nconduct the perturbation experiments. \n \n \n1561\n \n \n \n \nAlmost all the character -level perturbations \npresented here were already tested in adversarial \nattack scenarios (Heigold et al., 2018; Zeng et al., \n2020; Zhang et al., 2020 ), but were not yet \nimplemented in a non -adversarial testing \nframework, except the misspelling perturbation \nimplemented by CheckList. Among the word-level \nperturbations, Deletion, Repetition, \nSingular/plural verbs, Word order, and V erb tense \nwere not already used to test the robustness. \nHowever, Negation was included in CheckList, \nand Replacement with Synonyms  was used for \nadversarial attack (Dong et al., 2020 ; Ren et al., \n2019).  \n4.1 Character-level perturbation \nThese perturbation methods randomly select a \nword, denoted as Wordi, and apply perturbations to \nits characters. They are described in the following. \nInsertion. A character is randomly selected and \ninserted in a random position (except the first and \nlast position) if Wordi contains at least three \ncharacters. \nDeletion. A character is randomly selected and \ndeleted if Wordi contains at least three characters. \nThe last and first characters of Wordi are never \ndeleted. \nReplacement. A character is randomly selected \nand is replaced by an adjacent character on the \nkeyboard. \n                                                           \n4https://en.wikipedia.org/wiki/Wikipedia\n:Lists_of_common_misspellings/For_machin\nes/ \nSwapping. A character is randomly selected and \nswapped with the adjacent right or left character in \nWordi. \nRepetition. A character in a random position \n(except the first and last position) is selected and a \ncopy of it is inserted right after the selected \ncharacter. \nCommon misspelled words. If a word in the \ninput text appears in the Wikipedia corpus of \ncommon misspelled words4 , it is replaced by its \nmisspelling. \nLetter case changing toggles the letter case, i.e. \nconverts a lower case character to its upper case \nform and vice versa. The lett er case changing is \ndone for eith er the first or all the characters of \nWordi. The type of letter case changing is specified \nin a random manner.  \n4.2 Word-level perturbation \nDeletion randomly selects a word from the input \nsample and removes it.  \nRepetition selects a random word, makes a \ncopy of it, and inserts it right after the selected \nword. \nReplacement with synonyms replaces words \ncontained in the sample by their synonyms \nextracted from the WordN et lexical database \n(Miller, 1995). \nNegation. It identifies verbs in the sample, then \ninjects negations by converting positiv e verbs to \nnegative, or removes negation by conver ting \nnegative verbs to positive. The goal is to \nPerturbation  Original text Perturbed text \nCharacter-level \nInsertion Who was the first governor of Alaska? Who was the firsdt governor of Alaska? \nDeletion Mercury, what year was it discovered? Mercury, what year was it discovred? \nReplacement Who is the Prime Minister of Canada? Who is the Prime Monister of Canada? \nSwapping What is the primary language in Iceland? What is the primary lnaguage in Iceland? \nRepetition How many hearts does an octopus have? How many heartts does an octopus have? \nCMW What kind of gas is in a fluorescent bulb? What kind of gas is in a florescent bulb? \nLCC How many hearts does an octopus have? How many hearts does an OCTOPUS have? \nWord-level \nDeletion How much was a ticket for the Titanic? How much a ticket for the Titanic? \nRepetition What is another name for vitamin B1? What is another name name for vitamin B1? \nRWS What precious stone is a form of pure carbon? What valued rock is a form of pure carbon? \nNegation What planet is known as the “red” planet? What planet is not known as the “red” planet? \nSPV What does a barometer measure? What do a barometer measure? \nV erb tense Why in tennis are zero points called love? Why in tennis were zero points called love? \nWord order What is the most common eye color? What is the common most color eye? \nTable 2:  Character-level and word-level perturbation examples from the TREC question classification dataset. \nCMW: Common Misspelled Words, LCC: Letter Case Changing, RWS: Replacement With Synonyms, SPV: \nSingular/Plural Verbs. \n \n \n1562\n \n \n \n \ninvestigate the ability of the NLP system in \nadapting its outcome to reflect the injected or \nremoved negation. \nThis perturbation method operates based on a \nset of rules that assess verbs, subjects, and verb \ntenses based on POS tags, then applies an \nappropriate rule to construct the test sample. For \nexample, if the POS tag of a verb is VBZ, the verb \nappears in the third person simple present form. \nTherefore, the verb is replace by [does not + VBP] \nwhere VBP is the basic form of the verb, in order \nto inject negation into the sample. \nSingular/plural verbs. It simulates a common \nerror in real use cases, i.e. using plural form of a \nverb instead of the singular form, and vice versa, \nusually with a third -person subject. This  \nperturbation does not  usually change the text ’s \nmeaning in most NLP tasks if the task does not rely \non the subject-verb agreement. Therefore the NLP \nsystem should treat the perturbed sample as an \nunperturbed text. \nWord order. It randomly selects M consecutive \nwords from the sample and changes the order in \nwhich they appear in the text. The goal is to \ninvestigate whether the NLP system is sensitive to \nword ordering or it only decides based on the \npresence of words in the input. \nVerb tense . It converts present simple or \ncontinuous verbs to their corresponding past \nsimple or continuous forms, or vice versa. The goal \nis to assess the sensitivity of the NLP system to  \nchanging the verb tense  in tasks where the verb \ntense is not important to the output. In this case, the \nsystem’s output should not change after modifying \nthe verb tense. This method first extracts POS tags \nto identify verbs and their subjects. It then converts \nthe verb tense using the mlconjug3 package and \nreconstruct the sentence with the new verb tense.  \n5 Experimental results \nAll the experiments were performed on a computer \nwith an Intel Core i5-9600K CPU at 3.70GHz, 32 \nGB of RAM, and a GeForce RTX 2080 Ti graphic \ncard (GPU) with 11 GB dedicat ed memory. \nPerturbation methods ran on CPU; fine-tuning on \ntraining sets, and evaluating  on test sets and \nperturbed samples ran on GPU. \n5.1 Performance on perturbed inputs \nSince it has been proven that sentences that contain \nfew typos, misspellings, or minor c haracter-level \nerrors can be still fully understandable to humans \n(Belinkov and Bisk, 2018 ; Xu and Du, 2020 ), \ncharacter-level perturbations are not expec ted to \nchange the text ’s meaning  in most cases . \nTherefore, they can be automatically produced and \nused for testing the robustness of NLP systems. \nOn the other hand, some word -level \nperturbations may change the text ’s meaning. \nTask LM Test set \nCharacter-level perturbation methods \nInsertion Deletion Replace Swap Repeat CMW LCC \nTC \nBERT 90.4 77.4 76.2 76.1 76.5 78.8 58.4 78.3 \nRoBERTa 93.1 79.2 78.9 76.3 76.7 80.8 60.5 78.9 \nXLNet 92.0 78.1 78.3 76.5 75.2 80.2 61.5 77.4 \nELMo 84.8 80.4 78.5 74.7 75.6 79.6 61.9 80.8 \nSA \nBERT 92.2 77.1 75.6 75.5 78.3 77.9 62.0 76.7 \nRoBERTa 94.0 79.3 76.8 75.1 76.2 79.3 64.0 78.3 \nXLNet 93.1 78.3 78.7 75.9 73.8 81.1 65.6 78.9 \nELMo 87.6 79.7 79.0 76.2 78.1 78.4 64.1 79.1 \nNER \nBERT 92.6 83.6 80.7 81.4 82.5 81.9 71.3 81.2 \nRoBERTa 93.3 84.3 80.9 81.7 83.1 82.4 71.8 81.5 \nXLNet 92.7 83.9 81.1 81.3 82.7 81.5 71.6 81.3 \nELMo 90.2 83.0 80.2 80.9 82.2 81.3 70.8 81.0 \nSS \nBERT 82.5 72.9 71.5 73.0 74.3 74.2 68.6 73.8 \nRoBERTa 83.9 73.5 72.8 73.6 75.1 74.7 69.5 74.9 \nXLNet 83.3 73.3 72.0 73.2 74.6 74.1 67.9 74.4 \nELMo 80.7 71.1 70.9 72.3 73.8 72.5 67.0 72.6 \nQA \nBERT 91.6 82.7 80.5 81.1 81.9 79.8 68.6 80.7 \nRoBERTa 94.9 84.1 81.7 82.9 83.2 81.6 72.5 84.0 \nXLNet 93.4 83.5 81.1 82.3 81.9 82.8 71.5 83.3 \nELMo 85.5 80.6 79.5 76.0 78.3 80.1 67.9 81.1 \nTable 3:  Performance of the language model s on the test sets and character -level perturbed samples of the \ndownstream tasks. For every task and every perturbation method, the highest score is shown in bold face. \nCMW: Common Misspelled Words, LCC: Letter Case Changing. \n \n \n1563\n \n \n \n \nConsequently, the perturbed sa mples should be \nmonitored to make sure they are still meaningful \nwith respect to the NLP task at hand, and are \nconsistent with the original label in the dataset. \nOtherwise, they should not be used for testing the \nrobustness, or the label should be changed to \nreflect the change and preserve the consistency. \nWe separately applied every character -level \nperturbation method to all test samples in a dataset, \nand all the resulting perturbed samples were used \nto evaluate the robustness of the language models. \nA hyperparameter named Perturbation Per Sample \n(PPS) specified the maximum number of \nperturbations in a sample. \nWe monitored and filtered perturbed samples \nresulted from three word -level perturbations that \nmay change the text ’s meaning. These \nperturbations are  Deletion, Negation, and \nReplacement with synonym. For every sample \nwhose meaning was changed by the se three \nmethods, and a change in the test set label was \nnecessary to preserve consistency, we altered the \nlabel if it was applicable. If a proper label could not \nbe assigned to the perturbed sample or the resulting \ntext was no longer meaningful, we excluded the \nsample from the evaluations. Since monitoring and \nfiltering every single perturbed sample was \nextremely time -consuming (such that \napproximately one minute was needed on average \nto check the meaningfulness of a perturbed sample \nand its consistency with the test set label ), we \ncorrected labels and filtered perturbed samples for \nthe above three methods until 200 samples were \ncollected for every dataset;  then we used these \nsamples to evaluate the models on perturbed \ninputs.  We performed this manual curation of \nperturbed samples for all values of PPS that we \nexperimented, i.e. values in the range  [1, 4].  \nAppendix B presents the number of perturbed \nsamples checked in the manual curation procedure \nuntil reaching 200 test samples for every dataset \nand different values of PPS. The manual curation \nwas performed by three annotators who had \nsufficient English language knowledge to properly \njudge about the meaningfulness and consistency of \nperturbed samples. \nSince the rest of word-level perturbations are not \nexpected to change the text’s meaning with respect \nto the NLP tasks in our experiments , we did not \nmonitor and filter them; they were produced and \nused automatic ally. Again, the PPS \nhyperparameter controlled the maximum number \nof perturbations in every sample.  \nTable 3 and Table 4 present the performance of \nthe language models on character-level and word-\nlevel perturbed samples, respectively. These results \nare reported for PPS=1. The performance of the \nlanguage models on original, unperturbed test sets \nTask LM Test set \nWord-level perturbation methods \nDeletion Repeat RWS Negation SPV VT WO \nTC \nBERT 90.4 75.1 89.3 65.7 89.1 88.2 89.0 74.5 \nRoBERTa 93.1 76.2 88.7 73.2 90.3 89.5 89.4 78.5 \nXLNet 92.0 76.2 87.5 72.7 89.4 89.0 89.6 83.1 \nELMo 84.8 72.9 82.8 75.1 83.5 83.6 81.2 62.9 \nSA \nBERT 92.2 73.7 87.6 67.5 84.6 88.2 90.1 76.4 \nRoBERTa 94.0 74.5 90.1 74.2 83.9 88.7 88.6 77.5 \nXLNet 93.1 74.7 88.5 74.1 82.3 88.6 89.3 83.8 \nELMo 87.6 72.0 80.6 73.1 75.4 84.6 82.9 65.9 \nNER \nBERT 92.6 81.4 83.1 74.1 85.3 88.2 89.1 70.7 \nRoBERTa 93.3 82.3 83.9 74.5 85.8 88.6 89.4 71.1 \nXLNet 92.7 81.9 83.7 73.9 85.6 88.3 88.7 74.8 \nELMo 90.2 79.7 82.1 69.3 82.4 85.1 84.9 68.5 \nSS \nBERT 82.5 72.6 74.1 69.4 68.5 75.2 75.6 72.0 \nRoBERTa 83.9 74.1 74.8 70.0 69.2 75.7 76.7 73.9 \nXLNet 83.3 73.3 74.5 69.8 68.7 75.8 76.2 75.3 \nELMo 80.7 69.8 71.7 67.4 66.0 73.2 72.8 72.6 \nQA \nBERT 91.6 78.4 89.6 71.5 84.9 88.0 90.3 76.7 \nRoBERTa 94.9 79.9 89.3 78.1 86.5 89.7 91.2 79.0 \nXLNet 93.4 79.2 89.5 77.3 86.1 89.1 90.9 85.8 \nELMo 85.5 73.5 81.4 75.0 82.7 84.1 81.9 67.3 \nTable 4:  Performance of the language models on the test sets and word -level perturbed samples of the \ndownstream tasks. For every task and every perturbation method, the highest score is shown in bold face. For \nthree perturbation methods, i.e.  Deletion, Negation, and RWS, 200 perturbed samples were used in the \nexperiments. RWS: Replacement With Synonyms, SPV: Singular/Plural Verbs , VT: Verb Tense, WO: Word \nOrder. \n \n \n1564\n \n \n \n \nis also reported in both tables for every NLP task. \nWe performed five separate fine-tuning runs to test \nif the performance of the NLP models on the \noriginal test set and perturbed samples vary \nbetween individual runs. Since there was no \nstatistically significant difference between multiple \nruns (with respect to a t -test with a significance \nlevel of p=0.05), we only report the results of the \nfirst fine -tuning and testing run.  The language \nmodels were neither pretrained nor fine-tuned on \nperturbed samples. The perturbation methods were \nonly applied to the test sets.  As the results show, \nthe language models are sensitive to the \nperturbations and their performance decreases \nwhen the input is slightly noisy.  However, \nRoBERTa still performs better than the other \nmodels, and ELMo obtains the lowest scores in \ngeneral.  \nThe results suggest that some language models \ncan handle specific types of perturbation more \neffectively than other models. ELMo obtains \nhigher scores than BERT and even performs on par \nwith XLNet and RoBERTa on some character -\nlevel perturbations. This can be due to its pure  \ncharacter-based re presentation that enables the \nmodel to use morphological clues, leading to a \nmore robust model against character-level noises.  \nXLNet is shown to handle perturbations to word \nordering more efficiently than the others. This can \nbe an effect of the permutation language modelling \nthat may allow the model to still capture the \ncontext and perform more accurately when some \ncontext words appear in a different order.  The \nresults also suggest those models that were \npretrained on larger corpora such as RoBERTa and \nXLNet are more robust when words are replaced \nby their synonyms.  Furthermore, when the \nnegation perturbation has more impact on the task \nat hand, e.g. sentiment analysis, the models are less \nstable and handle the noise less efficiently than on \nother tasks. Observing the results, we can also \npoint out the LSTM -based model, i.e. ELMo, is \nmore sensitive to the order of words in a s ample \nthan the transformer-based models.   \nTable 5 presents the absolute decrease in the \nperformance of the language models for different \nPPS values in the range [1, 4]. For every language \nmodel, the average of absolut e decrease in  \nperformance is separately reported  on character-\nlevel and worl-level perturbations for every NLP \ntask. As can be shown, the models are generally \nmore sensitive to character-level perturbations than \nword-level ones. Perturbed inputs causes the \nmodels to make erronous outcomes on th e \nsentiment analysis task more often than on the \nother tasks . On the other hand, the question \nanswering task suffers less than the other tasks \nfrom noisy inputs.  \nFigure 1  represents six examples for which \nperturbations to the input led the RoBERTa model \nTask LM \nCharacter-level perturbations  Word-level perturbations \nPPS=1 PPS=2 PPS=3 PPS=4 PPS=1 PPS=2 PPS=3 PPS=4 \nTC \nBERT −15.8 −17.2 −18.0 −18.3  −8.8 −10.2 −13.1 −13.8 \nRoBERTa −17.1 −17.9 −18.5 −18.9  −9.4 −11.0 −12.7 −13.3 \nXLNet −16.6 −17.4 −19.2 −19.7  −8.0 −10.3 −12.4 −12.9 \nELMo −8.8 −10.0 −11.2 −11.8  −7.3 −9.1 −11.5 −13.2 \nSA \nBERT −17.4 −18.9 −20.1 −21.3  −11.0 −13.5 −15.1 −16.7 \nRoBERTa −18.4 −19.7 −21.2 −21.9  −11.5 −12.9 −14.2 −14.8 \nXLNet −17.0 −19.4 −20.6 −21.7  −10.0 −12.4 −14.3 −15.6 \nELMo −11.2 −14.4 −16.0 −17.5  −11.2 −13.8 −14.9 −16.1 \nNER \nBERT −12.2 −14.6 −16.3 −16.9  −10.8 −12.7 −14.0 −14.9 \nRoBERTa −12.4 −14.0 −14.8 −16.7  −11.0 −12.5 −13.6 −14.3 \nXLNet −12.2 −13.8 −14.5 −15.0  −10.2 −11.9 −13.1 −14.0 \nELMo −10.2 −12.5 −13.1 −13.8  −11.3 −13.0 −15.2 −16.1 \nSS \nBERT −9.8 −11.3 −12.9 −13.6  −10.0 −12.2 −13.8 −14.5 \nRoBERTa −10.4 −11.8 −13.0 −14.3  −10.4 −12.1 −13.2 −14.1 \nXLNet −10.5 −12.0 −13.2 −14.1  −9.9 −11.4 −12.8 −13.3 \nELMo −9.2 −10.6 −11.7 −13.2  −10.2 −12.3 −13.9 −15.4 \nQA \nBERT −12.2 −14.2 −15.0 −16.4  −8.8 −10.2 −12.5 −13.0 \nRoBERTa −13.4 −15.1 −15.8 −16.5  −10.1 −11.0 −12.3 −13.5 \nXLNet −12.4 −13.5 −16.4 −18.9  −7.9 −9.5 −11.2 −13.1 \nELMo −7.8 −9.7 −11.3 −12.0  −7.5 −10.1 −12.0 −13.9 \nTable 5:  Absolute decrease in the performance of the language models, on different NLP tasks, for character-\nlevel and word-level perturbations, and with different values of the hyperparameter Perturbation per Sample \n(PPS). For every task and every value of the hyperparameter PPS, the lowest decrease in the performance is \nshown in bold face. \n \n \n1565\n \n \n \n \nto make  wrong decisions, but the model made \ncorrect decisions on the respective original inputs. \nAs can be seen, examples 1 -3 contain minor \ncharacter-level noise that cause s the model make \nwrong decisions, however, the perturbed text still \nseems understandable. In example 4, ‘diameter’ \nwas replaced by ‘diam’ and ‘golf’ was replaced by \n‘golf_game’, but the model failed to handle these \nchanges. In example 5, two repetitive words led the \nmodel to estimate a lower similarity score, \nhowever, the semantic remained unchanged . \nFinally, example 6 shows how removing a single \nword led the model to choose a wrong answer.  \n5.2 User study \nWe conducted a user study with 20 participants to \ninvestigate how understandable the perturbed texts \nare to humans. We created a set of perturbations by \nrandomly selecting perturbed samples from the  \ndatasets used in the experiments.  The s amples \ncovered all types of character-level and word-level \nperturbations.  \nIn the first part of the study, each participant was \ngiven 30 perturbed samples from those \nperturbation methods that are not expected to \nchange the text’s meaning with respect to the NLP \ntasks at hand. These are all  the character-level \nperturbations and three word -level perturbations, \ni.e. Repetition, Singular/plural verbs, and Verb \ntense. The participants were also given the original \ntext along with every perturbed sample, and were \nasked to  judge if the perturbed text is \nunderstandable and still conveys the same \nmeaning. Every sample contained one, two, or \nthree perturbations.  \nAccording to the user evaluations, on average, \n94% of the perturbed samples from this set were \nunderstandable and still conveyed the same \nmeaning as the original text. These results are well \nin agreement with our discussion in Section 6.2, i.e. \nthe majority of our proposed perturbations can be \nautomatically produced and used without needing \nhuman supervision to ensure understandability and \nconsistency.   \nIn the second part of the study, each participant \nwas given 20 perturbed samples from those perturbation \nmethods that may change the text ’s meaning or \nresult in meaningle ss text. The y are the rest of \nword-level perturbations, i.e. Deletion, \nReplacement with synonyms, Negation, and Word \norder. The participants were also given the original \ntext along with every perturbed sample, and were \nasked to judge (with respect to the task at hand) if \nthe perturbed text is still meaningful and consistent \nwith the test set label.  \nAccording to the user evaluations, on average, \n39% of the perturbed samples from this set were \nstill meaningful and consistent with the label, 12% \n \nFigure 1:  Six examples of input perturbations from the three NLP tasks for which the RoBERTa model made \nwrong decisions, but it made correct decisions on the respective original inputs. \n \n \n\n1566\n \n \n \n \nof the perturbed samples were meaningful but the \nlabel should be changed, and 49% of the perturbed \nsamples were no longer meaningful. These results \nimply that some perturbations need to be \nmonitored, corrected, or filtered to make sure they \nare understandable, me aningful, and consistent \nwith the test set label. This helps to fairly estimate \nthe robustness of NLP systems to input \nperturbations. \n6 Related work \nTypical performance measures such as accuracy, \nprecision, recall, etc. may not properly reflect how \nNLP systems behave in real-world use cases. This \nhas motivated many studies to devise novel \nmethods for investigating different capabilities and \nvulnerabilities of text processing systems. \nBehavioral testing introduces targeted changes to \ntextual inputs to test linguisti c capabilities of \nsystems (Ribeiro et al., 2020 ). Explanat ions \nprovide simplified representations of what a \ncomplex NLP model has learned (Moradi and \nSamwald, 2021a, b; Ribeiro et al., 2016). This can \nhelp to identify biases and errors in NLP models. \nAdversarial perturbations have been widely \nstudied to assess the robustness of NLP systems \nagainst adversarial samples crafted to fool a model \n(Alshemali and Kalita, 2020 ; Ren et al., 2019 ; \nZhang et al., 2020). However, adversarial samples \nresemble a very specific type of noise. Moreover, \nmost of previous work on adversarial perturbation \nto NLP models  focused on misspelling attacks \n(Jones et al., 2020; Pruthi et al., 2019; L. Sun et al., \n2020). The perturbation methods implemented in \nthis paper represented a wide range of noises that \nan NLP system may face in real-world situations. \nIntroducing noise and changing textual inputs \nwere already adopted to assess the ability of \nmodels in capturing specific linguistic features \nsuch as learning syntax -sensitive dependencies \n(Linzen et al., 2016), for specific NLP tasks such \nas machine translation (Belinkov and Bisk, 2018), \nfor detecting biases in language models \n(Prabhakaran et al., 2019 ), or to identify \nsusceptible entities in text documents (M. Sun et \nal., 2018 ). In this paper, we investigate d the \nrobustness on a wide range of tasks, and for various \ntypes of character-level and word -level noises in \ntext. \n7 Conclusion \nIn this paper, we introduced and implemented a set \nof non-adversarial perturbation methods that can \nbe used to evaluate the robustness of NLP systems. \nWe extensively investigated the robustness of \nhigh-performance neural language models to noisy \ninput texts. The evaluations on various NLP tasks \nimply that these models are sensitive to different \ncharacter-level and word-level perturbations to the \ninput, and the models ’ performance can decrease \nwhen the input contains slight noise.  The results \nsuggest that it may be too simplistic to only rely on \naccuracy scores obtained on benchmark datasets \nwhen evaluating the robustness of NLP systems. \nThe proposed perturbations can be used, along \nwith other methodologies such as CheckList, to \ntest how robust and reliable NLP systems can \noperate in real -world settings. The experimental \nresults demonstrated that the perturbation methods \nare effective tools for evaluating NLP systems \nagainst noisy data. The user study revealed that \nonly few perturbation methods need to be \nmonitored to make sure they produce meaningful \nand consistent samples. Most of the perturbation \nmethods can be used automatically to produce \nnoisy test samples.  They can be also used as a \nbaseline for evaluating adversarial attacks against \nnon-adversarial perturbations. \nFuture work may include helping users assess \nmeaning preservation and grammatical correctness \nin a semi -automatic manner. Sentence encoders \nsuch as InferSent (Conneau et al., 2017), Universal \nSentence Encoder (Cer et al., 2018 ), and BERT \ntrained for semantic similarity  (Reimers and \nGurevych, 2019) can be used to give users clues \nhow semantically similar the origi nal and \nperturbed sentences are. Moreover, users can be \nprovided with information about grammatical \nerrors in the perturbed text using LanguageTool  \n(Naber, 2003) or other grammar checking tools. \nReferences  \nBasemah Alshemali, and Jugal Kalita. 2020. \nImproving the Reliability of Deep Neural Networks \nin NLP: A Review. Knowledge-Based Systems, 191: \n105210. \nhttps://doi.org/10.1016/j.knosys.2019.105210. \nYonatan Belinkov, and Yonatan Bisk. 2018. Synthetic \nand Natural Noise Both Break Neural Mach ine \nTranslation. In 6th International Conference on \nLearning Representations, ICLR 2018.  \n1567\n \n \n \n \nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez -\nGazpio, and Lucia Specia. 2017. SemEval -2017 \nTask 1: Semantic Textual Similarity Multilingual \nand Crosslingual Focuse d Evaluation. In \nProceedings of the 11th International Workshop on \nSemantic Evaluation (SemEval -2017). Association \nfor Computational Linguistics, pages 1 -14. \nhttps://doi.org/10.18653/v1/S17-2001. \nDaniel Cer, Yinfei Yang, Sheng -yi Kong, Nan Hua, \nNicole Limtiaco, Rhomni St John, Noah Constant, \nMario Guajardo -Céspedes, Steve Yuan, and Chris \nTar. 2018. Universal sentence encoder. arXiv \npreprint arXiv:1803.11175.  \nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc \nBarrault, and Antoine Bordes. 2017. Supervised \nLearning of Universal Sentence Representations \nfrom Natural Language Inference Data. In \nProceedings of the 2017 Conference on Empirical \nMethods in Natural Language Processing . \nAssociation for Computational  Linguistics, pages \n670-680. https://doi.org/10.18653/v1/D17-1070. \nJacob Devlin, Ming -Wei Chang, Kenton Lee, and \nKristina Toutanova. 2018. Bert: Pre-training of deep \nbidirectional transformers for langua ge \nunderstanding. arXiv preprint arXiv:1810.04805.  \nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong \nLiu. 2020. Towards robustness against natural \nlanguage word substitutions. In International \nConference on Learning Representations.  \nJavid Ebrahimi, Anyi  Rao, Daniel Lowd, and Dejing \nDou. 2018. HotFlip: White -Box Adversarial \nExamples for Text Classification. In Proceedings of \nthe 56th Annual Meeting of the Association for \nComputational Linguistics (Volume 2: Short \nPapers). Association for Computational \nLinguistics, pages 31-36.  \nAlhussein Fawzi, Seyed -Mohsen Moosavi -Dezfooli, \nand Pascal Frossard. 2016. Robustness of \nclassifiers: from adversarial to random noise. arXiv \npreprint arXiv:1608.08967.  \nGeorg Heigold, Günter Neumann, and Josef van \nGenabith. 2018. H ow Robust Are Character -Based \nWord Embeddings in Tagging and MT Against \nWrod Scramlbing or Randdm Nouse? In \nProceedings of the 13th Conference of the \nAssociation for Machine Translation in the \nAmericas.  \nErik Jones, Robin Jia, Aditi Raghunathan, and Percy \nLiang. 2020. Robust Encodings: A Framework for \nCombating Adversarial Typos. In Proceedings of \nthe 58th Annual Meeting of the Association for \nComputational Linguistics . Association for \nComputational Linguistics, pages 2752 -2765. \nhttps://doi.org/10.18653/v1/2020.acl-main.245. \nXin Li, and Dan Roth. 2002. Learning question \nclassifiers. In COLING 2002: The 19th \nInternational Conference on Computational \nLinguistics.  \nBin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, \nXirong Li, and Wenchang Shi. 2018. Deep text \nclassification can be fooled. In Proceedings of the \nTwenty-Seventh International Joint Conference on  \nArtificial Intelligence (IJCAI-18). pages 4208-4215. \nhttps://doi.org/10.24963/ijcai.2018/585. \nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. \n2016. Assessing the ability of LSTMs to learn \nsyntax-sensitive dependencies. Transactions of the \nAssociation for Computational Linguistics, 4 : 521-\n535.  \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, \nMandar Joshi, Danqi Chen, Omer Levy, Mike \nLewis, Luke Zettlemoyer, and Veselin Stoyanov. \n2019. Roberta: A robustly optimized bert \npretraining approach. arXiv preprint \narXiv:1907.11692.  \nGeorge A. Miller. 1995.  WordNet: a lexical database \nfor English. Commun. ACM, 38 (11): 39 –41. \nhttps://doi.org/10.1145/219717.219748. \nMilad Moradi, and Matthias Samwald. 2021a. \nExplaining Black-Box Models for Biomedical Text \nClassification. IEEE Journal of Biomedical and \nHealth Informatics, 25 (8): 3112 -3120. \nhttps://doi.org/10.1109/JBHI.2021.3056748. \nMilad Moradi, and Matthias Samwald. 2021b. Post -\nhoc explanation of black -box classifiers using \nconfident itemsets. Expert Systems with \nApplications, 165 : 113941. \nhttps://doi.org/10.1016/j.eswa.2020.113941. \nDaniel Naber. 2003. A rule -based style and grammar \nchecker. Citeseer.  \nXing Niu, Prashant Mathur, Georgiana Dinu, and Yaser \nAl-Onaizan. 2020. Evaluating Robustness to Input \nPerturbations for Neural Machine Translation. In \nProceedings of the 58th Annual Meeting of the \nAssociation for Computational Linguistics . \nAssociation for Computational Linguistics, pages \n8538-8544.  \nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt \nGardner, Christopher Clark, Kenton Lee, and Luke \nZettlemoyer. 2018. Deep Contextualized Word \nRepresentations. In Proceedings of the 2018 \nConference of the North American Chapter of the \nAssociation for Computational Linguistics: Human \nLanguage Technologies . Association for \nComputational Linguistics, pages 2227 -2237. \nhttps://doi.org/10.18653/v1/N18-1202. \nVinodkumar Prabhakaran, Ben Hutchinson, and \nMargaret Mitchell. 2019. Perturbation Sensitivity \n1568\n \n \n \n \nAnalysis to Detect Unintended Model Biases. In \nProceedings of the 2019 Conference on Empirical \nMethods in Natural Language Processing and the \n9th International Joint  Conference on Natural \nLanguage Processing (EMNLP -IJCNLP). \nAssociation for Computational Linguistics, pages \n5740-5745.  \nDanish Pruthi, Bhuwan Dhingra, and Zachary C. \nLipton. 2019. Combating Adversarial Misspellings \nwith Robust Word Recognition. In Proceedings of \nthe 57th Annual Meeting of the Association for \nComputational Linguistics . Association for \nComputational Linguistics, pages 5582 -5591. \nhttps://doi.org/10.18653/v1/P19-1561. \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, \nDario Amodei, and Ilya Sutskever. 2019. Language \nmodels are unsupervised multitask learners. OpenAI \nblog, 1(8): 9.  \nNils Reimers, and Iryna Gurevych. 2019. Sentence -\nBERT: Sentence Embeddings using Siamese BERT-\nNetworks. In Proceedings of the 2019 Conference \non Empirical Methods in Natural Language \nProcessing and the 9th International Joint \nConference on Natural Language Processing . \nAssociation for Computational Linguistics, pages \n3982-3992. https://doi.org/10.18653/v1/D19-1410. \nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. \n2019. Generating natural language adversarial \nexamples through probability weighted word \nsaliency. In Proceedings of the 57th annual meeting \nof the associa tion for computational linguistics . \npages 1085-1097.  \nMarco Tulio Ribeiro, Sameer Singh, and Carlos \nGuestrin. 2016. \"Why Should I Trust You?\": \nExplaining the Predictions of Any Classifier. In \nProceedings of the 22nd ACM SIGKDD \nInternational Conference on K nowledge Discovery \nand Data Mining . ACM, pages 1135 -1144. \nhttps://doi.org/10.1145/2939672.2939778. \nMarco Tulio Ribeiro, Tongshuang Wu, Carlos \nGuestrin, and Sameer Singh. 2020. Beyond \nAccuracy: Behavioral Testing of NLP Models with \nCheckList. In Proceedings of the 58th Annual \nMeeting of the Association for Computational \nLinguistics. Association for Computational \nLinguistics, pages 4902-4912.  \nRichard Socher, Alex Perelygin, Jean Wu, Jason \nChuang, Christ opher D Manning, Andrew Y Ng, \nand Christopher Potts. 2013. Recursive deep models \nfor semantic compositionality over a sentiment \ntreebank. In Proceedings of the 2013 conference on \nempirical methods in natural language processing . \npages 1631-1642.  \nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari \nAsai, Jia Li, Philip Yu, and Caiming Xiong. 2020. \nAdv-BERT: BERT is not robust on misspellings! \nGenerating nature adversarial samples on BERT. \narXiv preprint arXiv:2003.04985.  \nMengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, \nand Jiayu Zhou. 2018. Identify Susceptible \nLocations in Medical Records via Adversarial \nAttacks on Deep Predictive Models. In Proceedings \nof the 24th ACM SIGKDD International Conference \non Knowledge Discovery &amp; Data Mining . \nAssociation for Computing Machinery, pages 793–\n801. https://doi.org/10.1145/3219819.3219909. \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien \nChaumond, Clement Delangue, Anthony Moi, \nPierric Cistac, Tim Rault, Remi Lou f, Morgan \nFuntowicz, Joe Davison, Sam Shleifer, Patrick von \nPlaten, Clara Ma, Yacine Jernite, Julien Plu, \nCanwen Xu, Teven Le Scao, Sylvain Gugger, \nMariama Drame, Quentin Lhoest, and Alexander \nRush. 2020. Transformers: State-of-the-Art Natural \nLanguage Pro cessing. In Proceedings of the 2020 \nConference on Empirical Methods in Natural \nLanguage Processing: System Demonstrations . \nAssociation for Computational Linguistics, pages \n38-45. https://doi.org/10.18653/v1/2020.emnlp-\ndemos.6. \nJincheng Xu, and Qingfeng Du. 2020. TextTricker: \nLoss-based and gradient -based adversarial attacks \non text classification models. Engineering \nApplications of Artificial Intelligence, 92 : 103641. \nhttps://doi.org/10.1016/j.engappai.2020.103641. \nYi Yang, Wen-tau Yih, and Christopher Meek. 2015. \nWikiqa: A challenge dataset for open -domain \nquestion answering. In Proceedings of the 2015 \nconference on empirical methods  in natural \nlanguage processing. pages 2013-2018.  \nZhilin Yang, Zihang Dai, Yiming Yang, Jaime \nCarbonell, Russ R Salakhutdinov, and Quoc V Le. \n2019. Xlnet: Generalized autoregressive pretraining \nfor language understanding. In Advances in neural \ninformation processing systems. pages 5753-5763.  \nGuoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji \nZhang, Bairu Hou, Yuan Zang, Zhiyuan Liu, and \nMaosong Sun. 2020. OpenAttack: An Open -source \nTextual Adversarial Attack Toolkit. arXiv preprint \narXiv:2009.09191.  \nWei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, \nand Chenliang Li. 2020. Adversarial Attacks on \nDeep-learning Models in Natural Language \nProcessing: A Survey. ACM Trans. Intell. Syst. \nTechnol., 11 (3): Article 24. \nhttps://doi.org/10.1145/3374217. \n \n \n \n1569\n \n \n \n \nAppendix A. Fine-tuning hyperparameters \n \nTable 6, 7, 8, and 9 present the hyperparameter \nvalues of the BERT, RoBERTa, XLNet, and ELMo \nmodels, respectively, in the fine -tuning \nexperiments on different NLP tasks. Those \nhyperparameters not included in the tables were \nused with their default values specified by the \noriginal models.  \n  \nHyperparameter TC SA QA NER SS \nMax sequence length 64 64 256 64 64 \nBatch size 12 12 2 12 12 \nLearning rate 2e-5 1e-5 3e-5 1e-5 2e-5 \nNum epochs 20 15 10 20 20 \nTable 6:  Fine-tuning hyperparameters for the BERT language model on different tasks . TC: Text \nClassification, SA: Sentiment Analysis, QA: Question Answering, NER: Named  Entity \nRecognition, SS: Semantic Similarity. \n \n \nHyperparameter TC SA QA NER SS \nMax sequence length 64 64 256 64 64 \nBatch size 12 12 2 12 12 \nLearning rate 1e-5 1e-5 1.5e-5 2e-5 2e-5 \nWeight decay 0.1 0.1 0.01 0.1 0.01 \nLearning rate decay Linear Linear Linear Linear Linear \nWarmup ratio 0.06 0.06 0.06 0.05 0.05 \nNum epochs 20 15 10 20 20 \nTable 7:  Fine-tuning hyperparameters for the RoBERTa language model on different tasks . TC: \nText Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity \nRecognition, SS: Semantic Similarity. \n \n Hyperparameter TC SA QA NER SS \nMax sequence length 128 128 256 128 128 \nBatch size 8 8 2 8 8 \nLearning rate 2e-5 2e-5 2.e-5 1e-5 1e-5 \nNum steps 6K 6K 4K 6K 6K \nLearning rate decay Linear Linear Linear Linear Linear \nTable 8:  Fine-tuning hyperparameters for the XLNet language model on different tasks. TC: Text \nClassification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity \nRecognition, SS: Semantic Similarity. \n \n \nHyperparameter TC SA QA NER SS \nn_highway 2 2 2 2 2 \nDroupout 0.2 0.2 0.2 0.2 0.2 \nBatch size 128 128 256 128 128 \nProjection dim 512 512 512 512 512 \nNum epochs 20 20 10 20 20 \nTable 9:  Fine-tuning hyperparameters for the ELMo language model on different tasks. TC: Text \nClassification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity \nRecognition, SS: Semantic Similarity. \n \n \n1570\n \n \n \n \nAppendix B. Manual curation of perturbed \nsamples  \n \nTable 10 shows how many perturbed samples \n(word-level deletion) were checked in the manual \ncuration procedure until reaching 200 test samples \nfor every dataset and different values of \nPerturbation Per Sample (PPS). Table 11 and Table \n12 show the same statistics for word-level negation \nand word-level replacement with synonym , \nrespectively.  \n \nDataset \nPerturbation Per Samples \nPPS=1 PPS=2 PPS=3 PPS=4 \nTREC 221 269 341 435 \nSST 238 301 345 452 \nCoNLL 261 327 394 468 \nSTS 240 281 354 409 \nWQA 219 248 283 351 \nTable 10:  The number of perturbed samples (word -level deletion) \nchecked in the manua l curation procedure until reaching 200 test \nsamples for every dataset and different values of Perturbation Per \nSample (PPS). \n \n Dataset \nPerturbation Per Samples \nPPS=1 PPS=2 PPS=3 PPS=4 \nTREC 235 251 268 268 \nSST 291 317 325 325 \nCoNLL 209 218 226 226 \nSTS 253 269 291 291 \nWQA 295 314 317 317 \nTable 11:  The number of perturbed samples (word -level negation) \nchecked in the manual curation procedure until reaching 200 test \nsamples for every dataset and different values of Perturbation Per \nSample (PPS). \n \n Dataset \nPerturbation Per Samples \nPPS=1 PPS=2 PPS=3 PPS=4 \nTREC 239 266 295 308 \nSST 221 249 273 290 \nCoNLL 213 228 236 251 \nSTS 230 251 284 303 \nWQA 215 233 256 287 \nTable 12:  The number of perturbed samples (word -level replacement \nwith synonym) checked in the manual curation procedure until reaching \n200 test samples for every dataset and different values of Perturbation \nPer Sample (PPS). \n \n ",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.8455682992935181
    },
    {
      "name": "Computer science",
      "score": 0.810187816619873
    },
    {
      "name": "Language model",
      "score": 0.5746038556098938
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5393658876419067
    },
    {
      "name": "Artificial neural network",
      "score": 0.42552292346954346
    },
    {
      "name": "Natural language processing",
      "score": 0.372558057308197
    },
    {
      "name": "Machine learning",
      "score": 0.36339622735977173
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}