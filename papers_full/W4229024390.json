{
  "title": "Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion",
  "url": "https://openalex.org/W4229024390",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2010438866",
      "name": "Chen Xiang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A693219661",
      "name": "Zhang Ningyu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1978908041",
      "name": "Li, Lei",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2372640170",
      "name": "Deng, Shumin",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3210421445",
      "name": "Tan, Chuanqi",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2116197244",
      "name": "Xu Changliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097369839",
      "name": "Huang, Fei",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105471130",
      "name": "Si Luo",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2048668577",
      "name": "Chen, Huajun",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2094728533",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2945302307",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2810583043",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W2946165673",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2798385737",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3167077707",
    "https://openalex.org/W2963870853",
    "https://openalex.org/W2798298921",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2962982907",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3093922720",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3035017890",
    "https://openalex.org/W3207311065",
    "https://openalex.org/W2979129662",
    "https://openalex.org/W2526174222",
    "https://openalex.org/W3045384077",
    "https://openalex.org/W2987734933",
    "https://openalex.org/W3035448883",
    "https://openalex.org/W2251135946",
    "https://openalex.org/W3188999884",
    "https://openalex.org/W3166051255",
    "https://openalex.org/W3207972321",
    "https://openalex.org/W3166170409",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3176858586",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W2747329762",
    "https://openalex.org/W3194836374",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4306802252",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W3174503624",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3003940182",
    "https://openalex.org/W3127151332",
    "https://openalex.org/W2912500072",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W2807480793",
    "https://openalex.org/W639708223",
    "https://openalex.org/W4299527668",
    "https://openalex.org/W2788647998",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2914584698",
    "https://openalex.org/W2949434543"
  ],
  "abstract": "Multimodal Knowledge Graphs (MKGs), which organize visual-text factual\\nknowledge, have recently been successfully applied to tasks such as information\\nretrieval, question answering, and recommendation system. Since most MKGs are\\nfar from complete, extensive knowledge graph completion studies have been\\nproposed focusing on the multimodal entity, relation extraction and link\\nprediction. However, different tasks and modalities require changes to the\\nmodel architecture, and not all images/objects are relevant to text input,\\nwhich hinders the applicability to diverse real-world scenarios. In this paper,\\nwe propose a hybrid transformer with multi-level fusion to address those\\nissues. Specifically, we leverage a hybrid transformer architecture with\\nunified input-output for diverse multimodal knowledge graph completion tasks.\\nMoreover, we propose multi-level fusion, which integrates visual and text\\nrepresentation via coarse-grained prefix-guided interaction and fine-grained\\ncorrelation-aware fusion modules. We conduct extensive experiments to validate\\nthat our MKGformer can obtain SOTA performance on four datasets of multimodal\\nlink prediction, multimodal RE, and multimodal NER. Code is available in\\nhttps://github.com/zjunlp/MKGformer.\\n",
  "full_text": "Hybrid Transformer with Multi-level Fusion for\nMultimodal Knowledge Graph Completion\nXiang Chen\nNingyu Zhangâˆ—\nZhejiang University\nAZFT Joint Lab for Knowledge Engine\nHangzhou Innovation Center\nHangzhou, Zhejiang, China\nxiang_chen@zju.edu.cn\nzhangningyu@zju.edu.cn\nLei Li\nShumin Deng\nZhejiang University\nAZFT Joint Lab for Knowledge Engine\nHangzhou Innovation Center\nHangzhou, Zhejiang, China\nleili21@zju.edu.cn\n231sm@zju.edu.cn\nChuanqi Tan\nAlibaba Group\nHangzhou, Zhejiang, China\nchuanqi.tcq@alibaba-inc.com\nChangliang Xu\nState Key Laboratory of Media\nConvergence Production Technology\nand Systems\nBeijing, China\nxu@shuwen.com\nFei Huang\nLuo Si\nAlibaba Group\nHangzhou, Zhejiang, China\nf.huang@alibaba-inc.com\nluo.si@alibaba-inc.com\nHuajun Chenâˆ—\nZhejiang University\nAZFT Joint Lab for Knowledge Engine\nHangzhou Innovation Center\nHangzhou, Zhejiang, China\nhuajunsir@zju.edu.cn\nABSTRACT\nMultimodal Knowledge Graphs (MKGs), which organize visual-\ntext factual knowledge, have recently been successfully applied\nto tasks such as information retrieval, question answering, and\nrecommendation system. Since most MKGs are far from complete,\nextensive knowledge graph completion studies have been proposed\nfocusing on the multimodal entity, relation extraction and link pre-\ndiction. However, different tasks and modalities require changes\nto the model architecture, and not all images/objects are relevant\nto text input, which hinders the applicability to diverse real-world\nscenarios. In this paper, we propose a hybrid transformer with\nmulti-level fusion to address those issues. Specifically, we lever-\nage a hybrid transformer architecture with unified input-output\nfor diverse multimodal knowledge graph completion tasks. More-\nover, we propose multi-level fusion, which integrates visual and\ntext representation via coarse-grained prefix-guided interaction\nand fine-grained correlation-aware fusion modules. We conduct\nextensive experiments to validate that our MKGformer can obtain\nSOTA performance on four datasets of multimodal link prediction,\nmultimodal RE, and multimodal NER1.\nCCS CONCEPTS\nâ€¢ Information systems â†’Information extraction; Multime-\ndia content creation .\nâˆ—Corresponding author.\n1Code is available in https://github.com/zjunlp/MKGformer.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR â€™22, July 11â€“15, 2022, Madrid, Spain\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-8732-3/22/07. . . $15.00\nhttps://doi.org/10.1145/3477495.3531992\nKEYWORDS\nknowledge graph completion; multimodal; relation extraction; named\nentity recognition\nACM Reference Format:\nXiang Chen, Ningyu Zhang, Lei Li, Shumin Deng, Chuanqi Tan, Changliang\nXu, Fei Huang, Luo Si, and Huajun Chen. 2022. Hybrid Transformer with\nMulti-level Fusion for Multimodal Knowledge Graph Completion. In Pro-\nceedings of the 45th International ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval (SIGIR â€™22), July 11â€“15, 2022, Madrid, Spain.\nACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3477495.3531992\n1 INTRODUCTION\nSuperman Returns Visual Effect \nArt Director Film Crew Role\nâœ” âœ” âœ” âœ” âœ˜ âœ˜\nHead entities Tail entities\n(a) Examples of Multimodal Link Prediction\nInput Text: [HEAD]Loris Karius is reported to\nfeel let down by the signing ofÂ [TAIL]Alisson\nand wants to leave Liverpool.\nRelation Text Relation Image\nâœ” âœ” âœ˜ âœ˜\nGold Relation:Â Person/Person/Peer\nShirt0\nPerson0\nPerson1\nShirt1\nWhole image Shirt0 Shirt1Person0Person1\nObject \nDetection \n(b) Examples of Multimodal Relation Extraction\nFigure 1: Illustration of examples of multimodal knowledge\ngraph facts. Imaged/objects with âœ” around the same en-\ntity have relevant visual features. In contrast, the other im-\nages/objects with âœ— are irrelevant with the corresponding\nentity.\narXiv:2205.02357v5  [cs.CL]  18 Sep 2023\nKnowledge graphs (KGs) can provide back-end support for a va-\nriety of knowledge-intensive tasks in real-world applications, such\nas recommender systems [ 17, 57], information retrieval [ 10, 50]\nand time series forecasting [8]. Since KGs usually contain visual\ninformation, Multimodal Knowledge Graphs (MKGs) recently have\nattracted extensive attention in the community of multimedia, nat-\nural language processing and knowledge graph [24, 38]. However,\nmost MKGs are far from complete due to the emerging entities and\ncorresponding relations. Therefore, multimodal knowledge graph\ncompletion (MKGC), which aims to extract knowledge from the\ntext and image to complete the missing facts in the KGs, has been\nproposed [5, 23, 56, 59]. Concretely, visual data (images) can be re-\ngarded as complementary information used for MKGC tasks, such\nas multimodal link prediction, multimodal named entity recognition\n(MNER) and multimodal relation extraction (MRE).\nFor example, as shown in Figure 1, for the multimodal link pre-\ndiction task, each entity possesses many associated images, which\ncan enhance the entity representation for missing triple predic-\ntion; while for MNER and MRE, each short sentence contains a\ncorresponding image to complement textual contexts for entity and\nrelation extraction. Benefit from the development of multimodal\nrepresentation learning [14], it is intuitive to fuse the heterogeneous\nfeatures of KG entities and the visual information with similar se-\nmantics in unified embeddings.\nTo this end, Xie et al. [48] propose to integrate image features\ninto the typical KG representation learning model for multimodal\nlink prediction. Besides, Sergieh et al . [33] and Wang et al . [47]\njointly encode and fuse the visual and structural knowledge for\nmultimodal link prediction through simple concatenation and auto-\nencoder, respectively. On the other hand, Zheng et al. [61] present\nan efficient modality alignment strategy based on scene graph for\nthe MRE task. Zhang et al. [55] fuse regional image features and\ntextual features with extra co-attention layers for the MNER task.\nAlthough previous studies for multimodal KGC have shown\npromising improvements compared with the unimodal methods,\nthose approaches still suffer from several evident limitations as\nfollows: (1) Architecture universality. Different MKGC tasks and\nmodality representation demand changes to the model architec-\nture. Specifically, different subtasks require task-specific, separately-\nparameterized fusion module on top of diverse encoder architec-\ntures (e.g., ResNet [ 16], Fast-RCNN [ 32] for visual encoder, and\nBERT [9], word2vec [29] for textual encoder). Therefore, a unified\nmodel should be derived to expand the application of the diverse\nsubtasks of multimodal KGC more effectively and conveniently. (2)\nModality contradiction. Most existing multimodal KGC models\nlargely ignore the noise of incorporating irrelevant visual informa-\ntion, which may result in modality contradiction. To be specific, in\nmost multimodal KG, each entity possesses many associated images;\nhowever, parts of images may be irrelevant to entities, and some\nimages even contain a lot of background noise which may mislead\nentity representation. For example in Figure 1(a), each entity has\nmany associated images, but the third image of the head entity has\nlittle relevance to the semantic meaning of â€œSuperman Returnsâ€ for\nmultimodal link prediction. Meanwhile, current SOTA methods for\nMNER and MRE tasks usually utilize valid visual objects by select-\ning top salient objects with the higher object classification scores,\nwhich may also introduce noise from irrelevant or redundant ob-\njects, such as the â€œShirt0â€ and â€œShirt1â€ objects in the Figure 1(b).\nIn practice, irrelevant images/objects may directly exert adverse\neffects on multimodal KGC.\nTo overcome the above barriers, we propose MKGformer, a\nhybrid transformer for unified multimodal KGC, which implements\nthe modeling of the multimodal features of the entity cross thelast\nfew layers of visual transformer and the textual transformer with\nmulti-level fusion, namely M-Encoder. Previous works [7, 22]\nindicate that the pre-trained models (PLMs) can activate knowledge\nrelated to the input at the self-attention layer and Feed-Forward\nNetwork (FFN) layer in Transformer Encoder. Inspired by this, we\nconsider the visual information as supplementary knowledge and\npropose multi-level fusion at the transformer architecture.\nSpecifically, we first present a coarse-grained prefix-guided\ninteraction module at the self-attention part of M-Encoder to\npre-reduce modal heterogeneity for the next step. Second, the\ncorrelation-aware fusion module is proposed in the FFN part of\nM-Encoder to obtain the fine-grained image-text representations\nwhich can alleviate the error sensitivity of irrelevant images/objects.\nIn particular, apart from multimodal link prediction, MKGformer\ncan be more generally applied to MRE and MNER tasks with a\nsimple modification of task-specific head as shown in Figure 2(a).\nIn a nutshell, the contributions of this paper can be summarized:\nâ€¢To the best of our knowledge, our work is the first to pro-\npose a hybrid transformer framework that can be applied to\nmultiple multimodal KGC tasks. Intuitively, leveraging a uni-\nfied transformer framework with similar arithmetic units to\nencode text descriptions and images inside transformers nat-\nurally reduces the heterogeneity to model better multimodel\nentity representation.\nâ€¢We propose multi-level fusion with coarse-grained prefix-\nguided interaction module and fine-grained correlation-aware\nfusion module in blocks of transformers to pre-reduce the\nmodal heterogeneity and alleviate noise of irrelevant visual\nelements, respectively, which are empirically necessary for\ndiverse MKGC tasks.\nâ€¢We perform comprehensive experiments and extensive anal-\nysis on three tasks involving multimodal link prediction,\nMRE and MNER. Experimental results illustrate that our\nmodel can effectively and robustly model the multimodal\nrepresentations of descriptive text and images and substan-\ntially outperform the current state-of-the-art (SOTA) models\nin standard supervised and low-resource settings.\n2 RELATED WORKS\n2.1 Multimodal Knowledge Graph Completion\nMultimodal KGC has been widely studied in recent years, which\nleverages the associated images to represent relational knowledge\nbetter. Previous studies mainly focus on the following three tasks:\n2.1.1 Multimodal Link Prediction. Existing methods for multimodal\nlink prediction focus on encoding image features in KG embeddings.\nXie et al. [48]extend TransE [3] to obtain visual representations that\ncorrespond to the KG entities and structural information of the KG\nseparately. Sergieh et al. [33],Zhao et al. [60] and Wang et al. [47]\nfurther propose several fusion strategy to encoder the visual and\nstructural features into unified embedding space. Recently, Wang\net al. [46] study the noise from irrelevant images corresponding\nto entities and designs a forget gate with an MRP metric to select\nvaluable images for multimodal KGC.\n2.1.2 Multimodal Relation Extraction. Recently, Zheng et al. [62]\npresent a multimodal RE dataset with baseline models. The ex-\nperimental results illustrate that utilizing multimodal information\nimproves RE performance in social media texts. Zheng et al. [61]\nfurther revise the multimodal RE dataset and presents an efficient\nalignment strategy with scene graphs for textual and visual repre-\nsentations. Wan et al. [45] also present four multimodal datasets to\nhandle the lack of multimodal social relation resources and propose\na few-shot learning based approach to extracting social relations\nfrom both texts and face images.\n2.1.3 Multimodal Named Entity Recognition. Zhang et al. [58], Lu\net al. [25], Moon et al. [31] and Arshad et al. [1] propose to encode\nthe textual information with RNN and model the whole image\nrepresentation through CNN in the early stages. Recently, Yu et al.\n[53], Zhang et al. [55]propose to leverage regional image features to\nrepresent objects to exploit fine-grained semantic correspondences\nbased on transformer and visual backbones since informative object\nfeatures are more important than the whole images for MNER tasks.\nSun et al . [37] propose a text-image relation propagation-based\nmultimodal BERT, namely RpBERT, to reduce the interference from\nwhole images. However, RpBERT only focuses on the irrelevance\nof the whole image but ignores the noise brought by irrelevant\nobjects.\nIn conclusion, MKGC can handle the problem of extending a KG\nwith missing triples, thus, have received significant attention. How-\never, different tasks and modalities demand changes to the model\narchitecture, hindering the applicability of diverse real-world sce-\nnarios. Therefore, we argue that a unified model should be derived\nto expand the application of the diverse tasks of multimodal KGC\nmore effectively and conveniently.\n2.2 Pre-trained Multimodal Representation\nThe pre-trained multimodal visual-language models have recently\ndemonstrated great superiority in many multimodal tasks (e.g.,\nimage-text retrieval and visual question answering ). The exist-\ning visual-linguistic pre-trained models can be summarized as two\naspects: 1) Architecture. The single-stream structures include VL-\nBERT [36], VisualBERT [21], Unicoder-VL [20], and UNITER [6],\nwhere the image and textual embeddings are combined into a se-\nquence and fed into transformer to obtain contextual representa-\ntions. While two-stream structures separate visual and language\nprocessing into two streams with interacting through cross-modality\nor co-attentional transformer layers, which includes LXMERT [40]\nand ViLBERT [26]. 2) Pretraining tasks . The pretraining tasks\nof multimodal models usually involve masked language modeling\n(MLM), masked region classification (MRC), and image-text match-\ning (ITM). However, the ITM task is based on the hypothesis that\nthe text-image pairs in the caption datasets are highly related; how-\never, there are much noise brought by irrelevant images/objects,\nthus not being completely satisfied with the ITM task. Further,\nmost of the above models are pre-trained on the datasets of image\ncaption, such as the Conceptual Captions [ 34] or COCO caption\ndataset [4] or visual question answering datasets. Thus, the target\noptimization objects of the above pre-trained multimodal models\nare less relevant to multimodal KGC tasks.\nTherefore, directly applying these pre-trained multimodal meth-\nods to the multimodal KGC may not produce a good performance\nsince multimodal KGC mainly focuses on leveraging visual infor-\nmation to enhance the text rather than on relying on information\nof both sides. Unlike previous methods that focus on learning pre-\ntrained multimodal representation, we regard the image as supple-\nmentary information for knowledge graph completion and propose\na hybrid transformer with multi-level fusion.\n3 OUR APPROACH\nIn this section, we present the overall framework of MKGformer,\nwhich is a general framework that can be applied to widespread\nmultimodal KGC tasks. To facilitate understanding, we introduce\nits detailed implementation, including the unified multimodal KGC\nframework in Section 3.1, the hybrid transformer architecture in\nSection 3.2 and the detailed introduction of M-Encoder in Sec-\ntion 3.3.\n3.1 Unified Multimodal KGC Framework\nAs shown in Figure 2(a), the unified multimodal KGC framework\nmainly includes hybrid transformer architecture and task-specific\nparadigm. Specifically, we adopt ViT and BERT as visual trans-\nformer and textual transformer models, respectively and conduct\nthe modeling of the multimodel representations of the entity across\nthe last ğ¿ğ‘€ layers of transformers. We introduce its detailed imple-\nmentation of task-specific paradigm in the following parts.\n3.1.1 Applying to Multimodal Link Prediction. Multimodal Link\nPrediction is the most popular task for multimodal KGC, which\nfocuses on predicting the tail entity given the head entity and the\nquery relation, denoted by (ğ‘’â„,ğ‘Ÿ, ?). And the answer is supposed to\nbe always within the KG. In terms of the imagesğ¼â„that related to the\nentity ğ‘’â„, we propose to model the distribution over the tail entity\nğ‘’ğ‘¡ as ğ‘(ğ‘’ğ‘¡|(ğ‘’â„,ğ‘Ÿ,ğ¼ â„)). As shown in Figure 2(a), to fully leverage the\nadvantage of pre-trained models, we design the specific procedure\nfor link prediction similar to masked language modeling of pre-\ntrained language models (PLMs). We take first step to model the\nimage-text incorporated entity representations and then predict the\nmissing entity (ğ‘’â„,ğ‘Ÿ, ?)over the multimodal entity representations.\nImage-text Incorporated Entity Modeling. Unlike previous\nwork simply concatenate or fuse based on particular visual and\ntextual features of entities, we fully leverage the \"masked language\nmodeling\" (MLM) ability of pre-trained transformers to model\nimage-text incorporated multimodal representations of the entities\nin the knowledge graph. To be more specific, given an entity de-\nscription ğ‘‘ğ‘’ğ‘– = (ğ‘¤1,...,ğ‘¤ ğ‘›)and its corresponding multiple images\nğ¼ğ‘’ğ‘– = {ğ¼1,ğ¼2,...,ğ¼ ğ‘œ}, we feed the patched images of the entityğ‘’ğ‘– into\nthe visual side of hybrid transformer architecture and convert the\ntextual side input sequence of hybrid transformer architecture as:\nğ‘‡ğ‘’ğ‘– = [CLS]ğ‘‘ğ‘’ğ‘– is the description of [MASK][SEP]. (1)\nHead EntityRelation Mask Tail\n                            T-Encoder\nToken Embedding\n \nM-Encoder \n[CLS]      [SEP]   [SEP]   [MASK]   [SEP]\nLearnable \nV-Encoder\nImage Embedding\n1 0 2 3 4 \n \nÂ·Â·Â·\nÂ·Â·Â·5 \nMultimodal Entity  \nEmbedding\nÂ·Â·Â· Â·Â·Â·\n[CLS] Mark Cuban is sad what happended [SEP]\n[CLS] <s>Taylor Hill</s> holding <o>Jun</o> ... [SEP]\nâ‘   Multimodal Link Prediction \nÂ·Â·Â·\nâ‘¡  Multimodal NER\nÂ·Â·Â·\nâ‘¢  Multimodal RE\nâ‘ \nHead Tail\nCRF\nâ‘¡â‘¢\nNER\nhead\nRE  head\n[CLS]\n[CLS]\n[CLS]\nÂ·Â·Â· Â·Â·Â·\nB-PER OÂ·Â·Â·\nSoftmax\n  \n(a) Unified Multimodal KGC Framework.\nTextual Hidden StatesVisual Hidden States\nSoftmax \nToken-wise\nSimilarity\nx \n... \nAttention\nQ K V Q K V \nFine-grained Fusion\nCorrelation-aware  \nFusion Module\nAttention\nAdd & Layer Norm Add & Layer Norm\nAdd & Layer Norm\nPrefix-guided \n Interaction Module\nSimilarity-aware \nAggregator\nFeed Forward\nAdd & Layer Norm\n (b) Detailed M-Encoder.\nFigure 2: Illustration of MKGformer for (a) Unified Multimodal KGC Framework and (b) Detailed M-Encoder.\nWe extend the word embedding layer of BERT to treat each token\nembedding as corresponding multimodal representation ğ¸ğ‘’ğ‘– of ğ‘–-th\nentity ğ‘’ğ‘–. Then we train the MKGformer to predict the [MASK] over\nthe multimodal entity embedding ğ¸ğ‘’ğ‘– with cross entropy loss for\nclassification:\nLğ‘™ğ‘–ğ‘›ğ‘˜ = âˆ’ğ‘™ğ‘œğ‘”(ğ‘(ğ‘’ğ‘–|(ğ‘‡ğ‘’ğ‘–)), (2)\nNotably, we freeze the whole model except the newly added param-\neters of multimodal entity embedding. We argue that the modified\ninput can guide MKGformer to incorporate the textual and visual\ninformation into multimodal entity embeddings attentively.\nMissing Entity Prediction. Given a ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’ = (ğ‘’â„,ğ‘Ÿ,ğ‘’ ğ‘¡) âˆˆG,\nKGC models predict the ğ‘’â„ or ğ‘’ğ‘¡ in the head or tail batch. Similarly,\nwe treat the link prediction as the MLM task, which uses entity ğ‘’â„,\nentity description ğ‘‘ğ‘’â„, relation ğ‘Ÿ and the entity images ğ¼ğ‘’â„ to pre-\ndict the masked tail entity over the multimodal entity embeddings\ndescribed above. Specifically, we also process the multiple patched\nimages of the entity ğ‘’â„ into the visual side of hybrid transformer\narchitecture and convert this triple (ğ‘’â„,ğ‘Ÿ, ?)to the input sequence\nof on the text side as follows:\nğ‘‡(ğ‘’â„,ğ‘Ÿ,?)= [CLS]ğ‘’â„ ğ‘‘ğ‘’â„[SEP]ğ‘Ÿğ‘—[SEP][MASK][SEP]. (3)\nFinally, we train the MKGformer to predict the [MASK] over the\nmultimodal entity embedding ğ¸ğ‘’ğ‘– via binary cross-entropy loss for\nmultilabel classification with the consideration that the prediction\nof ğ‘’ğ‘¡ is not unique in link prediction.\n3.1.2 Applying to MRE. Relation extraction aims at linking relation\nmentions from text to a canonical relation type in a knowledge\ngraph. Given the text ğ‘‡ and the corresponding image ğ¼, we aim to\npredict the relation between an entity pair (ğ‘’â„,ğ‘’ğ‘¡)and outputs the\ndistribution over relation types asğ‘(ğ‘Ÿ|(ğ‘‡,ğ¼,ğ‘’ â„,ğ‘’ğ‘¡)). Specifically, we\ntake the representation of the special token [CLS] from the final\noutput embedding of hybrid transformer architecture to compute\nthe probability distribution over the class set Ywith the softmax\nfunction ğ‘(ğ‘Ÿ|(ğ¼,ğ‘‡,ğ‘’ â„,ğ‘’ğ‘¡))= Softmax(Wğ’‰ğ‘€ğ‘¡\nğ¿ğ‘€[CLS]). ğ’‰ğ‘€ğ‘¡\nğ¿ğ‘€\nâˆˆRğ‘›Ã—ğ‘‘ğ‘‡\ndenotes the final sequence representation of the ğ¿ğ‘€-th layer from\ntextual side of M-Encoder in hybrid transformer architecture. The\nparameters of the model and W are fine-tuned by minimizing the\ncross-entropy loss over ğ‘(ğ‘Ÿ|(ğ¼,ğ‘‡,ğ‘’ â„,ğ‘’ğ‘¡))on the entire train sets.\n3.1.3 Applying to MNER. MNER is the task of extracting named\nentities from text sequences and corresponding images. Given a\ntoken sequence ğ‘‡ = {ğ‘¤1,...,ğ‘¤ ğ‘›}and its corresponding image\nğ¼, we focus on modeling the distribution over sequence tags as\nğ‘(ğ‘¦|(ğ‘‡,ğ¼ )), where ğ‘¦is the label sequence of tags ğ‘¦ = {ğ‘¦1,...,ğ‘¦ ğ‘›}.\nWe assign the procedure of MKGformer with CRF [ 18] function\nsimilar to previous multimodal NER tasks for the fair comparison.\nFor a sequence of tagsğ‘¦ = {ğ‘¦1,...,ğ‘¦ ğ‘›}, we calculate the probability\nof the label sequence ğ‘¦ over the pre-defined label set ğ‘Œ with the\nBIO tagging schema as described in [18].\n3.2 Hybrid Transformer Architecture\nThe hybrid transformer architecture of MKGformer mainly in-\ncludes three stacked modules: (1) the underlying textual encoder is\ndesigned to capture basic syntactic and lexical information from\nthe input tokens, namely (T-Encoder), (2) the underlying visual\nencoder (V-Encoder), which is responsible for capturing basic vi-\nsual features from the input patched images, and (3) the upper\nmultimodal encoder (M-Encoder) is adopted to model image-text\nincorporated entity representations inside the underlying visual\ntransformer and textual transformer. Besides, we denote the number\nof V-Encoder layers as ğ¿ğ‘‰, the number of T-Encoder layers as ğ¿ğ‘‡,\nand the number of M-Encoder layers as ğ¿ğ‘€, where ğ¿ğ‘‰ğ‘–ğ‘‡ = ğ¿ğ‘‰ +ğ¿ğ‘€\nand ğ¿ğµğ¸ğ‘…ğ‘‡ = ğ¿ğ‘‡ +ğ¿ğ‘€.\nRecap of the Transformer Architecture. Transformer [44] is\nnow the workhorse architecture behind most SOTA models in CV\nand NLP, which is composed of ğ¿stacked blocks. While each block\nmainly includes two types of sub-layers: multi-head self-attention\n(MHA) and a fully connected feed-forward network (FFN). Layer\nNormalization (LN) and residual connection are also used in each\nlayer. Given the input sequence vectorsğ’™ âˆˆRğ‘›Ã—ğ‘‘, the conventional\nattention function maps ğ’™ to queries ğ‘¸ âˆˆRğ‘›Ã—ğ‘‘ and key-value pairs\nğ‘² âˆˆRğ‘›Ã—ğ‘‘,ğ‘½ âˆˆRğ‘›Ã—ğ‘‘:\nAttn(ğ‘¸,ğ‘²,ğ‘½)= softmax(ğ‘¸ğ‘² ğ‘‡\nâˆš\nğ‘‘\n)ğ‘½, (4)\nwhere ğ‘› denotes the length of sequence. MHA performs the at-\ntention function in parallel over ğ‘â„ heads, where each head is\nseparately parameterized by ğ‘¾ (ğ‘–)\nğ‘ ,ğ‘¾ (ğ‘–)\nğ‘˜ ,ğ‘¾ (ğ‘–)\nğ‘£ âˆˆRğ‘‘Ã—ğ‘‘â„ to project\ninputs to queries, keys, and values. The role of MHA is to compute\nthe weighted hidden states for each head, and then concatenates\nthem as:\nMHA(ğ’™)= [head1; Â·Â·Â· ; headh]ğ‘¾ğ‘œ,\nğ‘¸ (ğ‘–),ğ‘² (ğ‘–),ğ‘½ (ğ‘–)= ğ’™ğ‘¾ (ğ‘–)\nğ‘ ,ğ’™ğ‘¾ (ğ‘–)\nğ‘˜ ,ğ’™ğ‘¾ (ğ‘–)\nğ‘£\nheadi = Attn(ğ‘¸ (ğ‘–),ğ‘² (ğ‘–),ğ‘½ (ğ‘–)),\n(5)\nwhere ğ‘¾ğ‘œ âˆˆRğ‘‘Ã—ğ‘‘ and ğ‘‘ denotes the dimension of hidden embed-\ndings. ğ‘‘â„ = ğ‘‘/ğ‘â„ is typically set in MHA. FFN is another vital\ncomponent in transformer, typically consisting of two layers of\nlinear transformations with a ReLU activation function as follows:\nFFN(x)= ReLU(ğ’™ğ‘¾1 +b1)ğ‘¾2 +b2, (6)\nwhere ğ‘¾1 âˆˆRğ‘‘Ã—ğ‘‘ğ‘š, ğ‘¾2 âˆˆRğ‘‘ğ‘šÃ—ğ‘‘.\nV-Encoder. We adopt the first ğ¿ğ‘‰ layers of ViT [11] pre-trained\non ImageNet-1k from [42] as the visual encoder to extract image\nfeatures. Given ğ‘œimages ğ¼ğ‘’ğ‘– of the entity ğ‘’ğ‘–2, we rescale each image\nto unified ğ»Ã—ğ‘Š pixels, and theğ‘–-th input imageIğ‘– âˆˆRğ¶Ã—ğ»Ã—ğ‘Š(1 â‰¤\nğ‘– < ğ‘œ)is first reshaped into ğ‘¢ = ğ»ğ‘Š/ğ‘ƒ2 flattened 2D patches, then\npooled and projected as ğ‘‹ğ‘–ğ‘ğ‘ âˆˆRğ‘¢Ã—ğ‘‘ğ‘‰, where the resolution of\nthe input image is ğ» Ã—ğ‘Š, ğ¶ is the number of channels and ğ‘‘ğ‘‰\ndenotes the dimension of hidden states of ViT. We concatenate the\npatched embeddings of ğ‘œ images to get the visual sequence patch\nembeddings ğ‘‹ğ‘ğ‘ âˆˆRğ‘šÃ—ğ‘‘ğ‘‰, where ğ‘š= (ğ‘¢Ã—ğ‘œ).\nğ‘‹ğ‘‰\n0 = ğ‘‹ğ‘ğ‘ +ğ‘‰ğ‘ğ‘œğ‘ \nÂ¯ğ‘‹ğ‘‰\nğ‘™ = MHA(LN(ğ‘‹ğ‘‰\n0 ))+ ğ‘‹ğ‘‰\nğ‘™âˆ’1,ğ‘™ = 1...ğ¿ğ‘‰\nğ‘‹ğ‘‰\nğ‘™ = FFN(LN(Â¯ğ‘‹ğ‘‰\nğ‘™ ))+ Â¯ğ‘‹ğ‘‰\nğ‘™ ,ğ‘™ = 1...ğ¿ğ‘‰,\n(7)\nwhere ğ‘‰ğ‘ğ‘œğ‘  âˆˆRğ‘šÃ—ğ‘‘ğ‘‰ represents the corresponding position em-\nbedding layer, embedding, ğ‘‹ğ‘‰\nğ‘™ is the hidden states of the ğ‘™ layer of\nvisual encoder.\n2Here, we take the multimodal link prediction as example. As for multimodal NER and\nRE, we choose the top ğ‘œsalient objects according to the text.\nT-Encoder. We leverage the first ğ¿ğ‘‡ layers of BERT [9] as the\ntext encoder, which also consists of ğ¿ğ‘‡ layers of MHA and FFN\nblocks similar to the visual encoder except that LN comes after\nMHA and FFN. To be specific, a token sequence {ğ‘¤1,...,ğ‘¤ ğ‘›}is\nembedded to ğ‘‹ğ‘¤ğ‘‘ âˆˆRğ‘›Ã—ğ‘‘ğ‘‡ with a word embedding matrix, and\nthe textual representation is calculated as follows:\nğ‘‹ğ‘‡\n0 = ğ‘‹ğ‘¤ğ‘‘ +ğ‘‡ğ‘ğ‘œğ‘ \nÂ¯ğ‘‹ğ‘‡\nğ‘™ = LN(MHA(ğ‘‹ğ‘‡\nğ‘™âˆ’1))+ ğ‘‹ğ‘‡\nğ‘™âˆ’1,ğ‘™ = 1...ğ¿ğ‘‡\nğ‘‹ğ‘‡\nğ‘™ = LN(FFN(Â¯ğ‘‹ğ‘‡\nğ‘™ ))+ Â¯ğ‘‹ğ‘‡\nğ‘™ ,ğ‘™ = 1...ğ¿ğ‘‡,\n(8)\nwhere ğ‘‡ğ‘ğ‘œğ‘  âˆˆ R(ğ‘›)Ã—ğ‘‘ğ‘‡ denotes position embedding, ğ‘‹ğ‘‡\nğ‘™ is the\nhidden states of the ğ‘™ layer for the output textual sequence.\nM-Encoder. Multimodal KGC mainly faces the issues of het-\nerogeneity and irrelevance between different modalities. Different\nfrom previous works leveraging extra co-attention layers to inte-\ngrate modality information, we propose to model the multimodal\nfeatures of the entity cross the lastğ¿ğ‘€ layers of ViT and BERT with\nmulti-level fusion, namely M-Encoder. To be specific, we present\na Prefix-Guided Interaction module ( PGI) at the self-attention\nblock to pre-reduce the modality heterogeneity. We also propose\na Correlation-Aware Fusion module ( CAF) in the FFN layer to\nreduce the impact of noise caused by irrelevant image elements.\nHere, we omit the calculation of LN and residual connection for\nsimplicity.\nâ„ğ‘€ğ‘¡\n0 = ğ‘‹ğ‘‡\nğ¿ğ‘‡\nâ„ğ‘€ğ‘£\n0 = ğ‘‹ğ‘‰\nğ¿ğ‘‰\nÂ¯â„ğ‘€ğ‘¡\nğ‘™ ,Â¯â„ğ‘€ğ‘£\nğ‘™ = PGI(â„ğ‘€ğ‘¡\nğ‘™âˆ’1,â„ğ‘€ğ‘£\nğ‘™âˆ’1),ğ‘™ = 1...ğ¿ğ‘€\nâ„ğ‘€ğ‘¡\nğ‘™ ,â„ğ‘€ğ‘£\nğ‘™ = CAF(Â¯â„ğ‘€ğ‘¡\nğ‘™âˆ’1,Â¯â„ğ‘€ğ‘£\nğ‘™âˆ’1),ğ‘™ = 1...ğ¿ğ‘€.\n(9)\n3.3 Insights of M-Encoder\n3.3.1 Prefix-guided Interaction Module. Inspired by the success\nof textual prefix tuning [22] and corresponding analysis [15], we\npropose a prefix-guided interaction mechanism to pre-reduce the\nmodality heterogeneity through the calculation of multi-head at-\ntention at every layer, which is performed on the hybrid keys and\nvalues. In particular, we redefine the computation of visual headğ‘€ğ‘£\nğ‘–\nand textual headğ‘€ğ‘¡\nğ‘– in Eq. 5 as:\nheadğ‘€ğ‘¡ = Attn(ğ’™ğ‘¡ğ‘¾ğ‘¡\nğ‘,ğ’™ğ‘¡ğ‘¾ğ‘¡\nğ‘˜,ğ’™ğ‘¡ğ‘¾ğ‘¡\nğ‘£),\nheadğ‘€ğ‘£ = Attn(ğ’™ğ‘£ğ‘¾ğ‘£\nğ‘,[ğ’™ğ‘£ğ‘¾ğ‘£\nğ‘˜,ğ’™ğ‘¡ğ‘¾ğ‘¡\nğ‘˜],[ğ’™ğ‘£ğ‘¾ğ‘£\nğ‘£,ğ’™ğ‘¡ğ‘¾ğ‘¡\nğ‘£]),\n(10)\nWe also derive the variant formula of Eq. 10 and provide another\nperspective of prefix-guided interpolated attention:3\nheadğ‘€ğ‘£ = Attn(ğ’™ğ‘£ğ‘¾ğ‘£\nğ‘,[ğ’™ğ‘£ğ‘¾ğ‘£\nğ‘˜,ğ’™ğ‘¡ğ‘¾ğ‘¡\nğ‘˜],[ğ’™ğ‘£ğ‘¾ğ‘£\nğ‘£,ğ’™ğ‘¡ğ‘¾ğ‘¡\nğ‘£]),\n= softmax\u0000ğ‘¸ğ‘£[ğ‘²ğ‘£; ğ‘²ğ‘¡]âŠ¤\u0001 \u0014ğ‘½ğ‘£\nğ‘½ğ‘¡\n\u0015\n= (1 âˆ’ğœ†(ğ’™ğ‘£))softmax(ğ‘¸ğ‘£ğ‘²ğ‘£âŠ¤)ğ‘½ğ‘£ +ğœ†(ğ’™ğ‘£)softmax(ğ‘¸ğ‘£ğ‘²ğ‘¡âŠ¤)ğ‘½ğ‘¡\n= (1 âˆ’ğœ†(ğ’™ğ‘£))Attn(ğ‘¸ğ‘£,ğ‘²ğ‘£,ğ‘½ğ‘£)\n|                 {z                 }\nstandard attention\n+ğœ†(ğ’™ğ‘£) Attn(ğ‘¸ğ‘£,ğ‘²ğ‘¡,ğ‘½ğ‘¡)\n|                {z                }\nCross-modal Interaction\n,\n(11)\n3Without loss of generalization, we ignore the softmax scaling factor\nâˆš\nğ‘‘for ease of\nrepresentation.\nğœ†(ğ’™ğ‘£)=\nÃ\nğ‘– exp(ğ‘¸ğ‘£ğ‘²âŠ¤\nğ‘¡ )ğ‘–Ã\nğ‘– exp(ğ‘¸ğ‘£ğ‘²âŠ¤\nğ‘¡ )ğ‘– +Ã\nğ‘— exp(ğ‘¸ğ‘£ğ‘²âŠ¤ğ‘£ )ğ‘—\n. (12)\nwhere ğœ†(ğ’™ğ‘£)denotes the scalar for the sum of normalized attention\nweights on the textual key and value vectors.\nRemark 1. As shown in Eq. 11, the first term Attn(ğ‘¸ğ‘£,ğ‘²ğ‘£,ğ‘½ğ‘£)\nis the standard attention in the visual side, whereas the second term\nrepresent the cross-modal interaction. Monolithic in the sense that\nthe prefix-guided interaction mechanism down-weights the original\nvisual attention probabilities by a scalar factor (i.e., 1 âˆ’ğœ†) and re-\ndistributes the remaining attention probability mass ğœ†to attend to\ntextual attention, which likes the linear interpolation. By applying\nthis to the attention flow calculation over hidden visual states and\nhidden textual states, MKGformer learns coarse-grained modality\nfusion to pre-reduce the modality heterogeneity.\n3.3.2 Correlation-aware Fusion Module. To alleviate the adverse\neffects of noise, we apply a correlation-aware fusion module to\nconduct the token-wise cross-modal interaction (e.g., word-patch\nalignment) between the two modalities. Specifically, we denote ğ‘š\nand ğ‘›as the sequence length of the visual vectors ğ’™ğ‘£ âˆˆRğ‘šÃ—ğ‘‘ and\ntextual vectors ğ’™ğ‘¡ âˆˆRğ‘›Ã—ğ‘‘ respectively, which are the correspond-\ning output features of the prefix-guided interaction module. For the\ntextual tokens, we compute its similarity matrix of all visual tokens\nas follows:\nğ‘º = ğ’™ğ‘¡(ğ’™ğ‘£)âŠ¤. (13)\nWe then conduct softmax function over similarity matrix ğ‘º of ğ‘–-th\ntextual token and use the average token-wise aggregator over visual\ntokens in the image as follows:\nAggğ‘–(ğ’™ğ‘£)= softmax(ğ‘ºğ‘–)ğ’™ğ‘£,(1 â‰¤ğ‘– < ğ‘›) (14)\nAgg(ğ’™ğ‘£)= [Agg1 (ğ’™ğ‘£); ...,Aggğ‘›(ğ’™ğ‘£)] (15)\nwhere Aggğ‘– denotes the similarity-aware aggregated visual repre-\nsentation for ğ‘–-th textual token. Inspired by the finding [13] that\nthe FFN layer learns task-specific textual patterns, we propose to\nincorporate similarity-aware aggregated visual hidden states into\ntextual hidden states in FFN layers and modify the calculation of\nthe FFN process as:\nFFN(ğ’™ğ‘¡)= ReLU(ğ’™ğ‘¡ğ‘¾1 +b1 +Agg(ğ’™ğ‘£)ğ‘¾3)ğ‘¾2 +b2, (16)\nwhere ğ‘¾3 âˆˆRğ‘‘Ã—ğ‘‘ğ‘š represent the new added parameters for aggre-\ngated visual hidden states.\nRemark 2. Note that the token-wise similarity in Equation 13, 14\nand 15 indicates that we would like to obtain the closest image patch\nfor each textual token. By inserting the similarity-aware aggregated\nvisual representation into the FFN calculation in the textual side, our\nMKGformer learns fine-grained alignment between image patches\nand textual tokens, which makes our modal more robust to the noise\nof irrelevant images of entities in KG.\n4 EXPERIMENTS\nWe next introduce the experimental settings of MKGformer in three\ntasks: multimodal link prediction, multimodal RE, and multimodal\nNER. Following results show that MKGformer can outperforms the\nother baselines in both standard supervised and few-shot settings.\n4.1 Experimental Setup\n4.1.1 Datasets. We adopt two publicly available datasets for mul-\ntimodal link prediction, including: 1) WN18-IMG: WN18 [3] is a\nknowledge graph originally extracted from WordNet [30]. While\nWN18-IMG is an extended dataset of WN18 [3] with 10 images for\neach entity. FB15K-237-IMG: FB15K-237-IMG4 [3] has 10 images\nfor each entity and is a subset of the large-scale knowledge graph\nFreebase [2], which is a popular dataset in knowledge graph com-\npletion. Detailed statistics are shown in Table 3. For multimodal\nRE, we evaluate on MNRE [61], a manually-labeled dataset for\nmultimodal neural relation extraction, where the texts and image\nposts are crawled from Twitter. For multimodal NER, we conduct\nexperiments on public Twitter dataset Twitter-2017 [25], which\nmainly include multimodal user posts published on Twitter during\n2016-2017.\n4.1.2 Compared Baselines. We compare our MKGformer with sev-\neral baseline models for a comprehensive comparison to demon-\nstrate the superiority of our MKGformer. Firstly, we choose the\nconventional text-based models for comparison to demonstrate the\nimprovement brought by the visual information. Secondly, we also\ncompare our MKGformer with VisualBERT [21] and ViLBERT [27],\nwhich are pre-trained visual-language model with single-stream\nstructure and two-stream structure respectively. Besides, we further\nconsider another group of previous SOTA multimodal approaches\nfor multimodal knowledge graph completion models as follows:\nMultimodal link prediction: 1) IKRL [48], which extends\nTransE to learn visual representations of entities and structural\ninformation of KG separately; 2) TransAE [47], which combines\nmultimodal autoencoder with TransE to encode the visual and tex-\ntural knowledge into the unified representation, and the hidden\nlayer of the autoencoder is used as the representation of entities in\nthe TransE model. 3) RSME [46], which designs a forget gate with\nan MRP metric to select valuable images for the multimodal KG\nembeddings learning. Multimodal RE: 1) BERT+SG is proposed in\n[61] for MRE, which concatenates the textual representation from\nBERT with visual features generated by scene graph (SG) tool [41].\n2) MEGA [61] designs the dual graph alignment of the correlation\nbetween entities and objects, which is the newest SOTA for MRE.\nMultimodal NER: 1) AdapCoAtt-BERT-CRF[58], which designs an\nadaptive co-attention network to induce word-aware visual repre-\nsentations for each word; 2) UMT [53], which extends Transformer\nto multi-modal version and incorporates the auxiliary entity span\ndetection module; 3) UMGF [55], which proposes a unified multi-\nmodal graph fusion approach for MNER and achieves the newest\nSOTA for MNER.\n4.1.3 Settings. Notably, we assign the layer of M-Encoder asğ¿ğ‘€ =\n3 and conduct experiments with BERT_base and ViT-B/32 [9] for\nall experiments. We further conduct extensive experiments in the\nlow-resource setting by running experiments over five randomly\nsampled Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› for each task and report the average results on\ntest set. For the few-shot multimodal link prediction and MRE, we\nfollow the settings of [12]. We adopt whole images corresponding\n4Since the dataset FB15k has the inverse relation. Therefore, we adopt corresponding\nsub-datasets FB15k-237 to mitigate the problem of the reversible relation between.\nThe multimodal datasets of link prediction can be acquired from https://github.com/\nwangmengsd/RSME, which is the public code of RSME.\nTable 1: Results of the link prediction on FB15K-237-IMG and WN18-IMG. Note that the universal pre-trained vision-language\nmodel cannot be directly applied to the multimodal link prediction; thus, we follow KG-BERT to leverage the pre-trained VL\nmodel for multimodal link prediction.\nModel FB15k-237-IMG WN18-IMG\nHits@1â†‘ Hits@3â†‘ Hits@10â†‘ MRâ†“ Hits@1â†‘ Hits@3â†‘ Hits@10â†‘ MRâ†“\nUnimodal approach\nTransE [3] 0.198 0.376 0.441 323 0.040 0.745 0.923 357\nDistMult [49] 0.199 0.301 0.446 512 0.335 0.876 0.940 655\nComplEx [43] 0.194 0.297 0.450 546 0.936 0.945 0.947 -\nRotatE [39] 0.241 0.375 0.533 177 0.942 0.950 0.957 254\nKG-BERT [52] - - 0.420 153 0.117 0.689 0.926 58\nMultimodal approach\nVisualBERT_base [21] 0.217 0.324 0.439 592 0.179 0.437 0.654 122\nViLBERT_base [27] 0.233 0.335 0.457 483 0.223 0.552 0.761 131\nIKRL(UNION) [48] 0.194 0.284 0.458 298 0.127 0.796 0.928 596\nTransAE [47] 0.199 0.317 0.463 431 0.323 0.835 0.934 352\nRSME (ViT-B/32+Forget) [46]0.242 0.344 0.467 417 0.943 0.951 0.957 223\nMKGformer 0.256 0.367 0.504 221 0.944 0.961 0.972 28\nTable 2: Performance of low-resource setting (8-shot) FB15K-\n237-IMG for multimodal link prediction.\nModel FB15K-237-IMG\nMRâ†“ Hit@10â†‘\nTransE [3] 5847 0.0925\nDistMult [49] 5791 0.0059\nComplEx [43] 6451 0.0046\nRotatE [39] 7365 0.0066\nKG-BERT [52] 2023 0.0451\nVisualBERT_base [21] 5983 0.0772\nViLBERT_base [27] 5754 0.0831\nIKRL [48] 4913 0.0973\nRSME(ViT-B/16+Forget) [46]2895 0.1240\nMKGformer 2785 0.1344\nTable 3: Dataset statistics for Multimodal Link Prediction.\nDataset #Rel. #Ent. #Train #Dev #Test\nFB15k-237-IMG 237 14,541 272,115 17,535 20,466\nWN18-IMG 18 40,943 141,442 5,000 5,000\nto entities for multimodal link prediction tasks. As for the MRE\nand MNER tasks, we follow [ 55] to adopt the visual grounding\ntoolkit [51] for extracting local visual objects with top ğ‘šsalience.\n4.2 Overall Performance\n4.2.1 Multimodal link prediction. The experimental results in Ta-\nble 1 show that incorporating the visual features is generally helpful\nfor link prediction tasks, indicating the superiority of our MKG-\nformer. Compared to the multimodal SOTA method RSME, MKG-\nformer has an increase of 1.4% hits@1 scores and 3.7% hits@10\nscores on FB15k-237-IMG. We further observe that VisualBERT\nand ViLBERT perform even worse than SOTA modal RSME. Note\nthat the implementation of MKGformer is also relatively efficient\n5 10 20 30 40 50\nProportion of resources for training(%)\n65.0\n70.0\n75.0\n80.0\n85.0F1 Score(%)\nMKGformer\nBERT-CRF\nUMGF\nVisualBERT\nViLBERT\nFigure 3: Performance of low-resource setting on Twitter-\n2017 dataset for multimodal NER task.\nand straightforward compared with previous knowledge graph\nembedding approaches [3] that iteratively query all entities.\n4.2.2 Multimodal RE and NER. From the experimental results shown\nin Table 4, we can find that our MKGformer is superior to the newest\nSOTA models UMGF and MEGA, which improves 1.98% F1 scores\nfor the Twitter-2017 dataset and 15.44% F1 scores for the MNRE\ndataset. We further modify the typical pre-trained vision-language\nmodel VisualBERT and ViLBERT with [CLS] classifier for the MRE\ntask and CRF classifier for the MNER task to conduct experiments\nfor comparison. We notice that VisualBERT and ViLBERT perform\nworse than our methods. We hold that the poor performance of the\npre-trained multimodal model may be attributed to the fact that\nthe pre-training datasets and objects have gaps in information ex-\ntraction tasks. This finding also demonstrate that our MKGformer\nis more beneficial for multimodal MNER and MRE tasks.\nTable 4: Performance comparison of the different competitive baseline approaches for multimodal RE and NER. â€œ(CRF)â€\nrepresents CRF is only for MNER dataset Twitter-2017.\nModality Methods MNRE Twitter-2017\nPrecision Recall F1 Precision Recall F1\nText\nCNN-BiLSTM-(CRF) [28] 60.18 46.32 52.35 80.00 78.76 79.37\nHBiLSTM-(CRF) [19] 60.22 47.13 52.87 82.69 78.16 80.37\nPCNN [54] 62.85 49.69 55.49 83.28 78.30 80.72\nBERT-(CRF) 63.85 55.79 59.55 83.32 83.57 83.44\nMTB [35] 64.46 57.81 60.86 83.88 83.22 83.55\nText+Image\nAdapCoAtt-BERT-(CRF) [58]64.67 57.98 61.14 85.13 83.20 84.10\nUMT [53] 62.83 61.32 62.56 85.28 85.34 85.31\nBERT_base+SG [61] 62.95 62.65 62.80 84.13 83.88 84.00\nVisualBERT_base [21] 56.34 58.28 57.29 84.06 85.39 84.72\nViLBERT_base [27] 64.50 61.86 63.16 84.62 85.47 85.04\nUMGF [55] 64.38 66.23 65.29 86.54 84.50 85.51\nMEGA [61] 64.51 68.44 66.41 84.03 84.75 84.39\nMKGformer 82.67 81.25 81.95 86.98 88.01 87.49\nTable 5: Results of compared models for MRE in the low-\nresource setting. We report the results of ğ¹1 score and adopt\nğ¾ = 1,5,10,20 (# examples per class).\nDataset Model ğ¾= 1 ğ¾= 5 ğ¾= 10 ğ¾= 20\nMNRE\nBERT [9] 3.67 6.27 12.65 18.94\nVisualBERT [21] 2.93 4.91 6.10 14.96\nViLBERT [27] 3.89 7.78 13.38 18.45\nMEGA [61] 9.84 11.71 15.39 20.26\nMKGformer 12.04(+2.2) 18.54(+6.83) 21.09(+5.7) 40.93(+20.67)\nTable 6: Performance when the layer of M-Encoder varies, we\ntake different levels of the activation states into computation,\nwhere 1 indicates the bottom layer and 12 indicates the top\nlayer.\nMethods FB15k-237-IMGMNRE Twitter-2017\nHits@10 F1 F1\n(layer 12) 0.485 80.21 85.25\n(layer 10-12) 0.504 81.95 87.49\n(layer 7-12) 0.503 82.20 87.62\n(All layers) 0.507 82.25 87.60\n4.3 Low-Resource Evaluation\nPrevious experiments illustrate that our methods achieve improve-\nments in standard supervised settings. We further report the exper-\nimental results in low-resource scenarios compared with several\nbaselines in Figure 3, Table 2 and Table 5.\n4.3.1 Q1: Does the pre-trained vision-language model successful in\nthe low-resource multimodal KGC? . As shown in Figure 3, Table 2\nand Table 5, VisualBERT and ViLBERT yield slightly improvements\ncompared with typical unimodal baselines in low-resource settings\nwhile obtain worse results than previous multimodal SOTA meth-\nods. It reveals that applying these pre-trained multimodal methods\nto the multimodal KGC may not always achieve a good performance.\nThis result may be attributed to the fact that the pre-training dataset\nand objects of the above visual-language models are less relevant\nto KGC tasks, which may bias the original language modeling ca-\npabilities and knowledge distribution of BERT.\n4.3.2 Q2: Whether hybrid transformer framework data-efficient?\nSince pre-trained multimodal models do not show promising ad-\nvantages in low-resource settings, we further analyze whether the\nhybrid transformer framework is data-efficient. We hold that lever-\naging a hybrid transformer framework with similar arithmetic units\nto fuse entity description and images inside transformers intuitively\nreduces heterogeneity, playing a more critical role in low-resource\nsettings. Thus, we compare with previous multimodal KGC SOTA\nmodels in low-resource settings. The experimental results in Fig-\nure 3, Table 2 and Table 5 indicate that the performance of MKG-\nformer still outperforms the other baselines, which further proving\nthat our proposed method can more efficiently leverage multimodal\ndata. This success may be attributed to the prefix-guided fusion\nmodule in MKGformer leveraging a form similar to linear inter-\npolation to fuse features in the attention layer, thus, effectively\npre-reducing modal heterogeneity.\n4.4 Sensitivity Analysis of M-Encoder Layers\nWe assign the M-Encoder with last three layers of ViT and BERT\nin the previous experiments. However, it is intuitive to investigate\nwhether the performance of MKGformer is sensitive to the layers\nof the M-Encoder. Thus, we take the M-Encoder in different layers\ninto computation for further analysis. As shown in Table 6, the\nperformance of MKGformer on FB15k-237-IMG, MNRE and Twitter-\n2017 only achieve improvements of 0.3% Hits@10 scores, 0.3% F1\nscores and 0.11% F1 scores by conducting M-Encoder with all 12\nlayers. Furthermore, the performance of assigning M-Encoder with\nonly one layer also drops slightly compared with the original result\n(three layers M-Encoder), showing that M-Encoder applied to higher\ntransformer layers can stimulate knowledge and perform modality\nfusion for downstream tasks more efficiently. It also reveals that\nour approach is not sensitive to the layers of the M-Encoder.\n4.5 Ablation Study\n4.5.1 Ablation Setting. The ablation study is further conducted to\nprove the effects of different modules in MKGformer: 1) (Indepen-\ndent) indicate that we add extra three layers M-Encoder based on\nViT and BERT rather than conduct fusion inside them; 2) w/o PGI\nTable 7: The first row shows the split of the relevance of image-text pairs, and the several middle rows indicate representative\nsamples together with their entity-object attention and token-wise similarity in the test set of MNRE datasets, and the bottom\nfive rows in the figure show predicted relation of different approaches on these test samples.\nRelevant Image-text Pair Irrelevant Image-text Pair\nInfinity War Director ConfirmsHulk is NOT Afraid of Thanos . History beckons as Trump - Kim summit kicks off in Singapore.\nGold Relations:per/per/peer per/per/peer\nBERT: per / per /couple âœ—\nVisualBERT: per / per /peer âœ”\nMEGA: per / per /peer âœ”\nOurs: per / per /peer âœ”\nOurs(w/o CAF): per / per /peer âœ”\nper / per /peer âœ”\nper / org /member_of âœ—\nper / loc /place_of_residence âœ—\nper / per /peer âœ”\nper / per /member_of âœ—\nWN18-IMG MNRE Twitter-2017\nDifferent Datasets for Evaluation\n75\n80\n85\n90\n95Score (%) for Multimodal KGC\nMKGformer\nMKGformer (Independent)\nMKGformer w/o PGI \nMKGformer w/o CAF\nFigure 4: Ablation study results of MKGformer.\nrefers to the model without the prefix-guided interaction module;3)\nw/o CAF refers to the model without the whole correlation-aware\nfusion module. We report detailed experimental results in Figure 4\nand observe that ablation models both show a performance decay,\nwhich demonstrates the effectiveness of each component of our\napproach.\n4.5.2 Effectiveness of Internal Hybrid Fusion in Transformer. A spe-\ncific feature of our method is that we conduct modal fusion inside\nthe dual-stream transformer rather than adding a fusion layer out-\nside the transformer like IKRL [48], UMGF [55] and MEGA [61]. To\nthis end, we add extra three layers M-Encoder based on ViT and\nBERT to evaluate the impact of the internal fusion mechanism. We\nobserve that the performance of MKGformer (Independent) drops\non three sub-tasks of multimodal KGC, revealing the effectiveness\nof internal fusion in Transformer.\n4.5.3 Importance of multi-level fusion. The highlights of our MKG-\nformer is the multi-level fusion in M-Encoder with coarse-grained\nprefix-guided interaction module and fine-grained correlation-aware\nfusion module. We argue that these two parts can mutually re-\ninforce each other: the heterogeneity reduced visual and textual\nfeatures can help the correlation-aware module better understand\nfine-grained information. On the contrary, the prefix-guided interac-\ntion module in the next layer can reduce the modality heterogeneity\nmore gently based on fine-grained fusion in the last layer. The re-\nsults shown in Figure 4 demonstrate that multi-level fusion holds\nthe most crucial role in achieving excellent performance. At the\nsame time, the case analysis in Table 7 also reveals the impact of\nthe correlation-aware module for alleviating error sensitivity.\n4.6 Case Analysis for Image-text Relevance\nTo further analyze the robustness of our method for error sensitiv-\nity, we conduct a specific case analysis on the multimodal RE task\nas indicated in Table 7. We notice that VisualBERT, MEGA, and\nour method can recognize the relation for the relevant image-text\npair. Through the visualization of the case, we can further notice:\n1) The attention weights in the prefix-guided interaction module\nreveal that our model can capture the significant attention between\nrelevant entities and objects. 2) The similarity matrix also shows\nthat the entity representation from our model is more similar to\nthe corresponding object patch. Moreover, in the situation that\nimage represents the abstract semantic that is irrelevant to the\ntext, only our method success in prediction due to MKGformer cap-\ntures the more fine-grained multimodal features. It is worth noting\nthat another two multimodal baselines fail in irrelevant image-text\npairs while text-based BERT and ours still predict correctly. These\nobservations reveal that irrelevant visual features may hurt the per-\nformance, while our model can learn more robust and fine-grained\nmultimodal representation, which is essential for reducing error\nsensitivity.\n5 CONCLUSION AND FUTURE WORK\nIn this paper, we present a hybrid Transformer network for mul-\ntimodal knowledge graph completion, which presents M-Encoder\nwith multi-level fusion at the last several layers of ViT and BERT\nto conduct image-text incorporated entity modeling. To the best\nour knowledge, MKGformer is the first work leveraging unified\ntransformer architecture to conduct various multimodal KGC tasks,\ninvolving multimodal link prediction, multimodal relation extrac-\ntion, and multimodal named entity recognition. Concretely, we\npropose a prefix-guided interaction module at the self-attention\nlayer to pre-reduce modality heterogeneity and further design a\ncorrelation-aware fusion module which realize token-wise fine-\ngrained fusion at the FFN layer to mitigate noise from irrelevant\nimages/objects. Extensive experimental results on four datasets\ndemonstrate the effectiveness and robustness of our MKGformer.\nIn the future, we plan to 1) apply our approach to more image\nenhanced natural language processing and information retrieval\ntasks, such as multimodal event extraction, multimodal sentiment\nanalysis, and multimodal entity retrieval; 2) apply the reverse ver-\nsion of our approach to boost visual representation with text for\nCV; 3) extend our approach to pre-training of multimodal KGC.\nACKNOWLEDGMENTS\nWe want to express gratitude to the anonymous reviewers for\ntheir hard work and kind comments. This work is funded by NSFC\nU19B2027/91846204, National Key R&D Program of China (Funding\nNo.SQ2018YFC000004), Zhejiang Provincial Natural Science Foun-\ndation of China (No. LGG22F030011), Ningbo Natural Science Foun-\ndation (2021J190), and Yongjiang Talent Introduction Programme\n(2021A-156-G). Our work is supported by Information Technology\nCenter and State Key Lab of CAD&CG, ZheJiang University.\nREFERENCES\n[1] Omer Arshad, Ignazio Gallo, Shah Nawaz, and Alessandro Calefati. 2019. Aiding\nintra-text representations with visual context for multimodal named entity recog-\nnition. ArXiv preprint abs/1904.01356 (2019). https://arxiv.org/abs/1904.01356\n[2] Kurt Bollacker, Georg Gottlob, and Sergio Flesca. 2008. Freebase: a collaboratively\ncreated graph database for structuring human knowledge. In KDD. 1247â€“1250.\n[3] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-\nsana Yakhnenko. 2013. Translating embeddings for modeling multi-relational\ndata. In Proceedings of the 26th International Conference on Neural Information\nProcessing Systems . 2787â€“2795.\n[4] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta,\nPiotr DollÃ¡r, and C. Lawrence Zitnick. 2015. Microsoft COCO Captions: Data\nCollection and Evaluation Server. CoRR abs/1504.00325 (2015). arXiv:1504.00325\nhttp://arxiv.org/abs/1504.00325\n[5] Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan,\nFei Huang, Luo Si, and Huajun Chen. 2021. KnowPrompt: Knowledge-aware\nPrompt-tuning with Synergistic Optimization for Relation Extraction. CoRR\nabs/2104.07650 (2021). arXiv:2104.07650 https://arxiv.org/abs/2104.07650\n[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,\nYu Cheng, and Jingjing Liu. 2020. UNITER: UNiversal Image-TExt Representation\nLearning. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow,\nUK, August 23-28, 2020, Proceedings, Part XXX (Lecture Notes in Computer Science,\nVol. 12375), Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm\n(Eds.). Springer, 104â€“120. https://doi.org/10.1007/978-3-030-58577-8_7\n[7] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2021. Knowledge Neu-\nrons in Pretrained Transformers. CoRR abs/2104.08696 (2021). arXiv:2104.08696\nhttps://arxiv.org/abs/2104.08696\n[8] Shumin Deng, Ningyu Zhang, Wen Zhang, Jiaoyan Chen, Jeff Z. Pan, and Huajun\nChen. 2019. Knowledge-Driven Stock Trend Prediction and Explanation via\nTemporal Convolutional Network. In Companion of The 2019 World Wide Web\nConference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019 , Sihem Amer-\nYahia, Mohammad Mahdian, Ashish Goel, Geert-Jan Houben, Kristina Lerman,\nJulian J. McAuley, Ricardo Baeza-Yates, and Leila Zia (Eds.). ACM, 678â€“685.\nhttps://doi.org/10.1145/3308560.3317701\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n4171â€“4186. https://doi.org/10.18653/v1/N19-1423\n[10] Laura Dietz, Alexander Kotov, and Edgar Meij. 2018. Utilizing Knowledge Graphs\nfor Text-Centric Information Retrieval. In The 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval, SIGIR 2018, Ann\nArbor, MI, USA, July 08-12, 2018 , Kevyn Collins-Thompson, Qiaozhu Mei, Brian D.\nDavison, Yiqun Liu, and Emine Yilmaz (Eds.). ACM, 1387â€“1390. https://doi.org/\n10.1145/3209978.3210187\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. In9th Interna-\ntional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021 . OpenReview.net. https://openreview.net/forum?id=YicbFdNTTy\n[12] Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language\nModels Better Few-shot Learners. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021 , Chengqing Zong, Fei Xia, Wenjie Li, and\nRoberto Navigli (Eds.). Association for Computational Linguistics, 3816â€“3830.\nhttps://doi.org/10.18653/v1/2021.acl-long.295\n[13] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer\nFeed-Forward Layers Are Key-Value Memories. InProceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 7-11 November, 2021 , Marie-Francine\nMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association\nfor Computational Linguistics, 5484â€“5495. https://aclanthology.org/2021.emnlp-\nmain.446\n[14] Wenzhong Guo, Jianwen Wang, and Shiping Wang. 2019. Deep Multimodal\nRepresentation Learning: A Survey. IEEE Access 7 (2019), 63373â€“63394. https:\n//doi.org/10.1109/ACCESS.2019.2916887\n[15] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham\nNeubig. 2021. Towards a Unified View of Parameter-Efficient Transfer Learning.\nCoRR abs/2110.04366 (2021). arXiv:2110.04366 https://arxiv.org/abs/2110.04366\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 . IEEE\nComputer Society, 770â€“778. https://doi.org/10.1109/CVPR.2016.90\n[17] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y. Chang.\n2018. Improving Sequential Recommendation with Knowledge-Enhanced Mem-\nory Networks. In The 41st International ACM SIGIR Conference on Research &\nDevelopment in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12,\n2018, Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and\nEmine Yilmaz (Eds.). ACM, 505â€“514. https://doi.org/10.1145/3209978.3210017\n[18] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,\nand Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In\nProceedings of the 2016 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies . Association for\nComputational Linguistics, San Diego, California, 260â€“270. https://doi.org/10.\n18653/v1/N16-1030\n[19] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,\nand Chris Dyer. 2016. Neural Architectures for Named Entity Recognition.\nIn NAACL HLT 2016, The 2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies,\nSan Diego California, USA, June 12-17, 2016 , Kevin Knight, Ani Nenkova, and\nOwen Rambow (Eds.). The Association for Computational Linguistics, 260â€“270.\nhttps://doi.org/10.18653/v1/n16-1030\n[20] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-\nVL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training.\nIn The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2020, New York, NY, USA, February 7-12, 2020 . AAAI Press, 11336â€“11344.\nhttps://aaai.org/ojs/index.php/AAAI/article/view/6795\n[21] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.\n2019. Visualbert: A simple and performant baseline for vision and language.\nArXiv preprint abs/1908.03557 (2019). https://arxiv.org/abs/1908.03557\n[22] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous\nPrompts for Generation. In Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Pa-\npers), Virtual Event, August 1-6, 2021 , Chengqing Zong, Fei Xia, Wenjie Li, and\nRoberto Navigli (Eds.). Association for Computational Linguistics, 4582â€“4597.\nhttps://doi.org/10.18653/v1/2021.acl-long.353\n[23] Kun Liu, Yao Fu, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang Huang, and\nSheng Gao. 2021. Noisy-Labeled NER with Confidence Estimation. In Proceedings\nof NAACL. Association for Computational Linguistics, 3437â€“3445. https://doi.\norg/10.18653/v1/2021.naacl-main.269\n[24] Ye Liu, Hui Li, Alberto GarcÃ­a-DurÃ¡n, Mathias Niepert, Daniel OÃ±oro-Rubio,\nand David S. Rosenblum. 2019. MMKG: Multi-modal Knowledge Graphs. In\nThe Semantic Web - 16th International Conference, ESWC 2019, PortoroÅ¾, Slovenia,\nJune 2-6, 2019, Proceedings (Lecture Notes in Computer Science, Vol. 11503) , Pascal\nHitzler, Miriam FernÃ¡ndez, Krzysztof Janowicz, Amrapali Zaveri, Alasdair J. G.\nGray, Vanessa LÃ³pez, Armin Haller, and Karl Hammar (Eds.). Springer, 459â€“474.\nhttps://doi.org/10.1007/978-3-030-21348-0_30\n[25] Di Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang, and Heng Ji. 2018. Visual\nAttention Model for Name Tagging in Multimodal Social Media. InProceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) . Association for Computational Linguistics, Melbourne, Australia,\n1990â€“1999. https://doi.org/10.18653/v1/P18-1185\n[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT:\nPretraining Task-Agnostic Visiolinguistic Representations for Vision-and-\nLanguage Tasks. In Advances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada , Hanna M. Wallach, Hugo\nLarochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Ro-\nman Garnett (Eds.). 13â€“23. https://proceedings.neurips.cc/paper/2019/hash/\nc74d97b01eae257e44aa9d5bade97baf-Abstract.html\n[27] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT:\nPretraining Task-Agnostic Visiolinguistic Representations for Vision-and-\nLanguage Tasks. In Advances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada , Hanna M. Wallach, Hugo\nLarochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Ro-\nman Garnett (Eds.). 13â€“23. https://proceedings.neurips.cc/paper/2019/hash/\nc74d97b01eae257e44aa9d5bade97baf-Abstract.html\n[28] Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end Sequence Labeling via Bi-\ndirectional LSTM-CNNs-CRF. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers . The Association for Computer Linguistics.\nhttps://doi.org/10.18653/v1/p16-1101\n[29] TomÃ¡s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nEstimation of Word Representations in Vector Space. In 1st International Con-\nference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May\n2-4, 2013, Workshop Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.).\nhttp://arxiv.org/abs/1301.3781\n[30] George A. Miller. 1995. WordNet: A Lexical Database for English.Commun. ACM\n38, 11 (1995), 39â€“41.\n[31] Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. 2018. Multimodal\nNamed Entity Recognition for Short Social Media Posts. InProceedings of the 2018\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers) . Association\nfor Computational Linguistics, New Orleans, Louisiana, 852â€“860. https://doi.\norg/10.18653/v1/N18-1078\n[32] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-\nCNN: Towards Real-Time Object Detection with Region Proposal Networks.\nIn Advances in Neural Information Processing Systems 28: Annual Conference on\nNeural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,\nCanada, Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama,\nand Roman Garnett (Eds.). 91â€“99. https://proceedings.neurips.cc/paper/2015/\nhash/14bfa6bb14875e45bba028a21ed38046-Abstract.html\n[33] Hatem Mousselly Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth.\n2018. A Multimodal Translation-Based Approach for Knowledge Graph Represen-\ntation Learning. InProceedings of the Seventh Joint Conference on Lexical and Com-\nputational Semantics, *SEM@NAACL-HLT 2018, New Orleans, Louisiana, USA, June\n5-6, 2018 , Malvina Nissim, Jonathan Berant, and Alessandro Lenci (Eds.). Associa-\ntion for Computational Linguistics, 225â€“234. https://doi.org/10.18653/v1/s18-\n2027\n[34] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Concep-\ntual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic\nImage Captioning. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 1: Long Papers , Iryna Gurevych and Yusuke Miyao (Eds.). Association for\nComputational Linguistics, 2556â€“2565. https://doi.org/10.18653/v1/P18-1238\n[35] Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski.\n2019. Matching the Blanks: Distributional Similarity for Relation Learning. In\nProceedings of the 57th Conference of the Association for Computational Linguistics,\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , Anna Ko-\nrhonen, David R. Traum, and LluÃ­s MÃ rquez (Eds.). Association for Computational\nLinguistics, 2895â€“2905. https://doi.org/10.18653/v1/p19-1279\n[36] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.\n2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In\n8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=\nSygXPaEYvH\n[37] Lin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fangsheng Weng. 2021. RpBERT:\nA Text-image Relation Propagation-based BERT Model for Multimodal NER. In\nThirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third\nConference on Innovative Applications of Artificial Intelligence, IAAI 2021, The\nEleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,\nVirtual Event, February 2-9, 2021 . AAAI Press, 13860â€“13868. https://ojs.aaai.org/\nindex.php/AAAI/article/view/17633\n[38] Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang,\nZhongyuan Wang, and Kai Zheng. 2020. Multi-modal Knowledge Graphs for\nRecommender Systems. In CIKM â€™20: The 29th ACM International Conference\non Information and Knowledge Management, Virtual Event, Ireland, October 19-\n23, 2020 , Mathieu dâ€™Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and\nPhilippe CudrÃ©-Mauroux (Eds.). ACM, 1405â€“1414. https://doi.org/10.1145/\n3340531.3411947\n[39] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl-\nedge Graph Embedding by Relational Rotation in Complex Space. In ICLR.\n[40] Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder\nRepresentations from Transformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP) . Association for\nComputational Linguistics, Hong Kong, China, 5100â€“5111. https://doi.org/10.\n18653/v1/D19-1514\n[41] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. 2020.\nUnbiased Scene Graph Generation From Biased Training. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle,\nWA, USA, June 13-19, 2020 . Computer Vision Foundation / IEEE, 3713â€“3722.\nhttps://doi.org/10.1109/CVPR42600.2020.00377\n[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and HervÃ© JÃ©gou. 2021. Training data-efficient image transformers\n& distillation through attention. InProceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of\nMachine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.). PMLR,\n10347â€“10357. http://proceedings.mlr.press/v139/touvron21a.html\n[43] ThÃ©o Trouillon, Johannes Welbl, Sebastian Riedel, Ã‰ric Gaussier, and Guillaume\nBouchard. 2016. Complex embeddings for simple link prediction. In Proceedings\nof the 33rd International Conference on Machine Learning . 2071â€“2080.\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nAll you Need. In Advances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\nGarnett (Eds.). 5998â€“6008. https://proceedings.neurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[45] Hai Wan, Manrong Zhang, Jianfeng Du, Ziling Huang, Yufei Yang, and Jeff Z. Pan.\n2021. FL-MSRE: A Few-Shot Learning based Approach to Multimodal Social Re-\nlation Extraction. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI\n2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,\nIAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intel-\nligence, EAAI 2021, Virtual Event, February 2-9, 2021 . AAAI Press, 13916â€“13923.\nhttps://ojs.aaai.org/index.php/AAAI/article/view/17639\n[46] Meng Wang, Sen Wang, Han Yang, Zheng Zhang, Xi Chen, and Guilin Qi. 2021. Is\nVisual Context Really Helpful for Knowledge Graph? A Representation Learning\nPerspective. In MM â€™21: ACM Multimedia Conference, Virtual Event, China, October\n20 - 24, 2021 , Heng Tao Shen, Yueting Zhuang, John R. Smith, Yang Yang, Pablo\nCesar, Florian Metze, and Balakrishnan Prabhakaran (Eds.). ACM, 2735â€“2743.\nhttps://doi.org/10.1145/3474085.3475470\n[47] Zikang Wang, Linjing Li, Qiudan Li, and Daniel Zeng. 2019. Multimodal Data\nEnhanced Representation Learning for Knowledge Graphs. In International Joint\nConference on Neural Networks, IJCNN 2019 Budapest, Hungary, July 14-19, 2019 .\nIEEE, 1â€“8. https://doi.org/10.1109/IJCNN.2019.8852079\n[48] Ruobing Xie, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2017. Image-\nembodied Knowledge Representation Learning. In Proceedings of the Twenty-\nSixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Mel-\nbourne, Australia, August 19-25, 2017 , Carles Sierra (Ed.). ijcai.org, 3140â€“3146.\nhttps://doi.org/10.24963/ijcai.2017/438\n[49] Bishan Yang, Wen tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em-\nbedding entities and relations for learning and inference in knowledge bases. In\nICLR.\n[50] Zuoxi Yang. 2020. Biomedical Information Retrieval incorporating Knowledge\nGraph for Explainable Precision Medicine. In Proceedings of the 43rd International\nACM SIGIR conference on research and development in Information Retrieval, SIGIR\n2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi Cheng,\nJaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 2486.\nhttps://doi.org/10.1145/3397271.3401458\n[51] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and\nJiebo Luo. 2019. A Fast and Accurate One-Stage Approach to Visual Grounding.\nIn ICCV. https://doi.org/10.1109/ICCV.2019.00478\n[52] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for Knowledge\nGraph Completion. CoRR abs/1909.03193 (2019). arXiv:1909.03193 http://arxiv.\norg/abs/1909.03193\n[53] Jianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020. Improving Multimodal Named\nEntity Recognition via Entity Span Detection with Unified Multimodal Trans-\nformer. In Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics . Association for Computational Linguistics, Online, 3342â€“3352.\nhttps://doi.org/10.18653/v1/2020.acl-main.306\n[54] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant Supervi-\nsion for Relation Extraction via Piecewise Convolutional Neural Networks. In\nProceedings of the 2015 Conference on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015 , LluÃ­s MÃ rquez, Chris\nCallison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (Eds.). The Association\nfor Computational Linguistics, 1753â€“1762. https://doi.org/10.18653/v1/d15-1203\n[55] Dong Zhang, Suzhong Wei, Shoushan Li, Hanqian Wu, Qiaoming Zhu, and\nGuodong Zhou. 2021. Multi-modal Graph Fusion for Named Entity Recognition\nwith Targeted Visual Guidance. In Thirty-Fifth AAAI Conference on Artificial\nIntelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 . AAAI Press,\n14347â€“14355. https://ojs.aaai.org/index.php/AAAI/article/view/17687\n[56] Ningyu Zhang, Xiang Chen, Xin Xie, Shumin Deng, Chuanqi Tan, Mosha Chen,\nFei Huang, Luo Si, and Huajun Chen. 2021. Document-level Relation Extraction\nas Semantic Segmentation. In Proceedings of IJCAI , Zhi-Hua Zhou (Ed.). ijcai.org,\n3999â€“4006. https://doi.org/10.24963/ijcai.2021/551\n[57] Ningyu Zhang, Qianghuai Jia, Shumin Deng, Xiang Chen, Hongbin Ye, Hui\nChen, Huaixiao Tou, Gang Huang, Zhao Wang, Nengwei Hua, and Huajun Chen.\n2021. AliCG: Fine-grained and Evolvable Conceptual Graph Construction for\nSemantic Search at Alibaba. In KDD â€™21: The 27th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\n2021, Feida Zhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 3895â€“3905.\nhttps://doi.org/10.1145/3447548.3467057\n[58] Qi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang. 2018. Adaptive Co-\nattention Network for Named Entity Recognition in Tweets. In Proceedings\nof the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the\n30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI\nSymposium on Educational Advances in Artificial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018 , Sheila A. McIlraith and Kilian Q.\nWeinberger (Eds.). AAAI Press, 5674â€“5681. https://www.aaai.org/ocs/index.php/\nAAAI/AAAI18/paper/view/16432\n[59] Yu Zhao, Xiangrui Cai, Yike Wu, Haiwei Zhang, Ying Zhang, Guoqing Zhao, and\nNing Jiang. 2022. MoSE: Modality Split and Ensemble for Multimodal Knowledge\nGraph Completion. CoRR abs/2210.08821 (2022). https://doi.org/10.48550/arXiv.\n2210.08821 arXiv:2210.08821\n[60] Yu Zhao, Xiangrui Cai, Yike Wu, Haiwei Zhang, Ying Zhang, Guoqing Zhao, and\nNing Jiang. 2022. MoSE: Modality Split and Ensemble for Multimodal Knowledge\nGraph Completion. CoRR abs/2210.08821 (2022). https://doi.org/10.48550/arXiv.\n2210.08821 arXiv:2210.08821\n[61] Changmeng Zheng, Junhao Feng, Ze Fu, Yi Cai, Qing Li, and Tao Wang. 2021.\nMultimodal Relation Extraction with Efficient Graph Alignment. InMM â€™21: ACM\nMultimedia Conference, Virtual Event, China, October 20 - 24, 2021 , Heng Tao\nShen, Yueting Zhuang, John R. Smith, Yang Yang, Pablo Cesar, Florian Metze,\nand Balakrishnan Prabhakaran (Eds.). ACM, 5298â€“5306. https://doi.org/10.1145/\n3474085.3476968\n[62] Changmeng Zheng, Zhiwei Wu, Junhao Feng, Ze Fu, and Yi Cai. 2021. MNRE: A\nChallenge Multimodal Dataset for Neural Relation Extraction with Visual Evi-\ndence in Social Media Posts. In 2021 IEEE International Conference on Multimedia\nand Expo (ICME) . 1â€“6. https://doi.org/10.1109/ICME51207.2021.9428274",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8183591961860657
    },
    {
      "name": "Transformer",
      "score": 0.6237102150917053
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5176846981048584
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48397159576416016
    },
    {
      "name": "Unary operation",
      "score": 0.43852195143699646
    },
    {
      "name": "Knowledge graph",
      "score": 0.4277365803718567
    },
    {
      "name": "Modalities",
      "score": 0.4217430055141449
    },
    {
      "name": "Graph",
      "score": 0.4118387997150421
    },
    {
      "name": "Machine learning",
      "score": 0.3860706388950348
    },
    {
      "name": "Information retrieval",
      "score": 0.35411518812179565
    },
    {
      "name": "Theoretical computer science",
      "score": 0.19266584515571594
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ],
  "cited_by": 173
}