{
  "title": "Advancing bioinformatics with large language models: components, applications and perspectives",
  "url": "https://openalex.org/W4390783938",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1938222095",
      "name": "Liu Jiajia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2636252976",
      "name": "Yang Mengyuan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Yu, Yankai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2295387447",
      "name": "Xu Haixia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225979702",
      "name": "Wang Tian-gang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1976779484",
      "name": "Li Kang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186664905",
      "name": "Zhou Xiaobo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2985765936",
    "https://openalex.org/W2062458432",
    "https://openalex.org/W3204213738",
    "https://openalex.org/W4283219703",
    "https://openalex.org/W4290546063",
    "https://openalex.org/W2572375651",
    "https://openalex.org/W4385955324",
    "https://openalex.org/W2788348358",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4365517193",
    "https://openalex.org/W4206905277",
    "https://openalex.org/W3166730506",
    "https://openalex.org/W4318294156",
    "https://openalex.org/W3208999135",
    "https://openalex.org/W4206092375",
    "https://openalex.org/W3165171933",
    "https://openalex.org/W4384818053",
    "https://openalex.org/W2949342052",
    "https://openalex.org/W4281790889",
    "https://openalex.org/W4200321201",
    "https://openalex.org/W4295165394",
    "https://openalex.org/W4319065545",
    "https://openalex.org/W2799796757",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2801443765",
    "https://openalex.org/W2885949257",
    "https://openalex.org/W2889236955",
    "https://openalex.org/W3193839790",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3093858607",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W3104705366",
    "https://openalex.org/W2747545374",
    "https://openalex.org/W3005844923",
    "https://openalex.org/W3151678771",
    "https://openalex.org/W4387966979",
    "https://openalex.org/W4297179162",
    "https://openalex.org/W4287724045",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W4283068487",
    "https://openalex.org/W4234609530",
    "https://openalex.org/W4316096017",
    "https://openalex.org/W4287071232",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4286744503",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W2004889695",
    "https://openalex.org/W2002226820",
    "https://openalex.org/W4292265045",
    "https://openalex.org/W4281653422",
    "https://openalex.org/W3042910002",
    "https://openalex.org/W4206334877",
    "https://openalex.org/W4200139236",
    "https://openalex.org/W4239026366",
    "https://openalex.org/W4302362590",
    "https://openalex.org/W4385373960",
    "https://openalex.org/W4285405240",
    "https://openalex.org/W2115122333",
    "https://openalex.org/W4281768709",
    "https://openalex.org/W4220946902",
    "https://openalex.org/W4366526178",
    "https://openalex.org/W4312607928",
    "https://openalex.org/W4309485183",
    "https://openalex.org/W4318679072",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4322832102",
    "https://openalex.org/W4392168151",
    "https://openalex.org/W4221157572",
    "https://openalex.org/W4210421570",
    "https://openalex.org/W2173256032",
    "https://openalex.org/W2184331293",
    "https://openalex.org/W4205953903",
    "https://openalex.org/W2150695488",
    "https://openalex.org/W4394763992",
    "https://openalex.org/W2782708334",
    "https://openalex.org/W3184675023",
    "https://openalex.org/W2743416243",
    "https://openalex.org/W4206078729",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W4205143445",
    "https://openalex.org/W4318751307",
    "https://openalex.org/W3106188259",
    "https://openalex.org/W2572662684",
    "https://openalex.org/W2979911343",
    "https://openalex.org/W4283810898",
    "https://openalex.org/W3084084616",
    "https://openalex.org/W4387346412",
    "https://openalex.org/W4285596404",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W2152656267",
    "https://openalex.org/W4323348775",
    "https://openalex.org/W4225930681",
    "https://openalex.org/W4304172601",
    "https://openalex.org/W4319335178",
    "https://openalex.org/W2964022491",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3217568487",
    "https://openalex.org/W4306404289",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4256407664",
    "https://openalex.org/W2161689979",
    "https://openalex.org/W4309506674",
    "https://openalex.org/W3087291937",
    "https://openalex.org/W2972589977",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W3199180046",
    "https://openalex.org/W4224220091",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W3095930322",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W2583907533",
    "https://openalex.org/W2089806726",
    "https://openalex.org/W2109126584",
    "https://openalex.org/W4283212141",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W4225917625",
    "https://openalex.org/W4214937996",
    "https://openalex.org/W3143156272",
    "https://openalex.org/W4361284397",
    "https://openalex.org/W4280625391",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W4300985631",
    "https://openalex.org/W4309490745",
    "https://openalex.org/W2800555991",
    "https://openalex.org/W4289406437",
    "https://openalex.org/W2807110181",
    "https://openalex.org/W4382490702",
    "https://openalex.org/W4316339774",
    "https://openalex.org/W4377029825",
    "https://openalex.org/W2950118240",
    "https://openalex.org/W2916020270",
    "https://openalex.org/W2127640925",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W4308834893",
    "https://openalex.org/W2894687190",
    "https://openalex.org/W2093025795",
    "https://openalex.org/W4321452549",
    "https://openalex.org/W4296907865",
    "https://openalex.org/W3013151148",
    "https://openalex.org/W4321254242",
    "https://openalex.org/W4226059287",
    "https://openalex.org/W3112376646",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4296613150",
    "https://openalex.org/W4362522288",
    "https://openalex.org/W4365455552",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3195980265",
    "https://openalex.org/W4377093052",
    "https://openalex.org/W4294495243",
    "https://openalex.org/W4283390570",
    "https://openalex.org/W2893675751",
    "https://openalex.org/W2992033677",
    "https://openalex.org/W4317033517",
    "https://openalex.org/W4316339987",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4220798957",
    "https://openalex.org/W4367602258",
    "https://openalex.org/W4291302261",
    "https://openalex.org/W4210371902",
    "https://openalex.org/W4385230279",
    "https://openalex.org/W2953008890",
    "https://openalex.org/W1501531009",
    "https://openalex.org/W4388539614",
    "https://openalex.org/W4378806473",
    "https://openalex.org/W4327676467"
  ],
  "abstract": "Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will provide a comprehensive overview of the essential components of large language models (LLMs) in bioinformatics, spanning genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis. Key aspects covered include tokenization methods for diverse data types, the architecture of transformer models, the core attention mechanism, and the pre-training processes underlying these models. Additionally, we will introduce currently available foundation models and highlight their downstream applications across various bioinformatics domains. Finally, drawing from our experience, we will offer practical guidance for both LLM users and developers, emphasizing strategies to optimize their use and foster further innovation in the field.",
  "full_text": "Advancing bioinformatics with large language models: components, \napplications and perspectives \nJiajia Liu1,‚Ä†, Mengyuan Yang2,‚Ä†, Yankai Yu3,‚Ä†, Haixia Xu1,‚Ä†,  Tiangang Wang1, Kang Li4, Xiaobo Zhou1,5,6,* \n \n1Center for Computational Systems Medicine, McWilliams School of Biomedical Informatics, The \nUniversity of Texas Health Science Center at Houston, Houston, Texas, 77030, USA \n2Department of Cell Biology and Genetics, School of Basic Medical Sciences, Xi ‚Äôan Jiaotong University \nHealth Science Center, Xi‚Äôan, China \n3School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, Sichuan \n611756, China \n4West China Biomedical Big Data Center, West China Hospital, Sichuan University, Chengdu, Sichuan \n610041, China \n5McGovern Medical School, The University of Texas Health Science Center at Houston, Houston, TX \n77030, USA \n6School of Dentistry, The University of Texas Health Science Center at Houston, Houston, TX 77030, USA \n‚Ä†These authors have contributed equally to this work. \n \n \n* Address correspondence to: \nXiaobo Zhou, Ph.D.  \nMcWilliams School of Biomedical Informatics \nThe University of Texas Health Science Center at Houston \n7000 Fannin St., Houston, TX 77030 \nPhone: 713-500-3923 \nEmail: Xiaobo.Zhou@uth.tmc.edu  \n  \nAbstract \nLarge language models (LLMs) are a class of artificial intelligence models based on deep learning, \nwhich have great performance in various tasks, especially in natural language processing  (NLP). \nLarge language models typically consist of artificial neural networks with numerous parameters, \ntrained on large amounts of unlabeled input using self-supervised or semi -supervised learning. \nHowever, their potential for solving bioinformatics problems may even exceed their proficiency \nin modeling human language. In this review, we will provide a comprehensive overview of the \nessential components of large language models (LLMs) in bioinformatics, spanning genomics, \ntranscriptomics, proteomics, drug discovery, and single-cell analysis. Key aspects covered include \ntokenization methods for diverse data types, the architecture of transformer models, the core \nattention mechanism, and the pre -training processes underlying these models. Additionally, we \nwill introduce currently available foundation models and highlight their downstream applications \nacross various bioinformatics domains. Finally, drawing from our experience, we will offer \npractical guidance for both LLM users and developers, emphasizing strategies to optimize their \nuse and foster further innovation in the field. \n \n1. Introduction \nSignificant progress has been made in the field of natural language processing with the advent of \nlarge language models. Examples of these models include OpenAI‚Äôs GPT -X [1] and Google‚Äôs \nBERT [2] models. These models are transformative because they can understand, generate, and \nmanipulate human language at an unprecedented scale. Vast Large language models are typically \ntrained on datasets that encompass a significant portion of the internet ‚Äôs text, enabling them to \nlearn the complexities of language and context.  These models are built upon a neural network \narchitecture called transformers  [3]. The transformer  architecture revolutionized NLP due to its \nparallelization, scalability, and ability to  capture long -range dependencies in text. Instead of \nrelying on recurrent or convolutional layers, transformers use self -attention mechanisms, as \npreviously described, which allow them to assess the importance of every word in a sentence when \nunderstanding context. This innovation is key to their remarkable performance. \nThe training regimen for large language models comprises two phases: pre-training and fine-tuning. \nDuring pre-training, the model is trained on an extensive corpus of text data to acquire proficiency \nin grammar, factual knowledge, reasoning abilities, and word understanding. Fine-tuning tailors \nthese models for specific tasks like translation, summarization, or question -answering. The \nadaptability of large language models is a major advantage; they can excel at  various NLP tasks \nwithout task -specific architectures.  However, they have found applications in diverse fields \nbeyond NLP, including biology, healthcare, education, finance, customer service, and more.  In \nparticular, there have been many successful applications of large language models in the field of \nbioinformatics. In this manuscript, we focus on the applications of large language models to \nseveral bioinformatic tasks through five areas:  DNA level, RNA level, protein level , drug \ndiscovery and single-cell analysis. Applications of LLMs in genomics focus on LLMs using DNA \nsequence; applications of LLMs focus on in transcriptomics using RNA sequence; applications of \nLLMs in proteomics focus on LLMs using protein sequence; a pplications of LLMs in drug \ndiscovery focus on LLMs using Molecular SMILES (seq) and applications of LLMs in single-cell \nanalysis focus on LLMs using scRNA-seq, scMulti-omics and spatial transcriptomics data (Figure \n1). \n \n2. Understanding the Building Blocks of Large Language Models in Bioinformatics \nBuilding large language models involves several critical components, including tokenization \nmethods, embedding techniques, attention mechanisms, transformer architectures, and the training \nprocesses for large-scale models. Each of these elements plays a vital role in enabling the models \nto process, understand, and generate complex data. \n2.1 Tokenization and input embedding \nTokenization methods are essential for processing raw input data, breaking it down into smaller, \nmanageable units (tokens) that can be analyzed and processed by models. The choice of \ntokenization method varies depending on the type of data being handled (Figure 2a, Table 1).  \nIn DNA and RNA sequence data, tokenization converts raw nucleotide sequences (A, T, C, G for \nDNA or A, U, C, G for RNA) into a numerical format suitabl e for computational models. A \ncommon method is one-hot encoding, where each nucleotide is represented as a binary vector with \na ‚Äò1‚Äô indicating its position (e.g., [1,  0, 0, 0] for A in DNA), as used in RNA -FM [4] and RNA-\nMSM [5]. Another widely adopted approach is k-mer tokenization, which segments sequences into \noverlapping substrings of fixed length ‚Äòk‚Äô (e.g., for k=3, ‚ÄúATGC‚Äù becomes ‚ÄúATG‚Äù and ‚ÄúTGC‚Äù). \nThis method is employed in models like DNABERT [6], DNAGPT  [7], and RNABERT  [8]. \nAdditionally, specialized tokens such as ‚Äò[IND]‚Äô can be introduced to mark the start or end of \nsequences or to handle unknown characters or gaps, as demonstrated in RNAErnie [9]. \nIn protein language models, the input data primarily includes multiple sequence alignments \n(MSAs), protein sequences, biomedical/biological text, and cDNA. The basic units of MSAs and \nprotein sequences are amino acids, leading most protein language models to use Single Amino \nAcid Tokenization, where protein sequences are segmented into individual amino acids. This \napproach is akin to the k -mers method used for DNA and RNA sequences and is employed in \nmodels such as ESM-1b [10], ProtTrans [11], and ProGen [12]. For biomedical and biological text, \nincluding general descriptions, conditioning tags in generative models, and resources like Gene \nOntology (GO), tokenization methods from natural language processing (NLP) are widely used. \nMethods like WordPiece Tokenization build vocabulary using frequency-based greedy algorithms \nand segment text into discrete tokens, as demonstrated in ProtST  [13]. For cDNA data, \ntokenization is similar to that of protein sequences but differs in the basic unit. Instead of amino \nacids, sequences are tokenized into codons, or triplets of nucleotides, as seen in CaLM [14]. \nIn drug discovery, small molecule drugs account for 98% of commonly used medications [1]. \nLLMs leverage four main tokenization methods to uncover molecular patterns and drug -target \ninteractions. Atom-level tokenization treats molecules as sequences of individual atoms, analogous \nto character-level text representation, as seen in K -BERT [15]. MolGPT [16] utilizes a SMILES \ntokenizer that segments molecular structures into units such as atoms, bond types, and ring markers. \nA Graph-based VQ-VAE approach enhances this by encoding atoms into context -aware discrete \nvalues, distinguishing roles like aldehyde versus ester carbons, based on latent codes derived from \na graph-based Vector Quantized Variational Autoencoder (VQ -VAE). This method categorizes \natoms into chemically meaningful sub -classes, enriching the molecular vocabulary. Fingerprint \ntokens, another met hod, represent molecules through binary or numerical vectors summarizing \nmolecular properties or structural patterns, as seen in SMILES-BERT [17].  \nTokenization methods for single -cell p rofiles include four main strategies. Gene \nranking/reindexing-based methods rank genes by expression levels and create tokens using ranked \ngene symbols or unique integer identifiers, as seen in Geneformer  [18] and tGPT [19]. Binning-\nbased methods divide gene expression into predefined intervals, assigning tokens based on the \ncorresponding bin, used in models like scBERT [20] and scGPT [21]. Gene set or pathway-based \nmethods group genes into biologically meaningful sets, such as pathways or Gene Ontology terms, \nwith tokens representing the activation of these sets, exemplified by TOSICA  [22]. Patch-based \nmethods segment gene expression vectors into equal -sized sub-vectors, as seen in CIForm [152]. \nAlternatively, convolutional neural networks (CNNs) can be used to transform the reshaped gene \nexpression matrix into several flattened 2D patches, as demonstrated by scTranSort [23]. Another \nvariation involves reshaping the sub-vectors into a gene expression matrix after segmentation, as \nemployed in scCLIP [24]. In addition to the four methods mentioned above, a more direct approach \ninvolves projecting g ene expression directly, as seen in models like scFoundation  [25], and \nscMulan [26]. Alternatively, some methods tokenize cells instead of genes, as exemplified by \nmodels such as CellPLM  [27], ScRAT [28], and mcBERT [29], which utilize cell tokens during \nmodel training  (Table 1 ). These strategies allow models to capture biological structure and \nvariability, tailoring tokenization to single-cell data characteristics.  \nAfter tokenization, embedding converts tokens into continuous vector representations, capturing \nthe semantic relationships between them. Positional encoding represents the token order by adding \nvectors that encode the relative or absolute positions of toke ns in the sequence. The final step \ninvolves combining the token embeddings with the positional embeddings to create a unified input \nembedding, which is then fed into the model for further processing (Figure 2b). \n2.2 Architecture of transformer models \nTransformers are the foundational architecture in large language models (LLMs) and consist of \ntwo main components: the encoder and the decoder. The encoder takes the input data and processes \nit in parallel across multiple layers to capture relationships within the sequence. The decoder, on \nthe other hand, generates output sequences based on the encoder‚Äôs processed information, typically \nused in tasks like translation or text generation. Each component is built on layers of multi -head \nattention, add and norm layer, and feed-forward layer (Figure 2c). \nAttention Mechanism : A key innovation of the transformer is the attention mechanism, \nparticularly self-attention [3], which allows the model to weigh the importance of different tokens \nin a sequence relative to each other. In self -attention, each token computes a score based on how \nmuch attention it should pay to other tokens in the sequence. This is done by calculating three key \ncomponents: Query (Q), Key (K), and Value (V) vectors for each token (Figure 2d). The attention \nscore is computed as the dot product between the Query of one token and the Key of another token, \nfollowed by a softmax operation to normalize the scores. These scores are then used to weight the \nValue vectors, which are aggregated to form the output representation for each token as following \n[3]: \nùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ,ùêæ,ùëâ) = ùë†ùëúùëìùë°ùëöùëéùë•(\nùëÑùêæùëá\n‚àöùëëùëò\n)ùëâ                                           (1) \nMulti-head attention extends this idea by running multiple attention mechanisms (or ‚Äúheads‚Äù) in \nparallel. Each attention head processes the input tokens in a slightly different way by using \ndifferent sets of learned weights for the Q, K, and V vectors. The results of all heads are \nconcatenated and linearly transformed, allowing the model to capt ure different aspects of \nrelationships between tokens simultaneously. This mechanism enables the model to focus on \nvarious parts of the input sequence at once, learning different types of interactions between tokens. \nFor example, in single -cell foundation models, self -attention can help identify important gene \ninteractions by determining which genes (tokens) should focus on each other during processing. \nIn this way, multi-head attention allows the model to capture complex relationships between genes \nin single-cell RNA-seq data, where multiple aspects of gene expression (such as co -expression \npatterns or functional relationships) need to be captured simultaneously.  \nAdd and norm layer: The add and norm layer performs layer normalization and residual \nconnections, which help stabilize training by ensuring that the output from each layer is added to \nthe input before being normalized. This allows for smoother gradient flow and avoids the vanishing \ngradient problem. \nFeed-forward layer: After the attention mechanism, the feed -forward network is a fully \nconnected neural network (Figure 2e), helping the model learn complex mappings and capture \nmore abstract representations of the input data. \n2.3 BERT and GPT models \nBERT and GPT stand as two exceptional language models. Both BERT and GPT leverage the \ntransformer architecture, employing attention mechanisms to grasp dependencies within input data. \nBERT (Bidirectional Encoder Representations from Transformers) . BERT is basically an \nencoder stack of transformer architecture, which was introduced by Google in 2018 [2]. BERT is \ntrained using a bidirectional approach, meaning it considers context from both the left and right of \neach token during training. This enables BERT to capture richer, more context -aware \nrepresentations. BERT is typically pre-trained using a masked language model (MLM) task, where \nrandom tokens in a seq uence are masked, and the model is tasked with predicting them. This \nbidirectional training allows BERT to better understand the full context of a sequence or biological \nsentence, such as a cell (Figure 2f). For example, scBERT, a single -cell adaptation of BERT, \napplies this approach to single-cell RNA-seq data. By masking random gene tokens and predicting \nthem during pretraining, scBERT learns complex dependencies and co -expression patterns \nbetween genes. Th is enables it to capture the full transcriptional context of individual cells, \nimproving downstream tasks like cell type classification. \nGPT (Generative Pretrained Transformer ). Introduced by OpenAI  [1], GPT is based on a \ndecoder stack of transformer architecture . Unlike BERT, GPT uses a unidirectional training \napproach, processing the input sequence from left to right  (Figure 2 f). It is trained using \nautoregressive learning, where each token is predicted based on the previous ones, making it \nparticularly suited for generational tasks. GPT excels in zero-shot learning, where they perform \ntasks without needing task-specific training data. For example, DNAGPT leverages its pretrained \nknowledge to perform tasks like predicting DNA motifs or identifying regulatory elements without \nexplicit task-specific training. When prompted with a sequence such as ‚ÄúFind the transcription \nfactor binding motif in the following DNA sequence: AGCTTAGGCC...‚Äù, DNAGPT can identify \nor generate plausible motifs based on its understanding of DNA patterns learned during pretraining. \n \n3. Foundation models in bioinformatics \n3.1 Key components of biological foundation models \nFoundation models are a category of large -scale, pre-trained models designed to be versatile and \nadaptable to various downstream tasks. They are built upon several fundamental components that \nenable their widespread applicability and effectiveness across d omains. First, foundation models \nare trained on extensive and diverse datasets to capture broad, generalizable patterns. In single -\ncell biology, for example, datasets with millions of cells spanning multiple tissues and conditions \nare often used. Second, the architecture of foundation models is typically designed for flexibility \nand scalability. Their architecture, often transformer-based (e.g., GPT and BERT), are specifically \ndesigned for flexibility and scalability . Third, Self-supervised learning is a core training strategy \nfor foundation models. By creating tasks such as masked prediction, contrastive learning, or next-\ntoken prediction, models can learn representations without requiring labeled data. Fourth, \nfoundation models exhibit multi-task transferability, leveraging a two-step process of pre-training \nand fine-tuning (Figure 3). During pre-training, these models are trained on large -scale datasets \nto develop robust generalization capabilities by capturing broad patterns and knowledge. Fine -\ntuning involves adapting the pre-trained model to specific tasks by exposing it to unique data and \nadditional training. This approach enables foundation models to adjust effectively to diverse \napplications while maintaining their versatility across a wide range of domains. Last but not least, \ntraining foundation models requires significant computational resources, often involving GPU or \nTPU clusters. Foundation models typically feature billions or even trillions of parameters. \n3.2 Foundation models in different biological domains \nDNA foundation models. Currently, DNA sequence-based foundation models are powerful tools \nthat leverage advanced deep learning architectures to analyze and interpret genomic data  [30]. \nThese models are built on frameworks like BERT and GPT, which have been adapted for the \nspecific challenges of genomic sequences (Table 2). For example, DNABERT [6]is a BERT-based \nmodel trained on the human reference genome, enabling it to capture the contextual relationships \nbetween nucleotides and perform tasks such as sequence classification and variant prediction. \nExpanding beyond a single species, Nucleotide Transformer  [31] and Genomic Pre -trained \nNetwork (GPN) [32] are transformer-based models that incorporate the multiple species reference \ngenomes, providing a broader understanding of genomic diversity. DNABERT-2 [33] takes this a \nstep further by training on multi-species genomic data from 135 species, allowing for cross-species \ngenomic analysis. Similarly, GROVER [34], another BERT-based model, is focused on the human \nreference genome and is designed for applications such as understanding gene expression and \nfunctional genomics. On the other hand, DNAGPT, based on the GPT architecture, is trained not \nonly on the human reference genome but also on reference genomes from nine other species, \nfacilitating tasks such as sequence generation and evolutionary analysis. Together, these DNA \nsequence-based foundation models represent a leap forward in computational genomics, enabling \nmore accurate predictions, better understanding of genetic variation, and advancements in \npersonalized medicine. \nRNA foundation models. RNA sequence-based language models, particularly BERT -based and \nTransformer-based models, have gained significant traction in the analysis of RNA sequences due \nto their ability to understand the complex patterns and structures of RNA. These models are trained \nusing a wide variety of RNA types, including non -coding RNAs (ncRNAs), coding RNA, and \nuntranslated regions (UTRs), across diverse organisms  (Table 2). For instance, RNABERT  [8], \nRNA-FM [4], RNA-MSM [5], and UNI-RNA [35] focus on all ncRNA types from a broad range \nof species, enabling insights into RNA function and interactions. Models like SpliceBERT  [36] \nspecialize in coding RNA sequences from 72 vertebrates, while 3UTRBERT  [37] is specifically \ndesigned for human mRNA transcripts, particularly the 3' untranslated regions. Additionally, \nUTR-LM [38] focuses on 5' UTR sequences from five species, and RNAErnie [9], a Transformer-\nbased model, covers a wide range of ncRNAs. These models are part of a rapidly growing field \naimed at advancing RNA sequence analysis, facilitating the study of RNA biology and its role in \nvarious biological processes and diseases. Through the use of these RNA-based language models, \nresearchers can make significant strides in understanding RNA structure, function, and regulatory \nmechanisms. \nProtein foundation models.  Foundation models for proteins can be directly utilized to obtain \nhigh-quality protein embeddings and support various downstream applications. The foundational \nprotein models listed in Table 2  not only fulfill these requirements but also exhibit unique \ncharacteristics. For example, TAPE [39] made a significant contribution by introducing a \ncomprehensive benchmark for protein bioinformatics tasks. ESM-1b [40] applied the transformer \narchitecture of large language models in a highly standardized manner to protein representation \nlearning. This model has since been widely used to generate protein sequence embeddings, and its \nvariants can also be found via the sam e link provided in Table 2. ProtTrans [11], compared to \nESM-1b, significantly expanded the model architecture, the number of parameters, and the size of \nthe training dataset. It has been widely adopted as a frozen encoder for protein sequences. \nProtGPT2 [41], as its name suggests, extends GPT-2 into the protein domain (with links providing \ndetails on the GPT-2 training framework). Recent foundation models like ProtBert [42] and KeAP \n[43] integrate bio medical text information alongside protein sequences. Notably, KeAP \nincorporates a knowledge graph to enhance this integration . Both  models demonstrate that \nmultimodal fusion within proteomics often produces more expressive features. CaLM [14], on the \nother hand, represents proteins using cDNA, embedding cross-omics biological information. From \nthe perspective of algorithmic advancements, the integration of multimodal information within a \nsingle omics domain, as well as cross-omics data fusion, represents key strategies for constructing \nunified large-scale biological models. \nDrug discovery foundation models. It has been postulated that the total number of potential drug \nlike candidates range from 10 23 to 10 60 molecules[44]. Foundation models leverage diverse \ntokenization strategies, embedding techniques, and pre-training mechanisms to enhance molecular \nrepresentation learning, facilitating the optimization of various downstream tasks  (Table 2). For \ninstance, Mol-BERT [45] employs a context -aware tokenizer to encode atoms into chemically \nmeaningful discrete values, although this approach results in an unbalanced atom vocabulary. \nSMILES-BERT [46], a semi -supervised model incorporating an attention -based Transformer \narchitecture, utilizes datasets such as LogP, PM2, and PCBA-686978 to pre-train the model via a \nMasked SMILES Recovery (MSR) task. This model demonstrates strong generalization \ncapabilities, enabling its application to diverse molecular property prediction tasks through fine -\ntuning. Similarly, Mol-GPT [47] facilitates the generation of molecules with specific scaffolds and \ndesired molecular properties by conditioning the generation process on scaffold SMILES strings \nand property values. Notably, SynerGPT  [48] enables a pre -trained GPT model to perform in -\ncontext learning of ‚Äúdrug synergy functions‚Äù , showcasing potential for future advancements in \npersonalized drug discovery. These foundation models  developed based on distinct strategies, \neffectively learn representations from raw sequence data and molecular descriptors. They provide \nsignificant insights into the design of small -molecule drugs, drug -drug interactions, and drug -\ntarget interactions. \nSingle-cell foundation models. Foundation models in single-cell analysis are revolutionizing the \nfield by offering scalable and versatile solutions for a wide range of tasks, leveraging both cell and \ngene-level representations (Table 2). Models like scBERT  [20], tGPT [19], scMulan [26], UCE \n[49] and CancerFoundation [50] focus on learning robust cell representations, effectively \nsupporting applications such as cell clustering, cell type annotation, batch effect correction,  \ntrajectory inference and drug response prediction. These models excel at analyzing heterogeneous \ncellular populations and uncovering cellular dynamics. In contrast, models like scGPT [21], \nscFoundation [25], Geneformer [18]  GeneCompass [51] and scPRINT [52] combine the ability to \nlearn both cell and gene-level representations. They capture inter-gene relationships and regulatory \nnetworks, making them highly effective for tasks such as gene expression profiling, gene \nregulatory network (GRN) inference, gene pertur bation prediction, and drug dose -response \nprediction. Notably, scGPT can also handle single -cell multi-omics data, facilitating tasks like \nscRNA-seq and scATAC -seq integration. Another notable model is Nicheform er [53], a  \nfoundation model specifically designed for spatial transcriptomics. It focuses on learning cell \nrepresentations while being highly adaptable to various downstream tasks in spatial \ntranscriptomics, such as spatial label prediction (e.g., cell type, niche, and region labels), niche \ncomposition analysis, and neighborhood density prediction. Additionally, Nicheformer can \ngenerate joint embeddings of scRNA -seq and spatial transcriptomics data, facilitating the \nintegration of these modalities for a more comprehensive understanding of cellular and spatial \ninteractions. \n \n4. Applications of large language models in bioinformatics \nLarge language models (LLMs) have seen numerous successful applications in bioinformatics, \naddressing a wide array of tasks across DNA, RNA, protein, drug discovery, and single -cell \nanalysis (Figure 4 ). These applications highlight the adaptability and potential of LLMs in \novercoming bioinformatic challenges, enabling deeper insights into complex biological systems \nand fostering advancements across multiple domains. \n4.1 Applications of large language models in genomics \nThe DNA language models take DNA sequence as input, use transformer, BERT, GPT models to \nsolve multiple biological tasks, including genome -wide variant effects prediction, DNA cis -\nregulatory regions prediction, DNA-protein interaction prediction, DNA methylation (6mA,4mC \n5hmC) prediction, splice sites prediction from DNA sequence  (Table 3, Supplementary Figure \n1). A detailed list of DNA language models, their downstream tasks, and the datasets used can be \nfound in Supplementary Table 1. \nGenome-wide variant effects prediction. Genome-wide variant effects prediction is crucial for \nunderstanding the role of DNA mutations in species diversity. Genome -wide association studies \n(GWAS) provide valuable insights but often struggle to identify specific causal variants  [30, 54]. \nThe Genome Prediction Network (GPN)  [32]  addresses this by using unsupervised pre -training \non genomic DNA sequences. During this process, GPN predicts nucleotides at masked positions \nwithin a 512 -bp DNA sequence. This model is particularly effective at predicting rare variant \neffects, often mis sed by traditional GWAS methods. Additionally, models like DNABERT, \nDNABERT-2, and the Nucleotide Transformer also predict variant effects from DNA sequences. \nThese advancements highlight ongoing efforts to better understand how DNA mutations contribute \nto biological diversity. \nCis-regulatory regions prediction. Cis-regulatory sequences, such as enhancers and promoters, \nplay crucial roles in gene expression regulation, influencing development and physiology  [55]. \nHowever, identifying these sequences remains a major challenge  [56]. Pre -trained models like \nDNABERT, DNABERT-2, GROVER, and DNAGPT have been developed to predict promoter \nregions and their activities with high accuracy. BERT -Promoter [57] utilizes a pre-trained BERT \nmodel for feature representation and SHAP analysis to filter data, improving prediction \nperformance and generalization over traditional methods. Enhancers, which bind transcription \nfactors to regulate gene expression  [58, 59] , are predicted by iEnhancer -BERT [60], which \nleverages DNABERT and uses a novel transfer learning approach. This model employs output \nfrom all transformer encoder layers and classifies features with a Convolutional Neural Network \n(CNN). These advancements highlight the growing trend of treati ng biological sequences as a \nnatural language for computational modeling, offering new tools for identifying cis -regulatory \nregions and understanding their roles in diseases. \nDNA-protein interaction  prediction. Accurate identification of DNA -protein interactions is \ncrucial for gene expression regulation and understanding evolutionary processes  [61]. Several \nDNA language models, including DNABERT, DNABERT -2, and GROVER, have been \ndeveloped to predict protein-DNA binding from ChIP-seq data. TFBert [62] is a pre-trained model \nspecifically designed for DNA-protein binding prediction, which treats DNA sequences as natural \nsentences and k -mer nucleotides as words, allowing effective context extraction. Pre -trained on \n690 ChIP -seq datasets, TFBert delivers s trong performance with minimal fine -tuning. The \nMoDNA [63] framework introduces domain knowledge by incorporating common DNA \nfunctional motifs. During self-supervised pre-training, MoDNA performs tasks such as k-mer and \nmotif prediction. Pre -training on extensive unlabeled genome data, MoDNA acquires semantic -\nlevel genome representations, enhancing predictions for promoter regions and transcription factor \nbinding sites. Essentially, MoDNA functions as a biological language model for DNA -protein \nbinding prediction. \nDNA methylation  prediction. DNA methylation is a key biological process in epigenetic \nregulation and is linked to various medical conditions and applications, such as metagenomic \nbinning [64]. DNA methylation types depend on the nucleotide where the methyl group attaches  \n[65]. Several models predict DNA methylation with varying accuracy. BERT6mA [66] is designed \nfor predicting 6-methyadenine (6mA) sites, while iDNA-ABT [67], iDNA-ABF [68], and MuLan-\nMethyl [69] are versatile models predicting various methylation types (6mA, 5hmC, 4mC). iDNA-\nABT, a deep learning model, integrates BERT with transductive information maximization (TIM), \nthough it has yet to fully explore feature representation. iDNA-ABF uses a multi-scale architecture, \napplying multiple tokenizers for diverse embeddings, and MuLan -Methyl employs four \ntransformer-based models (DistilBERT [70], ALBERT[71], XLNet [72], and ELECTRA [73]) to \npredict methylation sites, enhancing performance through joint model utilization. \nDNA level splice site identification. Accurate pre-mRNA splicing is essential for proper protein \ntranslation, driven by splice site selection. Identifying splice sites is challenging, particularly with \nprevalent GT-AG sequences [74]. To address this, DNABERT and DNABERT-2 were developed, \ntrained on 10,000 donor s, acceptor, and non -splice site sequences from the human reference \ngenome to predict splice sites. DNABERT showed high attention to intronic regions, suggesting \nthe functional role of intronic splicing enhancers and silencers as cis -regulatory elements in \nsplicing regulation. This highlights DNABERT‚Äôs potential in understanding splicing mechanisms. \n4.2 Applications of large language models in transcriptomics \nThe RNA language models take RNA sequences as input, use transformer, BERT, GPT models to \nsolve multiple biological tasks, including RNA 2D/3D structure prediction, RNA structural \nalignment,, RNA family clustering, RNA splice sites prediction from RNA sequence, RNA N7 -\nmethylguanosine modification prediction, RNA 2 ‚Äô-O-methylation modifications prediction, \nmultiple types of RNA modifications prediction, predicting the association between miRNA, \nlncRNA and disease, identifying lncRNAs, lncRNAs‚Äô coding potential prediction, protein \nexpression and mRNA degradation prediction (Table 3, Supplementary Figure 1). A detailed list \nof RNA language models, their downstream tasks, and the datasets used can be found in \nSupplementary Table 1. \nSecondary structure prediction. RNA secondary structure prediction is a major challenge for \nRNA structural biologists, with models holding potential for RNA -targeting drug development  \n[75]. Several RNA language models, such as RNABERT [8], RNA-MSM [5], RNA-FM [4], and \nUNI-RNA [35], have been devel oped to predict RNA structures with varying sophistication. \nRNABERT uses BERT architecture to predict structural features like base-pairing and stem loops. \nRNA-MSM integrates sequence and structural information to predict local and long-range folding \npatterns. RNA-FM focuses on RNA folding, stability, and energetics, including pseudoknots. UNI-\nRNA combines sequence and structure predictions across various RNA types. These models \nadvance RNA structure prediction by applying deep learning and advanced techniques to improve \nunderstanding of RNA folding and function. \nRNA splicing prediction . RNA splicing is crucial for gene expression in eukaryotes, and \nadvancements have been made in sequence -based splicing modeling through models like \nSpliceBERT [36] and UNI-RNA [35]. SpliceBERT, based on BERT, is trained to predict RNA \nsplicing events by capturing lon g-range dependencies, identifying splice sites, and predicting \nalternative splicing events. UNI-RNA, a more generalized model, integrates multiple RNA tasks, \nincluding splicing, and combines sequence and structural data to predict splicing regulatory \nelements and interactions with splicing factors. These models enhance the understanding of RNA \nsplicing, gene regulation, and its role in diseases, providing powerful tools f or studying splicing \ndefects and mutations. \nlncRNAs identification and lncRNAs‚Äô coding potential  prediction. Long non-coding RNAs \n(lncRNAs) play significant regulatory roles in cancer and diseases, and their small Open Reading \nFrames (sORFs), once thought weak in protein translation, are now known to encode peptides [76]. \nIdentifying lncRNAs with sORFs is crucial for discovering new regulatory factors. LncCat  [77] \naddresses this challenge by using category boosting and ORF-attention features, including BERT \nfor peptide sequence representation, to improve prediction accuracy for both long ORF and sORF \ndatasets. It demonstrates effectiveness across multiple species and Ribo-seq datasets in identifying \nlncRNAs with sORFs.  In predicting translatable sORFs in lncRNAs (lncRNA -sORFs), LSCPP-\nBERT [78]  is a novel method designed for plants, leveraging pre -trained transformer models for \nreliable coding potential prediction. LSCPP -BERT is poised to impact drug development and \nagriculture by enhancing understanding of lncRNA coding potential. \nRNA‚ÄìRBP interactions prediction. RNA sequences differ from DNA sequences by a single base \n(thymine to uracil), maintaining largely congruent syntax and semantics. BERT ‚Äôs versatility \nextends to Cross -linking and Immunoprecipitation  data, particularly in predicting RNA -binding \nprotein (RBP) binding preferences. BERT-RBP [79]  is a model pre-trained on a human reference \ngenome, designed to forecast RNA-RBP interactions. It outperforms existing models when tested \non eCLIP -seq data from 154 RBPs and can identify transcript regions and RNA secondary \nstructures based on sequence a lone. BERT-RBP demonstrates BERT‚Äôs adaptability in biological \ncontexts and its potential to advance RNA-protein interaction understanding. \nRNA-RNA interaction prediction. RNA‚ÄìRNA interactions occur between various RNA species, \nincluding long non-coding RNAs, mRNAs, and small RNAs (e.g., miRNAs and lncRNAs), driven \nby complementary sequences, secondary structures, and other motifs [80]. Accurate prediction of \nthese interactions provides insights into RNA -mediated regulation, enhancing understanding of \nbiological processes like gene expression, splicing, and translation.  RNAErnie, used for this \npurpose, employs a TBTH architecture combining RNAErnie with a hybrid network (CNN, Bi -\nLSTM, and MLP) to predict RNA ‚ÄìRNA interactions. This approach demonstrates RNAErnie's \npotential in advancing RNA-based regulatory network studies. \nRNA modification prediction. Post-transcriptional RNA modifications, such as N7 -\nmethylguanosine (m7G) and 2‚Äô -O-methylation (Nm), regulate gene expression and are linked to \ndiseases [76, 81]. Identifying modification sites is essential but challenging due to the high cost \nand time required by experimental methods. Computational tools like BERT -m7G [82] and \nBert2Ome [83] address this issue. BERT-m7G uses a stacking ensemble approach to identify m7G \nsites directly from RNA sequences, offering an efficient, cost -effective alternative. Bert2Om \ncombines BERT and CNN to predict 2‚Äô -O-methylation sites, outperforming existing me thods \nacross datasets and species. These tools enhance the accuracy, scalability, and efficiency of RNA \nmodification site identification, advancing research into RNA modifications and their roles in gene \nregulation and disease. \nProtein expression and mRNA degradation prediction. mRNA vaccines are a cost -effective, \nrapid, and safe alternative to traditional vaccines, showing high potency [84]. These vaccines work \nby introducing mRNA that encodes a viral protein. CodonBERT  [85] is a model specifically \ndesigned for mRNA sequences to predict protein expression. It uses a multi -head attention \ntransformer architecture and was pre -trained on 10 million mRNA sequences from various \norganisms. This pre -training enables CodonBERT to excel  in tasks like protein expression and \nmRNA degradation prediction. Its ability to integrate new biological information makes it a \nvaluable tool for mRNA vaccine development. CodonBERT surpasses existing methods, \noptimizing mRNA vaccine design and improvin g efficacy and applicability in immunization. Its \nstrength in predicting protein expression enhances mRNA vaccine development efficiency and \neffectiveness. \n5‚Äô UTR-based mean ribosome loading prediction and mRNA subcellular localization \nprediction. The 5‚Äô UTR sequence plays a critical role in regulating translation efficiency. RNA \nsequence models like 3UTRBERT, UNI-RNA, UTR-LM, RNA-FM, and Nucleotide Transformer \nhave been developed to predict key features of the 5 ‚Äô UTR, focusing on ribosome loading \nefficiency and mRNA localization. These models use Tran sformer-based architecture to analyze \nsequence patterns, motifs, and structural elements. For example, 3UTRBERT  [37] and RNA-FM \n[4] predict ribosome loading efficiency, identifying regions likely to recruit ribosomes for \ntranslation initiation. UTR -LM [38], UNI-RNA [35], and Nucleotide Transformer  [31] predict \nmRNA subcellular localization, determining where mRNA will localize in the cell (cyto plasm, \nribosomes, or nucleus), which is crucial for regulating mRNA stability and translation. Together, \nthese models provide valuable insights into gene expression, translation control, and RNA \nlocalization, advancing molecular biology research. \n4.3 Applications of large language models in proteomics \nProtein is an indispensable molecule in life, assuming a pivotal role in the construction and \nsustenance of vital processes. As the field of protein research advances, there has been a substantial \nsurge in the accumulation of protein data [86]. In this context, the utilization of large language \nmodels emerges as a viable approach to extract pertinent and valuable information from these vast \nreservoirs of data. Several pre -trained protein language models (PPLMs) have been proposed to \nlearn the c haracteristic representations of proteins data (e.g., protein sequences, gene ontology \nannotations, property descriptions), then applied to different tasks by fine -tuning, adding or \naltering downstream networks, such as protein structure, post-translational modifications (PTMs), \nand biophysical properties, which align with corresponding downstream tasks like secondary \nstructure prediction, major PTMs prediction, and stability prediction [87, 88].  \nEven though antibodies are classified as proteins, the datasets of antibodies and subsequent tasks \ndiffer significantly from those of proteins. Through the establishment and continuous updates of \nthe Observed Antibody Space (OAS) database [89], a substantial amount of antibody sequence \ndata has become available, which can be utilized to facilitate the development of pre -trained \nantibody large language models (PALMs). PALMs primarily delve into downstream topics \nencompassing therapeutic antibody  binding mechanisms, immune evolution, and antibody \ndiscovery, which correspond to tasks like paratope prediction, B cell maturation analysis, and \nantibody sequence classification (Table 3, Supplementary Figure 2).  \nIn this section, some of the popular protein -related large language models of recent years are \nintroduced, as well as corresponding important downstream tasks. It is important to emphasize that \nboth PPLM and PALM are capable of handling not only the downstream tasks introduced in this \nsection. For further details, additional information can be referenced within Supplementary Table \n2. \nSecondary structure and contact prediction . Protein structure is critical to its function and \ninteractions [90]. However, traditional experimental techniques for protein structure analysis are \ntime-consuming and labor-intensive. With the rise of deep learning, large language models have \ndemonstrated significant advantages in computational efficiency and prediction accuracy for \nprotein structure prediction [91]. MSA Transformer [92] introduces a protein language model that \nprocesses MSAs using a unique mechanism of interleaved row and column attention. Trained with \na MLM objective across diverse protein families, it outperformed earlier unsupervised approaches \nand showed greater parameter efficiency than previous models. Drawing on insights from BERT, \nlarge parameter models tend to achieve better performance for predicting secondary structures and \ncontacts.  Few models have more parameters than the largest models in ProtTrans [11], which \nincludes a series of  autoregressive models (Transformer-XL [93], XLNet [72]) and four encoder \n(BERT [2], Albert [71], Electra [73], T5 [94]) trained on datasets like UniRef [95] and BFD [96], \ncomprising up to 393 billion amino acids. Model sizes vary from millions to billions of parameters. \nNotably, ProtTrans made a significant breakthrough in per-residue predictions. \nProtein sequence generation. Protein sequence generation holds significant potential in drug \ndesign and protein engineering  [97]. Using machine learning or deep learning, generated \nsequences aim for good foldability, stable 3D structures, and specific functional properties, such \nas enzyme activity and antibody binding. The development of large language models, combined \nwith conditional models, has greatly advanced protein generation [98]. ProGen [12] incorporates \nUniprotKB keywords as conditional tags, covering over 1,100 categories like 'biological process' \nand 'molecular function'. Proteins generated by ProGen, assessed for sequence similarity, \nsecondary structure, and conformational energy, exhibit desirable structural properties.  In 2022, \nProtGPT2 [41] inspired by GPT-x was developed. ProtGPT2-generated proteins show amino acid \npropensities like natural proteins. Prediction of disorder and secondary structure reveals that 88% \nof these proteins are globular, resembling natural sequences.  Employing AlphaFold [99, 100] on \nProtGPT2 sequences produces well-folded, non-idealized structures with unique topologies not \nseen in current databases, suggesting ProtGPT2 has effectively learned ‚Äúprotein language‚Äù. \nProtein function prediction . Proteins are essential in cellular metabolism, signal transduction, \nand structural support, making their function critical for drug development and disease analysis. \nHowever, predicting and annotating protein functions is challenging due to their complexit y. \nPPLMs offer effective solutions to these challenges  [101, 102] . ProtST [103] introduced a \nmultimodal framework combining a PPLM for sequences and a biomedical language model (BLM) \nfor protein property descriptions. Through three pre -training tasks , unimodal mask prediction, \nmultimodal representation alignment, and multimodal mask prediction , the model excels in tasks \nlike protein function annotation, zero -shot classification, and functional protein retrieval from \nlarge databases.  While most methods focus on increasing model parameters to improve \nperformance, CaLM [14] introduces an alternative representation, the cDNA sequence, akin to an \namino acid sequence, as input. The core idea lies in the relationship between synonymous codon \nusage and protein structure  [104], and the information encoded in codons is no less than that of \namino acids. Experimental results demonstrate that even with a small parameter language model, \nusing cDNA sequences as input enhances performance in tasks such as protein function prediction, \nspecies recognition, prediction of protein and transcript abundance, and melting point estimation. \nMajor post-translational modification prediction.  Post-translational modifications (PTMs) are \nchemical changes, such as phosphorylation, methylation, and acetylation, that alter protein \nstructure and function after translation. PTMs influence protein stability, localization, interactions, \nand function, making their study crucial for disease diagnosis and therapeutic strategies [105, 106]. \nLanguage models can effectively predict PTMs and related tasks like signal peptide prediction. \nProteinBERT [42] , with only ~16M parameters, is not large enough but performs well due to its \ninclusion of Gene Ontology (GO) annotation tasks. By incorporating GO interactions with protein \nsequences, ProteinBERT achieves strong performance on PTM prediction and other pr otein \nproperty benchmarks, outperforming models with larger parameter sizes. \nEvolution and mutation prediction . Protein evolution and mutation drive functional diversity, \naiding adaptation to environmental changes and offering insights into protein function origin, \nwhich can inform drug development and disease treatment [107, 108]. UniRep [109], built on the \nLSTM architecture, was trained on the UniRef50 [95] and excelled in tasks like remote homology \ndetection and mutation effect prediction. ESM-1b [40] , a deep transformer model trained on 250 \nmillion sequences, with 33 layers and 650 million parameters, captures essential protein sequence \npatterns through self-supervised learning. ESM-1b is also integral to frameworks like PLMSearch \n[110] and DHR  [111] , which enable fast, sensitive homology searches. PLMSearch uses \nsupervised training, while DHR relies on unsupervised contrastive learning and enhances structure \nprediction models like AlphaFold2 [100]. \nBiophysical properties prediction. Biophysical properties of proteins, such as fluorescence and \nstability landscapes [112], are crucial for understanding protein folding, stability, and \nconformational changes, with significant implications for drug design, protein engineering, and \nenzyme engineering. Deep learning advancements have enabled more accurate prediction of these \nproperties using PPLMs.  TAPE benchmark [39] established standardized tasks for evaluating \nprotein, including fluorescence and stability landscape prediction. In 2022, PromptProtein [113],  \na prompt-based pre-trained model, incorporated multi-task pre-training and a fine-tuning module \nto improve task -specific performance. It outperformed existing methods in function and \nbiophysical properties prediction, demonstrating substantial gains in predictive accuracy.  \nProtein-protein interaction and binding affinity prediction. Protein-protein interactions (PPIs) \nare crucial for biological functions, and their prediction is also vital for drug discovery and design. \nPPLMs provide efficient, accurate predictions of PPI types and binding affinities [114, 115]. KeAP \nmodel [43], like ProtST, aims to integrate fine -grained information beyond OntoProtein [116]. \nKeAP uses a triplet format (Protein, Relation, Attribute) as input, processed by encoders and a \ncascaded decoder based on the Transformer architecture. Using MLM for pre -training, KeAP \nemploys a cross -attention fusion mechanism to capture detailed protei n information, achieving \nsuperior performance on tasks such as PPI identification and binding affinity estimation. \nAntigen-receptor binding and antigen -antibody binding prediction . Antigen proteins are \nprocessed into neoantigen peptides that bind to the Major Histocompatibility Complex (MHC), \nforming pMHC complexes. These complexes are presented to T -cells, stimulating antibody \nproduction by B-cells, which triggers an immune response  [117] . Predicting peptide binding to \nMHC molecules is a key focus of language models in this process [118, 119]. MHCRoBERTa \n[120] uses a pretrained BERT model to predict pMHC -I binding by learning the biological \nmeaning of amino acid sequences. BERTMHC [121], trained on 2,413 MHC ‚Äìpeptide pairs, \nfocuses on pMHC-II binding prediction, filling a gap in this area.  \nAnother goal is predicting the binding specificity of adaptive immune receptors (AIRs), \nparticularly TCRs. TCR-BERT [122] learns TCR CDR3 sequences to predict antigen specificity \nbut lacks the ability to model the interaction between TCR chains. SC-AIR-BERT [123] addresses \nthis by pre -training a model that outperforms others in TCR and BCR binding specificity.  \nAdditionally, the Antiformer  [124] integrates RNA -seq and BCT -seq data in a graph -based \nframework to improve antibody development. In antibody modeling, three recent models focus on \nunique tasks. AbLang [125], built on RoBERTa [126], excels at restoring lost residues during \nsequencing and outperforms other models in accuracy and efficiency. AntiBERTa [127] \nunderstands antibody \"language\" through tasks like predicting immunogenicity and binding sites. \nEATLM [128] , with its unique pre -training tasks (Ancestor Germline Prediction and Mutation \nPosition Prediction), contributes a reliable benchmark for antibody language models. \n4.4 Applications of large language models in drug discovery \nDrug discovery is an expensive and long-term process that exhibits a low success rate. During the \nearly stages of drug discovery, computer -aided drug discovery, employing empirical or expert \nknowledge algorithms, machine learning algorithms, and deep learning algorithms, serve to \naccelerate the generation and screening of drug molecules and their lead compounds [129-131]. It \nspeeds up the entire drug discovery process, especially the development of small molecule drugs. \nAmong commonly used medications, small molecule drugs can account for up to 98% of the total \n[132]. The structure of small molecule drugs exhibits excellent spatial dispersibility, and their \nchemical properties determine their good drug -like properties and pharmacokinetic properties \n[133]. With the development of deep learning and the proposal of large language models, it has \nbecome easy to apply these methods to discover hidden patterns of molecules and interactions \nbetween molecules for drugs (such as small molecules) and targets (such as proteins and RNA) \nthat can be easily represented as sequence data. The Simplified Molecular -Input Line -Entry \nSystem (SMILES) string and chemical fingerprint are commonly used to represent molecules.  \nAdditionally, through the pooling process of graph neural networks(GNN), small molecules can \nbe transformed into sequential representations [134]. With the protein sequence, large language \nmodels can engage in drug discovery through various inputs. Within this section, key tasks within \nthe early drug discovery process that have effectively leveraged larg e language models will be \nintroduced (Table 3, Supplementary Figure 3). A detailed list of drug discovery language models, \ntheir downstream tasks, and the datasets used can be found in Supplementary Table 3. \nDrug-like molecular properties prediction . In drug discovery, significant focus is placed on \nproperties like ADMET and PK to develop more effective, accessible, and safe drugs [135, 136]. \nLarge language models (LLMs) are used for molecular property prediction, including these \nproperties. Since molecular SMILES representations are consistent, models can be easily improved \nand fine-tuned for specific tasks based on researchers' requirements. SMILES-BERT [17] departed \nfrom the usage of knowledge -based molecular fingerprints as input. Instead, it adopted a \nrepresentation method where molecules were encoded as SMILES sequences and employed as \ninput for both pre -training and fine -tuning within a BERT -based model.  This novel approach \nyielded superior outcomes across various downstream molecular property prediction tasks, \nsurpassing the performance o f previous models reliant on molecular fingerprints. ChemBERTa \n[137] is a BERT-based model that focuses on the scalability of large language models, exploring \nthe impact of pre -training dataset size, tokenizer, and string representation.  Subsequently, \nChemBERTa-2[138] improved upon ChemBERTa by using a larger dataset of 77 million \ncompounds from PubChem, enhancing its ability to learn from diverse chemical structures. It also \nintegrates advanced self -supervised learning techniques and fine -tuning strategies, resulting in \nbetter generalization performance across various downstream tasks . K-BERT [15] stands out by \nusing three pre -training tasks: atom feature prediction, molecular feature prediction, and \ncontrastive learning. This approach enables the model to understand the essence of SMILES \nrepresentations, resulting in exceptional performance across 15 drug datasets, highlighting its \neffectiveness in drug discovery. Giv en the importance of graph neural networks in the \ndevelopment of molecular pre-training models, Mole-BERT [139] introduces atom-level Masked \nAtoms Modeling (MAM) task and graph-level Triplet Masked Contrastive Learning (TMCL) task. \nThese tasks enable the network to acquire a comprehensive understanding of the ‚Äúlanguage‚Äù \nembedded within molecular graphs. By adopting this approach, the network demonstrates \nexceptional performance across eight downstream tasks, showcasing its adaptability and \neffectiveness in diverse applications. \nDrug-like molecules generation. It is very difficult to chase the full coverage of the enormous \ndrug-like chemical space (estimated at mo re than 10 63 compounds), and traditional virtual \nscreening libraries usually contain less than 10 7 compounds and are sometimes not available. In \nsuch circumstances, the utilization of deep learning methods to generate molecules exhibiting \ndrug-like properties emerges as a viable approach [140, 141] . Inspired by the generative pre -\ntraining model GPT, MolGPT [16] model was introduced. In addition to performing the next token \nprediction task, MolGPT incorporates an extra training task for conditional prediction, facilitating \nthe capability of conditional generation. Beyond its capacity to generate innovative and efficacious \nmolecules, the model has demonstrated an enhanced ability to capture the statistical characteristics \nwithin the dataset. \nDrug-target interaction predictions . The investigation of Drug -Target Interaction (DTI) holds \nparamount significance in the realm of drug development and the optimization of drug therapy. \nUnderstanding drug -target interactions aids in pharmaceutical design, accelerates drug \ndevelopment, and reduces time and resource costs in lab experimentation and trial -and-error \nmethods [142, 143]. During the exploration of DTI, diligent focus is placed on the prediction of \ndrug-target binding affinity. DTI-BERT employs a fine-tuned ProtBERT [144] model to process \nprotein sequences and applies discrete wavelet transform to drug molecular fingerprints. . \nTransDTI [145] is a multi-class classification and regression workflow. This model not only uses \nfine-tuned SMILES-BERT to extract drug features, but also expands the selection of fine -tuned \nlarge protein models. After acquiring potential representations of drug -target pairs, the authors \nsubject the representations to downstrea m neural networks for the completion of a multi -\nclassification task. Additionally, The Chemical-Chemical Protein-Protein Transferred DTA (C2P2) \n[146] method uses pre -trained protein and molecular large language models to capture the \ninteraction information within molecules. Given the relatively limited scale of the DTI dataset, \nC2P2 leverages protein-protein interaction (PPI) and chemical-chemical interaction (CCI) tasks to \nacquire knowledge of intermolecular interactions and subsequently transfer this knowledge to \naffinity prediction tasks [147]. It is worth highlighting that in scenarios involving the docking or \nwhen emphasizing the spatial structure of a complex, methodologies incorporating 3D convolution \nnetworks, point clouds -based networks, and graph networks are often employed [148-151]. In \nsituations where the molecular structure is unknown, but the sequence is available, the prediction \nof DTI using large-scale models still holds significant promise. \nDrug synergistic effects predictions. Combination therapy is common for complex diseases like \ncancer, infections, and neurological disorders, often surpassing single-drug treatments. Predicting \ndrug pair synergy, where combining drugs boosts therapeutic effects, is vital in drug development. \nHowever, it‚Äôs challenging due to many drug combinations and complex biology [152, 153] . \nVarious computational methods, including machine learning, help predict drug pair synergy. Carl \nEdwards et al. introduced SynerGPT [48], which is based on GPT trained to in-context learn drug \nsynergy functions without relying on domain -specific knowledge.  Wei Zhang et al. [154] \nintroduced DCE -DForest [154], a m odel for predicting drug combination synergies. It uses a \npretrained drug BERT model to encode the drug SMILES and then predicts synergistic effects \nbased on the embedding vectors of drugs and cell lines using the deep forest method. Mengdie \nXua et al. [155] utilized a fine-tuned pre-trained large language model and a dual feature fusion \nmechanism to predict synergistic drug combinations. Its input includes hashed atom pair molecular \nfingerprints of drugs, SMILES string encodings, and cell line gene expressio ns. They conducted \nablation analyses on the dual feature fusion network for drug-drug synergy prediction, highlighting \nthe significant role of fingerprint inputs in ensuring high-quality drug synergy predictions. \n4.5 Applications of large language models in single-cell analysis \nLarge language models have demonstrated significant applications in single -cell analysis, \nincluding cell-level tasks such as identifying cell types, determining cell states, and discovering \nnovel cell populations; gene-level tasks like inferring gene regul atory networks; and multi-omics \ntasks, such as integrating single -cell multi-omics (scMulti-omics) data (Supplementary Figure \n4). Additionally, this section will explore emerging language models based on spatial \ntranscriptomics (Table 3). A detailed list of single -cell large language models, their downstream \ntasks, and the datasets used can be found in Supplementary Table 4. \nCell-level tasks . Cell-level tasks, such as cell clustering, cell type annotation, novel cell type \ndiscovery, batch effect removal and trajectory inference, are critical in single-cell analysis. These \ntasks often rely on cell representations learned during pretraining, which are subsequently fine -\ntuned for different tasks. Single -cell language models derive cell representations in two primary \nways. The first method utilizes a special class token (<cls>) appended to the i nput sequence; its \nembedding is updated through the transformer layers, and the final embedding at the <cls> position \nserves as the cell representation. The second method generates a cell embedding matrix from the \nmodel output, where each row represents a specific cell. Both approaches facilitate downstream \ntasks, as demonstrated by TOSICA  [22], which uses the <cls> token to predict cell type \nprobabilities using the whole conjunction neural network cell type classifier to annotate single \ncells, and iSEEK [156], which generates cell embedding for cell clustering, cell type annotation, \nand developmental trajectory exploration. Models like scBERT [20] and UCE [49] leverage multi-\nhead attention mechanisms to extract information from diverse representation subspaces, \ndiscerning subtle differences between novel and known cell types. Their large receptive fields \ncapture long -range gene -gene interactions, enabling compr ehensive characterization of novel \ncellular states. Addressing batch effects, which arise from variations in species, tissues, operators, \nand experimental protocols, remains a significant challenge in single-cell analysis. Large language \nmodels, pretrained on extensive datasets, utilize attention mechanisms to incorporate prior \nbiological knowledge, enabling batch -insensitive data annotation. Without relying on explicit \nbatch information, models such as CIForm [152] have dem onstrated effectiveness in both intra -\ndataset and inter-dataset scenarios. They handle annotations across diverse species, organs, tissues, \nand technologies while also supporting the integration of reference and query data from various \nsequencing platforms or studies. This capability allows them to address batch effects in single-cell \nanalysis. Drug response or sensitivity prediction is a classification task akin to cell type annotation, \nwhere a classifier is appended to the learned cell embeddings to predict whether a cell will respond \nto or exhibit sensitivity to a specific drug. Models like sc Foundation [25] and CellLM  [157] \neffectively utilize this approach, leveraging the robust cell representations learned during \npretraining to enhance prediction accuracy. \nGene-level tasks. Gene-level tasks, such as gene expression prediction, gene regulatory network \n(GRN) inference, gene perturbation prediction, and drug dose-response prediction, are integral to \nunderstanding single -cell transcriptomics.  Self-attention mechanisms have transformed deep \nlearning by enabling context-aware models that prioritize relevant elements in large input spaces. \nThese models, particularly transformers, are well -suited for modeling the context -dependent \ndynamics of gene regulatory networks. By focusing on key interactions, transformers can \neffectively capture the complexities of regulatory relationships, such as  the attention matrix in \nGeneformer [18] and scGPT [21] reflect which genes that gene pays attention to and which genes \npay attention to that gene, aiding to infer gene regulation network. Geneformer is pretrained on a \nvast repository of single -cell transcriptomes to learn gene relationships for diverse downstream \napplications, including predicting dosage-sensitive disease genes, identifying downstream targets, \nforecasting chromatin dynamics, and modeling network dynamics. In addition, after pretraining \nand fine -tuning, single -cell language m odels output gene e mbeddings that can be utilized for \nfunctional analysis of scRNA-seq data. For instance, scGPT [21] serves as a generalizable feature \nextractor leveraging zero -shot learning, enabling applications in gene expression prediction and \ngenetic perturbation prediction. Similarly, in scFoundation [25], zero-expressed genes and masked \ngenes are combined with the output from the transformer -based encoder. This combined \ninformation is then input into the decoder and projected to gene expression values through a \nmultilayer perceptron (MLP). The gene context expression is employed to formulate a cell-specific \ngene graph, facilitating the prediction of perturbations using the GEARS [158] model. It is worth \nnoting that genes have a lot of prior knowledge that can be used to enhance many gene-level tasks. \nFor example, GeneCompass [51] incorporates four types of biological prior knowledge, including \nGRN, promoter information, gene family annotation and gene -co-expressed relationship, making \nit capable for various gene tasks.  \nscMulti-omics tasks. Studying single -cell multi -omics data requires integrating diverse \ninformation from genomics, transcriptomics, epigenomics, and proteomics at the single-cell level. \nThe adaptability, generalization capabilities, and feature extraction strengths of large language \nmodels make them effective in addressing challenges such as feature variance, data sparsity, and \ncell heterogeneity inherent in single -cell multi-omics datasets. scMulti-omics integration can be \nviewed as a specialized form of batch effect removal. For example, scGPT  [21] treats each \nmodality as a distinct batch and incorporates a special modality token to represent input features \n(such as genes, regions, or proteins) associated with each modality. This approach helps the \ntransformer model balance attention across modalities, preventing overemphasis on intra-modality \nfeatures while integrating inter -modality relationships effectively. Another approach involves \nprocessing different modalities through separate transformers before projecting their embeddings \ninto a common late nt space. Models like scMVP  [159] use mask attention -based encoders for \nscRNA-seq data and transformer -based multi-head self-attention encoders for scATAC -seq. By \naligning variations between different omics in this latent space, scMVP captures joint profiling of \nscRNA-seq and scATAC-seq, achieving paired integration where gene expression and chromatin \naccessibility are studied within the same cells.  Graphs are increasingly recognized as powerful \ntools for characterizing feature heterogeneity in scMulti -omics integration. For example, \nDeepMAPS [160] leverages graph transformers to construct cell and gene graphs, learning both \nlocal and global features that build cell -cell and gene -gene relationships for data integration, \ninference of biological networks from scMulti-omics data and cell-cell communication.  \nRecent advances in sequencing technologies that capture multiple modalities within the same cell \nhave enabled the development of computational tools for cross-modality prediction. One approach \ninvolves training large language models on paired datasets to p redict one modality from another. \nFor instance, scTranslator [161], pre-trained on paired bulk and single-cell data, fine-tunes to infer \nprotein abundance from scRNA -seq data by minimizing the mean squared error (MSE) between \npredicted and actual protein levels. Another strategy leverages graph learning with prior \nknowledge to model feature relationships. For example, scMoFormer [162] can not only translate \ngene expression to protein abundance, but is also applicable to multi-omics predictions, including \nprotein abundance to gene expression, chromatin accessibility to gene expression, gene expression \nto chromatin accessibility using graph transformers. Taking protein prediction task as an example, \nscMoFormer constructs cell-gene graph, gene-gene graph, protein-protein graph, and gene-protein \ngraph based on gene expression profiles and prior knowledge from STRING database [163]. Each \nmodality has a separate transformer to learn the global information that may not be included in \nprior knowledge. Message -passing graph neural networks (GNNs) link nodes across various \ngraphs, while transformers are employed to precisely map gene expression to protein abundance. \nSpatial transcriptomics tasks. The rapid development of single-cell and spatial transcriptomics \nhas advanced our understanding of cellular heterogeneity and tissue architecture. Spatial \ntranscriptomics retains cells ‚Äô native spatial context, enabling insights into cellular interactions. \nLarge language models address the challenge of high -dimensional spatial data analysis by \nintegrating spatial and molecular information, enhancing tissue-specific pattern interpretation. For \nexample, Nicheformer [53] is the latest large language model in spatial transcriptomics. It \nintegrates extensive spatial transcriptomics and single -cell transcriptomics data, leveraging \nmetadata across multiple modalities, species, and sequencing technologies. By doing so, \nNicheformer is capable of learning joint information from single -cell and spatial transcriptomics, \nenabling the resolution of various spatial prediction tasks even with limited data. Spaformer [164] \nis another transformer-based model designed for spatial transcriptomics data. Spaformer is \ndesigned to address two key challenges: how to encode spatial information of cells into a \ntransformer model and how to train a transformer to overcome the sparsity of spatial \ntranscriptomics data, enabling data imputation. Spatial transcriptomics, as one of the most popular \ntechnologies in recent years, focuses on integrating single -cell resolution gene expression data \nwith tissue spatial information to reveal spatial relationships and functional characteristics among \ncells. However, large language models (LLMs) specifically designed for spatial transcriptomics \nare still in their early stages of development. The creation of these models faces unique challenges, \nsuch a s effectively integrating high -dimensional gene expression data with complex spatial \ninformation and addressing the sparsity and irregularity of the data. \nIn addition to the single -cell large language models discussed  above, another category of single -\ncell prediction models leverages natural language , utilizing textual data such as human -readable \ndescriptions of gene functions and biological features to support various single -cell analyses. For \nexample, GPT-4 [165] leverages its strong contextual understanding for interpreting high -\ndimensional single-cell analysis for accurate cell type annotation. GenePT [166] utilizes OpenAI‚Äôs \nChatGPT text embedding to classify gene properties and cell types effectively . More and more  \nmodels demonstrate that natural language pretraining can significantly boost performance on \nsingle-cell downstream tasks, including cell generation [167], cell identity (e.g., cell type, pathway, \nand disease information)  [167-171], and gene enrichment analysis   [169]. These models \ndemonstrate significant potential in advancing single-cell analysis by integrating natural language \nprocessing techniques. However, the reliance on textual data  may constrain performance in less -\nannotated or novel datasets. \n5. Conclusion and Suggestions on large language models in bioinformatics \n5.1 Summary of large language models in bioinformatics \nLarge language models (LLMs) have catalyzed transformative progress across biological \ndisciplines, including genomics, transcriptomics, proteomics, drug discovery, and single -cell \nanalysis. These models, trained on vast datasets, address challenges like th e sparsity, high \ndimensionality, and heterogeneity of biological data while capturing the complexity of sequence \nrelationships. Tokenization methods are pivotal, converting sequences into manageable formats , \nsuch as, for genomics and transcriptomics, k -mer encoding is prevalent, segmenting DNA/RNA \nsequences into overlapping units. In proteomics, amino acid residue -based tokenization captures \nprotein structure and function. These preprocessing strategies enable LLMs to interpret biological \nlanguage effectively. \nRepresentation learning allows LLMs to uncover contextual and hierarchical relationships within \nbiological data, forming the basis for various downstream applications. These tasks can be grouped \ninto four primary categories: 1)  Classification/Prediction Tasks: Examples include identifying \nfunctional genomic elements (e.g., promoters, enhancers), predicting protein structures and \ninteractions, and cell type  annotation in single-cell data. 2) Generation Tasks: LLMs can create \nbiologically relevant sequences, such as gene expression imputation and synthetic DNA, RNA, or \nprotein sequences, aiding in vaccine development or enzyme engineering. 3) Interaction Tasks: \nThese involve modeling interactions like drug-target binding, cell-cell interaction, protein-protein \ninteractions, or cross-omics relationships (e.g., gene expression to protein abundance). 4) Transfer \nLearning Tasks: Pretrained LLMs, such as DNABERT and scGPT, are fine -tuned for specific \napplications, including single -cell data annotation or predicting RNA modifications like N6 -\nmethyladenosine sites. Despite their capabilities, challenges persist. Biological data often exhibit \nsparsity, as seen in single-cell and spatial transcriptomics, and irregularity due to sequencing errors \nor noise. To address this , LLMs must effectively integrate multi -modal data, balance \ncomputational efficiency, and ensure interpretability of their outputs. As foundational models \nevolve, their ability to unify diverse biological datasets into a single framework for prediction, \ngeneration, interaction, and transfer learning tasks will continue to reshape our understanding and \napplications of biological systems. \n5.2 Guidance on how to use and develop LLMs in practice \nLarge Language Models offer immense potential in bioinformatics and other fields, but their \neffective utilization and development require distinct approaches for end -users and developers  \n(Figure 5). \nFor LLM users, the process begins by clearly defining the research domain and task, specifying \nthe relevant omics level (e.g., genomics, transcriptomics, proteomics) and identifying whether the \nobjective involves classification or prediction, generation, interaction, or transfer learning. A well-\ndefined objective streamlines the selection of appropriate models and workflows. Next, users \nshould choose models pretrained on data relevant to their domain, as de tailed in Table 2, which \nincludes information on foundation models, their training data types, and availability. For instance, \nDNABERT is ideal for genomics tasks, while scGPT is tailored for single -cell analysis. \nAdditionally, users must assess computational requirements and ensure compatibility with thei r \ndataset size and complexity. Proper data preparation is critical, including aligning data with model \nrequirements, addressing missing values, and incorporating metadata like cell types or genomic \nregions. Table 1  provides common tokenization methods for reference. To leverage transfer \nlearning, users can fine -tune foundation models listed in Table 2  for their specific dataset, \noptimizing performance through hyperparameter tuning, early stopping, and cross -validation. \nAlternatively, users can utilize predeveloped models listed in Supplementary Tables 1 ‚Äì4 for \nsimilar tasks to obtain results directly. Finally, rigorous evaluation using metrics l ike accuracy, \nprecision, and recall is essential, complemented by interpretation tools such as attention maps or \nfeature embeddings to extract meaningful biological insights (Figure 5). \nFor LLM developers, it is essential to first understand domain-specific challenges to address issues \nlike sparsity, heterogeneity, and high dimensionality. For example, single -cell and spatial \ntranscriptomics datasets often suffer from sparsity and noise, necessitating innovative solutions in \nmodel architecture. Second, developers should choose or develop tokenization strategies tailored \nto biological data. For instance, k-mer encoding works well for DNA/RNA sequences, while gene \nranking-based tokenization is effective for scRNA -seq data. Exploring hybrid tokenization can \nenhance cross-modal understanding. Third, in model development, developers should employ or \ndesign novel transformer structures. For example, scBERT utilizes Performer to improve \nscalability. Incorporating knowledge -based information into model training can further enhance \nperformance. For instance, GeneCompass integrates four types of biological  prior knowledge  \nincluding GRNs, promoter information, gene family annotation, and gene co -expression \nrelationships, making it versatile for various gene-related tasks. Similarly, basic protein language \nmodels, which are often limited to MSA and protein sequences, can be improved by incorporating \nadditional modalities like 3D structural data. This can be achieved by converting such modalities \ninto sequence formats or integrating large models to collectively capture multi-modal information \nusing fusion techniques. Moreover, combining Graph Neural Networks (GNNs) with transformers \nhas led to significant advancements. For example, scMoFormer constructs cell -gene, gene-gene, \nprotein-protein, and gene-protein graphs for multi-omics predictions, while DeepMAPS uses cell-\ngene graphs to estimate gene importance. GNNs excel in capturing local interactions, while \ntransformers effectively model long-range dependencies, enabling comprehensive representations \nof intricate relationships in single-cell data. Fourth, novel tasks that can be explored in developing \nLLMs for bioinformatics include causal inference in multi-omics, such as determining how DNA \nvariations influence mRNA abundance or protein expression. Spatial transcriptomics \ninterpretation can model cell spatial organization within tissues. Epigenetic modulation prediction \nfocuses on regulatory roles of histone modifications, DNA methylation, or chromatin accessibility. \nSynthetic biology applications can involve generating optimized gene or protein sequences, while \ncross-species genomics identifies conserved functional genomic elements. These tasks exemplify \nhow LLMs can tackle emerging challenges in biological research. Fifth, developers should expand \nLLMs to accommodate emerging data types, such as CODEX imaging data and long -read \nsequencing data, which bring unique challenges in terms of data structure, preprocessing, and \nrepresentation. Lastly, validation, application, and interpretability should be prioritized. \nDevelopers should not only evaluate models on specific tasks but also ensure that foundational \nchallenges, such as the impact of sparsity in scRNA-seq data on cell type annotation performance, \nare fully addressed to enhance the robustness and utility of the models (Figure 5). \n \nAcknowledgements \nWe would like to express our gratitude to our colleagues and friends who provided invaluable \nadvice and support throughout the duration of this study. \n \nFunding \nThis work was partially supported by the National Institutes of Health [ R01LM014156, \nR01GM153822, R01CA241930 to X.Z] and the National Science Foundation [2217515, 2326879 to \nX.Z]. The funders had no role in study design, data collection and analysis, decision to publish or \npreparation of the manuscript. Funding for open access charge: Dr & Mrs Carl V. Vartian Chair \nProfessorship Funds to Dr. Zhou from the University of Texas Health Science Center at Houston. \nConflict of interest statement. None declared. \n \nReferences \n1. Radford, A., et al., Improving language understanding by generative pre-training. 2018. \n2. Devlin, J., et al., Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv:1810.04805, 2018. \n3. Vaswani, A., et al., Attention is all you need. Advances in neural information processing systems, \n2017. 30. \n4. Chen, J., et al., Interpretable RNA foundation model from unannotated data for highly accurate \nRNA structure and function predictions. bioRxiv, 2022: p. 2022.08. 06.503062. \n5. Zhang, Y., et al., Multiple sequence alignment-based RNA language model and its application to \nstructural inference. Nucleic Acids Research, 2024. 52(1): p. e3-e3. \n6. Ji, Y., et al., DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model \nfor DNA-language in genome. Bioinformatics, 2021. 37(15): p. 2112-2120. \n7. Zhang, D., et al., DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. \nbioRxiv, 2023: p. 2023.07. 11.548628. \n8. Akiyama, M. and Y. Sakakibara, Informative RNA base embedding for RNA structural alignment \nand clustering by deep representation learning. NAR genomics and bioinformatics, 2022. 4(1): p. \nlqac012. \n9. Wang, N., et al., Multi-purpose RNA language modelling with motif -aware pretraining and type-\nguided fine-tuning. Nature Machine Intelligence, 2024: p. 1-10. \n10. Rives, A., et al., Biological structure and function emerge from scaling unsupervised learning to \n250 million protein sequences. bioRxiv. 2019. \n11. Elnaggar, A., et al., ProtTrans: Towards Cracking the Language of Lifes Code Through Self -\nSupervised Deep Learning and High Performance Computing.  IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, 2021: p. 1-1. \n12. Madani, A., et al., Progen: Language modeling for protein generation.  arXiv preprint \narXiv:2004.03497, 2020. \n13. Xu, M., et al., Protst: Multi-modality learning of protein sequences and biomedical texts.  arXiv \npreprint arXiv:2301.12040, 2023. \n14. Outeiral, C. and C.M. Deane, Codon language embeddings provide strong signals for use in protein \nengineering. Nature Machine Intelligence, 2024. 6(2): p. 170-179. \n15. Wu, Z., et al., Knowledge-based BERT: a method to extract molecular features like computational \nchemists. Briefings in Bioinformatics, 2022. 23(3): p. bbac131. \n16. Bagal, V., et al., MolGPT: molecular generation using a transformer -decoder model. Journal of \nChemical Information and Modeling, 2021. 62(9): p. 2064-2076. \n17. Wang, S., et al. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. \nin Proceedings of the 10th ACM international conference on bioinformatics, computational biology \nand health informatics. 2019. \n18. Theodoris, C.V., et al., Transfer learning enables predictions in network biology.  Nature, 2023. \n618(7965): p. 616-624. \n19. Shen, H., et al., Generative pretraining from large-scale transcriptomes for single-cell deciphering. \niScience, 2023. 26(5): p. 106536. \n20. Yang, F., et al., scBERT as a large-scale pretrained deep language model for cell type annotation \nof single-cell RNA-seq data. Nature Machine Intelligence, 2022. 4(10): p. 852-866. \n21. Cui, H., et al., scGPT: toward building a foundation model for single -cell multi -omics using \ngenerative AI. Nat Methods, 2024. 21(8): p. 1470-1480. \n22. Chen, J., et al., Transformer for one stop interpretable cell type annotation.  Nat Commun, 2023. \n14(1): p. 223. \n23. Jiao, L., et al., scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene \nEmbeddings. Biomolecules, 2023. 13(4). \n24. Xiong, L., T. Chen, and M. Kellis. scCLIP: Multi-modal Single-cell Contrastive Learning Integration \nPre-training. in NeurIPS 2023 AI for Science Workshop. \n25. Hao, M., et al., Large-scale foundation model on single-cell transcriptomics. Nat Methods, 2024. \n21(8): p. 1481-1491. \n26. Bian, H., et al. scMulan: a multitask generative pre-trained language model for single-cell analysis. \nin International Conference on Research in Computational Molecular Biology. 2024. Springer. \n27. Wen, H., et al., CellPLM: pre-training of cell language model beyond single cells. bioRxiv, 2023: p. \n2023.10. 03.560734. \n28. Mao, Y., et al., Phenotype prediction from single-cell RNA-seq data using attention-based neural \nnetworks. Bioinformatics, 2024. 40(2). \n29. Querfurth, B.v., et al., mcBERT: Patient -Level Single -cell Transcriptomics Data Representation.  \nbioRxiv, 2024: p. 2024.11. 04.621897. \n30. Sarkar, S., Decoding\" coding\": Information and DNA. BioScience, 1996. 46(11): p. 857-864. \n31. Dalla-Torre, H., et al., The nucleotide transformer: Building and evaluating robust foundation \nmodels for human genomics. bioRxiv, 2023: p. 2023.01. 11.523679. \n32. Benegas, G., S.S. Batra, and Y.S. Song, DNA language models are powerful predictors of genome-\nwide variant effects.  Proceedings of the National Academy of Sciences, 2023. 120(44): p. \ne2311219120. \n33. Zhou, Z., et al., Dnabert-2: Efficient foundation model and benchmark for multi -species genome. \narXiv preprint arXiv:2306.15006, 2023. \n34. Sanabria, M., et al., DNA language model GROVER learns sequence context in the human genome. \nNature Machine Intelligence, 2024. 6(8): p. 911-923. \n35. Wang, X., et al., UNI-RNA: universal pre-trained models revolutionize RNA research. bioRxiv, 2023: \np. 2023.07. 11.548588. \n36. Chen, K., et al., Self-supervised learning on millions of primary RNA sequences from 72 vertebrates \nimproves sequence -based RNA splicing prediction.  Briefings in Bioinformatics, 2024. 25(3): p. \nbbae163. \n37. Yang, Y., et al., Deciphering 3'UTR Mediated Gene Regulation Using Interpretable Deep \nRepresentation Learning. Advanced Science, 2024. 11(39): p. 2407013. \n38. Chu, Y., et al., A 5‚Ä≤ UTR language model for decoding untranslated regions of mRNA and function \npredictions. Nature Machine Intelligence, 2024. 6(4): p. 449-460. \n39. Rao, R., et al., Evaluating protein transfer learning with TAPE.  Advances in neural information \nprocessing systems, 2019. 32. \n40. Rives, A., et al., Biological structure and function emerge from scaling unsupervised learning to \n250 million protein sequences. Proceedings of the National Academy of Sciences, 2021. 118(15): \np. e2016239118. \n41. Ferruz, N., S. Schmidt, and B. Hcker, ProtGPT2 is a deep unsupervised language model for protein \ndesign. Nature communications, 2022. 13(1): p. 4348. \n42. Brandes, N., et al., ProteinBERT: a universal deep-learning model of protein sequence and function. \nBioinformatics, 2022. 38(8): p. 2102-2110. \n43. Zhou, H.-Y., et al., Protein Representation Learning via Knowledge Enhanced Primary Structure \nModeling. bioRxiv, 2023: p. 2023-01. \n44. Polishchuk, P.G., T.I. Madzhidov, and A.J.J.o.c. -a.m.d. Varnek, Estimation of the size of drug -like \nchemical space based on GDB-17 data. 2013. 27: p. 675-679. \n45. MOLE-BERT: RETHINKING PRE-TRAINING GRAPH NEURAL NETWORKS FOR MOLECULES. \n46. Wang, S., et al., Smiles-Bert, in Proceedings of the 10th ACM International Conference on \nBioinformatics, Computational Biology and Health Informatics. 2019. p. 429-436. \n47. Bagal, V., et al., MolGPT: Molecular Generation Using a Transformer -Decoder Model. J Chem Inf \nModel, 2022. 62(9): p. 2064-2076. \n48. SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. \n49. Rosen, Y., et al., Universal cell embeddings: A foundation model for cell biology. bioRxiv, 2023: p. \n2023.11. 28.568918. \n50. Theus, A., et al., CancerFoundation: A single-cell RNA sequencing foundation model to decipher \ndrug resistance in cancer. bioRxiv, 2024: p. 2024.11. 01.621087. \n51. Yang, X., et al., GeneCompass: deciphering universal gene regulatory mechanisms with a \nknowledge-informed cross-species foundation model. Cell Res, 2024. 34(12): p. 830-845. \n52. Kalfon, J., et al., scPRINT: pre-training on 50 million cells allows robust gene network predictions. \nbioRxiv, 2024: p. 2024.07. 29.605556. \n53. Schaar, A., et al., Nicheformer: a foundation model for single‚Äêcell and spatial omics. 2024. Preprint \nat bioRxiv, 2024. 4: p. 589472. \n54. Sinden, R.R. and R.D. Wells, DNA structure, mutations, and human genetic disease.  Current \nopinion in biotechnology, 1992. 3(6): p. 612-622. \n55. Wittkopp, P.J. and G. Kalay, Cis-regulatory elements: molecular mechanisms and evolutionary \nprocesses underlying divergence. Nature Reviews Genetics, 2012. 13(1): p. 59-69. \n56. Yella, V.R., A. Kumar, and M. Bansal, Identification of putative promoters in 48 eukaryotic genomes \non the basis of DNA free energy. Scientific reports, 2018. 8(1): p. 4520. \n57. Le, N.Q.K., et al., BERT-Promoter: An improved sequence-based predictor of DNA promoter using \nBERT pre-trained model and SHAP feature selection. Computational Biology and Chemistry, 2022. \n99: p. 107732. \n58. Claringbould, A. and J.B. Zaugg, Enhancers in disease: molecular basis and emerging treatment \nstrategies. Trends in Molecular Medicine, 2021. 27(11): p. 1060-1073. \n59. Nasser, J., et al., Genome-wide enhancer maps link risk variants to disease genes.  Nature, 2021. \n593(7858): p. 238-243. \n60. Luo, H., et al. iEnhancer-BERT: A novel transfer learning architecture based on DNA -Language \nmodel for identifying enhancers and their strength . in International Conference on Intelligent \nComputing. 2022. Springer. \n61. Ferraz, R.A.C., et al., DNA‚Äìprotein interaction studies: a historical and comparative analysis. Plant \nMethods, 2021. 17(1): p. 1-21. \n62. Luo, H., et al., Improving language model of human genome for DNA ‚Äìprotein binding prediction \nbased on task-specific pre-training. Interdisciplinary Sciences: Computational Life Sciences, 2023. \n15(1): p. 32-43. \n63. An, W., et al. MoDNA: motif-oriented pre-training for DNA language model. in Proceedings of the \n13th ACM International Conference on Bioinformatics, Computational Biology and Health \nInformatics. 2022. \n64. Moore, L.D., T. Le, and G. Fan, DNA methylation and its basic function. Neuropsychopharmacology, \n2013. 38(1): p. 23-38. \n65. Zhang, L., et al., Comprehensive analysis of DNA 5-methylcytosine and N6-adenine methylation by \nnanopore sequencing in hepatocellular carcinoma.  Frontiers in cell and developmental biology, \n2022. 10: p. 827391. \n66. Tsukiyama, S., et al., BERT6mA: prediction of DNA N6 -methyladenine site using deep learning -\nbased approaches. Briefings in Bioinformatics, 2022. 23(2): p. bbac053. \n67. Yu, Y., et al., iDNA-ABT: advanced deep learning model for detecting DNA methylation with \nadaptive features and transductive information maximization.  Bioinformatics, 2021. 37(24): p. \n4603-4610. \n68. Jin, J., et al., iDNA-ABF: multi-scale deep biological language learning model for the interpretable \nprediction of DNA methylations. Genome biology, 2022. 23(1): p. 1-23. \n69. Zeng, W., A. Gautam, and D.H. Huson, MuLan-Methyl-Multiple Transformer -based Language \nModels for Accurate DNA Methylation Prediction. bioRxiv, 2023: p. 2023.01. 04.522704. \n70. Sanh, V., et al., DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.  arXiv \npreprint arXiv:1910.01108, 2019. \n71. Lan, Z., et al., Albert: A lite bert for self -supervised learning of language representations.  arXiv \npreprint arXiv:1909.11942, 2019. \n72. Yang, Z., et al., Xlnet: Generalized autoregressive pretraining for language understanding.  \nAdvances in neural information processing systems, 2019. 32. \n73. Clark, K., et al., Electra: Pre-training text encoders as discriminators rather than generators. arXiv \npreprint arXiv:2003.10555, 2020. \n74. Wilkinson, M.E., C. Charenton, and K. Nagai, RNA splicing by the spliceosome.  Annual review of \nbiochemistry, 2020. 89: p. 359-388. \n75. Zhang, J., et al., Advances and opportunities in RNA structure experimental determination and \ncomputational modeling. Nature Methods, 2022. 19(10): p. 1193-1207. \n76. Malbec, L., et al., Dynamic methylome of internal mRNA N 7-methylguanosine and its regulatory \nrole in translation. Cell research, 2019. 29(11): p. 927-941. \n77. Feng, H., et al., LncCat: An ORF attention model to identify LncRNA based on ensemble learning \nstrategy and fused sequence information.  Computational and Structural Biotechnology Journal, \n2023. 21: p. 1433-1447. \n78. Xia, S., et al. A multi -granularity information-enhanced pre -training method for predicting the \ncoding potential of sORFs in plant lncRNAs . in 2023 IEEE International Conference on \nBioinformatics and Biomedicine (BIBM). 2023. IEEE. \n79. Yamada, K. and M. Hamada, Prediction of RNA‚Äìprotein interactions using a nucleotide language \nmodel. Bioinformatics Advances, 2022. 2(1): p. vbac023. \n80. Fang, Y., X. Pan, and H. -B. Shen, Recent deep learning methodology development for RNA ‚ÄìRNA \ninteraction prediction. Symmetry, 2022. 14(7): p. 1302. \n81. Gibb, E.A., C.J. Brown, and W.L. Lam, The functional role of long non -coding RNA in human \ncarcinomas. Molecular cancer, 2011. 10(1): p. 1-17. \n82. Zhang, L., et al., BERT-m7G: a transformer architecture based on BERT and stacking ensemble to \nidentify RNA N7 -Methylguanosine sites from sequence information.  Computational and \nMathematical Methods in Medicine, 2021. 2021. \n83. Soylu, N.N. and E. Sefer, BERT2OME: Prediction of 2' -O-methylation Modifications from RNA \nSequence by Transformer Architecture Based on BERT. IEEE/ACM Transactions on Computational \nBiology and Bioinformatics, 2023. \n84. Pardi, N., et al., mRNA vaccines‚Äîa new era in vaccinology. Nature reviews Drug discovery, 2018. \n17(4): p. 261-279. \n85. Babjac, A.N., Z. Lu, and S.J. Emrich. CodonBERT: Using BERT for Sentiment Analysis to Better \nPredict Genes with Low Expression . in Proceedings of the 14th ACM International Conference on \nBioinformatics, Computational Biology, and Health Informatics. 2023. \n86. Gong, H., et al., Integrated mRNA sequence optimization using deep learning.  Brief Bioinform, \n2023. 24(1). \n87. Ding, W., K. Nakai, and H. Gong, Protein design via deep learning. Briefings in bioinformatics, 2022. \n23(3): p. bbac102. \n88. Qiu, Y. and G. -W. Wei, Artificial intelligence -aided protein engineering: from topological data \nanalysis to deep protein language models. arXiv preprint arXiv:2307.14587, 2023. \n89. Kovaltsuk, A., et al., Observed antibody space: a resource for data mining next -generation \nsequencing of antibody repertoires. The Journal of Immunology, 2018. 201(8): p. 2502-2509. \n90. Schauperl, M. and R.A. Denny, AI-based protein structure prediction in drug discovery: impacts \nand challenges. Journal of Chemical Information and Modeling, 2022. 62(13): p. 3142-3156. \n91. David, A., et al., The AlphaFold database of protein structures: a biologist‚Äôs guide.  Journal of \nmolecular biology, 2022. 434(2): p. 167336. \n92. Rao, R.M., et al. MSA transformer. in International Conference on Machine Learning. 2021. \n93. Dai, Z., et al., Transformer-xl: Attentive language models beyond a fixed -length context.  arXiv \npreprint arXiv:1901.02860, 2019. \n94. Raffel, C., et al., Exploring the limits of transfer learning with a unified text -to-text transformer. \nThe Journal of Machine Learning Research, 2020. 21(1): p. 5485-5551. \n95. UniProt: the universal protein knowledgebase in 2021.  Nucleic acids research, 2021. 49(D1): p. \nD480-D489. \n96. Steinegger, M. and J. Sding, Clustering huge protein sequence sets in linear time.  Nature \ncommunications, 2018. 9(1): p. 2542. \n97. Strokach, A. and P.M. Kim, Deep generative modeling for protein design.  Current opinion in \nstructural biology, 2022. 72: p. 226-236. \n98. Ferruz, N. and B. Hcker, Controllable protein design with language models.  Nature Machine \nIntelligence, 2022. 4(6): p. 521-532. \n99. Mirdita, M., et al., ColabFold: making protein folding accessible to all.  Nature methods, 2022. \n19(6): p. 679-682. \n100. Jumper, J., et al., Highly accurate protein structure prediction with AlphaFold.  Nature, 2021. \n596(7873): p. 583-589. \n101. Zhou, X., et al., I-TASSER-MTD: a deep-learning-based platform for multi-domain protein structure \nand function prediction. Nature Protocols, 2022. 17(10): p. 2326-2353. \n102. Ferruz, N., et al., From sequence to function through structure: Deep learning for protein design.  \nComputational and Structural Biotechnology Journal, 2023. 21: p. 238-250. \n103. Xu, M., et al. Protst: Multi -modality learning of protein sequences and biomedical texts . in \nInternational Conference on Machine Learning. 2023. PMLR. \n104. Rosenberg, A.A., A. Marx, and A.M. Bronstein, Codon-specific Ramachandran plots show amino \nacid backbone conformation depends on identity of the translated codon. Nature communications, \n2022. 13(1): p. 2815. \n105. Wang, H., et al., Protein post-translational modifications in the regulation of cancer hallmarks.  \nCancer Gene Therapy, 2023. 30(4): p. 529-547. \n106. de Brevern, A.G. and J. Rebehmed, Current status of PTMs structural databases: applications, \nlimitations and prospects. Amino Acids, 2022. 54(4): p. 575-590. \n107. Savino, S., T. Desmet, and J. Franceus, Insertions and deletions in protein evolution and \nengineering. Biotechnology Advances, 2022. 60: p. 108010. \n108. Horne, J. and D. Shukla, Recent advances in machine learning variant effect prediction tools for \nprotein engineering. Industrial \\& engineering chemistry research, 2022. 61(19): p. 6235-6245. \n109. Alley, E.C., et al., Unified rational protein engineering with sequence -based deep representation \nlearning. Nature methods, 2019. 16(12): p. 1315-1322. \n110. Liu, W., et al., PLMSearch: Protein language model powers accurate and fast sequence search for \nremote homology. Nature communications, 2024. 15(1): p. 2775. \n111. Hong, L., et al., Fast, sensitive detection of protein homologs using deep dense retrieval.  Nature \nBiotechnology, 2024: p. 1-13. \n112. Pucci, F., M. Schwersensky, and M. Rooman, Artificial intelligence challenges for predicting the \nimpact of mutations on protein stability.  Current opinion in structural biology, 2022. 72: p. 161-\n168. \n113. Wang, Z., et al. Multi-level Protein Structure Pre -training via Prompt Learning . in The Eleventh \nInternational Conference on Learning Representations. 2022. \n114. Tang, T., et al., Machine learning on protein --protein interaction prediction: models, challenges \nand trends. Briefings in Bioinformatics, 2023. 24(2): p. bbad076. \n115. Durham, J., et al., Recent advances in predicting and modeling protein --protein interactions.  \nTrends in biochemical sciences, 2023. \n116. Zhang, N., et al., Ontoprotein: Protein pretraining with gene ontology embedding.  arXiv preprint \narXiv:2201.11147, 2022. \n117. Janeway, C., et al., Immunobiology: the immune system in health and disease . Vol. 2. 2001: \nGarland Pub. New York. \n118. Peters, B., M. Nielsen, and A.J.A.R.o.I. Sette, T cell epitope predictions. 2020. 38: p. 123-145. \n119. O‚ÄôDonnell, T.J., A. Rubinsteyn, and U.J.C.s. Laserson, MHCflurry 2.0: improved pan-allele prediction \nof MHC class I-presented peptides by incorporating antigen processing. 2020. 11(1): p. 42-48. e7. \n120. Wang, F., et al., MHCRoBERTa: pan -specific peptide -MHC class I binding prediction through \ntransfer learning with label-agnostic protein sequences. Brief Bioinform, 2022. 23(3). \n121. Cheng, J., et al., BERTMHC: improved MHC‚Äìpeptide class II interaction prediction with transformer \nand multiple instance learning. Bioinformatics, 2021. 37(22): p. 4172-4179. \n122. Wu, K., et al., TCR-BERT: learning the grammar of T -cell receptors for flexible antigenbinding \nanalyses. 2021. \n123. Zhao, Y., et al., SC-AIR-BERT: a pre -trained single-cell model for predicting the antigen -binding \nspecificity of the adaptive immune receptor. Brief Bioinform, 2023. 24(4). \n124. Wang, Q., et al., AntiFormer: graph enhanced large language model for binding affinity prediction. \nBriefings in Bioinformatics, 2024. 25(5). \n125. Olsen, T.H., I.H. Moal, and C.M. Deane, AbLang: an antibody language model for completing \nantibody sequences. Bioinformatics Advances, 2022. 2(1): p. vbac046. \n126. Liu, Y., et al., Roberta: A robustly optimized bert pretraining approach.  arXiv preprint \narXiv:1907.11692, 2019. \n127. Leem, J., et al., Deciphering the language of antibodies using self -supervised learning. Patterns, \n2022. 3(7). \n128. Wang, D., F. Ye, and H. Zhou, On pre-trained language models for antibody. bioRxiv, 2023: p. 2023-\n01. \n129. Askr, H., et al., Deep learning in drug discovery: an integrative review and future challenges.  \nArtificial Intelligence Review, 2023. 56(7): p. 5975-6037. \n130. Xiaobo, Z. and S.T.C. Wong, High content cellular imaging for drug development.  IEEE Signal \nProcessing Magazine, 2006. 23(2): p. 170-174. \n131. Sun, X., et al., Multi-scale agent-based brain cancer modeling and prediction of TKI treatment \nresponse: incorporating EGFR signaling pathway and angiogenesis. BMC Bioinformatics, 2012. 13: \np. 218. \n132. Vargason, A.M., A.C. Anselmo, and S.J.N.b.e. Mitragotri, The evolution of commercial drug delivery \ntechnologies. 2021. 5(9): p. 951-967. \n133. Leeson, P.D. and B.J.N.r.D.d. Springthorpe, The influence of drug-like concepts on decision-making \nin medicinal chemistry. 2007. 6(11): p. 881-890. \n134. Ozcelik, R., et al., Structure-Based Drug Discovery with Deep Learning.  Chembiochem, 2023. \n24(13): p. e202200776. \n135. Li, Z., et al., Deep learning methods for molecular representation and property prediction.  Drug \nDiscovery Today, 2022: p. 103373. \n136. Chen, W., et al., Artificial intelligence for drug discovery: Resources, methods, and applications.  \nMolecular Therapy-Nucleic Acids, 2023. \n137. Chithrananda, S., G. Grand, and B. Ramsundar, ChemBERTa: large -scale self -supervised \npretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020. \n138. ChemBERTa-2: Towards Chemical Foundation Models. \n139. Xia, J., et al. Mole-bert: Rethinking pre -training graph neural networks for molecules . in The \nEleventh International Conference on Learning Representations. 2022. \n140. Bilodeau, C., et al., Generative models for molecular discovery: Recent advances and challenges.  \nWiley Interdisciplinary Reviews: Computational Molecular Science, 2022. 12(5): p. e1608. \n141. Meyers, J., B. Fabian, and N. Brown, De novo molecular design and generative models.  Drug \nDiscovery Today, 2021. 26(11): p. 2707-2715. \n142. Abbasi, K., et al., Deep learning in drug target interaction prediction: current and future \nperspectives. Current Medicinal Chemistry, 2021. 28(11): p. 2100-2113. \n143. Zhang, Z., et al., Graph neural network approaches for drug-target interactions. Current Opinion \nin Structural Biology, 2022. 73: p. 102327. \n144. Zheng, J., X. Xiao, and W. -R. Qiu, DTI-BERT: identifying drug -target interactions in cellular \nnetworking based on BERT and deep learning method. Frontiers in Genetics, 2022. 13: p. 859188. \n145. Kalakoti, Y., S. Yadav, and D. Sundar, TransDTI: transformer-based language models for estimating \nDTIs and building a drug recommendation workflow. ACS omega, 2022. 7(3): p. 2706-2717. \n146. Kang, H., et al., Fine-tuning of bert model to accurately predict drug --target interactions.  \nPharmaceutics, 2022. 14(8): p. 1710. \n147. Nguyen, T.M., T. Nguyen, and T. Tran, Mitigating cold -start problems in drug -target affinity \nprediction with interaction knowledge transferring.  Briefings in Bioinformatics, 2022. 23(4): p. \nbbac269. \n148. Ragoza, M., et al., Protein--ligand scoring with convolutional neural networks. Journal of chemical \ninformation and modeling, 2017. 57(4): p. 942-957. \n149. Li, S., et al. Structure-aware interactive graph neural networks for the prediction of protein-ligand \nbinding affinity. in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& \nData Mining. 2021. \n150. Jiang, D., et al., InteractionGraphNet: a novel and efficient deep graph representation learning \nframework for accurate protein --ligand interaction predictions.  Journal of medicinal chemistry, \n2021. 64(24): p. 18209-18232. \n151. Wang, Y., et al., A point cloud -based deep learning strategy for protein --ligand binding affinity \nprediction. Briefings in Bioinformatics, 2022. 23(1): p. bbab474. \n152. Hecht, J.R., et al., A randomized phase IIIB trial of chemotherapy, bevacizumab, and panitumumab \ncompared with chemotherapy and bevacizumab alone for metastatic colorectal cancer.  2009. \n27(5): p. 672-680. \n153. Tol, J., et al., Chemotherapy, bevacizumab, and cetuximab in metastatic colorectal cancer. 2009. \n360(6): p. 563-572. \n154. Zhang, W., et al., DCE-DForest: a deep forest model for the prediction of anticancer drug \ncombination effects. Computational and Mathematical Methods in Medicine, 2022. 2022. \n155. Xu, M., et al., DFFNDDS: prediction of synergistic drug combinations with dual feature fusion \nnetworks. Journal of Cheminformatics, 2023. 15(1): p. 1-12. \n156. Shen, H., et al., A universal approach for integrating super large -scale single-cell transcriptomes \nby exploring gene rankings. Brief Bioinform, 2022. 23(2). \n157. Zhao, S., J. Zhang, and Z. Nie, Large-scale cell representation learning via divide -and-conquer \ncontrastive learning. arXiv preprint arXiv:2306.04371, 2023. \n158. Roohani, Y., K. Huang, and J. Leskovec, GEARS: Predicting transcriptional outcomes of novel multi-\ngene perturbations. BioRxiv, 2022: p. 2022.07. 12.499735. \n159. Li, G., et al., A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq \ndata. Genome Biol, 2022. 23(1): p. 20. \n160. Ma, A., et al., Single-cell biological network inference using a heterogeneous graph transformer.  \nNat Commun, 2023. 14(1): p. 964. \n161. Linjing, L., et al., A pre-trained large language model for translating single -cell transcriptome to \nproteome. bioRxiv, 2023: p. 2023.07.04.547619. \n162. Tang, W., et al. Single-cell multimodal prediction via transformers. in Proceedings of the 32nd ACM \nInternational Conference on Information and Knowledge Management. 2023. \n163. Szklarczyk, D., et al., The STRING database in 2023: protein -protein association networks and \nfunctional enrichment analyses for any sequenced genome of interest.  Nucleic Acids Res, 2023. \n51(D1): p. D638-D646. \n164. Wen, H., et al., Single cells are spatial tokens: Transformers for spatial transcriptomic data \nimputation. arXiv preprint arXiv:2302.03038, 2023. \n165. Hou, W. and Z. Ji, Assessing GPT-4 for cell type annotation in single -cell RNA-seq analysis. Nat \nMethods, 2024. 21(8): p. 1462-1465. \n166. Chen, Y. and J. Zou, GenePT: a simple but effective foundation model for genes and cells built from \nChatGPT. bioRxiv, 2024: p. 2023.10. 16.562533. \n167. Levine, D., et al., Cell2Sentence: teaching large language models the language of biology. BioRxiv, \n2023: p. 2023.09. 11.557287. \n168. Zhao, S., et al., Langcell: Language-cell pre-training for cell identity understanding. arXiv preprint \narXiv:2405.06708, 2024. \n169. Lu, Y.-C., et al., scChat: A Large Language Model -Powered Co-Pilot for Contextualized Single-Cell \nRNA Sequencing Analysis. bioRxiv, 2024: p. 2024.10. 01.616063. \n170. Liu, T., et al., scelmo: Embeddings from language models are good learners for single -cell data \nanalysis. bioRxiv, 2023: p. 2023.12. 07.569910. \n171. Heimberg, G., et al., Scalable querying of human cell atlases via a foundational model reveals \ncommonalities across fibrosis-associated macrophages. BioRxiv, 2023: p. 2023.07. 18.549537. \n \nMain figures \n \n \nFigure 1. Summary of the application of large language models in bioinformatics in this \nreview. Applications of large language models in bioinformatics include applications in genomics, \ntranscriptomics, proteomics, drug discovery  and single-cell analysis. Applications of LLMs in \ngenomics focus on LLMs using DNA sequence; applications of LLMs in transcriptomics focus on \nusing RNA sequence; applications of LLMs in proteomics focus on LLMs using protein sequence; \napplications of LLMs in drug discovery focus on LLMs using molecular data and applications of \nLLMs in single -cell analysis focus on LLMs using scRNA-seq, scMulti-omics and spatial \ntranscriptomics data. Each corresponds to a variety of biological downstream tasks. \n  \n\n \nFigure 2. Building blocks of large language models in bioinformatics. a, tokenization methods \ntailored to various data types, including DNA/RNA sequences, proteins, small molecules, and \nsingle-cell data. b, input embedding strategies used in large language models to encode tokenized \ndata. c, schematic representation of the transformer architecture, a foundational structure in LLMs. \nd, the attention mechanism, enabling models to focus on important features in sequences. e, the \nfeed-forward network, a critical component of transformers for learning hierarchical \n\nrepresentations. f, pre-training processes for BERT and GPT-based models, highlighting BERT's \nbidirectional prediction approach and GPT's left-to-right prediction strategy. \n  \n \nFigure 3. Schematic diagram of the large language model pretraining and fine-tuning process. \nThe workflow begins with tokenizing the input data, which is then fed into the embedding layer \nand transformer models. The training process comprises two stages: pretraining and fine -tuning. \nPretraining employs self -supervised learning on large -scale, unla beled reference datasets to \ndevelop a general-purpose model with robust generalization capabilities. Fine-tuning builds upon \nthe pretrained model, involving task -specific training to optimize performance for designated \napplications. \n  \n\n \n\nFigure 4. Downstream task s of large language models in bioinformatics. Large language \nmodels (LLMs) have seen numerous successful applications in bioinformatics, addressing a wide \narray of tasks across DNA, RNA, protein, drug discovery, and single-cell analysis. \n  \n \nFigure 5. Guidance for LLM users and developers on how to use and develop LLM in \npractice. Guidance for LLM users includes steps such as clarifying the task, selecting an \nappropriate model, preparing the dataset, training the model, and evaluating its performance. For \nLLM developers, the focus involves identifying domain -specific challenges, designing \ntokenization strategies, advancing model architectures, exploring novel tasks and data types, and \nassessing model capabilities comprehensively. \n \n \n\nTables \nTable 1. Tokenization methods for different types of data \nApplication area Data type Method Example \nGenomics/Transcriptomics DNA/RNA sequence \nOne-hot encoding RNA-FMÔºåRNA-MSM \nFixed-length k-mers \nDNABERTÔºåNucleotide TransformerÔºåDNABERT-2Ôºå\nDNAGPTÔºåRNABERT \nSpecial ‚Äò[IND]‚Äô token RNAErnie \nProteomics \nMSAs/Protein \nsequences Single Amino Acid Tokenization MSA Transformer/TAPE, ESM-1b, ProtTrans, Progen \nBiomedical text WordPiece ProtST \ncDNA Single condo Tokenization CaLM \nDrug discovery \nSimplified \nMolecular-Input \nLine-Entry system \n(SMILES) \nRandom token K-BERT  \nSmilesTokenizer ChemBERTa, ChemBERTa-2, MolGPT \nGraph VQ-VAE Mole-BERT \nfingerprint SMILES-BERT \nSingle-cell analysis Expression profiles \nGene expression Ranking Geneformer, tGPT, iSEEEK \nBinning scBERT, scGPT, scFormer, CellLM, BioFormers, \nCancerFoundation \nGene set/Pathway tokens TOSICA \nPatches CIForm, scTranSort, scCLIP \n  Gene value projection scTranslator, scFounfation, scMulan, scGREAT \n  Cell tokens CellPLM, ScRAT, mcBERT \n \nTable 2. Foundation models in bioinformatics \nApplicatio\nn area Model Architecture Pre-training Data Code available \nGenomics \nGPN Transformer-based Reference genomes from 8 species https://github.com/songlab-cal/gpn \nNucleotide \nTransformer Transformer-based \n3.2 billion nucleotides in GRCh38/hg38 reference assembly, 20.5 \ntrillion nucleotides including 125 million mutations (111 million SNPs, \n14 million indels), and 174 billion nucleotides from 850 species \nhttps://github.com/instadeepai/nucleotide-\ntransformer \nDNABERT BERT-based 2.75 billion nucleotide based human genome dataset https://github.com/jerryji1993/DNABERT \nDNABERT-2 BERT-based 2.75 billion nucleotide based human genome dataset and 32.49 billion \nnucleotide bases from 135 species, spread across 6 categories \nhttps://github.com/MAGICS-\nLAB/DNABERT_2 \nMoDNA BERT-based Same as Nucleotide Transformer https://github.com/uta-smile/MoDNA \nGROVER BERT-based Homo sapiens (human) genome assembly GRCh37 (hg19) https://github.com/rowanz/grover \nMuLan-Methyl BERT-based 3 main types of DNA methylation sites (6mA, 4mC, and 5hmC) across \n12 genomes, in total 250,599 positive samples https://github.com/husonlab/mulan-methyl \niDNA-ABF BERT-based Same as MuLan-Methyl https://github.com/FakeEnd/iDNA_ABF \niDNA-ABT BERT-based Same as MuLan-Methyl https://github.com/YUYING07/iDNA_ABT \nDNAGPT GPT-based Reference genomes from the Ensembl database include 3 billion bps, \nwith a total of 1,594,129,992 bps across 9 species \nhttps://github.com/TencentAILabHealthcar\ne/DNAGPT \nTranscripto\nmics \nRNABERT BERT-based 76 237 human-derived small ncRNAs from RNAcentral https://github.com/mana438/RNABERT \nRNA-FM BERT-based About 27 million ncRNA sequences across 47 different databases https://github.com/ml4bio/RNA-FM \nRNA-MSM BERT-based 4069 RNA families from rfam https://github.com/yikunpku/RNA-MSM \nSpliceBERT BERT-based 2 million sequences and approximately covering 65 billion nucleotides \nof 72 vertebrates from UCSC genome browser https://github.com/biomed-AI/SpliceBERT \nUNI-RNA BERT-based 23‚Äâmillion ncRNA sequences obtained from the RNAcentral database https://github.com/ComDec/unirna-tools \n3UTRBERT BERT-based 108,573 unique mRNA transcripts from the GENCODE and each \ncontains 3,754 nucleotides (median 3048 nts) on average. https://github.com/yangyn533/3UTRBERT \nUTR-LM BERT-based 214,349 unlabeled 5‚Ä≤ UTR sequences from Ensembl across 5 species https://github.com/a96123155/UTR-LM \nRNAErnie Transformer- based 23‚Äâmillion ncRNA sequences obtained from the RNAcentral database https://github.com/CatIIIIIIII/RNAErnie \nProteomics \nTAPE Transformer-based 31 million protein sequences from Pfam https://github.com/songlab-cal/tape \nESM-1b Transformer-based 250 million protein sequences from UniRef50 https://github.com/facebookresearch/esm \nProtTrans \nTransformer-XL, \nXLNet, BERT, \nAlbert, Electra, T5 \nAbout 2.3 billion protein sequences from UniRef and BFD https://github.com/agemagician/ProtTrans \nProtGPT2 GPT-based 50 million protein sequences from UniRef50 https://huggingface.co/docs/transformers/\nmain_classes/trainer \nProteinBERT BERT-based 106 million protein sequences with GO annotations from UniRef50 https://github.com/nadavbra/protein_bert \nKeAP BERT-based 5 million Triplet in the format of (Protein, Relation, Attribute) with \nnearly 600k protein, 50k attribute terms, and 31 relation terms included https://github.com/RL4M/KeAP \nCaLM Transformer-based 9,858,385 cDNA sequences of seven model organisms https://github.com/oxpig/CaLM \nDrug \ndiscovery \nSMILES-BERT BERT-based Two datasets from NCATS (NIH) and 128 datasets from PubChem https://github.com/uta-smile/SMILES-\nBERT \nChemBERTa BERT-based 77 million unique SMILES https://github.com/seyonechithrananda/be\nrt-loves-chemistry \nK-BERT BERT-based Book review dataset contains 20,000 positive and \n20,000 negative reviews collected from Douban https://github.com/autoliuweijie/K-BERT \nMole-BERT BERT-based 2 million molecules https://github.com/junxia97/Mole-BERT \nMolGPT GPT-based Datasets from MOSES and GuacaMol https://github.com/devalab/molgpt \nProtBERT BERT-based Datasets from Uniref50, UniRef100 and BFD https://github.com/agemagician/ProtTrans\n/ \nDeepDDS BERT-based Datasets from NCI-ALMANAC https://github.com/sorachel/DFFNDDS \nSynerGPT GPT-based Datasets from DrugCombDB Code will be made available upon \npublication \nSingle-cell \nanalysis \nscBERT BERT-based 1,126,580 cells from 209 datasets across 74 tissues and 451,513 cells \nfrom four sequencing platforms \nhttps://github.com/TencentAILabHealthcar\ne/scBERT \nscGPT GPT-based 33 million human cells from the CellXGene collection https://github.com/bowang-lab/scGPT \nGeneformer BERT-based 29.9 million human single-cell transcriptomes https://huggingface.co/ctheodoris/Genefor\nmer \nscFoundation BERT-based About 50 million human single-cell transcriptomic profiles  https://github.com/biomap-\nresearch/scFoundation \ntGPT GPT-based 22.3 million single-cell transcriptomes https://github.com/deeplearningplus/tGPT \nGeneCompass BERT-based over 120 million single-cell transcriptomes from humans and mice https://github.com/xCompass-\nAI/GeneCompass \nscMulan GPT-based More than 10 million manually annotated single-cell RNA-seq data https://github.com/SuperBianC/scMulan \nUCE BERT-based 300 datasets from the CellXGene corpus includes over 36 million cells, \n1,000+ cell types, dozens of tissues, and eight species https://github.com/snap-stanford/uce \nscPRINT  BERT-based More than 50M cells from theCellXGene database https://github.com/cantinilab/scPRINT \nCancerFoundati\non BERT-based 50 million cells with roughly a quarter being tumor cells https://github.com/BoevaLab/CancerFoun\ndation \nNicheformer BERT-based 57 million dissociated and 53 million spatially resolved cells across 73 \ntissues from both human and mouse https://github.com/theislab/nicheformer \n \n \n  \nTable 3. Large language models for downstream tasks in bioinformatics \nInput data Biological tasks Models \nDNA sequence \nGenome-wide variant effects prediction DNABERT, DNABERT-2, GPN, Nucleotide Transformer \nDNA cis-regulatory regions prediction DNABERT, DNABERT-2, BERT -Promoter, iEnhancer -\nBERT, Nucleotide Transformer \nDNA-protein interaction prediction DNABERT, DNABERT-2, TFBert, GROVER, and \nMoDNA \nDNA methylation (6mA,4mC 5hmC) prediction BERT6mA, iDNA-ABF, iDNA-ABT, and MuLan-Methyl \nRNA splice sites prediction from DNA sequence DNABERT, DNABERT-2 \nRNA sequence \nRNA 2D/3D structure prediction RNA-FM, RNA-MSM, and RNA-FM \nRNA structural alignment, RNA family clustering RNABERT \nRNA splice sites prediction from RNA sequence SpliceBERT \nRNA N7-Methylguanosine modification prediction BERT-m7G \nRNA 2'-O-methylation Modifications prediction Bert2Ome \nMultiple types of RNA modifications prediction Rm-LR \nPredicting the association between miRNA, lncRNA and \ndisease BertNDA \nIdentifying lncRNAs LncCat \nProtein expression and mRNA degradation prediction CodonBERT \nProtein sequences \nMSAs \nGene ontology annotations \nTriplets of protein-relation-attribute \nProtein property descriptions \ncDNA sequences \nSecondary structure and contact prediction MSA Transformer, ProtTransÔºåSPRoBERTa, TAPE, KeAP \nProtein sequence generation ProGen, ProtGPT2 \nProtein function prediction SPRoBERTa, ProtST, PromptProtein, CaLM \nMajor PTMs prediction ProteinBERT \nEvolution and mutation prediction SPRoBERTa, UniRep, ESM-1b, TAPE, PLMsearch, DHR \nBiophysical properties prediction TAPE, PromptProtein \nProtein-protein interaction and binding affinity prediction KeAP \nAntigen-Receptor binding prediction MHCRoBERTa, BERTMHC, TCR-BERT, SC-AIR-BERT, \nAntiformer \nAntigen-Antibody binding prediction AbLang, AntiBERTa, EATLM \nMolecular SMILES Predicting Molecular Properties SMILES-BERT, ChemBERTa, K-BERT \nGenerating Molecules MolGPT \nMolecular graphs Predicting Molecular Properties MOLE-BERT \nMolecular fingerprints and protein \nsequences Predicting Drug-Target Interaction TransDTI, FG-BERT \nMolecular SMILES and protein \nsequences Predicting Synergistic Effects SynerGPT, C2P2 \nscRNA-seq data \nCell clustering tGPT, scFoundation, UCE, iSEEEK, CellPLM , \nBioFormers, mcBERT \nCell type annotation \nscBERT, scGPT, CIForm, TOSICA, scTransSort, \nTransCluster, Geneformer, GeneCompass, scMulan, \nCellLM, CellPLM, scPRINT \nNew cell type identification scBERT, TOSICA, UCE \nBatch effect removal scBERT, scGPT, CIForm, TOSICA, Geneformer, scMulan, \niSEEEK, scPRINT, CancerFoundation, mcBERT \nTrajectory inference/Pseudotime analysis tGPT, scMVP, iSEEEK \nDrug response/sensitivity prediction scFoundation, CellLM, CancerFoundation \nGene network inference scGPT, Geneformer, GeneCompass, iSEEEK, scGREAT , \nBioFormers, scPRINT \nGene perturbation prediction scGPT, scFoundation, GeneCompass, CellPLM , \nBioFormers \nGene expression prediction scGPT, scMVP, scFoundation, GeneCompass, CellPLM , \nBioFormers \ncis-regulatory element identification scMVP \nDrug dose -response prediction , Gene dosage sensitivity \nprediction GeneCompass \nscMuti-omics data \nSingle-cell multi-omics integration scGPT, scMVP, DeepMAPS, scCLIP \nBiological network inference DeepMAPS Cell-cell communications \nTranslating gene expression to protein abundance scTranslator, scMoFormer \nSingle-cell multimodal prediction scMoFormer \nIntegrative regulatory inference scTranslator \nSingle-cell spatial transcriptomics \ndata \nSpatial transcriptomics imputation CellPLM, Nicheformer, SpaFormer \nSpatial label prediction \nSpatial neighborhood density prediction \nSpatial neighborhood composition prediction \nNicheformer \n \nSupplementary figures \n \nSupplementary figure 1. Applications of large language models in genomics and \ntranscriptomics. DNA language models take DNA sequence as input, use transform er, BERT, \n\nGPT models to solve multiple biological tasks, including genome-wide variant effects prediction, \nDNA cis -regulatory regions prediction, DNA -protein interaction prediction, DNA methylation \n(6mA,4mC 5hmC) prediction, splice sites prediction from DNA sequence. The RNA language \nmodels take RNA sequences as input, use transform er, BERT, GPT  models to solve multiple \nbiological tasks, including RNA 2D/3D structure prediction, RNA structural alignment,, RNA \nfamily clustering, RNA splice sites prediction from RNA sequence, RN A N7-methylguanosine \nmodification prediction, RNA 2‚Äô-O-methylation modifications prediction, multiple types of RNA \nmodifications prediction, predicting the association between miRNA, lncRNA and disease, \nidentifying lncRNAs, lncRNAs‚Äô coding potential prediction, protein expression and mRNA \ndegradation prediction. \n  \n \nSupplementary figure 2. Applications of large language models in proteomics.  The protein \nlanguage models take multiple sequence alignment, protein sequence, gene ontology and protein-\nrelation-attribute as input, use transformer, BERT, GPT models to solve multiple biological tasks, \nincluding predicting secondary structure, predicting protein generation, predicting protein function, \n\npredicting post -translational modifications, predicting evolution and mutation, predicting \nbiophysical properties, predicting protein -protein interaction and predicting antigen -receptor or \nantigen-antibody binding. \n  \n \nSupplementary f igure 3. Applications of large language models in drug discovery . The \nlanguage models  for drug discovery  take molecular SMILES, protein sequence, molecular \nfingerprints and molecular graphs as input, use transformer, BERT, GPT models to solve multiple \n\nbiological tasks, including predicting molecular properties, predicting drug-target interaction, \ngenerating molecules and predicting synergistic effects. \n  \n \nSupplementary figure 4. Applications of large language models in single-cell analysis. The \nsingle-cell language models  take gene expression or single -cell multi -omics data as input, use \ntransformer, BERT, GPT models to solve multiple biological tasks, including cell type annotation, \nbatch effect removal, multi -omics integration, gene regulation network inference perturbation \nprediction, dropout imputation. \n\nSupplementary Tables \nSupplementary Table 1. Detailed information of large language models for genomic and transcriptomic tasks \nApplicatio\nn area Models Ref Publication \ntime Parameters Architec\nture \nFine-tuning datasets \nDownstream tasks \nData type Source Size \nDNA \nsequence \nlanguage \nmodel \nDNABERT [1] Aug 2021 \n12 transformer \nlayers with 768 \nhidden units and \n12 attention heads \nin each layer \nBERT-\nbased \nDNA \nsequence \nHuman TA TA and non-TA TA \npromoters of 10 000 bp \nlength[2] and ChIP-seq \ndataset [3] \n3,065 human TA TA and 26,533 \nnon-TATA promoter-containing \nsequences and 690 ChIP-seq \ndataset covers 161 transcription \nfactor binding profiles in 91 \nhuman cell lines \nTranscription factor \nbinding sites prediction \nDNA sequence - Motif analysis  \nAssembly GRCh38 FASTA \nfile [4] \n10,000 donor, acceptor, and \nnon-splice site sequences \nSplice donor and \nacceptor sites prediction \ndbSNP release 153 [5] 700 million short genetic \nvariants \nIdentifying effects of \ngenetic variants \nDNABERT-2 [6] July 2023 \nbatch size is 32, \nwarmup step is \n50, and weight \ndecay is 0.01 \nBERT-\nbased \nDNA \nsequence \nTATA and non-TA TA \npromoters downloaded from \nEukaryotic Promoter \nDatabase (EPDnew) [2] \n3,065 human TA TA and 26,533 \nnon-TATA promoter-containing \nsequences  \nPromoter detection and \ncore promoter detection \nChIP-seq datasets [7] \n161 TF binding profiles in 91 \nhuman cell lines(human) and \n78 mouse ENCODE ChIP-seq \ndata \nTranscription factor \nbinding site prediction \nEnsembl GRCh38 human \nreference genome [4] \n10,000 splice donors, \nacceptors, and non-splice site \nsequences. \nSplice site prediction \nHistone modification (Yeast) \nH3, H3K14ac, H3K36me3, \nH3K4me1, H3K4me2, \nH3K4me3, H3K79me3, \nH3K9ac, H4, H4ac \nEpigenetic marks \nprediction \nSARS_CoV_2 variants [8] \n9 types of SARS_CoV_2 \nvariants, including Alpha, \nBeta,Delta, Eta, Gamma, Iota, \nKappa, Lambda and Zeta. \nCovid variant prediction \nNucleotide [9] Jan 2023 2 hidden layers Transfor DNA Annotated DNA sequence 90,000 sequences annotated by Detect known genomic \nTransformer mer-\nbased \nsequence [10]  Ensembl (‚Äú5‚Äô UTR‚Äù, ‚Äú3‚Äô UTR‚Äù, \n‚Äúexon‚Äù, ‚Äúintron‚Äù, ‚Äúenhancer‚Äù, \n‚Äúpromoter‚Äù, ‚ÄúCTCF binding \nsite‚Äù, ‚Äúopen chromatin‚Äù, and \n‚Äútranscription factor binding \nsites‚Äù. ) \nelements \nDNA sequence with SNP \n[11] \nIndependent dataset of \ngenetically diverse human \ngenomes, originating from 7 \ndifferent meta-populations \nDetect human genetic \nvariation \n1000 Genomes Project SNPs \n[12]  \nchromosome 22 sequence with \n17 variant categories (e.g. stop \ngained, missense, intergenic) \nPredict the impact of \nmutations \nDNAGPT [13] July 2023 \n12 layers of \ntransformer \nblocks based on \nunidirectional \nattention, with \neach layer \ncontaining 12 \nattention heads \nand a hidden layer \nsize of 768 \nGPT-\nbased \nDNA \nsequence \nDNA sequence from \nDeepGSR [14] \n20,933, 18,693, 12,082, and \n27,203 true polyadenylation \nsignals data; and 28,244, \n25,205, 17,558, and 30,283 true \ntranslation initiation sites for \nhuman, mouse, bovine, and \nfruit fly, respectively which are \nused as ground-truth, non-\ngenomic signals and regions \nsequences from the genome \nsequences and combined them \nwith the true cases \nGenomic signals and \nregions prediction \nDNA sequence from Xpresso \n[15] \n18,377 and 21,856 promoters \nas well as the mRNA half-lives \nin human and mouse \nrespectively and held out 1000 \ncases in each specie \nmRNA expression level \nprediction \nGROVER [16] July 2023 \n12 transformer \nlayers, 5,000 \nembeddings  \nBERT-\nbased \nDNA \nsequence CTCF ChIP-seq data [17] \n~85,000 binding motifs, only \n~32,000 are indeed bound by \nCTCF \nProtein-DNA binding \nprediction \nGPN [18] \nBioRxiv \nposted April \n2023 \n25 convolutional \nblocks with a \nfeed-forward \nlayer, \n512embedding \nsizes of the pre-\ntrianed foundation \nmodel \nBERT-\nbased \nDNA \nsequence \nDNA sequence - DNA motifs predictions \n1001 Genomes Project [12] 10 million SNPs Variant effect prediction \nBERT-\nPromoter [19] Aug 2022 \nBERT model \nincluded 12 \nlayers, 768-\nhidden, 12 heads, \nand 110,000,000 \nparameters \nBERT-\nbased \nDNA \nsequence \nChIP-chip data, gSELEX \npeaks, ChIP-exo plus RNA-\nseq [20, 21] \n3382 promoters (1591 strong \npromoter samples and 1791 \nweak promoter samples) and \n3382 non-promoters \nDNA promoter prediction \nTFBert [22] Mar 2023  12-layer encoder BERT-\nbased \nDNA \nsequence ChIP-seq datasets [7] \n690 ChIP-seq dataset contains a \ntraining set (80%) and a \ncorresponding test set (20%) \nDNA‚Äìprotein binding \nsites prediction \nMoDNA [23] Aug 2022 - BERT-\nbased \nDNA \nsequence \nSame experiment data with \nDNABERT [2, 3] \n3,065 human TA TA and 26,533 \nnon-TATA promoter-containing \nsequences and 690 ChIP-seq \ndataset covers 161 transcription \nfactor binding profiles in 91 \nhuman cell lines \nPromoter Prediction \nCHIP-Seq datasets [3] \n690 CHIP-Seq datasets of \nuniform TFBS contains 161 \nTFs covering 91 human cell \ntypes \nTranscription Factor \nBinding Sites Prediction \niEnhancer-\nBERT [24] Aug 2022 \n12-layer \ntransformer \narchitecture with \nsimple fine-tuning \nBERT-\nbased \nDNA \nsequence \n15 chromatin states of 9 cell \ntypes [25] \n2968 samples including 1484 \nnon-enhancers, 742 strong \nenhancers and 742 weak \nenhancers \nIdentifying Enhancers \nand Their Strength \nBERT6mA [26] Mar 2022 \nThe hidden size of \nthe LSTM unit is \nset to 128.DNA \nsequence \nembedding with \nWord2vec. \nBERT-\nbased \nDNA \nsequence \nNuclei purification, MNase-\nseq and ChIP-seq [27-31] \n6mA and non6mA data in 11 \nspecies including Arabidopsis \nthaliana (31873 6mAs and non-\n6mAs), Caenorhabditis elegans \n(79616 mAs and non-6mAs), \nCasuarina equisetifpolia (6066 \n6mAs and non-6mAs), \nDrosophila melanogaster (11 \n191 6mAs and non-6mAs), \nFragaria vesca (3102 6mAs and \nnon-6mAs), H. sapiens (18 335 \n6mAs and non-6mAs), Rosa \nchinensis (599 6mAs and non-\n6mAs), Saccharomyces \ncerevisiae (37866mAs and non-\n6mAs), Thermus thermophilus \n(107 600 6mAs and non-\n6mAs), Ts. SUP5‚Äì1 (3379 \nDNA N6-methyladenine \nsite prediction \n6mAs and non6mAs) and Xoc. \nBLS256 (17 215 6mAs and \nnon-6mAs) \niDNA-ABF [32] Oct 2022 \n12 transformer \nlayers with 768 \nhidden units and \n12 attention heads \nin each layer \nBERT-\nbased \nDNA \nsequence \nChIP-seq data, ATAC-seq \ndata, and histone \nmodifications (HM) data of \nthree human cell lines [33, \n34], and DNA methylation \ndataset from the iDNA-MS \n[35] \n3 main types of DNA \nmethylation sites (6mA,4mC, \nand 5hmC) across 12 genomes \n(1 bacteria and 11 eukaryotes), \nin total 250,599 positive \nsamples \nDNA 6mA, 4mC, and \n5hmC prediction \nDNA sequence - Motifs analysis \niDNA-ABT [36] Sep 2021 \n12 transformer \nlayers with 12 \nattention heads in \neach layer. \nBERT-\nbased \nDNA \nsequence \nChIP-seq data, ATAC-seq \ndata, and histone \nmodifications (HM) data of \nthree human cell lines [33, \n34] , and DNA methylation \ndataset from the iDNA-MS \n[35] \n3 main types of DNA \nmethylation sites (6mA,4mC, \nand 5hmC) across 12 genomes \n(1 bacteria and 11 eukaryotes), \nin total 250,599 positive \nsamples \nDNA 6mA, \n4mC, and 5hmC \nprediction \nDNA sequence - Motifs analysis \nMuLan-\nMethyl [37] July 2023 \n12 layers in the \nencoder \nstack, 768 hidden \nunits for feed-\nforward networks, \nand 12 attention \nheads. \nBERT-\nbased \nDNA \nsequence \nChIP-seq data, ATAC-seq \ndata, and histone \nmodifications (HM) data of \nthree human cell lines [33, \n34] , and DNA methylation \ndataset from the iDNA-MS \n[35] \n3 main types of DNA \nmethylation sites (6mA,4mC, \nand 5hmC) across 12 genomes \n(1 bacteria and 11 eukaryotes), \nin total 250,599 positive \nsamples \nDNA 6mA, \n4mC, and 5hmC \nprediction \nRNA \nsequence \nlanguage \nmodel \nRNA-MSM [38] Nov 2023 \n12 attention heads \nwith embedding \nsize \nof 768 \nBERT-\nbased \nRNA \nsequence \nRNA secondary structure and \nthree-dimensional RNA \nstructures [39] \nThe training, validation, and \ntest sets have 405, 40, and 70 \nRNAs. \nRNA secondary structure \nprediction \nRNA secondary structure and \nthree-dimensional RNA \nstructures [39] \nThe training, validation, and \ntest sets have 405, 40, and 70 \nRNAs. \nRNA solvent \naccessibility prediction \nRNA-FM [40] Arxiv posted \nApr 2022 \n12 transformer-\nbased \nbidirectional \nencoder blocks \nand 640 \nembedding \nBERT-\nbased \nRNA \nsequence \nRNA secondary structure \n[41, 42] \n37149 structures from 8 RNA \ntypes of RNAStralign and 3975 \nRNA structures from 10 RNA \ntypes of ArchiveII \nRNA secondary structure \nprediction  \nRNA secondary structure \n[41, 42] \n37149 structures from 8 RNA \ntypes of RNAStralign and 3975 \nRNA structures from 10 RNA \ntypes of ArchiveII \nRNA 3D closeness \nprediction \nwhole genome of Severe \nacute respiratory syndrome \ncoronavirus 2 (SARS-CoV-2) \n[43] \nWhole genome  \nSARS-CoV-2 genome \nstructure and evolution \nprediction \nIn vivo RNA secondary \nstructure profiles for RNA-\nprotein interaction [44] \n- Protein-RNA interaction \nprediction \nHuman 5‚ÄôUTR library [45] \n83,919 5‚ÄôUTRs of 75 different \nlengths and their corresponding \nmean ribosome loadings \nmRNA 5‚Äô UTR-based \nmean ribosome loading \nprediction \nRNABERT \n [46] Feb 2022 \n6 hidden layers of \nBERT, an \nembedding layer, \none bidrectional-\nLSTM unit, two \ndense layers one \nwith ReLU \nactivation and a \nsoftmax output \nlayer of LSTM \nBERT-\nbased \nRNA \nsequence \nRNA (ncRNA) families from \nRFam database[46] 31 RNA families \nclassifying RNA families \nRNA secondary structure \nprediction \nSpliceBERT [47] Mar 2024 \n6 transformer \nencoder layers, \n512 hidden layer \nand 16 attention \nheads \nBERT-\nbased \nRNA \nsequence \nReference genomes in fasta \n[48] \nThe pre-mRNA sequences from \n72 vertebrate genomes for pre-\ntraining \nEstimating splice sites \nBERT-m7G [49] Aug 2021 - BERT-\nbased \nRNA \nsequence \nRNA sequence with N7-\nmethylguanosine sites and \nRNA sequence without N7-\nmethylguanosine sites \nAlkAniline-Seq, MeRIP-seq, \nand miCLIP-seq [50] \n741 RNA sequences with N7-\nmethylguanosine sites and 741 \nRNA sequences without N7-\nmethylguanosine sites \nRNA N7-\nmethylguanosine sites \nprediction \nM6A-BERT-\nStacking [51] March 2023 \n12 transformer \nlayers with 12 \nattention heads in \neach layer. \nBERT-\nbased \nRNA \nsequence \nRNA sequence with m6A \nsites and RNA sequence \nwithout m6A sites identified \nfrom MeRIP , m6A-seq, PA-\nm6A-seq, and miCLIP [52] \n11 datasets including 3000~ \n16000 RNA sequences for each \ndataset  \nRNA m6A sites \nprediction \nBert2Ome [53] May 2023 \n16 heads, 12 \nlayers, and 1024 \nhidden units \nBERT-\nbased \nRNA \nsequence \n2-O-methylation \nmodification sites from \nRMBase database [54] \n215 positive, 215 negative \ninstances for the training part \nand 46 positive, 114 negative \ninstances for the testing part. \nRNA 2-O-methylation \nprediction \nRm-LR [55] Sep 2023 6 transformer BERT- RNA Transcriptomic-wide 20 different epi-transcriptome Multiple types of RNA \nencoder layers, \nwith a hidden \nlayer size of 512 \nand 16 attention \nheads \nbased sequence profiling data derived from \nthe MultiRM, GEO, \nRMBase, RADAR [54, 56-\n58] \nprofiles based on various base \nresolution techniques \nmodifications prediction \nBertNDA [59] Nov 2023 8 layers BERT-\nbased \nRNA \nsequence \nmiRNA-disease associations, \nlncRNA-disease associations, \nand miRNA-lncRNA \nassociations [60-63] \n1000 positive pairs and 1000 \nnegative pairs \nncRNA-Disease \nAssociation Prediction \nLncCat [64] Feb 2023 - BERT-\nbased \nRNA \nsequence \nlncRNAs and protein-coding \ntranscripts of five species \n[10, 63, 65] \n22960 coding transcripts and \n21081 lncRNAs of human. \n20707 coding transcripts and \n10707 lncRNAs of mouse. \n15891 coding transcripts and \n4382 lncRNAs of zebrafish. \n4693 coding transcripts and \n5377 lncRNAs of wheat \n20584 coding transcripts and \n3897 lncRNAs of chicken \nIdentify lncRNA \nLSCPP-\nBERT [66] Dec 2023 \n4 identical layers \nand each layer is \ndivided into two \nsublayers \nBERT-\nbased \nRNA \nsequence \nlncRNAs sequences from \nmultispecies [67] \n593251 plant lncRNAs \nsequences \nlncRNA-sORFs coding \npotential prediction \nCodonBERT [68] Oct 2023 \n12 layers of \nbidirectional \ntransformer \nencoders, Each \ntransformer layer \nwith 12 self-\nattention heads \nBERT-\nbased \nRNA \nsequence \nmRFP Expression dataset; \n[69] Fungal expression \ndataset; E. coli proteins \ndataset; mRNA stability \ndataset; Tc-Riboswitches \ndataset [70] \nExperimental data for protein \nexpression (2308 low \nexpression proteins, 2067 \nmedium expression proteins, \nand 1973 high expression \nproteins, respectively); \nmRNA properties \nprediction \nSARS-CoV-2 V accine \ndegradation dataset - Vaccine expression \nprediction \nRNA-\nTorsionBERT [71] Jun 2024 18 layer  BERT-\nbased \nRNA \nsequence \nPDB structure and removed \nthe structures from the \nnonredundant Training \n4,267 structures with sequences \nfrom 11 to 508 nucleotides \nRNA 3D structure \nprediction \nUNI-RNA [72] \nbioRxiv \nposted \nJul 2023 \n- BERT-\nbased \nRNA \nsequence \nnon-coding RNA sequences \nfrom RNAcentral, nucleic \nacid data from NCBI‚Äôs \ndatabase, and genomic data \nfrom repositories such as \nGenome Warehouse[73-75]  \n37,149 RNA structures \n7,600 held-out real human \n5‚ÄôUTRs \n3 million distinct UTR \nsequences  \nRNA secondary structure \nprediction \nRNA distance map \nprediction \nmRNA 5‚Ä≤-UTR mean \nribosome load prediction \nAlternative \npolyadenylation isoform \nprediction \nRNA splice site \nprediction \nncRNA classification \nRNA modification \nprediction \nUTR-LM [76] Apr 2024 \nSix-layer \ntransformer with \n16 multi-head \nself-attention. \nTransfor\nmer-\nbased \nRNA \nsequence \nUnlabeled 5' UTR sequences \nfrom three sources: the \nEnsembl database \n214,349 unlabeled 5' UTR \nsequences  \nMean ribosome loading \nprediction. \nmRNA expression level \nand translation efficiency \nprediction \nInternal ribosome entry \nsite identification \nAttention-based motif \ndetection \n3UTRBERT [77] Aug 2024 \n12 identical \nTransformer \ncomponentsÔºå\nEach layer \ncontained a multi-\nhead self-attention \nmodule and a \nposition-wise \nfully connected \nfeed-forward layer \nBERT-\nbased \nRNA \nsequence \n3‚ÄôUTR of human \nmRNAtranscript \neCLIP datasets \nRNA localization data \n108 573 unique mRNA \ntranscripts \n1000 samples RNA binding \nsamples with1000 sequences  \n17 023 mRNAs \nmRNA subcellular \nlocalization prediction \n RNAErnie [78] May 2024 \n12-layer \ntransformer and a \nhidden state \ndimension of 768 \nTransfor\nmer-\nbased \nRNA \nsequence \nnon-coding RNA sequences \nfrom RNAcentral  \nRNA grouping \nRNA sequence \nclassification \nRNA‚ÄìRNA interaction \nprediction \nRNA secondary structure \nprediction \n \n  \nReferences \n1. Ji, Y ., et al., DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA -language in genome.  \nBioinformatics, 2021. 37(15): p. 2112-2120. \n2. Dreos, R., et al., EPD and EPDnew, high-quality promoter resources in the next-generation sequencing era. Nucleic acids research, 2013. \n41(D1): p. D157-D164. \n3. Consortium, E.P., An integrated encyclopedia of DNA elements in the human genome. Nature, 2012. 489(7414): p. 57. \n4. Cunningham, F., et al., Ensembl 2019. Nucleic acids research, 2019. 47(D1): p. D745-D751. \n5. Sherry, S.T., et al., dbSNP: the NCBI database of genetic variation. Nucleic acids research, 2001. 29(1): p. 308-311. \n6. Zhou, Z., et al., Dnabert-2: Efficient foundation model and benchmark for multi-species genome. arXiv preprint arXiv:2306.15006, 2023. \n7. Zeng, H., et al., Convolutional neural network architectures for predicting DNA ‚Äìprotein binding. Bioinformatics, 2016. 32(12): p. i121-\ni127. \n8. Chen, K., H. Zhao, and Y . Yang, Capturing large genomic contexts for accurately predicting enhancer-promoter interactions. Briefings in \nBioinformatics, 2022. 23(2): p. bbab577. \n9. Dalla-Torre, H., et al., The nucleotide transformer: Building and evaluating robust foundation models for human genomics. bioRxiv, 2023: \np. 2023.01. 11.523679. \n10. Howe, K.L., et al., Ensembl 2021. Nucleic acids research, 2021. 49(D1): p. D884-D891. \n11. Bergstr√∂m, A., et al., Insights into human genetic variation and population history from 929 diverse genomes.  Science, 2020. 367(6484): \np. eaay5012. \n12. Alonso-Blanco, C., et al., 1,135 genomes reveal the global pattern of polymorphism in Arabidopsis thaliana.  Cell, 2016. 166(2): p. 481-\n491. \n13. Zhang, D., et al., DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks.  bioRxiv, 2023: p. 2023.07. \n11.548628. \n14. Kalkatawi, M., et al., DeepGSR: an optimized deep-learning structure for the recognition of genomic signals and regions. Bioinformatics, \n2019. 35(7): p. 1125-1132. \n15. Agarwal, V . and J. Shendure, Predicting mRNA abundance directly from genomic sequence using deep convolutional neural networks. Cell \nreports, 2020. 31(7). \n16. Sanabria, M., et al., DNA language model GROVER learns sequence context in the human genome.  Nature Machine Intelligence, 2024. \n6(8): p. 911-923. \n17. de Souza, N., The ENCODE project. Nature methods, 2012. 9(11): p. 1046-1046. \n18. Benegas, G., S.S. Batra, and Y .S. Song, DNA language models are powerful zero-shot predictors of genome-wide variant effects. bioRxiv, \n2022: p. 2022.08. 22.504706. \n19. Le, N.Q.K., et al., BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre -trained model and SHAP \nfeature selection. Computational Biology and Chemistry, 2022. 99: p. 107732. \n20. Gama-Castro, S., et al., RegulonDB version 9.0: high -level integration of gene regulation, coexpression, motif clustering and beyond.  \nNucleic acids research, 2016. 44(D1): p. D133-D143. \n21. Xiao, X., et al., iPSW (2L)-PseKNC: A two-layer predictor for identifying promoters and their strength by hybrid features via pseudo K -\ntuple nucleotide composition. Genomics, 2019. 111(6): p. 1785-1793. \n22. Luo, H., et al., Improving language model of human genome for DNA ‚Äìprotein binding prediction based on task -specific pre-training. \nInterdisciplinary Sciences: Computational Life Sciences, 2023. 15(1): p. 32-43. \n23. An, W., et al. MoDNA: motif-oriented pre-training for DNA language model. in Proceedings of the 13th ACM International Conference on \nBioinformatics, Computational Biology and Health Informatics. 2022. \n24. Luo, H., et al. iEnhancer-BERT: A novel transfer learning architecture based on DNA-Language model for identifying enhancers and their \nstrength. in International Conference on Intelligent Computing. 2022. Springer. \n25. Ernst, J., et al., Mapping and analysis of chromatin state dynamics in nine human cell types. Nature, 2011. 473(7345): p. 43-49. \n26. Tsukiyama, S., et al., BERT6mA: prediction of DNA N6 -methyladenine site using deep learning -based approaches.  Briefings in \nBioinformatics, 2022. 23(2): p. bbac053. \n27. Xiao, C.-L., et al., N6-methyladenine DNA modification in the human genome. Molecular cell, 2018. 71(2): p. 306-318. e7. \n28. Ye, G., et al., De novo genome assembly of the stress tolerant forest species Casuarina equisetifolia provides insight into secondary growth. \nThe Plant Journal, 2019. 97(4): p. 779-794. \n29. Ye, P., et al., MethSMRT: an integrative database for DNA N6-methyladenine and N4-methylcytosine generated by single-molecular real-\ntime sequencing. Nucleic acids research, 2016: p. gkw950. \n30. Liu, Z.-Y ., et al., MDR: an integrative DNA N6 -methyladenine and N4-methylcytosine modification database for Rosaceae.  Horticulture \nresearch, 2019. 6. \n31. Wang, Y ., et al., N6-adenine DNA methylation is associated with the linker DNA of H2A. Z-containing well-positioned nucleosomes in Pol \nII-transcribed genes in Tetrahymena. Nucleic acids research, 2017. 45(20): p. 11594-11606. \n32. Jin, J., et al., iDNA-ABF: multi -scale deep biological language learning model for the interpretable prediction of DNA methylations.  \nGenome biology, 2022. 23(1): p. 1-23. \n33. Luo, Y ., et al., New developments on the Encyclopedia of DNA Elements (ENCODE) data portal. Nucleic acids research, 2020. 48(D1): p. \nD882-D889. \n34. Zhang, J., et al., An integrative ENCODE resource for cancer genomics. Nature communications, 2020. 11(1): p. 3696. \n35. Lv, H., et al., iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes. Iscience, 2020. 23(4). \n36. Yu, Y ., et al., iDNA-ABT: advanced deep learning model for detecting DNA methylation with adaptive features and transductive information \nmaximization. Bioinformatics, 2021. 37(24): p. 4603-4610. \n37. Zeng, W., A. Gautam, and D.H. Huson, MuLan-Methyl-Multiple Transformer-based Language Models for Accurate DNA Methylation \nPrediction. bioRxiv, 2023: p. 2023.01. 04.522704. \n38. Zhang, Y ., et al., Multiple sequence alignment -based RNA language model and its application to structural inference.  Nucleic Acids \nResearch, 2024. 52(1): p. e3-e3. \n39. Singh, J., et al., RNA secondary structure prediction using an ensemble of two -dimensional deep neural networks and transfer learning.  \nNature communications, 2019. 10(1): p. 5407. \n40. Chen, J., et al., Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions.  \nbioRxiv, 2022: p. 2022.08. 06.503062. \n41. Tan, Z., et al., TurboFold II: RNA structural alignment and secondary structure prediction informed by multiple homologs.  Nucleic acids \nresearch, 2017. 45(20): p. 11570-11581. \n42. Sloma, M.F. and D.H. Mathews, Exact calculation of loop formation probability identifies folding motifs in RNA secondary structures.  \nRNA, 2016. 22(12): p. 1808-1818. \n43. Wu, F., et al., A new coronavirus associated with human respiratory disease in China. Nature, 2020. 579(7798): p. 265-269. \n44. Sun, L., et al., Predicting dynamic cellular protein‚ÄìRNA interactions by deep learning using in vivo RNA structures.  Cell research, 2021. \n31(5): p. 495-516. \n45. Sample, P.J., et al., Human 5‚Ä≤ UTR design and variant effect prediction from a massively parallel translation assay. Nature biotechnology, \n2019. 37(7): p. 803-809. \n46. Akiyama, M. and Y . Sakakibara, Informative RNA base embedding for RNA structural alignment and clustering by deep representation \nlearning. NAR genomics and bioinformatics, 2022. 4(1): p. lqac012. \n47. Chen, K., et al., Self-supervised learning on millions of primary RNA sequences from 72 vertebrates improves sequence-based RNA splicing \nprediction. Briefings in Bioinformatics, 2024. 25(3): p. bbae163. \n48. Haeussler, M., et al., The UCSC genome browser database: 2019 update. Nucleic acids research, 2019. 47(D1): p. D853-D858. \n49. Zhang, L., et al., BERT-m7G: a transformer architecture based on BERT and stacking ensemble to identify RNA N7-Methylguanosine sites \nfrom sequence information. Computational and Mathematical Methods in Medicine, 2021. 2021. \n50. Dai, C., et al., Iterative feature representation algorithm to improve the predictive performance of N7-methylguanosine sites. Briefings in \nBioinformatics, 2021. 22(4): p. bbaa278. \n51. Li, Q., et al., M6A-BERT-Stacking: A Tissue-Specific Predictor for Identifying RNA N6-Methyladenosine Sites Based on BERT and Stacking \nStrategy. Symmetry, 2023. 15(3): p. 731. \n52. Dao, F.-Y ., et al., Computational identification of N6-methyladenosine sites in multiple tissues of mammals. Computational and structural \nbiotechnology journal, 2020. 18: p. 1084-1091. \n53. Soylu, N.N. and E. Sefer, BERT2OME: Prediction of 2' -O-methylation Modifications from RNA Sequence by Transformer Architecture \nBased on BERT. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2023. \n54. Xuan, J.-J., et al., RMBase v2. 0: deciphering the map of RNA modifications from epitranscriptome sequencing data. Nucleic acids research, \n2018. 46(D1): p. D327-D334. \n55. Liang, S., et al., Rm-LR: A long-range-based deep learning model for predicting multiple types of RNA modifications. Computers in Biology \nand Medicine, 2023. 164: p. 107238. \n56. Song, Z., et al., Attention-based multi-label neural networks for integrated prediction and interpretation of twelve widely occurring RNA \nmodifications. Nature communications, 2021. 12(1): p. 4011. \n57. Barrett, T., et al., NCBI GEO: archive for functional genomics data sets‚Äîupdate. Nucleic acids research, 2012. 41(D1): p. D991-D995. \n58. Ramaswami, G. and J.B. Li, RADAR: a rigorously annotated database of A -to-I RNA editing. Nucleic acids research, 2014. 42(D1): p. \nD109-D113. \n59. Ning, Z., et al., BertNDA: a Model Based on Graph-Bert and Multi-scale Information Fusion for ncRNA-disease Association Prediction. \nbioRxiv, 2023: p. 2023.05. 18.541387. \n60. Li, Y ., et al., HMDD v2. 0: a database for experimentally supported human microRNA and disease associations.  Nucleic acids research, \n2014. 42(D1): p. D1070-D1074. \n61. Jiang, Q., et al., miR2Disease: a manually curated database for microRNA deregulation in human disease.  Nucleic acids research, 2009. \n37(suppl_1): p. D98-D104. \n62. Bao, Z., et al., LncRNADisease 2.0: an updated database of long non -coding RNA-associated diseases. Nucleic acids research, 2019. \n47(D1): p. D1034-D1037. \n63. Gao, Y ., et al., Lnc2Cancer 3.0: an updated resource for experimentally supported lncRNA/circRNA cancer associations and web tools \nbased on RNA-seq and scRNA-seq data. Nucleic acids research, 2021. 49(D1): p. D1251-D1258. \n64. Feng, H., et al., LncCat: An ORF attention model to identify LncRNA based on ensemble learning strategy and fused sequence information. \nComputational and Structural Biotechnology Journal, 2023. 21: p. 1433-1447. \n65. O'Leary, N.A., et al., Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation.  \nNucleic acids research, 2016. 44(D1): p. D733-D745. \n66. Xia, S., et al. A multi-granularity information-enhanced pre-training method for predicting the coding potential of sORFs in plant lncRNAs. \nin 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2023. IEEE. \n67. Di Marsico, M., et al., GreeNC 2.0: a comprehensive database of plant long non-coding RNAs. Nucleic Acids Research, 2022. 50(D1): p. \nD1442-D1447. \n68. Babjac, A.N., Z. Lu, and S.J. Emrich. CodonBERT: Using BERT for Sentiment Analysis to Better Predict Genes with Low Expression . in \nProceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. 2023. \n69. Nieuwkoop, T., et al., Revealing determinants of translation efficiency via whole-gene codon randomization and machine learning. Nucleic \nacids research, 2023. 51(5): p. 2363-2376. \n70. Byrska-Bishop, M., et al., High-coverage whole-genome sequencing of the expanded 1000 Genomes Project cohort including 602 trios.  \nCell, 2022. 185(18): p. 3426-3440. e19. \n71. Bernard, C., et al., RNA-TorsionBERT: leveraging language models for RNA 3D torsion angles prediction.  bioRxiv, 2024: p. 2024.06. \n06.597803. \n72. Wang, X., et al., UNI-RNA: universal pre-trained models revolutionize RNA research. bioRxiv, 2023: p. 2023.07. 11.548588. \n73. RNAcentral: a hub of information for non-coding RNA sequences. Nucleic Acids Research, 2019. 47(D1): p. D221-D229. \n74. Sayers, E.W., et al., Database resources of the national center for biotechnology information.  Nucleic acids research, 2022. 50(D1): p. \nD20-D26. \n75. Chen, M., et al., Genome Warehouse: a public repository housing genome -scale data. Genomics, Proteomics and Bioinformatics, 2021. \n19(4): p. 584-589. \n76. Chu, Y ., et al., A 5 ‚Ä≤ UTR language model for decoding untranslated regions of mRNA and function predictions.  Nature Machine \nIntelligence, 2024. 6(4): p. 449-460. \n77. Yang, Y ., et al., Deciphering 3'UTR Mediated Gene Regulation Using Interpretable Deep Representation Learning.  Advanced Science, \n2024. 11(39): p. 2407013. \n78. Wang, N., et al., Multi-purpose RNA language modelling with motif -aware pretraining and type -guided fine -tuning. Nature Machine \nIntelligence, 2024: p. 1-10. \n \nSupplementary Table 2. Detailed information of large language models for proteomic tasks \nApplication \narea Models Ref Publication \ntime Parameters Architecture \nDatasets \nDownstream tasks \nData type Source Size \nProtein \nLarge \nLanguage \nModels \nMSA \nTransformer [1] Jul 2021 \n100M parameters model with \n12 layers, 768 embedding \nsize, and 12 attention heads \nTransformer-\nbased \nMSAs \nCAMEO [2] 131 domains (129 \nevaluated) \nUnsupervised contact prediction, \nsupervised contact prediction, \nsecondary structure prediction \nCASP13-FM [3] \n31 free modeling \ndomains (from 25 \ntargets) \ntrRosetta training \nset [4] \n15,051 MSAs and \nstructures (14,842 \nused) \nProtein sequences \nCB513 [5] 513 protein sequences \nNetsurf dataset [6] \n12,185 crystal \nstructures obtained \nfrom the PDB [7] \nUniRep [8] Dec 2019 \n18.2M parameters (a 1,900-\nhidden unit mLSTM with \namino-acid character \nembeddings) \nLSTM-based Protein sequences \nUniRef50 [9] 24M \nPredicting stability of naturally \noccurring and de novo designed \nproteins, Prediction of \nfunctional effects of single \nmutations in diverse proteins \nMini protein \ndataset from [10] \n1,432 out of 5,570 test \nset and 1,416 out of \n5,571 validation set \nDMS dataset [11] ~65,420 variants across \nthe 8 proteins \navGFP dataset \n[12] \n32,400 variants derived \nfrom 27 homologs of \navGFP (used) \nTAPE [13] Dec 2019 38M parameters \n(ResNet, \nTransformer, \nLSTM)-\nbased \nProtein sequences \nNetsurf dataset [6] 12,185 protein \nsequences Secondary structure (SS) \nprediction (structure prediction \ntask), contact prediction \n(structure prediction task), \nremote homology detection \n(evolutionary understanding \ntask), fluorescence landscape \nprediction (protein engineering \ntask), stability landscape \nprediction (protein engineering \ntask) \nProteinNet dataset \n[14] \n~332,283,871 protein \nsequences \nDeepSF dataset \n[15] \nTraining set includes \n16,712 proteins \nspanning 1,195 folds \n[16]; test datasets \ninclude 2,533 protein \ndomains across 550 \nfolds from SCOP 2.06 \n[16], a subset of SCOP \n1.75 and the CASP \ndataset [17, 18]  \navGFP dataset \n[12] \n~51,715 protein \nsequences \nDataset from [10] ~ 46,800 protein \nsequences \nESM-1b [19] Dec 2020 A model with ‚àº650M \nparameters (33 layers) \nTransformer-\nbased Protein sequences \nSCOPe [20] 15,297 protein \nsequences  \nRemote homology detection, \nprediction of secondary \nstructure, long-range residue‚Äì\nresidue contacts, mutational \neffect prediction, etc. \nCB513 [5] 513 protein sequences; \nCASP13 [21] 431 domains \nEnvision (DMS \ndataset) [11] \nand \nDeepSequence \n[22] \nOver 700,000 variant \neffect measurements \nfrom over 100 large-\nscale experimental \nmutagenesis datasets \nProtTrans [23] Aug 2021 From millions to billions \nparameters (224M-11B) \n(Transformer\n-XL, XLNet, \nBERT, \nAlbert, \nElectra, T5)-\nbased \nProtein sequences \nCB513 [5] 513 protein sequences \nPer-residue secondary structure \npredictionÔºåper-protein \nlocalization & membrane \nprediction \nTS115 [5] 115 protein sequences \nCASP12 [24] ~102 protein sequences \nNEW364 [23] 364 protein sequences \nDeepLoc [25] ~19,817 protein \nsequences \nSCOPe 2.07 [26] \n14,323 protein \nsequences (non-\nredundant at PIDE < \n40%) \nSPRoBERTa [27] Sep 2022 \n12 Transformer encoder \nlayers, the embedding size is \n768, the feed-forward hidden \nunits are 3072 and the \nattention heads are 12 \nBERT-based Protein sequences \nNetsurf dataset [6] \n12,185 protein \nsequences \n Secondary structure prediction, \ncontact prediction, \n remote homology \nprediction, protein function \nprediction or Gene Ontology \n(GO) term prediction \nCB513 [5] 513 protein sequences \n \nCASP12 [24] ~102 protein sequences \n \nDeepSF dataset \n[15] \nConsistent with the \nsize used TAPE [13] \nDeepFRI [28] \n~284,832 protein \nsequences with GO \nannotations \nPromptProtei\nn [29] Sep 2022 \n650M parameters with 33 \nlayers and 20 attention \nheads. The embedding size is \n1280 \nTransformer-\nbased Protein sequences \nEC dataset from \n[28] \n19,199 protein \nsequences \nEnzyme commission and Gene \nOntology prediction, stability \nlandscape prediction, \nfluorescence landscape \nprediction, thermostability \nlandscape prediction, adeno-\nassociated virus (AA V) \nlandscape prediction, GB1 \nlandscape prediction, antibody-\nantigen affinity prediction \nGO dataset from \n[28] \n36,641 protein \nsequences \n \nProtein \nengineering \ndataset from \nTAPE and FLIP \n[13, 30] \n68,965 protein \nsequences used in \nstability prediction, \n54,025 used in \nfluorescence landscape \nprediction, 28,131 used \nin thermostability \nlandscape prediction, \n82,583 in AA V \nprediction, 8,733 in \nGB1 prediction \nProGen [31] Mar 2020 \n1.2B parameters. Sequence \nlength is 512. The model has \ndimension d = 1028, inner \ndimension f = 512, 36 layers, \nand 8 heads per layer. \nDropout with probability 0.1 \nfollows the residual \nconnections in each layer \nTransformer-\nbased \nProtein sequences, \nConditioning tags \nUniparc [32], \nUniprotKB [33], \nSWISS-PROT \n[34], TrEMBL \n[35], Pfam [36], \nand NCBI \ntaxonomic \ninformation [37] \n281M \nControllable protein generation \nand two case study: completing \nVEGFR2 kinase domain, zero-\nshot fitness selection for protein \nGB1 \nTranception [38] Jun 2022 700M parameters Transformer-\nbased Protein sequences Protein Gym (a \nDMS dataset) [38] \n~1.8M protein \nsequences Fitness prediction \nProtGPT2 [39] Jul 2022 \n738M parameters. The model \nconsists of 36 layers with a \nmodel dimensionality of \n1280. The architecture \nmatches that of the \npreviously released GPT2-\nlarge \nGPT-based Protein sequences \nUniRef50 [40] \n10,000 protein \nsequences \n Sequence dataset generation, \nhomology detection, disorder \nprediction ProtGPT2 dataset \n[39] \nGenerated 10,000 \nprotein sequences \nProteinBERT [41] Feb 2022 \n16M parameters. The model \narchitecture consists of two \nalmost parallel paths: one for \nlocal representations with \nBERT-based \nProtein sequences, \nGene Ontology \n(GO) annotations \nSecondary \nstructure dataset \nfrom  \n8,678 sequences (train) \n[13, 42] \n(Secondary structure, disorder, \nRemote homology, fold classes, \nsignal peptide, major PTMs, \nneuropeptide cleavage, Disorder dataset 8,678 sequences (train) \nd=128 and the other for \nglobal representations with \nd=512 \nfrom  [42] fluorescence, stability) \nprediction Remote homology \ndataset from  \n12,312 sequences \n(train) [13, 43, 44] \nFold classes \ndataset from \n15,680 sequences \n(train) [43, 44] \nSignal peptide \ndataset from \n16,606 sequences \n(train) [45] \nMajor PTMs \ndataset from \n43,356 sequences \n(train) [46] \nNeuropeptide \ncleavage dataset \nfrom \n2,727 sequences (train) \n[47, 48] \nFluorescence \ndataset from \n21,446 sequences \n(train) [12, 13] \nStability dataset \nfrom \n53,679 sequences \n(train) [10] \nProtST [49] Jan 2023 \nDepends on the parameters \nof the chosen language \nmodel \nMulti-\nmodels-based \n(Protein \nLanguage \nModels and \nBiomedical \nLanguage \nModels) \nProtein sequences, \nproperty \ndescriptions \nProtDescribe [50, \n51] \n553,052 aligned pairs \nof protein sequence \nand property \ndescription \nProtein localization prediction, \nfitness landscape prediction, \nprotein function annotation \n(totally 11 downstream tasks) \nKeAP [52] Jan 2023 \nDepends on the parameters \nof the chosen language \nmodel \nMulti-\ncascade Bert-\nlike network \nTriplet in the format \nof (Protein, \nRelation, Attribute) \nProteinKG25 [53] \n5M with nearly 600k \nprotein, 50k attribute \nterms, and 31 relation \nterms included \nAmino acid contact prediction, \nprotein homology detection, \nprotein stability prediction, \nprotein-protein interaction \nidentification, protein-protein \nbinding affinity prediction, and \nsemantic similarity inference \nCaLM [54] Jan 2024 \n86M parameters. 12 \ntransformer layers contain 12 \nattention heads, with \ndimension 768. Similar to \narchitectures of ESM family \nTransformer-\nbased \nProtein-coding \nDNA (cDNA) \ncDNA dataset \nobtained from the \nEuropean \nNucleotide \nArchive with a \ntimestamp of \nApril 2022 \n \n9,858,385 cDNA \nsequences of seven \nmodel organisms \nMelting point prediction, \nsolubility prediction, subcellular \nlocalization prediction and \nfunction prediction \nMelting \ntemperature \ndataset [30, 55] \n- \nSubcellular \nlocalization \ndataset [30, 56] \n- \nSolubility dataset \n[57] - \nGene ontology \ndataset [58] - \nTranscriptomics \ndataset [59]   \n \n- \nProteomics \ndataset [60] - \nPLMSearch [61] Mar 2024 \nESM-1b (650M parameters) \n[19] and ProtT5-XL-\nUniRef50 (3B parameters) \n[23] \nTransformer-\nbased Protein sequences \nSCOPe40 [20, \n62], New protein \nsearch test, Swiss-\nProt [34], \nCATHS40 [63] \n~489,764 sequences \nfor training Homologous protein search \nDHR [64] Jul 2024 2 ESM-1b (650M parameters \nper encoder) [19] \nTransformer-\nbased Protein sequences UniRef [9], \nSCOPe [20, 62] ~2 M Protein homolog detection \nAntibody \nLarge \nLanguage \nModels \n \nMHCRoBER\nTa [65] Dec 2021 Model with 12multi-heads \nand 5 self-attention layers. \nROBERTa-\nbased Protein sequences UniProtKB [66] \n \n565,254 protein \nsequences \nPredicting the binding of peptide \nand major histocompatibility \ncomplex (MHC) \nImmune Epitope \nDatabase (IEDB) \n[67] \nMHCclass I \ntransmembrane \nproteins containing \nHLA-A(1,777 \nsequences)ÔºåHLA-B \n(2,100 sequences) and \nHLA-C(1,931 \nsequences) \nBERTMHC [68] Jun 2021 \nThe model has 12 layers with \n12 self-attention heads in \neach layer \nBERT-based Protein sequences \nThe data from \nKamilla \nKj√¶rgaard Jensen \n[69] \n \n2,413additional MHC‚Äì\npeptide pairs covering \n47 MHC class II \nalleles. \nPredicting precisely the binding \nand presentation of peptides to \nmajor histocompatibility \ncomplex (MHC) alleles Immune Epitope \nDatabase (IEDB) \n[67] \n95,638 peptides \nTCR-BERT [70] \nBioRxiv \nposted Nov \n2021 \n12 stacked transformer \nblocks with 8 attention \nheads, utilizing a hidden \nrepresentation \ndimensionality of 768 and \nfeaturing 12 transformer \nlayers. \nBERT-based Protein sequences \nPan immune \nrepertoire \ndatabase (PIRD) \n[71] \n47,040 TRB sequences \nand 4,607 TRA \nsequences. \nAntigen specificity classification VDJdb [72] \n58,795 human TCRs \nand 3,353 mouse \nTCRs. \nTCRdb [73] \n139,00,913 TRB \nsequences of unknown \nantigen binding \naffinity. \nAntiformer [74] July 2024 \nhe transformer encoder \ninvolves 12 stacking layers \nof transformer with the \nmulti-head self-attention and \nfeed forward network. \nBERT-based Protein sequences \nGene expression \nThe OAS \ndatabase[75] \n55 BCR-seq datasets \ncontaining 600 million \nsequences \nBinding specificity prediction \nSC-AIR-\nBERT [76] May 2023 \nSix standard transformer \nlayers and each layer has \nfour attention heads, the \nhidden representation \ndimensionality is 512 and the \ninter mediate representation \ndimensionality is 2048 \nBERT-based Protein sequences \nVDJdb [72] 23,358 unique paired \nTCRs \nBinding specificity prediction \nImmune Epitope \nDatabase (IEDB) \n[67] \n18,662 paired TCR \nsand 589 paired BCRs \nhuARdb [77] \n612,077 high-\nconfidence paired full-\nlength Œ±/Œ≤ chains of \nTCR sequences \nCoV-AbDab [78] \n1,105,906 paired \nantibody heavy/light \nchains \nAbLang [79] Jun 2022 \nConsists of three modules \n(Each of AbRep‚Äôs 12 \ntransformer blocks has 12 \nattenuated heads, an inner \nhidden size of 3072 and a \nhidden size of 768. From \nAbRep, the rescodings (768 \nvalues for each residue) are \nobtained. AbHead follows \nthe design of RoBERTa‚Äôs[80] \nhead model, with a hidden \nsize of 768) \nBERT-based Human antibody \nsequences \nObserved \nAntibody Space \n(OAS) database \n[81] \nTraining sets of 14,126 \n724 heavy and 187,068 \nlight sequences, and \ntwo evaluation sets of \n100,000 heavy and \n50,000 light sequences Sequence specific predictions, \nresidue specific predictions, \namino acid predictions \nData from [82] \n10,000 naive and \n10,000 memory B-cell \nsequences \nAntiBERTa  Jul 2022 \n86M parameters. A 12-layer \ntransformer model. Attention \nheads is 12, embedding \ndimension is 768, \nfeedforward layer dimension \nis 3072 \nBERT-based Human antibody \nsequences \nSAbDab [83] \nTraining/validation/test \nsplit of 720/90/90 \n Trace the B cell origin of the \nantibody, quantify \nimmunogenicity, predict the \nantibody‚Äôs binding site \nBCR repertoire \ndataset [82] - \nTheraSAbDab \n[84] \n191 non-redundant \ntherapeutic antibodies \nEATLM [85] Jan 2023 \n86M parameters (12 layers, \n12 heads, and 768 hidden \nstates) \nTransformer-\nbased \nHuman antibody \nsequences \nDataset from [86] \nTraining/validation/test \nsplit of \n15,128/3,242/3,242 \nAccurate antigen-binding \npredictionÔºå paratope \npredictionÔºå B cell analysisÔºå \nantibody discovery \nParatope data \nfrom [87]  \n1,662 CDR segments \non 277 antibodies \nData from [88] 88,094 sequences with \n6 maturation stages \nA subset of the \nOAS database \n[81] \nAntibody sequences \nfrom 133 SARS-CoV-2 \npatients and 87 health \npersons \n \nReferences \n1. Rao, R.M., et al. MSA transformer. in International Conference on Machine Learning. 2021. \n2. Haas, J., et al., Continuous Automated Model EvaluatiOn (CAMEO) complementing the critical assessment of structure prediction in \nCASP12. Proteins: Structure, Function, and Bioinformatics, 2018. 86: p. 387-398. \n3. Shrestha, R., et al., Assessing the accuracy of contact predictions in CASP13.  Proteins: Structure, Function, and Bioinformatics, 2019. \n87(12): p. 1058-1068. \n4. Yang, J., et al., Improved protein structure prediction using predicted interresidue orientations. Proceedings of the National Academy of \nSciences, 2020. 117(3): p. 1496-1503. \n5. Cuff, J.A. and G.J. Barton, Evaluation and improvement of multiple sequence methods for protein secondary structure prediction. Proteins: \nStructure, Function, and Bioinformatics, 1999. 34(4): p. 508-519. \n6. Klausen, M.S., et al., NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning.  Proteins: Structure, \nFunction, and Bioinformatics, 2019. 87(6): p. 520-527. \n7. Berman, H.M., et al., The protein data bank. Nucleic acids research, 2000. 28(1): p. 235-242. \n8. Alley, E.C., et al., Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 2019. 16(12): \np. 1315-1322. \n9. Suzek, B.E., et al., UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, \n2015. 31(6): p. 926-932. \n10. Rocklin, G.J., et al., Global analysis of protein folding using massively parallel design, synthesis, and testing.  Science, 2017. 357(6347): \np. 168-175. \n11. Gray, V .E., et al., Quantitative missense variant effect prediction using large-scale mutagenesis data. Cell systems, 2018. 6(1): p. 116-124. \n12. Sarkisyan, K.S., et al., Local fitness landscape of the green fluorescent protein. Nature, 2016. 533(7603): p. 397-401. \n13. Rao, R., et al., Evaluating protein transfer learning with TAPE. Advances in neural information processing systems, 2019. 32. \n14. AlQuraishi, M., ProteinNet: a standardized data set for machine learning of protein structure. BMC bioinformatics, 2019. 20: p. 1-10. \n15. Hou, J., B. Adhikari, and J. Cheng, DeepSF: deep convolutional neural network for mapping protein sequences to folds.  Bioinformatics, \n2018. 34(8): p. 1295-1303. \n16. Murzin, A.G., et al., SCOP: a structural classification of proteins database for the investigation of sequences and structures.  Journal of \nmolecular biology, 1995. 247(4): p. 536-540. \n17. Kinch, L.N., et al., CASP9 target classification. PROTEINS: structure, function, and bioinformatics, 2011. 79(S10): p. 21-36. \n18. Kinch, L.N., et al., CASP 11 target classification. Proteins: Structure, Function, and Bioinformatics, 2016. 84: p. 20-33. \n19. Rives, A., et al., Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings \nof the National Academy of Sciences, 2021. 118(15): p. e2016239118. \n20. Fox, N.K., S.E. Brenner, and J. -M. Chandonia, SCOPe: Structural Classification of Proteins ‚Äîextended, integrating SCOP and ASTRAL \ndata and classification of new structures. Nucleic acids research, 2014. 42(D1): p. D304-D309. \n21. Moult, J., et al., Critical assessment of methods of protein structure prediction: Progress and new directions in round XI. Proteins: Structure, \nFunction, and Bioinformatics, 2016. 84: p. 4-14. \n22. Riesselman, A.J., J.B. Ingraham, and D.S. Marks, Deep generative models of genetic variation capture the effects of mutations.  Nature \nmethods, 2018. 15(10): p. 816-822. \n23. Elnaggar, A., et al., ProtTrans: Towards Cracking the Language of Lifes Code Through Self -Supervised Deep Learning and High \nPerformance Computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021: p. 1-1. \n24. Abriata, L.A., et al., Assessment of hard target modeling in CASP12 reveals an emerging role of alignment -based contact prediction \nmethods. Proteins: Structure, Function, and Bioinformatics, 2018. 86: p. 97-112. \n25. Almagro Armenteros, J.J., et al., DeepLoc: prediction of protein subcellular localization using deep learning. Bioinformatics, 2017. 33(21): \np. 3387-3395. \n26. Chandonia, J.-M., N.K. Fox, and S.E. Brenner, SCOPe: classification of large macromolecular structures in the structural classification \nof proteins‚Äîextended database. Nucleic acids research, 2019. 47(D1): p. D475-D481. \n27. Wu, L., et al., SPRoBERTa: protein embedding learning with local fragment modeling. Briefings in Bioinformatics, 2022. 23(6): p. bbac401. \n28. Gligorijevi, V ., et al., Structure-based protein function prediction using graph convolutional networks. Nature communications, 2021. 12(1): \np. 3168. \n29. Wang, Z., et al. Multi-level Protein Structure Pre-training via Prompt Learning . in The Eleventh International Conference on Learning \nRepresentations. 2022. \n30. Dallago, C., et al., FLIP: Benchmark tasks in fitness landscape inference for proteins. bioRxiv, 2021: p. 2021-11. \n31. Madani, A., et al., Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497, 2020. \n32. Leinonen, R., et al., UniProt archive. Bioinformatics, 2004. 20(17): p. 3236-3237. \n33. Bairoch, A., et al., The universal protein resource (UniProt). Nucleic acids research, 2005. 33(suppl\\_1): p. D154-D159. \n34. Bairoch, A., et al., Swiss-Prot: juggling between evolution and stability. Briefings in bioinformatics, 2004. 5(1): p. 39-55. \n35. Boeckmann, B., et al., The SWISS-PROT protein knowledgebase and its supplement TrEMBL in 2003. Nucleic acids research, 2003. 31(1): \np. 365-370. \n36. Bateman, A., et al., The Pfam protein families database. Nucleic acids research, 2004. 32(suppl\\_1): p. D138-D141. \n37. Federhen, S., The NCBI taxonomy database. Nucleic acids research, 2012. 40(D1): p. D136-D143. \n38. Notin, P., et al. Tranception: protein fitness prediction with autoregressive transformers and inference -time retrieval. in International \nConference on Machine Learning. 2022. \n39. Ferruz, N., S. Schmidt, and B. Hcker, ProtGPT2 is a deep unsupervised language model for protein design. Nature communications, 2022. \n13(1): p. 4348. \n40. UniProt: the universal protein knowledgebase in 2021. Nucleic acids research, 2021. 49(D1): p. D480-D489. \n41. Brandes, N., et al., ProteinBERT: a universal deep-learning model of protein sequence and function. Bioinformatics, 2022. 38(8): p. 2102-\n2110. \n42. Moult, J., et al., Critical assessment of methods of protein structure prediction (CASP) ‚ÄîRound XII. Proteins: Structure, Function, and \nBioinformatics, 2018. 86: p. 7-15. \n43. Andreeva, A., et al., SCOP2 prototype: a new approach to protein structure mining. Nucleic acids research, 2014. 42(D1): p. D310-D314. \n44. Andreeva, A., et al., The SCOP database in 2020: expanded classification of representative family and superfamily domains of known \nprotein structures. Nucleic acids research, 2020. 48(D1): p. D376-D382. \n45. Armenteros, J.J.A., et al., SignalP 5.0 improves signal peptide predictions using deep neural networks. Nature biotechnology, 2019. 37: p. \n420-423. \n46. Hornbeck, P.V ., et al., PhosphoSitePlus, 2014: mutations, PTMs and recalibrations. Nucleic acids research, 2015. 43(D1): p. D512-D520. \n47. Ofer, D. and M. Linial, ProFET: Feature engineering captures high-level protein functions. Bioinformatics, 2015. 31(21): p. 3429-3436. \n48. Brandes, N., D. Ofer, and M. Linial, ASAP: a machine learning framework for local protein properties. Database, 2016. 2016: p. baw133. \n49. Xu, M., et al. Protst: Multi-modality learning of protein sequences and biomedical texts. in International Conference on Machine Learning. \n2023. PMLR. \n50. Bairoch, A. and R. Apweiler, The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000.  Nucleic acids research, \n2000. 28(1): p. 45-48. \n51. Xu, M., et al., Protst: Multi-modality learning of protein sequences and biomedical texts. arXiv preprint arXiv:2301.12040, 2023. \n52. Zhou, H.-Y ., et al., Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling. bioRxiv, 2023: p. 2023-01. \n53. Zhang, N., et al., Ontoprotein: Protein pretraining with gene ontology embedding. arXiv preprint arXiv:2201.11147, 2022. \n54. Outeiral, C. and C.M. Deane, Codon language embeddings provide strong signals for use in protein engineering.  Nature Machine \nIntelligence, 2024. 6(2): p. 170-179. \n55. Jarzab, A., et al., Meltome atlas‚Äîthermal proteome stability across the tree of life. Nature methods, 2020. 17(5): p. 495-503. \n56. Thumuluri, V ., et al., DeepLoc 2.0: multi-label subcellular localization prediction using protein language models. Nucleic acids research, \n2022. 50(W1): p. W228-W234. \n57. Sridharan, S., et al., Proteome-wide solubility and thermal stability profiling reveals distinct regulatory roles for ATP .  Nature \ncommunications, 2019. 10(1): p. 1155. \n58. Unsal, S., et al., Learning functional properties of proteins with language models. Nature Machine Intelligence, 2022. 4(3): p. 227-245. \n59. Uhln, M., et al., Tissue-based map of the human proteome. Science, 2015. 347(6220): p. 1260419. \n60. Wang, M., et al., PaxDb, a database of protein abundance averages across all three domains of life.  Molecular \\& cellular proteomics, \n2012. 11(8): p. 492-500. \n61. Liu, W., et al., PLMSearch: Protein language model powers accurate and fast sequence search for remote homology.  Nature \ncommunications, 2024. 15(1): p. 2775. \n62. Chandonia, J. -M., et al., SCOPe: improvements to the structural classification of proteins --extended database to facilitate variant \ninterpretation and machine learning. Nucleic acids research, 2022. 50(D1): p. D553-D559. \n63. Sillitoe, I., et al., CATH: increased structural coverage of functional space. Nucleic acids research, 2021. 49(D1): p. D266-D273. \n64. Hong, L., et al., Fast, sensitive detection of protein homologs using deep dense retrieval. Nature Biotechnology, 2024: p. 1-13. \n65. Wang, F., et al., MHCRoBERTa: pan-specific peptide-MHC class I binding prediction through transfer learning with label-agnostic protein \nsequences. Brief Bioinform, 2022. 23(3). \n66. Boutet, E., et al., UniProtKB/Swiss-Prot: the manually annotated section of the UniProt KnowledgeBase, in Plant bioinformatics: methods \nand protocols. 2007, Springer. p. 89-112. \n67. Vita, R., et al., The Immune Epitope Database (IEDB): 2018 update. Nucleic Acids Res, 2019. 47(D1): p. D339-D343. \n68. Cheng, J., et al., BERTMHC: improved MHC ‚Äìpeptide class II interaction prediction with transformer and multiple instance learning.  \nBioinformatics, 2021. 37(22): p. 4172-4179. \n69. Improved methods for predicting peptide binding affinity to MHCclass II molecules. 2017. \n70. Wu, K., et al., TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding analyses. 2021. \n71. Zhang, W., et al., PIRD: Pan Immune Repertoire Database. Bioinformatics, 2020. 36(3): p. 897-903. \n72. Bagaev, D.V ., et al., VDJdb in 2019: database extension, new analysis infrastructure and a T -cell receptor motif compendium.  Nucleic \nAcids Research, 2020. 48(D1): p. D1057-D1062. \n73. Chen, S.Y ., et al., TCRdb: a comprehensive database for T-cell receptor sequences with powerful search function. Nucleic Acids Res, 2021. \n49(D1): p. D468-D474. \n74. Wang, Q., et al., AntiFormer: graph enhanced large language model for binding affinity prediction.  Briefings in Bioinformatics, 2024. \n25(5). \n75. Olsen, T.H., F. Boyles, and C.M.J.P.S. Deane, Observed Antibody Space: A diverse database of cleaned, annotated, and translated unpaired \nand paired antibody sequences. 2022. 31(1): p. 141-146. \n76. Zhao, Y ., et al., SC-AIR-BERT: a pre-trained single-cell model for predicting the antigen-binding specificity of the adaptive immune receptor. \nBrief Bioinform, 2023. 24(4). \n77. Wu, L., et al., huARdb: human Antigen Receptor database for interactive clonotype-transcriptome analysis at the single-cell level. Nucleic \nAcids Res, 2022. 50(D1): p. D1244-D1254. \n78. Raybould, M.I.J., et al., CoV-AbDab: the coronavirus antibody database. Bioinformatics, 2021. 37(5): p. 734-735. \n79. Olsen, T.H., I.H. Moal, and C.M. Deane, AbLang: an antibody language model for completing antibody sequences.  Bioinformatics \nAdvances, 2022. 2(1): p. vbac046. \n80. Liu, Y ., et al., Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. \n81. Kovaltsuk, A., et al., Observed antibody space: a resource for data mining next-generation sequencing of antibody repertoires. The Journal \nof Immunology, 2018. 201(8): p. 2502-2509. \n82. Ghraichy, M., et al., Different B cell subpopulations show distinct patterns in their IgH repertoire metrics. Elife, 2021. 10: p. e73111. \n83. Dunbar, J., et al., SAbDab: the structural antibody database. Nucleic acids research, 2014. 42(D1): p. D1140-D1146. \n84. Marks, C., et al., Humanization of antibodies using a machine learning approach on large -scale repertoire data. Bioinformatics, 2021. \n37(22): p. 4041-4047. \n85. Wang, D., F. Ye, and H. Zhou, On pre-trained language models for antibody. bioRxiv, 2023: p. 2023-01. \n86. Mason, D.M., et al., Optimization of therapeutic antibodies by predicting antigen specificity from antibody sequence via deep learning.  \nNature Biomedical Engineering, 2021. 5(6): p. 600-612. \n87. Liberis, E., et al., Parapred: antibody paratope prediction using convolutional and recurrent neural networks. Bioinformatics, 2018. 34(17): \np. 2944-2950. \n88. Mroczek, E.S., et al., Differences in the composition of the human antibody repertoire by B cell subsets in the blood.  Frontiers in \nimmunology, 2014. 5: p. 96. \n \nSupplementary Table 3. Detailed information of large language models for drug-discovery tasks \nApplication \narea Models Ref Publication \ntime Parameters Architecture \n \nDatasets Downstream \ntasks Data type Source Size \nPredictions \nof \nMolecular \nProperties \nSMILES-\nBERT [1] Sep 2019 \n6 Transformer encoder layers, the feed-\nforward hidden units are 1024 and the \nattention heads are 4 \nBERT-based SMILES ZINC [2] 18.69 M used, totally \nmore than 35 M \nLogP prediction, PM2 prediction, \nand molecular property prediction \nChemBERTa [3] Oct 2020 \nImplementation of RoBERTa uses 12 \nattention heads and 6 layers, resulting in 72 \ndistinct attention mechanisms. \nBERT-based SMILES PubChem \n[4] \nCurated a dataset of 77 \nM unique SMILES from \nPubChem, divided this \ndataset into subsets of \n100 K, 250 K, 1 M, and \n10 M. \nBinary classification prediction of \nbarrier permeability properties, \nbinary classification of clinical \ntrial toxicity, whether the \ncompound inhibits HIV \nreplication for binary \nclassification between active and \ninactive \nChemBERTa-\n2 [5] Sep 2022 \nImplementation of RoBERTa uses 12 \nattention heads and 6 layers, resulting in 72 \ndistinct attention mechanisms. \nBERT-based SMILES PubChem  \nOver a large corpus of \n77 million SMILES \nstrings \nbrain \npenetrability, toxicity, solubility, \nand on-target inhibition \nK-BERT [6] Apr 2022 \nThe hidden size of the transformer encoder \nis 768, and the number of the attention \nheads is 12. Six transformer encoders were \nused in KBERT. \nBERT-based SMILES ChEMBL \n[7] 1.8 M used \nRelated tasks on 15 drug-\ndiscovery-related datasets \nincluding carcinogenicity, \nrespiratory toxicity and drug \ninduced liver injury. \nMOLE-\nBERT [8] Apr 2023 A 5-layer Graph Isomorphism Networks \n(GINs) whose hidden dimension is 300 \nBERT-style, \ngraph-based \nMolecular \ngraphs ZINC15 [9] \n2 million molecules \nsampled from the \ndataset \nRelated tasks on 8 drug-\ndiscovery-related datasets \nincluding barrier permeability \nproperties and clinical trial \ntoxicity \nGeneration \nof \nMolecules \nMolGPT [10] Oct 2021 \n6 M parameters.  Each self -attention layer \nreturns a vector of size 256 that is taken as \ninput by the fully connected network. The \nhidden layer of the neural network outputs \na vector of size 1024 and passes it through \nGELU activation layer. The final layer of \nthe fully connected neural network returns \na vector of size 256, that is then used as \ninput for the next decoder block. MolGPT \nconsists of eight such decoder blocks \nGPT-based SMILES \nMOSES \n[11] and \nGuacaMol \n[12] \n1.9 M, 1.6 M Generating molecules \nDrug-Target \nInteraction \nDTI-BERT [13] Jun 2022 \nThe proteins can be represented via 1024-\nD vectors (dimensionality of the features \nextracted by the ProtBert model). Drug \nmolecular fingerprints are represented by \n128-D vectors through semi decomposition \nprocess discrete wavelet transform (DWT). \nSecondly, the 1152-D vectors (a \nconcatenation of protein sequence feature \nand drug feature) are fed into the feature \nextraction model to generate interaction \ninformation \nBERT-based \nMolecular \nfingerprints \nand protein \nsequence \npairs \nDrugBanks \n[14], \nBRENDA, \nSuperTarget, \nand KEGG \nBRITE [15] \n4,803 drug-target pairs \nin positive subsets 9,606 \nsynthesized negative \npairs \nA pair belongs to an interactive \ndrug-target pair or non-interactive \ndrug-target pair \nTransDTI [16] Jan 2022 Consist of SMILES-BERT and fine-tuned \nlarge protein models. BERT-based \nMolecular \nSMILES \nand protein \nsequence \npairs \nKIBA [17] ,  \ngold-\nstandard \nexternal \ndata sets \nfrom DTI-\nMLCD [18] \n30, 474 compounds, 961 \ntargets and 61, 624 \ninteractions \nA three-classification based on \nbinding affinity \nC2P2 [19] Jul 2022 Consist of ChemBERTa and ESM model BERT-based \nMolecular \nSMILES \nand protein \nsequence \npairs \nSTRING \n[20],  \nSTITCH \n[21],  \nDavis [22], \nand  \nPDBBind \nv2019 [23, \n24] \nOver 67.6 million \nproteins with over 20 \nbillion protein‚Äìprotein \npairs, over 0.5 million \nchemicals with over 1.6 \nbillion interactions, \n30,056 interactions, \n14,011 interactions \nBinding affinity prediction \nHyeunseok \nKang et al. [25] Aug 2022 Consist of ChemBERTa and ProtBERT \nmodel BERT-based \nMolecular \nSMILES \nand protein \nsequence \npairs \nBIOSNAP \n[26], DA VIS \n[22] and \nBindingDB \n[27] \n27,482 interactions, \n11,103 interactions, \n32,601 interactions \nBinding affinity prediction \n \n  \nReferences \n1. Wang, S., et al. Smiles-bert: large scale unsupervised pre -training for molecular property prediction . in Proceedings of the 10th ACM international conference on \nbioinformatics, computational biology and health informatics. 2019. \n2. Irwin, J.J., et al., ZINC: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 2012. 52(7): p. 1757-1768. \n3. Chithrananda, S., G. Grand, and B. Ramsundar, ChemBERTa: large -scale self -supervised pretraining for molecular property prediction.  arXiv preprint \narXiv:2010.09885, 2020. \n4. Kim, S., et al., PubChem 2019 update: improved access to chemical data. Nucleic acids research, 2019. 47(D1): p. D1102-D1109. \n5. ChemBERTa-2: Towards Chemical Foundation Models. \n6. Wu, Z., et al., Knowledge-based BERT: a method to extract molecular features like computational chemists. Briefings in Bioinformatics, 2022. 23(3): p. bbac131. \n7. Mendez, D., et al., ChEMBL: towards direct deposition of bioassay data. Nucleic acids research, 2019. 47(D1): p. D930-D940. \n8. Xia, J., et al. Mole-bert: Rethinking pre-training graph neural networks for molecules. in The Eleventh International Conference on Learning Representations. 2022. \n9. Sterling, T. and J.J. Irwin, ZINC 15--ligand discovery for everyone. Journal of chemical information and modeling, 2015. 55(11): p. 2324-2337. \n10. Bagal, V ., et al., MolGPT: molecular generation using a transformer-decoder model. Journal of Chemical Information and Modeling, 2021. 62(9): p. 2064-2076. \n11. Polykovskiy, D., et al., Molecular sets (MOSES): a benchmarking platform for molecular generation models.  Frontiers in pharmacology, 2020. 11: p. 565644. \n12. Brown, N., et al., GuacaMol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 2019. 59(3): p. 1096-1108. \n13. Zheng, J., X. Xiao, and W. -R. Qiu, DTI-BERT: identifying drug-target interactions in cellular networking based on BERT and deep learning method.  Frontiers in \nGenetics, 2022. 13: p. 859188. \n14. Wishart, D.S., et al., DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic acids research, 2018. 46(D1): p. D1074-D1082. \n15. Hu, J., et al., GPCR--drug interactions prediction using random forest with drug -association-matrix-based post-processing procedure. Computational biology and \nchemistry, 2016. 60: p. 59-71. \n16. Kalakoti, Y ., S. Yadav, and D. Sundar, TransDTI: transformer-based language models for estimating DTIs and building a drug recommendation workflow. ACS omega, \n2022. 7(3): p. 2706-2717. \n17. Tang, J., et al., Making sense of large -scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis.  Journal of Chemical Information and \nModeling, 2014. 54(3): p. 735-743. \n18. Chu, Y ., et al., DTI-MLCD: predicting drug-target interactions using multi-label learning with community detection method. Briefings in bioinformatics, 2021. 22(3): \np. bbaa205. \n19. Nguyen, T.M., T. Nguyen, and T. Tran, Mitigating cold -start problems in drug -target affinity prediction with interaction knowledge transferring.  Briefings in \nBioinformatics, 2022. 23(4): p. bbac269. \n20. Szklarczyk, D., et al., The STRING database in 2021: customizable protein --protein networks, and functional characterization of user -uploaded gene/measurement \nsets. Nucleic acids research, 2021. 49(D1): p. D605-D612. \n21. Kuhn, M., et al., STITCH: interaction networks of chemicals and proteins. Nucleic acids research, 2007. 36(suppl\\_1): p. D684-D688. \n22. Davis, M.I., et al., Comprehensive analysis of kinase inhibitor selectivity. Nature Biotechnology, 2011. 29(11): p. 1046-1051. \n23. Wang, R., et al., The PDBbind database: Collection of binding affinities for protein- ligand complexes with known three-dimensional structures. Journal of medicinal \nchemistry, 2004. 47(12): p. 2977-2980. \n24. Wang, R., et al., The PDBbind database: methodologies and updates. Journal of medicinal chemistry, 2005. 48(12): p. 4111-4119. \n25. Kang, H., et al., Fine-tuning of bert model to accurately predict drug--target interactions. Pharmaceutics, 2022. 14(8): p. 1710. \n26. Zitnik, M., R. Sosic, and J. Leskovec, BioSNAP Datasets: Stanford biomedical network dataset collection. Note: http://snap. stanford. edu/biodata Cited by, 2018. 5(1). \n27. Liu, T., et al., BindingDB: a web-accessible database of experimentally determined protein --ligand binding affinities. Nucleic acids research, 2007. 35(suppl\\_1): p. \nD198-D201. \n \nSupplementary Table 4. Detailed information of large language models for single-cell tasks \nApplicati\non area Models Ref Publicat\nion time Parameters Architecture \nFine-tuning datasets \nDownstream tasks Data type Source Size \nSingle-\ncell large \nlanguage \nmodels \nscBERT [1] Sep \n2022 \nsix Performer encoder \nlayers and ten heads for \neach layer, 200 dimensions \nof gene embedding using \ngene2vec \nBERT-based scRNA-seq \nThe Panglao dataset [2] \n209 human single-cell datasets \ncomprising 74 tissues with \n1,126,580 cells \nCell type annotation, \nnovel cell type \ndiscovery, robustness \nto batch effects and \nmodel interpretability \nZheng68k dataset [3] 68,450 cells \nPancreas datasets [4-7] - \nMacParland dataset [8] 8,444 cells \nHeart datasets [9, 10] \n451,513 cells for pretraining and \nthe 287,269 cells for \nbenchmarking \nLung dataset [11] 39,778 cells \nHuman Cell Atlas dataset [12] 84,363 cells from 27 cell types \namong 15 major organs \nscGPT [13] Feb \n2024 \n12 stacked transformer \nblocks with 8 attention \nheads, 512 embedding sizes \nof the pre-trianed \nfoundation model, 512 \nhidden sizes of the fully \nconnected layer \nGPT-based \nscRNA-seq \nCELLxGENE scRNA-seq \nhuman PBMC Collection [14] \n33 million human PBMC \nscRNA-seq samples \nGene network \ninference \nPBMC 10K dataset [15] Two scRNA-seq data of 7,982 \ncells and 4,008 cells. Multi-batch \nintegration Immune Human dataset [16] 33,506 cells \nhPancreas dataset [4-7, 17, 18] 10,600 cells in the reference set \nand 4,218 cells in the query set Cell type annotation \nAdamson dataset [19] \n87 unique one-gene \nperturbations, each replicated in \naround 100 cells Genetic perturbation \nprediction \nNorman dataset [20] \n131 two-gene perturbations and \n105 one-gene perturbations. \nEach perturbation is replicated \nin around 300-700 cells. \nSingle-cell \nmulti-omics \n10X Multiome PBMC [21] 9,631 cells \nMulti-omic \nintegration ASAP PBMC [22] \nFour datasets each contain \n5,023, 3,666, 3,517, and 4,849 \ncells respectively \nCIForm [23] July \n2023 64 attention heads BERT-based scRNA-seq \nPancreas datasets [4-7] - \nCell type annotation \nImmune datasets [24-26] - \nBrain datasets [27-29] - \nTabula Muris dataset [30] Nearly 100000 cells from 20 \norgans and tissues \nZheng68k dataset [3] 68,450 cells \nZhangTdataset [31] 8,530 cells from 20 subtypes \nAllen mouse brain dataset [32] 12832cells \nTOSICA [17] Jan \n2023 - BERT-based scRNA-seq \nhuman pancreas (hPancreas) \n[4-7] \n10,600 cells for training and \n4,218 for testing \nCell type annotation, \nnew cell type \ndiscovery and batch \ncorrection \nhuman bone (hBone, \nGSE152805) [33] \n14,615 cells for training and \n11,525 for testing \nhuman artery (hArtery, \nGSE159677) [34] \n10,960 cells for training and \n35,399 for testing \nmouse brain (mBrain) [28-30, \n35] \n48,801 cells for training and \n7,394 for testing \nmouse pancreas (mPancreas, \nGSE132188) [36] \n25,465 cells for training and \n10,886 for testing \nmouse \natlas (mAtlas, GSE132042) \n[37] \n78,672 cells for training and \n277,541 for testing \nscTransSort [38] Mar \n2023 12 layers of transformer BERT-based scRNA-seq \nhuman cell atlas dataset 295,805 cells from 35 tissues \nCell type annotation mouse cell atlas dataset \n105,148 cells from 26 tissues \nand 103,148 cells from 26 \ntissues \ndata processed by Shao X et al. \n[39] - \nTransCluster [40] Oct \n2022 5 attention heads \nimproved \nTransformer \nmodel \nscRNA-seq \nHuman training data [41] - Cell type \nIdentification The Shao dataset [39] - \nThe Baron dataset [4] - \nGeneformer [42] May \n2023 \nsix transformer encoder \nunits, input size of 2,048, \n256 embedding dimensions, \nfour attention heads per \nlayer and feed forward size \nof 512 \nBERT-based scRNA-seq \niPSC differentiation data \nAssayed in parallel on the Drop-\nseq (single cell) or DroNc-seq \n(single nucleus) platform  \nBatch effect removal, \ncell type annotation \nHuggingface Dataset [43] \na largescale pretraining corpus, \nGenecorpus-30M, comprising \n29.9 million human single-cell \ntranscriptomes \ndiscover key network \nregulators and \ncandidate therapeutic \ntargets \ntGPT [44] May \n2023 \n8 transformer decoder \nblocks with 1024 hidden \nunits and 16 attention heads \nGPT-based scRNA-seq \nHuman Cell Atlas Census of \nImmune Cells (HCA) [45] \n282,588 Bone marrow cells \nfrom 64 healthy donors in \nHuman Cell Atlas (HCA) \nproject \nsingle-cell clustering, \ninference of \ndevelopmental \nlineage \nHuman cell Landscape \n(HCL) [41] 586,135 human cells \nTabula Mursi dataset [30] 54,865 cells \nMacaque Retina dataset [46] 124,965 cells \nBulk RNA-\nseq \nThe Cancer Genome Atlas \n(TCGA) [47] 9,318 bulk samples interrogation of \nfeature representation \nof bulk tissues in \nrelation to genomic \nalterations, prognosis \nand treatment \nresponse of \nimmunotherapy \nGenotype-Tissue Expression \nProject (GTEx) [48] 11,688 bulk samples \nDeepMAPS [49] Feb \n2023 - \nBERT-based, \ngraph \ntransformer \nscRNA-seq Multiple scRNA-seq data [4, \n16] \nThree scRNA-seq datasets with \n20,125 cells, 14,878 cells and \n16,382 cells Cell clustering, infer \ncell-type-specific \nbiological networks \nfrom scMulti-omics \ndata \nSingle-cell \nmulti-omics \nCITE-seq data [50] \nThree CITE-seq datasets with \n25,171 cells, 32,029 cells and \n16,750 cells \nscRNA-ATAC-seq data \nFour scRNA-ATAC-seq \ndatasets with 3,009 cells, 11,898 \ncells, 3,233 cells and 10,970 \ncells \nscMVP [51] Jan \n2022 \n8 self-attention heads and \neach head takes 16-\ndimension feature \nTransformer-\nbased \nPaired \nscRNA-seq \nand scATAC-\nseq data \nsci-CAR cell line dataset [52] \n293T cell line, 3T3 cell line, \n293T/3T3 cell mixture, and \nA549 cell line treated with \ndexamethasone (DEX) for 0 h, 1 \nh, and 3 h \ndimensionality \nreduction, cell \nclustering, and \ndevelopmental \ntrajectory inference \nand generate separate \nimputations for \ndifferential analysis \nand cis-regulatory \nelement identification \nPaired-seq cell line dataset [53] \nderived from HEK293, HepG2, \nand their cell \nline mixture \nSNARE-seq cell line dataset \n[54] 5,081 cells \nSHARE-seq [55] 67,418 cells \nscTranslator [56] \nBioRxiv \nposted \nJuly \n2023 \n8-headed attention \nmechanism in each sub-\nlayer, 117 \nmillion parameters. \nTransformer-\nbased \nBulk datasets \nThe Cancer Genome Atlas \n(TCGA) data [57-60] \n31 cancer types and 18,227 \nsamples in total \ntranslate single-cell \ntranscriptome to \nproteome, predict \nprotein abundance \nThe data from Clinical \nProteomic Tumor Analysis \nConsortium (CPTAC) [61-67] \nThe dataset from Broad \nInstitute [68, 69] \nThe dataset from Memorial \nSloan Kettering Cancer Center \n(MSKCC) [70] \nSingle-cell The Seurat v4 PBMCs dataset 161,764 human peripheral blood \ndatasets [71] mononuclear cells \nThe REAP-seq PBMCs dataset \n[72] \n4,330 PBMCs with \nsimultaneous measurements of \n44 proteins and 21,005 \ntranscriptome genes \nThe CITE-seq CBMCs dataset \n[73] \nsimultaneous measurements of \n13 cellular proteins and 16,508 \ntranscriptome genes. This \ndataset includes 8,005 cord \nblood mononuclear cells \n(CBMCs) \nThe single-cell pan-cancer \ndataset [74] \n65,698 myeloid cells with only \nsingle-cell transcriptome data, \ninvolving 15,844 genes \nscFoundation [75] June \n2024 100 million parameters BERT-based Single-cell \ndatasets \nBaron dataset [4] - gene expression \nenhancement, tissue \ndrug response \nprediction, cell \nclustering, single-cell \ndrug response \nclassification, and \nsingle-cell \nperturbation \nprediction \nZheng68K dataset [3] 68,450 cells \nCancer drug response dataset \n[76] - \nSingle cell drug response \nclassification dataset - \nPerturbation dataset [77] - \nscMoFormer [78] Oct \n2023 Graph transformer Transformer-\nbased \nSingle-cell \nmulti-omics \nJoint measurements of gene \nexpression and surface protein \nlevels datasets from the \nNeurIPS multimodal single-\ncell integration competition of \nthe year 2021 [25] and 2022 \n- \nuse gene expression \n(RNA) to predict \nsurface protein \nlevel, protein levels to \ngene expression, gene \nexpression to \nchromatin \naccessibility and \nchromatin \naccessibility to gene \nexpression \nGeneCompas\ns [79] Sep \n2024 \n12-layer transformer \nframework, 100 million \nparameters \nBERT-based single-cell \ntranscriptomes \nCHIP-Atlas related to PBMC \ncells on GSE43036 [80] - \nGene embedding \nanalysis, Gene \nexpression profiling \nprediction \nhuman multiple sclerosis - cell type annotation  \n(hMS), lung (hLung) and liver \n(hLiver) datasets, and mouse \nbrain (mBrain), lung (mLung) \nand pancreas (mPancreas) \ndatasets. \nImmune Human [16] 33,506 cells GRN inference  \ndataset provided by Srivatsan \net al. [81] - Drug dose response \nprediction  \npredefined dosage-sensitive \nand nonsensitive gene datasets \n[42] \n- \nGene dosage \nsensitivity \npredictions.  \nNorman dataset [20] \n131 two-gene perturbations and \n105 one-gene perturbations. \nEach perturbation is replicated \nin around 300-700 cells. \nIn silico perturbation \nmouse embryonic stem cells \n(ESCs) [82] - In silico quantitative \nperturbation  \nscMulan [83] \nBioRxiv \nposted \nJan \n2024 \n24-layer transformer, 368 \nmillion parameters GPT-based \nsingle-cell \ntranscriptomic \ndata \nAHCA_BoneMarrow [12] 3,000 bone marrow cells from a \nsingle adult donor \nzero-shot cell type \nannotation \nSimonson2023 [84] 60,345 cells from 8 human left \nventricle samples  \nSuo2022 [85] \n140,000 liver cells from 14 fetal \ndonors at various \ndevelopmental stages  \nIntestine_HCL_55k dataset \n[41] \n55,214 intestinal cells across 24 \ncell types  \nImmune cell dataset [86] 274,346 cells spanning 18 \nbatches  batch integration \nLung dataset [16] 32,472 cells from 16 donors  \nsix organs within the hECA-\n10M dataset [83] 3,000 conditions conditional cell \ngeneration \nUCE [87] \nBioRxiv \nposted \nNov \n2023 \na 33-layer model consisting \nof over 650 million \nparameters \nBERT-based \nsingle-cell \ngene \nexpression \ndataset \nTabula Sapiens v2 dataset \nhuman data from 581,430 cells, \n27 tissues, batches and 162 \nunique cell types \nzero-shot embedding \nof new datasets \na dataset of green monkey \nlymph node and lung cells [88] 17 cell types \nCell type embedding \nfor new species \nnaked mole rat spleen and \ncirculating immune cells [89] 24 cell types \ntwo distinct chicken datasets \n(chick retina [90] and \n15 cell types in chicken heart \ndataset \ndeveloping chick heart [91]) \nmouse renal cells [92]  \ndecode the function of \nnewly discovered cell \ntypes \nCellLM [93] \narXiv \nposted \nJune \n2023 \n Performer model, 10 \nlayers, 16 attention heads, \nover 50 million parameters  \nBERT-based \nscRNA-seq \ndata \nZheng68k dataset [3] \n68,450 human peripheral blood \nmononuclear cells (PBMCs) \nwith 11 highly related cell types Cell type annotation \nThe pancreas Baron dataset [4]  8,562 cells categorized into 13 \ndifferent cell types \nHuman lung cancer cells \n(GSE149383) [94] 2,739 cells Single-cell drug \nsensitivity prediction human oral squamous cancer \ncells (GSE117872) [95-97] 1,302 cells \nSingle-omics \ncell line data \ncell lines integrating from \nCCLE [98] and GDSC [99] 555 cell lines and 223 drugs \nSingle-omics cell line \ndrug sensitivity \nprediction \nscCLIP [100] Oct \n2023 Vanilla transformers \ntransformer-\nbased \nencoders \nPaired scRNA \nand scATAC-\nseq data \n Fetal Atlas [101, 102] 377, 134 cells  integration of multi-\nmodal single-cell \nsequencing data  Brain [103] - \niSEEEK [104] Jan \n2022 \n8 transformer layers each \nwith 576 hidden units and 8 \nattention heads \nBERT-based \nsingle-cell \nexpression \ndata  \nPBMCs [105] 43,073 cells  \nCell clustering \nHuman Cell Atlas Census of \nImmune Cells (HCA) [45] \n282,588 Bone marrow cells \nfrom 64 healthy donors in \nHuman Cell Atlas (HCA) \nproject \nTabula Muris dataset [30] Nearly 100000 cells from 20 \norgans and tissues \nZheng68k dataset [3] 68,579 cells \nthe dataset of FACS-sorted \nCD4/8+ T cells [31, 106, 107] \n12,670 CD4+ and 9,012 CD8+ T \ncells  \nIdentify gene‚Äìgene \ninteraction \nnetworks  \nCellPLM [108] \nBioRxiv \nposted \nOct \n2023 \nover 80 million parameters BERT-based \nscRNA-seq \nand spatially-\nresolved \ntranscriptomic \n(SRT) data \ndataset from Li et al. [109] 48, 082 cells  zero-shot clustering \nPBMC 5K and Jurkat from 10x \nGenomics \n33,538 cells in PBMC 5K and \n32,738 cells in Jurkat  scRNA-seq denoising \ntwo spatial transcriptomic \ndatasets at single-cell \nresolution, i.e., Lung2 and \nLiver2  \n836,739 cells in Lung2 and \n598,141 cells in Liver2  \nspatial transcriptomic \nimputation \nhPancreas [17] and Multiple \nSclerosis (MS) [110] - cell type annotation \nthe Adamson Perturb-Seq 87 one-gene perturbations in the perturbation \ndataset [19] and the Norman \nPerturb-Seq dataset [20] \nAdamson Perturb-Seq dataset, \nand 131 two-gene perturbations \nand 105 one-gene perturbations \nin the Norman Perturb-Seq \ndataset \nprediction \nscGREAT [111] Feb \n2024 - Transformer-\nbased \nsingle-cell \ntranscriptomic\ns \nhuman embryonic stem cells \n(hESC) (GSE75748) [112] - \ngene regulatory \nnetwork inference \nhuman mature hepatocytes \n(hHEP) (GSE81252) [113, 114] - \nmouse dendritic cells (mDC) \n(GSE48968) [115] - \nmouse embryonic stem cells \n(mESC) (GSE98664) [116] - \nmouse hematopoietic stem \ncells with erythroid-lineage \n(mHSC-E)  \nmouse hematopoietic stem \ncells with granulocyte-\nmonocyte-lineage (mHSC-\nGM) \nmouse hematopoietic stem \ncells with lymphoid-lineage \n(mHSC-L) (GSE81682) [117] \n- \nBioFormers [118] \nbioRxiv \nposted \nDec \n2023 \nAn 8-layer transformer \nencoder model with 8 self-\nattention heads per layer \nand a hidden state \ndimension of 512 \nBERT-based scRNA-seq \nPBMC [15] 7,982 cells and 3,346 genes Cell clustering and \nidentification \n PBMC 4k and 8k [119] 11,990 cells and 2,000 HVGs Gene expression \nprediction \nPerturb-seq dataset [19] \n87 single-gene perturbations, \nwith ~100 cells per perturbation \nand a control set of at least \n7,000 unperturbed cells \ngenetic perturbation \nprediction and gene \nnetwork inference \nscPRINT [120] \nbioRxiv \nposted \nJuly \n2024 \n2M to 100M parameters, 4-\nlayer transformer encoder \nmodel with 2 self-attention \nheads per layer \nBERT-based scRNA-seq \nthree test datasets of kidney, \nretina, and colon tissues [121-\n123] \ncomprising 26 cell types \ngene network \ninference, denoising, \nbatch effect \ncorrection, and cell \nlabel prediction \nperturb-seq [124] and ChIP-seq \n[125] - \n3 test datasets of ciliary body, \ncolon, and retina [122, 126, \n127] \n- \npremalignant neoplasms from - \nhuman prostate tissues [128] \nScRAT [129] Feb \n2024 \none-layer transformer \nencoder model with 8 self-\nattention heads  \nTransformer-\nbased scRNA-seq \nCOMBAT [130] and Haniffa \ndatasets [131] 835,937 and 528,438 cells disease diagnosis \nSC4 with COVID samples \n[132] \n501,943 cells for severity \nprediction \n1,289,496 cells for stage \nprediction \npredict severity and \nstage  \nCancerFound\nation [133] \nbioRxiv \nposted \nNov \n2024 \nSix transformer layers, 10.8 \nmillion parameters BERT-based \nscRNA-seq \nglioblastoma dataset [134] four distinct malignant cell \nstates batch integration \nCCLE [98] and GDSC [99] - Drug response \nprediction \nbulk RNA-seq \ndata  TCGA data [135, 136] 21 cancer types Survival prediction \nmcBERT [137] \nbioRxiv \nposted \nNov \n2024 \n12 blocks, each with 12 \nattention heads BERT-based scRNA-seq \nHeart [9, 84, 138-141] \nRefer to Table 1 in [137] \npatient-level \nrepresentation, \ndisease clustering, \nphenotypical \ninterpretation/batch \neffect removal \nKidney [142-146] \nPBMC [130, 147, 148] \nLung [149] \nNicheformer [150] \nbioRxiv \nposted \nOct \n2024 \nThe transformer block \nleverages 12 transformer \nencoder units19,27 with 16 \nattention heads per layer \nand a feed-forward network \nsize of 1,024 to generate a \n512-dimensional embedding \nof the pretraining dataset, \nresulting in altogether \n49.3M parameters. \nBERT-based \nsingle-cell and \nspatial \ntranscriptomic\ns data \nMERFISH mouse brain [151] 4.3 million cells across 59 tissue \nsections  \nLabel prediction (cell \ntype, niche and region \nlabels) \nCosMx human liver [152] 332,877 healthy cells and \n460,441 cancer cells \nniche label prediction \n(healthy data only), \nniche composition \nprediction \nCosMx human lung [152] \nfive different donors (301,611, \n89,975, 227,110, 71,304 and \n81,236 cells, respectively)  \nniche composition \nprediction \nXenium human lung from 10X \nGenomics \n295,883 healthy cells and \n531,165 cancer cells  neighborhood density \nprediction  Xenium human colon from \n10X Genomics \n275,822 healthy cells and \n587,115 cancer cells \nSpaFormer [153] \narXiv \nposted \nFeb \n2023 \n2 layers and 8 heads Transformer-\nbased \nSpatial \ntranscriptomic\ns data \nLung 5 data generated by the \nCosMX platform [152] 99,656 cells \n spatial \ntranscriptomic \nimputation \nKidney 1139 data generated by \nthe CosMX platform [152] 61,283 cells \nLiver normal generated by the \nCosMX platform [152] 305,730 cells \n \nReferences \n1. Yang, F., et al., scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data. Nature \nMachine Intelligence, 2022. 4(10): p. 852-866. \n2. Franzen, O., L.M. Gan, and J.L.M. Bjorkegren, PanglaoDB: a web server for exploration of mouse and human single-cell RNA \nsequencing data. Database (Oxford), 2019. 2019. \n3. Zheng, G.X., et al., Massively parallel digital transcriptional profiling of single cells. Nat Commun, 2017. 8: p. 14049. \n4. Baron, M., et al., A Single-Cell Transcriptomic Map of the Human and Mouse Pancreas Reveals Inter- and Intra-cell Population \nStructure. Cell Syst, 2016. 3(4): p. 346-360 e4. \n5. Muraro, M.J., et al., A Single-Cell Transcriptome Atlas of the Human Pancreas. Cell Syst, 2016. 3(4): p. 385-394 e3. \n6. Segerstolpe, A., et al., Single-Cell Transcriptome Profiling of Human Pancreatic Islets in Health and Type 2 Diabetes. Cell Metab, 2016. \n24(4): p. 593-607. \n7. Xin, Y ., et al., RNA Sequencing of Single Human Islet Cells Reveals Type 2 Diabetes Genes. Cell Metab, 2016. 24(4): p. 608-615. \n8. MacParland, S.A., et al., Single cell RNA sequencing of human liver reveals distinct intrahepatic macrophage populations. Nat Commun, \n2018. 9(1): p. 4383. \n9. Litvinukova, M., et al., Cells of the adult human heart. Nature, 2020. 588(7838): p. 466-472. \n10. Tucker, N.R., et al., Transcriptional and Cellular Diversity of the Human Heart. Circulation, 2020. 142(5): p. 466-482. \n11. Lukassen, S., et al., SARS-CoV-2 receptor ACE2 and TMPRSS2 are primarily expressed in bronchial transient secretory cells. EMBO J, \n2020. 39(10): p. e105114. \n12. He, S., et al., Single-cell transcriptome profiling of an adult human cell atlas of 15 major organs. Genome Biol, 2020. 21(1): p. 294. \n13. Cui, H., et al., scGPT: toward building a foundation model for single-cell multi-omics using generative AI. Nat Methods, 2024. 21(8): p. \n1470-1480. \n14. (n.d.)., C.Z.I., CZ CELLxGENE Discover. 2022, https://cellxgene.cziscience.com/. \n15. Gayoso, A., et al., A Python library for probabilistic analysis of single-cell omics data. Nat Biotechnol, 2022. 40(2): p. 163-166. \n16. Luecken, M.D., et al., Benchmarking atlas-level data integration in single-cell genomics. Nat Methods, 2022. 19(1): p. 41-50. \n17. Chen, J., et al., Transformer for one stop interpretable cell type annotation. Nat Commun, 2023. 14(1): p. 223. \n18. Lawlor, N., et al., Single-cell transcriptomes identify human islet cell signatures and reveal cell-type-specific expression changes in type \n2 diabetes. Genome Res, 2017. 27(2): p. 208-222. \n19. Adamson, B., et al., A Multiplexed Single-Cell CRISPR Screening Platform Enables Systematic Dissection of the Unfolded Protein \nResponse. Cell, 2016. 167(7): p. 1867-1882 e21. \n20. Norman, T.M., et al., Exploring genetic interaction manifolds constructed from rich single-cell phenotypes. Science, 2019. 365(6455): p. \n786-793. \n21. Cusanovich, D.A., et al., Multiplex single cell profiling of chromatin accessibility by combinatorial cellular indexing. Science, 2015. \n348(6237): p. 910-4. \n22. Mimitou, E.P., et al., Scalable, multimodal profiling of chromatin accessibility, gene expression and protein levels in single cells. Nat \nBiotechnol, 2021. 39(10): p. 1246-1258. \n23. Xu, J., et al., CIForm as a Transformer-based model for cell-type annotation of large-scale single-cell RNA-seq data. Brief Bioinform, \n2023. 24(4). \n24. Sun, Z., et al., A Bayesian mixture model for clustering droplet-based single-cell transcriptomic data from population studies. Nat \nCommun, 2019. 10(1): p. 1649. \n25. Oetjen, K.A., et al., Human bone marrow assessment by single-cell RNA sequencing, mass cytometry, and flow cytometry. JCI Insight, \n2018. 3(23). \n26. Dahlin, J.S., et al., A single-cell hematopoietic landscape resolves 8 lineage trajectories and defects in Kit mutant mice. Blood, 2018. \n131(21): p. e1-e11. \n27. Zeisel, A., et al., Molecular Architecture of the Mouse Nervous System. Cell, 2018. 174(4): p. 999-1014 e22. \n28. Saunders, A., et al., Molecular Diversity and Specializations among the Cells of the Adult Mouse Brain. Cell, 2018. 174(4): p. 1015-1030 \ne16. \n29. Rosenberg, A.B., et al., Single-cell profiling of the developing mouse brain and spinal cord with split-pool barcoding. Science, 2018. \n360(6385): p. 176-182. \n30. Tabula Muris, C., et al., Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris. Nature, 2018. 562(7727): p. 367-372. \n31. Zhang, L., et al., Lineage tracking reveals dynamic relationships of T cells in colorectal cancer. Nature, 2018. 564(7735): p. 268-272. \n32. Tasic, B., et al., Shared and distinct transcriptomic cell types across neocortical areas. Nature, 2018. 563(7729): p. 72-78. \n33. Chou, C.H., et al., Synovial cell cross-talk with cartilage plays a major role in the pathogenesis of osteoarthritis. Sci Rep, 2020. 10(1): p. \n10868. \n34. Alsaigh, T., et al., Decoding the transcriptome of calcified atherosclerotic plaque at single-cell resolution. Commun Biol, 2022. 5(1): p. \n1084. \n35. Zeisel, A., et al., Brain structure. Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq. Science, 2015. \n347(6226): p. 1138-42. \n36. Bastidas-Ponce, A., et al., Comprehensive single cell mRNA profiling reveals a detailed roadmap for pancreatic endocrinogenesis. \nDevelopment, 2019. 146(12). \n37. Tabula Muris, C., A single-cell transcriptomic atlas characterizes ageing tissues in the mouse. Nature, 2020. 583(7817): p. 590-595. \n38. Jiao, L., et al., scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene Embeddings. Biomolecules, 2023. 13(4). \n39. Shao, X., et al., scDeepSort: a pre-trained cell-type annotation method for single-cell transcriptomics using deep learning with a \nweighted graph neural network. Nucleic Acids Res, 2021. 49(21): p. e122. \n40. Song, T., et al., TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer. \nFront Genet, 2022. 13: p. 1038919. \n41. Han, X., et al., Construction of a human cell landscape at single-cell level. Nature, 2020. 581(7808): p. 303-309. \n42. Theodoris, C.V ., et al., Transfer learning enables predictions in network biology. Nature, 2023. 618(7965): p. 616-624. \n43. Lhoest, Q., et al., Datasets: A community library for natural language processing. arXiv preprint arXiv:2109.02846, 2021. \n44. Shen, H., et al., Generative pretraining from large-scale transcriptomes for single-cell deciphering. iScience, 2023. 26(5): p. 106536. \n45. Regev, A., et al., The human cell atlas white paper. arXiv preprint arXiv:1810.05192, 2018. \n46. Peng, Y .R., et al., Molecular Classification and Comparative Taxonomics of Foveal and Peripheral Cells in Primate Retina. Cell, 2019. \n176(5): p. 1222-1237 e22. \n47. Johnson, W.E., C. Li, and A. Rabinovic, Adjusting batch effects in microarray expression data using empirical Bayes methods. \nBiostatistics, 2007. 8(1): p. 118-27. \n48. Huang, T.X. and L. Fu, The immune landscape of esophageal cancer. Cancer Commun (Lond), 2019. 39(1): p. 79. \n49. Ma, A., et al., Single-cell biological network inference using a heterogeneous graph transformer. Nat Commun, 2023. 14(1): p. 964. \n50. Luecken, M.D., et al. A sandbox for prediction and integration of DNA, RNA, and proteins in single cells. in Thirty-fifth conference on \nneural information processing systems datasets and benchmarks track (Round 2). 2021. \n51. Li, G., et al., A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-seq data. Genome Biol, 2022. 23(1): p. \n20. \n52. Cao, J., et al., Joint profiling of chromatin accessibility and gene expression in thousands of single cells. Science, 2018. 361(6409): p. \n1380-1385. \n53. Zhu, C., et al., An ultra high-throughput method for single-cell joint analysis of open chromatin and transcriptome. Nat Struct Mol Biol, \n2019. 26(11): p. 1063-1070. \n54. Chen, S., B.B. Lake, and K. Zhang, High-throughput sequencing of the transcriptome and chromatin accessibility in the same cell. Nat \nBiotechnol, 2019. 37(12): p. 1452-1457. \n55. Ma, S., et al., Chromatin Potential Identified by Shared Single-Cell Profiling of RNA and Chromatin. Cell, 2020. 183(4): p. 1103-1116 \ne20. \n56. Linjing, L., et al., A pre-trained large language model for translating single-cell transcriptome to proteome. bioRxiv, 2023: p. \n2023.07.04.547619. \n57. Cancer Genome Atlas Research, N., Comprehensive molecular characterization of clear cell renal cell carcinoma. Nature, 2013. \n499(7456): p. 43-9. \n58. Ciriello, G., et al., Comprehensive Molecular Portraits of Invasive Lobular Breast Cancer. Cell, 2015. 163(2): p. 506-19. \n59. Fishbein, L., et al., Comprehensive Molecular Characterization of Pheochromocytoma and Paraganglioma. Cancer Cell, 2017. 31(2): p. \n181-193. \n60. Kahles, A., et al., Comprehensive Analysis of Alternative Splicing Across Tumors from 8,705 Patients. Cancer Cell, 2018. 34(2): p. 211-\n224 e6. \n61. Cao, L., et al., Proteogenomic characterization of pancreatic ductal adenocarcinoma. Cell, 2021. 184(19): p. 5031-5052 e26. \n62. Dou, Y ., et al., Proteogenomic Characterization of Endometrial Carcinoma. Cell, 2020. 180(4): p. 729-748 e26. \n63. Gillette, M.A., et al., Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. Cell, 2020. 182(1): \np. 200-225 e35. \n64. Krug, K., et al., Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. Cell, 2020. 183(5): p. 1436-1456 e31. \n65. Petralia, F., et al., Integrated Proteogenomic Characterization across Major Histological Types of Pediatric Brain Cancer. Cell, 2020. \n183(7): p. 1962-1985 e31. \n66. Satpathy, S., et al., A proteogenomic portrait of lung squamous cell carcinoma. Cell, 2021. 184(16): p. 4348-4371 e40. \n67. Wang, L.B., et al., Proteogenomic and metabolomic characterization of human glioblastoma. Cancer Cell, 2021. 39(4): p. 509-528 e20. \n68. Encyclopedia, T.C.C.L., et al., Consistency of drug profiles and predictors in large-scale cancer cell line data. Nature, 2015. 528(7580): \np. 84. \n69. Nusinow, D.P., et al., Quantitative Proteomics of the Cancer Cell Line Encyclopedia. Cell, 2020. 180(2): p. 387-402 e16. \n70. Pietzak, E.J., et al., Genomic Differences Between \"Primary\" and \"Secondary\" Muscle-invasive Bladder Cancer as a Basis for Disparate \nOutcomes to Cisplatin-based Neoadjuvant Chemotherapy. Eur Urol, 2019. 75(2): p. 231-239. \n71. Hao, Y ., et al., Integrated analysis of multimodal single-cell data. Cell, 2021. 184(13): p. 3573-3587 e29. \n72. Peterson, V .M., et al., Multiplexed quantification of proteins and transcripts in single cells. Nat Biotechnol, 2017. 35(10): p. 936-939. \n73. Stoeckius, M., et al., Simultaneous epitope and transcriptome measurement in single cells. Nat Methods, 2017. 14(9): p. 865-868. \n74. Cheng, S., et al., A pan-cancer single-cell transcriptional atlas of tumor infiltrating myeloid cells. Cell, 2021. 184(3): p. 792-809 e23. \n75. Hao, M., et al., Large-scale foundation model on single-cell transcriptomics. Nat Methods, 2024. 21(8): p. 1481-1491. \n76. Liu, Q., et al., DeepCDR: a hybrid graph convolutional network for predicting cancer drug response. Bioinformatics, 2020. 36(Suppl_2): \np. i911-i918. \n77. Roohani, Y ., K. Huang, and J. Leskovec, Predicting transcriptional outcomes of novel multigene perturbations with GEARS. Nat \nBiotechnol, 2023. \n78. Tang, W., et al. Single-cell multimodal prediction via transformers. in Proceedings of the 32nd ACM International Conference on \nInformation and Knowledge Management. 2023. \n79. Yang, X., et al., GeneCompass: deciphering universal gene regulatory mechanisms with a knowledge-informed cross-species foundation \nmodel. Cell Res, 2024. 34(12): p. 830-845. \n80. Qiao, Y ., et al., Synergistic activation of inflammatory cytokine genes by interferon-gamma-induced chromatin remodeling and toll-like \nreceptor signaling. Immunity, 2013. 39(3): p. 454-69. \n81. Srivatsan, S.R., et al., Massively multiplex chemical transcriptomics at single-cell resolution. Science, 2020. 367(6473): p. 45-51. \n82. Garipler, G., et al., The BTB transcription factors ZBTB11 and ZFP131 maintain pluripotency by repressing pro-differentiation genes. \nCell Rep, 2022. 38(11): p. 110524. \n83. Bian, H., et al. scMulan: a multitask generative pre-trained language model for single-cell analysis. in International Conference on \nResearch in Computational Molecular Biology. 2024. Springer. \n84. Simonson, B., et al., Single-nucleus RNA sequencing in ischemic cardiomyopathy reveals common transcriptional profile underlying end-\nstage heart failure. Cell Rep, 2023. 42(2): p. 112086. \n85. Suo, C., et al., Mapping the developing human immune system across organs. Science, 2022. 376(6597): p. eabo0510. \n86. Lotfollahi, M., et al., Mapping single-cell data to reference atlases by transfer learning. Nat Biotechnol, 2022. 40(1): p. 121-130. \n87. Rosen, Y ., et al., Universal cell embeddings: A foundation model for cell biology. bioRxiv, 2023: p. 2023.11. 28.568918. \n88. Dominguez Conde, C., et al., Cross-tissue immune cell analysis reveals tissue-specific features in humans. Science, 2022. 376(6594): p. \neabl5197. \n89. Speranza, E., et al., Single-cell RNA sequencing reveals SARS-CoV-2 infection dynamics in lungs of African green monkeys. Sci Transl \nMed, 2021. 13(578). \n90. Hilton, H.G., et al., Single-cell transcriptomics of the naked mole-rat reveals unexpected features of mammalian immunity. PLoS Biol, \n2019. 17(11): p. e3000528. \n91. Yamagata, M., W. Yan, and J.R. Sanes, A cell atlas of the chick retina based on single-cell transcriptomics. Elife, 2021. 10. \n92. Orozco, L.D., et al., Integration of eQTL and a Single-Cell Atlas in the Human Eye Identifies Causal Genes for Age-Related Macular \nDegeneration. Cell Rep, 2020. 30(4): p. 1246-1259 e6. \n93. Zhao, S., J. Zhang, and Z. Nie, Large-scale cell representation learning via divide-and-conquer contrastive learning. arXiv preprint \narXiv:2306.04371, 2023. \n94. Aissa, A.F., et al., Single-cell transcriptional changes associated with drug tolerance and response to combination therapies in cancer. \nNat Commun, 2021. 12(1): p. 1628. \n95. Sharma, A., et al., Longitudinal single-cell RNA sequencing of patient-derived primary cells reveals drug-induced infidelity in stem cell \nhierarchy. Nat Commun, 2018. 9(1): p. 4931. \n96. Ravasio, A., et al., Single-cell analysis of EphA clustering phenotypes to probe cancer cell heterogeneity. Commun Biol, 2020. 3(1): p. \n429. \n97. Suphavilai, C., et al., Predicting heterogeneity in clone-specific therapeutic vulnerabilities using single-cell transcriptomic signatures. \nGenome Med, 2021. 13(1): p. 189. \n98. Barretina, J., et al., The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity. Nature, 2012. \n483(7391): p. 603-7. \n99. Iorio, F., et al., A Landscape of Pharmacogenomic Interactions in Cancer. Cell, 2016. 166(3): p. 740-754. \n100. Xiong, L., T. Chen, and M. Kellis. scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training. in NeurIPS 2023 AI \nfor Science Workshop. \n101. Cao, J., et al., A human cell atlas of fetal gene expression. Science, 2020. 370(6518). \n102. Domcke, S., et al., A human cell atlas of fetal chromatin accessibility. Science, 2020. 370(6518). \n103. Anderson, A.G., et al., Single nucleus multiomics identifies ZEB1 and MAFB as candidate regulators of Alzheimer's disease-specific cis-\nregulatory elements. Cell Genom, 2023. 3(3): p. 100263. \n104. Shen, H., et al., A universal approach for integrating super large-scale single-cell transcriptomes by exploring gene rankings. Brief \nBioinform, 2022. 23(2). \n105. Kang, H.M., et al., Multiplexed droplet single-cell RNA-sequencing using natural genetic variation. Nat Biotechnol, 2018. 36(1): p. 89-\n94. \n106. Guo, X., et al., Global characterization of T cells in non-small-cell lung cancer by single-cell sequencing. Nat Med, 2018. 24(7): p. 978-\n985. \n107. Zheng, C., et al., Landscape of Infiltrating T Cells in Liver Cancer Revealed by Single-Cell Sequencing. Cell, 2017. 169(7): p. 1342-1356 \ne16. \n108. Wen, H., et al., CellPLM: pre-training of cell language model beyond single cells. bioRxiv, 2023: p. 2023.10. 03.560734. \n109. Li, Y ., et al., Single-Cell Transcriptome Analysis Reveals Dynamic Cell Populations and Differential Gene Expression Patterns in \nControl and Aneurysmal Human Aortic Tissue. Circulation, 2020. 142(14): p. 1374-1388. \n110. Schirmer, L., et al., Neuronal vulnerability and multilineage diversity in multiple sclerosis. Nature, 2019. 573(7772): p. 75-82. \n111. Wang, Y ., et al., scGREAT: Transformer-based deep-language model for gene regulatory network inference from single-cell \ntranscriptomics. iScience, 2024. 27(4): p. 109352. \n112. Chu, L.F., et al., Single-cell RNA-seq reveals novel regulators of human embryonic stem cell differentiation to definitive endoderm. \nGenome Biol, 2016. 17(1): p. 173. \n113. Mora-Bermudez, F., et al., Differences and similarities between human and chimpanzee neural progenitors during cerebral cortex \ndevelopment. Elife, 2016. 5. \n114. Camp, J.G., et al., Multilineage communication regulates human liver bud development from pluripotency. Nature, 2017. 546(7659): p. \n533-538. \n115. Shalek, A.K., et al., Single-cell RNA-seq reveals dynamic paracrine control of cellular variation. Nature, 2014. 510(7505): p. 363-9. \n116. Hayashi, T., et al., Single-cell full-length total RNA sequencing uncovers dynamics of recursive splicing and enhancer RNAs. Nat \nCommun, 2018. 9(1): p. 619. \n117. Nestorowa, S., et al., A single-cell resolution map of mouse hematopoietic stem and progenitor cell differentiation. Blood, 2016. 128(8): \np. e20-31. \n118. Amara-Belgadi, S., et al., BIOFORMERS: A SCALABLE FRAMEWORK FOR EXPLORING BIOSTATES USING TRANSFORMERS. \nbioRxiv, 2023: p. 2023.11. 29.569320. \n119. Zheng, G.X., et al., Massively parallel digital transcriptional profiling of single cells. Nature communications, 2017. 8(1): p. 14049. \n120. Kalfon, J., et al., scPRINT: pre-training on 50 million cells allows robust gene network predictions. bioRxiv, 2024: p. 2024.07. \n29.605556. \n121. Kong, L., et al., The landscape of immune dysregulation in Crohn's disease revealed through single-cell transcriptomic profiling in the \nileum and colon. Immunity, 2023. 56(2): p. 444-458 e5. \n122. Wang, S.K., et al., Single-cell multiome of the human retina and deep learning nominate causal variants in complex eye diseases. Cell \nGenom, 2022. 2(8). \n123. Marshall, J.L., et al., High-resolution Slide-seqV2 spatial transcriptomics enables discovery of disease-specific cell neighborhoods and \npathways. iScience, 2022. 25(4): p. 104097. \n124. Dixit, A., et al., Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens. Cell, \n2016. 167(7): p. 1853-1866 e17. \n125. Park, P.J., ChIP-seq: advantages and challenges of a maturing technology. Nat Rev Genet, 2009. 10(10): p. 669-80. \n126. Burclaff, J., et al., A Proximal-to-Distal Survey of Healthy Adult Human Small Intestine and Colon Epithelium by Single-Cell \nTranscriptomics. Cell Mol Gastroenterol Hepatol, 2022. 13(5): p. 1554-1589. \n127. van Zyl, T., et al., Cell atlas of the human ocular anterior segment: Tissue-specific and shared cell types. Proc Natl Acad Sci U S A, \n2022. 119(29): p. e2200914119. \n128. Joseph, D.B., et al., Single-cell analysis of mouse and human prostate reveals novel fibroblasts with specialized distribution and \nmicroenvironment interactions. J Pathol, 2021. 255(2): p. 141-154. \n129. Mao, Y ., et al., Phenotype prediction from single-cell RNA-seq data using attention-based neural networks. Bioinformatics, 2024. 40(2). \n130. julian.knight@well.ox.ac.uk, C.O.-M.-o.B.A.C.E.a. and C.O.-M.-o.B.A. Consortium, A blood atlas of COVID-19 defines hallmarks of \ndisease severity and specificity. Cell, 2022. 185(5): p. 916-938 e58. \n131. Stephenson, E., et al., Single-cell multi-omics analysis of the immune response in COVID-19. Nat Med, 2021. 27(5): p. 904-916. \n132. Ren, X., et al., COVID-19 immune features revealed by a large-scale single-cell transcriptome atlas. Cell, 2021. 184(7): p. 1895-1913 \ne19. \n133. Theus, A., et al., CancerFoundation: A single-cell RNA sequencing foundation model to decipher drug resistance in cancer. bioRxiv, \n2024: p. 2024.11. 01.621087. \n134. Neftel, C., et al., An Integrative Model of Cellular States, Plasticity, and Genetics for Glioblastoma. Cell, 2019. 178(4): p. 835-849 e21. \n135. Wissel, D., et al., Survboard: standardised benchmarking for multi-omics cancer survival models. bioRxiv, 2022: p. 2022.11. 18.517043. \n136. Cancer Genome Atlas Research, N., et al., The Cancer Genome Atlas Pan-Cancer analysis project. Nat Genet, 2013. 45(10): p. 1113-20. \n137. Querfurth, B.v., et al., mcBERT: Patient-Level Single-cell Transcriptomics Data Representation. bioRxiv, 2024: p. 2024.11. 04.621897. \n138. Chaffin, M., et al., Single-nucleus profiling of human dilated and hypertrophic cardiomyopathy. Nature, 2022. 608(7921): p. 174-180. \n139. Koenig, A.L., et al., Single-cell transcriptomics reveals cell-type-specific diversification in human heart failure. Nat Cardiovasc Res, \n2022. 1(3): p. 263-280. \n140. Reichart, D., et al., Pathogenic variants damage cell composition and single cell transcription in cardiomyopathies. Science, 2022. \n377(6606): p. eabo1984. \n141. Kuppe, C., et al., Spatial multi-omic map of human myocardial infarction. Nature, 2022. 608(7924): p. 766-777. \n142. Lake, B.B., et al., An atlas of healthy and injured cell states and niches in the human kidney. Nature, 2023. 619(7970): p. 585-594. \n143. Kuppe, C., et al., Decoding myofibroblast origins in human kidney fibrosis. Nature, 2021. 589(7841): p. 281-286. \n144. Muto, Y ., et al., Defining cellular complexity in human autosomal dominant polycystic kidney disease by multimodal single cell analysis. \nNat Commun, 2022. 13(1): p. 6497. \n145. Wilson, P.C., et al., Multimodal single cell sequencing implicates chromatin accessibility and genetic background in diabetic kidney \ndisease progression. Nat Commun, 2022. 13(1): p. 5253. \n146. Muto, Y ., et al., Single cell transcriptional and chromatin accessibility profiling redefine cellular heterogeneity in the adult human \nkidney. Nat Commun, 2021. 12(1): p. 2190. \n147. Perez, R.K., et al., Single-cell RNA-seq reveals cell type-specific molecular and genetic associations to lupus. Science, 2022. 376(6589): \np. eabf1970. \n148. Yoshida, M., et al., Local and systemic responses to SARS-CoV-2 infection in children and adults. Nature, 2022. 602(7896): p. 321-327. \n149. Sikkema, L., et al., An integrated cell atlas of the lung in health and disease. Nat Med, 2023. 29(6): p. 1563-1577. \n150. Schaar, A., et al., Nicheformer: a foundation model for single‚Äêcell and spatial omics. 2024. Preprint at bioRxiv, 2024. 4: p. 589472. \n151. Yao, Z., et al., A high-resolution transcriptomic and spatial atlas of cell types in the whole mouse brain. Nature, 2023. 624(7991): p. 317-\n332. \n152. He, S., et al., High-plex multiomic analysis in FFPE at subcellular level by spatial molecular imaging. bioRxiv 467020. 2021. \n153. Wen, H., et al., Single cells are spatial tokens: Transformers for spatial transcriptomic data imputation. arXiv preprint \narXiv:2302.03038, 2023. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7507871389389038
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6554940938949585
    },
    {
      "name": "Language model",
      "score": 0.5068338513374329
    },
    {
      "name": "Natural language processing",
      "score": 0.45819908380508423
    },
    {
      "name": "Natural language",
      "score": 0.42585238814353943
    },
    {
      "name": "Focus (optics)",
      "score": 0.42373162508010864
    },
    {
      "name": "Machine learning",
      "score": 0.35961031913757324
    },
    {
      "name": "Data science",
      "score": 0.33150598406791687
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ]
}