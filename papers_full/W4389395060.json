{
  "title": "EstraNet: An Efficient Shift-Invariant Transformer Network for Side-Channel Analysis",
  "url": "https://openalex.org/W4389395060",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2159563444",
      "name": "Suvadeep Hajra",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    },
    {
      "id": "https://openalex.org/A2693750049",
      "name": "Siddhartha Chowdhury",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    },
    {
      "id": "https://openalex.org/A2150698662",
      "name": "Debdeep Mukhopadhyay",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    },
    {
      "id": "https://openalex.org/A2150698662",
      "name": "Debdeep Mukhopadhyay",
      "affiliations": [
        "New York University",
        "Indian Institute of Technology Kharagpur"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3081832642",
    "https://openalex.org/W1562542037",
    "https://openalex.org/W2990296674",
    "https://openalex.org/W2952010918",
    "https://openalex.org/W2155938490",
    "https://openalex.org/W1599906532",
    "https://openalex.org/W2752892152",
    "https://openalex.org/W1688518862",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2905016804",
    "https://openalex.org/W1595155753",
    "https://openalex.org/W4313142681",
    "https://openalex.org/W3129821495",
    "https://openalex.org/W4385281805",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3163721282",
    "https://openalex.org/W3174445211",
    "https://openalex.org/W3168492867",
    "https://openalex.org/W3178245564",
    "https://openalex.org/W3090311997",
    "https://openalex.org/W1461374287",
    "https://openalex.org/W2556867355",
    "https://openalex.org/W2904533476",
    "https://openalex.org/W3208177954",
    "https://openalex.org/W2144902422",
    "https://openalex.org/W1499081748",
    "https://openalex.org/W3155806510",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W1543721789",
    "https://openalex.org/W3003917182",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2996022685",
    "https://openalex.org/W2939963136",
    "https://openalex.org/W3008730847",
    "https://openalex.org/W1862426464",
    "https://openalex.org/W4231734269",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W1537808739",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4289435181",
    "https://openalex.org/W2949384808",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2602337255",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4289115712",
    "https://openalex.org/W3093679526",
    "https://openalex.org/W2955805282",
    "https://openalex.org/W3197762246",
    "https://openalex.org/W4230633622",
    "https://openalex.org/W2913314773",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W3212115192",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Deep Learning (DL) based Side-Channel Analysis (SCA) has been extremely popular recently. DL-based SCA can easily break implementations protected by masking countermeasures. DL-based SCA has also been highly successful against implementations protected by various trace desynchronization-based countermeasures like random delay, clock jitter and shuffling. Over the years, many DL models have been explored to perform SCA. Recently, Transformer Network (TN) based model has also been introduced for SCA. Though the previously introduced TN-based model is successful against implementations jointly protected by masking and random delay countermeasures, it is not scalable to long traces (having a length greater than a few thousand) due to its quadratic time and memory complexity. This work proposes a novel shift-invariant TN-based model with linear time and memory complexity. he contributions of the work are two-fold. First, we introduce a novel TN-based model called EstraNet for SCA. EstraNet has linear time and memory complexity in trace length, significantly improving over the previously proposed TN-based model’s quadratic time and memory cost. EstraNet is also shift-invariant, making it highly effective against countermeasures like random delay and clock jitter. Secondly, we evaluated EstraNet on three SCA datasets of masked implementations with random delay and clock jitter effect. Our experimental results show that EstraNet significantly outperforms several benchmark models, demonstrating up to an order of magnitude reduction in the number of attack traces required to reach guessing entropy 1.",
  "full_text": "IACR Transactions on Cryptographic Hardware and Embedded Systems\nISSN 2569-2925, Vol. 2024, No. 1, pp. 336–374. DOI:10.46586/tches.v2024.i1.336-374\nEstraNet: An Eﬃcient Shift-Invariant\nTransformer Network for Side-Channel Analysis\nSuvadeep Hajra1, Siddhartha Chowdhury1 and Debdeep Mukhopadhyay1,2\n1 Indian Institute of Technology Kharagpur, Kharagpur, India{suvadeep.hajra,siddhartha.\nchowdhury27}@kgpian.iitkgp.ac.in,debdeep@cse.iitkgp.ac.in\n2 New York University, Abu Dhabi, UAEdebdeep@nyu.edu\nAbstract. DeepLearning(DL)basedSide-ChannelAnalysis(SCA)hasbeenextremely\npopular recently. DL-based SCA can easily break implementations protected by\nmasking countermeasures. DL-based SCA has also been highly successful against\nimplementations protected by various trace desynchronization-based countermeasures\nlike random delay, clock jitter and shuﬄing. Over the years, many DL models have\nbeen explored to perform SCA. Recently, Transformer Network (TN) based model\nhas also been introduced for SCA. Though the previously introduced TN-based model\nis successful against implementations jointly protected by masking and random delay\ncountermeasures, it is not scalable to long traces (having a length greater than a few\nthousand) due to its quadratic time and memory complexity. This work proposes\na novel shift-invariant TN-based model with linear time and memory complexity.\nThe contributions of the work are two-fold. First, we introduce a novel TN-based\nmodel called EstraNet for SCA. EstraNet has linear time and memory complexity in\ntrace length, signiﬁcantly improving over the previously proposed TN-based model’s\nquadratic time and memory cost. EstraNet is also shift-invariant, making it highly\neﬀective against countermeasures like random delay and clock jitter. Secondly, we\nevaluated EstraNet on three SCA datasets of masked implementations with random\ndelay and clock jitter eﬀect. Our experimental results show that EstraNet signiﬁcantly\noutperforms several benchmark models, demonstrating up to an order of magnitude\nreduction in the number of attack traces required to reach guessing entropy1.\nKeywords: SCA · Transformer Network· Shift-invariance\n1 Introduction\nThe power consumption or electromagnetic emission of a CMOS device depends on\nthe data being processed within the device. Side-Channel Analysis (SCA) exploits this\ndependency to recover the secret key of a cryptographic device. Non-proﬁling SCA such\nas diﬀerential power analysis (DPA) [KJJ99], correlation power analysis (CPA) [BCO04],\nand mutual information analysis [GBTP08] perform attacks on a target device without\nprior characterization of its leakage behavior. In contrast, proﬁling SCA assumes that the\nadversary possesses a clone of the target device under his control. The attacker utilizes the\nclone device to characterize the leakage behavior, learning an approximate leakage model\nfor the target device. This learned model is then used to perform actual attacks. Examples\nof proﬁling attacks include template attack [CRR02] and stochastic attack [SLP05].\nProﬁling attacks have received signiﬁcant attention in the SCA literature due to their\nability to provide worst-case evaluations of cryptographic devices against SCA. Classical\nproﬁling attacks, such as the template attack, heavily rely on the selection of informative\nsample points, commonly referred to as Points-of-Interests (POIs), before conducting the\nattack. However, selecting POIs becomes challenging for implementations protected by\nLicensed under Creative Commons License CC-BY 4.0.\nReceived: 2023-07-15 Accepted: 2023-09-15 Published: 2023-12-04\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 337\ncountermeasures like masking [CG00], random delay, and clock jitter [WP20]. In recent\nyears, Deep Learning (DL) has emerged as a highly successful approach for proﬁling\nSCA [MZ13, MPP16]. DL-based SCA (DLSCA) demonstrates remarkable performance\neven without precise POI selection. Instead, approximately selecting an attack window\ncontaining some POIs is suﬃcient for DLSCA to yield favorable results. Recent studies\n[LZC+21, PWP21, KP22] have shown that the eﬀectiveness of DL models can be signif-\nicantly enhanced by conducting attacks on larger attack windows or full-length traces.\nUtilizing a larger attack window increases the likelihood of including more POIs, thus\nimproving the performance by combining their leakages. Additionally, it is relatively easier\nto ﬁnd a larger attack window containing some POIs. This work aims to improve the\nperformance of DLSCA when applied to longer traces or larger attack windows.\nVarious deep learning (DL) models, including the Feed Forward Network (FFN) [MZ13,\nMHM13, MPP16], Convolutional Neural Network (CNN) [MPP16, CDP17, BPS+20, ZS20,\nPSK+18], and Recurrent Neural Network (RNN) [MPP16, Mag19, LZC+21], have been\nextensively investigated for proﬁling SCA. Recently, a Transformer Network (TN)-based\nmodel called TransNet was introduced in [HSAM22] for proﬁling SCA, demonstrating\nsuccessful attacks against implementations protected by masking and random delay coun-\ntermeasures. Since TNs are better at capturing the dependency among distant POIs\nthan other DL models like CNNs or RNNs [VSP+17], they are a natural choice for those\nSCAs which requires combining leakages from distant POIs. Furthermore, it was shown in\n[HSAM22] that TN could be made shift-invariant, making TransNet highly eﬀective against\nlarge trace desynchronizations. However, the quadratic time and memory complexity of\nTransNet with respect to the trace length makes it impractical for traces having lengths\ngreater than a few thousand. This study introduces a novel TN-based model called Es-\ntraNet, which exhibits linear time and memory complexity. The linear complexity enables\nEstraNet to scale eﬀectively to traces with lengths exceeding10K. Additionally, EstraNet\nis shift-invariant, making it robust against countermeasures like random delay and clock\njitter. We conduct evaluations of EstraNet on three large-scale datasets, demonstrating\ncomparable or signiﬁcantly better performance compared to three CNN and LSTM-based\nbenchmark models. More precisely, the contributions of the work are as follows:\n1. We propose EstraNet1, a novel TN-based model for SCA. EstraNet has a linear time\nand memory complexity in terms of trace length. EstraNet is also shift-invariant,\nmaking it eﬀective against countermeasures like random delay and clock jitter. Our\ncontributions in EstraNet architecture are two-fold:\n(a) The self-attention layer is the major component of a TN. We propose a novel\nself-attention layer, called GaussiP attention, with linear time and memory\ncomplexity. The attention layer incorporates relative positional encoding making\nit suitable for the shift-invariance of EstraNet.\n(b) Due to the incompatibility of conventional normalization techniques, such as\nbatch normalization and layer normalization in TNs for SCA, we introduce a\nnovel normalization approach called layer-centering in EstraNet.\n2. We conducted experimental evaluations of EstraNet on three datasets of masked\nimplementations. The main observations of our experiments are as follows:\n(a) We compared the performance of EstraNet with three CNN and LSTM-based\nbenchmark models on the datasets. Additionally, we introduced random dis-\nplacements in the traces to assess the models’ robustness against random delay.\nThe results indicate that EstraNet performs similarly or signiﬁcantly better\nthan the benchmark models. More precisely, it requires upto90% less attack\ntraces to reach the guessing entropy1 compared to the benchmark models.\n1The Tensorﬂow implementation can be available at https://github.com/suvadeep-iitb/EstraNet.git\n338 EstraNet\n(b) We conducted additional comparisons between EstraNet and the benchmark\nmodels on the datasets after adding clock jitter eﬀect. The experiments demon-\nstrate that EstraNet can reach the guessing entropy1 using fewer than100\nattack traces most of the time, while the benchmark models struggle to reach\nthe same using as many as5K traces. Even in cases where the benchmark\nmodels performs relatively well, they still required an order of magnitude more\nattack traces compared to EstraNet.\n(c) We conducted several studies to assess the inﬂuence of several hyperparameters\non the performance of EstraNet. Additionally, we performed an ablation study\nto analyze the impact of diﬀerent design choices and training setup.\nThe organization of the paper is as follows. In Section 2, we introduce the necessary\nnotations and SCA background. Section 3 brieﬂy describes vanilla TN along with its prime\ncomponent, self-attention operation. The section also brieﬂy outlines an approach to make\nthe self-attention and, thus, the TN models linear in time and memory complexity. In\nSection 4, we propose a novel self-attention operation with relative positional encoding and\nlinear time and memory cost. Section 5 introduces the overall architecture of EstraNet.\nIn Section 6, we provide the experimental results. Section 7 discusses the limitations of\nEstraNet and depicts some future work directions. Finally, in Section 8, we conclude the\nwork.\n2 Preliminaries\n2.1 Notations\nWe use the following notational conventions throughout the paper. We use a letter in\nthe capital (likeX) to represent a random variable. The corresponding small letter (like\nx) and calligraphic letter (likeX) are respectively used to represent an instantiation and\nthe domain of the random variable. Similarly, we use a capital letter in bold (likeX) to\nrepresent a random vector and the corresponding small letter in bold (likex) to represent\nan instantiation of the random vector. A matrix is represented by a capital letter in Roman\nstyle (likeM). We represent thei-th elements of a vectorx by x[i] and the element ofi-th\nrow andj-th column of a matrixM by M[i,j]. We use the notationP[·] to represent the\nprobability mass/density function andE[·] to represent expectation.\n2.2 Side-Channel Analysis\nThe power consumption or electromagnetic (EM) emission of a semiconductor device\ndepends on the values being manipulated within the device. SCA exploits this behavior\nof semiconductor devices to gain information about some intermediate sensitive variables\nof a cryptographic implementation and, hence, the device’s secret key. More precisely,\nin an SCA, an adversary takes control of the target device, also known as Device Under\nTest (DUT), and collects power or EM measurements, referred to as traces, by executing\nthe encryption (resp. decryption) algorithm multiple times with diﬀerent plaintexts (resp.\nciphertexts). Then the adversary performs a statistical test to infer the device’s secret key.\nSCA can be of two types: proﬁling SCA and non-proﬁling SCA. In a proﬁling SCA,\nthe adversary is assumed to possess a clone of the DUT under his control. Using the\nclone device, he can build a proﬁle of the DUT’s power consumption or EM emission\ncharacteristic and use that proﬁle for performing the actual attack. On the other hand, in\na non-proﬁling SCA, the adversary does not possess any clone device and, thus, cannot\nbuild any power/EM proﬁle of DUT. Instead, he tries to recover the secret key from the\ntraces of DUT only. In this paper, we consider proﬁling SCA only.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 339\n2.3 Proﬁling SCA\nA proﬁling SCA is performed in two phases. In the ﬁrst phase, known as the proﬁling\nphase, the adversary sets some known key in the clone device and collects a large number of\ntraces by executing the encryption (resp. decryption) operations on some known plaintexts\n(resp. ciphertexts) using the device. For each trace, the adversary computes the value\nof an intermediate secret variableZ = F(X,K), whereX represents a component of the\nrandom plaintext (or ciphertext),K represents a component of the (possibly random) key,\nF(·,·) is a cryptographic primitive. Then the adversary uses the traces to build a model\nfor\nP[L|Z] = P[L|F(X,K)] (1)\nwhere L represents a random vector corresponding to the traces. The conditional proba-\nbilities P[L|Z] serve as the leakage templates in the second phase.\nIn the second phase, also known as the attack phase, the adversary collects several trace-\nplaintext pairs{(˜li,˜pi)}Ta−1\ni=0 , where˜li, ˜pi are thei-th trace and plaintext (or ciphertext)\nrespectively, andTa is the total number of attack traces, executing the DUT for varying\nplaintexts. For all the traces, the secret keyk∗is unknown but ﬁxed. Finally, the adversary\ncomputes the score for each possible key as\nˆδk =\nTa−1∑\ni=0\nlog P[Z = F(˜pi,k)|L = ˜li] ∝\nTa−1∑\ni=0\nlog\n(\nP[L = ˜li|Z = F(˜pi,k)] ×P[F(˜pi,k)]\n)\n(2)\nThe keyˆk= argmaxk ˆδk is chosen as the predicted key. Ifˆk= k∗holds, the prediction is\nsaid to be correct. The rank of the correct key in the list of all the possible keys sorted by\ntheir scoresˆδk is used as a metric for the degree of success of the attack. This attack is\nalso called a Template attack as the estimatedP[L|Z] in Eq. 1 can be considered as the\nleakage template for diﬀerent values of the sensitive variableZ.\n2.4 Deep Learning based Proﬁling SCA\nIn Deep Learning (DL) based proﬁling SCA, the adversary trains a DL model which takes\na traceL as input and generates a probability distribution over all possible values of the\nsensitive variableZ. More precisely, letf(·; θ∗) be the trained DL model withθ∗be the\nmodel parameters learned during training. Thus, the output of the DL model for a tracel\ncan be written as\np = f(l; θ∗) (3)\nwhere p ∈R|Z|such thatp[i], fori= 0,···,|Z|−1, represents the predicted probability\nfor the intermediate variableZ = i. During the attack phase, given the set of attack\ntrace-plaintext pairs{(˜li,˜pi)}Ta−1\ni=0 , the score of each keyk∈K is computed as\nˆδk =\nTa−1∑\ni=0\nlog pi[F(˜pi,k)] (4)\nwhere pi = f(˜li; θ∗) is the predicted probability vector for thei-th trace. Like template\nattack, ˆk= argmaxk ˆδk is chosen as the guessed key. Alternatively, the rank of the correct\nkey in the list of all possible keys sorted by their scoresˆδk can be considered as a metric\nfor the degree of the attack’s success.\nVarious DL models (Feed Forward Network [MZ13, MHM13, MPP16], Convolutional\nNeural Network [MPP16, CDP17, BPS+20, ZS20, PSK+18], Recurrent Neural Network\n340 EstraNet\n[MPP16, Mag19, LZC+21]) have been used for SCA. Recently, [HSAM22] has introduced\na shift-invariant TN model, TransNet, for SCA. However, their proposed model only\napplies to short traces (having lengths less than a few thousand) as the time and memory\ncomplexity of the model is quadratic in trace length. This work introduces a shift-invariant\nTN with linear time and memory complexity. Thus, in the next section, we describe the\narchitecture of TN. We also brieﬂy outline the existing methods to make the TN models’\nself-attention operation; thus, the TN models themselves shift-invariant and have linear\ncomputational costs. Section 4 proposes a novel shift-invariant self-attention operation for\nSCA having a linear cost.\n3 Transformer Network\nLike all other DL models, TN also has a layered structure. It consists of multiple\ntransformer layers stacked one after another. Each transformer layer takes an input\nsequence X = [x0,...,xn−1]T ∈Rn×d of n feature vectors as input and transforms it into\nanother sequenceY = [y0,...,yn−1]T ∈Rn×d where n corresponds to the trace length\nor sequence length andd is the dimension of each feature vector. The transformer layer\nconsists of the self-attention layer and position-wise feed-forward layer. More precisely, if\nwe denote the self-attention layer by the functionfSA : Rn×d ↦→Rn×d and the position-wise\nfeed-forward layer byfPFF : Rn×d ↦→Rn×d, then the outputY of a transformer layer can\nbe computed as\nˆY = fSA(X) + X\nY = fPFF (ˆY) + ˆY\nThe function fPFF independently transforms each feature vector using a feed-forward\nnetwork, thereby enhancing the non-linear characteristics of the model. In contrast, the\nself-attention layer captures the interdependencies between input features by transforming\neach feature based on its relation to other features. As a result, the self-attention layer\nplays a critical role in TN’s ability to capture the dependency among the distant features.\nIn the context of SCA, the inputX corresponds to the input of an intermediate layer,\nwhere n represents the input length (which is equal to the trace length for the ﬁrst layer),\nand d represents the feature dimension of the preceding layer. Similarly,Y corresponds to\nthe output of the layer. The self-attention layer possesses the ability to combine leakage\ninformation from multiple POIs, leading to higher SNR outputs. Speciﬁcally, it can\ncombine the leakages of diﬀerent shares from a masked implementation to reconstruct the\nunmasked secret in some output feature vectors.\nIn the following section, we present a brief description of the self-attention layer.\n3.1 Self-attention Layer\nGiven the input sequenceX = [x0,...,xn−1]T ∈Rn×d, the self-attention layer computes\nthe output sequenceˆV = [ˆv0,...,ˆvn−1]T ∈Rn×dv as follows\nˆvi =\nn−1∑\nj=0\nsoftmax\n(qT\ni kj√\nd\n)\nvj =\nn−1∑\nj=0\nexp\n(\nqT\ni kj/\n√\nd\n)\n∑n−1\nl=0 exp\n(\nqT\ni kl/\n√\nd\n)vj (5)\nwhere\nqi = Wqxi, ki = Wkxi, vi = Wvxi\nfor i = 0,...,n −1, Wq,Wk ∈Rdk×d and Wv ∈Rdv×d. The ﬁnal output of the self-\nattention layer is computed asˆY = ˆVWT\no + X where Wo ∈Rd×dv is the projection matrix\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 341\nwhich projects thedv-dimensional ˆvjs back intod-dimensional vector space. The matrices\nWq,Wk,Wv and Wo are the parameters of the DL model which are learned during the\ntraining and thedk and dv are two hyper-parameters known as the key and value dimension\nrespectively. The scalarsoftmax\n(\nqT\ni kj√\nd\n)\ncan be thought of as the attentionˆvi pays to the\ninput featurexj. This attention mechanism plays the major role in the TN’s ability to\ncapture long-distance dependency. If there exists some dependency between two input\nfeatures, sayxi and xj, the attention fromˆvi to xj can be large, making thei-th output\nfeature ˆyi = Woˆvi + xi dependent on bothxi and xj, thus, capturing the interrelations\nbetween those. Moreover, unlike CNN and RNN, the self-attention layer can capture the\ndependency between xi and xj in a constant number of steps even when the distance\nbetween i and j is large. Indeed, in [VSP+17], it has been argued that TN is better than\nCNN and RNN in capturing long distant dependency. [HSAM22] has demonstrated that\nthe TN’s ability of capturing long distant dependency can be utilized to make it highly\neﬀective in attacking software implementation of masked countermeasure in which the\nleakages of multiple shares (that can be far apart in time dimension) need to be combined\nfor a successful attack.\nMultihead Self-Attention One self-attention operation with a set of the parameters\nWq,Wk and Wv is called one attention head. In practice, several attention heads are\nparallelly used in the self-attention layer. Thus, anH-head self-attention layer is computed\nas\nˆV(i) = fSA(X; W(i)\nq ,W(i)\nk ,W(i)\nv ), i = 0,...,H −1\nˆV = concat\n(\nˆV(0),...,ˆV(H−1)\n)\n,\nˆY = ˆVWT\no + X\nwhere, the operationconcat\n(\nˆV(0),...,ˆV(H−1)\n)\ndenotes the row-wise concatenation of the\nmatrices ˆV(0),...,ˆV(H−1). Thus, in this new setting, ˆV(i) ∈Rn×dv , ˆV ∈Rn×Hdv and\nWo ∈Rd×Hdv . For simplicity, by self-attention, we will imply the single-head self-attention\nlayer only, though our observations can be easily extended to multihead self-attention.\nOne main drawback of the vanilla self-attention operation is that any parallel implemen-\ntation of self-attention has quadratic memory and computation cost with respect to the\ninput length. In SCA, the trace length can be very large (in the order of105). Quadratic\ncomplexity of the self-attention layer prevents TN from being applied to very long traces\n[HSAM22]. Several variations of self-attention operation have been introduced, which\noperate in linear time and memory. In the next section, we describe one such approach.\n3.2 Self-attention with Linear Complexity\nRewriting the last term of Eq. (5), the self-attention operation can be given by\nˆvi =\n∑n−1\nj=0 exp\n(\nqT\ni kj/\n√\nd\n)\nvj\n∑n−1\nj=0 exp\n(\nqT\ni kj/\n√\nd\n) . (6)\nIn [KVPF20], Katharopoulos et al. have replaced the exponential function of the form\nexp(qT k/\n√\nd) by a positive functionk(q,k) such thatk(q,k) is factorizable asφ(q)T φ(k)\nfor some feature mapφ: Rdk → Rd′\nk and k(q,k) ≥0 for allk,q ∈Rdk . A functionk(q,k)\nwhich is factorizable asφ(q)T φ(k) is known as (positive semi-deﬁnite) kernel function\n[Wik22]. Thus, replacing the exponential function in Eq.(6) by a kernelk(·,·) such that\n342 EstraNet\nk(·,·) ≥0, the vectorˆvi can be computed as\nˆvT\ni =\n∑n−1\nj=0 k(qi,kj) vT\nj\n∑n−1\nj=0 k(qi,kj)\n=\n∑n−1\nj=0 φ(qi)T φ(kj)vT\nj\n∑n−1\nj=0 φ(qi)T φ(kj)\n=\nφ(qi)T ∑n−1\nj=0 φ(kj)vT\nj\nφ(qi)T ∑n−1\nj=0 φ(kj)\n. (7)\nSince all ˆvis share the terms∑n−1\nj=0 φ(kj)vT\nj and ∑n−1\nj=0 φ(kj) in Eq. (7), those can be\ncomputed using linear time and memory2. However, since they have used a diﬀerent kernel\nk(q,k) than theexp(qT k), the resultant self-attention operation diﬀers from the vanilla\nsoftmax self-attention (i.e. Eq. (5)).\n3.2.1 Feature Map for Softmax Self-attention\nRecently, several works [PPY+21, CLD+21] have proposed self-attentions which approxi-\nmate the softmax self-attention and works in linear time and memory. The theoretical\nfoundation of the works lies in the approximation of the Gaussian kernel, i.e., the kernel of\nthe formexp\n(\n−||q −k||2\n2/2σ2)\nusing random Fourier features [RR07, YSC+16, CRW17].\nMore precisely, the Fourier feature map deﬁned as\nφfr(x) = 1√de\n[\nsin(wT\n0 x),...,sin(wT\nde−1x),cos(wT\n0 x),...,cos(wT\nde−1x)\n]T\n, (8)\nwhere w0,...,wde−1 are i.i.d (independent and identically distributed) samples fromd\ndimensional Gaussian distribution with zero mean and identity covariance matrix, can\napproximate the Gaussian kernel as\nexp(−||q −k||2\n2/2) ≈φfr(q)T φfr(k).\nThus, φfr(x) is a feature map for the Gaussian Kernel. Peng et al. [PPY+21] have used\nthe above feature map for the Gaussian kernel to obtain a feature map for the kernel\nexp\n(\nqT k\n)\n. Concretely, exp(qT k) can be written as\nexp\n(\nqT k\n)\n= exp\n(||q||2\n2\n2\n)\nexp\n(\n−||q −k||2\n2\n2\n)\nexp\n(||k||2\n2\n2\n)\nThus, using the feature map:\nφtri(x) = exp\n(\n||x||2\n2/2\n)\n√de\n[\nsin(wT\n0 x),...,sin(wT\nde−1x),cos(wT\n0 x),...,cos(wT\nde−1x)\n]T\n, (9)\nwe can approximate the kernelexp\n(\nqT k\n)\nas φtri(q)T φtri(k) where de is a hyper-parameter\nknown as the dimension of the kernel feature map. However, in [CLD+21], Choromanski et\nal. pointed out thatφtri might lead to unstable behavior of self-attention due to potentially\nnegative components likesin(wT\nj x)s andcos(wT\nj x)s inφtri. They resolved the issue by\nproposing positive random feature map:\nφpos(x) = exp\n(\n−||x||2\n2/2\n)\n√de\n[\nexp(wT\n0 x),...,exp(wT\nde−1x)\n]T\n, (10)\nwhere w0,...,wde−1 are as deﬁned in Eq.(8). Since φpos(x) can have only positive\ncomponents, it solves the unstable behavior ofφtri.\n2Note that, with this reformulation of self-attention operation, the network’s ability to learn long-\ndistance dependency is not compromised as every input feature is still connected to every output feature by\na constant number of steps. However, such reformulation introduces some approximation errors [CLD+21].\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 343\n3.3 Relative Positional Self-attention with Linear Complexity\nIn self-attention of the form of Eq.(5), the attention an output featureˆvi pays to an input\nfeature xj does not depend on their positions i.e. the indicesi and j. More precisely, the\nattention paid by the output featureˆvi to the input featurexj is proportional tok(qi,kj)\nwhich is a function of the vectorsqi and kj not their positionsi and j. Thus, if the input\nsequence is permuted, the output sequence of the self-attention layer will also be permuted\nsimilarly. However, in SCA, we want the attention to be more on the POIs rather than\nall sample points having equal attention. Moreover, in the presence of countermeasures\nlike random delay and clock jitters, the distances between the POIs remain approximately\nsame though their absolute positions (the indices at which they appear) vary from trace\nto trace. Thus, we want the attention an output featureˆvi pays to an input featurexj\nto depend on their relative positions ori−j. Such modeling of attention probabilities\nis referred to as relative positional encoding. In self-attention with relative positional\nencoding, the terms of the formexp\n(\nqT\ni kj\n)\nin Eq.(5) is generalized by a positive function\nof the formf(qi,kj,i −j) [SUV18, DYY+19].\nIn self-attention with relative positional encoding and linear complexity, the kernel\nk(qi,kj) in Eq.(7) is replaced by another kernel of the formkr (qi,kj,i −j) such that\nthe new kernel is (approximately) factorizable askr (qi,kj,i −j) = φq(qi,i)T φk(kj,j).\nThus, by replacing the termsk(qi,kj) in Eq.(7) by the relative position aware kernel, we\nget self-attention with relative positional encoding, which can be performed in linear time\nand memory as follows:\nˆvT\ni =\n∑n−1\nj=0 kr (qi,kj,i −j) vT\nj\n∑n−1\nj=0 kr (qi,kj,i −j)\n=\nφq(qi,i)T ∑n−1\nj=0 φk(kj,j)vT\nj\nφq(qi,i)T ∑n−1\nj=0 φk(kj,j)\n. (11)\nIn literature, several self-attention with relative positional encoding which work in linear\ntime and memory have been introduced. For example, [LCW+21, LSL+21, SLP+21] have\nused feature maps of the formφq(qi,i) = φ(Miqi) and φk(kj,j) = φ(Njkj) where the\nmatrix MT\ni Nj is a function ofi−j and φ(·) is given by Eq.(9) or (10). [LCW+21, LSL+21]\nhave further generalized the matricesMi and Njs by some small DL models. In [Che21],\nChen et al. have used feature map of the formφq(qi,i) = M i ˜φq(qi) and φk(kj,j) =\nNj ˜φq(kj) such thatMT\ni Nj becomes a function ofi−j. [LLC+21] has used the kernel of the\nform k(qi,kj,i −j) = exp\n(\nqT\ni kj + bj−i\n)\n. Hereb−(n−1),...,b0,...,bn−1 are some relative\npositional biases whose values are also learned during training. They have further shown\nthat the self-attention operation with the above kernel can be performed inO(nlog n)\ntime using Fast Fourier Transform (FFT). [HSGB21] has used a kernel of the form\nk(qi,kj,i −j) = exp\n(\nqT\ni kj + φr(i)T φr(j)\n)\nwhere φr(·) is a feature map for the position\nindices i and j satisfying φr(i)T φr(j) to be a function ofi−j.\nIn this next section, we propose a novel attention for SCA.\n4 Self-attention in SCA\nThe informative sample points in power or EM traces are sparse. In other words, only\na few sample points in the traces are high SNR sample points, and the rest of those are\nnoisy. To see this, we plot the SNR of four secret shares on two very widely used SCA\ndatasets, namely ASCAD ﬁxed key3 and ASCAD random key4 datasets in Figure 1. A\npeak or a high value in the plots indicates the informative sample points. It can be seen\nin the ﬁgures that the SNR at most of the sample points is close to zero, implying the\ninformativeness of only a few sample points. Thus, the self-attention operation should be\n3https://github.com/ANSSI-FR/ASCAD/tree/master/ATMEGA_AES_v1/ATM_AES_v1_ﬁxed_key\n4https://github.com/ANSSI-FR/ASCAD/tree/master/ATMEGA_AES_v1/ATM_AES_v1_variable_\nkey\n344 EstraNet\n0.0 0.2 0.4 0.6 0.8 1.0\n·105\n0\n5\n10\n15\nTime\nSNR\nSbox(p3 ⊕ k3) ⊕ m3 Sbox(p3 ⊕ k3) ⊕ mout\nm3 mout\n(a) Plots of the SNR of four secrete shares on ASCAD ﬁxed key dataset.\n0 0.5 1.0 1.5 2.0 2.5\n·105\n0\n2\n4\n6\n8\n10\nTime\nSNR\nSbox(p3 ⊕ k3) ⊕ m3 Sbox(p3 ⊕ k3) ⊕ mout\nm3 mout\n(b) Plots of the SNR of four secrete shares on ASCAD random key dataset.\nFigure 1: Plots of the informative sample points on two widely used SCA datasets.\n1\n 0 1 2 30.0\n0.2\n0.4\n0.6\n0.8\n1.0\nscale = 20\n(a) Histogram for scale 20.\n1\n 0 1 2 30.0\n0.5\n1.0\n1.5\n2.0\n2.5\nscale = 10 (b) Histogram for scale 10.\n1\n 0 1 2 30\n1\n2\n3\n4\n5\n6\nscale = 5 (c) Histogram for scale 5.\nFigure 2: Plots of the histogram of the attention values of the randomly initialized\nself-attention scheme of [HSGB21] for three diﬀerent scales.\nable to put high attention to only a few sample points while putting close to zero attention\nto the rest.\nHowever, the existing self-attention with linear complexity like [LCW+21, LSL+21,\nHSGB21] puts signiﬁcantly non-zero attention to most of the input feature vectors making\nthe output feature vectors inﬂuenced by the noisy (low SNR) sample points. Figure 2 plots\nthe histogram of the attention scores of the randomly initialized self-attention scheme\nof [HSGB21] for three diﬀerent values of the scale hyper-parameter. As depicted in the\nplots, the attention scores are centered around 1, indicating that the self-attention scheme\nassigns attention close to 1 to the majority of input feature vectors. Similar observations\nhave been made for other self-attention schemes, such as the one proposed by Li et al.\n[LSL+21]. Another disadvantage of those schemes is that their attention scores result from\ncomplicated interactions among many parameters. Consequently, it is diﬃcult to train the\nnetwork to generate sparse attention scores.\nTo address the aforementioned limitations of existing self-attention mechanisms, this\nsection introduces a novel attention that employs a Gaussian kernel on the relative positions\nof input features to generate attention scores. The proposed self-attention produces sparse\nattention scores, as discussed in Section 4.2. By utilizing relative positional encoding,\nit achieves shift invariance. Additionally, the proposed method exhibits linear time and\nmemory complexity with respect to the trace length, enabling scalability to longer traces.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 345\n4.1 GaussiP: Gaussian Positional Attention\nThis section introduces the proposed Gaussian Positional attention, also called GaussiP\nattention. The GaussiP attention exhibits linear time and memory complexity with respect\nto the input length, making it computationally eﬃcient. The degree of sharpness and\nsparseness in the attention scores can be controlled by adjusting a suitable parameter.\nFurthermore, the attention mechanism enables high attention to be assigned to distant\nfeatures, facilitating the ﬂow of information over long distances.\nThe subsequent sections delve into the details of the GaussiP attention. Firstly, we\ndescribe the kernel function utilized in the GaussiP attention. Next, we introduce the\nfeature map employed for factorizing the kernel function, enabling eﬃcient computation.\nLastly, we address a signiﬁcant drawback of the proposed kernel function and present our\nsolution, which involves utilizing multiple heads in the attention.\n4.1.1 Deciding the Kernel Function\nUnlike most of the existing self-attention, we use a Gaussian kernel for our proposed\nattention:\nkr (qi,kj,i −j) = exp\n(\n−||φq(qi,i) −φk(kj,j)||2\n2\n2\n)\n(12)\nwith\nφq(q,i) =\n)\nβ1q\nβ2spWp(b + ip)\n[\nand φk(k,j) =\n)\nβ1k\nβ2spWp(b + jp + cpnp)\n[\n(13)\nfor i,j = 0,1,...,n −1 where β1,β2 ∈[0,+∞) are two hyperparameters,p,b ∈Rdp are\ntwo predeﬁned constants,Wp ∈Rdp×d is a matrix with entries drawn from a uniform\nrandom distribution, andsp ∈(0,+∞), cp ∈[−1,1] are two trainable parameters. Thus,\nwith the above deﬁnedφq and φk, the proposed kernel takes the form\nkr(qi,kj,i −j) = exp\n(\n−β2\n1 ||qi −kj||2\n2 + β2\n2 s2\np(i−j−cpn)2||Wpp||2\n2\n2\n(\n(14)\nThe partβ2\n1 ||qi −kj||2\n2 in the above equation inﬂuences the kernel scores based on the\ncontents of the input feature vectors while the partβ2\n2 s2\np(i−j−cpn)2||Wpp||2\n2 inﬂuences\nthe scores based on the relative positions, i.e.,i−j of the feature vectors. In our initial\nset of experiments, we found that the ﬁrst part does not positively eﬀect the performance\nof EstraNet. Thus, we setβ1 = 0 in Eq. (13) which simpliﬁesφq(q,i) and φk(k,i) as\nφq(i) = β2spWp(b + ip) and φk(j) = β2spWp(b + jp + cpnp) (15)\nresulting into the following simpliﬁed kernel:\nkGPA (i−j) = exp\n(\n−||φq(i) −φk(j)||2\n2\n2\n)\n= exp\n(\n−β2\n2 s2\np(i−j−cpn)2||Wpp||2\n2\n2\n(\n(16)\nThe above equation shows several important properties of the proposed kernel function.\nFirstly, it can be observed that the maximum value of the kernel output occurs when\ni−cpn= j. In simpler terms, thei-th output feature vector assigns maximum attention\nto the(i−cpn)-th input feature vector. Thus, the attention mechanism facilitates the\nﬂow of information from the(i−cpn)-th index to thei-th index, enabling the learning\nof long distant dependencies. Secondly, the precision of the attention can be controlled\n346 EstraNet\n1000\n 750\n 500\n 250\n 0 250 500 750 1000\ni j cpn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0kr(i j)\ns = 10\ns = 50\ns = 150\nFigure 3: Plots ofkGPA (i−j) vs. (i−j−cpn) for diﬀerent values ofs(= β2sp).\nby appropriately setting the hyper-parameterβ2 or learning an appropriate value for the\nparameter sp during the training process. Let us denotes= β2sp. If the value ofsis large,\nthe attention will be concentrated within a small region of the input traces. Conversely, if\nthe value ofs is small, the attention will be spread over a larger region. To illustrate this,\nFigure 3 displays the kernel scores for various values ofs. It can be observed that as the\nvalue ofs increases, the kernel scores become more concentrated in smaller regions.\n4.1.2 Deciding the Feature Map for the Kernel Function\nSince, we are using a Gaussian kernel, following the work of [CLD+21], a feature map with\nonly positive entries can be given for the kernel as follows:\nφ′(x) = exp\n(\n−||x||2\n2\n)\n√de\n[\nexp(wT\n0 x),...,exp(wT\nde−1x)\n]T\nHowever, we have observed that the approximation error ofφ′signiﬁcantly increases as\nthe norms of the input featuresx become larger. It should be noted that in order to\nconcentrate the attention scores on a smaller segment of the input traces, the norms of\nthe input features need to be suﬃciently large. As an alternative, the Fourier featuresφfr\ndeﬁned in Eq..(8) can be used as the feature map for the kernel. However, as pointed\nout in [CLD+21], the kernel scoreskGPA (i−j) approximated byφfr(φq(i))T φfr(φk(j))\ncan be potentially negative, causing the increase in the variance of the normalized kernel\nscore kGPA (i−j)/∑n−1\nk=0 kGPA (i−k), which, in turn, causes the unstable behavior of the\nself-attention. To address this issue, we propose to approximate the normalizing factor of\nthe kernel given in Eq. (16) in a closed form as follows:\nn−1∑\nk=0\nkGPA (i−k) ≈\n∫∞\nx=−∞\nkGPA (x)dx≈ 1\nβ2sp||Wpp||2\n(17)\nThe justiﬁcation for the approximation can be found in Appendix A. Using the above\napproximation in the proposed attention, the expression of the output feature vectorsˆvi\ncan be given as:\nˆvT\ni =\n∑n−1\nj=0 kGPA (i−j) vT\nj\n∑n−1\nk=0 kGPA (i−k)\n≈β2sp||Wpp||2\n)\n)φfr(φq(i))T\nn−1∑\nj=0\nφfr(φk(j))vT\nj\n(\n (18)\nwhere φfr(·) is given by Eq. 8, andφq(i) = β2spWp(b + ip) and φk(j) = β2spWp(b + jp +\ncpnp).\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 347\n0.5\n 0.0 0.5 1.0 1.5\n10 1\n100\n101\n(a) Histogram fors = 10.\n0.5\n 0.0 0.5 1.0 1.5\n10 1\n100\n101 (b) Histogram fors = 50.\n0.5\n 0.0 0.5 1.0 1.5\n10 1\n100\n101 (c) Histogram fors = 150.\nFigure 4: Plots of the histogram of the unnormalized kernel scores approximated by the\nFourier feature mapφtri. The histogram is plotted for three diﬀerent values ofs (i.e.\nβ2sp).\nTable 1:The set of hyper-parameters of the proposed GaussiP attention.\nNotation Description Notation Description\nd model dimension dk key dimension\ndv value dimension de dimension of kernel feature map\nn input or trace length H number of heads\nβ2 distance based scaling\nin attention kernel\n4.1.3 Making the Attention Distribution Multi-modal\nThe major problem with the Gaussian kernel is that its attention scores follow a uni-modal\ndistribution. In other words, it puts high attention over a small contiguous segment of the\ntraces and assigns small attention scores to the rest of the parts (as seen in Figure 3). The\nuni-modal attention distribution limits the ﬂow of information from one region to a distant\nregion. To make the attention distribution multi-modal, we use multi-head attention\n(kindly refer to Section 3.1 for a description of multi-head attention) with diﬀerent heads\nhaving diﬀerentsp and cp parameters. Since diﬀerent heads have diﬀerentcp values, they\nput high attention to diﬀerent parts of the input traces allowing the ﬂow of information\nto an output feature vector from diﬀerent regions of the input traces. Similarly, separate\nsp for each head enables them to scale the attention independently. We found that the\nproper initialization ofcp and sp for each head is crucial for the successful training of\nEstraNet. We initializesp for all heads to the same value1. However, we initializecp for\nh-th head to(1 + 2h)/2H for h= 0,1,...,H −1. In other words,cps are initialized so\nthat diﬀerent attention heads focus on diﬀerent parts of the input sequence. Appendix B\nplots the attention probabilities learned at the attention heads of a EstraNet model.\nThe set of hyper-parameters of the proposed GaussiP attention are shown in Table 1.\n4.2 Diﬀerence with the Existing Alternatives\nThe functional distinctions between the proposed GaussiP attention and existing foremost\nalternatives are summarized in Table 2. Notably, the attention scores in the GaussiP\nattention exhibit sparsity. Figure 4 illustrates that, for signiﬁcantly large values ofs= β2sp,\nthe majority of attention scores are concentrated around zero, while only a few scores are\nsubstantially greater than zero. This characteristic aligns well with the nature of side-\nchannel traces, where informative sample points are typically sparse. While methods such as\nLuo et al. [LLC+21] oﬀer greater ﬂexibility in learning arbitrary probability distributions,\nthey require complex operations such as Fast Fourier Transform (FFT) for eﬃcient\n348 EstraNet\nTable 2: Diﬀerences between the proposed GaussiP attention and the foremost self-\nattentions (with linear complexity and relative positional encoding) in TN literature.\nSelf-attention\nConventional Proposed\nAttention form ˆvT\ni =\n∑n−1\nj=0 kr(qi,kj,i−j)vT\nj\n∑n−1\nj=0 kr(qi,kj,i−j)\nˆvT\ni = β2sp||Wpp||\n∑n−1\nj=0 kr(i −j)vT\nj\nKernel form kr(qi,kj,i −j)\n= exp\n(\nφq(qi,i)T φk(kj,j)\n) kr(i−j) = exp\n(\n−||φq(i) −φk(j)||2)\nKernel feature\nmap φtri, φpos or φfr φfr\nPositional\nEncoding linear or trigonometric linear\nimplementation. The self-attention proposed by Guo et al. [GZL19] employs a Gaussian\nprior over input features to bias the attention scores based on relative distances. However,\nunlike our scheme, they utilize a softmax-based self-attention mechanism. Additionally,\ntheir scheme emphasizes high attention to nearby features, whereas our scheme can put\nhigh attention to distant features. Although the self-attention approach presented by\nLiutkus et al. [LCW+21] can be represented as a Gaussian kernel with a Fourier feature\nmap for certain hyper-parameter conﬁgurations, their attention scores are maximized\nfor i−j = 0. In other words, in their scheme, each position or sample point assigns\nmaximum attention to itself, making the propagation of information from one region to\na distant region challenging. Conversely, in our proposed scheme, for suﬃciently large\nβ2, the attention scores are maximized wheni−j ≈ncp, wheren denotes the sequence\nlength andcp ∈[0,1] represents a trainable parameter. This allows each output feature\nto allocate high attention to a distant region, enabling the ﬂow of information over long\ndistances.\nIn the subsequent section, we present the architectural design of EstraNet.\n5 EstraNet Architecture\nThis section provides a detailed overview of the EstraNet architecture. In Section 5.1, we\nintroduce a layering-centering normalization technique. Subsequently, in Section 5.2, we\npresent the structure of a single layer of EstraNet. Finally, in Section 5.3, we describe the\ncomplete architecture of EstraNet.\n5.1 Layer-Centering\nIn the conventional TN architecture, layer normalization is often employed to ensure\nthe stability of training [XYH+20]. However, it has been observed in [HSAM22] that\napplying layer normalization to TN layers makes the network untrainable for the SCA\ndatasets. Our experiments also found that incorporating either layer normalization or\nbatch normalization in EstraNet layers makes its performance poor. Consequently, we\nintroduce a novel “layer-centering” operation as an alternative approach.\nTo understand the layer-centering operation, let us assume that[x0,...,xn−1]T ∈Rn×d\nrepresents the input to the layer. Given the input, we start by computing the mean of\neach vector in the input sequence:\nµi = 1\nd\nd−1∑\nj=0\nxi[j], for i= 0,...,n −1\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 349\nMulti−Head\nLayer Norm\naddition\naddition\nPoswiseFF\nLayer Norm\nSelf−attention\nx\ny\n(a) Standard TN layer.\nPoswiseFF\nMulti−Head\nRelPositional\naddition\naddition\nSelf−attention\ny\nx (b) TransNet layer.\naddition\nPoswiseFF\naddition\nLayer Centering\nLayer Centering\nMulti−Head\nAttention\nGaussiP\nx\ny (c) EstraNet layer.\nFigure 5: Figure 5a shows a single layer of standard TN with pre-layer normalization\n[XYH+20]. Figure 5b shows a single layer of TransNet proposed in [HSAM22]. Figure 5c\ndepicts a single layer of the proposed EstraNet.\nwhere x[j] denotes thej-th element ofx and d is the model/feature dimension. Finally,\nthe input vectors are re-centered as\n¯xi = xi −µi + clc, for i= 0,...,n −1 (19)\nwhere clc ∈Rd is a trainable parameter. The resulting sequence[¯x0,...,¯xn−1] is the\noutput of the layer in the layer-centering operation. Speciﬁcally, each element of the input\nsequence is ﬁrst centered to0 and then re-centered toclc, which is a parameter learned\nduring the training process. It is worth noting that while layer-normalization involves\nre-centering and re-scaling each vector in the input sequence, layer-centering involves only\nre-centering the elements without any re-scaling.\n5.2 Single Layer of EstraNet\nThe EstraNet layer, as illustrated in Figure 5c, is similar to a standard TN layer depicted\nin Figure 5a. However, it incorporates the proposed multi-head GaussiP attention from\nSection 4.1 instead of the vanilla multi-head self-attention and the layer-centering instead\nof the layer-normalization. Compared to the TransNet layer shown in Figure 5b, EstraNet\nincorporates the novel multi-head GaussiP attention. The attention operation has a\nlinear time and memory complexity with respect to the input length. Conversely, the\nmulti-head self-attention employed in the TransNet layer exhibits quadratic memory and\ntime complexity. Furthermore, the TransNet layer lacks any normalization layer, whereas\nthe EstraNet layer utilizes layer-centering for input normalization.\n350 EstraNet\n.  .  .\n.  .  .\n.  .  .. . ..  .  .\nSoftmax Attention\nConvolutional Block with Avg. Pooling\nConvolutional Block with Avg. Pooling\nEstraNet Layer\nEstraNet Layer\nt0 t1 tn−2 tn−1\nx0\ny0\n¯y\nxm−1\nym−1\nFigure 6: EstraNet Architecture.\n5.3 EstraNet Architecture\nEstraNet follows the general multilayer architecture of TN models with some notable\nexceptions. The architecture is shown in Figure 6. The input to the EstraNet model is\na one-dimensional trace denoted ast = [t0,...,tn−1] ∈Rn. However, the EstraNet layer\nassumes the input to be two-dimensional. Thus, we pass each input trace through several,\nsay Lconv, convolutional and average-pooling layers, which convert the input tracest into\na sequence of vectorsX = [ x0,...,xm−1]T ∈Rm×d where m is the (possibly reduced)\nlength of the output sequenceX and d is the dimension of its feature vectors. Note that\nby setting a large value for the pool size, denoted asps, of the average-pooling layers,\nwe can makem ≪ n enabling the eﬃcient processing of the sequence by the following\nlayers of the EstraNet. Then the output of the ﬁnal convolutional layersX is passed\nthrough several EstraNet layers resulting in the outputY = [y0,...,ym−1]T ∈Rm×d. The\noutput sequenceY of m d-dimensional feature vectors is then reduced into a single vector\n¯y ∈Rd using a (multi-head) softmax-attention layer. The vector¯y is then passed to the\nclassiﬁcation layer to generate prediction probabilities.\nIn the following section, we present the results of our experiments.\n6 Experimental Results\nThis section presents the experimental evaluation of EstraNet. We provide an overview\nof the datasets used for the evaluation in Section 6.1. The methodology for selecting the\nattack window is described in Section 6.2. We provide the detailed information about the\nbenchmark models used in this study in Section 6.3. The training process of EstraNet is\nexplained in Section 6.4. Section 6.5 elaborates on the experiment setup and evaluation\nmethods. A comparative study between EstraNet and the benchmark models in the\npresence of a combinations of masking, random delay, and clock jitter countermeasures is\nprovided in Sections 6.6 to 6.8. In Section 6.9, we perform an ablation study to investigate\nthe impact of various design choices in EstraNet. Section 6.10 discusses the impact of\nseveral hyperparameter choices in EstraNet’s performance. The training time of EstraNet\nis compared with that of the benchmark models in Section 6.11. Finally, Section 6.12\ninvestigates the impact of data augmentation in the shift-invariance of EstraNet.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 351\n6.1 Dataset Details\nFor evaluating EstraNet, we select three datasets of software implementation of ciphers\nprotected by masking countermeasures. Since, in the software implementation of masking\ncountermeasures, diﬀerent shares of the intermediate secret variable leak at diﬀerent regions\nof the power/EM traces, they are the ideal candidates for evaluating EstraNet’s ability\nto capture long-distance dependency. Additionally, we have added random delay and the\nclock jitter eﬀect [WP20] in the traces to evaluate the shift-invariance of EstraNet. This\nsection provides the details of the datasets.\nASCAD Fixed Key (ASCADf) ASCAD ﬁxed key dataset5 is a collection of60K traces\nof a ﬁrst-order masked implementation of AES running on an 8-bit ATMega8515 mi-\ncrocontroller. We divided the entire dataset into three splits: proﬁling, validation, and\ntest containing50K, 5K, and5K traces, respectively. Following the common practice\nin the literature [BPS+20], we attack the third S-box operation of the ﬁrst round of the\ncipher. We use the identity leakage model to generate the labels for the proﬁling traces\nas it is found to perform better than Hamming weight leakage model in previous studies\n[BPS+20].\nASCAD Random Key (ASCADr)As the ASCADf dataset, ASCAD random key dataset6\nis a collection of traces of a ﬁrst-order masked implementation of AES running on an 8-bit\nATMega8515 microcontroller. However, in the ASCADf dataset, the secret key is ﬁxed\nfor all proﬁling traces, whereas in the ASCADr dataset, the secret key randomly varies\nfor each proﬁling trace. The proﬁling split of the dataset contains200K traces, while the\nattack split contains100K traces. We created validation and test splits by selecting10K\ntraces for each split from the100K attack traces. Like the ASCADf dataset, we attack the\nthird S-box operation of the ﬁrst round of the cipher. We use the identity leakage model\nto generate the labels for the proﬁling traces on this dataset also.\nCHES 2020 CTF SW3 (CHES20)Clyde-128 is a tweakable block cipher that supports\nside-channel resilient and eﬃcient bit-slicing implementation on 32-bit microprocessors\n[BBB+20]. Spook SCA CTF7 is an SCA challenge for masked implementations of the\nClyde-128. The challenge consists of multiple datasets for diﬀerent implementations of the\ncipher. We select the dataset collected from a second-order masked implementation of the\ncipher running on an ARM Cortex-M0 microcontroller. The dataset contains200K and\n500K proﬁling and attack traces. We select all200K proﬁling traces as the train set and\n10K traces for validation and test set each from the attack traces. Since Clyde-128 is an\nLS design, it works on(4 ×32)-bit state, with4 being the size of the non-linear S-box and\n32 being the size of the linear L-box. We target the four bits of the17th column from the\nleft of the ﬁrst round S-box operation. We chose the17th column as we found the SNR for\nthese bits to be high. Since each bit of an S-box is processed separately in the bit-slicing\nimplementation of the cipher, we use a multilabel loss having a sigmoid function for each\noutput bit of the S-box (as introduced in [ZXF+19, ZXF+21]) to attack the four target\nbits.\nThe statistics of all three datasets are summarized in Table 3.\n5https://github.com/ANSSI-FR/ASCAD/tree/master/ATMEGA_AES_v1/ATM_AES_v1_ﬁxed_key\n6https://github.com/ANSSI-FR/ASCAD/tree/master/ATMEGA_AES_v1/ATM_AES_v1_variable_\nkey\n7https://ctf.spook.dev/\n352 EstraNet\nTable 3:Dataset statistics.\nASCADf ASCADr CHES20\nProﬁling dataset size 50000 200000 200000\nValidation dataset size 5000 10000 10000\nTest dataset size 5000 10000 10000\nTrace length 20000 10000 10000\n6.2 Attack Window Selection\nThe trace length of the above three datasets varies from62500 (for the CHES20 dataset) to\n250K (for the ASCADr dataset). Instead of performing the evaluation on the full-length\ntraces, we perform it on a selected attack window from the full-length traces. We consider\ntwo sizes for the attack window:10K and 40K. Thus, for the ﬁrst set of experiments, we\nselect an attack window of size10K for each of the three datasets and perform attacks on\nthe window. And for the second set of experiments, we use an attack window of size40K.\nTable 4:The selected attack windows on the three datasets.\nDataset Attack Window Size\n10K 40K\nASCADf [40000,50000] [30000 ,70000]\nASCADr [78000,88000] [70000 ,110000]\nCHES20 [46000,56000] [20000 ,60000]\nThe selected attack windows for the three datasets are given in Table 4. The attack\nwindows are selected using SNR-based methods. Thus, we calculated the SNR at each\nsample point and selected a window of respective size such that the window contains the\nmost number of high SNR sample points. In many scenarios (e.g., in the presence of\ncountermeasures like masking, desynchronization, and clock jitter), the SNR-based method\nmay not work. However, note that one can still be able to identify the attack window\n(with size in the order of10K, which is signiﬁcantly large) based on a combination of the\nknowledge of the implementation and intelligent guess (such as in scheme-aware threat\nmodel [MCLS23]). In the worst case, an adversary can iteratively repeat the attack by\nselecting diﬀerent attack window at diﬀerent segments of the traces. For example, if the\nﬁrst round of the cipher spans over100K sample points, one can repeat the attack on\nmultiple windows of size40K, such as[0,40K], [20K,60K], [40K,80K], and[60K,100K].\nNote that, recent research [LZC+21] has demonstrated the possibility of directly performing\nattacks on traces with a length in the order of100K and achieving good results. However,\nthe results presented in Section 6.8 reveal that the performance of these models signiﬁcantly\ndeteriorates in challenging scenarios, such as in the presence of clock jitter. Therefore,\nEstraNet can be a better alternative for the worst-case security evaluation in such situations.\nFrom now on, we will use the terms “window size” and “trace length” interchangeably to\nrefer to the size of the attack window.\n6.3 Benchmark Models\nThis section brieﬂy describes the (existing) DL models used as the benchmark models\nin our experiments. Many DL models have been introduced in SCA literature over the\nlast few years. Among all those models, we have selected three models which have been\nintroduced to deal with long traces and/or large trace desynchronizations. The models are\nbrieﬂy described below. The detailed architectures of the models are given in Appendix C.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 353\n6.3.1 PolyCNN [MBC +20]\nIn [MBC+20], Masure et al. have proposed a CNN model to attack AES implementation\nprotected by code polymorphism. Using their proposed model, they successfully recovered\nthe secret key using less than20 attack traces. Since their model is suitable for long traces,\nwe use it as a benchmark model. We trained the model on the three datasets for10K\nepochs using Adam optimizer and a constant learning rate of1e-5.\n6.3.2 EﬀCNN [ZBHV20]\nIn [ZBHV20], Zaid et al. have proposed a methodology for creating CNN models to\nbe eﬀective against desynchronized traces. They have further demonstrated that their\nmethodology can be used to construct CNN models to perform successful attacks on\nseveral datasets. Thus, their models are good candidates for being benchmark models. We\nconstructed three models for the three datasets and used those as the benchmark models.\nThe models have been trained for2K epochs using Adam optimizer and a constant learning\nrate of2.5e-5.\n6.3.3 LSTMNet [LZC +21]\nIn [LZC+21], Lu et al. proposed to use LSTM-based models to perform attacks on full-\nlength traces. Their experimental study demonstrated that the LSTM-based models could\nbe used to conduct successful attacks on both synchronized and desynchronized datasets.\nThus, we use their models as benchmark models. On the ASCADf and ASCADr datasets,\nwe took the respective models from their online repository8 and trained the models. Since\nno LSTM-based model is available for the CHES20 dataset, we use their model for the\nASCADr dataset to train on the CHES20 dataset. The models were trained for4K epochs\nusing Adam optimizer and a constant learning rate of1e-4.\n6.4 Training Details and Hyper-parameter settings of EstraNet\n6.4.1 Training details\nWe use the cross-entropy loss and Adam optimizer to train EstraNet. For the learning\nrate schedule, cosine-decay with linear warmup schedule [ZLLS20] has been used. More\nprecisely, we increase the learning rate linearly from0 to 2.5e-4 for twarmup steps of gradient\nupdate and then gradually decreases to0.004 ×2.5e-4 following a cosine curve for the\nremaining tmax −twarmup steps wheretmax is the maximum training steps andtwarmup\n(satisfying twarmup <tmax) is the number of warmup steps. In Appendix D, we describe\nthe learning rate schedule in more detail.\n6.4.2 Hyper-parameter settings\nWe adopted the common hyper-parameters like the number of EstraNet layersL, and\nmodel dimensiond from [HSAM22]. The value dimensiondv and the number of heads\nH in GaussiP attention have been set to32 and 8, respectively. Following [BPS+20] and\n[HSAM22], the kernel width of the ﬁrst convolutional layer has been set to11. The kernel\nwidth of the subsequent convolutional layers has been set to3 (as in [LZC+21]). Unless\nstated otherwise, we set the number of convolutional layers and the pool size of each\naverage-pooling layer to2 and 10, respectively. However, we found that, on the ASCADf\ndataset, a pool size of10 leads to poor performance as the sampling rate of the dataset is\ncomparatively low. Thus, we set it to a slightly smaller value,8. The dimension of the\nfeature map of the GaussiP attention kernel (denoted asde) has been set to512. We set\n8https://github.com/lxj-sjtu/TCHES2021_Pay_attention_to_the_raw_traces\n354 EstraNet\nTable 5: The minimum number of traces (lesser is better) required to reach guessing\nentropy 1 by EstraNet and the benchmark models for trace length10K. The models have\nbeen evaluated on attack traces with attack desync0 (no desync), 200 and 400. The\ncolumns titledBest, Med., andAvg. respectively show the best, median and average results\nof three independently trained models.\nDataset Model Attack Desync0 Attack Desync200 Attack Desync400\nBest Med. Avg. Best Med. Avg. Best Med. Avg.\nPolyCNN 65 68 83 .0 44 54 54 .7 36 44 42 .3\nASCADf EﬀCNN 40 42 44 .7 24 26 26 .0 18 28 24 .7\nLSTM 16 26 30 .7 21 37 32 .3 21 31 35 .7\nEstraNet 13 15 14 .3 12 13 12 .7 9 13 12 .3\nPolyCNN 21 28 28 .0 9 10 10 .7 9 9 10 .3\nASCADr EﬀCNN 32 35 36 .0 15 22 22 .0 16 20 20 .7\nLSTM 5 6 7 .7 6 9 9 .0 6 7 7 .7\nEstraNet 5 5 5 .0 5 6 5 .7 4 5 5 .0\nPolyCNN 15 22 19 .7 15 19 20 .3 16 18 19 .0\nCHES20 EﬀCNN 34 58 58 .0 36 74 62 .3 30 90 74 .0\nLSTM 4 46 47 .3 5 67 47 .0 5 46 45 .7\nEstraNet 5 6 6 .7 5 6 6 .7 4 7 7 .3\ntmax, the maximum training steps, andtwarmup, the warmup steps of EstraNet training to\n4M and 1M for ACSADf and ASCADr datasets. However, using1M as the warmup steps\n(twarmup) leads to unstable training on the CHES20 dataset. Thus, we set it to2M. We\nfound that the scaling hyper-parameterβ2 inﬂuences the performance of EstraNet highly.\nTherefore, we tuned it over three values:10, 50, and150 for each dataset. The rest of the\nhyper-parameter values have been found based on some initial experiments. Section 6.10\ndiscusses the inﬂuence of several important hyper-parameters in EstraNet’s performance.\n6.5 Experimental Setup\nSince we aim to evaluate the robustness of the models to misalignments in the traces, we\ntrained each model on desynchronized proﬁling traces. Data augmentation techniques were\nalso applied by introducing random displacements to each proﬁling trace on the ﬂy during\ntraining. Thus, in diﬀerent epochs, the same proﬁling trace is shifted by diﬀerent values.\nIt should be noted that this data augmentation was also implemented when training the\nbenchmark models. To reduce training time, early stopping of training was employed. In\nother words, we evaluated the trained model at regular intervals during the progress of the\ntraining and stopped the training when no signiﬁcant improvement was observed on the\nvalidation dataset. The intermediate model that exhibited the best performance on the\nvalidation dataset was selected for the ﬁnal evaluation on the test dataset.\nThe guessing entropy of each model on a dataset is computed by repeating the attack\n100 times on randomly permuted traces of the dataset. For each experiment, we repeat\neach model’s training (starting from the random initialization) three times. We report the\nbest, median, and average results of the repeated experiments.\n6.6 Experimental Results for Trace Length10K\nThis section presents a comparative analysis of EstraNet with the benchmark models\nfor the attack window size of10K (refer to Section 6.2 for details on attack window\nselection). To assess the robustness of the deep learning (DL) models against random\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 355\ndelay countermeasures, we applied a proﬁling desynchronization of200. In other words,\nwe independently desynchronized each proﬁling trace by a maximum displacement of\n200. During training, we have used data augmentation. More precisely, we randomly\ndesynchronized each proﬁling trace by a maximum displacement of200 on the ﬂy (refer\nto Section 6.5 for detailed description). The trained models were then evaluated on the\nattack set with attack desynchronizations of0, 200, and400. Each DL model was trained\nindependently three times. Table 5 provides the best, median, and average values ofTGE1\n(the minimum number of traces required to achieve guessing entropy1) obtained from the\nthree trained models for the three attack desynchronization scenarios.\nBased on the observations from the table, it can be noted that on the ASCADf\ndataset, EstraNet consistently outperforms the other three methods. It demonstrates an\nimprovement of more than50% in the majority of cases in terms of all three metrics and\nacross all three attack desynchronization scenarios. On the ASCADr dataset, EstraNet\nperforms signiﬁcantly better than EﬀCNN and similar to LSTMNet with respect to all\nthree metrics. For the dataset, while PolyCNN shows similar performance to EstraNet\nfor attack desynchronizations of200 and 400, it performs poorly compared to EstraNet\nfor attack desynchronization of0. Finally, on the CHES20 dataset, EstraNet showcases\nmore than60% improvement over PolyCNN and EﬀCNN in terms of all three metrics.\nAlthough LSTMNet has comparable results to EstraNet in terms of the bestTGE1 on that\ndataset, it requires6 to 12 times more traces to achieve guessing entropy1 according to\nthe median or average values. In summary, it can be concluded that EstraNet consistently\nperforms signiﬁcantly better, with improvements of over50%, compared to the benchmark\nmodels in the majority of cases, though gains are marginal sometimes. Figure 8 plots the\nmedian guessing entropy of the DL models with respect to the number of attack traces on\nthe three datasets for attack desync400. The plots also support the above observations.\nPolyCNN EﬀCNN LSTMNet EstraNet\n1 10 100\n1\n4\n16\n(a) ASCADf dataset.\n1 10 100\n1\n4\n16\n(b) ASCADr dataset.\n1 10 100\n1\n2\n4\n(c) CHES20 dataset.\nFigure 8:Plots of the guessing entropy vs. the number of attack traces for the experiments\nwith trace length10K. The attack traces are desynchronized by a maximum displacement\nof 400.\n6.7 Experimental Results for Trace Length40K\nThis section presents a comparative analysis with the benchmark models for the attack\nwindow size of40K. In contrast to Section 6.6, where the trained models were evaluated\nagainst smaller attack desynchronizations (0, 200, and 400), this section evaluates the\nmodels in the presence of larger attack desynchronization (600 and 1000). For the\nexperiments, a proﬁling desynchronization of 600 was used, resulting in the random\nshifting of each proﬁling trace by a maximum displacement of600. Furthermore, data\naugmentation was incorporated during training by additionally desynchronizing each\nproﬁling trace on-the-ﬂy with a maximum displacement of400. For each dataset, we\nindependently trained each DL model three times. Table 6 provides the best, median, and\n356 EstraNet\nTable 6: The minimum number of traces (lesser is better) required to reach guessing\nentropy 1 by EstraNet and the benchmark models for trace length40K. The models have\nbeen evaluated on attack traces with attack desyncs600 and 1000. The columns titled\nBest, Med., and Avg. respectively show the best, median and average results of three\nindependently trained models. The ‘-’ entries in the table indicate that the average value is\nnot available as some of the independently trained models failed to reach guessing entropy\n1 using 5K attack traces.\nDataset Model Attack Desync600 Attack Desync1000\nBest Med. Avg. Best Med. Avg.\nPolyCNN 343 534 − 548 553 −\nASCADf EﬀCNN 353 631 − 198 747 −\nLSTM 42 72 242 .3 30 84 164 .7\nEstraNet 21 22 23 .0 22 26 25 .3\nPolyCNN 185 271 482 .7 236 246 541 .7\nASCADr EﬀCNN 41 72 63 .7 49 85 76 .3\nLSTM 823 1158 1292 1260 1722 1794\nEstraNet 23 36 55 .7 28 31 62 .0\nPolyCNN >5K > 5K − >5K > 5K −\nCHES20 EﬀCNN >5K > 5K − >5K > 5K −\nLSTM 2 4 − 3 3 −\nEstraNet 4 11 22 .3 5 7 21 .2\naverage values ofTGE1 (the minimum number of attack traces required to achieve guessing\nentropy 1) obtained from the three trained models.\nIn this scenario, some trained models failed to reach guessing entropy1 using 5K attack\ntraces. Entries with a dash symbol (‘-’) in Table 6 indicate that the averageTGE1 is not\navailable as some of the independently trained models failed to reach guessing entropy1\nusing 5K attack traces. While comparing the results of EstraNet with the benchmark\nmodels on the ASCADf dataset, EstraNet demonstrates a signiﬁcant improvement of at\nleast 90% compared to PolyCNN and EﬀCNN. It also showcases improvements ranging\nfrom 26 to 90% compared to LSTMNet. Moreover, in terms of training stability, EstraNet\nperforms better than the other models as PolyCNN and EﬀCNN fail to reach the guessing\nentropy once out of the three training trials, and LSTMNet performs poorly in terms of the\naverage TGE1. On the ASCADr dataset, EstraNet exhibits substantial improvements of85\nto 90% compared to PolyCNN and LSTMNet for all three metrics. It also demonstrates\nan improvement of10 to 60% compared to EﬀCNN. In the CHES20 dataset, none of the\nPolyCNN and EﬀCNN models could bring downTGE1 below 5K. Although EstraNet\nperforms slightly worse than LSTMNet in terms of the best and medianTGE1 values\non this dataset, it demonstrates more stability with an averageTGE1 of around20. In\ncontrast, in one of the three training trials, LSTMNet failed to bring down theTGE1 below\n5K. In summary, it can be concluded that EstraNet oﬀers signiﬁcant improvements (up\nto 90%) over the benchmark models on the ASCADf and ASCADr datasets. Though\nLSTMSNet performs slightly better than EstraNet in terms of the best and medianTGE1\non the CHES20 dataset, its performance is signiﬁcantly unstable compared to EstraNet.\nFigure 10 visually depicts the median guessing entropy of diﬀerent methods with respect\nto the number of attack traces for attack desync1000, further conﬁrming the observations\nof Table 6.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 357\nPolyCNN EﬀCNN LSTMNet EstraNet\n1 10 100 1,000\n1\n4\n16\n64\n(a) ASCADf dataset.\n1 10 100 1,000\n1\n4\n16\n64\n(b) ASCADr dataset.\n1 10 100 1,000\n1\n2\n4\n8\n(c) CHES20 dataset.\nFigure 10:Plots of the guessing entropy vs. the number of attack traces for the experiments\nwith trace length40K. The attack traces are desynchronized by a maximum displacement\nof 1000.\n1,000 1,500 2,000\nSample points\nAmplitude\nFigure 11: Sample traces after adding clock jitter eﬀect.\n6.8 Experimental Results in the Presence of Clock Jitter Eﬀect\nThis section presents a comparative analysis of EstraNet with the benchmark models\non three datasets with added clock jitter eﬀect9. We pre-processed the traces using the\napproach proposed in [CDP17, WP20] to introduce the clock jitter eﬀect into the datasets.\nAppendix E provides the details of the algorithm employed to add the clock jitter eﬀect.\nFigure 11 illustrates several sample traces after adding the clock jitter eﬀect. As in the\nprevious experiments, the DL models in this section were trained with data augmentation,\nwhere each proﬁling trace was independently desynchronized by a maximum displacement\nof 200 during training. Also, as in the previous experiments, we independently trained\neach DL model three times. Table 7 presents the best, median, and average ofTGE1 (the\nminimum number of attack traces required to achieve guessing entropy1) values obtained in\nthe three training trials. It should be noted that the elastic alignment technique [vWWB11]\nis a well-known method used to align traces aﬀected by the clock jitter. Therefore, for each\ndataset, we trained the benchmark models on the misaligned traces (i.e., traces obtained\nafter adding the clock jitter eﬀect) and the traces obtained after aligning the misaligned\ntraces using elastic alignment. Both sets of results are presented in Table 7.\nAnalysis of the table reveals that on the ASCADf dataset, both PolyCNN and EﬀCNN\nperform poorly on the misaligned traces, although their performance improves signiﬁcantly\nafter elastic alignment. However, they still require5 to 12 times more traces to reach\nguessing entropy1 compared to EstraNet. In contrast, none of the LSTMNet models are\nable to reach guessing entropy1 using 5K traces even after elastic alignment. On the\nASCADr dataset, both PolyCNN and LSTMNet exhibit poor performance. With one\nexception, none of the models can reach guessing entropy1 using 5K traces. Though\nEstraNet fails to reach guessing entropy1 in one of three training trials on the dataset, it\n9It is worth noting that prior works [CDP17, WP20] have examined the eﬀectiveness of DLSCA against\nthe clock jitter eﬀect. However, the work of [WP20] assumes a slightly diﬀerent attack setup where the\nadversary possesses a clean trace alongside each noisy trace in the dataset. In contrast, we consider a\nweaker adversary with no clean traces as in [CDP17].\n358 EstraNet\nTable 7:The minimum number of attack traces (lesser is better) required to reach guessing\nentropy 1 by EstraNet and the benchmark models on the datasets with the added clock\njitter eﬀect. The column titledElastic Alignmentindicates whether the traces have been\naligned using elastic alignment [vWWB11] prior to perform the attack. The columns titled\nBest, Med., and Avg. respectively show the best, median and average results of three\nindependently trained models. The ‘-’ entries in the table indicate that the average value is\nnot available as some of the independently trained models failed to reach guessing entropy\n1 using 5K attack traces.\nDataset Model Elastic TGE1\nAlignment Best Med. Avg.\nPolyCNN No 610 >5K −\nYes 219 412 350 .3\nEﬀCNN No 1814 2384 2327 .7\nASCADf Yes 345 394 562 .3\nLSTM No >5K > 5K −\nYes >5K > 5K −\nEstraNet No 42 42 48 .3\nPolyCNN No >5K > 5K −\nYes 363 >5K −\nEﬀCNN No 1422 1540 2148 .3\nASCADr Yes 405 458 449 .0\nLSTM No >5K > 5K −\nYes >5K > 5K −\nEstraNet No 117 566 −\nPolyCNN No >5K > 5K −\nYes >5K > 5K > 5K\nEﬀCNN No 576 4651 −\nCHES20 Yes >5K > 5K > 5K\nLSTM No >5K > 5K −\nYes 3708 >5K −\nEstraNet No 26 28 32 .0\nshows an improvement of almost70% over EﬀCNN in terms of the bestTGE1. For the\nCHES20 dataset, both PolyCNN and LSTMNet perform poorly, as all of their models\nrequire either more than or close to5K traces to reach the guessing entropy1. Although\nthe best EﬀCNN model requires576 traces to reach guessing entropy1, its other models\nrequire either close to or greater than5K traces for the same. In contrast, all EstraNet\nmodels require only25 to 40 traces to reach the guessing entropy1. In summary, the\nbenchmark models fail to reach guessing entropy1 using 5K attack traces in the majority\nof cases, and even in those cases they perform well, their performance is an order of\nmagnitude worse than that of EstraNet.\nFinally, we would like to highlight that in the presence of clock jitter, the relative\ndistances between the POIs ﬂuctuates signiﬁcantly across diﬀerent traces. Although the\ndesign of GaussiP attention assumes constant relative distances between the POIs in all\ntraces, EstraNet demonstrates better robustness to such ﬂuctuations compared to the\nbenchmark models. This robustness can be attributed to two key factors. Firstly, GaussiP\nattention uses Gaussian attention which is resilient to minor variations in relative distances.\nFor instance, if an attention head assigns signiﬁcant attention to a trace segment[s,t], the\nattention output can remain almost same even if some POIs shift their positions within the\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 359\nTable 8: Comparison of the use of softmax-attention and global average-pooling in\nEstraNet. Both models have been trained thrice on the ASCADf dataset using the setup of\nSection 6.7. We report the number of attack traces required to reach the guessing entropy\n1 in the three training runs of the two models.\nModel Attack Desync 600 Attack Desync 1000\n1 2 3 1 2 3\nEstraNet with\nSoftmax-attention 22 26 21 26 28 22\nEstraNet with global\naverage-pooling 36 101 1734 39 110 1327\nsegment. Secondly, in EstraNet, the input traces propagate though multiple convolutional\nand average-pooling layers before reaching the ﬁrst GaussiP attention layer. As a result,\nsome of the ﬂuctuations in relative distances are absorbed during the propagation through\nthe average-pooling layers. Indeed, the above experimental results indicate that EstraNet\nachieves superior robustness to ﬂuctuations in the relative distances introduced by the\nclock jitter eﬀect compared to the benchmark models.\n6.9 Ablation Study\nThe EstraNet model incorporates two novel layers: the GaussiP attention layer and the\nlayer-centering normalization layer. Additionally, EstraNet utilizes a softmax-attention\nlayer. This section aims to explore the impact of integrating these layers into EstraNet\nand determine their contribution to its improved performance.\n6.9.1 Ablation study of softmax-attention\nIn this section, we assess the performance of EstraNet without using any softmax-attention\naltogether. Thus, we replace the softmax-attention with a global average-pooling layer,\nkeeping all other hyper-parameters same. Table 8 presents a comparison between EstraNet\nwith global average-pooling and the vanilla EstraNet (i.e., EstraNet with softmax-attention)\non the ASCADf dataset using an attack window size of40K. Note that, as before, both\nmodels were trained three times. The table displays the minimum number of attack traces\nrequired to reach the guessing entropy of1 (denoted asTGE1) obtained from the three\ntraining runs for each model.\nFrom the table, we observe that theTGE1 for EstraNet with softmax-attention ranges\nfrom 21 to 28. In contrast, for EstraNet with global average-pooling, it reaches as high as\n1734 in some training runs. This indicates that employing softmax-attention instead of\nglobal average-pooling in EstraNet leads to a signiﬁcant improvement in its performance.\n6.9.2 Ablation study of layer-centering layer\nThis section focuses on assessing the impact of the newly introduced layer-centering layer\nin EstraNet. Toward that goal, we conducted experiments using two other normalization\nmethods, namely layer normalization and batch normalization. We also included the\nresults of EstraNet without any normalization, as [HSAM22] reported its eﬀectiveness in\nTransNet. The pool size hyper-parameter was varied for each normalization method, and\nthe attack performance was evaluated. Each experiment was repeated three times, and\nthe results are presented in Table 9.\nFrom the table, it can be observed that EstraNet with layer-centering maintains consis-\ntent performance across diﬀerent pool sizes. Conversely, for other normalization methods\n360 EstraNet\nor without any normalization, the performance of EstraNet deteriorates signiﬁcantly as the\npool size decreases from 8 to 4. Moreover, in the case of batch normalization, EstraNet fails\nto achieve the guessing entropy of1 using considerably fewer than5K attack traces for all\npool sizes. These results highlight the robustness of EstraNet when using layer-centering\ncompared to other alternatives.\nTable 9:Attack results of EstraNet using various normalization methods. All models have\nbeen independently trained thrice on the ASCADf dataset using the setup of Section 6.7.\nWe report the number of attack traces required to reach the guessing entropy1 in the\nthree training runs of each model.\nNormalization Method Pool size Attack Desync600 Attack Desync1000\n1 2 3 1 2 3\n8 22 26 21 26 28 22\nLayer Centering 6 23 28 17 23 27 17\n4 25 23 35 24 22 35\n8 33 34 29 42 28 21\nNo Normalization 6 18 51 313 37 33 381\n4 117 899 >5K 87 1045 >5K\n8 483 2380 53 274 1735 57\nLayer Normalization 6 4635 67 3854 3612 80 3596\n4 >5K 140 >5K >5K 133 >5K\n8 >5K > 5K > 5K >5K > 5K > 5K\nBatch Normalization 6 >5K 4500 >5K >5K 4138 >5K\n4 >5K > 5K > 5K >5K > 5K > 5K\n6.9.3 Ablation study of the proposed GaussiP attention layer\nThis section assesses the performance of EstraNet by substituting the proposed GaussiP\nattention layer in EstraNet with alternative self-attention layers that incorporate relative\npositional encoding and have linear complexity. In the TN literature, several such self-\nattention layers have been introduced (as discussed in Section 3.3). Due to computational\nconstraints, it is not feasible to verify all of these options. Hence, we select the self-\nattentions proposed in [LCW+21] and [HSGB21] as alternatives to GaussiP attention. We\nemploy the same training setup to train the modiﬁed EstraNet models with the GaussiP\nattentions replaced by the ones proposed in [LCW+21] and [HSGB21]. Figure 12 displays\nthe training loss of the modiﬁed EstraNet models over the course of the initial1.2 million\ntraining steps. The ﬁgures demonstrate that the training loss of the EstraNet models\nwith alternative self-attention layers does not even start to decrease even after1.2 million\ntraining steps, indicating the challenge of training these models eﬀectively. It is worth\nnoting that it might be possible to adopt the existing self-attention layers in the context\nof SCA through non-trivial modiﬁcations. Nonetheless, the results presented in Figure 12\nsuggest that such adaptations is subject to intense research.\n6.10 Choice of the Hyper-parameters\nIn our experiments, we found that setting most of the hyper-parameters of EstraNet to\ntheir default values provides good results. However, selecting the appropriate value for\na few hyper-parameters is crucial for its good performance. This section illustrates the\nchoice of those hyper-parameters.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 361\n2 ·10−2 0.4 0.8 1.2\n·106\n4\n6\n8\nTraining Steps\nTraining Loss\nmodel 1 model 2\nmodel 3\n(a) EstraNet models with self-attention of [HSGB21].\n2 ·10−2 0.4 0.8 1.2\n·106\n4\n6\n8\nTraining Steps\nTraining Loss\nmodel 1 model 2\nmodel 3\n(b)EstraNetmodelswithself-attentionof[ LCW+21].\nFigure 12: Training loss vs. training steps for EstraNet with its self-attention replaced by\nthe self-attentions proposed in [HSGB21] (Figure 12a) and [LCW+21] (Figure 12b).\n6.10.1 Choice of number of heads in GaussiP attention layer\nIn all our previous experiments, we found that setting the number of heads, denoted asH,\nin the GaussiP attention layer to the default value of8 resulted in good performance. In\nthis section, we aim to evaluate the sensitivity of EstraNet’s performance to the choice\nof H. To accomplish this, we trained EstraNet models using diﬀerent values ofH on the\nASCADf datasets while keeping the remaining hyperparameters at their default values.\nSimilar to earlier experiments, we trained three independent EstraNet models for each\nvalue ofH and report theTGE1 (the number of attack traces required to reach guessing\nentropy 1) in Table 10.\nThe table shows that the performance of EstraNet is signiﬁcantly unstable for smaller\nvalues ofH, such as4 and 6. More precisely, forH = 4 and H = 6, EstraNet requires\nup to367 and over5K attack traces, respectively, to reach guessing entropy1, whereas,\nfor the defaultH = 8, it requires fewer than30 traces in the all the three training trials.\nThis unstable behavior of EstraNet can be attributed to the limited ﬂow of information to\ndistant sample points when using smaller values ofH. Indeed, as explained in Section 4.1.3,\nincreasing the number of heads in the GaussiP attention layer allows for more information\nﬂow from one sample point to distant sample points. Therefore, EstraNet performs very\nwell with signiﬁcantly larger values ofH, such as8 and 10. However, the performance\nslightly deteriorates forH = 12. We attribute this deterioration in performance to the\nincreased number of parameters in the GaussiP attention layer when using a very large\nvalue ofH. In conclusion, these experiments reveal that there exists an optimal range for\nthe choice ofH in EstraNet. In our experiments, we have found that anH value within\nthe range of8 −10 consistently performs well across the datasets.\nTable 10:Attack results of EstraNet for the diﬀerent values ofH (the number of heads in\nGaussiP attention) hyper-parameters. As before, for each value ofH, the model has been\nindependently trained thrice on the ASCADf dataset using the setup of Section 6.7. We\nreport the number of attack traces required to reach the guessing entropy1 in the three\ntraining runs.\nH Attack Desync600 Attack Desync1000\n1 2 3 1 2 3\n4 367 23 11 226 29 13\n6 12 >5K 29 14 >5K 25\n8 23 28 17 23 27 17\n10 17 17 22 14 18 20\n12 22 65 28 27 53 28\n362 EstraNet\nTable 11:The minimum number of attack traces (lower is better) required to reach the\nguessing entropy1 on the ASCADf dataset with attack window size10K by EstraNet\nfor diﬀerent values ofβ2. For each experiment, the average result of three independently\ntrained models is reported.\nβ2 Attack Desync\n0 200 400\n300 >5K >5K >5K\n150 19.3 15.0 15.7\n50 70.0 67.3 64.0\n10 >5K >5K >5K\n6.10.2 Choice of distance based scaling in GaussiP attention\nThe selection of the distance-based scaling hyper-parameter, denoted asβ2 in Table 1,\nplays a critical role in achieving satisfactory performance with EstraNet. Table 11 presents\nthe experimental results of attacking the ASCADf dataset using an attack window size of\n10K for four distinct values ofβ2 in EstraNet. The ﬁndings demonstrate the sensitivity of\nEstraNet’s performance to the choice ofβ2. Speciﬁcally, forβ2 = 10 and 300, EstraNet\nfails to attain a guessing entropy of1 even with as many as5K traces, whereas it achieves\nits optimal performance withβ2 = 150. Notably, with a trace length (attack window\nsize) of 10K, EstraNet performs most eﬀectively withβ2 = 150 on the ASCADf and\nCHES20 datasets. However, for the ASCADr dataset, the optimal performance is observed\nwith β2 = 50. In general, for the trace length10K, we observed that EstraNet exhibits\nfavorable performance within the range ofβ2 values between50 and 150. Furthermore,\nour investigations reveal that as the trace length increases from10K to 40K, the range\nof β2 values associated with EstraNet’s optimal performance increases. For instance, for\nthe attack window size of40K, the preferredβ2 values on the ASCADr and CHES20\ndatasets are200 and 450, respectively. Notably, these values are respectively4 and 3 times\nthat of the optimal values obtained for the attack window size of10K. In conclusion, we\nrecommend tuning theβ2 hyper-parameter based on some validation data. We also want\nto mention that the performance of EstraNet generally improves as the value ofβ2 gets\nlarger. However, settingβ2 to a too-large value makes the EstraNet model untrainable.\nIn other words, while training an EstraNet model with a very largeβ2, the training loss\ndoes not fall below the level of random loss. Therefore, we suggest gradually increasing\nthe value ofβ2 during the tuning process until the model becomes untrainable.\n6.10.3 Choice of the feature map dimension of GaussiP attention kernel\nIn all previous experiments, we have utilized a feature map dimension of512 for the\nGaussiP attention kernel (referred to asde in Table 1), which is signiﬁcantly large. However,\nemploying a large value ofde signiﬁcantly increases the memory and compute requirements\nfor training the EstraNet model. Therefore, this section aims to assess the performance of\nEstraNet when using smaller values ofde. Table 12 presents a performance comparison\nbetween EstraNet withde = 512 and EstraNet with reduced feature map dimensions:256\nand 128. The experimental results indicate that the performance of EstraNet with reduced\nfeature map dimensions is comparable to that achieved withde = 512. These ﬁndings\nsuggest that the performance of EstraNet remains stable for a wide range ofde, indicating\nthe possibility of making EstraNet more memory and compute eﬃcient using a smallerde.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 363\nTable 12:The minimum number of attack traces (lower is better) required to reach the\nguessing entropy1 on the ASCADf dataset by EstraNet for diﬀerent values ofde. The\ncolumns titledBest, Med., andAvg. respectively show the best, median and average results\nof three independently trained models.\nde Attack Desync0 Attack Desync200 Attack Desync400\nBest Med. Avg. Best Med. Avg. Best Med. Avg.\n512 14 22 19 .3 8 18 15 .0 9 17 15 .6\n256 13 15 16 .3 9 11 13 .0 12 14 14 .3\n128 12 14 14 .0 11 12 11 .7 12 14 13 .7\nTable 13:Training time of the benchmark models compared to EstraNet.\nDataset EstraNet PolyCNN EﬀCNN LSTMNet\nASCADf 1.0x 5.9x 7.0x 6.3x\nASCADr 1.0x 4.5x 12.8x 4.8x\nCHES20 1.0x 3.0x 5.7x 0.9x\n6.11 Training Time: Comparison with the Benchmarks\nThis section presents a comparative analysis of the training times between EstraNet and\nthe benchmark models. Table 13 provides the comparison for the trace length10K. Upon\nexamining the table, it is evident that the training time of PolyCNN is three to six times\nlonger than that of EstraNet. Similarly, the training time of EﬀCNN is six to thirteen\ntimes greater compared to EstraNet. On the other hand, the training time of LSTMNet\nis approximately ﬁve to six times larger on the ASCADf and ASCADr datasets, while\nit is almost the same on the CHES20 dataset. Hence, with few exceptions, the training\nof EstraNet is approximately three to thirteen times faster than the benchmark models.\nNote that training EstraNet may involve tuning certain hyperparameters (β2 in particular).\nHowever, given that the training time of a single EstraNet model is signiﬁcantly smaller\nthan that of the benchmark models, the hyperparameter tuning process can be completed\nin a comparable time to the training time of the benchmark models.\n6.12 Shift-invariance: Dependence on Data Augmentation\nThe shift-invariance of a DL model is deﬁned for inﬁnite-length input [HSGB21]. However,\nwhen dealing with inputs of ﬁnite length, the model’s shift-invariance tends to break down\nnear the input boundaries. Additionally, incorporating subsampling layers (e.g., average-\npooling layer) into a DL model further diminishes its shift-invariance [Zha19, HSAM22].\nThe reduced shift invariance could lead to overﬁtting during the model’s training. Moreover,\na model that exhibits greater shift-invariance is inherently less reliant on desynchronization\nin proﬁling traces and data augmentation [HSAM22]. This section evaluates the impact of\nthe employed data augmentation on the shift-invariance of EstraNet. Toward that goal,\nwe repeated the experiments of Section 6.6. However, this time, the models are trained\nwithout any data augmentation. We compare the performance of EstraNet while trained\nwith and without data augmentation in Table 14.\nThe ﬁndings of Table 14 reveal that, on the ASCADr and CHES20 datasets, the\nperformance of EstraNet remains almost identical whether or not data augmentation\nis used during training. Conversely, on the ASCADf dataset, the model’s performance\nsigniﬁcantly declines when trained without data augmentation. This observed drop in\nperformance on the ASCADf dataset can be attributed to the relatively lesser number of\nthe proﬁling traces it contains, amounting to only50K as opposed to the200K proﬁling\ntraces present in both the ASCADr and CHES20 datasets. The limited training samples\n364 EstraNet\ncause EstraNet to overﬁt while trained without data augmentation, resulting in signiﬁcantly\npoorer performance than the model trained with data augmentation.\nTable 14:EstraNet’s performance with and without data augmentation. The rest of the\nexperimental setup is the same as in Section 6.6.\nDataset Attack w Data Augmentation w/o Data Augmentation\nDesync Best Med. Avg. Best Med. Avg.\n0 13 15 14 .3 79 390 286 .7\nASCADf 200 12 13 12 .7 139 350 319 .3\n400 9 13 12 .3 124 311 308 .0\n0 5 5 5 .0 6 7 7 .0\nASCADr 200 5 6 5 .7 6 6 6 .0\n400 4 5 5 .0 5 6 6 .3\n0 5 6 6 .7 4 4 7 .3\nCHES20 200 5 6 6 .7 3 6 7 .0\n400 4 7 7 .3 4 4 6 .3\nTable 17 of Appendix F compares the shift-invariance of EstraNet with that of the\nbenchmark models. Like EstraNet, the performance of the benchmark models also vastly\ndeteriorates on the ASCADf dataset while the deterioration is marginal on the ASCADr\nand CHES20 datasets. While comparing the performance of EstraNet to the benchmark\nmodels in the absence of data augmentation, EstraNet performs signiﬁcantly better on\nthe ASCADr and CHES20 datasets. On the ASCADf dataset, PolyCNN and LSTMNet\nfail to reach guessing entropy1 using 5K attack traces while EstraNet and EﬀCNN shows\nsimilar performance.\n7 Limitations and Future Works\nComparing the results for the trace length of40K (Table 6) with those for the trace length\nof 10K (Table 5), it is evident that the performance of EstraNet slightly deteriorates with\nthe increase in the trace length beyond10K though the deterioration is signiﬁcantly less\ncompared to the other DL models. To investigate the decline in EstraNet’s performance\non longer traces further, we conducted additional experiments utilizing the ASCADf\ndataset with a trace length of60K sample points. For the experiments, we employed the\nattack setup of Section 6.7. The results of these experiments are provided in Table 15.\nUpon examination of Table 15, it becomes evident that the performance of EstraNet\nhas deteriorated signiﬁcantly as compared to the performance for the trace lengths of\n10K (Table 5) and40K (Table 6) though the performance is signiﬁcantly better than the\nbenchmark DL models. Therefore, improving EstraNet’s performance further on longer\ntraces (e.g., with a trace length>40K) can be an important research direction to explore.\nThe resilience of a DL model to low SNR traces constitutes a pivotal requirement for\nensuring the model’s eﬃcacy across a wide spectrum of datasets. To assess the robustness\nof EstraNet on low SNR datasets, we conducted further experiments on the ASCADf\ndataset by adding Gaussian noise with standard deviations (std.) of2 and 4. The addition\nof Gaussian noise reduces the peak SNR by approximately30% and 60%, respectively.\nThese experiments were conducted on traces of length10K, employing the experimental\nsetup of Section 6.6. The resulting ﬁndings are presented in Table 16. By comparing the\nresults of EstraNet in Table 16 with those in Table 5, we observe that the performance\nof EstraNet has deteriorated marginally for noise std.2. However, the performance is\nsigniﬁcantly deteriorated for noise std.4, though the performance is remarkably better\nthan the benchmark models. Nonetheless, whether the performance of EstraNet can be\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 365\nTable 15: The minimum number of traces (lesser is better) required to reach guessing\nentropy 1 by the DL models on the ASCADf dataset for trace length60K. The columns\ntitled Best, Med., andAvg. respectively show the best, median, and average results of the\nthree independently trained models. The ‘-’ entries in the table indicate that the average\nvalue is not available as some of the independently trained models failed to reach guessing\nentropy 1 using 5K attack traces.\nModel Attack Desync600 Attack Desync1000\nBest Med. Avg. Best Med. Avg.\nPolyCNN >5K > 5K − >5K > 5K −\nEﬀCNN >5K > 5K − >5K > 5K −\nLSTM 4809 >5K − 4391 >5K −\nEstraNet 111 3342 − 131 3771 −\nimproved further on low SNR scenarios by using a better training method or improving\nthe model architecture can be an interesting future work.\nTable 16: The minimum number of traces (lesser is better) required to reach guessing\nentropy 1 by the DL models on the ASCADf dataset with added Gaussian noise. The\ncolumns titled Best, Med., and Avg. respectively show the best, median, and average\nresults of the three independently trained models. The ‘-’ entries in the table indicate that\nthe average value is not available as some of the independently trained models failed to\nreach guessing entropy1 using 5K attack traces.\nNoise Std. Model Attack Desync0 Attack Desync400\nBest Med. Avg. Best Med. Avg.\nPolyCNN 736 881 947 .3 382 407 459 .0\n2 EﬀCNN 307 394 528 .3 139 210 198 .7\nLSTM 342 935 1267 .7 381 849 973 .7\nEstraNet 26 45 39 .0 27 39 39 .7\nPolyCNN 3322 4080 3915 .7 1847 3180 −\n4 EﬀCNN 3509 3950 3862 .7 2098 2360 2405 .3\nLSTM 3835 4878 − 2539 2716 −\nEstraNet 341 624 731 .0 482 559 630 .0\nThe experimental results of Sections 6.6 and 6.7 reveal that EstraNet exhibits signiﬁcant\nperformance improvements over the benchmark models when faced with random delay or\nclock jitter countermeasure in combination with masking countermeasure. In the presence\nof random delay or masking countermeasures, the relative distances between the POIs\nremain approximately the same. However, in the presence of countermeasures such as\nrandom delay interrupt [CK09] and shuﬄing [HOM06], the relative distances between\nthe POIs vary drastically. It would also be interesting to investigate the adaptation of\nthe TN-based model against these countermeasures. It is worth mentioning that existing\nresearch [DRS+12, WP20] has demonstrated that these countermeasures can signiﬁcantly\ndeteriorate the performance of DL models. However, they have also introduced the\nHidden Markov Model and autoencoder-based methods to denoise traces protected by\nsuch countermeasures, albeit in weaker settings. Similar approaches could be explored in\nconjunction with EstraNet to make it robust to these countermeasures.\nWhen comparing EstraNet with the benchmark models in terms of their attack perfor-\nmance on desynchronized traces, EstraNet demonstrates a remarkable improvement with\nup to90% reduction in the number of attack traces required to reach the guessing entropy\n1. Additionally, EstraNet showcases signiﬁcant improvements over the benchmark models\n366 EstraNet\non the datasets with clock jitter eﬀects. On such scenarios, while the benchmark models\noften struggle to reach the guessing entropy1 even with5K attack traces, EstraNet models\ncan reach it using less than100 traces most of the time. However, it is important to note\nthat this does not imply that CNN-based and RNN-based models cannot be developed\nto perform well on such scenarios. Thus, developing CNN and RNN-based DL models to\nperform well against datasets with highly desynchronized traces and clock jitter eﬀects\ncan be explored in the future.\n8 Conclusions\nDeep learning (DL) models have shown great success in SCA; however, selecting an\nappropriate model architecture remains crucial in achieving good performance. This work\nintroduces a novel DL model called EstraNet for SCA. EstraNet exhibits linear time and\nmemory complexity, signiﬁcantly improving quadratic time and memory complexity of the\npreviously proposed TN-based model, TransNet. This linear complexity makes EstraNet\napplicable to traces with lengths exceeding10K. Moreover, EstraNet is shift-invariant,\nmaking it resilient to misalignments in the traces.\nThe EstraNet architecture incorporates two major contributions. First, we propose a\nnovel attention operation called GaussiP attention, which has linear time and memory\ncosts. By incorporating relative positional encoding within the attention operation,\nEstraNet achieves shift-invariance. Additionally, the sparsity of attention probabilities in\nGaussiP attention makes it particularly suitable for SCA. Second, due to the limitations of\nstandard normalization techniques, such as batch normalization and layer normalization for\nEstraNet, we introduce a novel normalization method called layer-centering. The proposed\nnormalization method improves the stability of EstraNet training signiﬁcantly.\nWe conducted extensive experimental evaluations of EstraNet on three datasets con-\nsisting of masked implementations. We introduced random displacements to the datasets\nto assess EstraNet’s robustness against random delay countermeasure. The results demon-\nstrate that EstraNet achieves up to90% decrease in the number of attack traces required to\nreduce the guessing entropy to1 compared to three benchmark models. When comparing\nthe attack performance of EstraNet with the benchmark models on datasets incorporating\nclock jitter eﬀects, EstraNet provides up to an order of magnitude reduction in the number\nof attack traces required to reach guessing entropy1. Additionally, we investigated the\nimpact of various hyperparameters on EstraNet’s performance. Overall, the experimental\nﬁndings highlight EstraNet’s potential as a promising avenue for advancing SCA research.\nAcknowledgments\nThe authors express their gratitude to the anonymous reviewers for their valuable insights\nand suggestions that greatly contributed to the enhancement of this paper. Additionally,\nthey would like to acknowledge the Prime Minister’s Research Fellowship, India for funding\nthe ﬁrst author and the computing infrastructure. Furthermore, the authors are pleased to\nthank the Department of Science and Technology (DST), Government of India, the IHUB\nNTIHAC Foundation at C3i Building, IIT Kanpur, and the Centre on Hardware-Security\nEntrepreneurship Research & Development (HERD), Meity, India, for partially supporting\nadditional lab infrastructure.\nReferences\n[BBB+20] Davide Bellizia, Francesco Berti, Olivier Bronchain, Gaëtan Cassiers, Sébastien\nDuval, Chun Guo, Gregor Leander, Gaëtan Leurent, Itamar Levi, Charles\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 367\nMomin, Olivier Pereira, Thomas Peters, François-Xavier Standaert, Balazs\nUdvarhelyi, and Friedrich Wiemer. Spook: Sponge-based leakage-resistant\nauthenticated encryption with a masked tweakable block cipher.IACR Trans.\nSymmetric Cryptol., 2020(S1):295–349, 2020.\n[BCO04] Eric Brier, Christophe Clavier, and Francis Olivier. Correlation power analysis\nwith a leakage model. In Marc Joye and Jean-Jacques Quisquater, editors,\nCHES, USA, 2004, volume 3156 ofLNCS, pages 16–29. Springer, 2004.\n[BPS+20] Ryad Benadjila, Emmanuel Prouﬀ, Rémi Strullu, Eleonora Cagli, and Cécile\nDumas. Deep learning for side-channel analysis and introduction to ASCAD\ndatabase. J. Cryptogr. Eng., 10(2):163–188, 2020.\n[CDP17] Eleonora Cagli, Cécile Dumas, and Emmanuel Prouﬀ. Convolutional neural\nnetworks with data augmentation against jitter-based countermeasures - pro-\nﬁling attacks without pre-processing. InCHES, Taiwan, 2017, volume 10529\nof LNCS, pages 45–68. Springer, 2017.\n[CG00] Jean-Sébastien Coron and Louis Goubin. On boolean and arithmetic masking\nagainst diﬀerential power analysis. InCHES, USA, 2000, volume 1965 of\nLNCS, pages 231–237. Springer, 2000.\n[Che21] Peng Chen. PermuteFormer: Eﬃcient relative position encoding for long\nsequences. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\nScott Wen-tau Yih, editors,EMNLP 2021, Dominican Republic, 2021, pages\n10606–10618. ACL, 2021.\n[CK09] Jean-Sébastien Coron and Ilya Kizhvatov. An eﬃcient method for random\ndelay generation in embedded software. InCHES, Switzerland, 2009, volume\n5747 ofLNCS, pages 156–170. Springer, 2009.\n[CLD+21] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou\nSong, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and\nAdrian Weller. Rethinking attention with performers. InICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[CRR02] Suresh Chari, Josyula R. Rao, and Pankaj Rohatgi. Template attacks. In\nBurton S. Kaliski Jr., Çetin Kaya Koç, and Christof Paar, editors,CHES,\nUSA, 2002, volume 2523 ofLNCS, pages 13–28. Springer, 2002.\n[CRW17] Krzysztof Marcin Choromanski, Mark Rowland, and Adrian Weller. The\nunreasonable eﬀectiveness of structured random orthogonal embeddings. In\nIsabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob\nFergus, S. V. N. Vishwanathan, and Roman Garnett, editors,NIPS 2017,\nDecember 4-9, 2017, USA, pages 219–228, 2017.\n[DRS+12] François Durvaux, Mathieu Renauld, François-Xavier Standaert, Loïc van\nOldeneel tot Oldenzeel, and Nicolas Veyrat-Charvillon. Eﬃcient removal of\nrandom delays from embedded software implementations using hidden markov\nmodels. In Stefan Mangard, editor,CARDIS, Austria, volume 7771 ofLecture\nNotes in Computer Science, pages 123–140. Springer, 2012.\n[DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le,\nand Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond\na ﬁxed-length context. InACL, Italy, 2019, pages 2978–2988. Association for\nComputational Linguistics, 2019.\n368 EstraNet\n[GBTP08] Benedikt Gierlichs, Lejla Batina, Pim Tuyls, and Bart Preneel. Mutual\ninformation analysis. In Elisabeth Oswald and Pankaj Rohatgi, editors,CHES,\nUSA, 2008, volume 5154 ofLNCS, pages 426–442. Springer, 2008.\n[GZL19] Maosheng Guo, Yu Zhang, and Ting Liu. Gaussian transformer: A lightweight\napproach for natural language inference. InAAAI 2019, Hawaii, pages 6489–\n6496. AAAI Press, 2019.\n[HOM06] Christoph Herbst, Elisabeth Oswald, and Stefan Mangard. An AES smart\ncard implementation resistant to power analysis attacks. In Jianying Zhou,\nMoti Yung, and Feng Bao, editors,ACNS, Singapore, 2006, volume 3989 of\nLNCS, pages 239–252, 2006.\n[HSAM22] Suvadeep Hajra, Sayandeep Saha, Manaar Alam, and Debdeep Mukhopadhyay.\nTransNet: Shift invariant transformer network for side channel analysis. In\nLejla Batina and Joan Daemen, editors,AFRICACRYPT 2022, Fes, Morocco,\n2022, Proceedings, LNCS, pages 371–396. Springer Nature Switzerland, 2022.\n[HSGB21] Max Horn, Kumar Shridhar, Elrich Groenewald, and Philipp F. M. Baumann.\nTranslational equivariance in kernelizable attention.CoRR, abs/2102.07680,\n2021.\n[KJJ99] Paul C. Kocher, Joshua Jaﬀe, and Benjamin Jun. Diﬀerential power analysis.\nIn CRYPTO, USA, 1999, volume 1666 ofLNCS, pages 388–397. Springer,\n1999.\n[KP22] Sengim Karayalcin and Stjepan Picek. Resolving the doubts: On the construc-\ntion and use of resnets for side-channel analysis.IACR Cryptol. ePrint Arch.,\npage 963, 2022.\n[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.\nTransformers are RNNs: Fast autoregressive transformers with linear attention.\nIn ICML 2020, 13-18 July 2020, Virtual Event, volume 119 ofProceedings of\nMachine Learning Research, pages 5156–5165. PMLR, 2020.\n[LCW+21] Antoine Liutkus, Ondrej Cífka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan\nYang, and Gaël Richard. Relative positional encoding for transformers with\nlinear complexity. In Marina Meila and Tong Zhang, editors,ICML 2021,\nVirtual Event, volume 139 ofProceedings of Machine Learning Research, pages\n7067–7079. PMLR, 2021.\n[LLC+21] Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng,\nGuolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized\nattention with relative positional encoding. In Marc’Aurelio Ranzato, Alina\nBeygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan,\neditors, NeurIPS 2021, 2021, virtual, pages 22795–22807, 2021.\n[LSL+21] Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable fourier\nfeaturesformulti-dimensionalspatialpositionalencoding. InMarc’AurelioRan-\nzato, AlinaBeygelzimer, YannN.Dauphin, PercyLiang, andJenniferWortman\nVaughan, editors,NeurIPS 2021, 2021, virtual, pages 15816–15829, 2021.\n[LZC+21] Xiangjun Lu, Chi Zhang, Pei Cao, Dawu Gu, and Haining Lu. Pay attention\nto raw traces: A deep learning architecture for end-to-end proﬁling attacks.\nTCHES, 2021(3):235–274, 2021.\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 369\n[Mag19] Houssem Maghrebi. Deep learning based side channel attacks in practice.\nIACR Cryptol. ePrint Arch., 2019:578, 2019.\n[MBC+20] Loïc Masure, Nicolas Belleville, Eleonora Cagli, Marie-Angela Cornelie,\nDamien Couroussé, Cécile Dumas, and Laurent Maingault. Deep learning side-\nchannel analysis on large-scale traces - A case study on a polymorphic AES.\nIn ESORICS, UK, 2020, volume 12308 ofLNCS, pages 440–460. Springer,\n2020.\n[MCLS23] Loïc Masure, Valence Cristiani, Maxime Lecomte, and François-Xavier Stan-\ndaert. Don’t learn what you already know: Scheme-aware modeling for proﬁling\nside-channel analysis against masking.IACR Trans. Cryptogr. Hardw. Embed.\nSyst., 2023(1):32–59, 2023.\n[MHM13] Zdenek Martinasek, Jan Hajny, and Lukas Malina. Optimization of power\nanalysis using neural network. InCARDIS, Germany, 2013, volume 8419 of\nLNCS, pages 94–107. Springer, 2013.\n[MPP16] Houssem Maghrebi, Thibault Portigliatti, and Emmanuel Prouﬀ. Breaking\ncryptographic implementations using deep learning techniques. InSPACE,\nIndia, 2016, volume 10076 ofLNCS, pages 3–26. Springer, 2016.\n[MZ13] Zdenek Martinasek and Vaclav Zeman. Innovative method of the power\nanalysis. Radioengineering, 22(2):586–594, 2013.\n[PPY+21] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith,\nand Lingpeng Kong. Random feature attention. InICLR 2021, Virtual Event,\nAustria, 2021. OpenReview.net, 2021.\n[PSK+18] Stjepan Picek, Ioannis Petros Samiotis, Jaehun Kim, Annelie Heuser, Shivam\nBhasin, and Axel Legay. On the performance of convolutional neural networks\nfor side-channel analysis. InSPACE, India, 2018, volume 11348 ofLNCS,\npages 157–176. Springer, 2018.\n[PWP21] Guilherme Perin, Lichao Wu, and Stjepan Picek. Exploring feature selection\nscenarios for deep learning-based side-channel analysis.Cryptology ePrint\nArchive, 2021.\n[RR07] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel\nmachines. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis,\neditors, NIPS, British Columbia, Canada, 2007, pages 1177–1184. Curran\nAssociates, Inc., 2007.\n[SLP05] Werner Schindler, Kerstin Lemke, and Christof Paar. A stochastic model for\ndiﬀerential side channel cryptanalysis. In Josyula R. Rao and Berk Sunar,\neditors, CHES, UK, 2005, volume 3659 ofLNCS, pages 30–46. Springer, 2005.\n[SLP+21] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer:\nEnhanced transformer with rotary position embedding.CoRR, abs/2104.09864,\n2021.\n[SUV18] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with rela-\ntive position representations. InNAACL-HLT, USA, 2018, pages 464–468.\nAssociation for Computational Linguistics, 2018.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, USA, 2017, pages 5998–6008, 2017.\n370 EstraNet\n[vWWB11] Jasper G. J. van Woudenberg, Marc F. Witteman, and Bram Bakker. Improv-\ning diﬀerential power analysis by elastic alignment. In Aggelos Kiayias, editor,\nCT-RSA, USA, 2011, volume 6558 ofLNCS, pages 104–119. Springer, 2011.\n[Wik22] Kernel method — Wikipedia, the free encyclopedia, 2022. [Online; accessed\n29-August-2022].\n[WP20] Lichao Wu and Stjepan Picek. Remove some noise: On pre-processing of\nside-channel measurements with autoencoders.IACR Trans. Cryptogr. Hardw.\nEmbed. Syst., 2020(4):389–415, 2020.\n[XYH+20] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen\nXing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer\nnormalization in the transformer architecture. InICML, 2020, volume 119 of\nProceedings of Machine Learning Research, pages 10524–10533. PMLR, 2020.\n[YSC+16] Felix X. Yu, Ananda Theertha Suresh, Krzysztof Marcin Choromanski,\nDaniel N. Holtmann-Rice, and Sanjiv Kumar. Orthogonal random features.\nIn Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon,\nand Roman Garnett, editors,NIPS 2016, December 5-10, 2016, Spain, pages\n1975–1983, 2016.\n[ZBHV20] GabrielZaid, LilianBossuet, AmauryHabrard, andAlexandreVenelli. Method-\nology for eﬃcient CNN architectures in proﬁling attacks.TCHES, 2020(1):1–36,\n2020.\n[Zha19] Richard Zhang. Making convolutional networks shift-invariant again. InICML,\n2019, USA, volume 97 ofProceedings of Machine Learning Research, pages\n7324–7334. PMLR, 2019.\n[ZLLS20] Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola.Dive into\nDeep Learning. 2020. https://d2l.ai.\n[ZS20] Yuanyuan Zhou and François-Xavier Standaert. Deep learning mitigates but\ndoes not annihilate the need of aligned traces and a generalized resnet model\nfor side-channel attacks.J. Cryptogr. Eng., 10(1):85–95, 2020.\n[ZXF+19] Libang Zhang, Xinpeng Xing, Junfeng Fan, Zongyue Wang, and Suying Wang.\nMulti-label deep learning based side channel attack. InAsianHOST, China,\n2019, pages 1–6. IEEE, 2019.\n[ZXF+21] Libang Zhang, Xinpeng Xing, Junfeng Fan, Zongyue Wang, and Suying Wang.\nMultilabel deep learning-based side-channel attack.IEEE Transactions on\nComputer-Aided Design of Integrated Circuits and Systems, 40(6):1207–1216,\n2021.\nAppendices\nA ApproximationofGaussiPAttention’sNormalizationFac-\ntor\nIn Eq (17), we have used the following approximation:\nn−1∑\nj=0\nkGPA (i−j) =\nn−1∑\nj=0\nexp\n(\n−β2\n2 s2\np(i−j−cpn)2||Wpp||2\n2\n2\n)\n≈ 1\nβ2sp||Wpp||\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 371\nIn this section, we provide a justiﬁcation for the same. Toward that goal, let us deﬁne the\nshifted kernel function as follows:\n¯k(j) = kGPA (j+ cpn) = exp\n(\n−β2\n2 s2\np||Wpp||2\n2j2\n2\n)\nThus, the shifted kernel¯k(j) attains its maximum atj = 0 and decreases monotonically\nas j goes to either+∞ or −∞. Thus, we have\nn−1∑\nj=0\nkGPA (i−j) =\nn−1∑\nj=0\n¯k(i−j−cpn) =\ni−cpn∑\nj=i−cpn−n+1\n¯k(j)\n≈\n∞∑\nj=−∞\n¯k(j) = ¯k(0) +\n−1∑\nj=−∞\n¯k(j) +\n∞∑\nj=1\n¯k(j)\n≈¯k(0) +\n∫0\nj=−∞\n¯k(j) +\n∞∑\nj=0\n¯k(j) = ¯k(0) +\n∫∞\nj=−∞\n¯k(j) = 1 +\n∫∞\nj=−∞\n¯k(j)\n≈\n∫∞\nj=−∞\n¯k(j) =\n√\n2π\nβ2sp||Wpp||\n≈ 1\nβ2sp||Wpp||2\nIn the above derivation, there are four steps of approximations. Note that, in the last\napproximation step, we have ignored the constant factor\n√\n2π of\n√\n2π/β2sp||Wpp||2 as a\nsmall constant factor like2 and 3 can be consumed in the hyper-parameterβ2. In the\nthird approximation step, we have ignored the additive term1 of 1 +\n∞\nj=−∞\n¯k(j). The\napproximation can be justiﬁed as, for longer traces,\n∞\nj=−∞\n¯k(j) ≫ 1 holds. Similarly, the\napproximations in the third approximation step are also quite tight for longer traces. How-\never, the ﬁrst approximation i.e. the approximation of∑i−cpn\nj=i−cpn−n+1 ¯k(j) by ∑∞\nj=−∞\n¯k(j)\ncan be signiﬁcantly loose in some scenarios. To justify the approximation, we will consider\ntwo scenarios. In the ﬁrst scenario, let us assume thati−cpn−n+ 1 ≪ 0 ≪ i−cpn. In\nthat case\ni−cpn∑\nj=i−cpn−n+1\n¯k(j) =\n∞∑\nj=−∞\n¯k(j) −\n\n\ni−cpn−n∑\nj=−∞\n¯k(j) +\n∞∑\nj=i−cpn+1\n¯k(j)\n(\n(\nSince ¯k(j) decays exponentially to0 when j goes further away from0, ∑i−cpn−n\nj=−∞ ¯k(j) +∑∞\nj=i−cpn+1\n¯k(j) remains close to0 in this scenarios. In the second scenario, let us assume\nthat i−cpn≪ 0. Thus, in this scenario\ni−cpn∑\nj=i−cpn−n+1\n¯k(j) ≈0\nas each¯k(j) for j ∈[−cpn−n+ 1,−cpn] is close to0. However, note that normalizing\nthe attention scores by a value close zero can lead to the explosion of the attention\nscores which may lead to the unstable behaviour of the model. On the other hand,\nby approximating∑i−cpn\nj=i−cpn−n+1 ¯k(j) by the upper limit∑∞\nj=−∞\n¯k(j) ≈1/β2sp||Wpp||2\navoids such problem. Overall, we ﬁnd that approximating the normalizing constant∑n−1\nj=0 kGPA (i−j) by 1/β2sp||Wpp||2 leads to better stability of EstraNet.\n372 EstraNet\nFigure 13: The contour plots of the attention scores learned at the eight diﬀerent heads\nof the ﬁrst layer’s GaussiP attention operations of EstraNet on the ASCADf dataset. The\nattention probabilities are represented as a4K×4K image A where A[i,j] represents the\nattention fromqi to kj.\nB Plots of the Attention Probabilities in the GaussiP At-\ntention Heads\nFigure 13 plots the attention scores learned at diﬀerent heads of the ﬁrst layer’s GaussiP\nattention operation of EstraNet on the ASCADr datasets. The value ati-th row and\nj-th column of the images indicates the attention scores fromqi to kj. From the plots,\nwe observe that the attention scores are the same for(i,j)-pairs having the samei−j,\nimplying that the attention scores are the function of the relative distance of thei and\nj. Additionally, the attention scores at each attention head are high for a small range of\nrelative distances and close to zero for the rest showing the sparsity of the attention scores.\nC Detailed Architecture of the Benchmark Models\nPolyCNN The PolyCNN model is a CNN model proposed in [MBC+20]. It has six\nconvolutional blocks followed by a global average-pooling layer. Each convolutional\nblock is composed of a convolutional, a batch-normalization, a ReLU, and an average-\npooling layer. The number of features of the convolutional layers are respectively set to\n10,20,40,40,80 and 100. The kernel width of the ﬁrst convolutional layer is set to10 and\nto 11 for the rest. The pool size and stride of the average-pooling layer are set to25 in the\nﬁrst convolutional block and to5 for the rest. We have used this model for trace length\n40K. However, this model was not applicable to the smaller trace length10K as the trace\nlength becomes1 after the ﬁrst5 convolutional blocks. Thus, we removed the one (third)\nconvolutional block from the original PolyCNN model to use it for trace length10K.\nEﬀCNN In [ZBHV20], Zaid et al. proposed a methodology for constructing CNN models\nrobust to desynchronizations in the traces. Their models have three convolutional blocks\nfollowed by a ﬂattening layer and three fully-connected layers. Each convolutional block is\ncomposed of a convolutional, a SELU activation, a batch-normalization, and an average-\npooling layer. The number of features in the convolutional layers are respectively set to\n32, 64 and 128. The kernel widths of the layers are respectively set to1,T/2,n/I where\nn is the trace length,T is the maximum assumed amount of desynchronization, andI is\nthe assumed number of POIs in the traces. Similarly, the pool sizes and the strides in the\nSuvadeep Hajra, Siddhartha Chowdhury and Debdeep Mukhopadhyay 373\n0 20 40 60 80\ntraining step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0learning rate\nFigure 14: Plots of learning rate vs. training step for cosine decay with linear warmup\nlearning rate schedule.\naverage-pooling layers are respectively set to2,T/2,n/I. For all experiments, we setI to\n10. For the experiments of Sections 6.6 and 6.8, we setT = 400 and for the experiments of\nSection 6.7, we setT = 1000.\nLSTMNet In [LZC+21], Lu et al. have proposed the LSTM-based models. The models for\nthe desynchronized ASCADf and ASCADr datasets have six convolutional blocks, followed\nby bidirectional LSTM layers, followed by softmax-attention. Each convolutional block is\ncomposed of a convolutional layer, a batch-normalization layer, an ELU activation, and an\naverage-pooling layer. The number of ﬁlters of the convolutional layers are respectively set\nto 4,8,16,32,64 and 128. The pool sizes of the average-pooling layers are all set to2. The\nkernel width of all convolutional layers except the ﬁrst one is set to3. The kernel width of\nthe ﬁrst convolutional layer is respectively set to26 and 63 for the ASCADf and ASCADr\ndatasets. [LZC+21] has not provided any model for the CHES20 dataset. Thus, we use\nthe model of the ASCADr dataset to perform the attacks on the CHES20 dataset.\nD Learning Rate Schedule for EstraNet\nIn cosine decay with linear warmup learning rate scheduling, the learning rate is ﬁrst\nlinearly increased from0 to some maximum value, saylmax over the ﬁrsttwarmup steps.\nThen the learning rate is gradually decreased to some minimum valuelmin over the\nremaining tmax −twarmup steps wherelmax and lmin are respectively the maximum and\nminimum learning rate,tmax is the total number of training steps, andtwarmup is the\nnumber of warmup steps. Thus, the learning rate att-th training step is given by\nlt =\n{ t\ntwarmup\n×lmax for t≤twarmup\nlmin + lmax−lmin\n2 ×\n(\n1 + cos\n(\nt−twarmup\ntmax−twarmup\n×π\n))\nfor t>t warmup\nFigure 14 plots the learning rate vs training steps in cosine decay with linear learning\nrate schedule fortmax = 100, twarmup = 10, lmax = 1 and lmin = 0.004.\nE Adding Clock Jitter Eﬀect\nFor adding clock jitter eﬀect, we perform the pre-processing of each trace of the datasets\nas follows. For each sample points in the original trace we perform one of the following\nthree actions each with one-third probability: a) we do not add it in the new trace, b)\nsimply add it or c) add it along with another additional sample points with magnitude\nequal to the average of the sample point and the next sample point. Note that the above\npre-processing is equivalent to adding clock jitter eﬀect using the algorithm of [WP20] with\nthe clock_jitters_level set to1. However, after doing the above pre-processing, none of\n374 EstraNet\nTable 17: The minimum number of traces (lesser is better) required to reach guessing\nentropy 1 by EstraNet and the benchmark models for trace length10K. The models have\nbeen evaluated on attack traces with attack desync0 (no desync), 200, and 400. The\ncolumns titled Best, Med., and Avg. respectively show the best, median, and average\nresults of three independently trained models. During the training of the models, no data\naugmentation is used.\nDataset Model Attack Desync0 Attack Desync200 Attack Desync400\nBest Med. Avg. Best Med. Avg. Best Med. Avg.\nPolyCNN >5K > 5K − >5K > 5K − >5K > 5K −\nASCADf EﬀCNN 277 440 384 .0 257 288 298 .0 730 839 1064 .3\nLSTM >5K > 5K − >5K > 5K − >5K > 5K −\nEstraNet 79 390 286 .7 139 350 319 .3 124 311 308 .0\nPolyCNN 14 19 21 .3 11 17 17 .0 48 84 93 .0\nASCADr EﬀCNN 13 16 17 .7 13 14 15 .0 42 75 64 .3\nLSTM >5K > 5K − >5K > 5K − >5K > 5K −\nEstraNet 6 7 7 .0 6 6 6 .0 5 6 6 .3\nPolyCNN 14 19 19 .3 13 14 17 .7 16 24 22 .3\nCHES20 EﬀCNN 12 22 23 .0 13 27 23 .0 22 26 28 .3\nLSTM 6 17 14 .0 5 14 11 .7 7 16 13 .3\nEstraNet 4 4 7 .3 3 6 7 .0 4 4 6 .3\nthe DL models performed well10 as many informative sample points were getting removed\nfrom the traces during the pre-processing making the informative sample points more\nsparser in the pre-processed traces. We circumvented the loss of informative sample points\nduring the pre-processing as follows. Prior to adding the clock jitter eﬀect in the traces,\nwe double the number of sample points in the traces by repeating each sample points twice.\nNote that such pre-processing is equivalent to doubling the sampling rate of the trace\nacquisition setup. Since, during the addition of clock jitter eﬀect, at most one consecutive\nsample point of the original traces gets removed, by repeating each sample point of the\noriginal trace twice prior to adding clock jitter eﬀect, we make sure that each sample point\nof the original traces appears at least once in the pre-processed traces while having the\nclock jitter aﬀect at the same time.\nF Ablation Study of Data Augmentation\nIn this Section, we investigate the impact of the data augmentation on the shift-invariance\nof the DL models. Toward that goal, we performed the experiments of Section 6.6 without\nany data augmentations. The resultant outcomes are detailed in Table 17. The ﬁndings\nshow a signiﬁcant decline in the performance of all models on the ASCADf dataset while\nthe models are trained without data augmentations. Speciﬁcally, PolyCNN and LSTMNet\nwere unable to reach the guessing entropy1 even using5K attack traces. The performance\nof EstraNet is similar to or signiﬁcantly better than EﬀCNN on the dataset. The LSTMNet\nmodel fails to reach the guessing entropy1 on the ASCADr dataset, although it performed\nclosely to EstraNet on the CHES20 dataset. On the ASCADr and CHES20 datasets,\nEstraNet demonstrated slightly superior performance compared to PolyCNN and EﬀCNN.\nOverall, the better performance of EstraNet than the benchmark models suggests better\nshift-invariance of EstraNet architecture.\n10In fact, on one dataset, only EstraNet worked signiﬁcantly well. The rest of the methods were not\nworking well.",
  "topic": "Jitter",
  "concepts": [
    {
      "name": "Jitter",
      "score": 0.7144677639007568
    },
    {
      "name": "Computer science",
      "score": 0.7107371091842651
    },
    {
      "name": "Shuffling",
      "score": 0.5871194005012512
    },
    {
      "name": "Scalability",
      "score": 0.47313204407691956
    },
    {
      "name": "Algorithm",
      "score": 0.445162296295166
    },
    {
      "name": "Implementation",
      "score": 0.43505656719207764
    },
    {
      "name": "Quadratic equation",
      "score": 0.4237949848175049
    },
    {
      "name": "Computer engineering",
      "score": 0.35267627239227295
    },
    {
      "name": "Theoretical computer science",
      "score": 0.35155627131462097
    },
    {
      "name": "Parallel computing",
      "score": 0.33298343420028687
    },
    {
      "name": "Mathematics",
      "score": 0.15196990966796875
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145894827",
      "name": "Indian Institute of Technology Kharagpur",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 25
}