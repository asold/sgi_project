{
  "title": "Transformer-Based Self-Supervised Learning for Emotion Recognition",
  "url": "https://openalex.org/W4223523352",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223611468",
      "name": "Vazquez-Rodriguez, Juan",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": null,
      "name": "Lefebvre, Gr\\'egoire",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A4223611470",
      "name": "Cumin, Julien",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A4223611471",
      "name": "Crowley, James L.",
      "affiliations": [
        "Centre Inria de l'Université Grenoble Alpes",
        "Institut polytechnique de Grenoble",
        "Centre National de la Recherche Scientifique",
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A4223611469",
      "name": "Lefebvre, Grégoire",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3206603478",
    "https://openalex.org/W3016138882",
    "https://openalex.org/W6763376791",
    "https://openalex.org/W6759836712",
    "https://openalex.org/W3004330901",
    "https://openalex.org/W2778978785",
    "https://openalex.org/W2996229676",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2111072639",
    "https://openalex.org/W2165306728",
    "https://openalex.org/W6751421292",
    "https://openalex.org/W6751956528",
    "https://openalex.org/W3193719728",
    "https://openalex.org/W2999294457",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W3205744895",
    "https://openalex.org/W6779248606",
    "https://openalex.org/W3134831898",
    "https://openalex.org/W3122349645",
    "https://openalex.org/W2948303854",
    "https://openalex.org/W6754988648",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W6753505025",
    "https://openalex.org/W2980621221",
    "https://openalex.org/W6802863937",
    "https://openalex.org/W6862411198",
    "https://openalex.org/W6862644937",
    "https://openalex.org/W6754042814",
    "https://openalex.org/W2599124244",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2790404832",
    "https://openalex.org/W2765362197",
    "https://openalex.org/W4200288811",
    "https://openalex.org/W6760220024",
    "https://openalex.org/W3127142027",
    "https://openalex.org/W3172498913",
    "https://openalex.org/W2002055708",
    "https://openalex.org/W2547146855",
    "https://openalex.org/W2731964405",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3005387090",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W3083367988",
    "https://openalex.org/W3082142255",
    "https://openalex.org/W2901027086",
    "https://openalex.org/W2810418809",
    "https://openalex.org/W3108792608",
    "https://openalex.org/W2980927909",
    "https://openalex.org/W3084283759",
    "https://openalex.org/W6680300913",
    "https://openalex.org/W3088631780",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W4200042752",
    "https://openalex.org/W6773071120",
    "https://openalex.org/W3195146636",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3148757058",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2954731415",
    "https://openalex.org/W2883265831",
    "https://openalex.org/W4393776516",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4289763996",
    "https://openalex.org/W3200271021",
    "https://openalex.org/W4393873952",
    "https://openalex.org/W2917094047",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4393440725",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W4393707904",
    "https://openalex.org/W2807324579",
    "https://openalex.org/W2887997593",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2919854899",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W3002709689",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W3205293163",
    "https://openalex.org/W2945616044"
  ],
  "abstract": "In order to exploit representations of time-series signals, such as physiological signals, it is essential that these representations capture relevant information from the whole signal. In this work, we propose to use a Transformer-based model to process electrocardiograms (ECG) for emotion recognition. Attention mechanisms of the Transformer can be used to build contextualized representations for a signal, giving more importance to relevant parts. These representations may then be processed with a fully-connected network to predict emotions. To overcome the relatively small size of datasets with emotional labels, we employ self-supervised learning. We gathered several ECG datasets with no labels of emotion to pre-train our model, which we then fine-tuned for emotion recognition on the AMIGOS dataset. We show that our approach reaches state-of-the-art performances for emotion recognition using ECG signals on AMIGOS. More generally, our experiments show that transformers and pre-training are promising strategies for emotion recognition with physiological signals.",
  "full_text": "arXiv:2204.05103v2  [q-bio.NC]  3 Jun 2022\nTR A N S F OR M E R -B A S E D SE L F -S U P E RV IS E D LE A R N I N G F O R\nEM OT I O N RE C O G N I T IO N\nJuan V azquez-Rodriguez1,2, Grégoire Lefebvre1, Julien Cumin1 and James L. Crowley2\n1 Orange Labs, Grenoble, France\nEmail: {juan.vazquezrodriguez, gregoire.lefebvre, juli en1.cumin}@orange.com\n2 Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP , LIG, Greno ble, France\nEmail: james.crowley@inria.fr\nABSTRACT\nIn order to exploit representations of time-series signals , such as physiological signals, it is essen-\ntial that these representations capture relevant informat ion from the whole signal. In this work, we\npropose to use a Transformer-based model to process electro cardiograms (ECG) for emotion recog-\nnition. Attention mechanisms of the Transformer can be used to build contextualized representations\nfor a signal, giving more importance to relevant parts. Thes e representations may then be processed\nwith a fully-connected network to predict emotions.\nT o overcome the relatively small size of datasets with emoti onal labels, we employ self-supervised\nlearning. W e gathered several ECG datasets with no labels of emotion to pre-train our model, which\nwe then ﬁne-tuned for emotion recognition on the AMIGOS data set. W e show that our approach\nreaches state-of-the-art performances for emotion recogn ition using ECG signals on AMIGOS.\nMore generally, our experiments show that transformers and pre-training are promising strategies\nfor emotion recognition with physiological signals.\n1 Introduction\nWhen processing time-series signals with deep learning app roaches, it is useful to be able to aggregate information\nfrom the whole signal, including long-range information, i n a way that the most relevant parts are given more im-\nportance. One way of doing this is by employing an attention m echanism [1] that uses attention weights to limit\nprocessing to relevant contextual information, independe nt of distance.\nArguably, the Transformer [2] is one of the most successful a ttention-based approaches. Developed for Natural Lan-\nguage Processing (NLP), the Transformer uses attention mec hanisms to interpret sequences of words, and is suitable\nfor use in other tasks requiring interpretation of sequence s, such as time series forecasting, [3], analysis of medical\nphysiological signals [4, 5], and recognition of human acti vity from motion [6].\nPhysiological signal analysis can be seen as a form of time-s eries analysis and are thus amenable to processing with\nTransformers. Moreover, these signals can be used to predic t emotions [7], and sensors for these types of signals can\nbe incorporated into wearable devices, as a non-invasive me ans for monitoring the emotional reaction of users. Several\nworks in this direction have emerged using signals like elec trocardiograms (ECG) [8, 9], electroencephalograms (EEG)\n[10, 11], electrodermal activity (EDA) [12], and other type s of physiological signals [13, 14].\nEstablished approaches for deep learning with Convolution s and Recurrent networks require large datasets of labeled\ntraining data. However, providing ground truth emotion lab els for physiological data is a difﬁcult and expensive\nprocess, limiting the availability of data for training [15 , 16, 17]. Pre-training models with self-supervised learni ng\ncan help to overcome this lack of labeled training data. With such an approach, during pre-training the model learns\ngeneral data representations using large volumes of unlabe led data. The model is then ﬁne tuned for a speciﬁc task\nusing labeled data. This approach has been successfully use d in other domains including NLP [18, 19] and Computer\nV ision [20, 21]. It has also been successfully used in affect ive computing, in tasks like emotion recognition from\nphysiological signals [9, 22] and from speech [23], persona lity recognition [24], and facial expression recognition\n[25, 26, 27].\nIn this paper, we address the problem of predicting emotions from ECG signals. W e are interested in obtaining\ncontextualized representations from these signals using a Transformer-based architecture, and then using these rep-\nresentations to predict low/high levels of arousal and vale nce. W e believe that the contextualized representations\nobtained with the Transformer should capture relevant info rmation from the whole signal, which the performance of\nthe downstream task of emotion recognition should beneﬁt fr om. Our main contributions are: 1. W e show that it is\nfeasible to use a Transformer-based architecture for emoti on prediction from ECG signals. 2. W e show that using a\nself-supervised technique to pre-train the model is useful for ECG signals, achieving superior performance in emo-\ntion recognition than a fully-supervised approach. 3. W e sh ow that our pre-trained Transformer-based model reaches\nstate-of-the-art performances on a dataset of the literatu re.\n2 Related W ork\nTraditional techniques for emotion recognition from physi ological signals include Gaussian naive Bayes, Support\nV ector Machines, k-Nearest Neighbours, and Random Forests . [16, 17, 28, 29, 30, 31]. These approaches typically\nuse manually-selected time and frequency features derived from intuition and domain knowledge. Shukla et al. [12]\nshow that commonly used features for arousal and valence pre diction are not necessarily the most discriminant. This\nillustrates the difﬁculty of selecting good hand-crafted f eatures.\nT o overcome this, researchers have increasingly used deep l earning techniques to extract features from physiological\nsignals for emotion recognition. A common approach, descri bed by Santamaria et al. [8], is to use a 1D Convolutional\nNeural Network (CNN) to extract the features (also called re presentations), followed by a fully-connected network\n(FCN) used as classiﬁer to predict emotions. As an alternati ve, Harper and Southern [32] use a Long Short-T erm\nMemory (LSTM) network concurrently with a 1D-CNN. Siddhart h et al. [33], ﬁrst convert signals into an image using\nspectrograms [34], and then use a 2D-CNN for feature extract ion, followed by an extreme learning machine [35] for\nclassiﬁcation.\nOne drawback of these CNN-based approaches is that they do no t take context into account: after training, kernel\nweights of the CNN are static, no matter the input. For this re ason, attention-based architectures such as the Trans-\nformer [2], capable of incorporating contextual informati on, have started to be used for emotion prediction. Trans-\nformers have been successfully used to recognize emotions w ith multimodal inputs composed of text, visual, audio\nand physiological signals [36, 37, 38, 39, 40]. In addition, Transformers have been used to process time-series in gen-\neral [3, 41], and also to process uni-modal physiological si gnals in particular, with the aim of recognizing emotions.\nArjun et al. [42] employ a variation of the Transformer, the V ision Transformer [43] to process EEG signals for emo-\ntion recognition, converting the EEG signals into images us ing continuous wavelet transform. Behinaein et al. [44]\npropose to detect stress from ECG signals, by using a 1D-CNN f ollowed by a Transformer and a FCN as classiﬁer.\nMost of the approaches for measuring emotions, including th ose using multimodal physiological data, have relied on\nsupervised learning, and thus are limited by the availabili ty of labeled training data. Using self-supervised pre-tra ining\ncan improve performances of a model [45], as it allows to lear n more general representations, thus avoiding overﬁtting\nin the downstream task. This is especially important for tas ks with limited labeled data. Sarkar and Etemad [9] pre-\ntrain a 1D-CNN using a self-supervised task to learn represe ntations from ECG signals. Their self-supervised task\nconsists in ﬁrst transforming the signal, with operations s uch as scaling or adding noise, and then using the network\nto predict which transformation has been applied. Ross et al . [22] learn representations from ECG signals using auto-\nencoders based on 1D-CNN. In both approaches, once the repre sentations have been learned, they are used to predict\nemotions.\nIn contrast with the two previously mentioned approaches, w e propose to take into account contextual information dur-\ning pre-training by using a Transformer-based model. Such a n approach has been used for pre-training Transformers\nfrom visual, speech and textual modalities [23, 46, 47, 48, 4 9]. Haresamudram et al. use this approach to pre-train a\nTransformer for human activity recognition using accelero meter and gyroscope data [6]. Zerveas et al. [50] develop a\nframework for multivariate time-series representation le arning, by pre-training a Transformer-based architecture . How-\never, none of these works deal with uni-modal physiological signals. In this work, we have extended this approach for\nuse with ECG signals. Speciﬁcally, we investigate the effec tiveness of pre-training a Transformer for ECG emotion\nrecognition, which to the best of our knowledge has not been d one before.\n2\nINPUT ENCODER\n(1D-CNN)\n+\nPositional\nEncoding\nTRANSFORMER\nENCODER\nFULL Y -CONNECTED\nMasked values predictor\nMasked Signal\nSIGNAL\nENCODER\nContextualized\nRepresentations\nMasked V alues\nPrediction\nW eights\nTransfer\nINPUT ENCODER\n(1D-CNN)\n+\nPositional\nEncoding\nTRANSFORMER\nENCODER\nFULL Y -CONNECTED\nEmotion classiﬁer\nRaw Signal\nSIGNAL\nENCODER\n(Pre-trained)\nAggregated\nContextualized\nRepresentation\n(eCLS )\nEmotion\nPrediction\n(a) (b)\nFigure 1: Our approach with self supervised learning based o n a Transformer (a) and ﬁne-tuning strategy for learning\nthe ﬁnal emotion predictor (b).\n3 Our approach\nOur framework for using deep learning for emotion recogniti on is based on the following two steps: ﬁrst, we need to\nobtain contextualized representations from time-series s ignals using a deep model; then, we use those representation s\nto perform the targeted downstream task. In this paper, the c onsidered physiological time-series are raw ECG signals,\nand the downstream task is binary emotion recognition: pred icting high/low levels of arousal, and high/low levels of\nvalence.\nFor the ﬁrst step (see Figure 1.a), we developed a signal enco der based on deep neural networks and attention, to\nobtain contextualized representations from ECG signals. T he main component of the signal encoder is a Transformer\n[2]. This signal encoder is pre-trained with a self-supervi sed task, using unlabeled ECG data. For the second step (see\nFigure 1.b), we ﬁne-tune the whole model (the signal encoder and the fully-connected classiﬁer) for our downstream\ntask of binary emotion recognition, using labeled ECG data.\nIn the following subsections, we describe in detail the diff erent components of our approach.\n3.1 Learning Contextualized Representations\nAt the heart of our signal encoder is a Transformer encoder [2 ], which we use to learn contextualized representations of\nECG signals. In Transformers, contextual information is ob tained through an attention mechanism, with the attention\nfunction considered as a mapping of a query vector along with a group of key-value vector pairs to an output. In\nthe case of the Transformer encoder, each position in the out put pays attention to all positions in the input. Several\nattention modules (also called heads) are used, creating various representation subspaces and i mproving the ability\nof the model to be attentive to different positions. The Tran sformer encoder is constructed by stacking several layers\ncontaining a multi-head attention module followed by a full y-connected network applied to each position, with residua l\nconnections. Since our implementation of the Transformer i s almost identical to the one described in [2], we refer the\nreaders to this paper for further details.\nIn Figure 2, we present our signal encoder, which we describe in the remainder of this subsection.\nInput Encoder: to process an ECG signal with the Transformer, we ﬁrst encode it into s feature vectors of dimension\ndmodel that represent each one of the s values of the ECG signal. W e use 1D Convolutional Neural Netw orks (1D-CNN)\nto perform this encoding, like in [6, 36, 51]. Thus, for a raw i nput signal X = {x1, ..., x s} where xi is a single value,\nafter encoding X with the input encoder we obtain features F = {f1, ..., f s} where fi ∈ Rdmodel .\nCLS token: given that our downstream task is a classiﬁcation task, we ne ed to obtain a single representation of the\nwhole processed signal at the output of our signal encoder. S imilar to what is done in BER T [19], we append a special\nclassiﬁcation token (CLS) at the start of the feature sequen ce F , resulting in the sequence F ′ = {CLS, f 1, ..., f s}. W e\nuse a trainable vector of dimension dmodel as CLS token. At the output of the Transformer, we obtain an em bedding of\nthe CLS token ( eCLS ), along with the rest of the representations of the signal (s ee Figure 2 and Equation 2). Through the\n3\nCLS CNN CNN CNN ... Input Encoder\nFeatures F’ + + + + +\nPositional\nEncodingFeatures Z\nTRANSFORMER ENCODER\nContextualized\nRepresentations E\neCLS e1 e2 e3 ...\nFigure 2: Our Transformer-based signal encoder that produc es contextualized representations. The aggregated repre-\nsentation eCLS is used for classiﬁcation.\nattention mechanisms of the Transformer, eCLS is capable of aggregating information from the entire input signal and\nits contextualized representations. For this reason, at cl assiﬁcation time, eCLS can be used as input for the classiﬁer\nnetwork.\nP ositional Encoding: positional information of each input is required so that the Transformer can take into account the\nactual ordering of time-steps in the input sequence. As in [2 ], we use ﬁxed sinusoidal positional embeddings. W e sum\nthe positional embeddings with the features F ′:\nZ = {CLS + pe0, f 1 + pe1, ..., f s + pes}, (1)\nwhere pei ∈ Rdmodel is the positional embedding for time-step i. W e then apply layer normalization [52] to Z. Please\nrefer to [2] for details on how to obtain the positional embed dings.\nT ransformer Encoder: we obtain contextualized representations E using a Transformer encoder with h heads and l\nlayers on the sequence Z:\nE = {eCLS , e 1, ..., e s} = Transformerh,l(Z). (2)\nW e then use the representations E for emotion recognition, as is described in Section 3.3\n3.2 Pre-training T ask\nT o pre-train our signal encoder, we employ a self-supervise d approach inspired in BER T [19]. W e mask random\nsegments of a certain length by replacing them with zeros, an d then we train our model to predict the masked values,\nas shown in Figure 1a. Labeled data is not needed for this step .\nSimilar to [51], a proportion p of points is randomly selected from the input signal as start ing points for masked\nsegments, and then for each starting point the subsequent M points are masked. The masked segments may overlap.\nT o predict masked points, we use a fully-connected network ( FCN) on top of the signal encoder, as shown in Figure 1a.\nW e only predict values of masked inputs, as opposed to recons tructing the whole signal. W e use the mean square error\nbetween predicted and real values as the reconstruction los s Lr during pre-training:\nLr = 1\nNm\nNm∑\nj=1\n(ˆxj − xp(j))2, (3)\nwhere Nm is the number of masked values, ˆxj is the prediction corresponding to the jth masked value, and xp(j) is\nthe original input value selected to be the jth masked value, whose position is p(j) in the input signal.\n3.3 Fine-tuning\nW e ﬁne-tune our model to perform binary emotion prediction, as shown in Figure 1b. This step is supervised, using\nlabeled data. T o make the prediction, a FCN is added on top of t he signal encoder, using eCLS as input. W e initialize\nthe signal encoder with the weights obtained after pre-trai ning, while the FCN is randomly initialized. W e then\nﬁne-tune all the parameters of the model, including the pre- trained weights. For this task, we minimize the binary\ncross-entropy loss Lft :\nLft = − wpy log[σ(out)] − (1 − y) log[1− σ(out)] (4)\nwhere y is an indicator variable with value 1 if the class of the groun d truth is positive and 0 if it is negative, out is the\noutput of the classiﬁer, σ is the sigmoid function, and wp is the ratio of negative to positive training samples, used t o\ncompensate unbalances that may be present in the dataset.\n4\nT able 1: Comparison of different strategies of our approach on AMIGOS dataset\nArousal Acc. Arousal F1 V alence Acc. V alence F1\nAggregation Method\nMax-Pooling 1 0.85± 6.6e− 3 0.84± 6.4e− 3 0.78± 6.5e− 3 0.78± 6.6e− 3\nMax-Pooling 2 0.86± 7.4e− 3 0.84± 7.3e− 3 0.8± 6.3e− 3 0.8± 5.9e− 3\nA verage-Pooling 1 0.87± 8.3e− 3 0.87± 7.3e− 3 0.82± 6.2e− 3 0.82± 6.7e− 3\nA verage-Pooling 2 0.88± 4.4e− 3 0.87± 4.6e− 3 0.83± 6.4e− 3 0.83± 6.6e− 3\nLast Representation 0.85± 1.3e− 2 0.84± 1.2e− 2 0.8± 7.6e− 3 0.8± 8.0e− 3\nSegment Length 40 seconds 0.86± 1.2e− 2 0.85± 1.1e− 2 0.82± 1.0e− 2 0.81± 9.9e− 3\n20 seconds 0.87± 5.6e− 3 0.86± 6.4e− 3 0.82± 7.8e− 3 0.82± 8.1e− 3\nOur Best Approach CLS with 10s segment 0.88± 5.4e− 3 0.87± 5.4e− 3 0.83± 7.8e− 3 0.83± 7.4e− 3\nT able 2: No Pre-training vs pre-trained model\nPre-train Arousal Acc. Arousal F1 V alence Acc. V alence F1\nNo 0.85± 5.6e− 3 0.84± 5.8e− 3 0.8± 6.5e− 3 0.8± 6.4e− 3\nY es 0.88± 5.4e− 3 0.87± 5.4e− 3 0.83± 7.8e− 3 0.83± 7.4e− 3\n4 Experimental Setup\nIn this section, we describe the experimental choices taken to evaluate our approach for a downstream task of binary\nemotion recognition (high/low levels of arousal and valenc e), on ECG signals. W e present the datasets used, the\npre-processes employed, and the parametrization of our two steps of pre-training and ﬁne-tuning.\n4.1 Datasets\nFor pre-training, we only require datasets that contain ECG signals, regardless of why they were actually collected or\nwhich labeling they have, if any. The datasets that we use in o ur experiments are: ASCER T AIN [16], DREAMER [53],\nPsPM-FR [54], PsPM-HRM5 [55], PsPM-RRM1-2 [56], and PsPM-V IS [57]. W e also employ the AMIGOS dataset\n[17], taking care of not using the same data for pre-training and evaluating our model, as this dataset is also used for\nthe downstream task. T o gather as much data as possible, we us e all the ECG channels available in the datasets. For\nASCER T AIN, we discard some signals according to the quality evaluation provided in the dataset: if a signal has a\nquality level of 3 or worse in the provided scale, it is discar ded. In total, there are around 230 hours of ECG data for\npre-training.\nT o ﬁne-tune our model to predict emotions, we use the AMIGOS d ataset [17]. In this dataset, 40 subjects watched\nvideos specially selected to evoke an emotion. After watchi ng each video, a self-assessment of their emotional state is\nconducted. In this assessment, subjects rated their levels of arousal and valence on a scale of 1 to 9. Of the 40 subjects,\n37 watched a total of 20 videos, while the other 3 subjects wat ched only 16 videos. During each trial, ECG data were\nrecorded on both left and right arms. W e use data only from the left arm to ﬁne-tune our model. AMIGOS includes\na pre-processed version of the data, that was down-sampled t o 128Hz and ﬁltered with a low-pass ﬁlter with 60Hz\ncut-off frequency. W e use these pre-processed data for our e xperiments, including the pre-training phase. The ECG\ndata that we use for ﬁne-tuning amounts to around 65 hours of r ecordings.\n4.2 Signal Pre-processing\nW e ﬁrst ﬁlter signals with an 8 th order Butterworth band-pass ﬁlter, having a low-cut-off fr equency of 0.8Hz and a\nhigh-cut-off frequency of 50Hz. W e then down-sample the sig nals to 128 Hz, except for AMIGOS which already has\nthat sampling rate. Signals are normalized so they have zero -mean and unit-variance, for each subject independently.\nSignals are ﬁnally divided into 10-second segments (we also report results for segments of 20 seconds and 40 seconds).\n4.3 Pre-training\nAs stated previously, we use ASCER T AIN, DREAMER, PsPM-FR, P sPM-RRM1-2, PsPM-VIS, and AMIGOS for\npre-training. Since we also use AMIGOS for ﬁne-tuning, we ne ed to avoid using the same segments both for pre-\ntraining and for evaluating the model. T o do this, we pre-tra in two models, one using half of the data from AMIGOS,\n5\nand the second using the other half. When testing our model wi th certain segments from AMIGOS, we ﬁne-tune the\nmodel that was pre-trained with the half of AMIGOS that do not contain those segments. More details are given in\nSection 4.4. In total, both of our models are pre-trained wit h 83401 10-second segments.\nW e select a proportion of p = 0. 0325 points from each input segment to be the starting point of a ma sked span of\nlength M = 20, resulting in around 47% of the input values masked.\nThe input encoder is built with 3 layers of 1D-CNN with ReLU ac tivation function. W e use layer normalization [52]\non the ﬁrst layer, and at the output of the encoder. Kernel siz es are (65, 33, 17), the numbers of channels are (64,\n128, 256) and the stride for all layers is 1. This results in a r eceptive ﬁeld of 113 input values or 0.88s. W e selected\nthis receptive ﬁeld size because it is comparable with the ty pical interval between peaks on an ECG signal, which is\nbetween 0.6s and 1s, including when experiencing emotions [ 58].\nThe Transformer in our signal encoder has a model dimension dmodel = 256, 2 layers and 2 attention heads, with its\nFCN size of dmodel ·4 = 1024. The FCN used to predict the masked values consists of a singl e linear layer of size\ndmodel / 2 = 128followed by a ReLU activation function. An additional linea r layer is used to project the output vector\nto a single value, which corresponds to the predicted value o f a masked point.\nW e pre-train the two models for 500 epochs, warming up the lea rning rate over the ﬁrst 30 epochs up to a value of\n0.001 and using linear decay after that. W e employ Adam optim ization, with β1 = 0. 9, β2 = 0. 999, and L2 weight\ndecay of 0.005. W e use dropout of 0.1 at the end of the input enc oder, after the positional encoding, and inside the\nTransformer.\nW e tuned the number of layers and heads in the Transformer, th e learning rate, and the warm-up duration using the\nRay Tune framework [59] with BOHB optimization [60].\n4.4 Fine-T uning\nW e ﬁne-tune our model (both the signal encoder and FCN classi ﬁer) for emotion recognition with the AMIGOS\ndataset, using each of the 10-second segments as a sample. As labels, we use the emotional self-assessments given\nin the dataset. Since these assessments provide values of ar ousal and valence on a scale 1 to 9, we use the average\narousal and the average valence as threshold value to determ ine a low or a high level.\nW e use 10-fold cross-validation to evaluate our approach. R ecall that we pre-train two signal encoders. After dividing\nAMIGOS into 10 folds, we use folds 1 to 5 to pre-train one signa l encoder ( SE1), and folds 6 to 10 to pre-train the\nsecond one ( SE2) (and all data from the other datasets, for both). Then, when we ﬁne-tune the models to be tested\nwith folds 1 to 5, we use the weights from SE2 to initialize the signal encoder parameters. In a similar fa shion, we\nuse SE1 as initialization point of the signal encoder when we ﬁne-tu ne the models to be tested with folds 6 to 10.\nThis method allows us to pre-train, ﬁne-tune and test our mod el in a more efﬁcient way than pre-training 10 different\nmodels, one for each fold, while retaining complete separat ions between training and testing data.\nThe FCN classiﬁer used to predict emotions has two hidden lay ers of sizes [1024, 512] with ReLU activation functions,\nand an output layer that projects the output to a single value . W e ﬁne-tune one model to predict arousal and another\nto predict valence. For each task, we ﬁne-tune our model for 1 00 epochs using Adam optimization, with β1 = 0. 9,\nβ2 = 0. 999 and L2 weight decay of 0.00001. W e start with a learning rate of 0.00 01, and decrease it every 45 epochs\nby a factor of 0.65. W e keep using a dropout of 0.1 at the end of t he input encoder, after the positional encoding, and\ninside the Transformer. W e use dropout of 0.3 in the FCN class iﬁer.\nW e used Ray Tune with BOHB, as we did on pre-training, to tune t he learning rate, the learning rate schedule, the\nshape and dropout of the FCN classiﬁer, and the L2 weight decay.\n5 Results\nIn our results, we use as metrics the mean accuracy and mean F1 -score between positive and negative classes. W e\nreport the mean and conﬁdence intervals of the metrics acros s our 10 folds of cross-validation. The conﬁdence intervals\nare calculated using a t-distribution with 9 degrees of free dom, for a two-sided 95% conﬁdence.\n5.1 Comparing Aggregation Methods and Segment Lengths\nW e report in T able 1 the performances of our approach for diff erent strategical choices. Firstly, we compare different\naggregation approaches to combine the contextualized repr esentations at the output of the signal encoder, given to the\nFCN classiﬁer. Secondly, we compare performances for diffe rent segment lengths used to divide the input signals.\n6\nT able 3: Comparison of different methods on AMIGOS dataset\nModel Subj. Ind. Input Seg. Size Arousal\nAcc. Arousal F1 V alence\nAcc. V alence F1\nV arious\nexperiment\nprotocols\nGaussian Naive Bayes [17] Y es 20 seconds - 0.551 - 0.545\n1D-CNN [8] No 200 peaks 0.81 0.76 0.71 0.68\n2D-CNN [33] Y es Not segmented 0.83 0.76 0.82 0.80\n1D-CNN with LSTM [32] Y es Not segmented - - 0.81 0.80\nConvolutional autoencoder [22] No 10 seconds 0.85 0.89 - -\nOur protocol Pre-trained CNN [9] No 10 seconds 0.85± 5.4e− 3 0.84± 5.3e− 3 0.77± 5.5e− 3 0.77± 5.1e− 3\nPre-trained T ransformer (ours) No 10 seconds 0.88± 5.4e− 3 0.87± 5.4e− 3 0.83± 7.8e− 3 0.83± 7.4e− 3\nAggregation Method: we compared 4 strategies for aggregating representations, to be given as input to the FCN: max-\npooling, average-pooling, using only the last representat ion es, and using only the embedding of the CLS token eCLS\n(we call this strategy CLS). Max-pooling 1 and average-pool ing 1 are the result of max-pooling and average-pooling\nacross all representations, to obtain a single representat ion of size dmodel = 256. Max-pooling 2 was optimized on\nthe validation set: representations are reduced to a size of 64, divided into two groups, then max-pooling was applied\non each group and the results concatenated to obtain a single representation of size 128. A verage-pooling 2 was\noptimized on the validation set: representations are divid ed into 4 groups, average-pooling is applied on each group\nand the results concatenated to obtain a single representat ion of size 1024.\nW e see in T able 1 that the best results were obtained with aver age-pooling strategies and with CLS, with accuracies\nup to 0. 88 for arousal, for example. In the following experiments, we w ill thus use CLS as our aggregation method.\nIndeed, although results are practically identical for CLS and average-pooling 2 (e.g. 0.88 ± 5. 4e−3 compared to\n0.88± 4. 4e−3 accuracies for arousal), CLS has the advantage of being a com monly-used strategy for Transformers,\nwhich does not require any kind of tuning on validation data, contrary to average-pooling 2.\nSegment length: we compare 3 different segment lengths for dividing ECG sign als into input instances: 10, 20, and 40\nsecond segments. W e can see in T able 1 that shorter segments l ead to better results on average, both for arousal and\nvalence. For example for arousal, 10-second segments lead t o an accuracy of 0.88 ± 5. 4e−3, compared to 0.87 ± 5. 6e−3\nfor 20-second segments, and 0.86 ± 1. 2e−2 for 40-second segments.\nT wo explanations emerge for this observation: ﬁrstly, sinc e emotions are relatively volatile states, longer segmenta tion\nmight cover ﬂuctuating emotional states, thus making it har der to characterize emotion; secondly, longer segments\nshould require more complex models (i.e. bigger Transforme r and FCN), which are harder to train with the relatively\nrestricted amount of labeled data in AMIGOS. Moreover, shor ter segments are faster to process, allowing a high\nnumber of training epochs and smaller learning rates. In the following experiments, we will thus use 10-second\nsegments.\n5.2 Effectiveness of Pre-training\nT o demonstrate the effectiveness of our pre-training appro ach, we tested our architecture by ﬁne-tuning our model on\nAMIGOS with all parameters randomly initialized, instead o f using a pre-trained signal encoder (thus skipping step\n(a) of our process in Figure 1). As reported in T able 2, the pre -trained model is on average signiﬁcantly better than the\nmodel with no pre-training, for both accuracy and F1-score. For example, for arousal, the pre-trained model reaches\nan average accuracy of 0. 88 ± 5. 4e−3, compared to 0. 85 ± 5. 6e−3 for the model with no pre-training. These results\nillustrate the beneﬁts of pre-training Transformers for ou r task. Moreover, during our experiments, we observed that\nthe model with no pre-training had a tendency to overﬁt quick ly, which was not the case for the pre-trained model.\nPre-training the model on many different datasets should in crease its robustness to overﬁtting when ﬁne-tuning on a\nspeciﬁc dataset.\n5.3 Comparisons With Other Approaches\nW e report in T able 3 various state-of-the-art results for em otion recognition from ECG signals on the AMIGOS dataset.\nThe ﬁrst section of the table contains results from works whi ch all use different experiment protocols, such as differen t\nsegment sizes, different separations of data into training and test sets, subject dependent and independent evaluatio ns,\netc. These results are therefore not directly comparable wi th one another, nor are they directly comparable with ours.\nNevertheless, we report them to showcase the variety of stat e-of-the-art approaches published for this task, and give a\nrelative idea of achieved performances on AMIGOS.\n7\nT o compare our approach with another state-of-the-art appr oach as fairly as possible, it is required that both use exact ly\nthe same experiment protocol. For this, we fully retrained a nd tested the pre-trained CNN approach proposed by Sarkar\nand Etemad [9], with the experiment protocol we presented. T o this end, we use the implementation provided by the\nauthors1. T o ensure fair comparisons, the exact same data was used to p re-train, ﬁne-tune, and test both our approach\nand also Sarkar and Etemad’s approach, for each fold of cross -validation.\nW e see in T able 3 that our approach achieves better performan ce on average than Sarkar and Etemad’s approach\nwith the same experiment protocol, for both arousal and vale nce. For example, our approach achieves an F1-score of\n0.83± 7. 4e−3 for valence, compared to 0.77 ± 5. 1e−3 for the pre-trained CNN. These results are statistically si gniﬁcant\nwith p < 0. 01 following a t-test.\nThis ﬁnal set of results shows that our approach, and more gen erally self-supervised Transformer-based approaches,\ncan be successfully applied to obtain contextualized repre sentations from ECG signals for emotion recognition tasks.\n6 Conclusions and Perspectives\nIn this paper, we investigate the use of transformers for rec ognizing arousal and valence from ECG signals. This\napproach used self-supervised learning for pre-training f rom unlabeled data, followed by ﬁne-tuning with labeled dat a.\nOur experiments indicate that the model builds robust featu res for predicting arousal and valence on the AMIGOS\ndataset, and provides very promising results in comparison to recent state-of-the-art methods. This work showcases\nthat self-supervision and attention-based models such as T ransformers can be successfully used for research in affect ive\ncomputing.\nMultiple perspectives emerge from our work. New pre-traini ng tasks can be investigated: other methods such as\ncontrastive loss or triplet loss might be more efﬁcient with regards to the speciﬁcities of ECG signals, compared to\nmasked points prediction which we used in this work. Extendi ng our work to other input modalities (EEC, GSR,\nand even non-physiological inputs such as ambient sensors) and, in general, to process multimodal situations could\nprove useful for improving performances of emotion recogni tion. Finally, larger scale experiments, with new datasets\ncaptured in varied situations, will allow for a better under standing of the behaviour of our approach.\nAcknowledgements: This work has been partially supported by the MIAI Multidisc iplinary AI Institute at the Univ.\nGrenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003) .\nReferences\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio. Neu ral machine translation by jointly learning to\nalign and translate. In Y oshua Bengio and Y ann LeCun, editor s, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 201 5, Conference T rack Proceedings , 2015.\n[2] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkor eit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is All you Need. Advances in Neural Information Processing Systems , 30, 2017.\n[3] Shiyang Li, Xiaoyong Jin, Y ao Xuan, Xiyou Zhou, W enhu Che n, Y u-Xiang W ang, and Xifeng Y an. Enhancing\nthe Locality and Breaking the Memory Bottleneck of Transfor mer on Time Series Forecasting. In Advances in\nNeural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.\n[4] Genshen Y an, Shen Liang, Y anchun Zhang, and Fan Liu. Fusi ng Transformer Model with T emporal Features\nfor ECG Heartbeat Classiﬁcation. In 2019 IEEE International Conference on Bioinformatics and B iomedicine\n(BIBM), pages 898–905, November 2019.\n[5] David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon De nman, Clinton Fookes, and Lars Petersson. At-\ntention Networks for Multi-T ask Signal Analysis. In 2020 42nd Annual International Conference of the IEEE\nEngineering in Medicine Biology Society (EMBC) , pages 184–187, July 2020.\n[6] Harish Haresamudram, Apoorva Beedu, V arun Agrawal, Pat rick L. Grady, Irfan Essa, Judy Hoffman, and\nThomas Plötz. Masked reconstruction based self-supervisi on for human activity recognition. In Proceedings\nof the 2020 International Symposium on W earable Computers , pages 45–49, New Y ork, NY , USA, September\n2020. Association for Computing Machinery.\n[7] Lin Shu, Jinyan Xie, Mingyue Y ang, Ziyi Li, Zhenqi Li, Dan Liao, Xiangmin Xu, and Xinyi Y ang. A Review of\nEmotion Recognition Using Physiological Signals. Sensors, 18(7):2074, July 2018.\n1 https://code.engineering.queensu.ca/pritam/SSL-ECG\n8\n[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez -González, E. Abdulhay, and N. Arunkumar. Us-\ning Deep Convolutional Neural Network for Emotion Detectio n on a Physiological Signals Dataset (AMIGOS).\nIEEE Access , 7:57–67, 2019.\n[9] P . Sarkar and A. Etemad. Self-Supervised Learning for EC G-Based Emotion Recognition. In ICASSP 2020 -\n2020 IEEE International Conference on Acoustics, Speech an d Signal Processing (ICASSP) , pages 3217–3221,\nMay 2020.\n[10] T . Song, W . Zheng, P . Song, and Z. Cui. EEG Emotion Recogn ition Using Dynamical Graph Convolutional\nNeural Networks. IEEE T ransactions on Affective Computing , 11(3):532–541, July 2020.\n[11] H. Becker, J. Fleureau, P . Guillotel, F . W endling, I. Me rlet, and L. Albera. Emotion Recognition Based on High-\nResolution EEG Recordings and Reconstructed Brain Sources . IEEE T ransactions on Affective Computing ,\n11(2):244–257, April 2020.\n[12] J. Shukla, M. Barreda-Angeles, J. Oliver, G. C. Nandi, a nd D. Puig. Feature Extraction and Selection for Emotion\nRecognition from Electrodermal Activity. IEEE T ransactions on Affective Computing , pages 1–1, 2019.\n[13] Shuhao Chen, Ke Jiang, Haoji Hu, Haoze Kuang, Jianyi Y an g, Jikui Luo, Xinhua Chen, and Y ubo Li. Emotion\nRecognition Based on Skin Potential Signals with a Portable Wireless Device. Sensors, 21(3):1018, January\n2021.\n[14] Mangesh Ramaji Kose, Mitul Kumar Ahirwal, and Anil Kuma r. A new approach for emotions recognition\nthrough EOG and EMG signals. Signal, Image and V ideo Processing , 15(8):1863–1871, November 2021.\n[15] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Y azdani, T ouradj Ebrahimi,\nThierry Pun, Anton Nijholt, and Ioannis Patras. DEAP: A Data base for Emotion Analysis ;Using Physiological\nSignals. IEEE T ransactions on Affective Computing , 3(1):18–31, January 2012.\n[16] Ramanathan Subramanian, Julia W ache, Mojtaba Khomami Abadi, Radu L. V ieriu, Stefan Winkler, and Nicu\nSebe. ASCER T AIN: Emotion and Personality Recognition Usin g Commercial Sensors. IEEE T ransactions on\nAffective Computing , 9(2):147–160, April 2018.\n[17] J. A. Miranda Correa, M. K. Abadi, N. Sebe, and I. Patras. AMIGOS: A Dataset for Affect, Personality and\nMood Research on Individuals and Groups. IEEE T ransactions on Affective Computing , pages 1–1, 2018.\n[18] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gard ner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep Contextualized W ord Representations. In Proceedings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguisti cs: Human Language T echnologies, V olume 1 (Long\nP apers), pages 2227–2237, New Orleans, Louisiana, June 2018. Assoc iation for Computational Linguistics.\n[19] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. BER T: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Cha p-\nter of the Association for Computational Linguistics: Huma n Language T echnologies, V olume 1 (Long and Short\nP apers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Asso ciation for Computational Linguistics.\n[20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof frey Hinton. A Simple Framework for Contrastive\nLearning of V isual Representations. In Proceedings of the 37th International Conference on Machin e Learning ,\npages 1597–1607. PMLR, November 2020.\n[21] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin W an, Peizh ao Zhang, Zhicheng Y an, Masayoshi T omizuka,\nJoseph Gonzalez, Kurt Keutzer, and Peter V ajda. V isual Tran sformers: T oken-based Image Representation and\nProcessing for Computer V ision. arXiv:2006.03677 [cs, eess] , November 2020.\n[22] Kyle Ross, Paul Hungler, and Ali Etemad. Unsupervised m ulti-modal representation learning for affective com-\nputing with multi-corpus wearable data. Journal of Ambient Intelligence and Humanized Computing , October\n2021.\n[23] Manon Macary, Marie T ahon, Y annick Estève, and Anthony Rousseau. On the Use of Self-Supervised Pre-\nTrained Acoustic and Linguistic Features for Continuous Sp eech Emotion Recognition. In 2021 IEEE Spoken\nLanguage T echnology W orkshop (SLT) , pages 373–380, January 2021.\n[24] Siyang Song, Shashank Jaiswal, Enrique Sanchez, Georg ios Tzimiropoulos, Linlin Shen, and Michel V alstar.\nSelf-supervised Learning of Person-speciﬁc Facial Dynami cs for Automatic Personality Recognition. IEEE\nT ransactions on Affective Computing , pages 1–1, 2021.\n[25] Olivia Wiles, A. Sophia Koepke, and Andrew Zisserman. S elf-supervised learning of a facial attribute embedding\nfrom video. arXiv:1808.06882 [cs] , August 2018.\n[26] Y ong Li, Jiabei Zeng, Shiguang Shan, and Xilin Chen. Sel f-Supervised Representation Learning From V ideos\nfor Facial Action Unit Detection. In 2019 IEEE/CVF Conference on Computer V ision and P attern Rec ognition\n(CVPR), pages 10916–10925, Long Beach, CA, USA, June 2019. IEEE.\n9\n[27] Shuvendu Roy and Ali Etemad. Self-supervised Contrast ive Learning of Multi-view Facial Expressions. In\nProceedings of the 2021 International Conference on Multim odal Interaction , pages 253–257, Montréal QC\nCanada, October 2021. ACM.\n[28] Martin Gjoreski, Blagoj Mitrevski, Mitja Luštrek, and Matjaž Gams. An Inter-domain Study for Arousal Recog-\nnition from Physiological Signals. Informatica, 42(1), March 2018.\n[29] De ˘ger A yata, Y usuf Y aslan, and Mustafa E. Kamasak. Emotion Rec ognition from Multimodal Physiological\nSignals for Emotion A ware Healthcare Systems. Journal of Medical and Biological Engineering , 40(2):149–\n157, April 2020.\n[30] Y . Hsu, J. W ang, W . Chiang, and C. Hung. Automatic ECG-Ba sed Emotion Recognition in Music Listening.\nIEEE T ransactions on Affective Computing , 11(1):85–99, January 2020.\n[31] Lin Shu, Y ang Y u, W enzhuo Chen, Haoqiang Hua, Qin Li, Jia nxiu Jin, and Xiangmin Xu. W earable Emotion\nRecognition Using Heart Rate Data from a Smart Bracelet. Sensors, 20(3):718, January 2020.\n[32] R. Harper and J. Southern. A Bayesian Deep Learning Fram ework for End-T o-End Prediction of Emotion from\nHeartbeat. IEEE T ransactions on Affective Computing , pages 1–1, 2020.\n[33] S. Siddharth, T . Jung, and T . J. Sejnowski. Utilizing De ep Learning T owards Multi-modal Bio-sensing and\nV ision-based Affective Computing. IEEE T ransactions on Affective Computing , pages 1–1, 2019.\n[34] Sean A. Fulop and Kelly Fitz. Algorithms for computing t he time-corrected instantaneous frequency (reassigned)\nspectrogram, with applications. The Journal of the Acoustical Society of America , 119(1):360–371, January 2006.\n[35] Guang-Bin Huang, Qin-Y u Zhu, and Chee-Kheong Siew . Ext reme learning machine: Theory and applications.\nNeurocomputing, 70(1-3):489–501, December 2006.\n[36] Y ao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zic o Kolter, Louis-Philippe Morency, and Ruslan Salakhut-\ndinov. Multimodal Transformer for Unaligned Multimodal La nguage Sequences. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lingui stics, pages 6558–6569, Florence, Italy, July 2019.\nAssociation for Computational Linguistics.\n[37] Z. Wu, X. Zhang, T . Zhi-Xuan, J. Zaki, and D. C. Ong. Atten ding to Emotional Narratives. In 2019 8th\nInternational Conference on Affective Computing and Intel ligent Interaction (ACII) , pages 648–654, September\n2019.\n[38] Jian Huang, Jianhua T ao, Bin Liu, Zheng Lian, and Mingyu e Niu. Multimodal Transformer Fusion for Contin-\nuous Emotion Recognition. In ICASSP 2020 - 2020 IEEE International Conference on Acousti cs, Speech and\nSignal Processing (ICASSP) , pages 3507–3511, May 2020.\n[39] Cong Cai, Y u He, Licai Sun, Zheng Lian, Bin Liu, Jianhua T ao, Mingyu Xu, and Kexin W ang. Multimodal\nSentiment Analysis based on Recurrent Neural Network and Mu ltimodal Attention. In Proceedings of the 2nd\non Multimodal Sentiment Analysis Challenge , pages 61–67, V irtual Event China, October 2021. ACM.\n[40] W oan-Shiuan Chien, Huang-Cheng Chou, and Chi-Cun Lee. Self-assessed Emotion Classiﬁcation from Acoustic\nand Physiological Features within Small-group Conversati on. In Companion Publication of the 2021 Interna-\ntional Conference on Multimodal Interaction , pages 230–239, Montreal QC Canada, October 2021. ACM.\n[41] Neo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep Transformer Models for Time Series Forecasting:\nThe Inﬂuenza Prevalence Case. arXiv:2001.08317 [cs, stat] , January 2020.\n[42] Arjun Arjun, Aniket Singh Rajpoot, and Mahesh Raveendr anatha Panicker. Introducing Attention Mechanism for\nEEG Signals: Emotion Recognition with V ision Transformers . In 2021 43rd Annual International Conference\nof the IEEE Engineering in Medicine Biology Society (EMBC) , pages 5723–5726, November 2021.\n[43] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesniko v, Dirk W eissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylva in Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nImage is W orth 16x16 W ords: Transformers for Image Recognit ion at Scale. arXiv:2010.11929 [cs] , October\n2020.\n[44] Behnam Behinaein, Anubhav Bhatti, Dirk Rodenburg, Pau l Hungler, and Ali Etemad. A Transformer Architec-\nture for Stress Detection from ECG. In 2021 International Symposium on W earable Computers , pages 132–134,\nV irtual USA, September 2021. ACM.\n[45] Dumitru Erhan, Aaron Courville, Y oshua Bengio, and Pas cal V incent. Why Does Unsupervised Pre-training\nHelp Deep Learning? In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and\nStatistics, pages 201–208. JMLR W orkshop and Conference Proceedings, March 2010.\n[46] Aparna Khare, Srinivas Parthasarathy, and Shiva Sunda ram. Multi-Modal Embeddings Using Multi-T ask Learn-\ning for Emotion Recognition. In Interspeech 2020 , pages 384–388. ISCA, October 2020.\n10\n[47] W asifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Ba gher Zadeh, Chengfeng Mao, Louis-Philippe\nMorency, and Ehsan Hoque. Integrating Multimodal Informat ion in Large Pretrained Transformers. In Proceed-\nings of the 58th Annual Meeting of the Association for Comput ational Linguistics , pages 2359–2369, Online,\nJuly 2020. Association for Computational Linguistics.\n[48] S. Siriwardhana, T . Kaluarachchi, M. Billinghurst, an d S. Nanayakkara. Multimodal Emotion Recognition With\nTransformer-Based Self Supervised Feature Fusion. IEEE Access , 8:176274–176285, 2020.\n[49] Aparna Khare, Srinivas Parthasarathy, and Shiva Sunda ram. Self-Supervised Learning with Cross-Modal Trans-\nformers for Emotion Recognition. In 2021 IEEE Spoken Language T echnology W orkshop (SLT) , pages 381–388,\nJanuary 2021.\n[50] George Zerveas, Srideepika Jayaraman, Dhaval Patel, A nuradha Bhamidipaty, and Carsten Eickhoff. A\nTransformer-based Framework for Multivariate Time Series Representation Learning. In Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery & Data Mining , pages 2114–2124, V irtual Event Singapore,\nAugust 2021. ACM.\n[51] Alexei Baevski, Y uhao Zhou, Abdelrahman Mohamed, and M ichael Auli. W av2vec 2.0: A Framework for Self-\nSupervised Learning of Speech Representations. Advances in Neural Information Processing Systems , 33:12449–\n12460, 2020.\n[52] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. L ayer Normalization. arXiv:1607.06450 [cs, stat] ,\nJuly 2016.\n[53] Stamos Katsigiannis and Naeem Ramzan. DREAMER: A Datab ase for Emotion Recognition Through EEG\nand ECG Signals From Wireless Low-cost Off-the-Shelf Devic es. IEEE Journal of Biomedical and Health\nInformatics, 22(1):98–107, January 2018.\n[54] Athina Tzovara, Dominik R. Bach, Giuseppe Castegnetti , Samuel Gerster, Nicolas Hofer, Saurabh Khemka,\nChristoph W . Korn, Philipp C. Paulus, Boris B. Quednow , and M atthias Staib. PsPM-FR: SCR, ECG and respi-\nration measurements in a delay fear conditioning task with v isual CS and electrical US., August 2018.\n[55] Philipp C. Paulus, Giuseppe Castegnetti, and Dominik R . Bach. PsPM-HRM5: SCR, ECG and respiration\nmeasurements in response to positive/negative IAPS pictur es, and neutral/aversive sounds, June 2020.\n[56] Dominik R. Bach, Samuel Gerster, Athina Tzovara, and Gi useppe Castegnetti. PsPM-RRM1-2: SCR, ECG,\nrespiration and eye tracker measurements in response to ele ctric stimulation or visual targets, September 2019.\n[57] Y anfang Xia, Filip Melinš ˇcak, and Dominik R. Bach. PsPM-VIS: SCR, ECG, respiration an d eyetracker mea-\nsurements in a delay fear conditioning task with visual CS an d electrical US, July 2020.\n[58] Y an Wu, Ruolei Gu, Qiwei Y ang, and Y ue-jia Luo. How Do Amu sement, Anger and Fear Inﬂuence Heart Rate\nand Heart Rate V ariability? Frontiers in Neuroscience , 13:1131, 2019.\n[59] Richard Liaw , Eric Liang, Robert Nishihara, Philipp Mo ritz, Joseph E. Gonzalez, and Ion Stoica. Tune: A\nResearch Platform for Distributed Model Selection and Trai ning. arXiv:1807.05118 [cs, stat] , July 2018.\n[60] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Ro bust and Efﬁcient Hyperparameter Optimization at\nScale. In Proceedings of the 35th International Conference on Machin e Learning , pages 1437–1446. PMLR,\nJuly 2018.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7479314208030701
    },
    {
      "name": "Transformer",
      "score": 0.7289209365844727
    },
    {
      "name": "Emotion recognition",
      "score": 0.6957051753997803
    },
    {
      "name": "Exploit",
      "score": 0.6922494173049927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5474628806114197
    },
    {
      "name": "Speech recognition",
      "score": 0.5047527551651001
    },
    {
      "name": "Machine learning",
      "score": 0.489790678024292
    },
    {
      "name": "Feature learning",
      "score": 0.4384181797504425
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3916429281234741
    },
    {
      "name": "Engineering",
      "score": 0.10479336977005005
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210101348",
      "name": "Centre Inria de l'Université Grenoble Alpes",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I106785703",
      "name": "Institut polytechnique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210104430",
      "name": "Laboratoire d'Informatique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I19370010",
      "name": "Orange (France)",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I899635006",
      "name": "Université Grenoble Alpes",
      "country": "FR"
    }
  ],
  "cited_by": 34
}