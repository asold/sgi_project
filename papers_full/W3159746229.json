{
  "title": "Dual Transformer for Point Cloud Analysis",
  "url": "https://openalex.org/W3159746229",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281853116",
      "name": "Han Xian-feng",
      "affiliations": [
        "Southwest University"
      ]
    },
    {
      "id": null,
      "name": "Jin, Yi-Fei",
      "affiliations": [
        "Southwest University"
      ]
    },
    {
      "id": "https://openalex.org/A4288182595",
      "name": "Cheng, Hui-Xian",
      "affiliations": [
        "Southwest University"
      ]
    },
    {
      "id": "https://openalex.org/A4281853119",
      "name": "Xiao Guo-qiang",
      "affiliations": [
        "Southwest University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3041273049",
    "https://openalex.org/W3043238202",
    "https://openalex.org/W3157424380",
    "https://openalex.org/W3035272603",
    "https://openalex.org/W3035429765",
    "https://openalex.org/W3039448353",
    "https://openalex.org/W3012494314",
    "https://openalex.org/W3034664537",
    "https://openalex.org/W2986382673",
    "https://openalex.org/W2211722331",
    "https://openalex.org/W1644641054",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W6739778489",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W3035398346",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3014902535",
    "https://openalex.org/W2798777114",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W2962928871",
    "https://openalex.org/W2981199548",
    "https://openalex.org/W2251908874",
    "https://openalex.org/W3035541121",
    "https://openalex.org/W2962731536",
    "https://openalex.org/W2799162093",
    "https://openalex.org/W2896196878",
    "https://openalex.org/W3034482224",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2997814983",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2553307952",
    "https://openalex.org/W3175450634",
    "https://openalex.org/W6732286402",
    "https://openalex.org/W2606202972",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2810240468",
    "https://openalex.org/W2796426482",
    "https://openalex.org/W2963053547",
    "https://openalex.org/W2964253930",
    "https://openalex.org/W2963312728",
    "https://openalex.org/W2798270772",
    "https://openalex.org/W2963830382",
    "https://openalex.org/W2953823434",
    "https://openalex.org/W2997333789",
    "https://openalex.org/W2963123724",
    "https://openalex.org/W2963231572",
    "https://openalex.org/W2953399169",
    "https://openalex.org/W4214755140",
    "https://openalex.org/W3118806719",
    "https://openalex.org/W3205586691",
    "https://openalex.org/W3111535274",
    "https://openalex.org/W3155390614",
    "https://openalex.org/W3171433839",
    "https://openalex.org/W2963719584",
    "https://openalex.org/W2963158438",
    "https://openalex.org/W2948107928",
    "https://openalex.org/W2960986959",
    "https://openalex.org/W3034317823",
    "https://openalex.org/W3035739565",
    "https://openalex.org/W3034239841",
    "https://openalex.org/W3116959466",
    "https://openalex.org/W3201863705",
    "https://openalex.org/W2952689920",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2902302021",
    "https://openalex.org/W2805479641",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2949257818"
  ],
  "abstract": "Following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise multi-head self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT module as a fundamental component, we construct the DTNet for performing point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our proposed transformer framework for the tasks of 3D point cloud classification and segmentation, achieving highly competitive performance in comparison with the state-of-the-art approaches.",
  "full_text": "Dual Transformer for Point Cloud Analysis\nXian-Feng Han\nCollege of Computer and Information Science\nSouthwest University\nYi-Fei Jin\nCollege of Computer and Information Science\nSouthwest University\nHui-Xian Cheng\nCollege of Computer and Information Science\nSouthwest University\nGuo-Qiang Xiao\nCollege of Computer and Information Science\nSouthwest University\nAbstract—Following the tremendous success of transformer in\nnatural language processing and image understanding tasks, in\nthis paper, we present a novel point cloud representation learning\narchitecture, named Dual Transformer Network (DTNet), which\nmainly consists of Dual Point Cloud Transformer (DPCT) mod-\nule. Speciﬁcally, by aggregating the well-designed point-wise and\nchannel-wise multi-head self-attention models simultaneously,\nDPCT module can capture much richer contextual dependencies\nsemantically from the perspective of position and channel. With\nthe DPCT module as a fundamental component, we construct\nthe DTNet for performing point cloud analysis in an end-to-\nend manner. Extensive quantitative and qualitative experiments\non publicly available benchmarks demonstrate the effectiveness\nof our proposed transformer framework for the tasks of 3D\npoint cloud classiﬁcation and segmentation, achieving highly\ncompetitive performance in comparison with the state-of-the-art\napproaches.\nI. I NTRODUCTION\nThe rapid development of 3D acquisition devices has\nwitnessed the considerably important role of 3D point\nclouds played in a wide range of applications, such as vir-\ntual/augmented reality [1][2], autonomous driving [3][4] and\nrobotics [5][6]. Therefore, how to perform effective and efﬁ-\ncient point cloud analysis becomes an urgent requirement or\nan essential issue. Recently, deep learning techniques achieve\ntremendous success in 2D computer vision domain, which\nactually provide an opportunity for better understanding of\npoint cloud. However, the inherent unordered, irregular and\nsparse structure makes it unreasonable to directly apply the\ntraditional convolutional neural networks to 3D point clouds.\nTo address this problem, several existing state-of-the-art\napproaches attempt to transform the unstructured point cloud\ninto either voxel grid [7] or multi-views [8], then applying 3D\nCNN or 2D CNN for effective feature learning. Although these\nmethods have reported promising performance, they suffer\nfrom growing memory requirement, high computational cost\nas well as geometric information loss during transformation.\nAlternatively, following the great success of pioneering work\nPointNet [9], the family of Point-based approaches run directly\non raw point cloud to learn 3D representation via share Multi-\nLayer Perceptrons (MLPs) [10] or carefully-designed 3D con-\nvolutional kernel [11]. Nevertheless, they may be insufﬁcient\nto model long-range contextural correlation among points.\nRecently, Transformer have achieved a series of break-\nthroughs in the ﬁeld of Natural Language Processing (NLP)\nand 2D computer vision tasks [12][13]. As its core component,\nself-attention mechanism, on the one hand, is able to learn\nmuch richer contextural representation by capturing highly\nexpressive long-range dependencies in the input. On the other\nhand, it is invariant to points permutations. Transformer,\ntherefore, is an ideally mechanism suitable for point cloud\nprocessing.\nInspired by the superior performance of Transformer, we\nintroduce a well-designed end-to-end architecture, named Dual\nTransformer Network (DTNet) for point cloud analysis. Its\ncore component is our proposed Dual Point Cloud Transformer\n(DPCT) structure with the ability to aggregate the long-range\nspatial and channel context-dependencies of point-wise feature\nmaps for enhancement of feature representation. Speciﬁcally,\na DPCT module consists of two parallel branches. One is\npoint-wise multi-head self-attention attempting to spatially\nextract contextural information among point-wise features,\nwhile the other is channel-wise multi-head self-attention model\nfor capturing context dependencies in channel dimension. The\noutput of these two attention modules are then concatenated\nvia element-wise sum operation to improve the power of\nrepresentation. Finally, we construct a U-Net like architecture\nusing multi-scale DPCT modules to perform 3D point cloud\nanalysis and understanding tasks.\nQuantitative and quality evaluations conducted on challeng-\ning benchmark datasets demonstrate the effectiveness of our\nproposed model. And the superior performance further reﬂects\nthe high competitiveness of our DTNet against other state-of-\nthe-art point cloud learning networks on tasks of 3D shape\nclassiﬁcation, and segmentation.\nIn summary, we make the following contributions:\n• A well-designed Dual Point Cloud Transformer (DPCT)\nmodule based on multi-heads self-attention is proposed\nfor point cloud processing. This module can semanti-\ncally enhance the representation power of learned point\nfeatures by capturing long-range contextual dependencies\nfrom the position and channel correlations points of view.\n• Based on our DPCT moduel, we construct an end-to-end\nDual Transformer Network (DTNet) directly operating on\n3D point cloud for highly effective feature learning.\narXiv:2104.13044v1  [cs.CV]  27 Apr 2021\nFig. 1. Applications of our proposed Dual Transformer architecture, including 3D object classiﬁcation, part segmentation and semantic segmentation.\n• Extensive evaluations performed on three challenging\nbenchmarks demonstrate the effectiveness of our point\ncloud Transformer architecture, achieving competitive\nperformance on 3D object classiﬁcation, and segmenta-\ntion tasks.\nII. R ELATED WORK\nA. Deep Learning on Point Cloud\nMotivated by the outstanding performance of deep learning\ntechniques in 2D image analysis and understanding tasks,\nmore and more attention has been attracted to extending\ndeep learning on 3D point clouds for effective and efﬁcient\nrepresentations. However, it is unreasonable to directly apply\nthe standard operations in CNNs to point clouds, largely due\nto its irregular, unordered and sparse structure. To address this\nissue, recent researches based on different data format have\nbeen proposed, which can be divided into volumetric-based,\nmulti-view based, and point-based methods.\nVolumetric-based methods [7][14] voxelize the unstructured\npoint clouds into regular volumetric occupancy grid by quan-\ntization, which is well suitable for extracting feature repre-\nsentation with 3D convolutional neural networks. However,\nthese approaches suffer from cubically growing computational\ncomplexity and memory requirment with respect to volumetric\nresolution, as well as geometric information loss. To overcome\nthese limitations, hierarchical data structure-based methods\n[3], such as OctNet [15] and Kd-Net [16], have been proposed\nto concentrate on informative voxels while skipping the empty\nones [17]. PointGrid [14] improves the local geometric details\nlearning by incorporating points within each grid.\nMulti-view based methods aims to turn 3D problem into 2D\nproblem by projecting the point cloud space into a collection\nof 2D images with multiple views, so that 2D CNNs can\nbe applied to perform feature learning. The pioneering work\nMVCNN[18] simply leverages the max-pooling operation to\nextract multi-view features into a global descriptor. Wei et\nal.[19] designed a directed graph by treating the views as\ngraph nodes to construct the View-GCN. Although this type\nof methods have obtained remarkable performance on tasks\nlike object classiﬁcation [20][21][22] with the well-established\nimage CNNs, it is still difﬁcult to determine the appropriate\nnumber of views to cover the 3D objects while avoiding\ninformation loss and high computational consumption.\nPoint-based methods directly take the raw point clouds as\ninput for learning 3D representations without any voxelization\nor projection. As a pioneering work, PointNet [9] learns point-\nwise feature directly from the raw point cloud via shared MLP\nand obtains global representation with max-pooling, which\ncan achieve permutation invariance. Pointnet++[10] extends\nPointNet by integrating a hierarchical structure that takes\nboth global information and local details into consideration.\nSubsequently, the works begin to focus on deﬁning explicit\nconvolution kernels for points. KPConv [11] presents a de-\nformable point convolution using a collection of learnable\nkernel points. FPConv [23] runs directly on local surface of\npoint cloud in projection-interpolation manner for efﬁcient\nfeature learning. Similar to these methods, our DTNet also\ndeal with 3D point cloud directly without any intermediate\ngrid or image representation.\nB. Transformers in vision\nMore recently, Transformer networks[24] have made signif-\nicant process in Natural Language Processing domain [25][26]\nand have achieved astounding performance. Its great success\nattracts increasing interest in computer vision researchers to\ntransfer these models for 2D image tasks, such as classiﬁ-\ncation [27] and detection [13]. Actually, the core factor of\nTransformer is self-attention mechanism that has capability\nof explicitly modeling interactions between elements in the\ninput sequence. iGPT [12] uses transformer for image gen-\neration tasks with unsupervised training mechanism. Vision\nTransformer (ViT) [27] replaces the standard convolution with\ntransformers to perform large-scale supervised image classiﬁ-\ncation. DETR [13] is the ﬁrst to attack the detection issue\nusing transformer model from the perspective of prediction\nproblem.\nAdditionally, since self-attention does not depend on input\norder, and 3D point clouds can be treated as a set of points\nwith positional attributes, transformer is ideally-suited for\npoint cloud analysis. Therefore, we develop a point cloud\ntransformer network with our carefully-designed Dual Point\nCloud Transformer layer to correlate the point representations\nfor learning broad geometry and context dependencies.\nIII. P ROPOSED APPROACH\nIn this section, we propose a generic operation Dual Point\nCloud Transformer (DPCT) that leverages the fundamental\nidea of transformer to model point sets interactions in terms\nof spatial and channel dependencies. By stacking the DPCT\nlayers with increasing receptive ﬁeld, we construct an end-to-\nend architecture called Dual Transform Network (DTNet), for\npoint cloud analysis. The overall framework of our DTNet is\nshown in Figure 3.\nA. Dual Point Cloud Transformer\nGiven a D-dimension point cloud with N points P =\n{pi ∈ RD,i = 1,2,...,N }, we aim to ﬁnd a set function\nf with property of permutation invariance to input and high\nexpressiveness, which can project the input Pto a long-range\ncontext enhancement feature space F, f : P−→F . Actually,\nTransformer and its associated self-attention mechanism are\nparticularly suitable for this problem due to the usage of\nmatrix multiplication, summation operations and ability of\nhighlighting long-term relationships between elements. We,\ntherefore, develop a Dual Point Cloud Transformer based\non the advantage of transformer, which is composed of two\nindividual multi-head self-attention branches for learning in-\nteractions across points and channels simultaneously.\nPoint-wise Self-attention. In order to investigate the spatial\ncorrelation among points and generate long-range context-\ndependent representation, we construct a point-wise multi-\nhead self-attention module for enhancement of feature capa-\nbility.\nAs illustrated in the top of Figure 2, consider a point-wise\nfeature map from lth layer Fl ∈RN×C, we formulated our\nattention model as:\nFl+1\nPWSA = MHATPWSA (Fl) =Concat(AT1\nl+1,AT2\nl+1,..., ATM\nl+1)+Fl\n(1)\nWhere M denotes the number of self-attention blocks. AT(·)\noperation is deﬁned as:\nATm(Fl) =Sm\nl+1Vm\nl+1 = σ(Qm\nl+1(Km\nl+1)T /\n√\nC/M)Vm\nl+1\n(2)\nQm\nl+1 = FlWm\nQl+1 (3)\nKm\nl+1 = FlWm\nKl+1 (4)\nVm\nl+1 = FlWm\nVl+1 (5)\nThe m is the index of attention heads, Sm\nl+1 ∈RN×N is\nthe point-wise attention matrix of the mth head, measuring\nthe points’ impacts on each other. σ is the softmax operation.\nWm\nQl+1 ∈ RC×dq , Wm\nKl+1 ∈ RC×dk , Wm\nVl+1 ∈ RC×dv are\nlearnable weight parameters of three linear layers, where we\nset dq = dk = dv = dc = C/M.\nFrom Equation 2, we can conclude that the output point-\nwise feature map can be considered as a sum of features\nassembled from all points and the initial input, which spatially\ndescribes the long-range dependencies by integrating contex-\ntual information according to attention map.\nChannel-Wise Self-attention. In order to highlight the role of\ninteraction across different point-wise feature channels in im-\nprovement of contextural representation, we build the channel-\nwise multi-head self-attention model by adopting the similar\nstrategy for calculating point-wise attention. As shown in the\nbottom of Figure 2, the channel-wise attention is formally\ndeﬁned as:\nFl+1\nCWSA = MHATCWSA (Fl) =Concat(AT\n′1\nl+1,AT\n′2\nl+1,..., AT\n′M\nl+1)+Fl\n(6)\nAT\n′m(Fl) =Um\nl+1vm\nl+1 = Softmax((qm\nl+1)T km\nl+1/\n√\nC/M)vm\nl+1\n(7)\nqm\nl+1 = FlWm\nql+1 (8)\nkm\nl+1 = FlWm\nkl+1 (9)\nvm\nl+1 = FlWm\nvl+1 (10)\nWhere Um\nl+1 ∈Rdc×dc denotes the channel attention matrix,\nindicating the channel’s importance to each other. Wm\nql+1 ∈\nRC×dc ,Wm\nkl+1 ∈RC×dc ,Wm\nvl+1 ∈RC×dc are weight matrices\nof fully-connected layers. dc = C/M.\nSimilar to point-wise attention, we sum the input and\nfeatures across all channels to output the ﬁnal channel-wise\nattention feature map. By leveraging the self-attention on\nchannel level, we can encode much wider range of channel-\nwise relationships into representations to boost their capability.\nFinally, combining the above operations, our Dual Point\nCloud Transformer generates a much more discriminative\npoint-wise feature representation Fl+1\nDPCT containing long-\nrange spatial and channel contextual information by perform-\ning element-wise addition on point-wise attention features and\nchannel-wise attention features.\nFl+1\nDPCT = Fl+1\nPWSA ⊕Fl+1\nCWSA (11)\nB. Architecture of Dual Transformer Network\nBased on the proposed Dual Point Cloud Trans-\nformer(DPCT) block, we construct complete deep networks\nfor 3D point cloud analysis including classiﬁcation and seg-\nmentation model, as shown in Figure 3. It can be obvi-\nously noted that the networks take point clouds as input,\nthen progressively applying stacked Point Feature Network\n(PFN), DPCT layer, Feature Propagation (FP) Layer and Fully-\nconnected(FC) layer, where our DPCT layer is the critical\ncomponent for feature aggregation in our networks.\nFeature Down Sample Layer. In order to construct a hi-\nerarchical feature for multi-scale analysis, a Feature Down\nSample (FDS) layer is added before our DPCT to downsample\nthe point-wise feature maps as required. Speciﬁcally, consider\ninput Fl, farthest point sampling algorithm (FPS) is performed\nto generate sub feature map F\n′l ⊂Fl, then we aggregate all\npoint features in a r-ball neighboring region for each point in\nFig. 2. The details of our Dual Point Cloud Transformer.\nF\n′l, following by a linear transformation, batch normalization\n(BN) and ReLU operations. This FDS layer can be formally\ndeﬁned as:\nFl+1 = Relu(BN(W\n′l(Agg(FPS (Fl))))) (12)\nWhere Agg(·) indicates the local feature aggregation opera-\ntion. W\n′l denotes the learnable weight parameters of linear\ntransformation.\nFeature Up Sample Layer. In order to make dense prediction\nfor segmentation tasks, we choose to put well-designed Feature\nUp Sample Layer in the decoder stage to progressively im-\nprove the resolution of point-wise feature map to the original\nsize. K nearest neighbors interpolation is adopted to upsample\npoint set according Euclidean distance.Similar to PointNet++\n[10], we resort to skip connection and linear transformation\ntogether with batch normalization and ReLU for feature fusion\nbetween encoder and decoder.\nBackbone. Here, we make full use of the U-Net structure as\nbasic backbone. Speciﬁcally, the number of layers in encoder\nand decoder stages, hyperparameter like downsampling rates\nare set particularly for each speciﬁc task and will be given\ndetailedly in Experiment Section.\nIV. E XPERIMENTS\nIn this section, we evaluate the effectiveness and perfor-\nmance of our DTNet on three publicly available datasets,\nnamely ModelNet [28], and ShapNet [29] , for tasks of\nclassiﬁcation, and segmentation, respectively. In addition, our\npoint cloud transformer models are implemented with PyTorch\nframework on a single NVIDIA TITAN RTX 24G GPU. We\nuse Adam strategy to optimize the networks. The weight decay\nis set to 0.0001.\nA. Point Cloud Classiﬁcation\nDataset and metric. We evaluate our 3D object classiﬁ-\ncation transformer model on ModelNet40 [28] which consists\nof 12,311 CAD models from 40 classes with 9,843 shapes for\ntraining and 2,468 objects for testing. Following Point Net [9],\n1,024 points are uniformly sampled from each model. During\ntraining, we perform data augmentation by adopting a random\npoint dropout, a random scaling in [0.8, 1.25] and a random\nshift in [-0.1, 0.1]. Here, the overall accuracy (OA) is treated\nas evaluation metric for classiﬁcation task.\nNetwork conﬁguration. Our Transformer network for\nshape classiﬁcation task is shown in the bottom of\nFigure 3. We adopt three Point Feature Down Sample\nlayers, each associating with an individual Dual Point\nCloud Transformer module. Three fully-connected layers\nare appended at the end of our model. The number of\npoints and channels used in each layer are summarized\nas follows: INPUT(N=1.024, C=3)-FDS(N=512, C=128)-\nDPCT(N=512, C=320)-FDS(N=128, C=256)-DPCT(N=256,\nC=640)-FDS(N=1, C=1024)-DPCT(C=1024)-FC(512)-\nFC(256)-FC(40). The network is trained for 150 epcohs with\nbatch size of 16 and an initial learning rate of 0.001 that is\ndecayed by 0.7 every 20 epochs.\nPerformance Comparison. Performance of quantitative com-\nparison with the state-of-the-arts are reported in Table I. From\nthe results, it can be clearly seen that our DTNet achieves the\nhighest overall accuracy of 92.9%, outperforming PointNet\nand the second best method Point2Sequence by 3.7% and\n0.3%, respectively. This demonstrates the effectiveness of our\nmodel.\nAblation Study and Analysis. To further investigate the\ninﬂuence of our proposed transformer component, we carry\nout additional experiments on the classiﬁcation task of Mod-\nelNet40. We remove the DPCT modules from the DTNet as\nthe baseline. Table II presents the results of different design\nchoices of DTNet in terms of accuracy average class (ACC)\nand overall accuracy (OA) results.\nComparing with the baseline, we can clearly observe that\nFig. 3. Dual Transformer Architecture for Point Cloud Analysis. The top network is for segmentation task, while the bottom is 3D object classiﬁcation\nframework.\nTABLE I\n3D OBJECT CLASSIFICATION RESULTS ON MODEL NET40. T HE BEST\nRESULTS ARE SHOWN IN BOLD .\nMethod Representation Input Size ModelNet40\n3DShapeNets [28] V olumetric 303 77.3%\nV oxNet [7] V olumetric 323 83.0%\nMVCNN [8] Multi-view 12 × 2242 90.1%\nDeepNet [30] Points 5000 × 3 90.0%\nOctNet [15] V olumetric 1283 86.5%\nKd-Net [16] Points 215 × 3 88.5%\nPointNet [9] Points 1024 × 3 89.2%\nPointNet++ [10] Points+normals 5000 × 6 91.9%\nECC [31] Points 1000 ×3 83.2%\nDGCNN [32] Points 1024 × 3 92.2%\nPointCNN [33] Points 1024 × 3 92.5%\nKC-Net [34] Points 1024 × 3 91.0%\nFoldingNet [35] Points 2048 × 3 88.4%\nPoint2Sequence [36] Points 1024 × 3 92.6%\nOctreeGCNN [37] Points 1024 × 3 92.0%\nSFCNN [38] Points+normals 1024 × 6 92.3%\n3D-GCN [39] Points 1024 × 3 92.1%\nELM [40] Points 1024 × 3 92.2%\nFPConv [23] Points+normals - 92.5%\nSPH3D-GCN [41] Points 1000 × 3 92.1%\nDTNet Points 1024 × 3 92.9%\npoint-wise and channel-wise self-attention make remarkable\ncontributions to point cloud representation learning, achieving\nthe results of 92.8% and 92.6% in OA, respectively. And our\nDPCT module with integration of these two attention models\nobtain a signiﬁcant improvement over baseline by 0.9%. The\nTABLE II\nABLATION STUDIES ON THE MODEL NET40 DATASET. WE ANALYZE THE\nEFFECTS OF POINT -WISE SELF -ATTENTION (PWSA) AND CHANNEL -WISE\nSELF -ATTENTION (CWSA).\nMethod ACC OA\nBaseline 89.6 92.0\nBaseline + PWSA 89.7 92.8\nBaseline + CWSA 89.5 92.6\nDTNet 90.4% 92.9%\nresults convincingly validate the effectiveness and beneﬁt of\nour DPCT block.\nB. Point Cloud Part Segmentation\nDataset and metric. The object part segmentation task can\nbe treated as a per-point classiﬁcation problem. We choose\nto train and test our segmentation DTNet on ShapeNet Part\nbenchmark dataset [29], which consists of 16,881 objects\nfrom 16 different classes with a total of 50 part-level labels.\nFor experimental studies, we follow the ofﬁcially deﬁned\n14,007/2,874 training/testing split. And 2,048 points are sam-\npled from each shape as input. In addition, we perform\nthe same data augmentation as that for classiﬁcation. The\nstandard evaluation metrics, including mean of IoU over all\npart classes and category-wise IoU, are computed to report\nthe performance.\nNetwork conﬁguration. The network architecture for\npart segmentation is presented in the top of Figure 3.\nIn the encoder stage, the same structure as that for\nclassiﬁcation is used for multi-scale feature learning,\nwhile three feature propagation blocks (combination of\nFeature Up Sample layer and DPCT layer) are appended\nin decoder step. The detailed conﬁgurations for each\nlayer are set as: INPUT(N=2048, C=3)-FDS(N=512,\nC=320)-DPCT(N=512, C=320)-FDS(N=128, C=512)-\nDPCT(N=128, C=512)-FDS(N=1, C=1024)-DPCT(N=1,\nC=2014)-FUS(N=128, C=256)-DPCT(N=128, C=256)-\nFUS(N=512, C=128)-DPCT(N=512, C=128)-FUS(N=2048,\nC=128)-DPCT(N=2048, C=128)-FC(128)-FC(50). We train\nour DTNet for 80 epochs. The initial learning rate is set to\n0.0005 and decayed by half every 20 epoch. The batch size\nwe use is 16.\nPerformance Comparison. Quantitative comparison with pre-\nvious state-of-the-art models are given in Table III. Unlike\nPointNet, PointNet++ and SO-Net taking the surface normals\ntogether with point coordinates into account, our DTNet only\nuses XYZ coordinates as input feature. The segmentation\nresult using our DTNet reaches the highest performance with\nrespect to mIoU, achieving 85.6% and surpassing PointNet++\nand the previous best methods SFCNN, by 0.5% and 0.2%,\nrespectively. In particularly, comparing with these competitive\nmethods, our approach perform much better in some classes in\nterms of IoU, such as chair, lamp, skate, table etc. We visualize\nseveral 3D part segmentation in Figure 4. From these results,\nwe can verify the success of our DTNet for part segmentation\ntask.\nV. C ONCLUSION\nThe recent advent of Transformer provides an solution\nto point permutation challenge faced by deep learning on\npoint clouds. This paper introduced an end-to-end point cloud\nanalysis transformer model, called Dual Transformer Network.\nIts core component is our well-designed Dual Point Cloud\nTransformer, which can capture long-range context dependen-\ncies by investigating the point-wise and channel-wise rela-\ntionships. Extensive experiments on challenging benchmark\ndatesets show the remarkable effectiveness of our DTNet,\nachieving state-of-the-art performance on tasks of classiﬁca-\ntion, and segmentation. In the future, intensive study in Point\nCloud Transformer should be made, including computational\nconsumption reduction, design of new operations or networks\nand application to other tasks (semantic segmentation, shape\ncompletion, reconstruction,).\nACKNOWLEDGMENT\nREFERENCES\n[1] Z. Gojcic, C. Zhou, J. D. Wegner, L. J. Guibas, and T. Birdal, “Learning\nmultiview 3d point cloud registration,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2020, pp. 1759–\n1769.\n[2] H. Jiang, F. Yan, J. Cai, J. Zheng, and J. Xiao, “End-to-end 3d point\ncloud instance segmentation without detection,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 12 796–12 805.\n[3] Y . Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep\nlearning for 3d point clouds: A survey,” IEEE transactions on pattern\nanalysis and machine intelligence , 2020.\n[4] Q. Hu, B. Yang, L. Xie, S. Rosa, Y . Guo, Z. Wang, N. Trigoni, and\nA. Markham, “Randla-net: Efﬁcient semantic segmentation of large-\nscale point clouds,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2020, pp. 11 108–11 117.\n[5] E. Nezhadarya, E. Taghavi, R. Razani, B. Liu, and J. Luo, “Adaptive\nhierarchical down-sampling for point cloud classiﬁcation,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 12 956–12 964.\n[6] Y . Wang and J. M. Solomon, “Deep closest point: Learning represen-\ntations for point cloud registration,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2019, pp. 3523–3532.\n[7] D. Maturana and S. Scherer, “V oxnet: A 3d convolutional neural net-\nwork for real-time object recognition,” in 2015 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2015, pp.\n922–928.\n[8] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view\nconvolutional neural networks for 3d shape recognition,” in Proceedings\nof the IEEE international conference on computer vision, 2015, pp. 945–\n953.\n[9] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on\npoint sets for 3d classiﬁcation and segmentation,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n652–660.\n[10] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierar-\nchical feature learning on point sets in a metric space,” arXiv preprint\narXiv:1706.02413, 2017.\n[11] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and\nL. J. Guibas, “Kpconv: Flexible and deformable convolution for point\nclouds,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 2019, pp. 6411–6420.\n[12] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n“Generative pretraining from pixels,” in International Conference on\nMachine Learning. PMLR, 2020, pp. 1691–1703.\n[13] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\n[14] T. Le and Y . Duan, “Pointgrid: A deep network for 3d shape under-\nstanding,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2018, pp. 9204–9214.\n[15] G. Riegler, A. Osman Ulusoy, and A. Geiger, “Octnet: Learning deep\n3d representations at high resolutions,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 3577–\n3586.\n[16] R. Klokov and V . Lempitsky, “Escape from cells: Deep kd-networks for\nthe recognition of 3d point cloud models,” in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2017, pp. 863–872.\n[17] L. Wang, Y . Huang, Y . Hou, S. Zhang, and J. Shan, “Graph attention\nconvolution for point cloud semantic segmentation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 10 296–10 305.\n[18] W. Yin and H. Sch ¨utze, “Multichannel variable-size convolution for\nsentence classiﬁcation,” arXiv preprint arXiv:1603.04513 , 2016.\n[19] X. Wei, R. Yu, and J. Sun, “View-gcn: View-based graph convolutional\nnetwork for 3d shape analysis,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2020, pp.\n1850–1859.\n[20] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas,\n“V olumetric and multi-view cnns for object classiﬁcation on 3d data,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 5648–5656.\n[21] Y . Feng, Z. Zhang, X. Zhao, R. Ji, and Y . Gao, “Gvcnn: Group-view\nconvolutional neural networks for 3d shape recognition,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 264–272.\n[22] C. Ma, Y . Guo, J. Yang, and W. An, “Learning multi-view representation\nwith lstm for 3-d shape recognition and retrieval,” IEEE Transactions\non Multimedia, vol. 21, no. 5, pp. 1169–1182, 2018.\n[23] Y . Lin, Z. Yan, H. Huang, D. Du, L. Liu, S. Cui, and X. Han, “Fpconv:\nLearning local ﬂattening for point convolution,” in Proceedings of the\nGroundTruth\nPrediction\nGroundTruth\nPrediction\nFig. 4. Qualitative comparison between our DTNet and the ground truth on ShapeNet Part dataset.\nTABLE III\nPART SEGMENTATION RESULTS ON SHAPE NET PART DATASET . THE MEAN IOU ACROSS ALL THE SHAPE INSTANCES AND IOU FOR EACH CATEGORY ARE\nREPORTED .\nMethod mIoU aero bag cap car chair ep guitar knife lamp laptop motor mug pistol rocket skate table\nShapeNet [29] 81.4 81.0 78.4 77.7 75.7 87.6 61.9 92.0 85.4 82.5 95.7 70.6 91.9 85.9 53.1 69.8 75.3\nPointNet [9] 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6\nPointNet++ [10] 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\nKD-Net [16] 82.3 80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 71.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3\nSO-Net [42] 84.9 82.8 77.8 88.0 77.3 90.6 73.5 90.7 83.9 82.8 94.8 69.1 94.2 80.9 53.1 72.9 83.0\nRGCNN [43] 84.3 80.2 82.8 92.6 75.3 89.2 73.7 91.3 88.4 83.3 96.0 63.9 95.7 60.9 44.6 72.9 80.4\nDGCNN [32] 85.2 84.0 83.4 86.7 77.8 90.6 74.7 91.2 87.5 82.8 95.7 66.3 94.9 81.1 63.5 74.5 82.6\nSRN [44] 85.3 82.4 79.8 88.1 77.9 90.7 69.6 90.9 86.3 84.0 95.4 72.2 94.9 81.3 62.1 75.9 83.2\nSFCNN [38] 85.4 83.0 83.4 87.0 80.2 90.1 75.9 91.1 86.2 84.2 96.7 69.5 94.8 82.5 59.9 75.1 82.9\n3D-GCN [39] 85.1 83.1 84.0 86.6 77.5 90.3 74.1 90.9 86.4 83.8 95.6 66.8 94.8 81.3 59.6 75.7 82.6\nELM [40] 85.2 84.0 80.4 88.0 80.2 90.7 77.5 91.2 86.4 82.6 95.5 70.0 93.9 84.1 55.6 75.6 82.1\nWeak Sup. [45] 85.0 83.1 82.6 80.8 77.7 90.4 77.3 90.9 87.6 82.9 95.8 64.7 93.9 79.8 61.9 74.9 82.9\nDTNet 85.6 83.0 81.4 84.3 78.4 90.9 74.3 91.0 87.3 84.7 95.6 69.0 94.4 82.5 59.0 76.4 83.5\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 4293–4302.\n[24] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, “Spa-\ntial transformer networks,” arXiv preprint arXiv:1506.02025 , 2015.\n[25] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and\nD. Tran, “Image transformer,” in International Conference on Machine\nLearning. PMLR, 2018, pp. 4055–4064.\n[26] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\nrecognition,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2019, pp. 3464–3473.\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[28] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d\nshapenets: A deep representation for volumetric shapes,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition ,\n2015, pp. 1912–1920.\n[29] L. Yi, V . G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu, Q. Huang,\nA. Sheffer, L. Guibas et al. , “A scalable active framework for region\nannotation in 3d shape collections,” ACM Transactions on Graphics\n(TOG), vol. 35, no. 6, p. 210, 2016.\n[30] S. Ravanbakhsh, J. Schneider, and B. Poczos, “Deep learning with sets\nand point clouds,” arXiv preprint arXiv:1611.04500 , 2016.\n[31] M. Simonovsky and N. Komodakis, “Dynamic edge-conditioned ﬁlters\nin convolutional neural networks on graphs,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 3693–\n3702.\n[32] Y . Wang, Y . Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, “Dynamic graph cnn for learning on point clouds,” arXiv\npreprint arXiv:1801.07829, 2018.\n[33] Y . Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn:\nConvolution on χ-transformed points,” in Proceedings of the 32nd\nInternational Conference on Neural Information Processing Systems ,\n2018, pp. 828–838.\n[34] Y . Shen, C. Feng, Y . Yang, and D. Tian, “Mining point cloud local\nstructures by kernel correlation and graph pooling,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition , 2018,\npp. 4548–4557.\n[35] Y . Yang, C. Feng, Y . Shen, and D. Tian, “Foldingnet: Point cloud\nauto-encoder via deep grid deformation,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp. 206–\n215.\n[36] X. Liu, Z. Han, Y .-S. Liu, and M. Zwicker, “Point2sequence: Learning\nthe shape representation of 3d point clouds with an attention-based\nsequence to sequence network,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , vol. 33, no. 01, 2019, pp. 8778–8785.\n[37] H. Lei, N. Akhtar, and A. Mian, “Octree guided cnn with spherical ker-\nnels for 3d point clouds,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2019, pp. 9631–9640.\n[38] Y . Rao, J. Lu, and J. Zhou, “Spherical fractal convolutional neural\nnetworks for point cloud recognition,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp. 452–\n460.\n[39] Z.-H. Lin, S.-Y . Huang, and Y .-C. F. Wang, “Convolution in the cloud:\nLearning deformable kernels in 3d graph convolution networks for\npoint cloud analysis,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2020, pp. 1800–1809.\n[40] K. Fujiwara and T. Hashimoto, “Neural implicit embedding for point\ncloud analysis,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2020, pp. 11 734–11 743.\n[41] H. Lei, N. Akhtar, and A. Mian, “Spherical kernel for efﬁcient graph\nconvolution on 3d point clouds,” IEEE transactions on pattern analysis\nand machine intelligence , 2020.\n[42] J. Li, B. M. Chen, and G. H. Lee, “So-net: Self-organizing network\nfor point cloud analysis,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2018, pp. 9397–9406.\n[43] G. Te, W. Hu, A. Zheng, and Z. Guo, “Rgcnn: Regularized graph cnn for\npoint cloud segmentation,” inProceedings of the 26th ACM international\nconference on Multimedia , 2018, pp. 746–754.\n[44] Y . Duan, Y . Zheng, J. Lu, J. Zhou, and Q. Tian, “Structural relational\nreasoning of point clouds,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2019, pp. 949–958.\n[45] X. Xu and G. H. Lee, “Weakly supervised semantic point cloud seg-\nmentation: Towards 10x fewer labels,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2020, pp.\n13 706–13 715.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7736952304840088
    },
    {
      "name": "Cloud computing",
      "score": 0.53547203540802
    },
    {
      "name": "Transformer",
      "score": 0.41412559151649475
    },
    {
      "name": "Electrical engineering",
      "score": 0.13078200817108154
    },
    {
      "name": "Operating system",
      "score": 0.1206405758857727
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I142108993",
      "name": "Southwest University",
      "country": "CN"
    }
  ],
  "cited_by": 93
}