{
  "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts",
  "url": "https://openalex.org/W4285134706",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104573188",
      "name": "Zhengyan Zhang",
      "affiliations": [
        "Center for Information Technology",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2131512363",
      "name": "Yankai Lin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University",
        "Center for Information Technology",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1906085637",
      "name": "Peng Li",
      "affiliations": [
        "Tencent (China)",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "South China Institute of Collaborative Innovation",
        "Center for Information Technology",
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2070232376",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3024936740",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W97540112",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W1613249581",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W3102978878",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2970863760",
    "https://openalex.org/W3166574921",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2145889472",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W3035141718",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3170796112",
    "https://openalex.org/W3171750540",
    "https://openalex.org/W3113709932",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W3138895808",
    "https://openalex.org/W3173374050",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4298133217",
    "https://openalex.org/W3196295870",
    "https://openalex.org/W2950014519",
    "https://openalex.org/W2000420612",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4226515448",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963846996"
  ],
  "abstract": "Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 877 - 890\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMoEﬁcation: Transformer Feed-forward Layers are Mixtures of Experts\nZhengyan Zhang1,2, Yankai Lin3, Zhiyuan Liu1,2,4,5†,\nPeng Li3,6, Maosong Sun1,2,4,5,7†, Jie Zhou3\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2Beijing National Research Center for Information Science and Technology\n3Pattern Recognition Center, WeChat AI, Tencent Inc\n4International Innovation Center of Tsinghua University, Shanghai, China\n5Beijing Academy of Artiﬁcial Intelligence\n6Institute for AI Industry Research (AIR), Tsinghua University, China\n7Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China\nzy-z19@mails.tsinghua.edu.cn {liuzy,sms}@tsinghua.edu.cn\nAbstract\nRecent work has shown that feed-forward net-\nworks (FFNs) in pre-trained Transformers are\na key component, storing various linguistic\nand factual knowledge. However, the compu-\ntational patterns of FFNs are still unclear. In\nthis work, we study the computational patterns\nof FFNs and observe that most inputs only ac-\ntivate a tiny ratio of neurons of FFNs. This\nphenomenon is similar to the sparsity of the\nhuman brain, which drives research on func-\ntional partitions of the human brain. To ver-\nify whether functional partitions also emerge\nin FFNs, we propose to convert a model into\nits MoE version with the same parameters,\nnamely MoE ﬁcation. Speciﬁcally, MoEﬁca-\ntion consists of two phases: (1) splitting the\nparameters of FFNs into multiple functional\npartitions as experts, and (2) building expert\nrouters to decide which experts will be used\nfor each input. Experimental results show\nthat MoEﬁcation can conditionally use 10%\nto 30% of FFN parameters while maintain-\ning over 95% original performance for differ-\nent models on various downstream tasks. Be-\nsides, MoEﬁcation brings two advantages: (1)\nit signiﬁcantly reduces the FLOPS of infer-\nence, i.e., 2x speedup with 25% of FFN pa-\nrameters, and (2) it provides a ﬁne-grained\nperspective to study the inner mechanism of\nFFNs. The source code of this paper can\nbe obtained from https://github.com/\nthunlp/MoEfication.\n1 Introduction\nRecent years have witnessed great success of\nTransformer-based pre-trained language models\n†Corresponding authors\nPart of the work was done while Peng Li was working at\nTencent.\n(PLMs) (Devlin et al., 2019; Brown et al., 2021;\nHan et al., 2021), attracting many efforts to inter-\npret the inner mechanism of Transformer (Man-\nning et al., 2020; Kovaleva et al., 2019). However,\nmost of these works focus on the attention mecha-\nnism but ignore the feed-forward networks (FFNs),\nwhich constitute nearly two-thirds of model pa-\nrameters. Although recent work has shown that\nFFNs can be viewed as memory networks storing\namounts of knowledge (Geva et al., 2021; Dai et al.,\n2021), the computational patterns of FFNs are still\nunclear.\nIn this work, we study the activation patterns\nof FFNs in Transformer models and ﬁnd a phe-\nnomenon of sparse activation , i.e., only a tiny\nfraction of neurons are activated for a single input.\nFor example, when we perform inference on a ﬁne-\ntuned T5-Large model (Raffel et al., 2020) with\n700-million parameters, 90% inputs only activate\nless than 5% neurons1. This phenomenon is similar\nto the sparsity in the human brain (Olshausen and\nField, 1996; Gross, 2002), which drives research\non functional partitions of the human brain (Garey,\n1999). Inspired by such observation, we further\nraise up a question: do the functional partitions\nalso emerge in artiﬁcial neural models, i.e., FFNs\nin pre-trained Transformer?\nTo investigate this problem, we explore whether\na Transformer can be converted into an equiv-\nalent Mixture-of-Experts (MoE) model (Bengio,\n2013), which regards different functional partitions\nin FFNs as different experts conditionally activated.\nSpecially, we propose MoEﬁcation to discover the\nfunctional partitions (experts) in FFNs and build\nrouters for selecting experts. It consists of two\n1T5 uses ReLU as the activation function. We treat the\nneurons having positive outputs as activated neurons.\n877\nphases. (1) Expert Construction: Split a whole\nfeed-forward layer into multiple experts. The goal\nis to group those neurons that are often activated\nsimultaneously into the same expert network. (2)\nExpert Selection: Select those experts that con-\ntain as many activated neurons as possible for each\ninput to approximate to the original results.\nIn the experiments, we evaluate MoEﬁcation on\ntwo typical kinds of downstream tasks, including\nGLUE and QA benchmarks (Wang et al., 2019;\nRajpurkar et al., 2016; Lai et al., 2017), using T5\nand BERT (Raffel et al., 2020; Devlin et al., 2019).\nExperimental results verify that FFNs in Transform-\ners can be converted to mixtures of experts, and\nthus we can use only 10% to 30% of FFN param-\neters to maintain over 95% original performance,\nwhich veriﬁes that the pre-trained Transformers\nalso learn the functional partitions in FFNs. Be-\nsides, MoEﬁcation brings two advantages: (1) It\ncan signiﬁcantly speed up the inference of Trans-\nformers. Using 25% of FFN parameters brings 2x\nspeedup on CPU and 1.2x speedup on GPU. (2)\nWe can study MoEﬁed models to interpret the in-\nner mechanism of FFNs at a ﬁne-grained level. In\nthis work, we study their routing patterns and hope\nthese ﬁndings can help future work on the design\nand training of MoE models.\n2 Related Work\nInterpretation of Large-scale Transformers.\nDue to the success of Transformer-based PLMs,\nthere are many studies on the interpretation of\nTransformer, including the functionality of differ-\nent layers (Tenney et al., 2019; Jawahar et al., 2019;\nWang and Tu, 2020; Ramnath et al., 2020), and\nthe mechanisms of both attention networks and\nFFNs (Manning et al., 2020; Kovaleva et al., 2019;\nWallace et al., 2019). Recent work ﬁnd that the\nFFNs of Transformers can be viewed as memory\nnetworks storing lots of knowledge learned from\nlanguage modeling (Geva et al., 2021; Dai et al.,\n2021; Suau et al., 2020). Meanwhile, researchers\nexplore to modify the knowledge stored in FFNs\nand achieve promising results (De Cao et al., 2021;\nMeng et al., 2022). In this work, we show that how\nthe knowledge stored in FFNs is used, that is, most\nFFNs can be viewed as a MoE network where the\nknowledge is conditionally activated.\nLarge-scale PLMs with MoE. Jacobs et al.\n(1991) propose mixture-of-experts to build a sys-\ntem composed of many separate networks, which\nlearn to handle a subset of the training examples in-\ndependently. When deep neural networks achieve\ngreat success (Hinton et al., 2012; Krizhevsky et al.,\n2012; Goodfellow et al., 2013), Bengio (2013)\nthinks the model size is a key factor and MoE\nis an important technique to scaling model com-\nputation and proposes the idea of “conditional\ncomputation”. The ﬁrst large-scale MoE lan-\nguage model is proposed by Shazeer et al. (2017),\nwhich adds an MoE layer between two LSTM lay-\ners and independently assigns tokens to combi-\nnations of experts. Recently, GShard (Lepikhin\net al., 2021), Switch-Transformer (Fedus et al.,\n2021), BASELayer (Lewis et al., 2021), and Hash-\nLayer (Roller et al., 2021) study how to build large-\nscale Transformer-based models with MoE and op-\ntimal training strategies, which can fully utilize the\nmodel capacity. Different from them, we utilize the\nnaturally-existing sparse activation phenomenon\nto convert a model into its MoE version for better\nefﬁciency during inference.\nModel Acceleration for PLMs. Model acceler-\nation aims to reduce the time and space complexity\nof PLMs. There are several techniques including\nknowledge distillation (Sanh et al., 2019; Sun et al.,\n2019; Jiao et al., 2020), model pruning (V oita et al.,\n2019; Michel et al., 2019; Zhang et al., 2021), at-\ntention approximation (Wang et al., 2020; Kitaev\net al., 2020; Zaheer et al., 2020),and model quanti-\nzation (Zafrir et al., 2019; Zhang et al., 2020; Bai\net al., 2021), and dynamic inference (Xin et al.,\n2020; Li et al., 2021; Ye et al., 2021; Hou et al.,\n2020). Among these techniques, dynamic inference\nexplore to selectively omit unnecessary computa-\ntion for acceleration, which is similar to the target\nof MoEﬁcation. Previous work usually focuses on\nhow to dynamically drop layers to accelerate in-\nference (Huang et al., 2018; Wu et al., 2020; Li\net al., 2021), which introduces additional training\nobjectives and prediction strategies. In contrast,\nMoEﬁcation simpliﬁes models in a ﬁner granular-\nity, and does not change the process of training\nand inference. In summary, MoEﬁcation can be\nregarded as a novel direction diagonal with the\nabove-mentioned approaches.\n3 MoEﬁcation\nIn this section, we will introduce the general idea of\nMoEﬁcation and divide it into two phases: expert\nconstruction and expert selection.\n2\n878\nx W 1\nh\n\u0000 (h) W 2\nF (x)\nx\n W 1\nh\n\u0000 (h)\n W 2\nF (x)\nh\n\u0000 (h)\nx\nF (x)\n(a) FFN Computation Process (b) Unused elements and neurons\nRouter\nW2\n2\nW2\n1W1\n1\nW1\n2\nW1P =\n(d) FFN with MoE(c) Expert Construction\nP =\n2\n664\n0010\n1000\n0001\n0100\n3\n775\n<latexit sha1_base64=\"VcolkrXZ70Ky7qEFop/9/nPDolA=\">AAACYnicbZFNSwMxEIbT9auuX9Ue9RAsiqeyKxW9CKIXjxVsFbqlJOm0BpPskmTFsvQ3evYq/gWvatputX4MDHl5ZiYT3tBEcGOD4Lngzc0vLC4Vl/2V1bX1jdLmVtPEqWbQYLGI9S0lBgRX0LDcCrhNNBBJBdzQ+4tR/eYBtOGxuraDBNqS9BXvcUasQ50Sj6jEdXyKIwp9rjIqidX8cegHeB+PMhyfUeSHOQimIJgB4RR8d/kRqO7XfRh3SpWgGowD/xVhLiooj3qn9BJ1Y5ZKUJYJYkwrDBLbzoi2nAkY+lFqICHsnvSh5aQiEkw7G1syxHuOdHEv1i6VxWM6O5ERacxAUtfpXnhnftdG8L9aK7W9k3bGVZJaUGyyqJcKbGM88hd3uQZmxcAJwjR3b8XsjmjCrPuFH1uoHPrOlPC3BX9F87Aa1qpHV7XK2XluTxFto110gEJ0jM7QJaqjBmLoCb2hd/RRePV8b9MrT1q9Qj5TRj/C2/kE/UWsRw==</latexit>\nPT\nW2 =\nW1\n2\nW2\n2\nW1\n1 W2\n1\nInput or OutputPositive Neuron Matrix ElementNegative Neuron Unactivated Neuron Unused Element or Neuron\nFigure 1: An example of the sparse activation phenomenon and MoEﬁcation. (a) shows the computation process\nof an FFN for a given input. (b) shows the unused elements and neurons for this input. (c) shows how to construct\nexperts. (d) shows how the MoEﬁed model handles this input efﬁciently.\n3.1 Overall Framework\nMoEﬁcation aims to utilize the sparse activation\nphenomenon in the FFNs of Transformers to reduce\nthe computation cost.\nWe ﬁrst formally describe the sparse activation\nphenomenon. The FFNs of Transformers are two-\nlayer fully connected networks, which process an\ninput representation x ∈Rdmodel by\nh = xW1 + b1,\nF(x) =σ(h)W2 + b2, (1)\nwhere W1 ∈Rdmodel×dff and W2 ∈Rdff ×dmodel\nare the weight matrices, b1 ∈ Rdff and b2 ∈\nRdmodel are the bias vectors, andσ(·) is a non-linear\nactivation function, which prefers to retain positive\nvalues and discard negative ones. In this work,\nwe study the activation function ReLU (Nair and\nHinton, 2010), which is used by the original Trans-\nformer (Vaswani et al., 2017) and some widely-\nused Transformer-based PLMs (Sun et al., 2020;\nRaffel et al., 2020).\nSince there are many inactive (zero) values in\nthe intermediate output σ(h), the computation of\nthese values can be omitted for acceleration. Mean-\nwhile, different inputs will activate different neu-\nrons. Hence, we explore to select the possiblely-\nactivated neurons ofh before the FFN computation\ninstead of model pruning.\nWe show an example in Figure 1. In this FFN,\ndmodel is 2, dff is 4, and the bias vectors are omit-\nted for simpliﬁcation. For a given input representa-\ntion x, there are two positive values in h. Hence,\nwe only need to compute part of the FFN, i.e., a\n2 ×2 submatrix of W1 and a 2 ×2 submatrix of\nW2, to obtain the same output F(x). Correspond-\ningly, we can MoEfy the original FFN to have an\nMoE layer with two experts and select the one on\nthe right-hand side for this input x.\nFor MoEﬁcation, we ﬁrst split the FFN into sev-\neral independent parts, namely expert construction,\nand then design a router to select suitable experts\nfor each input, namely expert selection.\n3.2 Expert Construction\nIn this subsection, we introduce how to split an\nFFN into several parts. The core idea is to group\ntogether the neurons that are often activated simul-\ntaneously. In this way, for each input, we can select\na small number of experts to cover all its activated\nneurons. To achieve better parallel computation\nperformance, we set the size of each expert to be\nthe same. If the number of experts is k, the input\nand output dimension of experts is still dmodel and\ntheir intermediate dimension is de = dff\nk . Then,\nthe parameters of i-th expert are denoted by\nWi\n1 ∈Rdmodel×de ,bi\n1 ∈Rde ,Wi\n2 ∈Rde×dmodel . (2)\nGiven the result of splitting, we construct the cor-\nresponding permutation of intermediate neurons by( 1 2 ... d ff\nf(1) f(2) ... f(dff )\n)\n, where f(n) is the mapping\nfunction from the original neuron index to the per-\nmuted neuron index. We compute f(n) by\nf(n) = (e(n) −1)de + |{m|m≤n,e(m) =e(n)}|, (3)\nwhere e(n) is the expert index of the n-th neuron,\nwhich varies from 1 to k, and |{m|m≤n,e(m) =\ne(n)}|is the index of the n-th neuron in the expert.\nThen, we use its permutation matrixP ∈Rdff ×dff\nto permute the rows or columns of parameters and\nhave the following split:\n[W1\n1 ,W2\n1 ,..., Wk\n1 ] =W1P,\nb1\n1 ⊕b2\n1 ⊕... ⊕bk\n1 = b1P,\n[(W1\n2 )T ,(W2\n2 )T ,..., (Wk\n2 )T ] = (PT W2)T ,\n(4)\n3\n879\nwhere ⊕ represents the vertical concatenation.\nNote that the permutation will not inﬂuence the\noutput representation:\nσ(h)W2 + b2 = σ(h)P PT W2 + b2,\n= σ(hP)PT W2 + b2,\n= σ(xW1P + b1P)PT W2 + b2.\n(5)\nIn this work, we propose two methods to split an\nFFN into kparts.\nParameter Clustering Split . To take the pa-\nrameter information into consideration, we treat\nthe columns of W1 as a collection of vectors with\ndmodel dimension. Based on the intuition that the\nneurons with similar vectors will be activated simul-\ntaneously, we apply balanced K-Means (Malinen\nand Fränti, 2014) to the vector collection to obtain\nkclusters to construct the mapping function.\nCo-Activation Graph Split . To directly use\nthe information of co-activation, we construct a\nco-activation graph by counting co-activations of\nPLMs for the samples of the training set. Each\nneuron will be represented by a node in the graph,\nand the edge weight between two nodes are their\nco-activation values. The co-activation value is\ncomputed by\nco-activation(n,m) =\n∑\nx\nh(x)\nn h(x)\nm 1 h(x)\nn >0,h(x)\nm >0, (6)\nwhere h(x)\nn , h(x)\nm are the n-th and the m-th neurons\nof h for the input x and 1 h(x)\nn >0,h(x)\nm >0 indicates\nh(x)\nn and h(x)\nm are activated simultaneously. Then,\nwe apply graph partitioning algorithms (Karypis\nand Kumar, 1998) to the co-activation graph to ob-\ntain the split, where the internal connections for\neach group will be strong. Please refer to Ap-\npendix F for the details of the partitioning algo-\nrithm. It means that the neurons splitted into the\nsame group are often activated simultaneously for\nthe training samples.\n3.3 Expert Selection\nIn this subsection, we introduce how to create a\nrouter for expert selection. An MoEﬁed FFN pro-\ncessed an input x by\nFm(x) =\n∑\ni∈S\nσ(xWi\n1 + bi\n1)Wi\n2 + b2, (7)\nwhere Sis the set of the selected experts. If all ex-\nperts are selected, we have Fm(x) =F(x). Con-\nsidering that σ(xWi\n1 +bi\n1)Wi\n2 equals to 0 for most\nexperts, we try to select nexperts, where n < k,\nminimize ||Fm(x) −F(x)||2. The selection meth-\nods will assign a score si to each expert for the\ngiven input x and select the experts with the n\nhighest scores by\nS = arg max\nA⊂{1,2,...,k},|A|=n\n∑\ni∈A\nsi. (8)\nGroundtruth Selection for the intermediate\noutput σ(h). We can obtain the groundtruth se-\nlection, which minimizes ||concat({f(σ(xWi\n1 +\nbi\n1))1 (i∈S)}) −σ(h)||2, by a greedy algorithm.\nf is a padding function with zeros to match the\ndimension between σ(xWi\n1 + bi\n1) and σ(h). We\ncalculate the sum of positive values in each expert\nas si and select experts using Equation 8. This\nselection should approximate to the lower bound\nof ||Fm(x) −F(x)||2. Correspondingly, its perfor-\nmance will approximate to the ideal performance of\nan MoEﬁed model. Meanwhile, it is intractable to\ndirectly optimize ||Fm(x)−F(x)||2 because there\nare too many possible combinations of experts.\nSimilarity Selection. To utilize the parameter\ninformation, we average all columns of Wi\n1 and\nuse it as the expert representation. Given an input\nx, we calculate the cosine similarity between the\nexpert representation and x as si.\nMLP Selection. We train a multi-layer percep-\ntron (MLP), which takes thex as input and predicts\nthe sum of positive values in each expert. Then,\nwe use the prediction as si. This method tries to\napproximate to the performance of groundtruth se-\nlection.\n4 Experiment\n4.1 Experimental Setups\nModels and Hyperparameter. We use four vari-\nants of T5 (Raffel et al., 2020), which are the\n60-million-parameter T5-Small, the 200-million-\nparameter T5-Base, the 700-million-parameter T5-\nLarge, and the 3-billion-parameter T5-XLarge. The\nnon-linear activation function is ReLU (Nair and\nHinton, 2010). We use Adam as the optimizer and\na learning rate of 10−6 for ﬁne-tuning T5 models\non downstream tasks. The batch size is set to 64\nand the number of epochs is set to 3.\nDatasets. We use several natural language\nunderstanding datasets to evaluate our models.\nWe use SST-2 (Socher et al., 2013), MNLI-\nmatched (Williams et al., 2018), and RACE (Lai\net al., 2017) as the main evaluation datasets, which\ncover single-sentence classiﬁcation, sentence-pair\n4\n880\nclassiﬁcation, and reading comprehension. We re-\nport the results on their development sets. We also\nreport the results of MoEﬁcation in other datasets\nin Appendix A including the tasks in GLUE bench-\nmark (Wang et al., 2019) and SQuAD (Rajpurkar\net al., 2016).\nExpert Construction. For balanced K-Means,\nwe use an open-source implementation2. Besides\nParameter Clustering Split and Co-activation Graph\nSplit, we also implement Random Split as a naive\nbaseline, which uses an identity matrix as P. For\nthe number of neurons in each expert, if the number\nis small, there will be a lot of experts, making the\nrouting computation cost high. Meanwhile, if the\nnumber is large, there will be more inactive neurons\nin each expert for a given input, which is harmful to\nthe performance with the same amount of selected\nneurons. Hence, selecting the number of neurons in\neach expert is a trade-off between computation cost\nand accuracy. According to our pilot experiments,\nwe set the number of neurons in each expert de to\n32. Correspondingly, the number of experts varies\nfrom 64 to 512 (k= dff\nde ) for different T5 variants.\nWith the same expert size, the relative computation\ncost of routing for different models is the same as\nshown in Appendix E.\nExpert Selection. Besides Similarity Selection\nand MLP Selection, we also implement Random\nSelection, where we treat each expert as a col-\nlection of vectors with dmodel dimension and ran-\ndomly select one of them as the expert represen-\ntation. For Random Selection and Similarity Se-\nlection, the computation complexity for routing\nis O(kdmodel). For MLP Selection, we use a two-\nlayer feed-forward network as the architecture. The\ninput dimension is dmodel, the intermediate dimen-\nsion is k, and the output dimension is k. The non-\nlinear activation function is tanh(·). Its computa-\ntion complexity is O(kdmodel + k2). Compared to\nthe computation complexity of FFNs of the origi-\nnal model, O(dmodel ·dff ), the computation cost\nof routers is ignorable because kis much smaller\nthan dff . For example, kis 128 and dff is 4096\nfor T5-Large. For the training of our MLP routers,\nwe adopt cross-entropy as the training objective\nand use the Adam optimizer with the learning rate\nof 10−2. The batch size is set to 512 and the num-\nber of epochs is set to 10. We sample nearly 500\nthousand input representations from the training\n2https://github.com/ndanielsen/\nSame-Size-K-Means\nModel SST-2 MNLI RACE\nSmall 90.9 82.4 44.7\nSmall-Distill 91.9 82.6 50.6\nBase 94.0 86.4 71.7\nLarge 96.2 89.5 81.3\nXLarge 96.9 90.5 85.6\nTable 1: Original Performance of different models on\nthree downstream tasks. The model architecture is T5.\ndata and split them into the training and develop-\nment sets with the ratio of 9 : 1. Note that we only\nuse the activation information as supervision. The\ntraining time of each FFN is about several minutes\non a single GPU.\n4.2 MoEfy ReLU-based Models\nIn this subsection, we evaluate MoEﬁcation on dif-\nferent T5 models. We consider two factors: the\nmodel size and whether the model is compressed.\nFor the model size, we use ﬁve variants of T5 (Raf-\nfel et al., 2020), from T5-Small to T5-XLarge. For\nconvenience, we directly use the scale names as\nthe abbreviations. To investigate the inﬂuence of\nmodel compression, we compress T5-Large to T5-\nSmall by classic knowledge distillation (Hinton\net al., 2015). Speciﬁcally, the teacher model is\na ﬁne-tuned T5-Large and the student model is a\npre-trained T5-Small. The distilled model is de-\nnoted by T5-Small-Distill. The expert construction\nand selection methods used here are Co-activation\nGraph Split and MLP Selection, which are proved\nto be the best combination in Section 4.4.\nWe report the performance of these models on\nthree datasets, SST-2, MNLI, and RACE, in Ta-\nble 1. They are the representative datasets for\nsingle-sentence classiﬁcation, sentence-pair clas-\nsiﬁcation, and reading compression, respectively.\nThe original performance of PLMs grows as the\nmodel size grows, and knowledge distillation im-\nproves the performance of T5-small.\nWe ﬁrst calculate the activation statistics of dif-\nferent models by inputting the training data of each\ndataset. The results are shown in Figure 2. From\nthe ﬁgure, we have three observations. (1) The acti-\nvations of these models are sparse. Different from\nthe previous study on models trained with smaller\ndatasets, where the activation ratios are range from\n10% to 50% (Geva et al., 2021) 3, we ﬁnd most\n3Since the activation ratios of a randomly-initialized model\nare around 50%, we guess these models do not make full use\nof their parameters.\n5\n881\n(a) SST-2 (b) MNLI (c) RACE\nFigure 2: CDF of the ratio of activated neurons for each input with different models on three datasets.\n(a) SST-2 (b) MNLI (c) RACE\nFigure 3: Relative performance of MoEﬁed models with different sizes on three datasets. Dynamically selecting\n10% to 20% neurons can recover nearly 98% original performance for large models such as T5-XLarge.\ninputs activate less than 10% of the neurons. (2)\nThe activations of larger models are sparser than\nthose of smaller models. For example, 80% inputs\nonly activate less than 3% neurons in T5-XLarge\nwhile 40% inputs activate more than 3% neurons\nin T5-Small. (3) The sparsity is less related to dis-\ntillation than the model size. The CDF curve of\nT5-Small-Distill is close to that of T5-Small.\nThen, we compare the performance of MoEﬁed\nmodels with different sizes and ratios of selected\nneurons and report the results in Figure 3. To mea-\nsure the performance of MoEﬁcation, we calculate\nthe relative performance of the MoEﬁed model to\nthe original model. From the ﬁgure, we have four\nobservations. (1) MoEﬁcation works well with\nall models on all three datasets. MoEﬁed models\nuse 10% to 30% of FFN parameters while main-\ntaining over 95% original performance. (2) The\nlarger models can use fewer neurons to recover the\noriginal performance. For example, T5-XLarge\nachieves nearly 98% relative performance on SST-\n2 and MNLI with 10% neurons while T5-Small\nachieves the same results with 30% to 40% neu-\nrons. This result is consistent with the activation\nstatistics, that is, larger models are sparser. We\ncan expect that MoEﬁcation can provide better efﬁ-\n(a)\n<latexit sha1_base64=\"IiQcAAYHuIDgHl1FpQD455bTZiw=\">AAACHHicbVBNS8NAEN34WetX1aOXYBHqpSSi6FH04rEWW8WmlM122i7ubsLuRC0h/8Kr/TXexKvgjxHctD1odWDg8d4Mb+aFseAGPe/TmZtfWFxaLqwUV9fWNzZLW9tNEyWaQYNFItK3ITUguIIGchRwG2ugMhRwE95f5PrNA2jDI3WNwxjakvYV73FG0VJ3AcITphV6kHVKZa/qjcv9C/wpKJNp1Tqlr6AbsUSCQiaoMS3fi7GdUo2cCciKQWIgpuye9qFloaISTDsdX5y5+5bpur1I21bojtmfGymVxgxlaCclxYGZ1XLyP62VYO+0nXIVJwiKTYx6iXAxcvP33S7XwFAMLaBMc3urywZUU4Y2pF8uobQ/KHhkkZRUddOgnqVBbhiGaT3L8/Jn0/kLmodV/6h6fHVUPjufJlcgu2SPVIhPTsgZuSQ10iCMKPJMXsjIGTmvzpvzPhmdc6Y7O+RXOR/fKXSjMA==</latexit>\n(b)\n<latexit sha1_base64=\"ftA9/Pira2wA42vjPoM8P5ON+gI=\">AAACFXicbVBNT8JAEN3iF+IX6tFLIzHBC2kNRo9ELx4RRUhoQ3a3C2zY3Ta7Ww1p+hO8yq/xZrx69seYuIUeBJxkkpf3ZvJmHooYVdpxvq3C2vrG5lZxu7Szu7d/UD48elJhLDFp45CFsougIowK0tZUM9KNJIEcMdJB49tM7zwTqWgoHvUkIj6HQ0EHFENtqIcqOu+XK07NmZW9CtwcVEBezX75xwtCHHMiNGZQqZ7rRNpPoNQUM5KWvFiRCOIxHJKegQJyovxkdmpqnxkmsAehNC20PWP/biSQKzXhyExyqEdqWcvI/7RerAfXfkJFFGsi8NxoEDNbh3b2tx1QSbBmEwMgltTcauMRlBBrk86CC+LmB0FecMg5FEHitdLEywwRSlppavJyl9NZBU8XNbdeu7yvVxo3eXJFcAJOQRW44Ao0wB1ogjbAYAhewRuYWlPr3fqwPuejBSvfOQYLZX39At6kn9I=</latexit>\nFigure 4: (a) CDF of the ratio of activated neurons in\nBERT-Large on SST-2, MNLI, and RACE. (b) Relative\nperformance of MoEﬁed BERT-Large.\nciency with super large models. (3) Difﬁcult tasks\nrequire models to select more experts to maintain\nthe performance. From Table 1, we can see that the\naccuracy of RACE is much lower than the other\ntwo tasks, and hence we think RACE is more dif-\nﬁcult. Correspondingly, the relative performance\nwith 10% neurons on RACE is also lower than\nthose on the other tasks. (4) MoEﬁcation works\nsimilarly on T5-Small and T5-Small-Distill, which\nindicates that MoEﬁcation can work with knowl-\nedge distillation for more efﬁcient inference.\n4.3 MoEfy GeLU-based Models\nIn addition to using ReLU as the activation func-\ntion, many PLMs use GeLU (Hendrycks and Gim-\npel, 2016), including BERT (Devlin et al., 2019)\n6\n882\nand GPT (Brown et al., 2021). In this subsec-\ntion, we study whether BERT, which is the most\nrepresentative GeLU-based model, can be MoE-\nﬁed. Considering that GeLU gives negative inputs\nsmall activations instead of 0, we ﬁrst transform\na GeLU-based BERT into a ReLU-based BERT,\nand then MoEfy the ReLU-based model. Speciﬁ-\ncally, we initialize a ReLU-based BERT using the\npre-trained parameters of a BERT-Large4 and train\nthe ReLU-based BERT on the pre-training corpus\nfor the adaptation of the change of activation func-\ntions. In this work, we use the pre-training frame-\nwork provided by NVIDIA 5 and keep all hyper-\nparameters unchanged. Wikipedia and Bookcor-\npus are used as the pre-training corpus. In the\nexperiments, after 400 optimization steps, the pre-\ntraining loss is close to that of the original model.\nHence, the adaptation cost is much smaller than the\npre-training cost (about 10000 steps). Meanwhile,\nthe downstream performance of the ReLU-based\nmodel is comparable to the original model ( 93.1\nv.s 93.5 on SST-2 and 84.8 v.s 85.2 on MNLI).\nBased on this ReLU-based BERT-Large, we study\nthe sparse activation phenomenon and the effect of\nMoEﬁcation and report the results in Figure 4.\nFrom this ﬁgure, we have two observations: (1)\nThe sparse activation phenomenon still exists in\nBERT. For example, more than 80% of inputs ac-\ntivate less than 10% of neurons. It reveals the\ngenerality of the sparse activation phenomenon in\npre-trained Transformers. It will be an interesting\ndirection to explain this phenomenon empirically\nor theoretically in the future. (2) MoEﬁcation also\narchives good performance on BERT. For exam-\nple, selecting 30% to 40% of neurons can recover\n97% performance. Since the activation of BERT\nis slightly denser than that of T5, it requires more\nneurons to recover most performance.\n4.4 Comparisons of MoEﬁcation Strategies\nTo ﬁnd the most effective MoEﬁcation strategy, we\nevaluate different combinations of expert construc-\ntion and selection methods. We use T5-Large and\nalso set the ratio of selected neurons to 20%. The\nresults are shown in Table 2. From the table, we\nhave two observations:\n(1) For expert construction, Co-activation Graph\n4https://catalog.ngc.nvidia.com/orgs/\nnvidia/models/bert_pyt_ckpt_large_\npretraining_amp_lamb\n5https://github.com/NVIDIA/\nDeepLearningExamples\nConstruction Selection SST-2 MNLI RACE\n- - 96.2 89.5 81.3\nRandom\nGroundtruth 95.9 87.3 80.0\nRandom 65.9 36.3 29.2\nSimilarity 90.3 75.9 56.7\nMLP 94.1 84.1 75.0\nGroundtruth 95.5 88.8 80.9\nParameter Random 70.6 36.4 41.8\nClustering Similarity 86.7 66.3 63.6\nMLP 95.9 87.5 78.7\nGroundtruth 96.3 89.1 80.8\nCo-Activation Random 85.3 68.5 54.7\nGraph Similarity 92.2 81.4 71.0\nMLP 95.4 87.5 79.0\nTable 2: Comparisons of different combinations of\nexpert construction and selection methods using T5-\nLarge. The ﬁrst row is the original performance. The\nbest results in each group are underlined and the best\nresults on each dataset are in boldface.\nRatio FLOPS CPU GPU\n50.0% 1.50 1.43 1.15\n25.0% 2.00 1.98 1.20\n12.5% 2.40 2.28 1.47\nTable 3: Speedup of FLOPS, CPU and GPU with dif-\nferent ratios of selected neurons.\nSplit is the best method according to the overall\nperformance. Compared to the other two meth-\nods, Co-activation Graph Split directly uses the\nco-activation information to group the neurons ac-\ntivating simultaneously into the same expert.\n(2) For expert selection, the performance of\nGroundtruth Selection is close to that of the origi-\nnal model, which indicates that 20% parameters of\nFFNs are sufﬁcient to achieve good performance on\nT5-Large. Meanwhile, MLP Selection is the best\nexpert selection method and can work well with\nboth Parameter Clustering Split and Co-activation\nGraph Split.\n5 Analysis\nIn this section, we analyze the efﬁciency and rout-\ning patterns of MoEﬁed models.\n5.1 Efﬁciency Improvement\nIn this subsection, we show the efﬁciency im-\nprovement brought by MoEﬁcation. We synthe-\nsize a batch of sequences with the input and output\nlengths of 64 and evaluate T5-Large on the data. To\ncomprehensively show the efﬁciency improvement,\n7\n883\nFigure 5: Selection Frequency of 64 experts in each\nencoder layer of MoEﬁed T5-Small. The frequency of\nideal balance selection is 0.2 while the distribution is\nmuch unbalanced.\nwe report the relative speedup based on FLOPS,\nCPU, and GPU in Table 3. The FLOPS is estimated\naccording to the statistics provided by Brown et al.\n(2021). The results of CPU and GPU are tested\non an Intel Broadwell CPU and an NVIDIA Tesla\nV100 GPU, respectively.\nFrom this table, we have three observations:\n(1) MoEﬁcation can signiﬁcantly reduce the total\nFLOPS, such as 2x speedup in the ratio of 25%.\nMeanwhile, the speedup on CPU is close to that\non FLOPS. Considering that CPU is widely used\nfor model inference in real-world scenarios, MoEﬁ-\ncation is practical for the acceleration of various\nNLP applications. (2) The smaller the ratio, the\nsmaller the gain. For example, the gain of halving\n25% (to 12.5%) is 1.2x while the gain of halving\n50% (to 25%) is 1.3x. Although the FLOPS re-\nduction of feed-forward networks is linear in the\nratio, the cost of attention networks is unchanged\nand becomes the bottleneck. Hence, 20% is a good\nratio, which can have a signiﬁcant speedup (2x)\nand maintain most performance. (3) Since some\nof the operations of MoE cannot be easily paral-\nleled, the speedup on GPU is smaller than that\non GPU. Recently, some packages such as Fast-\nMoE (He et al., 2021) and Deepspeed-MoE (Rajb-\nhandari et al., 2022) are working on paralleling the\ninference of MoE models on distributed computing\nplatforms and already have some promising results.\nWe believe the bottleneck of parallel computing in\nMoE models will be well solved in the future.\n(a) The 8 most selected experts\n<latexit sha1_base64=\"uQ9s3lDgnZCE7dAlzOw8in4mOBU=\">AAACD3icbVC7TgJBFJ3FF+Jr1dJmItFgQ3YNRkqijSUmvBIgZHa4wITZR2buGsiGP7DxV2wsNMbW1s6/cXgUCp5kkpNz7p2Zc7xICo2O822l1tY3NrfS25md3b39A/vwqKbDWHGo8lCGquExDVIEUEWBEhqRAuZ7Eure8Hbq1x9AaREGFRxH0PZZPxA9wRkaqWOftxBGmOTYBa0MgBapH2qk5j7gCF0KowgU6knHzjp5Zwa6StwFyZIFyh37q9UNeexDgFwyrZuuE2E7YQoFlzDJtGINEeND1oemoQHzQbeTWZ4JPTNKl/ZCZU6AdKb+3kiYr/XY98ykz3Cgl72p+J/XjLFXbCciiGKEgM8f6sWSYkin5dCuUCa3HBvCuBLmr5QPmGKmC6UzpgR3OfIqqV3m3UL+6r6QLd0s6kiTE3JKcsQl16RE7kiZVAknj+SZvJI368l6sd6tj/loylrsHJM/sD5/AP3jnAo=</latexit>\n(b) The 8 least selected experts\n<latexit sha1_base64=\"3mbs5xbU/j+V1ATSweY8mqexfvg=\">AAACEHicbVC7TgJBFJ3FF+Jr1dJmIjFiQ3YNRkqijSUmvBIgZHa4wITZR2buGsiGT7DxV2wsNMbW0s6/cXgUCp5kkpNz7p2Zc7xICo2O822l1tY3NrfS25md3b39A/vwqKbDWHGo8lCGquExDVIEUEWBEhqRAuZ7Eure8Hbq1x9AaREGFRxH0PZZPxA9wRkaqWOftxBGmOS8C1oZAC1SCUwjNRcCR+hSGEWgUE86dtbJOzPQVeIuSJYsUO7YX61uyGMfAuSSad10nQjbCVMouIRJphVriBgfsj40DQ2YD7qdzAJN6JlRurQXKnMCpDP190bCfK3HvmcmfYYDvexNxf+8Zoy9YjsRQRQjBHz+UC+WFEM6bYd2hTK55dgQxpUwf6V8wBQzXSidMSW4y5FXSe0y7xbyV/eFbOlmUUeanJBTkiMuuSYlckfKpEo4eSTP5JW8WU/Wi/VufcxHU9Zi55j8gfX5A7PJnGs=</latexit>\nFigure 6: Input similarities between experts in the last\nencoder layer of MoEﬁed T5-Small. For the most\nselected experts, both the self-similarities and inter-\nsimilarities are low. For the least selected experts, the\nself-similarities are much higher than inter-similarities.\n5.2 Routing Patterns\nIn this subsection, we investigate the routing pat-\nterns of MoEﬁed models. First, we count the se-\nlection frequency of each expert. Previous work\nintroduces training objectives to ensure balance se-\nlection to make full use of model parameters (Lep-\nikhin et al., 2021; Fedus et al., 2021). We report the\nresults of the MoEﬁed T5-Small with 20% experts\non SST-2 in Figure 5. From the ﬁgure, we observe\nthat the frequency distribution of expert selection is\nmuch unbalanced. There are some commonly-used\nexperts, whose frequencies are higher than 80%.\nMeanwhile, there are also some long-tail experts\nwhose frequencies are lower than 10%.\nThen, we calculate the self-similarities and inter-\nsimilarities of inputs between experts by sampling\n10,000 inputs for each expert. We report the results\nof the last layer in Figure 6. For the most selected\nexperts, which are selected by most inputs, the\nself-similarities are close to the inter-similarities.\nFor the least selected experts, the self-similarities\nare much higher than the inter-similarities, which\nsuggests that the inputs of each expert have obvious\ncluster structure.\nFrom these results, we can conclude the routing\npatterns of MoEﬁed models: there are some gen-\neral experts, which can work for most inputs, and\nsome input-speciﬁc experts, which are seldom used\nand may work in speciﬁc domains or tasks. This\nobservation may inspire future work on training\nMoE models from scratch.\n6 Conclusion\nIn this work, we verify that Transformer FFNs are\nnaturally mixtures of experts and propose MoEﬁ-\ncation, which utilizes the sparse activation phe-\nnomenon in FFNs to convert a normal model to\n8\n884\nits MoE version with the same parameters. Ex-\nperimental results show that MoEﬁed models can\nachieve comparable performance to the original\nmodels using only 10% to 30% of FFN parame-\nters. Correspondingly, it signiﬁcantly reduces the\nFLOPS of inference, e.g., 2x speedup with 20%\nof FFN parameters. Besides, by studying the rout-\ning patterns of MoEﬁed models, we ﬁnd that there\nare general and input-speciﬁc experts, which may\ninspire future work on training MoE models. We\nhope MoEﬁcation can beneﬁt real-world applica-\ntions of PLMs with better efﬁciency and beneﬁt the\ninterpretation of the inner mechanism of FFNs.\nAcknowledgement\nThis work is supported by the National Key R&D\nProgram of China (No. 2020AAA0106502), In-\nstitute Guo Qiang at Tsinghua University, Beijing\nAcademy of Artiﬁcial Intelligence (BAAI), and\nInternational Innovation Center of Tsinghua Uni-\nversity, Shanghai, China. We thank Chenglei Si,\nTianyu Gao and other members of THUNLP for\ntheir helpful discussion and feedback. Zhengyan\nZhang conducted the experiments. Zhengyan\nZhang, Yankai Lin, Zhiyuan Liu, and Peng Li wrote\nthe paper. Maosong Sun and Jie Zhou provided\nvaluable advices to the research.\nReferences\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin\nJin, Xin Jiang, Qun Liu, Michael R. Lyu, and Irwin\nKing. 2021. Binarybert: Pushing the limit of BERT\nquantization. In Proceedings of ACL/IJCNLP, pages\n4334–4348.\nYoshua Bengio. 2013. Deep learning of representa-\ntions: Looking forward. In Proceedings of SLSP ,\npages 1–37.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2021. Language models are Few-Shot\nlearners. In Proceedings of NeurIPS , pages 1877–\n1901.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. , pages\n177–190.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu\nWei. 2021. Knowledge neurons in pretrained trans-\nformers. arXiv preprint arXiv:2104.08696.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nProceedings of EMNLP, pages 6491–6506.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL , pages 4171–\n4186.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of IWP, pages 9–16.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint 2101.03961.\nLaurence J Garey. 1999. Brodmann’s’ localisation in\nthe cerebral cortex’.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. In Proceedings of EMNLP ,\npages 5484–5495.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nTEP, pages 1–9.\nIan Goodfellow, David Warde-Farley, Mehdi Mirza,\nAaron Courville, and Yoshua Bengio. 2013. Max-\nout networks. In Proceedings of ICML, pages 1319–\n1327.\nCharles G Gross. 2002. Genealogy of the “grand-\nmother cell”. The Neuroscientist, 8(5):512–518.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu,\nXiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao,\nAo Zhang, Liang Zhang, Wentao Han, Minlie\nHuang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu,\nZhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-\nRong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun\nZhu. 2021. Pre-Trained models: Past, present and\nfuture. arXiv preprint 2106.07139.\nJiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Ji-\ndong Zhai, and Jie Tang. 2021. FastMoE: A fast\nMixture-of-Expert training system. arXiv preprint\n2103.13262.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (GELUs). arXiv preprint\n1606.08415.\n9\n885\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nGeoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,\nIlya Sutskever, and Ruslan R Salakhutdinov. 2012.\nImproving neural networks by preventing co-\nadaptation of feature detectors. arXiv preprint\n1207.0580.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic\nBERT with adaptive width and depth. In Proceed-\nings of NeurIPS.\nGao Huang, Danlu Chen, Tianhong Li, Felix Wu, Lau-\nrens van der Maaten, and Kilian Weinberger. 2018.\nMulti-Scale dense networks for resource efﬁcient\nimage classiﬁcation. In Proceedings of ICLR.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan,\nand Geoffrey E Hinton. 1991. Adaptive mixtures of\nlocal experts. Neural Comput., 3(1):79–87.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of ACL, pages 3651–\n3657.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of EMNLP, pages\n4163–4174.\nGeorge Karypis and Vipin Kumar. 1998. A fast and\nhigh quality multilevel scheme for partitioning irreg-\nular graphs. SIAM J. Sci. Comput., 20(1):359–392.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Pro-\nceedings of ICLR.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark se-\ncrets of BERT. In Proceedings of EMNLP-IJCNLP,\npages 4365–4374.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. ImageNet classiﬁcation with deep con-\nvolutional neural networks. In Proceedings of\nNeurIPS, pages 1106–1114.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of EMNLP, pages 785–794.\nDmitry Lepikhin, Hyoukjoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\nGShard: Scaling giant models with conditional com-\nputation and automatic sharding. In Proceedings of\nICLR.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. BASE layers:\nSimplifying training of large, sparse models. arXiv\npreprint 2103.16716.\nLei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li,\nJie Zhou, and Xu Sun. 2021. CascadeBERT: Accel-\nerating inference of pre-trained language models via\ncalibrated complete models cascade. In Findings of\nEMNLP, pages 475–486.\nMikko I. Malinen and Pasi Fränti. 2014. Balanced k-\nmeans for clustering. In Proceedings of SSSPR, vol-\nume 8621, pages 32–41.\nChristopher D. Manning, Kevin Clark, John Hewitt,\nUrvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. Proc. Natl. Acad. Sci.\nUSA, 117(48):30046–30054.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt. arXiv preprint arXiv:2202.05262.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proceedings of ICLR.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Pro-\nceedings of NeurIPS, pages 14014–14024.\nVinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed\nlinear units improve restricted boltzmann machines.\nIn Proceedings of ICML, pages 807–814.\nBruno A Olshausen and David J Field. 1996. Emer-\ngence of simple-cell receptive ﬁeld properties by\nlearning a sparse code for natural images. Nature,\n381(6583):607–609.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed Text-to-Text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Min-\njia Zhang, Reza Yazdani Aminabadi, Ammar Ah-\nmad Awan, Jeff Rasley, and Yuxiong He. 2022.\nDeepSpeed-MoE: Advancing mixture-of-experts in-\nference and training to power next-generation ai\nscale. arXiv preprint arXiv:2201.05596.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nEMNLP, pages 2383–2392.\nSahana Ramnath, Preksha Nema, Deep Sahni, and\nMitesh M. Khapra. 2020. Towards interpreting\nBERT for reading comprehension based QA. InPro-\nceedings of EMNLP, pages 3236–3242.\n10\n886\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\nand Jason Weston. 2021. Hash layers for large\nsparse models. arXiv preprint arXiv:2106.04426.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint 1910.01108.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe Sparsely-Gated Mixture-of-Experts layer. In\nProceedings of ICLR.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of EMNLP, pages 1631–1642.\nXavier Suau, Luca Zappella, and Nicholas Apostoloff.\n2020. Finding experts in transformer models. arXiv\npreprint arXiv:2005.07647.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of EMNLP , pages 4323–\n4332.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie\nLiu, Yiming Yang, and Denny Zhou. 2020. Mo-\nbileBERT: a compact Task-Agnostic BERT for\nResource-Limited devices. In Proceedings of ACL,\npages 2158–2170.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of ACL, pages 4593–4601.\nAshish Vaswani, Noam Shazeer, Niki Parmar, and\nJakob Uszkoreit. 2017. Attention is all you need. In\nProceedings of NeurIPS, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing Multi-Head\nSelf-Attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of ACL,\npages 5797–5808.\nEric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subra-\nmanian, Matt Gardner, and Sameer Singh. 2019. Al-\nlenNLP interpret: A framework for explaining pre-\ndictions of NLP models. In Proceedings of EMNLP-\nIJCNLP, pages 7–12.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of ICLR.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nWenxuan Wang and Zhaopeng Tu. 2020. Rethinking\nthe value of transformer components. In Proceed-\nings of COLING, pages 6019–6029.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTACL, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of NAACL-HLT, pages 1112–1122.\nWenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen,\nYi Yang, and Shilei Wen. 2020. Dynamic inference:\nA new approach toward efﬁcient video action recog-\nnition. arXiv preprint 2002.03342.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. DeeBERT: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings of\nACL, pages 2246–2251.\nDeming Ye, Yankai Lin, Yufei Huang, and Maosong\nSun. 2021. TR-BERT: dynamic token reduction for\naccelerating BERT inference. In Proceedings of\nNAACL-HLT, pages 5798–5809.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8bit BERT.\narXiv preprint 1910.06188.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In Proceedings of\nNeurIPS.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. TernaryBERT:\nDistillation-aware ultra-low bit BERT. In Proceed-\nings of EMNLP, pages 509–521.\nZhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun Liu,\nand Maosong Sun. 2021. Know what you don’t\nneed: Single-Shot Meta-Pruning for attention heads.\nAI Open, 2:36–42.\n11\n887\nA MoEﬁcation on Other Datasets\nFor text classiﬁcation, we use GLUE bench-\nmark (Wang et al., 2019), including MNLI-\nmatched (Williams et al., 2018), QNLI (Rajpurkar\net al., 2016), QQP6, RTE (Dagan et al., 2006), SST-\n2 (Socher et al., 2013), MRPC (Dolan and Brock-\nett, 2005), CoLA (Warstadt et al., 2019), and STS-\nB (Giampiccolo et al., 2007). For reading compre-\nhension, we use SQuAD (Rajpurkar et al., 2016)\nand RACE (Lai et al., 2017), which are the rep-\nresentative datasets for span extraction and multi-\nchoice QA, respectively. We report the results on\ntheir development sets. For MNLI, QNLI, QQP,\nRTE, SST-2, MRPC, RACE, we use accuracy as\nthe metric. For CoLA, we use matthews correla-\ntion coefﬁcient as the metric. For STS-B, we use\npearson and spearman correlation as the metrics.\nFor SQuAD, we use F1 score as the metric.\nWe evaluate MoEﬁcation on several downstream\nnatural language understanding tasks with T5-\nLarge. The ratio of selected neurons is set to 20%,\nwhich is sufﬁcient for T5-Large as show in Fig-\nure 2. In practice, there is still a gap between the\nperformance of MoEﬁed models and that of origi-\nnal models because selected experts cannot cover\nall positive neurons with a limited computation\nbudget. Hence, the outputs of MoEﬁed models will\nbe slightly different from those of original models.\nTo calibrate MoEﬁed models, we further ﬁne-tune\nthe models on the training set, namely parameter\ncalibration. Considering that current routers are\nbased on the ﬁrst layers of FFNs ( W1 and b1),\nwe only optimize the second layers of FFNs (W2\nand b2) to ensure routers can also work well af-\nter ﬁne-tuning. We use a small learning rate of\n10−7 for calibration. The other hyper-parameters\nremain the same as ﬁne-tuning. The results are\nshown in Table 4. MoEﬁed refers to the combi-\nnation of Co-activation Graph Split and MLP Se-\nlection. MoEﬁed+GT refers to the combination of\nCo-activation Graph Split and Groundtruth Selec-\ntion. MoEﬁed+Calib is the calibrated version of\nMoEﬁed. To calculate the average performance,\nwe also include SST-2, MNLI, and RACE.\nWe observe that MoEﬁcation introduces small\nperformance loss (about 1.5% on average) with an\n80% reduction of the computation cost in FFNs.\nMeanwhile, calibration can effectively deal with\nthe issue of the precision errors brought by MoEﬁ-\ncation. For example, MoEﬁed+Calib improves\n6https://data.quora.com\nMoEﬁed by nearly 4% on CoLA and achieves the\nsame average performance as MoEﬁed+GT.\nB Activation Statistics before\nFine-tuning\nWe count the activation statistics of PLMs be-\nfore ﬁne-tuning on the pre-training data containing\nabout 50,000 input tokens. The results are shown\nin Figure 7. We observe that PLMs before ﬁne-\ntuning also have the sparse activation phenomenon\nand ﬁne-tuning brings little change.\n0 5 10\nRatio of Activated Neurons (%)\n0\n20\n40\n60\n80\n100CDF (%)\nXLarge\nLarge\nBase\nSmall\nFigure 7: CDF of the ratios of activated neurons for\neach input with different models before ﬁne-tuning.\nThen, we compare the activations of pre-trained\nmodels and those of ﬁne-tuned models. We use\nthe average ratio of activated neurons as the index.\nThe results are shown in Table 5. We observe that\nﬁne-tuning increases the average activation ratio\nfor most models. The reason may be that differ-\nent neurons start to learn the same task-speciﬁc\npatterns during ﬁne-tuning. Interestingly, the in-\ncrease on RACE is smaller than that on the other\ndatasets. Since RACE is more difﬁcult than the\nother datasets, there should be more task-speciﬁc\npatterns in RACE and less neurons learn the same\npatterns. Moreover, the pre-training task MLM re-\nquires more patterns than RACE so the ratios of\nMLM are lowest.\nC Results of Graph Partition\nCo-activation Graph Split achieves good perfor-\nmance in expert construction. Here, we study\nwhether the co-activation graph is suitable for parti-\ntioning. We report the results of graph partition of\nT5-Large on SST-2 in Figure 8. Smaller ratios of\nedgecuts, which straddle partitions, mean that more\nco-activation pairs are included in experts. We only\n12\n888\nMNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B RACE SQuAD 1.1 Avg.\nOriginal 89.5 94.4 91.7 87.1 96.2 88.0 59.4 91.2/90.9 81.3 93.2 87.2\nMoEﬁed 87.5 93.2 90.2 86.4 95.4 87.5 55.5 90.6/90.3 79.0 92.2 85.7 (-1.5)\n+GT 89.1 94.1 91.4 86.4 96.3 88.3 58.8 90.9/90.8 80.8 93.2 86.9 (-0.3)\n+Calib 88.7 93.6 91.3 87.5 96.2 89.3 59.4 91.0/90.6 79.9 92.3 86.9 (-0.3)\nTable 4: Results of T5-Large on GLUE benchmark and two QA datasets. The last row reports the differences\nbetween the original model and MoE+Calib. MoEﬁed models with parameter calibration achieve comparable\nperformance to original models.\nSmall Base Large XLarge\nMLM 4.18 2.85 2.17 1.52\nSST-2 5.53 2.24 2.50 2.46\nMNLI 5.59 3.25 2.44 2.45\nRACE 4.94 3.08 1.98 1.79\nTable 5: Average ratio of activated neurons for each\ninput. MLM represents the pre-trained models with\nmasked language modeling. SST-2, MNLI, RACE rep-\nresent the ﬁne-tuned models on each dataset.\nreport the results of encoder layers because all ra-\ntios of decoder layers are smaller than 0.001. From\nthis ﬁgure, we can see that the overall ratio is small\nand these graphs are suitable for partitioning.\n0 5 10 15 20\nLayer\n0.0\n0.1\n0.2\n0.3Ratio of Edgecuts\nFigure 8: Ratio of edgecuts in different layers.\nD Accuracy of MLP Selection\nMLP selection trains MLPs to ﬁt the groundtruth\nselection. In this part, we report the accuracy of\nMLPs in T5-Large ﬁne-tuned on SST-2. The results\nare shown in Figure 9 and 10. The overall accuracy\nof the encoder is about 0.8 and the overall accuracy\nof the decoder is about 0.7.\nE Relative Cost of Routing\nIn this work, we set the number of neurons in each\nexpert to 32. Then, the number of experts in each\nlayer kis dff\n32 . In most Transformer models, dff =\n4dmodel. The computation complexity of Similarity\n0 5 10 15 20\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nFigure 9: Accuracy of MLPs of encoder layers.\n0 5 10 15 20\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nFigure 10: Accuracy of MLPs of decoder layers.\nSelection for each input is\nO(kdmodel) =O(d2\nmodel\n8 ). (9)\nThe computation complexity of FFNs for each in-\nput is\nO(dmodel ·dff ) =O(4d2\nmodel). (10)\nThen, the relative cost of routing to that of FFNs is\nconstant for different models. It is also similar to\nMLP Selection.\nF Graph Partitioning Algorithm\nThe goal of graph partitioning is to divide a graph\ninto several sub-graphs where the number of edges\ncrossing sub-graphs is minimized. In this work,\nwe use the graph partitioning algorithm proposed\nby Karypis and Kumar (1998). The graph partition-\ning algorithm consists of three phases: coarsening\nphase, partitioning phase, and reﬁnement phase.\n(1) In the coarsening phase, we create new super\nnodes by grouping nodes that are highly connected\ntogether. For example, if the weight of the edge\n13\n889\nFigure 11: Comparison between MoEﬁcation and\nmodel pruning.\nModel MLM Loss\nMoE Pre-training 3.09\nStandard Pre-training 2.88 (-0.21)\n+MoEﬁcation 3.02 (-0.07)\n+GT 2.95 (-0.14)\nTable 6: Comparisons of MoE models pre-trained from\nscratch and modiﬁed by MoEﬁcation. We report the\nMLM loss on the validation set. Standard pre-training\nwith MoEﬁcation is better than pre-training a MoE\nmodel from scratch.\nbetween two nodes is large, these two nodes will be\ngrouped together. In the setting of coarsening co-\nactivation graphs studied in this work, two neurons\nthat often activate simultaneously will be treated as\na new super neuron. (2) In the partitioning phase,\nwe start with an initial bipartition of the super node\ngraph and then iteratively search for super nodes\nfrom each part of the graph, such that swapping\nthem leads to a partition with a smaller number of\ncrossing edges. To divide a graph into kparts, we\nneed log krounds of bipartition. (3) In the reﬁne-\nment phase, we project super nodes to the original\nnodes and then continue to iteratively swap nodes\nto reduce the number of crossing edges.\nG Comparisons with Model Pruning\nBased on the ﬁne-tuned T5-Large on SST-2, we\ncompare MoEﬁcation with model pruning, which\nomits the weight having small values. The results\nare shown in Figure 11. We observe that model\npruning signiﬁcantly degrades the performance.\nHowever, MoEﬁcation achieves good performance\nby selectively activating parts of the network ac-\ncording to input.\nH MoEﬁcation vs. MoE pre-training\nIn this subsection, we compare the performance\nof two kinds of MoE models. The ﬁrst one is\npre-trained from scratch. The second one is trans-\nformed from a standard model by MoEﬁcation. For\nfair comparisons, we pre-train one MoE model and\none standard model with the same model size from\nscratch using WikiText-103 (Merity et al., 2017).\nThe pre-training objective is masked language mod-\neling (MLM). The model architecture is the same\nas T5-Small. For pre-training, we use the batch size\nof 4096, the learning rate of 0.01, the maximum\nsequence length of 512, and the Adam optimizer.\nThe number of experts is set to 64 and the router\nwill select 32 of them for a single input.\nWe report the MLM loss on the validation set in\nTable 6. From the table, we have two observations.\n(1) The loss of the standard pre-trained model is\nlower than that of the pre-trained MoE model. We\nguess that the optimization of MoE models is difﬁ-\ncult than that of the standard models because of the\nrestricted selection of MoE models. (2) MoEﬁed\nmodels achieve better performance than the pre-\ntrained MoE model. It indicates that pre-training a\nstandard model then conducting MoEﬁcation can\nbe a better option than pre-training an MoE model\nfrom scratch.\n14\n890",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7727310061454773
    },
    {
      "name": "Inference",
      "score": 0.4308803677558899
    },
    {
      "name": "Transformer",
      "score": 0.4195016622543335
    },
    {
      "name": "Artificial intelligence",
      "score": 0.26914680004119873
    },
    {
      "name": "Physics",
      "score": 0.08221334218978882
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I29955533",
      "name": "Center for Information Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210157323",
      "name": "South China Institute of Collaborative Innovation",
      "country": "CN"
    }
  ]
}