{
    "title": "Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation",
    "url": "https://openalex.org/W3171450754",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5032506972",
            "name": "Kyeongpil Kang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5028883212",
            "name": "Kyohoon Jin",
            "affiliations": [
                "Chung-Ang University"
            ]
        },
        {
            "id": "https://openalex.org/A5111176662",
            "name": "Soyoung Yang",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5013982522",
            "name": "Soojin Jang",
            "affiliations": [
                "Chung-Ang University"
            ]
        },
        {
            "id": "https://openalex.org/A5047912015",
            "name": "Jaegul Choo",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5016930939",
            "name": "Youngbin Kim",
            "affiliations": [
                "Chung-Ang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2533307837",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2766725164",
        "https://openalex.org/W2024069573",
        "https://openalex.org/W2135029798",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2757910899",
        "https://openalex.org/W2963715460",
        "https://openalex.org/W2057990742",
        "https://openalex.org/W2009370431",
        "https://openalex.org/W2122172122",
        "https://openalex.org/W2890499923",
        "https://openalex.org/W2027958710",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2141056424",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W2032281195",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2970619458",
        "https://openalex.org/W3004377989",
        "https://openalex.org/W4288265053",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2251996233",
        "https://openalex.org/W3143596294",
        "https://openalex.org/W4231510805",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2798608854",
        "https://openalex.org/W1880262756",
        "https://openalex.org/W1993341938",
        "https://openalex.org/W3100325687",
        "https://openalex.org/W2132968903",
        "https://openalex.org/W2807629868",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2775286224",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2887558369",
        "https://openalex.org/W2152879221",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W3104215796",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2050889279",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2962903028",
        "https://openalex.org/W2172221237",
        "https://openalex.org/W4285053591",
        "https://openalex.org/W2970119519",
        "https://openalex.org/W2794536892",
        "https://openalex.org/W3010843093",
        "https://openalex.org/W2808154809"
    ],
    "abstract": "Kyeongpil Kang, Kyohoon Jin, Soyoung Yang, Soojin Jang, Jaegul Choo, Youngbin Kim. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4031–4042\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n4031\nRestoring and Mining the Records of the Joseon Dynasty via Neural\nLanguage Modeling and Machine Translation\nKyeongpil Kang\nScatter Lab\nSeoul, South Korea\nkyeongpil@scatterlab.co.kr\nKyohoon Jin\nChung-Ang University\nSeoul, South Korea\nfhzh123@cau.ac.kr\nSoyoung Yang\nKAIST\nDaejeon, South Korea\nsy_yang@kaist.ac.kr\nSoojin Jang\nChung-Ang University\nSeoul, South Korea\nsujin0110@cau.ac.kr\nJaegul Choo\nKAIST\nDaejeon, South Korea\njchoo@kaist.ac.kr\nYoungbin Kim\nChung-Ang University\nSeoul, South Korea\nybkim85@cau.ac.kr\nAbstract\nUnderstanding voluminous historical records\nprovides clues on the past in various aspects,\nsuch as social and political issues and even\nnatural science facts. However, it is generally\ndifﬁcult to fully utilize the historical records,\nsince most of the documents are not written\nin a modern language and part of the contents\nare damaged over time. As a result, restoring\nthe damaged or unrecognizable parts as well as\ntranslating the records into modern languages\nare crucial tasks. In response, we present a\nmulti-task learning approach to restore and\ntranslate historical documents based on a self-\nattention mechanism, speciﬁcally utilizing two\nKorean historical records, ones of the most vo-\nluminous historical records in the world. Ex-\nperimental results show that our approach sig-\nniﬁcantly improves the accuracy of the trans-\nlation task than baselines without multi-task\nlearning. In addition, we present an in-depth\nexploratory analysis on our translated results\nvia topic modeling, uncovering several signiﬁ-\ncant historical events.\n1 Introduction\nHistorical records are invaluable sources of infor-\nmation on the lifestyle and scientiﬁc records of our\nancestors. Humankind has learned how to handle\nsocial and political problems by learning from the\npast. The historical records also serve as the evi-\ndence of intellectual accomplishment of humanity\nover time. Given such importance, there has been a\ngreat deal of nationwide efforts to preserve these\nhistorical records. For instance, UNESCO protects\nworld heritage sites, and experts from all around the\nworld have been converting and restoring historical\nrecords in a digital form for long-term preservation.\nA representative example is the Google Books Li-\nbrary Project1. However, despite the importance of\nthe historical records, it has been challenging to\nproperly utilize the records for the following rea-\nsons. First, the nontrivial amounts of the documents\nare partially damaged and unrecognizable due to\nunfortunate historical events or environments, such\nas wars and disasters, as well as the weak durability\nof paper documents. These factors result in difﬁcul-\nties to translate and understand the records. Second,\nas most of the records are written in ancient and out-\ndated languages, non-experts are difﬁcult to read\nand understand them. Thus, for their in-depth anal-\nysis, it is crucial to recover the damaged parts and\nproperly translate them into modern languages.\nTo address these issues existing in historical\nrecords, we formulate them as the task of language\nmodeling, especially for the recovery and neural\nmachine translation, by leveraging the advanced\nneural networks. Moreover, we apply topic model-\ning to the translated historical records to efﬁciently\ndiscover the important historical events over the\nlast hundreds of years. In particular, we utilize two\nrepresentative Korean historical records: the An-\nnals of the Joseon Dynasty and the Diaries of the\nRoyal Secretariat (hereafter we refer to them as\nAJD and DRS respectively). These records, which\ncontain 50 million and 243 million characters re-\nspectively, are recognized as the largest historical\nrecords in the world. Considering their high value,\nUNESCO recognized them as the Memory of the\n1https://support.google.com/websearch/answer/9690276\n4032\nLarge-scale\nancient documents\n…\n…\n…\n…\n以司謁□□下敎⽈, 時\n原任⼤⾂ ·閣⾂·宗親·\n儀賓· □□ ·宗正卿·⼆\n品以上 ·六曹·兩司⾧\n官·承·史, □□□□ ,\n留待。\n정언 김상(⾦尙)이 아뢰기를,\n“폐인에 대해서 의리로\n처단해야 한다는 논의가\n사헌부에서 나왔습니다만 ,\n본원의 관원들이 대부분\n일이 있어 나오지 않은 데다\n신만으로는\n또 아뢰기를,\n“□□□가 대신의 뜻으로\n와서 말하기를 , ‘추국하기\n위해서는 이른 아침에 와서\n모여야 하는데 지금 이미\n저녁이 되었습니다 .\n대간(臺諫)이 - 7, 8자 원문\n훼손 - 게다\n정원이 아뢰기를,\n“ 오늘 추국 (推鞫 )해야\n하는데 , 사헌부의\n대관(臺官)이 매일 사직하고,\n숙배(肅拜) - 1, 2자 원문\n훼손 - □원(員)이 있으니, -\n4, 5자 원문 훼손 - 추국은\n어떻게 해야겠습\n사알을 통해 구전으로\n하교하기를, \"시원임 대신,\n각신 , 종친 , 의빈 , 옥당 ,\n종정경 2품 이상, 육조,\n양사 장관, 승지, 사관은\n음식을 내릴 것이니 머물러\n기다리라.\" 하였다.\nTranslation\u0000/\u0000Restoration\u0000\nText\u0000mining\u0000via\u0000topic\u0000modeling\n…\n…\nTransformer\nFigure 1: Overview of the proposed approach of recovering, translating, and mining historical documents.\nWorld.2,3 These two historical corpora contain the\ncontents of ﬁve hundred years from the fourteenth\ncentury to the early twentieth century. In detail,\nAJD consists of administrative affairs with national\nevents, and DRS contains events that occurred\naround the kings of the Joseon Dynasty. These cor-\npora are valuable as they contain diverse informa-\ntion including international relations and natural\ndisasters. In addition, the contents of the records\nare objective since the writing rules are strict that\npolitical intervention, even from the kings, is not\nallowed by their independent institution.\nAlthough DRS contains a much larger amount\nof information than AJD, only 10–20% of DRS has\nbeen translated into the modern Korean language\nby a few dozens of experts for the last twenty years.\nThe complete translation of DRS is currently ex-\npected to additionally take more than 30–40 years\nif only human experts continue to translate them.\nApplying the neural machine translation models\ninto the historical records contains several issues.\nFirst, the pre-trained models for Chinese are not\nsuitable to DRS and AJD, mainly because of the\ndifferences between Hanja and the Chinese lan-\nguage. In the past, Korean historiographers bor-\nrowed the Chinese character to write the sentences\nspoken by Koreans. As a result, diverse characters\nhad been moderated or created, and considerable\ngrammatical differences exist between the Chinese\nlanguage and Hanja. Furthermore, several parts of\nthose records are damaged and require restoration\nas shown in Fig. 2. Therefore, these damaged parts\nshould be restored in order to translate them cor-\nrectly. In order to address these issues, we propose\n2http://www.unesco.org/new/en/communication-and-in\nformation/memory-of-the-world/register/full-list-of-register\ned-heritage/registered-heritage-page-8/the-annals-of-the-c\nhoson-dynasty/\n3http://www.unesco.org/new/en/communication-and-in\nformation/memory-of-the-world/register/full-list-of-register\ned-heritage/registered-heritage-page-8/seungjeongwon-ilgi-\nthe-diaries-of-the-royal-secretariat/\na model suitable for the historical documents using\nthe self-attention mechanism.\nOverall, we propose a novel multi-task approach\nto restore the damaged parts and translate the\nrecords into a modern language. Afterward, we\nextract the meaningful historical topics from the\nworld’s largest historical records as shown in Fig. 1.\nThis study makes the following contributions:\n• We design a model based on the self-attention\nmechanism with multi-task learning to restore\nand translate the historical records. Results\ndemonstrate that our methods are effective in\nrestoring the damaged characters and translat-\ning the records into a modern language.\n• We translate all the untranslated sentences in\nDRS. We believe that this dataset would be in-\nvaluable for researchers in various ﬁelds.4\n• We present a case study that extracts meaning-\nful historical events by applying topic modeling,\nhighlighting the importance of analysis of his-\ntorical documents.\n2 Related Work\nThis work broadly incorporates three different\ntasks: document restoration, machine translation,\nand document analysis. Therefore, this section de-\nscribes studies related to the restoration of dam-\naged documents, neural machine translation, and\nthe analysis of historical records.\n2.1 Neural Machine Translation\nRecently, neural machine translation (NMT) has\nachieved outstanding achievements. Based on the\nencoder-decoder architecture, the attention mecha-\nnism (Bahdanau et al., 2015) signiﬁcantly improves\nthe performance of NMT, by calculating the target\ncontext vector in the current time step via dynam-\nically combining the encoding vectors of source\n4The codes, trained model, and datasets are accessible via\nhttps://github.com/Kyeongpil/deep-joseon-record-analysis.\n4033\n(a) (b) (c) (d)\n(a)\n(b)\n(c)\n(d)\n川□□李時益, 行己悖戾\n尹知敬, □□李敏求·趙誠立\n□□悶迫不得已先赴母所\n抄同趙□陪臣來時\nFigure 2: Examples of damaged documents. Those\ncharacters that should be put in rectangles are damaged\nor unrecognizable.\nwords. The self-attention-based networks (Vaswani\net al., 2017) consider the correlations among all\nword pairs in the source and target sentences. Based\non the success of self-attention networks, Trans-\nformer architecture for language modeling has been\nproposed, showing the forefront performances (De-\nvlin et al., 2019; Radford et al., 2019). Especially,\nthe pre-training approaches further improve the per-\nformances, since they train the model robustly with\nseveral tasks using a large document corpus. In ad-\ndition, lightweight models, such as ALBERT (Lan\net al., 2019), are proposed to reduce the model\nsize while preserving the model performance. How-\never, as most of the recent approaches focus on\npre-training with documents written in a modern\nlanguage, the model for historical datasets does not\nexist. Therefore, we adopt a lightweight model in\nthe same manner as ALBERT to efﬁciently recon-\nstruct and translate millions of documents.\nRegarding the translation task for the historical\ndocuments, several studies attempt to translate the\nancient Chinese documents into modern Chinese\nlanguage (Zhang et al., 2019b; Liu et al., 2019a).\nHowever, as they mainly attempt to translate ar-\nchaic characters into the modern language using\npaired corpus, they do not fully utilize the unpaired\ncorpus. Therefore, we improve the performance\nof machine translation for historical corpora with\nmulti-task learning with the translation and restora-\ntion tasks, which fully utilize the paired and un-\npaired corpora.\n2.2 Restoration of Historical Documents\nUnfortunately, lots of characters in the historical\nrecords are damaged or misspelled. As shown in\nFig. 2, the damaged parts are prevalent in DRS,\nwhich signiﬁcantly degrade the quality of subse-\nquent translation tasks. To address this problem,\nseveral studies focus on normalizing the misspelled\nwords (Tang et al., 2018; Domingo and Nolla,\n2018), and others further apply language model-\ning to restore the parts of the documents via deep\nneural networks (DNNs) (Caner and Haritaoglu,\n2010; Assael et al., 2019).\nRecently, the Cloze-style approach of machine\nreading comprehension (masked language model-\ning; MLM) predicts the original tokens for those\npositions where the words in the original sentence\nare randomly chosen and masked or replaced (Her-\nmann et al., 2015). Several studies signiﬁcantly im-\nproved the model performance by pre-training the\nmodel via the Cloze-style approach. By utilizing\nthe MLM approach with the self-attention mecha-\nnism and the large-scale training dataset, numerous\nmodels improve the performances of various down-\nstream tasks including NMT task (Baevski et al.,\n2019; Devlin et al., 2019; Zhang et al., 2019a; Con-\nneau and Lample, 2019; Liu et al., 2019c; Clark\net al., 2019). However, to our knowledge, few stud-\nies apply such an MLM approach to restore the\ndamaged parts.\nMotivated by these studies, we design our model\nusing masked language modeling based on the self-\nattention architecture to recover the damaged docu-\nments considering their contexts.\n2.3 Analysis on Historical Records\nVarious studies apply the machine learning ap-\nproaches to analyze the historical records (Zhao\net al., 2014; Kumar et al., 2014; Mimno, 2012;\nKim et al., 2015; Bak and Oh, 2015, 2018). In ad-\ndition, researchers adopt neural networks such as\nconvolutional neural networks and autoencoders,\nfor page segmentation and optical character recog-\nnition to convert the historical records in a digital\nform (Chen et al., 2017; Clanuwat et al., 2019).\nGiven such digital-form records, analysts attempt\nto utilize the topic modeling to discover the histori-\ncally meaningful events (Yang et al., 2011).\nEspecially, using the translated AJD, researchers\ndiscover historical events such as magnetic storm\nactivities (Yoo et al., 2015; Hayakawa et al., 2017),\nmeteors (Lee et al., 2009), and solar activities (Jeon\net al., 2018). In political science, researchers ana-\nlyze the decision patterns of a royal family in the\nJoseon Dynasty (Bak and Oh, 2015, 2018). Besides,\nthe dietary patterns and dynamic social relations\namong key ﬁgures during the Joseon Dynasty have\n4034\nMulti-Head \nAttention\nFeed\nForward\nAdd & Norm\nAdd & Norm\nShared\nEncoder\nLinear\nMulti-Head \nAttention\nFeed\nForward\nAdd & Norm\nAdd & Norm\nRestoration\nEncoder\nHanja Input\nEmbedding\nMulti-Head \nAttention\nFeed\nForward\nAdd & Norm\nMulti-Head \nAttention\nAdd & Norm\nAdd & Norm\nTranslation\nDecoder\nKorean Input\nEmbedding\nLinear\nFigure 3: Overview of the proposed model for the\nrestoration and translation tasks.\nbeen investigated (Ki et al., 2018). However, exist-\ning studies mainly rely on the documents translated\nby human experts. Therefore, we ﬁrst translate the\ndocuments in AJD and DRS. Afterward, we ap-\nply topic modeling approaches to mine meaningful\nhistorical events over large-scale data.\n3 Proposed Methods\nThis section describes a multi-task learning ap-\nproach based on the Transformer networks to ef-\nfectively restore and translate the historical records.\nThe overview of our model is shown in Fig. 3.\nAJD and DRS datasets consist of Hanja sen-\ntences H= {h1,...,h N}and Korean sentences\nK= {k1,...,k N}, where each Korean sentence is\ntranslated from its corresponding Hanja sentence.\nHere, the Hanja represents the Chinese characters\nborrowed to write the Korean language in the past.\nEspecially, DRS contains additional Hanja sen-\ntences ˜H= {hN+1,...,h M}that are not trans-\nlated yet. Hence, we have in total M Hanja sen-\ntences in the Hanja corpus such that ˆH= H∪ ˜H\nand N Korean sentences in the Korean corpus K.\nConsidering the properties of AJD and DRS, we\ndesign a multi-task learning approach with docu-\nment restoration and machine translation, based\non the Transformer networks. As shown in Fig. 3,\nour model consists of embedding and output lay-\ners for Hanja and Korean, and three Transformer\nmodules: the shared encoder, the restoration en-\ncoder, and the translation decoder. The restoration\nencoder is an encoder for the restoration task. The\ntranslation decoder is used for translating Hanja\nsentences into modern Korean sentences, and the\nshared encoder is used for both the restoration and\ntranslation tasks. By sharing the encoder module\nfor both tasks, the shared encoder is trained with\na large-scale corpus, i.e., the Hanja-Korean paired\ndataset and the additional unpaired Hanja dataset.\nThe parameter sharing technique assists the model\nto learn rich information from the Hanja corpus.\nWe apply the cross-layer parameter-sharing tech-\nnique in the same manner as used in ALBERT (Lan\net al., 2019), which shares the attention parameters\nfor each Transformer encoder and decoder modules\nto reduce the model size and the inference time.\n3.1 Restoration of Damaged Documents\nThe restoration task for damaged documents is sim-\nilar to the MLM approach, which masks randomly\nchosen tokens in the input sentence and then pre-\ndicts their original tokens in the corresponding po-\nsition. We apply the MLM technique to restore the\ndamaged documents, especially in the case of the\nHanja sentences ˆH.\nFor word indices (whi\n1 ,...,w hi\nLi) in the Hanja\nsentence hi, where Li is the length of the i-th se-\nquence, several words are randomly selected and\nreplaced by a [MASK] token. We extract word em-\nbedding vectors (ehi\n1 ,...,e hi\nLi) ∈Rdemb from the\nHanja embedding layer combined with positional\nembedding vectors, where demb represents the di-\nmension size of the embedding space. Here, we\napply the factorized embedding parameterization\ntechnique to reduce model parameters (Lan et al.,\n2019). These embedding vectors are projected onto\nthe dmodel-dimensional embedding space through\na linear layer. Subsequently, the embedding vec-\ntors are transformed into the Hanja context vec-\ntors (ˆshi\n1 ,..., ˆshi\nLi) via the shared encoder and the\nrestoration encoder as\nshi\n1 ,...,s hi\nLi = fS(ehi\n1 ,...,e hi\nLi), (1)\nˆshi\n1 ,..., ˆshi\nLi = fR(shi\n1 ,...,s hi\nLi), (2)\nwhere fS and fR functions represent the shared\nencoder and the restoration encoder, respectively.\nThe Hanja context vectors is non-linearly trans-\nformed into the output vector zhi\nk ∈Rdemb via the\noutput layer. We also apply the factorized embed-\nding parameterization technique to the output lay-\ners for parameter reduction. We calculate the prob-\nability P( ˆwhi\nk,m|whi\n1 ,...,w hi\nLi) for the index mof\nthe original token ˆwhi\nk , using the softmax function\nas\nP( ˆwhi\nk,m|whi\n1 ,...,w hi\nLi ) = exp(Wh\nm\n⊤\nzhi\nk )\n∑|Vh|\nj exp(Wh\nj\n⊤zhi\nk )\n, (3)\nwhere |Vh|is the size of the Hanja vocabulary.\n4035\n3.2 Neural Machine Translation for\nHistorical Records\nIn order to facilitate the training of our transla-\ntion module, we exploit the Hanja-Korean paired\ndataset {(hi,ki)|hi ∈ H,ki ∈ K}. As shown\nin Fig. 3, we ﬁrst extract the Hanja context vec-\ntors (shi\n1 ,...,s hi\nLi) from the word tokens in the\nHanja sentence hi, using the shared encoder in\nthe same manner as in Eq. 1. Utilizing the Hanja\ncontext vectors and previously predicted Korean\nwords (wki\n1 ,...,w ki\nt−1), we subsequently calculate\nthe dmodel-dimensional Korean context vector ski\nt\nfor the current time step tas\nski\nt = fD(shi\n1 ,...,s hi\nLi,wki\n1 ,...,w ki\nt−1), (4)\nwhere fD represents the translation decoder lay-\ners. After calculating the Korean context vector\nski\nt , we non-linearly transform the context vector to\nthe output vector zki\nt ∈Rdemb, through the output\nlayer, along with the above-mentioned factorized\nembedding parameterization for parameter reduc-\ntion. Finally, we yield the probability that the word\nVm is generated from the t-th step as\nP(wki\nt,m|hi,wki\n1:t−1) = exp(Wk\nm\n⊤\nzki\nt )∑|Vk|\nj exp(Wk\nj\n⊤zki\nt )\n, (5)\nwhere |Vk|is the size of the vocabulary for the\nKorean corpus, andWk ∈R|Vk|×demb is the output\nlayer for the Korean corpus.\nAs previously mentioned, we employ the param-\neter sharing approach for the encoder module, (i.e.,\nthe shared encoder), thus enhancing the robustness\nof our model, especially with the Hanja dataset.\n3.3 Training and Inference\nIn order to train our model, we use the cross-\nentropy loss to maximize the probability of the\noriginal token indices for the masked tokens and\nthe target sentence for the translation task as\nLrst= −1\nM\n∑\nhi∈ˆHEk∼ξ(hi)\n[logP(whi\nk |hi)], (6)\nLtrs= −1\nN\n∑N\ni=1\n[1\n|ki|\n∑|ki|\nt=1P(wki\nt |hi,wki\n1:t−1)], (7)\nwhere ξ(·) is an operator that randomly selects the\ntokens from each sentence for MLM. In this study,\nwe apply not only unigram masking but also the\nn-gram masking techniques (i.e., bigrams and tri-\ngrams), as previously applied (Zhang et al., 2019a).\nFinally, the total loss is deﬁned as\nL= Lrst + Ltrs. (8)\nPaired Hanja Unpaired Hanja Korean\n#(Train data) 239,226 1,377,320 239,226\n#(Test data) 20,000 20,000 20,000\n1st Quartile 26 27 22\nMean 143.81 165.66 123.68\n3rd Qquatile 106 113 80\nMedian 52 55 40\nV ocab size 8,742 8,742 24,000\nTable 1: Dataset summary. The third to the sixth rows\nindicate the statistics for the length of each document.\nOur model is optimized by using the rectiﬁed\nAdam (Liu et al., 2019b) with the layer-wise adap-\ntive rate scheduling technique (You et al., 2017).\nWe also apply the gradient accumulation technique\nand update our model for each loss asynchronously,\nto increase the batch size and efﬁciently manage\nthe GPU memory.\nAfter training the model, the damaged tokens are\nreplaced by the [MASK] token during the restora-\ntion stage, and the model obtains the top-K char-\nacters with the highest probabilities, among which\nusers can choose and conﬁrm a correct characters\nin the position of the damaged parts. In addition,\nwe translate all the Hanja records that are not yet\ntranslated for further in-depth analysis. When trans-\nlating the Hanja sentence, we additionally apply\nbeam search with length normalization. The trans-\nlation task for all the untranslated records using 20\nV100 GPUs had a duration of approximately ﬁve\ndays.\n4 Experiments\nThis section ﬁrst describes our datasets and experi-\nmental settings.\n4.1 Datasets and Preprocessing\nTo train our model, we collect most of the docu-\nments of AJD and DRS, including those manually\ntranslated to date, provided by the National Insti-\ntute of the Korean History 5. The records contain\napproximately 250K documents for AJD and 1.4M\ndocuments for DRS.\nAfter collecting documents, we tokenize each\nHanja sentence into the character-level tokens, sim-\nilar to previous studies (Zhang et al., 2014; Li et al.,\n2018), and also tokenize each Korean sentence\nbased on the unigram language model (Kudo, 2018)\nprovided by Google’s SentencePiece library.6 Here,\nwe included those words appearing more than ten\ntimes in the Hanja vocabulary, the size of which\n5http://www.history.go.kr/\n6https://github.com/google/sentencepiece\n4036\nis about 8.7K words. For the Korean corpus, we\nlimit the size of the Korean vocabulary to 24K. The\nout-of-vocabulary words are replaced with UNK\n(unknown) tokens. To improve the stability and efﬁ-\nciency during the training stage, we ﬁlter out those\nHanja sentences with less than four tokens or more\nthan 350 tokens and those Korean sentences with\nless than four tokens or more than 300 tokens. Note\nthat the portion of sentences ﬁltered out from each\ndataset is less than 10%.\nTo evaluate the performance of our model, we\nrandomly select 20K sentences as a test dataset\nfor each of the paired and the unpaired sets. The\nsizes of the training set for the Hanja-Korean paired\ncorpus and the unpaired Hanja corpus are 240K and\n1.38M, respectively. The statistics of the dataset are\nsummarized in Table 1.\n4.2 Hyper-parameter Settings\nWe set hyper-parameters similarly to the BERT (De-\nvlin et al., 2019) base model. We set the size of the\nembedding dimension demb, the hidden vector di-\nmension dmodel, and the dimension of the position-\nwise feed-forward layers as 256, 768, and 3,072,\nrespectively. The shared encoder, the translation\ndecoder, and the restoration encoder consist of 12,\n12, and 6 layers, respectively. We use 12 attention\nheads for each multi-head attention layer. Overall,\nthe total number of parameters is around 168.8M.\n4.3 Mining Historical Records via Topic\nModeling\nAfter obtaining machine-translated outputs of the\nremaining records, we apply topic modeling to the\nfull set of documents for exploratory analysis of\nhistorical events. To be speciﬁc, the full set of docu-\nments include all of the manually translated records\nas well as machine-translated records by our model.\nBy using each translated record ki and its written\ndate information di, we ﬁrst parse the document\ninto morphemes and then use the only noun and\nadjective tokens. Afterward, we build the term-date\nmatrix V ∈RV×D where V is the vocabulary size\nand D is the number of dates in the total set of\nhistorical documents.\nIn this study, we utilize non-negative matrix fac-\ntorization (NMF) (Lee and Seung, 2001) as a topic\nmodeling method7. We ﬁrst assume that there ex-\n7Topic modeling includes several methods such as latent\nDirichlet allocation (LDA) (Blei et al., 2003)-based and non-\nnegative matrix factorization-based models (Lee and Seung,\n2001). We additionally tested topic modeling with LDA, but\nist K topics in the corpus. The term-date matrix\nV is decomposed into the term-topic weight ma-\ntrix W ∈RV×K and the date-topic weight matrix\nH ∈RD×K as\nW,H = arg min\nW,H≥0\n∥V −WH⊤∥\n2\nF + α·ψ(W,H), (9)\nwhere ∥·∥F represents the Frobenius norm, and ψ\nand αrepresent the L1 regularization function and\nthe regularization weight, respectively. We set the\nnumber of topics K as 208 and the regularization\nweight αas 0.1.\n5 Experimental Results\nThis section describes the results of the perfor-\nmances of our model for restoration and translation,\nfollowed by qualitative examples of each task as\nwell as topic modeling results.\n5.1 Document Restoration\nWe evaluate the performance of our model on\nthe document restoration task on the test dataset.\nWe also compare performance between the model\ntrained with and without multi-task learning. Ta-\nble 3 shows the results of top-K (HITS@K). The\ntop-10 accuracy of our proposed model is almost\n89%, which indicates the high performance of our\nmodel and demonstrates that our model provides\nanalysts with appropriate options. However, the\nbaseline model, trained without multi-task learning,\nperforms slightly better than the one with multi-\ntask learning. This shows that the baseline model\nis more specialized in the document restoration\ntask. However, although our model performance is\nslightly lower than the baseline model, the beneﬁts\nof the multi-task learning approach are signiﬁcantly\nmanifested in the NMT task as shown in Table 5.\nAs our model shows the acceptable performances\non both the restoration and the translation tasks, we\nconclude that our model learns the purpose of our\nresearch well via multi-task learning. We will fur-\nther discuss the main beneﬁts of multi-task learning\nin Section 5.2.\nWe further investigate the qualitative results of\nthe document restoration task. Table 2 shows four\nrandomly sampled, example pairs. As shown in the\nﬁrst three rows of this table, the model also has the\nability to predict bi-gram and tri-gram character-\nlevel tokens because the model is trained using\nthe results of NMF are slightly better than those of LDA.\n8We set the number of topics as 20 after we conducted\nexperiments by varying the topic numbers, such as 10, 15, 20,\n30, and 50.\n4037\nOriginal 上在慶德宮. 停常參· 經筵.\nPredicted 上在慶熙宮. 停常參· 經筵.\nOriginal 右承旨李世用疏曰云云. 省疏具悉. 疏辭, 下該曹稟處.\nPredicted 右承旨李世用疏曰云云. 省疏具悉. 疏辭, 下該曹稟處.\nOriginal 玉堂子. 答曰, 省具悉. 辭當採用焉. 內下記草\nPredicted 玉堂子. 答曰, 省具悉. 辭當採用焉. 內下記草\nOriginal 又曰, 假注書金基龍, 身病猝重, 勢難察任, 今姑改差, 何如? 傳曰, 允.\nPredicted 又曰, 假注書李基淳, 身病猝重, 勢難察任, 今姑改差, 何如? 傳曰, 允.\nTable 2: Our model prediction results. Blue- and red-colored letters represent masked and predicted ones, respec-\ntively.\nHITS@1 HITS@5 HITS@10\nBaseline 77.83% 88.29% 90.89%\nFull model 75.20% 86.21% 89.09%\nTable 3: Top-K accuracies for the restoration task.\nn-gram-based MLM. Furthermore, although each\ncharacter is not exactly the same as the original one,\nthe last example in the table shows that our model\nrestores the proper format of the name part. How-\never, predicting the exact name is a difﬁcult task for\nhuman experts, even when considering the context\nof the sentence, as prior knowledge is necessary\nto predict the exact name. Therefore, we quanti-\ntatively measured the model performance on the\nproper nouns, e.g. person and location names, using\n200 samples of them. The average top-10 accuracy\nis only 8.3%, signiﬁcantly lower than the overall\naccuracy, which is larger than 89%. We conjecture\nthat the degradation is mainly due to the difﬁculty\nin maintaining the information of the proper nouns,\nwhich would require external knowledge. We leave\nit as our future work.\n5.2 Machine Translation Quality\nTo investigate the performance of the machine\ntranslation task, we translate the Hanja sentences\nin the test dataset and then evaluate the model per-\nformance. As shown in Table 5, the results for the\ntranslation task are evaluated by BLEU (Papineni\net al., 2002), METEOR (Banerjee and Lavie, 2005),\nand ROUGE-L (Lin, 2004). In this result, “Full”\nrepresents our proposed model trained by multi-\ntask learning of the translation and the restoration\ntasks. Therefore, the model is trained to take both\nthe translated and untranslated sentences. On the\nother hand, “Base” represents the model trained\nonly by the translation task, and thus, the model\nis trained to accept only the translated sentences.\nOur model outperforms the baseline model with a\nsigniﬁcant margin.\nFurthermore, we generate sentences using the\nbeam search method with the length normalization.\nIn this study, we compare the greedy search and\nthe beam search with a beam size of 3. As shown\nin Table 5, results obtained with a beam size of 3\nare slightly better than the greedy search method.\nFinally, the BLEU score of our model is obtained\nas 0.5410, which indicates that our model performs\nreasonably well, compared to other recent models\ntrained in other languages.\nWe additionally compared our model to the\nmodel trained via the pretraining-then-ﬁnetuning\napproach. As shown in Table 6, the BLEU score\nof this approach is 0.3755, which is 5.9% higher\nthan that of the model trained from scratch but\n28.7% lower than our multi-task learning approach.\nThe results can be explained for two reasons.\nFirst, as the size of unpaired data is much larger\nthan that of paired data, the multi-task learning\nfully utilizes the paired and unpaired data for the\ntranslation task, compared to the pretraining-then-\nﬁnetuning approach. Second, The pretraining-then-\nﬁnetuning approach has a catastrophic forgetting\nproblem (Chen et al., 2020). In other words, the\nﬁnetuning step can fail to maintain the knowledge\nacquired at the pretraining step. However, as both\nreconstruction and translation tasks are crucial for\nhistorical documents, such a forgetting issue is crit-\nical to our tasks.\nWe also tested the quality of the Hanja-Korean\ntranslation task using a Chinese-Korean machine\ntranslator. As few publicly available machine trans-\nlation models for Chinese-Korean exist, we used\nGoogle Translate9 instead. The translator failed\nto translate given Hanja sentences in most cases,\nmainly because Hanja and Chinese have different\nproperties in terms of grammar and word meanings.\nTo investigate the translation performance qual-\nitatively, we sampled translated samples. Table 4\n9https://translate.google.co.kr\n4038\nOriginal 上在昌慶宮. 停常參· 經筵.\nPredicted 상이 창경궁에 있었다. 상참과경연을 정지하였다.\nPredicted (Eng.) King was in the Changkyeong palace. He stopped the discussion of political affairs\nwith other ofﬁcers.\nOriginal 答大司憲南龍翼疏曰, 省疏具悉. 內局提調之任, 當勉副, 卿其勿辭, 救護母病,\n從速上來察職.\nPredicted 대사헌 남용익의 상소에 답하기를, “상소를 보고잘 알았다. 내국제조의 직임은\n부지런히 마지못해 경의 뜻을 따라주니, 경은 사직하지 말고어미를 구호하는\n데에 속히 올라와직임을 살피라.”하였다.\nPredicted (Eng.) Replying to the Prosecutor General Namyongik’s memorial, the king said, “I looked\nat the memorial and thoroughly understood what it meant. As the position of the\ndirector at the ofﬁce of the royal physicians cannot help but agree to your message,\nyou should not resign your position, care for your mother’s illness, and come back\nto be responsible for your duties quickly.”\nOriginal 夜一更, 月暈. 五更, 西方坤方, 有氣如火光.\nPredicted 밤 1경에 달무리가졌다. 5경에 서방, 곤방에 화광같은 기운이 있었다.\nPredicted (Eng.) The moon has a ring around it at 7-9 PM. At 3-5 AM, there was the light of the ﬁre\nin the west and south-west.\nTable 4: Examples of original Hanja sentences, ground-truth sentences, and predicted sentences. For readability,\nwe appended English sentences corresponding to the predicted sentences in each row.\nBLEU METEOR ROUGE-L\nBase (1) 0.3547 0.3488 0.6082\nBase (3) 0.3536 0.3482 0.6127\nFull (1) 0.5269 0.4594 0.7463\nFull (3) 0.5410 0.4719 0.7606\nTable 5: Results of the performance of the translation\ntask. “Base” and “Full” represent the model trained\nonly using the machine translation task and the model\ntrained using multi-task learning with machine transla-\ntion and restoration tasks, respectively.\nMulti-task Scratch Pipelining\nBLEU 0.5410 0.3536 0.3755\nTable 6: BLEU scores of the models trained with\nmulti-task learning, scratch, and the pretraining-then-\nﬁnetuning approach (pipelining), respectively.\nshows the sentences translated from the untrans-\nlated documents by our model. For readability, we\nappend English sentences corresponding to the pre-\ndicted sentences in each row. Each result indicates\nthat our model generates the modern sentences cor-\nresponding to contexts of the source Hanja sen-\ntences. Interestingly, the third example in the table\nis related to the astronomical observation of the\naurora. Later, we found prior studies conﬁrming\nthat the red energy mentioned in our document was\nan aurora (Zhang, 1985; Stephenson and Willis,\n2008). This highlights the importance of the ma-\nchine translation task of the historical records, as\nit is essential to survey by researchers in various\nﬁelds such as astrophysics and geology. Therefore,\nwe further analyze the documents with the topic\nmodeling approach.\n5.3 Results of Topic Modeling\nAs described in Section 4.3, we calculate the term-\ntopic weight matrix W and date-topic weight ma-\ntrix H. We select three interesting topics from\nthe total of K topics and visualize the term-topic\nweights in W using the word cloud and the date-\ntopic matrix H in a smoothed time-series graph for\neach topic. Fig. 4 shows the results.\nThe ﬁrst topic is related to troops and military ex-\nercise. As shown in the red dashed box in the time-\nseries graph, the weights dramatically decrease in\n1882, while the weights continuously increase after\nthe biggest war in 1592. In fact, a coup attempt of\nthe old-fashioned soldiers occurred in 1882, caus-\ning the national intervention of neighboring coun-\ntries and the decline of self-reliant defense. The\nﬁfteenth topic is related to war and national de-\nfense. Although this topic is related to the preced-\ning military topic, it is more related to the inter-\nnational relationship compared to the ﬁrst one. In\nthe early years of the dynasty, northern enemies\nand pirates frequently invaded Joseon, which re-\nveals as the large topical weights in the beginning.\n4039\nTopic\u00001 Topic\u000015 Topic\u000018\nNational security and \nforeign intervention\nForeign invasion Mounder\nminimumLeonids\nmeteor\nFigure 4: Three topics extracted from topic modeling. We translated topic keywords into English for readability.\nThe weights increase in the late sixteenth century,\nand the weight maintains at a high level until 1637\nwhen three great wars broke out in Joseon.\nThe eighteenth topic is related to astronomical\nobservations such as a halo and a meteor shower.\nIn the mid-sixteenth century, people observed the\nLeonids, as shown in the ﬁrst red box of the graph.\nWe later found that experts in astronomy also dis-\ncovered this in the past, using AJD (Yang et al.,\n2005). Moreover, from the mid-seventeenth cen-\ntury to the early eighteenth century, the number of\nsunspots was low. Solar observers name this event\nas the Maunder minimum (Eddy, 1976; Shindell\net al., 2001). This event caused abnormal climate\nphenomena, such as the third example in Table 4,\nas shown in the second red box of the graph. This\ntopic demonstrates the importance of the use of\nhistorical records since it is difﬁcult to easily spot\nthe phenomena that occurred centuries ago.\nNote that previous studies mainly attempted to\nexploit only AJD or translated parts of DRS. How-\never, we utilize both AJD and the majority of DRS\nrecords by applying advanced NMT techniques.\nWhen performing topic modeling by using only\nthose manually translated sentences, it failed to in-\nclude topics such as the health of the royal families\nand actions against treason sinners, which were\nrevealed by our approach. It is because the volu-\nminous documents that have not been manually\ntranslated contain their own topics. Thereby, we\nextract several valuable topics even with no special\nknowledge in the Hanja domain. Translating the\nhistorical records into modern languages expands\nour knowledge base, and analysis of the records\nusing machine translation and text mining tech-\nniques may help the analysts effectively explore\nthe historical records.\n6 Conclusions\nIn this paper, we proposed a novel approach to\ntranslate and restore the historical records of the\nJoseon dynasty by formulating the multi-task learn-\ning task based on the self-attention mechanism.\nOur approach signiﬁcantly increases the transla-\ntion quality by learning the rich contents in large\ndocuments. We anticipate these tasks are the ﬁrst\nsteps towards translating the ancient Korean his-\ntorical records into modern languages such as En-\nglish. Furthermore, the model effectively predicts\nthe original words from the damaged parts of the\ndocuments, which is an essential step for restoaring\nthe damaged documents. Results from text mining\napproaches show that our approaches have the po-\ntential in supporting analysts in effectively explor-\ning the large volume of historical documents. We\nalso expect researchers from diverse domains can\nexplore documents and discover historical ﬁndings\nsuch as astronomical phenomena and undiscovered\ninternational affairs, with no special domain knowl-\nedge. As future work, we will also leverage the\ntransfer learning approach to translate historical\ndocuments into other languages, such as English\nor French. We also plan to apply knowledge graph-\nbased machine learning approaches, e.g. knowl-\nedge graph embedding and graph neural networks,\nto discover historical events and relations.\nAcknowledgements\nThis work was supported by Institute for Infor-\nmation & communications Technology Planning\n4040\n& Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No.2020-0-00368, A Neural-\nSymbolic Model for Knowledge Acquisition and\nInference Techniques, No.2019-0-00075, Artiﬁcial\nIntelligence Graduate School Program (KAIST),\nand No.2021-0-01341, Artiﬁcial Intelligence Grad-\nuate School Program (Chung-Ang University)) and\nthe National Research Foundation of Korea (NRF)\ngrant funded by the Korean government (MSIT)\n(No.NRF-2019R1A2C4070420).\nReferences\nYannis Assael, Thea Sommerschield, and Jonathan\nPrag. 2019. Restoring ancient text using deep learn-\ning: a case study on Greek epigraphy. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 6368–6375, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5360–5369, Hong\nKong, China. Association for Computational Lin-\nguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. the Inter-\nnational Conference on Learning Representations\n(ICLR).\nJinYeong Bak and Alice Oh. 2015. Five centuries of\nmonarchy in korea: mining the text of the annals of\nthe joseon dynasty. In Proceedings of the SIGHUM\nWorkshop on Language Technology for Cultural Her-\nitage, Social Sciences, and Humanities.\nJinYeong Bak and Alice Oh. 2018. Conversational\ndecision-making model for predicting the king’s de-\ncision in the annals of the joseon dynasty. In Proc.\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the Proc. the Annual Meeting of the Association\nfor Computational Linguistics (ACL) workshop on\nintrinsic and extrinsic evaluation measures for ma-\nchine translation and/or summarization.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of ma-\nchine Learning research.\nGulcin Caner and Ismail Haritaoglu. 2010. Shape-dna:\neffective character restoration and enhancement for\narabic text documents. In International Conference\non Pattern Recognition.\nKai Chen, Mathias Seuret, Jean Hennebert, and Rolf In-\ngold. 2017. Convolutional neural networks for page\nsegmentation of historical document images. In In-\nternational Conference on Document Analysis and\nRecognition.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\nTarin Clanuwat, Alex Lamb, and Asanobu Kitamoto.\n2019. Kuronet: Pre-modern japanese kuzushiji char-\nacter recognition with deep learning. In 2019 In-\nternational Conference on Document Analysis and\nRecognition (ICDAR).\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than gener-\nators. In Proc. the International Conference on\nLearning Representations (ICLR).\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Proc. the\nAdvances in Neural Information Processing Systems\n(NIPS).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAProc. the Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nMiguel Domingo and Francisco Casacuberta Nolla.\n2018. Spelling normalization of historical docu-\nments by using a machine translation approach. In\nProceedings of the Conference of the European As-\nsociation for Machine Translation.\nJohn A Eddy. 1976. The maunder minimum. Science.\nHisashi Hayakawa, Kiyomi Iwahashi, Yusuke Ebihara,\nHarufumi Tamazawa, Kazunari Shibata, Delores J\nKnipp, Akito D Kawamura, Kentaro Hattori, Ku-\nmiko Mase, Ichiro Nakanishi, et al. 2017. Long-\nlasting extreme magnetic storm activities in 1770\nfound in historical documents. The Astrophysical\nJournal.\nKarl Moritz Hermann, Tomáš Koˇcisk`y, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Proc. the Advances in Neural\nInformation Processing Systems (NIPS).\n4041\nJunhyeok Jeon, Sung-Jun Noh, and Dong-Hee Lee.\n2018. Relationship between lightning and solar ac-\ntivity for recorded between ce 1392–1877 in ko-\nrea. Journal of Atmospheric and Solar-Terrestrial\nPhysics.\nHo Chul Ki, Eun-Kyoung Shin, Eun Jin Woo, Eu-\nnju Lee, Jong Ha Hong, and Dong Hoon Shin.\n2018. Horse-riding accidents and injuries in histor-\nical records of joseon dynasty, korea. International\njournal of paleopathology.\nHannah Kim, Jaegul Choo, Jingu Kim, Chandan K\nReddy, and Haesun Park. 2015. Simultaneous dis-\ncovery of common and discriminative topics via\njoint nonnegative matrix factorization. In Proc. the\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining (KDD).\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nNeethu S Kumar, Dinesh S Kumar, S Swathikiran, and\nAlex Pappachen James. 2014. Ancient indian docu-\nment analysis using cognitive memory network. In\nInternational Conference on Advances in Comput-\ning, Communications and Informatics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. In Proc. the Inter-\nnational Conference on Learning Representations\n(ICLR).\nDaniel D Lee and H Sebastian Seung. 2001. Algo-\nrithms for non-negative matrix factorization. In\nProc. the Advances in Neural Information Process-\ning Systems (NIPS).\nKi-Won Lee, Hong-Jin Yang, and Myeong-Gu Park.\n2009. Orbital elements of comet c/1490 y1 and the\nquadrantid shower. Monthly Notices of the Royal As-\ntronomical Society.\nYanyang Li, Tong Xiao, Yinqiao Li, Qiang Wang,\nChangming Xu, and Jingbo Zhu. 2018. A simple\nand effective approach to coverage-aware neural ma-\nchine translation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 292–\n297, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nDayiheng Liu, Kexin Yang, Qian Qu, and Jiancheng Lv.\n2019a. Ancient–modern chinese translation with a\nnew large training dataset. ACM Transactions on\nAsian and Low-Resource Language Information Pro-\ncessing.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2019b. On the variance of the adaptive learning rate\nand beyond. In Proc. the International Conference\non Learning Representations (ICLR).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv:1907.11692.\nDavid Mimno. 2012. Computational historiography:\nData mining in a century of classics journals. Jour-\nnal on Computing and Cultural Heritage.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proc. the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog.\nDrew T Shindell, Gavin A Schmidt, Michael E Mann,\nDavid Rind, and Anne Waple. 2001. Solar forcing\nof regional climate change during the maunder min-\nimum. Science.\nF Richard Stephenson and David M Willis. 2008.\n‘vapours like ﬁre light’are korean aurorae. Astron-\nomy & Geophysics.\nGongbo Tang, Fabienne Cap, Eva Pettersson, and\nJoakim Nivre. 2018. An evaluation of neural ma-\nchine translation models on historical spelling nor-\nmalization. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n1320–1331, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. the Advances in Neural Informa-\ntion Processing Systems (NIPS).\nHong-Jin Yang, Changbom Park, and Myeong-Gu Park.\n2005. Analysis of historical meteor and meteor\nshower records: Korea, china, and japan. Icarus.\nTze-I Yang, Andrew Torget, and Rada Mihalcea. 2011.\nTopic modeling on historical newspapers. In Pro-\nceedings of the 5th ACL-HLT Workshop on Lan-\nguage Technology for Cultural Heritage, Social Sci-\nences, and Humanities, pages 96–104, Portland, OR,\nUSA. Association for Computational Linguistics.\n4042\nChulsang Yoo, Minkyu Park, Hyeon Jun Kim, Juhee\nChoi, Jiye Sin, and Changhyun Jun. 2015. Classiﬁ-\ncation and evaluation of the documentary-recorded\nstorm events in the annals of the choson dynasty\n(1392–1910), korea. Journal of Hydrology.\nYang You, Igor Gitman, and Boris Ginsburg. 2017.\nLarge batch training of convolutional networks.\narXiv:1708.03888.\nMeishan Zhang, Yue Zhang, Wanxiang Che, and Ting\nLiu. 2014. Character-level chinese dependency pars-\ning. In Proc. the Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019a. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nZhiyuan Zhang, Wei Li, and Qi Su. 2019b. Automatic\ntranslating between ancient chinese and contempo-\nrary chinese with limited aligned corpora. In CCF\nInternational Conference on Natural Language Pro-\ncessing and Chinese Computing.\nZW Zhang. 1985. Korean auroral records of the period\nad 1507-1747 and the sar arcs. Journal of the British\nAstronomical Association.\nHuidong Zhao, Bin Wu, Haoyu Wang, and Chuan Shi.\n2014. Sentiment analysis based on transfer learn-\ning for chinese ancient literature. In International\nConference on Behavioral, Economic, and Socio-\nCultural Computing."
}