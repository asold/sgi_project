{
    "title": "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
    "url": "https://openalex.org/W3196679537",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2175883443",
            "name": "Michael Glass",
            "affiliations": [
                "IBM Research - Thomas J. Watson Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A2521467645",
            "name": "Gaetano Rossiello",
            "affiliations": [
                "IBM Research - Thomas J. Watson Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A2283321699",
            "name": "Md. Faisal Mahbub Chowdhury",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2080393478",
            "name": "Alfio Gliozzo",
            "affiliations": [
                "IBM Research - Thomas J. Watson Research Center"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2964046079",
        "https://openalex.org/W3091432621",
        "https://openalex.org/W3119164154",
        "https://openalex.org/W2759211898",
        "https://openalex.org/W3175475697",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W3021533447",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W1869500417",
        "https://openalex.org/W3032558086",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2251913848",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3033176962",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W3120772090",
        "https://openalex.org/W3104748221",
        "https://openalex.org/W2962881743",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3175627818",
        "https://openalex.org/W2889832871",
        "https://openalex.org/W2807441601",
        "https://openalex.org/W2593864460",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2963469388",
        "https://openalex.org/W3093871960",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2785611959",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W3034891697",
        "https://openalex.org/W4299585995",
        "https://openalex.org/W2912924812"
    ],
    "abstract": "Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to ‘fill’ the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1939–1949\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1939\nRobust Retrieval Augmented Generation for Zero-shot Slot Filling\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Alﬁo Gliozzo\nIBM Research AI\nThomas J. Watson Research Center, NY\nAbstract\nAutomatically inducing high quality knowl-\nedge graphs from a given collection of docu-\nments still remains a challenging problem in\nAI. One way to make headway for this prob-\nlem is through advancements in a related task\nknown as slot ﬁlling. In this task, given an\nentity query in form of [E NTITY , S LOT, ?],\na system is asked to ‘ﬁll’ the slot by generat-\ning or extracting the missing value exploiting\nevidence extracted from relevant passage(s) in\nthe given document collection. The recent\nworks in the ﬁeld try to solve this task in an\nend-to-end fashion using retrieval-based lan-\nguage models. In this paper, we present a\nnovel approach to zero-shot slot ﬁlling that ex-\ntends dense passage retrieval with hard neg-\natives and robust training procedures for re-\ntrieval augmented generation models. Our\nmodel reports large improvements on both T-\nREx and zsRE slot ﬁlling datasets, improving\nboth passage retrieval and slot value genera-\ntion, and ranking at the top-1 position in the\nKILT leaderboard. Moreover, we demonstrate\nthe robustness of our system showing its do-\nmain adaptation capability on a new variant of\nthe TACRED dataset for slot ﬁlling, through a\ncombination of zero/few-shot learning. We re-\nlease the source code and pre-trained models1.\n1 Introduction\nSlot ﬁlling is a sub-task of Knowledge Base Pop-\nulation (KBP), where the goal is to recognize a\npre-determined set of relations for a given entity\nand use them to populate infobox like structures.\nThis can be done by exploring the occurrences of\nthe input entity in the corpus and gathering infor-\nmation about its slot ﬁllers from the context in\nwhich it is located. A slot ﬁlling system processes\nand indexes a corpus of documents. Then, when\nprompted with an entity and a number of relations,\n1Our source code is available at: https://github.\ncom/IBM/kgi-slot-filling\nFigure 1: Slot Filling task\nit ﬁlls out an infobox for the entity. Some slot ﬁll-\ning systems provide evidence text to explain the\npredictions. Figure 1 illustrates the slot ﬁlling task.\nMany KBP systems described in the literature\ncommonly involve complex pipelines for named en-\ntity recognition, entity co-reference resolution and\nrelation extraction (Ellis et al., 2015). In particu-\nlar, the task of extracting relations between entities\nfrom text has been shown to be the weakest com-\nponent of the chain. The community proposed dif-\nferent solutions to improve relation extraction per-\nformance, such as rule-based (Angeli et al., 2015),\nsupervised (Zhang et al., 2017), or distantly su-\npervised (Glass et al., 2018). However, all these\napproaches require a considerable human effort\nin creating hand-crafted rules, annotating training\ndata, or building well-curated datasets for boot-\nstrapping relation classiﬁers.\nRecently, pre-trained language models have been\nused for slot ﬁlling (Petroni et al., 2020), opening\na new research direction that might provide an ef-\nfective solution to the aforementioned problems.\nIn particular, the KILT benchmark (Petroni et al.,\n2021), standardizes two zero-shot slot ﬁlling tasks,\nzsRE (Levy et al., 2017) and T-REx (Elsahar et al.,\n2018), providing a competitive evaluation frame-\nwork to drive advancements in slot ﬁlling. How-\never, the best performance achieved by the current\nretrieval-based models on the two slot ﬁlling tasks\nin KILT are still not satisfactory. This is mainly\n1940\ndue to the lack of retrieval performance that affects\nthe generation of the ﬁller as well.\nIn this work, we propose KGI (Knowledge\nGraph Induction), a robust system for slot ﬁll-\ning based on advanced training strategies for both\nDense Passage Retrieval (DPR) and Retrieval Aug-\nmented Generation (RAG) that shows large gains\non both T-REx (+38.24% KILT-F1) and zsRE\n(+21.25% KILT-F1) datasets if compared to previ-\nously submitted systems. We extend the training\nstrategies of DPR withhard negative mining(Simo-\nSerra et al., 2015), demonstrating its importance in\ntraining the context encoder.\nIn addition, we explore the idea of adaptingKGI\nto a new domain. The domain adaptation process\nconsists of indexing the new corpus using our pre-\ntrained DPR and substituting it in place of the orig-\ninal Wikipedia index. This enables zero-shot slot\nﬁlling on the new dataset with respect to a new\nschema, avoiding the additional effort needed to re-\nbuild NLP pipelines. We provide a few additional\nexamples for each new relation, showing that zero-\nshot performance quickly improves with a few-shot\nlearning setup. We explore this approach on a vari-\nant of the TACRED dataset (Alt et al., 2020) that\nwe speciﬁcally introduce to evaluate the zero/few-\nshot slot ﬁlling task for domain adaption.\nThe contributions of this work are as follows:\n1. We describe an end-to-end solution for slot\nﬁlling, called KGI, that improves the state-of-\nthe-art in the KILT slot ﬁlling benchmarks by\na large margin.\n2. We demonstrate the effectiveness of hard neg-\native mining for DPR when combined with\nend-to-end training for slot ﬁlling tasks.\n3. We evaluate the domain adaptation of KGI\nusing zero/few-shot slot ﬁlling, demonstrat-\ning its robustness on zero-shot TACRED, a\nbenchmark released with this paper.\n4. We publicly release the pre-trained models\nand source code of the KGI system.\nSection 2 present an overview of the state of\nthe art in slot ﬁlling. Section 3 describes our KGI\nsystem, providing details on the DPR and RAG\nmodels and describing our novel approach to hard\nnegatives. Our system is evaluated in Sections 4\nand 5 which include a detailed analysis. Section 6\nconcludes the paper and highlights some interesting\ndirection for future work.\n2 Related Work\nThe use of language models as sources of knowl-\nedge (Petroni et al., 2019; Roberts et al., 2020;\nWang et al., 2020; Petroni et al., 2020), has opened\ntasks such as zero-shot slot ﬁlling to pre-trained\ntransformers. Furthermore, the introduction of re-\ntrieval augmented language models such as RAG\n(Lewis et al., 2020b) and REALM (Guu et al.,\n2020) also permit providing textual provenance\nfor the generated slot ﬁllers.\nKILT (Petroni et al., 2021) was introduced with\na number of baseline approaches. The best per-\nforming of these is RAG (Lewis et al., 2020b).\nThe model incorporates DPR (Karpukhin et al.,\n2020) to ﬁrst gather evidence passages for the\nquery, then uses a model initialized from BART\n(Lewis et al., 2020a) to do sequence-to-sequence\ngeneration from each evidence passage concate-\nnated with the query in order to generate the answer.\nIn the baseline RAG approach only the query en-\ncoder and generation component are ﬁne-tuned on\nthe task. The passage encoder, trained on Natural\nQuestions (Kwiatkowski et al., 2019) is held ﬁxed.\nInterestingly, while it gives the best performance\nof the baselines tested on the task of producing slot\nﬁllers, its performance on the retrieval metrics is\nworse than BM25 (Petroni et al., 2021). This sug-\ngests that ﬁne-tuning the entire retrieval component\ncould be beneﬁcial. Another baseline in KILT is\nBARTLARGE ﬁne-tuned on the slot ﬁlling tasks\nbut without the usage of the retrieval model.\nIn an effort to improve the retrieval performance,\nMulti-task DPR (Maillard et al., 2021) used the\nmulti-task training of the KILT suite of benchmarks\nto train the DPR passage and query encoder. The\ntop-3 passages returned by the resulting passage\nindex were then combined into a single sequence\nwith the query and a BART model was used to\nproduce the answer. This resulted in large gains in\nretrieval performance.\nDensePhrases (Lee et al., 2021) is a different\napproach to knowledge intensive tasks with a short\nanswer. Rather than index passages which are then\nconsumed by a reader or generator component, it\nindexes the phrases in the corpus that can be poten-\ntial answers to questions, or ﬁllers for slots. Each\nphrase is represented by the pair of its start and end\ntoken vectors from the ﬁnal layer of a transformer\ninitialized from SpanBERT (Joshi et al., 2020).\nGENRE (Cao et al., 2021) addresses the retrieval\ntask in KILT slot ﬁlling by using a sequence-to-\n1941\nsequence transformer to generate the title of the\nWikipedia page where the answer can be found.\nThis method can produce excellent scores for re-\ntrieval but it does not address the problem of pro-\nducing the slot ﬁller. It is trained on BLINK (Wu\net al., 2020) and all KILT tasks jointly.\nOpen Retrieval Question Answering (ORQA)\n(Lee et al., 2019) introduced neural information\nretrieval for the related task of factoid question\nanswering. Like DPR, the retrieval is based on a bi-\nencoder BERT (Devlin et al., 2019) model. Unlike\nDPR, ORQA projects the BERT [CLS] vector to\na lower dimensional (128) space. It also uses the\ninverse clozepre-training task for retrieval, while\nDPR does not use retrieval speciﬁc pre-training.\n3 Knowledge Graph Induction\nFigure 2 shows KGI, our approach to zero-shot slot\nﬁlling, combining a DPR model and RAG model,\nboth trained for slot ﬁlling. We initialize our mod-\nels from the Natural Questions (Kwiatkowski et al.,\n2019) trained models for DPR and RAG available\nfrom Hugging Face (Wolf et al., 2020)2. We then\nemploy a two phase training procedure: ﬁrst we\ntrain the DPR model, i.e. both the query and con-\ntext encoder, using the KILT provenance ground\ntruth. Then we train the sequence-to-sequence gen-\neration and further train the query encoder using\nonly the target tail entity as the objective. It is\nimportant to note that the same query encoder com-\nponent is trained in both phases.\nQuery Encoder\nGenerator\nPassage Encoder\nDPR\nRAG\nKILT Knowledge \nSource\nhead [SEP] \nrelation\ntail\nPassages\nANN \nIndex\nFigure 2: KGI Architecture\n3.1 DPR for Slot Filling\nOur approach to DPR training for slot ﬁlling is an\nadaptation of the question answering training in the\n2https://github.com/huggingface/\ntransformers\noriginal DPR work (Karpukhin et al., 2020). We\nﬁrst index the passages using a traditional keyword\nsearch engine, Anserini3. The head entity and the\nrelation are used as a keyword query to ﬁnd the top-\nk passages by BM25. Passages with overlapping\nparagraphs to the ground truth are excluded as well\nas passages that contain a correct answer. The re-\nmaining top ranked result is used as a hard negative\nfor DPR training. This is the hard negative mining\nstrategy used by DPR (Karpukhin et al., 2020) and\nMulti-DPR (Maillard et al., 2021).\nhead1 [SEP] relation1\nhead2 [SEP] relation2\nhead3 [SEP] relation3\nPassage1\n+\nPassage1\n-\nPassage2\n+\nPassage2\n-\nPassage3\n+\nPassage3\n-\np1\n+ p1\n- p2\n+ p2\n- p3\n+ p3\n-\nq1\nq2\nq3\nsoftmax\nby row\npositive hard negative batch negatives\nPassage Encoder\nQuery Encoder\nFigure 3: DPR Training\nAfter locating a hard negative for each query,\nthe DPR training data is a set of triples: query,\npositive passage (given by the KILT ground truth\nprovenance) and the hard negative passage. Figure\n3 shows the training process for DPR. For each\nbatch of training triples, we encode the queries and\npassages independently. The passage and query\nencoders are BERT (Devlin et al., 2019) models.\nThen we ﬁnd the inner product of all queries with\nall passages. The negatives for a given query are\ntherefore the hard negative and the batch negatives,\ni.e. the positive and hard negative passages for\nother queries in the batch. After applying a softmax\nto the score vector for each query, the loss is the\nnegative log-likelihood for the positive passages.\nUsing the trained DPR passage encoder we gen-\nerate vectors for the approximately 32 million pas-\nsages in our segmentation of the KILT knowledge\nsource. Though this is a computationally expensive\nstep, it is easily parallelized. The passage-vectors\nare then indexed with an ANN (Approximate Near-\nest Neighbors) data structure, in this case HNSW\n(Hierarchical Navigable Small World)(Malkov and\nYashunin, 2018) using the open source FAISS li-\nbrary (Johnson et al., 2017)4. We use scalar quanti-\nzation down to 8 bits to reduce the memory size.\n3https://github.com/castorini/anserini\n4https://github.com/facebookresearch/\nfaiss\n1942\nThe query encoder is also trained for slot ﬁll-\ning alongside the passage encoder. We inject the\ntrained query encoder into the RAG model for Nat-\nural Questions. Due to the loose coupling between\nthe query encoder and the sequence-to-sequence\ngeneration of RAG, we can update the pre-trained\nmodel’s query encoder without disrupting the qual-\nity of the generation.\nUnlike previous work on zero-shot slot ﬁlling,\nwe are training the DPR model speciﬁcally for the\nslot ﬁlling task. In contrast, the RAG baseline\n(Petroni et al., 2021) used DPR pre-trained on Nat-\nural Questions, and Multi-DPR (Maillard et al.,\n2021) trained on all KILT tasks jointly.\n3.2 RAG for Slot Filling\nFigure 4 illustrates the architecture of RAG (Lewis\net al., 2020b). The RAG model is trained to predict\nthe ground truth tail entity from the head and rela-\ntion query. First the query is encoded to a vector\nand the top-k (we use k = 5) relevant passages\nare retrieved from the ANN index. The query is\nconcatenated to each passage and the generator pre-\ndicts a probability distribution over the possible\nnext tokens for each sequence. These predictions\nare weighted according to the score between the\nquery and passage - the inner product of the query\nvector and passage vector.\ntailhead [SEP] \nrelation\nQuery EncoderANN \nIndex\nGenerator\nMarginalize\nPassages\nFigure 4: RAG Architecture\nMarginalization then combines the weighted\nprobability distributions to give a single probabil-\nity distribution for the next token. This enables\nRAG to train the query encoder through its impact\nin generation, learning to give higher weight to\npassages that contribute to generating the correct\ntokens. Formally, the inputs to the BART model are\nsequences (sj = pj [SEP] q) that comprise a query\nq plus retrieved passage pj. The probability for\neach sequence is determined from the softmax over\nthe retrieval scores (zr) for the passages. The prob-\nability for each output token ti given the sequence\nsj is a softmax over BART’s token prediction log-\nits. Therefore the total probability for each token\nti is the log-likelihood summed over all sequences,\nweighted by each sequence’s probability.\nP(sj) =softmax(zr)j\nP(ti|sj) =softmax(BART(sj)i)ti\nP(ti) =\n∑\nj\nP(ti|sj) ·P(sj)\nBeam search is used at inference time to select\nthe overall most likely tail entity. This is the stan-\ndard beam search for natural language generation\nin deep neural networks (Sutskever et al., 2014),\nthe only difference is in the way the next-token\nprobabilities are obtained.\n3.3 Dense Negative Sampling\nAs Figure 2 shows, the DPR question encoder is\ntrained both by DPR and later by RAG. To examine\nthe inﬂuence of this additional training from RAG\non the retrieval performance, we compare retrieval\nmetrics before and after RAG ﬁne-tuning. Table\n1 shows the large gains from training with RAG\nafter DPR. Note that RAG training is using the\nweak supervision of the passage’s impact in pro-\nducing the correct answer, rather than the ground\ntruth provenance of DPR training. Since this is\nlikely a disadvantage, we explore the other key dif-\nference with DPR and RAG training: RAG uses\nnegatives drawn from the trained index rather than\nfrom BM25.\nT-REx zsRE\nR-Prec R@5 R-Prec R@5\nDPRNQ 19.50 29.80 45.49 60.77\nDPRNQ+RAG 53.04 65.54 68.13 79.19\nDPRBM25 49.02 63.34 94.55 98.17\nDPRBM25+RAG 65.02 75.52 96.89 98.01\nDPRDNS 42.62 55.09 97.53 99.30\nDPRDNS +RAG 74.34 82.89 98.60 99.70\nTable 1: Analysis of retrieval by DPR and RAG on Dev\nsets\nTo replicate this feature of RAG in DPR, we\nintroduce hard negatives mined from the learned\nindex. Using the KILT trained DPR models, we\nindex the passages. Then we gather hard negatives\nfor DPR training, with one difference: rather than\nlocating the hard negative passages by BM25, we\nﬁnd the passage by ANN search over the learned\ndense vector index. We train for an additional\ntwo epochs using these hard negatives. Table 1\nshows the performance of the different approaches\nto retrieval. DPRNQ is the DPR model pre-trained\n1943\nInstances Relations\nDataset Train Dev Test Train Dev Test\nzsRE 148K 3724 4966 84 12 24\nT-REx 2284K 5000 5000 106 104 104\nTable 2: Slot ﬁlling datasets in KILT\non Natural Questions. DPR BM25 further trains\nDPRNQ on the KILT data with BM25 hard nega-\ntives. Rows with +RAG further train the question\nencoder through RAG. The row DPRDNS (Dense\nNegative Sampling) shows the performance of re-\ntrieval immediately after DNS training. Surpris-\ningly, this results in lower performance for T-REx\nrelative to DPRBM25. However, further training\nthe DNS model with RAG results in our best per-\nformance for both T-REx and zsRE. Since RAG\ndoes not update the context encoder, DNS training\nis the only training for the context encoder when\nnegatives are drawn from the dense vector index.\nAfter training with DNS the FAISS indexing\nwith scalar quantization becomes prohibitively\nslow. We therefore remove all quantization and\nuse four shards (the index is split into four, with the\nresults of each query merged) for our experiments\nwith DNS enabled KGI.\n4 KILT Experiments\nTable 2 gives statistics on the two zero-shot slot\nﬁlling datasets in KILT. While the T-REx dataset is\nlarger by far in the number of instances, the train-\ning sets have a similar number of distinct relations.\nWe use only 500k training instances of T-REx in\nour experiments to increase the speed of experi-\nmentation.\nSince the transformers for passage encoding and\ngeneration can accept a limited sequence length,\nwe segment the documents of the KILT knowledge\nsource (2019/08/01 Wikipedia snapshot) into pas-\nsages. The ground truth provenance for the slot\nﬁlling tasks is at the granularity of paragraphs, so\nwe align our passage segmentation on paragraph\nboundaries when possible. If two or more para-\ngraphs are short enough to be combined, we com-\nbine them into a single passage and if a single\nparagraph is too long, we truncate it.\n4.1 KGI Hyperparameters\nWe have not done hyperparameter tuning, instead\nusing hyperparameters similar to the original works\nHyperparameter DPR RAG\nlearn rate 5e-5 3e-5\nbatch size 128 128\nepochs 2 1\nwarmup instances 0 10000\nlearning schedule linear triangular\nmax grad norm 1 1\nweight decay 0 0\nAdam epsilon 1e-8 1e-8\nTable 3: KGI hyperparameters\non training DPR and RAG. Table 3 shows the hy-\nperparameters used in our experiments. We train\nour models on T-REx using only the ﬁrst 500k\ninstances. For KGI1 we use the same hyperparam-\neters except that zsRE is trained for two epochs.\nIn both KGI systems we use the default of ﬁve\npassages retrieved for each query for use in RAG.\n4.2 Model Details\nNumber of parameters KGI is based on RAG\nand has the same number of parameters: 2 ×\n110M for the BERTBASE query and passage en-\ncoders and 400M for the BARTLARGE sequence-\nto-sequence generation component: 620M in total.\nComputing infrastructure Using a single\nNVIDIA V100 GPU DPR training of two epochs\ntakes approximately 24 hours for T-REx and 2\nhours for zsRE. Using a single NVIDIA P100\nGPU RAG training for 500k T-REx instances takes\ntwo days and 147k instances of zsRE takes 15\nhours. The FAISS index on the KILT knowledge\nsource requires a machine with large memory, we\nuse 256GB memory - 128GB is insufﬁcient for the\nindexes without scalar quantization.\n4.3 Slot Filling Evaluation\nAs an initial experiment we tried RAG with its de-\nfault index of Wikipedia, distributed through Hug-\nging Face. We refer to this as RAG-KKS, or RAG\nwithout the KILT Knowledge Source, as reported\nin Table 4. Since the passages returned are not\naligned to the KILT provenance ground truth, we\ndo not report retrieval metrics for this experiment.\nMotivated by the low retrieval performance re-\nported for the RAG baseline by Petroni et al. (2021),\nwe experimented with replacing the DPR retrieval\nwith simple BM25 (RAG+BM25) over the KILT\nknowledge source. We provide the raw BM25\nscores for the passages to the RAG model, to\nweight their impact in generation. We also exper-\nimented with the Natural Questions trained DPR,\n1944\nR-Prec R@5 Acc. F1\nzsRE\nRAG-KKS 38.72 46.94\nRAG+BM25 58.86 80.24 45.73 55.18\nRAG+DPRNQ 68.13 79.19 46.03 55.75\nKGI0 96.24 97.53 69.58 77.24\nKGI1 98.60 99.70 71.32 78.85\nT-REx\nRAG-KKS 63.28 67.67\nRAG+BM25 46.40 67.31 69.10 73.11\nRAG+DPRNQ 53.04 65.54 73.02 76.97\nKGI0 61.30 71.18 76.58 80.27\nKGI1 74.34 82.89 84.04 86.89\nTable 4: Dev sets performance for different retrieval\nmethods\nwith only RAG training on KILT (RAG+DPRNQ ).\nWe use the approach explained in Section 3 to\ntrain both the DPR and RAG models. KGI0 is a\nversion of our system using DPR with hard negative\nsamples from BM25. The successor system, KGI1\nincorporates DPR training using DNS.\nThe metrics we report include accuracy and F1\non the slot ﬁller, where F1 is based on the recall and\nprecision of the tokens in the answer, allowing for\npartial credit on slot ﬁllers. Our systems, except for\nRAG-KKS, also provide provenance information\nfor the top answer. R-Precision and Recall@5 mea-\nsure the quality of this provenance against the KILT\nground truth provenance. Finally, KILT-Accuracy\nand KILT-F1 are combined metrics that measure\nthe accuracy and F1 of the slot ﬁller only when the\ncorrect provenance is provided.\nTable 4 reports an evaluation on the develop-\nment set, while Table 5 reports the test set per-\nformance of the top systems on the KILT leader-\nboard. KGI0 and KGI1 are our systems, while\nDensePhrases, GENRE, Multi-DPR, RAG for\nKILT and BARTLARGE are explained brieﬂy in\nSection 2. KGI1 gains dramatically in slot ﬁlling\naccuracy over the previous best systems, with gains\nof over 14 percentage points in zsRE and even more\nin T-REx. The combined metrics of KILT-AC and\nKILT-F1 show even larger gains, suggesting that\nthe KGI1 approach is effective at providing justify-\ning evidence when generating the correct answer.\nWe achieve gains of 21 to 41 percentage points in\nKILT-AC.\nRelative to Multi-DPR, we see the beneﬁt of\nweighting passage importance by retrieval score\nand marginalizing over multiple generations, com-\npared to the strategy of concatenating the top\nthree passages and running a single sequence-to-\nsequence generation. GENRE is still best in re-\ntrieval for T-REx, suggesting that at least for a\ncorpus such as Wikipedia, generating the title of\nthe page can be very effective. A possible explana-\ntion for this behaviour is that most relations for a\nWikipedia entity are mentioned in its correspond-\ning page.\n4.4 Analysis\nTo explore the effect of retrieval on downstream per-\nformance we consider two variants of our systems:\none using random passages from the index, forcing\nthe system to depend on implicit knowledge, and\nthe another using passages from the ground truth\nprovenance, to measure the upper bound perfor-\nmance for the ideal retrieval system. Evaluation is\nreported in Table 6 for 3 systems. By supplying\nthese systems with the gold standard passages, we\ncan see both the improvement possible through bet-\nter retrieval, and the value of good retrieval during\ntraining. The best system, KGI1 is the most effec-\ntive at generating slot ﬁllers from relevant explicit\nknowledge because it was trained on more cases\nof justifying explicit knowledge. However, given\nrandom passages it is the worst. It has sacriﬁced\nsome implicit knowledge for better capabilities in\nusing explicit knowledge.\nAs shown in Table 5, BART LARGE, which is\nthe best implicit-knowledge baseline system for\nKILT slot ﬁlling, is approximately 40 points lower\nin in accuracy on T-REx if compared toKGI1. To\nunderstand the impact of the explicit knowledge\nprovided by DPR, we examine the improvement\nof KGI over BARTLARGE. We consider two main\nhypotheses: 1) the value of explicit knowledge\ndepends on the relation, and 2) the value of explicit\nknowledge depends on the corpus frequency of the\nentities related.\nTo evaluate hypothesis 1, we consider the most\nfrequent 20 relations in the T-REx Dev set, each\noccurring at least 40 times. The relations with the\nlowest relative performance gain are taxonomy and\npartonomy relations: TAXON -RANK , SUBCLASS -\nOF, INSTANCE -OF, PART-OF and PARENT -TAXON\nas well as LANGUAGES -SPOKEN ,-WRITTEN -OR-\nSIGNED and SPORT . This suggest that essential\nproperties of entities are well encoded in the lan-\nguage model itself. Inspecting the LANGUAGES -\nSPOKEN ,-WRITTEN -OR-SIGNED we ﬁnd that sur-\n1945\nR-Prec Recall@5 Accuracy F1 KILT-AC KILT-F1\nzsRE\nKGI1 98.49 99.23 72.55 77.05 72.31 76.69\nKGI0 94.18 95.19 68.97 74.47 68.32 73.45\nDensePhrases 57.43 60.47 47.42 54.75 41.34 46.79\nGENRE 95.81 97.83 0.02 2.10 0.00 1.85\nMulti-DPR 80.91 93.05 57.95 63.75 50.64 55.44\nRAG (KILT organizers) 53.73 59.52 44.74 49.95 36.83 39.91\nBARTLARGE N/A N/A 9.14 12.21 N/A N/A\nT-REx\nKGI1 74.36 83.14 84.36 87.24 69.14 70.58\nKGI0 59.70 70.38 77.90 81.31 55.54 56.79\nDensePhrases 37.62 40.07 53.90 61.74 27.84 32.34\nGENRE 79.42 85.33 0.10 7.67 0.04 6.66\nMulti-DPR 69.46 83.88 0.00 0.00 0.00 0.00\nRAG (KILT organizers) 28.68 33.04 59.20 62.96 23.12 23.94\nBARTLARGE N/A N/A 45.06 49.24 N/A N/A\nTable 5: KILT leaderboard top systems performance on slot ﬁlling tasks\nPassages RAGNQ KGI0 KGI1\nRetrieved 70.58 76.58 84.04\nGold 88.66 89.46 90.20\nRandom 38.84 39.26 36.64\nTable 6: T-REx Accuracy with Random and Gold Re-\ntrieval\nface level information (i.e. French name vs. Rus-\nsian name) is often sufﬁcient for the correct predic-\ntion.\nIn contrast, the relations that gain the most from\nexplicit knowledge are: PERFORMER , MEMBER -\nOF-SPORTS -TEAM , AUTHOR , PLACE -OF-BIRTH ,\nCOUNTRY -OF-ORIGIN , CAST -MEMBER , DIREC -\nTOR . These relations are not central to the meaning\nof the head entity, like the taxonomy and parton-\nomy relations, and are not typically predictable\nfrom surface-level features.\nRegarding our second hypothesis, we might ex-\npect that more frequent entities have better repre-\nsentations in the parameters of a pre-trained lan-\nguage model, and that therefore the gain in perfor-\nmance due to use of explicit knowledge will show\na strong dependence on the corpus frequency of the\nhead or tail entity.\nTo test it, we group the Dev instances in T-Rex\naccording to the decile of the head or tail entity\nfrequency. We compute a macro-accuracy, weight-\ning all relations equally. Figure 5 shows the macro-\naccuracy of BARTLARGE and KGI1 for each decile\nof head and tail entity frequency. Although there\nis a general trend of higher accuracy for more fre-\nquent tail entities and lower accuracy for more fre-\nquent head entities, there is no pattern to the gain\nof explicit knowledge over implicit knowledge from\nentity frequency. There is a similar picture when\nconsidering the decile of the minimum of the head\nor tail entity frequency. This falsiﬁes our second\nhypothesis and suggests implicit knowledge is dis-\ntinct in kind from explicit knowledge, rather than\nmerely under-trained for low frequency entities.\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 2 3 4 5 6 7 8 9 10\nMacro Accuracy\nFrequency Decile\nBART by head KGI by head\nBART by tail KGI by tail\nFigure 5: Performance as a function of entity frequency\n5 Domain Adaptation Experiments\nIn this section, we evaluate the domain adaptation\ncapability of KGI. For this purpose, we re-organize\na dataset speciﬁcally designed to evaluate standard\nsupervised relation extraction models, such as TA-\nCRED, with the aim to create a zero-shot (and few-\nshot) slot ﬁlling benchmark where the documents\nare written with a different style than Wikipedia,\nand the relations in the KG are different from those\n1946\nin Wikidata. In order to perform an in-depth com-\nparison and analysis, we also propose a new set\nof ranking baselines and use metrics which are\nsuitable to better evaluate the slot ﬁlling task in a\nzero-shot setup.\n5.1 Zero-shot TACRED\nThe TACRED dataset was originally proposed\nby Zhang et al. (2017) with the goal to provide\na high-quality training set to supervise a relation\nextraction model which is shown to be competitive\non TAC-KBP 2015 (Ellis et al., 2015). The target\nKG schema consists of two infoboxes modeling the\nperson and organization entity types, with 41 rela-\ntion types in total. For our experiments, we adopt a\nrevisited version of TACRED (Alt et al., 2020), in\nwhich a second stage crowdsourcing is performed\nto further improve the quality of the annotations\nand resolve conﬂicts among relations.\nIn a typical supervised relation extraction setup,\na model is trained to predict (i.e. classify) the right\nrelation type given a textual passage and two en-\ntity mentions as inputs. In this paper we used the\nTACRED dataset as a slot ﬁlling benchmark, us-\ning the following procedure: 1) we ﬁrst create the\ncorpus by merging all the plain textual passages\nfrom the instances in the train, dev and test sets;\n2) we collect the annotated triples, i.e. subject-\nrelation-object, from the test data to come up with\na ground-truth KG to be used for slot ﬁlling evalua-\ntion5; 3) we remove all the triples from the original\ntest set where the subjects are pronouns. The result-\ning KG consists of 2673 slot ﬁlling test instances.\nSimilarly, we acquire a KG from the train/dev sets\nto further ﬁne-tune the KGI system as described in\nthe next section. To enable zero-shot experiments,\nwe also convert each relation label into a relation\nphrase by removing the namespaces per: and org:,\nand replacing the ‘_’ character with a space. Fi-\nnally, for each pre-annotated entity in the corpus,\nwe pre-compute an inverted index consisting of a\nlist of co-occurring entities in the textual passages.\nWe use this inverted index to compare our model\nwith a set of ranking baselines.\nAn example of the obtained ground truth is illus-\ntrated in Table 7: given the query[Dominick Dunne,\nemployee of, ?], a slot ﬁlling system is supposed\nto identify the missing slot with Vanity Fair, i.e.\nthe gold standard object in the KG, by retrieving it\n579.5% of the overall instances are labeled as no relation.\nWe exclude these instances from the ground truth KG, but we\nretain them in the textual corpus.\nfrom the collection of passages.\n5.2 Slot Filling Evaluation\nTask Given a slot ﬁlling query (e, s,?) and a list\nof possible slot values [v1, ..., vn], where e is the\nentity as subject, s is the slot/relation and vi are the\nobject candidates that co-occur withe in the corpus,\nwe can frame the zero-shot slot ﬁlling as a ranking\nproblem: argmaxi scoreM (e, s, vi). scoreM is a\nfunction that takes as input a triple and provide\na score based on the model M. Turning the slot\nﬁlling into a ranking problem has two advantages:\n1) we can compare the generative approach with\na new set of baselines, and 2) we can limit the\ngeneration of the slot values to a pre-deﬁned set of\ndomain speciﬁc entities.\nModels In order to adapt KGI1, as pre-trained\non T-REx, to the TACRED corpus, we indexed the\ntextual passages using DPR, as described in Sec-\ntion 3. Then we replaced the original Wikipedia\nindex with this new index. During the inference\nstep, we restrict the generation of the slot values\nusing the list of object candidates, i.e. the entities\nwhich co-occur with the subject from the inverted\nindex, to facilitate comparability to a set of rank-\ning baselines. To this aim, we adopt the technique\ndescribed by Cao et al. (2021) to restrict the vocab-\nulary of tokens during the generation.\nWe use three baselines to compare with our ap-\nproach for this zero-shot slot ﬁlling task. PMI is\nimplemented using the pointwise mutual informa-\ntion between e and vi based on their co-occurrence\nin the corpus. Also, we train aWord2Vec(Mikolov\net al., 2013) skip-gram model on the textual corpus,\nand we use it to implement the scoring function\nas cosine(e + s, vi), for each candidate ﬁller vi.\nIt is based on the assumption that a relation s be-\ntween two (multi)word embeddings e and v can be\nrepresented as an offset vector (v −e) =s ⇐⇒\n(e+s) =v (Rossiello et al., 2019; Vylomova et al.,\n2016). Finally, GPT-2 computes the perplexity of\nthe fragment of text by concatenating the tokens in\ne, s and each vi (Radford et al., 2019).\nMetrics Due to the similarity of slot ﬁlling with\nthe knowledge base completion task, we use Mean\nReciprocal Rank (MRR) and HIT@k, with k = [1,\n5, 10], as evaluation metrics (Bordes et al., 2013).\nNote that HIT@1 has the same meaning of the\naccuracy for the downstream task on KILT.\n1947\nSubject Relation/Slot Object Passage\nDominick Dunne per:employee_of Vanity Fair Dominick Dunne, the author, television personality and Vanity\nFair reporter who covered the trials of socialite Claus von Bulow.\nDominick Dunne per:age 83 Dominick Dunne, 83, a crime story author, in New York City.\nDominick Dunne per:siblings John Gregory Dunne Dominick Dunne, author of crime stories dies Born in 1925 in\nHartford, Connecticut, was part of a famous family that included\nhis brother, novelist and screenwriter John Gregory Dunne.\nALICO org:country_of_headquarters US AIA says IPO raised 205 billion US dollars AIG said Monday it\nhad also raised 162 billion dollars by selling unit American Life\nInsurance Company (ALICO) to MetLife Inc.\nALICO org:top_members Christopher J. Swift Alico’s chief ﬁnancial ofﬁcer, Christopher J. Swift, added that\nthe bonds were issued by companies in many commercial sectors,\nwhich diversiﬁed the portfolio.\nALICO org:parents AIG AIG said it had transferred ownership to the Federal Reserve\nBank of parts of two subsidiaries, ALICO which is active in life\nassurance in the United States and AIA which provides life assur-\nance abroad.\nTable 7: Examples of annotations from TACRED dataset for bothperson and organization infoboxes.\nMRR HIT@1 HIT@5 HIT@10\nPMI 20.20 10.89 26.49 37.30\nWord2Vec 25.24 13.83 34.92 47.60\nGPT-2 17.62 8.37 23.72 35.34\nKGI1 0-shot 43.98 28.51 64.31 76.06\nKGI1 1-shot 48.86 33.89 66.63 78.75\nKGI1 4-shot 53.28 38.8 70.45 79.35\nTable 8: Zero/few-shots results on TACRED\nHyperparameter RAG\nlearn rate 3e-6\nbatch size 1\nepochs 3\nwarmup instances 0\nlearning schedule linear\nmax grad norm 1\nweight decay 0\nAdam epsilon 1e-8\nTable 9: KGI1 hyperparameters for TACRED few-shot\nResults Table 8 reports the results of our evalu-\nation. KGI1 achieves substantially better perfor-\nmance than the aforementioned zero-shot base-\nlines on all evaluation metrics. However, HIT@1\nis ∼28% which is signiﬁcantly lower compared\nwith the numbers reported on the datasets in KILT.\nThis begs the question, how to further improve\nthe transfer learning capabilities of these genera-\ntive models? Interestingly, HIT@5/10 are high (i.e.\n∼64%/76%). This indicates our approach would\nbe useful in a human-in-the-loop scenario by pro-\nviding valuable candidates for the ﬁllers that can\nbe further validated.\nFor this purpose, we also conduct few-shot ex-\nperiments to understand the robustness of KGI1 by\nﬁne-tuning it with very limited amounts of train-\ning examples. We randomly pick n example(s) for\neach relation type from the TACRED training set,\nwith n = [1, 4]. Table 9 gives our hyperparameters\nfor the TACRED few-shot experiments. We show\nthat our system beneﬁts from additional domain\nspeciﬁc training data selected from TACRED. Just\nusing one example and four examples per relation,\nHIT@1 improves ∼5 and ∼10 percentage points\nrespectively.\n6 Conclusion\nIn this paper, we presented KGI, a novel approach\nto zero-shot slot ﬁlling. KGI improves Dense Pas-\nsage Retrieval using hard negatives from the dense\nindex, and implements a robust training procedures\nfor Retrieval Augmented Generation. We evaluated\nKGI on both T-REx and zsRE slot ﬁlling datasets,\nranking at top-1 position in the KILT leaderboard\nwith a net improvement of +38.24 and +21.25 per-\ncentage points in KILT-F1, respectively. More-\nover, we proposed and release a new benchmark\nfor zero/few-shot slot ﬁlling based on TACRED\nto evaluate domain adaptation where our system\nobtained much better zero-shot results compared\nwith the baselines. In addition, we have observed\nsigniﬁcant improvement in results for KGI when\nrapidly ﬁne-tuned in a few-shot setting. This work\nopens promising future research directions for slot\nﬁlling and other related tasks. We plan to apply\nDPR with dense negative sampling to other tasks in\nthe KILT benchmark, including dialogue and ques-\ntion answering. Likewise, an in-depth investigation\non more effective strategies for domain adaptation,\nsuch as the combination of zero-shot and few-shot\nlearning involving human-in-the-loop techniques,\nwould be another interesting direction to explore.\n1948\nReferences\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\nHennig. 2020. TACRED revisited: A thorough eval-\nuation of the TACRED relation extraction task. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1558–\n1569, Online. Association for Computational Lin-\nguistics.\nGabor Angeli, Melvin Jose Johnson Premkumar, and\nChristopher D. Manning. 2015. Leveraging linguis-\ntic structure for open domain information extraction.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n344–354, Beijing, China. Association for Computa-\ntional Linguistics.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In NIPS, pages 2787–2795.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn International Conference on Learning Represen-\ntations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJoe Ellis, Jeremy Getman, Dana Fore, Neil Kuster,\nZhiyi Song, Ann Bies, and Stephanie M. Strassel.\n2015. Overview of linguistic resources for the TAC\nKBP 2015 evaluations: Methodologies and results.\nIn TAC. NIST.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nMichael R. Glass, Alﬁo Gliozzo, Oktie Hassan-\nzadeh, Nandana Mihindukulasooriya, and Gaetano\nRossiello. 2018. Inducing implicit relations from\ntext using distantly supervised deep nets. In Interna-\ntional Semantic Web Conference (1), volume 11136\nof Lecture Notes in Computer Science, pages 38–55.\nSpringer.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics, 7:452–466.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021. Learning dense representations of\nphrases at scale. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6634–6647, Online. Association for\nComputational Linguistics.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6086–6096, Florence,\nItaly. Association for Computational Linguistics.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 333–342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\n1949\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nJean Maillard, Vladimir Karpukhin, Fabio Petroni,\nWen-tau Yih, Barlas O ˘guz, Veselin Stoyanov,\nand Gargi Ghosh. 2021. Multi-task retrieval\nfor knowledge-intensive tasks. arXiv preprint\narXiv:2101.00117.\nYu A Malkov and Dmitry A Yashunin. 2018. Efﬁcient\nand robust approximate nearest neighbor search us-\ning hierarchical navigable small world graphs.IEEE\ntransactions on pattern analysis and machine intelli-\ngence, 42(4):824–836.\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In NIPS, pages 3111–3119.\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus,\nTim Rocktäschel, Yuxiang Wu, Alexander H. Miller,\nand Sebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In AKBC.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rocktäschel, and\nSebastian Riedel. 2021. KILT: a benchmark for\nknowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2523–2544,\nOnline. Association for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nGaetano Rossiello, Alﬁo Gliozzo, Robert Farrell, Nico-\nlas Fauceglia, and Michael Glass. 2019. Learning re-\nlational representations by analogy using hierarchi-\ncal Siamese networks. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 3235–3245, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nEdgar Simo-Serra, Eduard Trulls, Luis Ferraz, Ia-\nsonas Kokkinos, Pascal Fua, and Francesc Moreno-\nNoguer. 2015. Discriminative learning of deep con-\nvolutional feature point descriptors. In Proceedings\nof the IEEE International Conference on Computer\nVision, pages 118–126.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nEkaterina Vylomova, Laura Rimell, Trevor Cohn, and\nTimothy Baldwin. 2016. Take and took, gaggle and\ngoose, book and read: Evaluating the utility of vec-\ntor differences for lexical relation learning. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1671–1682, Berlin, Germany. Asso-\nciation for Computational Linguistics.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs.\nCoRR, abs/2010.11967.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2020. Scalable zero-\nshot entity linking with dense entity retrieval. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6397–6407, Online. Association for Computa-\ntional Linguistics.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-\ngeli, and Christopher D. Manning. 2017. Position-\naware attention and supervised data improve slot\nﬁlling. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 35–45, Copenhagen, Denmark. Association\nfor Computational Linguistics."
}