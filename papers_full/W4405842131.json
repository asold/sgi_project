{
  "title": "Parameter-efficient fine-tuning of large language models using semantic knowledge tuning",
  "url": "https://openalex.org/W4405842131",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3011560027",
      "name": "Nusrat Jahan Prottasha",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2136876872",
      "name": "Asif Mahmud",
      "affiliations": [
        "Noakhali Science and Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A3187711127",
      "name": "Md. Shohanur Islam Sobuj",
      "affiliations": [
        "Hajee Mohammad Danesh Science and Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2629533315",
      "name": "Prakash Bhat",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2955646283",
      "name": "Md. Kowsher",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2092605288",
      "name": "Niloofar Yousefi",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A1864334393",
      "name": "Ozlem Ozmen Garibay",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A3011560027",
      "name": "Nusrat Jahan Prottasha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136876872",
      "name": "Asif Mahmud",
      "affiliations": [
        "Noakhali Science and Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A3187711127",
      "name": "Md. Shohanur Islam Sobuj",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2629533315",
      "name": "Prakash Bhat",
      "affiliations": [
        "University of Central Florida",
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2955646283",
      "name": "Md. Kowsher",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2092605288",
      "name": "Niloofar Yousefi",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A1864334393",
      "name": "Ozlem Ozmen Garibay",
      "affiliations": [
        "University of Central Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600466347",
    "https://openalex.org/W6811229060",
    "https://openalex.org/W4387034760",
    "https://openalex.org/W4387034804",
    "https://openalex.org/W6600669965",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W6602737616",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W6600103761",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4386804446",
    "https://openalex.org/W4394686628",
    "https://openalex.org/W4396686523",
    "https://openalex.org/W4396709273",
    "https://openalex.org/W6600407735",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4386187806",
    "https://openalex.org/W6792279967",
    "https://openalex.org/W6600283593",
    "https://openalex.org/W4385570529",
    "https://openalex.org/W4385571379",
    "https://openalex.org/W6600595061",
    "https://openalex.org/W4385572804",
    "https://openalex.org/W4385573784",
    "https://openalex.org/W4280636077",
    "https://openalex.org/W6602670149",
    "https://openalex.org/W4385570772",
    "https://openalex.org/W4401042872",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W4402669923",
    "https://openalex.org/W6602530451",
    "https://openalex.org/W6600529467",
    "https://openalex.org/W6610542590",
    "https://openalex.org/W3173419192",
    "https://openalex.org/W4319302538",
    "https://openalex.org/W3032945613",
    "https://openalex.org/W6600002382",
    "https://openalex.org/W6600686112",
    "https://openalex.org/W6609410732",
    "https://openalex.org/W6637031373",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2891575196",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W6603094881",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W6812490683",
    "https://openalex.org/W6756688054",
    "https://openalex.org/W6600175266",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W6603222412"
  ],
  "abstract": "Abstract Large Language Models (LLMs) are gaining significant popularity in recent years for specialized tasks using prompts due to their low computational cost. Standard methods like prefix tuning utilize special, modifiable tokens that lack semantic meaning and require extensive training for best performance, often falling short. In this context, we propose a novel method called Semantic Knowledge Tuning (SK-Tuning) for prompt and prefix tuning that employs meaningful words instead of random tokens. This method involves using a fixed LLM to understand and process the semantic content of the prompt through zero-shot capabilities. Following this, it integrates the processed prompt with the input text to improve the model’s performance on particular tasks. Our experimental results show that SK-Tuning exhibits faster training times, fewer parameters, and superior performance on tasks such as text classification and understanding compared to other tuning methods. This approach offers a promising method for optimizing the efficiency and effectiveness of LLMs in processing language tasks.",
  "full_text": "Parameter-efficient fine-tuning \nof large language models using \nsemantic knowledge tuning\nNusrat Jahan Prottasha1, Asif Mahmud2,5, Md. Shohanur Islam Sobuj3,5, Prakash Bhat4, \nMd Kowsher1, Niloofar Yousefi1 & Ozlem Ozmen Garibay1\nLarge Language Models (LLMs) are gaining significant popularity in recent years for specialized tasks \nusing prompts due to their low computational cost. Standard methods like prefix tuning utilize special, \nmodifiable tokens that lack semantic meaning and require extensive training for best performance, \noften falling short. In this context, we propose a novel method called Semantic Knowledge Tuning \n(SK-Tuning) for prompt and prefix tuning that employs meaningful words instead of random tokens. \nThis method involves using a fixed LLM to understand and process the semantic content of the prompt \nthrough zero-shot capabilities. Following this, it integrates the processed prompt with the input text \nto improve the model’s performance on particular tasks. Our experimental results show that SK-Tuning \nexhibits faster training times, fewer parameters, and superior performance on tasks such as text \nclassification and understanding compared to other tuning methods. This approach offers a promising \nmethod for optimizing the efficiency and effectiveness of LLMs in processing language tasks.\nThe domain of NLP has seen a remarkable transformation in recent years, primarily driven by the introduction \nof LLMs1. Transformer-based Language Models (TLMs) initially led about a revolution by showing outstanding \ncapabilities in capturing extensive dependencies 2. However, the challenges connected with adapting TLMs to \nvarious tasks, combined with their resource-intensive training, resulted to the development of more powerful \nmodels, such as GPT-33. With billions of parameters, these LLMs have not only boosted performance benchmarks \nacross various tasks but have also extended their applications into novel domains, including creative writing and \nmultimodal learning4.\nDespite their notable achievements, a in-depth analysis of LLMs reveals several major limitations. The \nextensive computational resources required for their training raise questions about environmental sustainability \nand restrict accessibility to research facilities with sufficient equipment and resources5.\nTo address this issue, there has been an increasing focus on the latest innovations in parameter-efficient fine-\ntuning methods6.As compared to retraining the entire model from scratch, fine-tuning LLMs has proven to be a \nmore rapid and efficient approach. Nevertheless, the fine-tuning of all parameters of LLMs remains a challenge \ndue to their vast size, which typically consists of billions of parameters. Despite this, the fine-tuning process still \nrequires extensive computational resources, much like the pretraining technique.\nAdapting to this challenge, adapter training has gained importance as a more efficient approach 7. This \napproach involves introducing domain-specific parameters, referred to as adapters, into pretrained models. \nThese adapters, which are made of small neural networks, are strategically inserted within or between the layers \nof the pretrained model. During the training process, only the parameters of these added adapters are updated, \nwhile the parameters of the pretrained model remain unchanged7.\nWhile adapters provide a more simple approach, they may not fully capture complex data patterns as \neffectively as fine-tuning the entire model. In addition, determining the optimal locations to insert adapters \nwithin the LLM can be challenging and may require experimentation. Nonetheless, prompt tuning complements \nadapter training by offering additional contextual information to guide the model’s understanding of the task \nat hand.\nPrompt tuning8 does not modify the underlying structure of the model, potentially resulting in quicker \ninference times and decreased resource consumption in contrast to utilizing adapters. Furthermore, prefix \ntuning9 has been proposed as a method to improve performance. Unlike prompt tuning, which updates only a \n1University of Central Florida, Orlando, FL 32816, USA. 2Noakhali Science & Technology University, Noakhali 3814, \nBangladesh. 3Hajee Mohammad Danesh Science & Technology University, Dinajpur 5200, Bangladesh. 4Amazon, \nNew Jersey, USA. 5These authors contributed equally:  Asif Mahmud and Md. Shohanur Islam Sobuj. email: \njahannusratprotta@gmail.com\nOPEN\nScientific Reports |        (2024) 14:30667 1| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports\n\nportion of a single layer, prefix tuning updates a section of every layer consistently, and it has shown improved \nperformance in succeeding tasks.\nThe use of prompts and prefix tuning techniques 8,9 can pose challenges in terms of the effectiveness and \ninterpretability of the employed prompts or prefixes. These methods generally utilize trainable virtual tokens \nwithin an adapter, which may not have essential semantic significance and require extensive training to acquire \ndomain-specific knowledge efficiently. Consequently, the performance of these techniques may not be optimal, \nparticularly when dealing with complex tasks, and extensive training is necessary to achieve optimal performance.\nTo overcome these challenges, we propose SK-Tuning, a novel approach that focuses on improving the \nperformance of fine-tuning LLMs for prompt and prefix tuning. Unlike standard techniques that depend on \nrandom virtual tokens, SK-Tuning utilizes genuine, semantically rich prompts or prefixes for adapter training. \nBy employing the LLM’s innate capacity to understand linguistic semantics, SK-Tuning strives to improve \nperformance by integrating semantic knowledge directly from prompts or prefixes.\nLLMs display remarkable zero-shot capabilities, allowing them to perform tasks without explicit training, as \nshown in recent studies 10. To maximize the potential of these capabilities, SK-Tuning utilizes LLM’s ability to \nunderstand prompts or instructions in a zero-shot manner. This approach speeds up the convergence process \nduring fine-tuning because we concentrate only on refining the semantic representation of the prompt or prefix.\nThe SK-Tuning method is presented in Fig. 1, which displays the stages for prompt and prefix tuning. At first, \nthe entire LLM is frozen to maintain its pretrained knowledge. Next, the frozen LLM is utilized to extract the \nsemantic representation from the prompt or prefix text. This representation is then educated with a small adapter \nto improve its task-specific intelligence. Lastly, the revised representation is combined with the embedding of the \ninput text, guaranteeing that the model effectively integrates both the semantic context provided by the prompt \nor prefix and the textual data of the input.\nWe perform wide-ranging experimental evaluations across a variety of downstream tasks, including sequence \nclassification, token classification, and NLI, to practically show the efficiency and excellence of SK-Tuning \ncompared to traditional fine-tuning methods. Furthermore, we compare SK-Tuning with other parameter-\nefficient approaches, such as prompt tuning, prefix tuning, p-tuning, and LoRA, highlighting its unique \nadvantages and contributions to the field of NLP .\nThe major contributions of this paper are summarized as follows:\n• This paper introduces SK-Tuning, a novel approach for fine-tuning LLMs using real, semantically meaningful \nprompt or prefix text.\nFig. 1. SK-Tuning approaches for Prefix (left) and Prompt (right). The dashed line represents the optimization \npath during the backward pass to the trainable adapter. Notably, in the context of prompt-tuning (on the \nright), the no sign signifies the discontinuation of the forward pass beyond a certain point. This is because we \nexclusively initialize layer-specific semantic information for the prompt, rendering the continuation of the \nforward pass unnecessary for the remaining layers.\n \nScientific Reports |        (2024) 14:30667 2| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\n• SK-Tuning improves training efficiency and convergence speed by utilizing the inherent semantic under -\nstanding of prompt or prefix text using LLM’s zero-shot capabilities, as a result allowing rapid adaptation to \nnew tasks.\n• In numerous experiments covering a variety of tasks, including sequence classification, token classification, \nand NLI, SK-Tuning has continually exhibited significant improvements in performance metrics..\n• The study includes a comprehensive evaluation against other parameter-efficient methods like prompt tuning, \nprefix tuning, p-tuning, and LoRA, highlighting SK-Tuning’s superior effectiveness in terms of performance \noutcomes and computational efficiency.\n• SK-Tuning reduces computational requirements and the number of trainable parameters compared to tradi-\ntional fine-tuning approaches, making it a more resource-efficient solution for adapting LLMs.The structure \nof this paper is as follows: section “ Related work” reviews related work, situating our approach within the \nbroader domain of parameter-efficient fine-tuning methods. Section “ Background study” provides a back -\nground study on existing tuning techniques, setting the stage for our proposed method. In section “SK-tuning \nprocedure” , we detail the SK-Tuning procedure, explaining its methodology and implementation. Section “Ex-\nperiments” presents our experiments, showcasing the performance improvements achieved through SK-Tun-\ning across various tasks. Section “Ablation study” offers an ablation study to further analyze the contributions \nof each component within SK-Tuning, reinforcing the paper’s key contributions to NLP . Section “Discussion” \nprovides a discussion on the implications and potential applications of SK-Tuning in practical settings. Sec -\ntion “Limitations” discusses the limitations and challenges experienced during the development and appli -\ncation of SK-Tuning. Finally, section  “ Conclusion” concludes the paper by summarizing the findings and \nhighlighting future research directions in fine-tuning methods for LLMs.\nRelated work\nThe importance of parameter-efficient fine-tuning (PEFT) methods in the field of NLP is immense, considering \nthe growing complexity of LLMs. These methods not only improve model performance but also significantly \nreduce computational and memory requirements, as demonstrated by recent academic research 11–15. The \neffectiveness of PEFT techniques is being thoroughly evaluated on a range of NLP tasks, as shown in 16. \nMoreover, an extensive body of research 17–23 consistently indicates that PEFT strategies considerably enhance \nthe performance of LLMs, even under limited-resource circumstances.\nPrompt tuning  is a novel approach that improves NLP and generation tasks by fine-tuning learnable \nparameters within the model 8. This technique enhances the model’s performance on specific roles by fine-\ntuning prompts, thereby optimizing its output. Improvements in prompt tuning have been achieved through \nthe implementation of the residual connections to strengthen performance and stability 24. This technique has \nalso been broadened to support continual learning environments, as illustrated in recent research 25,26. Current \nresearch focuses on dynamic prompt tuning, which adapts prompts in real time based on evolving contexts, as \nwell as hierarchical prompt tuning, which provides multilevel control over the model’s responses27,28.\nPrefix tuning is another powerful technique that adds learnable parameters as prefixes to the input of pre-\ntrained models, enabling modification to different applications with minimal changes to the model itself 21. \nThis method enables efficient domain-specific fine-tuning without requiring the retraining of the entire model, \nparticularly in resource-limited settings. Recent innovations introduce hierarchical prefix tuning, which \norganizes prefixes in a hierarchical manner to provide more detailed control over the model’s responses 29. \nAdditionally, dynamic prefix tuning allows for real-time adaptation based on the input context, thereby improving \nthe flexibility and adaptability of the model 30. Techniques such as MixPrompt 31 and E2VPT 32 have also been \nintroduced to combine and optimize the usage of input and key-value prompts, advancing the application of \nprefix tuning in natural language processing applications.\nLow-rank adaptation (LoRA)  first proposed by 20, is a fine-tuning technique designed to optimize \nmemory usage and has received considerable attention in the research community since its inception. The \nlatest developments have expanded the range of applications for LoRA, particularly in the area of multitask \nlearning, as illustrated by research conducted by33,34, and35. Practical applications of LoRA were further explored \nby36, while 37 focused on optimizing its memory efficiency. A notable innovation, ReLoRA, introduced by 38, \nincorporates a full-rank warm-up phase.19 proposed adaptive approaches that dynamically adjust the low-rank \nadaptation parameters. Additionally,39 presented the Low-Rank Kronecker Product (LoKr), and 40 developed \nResLoRA, which integrates residual pathways. Further contributions include the Low-Rank Hadamard Product \n(LoHa) by41, and the introduction of Orthogonal Finetuning (OFT) and OFT with butterfly factorization (BOFT) \nby42 and43, which utilize orthogonal matrices to transform pre-trained weight matrices, resulting in significant \nimprovements in both fine-tuning efficiency and performance.\nSubspace learning has become a crucial area of research, with a focus on optimizing model weights within \na low-dimensional space, thereby providing computational efficiency and improved performance in various \nmachine learning tasks44,45. This approach has been extensively utilized in meta-learning and continual learning \nframeworks, as shown by several studies 44–49. Latest improvements in adaptive subspace learning methods \nhave demonstrated significant improvements in generalization and robustness, especially in challenging \nenvironments50,51. Furthermore, incorporation of subspace learning into neural architecture search has proven \ninvaluable in identifying efficient and innovative architectures, optimizing both performance and resource \nutilization51–53. The efficacy of subspace learning is further highlighted in scenarios requiring rapid adaptation \nto new tasks with limited data, such as few-shot learning and online learning, where it allows robust model \nperformance despite data limitations54.\nProjected gradient descent (PGD)  has been significantly improved by the development of advanced \nmethodologies such as GaLore 55. Unlike traditional approaches, which treat the objective function as a black \nbox, GaLore creates gradients within multilayer neural networks, providing a more extensive and effective \nScientific Reports |        (2024) 14:30667 3| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\noptimization process56,57. This approach has displayed notable enhancements in the convergence rate of neural \nnetwork training, particularly in high-dimensional datasets, while also contributing to advanced stability during \nthe training process 58. Furthermore, GaLore addresses the challenges of gradient sparsity and redundancy, \nresulting in significant gains in training efficiency55. These innovations have not only strengthened the robustness \nof neural networks against adversarial attacks but also ensured more stable and reliable training dynamics, \nmarking a noteworthy improvement in the field59–61.\nMemory-efficient optimization is a pivotal area of research within the development of adaptive optimization \nalgorithms, particularly in the context of large-scale models where memory bounds are a significant challenge. \nFoundational studies by 62 have proven the efficiency of quantization techniques and combined gradient \ncomputation in considerably reducing memory usage during training63. Building upon these contributions, the \nlatest innovations have introduced hierarchical memory management systems that enable dynamic memory \nallocation and sparse gradient updates, thereby further optimizing memory utilization, as highlighted by 64. \nMoreover,18 proposed a memory-efficient fine-tuning approach, employing block-wise optimizing strategies \nthat dynamically adjust memory allocation, achieving superior performance across several benchmarks. In a \nsimilar vein,19 explored the use of low-rank factorization techniques to compress model parameters effectively \nwhile preserving model accuracy. Collectively, these innovations contribute to the deployment of large-scale \nmodels on resource-limited devices, ensuring computational efficiency and maintaining optimal performance.\nIn contrast to previous techniques, our proposed SK-Tuning method introduces a novel strategy that utilizes \nauthentic, semantically meaningful prompts or prefix texts during adapter training. This method capitalizes on \nthe zero-shot capabilities of large language models (LLMs) and their fundamental understanding of linguistic \nsemantics. As a result, SK-Tuning is designed to achieve faster convergence and enhance task performance. \nThrough extensive experimental evaluations and comprehensive comparative analysis, we establish the \nsuperiority of SK-Tuning over existing fine-tuning techniques. These findings highlight the significant potential \nof SK-Tuning to advance fine-tuning methodologies in the field of NLP .\nBackground study\nPrefix and prompt tuning are methods of adapting large pretrained language models to specific tasks or datasets \nwith minimal updates to the model parameters. These techniques have gained prominence due to their efficiency \nand effectiveness, particularly in scenarios where updating the entire model is computationally expensive or \nimpractical.\nPrefix tuning\nPrefix tuning involves appending a sequence of tunable vectors, known as the prefix, to the input of each layer of \nthe transformer model. Let us denote the transformer model as a function F that maps an input sequence x to \nan output y, i.e., y = F(x). In prefix tuning, this mapping is modified to y = F (p ⊕ x), where p represents the \nprefix and ⊕ denotes concatenation.\nMathematically, if we consider a transformer model with K layers, and each layer k performs a transformation \nFl, the modified transformation with prefix becomes:\n F ′\nk(pk,x )= Fk(pk ⊕ x) (1)\nwhere pk is the prefix for layer k. The prefixes {p1,p 2, ..., pK} are learnable parameters and are optimized \nduring the training process.\nPrompt tuning\nPrompt tuning, on the other hand, leverages the concept of natural language prompts. Here, the model is fed a \nprompt that guides it to generate outputs tailored to a specific task. In mathematical terms, given a pretrained \nmodel M, the objective is to find an optimal prompt p∗ such that the model’s performance on a task T is \nmaximized when the prompt is used as an input.\nFormally, for a task T and a set of task-specific examples {(xi,y i)}, prompt tuning aims to optimize the \nfollowing:\n \np∗ = arg max\np\n∑\ni\nlog M(yi|p ⊕ xi) (2)\nThis objective function maximizes the likelihood of the correct outputs yi given the inputs xi concatenated with \nthe optimal prompt p∗. Unlike prefix tuning, prompt tuning does not modify the internal workings of the model \nbut rather influences its outputs through carefully crafted input sequences.\nSK-tuning procedure\nProblem definition\nConsider a downstream task that utilizes a pretrained LLM denoted as M. Let the training dataset be represented \nas D = {(xi,y i)}N\ni=1, where each training example (xi,y i) consists of an input text xi and its associated true \nlabel yi.\nScientific Reports |        (2024) 14:30667 4| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nOur primary objective is to fine-tune the pretrained LLM M for these downstream tasks while keeping the \nmodel parameters Θ frozen. Specifically, we aim to achieve this fine-tuning by introducing a small number of \nadditional parameters, referred to as adapters or transformations, which enable task-specific adaptation without \nthe need to retrain the entire LLM. Each instance in our system is a pair (xi,y i) that defines a specific task \nconfiguration.\nMathematically, our goal can be expressed as follows:\nGiven a pretrained LLM with parameters Θ and a dataset D, we seek to find a set of trainable parameters Φ \nfor the adapters or transformations, such that:\n Φ∗ = arg min\nΦ\nL(MΘ,Φ, D) (3)\nwhere: - MΘ,Φ represents the fine-tuned model with frozen parameters Θ and trainable adapters/transformations \nΦ. - L is a task-specific loss function that quantifies the alignment between model predictions and true labels \nacross the training dataset D.\nOur objective is to ascertain the veracity of the true labels yi for the corresponding input texts xi by effectively \ntraining a small number of parameters (Φ) without altering the pretrained model’s core architecture or parameters \n(Θ). This approach aims to achieve parameter efficiency while tailoring the LLM to specific downstream tasks.\nSK-tuning for prefix\nSK-Tuning for Prefix enhances the versatility and performance of the pretrained LLM for downstream tasks \nby judiciously incorporating semantic knowledge from prefixes into the fine-tuning process. In the context \nof prefix-tuning a pretrained LLM M, traditionally, a mapping function from virtual trainable tokens to the \nLLM’s layer representation is employed to generate the layer’s trainable parameters. However, in our proposed \nSK-Tuning approach, we adopt a different strategy. We leverage the power of a pretrained LLM M, with its \nparameters frozen, to directly acquire semantic knowledge embeddings from the prefix tokens.\nLet p denote a prefix comprising a sequence of semantic knowledge tokens, with a length of l. The LLM model \nis assumed to have a dimension of d. Our objective is to extract the semantic hidden representation from each \nlayer of the LLM for the given input prefix p. Let m represent the total number of layers, which includes the \nattention mechanisms. For each layer, we obtain its hidden representation hp\nj . For m layers the representation \ncan be defined as follows:\n hp = MΘfrozen (p) ∈ Rl×d. (4)\nNext, we introduce a trainable adapter F, parameterized by Φ, which takes hp as input and yields a semantic \nprojection z with the same dimension:\n z = FΦ(hp) ∈ Rm×l×d. (5)\nNow, we possess z as the semantic representation of prefix tokens for every layer of M. During the processing of \ninput xi in MΘ, we concatenate zj∈m to the processing layer for the j-th layer of MΘ. This operation allows the \nj-th layer to access the corresponding semantic information from the prefix text.\nNow, if ri represents the final hidden output of xi, we can define:\n ri = MΘfrozen (xi,z ). (6)\nConsider a task-specific module C , parameterized by ζ, which embodies a downstream task:\n oi = Cζ (ri) (7)\nHere, oi represents the output of our task.\nOur training objective aims to minimize the loss function L, which quantifies the discrepancy between oi and \nthe target label yi, thereby indicating whether oi correctly represents the label for xi:\n \nmin\nΦ,ζ\nL (Cζ (MΘfrozen (xi, FΦ(MΘfrozen (p)))),y i) . (8)\nThis approach allows the fine-tuning process to concentrate explicitly on the representation and comprehension \nof labels, while simultaneously harnessing the intrinsic knowledge embedded within Θ. The adjustment of \nparameters Φ and ζ empowers the model to further refine its ability to map textual inputs to their corresponding \nlabels.\nScientific Reports |        (2024) 14:30667 5| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nSK-tuning for prompt\nSK-Tuning for prompts involves a systematic process of semantic knowledge embedding, trainable adapter \nintegration, concatenation, and a training objective. This approach allows for fine-tuning the pretrained LLM to \neffectively leverage semantic knowledge from prompts for improved performance in various downstream tasks. \nIn the SK-Tuning framework for prompts, we focus on enhancing the capabilities of a pretrained LLM (M) by \nincorporating semantic knowledge from sequential prompt tokens, denoted as p, of length . Let E  represent the \ntoken embedding layer of M, and consider ep ∈ Rl×d and exi ∈ Rn×d as the semantic embeddings for prompt \np and input text xi, respectively. Here, n is the sequence length of input xi.\nTo obtain the semantic representation of the prompt p and input text xi, we utilize the pretrained token \nembedding layer E  as follows:\n ep = E(p) ∼ MΘfrozen  (9)\nand\n exi = E(xi) ∼ MΘfrozen  (10)\nThis operation yields ep, which encapsulates the semantic information of the prompt, while MΘfrozen  ensures \nthat the model parameters remain frozen during this process.\nTo further enhance the representation of the prompt, we introduce a trainable adapter, denoted as G , which is \nparameterized by γ. This adapter takes ep as input and produces an updated embedding e′\np as follows:\n e′\np = Gγ (ep) ∈ Rl×d (11)\nThe adapter Gγ serves as a mechanism to refine the semantic knowledge captured in ep according to the specific \ndownstream task requirements, allowing for fine-tuning without modifying the frozen model parameters.\nThe task head, denoted as C , is designed to incorporate both the enhanced prompt representation e′\np and the \nsemantic embeddings of the input text exi . We achieve this through concatenation:\n oi = Cζ(MΘfrozen (e′\np ⊕ exi )) (12)\nHere, ⊕ represents the concatenation operation, and oi serves as the output for the downstream task, allowing \nthe model to leverage both prompt and input text information effectively.\nThe training objective for SK-Tuning of the prompt involves minimizing a loss function L. This loss function \nquantifies the difference between the predicted output and the target label yi, reflecting the model’s performance \non the specific task:\n \nmin\nγ,ζ\nL (Cζ (MΘfrozen (Gγ (ep) ⊕ exi ),y i) (13)\nHere, γ and ζ denote the parameters of the adapter G  and the task head C  respectively.\nAlgorithms\nIn this section, we describe two key algorithms that constitute the core of our proposed SK-Tuning approach for \nenhancing the fine-tuning of LLMs in the context of specific downstream tasks.\nSK-tuning for prefix\nThe first algorithm, outlined in Algorithm 1 (SK-Tuning for Prefix), details the process of incorporating semantic \nknowledge from prefixes into the fine-tuning of a pretrained LLM. The algorithm begins with inputs of a \npretrained language model M with frozen parameters Θ, a prompt text p, and a dataset (xi,y i)N\ni=1. The trainable \nparameters Φ and ζ are initialized. For each input example (xi, yi), the prompt text is processed through the \nfrozen LLM to obtain hp, which represents the hidden representation from each layer. This representation is then \ntransformed using a trainable adapter FΦ to yield z. Subsequently, the input text xi is processed, incorporating \nthe generated z for improved task-specific adaptation. Finally, the classification head Cζ  computes the output \noi for the downstream task, and the loss L(oi,y i) is computed. The trainable parameters Φ and ζ are updated \niteratively to minimize the loss.\nScientific Reports |        (2024) 14:30667 6| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nAlgorithm 1. SK-tuning for prefix\nSK-tuning for prompt\nThe second algorithm, described in Algorithm 2 (SK-Tuning for Prompt), focuses on leveraging semantic \nknowledge from sequential prompt tokens to enhance fine-tuning. It begins with inputs of a pretrained language \nmodel M with frozen parameters Θ, a prompt text p, and a dataset (xi, yi)N\ni=1. Trainable parameters γ and ζ are \ninitialized. For each input example (xi,y i), the embeddings of the prompt text p and input text xi are obtained \nthrough the pretrained token embedding layer E  while ensuring the core LLM parameters remain frozen. The \nprompt embedding ep and text embedding exi are then utilized to create an enhanced prompt representation \ne′\np using a trainable adapter Gγ. The classification head Cζ  combines this enhanced prompt representation with \nthe input text embedding and computes the output oi for the downstream task. As in the previous algorithm, the \nloss L(oi,y i) is computed, and the trainable parameters γ and ζ are updated iteratively to minimize the loss.\nAlgorithm 2. SK-tuning for prompt\nExperiments\nExperimental setup\nOur experiments utilize a computational setup with two NVIDIA RTX H100 GPUs (80GB VRAM each), an \nIntel®Xeon®Gold 6448Y 2.1 GHz 32 Core Processor. This system includes 128GB of RAM and a Dell 7.68TB \nEnterprise NVMe Read Intensive Drive, providing the necessary computational power and storage for efficient \nmodel training and evaluation.\nFor the implementation, we employed the PyTorch 65 deep learning framework for the implementation of \nour experiments. Additionally, we leveraged the Transformers library developed by Hugging Face66. This library \noffers a comprehensive set of tools and pretrained models for NLP tasks, facilitating the training and evaluation \nof LLMs on a variety of datasets.\nThe combination of these resources and software frameworks allowed us to conduct extensive experiments, \nenabling us to assess the performance and effectiveness of our proposed SK-Tuning approach across a range of \ndownstream tasks.\nLM results\nDatasets: We evaluate our SK-Tuning on CoLA, SST-2, MRPC, STS-B, QQP , MNLI, QNLI and RTE of the \nGLUE Benchmarks67. We compute the accuracy using the Matthews correlation for CoLA, accuracy/F1 score for \nScientific Reports |        (2024) 14:30667 7| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nMRPC and QQP , Pearson/Spearman correlation for STS-B, average matched accuracy for MNLI, and accuracy \nfor other NLU tasks in Table 1.\nModel selection & hyperparameter:  For the GLUE benchmark, the models we select for fine-tuning are \nRoBERTa-base RoBB with 125M parameters and RoBERTa-large RoBL with 355M parameters from 68. \nDropout, attention dropout, and weight decay rates are uniformly maintained at 0.2 across all tasks. The initial \nlearning rate was 1 × 10−4, subsequently fine-tuned to 2 × 10−5 and 2 × 10−6. All datasets have been trained \nover 10 epochs.\nResults: Table  1 presents a detailed performance comparison of various parameter-efficient fine-tuning \n(PEFT) methods applied to two versions of the RoBERTa model on GLUE tasks, highlighting the SK-Tuning \nmethods as particularly effective. These methods achieve competitive or superior performance across several \nmetrics while utilizing significantly fewer parameters-demonstrated by as low as 0.60M parameters for RoBB \nand 1.02M for RoBL. Notably, SK-Tuning (Prompt) and SK-Tuning (Prefix) consistently perform well across \ndifferent task types, such as SST2 and QQP , demonstrating a compelling balance between model efficiency and \ntask performance. This efficiency makes SK-Tuning an attractive option for scenarios requiring deployment in \nresource-constrained environments or where fast inference is crucial. The results underscore the potential of \nsmall, well-tuned models to match or even surpass the performance of larger, fully fine-tuned counterparts, \nsuggesting a promising direction for future research in NLP model optimization.\nLLM results\nWe conducted experiments on a diverse set of datasets to evaluate the performance of SK-Tuning across various \nNLP tasks, including sequence classification, token classification, and NLI. Our goal was to compare the \nperformance of SK-Tuning with existing models on these tasks. Subsequently, we provide extensive details on \nthe datasets utilized in our experiments.\nModel PEFT method # TPs CoLA SST2 MRPC STS-B QQP MNLI QNLI RTE Avg.\nRoBB\nFT 124.6M 67.07 95.89 90.24/93.98 92.87/91.61 91.18/89.02 88.27 92.67 78.20 87.04/91.53\nAdapterS 7.41M 63.32 94.31 90.44/93.18 91.25/90.94 90.81/86.55 87.33 92.06 73.56 85.38/90.22\nPrompt tuning 0.61M 49.37 92.09 70.83/81.72 82.44/83.11 82.99/78.35 80.57 80.03 58.12 74.55/81.06\nPrefix-tuning 0.96M 59.31 93.81 87.25/91.03 88.48/88.32 87.75/84.09 85.21 90.77 54.51 80.88/87.81\n(IA)3 0.66M 59.58 93.92 87.00/90.52 90.30/90.32 87.99/84.10 83.95 90.88 71.12 83.09/88.31\nBitFit 0.69M 61.32 94.72 89.22/92.41 90.34/90.27 88.12/84.11 84.64 91.09 77.98 84.67/88.93\nLoRA 0.89M 62.09 94.04 87.50/90.68 90.66/90.83 88.83/85.21 86.54 92.02 72.92 84.32/88.90\nAdaLoRA 1.03M 59.82 93.92 87.99/91.33 90.83/90.73 88.58/84.98 86.26 91.43 70.04 83.60/89.01\nMAM Adapter 46.78M 61.42 94.87 89.31/92.21 90.74/90.42 88.31/83.20 86.63 90.19 72.62 84.26/88.61\nPROPETL Adapter 1.87M 66.33 93.85 87.25/90.82 91.33/91.04 89.22/85.79 86.49 92.56 75.54 85.32/89.21\nPROPETL Prefix 10.49M 61.79 94.30 88.73/91.98 90.30/90.19 88.54/85.05 86.22 91.51 63.31 83.08/89.07\nPROPETL LoRA 1.77M 60.38 94.11 87.42/90.87 90.76/90.55 88.90/85.55 86.84 92.04 67.39 83.48/88.99\nSK-Tuning (Prompt) 0.60M 60.21 94.88 89.73/92.47 91.30/90.19 90.83/87.82 86.24 92.60 76.91 85.45/90.37\nSK-Tuning (Prefix) 0.84M 61.83 93.72 90.21/90.04 90.11/89.92 88.67/87.12 85.83 92.09 75.32 83.83/89.29\nRoBL\nFT 355.3M 69.78 97.54 92.22/94.28 93.74/92.96 93.30/89.68 92.42 96.61 89.23 90.60/92.30\nAdapterS 19.77M 67.03 96.37 89.94/92.54 92.58/92.42 92.19/88.50 91.00 94.31 85.25 88.58/91.15\nPrompt-tuning 1.07M 61.13 94.61 73.04/81.29 78.51/78.99 80.74/75.16 68.15 89.13 60.29 75.70/78.48\nPrefix-tuning 2.03M 59.01 95.76 88.24/91.37 90.92/91.07 88.88/85.45 89.30 93.32 74.01 84.93/89.29\n(IA)3 1.22M 61.15 94.61 86.52/90.33 92.22/86.25 89.45/86.25 88.63 94.25 81.23 86.00/87.61\nBitfit 1.32M 68.01 96.10 90.93/93.38 91.93/93.38 89.48/86.43 89.98 94.47 87.73 88.57/91.06\nLoRA 1.84M 64.47 96.67 87.50/91.19 91.66/91.44 90.15/86.91 90.76 95.00 79.78 86.99/89.84\nAdaLoRA 2.23M 65.85 94.95 89.46/92.34 92.05/91.80 89.60/86.30 90.36 94.62 77.98 86.85/90.14\nMAM Adapter 122.20M 67.39 95.81 90.12/92.77 92.44/92.18 90.87/86.65 90.62 94.31 86.62 88.52/90.53\nPROPETL Adapter 5.40M 65.55 96.27 89.71/92.54 91.92/91.67 90.67/87.74 91.37 95.20 88.89 88.69/90.65\nPROPETL Prefix 26.85M 62.24 96.17 90.04/92.92 90.70/90.49 89.30/86.30 90.33 94.73 79.71 86.65/89.90\nPROPETL LoRA 4.19M 61.90 95.93 89.06/92.19 91.66/91.38 90.93/88.05 90.53 94.93 83.57 87.31/90.54\nSK-Tuning (Prompt) 1.02M 67.13 96.43 91.10/93.22 92.54/92.11 92.10/88.73 90.42 95.42 87.11 89.01/91.34\nSK-Tuning (Prefix) 1.94M 66.33 96.08 90.96/93.09 91.87/90.68 90.23/87.93 89.97 96.10 86.99 86.86/89.66\nTable 1. Performance comparison of RoBERTa models on GLUE tasks: metrics include MCC for CoLA, \naccuracy for SST-2, accuracy/F1-score for MRPC and QQP , Pearson/Spearman correlation for STS-B, and \nAccuracy for MNLI, QNLI, and RTE. Significant values are in [bold, italics].\n \nScientific Reports |        (2024) 14:30667 8| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nClassification datasets\nSequence classification, a common task in NLP , involves labeling or categorizing text. In our study, we utilized \nfive datasets: CoLA, SST2 from the GLUE benchmark, along with the Emotion dataset, and the Fake News \nFilipino dataset.\n• Cola (https://huggingface.co/datasets/glue/viewer/cola/): The Corpus of Linguistic Acceptability (CoLA) 69 \nconsists of 10,657 sentences curated from 23 linguistic publications. Each sentence has been meticulously an-\nnotated by its original author for grammaticality or acceptability. The publicly available version of the dataset \nincludes 9,594 sentences for training and validation, while 1063 sentences are reserved for a separate held-out \ntest set.\n• SST-2 ( https://huggingface.co/datasets/sst2): The Stanford Sentiment Treebank is a dataset featuring fully \nlabeled parse trees, enabling a comprehensive examination of how sentiment compositionally influences \nlanguage. Derived from the dataset presented by Pang and Lee 70, the corpus comprises 11,855 individual \nsentences extracted from movie reviews. Employing the Stanford parser, the dataset encompasses a total of \n215,154 distinct phrases derived from these parse trees, with each phrase annotated by three human judges. \nThe experiments involving binary classification on complete sentences (distinguishing between negative or \nsomewhat negative versus somewhat positive or positive, with neutral sentences excluded) are denoted by the \ndataset acronym SST-2. The publicly available version includes 67,349 sentences designated for training along \nwith 872 for validation set, while 1,821 sentences are for the test set.\n• Emotion (https://huggingface.co/datasets/dair-ai/emotion): Emotion is a dataset71 of English Twitter  m e s s a g \ne s with six basic emotions: anger, fear, joy, love, sadness, and surprise. The publicly available version includes \n16,000 sentences designated for training along with 2000 for validation set, while 2000 sentences are for the \ntest set.\n• Fake News Filipino (https://huggingface.co/datasets/fake_news_filipino): A unique initiative in  l o w - r e s o u r \nc e fake news detection dataset72 for the Filipino language. Comprising 3,206 meticulously labeled news sam-\nples, evenly divided between authentic and fabricated content, this dataset represents a pioneering effort. We \npartitioned the dataset into 70%, 10% and 20% for training, validation, and testing purposes.\nToken classification datasets\nToken classification involves labeling individual tokens within a sentence. Named Entity Recognition (NER) \nis a prevalent task in token classification, aiming to assign labels to entities in a sentence, which may include \nindividuals, locations, or organizations. We have used 3 token classification datasets: CoNLL 2003, NCBI \nDisease, and WikiAnn dataset.\n• CoNLL 2003 (https://huggingface.co/datasets/conll2003): CoNLL-2003 serves as a named entity recognition \ndataset73 introduced within the framework of the CoNLL-2003 shared task, focusing on language-independ-\nent named entity recognition. This dataset comprises eight files that encompass two languages: English and \nGerman. We have utilized the English dataset and “ner tags” as labels for our experiment. The publicly avail-\nable version includes 14,041 examples designated for training along with 3250 for validation examples, while \n3453 examples are for testing.\n• NCBI Disease (https://huggingface.co/datasets/ncbi_disease): The dataset74 includes annotations for disease \nnames and concepts from the NCBI disease corpus, which is a compilation of 793 PubMed abstracts exten -\nsively annotated at both the mention and concept levels. There are 3 labels, 0 indicates no disease mentioned, \n1 signals the first token of a disease, and 2 the subsequent disease tokens. The publicly available version in -\ncludes 5433 examples designated for training along with 924 for validation examples, while 941 examples are \nfor testing.\n• WikiAnn (https://huggingface.co/datasets/wikiann): WikiANN, also known as PAN-X, is a multilingual  d a \nt a s e t   7 5   for named entity recognition. It comprises Wikipedia articles annotated with location (LOC), person \n(PER), and organization (ORG) tags using the IOB2 format. This specific version aligns with the balanced \ntrain, validation, and test splits of 20,000, 10,000, and 10,000, respectively introduced by Rahimi et al. (2019), \ncovering 176 out of the 282 languages featured in the original WikiANN corpus.\nEntailment datasets\nNLI involves the challenge of determining the truth (entailment), falsity (contradiction), or undetermined status \n(neutral) of a “hypothesis” based on a provided “premise. ” We have used 3 NLI datasets for this task: RTE, SNLI, \nand MRPC.\n• RTE (https://huggingface.co/datasets/glue/viewer/rte): The Recognizing Textual Entailment (RTE) datasets \noriginate from a series of annual challenges focused on textual entailment. The creators of the benchmark \namalgamated data from RTE176, RTE277, RTE378, and RTE579. Constructed examples are derived from news \nand Wikipedia text. To maintain consistency, the benchmark creators transformed all datasets into a two-class \nsplit, collapsing neutral and contradiction into “not entailment” for three-class datasets. The publicly available \nversion includes 2490 examples designated for training along with 277 for validation examples, while 3,000 \nexamples are for testing.\n• MRPC ( https://huggingface.co/datasets/glue/viewer/mrpc): The Microsoft Research Paraphrase Corpus \n(MRPC)80 comprises 5801 pairs of sentences extracted from newswire articles. Human annotators have la -\nbeled each pair to indicate whether it is a paraphrase or not. The entire dataset is split into a training subset, \nconsisting of 4076 sentence pairs (with 2753 identified as paraphrases), and a test subset, containing 1725 \npairs (with 1147 recognized as paraphrases).\nScientific Reports |        (2024) 14:30667 9| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\n• SNLI (https://huggingface.co/datasets/snli): The Stanford NLI (SNLI) corpus 81 is an assemblage of 570,000 \npairs of English sentences crafted by humans. These sentence pairs have been meticulously labeled to achieve \nbalanced classification, with the labels entailment, contradiction, and neutral. This corpus is designed to facil-\nitate the task of NLI. The publicly available version includes 550,152 examples designated for training along \nwith 10,000 for validation examples, while 10,000 examples are for testing.These datasets collectively cover a \nwide spectrum of NLP tasks, enabling comprehensive evaluations of SK-Tuning’s performance across various \ndomains and challenges.\nLarge language models\nIn our analysis, we utilized multiple Large Language Models (LLMs) to obtain extensive and detailed results. \nSpecifically, we employed Bloom 7b, Llama2 7b, Mistral 7b, Falcon 7b, and Phi-2 2.7b, each offering unique \nstrengths and capabilities that complemented one another.\n• Bloom: A 7B parameter LLM from BigScience, trained on an extensive corpus of text and code. Bloom dis -\nplays robust performance on various NLP tasks and offers several variants, including Bloom Text-to-Text and \nBloom Code82.\n• Llama2: Meta AI has introduced Llama 2, its most advanced LLM to date. Llama 2 showcases a diverse array \nof capabilities and potential applications, with model sizes ranging from 7 billion to 70 billion parameters. \nThis release provides access to both model weights and initial code for pretrained and fine-tuned Llama \nmodels, including variants such as Llama Chat (specialized for dialogue) and Code Llama (optimized for \nprogramming tasks)83.\n• Mistral: Mistral 7B is a freely available, open-source language model comprising 7.3 billion parameters that \ndemonstrates exceptional performance. Released in September 2023, it exhibits competitive results in com -\nparison to Meta’s LLaMA models, outperforming the 13B version on all benchmarks evaluated and equaling \nthe 34B version on numerous metrics. Developed using the transformers architecture and accessible via Bit-\nTorrent and Hugging Face, Mistral 7B presents a robust and accessible option for researchers and developers \nseeking a high-performing LLM84.\n• Falcon: The Falcon Large Language Model (LLM) is a generative LLM designed to advance applications and \nuse cases for future-proofing our world. Currently, the Falcon 180B, 40B, 7B, and 1.3B parameter artificial \nintelligence models, along with the high-quality REFINEDWEB dataset, constitute a comprehensive suite of \nofferings85.\n• phi-2: Phi-2, the most recent small language model (SLM) developed by Microsoft Research, is a 2.7 billion \nparameter model that showcases superior reasoning and language understanding capabilities compared to its \npredecessors, Phi-1 and Phi-1.586. The model was trained on a diverse dataset, comprising “textbook quality” \nweb data and synthetic textbooks/exercises generated using GPT-3.5. Phi-2 exhibits exceptional performance \nin various tasks, including Python code generation87. It is noteworthy that Phi-2 surpasses the performance of \nmodels up to 25 times larger in size. Furthermore, Phi-2 has been released under an MIT License, permitting \nits utilization in commercial applications.\nBaseline methods\nWe established the following baseline methods to evaluate the performance of our proposed approach:\n• Full fine-tuning: This methodology 88 involves the adjustment of all parameters within the pretrained lan -\nguage model to adapt it to the specific task at hand. It functions as a comprehensive adaptation approach; \nhowever, it can be computationally intensive.\n• Prefix tuning: This lightweight method 89 introduces trainable continuous vectors termed “prefixes” to the \ninput of each transformer layer, while the original model parameters remain fixed. Prefix-tuning is predicated \non the concept of prompting in language models, enabling ensuing tokens to attend to this prefix as if it were \ncomposed of “virtual tokens” . It presents a more efficient alternative to complete fine-tuning, particularly in \nlow-data scenarios.\n• Prompt tuning: This method90 employs natural language prompts called “soft prompts” to guide the model’s \nbehavior without modifying its internal parameters. This provides a flexible method to adapt models to dif -\nferent tasks without additional training.\n• P tuning: This method 91 introduces an optimized prompt tuning method, which exhibits efficacy across a \ndiverse spectrum of model scales and natural language tasks. The method addresses the suboptimal per -\nformance associated with prompt tuning when applied to pretrained models of typical size. Moreover, it \nendeavors to rectify the limitations observed in the performance of prompt tuning, particularly its inefficacy \nin challenging sequence labeling tasks.\n• LoRA: LoRA20 (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that involves learning \nlow-rank matrices to adapt the model while freezing most of its original parameters in a fixed state. This study \ninvestigated LoRA with rank 2 and rank 4 to evaluate its capability in optimizing the balance between perfor-\nmance and efficiency.These baseline methods represent a diverse range of fine-tuning strategies, allowing us \nto measure the comparative performance of our proposed approach.\nEvaluation metrics\nEvaluation metrics measure the performance of a model on a specific dataset by comparing the model’s \npredictions with ground truth labels. Various tasks have specific metrics, and we used accuracy and F1 score in \nour experiments.\nScientific Reports |        (2024) 14:30667 10| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nAccuracy\nAccuracy is a metric that measures the overall correctness of a model’s predictions. It is calculated as the ratio of \ncorrect predictions to the total number of predictions made by the model:\n Accuracy = True Positives (TP) + True Negatives (TN)\nTotal Predictions\nF1 score\nThe F1 score is the harmonic mean of precision and recall, providing a balanced measure considering both false \npositives and false negatives:\n F1 Score= 2 × Precision × Recall\nPrecision + Recall\nThe F1 score ranges from 0 to 1, with higher values indicating better overall performance in terms of precision \nand recall.\nIn these formulas, TP , TN, FP , and FN represent the counts of true positives, true negatives, false positives, and \nfalse negatives, respectively.\nHyperparameters setting\nIn our experiments, we carefully selected hyperparameters to ensure consistent and effective training across \nvarious datasets and tasks.\nFor the maximum sequence length, we set it to 128 for all datasets except for RTE, where we did not impose \na maximum sequence length.\nRegarding learning rates, we employed the following values:\n• For the sequence classification datasets, the learning rate was set to 1 × 10−3.\n• For the token classification datasets, a learning rate of 1 × 10−5 was used.\n• For the NLI datasets, the learning rate was set to 1 × 10−4.In terms of the number of training epochs:\n• \n• Sequence classification datasets were trained for 5 epochs.\n• Token classification datasets were trained for 10 epochs.\n• NLI datasets were trained for 10 epochs, with the exception of the SNLI dataset, which was trained for 2 ep -\nochs on each model.For all our datasets, regardless of the task or tuning method (P-Tuning, Prefix Tuning, or \nPrompt Tuning), we consistently used 20 virtual tokens during training.\nWe employ the Adaptive Moment Estimation with Weight Decay (ADAMW)92 optimizer for all experiments. \nThe ADAMW optimizer is an improved version of the traditional ADAM optimizer, which incorporates weight \ndecay directly into the optimization process to better handle regularization. Additionally, we set the weight decay \nvalue to 0.01 across all experiments to control regularization and ensure stable model training.\nResult analysis\nSee Tables 2, 3, 4, 5 and 6.\nSequence classification\nIn the realm of classification, we conducted a comprehensive evaluation across various LLMs, including Bloom, \nLlama2, Falcon, Mistral, and Phi-2, employing different fine-tuning techniques. For each model, we examined \nthe effectiveness of traditional approaches such as Finetuning, Prefix Tuning, Prompt Tuning, PTuning, Lora \nRank 2, and Lora Rank 4, and compared them to our proposed SK-Tuning methods, both for Prefix and Prompt. \nNotably, SK-Tuning consistently outperforms traditional methods across different datasets, showcasing its \nsuperior efficiency and effectiveness. The performances of various models on different datasets are documented \nin Tables 2, 3, 4, 5, and 6.\nAcross the “Fake News Filipino” dataset, SK-Tuning, especially when applied as SK-Tuning (Prefix), \ndemonstrates remarkable performance improvements compared to traditional approaches. It achieves the \nhighest accuracy and F1-score, emphasizing its capability to efficiently adapt LLMs to specific tasks while \nminimizing trainable parameters. In the “Emotion” dataset, SK-Tuning consistently outperforms other methods, \nindicating its robustness across different classification tasks. The same trend is observed in the “SST2” dataset, \nwhere SK-Tuning invariably achieves superior results. Lastly, in the “Cola” dataset, SK-Tuning (Prefix) and \nSK-Tuning (Prompt) perpetually outperform other approaches, underscoring their potential for enhancing \nsequence classification tasks.\nComparatively, traditional methods like Prefix Tuning and Prompt Tuning, although efficient in terms \nof parameters compared to Fine-tuning, tend to lag behind SK-Tuning in terms of accuracy and F1-score. \nFurthermore, SK-Tuning requires fewer trainable parameters, making it an attractive choice for practitioners \naiming to optimize performance while maintaining efficiency.\nScientific Reports |        (2024) 14:30667 11| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nToken classification\nIn this comprehensive analysis of token classification across various datasets, we conducted an extensive \nevaluation of five distinct models: Bloom, Llama2, Falcon, Mistral, and Phi-2, while exploring a range of fine-\ntuning techniques to understand their impact on performance, documented in Tables  7, 8, 9, 10 and  11. The \ndatasets used for evaluation included conll03, ncbi disease, and wiki ann, each representing different challenges \nin token classification.\nFirst and foremost, we observed that Full Fine-tuning consistently achieved high accuracy across all models \nand datasets. However, it also required a substantial percentage of parameters, potentially making it less feasible \nfor resource-constrained environments.\nTo address the trade-off between model efficiency and performance, we investigated several fine-tuning \ntechniques. Prefix Tuning, Prompt Tuning, and P-Tuning, which involve introducing a small fraction of \nparameters, showcased mixed results. While these techniques achieved decent accuracy in some cases, they \noften lagged behind in terms of F1-score, indicating challenges in maintaining a balance between precision and \nrecall.\nRemarkably, Lora Rank 2 and Lora Rank 4, with a moderate percentage of parameters, consistently \ndelivered a strong performance, especially in terms of the F1-score. These results underscore the importance of \nconsidering the architecture of the model when optimizing for token classification tasks, with Lora Rank models \ndemonstrating their effectiveness.\nFinally, SK-Tuning techniques, both Prefix and Prompt variants, stood out as noteworthy approaches. They \nrequired an extremely minimal percentage of additional parameters yet yielded competitive accuracy and \nremarkable F1 scores. This suggests that these techniques have the potential to strike a favorable balance between \nmodel efficiency and task effectiveness.\nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nFake News Filipino\nFull Fine-tuning 100.000 95.02 93.83\nPrefix Tuning 0.03493 70.99 68.18\nPrompt Tuning 0.00701 74.31 72.23\nP-Tuning 0.01582 72.97 70.19\nLora Rank 2 0.01413 90.13 88.87\nLora Rank 4 0.05794 93.56 90.05\nSK-Tuning (Prefix) 0.00035 92.86 90.63\nSK-Tuning (Prompt) 0.00016 91.03 89.13\nEmotion\nFull Fine-tuning 100.000 90.31 87.52\nPrefix Tuning 0.03521 74.75 68.11\nPrompt Tuning 0.00813 79.12 71.07\nP-Tuning 0.01593 69.45 70.23\nLora Rank 2 0.02413 86.76 80.23\nLora Rank 4 0.06831 87.52 82.01\nSK-Tuning (Prefix) 0.00161 88.21 82.64\nSK-Tuning (Prompt) 0.00104 86.82 82.14\nSST2\nFull Fine-tuning 100.000 97.93 97.81\nPrefix Tuning 0.03493 85.78 86.31\nPrompt Tuning 0.00715 92.45 92.78\nP-Tuning 0.01653 91.34 91.75\nLora Rank 2 0.01456 92.27 92.77\nLora Rank 4 0.02831 94.36 94.83\nSK-Tuning (Prefix) 0.00082 96.85 96.65\nSK-Tuning (Prompt) 0.00034 96.53 96.19\nCola\nFull Fine-tuning 100.000 87.05 89.93\nPrefix Tuning 0.03495 73.72 83.69\nPrompt Tuning 0.00723 82.74 87.70\nP-Tuning 0.01615 70.32 81.12\nLora Rank 2 0.01415 81.13 83.03\nLora Rank 4 0.02797 84.33 85.21\nSK-Tuning (Prefix) 0.00083 84.91 86.01\nSK-Tuning (Prompt) 0.00052 84.51 85.92\nTable 2. Sequence classification results for the bloom model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity.\n \nScientific Reports |        (2024) 14:30667 12| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nEntailment detection\nThe results of entailment detection using various models, including Bloom, Llama2, Falcon, Mistral, and \nPhi-2, are presented in Tables  12, 13, 14, 15 and  16 . Across all three datasets (RTE, MRPC, SNLI), full fine-\ntuning consistently achieves the highest accuracy and F1-score, with Bloom and Mistral models demonstrating \nremarkable results. This underscores the value of fine-tuning the entire model’s parameters to adapt to specific \nentailment tasks, as it allows the model to capture intricate patterns and nuances in the data.\nIn contrast, prefix tuning and prompt tuning techniques, which involve fine-tuning only a small fraction \nof the model’s parameters, tend to yield significantly lower accuracy and F1-scores. This suggests that limiting \nparameter updates to specific prefixes or prompts may not be sufficient for optimal entailment classification \nperformance, as these methods may struggle to capture the diverse and complex relationships present in the \ndata.\nThe Lora Rank 2 and Lora Rank 4 models deliver competitive results, particularly evident in the RTE dataset, \nwhere they outperform other techniques. This indicates that techniques like Lora Rank, which involve a moderate \namount of parameter modification, can strike a balance between model adaptation and computational efficiency.\nHowever, SK-Tuning, whether applied to prefixes or prompts, consistently performs well across datasets, \ndemonstrating its effectiveness as an alternative fine-tuning strategy. SK-Tuning achieves strong results with \na minimal increase in the number of parameters, making it a promising approach for entailment classification \ntasks where computational resources are a concern.\nAblation study\nEfficiency\nFigure 2 illustrates that SK-Tuning methods for Prefix and Prompt, demonstrate superior memory efficiency \nwith the lowest memory cost among the compared PEFT methods, making them ideal for resource-constrained \nenvironments. Despite their minimal memory footprint, these methods maintain competitive training efficiency, \nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nFake News Filipino\nFull Fine-tuning 100.000 95.22 93.90\nPrefix Tuning 0.03983 70.06 68.57\nPrompt Tuning 0.00743 73.72 72.07\nP-Tuning 0.01731 71.54 70.63\nLora Rank 2 0.01601 90.38 87.62\nLora Rank 4 0.03213 92.14 90.86\nSK-Tuning (Prefix) 0.00024 92.26 89.90\nSK-Tuning (Prompt) 0.00035 90.83 88.23\nEmotion\nFull Fine-tuning 100.000 91.11 87.92\nPrefix Tuning 0.03994 84.31 82.78\nPrompt Tuning 0.00864 85.37 82.50\nP-Tuning 0.01781 83.05 81.88\nLora Rank 2 0.01624 86.49 82.86\nLora Rank 4 0.03233 88.56 84.18\nSK-Tuning (Prefix) 0.00174 88.72 83.51\nSK-Tuning (Prompt) 0.00122 85.92 82.87\nSST2\nFull Fine-tuning 100.000 97.32 97.69\nPrefix Tuning 0.04855 85.78 86.31\nPrompt Tuning 0.00712 94.24 97.26\nP-Tuning 0.01753 95.55 96.62\nLora Rank 2 0.01607 86.97 81.93\nLora Rank 4 0.03191 87.11 82.03\nSK-Tuning (Prefix) 0.00088 96.52 96.48\nSK-Tuning (Prompt) 0.00037 96.50 96.35\nCola\nFull Fine-tuning 100.000 88.22 89.64\nPrefix Tuning 0.03984 71.18 83.29\nPrompt Tuning 0.00757 73.27 85.26\nP-Tuning 0.01751 69.12 81.74\nLora Rank 2 0.01603 82.25 83.43\nLora Rank 4 0.03213 84.18 83.88\nSK-Tuning (Prefix) 0.00092 85.01 86.22\nSK-Tuning (Prompt) 0.00060 84.36 85.85\nTable 3. Sequence classification results for the Llama2 model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 13| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nbalancing low parameter percentages with moderate training times, which highlights their effectiveness in \nachieving lightweight and fast fine-tuning. Compared to other methods like LoKr, LoHa, and LoRA, which show \nhigher memory costs and varying degrees of training efficiency, SK-Tuning stands out as a robust approach that \noptimizes both memory and computational resources, making it particularly advantageous for scenarios where \nefficiency is paramount.\nFaster convergence with SK-tuning\nIn this section, we present an ablation study comparing the convergence speed and performance of SK-Tuning \nwith traditional prompt and prefix tuning methods on three different downstream tasks: Token Classification, \nSequence Classification, and NLI. We hypothesize that SK-Tuning, leveraging semantic knowledge, will lead to \nfaster convergence due to the inherent zero-shot capabilities of LLMs93.\nAccelerated convergence in token classification\nIn the context of token classification tasks, we conducted a comprehensive comparison between SK-Tuning and \ntraditional tuning methods. We utilized two benchmark datasets, namely Wikiann and Conll03, both featuring \ntoken-level labels. Our primary objective was to analyze the convergence behavior, measured in terms of loss \nreduction, as training steps progressed.\nFigure  3 visually presents the convergence trajectories for SK-Tuning and traditional methods. Notably, \nwe observed a remarkable disparity in the convergence speed between these approaches. SK-Tuning, whether \napplied to prefixes or prompts, demonstrated a strikingly swift convergence compared to the conventional \ntuning method.\nThis accelerated convergence showcased in Fig. 3 serves as compelling evidence of the significant advantages \nbrought about by the incorporation of semantic knowledge. It underscores the ability of SK-Tuning to facilitate \nrapid adaptation to the intricacies of token classification tasks, emphasizing the practical utility of this approach.\nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nFake News Filipino\nFull Fine-tuning 100.000 94.05 92.93\nPrefix Tuning 0.03821 69.57 68.19\nPrompt Tuning 0.00732 72.35 70.78\nP-Tuning 0.01797 70.23 69.15\nLora Rank 2 0.00972 88.31 85.14\nLora Rank 4 0.05784 91.89 89.44\nSK-Tuning (Prefix) 0.00075 90.11 88.93\nSK-Tuning (Prompt) 0.00029 90.21 87.23\nEmotion\nFull Fine-tuning 100.000 88.53 85.94\nPrefix Tuning 0.03836 81.14 80.61\nPrompt Tuning 0.00841 87.25 84.19\nP-Tuning 0.01803 81.76 79.14\nLora Rank 2 0.01194 84.17 82.34\nLora Rank 4 0.05781 88.79 86.13\nSK-Tuning (Prefix) 0.00204 86.91 82.11\nSK-Tuning (Prompt) 0.00113 86.29 82.03\nSST2\nFull Fine-tuning 100.000 96.23 95.76\nPrefix Tuning 0.03818 90.18 91.36\nPrompt Tuning 0.00605 93.56 93.75\nP-Tuning 0.01781 90.33 91.26\nLora Rank 2 0.01193 91.13 92.07\nLora Rank 4 0.05789 91.72 92.17\nSK-Tuning (Prefix) 0.00093 94.73 94.02\nSK-Tuning (Prompt) 0.00038 95.02 95.01\nCola\nFull Fine-tuning 100.000 85.22 87.39\nPrefix Tuning 0.03826 70.03 82.23\nPrompt Tuning 0.00711 71.45 84.47\nP-Tuning 0.01792 68.07 81.73\nLora Rank 2 0.00973 82.14 82.38\nLora Rank 4 0.05741 84.66 85.33\nSK-Tuning (Prefix) 0.00094 83.74 85.03\nSK-Tuning (Prompt) 0.00065 84.01 85.11\nTable 4. Sequence classification results for the Falcon model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 14| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nAccelerated convergence in sequence classification\nFor the evaluation of SK-Tuning in sequence classification tasks, we conducted a comparative analysis against \ntraditional tuning methods. Our experimentation leveraged two benchmark datasets: Fake News and SST2, \nboth featuring sequences with corresponding labels. Our primary objective was to assess the convergence \nperformance, measured in terms of loss reduction, as the model underwent training iterations.\nFigure 4 offers a visual representation of the convergence patterns observed during sequence classification. \nNotably, the results depicted in the figure demonstrate the accelerated convergence achieved with SK-Tuning \nwhen compared to conventional tuning methods.\nThe swift convergence illustrated in Fig. 4 underscores the significant advantages bestowed by the integration \nof semantic knowledge into the fine-tuning process. This enhancement enables the model to quickly adapt to \nthe nuances of the specific sequence classification task, reaffirming the effectiveness of SK-Tuning in practical \nscenarios.\nAccelerated convergence in NLI\nIn the realm of NLI tasks, we conducted a comparative analysis pitting SK-Tuning against traditional tuning \nmethods. Our evaluation incorporated well-established datasets, including MRPC and SNLI, which consist \nof premise-hypothesis pairs and their corresponding entailment labels. The primary objective was to assess \nconvergence speed, measured in terms of training steps.\nFigure  5 visually illustrates the convergence dynamics observed during NLI tasks. Notably, the findings \nshowcased in the figure reveal the expedited convergence achieved through SK-Tuning when compared to \ntraditional tuning approaches.\nThe swift convergence depicted in Fig. 5 underscores the substantial advantages conferred by the integration \nof semantic knowledge into the fine-tuning process. This augmentation enhances the model’s adaptability, \nDataset Type Parameters Accuracy (%) F1-score (%)\nFake News Filipino\nFull Fine-tuning 100.000 97.92 94.72\nPrefix Tuning 0.03651 71.26 70.91\nPrompt Tuning 0.00169 74.12 72.27\nP-Tuning 0.01753 71.37 71.95\nLora Rank 2 0.07502 91.28 90.05\nLora Rank 4 0.17129 92.19 89.18\nSK-Tuning (Prefix) 0.00019 94.06 91.92\nSK-Tuning (Prompt) 0.00027 92.44 90.02\nEmotion\nFull Fine-tuning 100.000 93.53 89.09\nPrefix Tuning 0.03683 82.19 79.24\nPrompt Tuning 0.00736 86.17 81.77\nP-Tuning 0.01783 83.14 80.01\nLora Rank 2 0.01539 84.37 80.08\nLora Rank 4 0.01731 88.45 84.23\nSK-Tuning (Prefix) 0.00162 88.72 82.51\nSK-Tuning (Prompt) 0.00115 89.13 84.94\nSST2\nFull Fine-tuning 100.000 98.09 98.98\nPrefix Tuning 0.03673 91.20 92.28\nPrompt Tuning 0.00618 93.14 93.47\nP-Tuning 0.01764 90.76 91.15\nLora Rank 2 0.01512 92.65 93.03\nLora Rank 4 0.01726 94.53 94.67\nSK-Tuning (Prefix) 0.00080 96.97 97.07\nSK-Tuning (Prompt) 0.00031 96.93 97.14\nCola\nFull Fine-tuning 100.000 87.75 89.90\nPrefix Tuning 0.03652 72.21 80.43\nPrompt Tuning 0.00639 74.13 81.66\nP-Tuning 0.01754 71.23 79.76\nLora Rank 2 0.01505 83.44 84.65\nLora Rank 4 0.01712 85.32 86.04\nSK-Tuning (Prefix) 0.00082 85.92 86.22\nSK-Tuning (Prompt) 0.00046 85.01 86.36\nTable 5. Sequence classification results for the Mistral model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 15| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nenabling it to quickly grasp the nuances of NLI tasks and reaffirming the practical utility of SK-Tuning in \nadvancing NLI model performance.\nOur ablation study clearly demonstrates that SK-Tuning outperforms traditional prompt and prefix tuning \nmethods in terms of convergence speed across a range of downstream tasks. The incorporation of semantic \nknowledge, along with the zero-shot capabilities of LLMs, contributes to faster task adaptation. Additionally, \nSK-Tuning consistently leads to better performance, as shown in subsequent sections.\nAdapter layers\nIn this study, we investigate the impact of adapter layer complexity on the performance of fine-tuned models. \nSpecifically, we analyze how increasing the complexity of adapter layers affects various factors, including the \npercentage of parameters, computational cost, and convergence speed. We conducted experiments using the \nMistral 7B model on the SST2 dataset, and the results are presented in Table 17.\nAs shown in Table 17, increasing the number of adapter layers leads to a proportional increase in the number \nof parameters. This rise in complexity comes at the cost of increased computational resources and slower \nconvergence. While the performance of the model does show marginal improvements with more complex \nadapter layers, it is essential to note that these gains are relatively modest.\nFor instance, with just one adapter layer, the model exhibits a relatively small number of parameters, efficient \nconvergence, and high accuracy. However, as we progressively increase the complexity with additional layers, \nthe number of parameters surges significantly, computational requirements escalate, and convergence becomes \nsubstantially slower. Notably, the performance gains achieved by complex adapter layers are relatively modest.\nThe observed trend suggested that as the complexity of the adapter layers increased, the computational \ndemands and training time also increased substantially. This phenomenon can be attributed to the need for \nextensive training to capture and leverage semantic information effectively.\nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nFake News Filipino\nFull Fine-tuning 100.000 92.43 90.71\nPrefix Tuning 0.83914 66.28 66.31\nPrompt Tuning 0.14124 68.15 67.22\nP-Tuning 0.15824 67.33 66.87\nLora Rank 2 0.13741 83.35 81.46\nLora Rank 4 0.71651 86.67 84.29\nSK-Tuning (Prefix) 0.04263 89.36 88.63\nSK-Tuning (Prompt) 0.04924 88.64 87.11\nEmotion\nFull Fine-tuning 100.000 87.95 84.78\nPrefix Tuning 0.86523 77.27 76.25\nPrompt Tuning 0.14234 82.16 80.43\nP-Tuning 0.15845 77.36 75.81\nLora Rank 2 0.13748 82.67 80.25\nLora Rank 4 0.71656 85.03 82.66\nSK-Tuning (Prefix) 0.06271 82.63 80.64\nSK-Tuning (Prompt) 0.02423 85.82 83.14\nSST2\nFull Fine-tuning 100.000 94.63 94.24\nPrefix Tuning 0.83721 86.24 87.13\nPrompt Tuning 0.14231 88.19 88.04\nP-Tuning 0.15851 85.43 87.68\nLora Rank 2 0.13753 86.21 87.18\nLora Rank 4 0.71668 86.75 88.28\nSK-Tuning (Prefix) 0.02745 96.85 96.65\nSK-Tuning (Prompt) 0.01479 96.53 96.19\nCola\nFull Fine-tuning 100.000 84.23 85.13\nPrefix Tuning 0.82621 66.24 70.16\nPrompt Tuning 0.14123 67.47 70.81\nP-Tuning 0.15833 64.36 68.38\nLora Rank 2 0.13744 78.55 80.26\nLora Rank 4 0.71654 80.39 82.43\nSK-Tuning (Prefix) 0.03675 80.91 82.01\nSK-Tuning (Prompt) 0.05143 81.31 82.64\nTable 6. Sequence classification results for the Phi-2 model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 16| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nEffect of prompt and prefix text\nIn this ablation study, we investigate the influence of prompt and prefix text length on the performance of SK-\nTuning for sentiment classification using the SST-2 dataset. Our goal is to demonstrate that well-crafted prompt \nor prefix texts can outperform longer, less informative alternatives, despite the latter offering a larger number of \ntrainable parameters.\nWe conducted experiments with various prompt and prefix texts and evaluated their corresponding accuracy \non sentiment classification tasks using the Mistral model, which boasts 7 billion parameters. The table below \nsummarizes the results.\nThe results presented in Table  18 clearly illustrate that a concise and informative prompt text outperforms \nlonger and less focused alternatives. Despite the fact that longer prompts or prefixes provide more trainable \nparameters, our findings underscore the significance of crafting prompts that offer clear task instructions and \ncontext, resulting in enhanced model performance.\nFurthermore, to visualize the relationship between the prompt text and the input text, we analyzed the \nattention scores of the last layer. Specifically, we used the prompt text Classify the positive or negative sentiment \nof the text. in conjunction with input texts  I love this movie.  and I hate this movie. The figures in Fig.  6 depict \nthe attention scores, highlighting the sentimental connection between the prompt and the text. In the left figure, \nthe prompt text, particularly the word positive exhibits a strong attention score with love Conversely, in the right \nfigure, the prompt word negative shows a pronounced attention score with hate This observation suggests that \nincluding words like positive and negative in the prompt text significantly aids the model in making informed \ndecisions, thereby emphasizing the importance of crafting effective prompt texts.\nDiscussion\nWe present a comprehensive comparison of our proposed SK-Tuning method with established parameter-\nefficient fine-tuning techniques, including Prompt Tuning, Prefix Tuning, P-Tuning, LORA Rank 2, and LORA \nRank 4. Our evaluation encompassed a diverse set of downstream tasks across various domains within NLP . \nNotably, SK-Tuning for both prompts and prefixes consistently outperformed these traditional methods across \nseveral key metrics, including accuracy, F1 score, and parameter efficiency.\nOne of the key takeaways from our comparison is the remarkable performance gains achieved by SK-Tuning. \nIn terms of accuracy and F1 score, SK-Tuning consistently delivered superior results across the spectrum of \ntasks. This improvement underscores the effectiveness of leveraging semantically meaningful information in the \nfine-tuning process, as opposed to relying on arbitrary virtual tokens.\nEqually noteworthy is the efficiency of SK-Tuning. By minimizing the number of trainable parameters \nrequired for adaptation, our approach demonstrates a substantial reduction in computational resources while \nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nconll03\nFull Fine-tuning 100.000 98.53 82.47\nPrefix Tuning 0.03534 83.55 24.86\nPrompt Tuning 0.00843 85.23 28.73\nP-Tuning 0.01583 83.22 26.34\nLora Rank 2 0.01403 91.12 68.24\nLora Rank 4 0.06795 93.23 71.33\nSK-Tuning (Prefix) 0.00071 94.08 71.59\nSK-Tuning (Prompt) 0.00052 94.11 71.60\nNCBI disease\nFull Fine-tuning 100.000 98.53 92.46\nPrefix Tuning 0.03492 89.09 60.06\nPrompt Tuning 0.00742 91.17 75.34\nP-Tuning 0.01572 90.22 81.23\nLora Rank 2 0.01417 92.86 80.00\nLora Rank 4 0.06797 96.12 83.49\nSK-Tuning (Prefix) 0.00093 96.17 84.85\nSK-Tuning (Prompt) 0.00068 95.32 82.18\nWikiAnn\nFull Fine-tuning 100.000 90.50 60.14\nPrefix Tuning 0.03527 71.67 22.18\nPrompt Tuning 0.00732 76.23 31.78\nP-Tuning 0.01577 70.65 24.33\nLora Rank 2 0.01408 82.23 41.23\nLora Rank 4 0.06791 85.13 45.14\nSK-Tuning (Prefix) 0.00083 83.19 42.13\nSK-Tuning (Prompt) 0.00044 82.59 42.01\nTable 7. Token classification results for the Bloom model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 17| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nmaintaining or even enhancing task performance. This efficiency is particularly crucial in practical applications, \nwhere resource constraints often play a significant role.\nAnother noteworthy aspect of our study is the extensive evaluation across five different pretrained LLMs: \nBloom (7B), Falcon (7B), LLAMA2 (7B), Mistral (7B), and Phi2 (2.7B). Our results consistently indicate that \nSK-Tuning is a robust and versatile technique that can be applied to various LLM architectures, demonstrating \nits broad applicability and effectiveness across different model sizes and complexities.\nLimitations\nWhile SK-Tuning offers significant advantages in terms of performance and parameter efficiency, there are \nseveral key limitations that should be considered:\nTraining and inference time overhead\nOne of the primary limitations of SK-Tuning is the potential increase in inference or training time. Since it \nutilizes the pre-trained LLM twice during the forward pass: once to obtain semantic information from the \nprompt or prefix and again for processing the input data to get output. This dual usage of the LLM can lead to \nlonger training and inference time.\nDependency on pretrained models\nSK-Tuning relies heavily on the quality and capabilities of the underlying pretrained LLM. The success of prompt \nor prefix text tuning is linked to the zero-shot capabilities of the LLM. If the pretrained model does not have \na strong grasp of semantic knowledge or lacks certain linguistic skills, the effectiveness of SK-Tuning could be \nreduced. It needs significant training to accurately understand the semantic meaning of the prompt or prefix \ntext.\nSemantic knowledge acquisition\nThe effectiveness of SK-Tuning depends on using prompts or prefixes that are meaningful. The more relevant \nthe prompt is to the task, the better the performance, described in section “ Effect of prompt and prefix text ”.  \nHowever, creating or finding these meaningful prompts can be difficult and might require specific knowledge \nabout the domain. This challenge could limit how useful SK-Tuning is for certain tasks or datasets.\nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nconll03\nFull Fine-tuning 100.000 98.75 80.77\nPrefix Tuning 0.03964 82.28 66.56\nPrompt Tuning 0.00638 86.65 69.91\nP-Tuning 0.01731 80.11 65.11\nLora Rank 2 0.01426 88.67 63.34\nLora Rank 4 0.07122 91.32 69.03\nSK-Tuning (Prefix) 0.00044 93.63 70.83\nSK-Tuning (Prompt) 0.00063 93.02 70.19\nNCBI disease\nFull Fine-tuning 100.000 98.32 93.38\nPrefix Tuning 0.03976 88.23 68.23\nPrompt Tuning 0.00712 91.22 78.24\nP-Tuning 0.01733 90.15 77.23\nLora Rank 2 0.01424 92.48 80.18\nLora Rank 4 0.07125 95.34 82.87\nSK-Tuning (Prefix) 0.00083 96.22 84.73\nSK-Tuning (Prompt) 0.00061 96.28 84.89\nWikiAnn\nFull Fine-tuning 100.000 91.49 63.21\nPrefix Tuning 0.03986 81.15 35.17\nPrompt Tuning 0.00712 83.23 44.19\nP-Tuning 0.01743 81.29 38.11\nLora Rank 2 0.01434 84.82 47.90\nLora Rank 4 0.07125 86.56 49.39\nSK-Tuning (Prefix) 0.00082 86.99 50.61\nSK-Tuning (Prompt) 0.00052 86.69 49.58\nTable 8. Token classification results for the Llama2 model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 18| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nTuning hyperparameters\nLike other fine-tuning approaches, SK-Tuning involves hyperparameter tuning, including the design of the \nadapter architecture, the choice of semantic knowledge text, and the adjustment of task-specific modules. \nIdentifying the optimal hyperparameters can be a time-consuming and computationally intensive process.\nConclusion\nIn conclusion, our work introduces SK-Tuning as a pioneering approach to fine-tuning LLMs for specific \ndownstream tasks, with a strong emphasis on parameter efficiency. We have shown that traditional methods, \nrelying on learnable virtual tokens in adapters while keeping the LLM’s core parameters frozen, often fall short \nin terms of both efficiency and performance.\nSK-Tuning, on the other hand, revolutionizes the fine-tuning process by replacing arbitrary virtual tokens \nwith real, semantically meaningful prefixes. This innovation allows LLMs to tap into their intrinsic semantic \nknowledge, significantly reducing the need for extensive training iterations. Our experimental results across a \nFig. 2. Comparison of memory efficiency (left) and training efficiency (right) across various PEFT methods. \nS-Prefix and S-Prompt represent SK-Tuning applied to prefix tuning and prompt tuning, respectively. The left \nchart shows the memory cost in GB, highlighting the model weights and optimizations, while the right chart \ndisplays the percentage of parameters, total training time in hours, and iteration time per second.\n \nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nconll03\nFull Fine-tuning 100.000 97.82 79.03\nPrefix Tuning 0.03772 90.57 67.62\nPrompt Tuning 0.00832 91.26 70.15\nP-Tuning 0.01762 89.23 66.02\nLora Rank 2 0.01942 90.21 68.96\nLora Rank 4 0.09752 93.25 71.19\nSK-Tuning (Prefix) 0.00071 94.21 71.73\nSK-Tuning (Prompt) 0.00055 94.82 72.02\nNCBI disease\nFull Fine-tuning 100.000 97.93 90.88\nPrefix Tuning 0.03763 89.23 69.33\nPrompt Tuning 0.00721 92.05 82.28\nP-Tuning 0.01752 88.15 70.36\nLora Rank 2 0.01936 90.55 80.25\nLora Rank 4 0.09754 94.41 83.19\nSK-Tuning (Prefix) 0.00085 95.83 82.18\nSK-Tuning (Prompt) 0.00056 96.01 84.48\nWikiAnn\nFull Fine-tuning 100.000 89.23 62.09\nPrefix Tuning 0.03772 82.67 36.55\nPrompt Tuning 0.00836 83.33 43.32\nP-Tuning 0.01768 81.14 35.21\nLora Rank 2 0.01983 80.47 41.58\nLora Rank 4 0.09752 86.61 48.03\nSK-Tuning (Prefix) 0.00063 82.99 42.51\nSK-Tuning (Prompt) 0.00044 82.96 42.49\nTable 9. Token classification results for the Falcon model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 19| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nrange of downstream tasks, including sequence classification, token classification, and NLI, provide compelling \nevidence that SK-Tuning outperforms traditional approaches. Notably, this improvement is achieved with a \nreduced number of trainable parameters, emphasizing the efficiency of our method.\nBy prioritizing parameter efficiency and harnessing the latent semantic understanding of LLMs, SK-Tuning \nopens up new possibilities for efficient model adaptation across various real-world applications. We believe that \nour approach holds great promise for advancing the field of NLP , offering researchers and practitioners a valuable \ntool for achieving enhanced task performance while optimizing computational resources. As LLMs continue to \nplay a pivotal role in NLP , SK-Tuning represents a significant step forward in harnessing their full potential.\nFig. 3. Convergence comparison for token classification.\n \nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nconll03\nFull Fine-tuning 100.000 98.89 84.60\nPrefix Tuning 0.03634 83.31 58.54\nPrompt Tuning 0.00741 87.77 62.19\nP-Tuning 0.01743 81.15 67.59\nLora Rank 2 0.01494 88.32 68.14\nLora Rank 4 0.08694 92.05 70.66\nSK-Tuning (Prefix) 0.00062 95.29 72.70\nSK-Tuning (Prompt) 0.00044 95.89 72.03\nNCBI disease\nFull Fine-tuning 100.000 98.52 93.39\nPrefix Tuning 0.03627 88.49 74.25\nPrompt Tuning 0.00696 92.03 80.11\nP-Tuning 0.01735 87.13 63.29\nLora Rank 2 0.01483 94.58 82.37\nLora Rank 4 0.08698 96.88 83.15\nSK-Tuning (Prefix) 0.00081 96.98 85.06\nSK-Tuning (Prompt) 0.00053 97.01 85.15\nWikiAnn\nFull Fine-tuning 100.000 92.15 63.09\nPrefix Tuning 0.03633 81.91 36.03\nPrompt Tuning 0.00752 84.48 45.31\nP-Tuning 0.01733 81.04 35.02\nLora Rank 2 0.01495 82.08 42.22\nLora Rank 4 0.08692 85.33 45.95\nSK-Tuning (Prefix) 0.00095 86.73 46.21\nSK-Tuning (Prompt) 0.00052 85.99 45.52\nTable 10. Token classification results for the Mistral model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 20| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nFig. 4. Convergence comparison for sequence classification.\n \nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nconll03\nFull Fine-tuning 100.000 98.13 79.02\nPrefix Tuning 0.83844 78.27 56.63\nPrompt Tuning 0.14124 80.38 58.27\nP-Tuning 0.15814 76.54 56.07\nLora Rank 2 0.13746 84.44 62.15\nLora Rank 4 0.71649 86.56 65.43\nSK-Tuning (Prefix) 0.00951 90.42 71.73\nSK-Tuning (Prompt) 0.00838 91.15 71.98\nNCBI disease\nFull Fine-tuning 100.000 95.82 91.19\nPrefix Tuning 0.83823 82.42 63.38\nPrompt Tuning 0.13939 85.61 65.44\nP-Tuning 0.15794 81.17 67.63\nLora Rank 2 0.13748 86.23 78.45\nLora Rank 4 0.71493 87.34 78.26\nSK-Tuning (Prefix) 0.00994 89.22 80.63\nSK-Tuning (Prompt) 0.00438 90.82 81.95\nWikiAnn\nFull Fine-tuning 100.000 88.92 58.21\nPrefix Tuning 0.83832 74.37 31.57\nPrompt Tuning 0.01416 78.86 38.32\nP-Tuning 0.15812 75.23 32.26\nLora Rank 2 0.13748 79.04 39.88\nLora Rank 4 0.71649 81.53 44.47\nSK-Tuning (Prefix) 0.00851 82.03 42.93\nSK-Tuning (Prompt) 0.00693 83.18 43.06\nTable 11. Token classification results for the Phi-2 model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 21| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nFig. 5. Convergence comparison for sequence classification.\n \nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nRTE\nFull Fine-tuning 100.000 92.31 87.19\nPrefix Tuning 0.03493 70.03 64.06\nPrompt Tuning 0.00714 65.34 62.20\nP-Tuning 0.01584 71.11 69.23\nLora Rank 2 0.01402 80.25 80.01\nLora Rank 4 0.05804 84.45 83.26\nSK-Tuning (Prefix) 0.00076 83.88 82.76\nSK-Tuning (Prompt) 0.00053 84.92 83.87\nMRPC\nFull Fine-tuning 100.000 90.01 91.13\nPrefix Tuning 0.03494 73.56 81.70\nPrompt Tuning 0.00773 81.39 86.01\nP-Tuning 0.01562 78.08 84.38\nLora Rank 2 0.01393 80.21 82.29\nLora Rank 4 0.05799 83.88 84.84\nSK-Tuning (Prefix) 0.00082 88.99 86.28\nSK-Tuning (Prompt) 0.00054 89.03 86.37\nSNLI\nFull Fine-tuning 100.000 95.62 95.78\nPrefix Tuning 0.03492 87.32 87.26\nPrompt Tuning 0.00803 88.88 88.87\nP-Tuning 0.01594 86.22 86.54\nLora Rank 2 0.01412 91.37 91.36\nLora Rank 4 0.05813 93.23 93.68\nSK-Tuning (Prefix) 0.00085 92.54 92.98\nSK-Tuning (Prompt) 0.00060 93.75 94.02\nTable 12. Entailment classification results for the Bloom model. The best results are highlighted in bold, and \nthe second-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 22| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nRTE\nFull Fine-tuning 100.000 93.51 88.92\nPrefix Tuning 0.03982 70.15 65.23\nPrompt Tuning 0.00737 62.81 66.00\nP-Tuning 0.01753 67.24 66.21\nLora Rank 2 0.01612 81.04 80.67\nLora Rank 4 0.03224 83.43 81.44\nSK-Tuning (Prefix) 0.00077 85.73 84.01\nSK-Tuning (Prompt) 0.00052 83.72 83.43\nMRPC\nFull Fine-tuning 100.000 92.25 92.95\nPrefix Tuning 0.03973 79.41 80.01\nPrompt Tuning 0.00724 80.18 80.37\nP-Tuning 0.01745 74.56 82.67\nLora Rank 2 0.01601 80.48 82.02\nLora Rank 4 0.03218 81.89 83.11\nSK-Tuning (Prefix) 0.00082 85.97 86.37\nSK-Tuning (Prompt) 0.00051 85.03 85.37\nSNLI\nFull Fine-tuning 100.000 93.31 94.03\nPrefix Tuning 0.03986 86.34 86.33\nPrompt Tuning 0.00736 87.02 87.41\nP-Tuning 0.01752 85.17 86.27\nLora Rank 2 0.01613 90.21 90.87\nLora Rank 4 0.03228 91.15 91.85\nSK-Tuning (Prefix) 0.00095 91.43 91.98\nSK-Tuning (Prompt) 0.00069 90.97 91.04\nTable 13. Entailment classification results for the Llama2 model. The best results are highlighted in bold, and \nthe second-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 23| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nFig. 6. Attentional Insights—exploring the Sentimental Connection between Prompt Text and Input Text. \nThe left side of the figure reveals the attention scores between the prompt text, particularly the word ‘positive, ’ \nand the input text ‘I love this movie. ’ On the right side, the attention scores depict the relationship between the \nprompt word ‘negative’ and the input text ’I hate this movie. ’ These attention patterns shed light on how well-\ncrafted prompt texts enhance the model’s decision-making process.\n \nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nRTE\nFull Fine-tuning 100.000 93.22 87.67\nPrefix Tuning 0.03822 64.23 63.38\nPrompt Tuning 0.00813 66.51 66.02\nP-Tuning 0.01794 53.42 53.09\nLora Rank 2 0.01138 73.28 70.15\nLora Rank 4 0.01774 78.33 73.42\nSK-Tuning (Prefix) 0.00084 80.12 79.73\nSK-Tuning (Prompt) 0.00065 80.25 79.78\nMRPC\nFull Fine-tuning 100.000 90.21 90.83\nPrefix Tuning 0.03813 74.13 78.22\nPrompt Tuning 0.00715 80.04 80.19\nP-Tuning 0.01783 80.43 79.59\nLora Rank 2 0.00983 80.82 82.21\nLora Rank 4 0.01763 82.52 83.01\nSK-Tuning (Prefix) 0.00079 82.68 83.68\nSK-Tuning (Prompt) 0.00054 83.03 85.37\nSNLI\nFull Fine-tuning 100.000 92.53 92.97\nPrefix Tuning 0.03822 84.33 84.98\nPrompt Tuning 0.00843 86.13 86.93\nP-Tuning 0.01782 83.31 83.66\nLora Rank 2 0.01163 87.05 87.29\nLora Rank 4 0.06773 89.21 89.88\nSK-Tuning (Prefix) 0.00072 90.86 91.00\nSK-Tuning (Prompt) 0.00053 90.86 91.00\nTable 14. Entailment classification results for the Falcon model. The best results are highlighted in bold, and \nthe second-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 24| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nRTE\nFull Fine-tuning 100.000 94.67 89.82\nPrefix Tuning 0.03663 76.22 74.45\nPrompt Tuning 0.00732 80.34 80.17\nP-Tuning 0.01778 75.12 75.86\nLora Rank 2 0.01521 83.39 82.25\nLora Rank 4 0.06739 85.65 83.12\nSK-Tuning (Prefix) 0.00083 84.73 83.87\nSK-Tuning (Prompt) 0.00065 85.94 84.67\nMRPC\nFull Fine-tuning 100.000 93.02 94.21\nPrefix Tuning 0.03654 75.28 77.03\nPrompt Tuning 0.00722 80.34 82.17\nP-Tuning 0.01715 76.19 80.31\nLora Rank 2 0.01513 82.83 83.41\nLora Rank 4 0.06724 86.47 87.02\nSK-Tuning (Prefix) 0.00082 85.63 85.17\nSK-Tuning (Prompt) 0.00055 86.31 87.98\nSNLI\nFull Fine-tuning 100.000 94.21 95.32\nPrefix Tuning 0.03666 85.55 85.78\nPrompt Tuning 0.00744 86.35 86.21\nP-Tuning 0.01774 85.37 86.05\nLora Rank 2 0.01524 84.12 84.76\nLora Rank 4 0.06736 89.11 89.77\nSK-Tuning (Prefix) 0.00089 91.62 91.31\nSK-Tuning (Prompt) 0.00068 92.56 91.86\nTable 15. Entailment classification results for the Mistral model. The best results are highlighted in bold, and \nthe second-best result is italics for clarity except full fine-tuning.\n \nScientific Reports |        (2024) 14:30667 25| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nData availibility\nData information and URL are provided within the manuscript.\nDataset Type Parameters (%) Accuracy (%) F1-score (%)\nRTE\nFull Fine-tuning 100.000 90.37 85.74\nPrefix Tuning 0.83872 59.54 58.27\nPrompt Tuning 0.14234 61.18 61.84\nP-Tuning 0.15834 58.61 56.38\nLora Rank 2 0.13746 66.52 65.82\nLora Rank 4 0.71658 72.25 70.45\nSK-Tuning (Prefix) 0.00424 76.44 75.92\nSK-Tuning (Prompt) 0.00252 76.53 76.11\nMRPC\nFull Fine-tuning 100.000 89.31 90.21\nPrefix Tuning 0.83822 71.15 72.78\nPrompt Tuning 0.14345 73.16 75.28\nP-Tuning 0.15842 70.48 71.21\nLora Rank 2 0.13747 80.53 81.33\nLora Rank 4 0.71659 83.19 84.23\nSK-Tuning (Prefix) 0.00742 83.63 84.72\nSK-Tuning (Prompt) 0.00349 82.54 83.42\nSNLI\nFull Fine-tuning 100.00 90.54 91.02\nPrefix Tuning 0.83844 79.27 79.82\nPrompt Tuning 0.14149 81.30 81.80\nP-Tuning 0.15823 78.56 77.96\nLora Rank 2 0.13745 82.45 82.67\nLora Rank 4 0.71656 84.36 84.89\nSK-Tuning (Prefix) 0.00609 89.21 90.51\nSK-Tuning (Prompt) 0.00589 88.62 88.95\nTable 16. Entailment classification results for the Phi-2 model. The best results are highlighted in bold, and the \nsecond-best result is italics for clarity except full fine-tuning.\n \nPrompt / Prefix (Text) Prefix Prompt Token length\n“Classify the sentiment of the following text” 93.41 92.83 9\n“I have a piece of text and I need to understand its emotional tone. Could you classify the sentiment of the text:” 96.04 95.41 29\n“Considering the context and tone, can you classify the sentiment of the following text? Here’s the text” 95.13 94.82 24\n“Focus on the emotional cues present in the text:” 92.93 93.09 12\n“Let’s analyze the sentiment of this text together. I’ll provide the text, and you classify the sentiment of the text. Here’s the text:” 94.78 94.17 35\n“Classify the positive or negative sentiment of the text:” 96.83 96.52 11\nTable 18. Effect of prompt and prefix length on sentiment classification accuracy. Significant values are in \n[bold, italics].\n \nNumber of Layers Parameters Accuracy (%) F1-score (%) Convergence steps\n1 0.00031 96.93 97.14 1500\n3 0.02236 95.32 97.02 7900\n5 1.18583 96.98 97.25 16600\n7 3.46256 97.21 97.54 23900\n9 6.28583 97.16 97.37 53200\n11 9.89372 97.29 97.87 79400\nTable 17. Exploring the Trade-offs—adapter complexity vs. performance.\n \nScientific Reports |        (2024) 14:30667 26| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\nReceived: 18 February 2024; Accepted: 7 October 2024\nReferences\n 1. Zhu, Y . et al. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).\n 2. Greco, C. M. & Tagarelli, A. Bringing order into the realm of transformer-based language models for artificial intelligence and law. \nArtif. Intell. Law 2023,  1–148 (2023).\n 3. Brown, T. B. et al. Language models are few-shot learners (2020). arXiv: 2005.14165.\n 4. McIntosh, T. R., Susnjak, T., Liu, T., Watters, P . & Halgamuge, M. N. From google gemini to openai q*(q-star): A survey of reshaping \nthe generative artificial intelligence (ai) research landscape. arXiv preprint arXiv:2312.10868 (2023).\n 5. Raiaan, M. A. K. et al. A review on large language models: Architectures, applications, taxonomies, open issues and challenges. \nAuthorea Preprints (2023).\n 6. Liu, H. et al. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning (2022). arXiv: 2205.05638.\n 7. Houlsby, N. et al. Parameter-efficient transfer learning for NLP . In   Proceedings of the 36th International Conference on Machine \nLearning, vol. 97 of Proceedings of Machine Learning Research (eds. Chaudhuri, K. & Salakhutdinov, R.) 2790–2799 (PMLR, 2019).\n 8. Lester, B., Al-Rfou, R. & Constant, N. The power of scale for parameter-efficient prompt tuning (2021). arXiv: 2104.08691.\n 9. Li, X. L. & Liang, P . Prefix-tuning: Optimizing continuous prompts for generation (2021). arXiv: 2101.00190.\n 10. Huang, W ., Abbeel, P ., Pathak, D. & Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for \nembodied agents. In International Conference on Machine Learning 9118–9147 (PMLR, 2022).\n 11. Xin, Y . et al. Parameter-efficient fine-tuning for pre-trained vision models: A survey. arXiv preprint arXiv:2402.02242 (2024).\n 12. Liu, H. et al. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Adv. Neural Inf. Process. Syst. \n35, 1950–1965 (2022).\n 13. Nguyen, M., Kishan, K., Nguyen, T., Chadha, A. & Vu, T. Efficient fine-tuning large language models for knowledge-aware response \nplanning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases 593–611 (Springer, 2023).\n 14. Hernández, A., Ortega-Mendoza, R. M., Villatoro-Tello, E., Camacho-Bello, C. J. & Pérez-Cortés, O. Natural language understanding \nfor navigation of service robots in low-resource domains and languages: Scenarios in spanish and nahuatl. Mathematics 12, 1136 \n(2024).\n 15. Chow, K., Tang, Y ., Lyu, Z., Rajput, A. & Ban, K. Performance optimization in the llm world 2024. In Companion of the 15th ACM/\nSPEC International Conference on Performance Engineering 156–157 (2024).\n 16. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. & Neubig, G. Towards a unified view of parameter-efficient transfer learning. arXiv \npreprint arXiv:2110.04366 (2021).\n 17. Liu, X. et al. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint \narXiv:2110.07602 (2021).\n 18. Liu, X. et al. Gpt understands, too. AI Open (2023).\n 19. Zhang, Q. et al. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on \nLearning Representations (2023).\n 20. Hu, E. J. et al. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\n 21. Li, X. L. & Liang, P . Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).\n 22. Zaken, E. B., Ravfogel, S. & Goldberg, Y . Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199 (2021).\n 23. Ding, H., Zhao, X., Abdullah, S. N., Dewi, D. A. & Jiang, Z. Dynamic adaptive optimization for effective sentiment analysis fine-\ntuning on large language models. arXiv preprint arXiv:2408.11856 (2024).\n 24. Razdaibiedina, A. et al. Residual prompt tuning: Improving prompt tuning with residual reparameterization. arXiv preprint \narXiv:2305.03937 (2023).\n 25. Razdaibiedina, A. et al. Progressive prompts: Continual learning for language models. arXiv preprint arXiv:2301.12314 (2023).\n 26. Y ang, Y . et al. Recent advances of foundation language models-based continual learning: A survey. arXiv preprint arXiv:2405.18653 \n(2024).\n 27. Y ang, X. et al. Dynamic prompting: A unified framework for prompt tuning. arXiv preprint arXiv:2303.02909 (2023).\n 28. Zhao, H., He, R., Xiao, M. & Xu, J. Infusing hierarchical guidance into prompt tuning: A parameter-efficient framework for multi-\nlevel implicit discourse relation recognition. arXiv preprint arXiv:2402.15080 (2024).\n 29. Chen, L., Chou, H. & Zhu, X. Developing prefix-tuning models for hierarchical text classification. In Proceedings of the 2022 \nConference on Empirical Methods in Natural Language Processing: Industry Track 390–397 (2022).\n 30. Liu, X., Huang, H., Shi, G. & Wang, B. Dynamic prefix-tuning for generative template-based event extraction. arXiv preprint \narXiv:2205.06166 (2022).\n 31. Y ang, L. et al. Mixpave: Mix-prompt tuning for few-shot product attribute value extraction. In Findings of the Association for \nComputational Linguistics: ACL 9978–9991 (2023).\n 32. Han, C. et al. E  ˆ, 2vpt: An effective and efficient approach for visual prompt tuning. arXiv preprint arXiv:2307.13770 (2023).\n 33. Renduchintala, A., Konuk, T. & Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint \narXiv:2311.09578 (2023).\n 34. Sheng, Y . et al. S-lora: Serving thousands of concurrent lora adapters. arXiv preprint arXiv:2311.03285 (2023).\n 35. Xia, W ., Qin, C. & Hazan, E. Chain of lora: Efficient fine-tuning of language models via residual learning. arXiv preprint \narXiv:2401.04151 (2024).\n 36. Wang, Y ., Lin, Y ., Zeng, X. & Zhang, G. Multilora: Democratizing lora for better multi-task learning. arXiv preprint arXiv:2311.11501 \n(2023).\n 37. Dettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Adv. Neural Inf. Process. \nSyst. 36, 56 (2024).\n 38. Lialin, V ., Muckatira, S., Shivagunde, N. & Rumshisky, A. Relora: High-rank training through low-rank updates. In Workshop on \nAdvancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)  \n(2023).\n 39. Edalati, A. et al. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650 (2022).\n 40. Shi, S. et al. Reslora: Identity residual mapping in low-rank adaption. arXiv preprint arXiv:2402.18039 (2024).\n 41. Hyeon-Woo, N., Y e-Bin, M. & Oh, T.-H. Fedpara: Low-rank hadamard product for communication-efficient federated learning. \narXiv preprint arXiv:2108.06098 (2021).\n 42. Qiu, Z. et al. Controlling text-to-image diffusion by orthogonal finetuning (2024). arXiv: 2306.07280.\n 43. Liu, W . et al. Parameter-efficient orthogonal finetuning via butterfly factorization (2024). arXiv: 2311.06243.\n 44. Larsen, B. W ., Fort, S., Becker, N. & Ganguli, S. How many degrees of freedom do we need to train deep networks: a loss landscape \nperspective. arXiv preprint arXiv:2107.05802 (2021).\n 45. Gur-Ari, G., Roberts, D. A. & Dyer, E. Gradient descent happens in a tiny subspace. arXiv preprint arXiv:1812.04754 (2018).\n 46. Lee, Y . & Choi, S. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on \nMachine Learning 2927–2936 (PMLR, 2018).\nScientific Reports |        (2024) 14:30667 27| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\n 47. Chaudhry, A., Khan, N., Dokania, P . & Torr, P . Continual learning in low-rank orthogonal subspaces. Adv. Neural Inf. Process. Syst. \n33, 9900–9911 (2020).\n 48. Finn, C., Abbeel, P . & Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on \nMachine Learning 1126–1135 (PMLR, 2017).\n 49. Frikha, A., Krompaß, D., Köpken, H.-G. & Tresp, V . Few-shot one-class classification via meta-learning. In Proceedings of the AAAI \nConference on Artificial Intelligence, vol. 35 7448–7456 (2021).\n 50. Nunez, E. et al. Lcs: Learning compressible subspaces for efficient, adaptive, real-time network compression at inference time. In \nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 3818–3827 (2023).\n 51. Pham, H., Guan, M., Zoph, B., Le, Q. & Dean, J. Efficient neural architecture search via parameters sharing. In International \nconference on machine learning 4095–4104 (PMLR, 2018).\n 52. Chen, Y . et al. Automatic subspace evoking for efficient neural architecture search. arXiv preprint arXiv:2210.17180 (2022).\n 53. Ren, P . et al. A comprehensive survey of neural architecture search: Challenges and solutions. ACM Comput. Surv. (CSUR)  54, \n1–34 (2021).\n 54. Rajeswaran, A., Finn, C., Kakade, S. M. & Levine, S. Meta-learning with implicit gradients. Adv. Neural Inf. Process. Syst. 32, 859 \n(2019).\n 55. Zhao, J. et al. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507 (2024).\n 56. Chen, Y . & Wainwright, M.  J. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic \nguarantees. arXiv preprint arXiv:1509.03025 (2015).\n 57. Chen, H., Raskutti, G. & Yuan, M. Non-convex projected gradient descent for generalized low-rank tensor regression. J. Mach. \nLearn. Res. 20, 1–37 (2019).\n 58. Zhang, T. & Fan, X. Projected gradient descent algorithm for low-rank matrix estimation. arXiv preprint arXiv:2403.02704 (2024).\n 59. Mądry, A., Makelov, A., Schmidt, L., Tsipras, D. & Vladu, A. Towards deep learning models resistant to adversarial attacks. stat \n1050 (2017).\n 60. Croce, F . & Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In \nInternational Conference on Machine Learning 2206–2216 (PMLR, 2020).\n 61. Wong, E., Rice, L. & Kolter, J. Z. Fast is better than free: Revisiting adversarial training. arXiv preprint arXiv:2001.03994 (2020).\n 62. Shazeer, N. & Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine \nLearning 4596–4604 (PMLR, 2018).\n 63. Li, B., Chen, J. & Zhu, J. Memory efficient optimizers with 4-bit states. Adv. Neural Inf. Process. Syst. 36, 56 (2024).\n 64. Deshpande, A. et al. Spartan: sparse hierarchical memory for parameter-efficient transformers. arXiv preprint arXiv:2211.16634 \n(2022).\n 65. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library (2019). arXiv: 1912.01703.\n 66. Wolf, T. et al. Huggingface’s transformers: State-of-the-art natural language processing (2020). arXiv: 1910.03771.\n 67. Wang, A. et al. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint \narXiv:1804.07461 (2018).\n 68. Liu, Y . et al. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).\n 69. Warstadt, A., Singh, A. & Bowman, S. R. Neural network acceptability judgments (2019). arXiv: 1805.12471.\n 70. Socher, R. et al. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 \nConference on Empirical Methods in Natural Language Processing   (eds. Y arowsky, D. et al.)  1631–1642 (Association for \nComputational Linguistics, 2013).\n 71. Saravia, E., Liu, H.-C. T., Huang, Y .-H., Wu, J. & Chen, Y .-S. CARER: Contextualized affect representations for emotion recognition. \nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing   (eds. Riloff, E., et al.)  3687–3697  \n(Association for Computational Linguistics, 2018). https://doi.org/10.18653/v1/D18-1404.\n 72. Cruz, J. C. B., Tan, J. A. & Cheng, C. Localization of fake news detection via multitask transfer learning. In Proceedings of the \nTwelfth Language Resources and Evaluation Conference    (eds. Calzolari, N. et al.)  2596–2604 (European Language Resources \nAssociation, 2020).\n 73. Tjong Kim Sang, E. F . & De Meulder, F . Introduction to the CoNLL-2003 shared task. Language-independent named entity \nrecognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 142–147 (2003).\n 74. Zhao, S., Liu, T., Zhao, S. & Wang, F . A neural multi-task learning framework to jointly model medical named entity recognition \nand normalization (2018). arXiv: 1812.06081.\n 75. Pan, X. et al. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association \nfor Computational Linguistics (Volume 1: Long Papers)   (eds. Barzilay, R. & Kan, M.-Y .) 1946–1958 (Association for Computational \nLinguistics,  2017).  https://doi.org/10.18653/v1/P17-1178 .\n 76. Dagan, I., Glickman, O. & Magnini, B. The pascal recognising textual entailment challenge. In Machine Learning Challenges. \nEvaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment (eds. Quiñonero-Candela, J. et \nal.)  177–190 (Springer, 2006).\n 77. Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L. & Giampiccolo, D. The second pascal recognising textual entailment challenge. In \nProceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment (2006).\n 78. Giampiccolo, D., Magnini, B., Dagan, I. & Dolan, W .  B. The third pascal recognizing textual entailment challenge. In ACL-\nPASCAL@ACL (2007).\n 79. Bentivogli, L., Clark, P ., Dagan, I. & Giampiccolo, D. The fifth pascal recognizing textual entailment challenge. TAC 7, 8 (2009).\n 80. Dolan, W . B. & Brockett, C. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International \nWorkshop on Paraphrasing (IWP2005) (2005).\n 81. Bowman, S. R., Angeli, G., Potts, C. & Manning, C. D. A large annotated corpus for learning natural language inference (2015). \narXiv: 1508.05326.\n 82. Workshop, B. et al. Bloom: A 176b-parameter open-access multilingual language model (2023). arXiv: 2211.05100.\n 83. Geng, X.  & Liu, H An Open Reproduction of llama (Openllama, 2023).\n 84. Jiang, A. Q. et al. Mistral 7b (2023). arXiv: 2310.06825.\n 85. Penedo, G. et al. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only (2023). \narXiv: 2306.01116.\n 86. Li, Y . et al. Textbooks are all you need ii: phi-1.5 technical report (2023). arXiv: 2309.05463.\n 87. Gunasekar, S. et al. Textbooks are all you need (2023). arXiv: 2306.11644.\n 88. Howard, J. & Ruder, S. Universal language model fine-tuning for text classification (2018). arXiv: 1801.06146.\n 89. Li, X. L. & Liang, P . Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the \nAssociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: \nLong Papers) (eds. Zong, C. et al.) 4582–4597  (Association for Computational Linguistics, 2021).  h t t p s : / / d o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / 2 0 2 \n1 . a c l - l o n g . 3 5 3     .   \n 90. Lester, B., Al-Rfou, R. & Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing  (eds. Moens, M.-F . et al.)  3045–3059  (Association for Computational \nLinguistics, 2021). https://doi.org/10.18653/v1/2021.emnlp-main.243\nScientific Reports |        (2024) 14:30667 28| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/\n 91. Liu, X. et al. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual \nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers) (eds. Muresan S. et al.) 61–68.  h t t p s : / / d o i . o r g / 1 0 \n. 1 8 6 5 3 / v 1 / 2 0 2 2 . a c l - s h o r t . 8     (Association for Computational Linguistics,  2022).\n 92. Loshchilov, I. & Hutter, F . Decoupled weight decay regularization (2019). arXiv: 1711.05101.\n 93. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y . & Iwasawa, Y . Large language models are zero-shot reasoners. Adv. Neural Inf. Process. \nSyst. 35, 22199–22213 (2022).\nAuthor contributions\nConceptualization, M.K.; Methodology, N.J, M.K., A.M., and M.S.I.S; software, N.J, M.K., A.M., M.S.I.S, and N.J; \nformal analysis, M.K, P .B; investigation, A.M., M.S.I.S, M.K. and N.J. resources P .B, J.X. and A.M., data collec-\ntion M.S.I.S, A.M, and M.K, writing original draft preparation M.K., A.M., M.S.I.S, and N.J; writing review and \nediting, P .B, N.Y , O.O.G, and M.K.; visualization M.K., A.M., M.S.I.S, and N.J; supervision, N.Y , O.O.G, and P .B. \nAll authors reviewed the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to N.J.P .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\nThis is a U.S. Government work and not under copyright protection in the US; foreign copyright protection \nmay apply 2024 \nScientific Reports |        (2024) 14:30667 29| https://doi.org/10.1038/s41598-024-75599-4\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8590635061264038
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5758118629455566
    },
    {
      "name": "Prefix",
      "score": 0.571208655834198
    },
    {
      "name": "Language model",
      "score": 0.5396454930305481
    },
    {
      "name": "Process (computing)",
      "score": 0.5387398600578308
    },
    {
      "name": "Popularity",
      "score": 0.514717161655426
    },
    {
      "name": "Natural language processing",
      "score": 0.49129289388656616
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47950416803359985
    },
    {
      "name": "Semantic memory",
      "score": 0.4500848650932312
    },
    {
      "name": "Meaning (existential)",
      "score": 0.4220200181007385
    },
    {
      "name": "Machine learning",
      "score": 0.39035555720329285
    },
    {
      "name": "Linguistics",
      "score": 0.07497701048851013
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Cognition",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I106165777",
      "name": "University of Central Florida",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I315729180",
      "name": "Noakhali Science and Technology University",
      "country": "BD"
    },
    {
      "id": "https://openalex.org/I197842652",
      "name": "Hajee Mohammad Danesh Science and Technology University",
      "country": "BD"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    }
  ]
}