{
  "title": "Are Pretrained Language Models Symbolic Reasoners over Knowledge?",
  "url": "https://openalex.org/W3091922147",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2027506410",
      "name": "Nora Kassner",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A3092144790",
      "name": "Benno Krojer",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2035156685",
      "name": "Hinrich Schütze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2962790689",
    "https://openalex.org/W3035097102",
    "https://openalex.org/W2962727366",
    "https://openalex.org/W2997897037",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2909137510",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3137695714",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2121001250",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W1970575036",
    "https://openalex.org/W3017374003",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W2268733724",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2969809624",
    "https://openalex.org/W3035718362"
  ],
  "abstract": "How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.",
  "full_text": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 552–564\nOnline, November 19-20, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n552\nAre Pretrained Language Models Symbolic Reasoners Over Knowledge?\nNora Kassner∗, Benno Krojer∗, Hinrich Sch¨utze\nCenter for Information and Language Processing (CIS)\nLMU Munich, Germany\nkassner@cis.lmu.de\nAbstract\nHow can pretrained language models (PLMs)\nlearn factual knowledge from the training set?\nWe investigate the two most important mech-\nanisms: reasoning and memorization. Prior\nwork has attempted to quantify the number of\nfacts PLMs learn, but we present, using syn-\nthetic data, the ﬁrst study that investigates the\ncausal relation between facts present in train-\ning and facts learned by the PLM. For reason-\ning, we show that PLMs seem to learn to apply\nsome symbolic reasoning rules correctly but\nstruggle with others, including two-hop rea-\nsoning. Further analysis suggests that even\nthe application of learned reasoning rules is\nﬂawed. For memorization, we identify schema\nconformity (facts systematically supported by\nother facts) and frequency as key factors for its\nsuccess.\n1 Introduction\nPretrained language models (PLMs) like BERT\n(Devlin et al., 2019), GPT-2 (Radford et al., 2019)\nand RoBERTa (Liu et al., 2019) have emerged as\nuniversal tools that capture a diverse range of lin-\nguistic and – as more and more evidence suggests\n– factual knowledge (Petroni et al., 2019; Radford\net al., 2019).\nRecent work on knowledge captured by PLMs\nis focused on probing, a methodology that identi-\nﬁes the set of facts a PLM has command of. But\nlittle is understood about how this knowledge is\nacquired during pretraining and why. We analyze\nthe ability of PLMs to acquire factual knowledge\nfocusing on two mechanisms: reasoning and mem-\norization. We pose the following two questions:\na) Symbolic reasoning:Are PLMs able to infer\nknowledge not seen explicitly during pretraining?\nb) Memorization:Which factors result in success-\nful memorization of a fact by PLMs?\n∗*equal contribution\nWe conduct our study by pretraining BERT from\nscratch on synthetic corpora. The corpora are com-\nposed of short knowledge-graph like facts: subject-\nrelation-object triples. To test whether BERT has\nlearned a fact, we mask the object, thereby gener-\nating a cloze-style query, and then evaluate predic-\ntions.\nSymbolic reasoning. We create synthetic cor-\npora to investigate six symbolic rules (equivalence,\nsymmetry, inversion, composition, implication,\nnegation); see Table 1. For each rule, we create a\ncorpus that contains facts from which the rule can\nbe learned. We test BERT’s ability to use the rule\nto infer unseen facts by holding out some facts in\na test set. For example, for composition, BERT\nshould infer, after having seen that leopards are\nfaster than sheep and sheep are faster than snails,\nthat leopards are faster than snails.\nOur setup is similar to link prediction in the\nknowledge base domain and therefore can be seen\nas a natural extension of the question: “Language\nmodels as knowledge bases?” (Petroni et al., 2019).\nIn the knowledge base domain, prior work (Sun\net al., 2019; Zhang et al., 2020) has shown that\nmodels that are able to learn symbolic rules are\nsuperior to ones that are not.\nTalmor et al. (2019) also investigate symbolic\nreasoning in BERT using cloze-style queries. How-\never, in their setup, there are two possible reasons\nfor BERT having answered a cloze-style query cor-\nrectly: (i) the underlying fact was correctly inferred\nor (ii) it was seen during training. In contrast,\nsince we pretrain BERT from scratch, we have full\ncontrol over the training setup and can distinguish\ncases (i) and (ii).\nA unique feature of our approach compared to\nprior work (Sinha et al., 2019; Richardson et al.,\n2020; Weston et al., 2016; Clark et al., 2020) is that\nwe do not gather all relevant facts and present them\nto the model at inference time. This is a crucial\n553\nRule Deﬁnition Example\nEQUI Equivalence (e, r, a) ⇐⇒ (e, s, a) (bird, can, ﬂy) ⇐⇒ (bird, is able to, ﬂy)\nSYM Symmetry (e, r, f) ⇐⇒ (f, r, e) (barack, married, michelle) ⇐⇒ (michelle, married, barack)\nINV Inversion (e, r, f) ⇐⇒ (f, s, e) (john, loves, soccer) ⇐⇒ (soccer, thrills, john)\nNEG Negation (e, r, a) ⇐⇒ (e, not r, b) (jupiter, is, big) ⇐⇒ (jupiter, is not, small)\nIMP Implication (e, r, a) ⇒ (e, s, b), (e, s, c),... (dog, is, mammal) ⇒ (dog, has, hair), (dog, has, neocortex), ...\nCOMP Composition (e, r, f) ∧ (f, s, g) ⇒ (e, t, g) (tiger, faster than, sheep) ∧ (sheep, faster than, snail)\n⇒ (leopard, faster than, snail) with r = s = t\nTable 1: The six symbolic rules we investigate (cf. (Nayyeri et al., 2019)) with an example in natural language for\nentities e,f,g ∈E, relations r,s,t ∈Rand attributes a,b,c ∈A.\ndifference – note that human inference similarly\ndoes not require that all relevant facts are explicitly\nrepeated at inference time.\nWe ﬁnd that i) BERT is capable of learning some\none-hop rules (equivalence and implication). ii) For\nothers, even though high test precision suggests suc-\ncessful learning, the rules were not in fact learned\ncorrectly (symmetry, inversion and negation). iii)\nBERT struggles with two-hop rules (composition).\nHowever, by providing richer semantic context,\neven two-hop rules can be learned.\nGiven that BERT can in principle learn some rea-\nsoning rules, the question arises whether it does so\nfor standard training corpora. We ﬁnd that BERT-\nlarge has only partially learned the types of rules\nwe investigate here. For example, BERT has some\nnotion of “X shares borders with Y” being symmet-\nric, but it fails to understand rules like symmetry in\nother cases.\nMemorization. During the course of pretrain-\ning, BERT sees more data than any human could\nread in a lifetime, an amount of knowledge that sur-\npasses its storage capacity. We simulate this with\na scaled-down version of BERT and a training set\nthat ensures that BERT cannot memorize all facts\nin training. We identify two important factors that\nlead to successful memorization. (i) Frequency:\nOther things being equal, low-frequency facts are\nnot learned whereas frequent facts are. (ii) Schema\nconformity: Facts that conform with the overall\nschema of their entities (e.g., “sparrows can ﬂy” in\na corpus with many similar facts about birds) are\neasier to memorize than exceptions (e.g., “penguins\ncan dive”).\nWe publish our code for training and data gener-\nation. 1\n1https://github.com/BennoKrojer/\nreasoning-over-facts\n2 Data\nTo test PLMs’ reasoning capabilities, natural cor-\npora like Wikipedia are limited since it is difﬁcult\nto control what the model sees during training. Syn-\nthetic corpora provide an effective way of investi-\ngating reasoning by giving full control over what\nknowledge is seen and which rules are employed\nin generating the data.\nIn our investigation of PLMs as knowledge bases,\nit is natural to use (subject, relation, object) triples\nas basic units of knowledge; we refer to them as\nfacts. The underlying vocabulary consists of a set\nof entities e,f,g,... ∈ E, relations r,s,t,... ∈\nR and attributes a,b,c,... ∈ A, all represented\nby artiﬁcial strings such as e14, r3 or a35. Two\ntypes of facts are generated. (i) Attribute facts:\nrelations linking entities to attributes, e.g., (e,r,a)\n= (leopard, is, fast). (ii) Entity facts: relations\nlinking entities, e.g., (e,r,f ) = (Paris, is the capital\nof, France).\nIn the test set, we mask the objects and generate\ncloze-style queries of the form “er [MASK]”. The\nmodel’s task is then to predict the correct object.\n2.1 Symbolic Reasoning\nTable 1 gives deﬁnitions and examples for the six\nrules (EQUI, SYM, INV , COMP, IMP, NEG) we\ninvestigate. The deﬁnitions are the basis for our\ncorpus generation algorithms, shown in Figure 1.\nSYM, INV , COMP generate entity facts and EQUI,\nIMP, NEG attribute facts. We create a separate\ncorpus for each symbolic rule. Facts are generated\nby sampling from the underlying vocabulary. For\n§2.1, this vocabulary consists of 5000 entities, 500\nrelations and 1000 attributes. Half of the relations\nfollow the rule, the other half is used to generate\nrandom facts of entity or attribute type.\nWe can most easily think of the corpus genera-\ntion as template ﬁlling. For example, looking at\nSYM in Table 1, the template is (e,r,f ) ⇐⇒\n(f,r,e ). We ﬁrst sample a relation rfrom Rand\n554\nEQUI\nC = ∅,D = ∅\nfor i∈1 ...n do\n(r,s) ∼R×R\na∼A\nfor j ∈1 ...m do\ne∼E\naddC=Bernoulli(0.5)\nif addC then\nC=C∪{(e,r,a)}\nD=D∪{(e,s,a )}\nelse\nC=C∪{(e,s,a )}\nD=D∪{(e,r,a)}\nSYM\nC = ∅,D = ∅\nfor i∈1 ...n do\nr∼R\nfor j ∈1 ...m do\n(e,f) ∼E×E\nC=C∪{(e,r,f )}\nD=D∪{(f,r,e )}\nINV\nC = ∅,D = ∅\nfor i∈1 ...n do\n(r,s) ∼R×R\nfor j ∈1 ...m do\n(e,f) ∼E×E\nC=C∪{(e,r,f )}\nD=D∪{(f,s,e )}\nCOMP\nC = ∅,D = ∅\nfor i∈1 ...n do\n(r,s,t) ∼R×R×R\nfor j ∈1 ...m do\n(e,f,g ) ∼E×E×E\nC=C∪{(e,r,f )}\nC=C∪{(f,s,g )}\nD=D∪{(e,t,g )}\nIMP\nC = ∅,D = ∅\nfor i∈1 ...n do\n(r,s) ∼R×R\nfor k∈1 ...l do\nb∼A\nα∼A×... ×A\nfor j ∈1 ...m do\ne∼E\nC=C∪{(e,r,b)}\nfor a∈αdo\nD=D∪{(e,s,a )}\nNEG\nC = ∅,D = ∅\nfor i∈1 ...n do\nr∼R\nfor j ∈1 ...m do\ne∼E\na∼A\nb= antonym(a)\nnegated=Bernoulli(0.5)\nif negated then\nC=C∪{(e,not r,a)}\nD=D∪{(e,r,b)}\nelse\nC=C∪{(e,r,a)}\nD=D∪{(e,not r,b)}\nFigure 1: Pseudocode for symbolic reasoning corpus generation. “ a ∼A” stands for: a is randomly sampled\nfrom A. (“α∼A×... ×A”: a tuple of 4 attributes is sampled.) The vocabulary consists of entities e,f,g ∈E,\nrelations r,s,t ∈Rand attributes a,b,c ∈A. Train/test corpora are formed from C and D. n = 20, m = 800,\nl= 2. See §2.1 for details.\nFREQ\nC = ∅\nm= 1\nfor i∈1 ...n do\n(e,f) ∼E×E\nr∼R\nfor j ∈1 ...m do\nC = C∪{(e,r,f )}\nif i%(n/100) == 0then\nm+ = 1\nSCHEMA\nC = ∅\nfor i∈1 ...k do\nδ∼E×... ×E\nfor rin Rdo\nschema = Bernoulli(0.5)\nif schema then\nα∼A×...×A\nfor e∈δdo\nfor a∈αdo\nadd = Bernoulli(0.5)\nif add then\nC = C∪{(e,r,a)}\nelse\nexception = Bernoulli(0.5)\nif exception then\na∼A\nC = C∪{(e,r,a)}\nelse\nfor e∈δdo\nadd = Bernoulli(0.5)\nif add then\na∼A\nC = C∪{(e,r,a)}\nFigure 2: Pseudocode for memorization corpus gen-\neration. “ a ∼A” stands for: a is randomly sampled\nfrom A. (“δ∼E×... ×E”: a tuple of 250 entities is\nsampled. “ α ∼A×...×A”: a tuple of 10 attributes\nis sampled.) The vocabulary consists of entities e∈E,\nrelations r∈Rand attributes a∈A. Cis both training\nset and test set. n = 800,000, k = 250. See §2.2 for\ndetails.\nthen two entities e and f from E. We then add\n(e,r,f ) and (f,r,e ) to the corpus – this is one\ninstance of applying the SYM rule from which\nsymmetry can be learned. Similarly, the other rules\nalso generate instances.\nFor each of the other rules, the template ﬁlling\nis modiﬁed to conform with its deﬁnition in Ta-\nble 1. INV corresponds directly to SYM. COMP is\na two-hop rule whereas the other ﬁve are one-hop\nrules. EQUI generates instances from which one\ncan learn that the relations rand sare equivalent.\nIMP generates implication instances, e.g., (e,r,b)\n(= (dog, is, mammal)) implies (e,s,a 1) (= (dog,\nhas, hair)), (e,s,a 2) (= (dog, has, neocortex)) etc.\nPer premise we create four implied facts.\nFor NEG, we generate pairs of facts (e,r,a) (=\n(jupiter, is, big)) and (e,not r,b) (= (jupiter, is not,\nsmall)). We deﬁne the antonym function in Figure 1\n(NEG) as returning for each attribute its antonym,\ni.e., attributes are paired, each pair consisting of a\npositive and a negative attribute.\nEach of the six generation algorithms has the\nouter loop “for i ∈1. . .n” (where n = 20) that\nsamples one, two or three relations (and potentially\nattributes) and generates a subcorpus for these rela-\ntions; and the inner loop “for j ∈1. . .m” (where\nm= 800) that generates the subcorpus of instances\nfor the sampled relations.\nTrain/test split.The data generation algorithms\ngenerate two subsets of factsCand D, see Figure 1.\nFor each rule, we merge all of C with 90% of D\n(randomly sampled) to create the training set. The\nrest of D(i.e., the other 10%) serves as the test set.\nFor some of the cloze queries “ e r[MASK]”,\nthere are multiple correct objects that can be sub-\nstituted for MASK. Thus, we rank predictions and\ncompute precision at m, i.e., precision in the top\nmwhere mis the number of correct objects. We\naverage precision at mfor all cloze queries.\nThis experimental setup allows us to test to what\nextent BERT learns the six rules, i.e., to what extent\nthe facts in the test set are correctly inferred from\ntheir premises in the training set.\n2.2 Memorization\nFor memorization, the vocabulary consists of\n125,000 entities, 20 relations and 2250 attributes.\nEffect of frequency on memorization. Our\nﬁrst experiment tests how the frequency of a fact in-\nﬂuences its successful memorization by the model.\nFigure 2 (left, FREQ) gives the corpus generation\nalgorithm. The outer loop generates 800,000 ran-\ndom facts. These are divided up in groups of 8000.\nA fact in the ﬁrst group of 8000 is added once to\n555\nthe corpus, a fact from the second group is added\ntwice and so on. A fact from the last group is added\n100 times to the corpus. The resulting corpus Cis\nboth the training set and the test set.\nEffect of schema conformity.In this experi-\nment, we investigate the hypothesis that a fact can\nbe memorized more easily if it is schema confor-\nmant.\nFigure 2 (right, SCHEMA) gives the corpus gen-\neration algorithm. We ﬁrst sample an entity group:\nδ ∼E×... ×E. For each group, relations are\neither related to the schema (“if schema”) or are\nnot (else clause). For example, for the schema “pri-\nmate” the relations “eat” (eats fruit) and “climb”\n(climbs trees) are related to the schema, the rela-\ntion “build” is not since some primates build nests\nand treehouses, but others do not.\nFor non-schema relations, facts with random\nattributes are added to the corpus. In Figure 4,\nwe refer to these facts as (facts with) unique at-\ntributes. For relations related to the schema, we\nsample the attributes that are part of the schema:\nα ∼A×...×A(e.g., (“paranut”,. . . ,”banana”)\nfor “eat”). Facts are then generated involving these\nattributes and added to the corpus. In Figure 4, we\nrefer to these facts as (facts with)group attributes.\nWe also generate exceptions (e.g., “eats tubers”)\nsince schemas generally have exceptions.\nSimilarly, the two lines “add = Bernoulli(0.5)”\nare intended to make the data more realistic: for a\ngroup of entities, its relations and its attributes, the\ncomplete cross product of all facts is not available\nto the human learner. For example, a corpus may\ncontain sentences stating that chimpanzees and ba-\nboons eat fruit, but none that states that gorillas eat\nfruit.\nFor this second memorization experiment, train-\ning set and test set are again identical (i.e., = C).\nIn a ﬁnal experiment, we modify SCHEMA as\nfollows: exceptions are added 10 times to the cor-\npus (instead of once). This tests the interaction\nbetween schema conformity and frequency.\n3 BERT Model\nBERT uses a deep bidirectional Transformer\n(Vaswani et al., 2017) encoder to perform masked\nlanguage modeling. During pretraining, BERT ran-\ndomly masks positions and learns to predict ﬁllers.\nWe use source code provided by Wolf et al. (2019).\nFollowing (Liu et al., 2019), we perform dynamic\nmasking and no next sequence prediction.\nrule train test\nEQUI 99.95 98.28\nSYM 99.97 98.40\nINV 99.99 87.21\nIMP 100.00 80.53\nNEG 99.98 20.54\nCOMP 99.98 0.01\nANTI 100.00 14.85\nTable 2: Precision in % of completing facts for sym-\nbolic rules. Training corpora generated as speciﬁed in\nFigure 1. See §4.1 for detailed discussion.\n(A) NEG (B) COMP\nFigure 3: Learning curves for symbolic reasoning.\n(A) shows precision for NEG with a varying number\nof attributes. A reduction to 125 attributes enables\nBERT to successfully apply antonym negation to the\ntest set. (B) shows test set precision for COMP follow-\ning the standard setup (orange) and an enhanced ver-\nsion (blue). Only in enhanced, i.e., with the introduc-\ntion of additional facts adding more semantic informa-\ntion, is COMP generalized.\nFor symbolic rules, we start with BERT-base\nand tune hyperparameters. We vary the number\nof layers to avoid that rule learning fails due to\nover-parametrization, see appendix for details. We\nreport precision based on optimal conﬁguration.\nIn the memorization experiment, our goal is to\ninvestigate the effect of frequency on memorization.\nDue to a limited compute infrastructure, we scale\ndown BERT to a single hidden layer with 3 atten-\ntion heads, a hidden size of 192 and an intermediate\nsize of 768.\n4 Results, Analysis and Discussion\n4.1 Symbolic Reasoning\nTable 2 gives results for the symbolic reasoning\nexperiments. BERT has high test set precision\nfor EQUI, SYM, INV and IMP. As we see in Ta-\nble 1, these rules share that they are “one-hop”:\nThe inference can be straightforwardly made from\na single premise to a conclusion, e.g., “(barack\nmarried michelle)” implies “(michelle married\nbarack)”. The crucial difference to prior work is\nthat the premise is not available at inference time.\n“(michelle married barack)” is correctly inferred by\n556\nthe model based on its memory of having seen the\nfact “(barack married michelle)” in the training set\nand based on the successful acquisition of the sym-\nmetry rule. Table 2 seems to suggest that BERT is\nable to learn one-hop rules and it can successively\napply these rules in a natural setting in which the\npremise is not directly available.\nIn the rest of this section, we investigate these\nresults further for SYM, INV , NEG and COMP.\n4.1.1 Analysis of SYM and INV\nTable 2 seems to indicate that BERT can learn that\na relation ris symmetric (SYM) and thatsand tare\ninverses (INV) – the evidence is that it generates\nfacts based on the successfully acquired symmetry\nand inversion properties of the relations r, sand t.\nWe now show that while BERT acquires SYM and\nINV partially, it also severely overgenerates. Our\nanalysis points to the complexity of evaluating rule\nlearning in PLMs and opens interesting avenues for\nfuture work.\nOur ﬁrst observation is that in the SYM experi-\nment, BERT understands all relations to be sym-\nmetric. Recall that of the total of 500 relations,\n250 are symmetric and 250 are used to generate\nrandom facts. If we take a fact with a random\nrelation r, say (e,r,f ), and prompt BERT with\n“(f,r, [MASK])”, then eis predicted in close to\n100% of cases. So BERT has simply learned that\nany relation is symmetric as opposed to distinguish-\ning between symmetric and non-symmetric rela-\ntions.\nThis analysis brings to light that our setup is\nunfair to BERT: it never sees evidence for non-\nsymmetry. To address this, we deﬁne a new experi-\nment, which we call ANTI because it includes an\nadditional set of “anti” relations that are sampled\nfrom R∗ with R∗∩R=∅and |R|= |R∗|. ANTI\nfacts take the following form: (e,r,f ), (f,r,g )\nwith e̸= g. Using this ANTI template we follow\nthe standard data generation procedure. The corpus\nis now composed of symmetric, anti-symmetric\nand random facts. ANTI training data indicate to\nBERT that r ∈R∗ is not symmetric since many\ninstances of rfacts are seen, with speciﬁc entities\n(f in the example) occurring in both slots, but there\nis never a symmetric example.\nTable 2 (ANTI) shows that BERT memorizes\nANTI facts seen during training but on test, BERT\nonly recognizes 14.85% of ANTI facts as non-\nsymmetric. So it still generalizes from the 250 sym-\nmetric relations to most other relations (85.15%),\neven those without any “symmetric” evidence in\ntraining. So it is easy for BERT to learn the concept\nof symmetry, but it is hard to teach it to distinguish\nbetween symmetric and non-symmetric relations.\nSimilar considerations apply to INV . BERT suc-\ncessfully predicts correct facts once it has learned\nthat sand tare inverses – but it overgeneralizes\nby also predicting many incorrect facts; e.g., for\n(e,s,f ) in train, it may predict (f,t,e ) (correct),\nbut also (e,t,f ) and (f,s,e ) (incorrect).\nIn another INV experiment, we add, for each pair\nof (f,r,e ) and (e,s,f ) two facts that give evidence\nof non-symmetry: (f,r,g ) and (e,s,h ) with e̸= g\nand h ̸= f. We ﬁnd that test set precision for\nINV (i.e., inferring (e,s,f ) in test from (f,r,e ) in\ntrain) drops to 17% in this scenario. As for SYM,\nthis indicates how complex the evaluation of rule\nlearning is.\nIn summary, we have found that SYM and INV\nare learned in the sense that BERT generates cor-\nrect facts for symmetric and inverse relations. But\nit severely overgenerates. Our analysis points to\na problem of neural language models that has not\nreceived sufﬁcient attention: they can easily learn\nthat the order of arguments is not important (as\nis the case for SYM relations), but it is hard for\nthem to learn that this is the case only for a subset\nof relations. Future work will have to delineate\nthe exact scope of this ﬁnding – e.g., it may not\nhold for much larger training sets with millions of\noccurrences of each relation. Note, however, that\nhuman learning is likely to have a bias against sym-\nmetry in relations since the vast majority of verbs2\nin English (and presumably relations in the world)\nis asymmetric. So unless we have explicit evidence\nfor symmetry, we are likely to assume a relation is\nnon-symmetric. Our results suggest that neural lan-\nguage models do not have this bias – which would\nbe problematic when using them for learning from\nnatural language text.\n4.1.2 Analysis of NEG\nNEG was the only rule for which parameter tuning\nimproved performance. A reduction to four layers\nobtained optimal results.\nIn Table 2 we report a test set precision of\n20.54%. Why is negation more challenging than\nimplication? Implication allows the model to gen-\neralize over several entities all following the same\nrule (e.g., every animal that is a mammal has a\n2For example, almost all of the verb classes in (Levin,\n1993) are asymmetric.\n557\nneocortex). This does not hold for negation (e.g., a\nleopard is fast but a snail is not fast). BERT must\nlearn antonym negation from a large number of\npossible combinations. By reducing the number of\npossible combinations (decreasing the number of\nattributes from 1000 to 500, 250 and 125) BERT’s\ntest set precision increases, see Figure 3 (A). With\n125 attributes a precision of 91% is reached. A re-\nduction of attributes makes antonym negation very\nsimilar to implication.\nWe investigate BERT’s behavior concerning\nnegation further by adding an additional attribute\nset A∗, with A∗ ∩A=∅and |A|= |A∗|to the vo-\ncabulary. A∗ does not follow an antonym schema.\nWe samplea∈A∗, e∈E, r∈Rto add additional\nrandom facts of the type ( e, r, a) or (e, not r, a)\nto NEG’s training set. After training we test on\nthe additional random facts seen during training by\ninserting or removing the negation marker. We see\nthat BERT is prone to predict both (e, r, b) and (e,\nnot r, b) for b ∈A∗ (for 38%). Antonym negation\nwas still learned.\nWe conclude that antonym negation can be\nlearned via co-occurrences but a general concept\nof negation is not understood.\nThis is in agreement with prior work (Ettinger,\n2020; Kassner and Sch ¨utze, 2020) showing that\nBERT trained on natural language corpora is as\nlikely to generate a true statement like “birds can\nﬂy” as a factually false negated statement like\n“birds cannot ﬂy”.\n4.1.3 Analysis of COMP\nWhy does BERT not learn COMP? COMP differs\nfrom the other rules in that it involves two-hop rea-\nsoning. Recall that a novelty of our experimental\nsetup is that premises are not presented at infer-\nence time – two-hop reasoning requires that two\ndifferent facts have to be “remembered” to make\nthe inference, which intuitively is harder than a one-\nhop inference. Figure 3 (B) shows that the problem\nis not undertraining (orange line).\nSimilar to the memorization experiment, we in-\nvestigate whether stronger semantic structure in\nform of a schema can make COMP learnable. We\nrefer to this new experiment as COMP enhanced.\nData generation is deﬁned as follows: Entities are\ndivided into groups of 10. Relations are now de-\nﬁned between groups in the sense that the mem-\nbers of a group are “equivalent”. More formally,\nwe sample entity groups (groups of 10) E1, E2,\nE3 and relations r, s, t. For all e1 ∈E1,e2 ∈\nE2,e3 ∈E3, we add (e1,r,e2) and (e2,s,e 3) to\nCand (e1,t,e 3) to D. In addition, we introduce a\nrelation “samegroup” and add, for all em,en ∈Ei,\n(em,samegroup,en) to C – this makes it easy to\nlearn group membership. As before, the training\nset is the merger of C and 90% of Dand the test\nset is the rest of D.\nSimilar semantic structures occur in real data.\nThe simplest case is a transitive example: (r) planes\n(group 1) are faster than cars (group 2), ( s) cars\n(group 2) are faster than bikes (group 3), (t) planes\n(group 1) are faster than bikes (group 3).\nFigure 3 (B) shows that BERT can learn COMP\nmoderately well from this schema-enhanced corpus\n(blue curve): precision is clearly above 50% and\npeaks at 76%.\nThe takeaway from this experiment is that two-\nhop rules pose a challenge to BERT, but that they\nare learnable if entities and relations are embedded\nin a rich semantic structure. Prior work (Brown\net al., 2020) has identiﬁed the absence of “do-\nmain models” (e.g., a domain model for common\nsense physics) as one shortcoming of PLMs. To\nthe extent that PLMs lack such domain knowledge\n(which we simulate here with a schema), they may\nnot be able to learn COMP.\n4.2 Natural Language Corpora\nIn this section, we investigate to what extent the\nPLMs BERT and RoBERTa have learned SYM and\nINV from natural language corpora. See Table 3.\nFor “smaller/larger” (INV), we follow Talmor et al.\n(2019) and test which of the two words is selected\nas the more likely ﬁller in a pattern like “Jupiter\nis [MASK] than Mercury”. For the other three\nrelations (“shares borders with” (SYM), “is the op-\nposite of” (SYM), “is the capital of” / “’s capital is”\n(INV)), we test whether the correct object is pre-\ndicted in the pattern “er [MASK]” (as in the rest\nof the paper). We give the number of (i) consistent\n(“cons.”), (ii) correct and consistent (“correct”) and\n(iii) inconsistent (“inc.”) predictions. (A prediction\nis consistent and incorrect if it is consistent with\nthe rule, but factually incorrect.)\nIn more detail, we take a set of entities (coun-\ntries like “Indonesia”, cities like “Jakarta”) or adjec-\ntives like “low” that are appropriate for the relation\nand test which of the entities / adjectives is pre-\ndicted. For each of the ﬁve relations, we run both\nBERT-large-cased and RoBERTa-large and report\nthe more consistent result.\n558\nrelation rule completions examples\ncons. correct inc.\nshares borders with SYM 152 152 2\n(ecuador,peru)\n(togo,ghana), (ghana,nigeria)\nis the opposite of SYM 179 170 71\n(demand,supply)\n(injustice,justice), (justice,truth)\nis the capital of (C-of)\n’s capital is (s-C-is) INV 59 59 1\n(indonesia,s-C-is,jakarta)\n(canada,s-C-is,ottawa), (ottawa,C-of,ontario)\nis smaller/larger than\n(countries) INV 54 23 99\n(russia,larger,canada), (canada,smaller,russia)\n(brazil,smaller,russia), (russia,smaller,brazil)\nis smaller/larger than\n(planets) INV 9 9 36\n(jupiter,larger,mercury), (mercury,smaller,jupiter)\n(sun,bigger,earth), (earth,bigger,sun)\nTable 3: Can PLMs (BERT and RoBERTa) learn SYM and INV from natural language corpora? For\n“smaller/larger”, we follow Talmor et al. (2019) and test which of the two words is selected as a ﬁller in a pat-\ntern like “Jupiter is [MASK] than Mercury”. For the other three relations, we test whether the correct object is\npredicted (as in the rest of the paper). We give the number of (i) consistent (“cons.”), (ii) correct and consistent\n(“correct”) and (iii) inconsistent (“inc.”) predictions. Blue: consistent examples. Red: inconsistent examples. (We\nmake the simplifying assumption that “justice” can only have one opposite.)\nConsistency and accuracy are high for “shares\nborders with” and “capital”. However, this is most\nlikely due to the fact that many of these facts oc-\ncur verbatim in the training corpora of the two\nmodels. For example, Google shows 54,800 hits\nfor “jakarta is the capital of indonesia” and 1,290\nhits for “indonesia’s capital is jakarta” (both as a\nphrase). It is not possible to determine which factor\nis decisive here: successful rule-based inference or\nmemorization. The ultimate futility of this analysis\nis precisely the reason that we chose to work with\nsynthetic data.\nConsistency for “is the opposite of” is much\nlower than for the ﬁrst two relations, but still de-\ncent. To investigate this relation further, we also\ntested the relation “is the same as”. It turns out\nthat many of the “opposite” objects are also pre-\ndicted for “is the same as”, e.g., “high is the same\nas low” and “low is the same as high” where the\npredicted word is in italics. This indicates that the\nmodels have not really learned that “is the opposite\nof” is symmetric, but rather know that antonyms\nare closely associated and often occur together in\nphrases like “X is the opposite of Y”, “X and Y”,\n“X noun, Y noun” (e.g., “good cop, bad cop”) etc.\nApparently, this is then incorrectly generalized to\n“is the same as”.\nConsistency and accuracy are worse for\n“smaller/larger”. “smaller/larger” sentences of the\nsort considered here are probably rarer in genres\nlike Wikipedia than “shares borders with” and “is\nthe capital of”. A Wikipedia article about a coun-\ntry will always say what its capital is and which\ncountries it borders, but it will not enumerate the\ncountries that are smaller or larger.\nIn summary, although we have shown that pre-\ntrained language models have some ability to learn\nsymbolic rules, there remains considerable doubt\nthat they can do so based on natural corpora.\n4.3 Memorization\nExperimental results for the memorization experi-\nments are shown in Figure 4.\n(A) shows that frequent facts are memorized well\n(0.8 for frequency 100) and that rare facts are not\n(≈0.0 for frequencies <15).\n(B) shows that BERT memorizes schema confor-\nmant facts perfectly (“group attributes”). Accuracy\nfor exceptions is clearly lower than those of schema\nconformant facts: about 80%. The frequency of\neach fact in the training corpus in this experiment is\n1. Overall, the total amount of exceptions is much\nlower than the total amount of schema conformant\nfacts.\n(C) shows that exceptions are perfectly learned if\n10 copies of each exception are added to the corpus\n– instead of 1 in (B). In this case, limited capacity\naffects memorization of schema-conformant facts:\naccuracy drops to ≈0.9.\nIn summary, we ﬁnd that both frequency\nand schema conformity facilitate memorization.\nSchema conformant facts and exceptions compete\nfor memory if memory capacity is limited – depend-\ning on frequency one or the other is preferentially\nlearned by BERT.\n5 Limitations\nOur experimental design makes many simplifying\nassumptions: i) Variation in generated data is more\n559\n(A) frequency (B) schema conformity, (C) schema conformity,\nexceptions rare exceptions frequent\nFigure 4: Memorization experiments. We investigate the effect of frequency and schema conformity on memo-\nrization. (A) Frequent facts are memorized well (0.8 for frequency 100), rare facts are not ( ≈0.0 for frequencies\n<15). (B) BERT memorizes schema conformant facts perfectly (“group attributes”). Accuracy for rare exceptions\nis clearly lower (80%). (C) Exceptions are perfectly learned if 10 copies of each exception are added to the cor-\npus – instead of 1 in (B). In this case, limited capacity affects memorization of schema-conformant facts (“group\nattributes” drops to ≈0.9).\nlimited than in naturally occurring data. ii) Seman-\ntics are deliberately restricted to one rule only per\ngenerated corpus. iii) We do not investigate effects\nof model and corpus size.\ni) In natural corpora relations can have more\nthan two arguments, entities can have several to-\nkens, natural data are noisier than synthetic data\netc. Also, we study each rule in isolation.\nii) While our simpliﬁed corpora make learning\neasier in some respects, they may make it harder in\nothers. Each corpus is focused on providing train-\ning material for one symbolic rule, but it does not\ncontain any other “semantic” signal that may be\nhelpful in learning symbolic reasoning: distribu-\ntional signals, entity groupings, hierarchies, rich\ncontext etc. The experimental results of “COMP\nenhanced” indicate that indeed such signals are ben-\neﬁcial to symbolic rule learning. The interplay of\nsuch additional sources of information for learning\nwith symbolic rules is an interesting question for\nfollow up work.\niii) Results are based on BERT-base and scaled-\ndown versions of BERT-base only, just as training\ncorpora are orders of magnitude smaller than natu-\nral training corpora. We varied model and corpus\nsizes within the limits of our compute infrastruc-\nture, but did not systematically study their effect\non our ﬁndings.\nOur work is an initial exploration of the question\nwhether symbolic rules can be learned in principle,\nbut we view it mainly as a starting point for future\nwork.\n6 Related Work\nRadford et al. (2019) and Petroni et al. (2019) show\nin a zero-shot question answering setting that PLMs\nhave factual knowledge. Our main question is: un-\nder what conditions do PLMs learn factual knowl-\nedge and do they do so through memorization or\nrule-based inference?\nSun et al. (2019) and Zhang et al. (2020) show\nin the knowledge graph domain that models that\nhave the ability to capture symbolic rules like SYM,\nINV and COMP outperform ones that do not. We\ninvestigate this question for PLMs that are trained\non language corpora.\nTalmor et al. (2019) test PLMs’ symbolic reason-\ning capabilities probing pretrained and ﬁnetuned\nmodels with cloze-style queries. Their setup makes\nit impossible to distinguish whether a fact was in-\nferred or memorized during pretraining. Our syn-\nthetic corpora allow us to make this distinction.\nClark et al. (2020) test ﬁnetuned BERT’s reason-\ning capabilities, but they always make premise and\nconclusion locally available to the model, during\ntraining and inference. This is arguably not the way\nmuch of human inference works; e.g., the fact F\nthat X borders Y allows us to infer that Y borders\nX even if we were exposed to F a long time ago.\nRichardson et al. (2020) introduce synthetic\ncorpora testing logic and monotonicity reasoning.\nThey show that BERT performs poorly on these\nnew datasets, but can be quickly ﬁnetuned to good\nperformance. The difference to our work again is\nthat they make the premise available to the model\nat inference time.\nFor complex reasoning QA benchmarks (Yang\n560\net al., 2018; Sinha et al., 2019), PLMs are ﬁnetuned\nto the downstream tasks. Their performance is\ndifﬁcult to analyze: it is not clear whether any\nreasoning capability is learned by the PLM or by\nthe task speciﬁc component.\nAnother line of work (Gururangan et al., 2018;\nKaushik and Lipton, 2018; Dua et al., 2019; Mc-\nCoy et al., 2019) shows that much of PLMs’ per-\nformance on reasoning tasks is due to statistical\nartifacts in datasets and does not exhibit true rea-\nsoning and generalization capabilities. With the\nhelp of synthetic corpora, we can cleanly investi-\ngate PLMs’ reasoning capabilities.\nHupkes et al. (2020) study the ability of neural\nmodels to capture compositionality. They do not\ninvestigate our six rules, nor do they consider the ef-\nfects of fact frequency and schema conformity. Our\nwork conﬁrms their ﬁnding that transformers have\nthe ability to capture both rules and exceptions.\nA large body of research in psychology and cog-\nnitive science has investigated how some of our\nrules are processed in humans, e.g., Sloman (1996)\nfor implication. There is also a lively debate in\ncognitive science as to how important rule-based\nreasoning is for human cognition (Politzer, 2007).\nYanaka et al. (2020); Goodwin et al. (2020) are\nconcurrent studies of systematicity in PLMs. The\nﬁrst shows that monotonicity inference is feasible\nfor syntactic structures close to the ones observed\nduring training. The latter shows that PLMs can ex-\nhibit high over-all performance on natural language\ninference despite being non-systematic.\nRoberts et al. (2020) show that the amount of\nknowledge captured by PLMs increases with model\nsize. Our memorization experiments investigate\nthe factors that determine successful acquisition of\nknowledge.\nGuu et al. (2020) modify the PLM objective to\nincentivize knowledge acquisition. They do not\nconsider symbolic rule learning nor do they analyze\nwhat factors inﬂuence successful memorization.\nBased on perceptrons and convolutional neural\nnetworks, Arpit et al. (2017); Zhang et al. (2017)\nstudy the relation of generalizing from real struc-\ntured data vs. memorizing random noise in the\nimage domain, similar to our study of schema-\nconformant facts and outliers. They do not study\ntransformer based models trained on natural lan-\nguage.\n7 Conclusion\nWe studied BERT’s ability to capture knowledge\nfrom its training corpus by investigating its reason-\ning and memorization capabilities. We identiﬁed\nfactors inﬂuencing what makes successful mem-\norization possible and what is learnable beyond\nknowledge explicitly seen during training. We saw\nthat, to some extent, BERT is able to infer facts not\nexplicitly seen during training via symbolic rules.\nOverall, effective knowledge acquisition must\ncombine both parts of this paper: memorization\nand symbolic reasoning. A PLM is not able to\nstore an unlimited amount of knowledge. Through\nacquiring reasoning capabilities, knowledge gaps\ncan be ﬁlled based on memorized facts. A schema-\nconformant fact (“pigeons can ﬂy”) need not be\nmemorized if there are a few facts that indicate\nthat birds ﬂy and then the ability of ﬂight can be\nﬁlled in for the other birds. The schema conformity\nexperiments suggest that this is happening. It is\neasier to capture knowledge that conforms with a\nschema instead of memorizing facts one by one.\nThere are several directions for future work.\nFirst, we made many simplifying assumptions that\nshould be relaxed in future work. Second, how\ncan we improve PLMs’ ability to learn symbolic\nrules? We see two avenues here, either additional\ninductive biases could be imposed on PLMs’ archi-\ntectures or training corpora could be modiﬁed to\npromote learning of symbolic rules.\nAcknowledgements\nWe thank Peter Clark for helpful discussions and\nour reviewers for constructive feedback.\nThis work was funded by the German Federal\nMinistry of Education and Research (BMBF, Grant\nNo. 01IS18036A) and by Deutsche Forschungs-\ngemeinschaft (DFG, Grant ReMLA V: Relational\nMachine Learning for Argument Validation). The\nauthors of this work take full responsibility for its\ncontent.\nReferences\nDevansh Arpit, Stanisław Jastrzundeﬁnedbski, Nicolas\nBallas, David Krueger, Emmanuel Bengio, Maxin-\nder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron\nCourville, Yoshua Bengio, and Simon Lacoste-\nJulien. 2017. A closer look at memorization in\ndeep networks. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning - Volume 70,\nICML’17, page 233242. JMLR.org.\n561\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\n2020. Transformers as soft reasoners over language.\nIJCAI.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDrop: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In NAACL-\nHLT.\nAllyson Ettinger. 2020. What bert is not: Lessons from\na new suite of psycholinguistic diagnostics for lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nEmily Goodwin, Koustuv Sinha, and Timothy J.\nO’Donnell. 2020. Probing linguistic systematicity.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1958–1969, Online. Association for Computational\nLinguistics.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training.\nArXiv, abs/2002.08909.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and\nElia Bruni. 2020. Compositionality decomposed:\nHow do neural networks generalise? J. Artif. Intell.\nRes., 67:757–795.\nNora Kassner and Hinrich Sch¨utze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. ACL.\nDivyansh Kaushik and Zachary C. Lipton. 2018. How\nmuch reading does reading comprehension require?\na critical investigation of popular benchmarks. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , pages\n5010–5015, Brussels, Belgium. Association for\nComputational Linguistics.\nBeth Levin. 1993. English Verb Classes and Alterna-\ntions. The University of Chicago Press, Chicago.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nMojtaba Nayyeri, Chengjin Xu, Jens Lehmann, and\nHamed Shariat Yazdi. 2019. Logicenn: A neural\nbased knowledge graphs embedding model with log-\nical rules. ArXiv, abs/1908.07141.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nGuy Politzer. 2007. Reasoning with conditionals.\nTopoi, 26(1):79–95.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nKyle Richardson, Hai Hu, Lawrence S. Moss, and\nAshish Sabharwal. 2020. Probing natural language\ninference models through semantic fragments. In\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? ArXiv, abs/2002.08910.\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle\nPineau, and William L. Hamilton. 2019. CLUTRR:\nA diagnostic benchmark for inductive reasoning\nfrom text. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\n562\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4506–4515, Hong Kong, China. Association for\nComputational Linguistics.\nSteven A. Sloman. 1996. The empirical case for\ntwo systems of reasoning. Psychological Bulletin,\n119(1):3–22.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding\nby relational rotation in complex space. In Interna-\ntional Conference on Learning Representations.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019. oLMpics - on what\nlanguage model pre-training captures. ArXiv,\nabs/1912.13283.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nJason Weston, Antoine Bordes, Sumit Chopra, and\nTomas Mikolov. 2016. Towards ai-complete ques-\ntion answering: A set of prerequisite toy tasks. In\n4th International Conference on Learning Represen-\ntations, ICLR 2016, San Juan, Puerto Rico, May 2-4,\n2016, Conference Track Proceedings.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, and\nKentaro Inui. 2020. Do neural models learn sys-\ntematicity of monotonicity inference in natural lan-\nguage? In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6105–6117, Online. Association for Computa-\ntional Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2369–2380, Brussels, Belgium. Association\nfor Computational Linguistics.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-\njamin Recht, and Oriol Vinyals. 2017. Understand-\ning deep learning requires rethinking generalization.\nIn 5th International Conference on Learning Repre-\nsentations, ICLR 2017, Toulon, France, April 24 -\n26, 2017, Conference Track Proceedings.\nZhanqiu Zhang, Jianyu Cai, Yongdong Zhang, and Jie\nWang. 2020. Learning hierarchy-aware knowledge\ngraph embeddings for link prediction. In Thirty-\nFourth AAAI Conference on Artiﬁcial Intelligence.\n563\nA Hyperparameters\nA.1 Model hyperparameters\nFor all reported results we trained with a batch-size\nof 1024 and a learning rate of 6e-5.\nOur experiments for symbolic rules started with\nthe BERT-base model with 12 layers, 12 attention\nheads, hidden size of 768 and intermediate size of\n3072. For rules with a low test precision (NEG and\nCOMP) we then conducted a restricted grid search\n(restricted due to limited compute infrastructure):\nWe tried all possible numbers of layers from 1 to 12\nand then only considered the best result. For NEG\nthe best performance came from 4 layers, whereas\nCOMP did not show improvements for any number\nof layers. For NEG with 3 layers (which had a very\nsimilar performance to 4 layers) we exemplarily\ntested whether changing the attention heads, hidden\nsize or intermediate size improves precision. For\nthis we trained with the following 4 settings:\n•attention heads = 6, hidden size = 768, inter-\nmediate size = 3072\n•attention heads = 12, hidden size = 384, inter-\nmediate size = 1536\n•attention heads = 12, hidden size = 192, inter-\nmediate size = 768\n•attention heads = 12, hidden size = 96, inter-\nmediate size = 192\nHowever this did not further improve precision.\nA.2 Data hyperparameters\nIn previous iterations of our experiments, we had\nused different settings for generating our data. For\ninstance, we had varied the number of rules in our\ncorpora: 50 or 100 instead of the presented 20 rules.\nEven the sampling process itself can be tweaked\nto allow for less overlaps between rules and be-\ntween instances of one rule. However, we observed\nthe same trends and similar numbers across these\ndifferent settings.\nB Symbolic rules\nIn the following sections, we present illustrat-\ning corpora for INV, IMP and COMP enhanced.\nEach line is one datapoint. We also include the\ncontrol group at the end of each corpus that does\nnot follow any rule. In the case of composition en-\nhanced, ”{...}” indicates the sampled group which\nis not part of the actual dataset.\nWe illustrate our training corpora using real\nworld entities and relations. Note that the ac-\ntual corpora used for training are composed of an\nentirely synthetic vocabulary. For simplicity we\nshow grouped composition with enhancement with\ngroups of 4, instead of 10 as it is in the real data.\nB.1 INV\nParis CapitalOf France\nFrance HasCapital Paris\n...\nEgypt HasCapital Cairo (counterpart in test-set)\n...\nApple Developed iOS\niOS DevelopedBy Apple\n...\nGermany RandomRelation China\nCairo RandomRelation Norway\nB.2 IMP\n{(Flu), (Cough, RunningNose, Headache, Fever)}\nKevin HasDisease Flu\nKevin HasSymptom Cough\nKevin HasSymptom RunningNose\nKevin HasSymptom Headache\nKevin HasSymptom Fever\n...\nMariam HasDisease Flu\n...\nPeter RandomRelation Pain\nSarah RandomRelation Tooth\n564\nB.3 Comp enhanced\n{e8, e2, e4, e5}\ne8 ConnectedTo e2\ne8 ConnectedTo e4\ne8 ConnectedTo e5\ne2 ConnectedTo e8\n...\n{e15, e13, e12, e19}\ne15 ConnectedTo e13\ne15 ConnectedTo e12\n...\n{e25, e24, e29, e20}\ne25 ConnectedTo e24\ne25 ConnectedTo e29\n...\ne8 r1 e15\ne8 r1 e13\ne8 r1 e12\ne8 r1 e19\ne2 r1 e15\ne2 r1 e13\n...\ne5 r1 e19\n...\ne15 r2 e25\ne15 r2 e24\ne15 r2 e29\ne15 r2 e20\n...\ne19 r2 e20\n...\ne8 r3 e25\ne8 r3 e24\ne8 r3 e29\ne8 r3 e20\n...\n...\ne133 r61 e23\ne56 r61 e29\n...",
  "topic": "Memorization",
  "concepts": [
    {
      "name": "Memorization",
      "score": 0.6996868252754211
    },
    {
      "name": "Computer science",
      "score": 0.6583713889122009
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.6547929644584656
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5251227021217346
    },
    {
      "name": "Analogical reasoning",
      "score": 0.5107169151306152
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4732661247253418
    },
    {
      "name": "Key (lock)",
      "score": 0.4209345579147339
    },
    {
      "name": "Cognitive science",
      "score": 0.40076035261154175
    },
    {
      "name": "Natural language processing",
      "score": 0.38823992013931274
    },
    {
      "name": "Cognitive psychology",
      "score": 0.32640349864959717
    },
    {
      "name": "Machine learning",
      "score": 0.32521724700927734
    },
    {
      "name": "Psychology",
      "score": 0.26679301261901855
    },
    {
      "name": "Linguistics",
      "score": 0.14803335070610046
    },
    {
      "name": "Analogy",
      "score": 0.13557428121566772
    },
    {
      "name": "Programming language",
      "score": 0.08138620853424072
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    }
  ],
  "cited_by": 4
}