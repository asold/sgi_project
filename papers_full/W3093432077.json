{
    "title": "Chatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language transformer ensemble for text classification",
    "url": "https://openalex.org/W3093432077",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5080362193",
            "name": "Jordan J. Bird",
            "affiliations": [
                "Aston University"
            ]
        },
        {
            "id": "https://openalex.org/A5076983719",
            "name": "Anikó Ekárt",
            "affiliations": [
                "Aston University"
            ]
        },
        {
            "id": "https://openalex.org/A5032454100",
            "name": "Diego R. Faria",
            "affiliations": [
                "Aston University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2143927888",
        "https://openalex.org/W2026574758",
        "https://openalex.org/W2887423808",
        "https://openalex.org/W2944205279",
        "https://openalex.org/W2949154684",
        "https://openalex.org/W3047934910",
        "https://openalex.org/W3130406858",
        "https://openalex.org/W3045875328",
        "https://openalex.org/W2792467893",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2973048981",
        "https://openalex.org/W2963852536",
        "https://openalex.org/W1993676136",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2807186566",
        "https://openalex.org/W2809486926",
        "https://openalex.org/W1960634294",
        "https://openalex.org/W2139925198",
        "https://openalex.org/W2112257209",
        "https://openalex.org/W2981039167",
        "https://openalex.org/W1969340229",
        "https://openalex.org/W3011509861",
        "https://openalex.org/W2086039194",
        "https://openalex.org/W3039554467",
        "https://openalex.org/W2036317923",
        "https://openalex.org/W1977514866",
        "https://openalex.org/W3046029109",
        "https://openalex.org/W2917049430",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W3115242847",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3016833033",
        "https://openalex.org/W2792807988",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W2016589492",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W2498672755",
        "https://openalex.org/W2962712961",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3001555892",
        "https://openalex.org/W2955763246",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2250578619",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2962721878",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2798545263",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2992422826",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3107783893",
        "https://openalex.org/W3105721709",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W613690151",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3017852832"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)1 3\nJournal of Ambient Intelligence and Humanized Computing (2023) 14:3129–3144 \nhttps://doi.org/10.1007/s12652-021-03439-8\nORIGINAL RESEARCH\nChatbot Interaction with Artificial Intelligence: human data \naugmentation with T5 and language transformer ensemble for text \nclassification\nJordan J. Bird1  · Anikó Ekárt2 · Diego R. Faria1\nReceived: 21 August 2020 / Accepted: 5 August 2021 / Published online: 23 August 2021 \n© The Author(s) 2021\nAbstract\nIn this work we present the Chatbot Interaction with Artificial Intelligence (CI-AI) framework as an approach to the train-\ning of a transformer based chatbot-like architecture for task classification with a focus on natural human interaction with \na machine as opposed to interfaces, code, or formal commands. The intelligent system augments human-sourced data \nvia artificial paraphrasing in order to generate a large set of training data for further classical, attention, and language \ntransformation-based learning approaches for Natural Language Processing (NLP). Human beings are asked to paraphrase \ncommands and questions for task identification for further execution of algorithms as skills. The commands and questions \nare split into training and validation sets. A total of 483 responses were recorded. Secondly, the training set is paraphrased \nby the T5 model in order to augment it with further data. Seven state-of-the-art transformer-based text classification algo-\nrithms (BERT, DistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are benchmarked for both sets \nafter fine-tuning on the training data for two epochs. We find that all models are improved when training data is augmented \nby the T5 model, with an average increase of classification accuracy by 4.01%. The best result was the RoBERTa model \ntrained on T5 augmented data which achieved 98.96% classification accuracy. Finally, we found that an ensemble of the five \nbest-performing transformer models via Logistic Regression of output label predictions led to an accuracy of 99.59% on the \ndataset of human responses. A highly-performing model allows the intelligent system to interpret human commands at the \nsocial-interaction level through a chatbot-like interface (e.g. “Robot, can we have a conversation?”) and allows for better \naccessibility to AI by non-technical users.\nKeywords Chatbot · Human-machine interaction · Data augmentation · Transformers · Language transformation · Natural \nLanguage Processing\n1 Introduction\nAttention-based and transformer language models are a \nrapidly growing field of study within machine learning \nand artificial intelligence and for applications beyond. The \nfield of Natural Language Processing (NLP) has especially \nbeen advanced through transformers due to their approach \nto reading being more akin to human behaviour than clas-\nsical sequential techniques. With many industries turning \nto Artificial Intelligence (AI) solutions by the day, mod-\nels have a growing requirement for robustness, explain -\nability, and accessibility since AI solutions are becoming \nmore and more popular for those without specific tech -\nnical backgrounds in the field. Another interesting field \nthat is similarly being seen more often is that of data aug-\nmentation; that is, creating data from a set that in itself \n * Jordan J. Bird \n birdj1@aston.ac.uk\n Anikó Ekárt \n a.ekart@aston.ac.uk\n Diego R. Faria \n d.faria@aston.ac.uk\n1 Aston Robotics, Vision and Intelligent Systems Lab (ARVIS \nLab), Aston University, Birmingham, UK\n2 School of Engineering and Applied Science, Aston \nUniversity, Birmingham, UK\n3130 J. J. Bird et al.\n1 3\nincreases the quality of that set of data. The alternative to \ndata augmentation, which is unfortunately the case with \nmany modern NLP systems, is to gather more data. As an \nalternative to unwanted privacy concerns, data scientists \nmay instead find ways to augment the data as a friendlier \nalternative.\nIn this study, we bring together all of these aforemen-\ntioned concepts and fields of study to form a system that \nwe call Chatbot Interaction with Artificial Intelligence (CI-\nAI). A general overview of the approach can be observed \nin Fig.  1. As an alternative to writing code and managing \ndata, complex machine learning tasks such as conversational \nAI, sentiment analysis, scene recognition, brainwave clas-\nsification and sign language recognition among others are \ngiven accessibility through an interface of natural, social \ninteraction via both verbal and non-verbal communication. \nThat is, for example, a spoken command of “can we have \na conversation?” or a sign language command of “can-we-\ntalk” would command the system to launch a conversational \nAI program. For such a system to be possible, it needs to be \nrobust, since an interactive system that makes one mistake \nfor many successes would be considered a broken system. \nThe system needs to be accessible to a great number of \npeople with differing backgrounds, and thus must have the \nability to generalise by being exposed to a large amount of \ntraining data. Last, but by no means least, the system needs \nto be explainable; as given in a later example, if a human \nwere to utter the phrase, “Feeling sad today. Can you cheer \nme up with a joke?”, which features within that phrase lead \nto a correct classification and command to the chatbot to tell \na joke? Where does the model focus within the given text \nin order to correctly predict and fulfil the human’s request? \nThus, to achieve these goals, the scientific contributions of \nthis work are as follows: \n1. The collection of a seven-class command-to-task dataset \nfrom multiple human beings from around the world, giv-\ning a total of 483 data objects.\n2. Augmentation of the human data with a transformer-\nbased paraphrasing model which results in a final train-\ning dataset of 13,090 labelled data objects.\n3. Benchmarking of seven State-of-the-Art transformer-\nbased classification approaches for text-to-task com-\nmands. Each model is trained on the real training data \nand validation data, and is then trained on the real train-\ning data plus the paraphrased augmented data and vali-\ndation data. We find that all seven models are improved \nsignificantly when exposed to augmented data.\n4. A deep exploration of the best model. Firstly in order \nto discern the small amount of errors (1.04% errors) \nand how they were caused by seeing the largest errors \nin terms of loss and the class probability distributions. \nSecondly, the chatbot is given commands that were not \npresent during training or validation, and top features \n(words) are observed- interestingly, given their technical \nnature, the models focus keenly on varying parts of the \nsentence similar to a human reading.\n5. Stacked Generalisation approaches are explored in order \nto ensemble several highly performing models, results \nshow that the stack of multiple transformers outperform \nthe best singular model.\nThe rest of this article is structured as follows. Initially, the \nbackground and related studies are explored in Sect. 2. The \nmethod of the experiments are described in Sect.  3, and the \nresults from the experiments are then presented in Sect.  4. \nWith the best-performing model in mind, Sect.  4.1 then \nexplores the model in terms of the small number of errors \nmade, and how the model interprets new and unseen data (ie. \nshould the model be in deployment). Finally, conclusions are \ndrawn and future work is suggested in Sect. 5.\n2  Background and related works\nData scarcity often poses a problem in the field of \nNLP (Roller et al. 2020), given that even a large subject set \nof over one hundred individuals may still result in a rela-\ntively small amount of data collected in comparison to other \nfields, with consideration to the size of data usually required \nfor machine and deep learning models. Several works have \nsuggested that data augmentation is an important solution \nFig. 1  A general overview of \nthe proposed approach Small Dataset of human\nresponses\nIssue: Data ScarcityHuman-sourced\nconversational data Testing Dataset\nTraining dataset\nIssue: Data Scarcity\nData augmentation\nthrough T5\nSolution: Data Scarcity\nTransformer models for\ntext-task-classification Chatbot Application\nModel\nTrain: Human, Augmented\nValidated: Human\nLarge, augmented\ntraining dataset\n\n3131\nChatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language…\n1 3\nto these problems, that is, engineering synthetic data to \nincrease the size of a dataset. It is important that the syn-\nthetic data is not only different to the actual data, but also \nthat it contains useful knowledge to improve classifiers when \nattempting to understand language. For example, chatbot \nsoftware has been noted to improve in ability when syn-\nonymous terms are generalised as flags (Bird et al. 2018a). \nTechniques that have shown promise include random token \nperturbations (Wei and Zou 2019), back-translation (Shleifer \n2019), and inductive transfer learning (Howard and Ruder \n2018). Recently, it was noted that paraphrasing provides a \nstrong candidate for solving data scarce NLP problems (Ban-\nnard and Callison-Burch 2005; Marton et al. 2009; Lewis \net al. 2020) as well as language transformation (Sun et al. \n2020). In this work, we consider improving a data scarce \nproblem by augmenting the training dataset by paraphrasing \nit via a pre-trained Transformer model. In addition, the text \nclassification models themselves are also transformative in \nnature.\nThe Transformer is a new concept in the field of deep \nlearning (Vaswani et al. 2017). Transformers currently have \na primary focus on NLP, but state-of-the-art image process-\ning using similar networks have recently been explored (Qi \net al. 2020). With the idea of paying attention in mind, the \ntheory behind the exploration of Transformers in NLP is \ntheir more natural approach to sentences; rather than focus-\ning on one token at a time in the order that they appear and \nsuffering from the vanishing gradient problem (Schmid-\nhuber 1992), Transformer-based models instead pay atten-\ntion to tokens in a learned order and as such enable more \nparallelisation while improving upon many NLP problems \nthrough which many benchmarks have been broken (Vas-\nwani et al. 2017; Wang et al. 2018). For these reasons, such \napproaches are rapidly forming State-of-the Art scores for \nmany NLP problems (Tenney et al. 2019). For text data in \nparticular these include generation (Devlin and Chang 2018; \nRadford et al. 2019), question answering (Shao et al. 2019; \nLukovnikov et al. 2019), sentiment analysis (Naseem et al. \n2020; Shangipour ataei et al. 2020), translation (Zhang et al. \n2018; Wang et al. 2019b; Di Gangi et al. 2019), paraphras-\ning (Chada 2020; Lewis et al. 2020), and classification (Sun \net al. 2019; Chang et al. 2019). According to (Vaswani et al. \n2017), Transformers are based on calculation of scaled dot-\nproduct attention units. These weights are calculated for each \nword within the input vector of words (document or sen-\ntence). The output of the attention unit are embeddings for \na combination of relevant tokens within the input sequence. \nThis is shown later on in Sect. 4.1 where both correctly and \nincorrectly classified input sequences are highlighted with \ntop features that lead to such a prediction. Weights for the \nquery W q , key W k , and value W v are calculated as follows:\nThe query is an object within the sequence, the keys are \nvector representations of said input sequence, and the values \nare produced given the query against keys. Unsupervised \nmodels receive Q , K and V from the same source and thus \npay self-attention. For tasks such as classification and trans-\nlation, K and V are derived from the source and Q is derived \nfrom the target. For example, Q could be a class for the \ntext to belong to ie. for sentiment analysis “positive” and \n“neutral” and thus the prediction of the classification model. \nSecondly, for translation, values K  and V could be derived \nfrom the English sentence “Hello, how are you?” and Q the \nsequence “Hola, como estas?” for supervised English-Span-\nish machine translation. All of the State-of-the-Art mod-\nels benchmarked in these experiments follow the concept \nof Multi-headed Attention. This is simply a concatenation \nof multiple i attention heads h i to form a larger network of \ninterconnected attention units:\nIt is important to note that human beings also do not read \nin a token-sequential nature as is with classical models such \nas the Long Short Term Memory (LSTM) network (Hochre-\niter and Schmidhuber 1997). Figure 2 from a 2019 study on \nreading comprehension (Eckstein et al. 2019) shows human \nbehaviour while reading. It can be observed from this exam-\nple and other related studies (Shagass et al. 1976; Kruger \nand Steyn 2014; Wang et al. 2019a), that rather than simply \nreading left-to-right (or right-to-left (Wang et al. 2019a; \nMarquis et al. 2020)), instead attention is paid to areas of \n(1)Attention(Q, K , V )=softmax\n�\nQK T\n√\ndk\n�\nV .\n(2)\nMultiHead(Q, K , V)=Concatenate(head1 , ...,headh)W O\nheadi = Attention(QWQ\ni , KW K\ni , VW V\ni ).\nFig. 2  An eye-tracking study of \nnatural reading from (Eckstein \net al. 2019). The reader’s gaze \nnaturally follows a left-to-right \nreading pattern with a fluctua-\ntion back to the main area of \ninterest, where the main reading \ntime is greater than that of the \nrest of the sentence\n\n3132 J. J. Bird et al.\n1 3\ninterest within the document. Of course, a human being does \nnot follow the equations previously described, but it can be \nnoted that attention-based models are more similar to human \nreading comprehension than that of sequential models such \nas the LSTM. Later, in Sect.  4.1, during the exploration of \ntop features within correct classifications, it can be observed \nthat RoBERTa also focuses upon select areas of interest \nwithin a text for prediction.\nThe Text-to-Text Transfer Transformer (T5) model is a \nunified approach to text transformers from Google AI (Raffel \net al. 2019). T5 aims to unify NLP tasks by restricting output \nto text which is then interpreted to score the learning task; \nfor example, it is natural to have a text output for a transla-\ntion task (as per the previous example on English-Spanish \ntranslation), but for classification tasks on the other hand, \na sparse vector for each prediction is often expected—T5 \ninstead would output a textual representation of the class(es). \nThis feature allows T5 to be extended to many NLP tasks \noutside of those suggested and benchmarked in the origi-\nnal work. To give a specific example to this study, an Eng-\nlish–English translation of example “what time is it right \nnow?” to “could you tell me the time, please?” provides a \nparaphrasing activity. That is, to express the same mean -\ning of a text written in a different way. Text-to-text format-\nted problems such as paraphrasing are enabled due to T5’s \nencoder–decoder architecture, a diagram of which can be \nobserved in Fig.  3. The model is trained via teacher forc-\ning (Williams and Zipser 1989; Goodfellow et al. 2017) \nwhere ground truth is used as input; each training instance \nrequires a target for each input sequence. For example in \nsequence-to-sequence, an output with an early mistake in the \nsequence would be punished for every subsequent output, \nwhereas teacher-forcing allows for the discarding of early \nmistakes after calculating the error at that step. Ultimately \nthis leads to a learning process wherein statistical proper -\nties can be calculated quicker. Each encoder and decoder \nperforms self attention and encoder–decoder attention as can \nbe observed in Eq. 1.1\nChatbots are a method of human-machine interaction that \nhave transcended novelty to become a useful technology of \nthe modern world. A biological signal study from 2019 \n(Muscular activity, respiration, heart rate, and electrical \nbehaviours of the skin) found that textual chatbots provide \na more comfortable platform of interaction than with more \nhuman-like animated avatars, which caused participants to \ngrow uncomfortable within the uncanny valley (Ciecha -\nnowski et al. 2019). Many chatbots exist as entertainment \nand as forms of art, such as in 2018 (Candello et al. 2018) \nwhen natural interaction was enabled via state-of-art of the \nart methods for character generation from text (Haller and \nRebedea 2013). This allowed for 10,000 visitors to converse \nwith 19th century characters from Machado de Assis’ “Dom \nCasmurro”. It has been strongly suggested through multi-\nple experiments that natural interaction with chatbots will \nprovide a useful educational tool in the future for students \nof varying ages (Kerlyl et al. 2006; Leonhardt et al. 2007; \nBollweg et al. 2018 ). The main open issue in the field of \nconversational agents is data scarcity which in turn can lead \nto unrealistic and unnatural interaction, overcoming which \nare requirements for the Loebner Prize based on the Turing \ntest (Stephens 2002). Solutions have been offered such as \ndata selection of input (Dimovski et al. 2018), input sim-\nplification and generalisation (Bird et al. 2018a), and more \nrecently paraphrasing of data (Virkar et al. 2019). These \nrecent advances in data augmentation by paraphrasing in \nparticular have shown promise in improving conversational \nsystems by increasing understanding of naturally spoken \nlanguage (Hou et al. 2018; Jin et al. 2018).\n3  Proposed approach\nIn this section, the proposed approach followed by the \nexperiments are described, from data collection to modes \nof learning and classification. The main aim of this work is \nto enable accessibility to previous studies, and in particu-\nlar the machine learning models derived throughout them. \nAccessibility is presented in the form of social interaction, \nwhere a user requests to use a system in particular via natural \nlanguage and the task is derived and performed. The seven \ncommands are:\n– Scene Recognition (Bird et al. 2020b)—The participant \nrequests a scene recognition algorithm to be instantiated, \na camera and microphone are activated for multi-modal-\nity classification.\nX1\nX2\nX3\nX4\nY1\nY2\n.\nEncoderD ecoder\nFig. 3  Diagram of an encoder–decoder architecture\n1 Further detail on T5 can be found in (Raffel et al. 2019).\n3133\nChatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language…\n1 3\n– EEG Classification—The participant requests an EEG \nclassification algorithm to be instantiated and begins \nstreaming data from a MUSE EEG headband, there are \ntwo algorithms:\n– EEG Mental State Classification  (Bird et  al. \n2018b)—Classification of whether the participant \nis concentrating, relaxed, or neutral.\n– EEG Emotional State Classification (Bird et al. \n2019a)—Classification of emotional valence, posi-\ntive, negative, or neutral.\n– Sentiment Analysis of Text (Bird et al. 2019b)—The \nparticipant requests to instantiate a sentiment analysis \nclassification algorithm for a given text.\n– Sign Language Recognition (Bird et al. 2020a )—The \nparticipant requests to converse via sign language, a \ncamera and Leap Motion and Leap Motion are acti-\nvated for multi-modality classification. Sign language \nis now accepted as input to the task-classification layer \nof the chatbot.\n– Conversational AI (Bird et al. 2018a)—The participant \nrequests to have a conversation, a chatbot program is \nexecuted.\n– Joke Generator (Manurung et al. 2008; Petrović and \nMatthews 2013)—The participant requests to hear a \njoke, a joke-generator algorithm is executed and output \nis printed.\nEach of the given commands are requested in the form of \nnatural social interaction (either by keyboard input, speech \nconverted to text, or sign language converted to text), and \nthrough accurate recognition, the correct algorithm is exe-\ncuted based on classification of the human input. Tasks such \nas sentiment analysis of text and emotional recognition of \nEEG brainwaves, and mental state recognition compared to \nemotional state recognition, are requested in similar ways \nand as such constitutes a difficult classification problem. \nFor these problems, minute lingual details must be rec-\nognised in order to overcome ambiguity within informal \ncommunication.\nFigure  4 shows the overall view of the system. Key -\nboard input text, or speech and sign language converted \nto text provide an input of natural social interaction. \nThe chatbot, trained on the tasks, classifies which task \nhas been requested and executes said task for the human \nparticipant. Sign language, due to its need for an active \ncamera and hand-tracking, is requested and activated via \nkeyboard input or speech and itself constitutes a task. In \norder to derive the bold ‘Chatbot’ module in Fig.  5 shows \nthe training processes followed. Human data is gathered \nvia questionnaires which gives a relatively small dataset \nFig. 4  Overall view of the Chat-\nbot Interaction with Artificial \nIntelligence (CI-AI) system \nas a looped process guided by \nhuman input, through natural \nsocial interaction due to the \nlanguage transformer approach. \nThe chatbot itself is trained via \nthe process in Fig. 5\nChatbot\nControl via Social\nInteraction\nText-to-task\nText\nSpeech\nSign Language\nKeyboard Input\nConversational\nAI\nSign Language\nRecognition\nJoke Generator\nEEG\nClassification\nTask\nScene\nRecognition\nConverse until exit\n1. Generate Joke\n2. Output Text\n1. Decide task\n2. Output text\nAbilities to run based on human\nrequest via social interaction\n(Natural interaction with the\nchatbot)\n1. Analyse Scene\n2. Output Text\nVerbal\nCommunication\nNon-verbal\nCommunication\nNon-verbal\nCommunication\nSentiment\nAnalysis\n1. Analyse Sentiment\n2. Output Text\nFaux Conversational\nLoop for Task\nClassification Class output\nEnable Camera\nand Hand\nTracking for\nmulti-modality\nSL recognition\nHuman agent\nprovides input\n3134 J. J. Bird et al.\n1 3\n(even though many responses were gathered, the nature of \nNLP tends to require a large amount of mined data), split \ninto training and testing instances. The first experiment \nis built upon this data, and State-of-the-Art transformer \nclassification models are benchmarked. In the second set \nof more complex experiments, the T5 paraphrasing model \naugments the training data and generates a large dataset, \nwhich are then also benchmarked with the same models \nand validation data in order to provide a direct comparison \nof the effects of augmentation. Augmentation is performed \nby paraphrasing the data within the training set, which \ntherefore provides a greater number of training examples. \nSeveral metrics are used to compare models in terms of \nTrue Positives (TP ), True Negatives (TN ), False Positives \n(FP), and False Negatives (FN).\nAccuracy:\nPrecision:\nRecall:\n(3)Accuracy= TP + TN\nTP + FN + TN + FP .\n(4)Precision= TP\nTP + FP .\nAnd finally the F1-Score:\nA questionnaire was published online for users to pro -\nvide human data in the form of examples of commands that \nwould lead to a given task classification. Five examples \nwere given for each, and Table 1 shows some examples that \nwere presented. The questionnaire instructions were intro-\nduced with “For each of these questions, please write how \nyou would state the text differently to how the example is \ngiven. That is, paraphrase it. Please give only one answer \nfor each. You can be as creative as you want!”. Two exam-\nples were given that were not part of any gathered classes, \n“If the question was: ‘How are you getting to the cinema?’ \nYou could answer: ‘Are we driving to the cinema or are we \ngetting the bus?’ and “If the question was: ‘What time is \nit?’ You could answer: ‘Oh no, I slept in too late... Is it the \nmorning or afternoon? What’s the time?”’. These examples \nwere designed to show the users that creativity and diver -\nsion from the given example was not just acceptable but also \n(5)Recall= TP\nTP + FN .\n(6)F 1 = 2 × precision× recall\nprecision+ recall\nFig. 5  Data collection and \nmodel training process. In this \nexample, the T5 paraphrasing \nmodel is used to augment and \nenhance the training dataset. \nModels are compared when \nthey are augmented and when \nthey are not on the same valida-\ntion set, in order to discern what \naffect augmentation has\nHuman responses\n(Small dataset)\nQuestionnaires\n\"How would you\nsay this phrase?\" Testing Data (30%)\nTraining Data (70%) T5 Paraphrasing Model\n(Data Augmentation)\nLanguage\nTransformation Text\nClassification \n(Model Training)\nChatbot\nControl via Social\nInteraction\nText-to-task\nText-to-Task \nClassifier\n(Trained Model)\nMultiple Models\nBenchmarked\nLarge Training\nDataset \n(Augmented Data)\nTable 1  A selection of example \nstatements presented to the \nusers for paraphrasing\nOne example is given for each for readability purposes, but a total of five examples were presented to the \nparticipants\nExample Statement Class\n“Would you like to talk?” CHAT\n“Tell me a joke” JOKE\n“Can you tell what mood I’m in from my brainwaves?” EEG-EMOTIONS\n“Am I concentrating? Or am I relaxed? EEG-MENTAL-STATE\n“Look around and tell me where you are” SCENE-CLASSIFICATION\n“Is this message being sarcastic or are they genuine?” SENTIMENT-ANALYSIS\n“I cannot hear the audio, please sign instead” SIGN-LANGUAGE\n3135\nChatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language…\n1 3\nencouraged, so long as the general meaning and instruc-\ntion of and within the message was retained (the instruc-\ntions ended with “The example you give must still make \nsense, leading to the same outcome.”). Extra instructions \nwere given as and when requested, and participants did not \nsubmit any example phrases nor were any duplicates sub-\nmitted. A total of 483 individual responses were recorded. \nThe answers were split 70/30 on a per-class basis to provide \ntwo class-balanced datasets, firstly for training (and aug-\nmentation), and secondly for validation. That is, regardless \nof augmentation, the model is tested based on this validation \nset and are all thus directly comparable in terms of their \nlearning abilities. The T5 paraphrasing model which was \ntrained on the Quora question pairs dataset (Quora 2017) is \nexecuted a maximum of 50 times for each statement within \nthe training set, where the model will stop generating para-\nphrases if the limit of possibilities or 50 total are reached. \nOnce each statement had been paraphrased, a random sub-\nsample of the dataset on a per-class basis was taken set at the \nnumber of data objects within the least common class (sign \nlanguage). Concatenated then with the real training data, a \ndataset of 13,090 examples were formed (1870 per class). \nThis dataset thus constitutes the second training set for the \nsecond experiment, in order to compare the effects of data \naugmentation for the problem presented. The datasets for \nthese experiments are publicly available.2\nTable  2 shows the models that are trained and bench-\nmarked on the two training sets (Human, Human+T5), and \nvalidated on the same validation dataset. It can be observed \nthat the models are complex, and training requires a rela-\ntively high amount of computational resources. Due to this, \nthe pre-trained weights for each model are fine-tuned for two \nepochs on each of the training datasets.\n3.1  Statistical ensemble of transformer classifiers\nFinally, a further experiment is devised to combine the \nresults of the best models within an ensemble, which \ncan be observed in Fig.  6. The training and test datasets \nTable 2  An overview of \nmodels benchmarked and their \ntopologies\nModel Topology\nBERT (Devlin et al. 2018) 12-layer, 768-hidden, 12-heads, 110 M parameters\nDistilBERT (Sanh et al. 2019) 6-layer, 768-hidden, 2-heads, 66 M parameters\nRoBERTa (Liu et al. 2019) 12-layer, 768-hidden, 12-heads, 125 M parameters\nDistilRoBERTa (Liu et al. 2019; Wolf et al. 2019) 6-layer, 768-hidden, 12-heads, 82 M parameters\nXLM (Conneau and Lample 2019) 12-layer, 2048-hidden, 16-heads, 342 M parameters\nXLM-RoBERTa (Conneau et al. 2019) 12-layer, 768-hidden, 3072 feed-forward, 8-heads, \n125 M parameters\nXLNet (Yang et al. 2019) 12-layer, 768-hidden, 12-heads, 110 M parameters\nT5 (Paraphraser) (Raffel et al. 2019) 12-layer, 768-hidden, 12-heads, 220 M parameters\nFig. 6  An ensemble strategy \nwhere statistical machine \nlearning models trained on the \npredictions of the transform-\ners then classify the text based \non the test data predictions of \nthe transformer classification \nmodels\nBERT DistilBERT XLM-\nRoBERTa\nDistil-\nRoBERTaRoBERTa\nNumerical Vector of\nTransformers\nPredictions\nHuman responses\nTesting Data (30%)\nTraining Data (70%) T5 Paraphrasing Model\n(Language transformer)\nLarge Training Dataset\n(Augmented Data)\nLarge Training Dataset \n(Augmented Data)\nStatistical ML Model\nBenchmarking\nEnsemble\nResults\nSingle\nTransformer\nResults\nTesting Data\n(Human Data)\nTransformers (weak models omitted)\n2 https:// www. kaggle. com/ birdy 654/ human- robot- inter action- via- t5- \ndata- augme ntati on.\n3136 J. J. Bird et al.\n1 3\nare firstly distilled into a numerical vector of five predic-\ntions made by the five selected transformer models. These \nfeatures are analysed in terms of classification ability by \nway of their relative entropy. That is the change in entropy \n( E (s)=− ∑\nj pj × log(pj) ) in terms of the classification of a \nset P j with solution s . Relative entropy or information gain \nis thus given as InfoGain(T, a)= E (T )− E (T /uni007C.vara) in regards \nto the calculated Entropy E, for instances of original ruleset \nH(T) and comparative ruleset H (T|a). Following this, sta-\ntistical machine learning models are trained on the training \nset and validated by the test set in order to discern whether \ncombining the models together ultimately improves the abil-\nity of the model. The reasoning behind a statistical ensem-\nble is that it enables possible improvements to a decision \nsystem’s robustness and accuracy (Zhang and Ma 2012). \nGiven that nuanced differences between the transformers \nmay lead to ‘personal’ improvements in some situations and \nnegative impacts in others, for example when certain phrases \nappear within commands, a more democratic approach may \nallow the pros of some models outweigh the cons of others. \nEmploying a statistical model to learn these patterns by clas-\nsifying the class based on the outputs of the previous models \nwould thus allow said ML model to learn these nuanced \ndifferences between the transformers.\n3.2  Experimental hardware and software\nThe experiments were executed on an NVidia Tesla K80 \nGPU which has 4992 CUDA cores and 24 GB of GDDR5 \nmemory via the Google Colab platform. The Transform-\ners were implemented via the KTrain library (Maiya 2020), \nwhich is a back-end for TensorFlow (Abadi et al. 2015) \nKeras (Chollet et al. 2015). The pretrained weights for \nthe Transformers prior to fine-tuning were from the Hug-\ngingFace NLP Library (Wolf et al. 2019). The pre-trained \nT5 paraphrasing model weights were from (Chang 2020). \nThe model was implemeted with the HuggingFace NLP \nLibrary (Wolf et al. 2019) via PyTorch (Paszke et al. 2019) \nand was trained for two epochs ( ∼ 20 h) on the p2.xlarge \nAWS ec2.\nThe statistical models for the stacked generalisation \nensemble results were implemented in Python via the Scikit-\nlearn toolkit (Pedregosa et al. 2011) and executed on an Intel \nCore i7 Processor (3.7 GHz).\n4  Results\nTable 3 shows the overall results for all of the experiments. \nEvery single model, even the weakest XLM for this par -\nticular problem, was improved when training on the human \ndata alongside the augmented data which can be seen for \nthe increases in metrics in Table  4. This required a longer \ntraining time due to the more computationally intense nature \nof training on a larger dataset. T5 paraphrasing for data aug-\nmentation led to an average accuracy increase of 4.01 points, \nand the precision, recall, and F1 scores were also improved \nat an average of 0.05, 0.05, and 0.07, respectively.\nInterestingly, although the results strongly suggest that \nparaphrased data augmentation improves training, the read-\nability of the paraphrased data was relatively mixed and \nsome strange occurrences took place. For example, “Can \nTable 3  Classification results \nof each model on the same \nvalidation set, both with and \nwithout augmented paraphrased \ndata within the training dataset\nBold shows best model per run, underline shows the best model overall\nModel With T5 paraphrasing Without T5 paraphrasing\nAcc. Prec. Rec. F1 Acc. Prec. Rec. F1\nBERT 98.55 0.99 0.99 0.99 90.25 0.93 0.9 0.9\nDistilBERT 98.34 0.98 0.98 0.98 97.3 0.97 0.97 0.97\nDistilRoBERTa 98.55 0.99 0.99 0.99 95.44 0.96 0.95 0.95\nRoBERTa 98.96 0.99 0.99 0.99 97.93 0.98 0.98 0.98\nXLM 14.81 0.15 0.15 0.15 13.69 0.02 0.14 0.03\nXLM-RoBERTa 98.76 0.99 0.99 0.99 87.97 0.9 0.88 0.88\nXLNet 35.68 0.36 0.35 0.36 32.99 0.33 0.24 0.24\nAverage 77.66 0.78 0.78 0.78 73.65 0.73 0.72 0.71\nTable 4  Observed increases in training metrics for each model due to \ndata augmentation via paraphrasing the training dataset\nModel Increase of metrics\nAcc. Prec. Rec. F1\nBERT 8.3 0.06 0.09 0.09\nDistilBERT 1.04 0.01 0.01 0.01\nDistilRoBERTa 3.11 0.03 0.04 0.04\nRoBERTa 1.03 0.01 0.01 0.01\nXLM 1.12 0.13 0.01 0.12\nXLM-RoBERTa 10.79 0.09 0.11 0.11\nXLNet 2.69 0.03 0.11 0.12\nAverage 4.01 0.05 0.05 0.07\n3137\nChatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language…\n1 3\nyou stay a while and talk with me?” and “Would you mind to \nspeak with me for a little bit? Or would that be a problem?” \nare perfectly reasonable requests for a conversation. But, \nsome data such as “I want to talk to you. I am a university \nstudent. I’d just like to speak with you. I have everything \nto give!” is obviously an unnatural utterance, and yet also \nevidently contains some useful knowledge for the model to \nlearn. Likewise, this can be noted for other classes. To give \nanother example, “If you know British Sign Language then \nI would prefer to use it.” was produced by the paraphrasing \nmodel, and this indeed makes sense and is a useful utterance. \nSimilarly to the previous example, there were strange sug -\ngestions by the model such as “I want to sign but don’t want \nto speak. Do you know the signs of a sign?” and “Why do \nwe speak in leap motion without any real thought?”. Though \nthese sentences contain useful knowledge as can be seen \nfrom the increase in classification metrics, this suggests \nfuture work may be required to clean the augmented data \n(reducing the dataset by culling a selection of the worst out-\nputs) which may lead to better performance. This would also \nlead to a less computationally expensive approach given that \nthere would be fewer training examples with only those in \nutmost quality retained. These occurrences also suggest that \nalthough paraphrasing is useful for data augmentation when \ntraining to understand human utterances, it would be logical \nto not use such a model for data that is going to be presented \nto the user such as the chatbot’s responses, given that not \nall paraphrased data makes sense from an English language \nperspective. Additionally, although it did not occur in the \nparaphrasing of this dataset, questions on Quora (which the \nT5 is trained on) can be of a sexual nature and as such thus \nmay lead to inappropriate utterances by the chatbot.\nThe best performing model was RoBERTa when training \non the human training set as well as the augmented data. \nThis model achieved 98.96% accuracy with 0.99 precision, \nrecall and F1 score. The alternative to training only on the \nhuman data achieved 97.93% accuracy with stable preci-\nsion, recall and F1 scores of 0.98. The second best perform-\ning models were both the distilled version of RoBERTa and \nBERT, which achieved 98.55% and likewise 0.99 for the \nother three metrics. Interestingly, some models saw a drastic \nincrease in classification ability when data augmentation was \nimplemented; the BERT model rose from 90.25% classifica-\ntion accuracy with 0.93 precision, 0.9 recall and 0.9 F1 score \nwith a +8.3% increase and then more stable metrics of 0.99 \neach as described previously. In the remainder of this sec-\ntion, the 98.96% performing RoBERTa model when trained \nupon human and T5 data is explored further. This includes, \nexploration of errors made overall and per specific examples, \nas well as an exploration of top features within successful \npredictions made.\nFigure 7 shows a comparison between the model perfor-\nmance and number of trainable parameters. Note that the \nmost complex model scored the least in terms of classifi-\ncation ability. The best performing model was the second \nmost complex model of all. The least complex model, Dis-\ntilBERT, achieved a relatively high accuracy of 98.34%.\n4.1  Exploration of the best transformer model\nIn this section, we explore the best model. The best model, \nas previously discussed, was the RoBERTa model when \ntraining on both the collected training data and the para-\nphrased data generated by the T5 model.\nTable 5 shows the classification metrics for each indi -\nvidual class by the RoBERTa model. The error matrix for \nthe validation data can be seen in Fig.  8. The tasks of EEG \nmental state classification, scene recognition, and sign lan-\nguage were classified perfectly. Of the imperfect classes, \nthe task of conversational AI (‘CHAT’) was sometimes \nmisclassified as a request for a joke, which is likely due to \nthe social nature of the two activities. EEG emotional state \nclassification was rarely mistakenly classified as the mental \nstate recognition and sentiment analysis tasks, firstly due \nto the closely related EEG tasks and secondly as sentiment \nanalysis since data often involved terms synonymous with \nvalence or emotion. Similarly, the joke class was also rarely \nmisclassified as sentiment analysis, for example, “tell me \nsomething funny” and “can you read this email and tell me \nFig. 7  Comparison of each model’s classification ability and number \nof million trainable parameters within them\nTable 5  Per-class precision, recall, and F1 score metrics for the best \nmodel\nClass Prec. Rec. F1\nCHAT 1.00 0.99 0.99\nEEG-EMOTIONS 0.99 0.97 0.98\nEEG-MENTAL-STATE 0.99 1.00 0.99\nJOKE 0.98 0.98 0.98\nSCENE-CLASSIFICATION 1.00 1.00 1.00\nSENTIMENT-ANALYSIS 0.97 0.99 0.98\nSIGN-LANGUAGE 1.00 1.00 1.00\n3138 J. J. Bird et al.\n1 3\nif they are being funny with me?” (‘funny’ in the second \ncontext being a British slang term for sarcasm). The final \nclass with misclassified instances was sentiment analysis, as \nemotional state recognition, for the same reason previously \ndescribed when the error occurred vice-versa.\n4.2  Mistakes and probabilities\nIn this section, we explore the biggest errors made when \nclassifying the validation set by considering their losses.\nTable 6 shows the most confusing data objects within the \ntraining set and Fig.  9 explores which parts of the phrase \nthe model focused on to derive these erroneous classifica-\ntions. Overall, only five misclassified sentences had a loss \nabove 1; the worst losses were in the range of 1.05 to 6.24. \nThe first phrase, “what is your favourite one liner?”, may \nlikely have caused confusion due to the term “one liner” \nwhich was not present within the training set. Likewise, the \nterm “valence” in “What is the valence of my brainwaves?” \nwas also not present within the training set, and the term \n“brainwaves” was most common when referring to mental \nstate recognition rather than emotional state recognition. An \ninteresting error occurred from the command “Run emotion \nclassification”, where the classification was incorrectly given \nas EEG emotional state recognition rather than Sentiment \nAnalysis. The command collected from a human subject was \nambiguous, and as such the two most likely classes were the \nincorrect EEG Emotions at a probability of 0.672 and the \ncorrect Sentiment Analysis at a probability of 0.32. This \nraises an issue to be explored in future works, given the \nnature of natural social interaction, it is likely that ambigu-\nity will be present during conversation. Within this errone-\nous classification, two classes were far more likely than all \nother classes present, and thus a choice between the two in \nthe form of a question akin to human deduction of ambigu-\nous language would likely solve such problems and increase \naccuracy. Additionally, this would rarely incur the require-\nment of further effort from the user.\n4.3  Top features within unseen data\nFollowing the training of the model, this section explores \nfeatures within data when an unseen phrase or command is \nuttered. That is, the examples given in this section were not \ndata within the training or validation datasets, and thus are \nmore accurate simulations of the model within a real-world \nscenario given new data to process based on the rules learnt \nduring training.\nIn this regard, Fig. 10 shows an example of a correct pre-\ndiction of unseen data class, for each class. Interestingly, \nthe model shows behaviour reminiscent of human read-\ning (Biedert et al. 2012; Kunze et al. 2013) due to transform-\ners not being limited to considering a temporal sequence in \nchronological order of appearance. In the first example the \nmost useful features were ‘time to speak’ followed by ‘got’, \n‘to’ and ‘me’. The least useful features were ‘right now’, \nwhich alone would be classified as ‘SCENE-CLASSIFICA-\nTION’ with a probability of 0.781 due to many provided \ntraining examples for such class containing questions such \nas ‘where are you right now? Can you run scene recogni-\ntion and tell me?’. The second example also had a strong \nnegative impact from the word ‘read’ which alone would be \nclassified as ‘SENTIMENT-ANALYSIS’ with a probabil-\nity of 0.991 due to the existence of phrases such as ‘please \nread this message and tell me if they are angry with me’ \nbeing popular within the gathered human responses and as \nsuch the augmented data. This example found correct clas-\nsification due to the terms ‘emotions’ and ‘mind’ primarily, \nfollowed by ‘feeling’. Following these two first examples, \nthe remaining five examples were strongly classified. In the \nmental state recognition task, even though the term ‘men-\ntal state’ was specifically uttered, the term ‘concentrating’ \nwas the strongest feature within the statement given the \ngoal of the algorithm to classify concentrating and relaxed \nstates of mind. As could be expected, the ‘JOKE’ task was \nbest classified by the term ‘joke’ itself being present, but, \ninterestingly, the confidence of classification was increased \nwith the phrases ‘Feeling sad today.’ and ‘cheer me up’. The \nscene classification task was confidently predicted with a \nprobability of 1 mainly due to the terms ‘look around’ and \nFig. 8  Normalised confusion matrix for the best command classifica-\ntion model, which was RoBERTa when trained on human data and \naugmented T5 paraphrased data\n3139\nChatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language…\n1 3\n‘where you are’. The red highlight for the word ‘if’ alone \nwould be classified as ‘SENTIMENT-ANALYSIS’ with a \nprobability of 0.518 given the popularity of phrases along \nthe lines of ‘if they are emotion or emotion’. The sentiment \nanalysis task was then, again, confidently classified correctly \nwith a probability of 1. This was due to the terms ‘received \nthis email’, ‘if’, and ‘sarcastic’ being present. Finally, the \nsign language task was also classified with a probability of \n1 most due to the features ‘voice’ and ‘sign’. The red features \nhighlighted, ‘speaking with please’ would alone be classi-\nfied as ‘CHAT’ with a probability of 0.956, since they are \nstrongly reminiscent to commands such as, ‘can we speak \nabout something please?’. An interesting behaviour to note \nfrom these examples is the previously described nature of \nreading. Transformer models are advancing the field of NLP \nin part thanks due to their lack of temporal restriction, ergo \nthe limitations existent within models such as Recurrent or \nLong Short Term Memory Neural Networks. This allows \nfor behaviours more similar to a human being, such as when \nsomeone may focus on certain key words first before glanc-\ning backwards for more context. Such behaviours are not \npossible with sequence-based text classification techniques.\n4.4  Transformer ensemble results\nFollowing the previous findings, the five strongest mod-\nels which were BERT (98.55%), DistilBERT (98.34%), \nRoBERTa (98.96%), Distil-RoBERTa (98.55%), and \nTable 6  The most confusing sentences according to the model (all of those with a loss > 1) and the probabilities as to which class they were \npredicted to belong to\nKey—C1: CHAT, C2: EEG-EMOTIONS, C3: EEG-MENTAL-STATE, C4: JOKE, C5: SCENE-RECOGNITION, C6: SENTIMENT-ANALY-\nSIS, C7: SIGN-LANGUAGE\nText “What is your favourite one liner?”\nActual C4\nPredicted C6\nLoss 6.24\nPrediction probabili-\nties\nC1 C2 C3 C4 C5 C6 C7\n0.0163 0.001 0 0.002 0.001 0.977 0.002\nText “What is your favourite movie?”\nActual C1\nPredicted C4\nLoss 2.75\nPrediction probabili-\nties\nC1 C2 C3 C4 C5 C6 C7\n0.064 0.0368 0.007 0.513 0.338 0.022 0.02\nText “How do I feel right now?”\nActual C1\nPredicted C4\nLoss 2.75\nPrediction probabili-\nties\nC1 C2 C3 C4 C5 C6 C7\n0.007 0.01 0.352 0.434 0.016 0.176 0.005\nText “Run emotion classification”\nActual C6\nPredicted C2\nLoss 1.71\nPrediction probabili-\nties\nC1 C2 C3 C4 C5 C6 C7\n0 0.672 0.001 0.002 0.004 0.32 0\nText “What is the valence of my brainwaves?”\nActual C2\nPredicted C3\nLoss 1.05\nPrediction probabili-\nties\nC1 C2 C3 C4 C5 C6 C7\n0.001 0.349 0.647 0.001 0.001 0.002 0\n3140 J. J. Bird et al.\n1 3\nXLM-RoBERTa (98.76%) are combined into a preliminary \nensemble strategy as previously described. XLM (14.81%) \nand XLNet (35.68%) are omitted due to their low clas -\nsification abilities. As noted, it was observed previously \nthat the best score by a single model was RoBERTa which \nscored 98.96% classification accuracy, and thus the main \ngoal of the statistical ensemble classifier is to learn pat-\nterns that could possibly account for making up some of \nthe 1.04% of errors and correct for them. Initially, Table  7 \nshows the information gain rankings of each predictor by \n10 fold cross validation on the training set alone, interest -\ningly BERT is ranked the highest with an information gain \nof 2.717 (± 0.002). Following this, the results in Table  8 \nshow the results for multiple statistical methods of ensem-\nbling the predictions of the five Transformer models (with \nthe best performing approaches highlighted in bold); all \nof the models with the exception of Gaussian Naïve Bayes \ncould outperform the best single Transformer model by \nan accuracy increase of at least 0.42 points. The two best \nmodels which achieved the same score were Logistic \nRegression and Random Forests, which when ensembling \nthe predictions of the five transformers, could increase \nthe accuracy by 0.63 points over RoBERTa and achieve \nan accuracy of 99.59%.\nFinally, Fig. 11 shows the confusion matrix for both the \nLogistic Regression and Random Forest methods of ensem-\nbling Transformer predictions since the errors made by \nboth models were identical. Many of the errors have been \nmitigated through ensembling the transformer models, with \nminor confusion occuring between the ‘CHAT’ and ‘JOKE’ \nclasses and the ‘SENTIMENT ANALYSIS’ and ‘EEG-\nEMOTIONS’ classes.\n5  Conclusion and future work\nThe studies performed in this work have shown primarily \nthat data augmentation through transformer-based paraphras-\ning via the T5 model have positively useful effects on many \nstate-of-the-art language transformer-based classification mod-\nels. BERT and DistilBERT, RoBERTa and DisilRoBERTa, \nXLM, XLM-RoBERTa, and XLNet all showed increases in \nlearning performance when learning with augmented data \nfrom the training set when compared to learning only on the \noriginal data pre-augmentation. The best single model found \nFig. 9  Exploration and explana-\ntion for the errors made during \nvalidation which had a loss > 1 \n(five such cases)\nPRED: ‘SENTIMENT-ANALYSIS’\nACTUAL: ‘JOKE’\n(probability 0.977, score 2.910)\nContribution Feature\n3.994 Highlighted in text \n(sum)\n-1.084 <BIAS>\nWhat is your favouriteo ne liner?\nPRED: ‘JOKE’\nACTUAL: ‘CHAT’\n(probability 0.513, score 0.837)\nContribution Feature\n1.73 Highlighted in text \n(sum)\n-0.893 <BIAS>\nWhat is yourf avourite movie?\nPRED: ‘JOKE’\nACTUAL: ‘CHAT’\n(probability 0.434, score -0.415)\nContribution Feature\n0.394 Highlighted in text \n(sum)\n-0.81 <BIAS>\nHowd oI feel right now?\nPRED: ‘EEG-EMOTIONS’\nACTUAL: ‘SENTIMENT-ANALYSIS’\n(probability 0.32, score -1.980)\nContribution Feature\n-0.595 <BIAS>\n-1.385 Highlighted in text \n(sum)\nRune motion classification\nPRED: ‘EEG-MENTAL-STATE’\nACTUAL: ‘EEG-EMOTIONS’\n(probability 0.647, score 1.147)\nContribution Feature\n2.004 Highlighted in text \n(sum)\n-0.857 <BIAS>\nWhat is thev alence of my brainwaves?\n3141\nChatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language…\n1 3\nwas RoBERTa, which could classify human commands to \nan artificially intelligent system at a rate of 98.96% accu-\nracy, where errors were often due to ambiguity within human \nlanguage. A statistical ensemble of the five best transformer \nmodels then led to an increase accuracy of 99.59% when using \neither Logistic Regression or a Random Forest to process the \noutput predictions of each transformer, utilising small differ-\nences between the models when trained on the dataset. Given \nthat several related works present XLM as a strong candidate \nfor different language-based problems with a focus on multi-\nlingual training, it is possibly the case that there is not enough \ndata to fine-tune XLM away from consideration of multiple \nlanguages and this leads to weak results when working with \nonly English language. Thus in future when several languages \nmay be considered as input to the system, XLM could be revis-\nited in order to explore this conjecture. Although XLM did not \nperform well, the promising performance of XLM-RoBERTa \nshowed that models trained on a task do not necessarily under \nFig. 10  Exploration of the best \nperforming model by presenting \nunseen sentences and explain-\ning predictions. Green denotes \nuseful features and red denotes \nfeatures useful for another class \n(detrimental to probability)\n‘CHAT’\n(probability 0.998, score 6.028)\nContribution Feature\n0.546 Highlighted in text \n(sum)\n0.482 <BIAS>\nWhat are you doingr ight now?H ave you\ngot time to speakt om e?\n‘EEG-EMOTIONS’\n(probability 0.929, score 2.406)\nContribution Feature\n3.128 Highlighted in text \n(sum)\n-0.722 <BIAS>\nRead my mind andt ellm ew hate motions\nIa mf eeling.\n‘EEG-MENTAL-STATE’\n(probability 1, score 9.605) \nContribution Feature\n10.483 Highlighted in text \n(sum)\n-0.878 <BIAS>\nRun EEG mental stater ecognitions oI can \nseei fI am concentrating?\n‘JOKE’\n(probability 1, score 10.705) \nContribution Feature\n11.17 Highlighted in text \n(sum)\n-0.465 <BIAS>\nFeelings ad today. Cany ou cheerm eu p\nwith a joke?\n‘SCENE-CLASSIFICATION’\n(probability 1, score 10.948) \nContribution Feature\n11.791 Highlighted in text \n(sum)\n-0.844 <BIAS>\nlook around ands ee if you cant ellm ew here\nyou are.\n‘SENTIMENT-ANALYSIS’\n(probability 1, score 10.378) \nContribution Feature\n11.031 Highlighted in text \n(sum)\n-0.653 <BIAS>\nIj ustr eceivedt hise mail.C an yout ellm e\nif it sounds sarcastict o you please?\n‘SIGN-LANGUAGE’\n(probability 1, score 10.186) \nContribution Feature\n10.889 Highlighted in text \n(sum)\n-0.703 <BIAS>\nRather than speaking with my voice, canw e\nsign insteadp lease?\nTable 7  Information Gain ranking of each predictor model by 10 fold \ncross validation on the training set\nPredictor model (trans-\nformer)\nAverage ranking Information \nGain of predic-\ntions\nBERT 1 (± 0) 2.717 (± 0.002)\nDistilBERT 2 (± 0) 2.707 (± 0.002)\nDistilRoBERTa 3.1 (± 0.3) 2.681 (± 0.001)\nRoBERTa 3.9 (± 0.3) 2.676 (± 0.003)\nXLM-RoBERTa 5 (± 0) 2.653 (± 0.002)\n3142 J. J. Bird et al.\n1 3\nperform on another different task given the general ability of \nlingual understanding. With this in mind, and given that the \nmodels are too complex to train simultaneously, it may be use-\nful in the future to explore other methods of ensembling the \npredictions such as the addition of the original text alongside \nprediction vectors, which may allow for deeper understanding \nbehind why errors are made and allow for further NLP-based \nrules to overcome them. A preliminary ensemble of the five \nstrongest models showed that classification accuracy could be \nfurther increased by treating the outputs of each transformer \nmodel as attributes in themselves, for rules to be learnt from. \nThe experiment was limited in that attribute selection was \nbased solely on removing the two under performing models; in \nfuture, exploration could be performed into attribute selection \nto fine-tune the number of models used as input. Additionally, \nonly a predicted labels in the form of nominal attributes were \nused as input, whereas additional attributes such as probabili-\nties of each output class could be utilised in order to provide \nmore information for the statistical ensemble classifier. The \ndata in this work was split 70/30 and paraphrasing was exe-\ncuted on the 70% of training data only in order not to expose \na classification model to paraphrased text of data contained in \nthe testing set. This is performed in order to prevent training \ndata possibly baring strong similarity to test data (since the \noutput of the T5 may or may not be very similar to the input, \nand is difficult to control in this regard). In future, metrics \nsuch as the accuracy, precision, recall, and F1 scores etc. could \nbe made more scientifically accurate based on the knowledge \ngained from this study by performing K-fold Cross Validation \nor even Leave One Out Cross Validation if the computational \nresources are available to do so.\n6  Ethics\nAll users who answered the questionnaire agreed to the fol-\nlowing statement:\nThe data collected from this form will remain completely \nanonymous and used for training a transformation-based \nchatbot. The more examples of a command or statement the \nbot can observe, the more accurate it will be at giving the \ncorrect response. The responses will be expanded by explor-\ning paraphrases of answers and then further transformed by \na model pre-trained on a large corpus of text and fine-tuned \non the goal-based statements and requests given here.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nTable 8  Results for the \nensemble learning of \nTransformer predictions \ncompared to the best single \nmodel (RoBERTa)\nEnsemble method Accuracy Precision Recall F1 Difference \nover RoB-\nERTa\nLogistic Regression 99.59 0.996 0.996 0.996 0.63\nRandom Forest 99.59 0.996 0.996 0.996 0.63\nMultinomial Naïve Bayes 99.38 0.994 0.994 0.994 0.42\nBernoulli Naïve Bayes 99.38 0.994 0.994 0.994 0.42\nLinear Discriminant Analysis 99.38 0.994 0.994 0.994 0.42\nXGBoost 99.38 0.994 0.994 0.994 0.42\nSupport Vector Classifier 99.38 0.994 0.994 0.994 0.42\nBayesian Network 99.38 0.994 0.994 0.994 0.42\nGaussian Naïve Bayes 98.55 0.986 0.985 0.986 – 0.41\nFig. 11  Normalised confusion matrix for the best ensemble methods \nof Logistic Regression and Random Forest (errors made by the two \nwere identical)\n3143\nChatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language…\n1 3\nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAbadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Cor -\nrado GS, Davis A, Dean J, Devin M, Ghemawat S, Goodfellow \nI, Harp A, Irving G, Isard M, Jia Y, Jozefowicz R, Kaiser L, \nKudlur M, Levenberg J, Mané D, Monga R, Moore S, Murray \nD, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar \nK, Tucker P, Vanhoucke V, Vasudevan V, Viégas F, Vinyals \nO, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X (2015) \nTensorFlow: large-scale machine learning on heterogeneous \nsystems. https:// www. tenso rflow. org/, software available from \ntensorflow.org\nBannard C, Callison-Burch C (2005) Paraphrasing with bilingual par-\nallel corpora. In: Proceedings of the 43rd Annual Meeting of the \nAssociation for Computational Linguistics (ACL’05), pp 597–604\nBiedert R, Hees J, Dengel A, Buscher G (2012) A robust realtime \nreading-skimming classifier. In: Proceedings of the Symposium \non Eye Tracking Research and Applications, pp 123–130\nBird JJ, Ekárt A, Faria DR (2018a) Learning from interaction: An \nintelligent networked-based human-bot and bot-bot chatbot sys-\ntem. In: UK workshop on computational intelligence, Springer, \npp 179–190\nBird JJ, Manso LJ, Ribeiro EP, Ekart A, Faria DR (2018b) A study on \nmental state classification using EEG-based brain-machine inter-\nface. In: 2018 International conference on intelligent systems (IS), \nIEEE, pp 795–800\nBird JJ, Ekart A, Buckingham C, Faria DR (2019a) Mental emotional \nsentiment classification with an EEG-based brain-machine inter -\nface. In: Proceedings of the International Conference on Digital \nImage and Signal Processing (DISP’19)\nBird JJ, Ekart A, Buckingham CD, Faria DR (2019b) High resolu -\ntion sentiment analysis by ensemble classification. In: Intelligent \ncomputing-proceedings of the computing conference, Springer, \npp 593–606\nBird JJ, Ekárt A, Faria DR (2020a) British sign language recognition \nvia late fusion of computer vision and leap motion with transfer \nlearning to American sign language. Sensors 20(18):5151\nBird JJ, Faria DR, Premebida C, Ekárt A, Vogiatzis G (2020b) Look \nand listen: a multi-modality late fusion approach to scene classifi-\ncation for autonomous machines. arXiv preprint arXiv: 20071 0175\nBollweg L, Kurzke M, Shahriar KA, Weber P (2018) When robots \ntalk-improving the scalability of practical assignments in MOOCS \nusing chatbots. In: EdMedia+ innovate learning, association \nfor the advancement of computing in education (AACE), pp \n1455–1464\nCandello H, Pinhanez C, Pichiliani MC, Guerra MA, Gatti de Bayser M \n(2018) Having an animated coffee with a group of chatbots from \nthe 19th century. In: Extended abstracts of the 2018 CHI confer -\nence on human factors in computing systems, pp 1–4\nChada R (2020) Simultaneous paraphrasing and translation by fine-\ntuning transformer models. arXiv preprint arXiv: 20050 5570\nChang E (2020) Ellachang/T5-paraphraser: modified version of Goog-\nle’s t5 model that produces paraphrases of a given input sentence. \nhttps:// github. com/ EllaC hang/ T5- Parap hraser, (Accessed on \n08/11/2020)\nChang WC, Yu HF, Zhong K, Yang Y, Dhillon I (2019) X-bert: extreme \nmulti-label text classification with using bidirectional encoder rep-\nresentations from transformers. arXiv preprint arXiv: 19050 2331\nChollet F, et al. (2015) Keras. https:// keras. io\nCiechanowski L, Przegalinska A, Magnuski M, Gloor P (2019) In the \nshades of the uncanny valley: an experimental study of human-\nchatbot interaction. Futur Gener Comput Syst 92:539–548\nConneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, \nGuzmán F, Grave E, Ott M, Zettlemoyer L, Stoyanov V (2019) \nUnsupervised cross-lingual representation learning at scale. arXiv \npreprint arXiv: 19110 2116\nConneau A, Lample G (2019) Cross-lingual language model pretrain-\ning. In: Advances in Neural Information Processing Systems, pp \n7059–7069\nDevlin J, Chang MW, Lee K, Toutanova K (2018) Bert: pre-training of \ndeep bidirectional transformers for language understanding. arXiv \npreprint arXiv: 18100 4805\nDevlin J, Chang MW (2018) Open sourcing bert: State-of-the-art pre-\ntraining for natural language processing. Google AI Blog Weblog. \nhttps:// aigoo glebl og. com/ 2018/ 11/ open- sourc ing- berts tate- of- art- \npre html (Accessed 4 Dec 2019)\nDi Gangi MA, Negri M, Turchi M (2019) Adapting transformer to \nend-to-end spoken language translation. In: INTERSPEECH \n2019, International Speech Communication Association (ISCA), \npp 1133–1137\nDimovski M, Musat C, Ilievski V, Hossmann A, Baeriswyl M (2018) \nSubmodularity-inspired data selection for goal-oriented chatbot \ntraining based on sentence embeddings. arXiv preprint arXiv:  \n18020 0757\nEckstein G, Schramm W, Noxon M, Snyder J (2019) Reading l1 and l2 \nwriting: an eye-tracking study of tesol rater behavior. TESL-EJ \n23(1):n1\nGoodfellow I, Bengio Y, Courville A (2017) Deep learning (adaptive \ncomputation and machine learning series). Cambridge Massachu-\nsetts, pp 321–359\nHaller E, Rebedea T (2013) Designing a chat-bot that simulates an \nhistorical figure. In: 2013 19th international conference on control \nsystems and computer science, IEEE, pp 582–589\nHochreiter S, Schmidhuber J (1997) Long short-term memory. Neural \nComput 9(8):1735–1780\nHou Y, Liu Y, Che W, Liu T (2018) Sequence-to-sequence data aug-\nmentation for dialogue language understanding. arXiv preprint \narXiv: 18070 1554\nHoward J, Ruder S (2018) Universal language model fine-tuning for \ntext classification. arXiv preprint arXiv: 18010 6146\nJin L, King D, Hussein A, White M, Danforth D (2018) Using para-\nphrasing and memory-augmented models to combat data sparsity \nin question interpretation with a virtual patient dialogue system. \nIn: Proceedings of the thirteenth workshop on innovative use of \nNLP for building educational applications, pp 13–23\nKerlyl A, Hall P, Bull S (2006) Bringing chatbots into education: \ntowards natural language negotiation of open learner models. In: \nInternational Conference on Innovative Techniques and Applica-\ntions of Artificial Intelligence, Springer, pp 179–192\nKruger JL, Steyn F (2014) Subtitles and eye tracking: reading and \nperformance. Read Res Q 49(1):105–120\nKunze K, Ishimaru S, Utsumi Y, Kise K (2013) My reading life: \ntowards utilizing eyetracking on unmodified tablets and phones. \nIn: Proceedings of the 2013 ACM conference on Pervasive and \nubiquitous computing adjunct publication, pp 283–286\nLeonhardt MD, Tarouco L, Vicari RM, Santos ER, da Silva MdS \n(2007) Using chatbots for network management training through \nproblem-based oriented education. In: Seventh IEEE international \nconference on advanced learning technologies (ICALT 2007), \nIEEE, pp 845–847\n3144 J. J. Bird et al.\n1 3\nLewis M, Ghazvininejad M, Ghosh G, Aghajanyan A, Wang S, Zet-\ntlemoyer L (2020) Pre-training via paraphrasing. arXiv preprint \narXiv: 20061 5020\nLiu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zet-\ntlemoyer L, Stoyanov V (2019) Roberta: A robustly optimized bert \npretraining approach. arXiv preprint arXiv: 19071 1692\nLukovnikov D, Fischer A, Lehmann J (2019) Pretrained transformers \nfor simple question answering over knowledge graphs. In: Inter -\nnational semantic web conference, Springer, pp 470–486\nMaiya AS (2020) ktrain: a low-code library for augmented machine \nlearning arXiv arXiv: 2004. 10703 [cs.LG]\nManurung R, Ritchie G, Pain H, Waller A, O’Mara D, Black R (2008) \nThe construction of a pun generator for language skills develop-\nment. Appl Artif Intell 22(9):841–869\nMarquis A, Kaabi MA, Leung T, Boush F (2020) What the eyes hear: \nan eye-tracking study on phonological awareness in emirati arabic. \nFront Psychol 11:452\nMarton Y, Callison-Burch C, Resnik P (2009) Improved statistical \nmachine translation using monolingually-derived paraphrases. \nIn: Proceedings of the 2009 conference on empirical methods in \nnatural language processing, pp 381–390\nNaseem U, Razzak I, Musial K, Imran M (2020) Transformer based \ndeep intelligent contextual embedding for twitter sentiment anal-\nysis. Future Gener Comput Syst 113:58–69. https:// doi. org/ 10.  \n1016/j. future. 2020. 06. 050\nPaszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen \nT, Lin Z, Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, \nDeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang \nL, Bai J, Chintala S (2019) Pytorch: An imperative style, high-\nperformance deep learning library. In: Wallach H, Larochelle H, \nBeygelzimer A, d’ Alché-Buc F, Fox E, Garnett R (eds) Advances \nin neural information processing systems 32, Curran Associates, \nInc., pp 8024–8035, http:// papers. neuri ps. cc/ paper/ 9015- pytor ch- \nan- imper ative- style- high- perfo rmance- deep- learn ing- libra ry. pdf\nPedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel \nO, Blondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas \nJ, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E \n(2011) Scikit-learn: machine learning in Python. J Mach Learn \nRes 12:2825–2830\nPetrović S, Matthews D (2013) Unsupervised joke generation from big \ndata. In: Proceedings of the 51st annual meeting of the associa-\ntion for computational linguistics (volume 2: short papers), pp \n228–232\nQi D, Su L, Song J, Cui E, Bharti T, Sacheti A (2020) Imagebert: cross-\nmodal pre-training with large-scale weak-supervised image-text \ndata. arXiv preprint arXiv: 20010 7966\nQuora (2017) Quora question pairs | kaggle. https:// www. kaggle. com/c/ \nquora- quest ion- pairs, (Accessed on 08/11/2020)\nRadford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) \nLanguage models are unsupervised multitask learners. OpenAI \nblog 1(8):9\nRaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, \nLi W, Liu PJ (2019) Exploring the limits of transfer learning with \na unified text-to-text transformer. arXiv preprint arXiv: 19101 0683\nRoller S, Boureau YL, Weston J, Bordes A, Dinan E, Fan A, Gunning \nD, Ju D, Li M, Poff S et al (2020) Open-domain conversational \nagents: current progress open problems, and future directions. \narXiv preprint arXiv: 20061 2442\nSanh V, Debut L, Chaumond J, Wolf T (2019) Distilbert, a distilled \nversion of bert: smaller, faster, cheaper and lighter. arXiv preprint \narXiv: 19100 1108\nSchmidhuber J (1992) Learning complex, extended sequences using the \nprinciple of history compression. Neural Comput 4(2):234–242\nShagass C, Roemer RA, Amadeo M (1976) Eye-tracking performance \nand engagement of attention. Arch Gen Psychiatry 33(1):121–125\nShangipour ataei T, Javdan S, Minaei-Bidgoli B (2020) Applying \ntransformers and aspect-based sentiment analysis approaches on \nsarcasm detection. In: Proceedings of the second workshop on \nfigurative language processing, association for computational lin-\nguistics, pp 67–71, https:// doi. org/ 10. 18653/ v1/ 2020. figla ng-1.9, \nhttps:// www. aclweb. org/ antho logy/ 2020. figla ng-1.9\nShao T, Guo Y, Chen H, Hao Z (2019) Transformer-based neural net-\nwork for answer selection in question answering. IEEE Access \n7:26146–26156\nShleifer S (2019) Low resource text classification with ulmfit and back-\ntranslation. arXiv preprint arXiv: 19030 9244\nStephens KR (2002) What has the loebner contest told us about con-\nversant systems. Retrieved August 2021\nSun C, Qiu X, Xu Y, Huang X (2019) How to fine-tune bert for text \nclassification? In: China National Conference on Chinese Com-\nputational Linguistics, Springer, pp 194–206\nSun L, Xia C, Yin W, Liang T, Yu PS, He L (2020) Mixup-transfomer: \nDynamic data augmentation for nlp tasks. arXiv preprint arXiv:  \n20100 2394\nTenney I, Das D, Pavlick E (2019) Bert rediscovers the classical nlp \npipeline. arXiv preprint arXiv: 19050 5950\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez \nAN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. \nIn: Advances in neural information processing systems, pp \n5998–6008\nVirkar M, Honmane V, Rao SU (2019) Humanizing the chatbot with \nsemantics based natural language generation. In: 2019 Interna-\ntional Conference on Intelligent Computing and Control Systems \n(ICCS), IEEE, pp 891–894\nWang CC, Hung JC, Chen SN, Chang HP (2019) Tracking students’ \nvisual attention on manga-based interactive e-book while read-\ning: an eye-movement approach. Multimedia Tools Appl \n78(4):4813–4834\nWang Q, Li B, Xiao T, Zhu J, Li C, Wong DF, Chao LS (2019b) Learn-\ning deep transformer models for machine translation. arXiv pre-\nprint arXiv: 19060 1787\nWang A, Singh A, Michael J, Hill F, Levy O, Bowman SR (2018) \nGlue: A multi-task benchmark and analysis platform for natural \nlanguage understanding. arXiv preprint arXiv: 18040 7461\nWei J, Zou K (2019) Eda: Easy data augmentation techniques for boost-\ning performance on text classification tasks. arXiv preprint arXiv: \n19011 1196\nWilliams RJ, Zipser D (1989) A learning algorithm for continu -\nally running fully recurrent neural networks. Neural Comput \n1(2):270–280\nWolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac \nP, Rault T, Louf R, Funtowicz M, Brew J (2019) Huggingface’s \ntransformers: state-of-the-art natural language processing. arXiv: \n1910. 03771\nYang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV (2019) \nXlnet: Generalized autoregressive pretraining for language under-\nstanding. In: Advances in neural information processing systems, \n32\nZhang C, Ma Y (2012) Ensemble machine learning: methods and appli-\ncations. Springer, Berlin\nZhang J, Luan H, Sun M, Zhai F, Xu J, Zhang M, Liu Y (2018) Improv-\ning the transformer translation model with document-level con-\ntext. arXiv preprint arXiv: 18100 3581\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}