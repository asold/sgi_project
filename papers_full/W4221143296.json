{
  "title": "ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification",
  "url": "https://openalex.org/W4221143296",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2112124531",
      "name": "Zhou Yucheng",
      "affiliations": [
        "University of Technology Sydney",
        "Australian Institute of Business"
      ]
    },
    {
      "id": "https://openalex.org/A2014104297",
      "name": "Tao Shen",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2167037537",
      "name": "Xiubo Geng",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2140909072",
      "name": "Guodong Long",
      "affiliations": [
        "Australian Institute of Business",
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2123654898",
      "name": "Daxin Jiang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963993699",
    "https://openalex.org/W3114178062",
    "https://openalex.org/W4288262459",
    "https://openalex.org/W4293547730",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3176032431",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962714848",
    "https://openalex.org/W3098938591",
    "https://openalex.org/W2970453125",
    "https://openalex.org/W2888213795",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W3174225409",
    "https://openalex.org/W2914855263",
    "https://openalex.org/W3012590175",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3104007871",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3175910413",
    "https://openalex.org/W3119793337",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3137214022",
    "https://openalex.org/W2964080504",
    "https://openalex.org/W3035744524",
    "https://openalex.org/W2758815496",
    "https://openalex.org/W2915085887",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3123123873",
    "https://openalex.org/W2758362814",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2964669873",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3117246841",
    "https://openalex.org/W2889002152",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W3118202953",
    "https://openalex.org/W3103451137",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4302760329",
    "https://openalex.org/W2970169050",
    "https://openalex.org/W3173538745",
    "https://openalex.org/W2886970679",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3173937547",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3174704627",
    "https://openalex.org/W2963101081"
  ],
  "abstract": "Generating new events given context with correlated ones plays a crucial role in many event-centric reasoning tasks. Existing works either limit their scope to specific scenarios or overlook event-level correlations. In this paper, we propose to pre-train a general Correlation-aware context-to-Event Transformer (ClarET) for event-centric reasoning. To achieve this, we propose three novel event-centric objectives, i.e., whole event recovering, contrastive event-correlation encoding and prompt-based event locating, which highlight event-level correlations with effective training. The proposed ClarET is applicable to a wide range of event-centric reasoning scenarios, considering its versatility of (i) event-correlation types (e.g., causal, temporal, contrast), (ii) application formulations (i.e., generation and classification), and (iii) reasoning types (e.g., abductive, counterfactual and ending reasoning). Empirical fine-tuning results, as well as zero- and few-shot learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations), verify its effectiveness and generalization ability.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2559 - 2575\nMay 22-27, 2022c‚Éù2022 Association for Computational Linguistics\nClarET: Pre-training a Correlation-Aware Context-To-Event Transformer\nfor Event-Centric Generation and Classification\nYucheng Zhou1‚àó, Tao Shen2, Xiubo Geng2‚Ä†, Guodong Long1, Daxin Jiang2‚Ä†\n1Australian AI Institute, School of CS, FEIT, University of Technology Sydney\n2Microsoft\nyucheng.zhou.uts@gmail.com, guodong.long@uts.edu.au\n{shentao, xigeng, djiang}@microsoft.com\nAbstract\nGenerating new events given context with cor-\nrelated ones plays a crucial role in many event-\ncentric reasoning tasks. Existing works either\nlimit their scope to specific scenarios or over-\nlook event-level correlations. In this paper,\nwe propose to pre-train a general Correlation-\naware context-to-Event Transformer (ClarET)\nfor event-centric reasoning. To achieve this, we\npropose three novel event-centric objectives,\ni.e., whole event recovering, contrastive event-\ncorrelation encoding and prompt-based event\nlocating, which highlight event-level correla-\ntions with effective training. The proposed\nClarET is applicable to a wide range of event-\ncentric reasoning scenarios, considering its\nversatility of (i) event-correlation types (e.g.,\ncausal, temporal, contrast), (ii) application for-\nmulations (i.e., generation and classification),\nand (iii) reasoning types (e.g., abductive, coun-\nterfactual and ending reasoning). Empirical\nfine-tuning results, as well as zero- and few-\nshot learning, on 9 benchmarks (5 generation\nand 4 classification tasks covering 4 reasoning\ntypes with diverse event correlations), verify its\neffectiveness and generalization ability.\n1 Introduction\nAn ‚Äòevent‚Äô, usually a text span composed of a pred-\nicate and its arguments (Zhang et al., 2020b), is\na fine-grained semantic unit to describe the state\nof entities/things (e.g., He looks very worried) and\nhow they act (e.g., I grab his arms). Understanding\nevents and modeling their correlations are funda-\nmental to many reasoning tasks (Bhagavatula et al.,\n2020; Qin et al., 2019), e.g., abductive reasoning,\nstory ending classification and generation, counter-\nfactual reasoning, script reasoning. For instance, in\nthe left example of Figure 1, to generate the miss-\ning event [E] in the given context, it is essential\nto understand that there are four events (‚Äò it tries\n‚àóWork is done during internship at Microsoft.\n‚Ä†Corresponding author.\nContext: \nIt tries the knob but [E], so\nthe creature starts pounding \non the door to break it down.\nParagraph ùë•:\nIt tries the knob but it‚Äôs locked, so \nthe creature starts pounding on ‚Ä¶\nAn event mention ùëí:\nit‚Äôs locked\nNegative events { “ßùëí}ùëñ=1\nùëÄ :\nit‚Äôs smoked      he‚Äôs gone     ‚Ä¶\nOutput: \nit‚Äôs locked.\nFigure 1: Left: an example of abductive reasoning which\naims to generate the missing event [E] given correlated events\n(underlined) and connectives (w/ orange) in the context.Right:\na toy example (x, e,{¬Øe}M\ni=1) of event-rich data for better read-\ning. See Appendix A for real examples to pre-train our model.\nthe knob‚Äô, [E], ‚Äòthe creature starts pounding on the\ndoor‚Äô, and ‚Äò(the creature) to break it down‚Äô), and\nthen predict [E] based on the other three events and\nits correlations to them (i.e., the contrast relation\nindicated by ‚Äòbut‚Äô and the causal relation by ‚Äòso‚Äô).\nEvent-aware reasoning has gained much atten-\ntion and achieved promising success in recent years\n(Lv et al., 2020; Ding et al., 2019). However, many\nalgorithms are designed to solve only some specific\ntasks. For example, Qin et al. (2020) propose to\nimprove unsupervised decoding for counterfactual\nand abductive reasoning; Huang et al. (2021) and\nGuan et al. (2019) advance story ending generation\nvia incremental encoding and multi-level graph\nconvolutional networks. Although these works\nshow effectiveness in corresponding applications,\nthey are limited to specific scenarios, and cannot\ngeneralize well to a broad scope of reasoning.\nMeanwhile, some pioneering works follow a re-\ncently arising paradigm to conduct event-based pre-\ntraining for those downstream reasoning tasks (Yu\net al., 2020; Han et al., 2020a; Lin et al., 2020;\nZhou et al., 2021b). However, these solutions have\ntheir own limitations: COMeT (Hwang et al., 2021)\nlearns event correlations from a human-curated\nknowledge graph and thus limits its scalability. Han\net al. (2020a) and Lin et al. (2020) only model tem-\nporal relations and cannot be expanded to other re-\nlations (e.g., causal, contrast). EventBERT (Zhou\net al., 2021b) is proposed for event-based classifi-\n2559\ncations and is thus inapplicable to generation tasks.\nIn this work, we propose a general pre-training\nframework for event-centric reasoning by learning\na Correlation-aware context-to-Event Transformer\n(ClarET) from an event-rich text corpus. We pro-\npose three novel self-supervised objectives, dubbed\nas whole event recovering (WER), contrastive\nevent-correlation encoding and prompt-based event\nlocating, respectively. The first one aims to capture\nevent correlation by recovering a whole event from\nits masked context. The second one enhances the\nrepresentation of the masked event in WER by con-\ntrasting it with the gold event against the negative\nones. The last one is a simplified WER task by\nproviding hints in its prompt and thus facilitates\neffective learning for WER.\nClarET explicitly models event correlations and\ncontributes to various scenarios. From one aspect,\nit covers a variety of correlation types (e.g., causal,\ntemporal, contrast) attributed to correlation type-\nagnostic objectives. From another aspect, it is ap-\nplicable to both generation and classification task\nformulations by its unified structure. Lastly, it high-\nlights event-level correlations and thus is more ef-\nfective for diverse event-centric tasks, e.g., abduc-\ntive, counterfactual and ending reasoning.\nTo evaluate ClarET, we compare it with strong\nbaselines on 9 diverse benchmarks. While ClarET\nis continually pre-trained from BART (Lewis et al.,\n2020) with very limited extra resources, i.e., train-\ning on a small subset of BART-used corpus (i.e.,\n200M out of 2.2T tokens) within 90 GPU hours\n(only 0.13% of 70,000h BART pre-training), it\nachieves state-of-the-art (SoTA) performance on all\n5 generation benchmarks. It also outperforms all\nunified models on 4 classification benchmarks and\nachieves competitive, or even better, accuracy to\nstrong discriminative baselines. We further exhibit\nthat the ClarET provides a good initialization for\ndownstream tasks by zero- and few-shot learning.\n2 Related Work\nUnified Pre-trained Model. A recent trend is\nto pre-train unified (a.k.a. universal or general)\nmodels to boost downstream generation and clas-\nsification tasks, rather than masked language mod-\neling (MLM) only. GPT (Radford et al., 2019) is\nbased on auto-regressive language modeling but\nincompetent in classifications due to unidirectional\ncontextualizing. To remedy this, BART (Lewis\net al., 2020) trains seq2seq models as a text denois-\ning autoencoder with mask-infilling, etc; UniLM\n(Dong et al., 2019) designs advanced self-attention\nmasks in Transformer, leading to a partially auto-\nregressive MLM; GLM (Du et al., 2021) proposes\nan auto-regressive blank-filling objective based on\nTransformer, achieved by bi-/uni-directional atten-\ntion and 2D positional encoding. T5 (Raffel et al.,\n2020) pre-trains a text-to-text Transformer to re-\ncover the masked part of input by decoding. All\nthese general-purpose pre-trained models focus on\nrelatively short-span masking in random, whereas\nwe focus on masking a whole semantic unit (i.e.,\nevent) and propose novel training objectives to\ncircumvent problems in long-span event decod-\ning. Besides, they are also vulnerable to pretrain-\nfinetune inconsistency, leading to inferior event-\ncentric performance.\nTask-specific Models for Event Reasoning.\nMany recent works present task-specific neural\nmodels for various event-centric reasoning types,\nincluding (1) abductive reasoning (Ji et al., 2020;\nDong et al., 2021; Zhu et al., 2020), (2) counterfac-\ntual reasoning (Qin et al., 2019, 2020), (3) ending\nreasoning (Guan et al., 2019; Wang and Wan, 2019;\nYao et al., 2019; Huang et al., 2021; Guan et al.,\n2020; Wang et al., 2017; Li et al., 2018; Ding et al.,\n2019; Zhou et al., 2021c; Chaturvedi et al., 2017;\nSrinivasan et al., 2018), (4) incoherence reason-\ning (Mori et al., 2020). However, these methods\nare designed for the specific reasoning scenarios\nbased on task-specific models so hardly generalize\nto other scenarios. In contrast, we aim to pre-train\na general event-centric model for generalizing to\nvarious scenarios.\nEvent-centric Pre-training. With similar scopes,\nmany works focus on event-centric pre-training\nto promote event-related tasks as ‚Äòevent‚Äô is a self-\ncontained semantic unit and also an entry of com-\nmonsense reasoning. One paradigm is to pre-train\non corpora without human-labeling. Some methods\nfocus on more specific aspects of events and their\ncorrelations. DEER (Han et al., 2020b) performs\ntemporal and event masking predictions for tempo-\nral relations. Lin et al. (2021) propose to recover a\ntemporally-disordered or event-missing sequence\nfor temporal and causal relations. Wang et al.\n(2021) use AMR structure to design contrastive ob-\njectives for the event detection task. However, they\nare not general enough to various event reasoning\ntasks. In contrast, CoCoLM (Yu et al., 2020) learns\n2560\nan event-level MLM to generalize more. Event-\nBERT (Zhou et al., 2021b) states the ineffective-\nness of event-level MLM and exploits hard nega-\ntives via contrasting, contributing much to down-\nstream multi-choice tasks. However, these methods\nare only competent in discriminative tasks. The\nother paradigm is based on supervised pre-training\non similar tasks and then performs knowledge trans-\nfer, e.g., COMeT (Hwang et al., 2021), UnifiedQA\n(Khashabi et al., 2020) and UNICORN (Lourie\net al., 2021), but they require human-curated data.\nEvent-rich Corpus. Although raw corpora are\nviewed as off-the-shelf pre-training resources, a\nkey question is how to mine event-rich examples.\nHere, ‚Äòevent-rich‚Äô denotes that each example con-\ntains various events and entails adequate contexts\nto support event reasoning via either explicit or im-\nplicit event-correlation. This is crucial to learning\nevent-correlations and reducing unnecessary over-\nheads. Except for human-curated resources (e.g.,\nATOMIC (Sap et al., 2019) and ConceptNet (Speer\net al., 2017)), event-rich corpora are also mined via\nautomatic schemes. ASER (Zhang et al., 2020b)\nbuilds an event-based graph, where each node is an\nevent extracted from a text and the relation of an\nevent pair is predicted by a PDTB model. In con-\ntrast, EventBERT (Zhou et al., 2021b) operates on\npure text so filters out correlation-scarce contexts\nand extracts verb-rooted events. Besides, it offers\nevent sampling methods for hard negatives. We\nadopt this data processing method as both pure-text\nexamples and hard negatives are prerequisites of\ngeneric and robust pre-training.\n3 Methodology\n3.1 Prerequisite: Event-rich Corpus\nIn this work, we directly adopt event-rich data min-\ning and negative sampling methods from Zhou et al.\n(2021b) but focus our contributions on enlarging\napplication scope of event-centric tasks and over-\ncoming challenges raised in the new scope.\nEvent-rich Data Mining. To mine event-rich\ndata from raw corpus, we employ a story corpus,\nBOOK CORPUS (Zhu et al., 2015), and take a two-\nstep procedural (i.e., ‚Äò filter‚Äô and ‚Äòextraction‚Äô). It\nfilters out correlation-scarce paragraphs according\nto existence of connectives (i.e., discourse relation\nkeywords, e.g., however, while). Then, it highlights\nthe event spans in the filtered paragraphs byextract-\ning verb-rooted sub-trees in dependency trees of\nthe paragraphs. With a filtered paragraph x, we\nbuild each example as (x, e) where e is an event\nmention in x. We obtain 200M tokens (out of 1B in\nBOOK CORPUS ) in 3.9M filtered paragraphs. For\nclear notations, we denote a text piece as a lower\ncase letter (e.g., e). It is tokenized into a sequence\nas a bold (e.g., e = [e1, e2, . . .]), where a letter w/\nsubscript t is the t-th token in the sequence.\nNegative Event Sampling. Following Zhou et al.\n(2021b), we build a pool of events from the whole\ncorpus and then retrieve negative events by three\nheuristic schemes. Given an event e in (x, e), we\nsample its negative event, ¬Øe, in light of lexicon-\nbased (20% time), PoS-based (60% time) or in-\ndomain (20% time) retrieval. Consequently, given\nan event e, we sample M negative events, i.e.,\n{¬Øe}M\ni=1. Figure 1 (right) shows an integrated in-\nstance (x, e,{¬Øe}M\ni=1) of the event-rich corpus1.\n3.2 Pre-training Objectives\nWe first present whole event recovering as a back-\nbone pre-training objective in ¬ß3.2.1. After iden-\ntifying incompetence of the simple backbone, we\npropose two other objectives in ¬ß3.2.2 and ¬ß3.2.3.\nAn overview of the objectives is shown in Figure 2.\n3.2.1 Whole Event Recovering\nFor the objective of whole event recovering (WER),\nit is straightforward to leverage an encoder-decoder\nstructure, where a masked context is passed into the\nencoder to generate the missing part by decoding.\nSpecifically, given an evente in a paragraph x, we\nmask out e from x at the encoder side and then\ngenerate e at the decoder side, i.e.,\np(e|x/{e}; Œ∏) =\nY\nt\np(et|e<t, x/{e}; Œ∏), (1)\nwhere Œ∏ denotes parameters and x/{e} denotes re-\nplacing e in x with one special token [M]. We\nestimate Eq. (1) by the Transformer sequence-to-\nsequence (seq2seq) structure (Vaswani et al., 2017).\nFirst, we apply the Transformer encoder to x/{m}\nfor contextual embeddings for all tokens in x/{m}:\nH(enc)=Trans-Enc(x/{e}; Œ∏(enc)) ‚àà Rd√ón, (2)\nwhere n is the number of tokens in x/{e}. Then,\nthe Transformer decoder is employed to predict all\ntokens e of the event e in a recurrent manner, i.e.,\nÀúyt = Trans-Dec(e<t, H(enc); Œ∏(dec))‚ààR|V|, (3)\n1Experimental codes are released at https://github.\ncom/yczhou001/ClarET.\n2561\nFigure 2: An overview of self-supervised objectives for our\nCorrelation-aware context-to-Event Transformer (ClarET).\nwhere V denotes token vocabulary and Àúyt is the\npredicted categorical distribution over V. Lastly,\nthe training objective is defined as a maximum\nlikelihood estimation. Its loss function is written as\nL(wer) = ‚àí\nX\n(x,e)\n1\n|e|\nX|e|\nt=1\nlog Àúyt[y = et], (4)\nwhere ‚Äòyt[y = et]‚Äô denotes fetching the probability\nof the t-step gold token et ‚àà e from Àúyt.\nThis objective is similar to span recovering\nschema (Raffel et al., 2020; Joshi et al., 2020) but\ndiffers in that (i) each masked span is an event, i.e.,\nan integrated semantic unit, so much longer (up\nto 22 tokens and see Figure 4 for length distribu-\ntion), and (ii) only one event is masked out from\nthe context to facilitate event-correlation modeling\nbetween the event and its contexts.\nIntuitively, the success of Eq. (1) requires to\ncapture correlations between the masked event and\nremaining contexts but two major problems arise\ndue to WER with long event-level masking spans:\n(1) Implicit Event-correlation: The model recov-\ners an event based solely on token-level concur-\nrence as in a conditional language model (e.g., T5\nand BART), regardless of the rich event-level corre-\nlations between the events in context x/{e} and the\nmasked event e. Such a correlation-implicit model\nwould achieve inferior performance on downstream\nevent-centric correlation reasoning tasks.\n(2) Learning Difficulty: As the masked event\nis an integrated, self-contained, semantic unit, it\nis difficult for the conditional generation model\nto recover the whole event due to a lack of local\ncontexts. As a result, the model cannot effectively\nlearn from the long masked spans, which has been\nempirically proved in autoencoding MLM models.\nTo alleviate the two problems above, we propose\ntwo other novel self-supervised objectives in the\nfollowing. Briefly, we present contrastive event-\ncorrelation encoding to enhance correlations be-\ntween contexts and events, and prompt-based event\nlocating to reduce generation difficulty.\n3.2.2 Contrastive Event-correlation Encoding\nFor the implicit event-correlation problem, an intu-\nitive solution is to explicitly highlight the correla-\ntion from the masked context to the missing event\nat the encoder side. To achieve this, we resort to\ncontrastive learning to enhance the encoder-side\nrepresentation of the masked event by contrasting\nit with the embedding of the gold event mention e\nagainst those of negative ones ¬Øe. Particularly, we\nfirst derive the embedding of e and ¬Øe independently\nvia the Transformer encoder in Eq.(2), i.e.,\nc = Pool(Trans-Enc([CLS] + e; Œ∏(enc))), (5)\n¬Øc = Pool(Trans-Enc([CLS] + ¬Øe; Œ∏(enc))), (6)\nwhere [CLS] is a special token prefixed to each\nevent mention, and Pool(¬∑) denotes using the con-\ntextual embedding of [CLS] to represent the\nwhole event. Then, we enhance h[m], the con-\ntextual representation of [M] in x/{e} from H(enc)\nin Eq.(2), by contrasting it with c against ¬Øc, i.e.,\nL(cee)=max(0,Œª+d(h[m],c)‚àíd(h[m],¬Øc)), (7)\nwhere d(¬∑, ¬∑) denotes a distance metric of two vec-\ntors, which is Euclidean distance in this work. As a\nresult, the encoder-side correlation-aware represen-\ntation h[m] also offers a straightforward pathway\nto transmit event-level information to decoding so\nmitigates the learning difficulty to some extent.\n3.2.3 Prompt-based Event Locating\nAs for learning difficulty problem, we also pro-\npose a prompt-based event locating objective to\nreduce generative difficulty by providing hints in\nthe prompt. The basic idea is to simplify WER ob-\njective as an extractive generation task to locate and\ncopy a candidate/hint from the prompt, which aims\nat improving learning effectiveness. To this end,\nwe present two prompt-based generation schemas\nin the following.\n2562\nCorrect Event Selection. Inspired by advances\nof prompt-based multi-choice question answering,\nwe present correct event selection schema to select\nthe gold evente against negative ones{¬Øe}M\ni=1 based\non the contexts x/{e}. Given an event-masked para-\ngraph x/{e} suffixed with several candidate events\n{¬Øe}M\ni=1 containing the gold masked one e, it aims\nto generate the masked event e back, i.e.,\nÀÜx(ces) = x/{e} + Options: (a) e1; (b) e2; ¬∑¬∑¬∑ ,\nwhere [e1, e2, . . .] is a random permutation of\n[e, {¬Øe}M\ni=1] in case of position bias. We use a ran-\ndom permutation as all candidates are assigned\nwith distinct position embeddings during contex-\ntualizing, and a fixed permutation of gold events\nwill result in a learning shortcut (position bias) to\ndegrade the model. Thus, similar to Eq.(1), we can\ndefine its formula as p(e|ÀÜx(ces); Œ∏).\nWrong Event Tagging. The other schema is\nwrong event tagging to find the wrong event in\na corrupted paragraph, similar to incoherence rea-\nsoning. Thus, we re-write the encoder input as\nÀÜx(wet) = x/{e}&‚à™{¬Øe} + Event: [M] is wrong,\nwhere x/{e}&‚à™{¬Øe} denotes replacing the gold event\ne in x with a negative ¬Øe ‚àà {¬Øe}M\ni=1. Thus, we can de-\nfine the formula of this objective as p(¬Øe|ÀÜx(wet); Œ∏).\nBased on the two formulas above, we define the\nprompt-based event locating objective as\nL(pel) =\nX\n(x,e)\n‚àí 1\n|e|\nX\nt\nlog p(et|e<t, ÀÜx(ces); Œ∏)\n‚àí 1\n|¬Øe|\nX\nt\nlog p(¬Øet|¬Øe<t, ÀÜx(wet); Œ∏), (8)\nwhere Œ∏ = {Œ∏(enc), Œ∏(dec)}, ¬Øe is sampled in {¬Øe}M\ni=1.\n3.3 Model Pre-training and Fine-tuning\nSelf-supervised Pre-training. The final loss to\npre-train our ClarET is a linear combination of the\nthree losses above from Eq.(4, 7, 8), i.e.,\nL = L(wer) + L(cee) + L(pel). (9)\nWe set the margin Œª in Eq.(7) to 0.5 w/o tuning.\nSupervised Downstream Fine-tuning. For gen-\neration tasks, we simply leverage the formula in\nEq.(1) to establish fine-tuning objectives. For dis-\ncriminative (e.g., multi-choice) tasks, we can either\nformulate all tasks into generation as in GPT/T5\nor fine-tune with classifying heads as in BART.\nWith pilot experiments, we found the latter one can\nachieve better performance and adopted it.\n3.4 Comparing to Similar Works\nWhile we adopt the same data processing in Event-\nBERT (Zhou et al., 2021b) and share a similar mo-\ntivation to learn an event-centric pre-trained model,\nwe expand the scope from ‚Äòdiscriminative-only‚Äô in\nEventBERT into ‚Äòunified‚Äô by our context-to-event\nTransformer for a broad spectrum of scenarios.\nSuch an expansion is non-trivial since new chal-\nlenges arise in the unified formulation. Compared\nto the inefficient ‚Äò event-backfilling and contextu-\nalizing‚Äô paradigm in EventBERT, our model can\nexplicitly and effectively learn event-level corre-\nlations between contexts and events by our novel\ncontrastive and prompt-based objectives. More-\nover, COMeT (Bosselut et al., 2019; Hwang et al.,\n2021) is also a conditional generation model but\nfocuses on triple-level commonsense reasoning ‚Äì\ngiven (head event, relation) to generate tail events,\nwhose motivation, however, is orthogonal to ours.\nTherefore, we focus on a different motivation or\nscope, not to mention evaluation formulations.\n4 Experiments\nThis section begins with descriptions of down-\nstream datasets and experimental setups.\nDownstream Datasets. We conduct extensive\nevaluations on 9 datasets for 9 downstream tasks,\ni.e., 5 generation and 4 classification tasks. Gen-\neration tasks include abductive commonsense rea-\nsoning on ART (Œ±NLG) (Bhagavatula et al., 2020),\ncounterfactual story generation on TIMETRA VEL\n(Qin et al., 2019), story ending generation (Guan\net al., 2019), commonsense story generation (Guan\net al., 2020), and event process completion on APSI\n(Zhang et al., 2020a). Classification tasks include\nscript reasoning on MCNC (Li et al., 2018), ab-\nductive commonsense reasoning on ART (Œ±NLI)\n(Bhagavatula et al., 2020), narrative incoherence\ndetection on ROCStories (Mori et al., 2020), and\nstory cloze test (Mostafazadeh et al., 2016). Please\nrefer to Appendix C for their details.\nPre-training Setups. Instead of learning from\nscratch, we perform continual pre-training from\nBART-large (Lewis et al., 2020) due to limited\ncomputation resources. The batch size and number\nof training steps are 1152 and 160k. The model is\ntrained by Adam (Kingma and Ba, 2015) w/ learn-\ning rate of 1e-5 and warmup proportion of 0.03.\nThe gradient clip, dropout rate and weight decay\nare 1.0, 0.1 and 0.01. Notably, (i) BOOK CORPUS\n2563\nAbductive C.S.\nReasoning\nCounterfactual\nStory\nStory Ending\nGeneration\nC.S. Story\nGeneration\nEvent Process\nCompletion\nSize B-4 R-L BERT B-4 R-L BERT B-1 B-2 B-1 B-2 B-1 B-2\nSelected task-specific models with competitive performance\nGRF (Ji et al., 2020) - 11.62 34.62 - - - - - - - - - -\nIE+MSA (Guan et al., 2019) - - - - - - - 24.40 7.80 - - - -\nPlan&Write (Yao et al., 2019) - - - - - - - 24.40 8.40 30.80 12.60 - -\nFine-tuning with pre-trained unified (generative) model\nGPT2-S (Radford et al., 2019) 124M 2.23 22.83 48.74 69.27 65.72 60.53 39.23 13.08 32.20 14.10 35.25 11.75\nGPT2-M (Radford et al., 2019) 335M - - - 75.71 72.72 62.39 - - - - 45.43 14.81\nBART (Lewis et al., 2020) 400M 16.47 38.73 56.36 82.91 76.44 79.50 54.22 18.07 54.22 18.07 56.25 18.75\nGLM (Du et al., 2021) 335M 7.79 25.54 54.85 75.81 70.03 68.23 57.04 18.45 57.04 18.45 57.34 19.11\nClarET (ours) 400M 17.67 41.04 57.31 87.18 80.74 81.48 57.47 19.16 57.47 19.16 58.88 19.74\nTable 1: Fine-tuning results on five generation benchmark datasets. Previous state-of-the-art (SoTA) results are underlined,\n‚ÄòSize‚Äô denotes the number of model parameters, and ‚ÄòC.S.‚Äô is an abbreviation of CommonSense. Please refer to Appendix D.1 for\nthe reported results of more task-specific models on each dataset.\nAbductive C.S.\nReasoning\nScript\nReasoning\nNarrative Incoherence\nDetection\nStory Cloze\nTest\nSize ACC (%) ACC (%) ACC (%) ACC (%)\nSelected task-specific models with competitive performance\nHidden Coherence Model (Chaturvedi et al., 2017) - - - - 77.60\nGRU Context (Mori et al., 2020) - - - 52.20 -\nRoBERTa + Kown. Model (Zhou et al., 2021c) 469M - ::::63.62 - -\nFine-tuning with pre-trained discriminative model\nRoBERTa (Liu et al., 2019) 345M 82.35 61.53 73.94 87.10\nEventBERT (Zhou et al., 2021b) 345M ::::85.51 63.50 ::::75.03 ::::91.33\nFine-tuning with pre-trained unified model\nCALM (Zhou et al., 2021a) 770M 77.12 - - -\nUNICORN (Lourie et al., 2021) 770M 79.50 - - -\nBART (Lewis et al., 2020) 400M 80.74 61.34 72.48 87.01\nClarET (ours) 400M 82.77 64.61 74.88 91.18\nTable 2: Fine-tuning results on four classification benchmark datasets. We split pre-trained models into discriminative and\nunified groups since discriminative models usually outperforms unified ones in classification and our ClarET falls into the latter.\nPrevious SoTA discriminative and unified results are:::::waved and underlined, respectively. See Appendix D.2 for full results.\nhas already been used by BART pre-training and\nour data processing is based on heuristics without\nhuman-curated resources; (ii) Our continual pre-\ntraining only needs 90 GPU hours on 200M tokens,\ni.e., 0.13% of BART that consumes 70K hours on\n2.2T tokens (see Appendix B.1). Hence, ClarET\nwith zero newly introduced corpus and relatively\nnegligible computing overhead makes great lifts\nand preserves fair comparisons with baselines.\nFine-tuning Setups. For finetuning, we train the\nmodel with an Adam w/ learning rate of 1e-5 and\nwarmup proportion of 0.06. The dropout rate, batch\nsize and weight decay are 0.1, 32 and 0.01. For\ngenerative downstream tasks, we take BLEU- N\n(B-N) (Papineni et al., 2002), ROUGE-L (R-L)\n(Lin, 2004) and BERTScore (BERT) (Zhang et al.,\n2020c) as the evaluation metrics, while the accu-\nracy (ACC) is taken for classification tasks. Each\nfine-tuning runs with seeds 2, 10 and 1234, and we\nevaluate the best dev model on the test set.\n4.1 Main Evaluation\nFine-tuning for Generation. As shown in Ta-\nble 1, our proposed ClarET achieves SoTA perfor-\nmance across all generation tasks. For instance,\nClarET increases the ROUGE-L score by 2.3 ab-\nsolute value for abductive reasoning. The supe-\nrior performance of ClarET on the benchmarks\ndemonstrates that it can model event-level corre-\n2564\nMethod B-4 R-L BERT\nGPT (Qin et al., 2019) 1.25 18.26 59.50\nGPT2-S (Qin et al., 2019) 1.28 20.27 59.62\nGPT2-M (Qin et al., 2019) 1.51 19.41 60.17\nZero-Shot-Ranked (Qin et al., 2020) 2.26 25.81 60.07\nBART-large (Lewis et al., 2020) 7.08 30.60 61.58\nDELOREAN (Qin et al., 2020) 21.35 40.73 63.36\nClarET (ours) 23.75 43.03 63.93\nTable 3: Zero-shot results on generative Counterfactual Story.\nlation more effectively via few steps of continual\npre-training and provide a general solution for a\nvariety of event-centric correlation reasoning tasks.\nFine-tuning for Classification. Table 2 lists re-\nsults on 4 classification tasks. We find ClarET per-\nforms better than all task-specific models and uni-\nfied pre-trained models with 2%-4% improvement.\nIt achieves competitive accuracy to strong discrim-\ninative models, e.g., the gap between ClarET and\nEventBERT is ‚àº0.15 for narrative incoherence de-\ntection and story cloze test. However, EventBERT\nis a RoBERTa-based competitor using the identi-\ncal pre-training corpus. Its pre-training follows\n‚Äúevent-backfilling and contextualizing‚Äù (similar to\nmulti-choice QA), which has a small gap to down-\nstream classification tasks for strong performance\nbut brings two drawbacks. Firstly, its pre-training\nis slow due to repeat contextualizing over para-\ngraphs, leading to 5.6√ó longer GPU hours than\nours. In addition, its discriminative paradigm lim-\nits it specifically to classifications, regardless of\nwide generation tasks. The results show ClarET is\non par with the discriminative-only EventBERT\non classifications. This is non-trivial given the\nlarge formulation gap between our generative pre-\ntraining objectives and downstream multi-choice-\nstyle classification tasks, and attributed to our effec-\ntive event-correlation learning. In summary, these\nresults show ClarET serves as a unified pre-trained\nmodel for event-centric generation and classifica-\ntion tasks.\n4.2 Quantitative Analysis\nZero-shot Learning. It is essential to verify if the\ntargeted information was learned and retained by\na pre-trained model. Compared to MLM, our gen-\nerative recovering model is inherently applicable\nto event-centric multi-choice and generative formu-\nlations. For generation tasks, we apply Eq.(1) to\ngenerate answers. As shown in Table 3, ClarET\nachieves the best performance and outperformsDE-\nMethod ACC (%)\nRandom 20.00\nRoBERTa-large (Zhou et al., 2021b) 20.09\nDeBERTa-xlarge (Zhou et al., 2021b) 20.31\nBART-large (Lewis et al., 2020) 21.72\nEventBERT (Zhou et al., 2021b) 30.79\nClarET (ours) 32.15\nTable 4: Zero-shot results on discriminative Script Reasoning.\nNote that MLM-style models are evaluated by autoregression-\nlike operation (Zhou et al., 2021b).\n10 20 30 40 50\nTraining data size on CS (%)\n82\n83\n84\n85\n86BLEU-4\nClarET\nBART (Full data)\n10 20 30 40 50\nTraining data size on SR (%)\n58\n60\n62\n64Accuracy (%)\nClarET\nBART (Full data)\nFigure 3: Few-shot learning results compared with the basic\nmodel, BART-large, on generation (Counterfactual Story, CS)\nand classification (Script Reasoning, SR).\nLOREAN (which adapts auto-regression for coun-\nterfactual reasoning). For classification tasks, we\napply Eq.(1) to each option for its perplexity and\nselect the option with minimum. As shown in Ta-\nble 4, ClarET surpasses previous models and beats\nthe discriminative-only event-centric model, Event-\nBERT. Besides, the general-purpose pre-trained\nmodels perform nearly random guesses due to their\nincompetence in long-span event discrimination.\nFew-shot Learning. Since our model reduces\npretrain-finetune inconsistency for event-centric\ntasks and provides a good initialization for down-\nstream fine-tuning, it is also interesting to see few-\nshot performance by scaling down training data.\nAs shown in Figure 3, ClarET achieves similar per-\nformance to strong baselines with only 10%-30%\nof training data for fine-tuning.\nAblation study. To measure the contribution\nof each objective to the final fine-tuning results,\nwe conduct an ablation study on both generation\nand classification in Table 5. The first two abla-\ntions drop the two prompt schemas respectively in\nprompt-based event locating objective of Eq.(8),\nwhich verifies the effectiveness of reducing task\ndifficulty. Then, the third ablation removes con-\ntrastive event-correlation encoding and shows a\nsubstantial drop, which verifies the significance\nof explicit event-correlation learning. Next, we\nkeep only the prompt-based event locating objec-\ntive to make our model a prompt-learning discrim-\n2565\nMethod Gen-CS Cls-SR\nB-4 R-L ACC\nClarET (full, pre-trained by Eq.(9).) 87.18 80.74 64.61\n3 w/o correct event selection (prompt) 86.76 80.03 63.06\n3 w/o wrong event tagging (prompt) 86.33 79.84 63.89\n3 w/o contrastive encoding 85.84 78.69 63.24\n3 only prompt-based event locating 83.32 76.51 62.97\nBART-large (basic model) 82.91 76.44 61.34\nTable 5: Ablation study of the pre-training objectives in\nClarET, which is evaluated by fine-tuning on generation (Coun-\nterfactual Story, CS) and classification (Script Reasoning, SR).\nMethod ePPL on Dev\nClarET (full model) 8.27\nWER-Only Model 8.76\nTable 6: Event generation of ClarET and whole event re-\ncovering (WER-only) model on a pre-training event-masked\ndev set (2% held-out masked paragraphs by following Zhou\net al. (2021b)). The ‚ÄòePPL‚Äô, i.e., event perplexity, refers to\nevent-level token perplexity averaged over the dataset.\ninative model (sharing more close methodology\nwith EventBERT), however leading to a dramatic\ndecrease. Lastly, when removing all the objectives,\nour model degenerates to BART-large.\nComparison with Larger Model. A trend of pre-\ntraining models follows the law of ‚Äòlarger models\nfor better performance‚Äô but a crucial research ques-\ntion is ‚Äòhow to perform competitively with fewer\ncomputation resources‚Äô. To answer, we show extra\nfine-tuning results on the five generation datasets in\nTable 7 to compare our ClarET (400M parameters)\nwith T5-large (770M) and T5-base (220M). It is\nobserved (i) with 3√ó scale, T5-large notably out-\nperforms T5-base to support the above law and (ii)\nwith almost half model size, our ClarET performs\nvery competitively to T5-large (even better on 3\nout of 5 tasks), verifying the significance of our\nobjectives towards event-related knowledge.\nDifficulty of Event Generation. To exhibit the\nlearning difficulty in pre-training (as stated in\n¬ß3.2.1) and the effectiveness of our novel learn-\ning objectives, we conduct another ablation setting\nin Table 6. It is observed that ClarET achieves\nbetter event-level perplexity (ePPL), verifying the\ntwo novel objectives promote event generations\nand reduce difficulty of decoding.\nLong-span Event Generation. To further check\nif ClarET is more competitive on longer-span event\ngeneration, we compare it with BART-large and\nACR CS SEG CSG EPC\nSize R-L B-4 B-1 B-1 B-1\nT5-base 220M 38.40 81.02 52.64 41.28 56.53\nT5-large 770M 40.77 90.62 57.04 43.82 59.59\nClarET 400M 41.04 87.18 57.47 48.75 58.88\nTable 7: Fine-tuning generation results to compare with larger\npre-trained models. Column names are datasets corresponding\nto those of Table 1. See Appendix B.2 for full results of T5.\nFigure 4: Event generation performance on the event-masked\ndev set (refer to Table 6) with event-length bins.\nT5-base/-large by ‚Äò‚àílog‚Äô of Eq.(1). Different from\nrecovering paradigm of others, we follow the de-\nnoising paradigm to implement BART and calcu-\nlate its score by considering the masked part in\ndecoding. Figure 4 shows that (1) Line Chart: the\ngap between ClarET and the others becomes larger\nwith event length increasing as the general-purpose\nmodels only consider short-span masking in pre-\ntraining, leading to inferior event generation; and\n(2) Bar Chart: as for data distribution, although a\nmajority of data falls into the 6-8 bin, there are still\nmany examples with event length greater than nine.\nNatural Language Understanding (NLU). Our\nbasic model, BART-large, is presented for general\nNLU tasks. To exhibit our minor event-centric con-\ntinual pre-training would not interfere its NLU abil-\nity, we conduct fine-tuning experiments on GLUE\nbenchmark (Wang et al., 2019) as in Figure 5. It is\nobserved that, although slightly surpassed by the\ndiscriminative RoBERTa model, fine-tuning BART\nand ClarET achieve very comparable results, which\nverifies ClarET‚Äôs retention of NLU capability.\n4.3 Case Study and Error Analysis\nCase Study. As the first case in Figure 6, we\nconduct a case study on generative abductive rea-\nsoning task, where the fine-tuned ClarET generates\nan event semantically close to the gold reference,\nbut the BART does not. BART only generates a\npart of the answer but ignores the event-correlations\nfrom ‚ÄòThey were impressed with my phone‚Äô, while\nClarET completely captures the correlations in the\n2566\nFigure 5: Fine-tuning results on GLUE dev, which verifies\nClarET retains BART‚Äôs natural language understanding ability.\nContext: I went to the store to buy a phone. [E] They were\nimpressed with my phone.\nReference of the Gold Event [E]: I bought the latest model of the\nphone I wanted, and showed it to my friends.\nGeneration by ClarET:\nI bought a new phone and showed it to my friends. (BLEU-4: 34)\nGeneration by BART:\nI bought a new phone. (BLEU-4: 0)\nContext: Cora was starting her job as a kindergarten teacher. [E]\nAt the end of the day, they all told her how much they liked her!\nReference of the Gold Event [E]: Cora was nervous, but knew the\nstudents were nervous too, so she tried to be extra friendly.\nGeneration by ClarET:\nCora spent the whole day with her students. (BLEU-4: 0)\nFigure 6: Case study & error analysis on abductive reasoning.\ncontexts (e.g., ‚Äò to buy a phone ‚Äô and ‚Äò They were\nimpressed‚Äô,) and generate a much better result.\nError Analysis and Limitation. The second\ncase in Figure 6 shows that our ClarET is inef-\nfective when the gold event is very complicated. In\ndetail, the model focus only on ‚Äòat the end of the\nday‚Äô to generate ‚Äò ... spent the whole day ... ‚Äô but\nignore very subtle contexts, e.g., ‚Äòstarting her job ...\nteacher‚Äô and ‚Äòthey liked her‚Äô. To expand, we found\na problem in long-event decoding by pilot experi-\nments. As shown in Figure 7, it is observed that the\ngap of token-level perplexity between ClarET and\nWER-only gradually diminishes. This is because\nthe subsequent tokens in an event can be gener-\nated on the basis of previous generations on the\ndecoder side, rather than context-aware represen-\ntations from the encoder side. While a long span\nis masked, the model can see previous tokens in\nan event (i.e., e<t) in decoding and incline to per-\nform the t-th prediction based on e<t but not x/{e},\nespecially with a larger t. As a result, the model\nwould ‚Äòcheat‚Äô in the generation but learn decoder-\nside language modeling rather than context-aware\nrepresentations. In the future, we will exploit this\nproblem. Besides, due to computation resources,\nwe choose the model size with 400M and continual\n10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\nPosition (top-percentage) of a token\n0\n1\n2\n3Token perplexity\nClarET\nWER-Only\nFigure 7: Token-level perplexity w.r.t tokens‚Äô percentage\npositions in events on held-out dev set.\npre-training in 90h, limiting the performance.\n5 Conclusion\nWe present a novel correlation-aware context-to-\nevent Transformer to self-supervisedly learn event-\ncorrelation knowledge from text corpus and benefit\nvarious event-centric reasoning scenarios. Besides\nSoTA fine-tuning results on 5 generation and 4 clas-\nsification tasks, we conduct zero-/few-shot learn-\ning and extensive ablation studies to exhibit our\nmodel‚Äôs effectiveness. Lastly, we find our model\nis competitive to a twice larger general-purpose\nmodel, reduces learning difficulty for event genera-\ntion, and retains NLU ability from its basic model.\nAlthough this work learns context-to-event knowl-\nedge, our self-supervised objectives are applica-\nble to other semantically-meaningful text units be-\nsides events. For example, text units can be entities\nand concepts to learn relational and commonsense\nknowledge, which can benefit more downstream\ntasks.\n6 Ethical Statement\nThis work does not involve any sensitive data, but\nonly public unlabeled corpora, i.e., BookCorpus\n(Zhu et al., 2015) pre-processed by Zhou et al.\n(2021b), and crowd-sourced datasets released in\nprevious works, including ART (Bhagavatula et al.,\n2020), TIMETRA VEL (Qin et al., 2019), APSI\n(Zhang et al., 2020a), MCNC (Li et al., 2018),\nROCStories (Mori et al., 2020).\nReferences\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Wen-tau Yih, and Yejin\nChoi. 2020. Abductive commonsense reasoning. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2567\n2019. COMET: commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n4762‚Äì4779. Association for Computational Linguis-\ntics.\nSnigdha Chaturvedi, Haoruo Peng, and Dan Roth. 2017.\nStory comprehension for predicting what happens\nnext. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September\n9-11, 2017, pages 1603‚Äì1614. Association for Com-\nputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186. Association for Computational\nLinguistics.\nXiao Ding, Kuo Liao, Ting Liu, Zhongyang Li, and\nJunwen Duan. 2019. Event representation learning\nenhanced with external commonsense knowledge.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 4893‚Äì4902.\nAssociation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages\n13042‚Äì13054.\nYue Dong, Chandra Bhagavatula, Ximing Lu, Jena D.\nHwang, Antoine Bosselut, Jackie Chi Kit Cheung,\nand Yejin Choi. 2021. On-the-fly attention modula-\ntion for neural generation. In Findings of the Associ-\nation for Computational Linguistics: ACL/IJCNLP\n2021, Online Event, August 1-6, 2021 , volume\nACL/IJCNLP 2021 of Findings of ACL, pages 1261‚Äì\n1274. Association for Computational Linguistics.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. All\nNLP tasks are generation tasks: A general pretraining\nframework. CoRR, abs/2103.10360.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 889‚Äì898. Association for Computational Lin-\nguistics.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2019.\nStrategies for structuring story generation. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 2650‚Äì2660. Association for Computa-\ntional Linguistics.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings of the\n34th International Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August\n2017, volume 70 of Proceedings of Machine Learn-\ning Research, pages 1243‚Äì1252. PMLR.\nMark Granroth-Wilding and Stephen Clark. 2016. What\nhappens next? event prediction using a composi-\ntional neural network model. In Proceedings of the\nThirtieth AAAI Conference on Artificial Intelligence,\nFebruary 12-17, 2016, Phoenix, Arizona, USA, pages\n2727‚Äì2733. AAAI Press.\nJian Guan, Fei Huang, Minlie Huang, Zhihao Zhao,\nand Xiaoyan Zhu. 2020. A knowledge-enhanced\npretraining model for commonsense story generation.\nTrans. Assoc. Comput. Linguistics, 8:93‚Äì108.\nJian Guan, Yansen Wang, and Minlie Huang. 2019.\nStory ending generation with incremental encoding\nand commonsense knowledge. In The Thirty-Third\nAAAI Conference on Artificial Intelligence, AAAI\n2019, The Thirty-First Innovative Applications of\nArtificial Intelligence Conference, IAAI 2019, The\nNinth AAAI Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019 , pages 6473‚Äì\n6480. AAAI Press.\nRujun Han, Xiang Ren, and Nanyun Peng. 2020a. Deer:\nA data efficient language model for event temporal\nreasoning. arXiv preprint arXiv:2012.15283.\nRujun Han, Xiang Ren, and Nanyun Peng. 2020b.\nDEER: A data efficient language model for event\ntemporal reasoning. CoRR, abs/2012.15283.\nQingbao Huang, Linzhang Mo, Pijian Li, Yi Cai, Qing-\nguang Liu, Jielong Wei, Qing Li, and Ho-fung Leung.\n2021. Story ending generation with multi-level graph\nconvolutional networks over dependency trees. In\nThirty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI 2021, Thirty-Third Conference on In-\nnovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2021, Vir-\ntual Event, February 2-9, 2021, pages 13073‚Äì13081.\nAAAI Press.\nJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\n2568\nYejin Choi. 2021. (comet-) atomic 2020: On sym-\nbolic and neural commonsense knowledge graphs.\nIn Thirty-Fifth AAAI Conference on Artificial Intel-\nligence, AAAI 2021, Thirty-Third Conference on In-\nnovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2021, Virtual\nEvent, February 2-9, 2021, pages 6384‚Äì6392. AAAI\nPress.\nHaozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan\nZhu, and Minlie Huang. 2020. Language generation\nwith multi-hop reasoning on commonsense knowl-\nedge graph. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 725‚Äì736. Association for Computational Lin-\nguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Trans. Assoc. Comput. Linguistics, 8:64‚Äì\n77.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\naries with a single QA system. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, volume\nEMNLP 2020 of Findings of ACL, pages 1896‚Äì1907.\nAssociation for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871‚Äì7880.\nAssociation for Computational Linguistics.\nZhongyang Li, Xiao Ding, and Ting Liu. 2018. Con-\nstructing narrative event evolutionary graph for script\nevent prediction. In Proceedings of the Twenty-\nSeventh International Joint Conference on Artificial\nIntelligence, IJCAI 2018, July 13-19, 2018, Stock-\nholm, Sweden, pages 4201‚Äì4207. ijcai.org.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74‚Äì81.\nShih-Ting Lin, Nathanael Chambers, and Greg Durrett.\n2020. Conditional generation of temporally-ordered\nevent sequences. CoRR, abs/2012.15786.\nShih-Ting Lin, Nathanael Chambers, and Greg Durrett.\n2021. Conditional generation of temporally-ordered\nevent sequences. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 7142‚Äì7157. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavatula,\nand Yejin Choi. 2021. UNICORN on RAINBOW: A\nuniversal commonsense reasoning model on a new\nmultitask benchmark. In Thirty-Fifth AAAI Confer-\nence on Artificial Intelligence, AAAI 2021, Thirty-\nThird Conference on Innovative Applications of Arti-\nficial Intelligence, IAAI 2021, The Eleventh Sympo-\nsium on Educational Advances in Artificial Intelli-\ngence, EAAI 2021, Virtual Event, February 2-9, 2021,\npages 13480‚Äì13488. AAAI Press.\nThang Luong, Hieu Pham, and Christopher D. Manning.\n2015. Effective approaches to attention-based neural\nmachine translation. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2015, Lisbon, Portugal, Septem-\nber 17-21, 2015, pages 1412‚Äì1421. The Association\nfor Computational Linguistics.\nShangwen Lv, Fuqing Zhu, and Songlin Hu. 2020. In-\ntegrating external event knowledge for script learn-\ning. In Proceedings of the 28th International Confer-\nence on Computational Linguistics, COLING 2020,\nBarcelona, Spain (Online), December 8-13, 2020 ,\npages 306‚Äì315. International Committee on Compu-\ntational Linguistics.\nYusuke Mori, Hiroaki Yamane, Yusuke Mukuta, and\nTatsuya Harada. 2020. Finding and generating a\nmissing part for story completion. In Proceedings\nof the The 4th Joint SIGHUM Workshop on Com-\nputational Linguistics for Cultural Heritage, Social\nSciences, Humanities and Literature, pages 156‚Äì166.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James F. Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding\nof commonsense stories. In NAACL HLT 2016, The\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, San Diego California,\nUSA, June 12-17, 2016, pages 839‚Äì849. The Associ-\nation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n2569\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311‚Äì318. ACL.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced contex-\ntual word representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 43‚Äì54. Association for Compu-\ntational Linguistics.\nLianhui Qin, Antoine Bosselut, Ari Holtzman, Chan-\ndra Bhagavatula, Elizabeth Clark, and Yejin Choi.\n2019. Counterfactual story reasoning and generation.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 5042‚Äì5052.\nAssociation for Computational Linguistics.\nLianhui Qin, Vered Shwartz, Peter West, Chandra Bha-\ngavatula, Jena D. Hwang, Ronan Le Bras, Antoine\nBosselut, and Yejin Choi. 2020. Back to the future:\nUnsupervised backprop-based decoding for counter-\nfactual and abductive commonsense reasoning. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 794‚Äì805.\nAssociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, pages\n3027‚Äì3035.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence, February\n4-9, 2017, San Francisco, California, USA , pages\n4444‚Äì4451. AAAI Press.\nSiddarth Srinivasan, Richa Arora, and Mark Riedl. 2018.\nA simple and effective approach to the story cloze test.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT, New Orleans, Louisiana, USA, June\n1-6, 2018, Volume 2 (Short Papers) , pages 92‚Äì96.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nTianming Wang and Xiaojun Wan. 2019. T-CV AE:\ntransformer-based conditioned variational autoen-\ncoder for story completion. In Proceedings of the\nTwenty-Eighth International Joint Conference on Ar-\ntificial Intelligence, IJCAI 2019, Macao, China, Au-\ngust 10-16, 2019, pages 5233‚Äì5239. ijcai.org.\nZhongqing Wang, Yue Zhang, and Ching-Yun Chang.\n2017. Integrating order information and event rela-\ntion for script event prediction. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2017, Copenhagen,\nDenmark, September 9-11, 2017, pages 57‚Äì67. Asso-\nciation for Computational Linguistics.\nZiqi Wang, Xiaozhi Wang, Xu Han, Yankai Lin, Lei\nHou, Zhiyuan Liu, Peng Li, Juanzi Li, and Jie Zhou.\n2021. CLEVE: contrastive pre-training for event ex-\ntraction. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n6283‚Äì6297. Association for Computational Linguis-\ntics.\nBonnie Webber, Rashmi Prasad, Alan Lee, and Aravind\nJoshi. 2019. The penn discourse treebank 3.0 annota-\ntion manual. Philadelphia, University of Pennsylva-\nnia.\nJingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi-\naoyan Cai, and Xu Sun. 2018. A skeleton-based\nmodel for promoting coherence among sentences in\nnarrative story generation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018, pages 4306‚Äì4315. Association\nfor Computational Linguistics.\n2570\nJianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and\nDevi Parikh. 2018. Graph R-CNN for scene graph\ngeneration. In Computer Vision - ECCV 2018 - 15th\nEuropean Conference, Munich, Germany, September\n8-14, 2018, Proceedings, Part I , volume 11205 of\nLecture Notes in Computer Science, pages 690‚Äì706.\nSpringer.\nLili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nThe Thirty-Third AAAI Conference on Artificial Intel-\nligence, AAAI 2019, The Thirty-First Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 - February 1, 2019,\npages 7378‚Äì7385. AAAI Press.\nChanglong Yu, Hongming Zhang, Yangqiu Song, and\nWilfred Ng. 2020. Cocolm: Complex commonsense\nenhanced language model. CoRR, abs/2012.15643.\nHongming Zhang, Muhao Chen, Haoyu Wang, Yangqiu\nSong, and Dan Roth. 2020a. Analogous process\nstructure induction for sub-event sequence prediction.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 1541‚Äì\n1550. Association for Computational Linguistics.\nHongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song,\nand Cane Wing-Ki Leung. 2020b. ASER: A large-\nscale eventuality knowledge graph. In WWW ‚Äô20:\nThe Web Conference 2020, Taipei, Taiwan, April 20-\n24, 2020, pages 201‚Äì211. ACM / IW3C2.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020c. Bertscore: Eval-\nuating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 1441‚Äì1451. Association for Computa-\ntional Linguistics.\nWangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam,\nSeyeon Lee, and Xiang Ren. 2021a. Pre-training\ntext-to-text transformers for concept-centric common\nsense. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nYucheng Zhou, Xiubo Geng, Tao Shen, Guodong\nLong, and Daxin Jiang. 2021b. Eventbert: A pre-\ntrained model for event correlation reasoning. CoRR,\nabs/2110.06533.\nYucheng Zhou, Xiubo Geng, Tao Shen, Jian Pei, Wen-\nqiang Zhang, and Daxin Jiang. 2021c. Modeling\nevent-pair relations in external knowledge graphs for\nscript reasoning. In Findings of the Association for\nComputational Linguistics: ACL/IJCNLP 2021, On-\nline Event, August 1-6, 2021, volume ACL/IJCNLP\n2021 of Findings of ACL, pages 4586‚Äì4596. Associ-\nation for Computational Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19‚Äì27.\nIEEE Computer Society.\nYunchang Zhu, Liang Pang, Yanyan Lan, and Xueqi\nCheng. 2020. L2R2: leveraging ranking for abduc-\ntive reasoning. CoRR, abs/2005.11223.\nA Examples from Mined Pre-training\nCorpus\nThere are some mined pre-training examples shown\nin Table 8. As in (Zhou et al., 2021b), an example\nincludes a paragraph, events, a selected positive\nevent, connectives of the positive event, and sam-\npled negative events of the positive event.\nB More Details\nB.1 BART Pre-training Resources\nIn this section, we analyze BART pre-training re-\nsources in terms of text corpora and computation\nresources.\nAs for tokens in BART pre-training corpora,\nBART paper (Lewis et al., 2020) claims using the\nsame corpora as in RoBERTa (Liu et al., 2019) and\nT5 paper (Raffel et al., 2020) states RoBERTa uses\na 2.2T-token text corpus. Thus, we adopt ‚Äò2.2T‚Äô as\nthe number in the main paper.\nAs for BART pre-training computation over-\nheads, the contributor of BART official code reposi-\ntory said ‚ÄòWe trained for around 11-12 days on 256\ngpus.‚Äô at https://github.com/pytorch/\nfairseq/issues/1525, so the BART pre-\ntraining takes from 67584 to 73728 GPU hours.\nThus, we use ‚Äò70,000‚Äô as the number in the main\npaper.\nB.2 Full Results of T5 Model\nThe full results of T5-base and T5-large on the five\ngeneration tasks are shown in Table 9.\n2571\nExample 1\nParagraph\nIt was only months later, when she saw her friend‚Äôs thin gaunt face, her swollen belly and her quiet\ndesperation, that she had come to her senses. Then she had been filled with a combination of burning\nrage and deep shame. This had endured over the years undiminished.\nPositive Event she had been filled with a combination of burning rage\nConnectives when; then\nNegative Events\nhe had been loaded with a lot of vampire venom\nshe had been trained in the art of gentler speech\nI had been blessed with some sort of fire ability\nhe had been transformed into a piece of living statuary\nhe had beened from a block of pale marble\nI had been circumci sculptsed in the age of infantile apathy\n...\nExample 2\nParagraph\nThen, when she turned twenty one at the end of last year, she had decided to act on it. A driver‚Äôs\nlicense was something she needed for her business and the identity papers which went with it were\nneeded for a range of other reasons, such as enrolling Catherine for school at the start of this year.\nPositive Event papers which went with it were needed for a range of other reasons\nConnectives then; when; and\nNegative Events\nbookcases that stood against it had opened like a pair of French doors\nit had a little bit of magic in it , just for Lizzie\nit only gave her a place for a couple of days\nwhich occasionally crossed a small ridge sometimes of gravel , sometimes of sand\nthat she was going shopping in the city with a couple of other girls\npublish that proposal in the paper for three weeks\n...\nTable 8: Some mined pre-training examples.\nAbductive C.S.\nReasoning\nCounterfactual\nStory\nStory Ending\nGeneration\nC.S. Story\nGeneration\nEvent Process\nCompletion\nB-4 R-L BERT B-4 R-L BERT B-1 B-2 B-1 B-2 B-1 B-2\nT5-base 15.65 38.40 55.98 81.02 75.95 79.12 52.64 17.55 41.28 13.76 56.53 18.84\nT5-large 17.75 40.77 57.20 90.62 84.03 83.14 57.04 18.45 43.82 14.61 59.59 19.86\nTable 9: Full results of T5-base and T5-large on generation tasks.\nB.3 Connectives in Paragraph\nAs stated by Zhou et al. (2021b), connectives (i.e.,\ndiscourse relations in the contexts) play important\nroles to express correlations among events. There-\nfore, we also find every possible connective r to\neach (x, e) where r is a connective in x, which im-\nmediately links to the verb of e on the parsing tree\nof x. To leverage the connectives, we also apply the\ncorrect event selection in prompt-based event lo-\ncating objective to r its negatives {¬Ør}M\n1 , as correct\nconnective selection. Here, ¬Ør is randomly sampled\nfrom discourse relations in the PDTB annotation\nmanual (Webber et al., 2019). At 20% times, we\nuse correct connective selection to replace correct\nevent selection in the prompt-based event locating\nobjective.\nC Details of Evaluation Datasets\nWe detail the nine evaluation datasets in the follow-\ning. The training example in each dataset is shown\nin Table 10.\n‚Ä¢ ART (Œ±NLG). Given two observations in nat-\nural language, it aims to generate an explica-\ntive hypothesis between them. We follow the\nofficial data split (Bhagavatula et al., 2020)\nwith 169,654/1,532/3,059 in training/dev/test.\n‚Ä¢ TIMETRA VEL.Given an original story and\na counterfactual event, it aims to rewrite\nthe subsequent events to complete a story,\nwhich is compatible with the counterfactual\nevent. We follow the official data split (Qin\net al., 2019) with 98,159/5,613/7,484 in train-\ning/dev/test.\n2572\n‚Ä¢ Story Ending Generation. We evaluate the\nstory ending generation based on ROCStories,\nwhich aims to generate a story ending for a\ngiven story context. We follow the data split\n(Guan et al., 2019) with 90,000/4,081/4,081\nin training/dev/test.\n‚Ä¢ Commonsense Story Generation. It is based\non ROCStories. Given a leading context, it\naims to generate a reasonable story. We fol-\nlow the data split (Guan et al., 2020) with\n88,344/4,908/4,909 in training/dev/test.\n‚Ä¢ APSI. We evaluate event process completion\non the APSI dataset, where the goal is to gen-\nerate a subevent for a given event context. We\nfollow the data split (Zhang et al., 2020a) with\n13,501/1,316 in training/test.\n‚Ä¢ Multi-choice narrative cloze (MCNC).\nGiven an event chain, it aims to predict the\nsubsequent event from 5 candidates. We\nfollow the data split (Li et al., 2018) with\n140,331/10,000/10,000 in training/dev/test.\n‚Ä¢ ART (Œ±NLI). Given two observations in nat-\nural language, it aims to choose the most ex-\nplicative hypothesis from 2 candidates. We\nfollow the data split (Bhagavatula et al., 2020)\nwith 169,654/1532 samples in training/dev.\n‚Ä¢ ROCStories. We follow (Mori et al., 2020)\nto use ROCStories for narrative incoherence\ndetection. A random sentence is removed\nfor each five-sentence story, and the goal\nis to predict the missing position. We fol-\nlow the data split (Mori et al., 2020) with\n78,528/9,816/9,817 in training/dev/test.\n‚Ä¢ Story Cloze Test. Given a 4-sentence con-\ntext, it aims to select the right ending from\ntwo alternative endings. We follow the\ndata split (Mostafazadeh et al., 2016) with\n98,161/1,871/1,871 in training/dev/test.\nD Detailed Evaluation Results\nWe detail the full results on nine evaluation datasets\nas follows.\nD.1 Generation Tasks\nGeneration tasks include abductive commonsense\nreasoning (Œ±NLG), counterfactual story generation,\nstory ending generation, commonsense story gener-\nation, and event process completion. The detailed\nresults of these generation tasks are shown in Ta-\nble 11, Table 12, Table 13, Table 14, and Table 15,\nrespectively. ClarET achieves state-of-the-art per-\nformance on all five generation tasks. In addition,\npre-trained language models show their strong gen-\neration ability on story generation tasks, i.e., story\nending generation and commonsense story genera-\ntion.\nD.2 Classification Tasks\nClassification tasks include script reasoning, ab-\nductive commonsense reasoning (Œ±NLI), narrative\nincoherence detection, and story cloze test. The de-\ntailed results of these classification tasks are shown\nin Table 16, Table 17, Table 18, and Table 19, re-\nspectively. Compared with unified language mod-\nels, ClarET achieves state-of-the-art performance.\nAlthough strong discriminative models show their\ngreat ability on classification tasks, ClarET still\nachieves competitive performance.\n2573\nDataset 1: ART (Œ±NLG)\nInput Observation1: The hayride was in October.\nObservation2: It was the perfect start to the fall season.\nLabel Keeping tradition we drank hot cocoa on the ride.\nDataset 2: TIMETRA VEL\nInput\nPremise: On my way to work I stopped to get some coffee.\nInitial: I went through the drive through and placed my order.\nOriginal_ending: I paid the cashier and patiently waited for my drink.\nWhen she handed me the drink, the lid came off and spilled on me.\nThe coffee hurt and I had to go home and change clothes.\nCounterfactual: I went inside to place my order.\nLabel\nI paid the cashier and patiently waited at the counter for my drink.\nWhen she handed me the drink, the lid came off and spilled on me.\nThe coffee hurt and I had to go home and change clothes.\nDataset 3: Story Ending Generation\nInput\nDan‚Äôs parents were overweight.\nDan was overweight as well.\nThe doctors told his parents it was unhealthy.\nHis parents understood and decided to make a change.\nLabel They got themselves and Dan on a diet.\nDataset 4: Commonsense Story Generation\nInput Carrie had just learned how to ride a bike.\nLabel\nShe didn‚Äôt have a bike of her own.\nCarrie would sneak rides on her sister‚Äôs bike.\nShe got nervous on a hill and crashed into a wall.\nThe bike frame bent and Carrie got a deep gash on her leg.\nDataset 5: APSI\nInput Process name: Treat Pain. Process: Identify cause. learn injury.\nLabel Recognize symptom.\nDataset 6: Multi-choice narrative cloze (MCNC)\nInput\nContext: compare basketball. buck get basketball. whirl basketball bench. shout out basketball center.\nOptions: A. look basketball. B. weaken basketball. C. throw basketball lot youngster.\nD. client deny basketball. E. client deny basketball.\nLabel C\nDataset 7: ART (Œ±NLI)\nInput\nObservation1: Chad went to get the wheel alignment measured on his car.\nObservation2: The mechanic provided a working alignment with new body work.\nHypothesis1: Chad was waiting for his car to be washed.\nHypothesis2: Chad was waiting for his car to be finished.\nLabel 2\nDataset 8: ROCStories\nInput\nLaverne needs to prepare something for her friend‚Äôs party.\nShe decides to bake a batch of brownies.\nLaverne tests one of the brownies to make sure it is delicious.\nThe brownies are so delicious Laverne eats two of them.\nLabel 3\nDataset 9: Story Cloze Test\nInput\nContext: Rick grew up in a troubled household. He never found good support in family, and turned to gangs.\nIt wasn‚Äôt long before Rick got shot in a robbery. The incident caused him to turn a new leaf.\nOptions: A. He is happy now. B. He joined a gang.\nLabel A\nTable 10: The training examples on different datasets.\n2574\nMethod B-4 R-L BERT\nGPT2-Fixed (Bhagavatula et al., 2020) 0.00 9.99 36.69\nO1-O2-Only (Bhagavatula et al., 2020) 2.23 22.83 48.74\nCOMeT-T+GPT2 (Bhagavatula et al., 2020) 2.29 22.51 48.46\nCOMeT-E+GPT2 (Bhagavatula et al., 2020) 3.03 22.93 48.52\nFine-tuned GPT2-L (Dong et al., 2021) 13.52 18.01 -\nGRF (Ji et al., 2020) 11.62 34.62 -\nBART-large (Lewis et al., 2020) 16.47 38.73 56.36\nGLM-large (Du et al., 2021) 7.79 25.54 54.85\nClarET 17.67 41.04 57.31\nTable 11: Results on the Abductive Commonsense Reasoning\n(Œ±NLG).\nMethod B-4 R-L BERT\nHuman (Qin et al., 2019) 64.93 67.64 61.87\nGPT2-S (Radford et al., 2019) 69.27 65.72 60.53\nGPT2-M+Rec+CF (Qin et al., 2020) 75.92 70.93 62.49\nGPT2-M+Sup (Qin et al., 2020) 75.71 72.72 62.39\nBART-large (Lewis et al., 2020) 82.91 76.44 79.50\nGLM-large (Du et al., 2021) 75.81 70.03 68.23\nClarET 87.18 80.74 81.48\nTable 12: Results on the Counterfactual Story.\nMethod B-1 B-2\nSeq2Seq (Luong et al., 2015) 18.50 5.90\nTransformer (Vaswani et al., 2017) 17.40 6.00\nGCN (Yang et al., 2018) 17.60 6.20\nIE+MSA (Guan et al., 2019) 24.40 7.80\nT-CV AE (Wang and Wan, 2019) 24.30 7.70\nPlan&Write (Yao et al., 2019) 24.40 8.40\nMGCN-DP (Huang et al., 2021) 24.60 8.60\nGPT2-S (Radford et al., 2019) 39.23 13.08\nBART-large (Lewis et al., 2020) 54.22 18.07\nClarET 57.47 19.16\nTable 13: Results on the Story Ending Generation.\nMethod B-1 B-2\nConvS2S (Gehring et al., 2017) 31.20 13.20\nFusion (Fan et al., 2018) 32.20 13.70\nPlan&Write (Yao et al., 2019) 30.80 12.60\nSKRL (Xu et al., 2018) 26.70 8.80\nDSRL (Fan et al., 2019) 29.30 11.70\nGPT2-S (Guan et al., 2020) 32.20 14.10\nKE-GPT2 (Guan et al., 2020) 32.60 14.30\nBART-large (Lewis et al., 2020) 45.24 15.08\nClarET 48.75 16.25\nTable 14: Results on the Commonsense Story Generation.\nMethod B-1 B-2\nGPT2-S (Radford et al., 2019) 35.25 11.75\nGPT2-M (Radford et al., 2019) 45.43 14.81\nBART-large (Lewis et al., 2020) 56.25 18.75\nGLM-large (Du et al., 2021) 57.34 19.11\nClarET 58.88 19.74\nTable 15: Results on the Event Process Completion.\nMethod ACC\nDiscriminative Model\nRandom 20.00\nEvent-Comp (Granroth-Wilding and Clark, 2016) 49.57\nPairLSTM (Wang et al., 2017) 50.83\nSGNN (Li et al., 2018) 52.45\nSGNN + Int&Senti (Ding et al., 2019) 56.03\nRoBERTa-base (Liu et al., 2019) 56.23\nRoBERTa-large (Liu et al., 2019) 61.53\nRoBERTa + Rep. Fusion (Lv et al., 2020) 58.66\nEventBERT (Zhou et al., 2021b) 63.50\nRoBERTa + Kown. Model (Zhou et al., 2021c) 63.62\nUnified Model\nBART-large (Lewis et al., 2020) 61.34\nClarET 64.61\nTable 16: Results on the Script Reasoning.\nMethod ACC\nDiscriminative Model\nRandom 50.00\nBERT-base (Devlin et al., 2019) 61.88\nERNIE (Zhang et al., 2019) 63.04\nKnowBERT (Peters et al., 2019) 63.18\nBERT-large (Devlin et al., 2019) 66.75\nRoBERTa-large (Liu et al., 2019) 82.35\nEventBERT (Zhou et al., 2021b) 85.51\nUnified Model\nT5-base (Raffel et al., 2020) 61.10\nT5-large (Raffel et al., 2020) 77.80\nBART-large (Lewis et al., 2020) 80.74\nGLM-large (Du et al., 2021) 65.27\nCALM-large (Zhou et al., 2021a) 77.12\nUNICORN (Lourie et al., 2021) 79.50\nClarET 82.77\nTable 17: Results on the Abductive Commonsense Reasoning\n(Œ±NLI).\nMethod ACC\nDiscriminative Model\nRandom 20.00\nRoBERTa-large (Liu et al., 2019) 73.94\nMax-pool Context (Mori et al., 2020) 35.00\nGRU Context (Mori et al., 2020) 52.20\nEventBERT (Zhou et al., 2021b) 75.03\nUnified Model\nBART-large (Lewis et al., 2020) 72.48\nClarET 74.88\nTable 18: Results on the Narrative Incoherence Detection.\nMethod ACC\nDiscriminative Model\nRandom 50.00\nHidden Coherence Model (Chaturvedi et al., 2017) 77.60\nval-LS-skip (Srinivasan et al., 2018) 76.50\nRoBERTa-large (Liu et al., 2019) 87.10\nEventBERT (Zhou et al., 2021b) 91.33\nUnified Model\nFinetuned Transformer LM (Radford et al., 2018) 86.50\nBART-large (Lewis et al., 2020) 87.01\nClarET 91.18\nTable 19: Results on the Story Cloze Test.\n2575",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.708720326423645
    },
    {
      "name": "Event (particle physics)",
      "score": 0.6644877791404724
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5741459727287292
    },
    {
      "name": "Machine learning",
      "score": 0.5013682842254639
    },
    {
      "name": "Correlation",
      "score": 0.4881161153316498
    },
    {
      "name": "Context (archaeology)",
      "score": 0.45451921224594116
    },
    {
      "name": "Complex event processing",
      "score": 0.42281392216682434
    },
    {
      "name": "Mathematics",
      "score": 0.15380465984344482
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Process (computing)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802355861",
      "name": "Australian Institute of Business",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I114017466",
      "name": "University of Technology Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 32
}