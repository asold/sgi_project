{
    "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
    "url": "https://openalex.org/W2971292190",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Rietzler, Alexander",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287496001",
            "name": "Stabinger, Sebastian",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Opitz, Paul",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Engl, Stefan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962816513",
        "https://openalex.org/W2950621595",
        "https://openalex.org/W2963168371",
        "https://openalex.org/W2251648804",
        "https://openalex.org/W2160660844",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2903110172",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2954278700",
        "https://openalex.org/W2925618549",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2534704652",
        "https://openalex.org/W2964098749",
        "https://openalex.org/W2916076862",
        "https://openalex.org/W2979736514",
        "https://openalex.org/W2963428430"
    ],
    "abstract": "Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based Sentiment Analysis (ABSA), which has many applications e.g. in e-commerce, where data and insights from reviews can be leveraged to create value for businesses and customers. Recently, deep transfer-learning methods have been applied successfully to a myriad of Natural Language Processing (NLP) tasks, including ATSC. Building on top of the prominent BERT language model, we approach ATSC using a two-step procedure: self-supervised domain-specific BERT language model finetuning, followed by supervised task-specific finetuning. Our findings on how to best exploit domain-specific language model finetuning enable us to produce new state-of-the-art performance on the SemEval 2014 Task 4 restaurants dataset. In addition, to explore the real-world robustness of our models, we perform cross-domain evaluation. We show that a cross-domain adapted BERT language model performs significantly better than strong baseline models like vanilla BERT-base and XLNet-base. Finally, we conduct a case study to interpret model prediction errors.",
    "full_text": "Adapt or Get Left Behind:\nDomain Adaptation through BERT Language Model Finetuning for\nAspect-Target Sentiment Classiﬁcation\nAlexander Rietzler, Sebastian Stabinger, Paul Opitz, Stefan Engl\nDeepOpinion.ai at Innsbruck, Austria\n{firstname.lastname}@deepopinion.ai\nAbstract\nAspect-Target Sentiment Classiﬁcation\n(ATSC) is a subtask of Aspect-Based Sen-\ntiment Analysis (ABSA), which has many\napplications e.g. in e-commerce, where data\nand insights from reviews can be leveraged\nto create value for businesses and customers.\nRecently, deep transfer-learning methods\nhave been applied successfully to a myriad\nof Natural Language Processing (NLP) tasks,\nincluding ATSC. Building on top of the promi-\nnent BERT language model, we approach\nATSC using a two-step procedure: self-\nsupervised domain-speciﬁc BERT language\nmodel ﬁnetuning, followed by supervised\ntask-speciﬁc ﬁnetuning. Our ﬁndings on\nhow to best exploit domain-speciﬁc language\nmodel ﬁnetuning enable us to produce new\nstate-of-the-art performance on the SemEval\n2014 Task 4 restaurants dataset. In addition,\nto explore the real-world robustness of our\nmodels, we perform cross-domain evaluation.\nWe show that a cross-domain adapted BERT\nlanguage model performs signiﬁcantly better\nthan strong baseline models like vanilla\nBERT-base and XLNet-base. Finally, we\nconduct a case study to interpret model\nprediction errors.\n1 Introduction\nSentiment Analysis (SA) is an active ﬁeld of re-\nsearch in Natural Language Processing and deals\nwith opinions in text. A typical application of clas-\nsical SA in an industrial setting would be to clas-\nsify a document like a product review intopositive,\nnegative or neutral sentiment polarity.\nIn constrast to SA, the more ﬁne-grained task\nof Aspect Based Sentiment Analysis (ABSA) (Hu\nand Liu, 2004; Pontiki et al., 2015) aims to ﬁnd\nboth the aspect of an entity like a restaurant, and\nthe sentiment associated with this aspect.\nIt is important to note that ABSA comes in two\nvariants. We will use the sentence “I love their\ndumplings” to explain these variants in detail.\nBoth variants are implemented as a two-\nstep procedure. The ﬁrst variant is com-\nprised of Aspect-Category Detection (ACD) fol-\nlowed by Aspect-Category Sentiment Classiﬁca-\ntion (ACSC). ACD is a multilabel classiﬁcation\ntask, where a sentence can be associated with a\nset of predeﬁned aspect categories like“food” and\n“service” in the restaurants domain. In the second\nstep, ACSC, the sentiment polarity associated to\nthe aspect-category is classiﬁed. For our example-\nsentence the correct result is the tuple (“food”,\n“positive”).\nThe second variant consists of Aspect-Target\nExtraction (ATE) followed by Aspect-Target Sen-\ntiment Classiﬁcation (ATSC). ATE is a sequence\nlabeling task, where terms like “dumplings” are\ndetected. In the second step, ATSC, the senti-\nment polarity associated to the aspect-target is de-\ntermined. In our example the correct result is\n(”dumplings”, ”positive”) .\nIn this paper, we focus on ATSC. In recent\nyears, specialized neural architectures (Tang et al.,\n2016a,b) have been developed that substantially\nimproved modeling of this target-context relation-\nship. More recently, the Natural Language Pro-\ncessing community experienced a substantial shift\ntowards using pre-trained language models (Peters\net al., 2018; Radford and Salimans, 2018; Howard\nand Ruder, 2018; Devlin et al., 2019) as a base for\nmany down-stream tasks, including ABSA (Song\net al., 2019; Xu et al., 2019; Sun et al., 2019). We\nstill see huge potential that comes with this trend,\nwhich is why we approach the ATSC task using\nthe BERT architecture.\nAs shown by Xu et al. (2019), for the ATSC\ntask the performance of models that were pre-\ntrained on general text corpora is improved sub-\nstantially by ﬁnetuning the language model on\narXiv:1908.11860v2  [cs.CL]  19 Nov 2019\ndomain-speciﬁc corpora — in their case review\ncorpora — that have not been used for pre-training\nBERT, or other language models.\nWe extend the work by Xu et al. by further in-\nvestigating the behavior of ﬁnetuning the BERT\nlanguage model in relation to ATSC performance.\nIn particular, our contributions are:\n1. Analysis of the inﬂuence of the amount of\ntraining-steps used for BERT language model\nﬁnetuning on the Aspect-Target Sentiment\nClassiﬁcation performance.\n2. Findings on how exploiting BERT language\nmodel ﬁnetuning enables us to achieve new\nstate-of-the-art performance on the SemEval\n2014 restaurants dataset.\n3. Analysis of cross-domain adaptation between\nthe laptops and restaurants domains. Adap-\ntation is tested by self-supervised ﬁnetuning\nof the BERT language model on the target-\ndomain and then supervised training on the\nATSC task in the source-domain. In addition,\nthe performance of training on the combina-\ntion of both datasets is measured.\n2 Related Works\nWe separate our discussion of related work into\ntwo areas: ﬁrst, neural methods applied to ATSC\nthat have improved performance solely by model\narchitecture improvements. Secondly, methods\nthat additionally aim to transfer knowledge from\nsemantically related tasks or domains.\nArchitecture Improvements for Aspect-Target\nSentiment Classiﬁcation\nThe datasets typically used for Aspect-Target Sen-\ntiment Classiﬁcation are the SemEval 2014 Task 4\ndatasets (Pontiki et al., 2015) for the restaurants\nand laptops domain. Both datasets have only a\nsmall number of training examples. One common\napproach to compensate for insufﬁcient training\nexamples is to invent neural architectures that bet-\nter model ATSC. For example, in the past a big\nleap in classiﬁcation performance was achieved\nwith the use of the Memory Network architec-\nture (Tang et al., 2016b), which uses memory to\nremember context words and explicitly models at-\ntention over both the target word and context. It\nwas found that making full use of context words\nimproves their model compared to previous mod-\nels (Tang et al., 2016a) that make use of left- and\nright-sided context independently.\nSong et al. (2019) proposed Attention Encoder\nNetworks (AEN), a modiﬁcation to the trans-\nformer architecture. The authors split the Multi-\nHead Attention (MHA) layers into Intra-MHA and\nInter-MHA layers in order to model target words\nand context differently, which results in a more\nlightweight model compared to the transformer ar-\nchitecture.\nAnother recent performance leap was achieved\nby Zhaoa et al. (2019), who model dependen-\ncies between sentiment words explicitly in sen-\ntences with more than one aspect-target by using\na graph convolutional neural network. They show\nthat their architecture performs particularly well if\nmultiple aspects are present in a sentence.\nKnowledge Transfer for Aspect-Target\nSentiment Classiﬁcation Analysis\nOne approach to compensate for insufﬁcient train-\ning examples is to transfer knowledge across do-\nmains or across similar tasks.\nLi et al. (2019) proposed Multi-Granularity\nAlignment Networks (MGAN). They use this ar-\nchitecture to transfer knowledge from both an\naspect-category classiﬁcation task and also across\ndifferent domains. They built a large scale aspect-\ncategory dataset speciﬁcally for this.\nHe et al. (2018) transfer knowledge from\na document-level sentiment classiﬁcation task\ntrained on the Amazon review dataset (He and\nMcAuley, 2016). They successfully apply pre-\ntraining by reusing the weights of a Long Short\nTerm Memory (LSTM) network (Hochreiter and\nSchmidhuber, 1997) that has been trained on\nthe document-level sentiment task. In addition,\nthey apply multi-task learning where aspect and\ndocument-level tasks are learned simultaneously\nby minimizing a joint loss function.\nSimilarly, Xu et al. (2019) introduce a multi-\ntask loss function to simultaneously optimize on\nBERT model’s (Devlin et al., 2019) pre-training\nobjectives as well as a question answering task.\nIn contrast to the methods described above that\naim to transfer knowledge from a different source\ntask - like question answering or document-level\nsentiment classiﬁcation - this paper aims at trans-\nferring knowledge across different domains by\nself-supervised ﬁnetuning of the BERT language\nmodel.\n3 Methodology\nWe approach the Aspect-Target Sentiment Classi-\nﬁcation task using a two-step procedure. We use\nthe pre-trained BERT architecture as a basis. In\nthe ﬁrst step we ﬁnetune the pre-trained weights\nof the language model further in a self-supervised\nway on a domain-speciﬁc corpus. In the second\nstep we train the ﬁnetuned language model in a\nsupervised way on the ATSC end-task.\nIn the following subsections, we discuss the\nBERT architecture, how we ﬁnetune the language\nmodel, and how we transform the ATSC task into a\nBERT sequence-pair classiﬁcation task (Sun et al.,\n2019). Subsequently, we discuss the different end-\ntask training and domain-speciﬁc ﬁnetuning com-\nbinations we employ to evaluate our model’s gen-\neralization performance not only in-domain but\nalso cross-domain.\nFinally, we describe how we apply input re-\nduction, an interpretation method for neural NLP\nmodels, to the ATSC task.\n3.1 BERT\nThe BERT model builds on many previous innova-\ntions: contextualized word representations (Peters\net al., 2018), the transformer architecture (Vaswani\net al., 2017), and pre-training on a language mod-\neling task with subsequent end-to-end ﬁnetun-\ning on a downstream task (Radford and Sali-\nmans, 2018; Howard and Ruder, 2018). Due\nto being deeply bidirectional, the BERT archi-\ntecture creates powerful sequence representations\nthat perform extremely well on many downstream\ntasks (Devlin et al., 2019).\nThe main innovation of BERT is that instead\nof using the objective of next-word prediction, a\ndifferent objective is used to train the language\nmodel. This objective consists of two parts.\nThe ﬁrst part is the masked language model\nobjective, where the model learns to predict ran-\ndomly masked tokens from their context.\nThe second part is the next-sequence prediction\nobjective, where the model needs to predict if a\nsequence B would naturally follow the previous\nsequence A. This objective enables the model to\ncapture long-term dependencies better. Both ob-\njectives are discussed in more detail in the next\nsection.\nAs a base for our experiments we use the\nBERTBASE model, which has been pre-trained by\nthe Google research team. It has the following pa-\nrameters: 12 layers, 768 hidden dimensions per\ntoken and 12 attention heads. It has 110 million\nparameters in total.\nFor ﬁnetuning the BERT language model on a\nspeciﬁc domain we use the weights of BERT BASE\nas a starting point.\n3.2 BERT Language Model Finetuning\nAs the ﬁrst step of our procedure we perform lan-\nguage model ﬁnetuning of the BERT model us-\ning domain-speciﬁc corpora. Algorithmically, this\nis equivalent to pre-training. The domain-speciﬁc\nlanguage model ﬁnetuning as an intermediate step\nto ATSC has been described by Xu et al. (2019).\nAs an extension to their paper we investigate the\nlimits of language model ﬁnetuning in terms of\nhow end-task performance is dependent on the\namount of training steps.\nThe training input representation for language\nmodel ﬁnetuning consists of two sequencessA and\nsB in the format of “[CLS] sA [SEP] sB [SEP]”,\nwhere [CLS] is a dummy token used for down-\nstream classiﬁcation and [SEP] are separator to-\nkens.\nMasked Language Model Objective\nThe sequences A and B have tokens randomly\nmasked out in order for the model to learn to pre-\ndict them. The following example shows how\ndomain-speciﬁc ﬁnetuning could alleviate the bias\nfrom pre-training on a Wikipedia corpus: “The\ntouchscreen is an [MASK] device”. In the fact-\nbased context of Wikipedia the [MASK] could be\n“input” and in the review domain a typical guess\ncould be the general opinion word “amazing”.\nNext-Sentence Prediction\nIn order to train BERT to capture long-term de-\npendencies better, the model is trained to predict\nwhether sequence Bfollows sequence A. If this is\nthe case, sequence A and sequence B are jointly\nsampled from the same document in the order\nthey appear naturally. Otherwise the sequences are\nsampled randomly from the training corpus.\n3.3 Aspect-Target Sentiment Classiﬁcation\nThe ATSC task aims at classifying sentiment po-\nlarity into the three classes positive, negative, neu-\ntral with respect to an aspect-target. The input\nto the classiﬁer is a tokenized sentence s = s1:n\nand a target t= sj:j+m contained in the sentence,\nwhere j < j+ m ≤n. Similar to previous work\nby Sun et al. (2019), we transform the input into a\nformat compatible with BERT sequence-pair clas-\nsiﬁcation tasks: “[CLS] s[SEP] t[SEP]”.\nIn the BERT architecture the position of the\ntoken embeddings is structurally maintained af-\nter each Multi-Head Attention layer. Therefore,\nwe refer to the last hidden representation of the\n[CLS] token as h[CLS] ∈R768×1. The number of\nsentiment polarity classes is three. A distribution\np ∈[0,1]3 over these classes is predicted using a\nfully-connected layer with 3 output neurons on top\nof h[CLS], followed by a softmax activation func-\ntion\np= softmax(W ·h[CLS] + b),\nwhere b∈R3 and W ∈R3×768. Cross-entropy is\nused as the training loss. The way we use BERT\nfor classifying the sentiment polaritites is equiva-\nlent to how BERT is used for sequence-pair clas-\nsiﬁcation tasks in the original paper (Devlin et al.,\n2019).\n3.4 Domain Adaptation through Language\nModel Finetuning\nIn academia, it is common that the performance of\na machine learning model is evaluated in-domain.\nThis means that the model is evaluated on a test\nset that comes from the same distribution as the\ntraining set. In real-world applications this setting\nis not always valid, as the trained model is used to\npredict previously unseen data.\nIn order to evaluate the performance of a ma-\nchine learning model more robustly, its general-\nization error can be evaluated across different do-\nmains, i.e. cross-domain. To optimize cross-\ndomain performance, the model itself can be\nadapted towards a target domain. This procedure\nis known as Domain Adaptation, which is a spe-\ncial case of Transductive Transfer Learning in the\ntaxonomy of Ruder (2019). Here, it is typically\nassumed that supervised data for a speciﬁc task\nis only available for a source domain S, whereas\nonly unsupervised data is available in the target\ndomain T. The goal is to optimize performance\nof the task in the target domain while transferring\ntask-speciﬁc knowledge from the source domain.\nIf we map this framework to our challenge, we\ndeﬁne Aspect-Target Sentiment Classiﬁcation as\nthe transfer-task and BERT language model ﬁne-\ntuning is used for domain adaptation. In terms of\nwhich domain is ﬁnetuned on, the full transfer-\nprocedure can be expressed in the following way:\nDLM →DTrain →DTest .\nHere, DLM stands for the domain on which the\nlanguage model is ﬁnetuned and can take on the\nvalues of Restaurants, Laptops or (Restaurants ∪\nLaptops). The domain for training DTrain can\ntake on the same values; for the joint case the train-\ning datasets for laptops and restaurants are simply\ncombined. The domain for testing DTest can only\ntake the value Restaurants or Laptops.\nCombining ﬁnetuning and training steps gives\nus nine different evaluation scenarios, which we\ngroup into the following four categories:\nIn-Domain Training\nATSC is trained on a domain-speciﬁc dataset and\nevaluated on the test set from the same domain.\nThis can be expressed as\nDLM →T →T, where T is our target domain\nand can be either Laptops or Restaurants. It is ex-\npected that the performance of the model is high-\nest if DLM = T.\nCross-Domain Training\nATSC is trained on a domain-speciﬁc dataset and\nevaluated on the test set from the other domain.\nThis can be expressed as\nDLM →S →T, where S ̸= T are source and\ntarget domain and can be eitherLaptops or Restau-\nrants.\nCross-Domain Adaptation\nAs a special case of cross-domain training we ex-\npect performance to be optimal ifDLM = T. This\nis the variant of Domain Adaptation and is written\nas\nT →S →T.\nJoint-Domain Training\nATSC is trained on both domain-speciﬁc datasets\njointly and evaluated on both test sets indepen-\ndently. This can be expressed as\nDLM →(S∪T) →T, where S ̸= T are source-\nand target domain and can either be Laptops or\nRestaurants.\n3.5 Input Reduction for Model Interpretation\nInput reduction is an interpretation method for\nneural models introduced by Feng et al. (2018),\nwhich tries to ﬁnd a subset of the most important\nwords of a document that contribute most to a pre-\ndiction.\nWe use this interpretation method to illustrate\nthe predictions of our models on the test set in or-\nder to ﬁnd causes for classiﬁcation errors, and also\nto ﬁnd qualitative differences between our models\nand baseline models.\nThe input reduction method resembles a process\nthat iteratively removes unimportant words from\nthe input while the model’s prediction is main-\ntained. The idea is that the remaining set of words\none iteration before the prediction ﬂips are the\nmost important ones. As pointed out by Feng et al.\n(2018) for this method to work, a machine learning\nmodel needs to compute meaningful conﬁdence\nvalues for unseen input. For our task, we ﬁnd em-\npirically that the predicted probabilities computed\nfor our test set examples work well enough as a\nconﬁdence approximation, which means that most\nof the reduced input for the examples discussed in\nsubsection 4.5 allows for a meaningful interpreta-\ntion.\nLet x = [ x1,x2,...x n] be the input sen-\ntence represented as a list of tokens and p(y|x)\nthe predicted probability of label y, and y =\nargmaxˆy p(ˆy|x) the originally predicted label. The\nimportance of a word is deﬁned as\ng(xi) =p(y|x) −p(y|x−i).\nPut differently, the importance of a word is the\nprediction probability towards the original label of\na sentence containing the word minus the predic-\ntion probability of the sentence without the same\nword.\nWe apply this formula to iteratively remove the\nword with the lowest importance until the predic-\ntion changes to another label. Due to the nature of\nthe ATSC task, we make an exception for words\nthat are part of the aspect-target phrase, which we\ndo not remove during an iteration. This allows us\nto maintain the context with respect to the aspect-\ntarget.\n4 Experiments\nIn our experiments we aim to answer the following\nresearch questions (RQs):\nRQ1: How does the number of training iter-\nations in the BERT language model ﬁnetuning\nstage inﬂuence the ATSC end-task performance?\nAt what point does performance start to improve,\nwhen does it converge?\nRQ2: If trained in-domain, what ATSC end-\ntask performance can be reached through fully ex-\nploited ﬁnetuning of the BERT language model?\nRQ3: If trained cross-domain in the special case\nof domain adaptation, what ATSC end-task per-\nformance can be reached if BERT language model\nﬁnetuning is fully exploited?\n4.1 Datasets for Classiﬁcation and Language\nModel Finetuning\nWe conduct experiments using the two SemEval\n2014 Task 4 Subtask 2 datasets 1 (Pontiki et al.,\n2015) for the laptops and the restaurants domain.\nThe two datasets contain sentences with one or\nmultiple marked aspect-targets that each have a 3-\nlevel sentiment polarity ( positive, neutral or neg-\native) associated. In the original dataset the con-\nﬂict class is also present. Here, the conﬂict labels\nare dropped for reasons of comparability with Xu\net al. (2019). Detailed statistics for both datasets\nare shown in Table 1.\nFor BERT language model ﬁnetuning we pre-\npare three corpora for the two domains of lap-\ntops and restaurants. For the restaurants domain\nwe use Yelp Dataset Challenge reviews 2 and for\nthe laptops domain we use Amazon Laptop re-\nviews (He and McAuley, 2016). For the laptop\ndomain we ﬁltered out reviews that appear in the\nSemEval 2014 laptops dataset to avoid training\nbias for the test data. To be compatible with the\nnext-sentence prediction task used during ﬁne tun-\ning, we removed reviews containing fewer than\ntwo sentences from the corpora.\nFor the laptop corpus, 1,007,209 sentences are\nleft after pre-processing. For the restaurants do-\nmain, where more reviews are available, we sam-\npled 10,000,000 sentences to have a sufﬁcient\namount of data for fully exploited language model\nﬁnetuning. In order to compensate for the smaller\namount of ﬁnetuning data in the laptops domain,\nwe ﬁnetune for more epochs, 30 epochs in the case\nof the laptops domain compared to 3 epochs for\nthe restaurants domain, so that the BERT model\ntrains on about 30 million sentences in both cases.\nThis means that a single sentence can appear mul-\ntiple times with a different language model mask-\ning.\n1http://alt.qcri.org/semeval2014/task4\n2https://www.yelp.com/dataset/\nchallenge\nWe also create a mixed corpus to jointly ﬁne-\ntune on both domains. Here, we sample 1 mil-\nlion restaurant reviews and combine them with the\nlaptop reviews. This results in about 2 million re-\nviews that are ﬁnetuned for 15 epochs. The ex-\nact statistics for the three ﬁnetuning corpora are\nshown in the top of Table 1.\nWe release code to reproduce generation of our\nﬁnetuning corpora3.\nCorpus Sentences Finetuning Epochs\nLaptops 1,007,209 30\nRestaurants 10,000,000 3\nLapt.+Rest. 2,007,213 15\nDataset Positive Negative Neutral\nTrain Test Train Test Train Test\nLaptops 987 341 866 128 460 169\nRestaurants 2,164 728 805 196 633 196\nTable 1: Top: Detailed statistics of the corpora for\nBERT language model ﬁnetuning. Bottom: Number\nof labels for each category of the SemEval 2014 Task\n4 Subtask 2 laptop and restaurant datasets for Aspect-\nTarget Sentiment Classiﬁcation.\n4.2 Hyperparameters\nWe use BERTBASE4 (uncased) as the base for all of\nour experiments, with the exception of XLNetBASE\n(cased), which is used as one of the baseline mod-\nels.\nFor the BERT language model ﬁnetuning we\nuse 32 bit ﬂoating point computations using the\nAdam optimizer (Kingma and Ba, 2014). The\nbatchsize is set to 32 while the learning rate is set\nto 3 ·10−5. The maximum input sequence length\nis set to 256 tokens, which amounts to about 4 sen-\ntences per sequence on average. As shown in Ta-\nble 1, we ﬁnetune the language models on each\ndomain so that the model trains a total of about 30\nmillion sentences (≈7.5 million sequences).\nFor training the BERT and XLNet models on\nthe down-stream task of ATSC we use mixed 16\nbit and 32 bit ﬂoating point computations, the\nAdam optimizer, and a learning rate of 3 ·10−5\nand a batchsize of 32. We train the model for a to-\ntal of 7 epochs. The validation accuracy converges\n3https://github.com/deepopinion/\ndomain-adapted-atsc\n4We make use of both BERT-base-uncased and XLNet-\nbase-cased models as part of the pytorch-transformers\nlibrary: https://github.com/huggingface/\npytorch-transformers\nafter about 3 epochs of training on all datasets, but\ntraining loss still improves after that.\nIt is important to note that all our results re-\nported are the average of 9 runs with different ran-\ndom initializations. This is needed to measure sig-\nniﬁcance of improvements, as the standard devia-\ntion in accuray amounts to roughly 1% for all ex-\nperiments (see Figure 1).\n4.3 Compared Methods\nWe compare in-domain results to current state-\nof-the-art methods, which we will now describe\nbrieﬂy.\nSDGCN-BERT (Zhaoa et al., 2019) explicitly\nmodels sentiment dependencies for sentences with\nmultiple aspects with a graph convolutional net-\nwork. This method is current state-of-the-art on\nthe SemEval 2014 laptops dataset.\nAEN-BERT (Song et al., 2019) is an attentional\nencoder network. When used on top of BERT em-\nbeddings this method performs especially well on\nthe laptops dataset.\nBERT-SPC (Song et al., 2019) is BERT used in\nsentence-pair classiﬁcation mode. This is exactly\nthe same method as our BERT-base baseline and\ntherefore, we can cross-check the authors’ results.\nBERT-PT (Xu et al., 2019) uses multi-task ﬁne-\ntuning prior to downstream classiﬁcation, where\nthe BERT language model is ﬁnetuned jointly with\na question answering task. It has state-of-the-art\nperformance on the restaurants dataset prior to this\npaper.\nTo our knowledge, cross- and joint-domain\ntraining on the SemEval 2014 Task 4 datasets\nhas not been analyzed so far. Thus, we compare\nour method to two very strong baseline models:\nBERT-base and XLNet-base.\nBERT-base (Devlin et al., 2019) is using the pre-\ntrained BERT BASE embeddings directly on the\ndown-stream task without any domain speciﬁc\nlanguage model ﬁnetuning.\nXLNet-base (Yang et al., 2019) is a method also\nbased on general language model pre-training sim-\nilar to BERT. Instead of randomly masking tokens\nfor pre-training like BERT, a more general permu-\ntation objective is used, where all possible variants\nof masking are fully exploited.\nOur models are BERT models whose language\nmodel has been ﬁnetuned on different domain cor-\npora.\nBERT-ADA Lapt is the BERT language model\n0 5 10 15 20 25 30\nNr. LM-Finetuned Sentences (million)\n−1\n0\n1\n2\n3\n4\nAccuracy Improvement (%)\nAccuracy Improvement vs. Nr. LM-Finetuned Sentences\nLaptops\nRestaurants\nFigure 1: Absolute accuracy improvement of Aspect-\nTarget Sentiment Classiﬁcation as a function of the\nnumber of sentences the BERT language model has\nbeen ﬁnetuned on. Markers ( ■,♦) connected through\nthe lines are the averages (µ) over 9 runs, a single run is\nmarked as either a cross (×for restaurants) or a plus (+\nfor laptops). The standard deviation (σ) curves are also\ndrawn (µ±σ). The model is trained on the SemEval\n2014 Task 4 datasets and evaluated in-domain. The lan-\nguage models are ﬁnetuned on the target-domain cor-\npora. Best viewed in color.\nﬁnetuned on the laptop domain corpus.\nBERT-ADA Rest is the BERT language model\nﬁnetuned on the restaurant domain corpus.\nBERT-ADA Joint is the BERT language model\nﬁnetuned on the corpus containing an equal\namount of laptops and restaurants reviews.\n4.4 Results Analysis\nThe results of our experiments are shown in\nFigure 1 and Table 2 respectively.\nTo answer RQ1, which is concerned with details\nof domain-speciﬁc language model ﬁnetuning, we\ncan see in Figure 1 that ﬁrst of all, language model\nﬁnetuning has a signiﬁcant effect on ATSC end-\ntask performance. Secondly, we see that in the\nrestaurants domain the performance starts to in-\ncrease immediately, whereas in the laptops domain\nit takes about 10 million ﬁnetuned sentences be-\nfore a signiﬁcant increase can be measured. Af-\nter around 17 million sentences no signiﬁcant im-\nprovement can be measured. In addition, we ﬁnd\nthat the different runs have a high variance, which\nnecessitates averaging over 9 runs to measure dif-\nferences in model performance reliably.\nTo answer RQ2, which is concerned with in-\ndomain ATSC performance, we see in Table 2 that\nfor the in-domain training case, our models BERT-\nADA Lapt and BERT-ADA Rest achieve per-\nformance close to state-of-the-art on the laptops\ndataset and new state-of-the-art on the restaurants\ndataset with accuracies of 79.19% and 87.14%,\nrespectively. On the restaurants dataset, this cor-\nresponds to an absolute improvement of 2.2%\ncompared to the previous state-of-the-art method\nBERT-PT. Language model ﬁnetuning produces\na larger improvement on the restaurants dataset.\nWe think that one reason for that might be that\nthe restaurants domain is underrepresented in the\npre-training corpora of BERTBASE. Generally, we\nﬁnd that language model ﬁnetuning helps even if\nthe ﬁnetuning domain does not match the evalua-\ntion domain. We think the reason for this might\nbe that the BERT-base model is pre-trained more\non knowledge-based corpora like Wikipedia than\non text containing opinions. We show some evi-\ndence for this hypothesis in subsection 4.5. In ad-\ndition, we ﬁnd that the XLNet-base baseline per-\nforms generally stronger than BERT-base, but only\noutperforms the BERT-ADA models on the lap-\ntops dataset with an accuracy of 79.89% .\nTo answer RQ3, which is concerned with do-\nmain adaptation, we can see from the grayed out\ncells in Table 2, which correspond to the cross-\ndomain adaption case where the BERT language\nmodel is trained on the target domain, that do-\nmain adaptation works well with 2.2% absolute\naccuracy improvement on the laptops test set and\neven 3.6% accuracy improvement on the restau-\nrants test set compared to BERT-base.\nIn general, the ATSC task generalizes well\ncross-domain, with about a 2-3% drop in accu-\nracy compared to in-domain training. We think\nthe reason for this might be that syntactical rela-\ntionships between the aspect-target and the phrase\nexpressing sentiment polarity, as well as knowing\nthe sentiment-polarity itself, are sufﬁcient to solve\nthe ATSC task in most cases.\nFor the joint-training case, we ﬁnd that combin-\ning both training datasets improves performance\non both test sets. This result is intuitive, as more\ntraining data generally leads to better performance\nif the domains do not confuse each other. Interest-\ningly, for the joint-training case the BERT-ADA\nJoint model performs especially well when mea-\nsured by the Macro-F1 metric. A reason for this\nmight be that the SemEval 2014 datasets are im-\nbalanced due to dominance of positive labels. It\nseems like through ﬁnetuning the language model\non both domains the model learns to classify the\nneutral class much better, especially in the laptops\ndomain.\nTest Dataset Laptops Restaurants\nTrain Dataset Laptops Restaurants Lapt. + Rest. Restaurants Laptops Lapt. + Rest.\nTrain Type In → Cross↔ Joint∪ In→ Cross↔ Joint∪\nOther Methods Acc MF1 Acc MF1 Acc MF1 Acc MF1 Acc MF1 Acc MF1\nSDGCN-BERT 81.35 78.34 - - - - 83.57 76.47 - - - -\nAEN-BERT 79.93 76.31 - - - - 83.12 73.76 - - - -\nBERT-SPC 78.99 75.03 - - - - 84.46 76.98 - - - -\nBERT-PT 78.07 75.08 - - - - 84.95 76.96 - - - -\nBaselines\nXLNet-base 79.89 77.78 77.78 72.24 80.88 76.92 85.84 78.35 82.41 72.98 86.15 78.93\nBERT-base 77.69 72.60 75.86 70.78 78.81 74.47 84.92 76.93 80.07 69.93 85.03 77.35\nOurs\nBERT-ADA Lapt 79.19 74.18 77.92 72.99 80.23 75.77 85.51 78.09 80.68 72.93 86.22 79.79\nBERT-ADA Rest 78.60 74.09 76.16 70.46 79.14 74.93 87.14 80.05 83.68 72.91 87.89 81.05\nBERT-ADA Joint 78.96 74.18 75.91 69.84 79.94 78.74 86.35 78.89 82.23 73.03 87.69 81.20\nTable 2: Summary of results for Aspect-Target Sentiment Classiﬁcation for in-domain, cross-domain, and joint-\ndomain training on SemEval 2014 Task 4 Subtask 2 datasets. The cells with gray background correspond to the\ncross-domain adaptation case, where the language model is ﬁnetuned on the target domain. As evaluation metrics\naccuracy (Acc) and Macro-F1 (MF1) are used.\n4.5 Case Study\nThe goal of the case study is to ﬁnd answers to the\nfollowing questions:\n•What speciﬁcally causes the ﬁnetuned lan-\nguage models BERT-ADA Lapt and BERT-\nADA Rest to perform better than BERT-base?\n•What reasons can we ﬁnd by comparing con-\nﬂicting predictions made by these models?\n•What are speciﬁc reasons for erroneous clas-\nsiﬁcations?\n•What error types prevent us from performing\nat human expert level on ATSC?\nTo answer these questions we performed input\nreduction, which allows for a better interpreta-\ntion of sample predictions from the SemEval 2014\nRestaurant and Laptops test set, see Table 3. The\ninput reduction technique tries to isolate a set of\nwords from the sentence that contribute most to\nthe prediction. The theoretical details of input re-\nduction are discussed in subsection 3.5.\nSamples predicted correctly by the\ntarget-domain adapted model\nIn the following, we will discuss a selection of ex-\namples that are classiﬁed correctly by the best per-\nforming in-domain BERT-ADA and incorrectly by\nBERT-base. The error types for BERT-base are\nmentioned for all the examples next to their refer-\nence label.\nRC2 – restaurant-domain context needed:\n“not . . . sweet”– the negated sentiment contained\nin this phrase is identiﬁed correctly by BERT-base\nand could be interpreted as negative in isolation,\nbut an adjective like“ﬂuffy” carries a stronger pos-\nitive sentiment for BERT-ADA Rest.\nRC4 – general review-domain context needed:\nWe think that “should be” is an expression of-\nten found in text with opinions, thus both BERT-\nADA Lapt and Rest, which both have been ﬁne-\ntuned on review-speciﬁc text, predict this exam-\nple correctly. BERT-base is strongly inﬂuenced\nby “friendly” and cannot detect the sentiment-\nnegating function of “should be”.\nRC5 – restaurant-domain context needed:\nThe reduced input “gratuity” is detected as pos-\nitive for the BERT-ADA Laptop and BERT-base\nmodel. In contrast, the BERT-ADA Rest model\nreveals reduced input words “automatically” and\n“bill” to detect the negative sentiment correctly.\nLC2 – laptop-domain context needed:\n“very quiet” is classiﬁed as negative by BERT-\nbase whereas the same expression is classiﬁed\npositive by the BERT-ADA Lapt model. BERT-\nADA Rest classiﬁes “aren’t audible” as negative.\nRef. Restaurant Samples Aspect\nBase\nLapt\nRest\nGold\nRC1 Certainly not the best R sushi in New York, however, it is\nalways fresh, and the place is very cleanL, sterileB.\nplace – + + +\nRC2 the icing L MADE this cake, it was ﬂuffy R, not B ultra\nsweetB, creamy and light.\ncake – + + +\nRC3 The sangria’s - watered B,L,R downL. sangria + – – –\nRC4 The staff should L,R be a bit moreL friendlyB. staff + – – –\nRC5 15% B gratuity automaticallyR,L added to the billR. gratuity + + – –\nRE1 My friend had L a burger and I had these wonderfulB,R blue-\nberry pancakes.\nburger o + + o\nRE2 The sauce is excellent B,L,R (very fresh) with dabs of real\nmozzarella.\ndabs of real\nmozzarella\n+ + + o\nLaptop Samples\nLC1 the retina display display make pictures i R tookB,L yearsB\nago jawR dropping.\nretina dis-\nplay display\n– + + +\nLC2 The Mac mini is about 8x smaller than my old computer\nwhich is a huge bonus and runs B very quietB,L, actually B\nthe fans aren’t audibleR unlike my old pc\nfans – + – +\nLE1 the latest version does not B,R,L have a disc drive. disc drive – – – o\nLE2 Which it did not B,R have, onlyL 3 USB 2 ports. USB 2 ports – – – o\nTable 3: Shown are text samples from SemEval 2014 Restaurants and Laptops test-set that are predicted correctly\nfor the language model adapted to the target domain but predicted falsely with the bert-base model (RC1-5, LC1-\nLC2). In addition, samples which are predicted falsely by the target-domain adapted model are shown (RE1-2,\nLE1-2). The abbrevations stand for: B – BERT-base, L – BERT-ADA Lapt(op), R – BERT-ADA Rest(aurant) –\nall the language models used for prediction. The used down-stream-classiﬁers are trained in-domain. The reduced\ninput (set of words that inﬂuence prediction strongest) is formatted with underline and the subscript denotes the\ncorresponding model (B, L, R) used for computing the reduced input. If viewed in color, the corresponding\npredicted sentiment polarity of the reduced input corresponds to: green – positive, red – negative, gray – neutral,\nalternating green and red – both negative and positive for different models. Best viewed in color.\nSamples predicted incorrectly by the\ntarget-domain adapted model\nIn the following, we investigate examples that are\nclassiﬁed incorrectly by the BERT-ADA models.\nThis helps us to understand the remaining error\ntypes and shows a way forward for future work.\nThe majority of incorrect predictions come from\nthe ground-truth neutral class, which in most cases\nis confused with the positive class for restaurants\nand with the negative class for laptop reviews.\nRE1 – inﬂuenced by sentiment towards a different\naspect-target:\nThis examples was classiﬁed correctly only by the\nBERT-ADA Laptop model. The reduced input for\nthis model is the word “’had”, which is used a lot\nin fact based formulations like for example “the\nCPU had 3 GHz”. From experience, we think that\nthis type of formulation appears more often in the\nlaptops than in the restaurant domain. The BERT-\nADA Restaurant and BERT-base model both seem\nto be inﬂuenced by the sentiment associated to an-\nother aspect-target.\nRE2 – inﬂuenced by sentiment towards a different\naspect-target:\nWords indicating a certain kind of relation to the\naspect-target like “with” in this example could be\nused to separate the aspect-target speciﬁc senti-\nment from the general sentiment. We think that\nwith more supervised data this case should be\nsolvable by learning these relations in a general\nway.\nLE1 – absence of something like a part classiﬁed\nas negative:\n“not” is classiﬁed as negative by BERT-ADA\nLapt. In the laptops domain the largest remaining\nconfusion are neutral examples classiﬁed as neg-\native examples by our algorithm. It seems if ab-\nsences of parts like a “disk drive” are mentioned,\nthe algorithm tends to classify this as negative.\nIn other examples these statements of absence of\nthings actually imply a negative sentiment.\nLE2 – possibly incorrect ground truth:\nA handful of examples like this one are, in our\nopinion, labelled incorrectly. We think the word\n“only” indicates negative sentiment in this exam-\nples.\nTo summarize, we ﬁnd that in order to correctly\npredict aspect-target based sentiment, the context\nsensitivity of the sentiment expression plays a im-\nportant role in difﬁcult examples. By ﬁnetuning\nthe language model on domain-speciﬁc text the\nmodel is able to capture this knowledge most of\nthe time, even if such expressions are not directly\nobserved in the training set used for downstream-\nclassiﬁcation.\nWe see that especially neutral examples are\nmore difﬁcult to classify correctly. Some of these\nexamples could be solved for an applied real-\nworld case with more supervised data that allows\nto learn more abstract relationships between enti-\nties like sauce and its ingredients in example RS7\nand contain more fact-based formulations to dis-\ncriminate the neutral class better. We also think\nthat selecting ﬁnetuning corpora more carefully\nwith these error types in mind could also lead\nto improvements of classiﬁcation performance on\nthese datasets.\n5 Conclusion\nWe performed experiments on the task of Aspect-\nTarget Sentiment Classiﬁcation by ﬁrst ﬁnetun-\ning a pre-trained BERT model on a domain spe-\nciﬁc corpus with subsequent training on the down-\nstream classiﬁcation task.\nWe analyzed the behavior of the number of\ndomain-speciﬁc BERT language model ﬁnetuning\nsteps in relation to the end-task performance.\nWith the ﬁndings on how to best exploit BERT\nlanguage model ﬁnetuning we were able to train\nhigh performing models, of which the one trained\non SemEval 2014 Task 4 restaurants dataset\nachieves new state-of-the-art performance.\nWe further evaluated our models cross-domain\nto explore the robustness of Aspect-Target Senti-\nment Classiﬁcation. We found that with our setup,\nthis task transfers well between the laptops and the\nrestaurants domain.\nAs a special case we ran a cross-domain adap-\ntation experiments, where the BERT language\nmodel is speciﬁcally ﬁnetuned on the target do-\nmain. We achieve signiﬁcant improvement over\nunadapted models: one cross-domain adapted\nmodel performs even better than a BERT-base\nmodel that is trained in-domain.\nOverall, our ﬁndings reveal promising direc-\ntions for follow-up work. The XLNet-base model\nperforms strongly on the ATSC task. Here,\ndomain-speciﬁc ﬁnetuning could probably bring\nsigniﬁcant performance improvements. Another\ninteresting direction for future work would be\nto investigate cross-domain behavior for an addi-\ntional domain like hotels, which is more similar to\nthe restaurants domain.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186.\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,\nPedro Rodriguez, and Jordan Boyd-Graber. 2018.\nPathologies of neural models make interpretations\ndifﬁcult. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 3719–3728, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2018. Exploiting document knowledge\nfor aspect-level sentiment classiﬁcation. In ACL\n2018 - 56th Annual Meeting of the Association for\nComputational Linguistics, Proceedings of the Con-\nference (Long Papers), volume 2, pages 579–585.\nRuining He and Julian McAuley. 2016. Ups and\nDowns. In Proceedings of the 25th International\nConference on World Wide Web - WWW ’16, pages\n507–517, New York, New York, USA. ACM Press.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL 2018 - 56th Annual Meeting of the Association\nfor Computational Linguistics, Proceedings of the\nConference (Long Papers) , volume 1, pages 328–\n339.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the 2004\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining - KDD ’04 , page\n168.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nZheng Li, Ying Wei, Yu Zhang, Xiang Zhang, Xin Li,\nand Qiang Yang. 2019. Exploiting Coarse-to-Fine\nTask Transfer for Aspect-level Sentiment Classiﬁ-\ncation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, pages 4253—-4260.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos,\nHarris Papageorgiou, Ion Androutsopoulos, and\nSuresh Manandhar. 2015. SemEval-2014 Task 4:\nAspect Based Sentiment Analysis. In Proceedings\nof the 8th International Workshop on Semantic Eval-\nuation (SemEval 2014) , pages 27–35, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nAlec Radford and Tim Salimans. 2018.\nImproving Language Understanding\nby Generative Pre-Training. URL\nhttps://s3-us-west-2.amazonaws.com/openai-\nassets/research-covers/language-\nunsupervised/language understanding paper.pdf.\nSebastian Ruder. 2019. Neural Transfer Learning for\nNatural Language Processing. Ph.D. thesis.\nYouwei Song, Jiahai Wang, Tao Jiang, Zhiyue Liu, and\nYanghui Rao. 2019. Attentional encoder network\nfor targeted sentiment classiﬁcation. arXiv preprint\narXiv:1902.09314.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Uti-\nlizing BERT for Aspect-Based Sentiment Analysis\nvia Constructing Auxiliary Sentence. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 380–385.\nDuyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.\n2016a. Effective LSTMs for target-dependent senti-\nment classiﬁcation. In COLING 2016 - 26th Inter-\nnational Conference on Computational Linguistics,\nProceedings of COLING 2016: Technical Papers ,\npages 3298–3307.\nDuyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect\nLevel Sentiment Classiﬁcation with Deep Memory\nNetwork. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 214–224.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 2017-Decem, pages 5999–\n6009.\nHu Xu, Bing Liu, Lei Shu, and Philip S Yu. 2019.\nBERT Post-Training for Review Reading Compre-\nhension and Aspect-based Sentiment Analysis. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 2324–\n2335. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nPinlong Zhaoa, Linlin Houb, and Ou Wua. 2019. Mod-\neling sentiment dependencies with graph convolu-\ntional networks for aspect-level sentiment classiﬁ-\ncation. arXiv preprint arXiv:1906.04501."
}