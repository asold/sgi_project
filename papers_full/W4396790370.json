{
    "title": "Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning",
    "url": "https://openalex.org/W4396790370",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Thil, Lucas-Andre\\\"i",
            "affiliations": [
                "Maastricht University"
            ]
        },
        {
            "id": "https://openalex.org/A2280171115",
            "name": "Popa Mirela",
            "affiliations": [
                "Maastricht University"
            ]
        },
        {
            "id": "https://openalex.org/A3048461067",
            "name": "Spanakis, Gerasimos",
            "affiliations": [
                "Maastricht University"
            ]
        },
        {
            "id": null,
            "name": "Thil, Lucas-Andre√Ø",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3116323972",
        "https://openalex.org/W2889037901",
        "https://openalex.org/W2741672218"
    ],
    "abstract": "Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation. Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods. However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results. In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods. We also address a critical limitation in previous models' understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure. To rectify this, we propose methods to enhance true understanding and present a new baseline of results. Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58% average accuracy in SL and 36.69% when combined with a multimodal RL approach. This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks.",
    "full_text": "Navigating WebAI: Training Agents to Complete Web Tasks with\nLarge Language Models and Reinforcement Learning‚àó\nExtended Abstract‚Ä†\nLucas-Andre√Ø Thil\nMaastricht University\nthe Netherlands\nl.thil@student.maastrichtuniversity.nl\nMirela Popa\nMaastricht University\nthe Netherlands\nmirela.popa@maastrichtuniversity.nl\nGerasimos Spanakis\nMaastricht University\nthe Netherlands\njerry.spanakis@maastrichtuniversity.nl\nABSTRACT\nRecent advancements in language models have demonstrated re-\nmarkable improvements in various natural language processing\n(NLP) tasks such as web navigation. Supervised learning (SL) ap-\nproaches have achieved impressive performance while utilizing\nsignificantly less training data compared to previous methods. How-\never, these SL-based models fall short when compared to reinforce-\nment learning (RL) approaches, which have shown superior results.\nIn this paper, we propose a novel approach that combines SL and RL\ntechniques over the MiniWoB benchmark to leverage the strengths\nof both methods. We also address a critical limitation in previous\nmodels‚Äô understanding of HTML content, revealing a tendency to\nmemorize target elements rather than comprehend the underlying\nstructure. To rectify this, we propose methods to enhance true un-\nderstanding and present a new baseline of results. Our experiments\ndemonstrate that our approach outperforms previous SL methods\non certain tasks using less data and narrows the performance gap\nwith RL models, achieving 43.58% average accuracy in SL and 36.69%\nwhen combined with a multimodal RL approach. This study sets a\nnew direction for future web navigation and offers insights into the\nlimitations and potential of language modeling for computer tasks.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíMachine translation ; Plan-\nning with abstraction and generalization ; Deep belief net-\nworks; ‚Ä¢ Human-centered computing ‚ÜíNatural language\ninterfaces; Graphical user interfaces .\nKEYWORDS\nLarge Language Models, Machine Learning, Web, User Interfaces\nACM Reference Format:\nLucas-Andre√Ø Thil, Mirela Popa, and Gerasimos Spanakis. 2024. Navigating\nWebAI: Training Agents to Complete Web Tasks with Large Language\nModels and Reinforcement Learning: Extended Abstract. In Proceedings of\n‚àóProduces the permission block, and copyright information\n‚Ä†The full version of the author‚Äôs guide is available as acmart.pdf document\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain\n¬© 2024 Association for Computing Machinery.\nACM ISBN 979-8-4007-0243-3/24/04. . . $15.00\nhttps://doi.org/10.1145/3605098.3635903\nACM SAC Conference (SAC‚Äô24). (SAC‚Äô24), Avila, Spain, Article 4, 9 pages.\nhttps://doi.org/10.1145/3605098.3635903\n1 INTRODUCTION\nNeural networks have been used to complete web navigation tasks\nusing multimodal inputs such as Humphrey et al [15] which com-\nbined visual and text inputs from a web page. Others focused\non pure text and expanded over instruction mapping methods\n[13, 20, 24] but were very constrained in their language under-\nstanding tasks. The application of language models showed greater\ncapabilities in completing computer tasks, but were all largely con-\nstrained to their environment and had poor transfer abilities. Lately,\nthe Cambrian explosion of large language models opened the way\nfor more capable models requiring less data and offering better\ntransfer capabilities as shown by Gur et al. and Kim et al. [12, 16].\nEven though they achieved outstanding results over different web\nnavigation benchmarks such as Miniwob++, we highlight in this\nwork that they are in fact more limited than previously claimed.\nWe propose several approaches to rectify some of their issues with\nmemorizing information and highlight the challenges of combining\nLLMs with a joint multimodal representation of their environment.\nOur research primarily focuses on optimizing small-scale models\nsuch as grounding them more thoroughly to their environments,\nin order to benchmark their capabilities before contemplating fur-\nther scaling. This prioritization aligns with the imperative for fast\ninference times and adaptability to limited or novel data scenarios.\nIn this paper, we evaluate the limitations of the Miniwob++\nbenchmark and the recorded episodes used in previous works in\norder to present different processing paths to correct their shortcom-\nings. We reproduce the previous results obtained in the literature\nfocusing in the applications of LLMs for web navigation and show\nthat they overfit their benchmark and are unable to recover in\nslight changes in their environments, hampering the claims that\nthey could understand HTML content. We test our techniques on\nnewly trained models that we enhance by devising hierarchical\nplanning abilities that showcase superior results. Then, we com-\nbine them with a multimodal representation using visual inputs\nthat we trained in a supervised manner, and in a reinforcement\nlearning one over multiple phases. We present the abilities of these\nmodels to learn such representations, but they suffer from transfer\nabilities due to the nature of the architectures used.\nThis paper makes several key contributions. First, we provide\ninsights into the capabilities of agents trained in user interactions\nto adeptly navigate diverse web interfaces. Second, we introduce\na more robust evaluation that exposes the shortcomings of these\narXiv:2405.00516v1  [cs.LG]  1 May 2024\nSAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain L. Thil et al.\nmodels due to their tendencies to memorize a lot from their envi-\nronment and how to overcome them. We also set more accurate\nand grounded results over the Miniwob++ benchmark by taking\ninto account the tendencies of previous work to overfit by applying\nattacks to the environment. As we showed that previous works tend\nto learn the distribution of target elements, we provide directions\non how to correct it. Lastly, combined with the improvements over\nour models we deliver a comprehensive analysis of current meth-\nods‚Äô limitations in order to explore more performant architectures,\nfurther contributing to the field toward more capable approaches.\n2 RELATED WORK\nRecent advancements in AI-driven web navigation have led to\nvarious approaches to enhance intelligent agents‚Äô performance and\ncapabilities. This section highlights key methods and techniques\nthat have shaped this project.\n2.1 OpenAI Universe\nOpenAI‚Äôs Universe, released in 2016, aimed to develop a general-\npurpose agent for tasks like video games and web navigation, focus-\ning on Miniwob, a framework with over 80 embedded tasks [22][29].\nMiniwob serves as a reinforcement learning environment, allowing\ncontrolled experimentation with web navigation complexities.\n2.2 Neural Network Oriented Approaches\nSeveral neural-network-oriented models have been devised for\nweb navigation. Notably, Humphrey et al. ‚Äôs CC-Net combines RL\nwith supervised learning, achieving state-of-the-art results on the\nMiniwob Benchmark but requiring 2.4 million examples [15]. Liu et\nal. ‚Äôs Workflow Guided Exploration, which constrains actions during\nRL training, has also shown promise [20].\nOlder works, such as Pasupat et al. [24], focused on traditional\nneural network approaches, while He et al.[13] outlined limitations\nin multimodal environments. These works have identified chal-\nlenges and achieved successes but still face generalization issues.\nThe tradeoff between exploration and exploitation in RL is a\ncommon challenge, often requiring extensive data to converge\nto optimal solutions. While some models, like CC-Net, approach\nhuman capabilities, the need for large amounts of data highlights\nthe importance of investigating more efficient models.\nFigure 1 illustrates the average accuracy of different models over\nthe Miniwob benchmark, comparing training techniques such as\nSL, RL, combined approaches, and few-shots prompting examples.\n2.3 Large Language Models\nEarly works in web navigation with large language models (LLMs)\ninclude Nakano‚Äôs ‚ÄôWebGPT‚Äô [21], which used GPT-3 [8] as a brows-\ning assistant but was limited by not using raw web content. Yao\net al. ‚Äôs ‚ÄôWebShop‚Äô [32] created an e-commerce environment for\nlanguage models but faced restrictions in context window and\nadaptability.\nAttempts to pre-train LLMs on HTML content struggled with\nbenchmarking on classical NLP tasks, limiting their use in web navi-\ngation [2]. However, breakthroughs like Gur et al. ‚Äôs work on \"Under-\nstanding HTML with Large Language Models\" [12] demonstrated\nthe potential of LLMs in web content comprehension, surpassing\nFigure 1: Comparison of Existing Models Regarding the Web\nNavigation Task over the Miniwob Benchmark. Average com-\nparison over the tasks proposed on the Miniwob bench-\nmark between different training techniques and architec-\ntures [12, 15, 16, 20].\nprevious supervised learning (SL) approaches with significantly\nless data.\nKim et al. further showcased the potential of larger LLMs like\nGPT4 [16, 23] in web navigation tasks through iterative prompting,\nachieving state-of-the-art baselines in a few-shot manner.\nDespite these advancements, challenges remain in developing ef-\nficient and adapTable models for web navigation. Existing methods\noften require extensive training data and may struggle with fast\ninference times, highlighting the need for smaller, more capable\nmodels.\n3 METHODS\nOur methodology addresses the limitations in previous large lan-\nguage models (LLMs) for web navigation using the Miniwob bench-\nmark. We begin by describing the environment and dataset, then\nmove to model design in two stages.\nIn the first stage, we analyze various T5-based models, fine-\ntuning them with hierarchical planning techniques to overcome\nidentified limitations. In the second stage, we integrate the best-\nfine-tuned model with a multimodal neural network, using both\nsupervised learning (SL) and reinforcement learning (RL) to en-\nhance performance and adaptability.\nWe also conduct an ablation study to understand the models‚Äô\ninner workings and identify areas for improvement. Our approach\nemphasizes assessing performance at a small scale before integrat-\ning additional techniques.\n3.1 Miniwob++ Benchmark and Datasets\nThe Miniwob++ benchmark[20] offers over a hundred web-based\nenvironments, simulating web exploration scenarios 2. We utilize\nthirteen thousand human-made demonstrations provided by the\nFarama Foundation, enabling supervised training. The benchmark‚Äôs\nNavigating WebAI SAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain\nalignment with existing research and compatibility with reinforce-\nment learning (RL) through the gymnasium (previously GYM) en-\nvironment [7] and techniques makes it suitable for our study.\nFigure 2: Example of Miniwob Episodes. Each opened episode\nis timed and alongside it, a discounted reward is computed.\nThese episodes cover a wide range of tasks, and in our case,\nwe select a subset of 40 episodes that are suited to work with\nlanguage models in the fashion of Gur et al. [12].\nOur datasets include HTML and Document Object Models (DOM)\nelements parsed as dictionaries, with unique reference numbers\n(‚Äôref‚Äô) for identification. We also process mouse interaction data into\ntwo actions: ‚Äôclick‚Äô or ‚Äôtype_text‚Äô. Unlike previous works[15], we\nconcatenate adjacent typing actions into single actions, see Figures\n3 and 4. This approach aligns with Gur et al.[12], using only two\nseparate actions for efficiency and ease of implementation.\nFigure 3: Structure of Action History as Proposed by Gur et\nal. [12].\nFigure 4: Structure of T5 Input in its traditional form as\nProposed by Gur et al. [12].\nOur methodology processes and fine-tunes data from the Mini-\nwob benchmark for web navigation tasks. We create a hierarchical\nT5 model by identifying sub-tasks within episodes and translating\nhigh-level instructions into actionable plans [6]. The model is then\nfine-tuned for planning and action tasks [31], as shown in Figure 5.\n(1) Infer Model Plan: Devise a plan for the following instruction:\n‚ÄôClick the menu button, and then find and click on the item\nlabeled next‚Äô\nSubtasks = [‚ÄôClick menu button‚Äô, ‚Äôfind an item labeled next‚Äô,\n‚Äôclick on the next icon‚Äô]\n(2) Loop through the subtasks utterances and infer model: {Ac-\ntion History, subtask utterance, DOM} ‚Üí{Action output,\nreference, keydown string, boolean final state}\n(3) Perform action of the Miniwob environment: Perform action,\ncheck if the episode is terminated, and reward\nFigure 5: T5 Hierarchical Inference Process. We first infer the\nmodel to devise a navigation plan from the initial utterance,\nthen iterate through the subtask instructions individually to\ninfer the current action at each time step while evaluating\nthe state of the episode by means of the computed reward\nand terminal state.\n3.1.1 Episode Processing for Action History Extraction.We process\nepisodes to extract action history, following key steps as seen in\nFigure 5. The main steps consist of removing duplicate actions,\nand retaining only the last occurrence. Unnecessary actions, such\nas clicks on the < ùëèùëúùëëùë¶ > element, are discarded. Only the last\nkeydown action for each targeted element is retained, with specific\ncases considered for various interactions. Manual adjustments are\nmade if needed. To address task distribution imbalance, we down-\nsample 150 episodes in over-represented task suites, as seen in\nFigure 15.\n3.1.2 Task Planning Dataset. We identify and transform various\ntypes of sub-actions, such as clicking or selection-based actions. A\ngeneral function targets each Miniwob episode to produce a list\nof sub-tasks based on observed interactions. Specific challenges in\ntasks like flight booking or email forwarding are addressed, with\nsome episodes dropped to ensure clarity. The complexity of some\ntasks suggests that larger pre-trained language models may perform\nbetter.\nHere are two examples of the translation between Miniwob\nutterances and an action sequence for the hierarchical planning\ntask:\n‚Ä¢Example 1:\n‚Äì Utterance: \"Departure City\":\"Philadelphia\", \"Destination\nCity\":\"Charlotte\", \"Ticket Type\":\"Return flight\", \"Departure\nDay\":4, \"Returning Day\":26, \"Passengers\":2\n‚Äì Action sequence: Select Departure City Philadelphia; Se-\nlect Destination City Charlotte; Select the Departure Day\nto 4;\n‚Ä¢Example 2:\n‚Äì Utterance: Expand the section below and click submit.\n‚Äì Action sequence: Expand the section below; click submit;\nThe final dataset is composed of over eight thousand action\nepisodes, and in Figure 15 we describe the number of examples\nper task. The second dataset regarding task planning instruction\ncontains 10,960 episodes.\nSAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain L. Thil et al.\n3.2 Models\nIn this section, we delve into an array of models designed to tackle\nweb navigation tasks, featuring the WebN-T5 model as our corner-\nstone, which is based on the T5 architecture and has demonstrated\nsuperior performance due to its bi-directional attention encoder\n[12, 25]. Alongside WebN-T5, we explore variations such as T5-\nHierarchy (T5H) fine-tuned for hierarchical tasks, and our hybrid\nmodel, CC-NeT5, which combines elements from both T5 and CC-\nNet by Humphrey et al. [15]. These models have been trained using\ndifferent strategies like supervised learning (SL) and reinforcement\nlearning (RL), offering a rich comparative landscape for evaluating\ntheir effectiveness and limitations in web navigation.\n3.2.1 WebN-T5. We attempt to reproduce the models from ‚ÄôUn-\nderstanding Large Language Models‚Äô [12] but face challenges with\nreference numbers and element distribution. As seen in Figure 6,\nthe non-uniform distribution of elements may lead to bias or memo-\nrization [9] based on their location, or even worse entirely over the\ndistribution of the salient elements of the page [3]. We conduct an\nexperiment with ordered and randomized references to investigate\nthis issue. To mitigate this, we randomize the reference numbers\nin all episodes, forcing the models to base predictions on element\nfeatures.\nFigure 6: Distribution of Ordered Reference Numbers in the\nRecorded Actions. We can observe that the distribution of\nthe target elements is concentrated among several locations,\nwhich can be linked to the salient elements in the DOM and\ndisplayed on the page. One of our claims is that previous\nworks focused on learning these distributions by overfitting.\nThe T5 model was fine-tuned using ROUGE loss metrics [ 19],\nwhich measure the overlap between predicted and reference se-\nquences. However, there are limitations:\n‚Ä¢ROUGE metrics are primarily designed for text summariza-\ntion or translation, not action sequence prediction.\n‚Ä¢They do not account for the temporal order and structural\ndependencies in action sequences [26].\nThus, while ROUGE offers insights into the quality of generated\naction sequences, it may not fully capture their correctness and\neffectiveness.\n3.2.2 WebN-T5 Hierarchical Planning. We aim to train the agent\non lower-level tasks during an episode, addressing observed non-\noptimal actions in the original WebN-T5 [12]. The model is fine-\ntuned to divide tasks hierarchically, proposing a multi-step navi-\ngation plan by being trained on both datasets containing the task\nplanning and action episodes using a supervised learning approach.\n3.2.3 Multimodal Language Model with Reinforcement Learning.\nThe best multimodal model used over the Miniwob++ benchmark\nis CC-Net by Humphrey et al. [ 15] which achieved human-level\nperformance by using reinforcement learning. Their model used a\nPPO-based algorithm [28], V-MPO [30], for reinforcement learning\n(RL) which aims to improve performance without requiring exten-\nsive exploration. We used a similar learning approach, except that\nour model architecture combines a CC-Net-inspired model with\na fine-tuned T5 model for the hierarchical planning task detailed\nearlier. The motivation behind using reinforcement learning, is that\nmany of the recorded examples are not sufficient in covering the\nvariety of the cases proposed. Some environments require further\ntraining, and as the exploration space is very large, a method such\nas V-MPO is better suited to that perspective.\nThe architecture is derived from CC-Net, using a multimodal\napproach that includes predictions from the fine-tuned T5 model, a\nscreenshot of the current environment, and language information.\nThe architecture details can be seen in Figure 7 and are as follows:\n‚Ä¢Screenshot inputs are processed through four RESNET blocks.\n‚Ä¢Language inputs are embedded and passed through a trans-\nformer layer.\n‚Ä¢Outputs are fed into a multimodal block, concatenated with\nprevious actions, and processed through an LSTM block.\n‚Ä¢The final layer includes binary variables for action types and\ntensors for reference numbers and vocabulary indexes.\nWe adapt the architecture to deal with the vanishing/exploding\ngradient problem and use one-hot encoding for the experiment.\nThe design of the CC-NeT5 architecture‚Äôs loss function was an\niterative process, influenced by the nature of its output layer. The\nfinal output layer consists of:\n‚Ä¢Action: A binary value represented as a tensor of size 1.\n‚Ä¢Reference number: A tensor of length 500.\n‚Ä¢Keydown text: A tensor of size 8x1591 (8 times our vocabu-\nlary size).\nInitially, a mean-squared-entropy (MSE) loss function was used,\nbut due to sparse encoding and convergence issues, it was updated\nto a cross-entropy (CE) loss. This change involved predicting refer-\nence numbers and keydown text through a softmax function over\neach section of the output layer. The final loss function efficiently\nprocesses this output, with the action type tensor matched with\nits corresponding boolean value, and the reference number and\nkeydown text token indexes retrieved from the tokenizer.\nWhen using V-MPO, we sample actions from a normalized cat-\negorical distribution based on the activation weights of the final\nlayer, where each index represents the token position in our vocab-\nulary. This method allows us to derive a probability distribution\nfor sampling actions during policy inference for exploration. The\narchitecture involves a two-stage process, training the CC-Net-\nbased architecture with V-MPO, while the T5 model remains static.\nNavigating WebAI SAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain\nFigure 7: The Combined Architecture of T5-large fine-tuned over the Hierarchical Task, and CC-Net Multimodal Abilities over\nan RL Approach.\nAlthough the CC-Net part serves as an RL boost to the original\nT5 model, this approach may have limitations if the original T5\ninference is severely flawed.\nThe model‚Äôs accuracy is measured in an online environment over\nthe Miniwob benchmark. During training, we alternate between\noffline (SL) and online (RL) phases [ 1], using recorded Miniwob\nepisodes for the first offline phase and successful episodes from\nonline RL phases for subsequent offline ones [27].\nWe propose a new preset in Gymnasium‚Äôs RL environment, re-\nducing the action space to two types and adjusting the observation\nspace. The time limit for episodes is increased to thirty seconds,\nand a discounted reward is computed to train the models in an RL\nmanner.\n4 RESULTS\nThis section analyzes the outcomes of various trained models, in-\ncluding ablation studies, and compares them with existing models.\nThe models are benchmarked on the Miniwob benchmark, focusing\non click and typing actions, and the accuracy is measured over a\nhundred episodes.\n4.1 Model Performances\nOur experiments reveal that the T5-large model, fine-tuned on hi-\nerarchical tasks, achieves superior performance with an average\naccuracy of 43.58%. This outperforms the T5-base model, which\nreaches an average accuracy of 39.78%. Interestingly, a hybrid ap-\nproach combining T5-large with a CC-Net-inspired architecture\nyields an accuracy of 36.39% in its BC phase. However, this drops to\n33.86% after the RL phase, a phenomenon we discuss in subsection\n4.4.2 due to a covariate shift towards the T5 model. A comparison of\nthe different models‚Äô performance metrics are presented in Figure\n13, including the ablation study over their inputs.\n4.1.1 T5-Model Performance. Fine-tuned T5 models show variable\nperformance. T5-large scores 43.49% accuracy on hierarchical tasks,\nwhile its ablated version slightly edges it with 43.58%. In contrast,\nT5-base lags with 39.77% accuracy as seen in Figures 13a and 13b.\nThe performance advantage in tasks like ‚Äôclick-checkboxes-soft‚Äô\nindicates that larger models capitalize better on their pre-trained\nlinguistic skills.\nOur replication of WebN-T5 by Gur et al. [12] found that model\nperformance hinges on reference ordering. Training with random-\nized references improved its performance, validating our random-\nization process, as depicted in Figure 11.\n4.1.2 Evaluation of Original Papers. Our findings upon reproduc-\ning Gur et al. ‚Äôs work [12] reveal memorization tendencies rather\nthan genuine task understanding. Randomizing references resulted\nin performance drops, questioning the original claims but reaffirm-\ning the importance of data randomization in model training.\n4.2 History versus No History\nAn ablation study on action history reveals interesting insights.\nThe T5-base model‚Äôs performance drops by nearly two percent-\nage points when action history is removed. In contrast, models\nfine-tuned on hierarchical tasks, such as T5-large, show almost no\nperformance change, as detailed in Table 1 and Figures 13a and 13b.\nThis suggests that the hierarchical nature of the task allows the\nmodel to plan its actions more effectively, making it less reliant on\naction history. Furthermore, the randomization of references seems\nto make the model less dependent on previous sequences, providing\nanother layer of resilience to the removal of action history.\n4.3 Hierarchical Planning Improvements\nFine-tuning a model over the hierarchical task achieved higher\nresults than the compared WebN-based models. The following T5\nsizes have been fine-tuned which are T5-base and T5-large, achiev-\ning respectively 42.1% and 43.49% accuracy shown in Table 1. The\noriginal WebN achieved 46.4% accuracy over WebT5-large, and the\nreproduced T5-base size achieved 38.1% which shows that hierar-\nchical planning is an important component of these models.\nThe ablation study over the action history outlined a 14% de-\ncrease in performance over the T5-base model for hierarchical\nplanning and 0.2% over the T5-large. This is much less than the\nreported 6.4% reported by the original WebN-T5-large and WebN-\nT5-3B models showing that hierarchical planning is less sensitive\nto the action history as it tries to solve the episode by following an\noriginal plan step by step.\nNonetheless, we do observe significant rates of failures in com-\nplicated episodes, or when the environment changes as the agent\nfollows the original plan as seen in detail in Figures 13a and 13b\nSAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain L. Thil et al.\nFigure 8: Episode ex-\nample of choosing one\nout of four colors dis-\nplayed from the origi-\nnal utterance.\nFigure 9: Episode\nexample of checking\nboxes of words related\nto the one given in the\noriginal utterance.\nFigure 10: Increment\nthe spinner to a num-\nber of the desired\nvalue and click submit\nepisode example.\nFigure 11: Comparison and effects of fine-tuning with ordered references on a randomized reference test set, and when directly\nfine-tuned with randomized references.\nFigure 12: Ablation study of CC-Net5 after its initial SL phase\nand final RL one.\n4.4 Performance of Combining T5 and CC-Net\nThe combination of T5 and CC-Net was explored in two phases:\nsupervised learning (SL) and reinforcement learning (RL), with\nablation studies conducted in both.\n4.4.1 Results of Supervised Learning Phase.The initial SL phase\nachieved 36.69% accuracy, lower than the best T5-large model ob-\nserved in Table 1. Ablation studies revealed that the model was\nheavily dependent on the T5-large model, with complete failure\nwhen T5‚Äôs output was removed. Interestingly, the removal of visual\ninputs sometimes improved performance as seen in Figure 14a,\nindicating a complex interaction between modalities. Overall, the\nresults showed that the model learned a multimodal representation\nbut was mainly reliant on the T5-large model.\n4.4.2 Results of Reinforcement Learning Phase.The RL phase re-\nsulted in a further drop in accuracy to 33.86% observed in Table\n1. Ablation studies confirmed the model‚Äôs continued dependence\non the T5-large model as seen in Figures 12 for average accuracy\nand 14b for specific tasks. Several factors may have contributed to\nthis decline, including the model‚Äôs inherent complexity, sensitivity\nto hyperparameter tuning presented in Table 2, the nature of the\nRL environment, and possible covariate shift between SL and RL\nphases. These challenges highlight the intricacies of RL training\nand the need for careful analysis and tuning.\n4.5 Benchmarking Results\nThe model was benchmarked on a subset of Miniwob tasks, focus-\ning on 40 out of 80 available tasks. The results showed competitive\nperformance with previous supervised learning methods while us-\ning less training data. A key finding was the tendency of previous\nmodels to memorize rather than understand the distribution of tar-\nget elements. This work‚Äôs benchmarking results, though sometimes\nlower, offer a more robust and reliable assessment of performance.\nNavigating WebAI SAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain\n(a) Comparative results of T5-base fine-tuned over the navigation task over the Miniwob++ benchmark\nwith the ablation of its action history.\n(b) Comparative results of T5-large fine-tuned over the navigation task over the Miniwob++ benchmark\nwith the ablation of its action history.\nFigure 13: Comparative results of the different T5-only models over the navigation task with an ablation study of their inputs\nover the Miniwob++ benchmark.\n5 DISCUSSION AND LIMITATIONS\nModels trained solely on Miniwob are proficient but lack trans-\nferability to diverse web tasks. Pretrained models like T5 have\nlimitations in input context and are prone to overfitting. Our ap-\nproach in Miniwob++ establishes a more realistic baseline, albeit\nsometimes lower than prior works [ 10, 33], but we showed it is\nmore grounded to its environment.\nPerformance evaluation needs to be multifaceted, incorporat-\ning metrics beyond task accuracy, such as generalization abilities.\nModels remain data-intensive and susceptible to memorization; al-\nternative approaches like RLHF should be explored [10, 33]. Despite\ntheir limitations, large pre-trained models still perform best, but\nintermediate-sized models remain under-explored. Small models ex-\ncel in task-constrained settings but struggle with complexity, while\nlarge models offer better generalization but are often overquali-\nfied for tasks they solve [ 14, 18]. The integration of multimodal\napproaches, like ours with T5, reveals tokenization discrepancies\nthat need optimization [14, 18], and training improvements may\ninclude randomizing DOM elements and using ablated inputs to\nfocus on content rather than pattern memorization.\nWeb navigation automation involves critical ethical and legal\naspects that demand attention. Ensuring user privacy is paramount,\nrequiring secure data handling and compliance with regulations\nlike GDPR [11]. The growing capability of large language models\nto mimic humans raises ethical concerns about impersonation [5],\noutpacing traditional identifiers like the Turing test [ 4]. Finally,\nclear accountability must be established to navigate complex liabil-\nity issues, including potential violations of copyright laws and data\nprotection policies. These challenges highlight the need for robust\nregulations and well-defined licensing agreements.\n6 CONCLUSION\nAutomating web navigation tasks offers many benefits but also\npresents significant challenges.\nOur behavioral cloning and hierarchical planning models achieved\na top accuracy of 43.58% on the Miniwob++ benchmark, setting\na more grounded performance baseline by mitigating overfitting\ntendencies commonly seen in large language models (LLMs). While\nthe fine-tuned multimodal model excelled in behavioral cloning,\nits reinforcement learning phase was hindered by covariate shift\ndue to architectural limitations. We highlight the need for further\nexploration in multimodal architecture design and the potential of\nfine-tuned LLMs.\nThis work emphasizes the importance of understanding the lim-\nitations of LLMs over web navigation tasks, exploring optimal\narchitectures, and considering ethical implications. While large\nmodels excel in few-shot scenarios, smaller models are efficient for\nknown environments. The exploration of intermediate sizes and\nmultimodal models offers promising opportunities. Pre-processing\ntechniques can alleviate some issues, but further efforts are needed\nfor more capable models. Ethical considerations, including misuse,\nimpersonation, and accountability, must be addressed to ensure the\nSAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain L. Thil et al.\n(a) Comparative results of CC-NeT5 after the initial BC phase over the Miniwob++ benchmark with the\nablation of its visual and previous action inputs.\n(b) Comparative results of CC-NeT5 after the final RL phase over the Miniwob++ benchmark with the\nablation of its visual and previous action inputs.\nFigure 14: Comparative results of the CC-NeT5 architecture over the navigation task with an ablation study of their inputs over\nthe Miniwob++ benchmark for both supervised and reinforcement learning.\nsafety and integrity of the web. This work contributes by highlight-\ning these challenges and proposing techniques to overcome them,\npaving the way for future advancements in the field.\n7 APPENDIX\nTable 1: Average Accuracy of the Different Models, Fine-\nTuned T5 and Combined with CC-Net.\nModel Name Average Accuracy\nT5-Base Hierarchical 39.77%\nT5-Base Hierarchical No-History 38.02%\nT5-Large Hierarchical 43.49%\nT5-Large Hierarchical No-History 43.58%\nWebNT5-Base 37.94%\nWebNT5-Base No-History 35.99%\nWebNT5-Base Ordered Refs 35.25%\nWebNT5-Base Randomized Refs Test 24.19%\nCC-NeT5 Hierarchical (SL) 36.69%\nCC-NeT5 Hierarchical (SL+RL) 33.86%\nREFERENCES\n[1] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. 2020. An Opti-\nmistic Perspective on Offline Reinforcement Learning. arXiv:1907.04543 [cs.LG]\n[2] Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi\nGhosh, and Luke Zettlemoyer. 2021. HTLM: Hyper-Text Pre-Training and Prompt-\ning of Language Models. arXiv:2107.06955 [cs.CL]\n[3] Devansh Arpit, Stanis≈Çaw Jastrzƒôbski, Nicolas Ballas, David Krueger, Emmanuel\nBengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville,\nYoshua Bengio, and Simon Lacoste-Julien. 2017. A Closer Look at Memorization\nin Deep Networks. arXiv:1706.05394 [stat.ML]\nTable 2: Hyper-parameters used in V-MPO during the RL\n‚Äôonline‚Äô phase of our training, inspired by CC-Net training\nparameters.\nParameter Value\nOptimizer Adam [17]\nLearning rate 1e-4\nAdam b1 parameter 0.9\nAdam b2 parameter 0.999\nWeight decay (biases excluded) 1e-1\nVMPO ùõº 0.1\nVMPO ùúÇ 0.2\nAgent discount ùõæ 0.9\nBatch size SL 120\nTrajectory unroll length 64\nTarget-network update period ùëá 5\nMaximum number of steps per episode 10\n[4] Aladdin Ayesh. 2019. Turing Test Revisited: A Framework for an Alternative.\narXiv:1906.11068 [cs.AI]\n[5] Jean Baudrillard. 1981. Simulacres et Simulations . Galil√©e.\n[6] S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Re-\ninforcement Learning for Mapping Instructions to Actions. In Proceedings of\nthe Joint Conference of the 47th Annual Meeting of the ACL and the 4th Inter-\nnational Joint Conference on Natural Language Processing of the AFNLP . As-\nsociation for Computational Linguistics, Suntec, Singapore, 82‚Äì90. https:\n//aclanthology.org/P09-1010\n[7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John\nSchulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym.\narXiv:1606.01540 [cs.LG]\nNavigating WebAI SAC‚Äô24, April 8 ‚ÄìApril 12, 2024, Avila, Spain\nFigure 15: Distribution of task names from the original Miniwob dataset. We can observe that some tasks are over-represented,\ntherefore we sample a maximum of 150 episodes for the examples exceeding that number. This leads to an average amount of\n56.29 of episodes per task in the dataset.\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\narXiv:2005.14165 [cs.CL]\n[9] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian\nTramer, and Chiyuan Zhang. 2023. Quantifying Memorization Across Neural\nLanguage Models. arXiv:2202.07646 [cs.LG]\n[10] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\nAmodei. 2017. Deep Reinforcement Learning from Human Preferences. In Ad-\nvances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Ben-\ngio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Cur-\nran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/\nd5e2c0adad503c91f91df240d0cd4e49-Paper.pdf\n[11] European Parliament and Council of the European Union. [n. d.]. Regulation (EU)\n2016/679 of the European Parliament and of the Council . https://data.europa.eu/\neli/reg/2016/679/oj\n[12] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang,\nAakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2023.\nUnderstanding HTML with Large Language Models. arXiv:2210.03945 [cs.LG]\n[13] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wich-\ners, Gabriel Schubiner, Ruby Lee, Jindong Chen, and Blaise Ag√ºera y Arcas.\n2021. ActionBert: Leveraging User Actions for Semantic Understanding of User\nInterfaces. arXiv:2012.12350 [cs.CL]\n[14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-\nOptimal Large Language Models. arXiv:2203.15556 [cs.CL]\n[15] Peter C Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita\nChhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam\nSantoro, and Timothy Lillicrap. 2022. A data-driven approach for learning to\ncontrol computers. arXiv:2202.08137 [cs.LG]\n[16] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language Models can\nSolve Computer Tasks. arXiv:2303.17491 [cs.CL]\n[17] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Opti-\nmization. arXiv:1412.6980 [cs.LG]\n[18] Ian LeCun. 2023. Do large language models need sensory ground-\ning for meaning and understanding? https://drive.google.com/file/d/\n1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view\n[19] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out . Association for Computational Linguistics,\nBarcelona, Spain, 74‚Äì81. https://www.aclweb.org/anthology/W04-1013\n[20] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang.\n2018. Reinforcement Learning on Web Interfaces using Workflow-Guided\nExploration. In International Conference on Learning Representations (ICLR) .\nhttps://arxiv.org/abs/1802.08802\n[21] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu\nJiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew\nKnight, Benjamin Chess, and John Schulman. 2022. WebGPT: Browser-assisted\nquestion-answering with human feedback. arXiv:2112.09332 [cs.CL]\n[22] OpenAI. 2016. Universe. https://github.com/openai/universe\n[23] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[24] Panupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin Guu, and Percy Liang.\n2018. Mapping natural language commands to web elements. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 4970‚Äì4976. https:\n//doi.org/10.18653/v1/D18-1540\n[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text Transformer.\narXiv:1910.10683 [cs.LG]\n[26] Natalie Schluter. 2017. The limits of automatic summarisation according to\nROUGE. In Proceedings of the 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume 2, Short Papers . Association for\nComputational Linguistics, Valencia, Spain, 41‚Äì45. https://aclanthology.org/E17-\n2007\n[27] Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin\nBarekatain, Ioannis Antonoglou, and David Silver. 2021. Online and Offline Rein-\nforcement Learning by Planning with a Learned Model. arXiv:2104.06294 [cs.LG]\n[28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]\n[29] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang.\n2017. World of Bits: An Open-Domain Platform for Web-Based Agents. In\nProceedings of the 34th International Conference on Machine Learning (Proceedings\nof Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.).\nPMLR, 3135‚Äì3144. https://proceedings.mlr.press/v70/shi17a.html\n[30] H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark,\nHubert Soyer, Jack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala,\nNicolas Heess, Dan Belov, Martin Riedmiller, and Matthew M. Botvinick. 2019.\nV-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and\nContinuous Control. arXiv:1909.12238 [cs.AI]\n[31] Adam Vogel and Daniel Jurafsky. 2010. Learning to Follow Navigational Direc-\ntions. In Proceedings of the 48th Annual Meeting of the Association for Computa-\ntional Linguistics . Association for Computational Linguistics, Uppsala, Sweden,\n806‚Äì814. https://aclanthology.org/P10-1083\n[32] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2023. WebShop:\nTowards Scalable Real-World Web Interaction with Grounded Language Agents.\narXiv:2207.01206 [cs.CL]\n[33] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford,\nDario Amodei, Paul Christiano, and Geoffrey Irving. 2020. Fine-Tuning Language\nModels from Human Preferences. arXiv:1909.08593 [cs.CL]"
}