{
    "title": "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with\\n Transformers",
    "url": "https://openalex.org/W3171668871",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4222365801",
            "name": "Miech, Antoine",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225334462",
            "name": "Alayrac, Jean-Baptiste",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742358871",
            "name": "Laptev, Ivan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3191045335",
            "name": "Sivic, Josef",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2930261228",
            "name": "Zisserman, Andrew",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2593116425",
        "https://openalex.org/W2307770531",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2887051120",
        "https://openalex.org/W2613031639",
        "https://openalex.org/W2984008963",
        "https://openalex.org/W3034588855",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W3171668871",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W1627400044",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W3035635319",
        "https://openalex.org/W2124509324",
        "https://openalex.org/W1948751323",
        "https://openalex.org/W1556531089",
        "https://openalex.org/W3035356601",
        "https://openalex.org/W2131846894",
        "https://openalex.org/W2606473278",
        "https://openalex.org/W2989322838",
        "https://openalex.org/W3035118106",
        "https://openalex.org/W3035265375",
        "https://openalex.org/W2972073579",
        "https://openalex.org/W2951183276",
        "https://openalex.org/W3042994108",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W3100177202",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2294370754",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2990152177",
        "https://openalex.org/W2963881378",
        "https://openalex.org/W1889081078",
        "https://openalex.org/W2963389687",
        "https://openalex.org/W3009812836",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W3122640483",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2592335154",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2966848258",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2141362318",
        "https://openalex.org/W2750784772",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W3182683290",
        "https://openalex.org/W1947481528",
        "https://openalex.org/W877909479",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3105232955",
        "https://openalex.org/W3034971973",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2162006472",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W753847829",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3092470998",
        "https://openalex.org/W1957706851",
        "https://openalex.org/W2963785012",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2952787292",
        "https://openalex.org/W92662927",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2963350250",
        "https://openalex.org/W3037309139",
        "https://openalex.org/W2752099845",
        "https://openalex.org/W1773149199",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2975813532",
        "https://openalex.org/W2152790380",
        "https://openalex.org/W2070753207",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W2796207103",
        "https://openalex.org/W3034727271",
        "https://openalex.org/W1573040851",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W3139017368",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W2100398441",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3117585461",
        "https://openalex.org/W1905882502"
    ],
    "abstract": "Our objective is language-based search of large-scale image and video\\ndatasets. For this task, the approach that consists of independently mapping\\ntext and vision to a joint embedding space, a.k.a. dual encoders, is attractive\\nas retrieval scales and is efficient for billions of images using approximate\\nnearest neighbour search. An alternative approach of using vision-text\\ntransformers with cross-attention gives considerable improvements in accuracy\\nover the joint embeddings, but is often inapplicable in practice for\\nlarge-scale retrieval given the cost of the cross-attention mechanisms required\\nfor each sample at test time. This work combines the best of both worlds. We\\nmake the following three contributions. First, we equip transformer-based\\nmodels with a new fine-grained cross-attention architecture, providing\\nsignificant improvements in retrieval accuracy whilst preserving scalability.\\nSecond, we introduce a generic approach for combining a Fast dual encoder model\\nwith our Slow but accurate transformer-based model via distillation and\\nre-ranking. Finally, we validate our approach on the Flickr30K image dataset\\nwhere we show an increase in inference speed by several orders of magnitude\\nwhile having results competitive to the state of the art. We also extend our\\nmethod to the video domain, improving the state of the art on the VATEX\\ndataset.\\n",
    "full_text": "Thinking Fast and Slow: Efﬁcient Text-to-Visual Retrieval with Transformers\nAntoine Miech1* Jean-Baptiste Alayrac1*\nIvan Laptev2 Josef Sivic3 Andrew Zisserman1,4\n1DeepMind 2ENS/Inria 3CIIRC CTU 4VGG Oxford\n{miech,jalayrac}@google.com\nAbstract\nOur objective is language-based search of large-scale\nimage and video datasets. For this task, the approach\nthat consists of independently mapping text and vision to\na joint embedding space, a.k.a. dual encoders, is attrac-\ntive as retrieval scales and is efﬁcient for billions of im-\nages using approximate nearest neighbour search. An al-\nternative approach of using vision-text transformers with\ncross-attention gives considerable improvements in accu-\nracy over the joint embeddings, but is often inapplicable in\npractice for large-scale retrieval given the cost of the cross-\nattention mechanisms required for each sample at test time.\nThis work combines the best of both worlds. We make the\nfollowing three contributions. First, we equip transformer-\nbased models with a new ﬁne-grained cross-attention ar-\nchitecture, providing signiﬁcant improvements in retrieval\naccuracy whilst preserving scalability. Second, we intro-\nduce a generic approach for combining aFast dual encoder\nmodel with our Slow but accurate transformer-based model\nvia distillation and re-ranking. Finally, we validate our ap-\nproach on the Flickr30K image dataset where we show an\nincrease in inference speed by several orders of magnitude\nwhile having results competitive to the state of the art. We\nalso extend our method to the video domain, improving the\nstate of the art on the VATEX dataset.\n1. Introduction\nImagine yourself looking for an image that best matches\na given textual description among thousands of other im-\nages. One effective way would be to ﬁrst isolate a few\npromising candidates by giving a quick glance at all the\nimages with a fast process, e.g. by eliminating images that\n*Equal contribution.\n1D´epartement d’informatique de l’ENS, ´Ecole normale sup ´erieure,\nCNRS, PSL Research University, 75005 Paris, France.\n3Czech Institute of Informatics, Robotics and Cybernetics at the Czech\nTechnical University in Prague.\n4VGG, Dept. of Engineering Science, University of Oxford\nA man rides a bike with a cat wearing sunglasses.\nVision networkText network\nSimilarity\nCross-Attention network\nSimilarity\nFastDual encoderSlowCross-Attention\nDistillation(at training)\nReranking(at query time)\nA man rides a bike with a cat wearing sunglasses.\nFigure 1: On the left, the Fast models, a.k.a dual encoders, inde-\npendently process the input image and text to compute a similarity\nscore via a single dot product, which can be efﬁciently indexed\nand is thus amenable to large-scale search. On the right, the Slow\nmodels, a.k.a cross-attention models, jointly process the input im-\nage and text with cross-modal attention to compute a similarity\nscore. Fast and indexable models are improved bySlow models via\ndistillation at training time (ofﬂine). Slow models are accelerated\nand improved with the distilledFast approaches using a re-ranking\nstrategy at query time.\nhave clearly nothing in common with the description. In the\nsecond phase, you may start paying more attention to im-\nage details with a slow process, e.g. by grounding individ-\nual words of a query sentence to make sure the scrutinized\nimage is the best match.\nAnalogous to the fast process above, fast retrieval sys-\ntems can be implemented by separately encoding visual and\ntextual inputs into a joint embedding vector space where\nsimilarities can be computed by dot product. Such methods\nare regarded as indexable, i.e. they allow application of fast\napproximate nearest neighbour search [11, 32, 52, 64] and\nenable efﬁcient billion-scale image retrieval. However, the\naccuracy of such methods is limited due to the simplicity of\nvision-text interaction model deﬁned by the dot product in\nthe joint embedding space. We refer to these techniques as\nDual Encoders (DE) or Fast approaches.\nVision-text transformers compare each word to all loca-\n1\narXiv:2103.16553v1  [cs.CV]  30 Mar 2021\ntions in the image using cross-attention [12, 29, 46], allow-\ning for grounding, and can be related to the slow process\nmentioned earlier. Such methods, referred to here as Cross-\nattention (CA) or Slow approaches , signiﬁcantly boost re-\ntrieval performance. Modeling text-vision interactions with\nattention, however, makes these models slow and imprac-\ntical for large-scale image retrieval given the cost of the\ncross-attention mechanisms required for each sample at test\ntime. Hence, the challenge we consider is the following:\nHow to beneﬁt from accurate cross-attention mechanisms\nwhile preserving the fast and scalable visual search?\nOur short answer is: By thinking Fast and Slow [10].\nAs illustrated in Figure 1, we propose to combine dual en-\ncoder approaches with cross-attention via two complemen-\ntary mechanisms. First, we improve Fast DE models with\na novel distillation objective that transfers knowledge from\naccurate but Slow CA models to theFast and indexable dual\nencoders. Second, we propose to combine DE and CA mod-\nels with re-ranking where a few most promising candidates\nobtained with the Fast model are re-ranked using the Slow\nmodel. Our resulting approach is both fast and accurate.\nSince the speed of CA is not a bottleneck anymore, we\nfurther improve performance by enriching the vision-text\ncross-attention model with a novel feature map upsampling\nmechanism enabling ﬁne-grained attention. Note that our\nwork can also be applied to vision-to-text retrieval. How-\never, we focus on text-to-vision retrieval due to its wider\npractical application.\nContributions. (i) We ﬁrst propose a gradual feature up-\nsampling architecture for improved and ﬁne-grained vision\nand text cross-attention. Our model is trained with a bi-\ndirectional captioning loss which is remarkably competi-\ntive for retrieval compared to standard cross-modal match-\ning objectives. (ii) We introduce a generic approach for\nscaling-up transformer-based vision-text retrieval using two\ncore ideas: a method to distill the knowledge of Slow cross-\nattention models into Fast dual-encoders, and re-ranking\ntop results of the Fast models with the Slow ones. (iii) Fi-\nnally, we validate our approach on image retrieval with the\nCOCO [43] and Flickr30K [59] datasets and show we can\nreduce the inference time of powerful transformer-based\nmodels by 100 ×whilst also getting competitive results to\nthe state of the art. We also successfully extend our ap-\nproach to text-to-video retrieval and improve state of the art\non the challenging V ATEX [72] dataset.\n2. Related work\nVision and Language models. Driven by the signiﬁcant\nadvances in language understanding lead by Transform-\ners [13, 69], recent works have explored the use of these\narchitectures for vision and language tasks. Many of them\nin image [8, 37, 39, 40, 46, 66, 67, 77] or video [78]\nrely on pretrained object detectors used for extracting ROIs\nthat are viewed as individual visual words. A few other\nworks, such as PixelBERT [29] and VirTex [12] for im-\nages or HERO [38] for video, operate directly over dense\nfeature maps instead of relying on object detectors. In\nthese approaches, both vision and text inputs are fed into a\nTransformer-based model usually pretrained with multiple\nlosses such as a cross-modal matching loss, a masked lan-\nguage modelling or a masked region modelling loss. Other\nnon-Transformer based vision and text approaches used re-\ncurrent neural networks [14, 15, 36], MLP [70, 71], or bag-\nof-words [19, 50] text models. These models are then usu-\nally optimized with objectives such as CCA [19], max mar-\ngin triplet loss [15, 70, 71, 73, 74], contrastive loss [23]\nand, more related to our work, by maximizing text log-\nlikelihoods conditioned on the image [14]. In our work,\nwe focus on the powerful vision-text Transformer models\nfor retrieval and particularly address their scalability, which\nwas frequently neglected by prior work.\nLanguage-based visual search. A large number of vi-\nsion and language retrieval models [15, 19, 20, 36, 49, 50,\n54, 58, 70, 71, 73, 74, 76] use a dual encoder architecture\nwhere the text and vision inputs are separately embedded\ninto a joint space. These approaches can efﬁciently ben-\neﬁt from numerous approximate nearest neighbour search\nmethods such as: product quantization [32], inverted in-\ndexes [64], hierarchical clustering [52] or locality sensitive\nhashing [11], for fast and scalable visual search. In contrast,\nstate-of-the-art retrieval models rely on large vision-text\nmultimodal transformers [8, 29, 37, 39, 46, 47, 66, 67, 77].\nIn these approaches, both vision and text inputs are fed into\na cross-modal attention branch to compute the similarity be-\ntween the two inputs. This scoring mechanism based on\ncross-modal attention makes it particularly inadequate for\nindexing and thus challenging to deploy at a large scale.\nOur work aims at addressing this issue by connecting scal-\nable visual search techniques with these powerful yet non-\nindexable vision-text cross-attention based models.\nRe-ranking. Re-ranking retrieval results is standard in re-\ntrieval systems. In computer vision, the idea of geometric\nveriﬁcation [31, 56] is used in object retrieval to re-rank ob-\njects that better match the query given spatial consistency\ncriteria. Query expansion [9] is another re-ranking tech-\nnique where the query is reformulated given top retrieved\ncandidates, and recent work has brought attention mecha-\nnisms into deep learning methods for query expansion [21].\nRelated to language-based visual search, re-ranking by a\nvideo-language temporal alignment model has been used to\nimprove efﬁcient moment retrieval in video [16]. In con-\ntrast, we focus on transformer-based cross-attention models\nand develop a distillation objective for efﬁcient retrieval.\nDistillation. Knowledge distillation [3, 28] has proven to\nbe effective for improving performance in various com-\nputer vision domains such as weakly-supervised learn-\n2\ning [41, 60], depth estimation [22], action recognition [65],\nsemantic segmentation [44], self-supervised learning [57]\nor self-training [75]. One major application of distillation is\nin compressing large and computationally expensive mod-\nels in language analysis [62], object detection [5], image\nclassiﬁcation or speech recognition [28] into smaller and\ncomputationally less demanding models. In this work, we\ndescribe a distillation mechanism for the compression of\npowerful but non-indexable vision-text models into index-\nable models suitable for efﬁcient retrieval.\n3. Thinking Fast and Slow for Retrieval\nThis section describes our proposed approach to learn\nboth fast and accurate model for language-based image re-\ntrieval.\nOur goal is to train the model to output a similarity score\nbetween an input image x and a textual description y. In\nthis work, we focus on two families of models: theFast and\nthe Slow models, as illustrated in Figure 1.\nThe Fast model, referred to as the dual encoder ap-\nproach, consists of extracting modality-speciﬁc embed-\ndings: f(x) ∈Rd for the image and g(y) ∈Rd for the\ntext. The core property of this approach is that the simi-\nlarity between an image x and a text y can be computed\nvia a single dot product f(x)⊤g(y). Hence, these methods\ncan beneﬁt from approximate nearest neighbour search for\nefﬁcient large-scale retrieval [32, 52, 64].\nThe Slow model, referred to as the cross-attention ap-\nproach differs by a more complex modality merging strat-\negy based on cross-modal attention. We assume the given\nsimilarity score h(x,y) cannot be decomposed as a dot\nproduct and as such is not indexable. These models al-\nlow for richer interactions between the visual and textual\nrepresentations, which leads to better scoring mechanisms,\nthough at a higher computational cost.\nSection 3.1 introduces the ( Slow) cross-attention model\nconsidered in this work and details our contribution on the\nmodel architecture that leads to a more accurate text-to-\nimage retrieval system. Section 3.2 describes how we obtain\nboth a fast and accurate retrieval method by combining the\nadvantages of the two families of models.\n3.1. Thinking Slow with cross-attention\nGiven an image xand a text description y, a Slow cross-\nattention retrieval model hcomputes a similarity score be-\ntween the image and text as:\nh(x,y) =A(φ(x),y), (1)\nwhere φ is a visual encoder ( e.g. a CNN). A is a network\nthat computes a similarity score between φ(x) and yusing\ncross-attention [46, 69] mechanisms, i.e. the text attends to\nthe image or vice versa via multiple non-linear functions in-\nvolving both the visual and language representations. Such\nmodels emulate a slow process of attention which results in\nbetter text-to-image retrieval.\nWe propose two important innovations to improve such\nmodels. First, we introduce a novel architecture that en-\nables ﬁne-grained visual-text cross-attention by efﬁciently\nincreasing the resolution of the attended high-level image\nfeatures. Second, we propose to revisit the use of a cap-\ntioning loss [14] to train retrieval models and discuss the\nbeneﬁts over standard alternatives that use classiﬁcation or\nranking loss [8, 37, 39, 46, 66, 67, 77].\nA novel architecture for ﬁne-grained vision-text\ncross-attention. A typical approach to attend to visual fea-\ntures produced by a CNN is to consider the last convolu-\ntional layer [12, 29]. The feature map is ﬂattened into a set\nof feature vectors that are used as input to vision-language\ncross-attention modules. For example, a 224 ×224 input\nimage passed through a ResNet-50 [26] outputs a7 ×7 fea-\nture map that is ﬂattened into 49 vectors. While the last fea-\nture map produces high-level semantic information crucial\nfor grounding text description into images, this last feature\nmap is also severely downsampled. As a result, useful ﬁne-\ngrained visual information for grounding text descriptions\nmight be lost in this downsampling process.\nOne solution to the problem is to increase the input im-\nage resolution. However, this signiﬁcantly raises the cost of\nrunning the visual backbone. Inspired by previous work in\nsegmentation [2, 25, 61] and human pose estimation [53],\nwe instead propose to gradually upsample the last convo-\nlutional feature map conditioned on earlier higher resolu-\ntion feature maps, as illustrated in Figure 2. We choose\na lightweight architecture for this upsampling process in-\nspired by recent advances in efﬁcient object detection [68].\nIn Section 4, we show large improvements of this approach\nover several baselines and also show its complementarity\nto having higher resolution input images, clearly demon-\nstrating the beneﬁts of the proposed ﬁne-grained vision-\nlanguage cross-attention.\nBi-directional captioning objective for retrieval. A ma-\njority of text-vision transformer-based retrieval models [8,\n37, 39, 46, 66, 67, 77] rely on a cross-modal image-\ntext matching loss to discriminate positive image-text pairs\n(x,y) from negative ones. In this work, we instead explore\nthe use of a captioning model for retrieval. Given an input\ntext query y, retrieval can be done by searching the image\ncollection for the image x that leads to the highest likeli-\nhood of ygiven xaccording to the model. In detail, we take\ninspiration from VirTex [12] and design the cross-attention\nmodule A as a stack of Transformer decoders [69] taking\nthe visual feature mapφ(x) as an encoding state. Each layer\nof the decoder is composed of a masked text self-attention\nlayer, followed by a cross-attention layer that enables the\ntext to attend to the visual features and ﬁnally a feed forward\n3\nResNet\nTransformer Decoder\nA dog rides a bike outside.56 x 56 x 25628 x 28 x 51214 x 14 x 10247 x 7 x 204814 x 14 x 51228 x 28 x 51256 x 56 x 512\n7 x 7 x 512\n2x UP 2x UP2x UP\nGradual upsampling56 x 56 x 512\na[bos] outside\n…\n…ConvConvConv\n224 x 224 x 3\nQuery\nSimilarityconv2conv3conv4conv5\ndog\nl(dog)\nFigure 2: Our Slow retrieval model computes a similarity score h(x,y) between image xand query text description yby estimating the\nlog-likelihood of yconditioned on x. In other words, given an input text query y, we perform retrieval by searching for an image xthat\nis the most likely to decode caption y. l(.) denotes the log probability of a word given preceding words and the image. The decoder is a\nTransformer that takes as the conditioning signal a high-resolution (here 56 ×56) feature map φ(x). In this example, φ(x) is obtained by\ngradually upsampling the last convolutional layer of ResNet (7 ×7) while incorporating features from earlier high-resolution feature maps.\nThe decoder performs bidirectional captioning but, for the sake of simplicity, we only illustrate here the forward decoding transformer.\nlayer. One advantage of this architecture compared to stan-\ndard multimodal transformers [8, 37, 39, 46, 66, 67, 77] is\nthe absence of self-attention layers on visual features, which\nallows the resolution of the visual feature map φ(x) to be\nscaled to thousands of vectors. We write the input text as\ny = [y1,...,y L] where L is the number of words. For-\nmally, the model h scores a pair of image and text (x,y)\nas:\nh(x,y) =hfwd(x,y) +hbwd(x,y), (2)\nwhere hfwd(x,y) (resp. hbwd(x,y)) is the forward (resp.\nbackward) log-likelihood of the caption y given the image\nxaccording to the model:\nhfwd(x,y) =\nL∑\nl=1\nlog(p(yl|yl−1,...,y 1,φ(x); θfwd)),\n(3)\nwhere p(yl|yl−1,...,y 1,φ(x); θ) corresponds to the out-\nput probability of a decoder model parametrized by θ for\nthe token yl at position l given the previously fed tokens\nyl−1,...,y 1 and the encoded image φ(x). θfwd is the pa-\nrameters of the forward transformer models. hbwd(x,y) is\nthe same but with the sequence y1,...,y L in reverse order.\nThe parameters of the visual backbone, the forward and\nbackward transformer models are obtained by minimizing\nLCA = −∑n\ni=1 h(xi,yi) where nis the number of anno-\ntated pairs of images and text descriptions {(xi,yi)}i∈[1,n].\nWe show in Section 4 that models trained for captioning\ncan perform on-par with models trained with the usual con-\ntrastive image-text matching loss. At ﬁrst sight this may ap-\npear surprising as the image-text matching loss seems more\nsuited for retrieval, notably because it explicitly integrates\nnegative examples. However, when looked at more closely,\nthe captioning loss actually shares similarities with a con-\ntrastive loss: for each ground truth token of the sequence a\ncross entropy loss is taken (see Eq. (3)) which effectively\nmeans that all other tokens in the vocabulary are considered\nas negatives.\nIn this section, we have described the architecture\nand the chosen loss for training our accurate Slow cross-\nattention model for retrieval. One key remaining challenge\nis in the scaling of h(x,y) using Eq. (1) to large image\ndatasets as: (i) the network Ais expensive to run and(ii) the\nresulting intermediate encoded image, φ(x), is too large to\nﬁt the entire encoded dataset in memory. Next, we intro-\nduce a generic method, effective beyond the scope of our\nproposed Slow model, for efﬁciently running such cross-\nmodal attention-based models at a large scale.\n3.2. Thinking Faster and better for retrieval\nIn this section, we introduce an approach to scale-up the\nSlow transformer-based cross-attention model, described in\nthe previous section, using two complementary ideas. First,\nwe distill the knowledge of the Slow cross-attention model\ninto a Fast dual-encoder model that can be efﬁciently in-\ndexed. Second, we combine the Fast dual-encoder model\nwith the Slow cross-attention model via a re-ranking mech-\nanism. The outcome is more than 100×speed-up and, in-\nterestingly, an improved retrieval accuracy of the combined\nFast and Slow model. Next, we give details of the Fast\ndual encoder model, then explain the distillation of theSlow\nmodel into theFast model using a teacher-student approach,\nand ﬁnally describe the re-ranking mechanism to combine\nthe outputs of the two models. Because our approach is\nmodel agnostic, the Slow model can refer to any vision-text\ntransformer and the Fast to any dual-encoder model. An\noverview of the approach is illustrated in Figure 1.\n4\nFast indexable dual encoder models. We consider Fast\ndual encoder models, that extract modality speciﬁc embed-\ndings: f(x) ∈Rd from image x, and g(y) ∈Rd from text\ny. The core property of this approach is that the similar-\nity between the embedded image xand text y is measured\nwith a dot product, f(x)⊤g(y). The objective is to learn\nembeddings f(x) and g(y) so that semantically related im-\nages and text have high similarity and the similarity of unre-\nlated images and text is low. To achieve that we train these\nembeddings by minimizing the standard noise contrastive\nestimation (NCE) [24, 33] objective:\nLDE = −\nn∑\ni=1\nlog\n\n ef(xi)⊤g(yi)\nef(xi)⊤g(yi) + ∑\n(x′,y′)∈Ni\nef(x′)⊤g(y′)\n\n,\n(4)\nwhich contrasts the score of the positive pair (xi,yi) to a\nset of negative pairs sampled from a negative setNi. In our\ncase, the image encoder f is a globally pooled output of a\nCNN while the text encoder gis either a bag-of-words [50]\nrepresentation or a more sophisticated BERT [13] encoder.\nImplementation details are provided in Section 4.1.\nDistilling the Slow model into the Fastmodel. Given the\nsuperiority of cross-attention models over dual encoders for\nretrieval, we investigate how to distill [28] the knowledge\nof the cross-attention model to a dual encoder. To achieve\nthat we introduce a novel loss.\nIn detail, the key challenge is that, as opposed to stan-\ndard distillation used for classiﬁcation models, here we do\nnot have a small ﬁnite set of classes but potentially an inﬁ-\nnite set of possible sequences of words describing an image.\nTherefore, we cannot directly apply the standard formula-\ntion of distillation proposed in [28].\nTo address this issue, we introduce the following ex-\ntension of distillation for our image-text setup. Given an\nimage-text pair (xi,yi), we sample a ﬁnite subset of image-\ntext pairs Bi = {(xi,yi)}∪{ (x,yi)|x ̸= xi}, where\nwe construct additional image-text pairs with the same text\nquery yi but different images x. Note that this is similar to\nthe setup that would be used to perform retrieval of images\nx given a text query yi. In practice, we sample different\nimages x within the same training batch. We can write a\nprobability distribution measuring the likelihood of the pair\n(x,y) ∈ Bi according to the Slow teacher model h(x,y)\n(given by eq. (1)) over subset Bi as:\np(Bi)(x,y) = exp(h(x,y)/τ)∑\n(x′,y′)∈Bi exp(h(x′,y′)/τ), (5)\nwhere τ > 0 is a temperature parameter controlling the\nsmoothness of the distribution. We can obtain a similar dis-\ntribution from the Fast student model, by replacing h(x,y)\nfrom Eq. (5) by f(x)⊤g(y):\nq(Bi)(x,y) = exp(f(x)⊤g(y)/τ)∑\n(x′,y′)∈Bi exp(f(x′)⊤g(y′)/τ). (6)\nGiven the above deﬁnition of the sampled distributions,\nwe use the following distillation loss that measures the simi-\nlarity between the teacher distributionp(Bi) and the student\ndistribution q(Bi) as :\nLdistill =\nn∑\ni=1\nH(p(Bi),q(Bi)), (7)\nwhere His the cross entropy between the two distributions.\nThe intuition is that the teacher model provides soft tar-\ngets over the sampled image-text pairs as opposed to binary\ntargets in the case of a single positive pair and the rest of\nthe pairs being negative. Similarly to the standard distilla-\ntion [28], we combine the distillation loss (7) with the DE\nloss (4) weighted with α> 0 to get our ﬁnal objective as:\nmin\nf,g\nLdistill + αLDE. (8)\nRe-ranking the Fast results with the Slow model. The\ndistillation alone is usually not sufﬁcient to recover the full\naccuracy of the Slow model using the Fast model. To ad-\ndress this issue, we use the Slow model at inference time to\nre-rank a few of the top retrieved candidates obtained us-\ning the Fast model. First, the entire dataset is ranked by\nthe (Distilled) Fast model that can be done efﬁciently us-\ning approximate nearest neighbour search, which often has\nonly sub-linear complexity in the dataset size. Then the top\nK (e.g. 10 or 50) results are re-ranked by the Slow model.\nAs the Slow model is applied only to the top K results its\napplication does not depend on the size of the database.\nMore precisely, given an input text queryyand an image\ndatabase Xcontaining a large number ofmimages, we ﬁrst\nobtain a subset of Kimages XK (where K ≪m) that have\nthe highest score according to the Fast dual encoder model.\nWe then retrieve the ﬁnal top ranked image by re-ranking\nthe candidates using the Slow model:\narg max\nx∈XK\nh(x,y) +βf(x)⊤g(y), (9)\nwhere βis a positive hyper-parameter that weights the out-\nput scores of the two models. In the experimental Sec-\ntion 4, we show that combined with distillation, re-ranking\nless than ten examples out of thousands can be sufﬁcient to\nrecover the performance of the Slow model.\n4. Experiments\nIn this section, we evaluate the beneﬁts of our approach\non the task of text-to-vision retrieval. We describe the\n5\ndatasets and baselines used for evaluation in Section 4.1.\nIn Section 4.2 we validate the advantages of cross-attention\nmodels with captioning objectives as well as our use of\ngradually upsampled features for retrieval. Section 4.3 eval-\nuates the beneﬁt of the distillation and re-ranking. In Sec-\ntion 4.4, we compare our approach to other published state-\nof-the-art retrieval methods in the image domain and show\nstate of the art results in the video domain.\n4.1. Datasets and models\nMS-COCO [43]. We use this image-caption dataset for\ntraining and validating our approach. We use the splits\nof [7] (118K/5K images for train/validation with 5 captions\nper image). We only use the ﬁrst caption of each image to\nmake validation faster for slow models. C-R@1 (resp. C-\nR@5) refers to recall at 1 (resp. 5) on the validation set.\nConceptual Captions (CC) [63]. We use this dataset for\ntraining our models (2.7M training images (out of the 3.2M)\nat the time of submission). CC contains images and cap-\ntions automatically scraped from the web which shows our\nmethod can work in a weakly-supervised training regime.\nFlickr30K [59]. We use this dataset for zero-shot evalu-\nation (i.e. we train on COCO or CC and test on Flickr) in\nthe ablation study, as well as ﬁne-tuning when comparing to\nthe state of the art. We use the splits of [34] (29K/1014/1K\nfor train/validation/test with 5 captions per image). We re-\nport results on the validation set except in Section 4.4 where\nwe report on the test split. We abbreviate F-R@1 (resp. F-\nR@5) as the R@1 (resp. R@5) scores on Flickr.\nV ATEX [72]. V ATEX contains around 40K short 10 sec-\nonds clip from the Kinetics-600 dataset [4] annotated with\nmultiple descriptions. In this work, we only use the 10 En-\nglish captions per video clip and ignore the additional Chi-\nnese captions. We use the retrieval setup and splits from [6].\nModels. For each model, the visual backbone is a ResNet-\n50 v2 CNN [27] trained from scratch. Inputs are 224 ×224\ncrops for most of the validation experiments unless spec-\niﬁed otherwise. Models are optimized with ADAM [35],\nand a cosine learning rate decay [45] with linear warm-up\nis employed for the learning rate. The four main models\nused in this work are described next.\nNCE BoW is a dual-encoder (DE) approach where the text\nencoder is a bag-of-words [50] on top of word2vec [51] pre-\ntrained embeddings. The model is trained with the NCE\nloss given in Eq. (4) where the negative setNiis constructed\nas in [48]. We refer to NCE BoW as the Fast approach.\nNCE BERT is a DE approach where the text encoder is\na pretrained BERT base model [13]. We take the [CLS]\noutput for aggregating the text representation. The model is\nalso trained with the NCE loss given in Eq. (4).\nVirTex [12] is a cross-attention (CA) based approach that\noriginally aims at learning visual representations from text\ndata using a captioning pretext task. We chose this visual\nModel Type Train F-R@1 F-R@5 C-R@1 C-R@5\nFastNCE BoW DE COCO 27.2 54.1 24.8 53.7\nNCE BERT 24.4 48.0 24.2 52.0\nPixelBERT\nCA COCO\n30.0 55.1 25.1 52.5\nVirTex Fwd only 33.4 58.1 31.8 61.2\nVirTex 38.1 62.8 35.1 64.6\nFastNCE BoW DE CC 32.4 59.6 14.9 35.0\nNCE BERT 25.8 50.7 12.2 29.8\nPixelBERT\nCA CC\n30.4 57.7 14.1 33.6\nVirTex Fwd only 32.2 58.4 14.7 32.9\nVirTex 35.0 60.7 16.1 36.4\nTable 1: Dual encoder (DE) and Cross-attention (CA) compar-\nison. F-R@K corresponds to the recall at K on Flickr while C-\nR@K is the recall at K on COCO.\nFeature map Size F-R@1 F-R@5 C-R@1 C-R@5\nSlow96x96 384 44.8 70.5 39.0 67.7\nSlow56x56\n224\n42.2 66.8 38.5 65.2\nSlow28x28 40.4 66.3 37.4 66.8\nSlow14x14 39.2 63.8 36.8 64.9\nVirTexconv5(7x7)\n224\n38.1 62.8 35.1 64.6\nVirTexconv4(14x14) 38.9 64.4 34.9 63.5\nVirTexconv3(28x28) 32.4 57.9 30.4 58.3\nVirTexconv2(56x56) 20.6 41.1 18.3 43.0\nTable 2: Gradual upsampling with different feature map size.\nSize denotes the input image size. Models are trained on COCO.\ncaptioning model as another point of comparison for the\neffectiveness of Transformer-based captioning models for\ntext-to-vision retrieval.\nPixelBERT [29] is a CA approach trained with the standard\nmasked language modelling (MLM) and image-text match-\ning (ITM) losses for retrieval. One difference between our\nimplementation and the original PixelBERT is the use of\n224 ×224 images for a fair comparison with other models.\nNote that the main difference with VirTex is in the vision-\ntext Transformer architecture: PixelBERT uses a deep 12-\nlayer Transformer encoder while VirTex uses a shallow 3-\nlayer Transformer decoder to merge vision and language.\nWe chose PixelBERT and VirTex for their complemen-\ntarity and their simplicity since they do not rely on ob-\nject detectors. We reimplemented both methods so that we\ncould ensure that they were comparable. Next, we describe\nthe details of our proposed CA approach.\nSlow model architecture. For the upsampling, we follow\na similar strategy as used in BiFPN [68]. For the decoder,\nwe use a stack of 3 Transformer decoders with hidden di-\nmension 512 and 8 attention heads. Full details about the\narchitecture are provided in Appendix D.\n4.2. Improving cross-attention for retrieval\nIn this section, we provide an experimental study on the\nuse of cross-attention models for retrieval. All our results\nare validated on the COCO and the Flickr30K validation\n6\nStudent Teacher Train F-R@1 F-R@5 C-R@1 C-R@5\nFast None\nCOCO\n27.2 54.1 24.8 53.7\nSlow 37.7 64.7 32.5 62.1\nSlowupper bound 42.2 66.8 38.5 65.2\nFast None\nCC\n32.4 59.6 14.9 35.0\nSlow 33.4 60.1 17.2 38.1\nSlowupper bound 41.7 67.5 19.8 40.9\nTable 3: Distillation experiment with our proposedSlow approach\nas teacher and the Fast NCE BoW as student.\nsets with models pretrained on COCO and CC training sets.\nOur main ﬁndings are summarized below.\nCross-attention models are better than Dual Encoders.\nTable 1 compares various approaches for retrieval. We ob-\nserve that cross-attention models (PixelBERT and the Vir-\nTex variants), overall, outperform the dual encoders (NCE\nBoW and BERT). Interestingly, using a simple BoW text\nencoder performs better than using a BERT text encoder for\nthe DE models. This suggests that the complexity of the lan-\nguage model is not the key factor for good performance but\ninstead that complex merging strategy obtained from text-\nvision cross-attention may matter most for retrieval.\nCaptioning models are surprisingly good for retrieval.\nComparing ‘PixelBERT’ against the ‘VirTex Fwd only’ in\nTable 1 with the exact same input dimensions and visual\nbackbones, we see that using a captioning loss leads to bet-\nter results than using an image-text matching loss coupled\nwith a masked language modelling loss. Backward cap-\ntioning further improves retrieval performance. This result\ndemonstrates that captioning can be a strong alternative to\nthe usual image-text matching losses for retrieval.\nBeneﬁts of our gradual upsampling architecture design.\nIn Table 2, we provide the results using the proposed up-\nsampling strategy for our Slow model presented in Sec-\ntion 3.1 and illustrated in Figure 2. We observe signiﬁ-\ncant improvements over the VirTex baseline, denoted with\nconv5 (7x7), (more than 4% for R@1 on Flickr and more\nthan 3% on COCO) for the largest upsampling56 ×56. We\nalso conﬁrm that the performance gap does not just come\nfrom having a larger input feature map to attend to as the\nbaseline with the output of ResNet conv2, which has a\nresolution of 56 ×56, performs poorly. We believe it is im-\nportant to keep high-level abstraction in the feature maps\nwhile having high resolution which our proposed architec-\nture allows. It is also important to highlight that the pro-\nposed architecture leads to our best performing model and\ncan be combined with higher input resolution for further im-\nprovements. However, our proposed changes increase the\ninference time. Next, we explore how to recover the speed.\n4.3. Thinking Fast and Slow\nThis section focuses on getting both a fast and accurate\nmodel for retrieval. First, we evaluate the beneﬁt of the dis-\nModel Top K Dist. Train F-R@1 F-R@5 C-R@1 C-R@5F-Qt C-Qt\nSlow \u0017 \u0017\nCOCO\n44.8 70.4 39.0 67.7 4 s 19 s\nFast & Slow\n10 \u0017 44.0 63.0 38.6 61.5 0.12 s 0.12 s10 \u0013 47.2 70.1 40.5 67.8 0.12 s 0.12 s50 \u0017 46.7 65.6 40.2 68.2 0.60 s 0.60 s50 \u0013 47.6 73.2 40.9 70.0 0.60 s 0.60 sSlow \u0017 \u0017\nCC\n46.9 71.5 21.0 43.3 4 s 19 s\nFast & Slow\n10 \u0017 47.7 66.6 22.6 41.1 0.12 s 0.12 s10 \u0013 48.4 67.4 22.7 43.4 0.12 s 0.12 s50 \u0017 50.2 73.4 23.8 46.9 0.60 s 0.60 s50 \u0013 50.5 73.6 23.8 46.9 0.60 s 0.60 s\nTable 4: Combination of re-ranking and distillation. Dist.: dis-\ntillation. F-Qt (resp. C-Qt) is the query time in seconds on Flickr\nwith 1k images (resp. COCO with 5k images) using 1x V100 GPU.\ntillation from the Slow to the Fast model. Next, we evaluate\nthe beneﬁt of the re-ranking strategy and validate our com-\nbined approach on a large-scale retrieval experiment.\nDistillation improves dual encoder models. In Table 3,\nwe use our approach, denoted as Slow, to distill the knowl-\nedge to a Fast NCE BoW student dual encoder. The distil-\nlation improves the performance of theFast model with im-\nprovements of over 10% on R@1 when training on COCO,\nsigniﬁcantly reducing the gap between the Slow and Fast\nmodels. On the other hand, the improvements when training\non CC are moderate, but we believe the gap can be further\nreduced by training longer on CC as we found the distilla-\ntion often takes signiﬁcantly longer to converge.\nBeneﬁts of re-ranking. Table 4 provides the results from\nre-ranking. We see that with Kas low as 10, we are able to\nrecover or outperform the performance of theSlow model in\nterms of R@1 while signiﬁcantly decreasing the query time.\nCombining re-ranking with distillation leads to further im-\nprovements: on COCO, we can signiﬁcantly decrease from\nK = 50to K = 10the number of examples to re-rank to\noutperform the Slow model thanks to the distillation. In par-\nticular, we see a 100×reduction in retrieval time on COCO\nfrom our Slow to our Fast & Slow (K=10) model. Note that\nfor the rest of the experimental section, theSlow model runs\nwith an increased image resolution of 384 ×384 for better\nresults, albeit with slower inference.\nFigure 3 provides a more detailed visualization of the\neffect of re-ranking with respect to the number of top K\nexamples returned from the Fast distilled model. Notably,\nwe see on COCO that re-ranking as few as ﬁve images out\nof ﬁve thousand from the distilled Fast model is enough to\nreach the Slow model R@1 performance. More quantitative\nand qualitative results are given in Appendix B.\nDiscussion of scalability. We would like to emphasize that\nthe combination of the distillation and re-ranking would be\neven more appealing in the large-scale retrieval regime as\nour method allows application of fast approximate near-\nest neighbour search [11, 32, 52, 64] and hence can po-\ntentially scale to billion-scale image retrieval. As a result,\nour method scales sub-linearly with the number of test im-\nages and the time complexity mostly depends on the topK,\n7\n100\n 101\n 102\n 103\n0.34\n0.36\n0.38\n0.40\n 0.0\n0.1\n1.0\n10.0\n100.0\nTop K\nCOCO R@1\nFast&Slow\nSlow\nFast\nFigure 3: Retrieval result when varying the top-K retrieved ex-\namples from the distilled Fast model with varying β(See Eq. (9)).\nwhich is the number of calls to the Slow model.\n4.4. Comparison to the state of the art\nWe compare to the state of the art on Flickr30K in Ta-\nble 5 for the zero-shot and ﬁne-tuning setting.\nThe Fast model is distilled from the Slow model on the\npretraining dataset (COCO or CC). The Fast model and the\nSlow 384×384 models are then ﬁne tuned on the Flickr30K\ntraining set. When pretraining on CC, we signiﬁcantly out-\nperform the VilBERT [46] approach despite not using ex-\ntra information contained in object detectors. On COCO,\nwe outperform PixelBERT [29] with the same ResNet-50\nbackbone while neither training on Visual Genome (VG)\nannotations nor using high image resolution. Finally, we\nare still below the performance reported in UNITER [8]\nand OSCAR [40]. We believe this remaining gap can be at-\ntributed to (i) not using the same amount of pretraining data\n(UNITER was trained on the combination of four datasets:\nCOCO, CC but also Visual Genome (VG) and SBU and OS-\nCAR is trained on Flickr, CC, SBU and GQA [30]), (ii) not\nusing the same high input image resolution, (iii) not relying\non pre-trained object detectors, and (iv) having a smaller\nmodel (3 layers transformer with hidden dimension 512 vs.\n24 layers with dimension 1024 for UNITER). However our\nproposed approach enables fast retrieval at scale which is\nnot possible out of the box with any of the previously men-\ntioned methods. More importantly, our scaling approach\n(distillation and re-ranking) can also be applied to other\nmultimodal transformers including UNITER and OSCAR.\nExtension to video. Our approach can also be applied to\nvideo. To do so, we extend the architecture introduced in\nSection 3.1 to a TSM ResNet50 model [42] with the fol-\nlowing modiﬁcations. The input of the network is now a\nsequence of 32 frames at resolution224×224. Due to mem-\nory constraints, we only upsample the last feature map to a\n14 ×14 grid and allow the decoder to attend to the result-\ning spatio-temporal volume representing the video of shape\n32×14×14 (details in Appendix D.2). We use a pretrained\nMethod Object Det. Size Train Zero-shot F-R@1 F-R@5 F-R@10\nVILBERT [46] \u0013 Full\nCC \u0013 31.9 61.1 72.8FastandSlow(K=100) \u0017 384 48.7 74.2 82.4VILBERT [46] \u0013 Full \u0017 58.2 84.9 91.5FastandSlow(K=100) \u0017 384 68.2 89.7 93.9PixelBERT (R50) [29]\u0017 800COCO+VG \u0017 59.8 85.5 91.6\nFastandSlow(R50, K=100) 384 COCO \u0017 62.9 85.8 91.3Unicoder-VL [37] \u0013 FullCC + SBU\u0017 71.5 90.9 94.9UNITER [8] \u0013 FullCOCO+CC+SBU+VG\n\u0017 75.6 94.1 96.8\nOSCAR [40] \u0013 FullCOCO+CC+SBU+GQA\n\u0017 75.9 93.3 96.6\nFastandSlow(K=100) \u0017 384COCO+CC \u0017 72.1 91.5 95.2\nTable 5: Comparison to state of the art for text-to-image retrieval.\nOSCAR results were reproduced from recent work [17].\nMethod R@1 R@5 R@10\nDual [15] 31.1 67.4 78.9\nHGR [6] 35.1 73.5 83.5\nSupport-set [55] 45.9 82.4 90.4\nFast NCE BoW 42.3 79.1 88.0\nFast and Slow (7 ×7) (K=10) 47.5 81.4 88.0\nFast and Slow (14 ×14) (K=10) 50.5 83.4 88.0\nFast and Slow (14 ×14) (K=50) 50.5 84.6 91.7\nTable 6: Comparison to state of the art retrieval on V ATEX.\nTSM ResNet-50 network [1] on HowTo100M [48] and Au-\ndioSet [18] datasets. Results are given in Table 6. We ob-\nserve that: (i) the upsampling architecture is also beneﬁcial\nfor video, and (ii) our Fast and Slow model sets a new state\nof the art on this benchmark.\n5. Conclusion\nWe have shown how to scale-up powerful vision-\ntext transformer-based models for retrieval. In particu-\nlar, we have introduced an accurate but Slow text-vision\ntransformer-based architecture with ﬁne-grained cross-\nattention for retrieval. To make it scalable for text-to-visual\nsearch, we have augmented thisSlow model with aFast dual\nencoder model through a combination of distillation and re-\nranking. As a result, the combined Fast & Slow approach\nachieves better results than the Slow model while signiﬁ-\ncantly reducing the inference time by several orders of mag-\nnitude on large datasets. We emphasize that our approach is\nmodel agnostic and can be applied to any vision-text Trans-\nformer Slow model and dual-encoder Fast retrieval model.\nAcknowledgements. We would like to thank Lisa Anne\nHendricks for feedback. The project was partially\nfunded by the French ANR as part of the “Investisse-\nments d’avenir” program, reference ANR-19-P3IA-0001\n(PRAIRIE 3IA Institute), and the European Regional De-\nvelopment Fund under the project IMPACT (reg. no.\nCZ.02.1.01/0.0/0.0/15 003/0000468).\n8\nReferences\n[1] Jean-Baptiste Alayrac, Adri `a Recasens, Rosalia Schneider,\nRelja Arandjelovi´c, Jason Ramapuram, Jeffrey De Fauw, Lu-\ncas Smaira, Sander Dieleman, and Andrew Zisserman. Self-\nSupervised MultiModal Versatile Networks. In NeurIPS,\n2020. 8\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\nSegnet: A deep convolutional encoder-decoder architecture\nfor image segmentation. TPAMI, 2017. 3\n[3] Cristian Bucilu ˇa, Rich Caruana, and Alexandru Niculescu-\nMizil. Model compression. In Proceedings of the 12th ACM\nSIGKDD international conference on Knowledge discovery\nand data mining, 2006. 2\n[4] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe\nHillier, and Andrew Zisserman. A short note about kinetics-\n600. arXiv preprint arXiv:1808.01340, 2018. 6\n[5] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Man-\nmohan Chandraker. Learning efﬁcient object detection mod-\nels with knowledge distillation. In NIPS, 2017. 3\n[6] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained\nvideo-text retrieval with hierarchical graph reasoning. In\nCVPR, 2020. 6, 8\n[7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll ´ar, and C Lawrence Zitnick.\nMicrosoft coco captions: Data collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 6\n[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nLearning universal image-text representations. In ECCV,\n2020. 2, 3, 4, 8\n[9] Ondrej Chum, James Philbin, Josef Sivic, Michael Isard, and\nAndrew Zisserman. Total recall: Automatic query expansion\nwith a generative feature model for object retrieval. InICCV,\n2007. 2\n[10] Kahneman Daniel. Thinking, fast and slow, 2017. 2\n[11] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S\nMirrokni. Locality-sensitive hashing scheme based on p-\nstable distributions. In Proceedings of the twentieth annual\nsymposium on Computational geometry, 2004. 1, 2, 7\n[12] Karan Desai and Justin Johnson. VirTex: Learning Visual\nRepresentations from Textual Annotations. arXiv preprint\narXiv:2006.06666, 2020. 2, 3, 6\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n2, 5, 6\n[14] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\nMarcus Rohrbach, Subhashini Venugopalan, Kate Saenko,\nand Trevor Darrell. Long-term recurrent convolutional net-\nworks for visual recognition and description. InCVPR, 2015.\n2, 3\n[15] Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He,\nGang Yang, and Xun Wang. Dual encoding for zero-example\nvideo retrieval. In CVPR, 2019. 2, 8\n[16] Victor Escorcia, Mattia Soldan, Josef Sivic, Bernard\nGhanem, and Bryan Russell. Temporal localization of mo-\nments in video collections with natural language. arXiv\npreprint arXiv:1907.12763, 2019. 2\n[17] Gregor Geigle, Jonas Pfeiffer, Nils Reimers, Ivan Vulic,\nand Iryna Gurevych. Retrieve fast, rerank smart: Coopera-\ntive and joint approaches for improved cross-modal retrieval.\narXiv preprint arXiv:2103.11920, 2021. 8\n[18] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren\nJansen, Wade Lawrence, R Channing Moore, Manoj Plakal,\nand Marvin Ritter. Audio set: An ontology and human-\nlabeled dataset for audio events. In ICASSP, 2017. 8\n[19] Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazeb-\nnik. A multi-view embedding space for modeling internet\nimages, tags, and their semantics. IJCV, 2014. 2\n[20] Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hocken-\nmaier, and Svetlana Lazebnik. Improving image-sentence\nembeddings using large weakly annotated photo collections.\nIn ECCV, 2014. 2\n[21] Albert Gordo, Filip Radenovic, and Tamara Berg. Attention-\nbased query expansion learning. In ECCV, 2020. 2\n[22] Saurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross\nmodal distillation for supervision transfer. In CVPR, 2016. 3\n[23] Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang,\nJan Kautz, and Derek Hoiem. Contrastive learning for\nweakly supervised phrase grounding. In ECCV, 2020. 2\n[24] Michael Gutmann and Aapo Hyv ¨arinen. Noise-contrastive\nestimation: A new estimation principle for unnormalized sta-\ntistical models. In AISTATS, 2010. 5\n[25] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and Ji-\ntendra Malik. Hypercolumns for object segmentation and\nﬁne-grained localization. In CVPR, 2015. 3\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In CVPR,\n2016. 3\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In ECCV,\n2016. 6\n[28] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015. 2, 3, 5, 11\n[29] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu,\nand Jianlong Fu. Pixel-bert: Aligning image pixels with\ntext by deep multi-modal transformers. arXiv preprint\narXiv:2004.00849, 2020. 2, 3, 6, 8, 12\n[30] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 8\n[31] H. J ´egou, M. Douze, and C. Schmid. Hamming embedding\nand weak geometric consistency for large scale image search.\nIn ECCV, 2008. 2\n[32] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product\nquantization for nearest neighbor search. PAMI, 2010. 1, 2,\n3, 7\n[33] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. Exploring the limits of language\nmodeling. arXiv preprint arXiv:1602.02410, 2016. 5\n[34] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. In CVPR, 2015. 6\n[35] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6\n[36] Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. Asso-\nciating neural word embeddings with deep image represen-\ntations using ﬁsher vectors. In CVPR, 2015. 2\n[37] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang,\n9\nand Ming Zhou. Unicoder-VL: A Universal Encoder for Vi-\nsion and Language by Cross-modal Pre-training. In AAAI,\n2020. 2, 3, 4, 8\n[38] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu,\nand Jingjing Liu. Hero: Hierarchical encoder for video+ lan-\nguage omni-representation pre-training. In EMNLP, 2020.\n2\n[39] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. VisualBERT: A simple and perfor-\nmant baseline for vision and language. arXiv preprint\narXiv:1908.03557, 2019. 2, 3, 4\n[40] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In ECCV, 2020. 2, 8\n[41] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao,\nJiebo Luo, and Li-Jia Li. Learning from noisy labels with\ndistillation. In ICCV, 2017. 3\n[42] Ji Lin, Chuang Gan, and Song Han. TSM: Temporal shift\nmodule for efﬁcient video understanding. In ICCV, 2019. 8,\n13\n[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common Objects in Context. In\nECCV, 2014. 2, 6\n[44] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo,\nand Jingdong Wang. Structured knowledge distillation for\nsemantic segmentation. In CVPR, 2019. 3\n[45] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradi-\nent descent with warm restarts. In ICLR, 2017. 6\n[46] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In NeurIPS, 2019. 2, 3, 4, 8\n[47] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 12-in-1: Multi-task vision and lan-\nguage representation learning. In CVPR, 2020. 2\n[48] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan\nLaptev, Josef Sivic, and Andrew Zisserman. End-to-End\nLearning of Visual Representations from Uncurated Instruc-\ntional Videos. In CVPR, 2020. 6, 8\n[49] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a\nText-Video Embedding from Incomplete and Heterogeneous\nData. arXiv preprint arXiv:1804.02516, 2018. 2\n[50] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowto100M: Learning a text-video embedding by watching\nhundred million narrated video clips. In ICCV, 2019. 2, 5, 6\n[51] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Cor-\nrado, and Jeffrey Dean. Distributed representations of words\nand phrases and their compositionality. In NIPS, 2013. 6\n[52] Marius Muja and David G Lowe. Fast approximate nearest\nneighbors with automatic algorithm conﬁguration. VISAPP,\n2009. 1, 2, 3, 7\n[53] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-\nglass networks for human pose estimation. In ECCV, 2016.\n3\n[54] Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong\nRui. Jointly modeling embedding and translation to bridge\nvideo and language. In CVPR, 2016. 2\n[55] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian\nMetze, Alexander Hauptmann, Jo ˜ao Henriques, and Andrea\nVedaldi. Support-set bottlenecks for video-text representa-\ntion learning. arXiv preprint arXiv:2010.02824, 2020. 8\n[56] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\nAndrew Zisserman. Object retrieval with large vocabularies\nand fast spatial matching. In CVPR, 2007. 2\n[57] AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo.\nEvolving losses for unsupervised video representation learn-\ning. In CVPR, 2020. 3\n[58] Bryan A Plummer, Matthew Brown, and Svetlana Lazebnik.\nEnhancing video summarization via vision-language embed-\nding. In CVPR, 2017. 2\n[59] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\n2015. 2, 6\n[60] Ilija Radosavovic, Piotr Doll ´ar, Ross Girshick, Georgia\nGkioxari, and Kaiming He. Data distillation: Towards omni-\nsupervised learning. In CVPR, 2018. 3\n[61] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, 2015. 3\n[62] Victor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. DistilBERT, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108, 2019. 3\n[63] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In ACL,\n2018. 6\n[64] Josef Sivic and Andrew Zisserman. Video google: A text\nretrieval approach to object matching in videos. In ICCV,\n2003. 1, 2, 3, 7\n[65] Jonathan Stroud, David Ross, Chen Sun, Jia Deng, and Rahul\nSukthankar. D3d: Distilled 3d networks for video action\nrecognition. In WACV, 2020. 3\n[66] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations. In ICLR, 2019. 2, 3, 4\n[67] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. In\nEMNLP, 2019. 2, 3, 4\n[68] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcientdet:\nScalable and efﬁcient object detection. In CVPR, 2020. 3, 6,\n12\n[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 2, 3\n[70] Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik.\nLearning two-branch neural networks for image-text match-\ning tasks. PAMI, 2018. 2\n[71] Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning\ndeep structure-preserving image-text embeddings. In CVPR,\n2016. 2\n[72] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang. Vatex: A large-scale, high-\nquality multilingual dataset for video-and-language research.\n10\nIn ICCV, 2019. 2, 6\n[73] Michael Wray, Diane Larlus, Gabriela Csurka, and Dima\nDamen. Fine-grained action retrieval through multiple parts-\nof-speech embeddings. In ICCV, 2019. 2\n[74] Chao-Yuan Wu, R Manmatha, Alexander J Smola, and\nPhilipp Kr ¨ahenb¨uhl. Sampling matters in deep embedding\nlearning. ICCV, 2017. 2\n[75] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V\nLe. Self-training with noisy student improves imagenet clas-\nsiﬁcation. In CVPR, 2020. 3\n[76] Ran Xu, Caiming Xiong, Wei Chen, and Jason J Corso.\nJointly modeling deep video and compositional text to bridge\nvision and language in a uniﬁed framework. In AAAI, 2015.\n2\n[77] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson J Corso, and Jianfeng Gao. Uniﬁed vision-language pre-\ntraining for image captioning and vqa. In AAAI, 2020. 2, 3,\n4\n[78] Linchao Zhu and Yi Yang. Actbert: Learning global-local\nvideo-text representations. In CVPR, 2020. 2\nA. Appendix\nWe provide here more details about the main paper. Sec-\ntion B gives additional ablation results for the distillation\nmethod and the Flickr re-ranking curve (similarly to the\nCOCO re-ranking curve in the main paper). In Section C,\nwe provide additional details about our training hyperpa-\nrameters. Section D describes in more details the proposed\narchitecture for upsampling as well as the video architec-\nture extension. Finally, in Section E we provide qualitative\nresults of our approach.\nB. Additional quantitative results\nWhat matters for good distillation.In Table 7, we explore\nvarious text models for the Fast dual-encoder student when\nperforming distillation. Interestingly, the BoW model still\nseems to be the best ﬁt for distillation, hinting that complex\nlanguage models are not necessarily the most important for\nthe task we consider. This is in line with our ﬁndings in the\nmain paper about the use of more complex language mod-\nels for Noise-Contrastive Estimation (NCE) (Equation 4\nfrom the main paper) that did not lead to improvements.\nTable 8 shows that care should be given to the choice of\ntemperature used when performing the distillation and that\ncombining the distillation loss with the original loss is cru-\ncial to ensuring improvements. Note that we follow [28]\nand adapt the combination factor αwith respect to the tem-\nperature parameter τ. Based on that study, we use τ = 10\nand α\nτ2 = 0.001 for all other distillation experiments of the\npaper. The models were trained on COCO for 50k steps.\nFlickr re-ranking results when varying K. Figure 4 pro-\nvides a more detailed visualization of the effect of re-\nranking with respect to the number of top K examples re-\nturned from the Fast distilled model on the Flickr validation\nText model Depth F-R@1 F-R@5 C-R@1 C-R@5\nBag-of-words 1 35.6 60.8 31.2 61.1\nTransformer\n1 27.4 51.7 20.1 45.0\n3 26.3 49.7 19.4 43.6\n6 27.1 49.9 20.0 44.0\n12 28.9 50.0 19.6 43.5\nSlowupper bound 42.2 66.8 38.5 65.2\nTable 7: Distillation: Which text model to use for the dual en-\ncoder approach on COCO.\nτ α\nτ2 F-R@1 F-R@5 C-R@1 C-R@5\n1.0\n0.0 26.8 54.0 24.1 52.7\n0.1 28.2 54.0 24.9 55.8\n1.0 27.2 51.1 25.2 52.8\n10.0 19.1 39.6 19.0 42.6\n10.0\n0.0 34.0 61.6 31.1 61.0\n0.1 35.7 61.0 30.9 61.2\n1.0 34.5 61.0 31.7 61.0\n10.0 27.2 53.4 26.2 54.6\nTable 8: Distillation temperature and loss weighting ablation\nstudy. The models were trained on COCO for 50k steps.\n100\n 101\n 102\n 103\nTop K\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48Flickr R@1\n0.0\n0.1\n1.0\n10.0\n100.0\nFast&Slow\nSlow\nFast\nFigure 4: Flickr retrieval result when varying the top-K re-\ntrieved examples from the distilledFast model with varying\nβ.\nset. The models were trained on COCO. Here again, we see\nthat re-ranking as few as four images out of thousand from\nthe distilled Fast model is enough to reach the Slow model\nR@1 performance.\nC. Experiment details\nTraining time data augmentation. We ran-\ndomly crop images using the tensorﬂow function\ntf.image.sample distorted bounding box1 with the\nfollowing parameters:\n1https://www.tensorflow.org/api_docs/python/tf/\nimage/sample_distorted_bounding_box\n11\n• min object covered=0.2,\n• aspect ratio range=(3 / 4, 4 / 3),\n• area range=(0.2, 1.0).\nThe crops are then resized to 224 ×224 and randomly\nﬂipped from left to right with a probability of 0.5. Note\nthat while ﬂipping an image from left to right is a com-\nmon data augmentation technique in classiﬁcation, it can be\nproblematic in captioning annotation that mention parts of\nimages speciﬁcally to the left or right. However, we counted\nthat on Conceptual Caption around 1.3% of the captions ei-\nther mention the word left or right while this percent-\nage is down on COCO to 0.6% which is sufﬁciently low to\nnot cause a problem. For our video experiment we use the\nsame spatial augmentation but additionaly subsample tem-\nporal clips of 32 frames at 10 frame per second ( 3.2 sec-\nonds) from the original 10 seconds clips of V ATEX.\nTest time data augmentation. For all image experi-\nments, we simply take the central crop of the image to per-\nform the retrieval. For videos, we sample 4 temporally uni-\nformly clips over the video. Each clip has32 frames that are\ncentrally cropped. We average the visual-text score h(x,y)\nover the 4 clips to obtain the ﬁnal score.\nTraining hyper-parameters. In the following, we pro-\nvide the optimization details for each of the trained models\non COCO and CC. We recall that each model is trained us-\ning the ADAM optimizer with a cosine learning rate decay\nand 5k steps of warm up.\n• NCE BoW/BERT : The models are trained both on\nCOCO and CC using a total batch size of 1024 and\na base learning rate of 0.001. On COCO, the model is\ntrained for 20k steps while it is trained for 140k steps\non CC. A gradient clip of 30.0 is applied. When per-\nforming distillation, we instead train longer for COCO\nuntil 100k steps. The weight decay is set to 0.0001.\n• VirTex and Slow : The models are trained both on\nCOCO and CC using a base learning rate of 0.0004.\nOn COCO, the model is trained for 250k steps with a\nbatch size of 512 while it is trained for 500k steps with\na batch size of 1024 on CC. A gradient clip of 100.0 is\napplied. The weight decay is set to 0.0001.\n• PixelBERT: The models are trained both on COCO\nand CC using a total batch size of 1024 and a base\nlearning rate of 0.0001. On both CC and COCO, the\nmodel is trained for 800k steps. A gradient clip of\n30.0 is applied. The weight decay is set to 0.0001.\nNote that because we worked with smaller resolution\nimages compared to the original PixelBERT work, we\nCNN\nBackbone\nCNN -based Visual Encoder\n[CLS] The … a [MASK] umbrell\na [SEP]\nSentence Encoder \nTransformers\nCross -Modality \nAlignment\n[V]\n[V]\n…\n[V]\n[V]\nConv\nPooling\nRandom \nSampling\nPixel Feature Embedding\nEmbedding\nToken\nPosition\n[CLS]\nThe\n…\na\n[MASK]\numbrella\n[SEP]\n[CLS]\nthe\n…\na\n[MASK]\numbrella\n[SEP]\n[V]\n[V]\n…\n[V]\n[V]\nImage -Text \nMatching (ITM )\nMasked Language \nModel (MLM )\nSemantic\nSemantic\nEmbedding\nElementwise Sum\n[ȉ]   Special Token\nThe woman held a black \numbrella\nPre -Training \nTasks\nPixel -BERT\n[V]  Visual Token\n[MA TCH]\nblack\n:Removed from original implementation\nFigure 5: Illustration of the PixelBERT [29], architecture\nwith the pooling and sampling modules we have removed\nfrom the original implementation.\nremoved the downsampling block (which follows the\nResNet backbone and is composed of a max pooling\nand random sampling of pixels from the feature map).\nInstead we feed all the 7 ×7 visual features to the\ntransformer. Figure 5 provides an illustration of the ar-\nchitecture, taken from the original work [29], with the\nmodules we have removed to deal with smaller image\nresolution.\nThe models are trained using the JAX deep learning\nframework.\nD. Architecture details\nD.1. Upsampling strategy\nWe follow the same upsampling mechanism used in the\nBiFPN architecture [68]. In particular we perform the Fast\nnormalized fusion approach:\nPout = SepConv\n(w1 ·Pin + w2 ·Resize(Pprev)\nw1 + w2 + ϵ\n)\nwhere wi ≥0 is ensured by applying a ReLU after each\nwi, ϵ= 0.0001 is a small value to avoid numerical instabil-\nity, Resize is a 2 ×upsampling using a nearest neighbour\ninterpolation, Pin is the feature map input of the upsam-\npling block and Pprev is the feature map from the previous\nResNet feature maps with its dimensionality reduced to 512\nthrough a 1 ×1 convolution. SepConv is a separable depth\nwise convolution layer followed by a batch normalization\nand ReLU. A more detailed illustration of the upsampling\narchitecture is illustrated in Figure 2.\n12\nD.2. Video architecture with upsampling\nTSM-ResNet32 x 56 x 56 x 256\n32 x 28 x 28 x 51232 x 14 x 14 x 102432 x 7 x 7 x 2048 32 x 14 x 14 x 51232 x 7 x 7 x 512\n2x UP\nGradual spatial upsampling\n1x1x1 Conv, 512TSM conv4TSMconv5 1x3x3 SepConv, 5121x1x1 Conv, 512\nTSMconv2TSMconv3\nFigure 6: Upsampling architecture for videos: adaptation of\nthe upsampling architecture for the TSM network.\nIn order to apply our architecture to video for our exper-\niment on V ATEX, we adapt the image only architecture to\na video one that can handle spatio-temporal attention. For\nthat, we start from the TSM ResNet-50 architecture [42].\nThis architecture consists of a standard ResNet-50 model\nwhere Temporal Shift Modules (TSM) are inserted within\neach residual block. The Temporal Shift Module enables\ntemporal modeling by moving features along the time di-\nmension (forward and backward in time). One particularity\nof the model is that there is no temporal pooling hence the\ntemporal resolution stays the same everywhere in the net-\nwork. For that reason, we use the same spatial upsampling\nstrategy that we develop for the image network as illustrated\nin Figure 6 and simply adapts it to deal with the additional\ntemporal dimension. Due to memory constraints, we only\nspatially upsample the feature map to a 14 ×14. We use\nclips of 32 frames sampled at 10 frame per second for our\nV ATEX experiment (3.2 seconds of video). As a result, the\nTransformer can attend to a 32 ×14 ×14 spatio-temporal\nfeature map.\nE. Qualitative results\nE.1. Retrieval results\nWe also provide retrieval results on Flickr using four ap-\nproaches: Fast w/o distillation, Fast, Slow and Fast and\nSlow (K=50) . Figure 7 illustrates one retrieval example,\nwhere we show the top-5 retrieved examples for each model\n(ﬁrst row: Fast w/o distillation, second row: Fast , third\nrow: Slow). The last row shows the top K=50 re-ranked\nexamples from the Fast using the Slow model. The models\nare trained on COCO and evaluated on Flickr in a zero-shot\nmanner. Note that we have biased the results towards ex-\namples that failed for theFast w/o distillationmodel but are\nsuccessfully retrieved with re-ranking.\nFast w/o distillation Fast Slow Fast & Slow (K=50)\nA white dog drinks water on a mountainside\nrank\n1\n2\n3\n4\n5\nFigure 7: Retrieval qualitative examples on Flickr. First column:\nFast w/o distillation, Second column: Fast , Third column: Slow\nand the last column shows top K=50 re-ranked examples from the\nFast using the Slow model. The image with the green bound-\ning box is the annotated groundtruth for the query. Note that the\ngroundtruth image is not retrieved in the top-5 with theFast model\nwithout distillation and is retrieved at the 4th place with distilla-\ntion. The Slow model correctly retrieves it at the ﬁrst place simi-\nlarly to the re-ranking model. Moreover, we can see that the Slow\n& Fast approach is the only one that either retrieves a white dog\nor a dog on water.\nE.2. Attention maps\nIn this section, we provide a qualitative analysis of the\nattention maps between the text and the input image in the\nTransformer model. We start by describing how we obtain\nthese attention maps and how we display them. Next, we\nprovide our main observations and ﬁndings from analyzing\nthese feature maps. To conduct this qualitative study, we\ncompare two models trained on COCO: (i) our Slow model\n(see Figure 8) which can attend to the 56 ×56 upsampled\nfeature map and (ii) our reimplementation of the VirTeX\nmodel (see Figure 9) with decoder heads attending to a7×7\nfeature map.\n13\nExtracting and visualizing attention maps. Recall that\nour textual decoder is a 3 layer Transformer. Each of these\nlayers can perform cross-attention between an input word\ntoken and the whole image feature map of size H ×H in\norder to output prediction scores for the next word. In de-\ntail, at each layer and for each input text token, a query\ntext vector is produced and compared to the precomputed\nH ×H visual keys via dot product. The resulting unnor-\nmalized H×Hscores are then normalized with a softmax.\nThese normalized weights are then used to aggregateH×H\nvisual vectors, or values, that are used to update the cur-\nrent token representation in order to predict the scores of\nthe next word. This process is done in parallel for 8 atten-\ntion heads before concatenating the outputs of all heads in\na single vector.\nThis gives an opportunity to visualize the attention maps\nto see what are the regions of the images that are attended\nto in order to output a given word. We provide such visual-\nization in Figure 8 and Figure 9. Each ﬁgure has multiple\nexamples shown in different rows. On the left, we show the\ninput image in color. On the bottom right, we show the input\ntokens shifted backward by one word so that we direclty vi-\nsualize what the attention maps look like for the prediction\nof the current word. On the right, we show the attention\nmaps for the different layers of the transformer in yellow\noverlaid over the gray image. The attention map rows are\nordered so that the bottom one corresponds to the layer clos-\nest to the input text. The attention maps are resized to the\noriginal input image resolution (224×224) via bicubic sam-\npling. We only show a single attention head per token and\nper layer by selecting the one that has the highest average\nscore over the H ×H grid prior to applying the softmax.\nFor that study, we use as inputs the ground truth captions\nand images from the Flickr validation set in order to emu-\nlate what happens when the Transformer model is used for\nretrieval.\nAnalysis. Looking at Figure 8 and Figure 9 we make the\nfollowing observations.\nFirst of all, in both cases we see that there is some level\nof coherency between the attended regions and the tokens\nbeing predicted. For example, in the second row of Figure 8\n(the woman with the bicycle), we see that the last layer of\nthe transformer attends to the mouth of the woman for the\nword “smilling”, at the shirt of the woman to predict the\ncolor “peach” of the top, and ﬁnally attends to the region\ncontaining the bicycle to predict the last word.\nSecond, we observe that, as expected, the attention maps\nobtained from our Slow model in Figure 8 is indeed ﬁne\ngrained when compared to the original VirTeX attention\nmaps of Figure 9. This can notably be seen on the last\nrow of the ﬁgures, where the Slow model can attend more\nprecisely to the faces of the children thanks to the higher\n56 ×56 resolution feature map compared to the crude 7 ×7\nfeature map of the original VirTeX.\nThird, we note that for the Slow model, the attention be-\ncomes more focused for the higher layers that are closer\nto the output. This is notably true when being input the\nﬁrst word where the attention systematically covers the full\nimage for the ﬁrst layer before being reﬁned on a speciﬁc\nregion in the image.\nFinally, while these visualizations are not always per-\nfectly interpretable, we believe similar studies and inspec-\ntions are valuable to better understand how these models\nrelate text and vision.\n14\nTransformer layers\nTransformer layers\nTransformer layers\nTransformer layers\nA group of people are standing on a pile of wool in a truck .\na smiling woman in a peach tank top stands holding a mountain bike\na woman wearing a red coat is jumping outdoors .\na boy wearing headphones sits on a woman ‘ s shoulders .\na woman and three children are picking something out of a blue\nTransformer layers\ntrash can .\nFigure 8: Attention maps visualization for the Slow model with the 56 ×56 upsampled feature map. For each image, the attention maps\nare given so that the bottom row corresponds to the transformer layer closest to the input text. See main text in Appendix E.2 for details.\nBest seen in color on a screen.\n15\nTransformer layers\nTransformer layers\nTransformer layers\nTransformer layers\nA group of people are standing on a pile of wool in a truck .\na smiling woman in a peach tank top stands holding a mountain bike\na woman wearing a red coat is jumping outdoors .\na boy wearing headphones sits on a woman ‘ s shoulders .\na woman and three children are picking something out of a blue\nTransformer layers\ntrash can .\nFigure 9: Attention maps visualization for VirTex (7 ×7 attention map). For each image, the attention maps are given so that the bottom\nrow corresponds to the transformer layer closest to the input text. See main text in Appendix E.2 for details. Best seen in color on a screen.\n16"
}