{
  "title": "Emerging Cross-lingual Structure in Pretrained Language Models",
  "url": "https://openalex.org/W3035547806",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2555471392",
      "name": "Alexis Conneau",
      "affiliations": [
        "Johns Hopkins University",
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2130569026",
      "name": "Shijie Wu",
      "affiliations": [
        "Johns Hopkins University",
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2099815881",
      "name": "Haoran Li",
      "affiliations": [
        "Johns Hopkins University",
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "Meta (Israel)",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2130709233",
      "name": "Veselin Stoyanov",
      "affiliations": [
        "Meta (Israel)",
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2009284521",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W2952190837",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W2963047628",
    "https://openalex.org/W4288284086",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2925907129",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2767204723",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2740132093",
    "https://openalex.org/W2963206679",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W2741602058",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970850076",
    "https://openalex.org/W2252046065",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2949531524",
    "https://openalex.org/W2915128308",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W2970316683",
    "https://openalex.org/W2963759780",
    "https://openalex.org/W2949359738",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2950797315",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W4294103325",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W2960374072",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2594021297",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2963337368",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2915774325",
    "https://openalex.org/W2950342398",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W4289363269",
    "https://openalex.org/W2963571341"
  ],
  "abstract": "We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6022–6034\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n6022\nEmerging Cross-lingual Structure in Pretrained Language Models\nAlexis Conneau♥∗ Shijie Wu♠∗\nHaoran Li♥ Luke Zettlemoyer♥ Veselin Stoyanov♥\n♠Department of Computer Science, Johns Hopkins University\n♥Facebook AI\naconneau@fb.com, shijie.wu@jhu.edu\n{aimeeli,lsz,ves}@fb.com\nAbstract\nWe study the problem of multilingual masked\nlanguage modeling, i.e. the training of a sin-\ngle model on concatenated text from multi-\nple languages, and present a detailed study of\nseveral factors that inﬂuence why these mod-\nels are so effective for cross-lingual transfer.\nWe show, contrary to what was previously hy-\npothesized, that transfer is possible even when\nthere is no shared vocabulary across the mono-\nlingual corpora and also when the text comes\nfrom very different domains. The only require-\nment is that there are some shared parameters\nin the top layers of the multi-lingual encoder.\nTo better understand this result, we also show\nthat representations from monolingual BERT\nmodels in different languages can be aligned\npost-hoc quite effectively, strongly suggesting\nthat, much like for non-contextual word em-\nbeddings, there are universal latent symme-\ntries in the learned embedding spaces. For\nmultilingual masked language modeling, these\nsymmetries are automatically discovered and\naligned during the joint training process.\n1 Introduction\nMultilingual language models such as mBERT (De-\nvlin et al., 2019) and XLM (Lample and Conneau,\n2019) enable effective cross-lingual transfer — it\nis possible to learn a model from supervised data\nin one language and apply it to another with no\nadditional training. Recent work has shown that\ntransfer is effective for a wide range of tasks (Wu\nand Dredze, 2019; Pires et al., 2019). These work\nspeculates why multilingual pretraining works (e.g.\nshared vocabulary), but only experiment with a\nsingle reference mBERT and is unable to systemat-\nically measure these effects.\nIn this paper, we present the ﬁrst detailed em-\npirical study of the effects of different masked lan-\n∗Equal contribution. Work done while Shijie was intern-\ning at Facebook AI.\nguage modeling (MLM) pretraining regimes on\ncross-lingual transfer. Our ﬁrst set of experiments\nis a detailed ablation study on a range of zero-shot\ncross-lingual transfer tasks. Much to our surprise,\nwe discover that language universal representations\nemerge in pretrained models without the require-\nment of any shared vocabulary or domain similarity,\nand even when only a subset of the parameters in\nthe joint encoder are shared. In particular, by sys-\ntematically varying the amount of shared vocabu-\nlary between two languages during pretraining, we\nshow that the amount of overlap only accounts for\na few points of performance in transfer tasks, much\nless than might be expected. By sharing parameters\nalone, pretraining learns to map similar words and\nsentences to similar hidden representations.\nTo better understand these effects, we also ana-\nlyze multiple monolingual BERT models trained\nindependently. We ﬁnd that monolingual models\ntrained in different languages learn representations\nthat align with each other surprisingly well, even\nthough they have no shared parameters. This result\nclosely mirrors the widely observed fact that word\nembeddings can be effectively aligned across lan-\nguages (Mikolov et al., 2013). Similar dynamics\nare at play in MLM pretraining, and at least in part\nexplain why they aligned so well with relatively\nlittle parameter tying in our earlier experiments.\nThis type of emergent language universality has\ninteresting theoretical and practical implications.\nWe gain insight into why the models transfer so\nwell and open up new lines of inquiry into what\nproperties emerge in common in these represen-\ntations. They also suggest it should be possible\nto adapt pretrained models to new languages with\nlittle additional training and it may be possible to\nbetter align independently trained representations\nwithout having to jointly train on all of the (very\nlarge) unlabeled data that could be gathered. For\nexample, concurrent work has shown that a pre-\n6023\ntrained MLM model can be rapidly ﬁne-tuned to\nanother language (Artetxe et al., 2019).\nThis paper offers the following contributions:\n• We provide a detailed ablation study on cross-\nlingual representation of bilingual BERT. We\nshow parameter sharing plays the most impor-\ntant role in learning cross-lingual representa-\ntion, while shared BPE, shared softmax and\ndomain similarity play a minor role.\n• We demonstrate even without any shared sub-\nwords (anchor points) across languages, cross-\nlingual representation can still be learned.\nWith bilingual dictionary, we propose a sim-\nple technique to create more anchor points by\ncreating synthetic code-switched corpus, ben-\neﬁting especially distantly-related languages.\n• We show monolingual BERTs of different lan-\nguage are similar with each other. Similar\nto word embeddings (Mikolov et al., 2013),\nwe show monolingual BERT can be easily\naligned with linear mapping to produce cross-\nlingual representation space at each level.\n2 Background\nLanguage Model Pretraining Our work fol-\nlows in the recent line of language model pretrain-\ning. ELMo (Peters et al., 2018) ﬁrst popularized\nrepresentation learning from a language model.\nThe representations are used in a transfer learning\nsetup to improve performance on a variety of down-\nstream NLP tasks. Follow-up work by Howard\nand Ruder (2018); Radford et al. (2018) further\nimproves on this idea by ﬁne-tuning the entire lan-\nguage model. BERT (Devlin et al., 2019) signiﬁ-\ncantly outperforms these methods by introducing\na masked-language model and next-sentence pre-\ndiction objectives combined with a bi-directional\ntransformer model.\nThe multilingual version of BERT (dubbed\nmBERT) trained on Wikipedia data of over 100\nlanguages obtains strong performance on zero-\nshot cross-lingual transfer without using any par-\nallel data during training (Wu and Dredze, 2019;\nPires et al., 2019). This shows that multilingual\nrepresentations can emerge from a shared Trans-\nformer with a shared subword vocabulary. Cross-\nlingual language model (XLM) pretraining (Lam-\nple and Conneau, 2019) was introduced concur-\nrently to mBERT. On top of multilingual masked\nlanguage models, they investigate an objective\nbased on parallel sentences as an explicit cross-\nlingual signal. XLM shows that cross-lingual lan-\nguage model pretraining leads to a new state of the\nart on XNLI (Conneau et al., 2018), supervised and\nunsupervised machine translation (Lample et al.,\n2018). Other work has shown that mBERT out-\nperforms word embeddings on token-level NLP\ntasks (Wu and Dredze, 2019), and that adding\ncharacter-level information (Mulcaire et al., 2019)\nand using multi-task learning (Huang et al., 2019)\ncan improve cross-lingual performance.\nAlignment of Word Embeddings Researchers\nworking on word embeddings noticed early that em-\nbedding spaces tend to be shaped similarly across\ndifferent languages (Mikolov et al., 2013). This\ninspired work in aligning monolingual embeddings.\nThe alignment was done by using a bilingual dictio-\nnary to project words that have the same meaning\nclose to each other (Mikolov et al., 2013). This pro-\njection aligns the words outside of the dictionary as\nwell due to the similar shapes of the word embed-\nding spaces. Follow-up efforts only required a very\nsmall seed dictionary (e.g., only numbers (Artetxe\net al., 2017)) or even no dictionary at all (Conneau\net al., 2017; Zhang et al., 2017). Other work has\npointed out that word embeddings may not be as\nisomorphic as thought (Søgaard et al., 2018) es-\npecially for distantly related language pairs (Patra\net al., 2019). Ormazabal et al. (2019) show joint\ntraining can lead to more isomorphic word embed-\ndings space.\nSchuster et al. (2019) showed that ELMo em-\nbeddings can be aligned by a linear projection as\nwell. They demonstrate a strong zero-shot cross-\nlingual transfer performance on dependency pars-\ning. Wang et al. (2019) align mBERT representa-\ntions and evaluate on dependency parsing as well.\nNeural Network Activation Similarity We hy-\npothesize that similar to word embedding spaces,\nlanguage-universal structures emerge in pretrained\nlanguage models. While computing word embed-\nding similarity is relatively straightforward, the\nsame cannot be said for the deep contextualized\nBERT models that we study. Recent work intro-\nduces ways to measure the similarity of neural\nnetwork activation between different layers and\ndifferent models (Laakso and Cottrell, 2000; Li\net al., 2016; Raghu et al., 2017; Morcos et al.,\n2018; Wang et al., 2018). For example, Raghu et al.\n6024\n(2017) use canonical correlation analysis (CCA)\nand a new method, singular vector canonical cor-\nrelation analysis (SVCCA), to show that early lay-\ners converge faster than upper layers in convolu-\ntional neural networks. Kudugunta et al. (2019) use\nSVCCA to investigate the multilingual representa-\ntions obtained by the encoder of a massively mul-\ntilingual neural machine translation system (Aha-\nroni et al., 2019). Kornblith et al. (2019) argues\nthat CCA fails to measure meaningful similarities\nbetween representations that have a higher dimen-\nsion than the number of data points and introduce\nthe centered kernel alignment (CKA) to solve this\nproblem. They successfully use CKA to identify\ncorrespondences between activations in networks\ntrained from different initializations.\n3 Cross-lingual Pretraining\nWe study a standard multilingual masked language\nmodeling formulation and evaluate performance\non several different cross-lingual transfer tasks, as\ndescribed in this section.\n3.1 Multilingual Masked Language Modeling\nOur multilingual masked language models follow\nthe setup used by both mBERT and XLM. We use\nthe implementation of Lample and Conneau (2019).\nSpeciﬁcally, we consider continuous streams of 256\ntokens and mask 15% of the input tokens which\nwe replace 80% of the time by a mask token, 10%\nof the time with the original word, and 10% of the\ntime with a random word. Note the random words\ncould be foreign words. The model is trained to\nrecover the masked tokens from its context (Taylor,\n1953). The subword vocabulary and model param-\neters are shared across languages. Note the model\nhas a softmax prediction layer shared across lan-\nguages. We use Wikipedia for training data, prepro-\ncessed by Moses (Koehn et al., 2007) and Stanford\nword segmenter (for Chinese only) and BPE (Sen-\nnrich et al., 2016) to learn subword vocabulary.\nDuring training, we sample a batch of continuous\nstreams of text from one language proportionally\nto the fraction of sentences in each training corpus,\nexponentiated to the power 0.7.\nPretraining details Each model is a Transformer\n(Vaswani et al., 2017) with 8 layers, 12 heads and\nGELU activiation functions (Hendrycks and Gim-\npel, 2016). The output softmax layer is tied with\ninput embeddings (Press and Wolf, 2017). The em-\nbeddings dimension is 768, the hidden dimension\nof the feed-forward layer is 3072, and dropout is\n0.1. We train our models with the Adam optimizer\n(Kingma and Ba, 2014) and the inverse square root\nlearning rate scheduler of Vaswani et al. (2017)\nwith 10−4 learning rate and 30k linear warmup\nsteps. For each model, we train it with 8 NVIDIA\nV100 GPUs with 32GB of memory and mixed pre-\ncision. It takes around 3 days to train one model.\nWe use batch size 96 for each GPU and each epoch\ncontains 200k batches. We stop training at epoch\n200 and select the best model based on English dev\nperplexity for evaluation.\n3.2 Cross-lingual Evaluation\nWe consider three NLP tasks to evaluate perfor-\nmance: natural language inference (NLI), named\nentity recognition (NER) and dependency parsing\n(Parsing). We adopt the zero-shot cross-lingual\ntransfer setting, where we (1) ﬁne-tune the pre-\ntrained model on English and (2) directly transfer\nthe model to target languages. We select the model\nand tune hyperparameters with the English dev set.\nWe report the result on average of best two set of\nhyperparameters.\nFine-tuning details We ﬁne-tune the model for\n10 epochs for NER and Parsing and 200 epochs\nfor NLI. We search the following hyperparam-\neter for NER and Parsing: batch size {16, 32};\nlearning rate {2e-5, 3e-5, 5e-5}. For XNLI, we\nsearch: batch size {4, 8}; encoder learning rate\n{1.25e-6, 2.5e-6, 5e-6}; classiﬁer learning rate\n{5e-6, 2.5e-5, 1.25e-4}. We use Adam with ﬁxed\nlearning rate for XNLI and warmup the learning\nrate for the ﬁrst 10% batch then decrease linearly to\n0 for NER and Parsing. We save checkpoint after\neach epoch.\nNLI We use the cross-lingual natural language\ninference (XNLI) dataset (Conneau et al., 2018).\nThe task-speciﬁc layer is a linear mapping to a\nsoftmax classiﬁer, which takes the representation\nof the ﬁrst token as input.\nNER We use WikiAnn (Pan et al., 2017), a silver\nNER dataset built automatically from Wikipedia,\nfor English-Russian and English-French. For\nEnglish-Chinese, we use CoNLL 2003 English\n(Tjong Kim Sang and De Meulder, 2003) and a Chi-\nnese NER dataset (Levow, 2006), with realigned\nChinese NER labels based on the Stanford word\nsegmenter. We model NER as BIO tagging. The\ntask-speciﬁc layer is a linear mapping to a softmax\n6025\nFigure 1: On the impact of anchor points and param-\neter sharing on the emergence of multilingual represen-\ntations. We train bilingual masked language models and\nremove parameter sharing for the embedding layers and ﬁrst\nfew Transformers layers to probe the impact of anchor points\nand shared structure on cross-lingual transfer.\nFigure 2: Probing the layer similarity of monolingual\nBERT models. We investigate the similarity of separate\nmonolingual BERT models at different levels. We use an\northogonal mapping between the pooled representations of\neach model. We also quantify the similarity using the cen-\ntered kernel alignment (CKA) similarity index.\nclassiﬁer, which takes the representation of the ﬁrst\nsubword of each word as input. We report span-\nlevel F1. We adopt a simple post-processing heuris-\ntic to obtain a valid span, rewriting standalone\nI-X into B-X and B-X I-Y I-Z into B-Z I-Z\nI-Z, following the ﬁnal entity type. We report the\nspan-level F1.\nParsing Finally, we use the Universal Dependen-\ncies (UD v2.3) (Nivre, 2018) for dependency pars-\ning. We consider the following four treebanks:\nEnglish-EWT, French-GSD, Russian-GSD, and\nChinese-GSD. The task-speciﬁc layer is a graph-\nbased parser (Dozat and Manning, 2016), using\nrepresentations of the ﬁrst subword of each word\nas inputs. We measure performance with the la-\nbeled attachment score (LAS).\n4 Dissecting mBERT/XLM models\nWe hypothesize that the following factors play im-\nportant roles in what makes multilingual BERT\nmultilingual: domain similarity, shared vocabu-\nlary (or anchor points), shared parameters, and lan-\nguage similarity. Without loss of generality, we\nfocus on bilingual MLM. We consider three pairs\nof languages: English-French, English-Russian,\nand English-Chinese.\n4.1 Domain Similarity\nMultilingual BERT and XLM are trained on the\nWikipedia comparable corpora. Domain similar-\nity has been shown to affect the quality of cross-\nlingual word embeddings (Conneau et al., 2017),\nbut this effect is not well established for masked\nlanguage models. We consider domain difference\nby training on Wikipedia for English and a random\nsubset of Common Crawl of the same size for the\nother languages ( Wiki-CC). We also consider a\nmodel trained with Wikipedia only ( Default) for\ncomparison.\nThe ﬁrst group in Tab. 1 shows domain mismatch\nhas a relatively modest effect on performance.\nXNLI and parsing performance drop around 2\npoints while NER drops over 6 points for all lan-\nguages on average. One possible reason is that\nthe labeled WikiAnn data for NER consists of\nWikipedia text; domain differences between source\nand target language during pretraining hurt per-\nformance more. Indeed for English and Chinese\nNER, where neither side comes from Wikipedia,\nperformance only drops around 2 points.\n4.2 Anchor points\nAnchor points are identical stringsthat appear in\nboth languages in the training corpus. Translingual\nwords like DNA or Paris appear in the Wikipedia\nof many languages with the same meaning. In\nmBERT, anchor points are naturally preserved due\nto joint BPE and shared vocabulary across lan-\nguages. Anchor point existence has been suggested\nas a key ingredient for effective cross-lingual trans-\nfer since they allow the shared encoder to have at\nleast some direct tying of meaning across different\nlanguages (Lample and Conneau, 2019; Pires et al.,\n2019; Wu and Dredze, 2019). However, this effect\n6026\n40 60\nACC\nDefault\nWiki-CC\nNo anchors\nDefault anchors\nExtra anchors\nSep Emb\nSep L1-3\nSep L1-6\nSep Emb + L1-3\nSep Emb + L1-6\nEn-Fr XNLI\n0 20 40 60\nF1\nEn-Zh NER\n0 20 40\nLAS\nEn-Ru Parsing\nFigure 3: Cross-lingual transfer of bilingual MLM on three tasks and language pairs under different settings.\nOthers tasks and languages pairs follows similar trend. See Tab. 1 for full results.\nModel Domain BPE Merges Anchors Pts Share Param. SoftmaxXNLI (Acc) NER (F1) Parsing (LAS)fr ru zh ∆ fr ru zh ∆ fr ru zh ∆\nDefault Wiki-Wiki 80k all all shared 73.6 68.7 68.3 0.079.8 60.9 63.6 0.073.2 56.6 28.8 0.0\nDomain Similarity(§4.1)\nWiki-CCWiki-CC - - - - 74.2 65.8 66.5 -1.474.0 49.6 61.9 -6.271.3 54.8 25.2 -2.5\nAnchor Points(§4.2)\nNo anchors- 40k/40k 0 - - 72.1 67.5 67.7 -1.174.0 57.9 65.0 -2.472.3 56.2 27.4 -0.9Default anchors- 40k/40k - - - 74.0 68.1 68.9 +0.176.8 56.3 61.2 -3.373.0 57.0 28.3 -0.1Extra anchors- - extra - - 74.0 69.8 72.1 +1.876.1 59.7 66.8 -0.573.3 56.9 29.2 +0.3\nParameter Sharing(§4.3)\nSep Emb - 40k/40k 0* Sep Emb lang-speciﬁc 72.7 63.6 60.8 -4.575.5 57.5 59.0 -4.171.7 54.0 27.5 -1.8Sep L1-3 - 40k/40k - Sep L1-3 - 72.4 65.0 63.1 -3.474.0 53.3 60.8 -5.369.7 54.1 26.4 -2.8Sep L1-6 - 40k/40k - Sep L1-6 - 61.9 43.6 37.4 -22.661.2 23.7 3.1 -38.761.7 31.6 12.0 -17.8Sep Emb + L1-3- 40k/40k 0* Sep Emb + L1-3 lang-speciﬁc69.2 61.7 56.4 -7.873.8 46.8 53.5 -10.068.2 53.6 23.9 -4.3Sep Emb + L1-6- 40k/40k 0* Sep Emb + L1-6 lang-speciﬁc51.6 35.8 34.4 -29.656.5 5.4 1.0 -47.150.9 6.4 1.5 -33.3\nTable 1: Dissecting bilingual MLM based on zero-shot cross-lingual transfer performance. - denote the same as\nthe ﬁrst row (Default). ∆ denote the difference of average task performance between a model and Default.\nhas not been carefully measured.\nWe present a controlled study of the impact of an-\nchor points on cross-lingual transfer performance\nby varying the amount of shared subword vocab-\nulary across languages. Instead of using a sin-\ngle joint BPE with 80k merges, we use language-\nspeciﬁc BPE with 40k merges for each language.\nWe then build vocabulary by taking the union of\nthe vocabulary of two languages and train a bilin-\ngual MLM (Default anchors). To remove anchor\npoints, we add a language preﬁx to each word in\nthe vocabulary before taking the union. Bilingual\nMLM (No anchors) trained with such data has no\nshared vocabulary across languages. However, it\nstill has a single softmax prediction layer shared\nacross languages and tied with input embeddings.\nAs Wu and Dredze (2019) suggest there may\nalso be correlation between cross-lingual perfor-\nmance and anchor points, we additionally increase\nanchor points by using a bilingual dictionary to\ncreate code switch data for training bilingual MLM\n(Extra anchors). For two languages, ℓ1 and ℓ2,\nwith bilingual dictionary entries dℓ1,ℓ2 , we add an-\nchors to the training data as follows. For each\ntraining word wℓ1 in the bilingual dictionary, we\neither leave it as is (70% of the time) or randomly\nreplace it with one of the possible translations from\nthe dictionary (30% of the time). We change at\nmost 15% of the words in a batch and sample word\ntranslations from PanLex (Kamholz et al., 2014)\nbilingual dictionaries, weighted according to their\ntranslation quality 1.\nThe second group of Tab. 1 shows cross-lingual\ntransfer performance under the three anchor point\nconditions. Anchor points have a clear effect on\nperformance and more anchor points help, espe-\ncially in the less closely related language pairs (e.g.\nEnglish-Chinese has a larger effect than English-\nFrench with over 3 points improvement on NER\nand XNLI). However, surprisingly, effective trans-\nfer is still possible with no anchor points. Com-\n1Although we only consider pairs of languages, this pro-\ncedure naturally scales to multiple languages, which could\nproduce larger gains in future work.\n6027\nparing no anchors and default anchors, the perfor-\nmance of XNLI and parsing drops only around 1\npoint while NER even improve 1 points averaging\nover three languages. Overall, these results show\nthat we have previously overestimated the contribu-\ntion of anchor points during multilingual pretrain-\ning. Concurrently, Karthikeyan et al. (2020) simi-\nlarly ﬁnd anchor points play minor role in learning\ncross-lingual representation.\n4.3 Parameter sharing\nGiven that anchor points are not required for trans-\nfer, a natural next question is the extent to which we\nneed to tie the parameters of the transformer layers.\nSharing the parameters of the top layer is neces-\nsary to provide shared inputs to the task-speciﬁc\nlayer. However, as seen in Figure 1, we can pro-\ngressively separate the bottom layers 1:3 and 1:6\nof the Transformers and/or the embedding layers\n(including positional embeddings) (Sep Emb; Sep\nL1-3; Sep L1-6; Sep Emb + L1-3; Sep Emb +\nL1-6). Since the prediction layer is tied with the\nembeddings layer, separating the embeddings layer\nalso introduces a language-speciﬁc softmax pre-\ndiction layer for the cloze task. Additionally, we\nonly sample random words within one language\nduring the MLM pretraining. During ﬁne-tuning\non the English training set, we freeze the language-\nspeciﬁc layers and only ﬁne-tune the shared layers.\nThe third group in Tab. 1 shows cross-lingual\ntransfer performance under different parameter\nsharing conditions with “Sep” denote which layers\nis notshared across languages. Sep Emb (effec-\ntively no anchor point) drops more than No anchors\nwith 3 points on XNLI and around 1 point on NER\nand parsing, suggesting have a cross-language soft-\nmax layer also helps to learn cross-lingual repre-\nsentations. Performance degrades as fewer layers\nare shared for all pairs, and again the less closely\nrelated language pairs lose the most. Most notably,\nthe cross-lingual transfer performance drops to ran-\ndom when separating embeddings and bottom 6\nlayers of the transformer. However, reasonably\nstrong levels of transfer are still possible without\ntying the bottom three layers. These trends suggest\nthat parameter sharing is the key ingredient that\nenables the learning of an effective cross-lingual\nrepresentation space, and having language-speciﬁc\ncapacity does not help learn a language-speciﬁc\nencoder for cross-lingual representation. Our hy-\npothesis is that the representations that the models\nlearn for different languages are similarly shaped\nand models can reduce their capacity budget by\naligning representations for text that has similar\nmeaning across languages.\n4.4 Language Similarity\nFinally, in contrast to many of the experiments\nabove, language similarity seems to be quite im-\nportant for effective transfer. Looking at Tab. 1\ncolumn by column in each task, we observe per-\nformance drops as language pairs become more\ndistantly related. Using extra anchor points helps\nto close the gap. However, the more complex tasks\nseem to have larger performance gaps and having\nlanguage-speciﬁc capacity does not seem to be the\nsolution. Future work could consider scaling the\nmodel with more data and cross-lingual signal to\nclose the performance gap.\n4.5 Conclusion\nSummarised by Figure 3, parameter sharing is the\nmost important factor. More anchor points help\nbut anchor points and shared softmax projection\nparameters are not necessary for effective cross-\nlingual transfer. Joint BPE and domain similarity\ncontribute a little in learning cross-lingual repre-\nsentation.\n5 Similarity of BERT Models\nTo better understand the robust transfer effects of\nthe last section, we show that independently trained\nmonolingual BERT models learn representations\nthat are similar across languages, much like the\nwidely observed similarities in word embedding\nspaces. In this section, we show that independent\nmonolingual BERT models produce highly similar\nrepresentations when evaluated at the word level\n(§5.1.1), contextual word-level (§5.1.2), and sen-\ntence level (§5.1.3) . We also plot the cross-lingual\nsimilarity of neural network activation with center\nkernel alignment (§5.2) at each layer. We consider\nﬁve languages: English, French, German, Russian,\nand Chinese.\n5.1 Aligning Monolingual BERTs\nTo measure similarity, we learn an orthogonal map-\nping using the Procrustes (Smith et al., 2017) ap-\nproach:\nW⋆ = argmin\nW∈Od(R)\n∥WX −Y ∥F = UV T\n6028\nwith UΣV T = SVD(Y XT ), where X and Y are\nrepresentation of two monolingual BERT models,\nsampled at different granularities as described be-\nlow. We apply iterative normalization onX and Y\nbefore learning the mapping (Zhang et al., 2019).\n5.1.1 Word-level alignment\nIn this section, we align both the non-contextual\nword representations from the embedding layers,\nand the contextual word representations from the\nhidden states of the Transformer at each layer.\nFor non-contextualized word embeddings, we\ndeﬁne X and Y as the word embedding layers of\nmonolingual BERT, which contain a single embed-\nding per word (type). Note that in this case we only\nkeep words containing only one subword. For con-\ntextualized word representations, we ﬁrst encode\n500k sentences in each language. At each layer,\nand for each word, we collect all contextualized\nrepresentations of a word in the 500k sentences\nand average them to get a single embedding. Since\nBERT operates at the subword level, for one word\nwe consider the average of all its subword embed-\ndings. Eventually, we get one word embedding\nper layer. We use the MUSE benchmark (Con-\nneau et al., 2017), a bilingual dictionary induction\ndataset for alignment supervision and evaluate the\nalignment on word translation retrieval. As a base-\nline, we use the ﬁrst 200k embeddings of fastText\n(Bojanowski et al., 2017) and learn the mapping\nusing the same procedure as §5.1. Note we use a\nsubset of 200k vocabulary of fastText, the same as\nBERT, to get a comparable number. We retrieve\nword translation using CSLS (Conneau et al., 2017)\nwith K=10.\nIn Figure 4, we report the alignment results un-\nder these two settings. Figure 4a shows that the\nsubword embeddings matrix of BERT, where each\nsubword is a standalone word, can easily be aligned\nwith an orthogonal mapping and obtain slightly\nbetter performance than the same subset of fast-\nText. Figure 4b shows embeddings matrix with\nthe average of all contextual embeddings of each\nword can also be aligned to obtain a decent qual-\nity bilingual dictionary, although underperforming\nfastText. We notice that using contextual repre-\nsentations from higher layers obtain better results\ncompared to lower layers.\n5.1.2 Contextual word-level alignment\nIn addition to aligning word representations, we\nalso align representations of two monolingual\nBERT models in contextual setting, and evaluate\nperformance on cross-lingual transfer for NER and\nparsing. We take the Transformer layers of each\nmonolingual model up to layer i, and learn a map-\nping W from layer i of the target model to layeri of\nthe source model. To create that mapping, we use\nthe same Procrustes approach but use a dictionary\nof parallel contextual words, obtained by running\nthe fastAlign (Dyer et al., 2013) model on the 10k\nXNLI parallel sentences.\nFor each downstream task, we learn task-speciﬁc\nlayers on top of i-th English layer: four Trans-\nformer layers and a task-speciﬁc layer. We learn\nthese on the training set, but keep the ﬁrst i pre-\ntrained layers freezed. After training these task-\nspeciﬁc parameters, we encode (say) a Chinese\nsentence with the ﬁrst i layers of the target Chinese\nBERT model, project the contextualized represen-\ntations back to the English space using the W we\nlearned, and then use the task-speciﬁc layers for\nNER and parsing.\nIn Figure 5, we vary i from the embedding layer\n(layer 0) to the last layer (layer 8) and present\nthe results of our approach on parsing and NER.\nWe also report results using the ﬁrst i layers of a\nbilingual MLM (biMLM). 2 We show that aligning\nmonolingual models (MLM align) obtain relatively\ngood performance even though they perform worse\nthan bilingual MLM, except for parsing on English-\nFrench. The results of monolingual alignment gen-\nerally shows that we can align contextual represen-\ntations of monolingual BERT models with a simple\nlinear mapping and use this approach for cross-\nlingual transfer. We also observe that the model\nobtains the highest transfer performance with the\nmiddle layer representation alignment, and not the\nlast layers. The performance gap between monolin-\ngual MLM alignment and bilingual MLM is higher\nin NER compared to parsing, suggesting the syntac-\ntic information needed for parsing might be easier\nto align with a simple mapping while entity infor-\nmation requires more explicit entity alignment.\n5.1.3 Sentence-level alignment\nIn this case, X and Y are obtained by average\npooling subword representation (excluding spe-\ncial token) of sentences at each layerof mono-\nlingual BERT. We use multi-way parallel sentences\nfrom XNLI for alignment supervision and Tatoeba\n(Schwenk et al., 2019) for evaluation.\n2In Appendix A, we also present the same alignment step\nwith biMLM but only observed improvement in parsing.\n6029\nen-fr en-de en-ru en-zh\n40\n50\n60\n70\n80P@1\nBERT\nfastText\n(a) Non-contextual word embeddings alignment\n0 2 4 6 8\nLayer\n30\n40\n50\n60\n70\n80P@1\npair\nen-fr\nen-de\nen-ru\nen-zh\nmodel\nBERT\nfastText (b) Contextual word embedding alignment\nFigure 4: Alignment of word-level representations from monolingual BERT models on subset of MUSE bench-\nmark. Figure 4a and Figure 4b are not comparable due to different embedding vocabularies.\n0 1 2 3 4 5 6 7 8\nLayer\n30\n40\n50\n60\n70\n80F1\nNER\n0 1 2 3 4 5 6 7 8\nLayer\n20\n30\n40\n50\n60\n70LAS\nParsing\npair\nen-fr\nen-ru\nen-zh\nmodel\nMLM align\nbiMLM\nFigure 5: Contextual representation alignment of different layers for zero-shot cross-lingual transfer.\nFigure 6 shows the sentence similarity search\nresults with nearest neighbor search and cosine\nsimilarity, evaluated by precision at 1, with four\nlanguage pairs. Here the best result is obtained\nat lower layers. The performance is surprisingly\ngood given we only use 10k parallel sentences to\nlearn the alignment without ﬁne-tuning at all. As a\nreference, the state-of-the-art performance is over\n95%, obtained by LASER (Artetxe and Schwenk,\n2019) trained with millions of parallel sentences.\n0 2 4 6 8\nLayer\n60\n70\n80\n90\n100Accuracy\npair\nen-fr\nen-de\nen-ru\nen-zh\nmodel\nBERT\nLASER\nFigure 6: Parallel sentence retrieval accuracy after Pro-\ncrustes alignment of monolingual BERT models.\n5.1.4 Conclusion\nThese ﬁndings demonstrate that both word-level,\ncontextual word-level, and sentence-level BERT\nrepresentations can be aligned with a simple orthog-\nonal mapping. Similar to the alignment of word\nembeddings (Mikolov et al., 2013), this shows that\nBERT models are similar across languages. This\nresult gives more intuition on why mere parameter\nsharing is sufﬁcient for multilingual representations\nto emerge in multilingual masked language models.\n5.2 Neural network similarity\nBased on the work of Kornblith et al. (2019), we\nexamine the centered kernel alignment (CKA), a\nneural network similarity index that improves upon\ncanonical correlation analysis (CCA), and use it\nto measure the similarity across both monolingual\nand bilingual masked language models. The linear\nCKA is both invariant to orthogonal transforma-\ntion and isotropic scaling, but are not invertible to\nany linear transform. The linear CKA similarity\nmeasure is deﬁned as follows:\nCKA(X, Y) = ∥Y T X∥2\nF\n(∥XT X∥F∥Y T Y ∥F),\n6030\nBilingual\nMonolingual\nRandom\nL0\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nAVER\n0.76 0.75 0.52\n0.75 0.77 0.6\n0.74 0.74 0.58\n0.75 0.71 0.58\n0.73 0.66 0.6\n0.69 0.58 0.52\n0.64 0.48 0.44\n0.48 0.24 0.32\n0.55 0.4 0.3\n0.68 0.59 0.5\nen-en'\nBilingual\nMonolingual\nRandom\n0.61 0.65 0.46\n0.74 0.71 0.55\n0.71 0.7 0.52\n0.73 0.7 0.53\n0.73 0.64 0.55\n0.72 0.59 0.48\n0.71 0.5 0.41\n0.67 0.34 0.31\n0.62 0.4 0.28\n0.69 0.58 0.46\nen-fr\nBilingual\nMonolingual\nRandom\n0.66 0.64 0.46\n0.76 0.7 0.54\n0.72 0.69 0.52\n0.73 0.69 0.54\n0.73 0.63 0.56\n0.74 0.6 0.49\n0.7 0.52 0.42\n0.6 0.39 0.31\n0.64 0.43 0.28\n0.7 0.59 0.46\nen-de\nBilingual\nMonolingual\nRandom\n0.56 0.56 0.42\n0.67 0.65 0.5\n0.64 0.63 0.47\n0.65 0.64 0.48\n0.65 0.61 0.5\n0.64 0.56 0.44\n0.63 0.5 0.37\n0.6 0.34 0.29\n0.5 0.39 0.26\n0.62 0.54 0.41\nen-ru\nBilingual\nMonolingual\nRandom\n0.56 0.6 0.44\n0.65 0.67 0.51\n0.61 0.65 0.49\n0.59 0.64 0.5\n0.58 0.6 0.52\n0.59 0.56 0.46\n0.57 0.51 0.39\n0.5 0.37 0.3\n0.51 0.4 0.27\n0.57 0.56 0.43\nen-zh\nFigure 7: CKA similarity of mean-pooled multi-way parallel sentence representation at each layers. Note en ′\ncorresponds to paraphrases of en obtained from back-translation (en-fr-en ′). Random encoder is only used by\nnon-Engligh sentences. L0 is the embeddings layers while L1 to L8 are the corresponding transformer layers. The\naverage row is the average of 9 (L0-L8) similarity measurements.\nwhere X and Y correspond respectively to the ma-\ntrix of the d-dimensional mean-pooled (excluding\nspecial token) subword representations at layer l of\nthe n parallel source and target sentences.\nIn Figure 7, we show the CKA similarity of\nmonolingual models, compared with bilingual mod-\nels and random encoders, of multi-way paral-\nlel sentences (Conneau et al., 2018) for ﬁve lan-\nguages pair: English to English′(obtained by back-\ntranslation from French), French, German, Russian,\nand Chinese. The monolingual en′is trained on the\nsame data as en but with different random seed\nand the bilingual en-en′is trained on English data\nbut with separate embeddings matrix as in §4.3.\nThe rest of the bilingual MLM is trained with the\nDefault setting. We only use random encoder for\nnon-English sentences.\nFigure 7 shows bilingual models have slightly\nhigher similarity compared to monolingual models\nwith random encoders serving as a lower bound.\nDespite the slightly lower similarity between mono-\nlingual models, it still explains the alignment per-\nformance in §5.1. Because the measurement is also\ninvariant to orthogonal mapping, the CKA simi-\nlarity is highly correlated with the sentence-level\nalignment performance in Figure 6 with over 0.9\nPearson correlation for all four languages pairs. For\nmonolingual and bilingual models, the ﬁrst few lay-\ners have the highest similarity, which explains why\nWu and Dredze (2019) ﬁnds freezing bottom layers\nof mBERT helps cross-lingual transfer. The similar-\nity gap between monolingual model and bilingual\nmodel decrease as the languages pair become more\ndistant. In other words, when languages are simi-\nlar, using the same model increase representation\nsimilarity. On the other hand, when languages are\ndissimilar, using the same model does not help rep-\nresentation similarity much. Future work could\nconsider how to best train multilingual models cov-\nering distantly related languages.\n6 Discussion\nIn this paper, we show that multilingual representa-\ntions can emerge from unsupervised multilingual\nmasked language models with only parameter shar-\ning of some Transformer layers. Even without any\nanchor points, the model can still learn to map rep-\nresentations coming from different languages in\na single shared embedding space. We also show\nthat isomorphic embedding spaces emerge from\nmonolingual masked language models in differ-\nent languages, similar to word2vec embedding\nspaces (Mikolov et al., 2013). By using a linear\nmapping, we are able to align the embedding layers\nand the contextual representations of Transform-\ners trained in different languages. We also use the\nCKA neural network similarity index to probe the\nsimilarity between BERT Models and show that\nthe early layers of the Transformers are more sim-\nilar across languages than the last layers. All of\nthese effects were stronger for more closely related\nlanguages, suggesting there is room for signiﬁcant\nimprovements on more distant language pairs.\n6031\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages\n3874–3884, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451–462,\nVancouver, Canada. Association for Computational\nLinguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yo-\ngatama. 2019. On the cross-lingual transferabil-\nity of monolingual representations. arXiv preprint\narXiv:1910.11856.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv ´e J´egou. 2017.\nWord translation without parallel data. arXiv\npreprint arXiv:1710.04087.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTimothy Dozat and Christopher D Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning. arXiv preprint arXiv:1611.01734.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameter-\nization of IBM model 2. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2485–2494,\nHong Kong, China. Association for Computational\nLinguistics.\nDavid Kamholz, Jonathan Pool, and Susan Colow-\nick. 2014. PanLex: Building a resource for pan-\nlingual lexical translation. In Proceedings of the\nNinth International Conference on Language Re-\nsources and Evaluation (LREC’14), pages 3145–\n3150, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nK Karthikeyan, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics Companion\nVolume Proceedings of the Demo and Poster Ses-\nsions, pages 177–180, Prague, Czech Republic. As-\nsociation for Computational Linguistics.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey Hinton. 2019. Similarity of neural net-\nwork representations revisited. International Con-\nference on Machine Learning.\nSneha Kudugunta, Ankur Bapna, Isaac Caswell, and\nOrhan Firat. 2019. Investigating multilingual NMT\nrepresentations at scale. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\n6032\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1565–1575, Hong Kong,\nChina. Association for Computational Linguistics.\nAarre Laakso and Garrison Cottrell. 2000. Content\nand cluster analysis: assessing representational sim-\nilarity in neural systems. Philosophical psychology,\n13(1):47–76.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5039–5049, Brussels, Belgium. Association\nfor Computational Linguistics.\nGina-Anne Levow. 2006. The third international Chi-\nnese language processing bakeoff: Word segmen-\ntation and named entity recognition. In Proceed-\nings of the Fifth SIGHAN Workshop on Chinese\nLanguage Processing, pages 108–117, Sydney, Aus-\ntralia. Association for Computational Linguistics.\nYixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and\nJohn E Hopcroft. 2016. Convergent learning: Do\ndifferent neural networks learn the same representa-\ntions? In Iclr.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nAri Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation. In Advances\nin Neural Information Processing Systems, pages\n5727–5736.\nPhoebe Mulcaire, Jungo Kasai, and Noah A. Smith.\n2019. Polyglot contextual representations improve\ncrosslingual transfer. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 3912–3918, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJoakim et al. Nivre. 2018. Universal dependencies 2.3.\nLINDAT/CLARIN digital library at the Institute of\nFormal and Applied Linguistics ( ´UFAL), Faculty of\nMathematics and Physics, Charles University.\nAitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor\nSoroa, and Eneko Agirre. 2019. Analyzing the lim-\nitations of cross-lingual word embedding mappings.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n4990–4995, Florence, Italy. Association for Compu-\ntational Linguistics.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nBarun Patra, Joel Ruben Antony Moniz, Sarthak Garg,\nMatthew R. Gormley, and Graham Neubig. 2019.\nBilingual lexicon induction with semi-supervision\nin non-isometric embedding spaces. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 184–193, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 157–163, Valencia,\nSpain. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. Svcca: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In Advances in Neu-\nral Information Processing Systems, pages 6076–\n6085.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of con-\ntextual word embeddings, with applications to zero-\nshot dependency parsing. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1599–1613, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\n6033\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzman. 2019. Wiki-\nmatrix: Mining 135m parallel sentences in 1620\nlanguage pairs from wikipedia. arXiv preprint\narXiv:1907.05791.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nSamuel L Smith, David HP Turban, Steven Hamblin,\nand Nils Y Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. International Conference on Learning\nRepresentations.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 778–\n788, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nWilson L Taylor. 1953. cloze procedure: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nLiwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue\nWu, Kun He, and John Hopcroft. 2018. Towards un-\nderstanding learning representations: To what extent\ndo different neural networks learn the same represen-\ntation. In Advances in Neural Information Process-\ning Systems, pages 9584–9593.\nYuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu,\nand Ting Liu. 2019. Cross-lingual BERT trans-\nformation for zero-shot dependency parsing. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5721–\n5727, Hong Kong, China. Association for Computa-\ntional Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017. Adversarial training for unsupervised\nbilingual lexicon induction. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1959–1970, Vancouver, Canada. Association\nfor Computational Linguistics.\nMozhi Zhang, Keyulu Xu, Ken-ichi Kawarabayashi,\nStefanie Jegelka, and Jordan Boyd-Graber. 2019.\nAre girls neko or sh ¯ojo? cross-lingual alignment of\nnon-isomorphic embeddings with iterative normal-\nization. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3180–3189, Florence, Italy. Association\nfor Computational Linguistics.\n6034\nA Contextual word-level alignment of\nbilingual MLM representation\n0 2 4 6 8\nLayer\n30\n40\n50\n60\n70\n80F1\nNER\npair\nen-fr\nen-ru\nen-zh\nmodel\nMLM align\nbiMLM\nbiMLM align\n0 2 4 6 8\nLayer\n20\n30\n40\n50\n60\n70LAS\nParsing\npair\nen-fr\nen-ru\nen-zh\nmodel\nMLM align\nbiMLM\nbiMLM align\nFigure 8: Contextual representation alignment of differ-\nent layers for zero-shot cross-lingual transfer.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8139711022377014
    },
    {
      "name": "Natural language processing",
      "score": 0.7158639430999756
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6063677668571472
    },
    {
      "name": "Vocabulary",
      "score": 0.5969210863113403
    },
    {
      "name": "Language model",
      "score": 0.5400183796882629
    },
    {
      "name": "Embedding",
      "score": 0.5285769104957581
    },
    {
      "name": "Word (group theory)",
      "score": 0.4965680241584778
    },
    {
      "name": "Encoder",
      "score": 0.48133429884910583
    },
    {
      "name": "Process (computing)",
      "score": 0.47607558965682983
    },
    {
      "name": "Transfer (computing)",
      "score": 0.4690896272659302
    },
    {
      "name": "Linguistics",
      "score": 0.39584988355636597
    },
    {
      "name": "Programming language",
      "score": 0.1210930347442627
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ]
}