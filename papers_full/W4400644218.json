{
    "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
    "url": "https://openalex.org/W4400644218",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4322523682",
            "name": "Acharya Kamal",
            "affiliations": [
                "University of Maryland, Baltimore County"
            ]
        },
        {
            "id": "https://openalex.org/A4221759101",
            "name": "Velasquez Alvaro",
            "affiliations": [
                "University of Colorado Boulder"
            ]
        },
        {
            "id": null,
            "name": "Song, Houbing Herbert",
            "affiliations": [
                "University of Maryland, Baltimore County"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3207166518",
        "https://openalex.org/W4385573878",
        "https://openalex.org/W6855510783",
        "https://openalex.org/W6851775633",
        "https://openalex.org/W4382246105",
        "https://openalex.org/W6854474899",
        "https://openalex.org/W4391136507",
        "https://openalex.org/W4385572142",
        "https://openalex.org/W6849075151",
        "https://openalex.org/W6810270109",
        "https://openalex.org/W6802857829",
        "https://openalex.org/W4385571689",
        "https://openalex.org/W4390490761",
        "https://openalex.org/W6854515434",
        "https://openalex.org/W6856051742",
        "https://openalex.org/W4404918643",
        "https://openalex.org/W6856104514",
        "https://openalex.org/W4392414327",
        "https://openalex.org/W6856223801",
        "https://openalex.org/W4391094120",
        "https://openalex.org/W6857785731",
        "https://openalex.org/W6856289622",
        "https://openalex.org/W4307979480",
        "https://openalex.org/W2079145130",
        "https://openalex.org/W4236521339",
        "https://openalex.org/W6631553122",
        "https://openalex.org/W6655539933",
        "https://openalex.org/W2035301451",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W3209732473",
        "https://openalex.org/W2294370754",
        "https://openalex.org/W2783538964",
        "https://openalex.org/W2233116163",
        "https://openalex.org/W6639703010",
        "https://openalex.org/W6686067075",
        "https://openalex.org/W2884022415",
        "https://openalex.org/W2754084392",
        "https://openalex.org/W6684563725",
        "https://openalex.org/W6745499037",
        "https://openalex.org/W6638523607",
        "https://openalex.org/W6636510571",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W6637551013",
        "https://openalex.org/W6730179637",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2739879705",
        "https://openalex.org/W6748082341",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W2891177506",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6767101625",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W2982157312",
        "https://openalex.org/W6769906912",
        "https://openalex.org/W6768001205",
        "https://openalex.org/W2620998106",
        "https://openalex.org/W2997006708",
        "https://openalex.org/W2986349107",
        "https://openalex.org/W2964082701",
        "https://openalex.org/W6779702771",
        "https://openalex.org/W6771114979",
        "https://openalex.org/W2963514416",
        "https://openalex.org/W6756054015",
        "https://openalex.org/W6779744012",
        "https://openalex.org/W3135939397",
        "https://openalex.org/W6780805062",
        "https://openalex.org/W6805239564",
        "https://openalex.org/W4401042870",
        "https://openalex.org/W6772383348",
        "https://openalex.org/W6810220367",
        "https://openalex.org/W6846955524",
        "https://openalex.org/W4205737716",
        "https://openalex.org/W6811297048",
        "https://openalex.org/W6856017260",
        "https://openalex.org/W4389519291",
        "https://openalex.org/W6861084137",
        "https://openalex.org/W6784577980",
        "https://openalex.org/W6800875267",
        "https://openalex.org/W6811129797",
        "https://openalex.org/W6839198299",
        "https://openalex.org/W6845921028",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W6847076894",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6798182279",
        "https://openalex.org/W6810932996",
        "https://openalex.org/W6810738896",
        "https://openalex.org/W6803958908",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W6843975101",
        "https://openalex.org/W6849941170",
        "https://openalex.org/W6811340617",
        "https://openalex.org/W6847306236",
        "https://openalex.org/W6850625674",
        "https://openalex.org/W6802669662",
        "https://openalex.org/W4308760226",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W6766481131",
        "https://openalex.org/W6798398338",
        "https://openalex.org/W6811035422",
        "https://openalex.org/W3034368386",
        "https://openalex.org/W6743188669",
        "https://openalex.org/W2964062189",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W3174102142",
        "https://openalex.org/W6740493225",
        "https://openalex.org/W2887783173",
        "https://openalex.org/W2981819252",
        "https://openalex.org/W2964111476",
        "https://openalex.org/W2886756692",
        "https://openalex.org/W2798599891",
        "https://openalex.org/W6765492279",
        "https://openalex.org/W3035163969",
        "https://openalex.org/W2973136764",
        "https://openalex.org/W6737947904",
        "https://openalex.org/W6769311223",
        "https://openalex.org/W6847118041",
        "https://openalex.org/W6846002521",
        "https://openalex.org/W6846326645",
        "https://openalex.org/W6838691073",
        "https://openalex.org/W6793423792",
        "https://openalex.org/W6810583692",
        "https://openalex.org/W4205450747",
        "https://openalex.org/W4205537036",
        "https://openalex.org/W3202712981",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W6786251224",
        "https://openalex.org/W6794025307",
        "https://openalex.org/W3154575616",
        "https://openalex.org/W6803940137",
        "https://openalex.org/W6802629465",
        "https://openalex.org/W6802505033",
        "https://openalex.org/W3101204082",
        "https://openalex.org/W3034830866",
        "https://openalex.org/W3105516974",
        "https://openalex.org/W3173805051",
        "https://openalex.org/W6783948045",
        "https://openalex.org/W4283793025",
        "https://openalex.org/W3201662444",
        "https://openalex.org/W6758604235",
        "https://openalex.org/W6802078997",
        "https://openalex.org/W3087922520",
        "https://openalex.org/W2962833140",
        "https://openalex.org/W6764072591",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W3199814529",
        "https://openalex.org/W4205460703",
        "https://openalex.org/W6839328737",
        "https://openalex.org/W3034475796",
        "https://openalex.org/W3157894658",
        "https://openalex.org/W3174785103",
        "https://openalex.org/W3186988551",
        "https://openalex.org/W6777182126",
        "https://openalex.org/W3034649382",
        "https://openalex.org/W4296634084",
        "https://openalex.org/W2963233086",
        "https://openalex.org/W3173566921",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W4389523683",
        "https://openalex.org/W6848060374",
        "https://openalex.org/W6848757079",
        "https://openalex.org/W6859604231",
        "https://openalex.org/W4385572634",
        "https://openalex.org/W6852043138",
        "https://openalex.org/W6845576796",
        "https://openalex.org/W6846983559",
        "https://openalex.org/W4385571011",
        "https://openalex.org/W6853381761",
        "https://openalex.org/W6858885809",
        "https://openalex.org/W4390729056",
        "https://openalex.org/W4386432078",
        "https://openalex.org/W6853444283",
        "https://openalex.org/W6854084413",
        "https://openalex.org/W4389520259",
        "https://openalex.org/W4381930847",
        "https://openalex.org/W4387847108",
        "https://openalex.org/W6855991174",
        "https://openalex.org/W6860676074",
        "https://openalex.org/W6856696905",
        "https://openalex.org/W6857055897",
        "https://openalex.org/W6852449896",
        "https://openalex.org/W2731516819",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W3206959854",
        "https://openalex.org/W3203226129",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4387323141",
        "https://openalex.org/W2097333193",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2976135203",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W4200634402",
        "https://openalex.org/W4375819112",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3107969673",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4389524426",
        "https://openalex.org/W4386231419",
        "https://openalex.org/W4228998172",
        "https://openalex.org/W4386501849",
        "https://openalex.org/W3090866633",
        "https://openalex.org/W4388963454",
        "https://openalex.org/W4388890950",
        "https://openalex.org/W4288376504",
        "https://openalex.org/W4287391163",
        "https://openalex.org/W4300756893",
        "https://openalex.org/W4301194718",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W1525482321",
        "https://openalex.org/W1690739335",
        "https://openalex.org/W2962862931",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W2750784772",
        "https://openalex.org/W2954901156",
        "https://openalex.org/W4388685466",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4297779241",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W4223974161",
        "https://openalex.org/W4385067052",
        "https://openalex.org/W4288093001",
        "https://openalex.org/W4383605161",
        "https://openalex.org/W4382618722",
        "https://openalex.org/W4378465439",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W4298857601",
        "https://openalex.org/W4379539933",
        "https://openalex.org/W4386081793",
        "https://openalex.org/W4389599434",
        "https://openalex.org/W4286962702",
        "https://openalex.org/W3148330722",
        "https://openalex.org/W4309088836",
        "https://openalex.org/W2794557536",
        "https://openalex.org/W4288088047",
        "https://openalex.org/W4390962544",
        "https://openalex.org/W4226243662",
        "https://openalex.org/W4312205996",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W4226364033",
        "https://openalex.org/W2021021293",
        "https://openalex.org/W4286897388",
        "https://openalex.org/W2991786320",
        "https://openalex.org/W4391047180",
        "https://openalex.org/W3202099651",
        "https://openalex.org/W4385262268",
        "https://openalex.org/W4298181573",
        "https://openalex.org/W2561238782",
        "https://openalex.org/W4281481109",
        "https://openalex.org/W3040573126",
        "https://openalex.org/W4385889719",
        "https://openalex.org/W3212496002",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4385774833",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4303648559",
        "https://openalex.org/W2995607862",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4303443398",
        "https://openalex.org/W4386044080",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W4377864835",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3023690688",
        "https://openalex.org/W2766839578",
        "https://openalex.org/W4281657280",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W2950248853",
        "https://openalex.org/W4386269388",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4367000491",
        "https://openalex.org/W4385890089",
        "https://openalex.org/W3182414949",
        "https://openalex.org/W4377297670",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2969958763",
        "https://openalex.org/W1902934009",
        "https://openalex.org/W2949964376",
        "https://openalex.org/W4309212061",
        "https://openalex.org/W3216037316",
        "https://openalex.org/W2963534679",
        "https://openalex.org/W4385571309",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W4304697829",
        "https://openalex.org/W4308672130",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4318719086",
        "https://openalex.org/W4386080611",
        "https://openalex.org/W4387427818"
    ],
    "abstract": "This survey paper delves into the emerging and critical area of symbolic\\nknowledge distillation in Large Language Models (LLMs). As LLMs like Generative\\nPre-trained Transformer-3 (GPT-3) and Bidirectional Encoder Representations\\nfrom Transformers (BERT) continue to expand in scale and complexity, the\\nchallenge of effectively harnessing their extensive knowledge becomes\\nparamount. This survey concentrates on the process of distilling the intricate,\\noften implicit knowledge contained within these models into a more symbolic,\\nexplicit form. This transformation is crucial for enhancing the\\ninterpretability, efficiency, and applicability of LLMs. We categorize the\\nexisting research based on methodologies and applications, focusing on how\\nsymbolic knowledge distillation can be used to improve the transparency and\\nfunctionality of smaller, more efficient Artificial Intelligence (AI) models.\\nThe survey discusses the core challenges, including maintaining the depth of\\nknowledge in a comprehensible format, and explores the various approaches and\\ntechniques that have been developed in this field. We identify gaps in current\\nresearch and potential opportunities for future advancements. This survey aims\\nto provide a comprehensive overview of symbolic knowledge distillation in LLMs,\\nspotlighting its significance in the progression towards more accessible and\\nefficient AI systems.\\n",
    "full_text": "IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 1\nA Survey on Symbolic Knowledge Distillation of\nLarge Language Models\nKamal Acharya , Graduate Student Member, IEEE , Alvaro Velasquez , Member, IEEE and Houbing Herbert\nSong , Fellow, IEEE\nAbstract—This survey paper delves into the emerging and\ncritical area of symbolic knowledge distillation in Large Lan-\nguage Models (LLMs). As LLMs like Generative Pre-trained\nTransformer-3 (GPT-3) and Bidirectional Encoder Representa-\ntions from Transformers (BERT) continue to expand in scale and\ncomplexity, the challenge of effectively harnessing their extensive\nknowledge becomes paramount. This survey concentrates on\nthe process of distilling the intricate, often implicit knowledge\ncontained within these models into a more symbolic, explicit form.\nThis transformation is crucial for enhancing the interpretability,\nefficiency, and applicability of LLMs. We categorize the existing\nresearch based on methodologies and applications, focusing on\nhow symbolic knowledge distillation can be used to improve\nthe transparency and functionality of smaller, more efficient\nArtificial Intelligence (AI) models. The survey discusses the core\nchallenges, including maintaining the depth of knowledge in a\ncomprehensible format, and explores the various approaches and\ntechniques that have been developed in this field. We identify\ngaps in current research and potential opportunities for future\nadvancements. This survey aims to provide a comprehensive\noverview of symbolic knowledge distillation in LLMs, spotlighting\nits significance in the progression towards more accessible and\nefficient AI systems.\nImpact Statement—There is burgeoning interest in the po-\ntential of symbolic knowledge to enhance the interpretability,\nefficiency, and application scope of LLMs, transforming them\ninto more robust, understandable, and versatile tools. Despite\nthe recognition of its importance, there remains a notable\ndearth of comprehensive research that thoroughly examines and\nevaluates the process and implications of this integration. Existing\nliterature predominantly focuses on either the advancements in\nLLMs or content of the knowledge in the LLMs , with less\nemphasis on the symbolic knowledge distillation of LLMs. This\nsurvey aims to fill this critical gap by offering an extensive review\nof the current state of symbolic knowledge disitllation in LLMs\nby highlighting the methodologies, challenges, and advancements\nin this field.\nIndex Terms—Large Language Models, Symbolic Knowledge,\nSymbolic Knowledge Distillation\nI. I NTRODUCTION\nL\nARGE Language Models (LLMs) are a prominent topic\nin Artificial Intelligence(AI), with significant break-\nthroughs occurring frequently. Trained on extensive data sets\nManuscript received January 6, 2024. This work was supported in part by\nthe U.S. National Science Foundation under Grant No. 2309760 and Grant\nNo. 2317117.\nK. Acharya and H. Song are with the Security and Optimization for Net-\nworked Globe Laboratory (SONG Lab), Department of Information Systems,\nUniversity of Maryland, Baltimore County, Baltimore, MD 21250 USA (e-\nmail: kamala2@umbc.edu; h.song@ieee.org).\nA. Velasquez is with the Department of Computer Science, University of\nColorado, Boulder, CO 80309 USA (e-mail: alvaro.velasquez@colorado.edu).\nincluding websites, research papers, and books, LLMs encap-\nsulate knowledge within their numerous parameters. They can\nserve as knowledge bases[1], from which information can be\nextracted and formatted for various purposes, such as fine-\ntuning other models for specific tasks[2], validating actions[3],\nor generating larger and more accurate datasets[4]. However,\nthe knowledge embedded in LLMs is not immediately acces-\nsible and requires careful extraction and efficient utilization to\nyield effective results.\nThe knowledge within LLMs, stored in the weights of\ntheir parameters, can be converted into a more interpretable\nsymbolic form through the process of symbolic knowledge\ndistillation. The core challenge here lies in translating the\nimplicit, distributed knowledge encoded in the neural net-\nworks of LLMs into explicit, symbolic representations. This\ntransformation is essential for several reasons: to improve the\ntransparency and interpretability of the models, to facilitate\nknowledge transfer to smaller, more efficient models, and to\nenable more robust and explainable AI systems. By converting\nthe knowledge into symbolic form, it becomes possible to\nunderstand the reasoning behind the model’s decisions. This is\ncrucial for applications where understanding the ’why’ behind\npredictions or recommendations is as important as the out-\ncomes themselves. The process is fraught with complexities,\nincluding preserving the nuance and depth of the learned\nknowledge while making it comprehensible and utilizable in\na symbolic format.\nIn this paper, we introduce a detailed framework dedicated\nto symbolic knowledge distillation of LLMs, initiating our dis-\ncussion with a historical overview of symbolic knowledge dis-\ntillation and its evolutionary path to its current state. Following\nthis, we delve into an analysis of various traditional knowl-\nedge distillation methods and their comparison with symbolic\nknowledge distillation approaches. We further explore LLM\narchitectures, including their training and fine-tuning mecha-\nnisms. We classify symbolic knowledge distillation techniques\ninto three distinct categories: Direct, Multilevel, and Dis-\ntillation via Reinforcement Learning. Additionally, we have\ncompiled research papers focused on symbolic knowledge, as\nwell as those specifically addressing symbolic knowledge dis-\ntillation of LLMs. Our survey provides a thorough examination\nof the latest developments in symbolic knowledge distillation\nof LLMs, highlighting the methodologies, challenges, and\nprogress in the field, thereby offering valuable insights for the\nresearch community interested in further exploration of this\ndomain.\nThe rapid expansion of LLMs has led to the production\narXiv:2408.10210v1  [cs.CL]  12 Jul 2024\n2 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\nof numerous survey papers. All the previous survey papers\non LLMs cover different aspects except for the symbolic\nknowledge. Further exploring we find that no survey paper has\nbeen published related to the symbolic knowledge distillation.\nThe focus areas of existing survey papers on LLMs include:\n• Comprehensive overviews of LLMs[5], [6], [7]\n• Evaluation of LLMs[8]\n• Code generation[9]\n• LLMs in education[10]\n• LLM as Knowledge Base[11], [12]\n• Reasoning Knowledge in LLMs[13]\n• Explainability in LLMs[14]\n• Aligning LLMs with human[15]\n• Instruction tuning for LLM[16]\n• Model Compression in LLM[17]\n• Trustworthiness evaluation of LLM[18]\n• LLM for software engineering[19]\n• Hallucination in LLM[20]\n• Multimodal LLM[21]\n• LLMs for Robotics[22]\n• LLMs for Information Retrieval[23]\nOur work stands in contrast to existing approaches in sev-\neral key aspects. While traditional methods primarily focus\non either the performance enhancement of smaller models\nor the interpretability aspect of knowledge distillation, our\nframework synergizes these objectives.\nThe remainder of this paper is structured as follows: Sec-\ntion II reviews the milestones in knowledge distillation and\nLLM, establishing the context and background for our work.\nSection III details the preliminaries about symbolic knowledge\ndistillation and LLM, followed by Section IV , which presents\na thorough process of symbolic knowledge distillation in\nLLM. Section V discusses the related research work that has\nbeen carried out. In Section VI, we discuss opportunities\nthat have emerged from Symbolic Knowledge Distillation.\nSection VII is devoted to the challenges of implementing\nproposed Symbolic knowledge distillation applications. We\nidentify the obstacles and challenges that may arise. Section\nVIII highlights the Lesson Learned and Key Takeaways and\nfinally, in Section IX, we offer concluding remarks on our\nsurvey paper.\nII. M ILESTONES IN KNOWLEDGE DISTILLATION AND\nLARGE LANGUAGE MODELS\nOver the last seven decades, language technology has ad-\nvanced significantly. The Turing Test[24], conducted in 1950,\nwas one of the earliest milestones in this field, which laid\nthe foundation for the concept that machines can perform at\nthe level of humans and demonstrate the intelligence. In the\nsame year Shannon used concept of entropy and provided the\nway of prediction of the next letter when the preceding text is\nknown[25]. In 1964, ELIZA[26] was introduced as a Natural\nLanguage Processing (NLP) computer program which was de-\nsigned to mimic the conversational style of a psychotherapist.\nSHRDLU[27], introduced in 1968, was an early example of\nan interactive natural language understanding system which\ncan understand and respond to natural language commands\nrelated to a simplified world of objects. Following year was the\ndominance of the Statistical Language Model(SLM). Notable\nworks that lead the way were \"Introduction of Stochastic\nApproach for Parsing\"[28] in 1986 and \"Statistical Approach\nto machine translation\"[29] in 1990. Due to the problem like\nBrittleness Across Domains, False Independence Assumption\nand Shannon-Style Experiments, there was downfall of the\nSLMs[30].\nWith the introduction of Long Short-Term\nMemory(LSTM)[31] in 1997, we entered into the era\nof Neural Language Model(NLM). These models helped in\nlanguage processing by capturing the long term dependencies\nand successfully handling the vanishing gradients. In 2001,\nthe first neural language model was introduced which can be\ntrained using Stochastic Gradient Descent(SGD) algorithm\nand proved to be computationally efficient and scalable to\nlarger dataset.[32]. Neural Networks not only increased in\nscope and functionality but also in terms of the size[33]. The\nconcept of model compression[34] was introduced in 2006.\nModel compression and acceleration techniques was divided\ninto four different approaches[35]: parameter pruning and\nsharing[36][37][38][39][40], low-rank factorization[41][42],\ntransferred/compact convolutional layers[43] and knowledge\ndistillation[44].\nIn 2011, IBM Watson made significant strides in lan-\nguage processing by winning a Jeopardy game against human\ncompetitors[45]. Two years later, in 2013, the Word2Vec\nalgorithm[46] was introduced, which enabled computers to\nunderstand the context of a word and its relationship with other\nwords using dense vector representation where similar words\nare located close to each other. In 2014, seq2seq[47] was\nintroduced which used encoder to represent variable length\ninput sequence into fixed length vector and decoder to generate\noutput sequence. In the same year, Global Vectors for Word\nRepresentation(GloVe)[48] was introduced, which used co-\noccurance matrix to capture relationship between the words\nin corpus and was successful in capturing the local and global\ncontext informaiton. Knowledge distillation is a model com-\npression technique introduced in 2015 that transfers knowledge\nfrom a high-capacity teacher model to a more compact student\nmodel. Later in that year FitNets[49] was introduced that add\nan additional term along with the knowledge distillation loss.\nIn 2016, study[50] instead of utilizing representations from\na specific point in the network, employed attention maps as\nhints, comparing the mean squared error (MSE) between the\nattention maps of the student and teacher models. In same\nyear, SQuAD (Standford Question Answering Dataset)[51]\nwas introduced, which facilitated the development of question-\nanswering systems by being benchmark dataset for evaluating\nmachine reading comprehension.\nIn 2017, the Transformer[52] model was introduced, which\nenabled the development of advanced language models that\ncan learn relationships between words in a sentence more ef-\nficiently by using the concept of self-attention. In the following\nyear, 2017 [53] employed a similar approach. However, instead\nof utilizing representations or attention maps, they provided\nhints by using Gram matrices. In 2018, a supplementary mod-\nule called the paraphraser[54] is incorporated into the model.\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 3\nFig. 1. Milestones in history of LLM and Knowledge Distillation\nIn same year, ELMo (Embedding from Language Model)[55],\ncontext dependent representation of word was introduced\nwhich uses different embeddings for same word in different\ncontext. Universal Sentence Encoder[56] was also introduced\nin same year, which further enhanced language processing by\nintroducing embeddings for sentence representations and can\nhandle multiple languages.\nGeneral Language Understanding Evaluation(GLUE)[57],\na benchmark to evaluate the performance of NLP mod-\nels on a range of language understanding tasks, became a\nstandard evaluation framework for comparing different lan-\nguage models. Bidirectional Encoder Representations from\nTransformers(BERT)[58] and Generative Pre-Training-1(GPT-\n1)[59] were introduced in the same year, 2018 which begin the\nera of Pre-trained Language Model(PLM). In 2019, GPT-2[60]\nbecame the first language model to touch a billion scale of\nparameters. Later that year, T5[61] became the first language\nmodel to touch the 10 billion parameter scale. According to\n[62] published in 2019, the current approach of extracting hints\nmay not be optimal due to the loss of information caused\nby the ReLU transformation. To address this, they introduced\na modified activation function called marginReLU. In [63]\npublished in 2020, the student model learns from the inter-\nmediate representations of the teacher model by employing a\ncontrastive loss over these representations. As like the way hu-\nman way of learning, knowledge distillation was applied in the\nmodel; self-learning[64], mutual learning[65], teacher student\nlearning[44], teacher assistant[66] and continual learning[67].\nMoreover, the application of knowledge distillation extends\nbeyond transferring knowledge between models. It can also\nbe utilized in various other tasks, including adversarial attacks\n[68], data augmentation [69][70], data privacy and security\n[71], as well as dataset distillation [72][73]. Between 2010 and\n2020, the domain of transfer learning experienced significant\nexpansion, with numerous transfer learning models achieving\nstate-of-the-art results across various disciplines[74].\nGoogle Shard (GShard)[75], introduced in 2020, became the\nfirst language model to touch the 100 billion parameter scale.\nAnd in 2021, the Generalist Language Model (GLaM)[76]\nbecame the first language model to touch the trillion pa-\nrameter scale. Concept of symbolic knowledge distillation[2]\nwas introduced in the same year which is a technique for\ntraining smaller models using larger models as teachers and\ninvolves distilling knowledge symbolically. Since then sym-\nbolic knowledge distillation has been used in various areas\nsuch as reference free sentence summarization[3], compara-\ntive knowledge acquisition[77]. The scaling laws for neural\nlanguage models[78], reveal that model performance improves\npredictably with increases in model size, dataset size, and com-\nputational resources, following a power-law relationship. This\nmeans that larger models are significantly more efficient in\nlearning from data. In 2022 and 2023, this trend persisted, with\nvarious industry leaders introducing new large-scale language\nmodels that leveraged these principles to achieve enhanced\nperformance, demonstrating the continued advancement and\nefficacy of scaling up model size and computational power\nin the development of language models. Major technology\ncompanies are investing heavily in developing their own LLMs\nbecause they recognize the immense potential of these systems\nto revolutionize various industries, such as healthcare, finance,\nand customer service. Also, LLMs can help these companies\nmaintain their position as leaders in the field of AI and keep up\nwith competitors. Given the swift advancements in this field,\nthere is a pressing need to steer AI towards paths that prioritize\nsafety and responsibility 1.\nThe study[79] concludes that for compute-optimal training,\nboth the model size and the number of training tokens should\nbe scaled equally; specifically, each doubling of the model\nsize should be accompanied by a doubling of the number of\ntraining tokens. Conversely, study[80] suggest that the supply\nof high-quality language data will likely be depleted by 2026.\nIn contrast, low-quality language data and image data are pro-\njected to be exhausted between 2030 and 2050 for low-quality\nlanguage data, and between 2030 and 2060 for image data.\nThe current trajectory of rapidly increasing the parameters of\nLLMs, which depend on vast datasets, may decelerate unless\nthere are significant improvements in data efficiency or new\ndata sources are discovered. These findings have influenced the\ndevelopment of next-generation LLMs towards models capable\nof generating their own training data for self-improvement.\nFurthermore, LLMs will need to incorporate self-fact-checking\ncapabilities. These scenarios underscore the importance of\nsymbolic knowledge distillation and suggest a potential shift\nof LLMs towards this approach.\nIt has been utilized for labeling[81][82], where the teacher\nmodel generates outputs based on the provided input, and for\nexpansion[83][84], where the teacher model produces samples\nakin to given demonstrations through in-context learning. For\ndata generation[85] which involves synthesizing data accord-\ning to specific meta-information, such as a topic or entity,\nfeedback[86] which involves providing guidance on the stu-\ndent’s outputs, encompassing preferences, corrections, and ex-\npansions of challenging samples. Finally, for self-checking[87]\n1https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/3\n0/executive-order-on-the-safe-secure-and-trustworthy-development-and-use\n-of-artificial-intelligence/(last accessed on: [28/02/2024])\n4 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\nwhich entails the student model generating outputs, which are\nsubsequently filtered for high quality or self-evaluated by the\nstudent model.\nIII. B ACKGROUND AND PRELIMINARIES\nFor understanding the process of symbolic knowledge distil-\nlation of LLMs, we need to dive deeper into the two different\ntechnical theory of knowledge distillation followed by LLMs.\nFollowing sub-section will focus on that part.\nA. Knowledge Distillation\nKnowledge distillation is a technique used to transfer\nknowledge from a larger, more complex model (teacher) to\na smaller, simpler model (student) with the goal of retaining\nmuch of the teacher model’s performance[117]. This process\nis crucial in scenarios where computational resources are\nlimited or where deployment requires lightweight models.\nThere are various types of traditional knowledge distillation\ntechniques: response-based, feature-based and relation-based\nand one modern symbolic knowledge distillation, each with\nits unique approach and area of application:\n1) Response-based Knowledge Distillation: Response-\nbased knowledge distillation involves transferring knowledge\nfrom the teacher model’s final output layer to the student\nmodel, aiming to mimic the teacher’s final predictions. This\napproach is straightforward and has proven effective across\nvarious tasks, employing a loss function based on the diver-\ngence between the teacher’s and student’s logits. It’s widely\napplied in model compression and has been adapted for\ndifferent types of model predictions, including object detection\nand human pose estimation, where the teacher’s output may in-\nclude additional information like bounding box offsets[118] or\nheatmaps for landmarks[119]. A key application of response-\nbased knowledge distillation is in image classification[44],\nwhere \"soft targets\" – the probabilities assigned to each class\nby the teacher model – play a crucial role. These probabilities\nare adjusted using a temperature factor to control the soft-\nness of the targets, allowing the transfer of knowledge from\nthe teacher to the student. The distillation process typically\nemploys the Kullback-Leibler divergence loss to optimize\nthe similarity between the teacher’s and student’s probability\ndistributions.\nThis method is praised for its simplicity and effectiveness,\nparticularly in leveraging knowledge for training. However,\nits reliance on the final layer’s output means it may not\nfully utilize intermediate-level supervision from the teacher,\nan aspect crucial for representation learning in deep neural\nnetworks.\n2) Feature-based Knowledge Distillation: Feature-based\nknowledge distillation taps into the strength of deep neural\nnetworks to learn hierarchical feature representations, a pro-\ncess central to representation learning[120]. Unlike response-\nbased knowledge distillation, which focuses on the outputs\nof the last layer, feature-based distillation utilizes the outputs\nfrom intermediate layers, or feature maps, to guide the student\nmodel. This approach is particularly beneficial for training\nmodels that are both narrower and deeper, as it provides a\nricher set of training signals.\nThe concept was first introduced with Fitnets[49], aiming\nto improve student model training by matching feature acti-\nvations between the teacher and student directly. Following\nthis, several methodologies have been developed to facili-\ntate this matching process, either directly or indirectly[121].\nNotable contributions include the derivation of \"attention\nmaps\" to express the use of neuron selectivity transfer[122],\nmatching probability distributions in feature space[123], and\nintroducing \"factors\" for more interpretable intermediate\nrepresentations[54]. Techniques like route constrained hint\nlearning[124] and the use of activation boundaries[125] have\nbeen proposed to minimize the performance gap between\nteacher and student models, alongside innovative strategies\nlike cross-layer knowledge distillation[121] which adaptively\nmatches teacher and student layers.\nDespite the effectiveness of feature-based knowledge trans-\nfer in enriching the student model’s learning, challenges re-\nmain in selecting appropriate layers for hints and guidance due\nto the size discrepancies between teacher and student models.\nThis necessitates further exploration into how best to match\nthe feature representations between teacher and student models\neffectively.\n3) Relation-based Knowledge Distillation: Relation-based\nknowledge distillation goes beyond the scope of response-\nbased and feature-based methods by examining the relation-\nships between different layers or data samples within the\nteacher model. This approach delves into the dynamics be-\ntween feature maps, layers, and even the relationships between\ndifferent teachers or data samples, offering a more nuanced\nform of knowledge transfer.\nFlow of solution process (FSP)[53] utilizes the Gram matrix\nbetween two layers to encapsulate the relationships between\npairs of feature maps through inner product calculations.\nKnowledge distillation via singular value decomposition[126]\ndistill essential information from these relationships. [127]\nexplored multi-teacher scenarios by constructing graphs based\non logits and features from each teacher, modeling their\nimportance and relationships. [128] proposed a multi-head\ngraph-based distillation technique that leverages intra-data\nrelations between feature maps through a multi-head attention\nnetwork. [129] focused on pairwise hint information, allowing\nthe student model to mimic mutual information flows from\npairs of hint layers in the teacher model.\nThe distillation loss in relation-based knowledge distilla-\ntion is formulated based on the similarity and correlation\nfunctions between the feature representations of teacher and\nstudent models, aiming to capture and transfer the intricate\nrelationships present in the teacher’s architecture. Relation-\nbased knowledge can also encompass structured knowledge of\ndata, privileged information about input features, and various\nother categories, each represented by different loss functions\nlike Earth Mover distance, Huber loss, Angle-wise loss, and\nFrobenius norm. While recent advancements have introduced\nseveral types of relation-based knowledge, the challenge re-\nmains in effectively modeling the relational information from\nfeature maps or data samples for knowledge transfer. This area\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 5\nTABLE I\nTECHNICAL COMPANIES WITH THEIR LLM\nCompanies LLM Year Parameters(in billions) Corpus Size\nGoogle\nT5[61] 2019 11 1 trillion tokens\nGShard[75] 2020 600 1 trillion tokens\nmT5[88] 2021 13 1 trillion tokens\nGLaM[76] 2021 1200 1.6 trillion tokens\nFLAN[89] 2021 137 Not Available\nLaMDA[90] 2022 137 1.56T words, 168 billion tokens\nMinerva[91] 2022 540 38.5B tokens\nUL2 [92] 2022 20 1 trillion tokens\nPaLM[93] 2022 540 768 billion tokens\nFLAN-T5[94] 2022 11 Not Available\nFLAN-PaLM[94] 2022 540 Not Available\nGemini(https://gemini.google.com/app) 2024 Not Available Not Available\nOpenAI\nGPT-2[95] 2019 1.5 40GB (∼10 billion tokens)\nGPT-3[96] 2020 175 499 billion tokens\nCodex[97] 2021 12 100 billion tokens\nWebGPT[98] 2021 175 Not Available\nInstructGPT[99] 2022 175 Not Available\nChatGPT(https://openai.com/blog/chatgpt) 2022 Not Available Not Available\nGPT-4[100] 2023 Not Available Not Available\nEleutherAI\nGPT-J[101] 2021 6 825 GiB\nGPT-Neo[102] 2021 2.7 825 GiB\nGPT-NeoX[103] 2022 20 825 GiB\nDeepMind\nGopher[104] 2021 280 300 billion tokens\nAlphaCode[105] 2022 41 967 billion tokens\nChinchilla[79] 2022 70 1.4 trillion tokens\nSparrow[106] 2022 70 Not Available\nMeta\nGalactica[107] 2022 120 106 billion tokens\nOPT[108] 2022 175 180 billion tokens\nOPT-IML[109] 2022 175 Not Available\nLLaMA[110] 2023 65 1.4 trillion\nHugging Face\nT0[111] 2021 11 Not Available\nBLOOM[112] 2022 175 350 billion tokens (1.6TB)\nmT0[113] 2022 13 Not Available\nBaidu\nErnie 2.0 Large[114] 2019 1.5 Not Available\nErnie 3.0[115] 2021 10 375 billion tokens\nErnie 3.0 Titan[116] 2021 260 300 billion tokens\nErnie Bot (https://yiyan.baidu.com/) 2023 Not Available Not Available\ncontinues to be ripe for further research and exploration to\nenhance the efficacy of knowledge distillation techniques.\n4) Symbolic Knowledge Distillation: Contrary to the meth-\nods discussed earlier, symbolic knowledge distillation is cen-\ntered on the distillation and transmission of knowledge in\na symbolic format, including rules, logic, or symbolic rep-\nresentations. This method integrates structured knowledge\nbases and rules with machine learning models to boost their\nperformance and clarity. It encodes intricate, structured infor-\nmation in a manner that allows for manipulation in reasoning,\ninference, and decision-making processes. The importance of\nthis approach lies in its alignment with human methods of\ninterpreting and reasoning with knowledge, thus providing\nenhanced transparency and interpretability.\nSymbolic knowledge distillation represents a technique\nwithin machine learning where knowledge is extracted from\na complex, typically less transparent model (like a deep\nneural network) and converted into a symbolic, more under-\nstandable format. This methodology merges the principles of\nconventional knowledge distillation with those of symbolic\nAI, aiming to improve the interpretability, transparency, and\npossibly the efficiency of machine learning models. It serves\nas a bridge between the often \"black box\" nature of deep\nlearning models and the necessity for models that can be\ncomprehended and trusted by humans. Such a requirement\nis especially critical in sectors demanding high levels of\nresponsibility and explainability, including healthcare, finance,\nand autonomous driving. Although the specific mathematical\nmodel employed may vary based on the approach and the\nsymbolic representation chosen, the overall process typically\nincludes several defined steps.\nTraining the Teacher Model: A complex model (teacher)\nis trained on a dataset to achieve high performance. This model\ncan be a deep neural network, and its architecture and training\nprocess depend on the specific task (e.g., image recognition,\nNLP).\nExtracting Knowledge: The subsequent phase involves\nderiving insights from the teacher model, achievable through\nmultiple approaches, including: examining the neuron acti-\nvation patterns within the network; employing methods like\nLayer-wise Relevance Propagation (LRP)[130] or SHapley\nAdditive exPlanations(SHAP)[131] to assess the significance\nof various inputs in the network’s decision-making process;\nand identifying rules or patterns based on the decision bound-\naries established by the network.\nSymbolic Representation: The gathered knowledge is\nsubsequently converted into a symbolic representation. This\nprocess includes: developing decision trees or compiling sets\nof logical rules that mimic the neural network’s behavior, and\nutilizing graphical models or alternative structured forms to\n6 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\nFig. 2. Types of Traditional Knowledge Distillation (a) Response-based, (b)\nFeature-based and (c) Relation-based\nencapsulate the relationships and dependencies deciphered by\nthe network.\nTraining the Student Model: Following the translation\nof extracted knowledge into a symbolic form, a simpler and\nmore interpretable ’student’ model is trained to mimic this\nsymbolic representation. The training process involves two\nkey strategies. The symbolic representation may be used\ndirectly as a comprehensive set of rules for decision-making,\nallowing the student model to replicate decision processes\nbased on predefined logical rules or the student model is\ntrained to approximate the symbolic representation itself. This\napproach often incorporates conventional supervised learning\ntechniques, with the significant distinction that the symbolic\nknowledge extracted from the teacher model acts as a guide\nor target.\nEvaluation and Refinement: Once the student model has\nbeen trained to mimic the symbolic representation, it under-\ngoes evaluation to verify that it retains the critical knowledge\nand performance attributes of the teacher model. This assess-\nment might reveal the need for adjustments either to the sym-\nbolic representation itself or to the training methodology of the\nstudent model. Such refinements are crucial for ensuring that\nthe student not only approximates the teacher’s performance\nbut does so in a way that is both interpretable and transparent.\nThis emphasis on interpretability and transparency is key, as it\naims to produce a student model that not only performs well\nbut also provides insights into its decision-making processes,\nmaking it more understandable and trustworthy to users.\nB. Large Language Models\nLLMs are the foundation model for the language and has\nbeen the hot topic for past few years. Alot of opportunities\nhas been created in one hand and due to ineffective use, it\nhas also created some kind of fear among the users. In this\nsection we will focus on the architecture of LLM followed by\nthe training process.\n1) Architecture: Transformer[52] architecture is the back-\nbone of all the LLMs. Due to its features like parallelizable\ncomputation, attention based mechanism it has been able to\nreduced reliance in hand-crafted features and also improved\nthe performance in NLP tasks. All the LLMs are directly or\nin-directly has the root the in the transformer architecture.\nExisting all the LLMs can be found to be belonging into one\nof the following architecture:\nEncoder-Decoder Architecture: The underlying principle\nof this architecture involves transforming the input sequence\ninto a fixed-length vector form, and subsequently, transforming\nthis representation into the output sequence. The architecture\nis composed of two sets of Transformer blocks: one serving\nas the encoder and the other as the decoder. The encoder\nis tasked with processing the input sequence, utilizing a\nseries of multi-head self-attention layers to convert it into\nlatent representations. These representations are then leveraged\nby the decoder, which, through an autoregressive process,\ngenerates the output sequence by employing cross-attention\nmechanisms to focus on the latent representations provided by\nthe encoder. PLM like T5[61], BART[132] and Flan-T5[94]\nuses this architecture.\nCasual Decoder Architecture: The causal decoder archi-\ntecture is a type of decoder-only architecture used in language\nmodeling, where the input and output tokens are processed\nin the same fashion through the decoder. This architecture\nincorporates a unidirectional attention mask, which ensures\nthat each input token can only attend to past tokens and\nitself by masking all future attentions to zeros. The GPT-\nseries models, including GPT-1[59], GPT-2[60], and GPT-\n3[96], are representative language models of this architecture.\nMany other LLMs, such as OPT[108], BLOOM[133], and\nGopher[104], have also adopted the causal decoder architec-\nture.\nPrefix Decoder Architecture: The prefix decoder architec-\nture, also known as a non-causal decoder, is another type of\ndecoder-only architecture which revises the masking mecha-\nnism of causal decoders to enable bidirectional attention over\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 7\nFig. 3. Symbolic Knowledge Distillation\nTABLE II\nCOMPARISON OF TRADITIONAL AND SYMBOLIC KNOWLEDGE DISTILLATION PROCESS\nParameters Traditional Knowledge Distillation Symbolic Knowledge Distillation\nNature of Knowledge Transfer Soft outputs or logits which represent the teacher’s\nlearned probability distribution\nHuman-readable representations such as logical rules,\ndecision trees, or graphical models\nInterpretability and Transparency Student model remains a black-box neural network Student model, guided by symbolic representations offer\ninsights into the decision-making process\nMethods Used for Distillation Techniques such as temperature scaling are used to\nsoften the teacher’s outputs\nInvolve methods like Layer-wise Relevance Propagation\n(LRP) or SHAP\nStudent Model Mimic the teacher model Can be tune to behave differently than teacher model\nData Generation No Yes\nLayerwise Dependency Differnet layers have different influences No such dependency\nthe prefix tokens, while maintaining unidirectional attention\nonly on generated tokens. This allows the prefix decoders\nto bidirectionally encode the prefix sequence and predict the\noutput tokens autoregressively, where the same parameters\nare shared during encoding and decoding. Unlike the causal\ndecoder architecture, the prefix decoder architecture can in-\ncorporate bidirectional information into the decoding process,\nmaking it more suitable for tasks that require understanding\nthe context of the entire input sequence. Existing representative\nLLMs based on prefix decoders include GLM-130B[134] and\nU-PaLM[135].\n2) Training Process of Large Language Models: The whole\ntraining process of LLM can be divided into two phases:\nPre-trainning:Pre-training LLMs involves training on ex-\ntensive unlabeled text datasets to learn general language pat-\nterns and insights. The success of pre-training hinges on both\nthe scale and quality of the training corpus, with large, diverse\ndatasets allowing models to capture a wide array of language\npatterns and generalize effectively to new data.\nThe pre-training process unfolds in phases, starting with\ndata collection, which is divided into general and specialized\ndata sources. General data encompasses a wide range of\ntext, including webpages, conversations, Q&A portals, and\nbooks, while specialized data targets more niche content like\nresearch papers, code, and multilingual texts. The second\nphase, data pre-processing, focuses on refining the dataset\nby eliminating noisy, redundant, and irrelevant content. Tech-\nniques employed include quality filtering, deduplication (at\nsentence, document, and dataset levels), privacy protection\n(removing personal information), and tokenization (splitting\ntext into manageable units for the model). Given that LLMs\nare not typically retrained frequently, the pre-training phase\nmust be approached with precision, prioritizing a balanced mix\nof source materials[104], and ensuring both the quantity[110]\nand quality[136] of the data are optimal. Pre-training tasks\nmay involve language modeling[95], favored by decoder-only\narchitectures for predicting subsequent tokens, or de-noising\nautoencoding[132], which focuses on correcting or replacing\ncorrupted tokens.\nFine tuning or Adaptive tuning: The fine-tuning stage is\ncrucial for adapting pre-trained LLMs to specific domains or\ntasks, leveraging labeled examples or reinforcement learning\nto refine the model’s understanding and predictive capabilities.\nIt encompasses two main strategies: instruction tuning and\nalignment tuning.\nInstruction tuning entails the fine-tuning of a language\nmodel by incorporating explicit instructions or demonstrations\nduring training. This approach is designed to direct the model\ntowards desired behaviors and outcomes, facilitating a more\ntargeted response to tasks. The instructions for this tuning\ncan be derived from existing datasets reformatted to include\nclear directives or crafted to reflect specific human needs.\nAlignment tuning, on the other hand, aims to adjust the\nLLM’s outputs to match human expectations accurately, a\nprocess that may involve a trade-off known as the alignment\ntax[106]. This concept refers to potential compromises in the\nmodel’s capabilities as it is fine-tuned to prioritize outputs\nthat are deemed more acceptable or beneficial from a human\nperspective. The most commonly used alignment criterias are\nhelpfulness, honesty, and harmlessness[106][99]. Few other\ncriteria are also mentioned like behavior, intent, incentive, and\ninner aspects[137].\nIV. S YMBOLIC KNOWLEDGE DISTILLATION OF LARGE\nLANGUAGE MODELS\nSymbolic Knowledge Distillation of LLMs aimed at distill-\ning the extensive knowledge encapsulated within LLMs into\nmore interpretable and efficient forms. It’s central method-\nology revolves around transforming the latent knowledge of\nmodels like GPT-3 into symbolic or rule-based representations.\nIt involves a sophisticated process designed to transform the\nlatent, complex knowledge within these models into explicit,\nstructured, and interpretable forms. This process begins with\nthe careful crafting of customised prompts that guide LLMs to\ngenerate outputs rich in specific knowledge types. Following\nthis, NLP techniques like Named Entity Recognition (NER),\nPart-Of-Speech (POS) tagging, and dependency parsing, are\n8 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\nemployed to analyze and structure the responses. This step\nextract meaningful information and identify patterns within the\ntext, which are then transformed into structured knowledge\nformats such as logic rules, knowledge graphs, or semantic\nframes. It derives explicit rules and patterns from the LLMs’\nresponses, thereby facilitating the encoding of this information\ninto symbolic representations that can be easily understood and\nmanipulated.\nThe subsequent phase of this process involves the refine-\nment and validation of the generated symbolic representations\nto preserve depth of knowledge and to ensure their accuracy,\nconsistency, and practical utility. This includes refining the\nsymbolic knowledge using the human experts or using the\ntrained models to classify the generated knowledge on the\nbasis of quality. The refined symbolic knowledge base under-\ngoes validation against established benchmarks, allowing for\nthe assessment of enhancements and ensuring the symbolic\nrepresentations meet the required standards of quality and\nutility.\nThe creation of a high-quality knowledge base facilitates the\ntraining of smaller models, demonstrating that a quality dataset\ncan significantly improve the performance of models that are\n100 times smaller than their teacher counterparts[2]. This\nhighlights the efficacy of integrating symbolic knowledge into\nlanguage models, presenting a viable alternative to scaling up\nLLMs. Symbolic knowledge distillation generates smaller, yet\nmore efficient models, making them suitable for deployment\nin everyday practical applications, offering a more resource-\nefficient pathway to achieving high-quality outputs in language\nmodels.\nVarious approaches that are used to distill the symbolic\nknowledge of LLMs can be categorised as:\nA. Direct Distillation\nThe distillation of symbolic knowledge from LLMs like\nGPT-3 begins with the construction of a specific prompt. This\nprompt is designed to elicit responses that encapsulate com-\nmonsense or factual understanding. It could involve scenarios,\nquestions, or statements that require the application of general\nknowledge about the world. The effectiveness of this step\nhinges on the ability to craft prompts that are both clear and\ncontextually rich enough to guide the LLM towards producing\nrelevant and insightful outputs. Upon receiving the prompt,\nthe LLM generates a response based on its training and the\nintricacies of the provided context. These models, have been\nexposed to extensive and varied textual data, encompassing a\nwide array of commonsense situations and factual knowledge.\nThis extensive training enables them to generate responses\nthat are not only contextually appropriate but also rich in\ncommonsense and factual knowledge. The model’s response\nis a complex interplay of its learned patterns, linguistic un-\nderstanding, and the implicit knowledge embedded within its\ntraining corpus. This step translates the implicit knowledge\nwithin the model into explicit textual responses that can be\nfurther analyzed and utilized for knowledge extraction.\nThe generated text is then analyzed to extract knowledge.\nThis can be in the form of statements, inferences, or relation-\nships that are implicitly or explicitly expressed in the text.\nFig. 4. Overview of Direct Distillation process LLMs\nThe extraction process might involve additional processing\nsteps like parsing the text to identify relevant information\nor using templates to format the knowledge in a structured\nway. The knowledge base derived from this process can be\nfurther improved with the assistance of critics, who may\nbe human evaluators providing feedback on the quality and\nacceptability of the generated content. Once a substantial\nvolume of high-quality generated data has been accumulated,\nthis data can be utilized to train a critic model like RoBERTa,\nwhich can be used to evaluate the generated text for accuracy,\nrelevance, and coherence. The critic model can filter out lower-\nquality outputs, ensuring that only high-quality commonsense\nknowledge is retained. The high-quality knowledge can then\nbe distilled into structured formats like knowledge graphs or\nfurther trained into specialized models. This process involves\norganizing the knowledge in a way that can be easily utilized\nby other systems or models.\nB. Multilevel distillation of symbolic knowledge\nThis approach iteratively refines the knowledge transfer\nfrom a larger, pre-trained teacher model to a smaller, more\nefficient student model. The process begins with the teacher\nmodel, typically a LLM like GPT-3, generating initial knowl-\nedge base. The generated knowledge base is then filtered\nfor quality, focusing on aspects like accuracy and length.\nThe smaller student model, such as GPT2-Large, is initially\ntrained on this filtered dataset. Subsequently, the student model\ngenerates new knowledge base, which are again filtered to\nenhance quality. This cycle of generation and refining through\nfiltering is repeated iteratively, with each iteration aiming to\nimprove fidelity and succinctness of the distilled knowledge.\nDuring each iteration, various filters are applied to ensure\nthe quality which are fidelity filter, length filter or contextual\nfilter. The Fidelity Filter ensures a true representation of the\ninput sentence, verified using an off-the-shelf Natural Lan-\nguage Inference (NLI) model. The Length Filter controls the\nlength to fit within a predefined compression ratio, gradually\nguiding the model to produce increasingly concise output.\nA Contextual Filter is used in some cases, focusing on the\ncoherence in the larger context of the text. The process results\nin the development of increasingly efficient student models\nthat inherit the distillation ability of the teacher model but\nwith enhanced control over quality. This method allows for\nthe creation of high-quality, succinct dataset with diverse\ncompression ratios, without relying on pre-existing annotated\ndatasets.\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 9\nFig. 5. Overview of Multilevel Distillation process LLMs\nC. Distillation using Reinforcement Learning policy\nThe approach refines the policy of a LLM through a two-\nstep iterative process: generating and filtering data. The first\nstep, involves using the current LLM policy to generate a\nrange of output predictions for given contexts, effectively\naugmenting the training dataset. Initially, this policy might\nbe based on a supervised learning model, and the generated\noutputs may not be perfectly aligned with human preferences.\nHowever, this step is essential for creating a diverse set of\npotential outputs for further refinement. The generated data\nforms the basis for the next critical phase of the process.\nIn the second step, the data produced is ranked and filtered\nusing a filters like scoring function, typically a learned reward\nmodel trained on human preferences. This step is pivotal in\nselecting the best outputs that align with the desired human\noutcomes, as determined by the scores from the reward model.\nThe filtering threshold can be incrementally increased in\nsubsequent iterations, ensuring that only the top-performing\noutputs are selected for further training. The language model\nis then fine-tuned on this curated dataset with an offline RL\nobjective, adjusting its policy to produce outputs that are\nmore likely to receive high scores. This process of generating\nand filtering, repeated iteratively, serves as a feedback loop,\ncontinuously refining the model’s policy towards outputs in-\ncreasingly aligned with human preferences.\nAll three techniques mentioned have been successfully\napplied to various research areas, including commonsense\nreasoning[2], translation[4], summarisation[3] , and mathemat-\nical reasoning[138], among others, yielding significant results.\nFig. 7 provides an overview of all the areas explored so\nfar, with detailed discussions presented in the related works\nsection. Table.III offers insights into each research area,\ncategorizing them based on the techniques discussed above.\nV. R ELATED WORKS\nIn this segment, we begin by exploring the foundational\nwork that positions LLMs as a knowledge base and then delve\ninto research focused on analyzing the knowledge contained\nwithin LLMs. Lastly, we review efforts aimed at distilling this\nknowledge into a symbolic form. An overview of this concept\nis presented in Fig. 7.\nA. Knowledge Base of LLM\nLLM can act as a knowledge base or oracle that per-\nforms well on open-domain question answering without fine-\nFig. 6. Overview of Distillation process using RL\ntuning[1]. LLM can also function as the domain-specific KBs\nin biomedical field however they are highly influenced by\nprompt bias and synonym variance[139]. It rapidly and stably\nacquires linguistic knowledge, including syntax, grammar, and\nparts of speech, predominantly in the early stages of pre-\ntraining, showing little variation across different domains. In\ncontrast, the assimilation of factual and commonsense knowl-\nedge is slower, more sensitive to the domain of the training\ndata, and exhibits a more gradual progression throughout the\npre-training period[140].\nB. Consistency of Knowledge In LM\nThe research[141] sheds light on the consistency of knowl-\nedge in PLMs like BERT and RoBERTa. Their findings reveal\na concerning lack of consistency in these models, particularly\nwhen responding to paraphrased queries with factual content.\nThe study[142] adds another layer of complexity to this issue\nby highlighting the challenges PLMs face in accurately pro-\ncessing negated facts and their susceptibility to being misled\nby contextually irrelevant or misleading information.\nC. Editing the Knowledge in LLM\nEditing knowledge in LLMs has become a prominent\narea of research with several innovative approaches pro-\nposed to address this challenge. Constrained layer-wise fine-\ntuning[143] formulates knowledge modification as a con-\nstrained optimization problem and allows for fine-tuning\nspecific layers to update knowledge while retaining exist-\ning information. [144] introduced the concept of Knowledge\nNeurons, enabling pinpointing specific components responsi-\nble for factual knowledge within LLMs and providing the\nmeans to manipulate them for altering model output. The\nKNOWLEDGEEDITOR[145] offers an efficient way to update\nfactual knowledge in pre-trained LLMs without extensive\nretraining.The paper[146] introduces methods for detecting,\nupdating, and visualizing beliefs in LLM by using the Sequen-\ntial Local and Generalizing (SLAG) update objective. Model\nEditor Networks with Gradient Decomposition (MEND)[147]\nefficiently edit large-scale pre-trained models by transforming\ngradients during fine-tuning. Continual Knowledge Learning\n(CKL)[148] addresses the challenge of updating and main-\ntaining the relevancy of world knowledge in LLMs.\n10 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\nFig. 7. Overview of Related Works\nD. Reasoning with Knowledge in LLM\nThe research landscape concerning reasoning abilities in\nPLMs and transformers, has seen significant exploration and\ndevelopment. The paper[149] found that while BERT could\nlearn simpler one-hop rules, it struggled with more complex\ntwo-hop rules and distinguishing between symmetric and non-\nsymmetric relations. [150] demonstrates that transformers can\neffectively emulate reasoning over language, achieving high\naccuracy on various synthetic datasets that require different\ndepths of inference and can act as limited \"soft theorem\nprovers\". PROVER[151] extended [150] to answer binary\nquestions over rule-bases while generating corresponding\nproofs for enhanced interpretability. ProofWriter[152] stands\nout for its ability to produce implications and corresponding\nnatural language proofs from given theories, using the T5\ntransformer architecture. The paper[153] explores the capa-\nbility of Transformer Language Models (TLMs) in logical\nreasoning with natural language focusing on first-order logic\nproofs. The paper[154] explore the capacity of transformer\nmodels to perform deductive reasoning on logical theories\nexpressed in natural language by introducing a method for gen-\nerating challenging reasoning datasets whereas the paper[155]\nenhance the deductive reasoning abilities of PLMs using soft\nHorn rules and achieved high performance on unseen logical\nrules and showed improved understanding of logical properties\nlike negation and symmetry. The paper[156] introduces a novel\ndataset to evaluate the mathematical reasoning capabilities\nof neural networks, focusing on problems across arithmetic,\nalgebra, probability, and calculus.\nThe paper[157] integrates commonsense reasoning on nat-\nural language question-answering tasks by employing smaller\nlanguage models,and demonstrate competitive performance\nagainst large PLMs. RICA (Robust Inference using Common-\nsense Axioms)[158],found that PLMs are vulnerable to pertur-\nbation attacks, where minor changes in input data drastically\nalter their conclusions. The paper[159] presents the Common\nSense Explanations (CoS-E) dataset and the Commonsense\nAuto-Generated Explanation (CAGE) framework, which lever-\nages natural language explanations(human-like explanations)\nto improve model’s reasoning capabilities.\nE. Interpreting the Knowledge of LLM\nInterpreting the knowledge encoded in LLMs has been\nadvanced through various studies, each contributing unique\ninsights into how these models capture and process linguistic\ninformation. [160] argue that attention weights often don’t\nalign with other feature importance measures and can produce\nsimilar predictions despite different attention distributions.\nThis view is nuanced by [161], who suggest that attention\ncan serve as an explanation, but its validity depends on the\ncontext and testing methods. [162] also investigate attention in\ntext classification, finding that while there is some correlation\nbetween attention weights and model predictions, attention\nweights alone are not definitive indicators of input importance\nand propose that gradient-based attention weight rankings\nprovide a deeper understanding.\nThe study[163] include method for quantifying non-linearity\nin transformers, particularly in feed-forward networks. They\nreveal a non-distinct feature extraction process in BERT lay-\ners, influenced by skip connections. [164] demonstrate that\ntransformer layers function as key-value memories, capturing\ntextual patterns and inducing distributions over the output\nvocabulary, with lower layers focusing on shallow patterns\nand upper layers on semantic ones. [165] show that factual\nassociations in GPT models are tied to localized computations,\nparticularly in middle-layer feed-forward modules.\nF . Explainability in LLM\nThe study[166] investigates the application of Influence\nFunctions (IFs) to identify artifacts in models, comparing their\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 11\neffectiveness with that of common word-saliency methods.\nResearchers in study [167] compare IFs with simpler retrieval-\nbased methods and suggest that despite the complexity of\nIFs, simpler methods can achieve comparable performance.\nExploring further in study[168], they introduce Training-\nfeature attribution (TFA), which synergizes saliency maps and\ninstance attribution to effectively uncover artifacts. Researcher\nin [169] propose Human In the Loop Debugging using Influ-\nence Functions (HILDIF), a pipeline that employs influence\nfunctions for debugging deep text classifiers, allowing human\ninvolvement in enhancing model performance.\nIn a different approach, study [170] presents a novel method\nfor training language models to generate natural text expla-\nnations alongside their predictions, utilizing the text-to-text\nframework[61]. Addressing the challenge of inconsistency in\nnatural language explanations, [171] introduces an adversarial\nframework to identify and measure these inconsistencies. The\nProto-Trex model[172] uses prototypical examples to explain\nmodel predictions, thus mitigating the opacity often associated\nwith complex models. Research[173] enhances interpretability\nby extracting key text segments, termed \"rationales\", serving\nas justifications for model predictions. Study[174] works on\nimproving commonsense reasoning by employing contrastive\nexplanations generated through specialized prompts, aligning\nmodel reasoning more closely with human cognitive patterns.\nG. Symbolic Knowledge Distillation\nThe conducted research works in this area can be cate-\ngorised as follows:\n1) Commonsense Knowledge: The study[2] introduces a\ntransformative shift in the conventional practice, transitioning\nfrom the traditional ’from-human-to-corpus-to-machine’ ap-\nproach to an innovative ’from-machine-to-corpus-to-machine’\nparadigm through the introduction of symbolic knowledge\ndistillation. In their research, the authors not only succeed\nin creating a substantially larger common-sense dataset from\nATOMIC resource[175], approximately ten times larger than\npreviously manually synthesized datasets, but also enhance its\ndiversity and quality. Their novel approach involves training\nthe common-sense model using this newly generated knowl-\nedge graph. Despite being only 1/100th of its predecessor\nmodel, it outperforms the previous model, showcasing the\neffectiveness of their approach. The paper[176] introduces\nNOV ACOMET, an innovative open commonsense knowledge\nmodel that merges the strengths of both knowledge and\ngeneral task models. This model, built upon symbolic knowl-\nedge distilled from proprietary models like GPT-3, creates\nan auditable discrete knowledge graph, NOV ATOMIC, which\nfacilitates open-format training and application to a wide array\nof reasoning tasks. It demonstrates superior performance in\ncommonsense reasoning, outperforming comparable models\nin various benchmarks. The model’s training involves novel\ntechniques like commonsense field masking for enhanced\nflexibility in knowledge handling. Iterative Imitation and De-\ncoding for Distillation(I2D2)[177] framework employs a four-\nstage process that includes prompt construction, constrained\ndecoding using NeuroLogic Decoding, critic filtering, and self-\nimitation learning, where the model is iteratively refined based\non its own high-quality outputs. A new corpus, Gen-A-tomic,\nwas created to provide diverse and accurate commonsense\nknowledge. I2D2 demonstrated superior performance in accu-\nracy and precision over larger models like GPT-3, with GPT-2\nXL showing significant improvements through self-imitation\nlearning iterations.\n2) Translation: Reinforced Self-Training (ReST)[4] is a\nmethod to align LLMs with human preferences in the realm\nof machine translation. This approach incorporates reinforce-\nment learning from human feedback (RLHF) to enhance\nthe output quality. ReST initiates by generating a dataset\nthrough sampling from the initial LLM policy, followed by\nthe application of offline reinforcement learning algorithms to\nrefine the policy. This method is identified as more efficient\nthan traditional online RLHF techniques, primarily because\nit facilitates the creation of the training dataset in an offline\nmanner, promoting the reuse of data. The effectiveness of\nReST is demonstrated through significant improvements in\ntranslation quality, validated by both automated metrics and\nhuman evaluations across various machine translation bench-\nmarks.\n3) Summarisation: REFEREE[3] is a framework for\nreference-free sentence summarization that allows for direct\ncontrol of compression ratio. It uses Symbolic Knowledge\nDistillation to distill latent knowledge from PLMs, resulting\nin smaller but better summarizers with sharper controllability.\nThe framework employs iterative distillation of knowledge,\nwhere student models from previous iterations serve as teacher\nmodels in the next iteration. This iterative process also gen-\nerates a high-quality dataset of sentence-summary pairs with\nvarying compression ratios. The final student models outper-\nform the larger GPT3-Instruct model in terms of compression\nratio controllability without compromising the quality of the\nsummarization.\n4) Mathematical Proof and Reasoning: The paper[138]\npresents a method called expert iteration, which combines\nproof search with learning to improve language modeling in\nformal mathematics. The method involves finding new original\nproofs for the same statements and closing marginally harder\nstatements at each iteration, which in turn provides more useful\ntraining data for the next iteration. By interleaving proof search\nwith learning, expert iteration is able to dramatically outper-\nform proof search only. The paper demonstrates the effective-\nness of expert iteration on a manually curated set of problem\nstatements and achieves state-of-the-art results on the miniF2F\nbenchmark, a set of formalized statements of mathematical\nproblems from various competitions.The paper[178] explores\nthe concept of distilling abilities from LLMs into smaller\nones, specifically for enhancing their performance in multi-\nstep math reasoning tasks. The process begins with generating\na dataset using a larger model (like GPT-3.5) employing chain-\nof-thought reasoning, where the model details the steps leading\nto a solution. This dataset is then used to fine-tune a smaller T5\nmodel, with the aim of specializing its abilities in the specific\narea of multi-step reasoning. This fine-tuning process allows\nthe smaller model to learn the complex reasoning patterns\ndemonstrated by the larger model.\n12 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\n5) Visual Commonsense: Localized Symbolic Knowledge\nDistillation (LSKD)[179] enhances vision-language models by\nfocusing on localized regions within images. This method\naddresses a significant limitation in existing models, which\ninterpret images as a whole, by introducing Localized Visual\nCommonsense models that can specify and reason about\nmultiple distinct regions in an image. The authors develop a\nscalable framework for generating localized visual common-\nsense statements and establish the Localized Commonsense\nKnowledge Corpus, which aids in expanding the capabilities\nof vision+language models to include references-as-input. The\npaper highlights the state-of-the-art zero-shot performance of\nthese models on three localized visual reasoning tasks and\nshowcases the superiority of the student model over the teacher\nmodel through human evaluation.\n6) Instruction Generation: Traditional instruction-tuned\nmodels, reliant on human-written instruction data, often lack\ndiversity and creativity, constraining the generality of the\nmodel. SELF-INSTRUCT[180] mitigates this by enabling\nmodels to generate their own instructions, inputs, and outputs,\nwhich are then used for fine-tuning. This process involves\ngenerating task instructions, classifying them, creating in-\nstances via input-first or output-first approaches, and filtering\nout low-quality data. The approach significantly reduces the\nneed for human-labeled data, fostering a broader and more\ncreative instructional capability in LMs. The performance\nevaluation shows that the GPT3SELF-INST model, fine-\ntuned on this self-generated data, substantially outperforms\nthe vanilla GPT-3 in instruction-following tasks and closely\nmatches the performance of models like InstructGPT001.\nAlpaca[181] enhance the SELF-INSTRUCT data generation\npipeline by employing the more advanced text-davinci-003\nmodel for instruction data generation that explicitly defines\nthe requirements for instruction generation, aiming for more\nfocused and relevant outputs. The adoption of aggressive\nbatch decoding, producing 20 instructions simultaneously,\nsignificantly reduces data generation costs and simplifying the\npipeline by eliminating the distinction between classification\nand non-classification instructions and generating only a single\ninstance per instruction, instead of 2 to 3, streamlines the\nprocess. Evol-Instruct[182] is a novel method that uses LLMs\nto automatically generate a vast array of complex instructional\ndata. This approach begins with simple initial instructions and\nemploys the LLM to evolve these into more sophisticated and\ndiverse instructions through in-depth and in-breadth evolution\nprocesses. It enhances instructions by adding constraints, in-\ncreasing reasoning complexity, and diversifying topics, thus\ncreating a rich dataset for fine-tuning LLMs. This dataset\nis used to train the LLaMA model, resulting in WizardLM,\na model demonstrating superior performance in following\ncomplex instructions compared to human-generated datasets\nand existing models like ChatGPT.\n7) Handling queries: Vicuna-13B[183] is an open-source\nchatbot developed by fine-tuning the LLaMA model with\naround 70,000 user-shared ChatGPT conversations from\nShareGPT. It demonstrates superior performance, achieving\nover 90% of ChatGPT’s quality, and surpassing other models\nlike LLaMA and Stanford Alpaca. The training, which cost\napproximately $300, utilized advanced techniques for handling\nmulti-turn conversations. Despite its advancements, Vicuna-\n13B shares common LLM limitations, such as challenges in\nreasoning or math tasks, and has potential issues with factual\naccuracy and safety. Koala[184], a chatbot model developed\nby fine-tuning Meta’s LLaMA with web-sourced dialogue\ndata, including interactions with large models like ChatGPT.\nKoala demonstrates competitive performance against estab-\nlished models such as ChatGPT and Stanford’s Alpaca, par-\nticularly in handling real user queries. ASK ME ANYTHING\nPROMPTING (AMA)[185] is a prompting method for improv-\ning the performance of LLMs like GPT-3. AMA leverages\nmultiple effective but imperfect prompts, aggregating them\nusing weak supervision to enhance prediction quality. This\nmethod primarily utilizes open-ended question-answering for-\nmats, which are found to be more effective than restrictive\nprompts. AMA’s recursive use of the LLM to transform task\ninputs into these formats, combined with the aggregation of di-\nverse prompts, demonstrates significant improvements in LLM\npredictions. QAMELEON[186] is an innovative approach to\nmultilingual question answering (QA) systems, leveraging\nPLMs within a few-shot learning framework. PLMs generate\nQA pairs in multiple languages, significantly reducing the\nneed for extensive, language-specific training datasets. By\nrequiring only a minimal number of examples (as few as five\nper language), QAMELEON efficiently fine-tunes QA models,\novercoming traditional constraints of resource-intensive data\nannotation. This approach not only simplifies and accelerates\nthe development of multilingual QA systems but also achieves\nsuperior accuracy and efficiency, demonstrating its potential as\na scalable and effective solution in NLP.\n8) Labeling Data: The research paper[81] examines the\nefficacy of using GPT-3 for data labeling in NLP tasks, high-\nlighting its cost-effectiveness compared to traditional human\nlabeling. The study reveals that GPT-3 can reduce labeling\ncosts by 50% to 96% across various tasks, including sentiment\nanalysis, text classification, and summarization. The paper\nintroduces a novel framework that combines GPT-3 generated\npseudo labels with human labels, improving performance\nunder limited budgets. Furthermore, an active labeling strategy\nis explored, where low-confidence labels by GPT-3 are re-\nannotated by humans, enhancing label quality. Despite these\nbenefits, the paper notes that GPT-3 is more suited for low-\nstakes labeling tasks, as its reliability in high-stakes scenarios\nremains limited. The research[82] presents a novel method\nfor utilizing PLMs in tasks with scarce labeled training data.\nThis technique involves prompting the LM with multiple\nqueries about an example, and the model’s responses are\nthen interpreted as votes for specific labels or as abstentions.\nThis process, integrated within a weak supervision framework,\nleverages the capabilities of the LM as a labeling function.\nThe Snorkel system is subsequently employed to clean and\nrefine these noisy label sources, culminating in the creation of\nenhanced training data for an end classifier.\n9) Task Specific Small Models: The method, \"Distilling\nstep-by-step\"[187], involves extracting rationales from LLMs\nalongside output labels. These rationales, serving as detailed\nexplanations for model predictions, are then used in a multi-\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 13\ntask learning framework to train smaller models on both label\nand rationale prediction tasks. This technique significantly\nreduces the data and model size required, enabling smaller\nmodels to surpass the performance of LLMs more efficiently.\nThe paper demonstrates the effectiveness of this approach\nacross multiple datasets and tasks, showcasing it as a resource-\nefficient alternative to standard finetuning and traditional dis-\ntillation methods.\n10) Complex Reasoning: Orca [188] is designed to en-\nhance the capabilities of smaller models through imitation\nlearning from large foundation models (LFMs). Traditional\nmethods faced issues like limited imitation signals, small-\nscale homogeneous training data, and inadequate evaluation,\nleading to an overestimation of the small models’ capabilities.\nThese models often imitated the style but not the reasoning\nprocess of LFMs. Orca addresses these challenges by learning\nfrom GPT-4’s rich signals, including explanation traces, step-\nby-step thought processes, and complex instructions, with\nguidance from ChatGPT as a teacher. This approach en-\nables progressive learning through large-scale and diverse\nimitation data. Orca significantly outperforms state-of-the-art\ninstruction-tuned models like Vicuna-13B in complex zero-\nshot reasoning benchmarks, achieving more than a 100%\nimprovement in Big-Bench Hard (BBH) and a 42% im-\nprovement in AGIEval. Orca reaches parity with ChatGPT in\nBBH and exhibits competitive performance in professional and\nacademic exams like the SAT, LSAT, GRE, and GMAT, in\nzero-shot settings without Chain of Thought (CoT), though\nit still trails behind GPT-4. Orca 2[189] builds upon the\nOrca project, focusing on enhancing smaller LMs’ reasoning\nabilities. Orca 2 continues exploration, particularly addressing\nthe limitations of imitation learning, which had been the\nprimary method for training small LMs. This method, while\neffective in replicating the output of larger models, often fell\nshort in reasoning and comprehension skills. It introduces\nvarious reasoning techniques (e.g., step-by-step processing,\nrecall-then-generate, recall-reason-generate, extract-generate,\ndirect-answer methods) and focuses on teaching small LMs to\nchoose the most effective reasoning strategy for a given task.\nThis approach aims to enable small LMs to perform at their\nbest, regardless of their size, by utilizing more nuanced data\nand training strategies. The system is described as a \"Cautious\nReasoner,\" learning to execute specific reasoning steps and\nstrategize at a higher level how to approach particular tasks.\nVI. O PPORTUNITIES\nSymbolic Knowledge distillation of LLM has been one\nof the heated topics and has been gaining rapid popularity.\nAmong the various areas, the most prominent areas where it\ncan be applied are:\nA. Creation of larger, diversified and qualitative dataset\nIt offers significant potential in enhancing dataset qual-\nity and diversity. This process involves extracting structured\nknowledge from LLMs to create datasets that are not only\nlarger in scale but also exhibit a broader range of qualities\nand characteristics. These enriched datasets can be pivotal in\nTABLE III\nRELATED WORKS IN SYMBOLIC KNOWLEDGE DISTILLATION\nResearch Types Application\n[2] Direct Commonsense Reasoning\n[3] Multi-level Summarisation\n[4] RL based Translation\n[176] Direct Commonsense Reasoning\n[177] Direct Commonsense Reasoning\n[138] Direct Mathematical Proof and Reasoning\n[178] Direct Mathematical Proof and Reasoning\n[179] Direct Visual Commonsense Reasoning\n[180] Direct Instruction Generation\n[181] Direct Instruction Generation\n[182] Direct Instruction Generation\n[183] Direct Handling Queries\n[184] Direct Handling Queries\n[185] Direct Handling Queries\n[186] Direct Handling Queries\n[81] Direct Labeling Data\n[82] Direct Labeling Data\n[187] Direct Generating Task Specific Small Models\n[188] Direct Complex Reasoning\n[189] Direct Complex Reasoning\ntraining more robust and efficient machine learning models,\nleading to advancements in various domains such as NLP,\nimage recognition, and beyond. The ability to generate high-\nquality datasets from LLMs accelerates the development of\nmore sophisticated AI systems, contributing to advances in\nboth academic research and practical applications.\nB. Reduction in the cost by utilising machines in the low level\ntask under guidance on humans\nImplementing symbolic knowledge distillation in low-level\ntasks allows for the effective delegation of routine and repeti-\ntive tasks to machines, significantly reducing operational costs.\nBy leveraging the distilled knowledge from LLMs, machines\ncan perform these tasks with a high degree of accuracy\nand efficiency, under the supervision of human experts. This\ncollaboration between human intelligence and machine capa-\nbilities leads to optimized resource utilization, where humans\nfocus on more complex, creative, or decision-making tasks\nwhile machines handle the routine aspects, thereby enhancing\noverall productivity and cost-effectiveness.\nC. Smaller and more powerful models than LLMs for summa-\nrization, translation, common sense etc\nDistilling knowledge from LLMs into smaller models\npresents a promising avenue for creating compact yet powerful\nAI tools. These distilled models retain the core capabilities\nof their larger counterparts but with reduced computational\nrequirements. This makes them particularly suitable for appli-\ncations like text summarization, language translation, and com-\nmon sense reasoning, where efficiency and speed are crucial.\nThese smaller models offer the dual benefits of lower resource\nconsumption and faster processing times, making them ideal\nfor deployment in environments with limited computational\nresources or for applications requiring real-time responses.\n14 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\nTABLE IV\nRELATED WORKS IN SYMBOLIC KNOWLEDGE DISTILLATION WITH THEIR MAJOR COMPONENTS\nResearch Teacher Student Dataset Generated Size of Dataset\n[2] GPT-3(175B) COMET distil(1.5B) Commonsense Knowledge Graph 6.5M\n[3] GPT-3 REFEREE-CONTROL Sentence-summary pairs 100K\n[4] Encoder-Decoder Architecture Teacher Itself Translation Dataset N/A\n[176] GPT-3 NOV ACOMET NOV ATOMIC 2.2M\n[177] GPT-3 GPT-2 Gen-A-tomic 7M\n[138] Decoder Only Architecture Teacher Itself Tactic Dataset N/A\n[178] GPT-3.5 FlanT5 Math Reasoning N/A\n[179] ChatGPT BLIP-2 Localized Commonsense Knowledge 1M\n[180] GPT-3 Teacher Itself Instruction Dataset 82K\n[181] GPT-3.5 7B LLaMA Instruction Dataset 52K\n[182] ChatGPT WizardLM Instruction Dataset 250K\n[183] ChatGPT Vicuna-13B Conversational Dataset 70K\n[184] ChatGPT Koala-13B Conversational Dataset N/A\n[185] GPT3-175B GPT-J-6B Prompt Dataset N/A\n[186] PaLM-540B mT5-XL Multilingual QA 47173\n[81] GPT-3 RoBERTa Labeled Data 5.1K\n[82] GPT-3 T0++ Labeled Data N/A\n[187] 540B PaLM 770M T5 Rationales N/A\n[188] GPT-4 Orca(13B) Zero shot queries 5M\n[189] GPT-4 Orca-2 Progressive queries 817K\nD. Instruction tuning\nInstruction tuning, in the context of symbolic knowledge\ndistillation from LLMs, refers to the process of refining and\noptimizing AI models to better understand and execute specific\ninstructions. This approach enhances the model’s ability to\ninterpret and act upon user commands accurately, leading to\nmore intuitive and user-friendly AI systems. Instruction tuning\nis particularly relevant in applications where user interaction is\nkey, such as virtual assistants, educational tools, and interactive\nAI systems. By focusing on instruction tuning, developers\ncan create AI models that are not only powerful in their\ncapabilities but also align closely with user expectations and\nneeds, facilitating more effective and seamless human-AI\ninteractions.\nE. Novel Algorithm and Evaluation Benchmark\nSize alone does not determine the quality of language\ngeneration. Innovative approaches, such as those seen in\nI2D2[177], present a viable option, particularly in scenarios\nwhere utilizing massive models like GPT-3 is impractical.\nGiven that this field is in its infancy, the evaluation benchmarks\nare quite intricate and require significant refinement. Current\nevaluation techniques are from traditional knowledge distilla-\ntion benchmarks and must be updated to fit this novel area\nof study.Symbolic Knowledge Distillation of LLMs involves\ntwo components: the neural aspect (LLMs) and the symbolic\naspect (distilled symbolic knowledge). Together, these form a\nNeurosymbolic model, which necessitates the development of\nnew benchmarks for evaluation, testing, and validation[190].\nF . Creation of Open source data and open model\nThe concept of symbolic distillation presents an intriguing\navenue for creating open source data and models within the\nrealm of LLMs. Currently, many LLMs are proprietary and\ntrained on closed-source data, limiting accessibility and trans-\nparency. Symbolic distillation involves extracting symbolic\nknowledge and representations from LLMs, which can then\nbe used to generate open source data. This open data can\nserve as the foundation for training new models that are open\nsource, thereby democratizing access to advanced language\nmodels. By transitioning from closed source to open source,\nwe can promote transparency, collaboration, and innovation in\nthe field of NLP, aligning with the principles of open science\nand open AI.\nG. Self Improvement of LLMs\nReinforcement Learning from Human Feedback (RLHF)\nhas emerged as a prevalent method for refining LLMs. How-\never, the involvement of human input inherently constrains\nits efficacy and outcomes to the limits of human capabili-\nties. Upon undergoing fine-tuning, LLMs can surpass human\nperformance levels. Leveraging these enhanced models to\nautonomously fine-tune themselves, either via rewards[87] or\nprompt tuning or alternative mechanisms, presents a viable\nstrategy for eliminating the limitations imposed by human\nintervention opening the gateway for Superintelligence. When\nemploying Reinforcement Learning (RL) for fine-tuning LLMs\nby themselves, opting for Neurosymbolic RL approaches is\noften more advantageous. This is because Neurosymbolic RL\nnot only aids in the tuning process but also enhances the model\nwith the ability to interpret and explain its decision-making\nprocess comprehensively[191].\nH. Cross-domain Symbiosis\nSymbolic Knowledge extracted from LLMs extends its util-\nity beyond the linguistic domain. Studies, such [179], demon-\nstrate that textual knowledge can augment visual models by\noffering explanations and enhancing efficiency. This interdis-\nciplinary application can be further leveraged in diverse fields\nsuch as medical imaging, autonomous driving, and surveil-\nlance, serving not only to elucidate model outputs but also\nto improve transfer from one domain to another(simulation\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 15\nto real) by providing the semantic anchors[192]. This cross-\ndomain synergy highlights the potential of Symbolic Knowl-\nedge in broadening the applicability and understanding of\ncomplex AI systems.\nI. Industrial Applications\nSymbolic knowledge distillation reveals a critical insight:\nthe effectiveness of LLMs is significantly influenced not only\nby their size (number of parameters) but more importantly by\nthe quality of the datasets on which they are trained. It high-\nlights the significant role of symbolic knowledge distillation\nin enhancing domain-specific AI applications by fine-tuning\nLLMs with specialized corpora and instruction-following\ndata. Notable implementations include LawyerLLaMA[193]\nand LawGPT[194] for legal services, HuatuoGPT[195] and\nChatDoctor[196] for medical applications, XuanYuan[197] for\nfinance, DARWIN Series[198] and SciGLM[199] for scientific\nresearch. These tailored models demonstrate substantial im-\nprovements in accuracy, efficiency, and usability, showcasing\nthe transformative potential of symbolic knowledge distillation\nin various industries.\nVII. C HALLENGES\nA. Ensuring Data Quality and Diversity in Datasets\nWhile symbolic knowledge distillation from LLMs promises\nto enhance dataset quality, a major challenge is ensuring the\nhigh quality and representativeness of the generated data. The\ndatasets derived from LLMs may inherit biases or inaccuracies\npresent in the original training data of these models. This\ncan lead to the propagation of errors and skewed perspectives\nin the new datasets, affecting the reliability and fairness of\nAI systems trained on them. Ensuring data quality requires\nrigorous validation processes and mechanisms to identify and\nmitigate biases, which can be resource-intensive, complex, is\nstill an not so explored area.\nB. Balancing Automation and Human Oversight in Dataset\nGeneration\nWhile utilizing machines under human guidance can reduce\ncosts, achieving the right balance between automation and\nhuman oversight is challenging. Over-reliance on automa-\ntion may lead to oversight of nuanced or exceptional cases\nthat require human judgment. Conversely, excessive human\nintervention can negate the efficiency gains from automa-\ntion. Establishing effective protocols and systems for human-\nmachine collaboration, where machines handle routine tasks\nwhile humans oversee and intervene as needed, is crucial but\ndifficult to optimize.\nC. Developing Compact Models Without Compromising Per-\nformance\nCreating smaller models from LLMs that maintain high\nperformance levels is a significant challenge.There are research\nefforts to quantize LLMs to ultra-low bit sizes, their perfor-\nmance has been found lacking and does not meet the stan-\ndards required for industrial applications[200][201]. Symbolic\nKnowledge Distillation has shown promise in specific, nar-\nrower fields such as translation, summarization, and common-\nsense reasoning. However, it must evolve into a comprehensive\nsymbolic knowledge base capable of generalizing across all\ndomains. Developing these compact models requires sophis-\nticated techniques to compress and optimize the knowledge\ntransfer without losing the nuances and depth of the original\nmodel.\nD. Effective Instruction Tuning for Diverse Applications\nInstruction tuning in AI models poses the challenge of\nadapting to a wide range of instructions and use cases. Models\nmust be versatile enough to understand and execute a variety of\ncommands accurately across different domains and contexts.\nThis requires extensive training and fine-tuning, which can\nbe resource-intensive. Moreover, ensuring that the models\nremain adaptable and up-to-date with evolving user needs\nand language usage is an ongoing challenge, necessitating\ncontinuous monitoring and updates.\nE. Adaptability and Continuous Learning\nEnsuring that distilled models can adapt to new informa-\ntion and evolving data landscapes is challenging. Continuous\nlearning mechanisms that allow models to update their knowl-\nedge without compromising efficiency or requiring complete\nretraining are essential for keeping distilled models relevant\nand effective.\nVIII. L ESSON LEARNED AND KEY TAKEAWAYS\nA. Efficiency Through Distillation\nSymbolic knowledge distillation demonstrates a powerful\nmethod to enhance the efficiency of LLMs. By distilling\ncomplex, large-scale models into smaller, more manageable\nversions without significant loss in performance, researchers\ncan achieve remarkable efficiency gains. This approach not\nonly reduces computational requirements but also makes ad-\nvanced AI capabilities more accessible for applications with\nlimited resources.\nB. Advancement in Commonsense Reasoning\nThe transition to a ’from-machine-to-corpus-to-machine’\nparadigm marks a significant advancement in commonsense\nreasoning. This innovative approach, through the creation\nof extensive and diverse datasets like ATOMIC and models\nlike NOV ACOMET, underscores the potential of machine-\ngenerated knowledge in improving AI’s understanding and\napplication of commonsense knowledge.\nC. Innovation in Data Generation and Use by Collaborating\nHuman Intelligence and Machine Capabilities\nLLMs has the potential in generating high-quality, diverse\ndatasets. These datasets serve as a foundation for training more\nrobust models, emphasizing the importance of data quality,\ndiversity, and the innovative use of symbolic knowledge in\ndataset creation. The effective collaboration between human\n16 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\noversight and automated processes in dataset generation and\ntask execution highlights the synergistic potential of combin-\ning human intelligence with machine efficiency. This collabo-\nration is key to overcoming current limitations and unlocking\nnew capabilities in AI systems.\nD. Cross-Domain Applications\nThe applications of symbolic knowledge distillation extend\nbeyond NLP into areas such as visual commonsense reasoning\nand mathematical proof solving. This cross-domain applica-\nbility showcases the versatility of distilled models and their\npotential to revolutionize various fields by enhancing model\nperformance and understanding.\nE. Instruction Tuning and Generation\nThe development and refinement of techniques for instruc-\ntion tuning and generation signify a leap towards creating\nmore user-friendly and intuitive AI systems. Models capable\nof generating their own instructions or being finely tuned to\nunderstand and execute specific commands can lead to more\nnatural and effective human-AI interactions.\nF . Challenges and Opportunities\nWhile the advancements are notable, they also underscore\nchallenges such as ensuring data quality, balancing automation\nwith human oversight, and developing compact models with-\nout compromising performance. Addressing these challenges\npresents opportunities for further research and innovation\nin model training, dataset creation, and the development of\nalgorithms for enhanced capabilities and benchmark for the\nevaluation.\nTo address the identified gaps in current research on sym-\nbolic knowledge distillation, it is crucial to first ensure the\nquality and diversity of datasets through rigorous validation\nto identify and mitigate biases inherited from LLMs, ensuring\nthe trustworthy knowledge distillation. Balancing automation\nand human oversight is also essential; effective protocols for\nhuman-machine collaboration can optimize efficiency while\nensuring nuanced cases are handled appropriately.Though the\nsize of data required for efficient distillation is still unknown,\nresearch[202] propose that only 1000 high quality human\ncurated data is enough. Another challenge is developing\ncompact models without compromising performance, which\nrequires sophisticated techniques to compress and optimize\nknowledge transfer while maintaining the depth of the original\nmodels. Effective instruction tuning for diverse applications\ndemands extensive training and fine-tuning to ensure models\ncan accurately execute various commands across domains.\nEnsuring adaptability and continuous learning in distilled\nmodels is vital, necessitating mechanisms for ongoing updates\nwithout compromising efficiency. Addressing these areas will\nadvance symbolic knowledge distillation towards more reliable\nand practical applications.\nIX. C ONCLUSION\nThis survey paper has explored the emerging and crucial\ndomain of symbolic knowledge distillation in LLMs. As\nLLMs continue to grow in scale and complexity, the need\nto effectively extract and represent their extensive knowledge\nbecomes paramount. By categorizing existing research based\non methodologies and applications, we have highlighted how\nsymbolic knowledge distillation can enhance the transparency\nand functionality of smaller, more efficient AI models. This\ncomprehensive overview underscores the significance of sym-\nbolic knowledge distillation in advancing more accessible\nand efficient AI systems. While there is a notable lack of\ncomprehensive research in this area, our survey paper fills this\ncrucial gap by offering an extensive review of the current state\nof symbolic knowledge distillation in LLMs, shedding light on\nmethodologies, challenges, and advancements in this field.\nREFERENCES\n[1] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y . Wu, A. H. Miller,\nand S. Riedel, “Language models as knowledge bases?” arXiv preprint\narXiv:1909.01066, 2019.\n[2] P. West, C. Bhagavatula, J. Hessel, J. D. Hwang, L. Jiang, R. L. Bras,\nX. Lu, S. Welleck, and Y . Choi, “Symbolic knowledge distillation:\nfrom general language models to commonsense models,”arXiv preprint\narXiv:2110.07178, 2021.\n[3] M. Sclar, P. West, S. Kumar, Y . Tsvetkov, and Y . Choi, “Ref-\neree: Reference-free sentence summarization with sharper control-\nlability through symbolic knowledge distillation,” arXiv preprint\narXiv:2210.13800, 2022.\n[4] C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts,\nA. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu et al. , “Re-\ninforced self-training (rest) for language modeling,” arXiv preprint\narXiv:2308.08998, 2023.\n[5] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,\nB. Zhang, J. Zhang, Z. Dong et al. , “A survey of large language\nmodels,” arXiv preprint arXiv:2303.18223 , 2023.\n[6] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\nE. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language\nprocessing via large pre-trained language models: A survey,” ACM\nComputing Surveys, vol. 56, no. 2, pp. 1–40, 2023.\n[7] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\nN. Akhtar, J. Wu, S. Mirjalili et al. , “Large language models: a\ncomprehensive survey of its applications, challenges, limitations, and\nfuture prospects,” Authorea Preprints, 2023.\n[8] Y . Chang, X. Wang, J. Wang, Y . Wu, K. Zhu, H. Chen, L. Yang, X. Yi,\nC. Wang, Y . Wang et al. , “A survey on evaluation of large language\nmodels,” arXiv preprint arXiv:2307.03109 , 2023.\n[9] D. Zan, B. Chen, F. Zhang, D. Lu, B. Wu, B. Guan, W. Yongji, and J.-G.\nLou, “Large language models meet nl2code: A survey,” in Proceedings\nof the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , 2023, pp. 7443–7464.\n[10] E. Kasneci, K. Sessler, S. Küchemann, M. Bannert, D. Dementieva,\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier,\nS. Krusche, G. Kutyniok, T. Michaeli, C. Nerdel, J. Pfeffer,\nO. Poquet, M. Sailer, A. Schmidt, T. Seidel, M. Stadler, J. Weller,\nJ. Kuhn, and G. Kasneci, “Chatgpt for good? on opportunities and\nchallenges of large language models for education,” Learning and\nIndividual Differences, vol. 103, p. 102274, 2023. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1041608023000195\n[11] B. AlKhamissi, M. Li, A. Celikyilmaz, M. Diab, and M. Ghazvininejad,\n“A review on language models as knowledge bases,” 2022. [Online].\nAvailable: https://arxiv.org/abs/2204.06031\n[12] S. Razniewski, A. Yates, N. Kassner, and G. Weikum, “Language\nmodels as or for knowledge bases,” arXiv preprint arXiv:2110.04888 ,\n2021.\n[13] J. Huang and K. C.-C. Chang, “Towards reasoning in large language\nmodels: A survey,” arXiv preprint arXiv:2212.10403 , 2022.\n[14] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\nand M. Du, “Explainability for large language models: A survey,”arXiv\npreprint arXiv:2309.01029, 2023.\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 17\n[15] Y . Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang,\nX. Jiang, and Q. Liu, “Aligning large language models with human: A\nsurvey,” arXiv preprint arXiv:2307.12966 , 2023.\n[16] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\nT. Zhang, F. Wu et al., “Instruction tuning for large language models:\nA survey,” arXiv preprint arXiv:2308.10792 , 2023.\n[17] X. Zhu, J. Li, Y . Liu, C. Ma, and W. Wang, “A survey on model com-\npression for large language models,” arXiv preprint arXiv:2308.07633,\n2023.\n[18] Y . Liu, Y . Yao, J.-F. Ton, X. Zhang, R. G. H. Cheng, Y . Klochkov,\nM. F. Taufiq, and H. Li, “Trustworthy llms: a survey and guideline\nfor evaluating large language models’ alignment,” arXiv preprint\narXiv:2308.05374, 2023.\n[19] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo,\nand J. M. Zhang, “Large language models for software engineering:\nSurvey and open problems,” arXiv preprint arXiv:2310.03533 , 2023.\n[20] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\n2023.\n[21] J. Wu, W. Gan, Z. Chen, S. Wan, and P. S. Yu, “Multimodal large\nlanguage models: A survey,” arXiv preprint arXiv:2311.13165 , 2023.\n[22] F. Zeng, W. Gan, Y . Wang, N. Liu, and P. S. Yu, “Large language\nmodels for robotics: A survey,”arXiv preprint arXiv:2311.07226, 2023.\n[23] Y . Zhu, H. Yuan, S. Wang, J. Liu, W. Liu, C. Deng, Z. Dou, and J.-\nR. Wen, “Large language models for information retrieval: A survey,”\narXiv preprint arXiv:2308.07107 , 2023.\n[24] A. M. Turing, Computing machinery and intelligence. Springer, 2009.\n[25] C. E. Shannon, “Prediction and entropy of printed english,” Bell system\ntechnical journal, vol. 30, no. 1, pp. 50–64, 1951.\n[26] J. Weizenbaum, “Eliza—a computer program for the study of natural\nlanguage communication between man and machine,” Communications\nof the ACM , vol. 9, no. 1, pp. 36–45, 1966.\n[27] T. Winograd, “Procedures as a representation for data in a computer\nprogram for understanding natural language,” MASSACHUSETTS\nINST OF TECH CAMBRIDGE PROJECT MAC, Tech. Rep., 1971.\n[28] G. Sampson, “A stochastic approach to parsing,” in Coling 1986 Volume\n1: The 11th International Conference on Computational Linguistics ,\n1986.\n[29] P. F. Brown, J. Cocke, S. A. Della Pietra, V . J. Della Pietra, F. Jelinek,\nJ. Lafferty, R. L. Mercer, and P. S. Roossin, “A statistical approach\nto machine translation,” Computational linguistics, vol. 16, no. 2, pp.\n79–85, 1990.\n[30] R. Rosenfeld, “Two decades of statistical language modeling: Where\ndo we go from here?” Proceedings of the IEEE , vol. 88, no. 8, pp.\n1270–1278, 2000.\n[31] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[32] Y . Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic\nlanguage model,” Advances in neural information processing systems ,\nvol. 13, 2000.\n[33] Y . Idelbayev and M. Á. Carreira-Perpiñán, “Lc: A flexible, extensible\nopen-source toolkit for model compression,” in Proceedings of the\n30th ACM International Conference on Information & Knowledge\nManagement, 2021, pp. 4504–4514.\n[34] C. Bucilu ˇa, R. Caruana, and A. Niculescu-Mizil, “Model compression,”\nin Proceedings of the 12th ACM SIGKDD international conference on\nKnowledge discovery and data mining , 2006, pp. 535–541.\n[35] Y . Cheng, D. Wang, P. Zhou, and T. Zhang, “Model compression and\nacceleration for deep neural networks: The principles, progress, and\nchallenges,” IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 126–\n136, 2018.\n[36] J. Wu, C. Leng, Y . Wang, Q. Hu, and J. Cheng, “Quantized convo-\nlutional neural networks for mobile devices,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2016,\npp. 4820–4828.\n[37] M. Courbariaux, Y . Bengio, and J.-P. David, “Binaryconnect: Train-\ning deep neural networks with binary weights during propagations,”\nAdvances in neural information processing systems , vol. 28, 2015.\n[38] V . Sindhwani, T. Sainath, and S. Kumar, “Structured transforms for\nsmall-footprint deep learning,” Advances in Neural Information Pro-\ncessing Systems, vol. 28, 2015.\n[39] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights\nand connections for efficient neural network,” Advances in neural\ninformation processing systems , vol. 28, 2015.\n[40] Y . Wang, C. Xu, C. Xu, and D. Tao, “Packing convolutional neural\nnetworks in the frequency domain,” IEEE transactions on pattern\nanalysis and machine intelligence , vol. 41, no. 10, pp. 2495–2510,\n2018.\n[41] X. Yu, T. Liu, X. Wang, and D. Tao, “On compressing deep models\nby low rank and sparse decomposition,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2017, pp.\n7370–7379.\n[42] E. L. Denton, W. Zaremba, J. Bruna, Y . LeCun, and R. Fergus,\n“Exploiting linear structure within convolutional networks for effi-\ncient evaluation,” Advances in neural information processing systems ,\nvol. 27, 2014.\n[43] Y . Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model\ncompression and acceleration for deep neural networks,” arXiv preprint\narXiv:1710.09282, 2017.\n[44] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” arXiv preprint arXiv:1503.02531 , 2015.\n[45] R. High, “The era of cognitive systems: An inside look at ibm watson\nand how it works,” IBM Corporation, Redbooks , vol. 1, p. 16, 2012.\n[46] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of\nword representations in vector space,” arXiv preprint arXiv:1301.3781,\n2013.\n[47] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\nwith neural networks,” Advances in neural information processing\nsystems, vol. 27, 2014.\n[48] J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors\nfor word representation,” in Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP). Doha,\nQatar: Association for Computational Linguistics, Oct. 2014, pp.\n1532–1543. [Online]. Available: https://aclanthology.org/D14-1162\n[49] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y . Ben-\ngio, “Fitnets: Hints for thin deep nets,” arXiv preprint arXiv:1412.6550,\n2014.\n[50] S. Zagoruyko and N. Komodakis, “Paying more attention to attention:\nImproving the performance of convolutional neural networks via atten-\ntion transfer,” arXiv preprint arXiv:1612.03928 , 2016.\n[51] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\nquestions for machine comprehension of text,” arXiv preprint\narXiv:1606.05250, 2016.\n[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems , vol. 30, 2017.\n[53] J. Yim, D. Joo, J. Bae, and J. Kim, “A gift from knowledge distillation:\nFast optimization, network minimization and transfer learning,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 4133–4141.\n[54] J. Kim, S. Park, and N. Kwak, “Paraphrasing complex network: Net-\nwork compression via factor transfer,” Advances in neural information\nprocessing systems, vol. 31, 2018.\n[55] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,” in\nProceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) . New Orleans, Louisiana:\nAssociation for Computational Linguistics, Jun. 2018, pp. 2227–2237.\n[Online]. Available: https://aclanthology.org/N18-1202\n[56] D. Cer, Y . Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John,\nN. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar et al., “Universal\nsentence encoder,” arXiv preprint arXiv:1803.11175 , 2018.\n[57] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” arXiv preprint arXiv:1804.07461 , 2018.\n[58] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[59] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving\nlanguage understanding by generative pre-training,” 2018.\n[60] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herbert-V oss,\nJ. Wu, A. Radford, G. Krueger, J. W. Kim, S. Kreps et al. , “Release\nstrategies and the social impacts of language models,” arXiv preprint\narXiv:1908.09203, 2019.\n[61] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learn-\ning with a unified text-to-text transformer,” The Journal of Machine\nLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\n[62] B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Y . Choi, “A\ncomprehensive overhaul of feature distillation,” in Proceedings of the\n18 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\nIEEE/CVF International Conference on Computer Vision , 2019, pp.\n1921–1930.\n[63] Y . Tian, D. Krishnan, and P. Isola, “Contrastive representation distilla-\ntion,” arXiv preprint arXiv:1910.10699 , 2019.\n[64] L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng, “Revisit knowledge\ndistillation: a teacher-free framework,” 2019.\n[65] Y . Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual\nlearning,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2018, pp. 4320–4328.\n[66] S. I. Mirzadeh, M. Farajtabar, A. Li, N. Levine, A. Matsukawa, and\nH. Ghasemzadeh, “Improved knowledge distillation via teacher assis-\ntant,” in Proceedings of the AAAI conference on artificial intelligence ,\nvol. 34, no. 04, 2020, pp. 5191–5198.\n[67] M. Zhai, L. Chen, F. Tung, J. He, M. Nawhal, and G. Mori, “Lifelong\ngan: Continual learning for conditional image generation,” in Proceed-\nings of the IEEE/CVF international conference on computer vision ,\n2019, pp. 2759–2768.\n[68] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation\nas a defense to adversarial perturbations against deep neural networks,”\nin 2016 IEEE symposium on security and privacy (SP) . IEEE, 2016,\npp. 582–597.\n[69] H. Lee, S. J. Hwang, and J. Shin, “Self-supervised label augmentation\nvia input transformations,” in International Conference on Machine\nLearning. PMLR, 2020, pp. 5714–5724.\n[70] M. A. Gordon and K. Duh, “Explaining sequence-level knowledge\ndistillation as data-augmentation for neural machine translation,” arXiv\npreprint arXiv:1912.03334, 2019.\n[71] J. Wang, W. Bao, L. Sun, X. Zhu, B. Cao, and S. Y . Philip, “Private\nmodel compression via knowledge distillation,” in Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 33, no. 01, 2019, pp.\n1190–1197.\n[72] T. Wang, J.-Y . Zhu, A. Torralba, and A. A. Efros, “Dataset distillation,”\narXiv preprint arXiv:1811.10959 , 2018.\n[73] O. Bohdal, Y . Yang, and T. Hospedales, “Flexible dataset distillation:\nLearn labels instead of images,” arXiv preprint arXiv:2006.08572 ,\n2020.\n[74] S. Niu, Y . Liu, J. Wang, and H. Song, “A decade survey of transfer\nlearning (2010–2020),” IEEE Transactions on Artificial Intelligence ,\nvol. 1, no. 2, pp. 151–166, 2020.\n[75] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun,\nN. Shazeer, and Z. Chen, “Gshard: Scaling giant models with\nconditional computation and automatic sharding,” arXiv preprint\narXiv:2006.16668, 2020.\n[76] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\nY . Zhou, A. W. Yu, O. Firat et al. , “Glam: Efficient scaling of\nlanguage models with mixture-of-experts,” in International Conference\non Machine Learning . PMLR, 2022, pp. 5547–5569.\n[77] P. Howard, J. Wang, V . Lal, G. Singer, Y . Choi, and S. Swayamdipta,\n“Neurocomparatives: Neuro-symbolic distillation of comparative\nknowledge,” arXiv preprint arXiv:2305.04978 , 2023.\n[78] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\nfor neural language models,” arXiv preprint arXiv:2001.08361 , 2020.\n[79] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clarket al.,\n“Training compute-optimal large language models,” arXiv preprint\narXiv:2203.15556, 2022.\n[80] P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and\nA. Ho, “Will we run out of data? an analysis of the limits of scaling\ndatasets in machine learning,” arXiv preprint arXiv:2211.04325, 2022.\n[81] S. Wang, Y . Liu, Y . Xu, C. Zhu, and M. Zeng, “Want to reduce labeling\ncost? gpt-3 can help,” arXiv preprint arXiv:2108.13487 , 2021.\n[82] R. Smith, J. A. Fries, B. Hancock, and S. H. Bach, “Language models\nin the loop: Incorporating prompting into weak supervision,” arXiv\npreprint arXiv:2205.02318, 2022.\n[83] S. Chaudhary, “Code alpaca: An instruction-following llama model for\ncode generation,” Code alpaca: An instruction-following llama model\nfor code generation , 2023.\n[84] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin,\nS. Chen, and D. Zhang, “Wizardmath: Empowering mathematical\nreasoning for large language models via reinforced evol-instruct,”arXiv\npreprint arXiv:2308.09583, 2023.\n[85] N. Ding, Y . Chen, B. Xu, Y . Qin, Z. Zheng, S. Hu, Z. Liu, M. Sun,\nand B. Zhou, “Enhancing chat language models by scaling high-quality\ninstructional conversations,” arXiv preprint arXiv:2305.14233 , 2023.\n[86] Y . Jiang, C. Chan, M. Chen, and W. Wang, “Lion: Adversarial\ndistillation of closed-source large language model,” arXiv preprint\narXiv:2305.12870, 2023.\n[87] W. Yuan, R. Y . Pang, K. Cho, S. Sukhbaatar, J. Xu, and J. Weston,\n“Self-rewarding language models,” arXiv preprint arXiv:2401.10020 ,\n2024.\n[88] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\ntext-to-text transformer,” arXiv preprint arXiv:2010.11934 , 2020.\n[89] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\nA. M. Dai, and Q. V . Le, “Finetuned language models are zero-shot\nlearners,” arXiv preprint arXiv:2109.01652 , 2021.\n[90] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\nCheng, A. Jin, T. Bos, L. Baker, Y . Duet al., “Lamda: Language models\nfor dialog applications,” arXiv preprint arXiv:2201.08239 , 2022.\n[91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\nV . Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Soloet al., “Solv-\ning quantitative reasoning problems with language models,” Advances\nin Neural Information Processing Systems , vol. 35, pp. 3843–3857,\n2022.\n[92] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\nChung, D. Bahri, T. Schuster, S. Zheng et al., “Ul2: Unifying language\nlearning paradigms,” in The Eleventh International Conference on\nLearning Representations, 2022.\n[93] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmannet al., “Palm: Scaling\nlanguage modeling with pathways,” Journal of Machine Learning\nResearch, vol. 24, no. 240, pp. 1–113, 2023.\n[94] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li,\nX. Wang, M. Dehghani, S. Brahmaet al., “Scaling instruction-finetuned\nlanguage models,” arXiv preprint arXiv:2210.11416 , 2022.\n[95] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[96] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[97] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nH. Edwards, Y . Burda, N. Joseph, G. Brockmanet al., “Evaluating large\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374 ,\n2021.\n[98] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-\nassisted question-answering with human feedback,” arXiv preprint\narXiv:2112.09332, 2021.\n[99] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\nmodels to follow instructions with human feedback,” Advances in\nNeural Information Processing Systems , vol. 35, pp. 27 730–27 744,\n2022.\n[100] R. OpenAI, “Gpt-4 technical report. arxiv 2303.08774,” View in Article,\nvol. 2, p. 13, 2023.\n[101] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter\nautoregressive language model,” 2021.\n[102] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman,\n“GPT-Neo: Large Scale Autoregressive Language Modeling with\nMesh-Tensorflow,” Mar. 2021, If you use this software, please cite it\nusing these metadata. [Online]. Available: https://doi.org/10.5281/ze\nnodo.5297715\n[103] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Gold-\ning, H. He, C. Leahy, K. McDonell, J. Phang et al. , “Gpt-neox-\n20b: An open-source autoregressive language model,” arXiv preprint\narXiv:2204.06745, 2022.\n[104] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\nmodels: Methods, analysis & insights from training gopher,” arXiv\npreprint arXiv:2112.11446, 2021.\n[105] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al. , “Competition-\nlevel code generation with alphacode,” Science, vol. 378, no. 6624, pp.\n1092–1097, 2022.\n[106] A. Glaese, N. McAleese, M. Tr˛ ebacz, J. Aslanides, V . Firoiu, T. Ewalds,\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al. , “Improving\nalignment of dialogue agents via targeted human judgements,” arXiv\npreprint arXiv:2209.14375, 2022.\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 19\n[107] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\navia, A. Poulton, V . Kerkez, and R. Stojnic, “Galactica: A large\nlanguage model for science,” arXiv preprint arXiv:2211.09085 , 2022.\n[108] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\nC. Dewan, M. Diab, X. Li, X. V . Lin et al. , “Opt: Open pre-trained\ntransformer language models,” arXiv preprint arXiv:2205.01068, 2022.\n[109] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\nter, T. Wang, Q. Liu, P. S. Koura et al. , “Opt-iml: Scaling language\nmodel instruction meta learning through the lens of generalization,”\narXiv preprint arXiv:2212.12017 , 2022.\n[110] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\nOpen and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[111] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja, M. Dey, M. S.\nBari, C. Xu, U. Thakker, S. Sharma, E. Szczechla, T. Kim,\nG. Chhablani, N. V . Nayak, D. Datta, J. Chang, M. T. Jiang,\nH. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden,\nT. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Févry,\nJ. A. Fries, R. Teehan, S. Biderman, L. Gao, T. Bers, T. Wolf, and\nA. M. Rush, “Multitask prompted training enables zero-shot task\ngeneralization,” CoRR, vol. abs/2110.08207, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2110.08207\n[112] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c,\nD. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon et al. , “Bloom:\nA 176b-parameter open-access multilingual language model,” arXiv\npreprint arXiv:2211.05100, 2022.\n[113] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf\net al., “Crosslingual generalization through multitask finetuning,” arXiv\npreprint arXiv:2211.01786, 2022.\n[114] Y . Sun, S. Wang, Y . Li, S. Feng, H. Tian, H. Wu, and\nH. Wang, “ERNIE 2.0: A continual pre-training framework for\nlanguage understanding,” CoRR, vol. abs/1907.12412, 2019. [Online].\nAvailable: http://arxiv.org/abs/1907.12412\n[115] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\nY . Zhao, Y . Lu et al. , “Ernie 3.0: Large-scale knowledge enhanced\npre-training for language understanding and generation,” arXiv preprint\narXiv:2107.02137, 2021.\n[116] S. Wang, Y . Sun, Y . Xiang, Z. Wu, S. Ding, W. Gong, S. Feng,\nJ. Shang, Y . Zhao, C. Pang et al. , “Ernie 3.0 titan: Exploring larger-\nscale knowledge enhanced pre-training for language understanding and\ngeneration,” arXiv preprint arXiv:2112.12731 , 2021.\n[117] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A\nsurvey,” International Journal of Computer Vision, vol. 129, pp. 1789–\n1819, 2021.\n[118] G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker, “Learning ef-\nficient object detection models with knowledge distillation,” Advances\nin neural information processing systems , vol. 30, 2017.\n[119] F. Zhang, X. Zhu, and M. Ye, “Fast human pose estimation,” in\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2019, pp. 3517–3526.\n[120] Y . Bengio, A. Courville, and P. Vincent, “Representation learning: A\nreview and new perspectives,” IEEE transactions on pattern analysis\nand machine intelligence , vol. 35, no. 8, pp. 1798–1828, 2013.\n[121] D. Chen, J.-P. Mei, Y . Zhang, C. Wang, Z. Wang, Y . Feng, and C. Chen,\n“Cross-layer distillation with semantic calibration,” in Proceedings of\nthe AAAI Conference on Artificial Intelligence , vol. 35, no. 8, 2021,\npp. 7028–7036.\n[122] Z. Huang and N. Wang, “Like what you like: Knowledge distill via\nneuron selectivity transfer,” arXiv preprint arXiv:1707.01219 , 2017.\n[123] N. Passalis and A. Tefas, “Learning deep representations with proba-\nbilistic knowledge transfer,” in Proceedings of the European Confer-\nence on Computer Vision (ECCV) , 2018, pp. 268–284.\n[124] X. Jin, B. Peng, Y . Wu, Y . Liu, J. Liu, D. Liang, J. Yan, and\nX. Hu, “Knowledge distillation via route constrained optimization,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 1345–1354.\n[125] B. Heo, M. Lee, S. Yun, and J. Y . Choi, “Knowledge transfer via\ndistillation of activation boundaries formed by hidden neurons,” in\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 33,\nno. 01, 2019, pp. 3779–3787.\n[126] S. H. Lee, D. H. Kim, and B. C. Song, “Self-supervised knowledge\ndistillation using singular value decomposition,” in Proceedings of the\nEuropean conference on computer vision (ECCV) , 2018, pp. 335–350.\n[127] C. Zhang and Y . Peng, “Better and faster: knowledge transfer from\nmultiple self-supervised learning tasks via graph distillation for video\nclassification,” arXiv preprint arXiv:1804.10069 , 2018.\n[128] S. Lee and B. C. Song, “Graph-based knowledge distillation by multi-\nhead attention network,” arXiv preprint arXiv:1907.02226 , 2019.\n[129] N. Passalis, M. Tzelepi, and A. Tefas, “Heterogeneous knowledge\ndistillation using information flow modeling,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 2339–2348.\n[130] G. Montavon, A. Binder, S. Lapuschkin, W. Samek, and K.-R. Müller,\n“Layer-wise relevance propagation: an overview,” Explainable AI:\ninterpreting, explaining and visualizing deep learning , pp. 193–209,\n2019.\n[131] S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting\nmodel predictions,” Advances in neural information processing systems,\nvol. 30, 2017.\n[132] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and\ncomprehension,” arXiv preprint arXiv:1910.13461 , 2019.\n[133] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé et al., “Bloom: A 176b-\nparameter open-access multilingual language model,” arXiv preprint\narXiv:2211.05100, 2022.\n[134] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\nW. Zheng, X. Xia et al. , “Glm-130b: An open bilingual pre-trained\nmodel,” arXiv preprint arXiv:2210.02414 , 2022.\n[135] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Gar-\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399 ,\n2022.\n[136] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al. ,\n“Scaling laws and interpretability of learning from repeated data,”arXiv\npreprint arXiv:2205.10487, 2022.\n[137] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V . Mikulik, and G. Irv-\ning, “Alignment of language agents,” arXiv preprint arXiv:2103.14659,\n2021.\n[138] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, and\nI. Sutskever, “Formal mathematics statement curriculum learning,”\narXiv preprint arXiv:2202.01344 , 2022.\n[139] M. Sung, J. Lee, S. Yi, M. Jeon, S. Kim, and J. Kang, “Can\nlanguage models be biomedical knowledge bases?” arXiv preprint\narXiv:2109.07154, 2021.\n[140] L. Z. Liu, Y . Wang, J. Kasai, H. Hajishirzi, and N. A. Smith, “Probing\nacross time: What does roberta know and when?” arXiv preprint\narXiv:2104.07885, 2021.\n[141] Y . Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy,\nH. Schütze, and Y . Goldberg, “Measuring and improving consistency\nin pretrained language models,” Transactions of the Association for\nComputational Linguistics, vol. 9, pp. 1012–1031, 2021.\n[142] N. Kassner and H. Schütze, “Negated and misprimed probes for\npretrained language models: Birds can talk, but cannot fly,” in\nProceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics . Online: Association for Computational\nLinguistics, Jul. 2020, pp. 7811–7818. [Online]. Available: https:\n//aclanthology.org/2020.acl-main.698\n[143] C. Zhu, A. S. Rawat, M. Zaheer, S. Bhojanapalli, D. Li, F. Yu, and\nS. Kumar, “Modifying memories in transformer models,”arXiv preprint\narXiv:2012.00363, 2020.\n[144] D. Dai, L. Dong, Y . Hao, Z. Sui, B. Chang, and F. Wei, “Knowledge\nneurons in pretrained transformers,” arXiv preprint arXiv:2104.08696 ,\n2021.\n[145] N. De Cao, W. Aziz, and I. Titov, “Editing factual knowledge in\nlanguage models,” arXiv preprint arXiv:2104.08164 , 2021.\n[146] P. Hase, M. Diab, A. Celikyilmaz, X. Li, Z. Kozareva, V . Stoyanov,\nM. Bansal, and S. Iyer, “Do language models have beliefs? methods\nfor detecting, updating, and visualizing model beliefs,” arXiv preprint\narXiv:2111.13654, 2021.\n[147] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning, “Fast\nmodel editing at scale,” arXiv preprint arXiv:2110.11309 , 2021.\n[148] J. Jang, S. Ye, S. Yang, J. Shin, J. Han, G. Kim, S. J. Choi, and\nM. Seo, “Towards continual knowledge learning of language models,”\narXiv preprint arXiv:2110.03215 , 2021.\n[149] N. Kassner, B. Krojer, and H. Schütze, “Are pretrained lan-\nguage models symbolic reasoners over knowledge?” arXiv preprint\narXiv:2006.10413, 2020.\n20 IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020\n[150] P. Clark, O. Tafjord, and K. Richardson, “Transformers as soft reason-\ners over language,” arXiv preprint arXiv:2002.05867 , 2020.\n[151] S. Saha, S. Ghosh, S. Srivastava, and M. Bansal, “Prover: Proof\ngeneration for interpretable reasoning over rules,” arXiv preprint\narXiv:2010.02830, 2020.\n[152] O. Tafjord, B. D. Mishra, and P. Clark, “Proofwriter: Generating\nimplications, proofs, and abductive statements over natural language,”\narXiv preprint arXiv:2012.13048 , 2020.\n[153] N. Gontier, K. Sinha, S. Reddy, and C. Pal, “Measuring systematic\ngeneralization in neural proof generation with transformers,” Advances\nin Neural Information Processing Systems , vol. 33, pp. 22 231–22 242,\n2020.\n[154] K. Richardson and A. Sabharwal, “Pushing the limits of rule reasoning\nin transformers through natural language satisfiability,” in Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022,\npp. 11 209–11 219.\n[155] M. Saeed, N. Ahmadi, P. Nakov, and P. Papotti, “Rulebert: Teach-\ning soft rules to pre-trained language models,” arXiv preprint\narXiv:2109.13006, 2021.\n[156] D. Saxton, E. Grefenstette, F. Hill, and P. Kohli, “Analysing\nmathematical reasoning abilities of neural models,” arXiv preprint\narXiv:1904.01557, 2019.\n[157] P. Banerjee, S. Mishra, K. K. Pal, A. Mitra, and C. Baral, “Common-\nsense reasoning with implicit knowledge in natural language,” in 3rd\nConference on Automated Knowledge Base Construction , 2021.\n[158] P. Zhou, R. Khanna, S. Lee, B. Y . Lin, D. Ho, J. Pujara, and X. Ren,\n“Rica: Evaluating robust inference capabilities based on commonsense\naxioms,” arXiv preprint arXiv:2005.00782 , 2020.\n[159] N. F. Rajani, B. McCann, C. Xiong, and R. Socher, “Explain your-\nself! leveraging language models for commonsense reasoning,” arXiv\npreprint arXiv:1906.02361, 2019.\n[160] S. Jain and B. C. Wallace, “Attention is not explanation,” arXiv preprint\narXiv:1902.10186, 2019.\n[161] S. Wiegreffe and Y . Pinter, “Attention is not not explanation,” arXiv\npreprint arXiv:1908.04626, 2019.\n[162] S. Serrano and N. A. Smith, “Is attention interpretable?” arXiv preprint\narXiv:1906.03731, 2019.\n[163] S. Zhao, D. Pascual, G. Brunner, and R. Wattenhofer, “Of non-linearity\nand commutativity in bert,” in 2021 International Joint Conference on\nNeural Networks (IJCNN) . IEEE, 2021, pp. 1–8.\n[164] M. Geva, R. Schuster, J. Berant, and O. Levy, “Transformer\nfeed-forward layers are key-value memories,” arXiv preprint\narXiv:2012.14913, 2020.\n[165] K. Meng, D. Bau, A. Andonian, and Y . Belinkov, “Locating and editing\nfactual associations in gpt,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 17 359–17 372, 2022.\n[166] X. Han, B. C. Wallace, and Y . Tsvetkov, “Explaining black box\npredictions and unveiling data artifacts through influence functions,”\narXiv preprint arXiv:2005.06676 , 2020.\n[167] P. Pezeshkpour, S. Jain, B. C. Wallace, and S. Singh, “An empirical\ncomparison of instance attribution methods for nlp,” arXiv preprint\narXiv:2104.04128, 2021.\n[168] P. Pezeshkpour, S. Jain, S. Singh, and B. C. Wallace, “Combining\nfeature and instance attribution to detect artifacts,” arXiv preprint\narXiv:2107.00323, 2021.\n[169] H. Zylberajch, P. Lertvittayakumjorn, and F. Toni, “Hildif: Interactive\ndebugging of nli models using influence functions,” in Proceedings\nof the First Workshop on Interactive Learning for Natural Language\nProcessing, 2021, pp. 1–6.\n[170] S. Narang, C. Raffel, K. Lee, A. Roberts, N. Fiedel, and K. Malkan,\n“Wt5?! training text-to-text models to explain their predictions,” arXiv\npreprint arXiv:2004.14546, 2020.\n[171] O.-M. Camburu, B. Shillingford, P. Minervini, T. Lukasiewicz, and\nP. Blunsom, “Make up your mind! adversarial generation of inconsis-\ntent natural language explanations,” arXiv preprint arXiv:1910.03065 ,\n2019.\n[172] F. Friedrich, P. Schramowski, C. Tauchmann, and K. Kersting, “In-\nteractively providing explanations for transformer language models,”\narXiv preprint arXiv:2110.02058 , 2021.\n[173] T. Lei, R. Barzilay, and T. Jaakkola, “Rationalizing neural predictions,”\narXiv preprint arXiv:1606.04155 , 2016.\n[174] B. Paranjape, J. Michael, M. Ghazvininejad, L. Zettlemoyer, and\nH. Hajishirzi, “Prompting contrastive explanations for commonsense\nreasoning tasks,” arXiv preprint arXiv:2106.06823 , 2021.\n[175] M. Sap, R. Le Bras, E. Allaway, C. Bhagavatula, N. Lourie, H. Rashkin,\nB. Roof, N. A. Smith, and Y . Choi, “Atomic: An atlas of machine\ncommonsense for if-then reasoning,” in Proceedings of the AAAI\nconference on artificial intelligence , vol. 33, no. 01, 2019, pp. 3027–\n3035.\n[176] P. West, R. L. Bras, T. Sorensen, B. Y . Lin, L. Jiang, X. Lu, K. Chandu,\nJ. Hessel, A. Baheti, C. Bhagavatula et al. , “Novacomet: Open com-\nmonsense foundation models with symbolic knowledge distillation,”\narXiv preprint arXiv:2312.05979 , 2023.\n[177] C. Bhagavatula, J. D. Hwang, D. Downey, R. L. Bras, X. Lu, K. Sak-\naguchi, S. Swayamdipta, P. West, and Y . Choi, “I2d2: Inductive knowl-\nedge distillation with neurologic and self-imitation,” arXiv preprint\narXiv:2212.09246, 2022.\n[178] Y . Fu, H. Peng, L. Ou, A. Sabharwal, and T. Khot, “Specializing\nsmaller language models towards multi-step reasoning,” arXiv preprint\narXiv:2301.12726, 2023.\n[179] J. S. Park, J. Hessel, K. R. Chandu, P. P. Liang, X. Lu, P. West,\nY . Yu, Q. Huang, J. Gao, A. Farhadi et al. , “Localized symbolic\nknowledge distillation for visual commonsense models,” arXiv preprint\narXiv:2312.04837, 2023.\n[180] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\nand H. Hajishirzi, “Self-instruct: Aligning language model with self\ngenerated instructions,” arXiv preprint arXiv:2212.10560 , 2022.\n[181] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\nand T. B. Hashimoto, “Stanford alpaca: An instruction-following llama\nmodel,” https://github.com/tatsu-lab/stanford_alpaca, 2023.\n[182] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and\nD. Jiang, “Wizardlm: Empowering large language models to follow\ncomplex instructions,” arXiv preprint arXiv:2304.12244 , 2023.\n[183] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang,\nL. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, I. Stoica, and\nE. P. Xing, “Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality,” March 2023. [Online]. Available:\nhttps://lmsys.org/blog/2023-03-30-vicuna/\n[184] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\nand D. Song, “Koala: A dialogue model for academic research,” Blog\npost, April 2023. [Online]. Available: https://bair.berkeley.edu/blog/2\n023/04/03/koala/\n[185] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha, K. Bhatia, I. Chami,\nF. Sala, and C. Ré, “Ask me anything: A simple strategy for prompting\nlanguage models,” arXiv preprint arXiv:2210.02441 , 2022.\n[186] P. Agrawal, C. Alberti, F. Huot, J. Maynez, J. Ma, S. Ruder,\nK. Ganchev, D. Das, and M. Lapata, “Qameleon: Multilingual qa with\nonly 5 examples,” arXiv preprint arXiv:2211.08264 , 2022.\n[187] C.-Y . Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y . Fujii, A. Ratner,\nR. Krishna, C.-Y . Lee, and T. Pfister, “Distilling step-by-step! outper-\nforming larger language models with less training data and smaller\nmodel sizes,” arXiv preprint arXiv:2305.02301 , 2023.\n[188] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\nA. Awadallah, “Orca: Progressive learning from complex explanation\ntraces of gpt-4,” arXiv preprint arXiv:2306.02707 , 2023.\n[189] A. Mitra, L. Del Corro, S. Mahajan, A. Codas, C. Simoes, S. Agrawal,\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal et al. , “Orca\n2: Teaching small language models how to reason,” arXiv preprint\narXiv:2311.11045, 2023.\n[190] J. Renkhoff, K. Feng, M. Meier-Doernberg, A. Velasquez, and H. H.\nSong, “A survey on verification and validation, testing and evaluations\nof neurosymbolic artificial intelligence,” IEEE Transactions on Artifi-\ncial Intelligence, 2024.\n[191] K. Acharya, W. Raza, C. Dourado, A. Velasquez, and H. H. Song,\n“Neurosymbolic reinforcement learning and planning: A survey,” IEEE\nTransactions on Artificial Intelligence , 2023.\n[192] A. Velasquez, “Transfer from imprecise and abstract models to au-\ntonomous technologies (tiamat),” Defense Advanced Research Projects\nAgency (DARPA) Program Solicitation, 2023.\n[193] Q. Huang, M. Tao, C. Zhang, Z. An, C. Jiang, Z. Chen, Z. Wu,\nand Y . Feng, “Lawyer llama technical report,” arXiv preprint\narXiv:2305.15062, 2023.\n[194] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\nlegal large language model with integrated external knowledge bases,”\narXiv preprint arXiv:2306.16092 , 2023.\n[195] H. Zhang, J. Chen, F. Jiang, F. Yu, Z. Chen, J. Li, G. Chen, X. Wu,\nZ. Zhang, Q. Xiao et al., “Huatuogpt, towards taming language model\nto be a doctor,” arXiv preprint arXiv:2305.15075 , 2023.\n[196] Y . Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y . Zhang, “Chatdoctor:\nA medical chat model fine-tuned on a large language model meta-ai\n(llama) using medical domain knowledge,”Cureus, vol. 15, no. 6, 2023.\n[197] X. Zhang and Q. Yang, “Xuanyuan 2.0: A large chinese financial chat\nmodel with hundreds of billions parameters,” in Proceedings of the\nACHARY Aet al.: A SURVEY ON SYMBOLIC KNOWLEDGE DISTILLATION OF LARGE LANGUAGE MODEL 21\n32nd ACM International Conference on Information and Knowledge\nManagement, 2023, pp. 4435–4439.\n[198] T. Xie, Y . Wan, W. Huang, Z. Yin, Y . Liu, S. Wang, Q. Linghu, C. Kit,\nC. Grazian, W. Zhang et al. , “Darwin series: Domain specific large\nlanguage models for natural science,” arXiv preprint arXiv:2308.13565,\n2023.\n[199] D. Zhang, Z. Hu, S. Zhoubian, Z. Du, K. Yang, Z. Wang, Y . Yue,\nY . Dong, and J. Tang, “Sciglm: Training scientific language models\nwith self-reflective instruction annotation and tuning,” arXiv preprint\narXiv:2401.07950, 2024.\n[200] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao,\nY . Qiao, and P. Luo, “Omniquant: Omnidirectionally calibrated quan-\ntization for large language models,” arXiv preprint arXiv:2308.13137 ,\n2023.\n[201] Y . Shang, Z. Yuan, Q. Wu, and Z. Dong, “Pb-llm: Partially binarized\nlarge language models,” arXiv preprint arXiv:2310.00034 , 2023.\n[202] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat, P. Yu,\nL. Yu et al., “Lima: Less is more for alignment,” Advances in Neural\nInformation Processing Systems , vol. 36, 2024.\nKamal Acharya (Graduate Student Member, IEEE)\nreceived his Engineering degree in Electronics and\nCommunication Engineering from Tribhuvan Uni-\nversity, Kathmandu, Nepal in 2011 and Masters\ndegree in Information System Engineering from\nPurbanchal University, Kathmandu, Nepal in 2019.\nCurrently, he is pursuing PhD. in the Information\nSystems from University of Maryland, Baltimore\nCounty (UMBC), Baltimore, MD.\nHe has been involved in teaching profession for\nabout 7 years in the various universities of Nepal,\nTribhuvan University and Purbanchal Univesity were among few of them.\nHe is mainly associated with the courses like programming(C,C++,Python),\nComputer Networks and Computer Architecture. He is working as Graduate\nResearch Assistant in UMBC. He is also serving as an reviewer for IEEE\nTransactions on Artificial Intelligence, IEEE Transactions on Intelligent Trans-\nportation Systems and IEEE SMC Magazine. His preferred areas of research\nare Natural Language Processing(NLP), Deep Learning and Reinforcement\nLearning.\nAlvaro Velasquezis a program manager in the Inno-\nvation Information Office (I2O) of the Defense Ad-\nvanced Research Projects Agency (DARPA), where\nhe currently leads the Assured Neuro-Symbolic\nLearning and Reasoning (ANSR) program. Before\nthat, Alvaro oversaw the machine intelligence port-\nfolio of investments for the Information Directorate\nof the Air Force Research Laboratory (AFRL). Al-\nvaro received his PhD in Computer Science from\nthe University of Central Florida in 2018 and is a\nrecipient of the AAAI Distinguished Paper Award,\nthe National Science Foundation Graduate Research Fellowship Program\n(NSF GRFP) award, the University of Central Florida 30 Under 30 award, and\nbest paper and patent awards from AFRL. He has co-authored over 80 papers\nand two patents and serves as Associate Editor of the IEEE Transactions on\nArtificial Intelligence.\nHoubing Herbert Song (M’12–SM’14-F’23) re-\nceived the Ph.D. degree in electrical engineering\nfrom the University of Virginia, Charlottesville, V A,\nin August 2012.\nHe is currently a Professor, the Founding Director\nof the NSF Center for Aviation Big Data Analytics\n(Planning), the Associate Director for Leadership of\nthe DOT Transportation Cybersecurity Center for\nAdvanced Research and Education (Tier 1 Center),\nand the Director of the Security and Optimiza-\ntion for Networked Globe Laboratory (SONG Lab,\nwww.SONGLab.us), University of Maryland, Baltimore County (UMBC),\nBaltimore, MD. He is a Distinguished Visiting Fellow of the Scottish Informat-\nics and Computer Science Alliance (SICSA). Prior to joining UMBC, he was a\nTenured Associate Professor of Electrical Engineering and Computer Science\nat Embry-Riddle Aeronautical University, Daytona Beach, FL. He serves as an\nAssociate Editor for IEEE Transactions on Artificial Intelligence (TAI) (2023-\npresent), IEEE Internet of Things Journal (2020-present), IEEE Transactions\non Intelligent Transportation Systems (2021-present), and IEEE Journal on\nMiniaturization for Air and Space Systems (J-MASS) (2020-present). He was\nan Associate Technical Editor for IEEE Communications Magazine (2017-\n2020). He is the editor of ten books, the author of more than 100 articles\nand the inventor of 2 patents. His research interests include AI/machine\nlearning/big data analytics, cyber-physical systems/internet of things, and\ncybersecurity and privacy. His research has been sponsored by federal agencies\n(including National Science Foundation, National Aeronautics and Space\nAdministration, US Department of Transportation, and Federal Aviation\nAdministration, among others) and industry. His research has been featured\non popular news media outlets, including IEEE Spectrum, IEEE GlobalSpec’s\nEngineering360, IEEE Transmitter, insideBIGDATA, Association for Un-\ncrewed Vehicle Systems International (AUVSI), Security Magazine, CXOTech\nMagazine, Fox News, U.S. News & World Report, The Washington Times,\nand New Atlas.\nDr. Song is an IEEE Fellow, an Asia-Pacific Artificial Intelligence Associ-\nation (AAIA) Fellow, an ACM Distinguished Member, and a Full Member of\nSigma Xi. Dr. Song has been a Highly Cited Researcher identified by Web\nof Science since 2021. He is an ACM Distinguished Speaker (2020-present),\nan IEEE Computer Society Distinguished Visitor (2024-present), an IEEE\nCommunications Society (ComSoc) Distinguished Lecturer (2024-present),\nan IEEE Intelligent Transportation Systems Society (ITSS) Distinguished\nLecturer (2024-present), an IEEE Vehicular Technology Society (VTS) Distin-\nguished Lecturer (2023-present) and an IEEE Systems Council Distinguished\nLecturer (2023-present). Dr. Song received Research.com Rising Star of\nScience Award in 2022, 2021 Harry Rowe Mimno Award bestowed by IEEE\nAerospace and Electronic Systems Society, and 10+ Best Paper Awards from\nmajor international conferences, including IEEE CPSCom-2019, IEEE ICII\n2019, IEEE/AIAA ICNS 2019, IEEE CBDCom 2020, W ASA 2020, AIAA/\nIEEE DASC 2021, IEEE GLOBECOM 2021 and IEEE INFOCOM 2022. He\nhas been an IEEE Impact Creator since 2023."
}