{
  "title": "Inter-document Contextual Language model",
  "url": "https://openalex.org/W2472672021",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2664361591",
      "name": "Quan Hung Tran",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2329377932",
      "name": "Ingrid Zukerman",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1432492132",
      "name": "Gholamreza Haffari",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2207587218",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1606347560",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2613332842",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W126222424",
    "https://openalex.org/W2402268235"
  ],
  "abstract": "Quan Hung Tran, Ingrid Zukerman, Gholamreza Haffari. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",
  "full_text": "Proceedings of NAACL-HLT 2016, pages 762–766,\nSan Diego, California, June 12-17, 2016.c⃝2016 Association for Computational Linguistics\nInter-document Contextual Language Model\nQuan Hung Tranand Ingrid Zukerman and Gholamreza Haffari\nFaculty of Information Technology\nMonash University, Australia\nhung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu\nAbstract\nIn this paper, we examine the impact of\nemploying contextual, structural information\nfrom a tree-structured document set to derive a\nlanguage model. Our results show that this in-\nformation signiﬁcantly improves the accuracy\nof the resultant model.\n1 Introduction\nConventional Language Models (LMs) are based on\nn-grams, and thus rely upon a limited number of\npreceding words to assign a probability to the next\nword in a document. Recently, Mikolovet al. (2010)\nproposed a Recurrent Neural Network (RNN) LM\nwhich uses a vector representation of all the pre-\nceding words in a sentence as the context for lan-\nguage modeling. This model, which theoretically\ncan utilize an inﬁnite context window within a sen-\ntence, yields an LM with lower perplexity than that\nof n-gram-based LMs. However, the model does not\nleverage the wider contextual information provided\nby words in other sentences in a document or in re-\nlated documents.\nSeveral researchers have explored extending the\ncontextual information of an RNN-based LM.\nMikolov and Zweig (2012) proposed a context-\ndependent RNN LM that employs Latent Dirich-\nlet Allocation for modeling a long span of context.\nWang and Cho (2015) offered a bag-of-words repre-\nsentation of preceding sentences as the context for\nthe RNN LM. Ji et al. (2015) used a Document-\nContext LM (DCLM) to leverage both intra- and\ninter-sentence context.\nThese works focused on contextual information\nat the document level for LM, but did not con-\nsider information at the inter-document level. Many\ndocument sets on the Internet are structured, which\nmeans there are connections between different docu-\nments. This phenomenon is prominent in social me-\ndia, where all the posts are directly linked to several\nother posts. We posit that these related documents\ncould hold important information about a particu-\nlar post, including the topic and language use, and\npropose an RNN-based LM architecture that utilizes\nboth intra- and inter-document contextual informa-\ntion. Our approach, which was tested on the social\nmedia dataset reddit, yielded promising results,\nwhich signiﬁcantly improve on the state of the art.\n2 Dataset\nWe used pre-collected reddit data,1 which as of\nDecember, 2015, consists of approximately 1.7 bil-\nlion comments in JSON format. A comment thread\nstarts with a “topic”, which might be a link or an im-\nage. The users then begin to comment on the topic,\nor reply to previous comments. Over time, this pro-\ncess creates a tree-structured document repository\n(Figure 1), where a level indicator is assigned to\neach comment, e.g., a response to the root topic is\nassigned level 1, and the reply to a level n com-\nment is assigned level n+ 1. We parsed the raw\ndata in JSON format into a tree structure, removing\nthreads that have less than three comments, contain\ndeleted comments, or do not have comments above\n1https://www.reddit.com/r/datasets/\ncomments/3bxlg7/i_have_every_publicly_\navailable_reddit_comment\n762\nFigure 1: reddit example\nTable 1: Dataset statistics\n# of # of # of # of\nthreads posts sentences tokens\ntraining 1500 14592 40709 648624\ntesting 500 5007 13612 217164\nvalidation 100 968 2762 44575\nlevel 2. We randomly selected 2100 threads that ﬁt\nthese criteria. The data were then split into train-\ning/testing/validation sets. Table 1 displays some\nstatistics of our dataset.\n3 Baseline Neural Language Models\nOur inter-document contextual language model\nscaffolds on the RNN LM (Mikolov et al., 2010) and\nDCLMs (Ji et al., 2015), as described below.\nRNN-LSTM. Given a sentence {xt}t∈[1,...,N],\nwhere xt is the vector representation of thet-th word\nin the sentence, and N is the length of the sentence,\nMikolov et al.’s (2010) RNN LM can be deﬁned as:\nht = f(ht−1,xt) (1)\nyt ∼softmax (Woht−1 + bo) (2)\nwhere ht is the hidden unit at word t, and yt is\nthe prediction of the t-th word given the previous\nhidden unit ht−1. The function f in Equation 1\ncan be any non-linear function. Following the ap-\nproach in (Sundermeyer et al., 2012) and (Ji et al.,\n2015), we make use of Long Short-Term Memory\n(LSTM) units (Hochreiter and Schmidhuber, 1997)\nrather than the simple hidden units used in the orig-\ninal RNN LM. In our work, the word representation\nxt is obtained from the one-hot representation using\nan afﬁne transformation, as follows:\nxt = Wpot + bp (3)\nwhere ot is the one-hot representation, Wp is the\nprojection matrix, and bp is a bias term.\nDocument Context LMs (DCLMs). We re-\nimplemented two of Ji et al.’s (2015) DCLMs as our\nbaselines,2 viz Context-to-context (Figure 2a) and\nContext-to-output (Figure 2b). These models extend\nthe RNN-LSTM model by leveraging information\nfrom preceding sentences.\nThe context-to-context model (ccDCLM) con-\ncatenates the ﬁnal hidden unit of the previous sen-\ntence with the word vectors of the current sentence.\nThus, Equation 1 becomes:\nhi\nt = f(hi\nt−1,x′\ni,t) (4)\nx′\ni,t = concat\n(\nxi,t,hi−1\nNi−1\n)\n(5)\nwhere Ni−1 is the length of the previous sentence\nin the document, xi,t is the vector representation of\nthe t-th word in the i-th sentence, x′\ni,t is the con-\ncatenation of the vector representation xi,t and the\nprevious sentence’s ﬁnal hidden unithi−1\nNi−1 .\nThe context-to-output model (coDCLM) applies\nthe additional information directly to the word-\ndecoding phase. Thus, Equation 2 becomes:\nyi,t ∼softmax\n(\nWohi\nt−1 + W ′\nohi−1\nNi−1 + bo\n)\n(6)\n4 Inter-document Context Language\nModel\nWe now extend the DCLM by leveraging the in-\nformation at the inter-document level, taking ad-\nvantage of the structure of the repository — a tree\nin reddit. Speciﬁcally, by harnessing the infor-\nmation in documents related to a target document,\ni.e., its siblings and parent, the LM is expected to\ncontain additional relevant information, and hence\nlower perplexity. Formally, let’s call the sentence-\nlevel context vector hs, the parent document context\n2Ji et al.’s three options performed similarly.\n763\n(a) ccDCLM (b) coDCLM (c) Our model (PS-ccDCLM)\nFigure 2: Contextual language models; see Sections 3 and 4 for detailed descriptions.\nvector hp, the sibling context vectorhl, and the over-\nall context vector hc. Our framework is deﬁned as:\nhc = gh(hs,hl,hp) (7)\nx′\ni,t = gi(xi,t,hc) (8)\nht = f(ht−1,x′\nt) (9)\nyt ∼ softmax\n(\ngo(ht−1,x′\nt,hc)\n)\n(10)\nWe use the last hidden vector of the RNNs as the\nrepresentation of the parent post, the older-sibling,\nand the previous sentence. The deﬁnition of the con-\ntext function ( gh), the input function ( gi), and the\nword-decoding function (go) yields different conﬁg-\nurations.\nWe also explored two strategies of training the\nmodels: Disconnected (disC) and Fully Connected\n(fulC). In the disC-trained models, the error signal\nwithin a time step (i.e. a post or sentence) only af-\nfects the parameters in that time step. This is in con-\ntrast to the fulC-trained models, where the error sig-\nnal is propagated to the previous time steps, hence\ninﬂuencing parameters in those time steps too.\n4.1 Analysis of our modelling approach\nIn this section, we empirically analyze different\ntraining and modelling decisions within our frame-\nwork, namely DC vs FC training, as well as contex-\ntual information from parent vs sibling.\nThe Setup. For our analysis, we employed a sub-\nset of the data described in Table 1 which contains\n450 threads split into training/testing/validation sets\nwith 300/100/50 threads respectively. The hidden-\nvector and word-vector dimensions were set to 50\nand 70, respectively. The models were implemented\nin Theano (Bastien et al., 2012; Bergstra et al.,\n2010), and trained with RMSProp (Tieleman and\nHinton, 2012).\nTable 2: disC/fulC-trained models vs the baselines.\nModel Training Perplexity\n6-gram na 205\nRNN-LSTM na 182\nccDCLM disC 185\ncoDCLM disC 181\nccDCLM fulC 176\ncoDCLM fulC 172\ndisC vs fulC. We ﬁrst compared the disC and fulC\nstrategies, at the sentence level only, in order to se-\nlect the best strategy in a known setting. To this ef-\nfect, we re-implemented Ji et al.’s (2015) DCLMs\nwith the disC strategy, noting that Ji et al.’s original\nsentence-based models are fulC-trained. The results\nof this experiment appear in Table 2 which further\ncompares these models with the following baselines:\n(1) vanila RNN-LSTM, and (2) a 6-gram LM with\nKneser-Ney smoothing 3 (Kneser and Ney, 1995).\nThe disC-trained models showed no improvement\nover the RNN-LSTM, and lagged behind their fulC-\ntrained counterparts. The lower performance of the\ndisC-trained models may be due to not fully lever-\naging the contextual information; disC-training lose\ninformation, as the error signal from the current time\nstep is not used to calibrate the parameters of pre-\nvious time steps. Therefore, we make use of fulC\nstrategy to train our models in the rest of this paper.\nParent vs Sibling Context. The inter-document\ninformation in reddit’s case may come from a par-\nent post, sibling posts or both. We tested our models\nwith different combinations of inter-document con-\n3Tested with the SRILM toolkit (Stolcke et al., 2011).\n764\ntext information to reﬂect these options. At present,\nwe consider only the closest older-sibling of a post,\nas it is deemed the most related; different combina-\ntions of sibling posts are left for future work. We\ntested the following three context-to-context conﬁg-\nurations: parent only (P-ccDCLM), sibling only (S-\nccDCLM), and parent and sibling (PS-ccDCLM),\nwhich deﬁne the context function as Equation 11,\n12 and 13 respectively. The three conﬁgurations use\nthe same word-decoding function (Equation 15) and\nthe same input function (Equation 14).\nhc = concat (hs,hp) (11)\nhc = concat (hs,hl) (12)\nhc = concat (hs,hl,hp) (13)\nx′\ni,t = concat (xi,t,hc) (14)\nyi,t ∼ softmax (Woht−1 + bo) (15)\nThe results of this experiment appear in the ﬁrst\nthree rows of Table 3, which shows that the best-\nperforming model is PS-ccDCLM.\nAs discussed by Ji et al. (2015), the coDCLM\nmakes the hidden units of the previous sentence have\nno effect on the hidden units of the current sen-\ntence. While this conﬁguration might have some\nadvantages (Ji et al., 2015), applying it directly to\na larger context may lead to complications. Suppose\nwe use the last hidden unit of the previous docu-\nment as the context for the next document. With\nthe context-to-output approach, the last hidden unit\nsummarizes only the information in the last sentence\nof the previous document, and doesn’t reﬂect the en-\ntire document. We address this problem by not using\nthe context-to-output approach in isolation. Instead,\nwe use the context-to-output approach in tandem\nwith the context-to-context approach of ccDCLM.\nThis approach was tested in an additional parent-\nsibling conﬁguration (PS-ccoDCLM), as an alterna-\ntive to the best performing context-to-context con-\nﬁguration. The PS-ccoDCLM is similar to the PS-\nccDCLM except for the decoding equation, which is\nchanged into Equation 16.\nyi,t ∼softmax\n(\nWohi\nt−1 + W ′\nohc + bo\n)\n(16)\nBased on the results of these trials, we chose the\nbest-performing PS-ccDCLM (Figure 2c) as our ﬁ-\nnal system.\nTable 3: Comparing models incorporating parent (P)\nand/or sibling (S) contextual information.\nSystems Perplexity\nP-ccDCLM 172\nS-ccDCLM 174\nPS-ccDCLM 168\nPS-ccoDCLM 175\nTable 4: Results on the entire dataset.\nSystems Perplexity\n6-gram 209\nRNN-LSTM 184\nccDCLM 168\ncoDCLM 176\nPS-ccDCLM 159\n4.2 Results\nThe model perplexity obtained by the baselines and\nour best-performing model for the test set (Table 1)\nis shown in Table 4 — our system (PS-ccDCLM)\nstatistically signiﬁcantly outperforms the best base-\nline (ccDCLM), with α = 0.01, using the Fried-\nman test. The inter-sentence contextual informa-\ntion under the context-to-context regime (ccDCLM)\ndecreases model perplexity by 9% compared to the\noriginal RNN-LSTM, while the inter-document con-\ntextual information (PS-ccDCLM) reduces perplex-\nity by a further 5% compared to ccDCLM.\n5 Discussion and Future Work\nOur results show that including inter-document con-\ntextual information yields additional improvements\nto those obtained from inter-sentence information.\nHowever, as expected, the former are smaller than\nthe latter, as sentences in the same post are more re-\nlated than sentences in different posts. At present,\nwe rely on the ﬁnal hidden-vector of the sentences\nand the posts for contextual information. In the fu-\nture, we propose to explore other options, such as\nadditional models to combine the contextual infor-\nmation from all siblings in the tree structure, and ex-\ntending our model to structures beyond trees.\n765\nReferences\nFr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,\nJames Bergstra, Ian J. Goodfellow, Arnaud Berg-\neron, Nicolas Bouchard, and Yoshua Bengio. 2012.\nTheano: new features and speed improvements. Deep\nLearning and Unsupervised Feature Learning NIPS\n2012 Workshop.\nJames Bergstra, Olivier Breuleux, Fr ´ed´eric Bastien, Pas-\ncal Lamblin, Razvan Pascanu, Guillaume Desjardins,\nJoseph Turian, David Warde-Farley, and Yoshua Ben-\ngio. 2010. Theano: a CPU and GPU math expression\ncompiler. In Proceedings of the Python for Scientiﬁc\nComputing Conference (SciPy), June. Oral Presenta-\ntion.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2015. Document context lan-\nguage models. arXiv preprint arXiv:1511.03962.\nReinhard Kneser and Hermann Ney. 1995. Im-\nproved backing-off for M-gram language modeling. In\nICASSP-95, volume 1, pages 181–184. IEEE.\nTomas Mikolov and Geoffrey Zweig. 2012. Context de-\npendent recurrent neural network language model. In\nSLT, pages 234–239.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent neu-\nral network based language model. InINTERSPEECH\n2010, pages 1045–1048.\nAndreas Stolcke, Jing Zheng, Wen Wang, and Victor\nAbrash. 2011. SRILM at sixteen: Update and\noutlook. In Proceedings of IEEE Automatic Speech\nRecognition and Understanding Workshop, page 5.\nMartin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney.\n2012. LSTM neural networks for language modeling.\nIn INTERSPEECH, pages 194–197.\nTijmen. Tieleman and Geoffrey Hinton. 2012. Neu-\nral Networks for Machine Learning. http://www.\nyoutube.com/watch?v=O3sxAc4hxZU. [On-\nline].\nTian Wang and Kyunghyun Cho. 2015. Larger-\ncontext language modelling. arXiv preprint\narXiv:1511.03729.\n766",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7624753713607788
    },
    {
      "name": "Natural language processing",
      "score": 0.540979266166687
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38733136653900146
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    }
  ]
}