{
    "title": "Large language models (LLM) and ChatGPT: what will the impact on nuclear medicine be?",
    "url": "https://openalex.org/W4323652488",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Alberts, Ian L.",
            "affiliations": [
                "University Hospital of Bern",
                "University of Bern"
            ]
        },
        {
            "id": "https://openalex.org/A4306802890",
            "name": "Mercolli, Lorenzo",
            "affiliations": [
                "University Hospital of Bern",
                "University of Bern"
            ]
        },
        {
            "id": null,
            "name": "Pyka, Thomas",
            "affiliations": [
                "University of Bern",
                "University Hospital of Bern"
            ]
        },
        {
            "id": null,
            "name": "Prenosil, George",
            "affiliations": [
                "University Hospital of Bern",
                "University of Bern"
            ]
        },
        {
            "id": "https://openalex.org/A3162016059",
            "name": "Shi, Kuangyu",
            "affiliations": [
                "University of Bern",
                "University Hospital of Bern"
            ]
        },
        {
            "id": "https://openalex.org/A4306802891",
            "name": "Rominger, Axel",
            "affiliations": [
                "University of Bern",
                "University Hospital of Bern"
            ]
        },
        {
            "id": null,
            "name": "Afshar-Oromieh, Ali",
            "affiliations": [
                "University Hospital of Bern",
                "University of Bern"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4318071574",
        "https://openalex.org/W4317376696",
        "https://openalex.org/W4318263917",
        "https://openalex.org/W4310917376",
        "https://openalex.org/W4317390716",
        "https://openalex.org/W4205472032",
        "https://openalex.org/W4319083882",
        "https://openalex.org/W4315784554",
        "https://openalex.org/W2077429985",
        "https://openalex.org/W3125841294",
        "https://openalex.org/W3133576686",
        "https://openalex.org/W3009812606",
        "https://openalex.org/W2000785678",
        "https://openalex.org/W4312067799",
        "https://openalex.org/W4312190916",
        "https://openalex.org/W2946674875",
        "https://openalex.org/W2954093393",
        "https://openalex.org/W2088830013",
        "https://openalex.org/W2276614808",
        "https://openalex.org/W4211086513",
        "https://openalex.org/W4318257512",
        "https://openalex.org/W4297814571",
        "https://openalex.org/W4293580221",
        "https://openalex.org/W2913848079",
        "https://openalex.org/W4318719246",
        "https://openalex.org/W2798966449",
        "https://openalex.org/W3041174012",
        "https://openalex.org/W4318069287",
        "https://openalex.org/W2980951591",
        "https://openalex.org/W2738218048",
        "https://openalex.org/W3004612364",
        "https://openalex.org/W4207037407"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)1 3\nhttps://doi.or\ng/10.1007/s00259-023-06172-w\nEDITORIAL\nLarge language models (LLM) and ChatGPT: what will the impact \non nuclear medicine be?\nIan L. Alberts1  · Lorenzo Mercolli1 · Thomas Pyka1 · George Prenosil1 · Kuangyu Shi1 · Axel Rominger1 · \nAli Afshar‑Oromieh1\nReceived: 9 February 2023 / Accepted: 19 February 2023 \n© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2023\nThere has been substantial press recently regarding the \nimpressive performance of large language models (LLM), \nparticularly the OpenAI tool “Chat Generative Pre-Trained \nTransformer,” commonly known as “ChatGPT” [1 ]. LLM \nrepresent artificial intelligence (AI) tools based on multi-\nlayer recurrent neural networks that are trained on vast \namounts of data to generate human-like text (https://\n ai. googl \neblo\ng. com/ 2017\n/ 08/ tra\nns for\nmer- nov\nel- neu\nral- netw\no rk. html)\n. \nWhereas traditional language models are programmed to use \nstatistical techniques to predict the next word in a sentence, \nChatGPT uses transformer-based models that allow for the \nprocessing of vast amounts of data in parallel. The result is \na revolution in the ability of these models to understand and \ngenerate text. Their performance is remarkable, e.g., Chat-\nGPT is capable of writing lines of code, producing plays, \nstories, poetry as well as simulated scientific content such \nas abstracts. While there has been much fanfare in the media \nregarding this undoubtedly impressive performance, there is \nmuch less information available about how this might affect \nthe nuclear medicine community, or its reliability in produc-\ning nuclear medicine and molecular imaging-related content. \nIt is currently unclear to what extent ChatGPT might help \nas a collaborative tool, for example correcting or helping to \nimprove upon a researcher’s writing or as a tool for sum-\nmarising nuclear medicine literature.\nWithin seconds, ChatGPT is capable of producing con-\nvincing and grammatically fluent prose that is indistin -\nguishable from content produced by human researchers. \nThe threat that this poses to academic publishing models \nis already apparent [2 ]. Controversially, ChatGPT has (at \nthe time of writing) already been listed as a co-author on \nfour academic publications [3 ]. Anecdotally, students are \nalready using the tool as a writing assistant, raising issues of \nacademic integrity and plagiarism [4]. There are already 25 \nPubMed entries for “ChatGPT”, this will likely grow rapidly \nin the coming weeks and months.\nIn response, a number of journals are already implement-\ning editorial policies about the acceptability of AI-assisted \nwriting or clarifying issues around authorship [3 , 5]. Some \ninternet fora have already banned ChatGPT-generated \nanswers owing to their unreliability (https://\n met\na. st\nack \noverfl  ow. com/ ques\nt ions/ 421831/  tempo rar\ny- policy-  chatg \npt- is- banned)\n. \nRecent experience has shown that AI tools can be har -\nnessed to mass-produce questionable content on social \nmedia networks or social media bots that can deliberately \namplify misinformation [6 ]. This experience might por -\ntend the future of ChatGPT-generated academic content. A \nreport from the Copenhagen-based Institute for Future Stud-\nies estimates that 99% of the internet could be produced by \ngenerative AI by 2025 (https://\n cifs. dk/ new\ns/ what- if- 99- of- \nthe- met\nav erse- is- made- by\n- ai). A\nt present, ChatGPT is not \ncapable of producing an entire research paper sua sponte, \nalthough it is predicted and indeed conceivable that this \nmight soon be the case [7]. Nevertheless, it can already, even \nin the currently available beta version, produce a very con-\nvincing abstract [8]. We wonder whether conferences might \nsoon be flooded with AI-generated abstracts or whether \npredatory publishers [9] might be catalysed by the ability of \nChatGPT to churn out convincing but ultimately unreliable \ncontent. Even the review process could be influenced: there \nare already proposals to harness the ability of LLM to sum-\nmarise text as a tool for the sifting out of low-quality studies \nsubmitted to a journal. Once can imagine a not-too distant \nfuture where AI might generate and review research [10], \nThis article is part of the Topical Collection on Advanced Image \nAnalyses (Radiomics and Artificial Intelligence)\n * Ian L. Alberts \n Ian.alberts@insel.c\nh\n1 Department of Nuclear Medicine, Inselspital, Bern \nUniv\nersity Hospital, University of Bern, Freiburgstr. 18, \n3010 Bern, Switzerland\n/ Published online: 9 March 2023\nEuropean Journal of Nuclear Medicine and Molecular Imaging (2023) 50:1549–1552\n1 3\nwhich could then be cited by other AI-generated research \nor commented upon via an AI-generated letter to the editor. \nUntil recently, such a future might have sounded far-fetched. \nIn light of the astonishing pace with which LLM have been \nimplemented, we feel that the academic nuclear medicine \ncommunity urgently needs to confront this issue.\nTrustworthiness is a key concept in the academic AI lit-\nerature, and reports of misrepresentation of simple facts or \nhidden bias means that there is much work to be done in this \nregard [11]. As a group of clinicians and scientists working \nin a clinical nuclear medicine department, we wonder can \nthe nuclear medicine content presently produced by Chat-\nGPT be trusted? A particularly important milestone in the \ntraining of any physician is the summative board or licensing \nexamination. Such examinations have the purpose of pro-\ntecting the public by holding physicians accountable and \ntesting them according to a defined body of knowledge and \nmaintaining public confidence through the maintenance of \nprofessional standards [12]. In tort law, when defining what \nis negligent practice, the knowledge and practice of a body \nof professionals is taken to be the reference standard in com-\nmon law jurisdictions [13]. If AI tools are to be potential \ntools to assist, or even replace physicians, then their per -\nformance might be held to a similar standard. Indeed, this \nhas been done recently yielding mixed results. For example, \nShelmerdine et al. tested whether a commercially available \nAI tool (which interestingly also had a CE-conformity label) \nwas capable of passing the radiograph reporting section of \nthe United Kingdom Fellowship of the Royal College of \nRadiologists (FRCR). The tool performed poorly. Only \nwith special dispensation was it capable of passing only \ntwo of ten mock examinations and it ranked last alongside \nits 26 human peers [14]. In contrast, ChatGPT was capable \nof passing or being close to passing all three parts of the \nUnited States Medical Licensing Exam (USMLE) without \nadditional training or prompts [1 ] and a Chinese AI tool \nXiaoyi (meaning “little doctor”) was, with training, capable \nof passing the Chinese Medical Licensing Exam [15]. Read-\ners of this letter will doubtless be familiar with the hyper -\nbole surrounding the role of AI in diagnostic radiology and \nnuclear medicine, exemplified by Hinton’s famous and now \ncontroversial statement in 2013 that “we stop training radi-\nologists now” [16]. The effect of this hype, along with many \nother similar statements, has given cause for medical stu-\ndents to re-evaluate their career options [17], potentially at \nthe cost of recruitment in a field where there is an increasing \ndemand for services and shortage of trainees. We were there-\nfore intrigued to see how ChatGPT might fare in a nuclear \nmedicine examination, to assess whether it might pose a \nrisk to the integrity of online nuclear medicine examinations \nand whether it has a potential role in assisting candidates \npreparing for nuclear medicine examinations. There is also \nthe potential that patients might turn to these tools to help \nanswer their questions or allay their fears regarding nuclear \nmedicine treatments and investigations.\nThe Fellowship of the European Board Examination is a \ntwo-stage examination administered by the Nuclear Medi-\ncine Section of the European Union of Medical Specialists in \nclose cooperation with the European Association of Nuclear \nMedicine [18, 19]. The first step is a written examination, \nwhich since the SARS-CoV-2 pandemic has been delivered \nremotely and online [20]. The second is an oral examination \nthat is delivered alongside the EANM annual congress. The \nonline nature of the written exam is particularly prescient to \nChatGPT, since conceivably, a candidate could use OpenAI \ntools dishonestly to help pass an online examination (https://\n \narxiv\n. or\ng/ abs/ 2212. 09292\n). Alternatively, LLM hold promise \nas tools for teaching and learning. They could, for example, \nassist a medial trainee by providing tailored guides to the \nliterature [21].\nTo simulate the written part examination, the first author \n(and fellow of the EBNM) provided ChatGPT with 50 \nexample multiple-choice questions which are openly avail-\nable online as part of the training material for candidates \n(https://\n link\n. spr\nin g\ner. com/ ar\ntic le/ 10. 1007/ s00259- 011- 1949-\nz). The\ny can be taken to be indicative of the breadth and \ndepth of knowledge expected of candidates. The questions \ndid not require image interpretation. ChatGPT states that \nits training cut-off was 2021—the questions presented to it \ndate from 2009, meaning that ChatGPT was not expected \nto answer questions based on state-of-the-art knowledge on \nwhich it had not been trained.\nThe multiple-choice questions require the candidate to \nchoose the single correct answer from four or five possibili-\nties. In all 50 cases, ChatGPT provided a definitive answer. \nMarking these against the model answer provided in the \ntraining material revealed that ChatGPT was correct only \n34% of the time (17/50). With 11 answers requiring the can-\ndidate to choose from five possible responses and the remain-\nder having four possible responses, the mean probability of \nchoosing the correct by random chance was 0.24, suggesting \nthat ChatGPT was likely able to draw on some knowledge \nrather than simply guessing. With this performance, it is fair \nto say that ChatGPT would be unlikely to pass the exam if it \nwere to take it in real life, although this could change in the \nfuture with better training for the model.\nWe then sought to test ChatGPT’s ability to learn or be \ncorrected. The correct responses were provided via Chat-\nGPT’s feedback tool. The incorrect question was asked \nagain in an open and Socratic fashion, in the way a kindly \nexaminer might give a wayward candidate a second chance \nto demonstrate his or her knowledge in an oral examina-\ntion. For example, when first asked, “which benign lesion \ndoes not show increased uptake on a bone scan?” Chat-\nGPT incorrectly chose osteoid osteoma, which is entirely \nwrong; this entity is famous for its exquisite avidity for \n1550 European Journal of Nuclear Medicine and Molecular Imaging (2023) 50:1549–1552\n1 3\nbone-seeking radiopharmaceuticals. When corrected and \nasked the question again but in a Socratic fashion “Name \nme a benign lesion which does not show increased uptake \non a bone scintigraphy” ChatGPT responded as follows: “A \nbenign lesion that typically does not show increased uptake \non a bone scintigraphy is osteoarthritis. Osteoarthritis is a \ndegenerative joint disease that does not involve abnormal \nbone metabolism and therefore does not result in increased \nuptake of the radiotracer on a bone scan”. This is of course \nnonsense: osteoarthritis does involve increased bone turno-\nver and is a common incidental finding on bone scanning. \nMoreover, ChatGPT showed evidence of confabulation —\nosteoarthritis was not part of the question stem asked previ-\nously (the options were the correct choice of the bone cyst, \nor the incorrect options of Engelmann’s disease, Paget’s \ndisease, or fibrous dysplasia). It is also non-reproducible, \ncontext sensitive and non-linear; asking the same question \ncan reveal different answers. Medical regulators demand that \nlicensed physicians recognise and practice within the limits \nof their own knowledge and competence (https://\n www\n. gmc- \nuk. or\ng/ e\nthic al- guida nce/ e\nthic al- guida nce- f\nor- doct\no rs/ good- \nmedic al- pr\nact ice/ duties- of-a- doct\nor). Rather than stating that \nit did not know, or could not answer, ChatGPT ventured to \nprovide a superficial and possibly convincing answer which \nwas wrong, and if relied upon to help interpret a scan find-\ning, might be harmful. Although a very preliminary analy -\nsis of ChatGPT’s performance, it bears some similarity to \nadversarial examples. AI models can be fooled to produce \nincorrect output with a high degree of confidence [22, 23] \nand act as a test of the model’s robustness that goes beyond \nstandard metric such as out-of-sample error [24]. In the \ncase of LLM, however, constructing adversarial examples \nis notoriously difficult and this small analysis was able to \nprobe the limits of ChatGPT’s performance in generating \nnuclear medicine-related content [25–27]. As the technology \nmatures, future studies might be performed to systematically \ntest the robustness of LLM and their reliability in producing \nnuclear medicine related content.\nWe must highlight that the producers of ChatGPT would \nnot claim that it is, or should, be capable of passing a board \nexamination. Nor can it be expected that it be capable of \ninsight or self-awareness. We asked ChatGPT: “Are you \ncapable of passing a nuclear medicine board examination?” \nIt gave us the following response, which we quote verbatim: \n“No, as a language model AI, I do not have the capability to \ntake exams or have the physical ability to perform medical \nprocedures. I am trained to assist with answering questions \nand generating human-like text based on the input provided \nto me”. We then asked ChatGPT its thoughts on the reli-\nability of its answers, eliciting the following response: “My \nanswers are generated based on patterns in large amounts of \ntext data and while they can be helpful as a reference, they are \nnot a substitute for professional medical advice and should \nnot be used as the sole basis for making medical decisions. \nIt's always best to consult with a licensed medical profes-\nsional for accurate and up-to-date information. Additionally, \nthe information I have is based on the data I was trained on, \nwhich has a knowledge cut-off date of 2021, so newer devel-\nopments or updates in the medical field may not be reflected \nin my answers.” At face value, this modest answer acknowl-\nedges the limits of an LLM. However, in our view, it does \nnot adequately address the fact that the answers it provided \nwere not just unhelpful, but factually incorrect, misleading \nand delivered without hesitation. We would therefore urge \ncaution when weighing claims that LLMs might be used for \nsummarising medical records, drafting authorisation letters \nto insurers justifying treatment costs or in decision support \ntools for diagnosis [28].\nThe rapid pace of events cannot be overemphasised: what \nis written about the performance of ChatGPT today could \nbe invalidated within a matter of weeks, if not days. Indeed, \nin the days preceding the writing of this editorial, Microsoft \nannounced that it would integrate ChatGPT into its Bing \nsearch engine and ChatGPT functionality has already been \nembedded in the beta version of the recently introduced you.\ncom search engine. LambdaBard, dubbed a competitor to \nChatGPT and produced by Google, is eagerly anticipated \nwithin weeks (https://\n blog. goog\nle/ tec\nhn ology/ ai/ lamda/\n). \nChatGPT itself has only been available in stable release form \nfor a matter of days at the time of writing this editorial, and \na subscription ChatGPT plus with additional functionality is \nplanned (https://\n openai. com/ blog/ c\nhatg p\nt- plus/\n).\nIn summary, although ChatGPT is presently capable of \nproviding seemingly convincing content, including refer -\nenced abstracts that are capable of fooling peer reviewers \n[8], our preliminary analysis suggests that it is currently far \nfrom demonstrating the knowledge expected of a certified \nnuclear medicine physician in Europe in the setting of a \nstandardised exam. Candidates preparing for exams or prac-\nticing physicians should test the validity of any statement \ngenerated by these models for themselves and be aware that \nthe content can be unreliable. Given the performance dem-\nonstrated in this preliminary analysis, we do not see any evi-\ndence that ChatGPT would pose a threat to the integrity of \nany online nuclear medicine examination at the present time, \nalthough given the rapid pace of development, this could \nvery well change in the near future. Nevertheless, we believe \nthat the power of ChatGPT (or the lack thereof) shows an \nurgent need to address the ethical challenges of such systems \nin a systematic way [29–31]. We believe that the education \nand training of clinicians will have to adapt according to the \ndegree of agency that tools like ChatGPT will have in the \nmedical field. Finally, with tongue in cheek and contrary to \nHinton’s advice, it would be prudent to continue training \nnuclear medicine physicians and radiologists—at least for \nthe time being.\n1551European Journal of Nuclear Medicine and Molecular Imaging (2023) 50:1549–1552\n1 3\nDeclarations \nEthics approval This article does no t contain any studies with human \nparticipants or animals performed by any of the authors.\nConflict of interest AR has receiv\ned research support and speaker hon-\noraria from Siemens. All other authors report no conflict of interest.\nReferences\n 1. Looi MK. Sixty seconds on … ChatGPT. BMJ. 2023;380:205.  \nhttps:// doi. or\ng/ 10. 1136/ bmj. p205\n.\n 2. Mann Douglas L. Ar\ntificial intelligence discusses the role of arti-\nficial intelligence in translational medicine. JACC: Basic Transl \nSci. n.d.;0. https:// doi. or\ng/ 10. 1016/j. jacbts. 2023. 01. 001\n.\n 3. Thorp HH. ChatGPT is fun, but no\nt an author. Science. \n2023;379:313. https:// doi. or\ng/ 10. 1126/ scien ce. adg78 79\n.\n 4. St\nokel-Walker C. AI bot ChatGPT writes smart essays - should \nprofessors worry? Nature. 2022. https://  doi.  or\ng/ 10.  1038/  \nd41586- 022- 04397-7\n.\n 5. St\nokel-Walker C. ChatGPT listed as author on research papers: \nmany scientists disapprove. Nature. 2023;613:620–1. https://  doi. \norg/ 10. 1038/ d41586- 023- 00107-z\n.\n 6. Gisondi MA, Barber R, F\naust JS, Raja A, Strehlow MC, Westafer \nLM, et al. A deadly infodemic: social media and the power of \nCOVID-19 misinformation. J Med Internet Res. 2022;24:e35552. \nhttps://\n doi. or\ng/ 10. 2196/ 35552\n.\n 7. van Dis EAM, Bollen J, Zuidema W\n, van Rooij R, Bockting CL. \nChatGPT: five priorities for research. Nature. 2023;614:224–6. \nhttps://\n doi. or\ng/ 10. 1038/ d41586- 023- 00288-7\n.\n 8. Else H. Abs\ntracts written by ChatGPT fool scientists. Nature. \n2023;613:423. https:// doi. or\ng/ 10. 1038/ d41586- 023- 00056-7\n.\n 9. Beall J. Predat\nory publishers are corrupting open access. Nature. \n2012;489:179. https:// doi. or\ng/ 10. 1038/ 48917 9a\n.\n 10\n. Checco A, Br\nacciale L, Loreti P, Pinfield S, Bianchi G. AI-assisted \npeer review. Humanit Soc Sci Commun. 2021;8:25. https://  doi. \norg/ 10. 1057/ s41599- 020- 00703-8\n.\n 11. Kitamur\na FC, Marques O. Trustworthiness of artificial intelligence \nmodels in radiology and the role of explainability. J Am Coll Radiol. \n2021;18:1160–2. https://\n doi. or\ng/ 10. 1016/j. jacr\n. 2021. 02. 008\n.\n 12\n. Mirzaei S, Hus\ntinx R, Prior JO, Ozcan Z, Boubaker A, Farsad \nM, et al. Improving nuclear medicine practice with UEMS/\nEBNM committees. J Nucl Med: Off Publ Soc Nucl Med. \n2020;61:18N-20N.\n 13. Sokol DK. Ho\nw good a doctor do you need to be? BMJ Br Med \nJ. 2012;345:e7858. https:// doi. or\ng/ 10. 1136/ bmj. e7858\n.\n 14. Shelmerdine SC, Mar\ntin H, Shirodkar K, Shamshuddin S, Weir-\nMcCall JR, Collaborators F-AS. Can artificial intelligence pass \nthe Fellowship of the Royal College of Radiologists examination? \nMulti-reader diagnostic accuracy study. BMJ. 2022;379:e072826. \nhttps://\n doi. or\ng/ 10. 1136/ bmj- 2022- 072826\n.\n 15\n. Ram\npton V, Ko A. Robots, radiologists, and results. BMJ. \n2022;379:o2853. https:// doi. or\ng/ 10. 1136/ bmj. o2853\n.\n 16\n. La\nnglotz CP. Will Artificial intelligence replace radiologists? \nRadiol Artif Intell. 2019;1:e190058. https:// doi. or\ng/ 10. 1148/ r\nyai. \n20191 90058\n.\n 17\n. Hustinx R. Ph\nysician centred imaging interpretation is dying \nout - why should I be a nuclear medicine physician? Eur J Nucl \nMed Mol Imaging. 2019;46:2708–14. https://\n doi. or\ng/ 10. 1007/ \ns00259- 019- 04371-y.\n 18\n. Pons F\n, Bischof DA. The European board of nuclear medicine fel-\nlowship examination. Eur J Nucl Med Mol Imaging. 2006;33:109–\n10. https://\n doi. or\ng/ 10. 1007/ s00259- 005- 1963-0\n.\n 19\n. Neill\ny B, Dizdarevic S, Prvulovich L, Buscombe J, Lewington \nV. Nuclear medicine training and practice in the UK. Eur J Nucl \nMed Mol Imaging. 2016;43:800–3. https://\n doi. or\ng/ 10. 1007/ \ns00259- 015- 3255-7\n.\n 20. Ozcan Z, K\nulakiene I, Vaz SC, Garzon JRG, Boubaker A. Chal-\nlenges and possibilities for board exams in the Covid-19 era: \nexperience from the Fellowship Committee of European Board of \nNuclear Medicine. Eur J Nucl Med Mol Imaging. 2022;49:1442–\n6. https://\n doi. or\ng/ 10. 1007/ s00259- 022- 05716-w.\n 21\n. Baidoo-Anu D, Owusu Ansah L. Education in t\nhe Era of Gen-\nerative Artificial Intelligence (AI): Understanding the Potential \nBenefits of ChatGPT in Promoting Teaching and Learning. 2023. \nAvailable at SSRN: https://\n ssr\nn. com/ abs\ntr act= 43374 84\n; https:// \ndoi. or\ng/ 10. 2139/ ssr\nn. 43374 84\n.\n 22\n. Szeg\nedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Good-\nfellow I, et al. Intriguing properties of neural networks. arXiv \npreprint arXiv:13126199. 2013.  https://\n doi. or\ng/ 10. 48550/ arXiv\n. \n1312. 6199\n.\n 23\n. Goodfello\nw IJ, Shlens J, Szegedy C. Explaining and harness-\ning adversarial examples. arXiv preprint arXiv:14126572. \n2014. https://\n doi. or\ng/ 10. 48550/ arXiv\n. 1412. 6572\n.\n 24. Carlini N\n, Athalye A, Papernot N, Brendel W, Rauber J, Tsipras \nD, et al. On evaluating adversarial robustness. arXiv preprint \narXiv:190206705. 2019.  https://\n doi. or\ng/ 1\n0. 48550/ a\nrXiv. 1902\n. \n06705.\n 25\n. Zhuo TY\n, Huang Y, Chen C, Xing Z. Exploring AI Ethics of \nChatGPT: a diagnostic analysis. arXiv preprint arXiv:230112867. \n2023. https://\n doi. or\ng/ 10. 48550/ arXiv\n. 2301. 12867\n.\n 26\n. Al\nzantot M, Sharma Y, Elgohary A, Ho B-J, Srivastava M, Chang \nK-W. Generating natural language adversarial examples. arXiv \npreprint arXiv:180407998. 2018. https://\n doi. or\ng/ 10. 48550/ arXiv\n. \n1804. 07998\n.\n 27\n. Zhang H, Zhou H, Mia\no N, Li L. Generating fluent adversarial \nexamples for natural languages. arXiv preprint arXiv:200706174. \n2020. https://\n doi. or\ng/ 10. 48550/ arXiv\n. 2007. 06174\n.\n 28\n. Shen Y, Heacoc\nk L, Elias J, Hentel KD, Reig B, Shih G, et al. Chat-\nGPT and other large language models are double-edged swords. \nRadiology. 2023;230163. https://\n doi. or\ng/ 10. 1148/ r\nadiol. 230163\n.\n 29. Milano S, Taddeo M, Flor\nidi L. Recommender systems and their \nethical challenges. AI Soc. 2020;35:957–67.\n 30. Nyholm S. Attr\nibuting agency to automated systems: reflections \non human–robot collaborations and responsibility-loci. Sci Eng \nEthics. 2018;24:1201–19.\n 31\n. Sutton R\nT, Pincock D, Baumgart DC, Sadowski DC, Fedorak \nRN, Kroeker KI. An overview of clinical decision support sys-\ntems: benefits, risks, and strategies for success. NPJ Digit Med. \n2020;3:17.\nPublisher's note\n Spring\ner Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\n1552 European Journal of Nuclear Medicine and Molecular Imaging (2023) 50:1549–1552"
}