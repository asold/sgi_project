{
  "title": "Weighted Transformer Network for Machine Translation",
  "url": "https://openalex.org/W2767989436",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2304481799",
      "name": "Ahmed, Karim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281371573",
      "name": "Keskar, Nitish Shirish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2963991316",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2890177507",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2619184049",
    "https://openalex.org/W2545625743",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2540404261",
    "https://openalex.org/W2950738719",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2737711067",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2950855294",
    "https://openalex.org/W2953328958",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3103531650",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2949847915",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. Vaswani et al. (2017) propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers. While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge. We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15-40% faster. Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process. Our model improves the state-of-the-art performance by 0.5 BLEU points on the WMT 2014 English-to-German translation task and by 0.4 on the English-to-French translation task.",
  "full_text": "WEIGHTED TRANSFORMER NETWORK FOR\nMACHINE TRANSLATION\nKarim Ahmed, Nitish Shirish Keskar & Richard Socher\nSalesforce Research\nPalo Alto, CA 94103, USA\n{karim.ahmed,nkeskar,rsocher}@salesforce.com\nABSTRACT\nState-of-the-art results on neural machine translation often use attentional\nsequence-to-sequence models with some form of convolution or recursion.\nVaswani et al. (2017) propose a new architecture that avoids recurrence and con-\nvolution completely. Instead, it uses only self-attention and feed-forward layers.\nWhile the proposed architecture achieves state-of-the-art results on several ma-\nchine translation tasks, it requires a large number of parameters and training iter-\nations to converge. We propose Weighted Transformer, a Transformer with mod-\niﬁed attention layers, that not only outperforms the baseline network in BLEU\nscore but also converges 15 −40% faster. Speciﬁcally, we replace the multi-head\nattention by multiple self-attention branches that the model learns to combine dur-\ning the training process. Our model improves the state-of-the-art performance by\n0.5 BLEU points on the WMT 2014 English-to-German translation task and by\n0.4 on the English-to-French translation task.\n1 I NTRODUCTION\nRecurrent neural networks (RNNs), such as long short-term memory networks (LSTMs) (Hochreiter\n& Schmidhuber, 1997), form an important building block for many tasks that require modeling of\nsequential data. RNNs have been successfully employed for several such tasks including language\nmodeling (Melis et al., 2017; Merity et al., 2017), speech recognition (Xiong et al., 2017; Graves\net al., 2013), and machine translation (Wu et al., 2016; Bahdanau et al., 2014). RNNs make output\npredictions at each time step by computing a hidden state vectorht based on the current input token\nand the previous states. This sequential computation underlies their ability to map arbitrary input-\noutput sequence pairs. However, because of their auto-regressive property of requiring previous\nhidden states to be computed before the current time step, they cannot beneﬁt from parallelization.\nVariants of recurrent networks that use strided convolutions eschew the traditional time-step based\ncomputation (Kaiser & Bengio, 2016; Lei & Zhang, 2017; Bradbury et al., 2016; Gehring et al.,\n2016; 2017; Kalchbrenner et al., 2016). However, in these models, the operations needed to learn\ndependencies between distant positions can be difﬁcult to learn (Hochreiter et al., 2001; Hochreiter,\n1998). Attention mechanisms, often used in conjunction with recurrent models, have become an in-\ntegral part of complex sequential tasks because they facilitate learning of such dependencies (Luong\net al., 2015; Bahdanau et al., 2014; Parikh et al., 2016; Paulus et al., 2017; Kim et al., 2017).\nIn Vaswani et al. (2017), the authors introduce the Transformer network, a novel architecture that\navoids the recurrence equation and maps the input sequences into hidden states solely using atten-\ntion. Speciﬁcally, the authors use positional encodings in conjunction with a multi-head attention\nmechanism. This allows for increased parallel computation and reduces time to convergence. The\nauthors report results for neural machine translation that show the Transformer networks achieves\nstate-of-the-art performance on the WMT 2014 English-to-German and English-to-French tasks\nwhile being orders-of-magnitude faster than prior approaches.\nTransformer networks still require a large number of parameters to achieve state-of-the-art perfor-\nmance. In the case of the newstest2013 English-to-German translation task, the base model required\n65M parameters, and the large model required213M parameters. We propose a variant of the Trans-\nformer network which we call Weighted Transformer that uses self-attention branches in lieu of\n1\narXiv:1711.02132v1  [cs.AI]  6 Nov 2017\nthe multi-head attention. The branches replace the multiple heads in the attention mechanism of\nthe original Transformer network, and the model learns to combine these branches during training.\nThis branched architecture enables the network to achieve comparable performance at a signiﬁcantly\nlower computational cost. Indeed, through this modiﬁcation, we improve the state-of-the-art perfor-\nmance by 0.5 and 0.4 BLEU scores on the WMT 2014 English-to-German and English-to-French\ntasks, respectively. Finally, we present evidence that suggests a regularizing effect of the proposed\narchitecture.\n2 R ELATED WORK\nMost architectures for neural machine translation (NMT) use an encoder and a decoder that rely on\ndeep recurrent neural networks like the LSTM (Luong et al., 2015; Sutskever et al., 2014; Bahdanau\net al., 2014; Wu et al., 2016; Barone et al., 2017; Cho et al., 2014). Several architectures have been\nproposed to reduce the computational load associated with recurrence-based computation (Gehring\net al., 2016; 2017; Kaiser & Bengio, 2016; Kalchbrenner et al., 2016). Self-attention, which relies on\ndot-products between elements of the input sequence to compute a weighted sum (Lin et al., 2017;\nBahdanau et al., 2014; Parikh et al., 2016; Kim et al., 2017), has also been a critical ingredient in\nmodern NMT architectures. The Transformer network (Vaswani et al., 2017) avoids the recurrence\ncompletely and uses only self-attention.\nWe propose a modiﬁed Transformer network wherein the multi-head attention layer is replaced by a\nbranched self-attention layer. The contributions of the various branches is learned as part of the train-\ning procedure. The idea of multi-branch networks has been explored in several domains (Ahmed &\nTorresani, 2017; Gastaldi, 2017; Shazeer et al., 2017; Xie et al., 2016). To the best of our knowl-\nedge, this is the ﬁrst model using a branched structure in the Transformer network. In Shazeer et al.\n(2017), the authors use a large network, with billions of weights, in conjunction with a sparse expert\nmodel to achieve competitive performance. Ahmed & Torresani (2017) analyze learned branching,\nthrough gates, in the context of computer vision while in Gastaldi (2017), the author analyzes a\ntwo-branch model with randomly sampled weights in the context of image classiﬁcation.\n2.1 T RANSFORMER NETWORK\nThe original Transformer network uses an encoder-decoder architecture with each layer consisting\nof a novel attention mechanism, which the authors call multi-head attention, followed by a feed-\nforward network. We describe both these components below.\nFrom the source tokens, learned embeddings of dimensiondmodel are generated which are then mod-\niﬁed by an additive positional encoding. The positional encoding is necessary since the network\ndoes not otherwise possess any means of leveraging the order of the sequence since it contains no\nrecurrence or convolution. The authors use additive encoding which is deﬁned as:\nPE(pos,2i) = sin(pos/100002i/dmodel )\nPE(pos,2i+ 1) = cos(pos/100002i/dmodel ),\nwhere posis the position of a word in the sentence and iis the dimension of the vector. The authors\nalso experiment with learned embeddings (Gehring et al., 2016; 2017) but found no beneﬁt in doing\nso. The encoded word embeddings are then used as input to the encoder which consists of N\nlayers each containing two sub-layers: (a) a multi-head attention mechanism, and (b) a feed-forward\nnetwork.\nA multi-head attention mechanism builds upon scaled dot-product attention, which operates on a\nquery Q, key Kand a value V:\nAttention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV (1)\nwhere dk is the dimension of the key.\nIn the ﬁrst layer, the inputs are concatenated such that each of (Q,K,V ) is equal to the word vector\nmatrix. This is identical to dot-product attention except for the scaling factor dk, which improves\nnumerical stability.\n2\nMulti-head attention mechanisms obtain hdifferent representations of (Q, K, V), compute scaled\ndot-product attention for each representation, concatenate the results, and project the concatenation\nwith a feed-forward layer. This can be expressed in the same notation as Equation (1):\nheadi = Attention(QWQ\ni ,KW K\ni ,VW V\ni ) (2)\nMultiHead(Q,K,V ) =Concati(headi)WO (3)\nwhere the Wi and WO are parameter projection matrices that are learned. Note that WQ\ni ∈\nRdmodel×dk , WK\ni ∈Rdmodel×dk , WV\ni ∈Rdmodel×dv and WO ∈Rhdv×dmodel where hdenotes the number\nof heads in the multi-head attention. Vaswani et al. (2017) proportionally reduce dk = dv = dmodel\nso that the computational load of the multi-head attention is the same as simple self-attention.\nThe second component of each layer of the Transformer network is a feed-forward network. The\nauthors propose using a two-layered network with a ReLU activation. Given trainable weights\nW1,W2,b1,b2, the sub-layer is deﬁned as:\nFFN(x) = max(0,xW1 + b1)W2 + b2 (4)\nThe dimension of the inner layer is dff which is set to 2048 in their experiments. For the sake of\nbrevity, we refer the reader to Vaswani et al. (2017) for additional details regarding the architecture.\nFor regularization and ease of training, the network uses layer normalization (Ba et al., 2016) after\neach sub-layer and a residual connection around each full layer (He et al., 2016). Analogously,\neach layer of the decoder contains the two sub-layers mentioned above as well as an additional\nmulti-head attention sub-layer that receives as inputs (V,K) from the output of the corresponding\nencoding layer. In the case of the decoder multi-head attention sub-layers, the scaled dot-product\nattention is masked to prevent future positions from being attended to, or in other words, to prevent\nillegal leftward-ward information ﬂow.\nOne natural question regarding the Transformer network is why self-attention should be preferred to\nrecurrent or convolutional models. Vaswani et al. (2017) state three reasons for the preference: (a)\ncomputational complexity of each layer, (b) concurrency, and (c) path length between long-range\ndependencies. Assuming a sequence length of nand vector dimension d, the complexity of each\nlayer is O(n2d) for self-attention layers while it is O(nd2) for recurrent layers. Given that typically\nd > n, the complexity of self-attention layers is lower than that of recurrent layers. Further, the\nnumber of sequential computations is O(1) for self-attention layers and O(n) for recurrent layers.\nThis helps improved utilization of parallel computing architectures. Finally, the maximum path\nlength between dependencies is O(1) for the self-attention layer while it is O(n) for the recurrent\nlayer. This difference is instrumental in impeding recurrent models’ ability to learn long-range\ndependencies.\n3 P ROPOSED NETWORK ARCHITECTURE\nWe now describe the proposed architecture, the Weighted Transformer, which is more efﬁcient to\ntrain and makes better use of representational power.\nIn Equations (3) and (4), we described the attention layer proposed in Vaswani et al. (2017) com-\nprising the multi-head attention sub-layer and a FFN sub-layer. For the Weighted Transformer, we\npropose a branched attention that modiﬁes the entire attention layer in the Transformer network (in-\ncluding both the multi-head attention and the feed-forward network). The proposed attention layer\ncan be described as:\nheadi = Attention(QWQ\ni ,KW K\ni ,VW V\ni ), (5)\nheadi = headiWOi ×κi, (6)\nBranchedAttention(Q,K,V ) =\nM∑\ni=1\nαiFFN(headi). (7)\nwhere M denotes the total number of branches, κi,αi ∈R+ are learned parameters and WOi ∈\nRdv×dmodel . The FFN function above is identical to Equation (4). Further, we require that ∑κi = 1\nand ∑αi = 1so that Equation (7) is a weighted sum of the individual branch attention values.\n3\nDot-ProductAttention\nLinear\nFeed\tForward\nDot-ProductAttention\nLinear\nFeed\tForward\nMx\nMx\nInputs\nFeed\tForward\nFeed\tForward\nMx\nMx\nEncoderOutput\nOutputs\nOutput\tProbabilities\nMulti-branch\tEncoder\tLayer\nMulti-branch\tDecoder\tLayer\n:\tLearned\tAddition\tScaling.\t\n:\tLearned\tConcatenation\tScaling.\t\nLinear\nDot-ProductAttention\nLinear\nDot-ProductAttention\nMaskedAttention\nLinearEncoderOutput\nMaskedAttention\nLinear\nInput\tEmbedding\noutput\tEmbedding\nSoftmax\n! ! ! !\n\" \" \" \"\n\"!\nFigure 1: Our proposed network architecture.\nIn the equations above, κcan be interpreted as a learned concatenation weight and αas the learned\naddition weight. Indeed, κscales the contribution of the various branches before αis used to sum\nthem in a weighted fashion. We ensure that all bounds are respected during each training step by\nprojection.\nWhile it is possible that α and κ could be merged into one variable and trained, we found better\ntraining outcomes by separating them. It also improves the interpretability of the models gives that\n(α,κ) can be thought of as probability masses on the various branches.\nIt can be shown that if κi = 1 and αi = 1 for all i, we recover the equation for the multi-head\nattention (3). However, given the∑\ni κi = 1and ∑\ni αi = 1bounds, these values are not permissible\nin the Weighted Transformer. One interpretation of our proposed architecture is that it replaces the\nmulti-head attention by a multi-branch attention. Rather than concatenating the contributions of the\ndifferent heads, they are instead treated as branches that a multi-branch network learns to combine.\nThis mechanism adds O(M) trainable weights. This is an insigniﬁcant increase compared to the\ntotal number of weights. Indeed, in our experiments, the proposed mechanism added192 weights to\na model containing 213M weights already. Without these additional trainable weights, the proposed\nmechanism is identical to the multi-head attention mechanism in the Transformer. The proposed\nattention mechanism is used in both the encoder and decoder layers and is masked in the decoder\nlayers as in the Transformer network. Similarly, the positional encoding, layer normalization, and\nresidual connections in the encoder-decoder layers are retained. We eliminate these details from\nFigure 1 for clarity. Instead of using (α,κ) learned weights, it is possible to also use a mixture-of-\nexperts normalization via a softmax layer (Shazeer et al., 2017). However, we found this to perform\nworse than our proposal.\nUnlike the Transformer, which weighs all heads equally, the proposed mechanism allows for ascrib-\ning importance to different heads. This in turn prioritizes their gradients and eases the optimization\nprocess. Further, as is known from multi-branch networks in computer vision (Gastaldi, 2017), such\nmechanisms tend to cause the branches to learn decorrelated input-output mappings. This reduces\nco-adaptation and improves generalization. This observation also forms the basis for mixture-of-\nexperts models (Shazeer et al., 2017).\n4\n4 E XPERIMENTS\n4.1 T RAINING DETAILS\nThe weights κand αare initialized randomly, as with the rest of the Transformer weights.\nIn addition to the layer normalization and residual connections, we use label smoothing with\nϵls = 0.1, attention dropout, and residual dropout with probability Pdrop = 0.1. Attention dropout\nrandomly drops out elements (Srivastava et al., 2014) from the softmax in (1).\nAs in Vaswani et al. (2017), we used the Adam optimizer (Kingma & Ba, 2014) with (β1,β2) =\n(0.9,0.98) and ϵ = 10−9. We also use the learning rate warm-up strategy for Adam wherein the\nlearning rate lrtakes on the form:\nlr= d−0.5\nmodel ·min(iterations−0.5,iterations ·4000−1.5),\nfor the all parameters except (α,κ) and\nlr= (dmodel/N)−0.5 ·min(iterations−0.5,iterations ·400−1.5),\nfor (α,κ).\nThis corresponds to the warm-up strategy used for the original Transformer network except that\nwe use a larger peak learning rate for (α,κ) to compensate for their bounds. Further, we found\nthat freezing the weights (κ,α) in the last 10K iterations aids convergence. During this time, we\ncontinue training the rest of the network. We hypothesize that this freezing process helps stabilize\nthe rest of the network weights given the weighting scheme.\nWe note that the number of iterations required for convergence to the ﬁnal score is substantially\nreduced for the Weighted Transformer. We found that Weighted Transformer converges 15–40%\nfaster as measured by the total number of iterations to achieve optimal performance. We train the\nbaseline model for 100K steps for the smaller variant and300K for the larger. We train the Weighted\nTransformer for the respective variants for60K and 250K iterations. We found that the objective did\nnot signiﬁcantly improve by running it for longer. Further, we do not use any averaging strategies\nemployed in Vaswani et al. (2017) and simply return the ﬁnal model for testing purposes.\nIn order to reduce the computational load associated with padding, sentences were batched such that\nthey were approximately of the same length. All sentences were encoded using byte-pair encoding\n(Sennrich et al., 2015) and shared a common vocabulary. Weights for word embeddings were tied to\ncorresponding entries in the ﬁnal softmax layer (Inan et al., 2016; Press & Wolf, 2016). We trained\nall our networks on NVIDIA K80 GPUs with a batch containing roughly 25,000 source and target\ntokens.\n4.2 R ESULTS ON BENCHMARK DATA SETS\nWe benchmark our proposed architecture on the WMT 2014 English-to-German and English-to-\nFrench tasks. The WMT 2014 English-to-German data set contains 4.5M sentence pairs. The\nEnglish-to-French contains 36M sentence pairs.\nResults of our experiments are summarized in Table 1. The Weighted Transformer achieves a 1.1\nBLEU score improvement over the state-of-the-art on the English-to-German task for the smaller\nnetwork and 0.5 BLEU improvement for the larger network. In the case of the larger English-to-\nFrench task, we note a 0.8 BLEU improvement for the smaller model and a 0.4 improvement for\nthe larger model. Also, note that the performance of the smaller model for Weighted Transformer is\nclose to that of the larger baseline model, especially for the English-to-German task. This suggests\nthat the Weighted Transformer better utilizes available model capacity since it needs only 30% of\nthe parameters as the baseline transformer for matching its performance. Our relative improvements\ndo not hinge on using the BLEU scores for comparison; experiments with the GLEU score proposed\nin Wu et al. (2016) also yielded similar improvements.\nFinally, we comment on the regularizing effect of the Weighted Transformer. Given the improved\nresults, a natural question is whether the results stem from improved regularization of the model. To\ninvestigate this, we report the testing loss of the Weighted Transformer and the baseline Transformer\nagainst the training loss in Figure 2. Models which have a regularizing effect tend to have lower\n5\n0 1 2 3 4 5 6\nTrain Loss \n2\n4\n6\n8\n10Test Loss\nBaseline Transformer\nWeighted Transformer\nFigure 2: Testing v/s Training Loss for the newstest2013 English-to-German task. The Weighted\nTransformer has lower testing loss compared to the baseline Transformer for the same training loss,\nsuggesting a regularizing effect.\nModel EN-DE BLEU EN-FR BLEU\nTransformer (small) (Vaswani et al., 2017) 27.3 38.1\nWeighted Transformer(small) 28.4 38.9\nTransformer (large) (Vaswani et al., 2017) 28.4 41.0\nWeighted Transformer(large) 28.9 41.4\nByteNet (Kalchbrenner et al., 2016) 23.7 -\nDeep-Att+PosUnk (Zhou et al., 2016) - 39.2\nGNMT+RL (Wu et al., 2016) 24.6 39.9\nConvS2S (Gehring et al., 2017) 25.2 40.5\nMoE (Shazeer et al., 2017) 26.0 40.6\nTable 1: Experimental results on the WMT 2014 English-to-German (EN-DE) and English-to-\nFrench (EN-FR) translation tasks. Our proposed model outperforms the state-of-the-art models\nincluding the Transformer (Vaswani et al., 2017). The small model corresponds to conﬁguration (A)\nin Table 2 while large corresponds to conﬁguration (B).\ntesting losses for the same training loss. We see this effect in our experiments suggesting that the\nproposed architecture may have better regularizing properties. This is not unexpected given similar\noutcomes for other branching-based strategies such as Shake-Shake Gastaldi (2017) and mixture-\nof-experts Shazeer et al. (2017).\n4.3 S ENSITIVITY ANALYSIS\nIn Table 2, we report sensitivity results on the newstest2013 English-to-German task. Speciﬁ-\ncally, we vary the number of layers in the encoder/decoder and compare the performance of the\nWeighted Transformer and the Transformer baseline. The results clearly demonstrate the beneﬁt of\nthe branched attention; for every experiment, the Weighted Transformer outperforms the baseline\ntransformer, in some cases by up to 1.3 BLEU points. As in the case of the baseline Transformer,\nincreasing the number of layers does not necessarily improve performance; a modest improvement\nis seen when the number of layers N is increased from 2 to 4 and 4 to 6 but the performance de-\ngrades when N is increased to 8. Increasing the number of heads from 8 to 16 in conﬁguration (A)\nyielded an even better BLEU score. However, preliminary experiments with h = 16and h = 32,\nlike in the case with N, degrade the performance of the model.\n6\nModel Settings BLEU params\nN d model dff h M P drop train steps ×106\nTransformer (C) 2 512 2048 8 NA 0.1 100K 23.7 36\nWeighted Transformer(C) 2 512 2048 8 8 0.1 60K 24.8 36\nTransformer 4 512 2048 8 NA 0.1 100K 25.3 50\nWeighted Transformer 4 512 2048 8 8 0.1 60K 26.2 50\nTransformer (A) 6 512 2048 8 NA 0.1 100K 25.8 65\nWeighted Transformer(A) 6 512 2048 8 8 0.1 60K 26.5 65\nTransformer 8 512 2048 8 NA 0.1 100K 25.5 80\nWeighted Transformer 8 512 2048 8 8 0.3 60K 25.6 80\nTransformer (B) 6 1024 4096 16 NA 0.3 300K 26.4 213\nWeighted Transformer(B) 6 1024 4096 16 16 0.3 250K 27.2 213\nTable 2: Experimental comparison between different variants of the Transformer (Vaswani et al.,\n2017) architecture and our proposed Weighted Transformer. Reported BLEU scores are evaluated\non the English-to-German translation development set, newstest2013.\nFigure 3: Convergence of the (α,κ) weights for the second encoder layer of Conﬁguration (C) for\nthe English-to-German newstest2013 task. We smoothen the curves using a mean ﬁlter. This shows\nthat the network does prioritize some branches more than others and that the architecture does not\nexploit a subset of the branches while ignoring others.\nIn Figure 3, we present the behavior of the weights (α,κ) for the second encoder layer of the\nconﬁguration (C) for the English-to-German newstest2013 task. The ﬁgure shows that, in terms of\nrelative weights, the network does prioritize some branches more than others; circumstantially by\nas much as 2×. Further, the relative ordering of the branches changes over time suggesting that the\nnetwork is not purely exploitative. A purely exploitative network, which would learn to exploit a\nsubset of the branches at the expense of the rest, would not be preferred since it would effectively\nreduce the number of available parameters and limit the representational power. Similar results are\nseen for other layers, including the decoder layers; we omit them for brevity.\n4.4 R ANDOMIZATION BASELINE\nThe proposed modiﬁcation can also be interpreted as a form of Shake-Shake regularization proposed\nin Gastaldi (2017). In this regularization strategy, random weights are sampled during forward and\nbackward passes for weighing the various branches in a multi-branch network. During test time, they\nare weighed equally. In our strategy, the weights are learned instead of being sampled randomly.\nConsequently, no changes to the model are required during test time.\n7\nWeights (α,κ) BLEU\nLearned 24.8\nRandom 21.1\nUniform 23.4\nTable 3: Performance of the architecture with random and uniform normalization weights on\nthe newstest2013 English-to-German task for conﬁguration (C). This shows that the learned (α,κ)\nweights of the Weighted Transformer are crucial to its performance.\nIn order to better understand whether the network beneﬁts from the learned weights or if, at test\ntime, random or uniform weights sufﬁce, we propose the following experiment: the weights for the\nWeighted Transformer, including(α,κ) are trained as before, but, during test time, we replace them\nwith (a) randomly sampled weights, and (b) 1/M where M is the number of incoming branches.\nIn Table 3, we report experimental results on the conﬁguration (C) of the Weighted Transformer on\nthe English-to-German newstest2013 data set (see Table 2 for details regarding the conﬁguration).\nIt is evident that random or uniform weights cannot replace the learned weights during test time.\nPreliminary experiments suggest that a Shake-Shake-like strategy where the weights are sampled\nrandomly during training also leads to inferior performance.\n4.5 G ATING\nIn order to analyze whether a hard (discrete) choice through gating will outperform our normal-\nization strategy, we experimented with using gates instead of the proposed concatenation-addition\nstrategy. Speciﬁcally, we replaced the summation in Equation (7) by a gating structure that sums up\nthe contributions of the top kbranches with the highest probabilities. This is similar to the sparsely-\ngated mixture of experts model in Shazeer et al. (2017). Despite signiﬁcant hyper-parameter tuning\nof k and M, we found that this strategy performs worse than our proposed mechanism by a large\nmargin. We hypothesize that this is due to the fact that the number of branches is low, typically less\nthan 16. Hence, sparsely-gated models lose representational power due to reduced capacity in the\nmodel. We plan to investigate the setup with a large number of branches and sparse gates in future\nwork.\n5 C ONCLUSIONS\nWe present the Weighted Transformer that trains faster and achieves better performance than the\noriginal Transformer network. The proposed architecture replaces the multi-head attention in the\nTransformer network by a multiple self-attention branches whose contributions are learned as a\npart of the training process. We report numerical results on the WMT 2014 English-to-German\nand English-to-French tasks and show that the Weighted Transformer improves the state-of-the-art\nBLEU scores by 0.5 and 0.4 points respectively. Further, our proposed architecture trains15 −40%\nfaster than the baseline Transformer. Finally, we present evidence suggesting the regularizing effect\nof the proposal and emphasize that the relative improvement in BLEU score is observed across\nvarious hyper-parameter settings for both small and large models.\nREFERENCES\nKarim Ahmed and Lorenzo Torresani. BranchConnect: Large-Scale Visual Recognition with\nLearned Branch Connections. arXiv preprint arXiv:1704.06010, 2017.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\nAntonio Valerio Miceli Barone, Jindˇrich Helcl, Rico Sennrich, Barry Haddow, and Alexandra Birch.\nDeep architectures for neural machine translation. arXiv preprint arXiv:1707.07631, 2017.\n8\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural net-\nworks. arXiv preprint arXiv:1611.01576, 2016.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder\nfor statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nXavier Gastaldi. Shake-Shake regularization. arXiv preprint arXiv:1705.07485, 2017.\nJonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. A convolutional encoder model\nfor neural machine translation. arXiv preprint arXiv:1611.02344, 2016.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional\nSequence to Sequence Learning. arXiv preprint arXiv:1705.03122, 2017.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-\nrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international\nconference on, pp. 6645–6649. IEEE, 2013.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nSepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem\nsolutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):\n107–116, 1998.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J ¨urgen Schmidhuber, et al. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying Word Vectors and Word Classiﬁers:\nA Loss Framework for Language Modeling. arXiv preprint arXiv:1611.01462, 2016.\nŁukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, pp. 3781–3789, 2016.\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray\nKavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.\nYoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. Structured attention networks.\narXiv preprint arXiv:1702.00887, 2017.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nTao Lei and Yu Zhang. Training RNNs as fast as CNNs. arXiv preprint arXiv:1709.02755, 2017.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou,\nand Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\nG´abor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589, 2017.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM\nlanguage models. arXiv preprint arXiv:1708.02182, 2017.\nAnkur P Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. arXiv preprint arXiv:1606.01933, 2016.\n9\nRomain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\narXiv:1608.05859, 2016.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909, 2015.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538, 2017.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. Journal of machine learning\nresearch, 15(1):1929–1958, 2014.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nIn Advances in neural information processing systems, pp. 3104–3112, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,\n2017.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\nlation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\nSaining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-\nformations for deep neural networks. arXiv preprint arXiv:1611.05431, 2016.\nWayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong\nYu, and Geoffrey Zweig. The Microsoft 2016 conversational speech recognition system. In\nAcoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pp.\n5255–5259. IEEE, 2017.\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward\nconnections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016.\n10",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6852568984031677
    },
    {
      "name": "Computer science",
      "score": 0.556744396686554
    },
    {
      "name": "Machine translation",
      "score": 0.5196484327316284
    },
    {
      "name": "Translation (biology)",
      "score": 0.5164070725440979
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3438633382320404
    },
    {
      "name": "Electrical engineering",
      "score": 0.18528828024864197
    },
    {
      "name": "Engineering",
      "score": 0.18022215366363525
    },
    {
      "name": "Voltage",
      "score": 0.13287895917892456
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 134
}