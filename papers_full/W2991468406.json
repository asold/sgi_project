{
    "title": "When Low Resource NLP Meets Unsupervised Language Model: Meta-pretraining Then Meta-learning for Few-shot Text Classification",
    "url": "https://openalex.org/W2991468406",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2372640170",
            "name": "Deng Shu-min",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A693219661",
            "name": "Zhang, Ningyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2350131134",
            "name": "Sun Zhanlin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2350853824",
            "name": "Chen, Jiaoyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2048668577",
            "name": "Chen, Huajun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2601450892",
        "https://openalex.org/W2753160622",
        "https://openalex.org/W2975576859",
        "https://openalex.org/W3037856073",
        "https://openalex.org/W2964316912",
        "https://openalex.org/W2939413764",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950361018",
        "https://openalex.org/W2432717477",
        "https://openalex.org/W2930925340",
        "https://openalex.org/W3106003309",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2604763608",
        "https://openalex.org/W2921848006",
        "https://openalex.org/W2964105864",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W1560724230",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2162708558",
        "https://openalex.org/W2798280127"
    ],
    "abstract": "Text classification tends to be difficult when data are deficient or when it is required to adapt to unseen classes. In such challenging scenarios, recent studies have often used meta-learning to simulate the few-shot task, thus negating implicit common linguistic features across tasks. This paper addresses such problems using meta-learning and unsupervised language models. Our approach is based on the insight that having a good generalization from a few examples relies on both a generic model initialization and an effective strategy for adapting this model to newly arising tasks. We show that our approach is not only simple but also produces a state-of-the-art performance on a well-studied sentiment classification dataset. It can thus be further suggested that pretraining could be a promising solution for few-shot learning of many other NLP tasks. The code and the dataset to replicate the experiments are made available at https://github.com/zxlzr/FewShotNLP.",
    "full_text": "arXiv:1908.08788v2  [cs.IR]  21 Nov 2019\nWhen Low Resource NLP Meets Unsupervised Language Model:\nMeta-pretraining Then Meta-learning for Few-shot T ext Cla ssiﬁcation\nShumin Deng 1,2 ∗ Ningyu Zhang 2,4 ∗ Zhanlin Sun 2,5 ∗ Jiaoyan Chen 6 Huajun Chen 1,2,3 †\n1. College of Computer Science and T echnology, Zhejiang Uni versity\n2. Alibaba-Zhejiang University Frontier T echnology Resea rch Center Joint Lab for Knowledge Engine\n3. The First Afﬁliated Hospital of Zhejiang University\n4. Alibaba Group\n5. School of Computer Science, Carnegie Mellon University\n6. Department of Computer Science, Oxford University\n{231sm,huajunsir}@zju.edu.cn, zhanlins@andrew.cmu.edu\njiaoyan.chen@cs.ox.ac.uk, ningyu.zny@alibaba-inc.com\nAbstract\nT ext classiﬁcation tends to be difﬁcult when data are deﬁ-\ncient or when it is required to adapt to unseen classes. In\nsuch challenging scenarios, recent studies have often used\nmeta-learning to simulate the few-shot task, thus negating\nimplicit common linguistic features across tasks. This pa-\nper addresses such problems using meta-learning and un-\nsupervised language models. Our approach is based on the\ninsight that having a good generalization from a few ex-\namples relies on both a generic model initialization and an\neffective strategy for adapting this model to newly arising\ntasks. W e show that our approach is not only simple but also\nproduces a state-of-the-art performance on a well-studied\nsentiment classiﬁcation dataset. It can thus be further sug -\ngested that pretraining could be a promising solution for fe w-\nshot learning of many other NLP tasks. The code and the\ndataset to replicate the experiments are made available at\nhttps://github.com/zxlzr/FewShotNLP.\nIntroduction\nDeep learning (DL) has achieved great success in many\nﬁelds owing to the advancements in optimization tech-\nniques, large datasets, and streamlined designs of deep neu -\nral architectures. However, DL is notorious for requiring\nlarge labeled datasets, which limits the scalability of a de ep\nmodel to new classes owing to the cost of annotation. Few-\nshot learning generally resolves the data deﬁciency proble m\nby recognizing novel classes from very few labeled exam-\nples. This limitation in the size of samples (only one or very\nfew examples) challenges the standard ﬁne-tuning method\nin DL. Early studies in this ﬁeld applied data augmenta-\ntion and regularization techniques to alleviate the overﬁt ting\nproblem caused by data scarcity but only to a limited ex-\ntent. Instead, researchers have been inspired by explorati on\nof meta-learning (Finn and et al. 2017) to leverage the dis-\ntribution over similar tasks. However, existing meta-lear ning\napproaches for few-shot learning can not explicitly disent an-\ngle task-agnostic and task-speciﬁc representations, and t hey\n∗ All authors contributed equally to this work.\n† Corresponding author.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www .aaai.org). All rights reserved.\nare not able to take advantage of the knowledge of linguistic\nproperties via unsupervised language models.\nIn this paper, we raise the question that whether it is\npossible to boost the performance of low-resource nat-\nural language processing with the large scale of raw cor-\npus via unsupervised leaning , which require us to handle\nboth task-agnostic and task-speciﬁc representation learn ing.\nThus we propose a Meta-pretraining Then Meta-learning\n(MTM) approach motivated by the observation that meta-\nlearning leads to learning a better parameter initializati on\nfor new tasks than multi-task learning across all tasks. The\nformer meta-pretraining is to learn task-agnostic represe nta-\ntions that explicitly learns a model parameter initializat ion\nfor enhanced predictive performance with limited supervi-\nsion. The latter meta-learning considers all classes as com -\ning from a joint distribution and seeks to learn model pa-\nrameters that can be quickly adapted via using each class’s\ntraining instances to enhance predictive performance on it s\ntest set. In other words, our approach explicitly disentang les\nthe task-agnostic and task-speciﬁc feature learning. Expe ri-\nmental results demonstrate that the proposed model achieve s\nsigniﬁcant improvement on public benchmark datasets.\nApproach\nProblem Deﬁnition\nFew-shot text classiﬁcation (Y u and et al. 2018;\nGeng and et al. 2019) is a task in which a classiﬁer\nmust adapt new classes that are not seen in training, given\nonly a few examples for each of these new classes. T o\nbe speciﬁc, we have a labeled training set with a set of\ndeﬁned classes Ctrain. Our goal is to output classiﬁers\non the testing set with a disjoint set of new classes Ctest\nwhen only a small labeled support set is available. If the\nsupport set contains K labeled examples for each of the\nC unique classes, the target few-shot problem is called a\nC-way-K-shot problem. The sample set is usually too small\nto train a supervised classiﬁcation model. T o this end, we\ntry to utilize meta-learning method on the training set to\nextract task-agnostic knowledge, which may perform better\nfor few-shot text classiﬁcation on the test set.\nT raining Procedure\nT ask-agnostic Meta Pretraining. Given all the training\nsamples, we ﬁrst utilize pretraining strategies such as BER T\nto learn task-agnostic contextualized features that captu re\nlinguistic properties to beneﬁt downstream few-shot text\nclassiﬁcation tasks.\nMeta-learning T ext Classiﬁcation. Given the pretrained\nlanguage representations, we construct episodes to comput e\ngradients and update the model in each training iteration.\nAlgorithm 1 MTM Algorithm\nRequire: Training Datapoints D=\n{\nx(j), y(j)}\n1: Construct a task Tj with training examples using a sup-\nport set S(j)\nK and a test example D′\nj =\n(\nx(j), y(j))\n2: Randomly initialize θ\n3: Pre-train Dwith unsupervised language models\n4: Denote p(T) as distribution over tasks\n5: while not done do\n6: Sample batch of tasks Ti ∼p(T):\n7: for for all Ti do\n8: Evaluate ∇θ LTi (fθ ) using S(j)\nK\n9: Compute adapted parameters with gradient de-\nscent: θ′\ni = θ −α ∇θ LTi (fθ )\n10: Update θ ← θ −β∇θ\n∑\nTi∼p(T ) LTi\n(\nfθ′\ni\n)\nusing\neach D′\ni from Ti and LTi\nExperiments\nDatasets and Evaluation\nW e use the multiple tasks with the multi-domain sentiment\nclassiﬁcation dataset ARSC 1 . This dataset comprises En-\nglish reviews for 23 types of products on Amazon. For\neach product domain, there are three different binary clas-\nsiﬁcation tasks. These buckets then form 23 ×3 = 69\ntasks in total. W e select 12(4 ×3) tasks from four do-\nmains as the test set, with only ﬁve examples as support set\nfor each label in the test set. W e evaluate the performance\nby few-shot classiﬁcation accuracy following previous stu d-\nies in few-shot learning (Snell, Swersky, and Zemel 2017).\nT o evaluate the proposed model objectively with the base-\nlines, note that for ARSC, the support set for testing is ﬁxed\nby (Y u and et al. 2018); therefore, we need to run the test\nepisode once for each of the target tasks. The mean accu-\nracy from the 12 target tasks are compared to those of the\nbaseline models in accordance with (Y u and et al. 2018).\nEvaluation Results\nThe evaluation results are shown in T able 1:\nMTM is our current approach, Match Network\n(V inyals and et al. 2016) is a few-shot learning model\nusing metric-based attention method, Prototypical\nNetwork (Snell, Swersky, and Zemel 2017) is a deep\nmatrix-based method using sample averages as class pro-\ntotypes, MAML (Finn and et al. 2017) is a model-agnostic\n1 https://github.com/Gorov/DiverseFewShot Amazon\nmethod that is compatible with any model trained with\ngradient descent and applicable to a variety of learning\nproblems, Relation Network (Sung and et al. 2018) is a\nmetric-based few-shot learning model that uses a neural\nnetwork as the distance measurement and calculate class\nvectors by summing sample vectors in the support set,\nROBUSTTC-FSL (Y u and et al. 2018) is an approach that\ncombines adaptive metric methods by clustering the tasks,\nInduction-Network-Routing (Geng and et al. 2019) is\na recent state-of-the-art method which learn generalized\nclass-wise representations by combining the dynamic\nrouting algorithm with a typical meta-learning framework.\nFrom the results shown in T able 1, we observe that our\napproach achieves the best results amongst all meta-learni ng\nmodels. Note that, our model is task-agnostic, which means\nit can be easily adapted to any other NLP tasks.\nModel Mean Acc\nMatching Network 65.73\nPrototypical Network 68.15\nRelation Network 83.74\nMAML 78.33\nROBUSTTC-FSL 83.12\nInduction-Network-Routing 85.47\nMTM 90.01*\nT able 1: Comparison of mean accuracy (%) on ARSC. * in-\ndicates pvalue < 0. 01 in a paired t-test (10-fold) evaluation.\nConclusion\nIn this study, we attempt to analyze language meta-\npretraining with meta-learning for few-shot text classiﬁc a-\ntion. Results show that our model outperforms conventional\nstate-of-the-art few-shot text classiﬁcation models. In t he fu-\nture, we plan to apply our method to other NLP scenarios.\nAcknowledgments\nW e want to express gratitude to the anonymous reviewers\nfor their hard work and kind comments and this work is\nfunded by NSFC 91846204, national key research program\n2018YFB1402800, and Alibaba CangJingGe(Knowledge\nEngine) Research Plan.\nReferences\n[Finn and et al. 2017] Finn, C., and et al. 2017. Model-\nagnostic meta-learning for fast adaptation of deep network s.\nIn ICML.\n[Geng and et al. 2019] Geng, R., and et al. 2019. Few-shot\ntext classiﬁcation with induction network. In EMNLP.\n[Snell, Swersky, and Zemel 2017] Snell, J.; Swersky, K.; and\nZemel, R. 2017. Prototypical networks for few-shot learn-\ning. In NIPS, 4077–4087.\n[Sung and et al. 2018] Sung, F ., and et al. 2018. Learning to\ncompare: Relation network for few-shot learning. In CVPR,\n1199–1208.\n[V inyals and et al. 2016] V inyals, O., and et al. 2016. Match-\ning networks for one shot learning. In NIPS, 3630–3638.\n[Y u and et al. 2018] Y u, M., and et al. 2018. Diverse few-\nshot text classiﬁcation with multiple metrics. In NAACL."
}