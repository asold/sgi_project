{
    "title": "Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis",
    "url": "https://openalex.org/W2970975962",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2251042621",
            "name": "Hiroki Ouchi",
            "affiliations": [
                "Tohoku University",
                "RIKEN Center for Advanced Intelligence Project"
            ]
        },
        {
            "id": "https://openalex.org/A1749670362",
            "name": "Jun Suzuki",
            "affiliations": [
                "Tohoku University",
                "RIKEN Center for Advanced Intelligence Project"
            ]
        },
        {
            "id": "https://openalex.org/A2084773436",
            "name": "Kentaro Inui",
            "affiliations": [
                "Tohoku University",
                "RIKEN Center for Advanced Intelligence Project"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2982226611",
        "https://openalex.org/W2962803243",
        "https://openalex.org/W2151170651",
        "https://openalex.org/W2803570443",
        "https://openalex.org/W2963326042",
        "https://openalex.org/W2963463240",
        "https://openalex.org/W1604005662",
        "https://openalex.org/W2950333415",
        "https://openalex.org/W1584041440",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963011474",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963826681",
        "https://openalex.org/W2140177290",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W1623072288",
        "https://openalex.org/W2116512345",
        "https://openalex.org/W2963246595",
        "https://openalex.org/W2055963237",
        "https://openalex.org/W2126851059",
        "https://openalex.org/W2115792525",
        "https://openalex.org/W2155069789",
        "https://openalex.org/W2963022746",
        "https://openalex.org/W1682040381",
        "https://openalex.org/W2142012908",
        "https://openalex.org/W2951299559",
        "https://openalex.org/W2117339222",
        "https://openalex.org/W2964057329",
        "https://openalex.org/W1970849810",
        "https://openalex.org/W2120354757",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2111362445",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2102910670",
        "https://openalex.org/W2398133062",
        "https://openalex.org/W1882958252",
        "https://openalex.org/W91242194",
        "https://openalex.org/W2756978580",
        "https://openalex.org/W2151075664",
        "https://openalex.org/W2148603752",
        "https://openalex.org/W2099153428",
        "https://openalex.org/W4300490660",
        "https://openalex.org/W2540485556",
        "https://openalex.org/W2107008379",
        "https://openalex.org/W2594718649",
        "https://openalex.org/W2798921788",
        "https://openalex.org/W2104094955",
        "https://openalex.org/W2551835155",
        "https://openalex.org/W2158108973",
        "https://openalex.org/W2963563735",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2158847908",
        "https://openalex.org/W2556468274",
        "https://openalex.org/W2891691791",
        "https://openalex.org/W2915816387",
        "https://openalex.org/W2962690139",
        "https://openalex.org/W2891602716",
        "https://openalex.org/W3146885639"
    ],
    "abstract": "Hiroki Ouchi, Jun Suzuki, Kentaro Inui. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 3665–3671,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n3665\nTransductive Learning of Neural Language Models\nfor Syntactic and Semantic Analysis\nHiroki Ouchi1,2 Jun Suzuki2,1 Kentaro Inui2,1\n1 RIKEN Center for Advanced Intelligence Project 2 Tohoku University\nhiroki.ouchi@riken.jp, {jun.suzuki,inui}@ecei.tohoku.ac.jp\nAbstract\nIn transductive learning, an unlabeled test set\nis used for model training. While this setting\ndeviates from the common assumption of a\ncompletely unseen test set, it is applicable in\nmany real-world scenarios, where the texts to\nbe processed are known in advance. However,\ndespite its practical advantages, transductive\nlearning is underexplored in natural language\nprocessing. Here, we conduct an empirical\nstudy of transductive learning for neural mod-\nels and demonstrate its utility in syntactic and\nsemantic tasks. Speciﬁcally, we ﬁne-tune lan-\nguage models (LMs) on an unlabeled test set to\nobtain test-set-speciﬁc word representations.\nThrough extensive experiments, we demon-\nstrate that despite its simplicity, transductive\nLM ﬁne-tuning consistently improves state-of-\nthe-art neural models in both in-domain and\nout-of-domain settings.\n1 Introduction\nIn supervised learning, a model is trained on a\ntraining set and its generalization performance is\nevaluated on an unseen test set. In this setting, the\nmodel has no access to the test set during training.\nHowever, the assumption of a completely unseen\ntest set is not always necessary. In many cases,\ncertain aspects of the test set are already known at\ntraining time. For example, a company may want\nto annotate a large number of existing documents\nautomatically (Section 3). In such a scenario, the\ntexts to be processed are known in advance, and\nusing the model trained on the texts themselves to\nprocess them can be more efﬁcient. Using an un-\nlabeled test set in this way is the key idea behind\ntransductive learning.\nIn transductive learning (Vapnik, 1998), an un-\nlabeled test set is given in the training phase. That\nis, the inputs of the test set, i.e., the raw texts, can\nbe used during training, but the labels are never\nused. In the test phase, the trained model is eval-\nuated on the same test set. Despite its practical\nadvantages, transductive learning has received lit-\ntle attention in natural language processing (NLP).\nAfter the pioneering work of Joachims (1999),\nwho proposed a transductive support vector ma-\nchine for text classiﬁcation, transductive methods\nfor linear models have been investigated in only\na few tasks, such as lexical acquisition (Duh and\nKirchhoff, 2006) and machine translation (Uefﬁng\net al., 2007). In particular, transductive learning\nwith neural networks is underexplored.\nHere, we investigate the impact of transductive\nlearning on state-of-the-art neural models in syn-\ntactic and semantic tasks, namely syntactic chunk-\ning and semantic role labeling (SRL). Speciﬁcally,\ninspired by recent ﬁndings that language model\n(LM)-based word representations yield large per-\nformance improvement (Devlin et al., 2019), we\nﬁne-tune Embeddings from Language Models\n(ELMo) (Peters et al., 2018) on an unlabeled test\nset and use them in each task-speciﬁc model. Typ-\nically, LMs are trained on a large-scale corpus\nwhose word distributions are different from the\ntest set. By contrast, transductive learning allows\nus to ﬁt LMs directly to the distributions of the\ntest set. Our experiments show the effectiveness\nof transductive LM ﬁne-tuning.\nIn summary, our main contributions are:\n•This work is the ﬁrst to introduce an LM ﬁne-\ntuning method to transductive learning1.\n•Through extensive experiments in both in-\ndomain and out-of-domain settings, we\ndemonstrate that transductive LM ﬁne-tuning\nconsistently improves state-of-the-art neural\nmodels in syntactic and semantic tasks.\n1Our code and scripts are publicly available at\nhttps://github.com/hiroki13/transductive-language-models.\n3666\n2 Related Work\nTransductive learning. Vapnik advocated and\nformalized transductive learning (Vapnik, 1998;\nGammerman et al., 1998), which has been applied\nto text classiﬁcation (Joachims, 1999; Ifrim and\nWeikum, 2006) and image processing (Bruzzone\net al., 2006; Sener et al., 2016; Liu et al., 2019).\nAlthough some studies have presented transduc-\ntive methods for linear models in other tasks (Duh\nand Kirchhoff, 2006; Uefﬁng et al., 2007; Chen\net al., 2008; Alexandrescu and Kirchhoff, 2009),\ntransductive methods for neural models are under-\nexplored in NLP.\nUnsupervised domain adaptation. Transduc-\ntive learning is related to unsupervised domain\nadaptation, in which models are adapted to a tar-\nget domain by using unlabeled target domain texts\n(Ben-David et al., 2010; Shi and Sha, 2012). This\nsetting does not allow models to access the test\nset, which is the main difference between unsuper-\nvised domain adaptation and transductive learn-\ning. Various unsupervised adaptation methods\nhave been proposed for linear models (Blitzer\net al., 2006; Jiang and Zhai, 2007; Tsuboi et al.,\n2009; Søgaard, 2013). In the context of neural\nmodels, adversarial domain adaptation (Ganin and\nLempitsky, 2015; Ganin et al., 2016; Guo et al.,\n2018), importance weighting (Wang et al., 2017),\nstructural correspondence learning (Ziser and Re-\nichart, 2017), self/tri/co-training (Saito et al.,\n2017; Ruder and Plank, 2018), and other tech-\nniques orthogonal to transductive LM ﬁne-tuning\nhave been applied successfully in unsupervised\ndomain adaptation 2. Integrating these methods\nwith transductive LM ﬁne-tuning is an interesting\ndirection for future research.\nLM-based word representations. Recently,\nLM-based word representations pre-trained on\nunlabeled data have gained considerable atten-\ntion (Peters et al., 2018; Radford et al., 2018;\nDevlin et al., 2019). The most related method to\nours is Universal Language Model Fine-tuning\n(ULMFiT), which pre-trains an LM on a large\ngeneral-domain corpus and ﬁne-tunes it on the\ntarget task (Howard and Ruder, 2018). Inspired\nby these studies, we introduce LM-based word\nrepresentation in transductive learning.\n2Feature augmentation is considered a supervised domain\nadaptation method (Daume III, 2007; Kim et al., 2016).\nTransductiveLM Fine-tuning(2)(1)\n(3)\nFigure 1: Training procedure. (1) LM pre-training:\nthe LM is ﬁrstly pre-trained on the large-scale unla-\nbeled corpus Dlarge = {Xlarge\ni }Nlarge\ni=1 . (2) Transduc-\ntive LM ﬁne-tuning: the LM is then ﬁne-tuned on the\nunlabeled test set Dtest = {Xtest\ni }Ntest\ni=1 . Note that the\ntest set used for training is the identical one used in\nevaluation. (3) Task-speciﬁc model training: the task-\nspeciﬁc model is trained on the training set Dtrain =\n{(Xtrain\ni , Ytrain\ni )}Ntrain\ni=1 . Ldenotes the loss function.\n3 Neural Transductive Learning\nMotivation. Suppose that a company has re-\nceived a vast amount of customer reviews and\nwants to automatically process these reviews more\naccurately, even if it takes some time. For this pur-\npose, they do not have to build a model that works\nwell on new unseen reviews. Instead, they want a\nmodel that works well on only the reviews in hand.\nIn this situation, using these reviews themselves to\ntrain a model can be more efﬁcient. This is the\nkey motivation for developing effective and prac-\ntical transductive learning methods. Toward this\ngoal, we develop transductive methods for state-\nof-the-art neural models.\nProblem formulation. In the training phase, a\ntraining set Dtrain = {(Xtrain\ni , Ytrain\ni )}Ntrain\ni=1 and an\nunlabeled test set Dtest = {Xtest\ni }Ntest\ni=1 are used for\nmodel training, where Xi is an input, e.g., a sen-\ntence, and Yi represents target labels, e.g., labels\nfrom a set of syntactic or semantic annotations. In\nthe test phase, the trained model is used for pre-\ndicting labels and is evaluated on the same test set\nDtest.\nMethod. We present a simple transductive\nmethod for neural models. Speciﬁcally, we ﬁne-\ntune an LM on an unlabeled test set. Figure 1\nillustrates the training procedure that consists of\nthe following steps: (1) LM pre-training, (2)\nTransductive LM ﬁne-tuning and (3) task-speciﬁc\nmodel training. We ﬁrst train an LM on a large-\nscale unlabeled corpus Dlarge and then ﬁne-tune\nthe LM on an unlabeled test set Dtest. Finally, we\nuse the ﬁne-tuned LM as the embedding layer of\n3667\nTraining Development Test\nSents Preds Sents Preds Sents Preds\nBC 11.8k 28.9k 2.1k 5.0k 2,0k 5.4k\nBN 10.6k 3.1k 1.2k 3,9k 1.2k 3.7k\nMZ 6.9k 2.4k 0.6k 2.1k 0.7k 2.6k\nNW 34.9k 96.6k 5.8k 16.6k 1.8k 5.8k\nPT 21.5k 34.9k 1.7k 2.5k 1.2k 2.8k\nTC 12.8k 16.2k 1.6k 2.0k 1.3k 1.7k\nWB 16.9k 20.0k 2.3k 2.8k 0.9k 2.2k\nTable 1: Dataset statistics on the CoNLL-2012 dataset.\n1k = 1,000. Column “Sents” denotes the number of\nsentences in each dataset. Column “Preds” denotes the\nnumber of predicates in each dataset.\neach task-speciﬁc model and train the model on a\ntraining set Dtrain.\nΘ′ ← argminΘ Llm(Θ|Dlarge), (1)\nΘ\n′′\n← argminΘ′ Llm(Θ′|Dtest), (2)\nΦ′ ← argminΦ Ltask(Φ|Θ\n′′\n, Dtrain). (3)\nHere, Llm and Ltask are the loss functions for\nan LM and task-speciﬁc model, respectively. 3\nIn the LM pre-training and ﬁne-tuning phases\n(Eqs. 1 and 2), we ﬁrst train the initial LM param-\neters Θ and then ﬁne-tune the pre-trained parame-\nters Θ′. In the task-speciﬁc training phase (Eq. 3),\nwe ﬁx the ﬁne-tuned LM parameters Θ′′ used for\nthe embedding layer of a task-speciﬁc model, and\ntrain only the task-speciﬁc model parameters Φ.\n4 Experiments\nTasks. To investigate the effectiveness of trans-\nductive LM ﬁne-tuning for syntactic and semantic\nanalysis, we conduct experiments in syntactic\nchunking (Ramshaw and Marcus, 1999; Sang\nand Buchholz, 2000; Ponvert et al., 2011) and\nSRL (Gildea and Jurafsky, 2002; Palmer et al.,\n2005; Carreras and M `arquez, 2005) 4. The goal\nof syntactic chunking is to divide a sentence\ninto non-overlapping phrases that consist of\nsyntactically related words. The goal of SRL is to\nidentify semantic arguments for each predicate.\nFor example, consider the following sentence:\nThe man kept a cat\nSYNCHUNK [ NP ] [ NP ]\nSEMROLE [ A0 ] [ A1 ]\n3In our experiments (Section 4), both losses were given\nby the negative log-likelihood (Appendix A).\n4This paper addresses span-based, PropBank-style SRL.\nDetailed descriptions on other lines of SRL research (e.g.\ndependency-based SRL and FrameNet-based SRL) can be\nfound in Baker et al. (1998); Das et al. (2014); Surdeanu et al.\n(2008); Hajiˇc et al. (2009).\nIn syntactic chunking, given the input sentence,\nsystems have to recognize “The man” and “a cat”\nas noun phrases (NP). In SRL, given the input sen-\ntence and the target predicate “kept”, systems have\nto recognize “The man” as theA0 argument and “a\ncat” as the A1 argument. For syntactic chunking,\nwe adopted the experimental protocol by Ponvert\net al. (2011) and for SRL, we followed Ouchi et al.\n(2018) (details in Appendix A).\nDatasets. We perform experiments using the\nCoNLL-2012 dataset5. To investigate the perfor-\nmances under in-domain and out-of-domain set-\ntings, we use each of the seven domains in the\nCoNLL-2012 dataset. Table 1 shows the data\nstatistics. Each test set contains at most 2,000 sen-\ntences. Compared with previous studies, such as\nXiao and Guo (2013) that used 570,000 sentences\nas unlabeled data for unsupervised domain adapta-\ntion of syntactic chunking, our transductive exper-\niments can be regarded as a low-resource adapta-\ntion setting. As a large-scale unlabeled raw corpus\nfor LM training, we use the 1B word benchmark\ncorpus (Chelba et al., 2013).\nModel setup. We use ELMo (Peters et al., 2018)\nas an LM. For syntactic chunking, we use a vari-\nant of the Reconciled Span Parser (Joshi et al.,\n2018). For SRL, we use the span selection model\n(BiLSTM-Span model) (Ouchi et al., 2018). Each\nmodel is trained on a source domain training set\nand was evaluated on a target domain test set6. The\ndevelopment set is also the source domain, and\nit is used for hyperparameter tuning 7. Consider\nthe case where NW →BC, i.e., the source domain\nis the newswire NW and the target domain is the\nbroadcast conversation BC. We ﬁrst train ELMo\non the large-scale raw corpus (one billion word\nbenchmark corpus) and ﬁne-tune it on the BC test\nset. We then train syntactic and semantic models\nthat use the ﬁne-tuned ELMo on the NW training\nset. During the task-speciﬁc model training, we\nfreeze the ﬁne-tuned ELMo. We select hyperpa-\nrameters by using theNW development set. Finally,\nwe evaluate the trained model on theBC test set. In\nthe same way, we conduct training and evaluation\nfor each domain pair.\n5We used the version of OntoNotes downloaded at:\nhttp://cemantix.org/data/ontonotes.html.\n6We used the ofﬁcial evaluation scripts downloaded\nat https://www.clips.uantwerpen.be/conll2000/chunking/ and\nhttp://www.lsi.upc.edu/ srlconll/soft.html.\n7All models and hyperparameters are described in Appen-\ndices B, C, and D.\n3668\nsrc →tgt BC BN MZ NW PT TC WB Averaged F1\nSYNTACTIC CHUNKING\nBC 93.0 / 93.5 92.9 / 93.0 90.0 / 90.6 88.1 / 88.7 94.1 / 94.9 84.5 / 85.1 89.4 / 89.8 90.3 / 90.8\nBN 92.5 / 93.0 94.7 / 95.0 91.2 / 91.4 90.0 / 90.6 94.8 / 95.3 84.0 / 84.9 89.9 / 90.7 91.0 / 91.6\nMZ 89.9 / 90.8 91.6 / 92.3 92.3 / 92.5 89.2 / 90.0 93.4 / 94.3 80.5 / 82.2 89.9 / 90.9 89.5 / 90.4\nNW 91.4 / 92.0 93.7 / 93.9 92.2 / 92.6 94.2 / 94.5 95.7 / 96.1 83.3 / 84.2 92.3 / 92.9 91.8 / 92.3\nPT 87.1 / 88.2 86.9 / 87.5 85.6 / 86.9 81.0 / 82.7 97.5 / 97.7 79.0 / 80.0 86.9 / 88.1 86.3 / 87.3\nTC 87.3 / 88.2 87.2 / 87.5 84.1 / 85.4 80.8 / 82.3 93.0 / 93.7 89.3 / 89.5 85.4 / 86.5 86.7 / 87.6\nWB 91.8 / 92.3 93.4 / 93.7 91.7 / 92.2 91.0 / 91.5 95.6 / 96.0 83.6 / 85.1 93.0 / 93.5 91.4 / 92.0\nSEMANTIC ROLE LABELING\nBC 83.3 / 83.9 78.9 / 79.3 74.2 / 74.8 71.0 / 72.4 82.8 / 84.4 80.2 / 80.6 78.7 / 79.8 78.4 / 79.3\nBN 80.3 / 81.2 83.3 / 83.5 76.5 / 77.4 75.0 / 75.7 86.5 / 86.8 77.1 / 78.0 78.8 / 79.9 79.6 / 80.4\nMZ 76.4 / 77.3 76.6 / 77.3 80.2 / 80.6 73.8 / 74.8 84.8 / 87.2 72.8 / 73.3 77.5 / 78.7 77.4 / 78.5\nNW 79.2 / 80.1 79.8 / 80.0 79.5 / 80.0 83.8 / 84.4 88.3 / 89.0 75.5 / 76.5 81.1 / 81.8 81.0 / 81.7\nPT 71.2 / 72.1 67.4 / 67.8 66.6 / 68.0 64.7 / 66.0 92.8 / 93.0 72.6 / 73.9 76.2 / 77.2 73.1 / 74.0\nTC 73.8 / 74.1 67.6 / 67.8 64.5 / 64.9 59.2 / 60.2 79.0 / 80.4 83.3 / 83.6 71.3 / 72.5 71.2 / 71.9\nWB 74.1 / 74.4 71.7 / 72.4 72.0 / 72.8 71.4 / 72.0 87.8 / 88.8 76.3 / 76.7 81.8 / 82.4 76.4 / 77.1\nTable 2: Main results under cross-domain settings, src (“source”, training set)→tgt (“target”, test set). Cells show\nthe F1 scores of the baseline model (before the slash) and the transductive model (after the slash). Column “Av-\neraged F1” represents the F1 scores averaged across the target domains. Domains are as follows: BC = Broadcast\nConversation, BN = Broadcast News, MZ = Magazine, NW = Newswire, PT = New Testament, TC = Telephone\nConversation, and WB = Weblogs and Newsgroups.\nResults. Table 2 shows the F1 scores on each\ntest set. All reported F1 scores are the average of\nﬁve distinct trials using different random seeds. In\neach cell, the left-hand side denotes the F1 score of\nthe baseline (using a base LM without ﬁne-tuning)\nand the right-hand side denotes F1 of the trans-\nductive models (using a ﬁne-tuned LM on each\ntest set). In in-domain (same source/target do-\nmains, e.g., BC→BC) and out-of-domain (differ-\nent source/target domains, e.g., BC→NW) settings,\nall transductive models consistently outperformed\nthe baselines, which suggests that transductive LM\nﬁne-tuning improves performance of neural mod-\nels. Although the improvements were undramatic\n(around 1.0 F1 gain), these consistent improve-\nments can be regarded as valuable empirical re-\nsults because of the difﬁculty of unsupervised and\nlow-resource adaptation settings.\n5 Analysis\nComparison between unsupervised domain\nadaptation and transduction. In unsupervised\ndomain adaptation, target domain unlabeled data\n(the texts whose domain is the same as that of a\ntest set) is used for adaptation. Although the do-\nmain is identical between target domain data and a\ntest set, their word distributions are somewhat dif-\nferent. In transductive learning, because an unla-\nbeled test set can be used for training, it is possible\nto adapt LMs directly to the word distributions of\nthe test set. Here, we investigate whether adapt-\ning LMs directly to each test set is more effective\nSyntactic chunking Semantic role labeling\nCU T CU T\nBC 90.4 90.8 78.6 79.3\nBN 91.1 91.6 79.8 80.4\nMZ 90.0 90.4 77.9 78.5\nNW 92.1 92.3 81.1 81.7\nPT 87.1 87.3 73.5 74.0\nTC 87.1 87.6 71.3 71.6\nWB 91.8 92.0 76.6 77.1\nTable 3: Performance comparison between LM ﬁne-\ntuning on target domain unlabeled data of the same size\nas each test set, “Controlled Unlabeled data (CU),” and\ntransductive LM ﬁne-tuning on each test set (T). Cells\nshow the F1 scores averaged across the target domains.\nthan adapting LMs to each target domain unla-\nbeled data. Similarly to our transductive method\nshown in Figure 1, we ﬁrst train LMs on the large-\nscale unlabeled corpus (the 1B word benchmark\ncorpus) and then ﬁne-tune them on the unlabeled\ntarget domain data 8. In addition, we control the\nsizes of the target domain unlabeled data and test\nsets. That is, we use the same number of sen-\ntences in the unlabeled data of each target domain\nas in each test set. Table 3 shows the F1 scores\naveraged across all the target domains. The trans-\nductive models (T) consistently outperformed the\ndomain-adapted models (CU). This demonstrates\nthat adapting LMs directly to test sets is more ef-\nfective than adapting them to target domain unla-\nbeled data.\n8As target domain unlabeled data, we use the CoNLL-\n2012 training set of each domain.\n3669\nSyntactic chunking Semantic role labeling\nU U + T U U + T\nBC 90.5 91.0 79.0 79.4\nBN 91.3 91.6 80.1 80.6\nMZ 90.2 90.6 78.3 78.7\nNW 92.1 92.5 81.5 81.9\nPT 87.3 87.7 73.6 74.3\nTC 87.2 87.6 71.4 72.0\nWB 91.8 92.2 76.8 77.2\nTable 4: Performance comparison between LM ﬁne-\ntuning on target domain unlabeled data (U) and on the\ncombination of the unlabeled data and test sets (U +\nT). Cells show the F1 scores averaged across the target\ndomains.\nCoNLL 2000 2005 2012\nWSJ Brown\nBASE 96.6 87.7 78.3 86.2\nTRANS 96.7 87.9 * 79.5* 86.6*\nClark et al. (2018) 97.0 - - -\nPeters et al. (2017) 96.4 - - -\nHashimoto et al. (2017) 95.8 - - -\nWang et al. (2019) - 88.2 79.3 86.4\nLi et al. (2019) - 87.7 80.5 86.0\nOuchi et al. (2018) - 87.6 78.7 86.2\nHe et al. (2018) - 87.4 80.4 85.5\nTable 5: Standard benchmark results. Cells show\nthe F1 scores on each test set. The CoNLL-2000\nand CoNLL-2005/2012 datasets are used for syntactic\nchunking and SRL, respectively. Results of the trans-\nductive models (TRANS ) marked with * are statistically\nsigniﬁcant compared to the baselines (BASE ) using the\npermutation test (p <0.05).\nCombination of unsupervised domain adapta-\ntion and transduction. In real-world situations,\nlarge-scale unlabeled data of target domains is\nsometimes available. In such cases, LMs can be\ntrained on both the target domain unlabeled data\nand the test sets. Here, we investigate the effec-\ntiveness of using both datasets. Table 4 shows the\nF1 scores averaged across all the target domains.\nFine-tuning the LMs on the target domain unla-\nbeled data as well as each test set (U + T) showed\nbetter performance than ﬁne-tuning them only on\nthe target domain unlabeled data (U). This com-\nbination of tranduction with unsupervised domain\nadaptation further improves performance.\nEffects in standard benchmarks.Some studies\nindicated that when promising new techniques are\nonly evaluated on very basic models, determining\nhow much (if any) improvement will carry over to\nstronger models can be difﬁcult (Denkowski and\nNeubig, 2017; Suzuki et al., 2018). Motivated by\nsuch studies, we provide the results in standard\nbenchmark settings. For syntactic chunking, we\nuse the CoNLL-2000 dataset (Sang and Buchholz,\n2000) and follow the standard experimental proto-\ncol (Hashimoto et al., 2017). For SRL, we use the\nCoNLL-2005 (Carreras and M `arquez, 2005) and\nCoNLL-2012 datasets (Pradhan et al., 2012) and\nfollow the standard experimental protocol (Ouchi\net al., 2018). Table 5 shows the F1 scores of our\nmodels and those of existing models. The results\nof the baseline model were comparable with those\nof the state-of-the-art models, and the transduc-\ntive model consistently outperformed the baseline\nmodel9. Note that we cannot fairly compare the\ntransductive and existing models due to the differ-\nence in settings. These results, however, demon-\nstrate that transductive LM ﬁne-tuning improves\nstate-of-the-art chunking and SRL models.\n6 Conclusion\nIn this study, we investigated the impact of trans-\nductive learning on state-of-the-art neural mod-\nels in syntactic and semantic tasks. Speciﬁcally,\nwe ﬁne-tuned an LM on an unlabeled test set.\nThrough extensive experiments, we demonstrated\nthat, despite its simplicity, transductive LM ﬁne-\ntuning contributes to consistent performance im-\nprovement of state-of-the-art syntactic and seman-\ntic models in cross-domain settings. One inter-\nesting line of future work is to explore effective\ntransductive methods for task-dependent (neural)\nlayers. For instance, as some unsupervised do-\nmain adaptation methods can be applied to trans-\nductive learning, integrating them with transduc-\ntive LM ﬁne-tuning may further improve their per-\nformance. Another line of our future work is to\napply these transductive methods to various NLP\ntasks and investigate their performance.\nAcknowledgments\nThis work was partially supported by JSPS\nKAKENHI Grant Number JP19H04162 and\nJP19K20351. We would like to thank Benjamin\nHeinzerling, Ana Brassard, Sosuke Kobayashi,\nHitomi Yanaka, and the anonymous reviewers for\ntheir insightful comments.\n9While the improvements in SRL were statistically sig-\nniﬁcant compared to the baseline, the improvement in syn-\ntactic chunking was not. One reason for this is that the F1\nscore of the baseline in syntactic chunking is already high\nand there is less room for improvement. Since Clark et al.\n(2018) achieved 97.0 F1 with multi-task learning, missing\ninformation for further improvement might be derived from\nother tasks.\n3670\nReferences\nAndrei Alexandrescu and Katrin Kirchhoff. 2009.\nGraph-based learning for statistical machine trans-\nlation. In Proceedings of HLT-NAACL, pages 119–\n127.\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe.\n1998. The berkeley framenet project. In Proceed-\nings of ACL-COLING, pages 86–90.\nShai Ben-David, John Blitzer, Koby Crammer, Alex\nKulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. 2010. A theory of learning from different\ndomains. Machine learning, 79(1-2):151–175.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of ACL, page 120.\nLorenzo Bruzzone, Mingmin Chi, and Mattia Mar-\nconcini. 2006. A novel transductive svm for semisu-\npervised classiﬁcation of remote-sensing images.\nIEEE Transactions on Geoscience and Remote Sens-\ning, 44(11):3363–3373.\nXavier Carreras and Llu ´ıs M`arquez. 2005. Introduc-\ntion to the conll-2005 shared task: Semantic role la-\nbeling. In Proceedings of CoNLL, pages 152–164.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nYaodong Chen, Ting Wang, Huowang Chen, and Xis-\nhan Xu. 2008. Semantic role labeling of chinese\nusing transductive svm and semantic heuristics. In\nProceedings of IJCNLP, pages 919–924.\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018. Semi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of EMNLP, pages 1914–1925.\nDipanjan Das, Desai Chen, Andr ´e FT Martins, Nathan\nSchneider, and Noah A Smith. 2014. Frame-\nsemantic parsing. Computational linguistics ,\n40(1):9–56.\nHal Daume III. 2007. Frustratingly easy domain adap-\ntation. In Proceedings of ACL, pages 256–263.\nMichael Denkowski and Graham Neubig. 2017.\nStronger baselines for trustable results in neural ma-\nchine translation. In Proceedings of the First Work-\nshop on Neural Machine Translation, pages 18–27.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nKevin Duh and Katrin Kirchhoff. 2006. Lexicon acqui-\nsition for dialectal arabic using transductive learn-\ning. In Proceedings of EMNLP, pages 399–407.\nAlexander Gammerman, V olodya V ovk, and Vladimir\nVapnik. 1998. Learning by transduction. In Pro-\nceedings of Uncertainty in artiﬁcial intelligence ,\npages 148–155.\nYaroslav Ganin and Victor Lempitsky. 2015. Unsu-\npervised domain adaptation by backpropagation. In\nProceedings of ICML, pages 1180–1189.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Franc ¸ois Lavi-\nolette, Mario Marchand, and Victor Lempitsky.\n2016. Domain-adversarial training of neural net-\nworks. The Journal of Machine Learning Research,\n17(1):2096–2030.\nDaniel Gildea and Daniel Jurafsky. 2002. Automatic\nlabeling of semantic roles. Computational linguis-\ntics, 28(3):245–288.\nJiang Guo, Darsh Shah, and Regina Barzilay. 2018.\nMulti-source domain adaptation with mixture of ex-\nperts. In Proceedings of EMNLP, pages 4694–4703.\nJan Haji ˇc, Massimiliano Ciaramita, Richard Johans-\nson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs\nM`arquez, Adam Meyers, Joakim Nivre, Sebastian\nPad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,\nNianwen Xue, and Yi Zhang. 2009. The conll-2009\nshared task: Syntactic and semantic dependencies\nin multiple languages. In Proceedings of CoNLL ,\npages 1–18.\nKazuma Hashimoto, Yoshimasa Tsuruoka, Richard\nSocher, et al. 2017. A joint many-task model: Grow-\ning a neural network for multiple nlp tasks. In Pro-\nceedings of EMNLP, pages 1923–1933.\nLuheng He, Kenton Lee, Omer Levy, and Luke Zettle-\nmoyer. 2018. Jointly predicting predicates and argu-\nments in neural semantic role labeling. In Proceed-\nings of ACL, pages 364–369.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of ACL, pages 328–339.\nGeorgiana Ifrim and Gerhard Weikum. 2006. Trans-\nductive learning for text classiﬁcation using ex-\nplicit knowledge models. In Proceedings of Euro-\npean Conference on Principles of Data Mining and\nKnowledge Discovery, pages 223–234.\nJing Jiang and ChengXiang Zhai. 2007. Instance\nweighting for domain adaptation in nlp. In Proceed-\nings of ACL, pages 264–271.\nThorsten Joachims. 1999. Transductive inference for\ntext classiﬁcation using support vector machines. In\nProceedings of ICML, pages 200–209.\n3671\nVidur Joshi, Matthew Peters, and Mark Hopkins. 2018.\nExtending a parser to distant domains using a few\ndozen partially annotated examples. In Proceedings\nof ACL, pages 1190–1199.\nYoung-Bum Kim, Karl Stratos, and Ruhi Sarikaya.\n2016. Frustratingly easy neural domain adaptation.\nIn Proceedings of COLING, pages 387–396.\nZuchao Li, Shexia He, Hai Zhano, Yiqing Zhang, Zhu-\nosheng Zhang, Zhou. Xi, and Xiang Zhou. 2019.\nDependency or span, end-to-end uniform semantic\nrole labeling. In Proceedings of AAAI.\nYanbin Liu, Juho Lee, Minseop Park, Saehoon Kim,\nand Yi Yang. 2019. Transductive propagation net-\nwork for few-shot learning. In Proceedings of ICLR.\nHiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.\n2018. A span selection model for semantic role\nlabeling. In Proceedings of EMNLP , pages 1630–\n1642.\nMartha Palmer, Daniel Gildea, and Paul Kingsbury.\n2005. The proposition bank: An annotated cor-\npus of semantic roles. Computational linguistics ,\n31(1):71–106.\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proceedings of ACL, pages 1756–1765.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of NAACL, pages 2227–\n2237.\nElias Ponvert, Jason Baldridge, and Katrin Erk. 2011.\nSimple unsupervised grammar induction from raw\ntext with cascaded ﬁnite state models. In Proceed-\nings of ACL, pages 1077–1086.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in ontonotes. In Proceedings of\nCoNLL, pages 1–40.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nLance A Ramshaw and Mitchell P Marcus. 1999. Text\nchunking using transformation-based learning. In\nNatural language processing using very large cor-\npora, pages 157–176. Springer.\nSebastian Ruder and Barbara Plank. 2018. Strong\nbaselines for neural semi-supervised learning under\ndomain shift. In Proceedings of ACL, pages 1044–\n1054.\nKuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada.\n2017. Asymmetric tri-training for unsupervised do-\nmain adaptation. In Proceedings of ICML , pages\n2988–2997.\nErik F Sang and Sabine Buchholz. 2000. Introduction\nto the conll-2000 shared task: Chunking.\nOzan Sener, Hyun Oh Song, Ashutosh Saxena, and Sil-\nvio Savarese. 2016. Learning transferrable represen-\ntations for unsupervised domain adaptation. In Pro-\nceedings of NIPS, pages 2110–2118.\nYuan Shi and Fei Sha. 2012. Information-theoretical\nlearning of discriminative clusters for unsupervised\ndomain adaptation. In Proceedings of ICML, pages\n1275–1282.\nAnders Søgaard. 2013. Semi-supervised learning and\ndomain adaptation in natural language processing.\nSynthesis Lectures on Human Language Technolo-\ngies, 6(2):1–103.\nMihai Surdeanu, Richard Johansson, Adam Meyers,\nLlu´ıs M`arquez, and Joakim Nivre. 2008. The conll\n2008 shared task on joint parsing of syntactic and\nsemantic dependencies. In Proceedings of CoNLL,\npages 159–177.\nJun Suzuki, Sho Takase, Hidetaka Kamigaito, Makoto\nMorishita, and Masaaki Nagata. 2018. An empirical\nstudy of building a strong baseline for constituency\nparsing. In Proceedings of ACL, pages 612–618.\nYuta Tsuboi, Hisashi Kashima, Shohei Hido, Stef-\nfen Bickel, and Masashi Sugiyama. 2009. Di-\nrect density ratio estimation for large-scale covariate\nshift adaptation. Journal of Information Processing,\n17:138–155.\nNicola Uefﬁng, Gholamreza Haffari, and Anoop\nSarkar. 2007. Transductive learning for statistical\nmachine translation. In Proceedings of ACL, pages\n25–32.\nVladimir Vapnik. 1998. Statistical learning theory .\nWiley.\nRui Wang, Masao Utiyama, Lemao Liu, Kehai Chen,\nand Eiichiro Sumita. 2017. Instance weighting for\nneural machine translation domain adaptation. In\nProceedings of EMNLP, pages 1482–1488.\nYufei Wang, Mark Johnson, Stephen Wan, Yifang Sun,\nand Wei Wang. 2019. How to best use syntax in se-\nmantic role labelling. In Proceedings of ACL, pages\n5338–5343.\nMin Xiao and Yuhong Guo. 2013. Domain adaptation\nfor sequence labeling tasks with a probabilistic lan-\nguage adaptation model. In Proceedings of ICML,\npages 293–301.\nYftah Ziser and Roi Reichart. 2017. Neural structural\ncorrespondence learning for domain adaptation. In\nProceedings of CoNLL, pages 400–410."
}