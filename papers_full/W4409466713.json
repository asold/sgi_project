{
  "title": "Classifying legal interpretations using large language models",
  "url": "https://openalex.org/W4409466713",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5117160136",
      "name": "Gaspar Dugac",
      "affiliations": [
        "Swiss Data Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2202227152",
      "name": "Tilmann Altwicker",
      "affiliations": [
        "Swiss Data Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A5117160136",
      "name": "Gaspar Dugac",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2202227152",
      "name": "Tilmann Altwicker",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4404656228",
    "https://openalex.org/W2983632977",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4393160302",
    "https://openalex.org/W4386504863",
    "https://openalex.org/W6600466347",
    "https://openalex.org/W2903938540",
    "https://openalex.org/W2963829982",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4250723764",
    "https://openalex.org/W3125665395",
    "https://openalex.org/W2735738703",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W4206288253",
    "https://openalex.org/W4389403907",
    "https://openalex.org/W4381802325",
    "https://openalex.org/W4252591097",
    "https://openalex.org/W4230031316",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W6756688054",
    "https://openalex.org/W4386507404",
    "https://openalex.org/W4327519588",
    "https://openalex.org/W6612613465",
    "https://openalex.org/W4386435222",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W2902017239",
    "https://openalex.org/W4381855801",
    "https://openalex.org/W2789335858",
    "https://openalex.org/W2982540118",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3168427350",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2952087461",
    "https://openalex.org/W4387846583",
    "https://openalex.org/W4375958664",
    "https://openalex.org/W6602945829",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W7035904466",
    "https://openalex.org/W6698039833",
    "https://openalex.org/W4400272953",
    "https://openalex.org/W4251563327",
    "https://openalex.org/W2811181315",
    "https://openalex.org/W6600577311",
    "https://openalex.org/W6602430550",
    "https://openalex.org/W4312766166",
    "https://openalex.org/W2963149412",
    "https://openalex.org/W6600696048",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W6602254124",
    "https://openalex.org/W2026095380"
  ],
  "abstract": "Abstract In the civil law tradition, legal arguments are used to justify the outcomes of judicial decision-making. These arguments are formed relying on a canon of interpretation techniques (e.g. textual or teleological interpretation). We study the identifiability of interpretation techniques as they are employed by the European Court of Human Rights (ECtHR) from a computational law perspective using a unique dataset. We show how Large Language Models (LLMs) can be utilized to classify legal interpretations, and we compare their performance. We evaluate proprietary and opensource models using methods such as few-shot and zero-shot chain-of-thought prompting combined with self-consistency. Our results imply that feature-extraction using LLMs leads to robust outcomes while allowing for greater resource- and timeefficiency compared to human annotation. Furthermore, our results imply that LLMs can play a larger role in the extraction of more complex features that are of particular relevance from a legal perspective.",
  "full_text": "Vol.:(0123456789)\nArtificial Intelligence and Law\nhttps://doi.org/10.1007/s10506-025-09447-9\nORIGINAL RESEARCH\nClassifying legal interpretations using large language \nmodels\nGaspar Dugac1  · Tilmann Altwicker2\nAccepted: 25 February 2025 \n© The Author(s) 2025\nAbstract\nIn the civil law tradition, legal arguments are used to justify the outcomes of judicial \ndecision-making. These arguments are formed relying on a canon of interpretation \ntechniques (e.g. textual or teleological interpretation). We study the identifiability \nof interpretation techniques as they are employed by the European Court of Human \nRights (ECtHR) from a computational law perspective using a unique dataset. We \nshow how Large Language Models (LLMs) can be utilized to classify legal interpre-\ntations, and we compare their performance. We evaluate proprietary and opensource \nmodels using methods such as few-shot and zero-shot chain-of-thought prompt-\ning combined with self-consistency. Our results imply that feature-extraction using \nLLMs leads to robust outcomes while allowing for greater resource- and timeeffi-\nciency compared to human annotation. Furthermore, our results imply that LLMs \ncan play a larger role in the extraction of more complex features that are of particu-\nlar relevance from a legal perspective.\nKeywords Legal interpretations · Computational Law · European Court of Human \nRights · Large Language Models\n1 Introduction\nLaw is fundamentally about making legal arguments. Particularly in the civil law \ntradition, legal arguments are commonly constructed by employing various interpre-\ntation techniques (e.g. textual interpretation) on the legal sources (e.g., a statutory \n * Gaspar Dugac \n gdugac@gmail.com\n Tilmann Altwicker \n tilmann.altwicker@ius.uzh.ch\n1 Center for Legal Data Science, University of Zurich, Pestalozzistrasse 24, 8032 Zurich, \nSwitzerland\n2 Faculty of Law & Center for Legal Data Science, University of Zurich, Pestalozzistrasse 24, \n8032 Zurich, Switzerland\n G. Dugac, T. Altwicker \nlegal rule) (Walton et  al 2018; Greenawalt 2012). Deriving arguments from legal \nsources through the application of interpretation techniques is an important part of \nlegal reasoning carried out by the courts in countries following the civil law tradi-\ntion. In fact, the use of interpretation techniques is a quality indicator of good legal \nreasoning by the courts (with a focus on the international judiciary (Von Bogdandy \nand Venzke 2014)). Furthermore, law students are trained both in the identification \nand construction of legal arguments employing the various techniques of interpreta-\ntion (Huhn 2000).\nDespite their importance for both education and practice, the use of interpretation \ntechniques is rarely studied from the perspective of computational law. This is cer -\ntainly due to the fact that it is hard to provide clear definitions even for the canonical \ntechniques of interpretation (textual, systematic, historical and teleological interpre-\ntation) (for a proposal see Walton et al (2018)). Given the ambiguity of the defini-\ntions of the individual interpretation techniques, classification tasks carried out by \nhumans remain somewhat subjective.\nThis paper focuses on the judicial interpretation of legal rules contained in the \nEuropean Convention on Human Rights (ECHR). Specifically, we look at the use of \nthe techniques of interpretation by the European Court of Human Rights (ECtHR). \nThe ECtHR is one of the most influential human rights courts worldwide. It cur -\nrently has 46 Member States, and its case-law is relevant for more than 700 m peo-\nple.1 The ECtHR is composed of judges from all 46 Member States; the judges have \ndifferent backgrounds (some of them are career judges with a long-standing experi-\nence in the practice of law, others are law professors), some are trained in the civil \nlaw, others in the common law tradition.\nResearch into trends regarding the ECtHR’s usage of certain interpretation tech-\nniques relies on having the appropriate data. To the best of our knowledge, data on \nthe use of interpretation techniques by the ECtHR is scarce. The main reason for the \ndata scarcity on techniques of legal interpretation is that the annotation of such data \nis not an easy task, and automating such a process is far from trivial. An appropriate \ncollection process would require parsing through lengthy judgments while meticu-\nlously analyzing the text for the presence of certain interpretative arguments. The \ntraining of classical machine learning methods to solve this issue is an option, albeit \na difficult one. In order to achieve sufficient accuracy, large amounts of training \ndata are necessary. If one had such data, one could employ standard classification \ntechniques.\nTo circumvent the lack of large sets of training data, we suggest the usage of \nLarge Language Models (LLMs) for the classification of legal interpretations. The \nrelease of the Generative Pre-trained Transformer 3 (GPT-3) (Brown et  al 2020) \nthrusted LLMs and the few-shot learning setting into prominence. Since then major \ndevelopments in the field have been made at an increasingly higher pace. Naveed \net  al (2023) show that the number of models released in the past years has been \nstrongly increasing, totaling to 29 for the year 2023. They also show that the vast \n1 https:// www. coe. int/ en/ web/ portal.\nClassifying legal interpretations using large language models  \nmajority of models released in 2023 were open-source models, a shift from previous \nyears.\nIn this paper we evaluate several proprietary and open-weights models for clas-\nsifying techniques of legal interpretation on text excerpts obtained from judgments \nby the ECtHR. We aim to classify three legal interpretation techniques - textual, \nsystematic, and teleological. In Sect.  2, we provide a brief overview of legal inter -\npretation techniques and their use by the ECtHR. In Sect. 3, we discuss related work. \nUsing custom tailored prompts, in Sect.  4.2 we use this data to perform zero-shot \nand few-shot learning on our selection of models. In Sect.  4.2.1, we use chain-of-\nthought prompting as introduced by Wei et al (2023) in combination with the zero-\nshot reasoning prompting strategies suggested in Kojima et al (2023). We draw com-\nparisons between this approach and the results obtained through standard zero- and \nfew-shot learning strategies. Furthermore, following Wang et al (2023), in Sect.  4.3 \nwe implement the notion of self-consistency which is reported to lead to improve-\nments in performance for chain-of-thought-prompting. In Sect. 5 we show that these \ntwo strategies both lead to improvements in performance in certain settings. Fur -\nthermore, we show these methods outperform a baseline BERT-based (Devlin et al \n2019) classifier, LEGAL-BERT (Chalkidis et al 2020), pretrained on ECHR cases. \nSection 5.2 investigates similarities in the models’ reasoning language compared to \nhuman-written reasoning. Our research is the first step towards an automated pro-\ncess for the collection of interpretations data from large-scale legal documents.\n2  Overview of legal interpretation techniques and their use \nby the ECtHR\nAccording to Art. 45(1) ECHR, the ECtHR must give reasons for its judgments and \ndecisions. In the process of formulating legal arguments, the ECtHR often makes \nuse of the canon of legal interpretation techniques. To be more precise, the ECtHR \nrefers to these techniques when establishing the meaning of a Convention norm (the \ngoal of legal interpretation being to \"assign meaning to texts and other statements \nfor the purposes of establishing rights, obligations, and other consequences relevant \nin a legal text\" (Herdegen 2013)). In order to identify the meaning of a legal text, \njudges employ a set of interpretation techniques. In international law, the canonical \ntechniques of legal interpretation are textual, systematic, historical and teleological \ninterpretation (Hollis 2019). These techniques are laid out in Articles 31-33 of the \nVienna Convention on the Law of Treaties.2 Following these rules, the ECtHR also \nregularly applies the canonical techniques in its case-law (Dothan 2018). In addi-\ntion, the ECtHR developed further techniques of interpretation (e.g. evolutionary \ninterpretation, margin of appreciation) (Dothan 2018). However, in practice, the \nECtHR relies most frequently on three of the canonical techniques of interpreta-\ntion: textual, systematic and teleological interpretation (Altwicker et al 2024). Given \n2 https:// legal. un. org/ ilc/ texts/ instr uments/ engli sh/ comme ntari es/1_ 1_ 1966. pdf.\n G. Dugac, T. Altwicker \nthat historical interpretation and the court-specific techniques are rarely used by the \nECtHR, our work focuses on the small subset of the three most pervasive techniques.\nTextual interpretation refers to understanding the ordinary meaning of a term. \nThis method utilizes the dictionary sense to establish the meaning (Slocum and \nWong 2021); from a linguistic perspective (Hutton 2020). In specific fields like \nscience, law, or technology, the subject-specific meanings are critical. Sometimes, \ntherefore, the literature distinguishes arguments from ordinary meaning and argu-\nments from technical meaning (Walton et  al 2018). Usually, textual interpretation \nrelies on the ordinary meaning at the time of conclusion of the treaty (Dörr 2018). \nIn the context of the European Convention on Human Rights (ECHR), textual inter -\npretation often serves as a starting point only and sometimes as a check on other \ntechniques of interpretation (see Polgári (2021)). An example for the use of the tex-\ntual interpretation technique from the case-law of the ECtHR is this: \"[A]s a matter \nof textual interpretation, the wording of Article 5 §  4, clearly suggests that the court \nmust have the power to order a release if it finds the detention unlawful\" (Case of \nBuishvili v. the Czech Republic, judgment of 25 October 2012, app. no. 30241/11, \n§  39).3\nSystematic interpretation looks at the context and relationships between differ -\nent terms or rules (Walton et al 2018); on the theoretical prerequisites see Padjen \n(2020). It involves considering the entire treaty, including preambles and annexes \n(Dörr 2018). This technique of interpretation is based on the hypothesis that legal \nrules are interconnected or part of a normative \"pattern\" (Ammann 2020); the use \nof this technique aims to make a body of legal rules consistent (Feteris and Feteris \n2017). The ECtHR has developed a rich case-law making use of systematic interpre-\ntation, often indicated by the phrase of interpreting a Convention right \"in the light \nof\" another Convention right. This is an example: \"The two sentences of Article 2 \nof Protocol No. 1 must be interpreted not only in the light of each other but also, in \nparticular, of Articles 8, 9 and 10 of the Convention...\" (Case of Folgerø v. Norway, \njudgment of 29 June 2007, app. no. 15472/02, § 84). Sometimes, the ECtHR uses \nsystematic interpretation by linking Convention rights to (higher-level) Convention \nprinciples (see, e.g., the principle of the \"rule of law\" in the Case of Al-Dulimi and \nMontana Management Inc. v. Switzerland, app. no. 5809/08, judgment of 21 June \n2016, § 126).\nTeleological interpretation focuses on the purpose and objectives that the law-\nmaker intended to achieve with a rule (Walton et  al 2018). The use of the teleo-\nlogical interpretation technique requires taking into account the goals and the legal \ninterests the legal rule is to realize or protect (Ammann 2020). Teleological inter -\npretation is commonly considered as a technique that allows the judges most flex-\nibility in the construction of legal meaning (for the ECtHR see Dothan (2014)). An \nexample from the case-law in which teleological interpretation was employed is this: \n\"Observing that religious communities traditionally exist in the form of organised \nstructures, the Court has repeatedly found that the autonomous existence of religious \ncommunities is indispensable for pluralism in a democratic society and is, thus, an \n3 https:// hudoc. echr. coe. int/.\nClassifying legal interpretations using large language models  \nissue at the very heart of the protection which Article 9 affords\" (Case of Löffel-\nmann v. Austria, judgment of 12 March 2009, app. no., 42967/98, §  47).\nClassifying interpretation techniques in ECtHR judgments presents several chal-\nlenges. First, as already mentioned, there are no clear, universal definitions on the \ninterpretation techniques, making the classification task subjective to some degree. \nSecond, interpretation methods often overlap (e.g. teleological and systematic inter -\npretation), making it difficult to distinguish between them in practice. Third, the \nECtHR’s judgments are often highly contextual, influenced by specific legal, cul-\ntural, and political circumstances, requiring an understanding of the broader con-\ntext. Fourth, judges frequently do not explicitly label the interpretative methods they \nemploy when forming legal arguments about the Convention rights, necessitating \ninference based on broader patterns of reasoning. These factors make classifying \ninterpretation techniques in ECtHR judgments a complex task for human annotators.\n3  Related work\nArtificial intelligence in the context of law and legal research has been much dis-\ncussed (Chalkidis and Kampas 2019). For example, Nguyen et al (2018) use recur -\nrent neural networks to identify requisite and effectuation parts in legal texts, \nChalkidis et al (2018) use hierarchical recurrent neural networks for detecting con-\ntractual obligations and prohibition from text, Merchant and Pande (2018) use latent \nsemantic analysis for legal text summarization, while Chalkidis et al (2020) develop \nLEGAL-BERT, an adaption of BERT (Devlin et al 2019) obtained by performing \npre-training on legal domain-specific data.\nHowever, empirical research on legal interpretation techniques is limited. Zurek \nand Araszkiewicz (2013) provide a mathematical model for teleological interpreta-\ntions in the context of the provision of statutory law. Habernal et al (2023) annotated \nlegal arguments, including interpretation techniques, in 373 ECtHR judgments. \nHowever, the three interpretations at the focus of this paper only make up a com-\nbined 0.43% of the annotated dataset (a total of 55 observations). Furthermore, the \nmajority of the models evaluated in their work fail to make a single correct predic-\ntion for any of the interpretation techniques. The only exception was their variation \nof RoBERTa (Liu et al 2019) pretrained on legal data which shows some predictive \npower for teleological interpretations. Their work is further proof of the difficulty of \ncollecting legal interpretations data, as well as an opportunity to evaluate methods \nrequiring less training data.\nThe advent of LLMs has had a profound impact on the field of law. Much \nresearch has been done in areas where the proponents of LLMs believe such models \ncan assist, or even replace, lawyers. Choi et al (2023) report ChatGPT successfully \npasses several law school exams, while Katz et al (2023) report GPT-4 achieves a \nwell-above passing grade on the entirety of the Uniform Bar Examination. Models \nsuch as GPT-3 showed capability in understanding consumer contracts, as well as \nin performing legal rule classification (Kolt 2022; Liga and Robaldo 2023). Savelka \n(2023) evaluates the models’ capabilities in performing zero-shot semantic annota-\ntion on text excerpts from a variety of legal documents.\n G. Dugac, T. Altwicker \nLLMs have shown their reasoning capabilities on a variety of tasks (Wei et  al \n2023). Nay et  al (2023) evaluate reasoning capabilities of several models when it \ncomes to applying tax law, while Trozze et al (2023) study their reasoning and legal \ndrafting capabilities on security cases involving cryptocurrencies.\nDespite their reasoning abilities, LLMs have been known to hallucinate (Ji \net  al 2023; Ye and Durrett 2022), that is produce seemingly convincing text con-\ntaining completely false information. The discussion of interpretability and secu-\nrity of LLMs is ongoing (Bender et al 2021; Shen et al 2023; Yao et al 2024; Saha \net al 2023) and will remain important going forward. To circumvent these issues, \nas well as boost performance, several prompting methods have been researched. \nSelf-consistency introduced by Wang et  al (2023) has shown to improve reason-\ning performance, while Kojima et al (2023) introduce specific prompting strategies \nto aid reasoning. More specifically to the legal domain, Jiang and Yang (2023) use \nlegal syllogism to construct chain-of-thought prompts, while Yu et al (2022) derive \nprompts from specific legal reasoning techniques such as IRAC (Issue, Rule, Appli-\ncation, Conclusion). Furthermore, there has been increased standardization in how \nthese models are evaluated on legal tasks through the development of benchmarks \nsuch as LegalBench (Guha et al 2023).\nWhile the usage of LLMs for text classification and annotation has been evalu-\nated on a variety of topics, to the best of our knowledge, their use in the classifica-\ntion of legal interpretations has not yet been reported. We deem this work as the first \nstep towards using these models for identifying legal interpretations in large-scale \nlegal documents. Our hope is this paper can further the use of such models in the \nfield of law for automated interpretative technique annotation.\n4  Analysis setting\n4.1  Data and models\nWe extract ECtHR judgments of the years from 2006 till 2011 from the HUDOC 4 \ndatabase focusing on the majority opinion only (i.e. leaving out separate opinions). \nThese judgments were further split into smaller text excerpts. The data used in this \npaper was randomly sampled from a larger dataset consisting of 1181 observations \nwhile ensuring a balanced sample of the 3 interpretations. The original dataset was \nlabeled by 3 law students trained specifically for this task with inter-annotator reli-\nability evaluated using Fleiss’ Kappa ( /u1D705= 0.93179 ) (Fleiss 1971). The labels of \nthe sampled data were then further verified by a law professor. In total, 130 texts \nwere labeled for evaluation purposes. The dataset contains 40 texts with a systematic \ninterpretation, 40 with a textual interpretation, and 40 with a teleological interpre-\ntation. In order to evaluate whether models can identify the lack of an interpreta-\ntive argument, an important feature in order to deploy these models for data col-\nlection on large-scale documents, we include 10 texts containing no interpretative \n4 https:// hudoc. echr. coe. int/.\nClassifying legal interpretations using large language models  \narguments. The average length of these texts was 333 characters with a standard \ndeviation of 130 characters.\nThe evaluated models cover a small range of the most popular LLMs cur -\nrently available. We consider 4 models in this paper which we deemed most rel-\nevant. These include 2 proprietary models, and 2 open-weights models. We include \nGPT3.5 Turbo − by OpenAI (Brown et al 2020), Llama-2 by Meta AI (Touvron \net al 2023), Gemini Pro by Google AI (Team et al 2023) and Mixtral by MistralAI \n(Jiang et al 2024). We follow the model querying procedure outlined in (Akter et al \n2023) using TogetherAI5 and LiteLLM.6\nGPT3.5 Turbo− is a proprietary model developed by OpenAI. The model does \nnot have publicly available information on the training data nor the parameter size. \nWe avoid evaluating GPT-4 and GPT-4 Turbo (OpenAI et al 2023) due to the cost-\nprohibitive nature of running large-scale evaluations on these models. The current \ncost, per 1 million tokens, for GPT-4 Turbo is tenfold the price of its contemporaries \nwhich makes its usage on any large legal documents unrealistic. Hence, we narrow \nour focus on GPT−3.5 Turbo. More specifically, we use the gpt3.5-turbo-preview −\nmodel. The model was accessed through the OpenAI API.\nLlama-2 is a family of open-weights LLMs developed by Meta AI. In this paper \nwe narrow our focus on the 70 billion parameters Llama-2 model, the largest one \navailable, henceforth denoted as Llama-2(70b). In addition to being open-weights, \nthe relative small number of parameters of the Llama-2 models compared to its con-\ntemporaries has enabled the deployment of local instances of the models. This has \nlead to a further democratization of the LLM space (Naveed et al 2023). The model \nwas accessed through the API provided by TogetherAI using LiteLLM.\nGemini Pro is a proprietary model developed by Google AI (Team et al 2023). \nSimilarly to the OpenAI models, the training data and the parameter size have not \nbeen disclosed. We access the model through the Google VertexAI API. Further -\nmore, we disable the safety features of the model due to the fact that some of the \nevaluation data contains material where the court had to adjudicate on matters \nwhich Gemini Pro deems sensitive. Google has further announced Gemini Pro 1.5 7 \nwith a context window of up to 1 million tokens. We look forward to evaluating this \nmodel as soon as possible since such a large context window is a big step toward our \ngoal of automated interpretations data collection from large-scale documents using \nLLMs.\nMixtral is an open-weights, sparse mixture-of-experts model which claims to \noutperform Llama-2(70b) on most benchmarks, as well as offer comparable perfor -\nmance to GPT−3.5 Turbo (Jiang et al 2024). To be more specific, we use the Mix-\ntral-8x7b-Instruct-v0.1 version of the model. Similarly to Llama-2(70b), we access \nthe model through the TogetherAI API using LiteLLM.\nFinally, we compare all these models to a baseline BERT-based model. We use \nLEGAL-BERT by Chalkidis et al (2020) as a baseline. More specifically, we use \n5 https:// www. toget her. ai/.\n6 https:// litel lm. ai/.\n7 https:// stora ge. googl eapis. com/ deepm ind- media/ gemini/ gemini_ v1_5_ report. pdf.\n G. Dugac, T. Altwicker \nthe ECHR-BERT-BASE model which is pretrained on ECHR cases. Since our data-\nset is relatively small, we do not perform further training of the model on interpreta-\ntions-specific data. Rather, we use the model as an out-of-the-box classifier for legal \ninterpretations. This corresponds to evaluating the zero-shot performance of LLMs \nand is therefore a fair baseline for comparisons.\n4.2  Prompting strategies\nWe try several prompting approaches to evaluate which lead to best performance \non our dataset. The basic evaluation methods consists of zero-shot and 3-shot learn-\ning. We extend the analysis by implementing chain-of-thought reasoning (Wei et al \n2023; Kojima et al 2023). This is discussed in Sect. 4.2.1. In all prompting cases we \nsupply a general system text consisting of the task description. The zero-shot setting \nincludes only a test case, while in the 3-shot setting we provide 1 example per inter -\npretation type. The general prompting outline can be seen in Fig. 1.\n4.2.1  Extension to reasoning\nWhile work such as Brown et al (2020) suggests LLMs are few-shot learners, Wei \net  al (2023) show that allowing models to reason by employing chain-of-thought \nprompting strategies leads to improved performance. Moreover, Kojima et al (2023) \nfind that such models are also more than capable zero-shot reasoners. They recom-\nmend adding the phrase \"Let’s think step by step.\" to prompt the model to thoroughly \nexplain its reasoning. This has been shown to improve results on a variety of bench-\nmarks. Blair-Stanek et al (2023) show this procedure can, in certain cases, lead to \nimprovements in GPT-3’s statutory reasoning performance on a simplified dataset of \nthe U.S. tax code. Nori et al (2023) report GPT-4 has impressive potential to answer \nthe multiple-choice questions of the United States Medical Licensing Examination \n(USMLE). They implement the zero-shot reasoning procedure outlined in Kojima \net al (2023). However, they report that basic chain-of-thought prompting does not \nlead to performance benefits. Liévin et al (2023) evaluate additional domain-specific \nFig. 1  Prompting template for Zero- and 3-shot learning\nClassifying legal interpretations using large language models  \nchain-of-thought prompts on the most popular medical benchmarks. They report \nsignificant variations in performance depending on the provided prompt.\nWe follow the approach outlined in Kojima et al (2023). We add the phrase \"Let’s \nthink step by step.\" to the prompts used in the 0-shot learning setting. Figure 2 shows \nthe specific prompt used.\nIn the 3-shot reasoning case, instead of using the approach outlined above, we \nprovide custom reasoning examples written using expert knowledge. These exam-\nples should in a sense guide the models on how to reason on the test cases they are \nprovided. An example prompt can be seen in Fig.  2. In Sect.  5.1 we compare these \nmodel-generated reasonings to human-written ones.\n4.2.2  A note on the definitions of the interpretations\nIn all cases we purposefully avoid providing the definitions of the interpretations \nto the models. As discussed, clear definitions of even the canonical techniques of \ninterpretation are hard to provide. To avoid inducing biases with our selection of the \ndefinitions, we rely on the models’ internal knowledge of the definitions. We inves-\ntigate the internal knowledge across models by doing a similarity analysis of the \nmodel-generated definitions.\nWe prompt each model to provide a succinct definition of the 3 interpretations. \nUsing a pretrained Sentence-BERT model (Reimers and Gurevych 2019), a modi-\nfication of BERT using siamese and triple network structures, we generate text \nembeddings of the model-generated definitions. More specifically, we use the all-\ndistilroberta-v1 model with a maximum sequence length set to 512 word pieces. We \nthen run a community detection algorithm that discriminates based on the cosine-\nsimilarity score of the embeddings, a widely used measure of similarity between \ntwo vectors (Reimers et al 2019; Yang et al 2018). We set the minimum cluster size \nFig. 2  Prompting template for Zero- and 3-shot reasoning\n G. Dugac, T. Altwicker \nto 1. The premise is that if the model-generated definitions are similar enough, the \ncommunity detection algorithm with an appropriate cosine-similarity threshold will \nseparate the definitions into 3 clusters, one for each definition.\nWe find that with a cosine-similarity threshold of 0.85, all definitions are cor -\nrectly clustered into 3 clusters with each cluster only containing the appropriate defi-\nnition. With a threshold of 0.90, all teleological and textual interpretation definitions \nare contained within the same clusters, while the cluster containing the definitions \nof a systematic interpretation is missing the definition generated by Mixtral (Fig. 3).\nThese results imply that the internal knowledge of legal interpretations is very \nsimilar across models. This provides further justification for omitting the defini-\ntions of the interpretations from the employed prompts and establishes that all \nmodels have, to a certain degree, the same base knowledge of legal interpretation \ntechniques.\n4.3  Self‑consistency\nYe and Durrett (2022) show that in certain cases chain-of-thought prompting might \ndegrade performance compared to standard prompting methods. One possible way \nof addressing such behaviour is self-consistency as proposed by Wang et al (2023). \nSelf-consistency is the process of sampling a diverse set of reasoning paths and then \ndetermining the final answer by finding the most consistent answer in the answer \nset. They find striking improvements in performance on a variety of benchmarks \nwhile also outperforming existing methods for improving language model genera-\ntion quality. Self-consistency has been used to improve performance of LLMs on \nmedical benchmarks (Singhal et al 2023), and has been further extended with Tree-\nof-Thoughts and Graph-of-Thoughts frameworks (Long 2023; Yao et al 2023; Besta \net al 2024).\nWe implement self-consistency in the zero-shot reasoning case combined with \nthe \"Let’s think step by step\" prompt. In the 3-shot case we follow a similar imple-\nmentation approach, however, we include custom reasoning examples. We run all \nFig. 3  Clusters after running community detection (Left: cosine-similarity threshold equal to 0.85, \nRight: cosine-similarity threshold equal to 0.90; Sys=Systematic, Tel=Teleological, Tex=Textual)\nClassifying legal interpretations using large language models  \nmodels with temperature set to 1 in order to generate more varied responses and \nto take advantage of self-consistency.\n5  Results\nIn this section we report the results from the analysis described in Sect.  4. All \nanalysis settings were run for a total of 10 times to obtain more confident perfor -\nmance metrics.\nTable  1 shows the increase in accuracy scores moving from zero-shot and \n3-shot learning to reasoning, as discussed in Sect.  4.2.1. In other words, we \nreport the change in accuracy when implementing chain-of-thought prompt-\ning compared to standard prompting methods. We observe noticeable improve-\nments in the majority of cases, with the most drastic increases found for 3-shot \nLLama2(70b) and zero-shot GPT −3.5 Turbo whose accuracy scores increased by \n37.50% and 39.47%, respectively. However, we also notice a drastic decrease in \nperformance for Gemini Pro in the 3-shot setting. In Sect.  5.1, we show that this \ndecrease in performance can be somewhat mitigated using self-consistency.\nWe notice rather large increases in 5 of the 8 analysis settings presented in \nTable 1, with decreasing performance in only 2 cases. This is in line with estab-\nlished research stating that chain-of-thought prompting leads to improvements in \nmost evaluation cases. As discussed in Sect.  4.1, we use LEGAL-BERT as a base-\nline comparison. We find that LEGAL-BERT achieves an accuracy of 0.32. This \nis lower than the accuracy achieved by any of the LLMs considered. We investi-\ngate the statistical significance of the difference in accuracy scores between our \nbaseline LEGAL-BERT model and each LLM. To this extent, we employ a z-test \nof two proportions for the null hypothesis H0 ∶ ̂aLB = ̂aLLM , where ̂aLB denotes the \naccuracy of the LEGAL-BERT model, and ̂aLLM denotes the accuracy of an LLM \nmodel, at a significance level /u1D6FC∈( 0, 1) . A rejection of H0 in this setting is equiva-\nlent to saying that, with (1 − /u1D6FC)% confidence, the LLM classifier is more accurate \nthan the LEGAL-BERT classifier.\nTable 1  Comparison of model accuracy between Zero/3-shot learning and reasoning (parentheses denote \nstatistically significant result at a significance level /u1D6FC= 0.05)\nZero-shot 3-shot\nModel Learning Reasoning %-increase Learning Reasoning %-increase\nGPT−3.5 Turbo 0.38 (0.53) 39.47 (0.48) (0.59) 22.92\nGeminis Pro (0.58) (0.58) 0.0 (0.52) 0.41 −21.15\nLlama2(70b) (0.48) (0.49) 2.08 0.40 (0.55) 37.50\nMixtral (0.56) (0.55) −1.79 (0.53) (0.58) 9.43\n G. Dugac, T. Altwicker \n5.1  Effects of self‑consistency\nHere we report the results when applying the method of self-consistency, described \nin Sect.  4.3. As discussed previously, chain-of-prompting can lead to degraded \nperformance in certain cases. We have shown this is the case for some models in \nTable 1.\nWe see in Table  2 that self-consistency leads to further improvements in per -\nformance in most cases, the exception being Llama2(70b) and 3-shot GPT −\n3.5 Turbo. More importantly, we notice self-consistency leads to an increase in \nperformance in models for which chain-of-thought prompting led to decreased \nperformance, such as Mixtral and Gemini Pro. As stated in the previous section, \nGemini Pro saw a massive decrease in performance with 3-shot chain-of-thought \nprompting. Implementing self-consistency in this situation led to a 14.63% \nincrease in accuracy. However, this was still not enough to surpass the 3-shot \nlearning performance. As another example, zero-shot chain-of-thought prompt-\ning led to a minor decrease in performance for Mixtral. However, as we note from \nthe results, self-consistency led to an 10.91% increase in performance, surpassing \nthe original zero-shot learning score. Once again, in terms of accuracy, all LLMs \noutperform the baseline LEGAL-BERT model.\nIn Tables  3 and 4  we report F1 scores for the zero-shot and 3-shot settings, \nrespectively. We report individual F1 scores for each label category in a one-vs-\nrest fashion, as well as Micro F1 and Macro F1 scores. We note from these tables \nthat, across all models, the predictive power for teleological interpretations is \nmost prominent.\nFurthermore, we note that, in the zero-shot setting, only GPT −3.5 Turbo and \nGemini Pro have any predictive power for the case when there is no interpretation \npresent in the text. Being able to identify a lack of an interpretation is an impor -\ntant aspect of deploying these models for automated legal interpretation data collec-\ntion, hence models with predictive power in this category are of great importance. \nFinally, in Table 5 we present the F1 scores for the LEGAL-BERT baseline model. \nWe see the scores are significantly lower than for the LLMs considered in Table  3 \nand Table 4. In particular, the baseline model has noticeable predictive power only \nfor teleological interpretations. This improved performance on teleological interpre-\ntations is in line with what we observe in LLMs.\nTable 2  Comparison of model accuracy using Zero-/3-shot reasoning with and without consistency \n(parentheses denote statistically significant result at a significance level /u1D6FC= 0.05)\nZero-shot Reasoning 3-shot Reasoning\nModel No Cons Cons %-increase No Cons Cons %-increase\nGPT−3.5 Turbo (0.53) (0.62) 16.98 (0.59) (0.55) −6.78\nGemini Pro (0.58) (0.65) 12.07 0.41 0.47 14.63\nLlama2(70b) (0.49) (0.47) −4.08 (0.55) (0.54) −1.82\nMixtral (0.55) (0.61) 10.91 (0.58) (0.62) 6.90\nClassifying legal interpretations using large language models  \nThe results show there is still work to be done in developing these models before \nthey can be confidently deployed for collecting legal interpretations data. The per -\nformance is quite sensitive depending on the model used and the prompt setting. \nHence, converging to the \"one best model\" is not quite possible. However, what \nwe do observe is possible ways of ensuring the convergence of performance on the \ngiven task of classifying legal interpretations. We show that on average, and across \nmodels, by using chain-of-thought prompting in combination with self-consistency, \nwe can expect improvements in performance. This allows for procedures where, for \nexample, one performs chain-of-thought prompting and self-consistency on multiple \nmodels and then performs majority voting between the model-generated results to \nobtain an answer closer to ground-truth.\n5.2  Analysis of reasoning language\nIn the 3-shot reasoning setting we provided the models with human-written reason-\ning examples. We wish to compare how close the models get to emulating human \nreasoning on interpretative arguments, as well as how the reasoning differs across \nconsistency runs. For this purpose we make use of the Bilingual Evaluation Under -\nstudy (BLEU) score (Papineni et  al 2002). This metric has been widely used as \nmeans of quantifying the quality of machine-translated text compared to human-\nwritten references (Thompson et al 2022; Yang et al 2023; Fan et al 2021; Bubeck \net al 2023). We use SacreBLEU as introduced by Post (2018) which implements the \nTable 3  F1 Scores: 0-shot (top down: learning, reasoning, self-consistency)\nInterpretation GPT−3.5 Turbo Llama2(70b) Gemini Pro Mixtral\nSystematic 0.22 0.45 0.45 0.74\nTeleological 0.52 0.64 0.71 0.94\nTextual 0.16 0.39 0.64 0.43\nNo interpretation 0.0 0.0 0.09 0.0\nMicro F1 0.40 0.53 0.62 0.74\nMacro F1 0.30 0.49 0.60 0.70\nSystematic 0.59 0.55 0.56 0.50\nTeleological 0.81 0.58 0.76 0.74\nTextual 0.30 0.35 0.63 0.45\nNo interpretation 0.07 0.0 0.08 0.0\nMicro F1 0.61 0.51 0.65 0.58\nMacro F1 0.56 0.49 0.65 0.56\nSystematic 0.64 0.54 0.60 0.59\nTeleological 0.84 0.62 0.82 0.75\nTextual 0.35 0.29 0.67 0.44\nNo interpretation 0.01 0.0 0.10 0.0\nMicro F1 0.65 0.51 0.71 0.61\nMacro F1 0.61 0.48 0.70 0.59\n G. Dugac, T. Altwicker \noriginal BLEU procedure and ensures comparability and reproducibility of BLEU \nscores by avoiding variations due to different tokenization procedures.\nIn Table  6 we present the BLEU scores for the 3-shot reasoning case. Further -\nmore, we investigate similarity in the case of self-consistency. The table includes \nBLEU scores computed on the entirety of the test set, as well as scores computed \non a subset of the data with only correctly labeled samples. We see that the abil-\nity to emulate human provided reasoning on legal interpretative techniques is poor \nacross all models, with Llama2(70b) and Mixtral performing convincingly poorer \nthan their counterparts. Gemini Pro, on the other hand, shows the highest capabili-\nties in emulating human-written reasoning.\nWe do note that the set of human-written reasonings was quite small which could \nhave led to smaller than hoped for BLEU scores since it constrained the set of poten-\ntial matches between machine-generated reasonings and the human-written ones.\nTable 4  F1 Scores: 3-shot (top down: learning, reasoning, self-consistency)\nInterpretation GPT−3.5 Turbo Llama2(70b) Gemini Pro Mixtral\nSystematic 0.13 0.39 0.41 0.37\nTeleological 0.70 0.53 0.79 0.86\nTextual 0.50 0.05 0.51 0.50\nNo interpretation 0.17 0.17 0.22 0.45\nMicro F1 0.50 0.41 0.58 0.60\nMacro F1 0.44 0.32 0.57 0.58\nSystematic 0.62 0.66 0.47 0.56\nTeleological 0.72 0.65 0.77 0.82\nTextual 0.46 0.23 0.39 0.50\nNo interpretation 0.27 0.33 0.20 0.21\nMicro F1 0.62 0.57 0.56 0.65\nMacro F1 0.60 0.51 0.54 0.63\nSystematic 0.60 0.59 0.47 0.59\nTeleological 0.72 0.69 0.81 0.85\nTextual 0.38 0.26 0.33 0.48\nNo interpretation 0.22 0.12 0.20 0.22\nMicro F1 0.60 0.57 0.56 0.66\nMacro F1 0.57 0.52 0.54 0.64\nTable 5  F1 Scores: LEGAL-\nBERT Interpretation LEGAL-BERT\nSystematic 0.05\nTeleological 0.48\nTextual 0.0\nNo interpretation 0.0\nClassifying legal interpretations using large language models  \n6  Conclusion\nThe inner functioning of courts is often viewed as opaque from the outside. In this \npaper, we shed some light on the identifiability of judicial interpretation techniques \nused by the ECtHR. Research on judicial interpretation of the law is research on the \ninner functioning of courts because it reveals how the court justifies the outcomes of \nits decision-making. The use of the techniques of interpretation is, therefore, related \nto the quality of decision-making by the courts. In the civil law tradition, courts \ncommonly employ a canon of techniques of interpretation (comprising, among oth-\ners, textual interpretation). The study of judicial interpretation is mostly carried out \nby lawyers and legal theorists using qualitative methods.\nIn this paper, we looked at the identifiability of the textual, systematic and tele-\nological interpretation techniques as they are employed by the ECtHR from the per -\nspective of computational law. Despite the relevance of legal interpretation, studies \non judicial interpretation from the perspective of computational law are still scarce. \nA reason is that the use of interpretation techniques is hard to quantify because \nof the lack of clear-cut definitions of the various techniques and some uncertainty \ninvolved in the labelling of individual phrases. In other words, features concerning \nthe techniques of legal interpretation are a complex matter to extract and to classify.\nWe show how the problem of classification of techniques of legal interpretation \ncan be approached using LLMs. Our experiments revealed, first, that the models \nused here performed best on the technique of teleological interpretation - i.e. focus-\ning on the purpose and objectives of a legal norm. Second, we demonstrate that \nLLMs outperform a LEGAL-BERT baseline model on this task. Third, our study is \na proof-of-concept on how to automate the classification of the techniques of inter -\npretation used by the ECtHR. This is a task that - if carried out by humans - is very \nresources- and time-intensive. Fourth, by concentrating on the techniques of legal \ninterpretation, we focus on a \"complex\" feature of case-law in contrast to features \nthat are easier to extract (e.g. the composition of the judicial panel, the date of the \njudgment). It is precisely the more complex features of cases that require attention \nTable 6  Comparison of BLEU scores for 3-shot reasoning (top down: entire test set, subset of only cor -\nrectly labeled samples)\nSystematic Teleological Textual\n Model No Cons Cons No Cons Cons No Cons Cons\nGPT−3.5 Turbo 26.93 27.59 52.68 51.60 20.96 20.21\nGemini Pro 20.19 20.92 54.83 55.97 14.10 13.59\nLlama2(70b) 12.28 13.84 17.24 20.01 10.32 10.33\nMixtral 15.45 16.17 18.96 18.95 11.84 11.67\nGPT−3.5 Turbo 35.83 34.22 57.53 55.61 35.07 32.19\nGemini Pro 52.26 51.70 75.34 73.51 43.64 43.57\nLlama2(70b) 13.84 15.30 17.99 21.82 24.56 21.49\nMixtral 22.07 22.78 19.39 19.30 15.56 15.65\n G. Dugac, T. Altwicker \nfrom computational law because they contain information crucial from a legal \nperspective.\nOur study has a few limitations. A first limitation, from a legal perspective, is \nthat our data only comprises the majority opinion of judgments from 2006-2011. \nThus, we cannot exploit data from separate opinions, from decisions or from a larger \ntime range. Secondly, from a computational perspective, LLMs have been referred \nto as somewhat of a black-box. As discussed, their interpretability is an ongoing dis-\ncussion. Furthermore, given their nature, the replicability of results also comes into \nquestion, especially in settings where obtaining varied responses is key to improving \nperformance. Thirdly, our evaluation was done on relatively short textual excerpts. \nThis is partly due to the difficulty in obtaining labeled data on legal interpretations, \nbut also due to the limits of present-day models. With the largest context window \nbeing at 128k tokens, employing these models on large-scale legal documents is \nnot feasible. However, with the announcement of models with much larger context \nwindows, we hope this hurdle will become surmountable. Finally, from a resource-\noriented perspective, we do not evaluate the latest GPT-4/GPT-4 Turbo models \nwhich reportedly surpass all existing models on a variety of tasks. As discussed, this \nlimitation comes from the sizeable gaps in pricing between those models and the \nones evaluated here. This gap in required resources severely affects the feasibility \nof deploying these models on legal documents at a large scale. Our hope is that with \nthe advancements in model development and increased computational availability, \nthis resource-requirement gap will narrow.\nWith the development of new models, future work should focus on deploying \nthem on larger legal documents. Furthermore, we show how to gainfully employ \nLLMs in the process of collecting information on complex legal variables. This is a \nway to make computational law more relevant to legal research in general. Another \naspect we investigated was the models’ ability in emulating human-written reason-\nings. However, as mentioned the set of human-written examples was quite small. \nFuture analyses could use a larger reference set of human-written reasonings in \norder to investigate if reasoning similarity improves. Finally, another point of view \ncould be taken by using LLMs for generating synthetic data on legal interpretations \nand then using such data as training samples in more classical machine learning \nmethods.\nFunding Open access funding provided by University of Zurich. Schweizerischer Nationalfonds zur \nFörderung der Wissenschaftlichen Forschung 198894 Prof. Dr. Mr. Tilmann Altwicker.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen \nses/ by/4. 0/.\nClassifying legal interpretations using large language models  \nReferences\nAkter SN, Yu Z, Muhamed A, et al (2023) An in-depth look at gemini’s language abilities. arXiv: 2312. \n11444\nAltwicker T, Geering F, Gerber D et al (2024) Winning arguments about rights: an empirical analysis of \nargument construction at the european court of human rights. Eur J Empir Legal Stud 1(2):207–230\nAmmann O (2020) The interpretative methods of international law: What are they, and why use them? In: \nDomestic Courts and the Interpretation of International Law. Brill Nijhoff, p 191–222\nBender EM, Gebru T, McMillan-Major A, et al (2021) On the dangers of stochastic parrots: Can lan-\nguage models be too big? In: Proceedings of the 2021 ACM Conference on Fairness, Accountabil-\nity, and Transparency. Association for Computing Machinery, New York, NY, USA, FAccT ’21, p \n610-623, https:// doi. org/ 10. 1145/ 34421 88. 34459 22\nBesta M, Blach N, Kubicek A, et al (2024) Graph of thoughts: Solving elaborate problems with large \nlanguage models. arXiv: 2308. 09687\nBlair-Stanek A, Holzenberger N, Van Durme B (2023) Can gpt-3 perform statutory reasoning? In: Pro-\nceedings of the Nineteenth International Conference on Artificial Intelligence and Law. Association \nfor Computing Machinery, New York, NY, USA, ICAIL ’23, p 22-31, https:// doi. org/ 10. 1145/ 35945 \n36. 35951 63\nBrown TB, Mann B, Ryder N, et al (2020) Language models are few-shot learners. arXiv: 2005. 14165\nBubeck S, Chandrasekaran V, Eldan R, et al (2023) Sparks of artificial general intelligence: Early experi-\nments with gpt-4. arXiv: 2303. 12712\nChalkidis I, Kampas D (2019) Deep learning in law: early adaptation and legal word embeddings trained \non large corpora. Artif Intell Law 27(2):171–198. https:// doi. org/ 10. 1007/ s10506- 018- 9238-9\nChalkidis I, Androutsopoulos I, Michos A (2018) Obligation and prohibition extraction using hierarchical \nRNNs. In: Gurevych I, Miyao Y (eds) Proceedings of the 56th Annual Meeting of the Association \nfor Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguis-\ntics, Melbourne, Australia, pp 254–259, https:// doi. org/ 10. 18653/ v1/ P18- 2041\nChalkidis I, Fergadiotis M, Malakasiotis P, et al (2020) LEGAL-BERT: The muppets straight out of law \nschool. In: Cohn T, He Y, Liu Y (eds) Findings of the Association for Computational Linguistics: \nEMNLP 2020. Association for Computational Linguistics, Online, pp 2898–2904, https:// doi. org/ \n10. 18653/ v1/ 2020. findi ngs- emnlp. 261\nChoi JH, Hickman KE, Monahan A et al (2023) Chatgpt goes to law school. J Leg Edu 71:387. https:// \ndoi. org/ 10. 2139/ ssrn. 43359 05\nDevlin J, Chang MW, Lee K, et al (2019) Bert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv: 1810. 04805\nDörr O (2018) Article 31: General rule of interpretation. In: Vienna Convention on the Law of Treaties: \nA commentary, Springer, pp 557–616\nDothan S (2014) In defence of expansive interpretation in the european court of human rights. Camb Int \nLaw J 3(2):508–531\nDothan S (2018) The three traditional approaches to treaty interpretation: a current application to the \neuropean court of human rights. Fordham Int’l LJ 42:765\nFan A, Bhosale S, Schwenk H et al (2021) Beyond english-centric multilingual machine translation. J \nMach Learn Res 22:1–48\nFeteris ET, Feteris ET (2017) Legal argumentation and legal interpretation. A Survey of Theories on the \nJustification of Judicial Decisions, Fundamentals of Legal Argumentation\nFleiss J (1971) Measuring nominal scale agreement among many raters. Psycholo Bull 76(5):378–382. \nhttps:// doi. org/ 10. 1037/ h0031 619\nGreenawalt K (2012) Statutory and common law interpretation. Oxford University Press\nGuha N, Nyarko J, Ho DE, et al (2023) Legalbench: A collaboratively built benchmark for measuring \nlegal reasoning in large language models. arXiv: 2308. 11462\nHabernal I, Faber D, Recchia N et al (2023) Mining legal arguments in court decisions. Artif Intell Law. \nhttps:// doi. org/ 10. 1007/ s10506- 023- 09361-y\nHerdegen M (2013) Interpretation in international law. Max Planck Encyclopedia of Public International \nLaw Disponible en: http://opil ouplaw com\nHollis DB (2019) Interpretation. In: Concepts for International Law. Edward Elgar Publishing, p 549–565\nHuhn WR (2000) Teaching legal analysis using a pluralistic model of law. Gonz L Rev 36:433\n G. Dugac, T. Altwicker \nHutton C (2020) Legal interpretation: The category of ordinary meaning and its role in legal interpreta-\ntion. In: The Routledge Handbook of Forensic Linguistics. Routledge, p 79–92\nJi Z, Lee N, Frieske R et al (2023) Survey of hallucination in natural language generation. ACM Comput \nSur 55(12):1–38. https:// doi. org/ 10. 1145/ 35717 30\nJiang AQ, Sablayrolles A, Roux A, et al (2024) Mixtral of experts. arXiv: 2401. 04088\nJiang C, Yang X (2023) Legal syllogism prompting: Teaching large language models for legal judgment \nprediction. arXiv: 2307. 08321\nKatz DM, Bommarito MJ, Gao S et al (2023) Gpt-4 passes the bar exam. SSRN Electron J. https:// doi. \norg/ 10. 2139/ ssrn. 43892 33\nKojima T, Gu SS, Reid M, et al (2023) Large language models are zero-shot reasoners. arXiv: 2205. 11916\nKolt N (2022) Predicting consumer contracts. Berkeley Technol Law J 37:71\nLiga D, Robaldo L (2023) Fine-tuning gpt-3 for legal rule classification. Comput Law Secur Rev \n51:105864. https:// doi. org/ 10. 1016/j. clsr. 2023. 105864\nLiu Y, Ott M, Goyal N, et al (2019) Roberta: A robustly optimized bert pretraining approach. arXiv: 1907. \n11692\nLiévin V, Hother CE, Motzfeldt AG, et al (2023) Can large language models reason about medical ques-\ntions? arXiv: 2207. 08143\nLong J (2023) Large language model guided tree-of-thought. arXiv: 2305. 08291\nMerchant K, Pande Y (2018) Nlp based latent semantic analysis for legal text summarization. In: 2018 \nInternational Conference on Advances in Computing, Communications and Informatics (ICACCI), \npp 1803–1807, https:// doi. org/ 10. 1109/ ICACCI. 2018. 85548 31\nNaveed H, Khan AU, Qiu S, et al (2023) A comprehensive overview of large language models. arXiv: \n2307. 06435\nNay JJ, Karamardian D, Lawsky SB, et al (2023) Large language models as tax attorneys: A case study in \nlegal capabilities emergence. arXiv: 2306. 07075\nNguyen TS, Nguyen LM, Tojo S et al (2018) Recurrent neural network-based models for recognizing req-\nuisite and effectuation parts in legal texts. Artif Intell Law 26(2):169–199. https:// doi. org/ 10. 1007/ \ns10506- 018- 9225-1\nNori H, King N, McKinney SM, et al (2023) Capabilities of gpt-4 on medical challenge problems. arXiv: \n2303. 13375\nOpenAI, :, Achiam J, et al (2023) Gpt-4 technical report. arXiv: 2303. 08774\nPadjen IL (2020) Systematic interpretation and the re-systematization of law: the problem, co-requisites, \na solution, use. Int J Semiot Law Revue Int de SéMiotique Juridique 33(1):189–213\nPapineni K, Roukos S, Ward T, et al (2002) Bleu: a method for automatic evaluation of machine transla-\ntion. In: Isabelle P, Charniak E, Lin D (eds) Proceedings of the 40th Annual Meeting of the Associa-\ntion for Computational Linguistics. Association for Computational Linguistics, Philadelphia, Penn-\nsylvania, USA, pp 311–318, https:// doi. org/ 10. 3115/ 10730 83. 10731 35\nPolgári E (2021) The role of the vienna rules in the interpretation of the echr: a normative basis or a \nsource of inspiration? Erasmus L Rev 14:82\nPost M (2018) A call for clarity in reporting BLEU scores. In: Bojar O, Chatterjee R, Federmann C, et al \n(eds) Proceedings of the Third Conference on Machine Translation: Research Papers. Association \nfor Computational Linguistics, Brussels, Belgium, pp 186–191, https:// doi. org/ 10. 18653/ v1/ W18- \n6319, https:// aclan tholo gy. org/ W18- 6319\nReimers N, Gurevych I (2019) Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv: \n1908. 10084\nReimers N, Schiller B, Beck T, et al (2019) Classification and clustering of arguments with contextual-\nized word embeddings. In: Korhonen A, Traum D, Màrquez L (eds) Proceedings of the 57th Annual \nMeeting of the Association for Computational Linguistics. Association for Computational Linguis-\ntics, Florence, Italy, pp 567–578, https:// doi. org/ 10. 18653/ v1/ P19- 1054, https:// aclan tholo gy. org/ \nP19- 1054\nSaha T, Ganguly D, Saha S, et al (2023) Workshop on large language models’ interpretability and trust-\nworthiness (llmit). In: Proceedings of the 32nd ACM International Conference on Information and \nKnowledge Management. Association for Computing Machinery, New York, NY, USA, CIKM ’23, \np 5290-5293, https:// doi. org/ 10. 1145/ 35837 80. 36153 11\nSavelka J (2023) Unlocking practical applications in legal domain: Evaluation of gpt for zero-shot seman-\ntic annotation of legal texts. In: Proceedings of the Nineteenth International Conference on Artificial \nIntelligence and Law. ACM, ICAIL 2023, https:// doi. org/ 10. 1145/ 35945 36. 35951 61\nClassifying legal interpretations using large language models  \nShen X, Chen Z, Backes M, et al (2023) In chatgpt we trust? measuring and characterizing the reliability \nof chatgpt. arXiv: 2304. 08979\nSinghal K, Azizi S, Tu T et  al (2023) Large language models encode clinical knowledge. Nature \n620(7972):172–180. https:// doi. org/ 10. 1038/ s41586- 023- 06291-2\nSlocum BG, Wong J (2021) The vienna convention and ordinary meaning in international law. Yale J Int’l \nL 46:191\nTeam G, Anil R, Borgeaud S, et al (2023) Gemini: A family of highly capable multimodal models. arXiv: \n2312. 11805\nThompson NC, Greenewald K, Lee K, et  al (2022) The computational limits of deep learning. arXiv: \n2007. 05558\nTouvron H, Martin L, Stone K, et al (2023) Llama 2: Open foundation and fine-tuned chat models. arXiv: \n2307. 09288\nTrozze A, Davies T, Kleinberg B (2023) Large language models in cryptocurrency securities cases: Can \nchatgpt replace lawyers? arXiv: 2308. 06032\nVon Bogdandy A, Venzke I (2014) In whose name?: a public law theory of international adjudication. \nOUP Oxford\nWalton D, Sartor G, Macagno F (2018) Statutory interpretation as argumentation. Handbook of legal \nreasoning and argumentation pp 519–560\nWang X, Wei J, Schuurmans D, et  al (2023) Self-consistency improves chain of thought reasoning in \nlanguage models. arXiv: 2203. 11171\nWei J, Wang X, Schuurmans D, et al (2023) Chain-of-thought prompting elicits reasoning in large lan-\nguage models. arXiv: 2201. 11903\nYang W, Du H, Liew ZQ et al (2023) Semantic communications for future internet: fundamentals, appli-\ncations, and challenges. IEEE Commun Sur Tutor 25(1):213–250. https:// doi. org/ 10. 1109/ COMST. \n2022. 32232 24\nYang Y, Yuan S, Cer D, et al (2018) Learning semantic textual similarity from conversations. In: Augen-\nstein I, Cao K, He H, et al (eds) Proceedings of the Third Workshop on Representation Learning for \nNLP. Association for Computational Linguistics, Melbourne, Australia, pp 164–174, https:// doi. org/ \n10. 18653/ v1/ W18- 3022, https:// aclan tholo gy. org/ W18- 3022\nYao S, Yu D, Zhao J, et al (2023) Tree of thoughts: Deliberate problem solving with large language mod-\nels. arXiv: 2305. 10601\nYao Y, Duan J, Xu K, et al (2024) A survey on large language model (llm) security and privacy: The \ngood, the bad, and the ugly. arXiv: 2312. 02003\nYe X, Durrett G (2022) The unreliability of explanations in few-shot prompting for textual reasoning. In: \nOh AH, Agarwal A, Belgrave D, et al (eds) Advances in Neural Information Processing Systems, \nhttps:// openr eview. net/ forum? id= Bct2f 8fRd8S\nYu F, Quartey L, Schilder F (2022) Legal prompting: Teaching a language model to think like a lawyer. \narXiv: 2212. 01326\nZurek T, Araszkiewicz M (2013) Modeling teleological interpretation. In: Proceedings of the Fourteenth \nInternational Conference on Artificial Intelligence and Law. Association for Computing Machinery, \nNew York, NY, USA, ICAIL ’13, p 160-168, https:// doi. org/ 10. 1145/ 25146 01. 25146 19,\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.",
  "topic": "Legal aspects of computing",
  "concepts": [
    {
      "name": "Legal aspects of computing",
      "score": 0.6242995858192444
    },
    {
      "name": "Philosophy of law",
      "score": 0.623125433921814
    },
    {
      "name": "Computer science",
      "score": 0.555568277835846
    },
    {
      "name": "Natural language processing",
      "score": 0.5439746975898743
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4212634861469269
    },
    {
      "name": "Linguistics",
      "score": 0.3989759683609009
    },
    {
      "name": "Political science",
      "score": 0.2660629153251648
    },
    {
      "name": "Law",
      "score": 0.2527045011520386
    },
    {
      "name": "The Internet",
      "score": 0.19850581884384155
    },
    {
      "name": "World Wide Web",
      "score": 0.19027003645896912
    },
    {
      "name": "Philosophy",
      "score": 0.16405293345451355
    },
    {
      "name": "Comparative law",
      "score": 0.11010879278182983
    }
  ],
  "institutions": [],
  "cited_by": 2
}