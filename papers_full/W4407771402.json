{
  "title": "ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities",
  "url": "https://openalex.org/W4407771402",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2742708661",
      "name": "Zheng Chanjin",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": null,
      "name": "Yu, Zengyi",
      "affiliations": [
        "Zhejiang University of Technology",
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2222816766",
      "name": "Jiang Yi-lin",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A263282237",
      "name": "Zhang Mingzi",
      "affiliations": [
        "Zhejiang Normal University",
        "East China Normal University"
      ]
    },
    {
      "id": null,
      "name": "Lu, Xunuo",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2077334778",
      "name": "Jin Jing",
      "affiliations": [
        "Zhejiang Normal University"
      ]
    },
    {
      "id": null,
      "name": "Gao, Liteng",
      "affiliations": [
        "University of Shanghai for Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2163640453",
    "https://openalex.org/W4382940191",
    "https://openalex.org/W4206404688",
    "https://openalex.org/W1967211659",
    "https://openalex.org/W3130295820",
    "https://openalex.org/W4221062694",
    "https://openalex.org/W4200094933",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W2618362327",
    "https://openalex.org/W4231268356",
    "https://openalex.org/W4205336583",
    "https://openalex.org/W1887059452",
    "https://openalex.org/W1545975663",
    "https://openalex.org/W2164279716",
    "https://openalex.org/W3201672049",
    "https://openalex.org/W2095165457",
    "https://openalex.org/W2727720076",
    "https://openalex.org/W2901466771",
    "https://openalex.org/W4393118307",
    "https://openalex.org/W4220747294",
    "https://openalex.org/W4402716381",
    "https://openalex.org/W2076598554",
    "https://openalex.org/W2293068655",
    "https://openalex.org/W3158418101",
    "https://openalex.org/W3206733204",
    "https://openalex.org/W4388210637",
    "https://openalex.org/W4206312222",
    "https://openalex.org/W4283167234",
    "https://openalex.org/W2140881785",
    "https://openalex.org/W2508682278",
    "https://openalex.org/W2110406013",
    "https://openalex.org/W1557592757",
    "https://openalex.org/W1968149505",
    "https://openalex.org/W2108839216",
    "https://openalex.org/W2280291775",
    "https://openalex.org/W4402670135",
    "https://openalex.org/W3164459664",
    "https://openalex.org/W4392780604",
    "https://openalex.org/W4365806536",
    "https://openalex.org/W2541711215"
  ],
  "abstract": "Can Multimodal Large Language Models (MLLMs), with capabilities in perception, recognition, understanding, and reasoning, function as independent assistants in art evaluation dialogues? Current MLLM evaluation methods, which rely on subjective human scoring or costly interviews, lack comprehensive coverage of various scenarios. This paper proposes a process-oriented Human-Computer Interaction (HCI) space design to facilitate more accurate MLLM assessment and development. This approach aids teachers in efficient art evaluation while also recording interactions for MLLM capability assessment. We introduce ArtMentor, a comprehensive space that integrates a dataset and three systems to optimize MLLM evaluation. The dataset consists of 380 sessions conducted by five art teachers across nine critical dimensions. The modular system includes agents for entity recognition, review generation, and suggestion generation, enabling iterative upgrades. Machine learning and natural language processing techniques ensure the reliability of evaluations. The results confirm GPT-4o's effectiveness in assisting teachers in art evaluation dialogues. Our contributions are available at https://artmentor.github.io/.",
  "full_text": "ArtMentor: AI-Assisted Evaluation of Artworks to Explore\nMultimodal Large Language Models Capabilities\nChanjin Zheng\nchjzheng@dep.ecnu.edu.cn\nShanghai Institute of Artificial\nIntelligence for Education, East China\nNormal University\nShanghai, China\nFaculty of Education, East China\nNormal University\nShanghai, China\nZengyi Yu\nFaculty of Education, East China\nNormal University\nShanghai, China\n202105720431@zjut.edu.cn\nCollege of Education, Zhejiang\nUniversity of Technology\nHangzhou, Zhejiang, China\nYilin Jiang\nzjut_jiangyilin@163.com\nCollege of Education, Zhejiang\nUniversity of Technology\nHangzhou, Zhejiang, China\nMingzi Zhang\nFaculty of Education, East China\nNormal University\nShanghai, China\nwindyday@zjnu.edu.cn\nCollege of Education, Zhejiang\nNormal University\nJinhua, Zhejiang, China\nXunuo Lu\n13968860822@163.com\nSchool of Economy, Zhejiang\nUniversity of Technology\nHangzhou, Zhejiang, China\nJing Jin\nSchool of Education, Zhejiang Normal\nUniversity\nJinhua, Zhejiang, China\n383230730@qq.com\nTianchang Guanchao Primary School\nHangzhou, Zhejiang, China\nLiteng Gao\n2335060610@st.usst.edu.cn\nSchool of Artificial Intelligence\nScience and Technology, University of\nShanghai for Science and Technology\nShanghai, China\nAbstract\nCan Multimodal Large Language Models (MLLMs), with capabili-\nties in perception, recognition, understanding, and reasoning, act\nas independent assistants in art evaluation dialogues? Current\nMLLM evaluation methods, reliant on subjective human scoring\nor costly interviews, lack comprehensive scenario coverage. This\npaper proposes a process-oriented Human-Computer Interaction\n(HCI) space design for more accurate MLLM assessment and de-\nvelopment. This approach aids teachers in efficient art evaluation\nand records interactions for MLLM capability assessment. We in-\ntroduce ArtMentor, a comprehensive space integrating a dataset\nand three systems for optimized MLLM evaluation. It includes 380\nsessions from five art teachers across nine critical dimensions. The\nâˆ—These authors contributed equally: Chanjin Zheng, Zengyi Yu, Yilin Jiang.\nâ€ Corresponding author: Chanjin Zheng (Email at chjzheng@dep.ecnu.edu.cn).\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan\nÂ© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-1394-1/25/04\nhttps://doi.org/10.1145/3706598.3713274\nmodular system features entity recognition, review generation, and\nsuggestion generation agents, enabling iterative upgrades. Machine\nlearning and natural language processing ensure reliable evalu-\nations. Results confirm GPT-4oâ€™s effectiveness in assisting teach-\ners in art evaluation dialogues. Our contributions are available at\nhttps://artmentor.github.io/.\nCCS Concepts\nâ€¢ Human-centered computing â†’Laboratory experiments ;\nHuman computer interaction (HCI) ; HCI design and evaluation meth-\nods.\nKeywords\nAI-Assisted Artwork Evaluation, GPT-4o, Multimodal Large Lan-\nguage Models, Human-Computer Interaction Dataset Design, Entity\nRecognition, Multi-Agent for Iterative Upgrades.\nACM Reference Format:\nChanjin Zheng, Zengyi Yu, Yilin Jiang, Mingzi Zhang, Xunuo Lu, Jing Jin,\nand Liteng Gao. 2025. ArtMentor: AI-Assisted Evaluation of Artworks to\nExplore Multimodal Large Language Models Capabilities. In CHI Conference\non Human Factors in Computing Systems (CHI â€™25), April 26-May 1, 2025,\nYokohama, Japan. ACM, New York, NY, USA, 18 pages. https://doi.org/10.\n1145/3706598.3713274\narXiv:2502.13832v1  [cs.HC]  19 Feb 2025\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nFigure 1: A multi-agent data collection system from ArtMentor specifically designed to assess the GPT-4oâ€™s assistance\ncapabilities in art evaluation. It captures interactions across 380 evaluation sessions involving five art teachers and three agents\nof GPT-4o.\n1 INTRODUCTION\nMultimodal large language models (MLLMs), by seamlessly integrat-\ning various data types such as text and images, possess capabilities\nin multimodal perception, recognition, understanding, and reason-\ning [24]. Recent MLLMs, including GPT-4o [1], Gemini [51, 59], and\nClaude 3 [8], excel in tasks like image recognition, visual question\nanswering, cross-modal retrieval, and video understanding [62, 65].\nThese strengths in visual analysis and language generation offer sig-\nnificant potential for advancing art education assessment [54, 69].\nIn the realm of art education, particularly artwork evaluation, the\nintegration of the assessment process with broader dialogue-based\neducation is pivotal [ 25, 30]. This approach enhances studentsâ€™\nunderstanding and mitigates concerns of dehumanization. At the\nintersection of arts, education, and AI, a key question arises: Can\nMLLMs function as independent entities in collaborative evaluation\nprocesses, effectively supporting teachers and enriching educational\nexperiences? In artwork evaluation, MLLMs can identify visual\nelements (\"entities\") such as trees, faces, and art styles [14]. Based\non entity recognition results, MLLMs can generate artwork reviews\nand suggestions, functioning like an \"ArtMentor\". This raises a cru-\ncial question: How effectively can MLLMs assist elementary\nart teachers with entity recognition, review generation, and\nsuggestion generation?\nTo assist elementary art teachers effectively, it is crucial to un-\nderstand MLLMsâ€™ capabilities and limitations within specific HCI\ncontexts [11, 23]. This includes both their general performance and\ntheir behavior in educational settings [ 50]. MLLMs can identify\nentities in artworks, link them to historical art movements, and\nanalyze color schemes or compositions [6, 29]. However, their effec-\ntiveness relies heavily on the HCI methods used [45]. In specialized\nHCI art environments like elementary art classrooms, the depth\nand accuracy with which MLLMs comprehend artistic elements are\nvital.\nTo better tailor MLLMs for educational purposes, we have stream-\nlined the assistance process into three distinct phases. Each session\nstarts with an uploaded artwork, followed by the MLLMâ€™s automatic\nrecognition of entities within the artwork. Incorporating \"Thinking\nAloud\" [21] and \"Protocol Analysis\" [22], our system also encour-\nages students to create audio recordings that elucidate their creative\nideas and critically analyze their own work for art teachers. Art\nteachers refine the entities, including art style, until accurate. After\nentity recognition, MLLMs review the artworks and assign scores.\nHCI methods verify the appropriateness of these scores, after which\nMLLMs provide suggestions for modifying the artwork. Each stage\nof this process is managed by a designated agentâ€”namely, theentity\nrecognition agent , review generation agent , and suggestion genera-\ntion agent. This modular approach supports iterative updates and\ntargeted enhancements, ensuring adaptability and effectiveness in\neducational applications. This multi-agent framework raises key\nquestions: Can the entity recognition agent accurately identify the\nthemes in student artwork? How effective are thereview generation\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\nagent and suggestion generation agent in enhancing the artistic pro-\ncess? Systematically addressing these questions will offer insights\nfor refining our design. Without clear answers, assumptions about\nMLLMsâ€™ effectiveness in assisting teachers remain speculative.\nTo address these challenges, our study draws inspiration from\nMina Leeâ€™s works [ 34, 35], particularly her focus on evaluating\nthe writing capabilities of LLMs (GPT-3) through HCI datasets\nat CHI 2022, and her subsequent proposal of a design space for\nsystematically exploring intelligent interactive writing assistants at\nCHI 2024. Building on this foundation, we adopt an MLLM-driven\napproach that collects process-oriented data during interactions\nbetween art teachers and MLLMs, expanding the scope to the mul-\ntimodal domain and breaking down artistic evaluation capabilities\ninto multiple sub-abilities.\nThis dynamic and ongoing data type authentically reflects the\nMLLMâ€™s capabilities by capturing the evolving interaction, thus\navoiding the biases of result-oriented evaluations [28]. In HCI re-\nsearch, such process-oriented data is challenging to fabricate, of-\nfering a more reliable assessment of the modelâ€™s performance in\neducational contexts. However, due to factors like context vari-\nability and decoding parameters [37, 68], testing these capabilities\nposes significant challenges in HCI settings. For example, after\nmultiple interactions between an elementary school art teacher\nand MLLMs, repeatedly modifying and refining reviews and sug-\ngestions on student artworks, how can we accurately identify and\ndescribe the modelâ€™s specific contribution to the final reviews and\nsuggestions? How can we quantify the extent to which the MLLMs\nmeet the art teacherâ€™s assistance needs?\nThe design and analysis of HCI spaces , encompassing datasets,\nmulti-agent data collection systems and so on, are essential for\naddressing the evaluation challenges faced by MLLMs in educa-\ntional assistance contexts. To gather raw data, we utilized GPT-4o,\na representative MLLM, and developed the ArtMentor, as shown\nin Figure. 1. Specifically, the ArtMentor space allows for an in-\ndepth analysis of GPT-4oâ€™s ability to assist teachers across nine\nkey dimensions: realism, deformation, imagination, color richness,\ncolor contrast, line combination, line texture, picture organization,\nand transformation. Consequently, we organize both the review\ngeneration and suggestion generation agents into nine distinct\ndimensions\nWe demonstrate that the metrics designed to integrate\nHCI datasets with machine learning and natural language\nprocessing effectively quantify the assistance capabilities of\nMLLMs. Specifcally, we adapt standard machine learning metrics\nsuch as accuracy, precision, recall, and F1-score to evaluate entity\nrecognition capabilities, detailed in section 4.2.1. Additionally, we\nintroduce score acceptance Models outlined in section 4.2.3. Draw-\ning from natural language processing, we assess the generation\nof reviews and suggestions using two criteria: text modification\nlength and text similarity, as discussed in section 4.2.4. Lastly, we\nevaluate art style sensitivity to gauge the acceptance of art styles,\nwhich is elaborated in section 4.2.2.\nThis paper makes three contributions: (1) We introduce a multi-\nagent space named ArtMentor, which effectively collects process-\noriented HCI datasets to mitigate the fabrication issues often found\nin result-oriented data. The dataset and code for this space are freely\naccessible at https://artmentor.github.io/; (2) We develop compre-\nhensive evaluation metrics by integrating insights from machine\nlearning, natural language processing, and HCI to holistically assess\nthe assistance capabilities of MLLMs; (3) Through extensive data\ncollection and analysis, We identified underperforming dimensions\nin the multi-agent system and proposed iterative enhancements\nto improve overall performance. These contributions pave the way\nfor a more refined, process-oriented, and versatile approach to the\nevaluation of MLLMsâ€™ capabilities.\n2 RELATED WORK\n2.1 Capability Evaluation of MLLMs\n2.1.1 Types of Capability. The exploration of the capabilities and\nlimitations of MLLMs is crucial for designing effective multimodal\ninteractions. Directly aligning with how MLLMs assist in entity\nrecognition, review generation (including score), and suggestion gen-\neration for elementary art teachers, this foundational exploration is\ndivided into four key sections [24]:\nâ€¢Multimodal Perception: Examines MLLMsâ€™ understanding of\nspatial and relational dynamics within data from different modal-\nities. This includes: (1) Object localization, which involves de-\ntermining the position and orientation of objects within scenes,\ncrucial for spatial awareness [13, 63]; (2) Object relation, identify-\ning spatial and contextual relationships between objects [4, 41];\n(3) Object interaction, recognizing interactions that involve ac-\ntions, movements, or functional relationships within a visual\ncontext [12, 70]. This multifaceted approach corresponds to our\nsystemâ€™s capability to facilitate holistic assessment (art style recog-\nnition), capturing the intricate interplay of various artistic elements.\nâ€¢Multimodal Recognition: Focuses on the identification and\nclassification of entities, actions, and attributes across different\nmodalities, which includes: (1) Concept recognition, assessing\nmodelsâ€™ ability to categorize objects, actions, and scenes from\nvaried sensory inputs [36, 41, 63]; (2) Attribute recognition, eval-\nuating the detection of styles, emotions, and quantities across\ndifferent modalities [3, 41]; (3) Action recognition, interpreting\nactions within various contexts [ 18, 41]; (4) Text recognition,\ndetermining the ability to transcribe text from images, vital for\nprocesses like automated documentation [1, 41]. This aligns with\nour entity recognition process in the ArtMentor space.\nâ€¢Multi-modal Understanding: This section evaluates MLLMs\non their capability to process and make sense of data from mul-\ntiple sensory inputs, extending beyond textual information, to\nprovide a comprehensive understanding of multimodal data inte-\ngration [24]. This principle aligns with our systemâ€™s capability to\ngenerate in-depth reviews and nuanced scores for individual dimen-\nsions of art evaluation, demonstrating a profound understanding\nof specific artistic aspects.\nâ€¢Multimodal Reasoning: Investigates how MLLMs infer logi-\ncal conclusions from multimodal data. This section covers: (1)\nCommonsense reasoning, which evaluates modelsâ€™ ability to ap-\nply knowledge to interpret interactions and relationships within\nimages [39, 64]; (2) Relation reasoning, testing understanding of\nsocial, physical, or natural relations among various elements [42,\n51]; (3) Logic reasoning, assessing the application of logical princi-\nples in analyzing and interpreting multimodal information [5, 41].\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nThis is closely related to how we generate suggestions in the Art-\nMentor space based on the assessed dimensions.\n2.1.2 Methods to Evaluate. The evaluation of MLLMs encompasses\nseveral methodologies that ensure a comprehensive evaluation of\ntheir capabilities. These methods are divided into three primary\ncategories [24]:\nâ€¢Human Evaluation: Human evaluators play a crucial role in as-\nsessing the capabilities of MLLMs, especially in tasks that demand\nhigh comprehension levels and are challenging to quantify using\nstandard metrics. The evaluation focuses on multiple dimensions\nincluding: (1) Relevance, assessing whether the responses align\nwith the intended instructions [40]; (2) Coherence, determining if\nthe responses are logically structured and consistent; (3) Fluency,\nevaluating the naturalness and grammatical correctness of the\ngenerated outputs.\nâ€¢GPT-4 Evaluation: To complement human evaluation and ad-\ndress its resource-intensive nature, the instruction-following ca-\npabilities of GPT-4 are used to efficiently evaluate model-generated\noutputs. GPT-4 assesses the MLLMs on dimensions such as help-\nfulness, relevance, accuracy, and detail, scoring them on a scale\nfrom 1 to 10, where higher scores indicate better performance.\nThis approach not only provides scores but also detailed expla-\nnations for the evaluations, offering insights into the modelâ€™s\nstrengths and areas for improvement [1, 40].\nâ€¢Metric Evaluation: While qualitative insights from human and\nGPT-4 evaluations are valuable, traditional metrics are essential\nfor quantitatively assessing MLLM performance. These metrics\nprovide standardized and objective measurements across vari-\nous tasks: (1) For recognition capabilities, metrics like Accuracy\nand Average Precision are utilized [ 33, 36, 38]; (2) For percep-\ntion capabilities, measures such as mIoU, mAP, and Dice are\nadopted [17]; (3) For evaluating text or image generation capa-\nbilities, metrics such as BLEU, ROUGE, and METEOR are widely\nemployed [15, 31], providing clear indicators of a modelâ€™s perfor-\nmance in various applications.\nWhile human evaluation offers insightful perspectives, it is in-\nherently subjective and costly, with GPT-4 assessments potentially\nvarying due to fluctuations in prompts and parameters. Further-\nmore, Mina Lee and colleagues have deliberated on two methodolo-\ngies for investigating the generative capacities of large language\nmodels (LLMs): contextual inquiry and interaction logging anal-\nysis. Contextual inquiry, through interviews, provides profound\ninsights albeit with limited generalizability [ 35]; interaction log-\nging analysis, though broad in scope, lacks depth. Previous research\nhas largely been confined to specific tasks and settings. We aim to\nsynthesize these approaches, customizing them for multi-modal\ntasks using MLLMs, thereby addressing the limitations of inter-\nviews and validating the model across a wider spectrum, offering a\nmore comprehensive evaluation and insights for future research.\nConsequently, we integrate traditional machine learning metrics with\nour innovative natural language processing techniques to deliver a\nnuanced, robust, and reliable assessment of MLLMs.\n2.2 Process-oriented HCI Datasets in Education\n2.2.1 The Growing Focus on Educational Process Mining in HCI. In\nthe educational domain, the process of learning is often considered\nmore critical and analytically valuable than the final outcomes [61].\nCapturing this process, however, poses significant challenges due\nto the complexity of documenting and analyzing process-oriented\ndata, which tends to be sparse and less frequently analyzed. Com-\nmon applications of process mining techniques have been demon-\nstrated in online assessment data to analyze the processes involved\nin answering questions or requesting student feedback [48]. Addi-\ntionally, frameworks integrating educational process data mining\nhave been introduced to facilitate the handling of interactive pro-\ncess data and assist educators in analyzing educational processes\nbased on formal modeling [60]. Despite these advancements, the\ncollection and analysis of such data remain labor-intensive and\ninherently complex. The complexity of process-oriented data in\neducational settings correlates strongly with concepts such as sim-\nplicity, ease of use, uncertainty, and the context of application,\nmaking it a focal point for HCI designers [61]. These challenges un-\nderscore the need for innovative approaches in HCI to enhance the\nusability and effectiveness of process mining tools in educational\nenvironments. Consequently, the significance and ongoing challenges\nof Educational Process Mining in HCI have garnered increasing at-\ntention, highlighting the urgency for developing more efficient and\naccessible tools.\n2.2.2 Emerging Trends in Process-Oriented Artwork Evaluation. In\nthe field of artwork evaluation, process-oriented approaches are be-\nginning to take shape. Currently, there are very few widely validated\nmethods for automated process-oriented visual arts assessment. In\nfact, result-oriented methods are also scarce; one of the few ex-\namples is the Torrance Tests of Creative Thinkingâ€”Drawing Task,\nwhich scores creativity using artificial neural networks [16]. One\nemerging process-oriented method involves providing an initial\nartwork that allows students to further develop the piece by adding\npatterns. The evaluation is then conducted based on both the initial\nand final artworks [ 47]. This method attempts to document the\ncreative process of studentsâ€™ artwork creation as much as possi-\nble, but recording the entire creative process remains a significant\nchallenge. This raises an important question: Can the process of\nartwork evaluation under AI assistance be effectively documented?\nThrough the design of ArtMentor, we aim to facilitate interaction\nbetween educators and the system, deepening their understanding\nof studentsâ€™ artworks, thereby advancing the development of HCI\nin the domain of art education.\n2.3 Spaces for Iterative Multi-Agent Upgrades\n2.3.1 Fractionalization and Dominance. The CoAuthor study has\nsignificantly contributed to the HCI community by highlighting\nthe generative capabilities of large language models (LLMs) in cre-\native and argumentative writing contexts [ 35]. While CoAuthor\neffectively advocates for the curation and analysis of large inter-\naction datasets to make these capabilities more transparent and\naccessible, it does not segment the creative process into distinct\nphases that could provide deeper, context-specific insights. Building\non these viewpoints, we propose that applying a similar phased\napproach to the art evaluation processâ€”identifying distinct stages\nlike conception, development, and presentationâ€”could refine our\nanalyses even further. This approach, known as fractionalization,\ninvolves dividing a complex process into manageable segments,\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\nwith each segment handled by a dedicated agent. Dominance in\nthis context refers to the strategic control and optimization of each\nsegment by its respective agent, ensuring that the overall system\nmaintains coherence and maximizes efficiency. By implementing a\nsystem based on these principles, not only is the granularity of the\nobtained insights improved, but the overall effectiveness and adapt-\nability of the system within dynamic HCI environments are also\nenhanced. Similarly, when applied to the Evaluation of Artworks ,\nthis structured approach allows specialized agents to precisely man-\nage different stages of artistic creation and interpretation, thereby\nenhancing the precision and depth of art evaluations. Consequently,\nour ArtMentor is structured around multi-agent concepts based\non fractionalization and dominance principles.\n2.3.2 Living Artifact and Iterative Upgrades. Following the CoAu-\nthor research, Lee proposed a dynamic and adaptive framework\naimed at continuously enhancing technologies for writing assis-\ntance [34]. This design space, developed through collaborations\nwith experts from disciplines including HCI, Natural Language Pro-\ncessing, Information Systems, and Education, encompasses five key\ndimensionsâ€”task, user, technology, interaction, and ecosystem â€”and\ninvolved a comprehensive analysis of 115 papers to map the land-\nscape of writing assistants. The framework is designed as a living\nartifact, intended to evolve through community contributions of\nnew research, annotations, and discussions, keeping pace with ad-\nvancements in the field. However, while this framework is insight-\nful, it still relies on a traditional data collection system. Inspired by\nTalebiradâ€™s approach to enhancing LLMs through multi-agent sys-\ntems [58], our ArtMentor employs a similar structure to address\nthese limitations. By adopting a multi-agent architecture, we facili-\ntate iterative upgrades, enabling both the dataset and the supporting\nsystem to remain dynamic and responsive to emerging needs and\ndevelopments. This strategy not only enhances the adaptability of\nour system but also improves its capability to handle complex tasks\nefficiently, reflecting the collaborative environment and knowledge\nexchange among intelligent agents envisioned by Talebirad.\n3 DESIGN PRINCIPLES FOR ArtMentor\nIn this section, we discuss the design of four main components of\nArtMentor: a multi-agent data collection system, an HCI dataset, a\ndata analysis system , and an iterative upgrades system . The design\nof ArtMentor adheres to the principles of adjusting evaluation\ngranularity, providing immediate feedback, and progressively ap-\nproaching target capabilities [2]. To design appropriate evaluation\ngranularity, we draw inspiration from Henri Bergsonâ€™s exploration\nof consciousness [7], shifting the focus from evaluating the phys-\nical aspects of art pieces to assessing the creative abilities of the\nstudents who create them, thereby formulating nine dimensions.\nImmediate feedback is linked to our approach of HCI, where MLLMs\ncontinuously engage with art teachers and students, and this feed-\nback is meticulously recorded. Instead of directly evaluating the art\npieces, we divide the process into multiple sub-processes, including\nentity recognition, commentary generation, and suggestion genera-\ntion, thereby embodying the principle of progressively approaching\ntarget skills.\n3.1 Adjusting Evaluation Granularity\nOur research extends prior studies by designing a multi-agent data\ncollection system, which serves as a core component of ArtMen-\ntor [57]. Art, as a unique form of expression, is deeply rooted in the\npursuit of the essence of life, providing individuals with spiritual\nfulfillment [66]. In the era of big data, the evaluation of artworks\nfaces several technical challenges, primarily manifested in four ar-\neas: data dependency, limitations on creativity, emotional reduction,\nand the loss of intrinsic meaning [27]. For instance, instrumental\nrationality often causes data to become a rigid constraint on both\nteachersâ€™ and studentsâ€™ creative interpretations of art.\nTo address these challenges, our system is based on the prin-\nciple of relational self-expression, which elevates the evaluation\nprocess beyond mere objectivity. The core concept of relational\nself-expression draws inspiration from Henri Bergsonâ€™s philosophy\nof creative evolution, emphasizing the notion that relation precedes\nindividuality. To fully implement this idea, we incorporate insights\nfrom Bergsonâ€™s exploration of consciousness [7]. Bergsonâ€™s empha-\nsis on direct experience and pre-reflective consciousness seeks to\ncapture the undistorted reality of conscious states. Similarly, our\nsystem aims to evaluate artworks in their most authentic form. The\nsystem integrates multiple data types, focusing on images, audio,\nand textual data. Moreover, the evaluation process is implemented\nwithin a CHI framework, leveraging GPT-4o to assist art teachers in\nscoring. This hybrid approach combines machine intelligence with\nhuman expertise, ensuring both precision and contextual depth in\nartistic evaluation.\nOur evaluation framework encompasses multiple dimensions,\neach designed to capture the unique characteristics of artworks.\nThese dimensions include formative creativity (realism, deforma-\ntion, imagination), color expressiveness (color richness, color\ncontrast), line work richness (line combination, line texture),\nand conceptual thinking (picture organization, transformation).\nThis dimensional framework was developed by a member of our\nresearch team and represents an innovative contribution to the field,\nthough the corresponding work is yet to be formally published.\nâ€¢Realism: Evaluates the artworkâ€™s ability to realistically repro-\nduce subjects, capturing the precision and accuracy of represen-\ntation [9].\nâ€¢Deformation: Assesses the artworkâ€™s capacity to transform and\nrecreate reality, reflecting artistic innovation and reinterpretation\n[55].\nâ€¢Imagination: Examines the creativity and originality within the\nartwork, highlighting the artistâ€™s ability to introduce new and\nnovel perspectives [53].\nâ€¢Color Richness: Evaluates the diversity and aesthetic harmony\nof the color palette used in the artwork [44, 49].\nâ€¢Color Contrast : Assesses the visual impact and vibrancy of\ncolors, focusing on how contrasting hues interact within the\ncomposition [67].\nâ€¢Line Combination: Evaluates the arrangement and structural\ncoherence of lines, examining how line elements contribute to\nthe overall form [43].\nâ€¢Line Texture: Examines the expressiveness and tactile quality\nof line work, exploring the textural effects achieved through line\nvariations [19].\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nâ€¢Picture Organization: Evaluates the overall layout and arrange-\nment of elements within the artwork, assessing compositional\nbalance and spatial logic [43].\nâ€¢Transformation: Assesses the artistâ€™s ability to transform ab-\nstract concepts into tangible artistic expressions, exploring the\ndepth of conceptual execution [20].\nThis multidimensional evaluation framework, grounded in the\nprinciple of relational self-expression, represents an AI-driven ap-\nproach to artistic evaluation. It is based on the processes of data col-\nlection, organization, and accumulation, transitioning from object-\ncentric, observable, and dynamic assessments to human-centered\nevaluations. By constructing a human-machine collaborative eval-\nuation system, the framework ultimately aims to transcend the\nsurface self and reach the deep self, fostering the creative and emo-\ntional expression of artistic life.\n3.2 Providing Immediate Feedback\nThe design principles of the HCI dataset should prioritize both\nthe evaluation outcomes and the comprehensive recording of all\nuser-system interactions. The GOMS (Goals, Operators, Methods,\nand Selection Rules) model provides a systematic framework for\ntracking interactions, emphasizing the centrality of operators (user\nactions) and methods (task execution). This decomposition facili-\ntates researchers to isolate and analyze each component, clarifying\nhow users, such as art educators, navigate the system, refine their\nevaluations, and adjust feedback based on AI-assisted recommen-\ndations. [32].\nLeveraging the GOMS model, which emphasizes breaking down\ncomplex tasks into smaller, manageable components [52, 56], we\nadvocate dividing interaction processes into distinct phases to cap-\nture decision-making and feedback loops holistically. For example,\nin the initial phase of artwork evaluation, the dataset must capture\nevery interaction between the educator and the entity recognition\nagent, including the systemâ€™s identification results, the educatorâ€™s\nrevisions, and the final outcome. Subsequently, the review genera-\ntion agent records initial user ratings and comments, followed by\nagent revisions, user modifications, and final adjustments, culmi-\nnating in the userâ€™s submission. Finally, the suggestion generation\nagent data encapsulates the suggestions provided by MLLMs for\nimproving the artwork, which can be analyzed to understand how\nAI-assisted feedback is utilized and acted upon in real-world educa-\ntional contexts.\nEach phase is systematically recorded, ensuring a comprehen-\nsive dataset for analyzing and improving MLLM capabilities. This\ncomprehensive methodology is in line with the GOMS modelâ€™s em-\nphasis on evaluating human-system interaction by understanding\nnot only the results, but also the cognitive steps taken throughout\nthe process [26]. Moreover, the ability to refine each agentâ€™s per-\nformance based on the interaction data ensures that the dataset\nremains an integral tool in the ongoing evolution of MLLM-assisted\nartwork evaluation [46].\n3.3 Approaching Target Capabilities\nThe data analysis system should enable designers to extract mean-\ning from interactions and analyze them based on their own design\ngoals [10]. Instead of solely depending on subjective scoring of the\nprocedural data to evaluate MLLMsâ€™ assistance capabilitiesâ€”often\nprone to biasâ€”the system facilitates a more objective, iterative\nevaluation process across multiple rounds. Art teachers are empow-\nered to modify the data, prompting MLLMs to regenerate scores,\nreviews, and suggestions in subsequent rounds based on these ad-\njustments. In terms of assistance capabilities, we divide them into\nfour categories: entity recognition , style evaluation, scoring, and text\ngeneration (reviews and suggestions). These capabilities are detailed\nas follows:\nâ€¢Entity Recognition Capability : Inspired by machine learning\nclassification metrics, we define accuracy, recall, and other met-\nrics based on the interaction between the entity recognition agent\nand art educators. Following each round of teacher-driven entity\nmodifications (deleting or adding), we quantify the MLLMsâ€™ en-\ntity recognition capability, offering high interpretability through\nsuccessive rounds of human-AI interaction.\nâ€¢Style Evaluation Capability : We assess the MLLMsâ€™ style eval-\nuation capability by measuring the extent to which art educators\naccept or reject the styles identified by the MLLMs, calculated\nby whether the teacher deletes the recognized styles.\nâ€¢Scoring Capability: We extract the scores from the reviews to\nevaluate MLLMsâ€™ initial scoring of artworks, the manual scores\ngiven by teachers, and the scores regenerated by the MLLMs after\nhuman modifications. By comparing the initial MLLMs scores\nwith teachersâ€™ scores and tracking the similarity across multiple\nscore generation rounds, we assess the progression of MLLMsâ€™\nscoring capability.\nâ€¢Text Generation Capability : Both reviews and suggestions\nare treated as text outputs that can be evaluated using the same\nmetrics. Utilizing natural language processing techniques, we\ndivide the MLLMsâ€™ text generation capability into two main parts:\nthe degree of modification and the similarity of the text. These\nmetrics are derived from the modification length between initial\nand revised texts, and the word similarity measured through\ntokenization and semantic analysis over successive rounds.\n4 METHODOLOGY\nAccording to principles in Section 3, we develop ArtMentor,\nwhich includes 9 HCI processes, as shown in Figure. 2. First, the\nHCI dataset comprises these nine HCI processes, all of which are\ndocumented by the multi-agent data collection system and detailed\nin Section 4.1. Subsequently, we extract the significance of the pro-\ncesses into metrics, and provide feedback for iterative upgrades\nsystem through a data analysis system in Section 4.2.\n4.1 Documenting Processes of the Multi-agent\nData Collection System for an HCI Dataset\n4.1.1 Art Teachers and E-Agent Interaction Processes. In the multi-\nagent data collection system, the E-Agent is responsible for ex-\ntracting and classifying entities from artworks using a MLLM. We\ndemonstrate the details of Interaction Processes.\nProcess 1 (P1): Entity List Recognition. The E-Agent initiates\nthe process of recognizing entities from the given artworks. This\nprocess can be described by the following equation:\nğ¸ğ‘– = ğ´ğ‘”ğ‘’ (ğ‘€ğ¿,ğ´ğ‘Ÿğ‘¡ğ‘–,ğ¸ğ‘›ğ‘¡_ğ‘ğ‘Ÿğ‘š,Î˜,Î“), (1)\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\nFigure 2: ArtMentor Space comprises four primary components: a. Multi-Agent Data Collection System, b. HCI Dataset, c.\nData Analysis System, d. Iterative Upgrades System. The Multi-Agent Data Collection System includes three agents: Entity\nRecognition Agent (E-Agent), Review Generation Agent (R-Agent), and Suggestion Generation Agent (S-Agent). Both R-Agent\nand S-Agent perform nine dimensions, such as Realism and Deformation. Additionally, we have outlined nine HCI processes\n(from P1 to P9), where processes initiated by the computer are marked in green and those initiated by the human are marked in\norange. After data collection by the Multi-Agent system, we obtain an HCI dataset. We then apply five metrics to evaluate these\nfour capabilities. Based on the evaluation results, we aim to iteratively upgrade capabilities that underperform in the future.\nwhere ğ¸ğ‘– is the set of entities recognized from theğ‘–-th artwork, ğ´ğ‘”ğ‘’\nrepresents the E-Agent driven by the specific MLLM (ML), ğ´ğ‘Ÿğ‘¡ğ‘– is\nthe ğ‘–-th artwork being analyzed, ğ¸ğ‘›ğ‘¡_ğ‘ğ‘Ÿğ‘š is the prompt provided\nto the MLLM to guide the entity extraction,Î˜ is a set of parameters\nused in the extraction process, and Î“ is the set of dimension-related\nhyperparameters.\nProcess 2 (P2): Entity Right List Addition. During this process,\nthe teacher adds new correct entities that are not already present in\nthe set ğ¸ğ‘– . The set of entities newly added by the teacher is defined\nby the following equation:\nğ‘…ğ‘– = {ğ‘’ğ‘–ğ‘š |ğ‘š= 1,2,3,...,ğ‘€ ğ‘–,âˆ€ğ‘’ğ‘–ğ‘š âˆ‰ ğ¸ğ‘– }, (2)\nwhere ğ‘…ğ‘– represents the set of entities added by the teacher, and\nğ‘’ğ‘–ğ‘š âˆ‰ ğ¸ğ‘– for all ğ‘š.\nProcess 3 (P3): Entity Wrong List Deletion. The teacher re-\nmoves incorrect entities from ğ¸ğ‘– , defined as:\nğ‘Šğ‘– = {ğ‘’ğ‘–ğ‘ |ğ‘= 1,2,3,...,ğ‘„ ğ‘–,âˆ€ğ‘’ğ‘–ğ‘ âˆˆğ¸ğ‘– }, (3)\nwhere ğ‘Šğ‘– is the set of entities identified as incorrect by the teacher,\nand ğ‘’ğ‘–ğ‘ âˆˆğ¸ğ‘– for all ğ‘.\nFinal Entity List Update. Following the revisions of correct\nand incorrect entities, the final set of entities for each artwork is\nupdated by incorporating the revised correct entities and excluding\nthe incorrect ones. As defined by Eqs. 1, 2, and 3, the final entity\nlist is expressed as:\nË†ğ¸ğ‘– = ğ¸ğ‘– = ğ¸ğ‘– âˆªğ‘…ğ‘– \\ğ‘Šğ‘–, (4)\nwhere Ë†ğ¸ğ‘– denotes the final revised list of recognized entities. The\nnew ğ¸ğ‘– represents the updated set of entities for the ğ‘–-th artwork,\nrevised by the art teacher. After revision, Ë†ğ¸ğ‘– is equivalent to ğ¸ğ‘– .\n4.1.2 Art Teachers and R-Agent Interaction Processes. The R-Agent\nis responsible for generating reviews for artworks, which involves\nboth scoring (ranging from 1 to 5 as integers) and textual review\ngeneration. The primary processes involved are: review generation,\nreview modification, score extraction, and score adjustment.\nProcess 4 (P4): Review Generation. The R-Agent generates a\nreview for the ğ‘˜-th dimension of the ğ‘–-th artwork as defined by the\nfollowing equation:\nğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ = ğ´ğ‘”ğ‘Ÿ (ğ‘€ğ¿,ğ´ğ‘Ÿğ‘¡ğ‘–,ğ‘…ğ‘’ğ‘£_ğ‘ƒğ‘šğ‘¡ğ‘˜, Ë†ğ¸ğ‘–,ğœƒğ‘˜,ğ›¾ğ‘˜,ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ ,ğ‘“ ), (5)\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nwhere ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ is the review generated for the ğ‘˜-th dimension of the\nğ‘–-th artwork. If ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ is empty, the review is solely generated by\nthe computer; if ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ contains content, it indicates collaboration\nbetween the human and the computer in the review process. ğ´ğ‘”ğ‘Ÿ\nis the Review-Agent, ğ‘€ğ¿ denotes the specific MLLM, ğ´ğ‘Ÿğ‘¡ğ‘– refers\nto the artwork in question, ğ‘…ğ‘’ğ‘£_ğ‘ƒğ‘šğ‘¡ğ‘˜ is the prompt for the review,\nË†ğ¸ğ‘– is the final list of entities updated through interaction with the\nE-Agent and humans as per Eq. 4,ğœƒğ‘˜ and ğ›¾ğ‘˜ are the parameters and\ndimension-related hyperparameters used in generating the review,\nrespectively, and ğ‘“ is the function used for extracting scores from\nthe reviews.\nProcess 5 (P5): Review Modification. During this process, the\nteacher modifies the generated review. The modifications are quan-\ntified by the following equation:\nğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ = ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ âˆªğ¼ğ‘›ğ‘ ğ‘–ğ‘˜ \\ğ·ğ‘’ğ‘™ğ‘–ğ‘˜ , (6)\nwhere ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ represents the review following human modifications.\nThis review may be co-generated by both the computer and the\nhuman if ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ as defined in Eq. 5 is not empty. Ifğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ is empty, it\nindicates no computer involvement in the initial review generation.\nğ¼ğ‘›ğ‘ ğ‘–ğ‘˜ denotes the review content inserted by the teacher for the\nğ‘˜-th dimension of the ğ‘–-th artwork, and ğ·ğ‘’ğ‘™ğ‘–ğ‘˜ denotes the review\ncontent removed from the same dimension.\nProcess 6 (P6): Score Extraction. Scores are extracted from the\ngenerated review as follows:\nğ‘†ğ‘–ğ‘˜ = ğ‘“(ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ ), (7)\nwhere ğ‘†ğ‘–ğ‘˜ represents the score for the ğ‘–-th artwork along the ğ‘˜-th\ndimension. The function ğ‘“ extracts the score from the reviewğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ ,\nspecifically deriving from the procedures outlined in Process 4.\nProcess 7 (P7): Score Adjustment. This process involves the\nmodification of scores as detailed below:\nğ‘†ğ‘–ğ‘˜ = ğ‘†ğ‘–ğ‘˜ âˆ’Î”ğ‘ , (8)\nwhere ğ‘†ğ‘–ğ‘˜ denotes the adjusted score for the ğ‘–-th artwork on the\nğ‘˜-th dimension, and Î”ğ‘  represents the human modification, which\nmay be either an increase or a decrease. All scores are integers\nwithin a five-point scale.\nFinal Review Submission. In the concluding process, scores\nand reviews are finalized through collaborative efforts between\nhuman and computer. The initial reviews and scores produced by\nthe computer undergo thorough scrutiny and adjustments by the\nteacher, enhancing their accuracy and reliability. The final review\nis denoted as Ë†ğ‘…ğ‘’ğ‘£ğ‘–ğ‘˜ and includes the final score Ë†ğ‘†ğ‘–ğ‘˜ .\n4.1.3 Art Teachers and S-Agent Interaction. The S-Agent generates\nand revises artwork suggestions, involving processes likeSuggestion\nGeneration, Modification, and Final Submission. Both reviews and\nsuggestions are text-based with similar interaction processes.\nProcess 8 (P8): Generation :\nğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ = ğ´ğ‘”ğ‘  (ğ‘€ğ¿,ğ´ğ‘Ÿğ‘¡ğ‘–,..., ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ ), (9)\nwhere ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ is the suggestion, ğ´ğ‘”ğ‘  is the S-Agent, and ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ is the\nco-generated suggestion.\nProcess 9 (P9): Modification :\nğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ = ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ âˆªğ¼ğ‘›ğ‘ ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ \\ğ·ğ‘’ğ‘™ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ , (10)\nwhere ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ is the final suggestion,ğ¼ğ‘›ğ‘ ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ is the inserted content,\nand ğ·ğ‘’ğ‘™ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ is the removed content.\nFinal Submission : The finalized suggestions Ë†ğ‘†ğ‘¢ğ‘”ğ‘–ğ‘˜ , crafted\nthrough collaboration, are submitted.\n4.2 Evaluation from the Data Analysis System\nfor Iterative Upgrades System\nTo evaluate assistance capabilities of MLLMs, our data analysis\nsystem covers diverse metrics:Entity Classification Metrics (inspired\nby machine learning classification metrics), Art Style Metrics , Score\nAcceptance Metrics, and Text Acceptance Metrics. The insights gained\nfrom these metrics guide the iterative enhancement of capabilities.\n4.2.1 Entity Classification Metrics for E-Agent. For the E-Agent, we\ndemonstrate the design of entity classification metrics derived from\nthe confusion matrix. We defineTrue Positive (TP), Misrepresentation\n(MR), False Positive (FP) , and False Negative (FN) as follows:\nâ€¢TP: The number of entities correctly identified in the artwork\nby the E-Agent reflecting accurately recognized entities. The\ncalculation is expressed as:\nğ‘‡ğ‘ƒ = |ğ¸ğ‘– |âˆ’|ğ‘Šğ‘– |. (11)\nâ€¢MR: The number of entities incorrectly identified as other entities\nby the E-Agent, e.g., a \"horse\" misidentified as a \"donkey\". The\ncalculation follows:\nğ‘€ğ‘… = min(|ğ‘Šğ‘– |,|ğ‘…ğ‘– |). (12)\nâ€¢FP: The number of non-existent entities recognized by the E-\nAgent, e.g., mistakenly identifying nonexistent water in an art-\nwork. The calculation is adjusted for misrepresentations:\nğ¹ğ‘ƒ = max(|ğ‘Šğ‘– |âˆ’ğ‘€ğ‘…,0). (13)\nâ€¢FN: The number of entities overlooked by the E-Agent. This\ninvolves entities present in the artwork but not recognized, cal-\nculated by:\nğ¹ğ‘ = max(|ğ‘…ğ‘– |âˆ’ğ‘€ğ‘…,0). (14)\nBased on these definitions, we redefine the metrics of accuracy,\nprecision, recall, and F1-score to evaluate the E-Agentâ€™s entity recog-\nnition capabilities:\nğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ = ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ +ğ¹ğ‘ƒ +ğ¹ğ‘ +ğ‘€ğ‘…,\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ +ğ¹ğ‘ƒ +ğ‘€ğ‘…,\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ +ğ¹ğ‘ +ğ‘€ğ‘…,\nğ¹1 = 2 Ã—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› +ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ .\n(15)\nThese calculations provide a comprehensive evaluation of MLLMsâ€™\nperformance in entity recognition within artworks, facilitating tar-\ngeted improvements to the model.\n4.2.2 Art Style Metrics for E-Agent. We conceptualize art style as\na distinct entity and introduce the Art Style Sensitivity (ASS)\nmetric to assess the E-Agentâ€™s capability to accurately recognize\nand evaluate various art styles. The metric is defined as follows:\nASS = 1 âˆ’ğ·\nğ‘, (16)\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\nwhere ğ‘ represents the total number of art styles identified, and\nğ· denotes the number of incorrect recognitions as flagged and\ncorrected by the art teacher.\n4.2.3 Score Acceptance Metrics for R-Agent. As the scoring of art-\nworks becomes increasingly prevalent in automated assessment\nsystems, it is crucial to evaluate the scoring capabilities of the R-\nAgent. We focus on three key aspects: First: the score difference\nbetween initial scores provided by MLLMs and the scores assigned\nby art teachers in subsequent rounds ( Process 4 to Process 5 ).\nSecond, the consistency of scoring between MLLMs and art teach-\ners across all rounds. Third, the volatility of scores either from art\nteachers or MLLMs.\nScore Difference (SD) : This metric quantifies the deviation\nbetween the initial MLLM-generated scores and the modified scores\nassigned by art teachers in subsequent rounds ( Process 5 ). The\nequation is defined as:\nSD = 1\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\n\f\f\fğ‘†(1)\nMLLM âˆ’ğ‘†(ğ‘–)\nart teacher\n\f\f\f, (17)\nwhere ğ‘†(1)\nMLLM represents the initial MLLM score,ğ‘†(ğ‘–)\nart teacher denotes\nthe art teacherâ€™s modified score in theğ‘–-th round, and ğ‘ is the total\nnumber of rounds.\nScore Consistency (SC) : This metric evaluates the alignment\nbetween art teachersâ€™ scores and those of MLLMs by measuring\nthe Spearman correlation coefficient (ğœŒ) between the scores given\nby MLLMs and the scores modified by art teachers. The Spearman\ncoefficient is a non-parametric measure of rank correlation that\nassesses the monotonic relationship between two variables. It is\ncalculated as follows:\nSC = ğœŒ(ğ‘†MLLM,ğ‘†art teacher), (18)\nwhere ğ‘†MLLM and ğ‘†art teacher are vectors representing the series of\nscores from MLLMs and the corresponding modified scores by art\nteachers, across all rounds.\nScore Volatility (SV) : This metric quantifies the stability of\nscoring by either art teachers or MLLMs across all rounds, reflecting\nthe consistency of the scoring process. It is calculated using the\nstandard deviation:\nSV = std(ğ‘†scorer), (19)\nwhere ğ‘†scorer represents the vector of scores modified by a specific\nscorer, either an art teacher or an MLLM. This metric is crucial\nfor identifying instances of erratic scoring behavior that could\nundermine the reliability of the analysis.\n4.2.4 Text Acceptance Metrics for R-Agent and S-Agent. For R-Agent\nand S-Agent, natural language generation represents a core capa-\nbility, producing both reviews and suggestions in textual form.\nHowever, analyzing text within HCI contexts can be challenging.\nTo address this, we propose evaluating text through two key met-\nrics: text length and word similarity. These metrics respectively\nprovide insights into the conciseness and relevance of the generated\ntext.\nText Modification Rate (TMR) : This metric quantifies the\nacceptance of MLLM-generated text (e.g., reviews, suggestions) by\ncalculating the ratio of characters modified by the art teacher to\nthe original text. The formula is defined as:\nTMR = 1\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\nlen(ğ‘‡(ğ‘–)\nMLLM)âˆ’len(ğ‘‡(ğ‘–)\nremoved)\nlen(ğ‘‡(ğ‘–)\nMLLM)+len(ğ‘‡(ğ‘–)\nadded)\n, (20)\nwhereğ‘‡(ğ‘–)\nadded andğ‘‡(ğ‘–)\nremoved represent the number of characters added\nand removed by the art teacher in the ğ‘–-th round, respectively, and\nğ‘‡(ğ‘–)\nMLLM is the original number of characters in the MLLM-generated\ntext.\nText Similarity (TS) : We assess the similarity between the\noriginal MLLM-generated text and the modified text using cosine\nsimilarity metrics. Prior to similarity computation, both texts are\nvectorized using the Bag of Words (BoW) model, referred to as\n\"Wordbag\" in our context. This model represents each text as a\nvector, where each dimension corresponds to a word in the vocab-\nulary. The value in each dimension represents the frequency of\nthe corresponding word in the text. The vectorization process is\ndetailed as follows:\nğ‘‡MLLM = Wordbag(ğ‘‡MLLM),\nğ‘‡art teacher = Wordbag(ğ‘‡art teacher), (21)\nwhere ğ‘‡MLLM and ğ‘‡art teacher are the vectorized representations of\nthe MLLM-generated and art teacher-modified texts, respectively.\nCosine similarity is then used to compute the degree of similarity:\nTS = Cosine(ğ‘‡MLLM,ğ‘‡art teacher)= ğ‘‡MLLM Â·ğ‘‡art teacher\nâˆ¥ğ‘‡MLLM âˆ¥âˆ¥ğ‘‡art teacher âˆ¥. (22)\n5 GUIDE FOR ArtMentor SPACE\n5.1 Data Collection Interface\nThe user interface and interaction design of the ArtMentor space\nare specifically developed to provide art teachers with an intuitive\nand structured tool for evaluating artworks. The primary objective\nof this interface is to support detailed art analysis through a series\nof structured interaction processes.\nFigure 3: E-Agent and art teacher interaction collection.\nAs shown in Figure. 3, the interface allows art teachers to upload\nan artwork for analysis and provides multiple evaluation sections\nbased on different artistic dimensions. After uploading the artwork,\nthe interface displays the artwork and automatically recognizes\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nand lists entities within the artwork. Art teachers can add or re-\nmove entities to optimize the recognition results. Finally, the HCI\nprocesses saved in JSON files.\nAs shown in Figure. 4, the interface offers a set of categorized\nevaluation tools. Art teachers can select different dimensions (e.g.,\nrealism, deformation) to generate or manually input scores and\nreviews. Each dimension has specific evaluation tools, including\nbuttons for generating review and suggestions from MLLMs.\nFigure 4: R & S-Agent and art teacher interaction collection.\n5.2 Data Collection Procedure\n5.2.1 Participants and Artworks. In the ArtMentor space, we re-\ncruited five art teachers from diverse educational backgrounds,\nincluding both pre-service and in-service art teachers. The selec-\ntion of participants was aimed at offering a range of perspectives\nto facilitate a comprehensive evaluation of elementary studentsâ€™\nartworks. The artworks used in this study were categorized into\nthree groups: narrative illustrations (1-3), Chinese ink paintings\n(4-7), and artworks following the Egyptian frontal law (8-20), with\neach number range corresponding to its respective category.\n5.2.2 Art Evaluation Details. Participants were provided with de-\ntailed instructions on how to interact with the ArtMentor space.\nThey were required to evaluate each piece of artwork across multi-\nple dimensions, spending at least ten minutes on each evaluation. To\nenhance the multidimensional nature of the evaluation process and\ncapture more subtle aspects of artistic expression, students were al-\nlowed to provide audio explanations of their artworks. These audio\nexplanations allowed art teachers to gain insights into the studentsâ€™\ncreative processes, intentions, and emotional connections to their\nwork, aligning with our principle of evaluation beyond dehuman-\nization. The guidelines emphasized three primary requirements:\n(1) ensuring that each evaluation is thorough, covering multiple\nartistic dimensions; (2) actively using the MLLM-generated reviews\nand suggestions provided by the system to enhance their evalua-\ntions; (3) carefully considering the studentsâ€™ audio explanations to\ncapture nuanced information about the artistic process and intent.\n5.2.3 Post-Evaluation Interviews. After completing each evalua-\ntion session, participants were asked to participate in an interview\ndesigned to gather feedback on their experience with the ArtMen-\ntor space. The interview focused on five key areas: participantsâ€™\nperceptions of the accuracy of the MLLMâ€™s entity recognition, the\nusability and user interaction experience with the space, the effec-\ntiveness of MLLM in assisting with scoring and review or suggestion\ngeneration, the MLLMâ€™s performance across different functional\ndimensions, and the relevance of the MLLMâ€™s suggestions in sup-\nporting elementary studentsâ€™ artistic development. The detailed\ninterview questionnaire is provided in Appendix A. The content\nof the interviews will be used to support the result analysis, and\nsome materials can be accessed at link.\n5.3 HCI Dataset Overview\nGPT-4o [1] has been selected to assist in art education due to its\nadvanced capabilities in MLLMs. The dataset comprises evalua-\ntive feedback from five art teachers on a diverse collection of 20\nartworks, covering multiple key dimensions and providing a rich re-\nsource for analysis. It is worth noting that the teachers and students\nwho contributed to this dataset are from the first and second grades\nof a primary school, offering a valuable educational perspective\nfrom early childhood education.\nThe dataset includes 20 JSON files documenting the GPT-4oâ€™s\nentity and style recognition results for each artwork. Additionally,\nthere are 360 JSON files that record detailed reviews and suggestions\nprovided by the teachers for each dimension. This extensive dataset,\nwhich chronicles 380 sessions, lays a solid foundation for studying\nthe application of MLLMs in art education.\n6 RESULTS ANALYSIS SYSTEM IN ArtMentor\nIn this section, we regard art style as a distinct entity. Consequently,\nwe analyze the results concerning entity classification metrics and\nart style metrics in Section 6.1. Next, we examine the score genera-\ntion capabilities of GPT-4o in Section 6.2. Finally, we evaluate the\nreview and suggestion capabilities of GPT-4o in Section 6.3.\n6.1 Analysis of Entity Recognition Capability\nTo evaluate the entity recognition capabilities of GPT-4o, weini-\ntially focus on art style recognition as it offers a comprehensive\ninsight into the artwork, forming the basis of our analysis. Sub-\nsequently, we examine the average recognition capabilities across\nvarious entities. Finally, we delve into the finer details within the\nartwork to assess nuanced recognition performance .\n6.1.1 How capable is GPT-4o in recognizing art styles? As depicted\nin Figure. 5, GPT-4o demonstrates a robust art style recognition\ncapability with an 80% Art Style Sensitivity (ASS), as detailed in Sec-\ntion 4.2.2. Specifically, it accurately identifies narrative illustrations\n(artworks 1-3) and adheres to the Egyptian frontal law (artworks\n8-20).\nHowever, it occasionally misidentifies Chinese ink paintings\n(artworks 4-7) as watercolors. This error likely arises from a mis-\nunderstanding by GPT-4o that Chinese ink paintings can indeed\ninclude a variety of colors, not just simply black and white. This\nmisconception underscores the need for model optimization to bet-\nter understand and differentiate between complex art styles, paving\nthe way for future enhancements.\n6.1.2 What is the average entity recognition capability of GPT-4o?\nAs depicted in Figure. 7, precision and F1-score are notably high.\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\n                   \n           \n \n \n           \n  \n    \n  \n  \n  \n  \n \n \n \n \n1 2            3             4\n5             6             7            8\n9            10           11           12 \n13           14            15           16 \n17            18           19           20\nStyle\n80%\nIncorrect \nCorrect\nFigure 5: Recognition of art styles by GPT-4o across 20 artworks\n(Artwork Numbers 1-20).\nThe elevated precision may result from a high rate of false negatives\n(FN), as explained by the precision equation (Eq. 15) and the FN\nequation (Eq. 14). A detailed examination reveals that |ğ‘…ğ‘– |exceeds\n|ğ‘Šğ‘– |, suggesting art teachers often add more entities than they\nremove, indicative of a potential bias towards over-recognition.\nThis bias ensures no significant entities are overlooked, aligning\nwith GPT-4oâ€™s strength in minimizing false positivesâ€”a key factor\nin applications that demand high predictive accuracy. Moreover,\nthe F1-score of 0.881 demonstrates a strong, balanced performance\nin entity recognition between precision and recall.\nInterestingly, accuracy (0.833) and recall (0.836) closely align,\nsuggesting minimal false positives (FP), as indicated by the preci-\nsion formula (Eq. 15) and the false positive formula (Eq. 13). The\nnear equivalence of |ğ‘Šğ‘– |and misrepresentations (MR) supports\nhigh precision, showing that art teachers typically replace entities\nwith precision. This behavior underscores the modelâ€™s adaptabil-\nity and conservative approach to entity handling, enhancing its\ngeneralization across various art styles.\n6.1.3 How does GPT-4o perform in recognizing artwork details? To\nclarify the results further, we illustrate them with a diagram of four\nwaffle charts as shown in Figure. 6.\nThe first observation highlights a significant discrepancy be-\ntween recall (see Figure. 6c) and accuracy (see Figure. 6a) in the\nthirteenth artwork. Unlike other artworks where these metrics\nare usually similar, this piece features many abstract figures, po-\ntentially causing a high number of false positives. The second\nobservation concerns the near absence of false positives. In most\nartworks, precision is exceptionally high (often 1.000), as shown\nin Figure. 6b, suggesting that the model rarely misidentifies non-\nexistent entities and maintains stable performance across various\nartworks. However, styles like Chinese ink painting may require\nfurther optimization to enhance recall. Finally, metrics indicate\nunderperformance in the sixth and seventh artworks, classified\nunder Chinese ink painting. Their abstract nature poses challenges\nfor the model in accurately recognizing and classifying entities.\n6.2 Analysis of Score Generation Capability\nFirst, we examine the score volatility of both GPT-4o and art teach-\ners to validate the reliability of score generation. Next, we investi-\ngate any significant differences between the initial scores provided\nby GPT-4o and those assigned by art teachers prior to HCI. Finally,\nwe assess whether the scores from GPT-4o and art teachers con-\nverge or diverge after HCI, exploring potential changes towards\nconsensus or further discrepancy.\n6.2.1 What is the score volatility for GPT-4o and art teachers? The\nanalysis of Score Volatility (SV) provides significant insights into the\nstability of scoring by both human scorers and GPT-4o prior to HCI.\nArt teachers demonstrate consistent scoring patterns, assigning\nidentical scores to the same artwork across similar dimensions\nthroughout the evaluation process. Similarly, GPT-4o, operating at a\ntemperature setting of zero, consistently produces the same outputs\nfor identical artworks under the same conditions. The minimal score\nvolatility observed reinforces the reliability of the dataset, leading\nus to accept it as robust for further analysis.\n6.2.2 What are the differences between initial scores from GPT-\n4o and art teachers? The Score Difference (SD) metric uncovers\nscoring discrepancies across various artistic dimensions. As shown\nin Figure. 8a, Realism and Transformation display higher SDs of\n0.3208 and 0.2941, respectively, highlighting significant divergences\nbetween the modelâ€™s initial assessments and human evaluations.\nIn contrast, categories such as Deformation, Color Richness,\nColor Contrast, Line Combination, Line Texture, and Picture\nOrganization show minimal SDs, indicating a strong concordance\nbetween GPT-4o outputs and art teacher adjustments.\nImagination, with a moderate SD of 0.5000, signals the po-\ntential for further model tuning to achieve closer alignment with\nexpert judgments. Overall, while GPT-4o generally agrees with\nhuman scoring, it requires targeted improvements in areas like\nImagination to refine its evaluative accuracy.\n6.2.3 How do scores from GPT-4o and art teachers change after\nhuman-computer interaction? The Score Consistency (SC) analy-\nsis, employing the Spearman correlation coefficient, reveals strong\nalignment between GPT-4o and art teachersâ€™ scores post-HCI, as\ndepicted in Figure. 8b. SC values for key dimensions are: Real-\nism (0.9438), Deformation (0.9655), Imagination (0.5209), and\nTransformation (0.6555), among others. Notably, Realism saw\nsignificant score convergence in later rounds, suggesting effective\nmodel-human integration through iterative feedback.\nHowever, lower SC values in Imagination and Transforma-\ntion highlight areas needing further model calibration to better\nmatch human assessments. Overall, the post-HCI data indicates a\nstrong general agreement, with ongoing model refinements crucial\nfor uniform scoring accuracy across all dimensions.\n6.3 Analysis of Review and Suggestion\nGeneration Capability\nWe evaluate GPT-4oâ€™s ability to generate reviews and suggestions\nusing two metrics: the Text Modification Rate (TMR) and Text Sim-\nilarity (TS). These metrics quantify how well the generated texts\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„        \n  \nExcellent 0.88-1 (4)\nGood 0.75-0.88 (14)\nPoor \nï•„\nï•„\nï•„\n                             \n         \n0.867      0.769        0.8 75        0.833         0. 800         0.600        0. 667         0. 889       \n0-0.75 (2)\n0.857       0.857    \n11                        \n \n12           13           14         15           16           17 18           \n           \n19 \n1   \n            \n   \n2             3             4             5            6             7            8\n       0.778              0.875                0.90.875      0.818      41      0.889                        \n \n0.875      0.8 18      0.909       0.8 67 \n9            10\n 20\n(a) Accuracy.\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„        \n  \n   \nExcellent 0.88-1 (16)\nGood 0.75-0.88 ( 2)\nPoor \n                             \n                   11\n0.929      0. 909        0. 933         1.000         1.000         0.600        0. 667         1.000        1.000       1.000\n  \nï•„\nï•„\nï•„\n   \n                    \n           \n12           13           14          15           16          \n0-0.75 (2)\n 17 18  19 \n            \n   \n1   2             3             4            5             6             7             8\n       1.000             1.000                      0.875 1.000                      1.000      1.000      0.875       1.000       0.90 9      1.000   \n9            10\n 20 (b) Precision.\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„        \n  \n   \nExcellent 0.88-1 (5)\nGood 0.75-0.88 (13)\nPoor \n                             \n                   \n  \nï•„\nï•„11\nï•„\n    \n0-0.75 (2)\n       0.867        0. 769   0. 875              0.833              0.800           0.600        0. 667             \n           \n0.889 0.857 0.857\n         \n           \n12           13          14           15           16           17   18 19 \n            \n   \n   \n                \n2             3             4            5             6             7             1 8\n0.778                                        0.930.875 3 0.818   0.941   0.889              0.875       0.818   0.90 9   0.867\n9            10\n 20\n(c) Recall.\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„\nï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„ ï•„        \n  \nExcellent 0.88-1 (15)\nGood    0.75-0.88 (3)\nPoor \n                             \n                   \n  0-0.75 (2\nï•„\nï•„\nï•„ )\n        \n0.897       0. 833        0. 903        0.909         0.889         0.600           0.667         0.941            0.923       0.923\n         \n           \n11   12           13           14          15           16           17 18  19 \n            \n   \n1   2             3             4            5             6             7             8\n                                            0.875  0.9700.903       0.900 0.933      0.941    0.875          0.900        0.90 9      0.929\n9            10\n 20 (d) F1-Score.\nFigure 6: Entity classification metrics for GPT-4o across 20 artworks (Artwork Numbers 1-20).\n \n0.96\n0.94\n0.92\n0.90\n0.88\n0.86\n0.84\n0.82\n0.80\n0.78\nAccuracy Precision Recall F1\n0.833\n0.935\n0.836\n0.881\nRate\nFigure 7: Average entity recognition capability of GPT-4o.\nmeet art teachersâ€™ expectations and modifications, providing in-\nsights into the effectiveness of the modelâ€™s text generation. Detailed\nanalyses in the following subsubsections will explore the distinct\ncapabilities of GPT-4o in generating reviews and suggestions.\n6.3.1 How Effective is the Review Generation Capability of GPT-4o?\nThe Text Modification Rate (TMR) and Text Similarity (TS) metrics\nprovide insights into the alignment of GPT-4o-generated reviews\nwith expert evaluations.\nAs illustrated in Figure. 9, theTMR values, which indicate the ex-\ntent of textual modifications by art teachers, are shown for various\ndimensions. These include: Realism (0.881), Deformation (0.882),\nImagination (0.875), Color Richness (0.971), Color Contrast\n(0.969), Line Combination (0.957), Line Texture (0.978), Picture\nOrganization (0.964), and Transformation (0.956). These metrics\nsuggest that the generated reviews were closely aligned with the\nexpectations, requiring only minimal adjustments.\nAdditionally, as depicted in Figure. 10, the TS metrics highlight\nthe semantic consistency post-modification, with high similarity\nscores noted across various dimensions:Realism (0.992), Deforma-\ntion (0.997), Imagination (0.978), Color Richness (0.995), Color\nContrast (0.995), Line Combination (0.998), Line Texture (0.986),\nPicture Organization (0.993), and Transformation (0.999). These\nscores confirm that the modifications by art teachers maintained\nthe core content and intent of the original reviews , verifying GPT-4oâ€™s\ncapability to generate contextually appropriate and stylistically\nprecise content.\n6.3.2 How Effective is the Suggestion Generation Capability of GPT-\n4o? The effectiveness of GPT-4o in generating suggestions is also\nassessed using the TMR and TS metrics.\nAs shown in Figure. 11, the TMR values, based on the length of\nadded and deleted content, indicate that GPT-4o performs relatively\nwell in dimensions like Imagination (0.968), Realism (0.940), and\nDeformation (0.895), where fewer modifications were necessary.\nIn contrast, lower TMR values in Transformation (0.719), Line\nCombination (0.730), and Line Texture (0.731) suggest that sug-\ngestions in these areas required more extensive revisions by art\nteachers, both in terms of content addition and deletion.\nIn Figure. 12, the TS values, which focus on word semantics,\nshow high similarity in dimensions such as Realism (0.999), De-\nformation (0.999), and Imagination (0.997), indicating that the\ncore meaning of the suggestions remained largely intact. However,\ndimensions with lower TMR, such as Transformation (0.788),\nLine Texture (0.863), and Line Combination (0.879), also exhibit\nlower TS values, suggesting that not only was sentence length al-\ntered, but significant semantic changes were made as well. This\npattern contrasts with review generation, where modifications af-\nfected length without drastically changing semantics, indicating\nthat suggestion generation is less effective than review generation.\nOverall, the dimension requiring the most improvement is Trans-\nformation, as it shows substantial modifications in both TMR and\nTS, reflecting changes in both content length and semantics. En-\nhancing GPT-4oâ€™s suggestion generation in this area would greatly\nimprove its alignment with art teacher expectations.\n7 DISCUSSION\nIn this section, we delve into the intriguing question of whether\nan entity at the intersection of the arts, education, and AI/MLLMs\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\n0.01\n0.05\n0.09\n0.13\n0.17\n0.21\n0.25\n0.29\n0.33\n(a) Score Difference (SD).\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95 (b) Score Consistency (SC).\nFigure 8: Score Acceptance Metrics.\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\nFigure 9: Text Modification Rate (TMR) for R-Agent.\n0.97\n0.975\n0.98\n0.985\n0.99\n0.995\n1\nFigure 10: Text Similarity (TS) for R-Agent.\ncan emerge as an independent assistant within the teacher-student-\nmachine triadic dialogue system for the evaluation of artworks. To\naddress this, we will systematically examine the manifestations of\ncapabilities in multimodal perception, recognition, understanding,\nand reasoning across the three intelligent agents, thereby shedding\nlight on the potential of such a MLLM to autonomously contribute\nto artistic assessment.\n0.7\n0.73\n0.76\n0.79\n0.82\n0.85\n0.88\n0.91\n0.94\nFigure 11: Text Modification Rate (TMR) for S-Agent.\n0.7\n0.74\n0.78\n0.82\n0.86\n0.9\n0.94\n0.98\nFigure 12: Text Similarity (TS) for S-Agent.\n7.1 Multimodal Perception and Recognition\nCapabilities in GPT-4oâ€™s Entity Recognition\nIn our analysis of entity recognition performance, we observed two\nkey challenges. First, art teachers tended to add more entities than\nthey removed, suggesting that GPT-4oâ€™s initial entity recognition\noften overlooks relevant details. Second, GPT-4o emphasizes local\nfeatures, such as facial details or limbs, rather than recognizing\ncomplete entities, leading to an over-granulation effect. This reveals\na limitation in GPT-4oâ€™s ability to holistically interpret artwork.\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nHowever, despite these challenges, GPT-4o demonstrates remark-\nable multimodal perception and recognition capabilities. The high\nprecision and F1-score across various artworks indicate a strong\nability to identify and classify entities accurately. The modelâ€™s per-\nformance in recognizing art styles, as evidenced by an 80% Art\nStyle Sensitivity (ASS), further underscores its robustness in under-\nstanding and differentiating complex artistic expressions. GPT-4oâ€™s\nability to balance between holistic interpretation and detail orienta-\ntion is noteworthy. While it sometimes misses broader entities, its\nfocus on local features ensures that intricate details are not over-\nlooked. This dual approach is particularly valuable in the context\nof art analysis, where both the overall style and the finer details\ncontribute to the understanding of the artwork. The modelâ€™s adapt-\nability across different art styles, from narrative illustrations to\nChinese ink paintings, highlights its generalization capabilities.\nAlthough there are areas for improvement, such as better differen-\ntiation between Chinese ink paintings and watercolors, the overall\nperformance is commendable. This adaptability is crucial for appli-\ncations that require consistent performance across a diverse range\nof artistic styles. The near absence of false positives is a significant\nstrength of GPT-4o. High precision values indicate that the model is\nconservative in its entity recognition, avoiding the misidentification\nof non-existent entities. This approach is particularly important\nin scenarios where accuracy is paramount. While GPT-4o shows\nexcellent performance in both entity and style recognition, there is\nroom for further refinement. Future work could focus on enhanc-\ning the modelâ€™s ability to recognize broader entities and reducing\nthe over-granulation effect. Additionally, optimizing the modelâ€™s\nunderstanding of specific art styles, such as Chinese ink painting,\ncould further improve its performance.\nIn summary, GPT-4oâ€™s entity recognition and style recognition\ncapabilities are excellent. The modelâ€™s high precision, strong F1-\nscore, and robust art style sensitivity demonstrate its effectiveness\nin analyzing artwork. Despite some limitations, the modelâ€™s overall\nperformance is impressive, and it holds great potential for future\nadvancements in multimodal perception and recognition.\n7.2 Multimodal Understanding Capability in\nGPT-4oâ€™s Review Generation\nIn the Realism dimension, we observed an encouraging trend to-\nward convergence between GPT-4o and art teachers during the\nscoring process. Initial discrepancies in assessments gradually di-\nminished as both sides adjusted through repeated interactions, ul-\ntimately leading to closer alignment. This suggests potential for\nimproving the modelâ€™s adaptability over time.\nThe convergence observed in the Realism dimension is a tes-\ntament to GPT-4oâ€™s ability to learn and adapt through human-\ncomputer interaction. This adaptability is crucial for the modelâ€™s\nlong-term effectiveness, as it indicates a capacity for continuous\nimprovement in understanding and evaluating artistic nuances.\nGPT-4oâ€™s performance in generating reviews that align closely with\nart teachersâ€™ expectations across various dimensions, as evidenced\nby high TMR and TS metrics, demonstrates its holistic understand-\ning of artwork. The modelâ€™s ability to maintain the core content and\nintent of reviews even after modifications by art teachers highlights\nits robustness in generating contextually appropriate and stylisti-\ncally precise content. While the overall performance is strong, a\ncloser look at specific dimensions reveals areas for further refine-\nment. For instance, the Imagination dimension shows moderate\nTMR and TS values, indicating a need for enhanced understand-\ning of abstract and creative aspects of artwork. This suggests that\nGPT-4o could benefit from additional training data or algorithmic\nadjustments to better capture the subtleties of imaginative expres-\nsions. The iterative feedback loop established between GPT-4o and\nart teachers plays a pivotal role in refining the modelâ€™s review gen-\neration capabilities. This loop enables the model to receive targeted\nfeedback, making incremental improvements with each iteration.\nContinued engagement in this feedback loop is essential for achiev-\ning even greater accuracy and alignment with human evaluations.\nFuture work should focus on expanding GPT-4oâ€™s training to in-\nclude a wider range of artistic styles and dimensions, particularly\nthose that currently show lower convergence rates. Additionally,\nexploring advanced multimodal understanding techniques, such as\nintegrating visual and textual data more seamlessly, could further\nenhance the modelâ€™s review generation capabilities.\nIn summary, GPT-4oâ€™s multimodal understanding capability in\nreview generation is robust, with strong performance across various\nartistic dimensions. The modelâ€™s ability to converge with human\nassessments over time, coupled with its adaptability and holistic\nreview generation, underscores its potential as a valuable tool in\nart evaluation. Ongoing refinements and iterative feedback will be\nkey to unlocking even greater capabilities in the future.\n7.3 Multimodal Reasoning Capability in\nGPT-4oâ€™s Suggestion Generation\nThe analysis of GPT-4oâ€™s suggestion generation capability reveals\ninsights into its multimodal reasoning abilities, particularly in how\nit integrates visual and textual information to provide constructive\nfeedback.\nGPT-4o demonstrates strong performance in certain dimensions,\nas evidenced by high TMR and TS values inImagination, Realism,\nand Deformation. These high values indicate that the suggestions\ngenerated in these areas were closely aligned with art teachersâ€™\nexpectations, requiring minimal modifications. This suggests that\nGPT-4o effectively captures and reasons about the key elements of\nthese artistic dimensions. However, the model faces challenges in\nmore complex dimensions such as Transformation, Line Combi-\nnation, and Line Texture. The lower TMR and TS values in these\nareas indicate that the suggestions often required significant revi-\nsions, both in terms of content length and semantics. This suggests\nthat GPT-4oâ€™s multimodal reasoning capabilities may need further\nrefinement to better understand and articulate the nuances of these\ndimensions. While GPT-4o shows a strong ability to holistically\nunderstand artwork, as seen in its review generation performance,\nthe challenge lies in translating this understanding into detailed,\nactionable suggestions. The discrepancy between review and sug-\ngestion generation effectiveness highlights the need for the model\nto not only comprehend the artwork but also to provide precise\nand contextually relevant advice. The iterative feedback loop with\nart teachers plays a crucial role in enhancing GPT-4oâ€™s suggestion\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\ngeneration capabilities. By receiving detailed feedback on its sug-\ngestions, the model can learn to better align its outputs with human\nexpectations. Continued engagement in this feedback loop is es-\nsential for refining the modelâ€™s multimodal reasoning abilities. To\naddress the challenges identified, future work could explore ad-\nvanced multimodal reasoning techniques. For instance, integrating\nmore sophisticated visual analysis tools could help the model bet-\nter understand complex visual elements. Additionally, leveraging\ntechniques such as attention mechanisms and fine-grained feature\nextraction could enhance the modelâ€™s ability to provide detailed\nand accurate suggestions.\nIn summary, GPT-4oâ€™s multimodal reasoning capability in sug-\ngestion generation shows promise, with strong performance in\ncertain dimensions. However, there is room for improvement, par-\nticularly in handling more complex artistic dimensions. Ongoing\nrefinements, iterative feedback, and technological advancements\nwill be key to enhancing the modelâ€™s ability to generate contextu-\nally appropriate and actionable suggestions, thereby making it an\neven more valuable tool in the artistic domain.\n8 Conclusion\nIn our work, we delve into the fascinating question of whether an\nentity at the intersection of the arts, education, and AI/MLLMs can\nemerge as an independent assistant within the teacher-student-\nmachine triadic dialogue system for artwork evaluation. To explore\nthis, we have adopted a Human-Computer Interaction (HCI) space\ndesign and analysis approach. We have developed the ArtMen-\ntor space, which comprises four core components:a. Multi-Agent\nData Collection System , b. HCI Dataset , c. Data Analysis Sys-\ntem, and d. Iterative Upgrades System . The HCI dataset encom-\npasses 380 sessions across nine dimensions of artwork evaluation,\nutilizing process-based data to mitigate the inherent manipulation\nrisks associated with outcome-based data. Within the Data Analysis\nSystem, we have applied machine learning and natural language\nprocessing techniques to imbue the process data with meaning, ex-\ntracting metrics that objectively reflect MLLM performance while\nalso ensuring interpretability.\nOur comprehensive exploration reveals that an entity at the in-\ntersection of the arts, education, and AI/MLLMs can indeed emerge\nas an independent assistant (GPT-4o) within the teacher-student-\nmachine triadic dialogue system for the evaluation of artworks.\nDespite certain limitations, such as a tendency to overlook broader\nentities and an over-granulation effect in entity recognition, GPT-4o\nhas demonstrated remarkable capabilities in multimodal percep-\ntion, recognition, understanding, and reasoning. GPT-4oâ€™s high\nprecision, strong F1-score, and robust art style sensitivity in entity\nrecognition underscore its effectiveness in analyzing artwork. Its\nability to adapt and converge with human assessments over time,\nas evidenced in review generation, highlights its potential for con-\ntinuous improvement. Furthermore, the modelâ€™s performance in\ngenerating contextually appropriate and stylistically precise con-\ntent demonstrates its holistic understanding of artwork. While there\nare areas for further refinement, particularly in handling more com-\nplex artistic dimensions and reducing the over-granulation effect,\nthe overall performance of GPT-4o is impressive. Its adaptability\nacross different art styles and its conservative approach to entity\nrecognition, with a near absence of false positives, are significant\nstrengths. In suggestion generation, GPT-4o shows promise, with\nstrong performance in certain dimensions. However, the challenge\nlies in translating its holistic understanding into detailed, actionable\nsuggestions.\nIn summary, GPT-4o holds great potential as a valuable tool in\nart evaluation within the teacher-student-machine triadic dialogue\nsystem. Ongoing refinements, iterative feedback, and technological\nadvancements will be key to unlocking even greater capabilities in\nthe future, thereby further solidifying its role as an independent\nassistant in the artistic domain.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] John R Anderson, Albert T Corbett, Kenneth R Koedinger, and Ray Pelletier. 1995.\nCognitive tutors: Lessons learned. The journal of the learning sciences 4, 2 (1995),\n167â€“207.\n[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong\nZhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev,\nSimon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig\nSchmidt. 2023. OpenFlamingo: An Open-Source Framework for Training Large\nAutoregressive Vision-Language Models. arXiv preprint arXiv:2308.01390 (2023).\n[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang\nLin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language\nmodel for understanding, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966 3, 1 (2023).\n[5] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin,\nXinggang Wang, Chang Zhou, and Jingren Zhou. 2023. Touchstone: Evaluating\nvision-language models by language models. arXiv preprint arXiv:2308.16890\n(2023).\n[6] Siwar Bengamra, Olfa Mzoughi, AndrÃ© Bigand, and Ezzeddine Zagrouba. 2024. A\ncomprehensive survey on object detection in Visual Art: taxonomy and challenge.\nMultimedia Tools and Applications 83, 5 (2024), 14637â€“14670.\n[7] Henri Bergson. 1911. Essai sur les donnÃ©es immÃ©diates de la conscience . F. Alcan.\n[8] Anjanava Biswas and Wrick Talukdar. 2024. Robustness of Structured Data\nExtraction from In-Plane Rotated Documents Using Multi-Modal Large Language\nModels (LLM). Journal of Artificial Intelligence Research (2024).\n[9] Moinak Biswas. 2021. Realism. BioScope: South Asian Screen Studies 12, 1-2 (2021),\n158â€“161.\n[10] Ann E Blandford, Philip J Barnard, and Michael D Harrison. 1995. Using Interac-\ntion Framework to guide the design of interactive systems. International journal\nof human-computer studies 43, 1 (1995), 101â€“130.\n[11] Eva Cetinic and James She. 2022. Understanding and creating art with AI: Review\nand outlook. ACM Transactions on Multimedia Computing, Communications, and\nApplications (TOMM) 18, 2 (2022), 1â€“22.\n[12] Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, and Yin\nXie. 2024. Plug-and-play grounding of reasoning in multimodal large language\nmodels. arXiv preprint arXiv:2403.19322 (2024).\n[13] Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tian-\nbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, et al. 2024. GMAI-MMBench:\nA Comprehensive Multimodal Evaluation Benchmark Towards General Medical\nAI. arXiv preprint arXiv:2408.03361 (2024).\n[14] Shih-Yeh Chen, Pei-Hsuan Lin, and Wei-Che Chien. 2022. Childrenâ€™s digital art\nability training system based on ai-assisted learning: a case study of drawing\ncolor perception. Frontiers in psychology 13 (2022), 823078.\n[15] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Pi-\notr DollÃ¡r, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection\nand evaluation server. arXiv preprint arXiv:1504.00325 (2015).\n[16] David H Cropley and Rebecca L Marrone. 2022. Automated scoring of figural cre-\nativity using a convolutional neural network. Psychology of Aesthetics, Creativity,\nand the Arts (2022).\n[17] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser,\nand Matthias NieÃŸner. 2017. Scannet: Richly-annotated 3d reconstructions of\nindoor scenes. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition. 5828â€“5839.\n[18] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP:\nTowards General-purpose Vision-Language Models with Instruction Tuning.\narXiv:2305.06500 [cs.CV]\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\n[19] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli. 2020. Image quality\nassessment: Unifying structure and texture similarity.IEEE transactions on pattern\nanalysis and machine intelligence 44, 5 (2020), 2567â€“2581.\n[20] Yingbei Du. 2020. Research on the transformation and innovation of visual\nart design form based on digital fusion technology. Applied Mathematics and\nNonlinear Sciences (2020).\n[21] David W Eccles and GÃ¼ler Arsal. 2017. The think aloud method: what is it and\nhow do I use it? Qualitative Research in Sport, Exercise and Health 9, 4 (2017),\n514â€“531.\n[22] K Anders Ericsson. 2017. Protocol analysis. A companion to cognitive science\n(2017), 425â€“432.\n[23] Dejan Grba. 2022. Deep else: A critical framework for ai art. Digital 2, 1 (2022),\n1â€“32.\n[24] Jiaxing Huang and Jingyi Zhang. 2024. A Survey on Evaluation of Multimodal\nLarge Language Models. arXiv preprint arXiv:2408.15769 (2024).\n[25] Olga M Hubard. 2010. Three modes of dialogue about works of art. Art Education\n63, 3 (2010), 40â€“45.\n[26] Christian P Janssen and Duncan P Brumby. 2015. Strategic adaptation to task\ncharacteristics, incentives, and individual differences in dual-tasking. PloS one\n10, 7 (2015), e0130009.\n[27] Jing Jin and Runzhou Li. 2024. Implications, Concerns, and Transcendence of\nAI-Based Art Evaluation: Focusing on \"Elementary School Art\". Curriculum,\nTeaching Material, and Method 44, 05 (2024), 138â€“143. https://doi.org/10.19877/j.\ncnki.kcjcjf.2024.05.020\n[28] Bonnie E John and David E Kieras. 1996. Using GOMS for user interface de-\nsign and evaluation: Which technique? ACM Transactions on Computer-Human\nInteraction (TOCHI) 3, 4 (1996), 287â€“319.\n[29] David Kadish, Sebastian Risi, and Anders Sundnes LÃ¸vlie. 2021. Improving\nobject detection in art images using only style transfer. In2021 international joint\nconference on neural networks (IJCNN) . IEEE, 1â€“8.\n[30] Margaret Kelaher, Naomi Berman, David Dunt, Victoria Johnson, Steve Curry,\nand Lindy Joubert. 2014. Evaluating community outcomes of participation in\ncommunity arts: A case for civic dialogue. Journal of Sociology 50, 2 (2014),\n132â€“149.\n[31] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. 2019.\nAudiocaps: Generating captions for audios in the wild. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) .\n119â€“132.\n[32] Maurice Lamb, Rachel W Kallen, Steven J Harrison, Mario Di Bernardo, Ali Minai,\nand Michael J Richardson. 2017. To pass or not to pass: Modeling the movement\nand affordance dynamics of a pick and place task.Frontiers in psychology 8 (2017),\n1061.\n[33] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. 2018.\nA dataset of clinically generated visual questions and answers about radiology\nimages. Scientific data 5, 1 (2018), 1â€“10.\n[34] Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum,\nVipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David\nZhou, Emad A Alghamdi, et al. 2024. A Design Space for Intelligent and Interactive\nWriting Assistants. In Proceedings of the CHI Conference on Human Factors in\nComputing Systems . 1â€“35.\n[35] Mina Lee, Percy Liang, and Qian Yang. 2022. Coauthor: Designing a human-\nai collaborative writing dataset for exploring language model capabilities. In\nProceedings of the 2022 CHI conference on human factors in computing systems .\n1â€“19.\n[36] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang,\nand Ying Shan. 2023. SEED-Bench: Benchmarking Multimodal Large Language\nModels. (2023). arXiv:2307.16125 [cs.CV]\n[37] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang,\nand Ying Shan. 2024. SEED-Bench: Benchmarking Multimodal Large Language\nModels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 13299â€“13308.\n[38] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei\nYang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-med:\nTraining a large language-and-vision assistant for biomedicine in one day. arXiv\npreprint arXiv:2306.00890 (2023).\n[39] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan\nHuang, Shanghang Zhang, and Hongsheng Li. 2024. Draw-and-Understand:\nLeveraging Visual Prompts to Enable MLLMs to Comprehend What You Want.\narXiv preprint arXiv:2403.20271 (2024).\n[40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruc-\ntion tuning. arXiv preprint arXiv:2304.08485 (2023).\n[41] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo\nZhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023. MMBench: Is\nYour Multi-modal Model an All-around Player? arXiv preprint arXiv:2307.06281\n(2023).\n[42] Ziqiang Liu, Feiteng Fang, Xi Feng, Xinrun Du, Chenhao Zhang, Zekun Wang,\nYuelin Bai, Qixuan Zhao, Liyang Fan, Chengguang Gan, et al. 2024. II-Bench: An\nImage Implication Understanding Benchmark for Multimodal Large Language\nModels. arXiv preprint arXiv:2406.05862 (2024).\n[43] Paul J Locher, Pieter Jan Stappers, and Kees Overbeeke. 1999. An empirical eval-\nuation of the visual rightness theory of pictorial composition. Acta psychologica\n103, 3 (1999), 261â€“280.\n[44] Peng Lu, Zhijie Kuang, Xujun Peng, and Ruifan Li. 2015. Discovering harmony: A\nhierarchical colour harmony model for aesthetics assessment. InComputer Visionâ€“\nACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore,\nNovember 1-5, 2014, Revised Selected Papers, Part III 12 . Springer, 452â€“467.\n[45] Silverio MartÃ­nez-FernÃ¡ndez, Xavier Franch, Andreas Jedlitschka, Marc Oriol, and\nAdam Trendowicz. 2021. Developing and operating artificial intelligence models\nin trustworthy autonomous systems. In International Conference on Research\nChallenges in Information Science . Springer, 221â€“229.\n[46] Heramb Nemlekar, Jignesh Modi, Satyandra K Gupta, and Stefanos Nikolaidis.\n2021. Two-stage clustering of human preferences for action prediction in as-\nsembly tasks. In 2021 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 3487â€“3494.\n[47] John D Patterson, Baptiste Barbot, James Lloyd-Cox, and Roger E Beaty. 2024.\nAuDrA: An automated drawing assessment platform for evaluating creativity.\nBehavior Research Methods 56, 4 (2024), 3619â€“3636.\n[48] Mykola Pechenizkiy, Nikola Trcka, Ekaterina Vasilyeva, Wil MP van der Aalst,\nand PME De Bra. 2009. Process mining online assessment data. In Educational\nData Mining 2009: 2nd International Conference on Educational Data Mining:\nproceedings [EDMâ€™09], Cordoba, Spain. July 1-3, 2009 . International Working Group\non Educational Data Mining, 279â€“288.\n[49] Oksana Pylypchuk, Andrii Polubok, Olga Krivenko, Olena Safronova, Danylo\nKosenko, and Nataliia Avdieieva. 2021. Developing an Approach to Colour\nAssessment of Works of Art on Aim to Creating a Comfortable and Harmo-\nnious Interior. In 2021 International Conference on Social Sciences and Big Data\nApplication (ICSSBDA 2021) . Atlantis Press, 181â€“187.\n[50] Han Qiao, Vivian Liu, and Lydia Chilton. 2022. Initial images: using image\nprompts to improve subject representation in multimodal ai generated art. In\nProceedings of the 14th Conference on Creativity and Cognition . 15â€“28.\n[51] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy\nLillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat,\nJulian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024).\n[52] J Rowe, L Hughes, D Eckstein, and AM Owen. 2008. Rule-selection and action-\nselection have a shared neuroanatomical basis in the human prefrontal and\nparietal cortex. Cerebral cortex 18, 10 (2008), 2275â€“2285.\n[53] Michelle J Searle and Lyn M Shulha. 2016. Capturing the imagination: Arts-\ninformed inquiry as a method in program evaluation. Canadian Journal of\nProgram Evaluation 31, 1 (2016), 34â€“60.\n[54] Woosuk Seo, Joonyoung Jun, Minki Chun, Hyeonhak Jeong, Sungmin Na,\nWoohyun Cho, Saeri Kim, and Hyunggu Jung. 2022. Toward an AI-assisted\nAssessment Tool to Support Online Art Therapy Practices: A Pilot Study.. In\nECSCW.\n[55] S Sfarra, C Ibarra-Castanedo, D Ambrosini, D Paoletti, A Bendada, and X\nMaldague. 2014. Discovering the defects in paintings using non-destructive\ntesting (NDT) techniques and passing through measurements of deformation.\nJournal of Nondestructive Evaluation 33 (2014), 358â€“383.\n[56] Ping Shi, Peter B Luh, and David L Kleinman. 1992. The Impact of Individual and\nTeam Goals on Human Distributed Dynamic Decisionmaking: A Mathematical\nModel. In 1992 American Control Conference . IEEE, 1976â€“1977.\n[57] Heng-Jie Song, Zhi-Qi Shen, Chun-Yan Miao, Ah-Hwee Tan, and Guo-Peng Zhao.\n2007. The multi-agent data collection in HLA-based simulation system. In 21st\nInternational Workshop on Principles of Advanced and Distributed Simulation\n(PADSâ€™07). IEEE, 61â€“69.\n[58] Yashar Talebirad and Amirhossein Nadiri. 2023. Multi-agent collaboration: Har-\nnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314\n(2023).\n[59] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 (2023).\n[60] Nikola Trcka and Mykola Pechenizkiy. 2009. From local patterns to global models:\nTowards domain driven educational process mining. In 2009 Ninth international\nconference on intelligent systems design and applications . IEEE, 1114â€“1119.\n[61] Mehrnoosh Vahdat, Luca Oneto, Davide Anguita, Mathias Funk, and Matthias\nRauterberg. 2015. A learning analytics approach to correlate the academic achieve-\nments of students with interaction data from an educational simulator. In Design\nfor Teaching and Learning in a Networked World: 10th European Conference on\nTechnology Enhanced Learning, EC-TEL 2015, Toledo, Spain, September 15-18, 2015,\nProceedings 10 . Springer, 352â€“366.\n[62] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong\nChen. 2023. A survey on multimodal large language models. arXiv preprint\narXiv:2306.13549 (2023).\nArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities CHI â€™25, April 26-May 1, 2025, Yokohama, Japan\n[63] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu,\nXinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal\nmodels for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023).\n[64] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang,\nand Jianke Zhu. 2024. Osprey: Pixel Understanding with Visual Instruction\nTuning. arXiv:2312.10032 [cs.CV] https://arxiv.org/abs/2312.10032\n[65] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and\nDong Yu. 2024. Mm-llms: Recent advances in multimodal large language models.\narXiv preprint arXiv:2401.13601 (2024).\n[66] Jun Zhang. 2020. Two Systems of Chinese Life Aesthetics . Peopleâ€™s Publishing\nHouse, Beijing. 010â€“011 pages.\n[67] Jiajing Zhang, Yongwei Miao, and Jinhui Yu. 2021. A comprehensive survey on\ncomputational aesthetic evaluation of visual art images: Metrics and challenges.\nIEEE Access 9 (2021), 77164â€“77187.\n[68] Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing.\n2023. M3exam: A multilingual, multimodal, multilevel benchmark for examining\nlarge language models. Advances in Neural Information Processing Systems 36\n(2023), 5484â€“5505.\n[69] Liang Zhao, Eslam Hussam, Jin-Taek Seong, Assem Elshenawy, Mustafa Kamal,\nand Etaf Alshawarbeh. 2024. Revolutionizing art education: Integrating AI and\nmultimedia for enhanced appreciation teaching. Alexandria Engineering Journal\n93 (2024), 33â€“43.\n[70] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong\nLee, Xiaopeng Lu, and Jianwei Yin. 2023. VL-CheckList: Evaluating Pre-\ntrained Vision-Language Models with Objects, Attributes and Relations.\narXiv:2207.00221 [cs.CV] https://arxiv.org/abs/2207.00221\nA SYSTEM SETTINGS\nA.1 Server\nOur system is built using Python and Flask and is deployed locally.\nThe server specifications are as follows:\nâ€¢Processor: Intel Core i7-11800H\nâ€¢Memory: 16GB DDR4 RAM\nâ€¢Graphics: NVIDIA GeForce RTX 3060\nâ€¢Storage: 512GB NVMe SSD\nâ€¢Operating System: Windows 10\nA.2 Decoding Parameters\nWe used the following decoding parameters for configuring the\ninference process of the GPT-4o model:\nâ€¢Engine: GPT-4o\nâ€¢Response Length (Word Piece) :\nâ€“ Entity Agent: 100\nâ€“ Review and Suggestion Agents: 500\nâ€¢Temperature: 0 (This parameter controls the randomness\nof the generated text, with 0 indicating deterministic gener-\nation.)\nâ€¢Top P: 1 (This value controls the diversity of candidate words\nduring generation.)\nA.3 Prompt Configuration\nWhen generating outputs for the Entity Agent and Review Agent,\nwe utilized specially designed prompts to guide the model. Below\nare the details of the prompts for each agent:\nA.3.1 Entity Agent Prompt. The following prompt was used for\nthe Entity Agent to identify and list objects or features in an image:\nPrompt:\nIdentify and list the objects or features present in\nthe image using descriptive labels. Use simple, clear\nterms like â€™Faceâ€™, â€™Black hairâ€™, â€™Thick lipsâ€™, â€™Big earsâ€™,\netc. Ensure that each label is descriptive and that labels\nare separated by the symbol (â€™;â€™). For example: Face;\nBlack hair; Thick lips; Big ears;. Also, identify the art\nstyle of the image with a label starting with â€™## Style:â€™.\nA.3.2 Review Agent Prompts. The Review Agent generates eval-\nuations based on nine distinct assessment dimensions, with each\ndimension corresponding to a specific prompt. Below a example of\nprompts for selected dimensions:\nTable 1: Assessment Criteria for Realistic Artwork\nCriterion: Realistic. This criterion assesses the accuracy of\nproportions, textures, lighting, and perspective to create a\nlifelike depiction.\n5: The artwork exhibits exceptional detail and precision in\ndepicting realistic features. Textures and lighting are used\nmasterfully to mimic real-life appearances with accurate\nproportions and perspective. The representation is strik-\ningly lifelike, demonstrating advanced skills in realism.\n4: The artwork presents a high level of detail and accuracy\nin the portrayal of subjects. Proportions and textures\nare very well executed, and the lighting enhances the\nrealism. Although highly realistic, minor discrepancies\nin perspective or detail might be noticeable.\n3: The artwork represents subjects with a moderate level of\nrealism. Basic proportions are correct, and some textures\nand lighting effects are used to enhance realism. However,\nthe depiction may lack depth or detail in certain areas.\n2: The artwork attempts realism but struggles with accurate\nproportions and detailed textures. Lighting and perspec-\ntive may be inconsistently applied, resulting in a less\nconvincing depiction.\n1: The artwork shows minimal attention to realistic details.\nProportions, textures, and lighting are poorly executed,\nmaking the depiction far from lifelike.\nA.3.3 Suggestion Agent Prompt. The following table outlines the\nlogic used in the Python function that dynamically generates prompts\nfor the Suggestion Agent. This function is designed to provide\nimprovement suggestions for each dimension based on the user-\nprovided data: This function is used to dynamically generate prompts\nfor the Suggestion Agent, providing suggestions for improvements\nin various dimensions.\nFor a full list of prompts and detailed information, please refer\nto the following GitHub repository: https://github.com/ArtMentor/\nArtMentorApp/blob/main/ArtMentor_app.py\nB INTERVIEW QUESTIONNAIRE\nGreeting\nDear Participant,\nThank you for taking part in this interview. Your valuable in-\nsights will help us further optimize the ArtMentor system, enhanc-\ning its role in the field of art education. We are eager to learn about\nCHI â€™25, April 26-May 1, 2025, Yokohama, Japan Zheng et al.\nTable 2: Prompt Generation Logic for Suggestion Agent\nStep Logic Description\nEntity Extrac-\ntion\nExtracts entities from the â€˜la-\nbels_data[\"original\"]â€˜ list and updates\nthem by removing entities listed in â€˜la-\nbels_data[\"removed\"]â€˜. Newly added entities\nfrom â€˜labels_data[\"added\"]â€˜ are inserted into\nthe updated list. The result is concatenated\ninto a string of entity labels separated by\nsemicolons.\nScore Review Generates a prompt based on the score\nand review submitted by the user.\nThe current score is extracted from\nâ€˜score_Review_data[\"scores\"][\"current\"]â€˜,\nand the current review is extracted from\nâ€˜score_Review_data[\"Reviews\"][\"current\"]â€˜.\nThese are combined into a single prompt string.\nSuggestion In-\nclusion\nIncorporates the current suggestion pro-\nvided by the user into the prompt. The\nsuggestion is extracted from â€˜sugges-\ntion_data[\"suggestions\"][\"current\"]â€˜.\nFinal Prompt\nConstruction\nCombines all previous components into the fi-\nnal prompt. The dimension name (â€˜dimensionâ€˜)\nis included, and a message is added to inform\nthe Suggestion Agent to consider user feedback\nand focus on improving the specified dimension.\nThe final prompt instructs the model to output\na dictionary format for ease of processing.\nyour experiences and suggestions while using the system. Below\nare some questions we would like to understand from your per-\nspective. We appreciate you taking the time to provide thoughtful\nresponses.\nBasic Information\nTo better understand your background and how it might influence\nyour interaction with the ArtMentor system, we would appreciate\nif you could provide the following basic information:\n(1) What is your current role in the field of art education? (e.g.,\nTeacher, Art Mentor, School Administrator, etc.)\n(2) How many years of experience do you have in art education?\n(3) Have you used other educational technology tools before? If\nyes, please briefly describe your experience with them.\n(4) How often do you integrate technology into your teaching or\nmentoring activities? (e.g., Daily, Weekly, Occasionally)\n(5) What are your main goals when using the ArtMentor system?\n(e.g., Enhancing student creativity, Providing personalized feed-\nback, etc.)\nMain Interview Questions\n(1) What are your thoughts on the accuracy of the systemâ€™s entity\nrecognition in artworks? Do you believe the system met your\nexpectations in this regard?\n(2) Regarding the overall usability of the system, how would you\nrate your experience with the human-computer interaction? If\nthere are any areas for improvement, please provide detailed\nsuggestions.\n(3) Do you believe that the multimodal large language model ef-\nfectively assisted you in scoring, commenting, and generating\nsuggestions for the evaluation of artworks? Please briefly ex-\nplain your reasoning.\n(4) During your use of the system, which functional dimensions\ndid you find to be particularly well-executed, and which ones\ndo you think require improvement?\n(5) Do you consider the suggestions provided by the system to be\nappropriate for the artistic capabilities of elementary students?\nDo these suggestions have the potential to inspire creativity\nand enhance the studentsâ€™ artistic skills?\nClosing Remarks\nOnce again, thank you for your participation and support! Your\nfeedback will directly contribute to the ongoing improvement of the\nArtMentor system, helping us better meet the needs of our users. If\nyou have any additional suggestions or further comments, please\ndo not hesitate to contact us. We look forward to collaborating with\nyou again in the future. Wishing you continued success in your\nwork!",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8097258806228638
    },
    {
      "name": "Modular design",
      "score": 0.676834762096405
    },
    {
      "name": "Process (computing)",
      "score": 0.580080509185791
    },
    {
      "name": "Iterative and incremental development",
      "score": 0.5476679801940918
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.5250596404075623
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.4760425090789795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4577952027320862
    },
    {
      "name": "Perception",
      "score": 0.4551723599433899
    },
    {
      "name": "Space (punctuation)",
      "score": 0.45379602909088135
    },
    {
      "name": "Software engineering",
      "score": 0.3412291407585144
    },
    {
      "name": "Programming language",
      "score": 0.10814610123634338
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I55712492",
      "name": "Zhejiang University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I135237710",
      "name": "Zhejiang Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I148128674",
      "name": "University of Shanghai for Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 1
}