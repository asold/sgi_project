{
  "title": "Evaluation of large language models on mental health: from knowledge test to illness diagnosis",
  "url": "https://openalex.org/W4413005885",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2104772315",
      "name": "YiJun Xu",
      "affiliations": [
        "Shaoxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2164310366",
      "name": "Zhaoxi Fang",
      "affiliations": [
        "Shaoxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2156498384",
      "name": "Wei-Nan Lin",
      "affiliations": [
        "Shaoxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2116315720",
      "name": "Yue Jiang",
      "affiliations": [
        "Shaoxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2081511594",
      "name": "Wen Jin",
      "affiliations": [
        "Shaoxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2629414293",
      "name": "Prasanalakshmi Balaji",
      "affiliations": [
        "King Khalid University"
      ]
    },
    {
      "id": "https://openalex.org/A5108958879",
      "name": "Jiangda Wang",
      "affiliations": [
        "Shaoxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2101604561",
      "name": "Ting Xia",
      "affiliations": [
        "Shaoxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2104772315",
      "name": "YiJun Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164310366",
      "name": "Zhaoxi Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156498384",
      "name": "Wei-Nan Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116315720",
      "name": "Yue Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2081511594",
      "name": "Wen Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2629414293",
      "name": "Prasanalakshmi Balaji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5108958879",
      "name": "Jiangda Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101604561",
      "name": "Ting Xia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4405903187",
    "https://openalex.org/W4392822465",
    "https://openalex.org/W6891971393",
    "https://openalex.org/W4390722663",
    "https://openalex.org/W4387635147",
    "https://openalex.org/W4385245504",
    "https://openalex.org/W4386114394",
    "https://openalex.org/W4385373745",
    "https://openalex.org/W4389523980",
    "https://openalex.org/W4396758529",
    "https://openalex.org/W4392503764",
    "https://openalex.org/W4382776712",
    "https://openalex.org/W4366548761",
    "https://openalex.org/W4395009236",
    "https://openalex.org/W4367694325",
    "https://openalex.org/W4389519275",
    "https://openalex.org/W4393177920",
    "https://openalex.org/W4388733340",
    "https://openalex.org/W4388891105",
    "https://openalex.org/W4392605825",
    "https://openalex.org/W4361230825",
    "https://openalex.org/W4323570543",
    "https://openalex.org/W4396987255",
    "https://openalex.org/W4406665832",
    "https://openalex.org/W4403786592",
    "https://openalex.org/W4229930240",
    "https://openalex.org/W3202915159",
    "https://openalex.org/W4401413185",
    "https://openalex.org/W4402193006"
  ],
  "abstract": "Large language models (LLMs) have opened up new possibilities in the field of mental health, offering applications in areas such as mental health assessment, psychological counseling, and education. This study systematically evaluates 15 state-of-the-art LLMs, including DeepSeekR1/V3 (March 24, 2025), GPT-4.1 (April 15, 2025), Llama4 (April 5, 2025), and QwQ (March 6, 2025, developed by Alibaba), on two key tasks: mental health knowledge testing and mental illness diagnosis in the Chinese context. We use publicly available datasets, including Dreaddit, SDCNL, and questions from the CAS Counsellor Qualification Exam. Results indicate that DeepSeek-R1, QwQ, and GPT-4.1 outperform other models in both knowledge accuracy and diagnostic performance. Our findings highlight the strengths and limitations of current LLMs in Chinese mental health scenarios and provide clear guidance for selecting and improving models in this sensitive domain.",
  "full_text": "Evaluation of large language\nmodels on mental health:\nfrom knowledge test to\nillness diagnosis\nYijun Xu1, Zhaoxi Fang1,2, Weinan Lin1, Yue Jiang1, Wen Jin1,\nPrasanalakshmi Balaji3, Jiangda Wang1,2 and Ting Xia4*\n1Department of Computer Science and Engineering, Shaoxing University, Shaoxing, China,2Institute\nof Artiﬁcial Intelligence, Shaoxing University, Shaoxing, China,3Department of Computer Science,\nCollege of Computer Science, King Khalid University, Abha, Saudi Arabia,4School of Life and\nEnvironmental Sciences, Shaoxing University, Shaoxing, China\nLarge language models (LLMs) have opened up new possibilities in theﬁeld of\nmental health, offering applications in areas such as mental health assessment,\npsychological counseling, and education. This study systematically evaluates 15\nstate-of-the-art LLMs, including DeepSeekR1/V3 (March 24, 2025), GPT-4.1\n(April 15, 2025), Llama4 (April 5, 2025), and QwQ (March 6, 2025, developed by\nAlibaba), on two key tasks: mental health knowledge testing and mental illness\ndiagnosis in the Chinese context. We use publicly available datasets, including\nDreaddit, SDCNL, and questions from the CAS Counsellor Qualiﬁcation Exam.\nResults indicate that DeepSeek-R1, QwQ, and GPT-4.1 outperform other models\nin both knowledge accuracy and diagnostic performance. Ourﬁndings highlight\nthe strengths and limitations of current LLMs in Chinese mental health scenarios\nand provide clear guidance for sele cting and improving models in this\nsensitive domain.\nKEYWORDS\nlarge language models, model evaluation, mental health, knowledge test,\nillness diagnosis\n1 Introduction\nIn recent years, large language models (LLMs) have made remarkable strides in theﬁeld\nof natural language processing, emerging as one of the most signiﬁcant breakthroughs in\nartiﬁcial intelligence (AI) research. Since the introduction of the GPT (Generative\nPretrained Transformer) model by OpenAI in 2018, LLMs have continued to evolve and\nexpand (1). For instance, GPT-3, with its 175 billion parameters, and GPT-4, which\nincorporates multimodal processing capabilities, are able to understand and generate more\ncomplex, nuanced, and natural language texts. Concurrently, other research institutions\nand companies have also developed their own LLMs, such as DeepSeek-R1 (2), Google’s\nGemma (3), and others. These models have demonstrated exceptional performance across\nFrontiers inPsychiatry frontiersin.org01\nOPEN ACCESS\nEDITED BY\nAamir Malik,\nBrno University of Technology, Czechia\nREVIEWED BY\nSoyiba Jawed,\nNational University of Sciences and\nTechnology (NUST), Pakistan\nZhe Liu,\nEast China University of Science and\nTechnology, China\nWaqas Rasheed,\nUniversity of California, Irvine, United States\n*CORRESPONDENCE\nTing Xia\nxt@usx.edu.cn\nRECEIVED 14 June 2025\nACCEPTED 14 July 2025\nPUBLISHED 06 August 2025\nCITATION\nXu Y,Fang Z,Lin W,Jiang Y,Jin W,Balaji P,\nWang J andXia T (2025) Evaluation of large\nlanguage models on mental health: from\nknowledge test to illness diagnosis.\nFront. Psychiatry 16:1646974.\ndoi: 10.3389/fpsyt.2025.1646974\nCOPYRIGHT\n©2 0 2 5X u ,F a n g ,L i n ,J i a n g ,J i n ,B a l a j i ,W a n g\nand Xia This is an open-access article\ndistributed under the terms of theCreative\nCommons Attribution License (CC BY).The\nuse, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Data Report\nPUBLISHED 06 August 2025\nDOI 10.3389/fpsyt.2025.1646974\na wide range of natural language processing tasks, including\nlanguage understanding, text generation, and machine translation.\nGiven their ability to understand and produce human-like,\nﬂuent text, LLMs show signiﬁcant promise in the mental health\nﬁeld, with potential applications in mental health assessment,\npsychological counseling, and education ( 4, 5). For example,\nLLMs can be used to develop emotionally supportive chatbots\nthat offer instant emotional support and companionship to users\n(6–8). By analyzing users’language or social media content, LLMs\ncan detect emotional states and potential psychological issues,\nproviding professionals with insights and even offering treatment\nsuggestions and interventions (9–12). In the realm of mental health\neducation, LLMs can generate tailored educational content to\npromote mental health awareness and knowledge (13, 14). These\napplications not only enhance the accessibility and efﬁciency of\nmental health services but also introduce innovative tools and\nmethodologies for both research and practical applications in\nthe ﬁeld.\nIn addition to general-purpose LLMs, several domain-speciﬁc\nmodels have been developed for mental health tasks, include ZhiXin\n(15), MeChat (16), SoulChat (17), and MindChat (18). For instance,\nZhiXin (15), a Chinese-language mental health model, isﬁne-tuned\nusing psychiatric datasets and clinical guidelines. It outperforms\nstate-of-the-art models in me ntal disorder diagnosis while\nmaintaining safety, usability, and human-like responses. SoulChat\nChen et al. (17), developed by the School of Future Technology at\nSouth China University of Technology and the Guangdong\nProvincial Key Laboratory of Digital Twin, is a large-scale\npsychological dialogue model. Through theﬁne-tuning of mixed\nlong-text counseling instructions and multi-turn empathetic\ndialogue data, SoulChat signiﬁcantly enhances the model’s ability\nto express empathy in psychological counseling scenarios.\nWhile LLMs show great promise in mental health support,\npsychological assessment, an d therapeutic as sistance, the\nprofessional competence and potential risks associated with these\nmodels remain unclear. Therefore, it is essential to conduct\nthorough performance evaluations (19–22). Currently, there is a\ngrowing body of research focused on assessing the applicability of\nLLMs in the mental healthﬁeld (23–27). A seminal study in this\narea was conducted by (23), which evaluated the zero-shot binary\nclassiﬁcation capabilities of GPT-3.5-Turbo in detecting stress,\ndepression, and suicidality severity in social media text. Building\non this work, the authors in (24) expanded the evaluation by\nexamining GPT-3.5-Turbo’s performance in a broader range of\naffective computing tasks, including Big Five personality trait\nprediction, sentiment analysis, and suicide risk detection. In (25),\nthe authors assessed the ability of LLMs to respond to a set of 18\npsychological prompts to evaluate their potential applicability in\nmental health care. However, their study was limited to only two\nmodels: GPT-4 and ChatGPT.\nNote that most of the studies mentioned above have used\noutdated models like GPT-3.5 and Llama 2 (24, 26). In contrast,\nrecent months have seen the release of several highperformance\nLLMs, such as DeepSeek-R1, which in certain benchmark tests—\nsuch as mathematical reasoning and code generation— can match\nor even surpass the performance of GPT-4.0. Additionally, most of\nthe studies focused on a single aspect when assessing the\napplicability of LLMs in mental health, such as evaluating the\nmodel’s conversational abilities or text generation quality. Some\nstudies even employed manual evaluation methods (24, 25), which\ntested a limited number of models and tasks, and are inherently\nsusceptible to subjective biases.\nThis study aims to evaluate the performance of state-of-the-art\nLLMs in Chinese mental health scenarios, focusing on two critical\ntasks: mental health knowledge assessment and mental illness\ndiagnostic support. Our primary research questions are: (1) How\ndo the latest LLMs compare in terms of their knowledge and\ndiagnostic capabilities? (2) What factors in ﬂuence their\nperformance, beyond parameter size? We hypothesize that while\nlarger models may generally perform better, architectural\ninnovations andﬁne-tuning strategies could also play a signiﬁcant\nrole. This study addresses a gap in the literature by providing a\ncomprehensive evaluation of the latest models, including DeepSeek-\nR1/V3 (March 24, 2025), GPT-4.1 (April 15, 2025), Llama4 (April\n5, 2025), and QwQ (March 6, 2025, developed by Alibaba), which\nhave not been extensively tested in this domain. Initially, we\ncollaborate with professionals in the mental healthﬁeld to curate\nrepresentative test data, which includes public datasets from the\nweb and professional literature datasets we use Chinese translations\nof public social media datasets obtained through web scraping.\nAutomated tests were then conducted on the selected models using\nwell-designed prompts. This det ailed and holistic evaluation\nprovides a more realistic and nuanced view of LLM performance\nin the complex, dynamicﬁeld of mental health applications, offering\nvaluable insights for future model optimization and improvement.\n2 Methods\n2.1 Assessment tasks\nThis study systematically evaluates the performance of state-of-\nthe-art LLMs, including DeepSeek-R1, GPT-4.1, and QwQ, across\ntwo critical tasks in the mental health domain: mental health\nknowledge assessment and mental illness diagnosis.\n2.1.1 Mental health knowledge test\nThe mental health knowledge test is designed to assess the\nLLM’s mastery and comprehension of mental health concepts. A\ncomprehensive set of questions was gathered from the Chinese\nAcademy of Sciences (CAS) Counsellor Qualiﬁcation Exam (28),\nincluding both single-choice and multiple-choice questions. By\nanalyzing the model’s responses, we can evaluate the depth and\nbreadth of its knowledge base across various areas, such as\npersonality psychology, soci al psychology, developmental\npsychology, and mental disorders. Furthermore, the model ’s\nunderstanding of treatment methods — along with their\napplicable scenarios and principles— is evaluated. The results of\nXu et al. 10.3389/fpsyt.2025.1646974\nFrontiers inPsychiatry frontiersin.org02\nthis test will reveal whether the LLMs can provide accurate and\nreliable mental health knowled ge, which is critical for its\napplication in mental health edu cation, awareness campaigns,\nand counseling services.\n2.1.2 Mental illness diagnostic test\nThis test evaluates the model’s ability to assist in diagnosing\nmental health conditions. We utilized publicly available social\nmedia datasets, including Dreaddit (29) and SDCNL (30), which\nfeature posts related to mental health. The model analyzes the\ninformation in these posts and provides possible diagnostic results\nand recommendations for psychological disorders. The model’s\nperformance is evaluated by comparing its diagnostic results to\nstandard clinical diagnoses, calculating indicators such as accuracy\nand recall. This test is essential for evaluating the effectiveness of\nLLMs in clinical settings and offers valuable insights into their\npotential role in mental health diagnosis.\n2.2 Datasets\nTo assess the application capabilities of LLMs in theﬁeld of\nmental health, we utilized a combination of publicly available high-\nquality datasets and custom-c onstructed dataset, which was\ncollected using web crawlers. These strategies were implemented\nto ensure the reliability and validity of the evaluation outcomes.\n2.2.1 CAS counselor qualiﬁcation exam dataset\nThe CAS Counselor Qualiﬁcation Exam Dataset is derived from\npublicly available exam topics for the 2023–2024 period (28). It\ncovers both theoretical and operational aspects of mental health.\nThe theoretical topics include introduction to psychology,\npersonality and social psycholo gy, developmental psychology,\nmental health and mental dis orders, and introduction to\ncounseling. The operational skills section encompasses counseling\ntheory, psychological assessment, basic counseling skills, and\ncounseling methods. The dataset includes 744 single-choice and\n200 multiple-choice questions.\n2.2.2 Dreaddit\nDreaddit (29) is a publicly available dataset designed for social\nmedia stress analysis. It consists of raw data from the Reddit\ncommunity, spanning ﬁve forums (subreddits) focused on topics\nsuch as mental health, work stress, and related issues. The original\ndataset contains 190,000 posts, and all posts in this dataset had been\npre-labeled by Amazon Mechanical Turk workers as part of the\noriginal dataset construction ( 29). We do not perform any\nadditional annotation beyond using the provided labels.For this\nstudy, we selected a representative sample of 1,151 posts, including\n64 labeled with“stress”, 503 with“anxiety”, and 584 with“PTSD”.\n2.2.3 SDCNL\nSDCNL (30) is a dataset for the categorization of suicide and\ndepression-related content. The dataset was compiled through web\ncrawlers from two subreddits, r/SuicideWatch and r/Depression.\nThe SDCNL dataset comprises 1517 labeled posts, including 787\nannotated as “suicide” and 729 as “depression”, with balanced\nrepresentation ensured during evaluation. This dataset is essential\nfor evaluating the model’s ability to accurately classify content\nrelated to mental health crises.\n2.3 Evaluated LLMs\nTo ensure a comprehensive evaluation of LLMs across diverse\narchitectures and scales, this study incorporated a broad spectrum\nof models. The selected models included both state-of-the-art,\nlarge-scale models that excel in natural language processing tasks,\nas well as lightweight, resource-ef ﬁcient models designed for\ndeployment in constrained environments. In balancing cost and\nperformance, we chose representative model families, including the\nDeepSeek series, OpenAI’s GPT series, Google’s models, and Meta’s\nLlama series, as shown inTable 1.\n2.4 Prompt engineering and evaluation\nsetup\nAll models were tested via their respective APIs to ensure\nconsistency and access to the latest versions. No temperature or\nsystem prompt settings were adjusted, as we aimed to evaluate the\nmodels’ default performance. For each prompt, only the ﬁrst\nresponse generated by the model was considered. This approach\nwas chosen to maintain a standardized evaluation process across all\nmodels. For each dataset, the mental health-related content was\nstored in.xls or.csvﬁles, with each record represented as a string in a\nstructured format (e.g., Reddit post content, exam questions, or\nmetadata). These text strings were programmatically read into\nmemory using Python-based tools. Each entry was then formatted\ninto a task-speciﬁc prompt and sent to the LLMs via API. This\nprocess allowed us to automate the evaluation across multiple\ndatasets and ensure consistency across model inputs.\n3 Results\n3.1 Mental health knowledge test\nFirstly, we evaluate the performance of LLMs in the CAS\nQualiﬁcation Examination for Psychological Consultants. The\ndataset comprises 744 single-choice questions (SCQs) and 200\nmultiple-choice questions (MSQs). Table 2 shows the test results\nof various models. For single-choice questions, DeepSeek-R1 671B\nand DeepSeek-V3 perform best, with accuracy rates of 86.83% and\n84.68%. QwQ-32B, GLM-Z1, and GPT-4o also performed well, at\n84.27%, 78.90%, and 72.72%. Generally, multiple-choice question\naccuracy is lower than single-choice. QwQ-32B has the highest\nmultiple-choice accuracy at 64.0%, while DeepSeek-R1 and\nXu et al. 10.3389/fpsyt.2025.1646974\nFrontiers inPsychiatry frontiersin.org03\nDeepSeek-V3 has only 28.5% and 21%. In terms of overall\nperformance (see Figure 1), QwQ-32B and DeepSeek-R1 671B\nstand out with 79.98% and 74.47%.\n3.2 Mental illness diagnosis evaluation\nIn this task, we use Chinese translations of public social media\ndatasets like Dreaddit (29) and SDCNL (30). Models analyze user-\nposted information to give possible mental disorder diagnoses. We\nconduct two tests: one using Dreaddit to judge depression, and\nanother using SDCNL to assess suicide ideation. The test results are\ngiven inTable 3.\nIn the suicide ideation diagnosis, model accuracy differs greatly.\nGPT-4.1 has the highest accuracy at 69.53%, much higher than average,\nshowing strong risk identiﬁcation. DeepSeek-R1 671B also performed\nwell at 67.15%. Depression diagnosis performance varied more.\nLlama4-scout ranks ﬁrst with 76.98% accuracy, followed by\nGemma2- 27B (72.02%) and DeepSeek-V3 Pro (69.69%).\nComparisons show that medium-scale models (e.g., Gemma2-27B)\nwith certain optimization strategies outperformed some larger models.\nThis suggests that model architecture and training strategy\ncompatibility may be more effective than simply increasing\np a r a m e t e rc o u n t .W ea l s oa n a l y z ef a i l u r ec a s e si nt h ed i a g n o s t i ct a s k\nand ﬁnd that misclassiﬁcations often stemmed from symptom overlap\n(e.g., between anxiety and PTSD), ambiguous or metaphorical\nlanguage, and lack of clinical context. The LLMs also tend to default\nto high-frequency categories like“anxiety” when uncertain. These\nissues highlight the limitations of using single-turn, text-only inputs\nfor complex mental health assessments.\n4 Discussion\nThe evaluation results revealed signi ﬁcant performance\nvariations among models across tasks. In mental health\nTABLE 1 List of LLMs.\nNo. Model Parameters Release Date Company API Service\n1 DeepSeek-R1 671B January 20, 2025 DeepSeek Silicon Flow\n2 DeepSeek-v3(pro) 671B March 24, 2025 DeepSeek Silicon Flow\n3 DeepSeek-R1-1.5B 1.5B January 20, 2025 DeepSeek Silicon Flow\n4 DeepSeek-R1-32B 32B January 20, 2025 DeepSeek Silicon Flow\n5 GPT-4o 200B May 13, 2024 OpenAI OpenAI\n6 GPT 4.1 N/A April 15, 2025 OpenAI OpenAI\n7 GLM-4 9B 9B June 4, 2024 THUDM Silicon Flow\n8 GLM-4 32B 32B April 14, 2025 THUDM Silicon Flow\n9 GLM-Z1 32B 32B April 15, 2025 THUDM Silicon Flow\n10 Llama 3.3-70B 70B January 29, 2024 Meta Silicon Flow\n11 Llama4-scout 17B April 5, 2025 Meta LlmAPI\n12 Gemma-2-27b 27B June 28, 2024 Google Silicon Flow\n13 Gemma-3-27b 27B March 12, 2025 Google LlmAPI\n14 QwQ-32B 32B March 6, 2025 Alibaba Silicon Flow\n15 Qwen2.5-72B 72B September 18, 2024 Alibaba Silicon Flow\nTABLE 2 Results of mental health knowledge test.\nModels Single-\nChoice Questions\nMultiple-\nChoice Questions\nDeepSeek-\nR1 1.5B\n35.08% 14.50%\nDeepSeek-\nR1 32B\n73.39% 57.00%\nDeepSeek-\nR1 671B\n86.83% 28.50%\nDeepSeek-\nV3 Pro\n84.68% 21.00%\nGemma2-27B 61.02% 15.00%\nGemma3-27B 64.52% 39.00%\nGPT-4.1 73.52% 50.00%\nGPT-4o 72.72% 44.50%\nLlama4-scout 71.37% 38.00%\nLlama3.3-70B 69.35% 30.50%\nQwQ-32B 84.27% 64.00%\nQwen2.5-72B 81.45% 52.50%\nGLM-4 32B 76.75% 43.50%\nGLM-4 9B 64.38% 24.00%\nGLM-Z1 9B 78.90% 52.50%\nXu et al. 10.3389/fpsyt.2025.1646974\nFrontiers inPsychiatry frontiersin.org04\nknowledge tests, QwQ-32B achieved the highest accuracy of\n79.98%, with 84.27% and 64.00% for single-and multiple-choice\nquestions. In mental disorder diagnosis, GPT4.1 led in suicidal\ntendency diagnosis (69.53%), a nd llama4-scout in depression\ndiagnosis (76.98%). Typically, model scale and parameter count\npositively correlate with knowledge and pattern learning ability,\npotentially leading to better knowledge-testing performance. For\ninstance, DeepSeek-R1 1.5B’s smaller parameter count results in\nrelatively low accuracy in both question types. However, larger\nmodels like DeepSeek-R1 671B don’t have the highest multiple-\nchoice accuracy, indicating that model scale isn’t the only factor\ndetermining performance in all knowledge-based scenarios.\nMoreover, the low multiple-choice accuracy reveals that these\nLLMs still have signi ﬁcant limitations in handling complex\nmental health-related issues.\nBeyond parameter size, architectural innovations,ﬁne-tuning,\nand training datasets play crucial roles in determining model\nperformance. For instance, medium-scale models like Gemma2-\n27B may outperform larger models due to their optimized\narchitectures and training strategies. These factors enable models\nto better capture the nuances of mental health-related language and\nimprove diagnostic accuracy.\nOne limitation of our current approach is the reliance on social\nmedia text (e.g., Reddit) for mental illness diagnosis. While\ninformative, such data may not fully represent clinical\npresentation. Future work will incorporate electronic health\nrecords (EHR) and clinical notes to improve realism and\nrelevance. Collaborations with certi ﬁed psychologists and\npsychiatrists will also be pursued for expert validation.\nIt is also worth noting that using LLMs in psychiatry poses\nsigniﬁcant ethical risks. Hallucina tion (generation of false or\nmisleading information) can lead to incorrect diagnoses or harmful\nsuggestions. Additionally, LLMs may reﬂect biases from their training\ndata, reinforcing stereotypes or minimizing serious symptoms.\nPrivacy is another critical concern, especially when processing\npersonal or clinical data. We emphasize that LLM-generated\noutputs must be reviewed by qualiﬁed mental health professionals\nand accompanied by strong data governance practices.\nTABLE 3 Mental illness diagnosis test results.\nModel Suicidal Tendency\nDiagnosis\nDepression\nDiagnosis\nDeepSeek-\nR1 1.5B\n51.91% 44.66%\nDeepSeek-\nR1 32B\n56.07% 56.39%\nDeepSeek-\nR1 671B\n67.15% 61.86%\nDeepSeek-\nV3 Pro\n64.45% 69.59%\nGemma2-27B 57.52% 72.02%\nGemma3-27B 59.76% 64.47%\nGPT4.1 69.53% 61.51%\nGPT-4o 49.93% 58.73%\nLlama4-scout 61.41% 76.98%\nLlama3.3-70B 57.98% 63.60%\nQwQ-32B 58.44% 60.90%\nQwen2.5-72B 65.70% 59.51%\nGLM-4 32B 62.14% 58.12%\nGLM-4 9B 57.98% 56.65%\nGLM-Z1 9B 61.54% 63.60%\nFIGURE 1\nOverall accuracy of the knowledge test.\nXu et al. 10.3389/fpsyt.2025.1646974\nFrontiers inPsychiatry frontiersin.org05\n5 Conclusion\nThis study provides a comprehensive evaluation of 15 advanced\nLLMs in two key Chinese mental health tasks: knowledge\nassessment and illness diagnosis. Our results show that models\nlike DeepSeek, QwQ-32B and G PT-4.1 outperform others in\nspeciﬁc tasks, but signiﬁcant limitations remain, particularly in\nhandling complex, nuanced, or ambiguous cases. Model size\ngenerally correlates with per formance, but is not the sole\ndeterminant. Inconsistent accuracy on multiple-choice questions\nand misclassiﬁcations in diagnosis highlight the need for further\nimprovement. These ﬁndings underscore both the potential and\ncurrent limitations of LLMs in mental health applications. Future\nwork should incorporate clinical data, domainspeciﬁc ﬁne-tuning,\nand expert validation to build more reliable and ethically robust\nsystems for real-world use.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material. Further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nYX: Software, Data curation, Writing – original draft. ZF:\nWriting – review & editing, Supervision, Project administration.\nWL: Software, Data curation, Writing– original draft. YJ: Writing–\noriginal draft, Software, Data curation. WJ: Data curation, Software,\nWriting – original draft. PB: Writing – review & editing.\nJW: Software, Writing – original draft. TX: Funding acquisition,\nSupervision, Investigation, Writing– review & editing.\nFunding\nThe author(s) declare thatﬁnancial support was received for the\nresearch and/or publication of this article. This work was supported\nby the Zhejiang Provincial Basic Public Welfare Research Project\nunder grant no. LGF22F010006.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations,\nor those of the publisher, the editors and the reviewers. Any product\nthat may be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\n1. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. Gpt-4 technical\nreport. arXiv preprint arXiv:2303.08774. (2023). doi: 10.48550/arXiv.2303.08774\n2. Liu A, Feng B, Xue B, Wang B, Wu B, Lu C, et al. Deepseek-v3 technical report.\narXiv preprint arXiv:2412.19437. (2024). doi: 10.48550/arXiv.2412.19437\n3. Team G, Mesnard T, Hardin C, Dadashi R, Bhupatiraju S, Pathak S, et al. Gemma:\nOpen models based on gemini research and technology. arXiv preprint\narXiv:2403.08295. (2024). doi: 10.48550/arXiv.2403.08295\n4. Guo Z, Lai A, Thygesen JH, Farrington J, Keen T, Li K. Large language model for\nmental health: A systematic review. arXiv preprint arXiv:2403.15401 .( 2 0 2 4 ) .\ndoi: 10.48550/arXiv.2403.15401\n5. Hua Y, Liu F, Yang K, Li Z, Na H, Sheu YH, et al. Large language models in mental\nhealth care: A scoping review.arXiv preprint arXiv:2401.02984. (2024). doi: 10.48550/\narXiv.2401.02984\n6. Loh SB, Raamkumar AS. Harnessing large language models’empathetic response\ngeneration capabilities for online mental health counselling support.arXiv preprint\narXiv:2310.08017. (2023). doi: 10.48550/arXiv.2310.08017\n7. Lai TEA. Psy-llm: Scaling up global mental health psychological services with ai-\nbased large language models.arXiv preprint arXiv:2307.11991. (2023). doi: 10.48550/\narXiv.2307.11991\n8. Zheng Z, Liao L, Deng Y, Nie L. Building emotional support chatbots in the era of\nllms. arXiv preprint arXiv:2308.11584. (2023). doi: 10.48550/arXiv.2308.11584\n9. Xu X, Yao B, Dong Y, Yu H, Hendler JA, Dey AK, et al. Leveraging large language\nmodels for mental health prediction via online text data. arXiv preprint\narXiv:2309.08704. (2023). doi: 10.48550/arXiv.2307.14385\n10. Yang K, Ji S, Zhang T, Xie Q, Kuang Z, Ananiadou S. Towards interpretable\nmental health analysis with large language models.arXiv preprint arXiv:2304.03347.\n(2023). doi: 10.18653/v1/2023.emnlp-main.370\n11. Yang K, Zhang T, Kuang Z, Xie Q, Huang J, Ananiadou S. Mentallama:\nInterpretable mental health analysis on social media with large language models. In.\nProc ACM Web Conf 2024. (2024), 4489–500. doi: 10.1145/3589334\n12. Xu X, Yao B, Dong Y, Gabriel S, Yu H, Hendler J, et al. Mental-llm: Leveraging large\nlanguage models for mental health prediction via online text data.Proc ACM Interactive\nMobile Wearable Ubiquitous Technol.( 2 0 2 4 )8 : 1–32. doi: 10.1145/3643540\n13. Smith AEA. Old dog, new tricks? exploring the potential functionalities of\nchatgpt in supporting educational methods in social psychiatry.Int J Soc Psychiatry.\n(2023) 69:207640231178451. doi: 10.1177/00207640231178451\n14. Kumar HEA. Exploring the use of large language models for improving the\nawareness of mindfulness. In: Extended Abstracts of the 2023 CHI Conference on\nHuman Factors in Computing Systems. Association for Computing Machinery, New\nYork, NY, USA (2023). doi: 10.1145/3544549.3585614\n15. Zhao X, Gao Y, Zhang Y. Tuning llama model with mental disorders knowledge.\n(2024). doi: 10.21203/rs.3.rs-4250151/v1\n16. Qiu H, He S, Zhang A, Li Z, Lan Y. Smile: Single-turn to multi-turn inclusive\nlanguage expansion via chatgpt for mental health support. arXiv preprint\narXiv:2305.00450. (2023). doi: 10.48550/arXiv.2305.00450\n17. Chen Y, Xing X, Lin J, Zheng H, Wang Z, Liu Q, et al. Improving LLMs’Empathy,\nListening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations.\nFindings of the Association for Computational Linguistics: EMNLP 2023, 1170–1183. Association\nfor Computational Linguistics (ACL). (2023). doi: 10.18653/v1/2023.ﬁndings-emnlp.83\nXu et al. 10.3389/fpsyt.2025.1646974\nFrontiers inPsychiatry frontiersin.org06\n18. Dong X, Yan X. Mindchat: Psychological large language model(2023). Available\nonline at: https://github.com/X-D-Lab/MindChat. (Accessed January 20, 2025).\n19. Lawrence HR, Schneider RA, Rubin SB, Matarić MJ, McDuff DJ, Jones Bell M,\net al. The opportunities and risks of large language models in mental health.JMIR Ment\nHealth. (2024) 11:1–15. doi: 10.2196/59479\n20. De Choudhury M, Pendse SR, Kumar N. Beneﬁts and harms of large language models\ni nd i g i t a lm e n t a lh e a l t h .arXiv preprint arXiv:2311.14693. (2023). doi: 10.31234/osf.io/y8ax9\n21. Ji S, Zhang T, Yang K, Ananiadou S, Cambria E. Rethinking large language\nmodels in mental health applications. arXiv preprint arXiv:2311.11267 . (2023).\ndoi: 10.48550/arXiv.2311.11267\n22. Hadar-Shoval D, Asraf K, Mizrachi Y, Haber Y, Elyoseph Z. Assessing the\nalignment of large language models with human values for mental health integration:\nCross-sectional study using schwartz’s theory of basic values.JMIR Ment Health. (2024)\n11:1–15. doi: 10.2196/55988\n23. Lamichhane B. Evaluation of chatgpt for nlp-based mental health applications.\narXiv preprint arXiv:2303.15727. (2023). doi: 10.48550/arXiv.2303.15727\n24. Amin MM, Cambria E, Schuller BW. Will affective computing emerge from\nfoundation models and general ai? a ﬁrst evaluation on chatgpt. arXiv preprint\narXiv:2303.03186. (2023). doi: 10.48550/arXiv.2303.03186\n25. Moell B. Comparing the efﬁcacy of gpt-4 and chat-gpt in mental health care: A\nblind assessment of large language models for psychological support.arXiv preprint\narXiv:2405.09300. (2024). doi: 10.48550/arXiv.2405.09300\n26. Levkovich I. Evaluating diagnostic accuracy and treatment efﬁcacy in mental\nhealth: A comparative analysis of large language model tools and mental health\nprofessionals. Eur J Invest Health Psychol Educ . (2025) 15:9. doi: 10.3390/\nejihpe15010009\n27. Hanaﬁ A, Saad M, Zahran N, Hanafy RJ, Fouda ME. A comprehensive\nevaluation of large language models on mental illnesses. arXiv preprint\narXiv:2409.15687. (2024). doi: 10.48550/arXiv.2409.15687\n28. Institute of Psychology and Chinese Academy of Sciences. (2025). Available\nonline at: https://jcpx.psych.ac.cn/ (Accessed January 20, 2025).\n29. Turcan E, McKeown K. Dreaddit: A reddit dataset for stress analysis in social\nmedia. arXiv preprint arXiv:1911.00133. (2019). doi: 10.18653/v1/D19-62\n30. Haque A, Reddi V, Giallanza T. (2021). Deep learning for suicide and depression\nidentiﬁcation with unsupervised label correction, in:Artiﬁcial Neural Networks and\nMachine Learning–ICANN 2021:30th International Conference on Artiﬁcial Neural\nNetworks, Bratislava, Slovakia, September 14-17, 2021. pp. 436–47, Proceedings, Part V\n30 (Germany: Springer).\nXu et al. 10.3389/fpsyt.2025.1646974\nFrontiers inPsychiatry frontiersin.org07",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.7291429042816162
    },
    {
      "name": "Mental illness",
      "score": 0.6180949211120605
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6127747893333435
    },
    {
      "name": "Test (biology)",
      "score": 0.5222190618515015
    },
    {
      "name": "Psychology",
      "score": 0.45630118250846863
    },
    {
      "name": "Psychiatry",
      "score": 0.41872337460517883
    },
    {
      "name": "Medicine",
      "score": 0.3477499485015869
    },
    {
      "name": "Applied psychology",
      "score": 0.3209730386734009
    },
    {
      "name": "Geography",
      "score": 0.08471745252609253
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}