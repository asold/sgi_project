{
  "title": "Generation of 3D molecules in pockets via a language model",
  "url": "https://openalex.org/W4390880597",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1975597579",
      "name": "Wei Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282858200",
      "name": "Lvwei Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4310993319",
      "name": "Zaiyun Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229894064",
      "name": "Yanhao Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097633994",
      "name": "Han Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141906073",
      "name": "Jianqiang Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2094138992",
      "name": "Rong Bai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163990967",
      "name": "HuTing Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098885307",
      "name": "Jielong Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2090401057",
      "name": "Wei Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143988489",
      "name": "Bo Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104994689",
      "name": "Wenbiao Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975597579",
      "name": "Wei Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282858200",
      "name": "Lvwei Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4310993319",
      "name": "Zaiyun Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229894064",
      "name": "Yanhao Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097633994",
      "name": "Han Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141906073",
      "name": "Jianqiang Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2094138992",
      "name": "Rong Bai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163990967",
      "name": "HuTing Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098885307",
      "name": "Jielong Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2090401057",
      "name": "Wei Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143988489",
      "name": "Bo Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104994689",
      "name": "Wenbiao Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1975447903",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W3121082297",
    "https://openalex.org/W2994860160",
    "https://openalex.org/W3082411326",
    "https://openalex.org/W2969980075",
    "https://openalex.org/W3209764902",
    "https://openalex.org/W4280535976",
    "https://openalex.org/W3200806939",
    "https://openalex.org/W4323650483",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2023818227",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4226077530",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4226159083",
    "https://openalex.org/W2092285329",
    "https://openalex.org/W1995850373",
    "https://openalex.org/W1968319881",
    "https://openalex.org/W3137047737",
    "https://openalex.org/W2128983966",
    "https://openalex.org/W1985588649",
    "https://openalex.org/W2902812092",
    "https://openalex.org/W3030018498",
    "https://openalex.org/W2767891136",
    "https://openalex.org/W4318142102",
    "https://openalex.org/W3197817556",
    "https://openalex.org/W2335804930",
    "https://openalex.org/W4294740789",
    "https://openalex.org/W4386067102",
    "https://openalex.org/W4221149941",
    "https://openalex.org/W3158545781",
    "https://openalex.org/W2151697120"
  ],
  "abstract": "Abstract Generative models for molecules based on sequential line notation (for example, the simplified molecular-input line-entry system) or graph representation have attracted an increasing interest in the field of structure-based drug design, but they struggle to capture important three-dimensional (3D) spatial interactions and often produce undesirable molecular structures. To address these challenges, we introduce Lingo3DMol, a pocket-based 3D molecule generation method that combines language models and geometric deep learning technology. A new molecular representation, the fragment-based simplified molecular-input line-entry system with local and global coordinates, was developed to assist the model in learning molecular topologies and atomic spatial positions. Additionally, we trained a separate non-covalent interaction predictor to provide essential binding pattern information for the generative model. Lingo3DMol can efficiently traverse drug-like chemical spaces, preventing the formation of unusual structures. The Directory of Useful Decoys-Enhanced dataset was used for evaluation. Lingo3DMol outperformed state-of-the-art methods in terms of drug likeness, synthetic accessibility, pocket binding mode and molecule generation speed.",
  "full_text": "Nature Machine Intelligence | Volume 6 | January 2024 | 62–73 62\nnature machine intelligence\nArticle\nhttps://doi.org/10.1038/s42256-023-00775-6\nGeneration of 3D molecules in pockets via  \na language model\nWei Feng    1,3, Lvwei Wang1,3, Zaiyun Lin1,3, Yanhao Zhu1,3, Han Wang    1, \nJianqiang Dong    1, Rong Bai1, Huting Wang1, Jielong Zhou1, Wei Peng2, \nBo Huang    1  & Wenbiao Zhou    1 \nGenerative models for molecules based on sequential line notation \n(for example, the simplified molecular-input line-entry system) or \ngraph representation have attracted an increasing interest in the field \nof structure-based drug design, but they struggle to capture important \nthree-dimensional (3D) spatial interactions and often produce undesirable \nmolecular structures. T o address these challenges, we introduce \nLingo3DMol, a pocket-based 3D molecule generation method that combines \nlanguage models and geometric deep learning technology. A new molecular \nrepresentation, the fragment-based simplified molecular-input line-entry \nsystem with local and global coordinates, was developed to assist the model \nin learning molecular topologies and atomic spatial positions. Additionally, \nwe trained a separate non-covalent interaction predictor to provide \nessential binding pattern information for the generative model. Lingo3DMol \ncan efficiently traverse drug-like chemical spaces, preventing the formation \nof unusual structures. The Directory of Useful Decoys-Enhanced dataset was \nused for evaluation. Lingo3DMol outperformed state-of-the-art methods \nin terms of drug likeness, synthetic accessibility, pocket binding mode and \nmolecule generation speed.\nStructure-based drug design, which involves designing molecules \nthat can specifically bind to a desired target protein, is a fundamental \nand challenging drug discovery task 1. De novo molecule generation \nusing artificial intelligence has recently gained attention as a tool \nfor drug discovery. Earlier molecular generative models relied on \neither molecular string representations2–5 or graph representations6–9.  \nHowever, both representations disregard three-dimensional (3D) \nspatial interactions, rendering them suboptimal for target-aware \nmolecule generation. The increase of 3D protein–ligand complex \nstructures data10 and advances in geometric deep learning have paved \nthe way for artificial intelligence algorithms to directly design mol -\necules with 3D binding poses 11,12. For example, methods using 3D \nconvolutional neural networks 13 are used to capture 3D inductive \nbias, but they still struggle to convert atomic density grids into dis -\ncrete molecules.\nSome studies 14–17 proposed to represent pocket and molecule \nas 3D graphs and used graph neural networks (GNNs) for encoding \nand decoding. These GNN models use an autoregressive generation \nprocess that linearizes a molecule graph into a sequence of sampling \ndecisions. Although these methods can generate molecules with 3D \nconformations, they share some common drawbacks: (1) the generated \nmolecules often contain problematic, non-drug-like or not syntheti -\ncally available substructures such as very large rings (rings containing \nseven or more atoms) and honeycomb-like arrays of parallel, juxta -\nposed rings; (2) problematic topology: the generated molecules often \ncontain an excessive number of rings or none at all. An autoregressive \nsampling method has its inherent limitations. It can easily get stuck in \nlocal optima during the initial stages of molecule generation and may \naccumulate errors introduced at each step of the sampling process. For \nexample, as mentioned by ref. 18, although the model can accurately \nReceived: 14 June 2023\nAccepted: 27 November 2023\nPublished online: 15 January 2024\n Check for updates\n1Beijing StoneWise Technology Co Ltd, Beijing, China. 2Innovation Center for Pathogen Research, Guangzhou Laboratory, Guangzhou, China. 3These \nauthors contributed equally: Wei Feng, Lvwei Wang, Zaiyun Lin,Yanhao Zhu.  e-mail: huangbo@stonewise.cn; zhouwenbiao@stonewise.cn\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73\n 63\nArticle https://doi.org/10.1038/s42256-023-00775-6\ndo not fall into the categories of five- or six-membered rings, as well as \nfused five- or six-membered rings were considered to be complex rings.\nThe fine-tuning dataset was sourced from PDBbind (general set, \nv.2020)29, using the DUD-E dataset31 as homology filters. Specifically, \nwe excluded proteins from the training set that exhibited more than \n30% similarity to any target in DUD-E, as determined using MMseqs2 \n(ref. 32). This process resulted in the selection of 9,024 Protein Data \nBank (PDB) IDs, which represented approximately 46% of the pro -\ntein–ligand PDB IDs in the PDBbind database. Within these selected \nPDB IDs, non-crystallographic symmetry related protein–ligand com-\nplex molecules within an asymmetric unit were considered individual \nsamples. Additionally, samples were excluded from training if no NCIs \nwere recognized between the ligand and the pocket by the Open Drug \nDiscovery T oolkit (ODDT)33. As a result, we obtained a total of 11,800 \nsamples, which encompassed 8,201 PDB IDs (that is, 42% of protein–\nligand PDB IDs in PDBbind), in the fine-tuning dataset.\nThe NCI training dataset had the same samples as the fine-tuning \ndataset. The NCIs of the hydrogen bond, halogen bond, salt bridge and \npi–pi stacking in the PDBbind were labelled using ODDT. The anchors \nwere marked as the atoms in the pocket that are less than 4 Å away from \nany atom in the ligand. These labelled samples were used for the NCI/\nanchor prediction model, averaging 4.1 NCI atoms and 32.1 anchor \natoms per pocket sample.\nRegarding test dataset, the models were evaluated mainly using \nthe DUD-E dataset, which includes more than 100 targets and an aver-\nage of more than 200 active ligands per target. This dataset spans \ndiverse protein categories such as Kinase, Protease, GPCRs and ion \nchannels. More importantly, the experimentally measured affinity has \nbeen reported for the active compounds in DUD-E. Hence, it allows us \nto compare generated molecules with active ligands for various pro-\ntein targets. The target with the PDB ID 2H7L in the DUD-E dataset was \nexcluded as it is listed as an obsolete entry in the PDB.\nFor baselines in this study, two SOTA models, Pocket2Mol (ref. 16) \nand TargetDiff18, were used. Pocket2Mol is an autoregressive genera-\ntive GNN model, and TargetDiff is a diffusion-based model. These two \nmodels were, respectively, obtained from their official GitHub reposi-\ntory. As mentioned in their original research papers, these two models \nwere trained using the CrossDocked2020 dataset10.\nModel evaluation\nThe overall architecture and pretraining strategies are illustrated in  \nFig. 1a–c. A comprehensive description of the model development \nprocess is provided in Methods. In our evaluation, we conducted a com-\nparative analysis of Lingo3DMol with baseline methods. We propose \nto evaluate the generated molecules from mainly three perspectives: \nmolecular geometry, molecular property distribution and the binding \nmode within the pocket.\nMolecular geometry. The bond length distribution of the generated \nmolecules was assessed using a methodology similar to the one used \nin the TargetDiff study. Specifically, around 10,000 molecules were \ngenerated for each of the three models tested in the study. These mol-\necules were generated for 100 targets in the CrossDocked2020 dataset. \nThen we compared the bond length distribution of the generated \nmolecules with that of reference molecules, consisting of 100 ligands \nselected from the CrossDocked2020 dataset, as used in the TargetDiff \nstudy. Both our model and benchmark models exhibited favourable \nperformance, as indicated by similar mean bond lengths compared \nto the reference molecules (Extended Data Table 1).\nT o assess the dissimilarity between the atom–atom distance distri-\nbutions of the reference molecules and the model-generated molecules, \nwe used the Jensen–Shannon divergence metric, which was also used \nin the TargetDiff study. Notably, Lingo3DMol demonstrated the lowest \nJensen–Shannon divergence score for all atom distances and ranked \nsecond for carbon–carbon bond distances (Extended Data Fig. 1).\nposition the nth atom to create a benzene ring when the preceding n − 1 \ncarbon atoms are already in the same plane 16, accurate placement of \nthe initial atoms is often problematic because of insufficient context \ninformation, resulting in unrealistic fragments.\nIn addition, some methods based on other technical routes have \nbeen proposed for 3D molecule generation, such as those based on \ndiffusion models18–20. The representative method is TargetDiff18, which \nuses a graph-based diffusion model for non-autoregressive molecule \ngeneration. Despite its efforts to avoid an autoregressive method, it still \ngenerates a notable proportion of undesirable structures. This problem \nis possibly caused by the model’s relatively weak perception of molecu-\nlar topology, which is associated with its weak ability to directly encode \nor predict bonds. Consequently, although TargetDiff achieved improved \nperformance compared to earlier models, it still has room for improve-\nment in metrics such as quantitative estimate of drug likeness (QED)21 \nand synthetic accessibility score (SAS)22, highlighting the urgency of \nconfining the generated molecules to a drug-like chemical space23.\nWhile graph-based 3D molecular generation methods have \nshown great potential recently, they still face difficulties in reproduc-\ning reference molecules on a given pocket without any information \nleakage, which is an important benchmark for evaluation. T o address \nthe abovementioned problems, we propose Lingo3DMol. First, we \nintroduced a new sequence encoding method for molecules, called \nthe fragment-based simplified molecular-input line-entry system 24 \n(FSMILES). FSMILES encodes the size of the ring in all ring tokens, \nproviding additional contextual information for the autoregressive \nmethod and adopts ring-first traversal to achieve improved perfor -\nmance. Furthermore, we integrated local spherical12 and global Euclid-\nean coordinate systems into our model. Because bond lengths and \nbond angles in the ligand are essentially rigid25, directly predicting them \nis an easier task than predicting the Euclidean coordinates of the atoms. \nCombining of these two types of coordinate enables the model to \nconsider a larger spatial context while maintaining accurate substruc-\ntures. Moreover, non-covalent interactions (NCIs)26 and ligand–protein \nbinding patterns were also considered during molecule generation by \nincorporating a separately trained NCI/anchor predictor. We also used \n3D molecule denoising pretraining strategies similar to BART 27 and \nChemformer28 to improve the generalization ability of the model. Our \nmodel was fine-tuned with data from PDBbind2020 (ref. 29). Finally, \nwe evaluated Lingo3DMol on the Directory of Useful Decoys-Enhanced \n(DUD-E) dataset and compared it with state-of-the-art (SOTA) methods. \nLingo3DMol outperformed existing methods on various metrics.\nOur main contributions can be summarized as follows:\n•\t A new FSMILES molecule representation that incorporates both \nlocal and global coordinates is introduced, enabling the gener -\nation of 3D molecules with reasonable 3D conformations and \ntwo-dimensional (2D) topology.\n•\t A 3D molecule denoising pretraining method and an independent \nNCI/anchor model are developed to help overcome the problem \nof limited data and identify potential NCI binding sites.\n•\t\nThe proposed method outperforms SOTA methods in terms of \nvarious metrics, including drug likeness, synthetic accessibility \nand pocket binding mode.\nResults and discussion\nDatasets and baselines\nThe pretraining dataset was derived from an in-house virtual compound \nlibrary containing structures of more than 20 million commercially \naccessible compounds that are typically used for the virtual screen-\ning of drug hit candidates. Low-energy conformers were generated \nfor each molecule using ConfGen30. T o exclude molecules with low \ndrug likeness, we applied a filtering process that removed complex \nrings and retained molecules with less than three consecutive flexible \nbonds, resulting in 12 million molecules. In this study, any rings that \nNature Machine Intelligence | Volume 6 | January 2024 | 62–73 64\nArticle https://doi.org/10.1038/s42256-023-00775-6\nFurthermore, we conducted an analysis of ring size, considering that \nmolecules with large rings tend to be challenging to synthesize and may \npossess poor drug likeness. Our findings, as presented in Extended Data \nTable 2, revealed that our model exhibited a reduced tendency to generate \nmolecules with a ring size of more than seven compared to the TargetDiff \nand Pocket2Mol models. This observation suggests that our model shows \npromise in avoiding the generation of molecules with unfavourable ring \nsize, further enhancing its potential for drug development applications.\nLigand\nembedding\nLigand input\nIn pretraining\nPocket\nembedding\nPock et i nput\nI n f ine-t unin g and\nNCI/anch or pr ed iction I n f ine-t unin g\nNCI/anchor\ninput\nSelf\nattention\nIn NCI/anchor\nprediction\nNCI/anch or\nNCI/anchor\nPrediction head\nEnco de r\nDecoder\nembedding\nDec oder i nput\nFS MILES wi th\nlocal coordinates decoder\nNext FSMILES \ntoken\nCross attention\nCross attention\nSelf attention with bias\nSelf attention with bias\nAbsolute coordinates\ndecoder\na\nSa mpl e molec u l e : COc1ccc(-c2cccc(S(=O )( =O )N3C CN( c 4 ncc n c 4-c 4 ccc(O C)cc4) CC3)c 2)cc 1\nStep1:’start’O=S([*])([*])=O'sep'N1CCN([*])CC1'sep'c1nccnc1[*]'sep'c1ccc([*])cc1'sep'OC'sep'c1cccc([*])c1'sep'c1ccc([*])cc1'sep'OC'sep’'end_0'\nStep2:'start_0'O_0=_0S_0([*])_0([*])_0=_0O_0'sep_0’N_61_0C_6C_6N_6([*])_0C_6C_61_0'sep_0’c_61_0n_6c_6c_6n_6c_61_0[*]_0'sep_0’\nc_61_0c_6c_6c_6([*])_0c_6c_61_0'sep_0’O_0C_0'sep_0’c_61_0c_6c_6c_6c_6([*])_0c_61_0'sep_0’c_61_0c_6c_6c_6([*])_0c_6c_61_0'sep_0’O_0C_0\n'sep_0''end_0'\nb\nc (1) (2) (3) (4) (5)\nx, y, z\nAuxil iary r, θ, φ\nr, θ, φ\nReconstructed\nSam ple  mo lecul e Cutti ng me tho d illu stration\nFig. 1 | Overview of Lingo3DMol model development. a, Lingo3DMol architec-\nture. Three separate models are included: the pretraining model, the fine-tuning \nmodel and the NCI/anchor prediction model. These models share the same \narchitecture with slightly different inputs and outputs. b, Illustration of FSMILES \nconstruction. The same colour corresponds to the same fragment. c, Illustration \nof pretraining perturbation strategy. Step 1, original molecular state; step 2, \nremoval of edge information during pretraining; step 3, perturbation of the \nmolecular structure by randomly deleting 25% of the atoms; step 4, perturbation \nof the coordinates using a uniform distribution within the range (−0.5 Å, 0.5 Å) \nand step 5, perturbation of 25% of the carbon element types. These perturbations \nare applied in no particular order, and the pretraining task aims to restore the \nmolecular structure from step 5 to step 1.\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73\n 65\nArticle https://doi.org/10.1038/s42256-023-00775-6\nMolecular property and binding mode.  We proposed to test our \nmodel and baseline models using targets from DUD-E, because \na notable number of experimentally measured active compounds \nwere documented in this dataset. Specifically, there are over 20,000 \nactive compounds and their affinities against more than 100 targets, \nan average of over 200 ligands per target. This allows us to analysis the \nsimilarity between generated molecules and known active compounds.\nRegarding tools for binding pose evaluation, we propose to use \nGlide34 because it demonstrates superior ability in enriching active \ncompounds and its use is reported as a baseline in research studies \ninvestigating scoring functions35,36.\nRegarding evaluation metrics for binding pose assessment, three \noptions are available: min-in-place GlideSP score, in-place GlideSP \nscore and GlideSP redocking score. The min-in-place GlideSP score is \nobtained by using the ‘mininplace’ docking method, where the ligand \nstructure undergoes force-field-based energy minimization within \nthe receptor’s field before scoring. It requires accurate initial place -\nment of the ligand with respect to the receptor. The in-place GlideSP \nscore is generated using the ‘in-place’ docking method, which directly \nuses the input ligand coordinates for scoring without any docking or \nenergy minimization. The GlideSP redocking score involves docking \nthe generated molecules into the pocket, including the exploration of \nligand binding conformations and initial placement.\nAmong the three docking-related metrics for binding pose  \nevaluation, we advocate using the min-in-place GlideSP score for  \nthe following reasons. The in-place GlideSP score is excessively  \nsensitive to atomic distances between the ligand and pocket,  \nmaking it unsuitable for evaluating the quality of generated poses.  \nIn Extended Data Table 3, most molecules generated by all three  \nmethods (Lingo3DMol, Pocket2Mol and TargetDiff) exhibit positive \nscores, indicating steric clashes between the pockets and ligands. \nHowever, such clashes do not necessarily denote a poor molecule \nunless they cannot be rectified by force-field-based minimiza -\ntion. On the other hand, the min-in-place GlideSP score respects \nthe initial binding pose generated by the model and optimizes it \nthrough force-field-based adjustments to achieve a robust score. \nThe use of only the GlideSP redocking score, without considering \nthe min-in-place GlideSP score, would contradict the objective of \n3D generation as it disregards the original pose. In this study, we rec-\nommend considering the min-in-place GlideSP score as the primary \nmetric for binding pose evaluation, while also providing GlideSP \nredocking scores for contextual information.\nBefore conducting the binding mode evaluation, we empha -\nsize the importance of examining molecular property distributions. \nExtended Data Fig. 2 shows three molecules that exhibit notably good \ndocking scores (min-in-place GlideSP scores). However, despite their \nfavourable scores, none of these molecules can be considered as poten-\ntial drug candidates. Their exclusion stems from their poor drug like-\nness, as evidenced by low QED values, and low synthetic accessibility, \nreflected by high SAS values. This intriguing finding underscores the \npossibility that a superior performance on the binding pose evaluation \nmetric may result from the presence of non-ideal molecules. Conse -\nquently, it becomes crucial to eliminate molecules with inadequate \ndrug likeness or limited synthetic accessibility before calculating \nGlideSP scores. T o further investigate the importance of this filtering \ncriterion on a larger scale, we conducted an in-depth analysis of the \ndistributions of various key properties of the generated molecules \nusing heatmaps (Fig. 2a–e).\nIt is important to notice that, as shown in the Fig. 2c, the molecules  \nthat possess good min-in-place Glide scores (lower scores) are  \nmostly found outside the drug-like region indicated by the red box for \nPocket2mol and TargetDiff. T o define the drug-like region, we consi-\ndered a QED value of 0.3 or higher and a SAS value of 5 or lower, which \nencompass more than 80% of the molecules in DrugBank 37. Unlike \nbenchmark models, Lingo3DMol demonstrates a different pattern. \nSpecifically, Lingo3DMol tends to generate drug-like molecules with \nrelatively good min-in-place GlideSP scores.\nFor binding mode evaluation, building on the above analysis, we \nconducted a DUD-E-based evaluation of our model and the baseline \nmodels and the results are presented in Table 1. Although the average \nQED and SAS values do not notably differentiate our model from the \nbaselines, the percentage of drug-like molecules determined by com-\nbining QED and SAS indicates the superiority of our model.\nFurthermore, in addition to generating drug-like molecules, an \neffective molecule generative model should be capable of generating \nactive compounds. Since it is impractical to synthesize all generated \nmolecules for real-world testing, an alternative approach is to evaluate \nwhether the model can reproduce known active compounds or gener-\nate molecules that are highly similar to known active compounds. T o \nassess this, we introduced the metric ‘ECFP_TS > 0.5’ , representing \nthe percentage of targets with generated compounds that demon -\nstrate more than 0.5 Tanimoto similarity in terms of ECFP to active \ncompounds. Among the drug-like molecules generated by the three \nmodels, our model yields similar-to-active compounds for 33% of the \ntargets, surpassing Pocket2Mol’s 8% and TargetDiff’s 3%.\nAdditionally, for a 3D molecule generative model, it is crucial to \ngenerate molecules with favourable bindings in the target pockets. \nThis assessment can be approached from two perspectives: binding \nmode with the pocket (interactions with the pocket) and the ligand’s \nstrain energy, both of which are typically associated with good bind-\ning affinity38–40. We used the min-in-place GlideSP score to evaluate \npocket interactions and root-mean-square deviation (r.m.s.d.) versus \nlow-energy conformers to reflect ligand strain energy. Although the \nr.m.s.d. versus low-energy conformers metric does not directly quan-\ntify strain energy in the unit of kcal mol−1, it provides valuable informa-\ntion on how closely the generated conformers resemble the low-energy \nconformers in terms of their overall geometry. This metric serves as a \nproxy for evaluating the ligand’s strain energy. The generated confor-\nmations were compared against the top 20 lowest-energy conformers \nfrom ConfGen30. Lingo3DMol outperforms the baselines in terms of the \nmin-in-place GlideSP score and r.m.s.d. versus low-energy conformers, \nindicating the high quality of our generated conformations relative to \nthe baselines. Moreover, our molecular generation speed is faster than \nbenchmark methods (Extended Data Table 4).\nWhile our model exhibited slightly lower diversity in generating \ndrug-like molecules compared to the baselines, it is important to note \nthat the baselines’ higher diversity does not translate into a satisfactory \nability to generate similar-to-active compounds. This observation sug-\ngests that the baselines may explore the chemical space in a direction \nthat deviates from the region where known active compounds are \ntypically found.\nLast, we used Dice score to access the 3D shape similarity between \nthe reference compound observed in the crystal structure and the \ngenerated 3D molecules. By voxelizing the molecules and comparing \ntheir intersected and union points, we quantified the ‘intersection over \nunion’ ratio as a Dice score that ranges from 0 to 1, with 0 indicating no \nsimilarity. Although all the models demonstrated similar performance \naccording to this metric (Table 1), the Dice score contributed in improv-\ning our model throughout the model development process (further \ndetails are provided in the ‘ Ablation analysis’ section).\nAnother important point to consider is the issue of information \nleakage in the baselines during above DUD-E test. It is essential to \nnote that Pocket2Mol was trained on the CrossDocked2020 dataset, \nwhich did not exclude targets with high homology to DUD-E targets. \nAs a result, the performance of Pocket2Mol in this test may be overes-\ntimated due to the information leakage problem. On the other hand, \nour model was trained on a low-homology (less than 30% sequency \nidentity) dataset to mitigate this issue.\nT o ensure fair comparisons, we categorized DUD-E targets \ninto three groups on the basis of their sequency identity with the \nNature Machine Intelligence | Volume 6 | January 2024 | 62–73 66\nArticle https://doi.org/10.1038/s42256-023-00775-6\n(0.0, 0.1]\na\nb\nc\nd\ne\n6,000\n5,000\n4,000\n3,000\n2,000\n1,000\n0\n700\n800\n900\n1,000\n600\n500\n400\n300\n200\n100\n700\n800\n900\n1,000\n600\n500\n400\n300\n200\n100\n700\n800\n900\n1,000\n600\n500\n400\n300\n200\n100\n–9\n–8\n–7\n–6\n–5\n–4\n–3\n–10\n–9\n–8\n–7\n–6\n–5\n–4\n–3\n–10\n–9\n–8\n–7\n–6\n–5\n–4\n–3\n–10\n–9\n–8\n–7\n–6\n–5\n–4\n–3\n–10\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0\n–9\n–8\n–7\n–6\n–5\n–4\n–3\n–10\n–9\n–8\n–7\n–6\n–5\n–4\n–3\n–10\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0\n6,000\n5,000\n4,000\n3,000\n2,000\n1,000\n0\n6,000\n5,000\n4,000\n3,000\n2,000\n1,000\n0\n(0.0, 1.0]\n(0.1, 0.2]\n(1.0, 2.0]\n(0.2, 0.3]\n(2.0, 3.0]\nQED\nHeatmap A: count of mol_id Heatmap A: count of mol_id Heatmap A: count of mol_id\nHeatmap B: average of mol_weight Heatmap B: average of mol_weight Heatmap B: average of mol_weight\nHeatmap C: average of min-in-place\nGlideSP score\nHeatmap C: average of min-in-place\nGlideSP score\nHeatmap C: average of min-in-place\nGlideSP score\nHeatmap D: average of Glide\nredocking score\nHeatmap D: average of Glide\nredocking score\nHeatmap D: average of Glide\nredocking score\nHeatmap E: average of r.m.s.d. Heatmap E: average of r.m.s.d. Heatmap E: average of r.m.s.d.\nLingo3DMol Pocket2Mol TargetDiﬀ\nSAS\n(0.3, 0.4]\n(3.0, 4.0]\n(0.4, 0.5]\n(4.0, 5.0]\n(0.5, 0.6]\n(5.0, 6.0]\n(0.6, 0.7]\n(6.0, 7.0]\n(0.7, 0.8]\n(7.0, 8.0]\n(0.8, 0.9]\n(8.0, 9.0]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(0.0, 0.1]\n(0.1, 0.2]\n(0.2, 0.3]\nQED\n(0.3, 0.4]\n(0.4, 0.5]\n(0.5, 0.6]\n(0.6, 0.7]\n(0.7, 0.8]\n(0.8, 0.9]\n(0.9, 1.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\n(0.0, 1.0]\n(1.0, 2.0]\n(2.0, 3.0]\nSAS\n(3.0, 4.0]\n(4.0, 5.0]\n(5.0, 6.0]\n(6.0, 7.0]\n(7.0, 8.0]\n(8.0, 9.0]\n(9.0, 10.0]\nFig. 2 | Distributions of molecules generated by Lingo3DMol, Pocket2Mol and \nTargetDiff on DUD-E targets (n = 101). The drug-like region with QED ≥ 0.3 and \nSAS ≤ 5 is indicated with red boxes. a–e, Heatmaps show the distribution of key \nproperties for generated molecules, including count (a), molecular weight (b), \nminimum in-place GlideSP score (c), GlideSP redocking score (d) and r.m.s.d. versus \nlow-energy conformer (e). These distributions are depicted along SAS and QED.\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73\n 67\nArticle https://doi.org/10.1038/s42256-023-00775-6\nPocket2Mol training targets: severe (more than 90%), moderate \n(30–90%) and limited (less than 30%) information leakage. Across \nall categories, Lingo3DMol consistently outperformed Pocket2Mol, \nparticularly in terms of the min-in-place GlideSP score, Glide redock-\ning score and r.m.s.d. versus low-energy conformers (Extended Data \nFig. 3). It is intriguing to observe that the performance gap between \nthe two models widens as the impact of information leakage on the \nbaselines becomes negligible.\nCase analysis\nFor the case study, we selected the generated molecules from two \nperspectives: ECFP fingerprint similarity with known active com -\npounds and the docking score. In particular, a high similarity of ECFP \nfingerprints with known active compounds indicated the model’s capa-\nbility of generating similar substructures or topology features with \npositive molecules, and a good docking score indicated a stronger fit \nwith the pocket.\nAs shown in Fig. 3a,b, the molecules in ‘high similarity, good dock-\ning score’ group resembled positive molecules in terms of structure and \nbinding mode, demonstrating the ability of our model to reproduce \nactive compounds. However, this is insufficient for this study, because \ndrug design researchers in the real world are more interested in retriev-\ning the following two types of molecule: (1) active compounds that the \ndocking program or other virtual screen tools fail to detect, and (2) mole-\ncules with new scaffolds and good pocket binding affinity. The first issue, \n‘active compounds missed by the docking program’ , is often caused by \ninsufficient sampling of the binding pose, which is closely related to the \ndocking score. Our 3D molecule generation model provides a potential \nsolution to this issue. As shown in Fig. 3c,d, the generated molecules in \nthe ‘high similarity, poor docking score’ group are highly similar to the \npositive molecules but had poor docking scores (that is, −6.5 and −6.4, \nrespectively) when the docking program was used for binding pose \nsampling (that is, Glide redocking). Conversely, when a binding pose \ngenerated by our model was evaluated using the Glide scorer without \nconformation sampling, we obtained good scores of −8.7 and −8.8 (that \nis, min-in-place Glide score), respectively, for the two compounds. This \ndemonstrates the effectiveness of our generated 3D conformation for \nretrieving good molecules with poor docking scores. For the second \nissue of obtaining molecules with a new scaffold and good binding \nmode, we present two cases, shown in Fig. 3e,f, to demonstrate that our \nmodel can potentially generate molecules with these characteristics.\nAblation analysis\nEffective pretraining and fine-tuning analysis. Specifically, for DUD-E \ntargets, the molecules generated by the models with and without \npretraining were respectively compared with the molecules in the \npretraining set. We demonstrated that the molecules generated by the \npretraining model exhibited a higher degree of similarity to the mol-\necules in the pretraining set compared to those generated by the model \nwithout pretraining. This indicates that the model retained the effect \nof pretraining after the fine-tuning. The comparison of these methods \nis described in Supplementary Information Part 1. As shown in Table 2, \npretraining notably improves the percentage of drug-like molecules, \nmean QED, the percentage of ECFP_TS > 0.5, mean min-in-place GlideSP \nscore and diversity. We attribute this improvement to the effectiveness \nof pretraining, especially in scenarios with limited fine-tuning data. In \ndeep learning models, pretraining plays a crucial role in capturing rel-\nevant chemical patterns and features, allowing the model to generalize \nand generate molecules that align with desired properties even when \nfine-tuning data is limited.\nNCI prediction model ablation studies. During this ablation study, \nwe compared Lingo3DMol using randomly selected NCI sites to the \nstandard Lingo3DMol that uses a well-trained NCI site predictor. It \nis important to note that both approaches share the same molecule \ngeneration model. As shown in Table 2 , standard Lingo3DMol dem -\nonstrated superior performance in most of the metrics, especially \nin drug likeness and ECFP_TS > 0.5. This can be attributed to several \nfactors. One factor is that randomly selected NCI sites may result in \nthe selection of solvent-exposed regions of the pocket where polar \ngroups are more likely to be located. This may offer more accessible \nspace compared to the cavity where the reference molecule is located. \nAdditionally, the random selection of NCI sites tends to result in NCIs \nthat are spaced farther apart from each other. The combination of these \nfactors, including the preference for solvent-exposed regions and the \nspacing of selected NCIs, may contribute to the generation of larger \nmolecules and subsequently affect the QED score and the percentage \nof drug-like molecules.\nTable 1 | Comparison of generated drug-like molecules on \nDUD-E targets (n = 101)\nRandom \ntest\nPocket2Mol TargetDiff Lingo3DMol \n(ours)\nNumber of molecules \ngenerated\n100,195 98,332 92,727 100,428\nMean QED (↑) 0.69 0.46 0.50 0.53\nMean SAS (↓) 2.6 4.0 4.9 3.3\nNumber of drug-like \nmolecules\n98,432 59,936 45,210 82,637\nDrug-like molecules  \nas % of total generated  \nmolecules (↑)\n98% 61% 49% 82%\nThe comparison below involves only drug-like molecules\n  Mean molecular \nweight\n370 386 299 348\n ECFP_TS > 0.5 (↑) 17% 8% 3% 33%\n  Mean min-in-place \nGlideSP score (↓)\nN/A −6.7 −6.2 −6.8\n  Mean GlideSP \nredocking score (↓)\n−6.4 −7.5 −7.0 −7.8\n Mean QED (↑) 0.70 0.56 0.60 0.59\n Mean SAS (↓) 2.6 3.5 4.0 3.1\n Diversity (↑) 0.85 0.84 0.88 0.82\n Dice (↑) 0.21 0.24 0.28 0.25\n  Mean r.m.s.d. versus \nlow-energy conformer \n(Å,↓)\n4.0 1.1 1.1 0.9\nNote that for each method, we generated approximately 1,000 molecules per target. To \ndetermine the drug likeness and inclusion in the comparison, we considered molecules \nwith a QED score greater than or equal to 0.3 and a SAS less than or equal to 5. The metric \n‘ECFP_TS > 0.5’ represents the percentage of targets with generated compounds that are \nsimilar to active compounds on the basis of the Tanimoto similarity of ECFP4 (ref. 51). The \nmin-in-place GlideSP score and GlideSP redocking score were calculated using the Glide \nsoftware. The r.m.s.d. value indicates the differences between the generated conformers and \nthe low-energy conformers generated using ConfGen\n30. As for the random set, we randomly \nselected 1,000 molecules from our in-house commercial library for each target. As there are \nno ‘generated conformers’ for the random test molecules, the r.m.s.d. in this case represents \nthe differences between the docked conformer and the low-energy conformer. More details \nof molecular weight distribution can be found in Extended Data Fig. 6. Diversity reflects the \naverage pair-wise Tanimoto similarity of molecules generated for the same target. Dice score \nwas defined as the ratio of ‘intersection over union’ between the voxelized representations \nof the reference compounds observed in the crystal structure (that is, the PDB ID) and the \ngenerated molecules. To calculate Dice score, we created a grid with points at 0.5 Å intervals \nto cover both molecules. Each grid point was evaluated to determine whether it fell within  \n1.2 times of the covalent radius (referred as testing radius) of any atom in either molecule. \nGrid points within the testing radius of atoms in both molecules were considered as \nintersected points, while grid points within the testing radius of any atom in either molecule \nwere considered as union points. The Dice score, calculated as the ratio of intersected points \nover union points, ranges from 0 to 1, with a value of 0 indicating no similarity or overlap \nbetween the molecules. Bold face indicates the best performance.\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73 68\nArticle https://doi.org/10.1038/s42256-023-00775-6\nLast, it is worth noting that for over 95% of DUD-E targets, both our \ntraining set (PDBbind, general set, v.2020) and the benchmark models’ \ntraining set (CrossDocked2020) include at least one molecule with Tani-\nmoto similarity greater than 0.5 to the DUD-E actives in terms of ECFP4 \nfingerprints. However, the notable improvement in ECFP_TS > 0.5 for \nstandard Lingo3DMol compared to Lingo3DMol with random NCI \nand baseline models suggests that this improvement cannot be solely \nattributed to the model reproducing what it has seen during training.\nConclusion\nIn this study, we proposed a new molecule representation called \nFSMILES and developed Lingo3DMol, a model based on language \nmodelling and geometric deep learning techniques. Compared with \nbaselines, our model exhibited superior performance in generating \ndrug-like 3D molecules with better binding mode. This indicates the \npotential of our model for further exploration in drug discovery \nand design.\nNevertheless, challenges remain. Capturing all NCIs within a single \nmolecule is not straightforward due to the autoregressive generation \nprocess, and we plan to investigate this issue further. Representing \nmolecules and intermolecular interactions with electron densities \nperhaps offers a promising direction, and some related researches \nmay serve as a good starting point 26,41,42. Moreover, the equivariance \nproperty is a critical aspect of 3D molecule generation19,20,43. There are \nmany studies on rotational and translational equivariant models, such \nas GVP44 and Vector Neurons45. Currently, we use rotation and transla-\ntion augmentation to enhance the model, and we use SE(3) invariant \nfeatures such as distance matrices and local coordinates46 to alleviate \nthe problem. Last, we have assessed drug-like properties through case \nanalysis and used cheminformatics tools such as QED21 and SAS22 from \nRDKit47. However, a comprehensive and systematic evaluation of these \nproperties is an essential next step for further research.\nMethods\nIn this section, we present an overview of the Lingo3DMol architecture \nand its key attributes. The methodology comprises two models: the \ngeneration model, which is the central component, and the NCI/anchor \nprediction model, an essential auxiliary module. These models share \nthe same transformer-based architecture. In the following text, unless \nspecifically mentioned, model refers to the generation model.\nPDB ID 3G0E \nBM Sim 1.0 \nmin-in-place –10.3\nRedocking  –10.3\nPDB ID 2QD9 \nBM Sim 0.5\nmin-in-place  –8 .7\nRedocking –6.5\nPDB ID 3D4Q \nBM Sim 0.4\nmin-in-place  –8 .9\nRedocking –9.2\nPDB ID 2ETR \nBM Sim 0.4\nmin-in-place  –8 .3\nRedocking –8.8\nHigh similarity, poor docking score\nLow similarity, good docking score\nRedockingMin-in-place\nReference ligand\n(crystal structure) Similar active\ncompound\nLingo3DMol molecules (GlideSP scores)\nPDB ID 3BZ3 \nBM Sim 1.0 \nmin-in-place –9 .6\nRedocking –9.6\nPDB ID 3BZ3 \nBM Sim 0.6\nmin-in-place  –8 .8\nRedocking –6.4\nHigh similarity, good docking score\na\nb\nc\nd\ne\nf\nFig. 3 | Case study of generated molecules involving 3D binding mode and \n2D similarity with active compounds. Six cases are selected based on the \ndocking scores of generated molecules and their similarities to reference active \ncompounds. a,b, Cases with high similarity and good docking scores. c,d, Cases \nwith high similarity but poor docking scores. e,f, Cases with low similarity \nbut good docking scores. The reference binding mode is based on the crystal \nstructure from the PDB. The Lingo3DMol conformation represents the generated \nconformation, while the GlideSP redocking conformation was obtained by \ndocking the generated compound into the pocket using Glide with specific \nparameters (that is, docking_method:confgen and precision:sp). The most \nsimilar known active compound to the generated molecule is also displayed, \nnoting that it may not necessarily be the reference compound observed in the \ncrystal structure. The information provided includes the PDB ID for the reference \nbinding mode, the Tanimoto similarity between the Bemis–Murcko scaffolds \nof generated molecule and its most similar active compound, and the GlideSP \nredocking score.\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73\n 69\nArticle https://doi.org/10.1038/s42256-023-00775-6\nDefinitions and notations\nThe Lingo3DMol learns M ≅ P(M∣Pocket; μ), where μ is the parameter \nof the model, Pocket = (p1, p2…pn) is the set of atoms in the pocket and \npi = (typei, main/sidei, residuei, coordsi, hbd/hbai, NCI/anchori) indi-\ncates the information of the ith atom in the pocket, where ‘type’ denotes \nthe element type of the atom, ‘main/side’ denotes an atom on the main \nor the side chain, ‘residue’ denotes the residue type of the atom, ‘coords’ \nis the coordinates of the atom, ‘hbd/hba’ denotes whether an element \nis a hydrogen bond donor or acceptor and ‘NCI/anchor’ records \nwhether it is a possible NCI site or anchor point where a potential ligand \natom exists within a 4 Å range. Further details are provided in the  \nsection ‘NCI/anchor prediction model’ below. M=( FSMILES,{(ri)}\nK\ni=1)  \nis the representation of the ligand, ri is the coordinates of the ith atom \nof the ligand and K is the number of atoms in the ligand.\nFSMILES is a modified representation of SMILES24 that reorganizes \nthe molecule into fragments, using the normal SMILES syntax for each \nfragment. The entire molecule is then constructed by combining these \nfragments using a specific syntax in a fragment-first then depth-first \nmanner, as illustrated in Extended Data Fig. 4. This approach offers two \nkey advantages: enhanced 2D pattern learning through the use of sym-\nbols to represent fragments and local structures, and the prioritization \nof ring closure, enabling the generation of molecules with accurate ring \nstructures and bond angles. In FSMILES, the size of a ring is indicated \nfrom the first atom of the ring. For example, ‘C_6’ represents a carbon \natom in a six-membered ring. By providing both the atom type and \nthe ring size in advance, the model can more accurately predict the \ncorrect bond angles.\nThe molecule fragment cutting process in FSMILES involves select-\ning individual bonds that meet specific criteria, such as not being part \nof a ring, not connecting hydrogen atoms and having at least one end \nattached to a ring. This cutting process helps divide the ligand into \nfragments. The FSMILES construction process occurs in two steps  \n(Fig. 1b). First, the ligand is divided into fragments according to the \ncutting rule. Second, ring information is embedded in each FSMILES \ntoken, with the number following the element type’s underscore indi-\ncating the ring size. The symbol ‘*’ denotes the connection points of a \nfragment, while the preceding atom indicates the connection position. \nIn the depth-first growth model, each subsequent fragment connects to \nthe preceding asterisk. T o facilitate this, asterisks are stored in a stack \nand when encountering a new fragment the topmost asterisk is used \nto establish a connection.\nPretraining and fine-tuning\nPretraining strategy. In the pretraining phase, as illustrated in Fig. 1c,  \nwe introduced perturbations into the 3D molecular structure and  \nfed the perturbed molecule into the encoder. This model, which uses  \nan autoregressive approach, aims to reconstruct the perturbed  \nmolecule back to its original state in both 2D and 3D representations.\nFine-tuning.  For the fine-tuning phase, we used the pretrained  \nmodel and further fine-tuned it on the protein–ligand complex  \ndata. The primary task during this phase continued to be auto -\nregressive molecule generation. T o circumvent overfitting of the \nfine-tuning dataset, the initial three encoder layers were fixed during \nfine-tuning.\nThe pseudocode of the above training process is shown in  \nSupplementary Algorithm 1.\nModel architecture\nThe generation model and NCI/anchor prediction model were built \non the transformer-based structure with additional graph structural \ninformation encoded into the model similar to the previous study 48. \nThe generation model was trained by pretraining and fine-tuning. The \nNCI/anchor prediction model was trained on the basis of the genera-\ntion model’s pretrained parameters and fine-tuned additionally by its \nspecific prediction task. The overall architecture is shown in Fig. 1a. \nIn the following sections, we first discuss the encoder and decoder \ncomponents of the generation model, followed by an introduction to \nthe NCI/anchor prediction model.\nEncoder. During the pretraining stage, the input of the encoder is a \nperturbed 3D molecule, which includes the element type and Euclidean \ncoordinates. We can define the molecule as Menc =( menc\n1 ,menc\n2 …menc\nn ), \nmi = (typei, coordsi), where coordsi = (xi, yi, zi) and n is the number of \natoms. Input feature fpre can then be defined as follows:\nfcoords,i = MLP([Ecoords(xi),Ecoords(yi),Ecoords(zi)]), (1)\nfpre,i = Etype(typei)+f coords,i, (2)\nwhere\nEtype (typei) ∈ℝ H,fcoords,i ∈ℝ H\nEcoords(xi)∈ℝ H, Ecoords(yi)∈ℝ H, Ecoords(zi)∈ℝ H,\nwhere Etype and E coords represent the embedding functions for the \nelement type, and the corresponding coordinates, respectively. H \nis the size of the embedded vectors. The symbol ‘+’ represents the \nelement-wise addition operator and the symbol '[ ]' represents the \nconcatenation operator.\nDuring the fine-tuning stage, the input is changed from per -\nturbed molecules to pockets. The input feature Efine can be defined as  \nfollows:\nffine,i = Etype (typei)+Emain/side(main/sidei)+ Eresidue(residuei)\n+ fcoords,i +Ehbd/hba(hbd/hbai)+E NCI/Anchor(NCI/Anchori)\n(3)\nTable 2 | Comparison of generated drug-like molecules \ninvolved in ablation studies\nLingo3DMol \n(standard)\nLingo3DMol \n(with random \nNCI)\nLingo3DMol \n(without \npretraining)\nNumber of molecules \ngenerated\n100,428 99,170 23,982\nMean QED (↑) 0.53 0.35 0.46\nMean SAS (↓) 3.3 3.5 3.4\nNumber of drug-like molecules 82,637 46,502 16,966\nDrug-like molecules as % of \ntotal generated molecules (↑)\n82% 47% 71%\nThe comparison below involves only drug-like molecules\n Mean molecular weight 348 424 345\n ECFP_TS > 0.5 (↑) 33% 6% 3%\n  Mean min-in-place GlideSP \nscore (↓)\n−6.8 −5.8 −4.9\n  Mean GlideSP redocking \nscore (↓)\n−7.8 −7.2 −6.9\n Mean QED (↑) 0.59 0.51 0.56\n Mean SAS (↓) 3.1 3.3 3.1\n Diversity (↑) 0.82 0.83 0.70\n Dice (↑) 0.25 0.15 0.13\n  Mean r.m.s.d. versus \nlow-energy conformer (Å,↓)\n0.9 1.3 1.0\nNote that for Lingo3DMol standard and Lingo3DMol with random NCI, we generated \napproximately 1,000 molecules per target; for Lingo3DMol without pretraining, with the \nsame computational resources and time constraints, molecules can only be generated on  \n73 pockets. Bold face indicates the best performance.\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73 70\nArticle https://doi.org/10.1038/s42256-023-00775-6\nwhere\nEmain/side(main/sidei)∈ℝ H,Eresidue(residuei)∈ℝ H,\nEhbd/hba(hbd/hbai)∈ℝ H,ENCI/Anchor(NCI/Anchori)∈ℝ H\nwhere Emain/side, Eresidue, Ehbd/hba and ENCI/Anchor represent the embedding \nfunctions for the main or side chain, residue type, hydrogen bond \ndonor or acceptor and NCI/anchor point, respectively.\nDecoder. The molecule generation process is implemented by two \ndecoders: one 2D topology decoder (D 2D) generates FSMILES tokens \nand local coordinates, and the other decoder generates 3D global \ncoordinates (D3D). First, the next token is predicted using D 2D, then \nthe latest 2D token is input to D3D. The 3D global coordinates decoder \npredicts the global coordinates of the new fragment in the molecule. \nBoth decoders simultaneously predict the local coordinates r , θ  \nand ϕ, particularly the radial distance (r), bond angle (θ) and dihedral \nangle (ϕ ) of the molecule. The local coordinates prediction by D 3D  \nonly serves as an auxiliary training task.\nThe input of the decoder network for a molecule can be defined \nas Mdec =( mdec\n1 ,mdec\n2 …mdec\nn ), mi = (tokeni, global_coords i) and M bias =  \n(D, J), where D is a distance matrix of size n × n and J is an edge vector \nmatrix of size n  × n, where n  is the length of the sequence. M dec is  \ntransformed into embeddings using the same process as equation (2). \nThese embeddings are then fed into the 2D topology decoder and  \nglobal coordinates decoder, respectively. For non-atom tokens,  \nwe assigned the same coordinates as those of the most recently  \ngenerated atom.\nThe bias terms BD and BJ are derived from the distance and edge \nvector matrices D and J, respectively. Modified attention scores were \ncalculated by incorporating the following bias terms:\nAbiased = softmax(QK⊤\n√dk\n+BD +BJ)V. (4)\nFor FSMILES, we used a multilayer perceptron projection head \nto predict the next token on the basis of D 2D’s output. T o predict the  \nlocal coordinates, we established a local coordinates system with  \nthree atomic reference points: root1 is current position’s parent  \natom, root1’s parent atom is root2 and root2’s parent atom is root3.  \nThe parent atom is the atom that connects to the child atom.\nT o predict the radial distance r, we used features of root1 (hroot1) \nextracted from the D2D’s hidden representation, the current FSMILES \ntoken (Etype(cur)) and the molecule’s hidden representation ( Htopo) \nfrom D2D, Htopo = (h1, h2…hi), where each h represents a FSMILES token’s \nhidden representation and i represents the number of the generated \ntokens. For the polar angle θ, we concatenated the hidden representa-\ntions hroot1 and hroot2. T o predict the dihedral angle ϕ, we concatenated \nthe representations of all three roots.\nWe used multilayer perceptrons as projection heads to predict the \nlocal coordinates (r, θ, ϕ). The mathematical representations of these \nprocesses are as follows:\ndistance prediction r:\nr= argmax(softmax(MLP1 ([Etype(cur),Htopo,hroot1]))), (5)\nangle prediction θ:\nθ= argmax(softmax(MLP2 ([Etype(cur),Htopo,hroot1,hroot2]))), (6)\ndihedral angle prediction ϕ:\nϕ= argmax(softmax(MLP3 ([Etype(cur),Htopo,hroot1,hroot2,hroot3]))).\n(7)\nThe predicted local coordinates (r , θ, ϕ) are obtained by taking  \nthe argmax operator of the corresponding softmax output.\nWhen predicting global coordinates, as illustrated in Fig. 1a,  \nD3D receives the hidden representation of D 2D, and concatenate  \nthe predicted FSMILES token, and then predicts the global coordi -\nnate (x, y, z).\nFor pretraining and fine-tuning loss, in our model, the loss func-\ntion is a combination of multiple components that evaluates different \naspects of the predicted molecule. The overall loss function is defined \nas follows:\nL= LFSMILES +Labs_coord +Lr +Lθ +Lϕ +Lr_aux +Lθ_aux +Lϕ_aux, (8)\nwhere:\n•\t LFSMILES measures the discrepancy between the predicted and \nground-truth molecular topology.\n•\t Labs_coord evaluates the difference between the predicted and \nground-truth atomic coordinates.\n•\t Lr and Lr_aux measure the error between the predicted and \nground-truth radial distances.\n•\t Lθ and Lθ_aux assess the discrepancy between the predicted and \nground-truth bond angles.\n•\t Lϕ and Lϕ_aux evaluate the difference between the predicted and \nground-truth dihedral angles.\nAll the prediction tasks are treated as classification tasks. \nTherefore, the cross entropy loss is used for each individual loss  \ncomponent. Auxiliary prediction tasks r _aux, θ _aux and ϕ _aux are \nused only during training. They are not used during the actual infer-\nence process. For further details, please refer to the ‘Generation \nprocess’ section.\nNCI/anchor prediction model. In this work, we aimed to enhance the \ngeneration model by integrating NCI and anchor point information \nduring the fine-tuning and inference stage. T o achieve this, we used an \nNCI/anchor prediction model, which mirrors the generation model’s \narchitecture. This prediction model was initialized using the generation \nmodel’s pretrained parameters. Equipped with a specialized output \nhead, the encoder can predict whether a pocket atom will form an NCI \nwith the ligand or act as an anchor point.\nThis approach allowed us to enhance the generation model \nin two ways. First, we enriched the model’s input by incorporating  \nthe predicted NCI and anchor point data as distinct features of the \npocket atoms. Second, we started the molecule generation process \nby predicting the first atom of the molecule near a chosen NCI site. \nSpecifically, we sampled an NCI site within a pocket and generated the \nfirst small-molecule atom within a 4.5 Å radius of this site.\nThere are three main implications of our approach: (1) We can \nenhance the perception of NCI and pocket shape, which is critical for \ngenerating 3D molecules that can effectively interact with the target \nprotein. (2) By positioning the first atom near the atoms within the \nNCI pocket, we can increase the likelihood of obtaining a correct NCI \npair with a high degree of certainty. Although our model is designed \nto generate molecules that are prone to forming NCI pairs, it cannot \nensure the exact positioning of the generated molecule in relation to \nthe NCI pocket atoms. This is because the model does not explicitly \nenforce the coverage of all predicted NCI pocket positions, thereby \nallowing for some degree of variability in the positioning of the gener-\nated molecule. (3) Obtaining a good starting position: the autoregres-\nsive generation can be seen as a sequential decision-making problem. If  \nthe initial step is not chosen appropriately, it can affect every sub -\nsequent step. Our empirical study suggests that the NCI position as \na starting point is a better choice than a random sample from the \npredicted 3D coordinates distribution or by selecting the coordinates \nwith highest possibility.\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73\n 71\nArticle https://doi.org/10.1038/s42256-023-00775-6\nFor NCI/anchor prediction loss, we defined two loss functions: The \nNCI loss measures the difference between the predicted NCI sites and \nthe ground-truth NCI sites, and the anchor loss measures the difference \nbetween the predicted anchor points and the ground-truth anchor \npoints. Both loss functions use binary cross entropy.\nThe total loss for the model is the sum of the NCI loss, anchor loss \nand all other auxiliary losses from the original 3D generation task.\nLtotal = LNCI +LAnchor +Lgen, (9)\nwhere LNCI and LAnchor represent the NCI and anchor losses, respectively. \nLgen corresponds to the losses from the original 3D generation task that \nserved as only an auxiliary training objective.\nGeneration process\nHere we present the process of generating the final 3D molecule, as \nshown in Extended Data Fig. 5.\nAtom generation. First, the NCI/anchor prediction model predicts \nthe NCI sites and anchor atoms. We then sampled one NCI site from \nthese predictions, and generated the first ligand atom. This atom was \npositioned at the global coordinates (x, y, z) with the highest predicted \njoint probability, provided it lies within a 4.5 Å radius of the sampled \nNCI site. The subsequent atomic positions were generated iteratively. \nFor each step i, D2D predicts the (i + 1)th FSMILES token and local coordi-\nnates (r, θ, ϕ). On the basis of the (i + 1)th FSMILES token, we identified \nthe indices of the root1, root2 and root3 atoms. The 3D global coordi-\nnates decoder D3D was used to predict the probability distribution of \nthe (i + 1)th 3D coordinates (x, y, z) by incorporating information from \nthe (i + 1)th FSMILES token and the local coordinates.\nWithin a molecule, bond lengths and angles are largely fixed, and \nFSMILES fragments are consistently rigid and replicable. As a result, \nthe prediction of local spatial positions will get easier by using local \ncoordinates, which include bond length, bond angle and dihedral angle. \nBy contrast, the global coordinates offer a robust global 3D perception, \nwhich is essential for assessing the overall structural context.\nWe proposed a fusion method that combines the local and global \ncoordinates. Particularly, this method defines a flexible search space \naround the predicted local coordinates, then selects the global coordi-\nnates with the highest probability. The search space is defined as follows:\n•\t r ± 0.1 Å (angstrom) for distance.\n•\t θ ± 2° for the angle.\n•\t ϕ ± 2° for the dihedral angle.\nWithin this search space, we determined the position with the high-\nest joint probability in the predicted global coordinate distributions. \nThe generation process was repeated to extend the molecular structure \nprogressively. The pseudocode is shown in Supplementary Algorithm 2.\nSampling strategy. We used State(t) = (pocket, ligand(t)) to character-\nize the generative status at step t, where ligand(t) represents the ligand \nstate after step t  is completed. The Action(t ) consists of fragments \ngenerated by the encoder or decoder model on the basis of State(t ). \nThus, within the framework of this sampling strategy, the encoder or \ndecoder model functions as an action generator under the context of \nState(t). When the system adopts a certain Action(t) under the condi-\ntion State(t) = (pocket, ligand(t)), it moves to the next state State(t + 1) =  \n(pocket, ligand([t + 1)) with a probability of 1, as P(State(t + 1)∣State(t), \nAction(t)) = 1. It is particularly noted that State(0) = (pocket,  ).\nThe Reward(t) is defined to evaluate State(t) by using two metrics: \nthe model’s predictive confidence and the degree of anchor fulfilment. \nThe computation of the model’s predictive confidence involves averag-\ning the conditional probabilities of each token involved in Action(t). \nThe degree of anchor fulfilment was measured by calculating the  \nproportion of anchors that are within 4 Å of a ligand atom.\nT o sample at step t, the encoder or decoder model uses an inde-\npendent and identically distributed approach based on State(t ) to \ngenerate N instances of Action(t). Then, instances containing atoms \nwith less than 2.5 Å distance from non-hydrogen pocket atoms were dis-\ncarded to avoid potential clashes. From the remaining set of Action(t), \nwe individually summed the normalized model’s confidence and the \ndegree of anchor fulfilment to calculate their respective Rewards. We \nthen retained the top 0.2 × N  instances of Action(t ) with the highest \nRewards.\nThe entire sampling process is executed through a depth-first \nsearch methodology, ensuring a coherent and systematic progression \nthroughout the entire sampling procedure. The pseudocode is shown \nin Supplementary Algorithm 3.\nData availability\nThe evaluation dataset CrossDocked2020 are from the previous study \nTargetDiff18 and is available at their GitHub https://github.com/guanjq/\ntargetdiff, and the DUD-E31 dataset is a publicly available dataset and is \navailable on our GitHub https://github.com/stonewiseAIDrugDesign/\nLingo3DMol. The PDBbind29 dataset is publicly available at http://pdb-\nbind.org.cn/. The NCI training dataset’s NCI label are labelled using \nan open-source tool, ODDT33. The protein–ligand complex structures \nused for model fine-tuning, the generated molecules used for evalua-\ntion, and a part of the molecules used for pretraining are accessible via \nfigshare repository49. The full pretraining dataset is a private in-house \ndataset including molecules sourced from both commercial databases \nand publicly available databases. Due to contractual obligations with \nthe commercial database vendors, we are unable to share the full pre-\ntraining dataset publicly. Nonetheless, we are pleased to offer partial \ndata, specifically 1.4 million molecules, which were obtained from \npublicly available databases. T o request access to additional data, we \nkindly ask interested researchers to contact the corresponding authors \nwith a proposal outlining their non-commercial research intentions. \nOn receipt of a research proposal, we will review it on a case-by-case \nbasis and work towards finding a suitable solution that adheres to the \ncontractual obligations while promoting scientific progress.\nCode availability\nThe source code for inference and model architecture is publicly availa-\nble on GitHub. The pretraining, fine-tuning and NCI model checkpoints \nare also available on our GitHub https://github.com/stonewiseAID -\nrugDesign/Lingo3DMol and figshare repository50. The model is also \navailable as an online service at https://sw3dmg.stonewise.cn.\nReferences\n1. Anderson, A. C. The process of structure-based drug design. \nChem. Biol. 10, 787–797 (2003).\n2. Bjerrum, E. J. & Threlfall, R. Molecular generation with recurrent \nneural networks (RNNs). Preprint at https://arxiv.org/abs/1705.04612 \n(2017).\n3. Kusner, M. J., Paige, B. & Hernández-Lobato, J. M. Grammar \nvariational autoencoder. Preprint at https://arxiv.org/abs/ \n1703.01925 (2017).\n4. Segler, M. H., Kogej, T., Tyrchan, C. & Waller, M. P. Generating \nfocused molecule libraries for drug discovery with recurrent \nneural networks. ACS Cent. Sci. 4, 120–131 (2018).\n5. Xu, M., Ran, T. & Chen, H. De novo molecule design through the \nmolecular generative model conditioned by 3D information of \nprotein binding sites. J. Chem. Inform. Model. 61, 3240–3254 (2021).\n6. Li, Y., Vinyals, O., Dyer, C., Pascanu, R. & Battaglia, P. Learning \ndeep generative models of graphs. Preprint at https://arxiv.org/\nabs/1803.03324 (2018).\n7. Liu, Q., Allamanis, M., Brockschmidt, M. & Gaunt, A. L. \nConstrained graph variational autoencoders for molecule design. \nPreprint at https://arxiv.org/abs/1805.09076 (2018).\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73 72\nArticle https://doi.org/10.1038/s42256-023-00775-6\n8. Jin, W., Barzilay, R. & Jaakkola, T. Junction tree variational \nautoencoder for molecular graph generation. Preprint at  \nhttps://arxiv.org/abs/1802.04364 (2018).\n9. Shi, C. et al. GraphAF: a flow-based autoregressive model for \nmolecular graph generation. Preprint at https://arxiv.org/abs/ \n2001.09382 (2020).\n10. Francoeur, P. G. et al. Three-dimensional convolutional neural \nnetworks and a cross-docked data set for structure-based drug \ndesign. J. Chem. Inform. Model. 60, 4200–4215 (2020).\n11. Skalic, M., Sabbadin, D., Sattarov, B., Sciabola, S. & De Fabritiis, G.  \nFrom target to drug: generative modeling for the multimodal \nstructure-based ligand design. Mol. Pharm. 16, 4282–4291 (2019).\n12. Gebauer, N. W. A., Gastegger, M. & Schütt, K. T. Symmetry- \nadapted generation of 3D point sets for the targeted discovery  \nof molecules. Preprint at https://arxiv.org/abs/1906.00957  \n(2019).\n13. Ragoza, M., Masuda, T. & Koes, D. R. Generating 3D molecules \nconditional on receptor binding sites with deep generative \nmodels. Chem. Sci. 13, 2701–2713 (2022).\n14. Luo, S., Guan, J., Ma, J. & Peng, J. A 3D generative model for \nstructure-based drug design. Adv. Neural Inf. Process. Syst. 34, \n6229–6239 (2021).\n15. Liu, M., Luo, Y., Uchino, K., Maruhashi, K. & Ji, S. Generating 3D \nmolecules for target protein binding. Preprint at https://arxiv.org/\nabs/2204.09410 (2022).\n16. Peng, X. et al. Pocket2Mol: efficient molecular sampling based on \n3D protein pockets. Preprint at https://arxiv.org/abs/2205.07249 \n(2022).\n17. Li, Y., Pei, J. & Lai, L. Structure-based de novo drug design using \n3D deep generative models. Chem. Sci. 12, 13664–13675 (2021).\n18. Guan, J. et al. 3D equivariant diffusion for target-aware molecule \ngeneration and affinity prediction. Preprint at https://arxiv.org/\nabs/2303.03543 (2023).\n19. Garcia, S. V., Hoogeboom, E. & Welling, M. E(n) equivariant graph \nneural networks. Preprint at https://arxiv.org/abs/2102.09844 \n(2021).\n20. Hoogeboom, E., Garcia, S. V., Vignac, C. & Welling, M. Equivariant \ndiffusion for molecule generation in 3D. Preprint at https://arxiv.\norg/abs/2203.17003 (2022).\n21. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S. &  \nHopkins, A. L. Quantifying the chemical beauty of drugs.  \nNat. Chem. 4, 90–98 (2012).\n22. Ertl, P. & Schuffenhauer, A. Estimation of synthetic accessibility \nscore of drug-like molecules based on molecular complexity and \nfragment contributions. J. Cheminform. 1, 8 (2009).\n23. Polishchuk, P. G., Madzhidov, T. I. & Varnek, A. Estimation of  \nthe size of drug-like chemical space based on GDB-17 data.  \nJ. Comput. Aided Mol. Des. 27, 675–679 (2013).\n24. Weininger, D. Smiles, a chemical language and information \nsystem. 1. Introduction to methodology and encoding rules.  \nJ. Chem. Inf. Comput. Sci. 28, 31–36 (1988).\n25. Corso, G., Stärk, H., Jing, B., Barzilay, R. & Jaakkola, T. DiffDock: \ndiffusion steps, twists, and turns for molecular docking. Preprint \nat https://arxiv.org/abs/2210.01776 (2022).\n26. Ding, K. et al. Observing noncovalent interactions in experimental \nelectron density for macromolecular systems: a novel \nperspective for protein–ligand interaction research. J. Chem. Inf. \nModel. 62, 1734–1743 (2022).\n27. Lewis, M. et al. BART: denoising sequence-to-sequence \npre-training for natural language generation, translation, and \ncomprehension. Preprint at https://arxiv.org/abs/1910.13461 \n(2019).\n28. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer:  \na pre-trained transformer for computational chemistry.  \nMach. Learn. Sci. Technol. 3, 015022 (2022).\n29. Wang, R., Fang, X., Lu, Y. & Wang, S. The PDBbind database: \ncollection of binding affinities for protein-ligand complexes \nwith known three-dimensional structures. J. Med. Chem. 47, \n2977–2980 (2004).\n30. Watts, K. S. et al. Confgen: a conformational search method for \nefficient generation of bioactive conformers. J. Chem. Inf. Model \n50, 534–546 (2010).\n31. Mysinger, M. M., Carchia, M., Irwin, J. J. & Shoichet, B. K. Directory \nof useful decoys, enhanced (DUD-E): better ligands and decoys \nfor better benchmarking. J. Med. Chem. 55, 6582–6594 (2012).\n32. Mirdita, M., Steinegger, M., Breitwieser, F., Söding, J. & Levy Karin, E.  \nFast and sensitive taxonomic assignment to metagenomic \ncontigs. Bioinformatics 37, 3029–3031 (2021).\n33. Wojcikowski, M., Zielenkiewicz, P. & Siedlecki, P. Open drug \ndiscovery toolkit (ODDT): a new open-source player in the drug \ndiscovery field. J. Cheminform. 7, 26 (2015).\n34. Friesner, R. A. et al. Glide: a new approach for rapid, accurate \ndocking and scoring. 1. Method and assessment of docking \naccuracy. J. Med. Chem. 47, 1739–1749 (2004).\n35. Su, M. et al. Comparative assessment of scoring functions: the \nCASF-2016 update. J. Chem. Inf. Model. 59, 895–913 (2018).\n36. Shen, C. et al. Beware of the generic machine learning-based \nscoring functions in structure-based virtual screening.  \nBrief. Bioinform. 22, bbaa070 (2021).\n37. Wishart, D. S. et al. DrugBank 5.0: a major update to the DrugBank \ndatabase for 2018. Nucleic Acids Res. 46, D1074–D1082  \n(2017).\n38. Jain, A. N., Brueckner, A. C., Cleves, A. E., Reibarkh, M. &  \nSherer, E. C. A distributional model of bound ligand confor-\nmational strain: from small molecules up to large peptidic \nmacrocycles. J. Med. Chem. 66, 1955–1971 (2023).\n39. Gu, S., Smith, M. S., Yang, Y., Irwin, J. J. & Shoichet, B. K. Ligand \nstrain energy in large library docking. J. Chem. Inf. Model. 61, \n4331–4341 (2021).\n40. Ryde, U. & Soderhjelm, P. Ligand-binding affinity estimates \nsupported by quantum-mechanical methods. Chem. Rev. 116, \n5520–5566 (2016).\n41. Wang, L. et al. A pocket-based 3D molecule generative model \nfueled by experimental electron density. Sci. Rep. 12, 15100 \n(2022).\n42. Ma, W. et al. Using macromolecular electron densities to improve \nthe enrichment of active compounds in virtual screening. \nCommun. Chem. 6, 173 (2023).\n43. Xu, M. et al. GeoDiff: a geometric diffusion model for molecular \nconformation generation. Preprint at https://arxiv.org/abs/ \n2203.02923 (2022).\n44. Jing, B., Eismann, S., Soni, P. N. & Dror, R. O. Equivariant graph \nneural networks for 3D macromolecular structure. Preprint at \nhttps://arxiv.org/abs/2106.03843 (2021).\n45. Deng, C. et al. Vector neurons: a general framework for \nSO(3)-equivariant networks. Preprint at https://arxiv.org/abs/ \n2104.12229 (2021).\n46. Simm, G. N. C., Pinsler, R., Csányi, G. & Hernández-Lobato, J. M. \nSymmetry-aware actor-critic for 3D molecular design. Preprint at \nhttps://arxiv.org/abs/2011.12747 (2020).\n47. Landrum, G. et al. RDKit: open-source cheminformatics software. \nGitHub https://github.com/rdkit/rdkit (2016).\n48. Ying, C. et al. Do transformers really perform badly for graph \nrepresentation? Adv. Neural Inf. Process. Syst. 34, 28877–28888 \n(2021).\n49. Feng, W. et al. Data for Lingo3DMol. figshare https://figshare.com/\narticles/dataset/Data_for_Lingo3DMol/24550351 (2023).\n50. Feng, W. et al. Code for Lingo3DMol. figshare https://figshare.\ncom/articles/software/Code_for_Lingo3DMo/24633084  \n(2023).\nNature Machine Intelligence | Volume 6 | January 2024 | 62–73\n 73\nArticle https://doi.org/10.1038/s42256-023-00775-6\n51. Bajusz, D., Racz, A. & Heberger, K. Why is Tanimoto index an \nappropriate choice for fingerprint-based similarity calculations?  \nJ. Cheminform. 7, 20 (2015).\nAcknowledgements\nThis study was funded by the National Key R&D Program of China \n(grant no. 2022YFF1203004 received by B.H.). This work was \nalso supported by the Beijing Municipal Science and Technology \nCommission (grant no. Z211100003521001 received by J.Z.  \nand W.Z.).\nAuthor contributions\nW.Z. and B.H. conceived the study. W.Z. and J.Z. provided instructions \nfor artificial intelligence modelling. B.H. and H.W. provided \ninstructions on evaluation framework. W.F., L.W., Z.L., Y.Z., R.B., H.W. \nand J.D. developed the model. L.W. and R.B. prepared evaluation data. \nW.P. supported molecular docking tests.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-023-00775-6.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-023-00775-6.\nCorrespondence and requests for materials should be addressed to \nBo Huang or Wenbiao Zhou.\nPeer review information Nature Machine Intelligence thanks  \nBenoit Baillif and Sabina Podlewska for their contribution to the  \npeer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this license, visit http://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Table 1 | Comparison of bond lengths between the reference molecules and the generated molecules\nBond Reference Pocket2Mol TargetDiff Lingo3DMol\nMean Std Mean Std Mean Std Mean Std\nC-C 1.52 0.05 1.45 0.11 1.48 0.08 1.51 0.10\nC=C 1.39 0.07 1.37 0.10 1.39 0.07 1.40 0.12\nC:C 1.41 0.04 1.39 0.09 1.39 0.03 1.40 0.07\nC-N 1.42 0.07 1.39 0.10 1.41 0.07 1.46 0.23\nC=N 1.34 0.05 1.34 0.11 1.35 0.06 1.38 0.23\nC:N 1.36 0.03 1.35 0.10 1.36 0.04 1.36 0.07\nC-O 1.41 0.05 1.38 0.09 1.40 0.07 1.40 0.07\nC=O 1.24 0.04 1.26 0.09 1.28 0.05 1.23 0.07\nC:O 1.45 0.02 1.39 0.11 1.41 0.05 1.38 0.04\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Table 2 | Percentage of molecules containing rings with different size\nRing Size Reference Pocket2Mol TargetDiff Lingo3DMol\n3 1.62% 0.12% 0.00% 0.18%\n4 0.00% 0.02% 2.70% 1.28%\n5 29.55% 16.26% 29.71% 34.71%\n6 65.99% 79.83% 48.96% 63.45%\n7 0.81% 2.59% 11.70% 0.23%\n8 0.00% 0.34% 2.59% 0.11%\n9 0.00% 0.12% 0.85% 0.02%\n10+ 2.02% 0.72% 3.48% 0.01%\nComparison of the ring size distribution in molecules generated by different methods.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Table 3 | In-place GlideSP score analysis for DUD-E targets (N=101)\nLingo3DMol Pocket2Mol TargetDiff\n# of molecules generated 100,428 98,332 92,727\n% of molecules with positive in-place GlideSP score 84% 87% 60%\nMean in-place GlideSP score (All molecules) 8,450 8,744 6,127\nMean in-place GlideSP score (Molecules with positive in-place GlideSP score) 9,967 9,980 9,884\nMean in-place GlideSP score (Molecules with Negative in-place GlideSP score) -4.9 -5.1 -5.2\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Table 4 | Inference time for Lingo3DMol, Pocket2Mol and Target- Diff\nLingo3DMol Pocket2Mol TargetDiff\nRunning time (s, ↓) 874 ± 401 962 ± 622 1327 ± 405\nNote: We randomly selected 10 targets from DUD-E and recorded the time taken to generate 100 valid molecules for each target using an NVIDIA Tesla V100.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Fig. 1 | Comparison of atom-atom distance distributions in reference and generated molecules. (a) All atom pairs are considered in the analysis. \n(b) Only carbon-carbon atom pairs are considered.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Fig. 2 | Cases of generated molecules with good min-in-place GlideSP scores but not suitable as drug molecules. Three generated compounds, \nnamely Cpd. A, Cpd. B and Cpd. C, are showcased. These compounds exhibit good min-in-place GlideSP scores but have relatively poor QED or SAS. The generated \nbinding poses used for min-in-place GlideSP scoring are also provided.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Fig. 3 | Comparison between Lingo3DMol and Pocket2Mol \non the DUD-E dataset under varying degrees of information leakage. \nLingo3DMol has limited information leakage by excluding proteins that have \nmore than 30% sequence identity with DUD-E targets from their training set. The \nassessment of information leakage is done from the perspective of Pocket2Mol. \nSpecifically, DUD-E targets were categorized into three groups based on their \nsequency identity with Pocket2Mol training targets: severe (>90%, N = 74), \nmoderate (30-90%, N = 19), and limited (<30%, N = 8) information leakage. \nThe comparisons on average min-in-place GlideSP scores, GlideSP redocking \nscores, and RMSD vs. low energy conformers are listed in panel (a), (b), and (c), \nrespectively.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Fig. 4 | FSMILES based molecule growing process. Nine steps are listed to illustrate the fragment-by-fragment process of generating a molecule \nwithin a pocket.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Fig. 5 | Intuitive visualization of the 3D molecule generation \nprocess. In Step 1, based on the precomputed pocket NCI information, we select \nan NCI as the starting position. Within a radius r, we select the position with the \nhighest global coordinate probability. In each subsequent step, we predict the \nlocal coordinates r, θ, and φ, and combine them with the global coordinates to \ndetermine the final position.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00775-6\nExtended Data Fig. 6 | Distribution of molecular weight for molecules generated by different methods. This figure is provided as supplementary information  \nfor Table 1.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7129068374633789
    },
    {
      "name": "Notation",
      "score": 0.6790933012962341
    },
    {
      "name": "Traverse",
      "score": 0.5753361582756042
    },
    {
      "name": "Generative grammar",
      "score": 0.5679901838302612
    },
    {
      "name": "Representation (politics)",
      "score": 0.5524348020553589
    },
    {
      "name": "Graph",
      "score": 0.471696138381958
    },
    {
      "name": "Directory",
      "score": 0.46966421604156494
    },
    {
      "name": "Generative model",
      "score": 0.4439551830291748
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4349541664123535
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37299710512161255
    },
    {
      "name": "Mathematics",
      "score": 0.09489607810974121
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Arithmetic",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}