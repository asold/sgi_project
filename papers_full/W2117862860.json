{
  "title": "Classification of heterogeneous text data for robust domain-specific language modeling",
  "url": "https://openalex.org/W2117862860",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A4229180404",
      "name": "Ján Staš",
      "affiliations": [
        "Technical University of Košice"
      ]
    },
    {
      "id": "https://openalex.org/A2107154739",
      "name": "Jozef Juhar",
      "affiliations": [
        "Technical University of Košice"
      ]
    },
    {
      "id": "https://openalex.org/A1983026826",
      "name": "Daniel Hládek",
      "affiliations": [
        "Technical University of Košice"
      ]
    },
    {
      "id": "https://openalex.org/A4229180404",
      "name": "Ján Staš",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107154739",
      "name": "Jozef Juhar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983026826",
      "name": "Daniel Hládek",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2494206023",
    "https://openalex.org/W2140062987",
    "https://openalex.org/W2121082043",
    "https://openalex.org/W1717652683",
    "https://openalex.org/W2397093049",
    "https://openalex.org/W4212980771",
    "https://openalex.org/W2033681837",
    "https://openalex.org/W2151752770",
    "https://openalex.org/W2474958513",
    "https://openalex.org/W126222424",
    "https://openalex.org/W2149684865",
    "https://openalex.org/W1482214997",
    "https://openalex.org/W1560796642",
    "https://openalex.org/W72347498",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W1490175560",
    "https://openalex.org/W4307225209",
    "https://openalex.org/W195533127",
    "https://openalex.org/W2076598705",
    "https://openalex.org/W2125145463",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W2165236847",
    "https://openalex.org/W2056167975",
    "https://openalex.org/W2170079456",
    "https://openalex.org/W2288577906",
    "https://openalex.org/W2035259174",
    "https://openalex.org/W2487989883"
  ],
  "abstract": "The robustness of n-gram language models depends on the quality of text data on which they have been trained. The text corpora collected from various resources such as web pages or electronic documents are characterized by many possible topics. In order to build efficient and robust domain-specific language models, it is necessary to separate domain-oriented segments from the large amount of text data, and the remaining out-of-domain data can be used only for updating of existing in-domain n-gram probability estimates. In this paper, we describe the process of classification of heterogeneous text data into two classes, to the in-domain and out-of-domain data, mainly used for language modeling in the task-oriented speech recognition from judicial domain. The proposed algorithm for text classification is based on detection of theme in short text segments based on the most frequent key phrases. In the next step, each text segment is represented in vector space model as a feature vector with term weighting. For classification of these text segments to the in-domain and out-of domain area, document similarity with automatic thresholding are used. The experimental results of modeling the Slovak language and adaptation to the judicial domain show significant improvement in the model perplexity and increasing the performance of the Slovak transcription and dictation system.",
  "full_text": "Staš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nRESEARCH Open Access\nClassification of heterogeneous text data for\nrobust domain-specific language modeling\nJánStaš*,JozefJuhárandDanielHládek\nAbstract\nTherobustnessof n-gramlanguagemodelsdependsonthequalityoftextdataonwhichtheyhavebeentrained.The\ntextcorporacollectedfromvariousresourcessuchaswebpagesorelectronicdocumentsarecharacterizedbymany\npossibletopics.Inordertobuildefficientandrobustdomain-specificlanguagemodels,itisnecessarytoseparate\ndomain-orientedsegmentsfromthelargeamountoftextdata,andtheremainingout-of-domaindatacanbeused\nonlyforupdatingofexistingin-domain n-gramprobabilityestimates.Inthispaper,wedescribetheprocessof\nclassificationofheterogeneoustextdataintotwoclasses,tothein-domainandout-of-domaindata,mainlyusedfor\nlanguagemodelinginthetask-orientedspeechrecognitionfromjudicialdomain.Theproposedalgorithmfortext\nclassificationisbasedondetectionofthemeinshorttextsegmentsbasedonthemostfrequentkeyphrases.Inthe\nnextstep,eachtextsegmentisrepresentedinvectorspacemodelasafeaturevectorwithtermweighting.For\nclassificationofthesetextsegmentstothein-domainandout-ofdomainarea,documentsimilaritywithautomatic\nthresholdingareused.TheexperimentalresultsofmodelingtheSlovaklanguageandadaptationtothejudicial\ndomainshowsignificantimprovementinthemodelperplexityandincreasingtheperformanceoftheSlovak\ntranscriptionanddictationsystem.\nKeywords: Documentsimilarity;Languagemodeling;Speechrecognition;Termweighting;Textclassification;\nTopicdetection\n1 Introduction\nWith an increasing amount of the text data gathered from\nvarious web pages or electronic documents and growing\nn e e df o rm o r ea c c u r a t ea n dr o b u s tm o d e l so ft h eS l o v a k\nlanguage [1], a question of how to classify the text data\naccording to their content arises even more than expected.\nThis question is getting on importance with using hetero-\ngeneous text corpora, in which we do not have any knowl-\nedge about the document boundaries. In the case of the\ntask-oriented speech recognitionand domain-specific lan-\nguage modeling [2], these heterogeneous text data bring\nmany ambiguities caused by the overestimating suchn-\ngram probabilities that are typically unrelated with the\narea of speech recognition into the process of the train-\ning language models. Therefore, we were looking for a way\nof classification of the text data into predefined domains\n*Correspondence:jan.stas@tuke.sk\nDepartmentofElectronicsandMultimediaCommunications,Technical\nUniversityofKošice,ParkKomenského13,04120Košice,Slovakia\nas good way as possible and adjustment of the parame-\nters of language modeling for effectivelarge vocabulary\ncontinuous speech recognition(LVCSR).\nThere are two ways existing for assigning text data into\ndomains; using text classification or document clustering\nwith topic detection. The difference between them is that\nthe text classification is based on assigning the text data\ninto two or more predefined classes, whereasdocument\nclustering tries to group similar documents into a number\nof classes and find some relationship between them. The\nsimilarity of two documents represented by their feature\nvectors is usually based on computing cosine of the angle\nbetween them [3]. After clustering, the topic detection for\nevery cluster of documents is needed [4]. Unlike cluster-\ning, the classification is supervised learning technique and\nrequires the training data for classifying new documents.\nConsidering fact that we need to group text documents\no n l yi n t ot w oc l a s s e s ,w ef o c u s e do u rr e s e a r c ho nt h et e x t\nclassification techniques.\nA growing number of statistical methods have been\napplied to the problem of text classification in recent\n©2014Stašetal.;licenseeSpringer. ThisisanOpenAccessarticledistributedunderthetermsoftheCreativeCommons\nAttributionLicense(http://creativecommons.org/licenses/by/2.0),whichpermitsunrestricteduse,distribution,andreproduction\ninanymedium,providedtheoriginalworkisproperlycredited.\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 2 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nyears, including naïve Bayes classifier and probabilis-\nt i cl a n g u a g em o d e l s[5,6], similarity-based approaches\nusing k-nearest neighbor classifier [5,7], decision trees\nand neural networks [8], support vector machines [5,9],\nor semi-supervised clustering [10]. When large amount\nof documents is used, these algorithms usually suffer\nfrom a very high computational complexity. Moreover,\nfor correct estimation of parameters of these classifica-\ntion algorithms, a training corpus is needed. Therefore,\nwe proposed an algorithm based on computing simi-\nlarity between two documents and decision, which one\nwill appertain to the domain and one which will not,\nusing a threshold value calculated automatically on a\ndevelopment data set. This simple and effective algo-\nrithm classifies short text segments (such as paragraphs)\nfrom heterogeneous text corpora gathered from vari-\nous resources to the in-domain and out-of-domain data.\nClassified text data are then used in statistical language\nmodeling for enhancing its quality and robustness in the\ntask-oriented speech recognition.\nThe rest of this paper is organized as follows. Section 2\nstarts with a short overview about the source data used\neither for text classification, training acoustic and lan-\nguage models, and testing the Slovak LVCSR system.\nOur proposed approach for text classification based on\nthe key phrase identification, term weighting, measur-\ning similarity between two documents, and automatic\nthresholding is introduced in the Section 3. Section 4\npresents the speech recognition setup used for evaluat-\ning language models trained on classified text corpora.\nThe experimental results with adapted models of the Slo-\nvak language into the selected domain are discussed in\nthe Section 5. Finally, Section 6 summarizes the contri-\nbution of our work and concludes this article with future\ndirections.\n2S o u r c e d a t a\n2.1 Acoustic database\nFor testing language models using speech recognition sys-\ntem, the Slovak acoustic database was created, on which\nacoustic models have been trained. Speech database con-\nsists of three subsets (see the Table 1):\n• The first part is characterized by gender-balanced\nspeakers, contains 250 h of speech recordings\nobtained from 250 speakers together and consists of\ntwo parts: APD1 and APD2 databases. The APD1\ndatabase includes 100 h of readings of real\nadjustments from the court with personal data\nchanged, recorded in sound studio conditions. The\nAPD2 database consists of 150 h of read phonetically\nrich sentences, web texts, newspaper articles, short\nphrases, and spelled items, recorded in conference\nrooms using table and close-talk headset\nmicrophones [2].\n• The second PAR database includes 90 h of 90% male\nand 10% female speech recordings realized in the\nmain conference hall of the Slovak Parliament using\nconference gooseneck condenser microphones [11].\n• The mixture of Broadcast news (BN) databases\nconsists of 145 h of speech recordings acquired from\nmain and morning TV shows and 35 h from\nbroadcast news and TV and radio shows, together\nrealized with TV DVB-S PCI card [12].\nAll speech recordings were downsampled to 16-kHz 16-\nbit PCM mono format for training and testing. The whole\nacoustic database was manually annotated by our team of\ntrained annotators using the Transcriber tool [12], double\nchecked, and corrected.\n2.2 Text corpora\nThe main part of text corpora used for text classification\nand statistical language modeling was created by using\nour proposedsystem for gathering text datafrom various\nweb pages and electronic resources written in Slovak lan-\nguage [1]. From the retrieved text data, there was a large\namount of numerals, symbols, or grammatically incorrect\nwords filtered out and the rest of the data were normal-\nized into their pronounced form by additional process-\ning, such as word tokenization, sentence segmentation,\nnumerals transcription, and abbreviations expanding. The\nprocessed text corpora were later divided into smaller\ndomain-specific subcorpora ready for the training lan-\nguage models. Contemporary text corpora consists of\nfollowing subsets:\nTable 1 Acoustic database description\nAcoustic database Hours Sampling Resolution Microphone type Sound environment and conditions\n(kHz) (bit)\nAPD1database 100 48 16 Close-talkheadset Soundstudioconditions\nAPD2database 150 48 16 Close-talkheadset Officesandconferencerooms\nPARdatabase 90 44 24 Gooseneckcondenser MainconferencehalloftheSlovakParliament\nBN1database 145 48 16 TVDVB-SPCIcard Soundstudio,telephone,anddegradedspeech\nBN2database 35 48 16 TVDVB-SPCIcard Soundstudio,telephone,anddegradedspeech\nEvaluation data set 5.25 48 16 Close-talkheadset Soundstudio,offices,andconferencerooms\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 3 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\n• Slovak web corpuswas collected by crawling whole\nweb pages from various Slovak domains saved with\ninformation about date, title, URL, extracted text, and\nHTML source code.\n• Corpus of newspapersis a collection of articles that\nhave been gathered from the most popular online\nnews portals, magazines, and journals in the Slovak\nRepublic. This corpus was extended by a large\namount of newspaper articles downloaded via RSS\nchannels and collection of manually corrected speech\ntranscriptions of four main TV broadcast news and\nfive radio shows.\n•\nCorpus of legal texts(judicial corpus) was obtained\nfrom the Ministry of Justice of the Slovak Republic in\norder to develop the automatic dictation system for\ntheir internal purpose [2].\n•\nCorpus of fiction textswas created from1, 625\nelectronic books and other stories freely available on\nthe Internet written in Slovak language.\n• Corpus of contemporary blogsconsists of\nweb-extracted blog texts from main news portals in\nthe Slovak Republic saved without contribution’s\ncomments.\n•\nDevelopment data set(held-out data) was created\nfrom 10% randomly selected sentences from\n(in-domain) corpus of legal texts that were not used\nin the process of training language models.\n•\nSpeech annotations(transcriptions) of data obtained\nfrom acoustic database are a special portion of the\ntext corpus. Transcriptions also contain a large\namount of filled pauses and additional disfluent\nspeech events together with useful text. We have\ndiscovered that filled pauses have a positive effect on\nthe quality of language modeling, both for dictated or\nspontaneous speech. Therefore, we decided to\ninclude these speech transcriptions into the process\nof language modeling.\nThe complete statistics on the total number of tokens\nand sentences for particular text subcorpus are summa-\nrized in the Table 2.\nTable 2 Statistics on the text corpora\nText corpus Tokens Sentences Documents\nSlovakwebcorpus 748,854,697 50,694,708 2,803,412\nCorpusofnewspapers 554,593,113 36,326,920 2,022,483\nCorpusoflegaltexts 565,140,401 18,524,094 1,503,271\nCorpusoffictiontexts 101,234,475 8,039,739 367,956\nCorpusofcontemporaryblogs 55,711,674 4,071,165 211,533\nDevelopmentdataset 55,163,941 1,782,333 165,577\nSpeechannotations 4,434,217 485,800 5,520\nTotal 2,085,132,518 119,924,759 7,079,752\nMoreover, each text corpus was annotated using our\nproposed Slovak morphological classifier[13] based on a\nhidden Markov model (HMM) together with suffix-based\nword clustering function and restricted bymanually mor-\nphologically annotated lexicon of words.T h eH M Mh a s\nbeen trained on trigram statistics generated frommor-\nphologically annotated corpus together with the lexicon\ndelivered by the Slovak National Corpus [14]. Note that\nthe morphologically annotated corpus were then used in\nthe process of extraction of key phrases from development\ndata set of the proposed algorithm for classification of\nheterogeneous text data.\n3 Proposed text classification approach\nAs it was mentioned before, we proposed an effective\napproach for classification of heterogeneous text corpora\ninto the two data sets, the in-domain and out-of-domain\ndata, to increase the robustness of domain-oriented sta-\ntistical language modeling in the Slovak LVCSR system.\nOur algorithm is based on identifying key phrases with\ntheir occurrences in short text segments. Each text docu-\nment is represented as a vector of key phrases in a vector\nspace (a key phrase/document matrix). For reducing the\ninfluence of frequent key phrases in documents, term\nweighting was applied. The next step includes measuring\nthe similarity between reference and examined document\nto determine the closeness between them. Based on the\nautomatic thresholding, the algorithm then decides which\ntext document belongs or does not belong to the exam-\nined domain (in our case to the judicial one). The block\nscheme of the proposed text classification approach is\ndepicted in Figure 1.\nIn the following sections, the proposed text classifica-\ntion approach is described in more detail.\n3.1 Key phrase extraction\nThe first step in the process of classification of the text\ndata is to propose an algorithm for extracting key phrases\nfrom examined domain (from development data). Based\non morphologically annotated corpora, described in the\nSection 2.2, we created a set of 14morpho-syntactic pat-\nterns for extracting bigrams, trigrams, and quadrigrams\nfrom this corpora, summarized in the Table 3. Morpho-\nsyntactic patterns take into account part of speech of the\ncorresponding words and syntactic dependency between\nthem, unlike other statistical approaches based on com-\nputing pointwise mutual information,t score orχ\n2 score\nbetween n words. In order to prevent any occurrence of\nkey phrases from other domains in this list, we filtered\nout all key phrases from the other out-of-domain corpora,\nexcept corpus of legal texts. Using this approach, we cre-\nated a list of 5, 210 in-domain key phrases that are later\nused in the block key phrase identification and measur-\ning similarity between two documents. More details and\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 4 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nFigure 1The block scheme of the proposed approach for text classification.\nbackground on how the set of morpho-syntactic patterns\nwere created can be found in [15].\n3.2 Text segmentation\nIn general, text data gathered from the Internet are char-\nacterized by a large variety of domains or topics that are\ncontained in the web articles, from which the text corpus\nis composed. Moreover, in case of large-scale text docu-\nments, they may also contain more than one theme within.\nAs it was mentioned earlier, this problem is gaining on\nimportance when using heterogeneous text corpora, in\nwhich we have no knowledge about the document bound-\naries. Therefore, the next step in the text classification\nprocess includes segmentation of the used text corpora\ninto the small segments (paragraphs) with at least 300\nwords. This value was determined empirically from the\nTable 3 Morpho-syntactic patterns\nType Characterization Scheme\n2-gram Adjective+noun AS\nNumeral+noun NS\nNoun+noun SS\nAbbreviation+noun WS\n3-gram Adjective+adjective+noun AAS\nAdjective+noun+noun ASS\nAdverb+adjective+noun DAS\nNumeral+adjective+noun NAS\nNoun+adjective+noun SAS\nNoun+preposition+noun SES\nNoun+numeral+noun SNS\n4-gram Noun+preposition+adjective+noun SEAS\nNoun+preposition+noun+noun SESS\nNoun+noun+conjunction+noun SSOS\nstatistical observation and expresses the average number\nof words contained in one paragraph of a web-based arti-\ncle. By application of segmentation rules, we obtained a\ntotal of 6, 908, 655 short (300+ words) text segments -\ndocuments - entering to the process of text classification.\nThe statistics on the number of documents after text seg-\nmentation for particular subcorpus are resumed in the\nTable 2.\n3.3 Key phrase identification\nIn the next step, the key phrases were used incomput-\ning the frequency of their occurrence in examined text\nsegments of 300+ words. The key phrase identification\nprocess is similar to any topic detection approach. How-\never, in this process we have not considered removal\nof stop-words, because key phrases extracted using pro-\nposed morpho-syntactic patterns contained such part-of-\nspeech classes as prepositions or conjunctions (see the\nTable 3). Also lemmatization (or stemming) is very time-\nconsuming and would cause high memory requirements,\ntherefore it has not been introduced into this process of\ntext classification. Note that text segments that did not\ncontain any key phrases were automatically classified as\nout-of-domain data.\n3.4 Vector space modeling\nOne of the simplest way how to represent the occurrence\nof terms (key words or key phrases) in any text document\nis to use avector space model(VSM). In eachith docu-\nment, ¯di is represented as a feature vector of the termstj\nthat appear in this document as follows [5]:\n¯di = (ti,1, ti,2, ... , ti,N ).( 1 )\nUsing this approach, each short text segment was rep-\nresented by a vector of 5, 210 key phrases. With respect\nto the number of documents in the collection (see the\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 5 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nSection 3.2), we have received the matrix with 5, 210\ncolumns and 6, 908, 655 rows. However, the main disad-\nvantage of such representation is a very high dimension\nof this matrix and sparsity of values in the vector space,\nresulting in very high requirements on its storage.\n3.5 Term weighting\nAs it was mentioned earlier, term weighting was applied as\na feature selection algorithm for reducing the influence of\nfrequently occurring terms in a collection of documents.\nIn this research, we have tested three different weighting\nschema: (a) tf-idf, (b) Okapi BM25, and (c) Ltu factors.\nThe conventional term weighting came from the com-\nputing frequency of a term in a document using theterm\nfrequency and the frequency in the collection of docu-\nments in which the term appears, which is expressed as\nthe document frequency. A number of term weighting\nschemes based on these two frequency functions exist\nsuch as idf - inverse document frequency, expressed by\nthe negative reciprocal value of the document frequency;\nridf -residual idf, defined as the difference between actual\nidf and logarithm of idf predicted by Poisson distribution\nin a term distribution model; tf-idf and tf-ridf that com-\nbines term frequency and document frequency into one\nalgorithm, which can be scaledlogarithmically or normal-\nized byaugmented version [5]. Moreover, term weighting\ndoes not have to be performed on the entire collection of\ndocuments. It can be calculated on a small training corpus\nand used in clustering dynamic data streams using tf-icf\nweighting [16].\nBased on the previous research [17] focused on a com-\nparative study of term weighting schemes, we observed\nthat standard tf-idf achieved the best results in clustering\nof the Slovak text documents obtained from Wikipedia.\nTherefore, we used this weighting scheme in the proposed\nclassification too.\nThe tf-idf is a standard term weighting scheme used in\ninformation retrieval or data mining and combines term\nfrequency and inverse document frequency together. The\nimportance of tf-idf increases proportionally to the occur-\nrence of a word in the document and is offset by the\nfrequency of the word in the collection of documents\naccording to formula [5]\nw\ni,j = tfi,j × idfi = fi,j∑\nk fk,j\n× log N\ndfi\n,( 2 )\nwhere fi,j i st h en u m b e ro fo c c u r r e n c eo fat e r mti in a\ndocument dj and sum in the denominator of tfi,j com-\nponent expresses the number of occurrences of all terms\nt\ni in dj.T h e n ,N is the total number of documents, and\nthe denominator of idfi component expresses the total\nnumber of documents in a collection thatti occurs in\nwell-known as document frequency dfi.\nContemporary term weighting schemes take into\naccount additional factors such asm a x i m u mo ft e r mf r e -\nquency max(tfi,j) in a document,length of a documentdli,\nor average document lengthdlavg in a collection of docu-\nments. Between these, we can fit a simpleautomated text\nclassification (ATC), which uses the idf as the term impor-\ntance factor and Euclidean vector length as the document\nlength normalization factor, either Okapi BM25 or Ltu\nscoring [18] that were used in our experiments.\nThe Okapi BM25 score is defined as a bag-of-words\nretrieval function that ranks a collection of documents\nregardless of the inter-relationship between the terms\nwithin a document [5]. It is based on computing BM25-tf\nscore and idf component derived from the binary inde-\npendence model that is well-known from the probabilistic\ntheory in the information retrieval [19]:\nw\ni,j = BM25 − tfi,j × idf∗\ni = tfi,j\n0.5 + 1.5 × dli\ndlavg + tfi,j\n× log N − dfi + 0.5\ndfi + 0.5 ,\n(3)\nwhere tfi,j means term frequency,N is a total number of\ndocuments in the collection, dfi presents the document\nfrequency, dli document length and dlavg the average doc-\nument length for the collection. In addition, we can put\nthe Okapi BM25 scoring into the tf-idf scheme, which was\npresented in [20].\nIn Ltu term weighting scheme, L factor expresses the\nlogarithm of the term frequency,tf a c t o rt h einverse docu-\nment frequency,a n dut h elength normalization factor as\nfollows [21]:\nw\ni,j = L×t×u = (log tfi,j +1)×log N\ndfi\n× 1\n0.8 + 0.2 × dli\ndlavg\n.\n(4)\nAs we can see from these equations, both the Okapi\nBM25 and Ltu scores are only a certain variation of the\nconventional tf-idf weighting.\nThe problem of data sparsity and high dimension of\nVSM after term weighting can be efficiently eliminated\nusing latent semantic analysis/indexing (LSA/LSI) or its\nprobabilistic (pLSA) version that projects terms and doc-\numents into a space of co-occurring terms, also byprin-\ncipal component analysis (PCA), based on a singular\nvalue or eigen-value decomposition of a term/document\nmatrices [22]. However, this space reduction is very time-\nconsuming and computationally intensive considering a\nlarge amount of documents in our collection. There-\nfore, they were not implemented into the process of text\nclassification.\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 6 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\n3.6 Document similarity measurement\nThe next step involves measuring similarity of two docu-\nments. In this approach, we measured the document simi-\nlarity between reference and examined texts, not between\nall documents in a collection, commonly used in the\ntasks oriented on the document clustering. The reference\ntext contained weighted form of all key phrases which\noccurred in a development data set. Both reference and\nexamined text documents were represented by the vector\nof 5, 210 key phrases weighted according to the selected\nweighting scheme, described in the Section 3.5, so they\ncould be compared.\nBy empirical study of numerous similarity measures\ndescribed in [23], we have chosen three different mea-\nsures: (a) Bhattacharyya coefficient, (b) Jaccard correla-\ntion index, and (c) Jensen-Shannon divergence, satisfying\nthe conditions of non-negativity, symmetricity, triangle\ninequality,a n didentity, when distance is equal to 0.\nFor clustering phonemes in the process of training\nacoustic models, the Bhattacharyya coefficient is often\nused. In general, it can be used as a classification criterion\nin many other tasks oriented on clustering in information\ntheory. Therefore, we used this coefficient as one classi-\nfication criterion. Bhattacharyya coefficient comes from\nthe sum of geometric meansbetween two probability den-\nsity functions and specifies the separability of two classes\nx and y as follows:\ndBha =− ln\nN∑\ni=1\n√xiyi. (5)\nOn the contrary,Jaccard correlation indexis defined as\na harmonic mean between two probability density func-\ntions and expresses a scalar sum of two vectors. It comes\nfrom equation on computing cosine similarity [5], nor-\nmalized by absolute deviation of two distributionsx and y\naccording to the formula\ndJac =\n∑N\ni=1(xi + yi)2\n∑N\ni=1 x2\ni + ∑N\ni=1 y2\ni − ∑N\ni=1 xiyi\n. (6)\nJensen-Shannon divergencecomes from the principle of\nuncertainty. It is often used in information theory and\nnatural language processing as a special case ofrelative\nentropy approachsimilar to the averaged Kullback-Leibler\ndivergence, satisfying the condition of symmetry in the\nentire range of values. For two probability density func-\ntions x and y,i ti sc o m p u t e da s\ndJS = 1\n2\nN∑\ni=1\nxi ln\n( 2xi\nxi + yi\n)\n+ 1\n2\nN∑\ni=1\nyi ln\n( 2yi\nxi + yi\n)\n. (7)\n3.7 Automatic thresholding\nThe last step in the classification process is to correctly\nadjust the threshold that determines which documents\nwill appertain to the in-domain and which to the out-\nof-domain area. In general, this value is usually deter-\nmined empirically from long-term observation or can\nbe adjusted automatically based on a set of statistic val-\nues derived from development data. There are many\nalgorithms for automatic thresholding. A comprehensive\nstudy about those can be found in [24].\nWe used themedian (centroid) of a sequence of coef-\nficients derived from a set of values determining the\nsimilarity of two documents as a method of automatic\nthresholding (see the Section 3.6). The threshold value\nwas calculated on a development data set and its acqui-\nsition shares the same process with classification of the\ntext data described in the previous sections. This means\nthat the development data were divided into short text\nsegments consisting of at least 300 words, represented\nby VSM through the key phrases, and weighted, and\neach document was compared with the reference text\n(weighted list of key phrases) using one of the presented\nsimilarity measure. Using this process, we get a list of the\ncoefficients (one coefficient for each document in devel-\nopment data set) expressing distance to the target domain.\nThis list was sorted and the median value was selected as\na threshold.\nIn the Table 4, we can find the statistics of the number\nof in-domain and out-of-domain documents after apply-\ning the proposed classification approach to the segmented\ntext corpora for different term weighting scheme and\ndistance measure used in the step of measuring similar-\nity between the reference and examined documents with\nautomatic thresholding.\nThe performance between in-domain and out-of-\ndomain language models is summarized in the Table 5.\nModel perplexity evaluated on a development data set was\nused for testing the quality of the language models. Its\ncalculation will be introduced in the next section.\n4 Speech recognition setup\n4.1 Decoding\nFor evaluation of the quality of language modeling after\ntext classification and performance of the Slovak LVCSR,\nTable 4 The number of documents after text classification\nSimilarity/weighting tf-idf Okapi Ltu\nIn-domaindataset\nBhattacharyyacoefficient 1,166,806 607,004 698,061\nJaccardcorrelationindex 1,258,169 537,729 699,033\nJensen-Shannondivergence 2,305,230 956,243 698,062\nOut-of-domaindataset\nBhattacharyyacoefficient 5,741,849 6,301,651 6,210,594\nJaccardcorrelationindex 5,650,486 6,370,926 6,209,622\nJensen-Shannondivergence 4,603,425 5,952,412 6,210,593\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 7 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nTable 5 Model perplexity for particular language models\ncomputed on development data\nSimilarity/weighting tf-idf Okapi Ltu\nIn-domaindataset\nBhattacharyyacoefficient 14.1223 15.7542 17.2876\nJaccardcorrelationindex 14.0815 14.8402 17.2872\nJensen-Shannondivergence 15.0343 15.4863 17.2878\nOut-of-domaindataset\nBhattacharyyacoefficient 90.6770 25.7417 183.670\nJaccardcorrelationindex 75.0398 20.7094 162.901\nJensen-Shannondivergence 99.8450 24.3595 187.167\nwe configured a speech recognition setup based on Julius,\nan open-source continuous speech recognition engine\n[25]. Julius usestwo-level Viterbi search algorithm,w h e n\ninput speech is processed in the forward search with\nbigram model, and the final backward search is performed\nagain using the result obtained from the first search to\nnarrow the search space with reverse language model of\nthe highest order (in our case with trigram model). Pro-\nposed speech recognition setup is depicted in the Figure 2.\n4.2 Acoustic modeling\nThe speech recognition setup involves a set oftriphone\ncontext-dependent acoustic modelsbased on HMMs. All\nmodels have been generated from feature vectors contain-\ning 39 mel-frequency cepstral (MFC) coefficients,w h e r e\neach of four states had been modeled by 32 Gaussian\nmixtures. Acoustic models have been trained on four\ndatabases of annotated speech recordings, described in\nthe Section 2.1, using HTK Toolkit. The training set also\ninvolves model of silence, short pause, and additional\nFigure 2The Slovak LVCSR system.\nnoise events. Rare triphones have been modeled by the\neffective triphone mapping algorithm[11].\n4.3 Language modeling\nThe experimental results have been performed taking an\nadvantage of trigram models created using the SRI LM\nToolkit [26], restricted by the vocabulary size of 325, 555\nunique words and smoothed by theWitten-Bell back-off\nalgorithm. All models have been trained on the processed\ntext corpora size of about 2 billion of tokens in 120 million\nof sentences (see the Table 2) and divided into two parts,\nto the in-domain and out-of-domain data, after text clas-\nsification (see the Table 4). Particular models trained on\nin-domain and out-of-domain data were combined with a\nmodel trained on the small portion of text data obtained\nfrom speech transcriptions (see the Table 2). Finally, the\nresulting trigram model was composed from three inde-\npendent models andadapted to the judicial domainusing\nlinear interpolationwith computing interpolation weights\nby our proposed algorithm based on theminimization of\nperplexity on a development data set. The complete pro-\ncess of building the Slovak language models is depicted on\nthe Figure 3 and described in [1].\nIn this article we have compared the contribution of\nchanges performed in the vocabulary, also using better\ntext preprocessing steps, adding new text data, or intro-\nducing new principles into the Slovak language modeling\nduring the recent time periods. These contributions and\ndifferences between language models are summarized in\nthe Table 6.\nDuring this period, the named entities such as people\nnames, surnames, and geographical items were assigned\ninto the word classes in recognition dictionary. The\nvocabulary has been continually updated with the new\nwords, checked, and corrected. We have introduced filled\npauses into the language modeling as transparent words\nand model some geographically named entities as multi-\nwords. We have also tested a number of methods for lan-\nguage model adaptation to the ted domain and algorithms\nfor text classification and clustering.\n4.4 Evaluation\nFor evaluation of the Slovak language models after text\nclassification, three standard measures have been used.\nAccuracy (Acc) andCorrectness (Corr) are the standard\nextrinsic measures for evaluating the performance of the\nLVCSR system. IfN is the total number of words in an\nevaluation data set (reference),S, I,a n dD reflect the total\nnumber of substituted, inserted, and deleted words in rec-\nognized hypothesis, respectively, andH = N − (S + D) is\nthe total number of words in hypothesis, then\nAcc = H − I\nN and Corr = H\nN .( 8 )\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 8 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nFigure 3The block scheme of the process of building the Slovak language models.\nFor intrinsic evaluation of the quality of language mod-\neling, the model perplexity has been used.Model perplex-\nity (PPL) is defined as the reciprocal value of the weighted\n(geometric) probability assigned by the language model to\neach word in the test set and is related to cross-entropy\nH(W) by the equation\nPPL = 2H(W) = 1\nn√P(W),( 9 )\nwhere P(W) is the probability of sequence ofn words in a\nlanguage model.\nThe evaluation data set used for testing the perfor-\nmance of the LVCSR system and the quality of the Slovak\nlanguage modeling after text classification were repre-\nsented by randomly selected segments from the APD\ndatabases (see the Section 2.1, Table 1) containing 1, 950\nmale and 1, 476 female speech utterances with total length\nof about 5.25 h. These speech segments were not used in\nthe training of acoustic models and contain 41, 868 words\nin 3, 426 sentences and short phrases. We have decided to\ninclude also short phrases in the test set because people\nmake pauses in real conditions not only on the sentence\nboundaries, but also on phrase boundaries, usually before\nconjunctions.\n5 Experimental results\nThe experiments have been oriented on the evaluation\nof the model perplexity and performance of the Slovak\nLVCSR system on the evaluation (test) data after text clas-\nsification and statistical modeling of the Slovak language\nfrom judicial domain. The selection of this domain was\nintentional concerning our research oriented on develop-\nment of the Slovak automatic dictation and transcription\nsystem for the Ministry of Justice of the Slovak Republic\nin recent years [2]. The same approach for text clas-\nsification and statistical language modeling can be also\nused for several other domains, in the task of broad-\ncast news transcription, meeting speech recognition,\netc.\nAs it was mentioned in the Section 3, the statistics on\nthe numbers of in-domain and out-of-domain documents\nafter text classification regarding the used term weighting\nscheme in combination with selected similarity measure\na r er e s u m e di nt h eT a b l e4 .\nTable 6 Differences in the text processing and language modeling during the recent time periods\nPeriod\nDec 2011 Jul 2012 Dec 2012 Apr 2013 May 2013\nNo.ofpronunciationvariants 475,156 475,357 474,456 474,453 474,453\nNo.ofuniquewordforms 326,299 326,295 325,555 325,555 325,555\nNo.ofwordsunderclasses 97,471 97,680 97,678 97,678 97,678\nNo.ofclassesofwords 20 22 22 22 22\nNo.oftransparentwords 4 5 5 5 5\nVocabularyextension •• • • -\nWordclassesextension •• -- -\nAddingnewtextdata • -- ••\nAdditionaltextprocessing • - •• •\nFilledpausemodeling - ••• •\nNewtextclassification • --- •\n• Change was performed.\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 9 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nTable 7 Language model perplexity and performance of Slovak LVCSR system with different acoustic models\nAPD1+APD2 250 h APD1+APD2 250 h APD1+APD2 APD1+APD2\nText (table mic.) (close-talk mic.) +PAR 340 h +PAR+BN 520 h\nPPL classification sp. adapt.: no sp. adapt.: no sp. adapt.: no sp. adapt.: no\neval. set: gender-bal. eval. set: gender-bal. eval. set: gender-bal. eval. set: gender-bal.\nWeighting Similarity Acc % Corr % Acc % Corr % Acc % Corr % Acc % Corr %\n40.4302 Referencelanguagemodel 91.84 93.08 93.61 94.51 94.36 95.13 94.06 94.89\n36.0428 tf-idf Bhattacharyya 92.44 93.64 93.99 94.85 94.70 95.46 94.36 95.13\n35.9444 Jaccardindex 92.46 93.65 93.97 94.85 94.72 95.47 94.37 95.16\n38.1756 Jensen-Shannon 92.23 93.39 93.78 94.70 94.50 95.25 94.21 94.99\n38.1289 Okapi Bhattacharyya 92.17 93.34 93.77 94.65 94.61 95.34 94.27 95.02\n39.9782 Jaccardindex 92.10 93.31 93.60 94.54 94.48 95.21 94.11 94.89\n39.2267 Jensen-Shannon 92.27 93.42 93.77 94.67 94.61 95.36 94.18 94.95\n40.1325 Ltu Bhattacharyya 91.86 93.12 93.57 94.51 94.42 95.16 94.05 94.87\n40.1439 Jaccardindex 91.87 93.12 93.56 94.50 94.40 95.16 94.04 94.87\n40.1319 Jensen-Shannon 91.87 93.12 93.57 94.51 94.42 95.16 94.05 94.87\nAs we can see from this results, we achieved the best\nclass separation of in-domain and out-of-domain data\nin combination of Okapi BM25 weighting with similar-\nity based on computing Jaccard correlation index. Using\nthis combination, we yielded the in-domain data with the\nbest possible concentration of key phrases in it. On the\ncontrary, the worst separation of classes was observed\nwhen using tf-idf weighting and Jensen-Shannon diver-\ngence. Although this combination gives the largest num-\nber of text documents in the in-domain corpus, it has\na much weaker concentration of key phrases in it. If\nwe review the Ltu weighting, similar results of class\nseparation were noticed for any similarity measure we\nhave chosen. It would be interesting to discover the over-\nlap between classes for the same term weighting and\ndifferent distance/similarity measure. Their intersection\nor union could produce more interesting results in the\nfuture.\nHowever, if we look at the performance between in-\ndomain and out-of-domain language models using per-\nplexity evaluated on development data summarized in the\nTable 5, the text classification using tf-idf weighting with\nmeasuring similarity based on computing Jaccard correla-\ntion index or Bhattacharyya coefficient predetermines the\nTable 8 Language model perplexity and performance of the Slovak LVCSR system with gender-dependent acoustic\nmodels\nAPD1+APD2 APD1+APD2 APD1+APD2 APD1+APD2\nText +PAR 340 h +PAR 340 h +PAR 340 h +PAR 340 h\nPPL classification sp. adapt.: female sp. adapt.: male sp. adapt.: female sp. adapt.: male\neval. set: gender-bal. eval. set: gender-bal. eval. set: female sp. eval. set: male sp.\nWeighting Similarity Acc % Corr % Acc % Corr % Acc % Corr % Acc % Corr %\n40.4302 Referencelanguagemodel 90.15 91.68 92.72 93.80 95.72 96.48 94.10 94.87\n36.0428 tf-idf Bhattacharyya 91.23 92.50 93.23 94.18 95.97 96.68 94.34 95.06\n35.9444 Jaccardindex 91.26 92.55 93.24 94.22 95.98 96.68 94.73 95.11\n38.1756 Jensen-Shannon 90.71 92.10 92.92 93.94 95.81 96.54 94.23 94.94\n38.1289 Okapi Bhattacharyya 90.95 92.23 93.03 94.01 95.88 96.59 94.25 94.96\n39.9782 Jaccardindex 90.59 91.99 92.82 93.84 95.81 96.53 94.17 94.90\n39.2267 Jensen-Shannon 90.93 92.27 93.00 93.97 95.94 96.65 94.17 94.89\n40.1325 Ltu Bhattacharyya 90.19 91.70 92.72 93.78 95.73 96.49 94.10 94.85\n40.1439 Jaccardindex 90.18 91.70 92.73 93.78 95.76 96.51 94.11 94.86\n40.1319 Jensen-Shannon 90.18 91.70 92.72 93.78 95.73 96.49 94.10 94.85\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 10 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nTable 9 Model perplexity and performance of Slovak LVCSR system with different language and acoustic models\nAPD1+APD2 250 h APD1+APD2 250 h APD1+APD2 APD1+APD2\nLanguage (table mic.) (close-talk mic.) +PAR 340 h +PAR+BN 520 h\nPPL model sp. adapt.: no sp. adapt.: no sp. adapt.: no sp. adapt.: no\n(period) eval. set: gender-bal. eval. set: gender-bal. eval. set: gender-bal. eval. set: gender-bal.\nAcc % Corr % Acc % Corr % Acc % Corr % Acc % Corr %\n44.9254 Dec.2011 91.89 93.09 93.44 94.39 94.21 94.98 93.90 94.68\n38.9688 Jul.2012 92.33 93.55 93.78 94.69 94.46 95.26 94.30 95.11\n40.2543 Dec.2012 92.47 93.66 93.86 94.77 94.65 95.43 94.38 95.19\n44.3262 Apr.2013 92.35 93.56 93.76 94.69 94.53 95.33 94.30 95.12\n35.9444 May2013 92.46 93.65 93.97 94.85 94.72 95.47 94.37 95.16\noptimal combination in terms of the quality the text seg-\nments used only for in-domain language modeling. Using\nLtu factor, we observed significant degradation in the per-\nplexity of language models trained not only on in-domain,\nbut also on out-of-domain text data for each selected sim-\ni l a r i t ym e a s u r e .T h i si sp r o b a b l yc a u s e db yi n a p p r o p r i a t e\ns e t t i n go fat h r e s h o l di nt h el a s ts t e po ft h ep r o p o s e d\nalgorithm.\nAs regards the overall results performed on the ran-\ndomly selected speech utterances from judicial domain,\nthe first part of the experiments presented in the Tables 7\nand 8 were oriented on the computing of model perplexity\nand performance of the Slovak LVCSR system after lan-\nguage modeling trained on classified text corpora using\nproposed approach.\nThe first table summarizes the performance of the\nSlovak language modeling using acoustic models\ntrained on different speech databases, described in the\nSection 2.1. The results have shown that increasing the\namount of acoustic data that were close to the examined\ndomain with similar recording environment improved the\nrecognition accuracy. On the other hand, the BN database\ndegraded the results because the recording environment\nwas quite different to the evaluation data selected from\nthe APD databases.\nThe second table presents the quality of language mod-\neling using gender-dependent acoustic models (optimized\nto male and female speech) trained on the APD1, APD2\nand PAR databases, giving the best results in previous\nexperiment.\nIn the first two columns of the Table 8, the experi-\nmental results with acoustic models adapted to the male\nand female gender of speaker evaluated on the whole\ntest data set are presented. The next two columns show\nthe performance of language models in combination of\ngender-dependent acoustic models evaluated on the test\nspeech utterances per gender.\nAs we can see from these results, gender-dependent\nacoustic modeling can significantly improve the recogni-\ntion accuracy. If we look at the language model perplexity,\nwe have achieved significant reduction about 11% rel-\natively in comparison with the reference model trained\non unclassified text corpora, if we applied combination\nof tf-idf weighting with similarity based on Jaccard cor-\nrelation index in the text classification process. Similar\nresults were obtained in the accuracy and correctness\nevaluated by the LVCSR system. Slightly worse results\nwere noticed when using the Okapi BM25 and Ltu weight-\ning in combination with one of the selected similarity\nmeasure. However, we can say that the proposed text clas-\nsification approach had a significant impact on the overall\nrobustness of the Slovak language modeling.\nThe second part of the experiments presented in the\nTables 9 and 10 show the progress of acoustic and\nTable 10 Model perplexity and performance of the Slovak LVCSR system with different language and gender-dependent\nacoustic models\nAPD1+APD2 APD1+APD2 APD1+APD2 APD1+APD2\nLanguage +PAR 340 h +PAR 340 h +PAR 340 h +PAR 340 h\nPPL model sp. adapt.: female sp. adapt.: male sp. adapt.: female sp. adapt.: male\n(period) eval. set: gender-bal. eval. set: gender-bal. eval. set: female sp. eval. set: male sp.\nAcc % Corr % Acc % Corr % Acc % Corr % Acc % Corr %\n44.9254 12/2011 90.34 91.70 92.68 93.72 95.77 96.48 93.93 94.72\n38.9688 07/2012 91.23 92.53 93.18 94.24 95.85 96.61 94.21 95.00\n40.2543 12/2012 91.28 92.60 93.22 94.25 95.93 96.70 94.30 95.05\n44.3262 04/2013 91.26 92.58 93.24 94.22 95.92 96.67 94.21 94.99\n35.9444 05/2013 91.26 92.55 93.25 94.27 95.97 96.68 94.73 95.11\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 11 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nlanguage modeling in development of the Slovak tran-\nscription and dictation system from the judicial domain,\nobserved during the recent time periods.\nWith increasing amount of acoustic and linguistic data\nfrom the judicial domain, using gender-dependent acous-\ntic modeling and speaker adaptation based on max-\nimum likelihood linear regression (MLLR) as well as\nmuch better text preprocessing and classification for\nrobust domain-specific language modeling, we achieved\nthe speech recognition accuracy nearly 95% with a sig-\nnificant decrease in language model perplexity. Besides\nthe better text processing and classification of training\ndata, this result was achieved either by introducing classes\nof names, surnames, and other named entities into the\nrecognition dictionary; representation of geographically\nnamed entities and technical terms by multiword expres-\nsions; by modeling of filled pauses in a language; or by\neffective adaptation of language models to the ted domain\n(see the Table 6).\nIn the future, we want to build also a new evaluation data\nset containing different acoustic environments to compare\nthe performance of the Slovak LVCSR system for mixed\nend-user environments.\n6C o n c l u s i o n s\nThis paper proposed an algorithm for classification of\nheterogeneous text corpora to the in-domain and out-\nof-domain data with the aim of increasing robustness\nand quality of the statistical language modeling in task-\noriented continuous speech recognition. By combining\nstraightforward and effective methods used for text classi-\nfication and document clustering based on topic detection\nwith key phrases in short text segments, term weighting,\nmeasuring similarity between documents and automatic\nthresholding, we have achieved significant improvement\nin the quality of modeling of the Slovak language and per-\nformance of the Slovak automatic transcription and dicta-\ntion system. The proposed algorithm can also be used in\nclassification of heterogeneous text corpora into the other\ndomains depending on the used development data.\nFurther research should be also focused on a better\nkey phrase extraction in fully unsupervised manner with-\nout using morphologically annotated corpora or applica-\ntion of dimensionality reduction based on singular value\ndecomposition and using latent semantic indexing or\nprincipal component analysis for better representation of\ntext documents in the vector space despite of very high\ntime and memory requirements of this process. Based on\nthe initial tests with document clustering using the latent\nDirichlet allocation, our proposed classification approach\ngives the similar results in the model perplexity as well as\nthe recognition accuracy of the Slovak LVCSR system.\nBesides the better text preprocessing and classifica-\ntion of the training data, the robustness and quality of\nmodeling of the Slovak language can be enhanced by\naddition of large amount of text data from transcripts of\nreal speech recordings, introducing modeling of disflu-\nent speech in a language, or by adaptation of language\nmodels to a specific user, group of users, or conversation,\ndepending on the speech recognition task in which they\nwill be used, for example, broadcast news transcription or\nmeeting speech recognition.\nCompeting interests\nTheauthorsdeclarethattheyhavenocompetinginterests.\nAcknowledgements\nTheresearchpresentedinthispaperwaspartiallysupportedbytheMinistryof\nEducation,Science,ResearchandSportoftheSlovakRepublicunderthe\nresearchprojectsMSSR3928/2010-11(20%)andVEGA1/0386/12(30%)and\ntheResearchandDevelopmentOperationalProgramfundedbytheERDF\nundertheprojectITMS-26220220141(50%).\nReceived: 20 November 2013 Accepted: 26 March 2014\nPublished: 15 April 2014\nReferences\n1. JJuhár,JStaš,DHládek,in New Technologies - Trends, Innovations and\nResearch,ed.by CVolosencu.Recentprogressindevelopmentlanguage\nmodelforSlovaklargevocabularycontinuousspeechrecognition\n(InTechOpenAccess,Rijeka,2012),pp.261–276\n2. RuskoM,JJuhár,MTrnka,StašJ,SDarjaa,DHládek,RSabo,MPleva,\nRitomskýM,OndášS,in Proceedings of the 6th Language and Technology\nConference on HLT.RecentadvancesintheSlovakdictationsystemforthe\njudicialdomain(Pozna ´n,LTC,2013),pp.555–560\n3. AHuang,in Proceedings of the 6th New Zealand Computer Science Research\nStudent Conference.Similaritymeasuresfortextdocumentclustering\n(Christchurch,NZCSRSC,2008),pp.49–56\n4. LYue,SXiao,XLv,TWang,in Proceedings of 2011 International Conference\non Mechatronic Science, Electric Engineering and Computer.Topicdetection\nbasedonkeyword(Jilin,MEC,2011),pp.464–467\n5. CDManning,PRaghavan,HSchütze, Introduction to Information Retrieval.\n(CambridgeUniversityPress,Cambridge,2009)\n6. FPeng,DSchuurmans,SWang,AugmentingnaïveBayesclassifierswith\nstatisticallanguagemodels.Inf.Retr. 7(3–4),317–345(2004)\n7. STan,AneffectiverefinementstrategyforKNNtextclassifier.ExpertSyst.\nAppl.30(2),290–298(2006)\n8. NRemeikis,ISku ˇcas,MelninkaitéV,Textcategorizationusingneural\nnetworksinitializedwithdecisiontrees.Informatica. 15(4),551–564(2004)\n9. TJoachims,in Proceedings of the 10th European Conference on ML.Text\ncategorizationwithsupportvectormachines:learningwithmany\nrelevantfeatures(Chemnitz,ECML,1998),pp.137–142\n10. WZhang,TYoshida,XTang,in Proceedings of the 2nd International\nConference on Business Intelligence and Financial Engineering.Text\nclassificationusingsemi-supervisedclustering(Beijing,BIFE,2009),\npp.197–200\n11. SDarjaa,MCer ˇnak,MTrnka,MRusko,in Proceeding of INTERSPEECH 2011.\nEffectivetriphonemappingforacousticmodelinginspeechrecognition\n(Florence,INTERSPEECH,2011),pp.1717–1720\n12. MPleva,JJuhár,Buildingofbroadcastnewsdatabaseforevaluationofthe\nautomatedsubtitlingservice.Communications. 15(2A),124–128(2013)\n13. DHládek,StašJ,JJuhár,Dagger,in Proceedings of the 54th International\nSymposium ELMAR 2012.theSlovakmorphologicalclassifier(Zadar,\nELMAR,2012),pp.195–198\n14. RGarabík,in Proceedings of the 1st Workshop on Intelligent and Knowledge\nOriented Technologies.SlovakmorphologyanalyzerbasedonLevenshtein\neditoperations(Bratislava,WIKT,2006),pp.2–5\n15. StašJ,DHládek,JJuhár,MOloštiak,in Proceedings of the 7th International\nConference on Natural Language Processing, Corpus Linguistics and\nE-learning.AutomaticextractionofmultiwordunitsfromSlovaktext\ncorpora(Bratislava,SLOVKO,2013),pp.228–237\n16. JWReed,YJiao,TEPotok,BAKlump,MTElmore,ARHurson,TF-ICF,in\nProceedings of the 5th International Conference on Machine Learning and\nStaš et al. EURASIP Journal on Audio, Speech, and Music Processing2014, 2014:14 Page 12 of 12\nhttp://asmp.eurasipjournals.com/content/2014/1/14\nApplications.anewtermweightingschemeforclusteringdynamicdata\nsets(ICMLAOrlando,2006),pp.258–263\n17. ZlackýD,StašJ,JJuhár,A ˇCižmár,Term weighting schemes for Slovak text\ndocument clustering.(J.Electr.Electron.Eng,ed.),vol.6,(2013),pp.163–166\n18. RJin,CFalusos,AGHauptmann,in Proceedings of the 24th Annual\nInternational ACM Conference on Research and Development in Information\nRetrieval.Meta-scoring:automaticallyevaluatingtermweightingschemes\ninIRwithoutprecision-recall(NewOrleans,USA,SIGIRACM,NewYork,\n2001),pp.83–89\n19. SERobertson,SWalker,SJones,MMHancock-Beaulieu,MGatford,in\nProceedings of the 3rd Text Retrieval Conference.OkapiatTREC-3\n(Gaithersburg,TREC-3,1996),pp.109–126\n20. JSWhissell,ClarkeChLA,ImprovingdocumentclusteringusingOkapi\nBM25featureweighting.Inf.Retr. 14(5),466–487(2011)\n21. ASinghal,in Proceedings of the 6th Text Retrieval Conference.AT&Tat\nTREC-6(Gaithersburg,TREC-6,1998),pp.215–226\n22. SLee,JSong,YKim,Anempiricalcomparisonoffourtextmining\nmethods.J.Comp.Inf.Sys. 51(1),1–10(2010)\n23. SHCha,Comprehensivesurveyondistance/similaritymeasuresbetween\nprobabilitydensityfunctions.Intl.J.Math.Model.MethodsAppl.Sci. 1(4),\n300–307(2007)\n24. PLRosin,Edges:saliencymeasuresandautomaticthresholding.Technical\nNoteNo.I.95.58:InstituteforRemoteSensingApplications(1995)\n25. ALee,TKawahara,in em Proceedings of the 2009 Asia-Pacific Signal and\nInformation Processing Association Annual Summit and Conference.Recent\ndevelopmentofopen-sourcespeechrecognitionengineJulius(Sapporo,\nAPSIPAASC,2009),pp.131–137\n26. AStolcke,JZheng,WWang,VAbrash,in Proceedings of IEEE Automatic\nSpeech Recognition and Understanding Workshop.SRILMatsixteen:update\nandoutlook(Waikoloa,ASRU,2011),p.5pages\ndoi:10.1186/1687-4722-2014-14\nCite this article as:Staš et al.: Classification of heterogeneous text data\nfor robust domain-specific language modeling.EURASIP Journal on Audio,\nSpeech, and Music Processing20142014:14.\nSubmit your manuscript to a \njournal and beneﬁ t from:\n7 Convenient online submission\n7 Rigorous peer review\n7 Immediate publication on acceptance\n7 Open access: articles freely available online\n7 High visibility within the ﬁ  eld\n7 Retaining the copyright to your article\n    Submit your next manuscript at 7 springeropen.com",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8173526525497437
    },
    {
      "name": "Language model",
      "score": 0.7255138158798218
    },
    {
      "name": "Artificial intelligence",
      "score": 0.591249942779541
    },
    {
      "name": "Perplexity",
      "score": 0.5893235802650452
    },
    {
      "name": "Natural language processing",
      "score": 0.5568811297416687
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.46748870611190796
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4163323938846588
    },
    {
      "name": "Dictation",
      "score": 0.4155995845794678
    },
    {
      "name": "Weighting",
      "score": 0.4153570830821991
    },
    {
      "name": "Speech recognition",
      "score": 0.2793427109718323
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183764125",
      "name": "Technical University of Košice",
      "country": "SK"
    }
  ]
}