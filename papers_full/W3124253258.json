{
    "title": "Distilling Large Language Models into Tiny and Effective Students using pQRNN",
    "url": "https://openalex.org/W3124253258",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2529079372",
            "name": "Kaliamoorthi Prabhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227343800",
            "name": "Siddhant, Aditya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2793283502",
            "name": "Li, Edward",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221359262",
            "name": "Johnson, Melvin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3008374555",
        "https://openalex.org/W2962970071",
        "https://openalex.org/W2970458645",
        "https://openalex.org/W1724438581",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W2091671846",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2997710335",
        "https://openalex.org/W2924902521",
        "https://openalex.org/W2952436057",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2964299589",
        "https://openalex.org/W2575101493",
        "https://openalex.org/W2963674932",
        "https://openalex.org/W2016589492",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3101731278",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2973061659",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W2741049976",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W3085479580",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3094152627",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2930273985",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2777406049",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963974889",
        "https://openalex.org/W3013840636",
        "https://openalex.org/W3033159813",
        "https://openalex.org/W2971702703",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W3156414406",
        "https://openalex.org/W2336650964"
    ],
    "abstract": "Large pre-trained multilingual models like mBERT, XLM-R achieve state of the art results on language understanding tasks. However, they are not well suited for latency critical applications on both servers and edge devices. It's important to reduce the memory and compute resources required by these models. To this end, we propose pQRNN, a projection-based embedding-free neural encoder that is tiny and effective for natural language processing tasks. Without pre-training, pQRNNs significantly outperform LSTM models with pre-trained embeddings despite being 140x smaller. With the same number of parameters, they outperform transformer baselines thereby showcasing their parameter efficiency. Additionally, we show that pQRNNs are effective student architectures for distilling large pre-trained language models. We perform careful ablations which study the effect of pQRNN parameters, data augmentation, and distillation settings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN students achieve 95.9\\% of the performance of an mBERT teacher while being 350x smaller. On mATIS, a popular parsing task, pQRNN students on average are able to get to 97.1\\% of the teacher while again being 350x smaller. Our strong results suggest that our approach is great for latency-sensitive applications while being able to leverage large mBERT-like models.",
    "full_text": "Distilling Large Language Models into\nTiny and Effective Students using pQRNN\nPrabhu Kaliamoorthi, Aditya Siddhant, Edward Li, Melvin Johnson\nGoogle\n{prabhumk, adisid, gavinbelson, melvinp}@google.com\nAbstract\nLarge pre-trained multilingual models like\nmBERT, XLM-R achieve state of the art re-\nsults on language understanding tasks. How-\never, they are not well suited for latency crit-\nical applications on both servers and edge de-\nvices. It’s important to reduce the memory and\ncompute resources required by these models.\nTo this end, we propose pQRNN, a projection-\nbased embedding-free neural encoder that is\ntiny and effective for natural language pro-\ncessing tasks. Without pre-training, pQRNNs\nsigniﬁcantly outperform LSTM models with\npre-trained embeddings despite being 140x\nsmaller. With the same number of parameters,\nthey outperform transformer baselines thereby\nshowcasing their parameter efﬁciency. Addi-\ntionally, we show that pQRNNs are effective\nstudent architectures for distilling large pre-\ntrained language models. We perform careful\nablations which study the effect of pQRNN pa-\nrameters, data augmentation, and distillation\nsettings. On MTOP, a challenging multilin-\ngual semantic parsing dataset, pQRNN stu-\ndents achieve 95.9% of the performance of an\nmBERT teacher while being 350x smaller. On\nmATIS, a popular parsing task, pQRNN stu-\ndents on average are able to get to 97.1% of\nthe teacher while again being 350x smaller.\nOur strong results suggest that our approach is\ngreat for latency-sensitive applications while\nbeing able to leverage large mBERT-like mod-\nels.\n1 Introduction\nLarge pre-trained language models (Devlin et al.,\n2018; Lan et al., 2020; Liu et al., 2019; Yang\net al., 2019; Raffel et al., 2019) have demonstrated\nstate-of-the-art results in many natural language\nprocessing (NLP) tasks (e.g. the GLUE bench-\nmark (Wang et al., 2018)). Multilingual variants\nof these models covering 100+ languages (Con-\nneau et al., 2020; Arivazhagan et al., 2019; Fang\net al., 2020; Siddhant et al.; Xue et al., 2020;\nChung et al., 2020) have shown tremendous cross-\nlingual transfer learning capability on the chal-\nlenging XTREME benchmark (Hu et al., 2020).\nHowever, these models require millions of parame-\nters and several GigaFLOPS making them take up\nsigniﬁcant compute resources for applications on\nservers and impractical for those on the edge such\nas mobile platforms.\nReducing the memory and compute require-\nments of these models while maintaining accuracy\nhas been an active ﬁeld of research. The most\ncommonly used techniques are quantization (Gong\net al., 2014; Han et al., 2015a), weight prun-\ning (Han et al., 2015b), and knowledge distilla-\ntion (Hinton et al., 2015). In this work, we will\nfocus on knowledge distillation (KD), which aims\nto transfer knowledge from a teacher model to a\nstudent model, as an approach to model compres-\nsion. KD has been widely studied in the context\nof pre-trained language models (Tang et al., 2019;\nTurc et al., 2019; Sun et al., 2020). These methods\ncan be broadly classiﬁed into two categories: task-\nagnostic and task-speciﬁc distillation (Sun et al.,\n2020). Task-agnostic methods aim to perform dis-\ntillation on the pre-training objective like masked\nlanguage modeling (MLM) in order to obtain a\nsmaller pre-trained model. However, many tasks of\npractical interest to the NLP community are not as\ncomplex as the MLM task solved by task-agnostic\napproaches. This results in complexity inversion\n— i.e., in order to solve a speciﬁc relatively easy\nproblem the models learn to solve a general much\nharder problem which entails language understand-\ning. Task-speciﬁc methods on the other hand distill\nthe knowledge needed to solve a speciﬁc task onto\na student model thereby making the student very\nefﬁcient at the task that it aims to solve. This al-\nlows for decoupled evolution of both the teacher\nand student models.\narXiv:2101.08890v1  [cs.CL]  21 Jan 2021\nIn task-speciﬁc distillation, the most important\nrequirement is that the student model architectures\nare efﬁcient in terms of the number of training\nsamples, number of parameters, and number of\nFLOPS. To address this need we propose pQRNN\n(projection Quasi-RNN), an embedding-free neural\nencoder for NLP tasks. Unlike embedding-based\nmodel architectures used in NLP (Wu et al., 2016;\nVaswani et al., 2017), pQRNN learns the tokens\nrelevant for solving a task directly from the text\ninput similar to Kaliamoorthi et al. (2019). Specif-\nically, they overcome a signiﬁcant bottleneck of\nmultilingual pre-trained language models where\nembeddings take up anywhere between 47% and\n71% of the total parameters due to large vocabu-\nlaries (Chung et al., 2020). This results in many\nadvantages such as not requiring pre-processing\nbefore training a model and having orders of mag-\nnitude fewer parameters.\nOur main contributions in this paper are as fol-\nlows\n1. We propose pQRNN – a tiny, efﬁcient, and\nembedding-free neural encoder for NLP tasks.\n2. We demonstrate the effectiveness of pQRNNs\nas a student architecture for task-speciﬁc dis-\ntillation of large pre-trained language models.\n3. We propose a simple and effective strat-\negy for data augmentation which further im-\nproves quality and perform careful ablations\non pQRNN parameters and distillation set-\ntings.\n4. We show in experiments on two semantic-\nparsing datasets, that our pQRNN student\nis able to achieve >95% of the teacher per-\nformance while being 350x smaller than the\nteacher.\n2 Related Work\nDistillation of pre-trained models In task-\nspeciﬁc distillation, the pre-trained model is ﬁrst\nﬁne-tuned for the task and then distilled. Tang et al.\n(2019) distill BERT into a simple LSTM-based\nmodel for sentence classiﬁcation. In Sun et al.\n(2019), they also extract information from the in-\ntermediate layers of the teacher. Chatterjee (2019)\ndistills a BERT model ﬁne-tuned on SQUAD into\na smaller transformer model that’s initialized from\nBERT. Tsai et al. (2019) distills mBERT into a sin-\ngle multilingual student model which is 6x smaller\non sequence-labeling tasks. In task-agnostic dis-\ntillation, the distillation is performed at the pre-\ntraining stage. Some examples of this strategy\nare: MobileBERT (Sun et al., 2020) which dis-\ntills BERT into a slimmer version and MiniLM\n(Wang et al., 2020) distills the self-attention compo-\nnent during pre-training. In TinyBERT (Jiao et al.,\n2020) and Turc et al. (2019), the authors perform\na combination of the above two strategies where\nthey perform distillation both at the pre-training\nand ﬁne-tuning stages.\nModel Compression Apart from knowledge distil-\nlation, there has been an extensive body of work\naround compressing large language models. Some\nof the most prominent methods include: low-\nrank approximation (Lan et al., 2020), weight-\nsharing (Lan et al., 2020; Dehghani et al., 2019),\npruning (Cui et al., 2019; Gordon et al., 2020; Hou\net al., 2020), and quantization (Shen et al., 2019;\nZafrir et al., 2019).\n3 Proposed Approach\n3.1 Overview\nWe perform distillation from a pre-trained mBERT\nteacher ﬁne-tuned for the semantic parsing task.\nWe propose a projection-based architecture for the\nstudent. We hypothesise that since the student\nis task-speciﬁc, using projection would allow the\nmodel to learn the relevant tokens needed to repli-\ncate the decision surface learnt by the teacher. This\nallows us to signiﬁcantly reduce the number of pa-\nrameters that are context invariant, such as those\nin the embeddings, and increase the number of\nparameters that are useful to learn a contextual rep-\nresentation. We further use a multilingual teacher\nthat helps improve the performance of low-resource\nlanguages through cross-lingual transfer learning.\nWe propose input paraphrasing as a strategy for\ndata augmentation which further improves the ﬁnal\nquality.\n3.2 Data augmentation strategy\nLarge pre-trained language models generalize well\nbeyond the supervised data on which they are ﬁne-\ntuned. However we show that the use of supervised\ndata alone is not effective at knowledge transfer\nduring distillation. Most industry scale problems\nhave access to a large corpus of unlabeled data that\ncan augment the supervised data during distilla-\ntion. However, this is not true for public datasets.\nWe leverage query paraphrasing through bilingual\n  \nFigure 1: The pQRNN model architecture\npivoting (Mallinson et al., 2017) as a strategy to\novercome this problem. This is achieved by using\na in-house neural machine translation system (Wu\net al., 2016; Johnson et al., 2017) to translate a\nquery in the source language S to a foreign lan-\nguage T and then back-translating it into the orig-\ninal source language S, producing a paraphrase.\nWe show that this introduces sufﬁcient variation\nand novelty in the input text and helps improve\nknowledge transfer between the teacher and the\nstudent.\n3.3 Teacher Model Architecture: mBERT\nWe used mBERT (Devlin et al., 2018), a trans-\nformer model that has been pretrained on the\nWikipedia corpus of 104 languages using MLM\nand Next Sentence Prediction (NSP) tasks, as our\nteacher. The model architecture has 12 layers of\ntransformer encoder (Vaswani et al., 2017) with a\nmodel dimension of 768 and 12 attention heads. It\nuses an embedding table with a wordpiece (Schus-\nter and Nakajima, 2012) vocabulary of 110K result-\ning in approximately 162 million parameters. The\nmBERT teacher is then ﬁne-tuned on the multilin-\ngual training data for the relevant downstream task.\nThis results in a single multilingual task-speciﬁc\nteacher model for each dataset.\n3.4 Student Model Architectures: pQRNN\nThe pQRNN model architecture, shown in Figure 1,\nconsists of three distinct layers which we describe\nbelow.\n3.4.1 Projection Layer\nWe use the projection operator proposed in\n(Kaliamoorthi et al., 2019) for this work. The oper-\nator takes a sequence of tokens and turns it into a\nsequence of ternary vectors ∈PN , where P is the\nset {−1,0,1}and N is a hyper-parameter. This is\nachieved by generating a ﬁngerprint for each token\nwith 2N bits and using a surjective function to map\nevery two bits to P. The surjective function maps\nthe binary values {00,01,10,11}to {−1,0,0,1}\nand has a few useful properties. 1) Since the value\n0 is twice as likely as the non-zero values and the\nnon-zero values are equally likely, the representa-\ntion has a symmetric distribution. 2) For any two\nnon-identical tokens the resulting vectors are or-\nthogonal if the ﬁngerprints are truly random. This\ncan be proved by performing a dot product of the\nvectors ∈PN and showing that the positive and\nnegative values are equally likely, and hence the\nexpected value of the dot product is 0. 3) The norm\nof the vectors are expected to be\n√\nN/2 since N/2\nvalues are expected to be non zero.\n3.4.2 Bottleneck Layer\nThe result of the projection layer is a sequence of\nvectors X ∈PT×N , where T is the number of to-\nkens in the input sequence. Though this is a unique\nrepresentation for the sequence of tokens, the net-\nwork has no inﬂuence on the representation and the\nrepresentation does not have semantic similarity.\nSo we combine it with a fully connected layer to\nallow the network to learn a representation useful\nfor our task.\nReLU(BatchNorm(XW+ b))\nwhere W ∈RN×B and Bis the bottleneck dimen-\nsion.\n3.4.3 Contextual Encoder\nThe result of the bottleneck layer is invariant to\nthe context. We learn a contextual representation\nby feeding the result of the bottleneck layer to a\nstack of sequence encoders. In this study we used\na stack of bidirectional QRNN (Bradbury et al.,\n2016) encoders and we refer to the resulting model\nas pQRNN. We make a couple of modiﬁcations to\nthe QRNN encoder.\n1. We use batch normalization (Ioffe and\nSzegedy, 2015) before applying sigmoid and\ntanh to the Z/F/O gates. This is not straight-\nforward since the inputs are zero padded se-\nquences during training. We circumvented\nthis issue by excluding the padded region\nwhen calculating the batch statistics.\n2. We use quantization-aware training (Jacob\net al., 2017) to construct the model.\nThe hyper-parameters of the model are the state\nsize S, the kernel width kand the number of lay-\ners L. As described in (Bradbury et al., 2016), we\nuse zoneout for regularization. However, we use a\ndecaying form of zoneout which sets the zoneout\nprobability in layer l ∈[1,...,L ] as bl, where bis\na base zoneout probability. This is done based on\nthe observation that regularization in higher lay-\ners makes it harder for the network to learn a good\nrepresentation since it has fewer parameters to com-\npensate for it, while this is not the case in lower\nlayers.\n4 Experimental Setup\n4.1 Model Conﬁguration: pQRNN\nWe conﬁgure the pQRNN encoder to predict both\nintent and arguments as done in (Li et al., 2020).\nBut we decompose the probabilities using chain\nrule, as follows\nP(i,a1:T |q) =P(a1:T |i,q)P(i|q)\nwhere i, a1:T , and qare the intent, arguments and\nquery respectively. The ﬁrst term in the decompo-\nsition is trained with teacher forcing (Williams and\nZipser, 1989) for the intent values. The arguments\nare derived from the pQRNN outputs as follows\nP(a1:T |i,q) =Softmax(Linear(O) +Wδi)\nwhere O ∈RT×2S are the outputs of the bidirec-\ntional pQRNN encoder with state size S, W ∈\nRA×I is a trainable parameter, where I and Aare\nthe number of intents and arguments respectively,\nδi ∈RI is a one hot distribution on the intent label\nduring training and most likely intent prediction\nduring inference. The intent prediction from the\nquery is derived using attention pooling as follows\nP(i|q) =Softmax(Softmax(WpoolOT )O)\nwhere Wpool ∈R2S is a trainable parameter.\nTraining Setup: Using this model conﬁguration\nwe construct an optimization objective using cross-\nentropy loss on the intent and arguments. The ob-\njective includes an L2 decay with scale1e−5 on the\ntrainable parameters of the network. The training\nloss is optimized using Adam optimizer (Kingma\nand Ba, 2017) with a base learning rate of 1e−3\nand an exponential learning rate decay with de-\ncay rate of 0.9 and decay steps of 1000. Training\nis performed for 60,000 steps with a batch size\nof 4096 and synchronous gradient updates (Chen\net al., 2016). We evaluate the model continuously\non the development set during training and the best\nmodel is later evaluated on the test set to obtain the\nmetrics reported in this paper.\nModel Hyper-parameters: The pQRNN model\nconﬁguration we use in the experiments have a\nprojection feature dimension N = 1024, bottle-\nneck dimension B = 256, a stack of bidirectional\nQRNN encoder with number of layers L= 4, state\nsize S = 128and convolution kernel width k= 2.\nWe used an open source projection operator 1 that\npreserves information on sufﬁxes and preﬁxes. For\nregularization we use zoneout (Krueger et al., 2016)\nwith a layer-wise decaying probability of 0.5. We\nalso used dropout on the projection features with a\nprobability of 0.8.\n4.2 Datasets\nWe run our experiments on two multilingual\nsemantic-parsing datasets: multilingual ATIS and\nMTOP. Both datasets have one intent and some\nslots associated with a query. Given a query, both\nintent and slots have to be predicted jointly.\nLanguage Examples Intents Slotstrain dev test\nEnglish 4488 490 893 18 84\nSpanish 4488 490 893 18 84\nPortuguese 4488 490 893 18 84\nGerman 4488 490 893 18 84\nFrench 4488 490 893 18 84\nChinese 4488 490 893 18 84\nJapanese 4488 490 893 18 84\nHindi 1440 160 893 17 75\nTurkish 578 60 715 17 71\nTable 1: Data statistics for the MultiATIS++ corpus.\nMultilingual ATIS: It is a multilingual version\nof the classic goal oriented dialogue dataset ATIS\n(Price, 1990). ATIS contains queries related to\nsingle domain i.e. air travel. ATIS was expanded\nto multilingual ATIS by translating to 8 additional\nlanguages (Xu et al., 2020a). Overall, it has 18\nintents and 84 slots. Table 1 shows the summary of\nthe statistics of this dataset.\nMTOP: It is a multilingual task-oriented seman-\ntic parsing dataset covering 6 languages (Li et al.,\n1https://github.com/tensorﬂow/models/tree/master/research/seq ﬂow lite\nLanguage Examples Intents Slotstrain dev test\nEnglish 13152 1878 3757 113 75\nGerman 11610 1658 3317 113 75\nFrench 10821 1546 3092 110 74\nSpanish 11327 1618 3236 104 71\nHindi 10637 1520 3039 113 73\nThai 13152 1879 3758 110 73\nTable 2: Data statistics for the MTOP corpus.\n2020). It contains a mix of both simple as well as\ncompositional nested queries across 11 domains,\n117 intents and 78 slots. Similar to multilingual\nATIS, this dataset was also created by translating\nfrom English to ﬁve other target languages. The\ndataset provides both ﬂat and hierarchical represen-\ntations but for the purpose of our work, we only\nuse the former. Table 2 shows a summary of the\nMTOP dataset.\n4.3 Data Augmentation\nIt has been shown (Das et al., 2020) that generaliza-\ntion performance of deep learning models can be\nimproved signiﬁcantly by simple data augmenta-\ntion strategies. It is particularly useful for our case\nsince we are trying to replicate a function learnt\nby a teacher model that has been trained on much\nmore data than the student.\nOur method for data augmentation uses round-\ntrip translation through a pivot language which is\ndifferent from that of the original query. To get a\nnew utterances/queries, we take the original query\nfrom the dataset and ﬁrst translate it to the pivot lan-\nguage using our in-house translation system. The\nquery in the pivot language is then translated back\nto the original language. The round-trip translated\nqueries are now slightly different from the original\nquery. This increases the effective data size. The\nround-trip translated queries sometimes also have\nsome noise which has an added advantage as it\nwould make the model, which uses this data, robust\nto noise.\nOnce we get the queries, we obtain the intent\nand argument labels for these new queries by doing\na forward pass through the teacher model that was\ntrained on original queries. For our experiments\nin this paper, we run 3 variations: one where the\nnumber of augmented queries is equal to the size of\nthe original data and two other variations where the\naugmented data is 4x or 8x the size of the original\ndata.\n5 Results & Ablations\n5.1 MTOP Results\nTable 3 presents our results on the MTOP dataset.\nThe Reference subsection presents results from\nprior work. XLU refers to a bidirectional LSTM-\nbased intent slot model as proposed in Liu and Lane\n(2016); Zhang and Wang (2016) using pre-trained\nXLU embeddings. XLM-R (Conneau et al., 2020)\nrefers to a large multilingual pre-trained model with\n550M parameters. From the Baselines subsection,\nwe can see that the pQRNN baseline trained just\non the supervised data signiﬁcantly outperforms a\ncomparable transformer baseline by 9.1% on aver-\nage. Our pQRNN baseline also signiﬁcantly beats\nthe XLU biLSTM pre-trained model by 3% on\naverage despite being 140x smaller. In the Distil-\nlation subsection, we present our mBERT teacher\nresults which are slightly worse than the XLM-R\nreference system since mBERT has far fewer pa-\nrameters compared to XLM-R. For building the\nstudent, we experimented with distillation on just\nthe supervised data and with using 4x or 8x addi-\ntional paraphrased data. The best result from these\nthree conﬁgurations is presented aspQRNN student.\nIt can be observed that on average our distillation\nrecipe improves the exact match accuracy by close\nto 3% when compared to the pQRNN baseline. Fur-\nthermore, our student achieves 95.9% of the teacher\nperformance despite being 340x smaller.\n5.2 Multilingual ATIS Results\nTable 4 showcases the results for Multilingual ATIS\ndataset. For simplicity, we report just the intent ac-\ncuracy metric in Table 4 and show the argument\nF1 metric in Table 7 in the Appendix. The Refer-\nence subsection has LSTM-based results presented\nin (Xu et al., 2020b) and our own mBERT base-\nline. As expected, our mBERT system is signiﬁ-\ncantly better than the LSTM system presented in\n(Xu et al., 2020b). We see similar trends as seen\nin section 5.1 on this dataset as well. Our pQRNN\nbaseline signiﬁcantly outperforms a comparable\ntransformer baseline by avg. 4.3% intent accuracy.\nFurthermore, the pQRNN baseline performs sig-\nniﬁcantly better than the LSTM reference system\nby avg. 2.9%. It should be noted that the pQRNN\nbaseline outperforms the mBERT reference system\nby avg. 1.4% on intent accuracy. This is signiﬁcant\nsince pQRNN has 85x fewer parameters. In the\nExact Match Accuracy\n#Params en es fr de hi th avg\nReference\nXLU 70M (ﬂoat) 78.2 70.8 68.9 65.1 62.6 68.0 68.9\nXLM-R 550M (ﬂoat) 85.3 81.6 79.4 76.9 76.8 73.8 79.0\nBaselines\nTransformer 2M (ﬂoat) 71.7 68.2 65.1 64.1 59.1 48.4 62.8\npQRNN 2M (8bit) 78.8 75.1 71.9 68.2 69.3 68.4 71.9\nDistillation\nmBERT* teacher 170M (ﬂoat) 84.4 81.8 79.7 76.5 73.8 72.0 78.0\npQRNN student 2M (8bit) 81.8 79.1 75.8 70.8 72.1 69.5 74.8\nStudent/Teacher (%) 96.9 96.7 95.1 92.5 97.7 96.5 95.9\nTable 3: References, Baselines, Teacher and Student for MTOP dataset. Reference numbers have been taken from\n(Li et al., 2020). We use exact match accuracy as metric to compare w/references.\nIntent Accuracy\n#Params en de es fr hi ja pt tr zh avg\nReference\nLSTM 96.1 94.0 93.0 94.7 84.5 91.2 92.7 81.1 92.5 91.1\nmBERT 170M (ﬂoat) 95.5 95.1 94.1 94.8 87.8 92.9 94.0 85.4 93.4 92.6\nBaselines\nTransformer 2M (ﬂoat) 96.8 93.2 92.1 93.1 79.6 90.7 92.1 78.3 88.1 89.3\npQRNN 2M (8bit) 98.0 96.6 97.0 97.9 90.7 88.7 97.2 86.2 93.5 94.0\nDistillation\nmBERT* teacher170M (ﬂoat) 98.3 98.5 97.4 98.6 94.5 98.6 97.4 91.2 97.5 96.9\npQRNN student 2M (8bit) 97.3 91.0 88.5\nStudent/Teacher (%) 98.0 96.3 97.0\nTable 4: References, Baselines, Teacher and Student for multilingual ATIS dataset. Reference numbers have been\ntaken from (Xu et al., 2020a). We report both intent accuracy and argument f1 metrics to compare w/references.\nDistillation subsection, we present the results of\nour multilingual mBERT teacher which serves as\nthe quality upper-bound. We can see that the head-\nroom between the teacher and the pQRNN baseline\nis low for the following languages: en, es, fr, and\npt. Note that ja and zh are not space-separated\nlanguages, thereby requiring access to the original\nsegmenter used in the multilingual ATIS dataset\nto segment our paraphrased data and distill them\nusing the teacher. Since this is not publicly avail-\nable, we skip distillation on these two languages.\nThis leaves us with distillation results on de, hi and\ntr. From the Table, we can see that we obtain on\naverage 97.1% of the teacher performance while\nbeing dramatically smaller.\n5.3 pQRNN Ablation\nIn this section, we study the impact of various\nhyper-parameters on the pQRNN model perfor-\nmance. We use the German MTOP dataset for\nthis study. We report intent accuracy, slot F1 and\nexact match accuracy metrics for the baseline con-\nﬁguration in Table 5.\nWe ﬁrst studied the impact quantization had on\nthe results while keeping all other parameters un-\nchanged. From the Table, we can observe that\nquantization acts as a regularizer and helps improve\nthe performance albeit marginally. Furthermore, it\ncan be observed that disabling batch normaliza-\ntion results in a very signiﬁcant degradation of the\nmodel performance. Next, we studied the impact\nof the mapping function. Instead of using a bal-\nD 1 2 3 4 5 6 7\nQuantized Yes No\nBatch Norm Yes No\nBalanced Yes No\nZoneout Probability 0.5 0.0\nState Size 128 96\nBottleneck 256 128\nFeature Size 1024 512\nIntent Accuracy 92.7 92.6 90.8 91.7 89.2 92.2 91.3 92.1\nArgument F1 81.9 81.9 79.1 80.5 77.9 80.4 80.6 81.0\nExact Match 68.2 68.2 64.3 67.1 61.1 66.4 66.8 67.9\nTable 5: The table shows the impact of various hyper-parameters on the pQRNN model performance evaluated\non the de language in MTOP dataset. D is the default setting and in each of the ablations, we change exactly one\nhyper-parameter.\nanced map that results in symmetric and orthogonal\nprojections, we created projection that had positive\nvalue twice as likely as the non-positive values; this\ndegrades the performance noticeably. Setting the\nzoneout probability to 0.0, has the most signiﬁcant\nimpact on the model performance. We believe that\ndue to the small dataset size, regularization is es-\nsential to prevent over-ﬁtting. Finally, we study the\nimpact of the hyper-parameters such as N,B,S\ndeﬁned in Section 4.1 on the model performance\nby halving them. It can be observed that of the\nthree parameters halving Sdegrades the model per-\nformance the most especially for slot tagging. This\nis followed by Band then N.\n5.4 Effect of Data Augmentation\nen de es fr hi th\nteacher 84.4 76.5 81.8 79.7 73.8 72.0\n1x data 79.4 68.6 75.4 73.0 70.2 69.5\n4x data 81.2 70.6 78.5 75.8 72.1 63.9\n8x data 81.8 70.8 79.1 74.9 71.2 62.7\nTable 6: Effect of data augmentation on student’s per-\nformance on MTOP. The metric used is exact match\naccuracy.\nTable 6 captures the effect of the data size on dis-\ntillation. It shows the exact match accuracy for all\nlanguages on the MTOP dataset when we perform\ndistillation on just the supervised training data as\nwell as when we augment the training data with\nfour times and eight times the data using the para-\nphrasing strategy. The paraphrasing strategy uses\ndifferent pivot languages to generate the samples\nhence noise in the data is not uniform across all\naugmented samples. It can be observed that for all\nlanguages except Thai, when we use four times the\ndata, the model performance improves. We believe\nthe main reason for this is that the paraphrasing\ntechnique results in utterances that include differ-\nent ways to express the same queries using novel\nwords not part of the training set. The student\npQRNN model learns to predict the classes with\nthese novel words and hence generalizes better. For\nHindi and French, as we increase the size further to\neight times the original dataset we observe a slight\ndrop in performance. We believe this is mostly due\nto noise introduced in the paraphrased data such\nas punctuation that affects the generalization of\nthe BERT teacher. However for Thai, the distilla-\ntion performs worse as we augment data. Further\nanalysis revealed that the paraphrased data was seg-\nmented differently since we didn’t have access to\nthe segmenter used to create the original dataset.\nThis introduces a mismatch between the supervised\nand the paraphrased data which results in poor dis-\ntillation performance. These ﬁndings indicate that\none could achieve even better performance if they\nhave access to a large corpus of unlabeled data from\nthe input domain and have full control over the seg-\nmenters used during supervised data creation.\n5.5 Effect of scaling the teacher logits\nIn this section, we study the effect of scaling the\nteacher logits on the MTOP Spanish dataset. This\nis not the same as distillation temperature discussed\nin (Hinton et al., 2015), since we change the tem-\nperature of just the teacher model. We plot the\neval and train loss for training runs using differ-\n0\n1\n2\n3\n4\n0 20000 40000 60000\nEval Loss T=2 Eval Loss T=1 Eval Loss T=1/2 Eval Loss T=1/3 Eval Loss T=1/4\nTrain Loss T=2 Train Loss T=1 Train Loss T=1/2 Train Loss T=1/3 Train Loss T=1/4\nFigure 2: The effect of changing the scale of the teacher logits on the training and evaluation loss. The ﬁgure\nshows the training and eval loss in vertical axis against the training steps in horizontal axis.\nScale\nExact Match Accuracy on Test76.00%\n77.00%\n78.00%\n79.00%\n0.0 0.5 1.0 1.5 2.0 2.5\nFigure 3: The effect of changing the scale of teacher\nlogits on the exact matching accuracy on the test set.\nent scales in Figure 2. Scaling the logits with a\nmultiplier greater than 1.0 results in a more peaky\ndistribution whereas using a multiplier less than\n1.0 result in a softer distribution for the intent and\nslot prediction probabilities from the teacher. It can\nbe observed that as the scalar multiple is reduced\nthe eval loss reduces and remains low as the train-\ning progresses. However, with higher temperatures\n(e.g., T = 1,2) the eval loss ﬁrst reduces and then\nincreases indicating over-ﬁtting. From these obser-\nvations, we conclude that the exact match accuracy\non the test set improves as the scale of the logits is\nreduced. This is further demonstrated in Figure 3.\n6 Conclusion\nWe present pQRNN: a tiny, efﬁcient, embedding-\nfree neural encoder for NLP tasks. We show\nthat pQRNNs outperform LSTM models with pre-\ntrained embeddings despite being 140x smaller.\nThey are also parameter efﬁcient which is proven\nby their gain over a comparable transformer base-\nline. We then show that pQRNNs are ideal stu-\ndent architectures for distilling large pre-trained\nlanguage models (i.e., mBERT). On MTOP, a mul-\ntilingual task-oriented semantic parsing dataset,\npQRNN students reach 95.9% of the mBERT\nteacher performance. Similarly, on mATIS, a pop-\nular semantic parsing task, our pQRNN students\nachieve 97.1% of the teacher performance. In both\ncases, the student pQRNN is a staggering 350x\nsmaller than the teacher. Finally, we carefully ab-\nlate the effect of pQRNN parameters, the amount\nof pivot-based paraphrasing data, and the effect of\nteacher logit scaling. Our results prove that it’s pos-\nsible to leverage large pre-trained language models\ninto dramatically smaller pQRNN students without\nany signiﬁcant loss in quality. Our approach has\nbeen shown to work reliably at Google-scale for\nlatency-critical applications.\nAcknowledgments\nWe would like to thank our colleagues Catherine\nWah, Edgar Gonz`alez i Pellicer, Evgeny Livshits,\nJames Kuczmarski, Jason Riesa and Milan Lee for\nhelpful discussions related to this work. We would\nalso like to thank Amarnag Subramanya, Andrew\nTomkins, Macduff Hughes and Rushin Shah for\ntheir leadership and support.\nReferences\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, Wolfgang Macherey, Zhifeng Chen, and\nYonghui Wu. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2016. Quasi-recurrent neural net-\nworks.\nDebajyoti Chatterjee. 2019. Making neural machine\nreading comprehension faster.\nJianmin Chen, Rajat Monga, Samy Bengio, and\nRafal J ´ozefowicz. 2016. Revisiting distributed syn-\nchronous SGD. CoRR, abs/1604.00981.\nHyung Won Chung, Thibault F ´evry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2020. Re-\nthinking embedding coupling in pre-trained lan-\nguage models.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nBaiyun Cui, Yingming Li, Ming Chen, and Zhongfei\nZhang. 2019. Fine-tune BERT with sparse self-\nattention mechanism. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3548–3553, Hong Kong,\nChina. Association for Computational Linguistics.\nDeepan Das, Haley Massa, Abhimanyu Kulkarni, and\nTheodoros Rekatsinas. 2020. An empirical analysis\nof the impact of data augmentation on knowledge\ndistillation.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In International Conference on\nLearning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nYuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun, and\nJingjing Liu. 2020. Filter: An enhanced fusion\nmethod for cross-lingual language understanding.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir\nBourdev. 2014. Compressing deep convolutional\nnetworks using vector quantization.\nMitchell A. Gordon, Kevin Duh, and Nicholas An-\ndrews. 2020. Compressing bert: Studying the ef-\nfects of weight pruning on transfer learning.\nSong Han, Huizi Mao, and William J Dally. 2015a.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. arXiv preprint arXiv:1510.00149.\nSong Han, Jeff Pool, John Tran, and William J. Dally.\n2015b. Learning both weights and connections for\nefﬁcient neural networks. In Proceedings of the\n28th International Conference on Neural Informa-\ntion Processing Systems - Volume 1, NIPS’15, page\n1135–1143, Cambridge, MA, USA. MIT Press.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. arXiv preprint arXiv:2003.11080.\nSergey Ioffe and Christian Szegedy. 2015. Batch\nnormalization: Accelerating deep network train-\ning by reducing internal covariate shift. CoRR,\nabs/1502.03167.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Men-\nglong Zhu, Matthew Tang, Andrew G. Howard,\nHartwig Adam, and Dmitry Kalenichenko. 2017.\nQuantization and training of neural networks for\nefﬁcient integer-arithmetic-only inference. CoRR,\nabs/1712.05877.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. Tinybert: Distilling bert for natural language\nunderstanding.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nPrabhu Kaliamoorthi, Sujith Ravi, and Zornitsa\nKozareva. 2019. PRADO: Projection attention net-\nworks for document classiﬁcation on-device. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5012–\n5021, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nDavid Krueger, Tegan Maharaj, J´anos Kram´ar, Moham-\nmad Pezeshki, Nicolas Ballas, Nan Rosemary Ke,\nAnirudh Goyal, Yoshua Bengio, Hugo Larochelle,\nAaron C. Courville, and Chris Pal. 2016. Zoneout:\nRegularizing rnns by randomly preserving hidden\nactivations. CoRR, abs/1606.01305.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2020.\nMtop: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. arXiv preprint\narXiv:2008.09335.\nBing Liu and Ian Lane. 2016. Attention-based recur-\nrent neural network models for joint intent detection\nand slot ﬁlling. In Interspeech 2016, pages 685–689.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nJonathan Mallinson, Rico Sennrich, and Mirella Lap-\nata. 2017. Paraphrasing revisited with neural ma-\nchine translation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 1, Long Pa-\npers, pages 881–893, Valencia, Spain. Association\nfor Computational Linguistics.\nPatti Price. 1990. Evaluation of spoken language sys-\ntems: The atis domain. In Speech and Natural Lan-\nguage: Proceedings of a Workshop Held at Hidden\nValley, Pennsylvania, June 24-27, 1990.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152. IEEE.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W. Mahoney, and Kurt\nKeutzer. 2019. Q-bert: Hessian based ultra low pre-\ncision quantization of bert.\nAditya Siddhant, Melvin Johnson, Henry Tsai, Naveen\nAri, Jason Riesa, Ankur Bapna, Orhan Firat, and\nKarthik Raman. Evaluating the cross-lingual effec-\ntiveness of massively multilingual neural machine\ntranslation.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332, Hong Kong, China. Association for\nComputational Linguistics.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from BERT into simple neural\nnetworks. CoRR, abs/1903.12136.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical bert models for sequence labeling.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao,\nNan Yang, and Ming Zhou. 2020. Minilm: Deep\nself-attention distillation for task-agnostic compres-\nsion of pre-trained transformers. arXiv preprint\narXiv:2002.10957.\nR. J. Williams and D. Zipser. 1989. A learning algo-\nrithm for continually running fully recurrent neural\nnetworks. Neural Computation, 1(2):270–280.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nWeijia Xu, Batool Haider, and Saab Mansour. 2020a.\nEnd-to-end slot alignment and recognition for cross-\nlingual nlu. arXiv preprint arXiv:2004.14353.\nWeijia Xu, Batool Haider, and Saab Mansour. 2020b.\nEnd-to-end slot alignment and recognition for cross-\nlingual nlu.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\nXiaodong Zhang and Houfeng Wang. 2016. A joint\nmodel of intent determination and slot ﬁlling for\nspoken language understanding. In Proceedings of\nthe Twenty-Fifth International Joint Conference on\nArtiﬁcial Intelligence , IJCAI’16, page 2993–2999.\nAAAI Press.\nA Appendix\nFor the multilingual ATIS dataset, we present Ar-\ngument F1 metrics in table 7 below.\nArgument F1\n#Params en de es fr hi ja pt tr zh avg\nReference\nLSTM 94.7 91.4 75.9 85.9 74.9 88.8 88.4 64.4 90.8 83.9\nmBERT 170M (ﬂoat) 95.6 95.0 86.6 89.1 82.4 91.4 91.4 75.2 93.5 88.9\nBaselines\nTransformer 2M (ﬂoat) 93.8 91.5 73.9 82.8 71.5 88.1 85.9 67.7 87.9 82.6\npQRNN 2M (8bit) 95.1 93.9 87.8 91.3 79.9 90.2 88.8 75.7 92.2 88.3\nDistillation\nmBERT* teacher170M (ﬂoat) 95.7 95.0 83.5 94.2 87.5 92.9 91.4 87.3 92.7 91.3\npQRNN student 2M (8bit) 94.8 84.5 77.7\nStudent/Teacher (%) 99.7 96.6 89.0\nTable 7: References, Baselines, Teacher and Student argument F1 metrics for multilingual ATIS dataset. Reference\nnumbers have been taken from (Xu et al., 2020a). Intent Accuracy metric is reported in 4"
}