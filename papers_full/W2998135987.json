{
  "title": "Alignment-Enhanced Transformer for Constraining NMT with Pre-Specified Translations",
  "url": "https://openalex.org/W2998135987",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2095767802",
      "name": "Kai Song",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097659125",
      "name": "Kun Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127223396",
      "name": "Heng Yu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2021316858",
      "name": "Zhongqiang Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2105392445",
      "name": "Weihua Luo",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2146693851",
      "name": "Xiangyu Duan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095767802",
      "name": "Kai Song",
      "affiliations": [
        "Alibaba Group (China)",
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2097659125",
      "name": "Kun Wang",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2127223396",
      "name": "Heng Yu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2021316858",
      "name": "Zhongqiang Huang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105392445",
      "name": "Weihua Luo",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2146693851",
      "name": "Xiangyu Duan",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": [
        "Soochow University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2890256614",
    "https://openalex.org/W2560207749",
    "https://openalex.org/W2410217169",
    "https://openalex.org/W2759932073",
    "https://openalex.org/W6717886154",
    "https://openalex.org/W2954634210",
    "https://openalex.org/W2948569342",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2799708641",
    "https://openalex.org/W2608747952",
    "https://openalex.org/W2400065810",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6739189055",
    "https://openalex.org/W6682383452",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W6763952291",
    "https://openalex.org/W6643752348",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2797297135",
    "https://openalex.org/W2284660317",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2936627440",
    "https://openalex.org/W2798638375",
    "https://openalex.org/W6756381769",
    "https://openalex.org/W2963499882",
    "https://openalex.org/W2952682849",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2963877622",
    "https://openalex.org/W2963352809",
    "https://openalex.org/W2962714778",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W2903012348",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2971296520",
    "https://openalex.org/W1973923101",
    "https://openalex.org/W2537667581",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2437005631",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2951770285",
    "https://openalex.org/W2962708992",
    "https://openalex.org/W2912070261",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "We investigate the task of constraining NMT with pre-specified translations, which has practical significance for a number of research and industrial applications. Existing works impose pre-specified translations as lexical constraints during decoding, which are based on word alignments derived from target-to-source attention weights. However, multiple recent studies have found that word alignment derived from generic attention heads in the Transformer is unreliable. We address this problem by introducing a dedicated head in the multi-head Transformer architecture to capture external supervision signals. Results on five language pairs show that our method is highly effective in constraining NMT with pre-specified translations, consistently outperforming previous methods in translation quality.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nAlignment-Enhanced Transformer for\nConstraining NMT with Pre-Speciﬁed Translations\nKai Song,1,2 Kun Wang,1 Heng Yu,2 Yue Zhang,3\nZhongqiang Huang,2 Weihua Luo,2 Xiangyu Duan,1 Min Zhang1\n1Soochow University, Suzhou, China\n2Machine Intelligence Technology Lab, Alibaba Group, Hangzhou, China\n3School of Engineering, Westlake University, Hangzhou, China\n{songkai.sk, yuheng.yh, z.huang, weihua.luowh}@alibaba-inc.com,\nkwang1994@stu.suda.edu.cn, zhangyue@wias.org.cn,{xiangyuduan, minzhang}@suda.edu.cn\nAbstract\nWe investigate the task of constraining NMT with pre-\nspeciﬁed translations, which has practical signiﬁcance for\na number of research and industrial applications. Existing\nworks impose pre-speciﬁed translations as lexical constraints\nduring decoding, which are based on word alignments de-\nrived from target-to-source attention weights. However, mul-\ntiple recent studies have found that word alignment derived\nfrom generic attention heads in the Transformer is unreliable.\nWe address this problem by introducing a dedicated head\nin the multi-head Transformer architecture to capture exter-\nnal supervision signals. Results on ﬁve language pairs show\nthat our method is highly effective in constraining NMT with\npre-speciﬁed translations, consistently outperforming previ-\nous methods in translation quality.\n1 Introduction\nNeural machine translation (NMT) (Bahdanau, Cho, and\nBengio 2014; V aswani et al. 2017) takes an end-to-end ap-\nproach to generate translation from a source sentence, where\nno explicit word alignment is required during model training\nor decoding. NMT is less conﬁgurable and interpretable than\ntraditional phrase-based methods (Koehn, Och, and Marcu\n2003; Chiang 2007), making it difﬁcult to incorporate ex-\nternal resources such as user-provided glossaries (Alkhouli,\nBretschner, and Ney 2018), terminology dictionaries for spe-\nciﬁc domains (Dinu et al. 2019), and other resources. In\ntypical industrial applications, users need to produce pre-\nspeciﬁed translations in NMT’s output (Song et al. 2019).\nTo this end, prior studies focus on two main approaches.\nThe ﬁrst is to use placeholder tags (Crego et al. 2016;\nWang et al. 2017) to incorporate named-entity translations\ninto the NMT process. Speciﬁc placeholder tags are used\nto substitute named entities on both the source and target\nsides during training. During decoding, named-entities in the\nsource sentence are replaced with placeholder tags, which\nare translated into the corresponding target placeholder tags\nand then replaced with the translation of named-entities.\nThe second method employs pre-speciﬁed translations to\nguide NMT decoding directly, taking target dictionaries as\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nlexical constraints in the decoding process by imposing\nthem on the translation output (Hokamp and Liu 2017;\nPost and Vilar 2018).\nThe ﬁrst method works well for named-entities, but place-\nholder tags are too generic for general words and the trans-\nlation quality can be negatively affected due to the loss\nof word meaning. Recent research ﬁnds that the second\nmethod outperforms the placeholder approach (Song et al.\n2019). It retains the identity of a matched dictionary transla-\ntion and models its interaction with the decoding context.\nHowever, the decoding constraints do not explicitly esti-\nmate the correlation between the source and target sides\nof the pre-speciﬁed translations, as the lexical constraints\nare only imposed on the target side. It may hurt trans-\nlation ﬁdelity because there is little or no consideration\nfor which source word would produce the target constraint\nword. One way to improve the constraint method is to con-\nsider the aligned source words when generating target con-\nstraint words, as in (Alkhouli, Bretschner, and Ney 2018;\nCrego et al. 2016; Hasler et al. 2018) that rely on explicit\nword alignment based on the attention mechanism in NMT.\nHowever, it is well known that the multi-head attention\nmechanism employed in the Transformer architecture is ill-\nsuited for deriving accurate word alignment (Li et al. 2019;\nDing, Xu, and Koehn 2019).\nIn this paper, we consider the method of employing pre-\nspeciﬁed translations to guide NMT decoding directly. In-\nspired by LISA (Strubell et al. 2018), which shows that ex-\nternal supervision improves a Transformer-based model by\nbringing in syntactic information, we use a dedicated atten-\ntion head to learn the word alignment based on supervision\nfrom external alignment signals. Compared to existing meth-\nods (Crego et al. 2016; Hasler et al. 2018), which extract\nalignment information based on weights from generic at-\ntention heads, we use the dedicated attention head to learn\nexplicit word alignment and use it to guide the constrained\ndecoding process.\nOn 13 development and test sets across ﬁve different lan-\nguage pairs, our method achieves an average improvement\nof 4.48 BLEU score when using simulated pre-speciﬁed\ntranslations, in comparison to 3.51 BLEU score of the lexi-\ncal constraint approach (Post and Vilar 2018).\n8886\n2 Related Work\nHokamp and Liu (2017) propose modiﬁed beam search\nalgorithm, the grid beam search, which takes target-side\npre-speciﬁed translations as lexical constraints during beam\nsearch. One problem with this method is that translation ﬁ-\ndelity is not explicitly considered, as there is no indication\nbetween the matched source words and each pre-speciﬁc\ntranslation. Another drawback is that the decoding speed is\nsigniﬁcantly reduced. Post and Vilar (2018) give a faster ver-\nsion of Hokamp and Liu (2017)’s work by using dynamic\nbeam allocation in beam search to reduce decoding com-\nplexity.\nHasler et al. (2018) employ alignment between target-side\nconstraints and their corresponding source words, simulta-\nneously using ﬁnite-state machines and multi-stack (Ander-\nson et al. 2017) decoding to guide beam search. Arthur,\nNeubig, and Nakamura (2016) use alignment to inject an\nexternal lexicon into the inference process to improve the\ntranslations of low-frequency content words. Chatterjee et\nal. (2017) enhance the NMT decoder with the ability to pri-\noritize and adequately handle translation options of source\nwords, which are located by making use of alignment infor-\nmation.\nAlkhouli, Bretschner, and Ney (2018) study the quality of\nthe alignments extracted from target-to-source attention in\nTransformer, and propose to improve alignment accuracy by\ninjecting external alignment signals. Instead of using the at-\ntention weights over all source words to compute the source\ncontext, they add a special attention head whose source con-\ntext is computed only over the aligned source words accord-\ning to the external word alignment. In order to provide the\nalignment signal during decoding, a separate “self-attentive\nalignment model” is trained to learn source-side alignment\njumps, also from the same external word alignment. Their\nuse of a separate alignment model, however, to a large ex-\ntent increases decoding complexity, requiring special treat-\nment to optimize speed. In addition, a discrepancy between\nthe quality of alignment used in training and decoding af-\nfects model performance negatively.\nZenkel, Wuebker, and DeNero (2019) propose to add a\nseparate alignment layer to the Transformer architecture and\nlearn to focus its attention weights on relevant source words\nfor a given target word, in an unsupervised way from bilin-\ngual data without using external word alignment informa-\ntion. The learned word alignment from their attention layer\nis more accurate than that from a vanilla Transformer model.\nOur work is related to both Alkhouli, Bretschner, and\nNey (2018) and Zenkel, Wuebker, and DeNero (2019) in the\nsense that we all aim to improve alignment in the Trans-\nformer model. Our work differs from Alkhouli, Bretschner,\nand Ney (2018) in that we use a dedicated attention head to\ndirectly emulate target-to-source alignment behavior of an\nexternal alignment model, instead of using a separate align-\nment model to learn source-side alignment jumps. Zenkel,\nWuebker, and DeNero (2019) aim to learn word alignment\nusing a dedicated attention head without supervision from\nexternal word alignment. To our knowledge, we are the ﬁrst\nto use a dedicated attention head to learn word alignment for\nguiding constrained decoding in NMT.\nStrubell et al. (2018) present a linguistically-informed\nself-attention architecture, a neural network model that\ncombines multi-head self-attention with multi-task learning\nacross different NLP tasks. Our work is directly inspired by\nthis method, but we introduce external word alignment in-\nformation rather than syntactic information using dedicated\nattention heads.\n3 Background\n3.1 Transformer\nV aswani et al. (2017) use a self-attention network for\nboth encoder and decoder in NMT. The encoder is com-\nposed of N stacked neural layers. For time stepi in layer\nj, the hidden state h\nj\ni is calculated as follows: First, a\nself-attention sub-layer is employed to encode the con-\ntext. Then attention weights are computed as scaled dot\nproducts between the current queryh\nj−1\ni and all the keys\n{hj−1\n1 ,hj−1\n2 ,··· ,hj−1\nm }, normalized with a softmax func-\ntion. The context vector is then represented asweighted sum\nof the values projected from the hidden states in the previous\nlayer, which are{hj−1\n1 ,hj−1\n2 ,··· ,hj−1\nm }. The hidden states\nin the previous layer and the context vector are then con-\nnected by residual connection, followed by a layer normal-\nization function (Ba, Kiros, and Hinton 2016) to produce a\ncandidate hidden stateh\nj\n′\ni . Finally, another sub-layer, a feed-\nforward network (FFN), is connected with hj\n′\ni through a\nresidual connection, followed by a layer normalization func-\ntion to obtain the hidden statehj\ni .\nThe decoder is also composed of N stacked layers.\nFor time step t in layer j, a self-attention sub-layer is\ncalculated by employing self-attention mechanism over\nthe hidden states in the previous target layer, which are\n{s\nj−1\n1 ,sj−1\n2 ,··· ,sj−1\nt−1 }, resulting in candidate hidden state\nsj\n′\nt . Then, a target-to-source sub-layer is inserted after the\nﬁrst self-attention sub-layer. In particular, sj\n′\nt is taken as\nquery (Q), and the keys (K) and values (V) are projected\nfrom the source hidden states in the last layer of the en-\ncoder. The attention weights{αj\nt,1,αj\nt,2,··· ,αj\nt,m}are used\nto gain source contextcj\nt , which is a weighted sum of source-\nside hidden states. Another candidate statesj\n′′\nt is calculated\nby employing the source contextcj\nt and the candidate hidden\nstate sj\n′\nt , which is produced by the ﬁrst sub-layer. Finally, a\nlast feed-forward sub-layer is connected withsj\n′′\nt through a\nresidual connection, followed by a layer normalization func-\ntion to obtain the hidden statesj\nt .\nA softmax layer based on the decoder’s last layersN\nt is\nused to produce a probability distribution over the target-\nside vocabulary:\np(y\nt|y1,··· ,yt−1,x)=s o f t m a x (sN\nt ∗W), (1)\nwhere W is the learned weight matrix,x is the source sen-\ntence, and{y1,y2,··· ,yt}is the target words.\n8887\nFigure 1: Word alignment derived from a vanilla Trans-\nformer model.\n3.2 Alignment Extraction of Vanilla Transformer\nA common and naive way to extract word alignment from\nTransformer is to choose the source word with the maximum\nattention weight towards the current target word (Crego et\nal. 2016; Hasler et al. 2018; Arthur, Neubig, and Nakamura\n2016). In particular, an aligned source word is determined\nby choosing the source position which has the maximum ac-\ncumulated attention weights:\nγ(t) = argmax\ni∈{1,···,m}\n1\nN\nN∑\nj=1\nαj\nt,i, (2)\nwhere i is the candidate aligned source-side position. For\ndecoding stept in layerj, αj\nt,i is the attention weight of the\ni-th position in the source, which is calculated as described\nin Section 3.1.\nAs shown in Figure 1, the word alignment derived from\nthis method are quite erroneous. This problem is also noted\nin (Koehn and Knowles 2017; Li et al. 2019; Ding, Xu, and\nKoehn 2019).\n4 Method\n4.1 Supervised Alignment Using a Dedicated\nAttention Head\nInspired by Strubell et al. (2018), we extend Transformer’s\nmulti-head self-attention architecture by adding an addi-\ntional attention head, that is supervised by external align-\nment information. As shown in Figure 2, an additional atten-\ntion head is added to the target-to-source sub-layer of each\ndecoder layer.\nIn particular, at time steptin layerj, after calculating the\nﬁrst self-attention sub-layer’s hidden states\nj\n′\nt , two sets of\nattention weights are calculated in the target-to-source sub-\nlayer: the original multi-head attention and the additional at-\ntention. Both sets of attention weights are calculated using\nthe identical query (Q), keys (K) and values (V), but for\ndifferent purposes:\n1. The original multi-head attention weights\n{α\nj\nt,1,αj\nt,2,··· ,αj\nt,m} over source positions{1,··· ,m}\nare used to produce candidate hidden statesj\n′′\nt as in the\nvanilla Transformer.\n2. The additional attention weights {βj\nt,1,βj\nt,2,··· ,βj\nt,m}\ncapture the target-to-source word alignment, as super-\nvised by the external word alignment information.\nInputs\nInput Embedding\nPositional \nEncoding\nLayer 1\nOutputs (shift right)\nPositional\nEncoding\nMasked Multi-Head Attention\nMulti-Head\nAttention\nFeed forward\nOutput Embedding\nAdd&Norm\nAdd&Norm\nAdd&Norm\nLinear\nSoftmax\nAdditional \nAttention Head\nLexical Loss\nN/g6\nAlignment Loss\nLayer n\n···\n/g2/g4 /g2/g5…\nattention weights of the \noriginal multi-head\n/g1 /g3…\nattention weights of the \nadditional attention head\n/g1 /g3…\nFigure 2: Additional supervised attention head.\nTo supervise the additional attention using the external\nalignment information, we introduce an alignment loss:\nLalign\nt = −log\nm∑\ni=1\n(¯βt,i × ˆαt,i) (3)\nwhere ¯βt,i is the attention weight of thei-th source position\naveraged across all decoder layers:\n¯βt,i = 1\nN\nN∑\nj=1\nβj\nt,i (4)\nand ˆαt,i is set to 1 only if the target wordyt is aligned to the\nsource wordxi according to the external alignment supervi-\nsion, otherwise it is 0.\nThe ﬁnal objective functionL consists of both the trans-\nlation loss and the alignment loss:\nL =\nn∑\nt=1\n(Llexical\nt +λ∗Lalign\nt ) (5)\nwhere λ is set to 0.3 empirically, andLlexical\nt is the original\nword prediction loss:\nLlexical\nt = −log(p(yt|y1,··· ,yt−1,x)) (6)\n4.2 Alignment Extraction of\nAlignment-Enhanced Transformer\nDifferent from the baseline alignment extraction method de-\nscribed in Section 3.2, we use only the dedicated attention\nhead to determine the aligned source position at decoding\nstep t:\nγ(t) = argmax\ni∈{1,···,m}\n¯βt,i (7)\n8888\nTarget-to-source attention weights at decoding step /g15:\n/g5 /g9…/g6/g7\n/g12/g17\n/g12/g18\n/g12/g21/g19/g17\n/g12/g21\nTerminology Constraint: /g1/g22/g3/g23/g8/g32/g12/g20/g24/g2/g4/g2/g12/g20/g26/g33\n-inf\n-inf\n-inf\n-2.3\n-inf\n-inf\n-inf\nLoss\n/g12/g17\n/g12/g18\n/g12/g21/g19/g17\n/g12/g21\n-inf\n-inf\n-inf\n-0.3\n-inf\n-inf\n-inf\n/g12/g17\n/g12/g18\n/g12/g21/g19/g17\n/g12/g21\n-inf\n-inf\n-inf\n-0.7\n-inf\n-inf\n-inf\n/g12/g20/g24 /g12/g20/g25 /g12/g20/g26\n……\n/g15 /g15/g27/g16 /g15/g27/g14/g28/g16\n…\n……\n……\n……\nLoss Loss\n… /g13\n/g30/g10/g29/g13/g29/g11/g31\nDecoding steps: \nFigure 3: Dictionary-guided decoding.\nwhere ¯βt,i is the average of the attention weights from all\ndecoder layers to thei-th source position.\nDifferent from Alkhouli, Bretschner, and Ney (2018) and\nZenkel, Wuebker, and DeNero (2019), the alignment in our\nmethod is extracted by choosing the source position that has\nthe maximum averaged attention weight produced by the\nadditional supervised attention head, instead of the default\nmulti-heads.\n4.3 Dictionary-Guided Decoding\nAlkhouli, Bretschner, and Ney (2018) describe a\n“dictionary-guided decoding task” as a down-stream\ntask of leveraging Transformer’s alignment, which provides\nan efﬁcient way of using pre-speciﬁed translations to guide\nthe decoding procedure. In particular, at decoding stept,i f\nthe source aligned wordx\nj matches a dictionary translation\nwhich should be used as a translation constraint, the de-\ncoder’s output probabilities over the target-side vocabulary\nare reset. Inﬁnite cost is set for all except the constrained\nword which is the target-side of the pre-speciﬁed translation.\nInspired by the constrained decoding algorithm used\nin (Hokamp and Liu 2017; Post and Vilar 2018)’s work, we\nextend Alkhouli, Bretschner, and Ney (2018)’s method to\nenable the utilization of the pre-speciﬁed translation which\ncontains multi-words or single word that can be split into\nmultiple sub-words. The decoding process is shown in Fig-\nure 3. Considering a pre-speciﬁed translation that matches\nseveral contiguous tokens in the source sentence:\nx\nu:v →{ yc1 ,··· ,yck }, (8)\nwhere xu:v denotes source-side tokens from positionu to\nposition v, {yc1 ,··· ,yck } is the provided translation con-\nsisting of k tokens of target vocabulary. In decoding stept,\nif the aligned source positionj is inside (u,v), each of the\nfollowing kdecoding steps will be constrained. In particular,\nfor each decoding stepr(t ≤ r<t +k), the probability dis-\ntribution over target vocabulary will be reset to make target\ntoken y\ncr−t+1 the maximum one. In addition, for decoding\nsteps s (s ≥ t + k), if any of the source positions inside\n(u : v) is aligned, the above operation will not be repeat-\nedly applied and the decoding procedure is the same as the\nstandard NMT decoding.\nSince the only difference between dictionary-guided de-\ncoding and standard NMT decoding is the adjustment oper-\nation on the loss of each word in target vocabulary, no addi-\ntional calculation is required, so time and memory consump-\ntion are not affected.\n5 Experiments\nWe use an in-house re-implementation of Trans-\nformer (V aswani et al. 2017), similar to Google’s Ten-\nsor2Tensor. We test our method on ﬁve language pairs:\nEnglish to Romanian (En-Ro), English to German (En-De),\nEnglish to Russian (En-Ru), English to French (En-Fr) and\nChinese to English (Ch-En). BLEU\n1 (Papineni et al. 2002)\nand alignment error rate (AER)2 (Och and Ney 2000) are\nused for the evaluation of translation quality and alignment\nquality, respectively.\n5.1 Data\nOur training corpora are taken from the WMT news trans-\nlation task. In particular, the training corpora of En-De and\nEn-Ro are taken from WMT2014 and WMT2016, respec-\ntively. Training corpora for En-Ru, En-Fr and Ch-En are\ntaken from WMT2018. To validate our method on large\nscale data, corpora from both real bilingual texts and syn-\nthetic back-translation (Sennrich, Haddow, and Birch 2015a)\nare used for these three language pairs. The synthetic corpus\nis translated from “NewsCommonCrawl”, which can be ob-\ntained from the WMT task. The number of training sentence\npairs is 0.6M, 4.5M, 14M, 38M and 25M for En-Ro, En-De,\nEn-Ru, En-Fr and Ch-En, respectively.\nTo directly evaluate alignment extraction accuracy, we use\ntwo hand aligned, publicly available alignment test sets for\nEn-Ro\n3 and En-De4. The two test sets contain 480 sentences\nand 250 sentences for En-Ro and En-De, respectively.\n5.2 Experimental Settings\nBPE (Sennrich, Haddow, and Birch 2015b) is used in all\nexperiments, where the number of merge operations is set\nto 30K for En-Ru and Ch-En, and 50K for En-Ro, En-De\nand En-Fr. We use six self-attention layers for both the en-\ncoder and the decoder. The embedding size and the hid-\nden size are set to 512. Eight heads are used for the multi-\nhead self-attention architecture. The feed-forward layer has\n2,048 cells and ReLU (Krizhevsky, Sutskever, and Hinton\n1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl\n2https://github.com/lilt/alignment-scripts/blob/master/scripts\n3https://www-i6.informatik.rwth-aachen.de/goldAlignment/\n4http://web.eecs.umich.edu//∼mihalcea/wpt/index.html\n8889\n2012) is used as the activation function. Adam (Kingma\nand Ba 2014) is used for training; warmup steps are set to\n16,000; the learning rate is 0.0003. We use label smooth-\ning (Junczys-Dowmunt, Dwojak, and Sennrich 2016) with\na conﬁdence score of 0.9, and all the drop-out (Gal and\nGhahramani 2016) probabilities are set to 0.1. The vocabu-\nlary size is set to 30K for Ch-En and En-Ru, 50K for En-Ro,\nEn-Fr and Ch-En.\nAlignment Supervision. We align the bilingual train-\ning corpora with FastAlign\n5 (Dyer, Chahuneau, and Smith\n2013) for all language pairs. For En-Ro and En-De, we addi-\ntionally use word alignments produced by GIZA\n6 (Och and\nNey 2003) to compare the effect of the quality in alignment\nsupervision. Both FastAlign and GIZA are used with default\nsettings. The ﬁnal alignment is performed by symmetriz-\ning the alignments of both forward and backward directions\nwith the grow-diag-ﬁnal heuristic. All the training corpora\nare in sub-word format. We add a special token “eos” to the\nend of both source and target sentences and assume that they\nare aligned to each other.\nGold Pre-Speciﬁed Translations. In practice, pre-\nspeciﬁed translations can be provided by customers or\nthrough user feedback, which contain one identiﬁed transla-\ntion for speciﬁed source segment. To simulate pre-speciﬁed\ntranslations for different test sets, previous works (Hasler et\nal. 2018; Alkhouli, Bretschner, and Ney 2018; Post and Vi-\nlar 2018; Song et al. 2019) obtain dictionary entries by ex-\ntracting translation pairs from a test set and its reference.\nWhile these dictionary entries are always correct as mea-\nsured by the reference, some of them can already be gen-\nerated by the baseline systems, and thus are not useful for\nmeasuring the effectiveness of dictionary-guided decoding.\nMoreover, some dictionary entries can be generated by one\nof the baseline systems but not the other, making it unfair to\ncompare different approaches to dictionary-based decoding.\nIn order to address these two issues, we propose to extract\npre-speciﬁed translations that can correct translation errors\nfor all baseline systems, using the method described below.\nGiven a source sentence{x\n1,x2,...,x m} and its transla-\ntion output {oA\n1 ,oA\n2\n,...,o A\nr } produced from system A, the\nword alignment between them is obtained by FastAlign.\nThe alignment between the source sentence and the refer-\nence {y\n1,y2,...,y n} is also obtained by FastAlign. Given\na source word xi, if its aligned target word oA\nt is differ-\nent from the aligned reference word yj , a candidate pre-\nspeciﬁed translation (xi,yj ) belongs to system A is con-\nstructed for source word xi in the sentence. Additionally,\nfor a source phrase xu:v(u ≤ i<v ) and a target phrase\nyp:q(p<q ), pand q denote to the beginning and the end po-\nsition in the target sentence, anduand v denote to the begin-\nning and the end position in the source sentence. According\nto the phrase extraction conditions described in (Koehn et al.\n2007), ify\np:q is satisﬁed to be the phrase translation ofxu:v,\na pre-speciﬁed translation(xu:v,yp:q) is obtained, which is\nphrase-level and contains a mistranslated source word. The\nmaximum number of tokens on each side of pre-speciﬁed\n5https://github.com/clab/fast align\n6https://github.com/moses-smt/mgiza\ntranslations is set to 3.\nFor a given source sentence, the ﬁnal gold dictionary en-\ntries which are used by all the systems in our experiments are\nobtained by taking theintersection of different systems’ pre-\nspeciﬁed translations. We randomly select up to four pre-\nspeciﬁed translations per source sentence. The pre-speciﬁed\ntranslations relevant to a source sentence are used for all sys-\ntems, covering 6.7%, 6.64%, 6.59%, 7.36% and 7.34% of\nthe words in the reference for En-Ro, En-De, En-Fr, En-Ru\nand Ch-En, respectively. The statistic is calculated on devel-\nopment set.\n5.3 System Conﬁgurations\nWe compare the following systems, in which the pre-\nspeciﬁed translations described in Section 5.2 are used.\nBaseline 1: Transformer with Lexical Constraints.\nThe algorithm in Post and Vilar (2018), which is a way\nof constraining NMT with pre-speciﬁed translations, is\nre-implemented in our Transformer. Target-sides of pre-\nspeciﬁed translations are used as lexical constraints, which\nare imposed on the translation during decoding.\nBaseline 2: Vanilla Transformer with Dictionary-\nGuided Decoding. We use vanilla Transformer with\ndictionary-guided decoding described in Section 4.3 as an-\nother baseline. During decoding, aligned source words are\nfound by using the alignment extraction method described\nin Section 3.2.\nOur System: Alignment-Enhanced Transformer with\nDictionary-Guided Decoding. For all the language pairs,\nwe use FastAlign to obtain the alignment, that is used\nfor supervision during training. We initialize parameters of\nthe alignment-enhanced Transformer with the pre-trained\nvanilla Transformer model. The hidden size of the additional\nattention is set to 512. During decoding, instead of using the\nregular attention, the alignment is extracted from the dedi-\ncated attention head by using the method described in Sec-\ntion 4.2.\n5.4 Results\nWhen using pre-speciﬁed translations described in 5.2, the\nvanilla Transformer with dictionary-guided decoding does\nnot outperform the lexical constraint method in the ﬁve lan-\nguage pairs in 12 out of all the 13 test sets. In contrast,\nalignment-enhanced Transformer is better than the lexical\nconstraint method in 12 test sets out of all the 13 test sets for\nall ﬁve language pairs. On En-Fr, the alignment-enhanced\nTransformer with dictionary-guided decoding achieves an\naverage gain of 4.09 BLEU when using pre-speciﬁed trans-\nlations, the average gain is 1.47 BLEU higher than the lexi-\ncal constraint method and 1.96 BLEU higher than the vanilla\nTransformer with dictionary-guided decoding.\nWithout using any pre-speciﬁed translations, the\nalignment-enhanced Transformer achieves approximately\nthe same BLEU scores as the vanilla Transformer. As\nshown in Tables 1 and 2, there is no signiﬁcant difference\nin translation quality between the alignment-enhanced\nTransformer and the vanilla model. This result shows that\nour method retains the original translation quality when no\nconstraint is used.\n8890\nSystems En-Ro En-De En-Fr\ndev2016† test2016 △ test2013† test2014 △ dev2015† test2015 △\nTransformer 25.16 22.96 - 26.04 26.18 - 31.91 37.47 -\n+ dict. guid. 28.68 26.37 +3.47 28.69 28.09 +2.11 33.99 39.64 +2.13\n+ lexi. cons. 29.30 27.66 +4.42 27.88 28.56 +2.28 34.48 40.14 +2.62\nAlign. Enhan. 25.12 22.98 -0.01 26.15 26.11 +0.02 31.96 37.47 +0.03\n+ dict. guid. 29.39 28.17 +4.72 30.94 31.19 +4.96 35.89 41.67 +4.09\nTable 1: BLEU scores of En-Ro, En-De and En-Fr. “Transformer” is our in-house vanilla Transformer baseline. “Align.\nEnhan.” denotes alignment-enhanced Transformer, which is our proposed method. “dict. guid.” denotes dictionary-guided\ndecoding. “lexi. cons.” denotes lexical constraint decoding (Post and Vilar 2018). Pre-speciﬁed translations described in Section\n5.2 are used.† denotes the development set.\nSystems Ch-En En-Ru\ndev2017† test2017 test2018 △ test15† test16 test17 test18 △\nTransformer 19.08 20.68 20.07 - 33.53 32.12 36.73 32.12 -\n+ dict. guid. 20.32 21.60 21.35 +1.15 35.93 34.36 38.94 34.17 +2.22\n+ lexi. cons. 22.69 23.35 23.45 +3.22 37.85 36.65 41.31 36.39 +4.42\nAlign. Enhan. 19.13 20.82 19.73 -0.05 33.60 32.16 36.55 32.13 -0.02\n+ dict. guid. 22.73 23.94 23.43 +3.42 39.14 37.11 41.68 36.99 +5.10\nTable 2: BLEU scores of Ch-En and En-Ru. System descriptions are same with Table 1. Our vanilla Transformer implementation\nis heavily optimized, which can be inferred from the BLEU scores of “Transformer” across various public test sets.\nSystems En-Ro En-De\nFastAlign 40.1 27.2\nGIZA 28.8 18.2\nV anilla Transformer 60.7 75.7\nSuper. FastAlign 48.7 27.4\nSuper. GIZA 36.2 25.3\nTable 3: Alignment error rate (%) of different systems.\n“Transformer” denotes vanilla Transformer. “Super. FastAl-\nign” and “Super. GIZA” denote alignment-enhanced model\nsupervised with alignment produced by FastAlign and\nGIZA, respectively.\nAlignment Error Rate. To compare alignment accu-\nracy of the alignment-enhanced Transformer with the vanilla\nbaseline, we evaluate alignment error rate on the two align-\nment test sets described in Section 5.1. Since the training\ncorpus and the test sets are all represented in sub-word units,\nif any source sub-word unit is matched to one target-side\nsub-word unit, the corresponding source word and target-\nside word are supposed to be aligned.\nAs shown in Table 3, the alignment derived from the\nvanilla Transformer is far from being accurate, with 60.7%\nand 75.7% AER on En-Ro and En-De, respectively. The\nalignment-enhanced Transformer signiﬁcantly reduces the\nalignment error rate to 36.2% and 25.3% respectively.\nEffect of Better Supervision.We compare the AER of\nthe alignment-enhanced Transformer trained with different\nsupervision signals on two language pairs, En-Ro and En-\nDe. As shown in Table 3, GIZA can generate better align-\nment than FastAlign. As a result, the alignment-enhanced\nTransformer trained with GIZA alignment supervision de-\nrives more accurate alignment than the model trained with\nEn-Ro dev2016 test2016\nV anilla Transformer 29.63 27.36\nSuper. FastAlign 30.75 29.46\nSuper. GIZA 31.77 30.37\nTable 4: BLEU scores of different systems on En-Ro when\nusing user-provided dictionary entries. System descriptions\ncan be found in caption of Table 3.\nFastAlign alignment. The alignment derived from Trans-\nformer has each target word aligned to only one source word.\nThis is different from GIZA or FastAlign, which allows dif-\nferent heuristics to produce more complex alignments. In ad-\ndition, the alignment extracted from the alignment-enhanced\nTransformer supervised with GIZA outperform the align-\nment generated by FastAlign. Table 4 shows the impact\nmade by alignment of different qualities. Better alignment\nsupervision leads to better alignment extraction, which re-\nsults in better constrained translation\n7. We observe that bet-\nter alignment leads to better translation quality in dictionary-\nguided outputs, demonstrating the usefulness of improved\nalignment supervision.\nSample Outputs. Figure 4 compares translations of dif-\nferent systems when using dictionary entries. Given the\nsource sentence “xiongdi(brothers) lia(both) fouren(denied)\nmousha(murder)”, the vanilla Transformer fails to trans-\nlate “lia” and “fouren” adequately. When constraining NMT\nwith pre-speciﬁed translations “(lia, both)” and “(fouren, de-\n7Pre-speciﬁed translations used in this experiment are extracted\naccording to the three systems using the method described in Sec-\ntion 5.2, and is not same as the pre-speciﬁed translations used in\nthe experiments shown in Table 1.\n8891\nboth brothers denied the killing .\nthe brothers deny the murder .\nthe brothers denied the murder .\nboth brothers denied killing .\n(\u0001, both), (\u0002\u0003, denied)\nSource sentence:\nReference:\nVanilla Transformer:\nTransformer + dic. sugg. :\nAlign. Enhan.  + dic. sugg. :\ndictionary entries: \n(xiongdi)( lia)\n\u0002\u0001    \u0003    \u0003\u0001    \u0004\u0002/nobreakspace\u0005\n(fouren)( mousha)\nFigure 4: Translation sample of different systems. System\ndescriptions are given in the caption of Table 1. The char-\nacters in the brackets under each source word are Hanyu\nPinyin, which is an ofﬁcial romanization system for Chinese.\nSystem EnRo EnDe EnFr ChEn EnRu\nTransformer 77.00 56.41 49.54 38.57 48.01\nAlign. Enhan. 85.17 81.59 82.49 69.75 80.64\nTable 5: Constrain success rate (%) of vanilla Transformer\nand alignment-enhanced Transformer trained with supervi-\nsion obtained from FastAlign.\nnied)”, different methods lead to different outcomes.\nThe vanilla Transformer with dictionary-guided decoding\ncan correct the translation of “fouren” with the dictionary\nword “denied”. During decoding, the source word “fouren”\nis successfully aligned by using the alignment extraction de-\nscribed in Section 3.2, which matches the dictionary entries.\nThe original word prediction probability over the target vo-\ncabulary is changed to make the provided translation “de-\nnied” surface in the ﬁnal translation. However, this system\nfails to correct the translation of “lia” with the provided\ntranslation “both”, because of an error in the vanilla Trans-\nformer’s alignment extraction procedure.\nAveraged attention weights calculated using Equation 2\nare shown in Figure 5a. By using the method described\nin Section 3.2, the extracted alignment is shown in Fig-\nure 5b. The source word “lia” is not aligned to any target\nword, which is a frequent phenomenon in the vanilla Trans-\nformer (Li et al. 2019; Ding, Xu, and Koehn 2019). As a\nresult, the dictionary entry “(lia, both)” is not used during\nthe vanilla Transformer’s decoding.\nThe alignment-enhanced Transformer with dictionary-\nguided decoding can correct both the translation of “lia” and\nthe translation of “fouren”. During decoding, the attention\nweights of the alignment-enhanced Transformer calculated\nusing Equation 7 are shown in Figure 5c, leading to the ex-\ntracted word alignment in Figure 5d. The source words “lia”\nand “fouren” are both successfully aligned during decod-\ning, the pre-speciﬁed translations “both” and “denied” are\nadopted in the ﬁnal translation.\nConstrain Success Rate.Better alignments result in bet-\nter constrain success rate. Constrain success refers to the per-\ncentage of the pre-speciﬁed translations being correctly pro-\nduced in the output (Song et al. 2019), which relies heavily\non the successful alignment between source word and cer-\ntain target word during dictionary-guided decoding. Table\n5 gives a comparison between the vanilla Transformer and\n(a) Attention weights.\n (b) Word alignment.\n(c) Attention weights.\n (d) Word alignment.\nFigure 5: (a) and (c) are the attention weights of vanilla\nTransformer and alignment-enhanced Transformer, respec-\ntively. (b) and (d) are the word alignment derived from (a)\nand (c), respectively.\nthe alignment-enhanced Transformer. The latter improves\nthe constrain success rate on all the ﬁve language pairs.\nBy contrast, few pre-speciﬁed translations can successfully\ntake effect in the vanilla Transformer’s output because of its\nalignment errors. As a result, the vanilla Transformer with\ndictionary-guided decoding does not outperform the lexical\nconstraint methods, as shown in Table 1 and 2.\n6 Conclusion\nWe investigated a conceptually simple but empirically ef-\nfective approach for leveraging pre-speciﬁed translations in\nNMT, which is a common practice in many industrial ap-\nplications. Given a Transformer baseline model, an addi-\ntional attention head is supervised during training, and is\nused to derive better word alignment, leading to improve-\nment in dictionary-guided decoding. Results on extensive\nexperiments show consistent improvements over two state-\nof-the-art techniques.\nAcknowledgments\nWe thank the anonymous reviewers for their detailed and\nconstructed comments. Min Zhang and Y ue Zhang are the\ncorresponding authors. The research work is supported by\nthe National Natural Science Foundation of China (Grant\nNo. 61525205) and the Alibaba Group through Alibaba In-\nnovative Research Program. Thanks to Niyu Ge for the guid-\nance in writing.\n8892\nReferences\nAlkhouli, T.; Bretschner, G.; and Ney, H. 2018. On the\nalignment problem in multi-head attention-based neural ma-\nchine translation. InProceedings of the Third Conference on\nMachine Translation: Research Papers.\nAnderson, P .; Fernando, B.; Johnson, M.; and Gould, S.\n2017. Guided open vocabulary image captioning with con-\nstrained beam search. InEMNLP.\nArthur, P .; Neubig, G.; and Nakamura, S. 2016. Incorporat-\ning discrete translation lexicons into neural machine transla-\ntion. In EMNLP.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nChatterjee, R.; Negri, M.; Turchi, M.; Federico, M.; Specia,\nL.; and Blain, F. 2017. Guiding neural machine translation\ndecoding with external knowledge. In Proceedings of the\nSecond Conference on Machine Translation, 157–168.\nChiang, D. 2007. Hierarchical Phrase-based Translation.\nComputational Linguistics33(2):201–228.\nCrego, J.; Kim, J.; Klein, G.; Rebollo, A.; Yang, K.; Senel-\nlart, J.; Akhanov, E.; Brunelle, P .; Coquard, A.; Deng, Y .;\net al. 2016. Systran’s pure neural machine translation sys-\ntems. arXiv preprint arXiv:1610.05540.\nDing, S.; Xu, H.; and Koehn, P . 2019. Saliency-driven\nword alignment interpretation for neural machine transla-\ntion. WMT.\nDinu, G.; Mathur, P .; Federico, M.; and Al-Onaizan, Y .\n2019. Training neural machine translation to apply termi-\nnology constraints. InACL.\nDyer, C.; Chahuneau, V .; and Smith, N. A. 2013. A simple,\nfast, and effective reparameterization of ibm model 2. In\nNAACL.\nGal, Y ., and Ghahramani, Z. 2016. A theoretically grounded\napplication of dropout in recurrent neural networks. InNIPS.\nHasler, E.; De Gispert, A.; Iglesias, G.; and Byrne, B. 2018.\nNeural machine translation decoding with terminology con-\nstraints. In ACL.\nHokamp, C., and Liu, Q. 2017. Lexically constrained de-\ncoding for sequence generation using grid beam search. In\nACL.\nJunczys-Dowmunt, M.; Dwojak, T.; and Sennrich, R. 2016.\nThe amu-uedin submission to the wmt16 news translation\ntask: Attention-based nmt models as feature functions in\nphrase-based smt. In Proceedings of the First Conference\non Machine Translation.\nKingma, D. P ., and Ba, J. 2014. Adam: A method for\nstochastic optimization. Computer Science.\nKoehn, P ., and Knowles, R. 2017. Six challenges for neural\nmachine translation. InACL.\nKoehn, P .; Hoang, H.; Birch, A.; Callison-Burch, C.; Fed-\nerico, M.; Bertoldi, N.; Cowan, B.; Shen, W.; Moran, C.;\nZens, R.; et al. 2007. Moses: Open source toolkit for statis-\ntical machine translation. InACL.\nKoehn, P .; Och, F.; and Marcu, D. 2003. Statistical phrase-\nbased translation. InProc. NAACL/HLT.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In NIPS.\nLi, X.; Li, G.; Liu, L.; Meng, M.; and Shi, S. 2019. On the\nword alignment from neural machine translation. InACL.\nOch, F. J., and Ney, H. 2000. Improved statistical alignment\nmodels. In ACL.\nOch, F. J., and Ney, H. 2003. A systematic comparison\nof various statistical alignment models.Computational Lin-\nguistics.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine transla-\ntion. In ACL.\nPost, M., and Vilar, D. 2018. Fast lexically constrained\ndecoding with dynamic beam allocation for neural machine\ntranslation. In ACL\n.\nSennrich, R.; Haddow, B.; and Birch, A. 2015a. Improving\nneural machine translation models with monolingual data.\nComputer Science.\nSennrich, R.; Haddow, B.; and Birch, A. 2015b. Neural\nmachine translation of rare words with subword units. In\nACL.\nSong, K.; Zhang, Y .; Y u, H.; Luo, W.; Wang, K.; and Zhang,\nM. 2019. Code-switching for enhancing nmt with pre-\nspeciﬁed translation. InNAACL.\nStrubell, E.; V erga, P .; Andor, D.; Weiss, D.; and McCallum,\nA. 2018. Linguistically-informed self-attention for semantic\nrole labeling. InEMNLP.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. InNIPS.\nWang, Y .; Cheng, S.; Jiang, L.; Yang, J.; Chen, W.; Li, M.;\nShi, L.; Wang, Y .; and Yang, H. 2017. Sogou neural machine\ntranslation systems for wmt17. InProceedings of the Second\nConference on Machine Translation, 410–415.\nZenkel, T.; Wuebker, J.; and DeNero, J. 2019. Adding in-\nterpretable attention to neural translation models improves\nword alignment. arXiv preprint arXiv:1901.11359.\n8893",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8538413643836975
    },
    {
      "name": "Computer science",
      "score": 0.7650591135025024
    },
    {
      "name": "Machine translation",
      "score": 0.5821903944015503
    },
    {
      "name": "Decoding methods",
      "score": 0.5534331798553467
    },
    {
      "name": "Natural language processing",
      "score": 0.43292829394340515
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41515934467315674
    },
    {
      "name": "Architecture",
      "score": 0.41037869453430176
    },
    {
      "name": "Speech recognition",
      "score": 0.36823177337646484
    },
    {
      "name": "Algorithm",
      "score": 0.21497517824172974
    },
    {
      "name": "Voltage",
      "score": 0.20104071497917175
    },
    {
      "name": "Engineering",
      "score": 0.11030852794647217
    },
    {
      "name": "Electrical engineering",
      "score": 0.09743618965148926
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3133055985",
      "name": "Westlake University",
      "country": "CN"
    }
  ],
  "cited_by": 43
}