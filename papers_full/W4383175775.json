{
  "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
  "url": "https://openalex.org/W4383175775",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5081236757",
      "name": "Benjamin A. Levinstein",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5077832518",
      "name": "Daniel A. Herrmann",
      "affiliations": [
        "University of California, Irvine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3015470399",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4243332343",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4244135171",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W2085370846",
    "https://openalex.org/W2494350764",
    "https://openalex.org/W2911589237",
    "https://openalex.org/W4366732256",
    "https://openalex.org/W1594137490",
    "https://openalex.org/W2051460250",
    "https://openalex.org/W3007763732",
    "https://openalex.org/W2131221008",
    "https://openalex.org/W4298519949",
    "https://openalex.org/W2025720061",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W1996413092",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2883386984",
    "https://openalex.org/W2247153946",
    "https://openalex.org/W4310926773",
    "https://openalex.org/W4389519449",
    "https://openalex.org/W2312850508",
    "https://openalex.org/W2626674121",
    "https://openalex.org/W4288335764",
    "https://openalex.org/W4310923406",
    "https://openalex.org/W4388558411",
    "https://openalex.org/W2796971335",
    "https://openalex.org/W4286905034",
    "https://openalex.org/W2103628323"
  ],
  "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
  "full_text": "STILL NO LIE DETECTOR FOR LANGUAGE MODELS :\nPROBING EMPIRICAL AND CONCEPTUAL ROADBLOCKS\nB.A. Levinstein\nUniversity of Illinois at Urbana-Champaign\nbenlevin@illinois.edu\nDaniel A. Herrmann\nUniversity of California, Irvine\ndaherrma@uci.edu\nABSTRACT\nWe consider the questions of whether or not large language models (LLMs) have\nbeliefs, and, if they do, how we might measure them. First, we evaluate two\nexisting approaches, one due to Azaria and Mitchell (2023) and the other to\nBurns et al. (2022). We provide empirical results that show that these methods\nfail to generalize in very basic ways. We then argue that, even if LLMs have\nbeliefs, these methods are unlikely to be successful for conceptual reasons. Thus,\nthere is still no lie-detector for LLMs. After describing our empirical results we\ntake a step back and consider whether or not we should expect LLMs to have\nsomething like beliefs in the first place. We consider some recent arguments\naiming to show that LLMs cannot have beliefs. We show that these arguments\nare misguided. We provide a more productive framing of questions surrounding\nthe status of beliefs in LLMs, and highlight the empirical nature of the problem.\nWe conclude by suggesting some concrete paths for future work.\nKeywords Probes · CCS · Large Language Models · Interpretability\nOne child says to the other “Wow! After reading some text, the AI understands what water is!”\n. . .\nThe second child says “All it understands is relationships between words. None of the words\nconnect to reality. It doesn’t have any internal concept of what water looks like or how it feels\nto be wet. . . . ”\n. . .\nTwo angels are watching [some] chemists argue with each other. The first angel says “Wow!\nAfter seeing the relationship between the sensory and atomic-scale worlds, these chemists\nhave realized that there are levels of understanding humans are incapable of accessing. ” The\nsecond angel says “They haven’t truly realized it. They’re just abstracting over levels of\nrelationship between the physical world and their internal thought-forms in a mechanical way.\nThey have no concept of [*****] or [*****]. You can’t even express it in their language!”\n— Scott Alexander, Meaningful\narXiv:2307.00175v1  [cs.CL]  30 Jun 2023\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\n1 Introduction\nDo large language models (LLMs) have beliefs? And, if they do, how might we measure them?\nThese questions have a striking resemblance to both philosophical questions about the nature of\nbelief in the case of humans (Ramsey (2016)) and economic questions about how to measure\nbeliefs (Savage (1972)).1\nThese questions are not just of intellectual importance but also of great practical significance. It is\nnews to no one that LLMs are having a large effect on society, and that they will continue to do\nso. Given their prevalence, it is important to address their limitations. One important problem\nthat plagues current LLMs is their tendency to generate falsehoods with great conviction. This\nis sometimes called lying and sometimes called hallucinating (Ji et al. 2023; Evans et al. 2021).\nOne strategy for addressing this problem is to find a way to read the beliefs of an LLM directly\noff its internal state. Such a strategy falls under the broad umbrella of model interpretability,2 but\nwe can think of it as a form of mind-reading with the goal of detecting lies.\nDetecting lies in LLMs has many obvious applications. It would help us successfully deploy\nLLMs at all scales: from a university student using an LLM to help learn a new subject, to\ncompanies and governments using LLMs to collect and summarize information used in decision-\nmaking. It also has clear applications in various AI safety research programs, such as Eliciting\nLatent Knowledge (Christiano et al. (2021)).\nIn this article we tackle the question about the status of beliefs in LLMs head-on. We proceed in\ntwo stages. First, we assume that LLMs do have beliefs, and consider two current approaches\nfor how we might measure them, due to Azaria and Mitchell (2023) and Burns et al. (2022).\nWe provide empirical results that show that these methods fail to generalize in very basic ways.\nWe then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for\nconceptual reasons. Thus, there is still no lie-detector for LLMs.\nAfter describing our empirical results we take a step back and consider whether or not we should\nexpect LLMs to have something like beliefs in the first place. We consider some recent arguments\naiming to show that LLMs cannot have beliefs (Bender et al. (2021); Shanahan (2022)). We\nshow that these arguments are misguided and rely on a philosophical mistake. We provide a more\nproductive framing of questions surrounding the status of beliefs in LLMs. Our analysis reveals\nboth that there are many contexts in which we should expect systems to track the truth in order to\naccomplish other goals but that the question of whether or not LLMs have beliefs is largely an\nempirical matter.3\n2 Overview of Transformer Architecture\nThe language models we’re interested in are transformer models (Vaswani et al. 2017). In\nthis section, we provide a basic understanding of how such models work.4 In particular, we’ll\nbe focusing on autoregressive, decoder-only models such as OpenAI’s GPT series and Meta’s\nLLaMA series. The basic structure is as follows:\n1. Input Preparation:Text data is fed to the model. For example, let’s consider the phrase,\nMike Trout plays for the.\n1Diaconis and Skyrms (2018) give a concise and thoughtful introduction to both of these topics.\n2See Lipton (2018) for a conceptual discussion of model interpretability.\n3We provide code at https://github.com/balevinstein/Probes.\n4For an in depth, conceptual overview of decoder only transformer models, see (Levinstein 2023).\n2\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\n2. Tokenization: The input text is tokenized, which involves breaking it down into smaller\npieces called tokens. In English, these tokens are typically individual (sub)words or\npunctuation. So, our example sentence could be broken down into [Mike, Trout, plays,\nfor, the].\n3. Embedding: Each token is then converted into a mathematical representation known as\nan embedding. This is a vector of a fixed length that represents the token along with its\nposition in the sequence.5 For instance, Mikemight be represented by a list of numbers\nsuch as [0.1, 0.3, -0.2, . . . ].\n4. Passing through Layers:These embeddings are passed through a series of computational\nlayers. Each layer transforms the embeddings of the tokens based on the each token’s\ncurrent embedding, as well as the information received from previous tokens’ embeddings.\nThis procedure enables information to be ‘moved around’ from token to token across\nthe layers. It is through these transformations that the model learns complex language\npatterns and relationships among the tokens. For example, to compute the embedding\nfor playsin Mike Trout plays for theat a layer m, a decoder-only model can use\ninformation from the layer m − 1 embeddings for Mike, Trout, and plays, but not from\nforor the.\n5. Prediction: After the embeddings pass through the last layer of the model, a prediction\nfor what the next token will be is made using the embedding just for the previous token.\nThis prediction involves estimating the probabilities of all potential next tokens in the\nvocabulary. When generating new text, the model uses this distribution to select the\nnext token. For example, after processing the phrase Mike Trout plays for the, the\nmodel might predict Angelsas the next token given its understanding of this sequence\nof text. (In reality, the model will actually make a prediction for what comes after each\ninitial string of text. So, it will make predictions for the next token after Mike, after Mike\nTrout, after Mike Trout plays, etc.)\nThe power of transformer models comes from their ability to consider and manipulate information\nacross all tokens in the input, allowing them to generate human-like text and uncover deep patterns\nin language. Figure 1 provides a basic depiction of information flow in decoder-only models.\n3 Challenges in Deciphering the ‘Beliefs’ of Language Models\nFor now, let’s assume that in order to generate human-like text, LLMs (like humans) have beliefs\nabout the world. We might then ask how we can measure and discover their beliefs. This question\nimmediately leads to a number of problems:\n3.1 Unreliable Self-Reporting\nAsking an LLM directly about its beliefs is insufficient. As we’ve already discussed, models have\na tendency to “hallucinate” or even lie. So belief reports alone cannot be taken as trustworthy.\nMoreover, when asked about its beliefs, an LLM likely will not “introspect” and decode some\nembedding that contains information about its information state. Instead, it just needs to answer\nthe question in a reasonable way that accords with its training process.\n5After the model is trained, intuitively what these embeddings are doing is representing semantic and other\ninformation about the token along with information about what has come before it in the sequence.\n3\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nMike Trout plays for the\n⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩\n⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩\n⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩\n⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩\n⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩ ⟨•,•,•⟩\np1 p2 p3 p4 p5\nFigure 1: A simplified representation of a decoder-only transformer model processing the input\nstring Mike Trout plays for the. Each input token passes through several hidden layers. At\neach layer, each token is associated with a vector (represented by⟨•, •, •⟩). The final hidden layer\ngenerates a unique probability distribution (pi) over the next possible token for each input token.\n3.2 Limited Behavioral Evidence\nWhen trying to understand human beliefs, we have a rich tapestry of behavioral evidence to draw\nupon. We consider not only what people say, but also what they do. For instance, if someone\nconsistently invests in the S&P, we infer that they believe the S&P will go up in value, even if\nthey never explicitly state it. For LLMs, however, we have a limited behavioral basis for inferring\nbeliefs. The “behavior” of a language model is confined to generating sequences of tokens, which\nlacks the depth and breadth of human action.\n3.3 Contextuality of LLMs\nEverything one inputs and doesn’t input into the LLM is fair game for it to base its responses on.\nThrough clever prompting alone, there is no way to step “outside” of the language game the LLM\nis playing to get at what itreally thinks. This problem also plagues economists’ and psychologists’\nattempts to uncover the beliefs of humans. For example, economists have challenged the validity\nof the famous “framing effects” of Tversky and Kahneman (1981) by considering the possibility\nthat the subjects in the study updated on higher-order evidence contained in what was and wasn’t\nsaid to them, and the rest of the context of the experiment (Gilboa et al. (2020)).6\n3.4 Opaque and Alien Internal Structure\nWhile we can examine the embeddings, parameters, and activations within an LLM, the semantic\nsignificance of these elements is opaque. The model generates predictions using a complex\n6Lieder and Griffiths (2020) make a similar point.\n4\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nalgorithm that manipulates high-dimensional vectors in ways that don’t obviously resemble\nhuman thought processes.\nWe can paraphrase a metaphor from Quine to help us think about language models:\nDifferent [models trained on] the same language are like different bushes trimmed\nand trained to take the shape of identical elephants. The anatomical details of\ntwigs and branches will fulfill the elephantine form differently from bush to bush,\nbut the overall outward results are alike. (Quine 1960, p. 7)\nLLMs produce output similar to the output of humans competent in the same language. Trans-\nformer models are fundamentally different from humans in both structure and function. Therefore,\nwe should exercise caution in interpreting their outputs and be aware of the inherent limitations in\nour understanding of their internal processes.\n4 Interpreting the Minds of LLMs\nOne potential strategy to decipher the beliefs of transformer models is to bypass the opacity of\ntheir internal structure using an approach known as “probing” (Alain and Bengio 2016).\nAlthough the internals of LLMs are difficult for humans to decipher directly, we can use machine\nlearning techniques to create simplified models (probes) that can approximate or infer some\naspects of the information captured within these internal structures.\nAt a high-level, this works as follows. We generate true and false statements and feed them to the\nLLM. For each statement, we extract a specific embedding from a designated hidden layer to feed\ninto the probe. The probe only has access to the embedding and is ignorant of the original text\nfed into the LLM. Its task is to infer the “beliefs” of the LLM solely based on the embedding it\nreceives.\nIn practice, we focus on the embedding associated with the last token from a late layer. This is due\nto the fact that in autoregressive, decoder-only models like the LLMs we are studying, information\nflows forward. Therefore, if the LLM is processing a statement like The earth is round,\nthe embeddings associated with the initial token Thewill not receive any information from the\nsubsequent tokens. However, the embedding for the final word roundhas received information\nfrom all previous tokens. Thus, if the LLM computes and stores a judgement about the truth of the\nstatement The earth is round, this information will be captured in the embedding associated\nwith round.7 We use relatively late layers because it seems more likely that the LLM will try\nto determine whether a statement is true or false after first processing lower-level semantic and\nsyntactic information in earlier layers.\n4.1 Supervised Learning Approach\nThe first approach for training a probe employs supervised learning. This uses a list of statements\nlabelled with their truth-values. The statements are each run through the language model. The\nprobe receives as input the embedding for the last token from a specific layer of the large language\nmodel, and it outputs a number—intended to be thought of as a subjective probability—ranging\nfrom 0 to 1. The parameters of the probe are then adjusted based on the proximity of its output to\nthe actual truth-value of the statement.\n7The sentences in the dataset all ended with a period (i.e., full-stop) as the final token. We ran some initial tests to\nsee if probes did better on the embedding for the period or for the penultimate token. We found it did not make much\nof a difference, so we did our full analysis using the embeddings for the penultimate tokens.\n5\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nGenerate True/False\nStatements\nLLM\nLLM Text Output\nEmbedding\nExtraction Probe\nNumerical Beliefs\nFigure 2: High-level overview of how the probe measures the beliefs of the LLM on inputs of\ntrue and false statements. Instead of looking at the text the LLM itself ouputs, we look at the\nnumbers that the probe outputs.\nThis approach was recently investigated by Azaria and Mitchell (2023). They devised six labelled\ndatasets, each named according to their titular subject matter: Animals, Cities, Companies,\nElements, Scientific Facts, and Inventions. Each dataset contained a minimum of 876\nentries, with an approximate balance of true and false statements, totalling 6,084 statements\nacross all datasets. Table 1 provides some examples from these datasets.\nTable 1: Example Statements from Different Datasets\nDataset Statement Label\nAnimals The giant anteater uses walking for locomotion. 1\nThe hyena has a freshwater habitat. 0\nCities Tripoli is a city in Libya. 1\nRome is the name of a country. 0\nCompanies The Bank of Montreal has headquarters in Canada. 1\nLowe’s engages in the provision of telecommunication services. 0\nElements Scandium has the atomic number of 21. 1\nThalium appears in its standard state as liquid. 0\nFacts Comets are icy celestial objects that orbit the sun. 1\nThe freezing point of water decreases as altitude increases. 0\nInventions Ernesto Blanco invented the electric wheelchair. 1\nAlan Turing invented the power loom. 0\n6\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nAnimals Capitals Cities Companies Elements Facts Inventions\nLayer -1 .722 .970 .867 .722 .755 .826 .781\nLayer -4 .728 .973 .882 .766 .792 .821 .831\nLayer -8 .729 .967 .869 .742 .694 .810 .792\nTable 2: Binary Classification Accuracy for probes trained on LLaMA 30b embeddings.\n4.2 Azaria and Mitchell’s Implementation\nAzaria and Mitchell (2023) trained probes on the embeddings derived from Facebook’s OPT 6.7b\nmodel (Zhang et al. 2022).8 Their probes were all feedforward neural networks comprising four\nfully connected layers, utilizing the ReLU activation function. The first three layers consisted\nof 256, 128, and 64 neurons, respectively, culminating in a final layer with a sigmoid output\nfunction. They applied the Adam optimizer for training, with no fine-tuning of hyperparameters,\nand executed training over five epochs.\nFor each of the six datasets, they trained three separate probes on the five other datasets and then\ntested them on the remaining one (e.g., if a probe was trained onCities, Companies, Elements,\nFacts, and Inventions, it was tested on Animals). The performance of these probes was\nevaluated using binary classification accuracy. This process was repeated for five separate layers\nof the model, yielding fairly impressive accuracy results overall.\nThe purpose of testing the probes on a distinct dataset was to verify the probes’ ability to identify\na general representation of truth within the language model, irrespective of the subject matter.\n4.3 Our Reconstruction\nWe implemented a reconstruction of Azaria and Mitchell’s method with several modifications:\n• We constructed the probes for LLaMA 30b (Touvron et al. 2023), a model from Meta\nwith 33 billion parameters and 60 layers.\n• We utilized an additional dataset named Capitalsconsisting of 10,000 examples, which\nwas provided by Azaria and Mitchell. It has substantial overlap with the Citiesdataset,\nwhich explains some of the test accuracy.\n• We trained probes on three specific layers: the last layer (layer -1), layer 56 (layer -4),\nand layer 52 (layer -8).\n• We took the best of ten probes (by binary classification accuracy) for each dataset and\neach layer instead of the best of three.\nSimilar to the findings of Azaria and Mitchell, our reconstruction resulted in generally impressive\nperformance as illustrated in Table 2.\nIn addition to binary classification accuracy, we evaluated the calibration of the probes across the\ndifferent layers. Calibration provides another metric for evaluating the quality of the probes’ fore-\ncasts. Figure 3 illustrates these calibration curves for each layer when tested on the Scientific\nFactsdataset.\n8The ‘6.7b’ refers to the number of parameters (i.e., 6.7 billion).\n7\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nFigure 3: Calibration curves for probes tested on the Scientific Factsdataset at each layer.\n4.4 The Challenge of Generalization\nThis section explores our empirical findings, which suggest that probes in this setting often learn\nfeatures that correlate with truth in the training set, but do not necessarily generalize well to\nbroader contexts.\n4.4.1 Evaluating Performance on Negations\nCreating Boolean combinations of existing statements is one of the most straightforward ways to\ngenerate novel statements for testing a model’s generalization capabilities. Negation, the simplest\nform of Boolean operation, offers a useful starting point.9\nWe derived NegFacts and NegCompanies from Azaria and Mitchell’s datasets. These new\ndatasets contained the negations of some statements in Scientific Factsand Companiesre-\nspectively. For instance, the statement The earth orbits the sunfrom Scientific Facts\nis transformed into The earth doesn’t orbit the sunin NegFacts.\nGiven that the original datasets contained few Boolean statements, these negation datasets allowed\nus to test the probes on a simple new distribution.\nWe initially tested the probes trained on Animals, Capitals, Cities, Companies, Elements,\nand Inventions (i.e., trained all positive datasets except Scientific Facts) on NegFacts.\nSimilarly, we tested the probes trained on Animals, Capitals, Scientific Facts, Cities,\nElements, and Inventions on NegCompanies. Since roughly 50% of the statements in each\nof NegFactsand NegCompaniesare true, the accuracy of five of six of these probes was worse\nthan chance, as Table 3 illustrates.\nWe then tested a new set of probes on NegFacts, after training on all seven original datasets\n(including Scientific Facts) and NegCompanies, which consisted of 550 labeled negations\nof statements from Companies. Thus, these probes were trained on all positive variants of the\nnegated statements they were tested on, along with all positive examples fromCompanies and\ntheir negated counterparts. We did the same, mutatis mutandis with NegCompanies. Despite the\nexpanded training data, the performance was still surprisingly poor, as shown in Table 3.\n9In formal models of beliefs and credence, the main domain is usually an algebra over events. If we wish to identify\ndoxastic attitudes in language models, then we should check that those attitudes behave roughly as expected over such\nan algebra. Such algebras are closed under negation, so it is a motivated starting point.\n8\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nTable 3: Binary classification accuracy for NegFacts compared to Facts. ‘NegFacts 1’\n(‘NegCompanies1’) denotes the accuracy for probes trained only on positive datasets, excluding\nScientific Facts (Companies). ‘NegFacts2’ denotes the accuracy for probes trained on all\npositive datasets including Scientific Facts and NegCompanies, while ‘NegCompanies2 de-\nnotes the accuracy for probes trained on all positive datasets includingCompaniesand NegFacts.\nLayer Facts NegFacts 1 NegFacts2 Companies NegCompanies 1 NegCompanies2\n-1 .826 .408 .526 .722 .555 .567\n-4 .821 .373 .568 .766 .460 .629\n-8 .810 .373 .601 .742 .431 .596\nSince the probes failed to do well on NegFacts and NegCompanies even after training on all\npositive analogs along with other negative examples, it’s likely the original probes are not finding\nrepresentations of truth within the language model embeddings. Instead, it seems they’re learning\nsome other feature that correlates well with truth on the training sets but that does not correlate\nwith truth in even mildly more general contexts.\nOf course, we could expand the training data to include more examples of negation and other\nBoolean combinations of sentences. This likely would allow us to train better probes. However,\nwe have general conceptual worries about generalizing probes trained with supervised learning\nthat we will explore in the next subsection. Specifically, we will be delving into the potential\nshortcomings of relying on supervised learning techniques for probe training. These issues\nstem from the inherent limitations of supervised learning models and how they handle unknown\nscenarios and unseen data patterns.\n4.5 Conceptual Problems: Failure to Generalize\nIn the realm of machine learning, out-of-distribution generalization remains a pervasive challenge\nfor classifiers. One of the common pitfalls involves learning spurious correlations that may be\npresent in the training data, but do not consistently hold in more general contexts.\nConsider an example where a classifier is trained to distinguish between images of cows and\ncamels (Beery et al. 2018). If the training set exclusively features images of cows in grassy\nenvironments and camels in sandy environments, the classifier may learn to associate the envi-\nronmental context (grass or sand) with the animal, and using that to predict the label, rather than\nlearning the distinguishing features of the animals themselves. Consequently, when presented\nwith an image of a cow standing on sand, the classifier might erroneously label it as a camel.\nWe think there are special reasons to be concerned about generalization when training probes to\nidentify a representation of truth using supervised learning because supervised learning severely\nlimits the sort of data we can use for training and testing our probes. First, we need to use\nsentences we believe the model itself is in a position to know or infer from its own training data.\nThis is the easier part. The harder part is curating data that we can unambiguously label correctly.\nThe probe most directly is learning to predict the label, not the actual truth-value. These coincide\nonly when the labels are completely correct about the statements in the training and test set.\nWe ultimately want to be able to use probes we’ve trained on sentences whose truth-value we\nourselves don’t know. However, the requirement that we accurately label training and testing\ndata limits the confidence we can place in the probes’ capability of accurately identifying a\nrepresentation of truth within the model. For instance, consider the following statements:\n• Barry Bonds is the best baseball player of all time.\n9\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\n• If the minimum wage is raised to $15 an hour, unemployment will increase.\n• France is hexagonal.\n• We are morally responsible for our choices.\n• Caeser invaded Gaul due to his ambition.\nThese statements are debatable or ambiguous. We must also be cautious of any contentious\nscientific statements that lack full consensus or could be reconsidered as our understanding of the\nworld evolves.\nGiven these restrictions, it’s likely the probes will identify properties that completely or nearly\ncoincide with truth over the limited datasets used for training and testing. For instance, the probe\nmight identify a representation for:\n• Sentence is true and contains no negation\n• Sentence is true and is expressed in the style of Wikipedia\n• Sentence is true and can be easily verified online\n• Sentence is true and verifiable\n• Sentence is true and socially acceptable to assert\n• Sentence is true and commonly believed\n• Sentence is true or asserted in textbooks\n• Sentence is true or believed by most Westerners\n• Sentence is true or ambiguous\n• Sentence is accepted by the scientific community\n• Sentence is believed by person X\nOn the original datasets we used, if the probe identified representations corresponding to any of\nthe above, it would achieve impressive performance on the test set. Although we can refine our\ntraining sets to eliminate some of these options, we won’t be able to eliminate all of them without\ncompromising our ability to label sentences correctly.\nIndeed, if the labels are inaccurate, the probe might do even better if it identified properties\nlike “Sentence is commonly believed” or “Sentence corresponds to information found in many\ntextbooks” even when the sentence is not true.10\nThis situation can be likened to the familiar camel/cow problem in machine learning. But given\nthe constraints imposed by using supervised learning and limited data, isolating representations\nof truth from other coincidental properties is even more challenging than usual. The fact that\nprobes empirically seem to identify representations of something other than truth should make us\nwary of this method.\n4.6 Conceptual Problems: Probabilities Might not Correspond to Credences\nSo far we have been assuming that if the probes extracted accurate probabilities, that this would be\ngood evidence we were extracting the credences of the model. However, this is too quick. While\n10Azaria and Mitchell (2023) did an admirable job creating their datasets. Some of the statements were gener-\nated automatically using reliable tables of information, and other parts were automated using ChatGPT and then\nmanually curated. Nonetheless, there are some imperfect examples. For instance, in Scientific Facts, one\nfinds sentences like Humans have five senses: sight, smell, hearing, taste, and touch , which is\nnot unambiguously true.\n10\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nthese probes output probabilities for statements, these probabilities do not directly correspond\nto the “credences” of the underlying language model. This disparity arises because the probe is\ndirectly penalized based on the probabilities it reports, while the underlying model is not. Thus,\nthe probe aims to translate the information embedded within the language model’s representations\ninto probabilities in a manner that minimizes its own loss.\nConsider an illustrative analogy: Suppose I forecast stock prices, with rewards based on the\naccuracy of my predictions. However, my predictions are entirely reliant on advice from my\nuncle, who, unfortunately, is systematically inaccurate. If he predicts a stock’s price will rise, it\nactually falls, and vice versa. If I wish to make accurate forecasts, I need to reverse my uncle’s\npredictions. So, while my predictions are based entirely on my uncle’s advice, they don’t directly\nreflect his actual views. Analogously, the probe makes predictions based on the information in the\nembeddings, but these predictions don’t necessarily represent the actual “beliefs” of the language\nmodel.\nThis analysis suggests that there are further conditions that probabilities extracted by a probe must\nsatisfy in order to be properly considered credences. Going back to the example of my uncle, the\nproblem there was that the predictions I was making were not used by my uncle in the appropriate\nway in order to make decisions. Thus it makes sense to day that my predictions do not reflect my\nuncle’s views.\nThus, beyond merely extracting probabilities that minimize loss, there are other requirements that\nextracted representations must satisfy. In the context of natural langauge processing, Harding in a\nrecent paper (2023) has argued for three conditions that must hold of a a pattern of activations in\nneural models for it to count as a representation of a property:11\n1. Information. The pattern must have information about the property.\n2. Use. The system (in our case, LLM) must use the pattern to accomplish its task.\n3. Misrepresentation. It should be possible for the pattern to misrepresent the property.\nIn our context the property we care about is truth, and we might call the corresponding represen-\ntation belief. Thus we see what went wrong in the uncle example: even though the predictions I\nextracted from his advice were informative (satisfied information), they also violated use: my\nuncle did not use them, but instead used his own forecasts (benighted as they were) to buy stocks.\nSo it didn’t make sense to refer to my forecasts as his beliefs. For this same reason, depending on\nhow the actual LLM ends up using the representation the probe extracts, it might not make sense\nto call the probe’s outputs the LLM’s beliefs.\n4.7 Unsupervised Learning: CCS\nThe second approach for training a probe eschews the need for labelled data. Instead, it attempts\nto identify patterns in the language model’s embeddings that satisfy certain logical coherence\nproperties.\nOne particularly innovative implementation of this idea is the Contrast-Consistent Search (CCS)\nmethod proposed by Burns et al. (2022). The CCS method relies on training probes usingcontrast\npairs. For our purposes, we can think of a contrast pair as a set of statements x+ and x−, where\nx+ has no negation, and x− is the negated version of x+. For example, The earth is flat\nand The earth is not flatform a contrast pair. (One can also form contrast pairs picking\n11Harding makes these conditions precise in the language of information theory. Further development of the concept\nof representation in the context of probes strikes us as an important line of research in working to understand the\ninternal workings of deep learning models.\n11\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nup on other features instead. For example, Burns et al. (2022) uses movie reviews from the IMDb\ndatabase (Maas et al. 2011) prefixed with “The following movie review expresses a positive\nsentiment” and “The following move review expresses a negative sentiment” to create contrast\npairs.)\nCCS proceeds in the following manner:\n1. Create a dataset of contrast pairs of true or false statements. Each pair is of the form\n(x+\ni , x−\ni ), so the dataset is {(x+\n1 , x−\n1 ), . . . ,(x+\nn , x−\nn )}.\n2. Pass each statement through the network, and extract the embedding for the last token\nfrom a chosen layer.\n3. Train a probe pθ with parameters θ. The probe takes these embeddings as inputs and\noutputs numbers between 0 and 1. It is trained such that:\n(a) The probabilities given by the probe for the embeddings of x+\ni and x−\ni should sum\nup to (approximately) 1.\n(b) The probabilities given by the probe for the embeddings of x+\ni and x−\ni are distinct.\nThe underlying rationale behind step 3(a) is that if the model represents x+\ni as true, then it should\nrepresent x−\ni as false and vice versa. We can think of a successful probe as encoding a probability\nfunction (or something approximating a probability function) that underwrites the beliefs of the\nmodel. Thus, if a probe is able to find this representation within the embeddings, it should map\nthe embeddings of x+\ni and x−\ni to numbers whose sum is close to 1. This is the central insight\nbehind Burns et al.’s approach. As they put it, CCS finds a “direction in activation space that\nis consistent across negations” (p. 3). Step 3(b) is crucial in preventing the probe from trivially\nmapping every embedding to .5 to satisfy condition 3(a).\nTo implement the conditions in step 3, Burns et al. (2022) introduce two loss functions. The\nconsistency loss, given by\nLconsistency(θ; xi) := (1− pθ(emb(x+\ni )) − pθ(emb(x−\ni )))2,\npenalizes a probe for mapping the embeddings for x+\ni and x−\ni to numbers whose sum deviates\nfrom 1. (Here emb(x) denotes the embedding for x’s last token at the given layer.)\nThe confidence loss, defined as\nLconfidence(θ; xi) := min{pθ(emb(x+\ni )), pθ(emb(x−\ni ))}2,\npenalizes a probe for approximating the degenerate solution of returning.5 for every embedding.12\nThe total loss for the dataset, termed the CCS loss, is given by:\nLCCS(θ) := 1\nn\nnX\ni=1\nLconsistency(θ; xi) +Lconfidence(θ; xi).\nCrucially, this loss function does not take actual accuracy into account. It merely penalizes probes\nfor lack of confidence and (one type of) probabilistic incoherence.\nAn important caveat to note is that, while the trained CCS probe itself approximates probabilistic\ncoherence, its outputs do not correspond to the credences or subjective probabilities of the model.\n12Some readers may worry about a second degenerate solution. The model could use the embeddings to find\nwhich of x+\ni and x−\ni contained a negation. It could map one of the embeddings to (approximately) 1 and the other to\n(approximately) 0 to achieve a low loss. Burns et al. (2022) avoid this solution by normalizing the embeddings for each\nclass by subtracting the means and dividing by the standard deviations. However, as we’ll see below, for the datasets\nthat we used, such normalization was ineffective, and the probes consistently found exactly this degenerate solution.\n12\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nLconfidence pushes the probe to report values close to 0 or 1 only. To see why, suppose a probe\nat one stage of the training process returned .6 for x+\ni and .4 for x−\ni . It could get a better loss\nby reporting .99 for x+\ni and .01 for x−\ni regardless of the language model’s actual subjective\nprobabilities, and it will be pushed in this extreme direction by gradient descent. So, the probes\nthemselves are, at best, useful for determining what the model’s categorical beliefs are, not its\nprobabilities.13\nBurns et al. (2022) report two key findings. First, even when using a fully linear probe, CCS\nyields high accuracy rates—often over 80%—across numerous datasets for a number of different\nlanguage models.14 Second, binary classification using CCS tends to be slightly more accurate\nthan the LLMs’ actual outputs when asked whether a statement is true. This suggests that CCS\ncan identify instances where the language models internally represent a statement as true but\noutput text indicating it as false, or vice versa. (For a detailed description of their results, see p. 5\nof their paper).\nHowever, the performance of the CCS probe on GPT-J (Wang and Komatsuzaki 2021), the only\ndecoder-only model tested in the study, was less impressive, with an accuracy rate of only 62.1%\nacross all datasets. This is notably lower than the peak accuracy of 84.8% achieved by the\nencoder-decoder model UnifiedQA (Khashabi et al. 2020).\n4.8 Our Reconstruction\nWe reconstructed Burns et al.’s method using embeddings for LLaMA 30b with probes trained\nand tested on contrast pairs from the Scientific Factsand NegFactsdatasets, as well as the\nCompanies and NegCompanies datasets. These contrast pairs consist of simple sentences and\ntheir negations. This approach more closely resembles the examples given in the main text of\nBurns et al.’s paper, than do the longer and more structured contrast pairs that they actually used\nto train their probes, such as movie reviews from IMDb.\nWe experimented with a variety of different methods and hyperparameters. However, we found\nthat while CCS probes were consistently able to achieve low loss according toLCCS, their accuracy\nwas in effect no better than chance—it ranged from 50% to 57% depending on the training run.\n(Recall, the minimum possible accuracy for a CCS probe is 50%.) Low accuracy persisted even\nafter we normalized the embeddings for each class by subtracting the means and dividing by the\nstandard deviations, following the same procedure as Burns et al. (2022).\nUpon inspection, it is clear that CCS probes were usually able to achieve low loss simply by\nlearning which embeddings corresponded to sentences with negations, although they sometimes\nlearned other features uncorrelated with truth. Given the similarity of the outcomes across these\nexperiments, we report quantitative results from the probes we trained using a simple one hidden\nlayer MLP with 100 neurons followed by a sigmoid output function on layers 60, 56, and 52\nin Table 4. Recall these layers correspond to the last, fourth-last, and eighth-last layers of the\nLLaMA 30b, respectively.\nWe can confirm that, despite normalization, the probes were able to determine which embeddings\ncorresponded to positive and negative examples in layers -1 and -4 by checking the average values\nthe probes returned for members of each class. Probes found some other way to achieve low loss\n13One way to see that LCCS won’t incentive a probe to learn the actual credences of the model is to observe that this\nloss function is not a strictly proper scoring rule (Gneiting and Raftery 2007). However, use of a strictly proper scoring\nrule for training probes requires appeal to actual truth-values, which in turn requires supervised learning.\n14A linear probe is one that applies linear weights to the embeddings (and perhaps adds a constant), followed by a\nsigmoid function to turn the result into a value between 0 and 1. Linear probes have an especially simple functional\nform, so intuitively, if a linear probe is successful, the embedding is easy to extract.\n13\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nLayer LCCS LConfidence LConsistency Accuracy\n-1 .009 .004 .005 .552\n-4 .003 .002 .002 .568\n-8 .013 .002 .010 .502\nTable 4: Performance of CCS Probes at various layers on each component of the loss function\nand in terms of overall accuracy.\nLayer Positive Prediction Avg Negative Prediction Avg\n-1 .968 .035\n-4 .990 .012\n-8 .389 .601\nTable 5: Average prediction value in positive examples and negative examples at each layer.\nin layer -8, but they did not do any better in terms of accuracy as shown in Table 5. (Recall, only\nroughly half the positive examples and half the negative examples are actually true.)\nNow, one might think that this failure of our probes is itself fragile. Normalization by subtracting\nthe mean and dividing by the standard deviation was supposed to disguise the grammatical form\nof the sentences, but it did not. There is likely some more sophisticated normalization method\nthat would work better.\nWe agree that such alternative methods are likely possible. However, as we discuss in the next\nsection, we are not sanguine about the basic approach Burns et al. (2022) use for conceptual\nreasons.\n4.9 Conceptual Problems: Failure to Isolate Truth\nThe advantage of CCS and unsupervised approaches more generally over supervised approaches\nis that they do not restrict the training and testing data so severely. There is no need to find large\ncollections of sentences that can unambiguously be labeled as true or false. So, one may have\nhope that CCS (and unsupervised approaches) will generalize well to new sentences because we\nare less restricted in training.\nHowever, the fundamental issue we’ve identified is that coherence properties alone can’t guarantee\nidentification of truth. As demonstrated in our experiments, probes might identify sentence\nproperties, such as the presence or absence of negation, rather than truthfulness.\nFurther, probes could identify other, non-truth-related properties of sentences. For example,\nthey could associate truth with widespread belief, resulting in the classification “ x is true and\ncommonly believed” or even “x is believed by most people”.\nTo demonstrate this, consider any probability function Pr. The sum of the probabilities that a\nsentence x is true and commonly believed, and that it is false or not commonly believed, equals 1.\nIndeed, this equation holds for any sentence propertyP, where Pr(x∧P(x))+Pr(¬x∨¬P(x)) =\n1. Likewise, Pr(x∨P(x))+Pr(¬x∧¬P(x)) = 1.15 Checking for coherence over all Kolmogorov\nprobability axioms—which require probabilities to be non-negative, normalized, and additive—\n15These are both consequences of the fact that for any proposition A, Pr(A) + Pr(¬A) = 1: take A := x ∧ Pr(x),\nfor example, and apply de Morgan’s laws.\n14\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nwill rule out some properties P, but will not come close to isolating truth. This means that\ncoherence criteria alone can’t distinguish encodings of truth from encodings of other concepts.\nThe failure to isolate truth here is reminiscent of the issue we noted with supervised learning,\nwhere truth may align with some alternative property over a dataset. However, the reasons for\nthe failure differ. In the case of CCS and other unsupervised methods, the problem lies in the\ninability of formal coherence patterns alone to separate the encoding of truth from the encoding\nof other properties that differentiate positive from negative examples. If it’s generally easier to\nfind “directions in activation space” that differentiate examples but don’t correspond exclusively\nto truth, then CCS probes will either fail immediately or fail to generalize.16\n5 Do LLMs even have beliefs at all?\nOur investigation points in a negative direction: probing the beliefs of LLMs is more difficult\nthan it appeared after a first pass. Does this mean that we should be skeptical that LLMs have\nbeliefs all together?\nTo gain traction on this question we will consider arguments that intend to show that LLMs cannot\nhave beliefs, even in principle. These arguments rely on the claim that LLMs make predictions\nabout which tokens follow other tokens, and do not work with anything like propositions or\nworld-models.\nWe claim that these arguments are misguided. We will show that our best theories of belief and\ndecision making make it a very live possibility that LLMsdo have beliefs, since beliefs might very\nwell be helpful for making good predictions about tokens. We will argue that ultimately whether\nor not LLMs have beliefs is largely an empirical question, which motivates the development of\nbetter probing techniques.\n5.1 Stochastic Parrots & the Utility of Belief\nEven without having known the limitations of current probing techniques, some have expressed\ndeep skepticism that LLMs have anything resembling beliefs. For example, Bender et al. (2021)\nwrite:\nText generated by an LM is not grounded in communicative intent, any model of\nthe world, or any model of the reader’s state of mind. It can’t have been because\nthe training data never included sharing thoughts with a listener, nor does the\nmachine have the ability to do that. . . an LM is a system for haphazardly stitching\ntogether sequences of linguistic forms it has observed in its vast training data,\naccording to probabilistic information about how they combine, but without any\nreference to meaning: a stochastic parrot. (pp. 616-617)\nSimilarly, Shanahan (2022) writes,\nA bare-bones LLM doesn’t “really” know anything because all it does, at a funda-\nmental level, is sequence prediction. Sometimes a predicted sequence takes the\nform of a proposition. But the special relationship propositional sequences have\nto truth is apparent only to the humans who are asking questions. . . Sequences\nof words with a propositional form are not special to the model itself in the way\n16Burns et al. (2022) investigate other unsupervised approaches as well that appeal to principal component analysis\nand/or clustering (such as Bimodal Salience Search (p. 22)). We believe—with some changes—most of the conceptual\nissues for CCS apply to those as well.\n15\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nthey are to us. The model itself has no notion of truth or falsehood, properly\nspeaking, because it lacks the means to exercise these concepts in anything like\nthe way we do. (p. 5)\nThese arguments rely on the idea that all the LLM is doing is predicting the next token. Because\nof this, both deny that the LLM can be working with anything like a meaningful model of the\nworld. In other words, there is nothing propositional going on under the hood.\nShanahan doesn’t deny that LLMs might contain information about the world around them. He\ndoes, however, claim that LLMs don’t make judgements or have beliefs:\nOnly in the context of a capacity to distinguish truth from falsehood can we\nlegitimately speak of “belief” in its fullest sense. But an LLM — the bare-bones\nmodel — is not in the business of making judgements. It just models what words\nare likely to follow from what other words. The internal mechanisms it uses to do\nthis, whatever they are, cannot in themselves be sensitive to the truth or otherwise\nof the word sequences it predicts. Of course, it is perfectly acceptable to say that\nan LLM “encodes”, “stores”, or “contains” knowledge, in the same sense that an\nencyclopedia can be said to encode, store, or contain knowledge. . . But if Alice\nwere to remark that “Wikipedia knew that Burundi was south of Rwanda”, it\nwould be a figure of speech, not a literal statement. (p. 5)\nThe idea is that, since the LLM models which tokens are likely to follow other tokens, and doesn’t\ninteract with the world in any other way, it cannot be tracking the truth. This is similar to the\nargument in the Bender et al. quote above: since the LLM does not have “communicative intent”,\nit cannot be using any model of the world or the reader to make its predictions.\nThese arguments, however, rest on a mistake. While it is true that the ultimate output of an\nLLM is a token sampled from a probability distribution over tokens, and so the LLM is certainly\nmodeling what words are probable to come after other words, this does not mean that the internal\nmechanisms must be insensitive to truth. This is because it might very well be that a capacity\nto distinguish truth from falsehood is very useful for predicting the next token. In other words,\ntracking the truth of propositions could be a good means toward the end of predicting what token\ncomes next.\nThis is in line with a much more general feature of many types of goal directed action that can be\nmade precise with decision theory. Decision theory gives us our best models of rational choice.\nThe core idea of decision theory is an expected utility maximizer. When faced with a set of\noptions, an expected utility maximizer combines two different attitudes to compute which act to\ntake: beliefs in the form of a probability function, and desires in the form of a utility function.17\nThere is a precise sense in which all the agent cares about is the utility.18 The agent does not care\nabout belief for its own sake, but does have beliefs in order to take effective action.\nFor example, an investor may care purely about the return on her investments. She may take\nactions with the goal to maximize her profit. It would be a mistake to conclude from this that\nthe investor must not have beliefs, because she is merely doing profit maximization. Indeed, the\ninvestor’s beliefs about how various firms will perform will probably play a crucial role in helping\nher make decisions.\nSimilarly, it is a mistake to infer from the fact that the LLM outputs tokens that are likely to\nfollows its inputs that the LLM must not have beliefs. On the contrary, given that our best theories\n17The canonical formalization of this idea in economics and statistics is Savage’s Foundations of Statistics (1972).\nPhilosophers use Savage’s formulation, as well as Jeffrey’s inThe Logic of Decision (1990).\n18More precisely, utility is a numerical representation that captures how strongly an agent cares about outcomes.\n16\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nof intelligent behaviour involve belief as a crucial component, it should be a very live hypothesis\nthat the LLM is doing its best to track truths about the world, in order to maximize predictive\naccuracy.19\nEven beyond decision theory, philosophers have long held that true beliefs are useful for achieving\ngoals, and that they play the functional role of helping us take successful action (Millikan (1995);\nPapineau (1988)). Indeed, not only is it useful, but it is a common view that the instrumental utility\nof accurate beliefs applies selection pressure on agents and organisms to conform to epistemic\nnorms (Street (2009); Cowie (2014)). For example, in the context of forming true beliefs by\ninduction, Quine famously writes, “[c]reatures inveterately wrong in their inductions have a\npathetic but praiseworthy tendency to die before reproducing their kind” (p. 13, 1969).\nThis is very intuitive. It is easy to generate decision contexts (such as strategic board games,\ninvesting, figuring out how to get to Toronto from Prague, etc.) that do seem to push us to form\naccurate beliefs about the world.\nThis is not to say that it isnecessary that LLMs have beliefs, or that they necessarily have accurate\nbeliefs. There are contexts where there seems to be less pressure on us to form accurate beliefs\n(Stich (1990)). Importantly, there are two sub-cases to consider here. The first is the case in which\nthere is little or no selection pressure for forming true beliefs, but there is not selection against\nhaving beliefs. For example, Smead (2009) considers contexts in which there are evolutionary\nadvantages for misperceiving the payoffs of a strategic interaction (section 3.4). The second is the\none in which there is selection pressure against having beliefs altogether (or, more conservatively,\nthere is no selection pressure for having beliefs). For example, Godfrey-Smith (1991, 1998),\nSober (1994), Stephens (2001), and Smead (2015) have all developed models that characterize\nwhen an agent should (be expected to) learn from its environment and then select actions based\non what it learned, and when it should not. This later situation is one in which there is selection\npressure against (or at least none for) forming beliefs.\nThis leads us to the conclusion that, whether or not LLMs have beliefs, is largely an empirical\nmatter. In favour of the view expressed by folks like Shanahan and Bender et al., there certainly\nare contexts in which there is little to no selection pressure in favour of accurate beliefs, and\nindeed there are contexts that push against having beliefs altogether. On the other hand, there\nare plenty of contexts in which it is very useful to have an accurate map of the world, in order to\nguide action. Indeed, out best theories of rational choice witness this.\n6 Probing the Future\nWe’ve deployed both empirical and conceptual tools in order to investigate whether or not LLMs\nhave beliefs and, if so, how we might measure them. We demonstrated that current probing\ntechniques fail to generalize adequately, even when we set aside the conceptual question of\nwhether or it not it makes sense to ascribe beliefs to LLMs in the first place. We then considered\ntwo prominent arguments against the claim that LLMs have beliefs, and showed that they rest on\na mistake. Ultimately, the status of beliefs in LLMs is (largely) an empirical question.\nHere we outline two possible empirical paths one might pursue to make progress on measuring\nbeliefs. These are certainly not the only two. Finally, we discuss a possible line of investigation\none might take in order gain clarity on whether or not it makes sense to attribute beliefs to LLMs.\n19We are here ignoring nuances involving inner alignment (Hubinger et al. 2019).\n17\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\n6.1 Applying Pressure for Truth\nOne of the insights in §5.1 is that contexts can apply different amounts of pressure to track\nthe truth. This suggests a natural experiment to run: use prompt engineering to apply more\npressure for the LLM to track truth, and then probe the LLM. For example, at a first pass,\ninstead of just inputting a sentence like, Rome is the name of a country, we might in-\nput the prompt, I want you to think hard about the following sentence: “Rome\nis the name of a country.” Perhaps refined versions of such prompting will make the\nrepresentations of truth more perspicuous and easier for probes to find.20\n6.2 Replacing Truth with Chance\nSo far the prompts we tested the probes on all had clear truth values. This has the advantage that\nwe can use them for supervised learning, since we have the labels (the truth values). However, it\nalso limits the ability to generate large datasets, since generating and labeling true sentences is\ncostly. Furthermore, if we are selecting sentences that we think it is plausible the LLM might\n“know” (have high credence in), then our probing technique might not get good feedback on what\nthe state of the LLM is like when it is more uncertain.\nOne way we might address these issues is by systematically generating prompts\nthat describe chance set-ups, and then training and testing the probe on state-\nments about outcomes. For example, we might prompt the model with something\nlike, There is an urn with six yellow balls, four purple balls, and nothing\nelse. A ball is drawn uniformly at random. Then, within the scope of that prompt,\nwe can use statements like The ball drawn is purple, which we can easily label with a\nchance value of 0.4.\n6.3 The Question of World-Models\nIn §5.1 we pushed back against the claims in Shanahan (2022) and Bender et al. (2021) that all an\nLLM is doing is text prediction, and thus cannot be tracking the truth. We did this by showing\nthat there are many contexts in which tracking the truth is useful for other goals. We did not,\nhowever, fully address other parts of their arguments. In particular, both Shanahan and Bender\net al. are worried that the predictions of LLMs are not generated using any “model of the world”\n(p. 616, Bender et al. (2021)), and thus that the internal states cannot “count as a belief about the\nworld” (p. 6, Shanahan (2022)).\nIn contrast to the concern about doing mere sequence prediction, this concern focuses our attention\non how the LLM computes the distribution, and whether or not it does so in a way that corresponds\nto it having any kind of model of the world. For example, even if there is some sense in which the\nLLM does keep track of something like truth because of pressure to do so as described in §5.1, a\nskeptic might reply that the representation the system is using is not truth-apt.\n20The conceptual problems plaguing both types of probing techniques would still exist. However, the thought is that\nif the representation of truth is especially perspicuous, the probes might land upon such a representation naturally.\nThis is a highly empirical question.\n18\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nWe thus think that a productive way to proceed would be to characterize the latent variables21\nthat an LLM uses to predict text, and to evaluate whether or not these variables correspond to\nanything we might want to consider a world-model.22 This requires both empirical work (in order\nto understand what kind of latent variables LLMs actually work with) and conceptual work (in\norder to understand what it would take for a latent variable to be truth-apt).23\nAcknowledgments\nThanks to Amos Azaria, Dylan Bowman, Nick Cohen, Jacqueline Harding, Aydin Mohseni,\nBruce Rushing, Nate Sharadin, and audiences at UMass Amherst and the Center for AI Safety\nfor helpful comments and feedback. Special thanks to Amos Azaria and Tom Mitchell jointly\nfor access to their code and datasets. We are grateful to the Center for AI Safety for use of their\ncompute cluster. B.L. was partly supported by a Mellon New Directions Fellowship (number\n1905-06835) and by Open Philanthropy. D.H. was partly supported by a Long-Term Future Fund\ngrant.\nReferences\nAlain, G. and Y . Bengio (2016). Understanding intermediate layers using linear classifier probes.\narXiv preprint arXiv:1610.01644.\nAzaria, A. and T. Mitchell (2023). The internal state of an llm knows when its lying.\nBeery, S., G. van Horn, and P. Perona (2018). Recognition in terra incognita.\nBender, E. M., T. Gebru, A. McMillan-Major, and S. Shmitchell (2021). On the dangers of\nstochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM\nconference on fairness, accountability, and transparency, pp. 610–623.\nBurns, C., H. Ye, D. Klein, and J. Steinhardt (2022). Discovering latent knowledge in language\nmodels without supervision.\nChristiano, P., M. Xu, and A. Cotra (2021). Arc’s first technical report: Eliciting latent knowledge.\nCowie, C. (2014). In defence of instrumentalism about epistemic normativity. Synthese 191(16),\n4003–4017.\nDiaconis, P. and B. Skyrms (2018). Ten great ideas about chance. Princeton University Press.\nEvans, O., O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and\nW. Saunders (2021). Truthful ai: Developing and governing ai that does not lie. arXiv preprint\narXiv:2110.06674.\n21Latent variables are best understood in contrast to observable variables. Suppose, for example, that you are\ntrying to predict the outcomes of a series of coin tosses. The observable variables in this context would be the actual\noutcomes: heads and tails. A latent variable would be an unobservable that you use to help make your predictions.\nFor example, suppose you have different hypotheses about the bias of the coin and take the expected bias as your\nprediction for the probability of heads on the next toss. You are using your beliefs about the latent variable (the bias of\nthe coin) to generate your beliefs about the observables.\n22Using latent variables to compute probability distributions is commonplace in science and statistics (Everett\n(2013)). Though we do not have the space to do latent variable methods full justice, one reason for this is that using\ndistributions over latent variables in order to calculate a distribution over observable variables can have massive\ncomputational benefits (see, for example, chapter 16 of Goodfellow et al. (2016)). Thus, it would be fairly surprising\nif there weren’t a useful way to think of LLMs as using some kinds of latent variables in order to make predictions\nabout the next token. Indeed, there is already some preliminary work on what sorts of latent variables LLMs might be\nworking with (Xie et al. (2021); Jiang (2023)).\n23This question is related to the classic theoretician’s dilemma in the philosophy of science (Hempel (1958)).\n19\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nEverett, B. (2013). An introduction to latent variable models . Springer Science & Business\nMedia.\nGilboa, I., S. Minardi, L. Samuelson, and D. Schmeidler (2020). States and contingencies: How\nto understand savage without anyone being hanged. Revue économique 71(2), 365–385.\nGneiting, T. and A. E. Raftery (2007). Strictly proper scoring rules, prediction, and estimation.\nJournal of the American statistical Association 102(477), 359–378.\nGodfrey-Smith, P. (1991). Signal, decision, action. The Journal of philosophy 88(12), 709–722.\nGodfrey-Smith, P. (1998). Complexity and the Function of Mind in Nature. Cambridge University\nPress.\nGoodfellow, I., Y . Bengio, and A. Courville (2016).Deep learning. MIT press.\nHarding, J. (2023). Operationalising representation in natural language processing. arXiv preprint\narXiv:2306.08193.\nHempel, C. G. (1958). The theoretician’s dilemma: A study in the logic of theory construction.\nMinnesota Studies in the Philosophy of Science 2, 173–226.\nHubinger, E., C. van Merwijk, V . Mikulik, J. Skalse, and S. Garrabrant (2019). Risks from learned\noptimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820.\nJeffrey, R. C. (1990). The logic of decision. University of Chicago press.\nJi, Z., N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J. Bang, A. Madotto, and P. Fung (2023).\nSurvey of hallucination in natural language generation. ACM Computing Surveys 55(12), 1–38.\nJiang, H. (2023). A latent space theory for emergent abilities in large language models. arXiv\npreprint arXiv:2304.09960.\nKhashabi, D., S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi (2020).\nUnifiedqa: Crossing format boundaries with a single qa system.\nLevinstein, B. (2023). A conceptual guide to transformers.\nLieder, F. and T. L. Griffiths (2020). Resource-rational analysis: Understanding human cognition\nas the optimal use of limited computational resources. Behavioral and brain sciences 43, e1.\nLipton, Z. C. (2018). The mythos of model interpretability: In machine learning, the concept of\ninterpretability is both important and slippery. Queue 16(3), 31–57.\nMaas, A., R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts (2011). Learning word\nvectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association\nfor computational linguistics: Human language technologies, pp. 142–150.\nMillikan, R. G. (1995). White queen psychology and other essays for Alice. mit Press Cambridge.\nPapineau, D. (1988). Reality and representation. Mind 97(388).\nQuine, W. V . (1969). Natural kinds. In Essays in honor of Carl G. Hempel: A tribute on the\noccasion of his sixty-fifth birthday, pp. 5–23. Springer.\nQuine, W. V . O. (1960).Word and object. MIT Press.\nRamsey, F. P. (2016). Truth and probability. Readings in Formal Epistemology: Sourcebook,\n21–45.\nSavage, L. J. (1972). The foundations of statistics. Courier Corporation.\nShanahan, M. (2022). Talking about large language models. arXiv preprint arXiv:2212.03551.\nSmead, R. (2015). The role of social interaction in the evolution of learning. The British Journal\nfor the Philosophy of Science.\n20\nSTILL NO LIE DETECTOR FOR LANGUAGE MODELS\nSmead, R. S. (2009). Social interaction and the evolution of learning rules . University of\nCalifornia, Irvine.\nSober, E. (1994). The adaptive advantage of learning and a priori prejudice. Ethology and\nSociobiology 15(1), 55–56.\nStephens, C. L. (2001). When is it selectively advantageous to have true beliefs? sandwiching the\nbetter safe than sorry argument. Philosophical Studies 105, 161–189.\nStich, S. P. (1990). The fragmentation of reason: Preface to a pragmatic theory of cognitive\nevaluation. The MIT Press.\nStreet, S. (2009). Evolution and the normativity of epistemic reasons. Canadian Journal of\nPhilosophy Supplementary Volume 35, 213–248.\nTouvron, H., T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample (2023). Llama: Open\nand efficient foundation language models.\nTversky, A. and D. Kahneman (1981). The framing of decisions and the psychology of choice.\nscience 211(4481), 453–458.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin (2017). Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),Advances in Neural Information\nProcessing Systems, V olume 30. Curran Associates, Inc.\nWang, B. and A. Komatsuzaki (2021, May). Gpt-j-6b: A 6 billion parameter autoregressive\nlanguage model. https://github.com/kingoflolz/mesh-transformer-jax.\nXie, S. M., A. Raghunathan, P. Liang, and T. Ma (2021). An explanation of in-context learning as\nimplicit bayesian inference. arXiv preprint arXiv:2111.02080.\nZhang, S., S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V .\nLin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang,\nand L. Zettlemoyer (2022). Opt: Open pre-trained transformer language models.\n21",
  "topic": "Framing (construction)",
  "concepts": [
    {
      "name": "Framing (construction)",
      "score": 0.7302907705307007
    },
    {
      "name": "Empirical research",
      "score": 0.5037426352500916
    },
    {
      "name": "Positive economics",
      "score": 0.4468381404876709
    },
    {
      "name": "Social psychology",
      "score": 0.36530277132987976
    },
    {
      "name": "Psychology",
      "score": 0.3599846363067627
    },
    {
      "name": "Epistemology",
      "score": 0.3280071020126343
    },
    {
      "name": "Economics",
      "score": 0.2330218255519867
    },
    {
      "name": "History",
      "score": 0.09985288977622986
    },
    {
      "name": "Philosophy",
      "score": 0.08503910899162292
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204250578",
      "name": "University of California, Irvine",
      "country": "US"
    }
  ]
}