{
  "title": "Large Language Model Enhanced Knowledge Representation Learning: A Survey",
  "url": "https://openalex.org/W4409206472",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2045332956",
      "name": "Xin Wang",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2402322895",
      "name": "Zirui Chen",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2124602852",
      "name": "Haofen Wang",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2164563249",
      "name": "Leong Hou U",
      "affiliations": [
        "Macau University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2030397427",
      "name": "Zhao Li",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2075764511",
      "name": "Wenbin Guo",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2045332956",
      "name": "Xin Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2402322895",
      "name": "Zirui Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124602852",
      "name": "Haofen Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164563249",
      "name": "Leong Hou U",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2030397427",
      "name": "Zhao Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2075764511",
      "name": "Wenbin Guo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3004621088",
    "https://openalex.org/W3215677344",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W4386566782",
    "https://openalex.org/W4391758541",
    "https://openalex.org/W4391473819",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W2250342289",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W6600480908",
    "https://openalex.org/W2950393809",
    "https://openalex.org/W6727875242",
    "https://openalex.org/W3117339789",
    "https://openalex.org/W3173551127",
    "https://openalex.org/W3201503287",
    "https://openalex.org/W6600650897",
    "https://openalex.org/W4226142803",
    "https://openalex.org/W3174931845",
    "https://openalex.org/W4387708435",
    "https://openalex.org/W4285261975",
    "https://openalex.org/W4285172793",
    "https://openalex.org/W4313655989",
    "https://openalex.org/W4387430414",
    "https://openalex.org/W4225632115",
    "https://openalex.org/W4221021831",
    "https://openalex.org/W6600755281",
    "https://openalex.org/W4392637089",
    "https://openalex.org/W4385572435",
    "https://openalex.org/W4384895516",
    "https://openalex.org/W4389518745",
    "https://openalex.org/W4403791927",
    "https://openalex.org/W6600100092",
    "https://openalex.org/W4399957366",
    "https://openalex.org/W4389519012",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W4285817834",
    "https://openalex.org/W2125027602",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W2889234142",
    "https://openalex.org/W3099836348",
    "https://openalex.org/W4210493972",
    "https://openalex.org/W2964330146",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2251135946",
    "https://openalex.org/W2892094955",
    "https://openalex.org/W6629967362",
    "https://openalex.org/W4388282862",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W1426956448",
    "https://openalex.org/W2336384382",
    "https://openalex.org/W2556343638",
    "https://openalex.org/W6600266142",
    "https://openalex.org/W2888677580",
    "https://openalex.org/W2963485453",
    "https://openalex.org/W2951105272",
    "https://openalex.org/W2964279602",
    "https://openalex.org/W2914592219",
    "https://openalex.org/W3035134435",
    "https://openalex.org/W2996775350",
    "https://openalex.org/W6600224582",
    "https://openalex.org/W3129049019",
    "https://openalex.org/W4226052495",
    "https://openalex.org/W6600002382",
    "https://openalex.org/W4205109264",
    "https://openalex.org/W2971167006",
    "https://openalex.org/W3103296573"
  ],
  "abstract": "Abstract Knowledge Representation Learning (KRL) is crucial for enabling applications of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by projecting knowledge facts into vector spaces. Despite their effectiveness in modeling KG structural information, KRL methods are suffering from the sparseness of KGs. The rise of Large Language Models (LLMs) built on the Transformer architecture presents promising opportunities for enhancing KRL by incorporating textual information to address information sparsity in KGs. LLM-enhanced KRL methods, including three key approaches, encoder-based methods that leverage detailed contextual information, encoder-decoder-based methods that utilize a unified Seq2Seq model for comprehensive encoding and decoding, and decoder-based methods that utilize extensive knowledge from large corpora, have significantly advanced the effectiveness and generalization of KRL in addressing a wide range of downstream tasks. This work provides a broad overview of downstream tasks while simultaneously identifying emerging research directions in these evolving domains.",
  "full_text": "Vol.:(0123456789)\nData Science and Engineering (2025) 10:315‚Äì338 \nhttps://doi.org/10.1007/s41019-025-00285-y\nSURVEY PAPERS\nLarge Language Model Enhanced Knowledge Representation \nLearning: A¬†Survey\nXin¬†Wang1 ¬†¬∑ Zirui¬†Chen1¬†¬∑ Haofen¬†Wang2 ¬†¬∑ Leong¬†Hou¬†U3¬†¬∑ Zhao¬†Li1¬†¬∑ Wenbin¬†Guo1\nReceived: 29 August 2024 / Revised: 1 February 2025 / Accepted: 9 February 2025 / Published online: 7 April 2025 \n¬© The Author(s) 2025\nAbstract\nKnowledge Representation Learning (KRL) is crucial for enabling applications of symbolic knowledge from Knowledge \nGraphs (KGs) to downstream tasks by projecting knowledge facts into vector spaces. Despite their effectiveness in modeling \nKG structural information, KRL methods are suffering from the sparseness of KGs. The rise of Large Language Models \n(LLMs) built on the Transformer architecture presents promising opportunities for enhancing KRL by incorporating tex-\ntual information to address information sparsity in KGs. LLM-enhanced KRL methods, including three key approaches, \nencoder-based methods that leverage detailed contextual information, encoder-decoder-based methods that utilize a unified \nSeq2Seq model for comprehensive encoding and decoding, and decoder-based methods that utilize extensive knowledge \nfrom large corpora, have significantly advanced the effectiveness and generalization of KRL in addressing a wide range of \ndownstream tasks. This work provides a broad overview of downstream tasks while simultaneously identifying emerging \nresearch directions in these evolving domains.\nKeywords Knowledge graph¬†¬∑ Large language model¬†¬∑ Knowledge representation learning\n1 Introduction\nTriples in Knowledge Graphs (KGs) are an effective form of \nstructured knowledge representation, consisting of an entity, \na relation, and an object, where the object can be either \nanother entity or a literal. This structured fact is crucial for \nvarious downstream tasks such as link prediction¬†[1 ] (e.g., \npredicting \"Albert Einstein\" is associated with \"Physics\" \nthrough the relation \"field of study\"), triple classification¬†[2] \n(e.g., verifying if \"Paris is the capital of France\" is true), and \nrelation classification¬†[3 ] (e.g., classifying \"wrote\" as the \nrelation connecting \"J.K. Rowling\" and \"Harry Potter\"). To \nbetter leverage the symbolic knowledge contained in KGs for \nthese¬†downstream tasks, various¬†Knowledge Representation \nLearning (KRL) approaches have been developed. Notable \nmethods like TransE, RESCAL¬†[4 ], and R-GCN [5 ] focus \non embedding information (entities, relations, etc.) into low-\ndimensional vector spaces.\nDespite being effective in modelling KG structural infor-\nmation, these methods suffer from the sparseness of KGs. \nSpecifically, some KRL models are trained to preserve the \ninherent structure of KGs, favoring entities that are highly \nconnected. According to the research, it is widely observed \nthat the degrees of entities in KGs approximately follow the \npower-law distribution, resulting in a long tail populated \nwith massive unpopular entities of low degrees¬†[6 ]. Con-\nsequently, due to information sparsity, the performance of \nKRL methods tends to degrade when processing long-tail \nentities. In short, the sparseness of KGs significantly impacts \nthe effectiveness of representing low-degree entities, posing \nan existing challenge.\nTo tackle the above challenges, a promising solution is to \nenhance KRL approaches with large language models. Large \nLanguage Models (LLMs), built on the Transformer archi-\ntecture¬†[ 7], have gained significant popularity in the NLP \nfield due to their remarkable performance. By¬†being pre-\ntrained on vast text corpora, LLMs demonstrate profound \n * Xin Wang \n wangx@tju.edu.cn\n * Haofen Wang \n carter.whfcarter@gmail.com\n1 College of¬†Intelligence and¬†Computing, Tianjin University, \nTianjin, China\n2 College of¬†Design and¬†Innovation, Tongji University, \nShanghai, China\n3 Faculty of¬†Science and¬†Technology, University of¬†Macao, \nMacao, China\n316 X.¬†Wang et al.\ncontent comprehension and rich real-world knowledge, \nwhich can be utilized to address the information sparsity \nin KGs by incorporating textual information such as entity \ndescriptions, offering potential opportunities for better \nknowledge representations.\nNowadays, the potential of LLM-enhanced KRL meth -\nods has attracted increasing attention across both academia \nand industry. In the stages of LLM development, these \nmodels exhibit an increase in parameters and real-world \nknowledge, progress from good to strong understanding \ncapabilities, and transition from task-specific to more gen-\neralized outputs. This evolution can be illustrated through \nthe development of three Transformer architectures. Accord-\ningly, LLM-enhanced KRL methods can be categorized into \nthree types based on these architectures. In encoder-based \nmethods, the textual context is fully leveraged to capture \ndetailed contextual information for knowledge representa-\ntion. In encoder-decoder-based methods, a seq2seq model \nis utilized to intuitively and simply meet all requirements \nthrough efficient encoding and decoding. In decoder-based \nmethods, extensive knowledge from large corpora is fully \nharnessed for downstream tasks.\nFew existing surveys have¬†explored the enhancement of \nKRL using LLM. With respect to KRL, Ge et¬†al.¬†[8 ] pro-\nvide a comprehensive overview of the two main branches \nin KRL: distance-based and semantic matching-based \napproaches, with few references to LLM enhancement. Bis-\nwas et¬†al. [9] give an overview of the advancements in KRL \nby incorporating features such as semantics, multi-modal, \ntemporal, and multilingual aspects, while rarely addressing \nLLM empowerment. Cao et¬†al. [10] classify and analyze \nKRL models from the perspective of mathematical spaces, \nspecifically through algebraic, geometric, and analytical \nperspectives, excluding considerations related to LLM aug-\nmentation. During this period, Pan et¬†al. [11] mainly focus \non the integration of LLMs and KGs from the perspective of \ngeneral frameworks, KG-enhanced LLMs, LLM-augmented \nKGs, and Synergized LLMs + KGs. They demonstrate how \nto leverage their respective advantages in a complementary \nmanner, with a minor portion dedicated to LLM-enhanced \nKRL.\nTo the best of our knowledge, this paper is the first to pre-\nsent a detailed categorization of LLM-enhanced KRL meth-\nods, conduct comprehensive reviews of downstream tasks, \nand identify emerging directions in these rapidly evolving \nfields. The contributions of this survey are summarized as \nfollows:\n‚Ä¢ Comprehensive Survey of Techniques  This survey pro -\nvides a comprehensive overview of LLM-enhanced KRL \ntechniques, examining the integration of encoder-based, \nencoder-decoder-based, and decoder-based methods with \nLLMs.\n‚Ä¢ Review of Existing Evaluations We systematically com-\npile and analyze experimental results from recent studies, \nproviding a consolidated perspective on their respective \nmerits and challenges across various downstream tasks.\n‚Ä¢ Future Directions The survey explores the foundational \nprinciples of LLM-enhanced KRL and proposes six \npromising avenues for future research.\nThe rest of this survey is organized as follows. Section¬† 2 \noutlines the pre-LLM KRL approaches, including transla-\ntion model and semantic matching model. These methods \nlaid the groundwork for modern approaches and provide \nessential context for understanding subsequent advance-\nments. Section¬† 3 provides a detailed illustration of LLM-\nenhanced KRL methods, categorized by three types of \nTransformer: encoder-based, encoder-decoder-based, and \ndecoder-based methods. Each category is discussed along \nwith its representative works and the developments in sub-\nsequent research. Section¬†4 introduces widely used datasets \nand the discussion of various evaluation approaches, offering \ninsights into the relative strengths and weaknesses of differ-\nent methods. Section¬†5 introduces six potential future direc-\ntions, from evolving knowledge representation to addressing \ndownstream tasks with KRL+LLM. Finally, Sect.¬† 6 sum-\nmarizes this paper.\n2  Pre‚ÄëLLM KRL\nPrior to the emergence of LLMs, KRL approaches primar -\nily focused on modeling the structural information of KGs, \nwhich are known as the pre-LLM KRL methods. Depend-\ning on the metrics used by the score function to measure \nthe rationality of triples, these approaches can be classified \ninto two types: translation models relying on distances and \nsemantic matching models based on semantics.\n2.1  Translation Model\nIn the translation model, the scoring function measures the \nreliability of facts by calculating the distance between two \nentities.\nThe most representative model is the Trans series [12] \n[13]. Bordes et¬†al. were inspired by the concept of trans-\nlational invariance and proposed TransE. This model rep-\nresents entities and relations in a KG using simple vector \nforms. Relations between entities are modeled as translation \nvectors, and it is assumed that the embedding vectors of a \nvalid triple (h ,¬†r,¬†t) should satisfy the condition: h + r ‚âà t . \nThe scoring function is defined under the L1 or L2 norm para-\ndigm: fr(h,t)=‚Äñ/u1D421+ /u1D42B‚àí /u1D42D‚ÄñL 1 ‚àïL 2\n . Here, the L1 norm (e.g., \nManhattan distance) refers to the sum of the absolute dif-\nferences between the vector components, while the L2 norm \n317Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \n(e.g., Euclidean distance) refers to the square root of the sum \nof the squared differences between the vector components. \nThese norms are used to measure the closeness between vec-\ntors in the embedding space, ensuring that valid triples are \nscored lower.\nSince TransE can only handle one-to-one relations, many \nextensions have been developed to address one-to-many, \nmany-to-one, and many-to-many relations. For example, \nWang et¬†al. proposed TransH [14], in which each relation¬†is \nmodeled as a hyperplane. TransR [15] introduces a relation-\nspecific space, and TransD [13] constructs a dynamic pro-\njection matrix for each entity and relation, simultaneously \nconsidering the diversity of entities and relations.\n2.2  Semantic Matching Model\nUnlike the translation model, semantic matching models \nmeasure the credibility by the similarity between the under-\nlying semantics of the matched entities and the contained \nrelations in the embedding vector space.\nTensor decomposition is a crucial technique for obtaining \nlow-dimensional vector representations. One notable model \nin this domain is RESCAL, which is based on 3D tensor \ndecomposition. Another model, DistMult [16], uses a neural \ntensor to learn representations of entities and relations in \nKGs. To simplify the model, DistMult restricts the relation \nmatrix to a diagonal matrix, which means it can only handle \nsymmetric relations.\nLater on, more scholars began to explore the use of neu-\nral networks in KRL due to their large parameters and high \nexpressiveness. ConvE, a 2D convolutional neural network \nmodel, emerged as a notable example. ConvE is character -\nized by its fewer parameters and high computational effi-\nciency. In this model, the interaction between entities and \nrelations is captured by stacking the embedding vectors \nof head and tail entities into a 2D matrix. Convolutional \noperations are then applied to this matrix to extract the rela-\ntionships between them.\nThe R-GCN [5 ] model handles highly multi-relational \ndata features. It assigns different weights to different types \nof relations, which can easily result in an excessive number \nof parameters. To address this issue, R-GCN employs two \nregularization techniques: basis function decomposition \nand block diagonal decomposition. SACN [17]¬†consists of \ntwo modules: the encoder WGCN and the decoder Conv-\nTransE. The WGCN module assigns different weights to dif-\nferent relations, effectively transforming the multi-relation \ngraph into multiple single-relation graphs, each with its own \nstrengths and weaknesses.\nHowever, these pre-LLM KRL methods are designed to \npreserve the inherent structure of KGs. struggling to effec-\ntively represent long-tail entities, as they rely exclusively on \nthe structural information within KGs, favoring entities that \nare rich in such structural data. Consequently, they fall short \nin leveraging the textual information embedded in KGs, fail-\ning to incorporate the contents of entities and relations into \ntheir representations.\n3  LLM‚ÄëEnhanced KRL Methods\nTo address the limitations of pre-LLM KRL methods, LLMs \nhave significantly advanced KRL by overcoming the reliance \non structural information alone. These models leverage tex-\ntual information through attention mechanisms, enabling the \ncreation of context-sensitive knowledge representations that \nbetter capture the nuances of entities and relations within \nKGs. This section examines methods to enhance KRL with \nLLMs, categorized into three approaches: encoder-based \n(¬ß3.1), encoder-decoder-based (¬ß¬† 3.2), and decoder-based \n(¬ß¬†3.3).\nTable¬†1 presents a comparison of various methods uti-\nlized for KRL with LLMs, emphasizing their input types, \nTable 1  Comparison of different methods\nMethod Input type Advantages Challenges\nTriple encoding Triple as a unit Holistic representation of triples Struggles with unseen triples\nTranslation encoding Head and relation together Improves reasoning by entity-relation \ndependencies\nPotentially limited expressiveness\nIndependent encoding Separate head, relation, tail Flexible and modular, supports zero-shot \nlearning\nLacks integration between elements\nStructural encoding Triple sequence Efficient representation of structure and \nrelation\nDependent on sequence quality\nTextual fine-tuning Textual triples Simplifies adaptation, reduces need for \nretraining\nLimited by the generative capabilities\nDescription generating Triples as prompts Enhances low-resource entity representation Heavy reliance on prompt quality\nPrompt engineering Natural language prompts Utilizes vast pre-trained knowledge High computational cost\nStructural fine-tuning Structural and textual embeddings Combines textual and structural embeddings Complex integration of embeddings\n318 X.¬†Wang et al.\nadvantages, and challenges. Each method adopts a unique \napproach to encoding and processing triples or related \ninformation, catering to specific tasks and scenarios within \nthe broader scope of KRL. Encoder-based approaches like \ntriple-based representation treats the triple as a single unit, \nproviding a holistic representation but struggling with \nunseen triples. Translation-based representation pairs the \nhead and relation for improved reasoning, although it may \nbe less expressive. Independent representation handles \nhead, relation, and tail separately, supporting modularity \nand zero-shot learning but lacking integration across ele-\nments. Similarly, encoder-decoder-based structure-based \nrepresentation methods leverage triple sequences for effi-\ncient structural representation, while textual fine-tuning \nsimplifies adaptation by focusing on textual triples but is \nlimited by the generative power of the underlying model. \nDecoder-based approaches like description generation, \nwhich enhances representation in low-resource settings \nvia prompts but is heavily dependent on prompt quality, \nand prompt engineering, which harnesses the pre-trained \nknowledge of LLMs but at high computational costs. \nLastly, structural fine-tuning combines structural and tex-\ntual embeddings to balance textual and structural knowl-\nedge, though it requires sophisticated integration.\nAn overview of various LLM-enhanced KRL models is \nprovided in Table¬†2, which provides a comprehensive sum-\nmary of various LLM-enhanced KRL models, organized \nby year, model name, type, base model, and open-source \navailability. This table highlights key trends in the field, \nsuch as the early dominance of encoder-based architectures \nleveraging BERT [ 48] (e.g., KG-BERT [ 18], StAR [ 22]) \nand the growing adoption of encoder-decoder models (e.g., \nGenKGC [37], KGT5 [33]) in recent years for tasks like \ngenerative knowledge graph completion. The increasing use \nof decoder-based LLMs (e.g., KG-LLM [43], KoPA [44]) in \n2023 and 2024 indicates a shift towards leveraging genera-\ntive and reasoning capabilities. Open-source contributions \nfrom many models (e.g., KEPLER [ 27], PKGC [32]) have \nTable 2  An overview of various \nLLM-enhanced KRL models Year Model Type Base model Open source\n2019 KG-BERT [18] Encoder BERT Github\n2020 MTL-KGC [19] Encoder BERT Github\nPretrain-KGE [20] Encoder BERT ‚Äì\nK-BERT [21] Encoder BERT Github\n2021 StAR [22] Encoder BERT, RoBERTa Github\nKG-GPT2 [23] Decoder GPT2 ‚Äì\nBERT-ResNet [24] Encoder BERT Github\nMEM-KGC [25] Encoder BERT ‚Äì\nLaSS [26] Encoder BERT, RoBERTa Github\nKEPLER [27] Encoder RoBERTa Github\nBLP [28] Encoder BERT Github\nSimKGC [29] Encoder BERT Github\nMLMLM [30] Encoder RoBERTa Github\n2022 LP-BERT [31] Encoder BERT, RoBERTa ‚Äì\nPKGC [32] Encoder BERT, RoBERTa, LUKE Github\nKGT5 [33] Encoder-Decoder T5 Github\nOpenWorld KGC [34] Encoder BERT ‚Äì\nkNN-KGE [35] Encoder BERT Github\nLMKE [36] Encoder BERT Github\nGenKGC [37] Encoder-Decoder BART Github\nKG-S2S [38] Encoder-Decoder T5 Github\n2023 LambdaKG [39] Encoder-Decoder BERT, BART, T5 Github\nCSPromp-KG [40] Encoder BERT Github\nReSKGC [41] Encoder-Decoder T5 ‚Äì\nReasoningLM [42] Encoder RoBERTa Github\nKG-LLM [43] Decoder ChatGLM, LLaMA 2 Github\nKoPA [44] Decoder Alpaca, LLaMA, GPT 3.5 Github\n2024 CD [45] Decoder PaLM2 Github\nCP-KGC [46] Decoder Qwen, LLaMA 2, GPT 4 Github\nKICGPT [47] Decoder GPT 3.5 ‚Äì\n319Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nplayed a crucial role in advancing the field, though some \nmodels remain proprietary.\nTo establish a unified framework for the symbols and for-\nmulas utilized in subsequent sections, Table¬† 3 summarizes \nthe main notations and their descriptions.\n3.1  Encoder‚ÄëBased Methods\nEncoder-based methods mainly employ encoders like BERT \n[48] and RoBERTa [49], offering the advantage of leverag-\ning abundant textual information, such as entity descriptions, \nto enhance KRL. These methods differ in their encoding \nof KG triples. In triple-based representation (¬ß¬† 3.1.1), the \nhead entity (HE), relation, and tail entity (TE) are encoded \nas a single unit. Conversely, translation-based representa-\ntion (¬ß¬†3.1.2) encodes the head entity and relation together, \nwith the tail entity encoded separately, optimizing through \nvector space distances. independent representation (¬ß¬†3.1.3) \nprocesses the head entity, relation, and tail entity as separate \ninputs, encoding each independently.\n3.1.1  Triple‚ÄëBased Representation\nTriple-based representation methods represent the entire \nknowledge graph triple as a single unit, leveraging the inte-\ngrated semantic and relational information of the head and \ntail entities as well as the relation. This holistic representa-\ntion not only enhances downstream tasks but also captures \nthe contextual significance of entities by fully utilizing rich \nlanguage patterns. Figure¬†1 illustrates a typical model, KG-\nBERT [18], which uses triple-based representation.\nKG-BERT treats triples as textual sequences and fine-\ntunes a pre-trained BERT model to predict the plausibility \nof a given triple. The model takes as input the descriptions \nof the head entity (DHE) h , relation (DR) r , and tail entity \n(DTE) t:\nwhere Tokhi , Tokri , and Tokti represent the tokens of the \nhead entity, relation, and tail entity descriptions, respec-\ntively. The model computes the final hidden vector of the \nunique [CLS] token, which serves as the aggregate sequence \nrepresentation. The plausibility score of a triple /u1D70F=( h, r, t) \nis then computed as follows:\nwhere C is the aggregated sequence representation derived \nfrom the [CLS] token, KG-BERT emphasizes seman -\ntic nuances and relational integrity even when triples are \nincomplete or corrupted. The cross-entropy-based loss func-\ntion facilitates robust learning by effectively differentiating \nbetween noisy negative samples and valid triples:\nwhere y/u1D70F‚àà{ 0, 1} is the label indicating whether the triple \nis positive or negative, and D+ and D‚àí are the sets of posi-\ntive and negative triples, respectively. Negative triples are \ngenerated by corrupting either the head or the tail entity of \n(1)\nInput sequence:[CLS] Tokhi [SEP ] Tokri [SEP ] Tokti [SEP]\n(2)s/u1D70F= sigmoid(CW T)\n(3)L =‚àí\n/uni2211.s1\n/u1D70F‚ààD +‚à™D ‚àí\n(y/u1D70Flog(s/u1D70F0 )+( 1 ‚àí y/u1D70F) log(s/u1D70F1 ))\nTable 3  Notations of Concepts Notation Description\nh, r, t Head entity, relation, tail entity\nX(h, r) , X(t) Input sequences for head-relation and tail entity\nD, D+ , D‚àí Set of all triples, positive triples, negative triples\nu , v Contextualized embeddings of head-relation pair and tail entity\nsc , sd Classification score, Distance score\nh , r , t Embeddings for head entity, relation, and tail entity\nLc , Ld , L Classification loss, Distance-based loss, Total loss\n/u1D706 , /u1D6FE , n, K Margin, Weight factor, Number of negative samples, Sum limit\n/u1D70E Sigmoid function\n/u1D6FC , /u1D6FD Auxiliary loss weights\nK , I, X Transformed structural embeddings, Instruction prompt, Triple prompt\nFig. 1  An overview of triple-based representation methods\n320 X.¬†Wang et al.\na positive triple. Negative triples, generated through entity \ncorruption, simulate real-world ambiguities, making the \ntraining process more resilient to such scenarios.\nMTL-KGC [19] employs a multi-task learning framework \nto enhance knowledge graph completion. It incorporates addi-\ntional tasks like relation prediction and relevance ranking, sig-\nnificantly improving upon models such as KG-BERT by learn-\ning more relational properties and effectively distinguishing \nbetween lexically similar candidates.\nK-BERT [21] integrates structured domain knowledge \nfrom knowledge graphs directly into the pre-training process. \nIt retains the original BERT architecture but adds a mecha-\nnism to inject relevant triples from a knowledge graph into the \ntraining examples, enriching the language model with domain-\nspecific information.\nMLMLM [30] introduces a novel method for link predic-\ntion using Mean Likelihood Masked Language Models to \ngenerate potential entities directly. This approach leverages \nthe knowledge embedded in pre-trained MLMs to enhance the \ninterpretability and scalability of knowledge graphs.\nPKGC [32] is a pre-trained language model (PLM) based \nmodel for knowledge graph completion that utilizes prompt \nengineering to enhance knowledge utilization. This model \nadapts PLM for the task by converting knowledge triples into \nnatural language prompts, facilitating the effective use of latent \nknowledge¬†of the PLM.\nCSProm-KG [40] incorporates both structural and textual \ninformation into the knowledge graph completion process. \nThis model employs Conditional Soft Prompts that adapt the \ninputs based on the structure of the knowledge graph, allowing \npre-trained language models to efficiently utilize both types of \ninformation.\n3.1.2  Translation‚ÄëBased Representation\nTranslation-based representation methods focus on encoding \nthe head entity and relation while treating the tail entity sepa-\nrately. This technique relies on optimizing the vector space \ndistances between these entities, facilitating more accurate \nknowledge representation and reasoning. Figure¬†2 depicts a \nrepresentative model, StAR [22], that employs translation-\nbased representation.\nThis model divides each triple into two asymmetric parts: \none combining the head entity and relation, and the other con-\nsisting of the tail entity. The StAR model employs a Siamese-\nstyle textual encoder to convert these parts into contextualized \nrepresentations. The translation function is defined as follows:\nwhere X(h,r) =[ CLS ] Xh [SEP] Xr [SEP]  and  \nXt =[ CLS] Xt [SEP] are the input sequences for the head-\nrelation pair and the tail entity, respectively. Pool(‚ãÖ) denotes \n(4)\nu = Pool(Transformer-Enc(X(h,r))), v = Pool(Transformer-Enc(Xt))\nthe pooling operation to obtain the sequence-level represen-\ntation from the transformer‚Äôs output.\nThe plausibility score of a triple (h ,¬†r,¬†t) is calculated \nusing a combination of a deterministic classifier and a spa-\ntial measurement:\nwhere [u;u‚ó¶v;u ‚àí v;v] denotes the interactive concatenation \nof the contextualized embeddings u and v , and MLP(‚ãÖ) is a \nmulti-layer perceptron.\nThe model is trained using both a triple classification \nobjective and a contrastive loss:\nwhere D is the set of positive triples, N (tp) denotes the set \nof negative triples generated by corrupting positive triples, \nand /u1D706 is the margin.\nThe overall training loss is a weighted sum of the two \nobjectives:\nwhere /u1D6FE is a hyperparameter that balances the two losses.\nSimKGC [29] uses a bi-encoder architecture to enhance \ncontrastive learning efficiency in knowledge graph com-\npletion. It incorporates three types of negative sampling-\nin-batch, pre-batch, and self-negatives-improving training \nefficacy through larger negative sets and a novel InfoNCE \nloss function.\nLP-BERT [31] employs a multi-task pre-training strat-\negy, enhancing knowledge graph completion by predicting \nrelations between entities. This method integrates masked \nlanguage, entity, and relation modeling tasks, significantly \n(5)\nsc = softmax(MLP([u;u‚ó¶v;u ‚àí v;v])), sd =‚àí ‚Äñu ‚àí v‚Äñ2\n(6)Lc =‚àí 1\nÔøΩD ÔøΩ\nÔøΩ\ntp‚ààD\n‚éõ\n‚éú\n‚éú‚éù\nlog sc +\nÔøΩ\ntÔøΩ\np‚ààN (tp)\nlog(1 ‚àí sÔøΩ\nc)\n‚éû\n‚éü\n‚éü‚é†\n(7)Ld = 1\n/uni007C.varD /uni007C.var\n/uni2211.s1\ntp‚ààD\n/uni2211.s1\ntÔøΩ\np‚ààN (tp)\nmax(0, /u1D706‚àí sd + sÔøΩ\nd)\n(8)L = Lc + /u1D6FELd\nFig. 2  An overview of translation encoding methods\n321Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nimproving link prediction. It effectively manages unseen \nentities and relations by leveraging contextualized entity \nand relation information from large datasets.\n3.1.3  Independent Representation\nIndependent representation methods separately encode \neach component of a triple: the head entity, relation, and \ntail entity. This approach allows for flexible and modular \nrepresentations that benefit specific knowledge graph appli-\ncations. A significant advantage of independent representa-\ntion is its ability to enhance structural information within \nthe KG, providing zero-shot capabilities. Figure¬†3 shows the \narchitecture of a typical independent representation model, \nKEPLER [27].\nKEPLER generates embeddings using textual descrip-\ntions of entities, which reduces reliance on the frequency \nof training samples. This approach ensures that even low-\nfrequency entities can achieve high-quality representations \nthrough their textual descriptions, mitigating degradation in \nembedding quality caused by imbalanced data:\nwhere texth and textt are the descriptions for the head and tail \nentities, respectively, each prefixed with a special token ‚ü®s‚ü© . \nT ‚àà ‚Ñù/uni007C.varR/uni007C.var√ód is the relation embeddings matrix, and h , t, and \nr represent the embeddings for the head entity, tail entity, \nand relation, respectively. The encoding function E‚ü®s‚ü© lever-\nages pre-trained language models to capture contextual and \nsemantic relationships instead of depending solely on entity \nfrequency.\nKEPLER‚Äôs knowledge embedding objective uses negative \nsampling for optimization:\nwhere (h ÔøΩ\ni, r, tÔøΩ\ni) are negative samples, /u1D6FE is the margin, /u1D70E is the \nsigmoid function. The described mechanism ensures that \nembeddings are derived from descriptive semantics rather \n(9)h = E ‚ü®s‚ü©(texth ), t = E ‚ü®s‚ü©(textt), r = Tr,\n(10)\nLKE =‚àí log /u1D70E(/u1D6FE‚àí dr(h,t)) ‚àí 1\nn\nn/uni2211.s1\ni=1\nlog /u1D70E(dr(hÔøΩ\ni,tÔøΩ\ni)‚àí/u1D6FE),\nthan raw frequency, mitigating the risk of over-representing \nfrequently occurring entities while under-representing rare \nones. dr is the scoring function:\nwith the norm p  set to 1. The negative sampling policy \ninvolves fixing the head entity and randomly sampling a tail \nentity, and vice versa.\nThe BERT-ResNet [24] model extends the capabilities \nof encoder-based KRL methods by integrating BERT with \na deep residual network. This combination enhances the \nhandling of sparse connectivity in knowledge graphs. The \nmodel leverages BERT‚Äôs robust embeddings and ResNet‚Äôs \ndeep convolutional architecture, significantly improving \nentity ranking performance even with limited training data.\nThe BLP [28] model focuses on inductive link prediction \nby utilizing textual entity descriptions and pre-trained lan-\nguage models, emphasizing its capacity to generalize across \nunseen entities. This model surpasses previous approaches \nby incorporating dynamic graph embeddings, allowing it to \neffectively adapt to continuously evolving knowledge graphs \nwithout retraining.\n3.2  Encoder‚ÄëDecoder‚ÄëBased Methods\nEncoder-decoder-based methods employ models such \nas BART [50] and T5 [51], are known for their intuitive \nsimplicity, as all desired functionalities can be achieved \nthrough a straightforward seq2seq model. These meth-\nods are classified by the type of input sequence utilized. \nStructure-based representation methods (¬ß 3.2.1) resem -\nble Encoder-based methods, utilizing encoder-type triple \nsequences as input. Fine-tuning methods (¬ß 3.2.2), similar \nto Decoder-based methods, use natural language expres-\nsions of triples as decoder input.\n3.2.1  Structure‚ÄëBased Representation\nStructure-based representation leverages the structure of \nthe triples¬†by feeding them into the encoder as sequences. \nwhich allows the model to capture both the syntactic and \nrelational structure of the triples. By integrating struc-\ntural information with natural language input, this method \nenhances the ability to represent complex relations. Fig-\nure¬† 4 demonstrates the GenKGC [37] architecture that \nexemplifies structure-based representation.\nThis approach transforms knowledge graph completion \ninto a sequence-to-sequence generation task, leveraging \npre-trained language models to generate target entities \nfrom input sequences representing head entities and rela-\ntions. The structure-based representation is as follows:\n(11)dr(h, t)= ‚Äñh + r ‚àí t‚Äñp ,\nFig. 3  An overview of independent representation methods\n322 X.¬†Wang et al.\nGiven a triple with a missing tail entity (eh ,r,? ) , the \ninput sequence is constructed by concatenating the \ndescriptions of the head entity deh\n and the relation dr:\nwhere deh\n and dr are the textual descriptions of the head \nentity and the relation, respectively. The output sequence \ncorresponds to the target tail entity det\n.\nThe generative process is defined as:\nwhere x is the input sequence, y is the output sequence, and \nN is the length of the output sequence. The model is trained \nusing a standard sequence-to-sequence objective function:\nGenKGC improves representation learning and reduces \ninference time through relation-guided demonstration and \nentity-aware hierarchical decoding. Relation-guided dem-\nonstration augments the input sequence with examples of \ntriples that share the same relation:\nEntity-aware hierarchical decoding constrains the decod-\ning process using entity-type information, which reduces \nthe search space during generation and enhances inference \nefficiency.\nLambdaKG [39] advances the application of pre-trained \nlanguage models in knowledge graph embeddings by inte-\ngrating structural and textual information into a unified \nmodel architecture. It employs advanced training techniques \nsuch as prompt engineering, negative sampling, and relation-\naware modeling to enhance the efficiency and accuracy of \nknowledge graph representations.\n(12)Input sequence:[CLS] deh\n[SEP] dr [SEP]\n(13)pùúÉ(y/uni007C.varx)=\nN/uni220F.s1\ni=1\npùúÉ(yi/uni007C.vary<i,x)\n(14)L =‚àí log p/u1D703(y/uni007C.varx)\n(15)\nx =[ CLS ] demonstration(r)[ SEP] deh\n[SEP] dr [SEP]\n3.2.2  Textual Fine‚ÄëTuning\nTextual fine-tuning methods adapt pre-trained encoder-\ndecoder models using natural language descriptions of \ntriples. This approach tailors the model‚Äôs generative capa -\nbilities to specific knowledge representation tasks, offering \nsignificant advantages in terms of training and inference effi-\nciency. An illustration of a typical textual fine-tuning model \nKGT5 [33] is presented in Fig.¬†5.\nKGT5 fine-tunes a T5 model for both link prediction and \nquestion answering (QA) tasks. This process involves initial \npre-training on link prediction, followed by fine-tuning on \nQA datasets, balancing these tasks using a regularization \napproach.\nGiven a set of entities E  and relations R , a knowledge \ngraph K ‚äÜ E √ó R √ó E consists of triples (s ,¬† p,¬† o). KGT5 \ntreats KG link prediction and QA as sequence-to-sequence \ntasks, utilizing textual representations of entities and rela-\ntions. A verbalization scheme is employed to convert link \nprediction queries into textual queries.\nThe training process is described by the following \nformulas:\nwhere pa is the log probability of the predicted entity a , /u1D6FC \nis a constant hyperparameter, and N(e) represents the n-hop \nneighborhood of the entity e.\nThe model‚Äôs architecture and training process effectively \nreduce the model size while maintaining or improving per -\nformance on large KG and KGQA tasks.\nDuring the fine-tuning phase for QA tasks, KGT5 con-\ntinues training on link prediction objectives. This dual-task \nregularization ensures that the model retains its ability to \ngeneralize beyond the specific QA dataset, as the link predic-\ntion tasks anchor the learning to broader KG. Each training \nbatch contains an equal number of QA examples and link \nprediction examples. This balanced batching prevents over-\nfitting to the QA data while ensuring that the model remains \ngrounded in the broader structure of the KG.\n(16)Score(a)= pa + /u1D6FCifa ‚àà N (e)\n(17)Score(a)= pa otherwise\nFig. 4  An overview of structure-based representation methods\n Fig. 5  An overview of fine-tuning methods\n323Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nKG-S2S [38] is a state-of-the-art sequence-to-sequence \n(Seq2Seq) generative framework designed to address various \nchallenges in knowledge graph completion (KGC) across \ndifferent settings without requiring structural modifications \nfor each graph type. This model uniquely overcomes the \nlimitations of previous KGC approaches that are tightly cou-\npled with specific graph structures, restricting their adapt-\nability to new or evolving KGs. By treating all elements \nof knowledge graphs-entities, relations, and metadata-as \nsequences in a unified ‚Äòflat‚Äô text format, KG-S2S simplifies \ndata representation and enhances the model‚Äôs flexibility and \nscalability. It employs advanced fine-tuning techniques on \npre-trained language models, integrating novel mechanisms \nsuch as entity descriptions and relational soft prompts to \nenrich the model‚Äôs contextual understanding.\n3.3  Decoder‚ÄëBased Methods\nDecoder-based methods, utilizing models like LLaMA [52] \nand GPT-4 [53], are defined by the critical function of the \ndecoder in the representation learning process. These meth-\nods leverage much larger semantic knowledge without incur-\nring additional training overhead. Description generation \nmethods (¬ß¬†3.3.1) produce descriptive text for low-resource \nentities, employing encoder-based or encoder-decoder-based \nmethods for other tasks. Prompt engineering (¬ß¬† 3.3.2) uti-\nlizes the decoder as a question-answering tool, leveraging \nnatural language to retrieve triple information and execute \ndownstream tasks. Structural fine-tuning (¬ß¬†3.3.3) integrates \nstructural and textual embeddings, refining the decoder‚Äôs \noutput to enhance knowledge representation.\n3.3.1  Description Generation\nDescription generation methods enhance the representation \nof low-resource entities by creating descriptive text. This \napproach supplements the textual information, improving \nthe performance of previous methods and ensuring a richer, \nmore complete entity representation. Figure¬† 6 illustrates a \ntypical contextualization distillation (CD) [45] method¬†used \nin description generation method.\nThis method utilizes LLMs to convert concise structural \ntriplets into rich, contextual segments. The process involves \ngenerating descriptive contexts for knowledge graph com-\npletion using prompts. The main steps are as follows:\nGiven a triplet (h,¬†r,¬†t) where h is the head entity, r is the \nrelation, and t is the tail entity, the method generates descrip-\ntive context c using an LLM prompted with the triplet. The \nprocess can be formulated as:\n(18)p i = Template(h i,ri,ti)\nwhere p i is the prompt template filled with the triplet com-\nponents, and ci is the descriptive context generated by the \nLLM.\nTwo auxiliary tasks, reconstruction and contextualization, \nare introduced to train smaller knowledge graph comple-\ntion (KGC) models with these enriched triplets. The recon-\nstruction task aims to restore corrupted descriptive contexts \nusing masked language modeling (MLM), while the con-\ntextualization task trains the model to generate descriptive \ncontext from the original triplet. The losses for these tasks \nare defined as:\nwhere /u1D4C1 is the cross-entropy loss function, f is the model, Ii \nis the concatenated input of the head, relation, and tail, and \nN is the number of samples.\nThe final loss for training the KGC models combines the \nKGC loss with the auxiliary task losses:\nwhere /u1D6FC and /u1D6FD are hyperparameters that balance the contri-\nbutions of the auxiliary losses.\nThe CP-KGC [46] model represents a significant \nadvancement in text-based KGC. It specifically leverages \nConstrained-Prompt Knowledge Graph Completion with \nLarge Language Models to refine and enhance textual \ndescriptions within KGC datasets. CP-KGC uses simple, \ncarefully designed prompts to regenerate or supplement \nexisting textual descriptions, improving the overall expres-\nsiveness and utility of the data. This approach effectively \naddresses issues like hallucinations in text generation by \n(19)ci = LLM(pi)\n(20)L rec = 1\nN\nN/uni2211.s1\ni=1\n/u1D4C1(f(MLM (ci)),ci)\n(21)L con = 1\nN\nN/uni2211.s1\ni=1\n/u1D4C1(f(Ii),ci)\n(22)LÔ¨Ånal = Lkgc + /u1D6FC‚ãÖ Lrec + /u1D6FD‚ãÖ Lcon\nFig. 6  An overview of description generation methods\n324 X.¬†Wang et al.\nLLMs, ensuring more accurate and contextually relevant \noutputs.\n3.3.2  Prompt Engineering\nPrompt engineering leverages the natural language capabili-\nties of decoders, framing knowledge retrieval and represen-\ntation tasks as question-answering problems, which utilizes \nthe vast amount of real-world knowledge within the model. \nFigure¬†7 shows a representative model KG-LLM [43] using \nprompt engineering.\nThis method transforms triples into natural language \nprompts and employs LLMs to predict the plausibility of \nthe triples or complete missing information. The process is \nas follows:\nFor a given triple (h,¬†r,¬†t), where h is the head entity, r is \nthe relation, and t  is the tail entity, a prompt is constructed \nto query the LLM. For example, for triple classification, the \nprompt might be:\nThe LLM then generates a response indicating the plau-\nsibility of the triple. A scoring function quantifies this \nplausibility:\nwhere p(h,¬†r,¬†t) is the prompt generated for the triple (h,¬†r,¬†t), \nand LLM represents the large language model.\nThe training objective uses a cross-entropy loss to fine-\ntune the LLM on labeled triples:\nwhere D is the set of training triples, and y is the label indi-\ncating the truth of the triple.\nThe study performs specific instruction tuning on LLMs, \naligning their general natural language understanding \n(23)/u1D67F/u1D69B/u1D698/u1D696/u1D699/u1D69D‚à∂ /u1D678/u1D69C/u1D692/u1D69D/u1D69D/u1D69B/u1D69E/u1D68E/u1D69D/u1D691/u1D68A/u1D69Dhrt ?\n(24)s(h,r,t)=LLM(p(h, r,t))\n(25)\nL =‚àí\n/uni2211.s1\n(h,r,t)‚ààD\n[y log s(h,r,t)+( 1 ‚àí y) log(1 ‚àí s(h,r,t))]\nabilities with the processing of knowledge graph triples. \nFine-tuned models (e.g., KG-LLaMA and KG-ChatGLM) \nextract and utilize knowledge representations more effi-\nciently, further minimizing errors caused by language \nambiguity.\nThe KICGPT [47] model integrates an LLM with \nknowledge graph completion (KGC) methods to address \nchallenges in traditional KGC approaches. It significantly \nreduces training overhead by using in-context learning strat-\negies, eliminating the need for explicit model fine-tuning. \nThe model leverages the LLM‚Äôs extensive pre-trained knowl-\nedge base and a structure-aware KG retriever to improve \nthe handling of long-tail entities. This integration allows \nKICGPT to effectively utilize both structured KG informa-\ntion and the broad knowledge base of the LLM, providing a \nmore robust framework for KGC tasks. Notably, the model \nemploys ‚ÄòKnowledge Prompt‚Äô strategies, guiding the LLM \nwith structured prompts that incorporate KG information, \nthus enhancing its ability to make informed predictions \nabout missing entities in the KG.\n3.3.3  Structural Fine‚ÄëTuning\nStructural fine-tuning combines embeddings from structural \nand textual information, feeding them into the decoder. By \nincorporating structural information from KGs, this approach \noptimizes the output of LLMs, providing a more comprehen-\nsive representation of the KG triples. Figure¬†8 depicts a struc-\ntural fine-tuning model KoPA [44].\nThis method incorporates structural embeddings of entities \nand relations into LLMs to improve knowledge graph comple-\ntion tasks.\nKoPA‚Äôs core concept involves two primary steps: pre-train-\ning structural embeddings and using a prefix adapter to inject \nthese embeddings into the LLM. The structural embeddings \nare first learned through a self-supervised pre-training process, \ncapturing the knowledge graph‚Äôs structural information. The \nprocess is as follows:\nFor each triple (h,¬†r,¬†t), the structural embeddings h, r, t are \nlearned using the scoring function:\nThe pre-training objective involves a margin-based ranking \nloss with negative sampling:\nwhere (h ÔøΩ\ni, rÔøΩ\ni, tÔøΩ\ni) are negative samples, /u1D6FE is the margin, and /u1D70E \nis the sigmoid function.\n(26)F (h, r,t)= ‚Äñh + r ‚àí t‚Äñ2\n(27)\nLpre =‚àí log /u1D70E(/u1D6FE‚àí F (h,r,t)) ‚àí\nK/uni2211.s1\ni=1\nlog /u1D70E(F (hÔøΩ\ni,rÔøΩ\ni,tÔøΩ\ni)‚àí /u1D6FE)\nFig. 7  An overview of prompt engineering methods\n325Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nOnce pre-trained, KoPA uses a prefix adapter to map these \nstructural embeddings into the LLM‚Äôs textual token space. \nThis transformation is performed as follows:\nwhere P is a projection layer, and ‚äï denotes concatenation. \nThe transformed embeddings, known as virtual knowledge \ntokens, are prepended to the input sequence S:\nwhere I is the instruction prompt, and X is the triple prompt.\nThe fine-tuning objective for the LLM with KoPA involves \nminimizing the cross-entropy loss:\nwhere si are the tokens in the input sequence SKoPA.\nKG-GPT2 [23] is an adaptation of the GPT-2 language \nmodel for knowledge graph completion. KG-GPT2 lever -\nages GPT-2‚Äôs contextual capabilities to address incomplete \nknowledge graphs by predicting missing links and relation-\nships. This model treats each triple in a knowledge graph \nas a sentence, allowing GPT-2 to accurately classify the \nlikelihood of triples. By contextualizing triples, KG-GPT2 \nsurpasses traditional embedding techniques, incorporating \nricher linguistic and semantic information to enhance graph \ncompletion.\n4  Experiments and¬†Evaluations\nThis section systematically reviews existing experiments and \nevaluations to analyze the performance and effectiveness of \nvarious LLM-enhanced KRL methods discussed previously. \nBy consolidating results from various studies, we aim to pre-\nsent a structured overview of their potential and constraints.\n4.1  Datasets\nTo facilitate a systematic and comparative analysis of exist-\ning KRL methods, this section summarizes the datasets com-\nmonly used in previous studies, which serve as a foundation \n(28)K = P(h) ‚äï P(r) ‚äï P(t)\n(29)SKoPA = K ‚äï I ‚äï X\n(30)LKoPA =‚àí 1\n/uni007C.varSKoPA /uni007C.var\n/uni007C.varSKoPA /uni007C.var/uni2211.s1\ni=1\nlogP LLM (si/uni007C.vars<i)\nfor evaluating the generalizability and effectiveness of differ-\nent methods. The key statistics for each dataset are detailed \nin Table¬†4. This overview aims to consolidate the dispersed \ninformation and provide a unified reference for subsequent \nanalysis.\n4.2  Metrics\nThis section employs the commonly understood defini-\ntions of accuracy, precision, recall, and F1-score, which \nare widely recognized metrics for evaluating classification \ntasks. In addition to these, we introduce metrics specifically \ndesigned for ranking tasks: Mean Rank (MR), Mean Recip-\nrocal Rank (MRR), and Hits@K.\n‚Ä¢ Mean Rank is a metric for ranking tasks, such as knowl-\nedge graph completion. It represents the average rank \nposition of the correct entity or relation in the predicted \nlist. Lower MR values indicate better performance: \n‚Ä¢ Mean Reciprocal Rank evaluates the effectiveness of \na ranking algorithm. It is the average of the reciprocal \nranks of the correct answers, useful for information \nretrieval and question answering tasks. Higher MRR \nvalues indicate better performance: \n‚Ä¢ Hits@K measures the proportion of correct answers \nappearing in the top K  predicted results, commonly \nused in ranking tasks and knowledge graph completion. \nHigher Hits@K values indicate better performance: \n4.3  Downstream Tasks\nBuilding on the dataset overview, this section focuses on \ndownstream tasks commonly used to evaluate KRL mod-\nels. By summarizing these tasks, we propose a comparative \nframework that can illuminate the strengths and weaknesses \nof different methods under various conditions. While we \nsystematically organize findings from prior studies (Fig.¬†9), \nit is worth noting that certain metrics or settings may not \nalways align, making direct comparisons of some models \nless straightforward. Nevertheless, we hope these insights \n(31)MR = 1\nN\nN/uni2211.s1\ni=1\nranki\n(32)MRR = 1\nN\nN/uni2211.s1\ni=1\n1\nranki\n(33)Hits@K = 1\nN\nN/uni2211.s1\ni=1\n/u1D7D9(ranki ‚â§ K )\nFig. 8  An overview of structural fine-tuning methods\n326 X.¬†Wang et al.\ncan foster a deeper understanding of current challenges in \nthe field.\n4.3.1  Entity Type Classification\nDefinition 1  Entity Typing (ET) ET is a fundamental task \nthat involves assigning predefined types or categories to enti-\nties mentioned within a text. Formally, given a text corpus \nT and a set of entities E, the goal is to determine a mapping \nf ‚à∂ E ‚Üí C  , where C is the set of predefined types. Specifi-\ncally, for an entity e ‚àà E appearing in a context t ‚àà T  , the \ntask is to predict its type c ‚àà C . This can be expressed as:\n(34)f(e,t)= c, where e ‚àà E , t‚àà T , and c ‚àà C .\nTable¬† 5 presents experimental results for ET on two \nwidely used datasets, FIGER and Open Entity. Several nota-\nble trends emerge:\nTraditional feature-based methods such as NFGEC and \nUFET demonstrate early successes in capturing essential \nentity information, yet their performance lags behind the \nmore recent transformer-based approaches. Models built \nupon LLM, including BERT and RoBERTa, generally \nachieve higher accuracy and F1 scores, showcasing the \nimportance of contextual embeddings derived from exten-\nsive text corpora.\nMethods that incorporate external knowledge sources \n(e.g., ERNIE, KnowBert, and KEPLER) further improve \nET performance by injecting entity-related information into \ntheir representations. For instance, ERNIE achieves impres-\nsive results on FIGER (e.g., reaching 57.19% accuracy and \n73.39% in micro-F1), highlighting the benefits of entity-level \nTable 4  Statistics on Datasets Dataset # Ent # Rel # Train # Dev # Test Task Year\nFIGER [54] ‚Äì ‚Äì 2,000,000 10,000 563 ET 2015\nOpen Entity [55] ‚Äì ‚Äì 2,000 2,000 2,000 ET 2018\nTACRED [56] ‚Äì 42 68,124 22,631 15,509 RC 2017\nFewRel [57] ‚Äì 80 8,000 16,000 16,000 RC 2018\nWN11 [58] 38,696 11 112,581 2,609 10,544 TC 2013\nFB13 [58] 75,043 13 316,232 5,908 23,733 TC 2013\nWN9 [59] 6,555 9 11,741 1,337 1,319 TC 2022\nFB15K-237N [60] 13,104 93 87,282 7,041 8,226 TC 2022\nNations [61] 14 55 1,592 199 201 LP 2007\nFB15K [62] 14,951 1,345 483,142 50,000 59,071 LP 2013\nFB15k-237 [63] 14,541 237 272,115 17,535 20,466 LP 2015\nWN18RR [64] 40,943 11 86,835 3,034 3,134 LP, LP(ZS) 2018\nUMLS [64] 135 46 5,216 652 661 LP 2018\nYAGO3-10 [64] 103,222 30 490,214 2,295 2,292 LP 2018\nNELL-ONE [65] 68,545 822 189,635 1,004 2,158 LP(ZS) 2018\nCoDEx-S [66] 45,869 68 32,888 1,827 1,828 LP 2020\nCoDEx-M [66] 11,941 50 185,584 10,310 10,311 LP 2020\nCoDEx-L [66] 45,869 69 551,193 30,622 30,622 LP 2020\nWikidata5M-transductive [27] 4,594,485 822 20,614,279 5,163 5,133 LP 2021\nWikidata5M-inductive [27] 4,594,458 822 20,496,514 6,699 6,894 LP(ZS) 2021\nDiabetes [67] 7,886 67 56,830 1,344 1,936 LP 2022\nFig. 9  An overview of down-\nstream tasks achievable by each \nLLM-enhanced KRL models\n327Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nknowledge. Similarly, KEPLER shows a slight edge in F1 \nscore (76.20%) over comparable models on Open Entity, \nsuggesting that combining textual embeddings with knowl-\nedge graph features can enhance model robustness.\nNot all models report a full set of comparable metrics. \nFor example, KEPLER does not provide FIGER results. \nMeanwhile, KnowBert on Open Entity lacks certain detailed \nscores. Such reporting gaps limit direct comparisons across \nall methods. Hence, while we observe a consistent per -\nformance boost from integrating external knowledge into \nLLMs, we cannot definitively conclude that knowledge-\naugmented models universally dominate all ET benchmarks.\nThese findings underscore both the effectiveness of LLMs \nin capturing rich context and the potential gains from fusing \ntextual and structural information. To facilitate more com-\nprehensive evaluations, future research may focus on estab-\nlishing consistent experimental protocols and shared bench-\nmarks, enabling a deeper understanding of how ET models \nperform across diverse data domains and label distributions.\n4.3.2  Relation Classification\nDefinition 2  Relation Classification (RC) RC is a crucial \ntask that involves identifying the semantic relationship \nbetween a pair of entities within a sentence. Formally, \ngiven a sentence S  containing two entities e1 and e2 , and a \npredefined set of relation types R , the task is to determine \na mapping f ‚à∂( e1 ,e2 ,S ) ‚Üí r , where r ‚àà R is the relation \ntype. Specifically, for a pair of entities (e1 ,e2 ) in the context \nof sentence S, the goal is to predict the relation r. This can \nbe expressed as:\nTable¬†6 summarizes the performance of various RC mod-\nels on the FewRel and TACRED datasets.\nEarly methods such as CNN¬† [71] rely on convolutional \nfeature extractors and achieve an F1 score of 69.35%, illus-\ntrating the utility of neural architectures in capturing local \n(35)\nf(e1 ,e2 ,S)=r , where e1 ,e2 ‚àà E , S ‚àà T , and r ‚àà R.\ntext patterns. When large-scale pre-training is introduced, \ntransformer-based models markedly improve results. BERT_\nBase¬† [48] raises F1 to 84.89%, while ERNIE¬† [69], which \nincorporates entity-level knowledge, attains 88.32%. These \ngains highlight the value of infusing external knowledge \nfor more nuanced relation modeling. SemGL¬† [75] further \nreports high precision (95.11%) on FewRel, but lacks com-\nplete recall and F1 metrics, preventing direct, comprehensive \ncomparisons.\nOn TACRED, CNN and PA-LSTM exhibit moderate per-\nformance, trailing behind graph-based encoders like C-GCN¬† \n[73], which benefits from structured dependency information \n(F1¬†=¬†66.40%). Transformer-based systems perform better \nstill: BERT_Large hits 70.10% F1, and RoBERTa¬† [49] and \nMTB¬† [74] exceed 70.70% F1. Subsequent models leverage \nexternal knowledge to enhance representations. For example, \nKnowBert¬† [70] and ERNIE incorporate knowledge bases to \nsurpass 71% F1. The current best results among the listed \nsystems come from KEPLER¬† [27], an LLM-enhanced \napproach that achieves an F1 of 72.00%, illustrating the \nadded benefit of entity-description alignment in capturing \nrelational patterns.\nOverall, transformer-based encoders dominate, particu-\nlarly when combined with external knowledge. Traditional \nCNN or RNN architectures, despite strong early results, \nstruggle to keep pace with LLMs. Meanwhile, knowledge-\naware methods such as ERNIE, KnowBert, and KEPLER \nshowcase further advances by seamlessly blending textual \nand structural cues. However, direct model comparisons are \npartially hampered by incomplete or inconsistent reporting \nacross datasets.\n4.3.3  Relation Prediction\nDefinition 3 Relation Prediction (RP) RP is a task in KG \ncompletion that aims to predict the relationship between two \nentities within a knowledge graph. Given a set of entities E \nand a set of possible relations R , the objective is to predict \nTable 5  Experimental statistics \nunder the ET subtask\nThe best and second-best results in tables are shown in bold and underlined, respectively\nModel LLM-\nEnhanced\nYear FIGER Open Entity\nAcc Macro Micro P R F1\nNFGEC [68] √ó 2016 55.60 75.15 71.73 68.80 53.30 60.10\nUFET [55] √ó 2018 ‚Äì ‚Äì ‚Äì 77.40 60.60 68.00\nBERT [48] √ó 2018 52.04 75.16 71.63 76.37 70.96 73.56\nRoBERTa [49] √ó 2019 ‚Äì ‚Äì ‚Äì 77.40 73.60 75.40\nERNIE [69] √ó 2019 57.19 76.51 73.39 78.42 72.90 75.56\nKnowBert [70] √ó 2019 ‚Äì ‚Äì ‚Äì 78.70 72.70 75.60\nKEPLER [27] ‚úì 2021 ‚Äì ‚Äì ‚Äì 77.80 74.60 76.20\n328 X.¬†Wang et al.\nthe relation r ‚àà R between a given pair of head entity h ‚àà E \nand tail entity t ‚àà E  . Formally, the task can be defined as:\nFor a given pair of entities (h,¬†t), the task is to determine the \nmost likely relation r that connects them:\nTable¬†7 reports representative results on two benchmark \ndatasets, FB15K and YAGO3-10‚Äì100.\nClassical translational methods, such as TransE and \nPTransE, already achieve high performance on FB15K \n(MR of around 1.2 and Hits@1 between 93.6% and 95.7%). \nAmong these, ProjE attains Hits@1 of 95.7%, and KG-\nBERT further improves to 96.0%. This suggests that incor -\nporating textual information can help refine relational rea-\nsoning on FB15K. However, it remains unclear how these \nsame translational baselines would perform on more com-\nplex or diverse benchmarks, as most of them have not been \nevaluated on YAGO3-10‚Äì100.\nCompared to FB15K, YAGO3-10‚Äì100 presents richer \nsemantics and a more varied entity space. Older transla-\ntional methods provide no results on this dataset, making \ncross-method comparisons incomplete. Nonetheless, sev -\neral LLMs and fine-tuned approaches have been evalu-\nated. KGT5, an encoder-decoder model adapted for KGC, \nreports Hits@1 of 60%. General-purpose LLMs (ChatGPT, \nGPT-4, LLaMA-7B) yield Hits@1 scores of 39%, 56%, \nand 13%, respectively. Though they demonstrate some \nability to handle relation inference, their out-of-the-box \nperformance lags behind specialized KGC models. KG-\nLLM, a decoder-based model instruction-tuned for KGC, \nachieves the highest known Hits@1 of 71%. This under -\nscores that tailoring large models to KGC tasks-through \n(36)f ‚à∂ E √ó E ‚Üí R\n(37)r = f(h,t), where h,t ‚àà E and r ‚àà R.\nspecialized prompts, fine-tuning objectives, or both-can \nunlock better relational predictions.\nBecause earlier structural embeddings (e.g., TransE, \nDistMult) have not been tested on YAGO3-10‚Äì100, it is \ndifficult to determine whether their strong performance on \nFB15K would generalize to YAGO3-10‚Äì100‚Äôs more com-\nplex patterns. Similarly, while KGT5 performs reasonably \nwell, the absence of matching baselines (in terms of model \ntype or dataset coverage) prevents a direct head-to-head \ncomparison.\nOverall, on FB15K, incorporating textual context (e.g., \nKG-BERT) can outperform purely structural methods, \nsuggesting that text-informed embeddings bolster rela-\ntion prediction. On YAGO3-10‚Äì100, the top performance \nbelongs to KG-LLM, closely followed by KGT5, whereas \nTable 6  Experimental statistics \nunder the RC subtask\nThe best and second-best results in tables are shown in bold and underlined, respectively\nModel LLM-\nEnhanced\nYear FewRel TACRED\nP R F1 P R F1\nCNN [71] √ó 2015 69.51 69.64 69.35 70.30 54.20 61.20\nPA-LSTM [72] √ó 2017 ‚Äì ‚Äì ‚Äì 65.70 64.50 65.10\nC-GCN [73] √ó 2018 ‚Äì ‚Äì ‚Äì 69.90 63.30 66.40\nBERT_Base [48] √ó 2018 85.05 85.11 84.89 67.23 64.81 66.00\nBERT_Large [48] √ó 2018 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì 70.10\nRoBERTa [49] √ó 2019 ‚Äì ‚Äì ‚Äì 70.40 71.10 70.70\nMTB [74] √ó 2019 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì 71.50\nERNIE [69] √ó 2019 88.49 88.44 88.32 69.97 66.08 67.97\nKnowBert [70] √ó 2019 ‚Äì ‚Äì ‚Äì 71.60 71.40 71.50\nKEPLER [27] ‚úì 2021 ‚Äì ‚Äì ‚Äì 71.50 72.50 72.00\nSemGL √ó 2024 95.11 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nTable 7  Experimental statistics under the RP subtask\nThe best and second-best results in tables are shown in bold and \nunderlined, respectively\nModel LLM-\nEnhanced\nYear FB15K YAGO3-10‚Äì100\nMR Hits@1 Hits@1\nTransE [62] √ó 2013 2.5 84.3 ‚Äì\nTransR [15] √ó 2015 2.1 91.6 ‚Äì\nDKRL [76] √ó 2016 2.0 90.8 ‚Äì\nTKRL [77] √ó 2016 1.7 92.8 ‚Äì\nPTransE [78] √ó 2015 1.2 93.6 ‚Äì\nSSP [79] √ó 2017 1.2 ‚Äì ‚Äì\nProjE [80] √ó 2017 1.2 95.7 ‚Äì\nKG-BERT [18] ‚úì 2019 1.2 96.0 ‚Äì\nKGT5 [33] ‚úì 2022 ‚Äì ‚Äì 60\nChatGPT √ó 2023 ‚Äì ‚Äì 39\nGPT-4 √ó 2023 ‚Äì ‚Äì 56\nLLaMA-7B √ó 2023 ‚Äì ‚Äì 13\nKG-LLM [43] ‚úì 2023 ‚Äì ‚Äì 71\n329Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \ngeneral-purpose LLMs without specialized tuning show \nlower Hits@1. However, these results remain challeng -\ning to fully interpret due to the lack of consistent evalua-\ntions across older translation-based models and different \nLLMs on YAGO3-10‚Äì100. A more uniform experimental \nsetup across both classical and LLM-enhanced methods \non larger, semantically diverse datasets would help clarify \nthe extent to which textual integration, instruction-tuning, \nor architectural choices most significantly drive relation \nprediction improvements.\n4.3.4  Triple Classification\nDefinition 4 Triple Classification (TC) TC is a knowledge \ngraph completion task, aimed at determining the plausibil-\nity of a given knowledge graph triple. Formally, given a set \nof entities E , a set of relations R , and a knowledge graph \nK ‚äÜ E √ó R √ó E consisting of triples (h ,¬†r,¬†t), where h is the \nhead entity, r is the relation, and t  is the tail entity, the task \nis to classify each triple as either true or false. The classifica-\ntion function can be defined as:\nwhere f(h, r,t)= 1 indicates that the triple (h,¬†r,¬†t) is plausi-\nble (true), and f(h, r,t)= 0 indicates it is implausible (false).\nTable¬†8 summarizes the reported accuracy of representa-\ntive models on five widely used datasets of TC task: WN11, \nFB13, UMLS, CoDEx-S, and FB15k-237N.\nOne striking observation is that LLM-enhanced meth-\nods generally achieve higher accuracy. KG-LLM attains the \nhighest reported accuracy on WN11 (95.6%), while KoPA \n(38)f ‚à∂ E √ó R √ó E ‚Üí {0, 1}\nachieves 82.7% on CoDEx-S; both outperform earlier purely \nstructural methods. This boost in performance is closely tied \nto the ability of LLM-based approaches to exploit textual \nsemantics in addition to graph structure, thereby capturing \nsubtle relational nuances. Despite this overall trend favoring \nLLM-based approaches, certain structural models still per -\nform competitively on specific datasets. RotatE, for exam-\nple, reaches 92.1% accuracy on UMLS, suggesting that, for \nsimpler or smaller-scale data, well-tuned classical methods \nmay remain strong contenders.\nNevertheless, the impact of dataset characteristics on \nmodel performance is also evident. WN11 and FB13, \nwhich feature moderate scales and relation complexity, see \nlarge gains for encoder-based methods such as KG-BERT \nand LMKE, each exceeding 90% accuracy. By contrast, \nFB15k-237N and CoDEx-S often include more diverse or \nfine-grained relations, where decoder-based architectures \nlike KG-LLM and KoPA can excel. Notably, KG-LLM \nreaches 80.5% accuracy on FB15k-237N, surpassing older \ntechniques that lack the benefits of large-scale pretrained \nlinguistic knowledge. Still, generic large language models \n(e.g., GPT‚àí3.5, LLaMA) may struggle without knowledge \ngraph alignment: they show markedly lower accuracy (some-\ntimes below 30% on WN11), indicating that additional fine-\ntuning or structural alignment (e.g., prefix adapters, prompt \nengineering) is crucial.\nThe comparisons remain incomplete due to missing \nresults in the literature. Certain classic embedding models \n(e.g., TransE, ComplEx) have not been evaluated on newer \ndatasets such as CoDEx-S or specific variants like FB15k-\n237N, while some LLM-based methods do not report out-\ncomes on FB13 or UMLS. This lack of uniform baselines \nTable 8  Experimental statistics \nunder the TC subtask\nThe best and second-best results in tables are shown in bold and underlined, respectively\nModel LLM-\nEnhanced\nYear WN11 FB13 UMLS CoDEX-S FB15k-237N\nAcc Acc Acc Acc Acc\nNTN [58] √ó 2013 86.2 90.0 ‚Äì ‚Äì ‚Äì\nTransE [62] √ó 2013 75.9 81.5 78.1 72.1 ‚Äì\nDistMult [16] √ó 2014 87.1 86.2 86.8 66.8 ‚Äì\nComplEx [81] √ó 2016 ‚Äì ‚Äì 90.8 67.8 65.7\nRotatE [82] √ó 2019 ‚Äì ‚Äì 92.1 75.7 68.5\nKG-BERT [18] ‚úì 2019 93.5 90.4 89.7 77.3 56.0\nKG-GPT2 [23] ‚úì 2021 85.0 89.0 ‚Äì ‚Äì ‚Äì\nLaSS [26] ‚úì 2021 94.5 91.8 ‚Äì ‚Äì ‚Äì\nKGT5 [33] ‚úì 2022 72.8 66.3 ‚Äì ‚Äì ‚Äì\nLMKE [36] ‚úì 2022 ‚Äì 91.7 92.4 ‚Äì ‚Äì\nGPT‚àí3.5 √ó 2023 ‚Äì ‚Äì 67.6 54.7 60.2\nLLaMA-7B √ó 2023 21.1 9.1 ‚Äì ‚Äì ‚Äì\nLLaMA-13B √ó 2023 28.1 17.6 ‚Äì ‚Äì ‚Äì\nKG-LLM [43] ‚úì 2023 95.6 90.2 86.0 80.3 80.5\nKoPA [44] ‚úì 2023 ‚Äì ‚Äì 92.6 82.7 77.7\n330 X.¬†Wang et al.\nmeans that one cannot universally conclude that LLM-\nenhanced models will always outperform structural methods \nacross all scenarios. Where data does exist, however, trends \nlean strongly in favor of the methods integrating pretrained \nlanguage models, especially for more complex or semanti-\ncally rich datasets.\nWhile incorporating large-scale textual semantics con-\nfers clear benefits on triple classification, practical deploy -\nment of LLM-based methods still demands considerable \ncomputational overhead. Moreover, the promising results \nobserved thus far do not fully generalize to every domain \nor dataset in the absence of more exhaustive benchmark -\ning. Consequently, model choice should be guided by task-\nspecific considerations such as dataset size, relation com -\nplexity, resource constraints, and the availability of textual \ndescriptions. Although LLM-driven approaches show strong \npotential, systematic evaluations under varied conditions are \nessential to confirm these methods‚Äô broader applicability and \nto assess how best to reconcile performance gains with fea-\nsibility in real-world scenarios.\n4.3.5  Link Prediction\nDefinition 5 Link Prediction (LP) LP is a core task in KG \nresearch, aiming to infer missing links between entities. \nFormally, given a knowledge graph G =( E, R, T) where E \nrepresents the set of entities, R represents the set of relations, \nand T ‚äÜ E √ó R √ó E represents the set of triples (links), the \ntask is to predict the plausibility of a new triple (h,¬†r,¬†t) being \ntrue, where h, t‚àà E  and r ‚àà R . The goal is to find a scor -\ning function f ‚à∂ E √ó R √ó E ‚Üí ‚Ñù that assigns a high score to \nplausible triples and a low score to implausible ones. This \ncan be expressed as:\nAs shown in Tables¬† 9 and 10, early translation-based \nand semantic-matching methods can achieve respectable \nperformance on benchmarks such as WN18RR and FB15k-\n237. For example, DistMult reports an MRR of 44.4% and \nHits@10 of 50.4% on WN18RR, while TransE reaches \naround 47.4% in Hits@10 on FB15k-237. However, these \nstructurally guided models struggle with complex rela-\ntional patterns, often reflected in lower Hits@1 (com-\nmonly only 4‚Äì20% on WN18RR). Neural extensions like \nConvE and R-GCN improve feature expressiveness; for \ninstance, ConvE achieves an MRR of 31.2% on FB15k-\n237, outperforming simpler baselines (e.g., TransE with \nMRR 27.9%). Yet, they still face challenges on sparse \ngraphs or large-scale data.\nRecent approaches incorporating entity descriptions \nand prompts via large pre-trained encoders show further \n(39)f(h,r,t)‚âàTrue, for (h,r,t)‚àà G.\ngains. SimKGC achieves an MRR of 66.7% (Hits@10 = \n80.5%) on WN18RR, and CP-KGC reports MRR = 67.3%, \nHits@10 = 80.4%. Both substantially surpass purely struc-\ntural ComplEx, which has MRR around 44.9%. On FB15k-\n237, methods such as OpenWorld KGC and Pretrain-KGE \nreach MRR values of roughly 30‚Äì35%, indicating notice-\nable improvement over older baselines.\nEncoder-decoder and decoder-based generative models \nexhibit mixed success. KGT5 attains an MRR of 34.3% \non FB15k-237. Although this outperforms many classical \nmethods, it can trail behind strong retrieval-style methods \nlike SimKGC. On the other hand, prompt-based genera-\ntive approaches (e.g., CSPromp-KG) leverage textual cues \neffectively, pushing MRR to around 35‚Äì36% on FB15k-\n237 and 38% on Wikidata5M.\nAs listed in Table¬† 11, UMLS is a domain-focused, \nsmaller graph where structural baselines already score \nhigh Hits@10 (98‚Äì99%). LP-BERT, by leveraging tex-\ntual descriptions, further reduces mean rank to 1.18 and \nachieves Hits@10 of 100%. In contrast, Wikidata5M is \nconsiderably more diverse: DistMult here only reaches \nan MRR of 25.3%, while LLM-enhanced SimKGC and \nCSPromp-KG push the MRR to 35.8% and 38.0%, respec-\ntively. These improvements underscore how large language \nmodels can enrich entity representations, particularly \nunder high relational heterogeneity.\nAs shown in Tables¬† 12, 13, and 14, a more stringent \nsetting involves inductive or low-data evaluation. For \nNELL-One, purely structural methods typically yield \nMRRs below 25% in zero-/few-shot modes. By contrast, \ntext-enhanced KG-S2S attains 31% MRR under zero-shot. \nLikewise, SimKGC on the zero-shot split of Wikidata5M \nachieves an MRR of 71.4%, outperforming baseline encod-\ners (e.g., RoBERTa with only 7.4%). Such findings illus-\ntrate the strong inductive capacity of LLM-based tech-\nniques to handle unseen entities/relations, thanks to rich \ntextual pre-training.\nOverall, LLM-enhanced methods often surpass or closely \nmatch purely structural approaches, especially on large or \nmore complex graphs. In zero-shot scenarios, their reliance \non prior linguistic knowledge further drives performance \ngains. Nevertheless, a uniform advantage is not guaran -\nteed. Certain encoder-decoder methods, although adept at \ntext generation, can be less effective at large-scale struc -\ntural inference compared to advanced graph-based retrieval \nstrategies. Moreover, some of the latest decoder-based large \nlanguage models lack comprehensive experimental results \non all benchmarks, hindering fair one-to-one comparisons. \nFuture work may expand empirical coverage, refine prompt \nengineering, and strike a balance between accuracy and fea-\nsible inference costs.\n331Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nTable 9  Experimental statistics \nof WN18RR dataset under \nthe LP subtask in transductive \nsettings\nModel LLM-\nEnhanced\nYear WN18RR\nMR MRR Hits@1 Hits@3 Hits@10\nTransE [62] √ó 2013 2300 24.3 4.3 44.1 53.2\nTransH [14] √ó 2014 2524 ‚Äì ‚Äì ‚Äì 50.3\nDistMult [16] √ó 2014 3704 44.4 41.2 47.0 50.4\nTransR [15] √ó 2015 3166 ‚Äì ‚Äì ‚Äì 50.7\nTransD [13] √ó 2015 2768 ‚Äì ‚Äì ‚Äì 50.7\nComplEx [81] √ó 2016 3921 44.9 40.9 46.9 53.0\nConvE [64] √ó 2018 4464 45.6 41.9 47.0 53.1\nConvKB [83] √ó 2018 2554 24.9 ‚Äì ‚Äì 52.5\nR-GCN [5] √ó 2018 6700 12.3 8.0 13.7 20.7\nKBGAN [84] √ó 2018 ‚Äì 21.5 ‚Äì ‚Äì 48.1\nRotatE [82] √ó 2019 3340 47.6 42.8 49.2 57.1\nKBAT [85] √ó 2019 1921 41.2 ‚Äì ‚Äì 55.4\nCapsE [86] √ó 2019 718 41.5 ‚Äì ‚Äì 55.9\nQuatE [87] √ó 2019 3472 48.1 43.6 50.0 56.4\nTuckER [88] √ó 2019 ‚Äì 47.0 44.3 48.2 52.6\nHAKE [89] √ó 2019 ‚Äì 49,7 45.2 51.6 58.2\nAttH [90] √ó 2020 ‚Äì 48.6 44.3 49.9 57.3\nREFE [91] √ó 2020 ‚Äì ‚Äì ‚Äì ‚Äì 56.1\nGAATs [92] √ó 2020 1270 ‚Äì ‚Äì ‚Äì 60.4\nComplex-DURA [93] √ó 2020 57 ‚Äì ‚Äì ‚Äì ‚Äì\nDensE [94] √ó 2020 3052 49.1 44.3 50.8 57.9\nLineaRE [95] √ó 2020 1644 49.5 45.3 50.9 57.8\nRESCAL-DURA [96] √ó 2020 ‚Äì 49.8 45.5 ‚Äì 57.7\nCompGCN [97] √ó 2020 ‚Äì 47.9 44.3 49.4 54.6\nNePTuNe [98] √ó 2021 ‚Äì ‚Äì ‚Äì ‚Äì 55.7\nComplEx-N3-RP [99] √ó 2021 ‚Äì ‚Äì ‚Äì ‚Äì 58.0\nConE [100] √ó 2021 ‚Äì 49.6 45.3 51.5 57.9\nRot-Pro [101] √ó 2021 ‚Äì 45.7 39.7 48.2 57.7\nQuatDE [102] √ó 2021 1977 48.9 43.8 50.9 58.6\nNBFNet [103] √ó 2021 ‚Äì 55.1 49.7 ‚Äì 66.6\nKG-BERT [18] ‚úì 2019 97 21.6 4.1 30.2 52.4\nMTL-KGC [19] ‚úì 2020 89 33.1 20.3 38.3 59.7\nPretrain-KGE [20] ‚úì 2020 ‚Äì 48.8 43.7 50.9 58.6\nStAR [22] ‚úì 2021 51 40.1 24.3 49.1 70.9\nMEM-KGC [25] ‚úì 2021 ‚Äì 57.2 48.9 62.0 72.3\nLaSS [26] ‚úì 2021 35 ‚Äì ‚Äì ‚Äì 78.6\nSimKGC [29] ‚úì 2021 ‚Äì 66.7 58.8 72.1 80.5\nLP-BERT [31] ‚úì 2022 92 48.2 34.3 56.3 75.2\nKGT5 [33] ‚úì 2022 ‚Äì 54.2 50.7 ‚Äì 60.7\nOpenWorld KGC [34] ‚úì 2022 ‚Äì 55.7 47.5 60.4 70.4\nLMKE [36] ‚úì 2022 79 61.9 52.3 67.1 78.9\nGenKGC [37] ‚úì 2022 ‚Äì ‚Äì 28.7 40.3 53.5\nKG-S2S [38] ‚úì 2022 ‚Äì 57.4 53.1 59.5 66.1\nkNN-KGE [35] ‚úì 2023 ‚Äì 57.9 52.5 ‚Äì ‚Äì\nCSPromp-KG [40] ‚úì 2023 ‚Äì 57.5 52.2 59.6 67.8\nGPT‚àí3.5 √ó 2023 ‚Äì ‚Äì 19.0 ‚Äì ‚Äì\nLLaMA-7B √ó 2023 ‚Äì ‚Äì 8.5 ‚Äì ‚Äì\nLLaMA-13B √ó 2023 ‚Äì ‚Äì 10.0 ‚Äì ‚Äì\nKG-LLM [43] ‚úì 2023 ‚Äì ‚Äì 25.6 ‚Äì ‚Äì\n332 X.¬†Wang et al.\n5  Future Directions\nRecent advances in LLMs mark a significant inflection \npoint in KRL research, presenting exciting opportunities \nfor future exploration of effectively integrating the advan-\ntages of LLMs into KRL. We propose the following six \nrecommendations:\n5.1  Dynamic and¬†Multimodal Knowledge \nRepresentation\nAs knowledge evolves far beyond static triples, future KRL \napproaches must accommodate temporal or event-based rep-\nresentations that can handle the dynamic and evolving char-\nacteristics¬†inherent in real-world data. Modeling sequences \nof events and capturing causal dependencies enables a more \nfine-grained understanding of how entities and relations \nform, merge, or dissolve over time. Such temporal awareness \ncan reveal nuanced patterns that remain invisible under static \nassumptions, thereby enhancing both accuracy and timeli-\nness of knowledge graphs.\nAdopting a temporal perspective can also highlight pre-\nviously overlooked dynamics in evolving entities and their \ninteractions. For instance, an entity might gain or lose attrib-\nutes, or a relationship might change direction based on new \nobservations. These temporal updates not only demand \ncontinuous monitoring of knowledge states but also require \nmechanisms for effectively reconciling new information with \nexisting graph structures.\nBeyond temporal factors, extending KRL to multimodal \ndata such as text, images, and videos further enriches the \nscope of knowledge graphs. However, fusing different data \ntypes in a cohesive representation presents technical chal-\nlenges, including the risk of losing details specific to each \nmodality. Overcoming these obstacles often calls for spe-\ncialized modeling architectures and benchmarking strategies \nthat explicitly measure performance in dynamic, multimodal \ncontexts.\n5.2  Efficiency, Robustness, and¬†Explainability\nAlthough LLMs provide significant improvements for rep-\nresenting knowledge, the accompanying computational \noverhead restricts their straightforward deployment in \nnumerous real-world scenarios. Reducing model size, opti-\nmizing inference time, and carefully allocating resources \nbecome critical steps for practical adoption. Techniques such \nas model pruning and quantization, along with knowledge \ndistillation into more compact architectures, help strike a \nbalance between performance and efficiency.\nRobustness against noise and domain shifts is equally \nimportant in real-world knowledge contexts. Domain adap-\ntation, adversarial training, and probabilistic representations \ncan help LLM-enhanced KRL maintain stable performance \neven when new or contradictory information is intro-\nduced. Greater interpretability complements these efforts \nby increasing user trust. Transparent reasoning pipelines, \nattention visualization, and causal interpretability methods \nall contribute to a clearer understanding of how and why the \nsystem reaches a particular conclusion.\n5.3  Downstream Tasks with¬†KRL+LLM\nIntegrating KRL with LLM can significantly enhance the \nability to solve various problems and downstream tasks. \nFuture research should explore how these models can be \nutilized to improve human decision-making and knowledge \ndiscovery in different domains.\nInteractive systems enable humans to query and interact \nwith LLM-enhanced KRL models that enhance knowledge \nexploration and problem-solving. The natural language \ninterface allows users to ask questions and receive explana-\ntions, thus making complex knowledge graphs more straight-\nforward to understand.\nResearch could also develop collaborative learning frame-\nworks that incorporate continuous human feedback into the \nlearning process of the model. Techniques such as active \nlearning can improve understanding of the model. Promot-\ning a symbiotic relationship between humans and AI can \ncapitalize on the strengths of both parties to achieve superior \nperformance.\n5.4  Pre‚ÄëLLM Models and¬†Data Augmentation\nThe integration of LLMs into KRL goes far beyond \nmerely improving the understanding of textual informa-\ntion. Future research should explore a broader range of \nTable 9  (continued) Model LLM-\nEnhanced\nYear WN18RR\nMR MRR Hits@1 Hits@3 Hits@10\nCD [45] ‚úì 2024 ‚Äì 57.6 52.6 60.7 67.2\nCP-KGC [46] ‚úì 2024 ‚Äì 67.3 59.9 72.1 80.4\nKICGPT [47] ‚úì 2024 ‚Äì 56.4 47.8 61.2 67.7\nThe best and second-best results in tables are shown in bold and underlined, respectively\n333Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nTable 10  Experimental \nstatistics of FB15k-237 dataset \nunder the LP subtask in \ntransductive settings\nThe best and second-best results in tables are shown in bold and underlined, respectively\nModel LLM-\nEnhanced\nYear FB15k-237\nMR MRR Hits@1 Hits@3 Hits@10\nTransE [62] √ó 2013 223 27.9 19.8 37.6 47.4\nTransH [14] √ó 2014 255 ‚Äì ‚Äì ‚Äì 48.6\nDistMult [16] √ó 2014 411 28.1 19.9 30.1 44.6\nTransR [15] √ó 2015 237 ‚Äì ‚Äì ‚Äì 51.1\nTransD [13] √ó 2015 246 ‚Äì ‚Äì ‚Äì 48.4\nComplEx [81] √ó 2016 508 27.8 19.4 29.7 45.0\nConvE [64] √ó 2018 245 31.2 22.5 34.1 49.7\nConvKB [83] √ó 2018 257 24.3 ‚Äì ‚Äì 51.7\nR-GCN [5] √ó 2018 600 16,4 10.0 18.1 41.7\nKBGAN [84] √ó 2018 ‚Äì 27.7 ‚Äì ‚Äì 45.8\nRotatE [82] √ó 2019 177 33.8 24.1 37.5 53.3\nKBAT [85] √ó 2019 270 15.7 ‚Äì ‚Äì 33.1\nCapsE [86] √ó 2019 403 15.0 ‚Äì ‚Äì 35.6\nQuatE [87] √ó 2019 176 31.1 22.1 34.2 49.5\nTuckER [88] √ó 2019 ‚Äì 35.8 26.6 39.4 54.4\nHAKE [89] √ó 2019 ‚Äì 34.6 25.0 38.1 54.2\nAttH [90] √ó 2020 ‚Äì 34.8 25.2 38.4 54.0\nREFE [91] √ó 2020 ‚Äì ‚Äì ‚Äì ‚Äì 54.1\nGAATs [92] √ó 2020 187 ‚Äì ‚Äì ‚Äì 65.0\nComplex-DURA [93] √ó 2020 56 ‚Äì ‚Äì ‚Äì ‚Äì\nDensE [94] √ó 2020 169 34.9 25.6 38.4 53.5\nLineaRE [95] √ó 2020 155 35.7 26.4 39.1 54.5\nRESCAL-DURA [96] √ó 2020 ‚Äì 36.8 27.6 - 55.0\nCompGCN [97] √ó 2020 ‚Äì 35.5 26.4 39.0 53.5\nNePTuNe [98] √ó 2021 ‚Äì ‚Äì ‚Äì ‚Äì 54.7\nComplEx-N3-RP [99] √ó 2021 ‚Äì ‚Äì ‚Äì ‚Äì 56.8\nConE [100] √ó 2021 ‚Äì 34.5 24.7 38.1 54.0\nRot-Pro [101] √ó 2021 ‚Äì 34.4 24.6 38.3 54.0\nQuatDE [102] √ó 2021 90 36.5 26.8 40.0 56.3\nNBFNet [103] √ó 2021 - 41.5 32.1 - 59.9\nKG-BERT [18] ‚úì 2019 153 23.7 16.9 26.0 42.7\nMTL-KGC [19] ‚úì 2020 132 26.7 17.2 29.8 45.8\nPretrain-KGE [20] ‚úì 2020 ‚Äì 35.0 25.0 38.4 55.4\nStAR [22] ‚úì 2021 1117 29.6 20.5 32.2 48.2\nMEM-KGC [25] ‚úì 2021 ‚Äì 34.9 26.0 38.2 52.4\nLaSS [26] ‚úì 2021 108 ‚Äì ‚Äì ‚Äì 53.3\nSimKGC [29] ‚úì 2021 ‚Äì 33.6 24.9 36.2 51.1\nLP-BERT [31] ‚úì 2022 154 31.0 22.3 33.6 49.0\nKGT5 [33] ‚úì 2022 ‚Äì 34.3 25.2 ‚Äì 37.7\nOpenWorld KGC [34] ‚úì 2022 ‚Äì 34.6 25.3 38.1 53.1\nLMKE [36] ‚úì 2022 141 30.6 21.8 33.1 48.4\nGenKGC [37] ‚úì 2022 ‚Äì ‚Äì 19.2 35.5 43.9\nKG-S2S [38] ‚úì 2022 ‚Äì 33.6 25.7 37.3 49.8\nkNN-KGE [35] ‚úì 2023 ‚Äì 28.0 37.3 ‚Äì ‚Äì\nCSPromp-KG [40] ‚úì 2023 ‚Äì 35.8 26.9 39.3 53.8\nGPT‚àí3.5 √ó 2023 ‚Äì ‚Äì 23.7 ‚Äì ‚Äì\nCP-KGC [46] ‚úì 2024 ‚Äì 33.8 25.1 36.5 51.6\nKICGPT [47] ‚úì 2024 ‚Äì 41.2 32.7 44.8 55.4\n334 X.¬†Wang et al.\ncollaborative enhancements that can be realized by leverag-\ning the strengths of LLMs in conjunction with traditional \napproaches. One promising way is the development of mixed \nframeworks that allow pre-LLM models, which are often \nmore lightweight and efficient, to work collaboratively with \nLLMs. These models could handle more straightforward or \nroutine tasks, while LLMs focus on more complex reasoning \nand representation tasks. This division of labour can signifi-\ncantly enhance both the performance and efficiency of KRL, \nensuring that the strengths of various types of models are \nused in the most appropriate contexts.\nMoreover, data augmentation offers another layer of \nimprovement by expanding the diversity and richness of \ntraining data. This can involve generating synthetic data \nTable 11  Experimental \nstatistics of UMLS and \nWikidata5M datasets under \nthe LP subtask in transductive \nsetting\nThe best and second-best results in tables are shown in bold and underlined, respectively\nModel LLM-\nEnhanced\nYear UMLS Wikidata5M\nMR Hits@10 MRR Hits@1 Hits@3 Hits@10\nTransE [62] √ó 2013 1.84 98.9 25.3 17.0 31.1 39.2\nTransH [14] √ó 2014 1.80 99.5 ‚Äì ‚Äì ‚Äì ‚Äì\nDistMult [16] √ó 2014 5.52 84.6 25.3 20.9 27.8 33.4\nTransR [15] √ó 2015 1.81 99.4 ‚Äì ‚Äì ‚Äì ‚Äì\nTransD [13] √ó 2015 1.71 99.3 ‚Äì ‚Äì ‚Äì ‚Äì\nComplEx [81] √ó 2016 2.59 96.7 30.8 25.5 ‚Äì 39.8\nDKRL [76] √ó 2016 ‚Äì ‚Äì 16.0 12.0 18.1 22.9\nConvE [64] √ó 2018 1.51 99.0 ‚Äì ‚Äì ‚Äì ‚Äì\nRoBERTa [49] √ó 2019 ‚Äì ‚Äì 0.1 0 0.1 0.3\nRotatE [82] √ó 2019 ‚Äì ‚Äì 29.0 23.4 32.2 39.0\nQuatE [87] √ó 2019 ‚Äì ‚Äì 27.6 22.7 30.1 35.9\nComplEx-N3-RP [99] √ó 2021 ‚Äì 99.8 ‚Äì ‚Äì ‚Äì ‚Äì\nKG-BERT [18] ‚úì 2019 1.47 99.0 ‚Äì ‚Äì ‚Äì ‚Äì\nStAR [22] ‚úì 2021 1.49 99.1 ‚Äì ‚Äì ‚Äì ‚Äì\nLaSS [26] ‚úì 2021 1.56 98.9 ‚Äì ‚Äì ‚Äì ‚Äì\nKEPLER [27] ‚úì 2021 ‚Äì ‚Äì 21.0 17.3 22.4 27.7\nSimKGC [29] ‚úì 2021 ‚Äì ‚Äì 35.8 31.3 37.6 44.1\nLP-BERT [31] ‚úì 2022 1.18 100 ‚Äì ‚Äì ‚Äì ‚Äì\nKGT5 [33] ‚úì 2022 ‚Äì ‚Äì 33.6 28.6 36.2 42.6\nCSPromp-KG [40] ‚úì 2023 ‚Äì ‚Äì 38.0 34.3 39.9 44.6\nReSKGC [41] ‚úì 2023 ‚Äì ‚Äì 39.6 37.3 41.3 43.7\nTable 12  Experimental \nstatistics of FB15k-237N \ndataset under the LP subtask in \ntransductive setting\nThe best and second-best results in tables are shown in bold and underlined, respectively\nModel LLM-\nEnhanced\nYear FB15k-237N\nMRR Hits@1 Hits@3 Hits@10\nTransE [62] √ó 2013 25.5 15.2 30.1 45.9\nDistMult [16] √ó 2014 20.9 14.3 23.4 33.0\nComplEx [81] √ó 2016 24.9 18.0 27.6 38.0\nConvE [64] √ó 2018 27.3 19.2 30.5 42.9\nRotatE [82] √ó 2019 27.9 17.7 32.0 48.1\nCompGCN [97] √ó 2020 31.6 23.1 34.9 48.0\nKG-BERT [18] ‚úì 2019 20.3 13.9 20.1 40.3\nMTL-KGC [19] ‚úì 2020 24.1 16.0 28.4 43.0\nGenKGC [37] ‚úì 2022 ‚Äì 18.7 27.3 33.7\nKG-S2S [38] ‚úì 2022 35.4 28.5 38.8 49.3\nCSPromp-KG [40] ‚úì 2023 36.0 28.1 39.5 51.1\nCD [45] ‚úì 2024 37.2 28.8 41.0 53.0\n335Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \nfrom existing datasets, creating variations of textual, vis-\nual, or multimodal inputs. Such augmented data can help \npre-LLM models and LLM-enhanced models alike, ena-\nbling better generalization and robustness across different \ndomains.\n5.5  Graph‚ÄëCentric Instruction Tuning\nAnother promising angle is to adapt advanced LLMs to \ngraph-centric contexts via specialized prompts and instruc-\ntion-tuning procedures. By designing domain-relevant tem-\nplates that encapsulate the structure and semantics of knowl-\nedge graphs, LLMs can be primed to deliver consistent and \nprecise outputs aligned with triple-oriented representations. \nThis tailored prompting strategy may also help constrain \nthe inherently open-ended nature of LLM responses, which \nis especially important for tasks that require well-defined, \nstructured outputs.\nInstruction fine-tuning can further bridge the gap between \ntextual comprehension and formal graph outputs. Extending \nsequence-to-sequence or decoder-based LLMs to generate \nor update triples directly, rather than merely producing a \ntextual summary, allows for a smoother integration of large-\nscale pre-trained knowledge with KRL tasks. This strategy \nholds promise for applications ranging from data validation \nto large-scale knowledge graph construction.\n5.6  Incremental and¬†Continual Learning\nFinally, incremental and continual learning mechanisms \naddress the reality that knowledge is never static. To remain \ncurrent without retraining from scratch, LLM-enhanced \nKRL models must absorb new facts and relationships as they \nemerge, minimizing downtime and computational overhead. \nEmploying an on-the-fly update architecture, combined \nwith streaming data paradigms, allows knowledge graphs \nto reflect recent changes while still retaining core learned \npatterns.\nPreserving previously acquired knowledge becomes cru-\ncial in settings where frequent updates can otherwise desta-\nbilize the learned model. Regularization-based strategies and \nreplay mechanisms help mitigate catastrophic forgetting by \nensuring that newer information does not overwrite earlier \ninsights. Such solutions become especially significant in \ndomains where continuity and institutional memory are key \nto sustaining accurate, context-rich representations of the \nunderlying data.\n6  Conclusion\nEnhancing Knowledge Representation Learning (KRL) with \nLarge Language Models (LLMs) is an emerging research \narea that has garnered significant attention in both academia \nand industry. This article presents a comprehensive overview \nof recent advancements in this field. It begins by introduc-\ning the foundational work on KRL prior to the enhance-\nment by LLMs, highlighting traditional methods and their \nlimitations. Next, existing LLM-enhanced KRL methods are \ncategorized, focusing on encoder-based, encoder-decoder-\nbased, and decoder-based techniques, and a taxonomy is pro-\nvided based on the variety of downstream tasks they address. \nTable 13  Experimental \nstatistics under the LP subtask \nin inductive setting on NELL-\nOne dataset\nModel LLM-\nEnhanced\nYear NELL-One\nN-Shot MRR Hits@1 Hits@5 Hits@10\nGMatching [65] √ó 2018 Five-Shot 20 14 26 31\nGMatching [65] √ó 2018 One-Shot 19 12 26 31\nMetaR [104] √ó 2019 Five-Shot 26 17 35 44\nMetaR [104] √ó 2019 One-Shot 25 17 34 40\nStAR [22] ‚úì 2021 Zero-Shot 26 17 35 45\nKG-S2S [38] ‚úì 2022 Zero-Shot 31 22 41 48\nTable 14  Experimental \nstatistics under the LP subtask \nin inductive setting on \nWikidata5M dataset\nThe best and second-best results in tables are shown in bold and underlined, respectively\nModel LLM-\nEnhanced\nYear Wikidata5M\nN-Shot MRR MR Hits@1 Hits@3 Hits@10\nDKRL [76] √ó 2016 Zero-Shot 23.1 78 5.9 32.0 54.6\nRoBERTa [49] √ó 2019 Zero-Shot 7.4 723 0.7 1.0 19.6\nKEPLER [27] ‚úì 2021 Zero-Shot 40.2 28 22.2 51.4 73.0\nSimKGC [29] ‚úì 2021 Zero-Shot 71.4 ‚Äì 50.9 78.5 91.7\n336 X.¬†Wang et al.\nFinally, the paper discusses future research directions in this \nrapidly evolving domain.\nAuthor Contributions All the authors listed in this manuscript have \nmade substantial contributions to this article and signed appropriately.\nFunding The National Natural Science Foundation of China \n(62472311, U23B2057, 62176185).\n Data Availability Available.\nDeclarations \nConflict of interest The authors have no conficts of interest to declare \nthat are relevant to the content of this article.\nEthics Approval Not applicable.\nConsent to Participate All authors agreed to participate.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article‚Äôs Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article‚Äôs Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Kumar A, Singh SS, Singh K, Biswas B (2020) Link prediction \ntechniques, applications, and performance: a survey. Phys A: Stat \nMech Appl 553:124289\n 2. Jaradeh MY, Singh K, Stocker M, Auer S ( 2021) Triple \nclassification for scholarly knowledge graph completion. In: \nProceedings of the 11th knowledge capture conference, pp \n225‚Äì 232\n 3. Zeng D, Liu K, Lai S, Zhou G, Zhao J ( 2014) Relation clas-\nsification via convolutional deep neural network. In: Proceed-\nings of COLING 2014, the 25th international conference on \ncomputational linguistics: technical papers, pp 2335‚Äì 2344\n 4. Nickel M, Tresp V, Kriegel H-P, et¬†al ( 2011) A three-way \nmodel for collective learning on multi-relational data. In: Icml, \n11, pp 3104482‚Äì 3104584\n 5. Schlichtkrull M, Kipf TN, Bloem P, Van Den¬†Berg R, Titov I, \nWelling M (2018) Modeling relational data with graph convo -\nlutional networks. In: The Semantic Web: 15th international \nconference, ESWC 2018, Heraklion, Crete, Greece, Proceed-\nings 15, pp 593‚Äì 607 . Springer\n 6. Yu P, Ji H ( 2023) Shorten the long tail for rare entity and event \nextraction. In: Proceedings of the 17th conference of the euro-\npean chapter of the association for computational linguistics, \npp 1339‚Äì 1350\n 7. Vaswani, A (2017) Attention is all you need. Adv Neural \nInform Process Syst\n 8. Ge X, Wang YC, Wang B, Kuo C-CJ, et¬†al (2024) Knowledge \ngraph embedding: an overview. APSIPA Trans Signal Inform \nProcess 13(1)\n 9. Biswas R, Kaffee L-A, Cochez M, Dumbrava S, Jendal TE, \nLissandrini M, Lopez V, Menc√≠a EL, Paulheim H, Sack H et¬†al \n(2023) Knowledge graph embeddings: open challenges and \nopportunities. Trans Graph Data Knowl 1(1):4\n 10. Cao J, Fang J, Meng Z, Liang S (2024) Knowledge graph \nembedding: a survey from the perspective of representation \nspaces. ACM Comput Surv 56(6):1‚Äì42\n 11. Pan S, Luo L, Wang Y, Chen C, Wang J, Wu X (2024) Unify -\ning large language models and knowledge graphs: a roadmap. \nIEEE Trans Knowl Data Eng 36(7):3580‚Äì99\n 12. Bordes A, Usunier N, Garcia-Duran A, Weston J, Yakhnenko \nO (2013) Translating embeddings for modeling multi-relational \ndata. Adv Neural Inform Process Syst 26\n 13. Ji G, He S, Xu L, Liu K, Zhao J ( 2015) Knowledge graph \nembedding via dynamic mapping matrix. In: Proceedings of \nthe 53rd annual meeting of the association for computational \nlinguistics and the 7th international joint conference on natural \nlanguage processing (volume 1: Long Papers), pp 687‚Äì 696\n 14. Wang Z, Zhang J , Feng J, Chen Z (2014) Knowledge graph \nembedding by translating on hyperplanes. In: Proceedings of \nthe AAAI conference on artificial intelligence, vol 28\n 15. Lin Y, Liu Z, Sun M, Liu Y, Zhu X ( 2015) Learning entity \nand relation embeddings for knowledge graph completion. In: \nProceedings of the AAAI conference on artificial intelligence, \n29\n 16. Yang B, Yih W-t, He X, Gao J, Deng L (2014) Embedding enti-\nties and relations for learning and inference in knowledge bases. \narXiv preprint arXiv: 1412. 6575\n 17. Shang C, Tang Y, Huang J, Bi J, He X, Zhou B (2019) End-to-\nend structure-aware convolutional networks for knowledge base \ncompletion. In: Proceedings of the AAAI conference on artificial \nintelligence, 33: 3060‚Äì 3067\n 18. Yao L, Mao C, Luo Y (2019) KG-BERT: BERT for knowledge \ngraph completion. arXiv:  org/ abs/ 1909. 03193\n 19. Kim B, Hong T, Ko Y, Seo J (2020) Multi-task learning for \nknowledge graph completion with pre-trained language models. \nIn: Scott, D., Bel, N., Zong, C. (eds) Proceedings of the 28th \ninternational conference on computational linguistics, pp 1737‚Äì \n1743. International committee on computational linguistics, \nBarcelona, Spain (Online) . https:// doi. org/ 10. 18653/ v1/ 2020. \ncoling- main. 153 . https:// aclan tholo gy. org/ 2020. coling- main. 153\n 20. Zhang Z, Liu X, Zhang Y, Su Q, Sun X, He B (2020) Pre-\ntrain-kge: learning knowledge representation from pre-\ntrained language models. Find Assoc Comput Ling: EMNLP \n2020:259‚Äì266\n 21. Liu W, Zhou P, Zhao Z, Wang Z, Ju Q, Deng H, Wang P (2020) \nK-bert: enabling language representation with knowledge \ngraph. Proceed AAAI Conf Art Intell 34:2901‚Äì2908\n 22. Wang B, Shen T, Long G, Zhou T, Wang Y, Chang Y (2021) \nStructure-augmented text representation learning for effi-\ncient knowledge graph completion. Proceed Web Conf \n2021:1737‚Äì1748\n 23. Biswas R, Sofronova R, Alam M, Sack H (2021) Contex-\ntual language models for knowledge graph completion. In: \nMLSMKG@ PKDD/ECML, p 13\n 24. Lovelace J, Newman-Griffis D, Vashishth S, Lehman JF, Ros√© \nCP (2021) Robust knowledge graph completion with stacked \nconvolutions and a student re-ranking network. In: Proceedings \nof the conference. Association for computational linguistics. \nMeeting, vol 2021, p 1016 . NIH Public Access\n 25. Choi B, Jang D, Ko Y (2021) Mem-kgc: masked entity model \nfor knowledge graph completion with pre-trained language \nmodel. IEEE Access 9:132025‚Äì132032\n 26. Shen J, Wang C, Gong L, Song D (2022) Joint language seman-\ntic and structure embedding for knowledge graph completion. \narXiv preprint arXiv: 2209. 08721\n337Large Language Model Enhanced Knowledge Representation Learning: A¬†Survey  \n 27. Wang X, Gao T, Zhu Z, Zhang Z, Liu Z, Li J, Tang J (2021) \nKepler: a unified model for knowledge embedding and pre-\ntrained language representation. Trans Assoc Comput Ling \n9:176‚Äì194\n 28. Daza D, Cochez M, Groth P (2021) Inductive entity repre-\nsentations from text via link prediction. Proceed Web Conf \n2021:798‚Äì808\n 29. Wang L, Zhao W, Wei Z, Liu J (2022) Simkgc: Simple contras-\ntive knowledge graph completion with pre-trained language \nmodels. arXiv preprint arXiv: 2203. 02167\n 30. Clouatre L, Trempe P, Zouaq A, Chandar S (2020) Mlmlm: \nlink prediction with mean likelihood masked language model. \narXiv preprint arXiv: 2009. 07058\n 31. Li D, Zhu B, Yang S, Xu K, Yi M, He Y, Wang H (2023) \nMulti-task pre-training language model for semantic network \ncompletion. ACM Trans Asian Low-Resour Lang Inform Pro-\ncess 22(11):1‚Äì20\n 32. Lv X, Lin Y, Cao Y, Hou L, Li J, Liu Z, Li P, Zhou J (2022) \nDo pre-trained models benefit knowledge graph completion? \nA reliable evaluation and a reasonable approach. Association \nfor computational linguistics\n 33. Saxena A, Kochsiek A, Gemulla R (2022) Sequence-to-\nsequence knowledge graph completion and question answering. \narXiv preprint arXiv: 2203. 10321\n 34. Choi B, Ko Y (2023) Knowledge graph extension with a pre-\ntrained language model via unified learning method. Knowl-\nBased Syst 262:110245\n 35. Wang P, Xie X, Wang X, Zhang N (2023) Reasoning through \nmemorization: nearest neighbor knowledge graph embeddings. \nIn: ccf international conference on natural language processing \nand Chinese computing, Springer, pp 111‚Äì 122\n 36. Wang X, He Q, Liang J, Xiao Y (2022) Language models as \nknowledge embeddings. arXiv preprint arXiv: 2206. 12617\n 37. Xie X, Zhang N, Li Z, Deng S, Chen H, Xiong F, Chen M, \nChen H (2022) From discrimination to generation: Knowledge \ngraph completion with generative transformer. In: companion \nproceedings of the web conference 2022, pp 162‚Äì 165\n 38. Chen C, Wang Y, Li B, Lam K-Y (2022) Knowledge is flat: a \nseq2seq generative framework for various knowledge graph \ncompletion. arXiv preprint arXiv: 2209. 07299\n 39. Xie X, Li Z, Wang X, Xi Z, Zhang N (2022) Lambdakg: a library \nfor pre-trained language model-based knowledge graph embed-\ndings. arXiv preprint arXiv: 2210. 00305\n 40. Chen C, Wang Y, Sun A, Li B, Lam K-Y (2023) Dipping plms \nsauce: Bridging structure and text for effective knowledge graph \ncompletion via conditional soft prompting. arXiv preprint arXiv: \n2307. 01709\n 41. Yu D, Yang Y (2023) Retrieval-enhanced generative model for \nlarge-scale knowledge graph completion. In: Proceedings of the \n46th international acm sigir conference on research and develop-\nment in information retrieval, pp 2334‚Äì 2338\n 42. Jiang J, Zhou K, Zhao WX, Li Y, Wen J-R (2023) Reasoninglm: \nenabling structural subgraph reasoning in pre-trained language \nmodels for question answering over knowledge graph. arXiv pre-\nprint arXiv: 2401. 00158\n 43. Yao L, Peng J, Mao C, Luo Y (2023) Exploring large language \nmodels for knowledge graph completion. arXiv preprint arXiv:  \n2308. 13916\n 44. Zhang Y, Chen Z, Zhang W, Chen H (2023) Making large lan-\nguage models perform better in knowledge graph completion. \narXiv preprint arXiv: 2310. 06671\n 45. Li D, Tan Z, Chen T, Liu H (2024) Contextualization distilla -\ntion from large language model for knowledge graph completion. \narXiv preprint arXiv: 2402. 01729\n 46. Yang R, Zhu J, Man J, Fang L, Zhou Y (2024) Enhancing text-\nbased knowledge graph completion with zero-shot large language \nmodels: a focus on semantic enhancement arXiv:  org/ abs/ 2310. \n08279\n 47. Wei Y, Huang Q, Kwok JT, Zhang Y (2024) Kicgpt: large lan-\nguage model with knowledge in context for knowledge graph \ncompletion. arXiv preprint arXiv: 2402. 02389\n 48. Devlin J, Chang M-W, Lee K, Toutanova K (2019) BERT: pre-\ntraining of deep bidirectional transformers for language under -\nstanding. In: Burstein J, Doran C, Solorio T (eds) Proceedings \nof the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Vol 1 (Long and Short Papers), pp. 4171‚Äì 4186. \nAssociation for Computational Linguistics, Minneapolis, Minne-\nsota . https:// doi. org/ 10. 18653/ v1/ N19- 1423 . https:// aclan tholo \ngy. org/ N19- 1423\n 49. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis \nM, Zettlemoyer L, Stoyanov V (2019) RoBERTa: a robustly opti-\nmized BERT pretraining approach . arXiv:  org/ abs/ 1907. 11692\n 50. Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, \nLevy O, Stoyanov V, Zettlemoyer L (2020) BART: Denoising \nsequence-to-sequence pre-training for natural language gen-\neration, translation, and comprehension. In: Jurafsky D, Chai \nJ, Schluter N, Tetreault J (eds) Proceedings of the 58th annual \nmeeting of the association for computational linguistics, pp \n7871‚Äì 7880. Association for computational linguistics, Online ( \n2020). https:// doi. org/ 10. 18653/ v1/ 2020. acl- main. 703 . https:// \naclan tholo gy. org/ 2020. acl- main. 703\n 51. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, \nZhou Y, Li W, Liu PJ (2023) Exploring the limits of transfer \nlearning with a unified text-to-text transformer. arXiv:  org/ abs/ \n1910. 10683\n 52. Touvron H, Martin L, Kevin¬†Stone ea (2023) Llama 2: Open \nfoundation and fine-tuned chat models . arXiv:  org/ abs/ 2307.  \n09288\n 53. OpenAI Achiam J, Steven¬†Adler ea (2024) GPT-4 technical \nreport https:// arxiv. org/ abs/ 2303. 08774\n 54. Ling X, Singh S, Weld DS (2015) Design challenges for entity \nlinking. Trans Assoc Comput Ling 3:315‚Äì328\n 55. Choi E, Levy O, Choi Y, Zettlemoyer L (2018) Ultra-fine entity \ntyping. arXiv preprint arXiv: 1807. 04905\n 56. Zhang Y, Zhong V, Chen D, Angeli G, Manning CD (2017) Posi-\ntion-aware attention and supervised data improve slot filling. In: \nConference on empirical methods in natural language processing\n 57. Han X, Zhu H, Yu P, Wang Z, Yao Y, Liu Z, Sun M (2018) \nFewrel: a large-scale supervised few-shot relation classification \ndataset with state-of-the-art evaluation. arXiv preprint arXiv:  \n1810. 10147\n 58. Socher R, Chen D, Manning CD, Ng A (2013) Reasoning with \nneural tensor networks for knowledge base completion. Adv Neu-\nral Inform Process Syst26\n 59. Alam MM, Rony MRAH, Nayyeri M, Mohiuddin K, Akter MM, \nVahdati S, Lehmann J (2022) Language model guided knowledge \ngraph embeddings. IEEE Access 10:76008‚Äì76020\n 60. Lv X, Lin Y, Cao Y, Hou L, Li J, Liu Z, Li P, Zhou J (2022) \nDo pre-trained models benefit knowledge graph completion? A \nreliable evaluation and a reasonable approach. Association for \ncomputational linguistics\n 61. Kok S, Domingos P ( 2007) Statistical predicate invention. In: \nProceedings of the 24th international conference on machine \nlearning, pp 433‚Äì 440\n 62. Bordes A, Usunier N, Garcia-Duran A, Weston J, Yakhnenko \nO (2013) Translating embeddings for modeling multi-relational \ndata. Adv Neural Inform Process Syst 26\n 63. Toutanova K, Chen D, Pantel P, Poon H, Choudhury P, \nGamon M (2015) Representing text for joint embedding of \n338 X.¬†Wang et al.\ntext and knowledge bases. In: Proceedings of the 2015 confer -\nence on empirical methods in natural language processing, pp \n1499‚Äì 1509\n 64. Dettmers T, Minervini P, Stenetorp P, Riedel S (2018) Convolu-\ntional 2d knowledge graph embeddings. In: Proceedings of the \nAAAI conference on artificial intelligence, vol 32\n 65. Xiong W, Yu M, Chang S, Guo X, Wang WY (2018) One-shot \nrelational learning for knowledge graphs. arXiv preprint arXiv:  \n1808. 09040\n 66. Safavi T, Koutra D (2020) Codex: a comprehensive knowledge \ngraph completion benchmark. arXiv preprint arXiv: 2009. 07810\n 67. Zhu C, Yang Z, Xia X, Li N, Zhong F, Liu L (2022) Multimodal \nreasoning based on knowledge graph embedding for specific dis-\neases. Bioinformatics 38(8):2235‚Äì2245\n 68. Shimaoka S, Stenetorp P, Inui K, Riedel S (2016) An attentive \nneural architecture for fine-grained entity type classification. \narXiv preprint arXiv: 1604. 05525\n 69. Zhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q (2019) Ernie: \nenhanced language representation with informative entities. \narXiv preprint arXiv: 1905. 07129\n 70. Peters ME, Neumann M, Logan¬†IV RL, Schwartz R, Joshi V, \nSingh S, Smith NA (2019) Knowledge enhanced contextual word \nrepresentations. arXiv preprint arXiv: 1909. 04164\n 71. Zeng D, Liu K, Chen Y, Zhao J (2015) Distant supervision for \nrelation extraction via piecewise convolutional neural networks. \nIn: Proceedings of the 2015 conference on empirical methods in \nnatural language processing, pp 1753‚Äì 1762\n 72. Zhang Y, Zhong V, Chen D, Angeli G, Manning CD (2017) Posi-\ntion-aware attention and supervised data improve slot filling. In: \nConference on empirical methods in natural language processing\n 73. Zhang Y, Qi P, Manning CD (2018) Graph convolution over \npruned dependency trees improves relation extraction. arXiv \npreprint arXiv: 1809. 10185\n 74. Soares LB, FitzGerald N, Ling J, Kwiatkowski T (2019) Match-\ning the blanks: distributional similarity for relation learning. \narXiv preprint arXiv: 1906. 03158\n 75. Wu H, He Y, Chen Y, Bai Y, Shi X (2024) Improving few-shot \nrelation extraction through semantics-guided learning. Neural \nNetw 169:453‚Äì461. https:// doi. org/ 10. 1016/j. neunet. 2023. 10. 053\n 76. Xie R, Liu Z, Jia J, Luan H, Sun M (2016) Representation learn-\ning of knowledge graphs with entity descriptions. In: Proceedings \nof the AAAI conference on artificial intelligence, vol 30\n 77. Xie R, Liu Z, Sun M et¬† al (2016) Representation learn-\ning of knowledge graphs with hierarchical types. IJCAI \n2016:2965‚Äì2971\n 78. Lin Y, Liu Z, Luan H, Sun M, Rao S, Liu S (2015) Modeling \nrelation paths for representation learning of knowledge bases. \narXiv preprint arXiv: 1506. 00379\n 79. Xiao H, Huang M, Meng L, Zhu X (2017) Ssp: semantic space \nprojection for knowledge graph embedding with text descrip-\ntions. In: Proceedings of the AAAI conference on artificial \nintelligence, vol 31\n 80. Shi B, Weninger T (2017) Proje: Embedding projection for \nknowledge graph completion. In: Proceedings of the AAAI \nconference on artificial intelligence, vol 31\n 81. Trouillon T, Welbl J, Riedel S, Gaussier √â, Bouchard G ( 2016) \nComplex embeddings for simple link prediction. In: Interna -\ntional conference on machine learning, pp 2071‚Äì 2080 . PMLR\n 82. Sun Z, Deng Z-H, Nie J-Y, Tang J (2019) Rotate: knowledge \ngraph embedding by relational rotation in complex space. \narXiv preprint arXiv: 1902. 10197\n 83. Nguyen DQ, Nguyen DQ, Nguyen TD, Phung D (2019) A \nconvolutional neural network-based model for knowledge \nbase completion and its application to search personalization. \nSemantic Web 10(5):947‚Äì960\n 84. Cai L, Wang WY (2017) Kbgan: Adversarial learning for \nknowledge graph embeddings. arXiv preprint arXiv: 1711.  \n04071\n 85. Nathani D, Chauhan J, Sharma C, Kaul M (2019) Learning \nattention-based embeddings for relation prediction in knowl-\nedge graphs. arXiv preprint arXiv: 1906. 01195\n 86. Nguyen DQ, Vu T, Nguyen TD, Nguyen DQ, Phung D (2018) \nA capsule network-based embedding model for knowledge \ngraph completion and search personalization. arXiv preprint \narXiv: 1808. 04122\n 87. Zhang S, Tay Y, Yao L, Liu Q (2019) Quaternion knowledge \ngraph embeddings. Adv Neural Inform Process Syst32\n 88. Bala≈æeviƒá I, Allen C, Hospedales TM (2019) Tucker: tensor \nfactorization for knowledge graph completion. arXiv preprint \narXiv: 1901. 09590\n 89. Zhang Z, Cai J, Zhang Y, Wang J (2020) Learning hierarchy-\naware knowledge graph embeddings for link prediction. Pro-\nceed AAAI Conf Artif Intell 34:3065‚Äì3072\n 90. Chami I, Wolf A, Juan D-C, Sala F, Ravi S, R√© C (2020) Low-\ndimensional hyperbolic knowledge graph embeddings. arXiv \npreprint arXiv: 2005. 00545\n 91. Chami I, Wolf A, Juan D-C, Sala F, Ravi S, R√© C (2020) Low-\ndimensional hyperbolic knowledge graph embeddings. arXiv \npreprint arXiv: 2005. 00545\n 92. Wang R, Li B, Hu S, Du W, Zhang M (2019) Knowledge graph \nembedding via graph attenuated attention networks. IEEE \naccess 8:5212‚Äì5224\n 93. Zhang Z, Cai J, Wang J (2020) Duality-induced regularizer for \ntensor factorization based knowledge graph completion. Adv \nNeural Inform Process Syst 33:21604‚Äì21615\n 94. Lu H, Hu H (2020) Dense: an enhanced non-abelian group \nrepresentation for knowledge graph embedding. arXiv preprint \narXiv: 2008. 04548\n 95. Peng Y, Zhang J ( 2020) Lineare: simple but powerful knowl-\nedge graph embedding for link prediction. In: 2020 IEEE inter -\nnational conference on data mining (ICDM), pp 422‚Äì 431 . \nIEEE\n 96. Zhang Z, Cai J, Wang J (2020) Duality-induced regularizer for \ntensor factorization based knowledge graph completion. Adv \nNeural Inform Process Syst 33:21604‚Äì21615\n 97. Vashishth S, Sanyal S, Nitin V, Talukdar P (2019) Composition-\nbased multi-relational graph convolutional networks. arXiv pre-\nprint arXiv: 1911. 03082\n 98. Sonkar S, Katiyar A, Baraniuk RG (2021) Neptune: neural pow-\nered tucker network for knowledge graph completion. arXiv pre-\nprint arXiv: 2104. 07824\n 99. Chen Y, Minervini P, Riedel S, Stenetorp P (2021) Relation pre-\ndiction as an auxiliary training objective for improving multi-\nrelational graph representations. arXiv preprint arXiv: 2110.  \n02834\n 100. Bai Y, Ying Z, Ren H, Leskovec J (2021) Modeling heteroge -\nneous hierarchies with relation-specific hyperbolic cones. Adv \nNeural Inform Process Syst 34:12316‚Äì12327\n 101. Song T, Luo J, Huang L (2021) Rot-pro: modeling transitivity by \nprojection in knowledge graph embedding. Adv Neural Inform \nProcess Syst 34:24695‚Äì24706\n 102. Gao H, Yang K, Yang Y, Zakari RY, Owusu JW, Qin K (2021) \nQuatde: dynamic quaternion embedding for knowledge graph \ncompletion. arXiv preprint arXiv: 2105. 09002\n 103. Zhu Z, Zhang Z, Xhonneux L-P, Tang J (2021) Neural bellman-\nford networks: a general graph neural network framework for link \nprediction. Adv Neural Inform Process Syst 34:29476‚Äì29490\n 104. Chen M, Zhang W, Zhang W, Chen Q, Chen H (2019) Meta rela-\ntional learning for few-shot link prediction in knowledge graphs. \narXiv preprint arXiv: 1909. 01515",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8332069516181946
    },
    {
      "name": "Representation (politics)",
      "score": 0.635892391204834
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3926977217197418
    },
    {
      "name": "Data science",
      "score": 0.3831619918346405
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3783382177352905
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": []
}