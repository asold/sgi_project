{
  "title": "Superbloom: Bloom filter meets Transformer",
  "url": "https://openalex.org/W3006096721",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2110682187",
      "name": "Anderson John",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084984359",
      "name": "Huang, Qingqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226553925",
      "name": "Krichene, Walid",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226553924",
      "name": "Rendle, Steffen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051190893",
      "name": "Zhang Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2948131150",
    "https://openalex.org/W2962943936",
    "https://openalex.org/W2165621523",
    "https://openalex.org/W2123845384",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2962790997",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2599546125",
    "https://openalex.org/W3015356165",
    "https://openalex.org/W2946345909",
    "https://openalex.org/W2937556626",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1595781024",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963036352",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W2963639656",
    "https://openalex.org/W2626376796",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2886661492",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2070996757",
    "https://openalex.org/W1676820704",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.",
  "full_text": "Superbloom: Bloom ﬁlter meets Transformer\nJohn Anderson Qingqing Huang Walid Krichene\nSteffen Rendle Li Zhang\nGoogle Research\nFebruary 11, 2020\nAbstract\nWe extend the idea of word pieces in natural language models to machine learning tasks\non opaque ids. This is achieved by applying hash functions to map each id to multiple hash\ntokens in a much smaller space, similarly to a Bloom ﬁlter. We show that by applying a multi-\nlayer Transformer to these Bloom ﬁlter digests, we are able to obtain models with high accuracy.\nThey outperform models of a similar size without hashing and, to a large degree, models of a\nmuch larger size trained using sampled softmax with the same computational budget. Our key\nobservation is that it is important to use a multi-layer Transformer for Bloom ﬁlter digests to\nremove ambiguity in the hashed input. We believe this provides an alternative method to solving\nproblems with large vocabulary size.\n1 Introduction\nIn natural language processing, one recent development, made popular by [22] is to use a smaller\nsub-word vocabulary [14], or so calledword piece model. In such a model, only frequent words and\nword pieces are kept in the vocabulary. Each word is then segmented as a sequence of word pieces.\nBoth the input and the prediction are then represented in the smaller word piece space.\nThe word piece model has multiple beneﬁts. Besides its generalizability and compact size, one\ncrucial beneﬁt is that we can afford to compute the full softmax loss on its much smaller vocabulary.\nThis leads to more precise predictions, (measured e.g. using recall at k for small values of k),\ncompared to alternative approaches such as the sampled softmax method [2, 3] or the hierarchical\nsoftmax [13]. Word pieces have been shown to work well for natural language understanding (NLU)\ntasks. For example, the recent break-through of BERT [8] uses a vocabulary of about 30K word\npieces. The goal of this paper is to extend this idea to machine learning tasks where we have to\nmodel a large number of categorical values, which are represented by opaque ids (e.g. product ids,\nvideo ids) or named entities (e.g. Wikipedia or Knowledge Graph entities).\nWhile word pieces are a natural way for breaking up words, it is unclear how this could be\ndone for a set of arbitrary categorical values (referred to as vocabulary throughout the paper). We\nadopt a technique proposed by [15] of using multiple hashing to reduce the vocabulary size while\nstill keeping each entity identiﬁable. The idea is to map each id to multiple hash tokens in a smaller\nspace, similarly to a Bloom ﬁlter [5], and then embed and predict at the token, instead of the id, level.\nThis way, the vocabulary is reduced to a much smaller size, similar to the effect of word pieces.\nHowever, hashing introduces much ambiguity due to random collisions. To solve this problem, we\npropose to use hashing in combination with a multi-layer Transformer [20], based on observations\nthat the Transformer can disambiguate word meanings well using context. A hashed token can be\n1\narXiv:2002.04723v1  [cs.LG]  11 Feb 2020\nviewed as a word piece with many different meanings. We hope that a Transformer model is also\nable to remove the ambiguity of hash tokens using the context, i.e. the set of other input tokens.\nWith these motivations, we build Superbloom in which we apply a Transformer model to the\nBloom ﬁlter digest of the input. On the output side, the predictions are on the hashed tokens, similar\nto [15]. We demonstrate, through experiments, that Superbloom works well for tasks with a large\nvocabulary size – it can be efﬁciently trained and outperforms non-hashed models of a similar size,\nand larger models trained with sampled softmax with the same computational budget.\nThe key insight from our experiments is that the multiple-layer Transformer is effective for\nresolving the ambiguity in the hashed input, hence works particularly well for Bloom ﬁlter digests.\nFor instance, we ﬁnd that the model quality gap between a one layer and a twelve layer Transformer\nmodel is signiﬁcantly larger when using Bloom ﬁlter digests, compared to that when the vocabulary\nis not hashed. This capability of the Transformer to “unhash” the Bloom digest is a key difference\nto earlier work on feature hashing [21] and multiple hashing [15, 19, 7]. In addition, we propose\nan efﬁcient approximate inference algorithm, by applying “beam search” in the much smaller token\nspace. Such fast inference is another useful property of Superbloom.\nThe Superbloom model is trained using BERT’s masked language model task, i.e. on predicting\nhash tokens masked out from the input. Since the hashes have a much smaller vocabulary, this\naspect is closely related to the error-correcting output codes (ECOC) [9, 4] 1. In the ECOC model,\na multi-class classiﬁcation is converted into many binary classiﬁcation problems where redundancy\nis added to improve the robustness of the model. The prediction task of Superbloom can be viewed\nas reducing a large multi-class classiﬁcation problem to a few smaller multi-class classiﬁcation\nproblems. However, unlike the ECOC models, we do not reduce the problem all way down to\nbinary predictions. This might be the additional reason that Superbloom is able to achieve high\naccuracy.\n1.1 Related work\nLearning with a large vocabulary is a well-studied but still open research problem. [21] proposed\nfeature hashing which uses random hashing to reduce the input vocabulary size, and then learns em-\nbeddings for hashed ids in the smaller vocabulary. Several follow-up works propose to better resolve\ncollisions by using multiple hashes: [19] proposed to learn a weighted sum of hashed embeddings;\n[16] used an unweighted sum, but proposed instead to learn the hash function itself; and [6] pro-\nposed to learn both the hash function and the combiner, for which they use either a linear function\nor an LSTM. A key difference with the aforementioned work is that we do not resolve the hashing\nearly at the input of the model; instead, we feed all hashed embeddings to the Transformer and let\nit learn to resolve the hashing collisions using the context. Our experiments show that multi-layer\nTransformer models indeed have the capacity to resolve hashing collisions while learning a high\nquality model.\nBesides reducing the input space and memory usage, another set of related work focuses on\ndealing with large output vocabularies and improving training efﬁciency. A commonly used method\nis sampled softmax [2, 3] where for each gradient update, only a subset of the output vocabulary\nis considered. Another line of work is hierarchical softmax where classes are organized in clus-\nters [11] or in a tree structure [13] to allow for efﬁcient pruning of the output vocabulary. Through\nour experiments, we show that Superbloom, which allows us to train a full softmax on the hashed\nvocabularies, can lead to more accurate results than using sampled softmax on the larger output vo-\ncabulary. [15] proposed to use Bloom ﬁlters as a general tool in deep models, for both the input and\noutput. Our work demonstrates the efﬁciency of a multi-layer Transformer-like architecture to use\ncontextual information to resolve hash ambiguity. Indeed, we show that shallow models, even with\nattention, fail.\n1This connection is suggested by an anonymous reviewer.\n2\nL layers\ns1\n...\nsn\nh\nh1(s1)\n...\nhm(s1)\n...\nh1(sn)\n...\nhm(sn)\nEI\nx11\n...\nx1m\n...\nxn1\n...\nxnm\nAttention\nlayer norm\nFeed Forward\nlayer norm\ny11\n...\ny1m\n...\nyn1\n...\nynm\nEO\np11\n...\np1m\n...\npn1\n...\npnm\nsoftmax\nη1(t1)\n...\nηm(t1)\n...\nη1(tn)\n...\nηm(tn)\nη\nt1\n...\ntn\nLoss\n(SI)n (HI)mn Rmn×d Rmn×d ∆(HO)mn (HO)mn (SO)n\nInput hashing Input embedding\nInput layer Transformer layers Output layer Output hashing\nFigure 1: Superbloom model architecture\n2 Superbloom Model Architecture\nGiven discrete sets SI,SO, representing respectively the input and output spaces (e.g. word tokens\nor entities), the goal is to model a function that maps a sequence ofnelements2 in SI, to a sequence\nof probability distributions over SO. The space of probability distributions over a set Swill be\ndenoted by ∆(S) = {p∈R|S|\n+ : ∑\ns∈Sps = 1}.\nThe input and output entities are typically represented using embedding matrices EI ∈R|SI|×d\nand EO ∈R|SO|×d, which map each entity to an embedding vector of dimension d. This makes\ntraining and inference expensive if the number of entities is very large. In order to reduce the\nmodel size and improve efﬁciency, we hash each element as follows. Given mhash functions hj,\nj ∈{1,...,m }, each element sis represented by the hashes (h1(s),...,h m(s)), which we refer to\nas a Bloom digest, given its similarity to the Bloom ﬁlter data structure. The set of values the hashes\ncan take is much smaller than the original spaces SI,SO, which allows us to reduce the vocabulary\nsize and thus the size of embedding matrices.\nWe decompose the Superbloom model architecture into M = O◦(TL ◦···◦ T1) ◦I, as illus-\ntrated in Figure 1: an input layer (Sec. 2.1) I : (SI)n →Rmn×d which maps each item in the input\nsequence to membeddings of dimension d; Ltransformer layers (Sec. 2.2) Ti : Rmn×d →Rmn×d\nwhich apply transformations in the embedding space; and an output layer (Sec. 2.3) O: Rmn×d →\n∆(HO)mn mapping each embedding to a probability distribution. Since the model predicts distri-\nbutions over HO instead of SO, both training (Sec. 2.4) and inference (Sec. 2.5) need to be adapted\naccordingly.\n2.1 Input layer I : (SI)n →Rmn×d\nThe input layer consists of mhash functions3 hj : SI →HI, j ∈{1,...,m }and an embedding\nmatrix EI ∈R|HI|×d. The input sequence (s1,...,s n) is mapped to the sequence of embeddings\n(Eh1(s1),...,E hm(s1),Eh1(sn),...,E hm(sn)). For ease of notation, we will write xi,j = Ehj(si) ∈\nRd, and denote the sequence by(xi,j)i=1,...,n\nj=1,...,m. Throughout, we use subscripts i,j to denote the j-th\nhash of the i-th element.\n2We assume a ﬁxed sequence length for simplicity. This is also a useful assumption for practical implementation on a\nTPU, which requires ﬁxed input dimensions.\n3The hash functions and their inverse mappings are randomly generated and stored as look-up tables. When generating\nthe hash functions, we make sure that each hash bucket is evenly sized, and that there are no complete collisions.\n3\n2.2 Transformer layers T : Rmn×d →Rmn×d\nThe Transformer is an attention-based model that was initially proposed for sequence transduction\ntasks, and that has been used in various other settings such as BERT. For the intermediate layers of\nSuperbloom, we use the same architecture as the original transformer model [20], which we brieﬂy\nsummarize in Appendix A. Each transformer layer is a function T : Rmn×d →Rmn×d which maps\na sequence of mn embeddings in Rd to another sequence in the same space. We will denote by\n(yi,j)i=1,...,n\nj=1,...,m the output sequence of the last transformer layer, where each yi,j ∈Rd.\n2.3 Output layer: O : Rmn×d →∆(HO)mn\nSimilarly to the input layer, we have m hash functions ηj : SO → HO, j ∈ {1,...,m }for\nthe output space. We modify the original goal of predicting distribution over SO to predicting\ndistributions over HO, as follows. If (yi,j)i=1,...,n\nj=1,...,m is the output of the last transformer layer, then\nthe output layer Omaps each yi,j to\npi,j := σ(yi,j(EO)⊤) ∈∆(HO),\nwhere EO ∈R|HO|×d is an output embedding matrix, and σ is the softmax function. Note that\nin some problems, the input and output spaces coincide, so it can be advantageous to use identical\ninput and output hash functions, hj = ηj, and the same embedding matrices EI = EO.\n2.4 Training\nIf the target sequence in SO is (t1,...,t n), then the corresponding target sequence in HO is\n(ηj(ti))i=1,...,n\nj=1,...,m. We deﬁne the training objective4 as\nn∑\ni=1\nm∑\nj=1\nℓ(pi,j,ηj(ti)).\nwhere pi,j ∈∆(HO) are the model’s output distributions, and ℓ : ∆( HO) ×HO →R is a loss\nfunction, e.g. the cross-entropy loss. Note that we can pre-process the training data to map the\nelements in the original spaces (SI)n,(SO)n to the hash spaces (HI)mn,(HO)mn, and training\nproceeds entirely in the hash spaces.\nModel size and efﬁciency Compared to a model trained on the original space, the main ad-\nvantage of Superbloom is a reduction in the size of the embedding matrices EI,EO. For instance,\nif a α-to-one hashing is used (i.e., each hash bucket containsαelements), then |H|= |S|/αand the\nsize of the input matrices is reduced by a factor α. This not only reduces the memory cost, but may\nalso improve the efﬁciency of gradient updates during training. Consider a cross-entropy loss, for\neach training example, all elements in the output space have a non-zero gradient due to the partition\nfunction in softmax, and thus the full matrix EO needs to be updated at each step, unless approxi-\nmate methods such as sampled softmax [2] are used. Our experiments (see Section 3.3) show that\nthe cost of updating EO dominates that of training, and a reduction in vocabulary size allows us to\nsigniﬁcantly reduce training time without resorting to negative sampling.\n2.5 Inference\nFor each position iin the sequence, the model outputs mdistributions (pi,j)j=1,...m ∈∆(HO)m,\nand our goal is to use these distributions to rank the elements of SO. To simplify notation, we\n4Note that unlike ECOC [9], the task is not to predict the individual bits in the output Bloom digest, but rather to predict\n(a probability distribution over) the index of the mnon-zero bits.\n4\nAlgorithm 1 Approximate and exact inference in Superbloom\n1: Input: Beam width B, a maximum iteration number N, model outputs pj ∈∆(HO) and hash\ninverse look-up tables η−1\nj , for j = 1,...,m .\n2: For each j, sort pj\n3: for b= B,...,NB do\n4: Let pb\nj be the b-th largest value in pj.\n5: For each j, compute Sb\nj = {s∈SO : pj(ηj(s)) ≥pb\nj}.\n6: Score all candidates in Sb = Sb\n1 ∪···∪ Sb\nm. Let s⋆ = arg maxs∈Sb γ(s).\n7: if γ(s⋆) ≥∑\nj log pb\nj then break. ⊿This guarantees γ(s) ≤γ(s⋆) for all s.\n8: return s⋆.\nSB\n1\nSB\n2 SB\np1\np2\nSB\nS2B\np1\np2\nFigure 2: Illustration of approximate and exact inference, with a number of hashes m = 2, a four-\nto-one hashing scheme, and a beam width B = 2.\nassume in this section that iis ﬁxed and will omit it by writing pj instead of pi,j.\nOne simple way to rank items, as proposed by [15], is to compute, for eachs, γ(s) := ∑m\nj=1 log pj(ηj(s)).\nWhen SO is very large, this can be expensive, so instead of exhaustively scoring all itemss, we pro-\npose an iterative beam-search, given in Algorithm 1 and illustrated in Figure 2, that can be used\nto compute the top- k elements5, either exactly, or approximately by ﬁxing a maximum iteration\nnumber.\nLet us ﬁx a beam width b, and let pb\nj be the b-th largest value of pj, and let Sb\nj = {s ∈SO :\npj(ηj(s)) ≥pb\nj}. In words, Sb\nj are elements whose hash is in the top b values according to pj.\nSb\nj is obtained by pre-computing and storing inverse look-up tables6 η−1\nj for each hash function, and\nobserving that Sb\nj = ∪h:pj(h)≥pb\nj\nη−1\nj (h). This deﬁnes a set of candidates to score Sb := Sb\n1 ∪···∪\nSb\nm, and guarantees the following upper-bound: for all s /∈Sb, γ(s) ≤∑\nj log pb\nj. If the best scored\nelement s⋆ := arg maxs∈Sb γ(s) satisﬁes γ(s⋆) ≥∑\nj log pb\nj, then we have a certiﬁcate that s⋆ is\nthe best element over the entire set and the algorithm terminates. Otherwise, we increase the beam\nwidth and score a new set of candidates.\nAn example is illustrated in Figure 2 for m= 2, a beam width B = 2, and hash functions with\nα = 4 (four elements share the same hash value along each dimension). The subset of candidates\nto score during the ﬁrst iteration is highlighted in blue. The top element s⋆ is circled, and the solid\nline shows its γlevel set. In the left ﬁgure, the level set does not intersect the shaded area (unscored\nelements), thus we have a certiﬁcate that s⋆ is the exact maximizer. In the right ﬁgure, the level set\ndoes intersect the shaded area, so to ﬁnd the exact maximizer, a second iteration is performed where\nthe search region is extended (highlighted in red).\n5For simplicity, we describe the algorithm for k= 1, but we apply it for larger kin our experiments.\n6The cost of storing inverse look-up tables is dominated by that of storing embedding tables as a long as mα < dfor\nan α-to-one hashing scheme, since the inverse lookups have total size O(mα|HO|), while the embedding tables have size\nO(d|HO|). This is always the case in our experiments.\n5\nComputational complexity Consider an α-to-one hashing scheme. Sorting the vectors pj (line 2)\ncosts O(m|HO|log |HO|). Each iteration consists of computing Sb\nj (line 5) then scoring candidates\nin Sb (line 6) which costs O(m2Bα). The total cost for N iterations is O(m|HO|log |HO|+\nm2NBα) which can be signiﬁcantly cheaper than scoring all candidates O(|SO|). For example,\nwith the parameter setting described in Section 3.3, approximate inference is 10 times faster than\nexhaustive scoring. In Appendix B, we study the effect of the beam width on the quality of the\nmodel.\nRemark 1. While we describe a particular choice of ranking function γ, it is possible to generalize\nthe algorithm to other ranking functions that are increasing, in a sense described in Appendix C.\n3 Wikipedia entity prediction\nWe apply Superbloom to the Wikipedia entity prediction task, in which we use surrounding links\non a Wikipedia page to predict a held-out link. This task is derived from the same data set as many\nNLU tasks, but uses entities instead of natural language. We believe this study is complementary\nto previous NLU models trained on Wikipedia, that focus on modeling language. Indeed, we show\nthrough examples that the model can learn entity relations well and demonstrates a strong use of\ncontextual information.\nThe task needs to model about 5.3 million entity pages on Wikipedia. This vocabulary size is\ntwo orders of magnitude larger than in previous work that applies a Transformer model with full\nsoftmax loss [8, 23, 18]. Other works, such as [24] and [17], train a Transformer model with a large\nnumber of entities using sampled softmax, with either in-batch or in-example negative sampling.\nBut as we shall show, sampled softmax, even with a large number of 128K negative samples, results\nin much worse quality.\n3.1 Task\nWe take all the entity pages on the website en.wikipedia.org. For each page, we obtain the URL\nlinks to other Wikipedia entity pages. We only use “raw” links, i.e. links that explicitly appear\non the page. We obtain 5,281,889 pages and 462,588,415 links. Since the Wikipedia site usually\nremoves duplicates of links on each page, the distribution of pages is rather long tail. For example,\nthe top 100 most frequent pages represent only 3.8% of the total links, and the top 10% most frequent\npages represent about 60% of the total links.\nWe hold out 10% random entity pages for testing. For the training data, we apply a masking\nsimilar to BERT – from each page, we take a random contiguous segment of entities, of length up\nto n = 32 , and mask 15% of the segment. The task is then to predict the masked entities. We\nalso apply the same input perturbation, where for the input, each masked out link is either replaced\nwith a special [MASK] entity (with 80% probabilty), replaced with a random entity (with 10%\nprobability), or left unchanged (with 10% probability). For evaluation, we hold out (i.e. replace\nwith the [MASK] token) one random entity from a random segment on a test page. For quality\nevaluation, we use recall at kmetric (abbreviated as rec@kbelow), which represents the chance the\nheld out entity is in one of the top kpredictions.\n3.2 Model\nTo apply Superbloom, we ﬁrst create mhash maps from entities to hash tokens with a given hash\ndensity α. Each hash map is obtained by applying a random permutation to the vocabulary and map\nevery consecutive α entities to the same token. This way we guarantee each hash token to have\n6\nthe same number of collisions α.7 Special tokens [CLS], [MASK], [SEP], are each mapped to m\ntokens with no collisions. For example we create [MASK1],.., [MASKm] tokens corresponding to\n[MASK].\nWe apply the hashing to the input and target, to map each entity to m tokens as described in\nSection 2. We then apply the Transformer model to the input to predict the masked tokens. Unlike\nin BERT, we do not use position embeddings, in other words, we treat the input as a set instead of a\nsequence. Since the input and output spaces coincide, we use the same hash functions and the same\nembedding matrices in the input and output layer.\nWe carry out experiments on both the full vocabulary as well as a smaller subset consisting of the\ntop 500K entity pages. On the smaller vocabulary, we are able to train a baseline model with large\ncapacity, with no hashing and no sampling, which is useful for understanding the best achievable\nmodel quality.\nWe train all of our models on 16 Cloud TPUs. We use a batch size of 1024 for experiments\nwith full vocabulary and 4096 for experiments with 500K vocabulary. All the experiments use the\nAdam optimizer [12], and use a decreasing learning rate sequence with inverse square root decay,\nand initial learning rate 1e-4 for the full vocabulary and 2e-4 for the 500K vocabulary. All the\nexperiments have been run for more than 1 million steps to reach near convergence.\n3.3 Superbloom is more accurate\nWe experiment with two models of similar size: one is a baseline model (baseline) with full vo-\ncabulary of sizeNequal to the number of entities; the other is a Superbloom model (superbloom)\nwith a heavy 50 to 1 hashing. We set other hyper-parameters (such as the embedding dimension) so\nboth models have a similar size. We also compare to a large model (sampled-softmax) trained\nusing sampled softmax. Table 1 lists the hyper-parameters of each model. Recall that α denotes\nthe number of collisions ( 1 if there is no hashing), dthe embedding dimension, nA the number of\nattention heads, dF the dimension of intermediate hidden layers, and Lthe number of transformer\nlayers. In all of our experiments, we use two hash functions for Superbloom models. Hence their\nvocabulary size is 2N/α.\nmodel α d n A dF L #parameters #samples\nbaseline 1 48 4 1024 12 248M 5.3M\nsampled-softmax 1 512 8 2048 12 2.6G 128K\nsuperbloom 50 768 12 3072 12 229M 200K\nTable 1: Model parameters. “#samples” lists the number of samples in the softmax loss computa-\ntion. For baseline and superbloom, since there is no sampling, this number corresponds to the full\nvocabulary, 5.3M and 200K, respectively. For sampled-softmax, we use 128K samples.\nTable 2 shows the recall metrics of the models. For the Superbloom model, we set the beam\nwidth to B = 20 (our experiments suggest that it is sufﬁcient to set B = kin order to achieve the\nbest rec@k metric, see Appendix B for details).\nThe Superbloom model clearly outperforms, to a large extent, both the baseline and the sampled-\nsoftmax model. We note that the sampled-softmax model has much worse rec@ k than the other\ntwo models, and this gap is larger for smaller k. This is not surprising given the relatively small\npercentage (2.5%) of negative examples we can afford to sample.\nWhile the Superbloom model performs well overall, there is a possibility that it devotes most of\nthe embedding capacity to the top entities, so it loses accuracy on the less frequent entities. To test\n7The procedure described here is for simplicity. If we are concerned with space, we may use some space efﬁcient methods,\nfor example a perfect hash function [10].\n7\nmodel rec@1 rec@10 rec@20\nbaseline 36.2% 63.1% 68.2%\nsampled-softmax 3.1% 36.2% 55.1%\nsuperbloom 51.1% 72.3% 76.5%\nTable 2: Recall metrics for different models.\nthis, we plot the rec@1 value as a function of label frequency. In Figure 3, we show the mean rec@1\nfor every 10 percentile bucket in terms of the label frequency. We can observe that Superbloom\nis more accurate than the baseline in all the buckets. Another interesting phenomenon is that the\nmost challenging labels are those in the 20 and 30 percentile. One possible reason is that they lack\nthe higher predictability of the most frequent labels, and also the strong regularity of less frequent\nlabels.\n10 20 30 40 50 60 70 80 90 100\nlabel frequency (percentile)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7rec@1\nsuperbloom\nbaseline\nFigure 3: Rec@1 with respect to label frequency, starting from the most frequent labels.\nBesides the high predictive accuracy, the prediction from the model shows strong semantic abil-\nity and context dependency. We show some examples of predictions in Figure 4 in Appendix E. In\none set of examples, we pair “Copenhagen” with different entities, and observe that the predictions\nchange accordingly, depending on the context. Another observation is that despite the heavy hash-\ning, there are almost no unrelated entities in the top 10 predictions. The model even exhibits an\nability to perform certain analogy tasks (without being trained on such tasks) – for example, given\n“Tunisia Tunis Thailand”, it predicts “Bangkok” as the top result.\n3.4 Mulit-layer Transformer is important for Superbloom\nIntuitively, given the large noise introduced by hashing, it is more important for Superbloom to\nuse multiple attention layers in Transformer to “remove” the noise. To test this intuition, we run\nexperiments with a smaller vocabulary size of the top 500K entity pages (about 60% of the links).\nOn this smaller vocabulary size, we can afford to run a full softmax model with a larger embedding\ndimension.\nWe consider different embedding dimensions and model complexity. Table 3 lists the model\nparameters as well as the recall metrics for each model. We observe that for the baseline models, the\nquality difference is small between models of different complexity. For example, rec@1 of baseline-\nl12 (55.0%) is about 8% better than baseline-l1 (51.0%). Since a one layer Transformer is close to a\nbag-of-words (BOW) model, one may argue that it may be unnecessary to use a Transformer in this\ncase – instead one can use a larger dimension BOW model to achieve a similar accuracy.\nHowever, for Superbloom models, the quality improves signiﬁcantly with more layers. When\n8\nmodel α d n A dF L #parameters rec@1 rec@10 rec@20\nbaseline-l1 1 256 1 1024 1 123M 51.0% 70.4% 75.5%\nbaseline-l12 1 256 8 1024 12 132M 55.0% 73.7% 77.3%\nsuperbloom-d256l1 20 256 1 1024 1 13M 17.8% 35.8% 42.6%\nsuperbloom-d384l1 20 384 1 1536 1 21M 30.6% 52.9% 58.7%\nsuperbloom-d256l12 20 256 8 1024 12 21M 43.4% 60.1% 64.0%\nTable 3: Model parameters and recall metrics.\nincreasing the number of layers from 1 (superbloom-d256l1) to 12 (superbloom-d256l12), rec@1\nincreases from 17.8% to 43.4%. The multi-layer model also performs much better than the single\nlayer model with the same size (superbloom-d384l1). Note that previous work on hashed vocabu-\nlaries relies on BOW models, which are less expressive than even a single-layer transformer. This\nhighlights one of our key observations that multi-layer Transformer models are more effective for\nworking with hashed vocabularies.\n4 Experiments on natural language data\nIn this section, we apply Superbloom to natural language data. We consider a large vocabulary that\ncontains frequent unigrams and bigrams and use it to tokenize the text, then apply a Bloom ﬁlter to\nreduce the vocabulary size. We show that despite high hash collisions, the model can achieve high\naccuracy on natural language data. Since many named entities appear in the large vocabulary, we\nobserve that the model seems to make better predictions of named entities than the BERT model.\nWhile each hash id can be regarded as a word piece in an NLU model, there are important differ-\nences between hash ids and word pieces. First, hashing causes random collisions, while wordpiece\ntokenization can be viewed as a special hashing scheme based on the spelling – there is often co-\nherence between words that share a word piece. As suggested by the experiments in Appendix D,\nrandom hashing with Superbloom digests may outperform coherent hashing. In addition, as every\ntoken in the large vocabulary is hashed, we do not have unambiguous anchors (such as the exact\nword pieces) to help bootstrap the disambiguation process. Despite these differences, our experi-\nments suggest that even with high hashing collisionα= 40, the Transformer is capable of resolving,\nor unhashing, the Bloom ﬁlter digest effectively and produces highly accurate predictions and mean-\ningful embeddings.\nWe construct a vocabulary of size 1M by taking the union of standard BERT word piece vo-\ncabulary (∼30K) with the most frequent unigrams and bigrams, and follow the same procedure in\nBERT to create training examples. For Superbloom, we apply random hash maps to the 1M vocab-\nulary similar to the approach described in Section 3.2 to ensure an even number of collisions. The\nSuperbloom architecture is chosen to have a comparable model size to the baseline BERT model.\nWe compare four models: For the non-hashed baselines, we have a large model with embedding\ndimension d = 256, and a small model with d = 64. And we have two Superbloom models with\nsimilar model sizes. We list the parameters in Table 4. In Table 5 we list the recall metrics for the\nmodels. We observe that with comparable model size, Superbloom outperforms the baseline model\nin all the recall metrics, and the improvement is more signiﬁcant for smaller model size.\nSince many named entities are included in the larger vocabulary, the Superbloom model shows\nthat it may have better “understanding” or representation of those entities. We show some anecdotal\nevidence in Appendix E by comparing predictions of pretrained BERT and Superbloom model on\nsome ﬁll-in-the-blanks examples. The BERT model often predicts generic words, seemingly ignor-\ning other named entities in the sentence. The Superbloom model, on the other hand, can often ﬁll in\n9\nmodel α d n A dF L #parameters\nbaseline-h64 1 64 4 256 12 62.6M\nbaseline-h256 1 256 8 1024 12 254.4M\nhash40-h512 40 512 8 2048 12 62.3M\nhash20-h1024 20 1024 16 4096 12 246.3M\nTable 4: The model parameters.\nmodel name rec@1 rec@10 rec@20 model name rec@1 rec@10 rec@20\nbaseline-h64 28.4% 44.9% 48.6% baseline-h256 37.2% 57.4% 63.3%\nhash40-h512 31.7% 48.3% 52.9% hash20-h1024 39.2% 58.5% 64.5%\nTable 5: Recall metrics.\nthe blank with related entities.\n5 Conclusion\nOur experiments show that the multi-layer Transformer is effective for achieving high accuracy\non hashed inputs, represented using Bloom ﬁlter digests. Besides applying it to tasks with large\nvocabularies, it also points to a few interesting future research directions.\nThe Transformer model has been mostly studied in natural language settings and for sequence\ndata. In our setup, we show that it can work effectively with sets of hashed entities. We hope\nthat by investigating this simpler setup, it can help us better understand the properties of the Trans-\nformer. For example, due to hashing, each token is similar to words with multiple meanings, so\nits embedding can be viewed as a combination, possibly linear [1], of the embeddings of multiple\nentities. A multi-layer Transformer model may provide a mechanism for iteratively ﬁltering such\nnoisy representations, using the context. It would be interesting to further study this mechanism.\nWhile hashing adds noise to the learned representations, it can also increase the ﬂexibility of\nthese representations – when we hash multiple entities to the same token, the model is free to al-\nlocate the corresponding embedding unevenly among entities, which results in a different effective\nembedding dimension for each entity. Such learned capacity allocation might be more efﬁcient than\nusing a ﬁxed embedding size or frequency-based allocation. Of course, an effective “denoising”\nmodel is a pre-requisite for such an approach to work. Perhaps Superbloom, with its strong denois-\ning ability, can help further realize the potential of embedding models on hashed vocabularies.\nAcknowledgments We would like to thank Fernando Pereira for the useful discussions and\nfor suggesting the name of Superbloom.\nReferences\n[1] S. Arora, Y . Li, Y . Liang, T. Ma, and A. Risteski. Linear algebraic structure of word senses,\nwith applications to polysemy. Transactions of the Association for Computational Linguistics,\n6:483–495, 2018.\n10\n[2] Y . Bengio and J.-S. S ´en´ecal. Quick training of probabilistic neural nets by importance sam-\npling. In Proceedings of the conference on Artiﬁcial Intelligence and Statistics (AISTATS) ,\n2003.\n[3] Y . Bengio and J.-S. S´en´ecal. Adaptive importance sampling to accelerate training of a neural\nprobabilistic language model. Transactions on Neural Networks, 19(4):713–722, Apr. 2008.\n[4] A. Berger. Error-correcting output coding for text classiﬁcation. In IJCAI-99: Workshop on\nmachine learning for information ﬁltering, 1999.\n[5] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of\nthe ACM, 13(7):422–426, 1970.\n[6] T. Chen, M. R. Min, and Y . Sun. Learning k-way d-dimensional discrete codes for compact\nembedding representations. In Proceedings of the 35th International Conference on Machine\nLearning, pages 854–863, 2018.\n[7] A. Daniely, N. Lazic, Y . Singer, and K. Talwar. Short and deep: Sketching and neural networks.\nIn ICLR Workshop, 2017.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[9] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output\ncodes. Journal of artiﬁcial intelligence research, 2:263–286, 1994.\n[10] M. L. Fredman, J. Koml ´os, and E. Szemer ´edi. Storing a sparse table with 0(1) worst case\naccess time. J. ACM, 31(3):538–544, June 1984.\n[11] J. Goodman. Classes for fast maximum entropy training. In IEEE International Conference\non Acoustics, Speech, and Signal Processing (ICASSP ’01). IEEE, 2001.\n[12] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[13] F. Morin and Y . Bengio. Hierarchical probabilistic neural network language model. In R. G.\nCowell and Z. Ghahramani, editors,Proceedings of the Tenth International Workshop on Artiﬁ-\ncial Intelligence and Statistics, pages 246–252. Society for Artiﬁcial Intelligence and Statistics,\n2005.\n[14] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with sub-\nword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics, 2016.\n[15] J. Serr `a and A. Karatzoglou. Getting deep recommenders ﬁt: Bloom embeddings for sparse bi-\nnary input/output networks. In Proceedings of the Eleventh ACM Conference on Recommender\nSystems, RecSys ’17, pages 279–287, 2017.\n[16] R. Shu and H. Nakayama. Compressing word embeddings via deep compositional code learn-\ning. In ICLR, 2018.\n[17] L. B. Soares, N. FitzGerald, J. Ling, and T. Kwiatkowski. Matching the blanks: Distributional\nsimilarity for relation learning. arXiv preprint arXiv:1906.03158, 2019.\n[18] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang. Bert4rec: Sequential recommendation\nwith bidirectional encoder representations from transformer.arXiv preprint arXiv:1904.06690,\n2019.\n11\n[19] D. T. Svenstrup, J. Hansen, and O. Winther. Hash embeddings for efﬁcient word representa-\ntions. In Advances in Neural Information Processing Systems, 2017.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008, 2017.\n[21] K. Weinberger, A. Dasgupta, J. Attenberg, J. Langford, and A. Smola. Feature hashing for\nlarge scale multitask learning. In ICML, 2009.\n[22] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey, M. Krikun, Y . Cao, Q. Gao,\nK. Macherey, et al. Google’s neural machine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n[23] S. Zhang, Y . Tay, L. Yao, and A. Sun. Dynamic intention-aware recommendation with self-\nattention. arXiv preprint arXiv:1808.06414, 2018.\n[24] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu. Ernie: Enhanced language represen-\ntation with informative entities. arXiv preprint arXiv:1905.07129, 2019.\n12\nA Transformer architecture\nWe brieﬂy recall the Transformer architecture following [20]. Each transformer layer is a function\nT : Rmn×d →Rmn×d which transforms a sequence of mnembeddings8 in Rd to another sequence\nof mnembeddings in the same space. Identifying the sequence xi,j with the matrix X ∈Rmn×d,\nwe can write T as the composition T = F ◦A, where\n• Ais an attention function,\nA(X) =\nnA∑\na=1\nσ\n(\n(XQa)(XKa)⊤/\n√\ndA\n)\nXVaW⊤\na , (1)\nwhere a ∈{1,...,n A}indexes attention heads, dA ≤dis an internal embedding dimension\n(usually dA = d/nA), and for each a, Qa,Ka,Va,Wa ∈Rd×dA. Finally, σ: Rn×n →∆([n])n\nis the row-wise softmax function, given by\nσ(Y)ij = exp(Yij)∑n\nl=1 exp(Yil). (2)\nOne interpretation of the attention function is that it forms the i-th output embedding by taking\na convex combination of input embeddings weighted by the softmax weights, followed by a\nlow-rank transformation VaW⊤\na ∈Rd×d.\n• F is a fully connected feed-forward network given by F(X) = ReLU(XU1 + b1)U⊤\n2 + b2,\nwhere Ui ∈Rd×dF for some dF ≥d.\nA residual connection and layer normalization are also applied at each stage A,F.\nB The quality of beam search\nWe investigate the effect of beam search width (parameter B in Algorithm 1) on model quality.\nTable 6 shows rec@ k for k = 1 ,10,20 for different beam widths B, using a small number of\ntest examples, for the Superbloom model described in Section 3.2. In all our experiments, we run\napproximate inference with one step.\nWe observe that while the quality generally increases with an increased beam width, the increase\nin rec@k is only marginal when B ≥k. Thus, to obtain highest rec@ k, it is sufﬁcient to set the\nbeam width to B = k.\nbeam width rec@1 rec@10 rec@20\nB=1 53.0% 56.0% 56.0%\nB=10 53.2% 68.2% 69.1%\nB=20 53.2% 67.9% 71.0%\nB=100 53.2% 67.8% 71.5%\nTable 6: Recall metrics at different beam width.\n8A minor difference with the original Transformer model is that we operate on Rmn×d instead of Rn×d, since we have\nmembeddings for each element in the sequence.\n13\nC Beam search with general score functions\nThe beam search algorithm described in Algorithm 1 can be generalized to any ranking function that\nis increasing, in the following sense.\nThe output of the model deﬁnes, for each candidates∈SO, a vector of scores(pj(ηj(s))j=1,...,m.\nTo sort the candidates, one can deﬁne an aggregated score γ(s) = c((pj(ηj(s)))j) for any function\nc : Rm →R that induces a total ordering over Rm. One natural assumption to require of c is\nthat it be increasing, in the sense that if ρ ⪰ρ′element-wise then c(ρ) ≥c(ρ′). This holds for\nc(ρ) = ∑\nj log ρj (used in Section 2.5), but also includes a much larger class, e.g. c(ρ) = minj ρj\nor c(ρ) = max j ρj. The beam search Algorithm 1 can be immediately generalized to any such\nfunction. The only required change is to replace the condition on line 7 (γ(s⋆) ≥∑\nj log(pb\nj)) with\nγ(s⋆) ≥c(pb). To prove correctness, one needs to verify that the optimality certiﬁcate holds in this\ngeneral case, i.e.,\nLemma 1. Let γ(s) = c((pj(ηj(s)))j), where cis increasing in the sense deﬁned above, and letpb,\nSb\nj be as deﬁned in Algorithm 1. Then,\n∀s /∈Sb,γ(s) ≤c(pb).\nIt follows from the lemma that the algorithm can terminate whenever γ(s⋆) ≥ c(pb), since\nγ(s⋆) ≥γ(s) for all s∈Sb (by deﬁnition of s⋆) and for all s /∈Sb (by the lemma).\nProof. Since Sb = ∪j{s: pj(ηj(s)) ≥pb}, then the complement of Sb is the set {s: pj(ηj(s)) <\npb\nj∀j}. Thus, since c is increasing, it follows that for all s /∈ Sb, c((pj(ηj(s)))j) ≤ c(pb), as\nclaimed.\n14\nD Comparison of different hashing schemes\nWe have used random hashing functions in Superbloom. One natural alternative is “coherent” hash-\ning, in which we map similar entities to the same hash bucket. A potential beneﬁt of coherent\nhashing is that it may use embedding capacity more effectively by sharing it among similar entities.\nHowever, the downside is that it becomes difﬁcult to distinguish those similar entities.\nTo create a coherent hashing function, we ﬁrst run a co-occurrence factorization algorithm and\nthen group similar entities together using the following procedure, designed to guarantee equal-sized\nhash buckets. For each entity, in decreasing frequency order, we compute the nearest neighbors\n(scored using cosine similarity), then create a hash bucket that includes the elements and its α−1\nnearest neighbors which have not been already assigned a bucket. When creating a second coherent\nhash function, we add the constraint that any pair of elements that share a bucket for the ﬁrst hash\nfunction cannot be assigned to the same bucket in the second hash. This ensures that no two elements\nhave the same collision in both hash functions.\nWe carry out the experiments on the data set with smaller vocabulary (500K). We train different\nmodels that all use two hash functions, with the following conﬁgurations: both random, one random\nand one coherent; and both coherent. We also use different hashing densities α = 10 and α = 20.\nAll the models have the same hyper-parameters as the superbloom-l12 model in Section 3.4. The\nresults are given in the following table.\nmodel α #coherent hashing token rec@1 entity rec@1\nhash10-00 10 0 36.32% 52.50%\nhash10-01 10 1 38.19% 50.20%\nhash10-11 10 2 38.55% 34.70%\nhash20-00 20 0 33.39% 43.70%\nhash20-01 20 1 36.98% 41.10%\nhash20-11 20 2 37.65% 30.20%\nTable 7: Random hashing versus coherent hashing.\nWe observe that with coherent hashing, we get higher accuracy for predicting hash tokens but\nlower accuracy for predicting entities. And the entity recall@1 is signiﬁcantly lower when both hash\nfunctions are coherent. This indicates that with higher coherence, it becomes increasingly difﬁcult\nfor the model to make ﬁner distinctions between similar items.\n15\nE Examples of Wikipedia entity predictions\n1. Examples of pairing “Copenhagen” with different entities. The predictions vary according to the context, from Danish\ncities, to major European cities, to Danish royalty, and Danish culture. There is a one unrelated result (underlined), which\ndisappears in the presence of additional context.\nCopenhagen [MASK]\nDenmark Oslo Stockholm Paris Berlin Aarhus Danish language University of Copenhagen Sweden Copenhagen\nCopenhagen Aarhus [MASK]\nDenmark Odense Copenhagen Aalborg Aarhus Oslo Malm¨o Max Wilms Stockholm Esbjerg\nCopenhagen Paris [MASK]\nBerlin Denmark London Oslo Rome Vienna Stockholm New York City Brussels Hamburg\nCopenhagen Dynasty [MASK]\nDenmark Margrethe II of Denmark Danish language Copenhagen Catholic Church Rome Christian V of Denmark Jutland\nWhen We Wake Up Frederik, Crown Prince of Denmark\nCopenhagen Dynasty Danish language [MASK]\nDenmark German language Margrethe II of Denmark Catholic Church Copenhagen English language\nPrincess Benedikte of Denmark Danish language Frederik, Crown Prince of Denmark Christian V of Denmark\n2. Examples of Jazz musicians. These relatively long and rare name entities would not appear in the vocabulary of a word\npiece model.\nMiles Davis [MASK]\nJazz Columbia Records Miles Davis John Coltrane Dizzy Gillespie Bill Evans Album Sonny Rollins AllMusic\nCharles Mingus\nJohn Coltrane [MASK]\nMiles Davis AllMusic Jazz A Love Supreme Rolling Stone Elvin Jones Albert Ayler Tenor saxophone New York City\nDrum kit\nMiles Davis John Coltrane [MASK]\nJazz Charles Mingus Album AllMusic Miles Davis Dizzy Gillespie Thelonious Monk Sonny Rollins Charlie Parker\nBill Evans\n3. Example showing that the prediction is the set union if two entities are not related.\nMiles Davis Thailand [MASK]\nVietnam Bangkok Japan Miles Davis Cambodia Malaysia Jazz Indonesia Thai language Brazil Myanmar Rock music\nDizzy Gillespie John Coltrane\n4. Examples for completing location analogy task!\nTexas Austin, Texas Florida [MASK]\nMiami Houston Orlando, Florida Dallas Jacksonville, Florida Fort Lauderdale, Florida Tampa, Florida Georgia (U.S. state)\nTallahassee, Florida St. Petersburg, Florida\nTunisia Tunis Thailand [MASK]\nBangkok Philippines Montcau Tokyo Malaysia Singapore Indonesia Pattaya Vietnam Thai language\nFigure 4: Examples of Superbloom model predictions. For each example, we output the top 10\npredictions of the model (computed using Algorithm 1 with a beam width B = 10 ). The entity\nnames shown here are obtained by removing the preﬁx “https://en.wikipedia.org/wiki/” from the\nentity URL.\n16\nF Examples of natural language entity predictions\nMiles Davis is a Jazz musician, he is similar to [MASK].\nBERT: jazz himself beethoven him davis chopin bowie williams jones\nbaseline-h256: miles davis john coltrane bill evans charlie parker louis armstrong sonny rollins\nkeith jarrett thelonious monk jazz duke ellington\nhash20-h1024: miles davis john coltrane charlie parker thelonious monk dizzy gillespie bill evans\nbillie holiday duke ellington humans is louis armstrong\nEmpire state building is an iconic site of [MASK1] , it is close to [MASK2] .\n[MASK1]\nBERT: architecture chicago manhattan downtown pittsburgh art philadelphia history washington\namerica\nbaseline-h256: architecture modern art contemporary art modern architecture national signiﬁcance\nnew york art its day historical signiﬁcance the city\nhash20-h1024: the city new york lower manhattan manhattan the neighborhood downtown\nwall street the area harlem architecture\n[MASK2]\nBERT: downtown it chicago philadelphia rome london broadway manhattan chinatown campus\nbaseline-h256: downtown downtown pittsburgh city hall new york the city times square\ncolumbia university san francisco philadelphia the pentagon\nhash20-h1024: central park city hall times square wall street union station broadway\nlower manhattan the pentagon ﬁfth avenue carnegie hall\nFigure 5: Natural language ﬁll-in-the-blank examples. BERT is the base BERT model in [8];\nbaseline-h256 and hash20-h1024 are the Superbloom models with 1M vocabulary, with model pa-\nrameters listed in Table 4.\n17",
  "topic": "Bloom filter",
  "concepts": [
    {
      "name": "Bloom filter",
      "score": 0.8446454405784607
    },
    {
      "name": "Softmax function",
      "score": 0.799921989440918
    },
    {
      "name": "Hash function",
      "score": 0.7279627919197083
    },
    {
      "name": "Computer science",
      "score": 0.7080909609794617
    },
    {
      "name": "Transformer",
      "score": 0.6537348628044128
    },
    {
      "name": "Language model",
      "score": 0.5625280737876892
    },
    {
      "name": "Vocabulary",
      "score": 0.4840051829814911
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4591294229030609
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.4247957468032837
    },
    {
      "name": "Preprocessor",
      "score": 0.42123210430145264
    },
    {
      "name": "Algorithm",
      "score": 0.2950567305088043
    },
    {
      "name": "Deep learning",
      "score": 0.25380176305770874
    },
    {
      "name": "Computer vision",
      "score": 0.09168276190757751
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}