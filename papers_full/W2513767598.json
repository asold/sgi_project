{
    "title": "It-disambiguation and source-aware language models for cross-lingual pronoun prediction",
    "url": "https://openalex.org/W2513767598",
    "year": 2016,
    "authors": [
        {
            "id": "https://openalex.org/A2251698225",
            "name": "Sharid Loáiciga",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2250311742",
            "name": "Liane Guillou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2026345578",
            "name": "Christian Hardmeier",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2577914266",
        "https://openalex.org/W203948990",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W93838281",
        "https://openalex.org/W84597476",
        "https://openalex.org/W2151996595",
        "https://openalex.org/W2293778248",
        "https://openalex.org/W2606531491",
        "https://openalex.org/W2250781619",
        "https://openalex.org/W2251029673",
        "https://openalex.org/W576550507",
        "https://openalex.org/W1738081185",
        "https://openalex.org/W1837762084",
        "https://openalex.org/W126222424",
        "https://openalex.org/W2610630890",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2576212274",
        "https://openalex.org/W1973452851",
        "https://openalex.org/W2043126905",
        "https://openalex.org/W2250976259",
        "https://openalex.org/W1996010593",
        "https://openalex.org/W2514579439",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W2129657639",
        "https://openalex.org/W2137471318"
    ],
    "abstract": "We present our systems for the WMT 2016 shared task on cross-lingual pronoun prediction.The main contribution is a classifier used to determine whether an instance of the ambiguous English pronoun \"it\" functions as an anaphoric, pleonastic or event reference pronoun.For the English-to-French task the classifier is incorporated in an extended baseline, which takes the form of a source-aware language model.An implementation of the sourceaware language model is also provided for each of the remaining language pairs.",
    "full_text": "Proceedings of the First Conference on Machine Translation, V olume 2: Shared Task Papers, pages 581–588,\nBerlin, Germany, August 11-12, 2016.c⃝2016 Association for Computational Linguistics\nIt-disambiguation and source-aware language models for cross-lingual\npronoun prediction\nSharid Lo´aiciga\nD´epartement de Linguistique\nUniversity of Geneva\nsharid.loaiciga@unige.ch\nLiane Guillou\nCIS\nLMU Munich\nliane@cis.uni-muenchen.de\nChristian Hardmeier\nDept. of Linguistics & Philology\nUppsala University\nchristian.hardmeier@lingﬁl.uu.se\nAbstract\nWe present our systems for the WMT 2016\nshared task on cross-lingual pronoun pre-\ndiction. The main contribution is a clas-\nsiﬁer used to determine whether an in-\nstance of the ambiguous English pronoun\n“it” functions as an anaphoric, pleonas-\ntic or event reference pronoun. For the\nEnglish-to-French task the classiﬁer is in-\ncorporated in an extended baseline, which\ntakes the form of a source-aware language\nmodel. An implementation of the source-\naware language model is also provided for\neach of the remaining language pairs.\n1 Introduction\nThe WMT 2016 shared task on cross-lingual pro-\nnoun prediction focuses on the translation of the\nsubject position pronouns “it” and “they” for sev-\neral language pairs (Guillou et al., 2016). Both of\nthese pronouns perform multiple functions in text,\nand disambiguation is required if they are to be\ntranslated correctly into other languages (Guillou,\n2016). The pronoun “they” is typically used as an\nanaphoric pronoun, but may also be used generi-\ncally, for example in “ They say it always rains in\nScotland”. The pronoun “it” may be used as an\nanaphoric, pleonastic or event reference pronoun.\nExamples of these pronoun functions are provided\nin Figure 1.\nanaphoric I have a bicycle. It is red.\npleonastic It is raining.\nevent He lost his job. It came as a total\nsurprise.\nFigure 1: Examples of different pronoun functions\nAnaphoric pronouns corefer with a noun phrase\n(i.e. the antecedent). Pleonastic pronouns, in con-\ntrast, do not refer to anything but are required to\nﬁll the subject position in many languages, includ-\ning English, French and German. Event reference\npronouns may refer to a verb, verb phrase, clause\nor even an entire sentence.\nDifferent French pronouns are required when\ntranslating an instance of “it” depending on its\nfunction. For example, anaphoric “it” may be\ntranslated with the third-person singular pronouns\n“il” [masc.] and “elle” [fem.], or with an non-\ngendered demonstrative such as “cela”. The\nFrench pronoun “ce” may function as both an\nevent reference and a pleonastic pronoun, but “il”\nis used only as a pleonastic pronoun.\nAs revealed in an analysis of the systems sub-\nmitted to the DiscoMT 2015 shared task on pro-\nnoun translation (Hardmeier et al., 2015a), the\ntranslation of pleonastic and event reference pro-\nnouns poses a particular problem for MT systems\n(Guillou and Hardmeier, 2016). Poor performance\nmay be attributed to the inability of the systems to\ndisambiguate the various possible functions of the\npronoun “it”. In the case of systems that incorpo-\nrate coreference resolution and methods for iden-\ntifying instances of pleonastic “it”, inaccurate out-\nput may harm translation performance. No suit-\nable tools exist for the detection of event reference\npronouns in English.\nTo address the problem of disambiguating the\nfunction of “it”, we propose a classiﬁer that uses\ninformation from the current and previous sen-\ntences, as well as external tools, and indicates for\neach instance of “it” whether the pronoun func-\ntion is anaphoric, pleonastic or event reference.\nThe classiﬁer was trained using data from the Par-\nCor corpus (Guillou et al., 2014) and the Dis-\ncoMT2015.test dataset (Hardmeier et al., 2016).\nIn both corpora, pronouns are labelled accord-\ning to their function, following the ParCor an-\nnotation scheme. The classiﬁer is incorporated\n581\nin an extended baseline system for the English-\nto-French task. The extended baseline takes the\nform of a n-gram language model that operates\nover target-language lemmas, but also has access\nto the identity of the source-language pronouns.\nSource-aware language models are also provided\nfor the other tasks: English-to-German, German-\nto-English and French-to-English.\n2 Previous Work\nWork on pronoun translation, in which a com-\nplete machine translation pipeline is provided, has\nalso considered different functions of the pronoun\n“it”. Le Nagard and Koehn (2010) identify and ex-\nclude instances of pleonastic “it” in their English-\nto-French system. Guillou (2015) distinguishes\nbetween anaphoric vs. non-anaphoric pronouns in\nan English-to-French automatic post-editing sys-\ntem. Nov ´ak et al. (2013) consider the transla-\ntion of three different uses of “it” in English-\nto-Czech translation: referential it , referring to\na noun phrase, anaphoric it , referring to a verb\nphrase, and pleonastic it. These three categories\ncorrespond to those that we refer to as anaphoric,\nevent reference and pleonastic, respectively.\nWork by Navarretta (2004) and Dipper et al.\n(2011) has focused on resolving abstract anaphora\nin Danish and on the manual annotation of ab-\nstract anaphora in English and German. Abstract\nanaphora, in which pronouns refer to abstract en-\ntities such as facts or events, is referred to as event\nreference in this paper. The automatic detection of\ninstances of pleonastic “it” has been addressed by\nNADA (Bergsma and Yarowsky, 2011), and also\nby the Stanford sieve-based coreference resolution\nsystem (Lee et al., 2011).\nThe cross-lingual pronoun prediction task for-\nmalised by Hardmeier (2014) was ﬁrst introduced\nas a shared task at DiscoMT 2015 (Hardmeier et\nal., 2015a). The participants used a range of fea-\ntures in their classiﬁers, but this paper marks the\nﬁrst attempt to incorporate a component to disam-\nbiguate the various uses of “it”.\n3 Disambiguating “it”\n3.1 Data\nThe ParCor corpus (Guillou et al., 2014) and Dis-\ncoMT2015.test dataset (Hardmeier et al., 2016)\nwere used to train the classiﬁer. Under the Par-\nCor annotation scheme, which was used to anno-\ntate both corpora, pronouns are labelled accord-\ning to their function. For all instances of “it” la-\nbelled as anaphoric, pleonastic or event reference,\nthe sentence-internal position of the pronoun and\nthe sentence itself are extracted 1. The pronouns\n“this” and “that”, when used as event reference\npronouns, may in many cases be used interchange-\nably with the pronoun “it” (Guillou, 2016). Con-\nsider Ex. 1, in which the pronouns “this” and “it”\nmay be used to express the same meaning.\n(1) John arrived late. [This/it] annoyed Mary.\nTo increase the number of training examples,\ninstances of event reference “this” and “that” are\nreplaced with “it” and added to the training data.\nThe data was divided into 1504 instances for\ntraining, and 501 each for the development and\ntest sets. All sentences were shufﬂed before the\ncorpus was divided, promoting a balanced distri-\nbution of the classes (Table 1).\nData it-\nSet Event Anaphoric Pleonastic Total\nTraining 504 779 221 1504\nDev 157 252 92 501\nTest 169 270 62 501\nTotal 830 1301 375 2506\nTable 1: Distribution of classes in the training data\nAll classiﬁers were trained using the Stanford\nMaximum Entropy package (Manning and Klein,\n2003).\n3.2 Features\nTo parse the corpus, we used the joint part-of-\nspeech tagger and dependency parser of Bohnet\net al. (2013) from the Mate toolkit. We used the\npre-trained models for English that are available\nonline2. In addition, the corpus was lemmatised\nusing the TreeTagger lemmatiser (Schmid, 1994).\nAlthough other tools were used, we relied on the\noutput of these two parsers to extract most of our\nfeatures.\nFor each training example, we extract the fol-\nlowing information:\n1. Previous three tokens. This includes words\nand punctuation. It also includes the tokens\nin the previous sentence when theit- occupies\nthe ﬁrst position of the current sentence.\n1A small number of instances of “it” are labelled as cat-\naphoric or extra-textual in the corpora. These are excluded\nfrom the classiﬁer training data.\n2https://code.google.com/p/mate-tools/downloads/list\n582\n2. Next two tokens\n3. Lemmas of the next two tokens\n4. Head word. As the task is limited to subject\nit and they, most of the time the head word is\na verb.\n5. Whether the head word takes a ‘that’ comple-\nment (verbs only)\n6. Tense of head word (verbs only). This\nis computed using the rules described in\nLo´aiciga et al. (2014).\n7. Presence of ‘that’ complement in previous\nsentence. A binary feature which follows\nNavarretta (2004)’s conclusion (for Dan-\nish) that a particular demonstrative pronoun\n(dette) is often used to refer to the last men-\ntioned situation in the previous sentence, of-\nten expressed in a subordinated clause.\n8. Predications head. This refers to the pred-\nicative complements of the verbs be, appear,\nseem, look, sound, smell, taste, feel, become\nand get.\n9. Closest noun phrase (head) to the left\n10. Closest noun phrase (head) to the right\n11. Presence of a cleft construction. A binary\nfeature which refers to constructions con-\ntaining adjectives which trigger extraposed\nsentential subjects as in ‘ So it ’s difﬁcult\nto attack malaria from inside malarious soci-\neties, [...].\n12. Closest adjective to the right\n13. VerbNet selectional restrictions of the verb.\nVerbNet (Kipper et al., 2008) speciﬁes 36\ntypes of argument that verbs can take. We\nlimited ourselves to the values of ‘abstract’,\n‘concrete’ and ‘unknown’.\n14. Lemma of the head word\n15. Likelihood of head word taking an event sub-\nject (verbs only). An estimate of the like-\nlihood of a verb taking a event subject was\ncomputed over the Annotated English Giga-\nword v.5 corpus (Napoles et al., 2012). We\nconsidered two cases where an event subject\nappears often and may be identiﬁed by ex-\nploiting the parse annotation of the Gigaword\ncorpus. The ﬁrst case is when the subject is\na gerund and the second case is composed of\n“this” pronoun subjects.\n16. NADA probability. The probability that\nthe non-referential “it” detector, NADA\n(Bergsma and Yarowsky, 2011), assigns to\nthe instance of “it”.\nWe also experimented with other features and\noptions. For features 2 and 3, a window of three\ntokens showed a degradation in performance. For\nfeatures 9 and 10, we experimented with adding\ntheir WordNet type (WordNet (Princeton Univer-\nsity, 2010) contains 26 types of nouns), but this\nhad no effect. The feature combination of noun\nand adjectives to the left or right also had no ef-\nfect.\n3.3 Results\nFor development and comparison we built two dif-\nferent baselines. One is a 3-gram language model\nbuilt using KenLM (Heaﬁeld, 2011) and trained\nover a modiﬁed version of the annotated corpus in\nwhich every it is concatenated with its type (e.g.\nit event). For testing, the it position is ﬁlled with\neach of the threeit label and the language model is\nqueried. This baseline functions in a very similar\nway to the share-task own baseline.\nTable 2 presents the results of this baseline us-\ning 14-fold cross-validation and a single held-out\ntest set (all test-set mentions refer to the same test\nset). The motivation for the choice of the num-\nber of folds is threefold. First, we wanted to re-\nspect document boundaries; second, we aimed for\na fair proportion of the three classes in all folds;\nand, lastly, we tried to lessen the variance given\nthe relatively small size of the corpus. The second\nbaseline is a setting in which all instances of the\ntest set are set to the majority class it-anaphoric.\nA quick scan of Tables 2 and 3 anticipates one\nof the conclusions of this paper: predicting event\nreference pronouns is a complex problem. The\n3-gram baseline appears to be biased towards the\npleonastic class, as suggested by its high precision\nand very low recall for the event and anaphoric\nclasses and the opposite situation for the pleonas-\ntic class. While our own classiﬁer is more bal-\nanced, it achieves only moderate results with the\nevent class. Compared to both of the baselines, it\nshows only a very small improvement.\n583\n14-fold cross-validation\nPrecision Recall F1\nit- anaphoric 0.5985 0.2475 0.3502\nit- pleonastic 0.1521 0.6213 0.2444\nit- event 0.5275 0.2772 0.3633\nTest-set\nPrecision Recall F1\nit- anaphoric 0.7320 0.2629 0.3869\nit- pleonastic 0.1387 0.6935 0.2312\nit- event 0.5213 0.2899 0.3726\nTest-set majority class\nPrecision Recall F1\nit- anaphoric 0.5389 1 0.7004\nTable 2: Baselines for the classiﬁcation of the\nthree types of it.\nA manual inspection of the results shows that\ndiscriminating between anaphoric and event refer-\nence instances of it is indeed a very subtle process.\nDetermining the presence or the lack of a speciﬁc\n(np-like) antecedent requires the understanding of\nthe complete coreference chain. Take for instance\nthe following example taken from a dialogue in the\ncorpus:\n1You’re part of a generation that grew up\nwith the Internet, and it seems as if you\nbecome offended at almost a visceral\nlevel when you see something done that\nyou think will harm the Internet. 2Is\nthere some truth to it? 3It is. 4I think\nit’s very true. 5This is not a left or right\nissue. 6Our basic freedoms, and when I\nsay our, I don’t just mean Americans, I\nmean people around the world, it’s not a\npartisan issue .\nIn the example above the ﬁrst italicised it is\nan event reference pronoun while the second is\nan anaphoric pronoun. With access to the whole\ncoreference chain, one can see that the it in sen-\ntence 3 refers to the event expressed in the ﬁrst\nsentence, therefore it is annotated as an event. This\nsame entity is then referred to with the word is-\nsue in sentence 5, which in turn becomes the an-\ntecedent to the it in sentence 6. The classiﬁer,\nhowever, labelled these two instances as anaphoric\nand event respectively.\nIt is worth noting that from the 2031 segments\ncomposing the annotated corpus, 349 (17%) con-\ntain co-occurrences of between 2 and 7it pronouns\nwithin the same segment. We experimented in-\ncluding the previous it-label, when there are sev-\neral within the same sentence, as an additional fea-\nture and obtained important gains in performance.\nIt can be seen in the w/ oracle feature section of\nTable 3 that performance improves in almost all\ncases when this feature is used. The only excep-\ntion is for the it-pleonastic class of the test set.\nWe then tried to approximate this feature by us-\ning the relative position of the it-label to other it-\nlabels within the same sentence (e.g., ﬁrst, second,\netc.). Contrary to the oracle feature, the approx-\nimated feature did not lead to any improvement.\nModelling co-occurrences of pronouns seems like\na promising step in future work.\nBinary classiﬁcation (event vs. non-event) con-\nsistently underperformed when compared to the\nthree class set-up.\n4 Source-Aware Language Model\nThe pronoun prediction part of our models is\nbased on an n-gram model over target lemmas\nsimilar to the ofﬁcial shared task baseline. In ad-\ndition to the pure target lemma context, our model\nalso has access to the identity of the source lan-\nguage pronoun, which, in the absence of number\ninﬂection on the target words, provides valuable\ninformation about the number marking of the pro-\nnouns in the source and opens a way to inject the\noutput of the pronoun type classiﬁer into the sys-\ntem.\nOur source-aware language model is an n-gram\nmodel trained on an artiﬁcial corpus generated\nfrom the target lemmas of the parallel training data\n(Figure 2). Before every REPLACE tag occurring\nin the data, we insert the source pronoun aligned\nto the tag (without lowercasing or any other pro-\ncessing). The alignment information attached to\nthe REPLACE tag in the shared task data ﬁles is\nstripped off. In the training data, we instead add\nthe pronoun class to be predicted. Note that allRE-\nPLACE tags are placeholders for one word trans-\nlations guaranteed to correspond to a source pro-\nnoun it or they according to the shared-task data\npreparation (Hardmeier et al., 2015b; Guillou et\nal., 2016). The n-gram model used for this compo-\nnent is a 6-gram model with modiﬁed Kneser-Ney\nsmoothing (Chen and Goodman, 1998) trained\n584\nDev Test\nw/o oracle feature Accuracy Precision Recall F1 Accuracy Precision Recall F1\nit- anaphoric 0.703 0.685 0.758 0.719 0.707 0.716 0.756 0.735\nit- pleonastic 0.884 0.758 0.543 0.633 0.936 0.750 0.726 0.738\nit- event 0.715 0.545 0.541 0.543 0.703 0.564 0.521 0.542\nw/ oracle feature Accuracy Precision Recall F1 Accuracy Precision Recall F1\nit- anaphoric 0.725 0.705 0.778 0.740 0.727 0.729 0.785 0.756\nit- pleonastic 0.886 0.746 0.576 0.650 0.926 0.705 0.694 0.699\nit- event 0.739 0.586 0.567 0.576 0.729 0.611 0.538 0.572\nTable 3: Classiﬁcation results of the three types of it on the development and test sets.\nSource: It ’s got these ﬁshing lures on the bottom .\nTarget lemmas: REPLACE 0 avoir ce leurre de pˆeche au-dessous .\nSolution: ils\nLM training data: It REPLACE ils avoir ce leurre de pˆeche au-dessous .\nLM test data: It REPLACE avoir ce leurre de pˆeche au-dessous .\nFigure 2: Data for the source-aware language model\nwith the KenLM toolkit (Heaﬁeld, 2011).\nTo predict classes for an unseen test set, we\nﬁrst convert it to a format matching that of the\ntraining data, but with a uniform, unannotated RE-\nPLACE tag used for all classes. We then recover\nthe tag annotated with the correct solution using\nthe disambig tool of the SRILM language mod-\nelling toolkit (Stolcke et al., 2011). This tool runs\nthe Viterbi algorithm to select the most probable\nmapping of each token from among a set of pos-\nsible alternatives. The map used for this task triv-\nially maps all tokens to themselves with the ex-\nception of the REPLACE tags, which are mapped\nto the set of annotated REPLACE tags found in the\ntraining data.\nThe source-aware language model described\nhere is identical to the language model component\nincluded in the UU-Hardmeier submission (Hard-\nmeier, 2016).\n5 English-French “it” Disambiguation\nSystem\nWe used the classiﬁer described in Section 3 to an-\nnotate all instances of it from the source side of\nthe data which were mapped to a REPLACE item\naccording to the alignment provided. Afterwards,\na new source-aware language model is trained in\nthe manner described in Section 4. In this way, in-\nstead of the sentence ‘It ’s got these ﬁshing lures\non the bottom . ’ presented in Figure 2, the sys-\ntem receives the labelled input‘It anaphoric ’s got\nthese ﬁshing lures on the bottom . ’All the data pro-\nvided for the shared-task was used in training this\nsystem.\n6 Results and Analysis\nUnfortunately, following the submission of our\nsystem we identiﬁed an error related to the fea-\nture extraction process. We relied on contextual\ninformation of the previous sentence for some of\nour features. However, due to the 1 : N align-\nments, the context information was sometimes in-\naccurate. The correction of this problem produced\nthe results reported in the section titled Submitted\ncorrected in Table 4. The macro-averaged recall\nobtained is 57.03%, which is considerably better\nthan the result of the submitted system (48.92%),\nbut still slightly lower than the score of 59.84%\nwhich was obtained by the unmodiﬁed system.\nHowever, some pronouns present better scores\nusing the submitted corrected system than the un-\nmodiﬁed system. Precision, in particular, is higher\n(bolded scores in Table 4). This outcome is ex-\npected for the pronoun cela, which is the French\nneuter demonstrative pronoun frequently used for\nevent reference. However, there are also gains in\nprecision for on, elles and ils. In our opinion,\nthis suggests that while not directly treating any of\nthe other source-language pronouns (in the context\nof this shared-task, other source pronouns refers\nonly to they), the disambiguation of it positively\naffects the translation of the other target-language\n585\nSubmitted - w/o labels R: 59.84%\nPronoun Precision Recall F1\nce 89.66 76.47 82.54\nelle 40.00 60.87 48.28\nelles 27.27 12.00 16.67\nil 63.24 70.49 66.67\nils 67.82 83.10 74.68\ncela 76.47 41.94 54.17\non 36.36 44.44 40.00\nOTHER 88.37 89.41 88.89\nSubmitted - w/ labels R: 48.92%\nPronoun Precision Recall F1\nce 70.11 89.71 78.71\nelle 0.00 0.00 0.00\nelles 20.00 16.00 17.78\nil 70.97 36.07 47.83\nils 50.96 74.65 60.57\ncela 48.65 58.06 52.94\non 42.86 33.33 37.50\nOTHER 86.59 83.53 85.03\nSubmitted corrected - w/ labels R: 57.03%\nPronoun Precision Recall F1\nce 89.09 72.06 79.67\nelle 31.25 43.48 36.36\nelles 30.77 16.00 21.05\nil 54.43 70.49 61.43\nils 69.41 83.10 75.64\ncela 86.67 41.94 56.52\non 40.00 44.44 42.11\nOTHER 85.71 84.71 85.21\nTable 4: Final system\npronouns. The pronoun it, after all, is used three\ntimes more frequently thanthey in the training data\n(Lo´aiciga and Wehrli, 2015).\nLooking at the predictions, we conﬁrmed that\nboth source-aware language models produced\nidentical results almost all of the time, with the\nsystem without the labels producing more correct\npredictions in total. However, there are some few\nexamples where the system with the labels outper-\nforms both the baseline and the un-labelled one. A\ncontrastive example can be seen in Figure 3.\n7 Conclusions and Future Work\nDistinguishing between anaphoric and event refer-\nence realisations of “it” is a very complex task. In\nSource: it anaphoric just takes a pic-\nture of objective reality as\nit anaphoric is .\nLM w/o labels: il OTHER\nLM w/labels: elle OTHER\nBaseline: cela OTHER\nGold elle prendre juste un image\nobjectif de la r´ealit´e .\nFigure 3: Examples of predictions of the ﬁnal sys-\ntems. The Gold translation is lemmatized.\nparticular, it can be difﬁcult to determine the an-\ntecedent of an event reference pronoun. The iden-\ntiﬁcation of pleonastic realisations, on the other\nhand, is almost impossible in an n-gram context\nsuch as that provided by a language model. How-\never, it is feasible in the three class setting, and\nat the same time helpful for the disambiguation of\nthe event and anaphoric realisations.\nWhile our results are modest, they point towards\nan improvement in the general quality of pronoun\ntranslation. Accurate disambiguation of the pro-\nnoun “it” has the potential to help NLP applica-\ntions such as Machine Translation and Corefer-\nence Resolution.\nIn the near future, we will experiment with other\nclassiﬁcation algorithms suitable for small training\nsets. We also intend to experiment with features\nthat incorporate semantic knowledge in the form\nof statistics computed over external resources, in-\ncluding the Gigaword corpus. Last, with the gen-\nerated data from this shared-task, we plan to do\nbootstrap and experiment with self-training.\nAcknowledgments\nSL was supported by the Swiss National Sci-\nence Foundation under grant no. P1GEP1161877.\nCH and LG were supported by the Swedish Re-\nsearch Council under project 2012-916Discourse-\nOriented Statistical Machine Translation. Large-\nscale computations were performed on the Abel\ncluster, owned by the University of Oslo and\nthe Norwegian metacenter for High Performance\nComputing (NOTUR), under project nn9106k.\n586\nReferences\nShane Bergsma and David Yarowsky. 2011. NADA:\nA robust system for non-referential pronoun detec-\ntion. In Iris Hendrickx, Sobha Lalitha Devi, Ant´onio\nBranco, and Ruslan Mitkov, editors, Anaphora Pro-\ncessing and Applications: 8th Discourse Anaphora\nand Anaphor Resolution Colloquium (DAARC) ,\nLecture Notes in Artiﬁcial Intelligence, pages 12–\n23. Springer, Faro, Portugal.\nBernd Bohnet, Joakim Nivre, Igor Boguslavsky,\nRich´ard Farkas, Filip Ginter, and Jan Haji ˇc. 2013.\nJoint morphological and syntactic analysis for richly\ninﬂected languages. Transactions of the Association\nfor Computational Linguistics, 1:415–428.\nStanley F. Chen and Joshua Goodman. 1998. An em-\npirical study of smoothing techniques for language\nmodeling. Technical report, Computer Science\nGroup, Harvard University, Cambridge (Mass.).\nStefanie Dipper, Christine Rieger, Melanie Seiss, and\nHeike Zinsmeister. 2011. Abstract anaphors in ger-\nman and english. In Iris Hendrickx, Sobha Lalitha\nDevi, Ant ´onio Branco, and Ruslan Mitkov, editors,\nAnaphora Processing and Applications: 8th Dis-\ncourse Anaphora and Anaphor Resolution Collo-\nquium (DAARC), Lecture Notes in Artiﬁcial Intel-\nligence, pages 96–107. Springer, Faro, Portugal.\nLiane Guillou and Christian Hardmeier. 2016.\nPROTEST: A test suite for evaluating pronouns in\nmachine translation. In Proceedings of the Eleventh\nLanguage Resources and Evaluation Conference\n(LREC’16), Portoroˇz (Slovenia), May.\nLiane Guillou, Christian Hardmeier, Aaron Smith, J¨org\nTiedemann, and Bonnie Webber. 2014. ParCor\n1.0: A parallel pronoun-coreference corpus to sup-\nport statistical MT. In Proceedings of the Tenth\nLanguage Resources and Evaluation Conference\n(LREC’14), pages 3191–3198, Reykjav´ık (Iceland).\nLiane Guillou, Christian Hardmeier, Preslav Nakov,\nSara Stymne, J ¨org Tiedemann, Yannick Vers-\nley, Mauro Cettolo, Bonnie Webber, and Andrei\nPopescu-Belis. 2016. Findings of the 2016 WMT\nshared task on cross-lingual pronoun prediction. In\nProceedings of the First Conference on Machine\nTranslation (WMT16) , Berlin, Germany. Associa-\ntion for Computational Linguistics.\nLiane Guillou. 2015. Automatic Post-Editing for the\nDiscoMT Pronoun Translation Task. InProceedings\nof the Second Workshop on Discourse in Machine\nTranslation, pages 65–71, Lisbon, Portugal. Associ-\nation for Computational Linguistics.\nLiane Guillou. 2016. Incorporating Pronoun Function\ninto Statistical Machine Translation . Ph.D. thesis,\nUniversity of Edinburgh.\nChristian Hardmeier, Preslav Nakov, Sara Stymne, J¨org\nTiedemann, Yannick Versley, and Mauro Cettolo.\n2015a. Pronoun-focused MT and cross-lingual pro-\nnoun prediction: Findings of the 2015 DiscoMT\nshared task on pronoun translation. In Proceed-\nings of the 2nd Workshop on Discourse in Machine\nTranslation (DiscoMT 2015) , pages 1–16, Lisbon\n(Portugal).\nChristian Hardmeier, Preslav Nakov, Sara Stymne, J¨org\nTiedemann, Yannick Versley, and Mauro Cettolo.\n2015b. Pronoun-focused MT and cross-lingual pro-\nnoun prediction: Findings of the 2015 DiscoMT\nshared task on pronoun translation. In Proceedings\nof the Second Workshop on Discourse in Machine\nTranslation, DiscoMT 2015, Lisbon, Portugal.\nChristian Hardmeier, J ¨org Tiedemann, Preslav Nakov,\nSara Stymne, and Yannick Versely. 2016. Dis-\ncoMT 2015 Shared Task on Pronoun Transla-\ntion. LINDAT/CLARIN digital library at Institute\nof Formal and Applied Linguistics, Charles Uni-\nversity in Prague. http://hdl.handle.net/\n11372/LRT-1611.\nChristian Hardmeier. 2014. Discourse in Statistical\nMachine Translation. Ph.D. thesis, University of\nUppsala.\nChristian Hardmeier. 2016. Pronoun prediction with\nlatent anaphora resolution. In Proceedings of the\nFirst Conference on Machine Translation (WMT) ,\nBerlin (Germany).\nKenneth Heaﬁeld. 2011. KenLM: faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187–197, Edinburgh (Scotland, UK), July. Associa-\ntion for Computational Linguistics.\nKarin Kipper, Anna Korhonen, Neville Ryant, and\nMartha Palmer. 2008. A large-scale classiﬁcation\nof english verbs. Language Resources and Evalua-\ntion Journal, 42(1):21–40.\nRonan Le Nagard and Philipp Koehn. 2010. Aiding\nPronoun Translation with Co-Reference Resolution.\nIn Proceedings of the Joint Fifth Workshop on Statis-\ntical Machine Translation and MetricsMATR, pages\n252–261, Uppsala, Sweden. Association for Com-\nputational Linguistics.\nHeeyoung Lee, Yves Peirsman, Angel Chang,\nNathanael Chambers, Mihai Surdeanu, and Dan Ju-\nrafsky. 2011. Stanford’s Multi-Pass Sieve Corefer-\nence Resolution System at the CoNLL-2011 Shared\nTask. In Proceedings of the Fifteenth Conference on\nComputational Natural Language Learning: Shared\nTask, CONLL Shared Task ’11, pages 28–34, Port-\nland, Oregon. Association for Computational Lin-\nguistics.\nSharid Lo ´aiciga and ´Eric Wehrli. 2015. Rule-based\npronominal anaphora treatment for machine trans-\nlation. In Proceedings of the Second Workshop on\nDiscourse in Machine Translation, DiscoMT 2015,\nLisbon, Portugal.\n587\nSharid Lo´aiciga, Thomas Meyer, and Andrei Popescu-\nBelis. 2014. English-french verb phrase align-\nment in europarl for tense translation modeling.\nIn Proceedings of the 9th International Conference\non Language Resources and Evaluation, LREC’14,\npages 674–681, Reykjavik, Iceland. European Lan-\nguage Resources Association (ELRA).\nChristopher Manning and Dan Klein. 2003. Opti-\nmization, MaxEnt models, and conditional estima-\ntion without magic. In Tutorial at HLT-NAACL and\n41st ACL conferences, Edmonton, Canada and Sap-\nporo, Japan.\nCourtney Napoles, Matthew Gormley, and Benjamin\nVan Durme. 2012. Annotated gigaword. In Pro-\nceedings of the Joint Workshop on Automatic Knowl-\nedge Base Construction and Web-scale Knowledge\nExtraction, AKBC-WEKEX, pages 95–100, Mon-\ntreal, Canada. Association for Computational Lin-\nguistics.\nCostanza Navarretta. 2004. Resolving individ-\nual and abstract anaphora in texts and dialogues.\nIn Proceedings of the 20th International Confer-\nence on Computational Linguistics , COLING’04,\npages 233–239, Geneva, Switzerland. Association\nfor Computational Linguistics.\nMichal Nov ´ak, Anna Nedoluzhko, and Zden ˇeek\nˇZabokrtsk´y. 2013. Translation of “it” in a deep\nsyntax framework. In Proceedings of the Workshop\non Discourse in Machine Translation, pages 51–59,\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nPrinceton University. 2010. Wordnet.\nHelmut Schmid. 1994. Probabilistic part-of-speech\ntagging using decision trees. In Proceedings of\nInternational Conference on New Methods in Lan-\nguage Processing, Manchester, UK.\nAndreas Stolcke, Jing Zheng, Wen Wang, and Vic-\ntor Abrash. 2011. SRILM at sixteen: Update and\noutlook. In Proceedings of the IEEE Automatic\nSpeech Recognition and Understanding Workshop ,\nWaikoloa (Hawaii, USA).\n588"
}