{
  "title": "A foundational vision transformer improves diagnostic performance for electrocardiograms",
  "url": "https://openalex.org/W4379537722",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2502250637",
      "name": "Akhil Vaid",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2569178240",
      "name": "Joy Jiang",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2135575922",
      "name": "Ashwin Sawant",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1706690306",
      "name": "Stamatios Lerakis",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1915356598",
      "name": "Edgar Argulian",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2921312072",
      "name": "Yuri Ahuja",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2676053596",
      "name": "Joshua Lampert",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A4212339766",
      "name": "Alexander Charney",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2005450025",
      "name": "Hayit Greenspan",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2132513323",
      "name": "Jagat Narula",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A4225141478",
      "name": "Benjamin Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2170298636",
      "name": "GIRISH N. NADKARNI",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2502250637",
      "name": "Akhil Vaid",
      "affiliations": [
        "Hasso Plattner Institute",
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2569178240",
      "name": "Joy Jiang",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2135575922",
      "name": "Ashwin Sawant",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1706690306",
      "name": "Stamatios Lerakis",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1915356598",
      "name": "Edgar Argulian",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2921312072",
      "name": "Yuri Ahuja",
      "affiliations": [
        "NYU Langone Health"
      ]
    },
    {
      "id": "https://openalex.org/A2676053596",
      "name": "Joshua Lampert",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A4212339766",
      "name": "Alexander Charney",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2005450025",
      "name": "Hayit Greenspan",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2132513323",
      "name": "Jagat Narula",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A4225141478",
      "name": "Benjamin Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai",
        "Hasso Plattner Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2170298636",
      "name": "GIRISH N. NADKARNI",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai",
        "Hasso Plattner Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2081291025",
    "https://openalex.org/W4281727995",
    "https://openalex.org/W3140683790",
    "https://openalex.org/W4320725812",
    "https://openalex.org/W2968959539",
    "https://openalex.org/W3201016655",
    "https://openalex.org/W3033523981",
    "https://openalex.org/W2076118331",
    "https://openalex.org/W3140854437",
    "https://openalex.org/W2962949934",
    "https://openalex.org/W3135891860",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W3151922143",
    "https://openalex.org/W2915445509",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2794730358",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2328732220",
    "https://openalex.org/W2002645541",
    "https://openalex.org/W3133511216",
    "https://openalex.org/W2037450062",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2143668817",
    "https://openalex.org/W2526468814",
    "https://openalex.org/W3207866969"
  ],
  "abstract": "Abstract The electrocardiogram (ECG) is a ubiquitous diagnostic modality. Convolutional neural networks (CNNs) applied towards ECG analysis require large sample sizes, and transfer learning approaches for biomedical problems may result in suboptimal performance when pre-training is done on natural images. We leveraged masked image modeling to create a vision-based transformer model, HeartBEiT, for electrocardiogram waveform analysis. We pre-trained this model on 8.5 million ECGs and then compared performance vs. standard CNN architectures for diagnosis of hypertrophic cardiomyopathy, low left ventricular ejection fraction and ST elevation myocardial infarction using differing training sample sizes and independent validation datasets. We find that HeartBEiT has significantly higher performance at lower sample sizes compared to other models. We also find that HeartBEiT improves explainability of diagnosis by highlighting biologically relevant regions of the EKG vs. standard CNNs. Domain specific pre-trained transformer models may exceed the classification performance of models trained on natural images especially in very low data regimes. The combination of the architecture and such pre-training allows for more accurate, granular explainability of model predictions.",
  "full_text": "ARTICLE OPEN\nA foundational vision transformer improves diagnostic\nperformance for electrocardiograms\nAkhil Vaid 1,2,3,4 ✉, Joy Jiang1,2, Ashwin Sawant 5, Stamatios Lerakis6,7, Edgar Argulian6,7, Yuri Ahuja8, Joshua Lampert6,7,\nAlexander Charney 1,3,9,10, Hayit Greenspan11, Jagat Narula6,7, Benjamin Glicksberg 3,4 and Girish N Nadkarni 1,2,3,4,12\nThe electrocardiogram (ECG) is a ubiquitous diagnostic modality. Convolutional neural networks (CNNs) applied towards ECG\nanalysis require large sample sizes, and transfer learning approaches for biomedical problems may result in suboptimal\nperformance when pre-training is done on natural images. We leveraged masked image modeling to create a vision-based\ntransformer model, HeartBEiT, for electrocardiogram waveform analysis. We pre-trained this model on 8.5 million ECGs and then\ncompared performance vs. standard CNN architectures for diagnosis of hypertrophic cardiomyopathy, low left ventricular ejection\nfraction and ST elevation myocardial infarction using differing training sample sizes and independent validation datasets. Weﬁnd\nthat HeartBEiT has signiﬁcantly higher performance at lower sample sizes compared to other models. We alsoﬁnd that HeartBEiT\nimproves explainability of diagnosis by highlighting biologically relevant regions of the EKG vs. standard CNNs. Domain speciﬁc\npre-trained transformer models may exceed the classiﬁcation performance of models trained on natural images especially in very\nlow data regimes. The combination of the architecture and such pre-training allows for more accurate, granular explainability of\nmodel predictions.\nnpj Digital Medicine          (2023) 6:108 ; https://doi.org/10.1038/s41746-023-00840-9\nINTRODUCTION\nThe electrocardiogram (ECG) is a body surface-level recording of\nelectrical activity within the heart. Owing to its low cost, non-\ninvasiveness, and wide applicability to cardiac disease, the ECG is a\nubiquitous investigation and over 100 million ECGs are performed\neach year within the United States alone\n1 in various healthcare\nsettings. However, the ECG is limited in scope since physicians\ncannot consistently identify patterns representative of disease–\nespecially for conditions that do not have established diagnostic\ncriteria, or in cases when such patterns may be too subtle or\nchaotic for human interpretation.\nDeep learning has been applied to ECG data for several\ndiagnostic and prognostic use cases\n2– 6. The vast majority of this\nwork has been built upon Convolutional Neural Networks (CNNs)7.\nLike other neural networks, CNNs are high variance constructs8,\nand require large amounts of data to prevent overﬁtting9. CNNs\nmust also be purpose-built to accommodate the dimensionality of\nincoming data, and they have been used for interpreting ECGs\nboth as 1D waveforms and 2D images\n10.\nIn this context, interpreting ECGs as 2D images presents an\nadvantage due to widely available pre-trained models which often\nserve as starting points for modeling tasks on smaller datasets11.\nThis technique is described astransfer learning wherein a model\nthat is trained on a larger, possibly unrelated dataset isﬁne-tuned\non a smaller dataset that is relevant to a problem 12. Transfer\nlearning is especially useful in healthcare since datasets are limited\nin size due to limited patient cohorts, rarity of outcomes of\ninterest, and costs associated with generating useful labels. As a\nresult, vision models ﬁrst trained in a supervised manner on\nnatural images\n13 often form the basis of models used in\nhealthcare settings. Unfortunately, transfer learning with such\nnatural images is not a universal solution, and it is known to\nproduce suboptimal results when there exist substantial differ-\nences in the pre-training andﬁne-tuning datasets\n14.\nTransformer-based neural networks utilize the attention\nmechanism15 to establish and de ﬁne relationships between\ndiscrete units of input data known as tokens 16. A signi ﬁcant\nbeneﬁt that transformers allow for is unsupervised learning from\nlarge corpora of unlabeled data to learn relationships between\ntokens, and then utilize this information for other downstream\ntasks\n16. Due to the ease with which unstructured text can be\nbroken down into tokens, transformers have been tremendously\nsuccessful at Natural Language Processing (NLP) tasks17,18. Recent\nwork has extended the functionality of such models into vision-\nbased tasks, leading to the advent of the vision transformer\n16,19.\nThe ﬁrst vision transformers were pre-trained on immense\nlabeled datasets and then ﬁne-tuned on smaller datasets to\nindicate better performance over CNNs at natural image classiﬁca-\ntion20.M o r er e c e n t l y ,t h eBidirectional Encoder representation from\nImage Transformers (BEiT) approach has allowed large unlabeled\ndatasets to be leveraged for pre-training transformer neural\nnetworks\n21. This approach consists of converting parts of an input\nimage into discrete tokens or patches. Such tokens may be\nconsidered analogous to the words within a sentence and be used\nto pre-train a transformer in much the same way as a language\nmodel (Fig. 1). Since transformers consider global dependencies\n22\n1The Charles Bronfman Institute for Personalized Medicine, Icahn School of Medicine at Mount Sinai, New York, NY, USA.2Mount Sinai Clinical Intelligence Center, Icahn School of\nMedicine at Mount Sinai, New York, NY, USA.3Department of Genetics and Genomic Sciences, Icahn School of Medicine at Mount Sinai, New York, NY, USA.4The Hasso Plattner\nInstitute for Digital Health at Mount Sinai, New York, NY, USA.5Department of Medicine, Icahn School of Medicine at Mount Sinai, New York, NY, USA.6Mount Sinai Heart, Icahn\nSchool of Medicine at Mount Sinai, New York, NY, USA.7Department of Cardiology, Icahn School of Medicine at Mount Sinai, New York, NY, USA.8Department of Medicine, NYU\nLangone Health, New York, NY, USA.9The Pamela Sklar Division of Psychiatric Genomics, Icahn School of Medicine at Mount Sinai, New York, NY, USA.10Department of Psychiatry,\nIcahn School of Medicine at Mount Sinai, New York, NY, USA.11Department of Biomedical Engineering, Tel Aviv University, Tel Aviv 6997801, Israel.12Division of Nephrology,\nDepartment of Medicine, Icahn School of Medicine at Mount Sinai, New York, NY, USA.✉email: akhil.vaid@mssm.edu\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nbetween all features of provided inputs, such pre-training may be\nespecially advantageous for ECGs. Certain pathological patterns such\nas the S1Q3T3 occur in different parts of a recording23,a n dam o d e l\nwhich considers only contiguous regions may miss them entirely.\nWe create a vision transformer model pre-trained on a large\ncorpus of several million ECGs belonging to a diverse population.\nWe utilize this model to create specialized models for use cases\nwhere little data may be available. We then compare performance\nand saliency maps to baseline models subject to similar constraints.\nRESULTS\nPerformance at classiﬁcation of LVEF\nWe included 511,491 total ECGs from MSHS in the training orﬁne-\ntuning set, 20,448 samples from MSHS in testing, and 1,480 from\nMorningside in external validation. Low LVEF prevalence was 18%\nin the training set (Table1).\nHeartBEiT outperformed other CNN models at low LVEF\nclassiﬁcation at all fractions of training data (Fig.2;S u p p l e m e n t a r y\nTable 1). At 1% of training data (5114 samples), performance\n(AUROC: 0.86, 95% CI: 0.86– 0.86) was 28.4% better than the ViT-B/\n16 model (AUROC: 0.67, 95% CI 0.67 – 0.67), 5.2% better than\nEfﬁcientNet-B4 (AUROC: 0.82, 95% CI: 0.82– 0.82), and 2.4% better\nthan ResNet-152 (AUROC: 0.84, 95% CI: 0.84– 0.84) in internal testing\n(Supplementary Fig. 2). These trends were maintained across\nexternal validation with HeartBEiT (AUROC: 0.87, 95% CI: 0.87– 0.87)\noutperforming the CNNs by 4– 18% (Supplementary Fig. 3).\nUsing AUPRC as a metric, at 1% of training data and against a\nprevalence of 18.5% in the internal testing cohort, the HeartBEiT\nmodel (AUPRC: 0.59, 95% CI: 0.59– 0.59) outperformed ViT-B/16\n(AUPRC: 0.31, 95% CI 0.31– 0.31) by 90.3%, EfﬁcientNet-B4 (AUPRC:\n0.48, 95% CI: 0.48– 0.48) by 22.9% and the ResNet-152 (AUPRC:\n0.52, 95% CI: 0.52 – 0.52) by 13.5% (Supplementary Table 2,\nSupplementary Figs. 4 – 6). In the external validation cohort,\nHeartBEiT had the highest AUPRC of 0.73 (95% CI: 0.73– 0.73).\nWith 100% of the training data (511,491 samples), performance\nacross all models became more closely matched. In internal\ntesting, there was no performance differential among HeartBEiT,\nEfﬁcientNet-B4, and ResNet-152, and a differential of 1.1– 4.5% was\nobserved in external validation for AUROC. However, for AUPRC,\nFig. 1 Modeling workﬂow. Pre-training of the HeartBEiT model. (1) Each original ECG is partitioned into 14 × 14 patches (2) of 16 × 16 pixels.\nThese patches are tokenized, and some of them are masked (3). The Dall-E model (4) acts as the tokenizer and converts the image into discrete\ntokens (5) which are then made part of the Masked Image Modeling process (6). This allows for pre-training the HeartBEiT model’s attention\nmodules (7), and the model may then be used for downstreamﬁne-tuning and inference (8, 9) upon addition of a Multi-Layer Perceptron\nclassiﬁcation head (10).\nTable 1. Dataset Size with Outcome Prevalence.\nFine-tuning Testing External Validation\nLow LVEF\nNumber of ECGs (n) 511,491 128,687 1480\nOutcome Prevalence (%) 18.4 18.6 26.6\nHypertrophic Cardiomyopathy\nNumber of ECGs (n) 78,831 20,448 13,859\nOutcome Prevalence (%) 37.4 38.8 36.6\nSTEMI (PTB-XL database)\nNumber of ECGs (n) 17,449 4,352 -\nOutcome Prevalence (%) 5.7 5.4 -\nCells in bold typeface indicate an outcome.\nA. Vaid et al.\n2\nnpj Digital Medicine (2023)   108 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nHeartBEiT still had improved performance of 0-17.7% in internal\nand external datasets.\nGRAD-CAM analysis demonstrated areas around the QRS\ncomplexes of each lead were highlighted at 1% of training data\nby HeartBEiT (Supplementary Fig. 7a). When 100% of training data\nwere implemented, foci became more pronounced around the\nQRS complexes of lead I (Supplementary Fig. 7b).\nPerformance at diagnosis of HCM\nWe ﬁne-tuned the HeartBEiT transformer using 78,831 ECGs from\nfour hospitals of the MSHS. Testing was conducted on 20,448 ECGs\nfrom these hospitals, and 3,859 ECGs from a holdout set of\npatients from Morningside were used for external validation\n(Table 1). The prevalence of HCM in the training set was 38%.\nHeartBEiT outperformed the other models at diagnosis of HCM at\nall fractions of training data (Fig.3; Supplementary Table 1). At 1% of\ntraining data, performance of the HeartBEiT model at AUROC of 0.77\n(95% CI: 0.77– 0.77) exceeded that of ViT-B/16 by 26.2% and of\nEfﬁcientNet-B4 and ResNet-152 by 6.9% in internal testing\n(Supplementary Fig. 2). Similar results were seen for external\nvalidation with the HeartBEiT model which had an AUROC of 0.74\n(95% CI: 0.74– 0.74), outperforming ViT-B/16 (0.61, 95% CI 0.61– 0.61)\nby 21.3%, EfﬁcientNet-B4 (0.69, 95% CI: 0.68– 0.70) by 7.2%, and\nResNet-152 (0.68, 95% CI: 0.68– 0.69) by 8.8% (Supplementary Fig. 3).\nDifferences in performance were much more profound for\nAUPRC at 1% of training data in use (Supplementary Table 2;\nSupplementary Fig. 8). Using 1% of training data, against an\noutcome prevalence of 38.8% in the internal testing cohort, the\nHeartBEiT model (AUPRC: 0.67, 95%, CI: 0.67 – 0.67) exceeded\nperformance of ViT-B/16 (AUPRC: 0.49, 95% CI 0.49– 0.49) by 36.7%,\nEfﬁcientNet-B4 (AUPRC: 0.63, 95% CI: 0.63– 0.63) by 6.3% and the\nResNet-152 (AUPRC: 0.64, 95% CI: 0.64– 0.64) by 4.7% (Supple-\nmentary Fig. 5). In external validation, HeartBEiT continued to\nexhibit the best performance with AUPRC of 0.64 (95% CI:\n0.64– 0.64) (Supplementary Fig. 6).\nThe HeartBEiT performance advantage reduced gradually as the\namount of training data increased. Compared to 100% of the\ntraining data, the performance differential was up to 2.5% in\ninternal testing and 3.9% external validation for AUROC and up to\n4.2% and 7.1% for internal testing and external validation,\nrespectively, for AUPRC.\nGRAD-CAM analysis revealed that at 1% of the data, the QRS\ncomplexes of lead I, V2, and V5 and the ST segment of V6 were\ndenoted as important regions for predicting HCM by HeartBEiT\n(Supplementary Fig. 9a). In contrast, at 100% of the training data,\nkey areas identiﬁed by HeartBEiT became more focused to the\nbeginning of V5 (Supplementary Fig. 9b).\nPerformance at detection of STEMI\nThe PTB-XL dataset contains 21,799 total ECGs from 18,869\npatients: 17,449 ECGs were used forﬁne-tuning and 4352 to test\nthe model. The prevalence of STEMI was around 5.7% in the\ntraining set and 5.4% in the testing set (Table1).\nThe AUROC performance advantage of HeartBEiT was seen to\nbe greater at smaller fractions of training data used for training\n(Fig. 4; Supplementary Table 1). In internal testing, the AUROC of\nHeartBEiT was 0.88 (95% CI: 0.88– 0.89) with 4.8– 10% performance\nimprovement compared to the other models at 1% of training\ndata (Supplementary Fig. 2). This advantage changed to\napproximately 20.3%, 1.1%, and 2.2% in comparison to ViT-B/16,\nEfﬁcientNet-B4, and ResNet-152, respectively, when all available\ntraining data (17,449 samples) were used.\nThis performance advantage became much more profound for\nAUPRC, with HeartBEiT (AUPRC: 0.56, 95% CI 0.56 – 0.66) out-\nperforming ViT-B/16 (0.27, 95% CI 0.26–\n37) by 107.4%, ResNet-152\n(0.47, 95% CI 0.46– 0.47) by 19.1% and the EfﬁcientNet-B4 (0.40,\n95% CI 0.40– 0.41) by 40.0% at a 1% fraction of training data\n(Supplementary Table 2; Supplementary Fig. 5; Supplementary Fig.\n10). However, at 100% of training data, performance of HeartBEiT\nFig. 2 Left ventricular ejection fraction <= 40% classiﬁcation on ECGs. aInternal testing performance (4 Mount Sinai facilities).b Internal\ntesting performance difference.c External validation performance (Morningside patients).d External validation performance difference. Red\ndashed line in (b) and (d) indicates HeartBEiT performance.\nA. Vaid et al.\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   108 \n(AUPRC: 0.67, 95% CI 0.66– 0.67) became non-signiﬁcantly lower\nthan that of EfﬁcientNet-B4 (AUPRC: 0.68, 95% CI: 0.67– 0.68).\nFor STEMI detection, the ViT-B/16 vision transformer exhibited\ntraining instability when using more than 10% of training data\nwhile keeping other hyperparameters such as learning rate\nconstant. This instability was seen only for this outcome, and\nreported performance corresponds to best metrics achieved prior\nto the training methods erroring out.\nST segments in each lead were underscored as areas of\nimportance according to GRAD-CAM analysis of HeartBEiT at 1% of\nthe training data (Fig.5). At 100% of the training data, these areas\ndenoted by HeartBEiT became localized around ST segments of\nleads V3 and V4 (Supplementary Fig. 11).\nWasserstein distance\nThe average pairwise Wasserstein distance for the ECG vs ECG set\nwas 2.14. In comparison, this value was 45.48 for the ImageNet vs\nImageNet set, and 128.44 for the ECG vs ImageNet set\n(Supplementary Fig. 12).\nDISCUSSION\nUsing 8.5 million ECGs from 2.1 million patients collected over a\nperiod of four decades, we leveraged Masked Image Modeling to\ncreate a vision-based transformer (HeartBEiT) model for ECG data\nthat can act as a universal starting point for downstream training\non outcomes of interest. We ﬁne-tuned this model against two\noutcomes using data derived from four hospitals within the Mount\nSinai Health System, and externally validated derived models on\ndata from another hospital. We also ﬁne-tuned this model for\nSTEMI detection using data from the publicly available PTB-XL\ndatabase, followed by testing the derived model against a holdout\nset of patients. In each case, our model was compared against two\nCNNs and another vision transformer all subject to the same\ntraining conditions. Finally, we evaluated an additional aspect of\nFig. 4 STEMI detection on ECGs (PTB-XL database). aInternal testing performance.b Internal testing performance difference. Dashed red\nline in (b) indicates HeartBEiT performance.\nFig. 3 Hypertrophic cardiomyopathy classiﬁcation on ECGs. aInternal testing performance (4 Mount Sinai facilities).b Internal testing\nperformance difference.c External validation performance (Morningside patients).d External validation performance difference. Red dashed\nline in (b) and (d) indicates HeartBEiT performance.\nA. Vaid et al.\n4\nnpj Digital Medicine (2023)   108 Published in partnership with Seoul National University Bundang Hospital\nclinical usefulness of these models by creating saliency maps for\ninput samples.\nNeural network performance can be heavily inﬂuenced by the\namount of data available24, and overﬁtting can easily result in\nsmall data regimes25. However, curated labeled data is a scarce\nresource. This is especially true in the healthcare setting wherein\nperforming testing on patients, detecting pathologies of interest,\nand gathering data regarding clinical outcomes is laborious and\nexpensive. In addition to the ﬁnancial costs of acquiring and\nlabelling data, time may be an additional factor that precludes\nacquisition of larger datasets. During emergent public health\nconcerns, such as the recent COVID-19 pandemic, little data may\nbe available for the development of useful models. In such\ncircumstances, models that can work with a fraction of the data\nrequired for other approaches may assist in quicker, more\nappropriate diagnosis and triage.\nAcross all outcomes, datasets, and performance metrics, Heart-\nBEiT achieved equivalent performance with an order of magnitude\nless (100% vs 10%) training data.”. Further, in the very low data\nregime using only 1% of training data, HeartBEiT performance was\nequivalent to other models using 10 times as much data. This\nperformance was maintained in external validation not only for\nthe ﬁne-tuned models, but also for the pre-trained model when\nused with an altogether new dataset from an independent dataset\ncomprised of a geographically separated cohort of patients.\nOf special importance is the elevated difference in performance\nin the AUPRC– a better indicator of performance in datasets with\nheavy class imbalance wherein considering AUROC in isolation\nmay be less useful. Given relatively low event rates, medical\ndatasets tend to have such class imbalances. For example, in\ndetection of STEMI with an outcome prevalence of 5.6%, in the 1%\ntraining data regime, HeartBEiT exceeded the AUPRC of the CNNs\nby 19.1% and 40% respectively, while doubling the performance\nof the ImageNet vision transformer. These results also indicate\nthat pre-training on natural images isn’t always the most optimal\nsolution for creating healthcare related models – a fact further\nevidenced by the extent of the disparity in the average\nWasserstein distance between natural images and ECGs.\nAn emergent clinical advantage of using transformers with the\nexplainability framework described in this work is the granularity\nof the saliency mapping. Even at similar levels of performance, the\nCNNs shown tend to coalesce areas of importance, thereby\nobfuscating the strongest determinants of a prediction. In\ncomparison, saliency maps for transformers tend to focus on\nthese determinants. Such granular explainability may help both\nclinician adoption of deep learning models, as well as aid in\nunderstanding pathologies for which there are no diagnostic\nguidelines on an ECG. These factors are demonstrated well for\nSTEMI detection where the pathognomonic pattern is well\nestablished, and the ST segment is consistently highlighted even\nwhen using 1% of data forﬁne-tuning (Fig.5). In the case of LVEF\ndetermination, there exist no clear diagnostic guidelines that can\nassist human physicians. In this case, saliency maps tend to focus\non QRS complexes which indicate the net vector of depolarization\nof the majority of the cardiac ventricular musculature and point\nFig. 5 Saliency mapping for STEMI detection at 1% training data. aViT-B/16. b EfﬁcientNet-B4. c ResNet-152. d HeartBEiT. HeartBEiT\nlocalizes to the ST segments. Other models are more diffuse in highlighting features of importance and may be less useful clinically.\nA. Vaid et al.\n5\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   108 \ntowards the transformer’s ability to focus on the mechanisms\nunderlying the disease condition.\nOur work must be considered in light of certain limitations.\nTransformers tend to be very compute intensive to pre-train. We\nwere therefore limited in the size of the transformer model at 86 M\nparameters, as well as the dimensions of the input data we were\nable to utilize. However, we believe this work serves as evidence of\nthe viability and advantages of our HeartBEiT model, and future\nwork will deal with scaling up this model to enable better\nperformance prior to live deployment.\nIn conclusion, pre-trained transformer models enable robust\ndeep learning-based ECG classi ﬁcation even in severely data\nlimited regimes. More speciﬁc, better quality, granular saliency\nmaps can aid clinician acceptance of model predictions.\nMETHODS\nData sources\nWe utilized all available ECG data fromﬁve hospitals within the\nMount Sinai Health System (MSHS) to pre-train our model. These\nhospitals (Mount Sinai Hospital, Morningside, West, Beth Israel,\nand Brooklyn) serve a large patient population that is reﬂective of\nthe demographic diversity of New York City. ECG data were\nretrieved from the GE MUSE system for the years 1980 – 2021\ntotaling an approximate 8.5 million discrete ECG recordings for 2.1\nmillion patients. ECG data were obtained as structured XMLﬁles\ncontaining both raw waveforms as well as metadata associated\nwith patient identiﬁers, time, place, and indication.\nFor outcome speciﬁc ﬁne-tuning of the model, we collected\nground-truth labels for the value of the left ventricular ejection\nfraction (LVEF) from available echocardiogram reports. The\nmodeling task was classiﬁcation of patients for an LVEF ≤40%,\nwhich deﬁnes heart failure with reduced ejection fraction\n26.W e\nalso collected labels indicative of a diagnosis of Hypertrophic\nCardiomyopathy – a genetic disorder wherein the chambers of the\nheart undergo a pathological increase in thickness resulting in loss\nof cardiac function and predisposition to fatal arrhythmias. These\nlabels were generated using Natural Language Processing to parse\nunstructured echocardiogram reports for any mention of“HCM” /\n“Hypertrophic Cardiomyopathy”– with or without any intervening\nqualiﬁers regarding the obstructive nature of the pathology.\nFinally, we utilized the publicly available PTB-XL dataset for\nadditional external validation. This dataset contains 21,799 ECGs\nfrom 18,869 patients from October 1989 to June 1996. These data\nhave been annotated by two cardiologists and contain ground-\ntruth diagnostic labels such as whether an ECG is indicative of a\nnormal recording or changes suggestive of acute ischemia. ECG\nrecordings from this database were used toﬁne-tune models for\ndetection of ST-Elevation Myocardial Infarction (STEMI). STEMIs are\ncaused by acute loss of blood supply to heart tissue, and can result\nin a plethora of complications ranging from loss of contractile\nfunction to death.\nPreprocessing\nECGs utilized within this study each contain waveform data\nrecorded from one of twelve leads, with each lead representing a\ndifferent perspective on the heart ’s electrical activity. Both\ndatasets contain ECGs with either 5 or 10 s of waveform data\nper lead sampled at a rate of 500 Hz, for a total of 2500 or\n5000 samples. The MSHS dataset does not contain data regarding\nleads III, aVF, aVL, or aVR. However, these leads are derived since\nthey can be re-created from linear transformations of the vectors\nrepresenting the other leads. In order to maintain uniformity\nacross samples and datasets, all ECGs were truncated to\n2500 samples.\nWe corrected for noise within ECG recordings through\napplication of aButterworth bandpassﬁlter (0.5 Hz– 40 Hz) followed\nby the application of a median ﬁlter on raw waveform data.\nProcessed waveform data so derived was organized to maintain\norder of leads, and plotted to images with each image containing\na total of eight leads (I, II, and V1– V6). Images were saved in\nthe.png (Portable Network Graphics) format at a resolution of\n1000 × 1000 pixels to prevent compression artefacts. Additionally,\noutput images were stored with three channels of color to retain\ncompatibility with CNNs trained on ImageNet.\nTokens and tokenization\nTokens may be deﬁned as discrete pre-deﬁned sequences which\nare grouped and analyzed together on a semantic basis. In the\ncontext of language modeling, tokens may simply be the words\ncomprising a body of text. The process of separating out data into\nsuch discrete sequences and assigning unique numeric identiﬁers\nto them is referred to asTokenization\n27.\nMasked image modeling\nA method commonly used to pre-train language models is called\nMasked Language Modeling(MLM)28, wherein a set percentage of\nthe number of tokens input to the model are masked or hidden,\nand models are pre-trained by having them predict these masked\ntokens. Collection and labeling of data may be an expensive\nprocess, and such costs are ampli ﬁed for medical datasets. A\nsigniﬁcant advantage of MLM is that it allows for the usage of\nlarge quantities of unlabeled data to pre-train models.\nThe BEiT approach extends MLM intoMasked Image Modeling\n(MIM) wherein 2D input images are separated into patches\ncontaining raw pixels which are then converted to tokenized\nrepresentations of the input image (Fig. 1). This tokenization is\naccomplished using a separately trained image tokenizer that\napproximates each patch into a single numeric token. We used\nthe same publicly available image tokenizer (Dall-E) for conversion\nof ECG images as the original BEiT implementation.\nModel selection\nWe instantiated a 12-layer transformer model with a hidden layer\nsize of 768, and 12 attention heads for a total of approximately\n86 M parameters. This model, and its downstream derivatives are\nreferred to as“HeartBEiT” within the text of this work.\nWe compared the downstream problem-speciﬁc performance\nof this model to an equivalently sized ImageNet based vision\ntransformer (ViT-B/16: 86 M parameters), as well as CNN based\napproaches common to deep learning as applied to ECGs. These\ninclude the largest available pre-trained ResNet model (ResNet-\n152: 60 M parameters), and a computationally more inexpensive\narchitecture (EfﬁcientNet-B4: 19 M parameters) known to demon-\nstrate better performance at image classiﬁcation despite having\nfewer parameters. All baselines were pre-trained in a supervised\nmanner on the ImageNet1K dataset containing 1.2 M labeled\ntraining images.\nPre-training\nInput images were resized to 224 × 224 pixels, but otherwise\nsubject to no other pre-processing. As opposed to natural images,\nECG waveforms require maintenance of morphology and order.\nRandom to loss of information that may only exist within certain\nsegments of an ECG.\nInput images were split into square patches of 16 pixels each,\nfor a total of 196 patches per input image (Fig.5). 40% of the input\npatches were masked for input into the neural network. We used\nthe AdamW optimizer with a learning rate of 5e-4. The HeartBEiT\nmodel was pre-trained on a node consisting of 4 NVIDIA A100-40G\nGPUs. At approximately 6 h per epoch, pre-training the model for\n300 epochs took around 2.5 months. Model parameters saved at\nA. Vaid et al.\n6\nnpj Digital Medicine (2023)   108 Published in partnership with Seoul National University Bundang Hospital\nthe 300th epoch were used for downstreamﬁne-tuning in all cases\n(Supplementary Fig. 1).\nFine-tuning and statistical analysis\nPre-trained models were subjected to a ﬁne-tuning task to\ndemonstrate and compare performance at ECG based classiﬁca-\ntion. We used data from 4 hospitals for detection of LVEF of < 40%,\nand diagnosis of HCM. In either case, the performance of theﬁne-\ntuned model was externally validated on data from Morningside\nhospital. Data from the PTB-XL database were used toﬁne-tune\nthe pre-trained HeartBEiT model, as well as the other models for\ndetection of STEMI.\nData were separated into a training dataset, an internal testing\ndataset, and where applicable, an external validation dataset. We\nmodeled conditions of extreme data paucity by reducing training\ndata to either 1%, 10%, 25%, 50%, or 100%, and then testing\nresulting models against common testing data. In all cases,Group\nShufﬂe Splitting with a constant random seed was employed to\nensure no patients were present in both training and testing data,\nand that the same patients were part of either dataset across runs.\nWe set the classiﬁcation head of each model to a size of two\nneurons and utilizedCrossEntropy loss. The Adam optimizer on a\nOneCycle learning rate schedule between 3e-4 and 1e-3 over 30\nepochs was utilized for ﬁne-tuning and reported performance\nmetrics correspond to the best performance achieved across these\nepochs. Threshold independent Area Under the Receiver Operat-\ning Characteristic curve (AUROC) and Area Under the Precision\nRecall Curve (AUPRC) metrics were used to calculate and compare\nmodel performance. 95% conﬁdence intervals for areas under the\ncurve were generated through 500 iterations of the bootstrap.\nWasserstein distance\nThe Wasserstein distance\n29 is a metric of the cost required to\ntransform one distribution into another. Given two discrete\nimages, the magnitude of the Wasserstein distance between\nthem is directly proportional to how dissimilar they are. Higher\nWasserstein distances between pre-training andﬁne-tuning data\nmay lead to sub-optimal results with transfer learning.\nWe randomly sampled 1000 images each from both the\nImageNet and ECG datasets. All samples from within each cohort\nwere resized to 224 × 224 pixels and paired against all other\nsamples from the same cohort, as well as the other cohort for a\ntotal of 3 such combinations: ECG vs ECG, ECG vs ImageNet,\nImageNet vs ImageNet. Each such operation yielded a total of 10\n6\npairs. The Wasserstein distance was calculated for each resulting\npair of images and averaged across the combination of cohorts.\nExplainability\nModel explainability was generated using theGradient-weighted Class\nActivation Mapping(GradCAM) library30. Generated attributions were\nplotted as an overlay upon the original input image to demonstrate\nw h i c hp a r to fa ni n p u tc o n t r i b u t e dm o s tt oap r e d i c t i o n .\nSoftware\nAll analyses were performed using the pandas, numpy, Python\nImage Library (PIL), SciPy, scikit-learn, torchvision, timm, andPyTorch\nlibraries. Plotting was performed using thematplotlib and seaborn\nlibraries. All code was written for and within the 3.8.x version of\nthe Python programming language.\nReporting summary\nFurther information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nDATA AVAILABILITY\nMount Sinai data utilized in this study are not publicly available due to patient\nprivacy concerns. The PTB-XL dataset is publicly available for download at:https://\ndoi.org/10.13026/kfzx-aw45 The HeartBEiT model may be released to other\nresearchers on IRB-approved agreement with Mount Sinai Intellectual Partners.\nCODE AVAILABILITY\nModel creation code is not dataset speciﬁc, and is available at:https://github.com/\nakhilvaid/HeartBEiT.\nReceived: 13 January 2023; Accepted: 5 May 2023;\nREFERENCES\n1 .D r a z e n ,E . ,M a n n ,N . ,B o r u n ,R . ,L a k s ,M .&B e r s e n ,A .S u r v e yo fc o m p u t e r -\nassisted electrocardiography in the United States. J. Electrocardiol. 21,\nS98– S104 (1988).\n2. Vaid, A. et al. Automated Determination of Left Ventricular Function Using\nElectrocardiogram Data in Patients on Maintenance Hemodialysis.Clin. J. Am. Soc.\nNephrol. 17, 1017– 1025 (2022).\n3. Vaid, A. et al. Using deep-learning algorithms to simultaneously identify right and\nleft ventricular dysfunction from the electrocardiogram.Cardiovasc. Imaging 15,\n395– 410 (2022).\n4. Vaid, A. et al. Multi-center retrospective cohort study applying deep learning to\nelectrocardiograms to identify left heart valvular dysfunction.Commun. Med. 3,\n24 (2023).\n5. Mincholé, A., Camps, J., Lyon, A. & Rodríguez, B. Machine learning in the elec-\ntrocardiogram. J. Electrocardiol. 57, S61– S64 (2019).\n6. Aziz, S., Ahmed, S. & Alouini, M.-S. ECG-based machine-learning algorithms for\nheartbeat classiﬁcation. Sci. Rep. 11, 18738 (2021).\n7. Hong, S., Zhou, Y., Shang, J., Xiao, C. & Sun, J. Opportunities and challenges of\ndeep learning methods for electrocardiogram data: A systematic review.Com-\nputers Biol. Med.122, 103801 (2020).\n8. Geman, S., Bienenstock, E. & Doursat, R. Neural networks and the bias/variance\ndilemma. Neural Comput. 4,1 – 58 (1992).\n9. Alzubaidi, L. et al. Review of deep learning: Concepts, CNN architectures, chal-\nlenges, applications, future directions.J. Big Data8, 53 (2021).\n10. Gu, J. et al. Recent advances in convolutional neural networks.Pattern Recognit.\n77, 354– 377 (2018).\n11. Weimann, K. & Conrad, T. O. F. Transfer learning for ECG classiﬁcation. Sci. Rep.11,\n5251 (2021).\n12. Weiss, K., Khoshgoftaar, T. M. & Wang, D. A survey of transfer learning.J. Big Data\n3, 9 (2016).\n13. Deng, J. et al. In2009 IEEE conference on computer vision and pattern recognition.\n248– 255 (Ieee).\n14. Gavrilov, A. D., Jordache, A., Vasdani, M. & Deng, J. Preventing model overﬁtting\nand underﬁtting in convolutional neural networks.Int. J. Softw. Sci. Comput. Intell.\n(IJSSCI) 10,1 9– 28 (2018).\n15. Vaswani, A. et al. Attention is all you need. InAdvances in Neural Information\nProcessing Systems Vol. 30 (eds Guyon, I. et al.) (Curran Associates, Inc, 2017).\nhttps://proceedings.neurips.cc/paper_ﬁles/paper/2017/ﬁle/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n16. Khan, S. et al. Transformers in vision: A survey.ACM Computing Surveys (CSUR)54,\n1– 41 (2022).\n17. Wolf, T. et al. InProceedings of the 2020 conference on empirical methods in natural\nlanguage processing: system demonstrations.3 8– 45.\n18. Kalyan, K. S., Rajasekharan, A. & Sangeetha, S. Ammus: A survey of transformer-\nbased pretrained models in natural language processing. Preprint at https://\narxiv.org/abs/2108.05542 (2021).\n19. Liu, Z. et al. InProceedings of the IEEE/CVF International Conference on Computer\nVision. 10012– 10022.\n20. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image\nrecognition at scale.Preprint at https://arxiv.org/abs/2010.11929 (2020).\n21. Bao, H., Dong, L. & Wei, F. Beit: Bert pre-training of image transformers.Preprint at\nhttps://arxiv.org/abs/2106.08254 (2021).\n22. Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C. & Dosovitskiy, A. Do vision\ntransformers see like convolutional neural networks?Adv. Neural Inf. Process. Syst.\n34, 12116– 12128 (2021).\n23. Shahani, L. S1Q3T3 pattern leading to early diagnosis of pulmonary embolism.\nBMJ Case Rep.2012 https://doi.org/10.1136/bcr-2012-006569 (2012).\nA. Vaid et al.\n7\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   108 \n24. Raudys, S. J. & Jain, A. K. Small sample size effects in statistical pattern recogni-\ntion: Recommendations for practitioners.IEEE Trans. Pattern Anal. Mach. Intell.13,\n252– 264 (1991).\n25. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout:\na simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res.15,\n1929– 1958 (2014).\n26. Bozkurt, B. et al. Universal deﬁnition and classiﬁcation of heart failure: a report of\nthe heart failure society of America, heart failure association of the European\nsociety of cardiology, Japanese heart failure society and writing committee of the\nuniversal deﬁnition of heart failure.J. Card. Fail.27, 387– 413 (2021).\n27. Webster, J. J. & Kit, C. InCOLING 1992 volume 4: The 14th international conference\non computational linguistics.\n28. Ghazvininejad, M., Levy, O., Liu, Y. & Zettlemoyer, L. Mask-Predict: Parallel\nDecoding of Conditional Masked Language Models. InProceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\n6112– 6121. https://arxiv.org/abs/1904.09324 (Association for Computational\nLinguistics, Hong Kong, China, 2019).\n29. Rubner, Y., Tomasi, C. & Guibas, L. J. The Earth Mover’s Distance as a Metric for\nImage Retrieval. Int. J. Computer Vis.40,9 9– 121 (2000).\n30. Selvaraju, R. R. et al. InProceedings of the IEEE international conference on com-\nputer vision. 618– 626.\nACKNOWLEDGEMENTS\nThis study was funded by R01HL155915 and Clinical and translational award for\ninfrastructure UL1TR004419. The authors would like to thank Wei Guo, Lili Gai, and\nEugene Fluder of the High Performance Computing group at Mount Sinai for making\nthe infrastructure underlying this study possible.\nAUTHOR CONTRIBUTIONS\nThe study was designed by A.V.; The code was written by A.V.; Underlying data were\ncollected, analyzed, and visualized by A.V.; the ﬁrst draft of the manuscript was\nwritten by A.V. and J.J.; G.N.N. supervised the project. A.V. and G.N.N. had access to\nand veriﬁed the data. All authors provided feedback and approved theﬁnal draft for\npublication.\nCOMPETING INTERESTS\nDr. Nadkarni reports consultancy agreements with AstraZeneca, BioVie, GLG\nConsulting, Pensieve Health, Reata, Renalytix, Siemens Healthineers, and Variant\nBio; research funding from Goldﬁnch Bio and Renalytix; honoraria from AstraZeneca,\nBioVie, Lexicon, Daiichi Sankyo, Meanrini Health and Reata; patents or royalties with\nRenalytix; owns equity and stock options in Pensieve Health and Renalytix as a\nscientiﬁc cofounder; owns equity in Verici Dx; has receivedﬁnancial compensation as\na scientiﬁc board member and advisor to Renalytix; serves on the advisory board of\nNeurona Health; and serves in an advisory or leadership role for Pensieve Health and\nRenalytix. All other authors have reported that they have no relationships relevant to\nthe contents of this paper to disclose.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41746-023-00840-9.\nCorrespondence and requests for materials should be addressed to Akhil Vaid.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nA. Vaid et al.\n8\nnpj Digital Medicine (2023)   108 Published in partnership with Seoul National University Bundang Hospital",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6615179181098938
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6605616807937622
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5704222321510315
    },
    {
      "name": "Transformer",
      "score": 0.5509846806526184
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5282225608825684
    },
    {
      "name": "Transfer of learning",
      "score": 0.4422389268875122
    },
    {
      "name": "Artificial neural network",
      "score": 0.42845478653907776
    },
    {
      "name": "Machine learning",
      "score": 0.4234240651130676
    },
    {
      "name": "Engineering",
      "score": 0.11964711546897888
    },
    {
      "name": "Voltage",
      "score": 0.07129323482513428
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98704320",
      "name": "Icahn School of Medicine at Mount Sinai",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210086933",
      "name": "NYU Langone Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    }
  ],
  "cited_by": 75
}