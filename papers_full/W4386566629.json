{
  "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
  "url": "https://openalex.org/W4386566629",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3155175059",
      "name": "Terry Yue Zhuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098887313",
      "name": "Zhuang Li",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2121438158",
      "name": "Yujin Huang",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2703098962",
      "name": "Fatemeh Shiri",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2143497027",
      "name": "Weiqing Wang",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1432492132",
      "name": "Gholamreza Haffari",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2487707874",
      "name": "Yuan-fang Li",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3208933101",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2923325523",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3209208614",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4300996741",
    "https://openalex.org/W3028525609",
    "https://openalex.org/W3176393001",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W1970026646",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3200524025",
    "https://openalex.org/W2950787360",
    "https://openalex.org/W3171547145",
    "https://openalex.org/W1603729186",
    "https://openalex.org/W4226226396",
    "https://openalex.org/W3105932574",
    "https://openalex.org/W4294410794",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W2150406842",
    "https://openalex.org/W1483428028",
    "https://openalex.org/W4288363831",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W2913266441",
    "https://openalex.org/W2952781527",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W3201927751",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W2798803565",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W4298187912",
    "https://openalex.org/W4226053975",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W1736726159",
    "https://openalex.org/W4385573257",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2145748623",
    "https://openalex.org/W2997502936",
    "https://openalex.org/W4229907684",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W4287210714",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W4287864740",
    "https://openalex.org/W3179534853",
    "https://openalex.org/W2912070915",
    "https://openalex.org/W2766108848",
    "https://openalex.org/W4283366230",
    "https://openalex.org/W3155571973",
    "https://openalex.org/W4226389314",
    "https://openalex.org/W2963578915",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W3212748247",
    "https://openalex.org/W2799007037",
    "https://openalex.org/W2964271186"
  ],
  "abstract": "Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li. Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1090–1102\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nOn Robustness of Prompt-based Semantic Parsing\nwith Large Pre-trained Language Model:\nAn Empirical Study on Codex\nTerry Yue Zhuo12 and Zhuang Li2∗\nYujin Huang2 and Fatemeh Shiri2\nWeiqing Wang2 and Gholamreza Haffari2 and Yuan-Fang Li2\n1CSIRO’s Data61, Australia\n2Monash University, Australia\n{terry.zhuo, zhuang.li}@monash.edu\nAbstract\nSemantic parsing is a technique aimed at con-\nstructing a structured representation of the\nmeaning of a natural-language question. Re-\ncent advances in language models trained on\ncode have shown superior performance in gen-\nerating these representations compared to lan-\nguage models trained solely on natural lan-\nguage text. The existing fine-tuned neural se-\nmantic parsers are vulnerable to adversarial at-\ntacks on natural-language inputs. While it has\nbeen established that the robustness of smaller\nsemantic parsers can be enhanced through ad-\nversarial training, this approach is not feasi-\nble for large language models in real-world\nscenarios, as it requires both substantial com-\nputational resources and expensive human an-\nnotation on in-domain semantic parsing data.\nThis paper presents the first empirical study on\nthe adversarial robustness of a prompt-based\nsemantic parser based on CODEX , a state-of-\nthe-art (SOTA) language model trained on code.\nOur results demonstrate that the large language\nmodel of code is vulnerable to carefully crafted\nadversarial examples. To overcome this chal-\nlenge, we propose methods for enhancing ro-\nbustness without requiring substantial amounts\nof labelled data or intensive computational re-\nsources.\n1 Introduction\nSemantic parsing is a technique that transforms\nnatural-language utterances (NLs) into machine-\nreadable logical forms (LFs) and has been widely\napplied in various research fields, such as code gen-\neration, question-answering systems, and dialogue\nsystems (Kamath and Das, 2018). Most current\nstate-of-the-art semantic parsers are deep-learning\nmodels trained in a supervised manner using in-\ndomain data. However, this approach requires a\nlarge amount of in-domain semantic parsing data,\nwhich can be costly to obtain (Bapna et al., 2017).\n∗corresponding author\nTo address this issue, prompt-based semantic\nparsers based on large pre-trained language mod-\nels, such as Codex (Chen et al., 2021) and GPT-\nJ (Wang and Komatsuzaki, 2021), have become\na new choice for semantic parsing applications.\nPrompt-based semantic parsers learn to solve a new\ntask by in-context learning, instructing the parsers\nto generate correct LFs by constructing the prompt\nwith a few demonstration examples. Such a method\ncan significantly lower the cost of annotations by\nincluding only a few exemplars in the prompt and\nachieve comparable results to fully-supervised se-\nmantic parsers (Shin and Van Durme, 2022).\nRecent studies (Huang et al., 2021; Pi et al.,\n2022) show that fully-supervised semantic parsers\nare vulnerable to adversarial attacks, which per-\nturb input sentences into their semantic equivalent\nadversaries to mislead models to produce attacker-\ndesired outputs. Hence, to mitigate such attacks,\nvarious adversarial training methods (Tramer and\nBoneh, 2019; Shafahi et al., 2019; Ganin et al.,\n2016; Shafahi et al., 2020) have been proposed to\nimprove the adversarial robustness of the semantic\nparsers. In light of this, two main questions natu-\nrally arise: (1) Do prompt-based semantic parsers\nbased on large pre-trained language models also\nsuffer from adversarial attacks? (2) If so, how\ncan we improve the robustness of the large prompt-\nbased semantic parsers?\nTo address the former question, we evaluate the\nprompt-based semantic parsers on several evalua-\ntion sets built by different perturbation approaches\nmentioned in the AdvGLUE (Wang et al., 2021)\ndataset. Adopting the adversarial evaluation met-\nrics proposed by Huang et al. (2021), it is found\nthat the prompt-based semantic parsers are vulner-\nable to various types of adversarial attacks.\nAccording to the experimental results from the\nfirst step, we perform a three-fold experiment to\nanswer the latter questions. The first aspect of\nthe study aims to determine if the inclusion of\n1090\nadditional examples within the prompt during in-\ncontext learning improves the robustness of prompt-\nbased parsers. This hypothesis is based on prior\nresearch that has demonstrated that the increase\nin the size of the training data results in an en-\nhancement of robustness in fully-supervised mod-\nels (Pang et al., 2019). The second part of the study\naims to determine if the integration of few-shot\nadversarial examples within prompts can improve\nthe robustness of Codex. This was based on the\nobservation that conventional adversarial training\nmethods often include adversarial examples within\nthe training set (Miyato et al., 2016; Tramer and\nBoneh, 2019). Finally, the third part of the study\naims to evaluate if sampling methods other than\nrandom sampling can select more effective exam-\nples that improve the robustness of prompt-based\nparsers.\nIn this work, we perform a series of experi-\nments to probe CODEX , a large pre-trained model\ntrained on code, on two semantic parsing bench-\nmarks, GeoQuery (Zelle and Mooney, 1996) and\nScholar (Iyer et al., 2017). Our key findings\nfrom the above experiments are as follows:\n• Prompt-based semantic parsers are vulnerable\nto adversarial examples, particularly the ones\ncrafted by sentence-level perturbations.\n• In-context learning with more demonstration\nexamples in the prompt can improve the in-\ndomain robustness of prompt-based parsers.\n• Augmenting the prompt with adversarial ex-\namples has limited effect in improving the\nrobustness of prompt-based parsers.\n• The few-shot example sampling strategy with\nhigher language complexity can result in\nstronger robustness for the prompt-based\nparsers.\n2 Related Work\nPrompt-based Learning. Prompt-based learn-\ning is an alternative approach to supervised learn-\ning that aims to reduce the reliance on large human-\nannotated datasets (Liu et al., 2021). Unlike tradi-\ntional supervised models, which estimate the prob-\nability of an output given an input text, prompt-\nbased learning models estimate the probability of\nthe text directly. This is achieved by applying\nprompt functions to modify the input text into\nvarious prompt templates with unfilled slots. By\nfilling these slots, various Natural Language Pro-\ncessing (NLP) tasks can be completed, such as\ncommon-sense reasoning (Kojima et al., 2022),\nself-rationalization (Marasovi´c et al., 2021), and\ntext style transfer (Suzgun et al., 2022). The devel-\nopment of prompt-based methods has enabled zero-\nshot and few-shot learning in a variety of artificial\nintelligence domains (Ramesh et al., 2021; Yang\net al., 2022; Sanghi et al., 2022). Recent research\nhas also evaluated the capabilities of few-shot\nprompt-based learning for semantic parsing (Shin\nand Van Durme, 2022; Roy et al., 2022a; Drozdov\net al., 2022). Our contribution extends the current\nresearch by investigating the effect of prompts com-\nprising only a limited number of examples on the\nrobustness of prompt-based semantic parsers.\nAdversarial Robustness. Neural networks have\nachieved impressive performance across various do-\nmains. However, as demonstrated by Szegedy et al.\n(2014), neural models are vulnerable to adversar-\nial examples. Adversarial attacks in NLP normally\ntake on various forms, including character-level ma-\nnipulations (Hosseini et al., 2017; Ebrahimi et al.,\n2018; Belinkov and Bisk, 2018; Gao et al., 2018;\nEger et al., 2019; Boucher et al., 2022), sentence-\nlevel rewriting (Iyyer et al., 2018; Ribeiro et al.,\n2018; Zhao et al., 2018), and adversarial word sub-\nstitutions (Alzantot et al., 2018; Liang et al., 2018;\nZhang et al., 2019).\nThere has been an increasing interest in defend-\ning against adversarial attacks in large language\nmodels via adversarial training (Yi et al., 2021;\nRoss et al., 2022; Bartolo et al., 2021; Guo et al.,\n2021). Adversarial training involves incorporat-\ning adversarial examples in the training set, thus\nmaking the model robust to such attacks. How-\never, adversarial training can sometimes negatively\nimpact the generalization ability of the neural mod-\nels (Raghunathan et al., 2019; Min et al., 2021).\n3 Robustness Evaluation for\nPrompt-based Semantic Parsing\nThis section gives an overview of our evaluation\nframework, including the methods of constructing\nthe evaluation corpora and the evaluation metrics\nto evaluate the robustness of the prompt-based se-\nmantic parser.\n1091\nLinguistic\nPhenomenon\nSamples (Strikethrough = Original Text, red = Adversarial Perturbation)\nTypo\n(Word-level) NL: what can you tellte11 me about theth e population of missouri\nSubstitution\n(Word-level) NL: what canwill you tell me about thea population of missouri\nParaphrase\n(Sent.-level)\nNL: what can you tell me about the population of missouri\nWhat information can you provide on Missouri’s population?\nTable 1: Examples from Robustness Evaluation Set. We show 3 examples from GeoQuery. These examples are generated\nwith three different perturbations, and they all can successfully change the predictions of CODEX .\n3.1 Construction of the Evaluation Corpus\nA robust prompt-based semantic parser should be\nable to parse both the utterances and their adver-\nsarial counterparts into correct LFs. As proposed\nby Huang et al. (2021), an adversary of an utterance\nfor a semantic parser is defined as i) an utterance\nwith the same semantic meanings as the original\none given the human judgment and ii) an utterance\non which the semantic parser cannot produce cor-\nrect LF. Therefore, to evaluate the robustness of\nprompt-based semantic parsers, we craft the robust-\nness evaluation sets by perturbing the original utter-\nances in existing benchmark datasets with multiple\nadversarial perturbation methods. Such perturba-\ntions should not alter the semantics of the original\nutterances. Each example in a robustness evalu-\nation set is a perturbed utterance paired with its\nground-truth LF. Next, we introduce the details of\neach perturbation method and how we guarantee\nthe perturbations do not change the semantics. Ta-\nble 1 illustrates some meaning-preserved utterances\nafter perturbation in the robustness evaluation set of\nGeoQuery based on different perturbation meth-\nods. More examples can be found in Appendix B.\n3.1.1 Adversarial Perturbations\nFollowing the principles as in Wang et al. (2021) to\ndesign adversarial attacks, we perform five word-\nlevel perturbations and two sentence-level perturba-\ntions to generate seven robustness evaluation sets\nfor the standard evaluation set in each benchmark.\nWord-level Perturbations.\n• Typo-based (TB) uses TextBugger (Li et al.,\n2018) to replace two words in each utterance\nwith the typos.\n• Random Deletion (RD) randomly deletes\ntwo words in the utterance.\n• Random Swap (RS) swaps the positions of\ntwo random words in each utterance.\n• Context-aware Substitution (CS) leverages\nRoBERTa (Liu et al., 2019) to substitute two\nrandom words with their synonyms.\n• Context-aware Insertion (CI) inserts two\nmost probable words selected by RoBERTa at\ntwo random positions in each utterance.\nSentence-level Perturbations.\n• Rewriting-based (RB) chooses Quillbot1 (Fi-\ntria, 2021), a state-of-the-art (SOTA) commer-\ncial paraphrasing model, to rewrite the com-\nplete utterances. Quillbot has been demon-\nstrated as an effective tool to paraphrase ut-\nterances in semantic parsing data (Shiri et al.,\n2022).\n• Distraction-based (DB) appends interroga-\ntion statements to the end of each NL, inspired\nby StressTest (Naik et al., 2018). Specifically,\nwe design the following interrogation state-\nments: \"who is who; what is what; when\nis when; which is which; where is where\",\nin which the selected interrogative words are\nmore likely to appear in the utterance.\n3.1.2 Data Filtering\nIn order to ensure that the perturbed examples pre-\nserve the meaning of the original NL, we design a\ntwo-stage evaluation process:\nStep1: We first generate 20 adversarial exam-\nples against the original NL for each perturbation\nmethod and choose the top 10 candidates ranked\nbased on text similarity scores between the original\nand the perturbed ones, which are calculated by\nSentence-BERT (Reimers and Gurevych, 2019).\n1https://www.quillbot.com/paraphrasing\n1092\nStep2: We engage human experts to select the\nbest one among the 10 adversarial candidates pro-\nduced in Step1.\n3.2 Evaluation Metrics\nSince the output LFs of the prompt-based language\nmodels may not follow the same naming conven-\ntion (Shin et al., 2021; Shin and Van Durme, 2022)\nas the ground truth, previous string-based evalu-\nation metrics, including BLEU (Papineni et al.,\n2002) and Exact Match (Poon and Domingos,\n2009), are not suitable for prompt-based seman-\ntic parsers. Therefore, we follow Rajkumar et al.\n(2022) to report the execution accuracy, which is\nbased solely on the execution correctness of the\nLFs on the test sets, for the purpose of robustness\nevaluation.\nFollowing Huang et al. (2021), we report the\nexperiment results with three variants of execution\naccuracy, namely standard accuracy, perturbation\naccuracy and robust accuracy:\n• Standard Accuracy is measured on the stan-\ndard (original) test sets.\n• Perturbation Accuracy tests the perfor-\nmance of the model on perturbed test sets.\n• Robust Accuracy is defined as n/|Reval|.\nReval denotes a subset of the perturbed test\nsets, and nis the number of the utterances in\nReval that are parsed correctly. More specif-\nically, Reval consists of the examples whose\ncounterparts before perturbation are parsed\ncorrectly. Intuitively, Robust Accuracy esti-\nmates the quantity of cases that a parser can\nsuccessfully parse before perturbation but can-\nnot do so after perturbation, and hence shows\nthe robustness of the parsers against adversar-\nial perturbation.\n4 Improving Robustness of Prompt-based\nSemantic Parsers\nInstead of predicting the LF conditioned on the\ninput utterance, large language models such as\nCODEX could learn to solve a specific task by in-\ncontext learning. During in-context learning, the\nparser predicts the LF conditioned on a prompt\nwhich consists of a small list of utterance-LF pairs\nto demonstrate the semantic parsing task and, op-\ntionally, a table schema. To defend against adver-\nsarial attacks, one seminal approach is adversarial\ntraining. One of the most typical adversarial train-\ning methods augments the training data with adver-\nsarial examples, from which the machine learning\nmodel could learn robust features (Allen-Zhu and\nLi, 2022) by gradient descent. However, directly\nadapting conventional adversarial training is not\nsuitable for in-context learning. First, the number\nof demonstration examples is limited due to the re-\nstriction on the maximum number of tokens for the\npre-trained language model. As a result, we cannot\ninclude an arbitrary number of adversarial exam-\nples in the prompt, which might not include enough\nrobust features. Second, in-context learning does\nnot update the parameters of the language model.\nThe model would not be optimized towards learn-\ning the robust features in the adversarial examples\nthrough gradient descent.\nGiven the difference, it is unclear whether in-\ncontext learning could improve the robustness of\nthe parser as the conventional supervised training.\nIn this paper, we conduct the first investigation on\nin-context learning for model robustness. More\nspecifically, we examine the impact of variants of\nin-context learning and sampling methods on parser\nrobustness.\n4.1 Standard In-context Few-shot Learning\nIn our setting, given an input utterance x, the pre-\ntrained language model P(·; θ) predicts the LF y′\nconditioned on the prompt, which consists of a set\nof demonstration examples M= {(xi,yi)}N\ni=1,\nand a table schema T:\ny′= arg max\ny∈Y\nP(y|x,M,T; θ) (1)\nFor the few-shot setting, the number of demonstra-\ntion examples N is limited by a budget size.\n4.2 Adversarial In-context Few-shot Learning\nIn adversarial in-context learning, we include the\nperturbed adversarial examples, Madv, in the\ndemonstration examples:\ny′= arg max\ny∈Y\nP(y|x,M∪Madv,T; θ) (2)\n4.3 In-context Few-shot Selection\nCurrent in-context learning assumes there is an ex-\nample pool from where they can select prompting\nexamples. However, most of the works only ran-\ndomly pick examples from the pools. We argue that\nthe way to select the examples might deeply impact\n1093\nthe robustness of the prompt-based semantic parser.\nTherefore, we examine various strategies to select\nin-context few-shot examples.\nRandom Sampling (Random). We randomly\nsample N utterances from the example pool.\nConfidence-based Sampling (Confi-\ndence) (Duong et al., 2018). We score\neach utterance with the confidence of the parser on\nthe predicted LF given the utterance and the table\nschema. Then we select the ones with the lowest\nparser confidence scores2.\nDiversity-based Sampling. Following Li et al.\n(2021), we partition the utterances in the utterance\npool into N clusters with the K-means (Wu, 2012)\nclustering algorithm and select the example closest\nto the cluster centers. We measure the edit distance\n(Cluster-ED) (Wagner and Fischer, 1974), and Eu-\nclidean distances using utterance features of TF-\nIDF (Cluster-TF-IDF) (Anand and Jeffrey David,\n2011), or Contextual Word Embedding (Cluster-\nCWE) encoded by Sentence-BERT (Reimers and\nGurevych, 2019), between each pair of utterances\nfor K-means.\nPerplexity-based Sampling (Sen and Yilmaz,\n2020). We score each utterance with the perplex-\nity of GPT-2 on this utterance. Then we select the\nutterances with the highest (PPL. Asc) and lowest\n(PPL. Desc) perplexity scores, respectively.\n5 Experiments\n5.1 Setup\nEvaluation Datasets. We evaluate the robustness\nof the prompt-based semantic parsers via the ad-\nversarial robustness sets built on top of the test\nsets of GeoQuery (Finegan-Dollak et al., 2018)\nand Scholar (Finegan-Dollak et al., 2018) with\nthe proposed perturbation methods in Section 3.\nAs in Finegan-Dollak et al. (2018), we choose the\nquery splits of both GeoQuery and Scholar,\nwhere there is no LF template overlap among train,\ntest, and dev sets.\nPrompt-based Semantic Parser. We choose\nCODEX (Chen et al., 2021) as the representative\nprompt-based semantic parser for our evaluation.\n2The confidence scoring parser is a zero-shot model, mean-\ning that there are no examples present in the prompt. It op-\nerates solely based on the input utterance, instructions, and\nschema provided within the prompt. Please see Section 5.1\nfor more information on the prompt structure.\nIn recent studies, CODEX has performed com-\nparably via in-context few-shot semantic parsing\nto the SOTA-supervised trained neural semantic\nparsers (Shin and Van Durme, 2022; Roy et al.,\n2022b; Drozdov et al., 2022) in terms of execution\naccuracy.\nTo examine the vulnerability of large prompt-\nbased semantic parsers against adversarial exam-\nples, we choose the code-davinci-002 ver-\nsion of CODEX as it is the most powerful variant\namong all CODEX models, with 175B parameters.\nIn our experiments, we sample a maximum of 200\ntokens from CODEX with the temperature set to 0,\nwith the stop token to halt generation.\nPrompts. In this work, we adopt the prompt de-\nsign of Create Table + Select X as pre-\nsented in Rajkumar et al. (2022), which has been\nshown to be effective for semantic parsing using\nstatic prompting3.\nThe prompt for semantic parsing on CODEX\nconsists of CREATE TABLE commands, includ-\ning specifications for each table’s columns, for-\neign key declarations, and the results of executing\na SELECT * FROM T LIMIT X query on the\ntables via the column headers. As described in\nSection 4.3, we select NL-LF pairs as in-context\nfew-shot examples from the trainsets.\nTo guide the prompt-based semantic\nparser, we also include the textual instruc-\ntion of “ Using valid SQLite, answer\nthe following questions for the\ntables provided above.” as proposed\nby Rajkumar et al. (2022).\n5.2 Research Questions and Discussions\nOur experimental results answer the following four\nresearch questions (RQs) related to the robustness\nof CODEX .\nRQ1: How vulnerable is the prompt-based\nsemantic parser to adversarial examples?\nSettings. To answer RQ1, we evaluate the stan-\ndard accuracy and perturbation accuracy ofCODEX\non GeoQuery and Scholar test sets through\nzero-shot learning.\nResults. The zero-shot parsing performances of\nCODEX are shown in Table 2. Our first observa-\n3In contrast to the approach of Shin et al. (2021) which\ninvolves dynamically selecting few-shot examples from an\nexample pool, we refer to static prompting as being performed\nwith a fixed set of examples.\n1094\nCategory Pert. Strategy GeoQuery Scholar\nPert. Acc. Std. Acc. ∆ Pert. Acc. Std. Acc. ∆\nWord-level\nTB 53.85\n57.14\n-3.29 11.35\n12.21\n-0.86\nRD 50.55 -6.59 10.52 -1.69\nRS 37.36 -19.78 8.31 -3.90\nCS 42.31 -14.83 8.40 -3.81\nCI 38.46 -18.68 8.31 -3.90\nSentence-level RB 31.87 -25.27 5.22 -6.99\nDB 35.71 -21.43 7.88 -4.33\nTable 2: Results of perturbation accuracy (Pert. Acc.) and standard accuracy (Std. Acc.) of zero-shot performance\non GeoQuery and Scholar. The zero-shot prompt only contain the table information and initial semantic parsing\ninstruction. Perturbation accuracy is calculated based on each perturbation method.\ntion is that CODEX is more vulnerable to sentence-\nlevel perturbations than to word-level perturbations,\nas indicated by the more significant performance\ngaps between standard and perturbed accuracies on\nthe sentence-level perturbed test sets. Wang et al.\n(2021) observed that neural language models are\nvulnerable to human-crafted adversarial examples\nwhere there are complex linguistic phenomenons\n(e.g., coreference resolution, numerical reasoning,\nnegation). We observe that the rewriting model\ntrained on human paraphrase pairs also introduces\nsuch complex linguistic phenomenons.\nWith respect to the word-level perturbations,\nCODEX is most robust to typo-based perturbations,\nwhich is surprising as Wang et al. (2021) shows\ntypo-based perturbation is the most effective attack\nmethod for large language models like BERT (De-\nvlin et al., 2019) in the evaluation of natural lan-\nguage understanding tasks. However, utterances\nwith typos drop only 3% of the accuracy ofCODEX .\nRandom Deletion is also less effective than the\nother word-level methods, consistent with the ob-\nservations by Huang et al. (2021) on the fully-\nsupervised semantic parsers. This phenomenon\ncan be attributed to the fact that Random Deletion\nprimarily makes minor modifications to the stan-\ndard NL utterances, as this method often involves\nremoving non-functional words such as articles, for\nexample, “the” and “a.”\nAlthough CODEX is pre-trained on a consider-\nably large dataset, it does not show robustness on\nthe in-domain tasks. We conjecture that the reason\nis that zero-shot CODEX has not yet learned any in-\ndomain knowledge on GeoQuery or Scholar.\nSo in RQ2, we would address whether in-domain\nexamples would improve the robustness ofCODEX .\nTakeaways. Zero-shot CODEX is vulnerable to\nadversarial examples, especially sentence-level per-\nturbation of utterances, rather than to word-level\nperturbations.\nPert. Strategy 5-shot 10-shot 20-shot 30-shot 40-shot 50-shot\nTB 63.19 71.98 78.02 81.32 81.30 82.42RD 59.34 64.29 71.43 71.98 70.33 75.27RS 52.20 52.75 54.87 59.34 60.99 63.14CS 54.40 56.04 60.44 63.19 65.93 67.03CI 51.65 54.95 55.49 57.69 58.79 61.02RB 44.51 47.80 49.45 50.55 54.23 57.27DB 48.35 49.77 53.30 54.20 59.34 59.89\nAvg. Pert. Acc. 53.38 56.80 57.50 59.49 64.42 66.58\nStd. Acc. 66.48 74.37 79.67 81.87 83.52 84.62\nAvg. Robust Acc. 75.67 77.28 78.74 80.44 82.07 83.22\nTable 3: Few-shot performances on GeoQuery. We\nconduct {5, 10, 20, 30, 40, 50}-shot learning experi-\nments. Average perturbation accuracy (Avg. Pert. Acc.)\nis the average score of execution accuracies on different\nperturbation sets. Average robust accuracy (Avg. Ro-\nbust Acc.) is the average score of execution accuracies\non the test sets perturbed by different perturbation meth-\nods.\nPert. Strategy 3-shot 5-shot 10-shot\nTB 10.57 20.33 34.29\nRD 12.09 25.27 31.43\nRS 6.04 17.03 25.08\nCS 9.34 18.13 26.03\nCI 8.24 14.29 20.63\nRB 3.30 8.43 18.10\nDB 4.40 10.44 21.90\nAvg. Pert. Acc. 7.71 16.27 25.35\nStd. Acc. 14.29 23.08 40.32\nAvg. Robust. Acc. 51.12 53.10 55.97\nTable 4: Few-shot performances on Scholar. We\nconduct {3, 5, 10}-shot learning experiments.\nRQ2: Does standard in-context few-shot\nlearning improve the robustness?\nSettings. We respectively select up to 50 and 10\nexamples from GeoQuery and Scholar train\nsets4, with the random sampling strategy, to con-\nstruct prompts for parsers. Then, for each few-shot\n4Due to the larger size of table schema in Scholar, we\ncould only include up to 10 examples in the prompt.\n1095\nlearning experiment, we measure standard accu-\nracy, perturbation accuracy and robust accuracy on\nour various perturbed test sets.\nResults. Tables 3 and 4 show the performance of\nstandard in-context few-shot learning on the robust-\nness evaluation sets perturbed by different methods.\nWe observe that more standard examples in the\nprompt can evidently improve the robust accuracy\nof CODEX , which demonstrates the effectiveness\nof standard in-context few-shot learning in improv-\ning the robustness of semantic parsing. Although it\nperforms slightly worse on the test sets perturbed\nby typo-based methods than the one perturbed by\nthe random deletion in GeoQuery, we argue that\nthis is due to the performance variance, which does\nnot necessarily hurt the model robustness.\nThe performance gap between perturbation accu-\nracy with standard accuracy is enlarged when the\nnumber of in-context shots increases. However, the\nrobust accuracy grows slowly. This indicates that\nimproving the generalization ability of the parser\ndoes not necessarily mean the improvement of the\nrobustness. The trade-off between standard and\nrobust accuracies is a long-standing problem in ad-\nversarial training. Raghunathan et al. (2019) shows\nthat increasing the training sample size could elimi-\nnate such a trade-off. Our experiments demonstrate\nthat in-context learning follows similar patterns as\nsupervised adversarial training. It can be observed\nthat both objectives can be improved with a limited\nnumber of examples when compared to the zero-\nshot parser. However, the extent of improvement\nvaries.\nTakeaways. With more standard in-context ex-\namples, few-shot CODEX can be guided to achieve\nbetter robustness and standard execution perfor-\nmance.\nRQ3: Does adversarial in-context learning\nimprove robustness?\nSettings. In this work, we present the experi-\nmental results of CODEX on both GeoQuery and\nScholar datasets, using 10 and 5 in-context ex-\namples, respectively. In order to assess the ro-\nbustness of CODEX through adversarial in-context\nlearning, we first augment the standard few-shot\nexamples by incorporating examples whose utter-\nances have been perturbed using various perturba-\ntion methods. Subsequently, for each set of aug-\nmented examples, we calculate the average robust\naccuracy of CODEX based on the average of the\nAdv. L. Strategy GeoQuery ScholarAvg. Robust Acc. Std. Acc. Avg. Robust Acc. Std. Acc.No Adv. 77.28 74.37 53.10 23.08No Adv. (×2) 78.74 79.67 55.97 40.32TB 77.32 73.62 52.75 34.99RD 77.40 73.64 53.11 33.65RS 78.30 74.73 54.88 33.46CS 78.05 74.47 53.12 34.86CI 78.14 74.81 54.66 33.65RB 78.47 75.51 56.58 35.85DB 78.31 75.08 55.08 35.71\nTable 5: The results of the average robust accuracy\nobtained through Adversarial In-context Learning (Adv.\nL. Strategy) with different types of perturbed few-shot\nexamples. Additionally, we include results of applying\nthe method with only standard examples (No Adv.), as\nwell as with a doubled number of standard examples\n(No Adv. (×2)).\nparser robust accuracies on all robustness evalua-\ntion sets.\nResults. The experimental results of the various\nperturbation strategies applied to the in-context\nfew-shot examples are presented in Table 5. While\nthe approach of supervised adversarial training has\nbeen widely regarded as an effective means of en-\nhancing the robustness of machine learning mod-\nels, the results indicate that on both GeoQuery\nand Scholar, the robust accuracies are only\nmarginally improved through the application of\nadversarial in-context learning. Previous stud-\nies (Raghunathan et al., 2019; Huang et al., 2021)\nhave pointed out that supervised adversarial train-\ning can sometimes result in a decrease in stan-\ndard accuracy, even as it improves robust accu-\nracy. However, the results of adversarial in-context\nlearning diverge from this trend, with significant\nimprovement in standard accuracy, from 23% to\nmore than 33%, observed on Scholar, while ro-\nbust accuracy only experiences marginal improve-\nment. These observations indicate that adversarial\nin-context learning represents a distinct approach\nfrom supervised adversarial training in terms of\nenhancing the robustness of the model. Further-\nmore, the results suggest that simply incorporating\nadversarial examples into the prompt has a limited\nimpact on the robustness of parsers, in contrast to\nsupervised adversarial training.\nOf all the perturbation strategies analyzed, the\nresults indicate that CODEX achieves the best per-\nformance in terms of both standard and robust ac-\ncuracy through the application of RB adversarial\nin-context learning, but experiences the worst per-\nformance through TB adversarial in-context learn-\ning. The hypothesis is that RB produces utterances\n1096\nDataset Metric Confidence Cluster-CWE Cluster-ED Cluster-TF-IDF PPL. Asc PPL. Desc Random\nGeoQuery\nAvg. Robust Acc. 78.77 78.02 82.40 85.10 70.36 50.82 73.22\nAvg. Pert. Acc. 69.80 68.93 74.41 77.74 62.11 45.71 66.58\nStd. Acc. 74.73 78.41 81.32 85.64 73.14 70.76 74.18\nScholar\nAvg. Robust Acc. 55.31 60.24 60.68 62.61 53.97 47.18 55.97\nAvg. Pert. Acc. 28.55 29.81 30.28 31.44 22.25 7.47 25.35\nStd. Acc. 37.99 41.49 42.19 42.97 35.93 34.89 36.91\nTable 6: The performance of standard few-shot in-context learning using various sampling methods on the\nGeoQuery and Scholar datasets. The average robust accuracy, average perturbation accuracy, and standard\naccuracy are computed for each sampling method to assess their efficiency in this learning scenario.\nDataset LC. Metric Confidence Cluster-CWE Cluster-ED Cluster-TF-IDF PPL. Asc PPL. Desc Random\nGeoQuery\nTTR ↑ 7.68 7.24 8.47 10.26 5.94 3.22 6.44\nYule’s I↑ 68.55 64.37 69.59 71.49 62.94 43.41 58.14\nMTLD↑ 12.44 12.19 13.37 15.58 11.32 8.16 10.41\nScholar\nTTR ↑ 28.18 29.91 31.40 33.11 21.15 14.17 25.67\nYule’s I↑ 198.15 207.11 223.76 262.36 177.37 102.17 193.31\nMTLD↑ 15.68 15.63 16.34 19.49 11.94 13.12 14.36\nTable 7: Results of the language complexity of standard NLs sampled by different sampling strategies, measured by\nthree lexical diversity (LC.) metrics. For the ease of readability and comparison, we multiply both TTR scores and\nYule’s I scores by 100.\nwith more complex linguistic features, resulting in\nenhanced standard and robust accuracy during in-\ncontext learning. To test this hypothesis, the num-\nber of standard examples is doubled (No Adv.×2)\nto match the size of the examples augmented with\nthe adversarial examples. The results show that\nthe robust and standard accuracies of CODEX are\nhigher than those obtained through adversarial in-\ncontext learning, likely due to the greater diversity\nof linguistic variations in the doubled standard ex-\namples.\nTakeaways. The robustness of few-shot CODEX\ncan be marginally improved by adversarial in-\ncontext learning without significant negative im-\npacts on standard performances.\nRQ4: What is the impact of sampling\ntechniques on the robustness of parsers that\nutilize standard in-context few-shot learning?\nSettings. In order to compare the influence of\ndifferent sampling methods on the robustness of\nthe model, we vary standard in-context examples\non GeoQuery and Scholar with all 7 strategies\naforementioned in Section 4.3. We choose the 50-\nshot setting for GeoQuery and 10-shot setting for\nScholar.\nResults and Takeaways. We present standard\naccuracies in Table 6 when varying the sampling\nmethods for the few-shot example selection. We\nfirst observe that different sampling strategies im-\npact the robust and standard performance of the\nCODEX . Overall, the Cluster methods, which diver-\nsify the features of the selected utterances, perform\nbetter than the other sampling methods. On the\nother hand, PPL. Desc sampling method performs\nconsistently poorly than the other sampling meth-\nods. In brief, we conclude that CODEX is sensitive\nto the few-shot example sampling strategies.\nRQ4-Ablation: Why does CODEX react\ndifferently to various sampling strategies?\nSettings. The findings of RQ 1 and RQ 3 indi-\ncate that linguistic complexity has an impact on\nthe performance of CODEX . As a result, the re-\nsults of RQ 4 may also be influenced by linguistic\ncomplexity. To further explore this correlation,\nthree lexical diversity metrics, Type-Token Ratio\n(TTR) (Templin, 1957), Yule’s I (Yule, 1944), and\nMeasure of Textual Lexical Diversity (MTLD) (Mc-\nCarthy, 2005), are adopted to measure the lexical\ndiversity of the selected NLs from GeoQuery and\nScholar. The TTR is defined as the ratio of the\nnumber of unique tokens, also known as types, to\nthe total number of tokens. The TTR is used as\nan indicator of lexical diversity, with a higher TTR\nindicating higher lexical diversity. Yule’s Charac-\nteristic Constant (Yule’s K) is a measure of text\nconsistency that considers vocabulary repetition.\nYule’s K and its inverse, Yule’s I, are considered\nmore robust to variations in text length than the\nType-Token Ratio (TTR). MTLD is computed as\nthe average number of words in a text required to\nmaintain a specified TTR value.\n1097\nResults and Takeaways. Table 7 presents the\nlexical diversity of each set of NLs sampled by dif-\nferent approaches. The diversity scores obtained\nfrom the three metrics align with the performance\nof CODEX as presented in Table 6. For instance, the\nthree metrics indicate that the examples selected us-\ning the Cluster-TF-IDF method achieve higher lex-\nical diversity compared to those selected through\nthe other methods. Additionally, the Cluster-TF-\nIDF method also produces the highest results in\nterms of robust and standard accuracy for CODEX .\nThus, it can be inferred that an increase in the lexi-\ncal diversity of the few-shot examples leads to an\nimprovement in the robust and standard accuracy\nof CODEX .\n6 Conclusion\nThis study examines the robustness of semantic\nparsers in the context of prompt-based few-shot\nlearning. To achieve this objective, robustness eval-\nuation sets were curated to evaluate the robustness\nof the prompt-based semantic parser, CODEX . The\nresearch aims to identify methods to improve the\nrobustness of CODEX . The results of our com-\nprehensive experiments demonstrate that even the\nprompt-based semantic parser based on a large pre-\ntrained language model is susceptible to adversar-\nial attacks. Our findings also indicate that various\nforms of in-context learning can improve the ro-\nbustness of the prompt-based semantic parser. It\nis believed that this research will serve as a cata-\nlyst for future studies on the robustness of prompt-\nbased semantic parsing based on large language\nmodels.\nLimitations\nIn this study, we examine the robustness of the\nprompt-based semantic parser, CODEX , by focus-\ning on the impact of prompt design on its execution\nperformance. However, there is a need for future\nresearch to investigate more alternative adversar-\nial training strategies for prompt-based semantic\nparsers in order to advance this field. In addition,\nour focus is limited to text-to-SQL tasks, and we en-\ncourage further investigation into the robustness of\nsemantic parsers across different datasets and LFs.\nDespite these limitations, we emphasize the impor-\ntance of exploring more effective prompt design in\norder to enhance the robustness of prompt-based\nsemantic parsers, including CODEX , which shows\nnon-negotiable vulnerability.\nReferences\nZeyuan Allen-Zhu and Yuanzhi Li. 2022. Feature pu-\nrification: How adversarial training performs robust\ndeep learning. In 2021 IEEE 62nd Annual Sympo-\nsium on Foundations of Computer Science (FOCS),\npages 977–988. IEEE.\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial ex-\namples. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2890–2896.\nRajaraman Anand and Ullman Jeffrey David. 2011.\nMining of massive datasets. Cambridge University\nPress.\nAnkur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and\nLarry Heck. 2017. Towards zero-shot frame se-\nmantic parsing for domain scaling. arXiv preprint\narXiv:1707.02363.\nMax Bartolo, Tristan Thrush, Robin Jia, Sebastian\nRiedel, Pontus Stenetorp, and Douwe Kiela. 2021.\nImproving question answering model robustness with\nsynthetic adversarial data generation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 8830–8848.\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In International Conference on Learning Rep-\nresentations.\nNicholas Boucher, Ilia Shumailov, Ross Anderson, and\nNicolas Papernot. 2022. Bad characters: Impercepti-\nble nlp attacks. In 2022 IEEE Symposium on Security\nand Privacy (SP), pages 1987–2004. IEEE.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nAndrew Drozdov, Nathanael Schärli, Ekin Akyuürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. arXiv\npreprint arXiv:2209.15003.\nLong Duong, Hadi Afshar, Dominique Estival, Glen\nPink, Philip R Cohen, and Mark Johnson. 2018. Ac-\ntive learning for deep semantic parsing. In Proceed-\nings of the 56th Annual Meeting of the Association for\n1098\nComputational Linguistics (Volume 2: Short Papers),\npages 43–48.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018. Hotflip: White-box adversarial examples\nfor text classification. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 31–36.\nSteffen Eger, Gözde Gül ¸ Sahin, Andreas Rücklé, Ji-Ung\nLee, Claudia Schulz, Mohsen Mesgar, Krishnkant\nSwarnkar, Edwin Simpson, and Iryna Gurevych.\n2019. Text processing like humans do: Visually\nattacking and shielding nlp systems. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1634–1647.\nCatherine Finegan-Dollak, Jonathan K Kummerfeld,\nLi Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui\nZhang, and Dragomir Radev. 2018. Improving text-\nto-sql evaluation methodology. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n351–360.\nTira Nur Fitria. 2021. Quillbot as an online tool: Stu-\ndents’ alternative in paraphrasing and rewriting of\nenglish writing. Englisia: Journal of Language, Edu-\ncation, and Humanities, 9(1):183–196.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, François Lavi-\nolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks. The\njournal of machine learning research, 17(1):2096–\n2030.\nJi Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun\nQi. 2018. Black-box generation of adversarial text\nsequences to evade deep learning classifiers. In 2018\nIEEE Security and Privacy Workshops (SPW), pages\n50–56. IEEE.\nChuan Guo, Alexandre Sablayrolles, Hervé Jégou, and\nDouwe Kiela. 2021. Gradient-based adversarial at-\ntacks against text transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5747–5757.\nHossein Hosseini, Sreeram Kannan, Baosen Zhang, and\nRadha Poovendran. 2017. Deceiving google’s per-\nspective api built for detecting toxic comments.arXiv\npreprint arXiv:1702.08138.\nShuo Huang, Zhuang Li, Lizhen Qu, and Lei Pan. 2021.\nOn robustness of neural semantic parsers. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 3333–3342.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant\nKrishnamurthy, and Luke Zettlemoyer. 2017. Learn-\ning a neural semantic parser from user feedback. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 963–973.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1875–1885.\nAishwarya Kamath and Rajarshi Das. 2018. A survey\non semantic parsing. In Automated Knowledge Base\nConstruction (AKBC).\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In ICML 2022\nWorkshop on Knowledge Retrieval and Language\nModels.\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting\nWang. 2018. Textbugger: Generating adversarial\ntext against real-world applications. arXiv preprint\narXiv:1812.05271.\nZhuang Li, Lizhen Qu, and Gholamreza Haffari. 2021.\nTotal recall: a customized continual learning method\nfor neural semantic parsers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3816–3831.\nBin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian,\nXirong Li, and Wenchang Shi. 2018. Deep text clas-\nsification can be fooled. In IJCAI.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAna Marasovi ´c, Iz Beltagy, Doug Downey, and\nMatthew E Peters. 2021. Few-shot self-\nrationalization with natural language prompts.\narXiv preprint arXiv:2111.08284.\nPhilip M McCarthy. 2005. An assessment of the range\nand usefulness of lexical diversity measures and the\npotential of the measure of textual, lexical diversity\n(MTLD). Ph.D. thesis, The University of Memphis.\nYifei Min, Lin Chen, and Amin Karbasi. 2021. The\ncurious case of adversarially robust models: More\ndata can help, double descend, or hurt generalization.\nIn Uncertainty in Artificial Intelligence, pages 129–\n139. PMLR.\n1099\nTakeru Miyato, Andrew M Dai, and Ian Goodfel-\nlow. 2016. Adversarial training methods for\nsemi-supervised text classification. arXiv preprint\narXiv:1605.07725.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress test evaluation for natural language inference.\narXiv preprint arXiv:1806.00692.\nTianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun\nZhu. 2019. Improving adversarial robustness via pro-\nmoting ensemble diversity. In International Con-\nference on Machine Learning , pages 4970–4979.\nPMLR.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nXinyu Pi, Bing Wang, Yan Gao, Jiaqi Guo, Zhoujun\nLi, and Jian-Guang Lou. 2022. Towards robustness\nof text-to-sql models against natural and realistic ad-\nversarial table perturbation. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2007–2022.\nHoifung Poon and Pedro Domingos. 2009. Unsuper-\nvised semantic parsing. In Proceedings of the 2009\nconference on empirical methods in natural language\nprocessing, pages 1–10.\nAditi Raghunathan, Sang Michael Xie, Fanny Yang,\nJohn C Duchi, and Percy Liang. 2019. Adversar-\nial training can hurt generalization. arXiv preprint\narXiv:1906.06032.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabil-\nities of large language models. arXiv preprint\narXiv:2204.00498.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gen-\neration. In International Conference on Machine\nLearning, pages 8821–8831. PMLR.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3982–3992.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversarial\nrules for debugging nlp models. In Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nAlexis Ross, Tongshuang Wu, Hao Peng, Matthew E\nPeters, and Matt Gardner. 2022. Tailor: Generat-\ning and perturbing text with semantic controls. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 3194–3213.\nSubhro Roy, Sam Thomson, Tongfei Chen, Richard\nShin, Adam Pauls, Jason Eisner, and Benjamin\nVan Durme. 2022a. Benchclamp: A benchmark for\nevaluating language models on semantic parsing.\nSubhro Roy, Sam Thomson, Tongfei Chen, Richard\nShin, Adam Pauls, Jason Eisner, and Benjamin\nVan Durme. 2022b. Benchclamp: A benchmark\nfor evaluating language models on semantic parsing.\narXiv preprint arXiv:2206.10668.\nAditya Sanghi, Hang Chu, Joseph G Lambourne,\nYe Wang, Chin-Yi Cheng, Marco Fumero, and Ka-\nmal Rahimi Malekshan. 2022. Clip-forge: Towards\nzero-shot text-to-shape generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18603–18613.\nPriyanka Sen and Emine Yilmaz. 2020. Uncertainty and\ntraffic-aware active learning for semantic parsing. In\nProceedings of the First Workshop on Interactive and\nExecutable Semantic Parsing, pages 12–17, Online.\nAssociation for Computational Linguistics.\nAli Shafahi, Mahyar Najibi, Mohammad Amin Ghi-\nasi, Zheng Xu, John Dickerson, Christoph Studer,\nLarry S Davis, Gavin Taylor, and Tom Goldstein.\n2019. Adversarial training for free! Advances in\nNeural Information Processing Systems, 32.\nAli Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson,\nLarry S Davis, and Tom Goldstein. 2020. Universal\nadversarial training. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 34, pages\n5636–5643.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen Jr, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699–7715.\nRichard Shin and Benjamin Van Durme. 2022. Few-\nshot semantic parsing with language models trained\non code. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5417–5425, Seattle, United States.\nAssociation for Computational Linguistics.\nFatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Shirui Pan,\nWeiqing Wang, Reza Haffari, Yuan-Fang Li, and Van\nNguyen. 2022. Paraphrasing techniques for maritime\nqa system. In 2022 25th International Conference on\nInformation Fusion (FUSION), pages 1–8. IEEE.\n1100\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for\nzero-shot and few-shot arbitrary textual style trans-\nfer with small language models. arXiv preprint\narXiv:2205.11503.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian J. Goodfellow, and\nRob Fergus. 2014. Intriguing properties of neural\nnetworks. In 2nd International Conference on Learn-\ning Representations, ICLR 2014, Banff, AB, Canada,\nApril 14-16, 2014, Conference Track Proceedings.\nMildred C Templin. 1957. Certain language skills in\nchildren: Their development and interrelationships,\nvolume 10. JSTOR.\nFlorian Tramer and Dan Boneh. 2019. Adversarial train-\ning and robustness for multiple perturbations. Ad-\nvances in Neural Information Processing Systems ,\n32.\nRobert A Wagner and Michael J Fischer. 1974. The\nstring-to-string correction problem. Journal of the\nACM (JACM), 21(1):168–173.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan,\nYu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah,\nand Bo Li. 2021. Adversarial glue: A multi-task\nbenchmark for robustness evaluation of language\nmodels. In Thirty-fifth Conference on Neural In-\nformation Processing Systems Datasets and Bench-\nmarks Track (Round 2).\nJunjie Wu. 2012. Advances in K-means clustering: a\ndata mining thinking. Springer Science & Business\nMedia.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 36, pages 3081–\n3089.\nMingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang,\nXin Jiang, Qun Liu, and Zhiming Ma. 2021. Im-\nproved ood generalization via adversarial training\nand pretraing. In International Conference on Ma-\nchine Learning, pages 11987–11997. PMLR.\nGU Yule. 1944. The statistical study of literary vocabu-\nlary. cambridge, cambridge [eng.].\nJohn M Zelle and Raymond J Mooney. 1996. Learning\nto parse database queries using inductive logic pro-\ngramming. In Proceedings of the national conference\non artificial intelligence, pages 1050–1055.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing,\nLaurent El Ghaoui, and Michael Jordan. 2019. Theo-\nretically principled trade-off between robustness and\naccuracy. In International conference on machine\nlearning, pages 7472–7482. PMLR.\nZhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.\nGenerating natural adversarial examples. In Interna-\ntional Conference on Learning Representations.\n1101\nA Experiments\nDatasets. The GeoQuery dataset contains 877 pairs of NL-LF pairs about U.S. geographical infor-\nmation. On the other hand, Scholar contains pairs of NL-SQL regarding information about academic\npublications. Finegan-Dollak et al. (2018) proposed a dataset split for evaluating the compositional\ngeneralization capability of semantic parsers on several datasets, including GeoQuery and Scholar.\nThe proposed split, referred to as the query-split, presents a more challenging scenario for semantic\nparsing models. This paper utilizes the query-split, where the two test sets in our experiments include 182\nNL-LF pairs from GeoQuery and 315 NL-LF pairs from Scholar, respectively, during the evaluation\nof the prompt-based semantic parser.\nHyperparameters. We sample at most 200 tokens from CODEX with temperature 0, with the following\nstrings used as stop tokens to halt generation: “--”, “\\n\\n”, “;”, “#”.\nModel Versioning. The version of the code-davinci-002 model referred to in this paper is as of\nthe midpoint of the year 2022.\nB Adversarial Examples\nTable 8 lists the examples generated by all perturbation strategies.\nLinguistic\nPhenomenon\nSamples (Strikethrough = Original Text, red = Adversarial Perturbation)\nTypo\n(Word-level) NL: what can you tellte11 me about theth e population of missouri\nRandom Deletion\n(Word-level) NL: what can you tell me about the population of missouri\nRandom Swap\n(Word-level) NL: what can you tell me aboutmissouri the population of missouriabout\nContext-aware\nSubstitution\n(Word-level)\nNL: what canwill you tell me about thea population of missouri\nContext-aware\nInsertion\n(Word-level)\nNL: what what can you tell me about the exact population of missouri\nRewriting\n(Sent.-level)\nNL: what can you tell me about the population of missouri\nWhat information can you provide on Missouri’s population?\nDistraction\n(Sent.-level)\nNL: what can you tell me about the population of missouri\nwho is who; what is what; when is when; which is which; where is\nwhere\nTable 8: Examples from Robustness Evaluation Set. The adversarial utterances in this Table are generated by\napplying various perturbation strategies to a single utterance “what can you tell me about the population of missouri”\nsampled from the GeoQuery dataset. All of the generated utterances can successfully alter Codex’s output.\n1102",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.6498360633850098
    },
    {
      "name": "Natural language processing",
      "score": 0.6472854018211365
    },
    {
      "name": "Computer science",
      "score": 0.6245587468147278
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.620225191116333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5400517582893372
    },
    {
      "name": "Computational linguistics",
      "score": 0.5249431133270264
    },
    {
      "name": "Linguistics",
      "score": 0.3596212863922119
    },
    {
      "name": "Philosophy",
      "score": 0.2088056206703186
    },
    {
      "name": "Chemistry",
      "score": 0.12328043580055237
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I42894916",
      "name": "Data61",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    }
  ],
  "cited_by": 26
}