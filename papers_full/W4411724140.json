{
  "title": "Performance of Large Language Models in the Non-English Context: Qualitative Study of Models Trained on Different Languages in Chinese Medical Examinations",
  "url": "https://openalex.org/W4411724140",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2151067403",
      "name": "Zhong Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289985365",
      "name": "Liantan Duan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127362594",
      "name": "Shuo Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102772809",
      "name": "Lingyi Chi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2581285444",
      "name": "Dongfang Sheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4393867901",
    "https://openalex.org/W4391301614",
    "https://openalex.org/W4399701665",
    "https://openalex.org/W4386776067",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4387242094",
    "https://openalex.org/W4387639242",
    "https://openalex.org/W4396498719",
    "https://openalex.org/W4387326101",
    "https://openalex.org/W4323360926",
    "https://openalex.org/W4400727220",
    "https://openalex.org/W4387776858",
    "https://openalex.org/W2067786443",
    "https://openalex.org/W2094052723",
    "https://openalex.org/W2397288399",
    "https://openalex.org/W4403689553",
    "https://openalex.org/W4404518047",
    "https://openalex.org/W4392669753",
    "https://openalex.org/W4389519817",
    "https://openalex.org/W4401424596",
    "https://openalex.org/W4389235809",
    "https://openalex.org/W4394782456",
    "https://openalex.org/W2897360548",
    "https://openalex.org/W4385827193",
    "https://openalex.org/W4386867830"
  ],
  "abstract": "Abstract Background Research on large language models (LLMs) in the medical field has predominantly focused on models trained with English-language corpora, evaluating their performance within English-speaking contexts. The performances of models trained with non–English language corpora and their performance in non-English contexts remain underexplored. Objective This study aimed to evaluate the performances of LLMs trained on different languages corpora by using the Chinese National Medical Licensing Examination (CNMLE) as a benchmark and constructed analogous questions. Methods Under different prompt settings, we sequentially posed questions to 7 LLMs: 2 primarily trained on English-language corpora and 5 primarily on Chinese-language corpora. The models’ responses were compared against standard answers to calculate the accuracy rate of each model. Further subgroup analyses were conducted by categorizing the questions based on various criteria. We also collected error sets to explore patterns of mistakes across different models. Results Under the zero-shot setting, 6 out of 7 models exceeded the passing level, with the highest accuracy rate achieved by the Chinese LLM Baichuan (86.67%), followed by ChatGPT (83.83%). In the constructed questions, all 7 models exceeded the passing threshold, with Baichuan maintaining the highest accuracy rate (87.00%). In few-shot learning, all models exceeded the passing threshold. Baichuan, ChatGLM, and ChatGPT retained the highest accuracy. While Llama showed marked improvement over previous tests, the relative performance rankings of other models stayed similar to previous results. In subgroup analyses, English models demonstrated comparable or superior performance to Chinese models on questions related to ethics and policy. All models except Llama generally had higher accuracy rates for simple questions than for complex ones. The error set of ChatGPT was similar to those of other Chinese models. Multimodel cross-verification outperformed single model, particularly improving accuracy rate on simple questions. The implementation of dual-model and tri-model verification achieved accuracy rates of 94.17% and 96.33% respectively. Conclusions At the current level, LLMs trained primarily on English corpora and those trained mainly on Chinese corpora perform similarly well in CNMLE, with Chinese models still outperforming. The performance difference between ChatGPT and other Chinese LLMs are not solely due to communication barriers but are more likely influenced by disparities in the training data. By using a method of cross-verification with multiple LLMs, excellent performance can be achieved in medical examinations.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6524481773376465
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5802205801010132
    },
    {
      "name": "Natural language processing",
      "score": 0.552265465259552
    },
    {
      "name": "Language model",
      "score": 0.5452142953872681
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5018625259399414
    },
    {
      "name": "English language",
      "score": 0.4772772192955017
    },
    {
      "name": "Word error rate",
      "score": 0.4657362997531891
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4438464045524597
    },
    {
      "name": "Linguistics",
      "score": 0.34666571021080017
    },
    {
      "name": "Psychology",
      "score": 0.3155287504196167
    },
    {
      "name": "Mathematics education",
      "score": 0.16377535462379456
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}