{
  "title": "Representation biases in sentence transformers",
  "url": "https://openalex.org/W4386576824",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2620492014",
      "name": "Dmitry Nikolaev",
      "affiliations": [
        "University of Stuttgart"
      ]
    },
    {
      "id": "https://openalex.org/A204135759",
      "name": "Sebastian Padó",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2062774885",
    "https://openalex.org/W3015777882",
    "https://openalex.org/W3095156104",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W2140676672",
    "https://openalex.org/W3188381677",
    "https://openalex.org/W4213288032",
    "https://openalex.org/W3177474387",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W3173727191",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4200031601",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2135964261",
    "https://openalex.org/W3088016618",
    "https://openalex.org/W4206361803",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W2980508402",
    "https://openalex.org/W4285192810",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3153130246",
    "https://openalex.org/W4404752334",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4231385726",
    "https://openalex.org/W2607879133",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W3173902720",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W2914406948"
  ],
  "abstract": "Variants of the BERT architecture specialised for producing full-sentence representations often achieve better performance on downstream tasks than sentence embeddings extracted from vanilla BERT. However, there is still little understanding of what properties of inputs determine the properties of such representations. In this study, we construct several sets of sentences with pre-defined lexical and syntactic structures and show that SOTA sentence transformers have a strong nominal-participant-set bias: cosine similarities between pairs of sentences are more strongly determined by the overlap in the set of their noun participants than by having the same predicates, lengthy nominal modifiers, or adjuncts. At the same time, the precise syntactic-thematic functions of the participants are largely irrelevant.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3701–3716\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nRepresentation biases in sentence transformers\nDmitry Nikolaev Sebastian Padó\nIMS, University of Stuttgart\ndnikolaev@fastmail.com pado@ims.uni-stuttgart.de\nAbstract\nVariants of the BERT architecture specialised\nfor producing full-sentence representations of-\nten achieve better performance on downstream\ntasks than sentence embeddings extracted from\nvanilla BERT. However, there is still little un-\nderstanding of what properties of inputs de-\ntermine the properties of such representations.\nIn this study, we construct several sets of sen-\ntences with pre-defined lexical and syntactic\nstructures and show that SOTA sentence trans-\nformers have a strong nominal-participant-set\nbias: cosine similarities between pairs of sen-\ntences are more strongly determined by the\noverlap in the set of their noun participants\nthan by having the same predicates, lengthy\nnominal modifiers, or adjuncts. At the same\ntime, the precise syntactic-thematic functions\nof the participants are largely irrelevant.\n1 Introduction\nTransformer-based encoder-only models derived\nfrom the BERT architecture and pre-trained us-\ning similar objective and training regimens (De-\nvlin et al., 2019; Liu et al., 2019) have become\nthe standard tool for downstream tasks at the level\nof individual tokens and token sequences (Tenney\net al., 2019; Wang et al., 2021). Whole-sentence\nrepresentations can also be easily extracted from\nthe outputs of these models by either using the\nembedding of the special [CLS] token, in cases\nwhere the model was trained on the next-sentence-\nprediction task, or averaging or max-pooling the\nembeddings of all tokens produced by the model\n(Zhelezniak et al., 2019). While both approaches\nare widely used in practice, it has been argued\nthat these representations are not well suited for\nsentence-level downstream tasks. Several modifica-\ntions to the architecture and training regime were\nproposed, which are known collectively as sentence\ntransformers (STs; Reimers and Gurevych, 2019).\nSTs have achieved state-of-the-art performance\non downstream tasks such as semantic search and\nquestion answering (Santander-Cruz et al., 2022;\nHa et al., 2021). Their analysis, however, has re-\nceived considerably less attention than the analysis\nof the vanilla BERT model and its variants (Rogers\net al., 2020; Conia and Navigli, 2022). In fact, these\nmodels are often considered to be uninterpretable\n(Minaee et al., 2021).\nA common feature of STs is that they are fine-\ntuned to produce similar vector-space representa-\ntions for semantically similar sentences. This ob-\njective induces a complex loss landscape shaped by\nthe available training data. The original Sentence-\nBERT model (Reimers and Gurevych, 2019) was\ntrained on natural language inference data, and sen-\ntences were considered to be semantically similar\nif their NLI label was that of entailment. SOTA\nmodels were trained on a much larger web-crawled\ncorpus including more than 1 billion sentence pairs\nmined from sources such as Reddit conversations,\nduplicate question pairs from WikiAnswers, etc.1\nThe richness and variability of this dataset begs the\nquestion of what notion of semantic similarity is\nimplicitly learned by the models trained on it.\nIn this study, we begin addressing this question\nthrough analysis of natural-looking synthetic sen-\ntences with controlled syntactic and lexical content.\nWe concentrate on three questions.\nFirst, we test if STs have part-of-speech biases.\nWe show that, all other things being equal, informa-\ntion provided by nouns plays more important role\nthan the information provided by verbs, both in\nsimple sentences and in sentences with coordinated\nverbal phrases.\nSecond, we compare the relative importance of\nthe overlap in the sets of participants in two sen-\ntences with that of how many participants have\nidentical syntactic functions. We show that raw\nlexical overlap is relatively more important than\nhaving the same nouns in the same syntactic slots.\n1See the list at https://huggingface.co/\nsentence-transformers/all-mpnet-base-v2\n3701\nThird, we check how strongly sentence represen-\ntations are affected by other sentential elements,\nsuch as adverbials and nominal modifiers of differ-\nent types and lengths. We show that, unlike BERT\nwith token averaging, STs seem to largely disregard\nthese components in favor of nominal participants.\nThe paper is structured as follows: § 2 presents\nthe methodology that we follow in our analyses\nand the models we employ; § 3 presents the case\nstudies and their results; § 4 provides an overall\ndiscussion; § 5 surveys related work; § 6 concludes\nthe paper.\n2 Methods and Experimental Setup\nWe experiment with representations produced\nby three models. Two are SOTA STs: all-\nmpnet-base-v2 (MPNET) is an instance of\nmpnet-base (Song et al., 2020) fine-tuned on\nthe 1B sentence-pair corpus using the training ar-\nchitecture from Reimers and Gurevych (2019);\nall-distilroberta-v1 (DistilRoberta) is\na distilled instance of roberta-base (Sanh\net al., 2019) fine-tuned in the same way.\nThe third model is the vanilla pre-trained\nbert-large-uncased (BERT), as a point of\ncomparison for the first two.\nAll models were downloaded from HuggingFace.\nStandard APIs from the Sentence Transformers\nlibrary2 were used to compute embeddings using\nMPNET and DistilRoberta; for the vanilla BERT\nmodel, we averaged the embeddings of all sentence\ntokens, including [CLS] and [SEP].3\nWe structure the presentation as a series of case\nstudies. For each case study, we construct a set of\nsentences controlled for lexical content and syntac-\ntic structure. Sentences are created in such a way as\nto be grammatically correct, look naturalistic, and\nas far as possible not bias the analysis.4 They are\narguably less complex and variable than examples\nsampled from real-word corpora; however, we be-\nlieve that an analysis based on simple sentences is\na reasonable first step towards a better understand-\ning of model representations, as previous work has\n2https://www.sbert.net/index.html,\nReimers and Gurevych (2019).\n3We experimented with omitting the special tokens, but\nthis led to sentence representations dominated by punctuation\nsigns and other undesired effects. In line with previous work\n(Ma et al., 2019), we also found that using [CLS] embeddings\nleads to bad results due to their high redundancy, and we do\nnot discuss them.\n4Sentence-generating and model-fitting scripts can be\nfound in the Supplementary Materials.\nshown for sentiment analysis (Kiritchenko and Mo-\nhammad, 2018) and syntactic analysis (Marvin and\nLinzen, 2018).\nFor each case study, we compute embeddings\nfor all sentences, together with cosine similarities\nbetween embeddings of sentence pairs. We analyze\nthe similarities by means of regression modelling.\nMore precisely, we regress cosine similarities, z-\nscored to improve comparability between encoders,\non the properties of sentence pairs, such as lexical\noverlap, presence of identical participants in identi-\ncal syntactic positions, or POS tags of participants.\nWe inspect the coefficients of the resulting regres-\nsion fits to assess the relative importance of these\nproperties. Since (almost) all properties are coded\nas binary variables, their magnitudes are directly\ncomparable in terms of importance.\nFor terminological clarity, we will use the term\nmodels to refer to the regression models we use to\nanalyse the impact of sentence properties on rep-\nresentational similarity. We call the transformers\ncomputing these embeddings encoders.\nWhere the features of sentence pairs can be\nstraightforwardly related to simple properties of\nindividual sentences (e.g., in case when we are\ntesting if they have the same subject or direct ob-\nject), we also project sentence embeddings on a\n2-D surface using UMAP (McInnes et al., 2018)5\nand check if the spatial organisation of the points\nis in line with our observations.\nLexical choice A potential confound of our ex-\nperimental setup is lexical choice, which is never\ncompletely neutral. For example, by taking a se-\nmantically close pair of verbs, we can considerably\nreduce the effect of predicate mismatch between\ntwo sentences. Moreover, encoders can react id-\niosyncratically to particular words and word com-\nbinations. Including all combinations of words and\ntheir positions in sentence pairs as predictor vari-\nables is not a solution, however, as it defeats the\npurpose of identifying structural patterns and, in\nthe limit, amounts to replicating the encoders. We\naddress this confound in three ways.\nFirst, we select nouns to be always at least as\ninterchangeable as words of other parts of speech\nin terms of belonging to similar mid-to-high fre-\nquency bands and referring to conceptually simple,\nconcrete objects. This follows from our working\nhypothesis that encoders give preferential treatment\n5We use the default settings and pairwise cosine dissimi-\nlarities as distance measure.\n3702\nto nominal elements, whose (generally entity re-\nferring) semantics is arguably easier to capture\nthan, for example, that of (generally event refer-\nring) verbs (Baroni and Lenci, 2011).\nSecond, we compare the analysis of the ST en-\ncoders against the analysis of the vanilla BERT en-\ncoder. As they are derived from averaging, vanilla\nBERT embeddings treat all words equally, so if our\nsentences, e.g., undersell differences in adverbs\nbecause we chose two nearly synonymous ones,\nthis should be visible in the small coefficient track-\ning the impact of adverbs in the regression model\nbased on BERT embeddings. As will be shown\nbelow, however, the hierarchy of coefficients for\nregression models of STs is very different from that\nfor vanilla BERT, which arguably indicates that the\nrole of lexical effects is minor.\nThird, we re-run all reported models on sen-\ntences of the same structure with different lexical\ncontent; see the Appendix for details. We observe\nhigh stability of coefficients across replications,\nhigher for STs than for vanilla BERT. This further\ncorroborates the validity of our generalisations.\n3 Case Studies\nThis section presents a series of case studies testing\nthe sensitivity of embeddings produced by sentence\ntransformers and BERT token averages to proper-\nties of input sentences. We start with analysing\nsimple intransitive sentences (§ 3.1) and simple\ntransitive sentences (§ 3.2). We then make specific\naspects of the structure more complex, analysing\nthe effect of lengthy NPs (§ 3.3) and coordinated\nVPs (§ 3.4). Finally, we look more closely at the\nsyntax-semantics interface by inverting the proto-\ntypical alignment of POS tags and syntactic func-\ntions (predicative nominals and gerund subjects,\n§ 3.5) and by testing the degree to which encoders\ntrack particular syntactic functions of verb argu-\nments (§ 3.6).\n3.1 Simple Intransitive Sentences\nData The main goal of the analysis of simple\nintransitive sentences is to check the relative con-\ntribution of their components to their embeddings.\nWe study a nearly-minimal sentence template with\na nominal subject, an adverbial adjunct, and an in-\ntransitive verb. We construct a set of 256 sentences\nof the form ‘[det] [subj] [adverb] [verb][punct]’,\nwhere det ranges over {a, the}; subj ranges over\nmpnet distilroberta bert\nSameDet 0.07 0.07 0.37\nSameAdv 0.33 0.31 0.45\nSamePred 0.74 0.61 0.58\nSamePunct 0.24 0.24 0.84\nSameSubj 2.26 2.40 1.27\nR-squared 0.67 0.71 0.48\nTable 1: A summary of the models predicting z-scored\npairwise cosine similarities between embeddings of sen-\ntences with intransitive verbs. All coefficients are sig-\nnificant with p <0.001.\na set of nouns, 6 adverb ranges over { quickly,\nslowly}, verb ranges over { appears, vanishes,\nstops, moves}, and punct, over {., !}. Here and in\nsubsequent experiments, the generation procedure\nassures that all sentence features are statistically\nindependent, which is a crucial prerequisite for\nlinear-regression modelling.\nModel The regression model matrix is based on\n32,640 pairs of generated sentences, which differ in\nthe value of at least one feature, with predictor vari-\nables SameDeterminer, SameAdverb, SameVerb,\nSamePunct, and SameSubj. We regress z-score-\ntransformed cosine similarities between sentence\nembeddings computed by three different encoders\non these predictor variables. The coefficients of the\nfitted models are shown in Table 1.7\nResults Three observations from Table 1 hold for\nall subsequent analyses.\n(i) The coefficients are positive for all models\nand all features. This means that sentence pairs\nwhich agree in some constituent are always more\nsimilar than sentence pairs that do not – as ex-\npected.\n(ii) The coefficient of determination ( R2) is\nlarger for ST-focused linear models. This means\nthat the embeddings computed by the ST encoders\nare more dependent on the features of the sentences\nwe track and less dependent on identities of lexical\nunits. (It can be noted that the fact that we achieve\nR2 ≈ 0.7 using only a few structural properties is\nremarkable in itself.)\n(iii) The differences among coefficients of the\nST-focused linear models are in general larger than\n6{cat, dog, artist, teacher, planet, star, wind, rain}\n7Replication models, fitted on sentences with the same\nstructure but different lexical content, are shown in Table 8 in\nthe Appendix.\n3703\n10\n 0 10\n0\n10\nmpnet\n0.0 2.5 5.0 7.5\n2.5\n5.0\n7.5\nbert\nsubj\ncat\ndog\nartist\nteacher\nplanet\nstar\nwind\nrain\n10\n 0 10\n0\n10\nmpnet\n0.0 2.5 5.0 7.5\n2.5\n5.0\n7.5\nbert\nadv\nquickly\nslowly\n10\n 0 10\n0\n10\nmpnet\n0.0 2.5 5.0 7.5\n2.5\n5.0\n7.5\nbert\npred\nappears\nvanishes\nstops\nmoves\n10\n 0 10\n0\n10\nmpnet\n0.0 2.5 5.0 7.5\n2.5\n5.0\n7.5\nbert\npunct\n.\n!\nFigure 1: UMAP projections of embeddings of sen-\ntences with intransitive verbs (left: sentence transformer,\nright: BERT).\nthose of the linear model analysing BERT: in the\nlatter, the biggest coefficient (1.27 for SameSubj)\nis only ≈ 3.5 times higher than the smallest one\n(0.37 for SameDet), while for the ST models this\nratio is above 30. This is connected to the fact that\nBERT-derived sentence representations are more\ndependent on semantically impoverished elements,\nsuch as determiners and punctuation signs, which\ndampen the effect of other constituents. For the\nsake of brevity, we do not analyse determiners and\npunctuation in subsequent experiments and keep\nthem constant as the and . respectively.\nTurning to the comparison of coefficients inside\nmodels, we see that STs pay considerably more\nattention to subjects than to predicates: all things\nbeing equal, sentences with different predicates and\nadverbs but the same subject will be more similar\nthan sentences with the same predicate and adverb\nand different subjects. The influence of punctuation\nis surprisingly strong, being comparable to that of\nadverbs, while the effect of determiners is very\nweak, albeit statistically significant.\nA plot of UMAP projections of sentence em-\nbeddings produced by MPNET and BERT, shown\nin Figure 1, underlines that while averaged BERT\nembeddings distinguish punctuation signs but do\nnot distinguish subjects, the situation is reversed\nfor the sentence transformer: it distinguishes sub-\njects cleanly but largely abstracts away from other\nstructural properties.\n3.2 Transitive Sentences\nData The transitive sentences used in the anal-\nysis are generated using the following template:\n‘The [subj] [adverb] [verb] the [obj].’ The range of\nnouns was slightly extended;8 the same adverbs as\nin the previous experiment were used, while verb\nranged over {sees, chases, draws, meets, remem-\nbers, pokes}. This produces 672 different sentences\nand 225,456 sentence pairs.\nModel The coding for SameAdv and SamePred\nremains as above. The main focus in this study is\non whether sentence similarities are dominated by\nthe sentences having the same subject, the same\ndirect object, or the same words in these two po-\nsitions even if their order were reversed. To test\nfor this, we added a categorical variable with the\nfollowing values:\n00 no overlap in subject and object (the baseline);\nA0 same subject, different objects;\n0B same object, different subjects;\n0A the subject of the first sentence is the object\nof the second;\nB0 the object of the first sentence is the subject\nof the second;\nBA subject and object are swapped;\nAB the same subject and object.\nResults A summary of the fitted models is given\nin Table 2.9 It demonstrates that when it comes to\nsimple transitive sentences, our understanding of\ntheir embeddings produced by sentence transform-\ners remains high, despite the sentences being more\ncomplex (R2 ≈ 0.7), while BERT embeddings\nbecome more unpredictable ( R2 ≈ 0.31). Fur-\nthermore, while BERT again essentially treats all\ntokens more or less equally, with adverbs slightly\ndiscounted, STs prioritise participants (even B0 has\nhigher coefficients than SamePred).\nOn the other hand, neither BERT nor STs priori-\ntise the exact syntactic function of the participants:\ncoefficients for A0 vs. 0A, 0B vs. B0, and AB vs.\n8To {cat, dog, teacher, artist, robot, machine, tree, bush,\nplanet, star, wind, rain}.\n9A summary of the replication model fits is provided in\nTable 9 in the Appendix.\n3704\nmpnet distilroberta bert\nSameAdv 0.49 0.36 0.56\nSamePred 0.73 0.42 0.78\nSubjObj_0A 1.27 1.40 0.65\nSubjObj_0B 1.31 1.45 0.69\nSubjObj_A0 1.44 1.45 0.75\nSubjObj_AB 2.98 3.08 1.60\nSubjObj_B0 1.37 1.42 0.58\nSubjObj_BA 2.85 2.98 1.39\nR-squared 0.74 0.73 0.31\nTable 2: A summary of the models predicting z-scored\npairwise cosine similarities between embeddings of sen-\ntences with transitive verbs. All coefficients are signifi-\ncant with p <0.001.\nBA are largely comparable across all models with\nBA ≈ A0 + 0B. That is, the effects of subjects\nand objects are largely independent of one another.\nA UMAP plot with the embeddings for the tran-\nsitive sentences is shown in Figure 2 in the Ap-\npendix. It demonstrates that STs arrive at a much\nmore fine-grained clustering of sentences, largely\ndominated by subjects and objects. They largely\ndiscount predicates and adverbs which are quite\nprominent in averaged BERT embeddings.\n3.3 Transitive Sentences with Long NP\nModifiers\nThe previous analyses showed that representations\ncomputed by STs are highly attuned to verb par-\nticipants but not to their particular syntactic roles.\nThis may mean that ST may be potentially misled\nby nouns in other positions in the sentence, which\nhave less relevance to the described situation. This\nstudy explores this possibility.\nData We repeat the analysis from § 3.2 using\nthe template of the form ‘The [subj] [modifier]\n[adverb] [verb] the [obj]’, with a smaller set of sub-\njects,10 and the modifier ranging over {with big\nshiny eyes, that my brother saw yesterday, whose\nphoto was in the papers , worth a great deal of\nmoney}. Altogether this gives 1,440 sentences and\n1,036,080 sentence pairs. The modifiers have inter-\nnal syntactic structure and contain a non-negligible\namount of lexical material that the models have to\n‘skip over’ if their representations were focused on\nthe participant structure of the matrix clause.\n10{cat, dog, rat, giraffe, wombat, hippo}\nmpnet distilroberta bert\nSameMod 1.01 1.02 1.62\nSameAdv 0.40 0.42 0.27\nSamePred 0.89 0.67 0.40\nSubjObj_0A 0.83 1.06 0.32\nSubjObj_0B 0.97 1.27 0.42\nSubjObj_A0 1.11 1.14 0.53\nSubjObj_AB 2.14 2.44 1.00\nSubjObj_B0 1.20 1.30 0.54\nSubjObj_BA 2.09 2.40 0.91\nR-squared 0.73 0.81 0.61\nTable 3: A summary of the models predicting z-scored\npairwise cosine similarities between embeddings of sen-\ntences with transitive verbs and lengthy subject modi-\nfiers. All coefficients are significant with p <0.001.\nModel The same coding strategy as in the preced-\ning section is used, augmented by a new binary vari-\nable, SameMod, tracking whether two sentences\nhave the same modifier for the subject.\nResults Both the model coefficients, shown in\nTable 3, and the UMAP plot, shown in Figure 3\nin the Appendix, indicate that BERT embeddings\nare highly sensitive to lengthy modifiers: 11 the\nSameMod coefficient in the linear model is larger\nthan the coefficients for the same predicate and the\nsame subject-object combination added together.\nThe situation is very different for STs: SameMod\nis more important than SamePred, especially for\nDistilRoberta, but, with one exception, not more\nimportant than even a partial overlap in participants.\nHaving the same participants, in either the same or\nswapped syntactic functions, is more than twice as\nimportant. We take this as evidence that STs have\na specific bias towards matrix-clause participant\nsets, that is, the nouns that fill a thematic role of the\nmain predicate, while their precise functions and\nnouns found in other positions in the sentence are\nless important.\n3.4 Coordinated Verbal Phrases\nThe analyses presented above show that the main\npredicate of the sentence has only a limited influ-\nence on the representations computed by STs, com-\npared to its subjects and objects. Here, we show\nthat this effect still holds if there is more than one\nmain predicate.\n11The results of the replication fits are shown in Table 10 in\nthe Appendix.\n3705\nmpnet distilroberta bert\nV1Same 0.41 0.26 0.21\nV2Same 0.13 0.08 0.23\nV3Same 0.36 0.34 0.41\nN1Same 0.33 0.35 0.23\nN2Same 0.12 0.22 0.30\nN3Same 0.56 0.57 0.41\nR-squared 0.11 0.1 0.09\nTable 4: A summary of the models predicting z-scored\npairwise cosine similarities between embeddings of sen-\ntences with coordinated VPs from binary predictors. All\ncoefficients are significant with p <0.001.\nData Using the same sets of nouns and transi-\ntive verbs as in the previous experiment, we con-\nstruct sentences of the form ‘The man [verb1]\nthe [noun1], [verb2] the [noun2], and [verb3] the\n[noun3]’, where triples of verbs and nouns are\ntaken from the Cartesian product of the sets of\nall noun and verb combinations of size 3 without\nreplacement. To alleviate a possible ordering bias,\nall verb and noun triples are shuffled for each sen-\ntence. This results in 400 sentences and 79,800\nsentence pairs.\nModels and results The analysis proceeds in\nthree stages. First, we check if positions 1, 2, and\n3 have different importance by regressing the nor-\nmalised cosine similarity on six binary variables\nN[oun]1Same, V[erb]1Same, N2Same, etc. The\nmodels, summarised in Table 4,12 show low coeffi-\ncients of determination (with R2 around 0.1), but\nthey indicate that positions are of unequal impor-\ntance: BERT gives more weight to the last noun\nand the last verb, while STs focus on the first and\nthe last N-V pair and largely ignore the second one.\nA significantly better fit can be achieved by re-\nplacing binary predictors with overlap scores for\nnouns and verbs. As Table 5 13 shows, this type\nof model, even though it contains only 2 variables\ninstead of 6, obtains R2 ≈ 0.65 for STs. It is also\nevident that all three models place more weight\non noun overlap than on verb overlap, with Dis-\ntilRoberta showing the biggest difference between\nthe two.\nThis raises the question of whether particular\nverb-noun collocations play a noticeable role, i.e.,\n12A summary of the replication fits is given in Table 11 in\nthe Appendix.\n13See Table 12 in the Appendix for the replication fits.\nmpnet distilroberta bert\nVerbOverlap 0.78 0.59 0.64\nNounOverlap 0.93 1.09 0.88\nR-squared 0.65 0.68 0.52\nTable 5: A summary of the models predicting z-scored\npairwise cosine similarities between embeddings of sen-\ntences with coordinated VPs from overlap scores. All\ncoefficients are significant with p <0.001.\nif a sentence containing chases the wombat will\nbe considerably more similar to another sentence\ncontaining the exact phrase compared to a sentence\ncontaining chases and wombat but not as a trigram.\nSimply adding n-gram overlap scores to the model\nis not possible, however, because it is highly cor-\nrelated with both noun overlap and verb overlap.\nIn order to obviate this obstacle, we first construct\nan auxiliary linear model predicting trigram over-\nlap from noun and verb overlap and then use the\nresiduals of this regression in the main model.\nThe results are ambiguous: on one hand, the\ncoefficient for residualised trigram overlap is sta-\ntistically significant with p <0.001. On the other\nhand, the effect is very weak (more than ten times\nweaker than that of either noun overlap or verb\noverlap), and the addition of trigram overlap to the\nmodel improves R2 by less than 0.001. This seems\nto indicate that trigram overlap is not important for\npractical purposes.\n3.5 Predicative Nominals with Gerund\nSubjects\nA potential weak point of our analysis is that parts\nof speech and syntactic functions are not decoupled:\nit is not yet clear whether the encoders pay attention\nto nouns or to subjects and objects.\nData To address this issue, we construct another\nset of sentences where the subject is a gerund and\nthe predicate is nominal. The template is ‘[gerund]\n[object] [copula] a [adjective] [predicate]’, where\ngerund ranges over { continuing, abandoning,\nstarting, completing}, object ranges over {it, them,\nthe project, the plan}, copula is one of {is, was, will\nbe, is going to be}, adjectives are {big, real, negli-\ngible, insignificant}, and the predicative nominal\nranges over {solution, mistake, failure, triumph}.\nThis gives 1024 sentences and 523,776 sentence\npairs. A variable copula provides an additional test\nas to whether the sentence encoders can recognise\n3706\nmpnet distilroberta bert\nSameSubj 0.82 0.70 0.31\nSameCop 0.35 0.30 0.55\nSameAdj 0.58 0.79 0.50\nSamePred 0.99 1.01 0.52\nSameObjNoun 1.01 1.04 0.60\nSameObjPron 0.44 0.50 0.42\nR-squared 0.50 0.54 0.22\nTable 6: A summary of the models predicting z-scored\npairwise cosine similarities between embeddings of sen-\ntences with gerund subjects and nominal predicates. All\ncoefficients are significant with p <0.001.\nmulti-word sequences with low semantic content.\nModel The sentence pair encoding includes four\nbinary variables (SameSubj, SameCop, SameAdj,\nSamePred) and a nominal variable for the direct ob-\nject, indicating whether objects are different (base-\nline), are identical and pronominal (SamePron), or\nare identical and nominal (SameNoun).\nResults The results in Table 614 demonstrate that\nall models treat both nominal predicates and nom-\ninal direct objects as more important than gerund\nsubjects. STs, moreover, pay less attention to iden-\ntical pronominal objects and discount multi-word\ncopula forms. R2 values for the ST model are\nlower than in the previous experiments (in the 0.50–\n0.55 range), which may potentially indicate a poor\nchoice of lexical items; however, replication ex-\nperiments with a different set of words (except for\ncopula forms) achieved comparable results. This\nsuggests that embeddings of sentences of this type\nare less easily explainable as additive combinations\nof individual words compared to the sentence types\nsurveyed previously.\n3.6 Revisiting Participant Sets: Ditransitive\nSentences\nOur final experiment revisits the opposition be-\ntween lexical overlap in verbal phrases and exact\nargument-predicate matching. In this case, we fo-\ncus on ditransitive verbs with two arguments: a\ndirect object and an oblique object which is an\nintegral part of the situation.15\n14See an overview of replication fits in Table 13 in the\nAppendix.\n15Many English ditransitive verbs can undergo the ‘dative\nalternation’, which swaps the oblique object with a preposi-\ntional phrase: Give the book to me/John vs. Give me/John a\nmpnet distilroberta bert\nSameAdv 1.05 1.07 0.64\nSamePred 0.93 0.64 0.83\nOverlap 0.90 1.00 0.91\nSPCRes 0.03 0.02 0.10\nR-squared 0.745 0.738 0.57\nR-squared\n(w/o SPCRes) 0.744 0.737 0.56\nTable 7: A summary of the models predicting z-scored\npairwise cosine similarities between embeddings of sen-\ntences with ditransitive verbs. SPCRes stands for Same-\nPosCountRes, i.e. the residuals of the number of iden-\ntical words in identical positions regressed on lexical\noverlap. All coefficients are significant with p <0.001.\nData All permutations of the triple of basic nouns\n{cat, dog, rat} are generated. For each permuta-\ntion, all three nouns are, in turn, replaced with\none of the members of the set of extra nouns { gi-\nraffe, wombat, hippo}; the original permutations\nare also used. This provides a set of unique triples\nof nouns where each pair of triples has from one\nto three nouns in common. The Cartesian product\nof this set of triples with a set of ditransitive verbs\n({describes, sells, shows}) and a set of adverbs\n({happily, quickly, secretly) is used to fill the tem-\nplate ‘The [noun1] [adverb] [verb] the [noun2] to\nthe [noun3].’ This procedures gives 540 sentences\nand 145,530 sentence pairs.\nModel The sentence pairs are coded for same\nadverb, same predicate, the number of matching\nnouns in matching positions (SamePosCount), and\nlexical overlap minus 1 (the baseline value of 0\ncorresponds to overlap of 1; each successive value\ncorresponds to increase in overlap). As with over-\nlapping words and trigrams above, these predictors\nare correlated. Therefore, we residualise Same-\nPosCount after regressing it on lexical overlap.\nResults Table 7 is inconclusive in a similar way\nto results from § 3.5. The coefficients for residu-\nalised SamePosCount are significant; however, in\nthe ST models, their size is very small, and Same-\nPosCount does not materially improve the predic-\ntive power. We conclude, therefore, that syntactic\npositions do not matter a great deal, in line with\nour ‘participant set’ interpretation from § 3.4.\nbook (Levin, 1993). Of the verbs we use, show and sell partic-\nipate in it, and the status of describe varies across speakers.\n3707\n4 Discussion\nOur analysis arguably goes some way towards ex-\nplaining why sentence transformers beat vanilla\nBERT-based models with token averaging on\nsentence-modelling tasks. Token averaging makes\nit impossible to distinguish between semantically\nrich and impoverished sentence elements, nor be-\ntween syntactically central vs. peripheral elements:\npunctuation signs and determiners contribute on\nthe same level as the matrix-clause predicate and\nmain participants, while lengthy modifiers, such\nas relative clauses, and multi-word copula forms\ndominate the representation.\nSentence transformers, on the other hand, learn\nto discount elements that only serve a grammatical\nfunction or present background information and fo-\ncus instead on the semantic kernel of the sentence.\nThe latter is in effect largely synonymous with the\nset of nominal elements in the main clause, first\nof all participants, but also predicative nominals.\nImportantly, despite their evident syntactic-analytic\ncapabilities (e.g., in our setting they can distinguish\nbetween participants of main and relatives clauses\nand between main and auxiliary verbs), STs seem\nto not pay much attention to the distinction between\nsubjects and direct or indirect objects. Instead they\nprioritise raw overlap in the set of nominal partic-\nipants of the matrix clause. This can be seen, by\nslightly abusing terminology of theoretical linguis-\ntics, as a focus on the aboutness/topic of sentences,\nwhat things they describe, and not on their predica-\ntion/comment, what they actually say about those\nthings (Hu and Pan, 2009).\nWe believe that this focus is not inherent to the\narchitecture of sentence transformers but reflects\nthe nature of the datasets used for fine-tuning STs.\nThe size of these datasets makes it impossible to\nconvincingly reason about their contents, but their\ngenres (QA pairs, Reddit threads, etc.) makes it\nplausible to expect a high degree of topic-based\noverlap: questions and conversations tend to re-\nvolve around entities (persons and things), with\ntheir actions and properties repeating less often.\nThis naturally leads to a focus on nouns referring\nto prominent entities, which are known to appear\npreferentially as subjects or objects for reasons of\ncoherence (Barzilay and Lapata, 2008), arguably a\ngood match to the patterns we observe.\n5 Related Work\nAnalysis of transformer-based models for sentence-\nlevel tasks, such as NLI, question answering, or\ntext classification, has largely followed the same\napproaches as found in the general BERTology\n(Rogers et al., 2020): probing, analysis of the ge-\nometry of the embedding space, extraction of parts\nof input that are particularly important for model\nperformance, and behavioural analysis. In this vein,\nLiu et al. (2021) and Peyrard et al. (2021) analyse\nthe attention patterns powering the performance of\ntransformer models on different types of sentence\nclassification, and Li et al. (2020) show that embed-\ndings of sentences computed by BERT-based mod-\nels, including siamese-fine-tuned sentence trans-\nformers, are anisotropic and can be improved via\nnormalisation. Chrysostomou and Aletras (2021)\nsurvey the existing methods for extracting ratio-\nnales from input sentences in the context of text\nclassification and propose an improved approach,\nwhile Luo et al. (2021) demonstrate that sentence\nembeddings derived by averaging BERT token rep-\nresentations suffer from artefacts arising from po-\nsitional embeddings. Zhelezniak et al. (2019) ar-\ngue that averaging should be replaced with max-\npooling.\nVery similar to ours is the approach adopted by\nMacAvaney et al. (2022), who construct a series of\nprobes to analyse the performance of several mod-\nels on the task of information retrieval. While their\nmethodology relies on high-level document statis-\ntics and wholistic document manipulation (word\nand sentence shuffling, token-frequency similar-\nity between the document and the query, textual\nfluency, etc.), our study analyses the role of lin-\nguistically motivated structural factors and thus\ncomplements their findings.\nOpitz and Frank (2022) aim at directly decom-\nposing the representations produced by sentence\ntransformers into several parts capturing different\nproperties of sentences reflected in AMR annota-\ntions (presence of negation, concepts included in\nthe sentence, etc.). While our study tries to as-\ncertain what meaning components dominate the\nrepresentations, Opitz and Frank assume that these\ncomponents are known in advance and are equally\nimportant: sentence embeddings in their modified\nSBERT model are split into 15 segments, each of\nwhich corresponds to one AMR-based meaning\ncomponent, plus a residual part to capture every-\nthing not covered by AMR annotations.\n3708\n6 Conclusion\nThis paper aims at making a contribution towards\na better understanding of sentence transformers,\nwhich are often seen as black boxes. We have\ndemonstrated that we can make surprisingly precise\ninferences about sentence-pair similarities using\nsimple linguistic features such as lexical overlap.\nThe crucial difference between bag-of-words dis-\ntributional models and current encoders is that STs\nhave became quite adept at disregarding ‘irrelevant’\nparts of the sentence and concentrating on its key\nelements. Unlike vanilla BERT sentence embed-\ndings obtained by token averaging, STs yield more\nstructured embeddings that focus on the matrix\nclause and are less tied to individual lexical items\nand strings of function words.\nThis progress, however, comes with a particu-\nlar type of bias: the structures that lead to high\nsentence similarity in STs, i.e. the overlap in nomi-\nnal ‘participant sets’, seem to mirror the dominant\ntype of paraphrases found in the data the STs were\ntuned on, and STs are not compelled to look at\nfiner structures of input sentences. At least without\nfurther fine tuning, this would appear to make them\nunsuitable for downstream tasks that require knowl-\nedge about more fine-grained aspects of sentence\nstructure, such as semantic roles (Conia and Nav-\nigli, 2022), or extra-propositional aspects, such as\nmonotonicity, negation, or modality (Yanaka et al.,\n2021; Nakov, 2016).\nAn interesting direction for future research\nwould be to explore the ways of decomposing sen-\ntence representations into additive aspects such as\nparticipant structure, main predication, etc. The\nadditional challenge here is that while theoretical\nsemantics has a lot to say about aspects of sentence\nmeaning (Pagin, 2016), there remains a lack of\nanalysis linking the notion of one-dimensional se-\nmantic similarity (Agirre et al., 2012) that underlies\nthe optimisation of current sentence transformers\nwith theoretically more substantial concepts.\nLimitations\nThe limitations of the proposed analysis are the\nfollowing:\n1. The analysis is based on synthetic data. This\nallows us to fully control the sentence struc-\nture and use balanced lexical material, but it\ndoes not necessarily reflect the performance\nof models on real-world data, especially when\nsentences or text fragments are much longer.\nHowever, synthetic data have generally shown\nto be a good first step toward understanding\nthe behaviour of complex models.\n2. The analysis does not cover graded distinc-\ntions between words, i.e. we did not experi-\nment with filling the slots with synonymous\nwords, as opposed to completely unrelated\nwords. This makes it impossible to decide if\nthe models are sensitive to word identities or\nto their actual semantics, as long as these two\nnotions are distinguishable.\n3. The outputs of the models are interpreted\nusing linear regression analysis anchored to\nthe properties of synthetic sentences. This\nkind of analysis makes it possible to disentan-\ngle additive effects of different components\nof sentence structure and provides statistical-\nsignificance estimates, while high R2 values\nindicate that our findings have some valid-\nity. However, it cannot fully account for the\nlexical effects (which we tried to safeguard\nagainst by carefully selecting template fillers),\nnon-linear effects, and hidden collinearity pat-\nterns (beyond those we addressed using resid-\nualised analysis).\n4. The range of models analysed in the paper is\nrestricted. It covers some amount of variabil-\nity (sentence transformers vs. vanilla BERT;\ntwo different variants of a base model for STs,\none of them distilled), but other combinations\nof model architecture and training/fine-tuning\nregime can lead to different outcomes.\nReferences\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. SemEval-2012 task 6: A\npilot on semantic textual similarity. In *SEM 2012:\nThe First Joint Conference on Lexical and Compu-\ntational Semantics – Volume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation (SemEval 2012), pages 385–\n393, Montréal, Canada. Association for Computa-\ntional Linguistics.\nMarco Baroni and Alessandro Lenci. 2011. How we\nBLESSed distributional semantic evaluation. In Pro-\nceedings of the GEMS 2011 Workshop on GEomet-\nrical Models of Natural Language Semantics, pages\n1–10, Edinburgh, UK. Association for Computational\nLinguistics.\n3709\nRegina Barzilay and Mirella Lapata. 2008. Modeling\nlocal coherence: An entity-based approach. Compu-\ntational Linguistics, 34(1):1–34.\nGeorge Chrysostomou and Nikolaos Aletras. 2021.\nVariable instance-level explainability for text clas-\nsification. arXiv preprint arXiv:2104.08219.\nSimone Conia and Roberto Navigli. 2022. Probing for\npredicate argument structures in pretrained language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4622–4632, Dublin,\nIreland. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nThi-Thanh Ha, Van-Nha Nguyen, Kiem-Hieu Nguyen,\nKim-Anh Nguyen, and Quang-Khoat Than. 2021.\nUtilizing SBERT for finding similar questions in\ncommunity question answering. In 2021 13th In-\nternational Conference on Knowledge and Systems\nEngineering (KSE), pages 1–6. IEEE.\nJianhua Hu and Haihua Pan. 2009. Decomposing the\naboutness condition for Chinese topic constructions.\nThe Linguistic Review, 26:371–384.\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining gender and race bias in two hundred senti-\nment analysis systems. In Proceedings of the Sev-\nenth Joint Conference on Lexical and Computational\nSemantics, pages 43–53, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nBeth Levin. 1993. English verb classes and alterna-\ntions: A preliminary investigation . University of\nChicago Press.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nShengzhong Liu, Franck Le, Supriyo Chakraborty, and\nTarek Abdelzaher. 2021. On exploring attention-\nbased explanation for transformer models in text clas-\nsification. In 2021 IEEE International Conference\non Big Data (Big Data), pages 1193–1203. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZiyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021.\nPositional artefacts propagate through masked lan-\nguage model embeddings. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5312–5327, Online. Association\nfor Computational Linguistics.\nXiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallap-\nati, and Bing Xiang. 2019. Universal text representa-\ntion from BERT: An empirical study. arXiv preprint\narXiv:1910.07973.\nSean MacAvaney, Sergey Feldman, Nazli Goharian,\nDoug Downey, and Arman Cohan. 2022. ABNIRML:\nAnalyzing the behavior of neural IR models. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:224–239.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nLeland McInnes, John Healy, and James Melville.\n2018. UMAP: Uniform manifold approximation\nand projection for dimension reduction. ArXiv,\nabs/1802.03426.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-\njes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.\n2021. Deep learning–based text classification: A\ncomprehensive review. ACM Computing Surveys\n(CSUR), 54(3):1–40.\nPreslav Nakov. 2016. Negation and modality in machine\ntranslation. In Proceedings of the Workshop on Extra-\nPropositional Aspects of Meaning in Computational\nLinguistics (ExProM), page 41, Osaka, Japan. The\nCOLING 2016 Organizing Committee.\nJuri Opitz and Anette Frank. 2022. SBERT studies\nmeaning representations: Decomposing sentence em-\nbeddings into explainable semantic features. In Pro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 625–638, Online only. Association for\nComputational Linguistics.\nPeter Pagin. 2016. Sentential semantics. In Maria Aloni\nand Paul Dekker, editors, Cambridge Handbook of\nFormal Semantics, Cambridge Handbooks in Lan-\nguage and Linguistics, pages 65–105. Cambridge\nUniversity Press.\nMaxime Peyrard, Beatriz Borges, Kristina Gligori´c, and\nRobert West. 2021. Laughing heads: Can transform-\ners detect what makes a sentence funny? arXiv\npreprint arXiv:2105.09142.\n3710\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nYamanki Santander-Cruz, Sebastián Salazar-Colores,\nWilfrido Jacobo Paredes-García, Humberto\nGuendulain-Arenas, and Saúl Tovar-Arriaga. 2022.\nSemantic feature extraction using SBERT for\ndementia detection. Brain Sciences, 12(2):270.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. MPNet: Masked and permuted pre-\ntraining for language understanding. In Proceedings\nof NeurIPS, pages 16857–16867.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,\nZhongqiang Huang, Fei Huang, and Kewei Tu. 2021.\nImproving named entity recognition by external con-\ntext retrieving and cooperative learning. CoRR,\nabs/2105.03654.\nHitomi Yanaka, Koji Mineshima, and Kentaro Inui.\n2021. SyGNS: A systematic generalization testbed\nbased on natural language semantics. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 103–119, Online. Association\nfor Computational Linguistics.\nVitalii Zhelezniak, Aleksandar Savkov, April Shen,\nFrancesco Moramarco, Jack Flann, and Nils Y . Ham-\nmerla. 2019. Don’t settle for average, go for the max:\nFuzzy sets and max-pooled word vectors. CoRR,\nabs/1904.13264.\n3711\nmpnet distilroberta bert\nSameDet 0.08 0.11 0.26\nSameAdv 0.38 0.38 0.96\nSamePred 1.02 0.95 0.49\nSamePunct 0.18 0.26 0.64\nSameSubj 2.15 2.17 0.65\nR-squared 0.71 0.71 0.43\nTable 8: A summary of the replication models predicting\nz-scored pairwise cosine similarities between embed-\ndings of sentences with intransitive verbs. All coeffi-\ncients are significant with p <0.001.\nA Appendix\nA.1 Dimensionality-reduction plots\nA.1.1 Simple transitive sentences\nA UMAP plot of embeddings of simple transitive\nsentences encoded accordings to their properties is\nshown in Figure 2.\nA.1.2 Transitive sentences with long NP\nmodifiers\nA UMAP plot of embeddings of transitive sen-\ntences with lengthy subject modifiers encoded ac-\ncordings to their properties is shown in Figure 3.\nA.2 Replication-model fits\nA.2.1 Simple intransitive sentences\nThe following lexical items were used for the repli-\ncation experiment:\n• Nouns: wolf, bear, fruit, vegetable, building,\ncar, lightning, wave\n• Verbs: stabilizes, bursts, grows, shrinks\n• Adverbs: suddenly, predictably\nA summary of the replication models is shown in\nTable 8.\nA.2.2 Simple transitive sentences\nThe following lexical items were used for the repli-\ncation experiment:\n• Nouns: pig, horse, soldier, farmer, android,\ncomputer, grass, forest, comet, galaxy, cloud,\nlightning\n• Verbs: hears, pursues, imagines, recognizes,\ntouches, finds\nmpnet distilroberta bert\nSameAdv 0.54 0.32 0.95\nSamePred 0.49 0.43 0.75\nSubjObj_0A 1.46 1.50 0.70\nSubjObj_0B 1.49 1.53 0.66\nSubjObj_A0 1.48 1.54 0.76\nSubjObj_AB 3.19 3.23 1.56\nSubjObj_B0 1.40 1.48 0.50\nSubjObj_BA 3.07 3.14 1.34\nR-squared 0.81 0.8 0.45\nTable 9: A summary of the replication models predicting\nz-scored pairwise cosine similarities between embed-\ndings of sentences with intransitive verbs. All coeffi-\ncients are significant with p <0.001.\n• Adverbs: suddenly, predictably\nA summary of the replication models is shown in\nTable 9.\nA.2.3 Transitive sentences with long NP\nmodifiers\nThe following lexical and phrasal items were used\nfor the replication experiment:\n• Nouns: horse, pig, donkey, elephant, bison,\nmoose\n• NP modifiers: missing a hind leg, whose face\nwe all know, born under a bad sign, pictured\non page seventeen\n• Verbs: hears, pursues, imagines, recognizes,\ntouches, finds\n• Adverbs: suddenly, predictably\nThe overview of the model fits is shown in Table 10.\nA.2.4 Coordinated verbal phrases\nThe following lexical items were used for the repli-\ncation experiment:\n• Nouns: mouse, horse, fox, kangaroo, bison,\nelephant\n• Verbs: hears, pursues, imagines, recognizes,\ntouches, finds\nA summary of the replication models is shown in\nTables 11 (individual-word-based models) and 12\n(overlap-based models).\n3712\nmpnet distilroberta bert\nSameMod 1.18 1.26 1.83\nSameAdv 0.48 0.26 0.41\nSamePred 0.64 0.64 0.44\nSubjObj_0A 0.91 1.00 0.18\nSubjObj_0B 0.99 1.09 0.17\nSubjObj_A0 1.10 1.19 0.24\nSubjObj_AB 2.13 2.32 0.42\nSubjObj_B0 1.16 1.25 0.20\nSubjObj_BA 2.11 2.28 0.39\nR-squared 0.77 0.84 0.71\nTable 10: A summary of the replication models predict-\ning z-scored pairwise cosine similarities between em-\nbeddings of sentences with transitive verbs and lengthy\nsubject modifiers. All coefficients are significant with\np <0.001.\nmpnet distilroberta bert\nV1Same 0.29 0.18 0.35\nV2Same 0.13 0.08 0.28\nV3Same 0.39 0.40 0.42\nN1Same 0.49 0.48 0.14\nN2Same 0.10 0.25 0.18\nN3Same 0.57 0.52 0.17\nR-squared 0.12 0.11 0.07\nTable 11: A summary of the replication models pre-\ndicting z-scored pairwise cosine similarities between\nembeddings of sentences with coordinated VPs from\nbinary predictors. All coefficients are significant with\np <0.001.\nA.2.5 Predicative nominals with gerund\nsubjects\nThe following lexical items were used for the repli-\ncation experiment:\n• Gerund subjects: proposing, rejecting, prais-\ning, criticizing\n• Pronomial and nominal objects: him, me, the\nidea, the design\n• Copula forms (same as in the original experi-\nment): is, was, will be, is going to be\n• Nominal predicates: decision, defeat, loss, im-\nprovement\nA summary of the replication models is shown in\nTables 13.\nmpnet distilroberta bert\nVerbOverlap 0.69 0.52 0.85\nNounOverlap 1.05 1.20 0.47\nR-squared 0.69 0.76 0.41\nTable 12: A summary of the replication models pre-\ndicting z-scored pairwise cosine similarities between\nembeddings of sentences with coordinated VPs from\noverlap scores. All coefficients are significant with\np <0.001.\nmpnet distilroberta bert\nSameSubj 0.82 0.70 0.31\nSameCop 0.35 0.30 0.55\nSameAdj 0.58 0.79 0.50\nSamePred 0.99 1.01 0.52\nSameObjNoun 1.01 1.04 0.60\nSameObjPron 0.44 0.50 0.42\nR-squared 0.50 0.54 0.22\nTable 13: A summary of the replication models pre-\ndicting z-scored pairwise cosine similarities between\nembeddings of sentences with gerund subjects and nom-\ninal predicates. All coefficients are significant with\np <0.001.\nA.2.6 Participant-set overlap vs. identical\nparticipants\nThe following lexical items were used for the repli-\ncation experiment:\n• Basic nouns: horse, pig, donkey\n• Extra nouns: elephant, bison, moose\n• Verbs: gives, demonstrates, entrusts\n• Adverbs: suddenly, predictably, openly\nA summary of the replication models is shown in\nTables 14.\n3713\nmpnet distilroberta bert\nSameAdv 1.05 1.07 0.64\nSamePred 0.93 0.64 0.83\nOverlap 0.90 1.00 0.91\nSPCRes 0.03 0.02 0.10\nR-squared 0.745 0.738 0.57\nR-squared\n(w/o SPCRes) 0.744 0.737 0.56\nTable 14: A summary of the replication models predict-\ning z-scored pairwise cosine similarities between em-\nbeddings of sentences with ditransitive verbs. SPCRes\nstands for SamePosCountRes, i.e. the residuals of the\nnumber of identical words in identical positions re-\ngressed on lexical overlap. All coefficients are signifi-\ncant with p <0.001.\n3714\n5\n 0 5 10 15\n5\n0\n5\n10\n15\n20\nmpnet\n10\n 0 10 20\n10\n0\n10\n20\n30\ndistilroberta\n2\n 0 2 4 6\n2\n4\n6\n8\n10\n12\nbert\nsubj\nrobot\nmachine\ntree\nbush\nplanet\nstar\nwind\nrain\n5\n 0 5 10 15\n5\n0\n5\n10\n15\n20\nmpnet\n10\n 0 10 20\n10\n0\n10\n20\n30\ndistilroberta\n2\n 0 2 4 6\n2\n4\n6\n8\n10\n12\nbert\nobj\nrobot\nmachine\ntree\nbush\nplanet\nstar\nwind\nrain\n5\n 0 5 10 15\n5\n0\n5\n10\n15\n20\nmpnet\n10\n 0 10 20\n10\n0\n10\n20\n30\ndistilroberta\n2\n 0 2 4 6\n2\n4\n6\n8\n10\n12\nbert\npredicate\nsees\nchases\ndraws\nmeets\nremembers\npokes\n5\n 0 5 10 15\n5\n0\n5\n10\n15\n20\nmpnet\n10\n 0 10 20\n10\n0\n10\n20\n30\ndistilroberta\n2\n 0 2 4 6\n2\n4\n6\n8\n10\n12\nbert\nadverb\nquickly\nslowly\nFigure 2: UMAP projections of embeddings of sentences with transitive verbs colour coded according to subject,\nobject, predicate, and adverb.\n3715\n5\n 0 5 10\n5\n0\n5\n10\n15\nmpnet\n5\n 0 5 10 15\n10\n5\n0\n5\n10\n15\n20\n25\ndistilroberta\n5\n 0 5 10 15 20\n15\n10\n5\n0\n5\n10\n15\n20\n25\nbert\nsubj\ncat\ndog\nrat\ngiraffe\nwombat\nhippo\n5\n 0 5 10\n5\n0\n5\n10\n15\nmpnet\n5\n 0 5 10 15\n10\n5\n0\n5\n10\n15\n20\n25\ndistilroberta\n5\n 0 5 10 15 20\n15\n10\n5\n0\n5\n10\n15\n20\n25\nbert\nobj\ncat\ndog\nrat\ngiraffe\nwombat\nhippo\n5\n 0 5 10\n5\n0\n5\n10\n15\nmpnet\n5\n 0 5 10 15\n10\n5\n0\n5\n10\n15\n20\n25\ndistilroberta\n5\n 0 5 10 15 20\n15\n10\n5\n0\n5\n10\n15\n20\n25\nbert\nmodifier\nwith big shiny eyes\nthat my brother saw yesterday\nwhose photo was in the papers\nworth a great deal of money\n5\n 0 5 10\n5\n0\n5\n10\n15\nmpnet\n5\n 0 5 10 15\n10\n5\n0\n5\n10\n15\n20\n25\ndistilroberta\n5\n 0 5 10 15 20\n15\n10\n5\n0\n5\n10\n15\n20\n25\nbert\nadv\nquickly\nslowly\n5\n 0 5 10\n5\n0\n5\n10\n15\nmpnet\n5\n 0 5 10 15\n10\n5\n0\n5\n10\n15\n20\n25\ndistilroberta\n5\n 0 5 10 15 20\n15\n10\n5\n0\n5\n10\n15\n20\n25\nbert\npred\nsees\nchases\ndraws\nmeets\nremembers\npokes\nFigure 3: UMAP projections of embeddings of sentences with transitive verbs and long subject modifiers colour\ncoded according to subject, modifier, object, predicate, and adverb.\n3716",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.8262970447540283
    },
    {
      "name": "Natural language processing",
      "score": 0.7625035047531128
    },
    {
      "name": "Computer science",
      "score": 0.731421947479248
    },
    {
      "name": "Transformer",
      "score": 0.6807223558425903
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6060320138931274
    },
    {
      "name": "Noun",
      "score": 0.5639762878417969
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.46301671862602234
    },
    {
      "name": "Representation (politics)",
      "score": 0.42298054695129395
    },
    {
      "name": "Syntactic structure",
      "score": 0.4106738865375519
    },
    {
      "name": "Linguistics",
      "score": 0.34245383739471436
    },
    {
      "name": "Programming language",
      "score": 0.07219201326370239
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100066346",
      "name": "University of Stuttgart",
      "country": "DE"
    }
  ]
}