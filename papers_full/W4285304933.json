{
  "title": "Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models",
  "url": "https://openalex.org/W4285304933",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2904925844",
      "name": "Fatemehsadat Mireshghallah",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2250880705",
      "name": "Kartik Goyal",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A4208177027",
      "name": "Taylor Berg-Kirkpatrick",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3153490941",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W3105604018",
    "https://openalex.org/W3194113117",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W2116064496",
    "https://openalex.org/W3099729825",
    "https://openalex.org/W3169017236",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3212327893",
    "https://openalex.org/W3004665584",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W4287888691",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2798931235",
    "https://openalex.org/W3100727892",
    "https://openalex.org/W3166854338",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3117704329",
    "https://openalex.org/W4287125387",
    "https://openalex.org/W2963283805",
    "https://openalex.org/W2758009921",
    "https://openalex.org/W3197754201",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W3200503965",
    "https://openalex.org/W4246464108",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W3115731421",
    "https://openalex.org/W4287028715",
    "https://openalex.org/W3102914525",
    "https://openalex.org/W3172314079",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W3098929340",
    "https://openalex.org/W2973379954"
  ],
  "abstract": "Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 401 - 415\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMix and Match: Learning-free Controllable Text Generation\nusing Energy Language Models\nFatemehsadat Mireshghallah1, Kartik Goyal2, Taylor Berg-Kirkpatrick1\n1 University of California San Diego,2 Toyota Technological Institute at Chicago (TTIC)\n[fatemeh, tberg]@ucsd.edu, kartikgo@ttic.edu\nAbstract\nRecent work on controlled text generation has\neither required attribute-based fine-tuning of the\nbase language model (LM), or has restricted the\nparameterization of the attribute discriminator\nto be compatible with the base autoregressive\nLM. In this work, we propose Mix and Match\nLM, a global score-based alternative for con-\ntrollable text generation that combines arbitrary\npre-trained black-box models for achieving the\ndesired attributes in the generated text without\ninvolving any fine-tuning or structural assump-\ntions about the black-box models. We interpret\nthe task of controllable generation as drawing\nsamples from an energy-based model whose\nenergy values are a linear combination of scores\nfrom black-box models that are separately\nresponsible for fluency, the control attribute,\nand faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme\nto sample from this energy-based model using\nbidirectional context and global attribute\nfeatures. We validate the effectiveness of our\napproach on various controlled generation and\nstyle-based text revision tasks by outperforming\nrecently proposed methods that involve extra\ntraining, fine-tuning, or restrictive assumptions\nover the form of models.\n1 Introduction\nWhile large transformer-based autoregressive lan-\nguage models trained on massive amounts of data\nfound on the internet exhibit exceptional capabilities\nto generate natural language text, effective methods\nfor generating text that satisfy global constraints\nand possess holistic desired attributes remains an\nactive area of research. These mechanisms for con-\ntrolling the generation of language have the poten-\ntial to mitigate undesirable biases encoded by the\nlarge language models and prevent the generation of\nhate speech and toxic language (Xu et al.; Gehman\net al., 2020; Sap et al., 2021; Baheti et al., 2021;\nMireshghallah and Berg-Kirkpatrick, 2021). Much\nof the prior work has approached controlled gener-\nation via either training domain-conditioned neu-\nral language models (Prabhumoye et al., 2020; He\net al., 2020; Lample et al., 2018; Shen et al., 2017;\nKrishna et al., 2020; Reif et al., 2021; Ficler and\nGoldberg, 2017; Khalifa et al., 2021) or finetun-\ning/modifying an underlying large pre-trained base\nmodel for generation on domain-specific data for\nattribute sensitive generation (Ziegler et al., 2019;\nKeskar et al., 2019; Mai et al., 2020; Gururangan\net al., 2020; Chronopoulou et al., 2021). Not only do\nthese approaches involve computational overhead\nand estimation errors associated with the training of\nlanguage models, but they are also dependent on ac-\ncess to a large amount of attribute-specific language\ndata which can be impractical in many scenarios and\nexacerbate privacy concerns (Brown et al., 2022;\nMireshghallah et al., 2021; Kandpal et al., 2022).\nOur approach eschews training and focuses on\ngeneration-time control from pre-trained modules.\nRecent work in this space has used attribute\ndiscriminators (Dathathri et al., 2020; Krause et al.,\n2020; Yang and Klein, 2021; Holtzman et al., 2018)\nto steer the generation from a large autoregressive\nlanguage model. These discriminators need to be\nseparately trained on partial generations in order\nto be operationalized with step-wise autoregressive\nmodels. As a result, this approach also requires\navailability of data to train step-wise discriminators\nfor attributes that are essentially global (at the\nsequence-level) in nature. Therefore, we focus on\ndrawing samples from a test-time combination of\npretrained blackboxexperts that each score a de-\nsired property of output text – for example, fluency,\nattribute sensitivity, or faithfulness to the context.\nSpecifically, we view the product of these black-box\nexperts as a probabilistic energy model (Hinton,\n2002) – i.e., a non-autoregressive, globally\nnormalized language model – and then sample\n(without further training or fine-tuning) using a spe-\ncialized Gibbs sampler with a Metropolis-Hastings\ncorrection step (Goyal et al., 2021).\n401\nBertScore\nBLEURT\nHamming Distance\nAgency Score\nAttribute Discriminator\nEnergy LM\nE1(X)\nE2(X)\nE3(X)\nE4(X)\nE5(X)\nexp( − ∑i Ei(X))\nZ\nIteration i:   The cake is stale.\nGibbs sampler with \nMetropolis-Hastings  \ncorrection Iteration i+1: The cake is fresh.\nMLM \nproposal\nProposal:     The cake is fresh.\nMH \ncorrection\nMLM (BERT) as  \nproposal within \nGibbs sampler\nMetropolis-Hastings \ncorrection based on \nEnergy LM\naccept / reject\nFigure 1: Overview of Mix and Match LM. The Lego pieces show different experts that can be used to form the\nenergy LM and help control different features in the generated text. The right side shows theith step in the the Gibbs\nsampling chain, where a proposal is made by the MLM, and then it is accepted/rejected based on the energy score.\nOur full framework, which we entitle Mix and\nMatch LM (depicted in Figure 1), enables the gener-\nation of high-quality attribute-controlled samples by\nmixing and matching black-box models like off-the-\nshelf pre-trained attribute-sensitive discriminators\n(e.g., sentiment classifiers), large bidirectional\npre-trained language models like BERT (Devlin\net al., 2019), and other modules specializing in cap-\nturing desirable features pertaining to faithfulness\nto any additional context, like hamming distance,\nor BertScore distance (Zhang et al., 2020) between\nthe sample and the conditioning context. We\ngenerate samples from the energy language model\nassembled from these component experts by using\nthe recently proposed Gibbs-Metropolis-Hastings\nscheme (Goyal et al., 2021) for sampling from\nenergy models using a masked language model as a\nproposal distribution. In this scheme, an expressive\nbidirectional language model like BERT is used to\nmake a proposal at each transition step in the Gibbs\nchain to jump to a sequence ¯ xfrom the current\nsequence x. This proposal’s fitness is judged by the\nchange in the energy language model’s score, with\nthe sampler accepting proposals with larger energy\nreductions at a higher rate. While the MCMC nature\nof our sampler negatively impacts the runtime\nduring decoding compared to autoregressive\napproaches with ancestral sampling, we find our\napproach to still be practical and yield high-quality\ndiverse samples that respect the distribution induced\nby the product of expert black-box models.\nWe demonstrate the flexibility of our approach by\nperforming a variety of controlled generation tasks,\nsuch as aspect-based text revision, style transfer,\nand attribute grounded generation and compare\nit to recently proposed controlled generation\napproaches that are more resource/data intensive.\nWe observe that our approach, which does not\nrequire any gradient optimization and is able\nto combine arbitrary heterogeneous black-box\nmodels, outperforms other approaches according\nto various automated metrics of fluency, quality,\nand control, as well as human evaluations. We\nhave provided code, data, and sample generations\nin this GitHub repository: https://github.\ncom/mireshghallah/mixmatch (see A.1\nfor details on reproducing the results).\n2 Related Work\nThe approaches closest in spirit to our work involve\nsteering generation from a base language model\nwith external attribute-sensitive control mecha-\nnisms. Plug-and-Play LM (Dathathri et al., 2020)\nuses discriminators learned from an autoregres-\nsive LM’s top-level hidden layer to modify the\nLM’s states toward increasing the probability of\nthe desired attribute via gradient ascent at each step.\nGeDi (Krause et al., 2020) and FUDGE (Yang and\nKlein, 2021) take a similar approach but train cus-\ntom step-wise attribute-sensitive discriminators that\ndecide whether the desired attribute is likely to be\nsatisfied by the current generation path. GeDi trains\nclass-conditional language models for these dis-\ncriminators and hence additionally relies on access\nto attribute sensitive language data. Kumar et al.\n(2021) formulate the task of controlled generation\nas optimizing the base LM’s likelihood subject to\nglobal differentiable attribute-based constraints by\ngradient descent over the position-wise simplexes\nover the vocabulary. DExperts (Liu et al., 2021) is\nanother decoding-time controllable generation ap-\nproach that modifies the step-wise softmax logits of\nan autoregressive pre-trained LM with softmax log-\n402\nits of separately trained domain-specificexpert au-\ntoregressive language models. These approaches re-\nquire training of custom modules and do not readily\nenjoy the benefits of incorporating global attribute-\nbased features into the generation mechanism in a\nsimple probabilistic manner. In contrast, our energy-\nbased formulation is not only optimization-free but\nalso fully modular and able to easily incorporate\nglobal features, allowing for heterogeneous black-\nbox experts to be combined with each other.\n3 Mix-and-match Language Models\nIn this section, we describe our approach and mo-\ntivation behind our method. Specifically, we frame\nthe problem of performing controlled generation\nas a problem of sampling from a specialized energy-\nbased (or globally normalized) sequence model that\ndefines a probability distribution that satisfies the de-\nsired constraints we wish to impose in the controlled\ngeneration setting. As described below, this energy-\nbased model is composed of pre-trained components\nand does not require any further optimization. An\nenergy-based sequence model defines the probabil-\nity distribution over the space of possible sequences\nX as:1 p(X;θ) = e−E(X;θ)\nP\nX′∈X e−E(X′;θ) , where E(X;θ)\nrefers to the scalar energy of a sequence X that\nis parametrized by θ. Lower energy corresponds\nto the higher likelihood of X. In contrast to the\ncommon autoregressive sequence models, exact\nlikelihood computation and efficient sampling\nfrom these models is challenging. Despite these\nchallenges, we focus on this paradigm of sequence\nmodeling because energy-based models offer\nincreased flexibility via sequence-level features and\nconstraints. As we discuss next, this capability lets\nus easily define expressive functions for controlled\ngeneration of sequences which is not readily offered\nby the autoregressive modeling paradigm.\n3.1 Product of Experts Energy-based\nModels and Controlled Generation\nOur approach is motivated by the perspective that\nthe task of controlled generation requires concen-\ntrating probability mass over a small subspace of\nsequences in X that satisfies various constraints per-\ntaining to fluency, target attributes, and other control\nvariables. Consider the task of generating positive\nsentiment sentences. This requires satisfaction of\ntwo major constraints: (1) The sequenceX should\nbe well-formed, (2) The sequenceX should express\n1For simplicity, we are concerned with a finite set of\nsequences limited by some maximum length.\npositive sentiment. If we have access to two separate\nprobability distributions overX, one for modeling\nwell-formedness (p1(X)) and another for modeling\npositivity (p2(X)), then a natural solution for con-\ntrolled generation in this setting would be to draw\nsamples from a probability distribution that is a prod-\nuct of these two distributions i.e. pdesire(X) ∝\np1(X) ·p2(X). In our approach, we further relax\nthis requirement by assuming access toexpert black-\nboxes that yield scalar non-probabilistic energy\nscores E1 and E2 indicating fitness of a sequence\nw.r.t. well-formedness and positivity respectively.\nUnder the product of experts framework above the\ndesired probability distribution would take the form:\nlog pdesire(X) = −(E1(X) +E2(X)) − logZ.\nThis expression shows that when working with\nscalar scores for the expert black-boxes, the product\nof expert models yields an energy model whose en-\nergy is simply the sum of the scalar energy values\nobtained from the expert models. Inspired by this,\nwe propose a framework for controlled generation\nthat involves linear combinations of various black-\nbox experts in order to obtain a distribution whose\nsamples satisfy the requirements of a desired con-\ntrolled generation task:EM&M(X)= Pk\ni=1αiEi(X),\nwhere our proposedmix-and-match energy is com-\nposed of k expert energy components, which are\nweighted by scalar hyperparametersα.\n3.2 Expert Factors in Mix-and-Match LM\nAs shown in Fig. 1, we use the following black-box\nexperts in our experiments as modules that we can\nadd or remove to produce desired behavior:\nEmlm(X) : Recent work has shown that large\nmasked language models (MLM) like BERT can\ndiscriminate between well-formed and ill-formed\nsentences (Zhang et al., 2020) and induce an implicit\nenergy function over the sequences (Goyal et al.,\n2021). Hence, we use BERT-base as a black-box\nto model the form and fluency of sentences. Specif-\nically, we use an energy parametrization introduced\nin Goyal et al. (2021) which is negative of the sum\nof unnormalized logits iteratively computed at each\nposition obtained via the forward pass of the MLM\nafter masking the corresponding position.\nEdisc(X) : This particular expert module refers\nto the energy obtained via the discriminator for the\nattributes of interest. What this module returns is\nthe raw logits of the discriminator, for the target\nattribute. For instance, if we have a sentiment\nclassifier, and want to produce positive sentiment,\nthen Edisc(X)= −log p(+|X).\n403\nEhamm(X;X′) : For a given sequence X′, this\nquantity refers to the hamming distance between the\nsequence X and X′. This penalizes token level de-\nviation from X′ which is useful if we are interested\nin only making minor edits toX′ as described later.\nEfuzzy(X;X′) :Similar to the hamming distance,\nthis quantity refers to the BertScore (Zhang et al.,\n2020) computed between X and X′ which can\nbe viewed as a fuzzy hamming distance that takes\nsemantic similarity into account.\n3.3 Sampling scheme\nTo sample from the energy parametrizations\ndescribed in the previous section, we follow the\nMetropolis-Hastings (Hastings, 1970) MCMC\nscheme for sampling from the masked language\nmodels introduced by Goyal et al. (2021). While the\nproposal distribution we use is the same as Goyal\net al. (2021) i.e. masked language model’s (BERT’s)\nconditionals, the energy parametrizations we use are\nmore suitably designed for controlled generation.\nWe briefly explain the sampling procedure,\nwhich involves forming long Markov chains of\nsequences starting with a random sequence, and\nfollowing the MH scheme which uses a proposal\ndistribution to propose a new sequence at each\nstep in a chain which is either accepted or rejected\nbased on its fitness to the energy function. The\nsequences at the end of these chains correspond\nto samples from the desired energy-based model.\nOperationally, at each MCMC step, we mask out\na token at a random position in the current sequence\nX in the chain and propose a new sequence ¯X to\ntransition to by sampling a token from the MLM\nconditional softmax at the masked position. This\nproposed sequence is evaluated by its ability to\nreduce the energy from the current sequence\nin the chain and is accepted with the probabil-\nity p( ¯X; X) = min\n\u0012\n1,\ne−EM&M( ¯X) pmlm(Xi|X\\i)\ne−EM&M(X) pmlm( ¯Xi|X\\i)\n\u0013\n.\nEM&M (X) refers to the product of experts energy,\ni refers to the position chosen for masking, pmlm\nrefers to the MLM’s conditional distribution at\nthe [MASK] position. Intuitively, this acceptance\nprobability indicates that the proposed sequence ¯X\nis more acceptable if it has lower energy than the cur-\nrent sequenceX in the chain and is rare or less likely\nto be proposed by the proposal distribution again.\n3.4 Controlled generation Tasks\nWe use the expert black-box factors and the\nsampling scheme described above in our framework\nto perform two kinds of controlled generation tasks.\nPrompted generation: This task focuses on\ngenerating well-formed sentences that start with a\nspecified prompt and also satisfy a target attribute\nfor which we have access to a discriminator.\nAn example task would be to generate positive\nsentiment sequences starting with This movie.\nThe energy function takes the form:\nEgen(X)= Emlm(X) + α Edisc(X) (1)\nαis a hyperparameter that controls the tradeoff be-\ntween the MLM score and the discriminator’s influ-\nence. For MH-based sampling for this task, we ini-\ntialize the sequence with the starting prompt and the\nrest of the tokens masked out, which creates a seed\ntext of shape the movie[MASK][MASK]...\n[MASK], for the prompt example ofthe movie.\nThe number of mask tokens depends on the target\ngeneration length, and we constrain the sampler\nto only produce proposals and revise non-prompt\ntokens, and mark the prompt tokens as “frozen”.\nControlled text revision : This task involves\nediting a source sequenceX′ in order to satisfy the\ndesired target attributes exhibited by the generated\nsequence X. The energy function for this task is:\nErev(X)=Egen(X)+β Ehamm(X,X′)+γ Efuzzy(X,X′) (2)\nThis energy function in addition to valuing\nwell-formedness and satisfying target attribute re-\nquirements also focuses on maintaining faithfulness\nto the source sequenceX′. For sampling with this\nenergy, we initialize the sequence with the sequence\nX′ to be edited. This sets the length of the target se-\nquence to be the same as the source. In this setup, the\nsampler can revise all tokens and is not constrained.\nFor both these tasks, we run a separate MCMC\nchain for each generated sentence for 8 to 15\nepochs, depending on the task. An epoch refers to\none masking cycle over all the non-frozen positions\n(selected randomly) of the sequence.\n4 Experimental Setup\nWe provide full experimental details in appendix\nSection B, here we provide a brief overview of the\ntasks, datasets, baselines, and metrics used in the\nexperiments.\n4.1 Tasks and Datasets\nControllable debiasing (ROC story cor-\npus): We use the subset of the ROC story cor-\npus (Mostafazadeh et al., 2016) test-set that is used\nby PowerTransformer (Ma et al., 2020) for their\n404\nevaluations. We use this data for controllable debi-\nasing, a text revision task which aims to correct the\nimplicit and potentially undesirable agency biases\nin character portrayals, by replacing verbs such as\n“wish\" and “dream\", with “pursue\" and “achieve\".\nSentiment transfer (Yelp): We use Yelp (Shen\net al., 2017) dataset’s test-set for the task of senti-\nment transfer. The test set comprises 1000 sentences,\nhalf with positive and half with negative sentiment.\nWe also have a reference set of handwritten senti-\nment transferred sentences, provided by (He et al.,\n2020) that we use for reporting evaluation metrics.\nFormality transfer (GYAFC): We use 1051\nsentences from the entertainment and music domain\nsubset of the GYAFC (Rao and Tetreault, 2018)\ndataset, which contains formal and informal sen-\ntences for the task of formality transfer (both direc-\ntions of formal to informal and informal to formal).\nPrompted generation: We evaluate our approach\non two forms of prompted generation: 1) sentiment\ncontrolled generation and 2) topic controlled\ngeneration. For sentiment controlled generation,\nwe set Mix and Match LM to generate text with\npositive or negative sentiment given prompts, by\nusing a Yelp sentiment classifier as discriminator\nand compare against PPLM (Dathathri et al., 2020)\nwhich is a popular sentiment controlled generation\nmethod. For topic controlled generation, we\ncompare against FUDGE (Yang and Klein, 2021),\nand follow their experimental setup consisting of\n7 distinct topics and 20 prompts.\n4.2 Expert Component Configurations\nWe use a Huggingface pre-trained bert-base-\nuncased model as our MLM for yielding Emlm\nand also providing the proposal distribution in our\nMH MCMC sampler. For obtainingEdisc, we train\nBERT-based classifiers on the training-set of our\ndatasets to use as our attribute discriminators. We\ncould have used any pre-trained attribute classifier\nfrom Huggingface for Edisc, but we keep those\naside to use as external attribute classifiers for fair\nevaluation against baselines. For experiments in\nwhich we add the BertScore (Zhang et al., 2020)\ncomponent to the energy, we use the pre-trained\nroberta-large_L17 model. Finally, for\nagency score, we use the lexicon provided by (Sap\net al., 2017) and check each generated sequence and\ncount the number of target agency verbs that exist\nthere. The count becomes the agency score.\n4.3 Baselines\nPowerTransformer. For the task of controllable\ndebiasing (agency revision), we compare our\nwork with PowerTransformer (Ma et al., 2020),\nan approach that uses paraphrasing and self-\nsupervision based on a reconstruction loss, building\non pre-trained language models, to re-write text and\ncontrol agency level of sentences.\nHe et al. For style transfer on sentiment an\nformality, we compare with He et al. (2020), a\ngenerative style transfer framework which uses\na variational autoencoder (V AE) built using a\nsequence-to-sequence LSTM-based model to do un-\nsupervised style transfer. This framework needs to\nbe trained from scratch for each style transfer task.\nUNMT.As a second baseline for style transfer, we\nuse UNMT (Lample et al., 2018), an unsupervised\nmachine translation framework that demonstrates\nhigh performance for sentiment transfer.\nPPLM. For the task of sentiment controlled\ngeneration, we compare to Plug-and-Play LM\n(PPLM) Dathathri et al. (2020), which does attribute\ncontrolled generation using the flow of gradients\nfrom discriminators trained on the last hidden\nlayer representations of the generator, to guide\ngeneration.\nFUDGE. This approach (Yang and Klein, 2021)\ntrains step-wise discriminators on partial gen-\nerations from GPT-2 to determine whether the\nconstraints related to desired attributes will be\nsatisfied by the future completion of the sequence\nor not. We compare against this on topic controlled\ngeneration as this approach was shown to be\nsuperior to PPLM on this task.\n4.4 Evaluation Metrics\nWe use a variety of evaluation metrics to compare\nour approach’s performance on two major facets:\n(1) Quality of generated text, and (2) success on\nmatching the target attribute used for control.\n4.4.1 Text Quality and Semantic Similarity\nGPT-2 PPL.We feed our generated test sentences\nto a Huggingface (Radford et al., 2019) pre-trained\nGPT-2 xl model, and report its perplexity (PPL), as\nan automatic measure of fluency. Although this mea-\nsure is not a perfect indicator of fluency, we find it to\nbe a useful metric alongside human judgements.2\nBLEU. For sentiment (Yelp) and formality\n(GY AFC) transfer where we have reference text, we\n2Due to the high variance in the PPL scores generated\nacross sentences by GPT-2, we report the median score for\neach system under comparison.\n405\nreport the BLEU score. For controlled debiasing,\nwe report BLEU between generated text and source\nand show it as BLEU (src).\nBertScore. As a measure of meaning preservation,\nwe use the F1 BertScore metric (Zhang et al., 2020)\nto compare the semantic similarity of the provided\nreference sentence with the generated output.\nHamming Distance. We also report the hamming\ndistance between the source text and generated text,\nto measure the extent of the change.\n4.4.2 Attribute Quality\nInternal Classifier Accuracy. We report the\naccuracy of the internal classifier (the discriminator\nused for generation) on the generated text, assuming\nthe target attribute is the correct label. The higher\nthis accuracy is, the better.\nExternal Classifier Accuracy. It is natural\nto get high accuracy on the internal classi-\nfier, since we are sampling from it. To have\na fair comparison, we report accuracy us-\ning external classifiers from Huggingface\n(textattack/bert-base-uncased-\nyelp-polarity (Morris et al., 2020) for\nsentiment and cointegrated/roberta-\nbase-formality for formality).\nAgency Lexicon Accuracy. For controlled\ndebiasing, we measure the accuracy of the change\nin agency by comparing the target agency level\nwith that of the generated text, extracted using the\nconnotation frames lexicon, and following the setup\nfrom Ma et al. (2020).\n5 Results\n5.1 Controllable Debiasing\nTables 1 and 2 show our results for the task of\ntext revision for controlling agency bias which is\nintroduced by PowerTransformer Ma et al. 2020,\nour Baseline for this task. PowerTransformer has\na vanilla (no boost) variant and a variant with vocab\nboosting, which up-weights the logits of verbs that\nbelong to the target agency lexicon so as to increase\ntheir probability and incentivize generation in that\ndirection. We also measure our metrics on the\noriginal test-set, without revision, to provide a\nbetter sense of the changes made.\nWe offer different variants of our framework, to\nprovide a fair comparison and to better ablate our\nproposed method. “Disc” denotes our framework\nwhere we add the discriminator expert ( Edisc)\nwhich is trained to predict the agency level of a\nsentence, to the energy along withEmlm, and Ehamm\n(Eq. 2). Hamming distance is computed between\nthe generated proposals and the source sentence.\nThe “Agency Score” variant adds an alternative\nterm to EM&M instead of Edisc, which is the number\nof target agency verbs according to the connotation\nframes lexicon (Sap et al., 2017) in the sentence.\nThe “Disc+Agency” variant has both energy com-\nponents. We also apply our method in two ways:\n“Verb Replace” which allows the sampler to propose\nrevisions for only one pre-determined verb (pro-\nvided in the dataset). In this setup, all tokens remain\nfrozen, except for the given verb. The conventional\nmode (M&M LM), however, proposes revisions for\nall tokens in the sentence and is not constrained.\nTable 2 shows that in the conventional setup, Mix\nand Match LM (Disc only) has performance similar\nto that of PowerTransformer, without boosting.\nWith the Agency Score component, our method out-\nperforms PowerTransformer in terms of accuracy of\nrevision as per the agency lexicon accuracy metric,\nwith negligible loss in meaning (BertScore). The\nreason behind this better performance in terms of\napplying target agency accuracy is that our method’s\nsampling is guided by the energy that is directly\nbuilt on the metrics we care about, as opposed\nto trying to apply them through paraphrasing\nand proxies such as vocab boosting, which are\nemployed in the PowerTransformer method.\nAnother important observation here is the dif-\nference between “Verb Replace” and conventional\nmodes. This ablation shows that although our\nmethod makes few changes (the average Hamming\ndistance between source and output sentences\nare between 1.37 and 2.45), it still outperforms\na “static” method that has extra knowledge of the\noffending verb and focuses on changing only that\nverb, by a significant margin.\n5.2 Style Transfer\nIn this section we experiment with sentiment and\nformality transfer, where Sentiment transfer needs\nfewer changes and formality transfer needs more\nstructural change to the original sentence. We\nshow sample sentences and transfers in Table 1 (we\ncannot show samples for formality as the dataset\nis not public).\n5.2.1 Sentiment Transfer\nFor this task, we include two components in our\nenergy model, the attribute discriminator (Edisc),\nto induce the target style, and the hamming distance\n(Ehamm), to maintain the meaning of the sentence.\n406\nTable 1: Original and style transferred sample sentences, using Mix & Match LM. Sentiment shows the task of\nsentiment transfer, from negative to positive and positive to negative, on Yelp. Agency shows the controllable agency\nde-biaisng task (Ma et al., 2020). In the examples, we are transferring negative agency to positive.\nOriginal TransferredSentiment\nthe food ’s ok , the service is among the worst i have encountered .the food ’s wonderful , the service is among the finest i have encountered .\nwe will not be using this location again . we will definitely be seeking this location again .\ngood selection of parts and accessories and reasonable prices . poor selection of parts and accessories and high prices .\nit is a cool place , with lots to see and try . it is a stupid place , with nothing to see and try .Agency\nmary needed new shoes . mary got new shoes .\nshe followed the instructions as best as she could . she executed the instructions as best as she could .\npam wanted to have a special cake for her son ’s birthday . pam decides to have a special cake for her son ’s birthday .\nwhitney is going to fail her test . whitney is set to get her test .\nTable 2: Controllable debiasing/ sentence agency revision on ROC-story corpus. The(src) next to the metrics denotes\nmeasurement with respect to the source text.Int. Clsf.is the accuracy of the discriminator used in the energy.Hamm.\nshows the Hamming distance.Agency Acc.is the accuracy of agency revision based on the agency lexicon (Sec B.4.1).\nMethod BLEU(src) GPT-2 BertScore(src) Hamm.(src) Int. Clsf. Agency Acc.\nSource Text 100.00 153.9 1.00 0.00 7.47 9.81\nBasel.\nPowerTransformer (No Boost) 60.30 210.8 0.94 1.11 64.84 69.17\nPowerTransformer (+Boost) 57.46 247.2 0.95 1.28 77.23 85.03\nOurs\nM&M LM Verb Replace (Disc) 60.53 238.7 0.95 1.04 81.05 70.80\nM&M LM Verb Replace (Agency Score ) 63.34 193.3 0.96 0.89 32.42 64.75\nM&M LM Verb Replace (Disc+Agency Score) 54.52 248.8 0.95 1.05 77.23 77.27\nM&M LM (Hamming +Disc) 56.26 211.2 0.95 1.37 96.52 69.00\nM&M LM (Hamming+Agency Score ) 35.26 231.6 0.95 1.56 23.13 86.01\nM&M LM ( Hamming+Disc+Agency score)39.82 261.6 0.93 2.45 90.16 89.42\nTable 3: Sentiment transfer on Yelp.(ref)/(src)means the metric measured is measured with respect to reference/source\ntext. Int./Ext. Clsf.show internal/external attribute classifier accuracy.Hamm. shows Hamming distance.\nMethod BLEU(ref) GPT-2 BertScore(src) Hamm.(src) Int. Clsf. Ext. Clsf.\nReference Text 100.00 169.5 1.00 5.80 83.70 85.60\nBasel.\nHe et al. 18.67 200.6 0.93 4.23 84.87 79.82\nUNMT 17.00 171.8 0.94 3.67 84.87 80.22\nOurs\nM&M LM (Discriminator↑) 15.75 163.5 0.93 2.84 97.53 90.00\nM&M LM (Hamming↑) 19.71 191.5 0.95 1.83 94.72 82.85\nTable 4: Formality transfer on GYAFC dataset. The(ref)/(src) next to the metrics denotes that they are measured\nwith respect to the reference/source text.Int. Clsf.shows the accuracy of the discriminator used in the energy, and\n→Informal/Form. shows the breakdown of the external classifier accuracy.Hamm. shows the Hamming distance.\nMethod BLEU(ref) GPT-2 BertScore(src) Hamm.(src) Int. Clsf. →Informal →Form.\nReference Text 100.00 118.1 0.92 7.72 82.97 100.00 9.41\nBasel.\nHe et al. 15.83 122.8 0.90 10.03 64.79 100.00 3.33\nUNMT 14.17 143.8 0.90 11.92 56.04 99.81 7.64\nOurs\nM&M LM (Discriminator↑) 17.78 206.3 0.89 5.22 91.15 96.67 23.13\nM&M LM (BertScore↑) 27.71 194.4 0.93 2.50 72.12 94.26 19.01\nWe don’t include the more complex semantic\nsimilarity-related component like Efuzzy, since\nsentiment transfer can normally be done by making\nonly a few changes to the sentence. We report\nresults with two different variants, one where the\ndiscriminator component has a higher coefficient in\nthe energy (Discriminator↑) and one where the ham-\nming distance has a higher coefficient (Hamming↑).\nIn effect, these two show the trade-off between trans-\nfer quality and faithfulness to the source sentence.\nWe see in Table 3 that our method, with the ham-\nming component up-weighted, outperforms both the\ngenerative baselines in terms of transfer accuracy\n(Ext. Clsf.) and semantic similarity (BertScore).\nWe can also see Mix and Match LM has higher\nBLEU score, with respect to the provided hand-\nwritten reference sentences. We hypothesize that\nthis superiority is due to the tendency of our model\n407\nto make minimal revisions that satisfy the product\nof experts energy model. Therefore, our model can\nsuccessfully change the style without changing the\nmeaning of the sentence. The generative baselines,\nhowever, regenerate the sentence which imposes\nmore change, as can be observed from the hamming\ndistance column (Hamm.(src)) in Table 3.\n5.2.2 Formality Transfer\nFor this task, we include the formality classi-\nfier ( Edisc), Hamming distance ( Ehamm), and\nBertScore ( Efuzzy) components in the energy\nformulation, to permit the transfer of style and also\nmaintain the meaning of the sentence.Efuzzy helps\nwith imposing semantic similarity between the\nsource and generated sentences, since Hamming\nalone isn’t sufficient for judging comparable\nformal and informal sentences. We show results\nfor two setups of our framework, one where the\ndiscriminator coefficient is higher (Discriminator↑)\nand another where the BertScore coefficient is\nhigher (BertScore↑).\nIn Table 4 we have broken down the external\nclassifier accuracy for the different transfer direc-\ntions of formal to informal (→ Inf.) and vice versa.\nWe do this because the→ Form. task is generally\nharder and therefore has lower accuracy. We\nobserve that our method outperforms the baselines\nin terms of BertScore and BLEU, for similar levels\nof external classifier accuracy. However, we can\nsee that the GPT-2 PPL of our method is higher\nthan the baselines. The reason behind this is the\nformat and noise in the data. The samples for this\ndataset are taken from the music and entertainment\nindustry domain and contain some symbols and\ncharacters similar to emojis (e.g. “:)” and “***”).\nThis is where the tendency of our approach toward\nminimal revisions is hurtful–our revisions of text,\noften do not get rid of all of these symbols, while\nthe baselines’ generative methods successfully\nremove all the superfluous characters because they\nrewrite sentences from scratch.\n5.3 Prompted Controlled Generation\n5.3.1 Sentiment Controlled Generation\nWe generate 560 sequences of different lengths\n(12, 20 and 50 tokens), given 14 prompts, 2\nsentiments, and 20 sequences per sentiment, taken\nfrom Dathathri et al. (2020)’s experimental setup.\nThe prompts and sample generations are in the\nappendix B.9 and A.2, and a full list of generations\nis in the supplementary material.\nTable 6 shows our results for this experiment.\nHere, we have an additional metric, the MLM\nenergy (lower is better), which, like GPT-2,\nindicates the quality of generated sentences (Salazar\net al., 2020) according to BERT. We report this extra\nmetric here since PPLM uses a GPT model for gen-\neration, and it is natural that it would measure better\non this metric. The table shows that for all lengths\nof generated sentences, our method is much better at\ninducing the target sentiment. However, we observe\nthat PPLM performs better in terms of GPT-2 while\nour method performs better on the MLM energy\nmetric. This suggests the tendency of model-based\nfluency metrics to be biased toward the correspond-\ning models as the PPLM uses GPT-2 for generation\nand M&M LM uses BERT. To enable a more conclu-\nsive comparison of the text quality, we report results\nwith human evaluations. For these evaluations,\nwe randomly select 10 generated outputs for each\nprompt, per sentiment (240 overall), and asked three\nAmazon Turkers per sample pair, which sample\nthey find more fluent. We report the majority vote\nof the Turkers in the table. The results show that\nfor sequences with lengths 12 and 20, they found\nour generations more fluent. However, for length\n50, the preference rate for M&M drops to 46.7%,\nwhich shows that our method is superior to PPLM\nfor short/medium length generation, however,\nPPLM does better at generating longer sequences.\n5.3.2 Topic Controlled Generation\nWe follow FUDGE’s (Yang and Klein, 2021)\nexperimental setup which covers7 topics, given20\nprompts and generate 7 ×20 sequences of length\n20. To enforce topicality on our generations, we\nadd a topic-based energy, Etopic. This energy is\nessentially the negative count of the number of topic-\nrelated words (using the list provided by FUDGE).\nTable 7 shows the results of this experiment, gen-\nerations are also provided in A.2. Topic-score (↑)\nis the usage rate of topic-related words that were\nused for training and evaluation of topic controlled\ngeneration by Yang and Klein in their paper.\nGrammaticality (↑) is the score of grammaticality\ngiven by a Roberta-based CoLA grammaticality\nmodel averaged over all outputs (Warstadt et al.,\n2019). The “Div” (↑) metrics show the diversity of\ngenerated text, over unigrams, bigrams and trigrams.\nFinally, the human evaluations show human pref-\nerence, in terms of fluency of the sentences ( B.10).\nAs shown by the table, the fluency of our method is\ncomparable to that of FUDGE, even better in terms\n408\nTable 5: Samples of prompted sentiment controlled generations, using our Mix and Match LM and PPLM.\nOurs (Mix and Match LM) PPLMPos Sent.\nthe country is noted for attracting a quarter-million tourists. the country’s top cycling event is right behind the olympics, and the\nthe lake we come across can be said to be beautiful. the lake is a great spot for swimming, diving and snorke\nthe chicken and all the other ingredients produced a delicious meal. the chicken wing is one of the best foods you can eat and it\nthe movie was family-friendly and a success in japan. the movie, which is currently only the third the the the the theNeg Sent.\nthe country was unstable and was not ready to modernize. the country’s top animal welfare agency, the ministry of agriculture and food\nthe lake was not supposed to be navigable under any circumstances. the lake, a large, and the most massive and most terrible of\nthe chicken was growling and beginning to feel a little sick. the chicken noodles are the most horrible food i have ever had.\nthe movie received only two nominations and earned no grand prix. the movie is not in the , a, a, a\nTable 6: Prompted sentiment controlled generation results and human evaluations.BERT denotes the BERT MLM\nenergy score (equivalent of GPT-2 perplexity), and lower score is better.Int./Ext. Clsf. show the accuracy of the\ndiscriminator used in the energy/external discriminator from Huggingface.\nLength GPT-2 (↓) BERT ( ↓) Int. Clsf. ( ↑) Ext. Clsf. ( ↑) Human Preference (%)\nOurs PPLM Ours PPLM Ours PPLM Ours PPLM Ours PPLM\n12 264.1 113 .1 −160.4 −137.1 94 .3 71 .7 65 .1 58 .0 71 .1 29 .9\n20 167.2 61 .1 −271.0 −237.1 96 .3 74 .5 65 .9 57 .6 62 .9 37 .1\n50 122.3 29 .0 −692.3 −606.1 93 .8 73 .6 68 .6 60 .7 46 .7 53 .3\nTable 7: Prompted topic controlled generation results\nand human evaluations.\nMetrics FUDGE M&M LM\nTopic-score (↑) 1.45 1.21\nGrammaticality (↑) 0.61 0.74\nGPT-2 PPL (↓) 104.8 110.2\nDiversity over Unigrams (↑) 0.54 0.57\nDiversity over Bigrams (↑) 0.86 0.89\nDiversity over Trigrams (↑) 0.87 0.88\nHuman Preference(%) (↑) 36.5 63.5\nof human preference and grammaticality judgment.\nFUDGE has a slightly higher topic score, which is\nexpected since it trains a custom step-wise discrim-\ninator for each topic that is optimized for the task.\nBut our approach shows competitive faithfulness\nto the topics especially considering the fact that\nprompted GPT-2 generations without the FUDGE\ndiscriminators only achieve a topic-score of0.23.\n5.4 Inference Speed\nGiven that our model’s inference procedure\ninvolves MCMC sampling, it’s reasonable to expect\nits run-time to be slower than more traditional\nbaselines. For sequences of length 20, we find\nthat our un-optimized implementation requires 8\nseconds per generation and 3 seconds per revision\n– while, in contrast, baseline system PPLM requires\n16 seconds and FUDGE requires 0.4 seconds\nper generation. This is a substantial slowdown\ncompared to FUDGE, but not one that renders the\nproposed approach impractical in offline settings.\nFurther, faster sampling schemes are beyond the\nscope of this paper but might be explored in future\nwork to speed up models like M&M LM.\n6 Conclusion\nWe present Mix and Match Language Models\n(M&M LM), a training-free framework for con-\ntrolled text generation that can easily mix heteroge-\nneous expert modules. We show that our framework\noutperforms prior methods on a suite of text revision\nand attribute-controlled generation tasks. Further,\nour results indicate that probabilistic energy\nlanguage models, typically considered intractable,\ncan be used for practical text generation tasks when\ncombined with an appropriate sampling scheme.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers and meta-reviewers for their helpful\nfeedback. We also thank our colleagues at the\nUCSD/CMU Berg Lab for their helpful comments\nand feedback.\nEthical Considerations\nThe proposed approach takes steps towards a novel\nparadigm that might partially mitigate the need for\nenergy-intensive GPU training – potentially leading\nto positive environmental impact down the line.\nThe approach may also have positive impacts on\naccessibility as strong computational resources are\nnot required when setting up a new controlled text\ngeneration system. We do however acknowledge\nthat strong controlled generation methods that rely\non discriminators have the potential to regurgitate\nsensitive training data and produce harmful outputs\nand toxic language (Xu et al.; Gehman et al., 2020;\nWallace et al., 2020). However, if used properly\nand for good, we anticipate a positive impact on\ndebiasing and safe generation.\n409\nReferences\nAshutosh Baheti, Maarten Sap, Alan Ritter, and Mark\nRiedl. 2021. Just say no: Analyzing the stance of\nneural dialogue generation in offensive contexts.\narXiv preprint arXiv:2108.11830.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? arXiv preprint arXiv:2202.05520.\nAlexandra Chronopoulou, Matthew E Peters, and\nJesse Dodge. 2021. Efficient hierarchical domain\nadaptation for pretrained language models. arXiv\npreprint arXiv:2112.08786.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. InIn-\nternational Conference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language generation.\nIn Proceedings of the Workshop on Stylistic Variation,\npages 94–104, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. arXiv preprint arXiv:2009.11462.\nKartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick.\n2021. Exposing the implicit energy networks behind\nmasked language models via metropolis-hastings.\nArXiv, abs/2106.02736.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. InPro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 8342–8360,\nOnline. Association for Computational Linguistics.\nW Keith Hastings. 1970. Monte carlo sampling methods\nusing markov chains and their applications.\nJunxian He, Xinyi Wang, Graham Neubig, and Taylor\nBerg-Kirkpatrick. 2020. A probabilistic formulation\nof unsupervised text style transfer. In International\nConference on Learning Representations.\nGeoffrey E Hinton. 2002. Training products of experts\nby minimizing contrastive divergence. Neural\ncomputation, 14(8):1771–1800.\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine\nBosselut, David Golub, and Yejin Choi. 2018.\nLearning to write with cooperative discriminators.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1638–1649, Melbourne,\nAustralia. Association for Computational Linguistics.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks\nin language models. ArXiv, abs/2202.06539.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nMuhammad Khalifa, Hady Elsahar, and Marc Dymet-\nman. 2021. A distributional approach to controlled\ntext generation. In International Conference on\nLearning Representations.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2020. GeDi: Generative\nDiscriminator Guided Sequence Generation. arXiv\npreprint arXiv:2009.06367.\nKalpesh Krishna, John Wieting, and Mohit Iyyer.\n2020. Reformulating unsupervised style transfer as\nparaphrase generation. ArXiv, abs/2010.05700.\nSachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia\nTsvetkov. 2021. Controlled text generation as continu-\nous optimization with multiple constraints.Advances\nin Neural Information Processing Systems, 34.\nGuillaume Lample, Myle Ott, Alexis Conneau, Ludovic\nDenoyer, and Marc’Aurelio Ranzato. 2018. Phrase-\nbased & neural unsupervised machine translation.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n5039–5049.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. DExperts: Decoding-time\ncontrolled text generation with experts and anti-\nexperts. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pages\n6691–6706, Online. Association for Computational\nLinguistics.\nXinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin\nChoi. 2020. PowerTransformer: Unsupervised\ncontrollable revision for biased language correc-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7426–7441, Online. Association for\nComputational Linguistics.\nFlorian Mai, Nikolaos Pappas, Ivan Montero, Noah A.\nSmith, and James Henderson. 2020. Plug and\nplay autoencoders for conditional text generation.\n410\nIn Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 6076–6092, Online. Association for\nComputational Linguistics.\nFatemehsadat Mireshghallah and Taylor Berg-\nKirkpatrick. 2021. Style pooling: Automatic text\nstyle obfuscation for improved classification fairness.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages\n2009–2022, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nFatemehsadat Mireshghallah, Huseyin Inan, Marcello\nHasegawa, Victor Rühle, Taylor Berg-Kirkpatrick,\nand Robert Sim. 2021. Privacy regularization: Joint\nprivacy-utility optimization in LanguageModels.\nIn Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 3799–3807, Online. Association\nfor Computational Linguistics.\nJohn Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. Textattack: A framework\nfor adversarial attacks, data augmentation, and adver-\nsarial training in nlp. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 119–126.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus and\ncloze evaluation for deeper understanding of common-\nsense stories. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 839–849, San Diego, California.\nAssociation for Computational Linguistics.\nShrimai Prabhumoye, Alan W Black, and Ruslan\nSalakhutdinov. 2020. Exploring controllable text\ngeneration techniques. In Proceedings of the 28th In-\nternational Conference on Computational Linguistics,\npages 1–14, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may i introduce the gyafc dataset: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn NAACL.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2021. A recipe\nfor arbitrary text style transfer with large language\nmodels. arXiv preprint arXiv:2109.03910.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin\nKirchhoff. 2020. Masked language model scoring.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nMaarten Sap, Marcella Cindy Prasettio, Ari Holtzman,\nHannah Rashkin, and Yejin Choi. 2017. Connotation\nframes of power and agency in modern films. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages\n2329–2334, Copenhagen, Denmark. Association for\nComputational Linguistics.\nMaarten Sap, Swabha Swayamdipta, Laura Vianna,\nXuhui Zhou, Yejin Choi, and Noah A Smith. 2021.\nAnnotators with attitudes: How annotator beliefs\nand identities bias toxic language detection. arXiv\npreprint arXiv:2111.07997.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel\ntext by cross-alignment. In Proceedings of the 31st\nInternational Conference on Neural Information\nProcessing Systems, pages 6833–6844.\nEric Wallace, Mitchell Stern, and Dawn Xiaodong Song.\n2020. Imitation attacks and defenses for black-box\nmachine translation systems. In EMNLP.\nAlex Warstadt, Amanpreet Singh, and Samuel R\nBowman. 2019. Neural network acceptability\njudgments. Transactions of the Association for\nComputational Linguistics, 7:625–641.\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Guru-\nrangan, Maarten Sap, Dan Klein, and UC Berkeley.\nDetoxifying language models risks marginalizing\nminority voices.\nKevin Yang and Dan Klein. 2021. FUDGE: Con-\ntrolled text generation with future discriminators.\nIn Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 3511–3535, Online. Association\nfor Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore:\nEvaluating text generation with bert. InInternational\nConference on Learning Representations.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Christiano,\nand Geoffrey Irving. 2019. Fine-tuning language\nmodels from human preferences. arXiv preprint\narXiv:1909.08593.\n411\nA Appendix\nA.1 Code and Data Directory Structure\nWe have provided all our code, data and our\ngenerations in https://github.com/\nmireshghallah/mixmatch, and our\ncheckpoints are uploaded anonymously here\nhttps://zenodo.org/record/5855005.\nThere is a readme file in the repo, which has\ninstructions on how to run generation and get eval-\nuation metrics. We have not included the data files\nfor the formality, since the GY AFC dataset requires\npermission for access, so we cannot release it.\nA.2 Sample Generations\nDue to page limitations in the body of the paper, we\ninclude more sample generations from our method\nin the form of tables here. We have no samples from\nthe formality transfer task, however, since the data\nused (GY AFC) is protected and needs permissions\nfor access, so we cannot publish it. However, we\nhave provided code needed to reproduce our results,\nonce access to the original data is gained. Table 8\nshows FUDGE generations versus Mix and Match\ngenerations.\nB Experimental Setup Details\nB.1 Tasks and Datasets\nControllable debiasing (ROC story cor-\npus): We use the subset of the ROC story\ncorpus (Mostafazadeh et al., 2016) test-set that is\nused by PowerTransformer (Ma et al., 2020) for\ntheir evaluations. We use this data for controllable\ndebiasing, a text revision task which aims to correct\nthe implicit and potentially undesirable agency\nbiases in character portrayals. This test-set consists\nof 549 sentences, where 224 sentences have low\nagency verbs (such as wish, dream, etc.) and the rest\nhave high agency (like pursue, achieve, etc.). The\ntask is to revise the sentences such that the meaning\nis preserved, but the agency of the sentence is\nchanged in the target direction.\nSentiment transfer (Yelp): We use Yelp (Shen\net al., 2017) dataset’s test-set for the task of\nsentiment transfer. The test set comprises of 1000\nsentences, half with positive and half with negative\nsentiment. We also have a reference set of hand\nwritten sentiment transferred sentences, provided\nby (He et al., 2020) that we use for reporting\nevaluation metrics.\nFormality transfer (GYAFC): We use 1051\nsentences from the test-set of the GYAFC (Rao\nand Tetreault, 2018) dataset, which contains formal\nand informal sentences for the task of formality\ntransfer (both directions of formal to informal and\ninformal to formal). Here we use the entertainment\nand music domain subset of this data, following the\nevaluation setup of (He et al., 2020). This dataset\nalso contains parallel data between formal and\ninformal sentences, which we use as reference for\nreporting evaluation metrics.\nPrompted generation: We evaluate our approach\non two forms of prompted generation: 1) sentiment\ncontrolled generation, and 2) topic controlled\ngeneration. on prompted generation. For sentiment\ncontrolled generation, we set Mix and Match LM\nto generate text with positive or negative sentiment\ngiven prompts (listed in Appendix B.9) by using\na Yelp sentiment classifier as discriminator and\ncompare against PPLM (Dathathri et al., 2020)\nwhich is a popular sentiment controlled generation\nmethod. For topic controlled generation, we\ncompare against FUDGE (Yang and Klein, 2021),\nand follow their experimental setup consisting of\n7 distinct topics and 20 prompts.\nB.2 Expert Component Configurations\nWe use a Huggingface pre-trained bert-base-\nuncased model as our MLM for yielding Emlm\nand also providing the proposal distribution in our\nMH MCMC sampler. For obtainingEdisc, we train\nBERT-based classifiers on the training-set of our\ndatasets to use as our attribute discriminators. Al-\nthough we could have used any pre-trained attribute\nclassifier from a model repository like Huggingface\nfor Edisc, we train our own classifier for controlled\nempirical comparison. As described later, we do\nuse pretrained Huggingface attribute classifiers\nas external attribute classifiers for fair evaluation\nagainst baselines. For experiments in which we add\nthe BertScore (Zhang et al., 2020) component to the\nenergy, we download the pre-trained roberta-\nlarge_L17 models from Huggingface, respec-\ntively. We have provided implementation details\nand hyperparameter ablations of all the experiments\nin Appendix B.6, B.7, B.8 and B.9.\nB.3 Baselines\nPowerTransformer. For the task of controllable\ndebiasing (agency revision), we compare our\nwork with PowerTransformer (Ma et al., 2020),\nan approach that uses paraphrasing and self-\nsupervision based on a reconstruction loss, building\non pre-trained language models, to re-write text and\ncontrol agency level of sentences.\nHe et al. For style transfer on sentiment an formal-\n412\nTable 8: Samples of prompted topic controlled generations, using our Mix and Match LM and FUDGE.\nOurs (Mix and Match LM) FUDGEComputer\nto review, please link to (chessworld.net/chessworld/download.html). to review, instead of using the \"n/a\" flag (like on our previous posts)\nin summary, key program clients are homeforge, blogdev and skeptic.net.in summary:- install and run a local mysql server on the host computer-\nadd a mysql table\nit has been shown using several techniques, including microscopy,\nelectron microscopy, and digital loansharking.\nit has been shown using ebpf/ebpis (extraction of a new ebp\nLegal\nthe connection to the assault was not without controversy, especially given\nthe expert testimony the prosecutor had provided.\nthe connection failed, however, under an audit of one of the two, the judge\nsaid. the\nto review, or submit information to the cdu regarding the current\n(constitutionally) electoral law.\nto review, the court’s decision not to review the case raises an important\nquestion. the court’s\nto conclude, when a claim is not true, the defendant’s claims are often\nnot true.\nto conclude, the court held a motion is properly made to dismiss a claim\nfor an award of attorney\nMilitary\nfoundational to this is the cold war, which eliminates all military defense\navailable to the enemy.\nfoundational to this is an attack on the conventional wisdom on the left\nthat the left is the party\nviews on the civil war fleet, the national maritime museum. views on the\nroyal navy, admiralty.\nviews on russia’s military buildup on the strength of his repeated\ninsistence, a number of\nto conclude, we all agree that constructive defense methods are not yet\navailable.\nconstructive defense? to conclude, the russian navy’s attack on the\nmalaysian ship, a taskforce carrying out exercises,\nPolitics\nan illustration of: the historical background, culture, and general political\nsignificance of the books’ contents.\nan illustration of an anti-democratic regime under a fascist dictatorship\nand its suppression of the popular opposition and\nthe issue focused on socialism, democracy, social justice and self-\ngovernment in countries across the globe.\nthe issue focused on religious freedom in the country’s constitution, a\nfundamental pillar of u.s.\nin this essay, king frederick iii of prussia was prominently featured in\namerican post-civil war culture.\nin this essay, the term \"political correctness\" is used to refer to political\ndemands imposed on the\nReligion\nthe issue focused on the inferiority of conservatives ( \"\" religious\nconservatives \"\" ) vs . atheists .\nthe issue focused on religious freedom, particularly when the bible teaches\nthat god is \"the creator.\"\nto summarise accurately the wind direction, additional characters may\nbe added to the classification table below.\nto summarise, if the present-day christian churches were a monastic order\nof the monks rather\nan illustration of the natural history of wales by francis bacon. bateson,\ncharles (1839).\nan illustration of an ancient bronze age village in the northern greek region\nof crete, which shows a\nScience\nprior to this date, the manuscript was not currently available on the\ninternet, and is cited rarely.\nprior to this experiment, the scientists had not seen a new species in the\narea since the late 1800\nthe relationship has inspired research into the role of women in economics,\nand contributions to feminist economic theory.\nthe relationship between energy use and energy use as a function of time\nwas also investigated using a linear mixed\nthe issue focused on developments in the field of \"darwinism, biology\nand human evolution\" research.\nthe issue focused on data retention, and the key elements of the retention\nmatrix, including retention of identifiers\nSpace\nfurthermore, the performance space is \"packed with classical music\" and\nis \"lavishly decorated\".\nfurthermore, the eighty-first star is the planet’s largest moon and it sits\ndirectly in between\nto conclude, an asteroid becomes, mathematically, the largest asteroid\nto ever be \"discovered\".\nto conclude, scientists behind spacemonkey, and a number of the other\nprojects that nasa is supporting\nto summarise other countries’respective territorial claims, including\nterritorial waters, islands, etc. .\nto summarise: x (1x a2 a19 a1 a2 b2\nTable 9: Sentiment transfer on Yelp dataset ablation study. The tuples in the first column show the(α,δ,β) set of\nparameters. We ablate the effect that different components have on the transfer.The(ref)/(src) next to the metrics\ndenotes that they are measured with respect to the reference/source text. Int./Ext. Clsf. show the accuracy of the\ndiscriminator used in the energy/external discriminator from Huggingface.Hamm. shows the Hamming distance.\n(Disc, MLM, Hamm.) BLEU GPT-2 BertScore Hamm. Int. Clsf. Ext. Clsf.\n(1,0,1) 4 .77 1611 .8 0 .88 5 .308 81 .7 67.4\n(1,0,0) 1 .12 3825 .3 0 .85 8 .378 99 .0 84.5\n(0,1,0) 3 .77 101 .3 0 .90 5 .92 24 .7 29.3\n(100,1,0) 2 .89 143 .0 0 .88 7 .067 99 .2 96.5\n(0,1,50) 23 .60 110 .0 0 .99 0 .002 4 .3 5.0\n(100,1,50) 19 .71 191 .5 0 .95 1 .838 94 .7 82.8\nity domains, we compare our work with He et al.\n(2020), a generative style transfer framework which\nuses a variational autoencoder (V AE) built using a\nsequence-to-sequence LSTM-based model to do un-\nsupervised style transfer. This framework needs to\nbe trained from scratch for each style transfer task.\nUNMT. As a second baseline for style transfer,\nwe compare our work with UNMT (Lample\n413\nTable 10: Formality transfer on GYAFC dataset ablation study. The tuples in the first column show the(γ,η) set\nof parameters. We ablate the effect the BLEURT and BertScore experts have on the transfer. The(ref)/(src) next\nto the metrics denotes that they are measured with respect to the reference/source text.Int. Clsf.shows the accuracy\nof the discriminator used in the energy, and→Informal/Form. shows the breakdown of the external classifier accuracy.\nHamm. shows the Hamming distance.\n(BLEURT,BertScore) BLEU GPT-2 BertScore Hamm. Int. Clsf. →Inf. →Form.\n(100,0) 14 .07 243 .9 0 .87 5 .93 89 .34 97.41 19.80\n(300,0) 13 .75 233 .9 0 .88 5 .88 89 .34 97.01 22.94\n(0,100) 17 .78 206 .3 0 .89 5 .22 91 .15 96.67 23.13\n(0,300) 18 .85 210 .9 0 .90 4 .91 88 .23 97.04 23.13\net al., 2018), an unsupervised machine translation\nframework that demonstrates high performance for\nsentiment transfer.\nPPLM. For the task of sentiment controlled\ngeneration, we compare our work to Plug-and-Play\nLM (PPLM) Dathathri et al. (2020), which does\nattribute controlled generation using the flow of\ngradients from discriminators trained on the last\nhidden layer representations of the generator, to\nguide generation.\nFUDGE. This approach (Yang and Klein, 2021)\ntrains step-wise discriminators on partial gen-\nerations from GPT-2 to determine whether the\nconstraints related to desired attributes will be\nsatisfied by the future completion of the sequence\nor not. We compare against this on topic controlled\ngeneration as this approach was shown to be\nsuperior to PPLM on this task.\nB.4 Evaluation Metrics\nWe use a variety of evaluation metrics to compare\nour approach’s performance on two major facets:\n(1) Quality of generated text, and (2) success on\nmatching the target attribute used for control.\nB.4.1 Text Quality and Semantic Similarity\nGPT-2 PPL.We feed our generated test sentences\nto a Huggingface (Radford et al., 2019) pre-trained\nGPT-2 xl model, and report its perplexity (PPL), as\nan automatic measure of fluency. Although this mea-\nsure is not a perfect indicator of fluency, we find it to\nbe a useful metric alongside human judgements.3\nBLEU. For sentiment (Yelp) and formality\n(GY AFC) transfer experiments, since we have refer-\nence text, we report the BLEU score. For controlled\ndebiasing, we report BLEU between generated text\nand source, and show it as BLEU (src).\nBertScore. As a measure of meaning preservation,\nwe use the F1 BertScore metric (Zhang et al., 2020)\n3Due to the high variance in the PPL scores generated\nacross sentences by GPT-2, we report the median score for\neach system under comparison.\nto compare the semantic similarity of the provided\nreference sentence with the generated output.\nHamming Distance. We also report the hamming\ndistance between the source text and generated text,\nto measure the extent of the change induced by our\nframework.\nB.4.2 Attribute Quality\nInternal Classifier Accuracy. To evaluate the\nquality of applying target attributes, we report\naccuracy of the internal classifier (the discriminator\nused for generation) on the generated text, assuming\nthe target attribute is the correct label. The higher\nthis accuracy is, the better.\nExternal Classifier Accuracy. Since the internal\nclassifier is the one we are sampling from, it is\nnatural that we would get high accuracy on it,\ncompared to our baselines. To create a more\nfair comparison, we also report classification\naccuracy using external classifiers, downloaded\nfrom Huggingface. For sentiment classification\nwe use textattack/bert-base-uncased-\nyelp-polarity (Morris et al., 2020), and for\nformality we use cointegrated/roberta-\nbase-formality.\nAgency Lexicon Accuracy. For the controlled\ndebiasing experiment, we measure the accuracy\nof the change in agency by comparing the target\nagency level with that of the generated text,\nextracted using the connotation frames lexicon, and\nfollowing the setup from Ma et al. (2020).\nB.5 Hyper-parameter\nand Component Selection\nSelection of components is based on the needs\nof the task and is straight forward. You add each\ncomponent you need, to satisfy some condition.\nIf you want to do sentiment controlled generation,\nyou add a sentiment classifier. Finding the hyper-\nparameters for each component (the multiplier in\nenergy) is also simple, since the trade-off between\n414\nthe different components is clear. For instance, as\nshown in Table 9, increasing the discriminator score\nresults in a more successful sentiment transfer, and\nincreasing the Hamming score results in keeping\nthe sentence the same.\nB.6 Controllable Debiasing:\nHyper parameters\nFor the results presented in Table 2, we ran the\nGibbs chain for 8 epochs (8 iterations over all the\ntokens) for the conventional mode of our method,\nand 30 iterations for verb replacement. We used the\nparameters α=100,β =50,θ=100, where θ is the\ncoefficient assigned to the agency scorer, andα and\nβ are defined in Equations 1 and 2.\nB.7 Sentiment Transfer: Hyperparameters\nIn this section we discuss the hyperparameters used\nfor sampling and see the effects of each one. For\nthe results presented in Table 3, we ran the Gibbs\nchain for 8 epochs (8 iterations over all the tokens),\nand used the parameters α = 100,β = 25 (for Dis-\ncriminator ↑) and α=100,β =50, for Hamming↑.\nα and β are defined in Equations 1 and 2.\nTable 9 shows six different scenarios, with\nsix different coefficeints for the Disciriminator\n(α), BERT MLM ( δ) and Hamming distance ( β)\ncomponents in the energy function, which helps\nunderstand the effect each expert has.\nB.8 Formality Transfer: Hyperparameters\nFor the results presented in Table 4, we ran the Gibbs\nchain for 5 epochs (5 iterations over all the tokens),\nand used the parameters α = 140,β = 15,γ = 100\n(for Discriminator ↑) and α=140,β =50,γ =300,\nfor BertScore ↑. α, β and γ are defined in\nEquations 1 and 2.\nTable 10 shows four different scenarios, with\nfour different coefficeints for the BLEURT\nand BertScore components in the energy func-\ntion, which helps understand the effect each\nexpert has. For BLEURT, we use pre-trained\nElron/bleurt-base-512 from Hugging-\nface.\nB.9 Prompts and Hyperparameters\nUsed for Controlled Generation\nWe have listed the prompts that we used for\ncontrolled text generation (these prompts are\ntaken from Dathathri et al. (2020)): the country,\nthe lake, the chicken, the movie, the pizza,\nthe painting, the year, the city, the book, the\npotato, the horse, the road, the president, once\nupon a time. We collect these prompts from\nPPLMs github repo, available at this url:https:\n//github.com/uber-research/PPLM/\ntree/master/human_annotation/\npplm_labeled_csvs.\nPPLM has multiple knobs to tune for sam-\npling, and after running a greed search we\nfound that gamma=1,num_iterations=10\n,step_size=0.1,kl_scale=0.01 and\ngm_scale=0.95 yeild the best results (reported\nin Table 6). We generated samples by running\nthe command python run_pplm.py -D\nsentiment, with the mentioned hyperparameters.\nFor FUDGE, we tune theλ parameter, and we find\nthat λ=10 works best.\nFor our method, we ran the Gibbs chain for 15\nepochs, and used hyperparameter α = 40, from\nEq. 1. We don’t use any experts other than the yelp\nsentiment classifier, so we don’t have any other\nhyperparamters.\nB.10 Human Evaluations\nWe used Amazon Mechanical Turk for our evalua-\ntions, where each HIT was a two choice question of\n“which sentence is more fluent?” and the providers\nwere paid $0.1 per HIT. We selected Turkers from\nEnglish speaking countries. We also had each each\nquestion answered 3 times (by 3 Turkers), to create\nredundancy and robustness.\nB.11 GPU Hours and Infrastructure\nOne of the main purposes of this work is to introduce\na paradigm in which we re-use existing models and\ndo not retrain. As such, we did not need GPUs for\ntraining (we finetuned two classifier for demonstra-\ntion purposes, which took less than two GPU hours).\nHowever, we do use GPUs for inference (less\ncomputationally intensive), for generating samples.\nWe used an in-house 4GPU server (NVIDIA\nRTX2080), and the samplings and hyperparameter\ntuning took an overall of around 10-14 full days on\nthe 4 GPUs.\n415",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7540716528892517
    },
    {
      "name": "Discriminator",
      "score": 0.6197553873062134
    },
    {
      "name": "Autoregressive model",
      "score": 0.5897887945175171
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5751800537109375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5555086731910706
    },
    {
      "name": "Language model",
      "score": 0.5492451786994934
    },
    {
      "name": "Fluency",
      "score": 0.5091435313224792
    },
    {
      "name": "Energy (signal processing)",
      "score": 0.5031089186668396
    },
    {
      "name": "Task (project management)",
      "score": 0.47779518365859985
    },
    {
      "name": "Text generation",
      "score": 0.46479079127311707
    },
    {
      "name": "Codebook",
      "score": 0.4542442560195923
    },
    {
      "name": "Black box",
      "score": 0.4468909502029419
    },
    {
      "name": "Machine learning",
      "score": 0.42511874437332153
    },
    {
      "name": "Mathematics",
      "score": 0.13171544671058655
    },
    {
      "name": "Statistics",
      "score": 0.11742648482322693
    },
    {
      "name": "Engineering",
      "score": 0.08244708180427551
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}