{
  "title": "The influence of prompt engineering on large language models for protein–protein interaction identification in biomedical literature",
  "url": "https://openalex.org/W4410049544",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4208720973",
      "name": "Yung-Chun Chang",
      "affiliations": [
        "Taipei Medical University Hospital",
        "Taipei Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2912564632",
      "name": "Ming-Siang Huang",
      "affiliations": [
        "Taipei Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2485033780",
      "name": "Yi Hsuan Huang",
      "affiliations": [
        "Taipei Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2164224704",
      "name": "Yi‐Hsuan Lin",
      "affiliations": [
        "Taipei Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A4208720973",
      "name": "Yung-Chun Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2912564632",
      "name": "Ming-Siang Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2485033780",
      "name": "Yi Hsuan Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164224704",
      "name": "Yi‐Hsuan Lin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2128900650",
    "https://openalex.org/W2034694321",
    "https://openalex.org/W3009912996",
    "https://openalex.org/W2004064706",
    "https://openalex.org/W4210945150",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2016928406",
    "https://openalex.org/W2097960255",
    "https://openalex.org/W4247761839",
    "https://openalex.org/W2342313026",
    "https://openalex.org/W2104012281",
    "https://openalex.org/W2123766055",
    "https://openalex.org/W6681287948",
    "https://openalex.org/W2157953759",
    "https://openalex.org/W1969114498",
    "https://openalex.org/W2766673096",
    "https://openalex.org/W1990794790",
    "https://openalex.org/W2049118507",
    "https://openalex.org/W2512456704",
    "https://openalex.org/W4304588441",
    "https://openalex.org/W2773368817",
    "https://openalex.org/W2964348125",
    "https://openalex.org/W2841071937",
    "https://openalex.org/W3169066049",
    "https://openalex.org/W4233069070",
    "https://openalex.org/W4367595583",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4390970405",
    "https://openalex.org/W4399868914",
    "https://openalex.org/W2055214702",
    "https://openalex.org/W2042500831",
    "https://openalex.org/W2101727078",
    "https://openalex.org/W2230749025",
    "https://openalex.org/W2769093805",
    "https://openalex.org/W2169974160",
    "https://openalex.org/W2166474856",
    "https://openalex.org/W1850865022",
    "https://openalex.org/W4394789579",
    "https://openalex.org/W3097717858",
    "https://openalex.org/W4402528490",
    "https://openalex.org/W4390023570",
    "https://openalex.org/W4391973028",
    "https://openalex.org/W4390880284",
    "https://openalex.org/W4386932783",
    "https://openalex.org/W4396553888",
    "https://openalex.org/W2127603354",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2465246611",
    "https://openalex.org/W2234952631",
    "https://openalex.org/W3070389369"
  ],
  "abstract": null,
  "full_text": "The influence of prompt \nengineering on large language \nmodels for protein–protein \ninteraction identification in \nbiomedical literature\nYung-Chun Chang1,2, Ming-Siang Huang1, Yi-Hsuan Huang1 & Yi-Hsuan Lin1\nIdentifying protein–protein interactions (PPIs) is a foundational task in biomedical natural language \nprocessing. While specialized models have been developed, the potential of general-domain large \nlanguage models (LLMs) in PPI extraction, particularly for researchers without computational \nexpertise, remains unexplored. This study evaluates the effectiveness of proprietary LLMs (GPT-3.5, \nGPT-4, and Google Gemini) in PPI prediction through systematic prompt engineering. We designed six \nprompting scenarios of increasing complexity, from basic interaction queries to sophisticated entity-\ntagged formats, and assessed model performance across multiple benchmark datasets (LLL, IEPA, \nHPRD50, AIMed, BioInfer, and PEDD). Carefully designed prompts effectively guided LLMs in PPI \nprediction. Gemini 1.5 Pro achieved the highest performance across most datasets, with notable F1-\nscores in LLL (90.3%), IEPA (68.2%), HPRD50 (67.5%), and PEDD (70.2%). GPT-4 showed competitive \nperformance, particularly in the LLL dataset (87.3%). We identified and addressed a positive prediction \nbias, demonstrating improved performance after evaluation refinement. While not surpassing \nspecialized models, general-purpose LLMs with appropriate prompting strategies can effectively \nperform PPI prediction tasks, offering valuable tools for biomedical researchers without extensive \ncomputational expertise.\nKeywords Relation extraction, Protein–protein interaction, Natural language processing, Large language \nmodel\nProtein–protein interactions (PPIs) are crucial in various physiological processes, such as gene expression, \nsignal transduction, and apoptosis, which directly impact health and disease 1. Aberrations in PPIs, induced by \nfactors like mutations or infections, are strongly linked to diseases such as cancer 2,3. Moreover, PPIs influence \na variety of industries, as can be seen in their role in food processing where enzymes like chymotrypsin are \nessential for breaking down wheat gluten proteins 4, and in agriculture where protein interactions affect fruit \nripening5. The surge in biomedical literature, exemplified by PubMed’s extensive database, presents a challenge \nto researchers to efficiently mine and extract actionable insights. To help address this opportunity, advances \nin Natural Language Processing (NLP), such as Named Entity Recognition (NER), Relation Extraction (RE) 6, \nand Question Answering (QA)7, offer significant potential. The introduction of transformer-based models like \nBERT8 in 2018 revolutionized this field and have led to specialized versions like BioBERT 9, SciBERT10, and \nClinical BERT11, enhancing task-specific model performance.\nEarly methodologies for extracting Protein-Protein Interactions (PPIs) from the literature included pattern-\nbased, co-occurrence, and machine learning strategies. Pattern-based techniques, which involve manually \nconstructing rules to identify protein pairs, provide a foundational approach12,13, while co-occurrence methods \nleverage occurrences of protein pairs within single sentences to facilitate extraction, albeit with limitations in \ntheir capacity to capture complex relationships14. Advanced approaches like machine learning use comprehensive \nfeature sets that combine linguistic and structural elements to enhance PPI extraction, though these methods \noften face challenges related to structural similarities within the data15,16. Furthermore, kernel-based techniques \nsuch as sub-tree 17, subset tree 18, partial tree 19, spectrum tree 20, and feature-enriched tree 21 kernels effectively \n1Graduate Institute of Data Science, Taipei Medical University, Taipei, Taiwan. 2Clinical Big Data Research Center, \nTaipei Medical University Hospital, Taipei, Taiwan. email: changyc@tmu.edu.tw\nOPEN\nScientific Reports |        (2025) 15:15493 1| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports\n\nuse high-dimensional sentence features to optimize the extraction process 22,23; and composite kernel methods \nrefine this approach by integrating multiple kernel types to enhance the extraction and analysis of textual \ninformation24–26. Additionally, the development of the gradient-tree boosting model, LpGBoost, marks a \nsignificant advance in computational efficiency through its use of shallow data representations27. Moreover, neural \nnetwork architectures, particularly convolutional (CNN) and recurrent neural networks (RNN), play a pivotal \nrole by facilitating feature extraction autonomously; this augments kernel methods and improves the overall \nefficacy of PPI extraction tasks 28–30. The integration of transformer-based BERT models represents a further \nevolution in PPI extraction methodologies. These models incorporate deep learning techniques with lexical and \nsyntactic processing to significantly advance the field and enable more precise and effective extraction of protein \ninteractions31,32. However, despite the effectiveness of domain-specific models, their broader application and \neffective implementation are often limited by the requirement for foundational computer science knowledge. \nIn contrast, the development of large language models (LLMs), such as OpenAI’s ChatGPT introduced in 2022, \nprovides a promising alternative. These models, pre-trained on a hugely wide range of datasets, are capable \nof generating contextually relevant responses across various domains without the need for task-specific fine-\ntuning33. Moreover, the LLMs in OpenAI’s Generative Pre-trained Transformer series have evolved from \nGPT-1 to the more robust and multimodal GPT-4, which demonstrate extensive capabilities in understanding \nand generating language 34,35. These advanced models can seamlessly support a range of applications without \nrequiring domain-specific adjustments36,37.\nCompared to open-source language models, which require technical expertise in computational methods \nfor training and prediction, proprietary LLMs such as ChatGPT and Gemini 38 enable non-technical users \nto efficiently obtain domain-specific reference results through well-designed prompts. This capability can \nsignificantly accelerate research processes in fields such as pharmaceutical development39. In this study, we use \nthe protein-protein interaction (PPI) prediction task as a case study to evaluate the performance of proprietary \nLLMs in responding to specifically designed prompts, to simulate the extent of assistance available to biomedical \nresearchers. Our objective is not only to assess the intrinsic capabilities and limitations of these models in \nspecialized tasks but also to explore the extent to which prompt optimization can enhance their performance. In \nsummary, this analysis aims to provide a clearer perspective on the applicability of proprietary LLMs in handling \ncomplex biological data, and in so doing, offer valuable insights into the practical deployment of AI technologies \nin the life sciences.\nMethods\nProtein–protein interaction (PPI) task definition\nThe definition of protein-protein interaction (PPI) can vary depending on the conceptual delineation of entities \nrequired for different applications. Since being established in the 1970s 40, the interdependence of DNA, RNA, \nand proteins has been well recognized. However, the gene-centered representation commonly adopted in \nbiomedical literature tends to exacerbate the ambiguity among these molecular entities. To improve the accuracy \nof biological interaction modeling, many studies have expanded the definition of protein entities to include \ngenes and RNA 41–44. Consequently, PPI datasets differ in their definitions of protein entities based on their \nintended applications. Moreover, the definition of interactions between proteins spans multiple perspectives, \nranging from direct physical contact to broader contextual associations described in textual sources. To mitigate \nerror propagation caused by sequential entity recognition and relation prediction, most PPI datasets provide \npre-annotated entities, which facilitate subsequent modeling efforts. In this study, we adhere to the entity \nannotations provided by each dataset and conduct sentence-level predictions. During the prompt development, \nthe model is presented with a pair of named entities and sentences containing these entities, and it is tasked with \npredicting whether the protein pairs interact. Sentences containing interacting entity pairs are then classified \nas positive instances, whereas those without interactions are categorized as negative instances. If a sentence \ncontains multiple entity pairs, each pair’s relationship is assessed separately. For example, if SigK and GerE are \ndetermined to have no interaction, the instance is labeled as negative, whereas if SigK and ykvP are identified \nas interacting within the same sentence, it is labeled as positive. This is illustrated in the examples provided in \nSupplementary Table 1.\nEvaluation dataset and experiment settings\nWe employed six PPI benchmark datasets to evaluate our prompting strategies: LLL, IEPA, HPRD50, AIMed, \nBioInfer, and PEDD. The LLL dataset, which originated from the Learning Language in Logic 2005 (LLL05) \nchallenge and was sourced from the Medline database, focuses on extracting protein/gene interactions with only \n77 sentences, i.e., a limited quantity of information45. The IEPA dataset comprises 486 sentences extracted from \nPubMed abstracts46. The HPRD50 dataset includes 50 random abstracts from the Human Protein Reference \nDatabase (HPRD), totaling 145 sentences with proteins/genes pre-tagged by ProMiner and interaction \nrelationships annotated by human experts13. The AIMed dataset contains 200 abstracts from PubMed, manually \nannotated with entities and their interaction relationships47. The BioInfer dataset comprises 1100 sentences and \nwas collected using the Database of Inter-acting Proteins (DIP) to identify PubMed search inputs related to \ninteracting entities; the selected sentences contain more than one pair of interacting entities 48. We also used \nthe recently published PEDD dataset, which was derived from the AICUP 2019 competition and focuses on \nabstracts in PubMed published after 2015 and from journals with impact factors greater than 5 to ensure higher-\nquality and more recent information 49. The original distribution of positive and negative PPI instances across \nthese datasets is presented in the “Raw data” column of Supplementary Table 2. It’s worth noting that since a \nsingle sentence may contain both positive and negative instances, the total sentence count is significantly less \nthan the sum of positive and negative instances.\nScientific Reports |        (2025) 15:15493 2| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\nOur experimental workflow followed a two-phase evaluation approach. In the first phase, we selected \nrepresentative samples from the five benchmark datasets (LLL, IEPA, HPRD50, AIMed, and BioInfer) for prompt \ndesign and initial evaluation using GPT-3.5 and GPT-4. In the second phase, we conducted comprehensive \nperformance evaluations using all six datasets (including PEDD) across multiple proprietary LLMs, including \nGPT-3.5, GPT-4, and Gemini 1.5 (both Flash and Pro versions). All experiments were implemented in Python \nusing the corresponding LLM APIs. To ensure consistent evaluation, we required all LLMs to output their \npredictions in a standardized JSON format, which helped the systematic calculation of the three standard metrics \nwe employed: recall, precision, and F1-score. Precision measures the accuracy of positive predictions by calculating \nthe proportion of true positive cases among all instances identified as positive; this indicates the model’s ability \nto avoid false positives. Recall quantifies the proportion of true positives that the system correctly recognizes and \nthus reflects the model’s ability to capture all relevant interactions. The F1-score provides a balanced measure of \nthe model’s overall performance by combining precision and recall into a single value. To investigate whether \nproviding additional context could improve LLM-based PPI extraction, we experimented with multi-sentence \ninputs (ranging from 1 to 5 sentences). While adding contextual information might theoretically enhance the \nmodel’s ability to infer interactions, it also introduces the possibility of additional noise, potentially affecting \nprediction reliability. Our results showed that multi-sentence inputs led to performance fluctuations across \ndifferent models and datasets, indicating that longer inputs do not consistently improve predictive accuracy. \nTo ensure methodological consistency and reproducibility, our primary evaluation was conducted using single-\nsentence inputs. A detailed analysis of this comparison is provided in the Results section.\nMethodological framework overview\nThis study presents a systematic approach to evaluate the capability of proprietary LLMs in PPI detection through \ncarefully designed prompting strategies. Our methodology comprises three main components: (1) prompt \nengineering and optimization, (2) context-aware prompt selection, and (3) systematic evaluation across multiple \ndatasets. In the prompt engineering phase, we developed six distinct prompting scenarios, progressively increasing \nin complexity from basic interaction queries to sophisticated entity-tagged formats. These scenarios were \ndesigned to assess how different levels of input structure and contextual information affect model performance. \nThe prompts vary in two key dimensions: the degree of entity tagging (from untagged to comprehensively tagged \nwith numerical identifiers) and the specificity of query statements (from simple interaction queries to structured \nJSON output requests). The context-aware selection mechanism enables adaptive prompt deployment based \non sentence characteristics. This component specifically addresses the challenges posed by various protein \nentity representations in the biomedical literature, including complicated entities (e.g., \"Arp2/3 complex\"). The \nselection process ensures that the most appropriate prompt is applied to each specific context. For systematic \nevaluation, we implemented a comprehensive workflow that processes PPI datasets sequentially, incorporating \npairwise protein labeling and context-specific prompt selection. This structured approach allows for consistent \nassessment across different datasets while accommodating their unique characteristics. The following sections \ndetail these components, beginning with an in-depth examination of the prompting scenarios, followed by the \nadvanced workflow that integrates these prompts into a coherent PPI detection system.\nPrompting scenarios\nTo systematically evaluate PPI extraction, we designed a comprehensive prompting strategy as illustrated in \nFig. 1. The strategy categorizes input data into two primary types: Basic Entities and Nested/Complex Entities. \nFor Basic Entities, we developed three distinct entity tagging strategies: (1) No Additional Tagging, where raw \ntext is processed directly; (2) Tag Only One Protein Pair, which focuses on specific protein pairs of interest; \nFig. 1. Prompting strategies for PPI identification.\n \nScientific Reports |        (2025) 15:15493 3| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\nand (3) Tag All Protein Entities with Repeated Proteins Assigned Numbers, which provides comprehensive \nprotein markup. These tagging strategies are coupled with progressively complex query statements: from basic \ninteraction verification (Prompt 1), to identifying PPIs without explicit protein information (Prompts 2-3), to \nfull protein-aware PPI identification (Prompts 4-5). For Nested/Complex Entities, we implemented a specialized \nPairwise Gene Tagging approach (Prompt 6) to handle compound entities with special characters. More detailed \nprocessing methods for these complex cases will be introduced later. All tagged entities use the \"<GENE></\nGENE>\" markup schema to standardize the identification of protein entities across different prompting scenarios.\nSupplementary Figure 1 showcases examples of input-output pairs for prompts 1 and 3, where ‘USER’ is the \ndesignated input and ‘ ASSISTANT’ the resulting output from the GPT models. In the sentence 'LLL.d2.s2’ , three \nprotein entities are present, which leads to three pairs of input questions generated through permutation and \ncombination. Example (a) in Supplementary Figure 1 illustrates a single prompt pair for prompt 1, while Example \n(b) demonstrates the application of prompt 3 for the same sentence. Given that prompts 2 and 3 elicit multiple \noutputs of PPI results from the models, the prompt descriptions have been enhanced to include a request for \nresponse in JSON format to simplify subsequent result aggregation and analysis. Descriptions underscored on \nthe input side in the figure show the disparities between prompt 3 and prompt 2. By enumerating all protein \nentity names in the sentence, the aim is to assess whether the model can enhance its association judgment \ncapability. The most intricate prompt, prompt 5, within the same query framework as prompt 3, entails explicit \nand comprehensive entity tagging and numbering for the input sentence. Figure  2 illustrates the schematic \nrepresentation of input sentence tagging.\nTo assess the efficacy of all prompts, we manually selected ten sentences containing protein entities from each \nof the five PPI benchmark datasets. The selected sentences met the criteria of generating at least 3 to 6 positive/\nnegative instances to maintain a homogeneous distribution in the sample set. The statistical data of the sample \ndataset are presented in Figure 2 of supplementary materials. The sample dataset was applied to both the GPT-\n3.5 and GPT-4 models to select the most reliable prompt.\nDuring the data examination, we established a systematic approach to address complex entity scenarios that \nextend beyond basic predefined prompts. Our analysis revealed three primary patterns requiring specialized \nhandling: symbol-based compound entities, keyword-indicated complexes, and nested entities. First, we \nimplemented symbol-based filtering to identify compound protein entities. Sentences containing protein names \nwith \"/\" or \"-\" symbols were flagged as compound protein entities. For example, \"Arp2/3 complex\" represents two \ndistinct proteins, “ Arp2” and \"Arp3,\" functioning as a unified complex. These marked protein names underwent \nadditional analysis to ensure accurate interpretation by the model. Second, we developed keyword-based \nidentification for complex naming entities. Sentences containing specific keywords such as \"complex,\" \"subunit,\" \nor “component” were identified as potential compound naming entities. For instance, \"The Arp2/3 complex \nbinds actin filaments\" exemplifies such cases. These sentences received specialized processing by the LLM to \nprevent prediction errors arising from abbreviations or naming variations. Third, we addressed nested entity \nscenarios, where both short-form and descriptive long-form protein names appear within the same context. For \nexample, in \"... two transmembrane receptors, the p75 neurotrophin receptor and the p140trk (trkA) tyrosine kinase \nreceptor, ...\", both “p75” and its longer form \"p75 neurotrophin receptor\" appear as nested entities. To avoid \nmanual intervention, we employed a gene abbreviation recognition module to detect and process these nested \nand compound terms automatically. The prompt instructs GPT to consider complete protein names rather than \nmerely abbreviations during PPI identification.\nStatistical analysis of the datasets revealed that these complex entity patterns appear exclusively in the \nAIMed and BioInfer datasets, comprising approximately 23% of instances within each dataset. To address these \nscenarios, we developed Prompt 6, which builds upon prompt 1 by incorporating comprehensive guidelines \nfor handling compound terms, nested entities, and their variations. The entity tagging strategy requires listing \nall protein entity names and examining protein pairs systematically, with sentence processing considered \ncomplete only after evaluating all possible protein pair combinations. Figure 3 demonstrates this approach using \nBioInfer dataset examples, with the underlined text highlighting compound entity descriptions and comparing \npredictions across different LLM models.\nFig. 2. Schematic representation of tagging in protein entities.\n \nScientific Reports |        (2025) 15:15493 4| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\nA scenario-based prompting framework for PPI detection\nSentences describing PPIs often vary widely due to differences in research topics, author writing styles, and \nexperimental types, which makes them challenging to categorize. To address these challenges, we developed a \ncomprehensive framework as illustrated in Fig. 4, which begins with prompt design and evaluation using sample \ndatasets. The framework initiates from the prompting scenarios (lower left in the figure, with detailed design \nprinciples shown in Fig.  1). In this phase, input sentences are categorized based on entity complexity: basic \nentities and complex entities (including complex, compound, or nested structures). For each category, candidate \nFig. 4. PPI prompting framework.\n \nFig. 3. Design of Prompt 6 for handling complex protein entities.\n \nScientific Reports |        (2025) 15:15493 5| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\nprompts undergo evaluation using GPT models to determine the most effective prompt for that specific scenario. \nThe result is Prompt 5 for basic entities and Prompt 6 for complex entities. Following prompt assessment, all \nPPI dataset sentences undergo scenario classification based on their entity complexity. The sentences are then \nprocessed using appropriate marking strategies to highlight gene locations: basic entities undergo gene tagging \nand numbering, while complex entities are processed through pairwise gene tagging. Finally, these processed \nsentences are combined with their corresponding scenario-specific prompts for performance evaluation using \nproprietary LLMs including GPT and Gemini models. This systematic approach ensures appropriate handling of \nvarious PPI descriptions while optimizing prompt selection based on entity complexity and sentence structure. \nThe performance of the predictions can be comprehensively evaluated through this structured methodology.\nResults\nDetermination of candidate prompts\nThe primary objective of the experiment was to ascertain the optimal general prompts (Prompts 1–5) to \nhelp the model discern the different PPIs. To achieve this, we employed the PPI sample dataset delineated in \nSupplementary Figure 2 as the focal point for evaluation and conducted performance assessments of general \nprompts across both the GPT-3.5 and GPT-4 models. The findings, shown in Table 1, consistently demonstrate \nthat Prompt 5 exhibited superior performance within the sample dataset across both model iterations. According \nto Supplementary Figure 3, the design strategy of Prompt 5 helps GPT accurately identify multiple proteins with \nthe same name during PPI prediction and subsequently lists the actual interacting groups based on content \nassociations. By subsequently leveraging the efficacy demonstrated by Prompt 5, we investigated the impact \nof inputting multiple sentences simultaneously on the performance of the GPT models (Table  2). The findings \nindicate demonstrate not only GPT-4’s superior performance over GPT-3.5 but also a more consistent effect on \nmodel performance with the single-sentence input methodology.\nTo evaluate the performance of Prompt 6, which is specifically designed to handle cases involving nested \nprotein named entities, we created an evaluation sample set. This set consists of 10 sentences each from the \nAIMed and Bioinfer datasets. In the AIMed dataset, there are 15 positive instances and 27 negative instances, \nLLL IEP A HPRD50 AIMed BioInfer Avg_perf\nGPT-3.5\n 1 sentence 70.0/66.7/68.3 63.2/85.7/72.7 66.7/92.3/77.4 57.9/84.6/68.8 61.9/76.5/68.4 63.9/81.2/71.1\n 2 sentences 62.1/85.7/72.0 60.9/100.0/75.7 60.0/69.2/64.3 57.9/84.6/68.8 57.1/70.6/63.2 59.6/82.0/68.8\n 3 sentences 66.7/57.1/61.5 65.0/92.9/76.5 46.7/53.8/50.0 55.0/84.6/66.7 55.6/58.8/57.1 57.8/69.4/62.3\n 4 sentences 66.7/66.7/66.7 64.7/78.6/71.0 73.3/84.6/78.6 64.7/84.6/73.3 52.4/64.7/57.9 64.4/75.8/69.5\n 5 sentences 60.0/57.1/58.5 65.0/92.9/76.5 66.7/76.9/71.4 57.9/84.6/68.8 44.0/64.7/52.4 58.7/75.2/65.5\nGPT-4\n 1 sentence 71.4/95.2/81.6 56.0/100.0/71.8 54.2/100.0/70.3 52.0/100.0/68.4 51.5/100.0/68.0 67.4/94.3/72.0\n 2 sentences 61.8/100.0/76.4 48.3/100.0/65.1 56.5/100.0/72.2 50.0/92.3/64.9 55.6/88.2/68.2 54.4/96.1/69.4\n 3 sentences 60.0/85.7/70.6 58.3/100.0/73.7 59.1/100.0/74.3 50.0/100.0/66.7 48.3/82.4/60.9 55.1/93.6/69.2\n 4 sentences 65.5/90.5/76.0 46.7/100.0/63.6 56.5/100.0/72.2 54.5/92.3/68.6 63.6/82.4/71.8 57.4/93.0/70.4\n 5 sentences 64.0/76.2/69.6 56.0/100.0/71.8 57.9/84.6/68.8 52.4/84.6/64.7 58.3/82.4/68.3 57.7/85.6/68.6\nTable 2. Prompt 5 performance (in %) of multiple-sentence input (P/R/F). Bold values indicate highest \nperformance scores in each comparison.\n \nLLL IEP A HPRD50 AIMed BioInfer Avg_perf\nGPT-3.5\n Prompt 1 50.0/28.6/36.4 100.0/7.1/13.3 55.0/84.6/66.7 52.2/92.3/66.7 66.7/58.8/62.5 64.8/54.3/49.1\n Prompt 2 43.8/33.3/37.8 50.0/50.0/50.0 66.7/61.5/64.0 50.0/76.9/60.6 60.0/52.9/56.3 54.1/54.9/53.7\n Prompt 3 57.7/71.4/63.8 50.0/64.3/56.3 56.5/100.0/72.2 44.0/84.6/57.9 50.0/58.8/54.1 51.6/75.8/60.9\n Prompt 4 50.0/42.9/46.2 28.6/14.3/19.0 38.5/76.9/51.3 31.6/46.2/37.5 51.7/88.2/65.2 40.1/53.7/43.8\n Prompt 5 70.0/66.7/68.3 63.2/85.7/72.7 66.7/92.3/77.4 57.9/84.6/68.8 61.9/76.5/68.4 69.9/81.2/71.1\nGPT-4\n Prompt 1 41.7/23.8/30.3 40.0/14.3/21.1 46.4/100.0/63.4 41.9/100.0/59.1 55.0/64.7/59.5 45.0/60.6/46.7\n Prompt 2 50.0/42.9/46.2 60.0/64.3/62.1 66.7/61.5/64.0 52.4/84.6/64.7 52.9/52.9/52.9 56.4/61.2/58.0\n Prompt 3 60.7/81.0/69.4 56.0/100.0/71.8 50.0/92.3/64.9 44.8/100.0/61.9 46.2/70.6/55.8 51.5/88.7/64.8\n Prompt 4 53.8/100.0/70.0 47.1/57.1/51.6 40.6/100.0/57.8 34.2/100.0/51.0 44.8/76.5/56.5 44.5/86.7/57.4\n Prompt 5 71.4/95.2/81.6 56.0/100.0/71.8 54.2/100.0/70.3 52.0/100.0/68.4 51.5/100.0/68.0 57.0/99.0/72.0\nTable 1. Performance (in %) of general prompts (P/R/F). Bold values indicate highest performance scores in \neach comparison.\n \nScientific Reports |        (2025) 15:15493 6| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\nwhile the Bioinfer dataset contains 42 positive instances and 30 negative instances. This sample composition \nallows for a balanced assessment of the prompt’s effectiveness in identifying relevant entities. Given their similar \nquery statement structures, Prompt 1 served as the baseline for evaluation. The results presented in Table  3 \nindicate that Prompt 6 outperforms Prompt 1 in both GPT-3.5 and GPT-4 models. This highlights the beneficial \nimpact of providing compound term information on the enhancement of large language model performance.\nPerformance evaluation of PPI datasets\nBased on the previously established prompt filtering process, we evaluated the performance of GPT-3.5, GPT-4, \nand Google Gemini 1.5 (both flash and pro versions) across six PPI datasets using the final Prompts 5 and 6. \nThe “Raw data” column of Table  4 presents comparative results across all models. It was notable that Gemini \n1.5 pro demonstrated superior performance across most datasets, achieving the highest F 1-scores in the LLL \n(90.3%), IEPA (68.2%), HPRD50 (67.5%), and PEDD (70.2%) datasets. GPT-4 showed competitive performance \nparticularly in the LLL dataset (87.3% F1-score), while generally maintaining stable performance across the other \ndatasets. GPT-3.5 demonstrated consistent but relatively lower performance compared to the other models. The \nvariation in model performance across datasets reflects the inherent challenges in their characteristics. The LLL \ndataset, despite being the smallest, yielded the highest performance across all models (ranging from 79.2 to \n90.3% F1-score), which suggests that its smaller size and potentially more straightforward entity relationships \nfacilitate better model comprehension. In contrast, more complex datasets like AIMed, BioInfer, and PEDD \nshowed lower performance across all models, with F 1-scores typically ranging from 37.9 to 70.2% for the raw \ndata. This performance pattern highlights the challenges posed by complex entity features and rich content in \nthese datasets.\nFurthermore, we identified a bias in all models towards predicting positive input instances, which led to a \ndiminished discriminatory capability when processing exclusively negative instances. To address this limitation, \nwe excluded sentences containing solely negative instances from the datasets during performance evaluation. \nThe “Refined data” column of Table 4 demonstrates the models’ performance after this modification. The LLL \ndataset, lacking such instances, maintained its original distribution and was excluded from refined evaluation. \nIn this scenario, all models showed improved precision and F1-scores across the refined datasets. For instance, in \nthe IEPA dataset, Gemini 1.5 pro achieved an F1-score of 89.7%, while GPT-4 and GPT-3.5 reached 86.4% and \n84.7% respectively. This consistent improvement across models and datasets further validates our observation \nregarding the models’ positive instance bias and demonstrates the effectiveness of our refinement strategy.\nDiscussion\nThe application of LLMs to biological relationship extraction presents both promising opportunities and \nnotable challenges. Our comprehensive evaluation of LLMs in PPI prediction reveals several key insights \nregarding model performance, limitations, and practical implications for the biomedical research community. \nIn this discussion, we first systematically examine the relationship between model performance and dataset \ncharacteristics, followed by a detailed analysis of model-specific results. We then address key limitations and \nchallenges before exploring future directions and potential applications. Our analysis demonstrates varying \nlevels of effectiveness across different models and datasets, with particularly notable performance patterns. \nPPI dataset\nRaw data Refined data\nGPT-3.5 GPT-4 Gemini 1.5 flash Gemini 1.5 pro GPT-3.5 GPT-4 Gemini 1.5 flash Gemini 1.5 pro\nLLL 86.3/73.2/79.2 81.5/93.9/87.3 84.0/83.5/85.8 87.9/95.4/90.3 - - - -\nIEPA 53.0/86.0/65.6 47.2/99.4/64.0 50.3/92.7/65.1 55.8/99.2/68.2 83.5/86.0/84.7 76.4/99.4/86.4 80.4/92.7/86.9 85.8/99.4/89.7\nHPRD50 57.4/71.2/63.6 48.4/94.5/64.0 52.0/83.0/63.2 60.8/97.6/67.5 80.6/71.2/75.6 76.6/94.5/84.6 78.3/89.7/83.3 82.2/96.8/87.1\nAIMed 26.6/79.2/39.8 23.8/93.4/37.9 25.1/86.3/38.1 28.9/96.7/41.3 44.0/79.2/56.6 41.5/93.4/57.5 43.7/86.4/58.3 46.5/96.2/60.8\nBioInfer 36.9/58.6/45.3 40.1/76.7/53.0 38.6/67.9/49.2 42.6/80.9/55.3 57.3/58.6/58.0 56.6/76.7/65.2 57.7/67.8/61.1 59.5/80.4/68.7\nPEDD 34.8/62.7/44.8 56.3/81.0/66.4 45.6/72.8/55.4 60.6/88.8/70.2 52.2/62.7/57.0 64.8/81.0/72.0 58.3/72.7/65.4 68.488.6/75.2\nTable 4. Performance (in %) on PPI datasets (P/R/F). Bold values indicate highest performance scores in each \ncomparison.\n \nAIMed BioInfer\nGPT-3.5\n Prompt 1 41.2/93.3/57.1 65.3/76.2/70.3\n Prompt 6 42.4/93.3/58.3 62.3/90.5/73.8\nGPT-4\n Prompt 1 40.0/93.3/56.0 62.3/78.6/69.5\n Prompt 6 43.8/93.3/59.6 62.1/97.6/75.9\nTable 3. Performance (in %) of complicated prompt design (P/R/F). Bold values indicate highest performance \nscores in each comparison.\n \nScientific Reports |        (2025) 15:15493 7| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\nThe most striking observation is the consistently superior performance achieved on the LLL dataset across all \nmodels, with F1-scores ranging from 79.2 to 90.3%, and Gemini 1.5 Pro achieving the highest F1-score of 90.3%. \nThis exceptional performance can be attributed to several factors, including LLL ’s smaller size, straightforward \nannotation scheme, and simpler grammatical structures, which provide a less challenging environment for \nmodel comprehension 50. These dataset-specific characteristics form the foundation for understanding the \nbroader patterns of model performance across our evaluation suite.\nThe performance variations across datasets reveal critical insights into model behavior and limitations. Beyond \nquantitative performance metrics, our linguistic analysis uncovered significant challenges in semantic parsing \nand referential understanding. We observed that LLMs demonstrate notable susceptibility to misclassification \nwhen confronted with complex interaction-related linguistic structures. Two illustrative examples underscore \nthese interpretative challenges. In the first instance, consider the sentence excerpted from BioInfer.d245 \" Here \nwe have now identified the reciprocal complementary binding site in alpha-catenin which mediates its interaction \nwith < GENE>beta-catenin</GENE> and < GENE>plakoglobin</GENE>.\" Lexical cues such as \" mediates\" and \n\"interaction\" precipitated a model-driven misinterpretation, erroneously suggesting a direct PPI relationship \nbetween beta-catenin and plakoglobin. Critically, the models overlooked the nuanced reality that alpha-\ncatenin separately interacts with these entities. The referential narrative structure, particularly the pronomial \nphrase \"its interaction,\" represents a significant interpretative vulnerability for contemporary LLMs. A second \nexample, excerpted from AIMed.d182, further highlights the challenges of referential disambiguation in LLMs. \nSpecifically, the sentence “ The MDM2 oncoprotein is a cellular inhibitor of the  < GENE>p53</GENE> tumor \nsuppressor in that it can bind the transactivation domain of  <GENE>p53</GENE> and downregulate its ability \nto activate transcription” demonstrates how repeated gene mentions require accurate coreference resolution to \npreserve semantic clarity. In this case, the two p53 entities plainly lack a protein-protein interaction. The pronoun \n“it” unambiguously references the sentence’s primary subject, \" MDM2 oncoprotein\". Nevertheless, the models \nconsistently failed to discern these critical contextual nuances, erroneously predicting an interconnection. \nThe marked contrast between simpler datasets like LLL and more complex ones such as AIMed and BioInfer \nhighlights how sentence complexity and grammatical structure significantly impact extraction accuracy. This \nis particularly evident in AIMed’s sophisticated sentence constructions and nested entities, which resulted in \nnotably lower precision scores of 26.6% and 23.8% for GPT-3.5 and GPT-4, respectively. The representation of \nbiological entities across different datasets also proves to be a crucial factor, with BioInfer’s complex compound \nentities and abbreviations significantly challenging model comprehension, as reflected in its lower F1-scores \nranging from 45.3 to 55.3% in the raw data. In contrast, datasets featuring more straightforward entity mentions, \nsuch as HPRD50, demonstrated more robust performance with F1-scores between 63.6 and 67.5%. Additionally, \nour analysis uncovered a consistent bias across all models toward predicting positive instances, leading us to \nrefine our evaluation approach by excluding sentences containing solely negative instances. This refinement \nresulted in substantial performance improvements, as evidenced by IEPA ’s F1-scores increasing from 65.6 to \n84.7% for GPT-3.5 and from 64.0 to 86.4% for GPT-4; this underscores the significant impact of data distribution \non model behavior.\nGemini 1.5 Pro demonstrated superior performance across most datasets, which can be attributed to its \nadvanced model architecture and optimization. This superior performance was particularly evident for the LLL \n(90.3% F1-score), IEPA (89.7% F1-score in refined data), and HPRD50 (87.1% F1-score in refined data) datasets. \nGPT-4 showed competitive performance, particularly in the LLL dataset (87.3% F 1-score), while maintaining \nstable performance across other datasets. GPT-3.5, while showing consistent performance, generally achieved \nlower scores compared to other models. To contextualize these results within the current research landscape, a \nrecent study by Rehana et al.51 evaluated GPT and BERT models on PPI tasks, achieving F1-scores of 86.49% (GPT-\n4 on LLL), 71.54% (GPT-4 on IEPA), and 65.0% (GPT-4 on HPRD50) using strategies such as protein dictionary \nnormalization and protein masking. While their approach relies on sophisticated preprocessing techniques, our \nprompt design approach achieves comparable or superior results while focusing solely on sentence structure, \nand in so doing, offers a more user-friendly approach for non-technical biomedical researchers. Further analysis \nof the LLMs’ behavior in specific linguistic contexts revealed additional performance patterns. We examined \nPPI sentences from the PEDD dataset containing inference-related keywords including whether, may, might, \ncould, potential, and other similar terms; we identified 2,570 cases with 282 positive and 2,288 negative instances \nfor evaluation. In these inferential contexts, Gemini 1.5 Pro maintained its superior performance with an F 1-\nscore of 0.2, followed by Gemini 1.5 Flash at 0.179 and GPT-3.5 at 0.163, while GPT-4 unexpectedly showed \nthe lowest performance at 0.154. The significantly lower F 1-scores in inferential contexts compared to general \nPPI detection suggest that LLMs struggle to interpret protein interactions when presented with hypothetical or \nuncertain relationships. Notably, all models exhibited high false positive rates in inferential contexts, with even \nthe best-performing Gemini 1.5 Pro producing 1,550 false positive cases. This observation provides additional \nevidence of a broader pattern in LLMs’ prediction behavior that warrants further investigation, as we discuss in \nour analysis of the study’s limitations.\nWhile these results demonstrate promising capabilities of LLMs in biomedical relationship extraction, several \nimportant limitations and challenges emerged from our analysis. The observed bias of LLMs towards positive \nPPI predictions warrants careful consideration. A comprehensive review discusses how biases can manifest in \nvarious natural language processing tasks, including literature mining, highlighting the tendency of LLMs to \ngenerate outputs that may favor certain narratives or perspectives 52. The ability to accurately identify both the \npresence and absence of protein interactions is crucial for understanding biological systems and developing \ntherapeutic interventions. Our findings suggest that current LLMs, despite their sophisticated architecture, may \nexhibit a systematic bias that could lead to false positive predictions in PPI detection tasks. This limitation \ncould be particularly problematic in exploratory research where identifying non-interacting protein pairs is \nas valuable as detecting interactions. Our analysis also revealed interesting variations in how different LLMs \nScientific Reports |        (2025) 15:15493 8| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\nidentify potential PPIs within established complexes. For example, when analyzing the sentence \"Arp2/3 complex \nfrom Acanthamoeba binds profilin and cross-links actin filaments \" and subsequently asked whether Arp2 and \nArp3 components interact with each other, we observed divergent interpretations: GPT-3.5 identified a positive \nPPI between Arp2 and Arp3 subunits within the complex, while GPT-4, Gemini 1.5 Flash, and Gemini 1.5 Pro \nclassified it as a negative interaction. This discrepancy highlights a fundamental challenge in how LLMs process \nrelationships between proteins that function as part of established complexes. The Arp2/3 example demonstrates \nthat even when addressing interactions between components of well-characterized molecular assemblies, LLMs \ncan produce inconsistent results. This variation represents an important consideration when employing LLMs \nfor biomedical knowledge extraction, particularly for questions about the internal structure of protein complexes \nthat may not be explicitly stated in the text.\nTraditional transformer-based models (BioBERT, PubMedBERT, SciBERT) have demonstrated superior \nperformance across these datasets, with BioBERT achieving F 1-scores ranging from 74.95% (HPRD50) to \n86.84% (LLL)51. Similarly, previous machine learning approaches have shown competitive results, with sdpCNN \nachieving 66% and 75.2% on AImed and BioInfer respectively 53, and the tree-based DSTK model achieving \nscores ranging from 71.01% (AImed) to 89.20% (LLL) 23. Based on these findings and observed limitations, we \nrecommend that practitioners implementing LLMs for PPI detection should: (1) implement additional validation \nsteps for negative predictions, (2) consider incorporating confidence scores or uncertainty measures in their \npredictions, and (3) potentially employ ensemble approaches that combine LLM predictions with traditional \nmachine learning methods that have shown robust performance in identifying negative instances. Looking \ntoward future developments, while the system testing results obtained through prompt design may not surpass \nvarious state-of-the-art deep learning models, LLMs offer unique advantages. Their ability to adapt prompt \nsuitability continuously without retraining for specific task demands54 and potential to outperform state-of-the-\nart models when fine-tuned on specific datasets 55 makes them particularly valuable for non-computer science \nexperts56. Recent research has shown promising strategies for improving LLM performance in biomedical \napplications. Few-shot learning approaches have demonstrated comparable results to SOTA models in tasks \nsuch as NER, relation extraction, summarization, and QA 55. In the Nephrology domain, chain-of-thought \n(CoT) strategies have successfully guided models in clinical decision support57. Future development of LLMs for \nbiomedical applications should explicitly address the positive prediction bias, perhaps through specialized pre-\ntraining strategies or architectural modifications that better balance the model’s ability to identify both positive \nand negative protein interactions. LLMs have already begun to appear in highly specialized clinical research, \nsuch as extracting diagnosis information from cancer examination reports58,59. Their performance on the recent \nPEDD dataset (achieving 70.2% F1-score compared to BioBERT’s 77.06%)49 demonstrates their ability to handle \ncontemporary biomedical literature effectively, though precise task completion still relies heavily on appropriate \nprompt guidance60. In an era emphasizing multi-objective applications, the integration of large language models \ncan provide a robust foundation for development and research across various domains. Our findings suggest that \nwhen combined with effective prompt engineering strategies and domain-specific considerations, LLMs have \nthe potential to revolutionize biomedical relationship extraction, particularly for researchers without extensive \ncomputational expertise.\nConclusion\nVerifying interactions in biomedical experiments requires precise conditions and results in complex biomedical \nliterature and PPI datasets. This study shows that general-purpose LLMs like the GPT and Gemini models \ncan reliably predict protein interactions and serve as valuable tools for non-experts. Using progressive prompt \ndesigns tailored to PPI dataset specificities, we addressed model biases, such as misclassifying negative instances \nas positive, by refining data preprocessing and prompt design to significantly enhance performance. Although \nthe GPT and Gemini models currently lag behind traditional state-of-the-art methods, the rapid advancement \nof LLM technology offers a promising future. Innovative techniques, such as chain-of-thought prompts and \nensemble predictions with multiple LLMs, are expected to further improve performance. The continued \nevolution of LLMs holds significant potential for advancing research across an ever diversifying range of fields.\nData availability\nThe five benchmark PPI datasets (AImed, Bioinfer, HPRD50, IEPA and LLL) analyzed in this study are publicly \navailable on the GitHub repository at  h t t p s :   /  / g i t h u  b . c o  m / m e t a  l  r t / p   p i - d a  t a  s e t /  t  r e e /  m a  s t e r  / c s v _ o u t p u t.  A d d i t i o \nn a l l y , the latest PPI dataset referenced in this study, PPED, can be accessed via the AI CUP 2019 platform at \nhttps://www.aicup.tw/ai-cup-2019.\nReceived: 10 December 2024; Accepted: 18 April 2025\nReferences\n 1. Berggård, T., Linse, S. & James, P . Methods for the detection and analysis of protein–protein interactions. Proteomics 7(16), 2833–\n2842 (2007).\n 2. Garner, A. L. & Janda, K. D. Protein-protein interactions and cancer: targeting the central dogma. Curr. Top. Med. Chem. 11(3), \n258–280 (2011).\n 3. Hoffmann, M. et al. SARS-CoV-2 cell entry depends on ACE2 and TMPRSS2 and is blocked by a clinically proven protease \ninhibitor. Cell 181(2), 271–280.e8 (2020).\n 4. Agyare, K. K., Addo, K. & Xiong, Y . L. Emulsifying and foaming properties of transglutaminase-treated wheat gluten hydrolysate \nas influenced by pH, temperature and salt. Food Hydrocolloids 23(1), 72–81 (2009).\n 5. Wang, S. et al. Phosphorylation of MdERF17 by MdMPK4 promotes apple fruit peel degreening during light/dark transitions. \nPlant Cell 34(5), 1980–2000 (2022).\nScientific Reports |        (2025) 15:15493 9| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\n 6. Kim, J.-D., et al. Overview of BioNLP shared task 2011 In Proceedings of the BioNLP Shared Task 2011 Workshop (Association for \nComputational Linguistics, 2011).\n 7. Tsatsaronis, G. et al. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. \nBMC Bioinform. 16, 1–28 (2015).\n 8. Devlin, J., et al. BERT: Pre-training of deep bidirectional transformers for language understanding. (North American Chapter of \nthe Association for Computational Linguistics, 2019).\n 9. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36(4), \n1234–1240 (2020).\n 10. Beltagy, I., Lo, K. & Cohan, A. SciBERT: A pretrained language model for scientific text. In Conference on Empirical Methods in \nNatural Language Processing (2019).\n 11. Alsentzer, E. et al. Publicly Available Clinical BERT Embeddings (Association for Computational Linguistics, 2019).\n 12. Huang, M. et al. Discovering patterns to extract protein–protein interactions from full texts. Bioinformatics 20(18), 3604–3612 \n(2004).\n 13. Fundel, K., Küffner, R. & Zimmer, R. RelEx—Relation extraction using dependency parse trees. Bioinformatics 23(3), 365–371 \n(2007).\n 14. Bunescu, R., et al. Integrating co-occurrence statistics with information extraction for robust retrieval of protein interactions from \nMedline. In Proceedings of the hlt-naacl bionlp Workshop on Linking Natural Language and Biology (2006).\n 15. Van Landeghem, S., et al. Extracting protein-protein interactions from text using rich feature vectors and feature selection. In 3rd \nInternational symposium on Semantic Mining in Biomedicine (SMBM 2008). (Turku Centre for Computer Sciences (TUCS), 2008).\n 16. Liu, B., et al. Dependency-driven feature-based learning for extracting protein-protein interactions from biomedical text. In Coling \n2010: Posters (2010).\n 17. Vishwanathan, S. & Smola, A. J. Fast kernels for string and tree matching. Adv. Neural. Inf. Process. Syst. 15, 569–576 (2003).\n 18. Collins, M., Parsing with a single neuron: Convolution kernels for natural language problems. Technical Report, (University of \nCalifornia at Santa Cruz, 2001).\n 19. Moschitti, A. Making tree kernels practical for natural language learning. In 11th conference of the European Chapter of the \nAssociation for Computational Linguistics (2006).\n 20. Kuboyama, T. et al. A spectrum tree kernel. Inf. Media Technol. 2(1), 292–299 (2007).\n 21. Sun, L. & Han, X. A feature-enriched tree kernel for relation extraction. In Proceedings of the 52nd Annual Meeting of the Association \nfor Computational Linguistics (Volume 2: Short Papers) (2014).\n 22. Li, L. et al. An approach to improve kernel-based protein–protein interaction extraction by learning from large-scale network data. \nMethods 83, 44–50 (2015).\n 23. Murugesan, G., Abdulkadhar, S. & Natarajan, J. Distributed smoothed tree kernel for protein–protein interaction extraction from \nthe biomedical literature. PLoS ONE 12(11), e0187379 (2017).\n 24. Miwa, M. et al. Protein–protein interaction extraction by leveraging multiple kernels and parsers. Int. J. Med. Inform. 78(12), e39–\ne46 (2009).\n 25. Li, L. et al. Integrating semantic information into multiple kernels for protein-protein interaction extraction from biomedical \nliteratures. PLoS ONE 9(3), e91898 (2014).\n 26. Chang, Y .-C. et al. PIPE: A protein–protein interaction passage extraction module for BioCreative challenge. Database 2016, \nbaw101 (2016).\n 27. Warikoo, N., Chang, Y .-C. & Ma, S.-P . Gradient boosting over linguistic-pattern-structured trees for learning protein–protein \ninteraction in the biomedical literature. Appl. Sci. 12(20), 10199 (2022).\n 28. Hsieh, Y .-L., et al. Identifying protein-protein interactions in biomedical literature using recurrent neural networks with long \nshort-term memory. In Proceedings of the eighth international joint conference on natural language processing (volume 2: short \npapers) (2017).\n 29. Peng, Y . & Lu, Z. Deep Learning for Extracting Protein–Protein Interactions from Biomedical literature (Association for Computational \nLinguistics, 2017).\n 30. Y adav, S., et al. Feature assisted bi-directional LSTM model for protein–protein interaction identification from biomedical texts \n(2018).\n 31. Su, P ., Peng, Y . & Vijay-Shanker, K. Improving BERT model using contrastive learning for biomedical relation extraction. In \nWorkshop on Biomedical Natural Language Processing (2021).\n 32. Warikoo, N., Chang, Y .-C. & Hsu, W .-L. LBERT: Lexically aware transformer-based bidirectional encoder representation model for \nlearning universal bio-entity relations. Bioinformatics 37(3), 404–412 (2021).\n 33. Wu, T. et al. A brief overview of ChatGPT: The history, status quo and potential future development. IEEE/CAA J. Autom. Sin. \n10(5), 1122–1136 (2023).\n 34. Radford, A., et al. Improving language understanding by generative pre-training. OpenAI preprint (2018).\n 35. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019).\n 36. Brown, T. et al. Language models are few-shot learners. Adv. Neural. Inf. Process. Syst. 33, 1877–1901 (2020).\n 37. Gallifant, J. et al. Peer review of GPT-4 technical report and systems card. PLOS Digital Health 3(1), e0000417 (2024).\n 38. Team, G., et al. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023).\n 39. Tripathi, S. et al. Large language models reshaping molecular biology and drug development. Chem. Biol. Drug Des. 103(6), e14568 \n(2024).\n 40. Crick, F . Central dogma of molecular biology. Nature 227(5258), 561–563 (1970).\n 41. Jaeger, S. et al. Integrating protein-protein interactions and text mining for protein function prediction. BMC Bioinform. 9, S2 \n(2008).\n 42. Krallinger, M., Valencia, A. & Hirschman, L. Linking genes to literature: text mining, information extraction, and retrieval \napplications for biology. Genome Biol. 9, 1–14 (2008).\n 43. Taha, K. & Y oo, P . D. Predicting the functions of a protein from its ability to associate with other molecules. BMC Bioinform. 17, \n1–28 (2016).\n 44. Al-Aamri, A. et al. Constructing genetic networks using biomedical literature and rare event classification. Sci. Rep. 7(1), 15784 \n(2017).\n 45. Nédellec, C. Learning language in logic-genic interaction extraction challenge. In Proceedings of the 4th Learning Language in Logic \nWorkshop (LLL05). Citeseer (2005).\n 46. Ding, J., et al. Mining MEDLINE: abstracts, sentences, or phrases?, In Biocomputing 2002, 326–337. (World Scientific, 2001).\n 47. Bunescu, R. et al. Comparative experiments on learning information extractors for proteins and their interactions. Artif. Intell. \nMed. 33(2), 139–155 (2005).\n 48. Pyysalo, S. et al. BioInfer: a corpus for information extraction in the biomedical domain. BMC Bioinform. 8, 1–24 (2007).\n 49. Huang, M.-S. et al. Surveying biomedical relation extraction: a critical examination of current datasets and the proposal of a new \nresource. Brief. Bioinform. 25(3), bbae132 (2024).\n 50. Gajendran, S., Manjula, D. & Sugumaran, V . Character level and word level embedding with bidirectional LSTM–Dynamic \nrecurrent neural network for biomedical named entity recognition from literature. J. Biomed. Inform. 112, 103609 (2020).\n 51. Rehana, H. et al. Evaluating GPT and BERT models for protein–protein interaction identification in biomedical text. Bioinform. \nAdv. 4(1), vbae133 (2024).\nScientific Reports |        (2025) 15:15493 10| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/\n 52. Guo, Y ., et al. Bias in large language models: Origin, evaluation, and mitigation. arXiv preprint arXiv:2411.10915 (2024).\n 53. Hua, L. & Quan, C. A shortest dependency path based convolutional neural network for protein-protein relation extraction. \nBiomed. Res. Int. 2016(1), 8479587 (2016).\n 54. Wang, J. et al. Review of large vision models and visual prompt engineering. Meta-Radiology 1, 100047 (2023).\n 55. Jahan, I. et al. A comprehensive evaluation of large language models on benchmark biomedical text processing tasks. Comput. Biol. \nMed. 171, 108189 (2024).\n 56. Wang, J., et al. Prompt engineering for healthcare: Methodologies and applications. arXiv preprint arXiv:2304.14670 (2023).\n 57. Miao, J. et al. Chain of thought utilization in large language models and application in nephrology. Medicina 60(1), 148 (2024).\n 58. Choi, H. S. et al. Developing prompts from large language model for extracting clinical information from pathology and ultrasound \nreports in breast cancer. Radiat. Oncol. J. 41(3), 209 (2023).\n 59. Huang, J. et al. A critical assessment of using ChatGPT for extracting structured data from clinical notes. npj Digital Med. 7(1), 106 \n(2024).\n 60. White, J., et al. A prompt pattern catalog to enhance prompt engineering with Chatgpt. arXiv preprint  arXiv:2302.11382. (2023).\nAuthor contributions\nYung-Chun Chang designed the architecture and supervised the project. Ming-Siang Huang wrote the main \nmanuscript text and refined the data interpretation. Yi-Hsuan Huang collected the data and conducted the ex -\nperiments. Yi-Hsuan Lin prepared all the tables and figures. All authors reviewed the manuscript.\nFunding\nThis research was supported by the National Science and Technology Council of Taiwan, grant number NSTC \n113-2221-E-038-019-MY3 and NSTC 113-2627-M-A49-002, as well as from the National Health Research In -\nstitutes, under grant number NHRI-12A1-PHCO-1823244. This work was also financially supported by the \nHigher Education Sprout Project, funded by the Ministry of Education (MOE) in Taiwan (grant number DP2-\nTMU-114-A-04).\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 9 9 2 9 0 - 4     .  \nCorrespondence and requests for materials should be addressed to Y .-C.C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:15493 11| https://doi.org/10.1038/s41598-025-99290-4\nwww.nature.com/scientificreports/",
  "topic": "Identification (biology)",
  "concepts": [
    {
      "name": "Identification (biology)",
      "score": 0.6601927280426025
    },
    {
      "name": "Computer science",
      "score": 0.5455446243286133
    },
    {
      "name": "Computational biology",
      "score": 0.4478797912597656
    },
    {
      "name": "Data science",
      "score": 0.4156482219696045
    },
    {
      "name": "Bioinformatics",
      "score": 0.34844571352005005
    },
    {
      "name": "Biology",
      "score": 0.23005002737045288
    },
    {
      "name": "Ecology",
      "score": 0.125713050365448
    }
  ]
}