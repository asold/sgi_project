{
  "title": "Analysis of LLMs for educational question classification and generation",
  "url": "https://openalex.org/W4402467510",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2526358607",
      "name": "Said Al Faraby",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2343857144",
      "name": "Ade Romadhony",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A246557958",
      "name": "Adiwijaya -",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4388088357",
    "https://openalex.org/W6802362319",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6796919981",
    "https://openalex.org/W6770517592",
    "https://openalex.org/W6798747615",
    "https://openalex.org/W1969598650",
    "https://openalex.org/W4360980141",
    "https://openalex.org/W6737001699",
    "https://openalex.org/W4316021318",
    "https://openalex.org/W6682850643",
    "https://openalex.org/W4387103552",
    "https://openalex.org/W6760572936",
    "https://openalex.org/W2098555131",
    "https://openalex.org/W6847416023",
    "https://openalex.org/W6637552196",
    "https://openalex.org/W6631875545",
    "https://openalex.org/W2158764547",
    "https://openalex.org/W6778379400",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W6858046781",
    "https://openalex.org/W6769437024",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W6855488573",
    "https://openalex.org/W6794800773",
    "https://openalex.org/W4390636334",
    "https://openalex.org/W6779072464",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W6676516034",
    "https://openalex.org/W3011091101",
    "https://openalex.org/W6600769638",
    "https://openalex.org/W4380575774",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W3197844043",
    "https://openalex.org/W6796497892",
    "https://openalex.org/W6779636632",
    "https://openalex.org/W6852564843",
    "https://openalex.org/W6772981003",
    "https://openalex.org/W6855537110",
    "https://openalex.org/W6772937283",
    "https://openalex.org/W6735103309",
    "https://openalex.org/W4365139036",
    "https://openalex.org/W3192849410",
    "https://openalex.org/W3001201563",
    "https://openalex.org/W3035359363",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2980345007",
    "https://openalex.org/W4385570036",
    "https://openalex.org/W4382656966",
    "https://openalex.org/W3035621030",
    "https://openalex.org/W3035408261",
    "https://openalex.org/W2152163007",
    "https://openalex.org/W4389520455",
    "https://openalex.org/W4389520362",
    "https://openalex.org/W4391914260",
    "https://openalex.org/W4205654804",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W3205791762",
    "https://openalex.org/W4401208776",
    "https://openalex.org/W4385569968",
    "https://openalex.org/W4385572124",
    "https://openalex.org/W3177334331",
    "https://openalex.org/W2964912036",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3173765893",
    "https://openalex.org/W2109609717",
    "https://openalex.org/W2988673764"
  ],
  "abstract": "Large language models (LLMs) like ChatGPT have shown promise in generating educational content, including questions. This study evaluates the effectiveness of LLMs in classifying and generating educational-type questions. We assessed ChatGPT's performance using a dataset of 4,959 user-generated questions labeled into ten categories, employing various prompting techniques and aggregating results with a voting method to enhance robustness. Additionally, we evaluated ChatGPT's accuracy in generating type-specific questions from 100 reading sections sourced from five online textbooks, which were manually reviewed by human evaluators. We also generated questions based on learning objectives and compared their quality to those crafted by human experts, with evaluations by experts and crowdsourced participants.Our findings reveal that ChatGPT achieved a macro-average F1-score of 0.57 in zero-shot classification, improving to 0.70 when combined with a Random Forest classifier using embeddings. The most effective prompting technique was zero-shot with added definitions, while few-shot and few-shot + Chain of Thought approaches underperformed. The voting method enhanced robustness in classification. In generating type-specific questions, ChatGPT's accuracy was lower than anticipated. However, quality differences between ChatGPT-generated and human-generated questions were not statistically significant, indicating ChatGPT's potential for educational content creation. This study underscores the transformative potential of LLMs in educational practices. By effectively classifying and generating high-quality educational questions, LLMs can reduce the workload on educators and enable personalized learning experiences.",
  "full_text": null,
  "topic": "Political science",
  "concepts": [
    {
      "name": "Political science",
      "score": 0.3079930543899536
    }
  ],
  "institutions": [],
  "cited_by": 21
}