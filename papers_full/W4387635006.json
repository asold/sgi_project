{
  "title": "Towards Foundation Models for Materials Science: The Open MatSci ML Toolkit",
  "url": "https://openalex.org/W4387635006",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5004865099",
      "name": "Kin Long Kelvin Lee",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5028502615",
      "name": "Carmelo Gonzales",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5063817417",
      "name": "Matthew Spellings",
      "affiliations": [
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5102805449",
      "name": "Mikhail Galkin",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5013678286",
      "name": "Santiago Miret",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5005128859",
      "name": "Nalini Kumar",
      "affiliations": [
        "Intel (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6964039048",
    "https://openalex.org/W2467249088",
    "https://openalex.org/W2940974134",
    "https://openalex.org/W1009610150",
    "https://openalex.org/W3093999435",
    "https://openalex.org/W2949095042",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W1980780902",
    "https://openalex.org/W2983301919",
    "https://openalex.org/W3023937119",
    "https://openalex.org/W4309870673",
    "https://openalex.org/W6912515347",
    "https://openalex.org/W4281691175",
    "https://openalex.org/W4244254628",
    "https://openalex.org/W2979402468",
    "https://openalex.org/W1992985800",
    "https://openalex.org/W2230728100",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3033607892",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W4366731970",
    "https://openalex.org/W3005610174",
    "https://openalex.org/W2997591727",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W1992156271",
    "https://openalex.org/W4321085451",
    "https://openalex.org/W6601487527",
    "https://openalex.org/W2981040094",
    "https://openalex.org/W3192953411",
    "https://openalex.org/W4378942352",
    "https://openalex.org/W3101694814",
    "https://openalex.org/W4287126984",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2963454111",
    "https://openalex.org/W3099406979",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W3125167458",
    "https://openalex.org/W4353007232",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W1678356000",
    "https://openalex.org/W4287325738",
    "https://openalex.org/W4286910206",
    "https://openalex.org/W3100220443",
    "https://openalex.org/W4300537282",
    "https://openalex.org/W3080555959",
    "https://openalex.org/W4307936700"
  ],
  "abstract": "Artificial intelligence and machine learning have shown great promise in\\ntheir ability to accelerate novel materials discovery. As researchers and\\ndomain scientists seek to unify and consolidate chemical knowledge, the case\\nfor models with potential to generalize across different tasks within materials\\nscience - so-called \"foundation models\" - grows with ambitions. This manuscript\\nreviews our recent progress with development of Open MatSci ML Toolkit, and\\ndetails experiments that lay the groundwork for foundation model research and\\ndevelopment with our framework. First, we describe and characterize a new\\npretraining task that uses synthetic data generated from symmetry operations,\\nand reveal complex training dynamics at large scales. Using the pretrained\\nmodel, we discuss a number of use cases relevant to foundation model\\ndevelopment: semantic architecture of datasets, and fine-tuning for property\\nprediction and classification. Our key results show that for simple\\napplications, pretraining appears to provide worse modeling performance than\\ntraining models from random initialization. However, for more complex\\ninstances, such as when a model is required to learn across multiple datasets\\nand types of targets simultaneously, the inductive bias from pretraining\\nprovides significantly better performance. This insight will hopefully inform\\nsubsequent efforts into creating foundation models for materials science\\napplications.\\n",
  "full_text": null,
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.6951777935028076
    },
    {
      "name": "Computer science",
      "score": 0.5668078064918518
    },
    {
      "name": "Open science",
      "score": 0.45058366656303406
    },
    {
      "name": "Software engineering",
      "score": 0.3435223698616028
    },
    {
      "name": "Data science",
      "score": 0.33334553241729736
    },
    {
      "name": "Archaeology",
      "score": 0.09914889931678772
    },
    {
      "name": "Physics",
      "score": 0.07459133863449097
    },
    {
      "name": "History",
      "score": 0.074494868516922
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1343180700",
      "name": "Intel (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210127509",
      "name": "Vector Institute",
      "country": "CA"
    }
  ]
}