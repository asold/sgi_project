{
    "title": "Statistical Language Modeling for Automatic Speech Recognition of Agglutinative Languages",
    "url": "https://openalex.org/W2099186380",
    "year": 2008,
    "authors": [
        {
            "id": "https://openalex.org/A5097697762",
            "name": "Ebru Arsoy",
            "affiliations": [
                "Yazd University"
            ]
        },
        {
            "id": "https://openalex.org/A343667153",
            "name": "Mikko Kurimo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5098170914",
            "name": "Murat Saralar",
            "affiliations": [
                "Yazd University"
            ]
        },
        {
            "id": "https://openalex.org/A5098460188",
            "name": "Teemu Hirsimki",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5098593185",
            "name": "Janne Pylkknen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5098416441",
            "name": "Tanel Alume",
            "affiliations": [
                "Tallinn University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5108501249",
            "name": "Haim Sak",
            "affiliations": [
                "Yazd University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2050938027",
        "https://openalex.org/W3183153947",
        "https://openalex.org/W2123261808",
        "https://openalex.org/W2032942114",
        "https://openalex.org/W2129812225",
        "https://openalex.org/W1538267912",
        "https://openalex.org/W2069712814",
        "https://openalex.org/W2010910318",
        "https://openalex.org/W153434986",
        "https://openalex.org/W2133692508",
        "https://openalex.org/W2156700117",
        "https://openalex.org/W2104605790",
        "https://openalex.org/W2117621558",
        "https://openalex.org/W1915022094",
        "https://openalex.org/W2026408911",
        "https://openalex.org/W1531783358",
        "https://openalex.org/W1797288984",
        "https://openalex.org/W1518332699",
        "https://openalex.org/W148400104",
        "https://openalex.org/W1607156211",
        "https://openalex.org/W76921649",
        "https://openalex.org/W813000",
        "https://openalex.org/W68797657",
        "https://openalex.org/W139293362",
        "https://openalex.org/W3146209407",
        "https://openalex.org/W1534448508",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W210770835",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2168119002",
        "https://openalex.org/W104222852"
    ],
    "abstract": "This work presents statistical language models trained on different agglutinative languages utilizing a lexicon based on the recently proposed unsupervised statistical morphs. The significance of this work is that similarly generated sub-word unit lexica are developed and successfully evaluated in three different LVCSR systems in different languages. In each case the morph-based approach is at least as good or better than a very large vocabulary wordbased LVCSR language model. Even though using sub-words alleviates the OOV problem and performs better than word language models, concatenation of sub-words may result in over-generated items. It has been shown that with sub-words recognition accuracy can be further improved with post processing of the decoder output (Erdoan et al., 2005; Arisoy &amp; Sara?lar, 2006). The key result of this chapter is that we can successfully apply the unsupervised statistical morphs in large vocabulary language models in all the three experimented agglutinative languages. Furthermore, the results show that in all the different LVCSR tasks, the morphbased language models perform very well compared to the reference language model based on very large vocabulary of words. The way that the lexicon is built from the word fragments allows the construction of statistical language models, in practice, for almost an unlimited vocabulary by a lexicon that still has a convenient size. The recognition was here restricted to agglutinative languages and tasks in which the language used is both rather general and matches fairly well with the available training texts. Significant performance variation in different languages can be observed here, because of the different tasks and the fact that comparable recognition conditions and training resources have not been possible to arrange. However, we believe that the tasks are still both difficult and realistic enough to illustrate the difference of performance when using language models based on a lexicon of morphs vs. words in each task. There are no directly comparable previous LVCSR results on the same tasks and data, but the closest ones which can be found are around 15% WER for a Finnish microphone speech task (Siivola et al., 2007), around 40% WER for the same Estonian task (Alum?e, 2005; Puurula &amp; Kurimo, 2007) and slightly over 30% WER for a Turkish task (Erdoan et al., 2005). Future work will be the mixing of the grammatical and statistical sub-word-based language models, as well as extending this evaluation work to new languages.",
    "full_text": " \n \n10 \nStatistical Language Modeling for Automatic \nSpeech Recognition of Agglutinative Languages \nEbru Arısoy1, Mikko Kurimo2, Murat Saraçlar1, Teemu Hirsimäki2, Janne \nPylkkönen2, Tanel Alumäe3 and Haşim Sak1 \n1Boğaziçi University, \n 2Helsinki University of Technology, \n3Tallinn University of Technology,  \n1Turkey  \n2Finland   \n3Estonia \n1. Introduction  \nAutomatic Speech Recognition (ASR) systems utilize statistical acoustic and language \nmodels to find the most probable word sequen ce when the speech signal is given. Hidden \nMarkov Models (HMMs) are used as acoustic models and language model probabilities are \napproximated using n-grams where the probability of a word is conditioned on n-1 previous \nwords. The n-gram probabilities are estimated by Maximum Likelihood Estimation. One of \nthe problems in n-gram language modeling is the data sparseness that results in non-robust \nprobability estimates especially for rare and unseen n-grams. Therefore, smoothing is \napplied to produce better estimates for these n-grams. \nThe traditional n-gram word language models are commonly used in state-of-the-art Large \nVocabulary Continuous Speech Recognition (LVSCR) systems. These systems result in \nreasonable recognition performances for la nguages such as English and French. For \ninstance, broadcast news (BN) in English can now be recognized with about ten percent \nword error rate (WER) (NIST, 2000) which results in mostly quite understandable text. Some \nrare and new words may be missing in the vo cabulary but the result has proven to be \nsufficient for many important applications, such as browsing and retrieval of recorded \nspeech and information retrieval from the sp eech (Garofolo et al., 2000). However, LVCSR \nattempts with similar systems in agglutinat ive languages, such as Finnish, Estonian, \nHungarian and Turkish so far have not resulted  in comparable performance to the English \nsystems. The main reason of this performance deterioration in those languages is their rich \nmorphological structure. In agglutinative languages, words are formed mainly by \nconcatenation of several suffixes to the roots and together wi th compounding and \ninflections this leads to millions of different, bu t still frequent word fo rms. Therefore, it is \npractically impossible to build a word-based vocabulary for speech recognition in \nagglutinative languages that would cover all the relevant words. If words are used as \nlanguage modeling units, there will be many out-of-vocabulary (OOV) words due to using \nlimited vocabulary sizes in ASR systems. It was shown that with an optimized 60K lexicon \nOpen Access Database www.intechweb.org\nSource: Speech Recognition, Technologies and Applications, Book edited by: France Mihelič and Janez Žibert,  \nISBN 978-953-7619-29-9, pp. 550, November 2008, I-Tech, Vienna, Austria\nwww.intechopen.com\n Speech Recognition, Technologies and Applications \n \n194 \nthe OOV rate is less than 1% for North Amer ican Business news (R osenfeld, 1995). Highly \ninflectional and agglutinative languages suffer from high number of OOV words with \nsimilar size vocabularies. In our Turkish BN transcription system, the OOV rate is 9.3% for a \n50K lexicon. For other agglutinative languages like Finnish and Estonian, OOV rates are \naround 15% for a 69K lexicon (Hirsimäki et al ., 2006) and 10% for a 60K lexicon respectively \nand 8.27% for Czech, a highly inflectional  language, with a 60K lexicon (Podvesky & \nMachek, 2005). As a rule of thumb an OOV word brings up on average 1.5 recognition errors \n(Hetherington, 1995). Therefore solving the OOV  problem is crucial for obtaining better \naccuracies in the ASR of agglutinative languages. OOV rate can be decreased to an extent by \nincreasing the vocabulary size. However, even  doubling the vocabulary is not a sufficient \nsolution, because a vocabulary twice as large (120K) would only reduce the OOV rate to 6% \nin Estonian and 4.6% in Turkish. In Finnish  even a 500K vocabulary of the most common \nwords still gives 5.4% OOV in the language model training material. In addition, huge \nlexicon sizes may result in confusion of acoustically similar words and require a huge \na m o u n t  o f  t e x t  d a t a  f o r  r o b u s t  l a n g u a g e  model estimates. Therefore, sub-words are \nproposed as language modeling units to alle viate the OOV and data sparseness problems \nthat plague systems based on word-based recognition units in agglutinative languages.  \nIn sub-word-based ASR; (i) words are decomposed into meaningful units in terms of speech \nrecognition, (ii) these units are used as vocabulary items in n-gram language models, (iii) \ndecoding is performed with these n-gram models and sub-word sequences are obtained, (iv) \nword-like units are generated from sub-word sequences as the final ASR output.  \nIn this chapter, we mainly focus on the de composition of words into sub-words for LVCSR \nof agglutinative languages. Due to inflection s, ambiguity and other phenomena, it is not \ntrivial to automatically split the words into meaningful parts. Therefore, this splitting can be \nperformed by using rule-based morphological analyzers or by some statistical techniques . \nThe sub-words learned with morphological anal yzers and statistical techniques are called \ngrammatical and statistical sub-words respec tively. Morphemes and stem-endings can be \nused as the grammatical sub-words. The statis tical sub-word approach presented in this \nchapter relies on a data-driven algorithm called Morfessor Baseline (Creutz & Lagus, 2002; \nCreutz & Lagus, 2005) which is a language independent unsupervised machine learning \nmethod to find morpheme-like units (called statistical morphs) from a large text corpus.  \nAfter generating the sub-word units, n-gram models are trained with sub-words similarly as \nif the language modeling units were words. In order to facilitate converting sub-word \nsequences into word sequences after decoding, word break symbols can be added as \nadditional units or special markers can be attached to non-initial sub-words in language \nmodeling. ASR systems that successfully utilize the n-gram language models trained for \nsub-word units are used in the decoding task. Finally, word-like ASR output is obtained \nfrom sub-word sequences by concatenating the sub-words between consecutive word \nbreaks or by gluing marked non-initial sub-words to initial ones. The performance of words \nand sub-words are evaluated for three agglut inative languages, Finnish, Estonian and \nTurkish. \nThis chapter is organized as follow: In Se ction 2, our statistical language modeling \napproaches are explained in detail. Section 3 contains the experimental setup for each \nlanguage. Experimental results are given in Section 4. Finally, this chapter is concluded with \na detailed comparison of the proposed approaches for agglutinative languages. \nwww.intechopen.com\nStatistical Language Modeling for Automatic Speech Recognition of Agglutinative Languages \n \n195 \n2. Statistical language modeling approaches \nThe morphological productivity of agglutinative languages makes it difficult to cons truct \nrobust and effective word-based language mode ls. With a dictionary size of a few hundred \nthousand words, we can still have OOV words, which are constructed through legal \nmorphological rules. Therefore, in addition to  words, sub-word units are utilized in LVCSR \ntasks for Finnish, Estonian and Turkish. Fig. 1 shows a phrase in each language segmented \ninto proposed grammatical and statistical sub-wo rd units. The details of these units will be \nexplained thoroughly in this section. \n \nFinnish example:\n Words:  pekingissä vieraileville suomalaisille kansanedustajille \n            Grammatical sub-words:  \n            Morphemes:  pekingi ssä # vieraile v i lle # suomalais i lle # kansa n edusta j i lle \n            Statistical sub-words: \n \n            Morphs:  peking issä # vieraile ville # suomalaisille # kansanedustaj ille \n \nEstonian example:\n Words:  teede ja sideministeerium on teinud ettepaneku \n               Grammatical sub-words:  \n               Morphemes:  tee de # ja  # side ministeerium # on # tei nud # ette paneku \n               Statistical sub-words:  \n               Morphs:  teede # ja # sideministeerium # on # te i nud # ettepaneku \n \nTurkish example:\n Words:  tüketici derneklerinin öncülüğünde \n            Grammatical sub-words:  \n            Morphemes:  tüketici # dernek leri nin # öncü lü ğ ü nde \n            Stem-endings: tüketici # dernek lerinin # öncü lü ğünde \n            Statistical sub-words:  \n            Morphs:  tüketici # dernek lerinin # öncü lü ğü nde \n \nFig. 1. Finnish, Estonian and Turkish phrases segmented into statistical and grammatical \nsub-words \n2.1 Word-based model \nUsing words as recognition units is a classical approach employed in most state-of-the-art \nrecognition systems. The word model has the ad vantage of having longer recognition units \nwhich results in better acoustic discrimination among vocabulary items. However the \nvocabulary growth for words is almost unlimited for agglutinative languages and this leads \nto high number of OOV words with moderate size vocabularies in ASR systems. It has been \nreported that the same size text corpora (40M wo rds) result in less than 200K word types for \nEnglish and 1.8M and 1.5M word types for Finnish and Estonian respectively (Creutz et al., \n2007a). The number of word types is 735K for the same size Turkish corpus. \n2.2 Sub-word-based models \nLarge number of OOV words and data sparse ness are the main drawbacks of the word-\nbased language modeling units in ASR of agglutin ative and highly inflectional languages.   \nTherefore, several sub-word units were expl ored for those languages to handle these \ndrawbacks. Naturally, there are ma ny ways to split the words in to smaller units to reduce a \nlexicon to a tractable size. However, for a sub-word lexicon suitable for language modeling \napplications such as speech recognition, several properties are desirable: \nwww.intechopen.com\n Speech Recognition, Technologies and Applications \n \n196 \ni. The size of the lexicon should be small enough that the n-gram modeling becomes more \nfeasible than the conventional word based modeling. \nii. The coverage of the target language by wo rds that can be built by concatenating the \nunits should be high enough to avoid the OOV problem. \niii. The units should be somehow meaningful, so that the previously observed units can \nhelp in predicting the next one. \niv. For speech recognition one should be able to determine the pronunciation for each unit. \nA common approach to find the sub-word units is to program the language-dependent \ngrammatical rules into a morphological analyzer and utilize it to split the text corpus into \nmorphemes. As an alternative approach, sub- word units that meet the above desirable \nproperties can be learned with unsupervised ma chine learning algorithms. In this section, \nwe investigated both of the approaches. \n2.2.1 Grammatical sub-words; morphemes and stem-endings \nUsing morphemes and stem-endings as recognition units is becoming a common approach in \nstatistical language modeling of morphologically rich languages. Morphemes were utilized as \nlanguage modeling units in agglutinative languages such as Finnish (Hirsimäki et al., 2006), \nEstonian (Alumäe, 2005) and Turkish (Hacioglu et al., 2003) as well as in Czech (Byrne et al., \n2001) which is a highly inflectional language. Merged morphemes were proposed instead of \nword phrases for Korean (Kwon and Park, 2003). In Kanevsky and Roukos (1998) stem-ending \nbased modeling was proposed for agglutinative la nguages and it is used in ASR of Turkish \nwith both surface form (Mengü şoğlu & Deroo, 2001; Bayer et al., 2006) and lexical form \n(Arısoy et al., 2007) representations of endings. In addition, a unified model using both words, \nstem-endings and morphemes was proposed for Turkish (Arısoy et al., 2006).  \nA morphological analyzer is required to obtain morphemes, stems and endings. However, \ndue to the handcrafted rules, morphological analyzers may suffer from an OOV problem, \nsince in addition to morphotactic and morphoph onemic rules, a limited root vocabulary is \nalso compiled in the morphological analyzer. For instance, a Turkish morphological parser \n(Sak et al., 2008) with  54,267 roots can anal yze 96.7% of the word tokens and 52.2% of the \nword types in a text corpus of 212M words wi th 2.2M unique words. An example output \nfrom this parser for Turkish word alın is given in Fig. 2. The English glosses are given in \nparenthesis for convenience. The inflectional morphemes start with a + sign and the \nderivational morphemes start with a - sign. Pa rt-of-speech tags are attached to roots in \nbrackets and lexical morphemes are followed by nominal and verbal morphological features \nin brackets. As was shown in Fig. 2, the morphological parsing of a word may result in  \nmultiple interpretations of that word due to complex morphology. This ambiguity can be \nresolved using morphological disambiguation tools for Turkish (Sak et al., 2007). \n \nalın[Noun]+[A3sg]+[Pnon]+[Nom] (forehead) \nal[Noun]+[A3sg]+Hn[P2sg]+[Nom] (your red) \nal[Adj]-[Noun]+[A3sg]+Hn[P2sg]+[Nom] (your red) \nal[Noun]+[A3sg]+[Pnon]+NHn[Gen] (of red) \nal[Adj]-[Noun]+[A3sg]+[Pnon]+NHn[Gen] (of red) \nalın[Verb]+[Pos]+[Imp]+[A2sg] ((you) be offended) \nal[Verb]+[Pos]+[Imp]+YHn[A2pl] ((you) take) \nal[Verb]-Hn[Verb+Pass]+[Pos]+[Imp]+[A2sg] ((you) be taken) \nFig. 2. Output of the Turkish morphological parser (Sak et al., 2008) with English glosses.  \nwww.intechopen.com\nStatistical Language Modeling for Automatic Speech Recognition of Agglutinative Languages \n \n197 \nTo obtain a morpheme-based language model, al l the words in the training text corpus are \ndecomposed into their morphemes using a morphological analyzer. Then a morphological \ndisambiguation tool is required to choose the correct analysis among all the possible \ncandidates using the given context. In Ar ısoy et al. (2007) the parse with the minimum \nnumber of morphemes is chosen as the correc t parse since the output of the morphological \nparser used in the experiments was not compatible with the available  disambiguation tools. \nAlso, a morphophonemic transducer is required  to obtain the surface form representations \nof the morphemes if the morphological parser output is in the lexical form as in Fig. 2.  \nIn statistical language modeling, there is a trade-off between using short and long units. \nWhen grammatical morphemes are used for language modeling, there can be some \nproblems related to the pronunciations of very  short inflection-type un its. Stem-endings are \na compromise between words and morphemes. They provide better OOV rate than words, \nand they lead to more robust language models than morphemes which require longer n-\ngrams. The stems and endings are also obtain ed from the morphological analyzer. Endings \nare generated by concatenating the consecutive morphemes.  \nEven though morphemes and stem-endings are logical sub-word choices in ASR, they require \nsome language dependent tools such as morp hological analyzers an d disambiguators. The \nlack of successful morphological disambiguation tools may result in ambiguous splits and the \nlimited root vocabulary compiled in the morphological parsers may result in poor coverage, \nespecially for many names and foreign words which mostly occur in news texts. \nOne way to extend the rule-based grammatical morpheme analysis to new words that \ninevitably occur in large corpora, is to sp lit the words using a similar maximum likelihood \nword segmentation by Viterbi search as in th e unsupervised word segmentation (statistical \nmorphs in section 2.2.2), but here using the lexicon of grammatical morphs. This drop s the \nOOV rate significantly and helps to choose the segmentation using the most common unit s \nwhere alternative morphological segmentations are available. \n2.2.2 Statistical sub-words; morphs \nStatistical morphs are morpheme-like units obtained by a data driven approach based on the \nMinimum Description Length (MDL) principle which learns a sub-word lexicon in an \nunsupervised manner from a training lexico n of words (Creutz & Lagus, 2005). The main \nidea is to find an optimal encoding of the data with a concise lexicon and a concise \nrepresentation of the corpus.  \nIn this chapter, we have adopted a similar approach as Hirsimäki et al. (2006). The \nMorfessor Baseline algorithm (Creutz & Lagus, 2005) is used to automatically segment the \nword types seen in the training text corp us. In the Morfessor Baseline algorithm the \nminimized cost is the coding length of the le xicon and the words in the corpus represented \nby the units of the lexicon. This MDL based cost function is especially appealing, because it \ntends to give units that are both as frequent an d as long as possible to suit well for both \ntraining the language models and also decoding of the speech. Full coverage of the language \nis also guaranteed by splitting the rare words in to very short units, even to single phonemes \nif necessary. For language models utilized in speech recognition, the lexicon of the statistical \nmorphs can be further reduced by omitting th e rare words from the input of the Morfessor \nBaseline algorithm. This operation does not reduce the coverage of the lexicon, because it \njust splits the rare words then into smaller units, but the smaller lexicon may offer a \nremarkable speed up of the recognition. The pronunciation of, especi ally, the short units \nmay be ambiguous and may cause severe proble ms in languages like English, in which the \nwww.intechopen.com\n Speech Recognition, Technologies and Applications \n \n198 \npronunciations can not be adequately determined from the orthography. In most \nagglutinative languages, such as Finnish, Esto nian and Turkish, rather simple letter-to-\nphoneme rules are, however, sufficient for most cases. \nThe steps in the process of estimating a langua ge model based on statistical morphs from a \ntext corpus is shown in Fig. 3. First word ty pes are extracted from a text corpus. Rare words \nare removed from the word type s by setting a frequency cut-off. Elimination of the rare \nwords is required to reduce the morph lexi con size. Then the remaining word types are \npassed through a word splitting transformation . Based on the learned morph lexicon, the \nbest split for each word is determined by performing a Viterbi search using within-word n-\ngram probabilities of the units. At this poin t the word break symbols, # (See Fig. 1), are \nadded between each word in order to incorporate that information in the statistical language \nmodels, as well. We prefer to use additional word break symbols in morph-based language \nmodeling since unlike stems, a statistical morph can occur at any position in a word and \nmarking the non-initial morphs increases the vocabulary size.  \n \n \nFig. 3. The steps in the process of estimating a language model based on statistical morphs \nfrom a text corpus (Hirsimäki et al., 2006). \nThe statistical morph model has several adva ntages over the rule-based grammatical \nmorphemes, e.g. that no hand-crafted rules are needed and all words can be processed, even \nthe foreign ones. Even if good grammatical morphemes are available for Finnish, it has been \nshown that the language modeling results by the statistical morphs seem to be at least as \ngood, if not better (Hirsimäki et al., 2006; Creutz et al., 2007b).  \n3. Experimental setups \nStatistical and grammatical units are used as  the sub-word approaches in the Finnish, \nEstonian and Turkish LVCSR experiments. Fo r language model training in Finnish and \nEstonian experiments we used the growing n-gram training algorithm (Siivola & Pellom, \n2005). In this algorithm, the n-grams that increase the traini ng set likelihood enough with \nrespect to the corresponding increase in the model size are accepted into the model (as in the \nMDL principle). After the growing process the model is further pruned with entropy based \npruning. The method allows us to train compact and properly smoothed models using high \norder n-grams, since only the necessary high-order statistics are collected and stored (Siivola \net al., 2007). Using the variable order n-grams we can also effectively control the size of the \nmodels to make all compared language models equally large. In this way the n-grams using \nshorter units do not suffer from a restricted span length which is the case when only 3-\ngrams or 4-grams are available. For language model training in Turkish, n-gram language \nmodels were built with SRILM toolkit (Stolcke, 2002). To be able to handle computational \nwww.intechopen.com\nStatistical Language Modeling for Automatic Speech Recognition of Agglutinative Languages \n \n199 \nlimitations, entropy-based pruning (Stolcke, 1998) is applied. In this pruning, the n-grams \nthat change the model entropy less than a given threshold are discarded from the model. \nThe recognition tasks are speaker independent fluent dictation of sentences taken from \nnewspapers and books for Finnish and Estoni an. BN transcription system is used for \nTurkish experiments. \n3.1 Finnish \nFinnish is a highly inflected language, in which words are formed mainly by agglutination \nand compounding. Finnish is also the language for which the algorithm for the \nunsupervised morpheme discov ery (Creutz & Lagus, 2002) was originally developed. The \nunits of the morph lexicon for the experiments in this paper were learned from a joint \ncorpus containing newspapers, books and newswire stories of totally about 150 million \nwords (CSC, 2001). We obtained a lexicon of 50K statistical morphs by feeding the learning \nalgorithm with the word list containing the 390K most commo n words. The average length \nof a morph was 3.4 letters including a word break symbol whereas the average word length \nwas 7.9 letters. For comparison we also create d a lexicon of 69K grammatical morphs based \non rule-based morphological analysis of the words. For language model training we used \nthe same text corpus and the growing n-gram  training algorithm (Siivola & Pellom, 2005) \nand limited the language model size to approx imately 40M n-grams for both statistical and \ngrammatical morphs and words. \nThe speech recognition task was speaker independent reading of full sentences recorded \nover fixed telephone line. Cross-word triphone models were trained using 39 hours from \n3838 speakers. The development set was 46 minutes from 79 new speakers and the \nevaluation set was another corresponding set. The models included tied state hidden HMMs \nof totally 1918 different states and 76046 Gaussi an mixture (GMM) components, short-time \nmel-cepstral features (MFCCs), maximum likelihood linear transformation (MLLT) and \nexplicit phone duration models (Pylkkönen & Kurimo, 2004). No speaker or telephone call \nspecific adaptation was performed. Real-time factor of recognition speed was about 10 xRT.  \n3.2 Estonian \nEstonian is closely related to Finnish and a similar language modeling approach was \ndirectly applied to the Estonian recognition ta sk. The text corpus used to learn the morph \nunits and train the statistical language model consisted of newspapers and books, altogether \nabout 127 million words (Segakorpus, 2005). As in the Finnish ex periments, a lexicon of 50K \nstatistical morphs was created using the Morf essor Baseline algorithm as well as a word \nlexicon with a vocabulary of 500K most common words in the corpus. The average length of \na morph was 2.9 letters including a word brea k symbol whereas the average word length \nwas 6.6 letters. The available grammatical morphs  i n  E s t o n i a n  w e r e ,  i n  f a c t ,  c l o s e r  t o  t h e  \nstem-ending models, for which a vocabulary  of 500K most common units was chosen. \nCorresponding growing n-gram language models (approximately 40M n-grams) as in \nFinnish were trained from the Estonian corpus.  \nThe speech recognition task in Estonian consisted of long sentences read by 50 randomly \npicked held-out test speakers, 8 sentences each (a  part of (Meister et al., 2002)). The training \ndata consisted of 110 hours from 1266 speakers re corded over fixed telephone line as well as \ncellular network. This task was more difficult  than the Finnish one, one reason being the \nmore diverse noise and recording conditions. The acoustic models were rather similar cross-\nwww.intechopen.com\n Speech Recognition, Technologies and Applications \n \n200 \nword triphone GMM-HMMs with MFCC featur es, MLLT transformation and the explicit \nphone duration modeling than the Finnish one, except larger: 3101 different states and 49648 \nGMMs (fixed 16 Gaussians per state). Thus, the recognition speed is also slower than in \nFinnish, about 30 xRT. No speaker or telephone call specific adaptation was performed. \n3.3 Turkish \nTurkish is another agglutinative language with relatively free wo rd order. The same \nMorfessor Baseline algorithm (Creutz & Lagu s, 2005) as in Finnish and Estonian was \napplied to Turkish texts as well. Using the 394K most common words from the training \ncorpus, 34.7K morph units were obtained. The training corpus consists of 96.4M words \ntaken from various sources: online books, newspapers, journals, magazines, etc. In average, \nthere were 2.38 morphs per word including the word brea k symbol. Therefore, n-gram \norders higher than words are required to track the n-gram word statistics and this results in \nmore complicated language models. The average length of a morph was 3.1 letters including \na word break symbol whereas the average word length was 6.4 letters.  As a reference model \nfor grammatical sub-words, we also performe d experiments with stem-endings. The reason \nfor not using grammatical morphemes is that they introduced several very short recognition \nunits. In the stem-ending model, we selected the most frequent 50K units from the corpus. \nThis corresponds to the most frequent 40.4K roots and 9.6K endings. The word OOV rate \nwith this lexicon was 2.5% for the test data. The advantage of these units compared to the \nother sub-words is that we have longer recognition units with an acceptable OOV rate. In \nthe stem-ending model, the root of each wo rd was marked instead of using word break \nsymbols to locate the word boundaries easily after recognition. In addition, a simple \nrestriction was applied to enforce the decoder not to generate consecutive ending sequences.  \nFor the acoustic data, we used the Turkis h Broadcast News database collected at Bo ğaziçi \nUniversity (Arısoy et et al., 2007). This data was partitioned into training (68.6 hours) and \ntest (2.5 hours) sets. The training and test data were disjoint in terms of the selected dates.  \nN-gram language models for different orders with interpolated Kneser-Ney smoothing were \nbuilt for the sub-word lexicons using the SR ILM toolkit (Stolcke, 2002) with entropy-based \npruning.  In order to eliminate the  effect of  language model pruning in sub-words, lattice \noutput of the recognizer was re-scored with the same order n-gram language model pruned \nwith a smaller pruning constant. The transcriptio ns of acoustic training data were used in \naddition to the text corpus in order to reduce  the effect of out-of-domain data in language \nmodeling. A simple linear interpolation approach was applied for domain adaptation. \nThe recognition tasks were performed using the AT&T Decoder (Mohri & Riley, 2002).  We \nused decision-tree state clustered cross-word  triphone models with approximately 7500 \nHMM states. Instead of using letter to phonem e rules, the acoustic models were based \ndirectly on letters. Each state of the sp eaker independent HMMs had a GMM with 11 \nmixture components. The HTK front-end (Young  et al., 2002) was used to get the MFCC \nbased acoustic features. The baseline acoustic models were adapted to each TV/Radio \nchannel using supervised MAP adaptation on the training data, giving us the channel \nadapted acoustic models. \n4. Experimental results \nThe recognition results for the three different  tasks: Finnish, Estonian and Turkish, are \nprovided in Tables 1-3. In addition to sub-word language models, large vocabulary word-\nwww.intechopen.com\nStatistical Language Modeling for Automatic Speech Recognition of Agglutinative Languages \n \n201 \nbased language models were built as the refe rence systems with similar OOV rates for each \nlanguage. The word-based reference language mo dels were trained as much as possible in \nthe same way as the corresponding morph language models. For Finnish and Estonian the \ngrowing n-grams (Siivola & Pellom, 2005) were used . For Turkish a conventional n-gram \nwith entropy-based pruning was built  by using SRILM toolkit similarly as for the morphs. \nFor Finnish, Estonian and Turkish experiments,  the LVCSR systems described in Section 3 \nare utilized. In each task the word error rate (WER) and letter error rate (LER) statistics for \nthe morph-based system is compared to corresponding grammatical sub-word-based and \nword-based systems. The resulting sub-word strings are glued to form the word-like units \naccording to the word break symbols included in the language model (see Fig. 1) and the \nmarkers attached to the units. The WER is comp uted as the sum of substituted, inserted and \ndeleted words divided by the correct number of words. In agglutinative languages the \nwords are long and contain a variable amount of morphemes. Thus, any incorrect prefix or \nsuffix would make the whole word incorrect. Therefore, in addition to WER, LER is \nincluded here as well. \n \nFinnish Lexico\nn OOV (%) WER (%) LER (%) \nWords 500 K 5.4 26.8 7.7 \nStatistical morphs 50 K 0 21.7 6.8 \nGrammatical morphemes 69 K 0* 21.6 6.9 \nTable 1. The LVCSR performance for the Finnish telephone speech task (see Section 3.1). The \nwords in (*) were segmented into grammatical morphs using a maximum likelihood \nsegmentation by Viterbi search. \n \nEstonian Lexicon OOV (%) WER (%) LER (%) \nWords 500 K 5.6 34.0 12.3 \nStatistical morphs 50 K 0 33.9 12.2 \nGrammatical morphemes 500 K 0.5* 33.5 12.4 \nTable 2. The LVCSR performance for the Estonian telephone speech (see Section 3.2). The \nwords in (*) were segmented into grammatical morphs using a maximum likelihood \nsegmentation by Viterbi search. \n \nTurkish Lexicon OOV (%) WER (%) LER (%) \nWords 100K 5.3 37.0 19.3 \nStatistical morphs 37.4K 0 35.4 18.5 \nGrammatical stem-endings 50K 2.5 36.5 18.3 \nTable 3. Turkish BN transcription performance with channel adapted acoustic models (see \nSection 3.3). Best results are obtained with 3-gram word, 5-gram morph and 4-gram stem-\nending language models. Note that roots are marked in stem-endings instead of using word \nbreak symbols. \nIn all three languages statistical morphs perfor m almost the same or better than the large \nvocabulary word reference models with smalle r vocabulary sizes. The performance of the \nmorph model is more pronounced in the Finnish system where the Morfessor algorithm was \nwww.intechopen.com\n Speech Recognition, Technologies and Applications \n \n202 \noriginally proposed. In addi tion, grammatical morphemes achieve similar performances \nwith their statistical counterparts. Even thou gh grammatical stem-endings in the Turkish \nsystem attain almost the same LER with the statistical morphs, statistical morphs perform \nbetter than stem-endings in terms of the WER. \n5. Conclusion \nThis work presents statistical language models trained on different agglutinative languages \nutilizing a lexicon based on the recently proposed unsupervised statistical morphs. The \nsignificance of this work is that similarly generated sub-word unit lexica are developed and \nsuccessfully evaluated in three different LVCSR systems in different languages. In each case \nthe morph-based approach is at least as good or better than a very large vocabulary word-\nbased LVCSR language model. Even though using sub-words alleviates the OOV problem \nand performs better than  word language mode ls, concatenation of sub-words may result in \nover-generated items. It has been shown that with sub-words recognition accuracy can be \nfurther improved with post proces sing of the decoder output (Erdo ğan et al., 2005; Ar ısoy  \n& Saraçlar, 2006). \nThe key result of this chapter is that we ca n successfully apply the unsupervised statistical \nmorphs in large vocabulary language models in all the three experimented agglutinative \nlanguages. Furthermore, the results show that  in all the different LVCSR tasks, the morph-\nbased language models perform very well compared to the reference language model based \non very large vocabulary of words. The way that the lexicon is built from the word \nfragments allows the construction of statistica l language models, in practice, for almost an \nunlimited vocabulary by a lexicon that still has a convenient size. The recognition was here \nrestricted to agglutinative languages and tasks in which the language used is both rather \ngeneral and matches fairly well with the availa ble training texts. Significant performance \nvariation in different languages can be observed  here, because of the different tasks and the \nfact that comparable recognition conditions and training resources have not been possible to \narrange. However, we believe that the tasks ar e still both difficult and realistic enough to \nillustrate the difference of performance when using language models based on a lexicon of \nmorphs vs. words in each task. There are no di rectly comparable previous LVCSR results on \nthe same tasks and data, but the closest ones which can be found are around 15% WER for a \nFinnish microphone speech task (Siivola et  al., 2007), around 40% WER for the same \nEstonian task (Alumäe, 2005; Puurula & Kuri mo, 2007) and slightly over 30% WER for a \nTurkish task (Erdoğan et al., 2005).   \nFuture work will be the mixing of the grammatical and statistical sub-word-based language \nmodels, as well as extending this evaluation work to new languages. \n6. Acknowledgments \nThe authors would like to thank Sabanc ı and ODTÜ universities for the Turkish text data \nand AT&T Labs – Research for the software. This research is partially supported by \nTÜBİTAK (The Scientific and Technological Research Council of Turkey) BDP (Unified \nDoctorate Program), TÜB İTAK Project No: 105E102, Bo ğaziçi University Research Fund \nProject No: 05HA202 and the Acade my of Finland in the projects Adaptive Informatics and \nNew adaptive and learning methods in speech recognition. \nwww.intechopen.com\nStatistical Language Modeling for Automatic Speech Recognition of Agglutinative Languages \n \n203 \n7. References \nAlumäe, T. (2005). Phonological and morp hological modeling in large vocabulary \n continuous Estonian speech recognition system, Proceedings of Second Baltic \n Conference on Human Language Technologies, pages 89–94. \nArısoy, E.; Duta ğacı, H. & Arslan, L. M. (2006). A un ified language model for large \n vocabulary continuous speech recognition of Turkish. Signal Processing, vol. 86, \npp. 2844–2862. \nArısoy, E. & Saraçlar, M. (2006). Lattice exte nsion and rescoring based approaches for \nLVCSR  of Turkish, Proceedings of Interspeech, Pittsburgh, PA, USA. \nArısoy, E.; Sak, H. & Saraçlar, M. (2007).  Language modeling for automatic Turkish \n broadcast news transcription, Proceedings of Interspeech, Antwerp, Belgium. \nBayer, A. O.; Çilo ğlu, T & Yöndem, M. T. (2006). Investigation of different language models \n for Turkish speech recognition, Proceedings of 14th IEEE Signal Processing and \n Communications Applications, pp. 1–4, Antalya, Turkey. \nByrne, W.; Hajic, J.; Ircing, P. ; Jelinek, F.; Khudanpur, S.; Krbec, P. & Psutka, J. (2001). On \n large vocabulary continuous speech recognition of highly inflectional language - \n Czech, Proceedings of Eurospeech 2001, pp. 487–490, Aalborg, Denmark. \nCreutz, M. & Lagus, K. (2002). Unsupervised discovery of morphemes, Proceedings of the \n Workshop on Morphological and Phonological Learning of ACL-02, pages 21–30. \nCreutz, M. & Lagus, K. (2005). Unsupervis ed morpheme segmentation and morphology \n induction from text corpora using Morfessor. Technical Report A81, Publications in \n Computer and Information Science, Helsinki University of Technology. URL: \n http://www.cis.hut.fi/projects/morpho/. \nCreutz, M.; Hirsimaki, T.; Kurimo, M.; Puurula, A.; Pylkkönen, J.; Siivola, V.; Varjokallio, M.; \n Arısoy, E.; Saraçlar, M. & Stolcke, A. (2007a). Analysis of morph-based speech \n recognition and the modeling of out- of-vocabulary words across languages. \n Proceedings of HLT-NAACL 2007, pp. 380–387, Rochester, NY, USA. \nCreutz, M.; Hirsimäki, T.; Kurimo, M.; Puurula, A.; Pylkkönen, J.; Siivola, V.; Varjokallio, M.; \nArısoy, E.; Saraçlar, M. & Stolcke, A. (2007b). Morph-Based Speech Recognition and \nModeling of Out-of-Vocabulary Words Across Languages. ACM Transactions on \nSpeech and Language Processing, Vol. 5, No. 1, Article 3. \nErdoğan, H.; Büyük, O. & Oflazer, K. (2005). Incorporating language constraints in sub-\nword based speech recognition. Proceedings of IEEE ASRU, San Juan, Puerto Rico \nHacıoğlu, K.; Pellom, B.; Çiloğlu, T.; Öztürk, Ö;  Kurimo, M. & Creutz, M. (2003). On lexicon \n creation for Turkish LVCSR, Proceedings of Eurospeech, Geneva, Switzerland. \nHetherington, I. L. (1995). A characterization of the problem of new, out-of-vocabulary \n words in continuous-speech recognition and understanding. Ph.D. dissertation , \n Massachusetts Institute of Technology.  \nHirsimäki T.; Creutz, M.; Siiv ola, V.; Kurimo, M.; Virpioja, S. & J. Pylkkönen. (2006). \n Unlimited vocabulary speech recognition with morph language models applied to \n Finnish. Computer, Speech and Language, vol. 20, no. 4, pp. 515–541. \nGarofolo, J.; Auzanne, G. & Voorhees, E. ( 2000). The TREC spoken document retrieval \n track: A success story, Proceedings of Content Based Multimedia Information Access \n Conference, April 12-14. \nKanevsky, D.; Roukos, S.; & Sedivy, J. (1998). Statistical language model for inflected \n languages. US patent No: 5,835,888. \nwww.intechopen.com\n Speech Recognition, Technologies and Applications \n \n204 \nKwon, O.-W. & Park, J. (2003). Korean large vo cabulary continuous speech recognition with \n morpheme-based recognition units. Speech Communication, vol. 39, pp. 287–300. \nMeister, E.; Lasn, J. & Meister, L. (2002). Estonian SpeechDat: a project in progress,  \n Proceedings of the Fonetiikan Päivät–Phonetics Symposium 2002 in Finland, pages 21–26. \nMengüşoğlu, E. & Deroo, O. (2001). Turkish LVCS R: Database preparation and language \n modeling for an agglutinative language. Proceedings of ICASSP 2001, Student Forum , \n Salt-Lake City. \nMohri, M & Riley, M. D. DCD Library – Speec h Recognition Decoder Library. AT&T Labs – \n Research. http://www.research.att.com/sw/tools/dcd/. \nNIST. (2000). Proceedings of DARPA workshop on Automatic Transcription of Broa dcast News , \n NIST, Washington DC, May. \nPodvesky, P. & Machek, P. (2005). Speech recogn ition of Czech - inclusion of rare words \n helps, Proceedings of the ACL SRW, pp. 121–126, Ann Arbor, Michigan, USA. \nPuurula, A. & Kurimo M. (2007). Vocabulary Decomposition f or Estonian Open Vocabulary \n Speech Recognition. Proceedings of the ACL 2007. \nPylkkönen, J. & Kurimo, M. (2004). Duration  modeling techniques for continuous speech \n recognition, Proceedings of the International Conference on Spoken Language Processing. \nPylkkönen, J. (2005). New pruning criteria for efficient decoding, Proceedings of 9th European \n Conference on Speech Communication and Technology. \nRosenfeld, R. (1995). Optimizing lexical and n- gram coverage via judicious use of linguistic \n data, Proceedings of Eurospeech, pp. 1763–1766. \nSak, H.; Güngör, T. & Saraçlar, M. (2008). Turkish language resources: Morphological \nparser,  morphological disambiguator and web corpus, Proceedings of 6th \nInternational Conference on Natural Language Processing, GoTAL  2008, LNAI 5221, pp. \n417–427.. \nSak, H.;  Güngör, T. & Saraçlar, M. (2007). Mo rphological disambiguation of Turkish text \n with perceptron algorithm, Proceedings of CICLing 2007, LNCS 4394, pp. 107–118. \nSegakorpus–Mixed Corpus of Estonian. Tartu University. h ttp://test.cl.ut.ee/korpused/  \n segakorpus/. \nSiivola, V. & Pellom, B. (2005). Growing an n-gram language model, Proceedings of 9th \n European Conference on Speech Communication and Technology. \nSiivola, V.; Hirsimäki, T. & Virpioja, S.  (2007). On Growing and Pruning Kneser-Ney \n Smoothed N-Gram Models. IEEE Transactions on Audio, Speech and Language \n Processing, Volume 15, Number 5, pp. 1617-1624. \nStolcke, A. (2002). SRILM - an exte nsible language modeling toolkit, Proceedings of the \n International Conference on Spoken Language Processing, pages 901–904. \nStolcke, A. (1998). Entropy-based pr uning of back-off language models, Proceedings of \n DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274. \nwww.intechopen.com\nSpeech Recognition\nEdited by France Mihelic and Janez Zibert\nISBN 978-953-7619-29-9\nHard cover, 550 pages\nPublisher InTech\nPublished online 01, November, 2008\nPublished in print edition November, 2008\nInTech Europe\nUniversity Campus STeP Ri \nSlavka Krautzeka 83/A \n51000 Rijeka, Croatia \nPhone: +385 (51) 770 447 \nFax: +385 (51) 686 166\nwww.intechopen.com\nInTech China\nUnit 405, Office Block, Hotel Equatorial Shanghai \nNo.65, Yan An Road (West), Shanghai, 200040, China \nPhone: +86-21-62489820 \nFax: +86-21-62489821\nChapters in the first part of the book cover all th e essential speech processing techniques for buildi ng robust,\nautomatic speech recognition systems: the represent ation for speech signals and the methods for speech -\nfeatures extraction, acoustic and language modeling , efficient algorithms for searching the hypothesis  space,\nand multimodal approaches to speech recognition. Th e last part of the book is devoted to other speech\nprocessing applications that can use the informatio n from automatic speech recognition for speaker\nidentification and tracking, for prosody modeling i n emotion-detection systems and in other speech pro cessing\napplications that are able to operate in real-world  environments, like mobile communication services a nd smart\nhomes.\nHow to reference\nIn order to correctly reference this scholarly work , feel free to copy and paste the following:\nEbru Arısoy, Mikko Kurimo, Murat Saraçlar, Teemu Hi rsimäki, Janne Pylkkönen, Tanel Alumäe and Haşim Sa k\n(2008). Statistical Language Modeling for Automatic  Speech Recognition of Agglutinative Languages, Spe ech\nRecognition, France Mihelic and Janez Zibert (Ed.),  ISBN: 978-953-7619-29-9, InTech, Available from:\nhttp://www.intechopen.com/books/speech_recognition/ statistical_language_modeling_for_automatic_speech_ r\necognition_of_agglutinative_languages\n\n© 2008 The Author(s). Licensee IntechOpen. This chapter is distributed\nunder the terms of the \nCreative Commons Attribution-NonCommercial-\nShareAlike-3.0 License\n, which permits use, distribution and reproduction for\nnon-commercial purposes, provided the original is properly cited and\nderivative works building on this content are distributed under the same\nlicense."
}