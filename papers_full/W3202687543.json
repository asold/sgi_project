{
  "title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content",
  "url": "https://openalex.org/W3202687543",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2791192092",
      "name": "Alan Lundgard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2022576506",
      "name": "Arvind Satyanarayan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2791192092",
      "name": "Alan Lundgard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2022576506",
      "name": "Arvind Satyanarayan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6712147730",
    "https://openalex.org/W2121278608",
    "https://openalex.org/W2165466912",
    "https://openalex.org/W2007827489",
    "https://openalex.org/W2146751667",
    "https://openalex.org/W2057718781",
    "https://openalex.org/W2972354974",
    "https://openalex.org/W2549665412",
    "https://openalex.org/W6763105583",
    "https://openalex.org/W3010343788",
    "https://openalex.org/W2969478830",
    "https://openalex.org/W2959442326",
    "https://openalex.org/W3028907449",
    "https://openalex.org/W2897132999",
    "https://openalex.org/W3031494326",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W1972170794",
    "https://openalex.org/W2083928261",
    "https://openalex.org/W2940562782",
    "https://openalex.org/W2795622273",
    "https://openalex.org/W3175819408",
    "https://openalex.org/W3121499663",
    "https://openalex.org/W3164652469",
    "https://openalex.org/W7075617674",
    "https://openalex.org/W4239920099",
    "https://openalex.org/W6757890248",
    "https://openalex.org/W6630893070",
    "https://openalex.org/W2041277401",
    "https://openalex.org/W2146695947",
    "https://openalex.org/W2133353349",
    "https://openalex.org/W2159876113",
    "https://openalex.org/W2109104042",
    "https://openalex.org/W2481240925",
    "https://openalex.org/W6745915114",
    "https://openalex.org/W2995286368",
    "https://openalex.org/W2108982845",
    "https://openalex.org/W2995255438",
    "https://openalex.org/W2789636240",
    "https://openalex.org/W2295423240",
    "https://openalex.org/W2041432983",
    "https://openalex.org/W2610600445",
    "https://openalex.org/W1987135115",
    "https://openalex.org/W2772971659",
    "https://openalex.org/W1006997171",
    "https://openalex.org/W3081277912",
    "https://openalex.org/W6784622994",
    "https://openalex.org/W2132083769",
    "https://openalex.org/W3127220606",
    "https://openalex.org/W3123633262",
    "https://openalex.org/W3126172904",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6836948611",
    "https://openalex.org/W2995882565",
    "https://openalex.org/W2044988964",
    "https://openalex.org/W3092027641",
    "https://openalex.org/W2578190051",
    "https://openalex.org/W2592469568",
    "https://openalex.org/W2063534350",
    "https://openalex.org/W3025239629",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W3128915501",
    "https://openalex.org/W2945433338",
    "https://openalex.org/W2888660171",
    "https://openalex.org/W3159989291",
    "https://openalex.org/W3205973265",
    "https://openalex.org/W6670348701",
    "https://openalex.org/W2918035772",
    "https://openalex.org/W2970875847",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2046123750",
    "https://openalex.org/W3162205072",
    "https://openalex.org/W7072257459",
    "https://openalex.org/W2766321876",
    "https://openalex.org/W1952173784",
    "https://openalex.org/W2054901814",
    "https://openalex.org/W3162032064",
    "https://openalex.org/W2073800769",
    "https://openalex.org/W2725765016",
    "https://openalex.org/W3008191831",
    "https://openalex.org/W3153965221",
    "https://openalex.org/W3029025901",
    "https://openalex.org/W6766455673",
    "https://openalex.org/W2534380090",
    "https://openalex.org/W6607692945",
    "https://openalex.org/W2093889171",
    "https://openalex.org/W2516678343"
  ],
  "abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.",
  "full_text": "© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics.\nThe ﬁnal version of this record is available at: 10.1109/TVCG.2021.3114770.\nAn accessible HTML version of the article is available at: http://vis.csail.mit.edu/pubs/vis-text-model/.\nAccessible Visualization via Natural Language Descriptions:\nA Four-Level Model of Semantic Content\nAlan Lundgard and Arvind Satyanarayan\nFig. 1. Visualizations like “Flatten the Curve” (A) efﬁciently communicate critical public health information, while simultaneously\nexcluding people with disabilities [11,28]. To promote accessible visualization via natural language descriptions (B, C), we introduce a\nfour-level model of semantic content. Our model categorizes and color codes sentences according to the semantic content they convey.\nAbstract— Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights,\nand to improve their accessibility for readers with disabilities. However, it is difﬁcult to evaluate the usefulness of these descriptions,\nand how effectively they improve access to meaningful information, because we have little understanding of the semantic content\nthey convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content\nconveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our\nmodel spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting\nstatistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex\ntrends and patterns); and elucidating domain-speciﬁc insights (e.g., social and political context). To demonstrate how our model can be\napplied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted\nreaders, and ﬁnd that these reader groups differ signiﬁcantly on which semantic content they rank as most useful. Together, our model\nand ﬁndings suggest that access to meaningful information is strongly reader-speciﬁc, and that research in automatic visualization\ncaptioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences.\nOur work further opens a space of research on natural language as a data interface coequal with visualization.\nIndex Terms—Visualization, natural language, description, caption, semantic, model, theory, alt text, blind, disability, accessibility.\n1 I NTRODUCTION\nThe proliferation of visualizations during the COVID-19 pandemic has\nunderscored their double-edged potential: efﬁciently communicating\ncritical public health information — as with the immediately-canonical\n“Flatten the Curve” chart (Fig. 1) — while simultaneously excluding\npeople with disabilities. “For many people with various types of disabil-\nities, graphics and the information conveyed in them is hard to read and\nunderstand,” says software engineer Tyler Littleﬁeld [28], who built a\npopular text-based COVID-19 statistics tracker after being deluged with\ninaccessible infographics [65, 94]. While natural language descriptions\nsometimes accompany visualizations in the form of chart captions or\nalt text (short for “alternative text”), these practices remain rare. Tech-\nnology educator and researcher Chancey Fleet notes that infographics\nand charts usually lack meaningful and detailed descriptions, leaving\ndisabled people with “a feeling of uncertainty” about the pandemic [28].\nFor readers with visual disabilities (approximately 8.1 million in the\nUnited States and 253 million worldwide [1]), inaccessible visualiza-\ntions are, at best, demeaning and, at worst, damaging to health, if not\naccompanied by meaningful and up-to-date alternatives.\nPredating the pandemic, publishers and education specialists have\nlong suggested best practices for accessible visual media, including\n• Alan Lundgard is with MIT CSAIL. E-mail: lundgard@mit.edu.\n• Arvind Satyanarayan is with MIT CSAIL. E-mail: arvindsatya@mit.edu.\nManuscript received 31 March 2021; accepted 15 July 2021. Date of\nPublication 30 September 2021; date of current version 8 October 2021. For\ninformation on obtaining reprints of this article, please send e-mail to:\nreprints@ieee.org. Digital Object Identiﬁer: 10.1109/TVCG.2021.3114770\nguidelines for tactile graphics [41] and for describing “complex images”\nin natural language [39, 99]. While valuable, visualization authors\nhave yet to broadly adopt these practices, for lack of experience with\naccessible media, if not a lack of attention and resources. Contem-\nporary visualization research has primarily attended to color vision\ndeﬁciency [21, 77, 79], and has only recently begun to engage with\nnon-visual alternatives [25, 67] and with accessibility broadly [53, 105].\nParallel to these efforts, computer science researchers have been grap-\npling with the engineering problem of automatically generating chart\ncaptions [27, 78, 83]. While well-intentioned, these methods usually\nneither consult existing accessibility guidelines, nor do they evaluate\ntheir results empirically with their intended readership. As a result, it is\ndifﬁcult to know how useful (or not) the resultant captions are, or how\neffectively they improve access to meaningful information.\nIn this paper, we make a two-fold contribution. First, we extend\nexisting accessibility guidelines by introducing a conceptual model for\ncategorizing and comparing the semantic content conveyed by natural\nlanguage descriptions of visualizations. Developed through a grounded\ntheory analysis of 2,147 natural language sentences, authored by over\n120 participants in an online study (§ 3), our model spans four levels\nof semantic content: enumerating visualization construction properties\n(e.g., marks and encodings); reporting statistical concepts and relations\n(e.g., extrema and correlations); identifying perceptual and cognitive\nphenomena (e.g., complex trends and patterns); and elucidating domain-\nspeciﬁc insights (e.g., social and political context) (§ 4). Second, we\ndemonstrate how this model can be applied to evaluate the effectiveness\nof visualization descriptions, by comparing different semantic content\nlevels and reader groups. We conduct a mixed-methods evaluation in\nwhich a group of 30 blind and 90 sighted readers rank the usefulness\nof descriptions authored at varying content levels (§ 5). Analyzing the\n1\narXiv:2110.04406v1  [cs.HC]  8 Oct 2021\nresultant 3,600 ranked descriptions, we ﬁnd signiﬁcant differences in\nthe content favored by these reader groups: while both groups generally\nprefer mid-level semantic content, they sharply diverge in their rankings\nof both the lowest and highest levels of our model.\nThese ﬁndings, contextualized by readers’ open-ended feedback, sug-\ngest that access to meaningful information is strongly reader-speciﬁc,\nand that captions for blind readers should aim to convey a chart’s trends\nand statistics, rather than solely detailing its low-level design elements\nor high-level insights. Our model of semantic content is not only\ndescriptive (categorizing what is conveyed by visualizations) and eval-\nuative (helping us to study what should be conveyed to whom) but also\ngenerative [7,8], pointing toward novel multimodal and accessible data\nrepresentations (§ 6.1). Our work further opens a space of research on\nnatural language as a data interface coequal with the language of graph-\nics [12], calling back to the original linguistic and semiotic motivations\nat the heart of visualization theory and design (§ 6.2).\n2 R ELATED WORK\nMultiple visualization-adjacent literatures have studied methods for\ndescribing charts and graphics through natural language — including\naccessible media research, Human-Computer Interaction (HCI), Com-\nputer Vision (CV), and Natural Language Processing (NLP). But, these\nvarious efforts have been largely siloed from one another, adopting\ndivergent methods and terminologies (e.g., the terms “caption” and\n“description” are used inconsistently). Here, we survey the diverse\nterrain of literatures intersecting visualization and natural language.\n2.1 Automatic Methods for Visualization Captioning\nAutomatic methods for generating visualization captions broadly fall\ninto two categories: those using CV and NLP methods when the chart\nis a rasterized image (e.g., JPEG s or PNG s); and those using structured\nspeciﬁcations of the chart’s construction (e.g., grammars of graphics).\n2.1.1 Computer Vision and Natural Language Processing\nAnalogous to the long-standing CV and NLP problem of automatically\ncaptioning photographic images [48, 58, 64], recent work on visual-\nization captioning has aimed to automatically generate accurate and\ndescriptive natural language sentences for charts [6, 22–24, 59, 78, 84].\nFollowing the encoder-decoder framework of statistical machine trans-\nlation [98, 107], these approaches usually take rasterized images of\nvisualizations as input to a CV model (the encoder), which learns the\nvisually salient features for outputting a relevant caption via a language\nmodel (the decoder). Training data consists of 〈chart, caption〉 pairs,\ncollected via web-scraping and crowdsourcing [83], or created syntheti-\ncally from pre-deﬁned sentence templates [47]. While these approaches\nare well-intentioned, in aiming to address the engineering problem of\nhow to automatically generate natural language captions for charts, they\nhave largely sidestepped the complementary (and prior) question:which\nsemantic content should be generated to begin with? Some captions\nmay be more or less descriptive than others, and different readers may\nreceive different semantic content as more or less useful, depending on\ntheir levels of data literacy, domain-expertise, and/or visual perceptual\nability [69, 71, 72]. To help orient work on automatic visualization\ncaptioning, our four-level model of semantic content offers a means of\nasking and answering these more human-centric questions.\n2.1.2 Structured Visualization Speciﬁcations\nIn contrast to rasterized images of visualizations, chart templates [96],\ncomponent-based architectures [38], and grammars of graphics [87]\nprovide not only a structured representation of the visualization’s con-\nstruction, but typically render the visualization in a structured manner\nas well. For instance, most of these approaches either render the output\nvisualization as Scalable Vector Graphics ( SVG) or provide a scene-\ngraph API. Unfortunately, these output representations lose many of\nthe semantics of the structured input (e.g., which elements correspond\nto axes and legends, or how nesting corresponds to visual perception).\nAs a result, most present-day visualizations are inaccessible to peo-\nple who navigate the web using screen readers. For example, using\nApple’s V oiceOver to read D3 charts rendered as SVG usually out-\nputs an inscrutable mess of screen coordinates and shape rendering\nproperties. Visualization toolkits can ameliorate this by leveraging\ntheir structured input to automatically add Accessible Rich Internet\nApplication (ARIA) attributes to appropriate output elements, in compli-\nance with the World Wide Web Consortium (W3C)’s Web Accessibility\nInitiative (W AI) guidelines [99]. Moreover, this structured input repre-\nsentation can also simplify automatically generating natural language\ncaptions through template-based mechanisms, as we discuss in § 4.1.\n2.2 Accessible Media and Human-Computer Interaction\nWhile automatic methods researchers often note accessibility as a wor-\nthy motivation [27, 30, 31, 78, 83, 84], evidently few have collaborated\ndirectly with disabled people [25, 71] or consulted existing accessibil-\nity guidelines [67]. Doing so is more common to HCI and accessible\nmedia literatures [73, 91], which broadly separate into two categories\ncorresponding to the relative expertise of the description authors: those\nauthored by experts (e.g., publishers of accessible media) and those\nauthored by non-experts (e.g., via crowdsourcing or online platforms).\n2.2.1 Descriptions Authored by Experts\nPublishers have developed guidelines for describing graphics appearing\nin science, technology, engineering, and math (STEM) materials [9, 39].\nDeveloped by and for authors with some expert accessibility knowl-\nedge, these guidelines provide best practices for conveying visualized\ncontent in traditional media (e.g., printed textbooks, audio books, and\ntactile graphics). But, many of their prescriptions — particularly those\nrelating to the content conveyed by a chart, rather than the modality\nthrough which the chart is rendered — are also applicable to web-based\nvisualizations. Additionally, web accessibility guidelines from W3C\nprovide best-practices for writing descriptions of “complex images”\n(including canonical chart types), either in a short description alt text\nattribute, or as a long textual description displayed alongside the visual\nimage [99]. While some of these guidelines have been adopted by visu-\nalization practitioners [19,29,32,34,88,101,102], we here bring special\nattention to the empirically-grounded and well-documented guidelines\ncreated by the WGBH National Center for Accessible Media [39] and\nby the Benetech Diagram Center [9].\n2.2.2 Descriptions Authored by Non-Experts\nFrequently employed in HCI and visualization research, crowdsourcing\nis a technique whereby remote non-experts complete tasks currently\ninfeasible for automatic methods, with applications to online acces-\nsibility [13], as well as remote description services like Be My Eyes.\nFor example, Morash et al. explored the efﬁcacy of two types of non-\nexpert tasks for authoring descriptions of visualizations: non-experts\nauthoring free-form descriptions without expert guidance, versus those\nﬁlling-in sentence templates pre-authored by experts [72]. While these\napproaches can yield more richly detailed and “natural”-sounding de-\nscriptions (as we discuss in § 5), and also provide training data for\nauto-generated captions and annotations [56, 83], it is important to be\nattentive to potential biases in human-authored descriptions [10].\n2.3 Natural Language Hierarchies and Interfaces\nApart from the above methods for generating descriptions, prior work\nhas adopted linguistics-inspired framings to elucidate how natural lan-\nguage is used to describe — as well as interact with — visualizations.\n2.3.1 Using Natural Language to Describe Visualizations\nDemir et al. have proposed a hierarchy of six syntactic complexity\nlevels corresponding to a set of propositions that might be conveyed by\nbar charts [27]. Our model differs in that it orders semantic content —\ni.e., what meaning the natural language sentence conveys — rather than\nhow it does so syntactically. Thus, our model is agnostic to a sen-\ntence’s length, whether it contains multiple clauses or conjunctions,\nwhich has also been a focus of prior work in automatic captioning [83].\nMoreover, whereas Demir et al. speculatively “envision” their set of\npropositions to construct their hierarchy, we arrive to our model em-\npirically through a multi-stage grounded theory process (§ 3). Perhaps\n2\n© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics.\nThe ﬁnal version of this record is available at: 10.1109/TVCG.2021.3114770.\nAn accessible HTML version of the article is available at: http://vis.csail.mit.edu/pubs/vis-text-model/.\nclosest to our contribution are a pair of papers by Kosslyn [57] and\nLivingston & Brock [66]. Kosslyn draws on canonical linguistic theory,\nto introduce three levels for analyzing charts: the syntactic relation-\nship between a visualization elements; the semantic meaning of these\nelements in what they depict or convey; and the pragmatic aspects of\nwhat these elements convey in the broader context of their reading [57].\nWe seeded our model construction with a similar linguistics-inspired\nframing, but also evaluated it empirically, to further decompose the\nsemantic levels (§ 3.1). Livingston & Brock adapt Kosslyn’s ideas to\ngenerate what they call “visual sentences”: natural language sentences\nthat are the result of executing a single, speciﬁc analytic task against\na visualization [66]. Inspired by the Sentence Veriﬁcation Technique\n(SVT ) [85, 86], this work considers visual sentences for assessing graph\ncomprehension, hoping to offer a more “objective” and automated alter-\nnative to existing visualization literacy assessments [35, 63]. While we\nadopt a more qualitative process for constructing our model, Livingston\n& Brock’s approach suggests opportunities for future work: might our\nmodel map to similarly-hierarchical models of analytic tasks [5, 17]?\n2.3.2 Using Natural Language to Interact with Visualizations\nAdjacently, there is a breadth of work on Natural Language Interfaces\n(NLIs) for constructing and exploring visualizations [43, 50, 75, 90].\nWhile our model primarily considers the natural language sentences that\nare conveyed by visualizations (cf., natural language as input for chart\nspeciﬁcation and exploration) [93], our work may yet have implications\nfor NLIs. For example, Hearst et al. have found that many users of\nchatbots prefer not to see charts and graphics alongside text in the\nconversational dialogue interface [42]. By helping to decouple visual-\nversus-linguistic data representations, our model might be applied\nto offer these users a textual alternative to inline charts. Thus, we\nview our work as complementary to NLIs, facilitating multimodal and\nmore accessible data representations [51], while helping to clarify the\ntheoretical relationship between charts and captions [52, 80], and other\naccompanying text [2, 54, 55, 106].\n3 C ONSTRUCTING THE MODEL : E MPLOYING THE GROUNDED\nTHEORY METHODOLOGY\nTo construct our model of semantic content we conducted a multi-stage\nprocess, following the grounded theory methodology. Often employed\nin HCI and the social sciences, grounded theory offers a rigorous method\nfor making sense of a domain that lacks a dominant theory, and for\nconstructing a new theory that accounts for diverse phenomena within\nthat domain [74]. The methodology approaches theory construction in-\nductively — through multiple stages of inquiry, data collection, “coding”\n(i.e., labeling and categorizing), and reﬁnement — as well as empiri-\ncally, remaining strongly based (i.e., “grounded”) in the data [74]. To\nconstruct our model of semantic content, we proceeded in two stages.\nFirst, we conducted small-scale data collection and initial open coding\nto establish preliminary categories of semantic content. Second, we\ngathered a larger-scale corpus to iteratively reﬁne those categories, and\nto verify their coverage over the space of natural language descriptions.\n3.1 Initial Open Coding\nWe began gathering preliminary data by searching for descriptions\naccompanying visualizations in journalistic publications (including the\nwebsites of FiveThirtyEight, the New York Times and the Financial\nTimes), but found that these professional sites usually provided no\ntextual descriptions — neither as a caption alongside the chart, nor as\nalt text for screen readers. Indeed, often these sites were engineered\nso that screen readers would pass over the visualizations entirely, as\nif they did not appear on the page at all. Thus, to proceed with the\ngrounded theory method, we conducted initial open coding (i.e., mak-\ning initial, qualitative observations about our data, in an “open-minded”\nfashion) by studying preliminary data from two sources. We collected\n330 natural language descriptions from over 100 students enrolled in\na graduate-level data visualization class. As a survey-design pilot to\ninform future rounds of data collection (§ 3.2.1), these initial descrip-\ntions were collected with minimal prompting: students were instructed\nto simply “describe the visualization” without specifying what kinds of\nTable 1. Breakdown of the 50 curated visualizations, across the three\ndimensions: type, topic, and difﬁculty. (N.b., each column sums to 50.)\nCHART TYPE TOPIC DIFFICULTY\nbar 18 academic 15 easy 21\nline 21 business 18 medium 20\nscatter 11 journalism 17 hard 9\nsemantic content that might include. The described visualizations cov-\nered a variety of chart types (e.g., bar charts, line charts, scatter plots) as\nwell as dataset domains (e.g., public health, climate change, and gender\nequality). To complement the student-authored descriptions, from this\nsame set of visualizations, we curated a set of 20 and wrote our (the\nauthors’) own descriptions, attempting to be as richly descriptive as\npossible. Throughout, we adhered to a linguistics-inspired framing by\nattending to the semantic and pragmatic aspects of our writing: which\ncontent could be conveyed through the graphical sign-system alone, and\nwhich required drawing upon our individual background knowledge,\nexperiences, and contexts.\nAnalyzing these preliminary data, we proceeded to the next stage in\nthe grounded theory method: forming axial codes (i.e., open codes or-\nganized into broader abstractions, with more generalized meaning [74])\ncorresponding to different content. We began to distinguish between\ncontent about a visualization’s construction (e.g., its title, encodings,\nlegends), content about trends appearing in the visualized data (e.g.,\ncorrelations, clusters, extrema), and content relevant to the visualized\ndata but not represented in the visualization itself (e.g., explanations\nbased on current events and domain-speciﬁc knowledge). From these\naxial codes, different categories (i.e., groupings delineated by shared\ncharacteristics of the content) began to emerge [74], corresponding to a\nchart’s encoded elements, latent statistical relations, perceptual trends,\nand context. We reﬁned these content categories iteratively by ﬁrst\nwriting down descriptions of new visualizations (again, as richly as\npossible), and then attempting to categorize each sentence appearing in\nthat description. If we encountered a sentence that didn’t ﬁt within any\ncategory, we either reﬁned the speciﬁc characteristics belonging to an\nexisting category, or we created a new category, where appropriate.\n3.2 Gathering A Corpus\nThe prior inductive and empirical process resulted in a set of preliminary\ncontent categories. To test their robustness, and to further reﬁne them,\nwe conducted an online survey to gather a larger-scale corpus of 582\nvisualization descriptions comprised of 2,147 sentences.\n3.2.1 Survey Design\nWe ﬁrst curated a set of 50 visualizations drawn from the MassVis\ndataset [15, 16], Quartz’s Atlas visualization platform [81], examples\nfrom the Vega-Lite gallery [87], and the aforementioned journalistic\npublications. We organized these visualizations along three dimensions:\nthe visualization type (bar charts, line charts, and scatter plots); the\ntopic of the dataset domain (academic studies, business-related, or non-\nbusiness data journalism); and their difﬁculty based on an assessment\nof their visual and conceptual complexity. We labeled visualizations\nas “easy” if they were basic instances of their canonical type (e.g.,\nsingle-line or un-grouped bar charts), as ”medium” if they were more\nmoderate variations on canon (e.g., contained bar groupings, overlap-\nping scatterplot clusters, visual embellishments, or simple transforms),\nand as ”hard” if they strongly diverged from canon (e.g., contained\nchartjunk or complex transforms such as log scales). To ensure robust-\nness, two authors labeled the visualizations independently, and then\nresolved any disagreement through discussion. Table 1 summarizes the\nbreakdown of the 50 visualizations across these three dimensions.\nIn the survey interface, participants were shown a single, randomly-\nselected visualization at a time, and prompted to describe it in complete\nEnglish sentences. In our preliminary data collection (§ 3.1), we found\nthat without explicit prompting participants were likely to provide only\nbrief and minimally informative descriptions (e.g., sometimes simply\nrepeating the chart title and axis labels). Thus, to mitigate against this\noutcome, and to elicit richer semantic content, we explicitly instructed\nparticipants to author descriptions that did not only refer to the chart’s\n3\nline\nLevel 4\nLevel 3\nLevel 2\nLevel 1\nbar\nscatter\neasy\nmedium\nhard\neasy\nmedium\nhard\neasy\nmedium\nhard\nFig. 2. A visual “ﬁngerprint” [49] of our corpus, faceted by chart type and\ndifﬁculty. Each row corresponds to a single chart. Each column shows a\nparticipant-authored description for that chart, color coded according to\nour model. The ﬁrst column shows the provided Level 1 prompt.\nbasic elements and encodings (e.g., it’s title, axes, colors) but to also\nreferred to other content, trends, and insights that might be conveyed\nby the visualization. To make these instructions intelligible, we pro-\nvided participants with a few pre-generated sentences enumerating the\nvisualization’s basic elements and encodings (e.g., thecolor coded sen-\ntences in Table 3 A.1, B.1, C.1), and prompted them to author semantic\ncontent apart from what was already conveyed by those sentences. To\navoid biasing their responses, participants werenot told that they would\nbe read by people with visual disabilities. This prompting ensured that\nthe survey captured a breadth of semantic content, and not only the\nmost readily-apparent aspects of the visualization’s construction.\n3.2.2 Survey Results\nWe recruited 120 survey participants through the Proliﬁc platform. In\nan approximately 30-minute study compensated at a rate of $10-12 per\nhour, we asked each participant to describe 5 visualizations (randomly\nselected from the set of 50), resulting in at least 10 participant-authored\ndescriptions per visualization. For some visualizations, we collected\nbetween 10-15 responses, due to limitations of the survey logic for\nrandomly selecting a visualization to show participants. In total, this\nsurvey resulted in 582 individual descriptions comprised of 2,147 natu-\nral language sentences. We manually cleaned each sentence to correct\nerrors in spelling, grammar, punctuation (n.b., we did not alter the\nsemantic content conveyed by each sentence). We then labeled each\nsentence according to the content categories developed through our\nprior grounded theory process. As before, to ensure robustness, two\nauthors labeled each sentence independently, and then resolved any dis-\nagreement through discussion. This deliberative and iterative process\nhelped us to further distinguish and reﬁne our categories. For example,\nwe were able to more precisely draw comparisons between sentences\nreporting computable “data facts” [92, 100] through rigid or templa-\ntized articulation (such as “[x-encoding] is positively correlated with\n[y-encoding]”), with sentences conveying the same semantic content\nthrough more “natural”-sounding articulation (such as “for the most\npart, as [x-encoding] increases, so too does [y-encoding]”).\nIn summary, the entire grounded theory process resulted in four\ndistinct semantic content categories, which we organize into levels\nin the next section. A visual “ﬁngerprint” [49] shows how semantic\ncontent is distributed across sentences in the corpus (Fig. 2). Level 1\n(consisting of a chart’s basic elements and encodings) represents 9.1%\nof the sentences in the corpus. This is expected, since Level 1 sentences\nwere pre-generated and provided as a prompt to our survey participants,\nas we previously discussed. The distribution of sentences across the\nremaining levels is as follows: Level 2 (35.1%), Level 3 (42.9%), and\nLevel 4 (12.9%). The fairly-balanced distribution suggests that our\nsurvey prompting successfully captured natural language sentences\ncorresponding to a breadth of visualized content.\n4 A F OUR -LEVEL MODEL OF SEMANTIC CONTENT\nOur grounded theory process yielded a four-level model of semantic\ncontent for the natural language description of visualizations. In the\nfollowing subsections, we introduce the levels of the model and provide\nexample sentences for each. Table 2 summarizes the levels, and Ta-\nble 3 shows example visualizations from our corpus and corresponding\ndescriptions, color coded according to the model’s color scale. Ad-\nditionally, we offer practical computational considerations regarding\nthe feasibility of generating sentences at each level, with reference to\nthe present-day state-of-the-art methods described in Related Work.\nWhile we present them alongside each other for ease of explication,\nwe emphasize that the model levels and computational considerations\nare theoretically decoupled: the model is indexed to the semantic con-\ntent conveyed by natural language sentences, not to the computational\nmeans through which those sentences may or may not be generated.\n4.1 Level 1: Elemental and Encoded Properties\nAt the ﬁrst level, there are sentences whose semantic content refers to\nelemental and encoded properties of the visualization (i.e., the visual\ncomponents that comprise a graphical representation’s design and con-\nstruction). These include the chart type (bar chart, line graph, scatter\nplot, etc.), its title and legend, its encoding channels, axis labels, and\nthe axis scales. Consider the following sentence (Table 3.A.1).\nMortality rate is plotted on the vertical y-axis from 0 to 15%. Age is plotted on\nthe horizontal x-axis in bins: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79,\n80+.\nThis sentence “reads off” the axis labels and scales as they appear in the\nbar chart, with no additional synthesizing or interpretation. Sentences\nsuch as this are placed at the lowest level in the model because they\nrefer to content that is foundational to visualization construction—\ncomprising the elemental properties of the “language” of graphics [12].\nComputational Considerations.Semantic content at Level 1 is so\nfoundational that it has long been formalized — not only theoretically,\nas in Bertin’s Semiology of Graphics , but also mathematically and\nprogrammatically, as a “grammar of graphics” that precisely deﬁnes the\nalgorithmic rules for constructing canonical chart types. [104]. In the\ncase of these construction grammars, Level 1 content isdirectly encoded\nin the visualization’s structured speciﬁcation (i.e., mappings between\ndata ﬁelds and visual properties) [87]. Thus, for these grammars,\ngenerating sentences at Level 1 can amount to “ﬁlling in the blank” for\na pre-deﬁned sentence template. For example, given an appropriate\ntemplate, the following natural language sentence could be trivially\ncomputed using the data encoded in the visualization speciﬁcation.\n“This is a [chart-type] entitled [chart-title].\n[y-encoding] is plotted on the vertical y-axis from [y-min] to\n[y-max]. [x-encoding] is plotted on the horizontal x-axis from\n[x-min] to [x-max].”\nAnd similarly, for other sentence templates and elemental properties\nencoded in the visualization’s structured speciﬁcation. If the structured\nspeciﬁcation is not available, however, or if it does not follow a declar-\native grammar, then CV and NLP methods have also shown promise\nwhen applied to rasterized visualization images (e.g., JPEG s or PNG s).\nFor example, recent work has shown that Level 1 semantic content\ncan be feasibly generated provided an appropriate training dataset of\npre-deﬁned sentence templates [47], or by extracting a visualization’s\nstructured speciﬁcation from a rasterized visualization image [81].\n4\n© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics.\nThe ﬁnal version of this record is available at: 10.1109/TVCG.2021.3114770.\nAn accessible HTML version of the article is available at: http://vis.csail.mit.edu/pubs/vis-text-model/.\nTable 2. A four-level model of semantic content for accessible visualization. Levels are deﬁned by the semantic content conveyed by natural language\ndescriptions of visualizations. Additionally, we offer computational considerations for generating the semantic content at each level of the model.\n# LEVEL KEYWORDS SEMANTIC CONTENT COMPUTATIONAL CONSIDERATIONS\n4 contextual and\ndomain-specific\ndomain-speciﬁc insights, current events,\nsocial and political context, explanations\ncontextual knowledge and domain-speciﬁc\nexpertise (perceiver-dependent)\n3 perceptual and\ncognitive\ncomplex trends, pattern synthesis,\nexceptions, commonplace concepts\nreference to the rendered visualization and\n“common knowledge” (perceiver-dependent)\n2 statistical and\nrelational\ndescriptive statistics, extrema, outliers,\ncorrelations, point-wise comparisons\naccess to the visualization speciﬁcation or\nbacking dataset (perceiver-independent)\n1 elemental and\nencoded\nchart type, encoding channels, title, axis\nranges, labels, colors\naccess to the visualization speciﬁcation or\nrasterized image (perceiver-independent)\n4.2 Level 2: Statistical Concepts and Relations\nAt the second level, there are sentences whose semantic content refers\nto abstract statistical concepts and relations that are latent the visual-\nization’s backing dataset. This content conveys computable descriptive\nstatistics (such as mean, standard deviation, extrema, correlations) —\nwhat have sometimes been referred to as “data facts” because they are\n“objectively” present within a given dataset [92,100] (as opposed to pri-\nmarily observed via visualization, which affords more opportunities for\nsubjective interpretation). In addition to these statistics, Level 2 content\nincludes relations between data points (such as “greater than” or “lesser\nthan” comparisons). Consider the following sentences (Table 3.C.2).\nFor low income countries, the average life expectancy is 60 years for men and\n65 years for women. For high income countries, the average life expectancy is\n77 years for men and 82 years for women.\nThese two sentences refer to a statistical property: the computed mean\nof the life expectancy of a population, faceted by gender and country\nincome-level. Consider another example (Table 3.A.2).\nThe highest COVID-19 mortality rate is in the 80+ age range, while the lowest\nmortality rate is in 10-19, 20-29, 30-39, sharing the same rate.\nAlthough this sentence is more complex, it nevertheless resides at\nLevel 2. It refers to the extrema of the dataset (i.e., the “highest”\nand “lowest” mortality rates), and makes two comparisons (i.e., a\ncomparison between the extrema, and another between age ranges\nsharing the lowest mortality rate). All of the above sentences above\nshare the same characteristic, distinguishing them from those at Level 1:\nthey refer to relations between points in the dataset, be they descriptive\nstatistics or point-wise comparisons. Whereas Level 1 sentences “read\noff” the visualization’s elemental properties, Level 2 sentences “report”\nstatistical concepts and relations within the chart’s backing dataset.\nComputational Considerations. While semantic content at Level\n1 requires only reference to the visualization’s speciﬁcation, content\nat Level 2 also requires access to the backing dataset. Here, the two\ncategories of automatic methods begin to diverge in their computa-\ntional feasibility. For visualizations with a structured speciﬁcation,\ngenerating sentences at Level 2 is effectively as easy as generating\nsentences at Level 1: it requires little more computation to calculate\nand report descriptive statistics when the software has access to the\nbacking dataset (i.e., encoded as part of the visualization speciﬁcation).\nIndeed, many visualization software systems (such as Tableau’s Sum-\nmary Card, V oder [92], QuillNLG Plug-In for Power BI, and others)\nautomatically compute summary statistics and present them in natural\nlanguage captions. By contrast, for CV and NLP methods, generating\nLevel 2 sentences from a rasterized image is considerably more difﬁ-\ncult — although not entirely infeasible — depending on the chart type\nand complexity. For example, these methods can sometimes report\nextrema (e.g., which age ranges exhibit the highest and lowest mortality\nrates in 3.A.2) [26, 78]. Nevertheless, precisely reporting descriptive\nstatistics (e.g., the computed mean of points in a scatter plot) is less\ntractable, without direct access to the chart’s backing dataset.\n4.3 Level 3: Perceptual and Cognitive Phenomena\nAt the third level, there are sentences whose semantic content refers\nto perceptual and cognitive phenomena appearing in the visual repre-\nsentation of the data. When compared to, and defended against, other\nforms of data analysis (e.g., purely mathematical or statistical methods),\nvisualization is often argued to confer some unique beneﬁt to human\nreaders. That is, visualizations do not only “report” descriptive statis-\ntics of the data (as in Level 2), they also show their readers something\nmore: they surface unforeseen trends, convey complex multi-faceted\npatterns, and identify noteworthy exceptions that aren’t readily appar-\nent through non-visual methods of analysis (cf., Anscombe’s Quartet\nor the Datasaurus Dozen [70]). Level 3 sentences are comprised of\ncontent that refers to these perceptual and cognitive phenomena, usually\narticulated in “natural”-sounding (rather than templatized) language.\nConsider the following examples (Table 3.B.3 and 3.C.3, respectively).\nPrices of particular Big Techcorporations seem to ﬂuctuate but nevertheless\nincrease over time. Y ears2008-2009 are exceptions as we can see an extreme\ndrop in prices of all given corporations.\nThe low income countries are more scattered than the high income countries.\nThere is a visible gap between high and low income countries, indicated by\nthe Income-Age Divide line.\nThese sentences convey the “overall gist” of complex trends and pat-\nterns (e.g., stock prices “seem to ﬂuctuate but nevertheless increase”),\nsynthesize multiple trends to identify exceptions (e.g., “years 2008-\n2009 are exceptions as we can see an extreme drop” of multiple graphed\nlines at that point in time), and do so in “natural”-sounding language,\nby referencing commonplace concepts (such as “ﬂuctuate”, “extreme\ndrop”, “visible gap”). N.b., “natural”-sounding articulation is neces-\nsary but insufﬁcient for Level 3 membership, as it is also possible to\narticulate Level 1 or 2 content in a non-templatized fashion (§ 3.2.2).\nComputational Considerations.At Level 3, we begin to reach and\nexceed the limits of present-day state-of-the-art automatic methods.\nWhile there exist “off-the-shelf” statistical packages for computing\nbasic trends and predictions in a dataset (e.g., correlations, polynomial\nregressions, statistical inferences), visualizations allow us to perceive\nand articulate complex trends for which there may exist no line of “best\nﬁt”. While automatic methods may eventually approach (or exceed)\nhuman capabilities on well-deﬁned tasks [78], for now Level 3 semantic\ncontent is likely generated via human (rather than machine) perception\nand cognition [72]. Taking inspiration from the “mind-independent”\nversus “mind-dependent” ontological distinction [4], we deﬁne sen-\ntences at Levels 1 and 2 asperceiver-independent (i.e., their content can\nbe generated independently of human or machine perception, without\nreference to the visualization), while sentences at Level 3 are perceiver-\ndependent (i.e., their content requires a perceiver of some sort; likely\na human, although machine perception may increasingly sufﬁce for\ngenerating Level 3 content). Table 2 summarizes this distinction.\n4.4 Level 4: Contextual and Domain-Speciﬁc Insights\nFinally, at the fourth level, there are sentences whose semantic content\nrefers to contextual and domain-speciﬁc knowledge and experience.\nConsider the following two examples (Table 3.B.4 and 3.C.4).\nThe big drop in prices was caused by ﬁnancial crisis of 2007-2008. The crisis\nculminated with the bankruptcy of Lehman Brothers on September 15, 2008\nand an international banking crisis.\nPeople living in low-income countries tend to have a lower life expectancy than\nthe people living in high-income countries, likely due to many societal factors,\nincluding access to healthcare, food, other resources, and overall quality of\nlife.\n5\nTable 3. Example visualizations and descriptions from our corpus. Paragraph breaks in rows A and B indicate a description authored by a unique\nparticipant from our corpus gathering survey (§ 3.2.1), while row C shows an curated exemplar description from our evaluation (§ 5.1).\nVISUALIZATION DESCRIPTION\nA\n[bar, easy, journalism]\n[1] This is a vertical bar chart entitled “COVID-19 mortality rate by age” that\nplots Mortality rate by Age. Mortality rate is plotted on the vertical y-axis from 0\nto 15%. Age is plotted on the horizontal x-axis in bins: 10-19, 20-29, 30-39, 40-\n49, 50-59, 60-69, 70-79, 80+. [2] The highest COVID-19 mortality rate is in the\n80+ age range, while the lowest mortality rate is in 10-19, 20-29, 30-39, shar-\ning the same rate. [3] COVID-19 mortality rate does not linearly correspond to\nthe demographic age.\n[4] The mortality rate increases with age, especially around 40-49 years and\nupwards. [5] This relates to people’sdecrease in their immunity and the in-\ncrease of co-morbidity with age.\n[6] The mortality rate increases exponentially with older people. [7] There is\nno difference in the mortality rate in the range between the age of 10 and 39.\n[8] The range of ages between 60 and 80+ are more affected by COVID-19.\n[9] We can observe that the mortality rate is higher starting at 50 years old due\nto many complications prior. [10] As we decrease the age, we also decrease\nthe values in mortality by a lot, almost to none.\nB\n[line, medium, business]\n[1] This is a multi-line chart entitled “Big Tech Stock Prices” that plots price\nby date. The corporations include AAPL (Apple), AMZN (Amazon), GOOG\n(Google), IBM (IBM), and MSFT (Microsoft). The years are plotted on the hor-\nizontal x-axis from 2000 to 2010 with an increment of 2 years. The prices\nare plotted on the vertical y-axis from 0 to 800 with an increment of 200.\n[2] GOOG has the greatest price over time. MSFT has the lowest price over\ntime. [3] Prices of particular Big Tech corporations seem to ﬂuctuate but nev-\nertheless increase over time. Y ears2008-2009 are exceptions as we can see\nan extreme drop in prices of all given corporations. [4] The big drop in prices\nwas caused by ﬁnancial crisis of 2007-2008. The crisis culminated with the\nbankruptcy of Lehman Brothers on September 15, 2008 and an international\nbanking crisis.\n[5] At the beginning of 2008, every of this stock price went down, likely due to\nthe ﬁnancial crisis. [6] Then they have risen again and dropped again, more\nso than previously.\n[7] GOOG has the highest price over the years. MSFT has the lowest price\nover the years. [8] GOOG quickly became the richest one of the Big Tech cor-\nporations. [9] GOOG had experienced some kind of a crisis in 2009, because\ntheir prices drop rapidly, but then rebounded.\nC\n[scatter, hard, academic]\n[1] This is a scatter plot entitled “Bornin 2016: Life Expectancy Gap by Gender\nand Income” that plots Women Life Expectancy at Birth (Y ears)by Men Life\nExpectancy at Birth (Y ears).The Women Life Expectancy at Birth is plotted\non the vertical y-axis from 40 to 90 years. The Men Life Expectancy at Birth\nis plotted on the horizontal x-axis from 40 to 90 years. High Income Countries\nare plotted in dark green. Low Income Countries are plotted in light green. A\n45 degree line from the origin represents Equal Life Expectancy. [2]For low in-\ncome countries, the average life expectancy is 60 years for men and 65 years\nfor women. For high income countries, the average life expectancy is 77 years\nfor men and 82 years for women. [3] Overall, women have a slightly higher\nlife expectancy than men. Women live around 5 to 10 years longer than men.\nThe low income countries are more scattered than the high income countries.\nThere is a visible gap between high and low income countries, indicated by\nthe Income-Age Divide line. [4] People living in low-income countries tend to\nhave a lower life expectancy than the people living in high-income countries,\nlikely due to many societal factors, including access to healthcare, food, other\nresources, and overall quality of life. People who live in lower income countries\nare more likely to experience deprivation and poverty,which can cause related\nhealth problems.\n6\n© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics.\nThe ﬁnal version of this record is available at: 10.1109/TVCG.2021.3114770.\nAn accessible HTML version of the article is available at: http://vis.csail.mit.edu/pubs/vis-text-model/.\nThese sentences convey social and political explanations for an ob-\nserved trend that depends on an individual reader’s subjective knowl-\nedge about particular world events: the 2008 ﬁnancial crisis and\nglobal socio-economic trends, respectively. This semantic content\nis characteristic of what is often referred to as “insight” in visual-\nization research. Although lacking a precise and agreed-upon deﬁni-\ntion [20, 60, 61, 76, 95], an insight is often an observation about the\ndata that is complex, deep, qualitative, unexpected, and relevant [108].\nCritically, insights depend on individual perceivers, their subjective\nknowledge, and domain-expertise. Level 4 is where the breadth of\nan individual reader’s knowledge and experience is brought to bear in\narticulating something “insightful” about the visualized data.\nComputational Considerations.As with Levels 3, we say that Level\n4 semantic content is perceiver-dependent, but in a stronger sense. This\nis because (setting aside consideration of hypothetical future “artiﬁcial\ngeneral intelligence”) generating Level 4 semantic content is at-present\na uniquely human endeavor. Doing so involves synthesizing back-\nground knowledge about the world (such as geographic, cultural, and\npolitical relationships between countries), contextual knowledge about\ncurrent events (e.g., the fact that there was a global recession in 2008),\nand domain-speciﬁc knowledge (e.g., expertise in a particular ﬁeld\nof research or scholarship). However, bespoke systems for narrowly-\nscoped domains (e.g., those auto-generating stock chart annotations\nusing a corpus of human-authored news articles [45]) suggest that some\nLevel 4 content might be feasibly generated sooner rather than later.\nLastly, we brieﬂy note that data-driven predictions can belong to\neither Level 2, 3, or 4, depending on the semantic content contained\ntherein. For example: a point-wise prediction at Level 2 (e.g., com-\nputing a stock’s future expected price using the backing dataset); a\nprediction about future overall trends at Level 3 (e.g., observing that a\nsteadily increasing stock price will likely continue to rise); a prediction\ninvolving contextual or domain-speciﬁc knowledge at Level 4 (e.g., the\noutcome of an election using a variety of poll data, social indicators,\nand political intuition).\n5 A PPLYING THE MODEL : E VALUATING THE EFFECTIVENESS\nOF VISUALIZATION DESCRIPTIONS\nThe foregoing conceptual model provides a means of making struc-\ntured comparisons between different levels of semantic content and\nreader groups. To demonstrate how it can be applied to evaluate the\neffectiveness of visualization descriptions (i.e., whether or not they ef-\nfectively convey meaningful information, and for whom), we conducted\na mixed-methods evaluation in which 30 blind and 90 sighted readers\nﬁrst ranked the usefulness of descriptions authored at varying levels of\nsemantic content, and then completed an open-ended questionnaire.\n5.1 Evaluation Design\nWe selected 15 visualizations for the evaluation, curated to be repre-\nsentative of the categories from our prior survey ( § 3). Speciﬁcally,\nwe selected 5 visualizations for each of the three dimensions: type\n(bar, line, scatter), topic (academic, business, journalism), and difﬁculty\n(easy, medium, hard). For every visualization, participants were asked\nto rank the usefulness of 4 different descriptions, each corresponding to\none level of semantic content, presented unlabeled and in random order.\nWe piloted this rank-choice interface with 10 sighted readers recruited\nvia Proliﬁc and 1 blind reader, a non-academic collaborator proﬁcient\nwith Apple’s V oiceOver screen reader. Based on this pilot, we rewrote\nthe study instructions to be more intelligible to both groups of readers,\nadded an introductory example task to the evaluation, and improved the\nscreen reader accessibility of our interface (e.g., by reordering nested\nDOM elements to be more intuitively traversed by screen reader).\nIn addition to curating a representative set of visualizations, we also\ncurated descriptions representative of each level of semantic content.\nParticipant-authored descriptions from our prior survey often did not\ncontain content from all 4 levels or, if they did, this content was in-\nterleaved in a way that was not cleanly-separable for the purpose of a\nranking task (Fig. 2). Thus, for this evaluation, we curated and collated\nsentences from multiple participant-authored descriptions to create ex-\nemplar descriptions, such that each text chunk contained only content\nbelonging to a single semantic content level. Table 3.C shows one\nsuch exemplar description, whereas Table 3.A and B show the original\nun-collated descriptions. For each ranking task, readers were presented\nwith a brief piece of contextualizing text, such as the following.\n“Suppose that you are reading an academic paper about how life\nexpectancy differs for people of different genders from countries\nwith different levels of income. You encounter the following visu-\nalization. [Table 3.C] Which content do you think would be most\nuseful to include in a textual description of this visualization?”\nAdditionally, blind readers were presented with a brief text noting\nthat the hypothetically-encountered visualization was inaccessible via\nscreen reader technology. In contrast to prior work, which has evalu-\nated chart descriptions in terms of “efﬁciency,” “informativeness,” and\n“clarity” [39, 78], we intentionally left the deﬁnition of “useful” open\nto the reader’s interpretation. We hypothesize that “useful” descrip-\ntions may not be necessarily efﬁcient (i.e., they may require lengthy\nexplanation or background context), and that both informativeness and\nclarity are constituents of usefulness. In short, ranking “usefulness”\naffords a holistic evaluation metric. Participants assigned usefulness\nrankings to each of the 4 descriptions by selecting corresponding radio\nbuttons, labeled 1 (least useful) to 4 (most useful). In addition to these\n4 descriptions, we included a 5th choice as an “attention check”: a\nsentence whose content was entirely irrelevant to the chart to ensure\nparticipants were reading each description prior to ranking them. If a\nparticipant did not rank the attention check as least useful, we ﬁltered\nout their response from our ﬁnal analysis. We include the evaluation\ninterfaces and questions with the Supplemental Material.\n5.2 Participants\nParticipants consisted of two reader groups: 90 sighted readers recruited\nthrough the Proliﬁc platform, and 30 blind readers recruited through\nour friends in the blind community and through a call for participation\nsent out via Twitter (n.b., in accessibility research, it is common to\ncompare blind and sighted readers recruited through these means [14]).\n5.2.1 Participant Recruitment\nFor sighted readers qualiﬁcations for participation included English\nlanguage proﬁciency and no color vision deﬁciency, and blind readers\nwere expected to be proﬁcient with a screen reader, such as Job Access\nWith Speech (JAWS), NonVisual Desktop Access (NVDA), or Apple’s\nV oiceOver. Sighted readers were compensated at a rate of $10-12\nper hour, for an approximately 20-minute task. Blind readers were\ncompensated at a rate of $50 per hour, for an approximately 1-hour\ntask. This difference in task duration was for two reasons. First,\nparticipants recruited through Proliﬁc are usually not accustomed to\ncompleting lengthy tasks — our prior surveys and pilots suggested that\nthese participants might contribute low-quality responses on “click-\nthrough” tasks if the task duration exceeded 15–20 minutes — and thus\nwe asked each participant to rank only 5 of the 15 visualizations at a\ntime. Second, given the difﬁculty of recruiting blind readers proﬁcient\nwith screen readers, we asked each blind participant to rank all 15\nvisualizations, and compensated them at a rate commensurate with\ntheir difﬁcult-to-ﬁnd expertise [67]. In this way, we recruited sufﬁcient\nnumbers of readers to ensure that each of the 15 visualization ranking\ntasks would be completed by 30 participants from both reader groups.\n5.2.2 Participant Demographics\nAmong the 30 blind participants, 53% (n=16) reported their gender as\nmale, 36% (n=11) as female, and 3 participants “preferred not to say.”\nThe most common highest level of education attained was a Bachelor’s\ndegree (60%, n=18), and most readers were between 20 – 40 years old\n(66%, n=20). The screen reader technology readers used to complete\nthe study was evenly balanced: V oiceOver (n=10),JAWS (n=10), NVDA\n(n=9), and “other” (n=1). Among the 90 sighted participants, 69%\nreported their gender as male (n=62) and 31% as female (n=28). The\nmost common highest level of education attained was a high school\ndiploma (42%, n=38) followed by a Bachelor’s degree (40%, n=36),\nand most sighted readers were between 20 – 30 years old (64%, n=58).\n7\nTable 4. (Upper) Rankings [1=least useful, 4=most useful] of semantic\ncontent at each level of the model, for blind and sighted readers. The\nscale encodes the number of times a given level was assigned a given\nrank by a reader. Dotted contour lines delineate Regions with a threshold\nequal to µ + σ\n2 , each labeled with a capital letter A – F . (Lower) Shaded\ncells indicate signiﬁcant ranking differences pair-wise between levels.\nBLIND READ ERS SIGHTED READ ERS\nLEVELS 1 ×2 1 ×3 1 ×4 2 ×3 2 ×4 3 ×4\nBLIND p < 0.001 p < 0.001 p < 0.321 p < 0.148 p < 0.001 p < 0.001\nSIGHTED p < 0.001 p < 0.001 p < 0.001 p < 0.001 p < 0.001 p < 0.059\nOn a 7-point Likert scale [1=strongly disagree, 7=strongly agree],\nblind participants reported having “a good understanding of data vi-\nsualization concepts” (µ = 6.3, σ = 1.03) as well as “a good under-\nstanding of statistical concepts and terminology” (µ = 5.90, σ = 1.01).\nSighted participants reported similar levels of understanding: (µ = 6.7,\nσ = 0.73) and (µ = 5.67, σ = 1.06), respectively. Sighted participants\nalso considered themselves to be “proﬁcient at reading data visualiza-\ntions” (µ = 5.97, σ = 0.89) and were able to “read and understand all\nof the visualizations presented in this study” (µ = 6.44, σ = 0.71).\n5.3 Quantitative Results\nQuantitative results for the individual rankings (1,800 per blind and\nsighted reader groups) are summarized by the heatmaps in Table 4\n(Upper), which aggregate the number of times a given content level was\nassigned a certain rank. Dotted lines in both blind and sighted heatmaps\ndelineate regions exceeding a threshold — calculated by taking the\nmean plus half a standard deviation (µ + σ\n2 ) resulting in a value of 139\nand 136, respectively — and are labeled with a capital letter A – F.\nThese results exhibit signiﬁcant differences between reader groups.\nFor both reader groups, using Friedman’s Test (a non-parametric multi-\ncomparison test for rank-order data) the p-value is p < 0.001, so we\nreject the null hypothesis that the mean rank is the same for all four\nsemantic content levels [37]. Additionally, in Table 4 (Lower), we ﬁnd\nsigniﬁcant ranking differences when making pair-wise comparisons\nbetween levels, via Nemenyi’s test (a post-hoc test commonly coupled\nwith Friedman’s to make pair-wise comparisons). There appears to be\nstrong agreement among sighted readers that higher levels of semantic\ncontent are more useful: Levels 3 and 4 are found to be most useful (Re-\ngion 4.F), while Levels 1 and 2 are least useful (Regions 4.D and 4.E).\nBlind readers agree with each other to a lesser extent, but strong trends\nare nevertheless apparent. In particular, blind readers rank content and\nLevels 2 and 3 as most useful (Region 4.C), and semantic content at\nLevels 1 and 4 as least useful (Regions 4.A and 4.B).\nWhen faceting these rankings by visualization type, topic, or difﬁ-\nculty we did not observe any signiﬁcant differences, suggesting that\nboth reader groups rank semantic content levels consistently, regardless\nof how the chart itself may vary. Noteworthy for both reader groups,\nthe distribution of rankings for Level 1 is bimodal —– the only level to\nexhibit this property. While a vast majority of both blind and sighted\nreaders rank Level 1 content as least useful, this level is ranked “most\nuseful” in 101 and 87 instances by blind and sighted readers, respec-\ntively. This suggests that both reader groups have a more complicated\nperspective toward descriptions of a chart’s elemental and encoded\nproperties; a ﬁnding we explore further by analyzing qualitative data.\n5.4 Qualitative Results\nIn a questionnaire, we asked readers to use a 7-point Likert scale\n[1=strongly disagree, 7=strongly agree] to rate their agreement with a\nset of statements about their experience with visualizations. We also\nasked them to offer open-ended feedback about which semantic content\nthey found to be most useful and why. Here, we summarize the key\ntrends that emerged from these two different forms of feedback, from\nboth blind readers (BR) and sighted readers (SR).\n5.4.1 Descriptions Are Important to Both Reader Groups\nAll blind readers reported encountering inaccessible visualizations: ei-\nther multiple times a week (43%, n=13), everyday (20%, n=6), once or\ntwice a month (20%, n=6), or at most once a week (17%, n=5). These\nreaders reported primarily encountering these barriers on social media\n(30%, n=9), on newspaper websites (13%, n=4), and in educational\nmaterials (53%, n=16) — but, most often, barriers were encountered in\nall of the above contexts (53%, n=16). Blind readers overwhelmingly\nagreed with the statements “I often feel that important public infor-\nmation is inaccessible to me, because it is only available in a visual\nformat” (µ = 6.1, σ = 1.49), and “Providing textual descriptions of\ndata visualizations is important to me” (µ = 6.83, σ = 0.38).\n“I am totally blind, and virtually all data visualizations I en-\ncounter are undescribed, and as such are unavailable. This has\nbeen acutely made clear on Twitter and in newspapers around the\nCOVID-19 pandemic and the recent U.S. election. Often, visual-\nizations are presented with very little introduction or coinciding\ntext. I feel very left out of the world and left out of the ability to\nconﬁdently traverse that world. The more data I am unable to\naccess, the more vulnerable and devalued I feel. ”(BR5)\nBy contrast, sighted readers neither agreed nor disagreed regarding\nthe inaccessibility of information conveyed visually (µ = 4, σ = 1.57).\nSimilarly, they were split on whether they ever experienced barriers to\nreading visualizations, with 52% (n=47) reporting that they sometimes\ndo (especially when engaging with a new topic) and 48% (n=43) report-\ning that they usually do not. Nevertheless, sighted readers expressed\nsupport for natural language descriptions of visualizations (µ = 5.60,\nσ = 1.27). A possible explanation for this support is that — regardless\nof whether the visualization is difﬁcult to read — descriptions can still\nfacilitate comprehension. For instance, SR64 noted that “textual de-\nscription requires far less brainpower and can break down a seemingly\ncomplex visualization into an easy to grasp overview. ”\n5.4.2 Reader Groups Disagree About Contextual Content\nA majority of blind readers (63%, n=19) were emphatic that descrip-\ntions should not contain an author’s subjective interpretations, con-\ntextual information, or editorializing about the visualized data (i.e.,\nLevel 4 content). Consistent with blind readers ranking this as among\nthe least useful (Region 4.B), BR20 succinctly articulated a common\nsentiment: “I want the information to be simply laid out, not peppered\nwith subjective commentary... I just prefer it to be straight facts, not pre-\nsumptions or guesstimates. ”BR4 also noted that an author’s“opinions”\nabout the data “should absolutely be avoided, ”and BR14 emphasized\nagency when interpreting data: “I want to have the time and space to\ninterpret the numbers for myself before I read the analysis. ”By contrast,\nmany sighted readers 41% (n=37) expressed the opposite sentiment\n(Region 4.F) noting that, for them, the most useful descriptions often\n“told a story, ”communicated an important conclusion, or provided\ndeeper insights into the visualized data. As SR64 noted: “A description\nthat simply describes the visualization and its details is hardly useful,\nbut a description that tells a story using the data and derives a solution\nfrom it is extremely useful. ”Only 4% (n=4) of sighted readers explicitly\nstated that a description should exclude Level 4 semantic content.\n5.4.3 Some Readers Prefer Non-Statistical Content\nOverall, blind readers consistently ranked both Levels 2 and 3 as the\nmost useful (Region 4.C). But, some readers explicitly expressed pref-\nerence for the latter over the former, highlighting two distinguishing\ncharacteristics of Level 3 content: that it conveys not only descrip-\ntive statistics but overall perceptible trends, and that it is articulated\n8\n© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics.\nThe ﬁnal version of this record is available at: 10.1109/TVCG.2021.3114770.\nAn accessible HTML version of the article is available at: http://vis.csail.mit.edu/pubs/vis-text-model/.\nin commonplace or “natural”-sounding language. For instance, BR26\nremarked that a visualization description is “more useful if it contains\nthe summary of the overall trends and distributions of the data rather\nthan just mentioning some of the extreme values or means. ”Similarly,\nBR21 noted that “not everyone who encounters a data visualization\nneeds it for statistical purposes, ”and further exclaimed“I want to know\nhow a layperson sees it, not a statistician; I identify more with simpler\nterminology. ”These preferences help to further delineate Level 3 from\nLevels 2 and 4. Content at Level 3 is “non-statistical” in the sense\nthat it does only report statistical concepts and relations (as in Level\n2), but neither does it do away with statistical “objectivity” entirely,\nso as to include subjective interpretation or speculation (as content in\nLevel 4 might). In short, Level 3 content conveys statistically-grounded\nconcepts in not-purely-statistical terms, a challenge that is core to\nvisualization, and science communication more broadly.\n5.4.4 Combinations of Content Levels Are Likely Most Useful\nWhile roughly 12% readers from both blind and sighted groups indi-\ncated that a description should be as concise as possible, among blind\nreaders, 40% (n=12) noted that the most useful descriptions would\ncombine content from multiple levels. This ﬁnding helps to explain the\nbimodality in Level 1 rankings we identiﬁed in the previous section.\nAccording to BR9, Level 1 content is only useful if other informa-\ntion is also conveyed: “All of the descriptions provided in this survey\nwhich *only* elaborated on x/y and color-coding are almost useless. ”\nThis sentiment was echoed by BR5, who added that if Level 1 content\nwere “combined with the [Level 2 or Level 3], that’dmake for a great\ndescription. ”This ﬁnding has implications for research on automatic\nvisualization captioning: these methods should aim to generate not only\nthe lower levels of semantic content, but to more richly communicate a\nchart’s overall trends and statistics, sensitive to reader preferences.\n5.4.5 Some Automatic Methods Raise Ethical Concerns\nResearch on automatically generating visualization captions is often\nmotivated by the goal of improving information access for people with\nvisual disabilities [27, 78, 83, 84]. However, when deployed in real-\nworld contexts, these methods may not confer their intended beneﬁts,\nas one blind reader in our evaluation commented.\n“A.I. attempting to convert these images is still in its infancy. Face-\nbook and Apple auto-descriptions of general images are more\nof a timewaster than useful. As a practical matter, if I ﬁnd an\ninaccessible chart or graph, I just move on. ”(BP22)\nSimilarly, another participant (BR26) noted that if a description were\nto only describe a visualization’s encodings then“the reader wouldn’t\nget any insight from these texts, which not only increases the readers’\nreading burden but also conveys no effective information about the data. ”\nThese sentiments reﬂect some of the ethical concerns surrounding\nthe deployment of nascent CV and NLP models, which can output\naccurate but minimally informative content — or worse, can output\nerroneous content to a trusting audience [69,78]. Facebook’s automatic\nimage descriptions, for example, have been characterized by technology\neducator Chancey Fleet as “famously useless in the Blind community”\nwhile “garner[ing] a ton of glowing reviews from mainstream outlets\nwithout being of much use to disabled people” [33, 40]. Such concerns\nmight be mitigated by developing and evaluating automatic methods\nwith disabled readers, through participatory design processes [67].\n6 D ISCUSSION AND FUTURE WORK\nOur four-level model of semantic content — and its application to evalu-\nating the usefulness of descriptions — has practical implications for the\ndesign of accessible data representations, and theoretical implications\nfor the relationship between visualization and natural language.\n6.1 Natural Language As An Interface Into Visualization\nDivergent reader preferences for semantic content suggests that it is\nhelpful to think of natural language — not only as an interface for\nconstructing and exploring visualizations [36, 89, 93] — but also as an\ninterface into visualization, for understanding the semantic content they\nconvey. Under this framing, we can apply Beaudoin-Lafon’s framework\nfor evaluating interface models in terms of their descriptive, evaluative,\nand generative powers [7, 8], to bring further clarity to the practical\ndesign implications of our model. First, our grounded theory process\nyielded a model with descriptive power: it categorizes the semantic\ncontent conveyed by visualizations. Second, our study with blind and\nsighted readers demonstrated our model’sevaluative power: it offered a\nmeans of comparing different levels of semantic content, thus revealing\ndivergent preferences between these different reader groups. Third,\nfuture work can now begin to study our model’sgenerative power: its\nimplications for novel multimodal interfaces and accessible data rep-\nresentations. For instance, our evaluation suggested that descriptions\nprimarily intending to beneﬁt sighted readers might aim to generate\nhigher-level semantic content (§ 5.4.2), while those intending to beneﬁt\nblind readers might instead focus on affording readers the option to\ncustomize and combine different content levels (§ 5.4.4), depending on\ntheir individual preferences (§ 5.4.3). This latter path might involve au-\ntomatically ARIA tagging web-based charts to surface semantic content\nat Levels 1 & 2, with human-authors conveying Level 3 content. Or, it\nmight involve applying our model to develop and evaluate the outputs\nof automatic captioning systems — to probe their technological capabil-\nities and ethical implications — in collaboration with the relevant com-\nmunities (§ 5.4.5). To facilitate this work, we have released our corpus\nof visualizations and labeled sentences under an open source license:\nhttp://vis.csail.mit.edu/pubs/vis-text-model/data/.\n6.2 Natural Language As Coequal With Visualization\nIn closing, we turn to a discussion of our model’s implications for\nvisualization theory. Not only can we think of natural language as an\ninterface into visualization (as above), but also as an interface into data\nitself; coequal with and complementary to visualization. For example,\nsome semantic content (e.g., Level 2 statistics or Level 4 explanations)\nmay be best conveyed via language, without any reference to visual\nmodalities [42, 82], while other content (e.g., Level 3 clusters) may be\nuniquely suited to visual representation. This coequal framing is not a\ndeparture from orthodox visualization theory, but rather a return to its\nlinguistic and semiotic origins. Indeed, at the start of his foundational\nSemiology of Graphics, Jacques Bertin introduces a similar framing\nto formalize an idea at the heart of visualization theory: content can\nbe conveyed not only through speaking or writing but also through the\n“language” of graphics [12]. While Bertin took natural language as\na point of departure for formalizing a language of graphics, we have\nhere pursued the inverse: taking visualization as occasioning a return\nto language. This theoretical inversion opens avenues for future work,\nfor which linguistic theory and semiotics are instructive [68, 97, 103].\nWithin the contemporary linguistic tradition, subﬁelds like syntax,\nsemantics, and pragmatics suggest opportunities for further analysis\nat each level of our model. And, since our model focuses on English\nsentences and canonical chart types, extensions to other languages\nand bespoke charts may be warranted. Within the semiotic tradition,\nChristian Metz (a contemporary of Bertin’s) emphasized thepluralistic\nquality of graphics [18]: the semantic content conveyed by visual-\nizations depends not only on their graphical sign-system, but also on\nvarious “social codes” such as education, class, expertise, and — we\nhasten to include — ability. Our evaluation with blind and sighted read-\ners (as well as work studying how charts are deployed in particular\ndiscourse contexts [3, 44, 46, 62]) lends credence to Metz’s conception\nof graphics as pluralistic: different readers will have different ideas\nabout what makes visualizations meaningful (Fig. 1). As a means of\nrevealing these differences, we have here introduced a four-level model\nof semantic content. We leave further elucidation of the relationship\nbetween visualization and natural language to future work.\nACKNOWLEDGMENTS\nFor their valuable feedback, we thank Emilie Gossiaux, Chancey Fleet,\nMichael Correll, Frank Elavsky, Beth Semel, Stephanie Tuerk, Crystal\nLee, and the MIT Visualization Group. This work was supported by\nNational Science Foundation GRFP-1122374 and III-1900991.\n9\nREFERENCES\n[1] P. Ackland, S. Resnikoff, and R. Bourne. World Blindness and Visual\nImpairment. Community Eye Health, 2017.\n[2] E. Adar and E. Lee. Communicative Visualizations as a Learning Prob-\nlem. In TVCG. IEEE, 2020.\n[3] G. Aiello. Inventorizing, Situating, Transforming: Social Semiotics And\nData Visualization. In M. Engebretsen and H. Kennedy, editors, Data\nVisualization in Society. Amsterdam University Press, 2020.\n[4] K. M. Ali. Mind-Dependent Kinds. In Journal of Social Ontology, 2016.\n[5] R. Amar, J. Eagan, and J. Stasko. Low-level Components Of Analytic\nActivity In Information Visualization. In INFOVIS. IEEE, 2005.\n[6] A. Balaji, T. Ramanathan, and V . Sonathi. Chart-Text: A Fully Automated\nChart Image Descriptor. arXiv, 2018.\n[7] M. Beaudouin-Lafon. Instrumental Interaction: An Interaction Model\nFor Designing Post-WIMP User Interfaces. In CHI. ACM, 2000.\n[8] M. Beaudouin-Lafon. Designing Interaction, Not Interfaces. In AVI.\nACM, 2004.\n[9] Benetech. Making Images Accessible. http://diagramcenter.org/making-\nimages-accessible.html/.\n[10] C. L. Bennett, C. Gleason, M. K. Scheuerman, J. P. Bigham, A. Guo,\nand A. To. “It’s Complicated”: Negotiating Accessibility and\n(Mis)Representation in Image Descriptions of Race, Gender, and Disabil-\nity. In CHI. ACM, 2021.\n[11] C. T. Bergstrom. SARS-CoV-2 Coronavirus, 2020.\nhttp://ctbergstrom.com/covid19.html.\n[12] J. Bertin. Semiology of Graphics. University of Wisconsin Press, 1983.\n[13] J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller,\nA. Tatarowicz, B. White, S. White, and T. Yeh. VizWiz: Nearly Real-time\nAnswers To Visual Questions. In UIST. ACM, 2010.\n[14] J. P. Bigham, I. Lin, and S. Savage. The Effects of ”Not Knowing What\nYou Don’t Know” on Web Accessibility for Blind Web Users. InASSETS.\nACM, 2017.\n[15] M. A. Borkin, Z. Bylinskii, N. W. Kim, C. M. Bainbridge, C. S. Yeh,\nD. Borkin, H. Pﬁster, and A. Oliva. Beyond Memorability: Visualization\nRecognition and Recall. In TVCG. IEEE, 2016.\n[16] M. A. Borkin, A. A. V o, Z. Bylinskii, P. Isola, S. Sunkavalli, A. Oliva,\nand H. Pﬁster. What Makes a Visualization Memorable? In TVCG. IEEE,\n2013.\n[17] M. Brehmer and T. Munzner. A Multi-Level Typology of Abstract\nVisualization Tasks. In TVCG. IEEE, 2013.\n[18] A. Campolo. Signs and Sight: Jacques Bertin and the Visual Language\nof Structuralism. Grey Room, 2020.\n[19] A. Cesal. Writing Alt Text for Data Visualization, Aug. 2020.\n[20] R. Chang, C. Ziemkiewicz, T. M. Green, and W. Ribarsky. Deﬁning\nInsight for Visual Analytics. In CG&A. IEEE, 2009.\n[21] A. Chaparro and M. Chaparro. Applications of Color in Design for\nColor-Deﬁcient Users. Ergonomics in Design, 2017.\n[22] C. Chen, R. Zhang, S. Kim, S. Cohen, T. Yu, R. Rossi, and R. Bunescu.\nNeural Caption Generation Over Figures. InUbiComp/ISWC ’19 Adjunct.\nACM, 2019.\n[23] C. Chen, R. Zhang, E. Koh, S. Kim, S. Cohen, and R. Rossi. Figure\nCaptioning with Relation Maps for Reasoning. In WACV. IEEE, 2020.\n[24] C. Chen, R. Zhang, E. Koh, S. Kim, S. Cohen, T. Yu, R. Rossi, and\nR. Bunescu. Figure Captioning with Reasoning and Sequence-Level\nTraining. arXiv, 2019.\n[25] J. Choi, S. Jung, D. G. Park, J. Choo, and N. Elmqvist. Visualizing for\nthe Non-Visual: Enabling the Visually Impaired to Use Visualization. In\nCGF. Eurographics, 2019.\n[26] S. Demir, S. Carberry, and K. F. McCoy. Generating textual summaries\nof bar chartsGenerating Textual Summaries Of Bar Charts. In INLG.\nACL, 2008.\n[27] S. Demir, S. Carberry, and K. F. McCoy. Summarizing Information\nGraphics Textually. In Computational Linguistics. ACL, 2012.\n[28] M. Ehrenkranz. Vital Coronavirus Information Is Failing the Blind and\nVisually Impaired. Vice, 2020.\n[29] F. Elavsky. Chartability, 2021. https://chartability.ﬁzz.studio/.\n[30] S. Elzer, S. Carberry, D. Chester, S. Demir, N. Green, I. Zukerman, and\nK. Trnka. Exploring And Exploiting The Limited Utility Of Captions In\nRecognizing Intention In Information Graphics. In ACL. Association for\nComputational Linguistics, 2005.\n[31] S. Elzer, E. Schwartz, S. Carberry, D. Chester, S. Demir, and P. Wu. A\nBrowser Extension For Providing Visually Impaired Users Access To\nThe Content Of Bar Charts On The Web. In WEBIST. SciTePress, 2007.\n[32] C. Fisher. Creating Accessible SVGs, 2019.\n[33] C. Fleet. Things which garner a ton of glowing reviews from main-\nstream outlets without being of much use to disabled people. For in-\nstance, Facebook’s auto image descriptions, much loved by sighted\njournos but famously useless in the Blind community. Twitter, 2021.\nhttps://twitter.com/ChanceyFleet/status/1349211417744961536.\n[34] S. L. Fossheim. An Introduction To Accessible Data Visualizations With\nD3.js, 2020.\n[35] M. Galesic and R. Garcia-Retamero. Graph Literacy: A Cross-cultural\nComparison. In Medical Decision Making. Society for Medical Decision\nMaking, 2011.\n[36] T. Gao, M. Dontcheva, E. Adar, Z. Liu, and K. G. Karahalios. Data-\nTone: Managing Ambiguity in Natural Language Interfaces for Data\nVisualization. In UIST. ACM, 2015.\n[37] S. Garc´ıa, A. Fern´andez, J. Luengo, and F. Herrera. Advanced Nonpara-\nmetric Tests For Multiple Comparisons In The Design Of Experiments\nIn Computational Intelligence And Data Mining: Experimental Analysis\nOf Power. Information Sciences, 2010.\n[38] B. Geveci, W. Schroeder, A. Brown, and G. Wilson. VTK. The Architec-\nture of Open Source Applications, 2012.\n[39] B. Gould, T. O’Connell, and G. Freed. Effective Practices for De-\nscription of Science Content within Digital Talking Books. Technical\nreport, The WGBH National Center for Accessible Media, 2008.\nhttps://www.wgbh.org/foundation/ncam/guidelines/effective-practices-\nfor-description-of-science-content-within-digital-talking-books.\n[40] M. Hanley, S. Barocas, K. Levy, S. Azenkot, and H. Nissenbaum. Com-\nputer Vision and Conﬂicting Values: Describing People with Automated\nAlt Text. arXiv, 2021.\n[41] L. Hasty, J. Milbury, I. Miller, A. O’Day, P. Acquinas, and D. Spence.\nGuidelines and Standards for Tactile Graphics. Technical report, Braille\nAuthority of North America, 2011. http://www.brailleauthority.org/tg/.\n[42] M. Hearst and M. Tory. Would You Like A Chart With That? Incor-\nporating Visualizations into Conversational Interfaces. In VIS. IEEE,\n2019.\n[43] M. Hearst, M. Tory, and V . Setlur. Toward Interface Defaults for Vague\nModiﬁers in Natural Language Interfaces for Visual Analysis. In VIS.\nIEEE, 2019.\n[44] J. Hullman and N. Diakopoulos. Visualization Rhetoric: Framing Effects\nin Narrative Visualization. In TVCG. IEEE, 2011.\n[45] J. Hullman, N. Diakopoulos, and E. Adar. Contextiﬁer: automatic gener-\nation of annotated stock visualizations. In CHI. ACM, 2013.\n[46] J. Hullman, N. Diakopoulos, E. Momeni, and E. Adar. Content, Context,\nand Critique: Commenting on a Data Visualization Blog. In CSCW.\nACM, 2015.\n[47] S. E. Kahou, V . Michalski, A. Atkinson, A. Kadar, A. Trischler, and\nY . Bengio. FigureQA: An Annotated Figure Dataset for Visual Reasoning.\narXiv, 2018.\n[48] A. Karpathy and L. Fei-Fei. Deep Visual-Semantic Alignments for\nGenerating Image Descriptions. In TPAMI. IEEE, Apr. 2017.\n[49] D. A. Keim and D. Oelke. Literature Fingerprinting: A New Method for\nVisual Literary Analysis. In VAST. IEEE, 2007.\n[50] D. H. Kim, E. Hoque, and M. Agrawala. Answering Questions about\nCharts and Generating Visual Explanations. In CHI. ACM, Apr. 2020.\n[51] D. H. Kim, E. Hoque, J. Kim, and M. Agrawala. Facilitating Document\nReading by Linking Text and Tables. In UIST. ACM, 2018.\n[52] D. H. Kim, V . Setlur, and M. Agrawala. Towards Understanding How\nReaders Integrate Charts and Captions: A Case Study with Line Charts.\nIn CHI. ACM, 2021.\n[53] N. W. Kim, S. C. Joyner, A. Riegelhuth, and Y . Kim. Accessible Vi-\nsualization: Design Space, Opportunities, and Challenges. In CGF.\nEurographics, 2021.\n[54] H.-K. Kong, Z. Liu, and K. Karahalios. Frames and Slants in Titles of\nVisualizations on Controversial Topics. In CHI. ACM, Apr. 2018.\n[55] H.-K. Kong, Z. Liu, and K. Karahalios. Trust and Recall of Information\nacross Varying Degrees of Title-Visualization Misalignment. In CHI.\nACM, May 2019.\n[56] N. Kong, M. A. Hearst, and M. Agrawala. Extracting References Between\nText And Charts Via Crowdsourcing. In CHI. ACM, 2014.\n[57] S. M. Kosslyn. Understanding Charts and Graphs. Applied Cognitive\nPsychology, 1989.\n[58] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY . Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei.\n10\n© 2021 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics.\nThe ﬁnal version of this record is available at: 10.1109/TVCG.2021.3114770.\nAn accessible HTML version of the article is available at: http://vis.csail.mit.edu/pubs/vis-text-model/.\nVisual Genome: Connecting Language and Vision Using Crowdsourced\nDense Image Annotations. In IJCV. Springer, 2017.\n[59] C. Lai, Z. Lin, R. Jiang, Y . Han, C. Liu, and X. Yuan. Automatic\nAnnotation Synchronizing with Textual Description for Visualization. In\nCHI. ACM, 2020.\n[60] P.-M. Law, A. Endert, and J. Stasko. Characterizing Automated Data\nInsights. arXiv, 2020.\n[61] P.-M. Law, A. Endert, and J. Stasko. What are Data Insights to Profes-\nsional Visualization Users? arXiv, Aug. 2020.\n[62] C. Lee, T. Yang, G. Inchoco, G. M. Jones, and A. Satyanarayan. Viral\nVisualizations: How Coronavirus Skeptics Use Orthodox Data Practices\nto Promote Unorthodox Science Online. In CHI. ACM, 2021.\n[63] S. Lee, S.-H. Kim, and B. C. Kwon. Vlat: Development Of A Visualiza-\ntion Literacy Assessment Test. In TVCG. IEEE, 2016.\n[64] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick. Microsoft COCO: Common Objects in\nContext. In ECCV. Springer, 2014.\n[65] T. Littleﬁeld. COVID-19 Statistics Tracker, 2020. https://cvstats.net.\n[66] M. A. Livingston and D. Brock. Position: Visual Sentences: Deﬁnitions\nand Applications. In VIS. IEEE, 2020.\n[67] A. Lundgard, C. Lee, and A. Satyanarayan. Sociotechnical Considera-\ntions for Accessible Visualization Design. In VIS. IEEE, Oct. 2019.\n[68] A. M. MacEachren, R. E. Roth, J. O’Brien, B. Li, D. Swingley, and\nM. Gahegan. Visual Semiotics Uncertainty Visualization: An Empirical\nStudy. In TVCG. IEEE, 2012.\n[69] H. MacLeod, C. L. Bennett, M. R. Morris, and E. Cutrell. Understanding\nBlind People’s Experiences with Computer-Generated Captions of Social\nMedia Images. In CHI. ACM, 2017.\n[70] J. Matejka and G. Fitzmaurice. Same Stats, Different Graphs: Gener-\nating Datasets with Varied Appearance and Identical Statistics through\nSimulated Annealing. In CHI. ACM, 2017.\n[71] P. Moraes, G. Sina, K. McCoy, and S. Carberry. Evaluating The Accessi-\nbility Of Line Graphs Through Textual Summaries For Visually Impaired\nUsers. In ASSETS. ACM, 2014.\n[72] V . S. Morash, Y .-T. Siu, J. A. Miele, L. Hasty, and S. Landau. Guiding\nNovice Web Workers in Making Image Descriptions Using Templates.\nIn TACCESS. ACM, 2015.\n[73] M. R. Morris, J. Johnson, C. L. Bennett, and E. Cutrell. Rich Repre-\nsentations of Visual Content for Screen Reader Users. In CHI. ACM,\n2018.\n[74] M. Muller. Curiosity, Creativity, and Surprise as Analytic Tools:\nGrounded Theory Method. In J. S. Olson and W. A. Kellogg, editors,\nWays of Knowing in HCI. Springer, 2014.\n[75] A. Narechania, A. Srinivasan, and J. Stasko. NL4DV: A Toolkit for\nGenerating Analytic Speciﬁcations for Data Visualization from Natural\nLanguage Queries. In TVCG. IEEE, 2021.\n[76] C. North. Toward Measuring Visualization Insight. In CG&A. IEEE,\n2006.\n[77] J. R. Nu˜nez, C. R. Anderton, and R. S. Renslow. Optimizing Colormaps\nWith Consideration For Color Vision Deﬁciency To Enable Accurate\nInterpretation Of Scientiﬁc Data. PLOS ONE, 2018.\n[78] J. Obeid and E. Hoque. Chart-to-Text: Generating Natural Language\nDescriptions for Charts by Adapting the Transformer Model. arXiv,\n2020.\n[79] M. M. Oliveira. Towards More Accessible Visualizations for Color-\nVision-Deﬁcient Individuals. In CiSE. IEEE, 2013.\n[80] A. Ottley, A. Kaszowska, R. J. Crouser, and E. M. Peck. The Curious\nCase of Combining Text and Visualization. In EuroVis. Eurographics,\n2019.\n[81] J. Poco and J. Heer. Reverse-Engineering Visualizations: Recovering\nVisual Encodings from Chart Images. In CGF. Eurographics, 2017.\n[82] V . Potluri, T. E. Grindeland, J. E. Froehlich, and J. Mankoff. Examining\nVisual Semantic Understanding in Blind and Low-Vision Technology\nUsers. In CHI. ACM, 2021.\n[83] X. Qian, E. Koh, F. Du, S. Kim, and J. Chan. A Formative Study on\nDesigning Accurate and Natural Figure Captioning Systems. In CHI EA.\nACM, 2020.\n[84] X. Qian, E. Koh, F. Du, S. Kim, J. Chan, R. A. Rossi, S. Malik, and T. Y .\nLee. Generating Accurate Caption Units for Figure Captioning. In WWW.\nACM, 2021.\n[85] J. M. Royer. Developing Reading And Listening Comprehension Tests\nBased On The Sentence Veriﬁcation Technique (SVT). In Journal of\nAdolescent & Adult Literacy. International Literacy Association, 2001.\n[86] J. M. Royer, C. N. Hastings, and C. Hook. A Sentence Veriﬁcation\nTechnique For Measuring Reading Comprehension. Journal of Reading\nBehavior, 1979.\n[87] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-Lite:\nA Grammar of Interactive Graphics. In TVCG. IEEE, 2017.\n[88] D. Schepers. Why Accessibility Is at the Heart of Data Visualization,\n2020.\n[89] V . Setlur, S. E. Battersby, M. Tory, R. Gossweiler, and A. X. Chang.\nEviza: A Natural Language Interface for Visual Analysis. In UIST. ACM,\n2016.\n[90] V . Setlur, M. Tory, and A. Djalali. Inferencing Underspeciﬁed Natural\nLanguage Utterances In Visual Analysis. In IUI. ACM, 2019.\n[91] A. Sharif, S. S. Chintalapati, J. O. Wobbrock, and K. Reinecke. Under-\nstanding Screen-Reader Users’ Experiences with Online Data Visualiza-\ntions. In ASSETS. ACM, 2021.\n[92] A. Srinivasan, S. M. Drucker, A. Endert, and J. Stasko. Augmenting\nVisualizations with Interactive Data Facts to Facilitate Interpretation and\nCommunication. In TVCG. IEEE, 2019.\n[93] A. Srinivasan, N. Nyapathy, B. Lee, S. M. Drucker, and J. Stasko. Col-\nlecting and Characterizing Natural Language Utterances for Specifying\nData Visualizations. In CHI. ACM, 2021.\n[94] H. Sutton. Accessible Covid-19 Tracker Enables A Way For Visually Im-\npaired To Stay Up To Date. Disability Compliance for Higher Education,\n2020.\n[95] B. Tang, S. Han, M. L. Yiu, R. Ding, and D. Zhang. Extracting Top-K\nInsights from Multi-dimensional Data. In SIGMOD. ACM, 2017.\n[96] B. D. Team. Bokeh: Python Library For Interactive Visualization. Bokeh\nDevelopment Team, 2014.\n[97] P. Vickers, J. Faith, and N. Rossiter. Understanding Visualization: A\nFormal Approach Using Category Theory and Semiotics. In TVCG.\nIEEE, 2013.\n[98] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and Tell: A Neural\nImage Caption Generator. In CVPR, 2015.\n[99] W3C. WAI Web Accessibility Tutorials: Complex Images, 2019.\nhttps://www.w3.org/W AI/tutorials/images/complex/.\n[100] Y . Wang, Z. Sun, H. Zhang, W. Cui, K. Xu, X. Ma, and D. Zhang.\nDataShot: Automatic Generation of Fact Sheets from Tabular Data. In\nTVCG. IEEE, 2020.\n[101] L. Watson. Accessible SVG Line Graphs, 2017. https://tink.uk/accessible-\nsvg-line-graphs/.\n[102] L. Watson. Accessible SVG Flowcharts, 2018.\n[103] W. Weber. Towards a Semiotics of Data Visualization – an Inventory of\nGraphic Resources. In IV. IEEE, 2019.\n[104] L. Wilkinson. The Grammar of Graphics . Statistics and Computing.\nSpringer-Verlag, 2005.\n[105] K. Wu, E. Petersen, T. Ahmad, D. Burlinson, S. Tanis, and D. A. Szaﬁr.\nUnderstanding Data Accessibility for People with Intellectual and Devel-\nopmental Disabilities. In CHI 2021, 2021.\n[106] C. Xiong, L. V . Weelden, and S. Franconeri. The Curse of Knowledge in\nVisual Data Communication. In TVCG. IEEE, 2020.\n[107] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel,\nand Y . Bengio. Show, Attend and Tell: Neural Image Caption Generation\nwith Visual Attention. arXiv, 2016.\n[108] J. S. Yi, Y .-a. Kang, J. T. Stasko, and J. A. Jacko. Understanding and\nCharacterizing Insights: How Do People Gain Insights Using Information\nVisualization? In BELIV. ACM, 2008.\n11",
  "topic": null,
  "concepts": [],
  "institutions": []
}