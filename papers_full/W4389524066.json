{
  "title": "Meta-Learning Online Adaptation of Language Models",
  "url": "https://openalex.org/W4389524066",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5109609226",
      "name": "Nathan Hu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5074009646",
      "name": "Eric Mitchell",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5046006076",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5005431772",
      "name": "Chelsea Finn",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4295883599",
    "https://openalex.org/W4385573858",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W2072223048",
    "https://openalex.org/W2982593362",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W277886906",
    "https://openalex.org/W4287820586",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2151835407",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W4221148722",
    "https://openalex.org/W2150299430",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4286904805",
    "https://openalex.org/W2902625698",
    "https://openalex.org/W1991564165",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4287025449",
    "https://openalex.org/W3041435213",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W2325227998",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W2963559848",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W1801199632",
    "https://openalex.org/W4286982890",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W2963780471",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1570098300"
  ],
  "abstract": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective \"shelf life.\" While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4418–4432\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMeta-Learning Online Adaptation of Language Models\nNathan Hu* Eric Mitchell*\nChristopher D. Manning Chelsea Finn\nStanford University\n{zixia314,em7}@stanford.edu\nAbstract\nLarge language models encode impressively\nbroad world knowledge in their parameters.\nHowever, the knowledge in static language\nmodels falls out of date, limiting the model’s\neffective “shelf life.” While online fine-tuning\ncan reduce this degradation, we find that\nnaively fine-tuning on a stream of documents\nleads to a low level of information uptake.\nWe hypothesize that online fine-tuning does\nnot sufficiently attend to important informa-\ntion. That is, the gradient signal from impor-\ntant tokens representing factual information\nis drowned out by the gradient from inher-\nently noisy tokens, suggesting that a dynamic,\ncontext-aware learning rate may be beneficial.\nWe therefore propose learning which tokens to\nupweight. We meta-train a small, autoregres-\nsive model to reweight the language modeling\nloss for each token during online fine-tuning,\nwith the objective of maximizing the out-of-\ndate base question-answering model’s ability\nto answer questions about a document after\na single weighted gradient step. We call this\napproach Context-aware Meta-learned Loss\nScaling (CaMeLS). Across three different dis-\ntributions of documents, our experiments find\nthat CaMeLS provides substantially improved\ninformation uptake on streams of thousands of\ndocuments compared with standard fine-tuning\nand baseline heuristics for reweighting token\nlosses.\n1 Introduction\nLarge language models learn impressively broad\nworld knowledge through large-scale unsupervised\npre-training, which they can leverage for a wide\nvariety of downstream tasks (Brown et al., 2020;\nChowdhery et al., 2022; Bubeck et al., 2023). How-\never, large language models are typically static ar-\ntifacts, and as the world changes, the knowledge\nencoded in their parameters becomes stale. While\nretrieval-augmented models are one approach to\n* Equal contribution.\nFigure 1: The proposed method CaMeLS learns to rescale the\nper-token online loss, sparsifying the fine-tuning gradients to\nemphasize informative timesteps. The middle row shows the\nweights output by CaMeLS. The top and bottom rows show\nraw and weighted per-token gradient norms, respectively.\nmitigating the staleness issue, even very large lan-\nguage models often fail to correctly update their\nmemorized predictions when presented with coun-\nterfactual retrieved information (Longpre et al.,\n2021; Li et al., 2022; Si et al., 2023). Moreover,\npurely parametric language models are uniquely\nsuited for edge computing due to their compact\nsize (relative to a large retrieval index) and simplic-\nity of inference (Gerganov, 2023). Recent work\nhas thus considered variants of online fine-tuning\non a stream of documents to efficiently perform\ndirect updates to the knowledge inside of a large\nlanguage model (Lazaridou et al., 2021; Jang et al.,\n2022).\nIdeally, we could simply fine-tune a language\nmodel on an online stream of documents, and the\ninformation contained in those documents would\nbe readily available for the model to use in a variety\nof downstream tasks such as answering questions\nabout the information in the documents. Unfortu-\nnately, we find that in this online adaptation setting,\nfine-tuning with a well-tuned learning rate leads\nto a nearly negligible improvement in a question-\n4418\nFigure 2: We study the setting of a language model being adapted unsupervised (without annotation of important tokens) on an\nonline stream of documents and being later evaluated on queries (e.g., questions) about those documents. Downstream inputs are\nnot provided during the adaptation phase, requiring the model to integrate as much information as possible about the documents.\nanswering model’s ability to answer questions re-\nlating to the stream of documents. We hypothesize\nthat naive fine-tuning is not effective in the online\nadaptation setting because the negative log likeli-\nhood (NLL) loss does not accurately reflect the im-\nportance of a token. That is, tokens containing im-\nportant factual information may receive relatively\nsmall NLL loss and therefore a small fine-tuning\ngradient. For example, consider the NLL of the\nword Rishi and the word Reports in the phrase The\nUK Prime Minister is Rishi Sunak. Reports suggest\n. . . for a slightly out-of-date language model. Be-\ncause Rishi Sunak was a well-known politician be-\nfore becoming Prime Minister, a model may place\nreasonably high probability mass on his name (even\nif other completions are higher probability). On the\nother hand, ‘Reports’ will invariably receive low\nprobability, because the distribution over the first\nword in a sentence is unavoidably high entropy.\nThis hypothesis suggests that we can improve\nupon online adaptation by only fine tuning on a\nsubset of tokens which are most likely to lead to\nuseful updates. One natural approach to identify\nsuch factual tokens is through salient spans (Guu\net al., 2020). Another common technique used\nto weight words it via TF-IDF scores (Salton and\nMcGill, 1986). We find that fine-tuning while using\nthese heuristics does improve information uptake.\nHowever, it is unclear if such heuristic choices are\noptimal. As an alternative, we explore a method\nfor learning a per-token importance weights corre-\nsponding to the utility of fine-tuning on that token.\nHowever, such utility is difficult to define, and even\nwith a suitable definition, dense per-token annota-\ntions of utility are extremely time-consuming to\ncollect. We thus select a definition of utility that en-\nables using distant supervision of the utility of each\ntoken: a high utility token is one whose fine-tuning\ngradient improves a question-answering model’s\nability to answer questions about the contents of\nthe surrounding document.\nUsing this notion of a token’s utility for online\nlearning, we propose Context-aware Meta-learned\nLoss Scaling (CaMeLS), an approach to online\nadaptation that meta-trains an importance weight-\ning model to identify such tokens in a document.\nGiven a dataset of documents and queries, we use\na meta-learning loss to train our weighting model:\nfirst, in an ‘inner loop,’ we update a base model (a\nproxy for the model we will update at test time) us-\ning the gradient of NLL of the document, weighted\nby the outputs of the importance weighting model.\nNext, in the ‘outer loop’, the loss is computed by\nevaluating the updated base model’s performance\non the corresponding query. This outer loss is used\nto updated the parameters of the importance weight-\ning model. During online fine-tuning on a stream\nof documents, we simply re-weight the online loss\nusing the importance-weighting model’s output.\nAlthough the process used to train CaMeLS uses\na proxy model (i.e., a stand-in for the model we will\nupdate at test time), one might hope that the impor-\ntance of tokens would be independent of the model\nused for inner loop updates; to a significant degree,\nwe intuit that the importance of a token should\nbe an innate trait of underlying text. Indeed, we\nfind that the meta-learned importance weights gen-\neralize across models; for each dataset, we meta-\ntrain our importance weighting model once using\nDistilGPT-2 (Sanh et al., 2019) as the base model\nand successfully use these weighting model with-\nout modification to update GPT-J 6B (Wang and\nKomatsuzaki, 2021). Across three online adap-\ntation benchmarks based on streams of news and\nWikipedia articles, CaMeLS substantially improves\nknowledge acquisition over naive fine-tuning as\nwell as salient span and TF-IDF based baselines.\n4419\n2 Related Work\nAdapting to new data or task distributions is typi-\ncally studied in the context of continual or lifelong\nlearning (Thrun and Mitchell, 1995; Mitchell et al.,\n2018). Continual learning in deep networks in-\nvolves the challenge of simultaneously avoiding\ncatastrophic forgetting (McCloskey and Cohen,\n1989), the process under which a neural network’s\nperformance on old tasks or data is dramatically\ndegraded by the process of learning new informa-\ntion, while maintaining plasticity (Dohare et al.,\n2022), or the ability to adapt to the latest change\nin the data distribution, even after many changes\nhave already been experienced. While most work\nin continual learning considers sequences of super-\nvised data (Kirkpatrick et al., 2016; Lopez-Paz and\nRanzato, 2017; Shin et al., 2017; Chaudhry et al.,\n2019), some work also studies continual few-shot\n(Ren et al., 2021) or unsupervised learning (Rao\net al., 2019; Madaan et al., 2022), which is closer\nto the setting in this paper. However, these works\ntypically focus on streams of visual data.\nDynamic, or streaming, language models were\nfirst considered in the context of n-gram language\nmodels, combining a cache of recently-used words\nto update the predictive probabilities of a tri-gram\nmodel (Kuhn, 1988; Jelinek et al., 1991; Osborne\net al., 2014). Later work describes online EM-\nbased algorithms for efficiently updating n-gram\nmodels (Yogatama et al., 2014). Other studies in-\nvestigate the evolution of decontextualized word\nembeddings over as a result of temporal shifts in\nthe use of language (Kulkarni et al., 2015; Hamil-\nton et al., 2016) or the use of vector memories\nto store recent information when training recur-\nrent neural networks online (Rei, 2015). More\nrecently, several studies have explored methods\nfor updating large neural language models, typ-\nically through online fine-tuning on a stream of\ndocuments (Lazaridou et al., 2021) with architec-\ntural constraints (Jang et al., 2022) or explicit con-\nditioning on time (Dhingra et al., 2022) used as\nstrategies to reduce forgetting of old information.\nClark et al. (2022) use meta-learning to reduce the\ncompute requirements of online fine-tuning. How-\never, recent work suggests that while increasing\nthe size of language models may largely mitigate\nthe problem of forgetting old information (Driess\net al., 2023), improving the efficiency of acqui-\nsition of new knowledge is still a challenge, and\nthis problem is therefore the focus of the present\nwork. Other methods for dynamically updating the\nknowledge in parametric language models develop\nspecialized techniques, called model editors, de-\nsigned to make targeted edits to individual facts\n(Sinitsin et al., 2020; Mitchell et al., 2021; Meng\net al., 2022) or behaviors (Mitchell et al., 2022).\nHowever, model editors assume access to annota-\ntions of the tokens or facts that must be updated; in\nthis work, we study the problem of learning which\ntokens in an unlabeled sequence of documents are\nimportant.\n3 Meta-Learning Improved Online\nAdaptation of Large Language Models\nGiven an out-of-date language model and a stream\nof recent documents, we aim to update the model\nsuch that it effectively answers typical queries\nabout the documents in the stream. By focusing\nonly on retaining knowledge relevant to the ‘typi-\ncal’ queries, we avoid the need to completely mem-\norize the documents, making the problem tractable.\nWe study question-answering (QA) models specifi-\ncally, as the question-answer format makes assess-\ning a model’s knowledge straightforward. In this\nsection, we formalize this problem setting and then\ndescribe an approach to this setting, Context-aware\nMeta-learned Loss Scaling.\n3.1 Unsupervised Online Adaptation\nWe consider a setting in which an out-of-date\nmodel fθbase is updated with an online stream2 of\nrecent documents Dtest = {xi}, ultimately produc-\ning an updated model fθ′. The updated model fθ′\nis then evaluated with a set of queries Qtest = {qi}\nwith labels Ytest = {yi}, where the the ith query\nis drawn from a distribution of queries relating to\nith document: qi,yi ∼p(qi,yi|xi). For example,\nqi may be a question about some information in\ndocument xi, and yi the answer to that question\nimplied by the document. Crucially, when using\nDtest to update fθbase, we do not have access to\nQtest. Thus, our methodology for updating fθbase\nmust be broad rather than query specific. In order\nto make this problem tractable (i.e., not requiring\ncomplete memorization of the document stream),\nwe assume that we have an additional corpus of\ndocuments Dtrain and corresponding query samples\nQtrain and labels Ytrain generated by a similar gener-\native process toQtest,Ytest. This training set enables\n2Dtest is typically an online stream of documents, but could\nbe an arbitrary ordering over a static collection of documents.\n4420\nFigure 3: A single step of CaMeLS meta-training. In step\n1, the weighting model (red) produces a set of importance\nweights over the tokens in a given document. In step 2, the\nbase model (blue) is updated using a single gradient step\non the weighted NLL, producing an adapted model (pink).\nIn step 3, the weighting model is updated to improve the\nadapted base model’s ability to answer questions about the\ndocument. During test-time adaptation, steps 1 and 2 are\napplied repeatedly for each document in the test document\nstream.\nlearning the types of queries that may be of inter-\nest, informing how we should update our model to\nmaximize the performance on test queries while\nminimizing disturbance to its prior knowledge or\nbehaviors. We next describe an algorithm for lever-\naging this dataset to more efficiently update our\nbase model on the test stream of documents Dtest.\n3.2 CaMeLS: Context-aware Meta-learned\nLoss Scaling\nThe goal of CaMeLS is to distill the information\nin the training documents, queries, and labels into\na parameter vector ϕ. This vector summarizes the\noptimal way to update a base model on a document\nstream to maximize retention of information likely\nto be relevant to test queries . CaMeLS accom-\nplishes this goal by training a weighting model wϕ\n(a small autoregressive language model 3) that re-\nweights the online NLL loss used in typical online\nfine-tuning, focusing on the tokens whose NLL gra-\ndient is most useful for updating a small proxy base\nmodel’s knowledge. In other words, the weighting\nmodel is trained to re-weight the NLL loss such\nthat the proxy model is able to correctly answer\nquestions about a document after one gradient step\non the modified NLL of the document. The weight-\ning model is trained with an episodic bi-level opti-\nmization, which we explain next in detail (also see\nFigure 3).\nDuring each episode, a training document-query-\nlabel triple (x,q,y ) is sampled from Dtrain and a\n3Section 3.4 contains details on the weighting model’s\narchitecture.\nlocality example xloc from Dloc. Dloc is a dataset\nof unlabeled text representing the distribution over\nwhich we want the base model’s behavior to re-\nmain generally unchanged. For all experiments,\nwe use the OpenWebText dataset (Gokaslan et al.,\n2019) as Dloc. Let θbase denote the parameters of\nthe proxy base model at the start of the episode.\nThe update to the weighting model involves three\nsteps: 1) computing the weights for the training\ndocument, 2) updating the small proxy base model\non the weighted NLL on the training document,\nand 3) backpropagating the ‘outer loop’ loss 4 of\nthe updated proxy model on a query and label from\nthe training document. These steps are shown in\nFigure 3. Let L(fθ,x, a) denote the weighted NLL\nof fθ on document xusing weights a. Steps 1 & 2\nare described by the inner loop update rule:\nθ′= θbase −α∇θbase L(fθbase ,x,w ϕ(x)) (1)\nThe inner loop learning rate αcan be fixed, sam-\npled, or learned. For all of our experiments, we use\na fixed inner learning rate of α= 5e−4. After the\nupdated proxy model is computed, we compute an\nouter loop loss measuring the effectiveness of the\nweighted adaptation procedure on the document x:\nLouter = −logpθ′(y|q) +clocLloc(θbase,θ′,xloc) (2)\nIn addition to the negative log likelihood of label\ngiven the query and updated base model parame-\nters, the outer loss has a locality term Lloc which\nprevents the updated base model parameters from\nexcessively changing the base model’s behavior.\ncloc is set to .1 for all experiments. Lloc is the sum\nof the KL divergencesLi\nloc between the base model\nbefore and after adaptation conditioned on each\nprefix xi\nloc of the locality input xloc, with\nLiloc(θbase,θ′,xloc) =KL(pθbase(·|xiloc)∥pθ′(·|xiloc)) (3)\nFinally, we perform a single update to the weight-\ning model’s parameters by computing the gradient\nof the outer loop loss with respect to ϕ. We opti-\nmize ϕwith the Adam optimizer, using a learning\nrate of 1e-5. We accumulate outer loop gradients\nover 24 examples (document-query-label triples)\nsplit into 4 batches of 6 triples.\n4Performing a single update to the proxy model is the\ninner loop and updating the weighting model according to the\nupdated proxy model’s loss on the query is considered the\nouter loop of a bi-level optimization used to train CaMeLS.\n4421\n3.3 Mitigating Train-Test Shift\nThe single-step training procedure described above\noptimizes for effective knowledge retention for a\nsingle document. However, in our online adapta-\ntion setting, we may update for hundreds or thou-\nsands of documents before we evaluate on our\ndownstream queries. In order to mitigate this train-\ntest shift, we modify CaMeLS with two strate-\ngies. First, we do not use the same base model\nparameters during each episode of training. This is\ndone to prevent the weighting model from over-\nfitting to a single base model state. For most\ntraining episodes, the starting base model param-\neters adapted in the inner loop are the final base\nmodel parameters in the previous episode. Every\ncreset = 4 episodes of training, the starting base\nmodel parameters are reset to those of the origi-\nnal base model. Second, instead of performing an\ninner update on a single document, we sample an\ninner batch of k= 6document-query-label triples\n(x1,q1,y1),..., (xk,qk,yk) for each episode. A\nsequence of kinner loop updates is performed:\nθi = θi−1 −α∇θL(fθi−1 ,xi,wϕ(xi)) (4)\nwhere θ0 = θbase and θ′ = θk. The outer loss is\ncomputed as before, but now averaging the query-\nlabel loss over the inner batch. By allowing in-\nner loop updates to accumulate during adaptation,\nϕ learns an updating strategy that preserves the\nknowledge of prior updates and maintains the base\nmodel’s ability to learn from subsequent updates.\n3.4 Compute & Architecture of CaMeLS\nOptimizing bi-level objectives like the one used by\nCaMeLS is rather memory and compute-intensive,\nrequiring memory and compute proportional to the\ndepth of the inner loop (the batch size used for mul-\ntiple inner loop updates) and proportional to the\nsize of our base/proxy model - each inner loop step\ncreates an updated copy of the base model parame-\nters in the computation graph. However, CaMeLS\nonly requires a lightweight base model; our exper-\niments use DistilGPT-2 as the base model during\nmeta-training, but we find strong transfer to much\nlarger base models during evaluation. The weight-\ning model itself is also small; all experiments use\nDistilGPT-2 as the weighting model (a MLP with\na single hidden state of size 128 is used as the head\nto produce token weights). Using the base and\nweighting models described, we are able to train\nweighting models using 6 inner loop steps on a\nsingle NVIDIA A40 GPU.\nMethod Time Per Doc Total GPU Memory\nUniform 772.72 ms 46.62 GB\nCaMeLS 782.46 ms 48.18 GB\nTable 1: Compared to standard uniform fine-tuning, CaMeLS\nrequires slightly more GPU memory to store the weight model\nand is slightly slower per document. All compute measure-\nments were taken while adapting GPT-2 XL to StreamingQA\ndocuments using an 80GB NVIDIA A100.\nDataset Avg. text length Texts per stream\nStreamingQA ∼510 tokens 1665 articles\nSQuAD ∼150 tokens 1170 paragraphs\nArchivalQA ∼80 tokens 3001 paragraphs\nTable 2: Basic statistics of the data in our online document\nstreams. The sample text streams used to evaluate online\nadaptation vary significantly in length. For the SQuAD and\nArchivalQA datasets, the answer to each query is a span in\nits corresponding document; for StreamingQA, this is not the\ncase.\nWe next discuss the compute costs of using a\ntrained CaMeLS weighting model for online adap-\ntation. The additional compute needed for CaMeLS\nis very small compared to uniform fine-tuning —\nis a single forward pass of a weight model for each\ndocument we update on. For large models, the\nweight model overhead is small compared to the\ntime needed to run a forward and backward pass\nof the base model. Compared to standard uniform\nfine-tuning, CaMeLS requires slightly more GPU\nmemory to store the weight model and is slightly\nslower per document. Table 1 shows compute mea-\nsurements during online adaptation of GPT-2 XL\non StreamingQA.\n4 Experiments\nAfter outlining datasets and experimental details,\nwe present several experiments aimed at under-\nstanding CaMeLS’s behavior in unsupervised on-\nline adaptation. Section 4.3 studies the extent\nto which CaMeLS’s importance weights improve\nknowledge retention in online adaptation. Sec-\ntion 4.4 qualitatively and quantitatively explores\nthe weights themselves, suggesting several abla-\ntions of CaMeLS that we explore in Section 4.5.\nSection 4.6 evaluates the cross-dataset generaliza-\ntion of CaMeLS weights, and finally we examine\nthe forgetting and plasticity dynamics of CaMeLS\nwithin the document stream in Section 4.7.\n4.1 Datasets\nWe apply CaMeLS to three question answering\ndatasets with corresponding source articles. We\n4422\npartition the datasets into 5 splits. Three of these\nsplits (train, valid, test) are used for training, hy-\nperparameter tuning, and evaluating the CaMeLS\nweighting model. In order to fine-tune the initial\nQA base models from generic language models,\nwe reserve two more disjoint splits (in order to pre-\nvent reusing questions during initial QA tuning and\nonline adaptation), labeled QA train and QA valid.\nAdditional details on dataset splits and samples are\nin Appendix A. At evaluation time, a stream of\ndocuments is sampled from the test split. The doc-\numents length and text stream lengths are shown in\nTable 2. In the StreamingQA setting, models must\nadapt to an entire article as opposed to a selected\nparagraph, making it our most challenging setting.\nStreamingQA (Liška et al., 2022): The Stream-\ningQA dataset contains a combination of human-\nwritten and language model generated questions.\nQuestions are generated from English WMT news\narticles published between 2007 and 2020.\nSQuAD (Rajpurkar et al., 2016): The Stanford\nQuestion Answering Dataset (SQuAD) contains\nhuman generated questions from Wikipedia articles.\nThe answer to each question is a span contained in\na paragraph from Wikipedia.\nArchivalQA (Wang et al., 2022): The ArchivalQA\ndataset contains automatically generated questions.\nQuestions are generated from articles in the New\nYork Times Annotated Corpus (Sandhaus, Evan,\n2008). The answer to each question is a span con-\ntained in an article.\n4.2 Experimental protocol details\nWe conducted evaluations on two families of au-\ntoregressive language models, the GPT-2 (Rad-\nford et al., 2018) and GPT-Neo families (Black\net al., 2021), as well as GPT-J (Wang and Komat-\nsuzaki, 2021). We note that all models evaluated\nuse the same text tokenization. For all datasets, we\nfirst fine-tune each pretrained model on question-\nanswer pairs from that dataset. These tuned models\nrepresent the static language models we wish to\nupdate and will be referred to as base models. For\neach dataset, a single weighting model is trained.\nThe proxy language model used during weighting\nmodel training is DistilGPT-2 fine-tuned on the QA\ntrain split of the respective dataset.\nAt evaluation time, the base model is updated on\na stream of documents sampled from the test split\n5. The final adapted base model is evaluated on\n5We use an Adam Optimizer most experimental runs. Due\nGPT-2 Large GPT-Neo 1.3B GPT-2 XL GPT-Neo 2.7B GPT-J 6B\nBase Model\n0.0\n0.2\n0.4\n0.6\n0.8Relative F1 Improvement\nUniform\nUniform + QA-tuning\nSalient Spans\nTF-IDF + 5% Cutoff\nCaMeLS (Ours)\nStreamingQA\nGPT-Neo 1.3B GPT-2 XL\nBase Model\n0.2\n0.1\n0.0\n0.1\nRelative F1 Improvement\nSQuAD\nGPT-Neo 1.3B GPT-2 XL GPT-J 6B\nBase Model\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\nRelative F1 Improvement\nArchivalQA\nFigure 4: CaMeLS’s meta-learned weights improve knowl-\nedge uptake after online language model adaptation on a\nstream of data. The F1 score of the base model before and after\nadaptation with CaMeLS are computed on questions about the\ndocuments used for adaptation. The relative change in F1 is\nplotted. Top, lower left, and lower rightshow StreamingQA,\nSQuAD, and ArchivalQA datasets, respectively. Error bars\nare standard error over 4 sampled streams of test data.\nthe questions corresponding to the documents in\nthe sampled stream. We compare CaMeLS with 4\nbaselines. First is standard fine tuning or Uniform\nwhere tokens are equally weighted. In Uniform\n+ QA-tunewe additionally fine tune for question\nanswering after adaptation. Next we consider com-\nmon weighting heuristics. Salient Spans corre-\nsponds to assigning a uniform weight to tokens in\nsalient spans and no weight to all other tokens. In\nTF-IDF + 5% Cutoff, we first compute TF-IDF\nscores using the both the adaptation documents\nand additional in distribution documents. To ac-\ncount for stopwords, we remove the 5% of words\nwith lowest TF-IDF scores. The remaining TF-IDF\nscores are used to reweight the tokens. 6 For each\ncombination of base model and online adaptation\nstrategy, the learning rate used at test time was\nchosen via hyper parameter sweep on a stream of\ndocuments sampled from the validation set.7\n4.3 CaMeLS improves knowledge retention\nWe first compare the knowledge retained by\nCaMeLS and baselines for three different data dis-\ntributions in Figure 4. CaMeLS outperforms other\nto compute constraints, we use an Adafactor optimizer for\nadaptation of GPT-Neo 2.7B and GPT-J 6B.\n6TF-IDF scores are computed using a word level tokeniza-\ntion. These scores are then mapped to the BPE tokenization\nof the adapted language models.\n7All learning sweeps are conducted over the following\nvalues: [1e-4, 2.5e-5, 6.25e-6, 1.625e-6]. The optimal learning\nrates are in Appendix D.\n4423\n0 50 100 150 200 250\nImportance Weight\n10 3\n10 2\n10 1\nFrequency (log)\n0.00 0.25 0.50 0.75 1.00\nProportion\nDET\nADP\nAUX\nPRON\nVERB\nADV\nPUNCT\nADJ\nNOUN\nPROPN\nNUM\nPart of Speech\nbottom 10% of range central 80% of range top 10% of range\nFigure 5: The importance weight distribution learned by\nCaMeLS is bimodal, with proper nouns and numbers being the\nparts of speech most likely to have high importance weights.\nThe overall importance weight distribution (left) and the dis-\ntribution conditioned by part of speech (right) are shown on\nthe validation split of StreamingQA.\nonline adaptation approaches across a range of\ndatasets and weighting models. Despite the dif-\nference in scale between the proxy model used dur-\ning weight training and the evaluated base models,\nCaMeLS’s learned importance weights generalize\nwell to the largest base model we evaluate, GPT-J\n6B, which is over 70 times the size of the proxy\nmodel (DistilGPT-2, 82 million parameters) used\nduring training. We find that standard online fine\ntuning (uniform weighting) with Adam performs\nvery poorly on online adaptation. Even with a\ntuned learning rate and further training for question\nanswering post adaptation, uniform weighting fails\nto achieve a significant improvement for several\nmodels tested.\n4.4 Analysis of learned weights\nOne benefit of CaMeLS over other methods for\nmeta-learning model updating strategies is that\nlearned updating strategy, token weights, is inter-\npretable. Figure 1 shows the per-token weights on\nsample text and how they combine with the un-\nweighted gradient norms to produce sparsified per-\ntoken gradient norms. In this section, we provide\nadditional analysis of CaMeLS’s learned weights.\nWe examine the distribution of weighting model\noutputs on articles in the validation set of Stream-\ningQA in Figure 5. As our qualitative evaluations\nshow, we confirm that the distribution of weights\nover the entire validation split of StreamingQA is\nindeed sparse and bimodal. We thus interpret the\nweighting model as acting as a context-aware bi-\nnary classifier, determining if a token is informative\nor uninformative. When binning weights by part of\nspeech, we find that numbers and proper nouns are\nmost frequently assigned a high weight. This result\naligns with Lazaridou et al. (2021), who found that\nGPT-Neo 1.3B GPT-2 XL\nBase Model\n0.1\n0.0\n0.1\n0.2\n0.3\nRelative F1 Improvement\nUniform + QA-tune\nSalient Spans\nTF-IDF + 5% Cutoff\nPOS: Resample\nPOS: Mean\nBimodal CaMeLS\nCaMeLS\nFigure 6: Ablations of CaMeLS. Bimodal Ablationrestricts\nthe weighting model from outputting intermediate values\nwhile the POS ablations remove context dependence, con-\nditioning only on part of speech. While restricting CaMeLS\nto output only one of two values only slightly reduces perfor-\nmance, conditioning only on part of speech, rather than full\ncontext, drastically reduces knowledge retention.\nan outdated language model’s performance most\nrapidly declines on proper nouns and numbers.\n4.5 Ablations\nIn order to verify that context-aware weights are\ntruly necessary to achieving improved knowledge\nretention, we now examine several ablations of\nCaMeLS. In the POS: Resample ablation, the\nweight of each token is generated by sampling from\nthe distribution of importance weights on all tokens\nof the same part of speech. In the POS: Mean\nablation, each token is weighted by the mean im-\nportance weight assigned to tokens of that part of\nspeech. We additionally consider a Bimodal ab-\nlation where outputs of the weighting model are\nrounded to either the largest or smallest value in\nthe distribution of importance weights.\nFigure 6 shows the results on the StreamingQA\ndataset for two different base models. We observe\nthat ablating the weighting model to only output\ntwo values slightly reduces performance, while still\nachieving significant F1 improvement and outper-\nforming baseline approaches. The strong perfor-\nmance of the binary ablation suggests that a binary\ndecision of whether to train on a given token is an\neffective approach to online adaptation, though the\nfull version of CaMeLS that allows for variation in\nthe weight magnitude still performs best.\nIn contrast, neither part-of-speech ablation pro-\nduces effective knowledge retention, either per-\nforming worse than the uniform baseline or fail-\ning to significantly increase F1 score. This result\nstrongly suggests that although part of speech corre-\nlates strongly with learned weights, part of speech\nalone is not sufficient to determine when a token\ncontains important information. We conclude that\ncontext-awareness is indeed helpful for identifying\n4424\nCaMeLS\n (StreamingQA)\nCaMeLS\n (SQuAD)\nCaMeLS\n (ArchivalQA)\n0.298 0.089 0.233\n0.249 0.174 0.254\n0.153 0.040 0.268\nStreamingQA SQuAD ArchivalQA\nEvaluation Dataset\nUniform\n + QA-tune\nSalient Spans\nTF-IDF\n + 5% Cutoff\n0.096 0.043 0.060\n0.072 0.028 0.210\n0.086 0.158 0.181\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nAdaptation Approach\nFigure 7: CaMeLS weight models on unseen data distribu-\ntions (off-diagonals of top three rows) frequently outperforms\nbaseline online adaptation approaches (bottom three rows).\nEach CaMeLS model was trained on a single dataset (shown\nin parenthesis) and used to adapt GPT-2 XL on streams of data\nfrom various datasets.\nimportant tokens in online adaptation.\n4.6 Cross Dataset Transfer\nBeyond generalizing to new base models, we now\nstudy CaMeLS’s ability to generalize to new data\ndistributions. We evaluate CaMeLS’s performance\nfor all nine possible combinations of train and\ntest dataset, using StreamingQA, SQuAD, and\nArchivalQA. Figure 7 shows the results. We find\nthat CaMeLS trained on a different dataset still\ntypically outperforms the baseline methods, provid-\ning stronger evidence that the weighting scheme\nlearned by CaMeLS is general-purpose. The gener-\nalizability of CaMeLS’s weighting model is a key\nattribute increasing its practical utility.\n4.7 Forgetting and plasticity\nSo far, our evaluations have considered only the QA\naccuracy at the end of online adaptation. In this\nsection, we investigate the evolution of learning\nduring the online adaptation process. While adapt-\ning GPT-2 XL to data from StreamingQA, we eval-\nuate the intermediate models produced by CaMeLS\nand baseline methods every 200 document updates.\nResults are plotted for two learning rates. 6.250e-6\nis the optimal learning rate for the TF-IDF baseline\nwhile 2.500e-5 is the optimal learning rate for all\nother methods shown. Figure 8 shows the perfor-\nmance when intermediate models are evaluated on\nthe entire set of evaluation queries and additionally\nevaluated on a set of unrelated queries sampled\nfrom the QA validation spit. CaMeLS consistently\nimproves performance on test queries during online\nadaptation, while the best performing baseline —\n0 200 400 600 800 1000 1200 1400 1600\n0.08\n0.10\n0.12\n0.14\n0.16Test Query F1\nlr = 6.25e-6\n0 200 400 600 800 1000 1200 1400 1600\nlr = 2.5e-5\n0 200 400 600 800 1000 1200 1400 1600\n \n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20Unrelated F1\nlr = 6.25e-6\n0 200 400 600 800 1000 1200 1400 1600\n \nlr = 2.5e-5\nDocument Updates\nCaMeLS\nSalient Spans\nUniform + QA-tune\nQA-tuning\nTF-IDF + 5%% Cutoff\nInitial\nFigure 8: Base model performance during StreamingQA on-\nline adaptation of GPT-2 XL. Performance is evaluated every\n200 article updates on the downstream answering task (top)\nand on unrelated validation questions used in QA pretraining\n(bottom). Results are plotted for two learning rates. 6.250e-\n6 (left) is the optimal learning rate for the TF-IDF baseline\nwhile 2.500e-5 (right) is the optimal learning rate for all other\nmethods shown. Shaded regions are 1 standard error over 4\nruns. All adaptation methods lead to gradual degradation in\nunrelated questions performance. CaMeLS results in gradual\nincreases in base model test performance. Using its optimal\nlearning rate, uniform fine-tuning with post adaptation QA\ntuning are only realizes its performance increases after a post-\nadaptation QA-tuning step.\nuniform fine-tuning with a learning rate of 2.500e-\n5 and additional QA-tuning — results in gradual\ndegradation in test performance with improvement\nonly becoming realized after the post-adaptation\nQA-tuning step. Turning to performance on un-\nrelated queries, we see that all methods result in\na gradual degradation in performance on indepen-\ndent queries. At a learning rate of 6.250e-6, all\nmethods lead to comparable degradation in perfor-\nmance on unrelated queries. At a learning rate of\n2.5e-6 CaMeLS leads to the lowest drop in unre-\nlated query performance. Taken together, these\nresults suggest that the CaMeLS is able to more ef-\nfectively update the base model’s knowledge, while\nstill preserving the model’s pre-existing knowledge\nand its representation of the task.\nFinally, in Figure 9, we aim to answer the ques-\ntions how long does the model remember the an-\nswer to a question after observing it? We show\nthe average improvement in F1 score across test\nqueries against the number of timesteps since the\nmodel observed the document containing the an-\nswer to the query. Each adaptation method is ap-\nplied using a uniquely tuned learning rate. After\nthe 200 document sequence containing the rele-\nvant document, all methods see a clear average\nimprovement in F1 score, signifying learning is\nhappening. However, we also note that CaMeLS\n4425\nproduces both a higher initial improvement as well\nas a higher asymptotic improvement in F1 score.\nCaMeLS not only improves the immediate plas-\nticity of the model, integrating knowledge more\nreadily, but also reduces forgetting, preserving the\nnewly-integrated knowledge for longer.\n5 Discussion\nWhile large language models are powerful, keeping\nthem up-to-date remains a challenge. In this pa-\nper, we consider the unsupervised online language\nmodel adaptation setting, in which a language\nmodel’s knowledge must be updated using a stream\nof documents, without annotations of key facts or\ninformation. Finding that naive online fine-tuning\nprovides little retention of knowledge from the doc-\nument stream, we propose Context-aware Meta-\nlearned Loss Scaling (CaMeLS), a meta-learning al-\ngorithm that learns an importance weighting model\nto reweight the per-token loss of the online data\nstream. CaMeLS leverages side information of the\nform of paired documents and knowledge queries\nabout those documents to identify which tokens in\nthe documents are most likely to be informative for\nanswering downstream queries. Empirically, we\nfind that the importance weighting model learned\nby CaMeLS consistently improves knowledge re-\ntention across three datasets of documents and ques-\ntions. Crucially, we find that CaMeLS’s importance\nweighting model generalizes across outdated lan-\nguage models and datasets, meaning that an impor-\ntance weighting model can be trained once on a\nsmall proxy language model (such as DistilGPT-2)\nand then be immediately used to improve online\nadaptation of much larger models, like GPT-J 6B.\nThis transferrability of CaMeLS’s weighting model\nsignificantly increases its practical utility.\nLimitations & Future Work\nWhile our experiments suggest that learned im-\nportance weights consistently improve knowledge\nretention after unsupervised online adaptation, our\nstudy has several limitations. CaMeLS assumes\naccess to side information in the form of training\ndocument, query, and label triples. This require-\nment may be onerous in domains where labeling\nis expensive. Future work may apply CaMeLS to\nsettings without access to side information queries\nand labels, i.e., only a purely unlabeled stream of\ntraining documents, using the temporal structure\nof the data as the signal for learning. We study\n600\n 400\n 200\n 0 200 400 600 800\nDocument Updates since Adaptation\n0.01\n0.00\n0.01\n0.02\n0.03\n0.04\nF1 Increase\nDocument Adaptation\nCaMeLS\nSalient Spans\nUniform\nUniform + QA-tune\nTF-IDF + 5%% Cutoff\nFigure 9: While adapting GPT-2 XL on StreamingQA, we ex-\namine the average improvement in F1 score of queries against\nthe time since the model observed the corresponding docu-\nment. The shaded region represents the interval in which the\nsource document was presented. CaMeLS leads to a larger\ninitial improvement and asymptotic improvement in F1 score\nthan other methods. Although this mid-adaptation evaluation\ndoes not use QA-tuning, Uniform + QA-tune corresponds to\nuniform fine tuning using a learning rate optimized to down-\nstream performance given an additional QA-tuning step. Each\nadaptation method is applied using a uniquely tuned learning\nrate.\nadaptation on steams of thousands of documents.\nHowever, in order to effectively update outdated\nlanguage models in real-world scenarios, it is rea-\nsonable to expect a significantly larger volume of\ndocuments. Beyond dataset scale, our experiments\nstudy adaptation of base models up to 6B parame-\nters, but recent work suggests the continual learn-\ning dynamics of language models changes drasti-\ncally at extreme scale (100B+ parameters); future\nwork may increase the scale of the present study by\nconsidering adaptation on longer streams of docu-\nments using larger base evaluation models. Finally,\nwe study only question-answering models and the\nquestion-answering task, as it is the most direct\nform of knowledge retention assessment. Future\nwork may examine knowledge retention in other\ntypes of models through alternative downstream\ntasks that leverage the knowledge in the document\nstream more indirectly, as well as studying the abil-\nity to continually update general-purpose genera-\ntive models of language or dialogue models.\nAcknowledgements\nThe authors thank Huaxiu Yao for his input at mul-\ntiple stages of the project. CF and CDM are CI-\nFAR Fellows. EM gratefully acknowledges fund-\ning from a Knight-Hennessy Graduate Fellowship.\nThis research was supported in part by Juniper Net-\nworks.\n4426\nReferences\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with GPT-4. ArXiv\npreprint arXiv:2303.12712.\nArslan Chaudhry, Marc’Aurelio Ranzato, Marcus\nRohrbach, and Mohamed Elhoseiny. 2019. Efficient\nlifelong learning with a-GEM. In International Con-\nference on Learning Representations.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKevin Clark, Kelvin Guu, Ming-Wei Chang, Panupong\nPasupat, Geoffrey Hinton, and Mohammad Norouzi.\n2022. Meta-learning fast weight language models.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n9751–9757, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-Aware Language\nModels as Temporal Knowledge Bases. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:257–273.\nShibhansh Dohare, Richard S. Sutton, and A. Rupam\nMahmood. 2022. Continual backprop: Stochastic\ngradient descent with persistent randomness.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe\nYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\nmanet, Daniel Duckworth, Sergey Levine, Vincent\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus\nGreff, Andy Zeng, Igor Mordatch, and Pete Florence.\n2023. Palm-e: An embodied multimodal language\nmodel. In arXiv preprint arXiv:2303.03378.\nGeorgi Gerganov. 2023. llama.cpp. https://github.\ncom/ggerganov/llama.cpp.\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Ste-\nfanie Tellex. 2019. Openwebtext corpus. http:\n//Skylion007.github.io/OpenWebTextCorpus.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR,\nabs/2002.08909.\nWilliam L. Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016. Diachronic word embeddings reveal statisti-\ncal laws of semantic change. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1489–1501, Berlin, Germany. Association for Com-\nputational Linguistics.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. 2022. Towards continual\nknowledge learning of language models. In Interna-\ntional Conference on Learning Representations.\nF. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. 1991.\nA dynamic language model for speech recognition.\nIn Speech and Natural Language: Proceedings of a\nWorkshop Held at Pacific Grove, California, Febru-\nary 19-22, 1991.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2016. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114.\nRoland Kuhn. 1988. Speech recognition and the fre-\nquency of recently used words: A modified Markov\nmodel for natural language. In Coling Budapest 1988\nVolume 1: International Conference on Computa-\ntional Linguistics.\nVivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and\nSteven Skiena. 2015. Statistically significant detec-\ntion of linguistic change. In Proceedings of the 24th\nInternational Conference on World Wide Web, WWW\n’15, page 625–635, Republic and Canton of Geneva,\nCHE. International World Wide Web Conferences\nSteering Committee.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nTomáš Koˇciský, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom. 2021.\nMind the gap: Assessing temporal generalization\n4427\nin neural language models. In Advances in Neural\nInformation Processing Systems.\nWei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan\nXiao, and Hua Wu. 2022. Faithfulness in natural\nlanguage generation: A systematic survey of anal-\nysis, evaluation and optimization methods. ArXiv,\nabs/2203.05227.\nAdam Liška, Tomáš Koˇciský, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, Cyprien\nde Masson d’Autume, Tim Scholtes, Manzil Zaheer,\nSusannah Young, Ellen Gilsenan-McMahon, Sophia\nAustin, Phil Blunsom, and Angeliki Lazaridou. 2022.\nStreamingqa: A benchmark for adaptation to new\nknowledge over time in question answering models.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conflicts in question\nanswering. arXiv preprint arXiv:2109.05052.\nDavid Lopez-Paz and Marc’Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. Ad-\nvances in neural information processing systems, 30.\nDivyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin\nLiu, and Sung Ju Hwang. 2022. Representational\ncontinuity for unsupervised continual learning. In In-\nternational Conference on Learning Representations.\nMichael McCloskey and Neal J. Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Gordon H. Bower,\neditor, Psychology of Learning and Motivation, vol-\nume 24 of Psychology of Learning and Motivation,\npages 109–165. Academic Press.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associa-\ntions in GPT. ArXiv:2202.05262.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D. Manning. 2021. Fast model\nediting at scale. CoRR.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022. Memory-\nbased model editing at scale. In Proceedings of the\n39th International Conference on Machine Learning,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 15817–15831. PMLR.\nT. Mitchell, W. Cohen, E. Hruschka, P. Talukdar,\nB. Yang, J. Betteridge, A. Carlson, B. Dalvi, M. Gard-\nner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,\nT. Mohamed, N. Nakashole, E. Platanios, A. Rit-\nter, M. Samadi, B. Settles, R. Wang, D. Wijaya,\nA. Gupta, X. Chen, A. Saparov, M. Greaves, and\nJ. Welling. 2018. Never-ending learning. Commun.\nACM, 61(5):103–115.\nMiles Osborne, Ashwin Lall, and Benjamin Van Durme.\n2014. Exponential reservoir sampling for streaming\nlanguage models. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 687–692,\nBaltimore, Maryland. Association for Computational\nLinguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners. Ms.,\nOpenAI.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text.\nDushyant Rao, Francesco Visin, Andrei Rusu, Razvan\nPascanu, Yee Whye Teh, and Raia Hadsell. 2019.\nContinual unsupervised representation learning. In\nAdvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nMarek Rei. 2015. Online representation learning in re-\ncurrent neural language models. In Proceedings of\nthe 2015 Conference on Empirical Methods in Nat-\nural Language Processing, pages 238–243, Lisbon,\nPortugal. Association for Computational Linguistics.\nMengye Ren, Michael Louis Iuzzolino, Michael Curtis\nMozer, and Richard Zemel. 2021. Wandering within\na world: Online contextualized few-shot learning. In\nInternational Conference on Learning Representa-\ntions.\nG. Salton and M. J. McGill. 1986. Introduction to Mod-\nern Information Retrieval. McGraw-Hill, Inc., New\nYork, NY , USA.\nSandhaus, Evan. 2008. The new york times annotated\ncorpus.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. In NeurIPS\nEMC2 Workshop.\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon\nKim. 2017. Continual learning with deep generative\nreplay. In Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Lee Boyd-Graber, and\nLijuan Wang. 2023. Prompting GPT-3 to be reli-\nable. In The Eleventh International Conference on\nLearning Representations.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In ICLR.\nSebastian Thrun and Tom M. Mitchell. 1995. Lifelong\nrobot learning. Robotics and Autonomous Systems,\n15(1):25–46. The Biology and Technology of Intelli-\ngent Autonomous Agents.\n4428\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nJiexin Wang, Adam Jatowt, and Masatoshi Yoshikawa.\n2022. Archivalqa: A large-scale benchmark dataset\nfor open domain question answering over historical\nnews collections.\nDani Yogatama, Chong Wang, Bryan R. Routledge,\nNoah A. Smith, and Eric P. Xing. 2014. Dynamic\nlanguage models for streaming text. Transactions of\nthe Association for Computational Linguistics, 2:181–\n192.\n4429\nSQA SQuAD ArchivalQA\nSplit Nx/q Nx Nq Nx Nq\nTrain 21k 8.6k 39.9k 12.8k 21.7k\nValidation 1.7k 1.2k 5.6k 3.0k 5.3k\nTest 5k 2.1k 10.6k 5.0k 8.7k\nQA Train 40k - 40k - 12.4k\nQA Valid. 4k - 2.1k - 3k\nTable 3: Number of documents Nx and questions Nq for each\ndataset. Each document in StreamingQA (SQA) corresponds\nto a single question, while SQuAD and ArchivalQA contain\ndocuments corresponding to multiple questions.\nA Dataset Details\nThe sizes of dataset splits are shown in Table 3.\nSample documents, questions, and answers are\nshown in Table 4. Only documents from 2018 and\non-wards are used to the train, validation, and test\nsplits of StreamingQA. For SQuAD, the entirety\nof the validation set of SQuAD is used at our test\nsplit. The topics in training set of SQuAD are re-\npartitioned to form the other 4 splits. We divide the\nvalidation set of the ArchivalQA dataset to form\nour 5 splits. These splits are done temporally, us-\ning documents from 1987-1990 for QA Training,\n1991-1992 for QA Validation, 1993-2001 for Train-\ning, 2002-2003 for Validation, and 2004-2007 for\nTesting.\nB Larger Proxy Models\nWe conduct a preliminary investigation on the ef-\nfect of using a larger proxy model during CaMeLS\nmeta-training. By default, we use a QA-tuned Dis-\ntilGPT2 (82M) as the proxy model. We addition-\nally meta-train using a GPT-2 Small (117M) as the\nproxy model. Due to compute limitations we were\nnot able to meta-train using any larger proxy mod-\nels. Results on StreamingQA are shown in table\n5. We see no significant difference in performance\nin this setting. Qualitatively, the two weighting\nmodels generate similar outputs. We hypothesize\nthat CaMeLS learns a weighting which reflects the\ninnate importance of tokens in the text to answer-\ning the meta-training questions, rather than a proxy\nmodel specific token importance. We emphasize\nthat this is a hypothesis and believe a more rigor-\nous exploration of proxy model size is an exciting\ndirection for future work.\nC Combining CaMeLS with other online\nAdaptation Methods\nThere are various other methods for online adapta-\ntion which leverage the adaptation documents. Two\nsuch methods are in-context learning and retrieval.\nThis section shows preliminary experiments lever-\nDataset Document Question Answer\nStreamingQA Colin Farrell goes missing in new trailer March 2 (UPI) –\nColin Farrell joins the cast of Artemis Fowl in the latest\ntrailer for Disney’s upcoming fantasy-adventure film.\nFarrell is featured in the clip, released on Monday, as the\nmissing father of Ferdia Shaw’s Artemis Fowl who also\ngoes by the same name. Farrell’s character is a criminal\nmastermind who has mysteriously disappeared. Artemis\nFowl learns that his father has protected powerful secrets\nthat have kept mankind safe and learns that his\ndisappearance is connected to a secret fairy world. Artemis\nFowl, with help from his loyal protector Butler (Nonso\nAnozie), embarks on a dangerous journey into the unknown\nin order to save his father. . .\nWhat does Artemis\nFowl embark on?\na dangerous\njourney into the\nunknown\nSQuAD Luther is honoured on 18 February with a commemoration\nin the Lutheran Calendar of Saints and in the Episcopal\n(United States) Calendar of Saints. In the Church of\nEngland’s Calendar of Saints he is commemorated on 31\nOctober.\nWhen is Luther\ncommemorated in the\nLutheran Calendar of\nSaints?\n18 February\nArchivalQA If it feels like the Heat Miser (”Oh, some like it hot, but I\nlike it really hot”) has been lurking of late, it may be due to\nNBC’s coming remake of the animated 1974 television\nmovie ”The Year Without a Santa Claus.” The four-time\nTony Award winner Harvey Fierstein (”Hairspray”) signed\non this week to replace Chris Elliott in the role of the Heat\nMiser; Mr. Elliot had to bow out because of a scheduling\nconflict. The new version will be seen later this year.\nWho replaced Chris\nElliott as the Heat\nMiser?\nHarvey Fier-\nstein\nTable 4: Example documents, questions, and answers from the test split of each dataset.\n4430\nProxy Model GPT-Neo 1.3B GPT-2 XL\nDistilGPT2 (82M) 0.190 ± 0.017 0 .308 ± 0.018\nGPT-2 Small (117M) 0.176 ± 0.023 0 .309 ± 0.012\nTable 5: StreamingQA F1 Increase comparison for CaMeLS\nmeta-trained using DistilGPT2 (82M) and GPT-2 Small\n(117M) proxy models. Online adaptation of GPT-Neo 1.3B\nand GPT-2 XL is evaluated. In the tested setting, varying the\nproxy model size does not change CaMeLS performance.\nMethod GPT-2 XL GPT-Neo 1.3B\n5-shot ICL 0.1091 0.0533\nICL w/ CaMeLS 0.1594 0.1398\nTable 6: Adapting the base models with CaMeLS consistently\nimproves the F1 scores in a simple in-context learning setting.\naging CaMeLS in conjunction with these methods\non the ArchivalQA dataset. We show that CaMeLS\nis complementary to both in-context learning and\nretrieval; for both methods, the adaptation perfor-\nmance is improved by CaMeLS.\nIn our first set of experiments, we do five-shot\nin-context learning. We assume we can prompt\nthe model with the oracle document containing the\nanswer to the question (i.e., the best-case scenario\nfor in-context learning). The prompt is formatted\nas [ex. doc 1] [ex. q 1] [ex. ans 1]\n. . . [ex. doc 5] [ex. q 5] [ex. ans\n5] [oracle test doc] [test question]. We\nuse the base GPT-2 XL and GPT-Neo 1.3B models\n(QA-tuned models performed much worse with\nin-context learning). As shown in Table 6, we\nfind that adapting the base models with CaMeLS\nconsistently improves the F1 scores of in-context\nlearning.\nIn a second set of experiments, we consider a\nsimple retrieval setup. Results are shown in Ta-\nble 7. We fine-tune GPT-2 XL and GPT-Neo 1.3B\nto answer questions with the source document in\nthe context. We retrieved documents using ran-\ndom, oracle, and BM25 document retrieval. We use\nCaMeLS to update the parameters of the document-\nconditioned question-answering models. Across\nmodels and retrievers, Using CaMeLS to adapt\ndocument-conditioned question-answering models\nconsistently improves adaptation performance over\nvanilla retrieval.\nThese results use the CaMeLS weighting model\ntrained using a QA-proxy model on ArchivalQA.\nWe expect the performance of CaMeLS to increase\nif meta-trained using a proxy model and outer loss\nmore analogous to the evaluation setting. For ex-\nample, increase performance in the retrieval setting,\nwe could present the source document when com-\nputing the outer loss and using a document condi-\ntioned QA proxy model. We acknowledge that we\ndo not evaluate any baseline methods and think that\nextensive comparisons of parametric updating in\nconjunction with these other methods would be an\nexciting direction for future work. As is, these re-\nsults do show that parametric online adaptation can\nbe used to complement document-storage based\nmethods.\nD Optimal Online Adaptation Learning\nRates\nWhen evaluating loss reweighing methods in our\nexperiments, the learning rate used to adapt our\nbase models is found via a learning rate sweep. For\neach combination of dataset, base model, and adap-\ntation method, we test a range of learning rates to\nadapt the base model on a stream of documents\nfrom the validation split of the dataset. The best\nperforming learning rates are used for later exper-\niments on the test split of the dataset. We test the\nfollowing learning rates: [1e-4, 2.5e-5, 6.25e-6,\n1.625e-6]. The optimal learning rates found via\nthese sweeps are shown in Figure 10.\nGPT-2 XL GPT-Neo 1.3B\nMethod Random BM25 Oracle Random BM25 Oracle\nVanilla Retriever 0.0694 0.6812 0.7290 0.0624 0.6898 0.7401\nRetriever w/ CaMeLS 0.1156 0.7106 0.7565 0.1045 0.7356 0.7832\nTable 7: Using CaMeLS to adapt document-conditioned question-answering models consistently improves adaptation perfor-\nmance over vanilla retrieval.\n4431\n(a) StreamingQA\nMethod Base Model Learning Rate\nCaMeLS GPT-2 Large 2.500e-5\nGPT-Neo 1.3B 6.250e-6\nGPT-2 XL 2.500e-5\nGPT-Neo 2.7B 6.250e-6\nGPT-J 6B 6.250e-6\nTF-IDF + 5% Cutoff GPT-2 Large 1.625e-6\nGPT-Neo 1.3B 1.625e-6\nGPT-2 XL 6.250e-6\nGPT-Neo 2.7B 1.625e-6\nGPT-J 6B 1.625e-6\nSalient Spans GPT-2 Large 6.250e-6\nGPT-Neo 1.3B 1.625e-6\nGPT-2 XL 2.500e-5\nGPT-Neo 2.7B 1.625e-6\nGPT-J 6B 6.250e-6\nUniform + QA-tuning GPT-2 Large 1.625e-6\nGPT-Neo 1.3B 2.500e-5\nGPT-2 XL 2.500e-5\nGPT-Neo 2.7B 6.250e-6\nGPT-J 6B 2.500e-5\nUniform GPT-2 Large 1.625e-6\nGPT-Neo 1.3B 1.625e-6\nGPT-2 XL 1.625e-6\nGPT-Neo 2.7B 1.625e-6\nGPT-J 6B 1.625e-6\n(b) SQuAD\nMethod Base Model Learning Rate\nCaMeLS GPT-Neo 1.3B 6.250e-6\nGPT-2 XL 6.250e-6\nSalient Spans GPT-Neo 1.3B 1.625e-6\nGPT-2 XL 1.625e-6\nTF-IDF + 5% Cutoff GPT-Neo 1.3B 1.625e-6\nGPT-2 XL 1.625e-6\nUniform GPT-Neo 1.3B 1.625e-6\nGPT-2 XL 1.625e-6\nUniform + QA-tuning GPT-Neo 1.3B 1.625e-6\nGPT-2 XL 1.625e-6\n(c) ArchivalQA\nMethod Base Model Learning Rate\nCaMeLS GPT-Neo 1.3B 1.625e-6\nGPT-2 XL 6.250e-6\nGPT-J 6B 6.250e-6\nSalient Spans GPT-Neo 1.3B 1.625e-6\nGPT-2 XL 1.625e-6\nGPT-J 6B 6.250e-6\nTF-IDF + 5% Cutoff GPT-Neo 1.3B 1.625e-6\nGPT-2 XL 1.625e-6\nGPT-J 6B 6.250e-6\nUniform GPT-Neo 1.3B 2.500e-5\nGPT-2 XL 6.250e-6\nGPT-J 6B 6.250e-6\nUniform + QA-tuning GPT-Neo 1.3B 2.500e-5\nGPT-2 XL 6.250e-6\nGPT-J 6B 6.250e-6\nFigure 10: Optimal adaptation learning rates used to evaluate each combination of adaptation method and base model.\n4432",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8497278094291687
    },
    {
      "name": "Language model",
      "score": 0.7570180892944336
    },
    {
      "name": "Security token",
      "score": 0.7478402853012085
    },
    {
      "name": "Heuristics",
      "score": 0.6219968795776367
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5862839221954346
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5266528129577637
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5209999680519104
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5123947262763977
    },
    {
      "name": "ENCODE",
      "score": 0.4911479651927948
    },
    {
      "name": "Machine learning",
      "score": 0.4469988942146301
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}