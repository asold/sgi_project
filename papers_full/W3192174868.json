{
  "title": "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing",
  "url": "https://openalex.org/W3192174868",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2358915894",
      "name": "Chen Boyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2628901793",
      "name": "Li Peixia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744175372",
      "name": "Li, Baopu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3196104321",
      "name": "Li, Chuming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163221924",
      "name": "Bai Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096174188",
      "name": "Lin Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1968469221",
      "name": "Sun Ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2180000024",
      "name": "Yan, Junjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750650048",
      "name": "Ouyang, Wanli",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963136578",
    "https://openalex.org/W3129415623",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3034609471",
    "https://openalex.org/W3035377608",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W2964331719",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2767002384",
    "https://openalex.org/W2964444661",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2936599103",
    "https://openalex.org/W3034528588",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W2996407667",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3109403878",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2970790493",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3034869289",
    "https://openalex.org/W2971324138",
    "https://openalex.org/W2905672847",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2963536136",
    "https://openalex.org/W2966284335",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2903260438",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3108420015",
    "https://openalex.org/W2996621846",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W3105281812",
    "https://openalex.org/W3171516518"
  ],
  "abstract": "In this paper, we observe two levels of redundancies when applying vision transformers (ViT) for image recognition. First, fixing the number of tokens through the whole network produces redundant features at the spatial level. Second, the attention maps among different transformer layers are redundant. Based on the observations above, we propose a PSViT: a ViT with token Pooling and attention Sharing to reduce the redundancy, effectively enhancing the feature representation ability, and achieving a better speed-accuracy trade-off. Specifically, in our PSViT, token pooling can be defined as the operation that decreases the number of tokens at the spatial level. Besides, attention sharing will be built between the neighboring transformer layers for reusing the attention maps having a strong correlation among adjacent layers. Then, a compact set of the possible combinations for different token pooling and attention sharing mechanisms are constructed. Based on the proposed compact set, the number of tokens in each layer and the choices of layers sharing attention can be treated as hyper-parameters that are learned from data automatically. Experimental results show that the proposed scheme can achieve up to 6.6% accuracy improvement in ImageNet classification compared with the DeiT.",
  "full_text": "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing\nBoyu Chen1*, Peixia Li1*, Baopu Li2*, Chuming Li3, Lei Bai1, Chen Lin4,\nMing Sun3, Junjie Yan3, Wanli Ouyang1\n1 The University of Sydney, 2 BAIDU USA LLC,\n3 SenseTime Group Limited, 4 University of Oxford\nAbstract\nIn this paper, we observe two levels of redundancies\nwhen applying vision transformers (ViT) for image recog-\nnition. First, ﬁxing the number of tokens through the whole\nnetwork produces redundant features at the spatial level.\nSecond, the attention maps among different transformer\nlayers are redundant. Based on the observations above,\nwe propose a PSViT: a ViT with token Pooling and atten-\ntion Sharing to reduce the redundancy, effectively enhanc-\ning the feature representation ability, and achieving a better\nspeed-accuracy trade-off. Speciﬁcally, in our PSViT, token\npooling can be deﬁned as the operation that decreases the\nnumber of tokens at the spatial level. Besides, attention\nsharing will be built between the neighboring transformer\nlayers for reusing the attention maps having a strong cor-\nrelation among adjacent layers. Then, a compact set of the\npossible combinations for different token pooling and at-\ntention sharing mechanisms are constructed. Based on the\nproposed compact set, the number of tokens in each layer\nand the choices of layers sharing attention can be treated\nas hyper-parameters that are learned from data automati-\ncally. Experimental results show that the proposed scheme\ncan achieve up to 6.6% accuracy improvement in ImageNet\nclassiﬁcation compared with the DeiT in [33].\n1. Introduction\nDeep neural network (DNN) has been the core of the\nrecent popular artiﬁcial intelligence (AI) wave. It greatly\nenhances the state-of-the-art (SOTA) for many tasks in nat-\nural language processing (NLP) and computer vision (CV),\nwhich are two major application ﬁelds of AI. Among them,\ntransformer [34, 9, 19, 29] has found its almost ubiquitous\napplications in the ﬁeld of NLP due to its great advantage\nof long-range capture and parallelism capability compared\nto the previous prevalent recurrent neural network (RNN).\nRecently, transformer is reevaluated by researchers in the\n*Equal contribution\n……(a) KVATTQ KVATTQ KVATTQ KVATTQ\n(c)\nSharing\nKVATTQ ATTV\nSharing\nKVATTQ ATTV\nTOKEN   POOL\n(b) KVATTQ KVATTQ KVATTQ KVATTQ\nTOKEN   POOL\n……\nFigure 1. Token pooling and attention sharing. (a) The original\nViT. (b) Token pooling (TOKEN POOL in the ﬁgure) for ViT,\nwhere the number of tokens (input length) in the next layer can\nbe smaller than the number of tokens in the current layer, and the\ntoken dimension per token can increase. (c) Attention map (ATT\nin the ﬁgure) sharing for ViT, where the attention at the previous\ntransformer layer is copied to the current layer.\nﬁeld of CV and applied to solve some typical problems such\nas image generation [24] and image classiﬁcation [11, 33].\nThe ﬂexible attention mechanism of the transformer in the\nwhole pipeline intrigued the emergence of many follow-\ning works such as object detection [4], image segmenta-\ntion [36], and video instance segmentation [37]. The inspir-\ning achievements of the transformer in the above pioneering\nworks are turning the transformer into a capable alternative\nto the prevalent convolutional neural network (CNN) in CV\nﬁeld.\nIn this paper, we ﬁnd two redundancies in most vision\ntransformers. Our ﬁrst observation is that the ﬁxed num-\nber of tokens observed by the model produces the redun-\ndant features at the spatial level. As shown in traditional\nCNNs [42], deep neural networks tend to encode low-level\ninformation in shallow layers and high-level information\nsuch as semantic features in deep layers. A ﬁxed number\narXiv:2108.03428v1  [cs.CV]  7 Aug 2021\n0.89 0.91\nLayer2\nLayer1 Layer10\nLayer11\n0.91\nLayer7\nLayer6\nFigure 2. Attention maps from 6 different transformer layers. The\nattention maps of layers 1 and 2 have high normalized correlation,\ni.e. 0.89. Similarly for layers 6, 7 and layers 10, 11 with the nor-\nmalized correlation being 0.91.\nof tokens may be an improper design for this purpose, since\nthe ﬁxed number of tokens may not be enough for low-level\ninformation encoding and too redundant for high-level in-\nformation encoding.\nTo better capture the semantic features as well as reduc-\ning the computation, it is a common practice to downsam-\nple feature resolution in CNN as the layers get deeper. As\na counterpart in a transformer, the number of tokens should\nhave more ﬂexibility to adjust itself based on different lev-\nels. To achieve this, we propose the attention pooling mech-\nanism for adapting token numbers at different layers.\nThe second form of redundancy resides in the similar-\nity of the attention maps between adjacent layers. In ViT\nmodels, relations among all tokens are built in the trans-\nformer encoder according to the input feature embeddings.\nHowever, since feature embeddings between the neighbor-\ning layers change smoothly, the learned attention between\nneighboring layers may be similar as Fig. 2 shows. The\nstrong similarity between attention at adjacent layers moti-\nvates us to share the attention among adjacent layers. On the\nother hand, the attention takes 43% computation and 33%\nthe number of parameters in the transformer layer. Shar-\ning attention saves the computation required for obtaining\nattention.\nSpeciﬁcally, the number of tokens in each transformer\nlayer and the setting for attention sharing will affect the\nperformance of transformer. While it is intuitive for tun-\ning token number and attention sharing to be effective, it is\nchallenging to reach an optimal design for vision tasks. In-\nstead of relying on heuristic human design for getting the\noptimal design, we can resort to a more principled way by\nleveraging the recent success of Automated Machine Learn-\ning (AutoML) [18]. Speciﬁcally, we ﬁrst construct a set\nof possible choices, called search space in AutoML, which\nconsists of the possible design combinations for token pool-\ning and attention sharing. For token pooling, we search its\nlocation in the network. Searching the locations of token\npooling is equal to setting the optimal numbers of layers at\ndifferent stages (we refer to the layers with the same number\nof tokens as a stage).\nThe choice of whether a layer shares the attention with\nits neighbour forms a discrete optimization problem. Our\nobjective is to ﬁnd the optimal conﬁguration of attention\nsharing which minimizes the computational cost as well as\nmaximizing the performance. In this work, we apply Neural\nArchitecture Search to solve the conﬁguration.\nWe treat these key factors in the network design as hyper-\nparameters and learn these parameters from data with the\nAutoML method. The two differences between the original\nViT model and our proposed PSViT are shown as Fig. 1.\nOur contributions can be summarized as follows:\n• We present a PSViT scheme with token pooling and\nattention sharing to devise a better vision transformer\nthat can effectively enhance the feature representation\nability and control the computation allocation ﬂexibly\n• We propose a compact set of design choices for to-\nken pooling and attention sharing, based on which the\noptimal design choice is learned by leveraging off-the-\nshelf AutoML methods.\n• Comprehensive experimental results show that the pro-\nposed scheme has up to 6.6% accuracy improvement\nin ImageNet classiﬁcation compared with the DeiT\nin [33].\n2. Related Work\nTransformers for Vision. Transformer [34] was originally\nproposed to overcome the limited attention range and un-\nsuitability of RNN in NLP. It then quickly gained popu-\nlarity in the whole ﬁeld of NLP. In the past few years, a\nnew trend of applying transformer in the ﬁeld of CV is\nwitnessed. And transformer ﬁrst found its application for\nlow-level vision problem such as image generation [24] and\nimage super resolution [41]. Parmar et al. [24] advanced\nimage transformer to generate images through constraining\nthe self-attention mechanism only to local neighborhoods\nand achieved promising results. Yang et al. [41] came up\nwith a new texture transformer network for image super-\nresolution, where the low resolution and reference images\nare formulated as queries and keys in a transformer and ob-\ntained new SOTA for image super resolution.\nLater, transformer was extended to high-level vision\nproblems such as object detection [4, 44], image classiﬁ-\ncation [11, 5] and so on. To overcome the problem of some\n+ + +\n～Position Encoding 1Position Encoding 2Position Encoding 3\nN1 Layers in Stage1\nL L / 4\nN2 Layers in Stage2N3 Layers in Stage3\nGAP&CLS\n～ ～\n×N1\n…\nL×L L / 2\nFeature Embeddings ×N2 ×N3\nFigure 3. Pipeline of our proposed PSViT model. There are three stages in the model, which are separated from the proposed token pooling\nlayers. The token pooling layers help to reduce the spatial size (from L to L/4 in the Figure) and increase the dimension of feature\nembedding. Each stage has several layers (including the layer with independent or sharing attention). The attention maps from different\nlayers in the same stage have the same size. We apply Global Average Pooling (GAP) to the last features and send the generated vector to\na classiﬁer (CLS) for the ﬁnal classiﬁcation.\nhand-designed components such as non-maximum suppres-\nsion and anchor generation in the typical object detection, a\ncomplete end-to-end scheme (DETR) [4] based on trans-\nformer was proposed with impressive results. However,\nDETR still suffers from low convergence. As such, Zhu\net al. [44] introduces Deformable DETR, in which attention\nmodules only consider a small set of key sampling points,\nleading to much faster training speed and better perfor-\nmance especially for small objects. Dosovitskiy et al. [11]\nexplicitly deﬁned the concept of Vision Transformer (ViT),\nand directly applied transformer to image classiﬁcation with\non-par-with or even better performance. Chen et al . [5]\nutilized a sequence transformer to auto-regressively predict\npixels with no need of 2D input structure to learn useful\nrepresentations for images. The notable results reported by\nthem demonstrate the attractive potential of transformers for\nunsupervised representation learning.\nDifferent from all the previous transformer based works\nin the ﬁeld of CV , we concentrate on the token pooling and\nattention sharing that are not touched before. In addition,\nwe apply the AutoML method to achieve this goal, which\nmakes our work distinct from the manual design of these\nprevious works.\nAutomated Machine Learning (AutoML). Designing a\nbetter and efﬁcient DNN is worthwhile for all the tasks in\nCV . Previously, most researchers and engineers in the ﬁeld\nof DNN try to design better models based on heuristics and\nexperiences. Such a process tends to be very tedious. Fortu-\nnately, in the past few years, a novel methodology of design-\ning a better DNN model, that is, AutoML, arises quickly.\nCurrently, the AutoML methods for neural network archi-\ntecture design can be divided into three categories, rein-\nforcement learning (RL) based methods [45, 2, 32], neuro-\nevolutionary based methods [12, 15, 25] and differentiable\nmethods [23, 40, 35].\nRL based methods [45, 2] and neuro-evolutionary meth-\nods [22, 25] are good at searching discrete architectures,\nbut generally requires huge computational cost in search-\ning. To overcome the great computational burdens of RL\nand evolutionary methods during the search, differentiable\nsearch methods [23, 40, 35] relaxed the discrete search\nspace and applied SGD for searching. Then, it received a\nlot of attention due to its great advantage of searching efﬁ-\nciency [35], and it has been widely used in many computer\nvision problems such as segmentation [21, 6] , object de-\ntection [7, 13, 14], model compression [10, 38, 28] and so\non.\nInspired by the outstanding performance of AutoML for\nboth CV and NLP, we also utilize AutoML for better ViT\nmodels with the aim of boosting its performance effectively.\nSpeciﬁcally, the evolution based method of SPOS [15] is\ntaken as our hyper-parameters learning method due to its\nfast efﬁciency of the single path in the supernet searching\nand training. Nevertheless, AutoML has not been investi-\ngated for Transformer in image recognition and our search\nspace of token pooling and attention sharing has not been\nstudied in AutoML for CV .\n3. Method\nFirst, we give a brief introduction about transformer in\nSection 3.1. Then, we present the two essential mechanisms\nof our PSViT, including token pooling and attention sharing,\nin Section 3.2. Finally, we introduce the enhanced PSViT\nwith AutoML method in Section 3.3.\n3.1. Background about Transformer\nTransformer [34] is built with stacked self-attention lay-\ners and feed-forward neural networks.\nSelf-attention Layer. The self-attention layer utilizes the\nrelevance or interaction of one token to other tokens. The\ninput of self-attention layer is transformed into three differ-\nent vectors, i.e. the query vector q, the key vector k and the\nvalue vector v, and their corresponding packing form from\ndifferent inputs are Q, K, V respectively, we can calcu-\nlate their attention among different inputs by the following\nequation:\nAtt(Q, K, V) =Softmax ( Q ·(KT )√dh\n) ·V, (1)\nwhere dh is the number of features per token and ·denotes\nmatrix multiplication. The equation above means that the\nself-attention can be obtained by ﬁrst computing the dot\nproduct of the query with all the keys, i.e. Q ·(KT ). Then,\nthey are normalized with the softmax function to get atten-\ntion scores. Finally, the attention scores are multiplied by\nthe value vector. As such, the output of each token becomes\nthe weighted sum of all the tokens in a sequence/image. To\nboost the performance of vanilla self-attention, multi-head\nattention is further proposed by applying multiple attention\nfunctions to the input.\nVision Transformer. Dosovitskiy et al. [11] proposed ViT\nwhich directly applied the above transformer from NLP to\nthe image classiﬁcation task. They divided an image into\na sequence of ﬂattening 2D tokens that are treated as the\ninputs to transformer. A trainable linear projection was\nadopted for token embeddings with the aim of yielding con-\nstant widths for different layers in a transformer. Then, an-\nother learnable embedding is further applied to the sequence\nof embedding tokens. In addition, 1D positional encoding\nis also attached to the token embeddings to maintain the po-\nsitional information. When ViT is trained on large datasets,\nit can achieve a very impressive classiﬁcation accuracy of\n88.36%.\nBuilding upon this work, Touvron et al. [33] improved\nthe ViT training speed by introducing a novel teacher-\nstudent token distillation strategy speciﬁc to transformers\n(DeiT), yielding competitive results on ImageNet without\nexternal data as in ViT. We choose DeiT as our baseline\nmodel in this work for its great training efﬁciency.\n3.2. Token Pooling and Sharing for Transformer\nBased on the above review and problem analysis for\nthe current ViT, we propose two mechanisms named token\npooling and attention sharing to overcome the above three\nissues for the goal of a better ViT.\n3.2.1 Token Pooling\nThe token pooling mechanism adjusts the number of tokens\nfor each stage as Fig. 3 shows. When the network depth\nincreases, we decrease the number of tokens for removing\nspatial redundancy and increase the feature dimension for\naccommodating more high-level features that should be dif-\nferent from each other. There are two design options for the\ntoken pooling. The ﬁrst one is following the network design\nin [11], treating the image patches as 1D tokens and utiliz-\ning the additional CLS token for the classiﬁcation task. The\nother one is removing the CLS token and keeping the im-\nage patches in a 2D array, which is the same as the pooling\nstrategy in most networks for computer vision tasks, such\nas ResNet [17].\nFor the ﬁrst design, similar to [43], we achieve token\npooling by convolution and maxpooling for the convenience\nof implementation. Different from [43] that only decreases\nthe number of tokens, our goal is to enhance the feature rep-\nresentation ability. Therefore, the feature dimension is not\nchanged in [43] but will be changed in ours. Speciﬁcally,\nwe ﬁrst utilize 1D convolution with a small kernel size to\nchange the feature dimension (i.e., dimension of each token)\nand then decrease the number of tokens via 1D maxpooling.\nWe name our network with the above pooling strategy as\nPSViT-1D. In the second strategy, we adopt a 2D convolu-\ntional layer with stride 2 for token pooling, which is widely\napplied in many convolutional networks. Here, we name\nthe network with the second pooling strategy as PSViT-2D.\nSimply adding a token pooling layer after each layer will\ndecrease the model’s representation ability expeditiously.\nMotivated by the principle of deep networks in CV , such as\nVGGNet, ResNet and MobileNet, we add a pooling layer\nafter several layers and name the layers with the same token\nnumbers as a stage.\nInvestigation on Two Choices of Token dimension. To\nincrease the representation ability of the ViT while main-\ntaining its computational overhead in terms of FLOPS at\nthe same time, we may have two choices to modify the\nnetwork structure: increasing token dimension while de-\ncreasing the token number or keeping the tokens dimen-\nsion and increasing the transformer layer number. Based\non these two choices, we conducted some preliminary ex-\nperiments with the 1D pooling strategy in Table 1, which\nshows that increasing the feature dimensions while decreas-\ning the number of tokens is a better choice. Speciﬁcally, in\nthe experiments, the baseline model ﬁxes the feature size\nwhile the ‘Dimension1’ scheme in Table 1 ﬁxes the token\ndimensions and decreases the token numbers through the\nstages. To balance the total computation, several encoder\nlayers are added. The ‘Dimension2’ scheme in Table 1 in-\ncreases the token dimensions while decreasing the token\nnumbers. These two schemes design the architectures fol-\nlowing the principle that the computation is uniformly dis-\ntributed across the stages [20]. We can ﬁnd in Table 1, these\ntwo token dimension designs perform much better than the\nDeiT-Tiny model which ﬁxes the feature sizes through the\nwhole network. The method decreasing the token num-\nber while increasing the feature dimension (‘Dimension2’)\nTable 1. Preliminary experiments on ImageNet about Token Dimension Settings for Each Stage. ‘Token Dimensions’ denotes the channel\nnumber of tokens for the three stages. Similarly for ‘Token Numbers’ and ‘Layer Numbers’.\nModel DeiT-Tiny Token Dimension1 Token Dimension2\nw/o Pooling no Pooling 2 Poolings 2 Poolings\nToken Dimensions [192, 192, 192] [192, 192, 192] [192, 256, 384]\nToken Numbers [197, 197, 197] [197, 99, 50] [197, 99, 50]\nLayer Numbers [4, 4, 4] [4, 8, 20] [4, 4, 4]\nFLOPS(G) 1.3 1.3 1.3\nTop1 Acc 72.2% 75.0% 76.3 %\nachieves the best performance.\nAnalysis on the Design Choices. In CV tasks, there are\ntwo characteristics. First, as discussed in Section 1, high-\nlevel features have redundancy at spatial level. Therefore,\nit is reasonable to reduce the redundancy at the spatial level\nby token pooling. This is supported by the results in Ta-\nble 1, where two token pooling design performs much bet-\nter than the DeiT-Tiny model without token pooling. Sec-\nond, low-level features could be few but high-level features\nshould be more. Different layers in a deep network en-\ncode information of different levels. Low-level features,\ne.g. edges and textures, at shallow layers can be few and\ncan be shared for representing high-level features. In con-\ntrast, high-level features, e.g. attributes or objects of dif-\nferent viewpoints, at deeper layers are more difﬁcult to be\nshared. Therefore, most of the CNN designs, such as VG-\nGNet [30], ResNet [17] and MobileNet [26], follow the rule\nthat deeper layers have higher feature dimensions. This is\nalso validated by the results in Table 1, where increasing\nthe feature dimension at the deeper layer in ‘Dimension2’\nperforms better than ﬁxing the feature dimension in ‘Di-\nmension1’.\n3.2.2 Attention Sharing\nAs shown in Fig. 2, the attention maps in continuous multi-\nattention layers are similar. As one of the reasons, identity\nmapping is used in the transformer, making a transformer\nlayer function as the residual. In this case, the main features,\nthough changed by the residual, will not drastically change.\nTherefore, the features in adjacent layers will be similar to\neach other, leading to the attention maps among features\nsimilar to each other. This similarity leads to redundancy.\nHence, it is beneﬁcial to apply some sharing mechanisms so\nas to reduce redundancy and, at the same time, decrease the\ncomputation.\nSpeciﬁcally, we achieve the goal of attention sharing\nthrough reusing the attention calculation process between\nadjacent layers as shown in Fig. 1(c). If a layer reuses the\nattention scores from its preceding layer, then we do not\ncalculate the attention scores as well as the Q and K in\nEqn. (1). The attention calculated at the preceding layer is\nused as the input attention of this layer and the whole pro-\ncess of attention calculation in Eqn. (1) is simpliﬁed to the\ndot product between V and input attention scores.\nAttention sharing can help to remove the redundancy of\nthe attention map among adjacent transformer layers. On\nthe other hand, some adjacent layers might have very dif-\nferent features and sharing their attention maps becomes not\nso effective. Taking this into consideration, we should pro-\nvide the ﬂexibility so that the whole ViT still has the choice\nof using the original multi-head attention module without\nsharing attention maps. As such, we take the sharing atten-\ntion module as an optional choice to the original indepen-\ndent multi-head attention module in designing the overall\ntransformer architecture.\n3.3. AutoML Enhanced Transformer\nRecently, AutoML has greatly boosted the SOTA for\nmany models in CV and NLP. In this section, we also turn\nto AutoML for the goal of a better ViT.\n3.3.1 Search Space for PSViT\nAs mentioned before, token pooling refers to the operation\nthat decreases the number of tokens at the spatial level and\nincreases the channel number of features per token, and\nsuch an operation may greatly affect the feature represen-\ntation ability. Hence, we speciﬁcally consider its different\nforms in ViT. The design choices of each stage mainly in-\nclude three factors: the number of tokens Nt, the token di-\nmension Nf , and the number of layers for each stageNb. To\nget an optimal choice for these three factors, we construct\na search space that has multiple choices for them. Each el-\nement in the search space would lead to a new candidate\nnetwork architecture for the transformer.\nIn each layer, there are St possible choices for the num-\nber of tokens, Sf possible choices for the token dimension,\nand Ss possible choices for using the attention maps for\neach map. For L layers, the search space considering only\nthe token pooling would contain (St ·Sf ·Ss)L candidate\ndesigns. For example, when St = 4, Sf = 4, Ss = 4,\nand L = 36, then there are about 1.1 ∗1065 choices, about\n9.6 ∗1052 times the size of SPOS method [15]. In compar-\nison, SPOS has 4 choices in each layer, while this search\nspace has 64 in each layer. This search space is too large\nand has too many choices in each layer. It costs too much\ntime when searching algorithms are used for obtaining the\nbest architecture from this search space. Besides, too many\nchoices in each layer may make the existing fast searching\nalgorithm not effective, as found in [8]. Therefore, we need\nto reduce the search space.\nOur designs of the search space and reasons are as fol-\nlows:\n• According to the analysis in Section 3.2.1, we increase\nthe token dimension but decrease the token numbers\nas the depth increases. This helps to reduce the search\nspace by removing the network architectures that do\nnot ﬁt this rule.\n• Following the mature design of CNN, we constrain\nthat multiple layers within a stage have the same num-\nber of tokens and the same token dimension. To re-\nduce the search space and computation required by the\nsearching algorithm, we only use three stages of token\npooling.\n• For the number of layers at each stage, we only con-\nsider the limited number of layers to further reduce the\nsearch space and computation.\n• We provide ﬂexibility to apply attention sharing or not\nfor different layers.\nOur supernet design for Representing the Search Space.\nIf we consider the number of layers and the setting for at-\ntention map sharing in each stage as hyper-parameters for a\nnetwork, then a natural choice for learning them is the RL\nor EA based approaches, which are very slow in searching.\nTherefore, we employ weight sharing based methods [3, 1]\nas our searching algorithm, which requires deﬁning a su-\npernet to subsume all candidate architectures in the search\nspace. A possible design of supernet is shown in Fig. 4.\nThere are three stages in the supernet. A token pooling\nlayer is placed between stages to change the token num-\nber and features dimension. Each stage has six cells, where\na cell has three choices of paths, a basic transformer layer,\ntwo sharing layers with the attention map of the ﬁrst trans-\nformer layer being copied to the second layer, and identity\nmapping. A candidate architecture could choose one path\nout of three choices for each cell independently. With the\ninclusion of identity mapping, the candidate network can\nhave 0 (all cells choosing the identity layer) to 36 (all cells\nchoosing the sharing layers) transformer layers. Such su-\npernet design provides more feasibility to the transformer\narchitecture. For example, a candidate could only select\nbasic layers for all layers, which is equivalent to the trans-\nformer enhanced by two token pooling layers. As another\nBasicLayer\nSharingLayerIdentitySharingLayerToken Pooling\nBasicLayer\nSharingLayerIdentitySharingLayerToken Pooling\nBasicLayer\nSharingLayerIdentitySharingLayer \u00016\n\u00016\n\u00016\nFigure 4. The architecture of our supernet. There are three stages,\neach stage containing 6 cells. A cell has three choices of paths,\na basic transformer layer, sharing layers, and an identity layer. A\ncandidate network has a single path for each cell.\nexample, a candidate architecture could select the sharing\nlayers path for all the ﬁrst 6 cells at the ﬁrst stage and select\nidentity for the remaining cells, which has 12 transformer\nlayers with every two layers sharing attention map but with-\nout token pooling.\n3.3.2 AutoML for the Searching\nWith the search space and supernet deﬁned in section 3.3.1,\nwe ﬁnd the optimal network architecture for the transformer\nusing the SPOS [15], which is brieﬂy introduced below.\nTraining the supernet. The built supernet in section 3.3.1\nincludes all the candidate networks. For each cell of the su-\npernet, there are multiple choices. By activating only one\nchoice in each cell, a candidate network can be constructed.\nDuring training, SPOS only selects one candidate network\nby uniform sampling for each training iteration and updates\nthe parameters of the selected candidate network in the su-\npernet. The candidate network will inherit the trained pa-\nrameters from the supernet and keep updating these param-\neters. Once the supernet is trained, all the candidate net-\nworks can inherit the weights from it without being trained\nfrom scratch for searching.\nSearching from the trained supernet. After the supernet\nis trained, the best candidate network architecture can be\nobtained by further applying the evolutionary method to all\nthe candidate networks in the supernet. More details could\nbe found in [15].\n4. Experiments\nIn this section, we elaborate on the dataset, implementa-\ntion details, and experimental results to evaluate the perfor-\nmance of the proposed PSViT.\n4.1. Datasets and Implementation Details\nExperimental Dataset. To validate the effectiveness of the\nproposed PSViT, we conduct experiments on image classi-\nﬁcation. We conduct all classiﬁcation experiments on the\nImageNet 1000-class image classiﬁcation dataset, which is\na widely used benchmark for validating network architec-\nture design. This dataset has about 1 million training data.\nPart of the training data is used as the validation data. The\ntraining data without the validation data is used for training\nthe supernet. The validation data is used by the AutoML for\nsearching from the trained supernet.\nImplementation Details. We choose the DeiT [33] men-\ntioned before as our baseline since it is the current SOTA\nfor vision transformer in image classiﬁcation. DeiT has two\nmajor different versions, that is, the Small one and the Tiny\none, for efﬁcient training, which are derived from the basic\nversion of DeiT-B that has 12 transformer layers, a ﬁxed to-\nken dimension of 768, 12 heads, and feature dimension of\neach head dh being 64. The differences between the Small\none and the Tiny one are feature dimension and head num-\nber. More details about the other related details for DeiT can\nbe found in [33]. We follow the framework of DeiT [33]\nto obtain the feature embeddings, which are sent to trans-\nformer encoders for further processing. We implemented\nour model based on PyTorch.\nTraining settings. For the supernet training, mini-\nbatch Nesterov SGD optimizer with a momentum of 0.9 is\nadopted. The initial learning rate is set to 0.2 and adjusted to\n0 gradually by the cosine annealing learning rate decay. We\ntrain the network with a batch size of 1024 and L2 regular-\nization of 1e-4 for 100 epochs. Besides, the label smoothing\nis applied with a 0.1 smooth ratio. For subnet searching, we\nuse the same setting for the evolutionary algorithm as that\nin [15] under the FLOPS constraint. Speciﬁcally, the evolu-\ntionary algorithm samples Ns = 1000subnets with popula-\ntion size 50, max iterations 20, and retaining top-10 models\nduring the evolutionary search. After the optimal subnet is\nfound, we retrain the subnet. For retraining the subnet, we\nutilize the same training setting as that in [33] due to its\nefﬁciency.\n4.2. Classiﬁcation Results\nFor the classiﬁcation task, we compare our two type\nsearched models, PSViT-1D and PSViT-2D with two widely\napplied models, ResNet [17] and ResNext [39], and the\nSOTA pure transformer vision model DeiT [33]. More\nspeciﬁcally, ResNet18 and ResNet 50 are shown in Table 2\nTable 2. Comparison of ImageNet classiﬁcation performance be-\ntween different models. ‘Acc’ denotes the Top-1 accuracy and ‘*’\ndenotes the performance under our training setting, which is the\nsame as that in DeiT [33].\nModel FLOPS(G) Acc(%)\nResNet18 1.8 69.8\nResNet18∗ 1.8 68.5\nDeiT-Tiny 1.3 72.2\nPSViT-1D-Tiny 1.4 77.4\nPSViT-2D-Tiny 1.3 78.8\nResNet50 4.1 76.1\nResNet50∗ 4.1 78.5\nX50-32x4d 4.3 77.6\nX50-32x4d∗ 4.3 79.1\nDeiT-Small 4.6 79.6\nPSViT-1D-Small 4.9 80.7\nPSViT-2D-Small 4.4 81.6\nX101-64x4d 15.6 79.6\nX101-64x4d∗ 15.6 81.5\nViT-Base 17.6 77.9\nDeiT-Base 17.6 81.8\nPSViT-1D-Base 18.9 82.6\nPSViT-2D-Base 15.5 82.9\nsince they are provided in [33] and they have similar FLOPs\nas DeiT-Tiny and DeiT-Small respectively, as pointed out\nin [33]. Moreover, ResNetXt50-32x4d and ResNetXt101-\n64x4d that improved ResNet by aggregating the residual\nmodule are also included for comparison due to their out-\nstanding performance in image classiﬁcations. For a fair\ncomparison, the above corresponding models trained on our\nmachine using our training settings (the same as DeiT) are\nfurther represented with a symbol of * in the table. The re-\nsults for ViT-Base in Table 2 is copied from [11]. Note that\nusing the training setting of DeiT for both ViT and DeiT\nwill make ViT and DeiT having the same accuracy because\nthe only difference between DeiT and ViT is the training\nsetting.\nTable 2 shows the experimental results comparing our\nsearched models with other methods under different com-\nputation budgets. We can ﬁnd that under similar FLOPS,\nour models achieve better classiﬁcation accuracy. For ex-\nample, the accuracy of our searched 1D/2D model under\n1.3 GFLOPS is more than 5% / 6% higher (we use abso-\nlute accuracy improvement instead of relative improvement\nin this paper) than DeiT-Tiny with similar FLOPS and more\nthan 7% / 8% higher than the ResNet18, whose FLOPS is\n1.4 times of ours. Such huge improvements in terms of bet-\nter classiﬁcation accuracy and fewer FLOPS make the pro-\nposed PSViT a more attractive and powerful competitor to\nthe recent ViT and DeiT.\nFor ResNet50, ResNetXt, and DeiT-Small, our corre-\nsponding improved model obtained by the PSViT-1D also\noutperforms them respectively by a signiﬁcant margin of\n2.2%, 1.6%, and 1.1%. The PSViT-2D is 0.9% better\nthan the PSViT-1D with fewer FLOPS. While for the large\nmodel comparisons among ResNetXt-101, DeiT-Base, and\nour base one, our respective classiﬁcation gain is more than\n1% for the PSViT-2D-Base model.\nAll the above three results for image classiﬁcation show\nthe effectiveness of the proposed PSViT scheme. We at-\ntribute the gain of our searched models to the two key parts\nadvanced in this work: the proposed token pooling mod-\nule that can more effectively capture different levels of im-\nage features, and the attention sharing mechanism that ﬂex-\nibly adapts the computational cost for different transformer\nlayers, achieving a better trade-off between accuracy and\nmodel redundancy.\n4.3. Ablation Study\nTo have a better understanding of the proposed PSViT,\nwe carry out some ablation studies in this section, which are\ndetailed as follows. All experimental results are conducted\non ImageNet using the same training setting mentioned in\nSection 4.1.\nEffectiveness of Token Pooling, Attention Sharing, and\nAutoML. First of all, we evaluate our proposed mechanism\nthrough simple manually designed models as in Table 3.\nWe ﬁrst devise the ViT model with attention sharing be-\ntween two adjacent layers. To balance the computation,\nwe add additional layers at the end of the original model\nto reach similar FLOPS. The attention sharing improves the\naccuracy by 1.6%. Then, we evaluate the proposed token\npooling with the setting of ‘Token Dimension2’ described\nin Table 1. Token pooling improves the performance of\nthe model signiﬁcantly, that is, from 72.2% to 76.3% for\nPSViT-1D and to 76.7% for PSViT-2D.\nMoreover, we study the effectiveness of AutoML. We\nutilize SPOS method to search for an optimal architecture\nunder the FLOPS constraint, considering both token pool-\ning and attention sharing. The supernet for the SPOS is\nshown in Fig. 4. As shown in Table 3, there is obvi-\nous performance improvement of 1.1%/2.1% for PSViT-\n1D/PSViT-2D, which demonstrates the efﬁcacy of the uti-\nlized AutoML method of SPOS.\nToken Number Settings. As we mentioned before, the\nhyper-parameters about attention pooling have multiple\nchoices. We evaluate the different settings as in Table 4.\nSince we follow the principle of increasing token dimen-\nsions while decreasing token numbers, the only hyper-\nparameters should be the initial/ﬁnal token numbers. We\nevaluate two types of token numbers based on PSViT-1D,\ncorresponding to the way that input images are split. For\na model with input token having size 16 ×16 (models\nwith ‘/16’), the initial/ﬁnal tokens number is 197/50 while\nthe model with input token size being 8 ×8 (models with\n‘/8’) has 785/197 initial/ﬁnal tokens. The model Tiny/16\nis our searched model as mentioned and other models are\nobtained through scaling up (Small/16) or tuning the initial\ntoken numbers (Tiny/8, Small/16). For the Tiny models,\nthe model with fewer token numbers performs much bet-\nter. The Tiny model with more tokens has too many tokens\nwith lower feature dimension in the ﬁnal stage which leads\nto poor representation ability. Different from Tiny model,\nthe Small model with more tokens achieves better perfor-\nmance. The reason is that the token dimensions are enough\nfor the small model and the performance of the small model\nwith few tokens drops due to the possible overﬁtting caused\nby the superﬂuous parameters.\nAttention Sharing Settings. We further study different at-\ntention sharing paradigms. And the experimental results\nbased on 1D pooling are shown in Table 5. In the table,\n‘sharing 2’ means every 2 adjacent layers share the same\nattention. ‘sharing 3’ denotes every 3 adjacent layers share\nthe same attention, i.e. more layers sharing the same atten-\ntion. We can clearly see that applying sharing can yield\nobvious improvements in classiﬁcation accuracy compared\nto the vanilla ViT that does not share attention (‘no sharing’\nin Table 5), with a promising margin of 1.6%, and 0.9%\nrespectively for sharing in 2 layers and 3 layers. Sharing\nbetween 2 layers achieves better performance than sharing\namong 3 because features will change more when going\nthrough 3 layers and the attention may be different in 3 lay-\ners. For a better performance, we select sharing within 2\nlayers as our network design element in the supernet.\n4.4. Visualization\nVisualization. In Fig. 5, we show the visualization of\nlearned features from DeiT (the 2nd and 5th columns) and\nour PSViT (the 3rd and 6th columns). We utilize the Grad-\nCAM in [27] to show where is ‘important’ for predictions.\nWe can ﬁnd that our PSViT can focus on a more complete\nregion-of-interests than DeiT. In addition, our model has\nlarger feature dimensions which can capture more informa-\ntion than the baseline model, leading to possible better clas-\nsiﬁcation or recognition results.\n4.5. Searched Architectures\nFig. 6 shows the searched architectures of PSViT-1D and\nPSViT-2D. There are about 50% layers sharing the atten-\ntion, which shows that the proposed attention sharing mech-\nanism can achieve a great trade-off between accuracy and\ncomputation costs. For the computation allocation among\nthe three stages, both models allocate less computation in\nthe ﬁrst stage and more computation in the last stage. The\nfeatures in the ﬁrst stage have a larger spatial size. Applying\nself-attention to these features leads to a heavy computation\nburden. We think that is why the ﬁrst stage tends to adopt\nTable 3. ImageNet top-1 classiﬁcation accuracy for models with different mechanisms, i.e. attention sharing, token pooling, and using\nAutoML or manual design for these two factors.\nAttention Token AutoML FLOPS acc-1D acc-2D\nSharing Pooling (G) % %\n1.3 72.2 72.2\n✓ 1.3 73.8 -\n✓ 1.3 76.3 76.7\n✓ ✓ ✓ 1.3 77.4 78.8\nTable 4. Token Number Setting based on PSViT-1D. Tiny/8 and Tiny/16 have differences in the input token numbers. To ensure Tiny/8\nand Tiny/16 have similar FLOPs, token dimension and number of heads in the multi-head attention are adjusted correspondingly. Similarly\nfor Small/8 and Small/16. ‘Token Dimensions’ denotes numbers of features per token for the three stages, similarly for ‘Token numbers’.\n‘Num. heads’ denotes the number of heads in the multi-head attention for the three stages.\nModel Tiny/8 Tiny/16 Small/8 Small/16\nToken Dimensions [64, 144, 192] [192, 288, 384] [144, 256, 384] [288, 512, 768]\nNum. heads [1, 3, 3] [3, 6, 6] [3, 4, 6] [6, 8, 12]\nToken Numbers [785, 393, 197] [197, 99, 50] [785, 393, 197] [197, 99, 50]\nParams(M) 3.8 15.6 13.9 53.8\nFLOPS(G) 1.5 1.3 4.9 4.0\nTop1 Acc 72.71% 77.40% 80.70 % 78.32 %\nTable 5. ImageNet top-1 classiﬁcation accuracy for different shar-\ning settings. ‘no sharing’ denotes the network without sharing at-\ntention. ‘sharing 2’ and ‘sharing 3’ respectively denote every 2\nand 3 adjacent transformer layers share the same attention map.\nmodel FLOPS (G) acc-1D (%)\nno sharing 1.3 72.2\nsharing 2 1.3 73.8\nsharing 3 1.3 73.1\nattention sharing more times or has fewer total layers. It is\na good strategy to reduce unnecessary computation. Differ-\nently, applying self-attention on deeper layers which have\nless tokens is better. The saved computation allows us to in-\ncrease the feature dimension or layer number properly for a\nstronger feature representation. For the last layer of the two\nmodels, there is no attention sharing. The last layer deserves\nan independent self-attention operation because its output is\ndirectly used for the ﬁnal classiﬁcation. More computation\non the last layer will produce more accurate classiﬁcation\nresults.\n4.6. Comparison between PSViT-1D and PSViT-2D\nOur PSViT-1D and PSViT-2D have different searched ar-\nchitectures as shown in Fig. 6. For PSViT-1D, we set the\noutput token number as 50 (with the CLS token). Similarly,\nthe spatial size of the output feature in PSViT-2D is 7 ×7.\nBoth models have token pooling layers with a stride of 2,\nwhich means token number will reduce 50% in PSViT-1D\nand 75% for the PSViT-2D. In PSViT-1D, the token pool-\ning is applied to 1D features. The token number is 197 for\nthe ﬁrst stage and 99 for the second stage. For PSViT-2D,\nthe token number of the ﬁrst stage and the second stage are\n28×28 and 14×14, respectively. As the Table 2 shows, our\nPSViT-2D models achieve better performance than PSViT-\n1D models. We think the main reason is that 2D token\npooling can keep the structure information of 2D images\nwhile the 1D token pooling only considers the neighbour-\ning tokens on the x-axis but ignores those on the y-axis.\nBesides, 2D features from PSViT-2D is more suitable for\ndownstream tasks.\n4.7. Object detection and instance segmentation re-\nsults\nTo evaluate the transferability of our models, we contrast\nour PSViT-2D model to ResNet on the downstream tasks,\ni.e. object detection and instance segmentation. We apply\nthe Mask-RCNN-FPN [16] as our framework. We conduct\nexperiments on MSCOCO 2017 dataset, which consists of\n118K training images and 5K validation images. We report\nthe performance on the validation set.\nFor the network training, we ﬁrst initialize the backbone\nparameters with our pre-trained weights on ImageNet and\nadopt Xavier initialization on other layers in the neck and\nheads. Our models are trained with the batch size of 16 on\n8 V100 GPUs and optimized by AdamW with the initial\nlearning rate of 0.0001, 0.05 weight decay. We follow the\nstandard common 1x training setting (12 epochs) to train\nthe whole Mask R-CNN models with our searched models\nand ResNet as the backbone. During the training stage, we\nfollow the multi-scale training setting in [31]: resizing the\nshorter side of input between 480 and 800 while the longer\nOrigin DeiT PSViTOrigin DeiT PSViT\nFigure 5. Visualization of features for DeiT and our PSViT. Images in the 1st and 4th columns are from ImageNet.\nBasicLayerSharingLayerIdentitySharingLayerToken Pooling\nBasicLayerSharingLayerIdentitySharingLayerToken Pooling\nBasicLayerSharingLayerIdentitySharingLayer\u00016\n\u00016\n\u00016\nBasic Layer\nBasic Layer\nSharing LayerSharing Layer\nSharing LayerSharing Layer\nBasic Layer\nSharing LayerSharing Layer\nSharing LayerSharing LayerBasic Layer\nSharing LayerSharing LayerBasic LayerBasic LayerBasic Layer\nBasic LayerBasic Layer\nSharing LayerSharing LayerBasic LayerBasic Layer\nBasic LayerBasic LayerSharing LayerSharing LayerSharing LayerSharing LayerBasic Layer\n197×192\n99 ×256\n50 ×384\n50 ×384\n28 ×28 ×64\n14 ×14 ×192\n7 ×7 ×384\n7 ×7 ×384\nToken Pooling\nToken Pooling\nToken Pooling\nToken Pooling(a) PSViT-1D-Tiny\n(b) PSViT-2D-Tiny\nFigure 6. Searched architectures of PSViT-1D-Tiny and PSViT-2D-Tiny. The feature size does not change in the same stage.\nTable 6. Performance comparison between different backbones on the object detection and instance segmentation tasks.\nBackbone Object Detection Instance Segmentation\nAP AP50 AP75 APs APm APl AP AP50 AP75 APs APm APl\nResNet18 34.8 56.3 37.5 20.1 37.0 45.2 32.8 53.4 34.8 17.4 25.1 44.7\nPSViT-2D-Tiny 40.8 64.7 44.0 25.3 43.8 53.9 37.7 60.1 39.9 21.2 40.6 52.8\nResNet50 38.3 60.4 41.4 23.3 41.6 48.9 35.5 57.2 37.8 19.6 38.6 47.7\nResNet101 39.5 61.3 43.0 23.4 42.7 51.1 36.5 58.2 39.1 19.6 39.7 49.4\nside is at most 1333. During testing, the shorter side of the\ninput is ﬁxed as 800 pixels.\nThe results are shown in Table 6. Our PSViT-2D-\nTiny achieves 40.8% mAP on the object detection task and\n37.7% mAP on the instance segmentation task, which is 6\nand 4.9 points higher than that of ResNet18, respectively.\nOur PSViT-2D-Tiny even achieves better performance than\nResNet101. The much better performance than ResNets on\nboth object detection and instance segmentation shows the\ngreat transferability of our models.\n5. Conclusions\nIn this paper, we have introduced a new PSViT by mak-\ning use of token pooling and attention sharing. To effec-\ntively enhance the performance of current ViT, token pool-\ning and attention sharing are ﬁrst designed. Then, the search\nspace for a PSViT stage is designed. Based on the search\nspace, we further apply an efﬁcient AutoML method, SPOS,\nto obtain the optimal module design. Extensive comparison\nexperiments on the ImageNet image classiﬁcation dataset\nshow the effectiveness of the proposed PSViT since it can\nincrease the classiﬁcation accuracy a lot. The great perfor-\nmance on object detection and instance segmentation also\nvalidates the transferability of our models.\nReferences\n[1] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay\nVasudevan, and Quoc Le. Understanding and simplifying\none-shot architecture search. In International Conference on\nMachine Learning, pages 550–559, 2018.\n[2] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun\nWang. Efﬁcient architecture search by network transforma-\ntion. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 32, 2018.\n[3] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\nneural architecture search on target task and hardware.arXiv\npreprint arXiv:1812.00332, 2018.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229, 2020.\n[5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In International Conference on Ma-\nchine Learning, pages 1691–1703, 2020.\n[6] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang,\nYuan Li, and Zhangyang Wang. Fasterseg: Searching\nfor faster real-time semantic segmentation. arXiv preprint\narXiv:1912.10917, 2019.\n[7] Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng,\nXinyu Xiao, and Jian Sun. Detnas: Backbone search for\nobject detection. Advances in Neural Information Processing\nSystems, 32:6642–6652, 2019.\n[8] Yuanzheng Ci, Chen Lin, Ming Sun, Boyu Chen, Hongwen\nZhang, and Wanli Ouyang. Evolving search space for neural\narchitecture search. arXiv preprint arXiv:2011.10904, 2020.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[10] Xuanyi Dong and Yi Yang. Network pruning via trans-\nformable architecture search. In The Conference on Neural\nInformation Processing Systems (NeurIPS), pages 760–771,\n2019.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[12] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Ef-\nﬁcient multi-objective neural architecture search via lamar-\nckian evolution. arXiv preprint arXiv:1804.09081, 2018.\n[13] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:\nLearning scalable feature pyramid architecture for object\ndetection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 7036–\n7045, 2019.\n[14] Jianyuan Guo, Kai Han, Yunhe Wang, Chao Zhang, Zhaohui\nYang, Han Wu, Xinghao Chen, and Chang Xu. Hit-detector:\nHierarchical trinity architecture search for object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11405–11414, 2020.\n[15] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\nZechun Liu, Yichen Wei, and Jian Sun. Single path one-\nshot neural architecture search with uniform sampling. In\nEuropean Conference on Computer Vision, pages 544–560,\n2020.\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961–2969, 2017.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A\nsurvey of the state-of-the-art. Knowledge-Based Systems ,\n212:106622, 2021.\n[19] Xian Li, Asa Cooper Stickland, Yuqing Tang, and Xiang\nKong. Deep transformers with latent depth. In H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,\nAdvances in Neural Information Processing Systems , vol-\nume 33, pages 1736–1746. Curran Associates, Inc., 2020.\n[20] Feng Liang, Chen Lin, Ronghao Guo, Ming Sun, Wei Wu,\nJunjie Yan, and Wanli Ouyang. Computation reallocation for\nobject detection. In International Conference on Learning\nRepresentations, 2020.\n[21] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig\nAdam, Wei Hua, Alan L Yuille, and Li Fei-Fei. Auto-\ndeeplab: Hierarchical neural architecture search for semantic\nimage segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n82–92, 2019.\n[22] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha\nFernando, and Koray Kavukcuoglu. Hierarchical repre-\nsentations for efﬁcient architecture search. arXiv preprint\narXiv:1711.00436, 2017.\n[23] Hanxiao Liu, Karen Simonyan, and Yiming Yang.\nDarts: Differentiable architecture search. arXiv preprint\narXiv:1806.09055, 2018.\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International Conference on Machine\nLearning, pages 4055–4064, 2018.\n[25] Esteban Real, Chen Liang, David So, and Quoc Le. Automl-\nzero: Evolving machine learning algorithms from scratch.\nIn International Conference on Machine Learning , pages\n8007–8019, 2020.\n[26] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510–4520, 2018.\n[27] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-\ntra. Grad-cam: Visual explanations from deep networks via\ngradient-based localization. In ICCV 2017, Venice, Italy, Oc-\ntober 22-29, 2017, pages 618–626, 2017.\n[28] Guo Shaopeng, Wang Yujie, Li Quanquan, and Junjie Yan.\nDmcp: Differentiable markov channel pruning for neural\nnetworks. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2020.\n[29] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang,\nZhenguo Li, and James T Kwok. Sparsebert: Rethinking\nthe importance analysis in self-attention. arXiv preprint\narXiv:2102.12871, 2021.\n[30] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[31] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-\nfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan\nYuan, Changhu Wang, et al. Sparse r-cnn: End-to-end ob-\nject detection with learnable proposals. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14454–14463, 2021.\n[32] Yuan Tian, Qin Wang, Zhiwu Huang, Wen Li, Dengxin Dai,\nMinghao Yang, Jun Wang, and Olga Fink. Off-policy rein-\nforcement learning for efﬁcient and effective gan architecture\nsearch. In European Conference on Computer Vision, pages\n175–192, 2020.\n[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International Conference on Machine Learning ,\npages 10347–10357, 2021.\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017.\n[35] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuan-\ndong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu,\nKan Chen, et al. Fbnetv2: Differentiable neural architecture\nsearch for spatial and channel dimensions. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12965–12974, 2020.\n[36] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic\nsegmentation with mask transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5463–5474, 2021.\n[37] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8741–8750, 2021.\n[38] Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian,\nPeter Vajda, and Kurt Keutzer. Mixed precision quantiza-\ntion of convnets via differentiable neural architecture search.\narXiv preprint arXiv:1812.00090, 2018.\n[39] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017.\n[40] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.\nSnas: stochastic neural architecture search. arXiv preprint\narXiv:1812.09926, 2018.\n[41] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n5791–5800, 2020.\n[42] Matthew D Zeiler and Rob Fergus. Visualizing and under-\nstanding convolutional networks. InEuropean conference on\ncomputer vision, pages 818–833, 2014.\n[43] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang,\nJianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond\nefﬁcient transformer for long sequence time-series forecast-\ning. In Proceedings of AAAI, 2021.\n[44] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020.\n[45] Barret Zoph and Quoc V Le. Neural architecture search with\nreinforcement learning. arXiv preprint arXiv:1611.01578 ,\n2016.",
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.7395263910293579
    },
    {
      "name": "Security token",
      "score": 0.7133838534355164
    },
    {
      "name": "Transformer",
      "score": 0.46595579385757446
    },
    {
      "name": "Computer science",
      "score": 0.4342612028121948
    },
    {
      "name": "Psychology",
      "score": 0.417251318693161
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2799590229988098
    },
    {
      "name": "Computer security",
      "score": 0.2093408703804016
    },
    {
      "name": "Electrical engineering",
      "score": 0.12420395016670227
    },
    {
      "name": "Engineering",
      "score": 0.1056441068649292
    },
    {
      "name": "Voltage",
      "score": 0.06226503849029541
    }
  ],
  "institutions": [],
  "cited_by": 20
}