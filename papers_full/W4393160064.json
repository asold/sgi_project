{
    "title": "Multi-Modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models",
    "url": "https://openalex.org/W4393160064",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5104264415",
            "name": "Liqi He",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5002584135",
            "name": "Zuchao Li",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5003753939",
            "name": "Xiantao Cai",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5101538675",
            "name": "Wang Ping",
            "affiliations": [
                "Wuhan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778485988",
        "https://openalex.org/W2345720230",
        "https://openalex.org/W3020908159",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W6640963894",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W4307076412",
        "https://openalex.org/W6810484225",
        "https://openalex.org/W4296605665",
        "https://openalex.org/W4304731100",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4226317937",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2995558462",
        "https://openalex.org/W4313591130",
        "https://openalex.org/W4319165821",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W4385573026",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W3176395632",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4286894277",
        "https://openalex.org/W4285241608",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2560674852"
    ],
    "abstract": "Chain-of-thought (CoT) reasoning has exhibited impressive performance in language models for solving complex tasks and answering questions. However, many real-world questions require multi-modal information, such as text and images. Previous research on multi-modal CoT has primarily focused on extracting fixed image features from off-the-shelf vision models and then fusing them with text using attention mechanisms. This approach has limitations because these vision models were not designed for complex reasoning tasks and do not align well with language thoughts. To overcome this limitation, we introduce a novel approach for multi-modal CoT reasoning that utilizes latent space learning via diffusion processes to generate effective image features that align with language thoughts. Our method fuses image features and text representations at a deep level and improves the complex reasoning ability of multi-modal CoT. We demonstrate the efficacy of our proposed method on multi-modal ScienceQA and machine translation benchmarks, achieving state-of-the-art performance on ScienceQA. Overall, our approach offers a more robust and effective solution for multi-modal reasoning in language models, enhancing their ability to tackle complex real-world problems.",
    "full_text": "Multi-Modal Latent Space Learning for Chain-of-Thought Reasoning in\nLanguage Models\nLiqi He1,†, Zuchao Li1,†*, Xiantao Cai1, Ping Wang2,3\n1National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan,\n430072, China\n2Center for the Studies of Information Resources, Wuhan University, Wuhan 430072, China\n3School of Information Management, Wuhan University, Wuhan 430072, China\nheliqi@whu.edu.cn, zcli-charlie@whu.edu.cn, caixiantao@whu.edu.cn, wangping@whu.edu.cn\nAbstract\nChain-of-thought (CoT) reasoning has exhibited impressive\nperformance in language models for solving complex tasks\nand answering questions. However, many real-world ques-\ntions require multi-modal information, such as text and im-\nages. Previous research on multi-modal CoT has primarily\nfocused on extracting fixed image features from off-the-shelf\nvision models and then fusing them with text using attention\nmechanisms. This approach has limitations because these vi-\nsion models were not designed for complex reasoning tasks\nand do not align well with language thoughts. To overcome\nthis limitation, we introduce a novel approach for multi-\nmodal CoT reasoning that utilizes latent space learning via\ndiffusion processes to generate effective image features that\nalign with language thoughts. Our method fuses image fea-\ntures and text representations at a deep level and improves the\ncomplex reasoning ability of multi-modal CoT. We demon-\nstrate the efficacy of our proposed method on multi-modal\nScienceQA and machine translation benchmarks, achieving\nstate-of-the-art performance on ScienceQA. Overall, our ap-\nproach offers a more robust and effective solution for multi-\nmodal reasoning in language models, enhancing their ability\nto tackle complex real-world problems.\nIntroduction\nIn our daily lives, we are constantly bombarded with infor-\nmation from various sources, such as text, images, and more.\nTo make sense of this complex world, we need to be able to\nacquire and integrate multi-modal information effectively.\nFor example, as shown in Figure 1, when we see the slo-\ngan ”Please keep off the grass” (language modality) on the\nlawn of the park and a child is playing football on the same\n* Corresponding author. † Equal contribution. This work was\nsupported by the National Natural Science Foundation of China\n(No. 62306216), the Natural Science Foundation of Hubei Province\nof China (No. 2023AFB816), the Fundamental Research Funds for\nthe Central Universities (No. 2042023kf0133), National Natural\nScience Foundation of China [No. 72074171] [No. 72374161].\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n playground to   \nwill affect the \nThis behavior \ngrowth of the \nlawn, and the child \nshould go to the\nplay football.\nFigure 1: Language Thought\nlawn (visual modality) , we think of the negative effects of\ntrampling the grass on the park’s ecological environment and\nprepare to told the child to go to the football field (language\nthought) . These ideas come from our deep understanding\nand reasoning of linguistic and visual information, which\ncan be called language thought.\nIn recent years, chain-of-thought (CoT), which involves\na series of intermediate reasoning steps (also known as ra-\ntionale), has significantly enhanced the complex reasoning\nability of large language models by providing them with ac-\ncess to a portion of language thought (Wei et al. 2022).\nTraining smaller language-only models with less than 100\nbillion parameters for CoT reasoning remains a significant\nchallenge due to hallucination and tend to produce illogi-\ncal rationales. To address these problems more effectively,\nit is crucial to enable large language models to develop a\ndeeper understanding of multi-modal information and gen-\nerate more effective language thought. One solution that\nhas been proposed to help integrate information across vi-\nsual and linguistic modalities is Multi-Modal CoT (MM-\nCoT) (Zhang et al. 2023b). MM-CoT extracts fixed image\nfeatures and text representations and fused them to obtain\nmulti-modal features. MM-CoT adopts a two-stage frame-\nwork that includes rationale generation and answer infer-\nence, as shown in Figure 2 (c). This approach has been\nshown to outperform generating rationale and answer to-\ngether on question answering tasks (Zhang et al. 2023b).\nHowever, existing multi-modal CoT models rely on fixed\nimage features extracted by pre-trained vision extraction\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18180\nInput\nQ: A juggler can juggle 16 balls. \nHalf of the balls  are golf balls, \nadn half of the golf balls are blue.  \nHow many blue golf balls are \nthere?\nLet’s think step by step.\nInput\nQ: Roger has 5 tennis balls. He buys 2 \nmore cans of tennis  balls. Each can has 3 \ntennis balls. How many tennis balls  does \nhe have now?\nA: Roger started with 5 balls. 2 cans of 3 \ntennis balls each is 6 tennis balls. 5 + 6 = \n11. The answer is 11.\nQ: A juggler can juggle 16 balls. Half of \nthe balls are golf  balls, and half of the \ngolf balls are blue. How many blue  golf \nballs are there?\nOutput\nThe  juggler  can  juggle  16 balls. \nHalf of the balls are golf balls. So \nhere are 16 / 2 = 8 golf balls. Half \nof the golf balls are blue. So there \nare 8 / 2 = 4 blue golf balls. The \nanswer is 4. ✓\nOutput\nThere are 16 balls in total. Half of the \nballs are golf balls. There are 8 golf balls. \nHalf  of  the golf balls are blue.  The \nanswer is 4. ✓\n(a) Zero-shot-CoT (b) Few-shot-CoT\nVision\nLanguage\nWhich property do \nthese three  objects \nhave in common?\nContext: Select the \nbest answer.\n(A) shiny  (B) slippery \n(C)opaque\nFor each object, decide if it has that \nproperty. A opaque object does not let \nlight through. All three objects are \nopaque. A slippery  object is hard to \nhold onto or stand on. The tortoise \nshell and the crown are not  slippery. A \nshiny object reflects a lot of  light. You \ncan usually see your reflection  in a \nshiny object.  The basketball is not \nshiny. The property that all  three \nobjects have in common is opaque.\nRationale\nThe \nanswer \nis (C).\nAnswer\n(c) Multi-modal-CoT\nFigure 2: (a) Zero-shot-CoT (Kojima et al. 2022) (b) Few-\nshot-CoT (Wei et al. 2022) (c) Multi-modal-CoT (Zhang\net al. 2023b)\nmodels such as DETR (Carion et al. 2020) or CLIP (Rad-\nford et al. 2021). However, fixed image features do not align\nwell with flexible text queries. And vision models that ex-\ntract these features are not optimized for producing useful\nvisual information that would lead to effective rationales\ngenerated by language models. For example, while DETR\ndetects objects, its extracted features may only pay attention\nto the main objects in an image. Additionally, while CLIP is\ntrained on (image, text) pairs, it only extracts shallow image\ninformation. Shallow vision features may not help language\nmodels infer correct answers because they are not closed to\nthe reasoning. For example, as shown in Figure 1, we can\nnot synthetize language thought if we look at the lawn in the\npicture and the text on the banner separately. We hypothesize\nthat in both the stage of rationale generation and the stage of\nanswer inference for complex problem solving, there is a\nneed for deep understanding of visual features that capture\ndifferent information in images. Therefore, effectively uti-\nlizing different modalities remains a key challenge. In this\nwork, we propose an approach to enhance the complex rea-\nsoning ability of large language models by improving their\nability to synthesize and employ language thoughts. Our ap-\nproach leverages both language and vision information to\nachieve this goal. We propose to obtain a multi-modal latent\nspace that deeply fuses visual features and text representa-\ntions via a diffusion process. This allows our method to de-\nvelop deep-level understanding, alignment and reasoning of\nboth visual and linguistic modalities, resulting in more ef-\nfective language thought generation.\nDrawing inspiration from diffusion models , we employ\nthe diffusion process to learn a text-image aligned latent\nspace for language thought reasoning. The diffusion process\nentails the sequential application of multiple transformations\nto the latent space of image representation, where the level\nof noise is gradually augmented with each iteration. As a re-\nsult, a series of increasingly blurred representations of the\noriginal image input is generated, ultimately leading to ran-\ndom noise that follows a Gaussian distribution. During each\nstage of the noise prediction, the model acquires a novel rep-\nresentation of the joint text-image distribution that captures\nmore intricate dependencies and higher-level semantics. By\nrepeating this procedure across several iterations, the model\ncan acquire a deep and well-aligned latent space that en-\ncodes abundant information about both modalities. This ap-\nproach is particularly useful for CoT reasoning tasks, where\nthe goal is to reason about a long sequence of inputs and\ntheir corresponding image. By learning a deep latent space\nthat captures high-level dependencies between text and im-\nages, it is well-suited for complex reasoning tasks.\nWe conducted experiments on the ScienceQA benchmark,\nwhich contains questions that require reasoning based on\nprovided text and images. The results show that our pro-\nposed latent space learning is effective in generating use-\nful chain of thought (CoT) and inferring correct answers.\nWe achieved new state-of-the-art results on the ScienceQA\nbenchmark with about only 1 billion parameters, outper-\nforming the current SOTA baseline by 6.06% (base), 1.67%\n(large) respectively, and the strong ChatGPT system by\n18.18% with less than 1/100th of the parameters, demon-\nstrating the effectiveness of our approach. Our method also\ndemonstrates strong ability in generating effective CoT, as\nevidenced by the ROUGE-L score of the rationales outper-\nforming the baseline by 1.21. In addition to ScienceQA, we\nevaluated the effects of diffusion process for multi-modal\nlatent space learning in multi-modal machine translation,\nwhere it also brought significant improvements. These re-\nsults suggest that our proposed method is a general enhance-\nment and can benefit the multi-modal information process-\ning community. The code will be released at https://github.\ncom/shimurenhlq/DPMM-COT.\nRelated Work\nCoT Reasoning in LLMs\nCoT is a widely applicable method for enhancing com-\nplex reasoning in large language models (LLMs) (Wei et al.\n2022). CoT techniques assist LLMs in generating a series\nof logical reasoning steps, enabling them to think step by\nstep about a question and arrive at the correct answer. CoT\nhas significantly improved language models’ performance in\ngenerating rationales and inferring accurate answers in nu-\nmerous domains, including commonsense and arithmetic. In\nthis section, we will discuss the progress made in eliciting\nCoT reasoning by prompting and fine-tuning language mod-\nels.\nFor example, CoT of large language models dramati-\ncally improve the performance of large language models on\narithmetic problems and symbolic reasoning. Existing CoT\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18181\nQuestion: Which property do \nthese  three objects have in \ncommon?\nContext：Select the best answer .\nOptions：\nLanguage\n(A) shiny (B) slippery (C) opaque\nVision\nVAE Diffussion Process\nLatent Space\nz zT\nDenoising U-Net\nQ  \nKV\nQ  \nKV\nQ Q\nKV KV\nText  \nEncoder\nText  \nDecoder Answer\nFor each  object, decide  if it has  that \nproperty. A opaque  object does not let \nlight  through.  All  three  objects are \nopaque.  A slippery object is  hard  to \nhold  onto  or stand  on.  The  tortoise \nshell and the crown are not slippery. \nYou can usually  see your  reflection  in \na shiny  object.  T he basketball is  not \nshiny . The  property  that  all  three \nobjects  have  in common is opaque.\nRationale\nKV  \nQ\nFeature  \nFusion\nFigure 3: Overview of our multi-modal latent space learning via diffusion process for chain-of-thought reasoning in language\nmodels. Our framework consists of two stages: (i) rationale generation and (ii) answer inference.\nprompting can be categorized into two major paradigms:\nZero-shot-CoT (Figure 2 (a)) and Few-shot-CoT (Figure 2\n(b)). Zero-shot-CoT (Kojima et al. 2022) leverages a sin-\ngle prompt like “Let’s think step by step” to generate rea-\nsoning chains. Few-shot-CoT (Wei et al. 2022) uses reason-\ning demonstrations one by one. For example, prompting a\nPaLM 540B with eight chain-of-thought exemplars achieved\nstate-of-the-art accuracy on the GSM8K benchmark of math\nword problems, surpassing even finetuned GPT-3 with a ver-\nifier (Wei et al. 2022).\nMulti-Modal CoT Reasoning in LLMs\nTo address the issue of hallucinations, which can lead to in-\ncorrect answers, and handle real-world multi-modal tasks\neffectively, multi-modal information can guide models to\ngenerate logical rationales. Recent studies on multi-modal\nChain-of-Thought (CoT) outperform the previous state-of-\nthe-art large language model (ChatGPT, GPT-3.5) by 16 per-\ncentage points, achieving 91.68% accuracy and even sur-\npassing human performance on the ScienceQA benchmark\n(Zhang et al. 2023b).\nLeveraging vision information effectively and fusing\nvisual features with text representation in multi-modal\nChain-of-Thought (CoT) poses a significant challenge. Prior\nwork (Lu et al. 2022) has attempted to use image captions\nand incorporate them after text input, but this approach re-\nsults in substantial information loss of images. Other stud-\nies have proposed a method that encodes texts and images\nusing a Transformer encoder and convolutional neural net-\nwork, respectively (Zhang et al. 2023a). The two sequences\nof representations are then fused using an attention layer for\ncross-modal interaction. To extract image features, Zhang\net al. (Zhang et al. 2023b) employed off-the-shelf vision\nextraction models such as DETR (Carion et al. 2020) or\nCLIP (Radford et al. 2021) to obtain patch-level features and\nfused the information from the two modalities using an at-\ntention mechanism.\nMethod\nIn this section, we introduce our proposed Diffusion Process\nenhanced Multi-Modal CoT (DPMM-CoT) method. We fol-\nlow the Multi-Modal CoT (MM-CoT) approach proposed\nby Zhang et al. (Zhang et al. 2023b) as our baseline. The\noverview of our full model is illustrated in Figure 3.\nMulti-modal CoT\nTask Definition In multi-modal reasoning, a language in-\nput including a question XQ, its language context XL, the\noptions XO, and its corresponding image XV are usually\ngiven as inputs to the model. The model is required to an-\nswer a question XQ according to the options XO to obtain\nthe answer YA . In other words, the model is trained to max-\nimize the likelihood between the predicted answer ˆYA and\nthe true answer YA distribution. MM-CoT breaks down this\nproblem into two steps through the introduction of a ratio-\nnale YR: rationale generation and answer inference. In the\nrationale generation step, the model is required to predict\na rationale ˆYR that can infer the answer, which maximizes\nthe likelihood between predictions and the standard ratio-\nnale R distribution. Then, in the second stage, based on the\nrationale ˆYR, along with the language input including the\nquestion XQ, its language context XL, the options XO, and\nthe corresponding image XV , the model predicts the final\nanswer YA.\nText Encoder For the multi-modal CoT tasks, the text in-\nput differs between the two stages. In the stage of rationale\ngeneration, the text input includes language context XL, a\nquestion XQ, and multiple options XO. In the stage of an-\nswer inference, the text input comprises language context\nXL, a question XQ, multiple options XO, and rationale ˆYR\ngenerated from the first stage. We adopt the Transformer\nmodel for text encoding, which is initialized by the pre-\ntrained model UnifiedQA (Khashabi et al. 2020). We obtain\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18182\nthe text representation ZL as follows:\nZR\nL = ENCODER text([XL; XQ; XO]), (1)\nZA\nL = ENCODER text([XL; XQ; XO, ˆYR]), (2)\nwhere ZL = ZR\nL in the stage of rationale generation and\nZL = ZA\nL in the stage of answer inference.\nImage Feature Extraction and Feature Fusion In multi-\nmodal CoT, the image encoder plays a crucial role in the\nCoT process as it helps to provide additional context and\ninformation to the model. By incorporating visual features\nextracted from input images, the model gains a better un-\nderstanding of the overall context. Specifically, the image\nfeature ZV is first extracted by an image encoder:\nZV = ENCODER img(XV ). (3)\nBased on the acquired image features, in order to integrate\nthe image and text encoding features, a linear layer is first\nused to map the image features. This is primarily for two\npurposes: to unify the dimensions of image and text features,\nand to project the image features onto the same feature space\nthat can be fused with text features.\nZT\nV = Wh ∗ ZV , (4)\nwhere Wh is the learnable weight matrix.\nAs the image features and text features have different tem-\nporal lengths, we use an attention mechanism to project the\nimage features onto the length of the text features based on\nthe correlation between the image and text features. Specifi-\ncally, we useZL as the attention query , withZT\nV as attention\nkeys and values. The resultant projected image features are\nas follows:\nZattn\nV = Softmax (QKT\n√dk\n)V (5)\nwhere dk is the dimension of ZL, Q is the ZL, K and V are\nZT\nV .\nAs the roles of image features and text features in generat-\ning rationales and answers are not static or fixed, we choose\nto use a gate mechanism to fuse vision features and language\nrepresentation, i.e., let the model decide how to use the im-\nage and text features. The gated fusion mechanism ( (Zhang\net al. 2020) (Wu et al. 2021) (Li et al. 2022)) involves two\nsteps: obtaining a score vector between 0 and 1 to determine\nthe importance of each feature (Equation 6), and using the\nscores to fuse the text and attention features (Equation 7).\nα = Sigmoid(WlZL + WvZattn\nV ), (6)\nZfuse = (1− α) ∗ ZL + α ∗ Zattn\nV (7)\nwhere Wl and Wv are learnable parameters for gate projec-\ntion, Zfuse = ZR\nfuse in the stage of rationale generation and\nZfuse = ZA\nfuse in the stage of answer inference.\nText Decoder In multi-modal CoT, the text decoder is re-\nsponsible for generating rationales or inferring the final an-\nswers, taking into account the representation output Zfuse\nof the encoder and the previously decoded token to predict\nthe next one. For example, in the stage of rationale genera-\ntion, the decoder predicts the rationaleY = (y1, . . . , yN ) to-\nken by token, according to the last decoding state and source\ncontext. The rationale probability can be formulated as fol-\nlows:\nsi = SELFATTN (Y<i), (8)\nP(yi|Y<i, Zfuse ; θ) =Softmax (F F N(si+\nCROSS ATTN (si, Zfuse ))), (9)\nwhere θ is the model parameters, yi is the i-th token in\nY with N tokens, si denotes the decoding state at the i-th\ntimestep.\nTherefore, the sequence generation loss LSEQ for model\noptimization can be written as:\nLSEQ =\nNX\ni=1\n−log P(yi|Y<i, Zfuse ; θ) (10)\nwhere Y = YR in the stage of rationale generation, andY =\nYA in the stage of answer inference.\nMulti-modal Latent Space Learning\nIn the current multi-modal CoT work, such as MM-\nCoT (Zhang et al. 2023b), image feature extraction is per-\nformed using off-the-shelf image encoders trained on mod-\nels such as DETR (Carion et al. 2020) and CLIP (Radford\net al. 2021). However, this method has two major drawbacks.\nFirstly, due to the limitations of pre-training objectives, the\nextracted image features are usually shallow and generic in-\nformation that is not specifically optimized for reasoning,\nthus lacking deep semantic information which is required in\nreasoning. Secondly, the image features used for inference\nare highly dependent on language input, meaning that dif-\nferent image features are required for different language de-\nscriptions. Therefore, this work proposes a method of multi-\nmodal latent space learning, which learns flexible image fea-\ntures that are aligned with text inputs in the latent space and\noptimized for the inference process, thus possessing the deep\nsemantics required for reasoning.\nAs Richard Feynman once said, ”What I cannot create, I\ndo not understand.” Therefore, we argue that excellent cre-\nativity must contain excellent understanding. Drawing inspi-\nration from the outstanding generative performance of diffu-\nsion models, we apply the idea of a stable diffusion model\nto obtain a multi-modal latent space. Specifically, we use the\nconcept of a diffusion process to obtain better image fea-\ntures with deep semantics that align with text representa-\ntion. Firstly, we employ the Variational AutoEncoder (V AE)\n(Kingma and Welling 2014) as the image encoder to obtain\nthe latent vector of the image. Then, we add random noise to\nthe latent vector, which follows a Gaussian distribution with\ntime steps. Next, the latent vector is inputted into the UNet\nneural network (Ronneberger, Fischer, and Brox 2015). We\nfuse text representation and image features at a deep level\nby mapping text information into the intermediate layer of\nUNet through a cross-attention layer. By optimizing the dif-\nfusion process, which compares the predicted noise with true\nnoise, the model can obtain better image features with deep\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18183\nsemantics that align with text inputs. This is because the dif-\nfusion process enables the model to learn features that are\nnot only optimized for reasoning but also possess a high de-\ngree of stability and robustness to noise and other distur-\nbances. In this way, the model can obtain the latent space of\nan image with deep semantics from the perspective of diffu-\nsion.\nStable diffusion consists of two main parts: (1) the for-\nward (or diffusion) process and the reverse process. In the\ndiffusion process, random noise following a Gaussian dis-\ntribution is added to the image features of latent space. This\nprocess is entirely run on the latent space and is composed of\na V AE neural network and a scheduling algorithm. (2) The\nreverse process generates an image using an image decoder\nbased on the latent features and text representations. Exist-\ning studies (Kwon, Jeong, and Uh 2022) have shown that\nthe latent space already contains aligned semantic informa-\ntion, so we suppose that it can be utilized to fuse linguistic\nmodality and visual modality for reasoning.\nIn our DPMM-CoT, we first encode the image into the\nlatent space Z0\nV using V AE. Especially, during inference, the\nimage is encoded as a latent vector through the V AE and then\ndirectly fused with the text representation vector to generate\na rational or answer. Then we add random noise that follows\na Gaussian distribution to the latent space of the image.\nZ0\nV = ηV AE(XV ), (11)\nq(Zt\nV |Zt−1\nV ) =N(Zt\nV ;\np\n1 − βtZt−1\nV , βtI) (12)\nwhich indicates the diffusion process that adding noise that\nfollows a Gaussian distribution, where βt is the variance\nschedule, √1 − βtZt−1\nV is the mean,I is identity matrix, and\nη = 0.18215 is the scale factor.\nThe diffusion process of the diffusion model can be ex-\npressed as a Markov chain from t = 0to t = T:\nq(Z0:T\nV ) =q(Z0\nV )\nTY\nt=1\nq(Zt\nV |Zt−1\nV ). (13)\nWhen T → ∞, the final result will become a noisy im-\nage, similar to sampling from an isotropic Gaussian distribu-\ntion. However, we also use a closed-form formula to directly\nsample noisy images at a specific time step t, instead of de-\nsigning an algorithm to iteratively add noise to the image\nfollowing the practice of (Rombach et al. 2022).\nZt\nV = √\nαtZ0\nV +\n√\n1 − αtϵ (14)\nwhere αt = 1 - βt, αt = Qt\ni=1 αi. ϵ is an i.i.d. (indepen-\ndent identically distributed) standard normal random vari-\nable. It is important to distinguish them using different sym-\nbols and subscripts because they are independent and their\nvalues may differ after sampling.\nThe standard diffusion process involves predicting the\nnoise using UNet (Ronneberger, Fischer, and Brox 2015).\nBy utilizing a cross-attention layer to map text information\ninto the intermediate layer of UNet, we can merge text rep-\nresentation with image features. This integration of infor-\nmation from both modalities leads to a more comprehensive\nunderstanding of the underlying structure in the data. Text\nfeatures provide valuable additional semantic information\nthat may not be immediately evident from the visual content\nalone. Incorporating these features into the model allows us\nto better comprehend the context and meaning behind the\nvisual elements. Meanwhile, image features offer rich vi-\nsual information about the objects and scenes depicted in\nthe image. These features enable the model to identify pat-\nterns and relationships between different parts of the image.\nThe fusion of both image and text features through diffusion\nprocess enables the model to leverage the strengths of both\nmodalities, leading to improved latent space learning.\nSpecifically, we predict the noise ϵθ(Zt\nV , t, ZL) by UNet\nwith the attention mechanism between visual featureZt\nV and\ntext representation ZL as follows:\nϵθ(Zt\nV , t, ZL) =UNET(F F N(Softmax (QKT\n√\nd\n)V + Q))\n(15)\nwhere Q = W(i)\nQ · Zt\nV , K = W(i)\nK · ZL, V = W(i)\nV · ZL.\nZt\nV ∈ RN×di\nis an intermediate representation of UNet.\nTherefore, the latent diffusion process loss implemented\nwith Maximum Square Error (MSE) can be written as:\nLLDM = Eϵ∼N(0.1),ZL,t[||ϵ − ϵθ(Zt\nV , t, ZL)||2\n2] (16)\nwhere LLDM is the loss of latent diffusion model, θ is the\nparameters of the model, ϵ is the random noise (an inde-\npendent identically distributed standard normal random vari-\nable). So the total loss of the model is as follows:\nLtotal = LSEQ + LLDM . (17)\nDuring the training process, all model parameters, except\nfor the UNet parameters in the reverse process, are updated.\nExperiments\nSetup\nDatasets To assess CoT on LLMs, we followed the ap-\nproach of MM-CoT (Zhang et al. 2023b) and used the Sci-\nence Question Answering (ScienceQA) (Lu et al. 2022)\ndataset. In addition, we also conducted experiments on\nthe Multi30K multi-modal translation dataset (Elliott et al.\n2016) and followed the work of IKD-MMT (Peng, Zeng,\nand Zhao 2022).\nSettings In our experiment on CoT in LLMs, we em-\nployed a two-stage framework consisting of two procedures:\nrationale generation and answer inference. Both stages\nshared the same model architecture, namely the T5 encoder-\ndecoder architecture (Raffel et al. 2020).\nFor our experiment on multi-modal machine transla-\ntion (MMT), we employed the mT5 encoder-decoder archi-\ntecture, which was initialized using the pre-trained mT5-\nlarge (Xue et al. 2021) checkpoint, which had been pre-\ntrained on a multilingual corpus.\nMain Analysis\nTable 1 presents the main results of our study, which\ncompares the performance of various Visual Question An-\nswering (VQA) models. We evaluated our DPMM-CoT\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18184\nModel Size N\nAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman -\n90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nViL\nT (Kim, Son, and Kim 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al. 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al. 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUnifiedQABase (Khashabi et\nal. 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nUnifiedQABase w/ CoT (Lu et al. 2022) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nChatGPT (GPT\n-3.5) 175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nChatGPT (GPT-3.5) w/ CoT (Lu et al. 2022) 175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nMM-CoTBase(CLIP) 223M+151M\n87.97 80.88 87.36 88.32 84.78 88.15 86.34 86.29 86.32\nMM-CoTBase(DETR) (Zhang\net al. 2023b) 223M+60M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nDPMM-CoTBase 223M+83M 92.72 87.85 89.91 92.72 90.48 91.29 91.45 90.11 90.97\nMM-CoTLarge(DETR)\n(Zhang et al. 2023b) 738M+60M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nDPMM-CoTLarge 738M+83M 95.52 90.33 91.36 95.50 93.26 92.68 93.28 93.47 93.35\nTable 1: Main results on ScienceQA test set (%). Size = backbone model size. Question classes: NAT = natural science, SOC\n= social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6,\nG7-12 = grades 7-12. Results except ours are taken from (Lu et al. 2022) and (Zhang et al. 2023b).\nMethod (i) QCM→ R (ii) QCMR→ A\nMM-CoTBase 96.97 84.91\nDPMM-CoTBase 98.18 90.97\nTable 2: Performance of two-stage.\nModel EN-DE EN-FR\nTest16 Test17 MSCOCO Test16 Test17\nIKD-MMT 41.28 33.83 30.17 62.53 54.84\nmT5 38.56 33.01 28.10 61.71 53.84\nDPMM-MT 41.63 36.18 30.75 66.91 57.80\nTable 3: BLEU score of EN-DE and EN-FR tasks.\nmodel against MM-CoT, a baseline, and found that DPMM-\nCoTBase outperforms MM-CoT Base(DETR) by 6.06% and\nDPMM-CoTLarge outperforms MM-CoT Large(DETR) by\n1.67%. Notably, when questions involve visual context\n(IMG column), DPMM-CoT Base and DPMM-CoTLarge out-\nperform MM-CoT Base(DETR) and MM-CoT Large(DETR)\nby 7.58% and 4.46%, respectively.\nCompared to other VQA baselines, DPMM-CoT Large\noutperforms VisualBERT (Li et al. 2019) by 31.48%,\ndemonstrating that autoregressive language pre-training and\nlarger language models are effective for problem solving.\nAnd DPMM-CoTLarge surpasses the UnifiedQA model with\nCoT (Lu et al. 2022) by 19.24%. This suggests that only\nleveraging captions of images as visual context causes se-\nvere information loss and hallucination in CoT.\nAdditionally, we found that DPMM-CoTLarge outperforms\nthe strong LLM – ChatGPT by 18.18%, demonstrating that\nlanguage models under 1B parameters can perform bet-\nter than general LLMs when fine-tuned with appropriate\nnetwork designs and information. Moreover, our DPMM-\nCoTBase and DPMM-CoTLarge both outperform human per-\nformance, indicating the effectiveness of our model. These\nresults suggest that multi-modal latent space learning is sig-\nnificant for understanding flexible and deep visual informa-\ntion. In Table 2, the ROUGE-L results of rationals generated\nby DPMM-CoTBase and MM-CoTBase, as well as the accu-\nracy of answers inferred, are shown.\nTo verify that the improvement of DPMM-CoTLarge orig-\ninates from multi-modal latent space learning via the diffu-\nsion process rather than an increase in the number of param-\neters, we utilized fixed visual features extracted by clip-vit-\nbase-patch32 (Zhang et al. 2023b), which has 151M param-\neters. The result shows that while an increase in the number\nof parameters may contribute to improved performance on\nmulti-modal QA tasks, it is still far from our DPMM-CoT\nmodel. This suggests that our improvements are due to a\ndeeper understanding of visual information gained through\nmulti-modal latent space learning.\nFurther Analysis\nGeneralization to More Multi-modal Tasks To demon-\nstrate the generality of our method across different multi-\nmodal tasks, we conducted experiments on Multimodal Ma-\nchine Translation (MMT). The main results are presented\nin Table 3. We trained our Diffusion Process Enhanced\nMulti-Modal Machine Translation (DPMM-MT) model on\nthe Multi30K dataset, which includes English-to-French and\nEnglish-to-German translations. We then evaluated DPMM-\nMT on three test sets: test2016-flickr (Test16), test2017-\nflickr (Test17), and test2017-mscoco (MSCOCO). Firstly,\ncompared to the mT5 baseline that does not use image\nfeatures, we achieved significant improvements in En-De\nof 3.07, 3.17 and 2.65, and in En-Fr of 5.20 and 3.96,\nrespectively. This indicates the crucial role of image fea-\ntures in multimodal machine translation. We achieved a\nnew state-of-the-art (SOTA) result on Test17 and MSCOCO\nwith English-to-German translation and Test16 and Test17\nwith English-to-French translation. Specifically, DPMM-\nMT outperformed the previous SOTA by 2.35 (33.83 →\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18185\nModel NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nZero Tensor 92.72 87.85 89.91 92.72 90.48 91.29 91.45 90.11 90.97\nBlank Image 92.54 82.56 89.91 92.86 88.35 90.94 90.68 88.13 89.77\nTable 4: Results of different way of solving problems without images.\nModel N\nAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nOur model 92.72\n87.85 89.91 92.72 90.48 91.29 91.45 90.11 90.97\nw/o Stable Diffusion Pre-training 88.63 80.43 85.45 89.93 84.88 85.92 87.15 84.18 86.09\nw/o UNet 91.92 82.56 89.91 92.18 88.35 90.52 89.98 88.46 89.44\nw/ Frozen V AE 91.07 82.00 90.36 91.64 87.36 90.73 89.35 88.33 88.99\nTable 5: Ablation results of our method.\n36.18) and 0.58 (30.17 → 30.75) on Test17 and MSCOCO\nwith English-to-German translation, respectively. DPMM-\nMT outperformed the previous SOTA by 4.38 (62.53 →\n66.91) and 2.96 (54.84 → 57.80) on Test16 and Test17 with\nEnglish-to-French translation, respectively. For Test16 with\nEnglish-to-German translation, we also achieved compara-\nble results to the previous SOTA - Gated Fusion (Wu et al.\n2021). These improvements across multiple datasets suggest\nthat utilizing our proposed multi-modal latent space learning\nto extract deep image semantics is useful for enhancing the\nperformance of multi-modal machine translation.\nProblems without Images Since not all questions in the\nScienceQA task (or other real-life tasks) include images, our\nmethod needed to be adaptable to image-less questions. For\nthis purpose, we explored two approaches: using blank im-\nages or null tensors as input for these questions. We ana-\nlyzed the results using models DPMM-CoTBase, and the ex-\nperiment outcomes are presented in Table 4. Our findings\nshow that using zero tensors resulted in a 1.20% higher ac-\ncuracy than using blank images. This may be attributed that\nblank images may introduce misleading information during\nthe diffusion process.\nAblation Study\nTo illustrate the effect of each component in the Diffusion\nProcess on multi-modal latent space learning, we conducted\nan ablation study. As shown in Table 5, we tested whether\npre-trained stable diffusion module is useful for multi-modal\nlatent space learning. We randomly initialized the param-\neters of UNet and V AE, and evaluated the result without\nStable Diffusion Pre-training. The results show that diffu-\nsion models including V AE and UNet initialized from pre-\ntrained model are indeed useful for DPMM-CoT. The accu-\nracy declined by 4.88% (90.97%→ 86.09%), demonstrating\nthe importance of good initialization for producing effective\nmulti-modal latent space. Furthermore, we found that diffu-\nsion components initialized by random parameters actually\noutperform the baseline MM-CoT(DETR). This highlights\nthe ability of the diffusion process to deeply understand\nimage information after being trained on the ScienceQA\ndataset, producing effective image features aligned with text\nrepresentation.\nTo further demonstrate the importance of diffusion pro-\ncess in multi-modal latent space learning, we trained the\nmodel without UNet. The images were only encoded by\nV AE to produce latents. The result in Table 5 shows that ac-\ncuracy declined by 1.53% (90.97% → 89.44%), indicating\nthe significance of diffusion process to produce multi-modal\nlatent space. These results testify that diffusion process is a\nkey part of multi-modal latent space learning, and visual fea-\nture extraction by encoder alone is insufficient. By adding\nnoise and predicting noise with UNet guided by text repre-\nsentation, the multi-modal latent space learning gains a deep\nunderstanding of image with language thoughts.\nThe quality of the vision latent vector that V AE produces\nhas a significant impact on the effectiveness of the CoT. To\nprove this, we tried not updating the parameters of V AE\nduring CoT training but instead used pre-trained param-\neters from Stable-Diffusion-v1-4. The results (90.97% →\n88.99%) show that V AE trained with CoT is helpful in pro-\nducing better latent vectors for use in reasoning. This also\ndemonstrates that for reasoning tasks, it’s not enough to per-\nform only self-supervised pre-training.\nConclusion\nIn this work, we focuses on improving the production of\nmulti-modal latent spaces that can effectively understand\nboth linguistic and visual information at a deeper level.\nTo achieve this, we introduce DPMM-CoT, a multi-modal\nlatent space learning approach via diffusion process for\nCoT reasoning in language models. Our experimental re-\nsults demonstrate that our method performs exceptionally\nwell on multi-modal tasks. Notably, DPMM-CoTBase outper-\nforms MM-CoTBase by 6.06%, while DPMM-CoT Large out-\nperforms MM-CoT Large by 1.67%. We also conducted ad-\nditional experiments on multi-modal machine translation,\nwhich verified the effectiveness of our proposed multi-\nmodal latent space learning method on a wider range of\nmulti-modal tasks. Moreover, our concrete analysis shows\nthat our method enables language models to attain deeper,\nmore flexible, and aligned features for language thought,\nthereby enhancing their reasoning abilities. In the future, we\nplan to evaluate our method on more multi-modal tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18186\nReferences\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-End Object Detection\nwith Transformers. In Vedaldi, A.; Bischof, H.; Brox, T.;\nand Frahm, J., eds., Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020,\nProceedings, Part I, volume 12346 ofLecture Notes in Com-\nputer Science, 213–229. Springer.\nElliott, D.; Frank, S.; Sima’an, K.; and Specia, L. 2016.\nMulti30K: Multilingual English-German Image Descrip-\ntions. In Proceedings of the 5th Workshop on Vision and\nLanguage, 70–74. Berlin, Germany: Association for Com-\nputational Linguistics.\nKhashabi, D.; Min, S.; Khot, T.; Sabharwal, A.; Tafjord, O.;\nClark, P.; and Hajishirzi, H. 2020. UNIFIEDQA: Crossing\nFormat Boundaries with a Single QA System. In Findings\nof the Association for Computational Linguistics: EMNLP\n2020, 1896–1907. Online: Association for Computational\nLinguistics.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-\nlanguage transformer without convolution or region super-\nvision. In International Conference on Machine Learning,\n5583–5594. PMLR.\nKingma, D. P.; and Welling, M. 2014. Auto-Encoding Vari-\national Bayes. In Bengio, Y .; and LeCun, Y ., eds., 2nd In-\nternational Conference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Conference\nTrack Proceedings.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large Language Models are Zero-Shot Reasoners.\nCoRR, abs/2205.11916.\nKwon, M.; Jeong, J.; and Uh, Y . 2022. Diffusion\nModels already have a Semantic Latent Space. CoRR,\nabs/2210.10960.\nLi, B.; Lv, C.; Zhou, Z.; Zhou, T.; Xiao, T.; Ma, A.; and Zhu,\nJ. 2022. On Vision Features in Multimodal Machine Trans-\nlation. In Muresan, S.; Nakov, P.; and Villavicencio, A.,\neds., Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 6327–\n6337. Association for Computational Linguistics.\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-\nW. 2019. Visualbert: A simple and performant baseline for\nvision and language. arXiv preprint arXiv:1908.03557.\nLu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.; Zhu, S.;\nTafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to Ex-\nplain: Multimodal Reasoning via Thought Chains for Sci-\nence Question Answering. CoRR, abs/2209.09513.\nLu, P.; Qiu, L.; Chen, J.; Xia, T.; Zhao, Y .; Zhang, W.; Yu, Z.;\nLiang, X.; and Zhu, S.-C. 2021. Iconqa: A new benchmark\nfor abstract diagram understanding and visual language rea-\nsoning. arXiv preprint arXiv:2110.13214.\nPeng, R.; Zeng, Y .; and Zhao, J. 2022. Distill The Image to\nNowhere: Inversion Knowledge Distillation for Multimodal\nMachine Translation. In Goldberg, Y .; Kozareva, Z.; and\nZhang, Y ., eds.,Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11,\n2022, 2379–2390. Association for Computational Linguis-\ntics.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision. In\nMeila, M.; and Zhang, T., eds., Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, volume 139 ofProceedings\nof Machine Learning Research, 8748–8763. PMLR.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res., 21: 140:1–140:67.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-Resolution Image Synthesis with Latent\nDiffusion Models. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022, 10674–10685. IEEE.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-Net:\nConvolutional Networks for Biomedical Image Segmenta-\ntion. In Navab, N.; Hornegger, J.; III, W. M. W.; and Frangi,\nA. F., eds., Medical Image Computing and Computer-\nAssisted Intervention - MICCAI 2015 - 18th International\nConference Munich, Germany, October 5 - 9, 2015, Pro-\nceedings, Part III, volume 9351 of Lecture Notes in Com-\nputer Science, 234–241. Springer.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E. H.;\nLe, Q.; and Zhou, D. 2022. Chain of Thought Prompt-\ning Elicits Reasoning in Large Language Models. CoRR,\nabs/2201.11903.\nWu, Z.; Kong, L.; Bi, W.; Li, X.; and Kao, B. 2021. Good\nfor Misconceived Reasons: An Empirical Revisiting on the\nNeed for Visual Context in Multimodal Machine Transla-\ntion. arXiv preprint arXiv:2105.14462.\nXue, L.; Constant, N.; Roberts, A.; Kale, M.; Al-Rfou, R.;\nSiddhant, A.; Barua, A.; and Raffel, C. 2021. mT5: A Mas-\nsively Multilingual Pre-trained Text-to-Text Transformer. In\nToutanova, K.; Rumshisky, A.; Zettlemoyer, L.; Hakkani-\nT¨ur, D.; Beltagy, I.; Bethard, S.; Cotterell, R.; Chakraborty,\nT.; and Zhou, Y ., eds., Proceedings of the 2021 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, 483–498. As-\nsociation for Computational Linguistics.\nZhang, Z.; Chen, K.; Wang, R.; Utiyama, M.; Sumita, E.;\nLi, Z.; and Zhao, H. 2020. Neural Machine Translation with\nUniversal Visual Representation. In International Confer-\nence on Learning Representations.\nZhang, Z.; Chen, K.; Wang, R.; Utiyama, M.; Sumita, E.; Li,\nZ.; and Zhao, H. 2023a. Universal Multimodal Representa-\ntion for Language Understanding. CoRR, abs/2301.03344.\nZhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and\nSmola, A. 2023b. Multimodal Chain-of-Thought Reasoning\nin Language Models. CoRR, abs/2302.00923.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18187"
}